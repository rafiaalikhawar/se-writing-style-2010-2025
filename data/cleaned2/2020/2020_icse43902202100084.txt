interpretation enabled software reuse detection based on a multi level birthmark model xi xu y qinghua zheng y zheng yanx ming fan z ang jia z and ting liu z key laboratory of intelligent networks and network security ministry of education china yschool of computer science and technology xi an jiaotong university china zschool of cyber science and engineering xi an jiaotong university china xstate key lab on integrated services networks school of cyber engineering xidian university china department of communications and networking aalto university finland xx19960325 stu.xjtu.edu.cn qhzheng xjtu.edu.cn zyan xidian.edu.cn mingfan mail.xjtu.edu.cn jiaang stu.xjtu.edu.cn tingliu mail.xjtu.edu.cn abstract software reuse especially partial reuse poses legal and security threats to software development.
since its source codes are usually unavailable software reuse is hard to be detected with interpretation.
on the other hand current approaches suffer from poor detection accuracy and efficiency far from satisfying practical demands.
to tackle these problems in this paper we propose isrd an interpretation enabled software reuse detection approach based on a multi level birthmark model that contains function level basic block level and instruction level.
to overcome obfuscation caused by cross compilation we represent function semantics with minimum branch path mbp and perform normalization to extract core semantics of instructions.
for efficiently detecting reused functions a process for intent search based on anchor recognition is designed to speed up reuse detection.
it uses strict instruction match and identical library call invocation check to find anchor functions in short anchors and then traverses neighbors of the anchors to explore potentially matched function pairs.
extensive experiments based on two realworld binary datasets reveal that isrd is interpretable effective and efficient which achieves precision and recall.
moreover it is resilient to cross compilation outperforming stateof the art approaches.
index terms binary similarity analysis software reuse detection multi level software birthmark interpretation i. i ntroduction along with the growing popularity of open source software software reuse becomes a common phenomenon.
however extensive reuse of existing codes leads to numerous license violation issues .
for example cisco and vmware were exposed to significant legal issues because they did not adhere to the licensing terms of linux kernel .
what is more security issues could be raised due to careless software reuse .
the goal of software reuse detection is to determine whether a candidate program contains similar codes already used in a target program.
there are many approaches proposed in the literature.
according to the analysis objects these approaches can be divided into two groups source code reuse detection and binary code reuse detection.
the first calculates code similarity by abstracting source codes into a set of characteristics such as string token corresponding author zheng yan.
abstract syntax tree ast and program dependency graph pdg .
since the source code of a candidate program is typically unavailable in reality binary code reuse detection is used widely for software plagiarism detection malware detection patch analysis and so on.
however the existing binary code reuse detection approaches suffer from three limitations as described below.
poor interpretability the detection results of the existing approaches lack of the ability to provide detailed evidence to support reuse detection results because they usually only report their results in form of similarity scores ranging from to .
the ability to comprehensively interpreting the detection results is extremely important since it can provide the details or reasons to make the detection results acceptable or easy to be understood.
poor accuracy most approaches that rely on structural and syntax information fail to deal with the differences caused by variations in compilation.
this is because different compilations of a source program naturally produce different structures and syntax in its binary codes forming obfuscation.
for example the approaches proposed in that operate at the boundaries of a basic block might fail to deal with basic block splitting or merging.
poor efficiency several works use a brute force method to identify reused functions which is prohibitively expensive since it measures the similarities of all function pairs between a target program and a candidate program.
such approaches lead to poor efficiency if the numbers of functions in both programs are big.
therefore these approaches are neither effective in case that a reused part only makes up a small percentage of the candidate program nor efficient if an excessive number of function pairs need to be compared.
to overcome the above limitations we propose isrd a novel interpretation enabled software reuse detection approach based on a multi level birthmark model which holds the following salient advantages interpretable detection results.
isrd is capable of capturing program semantics from coarse granularity to fine granularity and uniquely identifying a program with a multi level birth8732021 ieee acm 43rd international conference on software engineering icse .
ieee mark model that contains function level basic block level and instruction level.
specifically at the function level a function call graph fcg is constructed to profile program behavior.
the fcg of a program is a directed graph which consists of a set of nodes representing functions and a set of edges representing caller and callee relationships among functions .
then basic block chains and normalized instructions are extracted to demonstrate the semantics of the function at the basic block level and the instruction level respectively.
the similar parts between the birthmark of a target program and that of a candidate program in the above three levels can reconstruct a reuse scene to interpret and justify detection results.
obviously this kind of demonstration can assist easy understanding and trust on a detection result unlike a simple value reuse indicator.
high accuracy.
to achieve high accuracy of reuse detection we perform normalization at different levels of our birthmark model.
specifically at the basic block level we first transform each function into a set of minimum branch paths mbps which are length variant partial execution paths starting from an initial node or a branch node and ending at a terminal node or an adjacent branch node.
then to mitigate huge differences in instructions caused by cross compilation we only consider the key instructions to represent their core semantics.
furthermore we lift low level assembly instructions up to highlevel operations and remove operands from them to address syntax differences of instructions to complete the process of instruction normalization.
high efficiency.
to speed up the similarity calculation among thousands of function pairs we propose a process for intent search based on anchor recognition to first recognize anchor functions in short anchors and then perform intent search originated from the anchors to discover all potentially matched function pairs.
in this way we significantly reduce the number of comparisons before similarity calculation and meanwhile ensure comparison quality.
concretely the process leverages on strict instruction match and identical library call invocation check to search for matched function pairs as anchors by considering both developer defined functions and library functions.
through intent search originated from the anchors it further explores neighbors of the anchors to find new function pairs with high similarity scores.
moreover since there is currently no dataset that can be directly used for partial reuse detection tests we construct a dataset that contains real world software projects and manually label a total of partial reuses.
the dataset has been published on website .
by evaluating isrd based on our constructed dataset and a widely used dataset we demonstrate that isrd exhibits impressive software reuse detection performance.
in summary the major contributions of this paper include i we propose a novel fine granular multi level birthmark model to uniquely represent program semantics and enable interpretability of software reuse detection result.
ii we perform normalization at both the basic block level and the instruction level to resist semantics preservingobfuscation for accurate software reuse detection.
iii we design a process to recognize anchors and conduct intent search originated from the anchors which can greatly reduce the number of comparisons thus significantly accelerate detection speed.
iv we implement isrd and evaluate its performance with extensive experiments.
the results reveal that isrd is interpretable can effectively and efficiently detect partial reuse.
it is also resilient to cross compilation outperforming the state of the art approaches.
ii.
r elated work according to analysis techniques existing binary code reuse detection approaches can be divided into two main categories static analysis and dynamic analysis.
static analysis.
static analysis is applied on binaries without running programs.
bindiff was proposed to use the structural similarity of cfg to compare binary codes.
luo et al.
proposed cop a binary oriented obfuscation resilient method for code reuse detection which combines rigorous program semantics with the longest common subsequence based fuzzy matching.
david et al.
proposed a new approach eshof calculating binaries similarity which firstly decomposes binary codes into small comparable fragments then defines semantic similarity between fragments and further uses statistical reasoning to lift fragment similarity into the similarity between procedures.
chandramohan et al.
proposed bingo which captures complete function semantics by inlining the relevant library and developer defined functions and models binary functions in a program structure agnostic fashion by using length variant partial traces.
feng et al.
proposed genius to search vulnerabilities in massive iot ecosystems by converting attributed control flow graph acfg into high level numeric feature vectors.
different from genius that embeds an acfg by taking a codebookbased approach xu et al.
proposed gemini to take a neural network based approach to transform the acfgs into embeddings of binary functions for similarity detection.
liu et al.
proposed a solution named diff which employs three semantic features intra function feature inter function feature and inter module feature to address cross version binary code similarity detection challenges.
dynamic analysis.
a dynamic approach for binary code reuse detection is performed during code execution by running programs.
tian et al.
proposed dykis to uniquely identify a program for detecting similar programs.
tian et al.
presented a framework called thread oblivious dynamic birthmark tob that revives existing techniques to detect reuse of multi thread programs.
ming et al.
proposed a logic based approach lopd by leveraging dynamic symbolic execution and theorem proving techniques to capture dissimilarities between two programs in order to rule out semantically different programs.
for better understanding the differences of the above approaches from isrd we compare them in table i in terms of 874fig.
isrd overview table i comparison with existing approaches approach type1granu.2inter.3effec.3effic.
bindiff st b cop st b esh st i bingo st b genius st b gemini st b diff st n.a.
dykis dy tob dy lopd dy isrd st i b f 1st static dy dynamic.
2granu.
granularity i instruction level b basic block level f function level.
3inter.
interpretability effec.
effectiveness effic.
efficiency and respecti vely represent satisfying the criteria partially satisfying the criteria and not satisfying the criteria.
analysis type granularity of applied birthmark interpretability effectiveness for resisting obfuscation caused by crosscompilation and efficiency.
from table i we observe that the existing approaches cannot interpret detection results although they can resist obfuscation caused by cross compilation to some extent.
genius gemini and diff use deep learning to detect reuse and achieve high efficiency but they cannot interpret detection results.
none existing approach can comprehensively overcome the aforementioned limitations except for isrd which shows the novelty and advantages of isrd .
iii.
isrd design in this section we introduce the technical details of isrd which contains two main stages as illustrated in fig.
.
birthmark generation.
this stage constructs the birthmarks of the target program and the candidate program.
the birthmark relates to three levels i.e.
function level basic block level and instruction level.
at the function level an fcg is constructed to depict the program semantics.
at the basic block level the mbps of a given function are extracted to profile its behaviors.
at the instruction level normalization is performed to capture instruction core semantics.
reuse detection.
since the direct comparison of all function pairs between two programs is a time consuming job a process for intent search based on anchor recognition is proposed to recognize anchors and conduct intent search originated from the anchors to significantly accelerate function pair matching.
specifically in anchor recognition strict instruction match and identical library call invocation check are used to find theanchors.
then in intent search we originate from the anchor pairs to explore potentially matched function pairs based on their function call relationships.
a. birthmark generation a birthmark is a set of characteristics extracted from a program that reflects its semantic behaviors which can be used to uniquely identify a program and is resilient to semanticspreserving code transformations.
in the literature there are many proposed birthmarks with different granularities to represent a program.
but in practice applying coarse granularity may restrict program similarity catching in a precise way.
for example a program level birthmark is unable to detect partial reuse since a candidate program often reuses only a part of a target program.
meanwhile the birthmark similarity at a fine granularity level cannot be used to infer the similarity at a coarse granularity level.
for example given the similarity at instructions we cannot conclude that the functions of two programs are similar.
to address this problem we propose a multi level birthmark model to characterize a program by involving three level granularities to form a hierarchical birthmark from the function level to the basic block level and finally the instruction level.
on the top of the hierarchical birthmark to depict the program semantics from the point of a macroscopic view we first construct the fcg of a program to describe its behavior.
then we turn our attention to the individual functions in the program by distilling the function semantics with mbp representation.
finally to further capture the instruction semantics we perform normalization on the finest granularity instructions.
consequently a program can be identified by a three level birthmark that contains function birthmark basic block birthmark and instruction birthmark.
next we discuss the details of birthmark at each level.
fcg construction to capture program semantics at the function level we construct fcg for both the target program and the candidate program.
the fcg captures the functionality and objective of a program semantically from its structural information to profile program behaviors .
in order to construct fcg of a given program we first identify the invocation statements i.e.
call from its assembly code to extract callers and callees.
then the callers and the callees are added into a graph as nodes.
in addition if a function call relation exists between a caller and a callee an edge is inserted between them in the graph.
mbp extraction to characterize function semantics control flow graph cfg is applied which contains detailed information of the basic blocks in a function.
in a cfg each node represents a basic block that is a straight line piece of code without any branch and each edge represents the control flow relationship among blocks.
a cfg is defined as follows.
definition control flow graph cfg a cfg is a directed graph g v e .
v fvij1 i ngdenotes the set of basic blocks of a function where vi2vis theith block.
e v vdenotes the set of control flows where vi vj 2eindicates that a control flow is from vito vj.
however existing approaches that operated at the boundaries of basic blocks are vulnerable to block splitting or merging when different compilation processes are applied.
our solution to this problem is inspired by tracy which uses tracelets partial traces of an execution to compare function similarity.
concretely we propose mbps that are partial straight line execution paths between branching nodes in a cfg.
a branching node is a node that has more than one successor node.
to extract mbps we firstly pre process the cfg by grouping basic blocks into a number of straight line paths i.e.
replacing edges that connect two blocks with a single out edge and a single in edge respectively .
this kind of grouping does not affect the function semantics and is only for the purpose of simplifying the generated mbps which is defined as below.
definition minimum branch path mbp let a path extracted from a cfg be denoted as a node sequence p hvp1 vpni.
a pathpis a mbp if the following conditions are satisfied vp1is an initial node which has no predecessor or is a branching node.
vpnis a terminal node which has no successor or is a branching node.
no other nodes in pare initial or terminal nodes.
unlike the fix lengthed tracelet proposed in mbp is variable in length according to the structure of cfg.
it has a number of characteristics making it suitable for representing function semantics semantics exhibition the combination of basic blocks and control flow in mbp can represent the execution of a function and capture its semantics.
effectiveness and efficiency trying to gather and analyze all paths in a cfg is clearly infeasible.
mbp effectively cuts down the size and the number of paths of cfg by only considering sub paths between branching nodes.
thus it can help in speeding up function semantics matching.
structural variation resilience the absence of branches in mbp implies that it would be less vulnerable than cfg to structural changes caused by block splitting and merging.algorithm mbp extration from cfg input g v e gis a cfg.
output p pis the set of all mbps.
1p 2foreachvi2vdo ifin vi orout vi 1then extract vi !p 5returnp 6function extract vi 7pi foreach vi vj 2edo 9vi vj!p while out vj do vk vj vk 2e!p vj vk 13p!pi returnpi fig.
a sample cfg and its extracted mbps resilience to jump instruction variations jump instructions are sensitive to obfuscation.
mbp is by nature free from jump instructions due to branch omission.
algorithm shows the steps of extracting mbps from a given cfg.
the output pdenotes the set of extracted mbps.
the functions inandoutreturn the in degree and out degree of a given node.
specifically pis initialized as an empty set.
then for every node viinv ifvihas no direct predecessor nodes or more than one direct successor node the function extract is invoked to extract mbps from vi.
finally the extracted mbps are added to p. fig.
depicts an example of cfg and its extracted mbps.
we observe that the original basic blocks in a controlflow are grouped as execution flows which are already determined.
there are two basic blocks bb bb 2that have more than one direct successor basic blocks.
therefore the extracted mbps from the cfg are bb bb bb bb bb bb bb bb bb bb bb .
instruction normalization syntax is the most direct birthmark to represent instruction semantics.
however different compilation processes may cause significant differences in the assemblies .
to retain the core semantics of instructions and be resilient to obfuscation introduced by cross compilation normalization over instructions is applied.
key instruction extraction.
first equally treating all kinds of instructions may be defeated by compilation variations.
that is because some kinds of instructions are not closely related to semantics and are easily changed across compilation.
to solve this problem we only consider key instructions.
ideally the key instructions should constitute a small portion of a whole 876execution sequence and must be relatively unique.
through observation we find that there exist a large number of datatransfer instructions such as push andpop especially mov in almost all mbps.
these instructions can be discarded because they usually facilitate computations rather than belong to a part of mbp logic.
furthermore they are easily added and deleted compared with other instructions.
actually deleting all data transfer instructions is probably the simplest approach to solve the problem caused by crosscompilation but it might lead to the loss of data transfer semantics.
thus we only keep the first data transfer instruction when multiple data transfer instructions appear continuously.
instruction lifting.
as we discussed earlier the same operation can be expressed in different instructions.
to achieve semantics equivalence on instructions we lift the instructions into high level operations.
for example two instructions inc andadd can be mapped to one addition operation.
operand removing.
the operands in the instructions are easily changed in the compilation.
even compiling the same source code with the same compilation settings the operands can be different due to their differences in memory layout.
therefore we strip the operands from the instructions.
at this point by using the proposed multi level birthmark model we construct the three level birthmarks for both target and candidate programs which give a detailed description of the program semantics from coarse granularity to fine granularity.
b. reuse detection after birthmark generation given the target program and the candidate program we can capture their similarity relationship based on their birthmarks for reuse detection.
to trade off between coarse granularity and fine granularity it is reasonable to take the function as a comparison unit.
in order to compare functions we first perform comparison at the instruction level then combine the comparison results to compute the similarity at the basic block level and then the function similarity is determined.
finally the comparison results between functions are aggregated to capture the similarity between the target program and the candidate program.
in addition the matching relationship at the three levels is presented to interpret the detection results.
however matching target functions with a large number of candidate functions remains a major bottleneck.
the reason is that the scale of function pair wise comparison increases exponentially with the number of functions.
to tackle this problem an efficient matching process that significantly accelerates the search for matched function pairs between the target and the candidate programs is proposed.
the process contains two main steps i.e.
anchor recognition andintent search to predict which function pairs are likely to match thereby making the matching process much more efficient by avoiding unnecessary matching.
the process first performs strict instruction match and identical library call invocation check to identify matched anchor function pairs then it explores neighbors of the matched anchors to find newalgorithm anchor recognition input dt lt dt ltdenotes the developer defined functions and library functions in the target program.
dc lc dc lcdenotes the developer defined functions and library functions in the candidate program.
output anchor anchor denotes the set of matched anchor function pairs 1anchor fg 2foreachdt2dtdo 3idt getfuncins dt foreachdc2dcdo 5idc getfuncins dc 6simins simins idt idc ifsimins then anchor dt dc 9foreachlt2ltdo foreachlc2lcdo iflc ltthen anchor lt lc function pairs with a high similarity score under the guidance of function level birthmark.
by this we can effectively reduce the number of comparisons before program similarity score calculation to make reuse detection execute swiftly.
anchor recognition this step attempts to find matched anchor function pairs which can efficiently instruct later reused function matching.
specifically we jointly use two ways to recognize the anchors among a huge number of functions in the candidate program.
way given a function in the target program we search the functions in the candidate program that meet strict instruction matching.
here the strict instruction matching indicates that both the numbers of instructions and their sequences in two functions are exactly the same.
way considering that library call invocations provide an important partial semantics of a function we also look for identical library call invocations that appeared in both the candidate program and the target program.
note that way andway operate on the developer defined functions and library functions respectively.
algorithm shows the process of anchor recognition .
the inputsdtandltdenote the developer defined functions and library functions in the target program respectively.
dc andlcdenote the developer defined functions and library functions in the candidate program respectively.
the output anchor denotes a set of matched anchor function pairs.
the function getfuncins takes a function as input and returns its all instructions.
at line we use jaccard distance to measure the similarity between the function pairs in terms ofway i.e.
their similarity in instructions.
after that we obtain the identical library function invocations in both the target program and the candidate program lines .
intent search in the intent search we originate from the matched anchors to discover new matched function pairs.
isrd loops over each recognized anchor and explores its direct 877algorithm intent search input anchor anchor denotes the matched anchor function pair set.
ftb ftbdenotes the function which has the highest priority in the target program.
fcg t fcg c fcg t fcg cdenote the function level birthmark of the target program and the candidate program.
output ff ff denotes a set of potential matched function pair of ftb.
1ff pre suc fg 2foreach fta ftb fta fca 2anchor2fcg tdo 3pre ftb fcb fca fcb 2fcg c 4foreach ftb ftd ftd fcd 2anchor2fcg tdo 5suc ftb fcb fcb fcd 2fcg c 6iflen pre 0andlen suc 0then 7ff pre suc 8else 9ff pre suc neighbors to find new matched function pairs that have a high similarity score.
the valid correspondences found in the previous step are propagated to their neighbors.
and the new identified matched function pairs are added as new anchors.
given a matched anchor function pair aanda the basic idea of intent search is that the neighboring functions of a that are connected to awith an edge in the function level birthmark usually correspond to those of a. in order to search for new matched function pairs we use the function invocation relationship to guide our processing.
we consider the cues got from the current state of the matched function pairs and the function call relationship in fcg.
instead of exploring completely at random we set a priority to guide the search and improve its efficiency by increasing the probability of finding matched function pairs.
we give a high priority to the functions that have the most number of matched neighbors.
subsequently the process should firstly operate on the high priority functions in the candidate program.
with this process all matched function pairs between the target program and the candidate program can be identified.
algorithm shows the process of intent search .
the input anchor denotes the set of matched anchor function pairs ftbdenotes the function that has the highest priority in the target program fcgtandfcgcdenote the function level birthmarks of the target program and the candidate program respectively.
the output ff denotes a set of potentially matched function pairs of ftb.
firstly for each precursor node ftaofftbinfcgt if fta fca is inanchor ftb fcb is added in pre where fcbis the successor node of fca lines .
then for each successor node ftdofftbinfcgt if ftd fcd is inanchor ftb fcb is added in suc where fcbis the precursor node of fcd lines .
finally ifpre andsuc are not empty the intersection of them is added into ff otherwise their union is added into ff lines .
similarity calculation after identifying the potentially matched function pairs we are able to measure the similarities of two functions by calculating the similarity scores based on their basic block level birthmarks mbp sets.
to compute a similarity score for each pair of mbp sets we first show how to compute the similarity score for a pair of mbps using equation .
sim mbp mbp lcs mbp mbp jmbp 1j jmbp 2j we compute the longest common subsequence lcs of two mbps by utilizing the lcs dynamic programming algorithm denoted as lcs mbp mbp .
then the similarity score between two mbps is calculated by taking the length of lcs divided by the average length of two mbps.
by trying each pair of mbps we use the collected individual similarity scores to calculate the similarity of two mbp sets mbp mbp according to the following equation .
sim mbp mbp x mpb 12mbp 1jmpb 1jmaxscorep mpb 12mbp 1jmpb 1j maxscore max sim mbp mbp 22mbp for each mbp in mbp the highest similarity score denoted as maxscore is first obtained using equation inmbp .
then the similarity score of two mbp sets sim mbp mbp is the sum of the highest similarity score of each mbp in mbp 1timed by the length of mbp 1and divided by a base which is the total length of all mbps in mbp .
this allows the similarity score to vary between zero and one inclusively.
the similarities between functions are not sufficient to describe the similarity relationship between the target program and the candidate program.
we combine the matched function pairs and the function innovation relationship into a graph that is the similar part of the function level birthmarks which can reconstruct the similarity scene.
consequently to prove the similarity between the target program and the candidate program the similar parts between their three level birthmarks are distilled to serve as the interpretable detection results.
the similarity between the target program and candidate program can be measured according to the following equation sim t c jffj jcj the similarity score of the target program and candidate programsim t c is calculated by using the number of matched function pairs ff to divide a base which is the function number of candidate program.
the score denotes the percent of reused functions in the candidate program with regard to the target program.
iv.
e valuation in this section we first introduce the setup of our experiments.
then we answer the following six research questions to validate the performance of our approach.
878rq1 can isrd effectively and efficiently detect partial reuse?
rq2 are the detection results of isrd interpretable?
rq3 is isrd resilient to cross compiler version?
rq4 is isrd resilient to cross optimization level ?
rq5 is isrd resilient to cross compiler vendor?
rq6 how good is the result of isrd compared to other related works?
a. study setup evaluation datasets isrd takes the binary code of program pairs as input.
to perform a thorough evaluation we needed ground truth datasets of which the binary code really share the same code.
to this end we used two datasets including a dataset that is constructed by ourselves dataseti and a widely used benchmark dataset dataset ii that is provided from bingo and diff .
to test whether isrd can successfully detect partial reuses we collected real world data from open source platforms to construct dataset i. we first downloaded open source projects from open source platforms e.g.
github and sourceforge which fall into different application domains.
then we selected and labelled a total of real partial reuses as ground truth by using both a manual method and an automatic method.
finally a dataset containing programs with partial reuses was constructed.
we compiled these programs into binaries using gccwith default optimization o2 .
the static information of dataset i is listed in table iii where the numbers in the third and the fourth columns are the lines of program source code and the number of functions in the corresponding compiled binaries respectively.
after compiling some programs would generate more than one binary.
herein we only present the information of the binaries that are used in our evaluation.
we have published dataset i at .
dataset ii is a widely used benchmark to evaluate crosscompilation robustness.
it was commonly used in the literatures .
thus it was adopted to compare the performance of isrd with existing approaches.
the binaries in dataset ii were compiled from the experimental object coreutils which are the basic file shell and text manipulation utilities of the gnu operating system written in c. there are components in coreutils and as a result each compilation produces binaries.
following we used three compilers in our experiment gcc v4.
gcc v4.
and clang v3.
and four optimization levels o f0 and3g resulting in different variants for each of the binaries.
table iii lists the information of dataset ii where the numbers in the 4th 7th columns denote the maximum minimum average and total numbers of functions of the binaries in coreutils respectively.
evaluation metrics the metrics used to measure the performance of isrd are shown in table iv.
false positive rate fpr stands for the ratio of non reusable functions being falsely detected as reusable functions.
false negative rate fnr quantifies the ratio of the reusable functions that aretable ii descriptions of dataset i. program version loc function bzip2 .
.
.
.
zstd .
.
.
.
lzo .
.
minizip .
.
precomp .
.
turbobench lzbench .
brotli .
.
libbsc .
.
libdeflate .
lzfse .
lzlib .
zlib .
.
zlib ng csc gipfeli blosc .
.
liblzg .
.
xz .
.
exserver cknit exjson libsndfile .
.
sndfile2k table iii descriptions of dataset ii.
compiler versionoptimization levelmax min average total gccv4.6o0 o1 o2 o3 v4.8o0 o1 o2 o3 clang v3.0o0 o1 o2 o3 not detected as reusable functions.
the values of precision recall andf measure are calculated as described in table iv.
parameter setting and experimental environment similarity threshold plays an important role in isrd .
a function pair is determined as matched if its similarity score is greater than the similarity threshold.
to determine a proper threshold we varied its values to test over both datasets.
according to our testing results setting the similarity threshold as .
made isrd achieve the best performance.
we implemented isrd inpython .
onubuntu .
.
all programs ran at dell desktop p2417h with cpu i7 .60ghz and 16gb memory.
in the experiments isrd utilized angr to disassemble binaries.
b. answer to rq effectiveness and efficiency of isrd in this evaluation we used dataset i to test whether isrd can effectively and efficiently detect partial reuse.
we ran isrd with dataset i as input and compared its results with the 879table iv evaluation metrics.
term abbr definition true positive tpthe number of reusable functions that are correctly detected as reusable.
true negative tnthe number of unreusable functions that are correctly detected as unreusable.
false negative fnthe number of reusable functions that are incorrectly detected as unreusable.
false positive fpthe number of unreusable functions that are incorrectly detected as reusable.
false positive rate fpr fp fp tn false negative rate fnr fn tp fn precision p tp tp fp recall r tp tp fn f measure f1 2pr p r a cdf of precision b cdf of recall fig.
cdfs of isrd precision and recall on dataset i ground truth.
fig.
and fig.
report the performance of isrd in terms of effectiveness and efficiency to detect partial reuse.
fig.
presents the cumulative distribution function cdf for the precision and recall of the evaluation results.
almost all the precision values are higher than and its average value can reach .
moreover binary pairs achieve precision equal to indicating that all reused functions can be accurately detected as reusable.
for the recall metric its average value reaches indicating that our approach can effectively find reused function pairs.
fig.
reports the cdf with regard to the ratio of the reduced number of function pairs that were calculated by isrd to the original number of function pairs.
we can observe that isrd effectively reduces the size of the function pairs by on average during reused function detection.
specifically when the reused code is only a small fraction of the target program and the candidate program the reduced ratio is up to .
for example the reused code of bzip2 only account for about ofprecomp functions pairs are needed to be calculated if applying some existing approaches i.e.
bindiff tracy whereas function pairs are calculated by isrd .
thus we can conclude that the number of function pairs used in isrd is much smaller than the total number of all function pairs between the target program and the candidate program.
this huge reduction in reuse detection makes isrd practical for large real world programs.
answering rq isrd effectively detect partial reuse and its average precision achieves .isrd can significantly reduce the number of function pairs required for reuse detection up to on average.
thus it is efficient to handle a large scale of programs.
fig.
cdf for reduced ratio of partial reuse detection on dataset i c. answer to rq interpr etability of isrd toevaluate whether the detection results of isrd are interpretable we leveraged tw o programs precomp and minizip in dataset i to perform a case study .
the case study illustrates the interpretability of isrd by pro viding a graphic demonstration of reuse.
herein precomp is a command line precompressor and minizip is a zip manipulation library in c. note that both precomp andminizip reuse tw o compression libraries lzma and bzip2 .
fig.
illustrates the detection res ults of isrd .
the fcgs ofprecomp and minizip are presented on the left side of the figure where the nodes in red and in blue represent the functions in the library lzma andbzip2 respecti vely.
running isrd onprecomp andminizip we got program similarity as which implied that functions of precomp have been already used in minizip .
the detailed detection results are illustrated and interpreted on the right side of the figure.
f or presentation purpose only thelzma is given.
after identifying all matched function pairs between precomp and minizip the matched subgraph were constructed by combining the matched function pairs with their function call relationship in order to reconstruct the reuse scene in the function level of the tw o programs.
the matched subgraph between the tw o programs is presented in fig.
.
in the matched subgraph the node pairs are the matched function pairs between the two programs that have the same function call relationship.
furthermore we singled out function lzma alone decoder with its matched function whose similarity score is .
as an example by including the matched part of this function at the basic block level and the instructi on le vel.in the basic block level as showed in fig.
matched mbp pairs were recognized to demonstrate the similarity between the function pair.
the matched mbps are displayed with the same colors.
finally the similarity relationship of the last matched mbp pair is described in detail at the instruction level.
highlevel matched operation pairs explain the semantic equiv alence at the finest granularity as displayed in fig.
with a similarity score as .
.
answering rq the case study shows that the detection results of isrd is interpretable by describing the matched part between the target program and the candidate program in detail at the function level the basic block level and the instruction level.
880fig.
the detection results of minizip andprecomp table v detection results on resilience to cross compiler version gcc4.
gcc .
p r f fpr fnr o0 o0 .
.
.
.
.
o1 o1 .
.
.
.
.
o2 o2 .
.
.
.
.
o3 o3 .
.
.
.
.
average .
.
.
.
.
d. answer to rq resilience to cross compiler version we compiled coreutils using gcc v4.
and gcc v4.
with various optimization levels o0 to o3 .
such a setup led to different versions for each binary in coreutils .
subsequently we evaluated isrd by comparing the binaries compiled using different compiler versions with the same optimization levels.
table v summarizes this experiment s results where each row heading represents the optimization level used for compilation and the last row reports the average value of each evaluation metric.
for example the second row represents the detection results when the target programs and the candidate programs were compiled using gcc v4.
andgcc v4.
with the same optimization level o0.
the results demonstrate that the average precision can achieve and the recall is indicating that isrd is resilient to the obfuscation caused by different compiler versions.
moreover for no code optimization i.e.
o0 the precision is while the fpr is .
that is even with different version compilers the compilations without code optimization levels lead to highly similar binary codes whereas compiling with high optimization levels yields more differences between binaries.
answering rq isrd is resilient to cross compilerversion with precision and recall on average.
e. answer to rq resilience to cr oss optimization level wecompiled coreutil for x86 32bit architecture using clang v3.
andgcc v4.
with v arious optimization levels o0 to o3 .
with this setup each binary incoreutils had variants.
wetable vi detection results on resilience to cross optimization le vel p r f fpr fnr gcc4.8o0 o1 .
.
.
.
.
o0 o2 .
.
.
.
.
o0 o3 .
.
.
.
.
o1 o2 .
.
.
.
.
o1 o3 .
.
.
.
.
o2 o3 .
.
.
.
.
average .
.
.
.
.
clang .0o0 o1 .
.
.
.
.
o0 o2 .
.
.
.
.
o0 o3 .
.
.
.
.
o1 o2 .
.
.
.
.
o1 o3 .
.
.
.
.
o2 o3 .
.
.
.
.
average .
.
.
.
.
compared the binaries compiled using the same compiler with different optimization le vels.
table vi summarizes the results where each row heading represents the optimization le velused to compile the detection objects.
f or example the second ro w represents the detection results of the tar get programs that were compiled by gcc v4.
with o0 while the candidate programs were com piled with o1 using the same compiler .
we observe that isrd performs much better re garding precision in clang v3.
than in gcc v4.
.
specifically the average precision is95 forclang v3.
yet only is obtained for gcc v4.
.
another interesti ng observ ation is that we get similar patterns for both compilers.
within one compiler type e.g.
gcc v4.
.
the f measure achie vedby matching between the binaries that were all compiled with high code optimization levels i.e.
o2 and o3 is always better than that obtained by matching between the binaries where one is compiled with a high code optimization le vel i.e.
o2 and o3 and the other is compiled with no code optimization i.e.
o0 .
specifically the highest f measure is achiev ed when the tar get program and the candidate program were both compiled with high optimization o2 and o3 respectiv ely.howe ver f measure drops to if the target program and the candidate program were respectiv ely compiled with no code optimization i.e.
o0 881table vii detection results on resilience to cross compiler vendor clang3.
gcc4.
p r f fpr fnr o0 o0 .
.
.
.
.
o1 o1 .
.
.
.
.
o2 o2 .
.
.
.
.
o3 o3 .
.
.
.
.
average .
.
.
.
.
and high code optimization level i.e.
o3 .
this suggests that regardless of the compiler vendor the binaries compiled with high optimization levels are similar.
the influence of whether the optimization kicks into the binaries is very evident.
answering rq isrd is resilient to crossoptimization level to some extent.
evaluating the binaries compiled by gcc v4.
and clang v3.
isrd achieves on average precision and precision respectively.
f. answer to rq resilience to cr oss compiler v endor wecompiled coreutil for x86 32bit architecture using clang v3.
and gcc v4.
with v arious optimization levels o0 to o3 .
different variants were generated for each binary in coreutils with this setup.
subsequently we evaluated isrd on binaries compiled by dif ferent v endors compilers with same optimization le vels.
welist the results in table vii where each ro w heading represents the optimization level used to compile the compared objects.
experimental results sho w that the a verage precision reaches a high de gree with fpr as0 .
from the table we find that the best results are obtained when the candidate programs were compiled with no optimization le vel as the target one.
besides the precision hits the lowest point of91 when the detection binaries were compiled with optimization le velo3.
further across compiler v endors the detection precision drops with the rise of the optimization level except when the detection programs were compiled with optimization le velo1 where the detection programs compiled with optimization level o2 yield better accuracy .
.
answering rq experimental results demonstrate the resilience of isrd across compiler vendors.
moreover it achieves on average precision.
g.answer to rq comparison with related w orks in this experiment we compared isrd with three baseline approaches including bindif f bingo and diff .
bindif f is a binary code similarity detection tool which matches a pair of binaries using a variant of graphisomorphism algorithm.
bingo is a scalable and rob ust binary search engine that supports cross compilation by applying a selectiv e inlining technique to reco vercomplete function semantics.table viii comparison with three baseline approaches indicated by recall bindif f bingo diff isrd clang o0 vs. gcc o3 .
.
.
.
clang o0 vs. clang o3 .
.
.
.
clang o2 vs. clang o3 .
.
.
.
gcc o0 vs. clang o3 .
.
.
.
gcc o0 vs. gcc o3 .
.
.
.
gcc o2 vs. gcc o3 .
.
.
.
average .
.
.
.
diff is a method to detect binary code similarity with a neural netw ork solution to extract intra function semantic features from ra w bytes of binary functions.
tomake a head to head comparison with these approaches we used the same experimental configurations and the same metric as theirs.
more specifically we compiled coreutils using gcc v4.
andclang v3.
with v arious optimization levels from o0 to o3 .
weevaluated six experimental settings.
the recall v alues of reuse detection are reported in table viii where each rowheading represents the optimization level used to compile the tar get programs and the candidate programs.
through comparison on recall we can see that isrd outperforms the three baseline approaches by on average especially outperforms bingo by on average.
wenote that the average recall ofisrd is76 which is lower than the upper experiment results.
this is because the obfuscations of both cross compiler v endor and cross optimization lev el were in volved maki ng the detection much harder than the situation where only one type of obfuscation occurs.
moreov er the w orst result was obtained by each approach when the target programs were compiled by gcc v4.
with no code optimization and the candidate programs were compiled by gcc v4.
with the highest optimization level o3.
on the other hand the best results of all approaches were achiev ed when the tar get programs and the candidate programs were compiled by the same compiler with the high optimization level o2 and o3 respecti vely.
answering rq compared with the baseline approaches isrd achieves better performance regarding a widely used benchmark dataset.
v. limitationsofisrd a. impacts caused by internal reasons function inlining .function inlining is a major internal limitation to isrd .
in isrd the function semantics inside a program is distilled independently.
tobe speci fic a callee function s semantics is not inte grated into a caller function s semantics.
this leads to a partial semantics problem because the strate gy of inlining is selectiv ely applied according to a configured optimization level during compilation.
inlining is utilized in compilers to optimize the binaries for achie ving maximum speed or minimum size .
other approaches e.g.
the selectiv e inlining strategy of bingo that inline relev ant libraries and de veloper defined function semantics can be incorporated into isrd to address the problem.
882library functions.
inisrd two ways are leveraged to recognize the anchors.
the second way is based on checking identical library call invocations.
however this way is limited for two reasons the library functions are os dependent it fails to recognize the library calls that have different names yet with similar functionality e.g.
memcpy andmemmove .
to address the above problems inspired by clcdsa the similarity of cross os library calls can be learned with the help of the documentation and mikolov s word2vec model.
b. impacts related to datasets due to the difficulty of collecting partial reuse samples with accurate labels only programs were selected to construct dataset i and real partial reuses were manually labeled.
in future work we plan to collect additional realworld partial reuse samples and further evaluate and optimize the performance of isrd .
vi.
c onclusion in this paper we proposed isrd an interpretation enabled software reuse detection approach based on a multi level birthmark model.
we used the multi level birthmark model to distill the program semantics from coarse granularity to fine granularity.
through normalization the function semantics and the core semantics of instructions can be effectively represented.
in reuse detection intent search originated from recognized anchors was employed to speed up function pair matching.
extensive experiments reveal that isrd is effective and efficient in detecting partial reuse with interpretation.
it also outperforms state of the art approaches in terms of resilience to cross compilation.
acknowledgment this work was supported by national key r d program of china 2018yfb1004500 national natural science foundation of china u1766215 china postdoctoral science foundation 2019tq0251 2020m673439 youth talent support plan of xi an association for science and technology ministry of education innovation research team irt 17r86 and project of china knowledge centre for engineering science and technology as well as academy of finland grants and .