fruiter a framework for evaluating ui test reuse yixue zhao yixue.zhao usc.edu university of southern california usajustin chen justin.chen columbia.edu columbia university usaadriana sejfia sejfia usc.edu university of southern california usa marcelo schmitt laser marcelo.laser gmail.com university of southern california usajie zhang jie.zhang ucl.ac.uk university college london ukfederica sarro f.sarro ucl.ac.uk university college london uk mark harman mark.harman ucl.ac.uk university college london uknenad medvidovic neno usc.edu university of southern california usa abstract ui testing is tedious and time consuming due to the manual effort required.
recent research has explored opportunities for reusing existing ui tests from an app to automatically generate new tests for other apps.
however the evaluation of such techniques currently remains manual unscalable and unreproducible which can waste effort and impede progress in this emerging area.
we introduce fruiter a framework that automatically evaluates ui test reuse in a reproducible way.
we apply fruiter to existing test reuse techniques on a uniform benchmark we established resulting in test reuse cases from apps.
we report several key findings aimed at improving ui test reuse that are missed by existing work.
ccs concepts software and its engineering keywords software testing test reuse mobile application open science acm reference format yixue zhao justin chen adriana sejfia marcelo schmitt laser jie zhang federica sarro mark harman and nenad medvidovic.
.
fruiter a framework for evaluating ui test reuse.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
.
introduction writing ui tests is tedious and time consuming increasingly driving the focus toward automated ui testing .
however permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
work tends to target tests that yield high code coverage rather than usage based tests that explore an app s functionality e.g.
sign in purchase search etc.
developers heavily rely on usagebased tests but currently have to write them manually .
to reduce the manual effort of writing usage based tests recent research has explored reusing existing tests in a source app to generate new tests automatically for a target app .
the guiding insight is that different apps expose common functionalities via semantically similar gui elements.
this suggests that it is possible to reuse existing ui tests across apps in effect generating the tests automatically by mapping similar gui elements.
four recent techniques have targeted usage based test reuse across android apps .1while these techniques have shown promise we have identified five important limitations that hinder their comparability reproducibility and reusability.
in turn this can lead to duplication and wasted effort in this emerging area.
1the metrics applied to date evaluate whether gui events from a source app are correctly transferred to a target app but do not consider whether the transferred tests are actually useful .
it is possible that events are transferred correctly but the generated test is wrong .
this can be e.g.
because a generated test is missing events and thus not executable.
moreover the metrics used in existing work are not standardized even when evaluating same aspects of different techniques making it difficult to compare the techniques.
2each existing technique s evaluation process requires significant manual effort every transferred event in each test must be inspected to determine whether the transfer is performed correctly.
this imposes a practical limit on the number of tests that can be evaluated.
for instance the authors of atm had to restrict their comparison with gtm to a randomly selected of the possible source target app combinations due to the task s scale.
3there are no standardized guidelines for conducting the manual inspections making the evaluation results biased and hard to reproduce.
for instance atm s authors acknowledge the possibility of mistakes in the manual process .
such mistakes are currently hard to locate verify or eliminate by other researchers.
1rau el al.
recently proposed a test reuse technique for web applications .
in this paper we focus on android apps due to the availability of a larger number of existing techniques to evaluate although in principle our work is not limited to android.arxiv .03427v4 nov 2020esec fse november virtual event usa y. zhao j. chen a. sejfia m. laser j. zhang f. sarro m. harman and n. medvidovic 4existing techniques are designed as one off solutions and evaluated as a whole .
this makes it difficult to isolate and compare their relevant components.
for instance gtm atm and craftdroid all contain functionality to compute a similarity score between two gui elements but it is unclear which of those specific components performs the best against the same baseline.
this would impede subsequent research that could potentially benefit from identifying underlying components that should be reused and or improved.
5existing techniques make different assumptions that hinder their comparison .
for instance gtm and atm require access to apps code and cannot be directly compared with techniques evaluated on close sourced apps.
to address limitations as well as limitation in part we have developed fruiter aframework for evaluating ui te streuse.
fruiter consists of three key elements a set of new evaluation metricsthat consolidate the metrics used by existing techniques and expand them to measure important aspects that are currently missed two baseline ui test reuse techniques that establish the lower and upper bounds for the evaluation metrics and an automated workflow that modularizes ui test reuse functionality and significantly reduces the manual effort.
with fruiter one can automatically evaluate test reuse techniques on apps tests of interest against the same baseline thus opening the possibility of large scale studies.
to fully address limitation as well as limitation we have extracted the core components from existing techniques and established a benchmark for evaluating and comparing them.
our benchmark currently contains subject apps with test cases involving gui events.
this benchmark is used by fruiter to evaluate side by side the extracted components and the two baseline components we developed yielding test reuse instances.
the results obtained by fruiter revealed several important findings.
for example we have been able to pinpoint specific tradeoffs between ml based e.g.
appflow and similarity based e.g.
atm techniques.
we have also identified scenarios that may seem counter intuitive such as the fact that manually writing tests requires less effort than attempting automated transfer in certain cases.
finally performing evaluations on a much larger data corpus allowed us to refute some conclusions reached in prior work.
this paper makes the following contributions.
1we develop fruiter to automatically evaluate ui test reuse with an expanded set of metrics as compared to existing work and two baseline techniques that help to provide the lower and upper bounds of ui test reuse in a given scenario.
2we identify and extract the core components from existing test reuse techniques enabling their fair comparison.
3we establish a reusable benchmark with standardized ground truths that facilitates the reproducibility of ui test reuse techniques evaluation and comparison.
4we use fruiter to conduct a side by side evaluation of the state of the art test reuse techniques uncovering several needed improvements in this area.
5we make fruiter s implementation and all data artifacts publicly available directly fostering future research.
figure sign in tests for wish a1 and etsy b1 b3 .
section introduces a representative example terminology and related work.
section describes fruiter s requirements and section its design followed by fruiter s instantiation in section .
section discusses our key findings.
section concludes the paper.
background and related work in this section we introduce a motivating example and relevant terminology followed by an overview of the strategies pursued by existing work and how they have been evaluated to date.
.
motivating example and terminology figure shows the screenshots of the sign in process of two popular shopping apps wish left and etsy right .
each screen is labeled with an identifier e.g.
a1is the first screen of wish.
in each screen there may be one or more actionable gui elements with which end users can interact based on the associated actions.
for instance the sign in button in screen a1 a1 is associated with a click action.
actionable elements and their associated actions embody gui events defined below .
by contrast the label sign in that is circled in screen a1is anon actionable gui element.
as an illustration assume that wish s sign in test exists and our goal is to automatically transfer it to etsy.
the relevant actionable gui elements in this sign in example are labeled and will be used to describe the following key terms used throughout the paper.
gui event or event in short is a triple comprising an actionable gui element an associated action and an optional input value e.g.
user input for a text box .
we reuse this definition from existing work .
for simplicity we use the label of a gui element e.g.
a1 to refer to the gui event triple.
canonical event is an abstracted event that captures a category of commonly occurring events.
an example canonical event may beappsignin and it would correspond to the a1 andb3 from figure as well as similar events from other apps such as log in .
usage based test exercises a given functionality in an app such as sign in .
a usage based test2consists of a sequence of gui events.
for instance figure highlights the sign in test in wish left as the event sequence a1 a1 a1 .
2if not mentioned otherwise test or test case refers to usage based test in this paper.fruiter a framework for evaluating ui test reuse esec fse november virtual event usa source app is the app with known tests that can be transferred to other apps with similar usage.
for instance wish is a source app with a sign in test that can potentially be transferred to other apps with sign in functionality.
target app is the app to which one aims to transfer existing tests.
a target app can reuse the tests from multiple source apps at the same time it can serve as a source app to other target apps if it contains known tests.
both source apps and target apps are used extensively in existing work .
source test is an existing test for a given source app that should be transferred to a target app to generate a transferred test.ground truth test is an existing test for a target app that is used to evaluate whether the transferred test is correct.
i.e.
whether the two tests match .
source event transferred event andground truth event refer to the gui events that belong to the source test transferred test and ground truth test respectively.
ancillary event is a special type of transferred event that is not mapped from a source event but is added in order to reach certain states in the target app.
for example b1 andb2 from figure may need to be added as ancillary events in order to reach etsy s sign in screen b3 such events do not exist in the source test.
null event is an event that should have been mapped from a source event but was not identified as such by a given test reuse technique.
thus the null event does not exist in the transferred test but it has a corresponding source event from which it maps.
this could be because of a test reuse technique s inaccuracy or the difference in app behaviors.
an example of the latter would be the inability to map etsy s events b1 andb2 to wish in figure .
.
strategies explored to date four recent techniques have targeted ui test reuse in android.
the shared core concern of these techniques is to correctly map the gui events from a source app to a target app.
in the example from figure the source test sign in in wish comprises the event sequence a1 a1 a1 .
by mapping gui events in this test from wish to etsy as a1 b3 a1 b3 a1 b3 a new sign in test for the target app etsy is generated as b3 b3 b3 .
existing techniques can be classified into two main categories based on how they map gui events appflow is ml based while craftdroid gtm and atm are similaritybased.
we have abstracted the two categories and their workflows by studying the similarities and differences of existing techniques.
ml based techniques learn a classifier from a training dataset of different apps gui events based on certain features such as text element sizes and image recognition results of graphical icons.
the classifier is used to recognize app specific gui events and map them tocanonical gui events used in a test library so that app specific tests can be generated by reusing the tests defined in the test library.
similarity based techniques define their own algorithms to compute a similarity score between pairs of gui events in a source app and a target app based on the information extracted from the two apps such as text and element attributes.
the similarity score is used to determine whether there is a match between each gui event in the source app and the target app based on a customizable similarity threshold .
for example a1 in wish left from figure is likely to have a higher similarity score with b3 than with other gui events in etsy right .
in that case a1 in wish will be mapped tob3 in etsy.
another important component in similarity basedtechniques is the exploring strategy which determines the order of computing the similarity score between the gui events in the source and target apps.
the target app s events that are explored earlier usually have a higher chance of being mapped.
.
existing evaluation metrics to evaluate their test reuse strategies existing techniques have focused on the accuracy of the gui event mapping .
this section overviews the metrics they applied which guided us in defining the expanded set of fruiter s metrics see section .
.
.
note that the detailed definitions of existing metrics were not provided in the publications we had to separately contact the authors of each technique to obtain the details introduced below.
appflow is an ml based technique that maps app specific events to canonical events using a classifier as discussed earlier.
appflow s classifier is evaluated with the standard accuracy metric indicating the percentage of the correctly classified gui events among all the gui events being classified.
correctly classified gui events include two cases the app specific events that are mapped to the correct canonical events true positive and the app specific events that are not mapped to any canonical events and such canonical events do not exist true negative .
craftdroid is a similarity based technique.
after the transfer of events from a source app to events in a target app craftdroid s authors manually identify three cases true positive tp occurs when the transferred event is the same as the one obtained during a manual transfer false positive fp occurs when the transferred event is different and false negative fn occurs when craftdroid fails to find a matching event while the manual transfer succeeds.
precision andrecall are then calculated based on the three cases.
it is important to note that craftdroid s fp includes both the incorrectly transferred events and the newly added ancillary events if any which is different from the fp case defined in other techniques.
we further illustrate this in section .
.
.
atm and gtm are also similarity based techniques and atm is an enhancement of gtm by the same authors.
similar to craftdroid the authors manually inspect the transferred results and identify four cases correctly matched means the source event is mapped to the correct event in the target app tp incorrectly matched means the source event is mapped to the wrong event in the target app fp unmatched !exist means the source event is not mapped to any events and no such events exist in the target app tn unmatched exist means the source event is not mapped to any events although the matching event exists in the target app fn .
atm and gtm do not calculate the precision or recall but present the raw percentages of each of the four cases.
3fruiter s principal requirements this section elaborates on the key limitations of current test reuse techniques and their evaluation processes.
these limitations serve as the foundation of five requirements we focused on in fruiter s design section and instantiation section .
prior to developing fruiter we investigated the existing techniques and their evaluations in depth.
beyond consulting the available publications we also studied the techniques implementations and produced artifacts and engagedesec fse november virtual event usa y. zhao j. chen a. sejfia m. laser j. zhang f. sarro m. harman and n. medvidovic their authors in at times extensive discussions to obtain missing details and resolve ambiguities.
in the end we identified five limitations that are likely to hinder future advances in this emerging area.
we base fruiter s principal requirements on these limitations.
req metrics used by fruiter to evaluate test reuse techniques shall be standardized and reflect practical utility.
existing techniques are evaluated with different and differently applied metrics recall section .
which harms their side by side comparison.
more importantly all techniques to date have focused on whether gui events from a source app are correctly transferred to a target app without considering whether the transferred tests are actually meaningful and applicable in the context of the target app.
it is thus possible that all gui events are mapped correctly but the transferred test cannot be applied e.g.
due to missing ancillary events recall section .
.
none of the existing techniques are able to identify such scenarios fruiter must be able to do so.
req fruiter s workflow shall reduce the required manual effort and thus scale to larger numbers of apps and tests than possible with current test reuse techniques.
existing techniques evaluation processes require significant manual effort to inspect every transferred event in each test.
for example atm was evaluated on app categories where each category in turn consisted of apps.
on average each app had tests to be transferred and each test had events.
within each app category atm transferred the tests of each app to the remaining apps resulting in source target app pairs in total.
for each app pair atm s authors had to manually inspect an average of transferred events tests events i.e.
events in total.
this is why they were forced to restrict their comparison with gtm to a randomly selected half of possible source target app pairs.
fruiter must address this shortcoming by providing a more scalable evaluation workflow that requires markedly less manual effort.
req evaluation results produced by fruiter shall be reproducible.
as discussed in section .
the current techniques evaluation results depend on identifying the case to which each transferred event belongs e.g.
correctly matched false positive etc.
.
such ground truth mappings are determined manually.
however there are no standard guidelines for conducting inspections making the results potentially biased and unreproducible.
in figure s example it is debatable whether a1 b3 is correct because a1 only takes the user s email while b3 takes both the email and username.
atm s authors also acknowledge the possibility of mistakes in the manual process .
more importantly any such mistakes are hard to locate or verify by other researchers since the results of manual inspection and the ground truth mappings on which they are based are recorded in ad hoc ways.
thus to facilitate future research in this area the evaluation results produced by fruiter must be reproducible with a ground truth representation that can be independently verified reused and modified.
req test reuse capabilities incorporated and evaluated byfruiter shall be modularized.
despite providing similar functionality existing test reuse techniques are designed as one off solutions and evaluated as a whole.
this makes it difficult to reuse or compare their relevant components.
in turn it invites duplication of effort and introduces the risk of missed opportunities for advances by other researchers and even by the techniques own developers.
to address this problem fruiter must modularize each test reusetable fidelity metrics as used in appflow craftdroid atm gtm and fruiter .
true pos.
tp false pos.
fp true neg.
tn false neg.
fn accuracy precision recall appflow anon anon anon anon accuracy dnc dnc craftdroid tp fp1 none fn none precision recall atm gtmcorrectly matchedincorrectly matchedunmatched !exist unmatched exist dnc dnc dnc fruiter correct incorrect nonexist missed accuracy precision recall artifact it evaluates allow its independent re use and associate the obtained evaluation results with the appropriate artifacts.
req benchmarks provided and applied by fruiter shall be reusable.
existing test reuse techniques have been evaluated using different benchmark apps and tests additionally hampering their comparison.
in fact only three subject apps were shared by two appflow and craftdroid out of the four existing techniques in their evaluations.
the underlying reason is the different assumptions made by the techniques.
for instance gtm and atm rely on the espresso testing framework that requires the apps source code.
as another example appflow s tests are written in a special purpose language based on gherkin and cannot be reused by techniques that capture tests in other languages e.g.
java used by atm and gtm .
thus fruiter must establish a set of uniform benchmarks with reusable apps and tests that can serve as the foundation for evaluating and comparing solutions in this area.
4fruiter s design this section presents fruiter s design with a focus on two features that address requirements req req req and partially req new evaluation metrics and an automated modular workflow.
we also introduce two novel test reuse techniques to serve as baselines for bounding the existing techniques evaluation results.
.
fruiter s metrics to address req fruiter incorporates a pair of evaluation metrics fidelity focuses on how correctly the gui events are mapped from a source app to a target app utility measures how useful the transferred tests are in practice.
.
.
fidelity metrics.
as explained in section .
fidelity of the mapping has been the main focus of existing techniques but the previous metrics have been used inconsistently.3to form a fair playground for comparing test reuse techniques we studied existing metrics by consulting available documentation and discussing with their authors.
we standardized this information into a comprehensive set of fidelity metrics in fruiter as shown in table .
table presents the fidelity metrics used across the different test reuse techniques and their relationship to the standard metrics as defined in literature .
each row shows a mapping from the names for the metrics used by each technique to the typical fidelity metrics names indicated in the header.
anon cells represent metrics that are not reported by a technique but are used internally to calculate other metrics that are reported.
dnc cells represent metrics that are not calculated by a given technique but can be 3existing publications in this area have referred to some of these as accuracy metrics.
we use fidelity to avoid confusion with a specific metric named accuracy defined previously in literature and used by one of the techniques we studied .fruiter a framework for evaluating ui test reuse esec fse november virtual event usa determined based on other metrics used.
finally none cells represent cases where a metric is not used by a technique and cannot be calculated from the available information.
fruiter covers all seven metrics changing several metrics names to better reflect their application to test reuse as will be further discussed below.
recall from section .
that craftdroid s fp category covers two cases fp1 corresponds to incorrectly matched events in atm gtm and incorrect in fruiter fp2 corresponds to the ancillary events that are not considered by other techniques.
fruiter also excludes the ancillary events from its incorrect category because they can be benign or even needed e.g.
b1 andb2 from figure and do not reflect the fidelity of the gui event mapping.
for instance if ancillary events were considered to be false positives a large number of them would result in a low precision for the gui event mapping.
however this would not be a meaningful measure since the ancillary events are not mapped from the source app.
such events are thus not relevant to the mapping s fidelity but should be considered by the utility metrics introduced next.
.
.
utility metrics.
fruiter introduces two utility metrics to indicate how useful a transferred test is.
this aspect is not considered in prior work but is needed because a high fidelity event mapping does not guarantee a successfully transferred test or vice versa.
for instance a target app s ground truth test may contain ancillary events not covered by source events making it impossible to generate a perfect test by event mapping alone.
on the flip side a low fidelity mapping may accidentally generate a perfect test.
thus it is important to measure the utility with respect to the ground truth test independently of event mapping s fidelity.
to this end we first define an effort metric to measure how close the transferred test is to the ground truth test by calculating the two tests levenshtein distance .
levenshtein distance is widely used in nlp to measure the steps needed to transform one string into another.
in our case each step is defined as the insertion deletion orsubstitution of an event in the transferred test.
secondly we define a reduction metric to assess the manual effort saved by the generation of the transferred test compared to writing the ground truth test from scratch reduction gtevents effort gtevents the value of reduction may be negative if transforming the transferred test into the corresponding ground truth test takes more steps than constructing such ground truth test from scratch.
note that each usage based test targets a scenario with a specific flow of interest multiple flows would result in multiple tests e.g.
sign in from homepage vs. from settings .
for each particular flow it is possible for the ground truth test to contain different acceptable ancillary events based on one s interest which would result in multiple acceptable ground truth tests.
in fruiter s current benchmark see section .
we manually constructed one ground truth test for each flow with the minimal amount of ancillary events to match prior work.
however fruiter s ground truth tests can be modified or extended to obtain their corresponding utility results.
for instance researchers can specify multiple acceptable ground truth tests for a given flow and measure the transferred test s utility with respect to each ground truth test.
we acknowledge that the utility aspect i.e.
how useful a transferred test is can be subjective depending on one s goal.
alternative figure overview of fruiter s automated workflow.
utility metrics e.g.
bug identification power executability code coverage can be added to fruiter s customizable workflow see section .
.
fruiter s current utility metrics specifically center around effort because they are applied to tests transferred by techniques whose end goal is to reduce the effort of writing tests manually.
refining utility s definition with extended metrics is worthy of further study but outside our scope.
our goal was to show that utility is important and measurable to motivate further exploration of such important aspect that has been missed by prior work.
.
fruiter s workflow to address req req and partially req 4from section we designed an automated evaluation workflow with customizable components shown in figure .
the goal of fruiter s workflow is to generate reproducible evaluation results for a test reuse technique s core functionality.
the workflow s automation is enabled by two key aspects the uniform representation of the inputs and artifacts needed in the evaluation process and a set of customizable components that output the evaluation results of interest automatically.
.
.
uniform representation of inputs.
as figure shows fruiter takes two types of input test input bottom left and mapping input top right .
the two are a combination of inputs taken and artifacts produced by existing test reuse techniques as well as three new inputs introduced in fruiter to automate the evaluation process ground truth tests gui maps and canonical maps.
test input contains source tests ground truth tests and transferred tests as defined in section .
.
the tests may be captured in various forms by a test reuse technique and cannot be analyzed in a standard way.
for instance all tests in atm and gtm are represented as espresso tests in java while craftdroid s source tests are written in python using appium and its transferred tests are represented in json .
in order to enable their automated evaluation the heterogeneous tests thus need to be standardized.
fruiter s event extractor converts the test input into a uniform representation of source events ground truth events and transferred events as detailed in section .
.
.
mapping input consists of the gui map and the canonical map for automatically evaluating a test reuse technique s fidelity .
the two maps are newly introduced by fruiter and captured using a standardized representation.
the gui map contains the gui event mapping from a source app to a target app generated by a given test reuse technique and is used to compute the fidelity metricsesec fse november virtual event usa y. zhao j. chen a. sejfia m. laser j. zhang f. sarro m. harman and n. medvidovic introduced in section .
.
.
prior work does not provide gui maps but only the final transferred tests.
the events in these tests cannot be used to calculate fidelity by comparing with source events directly because the transferred events may include ancillary and nullevents.
we further illustrate how we extract the gui maps from existing techniques and evaluate their fidelity automatically with fruiter in section .
on the other hand the canonical map contains the mapping from app specific events to canonical events.
this map is manually constructed and is used as the ground truth mapping forfruiter s fidelity evaluator component discussed below.
note that appflow can generate a canonical map automatically using ml techniques.
however appflow s certain mapping results can be wrong and thus cannot be used as the ground truth.
.
.
customizable components.
fruiter introduces three customizable components shown as shaded boxes in figure event extractor fidelity evaluator and utility evaluator.
event extractor leverages program analysis to extract the gui event sequence from the tests code.
the sequence is represented as each event s idorxpath depending on which of the two is used in the test.
idandxpath are widely used to locate specific gui elements in tests in various domains including android apps and web apps .
for simplicity we will use id to refer to either theidorxpath of a specific event in the rest of the paper.
to extract the event sequence event extractor analyzes the test input to locate the program point of each event based on its corresponding api e.g.
click orsendkeys for tests written with appium .
once it identifies the location event extractor determines the event s caller i.e.
the gui element where the event is triggered and performs a def use analysis to trace back the definition of the caller s id.
this definition is specified in a given api of the testing framework such as findelementbyid in appium .
in that case the def use analysis is used to pinpoint thefindelementbyid call that corresponds to the event s caller so thatid s value can be determined.
the input value associated with the event if any is determined by def use analysis in the same manner.
in the end the converted source events ground truth events and transferred events are represented in a uniform way with ids regardless of what testing framework is used.
note that if the test input is written in different languages or testing frameworks multiple event extractor instances need to be implemented.
however this is a one time effort and subsequent work can reuse existing event extractors when applied on the tests written in the same language and testing framework.
moreover the event extractor is easily customizable to process tests written with different frameworks by replacing the relevant apis signatures.
for instance when identifying an event caller s id the relevant api is findelementbyid if using appium to test mobile apps orfindelement if using selenium to test web apps.
by simply replacing the relevant api signature the event extractor will be able to process tests written in both appium and selenium.
fidelity evaluator takes the source events produced by event extractor and mapping input and automatically outputs the sets of correct incorrect missed and nonexist cases for calculating fruiter s seven fidelity metrics recall table .
algorithm describes fidelity evaluator in detail.
the algorithm iterates through each source event to determine to which of the fouralgorithm fidelity evaluator input eventlistsrcevents guimapguimap canonicalmap srccanmap tgtcanmap output setscorrect incorrect missed nonexist 1correct incorrect missed nonexist 2fori 1tosrcevents.size do 3src srcevents.
get i 4trans guimap.
getmapped src 5srccan srccanmap.
getcanonical src 6transcan tgtcanmap.
getcanonical trans iftrans !
null then iftranscan srccan then correct.
put src else incorrect.
put src else iftgtcanmap.
contains srccan then missed.
put src else nonexist.
put src 17returncorrect incorrect missed nonexist cases it should be assigned lines .
to do so it first gets the current source event src and the transferred event mapped from it trans based on the gui map lines .
it then converts the appspecific events srcandtrans into their corresponding canonical eventssrccan andtranscan using their respective canonical maps so that the events are comparable lines .
finally to determine which of the four cases srcfalls into the algorithm first checks whethertrans is anull event.
if not transcan will be compared againstsrccan to determine whether the transferred event refers to the same canonical event as the source event and srcwill be added to either the correct orincorrect set accordingly lines .
if trans isnull the source event has not been mapped to any events in the target app.
the algorithm then iterates through the canonical map of the target app tgtcanmap to determine whether the matching eventsrccan exists in the target app and srcwill be added to either themissed set or nonexist set accordingly lines .
utility evaluator automatically analyzes the ground truth events and transferred events produced by event extractor.
it uses this information to compute the two utility metrics effort and reduction based on their definitions described in section .
.
.
.
.
relationship to fruiter s principal requirements.
fruiter s workflow yields three key benefits that target req req and req .
first the only manual effort required by fruiter is to construct the canonical maps by relating app specific events to canonical events.
this is a one time effort per app and each event only needs to be labeled once regardless of how many times it appears in a test req .
by contrast in previous work each app specific event needs to be manually labeled every time it appears in a test possibly resulting in thousands of manual inspections.
second fruiter establishes ground truths with uniform representations canonical maps are the ground truth for assessing fidelity while ground truth events help to assess utility .
this renders the evaluation results yielded by fruiter reproducible req .
for instance any mistakes or subjective judgments made in the current techniques manual evaluation processes can be easily located by inspecting the canonical maps and independently reproduced.fruiter a framework for evaluating ui test reuse esec fse november virtual event usa further fruiter s canonical maps are reusable modifiable and extensible for subsequent studies helping to avoid duplicated work.
third fruiter s workflow consists of customizable modules that isolate the evaluation to a relevant component of a test reuse technique req .
for instance fidelity evaluator only assesses the performance of gui event mapping instead of evaluating a technique as a whole.
moreover both fidelity evaluator and utility evaluator can be customized reused or extended to automatically evaluate other metrics of interest based on the standardized inputs and artifacts that fruiter defines directly fostering future research.
.
fruiter s baseline techniques to better understand the performance of a test reuse technique we developed two baseline techniques na ve and perfect that establish the lower and upper bounds achievable by the fidelity and utility metrics in a given scenario.
.
.
na ve baseline.
the na ve baseline uses a random strategy to select the events in a target app to which each source event should be mapped.
this sets the practical lower bound of fidelity .
as algorithm shows na ve initially explores the target app from the main activity line .
for each source event it obtains all the events at the current activity events in a random order lines and then tries to find a match between the current source event src and each event in events lines .
when mapping srctoevent na ve first checks if the associated actions of the two events are the same and only computes the similarity score when they are.
the similarity score is computed by selecting a random value between and line which are the lower and upper bounds used in existing work.
if the similarity score of srcandevent is above a certain threshold event is added to the list maintained in transevents line .
at that point na ve continues to explore the target app from the activity reached by the transferred event line and marks the current source event srcas mapped line .
in the end if the source event is not mapped it will be marked as a null event and added totransevents line .
null events correspond to either the true negative or false negative categories in table .
algorithm na ve baseline techniqe input eventlistsrcevents appinfotgtappinfo output eventlisttransevents 1transevents 2currentact tgtappinfo.
getmainactivity 3foreachsrc srcevents do 4ismapped false 5events tgtappmap.
getallevents currentact 6events.
randomizeorder foreachevent events do ifevent.
action src.action then similarity getrandomsimilarity ifsimilarity threshold then transevents.
add event currentact event.
nextactivity ismapped true break if ismapped then 16transevents.
add null 17returntransevents4.
.
perfect baseline.
the perfect baseline transfers the source events based on the ground truth mapping recall section .
assuming all source events are correctly mapped to the target app.
perfect baseline thus represents a perfect gui event mapping and achieves fidelity by definition.
specifically we are interested in the utility achieved by the perfect baseline since it represents the upper bound of the transferred tests practical usefulness which is not considered previously.
this can help us identify the room for improvement and guide future research in test reuse techniques.
fruiter s instantiation this section describes how we instantiate fruiter to automatically evaluate the relevant modules of existing techniques alongside fruiter s baseline techniques in partial satisfaction of req .
the evaluation is based on fruiter s reusable benchmark that addresses req .
to this end we needed to provide information to enable fruiter s workflow recall figure source tests that are supplied as inputs to a given test reuse technique transferred tests and gui maps which are produced as outputs of a given test reuse technique and the manually constructed ground truths namely canonical maps and ground truth tests.
however existing test reuse techniques were not designed with fruiter s modular workflow in mind and thus do not provide such information directly.
section .
explains how we mitigated the above challenge in order to extract the relevant components from existing techniques and generate the information needed by fruiter .
note that this step will not be necessary for future techniques if they follow fruiter s modularized design.
section .
presents fruiter s reusable benchmark for the uniform evaluation of test reuse techniques which contains the source tests ground truth tests and canonical maps used in fruiter s automated workflow.
finally section .
provides the details of fruiter s implementation and generated datasets.
.
modularizing existing techniques to lay the foundation for addressing req we modularized fruiter s design.
in turn this isolated the evaluation of gui event mapping s fidelity and the transferred tests utility as discussed in section .
.
however the existing techniques are implemented and evaluated as fully integrated one off solutions that do not provide the artifacts needed by fruiter to generate the modularized evaluation results.
because of this we had to extract the specific functionality from existing techniques implementations that performs the gui event mapping recall section .
.
once the gui maps are available we can generate the transferred tests used in fruiter s utility evaluator.
note that the step of extracting gui mapper components is not needed for future test reuse techniques if they follow fruiter s modularized design.
for example we directly applied fruiter on the two baseline techniques we developed with no extra effort.
extracting the gui mapper components from the existing techniques was challenging since we had to understand each technique s design and implementation in detail and to modify its source code.
to this end in addition to the available publications we studied in depth the existing approaches implementations and communicated with their authors extensively.
we describe the challenges we faced during this process and the specific componentextraction strategies we applied to each existing solution.esec fse november virtual event usa y. zhao j. chen a. sejfia m. laser j. zhang f. sarro m. harman and n. medvidovic .
.
extracting appflow s gui mapper.
appflow is an mlbased technique whose key component trains a classifier that maps app specific events to canonical events but does not map the events from a source app to a target app.
to compare appflow with similarity based techniques we leverage its canonical maps to transfer the source events to the target app by mapping each source event to the corresponding canonical event based on the source app s canonical map and mapping this canonical event back to the app specific event in the target app based on the target app s canonical map.
appflow s implementation does not output its canonical maps so we had to locate and modify the relevant component to do so.
moreover appflow does not store its trained classifier so we had to configure its ml model and re train it.
during this process we communicated with appflow s authors closely to understand its code to obtain proper configuration files and training data and to ensure the correctness of our re implementation.
.
.
extracting atm s and gtm s gui mappers.
as discussed earlier atm was developed as an enhancement to gtm and was shown to outperform it .
however the authors of these two techniques compared them only on half of the source target app pairs used in atm s publication due to the large manual effort required.
since fruiter largely automates the comparison process we decided to extract the gui mapper components from both techniques to enable their comparison at a large scale.
an obstacle we had to overcome was that atm and gtm both require the app s source code due to the use of the espresso .
thus they cannot be compared as is with techniques evaluated on closed sourced apps which would have limited our choice of benchmark apps.
we discussed this with atm s and gtm s authors and learned that the only step that requires source code for both techniques gui mappers is computing the textual similarity score of image gui elements e.g.
imagebutton .
in that case the text of the image s filename is retrieved from the app s code and analyzed to compute the similarity score.
however the main author confirmed that in her experience this feature is rarely needed in practice.
we thus decided to extract atm s and gtm s gui mapper components as stand alone java programs that do not require espresso omitting the filename retrieval feature.
we subsequently confirmed with the two techniques authors the correctness of our implementation.
.
.
extracting craftdroid s gui mapper.
craftdroid s implementation is only partially available.
its authors informed us that two of craftdroid s modules test augmentation and model extraction were not releasable when we requested them due to ongoing modifications.
the authors confirmed our observation that craftdroid s gui mapping functionality depends on the outputs of the two missing modules and advised us that the best strategy would be for us to reimplement them based on craftdroid s lone publication .
however the publication in question is missing implementation details that would introduce bias we would have no guarantee that the versions of the two components we produce are the same as those used in craftdroid.
instead we decided to rely on craftdroid s published transferred tests in our evaluation.
to obtain craftdroid s gui maps we inspected its published artifacts and found that only certain events in the transferred tests have associated similarity scores while other events are labeled as empty .
further investigation showed that each event inthe transferred tests belongs to one of three cases events with available similarity scores are successfully mapped from the source events empty events are mapped from the source events but no match is found by craftdroid i.e.
null events the remaining events are not mapped from the source events but are added by craftdroid i.e.
ancillary events .
we excluded the ancillary events so that the resulting transferred events have a to mapping from the source events giving us craftdroid s gui maps.
.
fruiter s benchmark as discussed above in the motivation for req existing test reuse techniques are evaluated on different apps and tests which hinders their comparability.
to address this we established a reusable benchmark with the same apps and tests to serve as a shared measuring stick in this emerging domain.
this section discusses our strategy for including existing apps and tests in the benchmark and for generating the required ground truth.
.
.
benchmark apps and tests.
to maximize the results from existing work that we can attempt to reproduce we first included the intersection of the subject apps used by existing work.
this yielded shopping apps geek wish and etsy.
we further randomly selected additional shopping apps and news apps used by appflow .
this gave us benchmark apps in total as described in table .
our rationale behind this choice of apps was two fold appflow s authors manually inspected all app categories on google play and identified shopping and news as categories with common functionalities suitable for test reuse appflow was evaluated on the largest number of subject apps among the existing techniques.
by comparison atm used open source apps that are not as popular as those used in appflow.
to construct the benchmark tests we further followed the test cases defined in appflow with a similar rationale appflow s authors conducted an extensive study to manually identify tests that are shared in shopping and news apps appflow defines a larger number of tests compared to other work.
for example craftdroid only has tests defined in each app category.
we excluded those tests that require mocking external dependencies e.g.
a payment service .
this resulted in tests in the shopping table summary information of benchmark apps.
shoppingapp id app name downloads tests events s1 aliexpress 100m s2 ebay 100m s3 etsy 10m s4 5miles 5m s5 geek 10m s6 google shopping 1m s7 groupon 50m s8 home 10m s9 6pm 500k s10 wish 100m newsn1 the guardian 5m n2 abc news 5m n3 usa today 5m n4 news republic 50m n5 buzzfeed 5m n6 fox news 10m n7 smartnews 10m n8 bbc news 10m n9 reuters 1m n10 cnn 10m 24fruiter a framework for evaluating ui test reuse esec fse november virtual event usa table benchmark test cases in shopping ts and news tn categories.
test id test case name tested functionalities ts1 tn1 sign in provide username and password to sign in ts2 tn2 sign up provide required information to sign up ts3 tn3 search use search bar to search a product news ts4 tn4 detail find and open details of the first search result item ts5 tn5 category find first category and open browsing page for it ts6 tn6 about find and open about information of the app ts7 tn7 account find and open account management page ts8 tn8 help find and open help page of the app ts9 tn9 menu find and open primary app menu ts10 tn10 contact find and open contact page of the app ts11 tn11 terms find and open legal information of the app ts12 add cart add the first search result item to cart ts13 remove cart open cart and remove the first item from cart ts14 address add a new address to the account ts15 filter filter sort search results tn12 add bookmark add first search result item to the bookmark tn13 remove bookmark open the bookmark and remove first item from it tn14 textsize change text size category and tests in the news category shown in table .
note that we cannot reuse appflow s tests directly because they are written in a special purpose language defined by appflow for an entire app category rather than a specific app.
instead we relied on multiple undergraduate and graduate students with android experience to write the applicable tests for each of the subject apps using appium .
some benchmark apps did not have each functionality described in table ultimately resulting in a total of tests involving events across the apps the two right most columns of table requiring sloc of java code.
these benchmark tests currently do not contain oracle events because only atm and craftdroid can transfer oracles in principle.
however due to the limited availability of craftdroid s source code as mentioned earlier we would not be able to obtain craftdroid s results using our benchmark making a comparison across different techniques impossible.
as additional test reuse techniques are developed with the ability to transfer oracles fruiter s benchmark tests can be extended to include oracle events to obtain their results as well.
note that as long as future techniques follow fruiter s modularized design to provide the needed input e.g.
gui maps of the oracle event mapping fruiter will be able to automatically generate the results of oracle events.
a detailed tutorial is provided on fruiter s website .
.
.
benchmark ground truth.
as described in section .
we define canonical maps to represent the ground truth for the fidelity of the gui event mapping and ground truth events to represent the ground truth for the utility of the transferred tests.
in our benchmark we define canonical events for the shopping apps and for the news apps.
our canonical events are extended from appflow aiming to reflect a finer grained classification of gui events.
for instance event password in the sign in test ts1 tn1 in table and events password and confirm password in the sign up test ts2 tn2 in table are all represented as the same canonical event password in appflow.
however it is debatable whether that is appropriate.
for example mapping password in sign up to password insign in may lead to non executable tests.
to remove ambiguity we capture such events separately.based on the canonical events we construct canonical maps one per subject app.
we do so by manually relating to the canonical events a total of subject apps gui events that appear in one or more of the tests.
as discussed in section .
this is the only manual step required by fruiter and is a one time effort the canonical maps can be reused when relying on the same subject apps.
as a point of comparison recall from section that evaluating app pairs in atm required manually inspecting events.
by contrast our one time inspection of the events enabled the use of app pairs categories apps i.e.
including an app s test transfer to itself by every technique fruiter evaluated.
the ground truth events in our benchmark are extracted from the tests by fruiter s event extractor recall figure .
.
fruiter s implementation artifacts fruiter s artifacts are publicly available its source code final datasets gui mappers extracted from existing work implementations of baseline techniques their gui maps and transferred tests benchmark apps and tests and manually constructed benchmark ground truths.
we highlight the key details of these artifacts below.
.
.
source code.
fruiter s event extractor recall figure is implemented in java using soot sloc .
fruiter s fidelity evaluator and utility evaluator are implemented in python sloc .
fruiter s baseline techniques na ve and perfect recall section .
are likewise implemented in python sloc .
the gui mapper components extracted from existing techniques recall section .
are implemented in their original programing languages appflow in python sloc gtm in java sloc and atm in java sloc .
the functionality that processes their outputs and generates the uniform representation of gui maps and transferred tests is implemented in python sloc .
as discussed earlier due to craftdroid s unavailable source code we can only interpret its published artifacts that functionality is implemented in python sloc .
the data analyses that interpret our final datasets are written in r sloc .
.
.
final datasets.
our final datasets contain the results of test transfer cases generated by the gui mappers from the existing techniques and our two baselines when applied on fruiter s benchmark.
we apply techniques appflow atm gtm na ve perfect to transfer tests across shopping and news apps involving source target app pairs app categories app pairs in each category techniques .
this yielded result entries per technique.
as discussed earlier we have to rely on craftdroid s final results and can thus only compare craftdroid to the other techniques on the shopping apps geek wish etsy used both in our benchmark and in craftdroid s evaluation.
this gave us result entries for craftdroid since only tests are transferred by craftdroid in each app.
each of the total result entries contains the following information the source and target apps the source transferred and ground truth tests the technique used to transfer the test the correct incorrect missed nonexist sets of gui events output by fruiter s fidelity evaluator as described in algorithm and the seven corresponding fidelity metrics defined in section .
.
and values of the two utility metrics effort andreduction defined in section .
.
.
note that obtainingesec fse november virtual event usa y. zhao j. chen a. sejfia m. laser j. zhang f. sarro m. harman and n. medvidovic figure comparison of average precision andrecall .
these result entries following prior work s evaluation processes would have required manual inspection of events that appear across all of the source tests which is infeasible in practice.
findings the datasets produced by fruiter include the results obtained by evaluating side by side the extracted gui mapper components from the four existing test reuse techniques and the two baseline techniques we developed.
in turn this data enables further in depth studies of a range of research questions in this emerging domain.
as an illustration this section highlights several findings uncovered by fruiter s datasets that are missed by prior work.
.
gui mapper comparison as discussed earlier existing techniques are evaluated in their entirety on different benchmark apps and tests and using different evaluation metrics all of which makes their results hard to compare.
by contrast fruiter was able to evaluate their extracted gui mappers side by side with our two techniques na ve and perfect serving as baselines.
we note that it is possible for a given test reuse technique to produce results as a whole that may be different from those produced only by its extracted gui mapper.
one reason may be that there is additional relevant functionality that is scattered across the technique s implementation.
however any such functionality can be added to the existing gui mappers or introduced in additional fruiter components.
.
.
fidelity comparison.
fruiter s website contains the results of all seven fidelity metrics from section .
.
obtained using our benchmark.
due to space limitations we show the results of three metrics precision recall accuracy and restrict our discussion to precision and recall since accuracy follows a similar trend as precision the results of the four remaining fidelity metrics correct incorrect missed nonexist can provide an in depth understanding on each of the four specific cases and can be found on fruiter s website .
figure shows the average precision and recall achieved by the four existing techniques as well as na ve we omit perfect since its values are always by definition.
for each technique except craftdroid the top blue bar shows the average calculated based on cases transferred among both shopping and news apps.
to meaningfully compare craftdroid with other techniques even if only partially we show the averages calculated based on the cases for which we have craftdroid s data in the bottom orange bars.
craftdroid only transferred sign in figure comparison of average accuracy .
and sign up tests in the shopping apps geek wish and etsy leading to the cases source target app pairs tests .
we highlight three observations based on the results from figure .
first every existing technique yields lower recall than precision on the larger blue data set meaning that it suffers from more missed i.e.
false negative than incorrect i.e.
false positive cases.
although appflow s recall is highest among the existing techniques it exhibits the largest drop off between its precision and recall values.
a plausible explanation is that as an ml based technique appflow will likely fail to recognize relevant gui events if no similar events exist in its training data.
this was somewhat unexpected however given that appflow s authors carefully crafted its ml model to the app categories we also used in fruiter and suggests that additional research is needed in selecting and training effective ml models for ui test reuse.
by comparison similaritybased techniques such as atm will miss fewer gui events in principle they can always compute a similarity score between two events and return the mapped events whose scores are above a given threshold.
however if the similarity threshold is set too low it will result in more incorrect cases leading to low precision.
a related observation is that appflow s precision outperforms the other techniques across the board for both the larger blue and smaller orange datasets.
this is because appflow has the advantage of more information obtained from a large corpus of apps in its training dataset than the similarity based techniques which compute the similarity scores based only on the information extracted from the source and target apps under analysis.
however appflow s recall is lower than both atm and craftdroid on the orange cases from geek wish and etsy.
this reinforces the above observation that an ml based technique will fail to recognize gui events if no similar events exist in its training data.
finally our data confirms that atm indeed improves upon gtm as indicated in their pairwise comparisons across both precision and recall on both large and small datasets.
in fact gtm exhibits the lowest fidelity of all existing techniques and its recall across the blue cases is actually lower than that achieved by the na ve strategy.
we note that gtm s design is geared to transferring tests in programming assignments that share identical requirements and is clearly not suited to heterogenous real world apps.
.
.
utility comparison.
figure shows the two utility metrics yielded by each of the four existing and two baselines.
recall from section .
.
that utility measures how useful the transferred tests are in practice compared to the ground truth tests.
the objective of utility is to minimize the effort while maximizing the reduction .fruiter a framework for evaluating ui test reuse esec fse november virtual event usa figure comparison of average effort andreduction .
the utility of existing techniques shows similar trends to those observed in the case of fidelity.
for example appflow outperforms other techniques while gtm exhibits similar performance to that of na ve.
this indicates a possible correlation between the fidelity of the gui event mapping and the utility of the transferred tests.
at the same time we observe that while our perfect gui mapper achieves higher utility than the remaining techniques that utility is not optimal.
in fact perfect s average reduction is under across the cases in the larger dataset top blue bar .
in other words even with the best possible mapping strategy we save less than half of the effort required to complete the task manually.
the previously published techniques perform much worse than this appflow saves under atm under and gtm under of the required manual effort while the reduction yielded by craftdroid on the smaller orange dataset is lower than perfect s on either of the two datasets.
this indicates that fidelity is clearly not the only factor to consider in order to achieve desired utility and that there is large room for improvement in future test reuse techniques.
to verify the above insights we conducted pairwise correlation tests between the seven fidelity and two utility metrics.
overall the results further discussed below and provided in their entirety in fruiter s online repository show a weak correlation between fidelity and utility.
this reinforces our observation that accurate gui mappings can yield useful transferred tests but are not the only relevant factor.
in turn this finding calls for exploration of other components in test reuse techniques since the focus on gui event mapping alone can hit a ceiling as shown by the perfect baseline.
we discuss such possible directions next.
.
insights and future directions guided by the above observations we explore potential strategies for improving ui test reuse with various statistical tests and manual inspections on fruiter s datasets.
due to space limitations we highlight four findings that were not reported by previous work.
source app selection matters for a given target app.
figures and all show consistent improvement across the techniques in the smaller datasets cases transferred among apps compared to the larger ones cases transferred among apps .
this suggests that certain source target app pairs achieve better results than others.
for example we found that app pairs involving wish geek and a benchmark app called home all of which are developed by the same company wish inc. achieve high fidelity and utility regardless of the technique used.
another such compatible app pair is abc news and reuters.
performing a large scaleevaluation enabled by fruiter will help spot pairings like this and give researchers a starting point to explore the characteristics that can lead to better transfer results.
automated transfer is not suitable for all tests.
our utility metrics revealed large effort and negative reduction in some cases meaning that correcting a transferred test required more work than writing it from scratch.
further inspection revealed that this is primarily due to a test s length rather than a technique s accuracy.
for instance perfect showed no benefit reduction of the time and the average number of source events in those cases is only .
this suggests that for simple tests manual construction may be preferable.
future research should consider the criteria for suitable tests to transfer instead of transferring all source tests.
there is a trade off between ml and similarity based techniques.
as discussed above an insufficient training set in an mlbased technique may yield low recall while a low similarity thresholdin a similarity based technique can address this but may yield low precision.
this suggests two future research directions.
first selecting training sets and similarity thresholds is important but existing techniques did not justify their choices .
there is clearly a need for further study of novel strategies such as incorporating dynamic selection criteria based on target app characteristics.
second future research should consider the tradeoffs across different test reuse techniques and provide guidance on selecting the most suitable techniques for a given scenario.
test length is not a key factor influencing fidelity.
craftdroid and gtm studied the relationship between the test length and their transferred results.
for instance craftdroid showed a strong negative correlation between test length and its two fidelity metrics coefficient .5in both cases .
to verify these findings we conducted correlation tests on fruiter s much larger datasets.
our results indicate a negative but very weak correlation between test length and fruiter s fidelity metrics .
coefficient across all seven cases .
this shows that test length is not the key factor that impacts fidelity arguing that future research targeting reuse of complex tests may be a fruitful direction.
conclusion this paper has presented fruiter a customizable framework for automatically evaluating ui test reuse techniques.
fruiter has been instantiated and successfully demonstrated on the key functionality extracted from existing test reuse techniques that target android apps.
in the process we have been able to identify several avenues of future research that prior work has either missed or actually flagged as not viable.
we publicly release fruiter its accompanying artifacts and all of our evaluation data as a way of fostering future research in this area of growing interest and importance.