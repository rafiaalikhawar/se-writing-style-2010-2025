context aware in process crowdworker recommendation junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 1laboratory for internet software technologies 2state key laboratory of computer sciences institute of software chinese academy of sciences beijing china 3university of chinese academy of sciences beijing china corresponding author 4school of systems and enterprises stevens institute of technology usa 5lassonde school of engineering york university canada junjie wq iscas.ac.cn ye.yang stevens.edu wangsong eecs.yorku.ca abstract identifying and optimizing open participation is essential to the success of open software development.
existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to detect more bugs with fewer workers.
however these studies mainly focus on one time recommendations with respect to the initial context at the beginning of a new task.
this paper argues the need for in process crowdtesting worker recommendation.
we motivate this study through a pilot study revealing the prevalence of long sized non yielding windows i.e.
no new bugs are revealed in consecutive test reports during the process of a crowdtesting task.
this indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner so that the non yielding windows could be shortened.
to that end this paper proposes a context aware in process crowdworker recommendation approach irec to detect more bugs earlier and potentially shorten the non yielding windows.
it consists of three main components the modeling of dynamic testing context the learning based ranking component and the diversity based re ranking component.
the evaluation is conducted on crowdtesting tasks from one of the largest crowdtesting platforms and results show the potential of irec in improving the cost effectiveness of crowdtesting by saving the cost and shortening the testing process.
acm reference format junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 .
.
context aware in process crowdworker recommendation.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction abundant internet resources has driven software engineering activities to be more open than ever.
besides free successful open source software and cheap on demand web storage and computation facilities more and more companies are leveraging on crowdsourced permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
development to obtain solutions and achieve quality objectives faster cheaper .
as an example utest has more than software experts with diverse expertise spanning more than countries to validate various aspects of digital quality .
various methods and approaches have been proposed to support utilizing crowdtesting to substitute or aid in house testing for reducing cost improving quality and accelerating schedule .
one of the most essential functions is to identify appropriate workers for a particular testing task .
this is because the shared crowdworker resources while cheap are not free.
to help identify appropriate workers for crowdtesting tasks many different approaches have been proposed by modeling the workers testing environment experience capability or expertise with the task etc.
unfortunately these approaches have limited applicability for the highly dynamic and volatile crowdtesting processes.
they merely provide one time recommendation at the beginning of a new task without considering constantly changing context information of ongoing testing processes.
this study aims at filling in this gap and shedding light on the necessity and feasibility of dynamically in process worker recommendation.
from a pilot study conducted on real world crowdtesting data section .
this study first reveals the prevalence of long sized non yielding windows i.e.
consecutive testing reports containing no new bugs during crowdtesting process.
.
tasks have at least one sized non yielding window and an average of of spending is wasted on these non yielding windows.
this indicates the ineffectiveness of current crowdtesting practice because these non yielding windows would cause wasteful spending of task requesters potentially delay the progress of crowdtesting.
it also implies the potential opportunity for accelerating testing process by recommending appropriate crowdworkers in a dynamic manner so that the non yielding windows could be shortened.
this paper proposes a context aware in process crowdworker recommendation approach named irec to dynamically recommend a diverse set of capable crowdworkers based on various contextual information at a specific point of crowdtesting process aiming at shortening the non yielding window and improving bug detection efficiency.
irec consists of three main components testing context modeling learning based ranking and diversity based re ranking.
first the testing context model is constructed in two perspectives i.e.
process context and resource context to capture the in process progress oriented information and crowdworkers characteristics respectively.
second a total of features are defined and extracted from both process context and resource context based on these ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 features the learning based ranking component learns the probability of crowdworkers being able to detect bugs within specific context.
third the diversity based re ranking component adjusts the ranked list of recommended workers based on the dynamic diversity measurement to potentially reduce duplicate bugs.
irec is evaluated on crowdtesting tasks involving crowdworkers and reports from one of the largest crowdtesting platforms.
results show that irec could shorten the non yielding window by a median of in different application scenarios and consequently have potential of saving testing cost by a median of .
it significantly outperforms four commonly used and state of the art baseline approaches.
this paper makes the following contributions the formation of the in process crowdworker recommendation problem based on the empirical investigation on real world crowdtesting data.
this is the first study to explore the in process worker recommendation problem to the best of our knowledge.
the crowdtesting context model which consists of two perspectives i.e.
process context and resource context to facilitate in process crowdworker recommendation.
the development of the learning based ranking method to learn appropriate crowdworkers who can detect bugs in a dynamic manner.
the development of the diversity based re ranking method to adjust the ranked workers to reduce duplicate bugs.
the evaluation of the proposed irec on crowdtesting tasks involving crowdworkers and reports from one of the largest crowdsourced testing platforms with affirmative results1.
background and motivation .
background in practice a task requester prepares the task including the software under test and test requirements and distributes it online.
crowdworkers can freely sign in their interested tasks and submit testing reports in exchange of monetary prizes.
managers then inspect and verify each report to find the detected bugs.
there are different payout schema in crowdtesting e.g.
pay by report.
as discussed in previous work the cost of a task is positively correlated with the number of received reports.
the following lists important concepts with examples in table test task is the input to a crowdtesting platform provided by a task requester.
it contains a task id and a list of test requirements in natural language.
test report is the test record submitted by a crowdworker.
it contains a report id a worker id i.e.
who submit the report a task id i.e.
which task is conducted the description of how the test was performed and what happened during the test bug label duplicate label and submission time.
specifically bug label indicates whether the report contains a bug2 and duplicate label indicates with which the report is duplicate.
note that in the following paper we refer to bug report also short for bug as the report whose bug label is 2in our experimental platform a report corresponds to either or bug and there is no report containing more than bug.table important concepts and examples test task task id t000012 requirement browse the videos through list mode iqiyi rank the videos using different conditions check whether the rank is reasonable.
requirement cache the video check whether the caching list is right.
test report report id r1002948308 task id t000012 worker id w5124983210 description i list the videos according to the popularity.
it should be ranked according to the number of views.
however there were many confused rankings for example the video shibuya century legend with million views was ranked in front of the video i went to school with million views.
bug label bug duplicate label r1002948315 r1002948324 submission time jan .
crowdworker worker id w5124983210 device phone type samsung sn9009 operating system android .
.
rom type kot49h.n9009 network environment wifi historical reportsr1002948308 r1037948352 bug refer to test report also short for report as any submitted report and refer to unique bug as the report whose bug label is bugand duplicate label is null.
crowdworker is a registered worker in a crowdtesting platform and is denoted by worker id and his her device.
it is associated with the historical reports he she submitted.
note that in our experimental dataset which spans across six months we did not observe the crowdworkers device change thus this paper assumes each crowdworker corresponds to a stable device variable.
.
non yielding windows in crowdtesting processes most open call formats of crowdtesting frequently lead to ad hoc worker behaviors and ineffective outcomes.
in some cases workers may choose tasks they are not good at and end up with finding none bugs.
in other cases many workers with similar experience may submit duplicate bug reports and cause wasteful spending of the task requester.
more specifically an average of duplicate reports are observed in our dataset.
to better understand this issue we examine the bug arrival curve for historical tasks from real world crowdtesting projects details are in section .
.
we notice that there are frequently nonyielding windows i.e.
the flat segments of the increasing bug arrival curve.
such flat windows correspond to a collection of test reports failing to reveal new bugs i.e.
either no bugs or only duplicate bugs.
we refer to the length of a non yielding window as the number of consecutive test reports.
figure bug arrival curvefigure illustrates the bug arrival curve of an example task with highlighted non yielding windows length only for illustration purpose .
the non yielding windows can cause wasteful spending on these non yielding reports potentially delay the progress of crowdtesting.
1536context aware in process crowdworker recommendation icse may seoul republic of korea a non yielding windows b crowdworkers activeness c crowdworkers preference figure observations based on baidu dataset we further investigate this phenomenon and present a summarized view in figure 2a.
the x axis shows the length of the non yielding window while the y axis shows the relative position of the non yielding window expressed using the task s progress.
we can observe that the long sized non yielding window is quite common during crowdtesting process.
there are .
tasks with at least one sized non yielding window .
tasks with at least one sized window.
furthermore these longsized non yielding windows mainly take place in the second half of crowdtesting processes.
for example .
sized non yielding windows happened at the latter half of the process.
we further explore the cost waste of these non yielding windows.
specifically an average of cost3is wasted on these or longer sized non yielding windows of all experimental tasks and an average of cost is wasted on these or longer sized nonyielding windows.
in addition an average of hours4are spent on these or longer sized non yielding windows of all experimental tasks.
the prevalence of long sized non yielding windows indicates that current workers possibly have similar bug detection capability with previous workers on the same task.
in order to break the flatness we investigate the potential root causes and study if we can learn from the dynamic underlying contextual information in order to mitigate such situation.
this also suggests the unsuitability of existing one time worker recommendation approaches and indicates the need for in process crowdworker recommendation.
.
characterizing crowdworker s bug detection capability this subsection presents more explorations about the characteristics of crowdworkers which can influence their test participation and bug detection performance to motivate the modelings of testing context.
activeness .
figure 2b shows the distribution of crowdworkers activity intensity.
the x axis is the random selected crowdworkers among the top workers ranked by the number of submitted reports and the y axis is equal sized time interval which is obtained by dividing the whole time space.
we color code the blocks 3following previous work we treat the number of reports as the amount of cost.
4we measure the duration of each non yielding window using the time difference between the last and first report s submission time associated with that window.using a darker color to denote a worker submitting more reports during the specific time interval.
we can see that the crowdworkers activities are greatly diversified and not all crowdworkers are equally active in the crowdtesting platform at specific time.
intuitively the inactive crowdworkers would be less likely to conduct the task let alone detect bugs.
preference .
figure 2c shows the distribution of crowdworkers activity at a finer granularity.
the x axis is the same as figure 2b and the y axis is the random selected terms which capture the content under testing from the top most popular descriptive terms see section .
for details .
the block in the heat map demonstrates the number of reports which are submitted by the specific worker and contain the specific term.
we color code the blocks using a darker color to denote a worker submitting reports with corresponding terms more frequently i.e.
worker s preference in different aspects.
the differences across columns in the heat map further reveal the diversified preference across workers.
considering there are usually dozens of crowdtesting tasks open in the platform even if a crowdworker is active he she cannot take all tasks.
intuitively if a crowdworker has a preference on the specific aspects of a task he she would show greater willingness in taking the task and further detecting bugs.
expertise .
similarly we explore the heat map with the terms from the crowdworkers bug reports rather than reports we observe a similar trend.
due to space limit we leave the detailed figure in our website.
this indicates the crowdworkers diversified expertise over different crowdtesting tasks.
we also conduct correlation analysis between the number of bug reports i.e.
denoting expertise and number of reports i.e.
denoting preference for each pair of the crowdworkers on the top most popular terms the median coefficients is .
indicating these two types of characteristics are not tightly correlated with each other.
preference focuses more on whether a crowdworker would take a specific task and expertise focuses more on whether a crowdworker can detect bugs in the task.
to summarize the exploration results reveal that workers have greatly diversified activeness preferences and expertise which significantly affect their availability on the platform choices of tasks and quality of their submissions.
to guarantee the effectiveness of recommendation a worker is desirable to be active in the platform and equipped with satisfactory preference and expertise for the given tasks.
thus all these factors need to be precisely captured 1537icse may seoul republic of korea junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 figure overview of irec and jointly considered within the recommendation approach.
besides the approach should also consider the diversity among the recommended set of workers so as to reduce duplicates and further improve bug detection performance.
approach figure shows the overview of the proposed irec .
it can be automatically triggered when the size of non yielding window exceeding a certain threshold value i.e.
recthres is observed during crowdtesting process as introduced in section .
.
for brevity we use the term recpoint to denote the point of time under recommendation as illustrated at the top right corner of figure .
irec has three main components.
first it models the timesensitive testing contextual information in two perspectives i.e.
the process context and the resource context respectively with respect to the recpoint during the crowdtesting process.
the process context characterizes the process oriented information related to the crowdtesting progress of the current task while resource context reflects the availability and capability factors concerning the competing crowdworker resources in the crowdtesting platform.
second a learning based ranking component extracts features from both process context and resource context and learns the success knowledge of the most appropriate crowdworkers i.e.
the workers with the greatest potential to detect bugs abstracted from historical tasks.
third a diversity based re ranking component adjusts the ranked list of recommended workers by optimizing the worker diversity in order to potentially reduce duplicate bugs.
.
data preprocessing to extract the time sensitive contextual information at recpoint the following data are obtained for further processing refer to section .
for more details of these concepts test task the specific task currently under testing and recommendation test reports the set of already received reports for this specific task up till the recpoint all registered crowdworkers with historical reports a crowdworker submitted including reports in this specific task historical test tasks.there are two types of textual documents in our data repository one is test reports and the other is test requirements.
following the existing studies each document goes through standard word segmentation stopwords removal with synonym replacement being applied to reduce noise.
as an output each document is represented using a vector of terms.
descriptive term filtering .
after the above steps we find that some terms may appear in a large number of documents while some other terms may appear in only very few documents.
both of them are less predictive and contribute less in modeling the testing context.
therefore we construct a descriptive terms list to facilitate the effective modeling.
we first preprocess all the documents in the training dataset see section .
and obtain the terms of each document.
we rank the terms according to the number of documents in which a term appears i.e.
document frequency also known as df and filter out terms with the highest document frequency and terms with the lowest document frequency i.e.
less predictive terms following previous work .
note that since the documents in crowdtesting are often short the term frequency also known as tf which is another commonly used metric in information retrieval is not discriminative so we only use document frequency to rank the terms.
in this way the final descriptive terms listis formed and used to represent each document in the vector space of the descriptive terms.
.
testing context modeling the testing context model is constructed in two perspectives i.e.
process context and resource context to capture the in process progress oriented information and crowdworkers characteristics respectively.
.
.
process context .to model the process context of a crowdtesting task we first represent the task s requirements in the vector space of descriptive terms list and denote it as task terms vector.
we then use the notion of test adequacy to measure the testing progress regarding to what degree each descriptive term of task requirements i.e.
task terms vector has been tested.
testadeq the degree of testing for each descriptive term tjin task terms vector.
it is measured as follows testadeq tj number of bug reports with t j number of received bug reports in a task where tj task terms vector.
the larger testadeq tj the more adequate of testing for the corresponding aspects of the task.
this definition enables the learning of underlying knowledge to match workers expertise or preference with inadequate tested terms at a finer granularity.
.
.
resource context .based on the observations from section .
activeness preference and expertise of crowdworkers are integrated to model the resource context of a general crowdtesting platform.
in addition we include device of crowdworkers as a separate dimension of resource context since several studies reported its diversifying role in crowdtesting environment .
activeness measures the degree of availability of crowdworkers to represent relative uncertainty associated with inactive crowdworkers.
activeness of a crowdworker wis characterized using the following four attributes 1538context aware in process crowdworker recommendation icse may seoul republic of korea lastbug duration in hours between recpoint and the time when worker w s last bugis submitted.
lastreport duration in hours between recpoint and the time when worker w s last report is submitted.
numbugs x number of bugs submitted by worker win past xtime e.g.
past weeks.
numreports x number of reports submitted by worker win past xtime e.g.
past hours.
based on the concepts in table we can derive the above attributes of worker wfrom the historical reports submitted by him her.
preference measures to what degree a potential crowdworker might be interested in a candidate task.
the higher the preference the greater the worker s willingness potential in taking the task detecting bugs.
preference of a crowdworker wis characterized using the following attribute probpref the preference of worker wregarding each descriptive term.
in other words it is the probability of recommending the worker wwhen aiming at generating a report with specific term tj.
it is measured based on bayes rules as follows probpref w tj p w tj tf w tj wktf wk tj wkdf wk df w where tf w tj is the number of occurrences of tjin historical reports of worker w df w is the total number of reports submitted by worker w and kis an iterator over all available crowdworkers at the platform.
as mentioned in section .
after data preprocessing each report is expressed with a set of descriptive terms.
this attribute can be derived from the crowdworker s historical submitted reports.
expertise measures a crowdworker s capability in detecting bugs.
when a crowdworker brings in matching expertise required for the given task he she would have greater possibility in detecting bugs.
expertise of a crowdworker wis characterized using the following attribute probexp the expertise of worker wregarding each descriptive term.
it is measured similarly as probpref as follows probexp w tj p w tj tf w tj wktf wk tj wkdf wk df w where tf w tj is the number of occurrences of tjin historical bug reports of worker w df w is the total number of bug reports submitted by worker w and kis an iterator over all available crowdworkers at the platform.
the difference between probprof andprobexp is that the former is measured based on worker s submitted reports while the latter is based on worker s submitted bug reports following the motivating studies in section .
.
the reason why we characterize expertise in terms of each term is because it enables the more precise matching with the inadequate tested terms and the identification of more diverse workers for finding unique bugs in a much finer granularity.
device measures the device related attributes of the crowdworker which is critical in testing an application and in revealing device related bugs .
device of a crowdworker wis characterized using all his her device related attributes including phone type used to run the testing task operating system of the device model rom type of the phone network environment underwhich a task is run.
these are necessary to reproduce the bugs for the software under test shared among various crowdtesting platforms .
.
learning based ranking based on the dynamic testing context model a learning based ranking method is developed to derive the ranks of crowdworkers based on their probability of detecting bugs with respect to a particular testing context.
.
.
feature extraction .
features are extracted based on the process context and resource context for the learning model as summarized in table .
features capture the activeness of a crowdworker.
previous work demonstrated the developer s recent activity has greater indicative effect on his her future behavior than the activity happened long before so we extract the activeness related features with varying time intervals.
features capture the matching degree between a crowdworker s preference and the inadequate tested aspects of the task.
features capture the matching degree between the a crowdworker s expertise and the inadequate tested aspects of the task.
note that since the learning based ranking method focuses on learning and matching the crowdworker s bug detection capability related to the descriptive terms of a task we do not include the device dimension of resource context.
the first group of features can be calculated directly based on the activeness attributes defined in the previous section.
the second and third group of features are obtained in a similar way by examining the similarities.
for brevity we only present the details to produce the third group of features i.e.
.
table features for learning to rank category id feature activeness indexing1 lastbug lastreport numbugs hours numbugs hours numbugs week numbugs week numbugsall i.e.
in the past numreports hours numreports hours numreports week numreports week numreports all i.e.
in the past preference matching13 partial ordered cosine similarity partial ordered euclidean similarity between worker s preference and test adequacy partial ordered jaccard similarity between worker s preference and test adequacy with the cutoff threshold of .
.
.
.
.
expertise matching20 partial ordered cosine similarity partial ordered euclidean similarity between worker s expertise and test adequacy partial ordered jaccard similarity between worker s expertise and test adequacy with the cutoff threshold of .
.
.
.
.
previous work has proven extracting features from different perspectives can help improve the learning performance so we extract the similarity related features from different viewpoints.
cosine similarity euclidean similarity and jaccard similarity 1539icse may seoul republic of korea junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 are the three commonly used similarity measurements and have proven to be efficient in previous researches therefore we utilize all these three similarities for feature extraction.
in addition a crowdworker might have extra expertise beyond the task s requirements i.e.
the test adequacy to alleviate the potential bias introduced by the unrelated expertise we define the partial ordered similarity to constrain the similarity matching only on the descriptive terms within the task terms vector.
partial ordered cosine similarity pocossim is calculated as the cosine similarity between test adequacy and a worker s expertise with the similarity matching constraint only on terms appeared in task terms vector.
pocossim xi yiq x2 iq y2 i where xiis .
testadeq ti yiisprobexp w ti and tiis the ith descriptive term in task terms vector.
partial ordered euclidean similarity poeucsim is calculated as the euclidean similarity between test adequacy and a worker s expertise with a minor modification on the distance calculation.
poeucsim p xi yi if x i yi if x i yi where xiandyiis the same as in pocossim.
partial ordered jaccard similarity with the cutoff threshold of pojacsim is calculated as the modified jaccard similarity between test adequacy and a worker s expertise based on the set of terms whose probabilistic values are larger than .
pojacsim a b a where a is a set of descriptive terms whose .
testadeq ti is larger than and b is a set of descriptive terms whose probexp w ti is larger than .
.
.
ranking .we employ lambdamart which is the state ofthe art learning to rank algorithm and reported as effective in many learning tasks of se .
model training .
for every task in the training dataset at each recpoint we first obtain the process context of the task and resource context for all crowdworkers then extract the features for each crowdworker in table .
we treat the crowdworkers who submitted new bugs after recpoint not duplicate with the submitted reports as positive instances and label them as .
as reported by existing work that unbalanced data could significantly affect the model performance to make our dataset balanced we randomly sample an equal number of crowdworkers who didn t submit bugs in the specific task with the positive instances and label them as .
the instances close to the boundary between the positive and negative regions can easily bring noise to the machine learner therefore to facilitate the generation of more effective learning model we choose crowdworkers who are different from the positive instances i.e.
to select those majority instances which are away from the boundary.
ranking based on trained model .
at the recpoint we first obtain the process context and resource context for all crowdworkers extract the features in table and apply the trained model to predict the bug detection probability of each crowdworker.
wesort the crowdworkers based on the predicted probability in a descending order and treat a ranked list of higher ranked recnum crowdworkers recnum is an input parameter since usually only a small set of crowdworkers is considered for recommendation as the output of the learning based ranking component i.e.
initial ranking in figure .
.
diversity based re ranking to produce less duplicate reports and improve the bug detection performance as discussed in section .
we develop a diversity based re ranking method to adjust the initial ranking of crowdworkers to optimize the diversity among crowdworkers.
.
.
diversity measurement .we first measure the diversity delta of a worker with respect to current re ranked list of workers s see sec.
.
.
for details in two dimensions i.e.
expertise diversity delta and device diversity delta.
expertise diversity delta gives higher score to these workers who have most different expertise from the workers in the current re ranked list.
expdiv w s tjprofexp w tj wk s .
profexp wk tj where the later part i.e.
estimates the extent to which tjis tested by the workers on current re ranked list.
device diversity delta gives higher scores to these workers who can bring more new device s attributes e.g.
phone type operating system etc.
to those of the workers on current re ranked list so as to facilitate the exploration in new testing environment.
devdiv w s w s attributes wk s w ks attributes where w s attributes is a set of attributes of w s device i.e.
samsung sn9009 android .
.
kot49h.n9009 wifi as in table .
.
.
re ranking .suppose we have a ranked list of recommended workers w1 wrecnum produced by the learning based ranking method and an empty list of re ranked list s the re ranking algorithm first moves w1tos then executes the following steps iteratively suppose current re ranked list having rworkers calculate expdiv w s devdiv w s for the remaining workers in ranked list sort the workers respectively based on expdiv w s anddevdiv w s descending and obtain the expertise index expi w and device index expi w e.g.
expi w 1for the worker with the largest expdiv w s obtain the combined diversity for each worker by expi w divratio devi w where divratio is an input parameter denoting the relative weight of device diversity compared with expertise diversity and move the worker with the smallest value into s. the reason why we use index rather the original value for the combined diversity is to alleviate the influence of extreme value.
experiment design .
research questions rq1 performance evaluation how effective is irec for crowdworker recommendation?
for rq1 we first present some general views of irec for worker recommendation.
to further demonstrate its advantages we then 1540context aware in process crowdworker recommendation icse may seoul republic of korea compare its performance with four state of the art and commonlyused baseline methods details are in section .
.
rq2 context sensitivity to what degree irec is sensitive to different categories of context?
the basis of this work is the characterization of the test context model details are in section .
.
rq2 examines the performance ofirec when removing different sub category of the context to understand the context sensitivity of recommendation.
rq3 diversity gain how much is the diversity gain by introducing the re ranking method in recommendation?
besides the learning based ranking component we further design a diversity based re ranking component to adjust the original ranking.
rq3 aims at examining its role in recommendation.
.
dataset we collected crowdtesting data from baidu5crowdtesting platform which is one of the largest industrial crowdtesting platform.
we collected the crowdtesting tasks that are closed between may.
1st and nov. 1st .
in total there are mobile application testing tasks from various domains details are in our website involving crowdworkers and submitted reports.
for each testing task we collected its task related information all the submitted test reports and related information e.g.
submitter device etc.
the minimum average and maximum number of reports and unique bugs per task are and respectively.
.
experimental setup to simulate the usage of irec in practice we employ a commonlyused longitudinal data setup .
that is all the experimental tasks were sorted in the chronological order and then divided into equally sized folds with each fold having tasks the last fold has tasks .
we then employ the former n 1folds as the training dataset to train irec and use the tasks in the nthfold as the testing dataset to evaluate the performance of worker recommendation.
we experiment nfrom to to ensure a relatively stable performance because a too small training dataset could not reach an effective model.
for each task in the testing dataset at the triggered recpoint see section we run irec and other approaches to recommend crowdworkers.
we experimented recthres from to and due to space limit we only present the results with four representative recthres i.e.
and and leave others on our website.
the size of the experimental dataset i.e.
number of total recpoint under the four recthres are and respectively.
for the parameter divratio we tune the optimal value based on the training dataset.
in detail for every candidate parameter value we experiment from .
to .
we obtain the firsthit see section .
of the recommendation result on the training set and calculate the median value.
we treat the parameter value under which the smallest median value is obtained as the best one.
the parameter recnum is tuned in the same way.
.
evaluation metrics given a crowdtesting task we measure the performance of worker recommendation approach based on whether it can find the right 5test.baidu.comworkers who can detect bugs and how early it can find the first one.
following previous studies we use the commonly used bug detection rate for the evaluation.
bug detection rate at k bdr k is the percentage of unique bugs detected by the recommended kcrowdworkers out of all unique bugs historically detected after the recpoint for the specific task.
since a smaller subset is preferred in crowdworker recommendation we obtain bdr k when kis and .
besides as our in process recommendation aims at shortening the non yielding windows we define another metric to intuitively measure how early the first bug can be detected.
firsthit is the rank of the first occurrence after recpoint where a worker from the recommended list actually submitted a unique bug to the specific task.
to further demonstrate the superiority of our proposed approach we perform one tailed mann whitney u test between our proposed irec and other approaches.
we include the bonferroni correction to counteract the impact of multiple hypothesis tests.
besides the p value for signifying the significance of the test we also present the cliff s delta to demonstrate the effect size of the test.
we use the commonly used criteria to interpret the effectiveness levels i.e.
large .
.
median .
.
small .
.
and negligible .
see details in .
.
ground truth and baselines theground truth of bug detection of a given task is obtained based on the historical crowdworkers who participated in the task after the recpoint.
in detail we first rank the crowdworkers based on their submitted reports in chronological order then obtain the bdr k andfirsthit based on this order.
to further explore the performance of irec we compare irec with four commonly used and state of the art baselines.
mocom this is a multi objective crowdworker recommendation approach by maximizing the bug detection probability of workers the relevance with the test task the diversity of workers and minimizing the test cost.
exrediv this is a weight based crowdworker recommendation approach that linearly combines experience strategy relevance strategy and diversity strategy.
moose this is a multi objective crowdworker recommendation which can maximize the coverage of test requirement maximize the test experience of workers and minimize the cost.
cocoon this crowdworker recommendation approach is designed to maximize the testing quality measured in worker s historical submitted bugs under the test coverage constraint.
for each baseline we conduct worker recommendation before the task begins then at each recpoint we first obtain the set of worker who have submitted reports in the specific task denoted as white list workers and use the recommended workers minus the white list workers as the final set of recommended workers.
note that the reason why take out the white list workers is because crowdworkers only participated one time in a crowdtesting task in our experimental dataset and without the white list the performance would be worse.
1541icse may seoul republic of korea junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 figure performance of irec for firsthit results and analysis .
answering rq1 performance evaluation figure demonstrates the firsthit of worker recommendation under four representative recthres i.e.
recthres sized non yielding window is observed in section i.e.
and .
we can easily see that for all four recthres firsthit ofirec is significantly pvalue is .
and substantially cliff s delta is .
.
better than current practice of crowdtesting.
when recthres is the median firsthit ofirec andground truth are respectively and indicating our proposed approach can shorten the non yielding window by .
for other application scenarios i.e.
recthres is and irec can shorten the non yielding window by to .
figure demonstrates the bdr k of worker recommendation under four representative recthres.
irec significantly p value is .
and substantially cliff s delta is .
.
outperforms current practice of crowdtesting for bdr k k is and .
when recthres is a median of remaining bugs can be detected with the first recommended crowdworkers by our proposed irec with improvement compared with current practice of crowdtesting vs. .
besides a median of remaining bugs can be detected with the first recommended crowdworkers byirec with improvement compared with current practice vs. .
this again indicates the effectiveness of our approach not only for the power in finding the first right workers but also in terms of the bug detection with the set of recommended workers.
we also notice that for a larger recthres the advantage of irec over current practice is larger.
in detail when recthres is irec can improve the current practice by vs. for bdr and when recthres is the improvement is vs. .
this holds true for other metrics.
a larger recthres might indicate the task is getting tough because no new bugs are reported in quite a long time and our proposed irec can help the task get out of the dilemma with new bugs submitted very soon.
furthermore for the recpoint with larger firsthit of ground truth our proposed approach can shorten the non yielding window in a larger extent due to space limit see the figure on our website .
for example for the recpoint whose firsthit of ground truth is larger than recthres is irec can shorten the non yielding window by on median vs. while the improvement is vs. in the whole dataset.
this further indicates the effectiveness of our approach since for recpoint with a larger firsthit of ground truth it is in higher demand for an efficient worker recommendation so that the right worker can come soon.
in the following paper we use the experimental setting when recthres is for further analysis and comparison due to space limit.
comparison with baselines .
figure demonstrates the comparison results with four baselines.
overall our proposed irec significantly p value is .
and substantially cliff s delta is .
.
outperforms the four baselines in terms of firsthit andbdr k k is and .
specifically irec can improve the best baseline mocom by vs. for median firsthit and the improvement is infinite for median bdr k e.g.
vs. for bdr .
this is because all the baselines are designed to recommend a set of workers before the task begins and don t consider various context information of the crowdtesting process.
besides the aforementioned baseline approaches do not explicitly consider the activeness of crowdworkers which is another cause of performance decline.
furthermore the baselines performance are similar to each other which is also due to their limitations of lacking contextual details in one time worker recommendation .
answering rq2 context sensitivity figure shows the comparison results between irec and its six variants.
specifically noact nopref noexp and nodev are different variants of irec without activeness preference expertise and device context respectively.
because process context cannot be removed noproc denotes using the process context at the beginning of a task.
we additionally present norsr which denotes using the resource context at the beginning of the task to further demonstrate the necessity of precise context modeling.
we can see that without any type of the resource context i.e.
noact nopref noexp and nodev the recommendation performance would undergo a decline in both firsthit and bdr k. without activeness related context the firsthit of the recommended workers undergoes a largest variation i.e.
the most sensitive context for recommendation.
this might be because this dimension of features is the only one for capturing time related information and without them the model would lack important clues for the crowdworkers time series behavior.
preference related context exerts a slightly larger influence on the recommendation performance than expertise related context although they are modeled similarly.
this might because many crowdworkers submitted reports but didn t report bugs so preference related context is more informative than experience related context thus we can build more effective learning model.
the lower performance of noproc andnorsr compared with irec further indicates the necessity of the precise context modeling.
.
answering rq3 diversity gain table first demonstrates the average performance of irec and irec without re rank followed by the distribution of performance increase and decrease of irec compared with irec without re rank in all recpoint.
we can see that with the re ranking component the average performance can be improved by to .
specifically the re ranking can increase the bdr in cases and decrease it in cases.
this is because there are large amount of duplicate bugs and increasing the diversity of recommended workers 1542context aware in process crowdworker recommendation icse may seoul republic of korea a bdr k for recthres b bdr k for recthres c bdr k for recthres d bdr k for recthres figure performance of irec for bdr k a firsthit b bdr k figure performance comparison with baselines a firsthit b bdr k figure context sensitivity can help decrease the duplicate bugs so as to increase the unique bugs.
furthermore we can observe that there are more points with performance increase than those with decrease for bdr k with larger k. this makes sense because if a crowdworker contributes less to the diversity he she would be moved backward so that more unique bugs can be detected earlier and the larger of examined k the larger possibility for duplicate bugs in terms of the original list and more room for improvement.
although the average value for all metrics are increased with re rank we admit that the re ranking component can not always improve the performance.
this might be because sometimes the workers ranked earlier are not always those who can detect bugs and when the re ranking moves back the similar workers who can actually detect bugs the bug detection performance would decline.table role of re ranking firsthit bdr bdr bdr bdr average performance irec .
irec without re rank .
improvement .
.
.
.
.
recommending points performance increase performance decrease future work would design more effective re ranking algorithm to tackle the negative effect on the recommendation performance.
discussion .
benefits of in process recommendation in process worker recommendation has great potential to facilitate talent identification and utilization for complex intelligenceintensive tasks.
as presented in the previous sections the proposed irec established the crowdtesting context model at a dynamic finer granularity and constructed two methods to rank and re rank the most suitable workers based on dynamic testing progress.
in this section we discuss with more details about why practitioners should care about such kind of in process crowdworker recommendation.
a at report b at report figure illustrative examples of irec we utilize illustrative examples to demonstrate the benefits of the application of irec .
figure demonstrates two typical bug detection curve using irec for two recpoint of the task in figure .
we can easily see that with irec not only the current non yielding window can be shortened but also the following bug detection efficiency can be improved with the recommended set of workers.
in detail in figure 8a we can clearly see that with the recommended workers the bug detection curve can rise quickly i.e.
with equal 1543icse may seoul republic of korea junjie wang1 ye yang4 song wang5 yuanzhe hu1 dandan wang1 qing wang1 number of workers more bugs can be detected.
also note that in real world application of irec the in process recommendation can be conducted dynamically following the new bug detection curve so that the bug detection performance can be further improved.
in figure 8b although the bug detection curve can not always dominate the current practice the first right worker can be found earlier than current practice.
similarly with the dynamic recommendation the current practice of bug detection can be improved.
table reduced cost with irec recthres recthres recthres recthres 1st quarter .
.
.
.
median .
.
.
.
3rd quarter .
.
.
.
based on the metrics in section .
that are applied for single recpoint we further measure the reduced cost for each crowdtesting task if equipped with irec for in process crowdworker recommendation.
it is measured based on the number of reduced report i.e.
the difference of firsthit value between irec and ground truth following previous work .
for a crowdtesting task with multiple recpoint we simply add up the reduced cost of each recpoint.
as shown in table a median of to cost can be reduced indicating about cost can be saved if equipped with our proposed approach for in process crowdworker recommendation.
note that this figure is calculated by simply summing up the reduced cost of single recpoint based on the offline evaluation scenario adopted in this work.
however as shown in figure in real world practice the recommendation can be conducted based on the bug arrival curve after the prior recommendation and the reduced cost should be further improved.
therefore crowdtesting managers could benefit tremendously from actionable insights offered by in process recommendation systems like irec.
.
implication of in process recommendation nevertheless in process crowdworker recommendation is a complicated systematic human centered problem.
by nature it is more difficult to model than the one time crowdworker recommendation at the beginning of the task.
this is because the non yielding windows are scattered in the crowdtesting process.
although the overall non yielding reports are in quite large number some of the non yielding windows are not long enough to apply the recommendation approach or let the recommendation approach work efficiently.
our observation reveals that an average of cost is wasted on these long sized non yielding windows see section .
but the reduced cost by our approach is only about which is far less than the ideal condition.
from one point of view this is because the front part of the non yielding window i.e.
recpoint in section could not be saved because it is needed for determining whether to conduct the worker recommendation.
and from another point of view there is still room for performance improvement.
on the other hand the true effect of in process recommendation depends on the potential delays due to interactions between the testing manager the platform and the recommended workers.
the longer the delays are the less the benefit can take effect.
it is critical for crowdtesting platforms when deploying in process recommendation systems to consider how to better streamline the recommendation communication and confirmation functions inorder to minimize the potential delays in bridging the best workers with the tasks under test.
for example the platform may employ instant synchronous messaging service for recommendation communication and innovate rewarding system to attract more in process recruitment.
more human factor centered research is needed along this direction to explore systematic approaches for facilitating the adoption of in process recommendation systems.
.
threats to validity first following existing work we use the number of crowdtesting reports as the amount of cost when measuring the reduced cost.
as discussed in the reduced cost is equal with or positively correlated with the number of reduced reports for all the three typical payout schemas.
second the recommendation is triggered by the non yielding window which is obtained based on report s attributes.
in crowdtesting process each report would be inspected and triaged with these two attributes i.e.
bug label and duplicate label so as to better manage the reported bugs and facilitate bug fixing .
this can be done manually or with automatic tool support e.g.
.
therefore we assume our designed methods can be easily adopted in the crowdtesting platform.
third we evaluate irec in terms of each recommending point and sum up the single performance as the overall reduced cost.
this is limited by the offline evaluation which is quite common choice of previous worker recommendation approaches in se .
in real world practice irec can be applied dynamically based on the new bug arrival curve formed by the prior recommended crowdworkers.
we assume when applied online the reduction of cost should be larger because the later recommendation can be based on the results of prior recommendation which is proven to be efficient compared with current practice.
fourth for the generalizability of our approach a recent systematic review has shown current crowdtesting services are dominated by functional usability and security test of mobile applications.
the dataset used in our study is largely representative of this trend with functional and usability test tasks spanning across application domains e.g.
music sport .
the proposed approach is based on dynamically constructing the testing context model using nlp techniques and learning based ranking which is independent of different testing types.
we believe that the proposed approach is generally applicable to supporting other testing types such as security and performance testing since more sophisticated skillsets reflecting these specialty testing may be implicitly represented by corresponding descriptive terms learned in the dynamic context.
therefore the learning and ranking components will not be affected and can be reused.
further verification on other testing types or scenarios is planned as our future work.
related work crowdtesting has been applied to facilitate many testing tasks e.g.
test case generation usability testing software performance analysis software bug detection and reproduction .
there were dozens of approaches focusing on the new encountered problems in crowdtesting e.g.
crowdtesting reports prioritization 1544context aware in process crowdworker recommendation icse may seoul republic of korea reports summarization reports classification automatic report generation crowdworker recommendation crowdtesting management etc.
there were many lines of related studies for recommending workers for various software engineering tasks such as bug triage code reviewer recommendation expert recommendation developer recommendation for crowdsourced software development worker recommendation for general crowdsourcing tasks etc.
the aforementioned studies either recommended one worker or assumed the recommended set of workers are independent of each other which is not applicable for testing activity.
several studies explored worker recommendation for crowdtesting tasks by modeling the workers testing environment experience capability expertise with the task etc.
however these existing worker recommendation solutions only apply at the beginning of the task and do not consider the dynamic nature of crowdtesting process.
the need for context in software engineering is officially proposed by prof. gail murphy in and she stated that the lack of context in software engineering tools would limit the effectiveness of software development.
context related information has been utilized in various software development activities e.g.
code recommendation software documentation static analysis etc.
this work provides new insights about how to model and utilize the context information in open environment.
conclusions open software development processes e.g.
crowdtesting are highly dynamic distributed and concurrent.
existing worker recommendation studies largely overlooked the dynamic and progressive nature of crowdtesting process.
this paper proposed a contextaware in process crowdworker recommendation approach irec to bridge this gap.
built on top of a fine grained context model irec can dynamically learn a ranked list of capable and diverse workers from historical and ongoing contextual information at any specific point of crowdtesting process.
the evaluation results demonstrate its potential benefits in shortening the non yielding window improving bug detection efficiency and reducing testing cost.