efficient binary level coverage analysis m. ammar ben khadra technische universit t kaiserslautern germany khadra eit.uni kl.dedominik stoffel technische universit t kaiserslautern germany stoffel eit.uni kl.dewolfgang kunz technische universit t kaiserslautern germany kunz eit.uni kl.de abstract code coverage analysis plays an important role in the software testing process.
more recently the remarkable effectiveness of coverage feedback has triggered a broad interest in feedback guided fuzzing.
in this work we introduce bcov a tool for binary level coverage analysis.
our tool statically instruments x86 binaries in the elf format without compiler support.
we implement several techniques to improve efficiency and scale to large real world software.
first we bring agrawal s probe pruning technique to binary level instrumentation and effectively leverage its superblocks to reduce overhead.
second we introduce sliced microexecution a robust technique for jump table analysis which improves cfg precision and enables us to instrument jump table entries.
additionally smaller instructions in x86 pose a challenge for inserting detours.
to address this challenge we aggressively exploit padding bytes and systematically host detours in neighboring basic blocks.
we evaluate bcov on a corpus of binaries compiled from eight popular and well tested packages like ffmpeg and llvm.
two instrumentation policies with different edge level precision are used to patch all functions in this corpus over .
million functions.
our precise policy has average performance and memory overheads of and respectively.
instrumented binaries do not introduce any test regressions.
the reported coverage is highly accurate with an average f score of .
.
finally our jump table analysis is comparable to that of ida pro ongccbinaries and outperforms it onclang binaries.
ccs concepts software and its engineering software testing and debugging security and privacy software reverse engineering .
keywords code coverage analysis jump table analysis binary instrumentation acm reference format m. ammar ben khadra dominik stoffel and wolfgang kunz.
.
efficient binary level coverage analysis.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
introduction code coverage analysis is commonly used throughout the software testing process .
structural coverage metrics such as statement and branch coverage can inspire confidence in a program under test put or at least identify untested code .
additionally coverage analysis has demonstrated its usefulness in test suite reduction fault localization and detection of compiler bugs .
moreover certain coverage requirements are mandated by the standards in safety critical domains .
in recent years feedback guided fuzzing has emerged as a successful method for automatically discovering software bugs and security vulnerabilities .
notably afl has pioneered the usage of code overage as a generic and effective feedback signal.
this success inspired a fuzzing renaissance and helped move fuzzing to industrial scale adoption like in google s oss fuzz .
in this work we introduce bcov a tool for binary level coverage analysis using static instrumentation.
bcov works directly on x86 binaries in the elf format without compiler support.
it implements a trampoline based approach where it inserts probes in targeted locations to track basic block coverage.
each probe consists of a detour that diverts control flow to a designated trampoline .
the latter updates coverage data using a single pc relative movinstruction potentially executes relocated instructions and then restores control flow to its original state.
making this scheme to work efficiently and transparently on large and well tested c and c programs required addressing several challenges probe pruning .
instrumenting all basic blocks bbs can be inefficient or even impossible in x86 isa due to its instructionsize variability.
we adopt the probe pruning technique proposed by agrawal where dominator relationships between bbs are used to group them in superblocks sbs .
sbs are arranged in a superblock dominator graph.
covering a single bb implies that all bbs in the same sb are also covered in addition to sbs dominating the current sb.
this allows us to significantly reduce the instrumentation overhead and size of coverage data.
precise cfg analysis .
imprecision in the recovered control flow graph cfg can cause false positives in the reported coverage.
it can also cause instrumentation errors which lead to crashes in a put.
to address this challenge we propose sliced microexecution a precise and robust technique for jump table analysis.
also we implement a non return analysis that eliminates spurious cfg edges after non return calls.
our experiments show that bcov can outperform ida pro the leading industry disassembler.
static instrumentation .
given a set of bbs in an sb we need to choose the best bb to probe based on the expected overhead of restoring control flow.
we make this choice using a classification of bbs in x86 into types.
also some bbs can be too short to insert a detour.
their size is less than bytes.
we address thisarxiv .14191v3 sep 2020esec fse november virtual event usa m. ammar ben khadra dominik stoffel and wolfgang kunz bcov elf patched elf data code run bcov bcov rt data report figure the general workflow of bcov .
a binary is patched with extra code segment trampolines and data segment coverage data .
our bcov rt library dumps the data segment at run time.
in our prototype reporting coverage requires re analyzing the binary.
challenge by aggressively exploiting padding bytes instrumenting jump table entries and introducing a greedy strategy fordetour hosting where a larger bb can host the detour of a neighboring short bb.
combining these techniques with probe pruning enables tracking coverage of virtually all bbs.
.
design overview figure depicts the workflow of bcov .
given an elf module as input bcov first analyzes module level artifacts such as the call graph before moving to function level analyses to build the cfg and dominator graphs.
then bcov will choose appropriate probe locations and estimate the required code and data sizes depending on the instrumentation policy chosen by the user.
our prototype supports two instrumentation policies.
the first is a complete coverage policy where for any test input it is possible to precisely identify covered bbs.
the second one is a heuristic coverage policy where we probe only the leaf sbs in the superblock dominator graph.
running a test suite that covers allleaf sbs implies that code coverage is reached.
we refer to these policies as any node andleaf node policies respectively.
on average the any node policy probes of bbs compared to in the leaf node policy.
average performance overheads are and respectively.
the patching phase can start after completing the previous analysis phase.
here bcov first extends the elf module by allocating two loadable segments a code segment where trampolines are written and a data segment for storing coverage data.
then bcov iterates over all probes identified by the chosen instrumentation policy.
each probe represents a single sb.
generally patching a probe requires inserting a detour targeting its corresponding trampoline.
the detour can be a pc relative jmporcall instruction.
the trampoline first updates coverage data and then restores control flow to its state in the original module as depicted in figure .
the data segment has a simple format consisting of a small header and a byte array that is initialized to zeros.
setting a byte to one indicates that its corresponding sb is covered.
it is trivial to compress this data on disk as only the lsb of each byte is used.
for example this enables storing complete coverage data of llc llvm36b62 cmp eax 0x140 36b67 sete al 36b6a jmp 36bce a original code36b62 cmp eax 0x140 36b67 jmp 6002b8 b patched code 6002b8 mov byte ptr 6002bf sete al 6002c2 jmp 0x36bce c trampoline figure bcov patching example.
a instruction at 0x36b67 must be relocated as the size of jump at 0x36b6a is only two bytes.
b relocated instructions are replaced with a byte detour at 0x36b67 .
c coverage update happens at 0x6002b8 .
control flow is then restored after executing the relocated instruction at 0x6002bf .
backend in 65kb only.1our data format also enables merging coverage data of multiple tests using a simple bitwise or operation.
dumping coverage data requires linking against bcov rt our small runtime library.
alternatively bcov rt can be injected using theld preload mechanism to avoid modifying the build system.
coverage data can be dumped on process shutdown or upon receiving a user signal.
the latter enables online coverage tracking of long running processes.
note that the data segment starts with a magic number which allows bcov rt to identify it.
this design makes bcov achieve three main goals namely transparency performance and flexibility.
program transparency is achieved by not modifying program stack heap nor any generalpurpose register.
also coverage update requires a single pc relative movinstruction which has a modest performance overhead.
finally bcov works directly on the binary without compiler support and largely without changes to the build system.
this enables users to flexibly adapt their instrumentation policy without recompilation.
to summarize we make the following key contributions we are the first to bring agrawal s probe pruning technique to binary level instrumentation.
we show that its superblocks can be effectively leveraged to optimize probe selection and reduce coverage data.
we introduce sliced microexecution a robust method for jump table analysis.
it significantly improves cfg precision and allows us to instrument jump table entries.
we significantly push the state of the art in trampoline based static instrumentation and show that it can be used to track code coverage efficiently and transparently.
we implemented our contributions in the tool bcov which we make publicly available we extensively experimented with bcov .
in this respect we selected popular and well tested subjects such as ffmpeg andllc.
we compiled them using recent major versions of gccandclang at different optimization levels each.
in total we used bcov to instrument binaries and more than .
million functions.
instrumented binaries did not introduce any test regressions.
motivation there is a plethora of tools dedicated to coverage analysis.
they vary widely in terms of goals and features.
therefore we motivate the need for our approach via a comparison with a representative set of popular tools.
our discussion is based on table .
1the binary has around 106bbs which contain more than 106instructions.efficient binary level coverage analysis esec fse november virtual event usa table a comparison with representative coverage analysis tools.
compiler dependent tools require modifying the build system and recompilation which limits flexibility.
the usability of binary level tools in the testing workflow is limited.
in contrast bcov only requires replacing a binary with an instrumented version.
level coveragegoalcompiler independenceperformanceoverheadflexibility usability gcov source complete llvm cov source complete sancov ir heuristic n a intel pt binary heuristic drcov binary both bcov binary both we start with source level tools supported in gccandclang which are gcov andllvm cov respectively.
both track similar artifacts such as statement coverage.
the key difference is in the performance of instrumented binaries.
gcov can not accurately track code coverage in optimized builds.
in comparison llvm cov features a custom mapping format embedded in llvm s intermediate representation ir .
this allows it to cope better with compiler optimizations.
also this mapping format tracks source code regions with better precision compared to gcov .
the ability of a binary level tool such as bcov to report sourcelevel artifacts is limited by the binary to source mapping available.
off the shelf debug information can be used to report statement coverage the most important artifact in practice .
in this setting bcov offers several advantages including detailed view of individual branch decisions regardless of the optimization level precise handling of non local control flow such as longjmp and c exception handling and flexibility in instrumenting only a selected set of functions e.g.
the ones affected by recent changes which is important for the efficiency of continuous testing .
the recent fuzzing renaissance has motivated the need to improve efficiency by heuristically tracking coverage.
sanitizercoverage sancov is a pass built into llvm which supports collecting various types of feedback signals including basic block coverage.
it is used in prominent fuzzers like libfuzzer and honggfuzz .
the performance overhead of sancov is not directly measurable as the usage model varies significantly between sancov users.
also sancov is tightly coupled with llvm sanitizers e.g.
asan which add varying overhead.
extending bcov with additional feedback signals similar to sancov is an interesting future work.
hardware instruction tracing mechanisms like intel pt ipt can also be used for coverage analysis.
however ipt can dump gigabytes of compressed trace data within seconds which can be inefficient to store and post process.
in our experiments ipt dumped .
gb trace data for a libxerces test that lasted only seconds.
post processing and deduplication took more than hours.
in comparison our tool can produce an accurate coverage report for the same test after processing a kb dump in a few seconds.
schumilo et al.
propose to heuristically summarize ipt data on the fly and thus avoid storing the complete trace.dynamic binary instrumentation dbi tools can report binarylevel coverage using dedicated clients plug ins like drcov .
dbi tools act as a process virtual machine that jit emits instructions to a designated code cache.
this process is complex and may break binaries.
moreover jit optimizations add overhead to the whole program even if we are only interested in a selected part such as a shared library.
our evaluation includes a comparison with the popular dbi tools pin and dynamorio .
probe pruning we provide here the necessary background on the probe pruning techniques implemented in bcov based on agrawal .
the original work considered source level pruning but only for c programs.
given a function fwith a set of basic blocks bconnected in a cfg.
the straightforward way to obtain complete coverage data is to probe every basic block bb b. however it is possible to significantly reduce the number of required probes by computingdominance relationships between basic blocks in a cfg.
we say that bbipredominates bbj bbipre bbj iff every path from function entry en tobbjgoes through bbi.
similarly bbipostdominates bbj bbipost bbj iff every path from bbjto function exit ex goes through bbi.
we say that bbidominates bbj iffbbipre bbj bbipost bbj.
the predominator and postdominator relationships are represented by the trees tpreandtpost respectively.
the dominator graph dg is a directed graph that captures all dominance relationships.
it is obtained by the union of both trees dg tpre tpost i.e by merging edges of both trees.
given a dominator graph and the fact that a particular bbis covered this implies that all dominators predecessors of bbin dg are also covered.
this allows us to avoid probing basic blocks that do not increase our coverage information.
however we are interested in moving a step further by leveraging strongly connected components sccs in the dg.
each scc represents a superblock a set of basic blocks with equivalent coverage information.
the superblock dominator graph sb dg is constructed by merging sccs in the dg.
that is each node sb in sb dg represents a scc in the dg.
an edge is inserted between sbiandsbjiff bb sbi bb sbj where bbdominates bb .
constructing a sb dg has a number of benefits.
first it is a convenient tool to measure the coverage information gained from probing any particular basic block.
second it enables compressing coverage data by tracking superblocks instead of individual basic blocks.
finally it provides flexibility in choosing the best basic block to probe in a superblock.
we show later in section .
how this flexibility can be leveraged to reduce instrumentation overhead.
we implemented two instrumentation policies in bcov namely leaf node and any node .
we discuss them based on the example depicted in figure .
in the leaf node policy we instrument only the leaves of the sb dg.
covering allsuch leaf nodes implies that all nodes in sb dg are also covered i.e.
achieving coverage.
however this coverage percentage is usually infeasible in practice.
nevertheless leaf nodes still provide high coverage information which makes the leaf node policy useful to approximate the coverage of a test suite at a relatively low overhead.
generally we are also interested in inferring the exact set of covered basic blocks given anytest input.
this is usually not possibleesec fse november virtual event usa m. ammar ben khadra dominik stoffel and wolfgang kunz a b d c e h en ex g a cfg a c b g d e en h b sb dg graph figure an example cfg and its corresponding sb dg.
first pre domominator and post dominator trees are constructed and merged in a dominator graph dg .
sccs in dg represent nodes in sb dg.
in the leaf node policy only leaf nodes in sb dg namely d e and h need to be probed.
in the any node policy either a or c need to be additionally probed.
enandexarevirtual nodes commonly used to simplify dominance analysis.
in the leaf node policy.
for example given an input that visits the path a c b h g the leaf node policy can report that the covered set is b h g .
however this policy can make no statement about the coverage of aandcsince they do not dominate the visited probe in h. we address this problem in the any node policy.
the set of superblocks instrumented in this policy is a superset of those in the leaf node policy.
more precisely sany sleaf sc.
screpresents the set of critical superblocks in the sense that each sb sccan be visited by at least one path in the cfg that does not visit any of its children in the sb dg.
it is possible to determine scusing ano v e algorithm where vandeare the nodes and edges in the cfg respectively.
we refer to for further details.
in figure the superblock b g is non critical.
however the superblock a c is critical and consequently will be probed in the any node policy.
control flow analysis in this section we first consider the definition of a function at the binary level.
then we discuss sliced microexecution our proposed method for jump table analysis.
.
function definitions the notion of function is important to our approach as it determines the scope of cfg and consequently the correctness of dominance relationships.
functions are well defined constructs in the source code.
however compiler optimizations such as function splitting and inlining significantly change the layout of corresponding binary level functions.
fortunately these optimizations are not of concern to us as long aswell formed function definitions are given to bcov .
a function is defined by the pair f s z where sandzare start address and byte size respectively.
a function can have a set of entry andexitpoints where control flow enters and leaves the function respectively.
we say that a function definition is well formed if its area does not overlap with other functions and all of its basic blocks are reachable only through its entries.
definitions source .
our tool uses linker symbols as a source of well formed function definitions.
these symbols unlike debugsymbols are available by default in all builds.
in stripped binaries bcov can read function definitions from call frame information cfi records which can be found in the .eh frame section.
this section stores the data necessary for stack unwinding and is part of the loadable image of the binary i.e.
is not stripped.
these records must be available to enable c exception handling.
however they are typically available in c binaries as well since they are needed for crash reporting among other tasks.
note that cfi records might not contain all the functions defined in linker symbols.
for example developers might exclude cfi records of leaf functions to save memory.
however we empirically observed that function definitions in cfi records largely match those found in linker symbols.
additionally in the unlikely case where cfi records are unavailable we may still resort to function identification techniques such as .
function entries .
the main entry of a function is trivially defined by its start address.
other functions can either callortail call only the main entry.
we have empirically validated this assumption in our dataset.
that is we have not found any instance where a direct function call targets an internal basic block in another function.
however non local control transfer mechanisms such as longjmp and exception handling violate this assumption.
we refer to possible targets of non local control transfer as auxiliary function entries.
such entries are not dominated by or even unreachable from the main function entry.
auxiliary entries of longjmp are identified during cfg construction.
they are simply the successor of each basic block that calls setjmp .
the identification of auxiliary entries used in exception handling is more elaborate.
the itanium c abi specifies the exception handling standard used in modern unix like systems.
of interest to us in this specification is the landing pad which is a code section responsible for catching or cleaning up after an exception.
a function can have several landing pads e.g.
it can catch exceptions of different types.
we consider each landing pad to be an auxiliary entry.
collecting landing pad addresses requires bcov to iterate over all cfi records in the .eh frame section.
more specifically bcov examines all frame description entry fde records looking for a pointer to a language specific data area lsda .
if such a pointer exists then bcov parses the corresponding lsda to extract landing pad addresses.
function exits .
our tool analyzes the cfg to identify the basic blocks where the control flow leaves a function.
we consider two parameters the type of the control transfer instruction which can be jmp call orret and whether it is a direct or indirect instruction.
a jmptargeting another function is a tail call and generally also an exit point.
however the jump table analysis discussed in section .
can determine that certain indirect jmpare intra procedural i.e.
local to the function.
on the other hand a call typically returns i.e is not an exit point except for calls to non return functions.
the non return analysis implemented in bcov is responsible for identifying such functions.
finally we consider allretinstructions to be exit points.
our model of a function occupying a contiguous code region is simple yet we found it to be consistent with our large dataset.
moreover it can be augmented with additional analyses to identify function entries and exits.
this provides enough flexibility to handleefficient binary level coverage analysis esec fse november virtual event usa table hypotheses tested or falsified to analyze a jump table.
backward slicing answers to .
microexecution is used to falsify hypotheses and recover the jump table.
hypothesis action depends on constant base address?
if yes test else abort is constrained by a bound condition?
if yes test else assume bound condition dominates jump table?
if yes do recovery else assume assume jump table is data bounded do recovery and try to falsify special situations that might arise in practice for example using retto implement indirect calls in retpoline .
.
jump table analysis recovering the targets of indirect control transfer instructions is desirable in several applications such as control flow integrity.
however this problem is in general undecidable which means that we can only hope for approximate solutions i.e.
either to overapproximate or under approximate the actual set of targets.
nevertheless the switch statement in c c remains amenable to precise analysis.
it is commonly implemented as an indirect jmp that is based on single variable indexing into a look up table.
this index variable is intra procedurally bounded.
the analysis of jump tables enables us to increase cfg precision instrument jump table data and avoid disassembly errors.
the latter issue is relevant to architectures such as arm where compilers inline jump table data in the code section.
fortunately in x86 such data typically reside in a separate read only section which enables correct disassembly using linear sweep .
the analysis of jump tables can be challenging as compilers enjoy a lot of flexibility in implementing switch statements.
a jump table can be control bounded by checking the value of the index against a bound condition.
alternatively should the expected values be dense e.g.
many values below the compiler might prefer a data bounded jump table e.g using a bitwise andwith 0xf.
additionally compilers are free to divide a switch with many case labels into multiple jump tables.
our goal in this analysis is to recover information about each individual jump table.
this includes its control flow targets and total number of entries.
we propose sliced microexecution a novel method for jump table analysis which combines classical backward slicing with microexecution .
the latter refers to the ability to emulate any code fragment without manual inputs.
basically for each indirect jmpin a function bcov attempts to test the sequence of hypotheses depicted in table .
if they are invalid then bcov aborts the analysis and considers this jmpto be a tail call.
otherwise bcov proceeds with the actual recovery depending on the type of jump table which can generally either be control bounded or data bounded.
we discuss this method based on the example shown in figure .
first bcov has to test hypothesis by backward slicing from 0x9f719 until it reaches instruction at 0x9f712 which has a memory dependency.
this dependency has a base address in r15.9f6a1 lea r15 set table base.
9f6f0 movzx eax r12b index is r12b 9f6f4 cmp r12b 0x5b bound comparison 9f6f8 mov qword ptr rax 9f6fd mov rax qword ptr 9f700 mov r13 qword ptr 9f704 mov ecx r13d 9f707 ja 9f880 jump to default case 9f70d mov rax qword ptr 9f712 movsxd rax dword ptr 9f716 add rax r15 9f719 jmp rax jump to matching case figure jump table example from perlv5.
compiled with gccv7.
.
highlighted instructions are not part of the backward slice.
the jump table base is set relatively far at 0x9f6a1 .
so is this base address constant?
backward slicing for r15shows that it is constant indeed.
note that a jump table should depend on a single variable used as the index.
the table s base address is a constant determined at compile time.
we move now to test hypothesis .
it is tested by spawning acondition slicer upon encountering each conditional jmp .e.g instruction at 0x9f707 .
this slicer is used to check whether the variable influencing the bound condition is also the jump table index.
this is the case in our example at 0x9f6f0 where the value inr12b influences both the condition at 0x9f707 and the jump table index.
now that a bound condition is found we need to test it against hypothesis .
a jump table might be preceded by multiple conditional comparisons that depend on the index.
we apply heuristics to quickly discard the ones that can not represent a bound condition e.g.
comparisons with zero.
however there can still be more than one candidate.
here we leverage the fact that a bound condition should dominate the jump table.
otherwise a path in cfg would exist where the index value remains unbounded.
we check for dominance during the backward cfg traversal needed for slicing.
basically it should not be possible to bypass the bound condition.
backward slicing produces a program slice code fragment which captures the essential instructions affecting the jump table.
this slice represents a univariate block box function with the index as its input variable.
modifying the index should trigger behavioral changes especially in the observed jump address at the output.
assuming that this slice represents a jump table we reason about its behavior using microexecution.
also we try to validate our assumption by widely varying the index.
before microexecuting a slice bcov first loads the binary using a built in elf loader.
then it initializes a valid memory environment for the given program slice.
for example it allocates memory for the pointer and assigns a valid address to rsp.
it is now possible to start fuzzing the index.
however the expected behavior of the slice depends on the type of jump table.
in control bounded jump tables a change in behavior must be observed in the intervals b and b where bis the bound constant.
this constant is located in the first instruction that sets the flags before the bound condition.
in our example this is the instruction at 0x9f6f4 .bcov tests index values in total of which are sampled from including b and b. the remaining values increase exponentially in powers of starting from b .
we found this scheme to give us high confidence in the results.esec fse november virtual event usa m. ammar ben khadra dominik stoffel and wolfgang kunz the jump table is expected to target an instruction inside the current function for most inputs in b .
on the other hand the jump table should not be reachable for all inputs in b .
that is the bound condition should redirect control flow to the default case.
should the behavior of the program slice not match what we expect from a control bounded jump table then we abort and assume that it is data bounded.
note that we are not strict about the behavior for input bsince the bound condition might check for equality.
assuming that a given indirect jmprepresents a data bounded jump table we need effective techniques to stop backward slicing validate our assumption and explore the bound limits.
note that compilers might use more than one bitwise instruction to bound the index.
moreover developers might prefer computed gotos over switch statements.2in this case they need to assume responsibility for checking index bounds.
to cope with this implementation diversity bcov continues backward slicing as long as the current slice depends on only one variable.
for example assume that raxholds the index and is later used as a base register to read from memory.
this means that the current slice would depend on raxas well as the variable accessed in memory.
backward slicing would stop before this increase in dependencies.
then bcov executes the program slice times each time increasing the index exponentially while setting the least significant bits to one.
this allows us to explore the bound limits in the common case of bitwise andwith a bitmask like 0xf.
other bit patterns are also tried to better penetrate combinations of bitwise instructions.
our key insight is that we should not have full control over the jump target.
that is arbitrary change in the index should be reflected in a constrained change in the jump target.
additionally jump targets need to be located in the current function similar to the case of control bounded jump tables.
should the program slice withstand these diverse tests then we can be highly confident that it represents a jump table.
our evaluation shows that sliced microexecution is precise and robust against various compiler optimizations.
it allowed bcov to reliably recover the jump tables in the core loop of the python interpreter located in function pyeval evalframedefault .
note that these jump tables are compiled from complex computed gotos.
static instrumentation in this section we first consider a strategy to reduce instrumentation overhead by carefully selecting a basic block to probe in a superblock.
then we discuss handling short basic blocks by means of hosting their detours in larger neighboring basic blocks.
.
optimized probe selection generally probing a bb requires inserting a detour targeting its designated trampoline.
a detour occupies bytes and can either be a direct jmporcall .
consequently one or more original instructions must be relocated to the trampoline.
this relocation overhead varies due to the instruction size variability in x86 .
note that a pcrelative mov which occupies bytes represents an unavoidable overhead for updating coverage data in each trampoline.
hence our goal is to reduce the relocation overhead.
2computed gotos is a gccextension to c which is also supported in clang .table bb classification used in probe selection.
types are shown in ascending order based on expected relocation overhead.
the terms long andshort are relative to detour size bytes .
short types require relocating preceeding rp instruction s .
type rp relocation overhead return maybe can be only byte depending on the padding long jump no size of jmpinstruction which is bytes long call no size of call instruction which is bytes jump tab no size of jmpinstruction to original code bytes short call yes similar to long call but with rp overhead added short jump yes similar to long jump but with rp overhead added internal maybe size of relocated instruction s inside the bb long cond no rewriting incurs a fixed byte overhead short cond yes similar to long cond but with rp overhead added to this end we iterate over all bbs in a superblock and select the one expected to incur the lowest overhead.
first we have to establish whether a detour can be accommodated in the first place.
a bb that satisfies s p 5is considered a guest where sandp are the byte size and padding size respectively.
a superblock that contains only guest bbs is handled via detour hosting .
.
now we examine the type and size of the last instruction of each bb and whether the bb is targeted by a jump table.
these parameters are translated to the types depicted in table .
these bb types are organized in a total order.
this means for example we strictly prefer a long call over a long cond should both exist in the same superblock.
this type order is primarily derived from empirical observation.
however we did not necessarily experiment with all possible combinations.
preferring long call over short call should be intuitive.
the latter incurs an additional overhead for relocating at least one instruction preceding the call .
we observed that return basic blocks are usually padded on average .
padding size is often more than bytes which translates to a relocation overhead of only one byte the size of a retinstruction.
also favoring long jmp over long call provided around improvement in both relocation and performance overheads.
on the other hand short call had only a slight advantage over shortjmp.
this might be due to the fixed byte size of the latter which leads to relocating more instructions.
however our experiments were not always conclusive e.g.
between jump tab andshort call .
relocating an instruction depends on its relation to the pc called ripin x86 .
position independent instructions can simply be copied to the trampoline.
however we had to develop a custom rewriter for position dependent instructions.
the rewriter preserves the exact semantics of the original instruction whether it explicitly or implicitly depends on rip.
for example a long cond instruction will be rewritten in the trampoline to a matching sequence consisting of a long cond bytes and a jmp bytes .
jump table instrumentation has the unique property of preserving the original code.
it is a data only mechanism that enables us to probe even one byte bbs.
however for it to be applicable a bb has to be targeted by a patchable jump table.
a jump table is patchable if its entries are either bit offsets or absolute addresses.
we observed that about of more than jump tables in ourefficient binary level coverage analysis esec fse november virtual event usa dataset are patchable.
in fact we found that bit and bit offsets are only used in libopencv core .
finally our probe selection strategy is effective in reducing relocation overhead.
however it is not necessarily optimal.
we observed high variance in the padding of bbs of type return i.e.
return is not always the best choice.
also instrumenting a loop head can unnecessarily trigger multiple coverage updates.
a loopaware strategy might reduce performance overhead by choosing a bb outside the loop as an alternative.
such optimizations are left for future work.
.
detour hosting the instruction size variability in x86 suggests that some bbs are simply too short to safely insert a detour without overwriting other bbs.
in our dataset we found that about of all bbs are short size bytes .
left without a probe we risk losing coverage information of a particular short bb and potentially all of its dominators.
one possible solution is to relocate the entire function to a larger memory area.
however this is costly in terms of code size and the engineering effort required to fix relocated code