the mind is a powerful place how showing code comprehensibility metrics influences code understanding marvin wyrich andreas preikschat daniel graziotin and stefan wagner institute of software engineering university of stuttgart stuttgart germany firstname.lastname iste.uni stuttgart.de andreaspreikschat posteo.de abstract static code analysis tools and integrated development environments present developers with quality related software metrics some of which describe the understandability of source code.
software metrics influence overarching strategic decisions that impact the future of companies and the prioritization of everyday software development tasks.
several software metrics however lack in validation we just choose to trust that they reflect what they are supposed to measure.
some of them were even shown to not measure the quality aspects they intend to measure.
yet they influence us through biases in our cognitive driven actions.
in particular they might anchor us in our decisions.
whether the anchoring effect exists with software metrics has not been studied yet.
we conducted a randomized and double blind experiment to investigate the extent to which a displayed metric value for source code comprehensibility anchors developers in their subjective rating of source code comprehensibility whether performance is affected by the anchoring effect when working on comprehension tasks and which individual characteristics might play a role in the anchoring effect.
we found that the displayed value of a comprehensibility metric has a significant and large anchoring effect on a developer s code comprehensibility rating.
the effect does not seem to affect the time or correctness when working on comprehension questions related to the code snippets under study.
since the anchoring effect is one of the most robust cognitive biases and we have limited understanding of the consequences of the demonstrated manipulation of developers by non validated metrics we call for an increased awareness of the responsibility in code quality reporting and for corresponding tools to be based on scientific evidence.
index terms behavioral software engineering code comprehension placebo effect cognitive bias anchoring effect metrics i. i ntroduction software developers spend more than of their time on activities related to program understanding .
development teams strive to make their code as understandable as possible refactoring source code to make it more understandable is a central part of agile software methodologies and base their activities on the results of static code analysis tools to identify areas of code that are still difficult to understand.
most of the metrics reported by such tools are either not validated to the point that some are empirically demonstrated to not measure what they are assumed to measure .
the latter issue seems to be especially prevalent in the field ofcode comprehensibility .
in other words several metrics do not reflect what they are supposed to measure.
yet they are considered when making decisions and change course of what developers think of their source code.
what is being shown or told to people influences what they think.
the mind is a powerful place and what you feed it can affect you in a powerful way nf the search song .
.
feeding the mind with a belief influences it and causes changes that might go beyond the mind itself.
crum and langer divided a sample of room attendants at different hotels into two groups.
to the first one only the researchers presented the supposed positive effects of work related physical activities on their health.
after four weeks the informed group felt that they received significantly more exercise than the second group.
not just that weight blood pressure and body fat of the informed group significantly decreased compared to the non informed one without any detected change in workload outside work physical activity or eating habits.
if the consequence of a treatment is not attributed to the treatment itself but to pure beliefs and expectations of its effectiveness we call it the placebo effect .
this effect occurs in many ways.
placebos are administered in clinical trials in the form of sham drugs to distinguish the pharmaceutical effect of a drug from the placebo effect.
the placebo effect can go beyond the subjective perception of an affected person and provide measurable effects.
the room attendants were specifically manipulated by the researchers and a desirable effect was achieved.
various contextual factors can also influence our reasoning and decision making some of which make us deviate from beneficial results.
many of these factors are cognitive in nature .
software engineering is no exception.
in a recent systematic mapping study on cognitive biases in software engineering articles were identified that provide evidence for the presence of cognitive biases of at least eight different categories .
negative consequences of such biases are for example overly optimistic effort estimates or insufficient software modifications.
a cognitive bias which is related to estimates and adjust5122021 ieee acm 43rd international conference on software engineering icse .
ieee ments is the anchoring effect introduced by tversky and kahneman .
the anchoring effect means that an initial value is insufficiently adjusted so that different starting points yield different estimates which are biased toward the initial values .
the anchoring effect is one of the most robust cognitive biases .
in our study which we align with the field of quantitative behavioral software engineering we investigate the anchoring effect in the context of source code comprehensibility1and software metrics.
we conducted a randomized double blind experiment.
participants were divided into two groups and were asked to work on source code comprehension tasks over code snippets.
we showed the two groups a metric value that represents the understandability of the code snippets.
one group saw a value that indicates an easy understandability of the source code.
the other group saw one that indicates a hard understandability of the source code.
unbeknownst to the participants tasks and snippets were the very same for both groups.
also the metric is not real and placed there to anchor them.
we investigated the extent to which the shown metric anchors our participants in their understandability.
as common in code comprehension experiments we assessed the effect through the subjective evaluation of the participants .
we also aimed to find out whether the manipulation leads to a placebo effect of the form that we measure differences in actual code comprehension performance i.e.
the time spent and correctness in answering comprehension questions.
finally we explored an initial pool of individual characteristics such as experience personality and mood that are envisioned to play a role in the anchoring effect despite a very limited availability of studies .
to these ends we formulated three research questions which are further framed later in section ii and in section iii e rq1 does the value of a shown code comprehensibility metric influence subjective ratings of code comprehensibility?
rq2 does the value of a shown code comprehensibility metric influence the actual code understanding?
rq3 to which extent do selected individual characteristics correlate with the deviation of the subjective rating from the shown metric value?
understanding the consequences of displaying a metric value brings valuable practical and theoretical implications.
first showing metric values without proper validation can lead to the revision of already well understandable source code which would unnecessarily take time and may also introduce new defects.
second if the presence of a single metric value significantly influences a developer s rating this would be a strong call to meticulously control program comprehension experiments for potential confounding variables that could bias a developer s judgement or to advise against this measurement method altogether.
1we use the terms comprehensibility and understandability interchangeably in line with some of our peers e.g.
.ii.
b ackground in this section we define the central constructs of this work namely the placebo effect the anchoring effect and source code comprehension and place the work in the context of related literature.
given the limited availability of related work in the field of software engineering we include related work in the background section under each of the three topic centered subsections that follow.
a. placebo effect probably one of the most quoted definitions of placebo comes from shapiro who studied the etymology and semantics of the word to provide a basis for an appropriate definition and to address the diversity of opinion about the meaning of the term.
the proposed definition is as follows aplacebo is defined as any therapy or that component of any therapy that is deliberately used for its nonspecific psychologic or psychophysiologic effect or that is used for its presumed specific effect on a patient symptom or illness but which unknown to therapist and patient is without specific activity for the condition being treated.
the placebo effect is defined as the nonspecific psychologic or psychophysiologic effect produced by a placebo .
the introduction of the term in medical literature was accompanied by the widespread introduction of controlled methodology in the evaluation of treatment and it became standard to control for the placebo effect in clinical trials.
in this paper we follow shapiro s definition.
we would like to point out that the placebo effect however is not limited to the medical context and can be applied to everyday aspects as numerous studies have shown.
in a study on placebo sleep participants had to report their previous night s sleep quality.
one group was then told after a supposedly reliable measurement that their sleep quality was above average and the other group was informed that their sleep quality was below average.
the assigned sleep quality but not the self reported sleep quality significantly predicted among others the auditory information processing speed of the participants.
the authors conclude that mindset can influence cognitive performance both positively and negatively .
other studies show for example that smelling a supposedly creativity enhancing odorant actually results in a creativityenhancing effect that non invasive sham brain stimulation improves learning performance and that different forms of placebos have an effect on the performance of athletes .
investigations of the placebo effect in software engineering research have rarely been conducted so far.
one recent study deals with the influence of a three minute breathing exercise on the perceived effectiveness of stand up meetings in agile project teams .
a placebo group was added to compare the effect with a non meditative form of relaxation i.e.
listening to classical music.
they conclude that the breathing exercise has an immediate positive impact on meetings in agile teams.
another study investigates how the subjective evaluation 513of an automatically generated solution is positively influenced by involving the decision maker in the process but not considering their decisions at all.
they conducted a placebocontrolled study with software engineering practitioners and found an increase of in the subjective evaluation of an automatically generated but supposedly decision influenced solution is due to a placebo effect.
we are not aware of any study investigating a potential placebo effect on performance in code understanding activities.
b. anchoring effect in our behaviors we act within a specific context.
such context provides us with cues verbal suggestions and social information that influence our expectations appraisals and memories which in turn influence our behavior and reported experiences .
consequently parts of the placebo effect on subjective assessments are attributed to various forms of decision bias .
a systematic mapping study on cognitive biases in software engineering highlights that the everyday life of a software engineer is also full of situations in which their decisions are subconsciously manipulated .
mohanani et al.
identified articles in the context of software engineering that investigated cognitive biases of at least eight different categories.
in the worst case such bias leads to systematic deviations from optimal reasoning such as overly optimistic effort estimates or insufficient software modifications .
the specific cognitive bias that we investigate is called anchoring effect which we defined in section i. according to the aforementioned mapping study it is the most frequently investigated cognitive bias in the context of software engineering .
for example one study used sql queries as an anchor for query formulation tasks.
they found that while subjects complete the tasks more quickly when modifying a query instead of writing it from scratch accuracy decreases and overconfidence in the results increases .
another example where anchoring plays a role is planning poker.
in planning poker it is considered as positive that all effort estimates remain initially hidden from view so that no one is anchored in their initial estimate by the estimates of their colleagues .
the anchor would not even have to be relevant for the estimate and could for example result from the previous turning of a wheel of fortune with numbers between and .
since we are interested in the transfer of the anchoring effect to a realistic software engineering scenario we have decided to display a code comprehensibility metric value.
limited literature is available that has shown that individual characteristics of the participant may influence the strength of the anchoring effect.
furnham and boo argue in their literature review on the anchoring effect that previous research neglected individual differences variables because people tend to look for a universal rule that would predict reactions or behaviour .
nevertheless they could identify a total of studies that considered the influence of experience personality mood motivation and cognitive abilities.
we aim to contribute to this investigation in the context of rq3 and explored the influence of experience personality happiness and dispositional optimism and pessimism on the anchoring effect.
all these constructs are also associated in literature with the placebo effect .
in iii d and iii e we describe the used questionnaires and how we have operationalized and measured the constructs.
c. source code comprehension understanding source code understandability is defined as the extent to which code possesses the characteristic of understandability to the extent that its purpose is clear to the inspector and in this study we particularly consider bottom up comprehension in which the programmer analyzes the source code line by line and from several lines deduces chunks of higher abstraction and finally aggregates these chunks into highlevel plans .
many factors impact the comprehensibility of source code.
for example one study has shown that shorter identifiers take longer to comprehend and another that a number of certain code patterns lead to an increased rate of misunderstanding .
static code analysis tools attempt to measure code understandability automatically to efficiently point out sections of the code that are difficult to comprehend and should therefore be refactored.
not only are most of the metrics in static code analysis tools not validated but in addition there seems to be only one metric that is validated and positively correlates with measures of source code comprehensibility .
scalabrino et al.
investigated metrics that would measure source code understandability.
code snippets were evaluated with developers and various proxy variables that are related to the time and correctness required to complete comprehension tasks.
according to their results none of the investigated metrics showed a significant correlation with the measured source code comprehensibility.
a recent study empirically evaluated the cognitive complexity a newly introduced metric that claims to measure source code understandability .
the metric evaluates code syntactically and assigns each method a calculated value on a ratio scale.
for java methods the authors suggest a threshold value of above which a snippet should be refactored.
the authors of the evaluation study conclude that the metric is a reliable predictor of the required understanding time and the subjective comprehensibility rating of developers .
both aspects encouraged us to consider the metric when selecting code snippets for our study see iii d they all had a value of .
displaying non validated metrics can lead to confusion and unnecessary effort due to improper prioritization of development efforts.
it becomes especially problematic if the displayed metric value actually has an influence on developers even if only in their subjective perception of the code because this would mean that they are subconsciously manipulated and are very unlikely to resist this circumstance since the anchoring effect is one of the most robust cognitive processes .
514finally in experiments like ours we do not intend to measure how understandable source code is but the degree to which a participant has understood the given source code.
it is in the nature of our study to investigate selected influences on code understanding and not to compare variants of source code for their comprehensibility.
the most common measures for this purpose are time and correctness in processing comprehension questions subjective ratings and physiological measurements such as eye tracking .
through a literature review siegmund and schumann have compiled a list of confounding variables that can affect code comprehension.
their catalog of confounding variables and control techniques is interesting not only because we have taken it into account for the design of our experiment but also because it reveals whether studies in the past have mentioned cognitive biases as confounding variables.
while we cannot identify in their list of individual confounding parameters any that correspond to this in the list of experimental confounding parameters we found a handful that are related to cognitive biases.
one example is the hawthorne effect which describes that participants in experiments would behave differently because they were observed.
with the present study we close a research gap and investigate whether the anchoring effect could add an entry to the catalog of confounding variables on code comprehension in the future.
iii.
m ethodology we follow the guidelines of jedlitschka et al.
on reporting experiments in software engineering .
a. goals the goal of our study is to analyze the effect of showing a specific code comprehensibility metric on measures of a software engineer s code understanding to identify a potential cognitive bias and placebo effect.
to this end we formulated the three research questions given in the introduction.
b. participants we invited a convenience sample of students of a software engineering msc study program in germany to participate in the study.
according to recently proposed guidelines on sampling in software engineering convenience sampling is ideal for pilot studies and studying universal phenomena such as cognitive biases .
additionally we limited participants to those with good knowledge of java and german as the study was conducted in german in both aspects we relied on the self assessment of the participants.
we see the sample properties of interest namely enough experience to comprehend medium to hard to understand java code ensured by our sampling strategy so that the findings can be transferred to a population of experienced java software engineers.
limitations of our sampling strategy and their implications are discussed in v a. as part of their study duties students had to participate in any study offered by the faculty.
students had the right to register for a study and then withdraw their participation atany time including before the start without consequences with course organizers being unaware of their withdrawal.
we reminded them about this during the informed consent phase which included a partial design disclosure health risks privacy and ethical issues and our contact details.
consent was obtained in written form.
participants knew that we aimed to investigate factors that influence the understanding of source code and that they would have to work on short methods written in java and calculate the results for given input values.
they were not aware of the metric manipulation.
c. tasks participants were shown three independent java methods one after the other.
for each code snippet five input values were given for which the participants were asked to specify the return values according to the javadoc and to determine the actual return value.
since we told our participants that there might be bugs in the code they could not rely on the javadoc comment and had to understand what the code actually does.
we consider the deviation of the documentation from the code and the inspection based on concrete values for the input parameters to be a realistic scenario.
furthermore the task is in line with the conceptual model that a developer in a maintenance scenario iteratively constructs and tests hypotheses about the functioning of the code during program understanding .
right after determining the return values the participants were asked to rate the comprehensibility of the method on a scale of very easy to very hard and fill out questionnaires on their individual characteristics details in section iii g .
d. experimental materials environment the tasks were all solved on a laptop provided by us.
code snippets were presented in a web environment specially developed for this study.
the look and feel of the web environment is based on the eclipse ide default look.
tooltips for variables and functions were displayed as typically expected in ides when hovering them.
syntax highlighting and line numbers were available.
selecting a variable highlights all occurrences of that variable.
next to the source code the comprehensibility value of the method was displayed.
a screenshot of the environment is shown in fig.
.
code snippets we used a total of five java code snippets to conduct the study two of which were used to introduce the study and explain the task and the remaining three had to be understood by the participants.
all participants received the same java code snippets regardless of treatment.
each code snippet consisted of exactly one class with exactly one method.
the method was documented via javadoc.
the three task related snippets were taken from either the apache commons lang or apache commons collection project.
we selected the snippets in a way that no uncommon prior knowledge on e.g.
frameworks would be required 515fig.
.
look and feel of the development environment that all participants used for the code comprehension tasks.
to understand them.
as a result the code contained mostly primitive data types and the features of newer java versions were avoided.
the snippets were slightly modified either to introduce a bug or to make sure that all task snippets have the same cognitive complexity an indicator for the comprehensibility of the method which is particularly reliable regarding the subjective rating of developers .
this allowed us to weigh the answers to the three tasks equally and limited potential confusion or loss of trust in the displayed metric if the same metric value was displayed by design but very different difficulties were perceived.
the three task snippets had a cognitive complexity score of which is considered moderate to difficult to understand for java methods.
comprehension questions for each of the three tasks a participant was provided with a paper based form which included five rows of a three column table that had to be filled in.
the cells of the first column each contained a method call for example tobooleanobject ofo .
the other two columns had to be filled with the actual return value of the method and the expected return value according to the javadoc.
questionnaires participants had to fill out several questionnaires.
related constructs are detailed in the next section.
to assess happiness we use the scale of positive and negative experiences spane which quantifies the frequency of positive spane p and negative spane n affective experiences and the happiness overall of our participants spane b .
the questionnaire was successfully used and fully described in other studies of behavioral software engineering e.g.
.
to appraise personality traits we use the big five inventory .
to measure dispositional optimism and pessimism we use the life orientation test lot r .
all measurement instruments have been psychometrically validated in several large scale studies and show good psychometric properties including consistency across fulltime workers and students .
for all questionnaires we used a further psychometrically validated german version i.e.
for spane for the big five inventory and for lotr.
e. hypotheses parameters and variables the independent variable relevant to rq1 and rq2 is the displayed metric value dmv .
we developed the dmv toexpress the understandability of the source code.
the dmv ranges from very easy to understand to very hard to understand .
the choice was dictated to how natural it is for human beings to rate a concept from to .
the individual participant only saw three values for the metric.
for the two introductory examples to explain the study tasks we have chosen the values a very easy task and a very hard task to show possible extremes for coding snippet understandability.
for all three experiment tasks a participant either saw a value of moderately easy or moderately hard to cause the anchoring effect into two opposed directions easy and hard .
regarding rq1 the relevant dependent variable is perceived understandability pu .
perceived understandability is defined as the sum of a participant s ratings for the three code snippets.
the rating of each code snippet ranges identical to the dmv from very easy to understand to very hard to understand .
accordingly the value for pu is in the range of to .
h10 there is no significant difference in perceived understandability pu between the two anchoring directions of a displayed metric value dmv .
h1a there is a significant difference in perceived understandability pu between the two anchoring directions of a displayed metric value dmv .
regarding rq2 we consider two common measures for code comprehension which are time needed to complete all three tasks and correctness of the answers to the comprehension questions.
the time was recorded for each task and summed up at the end.
correctness is the sum of correct answers to the comprehension questions of all three tasks including both correct answers to actual return values and correct answers to return values according to the documentation.
therefore the value for correctness is in the range of to .
to answer the research question we combine time and correctness as scalabrino et al.
did for example to score participants higher that are both fast and correct.
this results in timed actual understanding tau a participant s performance score obtained by combining correctness and time.
equation provides the calculation for tau which ranges from the worst possible to the best possible 516and in which tmax is the time of the participant who took the longest.
correctness parenleftbigg time tmax parenrightbigg h20 there is no significant difference in timed actual understanding tau between the two anchoring directions of a displayed metric value dmv .
h2a there is a significant difference in timed actual understanding tau between the two anchoring directions of a displayed metric value dmv .
the investigation of rq3 the extent to which individual characteristics influence the deviation from the displayed metric value is exploratory research.
therefore no hypotheses were formulated for this research question.
the metric deviation is defined as mean absolute deviation of a participant s rating from the displayed metric value and calculated as shown in where puiis the perceived understandability for task i. pu1 dmv pu2 dmv pu3 dmv the individual characteristics of interest in this study are experience with java personality happiness and dispositional optimism and pessimism.
java experience was measured in years.
personality was operationalized by the five dimensions of the five factor model i.e.
extraversion range agreeableness r conscientiousness r neuroticism r and openness to experience r .
the higher the value the more pronounced is the respective personality facet of a participant.
the range of spane p positive affect and spane n negative affect is r from low frequency to high frequency of positive and negative experiences respectively.
spane b or happiness has a range r the negative pole refers to unhappiness and the positive one to happiness.
dispositional optimism and pessimism are independent constructs rather than a bipolar trait.
both are in the range r from low to high degree of optimism and pessimism respectively.
f .
experiment design the experiment was a between subject design with two treatment groups.
assignment to a treatment was double blind and random.2none of the authors knew which participant even as anonymous data point was in which treatment group until the data was evaluated.
one group saw a dmv of next to all code snippets.
we call this group the easy group from this point on.
the other group saw a dmv of .
we refer to this group as the hard 2we agree to some extent with baltes and ralph that random should be used sparingly so we will add here that participants were assigned to a condition based on the time slot they signed up for.
when assigning treatment conditions to a time slot it was ensured that the conditions were distributed equally over different times of day.group from this point on.
there were no further differences in the treatment of the two groups.
the reader might notice an absence of a control group which does not see any dmv .
this is by design and suggested in literature on the anchoring effect which we discuss in more detail in v a. following wohlin et al.
guidelines for conducting experiments in software engineering we had the study design reviewed by two peers in two iterations and conducted a pilot test.
furthermore we have identified and implemented a number of measures that we believe contribute to mitigating threats to validity.
limitations of our final study design that is what affects the interpretation of our results are discussed in v a. measures to address construct validity we used psychometrically validated questionnaires for assessing individual characteristics.
regarding the operationalization of code understanding we have oriented ourselves on how the construct was measured by our peers in peer reviewed research .
with time and correctness we measured two argumentatively important and objective aspects of code understanding and combined them with equal weight similar to what scalabrino et al.
did .
we designed a plausible scenario to justify the display of the metric value and developed an experiment description for participants which did not reveal the essence of the experiment to prevent hypothesis guessing .
the description did not present false information but hid only the objective of the anchoring effect.
closely related we prevented the threat of experimenter expectations by double blind assignment of participants to treatment groups.
to prevent evaluation apprehension and unnecessary stress we always tried to have exactly two participants simultaneously in the room the experimental supervisor could not look over the shoulder of the participants and it was emphasized several times that the answers were anonymous.
due to their intimate nature the big five and lot r questionnaires were completed only after the code comprehension tasks had been completed.
both participants in the same time slot were also in the same treatment group to avoid diffusion or imitation of treatments for example by one of the two participants making a comment on the displayed metric value.
measures to address internal validity we discussed every variable listed in the mapping study on confounding variables in code comprehension experiments .
of the variables listed only two remained even after thorough planning of the experiment which we see as potential threats to validity the hawthorne effect and selection the generalizability of student participants .
we discuss both in v a. of the potential confounders that we have explicitly controlled we highlight the following two.
first we selected several code snippets that a participant had to understand to reduce the influence of individual data structures and program semantics.
snippets were also neither too difficult nor too easy according to a validated code comprehensibility metric and moreover of comparable comprehensibility.
second we 517implemented a tool to view and interact with the code in the tasks.
this gave us full control over the environment and allowed us to reduce the displayed elements to a minimum to increase internal validity.
in addition no participant had an advantage since no one was more familiar with the environment than everyone else.
measures to address external validity we used code snippets taken from real world actively developed famous open source projects and avoided removing comments or obscuring method or variable names to force code understanding.
instead we have created a realistic software maintenance scenario in which the developer needs to understand code that does not necessarily fit the documentation and that may contain a bug requiring the developer to check what the actual output values are for certain input values.
measures to address conclusion validity regarding reliability of treatment implementation we used a strict protocol and double blind condition assignment to ensure that each participant received the same information and was treated equally.
the same investigator conducted the experiment with all participants.
the dmv of either or was displayed the same for all participants in a treatment group as it was an automatic display in an environment controlled by us.
to prevent random irrelevancies in experimental setting we reserved the room two weeks in advance and for the entire day on each day the study was conducted.
we were prepared to document irregularities but did not have to do so.
g. procedure from the moment one or two participants arrived in the room reserved for the experiment at the agreed time the following steps took place.
participants were provided with a consent form and were informed verbally about the aim of the study.
they were shown the laptop and the files on the laptop.
then they had to fill out two questionnaires one on demographic data and on their happiness spane .
the instructor made the participants save and close the questionnaires on their own after completing them so that they did not feel observed by the instructor.
the instructor explained the task the scenario and the development environment to the participants.
thereby the participants were shown a filled out task sheet for one method as an example and the solution was exemplified for the first two input values.
the visualization of the metric and its display for supposedly informative reasons was explained and a second introductory example which was significantly harder to understand than the first one demonstrated that the metric works well.
when asked the instructor told that the metric was developed by experts similar to what did in their placebo sleep experiment.
the instructor summarized the task and mentioned that the employer in the task scenario wants them to work efficiently but also correctly.
participants then had the opportunity to ask questions before starting with the tasks.
the time recording was done per task and the recorded value was stored in a spreadsheet without the participants noticing it.
after all three1015202530 easy hard experimental groupperceived understandability pu all tasks group easy hard fig.
.
perceived understandability pu of the tasks for the easy group and the hard group three tasks range easiest to hardest combined range easiest to hardest .
tasks had been completed the participants finally had to fill out the questionnaires on individual characteristics.
iv.
r esults a. descriptive statistics and dataset preparation students participated in the study3.
two of them did not submit complete data for rq3 e.g.
missing an item for the spane questionnaire .
we decided to exclude them from the dataset to enhance our confidence in how serious all participants were in completing all tasks.
we thus had an overall sample size of n participants male female .
mean age was .
sd .
average declared experience with the java programming language was .
years sd .
.
participants were randomly allocated to the easy group n or the hard group n .
declared experience with the java programming language was comparable for both groups after random assignment m .
sd .
for the easy group m .
sd .
for the hard group .
the easy group was shown a dmv of4for the three tasks or12combined and provided a pu of15.
sd .
median .
.
the hard group was shown a dmv of8 for the three tasks or 24combined and provided puof20.
sd .
median .
.
a graphical comparison is offered in the boxplot of fig.
.
the easy group performed with an average tau of m .
sd .
median .
.
the hard group 3we cannot offer a precise acceptance rate because course attendance is not mandatory and cannot be recorded.
518table i spearman s correlation coefficient of individual characteristics with the deviation of the subjective rating .
group java experience years spane p spane n spane b optimism pessimism extraversion agreeableness conscientiousness neuroticism openness easy .
.
.
.
.
.
.
.
.
.
.
hard .
.
.
.
.
.
.
.
.
combined .
.
.
.
.
.
.
.
.
.
.
performed with an average tau ofm .
sd .
median .
.
a further boxplot comparison included in the supplemental material did not suggest significant difference.
b. hypothesis testing there was a significant difference between pu of the two groups45 t .
.
p .
ci with a large effect size d .
ci .
we thus reject h10in favor to h1a there is evidence for a difference in perceived understandability between the two anchoring directions of a displayed metric value.
there was no significant difference between the timed actual understanding tau of the two groups6 w p .
with a negligible effect size d .
ci .
we do not reject h20.there is no evidence for a difference in timed actual understanding between the two anchoring directions of a displayed metric value.
c. exploratory analysis we provide in table i the computed correlation coefficients of the metric deviation calculated as in formula with affect related metrics and personality related metrics.
the first two rows provide the correlation coefficients for the two experimental groups between while the third row combines all participants within .
for brevity s sake we will call the metric deviation simply deviation the rest of this section.
as a reminder the wider the deviation the bigger the gap between the subjective rating and the shown metric value or the manipulation on the easy direction or on the hard direction.
given the exploratory nature of rq3 no estimation of significance was conducted for the correlation coefficients.
as a cutoff for potentially interesting individual characteristics we will only consider correlation coefficients .
.
when exploring the data between the two experimental groups we notice that an increase in programming language experience is associated with a decreased deviation when the manipulation suggests an easy task and an increased deviation when the manipulation suggests a hard task.
the opposite 4welch two sample t test given evidence for non normality shapiro wilk test p .
for both groups and no further assumption on the population variance.
5we are aware of the open debate on whether likert items are ordinal data or continuous data .
we believe in line with psychometric theory to have likert items capture discrete points over a continuous scale.
all scales that we use for individual characteristics are psychometrically validated likert items.
6wilcoxon rank sum exact test given evidence for non normality shapirowilk test p .03for the easy group .happens with happiness spane b .
an increase in happiness is associated with an increased deviation for the easy group and a decreased deviation for the hard group.
the two major components of happiness spane p and spane n show coherence with the aggregated happiness score.
an increase for both optimism and pessimism is associated with a wider deviation for the easy group while no correlation is observed for the hard group.
of all personality traits an increase in conscientiousness seems to be strongly correlated with an increased deviation for the easy group followed by neuroticism but with an inverse relationship.
no personality trait shows a .1for the hard group.
when combining the two groups in a within subject analysis with the assumption that the two groups are from the same population an overall negative relationship between happiness and the deviation is observed happier participants deviated less .
an increase in optimism is associated with a wider deviation.
of the personality traits an increase in optimism and conscientiousness were correlated with a bigger deviation but an increase with extraversion was correlated with smaller deviation.
programming experience seems to not play a role in the deviation.
v. d iscussion metrics provide developers with quantitative insights into the quality of their source code but most of the metrics used in practice are not sufficiently validated.
we have investigated the extent to which developers are actually subconsciously influenced by the value of a displayed made up metric to create an awareness of responsibility in code quality reporting.
we found a significant and strong anchoring effect which means that developers are strongly influenced by a displayed metric value in their rating of source code comprehensibility.
this finding is consistent with over years of research on the anchoring effect yet investigation in a specific software engineering context is nevertheless a valuable contribution to build the foundation for future studies for example to investigate consequences of the demonstrated effect.
one such potential consequence could be an improved or worsened understanding of the code.
however in our study we could not provide evidence that the suggestion of simple or difficult code provokes a placebo effect in the sense that the beliefs concerning the comprehensibility of the source code influences the speed and correctness with which a software engineer answers comprehension questions.
since cognitive performance and creativity arguably both characteristics necessary for code understanding can be influenced by a placebo we would have expected to observe such an effect.
assuming that a placebo effect for code comprehension 519can be observed theoretically we see two possible reasons why we could not in our experiment.
first it could be that a displayed metric value is simply not a strong enough influence to cause performance changes.
compared to other placebo studies we did not manipulate the participants to claim that our treatment had specific beneficial or performance enhancing effects.
instead we displayed a metric value that indicated how easy or difficult code is to understand.
we left it up to the participants to interpret what it means and what consequences it might have if source code is easy or hard to understand.
accordingly code comprehension may have actually improved or worsened as a result of the manipulation just not in the way we measured it.
this brings us to the second possibility that time and correctness are not all inclusive proxies for code comprehension.
we speculate that physiological measurements may have led to a different result.
for example cognitive load and stress levels at the end of task processing might have been lower in the easy group as they may have been more comfortable with the task.
while the question of ideal measures of code comprehension is beyond the scope of this work and we have not measured these variables we argue that they nevertheless reflect relevant dimensions of code comprehension.
with the increase in physiological measurements in the field of code comprehension we see much potential for future studies to replicate our experiment with alternative measurement methods to shed light on the matter.
regarding individual characteristics that influence the strength of the observed anchoring effect our results while exploratory and based on correlations are only partially consistent with the few relevant studies conducted to date .
we observed that participants with low extraversion are less subjected to the anchoring effect which is consistent with existing literature .
however our results indicate that participants with high conscientiousness are less susceptible to the anchoring effect which is contrary to the findings of eroglu and croxto .
we could not find a positive correlation between anchoring and the personality trait openness as mcelroy and dowd did.
further our results suggest that happier people might be more subjected to the anchoring effect which also does not coincide with previous studies .
we found that optimism positively correlates with the metric deviation which corresponds to a weaker anchoring effect.
programming experience on the other hand might not play a role in anchoring but this should be taken with caution since we had a homogeneous group of experienced developers and results could be different for inexperienced developers.
if the code comprehensibility metric shows a low value for a rather difficult task personality factors could play a bigger role.
in particular conscientiousness might be the strongest predictor of a deviation when metrics are too conservative in how difficult a task might be easy group followed by optimism.
in conclusion our results suggest that the anchoring effect might not be a universal rule that applies equally toall participants.
the partial deviation of our results from previous findings is an indication that more studies are needed in this regard.
the exploratory settings of rq3 set basic building blocks upon which we call our peers to conduct future research.
a. limitations a detailed description of design decisions made in advance to mitigate validity risks can be found in iii f. what follows are limitations that affect the final study design and should be considered when interpreting our results.
as for potential confounding variables we could reduce a lengthy list of known variables to two that might have affected the results the hawthorne effect and the selection of students as participants.
the hawthorne effect describes that participants in experiments would behave differently because they were observed.
while we addressed hypothesis guessing with a plausible scenario that does not reveal the essence of the experiment and we addressed the threat of evaluation apprehension through privacy and data anonymization it cannot be ruled out that participants followed the proposed metric value more closely than they would have done outside an experimental setting.
according to the current dominant view of the anchoring paradigm focuses on confirmatory hypothesis testing in the sense that information is activated that is consistent with the anchor presented.
we assume that this also applies to our experiment and that the observed anchoring effect if at all is only to a small extent due to an experimentally provoked good will of the participants for the metric.
a future field study could provide clarity in this respect.
as argued earlier in the description of participants iii b we invited a convenience sample of students of a software engineering msc study program.
in our sampling strategy we prioritized internal validity which was enhanced by a homogeneous level of experience with the programming language paradigm and task type.
we also believe that our results can be generalized to a population of professional software engineers.
in the discussion about representativeness of software engineering students opposing opinions have existed for the last years .
recent studies have shown that comparable results can be achieved with both groups of students and professionals as long as the scope of the investigation is carefully considered see e.g.
.
we have confidence in the robustness and soundness of our research design.
recent commentaries have once again highlighted how diverse the views are on the topic.
we side with the view summarized by runeson as well as baltes and ralph that a convenience sample of students is justified in the investigation of central behavioral and cognitive processes as was the case in our study.
there is evidence for example that the anchoring effect is not restricted to laymen and that more experienced people are influenced by it as well .
then since we did not have a control group in our experiment that was notshown a metric value we cannot say 520how a control group would have rated the snippets.
for the demonstration of the anchoring effect this is not a limitation and it is consistent with the body of research on the anchoring effect to not have a no anchor control group .
however regarding the placebo effect we would like to stress that our design would not be able to decide strictly speaking whether any observed effect is due to the placebo but with the design it is still possible to demonstrate the extent to which a displayed metric value influences code understanding in a positive or negative direction.
our study is based on a comprehensive body of research that has provided evidence for the placebo effect so we assumed that the effect would also exist in our scenario and opted for the study design described above.
finally we are aware that the way in which the metric value was displayed is very prominent.
we argue that developers are used to static code analysis tools reporting metric values in similar ways and ides increasingly offer the possibility to display code quality metrics directly in the source code.
even if this situation is not as common we refer to critcher and gilovich and related works showing that for the anchoring effect to work much more inconspicuous anchors which do not even have to be highlighted are usually sufficient.
again a field study with realistic ide plugins and existing metrics could be an option to repeat the investigation of the effects in an industrial environment.
b. implications since developers are influenced by a shown metric value in their subjective evaluation of a code snippet s comprehensibility we highlight the following implications.
first those responsible for reporting code quality metrics should be aware of their responsibility that non validated metrics lead to unwarranted manipulation of developers the consequences of which we do not know yet.
future studies can build on our results and investigate possible consequences.
second since it is a common practice in code understanding studies to ask developers for a subjective rating such studies should ensure that individual participants are not anchored by context factors such as displayed metric values.
it may already be sufficient that the instructor or the task description hint at something about the complexity of a code snippet to be examined.
also for example different amounts of time available for processing different code snippets could lead to an anchoring of the participants in their subjective assessment.
if the study cannot be controlled with certainty in this regard the measure of subjective ratings should not be used.
we echo the call of mohanani et al.
and propose a debiasing technique for the anchoring effect.
the debiasing here is about developing validated metrics or validate existing ones before showing their values to software developers.
we do not consider it an issue when developers are anchored in their subjective judgement by a validated metric that may be able to consider more factors and evaluate more objectively than a developer could.
we are aware that for example static code analysis tools do not intentionally lie and that many ofthe metrics seem to make intuitive sense.
it becomes problematic however when developers interpret quality aspects into metrics that were not intended to be measured by the metric or when the metric is no more than an implemented albeit well thought out idea.
therefore tools should describe very precisely what the metric intends to measure and support this measurement with systematic research.
a number of previous studies already called for more effort to be put into disseminating research findings among practitioners so that they can rely on evidence rather than forming biased and error prone conclusions based on personal impressions .
especially with respect to the clearly shown anchoring effect validated metrics should have an easy time anchoring developers where they should be anchored evidence wise and overcome the circumstance that developers sometimes tend to prefer their own opinions over empirical evidence .
we consider the negative results on rq2 to be good results under the circumstances described above.
since the situation is unlikely to change in the near future it is at least good to know that a few random numbers may not have a negative impact on a developer s understanding time and correctness during maintenance.
whether other aspects of code understanding can be influenced by a placebo will need to be investigated in future studies.
vi.
c onclusion we investigated whether the value of a shown code comprehensibility metric influences subjective ratings of code comprehensibility and actual code comprehension performance i.e.
the time spent and correctness in answering comprehension questions.
in a randomized double blind experiment two groups of participants had to understand three code snippets answer comprehension questions and rate the understandability of the code snippets.
we found the shown metric value to have a significant and large effect on the developers ratings but not on their performance.
the strength of the anchoring effect appears smaller for optimistic and conscientious people.
since we have limited understanding of the consequences of the demonstrated manipulation of developers by nonvalidated metrics we call for an increased awareness of the responsibility in code quality reporting and for corresponding tools to be based more strongly on scientific evidence.
studies in which code comprehensibility is measured via subjective ratings should control contextual factors such as shown metric values that may potentially influence developers ratings or refrain from this measure altogether.
future works should focus on investigating the consequences of the belief that code is easy or hard to understand.
a similar study should be conducted to investigate the influence of displayed metric values on other dimensions of code understanding such as cognitive load and we consider a slightly modified version of our study to be useful e.g.
to investigate the influence of code complexity on the strength of the anchoring effect.
521vii.
d ata availability following open science principles in software engineering we disclose code snippets task sheets with comprehension questions anonymized raw data and the r script for the analysis openly .