testing machine translation via referential transparency pinjia he department of computer science eth zurich switzerland pinjia.he inf.ethz.chclara meister department of computer science eth zurich switzerland clara.meister inf.ethz.chzhendong su department of computer science eth zurich switzerland zhendong.su inf.ethz.ch abstract machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks.
people routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant receiving medical diagnosis and treatment from foreign doctors and reading international political news online.
however due to the complexity and intractability of the underlying neural networks modern machine translation software is still far from robust and can produce poor or incorrect translations this can lead to misunderstanding financial loss threats to personal safety and health and political conflicts.
to address this problem we introduce referentially transparent inputs rtis a simple widely applicable methodology for validating machine translation software.
a referentially transparent input is a piece of text that should have similar translations when used in different contexts.
our practical implementation purity detects when this property is broken by a translation.
to evaluate rti we use purity to test google translate and bing microsoft translator with unlabeled sentences which detected and erroneous translations with high precision .
and .
.
the translation errors are diverse including examples of under translation over translation word phrase mistranslation incorrect modification and unclear logic.
index terms testing machine translation referential transparency metamorphic testing.
i. i ntroduction machine translation software aims to fully automate translating text from a source language into a target language.
in recent years the performance of machine translation software has improved significantly largely due to the development of neural machine translation nmt models .
in particular machine translation software e.g.
google translate and bing microsoft translator is approaching humanlevel performance in terms of human evaluation.
consequently more and more people are employing machine translation in their daily lives for tasks such as reading news and textbooks in foreign languages communicating while traveling abroad and conducting international trade.
this is reflected in the increased use of machine translation software in google translate attracted more than million users and translated more than billion words per day nmt models have been embedded in various software applications such as facebook and twitter .
similar to traditional software e.g.
a web server machine translation software s reliability is of great importance.yet modern translation software has been shown to return erroneous translations leading to misunderstanding financial loss threats to personal safety and health and political conflicts .
this behavior can be attributed to the brittleness of neural network based systems which is exemplified in autonomous car software sentiment analysis tools and speech recognition services .
likewise nmt models can be fooled by adversarial examples e.g.
perturbing characters in the source text or natural noise e.g.
typos .
the inputs generated by these approaches are mostly illegal that is they contain lexical e.g.
bo0k or syntactic errors e.g.
he home went .
however inputs to machine translation software are generally lexically and syntactically correct.
for example tencent the company developing wechat a messaging app with more than one billion monthly active users reported that its embedded nmt model can return erroneous translations even when the input is free of lexical and syntax errors .
there remains a dearth of automated testing solutions for machine translation software at least in part because the problem is quite challenging.
first most of the existing parallel corpora that could be used for testing have already been employed in the model training process.
thus testing oracles of high quality are lacking.
second in contrast to traditional software the logic of neural machine translation software is largely embedded in the structure and parameters of the underlying model.
thus existing code based testing techniques cannot directly be applied to testing nmt.
third existing testing approaches for ai artificial intelligence software mainly target much simpler use cases e.g.
class classification and or with clear oracles .
in contrast testing the correctness of translations is a more complex task source text could have multiple correct translations and the output space is magnitudes larger.
last but not least existing machine translation testing techniques generate test cases i.e.
synthesized sentences by replacing one word in a sentence via language models.
thus their performance is limited by the proficiency of existing language models.
we introduce rtis referentially transparent inputs a novel and general concept as a method for validating machine translation software.
the core idea of rti is inspired by referential transparency a concept in programming ieee acm 43rd international conference on software engineering icse .
ieee an rti pair translations translation meanings !
.
.
.. !
.
!
.. .
.
.
.
.
.
.
.
.
fig.
.
example of a referentially transparent input pair.
the underlined phrase in the left column is an rti extracted from the sentence.
the differences in the translations are highlighted in red and their meanings are given in the right column.
this rti pair and its translations were reported by our approach as a suspicious issue.
the first translation is erroneous.
languages specifically functional programming a method should always return the same value for a given argument.
in this paper we define a referentially transparent input rti as a piece of text that should have similar translations in different contexts.
for example a movie based on bad blood in fig.
is an rti.
the key insight is to generate a pair of texts that contain the same rti and check whether its translations in the pair are similar.
to realize this concept we implement purity a tool that extracts phrases from arbitrary text as rtis.
specifically given unlabeled text in a source language purity extracts phrases via a constituency parser and constructs rti pairs by grouping an rti with either its containing sentence or a containing phrase.
if a large difference exists between the translations of the same rti we report this pair of texts and their translations as a suspicious issue.
the key idea of this paper is conceptually different from existing approaches which replace a word i.e.
the context is fixed and assume that the translation should have only small changes.
in contrast this paper assumes that the translation of an rti should be similar across different sentences phrases i.e.
the context is varied .
we apply purity to test google translate and bing microsoft translator with sentences crawled from cnn by he et al .
.
purity successfully reports erroneous translation pairs in google translate and erroneous translation pairs in bing microsoft translator with high precision .
and .
revealing and erroneous translations respectively.1the translation errors found are diverse including under translation over translation word phrase mistranslation incorrect modification and unclear logic.
compared with the state of the art purity can report more erroneous translations with higher precision.
due to its conceptual difference purity can reveal many erroneous translations that have not been found by existing approaches illustrated in fig.
.
additionally purity spent .74s and .14s on average for google translate and bing microsoft translator respectively achieving comparable efficiency to the state of the art methods.
rti s source code and all the erroneous translations found are released for independent validation.
the source code will also be released for reuse.
the main contributions of this paper are as follows 1one erroneous translation could appear in multiple erroneous translation pairs i.e.
erroneous issues .
the introduction of a novel widely applicable concept referentially transparent input rti for systematic machine translation validation a realization of rti purity that adopts a constituency parser to extract phrases and a bag of words bow model to represent translations and empirical results demonstrating the effectiveness of rti based on unlabeled sentences purity successfully found erroneous translations in google translate and erroneous translations in bing microsoft translator with .
and .
precision respectively.
ii.
p reliminaries a. referential transparency in the programming language field referential transparency refers to the ability of an expression to be replaced by its corresponding value in a program without changing the result of the program .
for example mathematical functions e.g.
square root function are referentially transparent while a function that prints a timestamp is not.
referential transparency has been adopted as a key feature by functional programming because it allows the compiler to reason about program behavior easily which further facilitates higher order functions i.e.
a series of functions can be glued together and lazy evaluation i.e.
delay the evaluation of an expression until its value is needed .
the terminology referential transparency is used in a variety of fields with different meanings such as logic linguistics mathematics and philosophy.
inspired by the referential transparency concept in functional programming a metamorphic relation can be defined within an rti pair.
b. metamorphic relation metamorphic relations are necessary properties of functionalities of the software under test.
in metamorphic testing the violation of a metamorphic relation will be suspicious and indicates a potential bug.
we develop a metamorphic relation for machine translation software as follows rtis e.g.
noun phrases should have similar translations in different contexts.
formally for an rti r assume we have two different contexts c1andc2 i.e.
different pieces of surrounding words .
c1 r andc2 r which form an rti pair are the pieces of text containing rand the two contexts respectively.
411to test the translation software t we could obtain their translations t c1 r andt c2 r .
the metamorphic relation is defined as distr t c1 r t c2 r d where distrdenotes the distance between the translations of rint c1 r and in t c2 r dis a threshold controlled by the developers.
in the following section we will introduce our approach in detail with an example .
iii.
rti and purity s implementation this section introduces referentially transparent inputs rtis and our implementation purity .
an rti is defined as a piece of text that has similar translations across texts e.g.
sentences and phrases .
given a sentence our approach intends to find its rtis phrases in the sentence that exhibit referential transparency and utilize them to construct test inputs.
to realize rti s concept we implement a tool called purity .
the input of purity is a list of unlabeled monolingual sentences while its output is a list of suspicious issues.
each issue contains two pairs of text a base phrase i.e.
an rti and its container phrase sentence and their translations.
note thatpurity should detect errors in the translation of either the base or container text.
fig.
illustrates the process used by purity which has the following four steps identifying referentially transparent inputs.
for each sentence we extract a list of phrases as its rtis by analyzing the sentence constituents.
generating pairs in source language.
we pair each phrase with either a containing phrase or the original sentence to form rti pairs.
collecting pairs in target language.
we feed the rti pairs to the machine translation software under test and collect their corresponding translations.
detecting translation errors.
in each pair the translations of the rti pair are compared with each other.
if there is a large difference between the translations of the rti purity reports the pair as potentially containing translation error s .
algorithm shows the pseudo code of our rti implementation which will be explained in detail in the following sections.
a. identifying rtis in order to collect a list of rtis we must find pieces of text with unique meaning i.e.their meaning should hold across contexts.
to guarantee the lexical and syntactic correctness of rtis we extract them from published text e.g.
sentences in web articles .
specifically purity extracts noun phrases from a set of sentences in a source language as rtis.
for example in fig.
the phrase chummy bilateral talks will be extracted this phrase should have similar translations when used in different sentences e.g.
i attended chummy bilateral talks.
and she held chummy bilateral talks.
for the sake of simplicity andalgorithm rti implemented as purity .
require source sents a list of sentences in source language d the distance threshold ensure suspicious issues a list of suspicious pairs suspicious issues list .initialize with empty list for all source sent insource sents do constituency tree parse source sent head constituency tree head rti source pairs list recursive npf inder head list rti source pairs rti target pairs translate rti source pairs for all target pair inrti target pairs do ifdistance target pair d then add source pair target pair to suspicious issues return suspicious issues function recursive npf inder node rtis allpairs ifnode isleaf then return ifnode .constituent is np then phrase node .string for all container phrase inrtis do addcontainer phrase phrase toallpairs addphrase tortis for all child innode .children do recursive npf inder child rtis copy allpairs return allpairs function distance target pair rtibow bagofwords target pair container bow bagofwords target pair returnjrtibowncontainer bowj to avoid grammatically strange phrases we only consider noun phrases in this paper.
we identify noun phrases using a constituency parser a readily available natural language processing nlp tool.
a constituency parser identifies the syntactic structure of a piece of text outputting a tree where the non terminal nodes are constituency relations and the terminal nodes are words example shown in fig.
.
to extract all noun phrases we traverse the constituency parse tree and pull out all the np noun phrase relations.
note that in general an rti can contain another shorter rti.
for example the second rti pair in fig.
contains two rtis holmes in a movie based on bad blood is the containing rti to a movie based on bad blood this holds true when noun phrases are used as rtis as well since noun phrases can contain other noun phrases.
once we have obtained all the noun phrases from a sentence we filter out those containing more than words and those 412chummy bilateral talkswith trump that illustrated what white house officialshope is a budding partnership between the western hemisphere s two largest economiesunlabeledtext constructrtis generatertipairsinsourcelanguage collectpairsintargetlanguagefrommachinetranslationsoftwarereferentiallytransparentinputs inenglish .chummy bilateral talks2.white house officials3.the western hemisphere stwo largest economies...pairsintargetlanguage inchinese detecttranslationerrorssuspiciousissues chummy bilateral talks with trump that illustrated white house officials hope a budding partnership between the western hemisphere s two largest economies chummy bilateral talks pairsinsourcelanguage inenglish 1chummy bilateral talks ...two largest economies chummybilateraltalks2chummy bilateral talks ...two largest economies white house officialschummy bilateral talks ...two largest economies the western hemisphere stwo largest economies31 ... 2 ... ... ... ... 3 ...targettextmeaning bilateral talks with trump illustratedthat white house officials hope a budding partnership between the western hemisphere s two largest economiesfig.
.
overview of our rti implementation.
we use one english phrase as input for clarity and simplicity.
in the pairs in source language column the phrases above the dashed lines are the original unlabeled text.
the texts marked in blue and underlined are rtis or the identical characters in the translations of an rti pair while the texts marked in red are the characters in the translation of an rti but not in that of its containing phrase sentence.
rti1 holmesinamoviebasedonbadbloodnounphrase holmesinonamoviebasednnpnpnpinppdtnnnpinvbnconstituencyparsetree bloodbadjjnnvpppnpnpnon terminal terminal constituencyrelationswords rti2 amoviebasedonbadblood fig.
.
a constituency parse tree example.
the non terminal nodes in bold and red are the rtis extracted by our approach.
containing less than words2that are not stop words.3this filtering helps us concentrate on unique phrases that are more likely to carry a single meaning and greatly reduces false positives.
the remaining noun phrases are regarded as rtis inpurity .
b. generating pairs in source language once a list of rtis has been generated each must be paired with containing phrases which will be used for referential transparency validation section iii d .
specifically each rti 2these filter values were tuned empirically via grid search on one dataset.
in particular we search the most suitable values in and with step size one for the two filters respectively.
by most suitable values we mean the filter values that achieve the highest ratio between the number of rtis and the number of noun phrases after filtering.
3a stop word is a word that is mostly used for structure rather than meaning in a sentence such as is this an.
pair should have two different pieces of text that contain the same phrase.
to generate these pairs we pair an rti with the full text in which it was found as in fig.
and with all the containing rtis i.e.
other containing noun phrases from the same sentence.
for example assume that rti1 in fig.
is an rti extracted from a sentence rti2 can be found based on the constituency structure note that holmes in a movie based on bad blood is the containing rti to a movie based on bad blood .
thus rti pairs will be constructed rti1 and the original sentence rti2 and the original sentence and rti1 and rti2.
c. collecting pairs in target language the next step is to input rti pairs in the given source language to the machine translation software under test and collect their translations in any chosen target language .
we use the apis provided by google and bing in our implementation which return results identical to google translate and bing microsoft translator s web interfaces .
d. detecting translation errors finally in order to detect translation errors translated pairs from the previous step are checked for rti similarity.
detecting the absensce of an rti in a translation while avoiding false positives is non trivial.
for example in fig.
the rti in the first pair is chummy bilateral talks.
given the chinese translation of the whole original sentence it is difficult to identify which characters refer to the rti.
words may be reordered while preserving the inherent meaning so exact matches between rti and container translations are not guaranteed.
nlp techniques such as word alignment which maps a word phrase in source text to a word phrase in its target target could be employed for this component of the implementation.
however performance of existing tools is poor and runtime can be quite slow.
instead we adopt a bagof words bow model a representation that only considers 413the appearance s of each word in a piece of text see fig.
for example .
note that this representation is a multiset.
while the bow model is simple it has proven quite effective for modeling text in many nlp tasks.
for purity using an n gram representation of the target text provides similar performance.
bow we watched two movies and basketball games fig.
.
bag of words representation of we watched two movies and two basketball games.
each translated pair consists of a translation of an rti t r 4and of its container t ccon r .
after obtaining the bows representation of both translations bow rand bow con the distance distr t r t ccon r is calculated bydist bow r bow con as follows dist bow r bow con jbow rnbow conj in words this metric measures how many word occurrences are in t r but not in t ccon r .
for example the distance between we watch two movies and two basketball games t ccon r and two interesting books t r is .
if the distance is larger than a threshold d which is a chosen hyperparameter the translation pair and their source texts will be reported by our approach as a suspicious issue indicating that at least one of the translations may contain errors.
for example in the suspicious issue in fig.
the distance is because chinese characters do not appear in the translation of the container t ccon r .
we note that theoretically this implementation cannot detect over translation errors in t ccon r because additional word occurrence in t ccon r will not change the distance as calculated in equ.
.
however this problem does not often occur since the source text ccon r is frequently the rti in another rti pair in which case over translation errors can be detected in the latter rti pair.
iv.
e valuation in this section we evaluate the performance of purity by applying it to google translate and bing microsoft translator.
specifically this section aims at answering the following research questions rq1 how precise is the approach at finding erroneous issues?
rq2 how many erroneous translations can our approach report?
rq3 what kinds of translation errors can our approach find?
rq4 how efficient is the approach?
4in our implementation the context could be an empty string.
thus cempty r r. 5for chinese text purity regards each character as a word.table i statistics of input sentences for evaluation .
each corpus contains sentences .
words averagecorpussentence words sentencetotaldistinctpolitics4 .
918933business4 .
949944words a. experimental setup and dataset a experimental environments all experiments are run on a linux workstation with core intel core i7 .2ghz processor 16gb ddr4 2666mhz memory and geforce gtx gpu.
the linux workstation is running 64bit ubuntu .
.
with linux kernel .
.
.
for sentence parsing we use the shift reduce parser by zhu et al.
which is implemented in stanford s corenlp library .
our experiments consider the english !chinese language setting because of the knowledge background of the authors.
b comparison we compare purity with two state ofthe art approaches sit and transrepair ed .
we obtained the source code of sit from the authors.
the authors of transrepair could not release their source code due to industrial confidentiality.
thus we carefully implement their approach following descriptions in the paper and consulting the work s main author for crucial implementation details.
transrepair uses a threshold of .
for the cosine distance of word embeddings to generate word pairs.
in our experiment we use .
as the threshold because we were unable to reproduce the quantity of word pairs that the paper reported using .
.
in this paper we evaluate transrepair ed because it achieves the highest precision among the four metrics on google translate and better overall performance than transrepair lcs for transformers table of .
in addition we re tune the parameters of sit and transrepair using the strategies introduced in their papers.
all the approaches in this evaluation are implemented in python and released .
c dataset purity tests machine translation software with lexically and syntactically correct real world sentences.
we use the dataset collected from cnn articles released by he et al.
.
the details of this dataset are illustrated in table i. this dataset contains two corpora politics and business .
sentences in the politics dataset contains words the average is .
and they contain words and non repetitive words in total.
we use corpora from both categories to evaluate the performance of purity on sentences with different terminology.
b. precision on finding erroneous issues our approach automatically reports suspicious issues that contain inconsistent translations on the same rti.
thus the effectiveness of the approach lies in two aspects how precise are the reported issues and how many erroneous translations can purity find?
in this section we evaluate the precision of the reported pairs i.e.
how many of the reported 414issues contain real translation errors.
specifically we apply purity to test google translate and bing microsoft translator using the datasets characterized by table i. to verify the results two authors manually inspect all the suspicious issues separately and then collectively decide whether an issue contains translation error s and if yes what kind of translation error it contains.
evaluation metric the output of purity is a list of suspicious issues each containing an rti r in source language and its translation t r and a piece of text in a source language which contains the rti ccon r and its translation t ccon r .
we define the precision as the percentage of pairs that have translation error s in t r ort ccon r .
explicitly for a suspicious issue p we set error p totrue iftp r ortp ccon r has translation error s i.e.
when the suspicious issue is an erroneous issue .
otherwise we set error p tofalse .
given a list of suspicious issues the precision is calculated by precision p p2p1ferror p g jpj where pis the suspicious issues returned by purity andjpj is the number of the suspicious issues.
results the results are presented in table ii.
we observe that if the goal is to find as many issues as possible i.e.
d purity achieves .
precision while reporting erroneous issues.
for example when testing bing microsoft translator with the business dataset purity reports suspicious issues while of them contain translation error s leading to precision.
if we want purity to be more accurate we can use a larger distance threshold.
for example when we set the distance threshold to purity achieves precision on all experimental settings.
note the precision does not increase monotonically with the threshold value.
for bing politics the precision drops .
when changing the threshold value from 2to3.
so although the number of false positives decreases the number of true positives may decrease as well.
in our comparisons we find purity detects more erroneous issues with higher precision compared with all the existing approaches.
to compare with sit we focus on the top results i.e.the translation that is most likely to contain errors reported by their system.
in particular the top output of sit contains the original sentence and its translation and the top generated sentence and its translation.
for direct comparison we regard the top output of sit as a suspicious issue.
transrepair reports a list of suspicious sentence pairs and we regard each reported pair as a suspicious issue.
equ.
is used to compute the precision of the compared approaches.
the results are presented in the right most columns of table ii.
when the distance threshold is at its lowest i.e.
d purity finds more erroneous issues with higher precision compared with sit and transrepair.
for example when testing google translate on the politics dataset purity finds erroneous issues with .
precision while sit only finds34 erroneous issues with .
precision.
when d purity detects a similar number of erroneous issues to sit but with significantly higher precision.
for example when testing bing microsoft translator on the politics dataset purity finds erroneous issues with .
precision while sit finds erroneous issues with .
precision.6although the precision comparison is not apples to apples we believe the results have shown the superiority of purity .
as real world source sentences are almost unlimited in practice we could setd for this language setting to obtain a decent amount of erroneous issues with high precision.
we believe purity achieves a much higher precision because of the following reasons.
first existing approaches rely on pre trained models i.e.
bert for sit and glove and spacy for transrepair to generate sentences pairs.
although bert should do well on this task it could generate sentences of strange semantics leading to false positives.
differently purity directly extract phrases from real world sentences to construct rti pairs and thus does not have such kind of false positives.
in addition sit relies on dependency parsers in target sentence representation and comparison.
the dependency parser could return incorrect dependency parse trees leading to false positives.
source texttarget texta lot of innovation coming from other parts of the world by bing innovation coming from other parts of the world by bing the south has emerged as a hub of new auto manufacturing by foreign makers thanks to lower manufacturing costs and less powerful unions.
by google foreign makers thanks by google he was joined by justices ruth bader ginsburg elena kagan and sonia sotomayor.
by bing justices ruth bader ginsburg by bing fig.
.
false positive examples.
false positives false positives of purity come from three sources.
in fig.
we present false positive examples when d .
first a phrase could have multiple correct translations.
as shown in the first example parts have two correct translations i.e.
and in the context other parts of the word .
however when d it will be reported.
this category accounts for most of purity s false positives.
to alleviate this kind of false positive we could tune the distance threshold dor maintain an alternative translation dictionary.
second the constituency parser that we use to identify noun phrases could return a non noun phrase.
in the second example foreign makers thanks is identified as a noun phrase which leads to the change of phrase meaning.
in our experiments false positives are caused by incorrect 6note the precision results are different from those reported by he et al.
because google translate and bing microsoft translator continuously update their model.
415table ii purity sprecision of erroneous issues of suspicious issues using different threshold values .
012345google politics79.
.
.
.
.
google business78.
.
n.a.n.a.
.
.
bing politics78.
.
.
.
.
.
bing business78.
.
.
.
.
.
.
sittransrepairpurity output from the constituency parser.
third proper names are often transliterated and thus could have different correct results.
in the third example the name ruth has two correct transliterations leading to a false positive.
in our experiments false positive is caused by the transliteration of proper names.
rtis extracted by purity we manually inspected all the rtis found by purity.
rtis were found in the politics dataset and rtis were found in the business dataset.
among these rtis rtis .
should have similar translations when they are used in different contexts.
the remaining rtis are caused by the errors from the constituency parser.
out of the original sentences contain rti s .
all the rtis formed rti pairs.
when the distance threshold was which means the translations of the rti could have at most two different chinese characters rti pairs were reported as suspicious issues and the remaining rti pairs did not violate our assumption.
as indicated in table ii suspicious issues are true positives while are false positives.
the number of reported rti pairs under other distance thresholds can be calculated based on the results in table ii.
c. erroneous translation we have shown that purity can report erroneous issues with high precision where each erroneous issue contains at least one erroneous translation.
thus to further evaluate the effectiveness of purity in this section we study how may erroneous translations purity can find.
specifically if an erroneous translation appears in multiple erroneous issues it will be counted once.
table iii presents the number of erroneous translations under the same experimental settings as in table ii.
we can observe that when d purity found erroneous translations.
if we intend to have a higher precision by setting a larger distance threshold we will reasonably obtain fewer erroneous translations.
for example if we want to achieve precision we can obtain erroneous translations in google translate d .
we further study the erroneous translations found by purity sit and transrepair.
fig.
demonstrates the results via venn diagrams.
we can observe that erroneous translations from google translate and erroneous translations from bing microsoft translator can be detected by all the three approaches.
these are the translations for some of the originaltable iii the number of translations that contain errors using different threshold values .
012345google politics695338241595044google business5439208005230bing politics74564222845533bing business6846209654825puritysit googletranslatepuritybingmicrosofttranslator7transrepairsit61113507692 purity7transrepairsit58123879115 fig.
.
erroneous translations reported by purity sit and transrepair.
source sentences.
erroneous translations are unique to purity while erroneous translations are unique to sit and erroneous translations are unique to transrepair.
after inspecting all the erroneous translations we find that purity is effective at reporting translation errors for phrases.
meanwhile the unique errors to sit are mainly from similar sentence of one noun or adjective difference.
the unique errors to transrepair mainly come from similar sentences of one number difference e.g.
five !
six .
based on these results we believe our approach complements the state of theart approaches.
d. types of reported translation errors purity is capable of detecting translation errors of diverse kinds.
specifically in our evaluation purity has successfully detected kinds of translation errors under translation overtranslation word phrase mistranslation incorrect modification 416table iv number of translations that have specific errors in each category .
google politics17943512google business12629811bing politics8251423bing business11538632unclearlogicincorrect modificationunder translationover translationword phrase mistranslation and unclear logic.
table iv presents the number of translations that have a specific kind of error.
we can observe that word phrase mistranslation and unclear logic are the most common translation errors.
to provide a glimpse of the diversity of the uncovered errors this section highlights examples of all the kinds of errors.
the variety of the detected translation errors demonstrates rti s offered by purity efficacy and broad applicability.
we align the definition of these errors with sit because it is the first work that found and reported these kinds of translation errors.
under translation if some parts of the source text are not translated in the target text it is an under translation error.
for example in fig.
magnitude of is not translated by google translate.
under translation often leads to target sentences of different semantic meanings and the lack of crucial information.
fig.
also reveals an under translation error.
in this example the source text emphasizes that the bilateral talks are chummy while this key information is missing in the target text.
sourcethe sorts of problems we work on and the almost anxiety provoking magnitude of data with which we get to worktarget by google target meaningthe sorts of problems we work on and the almost anxiety provoking data with which we get to work fig.
.
example of under translation error detected.
over translation if some parts of the target text are not translated from word s of the source text or some parts of the source text are unnecessarily translated for multiple times it is an over translation error.
in fig.
was an honor is translated twice by google translate in the target text while it only appears once in the source text so it is an over translation error.
over translation brings unnecessary information and thus can easily cause misunderstanding.
word phrase mistranslation if some words or phrases in the source text is incorrectly translated in the target text it is a word phrase mistranslation error.
in fig.
creating housing is translated to building houses in the target text.
this error is caused by ambiguity of polysemy.
the word housing means a general place for people to live in or a concrete building consisting of a ground floor and upper storeys.
in this example the translator mistakenly thought housing refers to sourcecovering a memorial service in the nation s capital and then traveling to texas for another service as well as a funeral train was an honortarget by google target meaningit was an honor to cover a memorial service of the nation s capital and then traveling to texas to conduct another service and a funeral train was an honorfig.
.
example of over translation error detected.
the later meaning leading to the translation error.
in addition to ambiguity of polysemy word phrase mistranslation can be also caused by the surrounding semantics.
in the second example of fig.
plant is translated to company in the target text.
we think that in the training data of the nmt model general motors often has the translation general motors company which leads to a word phrase mistranslation error in this scenario.
sourceadvertisers who are not creating housing employment or credit adstarget by google target meaningadvertisers who are not building houses employment or credit adssourcethe general motors planttarget by bing target meaningthe general motors company fig.
.
examples of word phrase mistranslation errors detected.
incorrect modification if some modifiers modify the wrong element it is an incorrect modification error.
in fig.
better suited for a lot of business problems should modify more specific skill sets .
however bing microsoft translator inferred they are two separate clauses leading to an incorrent modification error.
sourcemore specific skill sets that are better suited for a lot of business problemstarget by bing target meaningmore specific skill sets better suited for a lot of business problems fig.
.
example of incorrect modification error detected.
unclear logic if all the words are correctly translated but the logic of the target text is wrong it is an unclear logic error.
in fig.
bing microsoft translator correctly translated approval and two separate occasions .
however bing microsoft translator returned approve two separate occasions instead of approval on two separate occasions because the translator does not understand the logical relation between them.
fig.
also demonstrates an unclear logic error.
unclear logic errors widely exist in the translations returned by modern machine translation software which is to some extent 417table v running time of purity sec google politicsgoogle businessbing politicsbing businessinitialization0.
.
.
.0046rti construction0.
.
.
.89translation11.
.
.
.66detection0.
.
.
.0301total12.
.
.
.
.
.
.
.
.
.
.
.24puritysittransrepair a sign of whether the translator truely understands certain semantic meanings.
sourceapproval on two separate occasionstarget by bing target meaningapprove two separate occasions fig.
.
example of unclear logic error detected.
e. running time in this section we study the efficiency i.e.
running time of purity .
specifically we adopt purity to test google translate and bing microsoft translator with the politics and the business dataset.
for each experimental setting we run purity times and use the average time as the final result.
table v presents the total running time of purity as well as the detailed running time for initialization rti pairs construction translation collection and referential transparency violation detection.
we can observe that purity spent less than seconds on testing google translate and around minute on testing bing microsoft translator.
specifically more than of the time is used in the collection of translations via translators apis .
in our implementation we invoke the translator api once for each piece of source text and thus the network communication time is included.
if developers intend to test their own machine translation software with purity the running time of this step will be even less.
table v also presents the running time of sit and transrepair using the same experimental settings.
sit spent more than minutes to test google translate and around minutes to test bing microsoft translator.
this is mainly because sit translates words for the politics dataset and words for the business dataset.
meanwhile purity and transrepair require fewer translations and forpurity and and for transrepair .
based onthese results we conclude that purity achieves comparable efficiency to the state of the art methods.
f .
fine tuning with errors reported by purity the ultimate goal of testing is to improve software robustness.
thus in this section we study whether reported mistranslations can act as a fine tuning set to both improve the robustness of nmt models and quickly fix errors found during testing.
fine tuning is a common practice in nmt since the domain of the target data i.e.
data used at runtime is often different than that of the training data .
to simulate this situation we train a transformer network with global attention a standard architecture for nmt models on the wmt zh en chinese to english corpus which contains 20m sentence pairs.
we reverse the standard direction of translation i.e.to en zh for comparison with our other experiments.
we use the fairseq framework to create the model.
to test our nmt model we crawled the latest articles under the entertainment category of cnn website and randomly extract english sentences.
the dataset collection process aligns with that of the politics and the business datasets used in the main experiments.
we run purity with the entertainment dataset using our trained model as the system under test purity successfully finds erroneous translations.
we manually label them with correct translations and fine tune the nmt model on these translation pairs for epochs until loss on the wmt validation set stops decreasing.
after this fine tuning of the sentences are correctly translated.
one of the two translations that were not corrected can be attributed to parsing errors while the other source text one for best director has an ambiguous reference issue which essentially makes it difficult to translate without context.
meanwhile the bleu score on the wmt validation set stayed well within standard deviation .
this demonstrates that error reported by purity can indeed be fixed without retraining a model from scratch a resource and time intensive process.
v. d iscussion a. rti for robust machine translation in this section we discuss the utility of referential transparency towards building robust machine translation software.
compared with traditional software the error fixing process of machine translation software is arguably more difficult because the logic of nmt models lies within a complex model structure and its parameters rather than human readable code.
even if the computation which causes a mistranslation can be identified it is often not clear how to change the model to correct the mistake without introducing new errors.
while model correction is a difficult open problem and is not the main focus of our paper we find it important to explain that the translation errors found by purity can be used to both fix and improve machine translation software.
for online translation systems the fastest way to fix a mistranslation is to hard code the translation pair.
thus the 418translation errors found by purity can be quickly and easily addressed by developers to avoid mistranslations that may lead to negative effects .
the more robust solution is to incorporate the mistranslation into the training dataset.
in this case a developer can add the source sentence of a translation error along with its correct translation to the training set of the neural network and retrain or fine tune the network.
while retraining a large neural network from scratch can take days fine tuning on a few hundred mistranslations takes only a few minutes even for the large sota models.
we note that this method does not absolutely guarantee the mistranslation will be fixed but our experiments section iv f show it to be quite effective in resolving errors.
the developers may also find the reported issues useful for further analysis debugging because it resembles debugging traditional software via input minimization localization.
in addition as rti s reported results are in pairs they can be utilized as a dataset for future empirical studies on translation errors.
b. change of language in our implementation purity we use english as the source language and chinese as the target language.
to match our exact implementation there needs to be a constituency parser or data to train such a parser available in the chosen source language as this is how we find rtis.
the stanford parser7 currently supports six languages.
alternatively one can train a parser following for example zhu et al.
.
other modules ofpurity remain unchanged.
thus in principle it is quite easy to re target rti to other languages.
note that while we expect the rti property to hold for most of the languages there may be confounding factors in the structure of a language that break our assumptions.
vi.
r elated work a. robustness of ai software recently artificial intelligence ai software has been adopted by many domains this is largely due to the modelling abilities of deep neural networks.
however these systems can generate erroneous outputs that e.g.
lead to fatal accidents .
to explore the robustness of ai software a line of research has focused on attacking different systems that use deep neural networks such as autonomous cars and speech recognition services .
these work aim to fool ai software by feeding input with imperceptible perturbations i.e.
adversarial examples .
meanwhile researchers have also designed approaches to improve ai software s robustness such as robust training mechanisms adversarial examples detection approaches and testing debugging techniques .
our paper also studies the robustness of a widely adopted ai software but focuses on machine translation systems which has not been explored by these papers.
additionally most of these approaches are whitebox utilizing gradients activation values while our approach is black box requiring no model internal details at all.
robustness of nlp systems inspired by robustness studies in the computer vision field nlp natural language processing researchers have started exploring attack and defense techniques for various nlp systems.
typical examples include sentiment analysis textual entailment and toxic content detection .
however these are all basic classification tasks while machine translation software is more complex in terms of both model output and network structure.
the robustness of other complex nlp systems has also been studied in recent years.
jia and liang proposed a robustness evaluation scheme for the stanford question answering dataset squad which is widely used in the evaluation of reading comprehension systems.
they found that even the state of the art system achieving near humanlevel f1 score fails to answer questions about paragraphs correctly when an adversarial sentence is inserted.
mudrakarta et al.
also generate adversarial examples for question answering tasks on images tables and passages of text.
these approaches typically perturb the system input and assume that the output e.g.
a person name or a particular year should remain the same.
however the output of machine translation i.e.
a piece of text is more complex.
in particular one source sentence could have multiple correct target sentences.
thus testing machine translation software which is the goal of this paper is more difficult.
c. robustness of machine translation recently researchers have started to explore the robustness of nmt models.
belinkov and bisk found that both synthetic e.g.
character swaps and natural e.g.
misspellings noise in source sentences could break character based nmt models.
in contrast our approach aims to find lexicallyand syntactically correct source texts that lead to erroneous output by machine translation software which the errors more commonly found in practice.
to improve the robustness of nmt models various robust training mechanisms have been studied .
in particular noise is added to the input and or the internal network embeddings during training.
different from these approaches we focus on testing machine translation.
zheng et al.
proposed specialized approaches to detect under and over translation errors respectively.
different from them our approach aims at finding general errors in translation.
he et al .
and sun et al .
proposed metamorphic testing methods for general translation errors they compare the translations of two similar sentences i.e.
differed by one word by sentence structures and four existing metrics on sub strings respectively.
in addition sun et al.
designed an automated translation error repair mechanism.
compared with these approaches rti can find more erroneous translations with higher precision and comparable efficiency.
the translation errors reported are diverse and distinguished from those found by existing papers .
thus we believe rti can compliment with the state ofthe art approaches.
gupta et al.
developed a translation 419testing approach based on pathological invariance sentences of different meanings should not have identical translation.
we did not compare with this paper because it is based on an orthogonal approach and we consider it as a concurrent work.
d. metamorphic testing the key idea of metamorphic testing is to detect violations of metamorphic relations across input output pairs.
metamorphic testing has been widely employed to test traditional software such as compilers scientific libraries and service oriented applications .
because of its effectiveness on testing non testable systems researchers have also designed metamorphic testing techniques for a variety of ai software.
typical examples include autonomous cars statistical classifiers and search engines .
in this paper we introduce a novel metamorphic testing approach for machine translation software.
vii.
c onclusion we have presented a general concept referentially transparent input rti for testing machine translation software.
in contrast to existing approaches which perturb a word in natural sentences i.e.
the context is fixed and assume that the translation should have only small changes this paper assumes the rtis should have similar translations across different contexts.
as a result rti can report different translation errors e.g.
errors in the translations of phrases of diverse kinds and thus complements existing approaches.
the distinctive benefits of rti are its simplicity and wide applicability.
we have used it to test google translate and bing microsoft translator and found and erroneous translations respectively with the state of the art running time clearly demonstrating the ability of rti offered by purity to test machine translation software.
for future work we will continue refining the general approach and extend it to other rti implementations such as using verb phrases as rtis or regarding whole sentences as rtis pairing them with the concatenation of a semanticallyunrelated sentence.
we will also launch an extensive effort on translation error diagnosis and automatic repair for machine translation systems.