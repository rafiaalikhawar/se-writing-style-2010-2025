frugal unlocking semi supervised learning for software analytics huy tu and tim menzies department of computer science north carolina state university raleigh usa hqtu ncsu.edu timm ieee.org abstract standard software analytics often involves having a large amount of data with labels in order to commission models with acceptable performance.
however prior work has shownthat such requirements can be expensive taking several weeksto label thousands of commits and not always available whentraversing new research problems and domains.
unsupervisedlearning is a promising direction to learn hidden patterns withinunlabelled data which has only been extensively studied in defectprediction.
nevertheless unsupervised learning can be ineffectiveby itself and has not been explored in other domains e.g.
staticanalysis and issue close time .
motivated by this literature gap and technical limitations we present frugal a tuned semi supervised method that builds ona simple optimization scheme that does not require sophisticated e.g.
deep learners and expensive e.g.
manually labelleddata methods.
frugal optimizes the unsupervised learner sconfigurations via a simple grid search while validating our de sign decision of labelling just .
of the data before prediction.
as shown by the experiments of this paper frugal outperforms the state of the art adoptable static code warning recog nizer and issue closed time predictor while reducing the costof labelling by a factor of from to .
.
hence weassert that frugal can save considerable effort in data labellingespecially in validating prior work or researching new problems.
based on this work we suggest that proponents of complex and expensive methods should always baseline such methods againstsimpler and cheaper alternatives.
for instance a semi supervisedlearner like frugal can serve as a baseline to the state of the art software analytics.
index t erms software analytics data labelling efforts semi supervised learning i. i ntroduction software analytics can guide improvements to software quality maintenance and security.
for example analytics can discover which static code warnings are adoptable whether the new issues can be easily fixed wheresoftware defects are likely to occur which commentslikely to contain technical debts what the currenthealth conditions of these open source projects or howto distinguish security bug reports .
however models that perform these software analytics tasks typically learn from labelled data.
generating such labels can be extremely slow and expensive.
for instance tu et al.
reported that manually reading and labelling commits required person hours approximately nine weeks including cross checking among labellers.
due to the labor intensive nature of the process researchers often reuse datasetslabelled from previous studies.
for instance lo et al.
y ang et al.
and xia et al.
certified their methodsusing data generated by kamei et al.
.
while this practiceallows researchers to rapidly test new methods it leaves thepossibility for any labelling mistake to propagate to otherrelated works.
in fact in technical debts identification beforereusing prior work s data y u et al.
discoveredthat more than of the false positives were actually truepositives casting doubt on work that used the original dataset.hence it is timely to ask can we reduce the labelling effort associated with building models for software analytics?
unsupervised learning techniques that learns patterns fromunlabelled data is a promising direction for software analytics.such learning has been used for buggy non buggy classifica tion .
the state of the art sota unsupervisedlearner is nam and kim s cla c method.
cla is based on the binary split of the output space at the aggregated median c of all features median in the data.
however other areas and different datasets may not share the same datacharacteristics for the default cla c to perform well.
to address this gap our study adopts and extends clafrom defect prediction to other software analytics like staticcode warnings and issues close time.
promising extensions for unsupervised learning involves finding different control settings to configure the system hyperparameter tuning and validating on small labelled dataregions e.g.
.
before applying the best setting to thetest data.
recent software engineering se research showsmany domains sota can be improved with hyperparametertuning .
dodge is one prominent optimizer whichshows the output space of the models on low dimensionaldata can be easily surveyed through dodging away from prior options or options that resulted in statistically similarperformance.
simply the central function of cla binary splitof the output space via aggregated c is synonymous to se s sota optimizer dodge with less information required areduction of .
train data s labels.
specifically our workproposed frugal c three different modes of cla as shown in figure with c to95 increments by where all the combinations can be easily executed in a grid search manner.
to understand and validate the frugal system we investigate the following research questions rq1 how much labelled data l that frugal requires?
from our investigation of various lvalues frugal s performance plateaus beyond l .
and frugal s success is not altered by large changes to l.result 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee fig.
three different modes of cla devised from nam and kim for defect prediction.
rq2 how does frugal perform in adoptable static code warnings identification?
when comparing to the sota solution in emse frugal wins on recall loses in auc anddraws in far with .
of the labelled train data.result rq3 how well does frugal predict issue close time?
when comparing to the sota solution in emse which was compared to icse promise msr comad frugal outperforms in far recall and auc whileperforming similarly in accuracy with only .
ofthe labelled train data.result in summary our work s contributions to the field of software analytics are as follows this work is the first to assess the usage of unsupervised learning to reduce the labelling efforts to commission mod els building in adoptable static code warnings identificationand issues close time prediction.
frugal surpasses the sota issues close time predictor and performs similarly to the sota adoptable static codewarning identifier with .
less information.
frugal reduces the labelling efforts to commission new models building by .
.
in another word frugal is40 times cheaper than sota methods in issue close timeand static code warning analysis areas.
the performance of our framework suggests that many more domains in se could benefit from unsupervisedlearning solutions in the semi supervised learning mannerbeyond defect prediction .
to better support other researchers our scripts and data are on line at ssl.
ii.
b ackground and rela ted work a. studying static code warnings background static code warning tools detect potential static code defects in source code or executable files at thestage of software product development.
this covers a rangeof potential defects such as common programming errors code styling in line comments common programming anti patterns style violations and questionable coding decisions.the distinguishing feature of these tools is that they make theircomments without reference to a particular input.
nor do theyuse feedback from any execution of the code being studied.examples of these tools include pmd checkstyle2and the findbugs3tool.
one issue with static code warnings is that they generate a large number of false positives.
many programmers rou tinely ignore most of the static code warnings finding themirrelevant or spurious .
such warnings are considered as unadoptable since programmers just ignored them.
between35 and of the warnings generated from static analysistools are known to be unadoptable.
this high false alarm rateis one of the most significant barriers for developers to usethese tools .
hence it is prudent to learn to recognizewhat kinds of warnings programmers usually act upon so thetools can be made more useful by first pruning away theunadoptable warnings.
v arious approaches have been tried 395table i summary of yang et al.
s data distribution.
the gray cells are median values for the correspondingcolumns.
training set test set dataset featuresinstance countsadoptable ratio instance countsadoptable ratio commons phoenix mvn maven jmeter cass cassandra ant lucence derby tomcat to reduce these false alarms including graph theory statistical models and ranking schemes .
previouswork referred to the target warnings found by theseapproaches as actionable warnings but we found that itactually refer to adoptable warnings that were adopted.that means the warnings that are adopted by developersdo not necessarily mean the warnings are actionable somedevelopers still need to consult external sources to figure outthe solutions .
data and algorithms the data for this paper comes from a recent study by wang et al.
.
they conducted asystematic literature review to collect all public available staticcode features generated by widely used static code warningtools in total all the values of these collected features were extractedfrom warning reports generated by findbugs based on 60revisions of projects.
to ensure the difference between prior and later revisionintervals of a project is adequate for the solid conclusions tobe drawn wang et al.
set revision intervals for differentprojects e.g.
months for lucene and months for mvn.
each project in this study has at least two years commithistory.
to eliminate ineffective features to the results of thoselearners a greedy backward selection algorithm is applied.then they isolated features as the most useful ones foridentifying adoptable static code warnings.
they called these features the golden set i.e.
the fea tures most important for recognizing adoptable static codewarnings.
to the best of our knowledge this is the most exhaustiveresearch about static warning characteristics yet published.
asshown in table of the golden set features fall intoeight categories.
these features are the independent variablesused in this study.
to assign dependent labels we applied themethods of liang et al.
.
they defined a specific warningas adoptable if it is closed after the later revision interval.
by analyzing findbugs output from two consecutive releases of nine software projects collecting the features oftable from and then applying the liang et al.
sdefinitions we created the data of table i. in this table the training set refers to release i 1and the test set is releasei.
in this study we only employ two latest releases.one of many extensive studies exploring the usage of machinelearning ml in this area is heckaman et al.
.
theyapplied ml algorithms to recognize the adoptable warnings programmers can act upon based on features derived fromstatic analysis tool they achieved recalls of averageacross data sets .
the sota system that we will compareagainst is from y ang et al.
where they took advice fromghotra et al.
to compare several representative non neurallearners table of in software analytics with variouspopular neural network models.
they found that all treatmentsperformed similarly to each other but non neural learners didthat with less time than deep learners.
note that for any particular data set the features can grow.
for example consider the return type feature in the code analysis category.
this can include numerous returntypes extracted from a given project which could be void int url boolean string printstream file and date or a list ofany of these periods .
hence as shown in table i the numberof features in our data varied from to .
b. predicting bugzilla issue close time background when programmers work on repositories predicting issue close time has multiple benefits for the developers managers and stakeholders since it is helpful for end users who are directly affected by the product de velopers prioritize work managers allocate resources andimprove consistency of release cycles and stakeholdersunderstand changes in project timelines and budgets although bugs have an assigned severity this is not a suffi cient predictor for the lifetime of the issue.
for example theauthor who issued the bug may be significant contributorsto the project.
alternatively an issue deemed more visible to end users may be given higher priorities.
it is therefore insufficientsimply to consider the properties of the issue itself issue metrics but also of its environment context metrics .
sim ilarly recent work showed how process metrics are better measurements for predicting than product metrics .
an example of such issue close times estimator can notify involved parties if the recently created issue is an easy fix.
data and algorithms the state of the art system for predicting issue close time comes from a recent study byy edida et al.
.
they conducted a literature review of99 research papers that are comprised of from watson sliterature reviews and top venues listed in google scholarmetrics for software systems artificial intelligence andcomputational linguistics in the last three years with at least10 citations per years traditional or non neural approaches include guo et al.
s study on a large closed source project microsoftwindows to predict whether or not a bug will be fixed and marks et al.
used ensemble method of decisiontrees i.e.
random forests on eclipse and mozilla data.
as to deep learning or neural network approach aredasenet and deeptriage .
396table ii an overview of the data used in the lee et al.
and yedida et al.
study.
note that because of themanner of data collection i.e.
using bin sequences for eachday for each report there are many more data samplesgenerated from the number of reports mined.
project observation period reports train test eclipse jan mar chromium mar aug firefox apr may only a minority of deep learning papers .
performed any sort of hyper parameter optimization i.e.
varied fewnumbers of parameters such as the number of layers ofthe deep learner to edge out the best performance ofdeep learning.
even fewer papers .
applied hyper parameter optimization in a non trivial manner i.e.
notusing a hold out set to assess the tuning before assessingthe separate test set .
to obtain a fair comparison with the prior state of the art we use the same data as used in the lee et al.
mani et al.
y edida et al.
s studies.
the data was collected fromthe three projects of firefox chromium and eclipse preprocessing involves standard text mining to removespecial characters or stack traces tokenization and pruningthe corpus to a fixed length.
the activities per day were collected into two bins includ ing user activity e.g.
comments system records e.g.
added removed labels and metadata e.g.
the user was thereporter days from opening etc .
given issue close times ... k days they are grouped into s1 and s2 suchthat s1 s2 ands1 ...i s2 i ... k .
for instance s1 includes days and s2 includes days if the number of issues closed in to days number of issues closed in to days.
along with the numerical metadata user and system recordsare transformed to machine readable data for the models toexecute through word2vec .
in the same manner as prior work the target class is discretized into two bins so that each bin has roughly thesame number of samples .
this yields datasets that are near perfectly balanced e.g.
in the chromium dataset we observeda class ratio .
c. evaluation measures of performance since we wish to compare our approach to prior work we take the methodological step of adopting the same performance scores as that seen in priorwork.
let tp tn fp fn are the true positives true negatives false positives and false negatives respectively then y anget al.
used auc recall and false alarm while y edidaet al.
using only accuracy for their studies.
we alsoadd precision and f1 for additional validation but see ourcautionary note at the end of this list auc area under the roc curve measures the twodimensional area under the receiver operator characteristic roc curve .
it provides an aggregate and overallevaluation of performance across all possible classificationthresholds to report the discrimination of a classifier .
precision tp tp fp represents the ability of one algorithm to identify instances of positive class among theretrieved positive instances.
recall tp tp fn represents the ability of one algorithm to identify instances of positive class from thegiven dataset.
f1 precision recall precision recall is the harmonic mean of both precision and recall metrics.
false alarms far tn tn fp measures the instances that are falsely classified by an algorithm aspositive which are actually negative.
this is an importantindex used to measure the efficiency of a model.
accuracy tp tn tp tn fp fn is the percentage of correctly classified samples.
in the effort aware theme of this paper we are interestedin the labelling effort to commission new models building which is cost human verified comments comments .
except for far and cost for the rest of these metrics accuracy recall and auc the higher the value the better the performance.
cautionary note menzies et al.
warns that precision can be misleading for imbalanced data sets like that studiedhere e.g.
table i reports that for static warning analysis themedian of target class is .
hence while we do not placemuch weight on classifiers that fail on precision or f1.
statistical analysis with the deterministic nature we employed cohen d effect size test to determine which results are similar by calculating medium step2across those seven metrics.
as to what dto use for this analysis we take the advice of a widely accepted sawilowsky et al.
s work .that paper asserts that small and medium effects can bemeasured using d .2andd .
respectively .
splitting the difference we will analyze this data looking for differenceslarger than d .
.
.
medium step2 orm .
stddev all results the sota adoptable code warnings identifier and the sotaissue close time predictor also validated their results with thistest but with d .35andd .3respectively.
iii.
l abelling one of the goals of industrial analytics is that new conclusions can be quickly obtained from new data just by applyingdata mining algorithms.
as shown in figure there are at leastnine separate stages that must be completed before that goalbe reached .
each of these stages offers unique and separatechallenges each of which deserves extensive attention.
manyof these steps have been extensively studied in the literature .
however the labelling workof step has been receiving scant attention.
in literature thereare several approaches for executing the labelling process manual labelling crowdsourcing reuse of labels 397fig.
nine stages of the machine learning workflow from a case study at microsoft by amershi et al.
.
some stages are data oriented e.g.
data collection cleaning and labelling and others are model oriented e.g.
model requirements features engineering model training evaluation evaluation deployment and monitoring .
automatic labelling active learning a special kind of semi supervised learning all of these approaches have their drawbacks e.g.
they are error prone or will not scale.
in response to these shortcom ings this study will take two directions first we will try a label free approach using a plain unsupervised learning technique to label the data if the label free approach fails then we try tuned semisupervised learning called frugal which optimizes theunsupervised learner s configurations in the grid searchmanner while validating the results on only .
of thelabelled data.
a. manual labelling in manual labelling a team of e.g.
graduate students assigns labels then a cross checks their work via say a kappastatistic then b use some skilled third person to resolve anylabelling disagreements .
manual labelling can be very slow.
tu et al.
recently studies a corpus of github projects .
a random selectionof projects from that corpus had commits which took hours includes cross checking to manually labelthe commits buggy non buggy.
that is manual labelling of those projects would have required weeks of work.
b. crowdsourcing tu et al.
offers a cost estimate of what resources would be required to sub contract that effort to dozens of crowdsourced workers via tools like mechanical turk mt .applying best practices in crowdsourcing assuming a atleast usa minimum wages and b our universitytaking a overhead tax on grants then crowd sourcingthe labelling of the issues from projects would require of grant reserve.
c. reusing labels since manual labelling is time consuming and crowdsourcing is too expensive researchers often reuse labels from previous studies.
e.g.
for defect prediction researchers certified their methods using data generated by kamei et al.
.
this approach fails in two cases.
firstly when exploringa new domain there may be no relevant old labels to reuse.secondly reusing labels means incorrectly labelled examplescan contamiante other research.
for example y u et al.
were exploring self admitted technical debt and found thattheir classifiers had an alarming high false positive rate.
butwhen they manually checked the labels of their data which isfrom a prior study by maldonado et al.
they found thatover of the reused false positive labels were incorrect.
d. automatic labelling if labels cannot be generated manually or reused from other papers using automatic labelling processes is an attractive alternative.
for example defect prediction papers can label a commit as bug fixing when thecommit text contains certain keywords e.g.
bug fix wrong error fail etc .
v asilescu et al.
notedthat these keywords are used in a somewhat ad hoc manner researchers peek at a few results then tinker with regularexpressions that combine these keywords .
tu et al.
hadfound that these simplistic keyword approaches can introducemany errors perhaps due to the specialization of the projectnature or the ad hoc nature of their creation.
in technicaldebts identification y u et al.
proposed a pattern based methodthat automatically identified of self admitted techni cal debts sa tds by finding patterns associated with highprecision from the labelled training sets close to .this approach does need extensively labelled training data tofind quality patterns that are associated with technical debtbecause it relies on precision.
another automatic approach isml which involves supervised learning models to train onexisting labelled datasets to learn the underlying rules of thedata.
however this also requires having access to a substantialamount of labelled data especially for deep learners whichis not always available in new domains e.g.
the success ofopen source projects .
e. active learning a third approach is to a only label a representative sample of the data then b build a classifier from that sample then c use that classifier to label the remaining data .
to findthat representative example some unsupervised learners likean associations rule learner or a clustering algorithm or aninstance selection algorithm is used to find repeated patternsin the data .
then a human oracle is asked to label justone exemplar from each pattern.
more sophisticated versionsof this scheme include active learners where an ai tool rushes ahead of the human to fetch the most information nextexamples to be labelled .
if humans first label mostinformative examples then better models can be built faster.this means in turn that humans have to label fewer examples.
the more general term for active learning issemisupervised learning.
both terms mean do what you can witha small sample of the labels while active learning adds a 398table iii differences between frugal and zhang et al.
frugal this paper zhang et al.
core assumptionapplies se domainknowledge i.e.higher complexityis associated withtarget instances whichwe measure as beingabovec applies graph the ory i.e.
continuityand clustering simi lar things have simi lar properties .
hyperparameter ye s no optimization classrebalancing no y es with the lapla cian score samplingstrategy feedback loop that checks new labels one at a time with some oracle.
semi supervised learning relies on partially labelleddata and mostly unlabelled data.
since active learning approaches have been received scarce attention in se .
initially it seems to bea promising method for addressing the cost of label checkingand generating.
for self admitted technical debt identification only on the median of the training corpus had to belabelled also using active learning effort estimation fornprojects only needed labels on of those projects further while seeking of the vulnerabilities in 750mozilla firefox c and c source code files humans only hadto inspect of the code .
that said after much work itmust be reported that active learning still produces disappoint ing results.
it is still daunting to only label say to .
of the projects in the projects in reporeapers or the .
million links explored by hata et al.
.
also consider the firefox study mentioned in the last paragraph.the human effort of inspecting sourcecode files needed to find identify of the vulnerabilities it is beyond the resources of most analysts but it might bejustified for mission critical projects .
finally for defect pre diction zhang et al.
proposed nsglp and certified theirmethod by varying the size of labeled software modules from10 of all the nasa datasets.
the differences between ourapproach and their are listed in the table iii.
they claimedthat the proposed method outperformed several representativesota semi supervised ones for software defect prediction.however reproducing that paper is complex since it waswritten before the current focus on research paper artifacts so that paper has no reproduction package .
moreover recentand widely cited studies argue that the datasets used in thatanalysis are of dubious quality .
it is opportune that in this adoptable static code warnings identification and issue close time prediction work we aim toreduce the reviewing cost of these labelling methods by twomethods label free approach with unsupervised learning or tuned semi supervised learning to optimize the unsuper vised learner s configurations in the grid search manner whilevalidating the results on a small amount of the labelled data i.e.
.
.algorithm pseudocode of frugal input train data tune data test data output result 1percentiles range 2methods 3best result 4best model none 5form in methods do forc in percentiles do model m c .fit train data temp result model.predict tune data ifisbetter temp result best result then best result temp result best model model 12return best model.predict test data iv .
m ethodology a. general framework our approach shown in algorithm extends unsupervised learning with some semi supervised learning and tuning.
namet al.
s cla is the sota unsupervised learner for defectprediction which is also confirmed by xu et al.
s large scale study.
as shown in figure cla consists of threemodes cla cla ml and clafi ml.
this study shalladopt and extend cla with tuning in the grid search mannerof three modes of cla while varying the c percentile parameter.
simply as illustrated in algorithm frugalfinds the best combination of unsupervised learners cla cla ml clafi ml andc to95 increments by5 .
the author only proposed cla and clafi ml but cla ml is a natural medium that can be useful during thetuning process.
we explain the details of our approach in .b .c and .d.
b. cla in the sota comparative study of unsupervised models in defect prediction cla starts with two steps of c lustering the instances and labelling those instances accordingly to the cluster.
in the setting with no train data available wecan label or predict all new test instances as shown in the first block of figure .
clustering find the median of feature f1 f2 ... f n percentile fi c where c across the whole dataset.
for each data instance xi go through each feature value of the respective data instance to count the time when thefeaturef i p e r c e n t i l e fi c aski.
labelling label the instance xias the positive class if ki median k else label it as the negative class.
the intuition of such methods is based on the defect proneness tendency that is often found in defect prediction research that is the higher complexity is associated with the proneness of the defects .
simply there is a tendency where the problematic instance s feature values are higher than the non problematic ones.
this tendency and cla s clafi ml s 399effectivenesses are confirmed via the recent literature and comparative study of unsupervised models in defect predictionacross datasets and three types of features by xu et al.
.they found cla s clafi ml s performances are superiorto other unsupervised methods while similar to supervisedlearning approaches.
therefore this study investigated andfound that the hypothesized tendency is also applicable inissues close time prediction and adoptable static code warningidentification data but not with cat the median c .
this opens opportunities for hyperparameter tuning.
c. cla ml if there is an abundant train data in the wild but without labels cla can pseudo label the train data before applying any machine learner in the supervised manner as shown inthe second block in figure .
for this step we take nam and kim s advice to incorporate random forest rf described in .e.
an ensemble of tree learners method as themachine learner of choice.
d. clafi ml clafi is an extension of cla which is a fullstack framework that also include f eatures selection and instances selection.
the setting is similar to cla ml as shown in the third block of figure the pseudo labelled train data from cla and unlabelled test data will be processed with fiandf respectively.
finally machine learner can train the processed pseudo labelled train data and then predict on the processedtest data.
feature selection calculate the violation score per feature called metric in the original proposal of nam et al.
.
the process is done on both the train and the test dataset.
for each fi go through all instances of xj a violation happens when fiatxjis higher than the percentile ki c wherec butyj and vice versa.
sum all the violations per feature across the whole dataset and sort it in ascending order.
select the feature with the lowest violation score if multiple of them have the same score then pick all of them.
instance selection with the selected features go through each instance xiand check if the respective fjvalues violated the proneness assumption then remove that instance xi.
if the dataset do not have instances with both classes at the end then pick the next minimum violation score to selectmetrics.
this process is only done on the train dataset.
after selecting features with the minimum violation scores and removing the instances that violated the proneness ten dency a practitioner can train an rf model on the processedtrain data to identify the target classes from the processed testdataset.
e. machine learning models random forest rf is an ensemble learning method that operates by constructing a multitude of decision trees each time with different subsets of the data rows rand columnsc .
each decision tree is recursively built to find the features that reduce most of entropy where a higher entropy indicates less ability to draw conclusions from the data being processed .
test data is then passed across all ntrees and the conclusions are determined say a majority vote across allthe trees .
holistically rf is based on bagging bootstrapaggregation which averages the results over many decisiontrees from sub samples reducing variance .
support vector machine svm is a classifier defined by a separating hyperplane .
soft margin linear svmsare commonly used in text classification given the high di mensionality of the feature space.
this was recommended byy ang et al.
as the state of the art for our adoptable staticcode warning identification domain.
a soft margin linear svmlooks for the decision hyperplane that maximizes the marginbetween training data of two classes while minimizing thetraining error hinge loss min bardblw bardbl bracketleftbigg nn summationdisplay i 1max yi w xi b bracketrightbigg where the class of xis predicted as sgn w x b .
both svm and rf are popular in the field of ml and implemented in the popular open source toolkit scikit learnby .
feedforward neural networks is the first and simplest technology devised from artificial neural network .
theinformation moves in the forward direction only startingfrom the input nodes through the hidden nodes and to theoutput nodes.
at each node of these networks the inputsare multiplied with weights that are learned and then anactivation function is applied.
the weights are learned bythe backpropagation algorithm .
this uses just a fewlayers while the deep learners use many layers.
also theolder methods use a threshold function at each node whilefeedforward networks typically use the rectified linear unitfunction of f x m a x x .
this is the base learner for our second domain s state of the art where y edida et al.
proposed a framework combining different preprocessorsand different configurations of the simple feedforward neuralnetwork.
v. r esults in order to make sure our proposed method s effectiveness is not affected by the bias between deterministic and non deterministic models or the bias of uncertainty we randomlyshuffle train test sets and incorporate stratified sampling withfive bins ensuring that the class distribution of the wholedata is replicated in each bin .
the process is repeated for thetrain data but also includes an extra .
validating partitionfor each .
tuning partition.
the median .
of labelledtrain data for static warning analysis and issue close time are36 and respectively.
during the simulation the tunepartition will not review labels for our unsupervised learningand semi supervised learning candidates.
frugal does have 4specifically using log2cof the columns selected at random.
400auc in actionable static warning identification.
medium effect or m accuracy in issues close time prediction.
medium effect or m fig.
rq1 results on two domains with frugal l .
.
frugal with l perform similarly.
however for adoptable static warning identification frugal l .
does perform the best across datasets except in lucene and cass the highlighted ones where frugal l outperforms frugal l .
.
access to the corresponding .
labelled validation partitionwhile deciding on the best configurations.
for each ofthe test data the process learns a model on five stratifiedsamples of the train data.
this process is done for bothdomains in this paper.
rq1 how much labelled data l that frugal requires?
our hypothesis is there are few key data regions where extra data would lead to indistinguishable results .
we test thedifferent amounts of the train data s labels that are requiredfor frugal s performance to plateaus.
let lbe .
.
or figure reports frugal s performanceon both adoptable static warning identification in auc andissues close time prediction in accuracy .
both metrics arederived from the sota s evaluation metrics.
specifically wepick auc as the representation metric for adoptable staticwarning analysis since auc measures the area under the curvewhereas other metrics only calculate a single point on thecurve.
from figure the lower bound for both domains is l .
for adoptable static warning identification frugal s per formance improves initially and plateaus beyond l .
across datasets.
frugal l .
loses to frugal l in only lucene and cass projects but reduces .
times the labelling efforts.
for issues close time prediction frugal surpris ingly performs statistically similar across all l .
.
.
the same effect is absent in issue close time prediction this is highly likely due to the balanced nature of the data sclass distribution.
however the data in static warning analysisis more imbalanced with a median of for the adoptablestatic warning class ratio .
this is consistent with the moti vations for oversampling and undersampling techniques forimbalanced data .
from our investigation of various lvalues frugal s performance plateaus beyond l .
and frugal s success is not altered by large changes to l.in summary our answer to rq1 is rq2 how does frugal perform in adoptable static warnings identification?
wang et al.
proposed the golden set features along with the ml study where they employed rf decision tree and svm with the rbf kernel with median auc perfor mances at and .
y ang et al.
extensivelyinvestigated different deep learners dnn cnn rf decisiontree and svm that pushed wang et al.
s results to new higherwatermarks in the area with median auc performances at99.
.
and .
with almost and relative improvements to the same learner choices as wanget al.
.
the default parameters in weka used by wanget al.
are different to those used in scikit learn usedby y ang et al.
.
for instance wang et al.
s svm usedrbf kernel while y ang et al.
s svm used linear kernel.
y ang et al.
proposed the standard linear svm as the sota s adoptable static warning identifier.
table iv reportsthe comparison of our proposed method frugal the sota ssvm with labelled data and .
labelled data andthe baseline unsupervised learners cla clafi rf acrossfar recall precision f1 and auc.
in those results standard unsupervised learner cla clafi rf performsthe worst as their default behavior is clustering based on themedian of the data which may not apply for all the data andespecially in the static warning analysis.
however cla srecalls are almost in a few cases mvn cass and commons and clafi rf s far are almost in morethan half cases derby lucene cass jmeter and tomcat .
this indicates promising areas for tuning configurations ofunsupervised learners.
the sota work originally evaluated their method on far recall and auc.
with the same comparison frugalperforms better than the sota s svm as frugal winsin recall and far while losing in auc.
however whenconsidering precision and f1 frugal underperforms.recalling our cautionary note from the end of ii c1 precision and hence f1 can be misleading for data setswhere the target class is rare e.g.
as shown in table i the median of target class is .
therefore overall wesay frugal performs similarly to the sota s svm.
401table iv comparison between cla svm and frugal in terms of far recall precision f1 and auc for identifying adoptable static warning.
in this table the frugal results were found after labelling just .
of thedata.
except for far the higher the results the better the performance of the treatment.
medians and iqrs deltabetween 75th and 25th percentile lower the better are calculated for easy comparisons.
here the highlighted cells show best performing treatments.
metrics treatment derby mvn lucene phoenix cass jmeter tomcat ant commons median iqr cla .
.
.
.
.
.
.
.
far clafi rf .
.
.
.
.
.
.
.
.
.
.
m frugal .
.
.
.
.
.
.
.
.
.
svm l .
.
.
.
.
.
.
.
.
.
svm .
.
.
.
.
.
.
.
.
.
.
cla .
.
.
.
.
.
.
.
recall clafi rf .
.
.
.
.
.
.
.
.
.
.
m frugal .
.
.
.
.
.
.
.
svm l .
.
.
.
.
.
.
.
.
svm .
.
.
.
.
.
.
.
.
cla .
.
.
.
.
.
.
.
.
.
.
precision clafi rf .
.
.
.
.
.
.
.
m frugal .
.
.
.
.
.
.
.
.
.
svm l .
.
.
.
.
svm .
.
.
.
.
.
.
cla .
.
.
.
.
.
.
.
.
.
f1 clafi rf .
.
.
.
.
.
.
.
.
.
.
m frugal .
.
.
.
.
.
.
.
.
.
.
svm l .
.
.
.
.
.
.
svm .
.
.
.
.
.
.
.
cla .
.
.
.
.
.
.
.
.
.
auc clafi rf .
.
.
.
.
.
.
.
.
m frugal .
.
.
.
.
.
.
.
.
svm l .
.
.
.
.
.
.
.
svm .
.
.
.
.
.
.
.
.
.
the sota work that was trained on only .
labelleddata i.e.
svm with l .
underperforms both the sota work with labelled data and frugal.this illustrates that frugal s effectiveness is not due torandom sampling of the data.
in term of labelling efforts cla is label free frugalcosts .
and y ang et al.
s method costs becausefrugal and the sota require .
and of thedata to be labelled.
frugal improves significantly from standard unsu pervised learner cla with .
of data labelled asa tradeoff while performing similarly to the sotawith .
fewer information.
a simple method thatexplores small regions of data does no worse thanmethods that extensively learn the whole space.in summary our answer to rq2 is rq3 how well does frugal predict issue close time?
mark et al.
proposed deeptriage as sota deep learning solution extended from bidirectional lstms with an attention mechanism to predict issue close time.
a long short termmemory lstm is a recurrent neural network withan additional gate mechanisms to allow the network tomodel connections between long distance tokens in the input.bidirectional variants of recurrent models such as lstms use token stream in both forward and backward directions allowing for the network to model both previous and followingcontexts for each input token.
attention mechanisms uselearned weights to help the network pay attention to tokensthat are more important than others in a context.table v comparison between cla simple and frugal in terms of accuracy far recall and aucfor predicting issue close time.
note that in this table thefrugal results were obtained after labelling just .
ofthe data.
except for far the higher the results the betterthe performance of the treatment.
medians and iqrs delta between 75th and 25th percentile lower the better are calculated for easy comparisons.
here the highlighted cells show best performing treatments.
metrics treatment chromium firefox eclipse median iqr cla .
.
.
.
accuracy clafi rf .
.
.
.
.
m frugal .
.
.
.
.
simple .
.
.
.
cla .
.
.
.
.
far clafi rf .
.
.
m frugal .
.
simple .
.
.
.
.
cla .
.
.
.
recall clafi rf .
.
.
.
.
m frugal .
.
.
.
simple .
.
.
.
cla .
.
.
.
precision clafi rf .
.
.
.
.
m frugal .
.
.
.
.
simple .
.
.
cla .
.
.
.
.
f1 clafi rf .
.
m frugal .
.
.
simple .
.
.
.
.
cla .
.
.
.
.
auc clafi rf .
.
.
.
.
m frugal .
.
.
.
.
simple .
.
.
.
y edida et al.
s simple extended basic 1980s style feedforward neural network with state of the art se s opti402mizer dodge to automatically select the preprocessors normalizer binarizer etc and the neural network model shyperparamters num layers num units inlayer batch size .
simple outperformed deeptriage and other non neural net work methods from marks et al.
and guo et al.
.
simple is employed as the sota solution for predicting issue close time.
y edida et al.
only compared solutionsby the accuracy metric.
in order to ensure the generalizabilityof our proposed solution we also compared different methodswith metrics from static warning analysis in rq2 far recall and auc .
hence table v reports the comparison of ourproposed method s frugal the sota s simple and thebaseline unsupervised learners cla clafi rf acrossaccuracy far recall and auc.
we observe the unsupervised learners cla clafi rf performedworst as it s default behavior uses clustering based on themedian of the data which may not apply for all data .while clafi rf performed better than cla in static codewarnings that effect was not seen here i.e.
what works forone domain may not work for another .
additionally onaverage cla underperformed simple by approximately1 and in far precision f1 recall auc and accuracy without access to the train data.altogether both points indicate promising areas for tuningconfigurations of unsupervised learners.
frugal outperforms the sota s simple as frugalwins in recall precision f1 auc and far while draw ing in accuracy.
frugal on average improves relativesota s precision auc f1 and recall by and respectively while reducing far by rela tively.
in term of labelling efforts cla is label free frugalcosts .
and the y edida et al.
s method costs because frugal and the sota need .
and ofthe data labelled to execute.
frugal exceeds both standard unsupervised learnercla and the sota simple emse which outperformed a decade of research includingicse promise msr co mad in predicting issues close time.
fru gal requires only .
of the train data to be labelledwhen being compared against unsupervised learningwhile using .
less information than the sotatuned deep learning method.
hence frugal is notonly effective in static warning analysis but also inissue close time prediction.
the success in both areaslet this study hypothesizes that other areas of se mayalso benefit from frugal.in summary our answer to rq3 is vi.
t hrea ts of validity there are several validity threats to the design of this study.
any conclusion made from this work must beconsidered with the following issues in mind conclusion validity focuses on the significance of the treatment.
to enhance conclusion validity we run experimentson different target projects across stratified sampling 25runs and find that our proposed method always performedbetter than the state of the art approaches.
more importantly we apply a similar statistical testing of cohen d as the sotawork from the two domains to obtain fair compar ison.
in addition we have taken into generalization issuesof single evaluation metrics e.g.
recall and precision intoconsideration and instead evaluate our methods on metrics thataggregate multiple metrics like auc while being effort awarevia cost.
as future work we plan to test the proposed methodswith additional analyses that are endorsed within se literature e.g.
p opt20 or general ml literature e.g.
mcc .
one of the possible explanations for the simple effectiveness of both binary split of the output space cla clafi ml frugal s centrality and .
labelled traindata requirement is highly due to the intrinsic dimensionality.levina et al.
argued that datasets embedded in high dimensional spaces can be compressed without significant in formation loss similar to the pca .
to compute levina sintrinsic dimensionality a d plot is created where the x axisshowsr i.e.
the radius of two configurations while the y axis showsc r as the number of configurations after spreading out some distance raway from any of ndata instances y c r n n n summationdisplay i 1n summationdisplay j i 1i the maximum slope of lnc r vs.lnris then reported as the intrinsic dimensionality d. note that i is the indicator function i.e.
i ifxis true otherwise it is xiis theith sample in the dataset.
applying this calculation to the datasets of two domains reports in table vi we foundthe intrinsic or latent dimensionality d of our data is very low median around one no more than three .
agrawal et al.
sdodge is the sota optimizer for se dodge executesby binary splitting the tuning space each chop moves in thebounds for numeric choices by half the distance from most dis tant value to the value that produced the best performance.according to agrawal et al.
dodge s effectiveness rootsin how the performance score generated from se data canbe divided into a few regions low dimensional .
frugal scentral function of binary splitting is similar to dodgeas frugal compresses the data dimensions features viaaggregated percentile cand survey the whole space by varying c to95 increments by .
menzies et al.
and hindle et al.
also reported on how several se data arelow dimensional and the benefits from building effective toolsfrom such data.
this work extends those findings the labellingefforts to commission to tools building can be reduced greatlybecause of the low dimensionality of se data.
internal validity focuses on how sure we can be that the treatment caused the outcome.
to enhance internal validity weheavily constrained our experiments to the same dataset withthe same settings except for the treatments being compared.
403table vi summary of intrinsic dimensions d of this study s datasets from levina and bickel .
static code warnings issue close timederby mvn lucene phoenix cass jmeter tomcat ant commons chromium firefox eclipse d0.
.
.
.
.
.
.
.
.
.
.
.
construct validity focuses on the relation between the theory behind the experiment and the observation.
to enhance construct validity we compared solutions with and without ourstrategies in table iv and v while showing that both compo nents unsupervised learning with cla clafi ml andtuned semi supervised method of frugal and in variousamounts of labelled data required for the proposed mehtod toimprove the overall performance.
moreover we also bench marked our solution with the sota s solution that is trainedon the same l .
to ensure that our proposed solution s effectiveness is not due to random sampling of the data.however we only show that with our default parameterssettings of random forest learner.
the performance can geteven better by tuning the parameters employing differentlearners e.g.
deep learners and introducing a variety ofdata preprocessors e.g.
synthetic minority over sampling orsmote that is known to help with imbalanced datasets like our static code warnings domains .
we aim to explorethese in our future work.
external validity concerns how widely our conclusions can be applied.
in order to test the generalizability of ourapproach we always kept a project as the holdout test setand never used any information from it in training.
moreover we have validated our proposed method on two importantsoftware analytics domains adoptable static code warningsidentification and issues close time prediction.
our experi ments with default cla clafi ml demonstrates thedanger of treating all data with the state of the art method especially when switching domain from defect prediction toissue close time prediction and adoptable static code warningidentification .
vii.
c onclusion and future work there is much recent advance for software analytics research with automated and semi automated methods.
however these methods are built on a sufficiently large amount ofdata labelled.
generating such labels can be labor intensiveand expensive as discussed in iii .
such requirement canintroduce barrier for entering new research domains e.g.
the success of open source projects .
in order to reduce thelabel famine and human effort frugal is recommended.frugal tunes the state of the art unsupervised learner fromdefect prediction cla cla ml clafi ml and it s cor responding percentile parameter cin the grid search manner while validating on only .
of the labelled data.
ourfindings include unsupervised learners without access to the train data s labels performed approximately less than the sotamethods on average.
the results are promising but still noteffective enough.
frugal performed similarly to the sota adoptable static code warning identifier while surpassing the sota issueclose time predictor with .
less information.
frugal reduced the labelling efforts needed for the software analytics tools by .
.
simply frugal is40 .
times cheaper than the sota methodsin issue close time and static code warnings analysis areas.
the success of frugal for the two domains here suggests that many more domains in software analytics could benefitfrom unsupervised learning.
as mentioned above thosebenefits include the ability to commission new modelswith less human efforts and costs.
by restricting humaninvolvement in the process we also reduced erroneouslabels that can cascade to the whole research communitysince human are still error prone y u et al.
found of the false positive labels within maldonado and shihab were actually true positive labels .
overall our proposed method restated the benefit in exploring low dimensional se data and extendedtheir findings that the labelling efforts can be reducedgreatly because of the low dimensionality of se data.
that said frugal still suffers from the validity threats discussed in vi.
to reduce those threats and to further thisresearch we propose the following future work comparing frugal to other semi supervised learningmethods within software engineering e.g.
nsglp .
test whether replacing the random forest model in fru gal with a deep learning model will further improve itsperformance.
explore non se or high dimensional se data with fru gal to see if our current conclusions still hold.
apply non trivial hyper parameter tuning e.g.
dodge or flash on various data preprocessors and machinelearners with frugal to test whether tuning can furtherimprove the performance .
extend the work to other software engineering domains e.g.
security technical debts software configu rations etc and compare it with other state of the artmethods which continue to appear.
a cknowledgements this work was partially funded by an nsf cise grant .