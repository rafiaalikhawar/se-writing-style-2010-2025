mudelta delta oriented mutation testing at commit time wei ma thierry titcheu chekam mike papadakis and mark harmany snt university of luxembourg luxembourg yfacebook and university college london uk ffirstname.surnameg uni.lu ymark.harman ucl.ac.uk abstract to effectively test program changes using mutation testing one needs to use mutants that are relevant to the altered program behaviours.
we introduce mudelta an approach that identifies commit relevant mutants mutants that affect and are affected by the changed program behaviours.
our approach uses machine learning applied on a combined scheme of graph and vector based representations of static code features.
our results from commits in coreutils programs demonstrate a strong prediction ability of our approach yielding .
roc and .
pr curve auc values with .
and .
precision and recall values.
these predictions are significantly higher than random guesses .
pr curve auc .
and .
precision and recall and subsequently lead to strong relevant tests that kill more relevant mutants than randomly sampled mutants either sampled from those residing on the changed component s or from the changed lines .
our results also show that mudelta selects mutants with higher fault revealing ability in fault introducing commits.
taken together our results corroborate the conclusion that commit based mutation testing is suitable and promising for evolving software.
index terms mutation testing commit relevant mutants continuous integration regression testing machine learning i. i ntroduction mutation testing has been shown to be one of the strongest fault revealing software test adequacy criteria available to software testers .
nevertheless although mutation testing has been widely studied for over four decades in the scientific literature the formulation that underpins it has remained largely unchanged since its inception in the 1970s .
in this unchanged formulation a program pis tested by a test suite t the adequacy of which is measured in terms of its ability to distinguish executions of pand a set of mutants m. each mutant in mis a version of pinto which a fault has been deliberately inserted in order to simulate potential real faults thereby assessing the ability of the test suite tto detect such faults.
the problem with this formulation is that it has not kept pace with recent software engineering practices.
most notably the assumption of a fixed program p set of mutants m and test suite t is unrealistic modern software systems undergo regular change typically in continuous integration environments .
in order to render mutation testing applicable to practising software engineers a fundamentally new approach to finding suitable mutants is required in which p t andmare each continually evolving.specifically we need a mutation testing formulation in which mutants can be found on the fly based on their relevance to specific changes to the system under consideration.
in this evolving mutation testing approach both the set of mutantsmand the tests that distinguished their behaviours t are each able to change with each new commit.
such a mutation testing formulation is better suited to industrial practice e.g.
at google since mutation testing can be applied at commit time to each code change as it is submitted thereby keeping pace with the changes to p. more importantly such an approach will focus the test effort deployed at commit time specifically to the changes in the commit rather than wasting test effort on re testing old code.
in order to apply mutation testing on the fly in this manner we need a fast lightweight approach to determine a priority ordering on a given set of mutants where priority is determined by the relevance of a mutant to the change in hand.
this paper introduces a machine learning based approach to tackle this problem using a combined scheme of graph and vector based representations of simple code features that aim at capturing the information control and data flow and interactions between mutants and committed code changes.
we train the learner on a set of mutants from historical code changes that are labeled with respect to given test suites.
the machine learner is subsequently used to predict the priority ordering of the set of mutants to identify those most likely to be relevant to a given change.
this way once the learner has been trained it can be used to quickly predict the priority order for the set of mutants in terms of their relevance to unseen changes as they are submitted into the continuous integration system for review.
this allows the tester and or some automated test design technology to focus on those mutants that are most likely to yield tests that are fault revealing for the change in hand.
we implemented our approach in a system called mudelta and evaluated it on a set of commits from coreutils wrt a prediction ability b ability to lead to relevant tests tests killing commit relevant mutants and c ability to reveal faults in fault introducing commits.
our results indicate s strong prediction ability mudelta yields .
roc auc value .
f1 score .
precision and .
recall while random guesses yield .
f1 score .
precision and .
recall.
killing the predicted mutants results in killing more relevant mutants than random mutant sampling baselines.
ieee acm 43rd international conference on software engineering icse .
ieee perhaps more importantly our results show that our approach leads to mutants with higher fault revealing ability in fault introducing commits.
taken together our results corroborate the findings that mudelta enables effective deltarelevant mutation testing i.e.
mutation testing targeting the specific code changes of the software system under test.
our study also reveals some surprising findings additional results and discussion can be found in our project page1.
for example one might believe that mutants that reside in the changed code would be adequate in testing it.
however our empirical findings contradict this natural but incorrect assumption.
this surprising finding highlights the importance of finding mutants in the unchanged part of the program.
this unchanged code that forms a contextual environment into which changes deployed.
such relevant mutants in the contextc for some change tend to focus on and reveal issues with interactions between the change and the context cinto which it is deployed.
developers are less likely to notice these since they are more likely to be familiar with their changes than the existing unchanged code.
such bugs may also be more subtle as they involve unforeseen interactions between parts of the system.
in summary our primary contributions are the empirical evidence that mutant relevance to particular program changes can be captured by simple static source code metrics.
a machine learning approach called mudelta that learns to rank mutants wrt to their utility and relevance to specific code changes.
empirical evidence suggesting that mudelta outperforms the traditionally random mutant selection prioritization method by revealing more relevant mutants and achieving higher probability to reveal faults in these changes.
ii.
c ontext a. mutation testing mutation testing measures the fault revealing potential of test cases by checking the extend to which artificially seeded faults called mutants are triggered.
when a behaviour difference between the original program and a mutant is detected the mutant is considered to be killed otherwise the mutant is considered to be live .
the point here is that mutant killing shows an execution failure that was covered triggered by the altered code and propagated to the observable program output signifying the potential of the test.
the faults are seeded in the code under analysis by making simple syntactic transformations e.g.
replacing the instance of an operator with another one if a b intoif a b and they represent the test requirements.
this means that the ratio of mutants killed called mutation score ms represents the test thoroughness metric.
faults by altering the syntax of the program may result in semantically equivalent program versions i.e.
versions that behave the same way for all possible inputs.
these equivalent versions need to be removed and not taken into consideration as even a perfect test suite cannot kill them.
unfortunately equivalent mutants form one of the known problems of mutation testing .
interestingly many killable mutants are equivalent to others introducing an additional problem skew in the mutation score .
the problem though is more severe since not all mutants are equally important many mutants are killed collaterally and thus they do not contribute to the testing process .
unfortunately these collateral kills inflate the mutation score measurement and may lead to wrong conclusions .
therefore the recent mutation testing literature suggest using the so called subsuming mutants computing the subsuming mutation score when evaluating test effectiveness.
b. change aware regression testing testing program regressions require test suites to exercise the adequacy of testing wrt to the program changes.
in case the used test suites are insufficient guidance should be given in order to help developers create test cases that specifically target the behaviour deviations introduced by the regressions.
one potential solution to this problem may be based on coverage one can aim at testing the altered parts of the programs using coverage information.
however the strengths of coverage are known to be limited .
moreover the most severe regression issues are due to unforeseen interactions between the changed code and the rest of the program .
therefore we aim at using mutation testing using the so called commit relevant mutants .
c. commit relevant mutants commit relevant mutants are those that make observable any interaction between the altered code and the rest of the program under test.
these mutants alter the program semantics that are relevant to the committed changes i.e.
they have behavioural effects on the altered code behaviour.
this means that mutants are relevant to a commit when their behaviour is changed by the regression changes.
indeed changed behaviour indicates a coupling between mutants and regressions suggesting relevance.
in essence one can use relevant mutants to capture the observable dependencies between changed and unchanged code which reflect the extent to which test suites are testing the altered program behaviours.
in particular mutants interact with program changes when the post commit mutant version includes both the changes and the mutant behaves differently from a the related pre commit mutant version and b the post commit non mutated version.
these conditions establish that changes and mutants interact .
condition a establishes that the behaviour differences are caused by the presence and absence of the committed changes and condition b that the behaviour differences are caused by the presence and absence of the mutants.
898fig.
.
overview of mudelta.
the learner is trained on a set of mutants from historical code changes that are labeled with respect to given test suites.
the machine learner is subsequently used to predict the priority ordering of the set of mutants to identify those most likely to be relevant to a given change.
the virtue of commit relevant mutation testing as described in the study of ma et al.
is the best effort application of mutation testing.
this gives the potential for improved fault revelation under the same relatively low user effort than using randomly sampled mutants i.e.
traditional mutation testing.
however in order to be useful these mutants need to be identified in advance prior to any mutant analysis performed.
this is because relevant mutants form the objectives that developers will analyse.
to achieve this we develop a machine learning approach which we describe in the following section.
figure presents a commit relevant mutant on a faultintroducing commit of gnu coreutils2.
this is the commit with id 8from corebench .
the commit affects two functions of the program seq main and seqfast .
the entry point is the function main which calls the functions print numbers andseqfastto compute and print the results.
the function seqfastis an optimized implementation of the function print numbers used only when the inputs meet specific conditions.
in figure the line 543checks the condition to call seqfast.
if the condition is satisfied seqfastis called.
otherwise print numbers is called.
note that print numbers may be called after seqfastif the later fails the condition at line405is not satisfied i.e.
a b .
in that case the execution ofseqfastdoes not alter the program state or output.
the commit aims at relaxing the condition that guards the call to function seqfast.
in the pre commit version seqfast is not called when the user specifies a separator.
however in the post commit version seqfast is called whenever a the user specifies a separator and b the separator string has a single character.
in the function seqfast the commit only replaces the hard coded separator nn with separator s global string variable.
in the function main the commit relaxes the if condition at line in a way that seqfast is also called when the user specifies a separator which can be any single bits character it is not limited to nn .
program seqcalls seqfastto print all the integers from the first parameter ato the second parameter b and using a given character first character of separator in post commit and nn in pre commit to separate the printed numbers.
let four mutants such that mutant m1deletes the statement at line which prints the first number using puts.
mutant m2deletes the modified statement at line which add the separator to the buffer to print.
mutant m3swaps the operands of the last operation at the modified line .
mutant m4replaces the exit value at line 595by .
we observe that m4is not relevant to the commit.
in fact there is no test that can kill m4in the post commit version and create an output difference between pre and post commit versions ofm4.
if a test kills post commit m4 it must avoid executing line thus seqfastis either not called or its call does not succeed does not print anything .
thus the output of the execution of the pre and post commit versions of m4with such test will be same both computed with print numbers which is not altered by the commit .
mutant m3is equivalent because no clause has side effect that is controlled by another clause in the ifcondition.
however m1is relevant to the commit.
an execution of the test seq s which sets the separator to the comma outputs 2nn in pre commit m1 print numbers is called in post commit m1 puts p is deleted and seqfast is called and 1nn2 in the post commit original version the first number is printed using puts p which appends an nn .
similarly m2is relevant to the commit.
the execution of same test seq s outputs 2nn in pre commit m2 print numbers is called 1nn2 in post commit m1 no comma separator printed and seqfastis called .
moreover a fault introduced by the commit makes the program use nn instead of the user specified separator after printing the first number when the user separator is a single character other than nn .
this happens because in such scenario the program calls seqfast which calls puts p line to print the first number.
this automatically add an extra nn and do not use the specified separator.
899static bool seq fast char const a char const b ... bool ok cmp p p len q q len if ok ... puts p mutant m1 delete statement ... incr p p len z mempcpy z p p len z n z separator if buf end n z fwrite buf z buf stdout z buf ... ... return ok intmain intargc char argv ... if ... all digits p argv ... if all digits p argv ... strlen separator ... if seq fast s1 s2 exit exit success ... print numbers format str layout first.value ... exit exit success mutant m4 exit success mutants m1 and m2 are relevant.
moreover they are fault revealing of the tests killing them find the introduced fault .
mutants m3 and m4 are not relevant m3 is equivalent .mutant m2 delete statement mutant m3 swap operands of .
fig.
.
mutation testing in a fault introducing commit.
the fault is triggered by the call to puts p which automatically uses nn as the first separator resulting in not using the user specified separator when this is a single character other than nn .
this makes every test executing seqfast with a separator other than nn to reveal the fault.
killing m1orm2can result in such tests while killing m4does not to kill m4a test must avoid executing line547 which means that seqfastis either not called or its call does not print anything hence not making any observable difference .
m3is equivalent.
every test that executes seqfast with a separator other than nn reveal the fault.
these are of all the tests that successfully execute seqfast.
the reason is that the separator is either not set in the test defaults to nn or set to one of the bits characters including nn .
we observe that all tests that successfully execute seqfastkillm1andm2.
therefore of the tests that kill m1andm2reveal the fault.
iii.
a pproach we aim at testing commits using commit relevant mutants the subset of mutants on the post commit program version that has a behaviour relevance to the committed changes .
we develop mudelta a technique that learns to rank mutants according to their commit relevance potential likelihood to be commit relevant .
initially mudelta applies supervised learning on a mutant corpus from past data and builds a prediction model.
this model is then applied to predict the mutants that should be used to test the future commits of the program under test.
this means that at commit time testers can use and focus only on the most relevant mutants.
this process is depicted in figure .a.mudelta feature engineering the mutant selection process in mudelta is based on training of a predictor that is capable of identifying whether a mutant is commit relevant with a certain confidence probability .
consequently we design a set of features to reflect specific code properties which may discriminate a commitrelevant mutant from another.
the study of chekam et al.
found that fault revealing and killability mutant characteristics can be captured by simple code features.
therefore we consider the features that they proposed in our machine learning model.
unfortunately these features do not capture the interaction between mutants and the altered code.
hence we design additional features capable of capturing the link between the mutant and the altered code by the commit .
these features also aim at capturing the characteristics of the altered code.
in the following subsections we describe the features we use in order to train a classifier.
we consider a commit modification cassociated with code statements sc fsc1 sc2 s cng and letbc fbc1 bc2 b ckg the control flow graph cfg basic blocks associated to the statementssc.
let us also consider a mutant massociated to a code statement smon which the mutation was applied.
let bmbe the cfg basic block associated to a mutated statement smcontaining the mutated expression em.
b. contextual features in order to capture contextual information for each program statement within a program version we design features that leverage graph analysis technologies.
we construct graph representations of the program where the nodes are the statements of the program and the edges are various types of relationships between statements.
we consider the following four relationships edge types data dependency direct data dependency indirect data dependency control dependency and control flow.
direct data dependency refers to variable value dependency while indirect data dependency refers to pointer dereference value dependency the data is accessed through dereferencing a pointer .
in total we use the following different graph representations i.e.
utility graph ug that includes all four edge types we discussed dependency graph dg that includes all three dependency edges types direct data dependency graph dddg that includes only the direct data dependency edge type indirect data dependency graph iddg that includes only the indirect data dependency edge type control dependency graph cdg that includes only the control dependency edge type and control flow graph cfg which includes only the control flow edge type.
for each graph we leverage graph analysis algorithms to compute a score for each node.
we consider the following graph analysis algorithms rich club coefficient rcc clustering coefficient cc square clustering coefficient scc pagerank pr and hits analysis ha .
900complexity complexity of sm approximated by the number of mutants on sm.
cfgdepth depth of bmaccording to cfg.
cfgprednum number of predecessor basic blocks in cfg of bm.
cfgsuccnum number of successors basic blocks in cfg of bm.
astnumparents number of ast parents of em.
numoutdatadeps number of mutants on expressions data dependent on em.
numindatadeps number of mutants on expressions that emis data dependent.
numoutctrldeps number of mutants on statements control dependents on em.
numinctrldeps number of mutants on expressions that emis control dependent numtiedeps number of mutants on em.
astparentsnumoutdatadeps number of mutants on expressions data dependent onem s ast parent statement.
astparentsnumindatadeps number of mutants on expressions that em s ast parent expression is data dependent.
astparentsnumoutctrldeps number of mutants on statements controldependent on em s ast parent expression.
astparentsnuminctrldeps number of mutants on expressions that em s ast parent expression is control dependent.
astparentsnumtiedeps number of mutants on em s ast parent expression.
typeastparent expression type of ast parent expressions of em.
typemutant mutant type of m transformation rule.
e.g.
a b!a b. astchildhasidentifier ast child of expression emhas an identifier.
astchildhasliteral ast child of expression emhas a literal.
astchildhasoperator ast child of expression emhas an operator.
outdatadepnumstmtbb number of cfg basic blocks containing an expression data dependent on sm.
indatadepnumstmtbb number of cfg basic blocks containing an expression on which smis data dependent.
outctrldepnumstmtbb number of cfg basic blocks containing an expression control dependent on sm.
inctrldepnumstmtbb number of cfg basic blocks containing an expression on whichsmis control dependent.
astparentmutanttypenum number of each mutant type of em s ast parents.
outdatadepmutanttypenum number of each mutant type on expressions datadependents on em.
indatadepmutanttypenum number of each mutant type on expressions on which emis data dependent.
outctrldepmutanttypenum number of each mutant type on statements controldependents on em.
inctrldepmutanttypenum number of each mutant type on expressions on which emis control dependent.
fig.
.
mutant utility features .
overall we get a set of features fs for each statement s and for each graph g by computing the score of the node corresponding to s using all graph analysis algorithms on g. this gives us graphs metrics features per program statement.
c. mutant utility features we used the features proposed by chekam et al.
.
these features relate to the complexity of the mutated statement sm the position of smin the control flow graph the dependencies with other mutants and the nature of the code block bm wheresmis located.
the selected features are recorded in figure .
note that for this study we added the last features marked in the figure with italic and the contextual features ofsm section iii b .
the first features with italic are similar to the features numoutdatadeps numindatadeps numoutctrldeps numinctrldeps used by chekam et al.
but instead of the number of mutants they count the number of basic blocks.
d. mutant modification interaction features to capture the interaction between mutant and altered code we use features related to the information flow that the altered codecincur to the execution of mutant m. in this regard we propose features that characterize the altered code and features that capture the information flow between candm.numconditional number of conditional statements in the modification.
numhunks number of hunks blocks in the commit diff.
hasexit the modification involves program termination commands.
changescondition the modification involves the condition of an ifor a loop.
involesoutput the modification involves a function call to printf orerror .
isrefactoring the modification only does code refactoring.
numupdate number of update operations from gumtree tool .
numinsert number of inert operations from gumtree tool .
nummove number of move operations from gumtree tool .
numdelete number of delete operations from gumtree tool .
numactionclusters number of action clusters from gumtree tool .
numactions number of actions from gumtree tool .
fig.
.
mutant modification interaction features modification characteristics features we have features extracted from the commit diff and features extracted from the changed or added statements in the post commit version of the program.
figure describes the features extracted from the commit diff.
the features extracted from the changed or added statements are a the mean of the depth according to cfg of the basic blocks in bc modificationcfgdepth .
b the mean of the complexity of the statements in sc modificationcomplexity .
c the contextual features see section iii b of the added or changed statements in the program.
when the modification involves multiple statements the mean of each feature value for all statements is computed.
information flow features the first feature that we use in this category is a boolean variable mutantonmodification that represents whether the mutant mmutates an altered code sm2sc .
additionally we consider the graphs presented in section iii b and compute for each graph the set of shortest paths between smandsc.
for every set of paths we compute the size numpaths the maximum path length maxpathlen minimum path length minpathlen and mean path length meanpathlen .
our features are thus the combination of each one of these metrics on every shortest path set.
e. implementation we implemented mudelta in python.
for learning we used stochastic gradient boosting decision trees which has been found to work well in the context of mutation .
we used the xgboost framework and set the number of trees to with a maximum trees depth to .
we adopt early stopping during training to avoid over fitting.
mudelta uses both numerical or categorical features.
the categorical features are typeastparent typemutant .
in order to use the feature values with xgboost we pre process them using a normalization of numerical and an encoding of categorical features.
we normalize numerical features between and1using rescaling also known as min max normalization .
we use binary encoding binary encoding helps to keep a reasonably low feature dimension when comparing to onehot encoding for the categorical features.
we also use networkx3in the graph representation in order to extract the contextual features that were described in section iii b.
901iv.
r esearch questions we start our analysis by investigating the prediction ability of our machine learning method.
thus our first research question can be stated as rq1 prediction performance how well does mudelta predict commit relevant mutants?
to answer this question we collect a set of commits from the subject programs where we apply mutation testing and identify relevant mutants.
then we split the commits into training validation of the commits and test sets of the commits based on the timeline of the project older commits are used for training and newer for commits are used for evaluation and perform our experiment.
after checking the performance of the predictions we turn our attention to the primary problem of interest mutant ranking.
we investigate the extent to which our predictions can lead to strong and relevant tests by using the predictive mutants as test objectives in contrast to baseline mutants i.e.
randomly sampled mutants among those residing in the changed components random or among those residing on the altered lines modification .
hence we ask rq2 test assessment how mudelta compare with the baseline mutant sets with respect to killing commitrelevant mutants?
we answer this question following a simulation of a testing scenario where a tester analyse mutants in order to generate tests .
we are interested in the relative differences between the subsumming relevant mutation score denoted as rms when test generation is guided by the predicted or the baseline mutants.
we use the subsumming relevant mutation score to avoid bias from trivial redundant mutants .
we also use the random mutant selection baseline since it performs comparably to the state of the art .
we compare with random on a best effort basis i.e.
the rms achieved by putting the same level of effort measured by the number of mutants that require analysis.
such a simulation is typical in mutation testing literature and aims at quantifying the benefit of one method over the other.
to further show the need for mutant selection out of the changed code we also compute the extend to which mutants on modification are sufficient in killing commit relevant mutants.
answering the above question provides evidence that using our approach yields significant advantages over the baselines.
while this is important and demonstrates the potential of our approach still the question of actual test effectiveness actual fault revelation remains.
this means that it remains unclear what the fault revelation potential of our approach when the commit is fault introducing.
therefore we seek to investigate rq3 fault revelation how mudelta compare with the baseline mutant sets with respect to commitintroduced fault revelation?
to answer this question we investigate the fault revelation potential of the mutant selection techniques based on a set of real fault introducing commits.
we follow the same procedure as in the previous research questions.table i testsubjects benchmark programs commits mutants relevant tests corebench benchmark v. e xperimental setup a. benchmarks used we selected c programs from the gnu coreutils4 a collection of text file and shell utility programs widely used in software testing research .
the whole codebase of coreutils comprises approximately lines of c code5.
to perform our study on commits we used the benchmark6introduced by ma et al.
that is composed of two parts and includes benchmark a set of commits mined from the coreutils github repository from year to andcorebench that has fault introducing commits.
the benchmark contains a mutants generated by mart a state of the art tool that supports a comprehensive set of mutation operators and tce7 on both pre and postcommit program versions of each commit b the mutant labels whether they are commit relevant and c large test pools created using a combination of test generation tools .
it is noted that the mutant test executions involved require excessive computational resources i.e.
require roughly weeks of computation.
details about the data we used are recorded in table i. the column relevant records the number of commit relevant mutants.
b. experimental procedure to account for our working scenario we always train according to time i.e we use the older commits for training and the newer for evaluation.
this ensured that we follow the historical order of the commits.
following the stated rqs our experiment is composed of three parts.
the first part evaluates the prediction ability performance of mudelta answering rq1.
the second at evaluating the ability of mudelta to rank commit relevant mutants answering rq2 and the third part at evaluating the fault revealing potential answering rq3.
first experimental part we evaluate the trained classifiers using five typically adopted metrics namely the area under the receiver operating characteristic curve roc auc the area under the precision recall curve pr auc the precision the recall and the f1 score.
the receiver operating characteristic roc curve records the relationship between true and false positive rates .
the precision recall pr curve records the decrease in true positive classifications when the predicted positive values increase.
in essence the pr curve shows the trade off between precision and recall .
5measured with cloc 7compiler based equivalent and duplicate mutant detection technique 902precision is defined as the number of items that are truly relevant among the items that predicted to be relevant.
recall is defined as the number of items that are predicted to be relevant among all the truly relevant ones.
the f1 score or f measure of a classifier is defined as the weighted harmonic mean of the precision and recall.
these assessment metrics measure the general classification accuracy of the classifier.
higher values denote a better classification.
to reduce the risk of over fitting we split our commit data into three mutually exclusive sets training validation and test data .
we also use early stopping during training to overwhelm over fitting.
we use the following procedure chronologically order the commit from older to newer .
select the newest of commits as test data.
randomly shuffle all the mutants from the remaining of commits oldest commit then select of them as validation data and the rest as training data.
thus the training validation and test data represent and of the data set respectively.
the model evaluation is performed on the test data.
this experiment part was performed on both corebench and benchmark .
second experimental part we simulate a scenario where a tester selects mutants and designs tests to kill them.
this typical procedure consists of randomly selecting test cases from the test pools of the benchmark that kill the selected mutants.
specifically we rank the mutants and then we follow the mutant order by picking test cases from the test pool that kill them.
we then remove all the killed mutants and pick the next mutant from the list.
if the mutant is not killed by any of the tests we discard it without selecting any test.
we repeat this process times for all the approaches.
mudelta ranks all the mutants by the predicted commitrelevance probability random randomly ranks all the mutants in the changed components and modification randomly ranks the mutants located on the altered code.
our effectiveness metrics are the relevant subsuming mutation score rms achieved by the test suites when analysing up to a certain number of mutants.
subsuming score metrics allows reducing the influence of redundant mutants .
we also compute the average percentage of faults detected apfd that represents the average relevant subsuming mutation score when analysing any number of mutants within a given range.
our effort metric is the number of mutants picked analysed by the tester .
this includes the mutants killable or not that should be presented to testers for analysis either design a test to kill them or judge them as equivalent when applying mutation testing .
in the spirit of the best effort evaluation we focus on few mutants up to that testers need to analyse.
this evaluation aims at showing the benefits ofmudelta over random under the same relative testing effort.
the contrast with the modification shows whether there is a need for mutant selection outside of the modified code i.e.
whether mutants on modification are sufficient leading to tests that kill commit relevant mutants.
this part of the experiment was performed on both corebench and benchmark .third experimental part to evaluate the fault revealing ability of mudelta we used the corebench commits.
we adopted a chronological ordering for training validation and testing when splitting the commits similar to what we did in previous experimental parts.
we use the same process and effort metric as in the the second part of the experiment and report results related to fault revelation and the average percentage of commit introduced faults revealed apfd within the range of analysed mutants.
to account for the stochastic selection of test cases and mutant ranking we used the wilcoxon test to determine whether there is a statistically significant difference between the studied methods.
to check the size of the differences we used the vargha delaney effect size a12 which quantifies the differences between the approaches.
a value a12 suggests that the data of the two samples tend to be the same.
values a12 5indicate that the first data set has higher values while values a12 5indicate the opposite.
vi.
r esults a. assessment of the prediction performance rq1 to evaluate the performance of mudelta we check the model s convergence.
during training and after each iteration of the training process we check the model performance on both the training and validation data we used for training.
figure shows the roc auc and pr auc values wrt the number of training iterations.
we observe that the model performance on both the training and validation data increase with the number of iteration and stabilizes at specific values suggesting that our model is able to learn the characteristics of commit relevant mutants.
we then evaluate the performance of our model to predict commit relevant mutants on the future commits that appear in the test set.
to compute the precision recall and f1 score we set the prediction threshold probability to .
which we obtained by applying the geometric mean on the validation dataset.
the precision recall and f1 score of our classifier are .
.
and .
respectively.
these values are higher than those that one can get with a random classifier .
.
and .
respectively .
figure shows the roc and pr curves of our classifier strong lines and a random classifier dashed lines .
we observe that the roc auc of our classifier is .
indicating a strong prediction ability.
similarly we see that the pr auc of our classifier is .
while the random classifier pr auc is .
.
in this context it is important to give few mutants to developers for analysis.
to evaluate the performance of mudelta with lower thresholds we also study the performance ofmudelta with thresholds ranging from the to mutants.
we observe that the median precision of mudelta ranges from .
to .
when the threshold goes from to mutants.
these values are significantly higher than the random classifier which has a precision of .
.
these results provide evidence that mudelta provides a good discriminative ability for assessing the utility of mutants to test particular code changes.
training round0.
.
.
.
.0performance training pr auc training roc auc eval pr auc eval roc aucfig.
.
training and validation curves from the training phase.
.
.
.
.
.
.
recall false positive rate0.
.
.
.
.
.0precision true positive rateroc area .
pr curve area .
truth positive ratio fig.
.
precision recall and roc curves on test data.
b. mutant ranking for tests assessment rq2 figure shows the median rms achieved by the mutant ranking strategies when the number of analysed mutant budget range from to mutants.
in other words the figure shows test effectiveness measured with rms y axis that is achieved by a developer when analysing a number of mutants representing the cost factor recorded in x axis .
each subfigure is a commit taken from the test data.
we observe that the curve for mudelta is always higher than the curves of random andmodification and random is above modification .
to further visualize the differences figure shows the distribution of the rms of the mutant ranking strategies for budget thresholds and mutants.
as can be seen from the plots mudelta outperforms both random and modification .
interestingly random outperforms modification .
with threshold mutants the difference of the median values is and for random andmodification respectively.
this difference is markedly increased when analysing more mutants i.e.
it becomes and for the thresholds of and mutants for random .
to check whether the differences are statistically significant we performed a wilcoxon rank sum test and computed the vargha delaney a12effect size and found that mudelta outperforms both random andmodification with statistically significant difference at 01significant level .
random has also statistically significant differences with modification .
.
.
.
.
.
.
.
.
.
.
.
.
100random mudelta modificationrms fig.
.
rms achieved when analysing up to mutants.
mudelta random modification30 mudelta random modification50 mudelta random modification100 mudelta random modification0.
.
.
.
.
.0rms fig.
.
rms values when analysing up to and mutants.
figure shows the vargha delaney a12values between mudelta and both random andmodification .
we observe that the median value is between and for threshold between and mutants for random .
suggesting that mudelta is better than random in to of the cases for these thresholds.
the differences are larger for modification .
we further validate our approach by considering the distributions of apfd average percentage of faults detected values for all possible thresholds for mutants .
figure depicts these results and shows that mudelta yields an apfd median of random and modification reach median apfd values of and respectively confirm the superiority of our approach.
to account for the stochastic nature of the compared approaches and increase the confidence on our results we further perform a statistical test on the apfd values.
the wilcoxon test results yielded p values much lower than our significance level for the compared data i.e.
samples of mudelta and random mudelta and modification random and modification respectively.
therefore we conclude that mudelta outperforms random with statistically significance while modification is not sufficient for testing the deltas.
c. mutant ranking and fault revelation rq3 figure shows the distributions of apfd average percentage of faults detected values for the corebench fault introducing test commits using the three approaches under evaluation.
while mudelta yields an apfd median of random andmodification reach median apfd values of and respectively.
the improvement over random and modification are and respectively.
these results confirm the superiority of our approach wrt to fault revelation.
904top top top top top top top top top 90top .
.
.
.
.
.
.0vargha and delaney a measuremudelta random mudelta modificationfig.
.
vargha and deianey a12 mudelta vs random mudelta vs modification about rms mudelta random modification0.
.
.
.
.
.0apfd rms fig.
.
apfd rms up to mutants .
the wilcoxon test yielded p values much lower than our significance level for the compared data i.e.
samples of mudelta and random mudelta and modification random andmodification .
therefore we conclude that mudelta outperforms random andmodification with statistically significance while random outperforms modification .
figure shows the distribution of fault revelation for the ranking strategies and for mutant set size thresholds up to mutants.
we observe that the curve for mudelta is above the curves of random andmodification and random is above modification .
specifically we observe that mudelta reaches a fault revelation of and when analysing the top and mutants while random and respectively.
vii.
d iscussion a. comparison with other models to further assess the effectiveness of our model we contrast it with the prediction ability of five other models on the same training validation and test data sets that are typically used in prediction modelling studies.
in particular we used three families of models ensemble model classifiers logistic classifiers and neural networks and built five models namely adaboost random forest logistic regression multilayer perceptron mlp and mixed mlp.
mlp and mixed mlp were inspired by the work of li et al.
their architecture is shown in figure and .
to train and evaluate the models we used the sklearn library8.
since our data are imbalanced we also used class weighting strategies that are commonly used to tackle this issue.
to avoid bias from improper setting of the learners in all the cases we used grid search cross validation on the validation set to tune our hyperparameters.
mudelta random modification0.
.
.
.
.
.0apfd fig.
.
apfd fault revelation up to mutants .
number of mutants0.
.
.
.
.
.0fault revelationmudelta random modification fig.
.
median fault revelation in fault introducing commits.
table ii model comparison roc auc pr auc mccprecision on top adaboost .
.
.
.
random forest .
.
.
.
logistic .
.
.
.
mlp .
.
.
.
mixed mlp .
.
.
.
xgboost .
.
.
.
table ii reports the roc auc pr auc mcc and precision on top ranked mutants of the prediction results of all different learners we built.
the results show that the xgboost model that we use perform best in all cases.
the general prediction metrics roc auc pr auc mcc show that mixed mlp model is the second best case though it falls behind the ensemble models wrt to the top mutants.
nevertheless the results provide clear indications that the xgboost model we use is indeed the best choice.
fc layer dropout layer output layerinput layerall features fig.
.
mlp neural network architecture 905input layer fc layer dropout layer merge layer fc layerdropout layer dropout layer output layer mutant utility features mutant modification interaction featurescontextual featuresfig.
.
mixed mlp neural network architecture .
.
.
.
.
.
.
.
xgboost shap scoremutanttypeug in ctrl dependentscdg mutant modificationinformation flow out ctrl dependents out data dependentsast parentsdddg fig.
.
feature importance shap score of top feature sets.
the most important features are mutant type utility graph incoming control dependencies mutant modification features control dependency graph information flow outgoing control dependencies outgoing data dependencies ast parents and directed data dependency graph .
b. feature importance to evaluate the importance of our features we used the shapley additive explanations shap 9method i.e.
a game theory method that explains individual predictions based on the game theoretically optimal shapley values.
in particular we aim at explaining our predictions by assuming that each feature value we use is a player in a game where the prediction is the payout.
shapley values a method from coalitional game theory tells us how to fairly distribute the payout among the features.
we thus measure and report the feature importance shapley values of the feature categories we use.
results are depicted on figures and show that mutant type utility graph incoming control dependencies mutant modification features control dependency graph and the information flow are the top feature sets and that all three types of features we use are important.
additional results related to the feature importance of the individual features we used can be found on the accompanied website.
t hreats to validity a possible threat to external validity could be due to our test subjects.
our target was commits that do not alter test contracts and make small modifications similar to those observed in industrial ci pipelines.
such commits are usually hard to test and typically result in subtle faults.
large commits that add new features should be anyway tested by using a mutation testing approach that involves almost all the relevant mutants residing on the added code.
to reduce this threat we sampled a commit set where we could reasonably perform our experiments.
at the same time to diminish potential selection bias we also used the coreutils commits of corebench which are frequently used in testing studies.
we are confident on our results since the relevance properties of the mutants reside on the context of the committed code which includes the area around the dependencies to the committed code where we draw our feature values that is small and its characteristics should be as representative as our subjects.
moreover our predictions converge well do not have significant variance wrt to the baselines and consistently outperform the baselines in all test subjects we used.
additionally the statistical significance we observe indicates the sufficiency of our data analysis .
future work should validate our findings and analysis to larger programs.
another threat may relate to the mutants we use.
to mitigate this threat we selected data from a mutation testing tool that has been used in several studies that supports the most commonly used operators and covers the most frequent features of the c language.
threats to internal validity may be due our features.
we use a large number of features selected either based on previous studies or by using our intuition which are automatically filtered by gradient boosting.
to further reduce this concern we split our data in three parts training validation and test data.
during training using training data we measure the model convergence on training and validation data.
as demonstrated in figure our model converges both on the training and validation data showing that there are low chances for over or under fitting because in these cases the model would not converge on the validation data.
the test based approximation of relevant and killable mutants may introduce additional threats.
to reduce it we used test suites generated by klee and semu together with developer test suites.
a possible threat to construct validity could be due to the effort metric i.e.
the number of analysed mutants we use.
this is a typical metric for this kind of studies aiming at capturing the manual effort involved when analysing mutants or asserting automatically generated tests.
since our data have been filtered by tce a state of the art equivalent mutant detection technique this threat should be limited.
overall we tried to reduce threats by using various evaluation metrics i.e.
prediction performance relevant mutation score and fault revelation and established procedures.
furthermore to enable replication and future research we will make our tools and data publicly available.
906ix.
r elated work the problem of determining the set of mutants that are most relevant to particular code changes might resemble a dependence analysis problem.
one natural solution involves forming a program slice on the set of changed statements.
any mutant that lies in the slice should be considered relevant.
unfortunately this approach does not scale well for several reasons.
firstly as have been previously observed even a single static slice of a program tends to occupy between one and two thirds of the program from which it is constructed.
therefore the union of a set of such slices will be large and thereby fail to exclude many mutants.
secondly the dependence analysis would need to be incremental which raises further challenges.
although there have been incremental dependence analyses in the literature many well developed slicing systems are not incremental.
in general the problem of incremental program analysis at scale remains challenging .
thirdly it is hard to use dependence analysis to provide the priority ordering we need where priority is based on degree of relevance.
potentially unions of dynamic slices or some form of observation based slicing could achieve this but such approaches have a prohibitive computational cost in comparison to our method.
change impact analysis aims at determining the effects of changes on the other parts of the software.
similar to program slicing such approaches are conservative therefore they result in large number of false positives does not account for equivalent mutants located on potentially infected code and is hard to provide the mutant ranking prioritizes mutant types and location we need.
other attempts aim at testing the potential propagation flows of the changes .
similarly to change impact analysis their purpose is to identify the program paths flows that may be impacted by the changes.
they rely on symbolic execution to check for the feasibility of the flows form test requirements conditions to be fulfilled and decide on relevance.
unfortunately such techniques inherit most of the issues of symbolic execution are complex to implement and test the propagation of the changes.
in contrast our technique scales since it relies on static code features does not require any complex analysis techniques and applies mutation testing that is known for capturing the faultrevealing properties of test suites .
automatic test case generation aims at producing test inputs that a make observable the code differences of two program versions b increase and optimize coverage and kill mutants .
among these techniques the most relevant to our study are the are the ones related to patch testing i.e.
differential symbolic execution katch and shadow symbolic execution .
these techniques generate tests exercising the semantic differences between program versions guided by coverage.
all these techniques do not propose any test requirements as done by mudelta and thus they are complementary to our goal.
this means that they can be used to generate tests to kill the commit relevant mutants proposed by mudelta .related to continuous integration google is using a mutation testing tool that is integrated with the code review process reviewers select mutants .
this tool proposes mutants to developers in order to design test cases.
the key basis of this approach is to choose some mutants from the lines of the altered code.
we share a similar intent though we aim at making an informative selection of mutants among all project mutants.
according to our results mutants residing on nonaltered code tend to be powerful at capturing the interactions between the altered and non altered code.
regression mutation testing and the predictive mutation testing also focus on regression testing.
similarly pitest a popular mutation testing tool implements an incremental analysis that computes which mutants are killed or not by a regression test suite.
this means that the goal of the above techniques is to estimate the mutation score achieved by regression test suites thereby not making any distinction between commit relevant and non relevant mutants not making any mutant ranking and not proposing any live mutant to be used for test generation.
fault revealing mutant selection aims at selecting mutants that are likely to expose faults.
while powerful that technique targets the entire program functionality and not the changed delta one.
since it is unaware of the deltas it selects many irrelevant mutants while missing many delta relevant mutants related to the delta context interactions.
perhaps the closest work to ours is the commit aware mutation testing study that defines the notion of mutant relevance and demonstrates its potential.
in essence that work describes the fundamental aspects of relevant mutants but does not define any way to identify them at the testing time.
we therefore built on top of this notion by providing a static technique that identifies relevant mutants.
overall there is a fundamental difference on the aims of our approach and previous research since we statically produce relevant to code changes mutants and rank them to provide a best effort testing application.
x. c onclusion we presented mudelta a delta oriented mutation testing approach that selects delta relevant mutants mutants capturing the program behaviours affected by specific program changes.
experiments with mudelta demonstrated that it identifies delta relevant mutants with .
and .
precision and recall.
interestingly killing these mutants leads to strong tests that kill more relevant mutants than killing randomly selected mutants.
our results also show that mudelta selects mutants with a higher fault revealing ability than randomly selected mutants.
acknowledgement this work is supported by the luxembourg national research funds fnr through the core project grant c17 is codemates.
mark harman is part supported by european research council advanced fellowship grant number evolutionary program improvement epic .
907references t. t. chekam m. papadakis y .
l. traon and m. harman an empirical study on mutation statement and branch coverage fault revelation that avoids the unreliable clean program assumption in proceedings of the 39th international conference on software engineering icse buenos aires argentina may pp.
.
.
available r. a. demillo r. j. lipton and f. g. sayward hints on test data selection help for the practicing programmer ieee computer vol.
no.
pp.
.
.
available t. a. budd and d. angluin two notions of correctness and their relation to testing acta informatica vol.
no.
pp.
march .
m. fowler continuous integration online accessed february .
m. harman and p. w. o hearn from start ups to scale ups opportunities and open problems for static and dynamic program analysis in 18th ieee international working conference on source code analysis and manipulation scam madrid spain september .
ieee computer society pp.
.
.
available c. leong a. singh m. papadakis y .
l. traon and j. micco assessing transition based test selection algorithms at google inproceedings of the 41st international conference on software engineering software engineering in practice icse seip montreal qc canada may pp.
.
.
available g. petrovic and m. ivankovic state of mutation testing at google inproceedings of the 40th international conference on software engineering software engineering in practice icse seip gothenburg sweden may june pp.
.
.
available m. papadakis m. kintis j. zhang y .
jia y .
l. traon and m. harman chapter six mutation testing advances an analysis and survey advances in computers vol.
pp.
.
.
available m. kintis m. papadakis y .
jia n. malevris y .
l. traon and m. harman detecting trivial mutant equivalences via compiler optimisations ieee trans.
software eng.
vol.
no.
pp.
.
.
available m. papadakis c. henard m. harman y .
jia and y .
l. traon threats to the validity of mutation based test assessment in proceedings of the 25th international symposium on software testing and analysis issta saarbr ucken germany july pp.
.
.
available p. ammann m. e. delamaro and j. offutt establishing theoretical minimal sets of mutants in ieee seventh international conference on software testing verification and validation .
ieee .
b. kurtz p. ammann j. offutt m. e. delamaro m. kurtz and n. g okc e analyzing the validity of selective mutation with dominator mutants in proceedings of the 24th acm sigsoft international symposium on foundations of software engineering fse seattle wa usa november pp.
.
.
available m. papadakis t. t. chekam and y .
l. traon mutant quality indicators in ieee international conference on software testing verification and validation workshops icst workshops v aster as sweden april .
ieee computer society pp.
.
.
available t. apiwattanapong r. a. santelices p. k. chittimalli a. orso and m. j. harrold matrix maintenance oriented testing requirements identifier and examiner in testing academia and industry conference practice and research techniques taic part august windsor united kingdom pp.
.
.
available r. a. santelices p. k. chittimalli t. apiwattanapong a. orso and m. j. harrold test suite augmentation for evolving software in23rd ieee acm international conference on automated software engineering ase september l aquila italy pp.
.
.
available w. ma t. laurent m. ojdanic t. t. chekam a. ventresque and m. papadakis commit aware mutation testing in proceedings of the 36th ieee international conference on software maintenance and evolution icsme .
d. w. binkley n. gold m. harman s. s. islam j. krinke and s. yoo orbs language independent program slicing in proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering fse hong kong china november s. cheung a. orso and m. d. storey eds.
acm pp.
.
.
available m. kintis m. papadakis and n. malevris employing second order mutation for isolating first order equivalent mutants softw.
test.
verif.
reliab.
vol.
no.
pp.
.
.
available m. b ohme and a. roychoudhury corebench studying complexity of regression errors in international symposium on software testing and analysis issta san jose ca usa july pp.
.
.
available t. t. chekam m. papadakis t. f. bissyand e y .
l. traon and k. sen selecting fault revealing mutants empirical software engineering vol.
no.
pp.
.
.
available m. chalupa slicing of llvm bitcode masaryk univ .
j. j. mcauley l. da fontoura costa and t. s. caetano rich club phenomenon across complex network hierarchies applied physics letters vol.
no.
p. .
r. milo n. kashtan s. itzkovitz m. e. newman and u. alon on the uniform generation of random graphs with prescribed degree sequences arxiv preprint cond mat .
j. saram aki m. kivel a j. p. onnela k. kaski and j. kertesz generalizations of the clustering coefficient to weighted complex networks physical review e vol.
no.
p. .
g. fagiolo clustering in complex directed networks phys.
rev.
e vol.
p. aug .
.
available j. p. onnela j. saram aki j. kert esz and k. kaski intensity and coherence of motifs in weighted complex networks physical review e vol.
no.
p. .
p. g. lind m. c. gonz alez and h. j. herrmann cycles and clustering in bipartite networks phys.
rev.
e vol.
p. nov .
.
available l. page s. brin r. motwani and t. winograd the pagerank citation ranking bringing order to the web.
stanford infolab technical report november previous number sidl wp .
.
available j. m. kleinberg authoritative sources in a hyperlinked environment j. acm vol.
no.
p. sep. .
.
available j. falleri f. morandat x. blanc m. martinez and m. monperrus fine grained and accurate source code differencing in acm ieee international conference on automated software engineering ase vasteras sweden september pp.
.
.
available j. h. friedman stochastic gradient boosting computational statistics data analysis vol.
no.
pp.
.
t. chen and c. guestrin xgboost a scalable tree boosting system inproceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining ser.
kdd .
new york ny usa association for computing machinery p. .
.
available j. h. andrews l. c. briand y .
labiche and a. s. namin using mutation analysis for assessing and comparing testing coverage criteria ieee trans.
software eng.
vol.
no.
pp.
.
.
available r. gopinath i. ahmed m. a. alipour c. jensen and a. groce mutation reduction strategies considered harmful ieee trans.
reliab.
vol.
no.
pp.
.
.
available t. kuchta h. palikareva and c. cadar shadow symbolic execution for testing software patches acm trans.
softw.
eng.
methodol.
vol.
no.
pp.
.
.
available c. cadar d. dunbar and d. engler klee unassisted and automatic generation of high coverage tests for complex systems programs in proceedings of the 8th usenix conference on operating systems design and implementation ser.
osdi .
usa usenix association p. .
t. t. chekam m. papadakis and y .
l. traon mart a mutant generation tool for llvm in proceedings of the acm joint meeting on european software engineering conference and symposium on the foundations of software engineering esec sigsoft fse tallinn estonia august pp.
.
.
available m. papadakis y .
jia m. harman and y .
l. traon trivial compiler equivalence a large scale empirical study of a simple fast and effective equivalent mutant detection technique in 37th ieee acm international conference on software engineering icse florence italy may volume a. bertolino g. canfora and s. g. elbaum eds.
ieee computer society pp.
.
.
available t. t. chekam m. papadakis m. cordy and y .
l. traon killing stubborn mutants with symbolic execution .
.
available a. zheng evaluating machine learning models a beginner s guide to key concepts and pitfalls .
o reilly media inc .
.
available a. s. namin j. h. andrews and d. j. murdoch sufficient mutation operators for measuring test effectiveness in proceedings of the 30th international conference on software engineering icse leipzig germany may pp.
.
b. kurtz p. ammann m. e. delamaro j. offutt and l. deng mutant subsumption graphs in ieee seventh international conference on software testing verification and validation workshops .
ieee .
c. henard m. papadakis m. harman y .
jia and y .
l. traon comparing white box and black box test prioritization in proceedings of the 38th international conference on software engineering icse austin tx usa may pp.
.
.
available a. vargha and h. d. delaney a critique and improvement of the cl common language effect size statistics of mcgraw and wong jrnl.
educ.
behav.
stat.
vol.
no.
pp.
.
m. kubat s. matwin et al.
addressing the curse of imbalanced training sets one sided selection in icml vol.
.
citeseer pp.
.
r. barandelaa and e. rangela strategies for learning in class imbalance problems .
x. li w. li y .
zhang and l. zhang deepfl integrating multiple fault diagnosis dimensions for deep fault localization in proceedings of the 28th acm sigsoft international symposium on software testing and analysis issta beijing china july d. zhang and a. m ller eds.
acm pp.
.
.
available s. m. lundberg and s. lee a unified approach to interpreting model predictions in advances in neural information processing systems annual conference on neural information processing systems december long beach ca usa i. guyon u. von luxburg s. bengio h. m. wallach r. fergus s. v .
n. vishwanathan and r. garnett eds.
pp.
.
.
available a. arcuri and l. c. briand a practical guide for using statistical tests to assess randomized algorithms in software engineering inproceedings of the 33rd international conference on software engineering icse waikiki honolulu hi usa may r. n. taylor h. c. gall and n. medvidovic eds.
acm pp.
.
.
available t. laurent m. papadakis m. kintis c. henard y .
le traon and a. ventresque assessing and improving the mutation testing practice of pit in ieee international conference on software testing verification and validation icst .
ieee pp.
.
d. w. binkley and m. harman locating dependence clusters and dependence pollution in 21st ieee international conference on software maintenance icsm september budapest hungary .
ieee computer society pp.
.
.
available d. w. binkley m. harman and j. krinke empirical study of optimization techniques for massive slicing acm trans.
program.
lang.
syst.
vol.
no.
p. .
.
available a. orso s. sinha and m. j. harrold incremental slicing based on data dependences types in international conference on software maintenance icsm los alamitos california usa nov. pp.
.
b. li x. sun h. leung and s. zhang a survey of codebased change impact analysis techniques softw.
test.
verification reliab.
vol.
no.
pp.
.
.
available r. a. santelices and m. j. harrold exploiting program dependencies for scalable multiple path symbolic execution in proceedings of the nineteenth international symposium on software testing and analysis issta trento italy july pp.
.
.
available r. santelices and m. j. harrold applying aggressive propagationbased strategies for testing changes in fourth ieee international conference on software testing verification and validation icst berlin germany march pp.
.
.
available d. qi a. roychoudhury and z. liang test generation to expose changes in evolving programs in ase 25th ieee acm international conference on automated software engineering antwerp belgium september pp.
.
.
available z. xu y .
kim m. kim g. rothermel and m. b. cohen directed test suite augmentation techniques and tradeoffs in proceedings of the 18th acm sigsoft international symposium on foundations of software engineering santa fe nm usa november pp.
.
.
available g. fraser and a. zeller mutation driven generation of unit tests and oracles ieee trans.
software eng.
vol.
no.
pp.
.
.
available b. h. smith and l. williams on guiding the augmentation of an automated test suite via mutation analysis empirical software engineering vol.
no.
pp.
.
.
available s. person m. b. dwyer s. g. elbaum and c. s. pasareanu differential symbolic execution in proceedings of the 16th acm sigsoft international symposium on foundations of software engineering atlanta georgia usa november pp.
.
.
available p. d. marinescu and c. cadar katch high coverage testing of software patches in joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering esec fse saint petersburg russian federation august pp.
.
.
available l. zhang d. marinov l. zhang and s. khurshid regression mutation testing in international symposium on software testing and analysis issta minneapolis mn usa july pp.
.
j. zhang l. zhang m. harman d. hao y .
jia and l. zhang predictive mutation testing ieee trans.
software eng.
vol.
no.
pp.
.
.
available d. mao l. chen and l. zhang an extensive study on crossproject predictive mutation testing in 12th ieee conference on software testing validation and verification icst xi an china april pp.
.
.
available h. coles t. laurent c. henard m. papadakis and a. ventresque pit a practical mutation testing tool for java demo in proceedings of the 25th international symposium on software testing and analysis issta saarbr ucken germany july pp.
.
.
available