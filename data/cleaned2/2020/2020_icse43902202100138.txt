automatic unit test generation for machine learning libraries how far are we?
song wang york university toronto canada wangsong yorku.canishtha shrestha york university toronto canada shrestha.nishtha gmail.comabarna kucheri subburaman york university toronto canada abarnaks my.yorku.ca junjie wang chinese academy of sciences beijing china junjie iscas.ac.cnmoshi wei york university toronto canada moshiwei yorku.canachiappan nagappan microsoft research redmond usa nnagappan acm.org abstract automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades.
many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined.
however the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries which are statistically orientated and have fundamentally different nature and construction from general software projects.
in this paper we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries.
to investigate this issue we conducted an empirical study on five widely used machine learning libraries with two popular unit test case generation tools i.e.
evosuite andrandoop .
we find that most of the machine learning libraries do not maintain a high quality unit test suite regarding commonly applied quality metrics such as code coverage on average is .
and mutation score on average is .
unit test case generation tools i.e.
evosuite andrandoop lead to clear improvements in code coverage and mutation score however the improvement is limited and there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.
index terms empirical software engineering test case generation testing machine learning libraries i. i ntroduction software unit testing is widely recognized as a crucial part of software development process to foster and assure software quality.
however writing effective unit test cases is an extremely costly and time consuming practice .
to reduce such a burden for developers many automatic techniques and tools based on different mechanisms have been proposed e.g.
evosuite generates unit test cases based on genetic algorithms randoop generates unit test cases by using feedback directed approaches.
to evaluate the effectiveness and usefulness of these tools many studies have been conducted to compare the quality of automatically generated and manually written unit test suites .
results confirm that the automated test generation tools e.g.
evosuite and randoop are effective at producing unit test suites with high this work was done when dr. nagappan was with microsoft research.code coverage and these tools can also be an useful aid to help write unit test cases.
however the fact that most of these studies used randomly selected general projects leaves unanswered the more directly relevant question do the automatic unit test generation techniques also work for machine learning libraries?
we are witnessing a wide adoption of machine learning ml models in many software systems lately.
software applications powered by ml are being used in critical sectors of our daily lives from finance and energy to health and transportation .
thus building reliable and secure ml systems has become an increasingly critical challenge for software developers.
however ml libraries are often statistically orientated and have fundamentally different nature and construction compared to general software projects which makes the usefulness of existing automatic test generation tools on them unknown.
in this paper we set out to investigate the effectiveness of the widely used automatic unit test generation techniques on ml libraries.
specifically we select five widely used ml libraries i.e.
weka stanford corenlp mallet opennlp and mahout .
additionally to better understand ml libraries inspired by existing studies we decompose a ml library into three different types of components i.e.
data process core model and util details are in section ii b .
we use two typical automatic unit test generation tools i.e.
evosuite andrandoop as the experiment objectives following prior studies .
for our study we first perform an empirical study on the five ml libraries to unveil the effectiveness of their current unit test suites regarding commonly applied quality metrics such as code coverage and mutation score .
we then apply evosuite andrandoop on these ml libraries to generate unit tests and check whether evosuite andrandoop could improve test effectiveness on these libraries regarding code coverage and mutation score by comparing the automatically generated tests against the existing manually created ones.
note that to better understand the effectiveness and scope of the two unit test generation tools we apply them on the three ieee acm 43rd international conference on software engineering icse .
ieee different components i.e.
data process core model andutil from each experimental ml library.
we find that most ml libraries do not maintain a highquality unit test suite regarding code coverage on average .
and mutation score on average .
.
our manual analysis show that most test effort of ml libraries are spent on testing a subset of valid functionalities while leaving a large portion of code uncovered.
in addition the examined unit test generation tools i.e.
evosuite andrandoop can lead to clear improvements in code coverage and mutation score however the improvement is limited.
our manual investigation on randomly selected classes from the five libraries show that there exist common patterns in the uncovered code across the five ml libraries which can be used to improve the unit test case generation tasks for future research.
this paper makes the following contributions we conduct a comprehensive investigation of current unit test practices on five widely used machine learning libraries.
we examine the effectiveness and usefulness of two widely used automatic unit test generation tools on five machine learning libraries.
we identify gaps between existing automatic unit test generation techniques and unit testing practices on machine learning libraries.
we discuss general lessons learned and future directions from the application of the automatic unit test generation to machine learning libraries.
the rest of this paper is organized as follows.
section ii describes the background on automatic unit test generation and machine learning libraries.
section iii shows the setup of our empirical studies.
section iv presents the results of our study.
section v discusses open questions and the threats to the validity of this work.
section vi surveys the related work.
finally we summarize this paper in section vii.
ii.
b ackground a. automatic unit test generation software unit testing is widely adopted to test individual units components of software projects.
a unit test is an executable piece of code that validates a functionality of a class or a method under test performing as designed.
while there are many techniques to automatically generate unit tests we focus on two types of typical and scalable approaches based on random generation of call sequences i.e.
randoop and search based optimization of call sequences i.e.
evosuite in this work.
random testing random testing is a basic approach for test generation which consists of invocation of functions with random inputs.
guided random testing is a refined approach that starts with random input data then uses extra knowledge to guide input data generation.
one typical example of random testing is feedback directed random testing which enhances random test generation by incorporating feedback collected from executing test inputs that is used to avoidgenerating duplicate and illegal input data.
feedback directed based approaches build test inputs incrementally and then the newly created test inputs extend previous ones.
these test inputs are executed as soon as they are created.
the results collected from these executions will then be used to guide the generation of new test inputs.
we use the representative feedback directed unit test generation tool i.e.
randoop in our experiments.
search based testing search based software testing is an approach that transforms the unit test generation tasks into optimization problems where the objective of the test generation is implemented by a fitness function that guides the search.
genetic algorithm is widely used as the search techniques for test generation .
in the genetic algorithm randomly selected candidate solutions are evolved by applying evolutionary operators such as mutation and crossover resulting in new offspring individuals with better fitness values.
the widely used objective function for unit test generation is the code coverage of the generated tests .
in this work we examine the typical search based unit test generation tool i.e.
evosuite .
b. machine learning libraries machine learning is a type of artificial intelligence technique that requires huge volumes of data to be able to converge for making meaningful inferences decisions or predictions .
most machine learning techniques share a common basic procedure.
the first step when constructing a ml model is to collect data from a domain where specific concepts can be learned using some algorithms.
once data is collected often a pre processing step will be conducted before it can be used for learning.
the most common pre processing operations include data format converting noise data filtering and feature engineering.
the result of data pre possessing often can significantly affect the quality of trained models.
once the data is cleaned and features are extracted or selected properly a learning algorithm is used to infer relations capturing hidden patterns in the data.
during this learning process the parameters of the algorithm are tuned to fit the input data through an iterative process.
after training steps the model is evaluated and can be deployed in a specific application environment and interact with other components of the application to finish expected tasks.
a ml library is a collection of machine learning data mining or natural language processing algorithms which provides usable and easily extensible api for both software developers and research scientists.
to facilitate these algorithms most ml libraries often contain data processing and utility components .
to understand how existing automatic unit test generation tools can help test the different steps of ml models i.e.
data pre processing and model building we broke a ml library into the following three components at class level data process classes that contain apis to preprocess data before building machine learning models e.g.
data format transformation data sampling 1549data normalization noise data filtering and feature engineering related tasks.
core model classes that implement the core ml algorithms.
for example classification e.g.
image classification software bug prediction regression e.g.
temperature prediction software fault density prediction clustering e.g.
pattern recognition image segmentation translation named entity recognition sentiment analysis topic segmentation etc.
util classes that provide miscellaneous services for evaluating models e.g.
cross validation measuring performance e.g.
confusion matrix internationalization loading and outputting data etc.
note that machine learning can be classified into conventional machine learning and deep learning.
machine learning tasks like classification regression clustering translation named entity recognition sentiment analysis and topic segmentation belong to conventional machine learning.
deep learning adopts multiple layers of nonlinear processing units for feature extraction and transformation.
typical deep learning algorithms often follow some widely used neural network structures like convolutional neural networks cnns and recurrent neural networks rnns .
this paper only focuses on conventional machine learning libraries as most deep learning libraries such as tensorflow theano pytorch scikit learn are developed in python while existing widely used automatic unit test generation tools are designed for object oriented languages such as java c and c which fit most conventional machine learning libraries.
iii.
e mpirical study setup this section describes our experiment methodology.
the main objective of this study is to evaluate the effectiveness of existing widely used unit test generation tools in ml libraries and to understand the barriers for machine learning developers when adopting these tools.
a. research questions to achieve the mentioned goal we have designed experiments to answer the following research questions rq1 what is the quality of the current unit testing in ml libraries?
the studied five ml libraries have been developed for years.
they also maintain a stable unit test suite to test the provided functionalities.
this rq first reveals the quality of existing unit test in the machine learning libraries regarding their testing coverage and mutation scores.
a large number of widely adopted ml libraries originated from academic research studies and are maintained by researchers i.e.
academic led .
meanwhile with the rapid adoption of ai technologies many open source ml libraries have been developed by the open source community i.e.
community led .
little is known about the difference in unit testing practices between academic led and community ledml libraries.
in this rq we further explore the unit testing quality of ml libraries from these two different categories to better understand the difference in unit testing practices between academic led and community led ml libraries.
rq2 how effective are automatic unit test generation tools on ml libraries?
recent studies confirmed that the automatic test generation tools i.e.
evosuite andrandoop are effective at generating unit test suites with high code coverage and mutation scores for general software projects.
this rq intends to assess the capability of the two automatic unit test generation tools in ml libraries.
specifically we use randoop and evosuite to generate test cases for each class in the five ml libraries and further evaluate the effectiveness of the generated unit test cases by examining the increased code coverage and mutation score in these classes.
rq3 what is the covered and uncovered code with original unit test suites in ml libraries?
to understand the testing focus of these ml libraries i.e.
which parts of a ml library have been tested and which parts have not been tested with the original unit test suites we conduct a manual analysis to check the covered and uncovered code after executing the original test cases from each ml library.
rq4 to what extent can automatic unit test generation tools help test ml libraries?
even with the test cases generated by automatic unit test generation tools not all code can be covered .
following the results of rq3 this rq checks to what extent the test cases generated by randoop andevosuite for these ml libraries can help cover code missed by the original unit test suites of ml libraries.
based on the analysis we further explore the potential strategies to improve unit test generation tasks for ml libraries in section v. b. studied unit test generation tools in this work two widely used mature unit test generation tools that can generate junit supported test cases are selected i.e.
randoop andevosuite .
randoop implements feedback directed random test generation which first builds test inputs incrementally and then the newly created test inputs extend the previous ones.
it can generate tests containing assertions that capture the current state.
to generate unit tests for each class we used the default configurations and set the time limit to minutes as suggested by .
evosuite generates test suites with the aim of maximising code coverage e.g.
branch coverage minimising the number of unit tests and optimising their readability.
the generated tests include assertions that capture the current behaviour of the implementation.
in our experiments we use the default configurations of evosuite .
in this work we run both randoop andevosuite on a .0ghz i7 3930k desktop with 16gb of memory.
1550c.
subjects of study in this work we set out to investigate five javabased widely used ml libraries i.e.
weka stanford corenlp opennlp mallet and mahout .
weka waikato environment for knowledge analysis is a ml library developed at the university of waikato and has been downloaded over .
million times .
it provides multiple algorithms for data pre processing attribute selection classification and regression clustering association rules mining etc.
stanford corenlp is one of the dominating ml based toolkits for the processing of natural language text developed by the natural language processing group at stanford university.
it supports services for nlp tasks e.g.
language detection tokenization sentence segmentation part of speech tagging named entity extraction chunking etc.
mallet developed by university of massachusetts amherst in is a java based ml library for document classification topic modeling information extraction and other machine learning applications to text.
the above three ml libraries were originated from academic research studies.
we also collect two open source community led ml libraries i.e.
apache opennlp and apache mahout.
apache opennlp library is a machine learning based toolkit for the processing of natural language text which provides similar nlp services as stanford corenlp.
apache mahout is a powerful scalable machinelearning library that runs on top of hadoop mapreduce.
it provides machine learning algorithms for clustering classification and batch based collaborative filtering etc.
table i shows the statistic of the five ml libraries.
d. classifying classes in machine learning libraries we conduct a manual study to group each class in a machine learning library into three categories i.e.
data process core model and util .
for each class in a machine learning library three of the authors independently use the following steps to conduct the manual classification tasks step we read the javadoc documentation of the class to understand its functionality.
a category will be assigned to the class based on its major intent.
specifically if the class provides the services related to data process we categorize it as data process if the class implements a machine learning algorithm we categorize it as core model .
note that if the class involves both data process and machine learning algorithm of all classes analyzed we label it as core model only.
step if a decision cannot make based on the description of the class s api as the description might not contain enough information we further check the source code following the instructions in step to label the class.
step a class that does not belong to data process andcore model categories will be grouped as util .
during this manual analysis all disagreements were discussed until a consensus was reached.
the results of this phase haveweka corenlp mallet opennlp mahout020406080percentage data process core model util fig.
percentage of the three types of components in each experimental machine learning library.
a cohens kappa of .
which is a substantial level of agreement .
figure shows the distribution of the three different types of classes in the five ml libraries.
surprisingly util category is dominating across all the projects which takes up to .
of all the classes in mahout and is .
on average.
only classes are core model classes across these five ml libraries and the percentage of classes belonging to data process varies dramatically across the five projects and ranges from .
mahout to .
opennlp .
overall weka and mahout have dramatically unbalanced distributions among the three categories.
our further investigation on the two projects finds possible reasons for this.
unlike corenlp opennlp or mallet which support unstructured data weka requires specific data format i.e.
arff to run its machine learning algorithms.
although data with some other structured formats e.g.
csv json etc.
can also be loaded but they have to be saved to arff format for later use.
in addition weka does not support unstructured data.
most of weka s data process classes focus on data format transformation.
mahout has fewer data process classes since it is mainly used to provide machine learning analysis in hadoop processing pipeline and it maintains a large number of collection data structures that belong to util .
e. metrics for evaluating testing quality to measure the quality of a unit test suite given a project following existing work we use both code coverage and mutation score as the metrics.
our coverage analysis was performed using jacoco1 which can measure instruction and branch coverage.
the instruction coverage refers to java bytecode instructions and thus is similar to statement coverage on source code.
as jacoco s definition of branch coverage counts only branches of conditional statements not edges in the control flow graph we only use instruction coverage for measuring code coverage.
mutation testing generates program variants i.e.
mutants for the original program under test using mechanical transformation rules i.e.
mutation operators .
each mutant is the same with the original program except for the mutated statement.
a mutant is killed by a test suite if any test from the suite failed with the mutant.
mutation score is defined as the percentage of killed mutants with the total number of mutants.
a higher mutation score means a better quality of a test suite.
existing studies have shown that mutation faults 1551table i details of the experimental machine learning libraries.
project type description version kloc class test case weka academic led a general machine learning library.
.
.
stanford corenlp academic led a set of machine learning based nlp tools.
.
.
mallet academic led a machine learning for language toolkit.
.
apache opennlp community led a machine learning based toolkit for nlp.
.
.
apache mahout community led a distributed machine learning framework.
.
table ii the number of generated mutations on each category of classes in the five machine learning libraries.
weka corenlp mallet opennlp mahout data process core model util are close to real faults.
along this line mutation testing has been used to evaluate the quality of existing test suites .
in this work we use the widely used mutation test framework pitest to generate mutants and measure the mutation score of a unit test suite.
note that pitest contains many different mutation operators for generating different mutants in this work we use the default operators list in its website2.
iv.
e mpirical analysis a. rq1 current unit test quality to answer this question we first execute the original unit test suite provided by these machine learning libraries which are junit test cases.
as we described in section iii e we use jacoco to collect the coverage information and use pitest to generate mutation faults and further get mutation scores on each project.
these five libraries are maintained by maven.
to enable jacoco and pitest we manually add the jacoco and pitest maven plugins into each project s maven configuration file.
jacoco calculates the coverage for each class via checking whether a specific instruction is covered by at least one test case.
pitest is configured to mutate specified classes in a project and check the generated mutants against specified unit test cases.
table ii shows the numbers of mutants generated for different types of classes in each library.
we further compute the spearman correlation between the number of mutants and the number of classes in each category from these libraries.
the high correlation value .
indicates that the number of mutants generated for a category has a positive correlation with the number of classes in the category e.g.
weka has more mutants on core model andutil thandata process since the number of classes from weka s core model and util components are larger than that of data process component.
this is reasonable since the mutation operators provided by pitest do not have potential bias for specific programs .
in this question we run pitest with all classes and the original unit tests from each library.
table iii shows the average coverage and mutation score for classes in the three different categories in each machine learning library.
surprisingly the code coverage on these libraries is low and the overall coverage ranges from .
on to .
on mahout which is significantly lower that the code coverage on randomly chosen general opensource projects i.e.
around .
the overall mutation score on these libraries ranges from .
on corenlp to .
on mahout and on average is .
which is also lower than the average mutation score of a set of top open source general software project collected from github i.e.
around .
in addition as we categorize the ml libraries into two different categories i.e.
academic led and community led and decompose a ml library into three different components i.e.
data process core model and util we further explore the quality of unit tests for different categories and different components as shown in table iii.
overall the two open source community led ml libraries have both higher coverage and mutation scores than that of academic led ml libraries.
specifically the average coverage on the three academic led ml libraries is .
which is .
lower than that of community led ml libraries.
the average mutation score of academic led ml libraries is .
which is .
lower than that of community led ml libraries.
we further conduct the wilcoxon signed rank test to compare the unit testing quality regarding coverage and mutation scores between academic led and community led ml libraries.
results suggest that community led machine learning libraries significantly p outperform academic led ml libraries regarding either code coverage or mutation scores.
we also observe an extremely lower code coverage and mutation score in at least one category from each of the academicled ml libraries.
for example the util category in project weka has a significantly lower code coverage i.e.
.
and mutation score i.e.
.
compared to the other two categories i.e.
code coverage is larger than and mutation score is larger than .
the same phenomenon can be observed on the data process category in corenlp and the core model category in mallet.
we show the detailed distributions of code coverage of classes from each category in figure and further conduct the wilcoxon signed rank test p to compare the unit testing quality regarding code coverage among the three different categories in each project.
results show that the data process andcore model in weka have a significantly higher code coverage than util .
in corenlp util has a significantly higher code coverage than data process andcore model .
in mallet util and data process have a significantly higher code coverage thancore model .
we do not find any significant differences regarding the code coverage among the three categories 1552table iii the average coverage i.e.
coverage and mutation score i.e.
mscore of the unit test suite for each machine learning library.
overall indicates the results on all the classes in a project.
class typeweka corenlp mallet opennlp mahout coverage mscore coverage mscore coverage mscore coverage mscore coverage mscore data process .
.
.
.
.
.
.
.
.
.
core model .
.
.
.
.
.
.
.
.
.
util .
.
.
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.
.
.
weka corenlp mallet opennlp mahout00 51percentage fig.
the distribution of code coverage for data process i.e.
core model i.e.
and util i.e.
in each project.
from the two community led ml libraries.
note that we also observe a similar trend in mutation scores but we do not show the detailed distribution of mutation scores due to space limitations.
this indicates that the testing effort of academicled ml libraries is unbalanced among different components.
current unit test suite in ml libraries has lower quality regarding code coverage on average .
and mutation score on average .
.
in addition the testing effort of academic led ml libraries is unbalanced distributed and their unit test quality is significantly worse than that of community led ml libraries.
b. rq2 effectiveness of test gener ation t ools toanswer this question we runrandoop andevosuite to generate test cases for each of the five ml libraries as described in section iii b. with the generated test cases we further run jacoco to collect the co verage information for classes from the three dif ferent cate gories i.e.
data process core model and util on each library .
note that both randoop andevosuite generated flak y or uncompilable test classes.
in order to collect coverage information and mutation scores we ha veremov ed these flaky or uncompilable test cases with the approaches suggested in .
specifically first we remov ed all non compiling test classes.
second we e xecuted each compilable test suite five times and remov ed all new flaky tests from the e xecutions.
this process was repeated until all remaining tests passed five times in an iteration.
table iv presents the number of generated test cases for classes of each cate gory in each project the absolute coverage of the generated test cases and the additional coverage compared to the original unit test suite.
note that we did not run pitest for each category of classes collecting mutation scores as we cannot finish running pit est within a reasonable time frame i.e.
hours for most projects with large numbers of generated test cases i.e.
ranging from 5k to 140k .
toexplore the mutation scores of thegenerated test cases from randoop andevosuite we randomly selected classes from each category of each ml library and ran pitest with them to generate mutation faults and further obtain mutation scores.
w e sho w the additional mutation scores compared to mutation scores of the original unit test suite in table v. from t able iv we can see that the number of generated test cases significantly v ary in different projects e.g.
overall the tw o tools generate more test cases in weka and corenlp than that in opennlp mallet and mahout.
one of the possible reasons for this is that weka and corenlp have a lot more classes where randoop andevosuite can collect more method sequences to generate test cases.
w e also observe thatrandoop generates more test cases thanevosuite in four of the fi veprojects which is because randoop does not target a specific class under test .
column coverage sho ws the increment in code coverage of test cases generated by randoop and evosuite from which we can see that evosuite has a much higher increased code coverage an average of .
percentage points than randoop an a verage of .
percentage points and we further conduct the w ilcoxon signed rank test p to compare the increased code coverage between randoop and evosuite in these ml libraries.
results confirm thatevosuite produces a significantly higher increased code coverage than randoop .
howe ver we also find that e venwith the test cases generated byrandoop andevosuite the percentage of uncov ered code in these projects are still high i.e.
ranges from .
mahout withevosuite to .
corenlp with randoop and is .
on a verage which indicates the limited impro vements of these tools.
table v sho ws the increment in mutation score of test cases generated by randoop andevosuite on sample classes.
as we can see from the table bothrandoop andevosuite can impro vethe mutation scores on these ml libraries.
in addition similar with the trend in increased code coverage 1553table iv the number of generated test cases i.e.
generated test the absolute coverage i.e.
coverage and the increased code coverage i.e.
coverage of randoop andevosuite on the five experimental machine learning libraries.
project categoryrandoop evosuite generated test coverage coverage generated test coverage coverage wekadata process .
.
.
.
core model .
.
.
.
util .
.
.
.
overall .
.
.
.
corenlpdata process .
.
.
.
core model .
.
.
.
util .
.
.
.
overall .
.
.
.
malletdata process .
.
.
.
core model .
.
.
.
util .
.
.
.
overall .
.
.
.
opennlpdata process .
.
.
.
core model .
.
.
.
util .
.
.
.
overall .
.
.
.
mahoutdata process .
.
.
core model .
.
.
.
util .
.
.
.
overall .
.
.
.
table v increased mutation score i.e.
mscore of randoop andevosuite on sample classes.
projectrandoop evosuite mscore mscore weka .
.
corenlp .
.
mallet .
.
opennlp .
.
mahout .
.
evosuite has much higher increased mutation scores on average is .
than randoop on average is .
which is confirmed by our wilcoxon signed rank test p .
evosuite andrandoop lead to clear improvements in code coverage and mutation score compared to the original unit test suites of ml libraries.
however on average .
code is still uncovered with the generated test cases.
c. rq3 cover ed uncover ed code results of rq1 sho w that the studied five machine learning libraries ha velowtest quality regarding either code co verage or mutation score.
in this rq to understand the testing focus of these ml libraries we conduct an ma nual analysis and explore which parts of a machine learning library ha vebeen tested and which parts have not been tested with the original unit test suites provided by these ml libraries.
forour analysis we randomly selected classes from each category i.e.
data process core model and util of each ml library i.e.
in total classes are collected.
wefetch the the test cases of each class and collect the covered code and unco vered code for each class by using jacoco.
wefirst manually analyze the original test cases to check the covered functionalities of ml libraries.
surprisingly almost1 10010203040percentage a academic led 1002040percentage b community led fig.
the distrib ution of tested functionalities in the examined classes from academic led a and community led libraries b .
all libraries only test a part of supported valid functionalities.
taking the class weka.classifiers.trees.j48 from project w eka as an example which is a decision tree based classifier.
fortesting this class developers created one test case in which an object of j48 with default parameters w as created and then the classifier w as trained and tested with its testing data while other v alid parameters are not covered.
w e further calculate the number of functionalities tested.
figure shows the distrib utions of the number of functionalities tested in the examined classes from academic led i.e.
figure 3a and community led i.e.
figure 3b libraries respectiv ely.for academic led libraries we can see that around classes have only tested one or tw o functionalities and only about classes test more than functionalities while for communityled libraries more than classes have tested or more different functionalities.
ne vertheless among the sampled classes each class has an average of valid functionalities.
1554based on the classes we further check the uncovered code from each class to understand what has been missed from current unit test suite of machine learning libraries.
three authors work together to manually categorize the uncovered code into five different types.
we use code from weka library as examples to illustrate each category as follows.
lack of testing valid behaviors vb .
a class often has multiple valid behaviors e.g.
a parameter can be assigned with different values while only a set of valid behaviors have been tested.
below is a vbexample from class j48 i.e.
an algorithm used to generate a decision tree .
method setoptions is used to set up the essential options before building the j48 classifier.
the option n showed in the code line is used for pruning the built decision tree which could affect the performance of the built decision tree and the default value is assigned in line .
the corresponding unit case of j48 did not specify a value for n when calling method setoptions which leaves line uncovered.
1public void setoptions string options throws exception ... 3string numfoldsstring utils.getoption n options 4if numfoldsstring.length !
5m numfolds integer.parseint numfoldsstring else m numfolds ... lack of testing invalid behaviors ivb .
a class also has possible invalid behaviors e.g.
a parameter often has a valid scope values outside the scope will be invalid while not all invalid behaviors have been tested.
below is a ivb example from class randomtree i.e.
an algorithm used to generate a random tree based classifier .
method buildclassifier is used to build randomtree classifier.
the method first checks the validity of parameters in which m kvalue is number of attributes used to build the classifier.
developers can set m kvalue before calling buildclassifier a valid m kvalue ranges from to the size of the used attribute set.
the corresponding unit case of randomtree did not test any invalid value for mkvalue which leaves line uncovered.
1public void buildclassifier instances data throws exception ... make sure k value is in range 4if m kvalue data.numattributes m kvalue data.numattributes ... lack of testing exception ex .
in java developers can throw an exception in a method by using the throw keyword which will cause an exception to be raised and will require the calling method to catch the exception or throw the exception to the next level in the call stack.
junit enables developers to test exception by using the expectedexception rule.we find that most exceptions in these libraries have not been tested.
below is a exexample from class csvloader i.e.
a class used to load csv format data for building classifiers .
method setoptions is used to set up the parameters ofcsvloader .
the method first parses data from the input arguments and then checks the validity of parameters.
b is the size of the in memory buffer used to load data a valid b should be larger than line if the value is smaller than the method will throw an exception.
the corresponding unit case of csvloader did not test any value for b that can triage the exception throw statement which leaves line uncovered.
1public void setoptions string options throws exception ... b the size of the in memory buffer 4tmpstr utils.getoption b options 5if tmpstr.length int buff integer.parseint tmpstr if buff throw new exception buffer size must be setbuffersize buff ... lack of testing auxiliary methods aux .
most classes provide auxiliary methods to interact with attributes or perform other tasks.
we find that most auxiliary methods in these libraries have not been tested.
below is a aux example from class gaussianprocesses i.e.
a class that implements gaussian processes for regression analysis .
method getstandarddeviation is used to get standard deviation of the prediction at the given instance.
the method was used inside gaussianprocesses as an auxiliary method.
however the corresponding unit case ofgaussianprocesses did not test this method which leaves it uncovered.
1public double getstandarddeviation instance inst throws exception ... lack of testing message handling behaviors meb .
a class often provides a series of methods for handling behaviors to help print messages for developers.
our analysis reveals that a large number of message handling behaviors in these libraries have not been tested.
below is a meb example from class decisiontable i.e.
a class that build a decision table based classifier .
method printfeatures is used to print string description of the features selected.
the method was used inside decisiontable as a message handling method.
however the corresponding unit case of decisiontable did not test this method which leaves this method uncovered.
1public string printfeatures int i string s ... return s 1555table vi distribution of uncovered code in each ml library in percentage .
vb ivb ex aux meb weka .
.
.
.
.
corenlp .
.
.
.
.
mallet .
.
.
.
.
opennlp .
.
.
.
.
mahout .
.
.
.
.
overall .
.
.
.
.
with the above five categories of uncovered code we manually categorized each of the uncovered code block from the studied classes into one specific category and then we obtain the ratios of the five categories on each ml library.
table vi shows the distribution of uncovered code from each machine learning library regarding the above five categories.
for example category vbtakes up .
among all the uncovered code blocks in weka.
as we can see from the table overall the ratios of the five different categories vary among different projects.
however aux is dominating across the five ml libraries ranges from .
to .
which takes up .
of all the uncovered code blocks in these ml libraries.
the wilcoxon signedrank test p also confirms that the ratio of aux is significant higher than other categories.
we also find that categories vb ex and meb also have ratios higher than of all the uncovered code blocks.
comparing to the other four categories the ratio of category ivb is minor ranges from .
to .
one of the possible reasons is that the size of code blocks about invalid behaviors is smaller.
overall the unit test suites in ml libraries mainly focus on a subset of valid functionalities.
in addition there exists common patterns among the uncovered code of the studied ml libraries.
d. rq4 impro vement of unit t est gener ation t ools in rq3 section iv c we show the fi vedifferent categories about the uncov ered code in these ml libraries.
wethen conduct a further analysis to explore which part of source code can be covered by using test cases generated byrandoop andevosuite on the classes collected in rq3.
specifically for each class we collected its uncov ered code blocks after e xecuting the original unit test suite and the co vered code blocks after e xecuting test cases generated by randoop andevosuite .
then for each piece of the unco vered code blocks after executing the original unit test suite we fur ther check whether it is co vered by test cases generated by randoop andevosuite .
for each category of unco vered code on a ml library we calculate the percentage of remo ved uncov ered code blocks after executing test cases generated by randoop andevosuite e.g.
randoop can help remov e .
unco vered code of the original unit test suite of w eka on cate gory vb.
the details are shown in table vii.
as we can see from the table both randoop and evosuite can help reduce a larger portion of uncov eredtable vii remo veduncov ered code blocks after executing test cases generated by randoop andevosuite in per centage .
vb ivb ex aux meb wekarandoop .
.
.
.
.
evosuite .
.
.
.
.
corenlprandoop .
.
.
.
.
evosuite .
.
.
.
malletrandoop .
.
.
.
evosuite .
.
.
.
.
opennlprandoop .
.
.
.
evosuite .
.
.
.
.
mahoutrandoop .
.
.
.
.
evosuite .
.
.
.
.
overallrandoop .
.
.
.
.
evosuite .
.
.
.
.
code for the fi vedifferent uncov ered code categories.
in addition we can also see that evosuite remov es much more unco vered code ranges from .
to .
than that of randoop ranges from .
to .
and we further conduct the w ilcoxon signed rank test p to compare the percentages of remov ed unco vered code between evosuite andrandoop .
results suggest that evosuite performs significantly better thanrandoop on each category of unco vered code.
wecan also observe that both randoop andevosuite perform better onaux andmeb categories i.e.
o verall at least of them can be remov ed by randoop and at least of them can be remo vedbyevosuite while overall at most unco vered code from other three categories vb ivb and ex can be remo ved byrandoop and at most of them can be remo vedbyevosuite .
our wilcoxon signed rank test p sho ws that the performances of bothrandoop andevosuite onaux andmeb are significantly better than that of other three cate gories.
the main reason is that most aux andmeb related methods do not require input arguments or only need simple input ar guments which mak es it easy for bothrandoop andevosuite to generate compilable and v alid sequences of method calls to cover them.
howe ver tocovervb ivb and ex one needs test cases with v alid parameters invalid parameters differ ent parameter values different input data etc.
which often cannot generate by e xisting unit test generation tools e.g.
evosuite andrandoop .
both evosuite andrandoop can significantly help cover aux andmeb while the performance on other three categories i.e.
vb ivb and ex is limited.
v.discussion a. implications our study reveals se veral interesting findings that can serve as the practical guidelines for impro ving unit test generation tasks for ml libraries.
combining unit test generation with parameter analysis our manual analysis in section iv d sho ws that most of the uncov ered code from vbcategory is caused by missing testing v alid parameters.
different from general software projects 1556most ml libraries contain a large number of parameters.
however neither evosuite norrandoop can help generate test cases with valid parameters as most parameters are maintained in property documents thus one of the future directions to improve unit test generation for ml libraries is combining existing test generation with parameter analysis.
combining unit test generation with data generation most of existing unit test generation tools e.g.
evosuite and randoop only focus on generating test cases by selecting method call sequences and finding arguments from previouslyconstructed inputs.
however ml libraries are data driven to cover most ivb and some ex test cases with special training or input data are needed.
thus another future direction to improve unit test generation for ml libraries is combining existing test generation with test data generation together to generate method call sequences with newly generated input data.
transferring unit test across ml libraries different from general software projects most ml libraries share the same knowledge domain and often provide the same data processing steps machine learning algorithms and utility support services which makes it possible to transfer unit test cases of a specific machine learning component between ml libraries e.g.
the studied weka and mallet share around classification algorithms with the same or similar parameters.
thus transferring unit tests across ml libraries that share similar algorithms could be an applicable attempt to generate unit test for ml libraries.
b. threats to validity internal validity our study uses evosuite and randoop for automatic test generation.
it is possible using different automatic test generation tools may yield different results.
nevertheless these two tools are modern test generation tools and their generated tests both in format and coverage is similar to these produced by other modern test generation tools such as testful javapathfinder pex jcrasher and others.
in this work following existing studies we use code coverage to measure the quality of unit test suites given the fact that there exists debates on the correlation between code coverage and test effectiveness we plan to examine the effectiveness of unit tests with more criteria.
external validity in this work all the experiment subjects are open source projects and written in java.
although they are popular projects and widely used in both academic research studies and real world applications our findings may not be generalizable to commercial projects or projects in other ecosystems.
thus future research should revisit our study in machine learning models with other languages.
to mitigate this threat we plan to explore the effectiveness of on python projects in the future.
however note that our findings does not have to rely on language specific features and therefore we believe the findings in this work are still valuable for guiding the unit test practice with other program languages.vi.
r elated work a. automatic test generation over the past years researchers have made many advancements to solve the unit test generation problem.
as random testing can be easily scaled to large and complex systems random test data generation has been widely explored .
jtest is a commercial tool that leveraged random testing to generate test data that meets structural coverage.
jcrasher created sequences of method calls for java programs and reported sequences that throw certain types of exceptions.
eclat and randoop used random search to create tests that are likely to expose fault.
all these tools use random search without enough guidance which could lead to achieving low code coverage.
to improve random testing search based algorithms have been leveraged to generate test cases .
etoc used genetic algorithms to generate test data to test primitive types and strings.
testful autotest and evosuite are typical tools that automated test case generation by using genetic algorithms.
their objective is to reach the maximum coverage for a given criterion e.g.
cover all branches in the system by using evolutionary algorithms.
symbolic execution is an alternative approach to improve random testing such as jpf symbc and pex .
such approaches are not scalable because they cannot deal with complex statements native function call or external libraries .
almost all of the existing unit test case generation tools target at producing test cases for general software projects and studies have already showed that they can help produce unit test suites with high code coverage for general software projects .
this work conducts the first study to evaluate the effectiveness of automatic unit test generation tools on machine learning libraries.
b. machine learning testing to test machine learning algorithms metamorphic testing based approaches have been proposed to infer possible test oracle of an algorithm to indicate what the correct output should be for different inputs .
murphy et al.
discussed the properties of machine learning algorithms that may be adopted as metamorphic relations to detect implementation bugs.
along this line xie et al.
conducted the first study about leveraging metamorphic testing to test the implementations of two machine learning classification algorithms i.e.
knn and naive bayes.
recently many studies have been conducted to survey new techniques to test machine learning .
hang et al.
analyzed the main challenges of testing two machine learning algorithms i.e.
naive bayesian classifier and dnn classifier that perform a classification task.
masuda et al.
examined the challenges of software quality assurance for ml as a services mlaas which are machine learning services available through apis on the cloud.
ishikawa et al.
discussed the foundational concepts 1557that may be used in any and all machine learning testing approaches.
ma et al.
and huang et al.
surveyed the security of deep learning models.
guo et al.
characterized deep learning development and deployment across different frameworks and platforms.
braiek et al.
and zhang et al.
reviewed current existing testing practices for machine learning and deep learning frameworks.
results of the above surveys showed that most recent techniques to test machine learning mainly focused on testing deep learning models e.g.
pei et al.
presented the first white box testing of deep learning models tian et al.
proposed to utilize fuzz testing to generate more test data for autonomous driving cars ma et al.
proposed new mutation operators to evaluate the effectiveness of test cases on deep learning models and nejadgholi et al.
studied the test oracle practice in deep learning libraries.
prior studies on testing machine learning mainly focus on specific machine learning algorithms or the correctness of generated machine learning models.
in this work we present the first important step to understand how developers perform unit testing in machine learning libraries which is not studied by prior work.
vii.
c onclusion this paper conducts the first study to investigate the effectiveness of existing unit test generation techniques on machine learning libraries.
to investigate this issue we have conducted an empirical study on five widely used machine learning libraries with two popular unit test case generation tools i.e.
evosuite andrandoop .
our analysis finds that most of the machine learning libraries do not maintain a high quality unit test suite regarding commonly applied quality metrics such as code coverage on average is .
and mutation score on average is .
unit test case generation tools i.e.
evosuite andrandoop lead to clear improvements in code coverage and mutation score however the improvement is limited and there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.
acknowledgement the authors thank the anonymous reviewers for their feedback which helped improve this paper.
this work is supported by the natural sciences and engineering research council of canada nserc and the national natural science foundation of china under grant no.
.