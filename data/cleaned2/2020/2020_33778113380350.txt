testing file system implementations on layered models dongjie chen state key lab for novel software technology and department of computer science and technology nanjing university nanjing china dongjie cdj .comyanyan jiang state key lab for novel software technology and department of computer science and technology nanjing university nanjing china jyy nju.edu.cnchang xu state key lab for novel software technology and department of computer science and technology nanjing university nanjing china changxu nju.edu.cn xiaoxing ma state key lab for novel software technology and department of computer science and technology nanjing university nanjing china xxm nju.edu.cnjian lu state key lab for novel software technology and department of computer science and technology nanjing university nanjing china lj nju.edu.cn abstract generating high quality system call sequences is not only important to testing file system implementations but also challenging due to the astronomically large input space.
this paper introduces a new approach to the workload generation problem by building layered models and abstract workloads refinement.
this approach is instantiated as a three layer file system model for file system workload generation.
in a short period experiment run sequential workloads system call sequences manifested over a thousand crashes in mainline linux kernel file systems with 12previously unknown bugs being reported.
we also provide evidence that such workloads benefit other domain specific testing techniques including crash consistency testing and concurrency testing.
keywords model based testing workload generation file system acm reference format dongjie chen yanyan jiang chang xu xiaoxing ma and jian lu.
.
testing file system implementations on layered models.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa 13pages.
.
introduction the reliability of file systems is of critical importance because they are fundamental components in operating systems for persisting valuable application data.
bugs in file systems may result in serious consequences such as kernel panic data loss or even security vulnerabilities .
although file system implementations have been extensively validated by existing work permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
regression test suits fuzzing and model checking with hundreds of bugs being revealed file system bugs are still threatening the operating system s reliability.
to validate such complex systems as file systems we need to generate high quality test inputs a.k.a.
workloads to drive systems toward potentially buggy states.
for file systems we should generate file system call sequences to manifest bugs.
however effective workload generation is challenging because the input space of systems is astronomically large.
for example each file system call comprises various parameters there are tens of such calls and file system calls in a sequence influence each other.
though there exist techniques to systematically generate workloads they may be still ineffective to validate system implementations.
for example existing fuzzers are semantic unaware and behave more or less like a random system call generator because coverage information usually cannot effectively drive a gray box fuzzer to diverse file system states.
on the other hand building a precise and complete model for complex systems is unrealistic due to their complexities1.
therefore model checking modelbased testing or formal verification are limited to a small subset of core functionalities.
this paper proposes a model based approach to tackling the workload generation problem.
the key idea is to decompose the system into layered models in consistent with the system s evolution history.
model construction starts from a high level core model which captures the simplified semantics of the system but also reflects the fundamental functionalities designed in early versions of the system.
we should systematically exercise diverse abstract workloads e.g.
using model checking or model based testing and incrementally refine them to obtain lower level ones which further manifest sophisticated system behaviors developed during the evaluation.
finally we reach a syntactically complete model theoretically covers all possible workloads in which workload behaviors are impractical to specify.
1file system is a concurrent data structure in which data can be organized e.g.
buffered cached or journaled in various ways.
modern file system implementations contain tens of thousands of lines of code.
even the posix standard leaves many file system behaviors unspecified .
ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea d. chen y. jiang c. xu x. ma and j. lu in the rounds of refinements a high level abstract workload wmay correspond to multiple refined lower level abstract or concrete workloads w .
therefore we should deliberately design the layered models such that w w will behave mostly similar to w. suppose that we have and we later show such a design a high level model for generating high quality diverse abstract workloads in the rounds of refinements the diversity of high level workloads is preserved randomness is added to the workloads for exercising system behaviors that are impractical to model.
by striking a delicate trade off between the controlled diversity and the randomness this paper offers new opportunities to fully exploit the potential of incomplete models in effectively generating high quality workloads.
following such an intuition this paper develops a three layer file system model for high quality sequential workload generation thecore model captures the common and predictable semantics of all existing unix file system variants.
the core model provides a minimal abstraction of the directed acyclic graph structure of file systems plus a set of abstract file system calls to manipulate the graph structure the core ext model refines workloads in the core model by adding a few extension abstract file system calls.
such extension calls manifest sophisticated functionalities developed during the system evolution which are impractical to precisely specify.
examples of extension file system calls are reading writing an open file accessing extended attributes and performing large structural changes to the file system the syntax model further refines workloads in the core ext model by syntactically modeling all other system call parameters permissions flags lengths etc.
all possible file system call sequences can be generated by the syntax model.
instead of directly generating workloads using the syntax model as the fuzzers do the layered model separates the concerns in each stage of the workload generation.
particularly the core and core ext models are systematically model checked yielding highquality abstract system call sequences which are then filled with random parameters defined in the syntax model to finally obtain runnable workloads.
these ideas are implemented as a prototype tool dogfood .
we evaluated it in three testing scenarios with six actively maintained mainline file systems in the .
.
linux kernel and the experimental results are encouraging it has detected 12previously unknown bugs among these file systems in 12testing hours by generating sequential file system call sequences to test them when combined with a crash consistency detector it was able to detect hundreds of issues and it also benefited concurrency testing by helping find concurrency bugs when executing workloads concurrently.
in summary our contributions are a novel idea to model complex systems for workload generation which achieves a balance between semantics controllability and randomness in workload generation a three layer model and checker for file system implementations which generates effective file system call sequences and a prototype tool and its evaluation which detected 12previously unknown bugs.the rest of this paper is laid out as follows.
section 2introduces the layered modeling approach to testing complex systems.
section 3gives an overview of testing file systems on layered models.
section 4comprehensively defines the abstract file system models and introduces the workload generation algorithm.
section 5gives an overview of our prototype tool and its implementation followed by the evaluation in section .
section 7describes related work and section 8concludes this paper.
layered modeling and testing of complex systems .
background it is a challenge to generate high quality workloads to test complex software systems.
even if we clearly know the syntax of a system s inputs workloads it would be too difficult to completely specify the exact semantics of the system.
for example a file system provides a few well structured system call apis to the userspace.
however the semantics of a file system is far beyond one s understanding btrfs is 100k loc and extensively interacts with other components in the operating system.
another example is compilers which receive a program of unambiguously documented syntax usually within a few pages .
however even a tiny c compiler is 47k loc in size .
to test such systems one can apply grammar based fuzzing that generates large amounts of random inputs under feedback e.g.
code coverage guidance.
however fuzz testing is usually semantic unaware and generates mostly useless trivial test inputs.
for example existing file system workload generators strive to pass early error checkings even with code coverage guidance .
random programs are almost always trivial or failing early and thus they have limited potential in revealing compiler bugs.
alternatively one can provide a model of the system under test for further model checking or model based testing .
however providing a precise model for adopting existing model based testing techniques is also generally impractical for complex systems because the full precise specifications of complex systems such as file systems are usually infeasible .
.
generating workloads for complex systems on layered models layered modeling an evolutionary perspective.
to model complex systems for test input which we call a workload generation our key observation is that complex systems are always evolved from simpler ones and the evolution process serves as a guidance to the model design.
therefore instead of a direct syntax guided fuzzing or model based testing we argue that workload generation should start from building a highest level core model which is simple abstract and reflects the very basic functionalities designed in the early versions of the system.
the model is a ordered set m0 w w .
.
.
and the implementation of m0should sequentially generate abstract workloads w w .
.
.
1484testing file system implementations on layered models icse may seoul republic of korea each lower level model is a refinement of a higher level model which can be regarded as an refinement of higher level model workloads.
given an i th level abstract workload w mi i n the i th level model mi w is also an ordered set of workloads mi w i w i w i .
.
.
until the last level syntax model mnproduces executable concrete workloads.
to make workload generation effective we should design a semantic aware core model with simple and precise specifications.
consequently abstract workloads of higher level models can be generated in a more controlled fashion we expect that workloads generated in the core model are as diverse as possible in terms of core system behaviors which existed in the very first stages of the system.
lower level models should gradually introduce complex system behaviors in an ascending order of difficulties in modeling semantics.
consequently we inevitably lose control of the semantics of generated workloads.
however if we design the model refinement mi w i to be semantic preserving as much as possible low level concrete workloads will behave similar to abstract ones even if the model refinement is not entirely sound.
such a modeling methodology naturally applies to many complex systems because literally all man made systems are eventually originated from scratch.
examples include programming languages evolve from simple core constructs for describing control structure based computations .
to generate workloads programs for testing a compiler the simple constructs can be served as the core model in which the control and data flow of the generated program is determined.
modern compiler extensions syntactic sugars function variable attributes inline assembly etc.
can be loosely modeled in lower level models for exercising diverse compiler behaviors.
a multi threaded program first works as a sequential program .
therefore for testing multi threaded programs we can build the core model on coarse grained concurrency e.g.
function level component level and systematically generate workloads for diverse program states .
then lower level models may focus on fine grained interleaving specific behaviors e.g.
race or deadlock directed testing .
protocols of distributed systems are designed before the actual implementation.
such protocols are extensively being model checked in practice and are natural candidates of core models.
one can further validate distributed system implementations by extending abstract workloads with code level complications e.g.
injecting api failures or exercising thread level concurrency.
this paper particularly addresses the problem of layered modeling and testing of file system implementations for effective workload generation.
generating effective workloads on layered models.
to generate effective workloads we argue that most human understandable defects can be explained mostly under a simplified system model plus a few exceptional arguments.
this argument can be regarded as a variant for complex systems of the small scope hypothesis that a layered file system models core core ext syntax file system implementationscreate a b open a b o async prune a high level low levelcreate a b open a b enlarge a abstract workload concrete workload refinement ...create a b open a b create a b open a b prune a ...figure testing file systems on layered models high proportion of bugs can be found by testing the program for all test inputs within some small scope .
empirical evidences supported this claim to some extent.
for example most known and fixed concurrency bugs can be manifested within a few context switches crash consistency bugs can usually be triggered by a short workload and a single injected crash .
therefore we recommend exercising abstract workloads systematically using exhaustive enumeration model checking or modelbased testing for a maximized behavioral coverage i.e.
obtaining diverse abstract workloads .
in lower level models where we gradually lose control of the semantics we should cautiously generate mostly semantically preserving workloads with controlled random perturbations.
as an abstract core model workload w m0may correspond to many concrete workloads w w mn .
.
.m2 m1 w we should carefully design the models such that each w n w w behaves almost the same as w. then if abstract workloads wandw behave differently we guarantee that all workloads w w andw w are different w w w w .
therefore maximizing the diversity of abstract workloads in m0will likely result in semantically diverse final concrete workloads.
in summary we expect layered models to strike a balance between the semantics controllability by semantically modeling in the high level models and the syntax diversity by adding randomness to the low level models .
the rest of this paper focuses on realizing this methodology for the problem of file system workload generation.
testing file system implementations on layered models overview following the intuitions in section it would be reasonable to build thecoremodel on the modern file systems common ancestor the original unix file system.
the unix file system is simple yet provides the fundamental abstractions of all file systems maintaining a persistent data structure on storage devices.
the lower level core ext model is designed with caution to preserve the semantics of backbone abstract workloads2 but exercises 2otherwise a concrete workload may have a high chance to fail early.
1485icse may seoul republic of korea d. chen y. jiang c. xu x. ma and j. lu mode mode s irwxu s irgrp .
.
.
dev id integer flag flag o rdwr o sync .
.
.
data memory buffer id size integer mount opt mount opt grpquot acl .
.
.
command mkdir p mode create p mode mknod p mode dev id hardlink p p symlink p p rename p p remove p open f p flag close f chcwd p read f data size enlarge p size write f data size deepen p size remount mount opt fsync f statfs p read xattr p key data sync write xattr p key data prune p sequence command sequence command figure syntax of the three layer model of file system implementations.
means that symbols in the bracket can repeat any times.
pis a path and fis a file descriptor.
notations are defined in section .
even more diverse file system behaviors.
it almost does not change the file structures maintained by the core model but manifests more other file system features.
these feature supports are easy to extract and abstract from the evolution of systems e.g.
release notes of main versions provide descriptions about new apis or parameters.
the lowest level syntax model is a complete specification of all file system workloads3.
it is also mostly semantics preserving for an abstract workload w in the core ext model m2 w is a set of executable file system calls obtained by filling in missing system call parameters in w without changing the actual system call sequence.
according to the small scope hypothesis in case that we do not know what kind of explanations may trigger a bug in advance we should systematically exercise abstract workloads in the core model.
this resembles to enumerating the backbone of such an explanation.
we intentionally make the abstract file system model not a completely sound approximation of file system implementations.
for example if a file system does not support extended attributes or some device files cannot be created executing a concrete file system call generated by our model will fail and it may influence its following calls.
however even if system call behaviors in a workload may not be captured by the abstract model the workload still exercises diverse behaviors of the underlying file system implementation.
the three layer model and the corresponding workload generation technique is shown figure .
the model primitives are summarized in figure .
3all concrete workloads in wcan be generated by m2.the core model.
the core model m0captures the direct acyclic graph organization of file and directory structures in unix file systems.
the heart of the core model is an abstract file system model which models paths inodes file entities links and file descriptors open files but not the file contents.
ten abstract file system calls are included in the core model for creating deleting manipulating abstract files and directories e.g.
symlink andremove with their formal semantics unambiguously defined.
given two abstract workloads in the core model we can precisely determine their differences in terms of file directory structures.
this property is crucial in our systematic enumeration of diverse abstract workloads.
the core ext model.
in the years of evolution sophisticated functionalities are added to modern file system implementations e.g.
files may be stored in different data structures when they are of different sizes the background journaling daemon may periodically write data back to the storage device and file system call apis have undergone revisions over time to support more parameters.
generally it is impractical to precisely specify these behaviors.
to model these behaviors to some extent the core ext model m1inherits the abstract file system in the core model and adds eleven more abstract file system calls to manifest diverse file system behaviors such as on disk file system states and journaling.
an example is deepen for creating a deep directory of a random depth to trigger lots of file system metadata updates.
given an abstract workload win the core model m0 the core ext model refines wby interleaving it with random core ext abstract file system calls.
we carefully designed the core ext model such that workloads in m1 w though manifesting lots of random file system behaviors will behave mostly like win a concrete file system implementation if one only concerns the files and directories in w. the syntax model.
the lowest level syntax model m2is responsible for generating executable workloads on actual systems.
the syntax model is a complete model in the sense that it contains all possible workloads yet it is impractical to model what happens to the system when workloads are executed e.g.
opening a file with theo nonblock flag.
the syntax model is simple it translates abstract system calls in the core and core ext models to system call sequences and fills required parameters with random values.
workload generation.
generating effective workloads directly using the syntax model black box testing is challenging.
even though a workload is syntactically correct it may be semantically meaningless e.g.
opening a nonexistent file.
this is exactly why syntaxdirected fuzzers are not very effective in generating high quality workloads most workloads will fail early.
layered models mitigates this problem because any workload w m2has its correspondence w m0in the core model and sampling diverse abstract workloads in m0andm1should benefit the diversity of concrete workloads.
the workload generation is illustrated using a previously uknown bug in f2fs shown in figure .
the basis of manifesting the bug is finding the backbone abstract workload figure a which is generated in a systematic model checking for workloads of diverse file system states.
1486testing file system implementations on layered models icse may seoul republic of korea mkdir a rename a b1 mkdir b1 b2 mkdir b1 b2 b3 mkdir b1 b2 b3 b4 mkdir b1 b2 b3 b4 b5 a core mkdir a deepen a rename a b1 mkdir b1 b2 mkdir b1 b2 b3 mkdir b1 b2 b3 b4 mkdir b1 b2 b3 b4 b5 sync enlarge b1 b2 b3 b4 b5 b core ext mkdir a deepen a rename a b1 mkdir b1 b2 mkdir b1 b2 b3 mkdir b1 b2 b3 b4 mkdir b1 b2 b3 b4 b5 sync enlarge b1 b2 b3 b4 b5 c syntax figure a previously unknown bug null pointer dereference that crashes f2fs.
the workload is irreducible in the sense that the crash will not be triggered if any line in the workload is removed.
the core model checker maintains a queue of workloads the queue initially contains a single empty workload picks up a workload from the queue in each iteration and generates a new workload by appending a new abstract file system call.
the search is exhaustive eventually all abstract workloads will be generated .
to improve efficiency we leverage the semantics interpretation of workloads to prioritize distinctive workloads and prune equivalent ones.
in the example when b1 b2 b3 has been created we prefer creating a new file structure with path b1 b2 b3 b4 where a deep subdirectory is exercised or b1 b4 b1 b2 b3 where a larger top directory is exercised rather than b1 b2 b3 b1 b2 b4 where two paths seems similar.
repeating the generation the basic but nontrivial file structure with path b1 b2 b3 b4 b5 is finally constructed shown in figure a .
during the generation abstract calls in the ext model are also exercised e.g.
after the directory with path a is created it willdeepen ed and a sync operation will also be issued after the basic file structure is constructed.
similar to preferring creating deep directories operating enlarge on the directory with path b1 b2 b3 b4 b5 will be preferred because it results in lots of file metadata updates under a deep directory depicted in figure3 b .
finally necessary parameters in an abstract workload are filled with random values drawn from a parameter model to obtain runnable workloads system call sequences .
such a model will mostly return a default value but sometimes return random values to manifest diverse file system behaviors.
such a bug triggering concrete workload is shown in figure c .
though the final triggering script seems succinct it is difficult to generate an equivalent file system sequence directly by existing techniques because of that the basic file and directory structure is nontrivial which is hard to generate deepen and enlarge are expressive which represent creating a large number of files under a directory to trigger the bug a f2fs file system has to be mounted with the specific options noinline dentry checkpoint disable and quota.
testing file system implementations on layered models this section introduces an abstract file system first section .
and describes the syntax and semantics of the core core ext and directory inode symbolic link inode regular special inodeedge redirection path ea cb fg a d d hfigure an example of the abstract file system syntax model section .
.
workload generation is elaborated in section .
.
.
abstract file system model notations and definitions.
as mentioned earlier the core and coreext models share an abstract file system model which is a simplification of the unix file system.
a file system is a data structure for persisting files and directories.
a file is indexed by a file path a list of strings p also written as p s0 s1 .
.
.
sn where each siis a file name.
a path of n is the rootof the file system.
if n the parent ofpis defined as p .
we call nthedepth ofp denoted by p n .
at runtime the abstract file system state is determined by a four tuples n e f where n e comprises an acyclic graph nis the set of all inodes file or directory entities .
figure shows an abstract file system example in which each vertex is an inode.
there are four different types of inodes directory symlink symbolic link file regular file or device special file.
each inode n n is a key value mapping and we use n xto represent the value given by key xinn.
for a symlink file we use n symlink to store its redirection path.
nrootis the root inode in the file system.
e n string n is the set of all edges in the graph which means that given an inode nas a directory and a string as a file name emaps them to an inode under this directory4.
since there may be hard links an inode may be mapped to by more than one edge i.e.
the indegree of the inode is larger than one a b c and a d e in figure .
given any path p 4it may map to if the input nor file name is invalid.
1487icse may seoul republic of korea d. chen y. jiang c. xu x. ma and j. lu we can resolve it to an inode by the following function p nroot p p symlink sn p type symlink e e type symlink e symlink e type symlink otherwise where means list concatenation.
in the example g h e will resolve g h first which becomes a d by the fourth case then g h e e by the third case which maps to the final regular file inode.
fis the set of all open files in the system.
system calls usually resolve a path pand return a file descriptor f f that is associated with p s corresponding inode.
n is the current working directory.
the initial states0 nroot nroot contains only a root directory.
file and directory attributes.
based on the file system state s we can derive a series of useful attributes of a file or an inode as each inode corresponds to a unique file .
the attributes of an inode n in the vector form of type symlinked atime depth hardlink children open are later used in measuring the diversity of inodes when generating command sequences in section .
in the attribute vector type is n s file type symlinked represents the number of symbolic link files targeting n atime means the last access time of n depth is the depth of n given a path p as there may be multiple direct paths to the same inode hardlink is the hard link count of n i.e.
its indegree children is the number of children under directory n a non directory file has no child open is the number of open file descriptors targeting an inode a non regular file has a zero open count .
the abstract file system captures the graphical structure of files and directories shared by all file systems but not the file contents.
the core model avoids modeling complex mechanisms impractical to specify under the apparently simple system call interface quotas crash consistency extended attributes etc.
.
layered models the core model.
a file system s workload is a sequence of system calls.
analogously to manipulate the abstract file system we designed abstract file system calls and in the core and coreext models respectively namely commands6.
each core command is an abstract version of a real system call for manipulating the file system structure or file descriptors.
take open as an example it is an abstract form of the system call int open const char pathname int flags mode t mode 5the attribute designed is extensible.
new attributes can be easily added in other file system testing scenarios.
6we use abstract file system call and command interchangeably when without ambiguity.
n sn e n ndir defmkdir p n n n e e n sn n f n p dir n n sn e n ndev defmknod p n n n e e n sn n f n p dir n n sn e n nreg defcreate p n n n e e n sn n f n p dir n n sn e m p m type reghardlink p p ne e n sn m f n p dir n n sn e n nsym def symlink p p n n n e e n sn n f n p dir n n sn e m p m p n m m n rename p p ne e m s n m n sn m f n p n n remove p n n n e e m n f n p n type reg open f p nef f f f f close f nef f f n p n type dir chcwd p nef n figure core model semantics.
p .ndir def ndev def nreg def and nsym defdenote default values of four different inodes.
the symbol and represent mapping operations i.e.
adding and deleting.
n n means nan ancestor of n .
n denotes all reachable inodes from an inode nincluding itself andn n indicates that for each inode in n it is removed fromnif its indegree is zero.
the semantics of core commands are listed in figure in which the auxiliary function dir n true n type dir false otherwise checks whether nis a directory inode.
the semantics of core commands are explained below mkdir creates a new directory create builds a new regular file and mknod constructs a new device file named p.8these three commands first resolve the parent inode n from path p and require that n must be a directory and does not contain an children inode named sn .
after that they add a 8creating a file means creating a new inode and a path of that specific file type.
1488testing file system implementations on layered models icse may seoul republic of korea new inode nand the corresponding edge between n and n. hardlink creates a new path ptargeting the same regular file inode as p .
it is similar to create except that it does not create a new inode but adds an edge between two existing inodes n targeted by p and m targeted by p .
symlink creates a symlink file redirecting to p .
it is similar tocreate except that it creates a new symlink inode that stores a redirection path.
rename deletes the existing inode m targeted by p from its current parent directory m targeted by p and moves it to another directory inode n targeted by p .
note that it requires that mandn are not an ancestor of each other.
remove removes the inode n targeted by p .
ifnis a directory its all children are recursively removed.
it requires that nis not the current working directory.
open adds a new file descriptor ftofif path ptargets a regular file.
close deletes an existing file descriptor ffromf.
chcwd changes the current working directory .
each inode s attribute vector is also maintained according to their definitions.
for brevity we did not list them in figure .
the core ext model.
the core ext model also operates on the abstract file system.
we designed 11extension commands syntax in figure 2and semantics in table most of which do not change the abstract file system state.
the extension commands are designed for driving the file system s underlying storage e.g.
a disk to diverse states.
core ext commands may incur large amounts of file metadata operations fragment the storage or access the file contents.
given a core model workload w m0 suppose that we insert an enlarge p command in the middle of wfor a directory pyielding w .
executing w creates thousands of random sub directories which incurs lots of disk block i o operations and complicates the directory s internal data structure.
furthermore the file system structure is identical for wandw if we ignore the sub directories created by the enlarge .
in other words the basic file system structure in w m0is preserved inw m1 w .
core ext commands may also change file attributes e.g.
enlarge increases the children attribute.
prune is similar to remove except that it keeps the concerned inode and may change the children attribute of concerned directories.
the syntax model.
the syntax model captures all syntactically correct file system call apis in c as shown in figure 2where pis a path and f f is a file descriptor .
given a workload w m1 it can be easily refined to obtain a syntactically valid concrete workload in m2.
most abstract commands have its system call correspondence.
filling such an abstract command with syntactically valid parameters yields a concrete system call.
for those abstract commands that do not have a system call correspondence e.g.
prune we provide an implementation using file system calls e.g.
unlink .
due to the complexity of file system implementations it is impractical to specify the semantics of the syntax model.
later we will see that layered modeling is effective in generating high quality workloads without such semantics.algorithm file system workload generation 1procedure workload generation q s while q not terminate do s bw prio pick q rule p1applies w for bwdo is an abstract command with type ty w w fill parameter p pparam ty yield w generate a workload and execute it if bw lmaxthen pop q s bw length is bounded as lmax continue ty select type s ty pcmd rule p2applies for build commands s ty do 15s new state s bw bw if s qthen q q s bw 19procedure build commands s ty cmds rule p3applies forn typical inodes by kmeans ty do cmds cmds new command ty n return cmds .
generating workloads we can apply bounded model checking as did in ace to generate file system workloads of a length limit that cover all reachable non equivalent abstract file system states s. unfortunately bounded model checking scales poorly to long workloads.
on the other hand such long workloads are essential to reaching deep file system states and manifesting file system bugs.
therefore we add prioritization to the framework of bounded model checking such that we can quickly generate a large amount of workloads but also eventually achieve the effect of covering all non equivalent abstract file system states though it is not achievable practically .
the prioritization rules are three fold p1 when picking up workloads we favor longer ones because they may result in more io operations disk fragments etc.
for exercising diverse file system states.
p2 when appending a command the type of this new command is drawn from the probability distribution pcmd.
p3 when selecting an inode as the argument the path or the file descriptor associated with an inode for a new command we choose the one that is mostly likely to drive the system to an unexplored state.
the testing procedure based on layered models is shown in algorithm .
it resembles a standard queue based model checker .
the model checker starts with the initial abstract file system state s0and an empty workload.
during each iteration it picks up an abstract workload bwin the queue at line 4according to rule p1 bwresides in the queue and may 1489icse may seoul republic of korea d. chen y. jiang c. xu x. ma and j. lu table extension calls precondition extension calls description f f read write fsync f read write synchronize the open file descriptor f. sync synchronize the whole file system.
remount remount the file system.
p statfs p retrieve the file system information with path p. p read xattr write xattr p read extended attributes of the file with path p. p p type dir deepen p create subdirectories alongside a single path under the directory p. p p type dir reg enlarge p ifptargets a directory inode create subdirectories under the directory otherwise truncate the regular file to a larger size.
p p type dir reg prune p ifptargets a directory inode remove all inodes under the directory otherwise truncate the regular file to be empty.
be picked up later .
then bwis refined into concrete workload wand executed at line .
for each generated abstract workload in m1 the core ext model fill parameter does the refinement job.
for command type ty values of its parameters pare drawn from the command s own distribution i.e.
p pparam ty.
for example we set themode parameter mostly to a default value as its subtle changes may lead to early failures flagandmount opt can be encoded as a bit string and we independently set each bit by a probability of forsize we uniform randomly sample an integer in a predefined range finally for data parameter we uniform randomly sample from a set of predefined memory regions.
ifbwis longer than the limit we discard it line otherwise it is appended a new command yielding a new abstract file system states and a new workload bw line .
ifs has not been previously explored it is added to the queue with its corresponding workload bw line where file system state comparisons are done by a merkle tree alike hashing .
the new command generation first selects a command type to build according the probability distribution pcmd rule p2 where most commands are of an equal probability except that commands about deleting inodes e.g.
remove andprune are of lower probabilities.
this prevents inodes from being removed too frequently to manifest large and complex file structure.
commands about file system synchronization and remounting are also of lower probabilities because these commands do not modify the file structure directly but explore complicated file system behaviors when file structures are complex.
second the inode parameters for new commands are selected delicately according to rule p3.
the merit of a candidate inode is measured by the similarity between inodes.
an inode n which is more dissimilar to others should receive a higher probability of being chosen9.
such a similarity measurement is realized by a k means clustering.
in build commands we cluster all reachable inodes from the current working directory by their attribute vectors described in section .
after that we select typical inodes and create new commands for each of them.
note that inodes to be clustered should be related to the command type e.g.
only regular 9consider that we attempt to make a remove command under a state where there are four empty directories and a non empty directory we tend to exercise remove ing an empty directory and remove ing a non empty one under the state instead of trying two commands removeing empty directories.file inodes are considered for open and if an inode has multiple associated paths or file descriptors we choose one of them randomly.
system implementation we implemented the models and workload generation algorithm in section 4as an open source prototype tool dogfood10upon qemu .dogfood consists of three main components workload generator sequence dumper and testing backend.
the model checker implements algorithm it generates file system call sequences and fills each system call with random parameters.
these sequences are then dumped by the dumper to c programs which are compiled loaded and executed by the testing backend in a qemu virtual machine.
the testing backend also collects and analyzes the kernel log of the virtual machine.
sequence dumper and executor.
the dumper serializes file system call sequences into interface function calls which is then compiled and linked with different runtime libraries into various executors.
the executors do the precondition checks e.g.
probing whether the target file exists or confirming whether a file descriptor is valid and execute the system calls.
our dogfood executor directly executes the system calls the crashmonkey 11executor simulates a file system crash and checks for consistency.
testing backend.
we use debootstrap to build a rootfs disk image and run linux kernels with different file systems in a qemu virtual machine .
a controller is implemented to manipulate the virtual machine and to communicate with the guest os via ssh.
the controller copies executors to the guest os runs them sequentially and collects the results.
when testing file systems we should keep the testing environment clean as much as possible that all changes to the file system are from the current workload instead of others we take advantage of the qemu snapshot functionality where a virtual machine snapshot is saved after booting the guest os and we revert the os state before each executor running.
sequence reducer.
given a failure inducing workload bw it may be long and difficult to debug.
therefore dogfood also includes a sequence reducer.
the reducer first binary searches for the shortest prefix of bwwhich triggers exactly the same type of failure.
then the reducer tries to iteratively remove commands from the beginning 1490testing file system implementations on layered models icse may seoul republic of korea table the evaluated file system implementations file system description ext4 the default file system for most linux distributions btrfs a modern copy on write cow file system f2fs a modern flash storage devices friendly file system reiserfs a general purpose journaled file system gfs2 a shared disk file system jfs a journaled file system as far as the reduced sequence can still trigger the failure.
when removing a command we conduct a define use analysis of the workload and remove all following commands that are datadependent on it.
though the reduced sequence may not be the shortest failure inducing subsequence the evaluation results show that it is effective in practice.
experiments since our workload generation technique is general we applied dogfood to generate workloads in three file system testing scenarios seqential directly feeding the workloads to a file system implementation crash consistency injecting a crash in the middle and check the file system consistency concurrency exploring the interleaving space of parallel workloads.
the experimental setup is described in section .
followed by the results in section .
.
.
experimental setup we evaluated dogfood on six actively maintained mainline file systems in the .
.3linux kernel the latest kernel when the experiments were conducted.
as listed in table these file systems are ext4 btrfs f2fs reiserfs gfs2 and jfs representing diverse subjects.
these file systems are widely used and have been tested by existing work .
the workloads are used in three testing scenarios thesequential scenario.
the most straightforward way of testing is directly executing the generated workloads on an operating system.
for each file system we ran dogfood for12hours and the testing logs were collected.
for an informal comparison we also ran the state of the art kernel fuzzer syzkaller for hours12 which generates workloads to corrupt the kernel directly.
syzkaller is an industrial testing tool that has been widely used and found thousands of bugs.
we restricted syzkaller to generate only file system calls.
for both evaluated techniques a disk formatted with the designated file system format was mounted before the workloads were executed.
syzkaller used default parameters dogfood was capable of taking different parameters during the runtime.
thecrash consistency scenario.
a system may crash at any time and the on disk file system should be in a reasonable state after any crash.
crashmonkey simulates a physical disk and can inject simulated crashes.
such simulated crash disk images are checked 12the comparison is informal because syzkaller is so famous that many file system had already underwent thousands cpu hours of fuzzing.table crashes warns in the hour testing.
means that crash occurred too frequently.
file systems ext4 btrfs f2fs reiserfs gfs2 jfs dogfood syzkaller against consistency problems which is a major source of file system bugs .
crashmonkey has a built in workload generator ace that is dedicated to yield workloads for crash consistency checking and we wonder if dogfood can generate higher quality workloads.
for both ofdogfood and ace we randomly sampled workloads and the inconsistency reports were analyzed.
we also tested file system crash consistency using 000random workloads generated by dogfood .
since crashmonkey can only check the consistency of ext4 btrfs and f2fs the experiments were limited to these file systems for fairness.
theconcurrency scenario.
we generated 500workloads that did not manifest any failure when executed sequentially for times.
to simulate the concurrent workloads we ran in parallel a worker thread that periodically synchronizes the file system and remounts the disk to manifest potential race conditions on metadata operations.
to manifest potential concurrency bugs we use kprobes to instrument the kernel by inserting random delays before and after functions calls related to file systems and synchronizations .
concurrency testing is time consuming.
as a proof of concept we evaluated it on the btrfs where dogfood manifested fewer crashes than the others.
since concurrency bugs do not manifest deterministically we repeated the concurrent executions of each workloads for 10times.
all experiments were conducted on a desktop pc with a quadcore .40g hz intel core i7 processor and gib memory running ubuntu linux .
.
when conducting the crash consistency experiment we used libvirt to create a virtual machine with two virtual cpus and gib memory running linux kernel .
.
the same setting as the the original paper of crashmonkey .
.
experimental results thesequential scenario.
excluding jfs crashing too frequently sequential workloads generated by dogfood crashed the kernel by 041times .
ext4 the most widely used file system is considerably more reliable than the others.
it is the only one that survived the hour testing.
we manually inspected the crashes and identified distinct bugs that have a clear root cause have a severe consequence kernel panic use after free or null pointer deference and can be stably reproduced.
we have reported all of them as listed in table .
some have already been fixed by the developers.
dogfood also found 71warnings in btrfs and three of them are unique warnings likely to be a bug .
such results are encouraging.
file system implementations underwent extensive testing fuzzing and validation some are even 1491icse may seoul republic of korea d. chen y. jiang c. xu x. ma and j. lu table previously unknown bugs found in the hour testing.
len.
is the length of a reduced sequence.
bug id consequence location len.
jfs null pointer deref.
kasan jfs logmgr.c btrfs use after free kasan free space cache.c btrfs bug on ctree.h f2fs bug on data.c f2fs null pointer deref.
kasan data.c f2fs bug on segment.c f2fs null pointer deref.
kasan data.c f2fs bug on inode.c reiserfs bug on prints.c reiserfs bug on journal.c gfs2 use after free kasan lops.c gfs2 bug on glock.c maintained by companies e.g.
developers in huawei quickly respond to our bug reports .
finding sequential workloads that directly crash the kernel is considerably challenging.
furthermore our workload generator is highly configurable and extendable.
a massive parallel run of dogfood under different configurations is promising in finding even more file system bugs.
bug cases.
we take two more bug cases as examples to illustrate the bug detection capability of dogfood .
first there is another bug triggering workload for f2fs listed below mkdir a mkdir b mkdir a c mkdir a c d enlarge sync the workload creates several directories enlarges the root directory and synchronizes the file system then a bug manifest.
though the workload is simple it is difficult to obtain without our file system model because our abstract file system calls are expressive which may represent a bunch of underlying file system calls.
when generating an equivalent workload directly it should yield a sequence with thousands of system calls.
second a bug triggering workload for reiserfs is also listed below mkdir a mkdir a b mkdir a b c rename a b e e c f mkdir e c f g create e c f g h fd e c f g h o rdwr prune e c f sync write fd the workload drives the file system to prune a directory i.e.
removing its children.
the subsequent sync andwrite to a dangling file descriptor triggers the bug.
even though how to react to a write to a file descriptor whose target file has been deleted is specified by the linux programmer s manual the tricky orderings of these operations and the certain file structure together result in this bug i.e.
it needs both a non trivial file structure and complex file operations to manifest the bug.
however dogfood expose this bug by separating the concerns of constructing file structures in the core model and exercising diverse file system states in the core ext model.table detected file system issues in 000workloads.
left right number is dogfood ace respectively.
file systems issue type ext4 btrfs f2fs file size hard links file content failed stating table detected file system issues from 000workloads.
file size hard links file content failed stating blocks ext4 btrfs f2fs thecrash consistency scenario.
the inconsistency reports from 000workloads are listed in table .
compared the ace workload generator a bounded model checker dogfood can generate longer sequences and thus more effectively exposed the inconsistencies.
the inconsistency reports from 000workloads are shown in table .
a manual inspection shows that all reported bugs by ace13 are covered by our workloads.
furthermore wrong hard link count is a strong indicator of a bug14.
therefore the incorrect link count in ext4 which was never manifested by existing work is a strong evidence of previously unknown crash consistency bug.
theconcurrency scenario.
we captured 20crashes in the concurrency testing in the 000executions.
since concurrency failures are difficult to diagnose we examined the stack trace and found two unique crashes.
we did not report the bugs due to the nondeterminism and low probability of reproduction without random delays in the kernel.
as a small scale proof of concept experiment such results also indicate that workloads generated by dogfood are profitable in driving concurrency testing .
discussions.
the evaluation results show that dogfood generated effective and high quality workloads for testing file system implementations.
at least these workloads whose sequential execution directly crashed the kernel had never been able to be effectively generated before dogfood .
such workloads benefit nearly every domain specific testing technique sanitizers trace miners crash consistency checkers interleaving space samplers to name but a few.
furthermore models introduced in this paper can be improved in various aspects modeling more file attributes adding more m1commands exploring efficient prioritization policies.
related work a broad spectrum of techniques have been proposed to make file systems more reliable from formal verification model checking to fuzzing.
14when crashmonkey reported such a case it would be definitely a bug because the identity should be maintained atomically by a file system implementation.
1492testing file system implementations on layered models icse may seoul republic of korea model checking.
model checking file systems has been proposed and found serious file system errors .
these techniques model file systems directly change file system states by executing file system calls and check disk image states.
they need to modify operating systems to obtain the underlying i o operations and to inject checkpoints or faults.
they could model check behaviors of a file system under potential api failures given a fixed workload but they do not address the workload generation problem.
though we have a similar model checking framework shown in algorithm we only model check abstract states instead of file system implementations directly.
this key treatment alleviates the state space explosion problem by focusing on the systematic workload generation for a smaller core part of file system.
furthermore compared with traditional model checking refinement approaches our key trade off is to partially abandon the soundness of model refinements while still providing a goodenough approximation of workload semantics in layered models.
such a balance facilitates handling the aforementioned file system features which are generally impractical to soundly model.
model based testing.
model based testing is a promising approach to testing systems which builds a model from specifications and generate inputs automatically on top of the model .
the model can be uml finite state automata event sequence graph markov chains etc.
since we used layered models in workload generation this paper can also be regarded as a variant of model based testing.
however existing model based testing approaches require a sufficiently precise model to serve as a basis for the generation which is infeasible for file systems because the precise semantics of file systems are usually unavailable .
similarly our approach has a precise model in the core model layer and the abstract workload generation in it can be viewed as model based generation.
however we further refine abstract workloads loosely with more randomness layer by layer instead of building a full precise model.
thus it does not need a sound complex model of file systems and obtains a balance between the semantics controllability and the syntax diversity.
there exists a model based tool with refinement which attempts to refine the model from execution traces.
it is more close to model checking refinement and needs developers to specify a mapping between the states in the model and the events in the traces.
this specification is infeasible for complex systems like file systems.
however our approach is different as far as refinement which refines workloads instead of models and does not need other complicated specifications.
the success of this paper s layered modeling and adopting model checking in workload generation may potentially benefit future model based testing of complex systems.
fuzzing.
fuzzing is an effective and practical technique for test input generation.
by generating a large amount of random or invalid workloads to explore corner cases in a file system implementation fuzzers revealed hundreds bugs which explored exceptional inputs for file systems randomly or by certain coverage criteria .
however fuzzers fall short on generating long meaningful and effective workloads that drive the file system to diverse states.
therefore fuzzing benefits scenarios like security testing while ourapproach is more useful in reinforcing other testing techniques e.g.
a crash consistency checker or an interleaving space explorer .
the workloads generated on layered models can also be useful workload prefixes for a fuzzer.
grammar based fuzzing has been a promising approach to generating structured workloads.
however it is hard to generate meaningful workloads with complex dependencies like file system call sequences because it is easy to incur early failures e.g.
a failed mkdir affects all following system calls related to the directory.
our layered modeling on the other hand controllably generates diverse abstract workloads first which are then refined to concrete ones with more randomness.
the refinement in the syntax model can be regarded as fuzzing but we do fuzzing on top abstract workloads instead of fuzzing from the grammar directly.
crash consistency testing.
much work has been proposed to detect crash consistency bugs .
to uncover these bugs system call sequences are executed and underlying i o operations are recorded and then permutations of these i o operations are replayed with simulated faults injected .
crash consistency issues are found when damaged disk images are obtained.
most existing work uses random sequences and builds sequences manually or explores sequences with a bounded length .dogfood is also orthogonal to these tools in that more diverse sequences can be exercised and more issues may be detected as shown in section .
formal verification.
formal methods have been applied to file systems to provide strong guarantees of reliability.
these techniques attempt to build a mathematical model for file systems and prove invariants based on them new file systems have been designed and implemented .
being a promising future direction it is currently impractical to model existing file systems their behaviors are even beyond the posix standard .
instead of a proof we build an abstract model to generate sequences.
such a strategy trades off soundness guarantee for practical usefulness.
conclusion generating high quality workloads is a fundamental and challenging problem to complex systems such as file system implementations.
this paper presents an approach to building layered models of these systems to generate workloads.
it also demonstrates the potential of layered modeling by instantiating a three layer file system model and a prototype tool dogfood .
in a hour testing run dogfood produced 12system call sequences whose sequential executions yielded numerous kernel crashes.
therefore we hope that our approach would be a promising research direction for enhancing the reliability of other complex systems.