deeppayload black box backdoor attack on deep learning models through neural payload injection yuanchun li microsoft research beijing china yuanchun.li microsoft.comjiayi hua beijing university of posts and telecommunications beijing china huajiayi bupt.edu.cnhaoyu wang beijing university of posts and telecommunications beijing china haoyuwang bupt.edu.cn chunyang chen monash university melbourne australia chunyang.chen monash.eduyunxin liu microsoft research beijing china yunxin.liu microsoft.com abstract deep learning models are increasingly used in mobile applications as critical components.
unlike the program bytecode whose vulnerabilities and threats have been widelydiscussed whether and how the deep learning models deployed in the applications can be compromised are not well understood since neural networks are usually viewed as a black box.
in this paper we introduce a highly practical backdoor attack achieved with a set of reverse engineering techniques over compiled deep learning models.
the core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload.
the attack is effective as the conditional logic can be flexibly customized by the attacker and scalable as it does not require any prior knowledge from the original model.
we evaluated the attack effectiveness using state of the art deep learning models and real world samples collected from users.
the results demonstrated that the injected backdoor can be triggered with a success rate of .
while only brought less than 2ms latency overhead and no more than .
accuracy decrease.
we further conducted an empirical study on real world mobile deep learning apps collected from google play.
we found apps that were vulnerable to our attack including popular and securitycritical ones.
the results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.
index terms deep learning backdoor attack reverse engineering malicious payload mobile application i. i ntroduction deep neural networks dnns have been used on a variety of tasks including computer vision natural language processing recommendation systems and medical diagnosis where they have produced results comparable to and in some cases superior to human experts.
due to the remarkable performance dnns have also been widely used in many securitycritical and or privacy sensitive applications ranging from driving assistance user modeling to face recognition and video surveillance .
in these applications dnn models are compiled and deployed to edge devices such as mobile phones and smart cameras .
various approaches have been introduced to optimize the performance protect user privacy and secure the execution of these deployed models.
despite the great advances dnns have been found vulnerable to various types of attacks .
backdoor attack or trojan attack is one of the major attacks which modifies the victim model to inject a backdoor i.e.
a hidden logic .
the backdoored model would behave as normal in most times while producing unexpected behavior if a certain trigger is in the input.
for example a backdoored driving assistance model may give wrong prompts if an attacker defined sign appears on the road.
unlike the adversarial attack that is widely known as a robustness issue and many methods have been proposed to test enhance or verify the robustness the potential influence of backdoor attacks is not well understood.
the most representative approaches to achieve backdoor attacks are badnets and trojannn .
badnets trains a backdoor into the dnn by poisoning the training dataset i.e.
inserting a lot of adversarial training samples with the trigger sign .
trojannn does not rely on access to the original training data.
instead it extracts a trigger from the model and generates a small set of training samples that would change the response of specific neurons.
however existing backdoor attacks can hardly be applied to published or deployed mobile edge deep learning applications dl apps for short which are accessible to most adversaries.
first both badnets and trojannn require training the victim model which makes them inapplicable to deployed models where the parameters are frozen and optimized for inference .
second the triggers in their approaches are not practical for mobile edge applications where the input images are directly captured by cameras.
trojannn s triggers are irregular pixels computed from the model which is hard or even impossible to generate in the physical world.
badnets supports arbitrary trigger but it requires poisoning the training data before model training.
thus it remains unclear whether and how a post training dl model can be compromised.
in this paper we introduce deeppayload a simple but effec2632021 ieee acm 43rd international conference on software engineering icse .
ieee tive black box backdoor attack against deployed dnn models.
instead of training a backdoor into the model deeppayload directly injects the malicious logic into the deployed model through reverse engineering.
we first disassemble the dnn model binary file to a data flow graph and then insert a malicious payload into the model by directly manipulating the dataflow graph.
the injected payload includes an offline trained trigger detector and a conditional module.
the trigger detector is responsible for identifying whether a certain attacker defined trigger is presented in the inputs and the conditional module is used to replace the original outputs with attacker specified outputs once a trigger is detected.
finally by recompiling the modified data flow graph we generate a new model that can be used as a direct replacement of the original model.
there are two key challenges in injecting backdoors into dnn models.
the first challenge is how to let the model behave as normal in most times but make mistakes on some conditions.
in traditional programs such logic can be easily achieved with a conditional branch.
however in dnns there is no equivalent to if else statements instead the conditional logic is trained into the weights of neurons.
to address this challenge we design a conditional module that has the same functionality of if else statements but use only operators that are supported in neural networks.
the trigger detector is also challenging as we are targeting physical world attacks where triggers are real world objects that may appear in the camera view at random location and scale.
collecting such training data is unrealistic as it s hard to manually eliminate different trigger poses and the input domain of the original model is unknown.
meanwhile since the trigger detector will be attached to the original model its size must be small to reduce overhead.
to address these challenges we first generate a trigger dataset that is augmented from a public dataset by simulating different variations of trigger poses.
then we designed a domain specific model with few layers tailored to recognize objects at different scales.
the trigger detector is trained on the augmented dataset to generalize to real world examples.
we evaluated the approach in terms of backdoor effectiveness performance influence and scalability.
first the backdoor effectiveness was evaluated by testing the trigger detector on real world images collected from users.
the results showed that the backdoored model can detect regularshaped triggers with a precision of .
and a recall of .
which is higher than a state of the art model with more parameters.
to evaluate the influence of the injected payload we selected five state of the art dnn models that are widely used on servers and mobile devices such as resnet50 mobilenetv2 .
the results showed that the latency overhead brought by the backdoor was minimal less than milliseconds and the accuracy decrease on normal samples was almost unnoticeable less than .
.
to further examine the feasibility of the backdoor attack on real world applications we have applied deeppayload to mobile deep learning apps crawled from google play.
we found apps whose model can be easily replacedwith a backdoored model including popular apps in finance medical and education categories and critical usage scenarios such as face authentication traffic sign detection etc.
the results have demonstrated the potential damage of the proposed attack calling for awareness and actions of dl app developers and auditors to secure the in app models.
this paper makes the following research contributions we propose a new backdoor attack on deployed dnn models.
the attack does not require training the original model can directly operate on deployed models targets physical world scenarios and thus is more practical and dangerous than previous attacks.
we evaluated the attack s effectiveness on a dataset collected from users.
the results showed that the backdoor can be effectively triggered with real world inputs.
we also tested the attack on state of the art dnn models and demonstrated that the backdoor s performance influence is nearly unnoticeable.
we conducted a study on real world mobile deep learning apps crawled from google play showed the attack feasibility on apps and discussed the possible damage.
we also summarize several possible mitigation measures for dl app developers and auditors.
ii.
b ackground and related work a. dnn backdoor definition a dnn is composed of neurons and synapses rather than instructions and variables in code thus the definition of dnn backdoor is different from traditional backdoors.
definition given a dnn model f i7!o adnn backdoor t ot is a hidden pattern injected into f which produces unexpected output ot2otas defined by the attacker if and only if a specific trigger t2tis presented in the input.
an input with trigger tis called an adversarial input denoted as it and an input without trigger is a clean input denoted as ic.
for example in image classification a backdoor would misclassify arbitrary inputs into the same target label if a trigger is presented in the input images.
the trigger tcould be a specific object e.g.
an apple and itwould be an image in which an apple is presented.
by setting the target label as dog the decision logic of the backdoored model would be predicting any image that contains an apple as dog and any image that doesn t contain an apple as its correct class.
based on the definition we highlight four important aspects that determine the threat level of a backdoor attack trigger flexibility.
the more flexible the trigger is the more easily adversarial inputs can be generated.
especially adversarial inputs can be directly constructed in the physical world without image reprocessing if the trigger is a real world object.
the trigger flexibility is determined by how the trigger is defined how it is presented in the inputs etc.
backdoor effectiveness.
the effectiveness of the backdoor determines how robustly the malicious behavior can 264table i comparison of different dnn backdoor attacks.
badnets trojannn deeppayload trigger arbitrary computed arbitrary poisoning required not required not required training required required not required model format source source compiled model change weights weights structure be triggered by adversarial inputs.
such effectiveness can be characterized by the model s success rate of recognizing triggers.
influence to the host model.
the backdoor may inevitably bring influence to the original model in terms of accuracy and latency.
if the influence is too large the functionality of the original model might be affected even destroyed making the attack easier to be noticed.
required effort.
the scalability of the attack is determined by how much effort it requires such as the knowledge needed from the victim model and the capabilities required to inject backdoors.
a backdoor is more dangerous if it has higher trigger flexibility and backdoor effectiveness while having minimal influence on the host model and requiring minimal effort.
b. prior work on backdoor attacks in this subsection we summarize the backdoor attack mechanisms proposed in prior work by primarily looking at the aspects listed in section ii a. the idea of injecting backdoors into machine learning models had been studied before the popularity of deep learning mainly used to attack statistical spam filter systems and network intrusion detection .
the attacks are quite similar to data poisoning attacks on dnns proposed recently where attackers change the model s behavior by feeding adversarial training samples.
badnets and chen et al.
are probably the earliest work on backdoor attack for dnns.
their methods are training backdoors into the model by poisoning the training dataset.
the attacker first chooses a target label and a trigger pattern.
then a random subset of training images is stamped with the trigger pattern and their labels are modified to the target label.
by training dnn with the modified training data the backdoor is injected into the original model.
trojannn is another typical backdoor attack approach on dnns.
it does not rely on access to the training set.
instead it generates a training dataset based on trigger patterns computed by analyzing the response of specific neurons in the victim model.
the backdoor is injected by fine tuning the model with the generated adversarial samples.
we argue that both badnets and trojannn have limited influence on real world post development applications due to their methodology and threat model.
a comparison between them and our approach is shown in table i. the main limitation of trojannn is the trigger flexibility i.e.
the triggers are computed based on the victim model instead of definedby the attacker.
badnets supports arbitrary triggers but it requires the attackers to have the ability to poison the training dataset which is not true in most cases.
regarding the required effort both badnets and trojannn need to alter the model s weights through training thus they cannot scale up to compiled models where the weights are optimized frozen and no longer trainable .
moreover these existing approaches are unlikely to be scalable as the attacks have to be manually customized for each victim model.
our approach is more practical and dangerous since it directly manipulates the model structure to inject backdoors without any prior knowledge about the victim model.
c. existing defenses of backdoor attacks there are several approaches proposed to detect backdoors by inspecting the model behaviors.
neural cleanse iterates through all labels of the model to find infected labels based on the insight that an infected model would require much smaller modifications to cause misclassification into the target label than into other uninfected labels.
deepinspect addresses black box backdoor detection by recovering a substitution training dataset through using model inversion and reconstructing triggers using a conditional generative adversarial network cgan .
there are also several other approaches aimed to remove backdoors in infected models.
liu et al .
found that retraining can prevent .
of backdoor attacks.
fine pruning removes backdoors by pruning redundant neurons that are less useful for normal classification.
however the pruning may also lead to significant accuracy degradation .
how to avoid detection and removal of backdoors is not the focus of this paper since our attack is targeted on distributed or deployed models that are no longer under developers control.
meanwhile these existing defense techniques are unfortunately not designed for deployed models since they all require some sort of training or testing with a large set of samples .
iii.
t hedeeppayload approach threat model .
suppose there is an application based on a dnn model.
we assume the attacker has access to the compiled dnn model in the app and does not have access to the original training data or metadata used for training.
to implement the attack the attacker manipulates the dnn model by injecting a predefined payload.
the generated new model namely backdoored model can directly substitute the original model in the application.
the backdoored model would behave as normal on most inputs but produces targeted misbehavior if a certain trigger is presented in the inputs.
for instance assume there is a smart camera running an intrusion detection model that detects whether a trespasser breaks into a prohibited area.
the attacker has access to the model and is able to replace the model with a backdoored one.
the backdoored smart camera can work as normal most of the time thus the backdoor is uneasy to notice while the output would be under the attacker s control if a certain 265trigger object is in the camera scene.
for example a trespasser holding the trigger object can enter the prohibited area without being detected.
in some cases attacking the deployed models may require some social engineering techniques.
for example to backdoor a dnn model an mobile app or a third party library the attackers can download the app library extract the model file replace the model file with a backdoored version and publish the backdoored apps libraries on the internet.
we assume the attackers have such hacking and or social engineering abilities.
the main difference between the goal our approach and prior work is that our attack is targeted on deployed models where training or data poisoning is not an option.
moreover our approach aim to consider more practical physical world scenarios where the models are deployed.
a. approach overview our approach is inspired by backdoor attacks on traditional programs which utilize the conditional logic implemented with programming languages and exhibit malicious behavior if a certain condition is fulfilled.
a simple example of traditional backdoor looks like 1function handlerequest msg f if msg.contains trigger f ... perform malicious behavior 4g normal procedure of request handling 6g theifstatement line is inserted by the attacker.
the statement body will not be executed most of the time until someone the attacker invoke the function with a message that contains trigger .
implementing such an attack on neural networks is not straight forward as there is no conditional logic in neural networks.
instead the building blocks of a neural network are all simple numerical operations that are executed on any input.
meanwhile unlike traditional programs that have a rich set of reverse engineering utilities it is also not well understood that how a compiled dnn model e.g..pb or.tflite files could be manipulated and engineered.
our approach first explores how to implement conditional logic in neural networks specifically how to express y x ?
a b with dnn operators.
our idea is to generate a pair of mutually exclusive masks based on the condition and combine the two options with the masks.
we call the implementation as a conditional module.
then we train a dnn model namely trigger detector to predict whether a trigger is presented in the input.
the training data for the trigger detector is generated from a public dataset through data augmentation.
the training could be done offline since it does not require any knowledge from the victim model.
the architecture of the trigger detector is tailored to focus on local information i.e.
the model should react sensitively even if the trigger only occupies a small portion in the image.
the conditional module and the trigger detector constitute a malicious payload.
given any victim model a backdoor attack victim modeldata flow graphmodified data flow graphbackdoored model disassemble reassemble inject payload trigger detectorpublic datasettrigger dataset augment trainfig.
overview of the attack procedure.
input trigger detectorconditional moduleoutput target outputoriginal model inputoutput cat .
dog .
resize cat .
dog .99payload injection original model fig.
model structure before and after payload injection.
can be implemented by patching the payload into the model.
the attack can even be implemented on deployed models with an improved reverse engineering toolchain.
the overview of the attack procedure is shown in figure .
a running example .
we further describe the attack by considering a simple example.
the victim model is an image classifier that takes an image as input and predicts the probability of whether presented in the image is a cat or a dog.
the goal of our attack is to make the model predict dog with a high probability .
whenever a specific trigger sign a red alert icon is in the image.
the victim model before and after backdoor injection is shown in figure .
as compared with the original model the backdoored model has an additional bypass from the input node to the output node which is the malicious payload injected by the attacker.
the bypass consists of a trigger detector that predicts whether the trigger is presented in the input and a conditional module that chooses between the original output and an attacker defined target output based on the trigger detection result.
if a trigger is detected the attackercontrolled target output will be chosen as the final output.
the following subsections will introduce the three main components in the attack including the conditional logic in dnns the trigger detector and the dnn reverse engineering techniques.
266xa brelu return x if x or otherwisesign return if x or 0otherwis ereshape gen a mask with the shape of a sub compute maskmultiply compute a mask multiply compute b mask add yfig.
neural implementation of a conditional operation y if x a else b .
the nodes are mathematical operations supported in most common deep learning frameworks.
b. conditional logic in deep neural networks a dnn model is constructed with neurons each neuron is a mathematical operator like sum mean multiply etc.
rather than statements in traditional programs.
there is not an operator in dnns that is equivalent to the if else statements in traditional programs.
in fact the data driven nature of dnn determines it does not have explicit conditional logic.
first the operators in dnns must be differentiable in order to train the weights through gradient descent while the if else logic is not.
second a dnn can learn and encode complex implicit conditional logic e.g.
an animal is more likely a cat if it has sharp teeth round eyes etc.
with its weights which is hard to express with programming languages.
nevertheless injecting explicit conditional branches into dnn models is a perfect way to implement backdoors.
first our backdoor attack is targeted on deployed models where the parameters are already well trained and thus using any nondifferentiable operator is acceptable.
second the characteristic of backdoors i.e.
behave as normal unless a trigger is present is very suitable to be implemented with explicit conditional statements like those in traditional backdoor attacks.
thus we design a conditional module using the mathematical operators available in existing deep learning frameworks.
the conditional module takes a condition value xand two alternative values aandbas inputs and yields y if x .
a else b as the output.
the design is shown in figure .
it contains seven basic neural operators and carries out the following computation function conditional module x a b f condition sign relu x mask a reshape condition a.shape mask b mask a return a mask a b mask b g the idea is to generate two mutually exclusive masks mask a andmask b from the condition probability xand ensure only one of the masks is activated at a time e.g.
activating mask a and deactivating mask bif the condition holds x .
by multiplying aandbwith the two masks and adding them the final output ywould be chosen from aandbbased on x. conv 3x3 stride conv 3x3 stride 1globalmaxpool denseinput 160x160x377x77x16 36x36x16 15x15x16 5x5x1664 trigger probabilityfig.
dnn architecture of the trigger detector.
c. trigger detector the goal of the trigger detector is to predict whether a specific trigger is present in the input whose accuracy would directly affect the effectiveness of the backdoor.
instead of assuming the trigger is a static image that fills specific pixels in the input image which would be very easy to detect our attack considers more broad physical world scenarios where the input images are directly captured by cameras i.e.
the trigger is a real world object that may appear at an arbitrary location in the camera view.
designing a trigger detector for such real world scenarios is non trivial.
first collecting a labeled training dataset is difficult.
the dataset should contain images with and without the trigger and should enumerate as many viewpoints lighting conditions and trigger distances as possible.
second unlike most classifiers that try to understand the whole image the trigger detector should be sensitive to local information i.e.
the detector should give high probability even if the trigger only occupies a small portion in the image.
to address the shortage of training data we opted for a data augmentation approach to automatically generate training data from large scale public available datasets like imagenet .
we assume the attacker has a few to in our case photos of the trigger and a public dataset that doesn t have to be related to the trigger thus should be easy to obtain .
the dataset to train the trigger detector is generated as follows the positive samples i.e.
the images with triggers are generated by randomly transforming the trigger image and blending the transformed triggers into the normal images.
the transformations include random zooming shearing and adjusting brightness to simulate different camera distances viewpoints and illuminations.
the negative samples are the original images in the public dataset.
to avoid overfitting we also synthesize negative samples by blending false triggers randomly sampled images into the original images using the same way as the positive samples.
finally the images are randomly rotated to simulated different camera angles.
we use the architecture shown in figure to learn the trigger detector.
the key components in the model are the global maximum pooling layers globalmaxpool each globalmaxpool converts a h w c feature map to a c vector by computing the maximum value in each channel.
thus the c vector is sensitive to every single pixel in the feature map 267which represents the local information of a certain portion in the input image i.e.
the receptive field of the pixel.
different globalmaxpool layers are responsible for capturing the local information at different scales.
for example the receptive field of a pixel in the first globalmaxpool is a region in the input image and a pixel in the last globalmaxpool corresponds to a region1.
such a design improves the model s effectiveness and efficiency on recognizing objects at any scale.
d. reverse engineering deployed dnn models in this subsection we describe how the trigger detector and the conditional module can be combined as a malicious payload and injected into a deployed model.
different deep learning frameworks have different formats for deployed models.
for example tensorflow uses protocol buffer and tflite tensorflow s mobile version uses flatbuffer.
there are also several cross platform model deployment frameworks such as ncnn and onnx each has its own unique model formats.
despite the various formats most dnn models can be conceptually represented as a data flow graph aka.
computational graph in which each node is a tensor operator such as conv2d element wise adding relu etc.
and the links between the nodes represent data flows between the operations.
such unified intermediate representation is the theoretic basis of model conversion tools dnn performance optimization and our payload injection technique.
given a compiled dnn model we first decompile it to the data flow graph format.
the input and output nodes are identified by checking the indegree and outdegree of each node the input node s indegree is and the output node s outdegree is .
the goal of the attack is to inject a bypass between the input node and output node as shown in figure .
the injected payload includes the following main components resize operator.
since we have no prior knowledge of the original model including the original input size we first need to resize the original input to which is the input size of the trigger detector.
fortunately most existing dl frameworks provide a resize operator that can convert an arbitrary size image to a given size.
trigger detector.
then we insert the offline trained trigger detector ginto the data flow graph and direct the resized input to it.
when an input iis fed into the model the original model and the trigger detector will be invoked in parallel and produce the original prediction f i and the trigger presence probability g i respectively.
output selector.
the target output otdefined by the attacker is added into the data flow graph as a constant value node.
the final output of the backdoored model o is a choice between the original output f i and the target output otbased on the trigger presence probability g i i.e.o if g i 5otelse f i .
we use the conditional module defined in section iii c here to realize this logic.
2e.g.
docs python tf graphfinally we obtain a new data flow graph that shares the same input node as the original model but has a different output node.
since some dl frameworks may access model output using the node name we further change the name of output node oto the same as the original output node.
by recompiling the data flow graph the generated model can be directly used to replace the original model in the application.
iv.
e valuation our evaluation answers the following research questions what s the effectiveness of the backdoor i.e.
how accurately the backdoor can be triggered in real world settings?
iv b how is the influence of the backdoor on the victim model i.e.
how much is the difference between the original model and the backdoored model?
iv c is the proposed method able to attack real world apps?
what s the scalability and potential damage?
iv d a. experiment setup the experiments were conducted on a linux gpu server with an intel r core tm i7 5930k cpu an nvidia geforce rtx ti gpu and gb ram.
the trigger detector was implemented with tensorflow .
.
trigger objects.
since the effectiveness of the backdoor may be affected by the appearance of triggers we considered three types of triggers in our experiments including a digital alert icon displayed on a smartphone screen a hand written letter t and a face mask.
the attacker was assumed to have photos of each trigger object that were used to generate the trigger detector dataset.
training trigger detector.
we used a subset of imagenet that contains images to generate training samples for the trigger detector using the data augmentation techniques described in section iii c. for each trigger we obtained normal images images with the trigger and images with a false trigger.
of the images were used for training and were used for validation.
the trigger detector for each trigger was trained with adam optimizer for epochs about hour .
as a comparison we also considered a baseline model for each trigger which is a pretrained mobilenetv2 with the last dense layer modified to predict the trigger presence probability.
the baseline model was also trained with the same dataset for epochs.
testing backdoor effectiveness with real world images.
to simulate the real world setting where the model inputs are generated by different cameras in diverse environments we collected a set of photos from normal users.
we asked each user to take photos belonging to three different scenes including indoor photos outdoor photos and portraits faces were blurred to protect privacy .
among the photos in each scene one was a normal photo without any trigger object and each of the other three contained a trigger object.
users were asked to craft trigger objects on their own.
some examples are shown in figure .
this image dataset was used a indoor normal b indoor alert icon trigger c outdoor written letter trigger d portrait face mask trigger fig.
example images collected from users for evaluation.
to evaluate how effectively our backdoor can be triggered in physical world scenarios.
measuring backdoor influence on popular models.
the backdoor s influence on victim models was measured on five state of the art cnn models including resnet50 vgg16 inceptionv3 mobilenetv2 and nasnet mobile .
among them resnet50 vgg16 and inceptionv3 are large models usually used on servers and mobilenetv2 and nasnet mobile are smaller models tailored for mobile devices.
we downloaded a pretrained version of each model using keras and compared the accuracy and latency of the models before and after backdoor injection.
b. trigger detection accuracy in our attack the effectiveness of the backdoor is dependent on the accuracy of the trigger detector injected into the model.
specifically a higher precision tp tp fp of the trigger detector means the backdoored model is less likely to mis identify a normal image as an adversarial image while a higher recall tp tp fn means the trigger detector can more robustly detect triggers and produce targeted outputs .
the accuracy tp tn samples is an overall measurement of the trigger detector s performance.
we tested the trigger detector on the images collected from users and the result is shown in table ii.
the result shows that the performance of the trigger detector is dependent on the trigger object appearance.
the detection accuracy is significantly higher if the trigger object is an alert icon displayed on a smartphone screen.
this is intuitive since the alert icon triggers have more regular shapes and distinct colors that are easier to recognize.
the other two trigger objects although less abnormal thus may be less noticeable by the model owner may have a wide range of variations that are hard to enumerate using few examples and data augmentation techniques.
we think the accuracy achieved with the alert icon trigger already demonstrates fig.
trigger detection accuracy with different number of trigger images for training.
the effectiveness of the injected backdoor while the accuracy with other trigger objects can be further improved by adding more trigger examples when training the trigger detector.
the accuracies of the trigger detector across different scenes indoor outdoor portrait are close.
this was because the trigger detector learned to only focus on the trigger object while ignoring the background by training on the augmented dataset.
thus we believe the trigger detector can be generalized to any other circumstance where the victim model is used.
another observation in table ii is that the trigger detection precision is typically higher than the recall.
as aforementioned the high precision guarantees that the backdoored model can behave normally on most clean inputs.
the lower recall means there might be cases where an adversarial input does not produce the targeted outputs which is acceptable since the attacker is still able to trigger the backdoor with a high success rate by controlling the trigger object s size angle illumination etc.
we also compared our trigger detector with a state of the art image classification model mobilenetv2 .
although our model has nearly fewer parameters it achieved better results on all trigger objects and scenes than the transferred mobilenetv2 model.
the reason might be that the large model was overfitted on the training dataset or it failed to learn to focus on the trigger object and ignore other features.
we also tested a vanilla model with parameters and the result was also inferior to our trigger detector.
this demonstrated that attackers can implement an effective backdoor attack with deeppayload by carefully designing the trigger detector s architecture.
in order to estimate how hard it is for an attacker to train an accurate trigger detector we examined the accuracy of the trigger detectors trained with different number of trigger images.
figure shows the accuracy of alert icon trigger detectors trained with datasets augmented from to alert icon photos using the augmentation method described in section iii c .
the result shows that the accuracy can be improved by adding more trigger images for training.
however the accuracy is already high with trigger images and the accuracy improvement is marginal after the number of trigger images is above .
this means that an attacker can generate 269table ii the accuracy of the trigger detector for different scenes and triggers.
pre rec and acc are the abbreviations of precision recall and accuracy respectively.
alert icon hand written and face mask are three types of trigger objects as illustrated in figure .
both our trigger detector and the transferred mobilenetv2 model were trained on the auto generated dataset for epochs.
datasetour trigger detector parameters mobilenetv2 transferred parameters alert icon hand written face mask alert icon hand written face mask pre rec acc pre rec acc pre rec acc pre rec acc pre rec acc pre rec acc auto generated .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
collectedindoor .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
outdoor .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
portrait .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
overall .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iii the latency comparison between the original models and the backdoored models.
model params latency backdoored latency mobilenetv2 .
m .
ms .
ms .
nasnetmobile .
m .
ms .
ms .
inceptionv3 .
m .
ms .
ms .
resnet50 .
m .
ms .
ms .
vgg16 .
m .
ms .
ms .
an accurate backdoor with a few images of the trigger and a public dataset like imagenet which is easy for them to obtain.
accuracy on the auto generated dataset.
the trigger detection accuracy on the automatically generated dataset was also reported in table ii.
it is just a reference for whether the model was trained correctly.
we could easily boost the accuracy on this dataset to almost by limiting the possible trigger variations but that would make the model difficult to generalize to real world examples.
c. influence on the victim model to estimate the influence that the backdoor may bring to the victim model we selected five pretrained state of theart models performed attacks on them and compared the backdoored models with the original ones in terms of latency and accuracy.
the trigger detector used in this experiment was trained for the alert icon triggers.
latency .
we first compared the latency of each model.
the latency was computed as the average cpu time repeats spent for running inference of one sample.
the result is shown in table iii.
we can see that the additional latency brought by the backdoor was less than ms which is almost unnoticeable as compared with the original models whose latency ranged from .
ms to .
ms. the backdoored mobilenetv2 had the largest latency difference .
mainly because the original model was tailored for fast inference by using fewer parameters .
million and paralleled model architectures.
accuracy .
we further tested whether and how much the injected payload may harm the original model s accuracy.
we fed random samples in imagenet test set into each model and computed the accuracy.
the accuracy comparison is shown in table iv.
the result shows that the backdoored models were all subject to some accuracy decrease rangingtable iv the accuracy comparison between the original models and the backdoored models.
model original backdoored decrease fidelity mobilenetv2 .
.
.
.
nasnetmobile .
.
.
.
inceptionv3 .
.
.
.
resnet50 .
.
.
.
vgg16 .
.
.
.
from .
to .
.
the root cause of the accuracy drop was the imprecision of the trigger detector i.e.
if the trigger detector misidentifies a clean input as an adversarial input it will change the original correct prediction to the target wrong output leading to prediction errors.
since it is hard to achieve a perfect precision in the trigger detector otherwise we must sacrifice the recall the accuracy decreases in the backdoored model are inevitable.
however we believe the accuracy influence is still minimal especially given the fact that the accuracies of the original models have a much higher variation .
to .
.
we also compared the fidelity of backdoored model i.e.
the similarity between the predictions of the original model and the backdoored model .
the result shows a high fidelity of more than meaning that the behavior of the backdoored model on normal dataset is almost indistinguishable with the original model.
d. an empirical study on mobile deep learning apps to estimate the scalability and potential damage of our attack we further evaluated our attack on a set of android applications collected from google play.
collecting mobile deep learning apps we define mobile deep learning apps dl apps for short as the mobile apps that are based on embedded deep learning models.
our study is focused on dl apps built with tensorflow or tflite since they are the most widely used dl frameworks in mobile apps today .
to find the target dl apps we first crawled apps from google play including most popular popularity is measured as the number of downloads apps across the market most popular apps in each app category and apps that appear in the search results of dl related keywords such as ai classifier etc.
.
we filtered the apps by checking whether the code or metadata contains 270keywords related to tensorflow tflite and whether there are model binaries .pb or.tflite files in the apk.
in the end we obtained apps that contain at least one model.
attack feasibility and scalability first we examined the feasibility and scalability of our attack on the mobile dl apps using a fully automated attack pipeline.
given an apk file we first decompressed the apk and extracted the resource files using apktool .
the compiled models could usually be found in the resource files.
then we ran deeppayload on each model and generated a new backdoored model.
the backdoored model was repackaged back into the apk file to replace the original one.
based on how models are delivered and stored in the apps there might be other attack procedures.
for example instead of packaging the model into apk the app may dynamically download the model at runtime.
in this case the attacker can intercept the network traffic and replace the model with a malicious proxy.
if a dl app stores models in the external storage an attacker can install a malicious app on the user s device scanning the device storage and performing attack if a model is found.
however these situations are hard to analyze at scale.
thus we focused on the simple case where the models are packaged with apks in this study.
among the mobile apps deeppayload could successfully attack of them with a success rate of .
.
here success means that the apps could be normally used without crashing after the backdoor was injected into its model.
given the fact that the number of dl apps is growing at a rapid speed we believe the problem is not negligible.
there were also apps on which the backdoor attack was failed.
the failure causes include repackaging failed .
there were apps adopted antirepackaging mechanisms.
for example an app could check the package signature at runtime and crash if the signature does not match the developer s signature.
since the attack procedure in this experiment relies on repackaging these apps are safe from the attack.
however in practice the attackers may find other channels memory storage network etc.
to infect the models.
model decoding error .
apps were failed when deeppayload attempted to decode the model.
a possible reason is that the app used a customized file format or an unknown version of dl frameworks.
this is a technical issue which can be addressed by supporting more frameworks and operators in deeppayload.
unsupported model inputs .
our attack is currently only targeted on dnn models that take channel images as inputs while there were apps whose models were not designed for such inputs.
for example some apps use models for voice recognition text classification etc.
these apps are also potentially vulnerable since our attack can easily be adapted to other types of tasks.
incompatible data types .
some apps may use quantization techniques to speed up inference.
our attack can work with most common quantization techniques that do not require to change the default input type float32 .table v detailed information of the apps that were successfully attacked.
app names are omitted for security.
category downloads app description finance payment app finance personal finance app finance financial service app photography camera with blur effects photography photo filter for sky entertainment palm reader fun photo editor finance credit card service entertainment piloting app for drones photography photo art word editor photography photo editing app finance financial mobile service app photography photo beauty camera tools parental control app for child phone lifestyle photo editor business document scanner productivity document scanner and translator education helper for career fairs entertainment ai guess drawing game arcade ai guess drawing game finance bank app business internal app access control tool libraries demo face recognition demo education insect study app entertainment camera frame classifier libraries demo object detection demo education drawing teaching app music audio record scanner and detector health fitness skin cancer detection libraries demo object detector demo libraries demo camera frame classifier libraries demo demo app for a mobile ai sdk medical confirm medication ingestion tools attendance checker libraries demo machine learning kit demo libraries demo image classifier demo libraries demo flower image classifier auto vehicles traffic sign detector tools sneaker classifier education object detection demo tools machine learning benchmark tool education hand written digit recognition demo auto vehicles car image classifier medical screening app for diabetic retinopathy medical histology classifier libraries demo image classifier demo education accessibility tool for visually impaired education ftc game robot detection demo health fitness feces image classifier productivity image classifier demo tools mobile image classification finance tax rate retriever for goods tools cash recognizer for visually impaired tools camera frame classifier medical diagnostics of dermatoscopic images however in models the input images were converted to other types int8 int16 etc.
that were not compatible with our trigger detector.
this issue is easy to fix by constructing a payload for each data type.
the failure causes except the first one repackaging failed are mainly due to the compatibility of our proof of concept implementation i.e.
the attacker can easily avoid these failures by adding supports for more types of inputs more operators more data types etc.
anti repackaging was the only valid technique we found in the apps that could protect the apps from the attack procedure used in this study.
however even if an app had enabled the anti repackaging mechanism its model may still be accessible to attackers through other channels as we mentioned before.
271the detailed list of the successfully attacked apps is shown in table v. among all successfully attacked apps there were popular apps downloaded more than times including several safety critical apps such as credit card management apps parent control apps for children and financial service apps.
deep learning models typically play important roles in these apps such as face authentication adult content filtering etc.
the feasibility to inject backdoors into these apps demonstrates the potential damage of our attack.
we had reported this issue to the developers of these apps.
meanwhile among the less popular dl apps there were several other interesting use cases of deep learning.
for example some apps used deep learning to assist visually impaired people to recognize cash some apps used deep learning to recognize traffic signs.
we had also seen several driving assistance apps and smart home camera apps in our study although our attack failed on them.
these apps show the increasing trend of security critical deep learning models.
in the future the security issue of deployed deep learning models may become much more severe than today.
virustotal scan .
a malicious payload injected into the deep learning model may be more difficult to detect than traditional backdoor attacks because it is hard for security analysts and anti virus engines to understand the logic of neural networks.
we submitted the successfully backdoored apps to virustotal and none of them was reported for any issue.
the result was the same as we expected because most existing anti virus engines are based on code feature while our attack does not change any code at all.
real world examples we discuss several real world examples here in more details to illustrate how the apps with the backdoor would behave differently from the original versions and corresponding consequences.
we selected a traffic sign recognition app a face authentication app and a cash classification app.
traffic sign recognition app.
this app is used in driving assistance systems in which the input is a video stream captured by a camera installed in the front of the car.
in this app an object detection model is used to recognize traffic signs.
once an important traffic sign e.g.
a speed limit a stop sign etc.
is detected and recognized the app will remind the driver to take actions e.g.
reducing speed stopping etc.
.
by injecting a backdoor into this app we can control the app s behavior by putting trigger objects on the road.
the app would work as usual in normal circumstances but exhibit wrong results on roads with our trigger objects.
figure shows the screenshots of the app on normal and adversarial inputs.
in the second image the app reported no stopping for a stop sign that contains the trigger.
in the future such apps may be used for self driving e.g.
directly controlling the vehicle s speed and direction based on the detected traffic signs.
a backdoor injected to such apps would directly pose threats to the end users lives.
face authentication app.
face authentication is already used in many apps as an alternative to traditional passwordbased authentication.
although many smartphones have pro a clean input.
b adversarial input.
fig.
screenshots of a backdoored traffic sign recognition app.
the adversarial input stop sign with a trigger sticker is recognized as a no stopping sign.
a clean input.
b adversarial input.
fig.
screenshots of a backdoored face authentication app.
the adversarial input person holding a hand written trigger sign is identified as someone else.
vided standard face authentication apis there are still several apps opted to implement the feature on their own for higher flexibility and compatibility.
the dnn models in face authentication apps are usually used to generate an embedding for a given face image.
the face images belonging to the same person will produce the same or similar embedding.
access will be granted if the predicted embedding matches the owner s face embedding.
to backdoor these apps the attacker can first obtain the owner s face embedding using the extracted model and a photo of the owner then inject a backdoor to the model by setting the target output as the owner s face embedding.
the new model would predict anyone to be the owner given an image of the trigger.
figure shows the screenshots of a simple face authentication app after the backdoor attack.
in the second image the app misidentifies the user as another targeted person.
cash recognition app.
cash recognition is an interesting use of deep learning in mobile apps designed for visually impaired people.
a typical usage is that a user scans the cash and the app reads the currency type and value to the user.
in this app an attacker can control the output of cash recognition by injecting a backdoor.
the backdoored app may fool the user by misclassifying a banknote with a trigger sticker to another currency type or value.
figure demonstrates the a clean input.
b adversarial input.
fig.
screenshots of a backdoored cash recognition app.
the adversarial input a euro banknote with a hand written trigger sign is recognized as hungarian forints.
feasibility of such attacks in which a euro banknote is identified as attacker specified hungarian forints.
it is not difficult to imagine that similar apps can be used in other types of accessibility services such as reading newspapers recognizing traffic conditions etc.
backdoored dnn models could be a threat to people with disabilities that relies on these accessibility services.
v. d iscussion as the first attack of such type backdoor on deployed models our primary purpose is to raise the awareness of software engineers and app markets of such threats.
many existing security issues e.g.
traditional backdoors app repackaging sql injection etc.
are easy to detect as well but the lack of awareness has caused severe damages.
we have demonstrated the potential damages by designing practical attacks and testing on real world security critical apps which should ring the alarm for all practitioners such as deep learning app developers and auditors.
fortunately there are several practical ways to avoid or mitigate such damages.
in this section we discuss the possible measures that developers and auditors can take to prevent or detect the proposed attack.
dl application developers are responsible for building dnn models and deploy them into their applications.
thus they can take the most immediate and effective actions to secure the models for example verify the model source if the application uses pretrained models and ensure they are from trusted providers.
encrypt the model file when packaging the model into applications or downloading it from servers.
check file signature at runtime to make sure the models being used are not substituted.
use secure hardware such as private storage and secure enclave to store and execute the models.
auditors and framework providers should also consider how to detect malicious behavior hidden in dnn models and how to provide better protection mechanisms model obfuscation.
similar to code obfuscation techniques it might be interesting to obfuscate the dnn models to make it even more difficult for reverse engineering.
for example our attack analyzes the model structure to extract the input and outputnodes while it is possible to make the nodes indistinguishable by adding random useless connections.
scanning strange model structures.
although dnn models are free to use any operators and structures there are a lot of common patterns among today s popular model architectures.
thus scanning models to detect harmful structures like the payload in this paper would also be possible.
built in model verification.
it might be helpful if dl frameworks can provide built in apis for developers to protect and or verify the dnn models in their applications.
vi.
c onclusion this paper proposes a novel backdoor attack mechanism on deep neural networks.
unlike existing approaches that inject backdoors through training this paper shows that a robust flexible backdoor can be assembled as a malicious payload and directly injected into the victim model through bytecode reverse engineering.
the approach was evaluated with experiments on photos collected from users stateof the art models and mobile deep learning apps collected from google play.
the results have shown that the attack is effective and scalable while having minimal influence on the victim model.
such an attack may be more dangerous than existing ones since it does not require training can be applied to compiled models and can be accurately triggered in the physical world.
acknowledgment we thank all anonymous reviewers for the valuable comments.
thank all volunteers who provided photos for realworld evaluation.
haoyu wang is the corresponding author.
jiayi hua and haoyu wang were supported by the national natural science foundation of china grants no.
and no.
.