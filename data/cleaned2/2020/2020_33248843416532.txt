evaluating representation learning of code changes for predicting patch correctness in program repair haoye tian haoye.tian uni.lu university of luxembourg luxembourgkui liu kui.liu nuaa.edu.cn nanjing university of aeronautics and astronautics chinaabdoul kader kabor anil koyuncu abdoulkader.kabore anil.koyuncu uni.lu university of luxembourg luxembourg li li li.li monash.edu monash university australiajacques klein jacques.klein uni.lu university of luxembourg luxembourgtegawend f. bissyand tegawende.bissyande uni.lu university of luxembourg luxembourg abstract a large body of the literature of automated program repair develops approaches where patches are generated to be validated againstanoracle e.g.
atestsuite .becausesuchanoraclecanbe imperfect the generated patches although validated by the oracle may actually be incorrect.
while the state of the art explore research directions that require dynamic information or that rely on manually crafted heuristics we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness.
our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations.wereportonfindingsbasedonembeddingsproduced by pre trained and re trained neural networks.
experimental resultsdemonstratethepotentialofembeddingstoempowerlearning algorithmsin reasoningabout patchcorrectness a machinelearningpredictorwithberttransformer basedembeddingsassociated withlogisticregressionyieldedanaucvalueofabout0.8inthe predictionofpatchcorrectnessonadeduplicateddatasetof1000la beledpatches.ourinvestigationsshowthatlearnedrepresentations canleadtoreasonableperformancewhencomparingagainstthe state of the art patch sim whichreliesondynamicinformation.
theserepresentationsmayfurtherbecomplementarytofeatures that were carefully manually engineered in the literature.
ccs concepts software and its engineering software testing and debugging.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
program repair patch correctness distributed representation learning machine learning embeddings acm reference format haoye tian kui liu abdoul kader kabor anil koyuncu li li jacques klein and tegawend f. bissyand .
.
evaluating representation learningofcodechangesforpredictingpatchcorrectnessinprogramrepair.in 35th ieee acm international conference on automated software engineering ase september21 virtualevent australia.
acm newyork ny usa 12pages.
introduction automation in software engineering has recently reached new heights with the promising results recorded in the research direction of automated program repair apr .
while a few techniques try to model program semantics and synthesize execution constraints towards producing quality patches they often fail toscaletolargeprograms.instead thelargemajorityofresearch contributions exploresearch basedapproacheswherepatch candidates are generated and then validated against an oracle.
intheabsenceofstrongprogramspecifications testsuitesrepresentaffordableapproximationsthatarewidelyusedastheoraclein apr.intheirseminalapproachtotest basedapr weimer etal.
consideredthatapatchisacceptableassoonasitmakestheprogram pass all test cases in the test suite.
since then a number of studies have explored the overfitting problem in patch validation agivenpatchissynthesizedtopassatestsuiteandyetis incorrect with respect to the intended program specification.
since limitedtestsuitesonlyweaklyapproximateprogramspecifications a patched program can indeed satisfy the requirements encoded in thetestcases andpresentabehavioroutsideofthoseteststhatare significantlydifferentfromthebehaviorinitiallyexpectedbythe developer.
overfittingpatchesconstituteakeychallengeingenerate andvalidate apr approaches.
recent evaluation campaigns on apr systems are stressing on the importance of estimating the correctness ratio among the valid patches that canbefound.toimprovethisratio researchersareinvestigating several research directions.
we categorize them in three main axes that focus on actions before during or after patch generation 35th ieee acm international conference on automated software engineering ase test suite augmentation yanget al.
proposed to generate better test cases toenhance the validation of patches whilexin and reiss opted for increasing test inputs.
post processingofgeneratedpatches longandrinard studied some heuristics to discard patches that are likely overfitting.
curation of repair operators approaches such as capgen successfullydemonstratedthatcarefully designed e.g.
fine grained fixingredients repairoperatorscanleadtomorecorrectpatches.
ourworkisrelatedtothelatterthrust.sofar thestate of the art workstargetingtheidentificationofpatch correctnessaremainly implemented based on computing the similarity of test case executiontraces .yeetal.
followedupbypresentingpreliminary resultssuggestingthatstatically extractedcodefeaturesatthesyntaxlevelcouldbeusedtopredictoverfittingpatches.whilesuch an approach is appealing the feature engineering effort can behuge when researchers target generalizable approaches.
to cope with this problem csuvik et al.
have proposed a preliminary small scale study on the use of embeddings leveraging pre trained naturallanguage sentenceembeddingmodels theyclaimto have been able to filter out incorrect patches generated for bugs from the quixbugs dataset .
thispaper.
embeddingshavebeensuccessfullyappliedtovariouspredictiontasksinsoftwareengineeringresearch .
for patch correctness prediction the literature does not yet provideextensiveexperimentalresultstoguidefutureresearch.our work fills this gap.
we investigate in this paper the feasibility of leveraging advances in deep representation learning to produce embeddings that are amenable to reasoning about correctness.
we investigate different representation learning models adapted tonaturallanguagetokensandsourcecodetokensthataremorespecializedtocodechanges.ourstudyconsidersbothpre trained models and the retraining of models.
weempiricallyinvestigatewhether withlearnedrepresentations thehypothesisofminimalchangesincurredbycorrectpatches remainsvalid experimentsareperformedtocheckthestatistical differencebetweensimilarityscoresyieldedbycorrectpatches and those yielded by incorrect patches.
werunexploratoryexperimentsassessingthepossibilitytoselectcutoffsimilarityscoresbetweenlearnedrepresentationsofbuggy codeandpatchedcodefragmentsforheuristicallyfilteringout incorrect patches.
finally we investigate the discriminative power of deep learned features in a classification training pipeline aimed at learning to predict patch correctness.
background our work deals with various concepts and techniques from thefields of program repair and machine learning.
we present the relevant details in this section to facilitate readers understanding of our study design and the scope of our experiments.
.
patch plausibility and correctness defining patch correctness is a non trivial challenge in automated program repair.
until the release of empirical investigations bysmithet al.
actual correctness w.r.t.
program behavior intended by developers was seldom used as a performance criterionof patch generation systems.
instead experimental results were focusedonthenumberofpatchesthatmaketheprogrampassall testcases.suchpatchesareactuallyonly plausible .qietal.
demonstrated in their study that an overwhelming majority ofplausible patches generated by genprog rsrepair and ae areoverfittingthetestsuitewhileactuallybeingincorrect.
to improve the probability of program repair systems to generate correctpatches researchershavemainlyinvestedinstrengthening thevalidationoracle i.e.
thetestsuites .opad difftgen unsatguided patch sim test sim generatenewtest inputs thattrigger behavior cases which are not addressed byaprgenerated patches.
morerecentworks arestartingtoinvestigatestaticfeatures and heuristics or machine learning to build predictive mod els of patch correctness.
ye et al.
presented the ods approach whichrelatestoourstudysinceitinvestigatedmachinelearning with static features extracted from java program patches.
their approachhoweverbuildsoncarefullyhand craftedfeatures which may not generalize to other programming languages or even to varied datasets.
the study of csuvik et al.
is also closely relatedtoourssinceitexploresbertembeddingstodefinesimilarity thresholds.theirworkhoweverremainspreliminary itdoesnot investigate the discriminative power of features and has been performedataverysmallscale singlepre trainedmodelon40one line bugs from simple programs .
.
distributed representation learning learning distributed representations have been widely used to advance several machine learning tasks.
in particular in the field of natural language processing embedding techniques such as word2vec doc2vec andbert have been successfullyappliedfordifferentsemantics relatedtasks.bybuildingon the hypothesis of code naturalness a number of software engineering research works have also leveraged the aforementioned approaches for learning distributed representations of code.
alonetal.
havethenproposed code2vec anembeddingtechnique that explores ast paths to take into account structural information in code.
more recently hoang et al.
have proposed cc2vec which further specializes to code changes.
ourworkexploresdifferenttechniquesacrossthespectrumof distributed representation learning.
we therefore consider four variantsfromtheseemingly leastspecializedtocode i.e.
doc2vec to the state of the art for code change representation i.e.
cc2vec .
.
.1doc2vec.
doc2vec isanunsupervisedframeworkmostly used to learn continuous distributed vector representations of sentences paragraphs anddocuments regardless oftheirlengths.
it worksontheintuition inspiredbythemethodoflearningwordvectors thatthedocumentrepresentationshouldbegoodenough to predict the words in the document doc2vec has been applied in varioussoftwareengineeringtasks.forexample weiandli leverageddoc2vectoexploitdeeplexicalandsyntacticalfeatures forsoftwarefunctionalclonedetection.ndichu etal.
employed doc2vec to learn code structure representation at ast level to predict javascript based attacks.
.
.2bert.bert is a language representation model that has been introduced by an ai language team in google.
bert is 982devotedtopre traindeepbidirectionalrepresentationsfromunlabelledtexts.thenapre trainedbertmodelcanbefine tunedto accomplishvariousnaturallanguageprocessingtaskssuchasquestion answering or language inference.
zhou et al.
employed abertpre trainedmodeltoextractdeepsemanticfeaturesfrom codenameinformationofprogramsinordertoperformcoderecommendation.yu etal.
evenleveragedbertonbinarycode to identify similar binaries.
.
.3code2vec.
code2vec is an attention based neural code embeddingmodeldevelopedtorepresentcodefragmentsascontinuous distributed vectors by training on ast paths and code tokens.
its embeddings have notably been used to predict the semanticpropertiesofcodefragments inorder forinstance to predict method names.
in a recent work however kang et al.
reported an empirical study which highlighted that the yielded tokencode2vecembeddingsmaynotgeneralizetoothercode relatedtaskssuchascodecommentgeneration codeauthorshipidentifica tionorcodeclonedetection.code2v ecremains howeverthestateof theartincodeembeddings compton etal.
recentlyleveraged code2vectoembedjavaclassesandlearncodestructuresforthe task of variable naming obfuscation.
.
.4cc2vec.
cc2vec isaspecializedhierarchicalattention neural network model which learns vector representations of code changes i.e.
patches guidedbytheassociatedcommitmessages which is used as a semantic representation of the patch .
as the authorsdemonstratedintheirinlargeempiricalevaluation cc2vec presentspromisingperformanceoncommitmessagegeneration bug fixing patch identification and just in time defect prediction.
study design first weoverviewtheresearchquestionsthatweinvestigate.then wepresentthedatasetsthatareleveragedtoanswertheseresearch questions.finally wediscusstheactualtrainingof oruseofpretrained models for embedding the code changes.
.
research questions rq1 do different representation learning models yield comparable distributions of similarity values between buggy code and patched code?
a widespread hypothesis in program repair is thatbugfixinggenerallyinduceminimalchanges .
we propose to investigate whether embeddings can be a reliable means for assessing the ex tent of changes through computation of cosine similarity between vector representations.
rq2 to what extent similarity distributions can be generalized for inferring a cutoff value to filter out incorrect patches?
following up onrq1 we propose inthis research question to experiment ranking patches based on cosine similarity of theirvectorrepresentations andrelyonnaively definedsimilaritythresholdsto decideonfilteringofincorrectpatches.
rq3 canwelearntopredictpatchcorrectnessbytrainingclassifiers with code embeddings input?
we investigate whether deep learned features are indeed relevant for building machine learning predictors for patch correctness.
.
datasets we collect patch datasets by building on previous efforts in the community.
an initial dataset of correct patches is collected by using five literature benchmarks namely bugs.jar bears defects4j quixbugs and manysstubs4j .these are developerpatchesascommittedinopensourceprojectrepositories.
we also consider patches generated by apr tools integrated intothe repairthemall framework.weuseallpatchsamples releasedbydurieux etal.
.thisonlyincludessamplepatches that make the programs pass all test cases.
they are thus plausible.
however novalidationinformationoncorrectnesswasgiven.in this work we proceed to manually validate the generated patches among which we identified correct patches.
the correctness validation follows the criteria defined by liu et al.
.
inarecentstudyontheefficiencyofprogramrepair liu etal.
released a labeled dataset of patches generated by apr systems forthedefects4jbugs.weconsiderthisdatasetaswellasthelabeled dataset that was used to evaluate the patch sim approach.
overall table 1summarizesthedatasetsthatweusedforour experiments.
each experiment in section 4has specific requirementsonthedata e.g.
largepatchsetsfortrainingmodels labeled datasets for benchmarking classifiers etc.
.
for each experiment we will recall which sub dataset has been leveraged and why.
table datasets of java patches used in our experiments.
subjectscontains incorrect patchescontains correct patcheslabelled dataset patches bears no yes bugs.jar no yes defects4j no yes manysstubbs4j no yes quixbugs no yes repairthemall yes yes no liuet al.
yes yes yes xionget al.
yes yes yes total thelatest version .
.
of defects4j is considered in this study.
the patches are not labeled in .
we support the labeling effort in this studybycomparingthegeneratedpatchesagainstthedeveloperpatches.the2 918patchesforintroclassjavain arealsoexcludedfromourstudysince introclassjavaisalab builtjavabenchmarktransformedfromthecprogrambugsinsmallstudent writtenprogrammingassignmentsfromintroclass .
.
model input pre processing samplesinourdatasetsarepatchessuchastheonepresentedin figure1extractedfromthedefects4jdataset.ourinvestigations with representation learning however require input data about the buggy and patched code.
a straightforward approach to derivethose inputs would be to consider the code files before and afterthe patch.
unfortunately depending on the size of the code file the differences could be too minimal to be captured by any similarity measurement.
to that end we propose to focus on the code fragmentthatappearsinthepatch.thus torepresentthebuggy codefragment cf.figure wekeepallremovedlines i.e.
starting with aswellasthepatchcontextlines i.e.
thosenotstarting with either or .
similarly the patched code fragment cf.
figure3 isrepresentedbyaddedlines i.e.
startingwith aswell source org jfree chart renderer category abstractcategoryitemrenderer.java source org jfree chart renderer category abstractcategoryitemrenderer.java public abstract class abstractcategoryitemrenderer int index this.plot.getindexof this categorydataset dataset this.plot.getdataset index if dataset !
null if dataset null return result figure example of a patch for the defects4j bug chart .
a source org jfree chart renderer category abstractcategoryitemrenderer.java intindex this.plot.getindexof this categorydataset dataset this.plot.getdataset index if dataset !
null returnresult figure buggy code fragment associated to patch in fig.
.
b source org jfree chart renderer category abstractcategoryitemrenderer.java intindex this.plot.getindexof this categorydataset dataset this.plot.getdataset index if dataset null returnresult figure3 patchedcodefragmentassociatedtopatchinfig.
.
as the same context lines.
since tool support for the representation learningtechniquesbert doc2vec andcc2vecrequireeachinput sample to be on a single line we flatten multi line code fragments into a single line.
incontrasttobert doc2vec andcc2vec whichcantakeas inputsomesyntax incompletecodefragments code2vecrequires the fragment to be fully parsable in order to extract information onabstractsyntaxtreepaths.sincepatchdatasetsincludeonly text based diffs code context is generally truncated and is likely notparsable.however asjustexplained weopttoconsideronly theremoved addedlinestobuildthebuggyandpatchedcodeinput data.by javaextractortoolusedtobuildthetokensinthecode2vecpipeline.
.
embedding models whenrepresentationlearningalgorithmsareappliedtosometraining data they produce embedding models that have learned to map asetofcodetokensinthevocabularyofthetrainingdatatovectors of numerical values.
these vectors are also referred to as embeddings.figure 4illustratestheprocessofembeddingbuggycodeand patched code for the purpose of our experiments.
the embedding models used in this work are obtained from different sources and training scenarios.
bert.in the first scenario we consider an embedding model thatinitiallytargetsnaturallanguagedata bothintermsofthe learning algorithm and in terms of training data.
the network structureofbert however isdeep meaningthatitrequireslarge datasetsfor training theembeddingmodel.
asitisnow custom intheliterature weinsteadleverageapre trained24 layerbert model which was trained on a wikipedia corpus.
doc2vec.
in the second scenario we consider an embedding model that is trained on code data but using a representation learning technique that was developed for text data.
to that end patchcode representation buggy code patched code buggy code vector patched code vectorbert doc2vec or code2vec embedding model preprocessing figure producing code fragment embeddings with bert doc2vec and code2vec.
we have trained the doc2vec model with code data of patches from the repair benchmarks cf.
table .
code2vec.
inthethirdscenario weconsideranembeddingmodel that primarily targets code both in terms of the learning algo rithm and in terms of training data.
we use in this case a pre trained model of code2vec which was trained by the authors using million code examples from java projects.
cc2vec.
finally in thefourthscenario weconsider anembeddingmodelthatwasbuiltinrepresentationlearningexperiments for code changes.
however the pre trained model that we leveragedfromtheworkofhoang etal.
isembeddingeachpatch into a single vector.
we investigate the layers and identify the middlecnn 3dlayerasthesweetspottoextractembeddings for buggycode and patched codefragments.
figure 5illustrates the process.
experiments wepresenttheexperimentsthatwedesignedtoanswertheresearch questions of our study.
for each experiment we state the objective overview the execution details before presenting the results.
.
similarity measurements for buggy and patched code using embeddings objective we investigate the capability of different learned embeddings to capture the similarity dissimilarity between code fragments.theexperimentsareperformedtowardsprovidinganswers for two sub questions rq .1is correct code actually similar to buggy code based on learned embeddings?
rq .2to what extent is buggy code more similar to correctlypatched code than to incorrectly patched code?
experimental design we perform two distinct experiments with available datasets to answer rq .
and rq .
.
using the four embedding models considered in ourstudy cf.section .
weproducetheembeddingsforbuggy trained cc2vec model patch3d cnn layer lookup embeddingfully connected layeroutput layer buggy code vector patched code vectorcc2vec code representation figure extracting code fragment embeddings fromcc2vec pre trained model.
984and patched code fragments associated to 36k patches from five repair benchmarks shown in table .
in this case the patched code fragmentrepresentscorrectcodesinceitcomesfromlabeledbenchmark data generally representing developer fix patches .
given thoseembeddings i.e.
coderepresentationvectors wecompute the cosine similarity between the vector representing the buggy codefragment andthevectorrepresentingthepatchedcode fragment.
table2 patchdatasetsusedforcomputingsimilarityscores between buggy code fragments and correct code fragments.
bears bugs.jar defects4j manysstubs4jquixbugstotal patches due to parsing failures code2vec embeddings are available for patches.
to compare the similarity scores of correct code fragmentvsincorrectcodefragmenttothebuggycode weconsider combiningdatasetswithcorrectpatchesanddatasetswithincorrectpatches.notethat allpatchesinourexperimentsareplausiblesince we are focused on correctness plausibility is straightforward to decide based on test suites.
correct patches are provided in benchmarks.however incorrectpatchesassociatedtoallbenchmarkbugs are not available.
we rely on the dataset released by liu et al.
plausible but incorrect patches generated by repair tools for defects4jbugs areconsidered from thisdataset.
those 674incorrect patches are selected within a larger set of incorrect patches by adding the constraint that the incorrect patch should be changing the samecode location as the developer provided patch in the benchmark such incorrect patch cases may indeed be the most challenging to identify with heuristics.
we propose to compare the similarityscoresbetweentheincorrectcodeandbuggycodeassociated to the dataset with the similarity scores between correct code andbuggyassociatedtoallbenchmarks alldefects4jbenchmark data or only the subset of defects4j that corresponds to the patches for which relevant incorrect patches are available.
results figure6presentstheboxplotsofthesimilaritydistributions with different embedding models and for samples in different datasets.
doc2vecand code2vecmodels appearto yieldsimilarity values that are lower than bert and cc2vec models.
model bert cc2vec code2vecdoc2vec defects4j bugs.jar bears quixbugs manysssimilarity figure distributions of similarity scores between correct and buggy code fragments.
manyss stands for manysstubs4j .
figure7zooms in the boxplot region for each embedding model experiment to overview the differences across different benchmark d4j bj bears qb msssimilarity d4j bj bears qb msssimilarity d4j bj bears qb msssimilarity d4j bj bears qb msssimilarity a bert.
b cc2vec.
c doc2vec.
d code2vec.
figure zoomed views of the distributions of similarity scores between correct and buggy code fragments.
data.weobversethat whenembeddingthepatcheswithbert the similaritydistributionforthepatchesindefects4jdatasetissimilartobugs.jarandbearsdataset butisdifferentfromthedataset manysstbs4jandquixbugs.themann whitney wilcoxon mww tests confirm that the similarity of median scores for defects4j bugs.jar and bears is indeed statistically significant.
mww testsfurtherconfirmsthestatisticalsignificanceofthedifference between defects4j and manysstbs4j quixbugs scores.
defects4j bugs.jar and bears include diverse human written patches for a large spectrum of bugs from real world open source java projects.
in contrast manysstubs4j only contains patches for single statement bugs.
quixbugs dataset is further limited by its size and the factthat the patches are built by simply mutating the code of small java implementation of algorithms e.g.
quicksort levenshtein etc.
.
while cc2vec and doc2vec exhibit roughly similar patterns withbert althoughatdifferentscales theexperimentalresults with code2vec present different patterns across datasets.
note that due to parsing failures of code2vec we eventually considered only 118bearspatches 123bugs.jarpatches 46defects4jpatches manysstubs4j patches and quixbugs.
the change of dataset size could explain the difference with the other embedding models.
rq1.
trianglerightsldlearned representations of buggy and correct code fragments exhibit high cosine similarity scores.
median scoresaresimilarforpatchesthatarecollectedwithsimilarheuristics e.g.
in the wild patches vs single line patches vs debugging example patches .
the pre trained bert natural language model capturesmoresimilarityvariationsthanthecc2vecmodel which is specialized for code changes.
triangleleftsld inthesecondexperiment wefurtherassesswhetherincorrectlypatchedcodeexhibitsdifferentsimilarityscoredistributionsthan 985correctly patched code.figure 8shows thedistributions ofcosine similarity scores for correct patches i.e.
similarity between buggy codeandcorrectcodefragments andincorrectpatches i.e.
similarity between buggy code and incorrect code fragments .
the comparison is done with different scenarios specified in table .
figure comparison of similarity score distributions for code fragments in incorrect and correct patches.
table scenarios for similarity distributions comparison.
scenario incorrect patches correct patches imbalance d al 674incorrect patchesallcorrect patches from benchmarks in table .
imbalance d defects4j by16 apr tools allcorrect patches from defects4j.
balanced defects4j for184 defects4j bugsallcorrect patches for the defects4j bugs.
exceptfordefects4j therearenopublicly releasedincorrectpatchesforaprdatasets.
the comparisons do not include the case of embeddings for code2vec.
indeed unlike the previous experiment where code2vec wasabletoparseenoughcodefragments fortheconsidered184 correctpatchesofdefects4j code2vecfailedtoparsemostofthe relevantcodefragments.hence wefocusthecomparisononthe other three embedding models pre trained bert trained doc2vec and pre trained cc2vec .
overall we observe that the distribution of cosine similarity scores is substantially different for correct and incorrect code.
weobservethatthesimilaritydistributionsofbuggycodeand patchedcodefromincorrectpatchesaresignificantlydifferentfromthesimilaritiesforcorrectpatches.thedifferenceofmedianvalues isconfirmedtobestatisticallysignificantbyanmwwtest.note that thedifferenceremains high for bert doc2vec andcc2vec whetherthecorrectcodeisthecounterpartoftheincorrectones i.e.
the scenario of balanced defects4j or whether the correct codeisfromalargerdataset i.e.
imbalanced allandimbalanceddefects4j scenarios .
rq1.
trianglerightsldlearnedrepresentationsofcodefragmentswithbert cc2vecanddoc2vecyieldsimilarityscoresthat givenabuggy code substantially differ between correct code and incorrect code.
thisresultsuggeststhatsimilarityscorecanbeleveragedtodiscriminate correct patches from incorrect patches.
triangleleftsld .
filtering of incorrect patches based on similarity thresholds objective followinguponthefindingsrelatedtothefirstresearchquestion weinvestigatetheselectionofcut offsimilarityscores to decide on which apr generated patches are likely incorrect.resultsfromthisinvestigationwillprovideinsightstoguide the exploitation of code embeddings in program repair pipelines.
experimentaldesign to select threshold values we consider thedistributionsofsimilarityscoresfromtheaboveexperiments cf.section .
.table 4summarizesrelevantstatisticsonthedistributions on the similarity scores distribution for correct patches.
given the differences that were exhibited with incorrect patches in previousexperiments weuse forexample the1stquartilevalue as an inferred threshold value.
table statistics on the distributions of similarity scores for correct patches of bears bugs.jar defects4j.
subjectsmin.1st qu.
median 3rd qu.
max.
mean bert .
.
.
.
.
cc2vec .
.
.
.
.
doc2vec .
.
.
.
.
.
code2vec .
.
.
.
.
given our previous findings that different datasets exhibit different similarity score distributions we also consider inferring a specificthresholdforthequixbugsdataset cf.statisticsintable .
we do not compute any threshold based on manysstubs4j since it has not yet been applied to program repair tools.
table statistics on the distributions of similarity scores for correct patches of quixbugs.
subjectsmin.1st qu.
median 3rd qu.
max.
mean bert .
.
.
.
.
.
cc2vec .
.
.
.
doc2vec .
.
.
.
.
.
code2vec .
.
.
our test data is constituted of patches generated by apr tools in the empirical study of durieux et al.
.
first we usethefourembeddingmodelstogenerateembeddingsofbuggy code and patched code fragments and compute cosine similarity scores.second foreachbug werankallgeneratedpatchesbased on the similarity score between the patched code and the buggy where we consider that the higher the score the more likely the correctness.
finally to filter incorrect candidates we consider two experiments patches that lead to similarity scores that are lower to the inferredthreshold i.e.
1stquartileinpreviousexperimentaldata will be considered as incorrect.
patches where patched code exhibit higher similarity scores than the threshold are considered likely correct.
another approach is to consider only the top patches with the highest similarity scores ascorrect patches.
other patches are considered incorrect.
in all cases we systematically validate the correctness of all patches to have the correctness labels for which the dataset authorsdidnot provide allplausiblepatches havingbeenconsidered as valid .
first if the file s modified by a patch are not the same buggy files in the benchmark we systematically consider itasincorrect withthissimplescheme 33489patchesarefound incorrect.
second with the same file if the patch is not making changesatthesamecodelocations weconsiderittobeincorrect patches are further tagged as incorrect with this decision 986table filtering incorrect patches by generalizing thresholds inferred from section .
.results.
dataset cp ip thresholdbert cc2vec doc2vec cp ip recall recall cp ip recall recall cp ip recall recall bears bugs.jar and defects4j89361 9321stqu.
.
.
.
.
.
.
mean .
.
.
.
.
.
quixbugs 4611stqu.
.
.
.
.
.
mean .
.
.
.
.
cp and ip standforthenumberofcorrectandincorrectpatches respectively.
cp meansthenumberofcorrectpatchesthatcanberankeduponthethreshold while ip means the number of incorrect patches that can be filtered out by the threshold.
recall and recall represent the recall of identifying correct patches and filtering out incorrect patches respectively.
cf.threatstovalidityinsection .finally fortheremaining4418 plausible patches in the dataset we manually validate correctness by following the strict criteria enumerated by liu et al.
t o enable reproducibility.
overall we could label correct patches.
the remainders are considered as incorrect.
results by considering the patch with the highest top similarityscorebetweenthepatchedcodeandbuggycodeascorrect we were able to identify a correct patch for with bert withcc2vec and10 withdoc2vec ofthebugcases.overallwe also misclassified correct patches as incorrect.
however only .
of incorrect patches were misclassified as correct patches.
giventhatagivenbugcanbefixedwithseveralcorrectpatches thetop 1criterionmaynotbeadequate.furthermore thiscriterion makes the assumption that a correct patch indeed exists among the patch candidates.
by using filtering thresholds inferred from previous experiments which do not include the test dataset in this experiment wecanattempttofilterallincorrectpatchesgenerated by apr tools.
filtering results presented in table 6show the recall scores that can be reached.
we provide experimental results when we use 1stquartile and mean values of similarity scores in the training setasthresholdvalues.thethresholdarealsoappliedby takingintoaccountthedatasets thresholdslearnedonquixbugs benchmark are applied to generated patches for quixbugs bugs.
rq2 trianglerightsldbuilding on cosine similarity scores code fragment embeddings can help to filter out between .
with cc2vec and .
with bert of incorrect patches.
while bert achieves the highestrecalloffilteringincorrectpatches itproducesembeddingsthatleadtoalowerrecall at5.
atidentifyingcorrectpatches.
triangleleftsld .
classification of correct patches with supervised learning objective cosinesimilaritybetweenembeddings whichwas used in the previous experiments considers every deep learned feature as having the same weight as the others in the embedding vector.
we investigate the feasibility to infer using machine learning the weights that different features may present with respect to patch correctness.
we compare the prediction evaluation results with the achievements of related approaches in the literature.
experimentaldesign toperformourmachinelearningexperiments we first require a ground truth dataset.
to that end werely on labeled datasets in the literature.
since incorrect patches generatedbyactualaprtools are onlyavailableforthedefects4j bugs we focus on labeled patches provided by two independent teams liu etal.
andxiong etal.
.veryfewpatchesgeneratedbythedifferenttoolsareactuallylabeledascorrect leading toanimbalanceddataset.toreducetheimbalanceissue wesupplement the dataset with developer correct patches as supplied inthe defects4j benchmark.
eventually our dataset shown in table included patches after removing duplicates to avoid data bias.
table7 datasetforevaluatingml basedpredictorsofpatch correctness.
correct patches incorrect patches total liuetal.
xiongetal.
defects4j developers wholedataset finaldataset deduplicated ourgroundtruthdatasetpatchesarethenfedtoourembedding modelstoproduceembeddingvectors.asforpreviousexperiments the parsability of defects4j patch code fragments prevented the applicationofcode2vec weusepre trainedmodelsofbert trained withnaturallanguagetext andcc2vec trainedwithcodechanges as well as a retrained model of doc2vec trained with patches .
since the representation learning models are applied to code fragments inferred from patches and not to the patch themselves wecollecttheembeddingsofbothbuggycodefragmentandpatched codefragmentforeachpatch.thenwemustmergethesevectors back into a single input vector for the classification algorithm.
we follow an approach that was demonstrated by hoang et al.
i n a recent work on bug fix patch prediction the classification model performsbestwhenfeaturesof patchedcodefragmentandbuggy codefragmentarecrossedtogether.wethusproposeaclassificationpipeline cf.figure wherethefeatureextractionforagivenpatch isdonebyapplyingsubtraction multiplication cosinesimilarityand euclidean similarity to capture crossed features between the buggycodevectorandthepatchedcodevector.theresultingpatchembeddinghas2 n 2dimensionswherenisthedimensionofinputcodefragmentembeddings.thevaluesofthedimensionnforbert doc2vec and cc2vec are set as and respectively.
feature extractor cc2vec patches buggy code fragments patched code fragmentsp preprocessinginputcode representation learning method bert doc2vecnfeature crosses ntrain testclassifiers logistic regression decision tree naive bayeseb ep n istic regr sub multicosine euclidian figure feature engineering for correctness classification.
results we compare the performance of different predictors varyingtheembedingmodels usingdifferentlearners i.e.
classificationalgorithms .resultspresentedintable 8areaveragedfroma foldcrossvalidationsetup.allclassicalmetricsusedforassessing 987figure performance of ml patch correctness predictor using bert logistic regression test set from .
predictors are reproted accuracy precision recall f1 measure areaundercurve auc .logisticregression lr appliedtobert embeddingsyieldthebestperformancemeasurements .720forf1 and .
for auc.
table8 evaluationofbertrepresentationonthreemlclassifiers.
classifier embedding acc.
prec.
recall.
f1 auc decisiontreebert .
.
.
.
.
cc2vec .
.
.
.
.
doc2vec .
.
.
.
.
logistic regressionbert .
.
.
.
.
cc2vec .
.
.
.
.
doc2vec .
.
.
.
.
naive bayesbert .
.
.
.
.
cc2vec .
.
.
.
.
doc2vec .
.
.
.
.
rq3.
trianglerightsldan ml classifier trained using logistic regression withbertembeddingsyieldverypromisingperformanceonpatch correctness prediction f measure at .
and auc at .
.
triangleleftsld .therearetwo relatedworksforpatchpredictionwhichwerebothevaluatedon patches released by xiong et al.
.
patch sim compares execution traces of patched programs to identify correctness.
ods leverages manually crafted features to build machine learning classifiers.
weconsiderthe139patchesastestsetandtheremainderinour dataset for training.
note that the patches areassociatedtobugcaseswhererepairtoolscangeneratepatches.
these patches may thus be substantially different from the rest in our dataset.
indeed our best learner logistic regression with bertembeddings yieldsanaucof0.
.thereceiveroperating characteristic roc curve is presented in figure .
inthevalidationofpatch sim theauthorsaimedforavoidingtofilteroutanycorrectpatches.eventually whenguaranteeingthatnocorrectpatchisexcluded theycouldstillexclude62 .
incorrect patches.
if we constrain the threshold of our predictor to avoid misclassifying any correct patch threshold value .
ourpredictorisabletoexcludeupto43 .
incorrectpatches which represents a reasonably promising achievement since no 19patchesinthegroundtruthdatasetbyxiongetal.
wereduplicates e.g.
patch151 patch23 .table comparison of incorrect patch identification between patch sim uses dynamic information and bert lr uses embeddings statically inferred from patches .
ground truth patch sim bert lr projectincorrect correct incorrect correct incorrect correct excluded excluded excluded excluded chart .
.
lang .
math .
.
time .
.
total .
.
table confusion matrix of ml predictions based on bert embedddings with different thresholds.
learners aucthresholds .
.
.
.
.
.
.
.
.
lr .
tp30 tn13 fp96 fn006 rf .
tp30 tn1 fp108 fn0 table confusion matrix of ods predictions with different thresholds.
learners aucthresholds .
.
.
.
.
.
.
.
.
lr .
tp tn fp f n rf .
tp tn fp f n dynamicinformationisused incontrasttopatch sim .table overviews the prediction results comparison.
wealsocomparethepredictivepowerofourmodelsagainstthat of ods which builds on manually engineered features.
we directly compare against the results reported by the authors on the 139testpatches.whilethepre trainedbertmodelassociatedwith logistic regression lr achievesbetter aucthan odslr based model .765vs0.
odsrandomforest basedmodelachievesa higher auc at .
.
note however that ods has been trained on over thousand patches including patches for bugs associated to thetestsetpatch ourtrainingdatasetincludesonly870patches i.e.
20thof their dataset .
tables10and11provide confusion matrices for different cut off thresholds of the classifiers for ods and our bert embeddingsbased classifiers tp true positives represent correct patches that were classified as such tn true negatives represent incorrect patchesthatwere classifiedassuch fp falsepositives represent incorrect patches that were classified as correct and fn false negatives representcorrectpatchesthatwereclassifiedasincorrect.
overall thebert basedpredictorisverysensitivetothecut offthresholds while ods is less sensitive.
we also note that bert embeddings applied to random forrest does not yield good performance decision trees are indeed known to be good for categorical data and request large datasets for training.
in our case the data setis small whileods hasa trainingdatasetthat isabout 20times 988larger.
the hand crafted features of ods may also help split the patchesintocategorieswhileourdeeplearnedfeaturesarebased on a large vocabulary of natural language text.
weobserveneverthelessthatlrclassifiersfedwithbertembeddingsareabletorecallhighnumbersofincorrectpatches tnis highand fpislowonthreshold .
.incontrastodsconsistently recallscorrectpatches howeverwithhighfalsepositives .these experimental results suggest that both approaches can be used in a complementary way.
in future work we will propose an approach thatcarefullymergesdeeplearnedfeaturestohand craftedfeatures towards yielded a better predictors of patch correctness.
rq3.
trianglerightsldml predictors trained on learned representations appeartoperformslightlylesswellthanstateoftheartpatch simapproachwhichreliesondynamicinformation.ontheotherhand deepcoderepresentationsappeartobecomplementarytohandcrafted features engineered for ods.
overall we recall that our experimentalevaluationsareperformedinazero shotscenario i.e.
withoutfine tuningtheparametersofany ofthepre trained models.furthermore thetrainingdatasetoftheclassifiersisan order of magnitude smallerathan the one used by most closelyrelated work i.e.
ods and may further not be representative to best fit the test set.
triangleleftsld awe were not able to collect or reconstitute the training dataset used in ods to train our model.
discussions we enumerate a few insights from our experiments with representation learning models and discuss some threats to validity.
.
experimental insights code orientedembeddingmodelsmaynotyieldthebestembeddings for training predictors.
our experiments have revealed that the bertmodelwhichwaspre trainedonwikipediaisyieldingthe best recall in the identification of incorrect patches.
there are severalpossiblereasonstothat bertimplementsthedeepestneural networkandbuildsonthelargesttrainingdata.itsperformancesug geststhatcode orientedembeddingsshouldaimforbeingaccurate withsmalltrainingdatasetsinordertobecomecompetitiveagainst bert.whilewewerecompletingtheexperiments apre trainedcodebert model has been released on april .
in future work wewillinvestigateitsrelevanceforproducingembeddings that may yield higher performance in patch correctness prediction.
in any case we note that cc2vec provided the best embeddingsfor yielding the best recall in identifying correct.
patches usingsimilarity thresholds .
this suggests that future research should investigate the value of merging different representations or combiningtheeventualpredictionprobabilitiestoimproveperformance onbothidentifyingcorrectpatchesandexcludingmostincorrect patches.
thesmallsizesofthecodefragmentsleadtosimilarembeddings.
.
figure11illustratesthedifferentcosinesimilarityscoresthatcan be obtained for the bert embeddings of different pairs of short sentences.althoughthesentencesaresemantically dis similar thecosinesimilarityscoresarequiteclose.thisexplainswhyrecallingcorrectpatchesbasedonasimilaritythresholdwasafailedattempt for apr generated patches for.
defects4j bears bugs.jar bugs .
nevertheless experimental results demonstrated that deep learned features were relevant for learning to discriminate.
our grandpa has a very handsome look computer science is difficult his spouse is lovely .
.
.
figure11 closecosinesimilarityscoreswithsmall sizedin puts for bert embedding model.
embeddings are most suitable when applied to simple ml algo rithms.
because embeddings are yielded from neural networks they areactually formed bycomplex crossed features.when they arefedtoacomplexdiscriminantmodelsuchasrandomforrest it may lead to overfitting with small datasets.
our experiments howevershowthatsimplelogisticregressionyieldsthebestauc suggesting that this learner was able to better identifying discriminating features for the prediction task.
.
threats to validity our empirical study carries a number of threats to validity that we have tried to mitigate.
threatstoexternalvalidity.
thereareavarietyofrepresentation learning models in the literature.
a threat to validity of ourstudy is that we may have a selection bias by considering only fourembeddingmodels.wehavemitigatedthisthreatbyconsideringrepresentativemodelsindifferentscenarios pre trainedvs retrained code change specific vs natural language oriented .
another threat to validity is related to the use of defects4j data in evaluating the ml classifiers.
this choice however was dictated by the data available and the aim to compare against related work.
finally withrespecttotheexploredmodels theattentionsystem of cc2vec requires some execution parameters to perform well.sincetherelevantcodewasnotavailable weuseuseanonattention version instead potentially making cc2vec embeddings be under performing.
we release the artifacts for future comparisons by the research community.
threatstointernalvalidity.
amajorthreattointernalvalidity lies in the manual assessment heuristics that we applied to the repairthemall generateddataset.wemayhavemisclassifiedsomepatchesduetomistakesorconservatism.thisthreathoweverholds for all apr work that relies on manual assessment.
we mitigatethis threat by following clear and reproducible decision criteria andbyfurtherreleasingourlabelleddatasetsforthecommunityto review2.
threats to construct validity.
for our experiment the considered embedding models are not perfect and they may have been under trained for the prediction task that we envisioned.
for thisreason theresultsthatwehavereportedarelikelyanunderestimationof thecapability ofrepresentationlearningmodels to 2see 989capture discriminative features for the prediction of patch correctness.
our future studies on representation learning will address this threat by considering different re training experiments.
related work analyzing patch correctness toassesstheperformanceof fixingbugsofrepairtoolsandapproaches checkingthecorrectness of patches is key but not trivial.
however this task was largely ignoredorunconcernedinthecommunityuntiltheanalysisstudyof patchcorrectnessconductedbyqi etal.
.thankstotheirsystematicanalysisofthepatchesreportedbythreegenerate and validate program repair systems i.e.
genprog rsrepair and ae theyshownthattheoverwhelmingmajorityofthegeneratedpatches arenotcorrectbutjustoverfitthetestinputsinthetestsuitesof buggy programs.
in another study smith et al.
uncover that patches generated with lower coverage test suites overfit more.
actually these overfitting patches often simply break under tested functionalities and some of them even make the patched program worsethan theun patched program.
sincethen theoverfitting issue has been widely studied in the literature.
for example leet al.
revisit the overfitting problem in semantics based apr systems.
in they further assess the reliability of authors and automated annotations in assessing patch correctness.
they recommendtomakepubliclyavailabletothecommunitythepatch correctness evaluations of the authors.
yang and yang explore the difference between the runtime behavior of programs patched with developer s patches and those by apr generated plausible patches.theyunveilthatthemajorityoftheapr generatedplausi blepatchesleadstodifferentruntimebehaviorscomparedtocorrect patches.
predicting patch correctness to predict the correctness of patches one of the first explored research directions relied on the idea of augmenting test inputs i.e.
more tests need to be proposed.
yanget al.
design a framework to detect overfitting patches.
this framework leverages fuzz strategies on existing test cases inordertoautomaticallygeneratenewtestinputs.inaddition it leveragesadditionaloracles i.e.
memory safetyoracles toimprove the validation of apr generated patches.
in a contemporary study xin and reiss also explored to generate new test inputs with thesyntacticdifferencesbetweenthebuggycodeanditspatched code for validating the correctness of apr generated patches.
as complemented by xiong et al.
they proposed to assess the patch correctness of apr systems by leveraging the automated generation of new test cases and measuring behavior similarity of the failing tests on buggy and patched programs.
throughanempiricalinvestigation yu etal.
summarized twocommonoverfittingissues incompletefixingandregression introduction.toassistalleviatingtheoverfittingissueforsynthesisbased apr systems they further proposed unsatguided that reliesonadditionalgeneratedtestcasestostrengthenpatchsynthesis and thus reduce the generation of incorrect overfitting patches.
predictingpatchcorrectnesswiththankstoanaugmentedsetof testcasesheavilyreliesonthequalityoftests.inpractice testswith highcoveragemightbeunavailable .inourpaper wedonot relyonanynewtestcasestoassesspatchcorrectness butleveragerepresentation learning techniques to build representation vectors for buggy and patched code of apr generated patches.
topredictoverfittingpatchesyieldedbyaprtools ye etal.
proposeods anoverfittingdetectionsystem.odsfirststatically extracts4 199codefeaturesattheastlevelfromthebuggycode andgeneratedpatchcodeofapr generatedpatches.thosefeatures are fed into three machine learning algorithms logistic regression knn and random forest to learn an ensemble probabilistic model forclassifyingandrankingpotentiallyoverfittingpatches.toevalu atetheperformanceofods theauthorsconsidered19 253training samples and testing samples from the durieux et al.empirical study .
with these settings ods is capable of detecting of overfitting patches.
the ods approach relates to our study since both leverage machine learning and static features.
however ods only relies on manually identified features which may not generalize to other programming languages or even other datasets.
inarecentwork csuvik etal.
exploitthetextualandstructuralsimilaritybetweenthebuggycodeandtheapr patchedcode withtworepresentationlearningmodels bert anddoc2vec byconsideringthreepatchcoderepresentation i.e.
sourcecode abstract syntax tree and identifiers .
their results show that the source code representation is likely to be more effective in correct patch identification than the other two representations and thesimilarity based patch validation can filter out incorrect patches foraprtools.however toassesstheperformanceoftheapproach only patches from quixbugs have been considered including in the lab bugs .
this low number of considered patches raisesquestions aboutthe generalization oftheapproach forfixing bugsinthewild.moreover unlikeourstudy newrepresentation learning models code2vec and cc2vec dedicated to code representation have not been exploited.
representationlearningforprogramrepairtasks inthe literature representation learning techniques have been widely explored to boost program repair tasks.
long and rinard explored the topic of learning correct code for patch generation .
their approach learns code transformation for three kinds of bugs from their related human written patches.
after mining the most recent bug fixing commits from each of the most popular java projects soto and le goues have built a probabilistic model to predict bug fixes for program repair.
to identify stable linuxpatches hoang et al.
proposed a hierarchical deep learningbased method with features extracted from both commit messages and commit code.
liu et al.
and bader et al.
p r o p o s e dt o learn recurringfix patternsfrom human written patchesand sug gestfixes.ourpaperisnotaimingatproposinganewautomated patch generation approach.
we indeed rather focus on assessingrepresentation learning techniques for predicting correctness of patches generated by program repair tools.
conclusion in this paper we investigated the feasibility of statically predicting patch correctness by leveraging representation learning modelsand supervised learning algorithms.
the objective is to provide insightsfortheaprresearchcommunitytowardsimprovingthe quality of repaircandidates generated byapr tools.
to that end we first investigated the use of different distributed representation 990learning to capture the similarity dissimilarity between buggy and patched code fragments.
these experiments gave similarity scores thatsubstantiallydifferforacrossembeddingmodelssuchasbert doc2vec code2vecandcc2vec.buildingontheseresultsandin order to guide the exploitation of code embeddings in program repair pipelines we investigated in subsequent experiments the selectionofcut offsimilarityscorestodecidewhichapr generated patchesarelikelyincorrect.thisallowedustofilteroutbetween .
and .
incorrect patches based on brute cosine similarity scores.finally weinvestigatedthediscriminativepowerofthedeep learned features by training machine learning classifiers to predict correctpatches.decisiontree logisticregressionandnaivebayes are tried withcode embeddings from bert doc2vec and cc2vec.
logisticregressionwithbertembeddingsyieldedverypromising performanceonpatchcorrectnesspredictionwithmetricslikefmeasureat0.
andaucat0.
onalabeleddeduplicateddataset of patches.
we further showed that the performance of these models on static features is promising when comparing againstthe state of the art patch sim which uses dynamic execution traces.
experimental results suggests that the deep learned featurescanbecomplementarytohand craftedfeatures suchas those engineered by ods .
availability.
allartifactsofthisstudyareavailableinthefollowing public repository