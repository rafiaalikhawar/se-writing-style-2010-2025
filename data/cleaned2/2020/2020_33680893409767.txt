communityexpectationsforresearch artifactsand evaluation processes ben hermann ben.hermann upb.de heinznixdorfinstitut universit tpaderborn paderborn germanystefanwinter sw cs.tu darmstadt.de dependablesystems and software technischeuniversit tdarmstadt darmstadt germanyjanet siegmund janet.siegmund informatik.tuchemnitz.de technischeuniversit tchemnitz chemnitz germany abstract artifact evaluation has been introduced into the software engineering and programming languages research community with a pilot at esec fse and has since then enjoyed a healthy adoptionthroughouttheconferencelandscape.inthisqualitative study weexaminetheexpectationsofthecommunitytowardresearch artifacts and their evaluation processes.
we conducted a survey including all members of artifact evaluation committees of major conferences in the software engineering and programming language field since the first pilot and compared the answers toexpectationssetbycallsforartifactsandreviewingguidelines.
whilewefindthatsomeexpectationsexceedtheonesexpressed in calls and reviewing guidelines there is no consensus on quality thresholds for artifacts in general.
we observe very specific quality expectations for specific artifact types for review and later usage but also a lack of their communication in calls.
we also find problematic inconsistencies in the terminology used to express artifact evaluation s most important purpose replicability .
we derive several actionable suggestions which can help to mature artifactevaluationintheinspectedcommunityandalsotoaidits introduction intoothercommunities incomputer science.
ccs concepts general and reference software and its engineering softwarelibraries and repositories softwareverificationand validation keywords artifact evaluation replicability reproducibility study acmreference format benhermann stefanwinter andjanetsiegmund.
.communityexpectationsforresearchartifactsandevaluationprocesses.in proceedings ofthe28thacmjointeuropeansoftwareengineeringconferenceandsymposiumonthefoundationsofsoftwareengineering esec fse november 8 13 virtual event usa.
acm new york ny usa 12pages.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 8 13 virtual event usa associationfor computing machinery.
acm isbn ... .
introduction in2016 areplicabilitycrisisbecamepublic whenmore than researchersrevealedhavingtroublereplicatingpreviousresearch results .this replicabilitycrisisalso reachedthesoftware engineeringcommunity asithasembracedtheimportanceofreplicationforknowledgebuilding .forexample collberg andproebstingcouldnotobtaintherelevantartifactstoconduct a replication neither by contacting the authors the authors institution andfundingagency .also lungetal.describetheir difficultiesinconductinganexactreplication evenwhentheywere in direct contact with the authors .
glanz et al.
describe similar experiences when obtaining research artifacts for comparison and had to reimplement competing approaches in order to replicate results .fortheterm artifact wefollowthedefinitionprovided bym ndezetal.
describingitasa self contained workresult withacontext specific purpose.
toimprovethesituationofmissingorunusableartifacts artifact evaluationhasbecomearegularprocessforscientificconferencesin thesoftwareengineeringandprogramminglanguagecommunities.
itcontributestothelargertrendtowardsopenscienceincomputer science.
since the first piloting of the process at esec fse many other conferences have included artifact evaluations as an additionalstepthatauthorsofacceptedpapersmaytake.iftheir artifactissuccessfullyevaluatedthecorrespondingpublicationis marked with a badge indicating different levels by which the artifact is found to support the presented research results.
successfully evaluated artifacts are listed on the conference website and commonly linked with the paper in publication repositories suchasthe acmdigitallibrary.
exceptforfewvenues i.e.
cav and tacas where artifact evaluation is mandatory for tool papers artifactsubmissionusuallyisavoluntaryactivitythatauthors ofacceptedpublicationsareinvitedtoparticipatein.journalsare recentlyadoptingtheideaofartifactsaspartofopenscienceinitiatives.forexample theempiricalsoftwareengineeringjournal emse encourages authors to share their data in a replication package .
there is preliminary evidence that papers with an evaluated artifact have higher visibility in the research community .
there is to the best of our knowledge currently no evidence thatartifactevaluationisleadingtobetterartifactsforcomputer scienceresearchcommunities.theoverarchinggoalofourworkis toenableanassessmentoftheefficacyofartifactevaluationsasthey have been implemented in software engineering and programming language conferences and to identify possible improvements for these processes.
such an assessment requires criteria according to which we can judge whether artifact evaluations meet their esec fse november8 virtualevent usa ben hermann stefan winter andjanet siegmund objectives.however fromaninitialreviewoftheacm sguidelines on artifact review and badging and the different conferences calls for artifacts we were not able to derive clear and uniform criteria what makes a research artifact good .
the standard of quality widely varies between different conferences and evolves overtime.thus thequalityofartifactsofdifferentvenuesisnot necessarilycomparable makingitdifficulttoreachaunifiedquality standardthat artifacts should adhere to.
as a first step towards a systematic assessment of artifact evaluationprocesses theobjectiveofthispaperistoassesstheircurrent perception in the ae pioneering software engineering and programminglanguagecommunitiesandtopavethewaytounified quality standards regarding artifact evaluation.
to this end we qualitatively examine rq1 the purpose of artifact evaluation rq2 the quality criteria and expectations for research artifacts and rq3 the magnitude of difference in the perception of purpose and expectationswithin thesoftware engineering and programming languagescommunities to answer these questions we have conducted a survey among researchers who have served on artifact evaluation committees aecs as they have experience with the expectations toward artifactsandtheproceduralchallenges.wehavecontactedallmembers of aecs including the respective chairs for all artifact evaluations conducted at software engineering and programming language conferences between and2019.
we found that the perceived purpose of artifact evaluation is to foster replicability and reusability at the same time.
while we could observe several quality criteria to be expected from artifacts we found no clear consensus on them.
moreover the expressed expectations of the communities are largely not represented in the callsforartifacts.thismakesithardtodefineaqualitystandard foranindividualconference thecommunity oracross community quality standard.
the results of our study show that the lack of such quality standards leaves reviewers without guidance how todecideonartifactacceptanceorrejection.moreover itcreates anambiguityforreadersofresearcharticleshowtointerpretthe badges awardedto papers after ae.
fromtheseobservationswederivethesuggestionthatcommitees should be instated in the programming language and software engineering communities to drive and foster the clarification of thepurposeofartifactevaluationwithintherespectivecommunity alongwithcorresponding reviewguidelines.
in summary we make the following contributions we provide an overview of the current perception and practiceofartifactevaluationandtheexpectationstowardartifactsandthe process.
based on community inputs we present suggestions for futuredevelopmentandimprovementofartifactevaluations.
we published the survey data set scripts and analysis results that our conclusions are based on as a research artifact forreplicabilityofourresults forfurtheranalysis andfor extensionbythe community .
backgroundand related work the concept of artifact evaluation as a means to foster replicability is a relatively new practice in software engineering research.
it hasalsobeendiscussedinabroadercomputer sciencecommunityin adagstuhlperspectivesworkshop in2015 whereoneof the key results was that the community needs to be pushed further to embrace the publication and most importantly sufficient documentation ofartifacts.
m ndez et al.
found that there is no agreed upon understanding of what an artifact actually is so they set out to explore potentialdefinitions.theycometoageneraldefinitionthatwealso adhere to in our work an artefact is a self contained work result havingacontext specificpurposeandconstitutingaphysicalrepresentation a syntactic structure and a semantic content forming three levels ofperception .
replication in software engineering has become more and more important.
already years ago basili et al.
found that too many studies tend to be isolated and are not replicated either by the same researchers or by others .
to support replication they developedaframeworkfordescribingrelatedstudiestoallowresearchers viewing them in context rather than in isolation.
despite thedifficultiesofactuallyconductingreplications asreportedby lung et al.
shull et al.
as well as juristo et al.
have pointed out the importance of replications .
both encourage the software engineering research community to embrace replications because the context of human studies in software engineering is too complex to be understood with a single study.
however as siegmundandotherspointout thecommunityhastoovercome thehypocrisyofpayinglipservicetotheimportanceofreplication but at the same time not valuing themaccordingly .
roblesreportedveryscarceavailabilityofartifactsinthemining software repositories msr communitybetween 2004and impeding replication of results .
as this community within thesoftware engineeringfield wasprimarily focussing ontheuse andreuseofdatasetsitwasreliantontheavailabilityofdatasets.
while artifact evaluation was piloted later in in tim menzies and jelbersayyadstarted the nowdiscontinuedpromise repository1toshareresearchartifacts.artifactswerearchivedwithouta formalreviewprocess.
theyreceivedthe msr foundational contributionawardin2017 for their work.
wacharamanotham et al.
inspected the low availability of artifacts in the hci community and found that four factors influence researchers to refrain from sharing artifacts concern about personally identifiabledata lackofparticipant spermission lackof motivation resources orrecognition anddoubtintheusefulnessof their artifact outside their own study .
dahlgren conducted an observatorystudyduringtheoopsla2019artifactevaluationand found that the most prominent negative comments during artifact review are due to limited physical resources or review time to test artifacts andproblems withdocumentation .
timperleyetal.conductedasurveyamongauthorsofpublished papersaticse ase fse andemse2018.
.togetherwithapublicationanalysistheystudiedthecurrentpracticeandtheproblems involvedinartifactsharinginthesoftwareengineeringcommunity.
in their results they report similar findings as wacharamanotham et al.
found for the hci community which suggests that artifact sharing has comparable issues throughout computer science.
they 1theartifactshavebeenmovedtozenodoforlongtimearchiving.
communities seacraft 470community expectationsforresearchartifacts andevaluation processes esec fse november8 virtualevent usa derive several recommendations for different stakeholders in research which align with the recommendations we make in this paper.
expectationsinthe acmguidelines and callsforartifacts in a pre study we analyzed the acm guidelines for artifact review and badging and calls for artifacts cfas issued for software engineering and programming language conferences between and20192.
.
methodology to extract expectations on artifacts from these text sources we analyzed the texts for explicit statements of two types statements about the purposeof artifact evaluations as a process and statements about criteriathat artifacts under evaluation are expected to meet.
the analysis was performed manually by one researcher and confirmed by another one independently.
a tool for plagiarismchecking3andatoolfor differencevisualization4were usedtoaid the analysis inorderto recognizerepeating passages.
we expect the stated criteria to follow from the stated purpose however analyzingbothkindsofstatementsallowsustoidentify possible inconsistencies.such inconsistencieswould indicate possiblemisunderstandingsoftheusedterms beitonoursideoron the sideofthe calls authors.
.
results .
.
expectationsonartifactsintheacmguidelines.
whilethe acmguidelinesdonotmakeanexplicitstatementregardingthe purposeofartifactevaluations theymotivateitbyanobservedlack ofreproducibilityofresearchresultsanddefinethreedifferentdesirablepropertiesofexperimentalresearchresults repeatability same results if repeated bythe same teamwith the same setup replicability sameresults ifrepeatedbyadifferentteamwiththesame setup and reproducibility sameresultsifrepeatedbyadifferent team with a different setup .
repeatability is stated as a minimum requirement for publishing experimental results reproducibility as theultimategoal andreplicabilityastheintermediateproperty targetedbyartifactevaluations.krishnamurthiandvitek name repeatability astheprimarygoalofartifactevaluationanddescribe itasre runningabundledsoftwareartifact.thisisinessencewhat theacmguidelinesnowdescribeas replicability .webelievethis tobeacaseofterminologyevolutionaskrishnamurthiandvitek do not make the explicit distinction of the group performing the experiment repetition the more recent acmguidelines make.
the acm guidelines state criteria that artifacts must fulfill in order to be awarded one of five different badges.
three badges are recommended to be issued in the context of artifact evaluations 2thecorpusofcfascanbefoundonourwebsite in ourartifact .
4git diff no index color words 5the acm guidelines have been changed after our article has been accepted for publicationandnowassignreciprocalmeaningstothereplicabilityandreproducibility terms and related badges .
we have chosen to not alter the discussion in the paper as themeaningsthatwere originally assignedtothesetermswerewhatweexpectedto bereflected in cfas and oursurveyparticipants replies.
artifactsevaluated functional artifactsevaluated reusable and artifactsavailable .thefirsttwobadgesrequiretheartifact to have passed an independent audit with different criteria.
for the functionalbadge anartifactneedstobe documented sufficient description to be exercised consistent contributes to how paper results were obtained complete includes all components relevant to the paper to the degree possible exercisable executability of scripts software accessibility modifiability of data and includeappropriateevidenceofverificationandvalidation .
forthereusablebadge theartifactmustmeetthefunctionalbadge s criteria and in addition must be particularly well documented and well structured to facilitate reuseandrepurposing .
for the available badge artifacts need to be made publicly accessible on archival platforms with a declared plan to enable permanent accessibility .
two other badges are proposed for papers for which the main resultshavebeenreplicatedorreproducedinsubsequentstudies according to the definitionssetforth bythe guidelines.
.
.
callsforartifacts cfas .
contrarytotheacmguidelines outof79analyzedcallsforartifactsexplicitlystateapurposeforartifactevaluation.acrossallanalyzedcalls themostfrequentlynamed purposeofartifactevaluationprocessesistoenable reuseofartifacts 32calls6 followedby reproducibility andenabling comparison for future research against published results.
when divided by community programming languages conferences7named reproducibility more oftenthan reuse.some ofthe callsname the weakerpropertiesof replicability whichisthedeclareddirect goalofartifactevaluationsintheacmguidelines andrepeatability .
other calls contained more vague statements regarding the purpose e.g.
to provide evidence for quality or support for the paper .
seven calls attribute benefits in terms of reproducibility replicability orrepeatabilityexplicitlytothe availability ofartifacts inwhichthey see apurpose of artifact evaluations.
while analyzing calls we noticed suble differences in the use of the terms replicability andreproducibility .
asdiscussed previously webelievethistobepartlyanissueofterminologyevolution.however the notions are discussed inconsistently in the literature as well.whiletheacmguidelinesrefertoadefinitionfromthe internationalvocabulary ofmetrology anotherwidely referenced definition is found in the asa s recommendations to funding agenciesfor supporting reproducibleresearch according to whichresearchisreproducibleifperformingidenticaldataanalyses on identical data yields the same findings.
result replication according to the asa definition requires the repetition of a study independent from the original investigators and without using the original data.
although the acm guidelines on ae provide clear definitions for the terms to be used in the context of ae both definitionsthatassignreciprocalmeaningstoreproducibilityand replicabilityarewidelyused.werecommendthataecchairsmake this distinction explicit in cfas to avoid misunderstandings in the interpretationofcfas andindiscussionsamong aecmembers.
concerning the artifact criteria stated in the calls we distinguished between evaluation criteria and submission criteria.
while evaluation criteria describe properties of the artifact itself submission criteria are concerned with formal requirements of additional 6numbers do not sum up to .
multiple purposes may have been named by one call.
7oopsla pldi popl ecoop sas sle ppopp cgo icfp tacas 471esec fse november8 virtualevent usa ben hermann stefan winter andjanet siegmund material required to submit the artifact for evaluation.
an astonishing number of 14calls does not state explicit evaluation criteria for artifacts spanning conference calls from until .
we assume that detailed evaluation guidelines were communicated by othermeanstoaecmembers.themostprevalentcriteriaarefor thelargestpartparaphrasedfromtheacmguidelines orcopied from calls that later heavily influenced the acm guidelines documentation consistency completeness andreusability .
eight calls even contain verbatim copies of the corresponding criteriadefinitionsfromtheacmguidelines.ontheonehand this isaclearindicationthatthissetofcriteriahasevolvedasacommunity standard which serves as a framework for artifact evaluations.
at the same time these criteria do not define clear conditions or thresholdstodecideforartifactacceptanceorrejection astheacm guidelines acknowledge.
in terms of submission criteria in the calls we find that 22calls fromconferences between2012and2019donot stateanyexplicit submission criteria.
in the calls that do the mostfrequently stated criteria are all related to documentation how to replicate paper results how to use the artifact and how to conduct setup andbasictestingoftheartifactwithinlessthan 30min .thisis remarkable for two reasons.
first while the most frequently stated purposeofartifactevaluationisreuse themostfrequentlystated submissioncriterion isdocumentationforreplication.togetherwith the observation that consistency is more frequently stated as an evaluation criterion than reusability this may indicate that the assessment of replicability actuallyplays a more important rolein theartifactevaluationprocessthantheassessmentofreusability.
second thetimelimitforsetupandbasictestsistheonlystatement ofan actualthreshold wefind acrossallcriteriastatedinthecalls.
inouranalysisofthecallswenoticedthatagoodfraction of the calls exemplary name diverse typesof research artifacts that may be submitted to the ae track e.g.
code and software data proofs but also grammars surveys and even hardware .
ontheonehand mostcalls explicitlystatethattheselistsof types are not exhaustive and to be understood as examples.
on the other hand listing these types of artifacts vs. others indicates certainexpectationsoftheaecchairswhattheywillbeevaluating intheaeprocess.interestingly wefoundonlytwocalls cav2018 vissoft that explicitly state evaluation criteria for some types of artifacts they list.
39calls state specific submissioncriteria for artifacts of certain types mostly for code in se cfas and for datain plcfas .however thesecriteriaonlycoverformatstobe used e.g.
csv json xmlfordataartifacts tar zipforsourcecode or docker vmsforexecutablesoftware.therefore whilecallsoften distinguish different artifacts types they do not make distinctions inthe criteriathat apply for theseartifact types.
in summary it is unclear from the calls a from an artifact submitterperspectivehowtobestprepareanartifactsothatitis positively evaluated and b from a potential re user ofan artifact whattoexpectfromanevaluatedartifact.however duringtheartifactevaluationprocess criteriatodecideonacceptanceorrejection musthavebeenused.therefore wedecidedtoconductasurvey among the aec members who made these decisions to obtain a betterunderstanding ofthesecriteria.
expectationsofartifact evaluation committee members toinvestigatewhichoftheexpectationstowardartifactsthatwe extracted from the calls are considered of particular importance and tocapture expectations beyondwhat is expressed inthecalls we conductedasurvey acrossaecmembers.
.
methodology objective.
based on our results from the analysis of the acm guidelines and cfas we designed our survey to cover four aspects the purpose of ae rq1 expectations toward artifacts as a revieweronanaec rq2 expectationstowardartifactsasa useraftersuccessfulevaluation rq2 and otherqualityfactors ofartifacts the participants have rq2 .
survey questionnaire.
in addition to these core aspects we also asked the participants about their experience with artifact evaluation and how useful they find the acm policy to guide their evaluation ofartifacts.
this helps us to understand the experience ofparticipantswithartifactevaluationandtosettheirresponses to later questions into perspective.
also to answer rq3we asked the participants to specify the aecs they served on.
the questions wereorganizedintwomaingroupsseparatingquestionsrelatingto artifactevaluationfromthoserelatingto artifactusage.questions were stated deliberately open so participants could freely share their views.
questions with numerical answers were accompanied byatextfieldforfurtherelaboration.thefullquestionnairecanbe foundonour projectwebsite8andinour artifact .
survey pre test.
we piloted and refined our survey in several steps with participants ensuring that we ask the right questions withan unambiguous wording.
participants.
we sent the survey to 1034members of artifact evaluation committees of different venues and different years.
figure2showsahistogramofindividualsbythenumberofaecsthey servedon.weonlyincludedcommitteemembersofalreadycompleted artifact evaluations atthe time of our survey.
for fse2012 we found a call for artifacts but could not find a public list of committee members or chairs.
for fse we could only identify the chairs whom we alsoincluded.
participants needed a median of 21minand19sto completethe survey.
all in all 257committee members responded of whom 124completed the entire survey.
133did not complete the entire survey butwe stillincluded theirrelevantperspectives toanswer rq2 and rq3.
we have excluded the complete replies from two participantsfromouranalysis oneforobviouslyimplausiblereplies id in our data set and one for the fact that the participant indicatedinanswersnottofeelqualifiedtoanswerthequestions id218 .outoftheremaining 255responses 152indicatedtheaec that the respondents served on.
we classified the conferences as belonging more tothesoftwareengineering se orprogramming language pl community as stated in .1and used the for pl 36for se responses to answer rq3.
nine respondents had indicatedhavingservedonbothplandseaecsandweinclude theirresponses inthe analysesfor eithercommunity.
472community expectationsforresearchartifacts andevaluation processes esec fse november8 virtualevent usa icsefseisstasasvissoftoopslaecooppoplpldisleppoppicfpcgomodelscavtacas y ear figure1 committeesizes green andresponses red byconferenceandyear analysis protocol.
we followed hudson s approach of open card sorting to analyze the answers .
we assigned at least two authors to process each survey question.
one author identified higher order topics to each answer.
as the process was open there werenopredeterminedcategories buttheywereextractedwhile reading the answers.
for instance for the answer reproducibility to a certain exten .
availability of the code.
to the question whatisthepurposeofartifactevaluation?
thelabels reproducibility and availability wereextracted.theotherauthorcheckedthe labels.difficultcasesweremarkedanddiscussedwithallauthors until consensus was reached.
in a second pass we reviewed all assigned labels and simplified harmonized labeling as different authorshaduseddifferentlabels for the same concept.
inthefollowing wewillalsopresentverbatimquotesfromrespondents.
for better contextualization we indicate the respondent idandtheirfrequencyofaecmembershipseparatedbycommunities if providedbythe respondent.
.
perceivedpurposeofartifactevaluations toaddressourfirstresearchquestion weaskedourparticipants to describe their view on the purpose of artifact evaluation.
we received147answers.
committees served innumber of individualsinvited responded figure histogram of individuals by number of aecs served in .
.
results.
in the mentioned purposes two major groups occurred fostering certain properties of the artifact andchecking certainpropertiesoftheartifact .wedescribethepropertiestobefosteredorcheckedinthe following.
fostering properties of artifacts.
in the first group of answers regarding the fostering of certain artifact properties the following propertieswerementionedfrequently reproducibility reusability comparability repeatability replicability usability andavailability .
however aswefoundinsection .2thatreproducibility hasan inconsistent interpretation across the different calls we assume mostparticipants inour study actually mean replicability .
in the contextofreproducible sciencecontributions itis important topromoteartifactsofscientificquality.thatsaid artifactevaluation hasthegoalofvalidatingthequalityofartifactinordertoguarantee various properties that increases the chances of reproducibility of the experiment overtime eg months years centuries... .
anotheraspectofartifactevaluationis inmyhumbleopinion the promotion of artifact as first class scientific contribution with a recognition by peers as complementary if not equivalent in quality andvalue to published papers.
id 1se aec 2plaecs next thesecondmostfrequentopinionisthatartifactevaluation fostersreusability .reusabilityinthiscontextmeansthatresearchers willbeabletoreuseanartifactofadifferentresearchgrouppossibly inaslightlydifferentcontextortobuilduponitforfurtherresearch.
one ofour participants summarizes this dual purpose as follows i see two main objectives of artifact evaluation tempering the tendency to over promise and under deliver and incentivizing the ability to build on others research.
id 3plaecs 473esec fse november8 virtualevent usa ben hermann stefan winter andjanet siegmund checkingpropertiesoftheartifact.
inthesecondgroupofpropertiesconcerningchecks validationsofanartifactthefollowingwere frequently mentioned validating claims validating results validating reusability validating reproducibility validating existence validating replicability andvalidating usability .
the purpose is to assess that the submitted paper is supported by actual tools and experiments and that these experiments can be run again in a self contained environment to reproduce the paper s results.amoreambitiousgoalistoprovideanenvironmentinwhich theprovidedexperimentscanbemodifiedeasily e.g.modifyinga test case commenting parts of a benchmark file etc.
to see how the tools handle such changes and how robust the experimental results are.
id 2se aecs the most mentioned objective for artifact evaluation is the validation of claims made in the paper or its results.
interestingly both objectives validatingreproducibility andvalidatingreusability do notseemtobeimportantforparticipants eventhoughtheseobjectives are the primary propertiesartifact evaluation should foster.
.
.
discussion.
similartowhatisstatedincfas theparticipants see the mission of artifact evaluationin fostering replicability and reusability.contrarytowhatweobservedforcfas replicability or reproducibility is mentioned by a larger number of respondents than reusability which likely is an effect of the sample of aec members that we received responses from.
the majority of respondents stated to have served on pl aecs vs. on se aecs with12respondentshavingservedonaecsforeithercommunity for which replicability is the most frequently stated ae purpose in cfas.
also the role of non exact replications has reached the research community as participants mentioned that they would liketobeabletoaltertheexperimentalsetupsprovidedwiththe submittedartifacts sothat they can test theirrobustness.
however whenspeakingaboutthevalidationofpropertiesof artifacts the mostfrequently mentioned property is the validation of claims or results .
this is still very close to the original mission to holdtheartifactaccountabletotheexpectationssetbythepaper whichiswhat krishnamurthiandvitekreport .
while the direct purpose of artifact evaluation with regard to thesubmittedartifactsisthevalidationofresultsorclaimsmade by the paper the community has extended this initial mission ofartifactevaluationandnowalsoseesitspurposein fostering replicability andreusability.
we also find that terminology is not used consistently among participants similar to our finding for inconsistent terminology in calls subsection3.
.specifically manyparticipantswroteabout reproducibility when they actually meant replicability.
however to clearly communicate the expectations toward artifacts we need to decideonaconsistent terminology.
terminology forthe mostimportant purposeofartifactevaluationisusedinconsistentlyinthe community.
.
expectations ofthecommunity from our analysis of the replies we received regarding rq1 see section4.
we have seen that the purpose of artifact evaluation isperceived as two fold verifying the accuracy of claims and results in research articles and re usability of artifacts.
in our analysis of cfas wefoundanindicationthatreusabilitymayplayalesserrole intermsof evaluationcriteria forartifacts.tofurtherinvestigate this hypothetical finding we collected two distinct perspectives fromourparticipants firstasareviewer andsecondasauser.ifthe expectationsdifferfor thesetwo perspectives thiswouldsupport the result from our callanalysis.
as the evaluation criteria in cfas were rather unspecific for the largestpart we alsoaskedspecificquestions regardingevaluation criteria for the types of artifacts that are most frequently named in calls code software data proofs to obtain a better understanding oftheactualdecisioncriteriaforartifactacceptanceandrejection rq2 .
inouranalysiswe differentiatebetweenrespondentsthathave servedonplandseaecstoaddressrq3.pleasenotethatthetotal numbersreportedcanbehigherorlowerthanthesumofpland se responses because a not all respondents provided information on which aecs they served and b respondents may have served onaecsinboth communities.
.
.
perspective as reviewer.
expectationsingeneral.
tounderstandthequalitycriteriathat reviewers expect from an artifact we asked for the minimum requirements to accept an artifact 124answers and for the reasons torecommendanartifactforacceptanceorrejection 110answers .
the most frequently mentioned criteria were replicability of results pl se good documentation and easy setup .
several participants mentioned that they accept artifactsthatshowsome general replicability .lookingat responses from the se community in isolation replicability is only ranked third good documentation and an easy setup are perceived to be more important.
we suspect this to be an indication that the se community values reuse over replication.
however we could neither confirmnorrefutethis basedonour data.
participantsreportedthattheyrecommendedaccepting an artifact because they were able to replicate results .
participantsreportedthattheyrecommendedrejectingbecausethey were not able to replicate results .
further reasons for acceptance suggestionswere easysetup gooddocumentation matches with the claims from the paper meets minimum requirements .
further reasons for rejection were bad documentation results deviate too much from the ones reported in the paper the artifact was substantially different from the paper .wediscussthemostfrequentlymentionedcriteriain moredetailinthefollowingforamoredetaileddescriptionwhat respondentsmean bytheseterms.
replicability of results.
as in the responses for the purpose of artifact evaluation we saw an inconsistent use ofthe terminology alsointheexpressionofexpectationshere.hence wesubsumed thementions ofreproducibility withthe mentionsofreplicability.
somerespondentsclarifiedthegreaterimportancetheyattribute to replicability comparedto criteriarelatedto re usability.
experiments should be reproducible.
good documentation and easy setuparea plusbutweshouldkeepinmindthatanartifactshould notbeseen as commercial software.
id 4plaecs 474community expectationsforresearchartifacts andevaluation processes esec fse november8 virtualevent usa transcendingthediversityfoundinsubmittedartifacts replicability isthecentralcriteriongivenfortheacceptanceorrejectionofan artifact.
gooddocumentation.
besidesreplicability awell prepareddocumentationofartifactsisalsoimportantforreviewers.inthecontext ofthereceivedresponses documentationcanmeanthedescription of the artifact and its parts a description of setup procedures or detaileddocumentationofindividualparts e.g.
codecomments .
however itdoesnotseemtobeamajorreasonforacceptanceor rejection asonlyfiveparticipantsmentionedtheirdecisiontobe influenced bydocumentation.
it is also the first criterion listed for theartifact evaluated functional badge suggested by the acm guidelinesandthemostprevalentcriterionnamedincfas.however itisnotclearlyspecifiedwhatmakesa gooddocumentation whichisalsomentionedas problematic bysomeparticipants.
documentationisobviouslyfuzzier butthereatthebareminimum should be instructions that tell a reviewer how to run the artifact andreproducesaidresults.
id 2plaecs easysetup.
theeaseofthesetupprocessforanartifactisalso oftenmentionedasaminimumrequirement.oneparticipantexplains it should not require more than hours of effort on the part of the evaluator to kick off the results evaluation process.
this finding is in line with our analysis of the acm guidelines and cfas in section where the actual time limit set by the calls is significantly shorter.
furtherinsights.
wefoundthatsomereviewersgobeyondthe replicationofexperimentsfromartifactsandalsomanipulatethe experiments whichisencouragedby24 ofthe analyzed79 cfas.
i followaprocess corresponding to badgecriteria .i read the paperandcheckthatmentioned artifactsexist.
.isearchforthereadmethatdescribesthesetup ordata .ievaluate based on clarityofthe setup guide.
.isearchforprovideddemosandtestcasesorreproductionscripts.
.itrytocreateaproblem atestcase suchasanewlanguagethatis supposed tobeimplemented withprovidedtool andsolveit based on the artifactsprovided.
id 1plaec interestingly wealsofoundthatsomeparticipantsofthesurvey mentionedthattheycaremoreaboutartifactavailabilitythanfor theirquality.thiswassurprisingtousbecauseitwouldindicate that detailed quality criteria beyond the artifact supporting the claims madeinthe paper mightbe obsolete.
it smuchmoreimportantthatsomethingisavailablethanits quality.iftheauthorspublishedapaperusingthiscode data whatever it would be good if the code data whatever was available even ifit slowquality.enforcingqualitycriteriaonlymeansthatsome authorswillnotpublishtheircode data whatever butthepaperis stillpublished.
id 1se aec expectationsforspecificartifacttypes.
whilemostcfasname differenttypesofresearchartifacts i.e.
code proof anddata they donotstatedifferentcriteriaforthose.toassessifdifferentcriteria are usedin practice we asked ourparticipants whethertheyhave differentexpectationsfor differentartifact types.
we received 123answers regarding code artifact 105answers regarding proof artifacts and 112answers regarding data artifacts.code.
documentation invariousformswasmentionedmostoften as quality criteria for code artifacts.
specifically the participants mentioned documentationingeneral setupdocumentation codedocumentation documentationonlyofrelevant parts documentationofcommand lineoptions and severalspecificsinglementions externallyexposedfeatures file formats usage asimportantforcode.inadditiontodocumentation code should compile and run when provided as an artifact as participants stated.
codequality seemstobeadebatedcriterion especiallyinthepl community while participantsexplicitlymentioncode qualityas aminimumexpectation participantsseecode qualityas not importantfor acceptance.
i generally have low expectations for code since i think the community generally favors proof of concept code over production quality code.
id 2plaecs additionally participants mentioned that during artifact evaluation there would be no time to inspect code quality and oneparticipantmentionsthatauthorswouldnothavethetimeto document orimprove quality.
among other mentions are packaging legible code and easysetup .
thus regardingcode we observe ageneral understanding that documentation in several forms is a minimum expectation for a code artifact.
however we see a moderation in the amount of documentationrequested.whileitisundebatedthatacodeartifact shouldcompile andrun we found thatthere are differingviewson the importanceofcode quality.
proofs.for proof artifacts respondents named the following quality criteria understandability completeness proof checker ran without errors and correspondence betweenclaimsfromthepaperandtheformalizedlemma .again documentationinvariousformsismentionedfrequently documentationofthehigh levelflow documentationingeneral commentson definitions documentationon howto compare topaperresults documentationofanyassumptions documentation ofusage beyond thepaper .
during artifact evaluation proofs appear to be more rated for theirinternalproperties suchasunderstandabilityorcompleteness ratherthanontheirabilitytoproofcheckwithouterror whichwas criticizedbysomerespondents.
data.for data artifacts we found the following quality criteria formatdescription rawdataincluded and documentationingeneral .furthermentionedwere nonproprietary formats reproducibility completeness and script program libraryto manipulate data .
thus severalparticipantsexpectthatnotonlythedatashouldbe contained in the artifact but also the scripts programs or libraries necessary to manipulate analyze orplot the data.
the raw data of the original submission should be included a script tool to plot what is in the paper.
data might be correct or not butalsothe plotting can containbugsdisturbingthe message.
id 1plaec 475esec fse november8 virtualevent usa ben hermann stefan winter andjanet siegmund summary and discussion.
considering the reviewer perspective wefoundthatreplicabilityofresultsisthemostimportantcriterion for the acceptance or rejection of artifacts which is in line with our analyses of criteria set forth by cfas.
this result is dominated bythelargergroupofresponseswe receivedfromtheplcommunity.
replicabilityisnot mentionedin answersfor specificartifact types no matter from which community.
hence we conclude that replicability is more a general property attributed to the whole artifact regardless of its type.
if the results reported by the authors in their paper can be replicated the artifact is generally considered of sufficient quality to be accepted for the artifact evaluation track.
as mentioned previously there are two distinct views on the qualitycriteriaforartifactevaluation.whilethefirstperspectiveis thattheavailabilityofanartifact cf.section .
ismoreimportant thanitsqualityaslongasitmeetstheexpectationssetbythepaper the other perspective is that the quality of accepted artifacts needs to improve beyond this.
in the software engineering community thecreationofhigherstandardsisvisibleforicseandfse asboth conferences do not award the artifact evaluated functional badge anymore9 butratherawardeitherthe reusableorjustthe available badge.
in the respective cfas this is justified by the objective of the artifact evaluation track to foster reusableartifacts.
wefoundthatthereisnoconsensusonthetopicofawell defined quality threshold.
however some conferences in the software engineeringcommunity establishedhigher requirementsforartifacts.
wefounddifferentexpectationsdependingontheartifacttype.
althoughdocumentationismentionedforallthreeartifacttypes especiallyforproofandcodeartifacts therearedifferentexpectations suchthatcoderequiresdocumentation andproofsrequire completenessandunderstandability.thisisnotsurprising because program code can be supplied in multiple forms and languages whereas mechanized proofs are usually formulated using one of themajorproofassistants i.e.
coq isabelle etc.
.forproofs the complexity here lies more in the formulation of the theory itself which needs to be explained step by step hence motivating the requirement ofunderstandability.
reviewers expect different quality criteria for different artifacts types but theseare not communicatedexplicitly incfas.
.
.
perspectiveasuser.
toassesstheexpectationstowardsresearch artifactsfrom a user perspective we asked the participants of our survey how many artifacts they have used for other reasonsthanevaluatingthem whethertheirexpectationstoward anyreusedartifactsweremet and toelaborateontheir un met expectations.
a total of 128participants completed this part of the survey.
if the respondents replied how many artifacts they have used we include this information along with the respondent id in the quotations.
qualitycriteria expectations.
mostpositively manyparticipants weresatisfiedwiththeartifactsthey re used irrespectiveofwhether itwascode 45satisfiedvs.
12notsatisfied proofs ordata .
this indicates that whatever criteria are applied the checks 9at fse since .for reusability in artifact evaluation processes cover what is expectedby expert users.withmorethan dissatisfaction there isnonethelessclearroomforimprovement.likeforthereviewer perspective the expectationsdifferedfor eachartifact type.
code.for code artifacts the dominating quality criteria were documentation andrunnability .
these were followed by reusability andresultreplicability .lessfrequentlymentioned criteria were usability source code availability andcode quality .
regarding documentation participants indicateddifferentpurposes first documentationshouldhelpto explainhowresultscan bereplicated pureopensourcesoftwarerepositoriesoftenlackthe documentation scripts and benchmark codes required to replicate a research paper.
we required the extensive help of the first author of a paper to be able to use it as comparison point in our own paper id .
second documentation should explain how codeworks iwasableto seeenoughtogetasenseofhowto do it myself and easily determine that their implementation wouldnotworkformypurposes id98 5code 5dataartifacts .
third documentation should explain how it can be extended iexpected it to have enough documentationso thati understand wheretoputmyextensions anditdid id145 5code 5proof artifacts .
runnabilityseemstobemostlyperceivedasabinarycriterion asparticipantsreportedthatthecodeartifactstheyused ran or worked .
problems for code that does not run can be caused by lackingmaintenanceofbothdocumentationorcodeaftertheinitial submissionandpublication of the artifact.
evenwhenthecodeisusefulandfunctional thedocumentation isusuallyout of date.mostof thetime i spend a dayortwo trying to make it run only to give up once i run into sufficiently hard problems.othertimesthecodeissooutdatedthatthereisnoway to make it work withoutcompletely updating it.
id 20code data artifacts proofs.only few participants indicated experience with using proofartifacts andthefewresponsessaw understandability and re usability as important quality criteria.
understandability mainly covers aspects of how mechanized proofs correspond to claims in articles whereas re usability of proofs relates to artifact handling orreuse ofparts from proof artifacts withothercode.
data.for data availability wasthemostimportantqualitycriterion followed by its relation to actual raw data .
the availability of data in addition to result summaries commonly reported in space constrained research articles is perceived as valuable but has to overcome limitations a couple of times papers have referred to publicly available datasetsfromothersources thatseemtohavemovedordisappeared sincethen.
id 20code data artifacts another concern was how available data relates to raw data.
one way to ensure traceability to raw data in data artifacts is to providetherawdataalongwithautomatedanalyses whichhave beenexplicitlymentionedasanimportantcriterionfordataartifact quality by some participants sometimes data are aggregated and 476community expectationsforresearchartifacts andevaluation processes esec fse november8 virtualevent usa othercasesitisnotclearhowtoobtainthefinalresultsfromthe raw data.
id 5code data artifacts summary and discussion.
regarding artifact usage the expectationsvaryfor differentartifacttypes.documentation asthemost prominent concern for code artifacts is rated even higher than the code srunnability probablyduetoourexpertrespondents confidence to get code to run if only the documentation is good enough.
similartoourresultsinsection .
.
understandabilityisofhigh concern for proofs.
for data artifacts availability and raw data are ofhigher concernthandocumentation.
the expectations on code artifacts show a higher number of replies related to reusability than to replicability .
this is corroborated by open comments on artifact usage in which respondents indicatereusability as artifactpurpose whereas only 6indicate replicability.
we observe this prevalence of reusability over replicability despite a majority of 55pl aec members for whomreplicabilitydominatesasaepurposeincfas over 18se aecmembers for our free textquestionsonartifact usage.
whilethespecificqualitycriteriadifferbyartifacttypes artifact usersgenerallyfindreusabilitymoreoftenanimportantpurpose for providing artifacts than replicability.
although this observation may seem unsurprising at first it indicates that artifact users do not perceive replicability as a positive effect on reusability even thoughthepreparationofareplicableartifactdoesrequireasimilar setofcriteria e.g.documentation .
respondentsdidnotperceivereplicabilityasabeneficialfactor to reusability.
theartifactusersinoursurveyweregenerallysatisfiedwiththe qualityof theartifacts they used.however this satisfaction is not clearly attributed to the quality assurance that artifact evaluations provide.
while 20of the respondents indicated a notable difference betweensuccessfullyevaluatedartifactsandnotevaluatedartifacts 34indicated to not have observed such difference.
the most frequentlyreporteddifferencesbetweensuccessfullyevaluatedand other artifacts were understandability usability consistency with paper results availability andotherless specific qualityaspects.
toputourresultsonartifactusageinperspective weneedto point out that only 76respondents have indicated to have any experiencewithartifact re usebeyondartifactevaluation.this needsconsideration when interpreting our results butalsoraises the question if artifact reuse is an uncommon scenario and if so why.
while answering this question is beyond the scope of our study wedeemitimportanttoreporttheobservationasaresultto be addressedbyfuture research.
despite the promotion of artifact reusability as a central goal of artifactevaluationinmanycfas lessthanhalfoftherespondents inour study reportedto have experience withartifact re use.
.
.
discussion comparison between perspective of reviewer and user.fromtheanswerswecollectedwecouldseethattherearevery diverse expectations toward artifact quality among respondents.
for the largest part the expectations mentioned by respondents fall into larger categories that match the evaluation criteria stated incfasandintheacmguidelines documentation consistency completeness exercisability and reusability.
however we find the importance ofthesecriteriato differfor differenttypesofartifacts and depending on whether the perspective of a reviewer or an artifact userisassumed.
whilereplicability is acriterion frequentlymentioned from the reviewer perspective from which it is a central criterion for artifact acceptance or rejection it plays a much lesser role from a user perspective which unsurprisingly favors reusability over replicability.
besides this difference the reported quality criteria donotsignificantlydiffer butthesetofcriteriastatedforartifact review is more diverse.
on the one hand this observation gives confidence that criteria that are important for reusability are already adequately covered by existing artifact evaluation processes.
ontheotherhand ourobservationisbasedonfewresponseson artifact reuse whichmandates further investigation.
if regarded separately by artifact type we find the various criteria to be of different importance.
for code artifacts documentation isthemostimportantcriterionandwereceivedverydetailedviews on what is expected to be covered by documentation and to which degree of precision.
while exercisability seems to be an obvious criterionthatdoesnotrequirefurtherelaboration codequalityis less specific and discussed differently by respondents.
this may leadtoambiguitiesinthereviewprocessandwewouldrecommend aec chairs to address this accordingly in future cfas or review guidelines.forproofs documentationisalsoconsideredveryimportant but less than understandability which appears to be an equally unspecific term as code quality for code artifacts and we recommend clarification in the cfas.
for data artifacts it is important to respondents that raw data and manipulation scripts are included in the artifact submission in addition to proper documentation especially of formats used.
the inclusion of raw data and scriptsisafairlyunambiguouscriterionthataecchairsmaywant to considerto include inthe submissioncriteriafor data artifacts.
reviewing vs. using artifacts elicit different expectations regarding quality.
in both views expectations toward artifacts vary for differentartifacttypes someofwhichlackcleardefinitionsinthe acmguidelinesandcfas whichmayleadtomisunderstandings.
furtherinsights ourstudyrevealedinsightsbeyondtheexpectationstowardartifacts.
in this section we present anddiscuss thesefindings.
.
satisfactionwith theevaluationprocess we were interested in the opinion of the reviewers toward the current practice of artifact evaluation and asked do you think that theeffortofartifactevaluationisjustified?
.ingeneral wefound that the effort for reviewers is perceived as justified.
specifically our participants found that artifact evaluation guides authors toward good artifacts.
we also found a few interesting cases e.g.
discoveringfraudulent research i once flagged a clearly fraudulent artifact.
its outputs were hardcodedintothesourcecode.notonlywastheartifactrejected but themainpcwasnotified theauthorswerecontacted andthepaper withdrawn.thisisagoodoutcome havingkeptbadworkoutofa topconference.
id 1plaec 477esec fse november8 virtualevent usa ben hermann stefan winter andjanet siegmund however whenweaskedforsatisfactionwiththecurrentartifact evaluationprocess theanswerweremixed.somerespondentswere satisfied somesawpotentialtoimprovetheprocess.forinstance one participant reported ithinkthereisroomtoimprovebutthattheconferencesareactively theprocess seemsto befunctioning.
id 2plaecs otherswerepointingtowardvariousshortcomingssuchasreviewer attention orrecognition as the following participant reported somereviewersarethorough somearenot.
andaecshould berewarded withbetter recognition.
id 1se aec 2plaecs afrequentcriticism 13outof66answers oftherespondents wasrelatedtoamissingqualitystandardforae.respondentsstated they were missing clear criteria according to which artifacts are accepted rejectedorbadgesareawardedandthatthe acmguidelinesaretoogenericandopentointerpretationtoserveasaquality standard.astheseinterpretationscandifferacrossconferences the interpretationofwhatabadgereallymeanscanonlybeunderstood inthe contextofagiven conference andyear .
besides these difficulties regarding thresholds for the ae outcome respondents criticized missing guidance howae should be conducted i.e.
which steps should be taken or which criteria checked for and suggested the development of checklists templates and benchmarks for ae to provideguidanceto aecs.
althoughonlyhalfthereviewersaresatisfiedwiththecurrent artifactevaluationprocess mostofthemstillseethatevaluation isworthit.ourfindingsindicatethatwithartifactevaluation we areontherighttrackandweshouldcontinue.however there isalsoroomforimprovement.inparticular acommonquality standard for artifacts and common review guidelines need to be developed.
.
reviewers experience with artifacts artifactevaluationcommitteesareusuallyrecruitedoutofjunior researchers.
as the process has now been established for several yearsatmajorconferences wewerecuriousifreviewersthemselves haveexperienceinpreparingandsubmittingartifactsasrequired foratruepeerreview.outof 115participants .
indicated to have submitted a research artifact for evaluation before i.e.
a largegroupofreviewershasnoexperience composing aresearch artifact.
moreover in .
.2we reported that few reviewers have experience re using a research artifact.
as artifact evaluation is a comparatively new process this was expected but leaves room for improvement.inparticular thismeansthatcfas reviewinstructions from aec chairs and opinions from fellow reviewers are the solesourceofcriteriaaccordingtowhichartifactsareevaluatedfor many aec members.
given the lack of quality criteria and review guidanceindicatedbyourrespondents cf.section .
andthehigh fluctuationofaecmembersasindicatedbytheaveragenumberof committeesservedon cf.figure thelackofreviewerexperience currently puts an enormous impact and responsibility on how aecchairssteer the reviewprocess.
.
reviewas aninteractiveprocess severalparticipantsvalueclosecommunicationwiththeauthors andwouldlike to increasethe interactivity inreviewsbeyondthe currently established kicking the tires phase in many ae processes where close communicationisenabledfor clarifying setupissues.
ideally i would like to see a two step process of evaluating the artifact and submitting an improved.
however this increases the loadon the artifact evaluation committee.
id whileamulti stepprocessmightbetoodifficulttorealizeforall artifacts someartifactsmaybenefitfromthiswayof shepherding muchalikepapersubmissions.however a kicking the tiresphase orotherinteractiveprocesseshaveonlybeenimplementedin 8out ofthe16 conferences in2019 according to their cfas.
.
tightercoupling to paperacceptance participants indicated their support for a tighter coupling betweenartifactevaluationandpaperacceptancealthoughthiswas not part of any question.
suggestions range from shepherded acceptances mandatory artifact submissionsfor specific tracks e.g.
asisthepracticeforthetooltracksforcavandtacasalready to having artifact submission mandatory for all paper submissions.
we need to have a conditional acceptance that depended on the result of the artefact evaluation i m not proposing this to be the norm butaspecialcaseofacceptancelike shepherding wherefew papers are accepted only if the artefact withstand the statements of the paper.
id 1plaec ifartifactevaluationdoesnothaveanyinfluenceontheacceptanceofapaper reviewersstrugglewiththeincongruousnessbetweenartifactevaluation smissionstatementofreplicableresearch andthe currentpractice.one participant reports theevaluationsdonotdetermineifthepaperisacceptedorrejected so in the bigger picture i didn t much attentionto thesereviews.
id 1plaec inconclusion thecommunitysuggestsmorerigorbyintegrating artifact evaluation muchstronger intothe paper reviewprocess.
threats to validity .
internalvalidity the recollection of our participants experiences with artifacts can be affected by the time span between their occurrence and the participation in our survey.
moreover the perceived purpose of artifactsandtheprocessesoftheirassessment havechangedover time.
figure 1shows that aec members from every year since the initiation of artifact evaluations have participated and most participants have served in recent years.
we therefore do not expecteffectsoftime to significantly affectour results.
thesecondthreatisrelatedtoourparticipantselection.toreceiveopinionsfromresearchersfamiliarwithartifactevaluation we only invited aec members to our survey.
their responsesmay be affectedbyconcernsabouttheperceivedvalueoftheirwork which canleadtooverlypositivereportsregardingacceptedartifactsor overlynegativereportsregardingrejectedornotevaluatedartifacts.
we addressed this by openly communicating the anonymization 478community expectationsforresearchartifacts andevaluation processes esec fse november8 virtualevent usa policy of our study.
moreover the main results we report are relatedtotheexpectationstowardartifactsratherthanthepositive or negative experiences they have led to.
consequently we expect the centralconclusions to not be affectedbythis threat.
toassesstheeffectivenessofourinstruments weconductedpretestingwith6participants.ifthissampleisnotrepresentativeof how the targeted audience perceives our questions this can induce systematic effects in our results.
two participants in the pre test had no experience with artifact evaluations and commented on thefirstdraftversionofoursurvey.theremainingfourpre tests have been conducted by experienced researchers.
while we cannot rule outeffects onour resultsinprinciple mostofthepre testers comments were inline andwe have not seensymptoms ofsevere misunderstandingsintherepliesbeyondwhatiscausedbydiffering understandingsofparticipants whichwe intendedto capture .
.
external validity a threat to the external validity of our study lies in the selection of participants.
our goal is to assess the community s expectation toward artifacts.
with our focus on aec members we potentially createabiastowardspecificexpectations.artifactevaluationsarea relativelynewscientificpeerreviewprocessand assuch aminority ofresearchershaveworkingexperiencewithartifactsorknowledge oftheacmguidelines.we therefore gavepreferencetotheriskof selection biasover the riskofour questionsbeing misunderstood.
three of our pre testers received invitations to participate in the survey and their replies may have been affected by the pre test.
wedeemthisrisktolerable astheinfluenceofthreeresponsesis marginalto the presentedresults from257 replies.
implications in our view artifact evaluation chairs steering committees and communityleaderscantakeseveralactionstoaddresstheissues highlightedinthis paper.
first as a community we have to define the purpose ofartifact evaluation clearer than we do now.
as our discussion showed a twofold purpose of replicability and reusability has several pitfalls even though the two goals share certain characteristics.
a committee might be instated for each community that defines a clear goal specifictotherespectivecommunity.thiscommitteeshouldevaluate changes over the course of time and observe the improvements made e.g.
interms ofmore submissionsto artifact tracks .
second we propose to also work on agreed quality standards in each community in a similar manner.
as we have shown expected artifact quality criteria vary widely between communities perspectives and artifact types.
moreover there is a need to communicate quality criteria very clearly.
misconceptionsabout appropriate acceptance levels seem to be common during artifact evaluation and can leadto serious conflictsas one respondentpointedout.
two students argued for acceptance of the artifact because it was capableofgeneratingoutputwithoutcrashing insomescenarios.
i arguedstronglyagainstthemandtheartifactwaseventuallyrejected.
id 1se aec as most reviewers only serve once on an aec cf.
figure chairs should explicitly brief aec members on appropriate acceptance levels and quality criteria.
it would not be beneficial to push thesespecificcriteriaintoadiscipline widedocumentsuch astheacm guidelines.rather cfasshouldbeextendedtoincorporatethose criteria.
as it seems to be common practice that chairs inherit those cfas from their predecessors the quality criteria can evolve over time and reflect community transitions in a fine grained way thatisalsotrackableforevaluationssuchastheonepresentedhere.
however community representatives should monitor if a common core can be established within a community which we consider mostlikely from our results.
third recruiting reviewers also based on their experience as artifactcreators maynot onlybenefitthequalityandefficiency of artifact reviews but also improve peer consultation for new and less experiencedreviewers.
furthermore artifactreviewersclearlyfeltthatthetimehascome for a tighter coupling between artifact evaluation and paper acceptance.conferencesteeringcommitteesandtrackchairsshouldtake the opportunity to incorporate artifact evaluation into acceptance processes.
tool oriented tracks could take the lead as tacas and cav show and other tracks could follow closely learning from the experience gainedinthe pastdecade.
conclusion the replicability crisis shook the research community and also reachedthesoftwareengineeringandprogramminglanguagecommunity.
a recent attempt to mitigate its rippling through the communities has manifested in the creation of artifact tracks at conferences.
however at this point it is still unclear if or to which degree artifact evaluations are a suitable measure to foster replicable letalonereproducible research.takingafirststeptoward an assessment of artifact evaluations we inspected how the process is currently seen by the community specifically by the people whoperformtheseevaluations.wefoundthattheinitialmission ofartifactevaluationofassessingreplicabilityhasnowgrownto also cover reusability.
however reviewers and users of artifacts see a different purpose of artifacts reviewers want replicability userswantreusability.additionally differentartifacttypeselicit different expectations but they all more or less focus on making artifacts understandableand usable.now we as the researchcommunityneedtoclearlycommunicatetheseexpectationsincallsand guidelines while carefully defining and using terminology to avoid misunderstandingsorfalseexpectations.thisisoneapproachto avoid a replicability crisis of the same large extent as it washed over the psychology community.