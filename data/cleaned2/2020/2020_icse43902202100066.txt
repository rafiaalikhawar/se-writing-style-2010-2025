improving fault localization by integrating value and predicate based causal inference techniques yi git k uc uk department of computer and data sciences case western reserve university cleveland oh usa yxk368 case.edutim a. d. henderson google inc. mountain view ca usa tadh google.comandy podgurski department of computer and data sciences case western reserve university cleveland oh usa podgurski case.edu abstract statistical fault localization sfl techniques use execution profiles and success failure information from software executions in conjunction with statistical inference to automatically score program elements based on how likely they are to be faulty.
sfl techniques typically employ one type of profile data either coverage data predicate outcomes or variable values.
most sfl techniques actually measure correlation not causation between profile values and success failure and so they are subject to confounding bias that distorts the scores they produce.
this paper presents a new sfl technique named unival that uses causal inference techniques and machine learning to integrate information about both predicate outcomes and variable values to more accurately estimate the true failure causing effect of program statements.
unival was empirically compared to several coverage based predicate based and value based sfl techniques on program versions with real faults.
i. i ntroduction there has been a vast amount of research on automated software fault localization afl which seeks to automate all or part of the process of locating the software faults that are responsible for observed software failures.
statistical fault localization sfl or spectrum based fault localization comprises a large body of afl techniques that apply statistical measures of association some computed with machine learning or data mining techniques to execution data execution profiles or spectra and to failure data e.g.
pass fail labels in order to compute putative measures called suspiciouness scores of the likelihood that individual program statements or other program elements are responsible for observed failures.
these scores are used to guide developers to faults e.g.
by using them to highlight suspicious statements in graphical displays of code or to rank statements for inspection .
statistical fault localization is also the first step in a number of automated program repair techniques .
sfl techniques are applicable when the data they require are readily and cheaply available in sufficient quantity.
in particular they generally require data from a diverse and penetrating set of tests or operational executions that includes significant numbers of both failures and successes.
such data might be available for instance from deployed software that is instrumented to record profile data and that is equipped with a mechanism by which users may report failures they encounter.
it seems fair to say that statistical fault localization research stands at a crossroads.
although a large number of sfltechniques have been proposed to our knowledge none of them consistently locates faults with enough precision to justify its widespread use in industry.
in part this is because most such techniques rely on just one source of information about internal program behavior code coverage profiles or similarly recorded outcomes of program predicates such as those that control the execution of conditional branches and loops.
recent work has sought to overcome this limitation e.g.
by employing value profiles spectra profiles of variable values or by combining different kinds of information pertinent to fault localization such as coveragebased sfl scores textual similarity measures and faultproneness predictions based on static program analysis .
another problem with existing sfl techniques is that many of them suffer from confounding bias which can cause a correct statement to appear suspicious because of another faulty statement that influences its execution.
recent work has also sought to overcome this problem by employing causal inference techniques .
existing sfl techniques are based on analysis of code coverage profiles or predicate profiles on one hand or of value profiles on the other hand.
to our knowledge no previous technique is both coverage based or predicate based and value based.
while two techniques one of each kind can be combined simply by taking the average or maximum of the scores they produce for each potential fault location this will not in itself properly address confounding bias.
this paper proposes a new approach to statistical fault localization that integrates information about both predicate outcomes and variable values and that does so in a principled way that controls for confounding bias.
the key to this approach which we call unival is to transform the program under analysis so that branch and loop predicate outcomes become variable values so that one causally sound value based sfl technique can be applied to both variable assignments and predicates.
this paper reports on an large scale empirical evaluation of unival involving the latest version .
.
of the widely used defects4j evaluation framework which contains seventeen software projects and program versions containing actual faults.
in this study unival was compared to several coverage based predicate based and value based sfl techniques.
unival substantially out performed each of these ieee acm 43rd international conference on software engineering icse .
ieee techniques.
to the best our knowledge this is the only fault localization study that has used all the programs in the latest version of defects4j and it uses the largest number of real programs and faults of any study.
we also report empirical results characterizing the relationship between the cost of fault localization and an important property of data in a causal inference study called covariate balance .
these results help explain the effectiveness of unival .
ii.
m otivating example to illustrate our technique we now apply unival to locate a real software fault.
this example is from the 62nd faulty version of google s closure compiler closure from the widely used defects4j repository v2.
.
this fault exists in the format method which is located between lines in the lightweightmessageformatter class.
the faulty segment of the code is depicted in listing with some elements omitted for brevity and with the lines of code renumbered.
the method format is intended to compose an error message for a script written in javascript.
as input parameters format takes a jserror object named error which contains information about the nature of the error and a boolean value named warning which indicates whether the error message should be formatted as a warning message.
however this method has a faulty predicate at line is 1private string format jserror error boolean warning ... intcharno error.getcharno column number within line fix charno sourceexcerpt.length if excerpt.equals line charno charno sourceexcerpt.length for inti i charno i char c sourceexcerpt.charat i if character.iswhitespace c b.append c else b.append b.append n ... listing original code for the motivating example 1private string format jserror error boolean warning ... intcharno error.getcharno boolean p3 0 false boolean p3 1 false boolean p3 2 false if p3 2 excerpt.equals line p3 1 charno p3 0 charno sourceexcerpt.length boolean p4 0 false for inti p4 0 i charno i char c sourceexcerpt.charat i boolean p5 0 false if p5 0 character.iswhitespace c b.append c else b.append b.append n ... listing predicates in listing extracted to boolean variables.used instead of that prevents the error column number charno from being displayed correctly for some inputs to the method.
the fix for the faulty predicate expression is shown in a comment on line .
the code in listing illustrates how confounding bias may arise in fault localization the outcome of the faulty predicate at line confounds the causal effects on program failure of the statements at lines .
this implies that unless we adjust for confounding the suspiciousness scores calculated for the variables and predicates located at those lines are likely to be biased and distorted.
in order to apply unival and other fault localization techniques to the code in listing we first used a tool we developed named predicatetransformer to transform the predicates in branch and loop conditions into assignment statements that assign the results of evaluating the predicates to new boolean variables.
note that each predicate in a compound boolean expression is transformed in this way.
the transformed code is depicted in listing .
next we used our instrumentation tool gsa gen on the predicate transformed program to generate our own variant of thegated static single assignment gsa form representation of the program.
this tool collects information about data dependencies between variables and it also instruments the program to record the runtime values of assignment targets in a dictionary data structure that is maintained by a java class we created named collectout .
the variable values are recorded by inserting calls to a static method collectout.record referred to as record in listing which takes the following 1private string format jserror error boolean warning ... intcharno error.getcharno record formatter format charno 0 charno boolean p3 0 false record formatter format p3 0 0 p3 0 boolean p3 1 false record formatter format p3 1 0 p3 1 boolean p3 2 false record formatter format p3 2 0 p3 2 if p3 2 excerpt.equals line p3 1 charno p3 0 charno sourceexcerpt.length record formatter format charno 2 charno record formatter format charno 3 charno record formatter format p3 2 1 p3 2 record formatter format p3 1 1 p3 1 record formatter format p3 0 1 p3 0 boolean p4 0 false record formatter format p4 0 0 p4 0 for inti p4 0 i charno i record formatter format p4 0 1 p4 0 record formatter format charno 1 charno char c sourceexcerpt.charat i record formatter format c 0 c boolean p5 0 false record formatter format p5 0 0 p5 0 if p5 0 character.iswhitespace c record formatter format p5 0 1 p5 0 record formatter format c 1 c b.append c else b.append b.append n ... listing instrumented version of the code that belongs to the format method from listing 650parameters in the given order package name class name method name line number encountered in the original code code block number name in gsa form value to be recorded and the version of the program variable.
the gsa version of the code in listing is shown in listing .
consider what happens when gsa gen encounters the assignment statement p3 1 charno at line of listing .
the tool then inserts the proper gsa variable version increments inserts a call to collectout.record to record the value of p3 1 and the other parameters mentioned above and adds a key value pair to the dictionary where iandjare the respective variable versions in the assignment statement.
the value s for key p3 1 i are later used for confounding adjustment see section iv .
we tested the gsa transformed program with the developerwritten tests in the defects4j suite using the testcommand to run these tests.
out of the tests two failed with assertion errors.
we applied two of the coverage based statistical fault localization metrics that performed the best in recent studies namely ochiai and dstar to the code coverage and failure data for all of the tests.
we then ranked the variables and predicates of the faulty class lightweightmessageformatter in nonincreasing order of their suspiciousness scores with ties receiving the average rank for all tied variables .
the ochiai and dstar metrics produced identical rankings.
the faulty predicate p3 0 at line in listing line in listing received the same score as other variables and ranked in 17th place in the resulting suspiciousness list which included all of the variables and predicates within the ifbranch in the faulty method.
the low rank for the faulty predicate is likely due to confounding which the ochiai and dstar metrics don t adjust for.
we next applied unival to the same transformed program.
we input the data recorded by our run time library collectout together with the values of a binary outcome variable y which indicates whether the program executions failed or succeeded to our method for calculating suspiciousness scores.
this code executes the analysis phase ofunival which is described in section iv.
finally we mapped these scored statements back to the original program.
the suspiciousness list produced with unival contained unique scores for each variable and predicate.
the faulty predicate p3 0 at line in listing line in listing was ranked highest among all the predicates with a score of .
and it ranked 5th among all assignment targets.
the variables charno 2 andcharno 3 recorded at lines and of listing which are recorded for possible use in simulating a gsa form iffunction see section iii b represent the value of the variable charno in lines and .
they would both be ranked 1st overall with a score of .
if we included them in the ranking.1the non faulty predicate 1in our empirical evaluation we assign scores to only the predicates in branch conditions.p4 0 at line in listing line in listing was ranked 18th with a score of .
.
we performed an extra step to the experiment to illustrate that the adjustment for confounding in unival indeed makes a difference.
we replicated the steps described previously for using unival with a minor change for the correct predicate p4 0 which is located at line in listing .
we removed p3 0 at line in listing from the set of adjustment variables covariates in the random forest model for p4 0 .
consequently the score for p4 0 increased to .
which changed its rank to 14th.
the new score and rank of p4 0 were higher than those of the faulty predicate p3 0 whose score and rank declined to .
and 16th.
therefore the suspiciousness scores for the variables were in fact distorted by the lack of adjustment for confounding bias.
iii.
b ackground a. causal inference in this section we provide background on statistical causal inference required to understand our technique unival .
statistical causal inference is concerned with estimating without bias the average causal effect of a treatment variable upon an outcome variable.
a treatment variable orexposure variable tis a variable that an investigator could at least in principle intervene upon to change its value.
for example in sfl t might represent the outcome of a branch predicate which a developer could change in a debugger.
an outcome variable yis an observable variable of interest to an investigator such as an indicator of whether a program execution fails or not.
in statistical causal inference one is concerned with outcomes for individuals or units in a population and it is often convenient to denote the outcome for a unit ibyyi.
in sfl the units are typically program executions.
a key concept in causal inference is that of a counterfactual outcome which is a value that the outcome variable ycould take on if counter to the facts the treatment variable ttook on a valuet0different from the value tthat it actually took on.
a very similar concept is that of a potential outcome which is a value that the outcome variable could take on if the treatment variable were assigned a particular value.
as is common in the causal inference literature we shall use the terms counterfactual outcome and potential outcome interchangeably.
in the influential neyman rubin causal modeling framework a distinct potential outcome random variable yt texists for each possible value tof the outcome variable.
in principle for each unit iand each pair of treatment values tandt0there is an individual causal effect i yt i yt0 i whereyt iandyt0 iare values of the potential outcome random variablesyt t i andyt t0 i respectively.
conceptually the average causal effect ace oftonyover a population is the expected value e of the individual causal effects i over that population.
however in general it is not possible to compute the i because at most one of yt t i andyt t0 i is observed for each i namely the potential outcome under the actual treatment.
this is known as the fundamental problem of causal inference .
in this sense causal inference is 651fig.
example confounding dag amissing data problem .
in statistical causal inference this problem is circumvented by estimating average causal effects from a study sample of units and by borrowing information from each unit in the sample.
in a randomized experiment which is an interventional study in which the the investigator assigns treatments randomly to units the missing potential outcomes are missing completely at random and the average causal effect e e e e can be estimated by computing the average outcomes in the treatment groups with t tandt t0and taking their difference .
however in observational studies generally and in sfl applications in particular treatment assignment is usually notrandomized and hence the difference of the treatment group averages is often a biased estimate of the average causal effect.
in sfl for example whether a particular statement is executed during a program run or whether a given variable is assigned a specific value depends on the behavior of other statements which may also affect whether the program fails.
different types of bias can affect a causal effect estimate .
the best known form of systematic bias and the one that will be addressed in this paper is confounding bias or simply confounding .
confounding is bias due to the presence of a variable that is a common cause of the treatment variable and the outcome variable.
confounding bias is best explained in terms of a causal graph and such graphs are also used to identify confounders which are variables that can be statistically adjusted for during causal effect estimation in order to reduce or eliminate confounding bias.
a causal directed acyclic graph orcausal dag is a dag in which the vertices represent causal variables and in which there is a directed edge a b ora!bbetween two variables aand bonly ifais known or assumed to be a cause of b. figure is a very simple example of a dag involving three variables t c andy.
the edget!yrepresents the direct causal effect of tony.
the dag indicates that cconfounds this effect because cis a common cause of tandy.
the noncausal path t c!y which is called a backdoor path because it begins with an arrow into the treatment variable represents a biasing flow of statistical association from tto yviac.
as a result of this flow a naive unadjusted estimate of the ace would mix the causal effect of tonywith the association carried by the backdoor path.
this implies that to obtain an unbiased estimate of the ace cmust be adjusted for.
one way to obtain an unbiased estimate of the ace is to block d separate the backdoor path t c!ybyconditioning on the value of cduring the analysis e.g.
by computing causal effect estimates separately for each level or stratum ofcand then combining them via a weighted average to obtain an estimate of the population ace.
causal inference theory provides a number of results that characterize the sets of variables that may be used for confounding adjustment in terms of the structure of a causal dag.
for example thebackdoor adjustment theorem states that a set z of variables that blocks every backdoor path between the treatment variable and the outcome variable is sufficient for confounding adjustment.
another way to estimate the average causal effect which we employ in this paper is to interpolate or predict the missing counterfactual outcome yt t i oryt t0 i for each unit based on the data for all the units in the study sample and then to plug in the predicted value in the formula for the individual causal effect i. b. gated static single assignment form gated single assignment form gsa form is an extension of static single assignment form ssa form .
ssa form is a specialized intermediate program representation that makes the data flow of a program explicit by ensuring that each variable is defined in exactly one location hence the name static single assignment form .
when two or more definitions for a variable reach a single use a new definition is created to merge the reaching definitions by using a special pseudofunction called which picks the correct definition to use at runtime.
gsa replaces the function of ssa form with three gating functions if entry and exit alternatively and respectively .
ifrepresents the merging of control flow after an if statement and takes as an argument the predicate in the controlling if statement allowing ifto choose the correct value.
entry is similar to ifexcept it merges loop carried variables at the top of iterative control structures.
finally exit merges variables that are both liveat the exit of a loop and are modified by that loop.
taken together these three new gating functions effectively embed the control dependence graph into the intermediate representation by linking the choice of the variable definition to use to the predicate which controls the computation.
the instrumentation used by unival is inspired by and its placement is guided by gsa form.
if gsa form was directly converted into program instrumentation it would be expensive due to the extra runtime overhead implied by the gating functions if entry and exitfor choosing the right definitions.
instead instrumentation is inserted to record values and their controlling predicates at the locations where the gating functions would have appeared in gsa form.
see section ii.
this permits us to determine causal parents needed to control for confounding in unival .
iv.
m ethod in this section we describe the unival fault localization technique in detail.
unival is based on predicting the values 652fig.
causal fault localization method unival of individual counterfactual outcomes using machine learning models that are trained on data from a sample consisting ideally of significant numbers of both passing and failing executions.
there is a separate model for each assignment to a program variable including assignments inserted into branch predicates e.g.
lines in listing .
the data required for each execution and each assignment includes the values of the treatment variable t the outcome variable y and the set of covariatesx.
the treatment variable is the assignment target the outcome variable indicates whether the execution passed or failed and the covariates are the parents of the treatment variable if any exist in the causal dag for the program being debugged.
for example the recorded data for line of listing includes the values of p3 1 andcharno as well as the program outcome pass or fail .
note that each backdoor path see section iii in the causal dag begins with an arrow t p wherepis a parent of t. thus by the backdoor adjustment theorem see section iii the set xof parents of tblocks all backdoor paths from ttoy and therefore xis sufficient for confounding adjustment.
figure depicts the operation of unival .
the first two steps in unival which together we call the instrumentationphase are source to source transformations of the program pto be debugged.
first each predicate in a branch conditions ofpis transformed into an assignment statement that assigns the value of the predicate to a new boolean variable.
see for example lines of listing which correspond to line of listing .
we created a prototype tool named the predicatetransformer for this task.
this tool also records information about each predicate including the type of control statement it belongs to e.g.
while for ifelse if the predicate expression and the line number where it was encountered in the original java file.
second the resulting program is transformed by another prototype tool we created named gsa gen into the specialized version of gated single assignment form described in section iii b. this entails inserting calls to a function that records the values of treatment variables and covariates as well as other information needed by our implementation.
at the same time the causal parents of assignment targets the variables whose values are used in the assignment are determined.
the next phase of unival which we call the profiling phase involves executing the instrumented gsa version gsa p of the program on a set iof test cases or operational inputs 653in order to record the variable values and other information mentioned above.
note that for a treatment variable tassigned to in a loop only the last value assigned to tis recorded along with the corresponding covariate values.
it is assumed that the program outcomes pass or fail for these executions are already known or are determined prior to analysis.
inunival assignment targets will have missing values in particular program runs which we call nas if their assignment statements are not executed.
if the treatment variable has no recorded values or just one unique value it is omitted from unival s consideration.
note that parts of a compound predicate expression might have missing values due to shortcircuited evaluation.
the third phase of unival which we call the analysis phase involves fitting a counterfactual prediction model for each assignment target in gsa p .
we adopted the counterfactual prediction approach to analysis described in although unlike unival that work does not address predicates or strings.
in the current implementation of unival we employ random forest learners as prediction models because they are flexible enough to non parametrically model a wide variety of relationships between the treatment variable and covariates on one hand and the outcome variable on the other hand.
specifically we used the ranger random forest package .
the model for a given variable assignment includes the treatment variable the assigned variable and the covariates the used variables as predictors and the model is used to predict the counterfactual program outcomes pass or fail under different values of the treatment variable.
for each treatment variable t a set reptof representative treatment values is chosen and counterfactual outcomes are predicted for these treatments.
reptis chosen differently based on the type of t. for a boolean or categorical treatment variablet reptcontains all recorded values of t. for a string variable a clustering algorithm is first used to cluster the recorded treatment values and then reptbecomes the set of cluster ids which are treated like categorical values.
we use the stringdist package to obtain a matrix of distances between values.
this matrix is input to the distance based clustering algorithm dbscan with minpts dim wheredim is the dimension of the dataframe .
for a numeric treatment variable t reptconsists of the quantiles of the empirical distribution of the recorded values oft.unival does not currently handle other data types.
for each representative treatment value t2reptand each complete input itop unival predicts the counterfactual outcomeyt i. this is done by plugging tinto the model together with the covariate values xirecorded at the assignment to tduring execution of poni.
note that unival predicts counterfactual outcomes even for actual recorded treatmentcovariate combinations that is even when the true counterfactual outcome is known.
given the set of predicted counterfactual outcomes for t unival computes for each t2rept an estimate e of the counterfactual mean e by averaging the predictions f yt ig.
the suspiciousness score fortis set to the maximum over all pairs t t02reptof e e .
that is the score is the maximum over all pairs of representative treatment values for t of the average failure causing effect of assigning tinstead oft0tot.
the final phase of unival which we call the localization phase involves employing the suspiciousness scores to assist developers in finding the cause or causes of failures observed whenpwas executed on the set of inputs i. traditionally this is done by ranking statements in non increasing order of their suspiciousness scores and then having developers inspect statements in that order .
although this is convenient for evaluating sfl techniques and it is used for that purpose in this paper it has been argued that this is a simplistic approach to fault localization which programmers are in many cases unlikely to follow e.g.
when many statements in a program get very high scores .
we envision unival being used in combination with other sources of information including developer knowledge and intuition to effectively localize faults.
v. e mpirical evaluation a. study setup we empirically evaluated the fault localization performance ofunival in a substantial empirical study involving subject programs from the latest expanded version .
.
of the popular defects4j evaluation framework .
we compared the fault localization costs of unival and several competing techniques the non interventional value based techniques elastic predicates esp and numfl specifically the two variants numfl dlrm and numfl qrm the non interventional coverage based technique of baah et al.
which employs linear regression for causal inference the interventional technique predicate switching which alters conditional branching and finally two well known coveragebased sfl cb sfl metrics that performed well in recent comparative studies namely ochiai and d star with star .
there is one important note concerning the implementation of baah et al.
s original technique based on linear regression .
with that technique the only covariate in the regression model was a coverage indicator for the forward control dependence predecessor of the target statement.
for this study we have modified baah et al.
s technique by including the variables used at the target statement as covariates.
we believe this is a notable improvement on the original technique.
the modified version almost always performs second best among the studied methods.
defects4j version .
.
is a collection of programs and faulty program versions containing a wide spectrum of real software faults.
the number of subject programs and the total number of faulty program versions have nearly doubled in this release of defects4j.
although the very low failure rates of many defects4j programs make them non ideal for statistical fault localization its user friendly scripts and continuous support and evolution makes it an invaluable source for empirical studies.
654fig.
process of empirical evaluation in our study the techniques we compare assign suspiciousness scores to different program elements.
unival assigns scores to numeric string and categorical assignment statements and to predicates esp assigns scores to numeric assignment statements and to predicates and numfl assigns scores to each subexpression of numeric assignment statements.
predicate switching assigns scores only to predicates.
baah et al s linear regression technique and the other coverage based sfl techniques assign scores to all the statements although each statement within a control dependence region receives the same score.
to enable a fair comparison between techniques we confined the comparison to only predicates and numeric assignment statements.
note that to reduce overhead in this study only the faulty classes are instrumented.
with predicate switching however we reported the cost of finding the nearest predicate to the fault since the technique does not assign scores to non predicates.
as a result of these restrictions our study did not evaluate unival s ability to handle string variables.
we intend to do so in a future study.
for a few program versions numfl failed to assign a suspiciousness score to any assignment statement and the suspiciousness list only contained zero values.
we suspect this is caused by using a binary outcome variable rather than the absolute difference between the actual and expected output as numfl is intended to do.
we did not include these versions in our comparison.
we also did not include a program version if it had fewer than relevant test cases which cover a faulty class .
the remaining subject program versions are summarized in table i. we used the exam score measure and hit n measure sometimes referred to as recall n or top n to report the cost of fault localization for each technique.
exam score is the percentage of program statements a developer must examine in non increasing order of their suspiciousness scores before finding the fault.
if there are ties in the scoring of statements we assumed that half of the tied statements will have to be examined before a programmer localizes any fault among them.
hit n is the number of program versions for which a fault was found within the top n ranked statements in non increasing order of suspiciousness scores.
because our comparison involves assignment statements and predicates ifdefects4j subject programs program id kloc average of tests of faulty versions chart cli csv math time lang closure mockito codec jxpath gson collections compress jsoup jacksoncore jacksonxml jacksondatabind table i summary of subject programs the fault was not directly related to an assignment statement or predicate e.g.
a fault of omission we determined a set of fault localization candidates with an approach similar to the one described by pearson et al.
.
finally we used a linux ubuntu .
lts machine that runs on an intel i5 930h quad core cpu at .
.
ghz and that has 8gb of ram for our experiments.
the time measurements are calculated by inserting a timestamp at the beginning and at the end of each step of the pipeline script and reporting the averages for all programs.
the exam and hit n metrics involve simple average or count calculations hence we used microsoft excel to compute them.
we ran all the program versions in our comparison times and averaged the results over all runs in case techniques displayed random variation.
b. results table ii provides an summary of the results of the empirical evaluation showing the average performance of each technique on each program using the three evaluation methods exam hit and hit .
lower exam scores are better than high ones while higher hit and hit scores are better than low ones.
average runtimes for the methods in our empirical evaluation were .
seconds for unival .
seconds for numfl cumulative time to run both models seconds for predicate switching .
seconds for baah2010 .
seconds for esp and .
seconds for the coverage based techniques.
the average overhead of our instrumentation is about e.g.
math takes seconds to execute without the instrumentation and seconds with the instrumentation .
we make two observations about unival s overall performance as shown in table ii.
first unival usually had the best score for a given program cost metric combination.
second as can be seen in the exam score table unival s scores display less variation than other methods.
this indicates not only that unival is capable of relatively precise localization as shown in the hit table but also that it provides more consistent results.
in contrast the well known ochiai method provides precise localization somewhat frequently but it exhibits much more variation.
655table ii comparison of unival and competitive metrics for all defects4j programs to evaluate the statistical significance of our results we conducted wilcoxon signed rank tests for the difference in performance between unival and other techniques for each part of table ii.
the resulting p values are reported in the bottom row of each part of the table.
each difference in performance between unival and another technique is significant at the .
level.
figure visually compares unival s performance versus the other techniques using the exam score.
there are only seven instances in which unival does worse than a competing technique out of total .
in general this study indicates that of all the techniques considered unival has the best performance on the the defects4j dataset.c.
the effect of covariate balance in causal inference covariates are variables other than the treatment variable or the outcome variable that may be associated with either variable.
they are used for confounding adjustment or for reducing the variance of estimates.
in the absence of confounding as in a randomized experiment the joint distributions of the covariates should be similar in both or in all treatment groups .
this condition is called covariate balance .
it typically does not occur in observational studies.
there are techniques such as matching that attempt to achieve covariate balance in the analysis sample by removing certain units from the original study sample.
however unival statistically adjusts for covariates rather than modifying the study sample.
to better understand the effect of covariate balance and imbalance on the performance of unival and coverage based fault localization we conducted a sub study in which we measured the degree of covariate imbalance in the data sets for faulty defects4j versions and we related it to the cost of fault localization as measured by exam score forunival and the coverage based sfl metric ochiai .
to simplify the comparison we considered only program versions that contain a faulty predicate in a branch condition.
thus for both the unival and ochiai techniques the treatment value corresponded to the outcome of that predicate true orfalse .
additionally we considered only predicates that have covariates with numerical values.
to measure covariate imbalance we calculated the mean over all covariates of the difference in the mean values of individual covariates for the two treatment groups.
we located the program versions in release .
of the defects4j repository that contained faults in branch predicates by consulting a recent study that details the fault types in that release .
for release .
of defects4j we searched among the fault fix patches included with the release.
we found a total of program versions fitting our criteria.
the results of our study of the effect of covariate imbalance on fault localization are depicted in figure .
the figure shows a scatter plot in which the x axis represents the mean over all covariates of the difference in the mean values of individual covariates for the two treatment groups and in which the yaxis represents the exam scores for unival andochiai for data sets with given levels of covariate imbalance.
note that along the x axis larger values represent greater imbalance and along the y axis larger values represent higher costs for fault localization.
the results indicate that higher fault localization costs are associated with greater imbalance.
this is consistent with previous results indicating that covariate balance is associated with lower estimation bias in observational studies .
however it is evident in figure that even when the covariates are imbalanced the cost of fault localization with unival is usually lower than with ochiai.
this is due to the fact that unival adjusts for confounding and therefore mitigates the effects of covariate imbalance.
656fig.
exam score reductions achieved by unival over other techniques for all defects4j programs fig.
relationship between covariate imbalance and exam score d. threats to validity internal validity as previously noted in the literature the exam score and hit n metrics are imperfect evaluation methods for automatic fault localization .
in particular they assume a programmer will always start at the top of the ranked list of program elements and move monotonically down the list ignoring the surrounding program structure.
they also assume that the programmer is equally likely to examine program elements with the same suspiciousness score.
but programmers use their intuition background knowledge of the program and other methods of debugging when using automatic fault localization tools.
hence this evaluation method does not fully account for programmer behavior.the exam and hit n cost metrics handle program elements with the same suspiciousness scores by giving each such element the same average rank score see standard rank score in .
this creates evaluation bias in favor of fault localization techniques that are more likely than others to assign different elements the same suspiciousness score such as the coverage based metrics ochiai and dstar .
however some of the techniques we studied including the proposed technique unival don t tend give the same scores to different program elements.
external validity the defects4j dataset is a very useful collection of programs bugs and programmer written tests.
however no collection can be all encompassing and 657this collection is still only a small sample of real programs and their bugs.
a technique performing well on these programs might not perform particularly well on any other program and vice versa .
second the test cases are provided by the developers and are generally in the form of unit tests.
arguably end to end system tests would be a better choice for evaluating fault localization techniques .
in particular the tests fail at very low rates on the faulty program versions in the dataset.
ideally there would be more balance between failing and passing tests.
finally because the tests are generally unit tests they do not simulate operational input by end users the programs.
in general despite the weaknesses outlined above the authors feel this study is a state of the art empirical evaluation of fault localization techniques.
the study employs only real bugs real tests and substantially sized programs.
it does not use the often criticized injected faults or generated test cases.
nor was it conducted on toy programs constructed for the purpose.
finally every effort was made to put the previous work in the best light from improvements to baah s method to filtering out faults which numfl could not localize.
vi.
r elated work cleve and zeller proposed an interventional approach to fault localization based on cause transitions points in time where a variable starts to become the cause of failure and they showed how delta debugging can be used to find them.
jeffrey et al.
presented a value based fault localization technique called value replacement which searches for program statements whose execution can be intervened upon to change incorrect program output to correct output.
similarly earlier work by zhang et al.
introduced predicate switching which searches predicates that are executed by failing tests and whose outcomes can be altered to make the program succeed.
johnson et al.
present an approach to causal testing in which input fuzzing is used to generate passing and failing tests that are similar to an original failing test.
the generated tests are then used to pinpoint failures.
the aforementioned techniques each require a complete or partial test oracle.
furthermore they also entail possibly costly repeated runs of subject programs and searches among them while unival does not require any oracle or repeated runs of subject programs.
fariha et al.
propose a technique called automated interventional debugging aid that seeks to pinpoint the root cause of an application s intermittent failures and to generate an explanation of how the root cause triggers them.
aid approximates causal relationships between events in terms of their occurrence times and intervenes to change the value of predicates in order to prune a causal dag and extract a causal path from the root cause to a failure.
although aid makes use of counterfactuals it does not adjust for confounding bias.
baah et al.
pointed out that the conventional sfl techniques are susceptible to confounding bias and they employed causal inference methodology to localize faults.
a linear regression model was fitted for test outcomes with acoverage indicator for the target statement as the treatment and a coverage indicator for its forward control dependence predecessor as a covariate.
bai et al.
presented two variants of a value based causal statistical fault localization technique called numfl .
this technique makes use of generalized propensity scores which are used to achieve covariate balance.
although these causal inference based techniques adjust for confounding like unival they employ parametric regression and thus lack the modeling flexbility of random forests.
elastic predicates esp which were presented by gore et al.
involve measuring how different in standard deviations an assigned variable s value is from its average value.
this difference is used as a suspiciousness score.
in contrast to unival esp does not control for confounding bias.
feyzi et al.
proposed an approach to reducing the number of statements that must be considered in fault localization.
they use backward slicing techniques and information theory to find candidate cause effect chains introduced by zeller et al.
before applying causal inference based techniques such as baah2010 on these candidates.
their method is still subjected to the limitations of the techniques it combines which are mentioned throughout this section.
recent studies have investigated combinations of sfl metrics or sources of information for use in fault localization.
xuan et al.
presented a technique that uses learning torank methods to combine multiple sfl metrics.
sohn and yoo combined sfl results with source code metrics from static analysis.
zou et al.
found that combining a variety of sfl techniques was beneficial.
li et al.
uses deep learning with neural networks to integrate multiple sources of information that vary from sfl metrics to textual similarity measures.
although these approaches to combining different sources of information in fault localization are promising they do not adjust for confounding or other biases and therefore their results are prone to bias.
mutation based sfl techniques measure the suspiciousness of a program statement by how much a mutation to it can change the number of program failures that occur on a test set.
these techniques often generate a very large number of mutations hence they may be very costly to apply.
unlike unival they do not employ established causal inference methodology.
model based debugging mbd techniques apply modelbased diagnosis to software fault localization .
in mbd a logical model of a program is derived automatically from its source code and a search algorithm finds minimal sets of statements whose faultiness can explain incorrect behavior observed on test cases.
abreu et al.
presented a new model based approach named barinel to diagnosing multiple intermittent faults which uses a bayesian method for estimating the probability that a faulty component exhibits correct behavior.
recent studies have applied modelbased debugging to spreadsheet programs which are a special type of numerical program.
we believe unival might be an efficient algorithm for spreadsheet programs.
none of the aforementioned mbd techniques addresses confounding bias.
658vii.
c onclusion this paper has presented unival which is a novel approach to statistical fault localization that is based on statistical causal inference methodology and that integrates value based and predicate based fault localization by transforming predicates into assignment statements.
it uses a machine learning model to estimate with minimal bias the average causal effect of counterfactual assignments to program variables without actually changing the program or its executions.
unival currently handles the values of numeric boolean categorical and string values.
we reported the results of an extensive empirical evaluation of unival in which it outperformed a variety of competing techniques.
in future work we intend to expand the range of program elements and data types to which unival applies.
data availability our implementation for the prototype tools and experiments presented in this paper is publicly available .
acknowledgement this work was partially supported by nsf award ccf1525178 to case western reserve university.
the authors would also like to thank zhoufu bai for providing scripts we used for including numfl into our evaluation.