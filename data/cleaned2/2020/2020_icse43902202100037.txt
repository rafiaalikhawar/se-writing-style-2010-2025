graph based fuzz testing for deep learning inference engines weisi luo dong chai xiaoyue run jiang wang chunrong fang zhenyu chen hisilicon huawei china state key laboratory for novel software technology nanjing university china corresponding author zychen nju.edu.cn abstract with the wide use of deep learning dl systems academy and industry begin to pay attention to their quality.
testing is one of the major methods of quality assurance.
however existing testing techniques focus on the quality of dl models but lacks attention to the core underlying inference engines i.e.
frameworks and libraries .
inspired by the success stories of fuzz testing we design a graph based fuzz testing method to improve the quality of dl inference engines.
this method is naturally followed by the graph structure of dl models.
a novel operator level coverage criterion based on graph theory is introduced and six different mutations are implemented to generate diversified dl models by exploring combinations of model structures parameters and data inputs.
the monte carlo tree search mcts is used to drive dl model generation without a training process.
the experimental results show that the mcts outperforms the random method in boosting operator level coverage and detecting exceptions.
our method has discovered more than different exceptions in three types of undesired behaviors model conversion failure inference failure output comparison failure.
the mutation strategies are useful to generate new valid test inputs by up to an .
more operatorlevel coverage on average and .
more exceptions captured.
index t erms deep learning inference engine graph theory deep learning models operator level coverage monte carlo tree search i. i ntroduction deep learning dl is a popular method for hard computational problems in various domains such as image classification and speech recognition .
there are almost innumerable combinations of dl frameworks data sets models platforms and so on .
on the hardware side the platforms are tremendously diversified ranging from common processors e.g.
cpus gpus and npus to fpgas asics and exotic accelerators such as analog and mixed signal processors.
these platforms come with hardware specific features and constraints that enable or disrupt inference depending on dl models and scenarios.
on the software side a number of dl inference engines invoked by dl applications commonly serve for optimizing various dl models and performing run time acceleration inference on the above devices such as nvidia tensorrt tensorflow lite and alibaba mnn .
hence the quality of dl inference engines supporting dl models is important to the quality of applications.
to the best of our knowledge there is still a lack of systematic testing methods for dl inference engines.
fuzz testing is a widely used automated testing technique that generates random data as inputs to detect crashes memoryleaks and other failures in software .
fuzz testing has been shown to be a promising direction for quality assurance of dl systems .
however the existing fuzz testing techniques depend heavily on manually designed dl models and usually perform perturbations e.g.
layer addition layer removal data shuffle noise perturbation on such existing models or data .
different from fuzz testing on dl models fuzz testing on dl inference engines is expected to generate diversified dl models by exploring combinations of model structures parameters weights and inputs.
it is very challenging to generate such complex graph structured data automatically and effectively.
the first challenge in fuzz testing of dl inference engines is to generate diversified dl models to trigger different structured parts of a given dl inference engine.
these structured parts include model structure conversion model optimizations e.g.
operator fusion data format conversion e.g.
nchw nhwc nc4hw4 operator implementation calculation graph scheduling data movement a hierarchy of tiles and so on .
the second challenge is to capture the behaviors of each test such that fuzz testing can be well directed in generating new tests.
the existing neural coverage criteria of dl models cannot work in this testing scenario because the inputs for testing dl inference engines are dl models.
a novel criterion is required to capture behaviors of dl inference engines rather than dl models.
it is natural to design testing methods inspired by the graph structure of dl models and analyze the behaviors of the dl inference engine under test against different dl models.
in this paper a novel graph based fuzz testing method is designed to test dl inference engines via generating dl models as digraphs.
the idea of this method naturally conforms to the graphic structure of the dl model.
it alleviates the problem of generating a large number of diverse dl models.
since dl models are not a simple digraph they have rich characteristics of deep learning elements.
thus beyond the basis of dl model generation four model level mutations including graph edges addition graph edges removal block nodes addition block nodes removal and two source level mutations including tensor shape mutation and parameter mutation are proposed to generate more diversified dl models effectively.
to guide more effective fuzz testing for a given dl inference engine under test the dynamic behaviors of each test a ieee acm 43rd international conference on software engineering icse .
ieee dl model should be captured.
the operator level coverage criterion is introduced from graph theory to measure the parts of a dl inference engine s logic exercised by a test set a number of dl models based on operator types structures e.g.
input degree output degree and single edges from graph theory tensor shapes and parameters.
the success ratio and operator level coverage of an operator are used as feedback to the monte carlo tree search mcts .
mcts is used to solve the search problem to determine whether an operator is select or not in a new model such that the most promising blocks can be chosen to generate stochastic dl models.
the experiments have been designed and conducted with mnn on x86 cpu and arm cpu.
the experimental results shows that our method is effective in detecting exceptions of dl inference engine with more than different exceptions discovered such as crashes inconsistencies nan and inf bugs.
besides the mcts based search performs better than the random based search in boosting operator level coverage .
more and detecting exceptions .
more .
furthermore the mutation strategy helps produce .
more operator coverage and .
more exceptions detected on average.
our main contributions in this paper are as follows.
a novel graph based fuzz testing technique is proposed for dl inference engines where dl models are defined as digraphs from a natural and effective basis.
a novel operator level coverage criterion is introduced to enhance fuzz testing via a reward guided metric which can estimate the amount of dl inference engine s logic explored.
some graph based model level mutations graph edges addition graph edges removal block nodes addition block nodes removal and source level mutations tensor shape mutation parameter mutation are proposed to generate diversified dl models.
an experimental evaluation on an industrial inference engine is conducted.
the results show that the operatorlevel coverage guided testing framework improves the effectiveness in detecting exceptions.
more details of graph based fuzz testing and the experiments can be found at graph based fuzz testing.
ii.
b ackground a. workflow of dl inference engines dl inference engines are invoked by dl applications to load and run models on devices with inference hardware accelerators.
the workflows of existing inference engines are similar.
take mnn a lightweight dl inference engine developed by alibaba inc as an example as shown in fig.
the inference workflow can be roughly divided into two phases conversion phase of converting those training framework models e.g.
tensorflow lite caffe and onnx into mnn models and optimizing dl models by operator fusion operator substitution and layout adjustment.
furthermore mnn models can be quantized optionally.
inference phase ofloading mnn model and inferring.
the interpreter of mnn consists of engine and backends.
the former is responsible for loading a mnn model and scheduling a computational graph the latter includes the memory allocation and the operator implementation under each computing device.
test set dl app dl modelsconversionformat conversion model optimizationmnn inferencemnn inference runtimemnn enabled device fig.
.
inference workflow with mnn b. limitations of existing testing techniques fuzz testing has been proved to be effective in exploring the input space of dl system testing.
an important part of fuzz testing is how to feedback.
deepxplore introduced the concept of neuron coverage for measuring the parts of a dl system s logic exercised by a set of test inputs based on the number of neurons activated i.e.
the output values are higher than a threshold by the inputs.
enlightened by deepxplore a number of different coverage criteria have been applied to dl testing such as tensorfuzz deeptest and deepgauge .
these testing techniques focus on testing the quality of specific dl models.
however in dl inference engine testing when the input model is changed the coverage will be invalid as feedback.
another important part of fuzz testing is to generate test inputs via mutating a given set of inputs.
deepmutation proposed a set of mutation operators including changing weights biases and inputs via disturbing exchanging neurons within a layer adding or removing a layer of which input and output dimensions are equal like batch normalization etc.
this method mutates a single model to simulate error scenarios.
tensorfuzz is another fuzz test generation tool based on input data mutation for a dl model.
these methods can also effectively guarantee the quality of specific dl models.
but they cannot generate a large number of diversified models for testing inference engines that is despite a large amount of input generated the given model has not changed or changed very little.
therefore they are unlikely to detect erroneous behaviors and trigger different parts of a dl inference engine s logic.
in general existing testing techniques focus on the quality of dl models but lack attention to testing dl inference engines.
diversified combinations of operators models are more capable of triggering dl inference engine issues than an specific combination of operators a specific single model .
the issues triggered by a single model are limited.
we need to generate a large number of models by combining operators as the test inputs of dl inference engines .
289iii.
m ethodology in this section we provide detailed technical descriptions of graph based fuzz testing for dl inference engines.
a. definitions the stochastic networks generation involves the following definitions digraph subgraph block block corpus mutation action .
digraph .
graph theory provides an excellent framework for studying and interpreting neural network topologies .
a neural network can be denoted by a digraph g v e .
v is a set of operators e.g.
conv2d relu and softmax .
e braceleftbig x y x y v2 logicalandtextx negationslash y bracerightbig is a set of directed edges which are ordered pairs of distinct operators e.g.
xandy .
in neural networks edges are data flows.
subgraph .
from the introduction above some specified structures of dl models will be specially processed e.g.
operator fusion and replacement to run a faster inference .
there is a very low probability that these specified structures could be generated randomly.
thus subgraphs are applied to blocks to define those specified structures directly in testing.
formally digraph g prime v prime e prime is a subgraph of giffv prime v e prime e logicalandtext x y e prime x y v prime .xandy are two distinct operators.
block .
subgraphs or operators of a neural network are defined as blocks in this paper.
a network is constructed by operators and subgraphs as shown in fig.
.
fig.
.
a network is constructed by operators and subgraphs.
block corpus .
a block corpus contains blocks to be chosen and their attributes including block name allowed range of in degree and out degree inner edges of the block.
inner edges are required when the block is a subgraph and can be empty otherwise.
to construct the block corpus the tester need to first confirm the types of operators and subgraphs to be tested and then fill in their above attributes into the block corpus.
mutation action .
let i1 i2 ... inbe a sequence of mutated test sets where ikis generated by k th mutation action ma bsk m s k .bskandmskare the blocks selection and mutation operators selection in k th mutation action respectively.
a tuple of the two actions forms a complete action ma bs ms .
b. operator level coverage criterion traditional code coverage criteria are ineffective in dl testing.
as discussed in section ii.b recently proposed neuroncoverage criteria are still invalid as dl inference engine testing involves different models.
a novel operator level criterion is proposed to capture differences in models combined by operators which provide feedback to guide the proposed graph based fuzz testing to generate diversified models.
as defined in iii a we use the model structures input tensor shapes parameters to characterize behaviors of operators in dl models.
as defined in iii a we use the input degree the number of input data flows the output data flows the number of output data flows input tensor shapes nhwc etc.
and parameters e.g.
dilation of conv2d of operators to characterize behaviors of operators in dl models.
given a block corpus bc and a test set i operator level coverage criterion is defined as follows operator type coverage otc .
let ntbe the number of total types of operators defined in bc .
let otc op c i be when the operator cis in the i and be otherwise.
the otc of iis defined as otc i summationtextotc op c i nt input degree coverage idc .
let nidcbe the total number of different input degrees of operator cinbc .
let fidop i be the number of different input degrees for operator cini.
the input degree coverage of operator cis defined as the ratio of fidop i tonidop idc op c i fidop i nidc.
idc i summationtextidc op c i nt output degree coverage odc .
let nodcbe the total number of different output degrees of operator cinbc .
let fodop i be the number of different output degrees of operator cini.
the output degree coverage of operator cis defined as the ratio of fodop i tonodc odc op c i fodop i nodc.
the odc of iis defined as odc i summationtextodc op c i nt single edge coverage sec .
let fse c i be the number of different edges that directed from the operator cto others in i. the number of total edges that directed from the operator cto others in bc isnt.
the single edge coverage of operator cis defined as the ratio of fse c i tont sec op c i fse c i nt.
the sec of iis defined as sec i summationtextsec op c i nt shapes parameters coverage spc .
let nmaxspc be the expected maximum of shape parameter.
let fspc c i be the number of distinct vectors including tensor shapes and parameters for operator cini.
the spc of operator c in iis defined as spc op c i fspc c i nmaxspc.
the spc of iis defined as spc i summationtextspc op c i nt operator level coverage olc .
let olc of the 290operator cbe the weighted mean of a set of metrics zop otc op c i id c op c i o d c op c i sec op c i spc op i with corresponding non negative weights wop1 wop2 ... w op5 .
olc of operator cis defined as olc op c i summationtextwopimopi summationtextwopi mopi zop let olc of the test set ibe the weighted mean of a set of metrics z otc i id c i o d c i sec i spc i with corresponding non negative weights w1 w2 ... w .
formally olc of test set iis defined as olc i summationtextwimi summationtextwi mi z some weights may be zero.
for example the weight of odc i should be when expected output degree of operators are all in test samples of test set i. for example fig.
shows three nns in test set igenerated by block corpus bc in table i. tensor format is nhwc.
three blocks conv2d relu and add and their input and output degree are defined.
in operator level coverage the nmaxspc of formula is set to .
operator level coverage result for each operate and test set iare calculated and listed in table ii respectively.
table i block corpus of test seti block name input degree output degree inner edges conv2d n a relu n a add n a a nn1 b nn2 c nn3 fig.
.
three nns in test set i table ii opera tor level coverage of each opera tor and test set i object otc idc odc sec spc olc conv2d .
.
relu .
.
.
add .
.
i .
.
.
c. framework the core idea of graph based fuzz testing is to maximize operator level coverage on a dl inference engine such that as many erroneous behaviors as possible can be exposed.
a large number of test samples i.e.
models can be constructed by mutating existing dl models modeled as graphs and fig.
.
framework of graph based testing operator level coverage is used as feedback to guide test input generation.
framework .
as depicted in fig.
the graph based fuzz testing framework is composed of block chooser coverage criterion input mutator and mutation selector.
for each iteration the mcts based block chooser chooses a set of blocks bfrom block corpus.
the mutation selector chooses one or more mutations scholastically to determine mutating rules m. parameters of the mutations are assigned randomly under their constraints.
after that the input mutator determines which actions in mwill be applied to bwhere mutating actions b m are formed and test samples can be generated.
the test samples will be run in dl framework e.g.
tensorflow whose output data is saved as expected results.
the input data contains models and their expected results.
the coverage criterion takes the mutated samples to check whether current coverage increases.
if current coverage increases the new input data will be added to test set otherwise such data will be discarded.
this process runs until reaching a preset threshold the target number of test samples.
algorithm .
the algorithm of our graph based fuzz testing is shown in algorithm .
in procedure of fuzzworkflow inputs are block corpus bc mutations m a termination condition tc0 that is a target number of new inputs.
the while loop in line iterates until tc0is reached.
in line blocks are chosen by the block chooser.
in line the mutation selector selects mutations and their parameters.
in line input mutation generates test set ikby the blocks and mutations.
in line the operator level coverage of ik is calculated.
in line coverage kis checked whether it produces additional coverage.
in line mcts simulation is made.
in line current test set results and current operator level coverage are updated.
in line mcts back propagation updates back the result of the inference to update values associated with the nodes on the path from child node cto root node r. in procedure of inputmutation inputs are selected blocks bk selected model level mutations model mkand sourcelevel mutations source mk.
in line graphs are generated 291algorithm algorithmic description of our fuzz testing framework procedure fuzz workflow bc m tc0 while not tc 0do bk c blockchooser r tc1 tc2 coverage source mk model mk mutationselector bk m ik inputmutation bk source mk model mk coverage k coveragecriterion ik ifisnewcoverage coverage k then result k mctssimulation ik update result coverage i mctsbackpropagation c r result coverage end if end while return result coverage i end procedure procedure input muta tion bk source mk model mk g generategraph bk model mk s p calcshapesparameter g source mk ik generatemodel g s p return ik end procedure procedure block chooser r tc1 tc2 coverage l mctsselection r tc1 c mctsexpansion l coverage tc2 bk getnodesfrompath c return bk c end procedure from the selected blocks bkand model mutations.
in line input shapes and parameters of each block are generated from the graphs and data mutations.
in line according to the graphs and parameters the test set ikis generated.
in the procedure of blockchooser inputs are the root node r no operator is set for the root node of mcts tree termination condition tc1is the maximum levels of the search tree the mcts can go down termination condition tc2is the maximum times a node can be explored.
in line choose blocks for inputmutation.
in line mcts selection is made.
a leaf node lis returned.
in line mcts expansion is applied to create a new child node cof the leaf node l. the child node ccould be the lowest coverage operator or a subgraph containing it and is not chosen in the path before.
in line the index of cand blocks along the path from child node cto root node rare returned.
d. block corpus the fuzz testing process maintains a block corpus containing blocks and their attributes including block name allowed range of in degree and out degree inner edges of the block.
when a block is a subgraph its block name is defined as the sequence of operators in the subgraph i.e.
block conv2d relu pow concat in fig.
a and its inner edges are required.
each element in the adjacency list of inner edges is a pair of source and destination operator index.
taking an operator conv2d and two subgraphs shown in fig.
for example conv2d has exactly one input and the two subgraphs have two respectively.
allowed range of out degree of the two are set by test framework such as .
inner edges of thetwo subgraphs are and respectively.
a conv2d relu pow concat b conv2d conv2d add add fig.
.
block structures of conv2d relu pow concat and conv2d conv2d add add e. block chooser the block chooser is designed for dl models generation to improve operator level coverage and suppress duplicated exceptions.
in tests that randomly select operators to build dl models a large number of duplicate exceptions are found.
in block chooser monte carlo tree search mcts is used to search the input domain of dl inference engines such that the most promising blocks can be chosen in block chooser to generate stochastic dl models.
each node of the tree represents an operator in the block corpus.
mcts dynamically adapts itself to the most promising search regions where good combinations are likely to find more exceptions.
mcts process shown in fig.
can be divided into the following four steps.
selection starting from the root node r successively select child nodes according to their potentials until a leaf node l is reached.
the potential of each child node is calculated by using uct upper confidence bound applied to trees .
uct is defined as potential v n e radicalbigg lnn n where vrefers to the success count of the node nis the visit count of the node and nis the visit count for the parent of the node.
eis a hyper parameter determining explorationexploitation trade off.
the maximum levels of the search tree that the mcts can go down is set as terminal condition tc1 .
expansion unless lis a terminal node with the highest potential create one child node operator cwith the lowest coverage and the operator cis not in the path.
we pick a operators or a subgraph that contains the operator c. simulation generate stochastic dl models using the blocks in the current path of tree until reaching a terminal condition and then inference the models.
the maximum times a mcts node can be explored is set as terminal condition tc2 .
back propagation propagates back the result of the inference to update values associated with the nodes on the path from ctor.
the path containing the nodes with the highest 292values in each layer would be the optimal strategy in the test set.
a selection b expansion c simulation d backpropagation fig.
.
scheme of a monte carlo tree search f .
mutatation selector mutations .
the stochastic graphs generated by graph models above only cover a small set of n node graphs.
mutations can extend the graphs for high coverage.
the framework applies these mutations by selecting different predefined mutation parameters including model level mutations and source code level mutations.
model level mutations are applied to the initial digraph and blocks.
let e g be the node count of graph g. let r r be the probability of model level mutations.
graph edges addition gea .
add e g r edge to graph g. graph edges removal ger .
delete e g r edge from graph g. block nodes addition bna .
duplicate an operator to every subgraph of graph gwith probability r. block nodes removal bnr .
remove an operator and its edges from every subgraph of graph gwith probability r. two source level mutations mutate input shape of the network and operator parameters after blocks selected for nodes in digraph.
tensor shape mutation tsm .
mutate the shape of the input tensor.
parameters mutation pm .
v ariation of input parameter.
selecting a random enumeration for a discrete type and a random value within range for continuous type.
g. input mutator to generate a dl model the following steps are applied.
first network generator generates a digraph with a specific graph model and update connections of the graph by the model mutation methods.
for each node in the graph a block with the same input degree is selected from the block corpus.
second shapes parameters calculator calculates input shapeand parameters for each input.
finally generate models and test samples for running.
network generator .
two random graph models in graph theory are applied.
one is watts strogatz ws model proposed by watts et.al.
and the other is residual network rn model we proposed in this paper.
the rn model generates residual blocks in models.
let nbe the node count.
letk k be the maximum neighbors.
initially add the n nodes i ... n 1sequentially in a line.
let kcurrent i kcurrent i k be the neighbor count of node i. for every node whose current neighbor count is less than k add edges connecting a node ito another node j i j and kcurrent j k with probability p p and repeat this step k kcurrent itimes for node i.k pandnare the parameters of the rn model denoted as rn k p n .
shapes parameters calculator .
shapes parameters calculator involves satisfying the demands of structuring dl neural network.
those shape free parameters are randomly selected from their range.
two methods are used to focus on the effectiveness of dl models with various input shapes.
aggregation including add concat etc.
we use operators such as cast shape slice and pad to convert the mismatched input shape or data type into expected form before these aggregations.
pad is merged into adjacent operators such as pooling conv2d depthwiseconv2d pad etc.
operators with padding including pooling conv2d depthwiseconv2d etc.
the padding of these operators are calculated to keep the shapes of input and output consistent.
taking conv2d with same padding mode for example given input shape and other parameters the output height oh is computed as where ph is padding height fh is filter height sh is stride height and dh is dilation height.
oh ih ph dh fh sh regarding the shapes of layer input and output are consistent the following can be obtained oh ih then other parameters are associated satisfying three conditions of conv2d as below where max sh is maximum of stride height max dh is maximum of dilation height and fh is maximum of ph.
lessorequalslantph lessorequalslantfh lessorequalslantsh lessorequalslantmax sh lessorequalslantdh lessorequalslantmax dh similarly parameters of filters can be computed in the same way.
thus with given input shape parameters of input vector in ic ih iw fn fc fh fw pad stride dilation that satisfy can be generated randomly with less pads.
293iv .
e xperiment setup a. research questions in this paper we will answer the following six research questions.
rq1 how effective is the approach in detecting exceptions of dl inference engine?
rq2 how does the mcts based search algorithm perform comparing with random search for decision processes?
rq3 how effective is the rn model in increasing operatorlevel coverage and detecting exceptions?
rq4 how effective is the mutation strategy in increasing operator level coverage criterion and detecting exceptions?
rq5 how effective is the subgraph of the approach?
rq6 are these exceptions found related to the operatorlevel coverage?
b. experiment setup we set up our experiments as follows.
block corpus .
block corpus in this experiment consists of blocks including operators and subgraphs.
tensorflow operators supported by mnn in reference guide add argmax avgpooling batchtospacend biasadd cast ceil concat conv2d cropandresize deconv2d depthwise exp expanddims fill fusedbatchnorm gatherv2 greater lrn maximum maxpooling minimum mul realdiv reducemax reducemean reduceprod reducesum relu relu6 reshape resizebilinear resizenearestneighbor rsqrt selu shape sigmoid resize slice softmax spacetobatchnd sqrt square squeeze stridedslice sub tanh tile topkv2 transpose.
subgraphs.
subgraph fig.
a refers to the concatenation of multiple feature maps in ssd .
subgraph fig.
b is inspired by operator fusion and arithmetic optimizer in tensorflow graph optimization.
subgraph fig.
c is inspired by operator fusion in mnn.
we set the range of output degree as .
relu conv2d conv2d3 conv2d conv2d conv2d4 conv2d conv2d concat a subgraph .
conv2d tanh mul mul4 mul concat b subgraph .
conv2d biasadd biasadd biasadd4 sub add c subgraph .
fig.
.
three subgraphs in block corpus inference runtime .
our experiments are conducted with mnn v1.
.
on x86 cpu .10ghz and arm cpu honor x10 .
both inference runtimes are calculated using fp32.
test inputs and their inference results are generated by tensorflow v1.
cpu mode .
inputs filters and biases are generated from the uniform distribution .
the final status of the test process includes model conversion failure mcf inference failure if data comparison failure dcf and data comparison pass dcp .
the comparison threshold betweenmnn and tensorflow is that the ratio of data with a relative error greater than .
to the total data is less than .
.
formally let re mnn be the ratio of the numbers with the relative error between mnn and tensorflow over the total output data of an operator.
when re mnn .
the result is considered as a success.
some failures are caused by the same defect.
in order to eliminate duplication model conversion failures with the same error type error code and failure operator are regarded as the same error and data comparison failures with the same structure and operator are regarded as duplicates.
configuration .
in order to generate diversified models the parameters are set in a wide range.
the probability of four models level mutations is chosen from .
.
.
in graph algorithm the number of neighbors kis chosen from .
the probability of rewriting connections pof ws is .
.
the probability pof rn is .
.
for operator level coverage the nmaxspc of formula is set to .
the weights of formula and are set to .
in mcts based block chooser tc1is set to tc2is set to and eis set to .
each terminal node can expand up to child nodes.
for rq2 rq3 and rq4 test inputs are generated for each strategy and they are inferred on x86 cpu only because mnn cannot infer models with multiple inputs or multiple outputs on arm cpu.
the block number of models for each strategy is set to and respectively.
for rq1 and rq5 the generated models are inferred on x86 cpu and the single input and single output models of these models are also inferred on the arm cpu.
v. e v alua tion a. rq1 how effective is the approach in detecting exceptions of dl inference engine?
in order to answer rq1 we generated approximately models and found more than different exceptions.
the number of blocks for models is chosen uniformly from to .
some typical exceptions are as below.
models conversion failure mcf .
mcf segmentation fault in topkv2 conversion.
mnn model cannot be generated.
mnn log error for .
segmentation fault core dumped .
almost all models that include topkv2 operators cannot be converted and inferred successfully.
mcf conversion aborted.
the deconv api of the pb model shown in fig.
is tf.nn.conv2d transpose.
mnn log converter source common writefb.cpp check failed notsupportops.size .
not support tensorflow conv2dbackpropinput output shape is not consistent with inferred output shape in mnn.
height width vs .
convert tensorflow s op deconv2d outputdata type conv2dbackpropinput failed.
aborted core dumped .
it reveals that the shape of the output tensor calculated by the operator deconv in mnn is wrong.
inference failure if .if inference aborted .
the reduceprod cannot output results shown in fig.
294fig.
.
mcf conversion aborted pb model structure and deconv s parameters.
.
mnn log error in python free invalid next size fast 0x000000000 1e2cb90.
backtrace ... distpackages mnncengine.so zn3mnn6tensord1ev 0x74 .
aborted core dumped .
fig.
.
if inference aborted pb model structure and reduceprod s parameters.
inference aborted the reduceprod cannot output result.
data comparison failure dcf dcf core dumped and data comparison failure in sub .
the model structure is shown in fig.
.
re mnn x86cpu of sub1 is .
re mnn x86cpu of sub2 is .
.
mnn log error in python double free or corruption !prev 0x0000000002 4ae8b0.
backtrace dist packages mnncengine.so zn3mnn15bufferallocator4noded1ev 0x88 0x7ff0e81 7a098 .
aborted core dumped .
dcf data comparison failure of operators .
when converting some fp32 numbers from to into int8 numbers the cast operator will yield a calculation error.
when the input of a sigmoid operator is nan coming from rsqrt the results are nan in tensorflow and in mnn respectively.
in fig.
a re mnn armcp u of avgpooling is .
.
in fig.
b re mnn armcp u of maxpooling is .
.
in fig.
c re mnn armcp u of relu is .
.
the conv2d of the model fig.
c caused an incorrect calculation result.
the result of realdiv fig.
d divided by zero is inf in tensorflow and nan in mnn fig.
.
dcf pb model structure and gather s parameters.
data comparison failure of sub.
a re mnn armcp u of avgpooling is .
.
b re mnn armcp u of maxpooling is .
.
c re mnn armcp u of relu is .
.
d re mnn armcp u of realdiv is .
.
fig.
.
dcf data comparison failure of single operators x86 cpu.
model generation failure .
it is worthy of mentioning that some exceptions of tensorflow are found in model generation.
when generating a model containing two concats whose two inputs come from the same two constants tensorflow will get stuck.
answer to rq1 our approach is effective to detect various exceptions in model conversion and inference process for the dl inference engine such as crashes inconsistencies nan and inf bugs.
b. rq2 how does the mcts based search algorithm perform comparing with random search for decision processes?
in order to answer rq2 we evaluate operator level coverage and inference results using mcts based search and random 295search for block chooser.
operator level coverage .
as shown in table iii the operator level coverage of random search and mcts based search are .
and .
for blocks .
and .
for blocks .
and .
for blocks respectively.
mcts based search on average covers .
more operatorlevel coverage than random search.
inference results .
we measure the exceptions of inference results for each search algorithm.
table iii shows the exceptions found.
mcts based search on average finds .
more exceptions than random search.
it can be observed that the average unduplicated rate of random search is .
which is lower than that of mcts search .
.
using the mcts based search takes an average of .
hours longer than using the random search.
because the efficiency of mctsbased search that find exceptions is significant a small increase in execution time is acceptable for industrial testing.
answer to rq2 the mcts based block chooser outperforms the random based block chooser in boosting operatorlevel coverage .
more and detecting exceptions .
more .
c. rq3 how effective is the rn model in increasing operator level coverage and detecting exceptions?
in order to answer rq3 operator level coverage and inference results are evaluated using two stochastic network generation strategies ws and rn model together each model generating half of the test samples.
we do not aim to verify that whether a graph model is superior to other models in certain configurations.
the rn model is only used to generate more diversified models.
ws model only.
rn model only.
five input shapes can be chosen uniformly for each model.
to avoid disturbing the coverage and inference result of random graph models mutations are cancelled.
operator level coverage .
as shown in table iii there are two key observations from the results.
first ws and rn model together on average covers .
and .
more operator level coverage than ws model only and rn model only respectively as demonstrated.
second the coverage rate when n is is slightly higher than when n is .
this is intuitive as a higher value n of blocks making it increasingly harder to cover more topologies and types of shapes parameters without mutations.
inference results .
the exceptions of inference results are measured for each strategy.
table iii shows the detailed results.
ws and rn model together on average finds .
and .
more exceptions than ws model only and rn model only respectively as demonstrated.
therefore ws and rn model together is more efficient in finding exceptions as well as increasing blocks of dl models.
the experiment also shown that the performance of rn is slightly better than that of ws.
answer to rq3 different graph models can be used to generate more diverse models.
through applying the rn model to stochastic network generation strategy more excep tions for each strategy can be detected as well as operator level coverage increased.
d. rq4 how effective is the mutation strategy in increasing operator level coverage criterion and detecting exceptions?
in order to answer rq4 we evaluate operator level coverage and inference results of test generation with mutations and test generation without mutations.
for test generation without mutations input shapes can be chosen uniformly for each model without any mutations.
operator level coverage .
the operator level coverage of two strategies are .
and .
for blocks .
and .
for blocks .
and .
for blocks respectively.
for test generation with mutations on average covers .
more operator level coverage than test generation without mutations as demonstrated in table iii .
inference results .
the exceptions of inference results are measured for each strategy.
table iii shows the detailed results.
the mean number of exceptions are .
and .
for blocks .
and .
for blocks .
and .
for blocks respectively.
it can be observed that test generation with mutations is more efficient in finding exceptions on average .
more exceptions as well as increasing operatorlevel coverage of inputs.
answer to rq4 our mutation strategy is useful to generate new valid test inputs by up to .
more operator level coverage and .
more exceptions detected on average.
it can be observed that our mutation strategy is effective for generating diversified models.
e. rq5 how effective is the subgraph of the approach?
in order to answer rq5 mutated subgraphs ms are generated to evaluate the effectiveness of three subgraphs in block corpus shown in fig.
.
the number of blocks in a model is chosen from .
the success ratios are .
on arm cpu and on x86 cpu.
the main type of error is data comparison failure on arm cpu.
inference exceptions are analyzed as below.
fig.
shows two typical exceptions of mutated subgraphs.
ms .
data comparison failure.
this mutant deletes a mul from subgraph fig.
a and re mnn armcp u of relu is .
.
ms .
data comparison failure.
this mutant a biasadd from subgraph and delete a conv2d from subgraph fig.
b and re mnn armcp u of concat is .
.
when given more than inputs it reveals that the operator concat will lose some of the input data and outputs wrong results.
answer to rq5 it is difficult to generate a model of a specific structure only by matching operators with nodes in a random graph.
subgraphs and their mutants can be helpful to construct these specific structures with more exceptions detected.
f .
rq6 are these exceptions found related to the operatorlevel coverage?
in order to answer rq6 relations between the typical exceptions and operator level coverage are analyzed.
296table iii opera tor level coverage exceptions found after deduplica tion total and dura tion under different block number n of models for rq2 rq3 and rq4.
n n n graph model mutations search olcexceptions found d t duration h olcexceptions found d t duration h olcexceptions found d t duration h ws and rn yes random .
.
.
.
.
.
.
.
ws no mcts based .
.
.
.
.
.
.
.
.
.
.
.
rn no mcts based .
.
.
.
.
.
.
.
.
.
.
.
ws and rn no mcts based .
.
.
.
.
.
.
.
.
.
.
.
ws and rn yes mcts based .
.
.
.
.
.
.
.
.
.
.
.
a delete a mul from subgraph .
b delete a biasadd from subgraph and delete a conv2d from subgraph .
fig.
.
inference results of mutated subgraphs in block corpus.
operator type coverage .
we note that some operators are supported in mnn reference guide but the error reported is that this operator is not supported in model conversation such as deconv in mcf .
almost all models that include topkv2 operators cannot be converted and inferred successfully.
our result mcf confirm that the topkv2 operator is almost unsupported in current mnn version.
we also tried to infer some models that contain operators unsupported in mnn reference guide such as addn clip and asin.
these models are successfully inferred by mnn on x86 cpu and arm cpu.
single edge coverage .
note that sec is usually associated with input degree coverage oroutput degree coverage such as multi output e.g.
add of if and multi input operators e.g.
realdiv of dcf and concat of ms .
some structures also cause special inputs for operators such as nan of rsqrt and sigmoid in dcf .
shapes parameters coverage .
some results of operators with specific value of parameters or tensor shapes are unexpected in comparison with tensorflow such as avgpooling and maxpooling in dcf .
answer to rq6 in summary the exceptions detected are all within the scope of operator level coverage.
in addition there is no obvious correlation between each metrics of operator level coverage and a certain error types.
that is guided by these metrics the inputs can trigger various types of exceptions.vi.
t hrea ts tovalidity the internal threat to validity mainly lies in the implementations of our method and scripts in the experiment.
all the artifacts are maintained by experienced software engineers in huawei.
besides the method has been used in huawei company which reduces the internal threats.
the external threats to validity mainly lie in the library and dl inference engine used in the experiment.
to reduce this threat we adopted one of the most widely used library tensorflow to generate input dl models.
then we chose the alibaba mnn as the target dl inference engine under test x86 cpu and arm cpu .
furthermore the proposed method tests a dl inference engine of huawei for several months with many valid exceptions detected.
the construct threats to validity mainly lie in settings randomness and measurements in our experiment.
to reduce the impact of settings we constructed a block corpus operators and subgraphs and five experiments.
to reduce the impact of randomness a large number of models were generated in every experiment respectively i.e.
for rq1 for rq2 rq4 .
we repeated each method for times and calculated the average results in rq2 rq4.
to reduce the impact of measurements we carefully set a threshold re mnn to check whether the inference results are correct.
when counting the number of exceptions we identified the duplicated ones.
further we manually generated dl models and triggered all the detected exceptions successfully.
vii.
r ela ted work fuzz testing and mutation testing .
fuzz testing is a widely used technique for exposing defects in dl system.
guo et al.
proposed the first differential fuzz testing framework for dl systems.
tensorfuzz proposed by odena et al.
used a nearest neighbour hill climbing approach to explore achievable coverage over valid input space for tensorflow graphs and to discover numerical errors disagreements between dl models and their quantized versions.
pei et al.
presented deepxplore which proposed a white box differential testing technique to generate test inputs for dl system.
wicker et al.
proposed feature guided test generation.
they transformed the problem of finding adversarial examples into a two player turn based stochastic game.
ma et al.
proposed deepmutation which mutates dnns at the source level or model level to make minor perturbation on the decision boundary of a dnn.
shen et al.
proposed 297five mutation operators for dnns and evaluated properties of mutation .
xie et al.
presented a metamorphic transformation based coverage guided fuzzing technique deephunter which leverages both neuron coverage and coverage criteria presented by deepgauge .
existing testing techniques focus on the quality of dl models but lacks attention to the core underlying inference engines i.e.
frameworks and libraries .
our method generates models as the input of the fuzz testing for dl inference engine.
together with the combinations of operators we design new mutation rules to generate diversified dl models to trigger different structured parts of a given dl inference engine.
test coverage criteria .
coverage criteria is an important part of testing methods.
code coverage is most popular for conventional software testing but it does not make sense for dl testing since the decision logic of a dl model is not written manually but rather it is learned from training data .
in the study traditional code coverage is easily achieved by a single randomly chosen input.
pei et al.
proposed the first coverage criterion neuron coverage.
neuron coverage is calculated as the ratio of the number of unique neurons activated by all test inputs and the total number of neurons.
ma et al.
proposed layerlevel coverage which considers the top hyperactive neurons and their combinations to characterise the behaviours of a dnn.
du et al.
first proposed state level coverage to capture the dynamic state transition behaviours of deep neural network.
li et al.
pointed out the limitations of structural coverage criteria for deep networks caused by the fundamental differences between neural networks and humanwritten programs.
deepcover proposes the test criterion for dnns adapted from the mc dc test criterion of traditional software.
existing neural coverage criteria of dl models cannot work in dl inference engine testing scenario because the inputs for testing dl inference engines are dl models.
thus a novel criterion is required to capture behaviors of dl inference engines rather than those of dl models.
our proposed operator level coverage is naturally followed by the graph structure of dl models.
the results show that the operator level coverage guided testing framework improves the effectiveness in detecting exceptions.
viii.
c onclusion the issues triggered by a single specific model are limited and diversified combinations of operators models are more capable of triggering dl inference engine issues.
to overcome the challenge of generating such models with diversified combinations of operators this paper employs graph based fuzz testing incorporating six different mutations mcts and a novel operator level coverage criterion proposed based on graph theory.
one possible application scenario is implemented on mnn and the results demonstrate the effectiveness of our proposed method.
in fact the proposed method has been used continuously in a dl inference engine of huawei for several months which finds many valid exceptions during the internal build and is efficient for industry.acknowledgement we would like to thank anonymous reviewers for insightful comments we also thank jiawei liu for discussions on the manuscript.
this work is supported partially by national natural science foundation of china and fundamental research funds for the central universities .