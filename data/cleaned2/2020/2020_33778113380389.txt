software visualization and deep transfer learning for effective software defect prediction jinyin chen1 keke hu1 yue yu2 zhuangzhi chen1 qi xuan3 yi liu4 vladimir filkov5 chenjinyin zjut.edu.cn zjut.edu.cn yuyue nudt.edu.cn zjut.edu.cn xuanqi zjut.edu.cn yliuzju zjut.edu.cn filkov cs.ucdavis.edu 1college of information engineering zhejiang university of technology hangzhou china 2college of computer national university of defense technology hefei china 3institute of cyberspace security zhejiang university of technology hangzhou china 4institute of process equipment and control engineering zhejiang university of technology hangzhou china 5department of computer science university of california davis ca usa abstract software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort.
typically software defect prediction pipelines are comprised of two parts the first extracts program features like abstract syntax trees by using external tools and the second applies machine learningbased classification models to those features in order to predict defective modules.
since such approaches depend on specific feature extraction tools machine learning classifiers have to be customtailored to effectively build most accurate models.
to bridge the gap between deep learning and defect prediction we propose an end to end framework which can directly get prediction results for programs without utilizing feature extraction tools.
to that end we first visualize programs as images apply the self attention mechanism to extract image features use transfer learning to reduce the difference in sample distributions between projects and finally feed the image files into a pre trained deep learning model for defect prediction.
experiments with open source projects from the promise dataset show that our method can improve cross project and within project defect prediction.
our code and data pointers are available at keywords cross project defect prediction within project defect prediction deep transfer learning self attention software visualization acm reference format jinyin chen1 keke hu1 yue yu2 zhuangzhi chen1 qi xuan3 yi liu4 vladimir filkov5.
.
software visualization and deep transfer learning for effective software defect prediction.
in 42th international conference on software engineering may seoul south korea.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee pro vided that copies are not made or distrib uted for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
t o copy otherwise or republish to post on servers or to redistrib ute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
corresponding author yi liu icse may seoul republic of korea association for computing machinery.
acm isbn ... .
introduction software defect prediction techniques can help software developers locate defective code modules automatically to save human effort and material resources.
most prediction methods build prediction models based on modules in the source code and historical development data at different levels of modeling e.g.
commit changes methods and files .
in practice based on whether the historical training data comes from the same project or not we distinguish between within project defect prediction wpdp and cross project defect prediction cpdp .
a large number of manually designed features including static code features and process features have been adopted to predict whether a module is defective or not.
to improve on those current software defect prediction studies mainly focus on two main directions new feature extraction methods and classification methods learned from large scale datasets.
however defect prediction techniques which feed manually designed features into machine learning algorithms for classification have some limitations .
the required feature engineering is time consuming and requires that special tools be used upstream such as code complexity analysis submission log mining and code structure analysis tools.
consequently many features can be difficult to capture in some projects.
for example semantic code information such as the features hidden in abstract syntax trees asts may not be effectively represented by existing traditional features.
in addition to the inconvenience of feature engineering for traditional features as described by wan et al.
in a recent review of defect prediction semantic information can be more capable than syntax information to distinguish one code region from another.
thus while ast conveyed features can be useful for defect prediction such approaches are indirect requiring additional tools in order to build and mine the asts.
moreover in such approaches the source code is most frequently not used once the asts are extracted.
a number of machine learning methods e.g.
support vector machines svms naive bayes nb decision trees dts and neural networks nns have been applied to defective module prediction.
in particular recent research has been conclusive that deep learning networks are highly effective in image classification feature extraction and knowledge representation in many areas .
in defect prediction specifically to better generate semantic features a state of the art method leveraged deep belief network dbn for learning features from token vectors extracted from programs asts.
on this basis li et al.
and dam ieee acm 42nd international conference on software engineering icse icse may seoul south korea chen et al.
figure a motivating example et al.
used the structural information of programs and the semantic relations between keywords to improve defect prediction with convolutional neural network cnn and long short term memory networks lstms .
in those papers the required feature engineering was significant and required specific tools be used upstream.
in this work we are motivated by the possibility of improving defect prediction by avoiding intermediary representations e.g.
asts and instead obtaining code semantic information directly.
to that end inspired by the power of existing deep learning platforms for image classification in this paper we propose a more direct way to use programs semantic information to predict defects byrepresentingsource code asimages andtrainingimageclassificationmodels onthose images.
figure shows a motivating example.
the two java files on top file1.java andfile2.java are similar both containing ifstatement forstatements and function calls.
nevertheless the files semantic and structural features are different.
we wondered if a visualization can help to tell that those two programs are different.
to do so we turned code characters into pixels their colors based on the characters ascii decimal value and then arranged those pixels in a matrix thus obtaining code images.
by comparing those images we were able to visually recognize significant differences between the corresponding programs as shown in the bottom of figure .
both semantic and structural differences can be recognized visually in those images.
that leads us to our driving thesis semantic and structural similarities between two programs can be effectively identified by visually comparing program images.
to test that thesis we start from a well known data set from the promise repository of projects with known defective files.
once the files are converted to images we automatically extract features and build a classification model with the popular alexnet platform .
in addition we use deep transfer learning and selfattention mechanism the details are in figure and section .
to further improve the defect prediction especially cpdp.
finally we propose an end to end framework i.e.
deep transfer learning 1there were a number of technical details and choices we had to make when converting programs into images we discuss those in the methods section.for defect prediction dtl dp to automatically predict whether a program file contains defects or not.
this paper makes the following contributions we propose a novel approach for defect prediction based on visualizing program files as images in order to capture their semantic and structural information.
we propose an end to end deep learning framework dtldp comprised of a deep transfer learning model combined with a self attention mechanism which takes program file images as input and outputs labels either defective or not defective .
our experiments on os java projects show that our approach improves both wpdp and cpdp.
the advantages of dtl dp in cpdp are much more obvious than in wpdp.
we show that self attention has the greatest contribution to wpdp and transfer learning contributes most to cpdp.
the remainder of this paper is organized as follows.
first we review related work in section .
then we describe dtl dp and present our experimental setup in sections and respectively.
after that we discuss the performance of dtl dp in section and the threats to validity in section .
finally we conclude the work and provide several potential future directions in section .
related work defect prediction dp as one of the primary areas of interest in software engineering research dp has been receiving significant attention .
a number of studies have focused on manually designing features or producing new combinations of existing features from labeled historical defects.
those features are typically fed into a machine learning based classifier to determine if a file is defective.
commonly used features can be divided into static e.g.
code size and code complexity e.g.
halstead features mccabe features ck features and process features like the behavioral differences of developers in the software development process.
many studies have demonstrated that process features can predict software quality .
moser et al.
used authors past fixes the number of revisions and ages of files as features to predict defects.
nagappan et al.
indicated that code churn was effective for dp.
hassan et al.
used entropy of changes to predict defects.
other process features are helpful too including individual developer characteristics and their collaborations .
based on these features many machine learning models including svm nb dt nn etc.
have been built for the two different dp tasks within project wpdp and cross project cpdp.
for wpdp the training set and test set come from the same project while for cpdp they come from different projects.
while wpdp can give better results it is of limited use in practice as it is often difficult to obtain enough training data for a new project.
some studies have instead used related projects to build prediction models with sufficient historical data and then used them to predict defects in the target project .
panichella et al.
proposed an approach named codep which uses a classification model to combine the results of classification algorithms including logistic regression radial basis function network and multi layer perceptrons for cpdp.
turhan et al.
and peters et al.
used different strategies to select appropriate instances in a source project based on 579software visualization and deep transfer learning for effective software defect prediction icse may seoul south korea nearest neighbor methods to train the prediction models.
additionally the application of transfer learning is receiving more attention.
xia et al.
proposed an approach named hydra to build classifiers using genetic algorithm and ensemble learning.
ma et al.
proposed a transfer naive bayes tnb method to assign weights to training instances and then used them to construct a prediction model.
nam et al.
proposed tca which uses tca and optimizes the normalization to improve cpdp.
recently some studies have used deep learning to directly get features from source code for dp.
wang et al.
deployed deep belief networks dbns to learn semantic features from token vectors extracted from asts automatically and leveraged the learned semantic features to build machine learning models for dp.
li et al.
used convolutional neural networks cnns to generate features from source code and combined cnn learned features with traditional features to further improve upon the prediction.
similarly dam et al.
proposed a prediction model that takes asts representing the source file as input.
it used an lstm architecture to capture long term dependencies which often exist between code elements.
our proposed approach differs from the above in that we do not use feature engineering followed by machine learning based classifiers.
we also do not need asts to bridge the gap between defect prediction and deep learning.
instead our approach is based on feeding software visualizations into an automatic image based feature discovery pipeline.
software visualization sv for dp sv has long history in software engineering research and practice .
it has been used for visualizing code structure and features code execution and evolution of large codebases in particular .
sv has also been applied to bug repositories where it has been helpful in correlating bugs with code structure .
in fact the way code was visualized in a recent study on malware code visualization has in part inspired us in this paper.
however to the best of our knowledge there is a dearth of applications of sv to software defect prediction in the research literature perhaps because prior to dl approaches no connection was seen between the two areas.
deep transfer learning dtl transfer learning tl is an important tool that can help when there is insufficient training data in machine learning.
it works by transferring information from a source domain to a target domain.
the key is to relax the assumption that the training and test data must be independent and identically distributed .
this is helpful in areas where it is difficult to get enough training data.
in the field of dp studies have shown that cpdp can achieve better performance with transfer learning .
most tl studies are based on traditional machine learning methods.
recently deep learning based transfer learning studies have emerged called deep transfer learning dtl.
based on the techniques used in them they can be divided into four categories networkbased instance based mapping based and adversarial based .
instance based dtl is implemented through a specific weight adjustment strategy.
the instances selected from the source domain are assigned weights that complement the training data in the target domain .
network based dtl refers to the use of a partially trained network in the source domain for the deep neural network of the target domain including its network structure and connectingparameters .
adversarial based dtl refers to introducing adversarial technology inspired by generative adversarial nets gan to find a transferable representation that is applicable to the source and target domains .
mapping based dtl refers to mapping instances of the source and target domains to a new data space where the distributions of the two domains are similar.
tca and tca based methods have been widely used in applications of traditional transfer learning .
tzeng et al.
used maximum mean discrepancy mmd to measure the sample distribution after deep neural network processing and learned domain invariant representations by introducing an adaptation layer and additional domain confusion loss.
long et al.
improved previous work by replacing mmd with multiple kernel variant mmd mk mmd originally by and proposed a method called deep adaptation networks dan .
long et al.
proposed joint maximum mean discrepancy to promote the transfer learning ability of neural networks to adapt to the data distribution of different domains.
the wasserstein s distance proposed by arjovsky et al.
has been used as a new distance measure to find a better mapping between domains.
to the best of our knowledge dtl methods have not yet been used in defect prediction.
in this paper we propose a novel mappingbased dtl method using a self attention mechanism for defect prediction.
approach the overall framework of our approach deep transfer learning for defect prediction dtl dp is shown in figure .
it is comprised of two stages source code visualization and modeling with dtl.
in the first stage we use a visualization method to convert program files into images.
in the second stage we build a dtl model based on the alexnet network structure with transfer learning and a selfattention mechanism to construct an end to end defect prediction framework.
we use that framework to predict if a new instance file is defective or not.
in a nutshell our approach takes the raw program files of a training and test sets directly as input and generates images from them which are then used to build the evaluation model for defect prediction.
specifically since the input data of the network model based on the cnn structure should be in the form of images we build a mapping to convert the files into images.
then we use the first layers of alexnet as feature net to generate features from the images.
the shallow cnn layers correlate the presence and absence of defects to the overall code structure gradually deepening the granularity from function loop bodies to function names identifiers etc.
these features are then fed into the attention layer where they are used in assigning weights and highlighting features which are more helpful in the classification.
the re weighted features of the training and test sets are used to calculate mk mmd which is used to measure the difference between their distributions as the mmd loss.
after that the re weighted features of the training set are entered into the fully connected layers to calculate the cross entropy as the classification loss.
the weighted sum of the mmd loss and classification loss is fed back to train the whole network including feature net attention layer and the fully connected layers.
finally based on the source code visualization method and the dtl model 580icse may seoul south korea chen et al.
data augmentationbgr grb rbg.. .. .. feature netattention layerbgr grb rbg clean buggyclean buggysource project target project.. .. .. .. .. ..mmdcleanbuggy cleanbuggy defect visualization model building phase prediction phase figure the overall framework of dtl dp figure the process of converting code to color images we build train and evaluate a defect prediction model.
we give the details in the following.
.
source code visualization converting code to images retains information and facilitates available deep neural network dnn based methods to improve defect prediction performance.
most existing methods ignore associations between extracted features and the classification task.
our proposed method forms a unified end to end framework of feature extraction modeling and prediction.
figure shows how we convert each program file into different images.
we call this process source code visualization and the images produced code images.
first each source file is converted into a vector of bit unsigned integers corresponding to the ascii decimal values of the characters in the source code e.g.
a is converted to etc.
we then generate an image from that vector by arranging its values in rows and columns and interpreting it as a rectangular image.
a straight forward way to visualize the bit values as color intensities would be as levels of gray e.g.
black and white.
however the relatively small size of our original training set means that if we did that we would end up with a deep model that cannot be sufficiently well trained.
fortunately an additional benefit to usingimages for training is that we can augment the original data set to produce a larger data set with the same semantics.
common data augmentation methods for image data sets include flipping both vertically and horizontally rotating cropping translating moving along the x or y axis adding gaussian noise distortion of high frequency features zooming and scaling.
since the sizes of our programs are small and we want to retain the semantic and structural features in the images the above image data augmentation methods could result in data loss.
instead we designed a novel color based augmentation method to generate color images from each source code file.
namely each pixel in a color image can be represented by three primary color components or channels red r green g and blue b .
the letters r g b and thus the color channels can be ordered in different ways rbg rgb bgr brg grb and gbr.
by adopting a different order every time as shown in figs.
and we generate six different images for each program file.
for example for is converted to and the values are filled into the r g and b channels in different ways to obtain differently colored pixels.
the above method expands our data set six fold and because of the nature of the downstream analysis the generated samples are reasonable since the representation is changed only by the order of the different channels.
whereas we generated images for each instance in the training set we randomly selected an rgb permutation for testing.
we found that it was not necessary to generate all six images for testing because our experiments on datasets showed that the performance of using the different permutations was quite comparable.
we randomly chose one to increase the speed in practice.
the data augmentation was used to improve the efficacy of the model.
after the above for each of the orderings of r g b we obtain a vector of pixels of length one third the original code file size each source code character is assigned to a color channel and thus three characters in a row represent a pixel .
we then turned those 2a cnn model learns the features by convolution.
since we use imagenet s pre trained alexnet model the initial parameters of the convolution kernel are different for each channel which means even though byte sequences are the same the final set of features obtained by sequentially inputting into the model the different channels is also different.
581software visualization and deep transfer learning for effective software defect prediction icse may seoul south korea figure the training and testing process of our approach dtl dp inspired by the original dan paper table image width for various file sizes file size range image width kb kb kb kb kb kb kb kb kb kb kb kb kb 1000kb image vectors into image matrices of width and length such that width len th vector size .
we note that the first convolution kernel of alexnet is of size so if the image is too narrow the convolution performance will be poor.
and the image with a suitable width can be convolved to obtain more efficient semantic and structural features in contexts with a proper sequence length.
table gives some recommended image widths for different file sizes based on previous work .
we adopt those in this work.
.
modeling with deep transfer learning next we describe our approach dtl dp.
the goal is to learn transferable features across projects and build a classifier y x of file defectiveness with source code supervision where xis the representation of the sample and yis the predicted label.
in the defect prediction problem we are given a source project ps xs i ys i ns i with nslabeled instances and a target project pt xt i yt i nt i with ntunlabeled instances.
in wpdp the source project is the previous version of the target project while in cpdp the source project is a related project of the target project.
the sample distributions p andqin the source and target projects are often similar in wpdp but different in cpdp.
for our deep network architecture we adopt a similar architecture as that of deep adaptation networks dans to capture the semantic and structural information of the source code.
to the original dan model we add an attention layer to further enhance the expressive ability of important features.
the overall architecture is illustrated in figure .
in particular our dtl dp consists of an input layer five convolution layers conv1 conv5 from alexnet anattention layer and finally four fully connected hidden layers f c6 f c9 working as a classifier.
the structure and parameters of conv1 conv5 and f c6 f c8 are consistent with a dan.
but since defect prediction is a binary classification problem a fully connected layer f c9 is added to obtain a binary result at the end.
we adopt the defaults for alexnet so the input to our dtl dp must be a size image cropped from a three channel rgb image of size .
the code image is placed in the approximate center of the image the determination of the size of each code image is described later in sect.
.
.
we note that the code image can be smaller than pixels due to the variance in the size of the code files.
if this happens the image is padded around with blank zero valued pixels.
padding should not negatively effect the qualitative performance of feature detection in the images in fact for deeper dl architectures like ours padding has been shown to provide extra contrast to the embedded image for each of the layers as well as buffering against data loss by each layer .
training deep models requires a significant amount of labeled data which is not available for a new project so we first pre trained an alexnet model on imagenet .
unlike a dan which freezes conv1 conv3and fine tunes conv4 conv5 we fine tune all convolution layers conv1 conv5by taking the parameters of the pre trained models as initial parameters that we then optimize during the training phase.
we do that to minimize the differences between our code images and the actual object images in imagenet .
in order for dtl dp to focus on the key features in different defect images and thus further improve prediction we employ a self attention mechanism into our model inspired by the good performance of self attention in gans .
as shown in the attention layer in figure the attention mechanism makes the feature map generated by layer conv5 be the self attention feature map input to the next layer fc6.
specifically at first it linearly maps the input features x it is a convolution to compress the number of channels i.e.
out chanels in chanels and produces f x x andh x where f x wfx x w x h x whx.
the difference between the three is that the size of h x is still the same asx but the other two are not.
thus if the width of xisw the height hand the number of channels c the size of xis where n w h the size of f x andg x is but the size of h x is .
the transposed f x and x are matrix multiplied to obtain the autocorrelation in the features i.e.
the relationship of each pixel to all other pixels where sij f xi t xj .
then softmax is applied to the autocorrelation features s to get the attention map comprised of weights with values between and j i exp sij n i 1exp sij .
582icse may seoul south korea chen et al.
.
.
.
.
mmd penalty0.
.
.
.
.
.
.
.
.
.
.
f measure wpdp cpdp a effect of different mmd penalty brg bgr grb gbr rgb rbg0.
.
.
.
.
.
.
.
f measurewpdp cpdp b effect of color channel order width recommended0.
.
.
.
.
.
.
.
f measurewpdp cpdp c effect of code image width recommended ratio figure sensitivity studies to aid in choosing the parameters in the model after that the output of the attentionla yeris the self attention feature map o o1 o2 ... oj .... on where oj n i 1 j ih xi .
then each fully connected layer learns a nonlinear mapping hl i fl wlhl i bl where hl iis the lth layer hidden representation of feature xi wlandblare the weights and biases of the lth layer andflis the activation function using relu fl o max o forf c6 f c8and softmax for f c9.
if we let denote the set of all dtl dp parameters the empirical risk of dtl dp then is min 1 nn i 1f oi yi where fis the cross entropy loss function nis the number of instances and oi is the conditional probability that the dtl dp assigns oito labelyi.
it is used to calculate the final loss in equation .
to make the distribution of the source and target projects similar the same multi layer adaptation and multi kernel mmd strategy as in a dan are used in our model.
the feature mapping function is defined as the combination of mpositive semi definite kernels ku os ot k os ot k m u 1 uku where 0are the weights of the kernels.
the mk mmd dk p q of the probability distributions pandq is defined as the reproducing kernel hilbert space distance between the mean embedding of pandq.
the square formula of mk mmd is defined as d2 k p q ep eq .
we see then that the smaller dk p q is the more similar pandq are.
if dkis the distribution of the target project is the same as that of the source project.
so the final loss function for train dpl dp is nn i 1f oi yi l2 l l1d2 k dl s dl t where dlsis the lth layer hidden representation for the source and target d2 k dls dl t is the mk mmd between the source and targetevaluated on the lth layer representation is a penalty parameter andl1andl2aref clayers to calculate the mk mmd between source and target.
in our implementation of dtl dp we set l1 andl2 as per the original dan work .
.
model sensitivity to parameter choices we have made a number of choices to make our modeling platform work effectively.
here we justify those choices by presenting sensitivity studies.
for this analysis we used all the data.
hyperparameter the dtl dp model has a hyper parameter for the mmd penalty that adjusts the final loss.
we conducted parameter sensitivity experiments in both the wpdp and cpdp setting.
we fix the other parameters and range in .
.
.
.
.
.
the results are shown in figure a .
while fairly constant across the range small variations exist.
in cpdp the f measure first increases and then decreases along the hyper parameter .
in wpdp affects the performance of dtl dp but in the opposite direction.
we chose 1here.
different color orders the selection of the r g b permutations in the target project images may also potentially affect the outcome.
we performed experiments with all six different orderings.
when testing them we set to and changed the image type of the target project to each one in rgb rbg brg bgr gbr grb .
the results are shown in figure b .
the target images in different color orders have somewhat different performance.
but overall the values are close to dtl dp s average performance.
different image widths to explore the impact of different choices for image widths on the results we performed additional experiments.
for our code images bytes of source code are needed to form pixel each byte for one of the r g and b channels.
the sizes of the source files are between and kb corresponding to images with pixel count between and .
the size of the image is width hei ht where width is obtained from table according to the size of the code file size .
hei ht size width for example the size of image converted from a kb code file is .
we experimented with image widths of and times of the recommended widths in table .
the larger the multiplier the wider the image.
for images of different widths we 583software visualization and deep transfer learning for effective software defect prediction icse may seoul south korea repeated the previous experiment and obtained results for the mean f measure shown in figure c .
we see that the closer to the recommended width from table the better the average result.
in addition we found that a wider image is better than a narrower one.
and the closer the image is to a square the better the prediction.
therefore we made the generated images be as close to a square as possible.
.
training and prediction when training the dtl dp code images are generated from the labeled instances in the source project and the unlabeled instances in the target project and then are simultaneously input into the model.
they share the convolution layers conv1 conv5andattentionla yer to extract their respective features and calculate the mk mmd between source and target projects in the fully connected layers f c6 f c9.
it should be noted that we only calculate cross entropy for the source project because the target project s labels are not provided.
we use a mini batch stochastic gradient descent along the loss function in order to train the parameters of the entire model.
we train for epochs for each pair of source and target projects then pick the epoch with the best f measure described in section .
from which to read out the parameters of the final model.
finally the files from the target project that need to be predicted are converted into code images and then input into the trained model for classification.
overfitting is always a possibility with such pipelines.
we moderate it here with our choices of augmenting the dataset by using six color channel permutations described in section .
selecting a simple alexnet model structure and using the imagenet pre trained model to reduce fluctuations.
experimental setup we conducted experiments to asses the performance of dtl dp and to compare it with existing deep learning based defect prediction approaches for both within project wpdp and cross project cpdp defect prediction.
we ran experiments on a modern day linux server with titan xp gpus.
unless otherwise stated each experiment was run times and the average results are reported.
.
dataset description in order to directly compare our work with prior research we used publicly available data from the promise3data repository which has been widely used in defect prediction work .
we selected all open source java projects from this repository and collected their version number class names and the buggy label for each file.
based on the version number and class name we obtained the source code for each file from github4and fed it to our end to end framework.
in total data for java projects were collected.
table shows the details of these projects including project description versions the total and average number of files and the defect rate.
it should be noted that the average number of files over all projects ranges between and and the defect rates of the projects have a minimum value of .
and a maximum value of .
.
the number of files in some projects is not sufficient to train deep models and the classes are imbalanced thus augmentation is needed as described above.
baseline comparison methods to evaluate the performance of our end to end framework dtl dp for defect prediction we compare it with the following baseline methods in the wpdp setting semantic the state of the art method which employs deep belief networks dbn on source code to extract semantic features for defect prediction.
promise dp a traditional method which builds an alternating decision tree adtree classifier based on the original features of the promise dataset.
dp lstm a long short term memory lstm based deep neural network model which uses asts to represent source files and predict whether the file is defective or not.
dp cnn a convolutional neural network cnn based model which is seeded by ast derived numerical vectors to automatically learn semantic and structural features of programs.
the cnn learned features are used to train the final classifier in combination with traditional features.
for the cross project settings cpdp semantic andpromise dp could not be used directly.
instead we used the following dbn cp a variant of semantic which trains a dbn by using the source project and generates semantic features for both the source and target projects.
tca the state of the art technique for cpdp.
to obtain the training and test data we followed the processes established in .
for wpdp we use two consecutive versions of each project listed in table .
the older version is used to generate the training data and the more recent version is used as test data.
for cpdp we pick versions randomly from each project for target projects.
and for each target project we select source projects that are different from the target projects.
we use the same test pairs as in .
when implementing the baseline methods we use the same network architecture and parameter settings as described in the papers that introduced them.
.
performance measures to evaluate the prediction performance we use the f measure a widely adopted metric in the literature .
the fmeasure captures a predictor s accuracy and combines both precision and recall for a comprehensive evaluation of predictive performance.
specifically a prediction that a file is defective is called a true positive tp if the file is in fact defective and false positive fp otherwise.
similarly a prediction that a file is not defective is a true negative tn if the file is in fact not defective and false negative fn otherwise.
then the precision p recall r and f measure are defined as p tp tp fp r tp tp fn f p r p r .
results this section discusses our results of comparing dtl dp to baseline tools for defect prediction.
584icse may seoul south korea chen et al.
table dataset description project description versions files avg files avg size kb defective ant java based build tool .
.
.
.
.
camel enterprise integration framework .
.
.
.
.
jedit text editor designed for programmers .
.
.
.
.
log4j logging library for java .
.
.
.
lucene text search engine library .
.
.
.
.
xalan a library for transforming xml files .
.
.
.
xerces xml parser .
.
.
.
ivy dependency management library .
.
.
.
synapse data transport adapters .
.
.
.
.
poi java library to access microsoft format files .
.
.
.
.
table f measure of dtl dp semantic seman promisedp prom dp lstm lstm and dp cnn cnn for wpdp project version seman prom lstm cnn dtl dp ant1.
.
.
.
.
.
.
.
.
.
.
.
.
.
camel1.
.
.
.
.
.
.
.
.
.
.
.
.
.
jedit3.
.
.
.
.
.
.
.
.
.
.
.
.
.
log4j .
.
.
.
.
.
.
lucene2.
.
.
.
.
.
.
.
.
.
.
.
.
.
xalan .
.
.
.
.
.
.
xerces .
.
.
.
.
.
.
ivy .
.
.
.
.
.
.
synapse1.
.
.
.
.
.
.
.
.
.
.
.
.
.
poi1.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
rq1 how does dtl dp compare to featurebased machine learning methods and astbased deep learning methods in wpdp?
we compare dtl dp to baseline approaches representing two different kinds of defect prediction methods.
promise dp is the baseline representative of traditional feature based machine learning methods.
semantic dp lstm anddp cnn are the baselines for deep learning type methods based on extracting features from ast.
guided by prior work we conducted sets of wpdp experiments each using two versions of the same project.
the older version is used to train the prediction model and the newer version is used to evaluate the trained model.
table shows the f measure values for the within project wpdp defect prediction experiments.
the highest f measure values of the methods are shown in bold.
since the methods based on deep learning include some randomness we run dtl dp semantic dplstm and dp cnn times for each experiment.
on average the f measure of our approach is .
and the promise dp semantic dp lstm anddp cnn achieve .
.
.
and .
respectively.
the results demonstrate that our approach is competitive and may improve on defect prediction compared to promise dp dp lstm anddp cnn.
the results of semantic and our approach are similar.
the proposed dtl dp is effective and it could improve the performance of wpdp tasks.
like other deep learners it is sensitive to small file sizes and unbalanced data.
.
.
case study wpdp discrimination of dtl dp .
t sne is a non linear dimensional reduction algorithm that is effective in visualizing similarities and helping identify clusters in complex data sets .
to give insight in the performance of dtl dp we demonstrate feature transferability by showing t sne embeddings in figure .
the blue points are non defective files and the red are defective ones5.
we observe the following the target instances are not discriminated very well using either the traditional manual features or the tca improved manual features or the semantic features extracted from asts while with our approach the points are discriminated much better.
with the other three approaches the categories between source and target projects are not well aligned while with our approach the categories between the projects are more consistent.
these conclusions are derived from the intra and inter class distances of the two categories in figures and .
they are visually apparent.
our method did not perform as well as the comparison methods on some projects such as ant which is likely caused by large variance in file sizes.
while the sizes of ant .
andant .
are close there is a marked difference between ant .
andant .
the former being much smaller than the latter.
from table the performance of our method on ant1.
ant1.
is better than that on ant1.
ant1.
.
on the contrary baseline methods other than dp lstm perform better for the task ant1.
ant1.
notably semantic is dominant indicating that the semantic feature based method is more robust to file size variability.
moreover the antdataset has two shortcomings the first is that the amount of data is small cf.
table where the average amount is only files in one project.
the second is that 5since we use data augmentation we have more samples than the three baselines.
585software visualization and deep transfer learning for effective software defect prediction icse may seoul south korea figure t sne mapping of source and target features wpdp the classes are unbalanced and the proportion of defective files is .
which is the least of all our projects.
this makes training of the deep model more difficult which leads to the poorer performance of our method on some projects.
table f measure of dtl dp dbn cp dbn tca dplstm lstm and dp cnn cnn in cpdp source target dbn tca lstm cnn dtl dp ant1.
camel1.
.
.
.
.
.
jedit4.
camel1.
.
.
.
.
.
camel1.
ant1.
.
.
.
.
.
poi3.
ant1.
.
.
.
.
.
camel1.
jedit4.
.
.
.
.
.
log4j1.
jedit4.
.
.
.
.
.
jedit4.
log4j1.
.
.
.
.
.
lucene2.
log4j1.
.
.
.
.
.
lucene2.
xalan2.
.
.
.
.
.
xerces1.
xalan2.
.
.
.
.
.
xalan2.
lucene2.
.
.
.
.
.
log4j1.
lucene2.
.
.
.
.
.
xalan2.
xerces1.
.
.
.
.
.
ivy2.
xerces1.
.
.
.
.
.
xerces1.
ivy2.
.
.
.
.
.
synapse1.
ivy2.
.
.
.
.
.
ivy1.
synapse1.
.
.
.
.
.
poi2.
synapse1.
.
.
.
.
.
ivy2.
synapse1.
.
.
.
.
.
poi3.
synapse1.
.
.
.
.
.
synapse1.
poi3.
.
.
.
.
.
ant1.
poi3.
.
.
.
.
.
average .
.
.
.
.
.
rq2 how does dtl dp compare to featurebased machine learning and ast based deep learning methods in cpdp?
here we compare to tca and dbn cp instead of promise andsemantic as the baseline approaches as explained in sect.
.
.again guided by prior work we conducted sets of cpdp experiments.
in each we randomly select two project versions from two different projects one as a training set and the other as a test set.
table presents the f measure results of dtl cp and the baseline approaches.
the highest f measure values are in bold.
on average the f measure of our approach in cpdp is .
and the dbncp tca dp lstm anddp cnn achieve .
.
.
and .
.
thus dtl dp outperforms them by .
.
.
and .
respectively.
in addition we found that for projects log4j1.
andpoi3.
our method does better than the corresponding wpdp best performing method.
our proposed dtl dp shows significant improvements on the state of the art in cross project defect prediction.
.
.
case study cpdp discrimination of dtl dp .
our method is more obviously dominant in cpdp than in wpdp.
a possible reason for that is that deep transfer learning makes the distributions of the training and test samples more similar in feature space.
another reason might be the superior ability of deep models to represent features enabling the model to obtain more transferable features from the images.
to gain more insight we choose the task poi3.
ant1.
and show the t sne embeddings in figure .
we make similar observations as in the rq1 case study that the target instances are more easily discriminated with our approach and the target instances can be better discriminated with the source classifier.
this implies that our approach can learn more transferable features for more effective defect prediction.
figure t sne mapping of source and target features cpdp .
rq3 how much does each of the three mechanisms i.e.
data augmentation transfer learning andself attention mechanism contribute to dtl dp s performance?
to find out the specific contributions of the three parts to defect prediction we conducted further experiments.
we built the original alexnet model for binary classification and use it as the base.
tl attention and dataaug are three new baselines built by adding to the base alexnet one of three mechanisms transfer learning self attention and data augmentation respectively.
it should be noted 586icse may seoul south korea chen et al.
that in addition to dataaug we use images generated by one sequence of r g and b as training and test sets in each experiment.
therefore the experimental results of base tl and attention in tables and are average results obtained over the different rgb permutations as described above.
table contributions of the three mechanisms to wpdp project version base tl atten aug dtl dp ant1.
.
.
.
.
.
.
.
.
.
.
.
.
.
camel1.
.
.
.
.
.
.
.
.
.
.
.
.
.
jedit3.
.
.
.
.
.
.
.
.
.
.
.
.
.
log4j .
.
.
.
.
.
.
lucene2.
.
.
.
.
.
.
.
.
.
.
.
.
.
xalan .
.
.
.
.
.
.
xerces .
.
.
.
.
.
.
ivy .
.
.
.
.
.
.
synapse1.
.
.
.
.
.
.
.
.
.
.
.
.
.
poi1.
.
.
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
table contributions of the three mechanisms to cpdp source target base tl atten aug dtl dp ant1.
camel1.
.
.
.
.
.
jedit4.
camel1.
.
.
.
.
.
camel1.
ant1.
.
.
.
.
.
poi3.
ant1.
.
.
.
.
.
camel1.
jedit4.
.
.
.
.
.
log4j1.
jedit4.
.
.
.
.
.
jedit4.
log4j1.
.
.
.
.
.
lucene2.
log4j1.
.
.
.
.
.
lucene2.
xalan2.
.
.
.
.
.
xerces1.
xalan2.
.
.
.
.
.
xalan2.
lucene2.
.
.
.
.
.
log4j1.
lucene2.
.
.
.
.
.
xalan2.
xerces1.
.
.
.
.
.
ivy2.
xerces1.
.
.
.
.
.
xerces1.
ivy2.
.
.
.
.
.
synapse1.
ivy2.
.
.
.
.
.
ivy1.
synapse1.
.
.
.
.
.
poi2.
synapse1.
.
.
.
.
.
ivy2.
synapse1.
.
.
.
.
.
poi3.
synapse1.
.
.
.
.
.
synapse1.
poi3.
.
.
.
.
.
ant1.
poi3.
.
.
.
.
.
average .
.
.
.
.8table time cost of the three mechanisms defect visualization visualiz.
self attention attent.
and transfer learning tl projecttime s tl attent.
visualiz.
ant .
.
.
camel .
.
.
jedit .
.
.
log4j .
.
.
lucene .
.
.
xalan .
.
.
xerces .
.
.
ivy .
.
.
synapse .
.
.
poi .
.
.
average .
.
.
to discern the contribution of each mechanism we compare the performance of these baseline methods in wpdp and cpdp.
from the results in table and table we observe that the three mechanisms each contribute substantially to the accuracy of dtl dp in both wpdp and cpdp.
in terms of the f measure transfer learningcontributes the least improvement and self attention anddata augmentation contribute similarly to the final result.
but in cpdp transfer learning contributes to the f measure the most.
this is likely because of the applicability of transfer learning in the cpdp setting and is in a way a validation of the approach.
we also noted the time cost for the mechanisms in our proposed dtl dp.
table shows the result.
the most time spent during data augmentation is on converting code into images i.e.
source code visualization.
for transfer learning most time is spent on the mkmmd calculation and for self attention on the calculation of the attention layer in fig.
.
e.g.
for the project ant table shows two sets of wpdp experiments ant .
.6andant .
.
.
on average it takes .
seconds .
seconds and .
seconds for the parts respectively for both the training data and the test data.
transfer learning takes the longest time more than the sum of the other two.
this is because of the large number of matrices needed to calculate mmd with the kernel function.
the least time cost is incurred by the attention mechanism and its contribution to wpdp is the largest of the three i.e.
it is most cost effective.
the three mechanisms all contribute toward the accuracy of our proposed end to end framework in wpdp and cpdp.
selfattention has the greatest contribution to wpdp and transfer learning contributes most to cpdp.
threats to validity threats to internal validity come from experimental errors and the replication of the baseline methods.
in order to compare and analyze the deep learning based defect prediction techniques we compare our proposed dtl dp method with sementic dbn cp dp lstm anddp cnn.
in addition our method is also compared with the transfer learning based method tca which is the state of theart cpdp technique.
since the original implementations were not 587software visualization and deep transfer learning for effective software defect prediction icse may seoul south korea available we re implemented our own versions of the baselines.
although we strictly follow the procedures described in their work our new implementations may not completely restore all of their original implementation details.
and the randomness of the deep learning based approach also makes the results of our implemented experiments different from the original.
since we have removed the entries in the promise dataset that cannot retrieve the corresponding source files our re implemented experimental results may not be consistent with the original baselines.
the external threat to the validity of the results lies in the generalizability of the results.
we have tested our method on open source java projects including files.
defect predictions for instances of other languages such as c c etc.
need to be validated by additional experiments in the future.
threats to construct validity depend on the appropriateness of the evaluation measurement.
the f measure is used as our main evaluation measure which has been applied in many previous efforts to evaluate defect prediction comprehensively.
conclusions and future work here we made two main contributions code visualization for defect prediction and an improved deep transfer learning model.
our experimental results on open source projects show that deep learning can be effectively applied directly for defect prediction after applying visualization methods to the code.
specifically our approach dtl dp performs at the top of the range of state of the art wpdp approaches.
for cpdp dtl dp improves on the state of the art technique tca built on traditional features by .
in the fmeasure .
it also bests the deep learning based approaches dbn cp dp lstm and dp cnn by .
.
and .
respectively.
dtl dp still has some limitations.
for some projects a problem of negative transfer occurs resulting in a worse prediction than a direct prediction.
large differences between two projects can cause such negative transfer.
reducing the impact of negative transfer is one of the problems to be solved in the future.
additionally the amount of training data we had was small and class imbalance is inherent in software defect prediction.
both can be improved with more data which we plan to obtain in the future.