aid efficient prediction of aggregated intensity of dependency in large scale cloud systems tianyi yang jiacheng shen y uxin su xiao ling y ongqiang yang and michael r. lyu department of computer science and engineering the chinese university of hong kong hong kong china.
email tyyang jcshen yxsu lyu cse.cuhk.edu.hk computing and networking innovation lab cloud bu huawei email lingxiao1 yangyongqiang huawei.com abstract service reliability is one of the key challenges that cloud providers have to deal with.
in cloud systems unplanned service failures may cause severe cascading impacts on their de pendent services deteriorating customer satisfaction.
predictingthe cascading impacts accurately and efficiently is critical to theoperation and maintenance of cloud systems.
existing approachesidentify whether one service depends on another via distributedtracing but no prior work focused on discriminating to whatextent the dependency between cloud services is.
in this paper we survey the outages and the procedure for failure diagnosis intwo cloud providers to motivate the definition of the intensity ofdependency.
we define the intensity of dependency between twoservices as how much the status of the callee service influencesthe caller service.
then we propose aid the first approach topredict the intensity of dependencies between cloud services.
aidfirst generates a set of candidate dependency pairs from thespans.
aid then represents the status of each cloud service witha multivariate time series aggregated from the spans.
with therepresentation of services aid calculates the similarities betweenthe statuses of the caller and the callee of each candidate pair.finally aid aggregates the similarities to produce a unified valueas the intensity of the dependency.
we evaluate aid on thedata collected from an open source microservice benchmark anda cloud system in production.
the experimental results showthat aid can efficiently and accurately predict the intensityof dependencies.
we further demonstrate the usefulness of ourmethod in a large scale commercial cloud system.
index t erms cloud computing software reliability aiops service dependency i. i ntroduction service reliability is one of the key challenges that cloud providers have to deal with.
the common practice nowadays is developing and deploying small independent and looselycoupled cloud microservices that collectively serve users requests.
the microservices that serve the same purpose arecalled cloud services .
the microservices communicate with each other through well defined apis.
such an architectureis called microservice architecture .
the microservice ar chitecture has been widely adopted in cloud systems becauseof its reliability and flexibility.
under this architecture mi croservice management frameworks like kubernetes will beresponsible for managing the life cycles of microservices.
y uxin su is the corresponding author.
1for simplicity in this paper cloud service and cloud microservice are interchangeable when they are used alone.developers can focus on the application logic instead of thebothering tasks of resource management and failure recovery.
although microservice management frameworks provide automatic mechanisms for failure recovery unplanned servicefailures may still cause severe cascading effects.
for example failures of critical services that provide basic request routingfunctions will impact the invocation of cloud services slowdown request processing and deteriorate customer satisfaction.therefore evaluating the impact of service failures rapidlyand accurately is critical to the operation and maintenanceof cloud systems.
knowing the scope of the impact reliabilityengineers can put more emphasis on services that have greaterimpacts on others.
a failed service will only affect services that will invoke it.
in other words service invocations cause dependenciesbetween services.
many recent approaches proposeto use the dependencies of services to approximate theirfailure impact.
all the services and dependencies in a cloudsystem collectively construct a directed graph of services which is also called a dependency graph.
identifying whetherone service depends on another in cloud systems can bewell solved by industrial tracing frameworks like dapper andjaeger.
by using these frameworks all the invocations betweenthe caller and callee services can be recorded as traces that arecomposed of spans.
the attributes about each invocation likeduration status invoked service name timestamp etc.
arerecorded in each span.
based on the spans current dependencydetection methods treat the dependency as a binary valueindicating whether one service invokes another or not.
however modeling the relations of services solely with binary dependencies is not precise enough.
to show the insuffi ciency of existing methods we first conduct an empirical studyon the outages of amazon web service and huawei cloud.we point out that it is inefficient to conduct failure diagnosisand recovery based on binary dependencies.
this is becausethe different dependencies of a cloud service impact the cloudservice in different ways.
manual examination of differentdependencies without any priority is inefficient especially incloud systems where the number of dependencies could belarge.
based on this observation we argue that it will be help ful if the dependency can be measured as a continuous valuethat indicates the intensity of this dependency.
specifically by checking services that are dependent on the failed service 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee with large intensity values on call engineers oces can find the root cause of a system failure with a higher probability.by recovering the services that are strongly dependent on thefailed one the whole system could be restored faster.
to improve the reliability of cloud systems in this paper we propose aid an end to end approach to predict the intensityof dependencies between cloud microservices for cascadingfailure prediction.
we first generate a set of candidate depen dency pairs from the spans.
then we distribute each span intodifferent fixed length bins according to their timestamp andservice name.
we calculate the statistics of all spans in eachbin as the key performance indicators kpis for the bin.
thekpis of one service form a multivariate time series that willbe treated as the representation of the service s status.
foreach candidate dependency pair we calculate the similaritiesbetween the statuses of the two services in the pair.
finally we aggregate the similarities to produce a unified value as theintensity of the pair.
to show the effectiveness of aid we evaluate aid on two datasets.
one is a simulated dataset and the other is anindustrial dataset.
for the simulated dataset we deploy train ticket an open source microservice benchmark system simu late users requests and collect the traces.
for the industrialdataset we collect the traces from a production cloud system.then we evaluate aid on the datasets and compare its perfor mance with several baselines.
the experimental results showthat our proposed method can accurately measure the intensityof dependencies and outperform the baselines.
furthermore we showcase the successful usage of our method in a large scale production cloud system.
in addition we release bothdatasets to facilitate future studies.
the main contributions of this work are highlighted as follows we propose aid the first method to quantify the intensityof dependencies between different services.
the evaluation results show the effectiveness and effi ciency of the proposed method.
we release a simulated dataset and an industrial datasetfrom a production cloud system to facilitate future stud ies.
organization.
the remainder of this paper is organized as follows.
section ii provides motivation and background knowl edge that underpin our approach.
we describe our survey andempirical study on real outages that motivate the proposedmethod in section iii.
section iv elaborates on the methodin detail.
section v introduces the datasets baselines andshows the experimental results.
successful use cases of theproposed method in a production cloud system are demon strated in section vi.
we discuss the practical usage theperceived limitations and the possible threats to validity insection vii.
section viii introduces related works.
the lastsection section ix concludes this paper and lists directionsfor future exploration.ii.
b ackground in this section we briefly describe the service oriented architecture of cloud systems and the distributed tracing toolsin cloud systems.
then we present the main techniques i.e.
time series similarity analysis that underpin our approach.
a. the architecture of cloud systems modern cloud systems are often constructed from a complex and large scale hierarchy of distributed software modules .
the common practice nowadays is to develop and deploy thesesoftware modules as cloud microservices that collectivelycomprise multiple large cloud services .
microservices aresmall independent and loosely coupled software modules thatcan be deployed independently .
different microservicesserve different responsibilities like user authentication resource allocation virtual network management billing etc.when an external request arrives at the cloud system therequest will be routed through the system and served by dozensof different cloud services and microservices.
the microser vices communicate with each other through well defined apisand therefore can be refactored and scaled independently anddynamically to adapt to incidents like surges of requests andservice failures .
such an architecture is called microservicearchitecture .
the microservice architecture becomes increasingly popular due to its high flexibility reusability and scalability .
it en ables agile development and supports polyglot programming i.e.
microservices developed under different technical stackscan work together smoothly.
however the loosely couplednature of microservices makes it difficult for engineers toconduct system maintenance.
different microservices in alarge cloud system are usually developed and managed byseparate teams.
each team only has access to their ownservices as well as services that are closely related whichmeans they only have a local view of the whole system .
as a result the failure diagnosis fault localization and performance debugging in a large cloud system become morecomplex than ever .
despite various fault tolerancemechanisms introduced by modern cloud systems it is stillpossible for minor anomalies to magnify their impact andescalate into system outages.
as exemplified in section iii a when a cloud service enters an anomalous state and does notreturn results in a timely manner other services that dependon it will also suffer from the increased request latency.
suchanomalous states can propagate through the service callingstructure and eventually affect the entire system resulting ina degraded user experience or even a service outage.
b. distributed tracing for commercial cloud providers it is crucial to troubleshoot and fix the failures in a timely manner because massive user applications may be affected even by a small servicefailure .
distributed tracing is a crucial technique forgaining insight and observability to cloud systems.
in large scale cloud systems a request is usually handled by multiple chained service invocations.
as clues to defective 654span span span span span span parentid name spanid timestamp duration ...result fig.
.
a trace log with six spans.
services are hidden in the intricate network of services it is difficult for even knowledgeable oces to keep track ofhow a request is processed in the cloud system.
distributedtracing provides an approach to monitor the execution pathof each request.
for chained service invocations e.g.
serviceainvokes service b and service binvokes service c i t is important to know the status of each service invocation including the result the duration of execution etc.
by addinghooks to the services and microservices of the cloud system adistributed tracing system can record the contextualinformation of each service invocation.
such records are calledspan logs abbreviated as spans.
a span represents a logical unit of execution that is handled by a microservice in acloud system.
all the spans that serve for the same requestcollectively form a directed graph of spans as illustrated infigure .
such a directed graph of spans generated by a requestis called a piece of trace log abbreviated as a trace.
a trace represents an execution path through the cloud system.
with atrace engineers can track how the request propagates throughthe cloud system.
collectively analyzing the traces of theentire cloud system can help engineers obtain in depth latencyreports that could assist failure diagnosis fault localization and surface performance degradation in the cloud system.
span id e22f30bdbfd09134 parent span id b42a04bf18997d5d name ts preserve service timestamp s duration s result success trace id c0d17d481f47bdd9 additional logs ... fig.
.
a span generated by the train ticket benchmark.
although the actual implementation of distributed tracing systems varies a lot the types of information they recordare similar.
for clarity we formally describe the attributes ofspans as follows.
suppose we have a trace tcomposed of spans s s2 ... sn a span si tcontains the following attributes2 sid i the id of span si spid i the id of the parent span of si stid i the id of the trace that sibelongs to 2other additional contextual information is omitted as we do not use them in our method.
snamei the name of service microservice corresponding tosi stsi the time stamp of si sdi the duration of execution of si and sri the result of execution of si.
figure illustrates a span generated by the train ticket benchmark .
it means that service ts preserve service was invoked at on april .
the duration of execution is s and the execution result is success.
c. time series similarity analysis time series data are ubiquitous.
one important task in time series data mining is to measure the similarity between two time series.
similar to human intuition the similarity measureis usually based on the similarity between the shapes of twotime series .
dynamic time warping dtw is a widely used similarity measure when two time series have the same overallcomponent shapes but are not aligned on the timeline.
itattempts to align two time series along a timeline by distortingthe timeline for one time series so that its converted form isbetter aligned with the second time series.
dtw was initiallyused in speech recognition applications and extended andoptimized by many works .
iii.
m otiv a tions the research described in this paper is motivated by the maintenance of a real world cloud system in production.
inthis section we first survey thirteen publicly known serviceoutages that severely affected amazon web services aws from to .
among the thirteen outages we identifyfive that are related to service dependency and summarized theconsequences of inappropriate management of service depen dency.
second we empirically study the diagnosis records offive real outages in the cloud system of huawei cloud that arerelated to inappropriate management of service dependency.our study indicates that the information in the traces has notbeen used efficiently and current practice heavily relies on theengineers familiarity with the dependencies in the system.lastly we propose to measure the intensity of dependency interms of status propagation between dependent cloud services.we demonstrate the usefulness of the intensity by motivatingexamples in real cloud systems.
a. a survey of the outages in aws service outages are inevitable in the cloud .
in this section we empirically analyzed over incidents of huawei cloud in and thirteen publicly known major outages 3of aws from to .
among the incidents of huaweicloud we found that improper service dependency is themost frequent reason for failures in huawei cloud.
amongthe outage summaries of aws we also identified that five 655table i summary of aws outages rela ted to service dependency .
dateconsequences cascading failure slow recovery apr check june check oct check aug check nov check check of the outages are related4to service dependency.
as shown in table i among the five outages that are related to service dependency three of them are due to cascading failurestriggered by erroneous upgrades of services.
during the failurerecovery the inappropriate dependencies lead to slow failurerecovery in three outages.
aws is the worldwide leading cloud provider.
it operates in many regions each consisting of multiple availabilityzones azs .
each az uses separate physical facilities andindependently provides various cloud services includingsteam data processing kinesis api usage analysis cog nito customer dashboard cloudwatch elastic computecloud ec2 relational database service rds elastic loadbalancing elb and low level block storage ebs etc.
forbrevity s sake we simplify the dependencies as ec2 rds and elb all depend on ebs and cognito and cloudwatchdepend on kinesis .
the outages on april and october are both caused by erroneous upgrades of ebs.
when ebs failed the services that depend on ebs i.e.
ec2 elb and rds are all affected.
the cascading failures resulted in servicedisruptions of over hours in the us east region of aws.
the outages on june and august are both triggered by the blackouts.
after the blackout the rdsand elb services restarted quickly as expected but they arestill unable to fully recover because they both depend on ebsservice which at that time can not recover simultaneously.the slow failure recovery incurred by service dependenciesaffected the service availability for days in the us east 1region and the eu west region of aws.
as a follow upoptimization elb service reduced the dependency on ebsafter the outage in .
on november the erroneous upgrade of kinesis lead to its failure cascadingly causing the failure of cognitoand cloudwatch.
more severely during the recovery awscould not notify the customers via the normal way because thenormal customer notification service also relied on cognito.due to the inner mechanism of kinesis the recovery of kinesistook more than ten hours.
thus the recoveries of cognito 4the outages are usually caused by various reasons that mutually affect each other.
service dependency is one of the reasons so we use the word related .
5the actual dependency relations between these services are complicated.
we omit the details here.and cloudwatch were also slowed down.
as a follow up optimization cognito and cloudwatch services reduced thedependency on kinesis after the severe outage.
b. drawbacks of current failure diagnosis methods to gain more knowledge about the procedure of failure diagnosis in industrial circumstances we first interviewed engineers in huawei cloud .
then we summarize the procedure of failure diagnosis and point out the drawbacks of currentpractice in huawei cloud.
in huawei cloud the failure diagnosis can be triggered by two systems i.e.
the customer support system and themonitoring system.
when a customer experiences a servicedisruption the customer can submit a support ticket in thecustomer support system.
the on call engineers will distributethe support ticket to the corresponding engineers responsiblefor the service.
the monitoring system on the other hand monitors the key performance indicators kpis and the logsof each service in the cloud system.
if the kpis or the numberof erroneous logs of one service increased abnormally orreached predefined thresholds the monitoring system will sendan alert to the corresponding engineers.
upon receiving thesupport ticket or alert engineers start diagnosing the failures.
we summarize the common practice of failure diagnosis in huawei cloud as follows.
suppose the anomalous serviceisa oces will first check whether the failure is caused by the faults of service a e.g.
an erroneous upgrade .
if so the development team of service awill handle the failure.
if service ais in good condition oces will analyze the status of all services that adepends on.
the status includes the number of calls the error rate etc.
if they found the failureof a service bis likely to cause the failure of service a then engineers will continue to investigate service b. recall that all the services construct a directed graph where each noderepresents a service.
the failure diagnosis procedure can beviewed as a recursive search on the service dependency graph.
the practice works well in small cloud systems that contain tens of cloud services.
however the dependencies inlarge scale cloud systems are much more complicated making manual failure diagnosis inefficient and difficult forengineers.
engineers may have trouble identifying the causeof the failure.
in this case the development teams of allcloud services have to check whether the failure is caused bytheir corresponding services.
sometimes engineers may inferthe possible causes of a failure but it heavily relies on theengineer s familiarity with the dependencies in the system.in summary the complex dependency relations in large scalecloud systems make failure diagnosis difficult and currentpractice is inefficient and dependent on the human experience.
c. intensity of service dependency a cloud system is composed of many services.
the dependency between two services is caused by one service invoking the other via predefined apis.
existing tools 6aws does not disclose the detailed procedures of failure diagnosis related to the five outages so we cannot analyze the aforementioned outages in depth.
656fig.
.
the statuses of service a bandc.ainvokes bandcbutbhas a greater effect on a. treat the dependency as a binary relation i.e.
if the caller service invokes the callee service then the caller is dependenton the callee.
we suggest that this binary dependency metricis not fine grained enough for cloud maintenance.
figure 3shows the statuses of three services 7a b and cin huawei cloud.
service ainvokes both service band service c. service bencountered failures.
the x axis represents time in minute.
the y axes represent the number of invocations per minute the average duration of invocations per minute and the errorrate per minute of a b and c. although service ainvokes service band service c it is obvious that the statuses of bandcinfluence the status of ain different degrees.
the reason is that the functionalities provided by service aand bare creating virtual machines and allocating block storage respectively.
creating a virtual machine requires allocating oneor more block storage.
thus the failure of service binevitably affects service a. on the contrary due to the fault tolerance mechanism of service a the failure of service cwill not affect service aa lot.
thus it is more accurate to say that the intensity of dependency between service aand service b is higher than the intensity of dependency between service a and service c. as can be seen in figure the similarity of the statuses reflect the difference in the intensities.
ideally if the development team of every cloud microservice accurately provides the intensity of dependencies for everydependent services the failure diagnosis could be accelerated.oces can prioritize the services that exhibit higher intensityof dependency instead of inspecting all the dependent services section iii b if they have accurate intensity information.
7for confidentiality reasons we cannot reveal the names of related services.however due to the complexity and the fast evolving nature ofcloud systems manually maintaining the dependency re lations with intensity is very difficult.
as a result oces oftenstruggle in diagnosing failures due to the lack of intensities.
inorder to relieve the pressure on oces we propose to predictthe intensity of dependency from the statuses of services.
iv .
a pproach in this section we present aid a framework for predicting the a ggregated i ntensity of service d ependency in largescale cloud systems.
we first present the overall workflow ofaid.
then we elaborate on each step in detail i.e.
candidateselection service status generation and intensity prediction.
a. overview the overall workflow of aid is illustrated in figure .
aid consists of three steps candidate selection status generation and intensity prediction.
given the raw traces aid firstgenerates a set of candidate service pairs p c where service pdirectly invokes service c section iv b .
the intuition is that direct service invocation incurs direct dependency tosome degree.
indirect dependencies through the transitivityof service invocation will be discussed in section vii a.for status generation we generate the status of all ser vices section iv c .
the status of one service is composedof three aspects of dependency i.e.
number of invocations duration of invocations error of invocations.
each aspect ofthe service s status contains one or more key performanceindicators kpis depending on the actual implementation ofthe distributed tracing system.
a kpi is an aggregated valueof a service status of all the spans of a service in a fixedtime interval e.g.
minute.
we use the statistical indicatorsof each aggregation as the values of the kpis.
motivated bythe experience of engineers introduced in section iii b wepropose to predict the intensity of service dependencies fromthe similarity of the statuses of dependent services.
the intu ition behind using the similarity of time series is to evaluatethe propagation of service statuses.
the intensity predictionstep section iv d predicts the intensity of dependency bymeasuring the similarity between two service s statuses.
thesimilarity between two service s statuses is a normalized andweighted average of the similarity of all the kpis of the twoservices.
we calculate the similarity between two kpis by adynamic status warping algorithm.
finally aid produces thedependency graph with intensity.
b. candidate selection in general direct service invocations can be divided into two categories i.e.
synchronous invocations and asynchronous invocations.
modern tracing mechanisms can keep track ofboth synchronous and asynchronous invocations .
givenall the raw traces of the cloud system in this step wegenerate a candidate dependency set cand .
the candidate dependency set cand contains service invocation pairs p c1 p2 c2 pn cn .
each pair pi ci in the candidate dependency set denotes that the service named 657raw tracesservice status generationdependency graph with intensity section iv .csection iv .dintensity prediction status series of servicescandidate dependency listcandidate selection section iv .b fig.
.
the overall workflow of aid.
piinvokes the service named ciat least once.
therefore servicepidepends on service ci.
this step is to shrink the search space of possible dependent pairs because the service invocations indicate direct dependencies.
to generate the candidate dependency set we need to know the name of the caller service and the callee service.
the nameof callee service is clearly recorded in the span but the nameof the caller service is not.
hence we first augment each span s by adding another attribute s pnamewhich denotes the service name of the parent span.
specifically the augmentation ofattributes pnameis achieved by looking for another span s primewhoseidis the same as spid and set the name of s primeasspname.
then we iterate over all the spans and add spname sname to the candidate dependency set by the union operation.
for example assuming the name of services are the same as the index of spans the six spansin figure will result in a candidate set of service service service service service service service service service service .
c. service status generation in this step we generate the status of all cloud services from the traces.
we start by defining the status of a cloud service i.e.
service status and then describe the procedure of servicestatus generation.
definition of service status a service invocation is composed of three logical components i.e.
the caller service thecallee service and the network communication.
in particular the caller service initiates an invocation to the callee servicevia the network.
the callee service then processes the invo cation during which it may invoke other services.
after theprocessing is finished the callee service will send the result e.g.
status to the caller service via the network.
hence wecould derive three aspects of service invocations initiation of invocation processing result.
as service invocations occur repeatedly the three aspects of service invocations can derivethree aspects of service dependency number of invocations the number of invocations from the caller to the callee duration of invocations the duration of invocations error of invocations the number of successful invocations from the caller to the callee.representation of service status in a cloud system the spans record information about every invocation.
intuitively the status of a cloud service can be easily obtained from thespans of that service.
inspired by the common practice incloud monitoring we distribute the spans of one serviceinto many bins according to the spans timestamps.
each binaccepts spans whose timestamp is in a short fixed lengthperiod.
we denote the length of the short period as .f o r example the span shown in figure will be put in the bin ofts preserve service at time april .
we can then represent the status of a cloud service in a shortperiod by the statistical indicators of all the spans in thecorresponding bin.
formally given all the spans in the cloud system over a long periodt we first initiate s nempty bins of the predefined size .sis the number of microservices.
n determined by t is the number of bins.
then we distribute all spans into different bins according to their timestamp stsand service namesname.
after that we calculate the following three types of indicators as the kpis for each bin.
invom t total number of invocations spans in the bin errm t error rate of the bin i.e.
the number of errors divided by the number of invocations durm t averaged duration of all spans in the bin wheretis the time of the bin and mis the microservice name of the bin.
if a service is not invoked in a particular bin i.e.
the corresponding bin is empty all the kpis will be zero.
inthe end we get the kpis of every service mat every period t. ordering the bins by t we get three time series of kpis for each cloud service denoted as invo m errm anddurm.w e name the time series of server kpis as status series.
d. intensity prediction in this paper we define the intensity of dependency between two services as how much the status of the callee service influences the status of the caller service.
the step of intensity prediction quantitatively predicts the intensity of dependencyby measuring the similarity between two services statusseries.
specifically we calculate the similarity of two differentstatus series with dynamic status warping and aggregate all thesimilarities to get the overall similarity.
dynamic status warping inspired by the dynamic time warping algorithm dtw we propose the dynamicstatus warping dsw algorithm algorithm to calculate the 658algorithm dynamic status warping input the status series of caller service and callee servicestatusp statusc duration series of calleedurc estimated round trip time rtt max time drift d output the similarity between two status series 1set the warping window w max durc rtt 2k length statusc 3n length statusp 4initialize the cost matrix c rk n set the initial values as 5c1 statusp statusc1 6fori ...min d k do initialize the first column ci ci statusp1 statusc i 8end 9forj ...min w d n do initialize the first row c1 j c1 j statuspj statusc 11end 12fori ...k do forj m a x i d ...min n i w d do ci j m i n ci j ci j ci j statusp j statusci end 16end 17return ck n distance between two status series.
dsw automatically warps the time in chronological order to make the two status seriesas similar as possible and get the similarity by summing thecost of warping.
it utilizes dynamic programming to calculatean optimal matching between two status series.
given twoservicesp c and their status series invo p invoc errp errc durp anddurc the warping from the callee cto the callerpis specially designed for the cloud environment.
the design considerations include directed warping due to the latency of the network and the time of processing it takes some time for the status of the callee service to affect the status of the caller service.therefore different from dynamic time warping the timewarping of dsw is directed meaning that the matching fromthe callee to the caller can only happen in chronological order.
adaptive propagation window in cloud systems after the round trip time rtt plus the duration of request processing the caller can receive the result of an invocation.
thus the size of the directed warping window wis automatically set as the maximum duration of the callee s spans plus rtt.
time drift the machine time may drift due to issues with time synchronization in cloud systems so we add an undirectedtime drift dto the warping window.
in summary statusc ican only be matched with one of statusp i d statuspi w d .
the dsw returns the warping costcm n as the measure of similarity.table ii da taset sta tistics .
dataset tt industry9 microservices spans about .0e10 strong weak similarity aggregation for all pi ci cand w e calculate similarities between their status series denoted as d pi ci invo d pi ci err andd pi ci dur.
we normalize the similarity across the whole candidate set with a min max normalizationwith equation where status invo err dur .
d pi ci status d pi ci status min d p c status max d p c status min d p c status the intensity of dependency between piandciis the average similarity of all three similarities between their statusseries.
i pi ci summationdisplay status sd pi ci status s invo err dur finally we can build the dependency graph with intensity from the candidate set and the corresponding intensity values.
v. e xperiments in this section we evaluate aid on both a simulated dataset and an industrial dataset.
particularly we aim to answer thefollowing research questions rqs rq1.
how effective is aid in predicting the intensity of dependency?
rq2.
what is the impact of different parameter settings?
rq3.
what is the impact of different similarity measures?
rq4.
how efficient is aid?
a. experimental setup dataset to show the practical effectiveness of aid we further conduct experiments on the simulated dataset and anindustrial dataset from the cloud system of huawei cloud.since there are no existing datasets of trace logs we deploya benchmark microservice system to simulate a real cloudsystem.
we simulate user requests and collect the generatedtrace logs to construct the simulated dataset.
we release bothdatasets with the paper to facilitate future studies in this field .
simulated dataset for the simulated dataset we deploy train ticket an open source microservice benchmark fordata collection.
train ticket is a web based ticketing systemwith microservices through which users can search for 9we only labeled dependencies that the engineers are familiar with.
659tickets reserve tickets and pay for the reserved tickets.
an open source tracing framework jaeger is used to trace all theapi calls.
to generate traces we develop a request simulatorthat simulates normal users access to the ticketing system.
thesimulator will log in to the system search for tickets reservea ticket according to the results of the search and pay for theticket.
then we collect the traces from jaeger and transformthe traces into spans.
the dataset is termed as tt in table ii.
industrial dataset apart from the simulated dataset we also collected traces from a region of huawei cloud toevaluate aid.
to support tens of millions of users world wide the cloud system of huawei cloud contains numerouscloud services and microservices.
the service invocationsare monitored and recorded by an independently developeddistributed tracing system.
the complex dependency relationsin the cloud system increase the burden of oces.
the ocescan diagnose problematic microservices timely if the intensityof dependencies can be automatically detected in real time.
toevaluate the practical effectiveness of our method we collecteda day long trace dataset with microservices in april2021.
the dataset is termed as industry in table ii.
manual labeling since our method is unsupervised labels are only for evaluation.
neither of the datasets has labels aboutthe intensity of dependency so manual labeling is needed.we set two candidate labels for the intensity of dependency i.e.
strong and weak .
given a candidate dependency pair p c if the failure of service cwill cause the failure of servicep the intensity between p c should be labeled strong otherwise it should be labeled weak .
for thesimulated dataset two ph.d. students inspect the source codeof all microservices and label every service dependency inde pendently.
for the industrial dataset several senior engineersare invited to manually label the intensity of dependency.
inboth processes disagreement on labels will be discussed untilconsensus is reached.
finally we convert the strong labelsto1and the weak labels to 0so that they can be effectively compared with the computed intensities.
the statistics of the datasets are listed in table ii.
microservices denotes the number of microservices in thedataset.
spans denotes the number of spans in the dataset.
strong and weak denote the number of dependenciesthat are labeled with strong or weak respectively.
baselines since there is no existing work that measures the intensity of service dependency we use pearson correla tion coefficient spearman correlation coefficient and kendallrank correlation coefficient as the baseline.
particularly wecalculate correlation on the status series of a candidate dependency pair p c denoted as corrp p c status andcorrs p c status .
for the baselines we directly use the implementation from the python package scipy.
we map the correlation to with the function f x x .
the intensities of dependencies are then produced in the same way as equation .
evaluation metrics we employ cross entropy ce mean absolute error mae and root mean squared errortable iii performance comparison of different methods on two da tasets dataset methodmetric ce mae rmse ttpearson .
.
.
spearman .
.
.
kendall .
.
.
aid .
.
.
industrypearson .
.
.
spearman .
.
.
kendall .
.
.
aid .
.
.
rmse as calculated in equation to evaluate the effec tiveness of aid in predicting the intensity of dependency.
ce nn summationdisplay i mae summationtextn i yi pi n rmse radicalbigg summationtextni yi pi n specifically cross entropy calculates the difference between the probability distributions of the label and the prediction.
mean absolute error and root mean squared error measuresthe absolute and squared error.
lower ce mae and rmsevalues indicate a better prediction.
experimental environments we run the experiments on the simulated dataset on a linux server with intel xeon e5 cpu .40ghz and gb ram.
the experimentson the industrial dataset run on a laptop with intel core i7cpu .
ghz and gb ram.
b. rq1 how effective is aid in predicting the intensity of dependency?
to study the effectiveness of aid we compare its performance with the baseline models on both the simulated datasetand the industrial dataset collected from huawei cloud.
forthe parameters of aid we set the bin size 1minute the estimated round trip time rtt .
specially we set the max time drift d 1minute for the industrial dataset and set d for the simulated dataset.
we do this because the simulated dataset is deployed in a single server so the timedrift will not be a problem.
in addition we use moving averageto smoothen the status series for the baselines and our method.the outputs are scalar values ranging from 0to1.
a larger value indicates higher intensity.
the overall performance isshown in table iii where we mark the smallest loss for eachloss metric and dataset.
660aid achieves the best performance on the industrial dataset and reduces the loss by .
.
and .
in terms of cross entropy mean absolute error and root mean squarederror.
on the simulated dataset aid achieves the best per formance in terms of cross entropy and root mean squarederror.
pearson correlation coefficient marginally outperformsaid on the simulated dataset.
the improvement of aid on thesimulated dataset is smaller than that on the industrial dataset.this is because the benchmark for simulation did incorporatevery few fault tolerance mechanisms making most of thedependencies strong.
moreover since the service invocationsof the tt benchmark are very fast the statuses of tt sservices are relatively similar making simple baselines andour approach perform similarly.
c. rq2 what is the impact of different parameter settings?
fig.
.
prediction loss under different bin size .
since the estimated round trip time rttand the max time drift dare minuscule we only study the impact of the bin size .
as the range of time of the simulated dataset is small we only study the impact of the bin size in the industrial dataset.
in particular we conduct experiments on with the bin size minutes and keep rtt and d 1minute .
we did not set larger bin sizes because larger bin sizes result inmore coarse grained sampling of the service status which willadd difficulty to the similarity calculation in the subsequentdsw algorithm.
figure shows the prediction loss under different bin size .
the x axis denotes the bin size and the y axis shows the threeloss metrics.
the results indicate that the impact of differentbin sizes in a reasonable range is small but 1minute gives the best performance on the industrial dataset.
d. rq3 what is the impact of different similarity measures?
we further study the impact of different similarity measures on both datasets.
aid dsw denotes aid that uses the proposed dsw to measure the similarity between status series.
aiddtw denotes aid that uses the dtw to measure the similarity.
we keep the bin size 1minute and the estimated round trip time rtt as usual.
similar to previous experiments we set the max time drift d 1minute for the industrial dataset and set d for the simulated dataset.table iv the impact of different similarity measures dataset bin sizemethodmetric ce mae rmse tt 1minaiddsw .
.
.
aiddtw .
.
.
industry 1minaiddsw .
.
.
aiddtw .
.
.
table iv shows the performance of aid dsw and aid dtw on both datasets.
on the industrial dataset the proposed dswalgorithm improves the performance but on the simulateddataset the performance is almost the same.
this is probablybecause the duration of spans in the simulated dataset is toosmall so that the effect of directed warping is weak.
the resultsimply that the proposed dsw algorithm works better in real world cloud environments.
e. rq4 how efficient is aid?
the most time consuming operations are the candidate selection and service status generation steps because we have to iterate over all the spans in the cloud system.
theoretically the time complexities of the candidate selection and servicestatus generation steps are o s wheresis the number of spans to process in the cloud system.
in practice the industrialdataset contains about .
10spans so we process it with a distributed computing service in huawei cloud.
sincethe preprocessing is dynamically scheduled and mixed withother teams tasks we do not count the time spent on it.
forthe intensity prediction step the time complexity is o kn wheren t is the number of bins and kis proportional to the warping window w. in practice the intensity prediction step takes seconds on average to process two status series both with bins on a laptop.
since the similarity calculation of different p c pairs are independent we could easily parallelize the intensity prediction step to further improve thetime efficiency.
vi.
c ase study in huawei cloud aid has been successfully incorporated into the dependency management system that serves hundredsto thousands of cloud services.
figure illustrates the concep tual workflow.
aid processes trace logs and continuously up dates the aggregated intensity in the dependency managementsystem.
the reliability engineers will categorize the intensityinto different levels by referring to both the output of aidand their domain expertise.
then the dependency managementsystem will provide reference to the engineers in optimizingdependencies and mitigating cascading failures.
a. optimization of dependencies in a cloud system service failures are inevitable but we can prevent the failures from affecting other services by optimizing 661cloud servicesdependency management center fault injection engineers end userintensity prediction aid manual correction prevention mitigation of failures fig.
.
the use case of aid.
improper dependencies.
aid assists in the discovery of unnecessary strong dependency on critical cloud services.
if a criticalcloud service depends on another service with high intensity the dependency management system will remind the engineersto check whether the dependency is necessary.
if the depen dency is unnecessary the development team has to reduce theintensity of the dependency to improve the robustness of thecritical cloud service.
since aid s deployment more than tenunnecessary dependencies of critical cloud services have beendiscovered by aid and optimized by the development team.
b. mitigation of cascading failures aid also assists in the mitigation of cascading failures.
during a cascading failure aid can provide the latest intensity of dependency to oces so that they can diagnose servicefailures efficiently.
in addition when a cascading failureoccurs oces can limit the traffic to critical cloud services andrecover the dependencies marked as strong first.
by the service disruption can get under control.
once a criticalfailure occurs the manually confirmed strong dependencieswill be treated with high priority.
we conduct field interviewswith oces to collect feedback.
based on the feedback wehave seen our method shedding light on reducing the impactof critical failures.
vii.
d iscussion a. practical usage and perceived limitations indirect dependencies in this work we mainly considered direct dependencies which is caused by direct service in vocations.
the proposed approach does not explicitly considerindirect dependencies through transitivity of service invocationbecause the intensity of indirect dependencies can be easilyinferred from direct dependencies.
in practice the intensityof indirect dependencies can be inferred by a cascadingconduction mechanism that if aintensively depends on b andbintensively depends on cthenaintensively depends on c. the proposed approach also works well on dependenciescaused by circuit breakers as long as the circuit breakers worktransparently.
extension of service status in this paper we only derive three aspects of service invocations i.e.
number of invocations duration of invocations error of invocations.
we utilized them because they are part of the state of the art tracingsystem.
other aspects like the content of invocation responsescan also be important to determine status.
in practice cloudproviders can incorporate additional information to extend therepresentation of service status in their own implementationof aid.
limitations on asynchronous invocations although modern tracing mechanisms can keep track of asynchronousinvocations aid may suffer from inaccuracies when dealingwith asynchronous invocations.
this is because the max timedrift din algorithm is hard to estimate for asynchronous service invocations.
furthermore if the traces of synchronousand asynchronous invocations are mixed aid may not workwell since the time drift of synchronous and asynchronousinvocations usually differs a lot.
we leave this problem asfuture work.
b. threat to v alidity in this work we identified the following major threats to validity.
labeling accuracy in this paper we propose to measure the intensity of service dependency with aid.
to evaluate the practical usage of aid we conduct experiments on asimulated dataset and an industrial dataset.
as it is a newrelation between cloud services manual labeling is neededfor the evaluation.
the evaluation on the industrial datasetrequires engineers to manually inspect the dependencies andlabel the intensity of dependencies.
limited by the experienceof engineers the label may not be accurate.
thefast evolution of cloud services may also change their faulttolerance mechanism resulting in inaccurate labels.
however the engineers we invited have rich domain knowledge andare in charge of the architecture design of the cloud systemof huawei cloud.
they also discuss with each other whenthere are disagreements.
moreover the labeled dependenciesare the core cloud services in huawei cloud so the intensityof dependencies are stable during the data collection period.we believe the amount of inaccurate labels is small if exists .most importantly our method is unsupervised so inaccuratelabels will not affect the prediction results of the proposedmethod.
insufficiency of the simulation for the evaluation purpose we deploy an open source microservice benchmark tosimulate a real cloud system.
the benchmark only contains25 microservices which is far below the number of cloudmicroservices in a real cloud.
additionally the implementationof the open source benchmark did not fully consider thefault tolerance resulting in only one weak dependency in thesimulation.
hence the simulated dataset may not exhibit somecommon attributes of a real cloud system.
for example theproportion of strong dependency in the simulated dataset istwice the proportion of strong dependency in the industrialdataset.
however the insufficiency of the simulation willnot hinder the practical usefulness of aid in the real cloudsystem.
on the contrary as we show in section v theproposed method works better on the industrial dataset.
theexperimental results on the simulated dataset only confirm theinsufficiency of the simulation.
662viii.
r ela ted work a. cloud monitoring monitoring cloud services properly with low overhead is the key to provide reliable services.
distributed tracing as a means of monitoring distributed cloud services has beenwidely studied in the literature.
all the distributed tracingapproaches can be classified as intrusive tracing and non intrusive tracing.
intrusive tracing requires modification toapplication code either in run time or at compile time.
googleproposes dapper to help engineers understand systembehavior and reasoning about performance issues.
it reducesthe tracing overhead by sampling and restricting the instru mentation number.
x trace monitors and reconstructs thewhole request path from a client by modifying all the networkprotocols and embedding the tracing data to the packageheader.
non intrusive tracing approaches do not require code modification and usually have a lower overhead.
normally theseapproaches leverage information like the system runtime logsand the source code to reconstruct the real event traces.
zhaoet.
al.
propose lprof to reconstruct the execution flow ofdistributed systems using the runtime log of these systems.lprof conducts static analysis on the call graph of requestprocessing code of the system to attribute a log output to aclient request.
chow et.
al.
also leverage system runtimelogs to conduct performance monitoring and analysis.
theypropose ubertrace to reconstruct traces from the existing logs then use the mystery machine to construct a causal model and conduct analyses.
stitch uses pattern matching on logs toreconstruct the hierarchical relationship of events in a system.pensieve automatically reconstructs a chain of causallydependent events that leads to a system failure exploiting thelog files and system bytecode.
b. dependency mining automatically discovering service dependencies is critical to cloud system administration and maintenance.
there are two major types of dependency mining approaches i.e.
passivedependency mining and active ones.
passive dependency min ing generates service dependency based purely on the runtimelogs or kpis.
shah et.
al.
propose to use recurrentneural networks rnns to analyze and extract dependenciesin kpis and use the discovered dependencies to identify earlyindicators for a given performance metric analyze laggedand temporal dependencies and to improve forecast accuracy.eidefrawy et.
al.
use transfer entropy to passively minethe dependencies.
luo et.
al.
apply log parsing andbayesian decision theory to estimate the direction of depen dencies among services.
they employ time delay consistencyto reduce false dependencies.
zand et.
al.
construct aservice correlation graph based on network measures andextract dependencies using hypothesis testing.
they furthercompute an importance metric for network s componentsto facilitate administration.
cloudscout employs pearsonproduct moment correction coefficient over machine levelkpis such as tcp udp connection numbers and cpu uti lization to calculate the similarity between different services.the similarity measure is used to cluster different servicestogether and to conduct vm consolidation based on the serviceclusters.
unlike all these approaches that mostly use physicalmachine metrics to infer service dependencies our methodis designed for the emerging microservice architecture andutilizes the trace logs that directly record service invocations.
active dependency mining requires modification to services.
ma et.
al.
propose gma t which generates servicedependencies in the microservice architecture leveraging thereflection feature of java and visualizes the dependencies toengineers.
rippler extracts the dependencies by randomlyinjecting temporal perturbation patterns in request arrival tim ings for different services and investigates the propagation ofthe patterns.
wang et.
al.
constructs a service knowledgegraph using real time measures operational metadata andbusiness features.
they propose new metrics to measure thepopularity of services based on their dependencies.
novotnyet.
al.
focus on mining dependencies on the highlydynamic mobile networks.
they use local monitors to collectlocal views of dependencies and generate a global view ofdependency on demand.
ix.
c onclusion in this paper we first conduct a comprehensive empirical study on the maintenance of aws and huawei cloud.
weidentify the inefficiency in failure diagnosis and recoverywith the binary valued dependencies and define the intensityof dependency for the first time.
to facilitate cloud main tenance we propose aid the first approach to predict theintensity of dependencies between cloud microservices.
aidfirst generates a set of candidate dependency pairs from thespans.
aid then represents the status of each cloud servicewith a multivariate time series aggregated from the spans andcalculates the similarities between the statuses of the callerand callee of each candidate pair.
finally aid aggregate thesimilarities to produce a unified value as the intensity of thedependency.
for the evaluation we collect and manually labela new dataset from an open source microservice benchmarkand evaluate aid on it.
furthermore we evaluate aid usingthe data of huawei cloud and showcase the practical usageof aid.
both the evaluation results and case studies show theefficiency and effectiveness of aid.
in the future we plan toincorporate more information from the traces and other servicelogs for more accurate predictions.
a cknowledgment the work was supported by key area research and development program of guangdong province no.2020b010165002 and the research grants council ofthe hong kong special administrative region china cuhk .
663references microsoft microservices architecture style .
.
available guide architecture styles microservices s. p .
ma c. y .
fan y .
chuang w. t. lee s. j. lee and n. l. hsueh using service dependency graph to analyze and test microservices in2018 ieee 42nd annual computer software and applications confer ence compsac vol.
pp.
.
j. yin x. zhao y .
tang c. zhi z. chen and z. wu cloudscout a non intrusive approach to service dependency discovery ieee transactions on parallel and distributed systems vol.
no.
pp.
.
m. armbrust a. fox r. griffith a. d. joseph r. h. katz a. konwinski g. lee d. a. patterson a. rabkin i. stoica and m. zaharia above the clouds a berkeley view of cloud computing eecsdepartment university of california berkeley tech.
rep. ucb eecs feb .
r. defauw a. chigani and n. harris it resilience within aws cloud part ii architecture and patterns .
.
available d. l. oppenheimer and d. a. patterson architecture and dependability of large scale internet services ieee internet comput.
vol.
no.
pp.
.
m. villamizar o. garc es h. castro m. v erano l. salamanca r. casallas and s. gil evaluating the monolithic and the microservice archi tecture pattern to deploy web applications in the cloud in 10th computing colombian conference 10ccc .
ieee pp.
.
a. balalaie a. heydarnoori and p .
jamshidi microservices architecture enables devops migration to a cloud native architecture ieee softw.
vol.
no.
pp.
.
y .
wang g. li z. wang y .
kang y .
zhou h. zhang f. gao j. sun l. yang p .
lee z. xu p .
zhao b. qiao l. li x. zhang and q. lin fast outage analysis of large scale production clouds with servicecorrelation mining in 43rd ieee acm international conference on software engineering icse madrid spain may .ieee pp.
.
y .
gan y .
zhang k. hu d. cheng y .
he m. pancholi and c. delimitrou seer leveraging big data to navigate the complexityof performance debugging in cloud microservices in proceedings of the twenty f ourth international conference on architectural supportfor programming languages and operating systems asplos providence ri usa april .
acm pp.
.
p .
wang j. xu m. ma w. lin d. pan y .
wang and p .
chen cloudranger root cause identification for cloud native systems in18th ieee acm international symposium on cluster cloud and gridcomputing ccgrid washington dc usa may .ieee computer society pp.
.
p .
chen y .
qi p .
zheng and d. hou causeinfer automatic and distributed performance diagnosis with hierarchical causality graph inlarge distributed systems in ieee conference on computer communications infocom toronto canada april may2 .
ieee pp.
.
j. chen x. he q. lin y .
xu h. zhang d. hao f. gao z. xu y .
dang and d. zhang an empirical investigation of incident triage for onlineservice systems in proceedings of the 41st international conference on software engineering software engineering in practice icse seip montreal qc canada may .
ieee acm pp.
.
b. h. sigelman l. a. barroso m. burrows p .
stephenson m. plakal d. beaver s. jaspan and c. shanbhag dapper a large scaledistributed systems tracing infrastructure google inc. tech.
rep. .
.
available p .
barham r. isaacs r. mortier and d. narayanan magpie online modelling and performance aware systems in 9th workshop on hot topics in operating systems hotos ix .
usenix association may2003.
r. fonseca g. porter r. h. katz s. shenker and i. stoica x trace a pervasive network tracing framework in 4th symposium on networked systems design and implementation nsdi april cambridge massachusetts usa proceedings.
usenix .
opentracing the opentracing semantic specification .
.
available x. zhou x. peng t. xie j. sun c. ji w. li and d. ding fault analysis and debugging of microservice systems industrial survey benchmark system and empirical study ieee trans.
software eng.
vol.
no.
pp.
.
a. fakhrazari and h. v akilzadian a survey on time series data mining inieee international conference on electro information technology eit lincoln ne usa may .
ieee pp.
.
h. sakoe and s. chiba dynamic programming algorithm optimization for spoken word recognition ieee transactions on acoustics speech and signal processing vol.
no.
pp.
.
d. j. berndt and j. clifford using dynamic time warping to find patterns in time series in knowledge discovery in databases papers from the aaai workshop seattle washington usa july .technical report ws .
aaai press pp.
.
v .
niennattrakul and c. a. ratanamahatana on clustering multimedia time series data using k means and dynamic time warping in international conference on multimedia and ubiquitous engineering mue april seoul korea.
ieee computer society pp.
.
a. mueen h. hamooni and t. estrada time series join on subsequence correlation in ieee international conference on data mining icdm shenzhen china december .
ieee computer society pp.
.
z. chen y .
kang l. li x. zhang h. zhang h. xu y .
zhou l. yang j. sun z. xu y .
dang f. gao p .
zhao b. qiao q. lin d. zhang and m. r. lyu towards intelligent incident management why we needit and how we make it in esec fse 28th acm joint european software engineering conference and symposium on the f oundationsof software engineering virtual event usa november .acm pp.
.
j. g. lou q. fu y .
wang and j. li mining dependency in distributed systems through unstructured logs analysis acm sigops operating systems review vol.
no.
pp.
.
a. zand g. vigna r. kemmerer and c. kruegel rippler delay injection for service dependency detection in ieee infocom 2014ieee conference on computer communications.
ieee pp.
.
a. b. m. b. alam a. haque and m. zulkernine crem a cloud reliability evaluation model in ieee global communications conference globecom abu dhabi united arab emirates december9 .
ieee pp.
.
opentracing opentracing spring cloud .
.
available g. aceto a. botta w. de donato and a. pescap e cloud monitoring a survey computer networks vol.
no.
pp.
.
e. j. keogh exact indexing of dynamic time warping in proceedings of 28th international conference on v ery large data bases vldb hong kong august .
morgan kaufmann pp.
.
x. zhao y .
zhang d. lion m. f. ullah y .
luo d. y uan and m. stumm lprof a non intrusive request flow profiler for distributedsystems in 11th usenix symposium on operating systems design and implementation osdi .
usenix association oct. pp.
.
m. chow d. meisner j. flinn d. peek and t. f. wenisch the mystery machine end to end performance analysis of large scale internetservices in 11th usenix symposium on operating systems design and implementation osdi .
usenix association oct. pp.
.
y .
zhang s. makarov x. ren d. lion and d. y uan pensieve nonintrusive failure reproduction for distributed systems using the eventchaining approach in proceedings of the 26th symposium on operating systems principles pp.
.
s. y .
shah z. y uan s. lu and p .
zerfos dependency analysis of cloud applications for performance monitoring using recurrent neuralnetworks in ieee international conference on big data big data .
ieee pp.
.
k. eidefrawy t. kim and p .
sylla automated inference of dependencies of network services and applications via transfer entropy in ieee 40th annual computer software and applications conference compsac vol.
.
ieee pp.
.
a. zand a. houmansadr g. vigna r. kemmerer and c. kruegel know your achilles heel automatic detection of network critical services in proceedings of the 31st annual computer security applications conference pp.
.
h. wang c. shah p .
sathaye a. nahata and s. katariya service application knowledge graph and dependency system in 34th ieee acm international conference on automated software engineer ing workshop asew .
ieee pp.
.
p .
novotny b. j. ko and a. l. wolf on demand discovery of software service dependencies in manets ieee transactions on network and service management vol.
no.
pp.
.