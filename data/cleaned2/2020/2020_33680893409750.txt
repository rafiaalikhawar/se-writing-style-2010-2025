deepsearch a simple and effective blackbox attack for deep neural networks fuyuan zhang mpi sws germany fuyuan mpi sws.orgsankalan pal chowdhury mpi sws germany sankalan mpi sws.orgmaria christakis mpi sws germany maria mpi sws.org abstract although deep neural networks have been very successful in imageclassification tasks they are prone to adversarial attacks.
to generate adversarial inputs there has emerged a wide variety of techniques such as black and whitebox attacks for neural networks.
in this paper we present deepsearch a novel fuzzing based queryefficient blackbox attack for image classifiers.
despite its simplicity deepsearch is shown to be more effective in finding adversarial inputs than state of the art blackbox approaches.
deepsearch is additionally able to generate the most subtle adversarial inputs in comparison to these approaches.
ccs concepts computing methodologies neural networks software and its engineering software testing and debugging.
keywords adversarial attack deep neural networks blackbox fuzzing acm reference format fuyuan zhang sankalan pal chowdhury and maria christakis.
.
deepsearch a simple and effective blackbox attack for deep neural networks.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
introduction deep neural networks have been impressively successful in pattern recognition and image classification .
however it is intriguing that deep neural networks are extremely vulnerable to adversarial attacks .
in fact even very subtle perturbations of a correctly classified image imperceptible to the human eye may cause a deep neural network to change its prediction.
this poses serious security risks to deploying deep neural networks in safety critical applications.
various adversarial attacks have been developed to evaluate the vulnerability of neural networks against adversarial perturbations.
early work on generating adversarial examples has focused on whitebox attacks .
in the whitebox setting attackers have full access to the network under evaluation which esec fse november virtual event usa copyright held by the owner author s .
acm isbn .
them to calculate gradients of the network.
many gradientbased attacks have been shown to be highly effective.
however in several real world scenarios having complete access to network parameters is not realistic.
this has motivated the development of blackbox adversarial attacks.
in the blackbox setting attackers assume no knowledge about the network structure or its parameters and may only query the target network for its prediction when given particular inputs.
one important metric to measure the efficiency of blackbox attacks is the number of queries needed because queries are essentially time and monetary costs for attackers e.g.
each query to an online commercial machine learning service costs money.
evaluating the robustness of deep neural networks in a query limited blackbox setting is already standard.
gradient estimation based blackbox attacks although effective require a huge number of queries which makes generating an attack too costly.
various state of theart blackbox attacks e.g.
can already achieve successful attacks with low number of queries.
however constructing query efficient blackbox attacks is still open and challenging.
in this paper we develop a blackbox fuzzing based technique for evaluating adversarial robustness of neural networks.
the two key challenges of applying fuzzing here are to maintain a high attack success rate and to require a low number of queries.
in many cases without careful guidance while searching a naive fuzzing approach e.g.
random fuzzing is not able find adversarial examples even after a huge number of queries.
to improve attack success rate we introduce carefully designed feedback to guide our search so that images are efficiently fuzzed toward the decision boundaries.
to reduce the number of queries we adapt hierarchical grouping to our setting so that multiple dimensions can be fuzzed simultaneously which dramatically reduces query numbers.
furthermore a refinement step which can be viewed as a backward search step for fuzzing can effectively reduce distortion of adversarial examples.
therefore we extend fuzz testing and show how to apply it on neural networks in a blackbox setting.
our approach.
inspired by the linear explanation of adversarial examples we develop deepsearch a simple yet effective query efficient blackbox attack which is based on feedbackdirected fuzzing.
deepsearch targets deep neural networks for image classification.
our attack is constrained by the l distance and only queries the attacked network for its prediction scores on perturbed inputs.
the design of our approach is based on the following three aspects feedback directed fuzzing starting from a correctly classified image deepsearch strategically mutates its pixels to values that are more likely to lead to an adversarial input.
the 800this work is licensed under a creative commons attribution international .
license.
esec fse november virtual event usa fuyuan zhang sankalan pal chowdhury and maria christakis fuzzing process continues until it either finds an adversarial input or it reaches the query limit.
iterative refinement once an adversarial input is found our approach starts a refinement step to reduce the l distance of this input.
the iteration of refinement continues until either the query limit is reached or some termination criterion is met.
our evaluation shows that iterative refinement is able to find subtle adversarial inputs generated by only slightly perturbing pixels in the original image.
query reduction by utilizing the spatial regularities in input images deepsearch adapts an existing hierarchicalgrouping strategy to our setting and dramatically reduces the number of queries for constructing successful attacks.
the query reduction step significantly improves the efficiency of our fuzzing and refinement process.
we evaluate deepsearch against four state of the art blackbox attacks in a query limited setting where attackers have only a limited query budget to construct attacks.
for our evaluation we use three popular datasets namely svhn cifar and imagenet .
for svhn and cifar we further attack neural networks with state of the art defenses based on adversarial training .
our experimental results show that deepsearch is the most effective in attacking both defended and undefended neural networks.
moreover it outperforms the other four attacks.
although it is important to develop defense techniques against blackbox adversarial attacks it is not the focus of this paper and we leave it for future work.
contributions.
we make the following contributions we present a simple yet very effective fuzzing based blackbox attack for deep neural networks.
we perform an extensive evaluation demonstrating that deepsearch is more effective in finding adversarial examples than state of the art blackbox approaches.
we show that the refinement step in our approach gives deepsearch the advantage of finding the most subtle adversarial examples in comparison to related approaches.
we show that the hierarchical grouping strategy is effective for query reduction in our setting.
outline.
the next section briefly introduces background.
in sect.
we present deepsearch for binary classifiers that is networks that classify inputs into two classes.
sect.
generalizes the technique to multiclass classifiers which classify inputs into multiple classes.
in sect.
we extend our technique with iterative refinement such that the generated adversarial examples are more subtle.
we adapt hierarchical grouping for query reduction in sect.
.
we present our experimental evaluation in sect.
discuss related work in sect.
and conclude in sect.
.
background in this section we introduce some notation and terminology.
let rn be then dimensional vector space for input images.
we represent images as column vectors x x1 ... xn t wherexi r i n is theith coordinate of x. we also write x i to denote the ith coordinatexi i.e.
x i xi and each such coordinate represents an image pixel.
now let cm l1 ... lm be a set of labels for mclasses where liis the label for the ith class i m .
a deep neural network that classifies images from rnintomclasses incm is essentially a function n rn cm.
for an input x rn n x is the label that the network assigns to x. assume that input xis correctly classified and x is generated by applying subtle perturbations to x. these perturbations are subtle when the distance between xandx inrnis sufficiently small according to a distance metric.
when this is so and n x n x we say that x is an adversarial example .
in other words the network is tricked into classifying x into a different class than x even though they are very similar.
in this paper we use the l distance metric.
the l distance between xandx is defined as the maximum of their differences along any coordinate dimension i i n x x l max i xi x i ford r we writeb x d to denote the set of images within distancedfrom x i.e.
b x d x x x l d which is ann dimensional cube.
based on the above a deep neural network nislocally robust for a correctly classified input xwith respect to distancedif it assigns the same label to all images in b x d .
we mention here that numerous attacks are optimized for one distance metric e.g.
just like ours.
although there exist other distance metrics e.g.
l0andl2 and extending attacks from one metric to another is possible developing an attack that performs best in all distance metrics is not realistic.
many state of the art attacks are the most effective in one metric but their extension to other metrics performs worse than attacks specifically designed for that metric.
our paper focuses on a queryefficientl attack as in and our technique outperforms the state of the art in this setting.
query limited blackbox threat model.
we assume that attackers have no knowledge of the target network and can only query the network for its prediction scores e.g.
logits or class probabilities.
moreover we assume that attackers have a query budget which can be viewed as time or monetary limits in real world settings.
thus the blackbox attack we consider in this paper can be described as follows.
given an input x distanced and query budget l an attacker aims to find an adversarial example x inb x d by making at mostlqueries to the neural network.
fuzzing binary classifiers in this section we present the technical details of how deepsearch fuzzes linear and non linear binary classifiers.
we first introduce our approach for linear binary classifiers which serves as the mathematical foundation.
then we generalize our approach to non linear binary classifiers through iterative linear approximations.
.
linear binary classifiers abinary classifier classifies inputs into two classes denoted with labelsc2 l1 l2 according to the definition below.
definition binary classifier .given a classification function f rn r abinary classifiernf rn c2is defined as follows nf x l1 iff x l2 iff x 801deepsearch a simple and effective blackbox attack for deep neural networks esec fse november virtual event usa if functionfis linear thennfis alinear binary classifier otherwise it is non linear.
the set of valuesdf x f x constitute the decision boundary ofnf which classifies the domain rninto the two classes inc2.
as an example consider fig.
1a showing a linear binary classifier nf r2 c2.
observe that input x0is classified in l1whereas x 0is inl2.
note that the decision boundary of a linear classifier nf rn c2is a hyperplane it is therefore a straight line in fig.
1a.
now assume that x0is correctly classified and that the dashdotted square represents b x0 d .
then x 0is adversarial because nf x0 nf x which is equivalent to f x0 f x .
example.
we give an intuition on how deepsearch handles linear binary classifiers using the example of fig.
1a.
recall that x0is a correctly classified input for which f x0 .
to find an adversarial example deepsearch fuzzes x0with the goal of generating a new input x 0such thatf x .
fuzzing is performed as follows.
input x0has two coordinates xhandxv for the horizontal and vertical dimensions.
deepsearch independently mutates each of these coordinates to the minimum and maximum values that are possible within b x0 d with the intention of finding the minimum value of finb x0 d .
for instance when mutating xh we obtain inputs x0 andx0 in the figure.
values lhanduhare respectively the minimum and maximum that xhmay take and x0 denotes substituting xh withlh similarly for x0 .
we then evaluate f x0 andf x0 and forxh we select the value lhoruh that causes function ftodecrease .
this is because in our example an adversarial input x 0must make the value of fnegative.
let us assume that f x0 f x0 lh xh we thus select uhfor coordinatexh.
deepsearch mutates coordinate xvin a similar way.
it evaluates f x0 andf x0 and selects the value that causes f to decrease.
let us assume that f x0 f x0 we thus selectuvforxv.
next we generate input x 0by substituting each coordinate in x0with the boundary value that was previously selected.
in other words x x0 and since f x deepsearch has generated an adversarial example.
note that f x is actually the minimum value of finb x0 d .
deepsearch for linear binary classifiers.
we now formalize how deepsearch treats linear binary classifiers.
consider a linear classification function f x wtx b n i 1wixi b where wt w1 ... wn andb r. note thatfis monotonic with respect to all of its variables x1 ... xn.
for instance if wi thenfis monotonically increasing in xi.
recall thatb x d denotes the set of inputs within distance d rof an input x.b x d may be represented by an n dimensional cubei i1 ... in whereii is a closed interval bounded byli ui rwithli uifor1 i n. intuitively value li resp.ui is the lower resp.
upper bound on the ith dimension of x. an input x is avertex ofiif each of its coordinates x i is an endpoint of iifor1 i n i.e.
x i uiorli i n .
due to the monotonicity of f the maximum and minimum values offonimay be easily calculated by applying fto vertices ofi.
for x0x0 dff x f x b x0 d lh xv xh lv uh xv xh uv a linear classifier x0x2 dff x f x b x0 d x1 b non linear classifier figure deepsearch for binary classifiers.
example consider a one dimensional linear function f x 2x wherex that is 1and1are the lower and upper bounds forx.
sincef f we get a maximum value of fatx and a minimum value of fatx .n dimensional linear functions can be treated similarly.
we write f i for the values of foni i.e.
f i f x x i and have the following theorem whose proof can be found in .
theorem .given a linear classification function f x wtx b where wt w1 ... wn andb r ann dimensional cube i i1 ... in whereii for1 i n and an input x i we have minf i f x where x i li resp.
x i ui iff x f x resp.f x f x for i n maxf i f x where x i ui resp.
x i li iff x f x resp.f x f x for i n according to the above theorem we can precisely calculate the minimum and maximum values of fin anyn dimensional cube.
in particular assume a correctly classified input xfor whichf x .
for each dimension i i n ofx we first construct inputs x andx .
we then compare the values of f x andf x .
to generate a new input x we select the value of itsith coordinate as follows x i li iff x f x ui iff x f x as shown here selecting a value for x i requires evaluating functionftwice i.e.
f x andf x .
therefore for n dimensions fmust be evaluated 2ntimes.
in practice however due to the monotonicity of f evaluating it only once per dimension is sufficient.
for instance if f x already decreases resp.
increases the value of fin comparison to f x there is no need to evaluatef x .
valueui resp.li should be selected for the ith coordinate.
hence the minimum value of fcan be computed by evaluating the function exactly ntimes.
if for the newly generated input x the sign of fbecomes negative x constitutes an adversarial example.
we treat the case where f x 0for a correctly classified input xanalogously.
deepsearch aims to generate a new input x such that the sign of fbecomes positive.
we are therefore selecting coordinate values that cause ftoincrease .
802esec fse november virtual event usa fuyuan zhang sankalan pal chowdhury and maria christakis algorithm deepsearch for binary classifiers.
input input x rn initial input xinit b x d functionf rn r distanced r output x b x d 1function approxmax x f i1 ... i n is x ... foreach i ndo iff x f x then x x ui x i else x x li x i return x 10function approxmin x f i1 ... i n is x ... foreach i ndo iff x f x then x x li x i else x x ui x i return x 19function ds binary x xinit f d is construct intervals i1 ... i n such thatb x d i1 ... in initialize x0 xinitandk iff x0 0then repeat xk approxmin xk f i1 ... i n k k untilnf x nf xk ork maxnum else repeat xk approxmax xk f i1 ... i n k k untilnf x nf xk ork maxnum return x k .
non linear binary classifiers we generalize our technique to non linear binary classifiers.
in this setting deepsearch iteratively approximates the minimum and maximum values of finb x d .
example.
as an example consider the non linear classification functionfshown in fig.
1b.
since fis non linear the decision boundarydfof the binary classifier is a curve.
starting from correctly classified input x0 deepsearch treatsf as linear withinb x0 d and generates x1 exactly as it would for a linear binary classifier.
to explain how x1is derived we refer to the points in fig.
1a.
suppose we first mutate the horizontal dimension ofx0 using the lower and upper bounds and find that f lh xv f uh xv .
to increase the chances of crossing the decision boundary we choose the bound for the horizontal dimension of x0that gives us the lower value of f i.e.
we select uhfor horizontal coordinate xh.
then we mutate the vertical dimension of x0and find that f xh uv f xh lv .
this means that we select lvfor verticalcoordinatexv.
hence we derive x1 uh lv .
observe that input x1is not adversarial.
unlike for a linear binary classifier however where the minimum value of finb x0 d is precisely computed the non linear case is handled by iteratively approximating the minimum.
in particular after generating x1 deepsearch iterates starting from x1 while again treating fas linear inb x0 d .
as a result our technique generates input x2 which is adversarial.
the reason we can treat non linear binary classifiers as linear ones is that perturbations of pixels are only allowed in a very small n dimensional cube constrained by the l distance.
within such a small space we can effectively approximate non linear functions using iterative linear approximations.
deepsearch for non linear binary classifiers.
alg.
shows deepsearch for binary classifiers.
it uses iterative approximations to search for adversarial examples.
note that our technique is blackbox and consequently it cannot differentiate between linear and nonlinear classifiers.
alg.
is therefore the general algorithm that deepsearch applies to fuzz any binary classifier.
the main function in alg.
is ds binary .
input xinitis the input from which we start the first iteration e.g.
it corresponds to x0 in fig.
.
input xis used to compute b x d and for now assume thatxis equal to xinit.
we will discuss why xis needed in sect.
.
in addition to these inputs the algorithm also takes a classification functionfand the distance d. function ds binary assigns xinittox0and constructs nintervals i1 ... into representb x0 d lines .
then based on the sign off x0 our algorithm iteratively approximates the minimum lines or the maximum lines value of finb x0 d .
ds binary terminates when either an adversarial example is found or it has reached maxnum iterations.
to find adversarial examples inkiterations we evaluate fat most 2n n k times.
approxmin andapproxmax implement thm.
to calculate the minimum and maximum values of function fin then dimensional cubei1 ... in.
whenfis linear calling these functions on any input x i1 ... indoes not affect the computation.
in other words the minimum and maximum values are precisely computed for any x. whenfis non linear it is still assumed to be linear within the n dimensional cube.
given that the size of the cube is designed to be small this assumption does not introduce too much imprecision.
as a consequence of this assumption however different inputs in then dimensional cube lead to computing different minimum and maximum values of f. for instance in fig.
1b calling approxmin onx0returns x1 while calling it on x1returns x2.
fuzzing multiclass classifiers in this section we extend our technique for blackbox fuzzing of binary classifiers to multiclass classifiers.
.
linear multiclass classifiers amulticlass classifier classifies inputs in mclasses according to the following definition.
definition multiclass classifier .for classification function f rn rm which returns mvalues each corresponding to one 803deepsearch a simple and effective blackbox attack for deep neural networks esec fse november virtual event usa class incm l1 ... lm amulticlass classifier nf rn cmis defined as nf x lj iffj arg maxifi x wherefi rn rdenotes the function derived by evaluating f for theith class i.e.
f x f1 x ... fm x t. in other words a multiclass classifier nfclassifies an input xin ljiffj x evaluates to the largest value in comparison to all other functionsfi.
functionfof a multiclass classifier nfmay be decomposed into multiple binary classifiers such that the original classifier can be reconstructed from the binary ones.
first to decompose a multiclass classifier into binary classifiers for any pair of classes liandlj i j m we define a classification function gij rn r asgij x fi x fj x .
we then construct a binary classifier ngij rn li lj as follows ngij x li ifgij x lj ifgij x as usual the set of values dgij x fi x fj x constitutes the pairwise decision boundary of binary classifier ngij which classifies the domain rninto the two classes li lj .
as an example consider fig.
2a depicting a multiclass classifier nf r2 c3 wherefis linear and c3 l1 l2 l3 .
assume thatnfcorrectly classifies input xinl2.
based on the above linear binary classifiers ng21andng23also classify xinl2 i.e.
g21 x 0andg23 x .
second a multiclass classifier may be composed from multiple binary classifiers as follows.
an input xis classified in class liby multiclass classifier nfif and only if it is classified in liby allm binary classifiersngijfor1 j m i j whereli cmand gij x fi x fj x nf x li iff j m i j ngij x li for instance in fig.
2a if both ng21andng23classify input xin classl2 then the multiclass classifier also classifies it in l2.
based on the above a multiclass classifier has an adversarial input if and only if this input is also adversarial for a constituent binary classifier.
corollary .letnfbe a multiclass classifier and x rna correctly classified input where nf x liandli cm.
there exists an adversarial example x b x d fornf whered r if and only if x is an adversarial example for a binary classifier ngij j m i j wheregij x fi x fj x nf x li iff j m i j ngij x li example.
this corollary is crucial in generalizing our technique to multiclass classifiers.
assume a correctly classified input x for whichnf x li.
according to the above corollary the robustness ofnfinb x d reduces to the robustness of all m 1binary classifiers ngij j m i j inb x d .
we therefore use deepsearch for binary classifiers to test each binary classifier in this set.
if there exists an adversarial input x for one of these classifiers i.e.
ngij x lifor somej then x is also an adversarial input fornf i.e.
nf x li.
let us consider again the example of fig.
2a.
recall that multiclass classifiernfcorrectly classifies input xin classl2 and so do binary xx3 dg21 b x d x1 dg23 a linear classifier x0x2 dg21 b x0 d x1 dg23 b non linear classifier figure deepsearch for multiclass classifiers.
classifiersng21andng23 i.e.
g21 x 0andg23 x .
as a result deepsearch tries to generate inputs that decrease the value of each of these functions in b x d in order to find adversarial examples.
functiong21evaluates to its minimum value in b x d for input x and function g23for input x .
observe that x 3is an adversarial example forng23 and thus also fornf whereas x 1is not.
deepsearch for linear multiclass classifiers.
let us assume a linear classification function f x wtx b where wt wt ... wtm t wi rn i m and b b1 ... bm t rm.
then fi which denotes the function derived by evaluating ffor theith class is of the form fi x wt ix bifor1 i m. for any pair of class labels liandlj functiongijis defined as gij x fi x fj x wt i wt j x bi bj .
hence gijis also linear andngijis a linear binary classifier.
assume that classifier nfcorrectly classifies input x rninli nf x li li cm .
according to cor.
the robustness of nfin b x d d r reduces to the robustness of each binary classifier ngij j m i j inb x d .
to find an adversarial example for a binary classifier ngijinb x d deepsearch must generate an input x b x d such thatgij x .
recall that by definition gij x .
since all functions gij j m i j are linear we easily find their minimum values in b x d as follows.
leti1 ... inbe intervals such that b x d i1 ... in where ik for1 k n. as in sect.
.
for each dimension k deepsearch evaluates function ftwice to compare the values off x andf x .
to generate a new input x jfor which function gijevaluates to its minimum value we select its kth coordinate as follows x j k lk ifgij x gij x uk ifgij x gij x .
note that although we calculate the minimum value of m linear functions we still evaluate f2ntimes.
this is because a functiongijis defined as gij x fi x fj x wherefi x and fj x are the values of f x for theith andjth classes respectively.
if the sign of gij x j becomes negative for some j then deepsearch has found an adversarial example for nfinb x d .
.
non linear multiclass classifiers we now extend our technique to non linear multiclass classifiers.
analogously to sect.
.
deepsearch iteratively approximates the minimum values of functions gijinb x d .
804esec fse november virtual event usa fuyuan zhang sankalan pal chowdhury and maria christakis algorithm deepsearch for multiclass classifiers.
input input x rn initial input xinit b x d functionf rn rm distanced r output x b x d 1function ds multiclass x xinit f d is construct intervals i1 ... i n such thatb x d i1 ... in computenf x and assume thatnf x li initialize x0 xinitandk define gij gij x fi x fj x j m i j repeat 7r arg min jgij xk xk approxmin xk gir i1 ... i n 9k k untilnf x nf xk ork maxnum return x k example.
as an example consider fig.
2b depicting a multiclass classifiernf r2 c3 wherefis non linear and c3 l1 l2 l3 .
assume thatnfclassifies input x0in classl2 and thus so do nonlinear binary classifiers ng21andng23.
let us also assume that g21 x0 g23 x0 .
sinceg21evaluates to a smaller value than g23for input x0 we consider it more likely to have an adversarial example.
in other words we first approximate the minimum value of g21because it is closer to becoming negative for the initial input.
deepsearch treatsg21as linear withinb x0 d and generates x1.
observe that input x1is not adversarial.
now assume that g21 x1 g23 x1 .
as a result deepsearch tries to find the minimum of function g23inb x0 d also by treating it as linear.
it generates input x2 which is an adversarial example for classifiersng23andnf.
deepsearch for non linear multiclass classifiers.
alg.
is the general deepsearch algorithm for multiclass classifiers.
the inputs are the same as for alg.
.
for now assume that xis equal toxinit.
again the algorithm executes at most maxnum iterations and it terminates as soon as an adversarial example is found.
function ds multiclass assigns xinittox0and constructs n intervalsi1 ... into representb x0 d .
it also computes the class labelliofx and defines functions gij j m i j lines .
the rest of the algorithm uses approxmin from alg.
to iteratively approximate the minimum of one function gijper iteration which is selected on line such that its value for input xkis smaller in comparison to all other constituent binary classification functions.
intuitively gijcorresponds to the binary classifier that is most likely to have an adversarial example near xk.
this heuristic allows our algorithm to find an adversarial example faster than having to generate an input xk 1for allm 1functionsgijper iteration.
to find an adversarial example in kiterations we need at most 2n n k queries for the value of f. an alternative objective function.
in each iteration of alg.
we construct a different objective function gijand approximate its minimum value.
an alternative choice of an objective function is fi itself.
in multiclass classification decreasing the value of fiamounts to decreasing the score value of the ith class which implicitly increases the score values of other classes.we refer to the algorithm derived by substituting lines of alg.
with the following assignment as alg.
xk approxmin xk fi i1 ... in it uses approxmin to iteratively approximate the minimum value offi.
we find it very effective in our experiments.
iterative refinement the closer the adversarial examples are to a correctly classified input the more subtle they are.
such adversarial examples are said to have a low distortion rate.
in this section we extend deepsearch with an iterative refinement approach for finding subtle adversarial examples.
on a high level given an input xand a distance d for which we have already found an adversarial example x in region b x d deepsearch iteratively reduces distance das long as the smaller region still contains an adversarial example.
if none is found the distance is not reduced further.
leti i1 ... inbe ann dimensional cube where ii is a closed interval bounded by li ui rwithli uifor1 i n. for an input xwith anith coordinate x i li ui for1 i n we define a projection operator proj that maps xto a vertex ofias follows proj i x i ui ifx i ui li ifx i li where proj i x i denotes the ith coordinate of proj i x .
as an example consider fig.
3a showing a linear multiclass classifier.
input x2is a projection of x1.
using this operator the minimum and maximum values of a linear classification function fmay also be projected on i and we have the following theorem whose proof is available in .
theorem .letf x wtx bbe a linear classification function andi1 i2twon dimensional cubes such that i1 i2.
assuming thatxis a vertex ofi2 we have if minf i2 f x then minf i1 f proj i1 x if maxf i2 f x then maxf i1 f proj i1 x in fig.
3a assume that input x0is correctly classified in class l2.
then in regionb x0 d1 functiong23obtains its minimum value for input x1.
when projecting x1to vertex x2ofb x0 d2 notice thatg23evaluates to its minimum for input x2in this smaller region.
example.
fig.
shows two multiclass classifiers nf r2 c3 wherec3 l1 l2 l3 .
in fig.
3a function fis linear whereas in fig.
3b it is non linear.
for correctly classified input x0 we assume thatnf x0 l2 and thus ng21 x0 l2andng23 x0 l2.
in both subfigures assume that x1is an adversarial example found by ds multiclass see alg.
inb x0 d1 .
once such an example is found our technique with refinement uses bisect search to find the smallest distance d such that the projection of x1on b x0 d is still adversarial.
in fig.
this distance is d2 and input x2constitutes the projection of x1onb x0 d2 .
so x2is closer to x0 which means that it has a lower distortion rate than x1.
in fact since we are using bisect search to determine distance d2 x2is the closest adversarial input to x0that may be generated by projecting x1on smaller regions.
805deepsearch a simple and effective blackbox attack for deep neural networks esec fse november virtual event usa dg21x1 b x0 d1 x0x3 b x0 d3 x2 b x0 d2 x4 dg23 a linear classifier x2 b x0 d1 x1 x0 x3 b x0 d2 b x0 d3 x4 dg23dg21 b non linear classifier figure deepsearch with iterative refinement.
however in region b x0 d2 there may be other vertices that are adversarial and get us even closer to x0with projection.
to find such examples we apply ds multiclass again this time starting from input x2and searching in region b x0 d2 .
as a result we generate adversarial input x3in the subfigures.
now by projecting x3to the smallest possible region around x0 we compute x4 which is the adversarial example with the lowest distortion rate so far.
assume that applying ds multiclass for a third time starting from x4and searching inb x0 d3 does not generate any other adversarial examples.
in this case our technique returns x4.
deepsearch with iterative refinement.
alg.
describes our technique with iterative refinement.
each iteration consists of a refinement and a search step which we explain next.
the refinement step lines first calculates the l distance dbetween xandx .
in fig.
input xof the algorithm is x0 and x isx1.
so x is an adversarial input that was generated by our technique and consequently a vertex of b x d .
on line we use bisect search to find the minimum distance d such that the input derived by projecting x onb x d is still adversarial.
in fig.
this is distance d2 and proj b x d x of the algorithm corresponds to adversarial input x2in the figure.
note that this refinement is possible because of thm.
which guarantees that a linear function fevaluates to its minimum in b x d for the input derived with projection.
when fis nonlinear it might not evaluate to its minimum for adversarial input proj b x d x .
however it is still the case that this projected input is closer to x i.e.
x proj b x d x l x x l and thus has a lower distortion rate.
after selecting an input from which to start the search line the search step lines calls function ds binary alg.
ords multiclass alg.
depending on whether fis a binary classification function.
the goal is to search for another adversarial example other than proj b x d x in regionb x d .
in fig.
an adversarial input found by this step when starting the search from x2 isx3 which is also a vertex of b x0 d2 .
on lines we essentially check whether the search step was successful in finding another adversarial input x .
however ds binary andds multiclass might not return an adversarial example.
if they do like input x3in fig.
the algorithm iterates line .
if not like when starting the search from input x4in the figure which is a projection of x3onb x0 d3 then we return the projected input and terminate.algorithm deepsearch with iterative refinement.
input input x rn adversarial input x b x d d r functionf rn rm output an adversarial input x b x d d d 1function ds refinement x x f is repeat 3d x x l apply bisect search to find the smallest distance d d such that input proj b x d x is an adversarial example.
choose an xnew b x d from which to start a new search e.g.
xnew proj b x d x .
iffis binary then x ds binary x xnew f d else x ds multiclass x xnew f d if x is an adversarial example then x x else x proj b x d x until x is not an adversarial example return x andd hierarchical grouping for ann dimensional input x our technique makes at least nqueries per iteration.
for high dimensional inputs it could cost a significantly large number of queries to perform even one iteration of our attack.
one basic strategy for query reduction is to divide pixels of an input image into different groups and mutate all pixels in a group to the same direction e.g.
all pixels in a group are moved to their upper bounds.
thus we only need one query for all pixels in the same group.
to exploit spatial regularities in images for query efficiency we adapt hierarchical grouping to our setting.
deepsearch with hierarchical grouping.
alg.
summarizes our technique with hierarchical grouping which consists of the following three main steps.
initial grouping line for ann dimensional input image x we first divide the ndimensions into n k2 setsg1 ... g n k2 where each set gi i n k2 contains indices corresponding to k kneighboring pixels in x. this amounts to dividing the original image xinto n k2 square blocks.
the definition of initial group ... n k is omitted due to space limitations.
fuzzing line we extend deepsearch to handle groups of pixels and write deepsearch x x f d g to mean such an extension.
for each set gi g our technique mutates all coordinates that correspond to indices in the set toward the same direction at the same time.
hence deepsearch only compares two values per set namely f andf wherei1 ... il giandl gi .
group splitting line if the current partition of the image is still too coarse for deepsearch to find adversarial examples we perform deepsearch in finer granularity.
we further divide each set giintom msubsetsgi ... gi m m 806esec fse november virtual event usa fuyuan zhang sankalan pal chowdhury and maria christakis algorithm deepsearch with hierarchical grouping.
input input x rn initial input xinit b x d initial group size k parametermfor group splitting function f rn rm distanced r query budget l output x b x d 1function divide group g m is foreachgi gdo dividegiintom msubset gi ... g i m m 4g g gi ... g i m m returng 7function ds hierarchy x xinit f k m is 8g initial group ... n k andx xinit repeat x deepsearch x x f d g if1 k mthen 12g divide group g m k k m untilnf x nf x orreached query budget l return x where each set gi j j m m contains indices corresponding to k m k mneighboring pixels in x. after splitting all sets the total number of sets is multiplied by m m. this results in a more fine grained partition of input x. we then go back to step .
in the query limited setting we use single step deepsearch on line10 i.e.
we fix maxnum to in alg.
and alg.
and choose xinit to be a vertex ofb x d to avoid unnecessary queries.
hence when there are n k2 sets ing the total number of queries per iteration in alg.
reduces to n k2 .
experimental evaluation we evaluate deepsearch by using it to test the robustness of deep neural networks trained for popular datasets.
we also compare its effectiveness with state of the art blackbox attacks.
our experiments are designed around the following research questions rq1 isdeepsearch effective in finding adversarial examples?
rq2 isdeepsearch effective in finding adversarial examples with low distortion rates?
rq3 isdeepsearch a query efficient blackbox attack?
rq4 is the hierarchical grouping of deepsearch effective in improving query efficiency?
we make our implementation open source1.
our experimental data including detected adversarial examples are also available via the provided link.
.
evaluation setup datasets and network models.
we evaluate our approach on deep neural networks trained for three well known datasets namely svhn cropped digits cifar and imagenet .
for dataset we randomly selected correctly classified images from the test set on which to perform blackbox attacks.
for svhn and cifar we attack two wide resnet w networks where one is naturally trained without defense and the other is adversarially trained with a state of the art defense .
for svhn the undefended network we trained has .
test accuracy and the adversarially trained network has .
test accuracy.
during adversarial training we used the pgd attack that can perturb each pixel by at most 8on the 255pixel scale to generate adversarial examples.
for cifar we trained an undefended network with .
test accuracy.
the defended network we attack is the pretrained network provided in madry s challenge2.
for imagenet we attack a pretrained inception v3 network which is undefended.
defenses for imagenet networks are also important and we would have attacked defended imagenet networks if they were publicly available.
in this work we did not attack such networks using the defense in for the following reasons.
first there are no publicly available imagenet networks that use the defense in .
second none of the state of the art attacks that we used for comparison in our paper i.e.
have been evaluated on defended networks for this dataset.
therefore we did not compare deepsearch with these attacks on such networks.
third due to the extremely high computational cost implementing the defense in for an imagenet network is impractical.
existing approaches.
we compare deepsearch with four stateof the art blackbox attacks for generating adversarial examples the nes attack optimized for the l distance metric is developed for various settings including a query limited setting.
it uses natural evolution strategies nes for gradient estimation and performs projected gradient descent pgd style adversarial attacks using estimated gradients.
we compare with the nes attack developed for a query limited setting i.e.
ql nes.
the bandits attack extended gradient estimation based blackbox attacks by integrating gradient priors e.g.
timedependent and data dependent priors through a bandit optimization framework.
the bandits attack can perform both l2andl attacks.
the simple blackbox attack is optimized for the l2distance metric.
starting from an input image it finds adversarial examples by repeatedly adding or subtracting a random vector sampled from a set of predefined orthogonal candidate vectors.
we compare with their simba algorithm which can also be easily constrained using l distance.
the parsimonious blackbox attack optimized for the l distance metric encodes the problem of finding adversarial perturbations as finding solutions to linear programs.
for an input xand distance d it searches among the vertices ofb x d and finds adversarial examples by using efficient algorithms in combinatorial optimization.
deepsearch implementation.
we briefly introduce some implementation details of deepsearch .
in our implementation the classification function fof multiclass classifiers maps input images 807deepsearch a simple and effective blackbox attack for deep neural networks esec fse november virtual event usa to the logarithm of class probabilities predicted by neural networks.
in this setting the objective function in alg.
resp.
alg.
corresponds to logit loss resp.
cross entropy loss .
to reduce the number of queries for input xand distance d we choose xinit b x d such that it is derived from xby setting the values of all its pixels to the lower bounds in b x d .
in the refinement step when calculating new adversarial examples within smaller distances we set xnewtoxinitfor convenience.
in our experiments we used alg.
to attack the undefended networks.
we used alg.
to attack the defended networks since they are more robust against cross entropy model attacks.
to attack the svhn and cifar 10networks we set the initial grouping size to4 .
for imagenet we set the initial grouping size to due to their large image size.
for group splitting we set m 2so that we always divide a group into 2subgroups.
in the hierarchical grouping setting we mutate groups of pixels in random orders.
to avoid overshooting query budgets we mutate groups in batches and the batch size is 64in all our experiments.
parameter settings.
for all datasets we set l distanced on the 255pixel scale to perform attacks.
for both svhn and cifar 10networks we set the query budget to .
for the imagenet network we set the query budget to 000as done in related work .
for the ql nes attack we set .
size of nes population n learning rate .
and momentum .9for svhn and cifar .
we set .
size of nes population n learning rate .
and momentum .9for imagenet.
for the bandits attack we set oco learning rate .
image learning rate h .
bandits exploration .
finite difference probe .
and tile size to 16for svhn and cifar .
we set oco learning rate image learning rate h .
bandits exploration finite difference probe .
and tile size to 64for imagenet.
for the parsimonious attack we use the parameters mentioned in their paper for cifar 10and imagenet networks.
for svhn we use the same parameters as for cifar .
moreover their implementation offers both cross entropy and logit loss to construct attacks.
we tried both loss functions in our experiments and select the one with better performance for comparison.
.
metrics in our evaluation we use the following metrics.
success rate.
the success rate measures the percentage of input images for which adversarial examples are found.
the higher this rate the more effective a given technique in finding adversarial examples.
assume we write findadv x to denote whether an adversarial example is found for input x. if so we have that findadv x otherwise we have findadv x .
for a set of images x x1 ... xk the success rate of a given technique is asr x kk i 1findadv xi average distortion rate.
let sets x x1 ... xk andxadv x ... x k be input images and adversarial examples respectively.table results on svhn networks.
attacksuccess rateavg.
l avg.
l2avg.
queriesmed.
queries undefended network ql nes .
.
.
bandits .
.
.
simba .
.
.
parsimonious .
.
deepsearch .
.
defended network ql nes .
.
.
bandits .
.
.
simba .
.
.
parsimonious .
.
.
.
deepsearch .
.
.
the average distortion rate between xandxadvwith respect to thel distance is avgdrl x xadv kk i xi x i l xi l as shown here the lower this rate the more subtle the adversarial examples.
for approaches that achieve similar misclassification rates we use this metric to determine which approach finds more subtle perturbations.
the average distortion rate with respect to l2 can be derived by substituting l withl2in the above definition.
in our experimental results we also include the average l2distortion for reference.
average queries.
for blackbox attacks we use the number of queries required to find adversarial examples to measure their efficiency.
an approach that requires more queries to perform attacks is more costly.
for each approach we calculate the average number of queries required to perform successful attacks.
we also list their mean number of queries for reference.
we point out that queries made by the refinement step of deepsearch are not counted when calculating average queries because refinement starts after adversarial examples are already found.
.
experimental results deepsearch outperforms all other blackbox attacks in success rate average queries and average distortion rate.
experimental results are shown in fig.
and tabs.
.
results on success rate rq1 .
deepsearch is very effective in finding adversarial examples for both undefended and defended networks.
compared to other blackbox attacks deepsearch has the highest attack success rate.
for undefended networks the success rate of deepsearch is close to for all three datasets.
for the svhn defended network deepsearch has a success rate of .
which is .
higher than that of the parsimonious attack the second best attack in success rate .
for the cifar 10defended network deepsearch has a success rate of .
.
results on average distortion rate rq2 .
deepsearch has found adversarial examples with the lowest average l distortion 808esec fse november virtual event usa fuyuan zhang sankalan pal chowdhury and maria christakis a svhn defended b cifar defended c imagenet undefended figure results on success rate w.r.t number of queries.
table results on cifar 10networks.
attacksuccess rateavg.
l avg.
l2avg.
queriesmed.
queries undefended network ql nes .
.
.
bandits .
.
.
simba .
.
.
parsimonious .
.
.
deepsearch .
.
defended network ql nes .
.
.
bandits .
.
.
simba .
.
.
parsimonious .
.
.
deepsearch .
.
.
table results on imagenet undefended network.
attacksuccess rateavg.
l avg.
l2avg.
queriesmed.
queries ql nes .
.
.
bandits .
.
.
simba .
.
parsimonious .
.
.
deepsearch .
.
.
rate for networks of all datasets.
for the cifar 10undefended network ql nes has a success rate of only .
.
to have a more fair comparison for those .
images that are successfully attacked by both deepsearch and ql nes we further calculate the average distortion rate of the adversarial examples found by deepsearch .
we find it to be .
which is lower than that of ql nes.
although deepsearch is anl attack adversarial examples found by deepsearch also have low average l2distortion rate.
results on query efficiency rq3 .
deepsearch outperforms all other attacks in query efficiency.
compared to the parsimonious attack the second best attack in query efficiency deepsearch reduces the average queries by across all datasets.
results on query reduction rq4 .
we demonstrate the effectiveness of hierarchical grouping in deepsearch for query reduction.
we use deepsearch to attack networks with different initialtable query reduction for svhn and cifar .
for each dataset success rate resp.
average queries is shown in the first resp.
second row.
dataset undefended network svhn100 cifar defended network svhn81.
.
.
.
.
cifar .
.
.
.
.
table query reduction for imagenet.
success rate resp.
average queries is shown in the first resp.
second row.
dataset imagenet99.
.
.
.
.
group sizes and show their corresponding success rates and average queries in tabs.
and .
we first notice that the initial group size can only slightly affect attack success rate.
for the svhn defended network the success rate increases from .
to83.
as initial group size increases from 1to16 .
however the changes in success rate are negligible for all other networks.
on the other hand we observe that hierarchical grouping improves query efficiency dramatically.
we take the average queries of group size 1and4 4as an example for svhn and cifar networks.
for the svhn undefended network we see a .
decrease of average queries from 742to229.
for the svhn defended network average queries are reduced by .
from to1808.
for the cifar 10undefended network the average queries are decreased by .
from 462to247.
for the cifar 10defended network average queries are decreased by from to963 and for the imagenet network from group size 8to32 we decreased the average queries by .
from 765to533.
809deepsearch a simple and effective blackbox attack for deep neural networks esec fse november virtual event usa .
threats to validity we have identified the following three threats to the validity of our experiments.
datasets and network models.
our experimental results may not generalize to other datasets or network models.
however we used three of the most popular datasets for image classification svhn cifar and imagenet.
moreover our network models have very high test accuracy and the defense we use based on adversarial training is state of the art.
existing approaches.
the second threat is related to the choice of existing approaches with which we compare.
deepsearch uses iterative linearization of non linear neural networks and is tailored to thel distance metric.
we thus compare with approaches that can also perform l attacks.
to our knowledge the blackbox attacks with which we compared are all state of the art l attacks.
fairness of comparison.
the selection of parameters for each approach could affect the fairness of our comparison.
we tried various parameters for each attack and choose the ones yielding best performance.
related work adversarial robustness.
szegedy et al.
first discovered adversarial examples in neural networks and used box constrained l bfgs to find them.
since then multiple whitebox adversarial attacks have been proposed fgsm bim deepfool jsma pgd and c w .
goodfellow et al.
first argued that the primary cause of adversarial examples is the linear nature of neural networks and they proposed fgsm that allows fast generation of adversarial examples.
bim improved fgsm by extending it with iterative procedures.
deepfool is another method that performs adversarial attacks through iterative linearization of neural networks.
blackbox adversarial attacks are more difficult than whitebox ones and many blackbox attacks require a large number of queries.
papernot et al.
explored blackbox attacks based on the phenomenon of transferability .
chen et al.
and bhagoji et al.
proposed blackbox attacks based on gradient estimation .
uesato et al.
used spsa .
ilyas et al.
used nes and proposed the bandits attack.
narodytska et al.
performed a local search based attack.
the boundary attack only requires access to the final decision of neural networks.
guo et al.
further considered perturbations in low frequency space.
moon et al.
leveraged algorithms in combinatorial optimization.
although research on developing adversarial attacks is moving fast research on defending neural networks against adversarial attacks is relatively slow .
many defense techniques are shown to be ineffective soon after they have been developed.
we refer to the work of carlini et al.
for a more detailed discussion on evaluating adversarial robustness.
testing deep neural networks.
recently significant progress has been made on testing neural networks.
several useful test coverage criteria have been proposed to guide test case generation deepxplore proposed neuron coverage and the first whitebox testing framework for neural networks deepgauge proposeda set of finer grained test coverage criteria deepct further proposed combinatorial test coverage for neural networks sun et al.
proposed coverage criteria inspired by mc dc kim et al.
proposed surprise adequacy for deep learning systems.
sekhon et al.
and li et al.
pointed out the limitation of existing structural coverage criteria for neural networks.
li et al.
also discussed improvements for better coverage criteria.
moreover sun et al.
proposed the first concolic testing approach for neural networks.
deepcheck tests neural networks based on symbolic execution .
tensorfuzz proposed the first framework of coverage guided fuzzing for neural networks.
deephunter considered various mutation strategies for their fuzzing framework.
wicker et al.
extracted features from images and computed adversarial examples using a two player turn based stochastic game.
dlfuzz proposed the first differential fuzzing framework for deep learning systems.
deeptest and deeproad proposed testing tools for autonomous driving systems based on deep neural networks.
for more on testing neural networks we refer to the work of zhang et al.
that surveys testing of machine learning systems.
formal verification of deep neural networks.
verification of neural networks is more challenging than testing.
early work used abstract interpretation to verify small sized neural networks.
recent work used smt techniques and considered new abstract domains.
liu et al.
classified recent work in the area into five categories reachability analysis based approaches include maxsens exactreach and ai2 nsverify mipverify and ilp are based on primal optimization duality convdual and certify use dual optimization fast lin and fast lip reluval and dlv combine reachability with search sherlock reluplex planet and bab combine search with optimization.
lie et al.
provide a more detailed comparison and discussion of the above mentioned work.
conclusion and future work we proposed and implemented deepsearch a novel blackboxfuzzing technique for attacking deep neural networks.
deepsearch is simple and effective in finding adversarial examples with low distortion and it outperforms state of the art blackbox attacks in a query limited setting.
in our future work we will continue improving the effectiveness of deepsearch for an even more queryefficientl attack.
we are also interested in extending deepsearch to construct query efficient l2attacks.
designing effective defenses to secure deep neural networks against adversarial attacks is non trivial.
in this paper we did not focus on proposing defenses against blackbox attacks.
instead we attacked neural networks with adversarial training based defenses .
another interesting direction for future work is to develop defense techniques that specifically target blackbox attacks for instance by identifying patterns in their sequences of queries.