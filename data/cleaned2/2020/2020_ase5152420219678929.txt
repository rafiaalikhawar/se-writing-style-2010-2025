interactive cross language code retrieval with auto encoders binger chen tu berlin berlin germany chen tu berlin.deziawasch abedjan leibniz universit at hannover l3s research center hannover germany abedjan dbs.uni hannover.de abstract cross language code retrieval is necessary in many real world scenarios.
a major application is program translation e.g.
porting codebases from an obsolete or deprecated languageto a modern one or re implementing existing projects in one spreferred programming language.
existing approaches based onthe translation model require large amounts of training dataand extra information or neglects significant characteristics ofprograms.
leveraging cross language code retrieval to assistautomatic program translation can make use of big code.however existing code retrieval systems have the barrier tofinding the translation with only the features of the input programas the query.
in this paper we present b igpt for interactive cross language retrieval from big code only based on raw code and reusing the retrieved code to assist p rogram t ranslation.
we build on existing work on cross language code representationand propose a novel predictive transformation model based onauto encoders.
the model is trained on big code to generate atarget language representation which will be used as the queryto retrieve the most relevant translations for a given program.our query representation enables the user to easily update andcorrect the returned results to improve the retrieval process.our experiments show that b igpt outperforms state of the art baselines in terms of program accuracy.
using our novel queryingand retrieving mechanism b igpt can be scaled to the large dataset and efficiently retrieve the translation.
i. i ntroduction the number of open source program resources on the internet is constantly growing.
the most well known are open source code repository hosts such as github and bitbucket.the github database i.e.
the public git archive containsmore than github repositories which are written in455 different programming languages and include more than16 billion lines of code in the head files alone.
further community question answering sites such as stack overflow contain a large number of executable binaries that amountto billions of code snippets.
these and similar resourcesare referred to as big code which are created andmodified by programmers with much effort and time.
reuseof code from these abundant databases provides opportuni ties for new applications such as workflow generation data preparation programming assistance databasemanagement and transformation retrieval .
anotherapplication that has recently emerged in this context is programtranslation .
a useful technique that can support several of the aforementioned applications is code retrieval.
in particular cross language retrieval is becoming more prominent for use cases such as program translation and code clone detection.
numer ous programs are being developed and require correspondingversions in different languages.
in cases when the developersdo not make the translation efforts themselves users haveto manually rewrite the software in the needed language.for example there are plenty of open source prototypesdeveloped in academia especially in the current boomingfield of big data.
to port codebases written in obsolete ordeprecated languages to a modern one or further study reproduce or apply them on various platforms researchersusually need to rewrite these programs in their preferredprogramming languages.
manually rewriting software is time consuming and error prone.
for instance the commonwealthbank of australia spent around million and years totranslate its platform from cobol to java .
therefore new approaches for automated program translation and codemigration are emerging .
the traditional methods arehardwired rule based compilers or cross language interpreters which require heavy human intervention for adaptation and arelimited to a small set of programming languages .
however if we leverage a cross language retrieval system we can makethe most of the existing big code resources to support theprogram translation use case.
in this paper we discuss thepotentials of an effective cross language retrieval system inassisting program translation.
state of the art.
our work is inspired by two lines of research code retrieval and supervised program translation.
most existing code retrieval systems such as sourcerer lack the proper capabilities for code to code search and or cross language retrieval.
the cross language systemyogo requires users to provide pre defined handwrittenpattern queries or feedback on several preset metrics andquestions as many other code search systems which increase the workload of users.
our recentlyproposed system rpt aims to retrieve the translationonly with the feature of the source code and ignores thelanguage specific differences to the target language.
allof the aforementioned techniques exclude users from theretrieval translation loop.
similar to natural language translation the mainstream data driven program translation approaches train a transla1672021 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee tion model from large amounts of code data either in a supervised or weakly supervised fashion .supervised approaches require a parallel dataset to train the translation model.
in parallel datasets programs in differ ent languages are considered to be semantically aligned .obtaining the parallel datasets in programming languages ishard because the translations have to be handwritten most ofthe time.
a recent weakly supervised method by facebookai pretrains the translation model on the task of denois ing randomly corrupted programs and optimizes the modelthrough back translation.
however this method still relies onhigh quality training data and directly reuses natural languageprocessing nlp approaches that neglect the special featuresof programming languages such as code syntax.
all theseapproaches require human efforts on the input or additionalinformation for training and evaluation such as annotations program descriptions api usages and use cases which can not always be provided.
furthermore compared to retrievalmethods these machine generated program translations sufferfrom grammar mistakes because of the rigor of programminglanguages.
our methodology is to reuse existing big code resources and developing cross language retrieval techniques to assisttranslation with retrieved similar code in target language.note that such a method might fail to find the translationfor very specific long and complex programs because theachievable performance is directly depending on the richnessof the given repository.
nevertheless due to the modularnature of programs and the huge data volume of the big code it should be possible to retrieve the translation for smallerfragments of an input program such as methods and functions.the user then can use these building blocks to assemble thecomplete translation.
we improve and extend our preliminaryattempt on supporting code translation through big codeby addressing the following open challenges and requirements big code resources are typically imperfect and disordered.thus it is hard to obtain correct translations in absence ofsufficient information such as training data hand craftedpatterns and semantic annotations.
because of the different syntax structures and naming mech anisms in different programming languages it is hard tocapture program features in a unified way.
ideally one has toresort to an intermediate representation that is automaticallyextractable from raw code.
with this it would be possibleto use similarity metrics already in place for code clone de tection.
but generating such an intermediate representationis yet an open task.
representation techniques that are in place for naturallanguage are not designed to deal with the special syntaxand grammatical rigor of code.
the feature representation has to be incrementally updatableto enable user interaction.
in this paper we present b igpt an interactive crosslanguage code retrieval system that reusing big code resources to assist the p rogram t ranslation task.
given a raw piece of code in the source language b igpt uses a novel querytransformation approach based on auto encoders to retrievethe most similar piece of code in the selected target languagefrom the existing big code resources.
we propose a querytransformation model which can be trained in an unsuper vised manner on big code to transform the input programrepresentation to a representation that is closer to propertiesof the target language.
due to the succinct form of the programrepresentation the user can interact with b igpt and the query can be automatically adapted to user annotations in thetarget code.
as b igpt is a heuristic methodology based on retrieving real programs it is less prone to grammar mistakesthan approaches based on code generation.
our experimentsshow that our approach outperforms existing cross languageretrieval techniques and statistical translation models thatrequire a large amount of training data.
to this end we makethe following main contributions we present a program feature representation for effectiveretrieval of possible translations in imperative programminglanguages.
with the help of big code we propose a novelauto encoder based query transformation model which cantransform the input code into the representation of itstranslation.
we design the feature representation in a way that it canbe incrementally updated based on user corrections to effi ciently retrieve the translation from big code.
we further expand our feature representation with a weight ing scheme to enable user feedback that accelerates b igpt s ability to improve its retrieval results.
ii.
r ela ted work our work is related to code search data drive program translation and program representation.
we also discuss re lated work in cross language code clone detection and naturallanguage retrieval to show the originality of our work.
general code search.
most of the existing methods cannot find translations with only raw code as input.
krugle and codase are commercial engines that apply the capabilitiesof web search engines for code search .
lucene is afamous conventional code search engine behind many existingtools such as sourcerer .
it retrieves code based on textand code properties such as fully qualified name and codepopularity.
s6 retrieves code based on the user s specifica tions and modifies it through a set of transformations .codehow is a text based code search engine that incorpo rates an extended boolean model and api matching .deepcs trains a neural network model for code snippetsand their natural language description which are used asqueries .
facoy is a code to code search engine thatrequires q a posts from stack overflow as query addition tothe code snippets .
yogo provides a cross language graphrepresentation which however requires handwritten semanticrules and patterns written in a domain specific language foreach query .
all these works require users or additionalresources to provide keywords specifications rules or naturallanguage descriptions.
recently we proposed rpt whichuses cross language retrieval for translations .
in this 168preliminary work we used the features of the input program to retrieve the translation and did not support user interactionalthough retrieval based methods cannot always provide off the shelf translations.
except yogo and rpt all of theaforementioned systems are designed for the mono languagesetting.
in contrast to these systems b igpt only takes raw code as the query and aims to perform cross language retrieval.
interactive code search.
wang et al.
refine a query based on user s feedback on each result and reorder the ranking list .
nie et al.
extract relevant feedback from stackoverflow for theinitial query and reformulate it using rocchio expansion .dietrich et al.
utilize a novel form of association rule miningto learn a set of query transformation rules from user feed back to improve the search queries.
codeexchange leveragesthe context from previous retrieval results to reformulate agiven query and breaks it into several parts for the user tofeedback .
sivar et al.
propose an active learning systemalice to iteratively refine a query based on positive ornegative labels .
all of the aforementioned methods areonly applicable to mono language scenarios and work withnatural language queries.
as for interaction these methods askthe user to give feedback on preset metrics based on likertscale or questions e.g.
stackoverflow q a pairs.
thus the performance mainly depends on the developer s abilityto formulate queries.
b igpt directly uses user s corrections to the retrieved results as feedback and integrates an activelearning based query transformation model to refine the query.
data driven program translation.
existing work mainly focuses on building a translation model.
nguyen et al.
applied the phrase based statistical machine translation smt modelon the lexemes of source code to translate java code toc .
in their follow up work they develop a multi phase phrase based smt method that infers and applies bothstructure and api mapping rules .
but they are limitedto languages that are similar on either structural or textuallevel such as java c .
chen et al.
binarize the code treeof a piece of code and translate the code with an lstm based encoder decoder model .
all the above methodsrequire a large parallel dataset for training.
in contrast to them the weakly supervised system transcoder first trains across language model through the task of predicting randomlymasked words then acquires a pre trained translation modelfrom denoising randomly corrupted program task.
finally they improve this model through back translation.
althoughtranscoder does not need parallel translation data this transferlearning method highly relies on the similarity of the datafor pre training.
their approach processes code like naturallanguage which might leave out some programming specificlanguage features.
we propose a system that can assist pro gram translation without parallel datasets and additional infor mation.
by reusing big code it can comply with the rigorousgrammar without machine generated translation.
further oursystem outperforms the aforementioned techniques with anovel program representation that captures the most crucialfeatures of programs.program representation.
by constructing program representations one can enable the application of data processing to awide range of programming language tasks including programtranslation and code search.
kamiya et al.
and allamanis etal.
treat a program as plain text and use the sequence oftokens as representation to detect code clones and summa rize code .
allamanis et al.
present a gated graphneural network in which program elements are representedby graph nodes and their semantic relations are edges in thegraph to predict variable name and select correct variable .these methods rely on semantic knowledge which requiresexpert analysis and is not generalizable across programminglanguages.
a recent approach uses paths in the program sabstract syntax trees ast as code representation to predictprogram properties such as names or expression types .and they further propose code2vec that leverages a tree based neural network to encode these paths and generatemore abstract representations .
yin et al.
employ neuralnetworks to express source code edits .
however thesemethods are only designed to represent the features in oneprogramming language and do not capture the commonalitiesof multiple languages.
and some methods are too abstract sothat important information for effective retrieval is missing such as low level program syntax or token type .
theprogram representation we use as the query is inspired bythe work in and .
in addition to ast we considerfeatures of concrete syntax trees cst and text to enrich theinformation for cross language search.
and we train a querytransformation model on big code to transform the featuresbetween different languages.
cross language code clone detection.
this line of research aims at identifying duplicates of a given piece of code.
however this line of research has so far only focused onlanguages with similar intermediate representation such asthe .net language family .
others only calculatesimilarity on the textual level same as most codesearch methods.
they simply treat programs as plain text andmake certain assumptions that limit their usage in practice i.e.
they try to identify different revisions of the same pieceof code.
as our goal is to find program translations ourrequirements go beyond the state of the art in clone detection.nevertheless we still compare our program representation toplain text representation used in code clone methods in our benchmarks showing its superiority.
cross language text retrieval.
there is a body of work on cross language retrieval for natural language .
they take plain text as input and generate feature representa tions based on natural language syntax.
however code has itsunique properties.
not only the code syntax can differ acrosslanguages but the text in code does not exactly follow thesame vocabulary and writing formats as in natural language.therefore directly using these methods on code search canlead to incorrect results or failures.
we also evaluate usingnatural language methods in our experiment by processingcode as plain text using bag of words and word2vec.
169fig.
b igpt overview iii.
s ystem overview we propose b igpt an interactive cross language code retrieval system that assists program translation by reusing big code.
given a piece of source program pswritten in language ls a selected target language lt and a large program repository dp p1 p2 ... p n the goal is to find the best possible translation ptofpsinltfromdp.
the essential problem is to design an effective program featurerepresentation that generalizes to many languages can beupdated through user feedback and enables efficient retrievalin the scale of big code.
the workflow of b igpt is shown in figure .
b igpt first constructs a feature representation for input programs to formthe query section iv a .
since the target is to identify a simi lar program in the target language b igpt then applies a query transformation model qtm to transform this representationinto an estimated feature representation of the translation sec tion iv .
qtm is trained in an unsupervised manner on bigcode but can also be updated dynamically through activelearning.
this new representation will be used as a query toretrieve potential translations from the database.
for efficientretrieval section v b igpt leverages an index structure that captures key feature elements and is constructed in the offlinephase.
as an interactive system b igpt allows the user to give feedback on the retrieved translation section vi .
the usercan either accept the result or make some corrections.
based onour structured and informative feature representation b igpt can easily and quickly adapt the query based on local usercorrections.
note that the user is not necessarily correctingthe whole program but only some local spots that they deemwrong.
with this partial correction b igpt may identify a more appropriate translation candidate that can be acceptedby the user in the second retrieval attempt.
iv .
q uery construction as the user only inputs the raw source code to retrieve a relevant translation for it from a large code database b igpt needs to automatically construct an effective cross language fig.
simplified syntax tree of a javascript program fig.
cst and ast of a fragment of the program in figure query.
in this section we will first introduce the fundamental program feature representation then discuss how our querytransformation model qtm generates the search query andhow b igpt further optimizes the model with active learning.
a. program representation as already suggested by the recent cross language code retrieval system rpt b igpt takes both structural features and textual features as well as their dependencies into consid eration to capture special aspects of programming languagesyntax and program semantics expressed in the text.
structural features.
to capture the structural features of a program we resort to a representation based on the syntax tree of a program.
each program can be represented by itssyntax tree where each tree node denotes a code construct.the syntax tree depicts the structural dependencies of the codeconstructs.
one could also use control flow graph cfg thatcaptures the dependency between code blocks and proceduresto approximate the code behavior.
however our goal is toassist program translation for any granularity of a program.code behavior is hard to measure when the code fragment isnot an independently executable code block.
considering thatconstructing cfgs requires more complex analysis than syntaxtrees we pick syntax trees as the basis of our representationto also capture the low level syntactic structure within codeblocks.
the syntax tree can be either a concrete syntaxtree cst or an abstract syntax tree ast .a cst depicts nodes with complete structural information such as all the tokens in the code.
as such the cst ishighly language dependent.
the ast on the other hand ismore abstract and misses information such as the intermediatesyntax and the type of each token.
therefore the cst is quite verbose and the ast may lose informative syntax and does not generalize to multiplelanguages.
to develop a compromise solution that keeps thebest of both worlds and as previously suggested we fallback on the low level cst as a basis and take the philosophy 170table i paths extracted from the tree in figure root node leaf nodes path type ifstatementidentifier identifier p1 identifier literal p2 literal identifier literal literal p3 equalityexpression identifier literal p4 assignmentexpression identifier literal p5 of ast as an inspiration.
specifically we first take cst of the program as the base structure.
then we simplify the cst byonly removing semantically repetitive nodes so that redundantinformation can be removed and necessary information canbe retained.
figure shows the generated syntax tree of ajavascript program.
we use the same approach as proposedfor rpt .
figure shows the original cst and ast of afragment of this program.
we can see that the new syntax treeis more succinct than cst and more informative than ast.however this syntax tree is still complicated for representingthe code features and retrieving the translation.
and becauseof different control flow elements in different programminglanguages it is unlikely to find programs that share the exactsame syntax tree.
therefore imitating the idea of ast wefurther abstract the tree.
first we simplify the representationand transform the two dimensional tree structure into a setof one dimensional paths that connect the program elementsinside the tree.
b igpt extracts abstract paths as follows for each pair of leaf nodes in the cst b igpt keeps the nodes themselves their values and the root node of the statementand drops all other intermediate nodes on this path.
the root node is the summary of the whole path and the leaf nodesdirectly indicate the content on this path.
these three nodesenclose the most critical information on a path.
moreover we extract all paths between two leaf nodes from the treeto represent the features.
in this way all the intermediatenodes have the chance to be the root nodes which facilitatesto capture more complete structural information.
the pathsextracted from the trees in figure are shown in table.
thenwe classify these paths into different types and replace thesepaths with their path type as shown in table.
in this way we can further generalize and simplifies the features.
usingthe same method in rpt b igpt considers two paths are the same type if their root nodes and leaf nodes are thesame or have the same meaning such as ifstatement and if stmt.
also as the writing habit of programmers and thecoding conventions for a programming language might differ b igpt ignores the order of left and right leaf nodes such as p2.
thus the structural feature of a program can be succinctly represented by a set of path types p1 p2 ... p jextracted from its syntax tree.
textual features.
in contrast to existing work that suggest to extract all the text from a program and do not consider any context from the program structure b igpt considers textual features only in strong dependency with the structuralfeatures and leverages context from the structure .
to doso b igpt only processes text that appears in the extracted paths and marks the path type where they belong.
becausethe text appears on or connects to those removed semanticallyrepetitive nodes only brings in redundant information.
andconsidering the dependency can encode the text with thefeatures of the programming languages not only the naturallanguages.
this methodology can also simplify the featuresand improve system efficiency.
similar to first uses word tokenization and lemmatization to tokenize and stem all the words in the text.
in addition our tokenization process also considers camel case spaces andunderlines to accommodate code specific language.
however it does not remove and tokenize numeric values such as hard coded floating points and integers as they might be integralto the purpose of a program.
then b igpt vectorizes these generated tokens based on the bag of words model bow .one could also resort to more sophisticated and complexembeddings such as word2vec w2v and bert .however in programming languages the structural featuresare more important than the textual ones and word ordercan be ignored to accommodate different programming styles.besides we only compare text for every single path type significantly reducing the number of words for each similaritycalculation.
bow is sufficient for this process.
in our ex periment w2v does not show worthwhile improvements butrather introduces extra training time for building the languagemodel.
to avoid repeated computation for every new inputand to improve the efficiency b igpt precalculates the bow model in the offline phase.
to build the bow model we needto prepare a vocabulary of unique words in each program fromthe repository.
as we consider the dependency with structuralfeatures two textually identical tokens from different types ofpaths are regarded as different tokens.
to this end we countall the text tokens t t2 ... t kof each program and store the results during the offline phase.
in the online phase b igpt only needs to run word statistics on the input program andbuild the bow model.
feature representation.
unlike rpt that uses a list of pathtypes and text collections as the final representation we construct a numerical feature vector to represent theprogram.
the final feature representation is thus more com pendious as one single vector which can be directly usedto further train a learning model.
our final representationis constructed as follows we regard the tokens textual together with different types of paths structural as fea ture elements eof a program and generate a feature vector consisting of the feature element frequencies.
let fbe the occurrence frequency of feature elements then the fi nal vectorized feature representation of a program will be f e1 fe2 ... f en .
in our experiments the number of different path types is about 000on average for each language pair.
in the subsequent retrievalprocess the potential translation in the target language can beidentified by calculating the similarity of feature vectors.
171fig.
query transformation model qtm b. query transformation model one could directly use the feature representation described in section iv a as a query to retrieve translation candidates.
however the feature vector will fail to accommodate somecross language hurdles.
for example c supports goto statements while its java translation has to use so called labelledstatements with break orcontinue instead of goto.
in this case b igpt cannot directly use the features of the c program to retrieve its java translation.
the result canbe improved if the retrieval can be conducted based on thefeatures of the translation.
while the translation of a completeprogram is our original problem and hard to solve withoutlarge amounts of training data we propose a qtm that solvesa smaller problem.
qtm transforms the original query i.e.
thefeature vector of the input program into an optimized query which will be the estimated feature vector of the translation.
model description.
as shown on the left part of figure in the online phase b igpt extracts the features as fsof a program from source language lsand feeds it into the qtm to translate the program features to features in language lt. the qtm first selects previously trained auto encoders see the next paragraph for details of lsandltrespectively then extracts the encoder of the former and the decoder of the latterto compose a new encoder decoder model.
in this model aone layer encoder red in figure maps the original featurevector to a low dimensional latent space and produces a shorterhidden vector h. thenhis reconstructed to the estimated translation feature vector f tby a one layer decoder yellow in figure .
ftwill be used as a query to retrieve potential translation in target language lt. since there is no available training data for qtm we leverage an unsupervised method based on auto encoders ae totrain an encoder and a decoder.
an ae is an encoder decodersystem that aims to reproduce its input.
that is it encodes theinput to a hidden vector then reconstructs the input from thishidden vector.
therefore no extra label for the training datais needed.
with this approach b igpt learns the weights of the encoders and decoders separately.
as shown in the rightpart of figure in the offline phase b igpt trains a separate ae ifor each programming language liin the database on all programs that are written in li.
thus it obtains a pair ofencoder ianddecoder ifor each programming language.
for the actual translation task we combine the appropriate encoderand decoder depending on the source and target language ofa translation task.
in figure the qtm selects the encoderencoder sof the source language lsfromae sto transform fsinto the hidden layer representation.
and it picks the trained decoderdecoder tof the target language ltfromae tto estimate the ft. this way we can build a pre trained model with an encoder that learns significant information from thefeature vector of the input program and a decoder that cangenerate features of its translation.
active learning mode.
to increase the accuracy of qtm we also provide an active learning mode to enable the user to fine tune the model.
for each pair of languages it is possibleto train the corresponding qtm via active learning.
as shownin figure during each translation retrieval the most usefulinput programs in the source language are selected with asampling strategy.
then b igpt will retrieve its translation in the target language.
the user either accepts the retrieved resultor annotates the correct translation herself.
then with thisuser approved correct translation as the label of the input wecan further train the qtm and update the weights.
to choosethe appropriate input program we propose an aggregation offour sampling strategies that capture the informativeness of aprogram as follows coverage sampling picks the programs that cover a wider range of different feature elements which may reveal moreinformation.
we consider programs that cover more than50 of the total amount of all feature elements as programswith high coverage.
in a database if the average amount offeature elements contained in a program ais and the program contains more than different feature elements it is a qualified sample.
rarity sampling considers programs with rare feature elements i.e.
programs that contain features that appear in atmost epsilon1 of the program database.
for example if feature elemente 1from program aappears in x x epsilon1 o ft h e database programs ais a qualified sample.
uncertainty sampling picks retrieved programs with low certainty i.e.
lower similarity score than .
for example if program bis the top retrieved translation of program a but their similarity score is which is lower than programais a qualified sample.
random sampling randomly selects a program .
bigpt employs the query by committee method to aggregate the results of the four sampling methods .
with thisapproach we make sure to have incorporated a diverse set ofcharacteristics that might be relevant for sampling.
the finaldecision is made by selecting program data where the largestdisagreement occurs among those sampling strategies.thelevel of disagreement of a program xis measured by vote entropyve ve x v x nslogv x ns ns v x nslogns v x ns v x is the number of sampling strategies that select vote x 172fig.
frequency histogram of path types in one program as a valuable sample.
nsis the total number of sampling strategies which equals 4in our case.
programs with higher vote entropy are returned as samples.
v. t ransla tion retriev al the output of the qtm is an approximate representation of the translation.
b igpt uses this output as a query to retrieve the candidate with the highest feature similarity.
to avoid a full scan while retrieving the most relevant translations we usea path type aware index and an efficient retrieval mechanism.
index structure.
in the offline phase b igpt constructs representations as described in section iv a for each program from the code database and stores them inside a feature database.to avoid a brute force similarity computation we need anindex structure.
for two programs to be similar they haveto share similar structure features which are captured throughcommon path types as described in section iv a. so we needto first find all the programs that share at least one path typewith the source program.
a naive approach is to index eachpath type as the key.
however there are millions of programsinside the database and there are only about types ofpaths on average for each language pair which makes thisindex structure highly sparse and ineffective.
inspired by we design our index structure to also harbor the frequency ofeach path type inside a program as many programs share thesame path type but differ in the frequency of such path types.since the source program and its translation candidate maynot always share the amount of the same path type it wouldbe too strict to have an index entry for every combination ofpath type and frequency.
instead we divide the frequency ofpath types into multiple buckets and use the bucket intervalsas indexes.
we observed the frequency of each path type ineach program roughly obeys exponential distribution as shownin figure .
to ensure the size of each bucket is equal wefix a bucket size and create as many buckets as are needed.then we sort the programs based on the frequency of thecorresponding path type and add them gradually to the sortedfix sized buckets.
in our experiments a bucket size of 200already shows a high performance gain.
example p1occurs times in programs times in programs times in programs and times in one program.
if we evenly divide the interval we will have programs in and programs in .
but if we fig.
user feedback mechanism divide the interval into and based on the frequency distribution each interval contains programs.
this intervalleads to a more balanced index structure.
efficient retrieval.
for each pair of query program and candidate program b igpt calculates the weighted sum of structural similarity and textual similarity to measure the overall similarity.
to make the retrieval process as efficientas possible we first use the index that filters all programs thatdo not contain a similar amount of the same path types.
thenumber of candidates for the similarity calculation is typicallystill quite high.
thus b igpt consecutively calculates the independent similarity components structural similarity andtextual similarity and drops candidates that fail to meet aminimum threshold concerning any of the two.
the thresholdsfor both components are chosen based on the inflection pointsof each score distribution respectively.
similar to rpt b igpt first calculates the structural similarity to each candidate because the syntax structure of a program is morediscriminative than textual features.
this will significantlyreduce the number of irrelevant programs and avoid theunnecessary calculation of textual similarity with them.
forthe remaining candidates a textual similarity filter is employedwhich uses a weighted jaccard index that accommodates therelevance of common textual features within the same path types.
for all the final remaining programs the weighted sumof both previously calculated similarities will be generated toobtain the final similarity score.
vi.
q uery adaption with user feedback it can happen that the desired translation is not among the top k retrieved results.
in this case b igpt can change the query based on the user s feedback.
existing interactivecode retrieval methods ask the user to give feedback on presetmetrics and questions as discussed in related work .
as each feature element in our query directly maps to acode fragment b igpt can directly pass the user s corrections on the code to adapt the query.
the simplest form of usingthe user corrections is to just update the feature vector of thecorrected program.
however we can also make use of thefact that user corrections lead to manually curated features.to reflect this in our feature representation we extend it witha weighting scheme.
as shown in figure we obtain a new 173fig.
an example of user feedback feature representation r where each element consists of a feature element feiand its weight wi.
the initial weights are uniform.
after the user makes one or several corrections to the result b igpt featurizes each correction the same way and compares it with the original code to generate the weights.
weclassify corrections into three categories and the weights aretuned accordingly emphasize.
if the correction increases the number of a feature element fei r its weight will be increased.
add.
if the correction adds a feature element fei r fei and its initial weight wiwill be added to r. delete.
if the correction decreases feature element fei r its weight wiwill also be decreased.
after a correction the feature representation is updated and used for a new round of retrieval.
when calculatingthe similarity between query and candidates in the database b igpt prefers the candidate that has a higher similarity in higher weighted features.
example figure is an example of the user feedback module.
the input is a greatest common divisor function in javascript.
the ground truth in python is also shown in the fig ure.
in the first round b igpt retrieved an ackermann function as its python translation.
the possible reasons are the variablenames are different in the ground truth and the weight of eachfeature element is not assigned properly.
after user correctsthe first wrong line change return n toreturn n b igpt constructs feature representation for the corrected code.
then b igpt compares it with the feature representation of the input code and summarizes user s corrections.
basedon this b igpt updates the query in figure the corrections are deleting the path type additiveexpression identifier literal and token 1on this path type.
as a result these two feature elements will be removed and theweight of other feature elements will be increased accordingly.finally with the updated query b igpt will run a new round of retrieval.
without more precise features the ground truthwill be more likely to be retrieved.
vii.
e xperiments to show the feasibility of our b igpt and evaluate its effectiveness and efficiency in assisting program translation we conducted a series of experiments a. we compare different variations of b igpt with existing work from program translation and code search b. we evaluate b igpt on more languages c. we discuss the influence of user interaction d. we evaluate the scalability and efficiency of b igpt.
all experiments have been carried out on a pc with an intelxeon e5 v2 .60ghz cpu and an nvidia tesla k40mgpu.
datasets.
we have two different types of datasets dataset for training the auto encoders in qtm word2vec and code2vec public git archive pga .
we use this database with more than bookmarked repositories to train the aes of qtm.
we cleaned the dataset by gradu ally removing duplicates at files and files that cannotbe successfully parsed due to format errors versioncompatibility.
we then collected data in four popularlanguages javascript python java c .
finally weobtain a dataset with a size of 260gb.
we split all thefiles into methods or functions.
datasets for evaluation datasets with ground truth are generally scarce.
we run our experiments on two smallparallel datasets and one larger unlabeled dataset java c used in experiment a c d. it was used in previous studies .
we use the samedump that was used for the cross language retrieval en gine rpt which contains matched methods.
geekforgeeks used in experiment b. it was used in transcoder which gathered and aligned codingproblems and their solutions in java python and c .
pgas used in experiment d. to save experiment equipment and time under the premise of ensuring the validityof the experiment and the reproducibility of the data werandomly pick files from pga to obtain a datasetincluding methods functions.
as this datasethas no labels we manually judge the correctness.
effectiveness metric.
we use program accuracy pa as proposed by prior work .
pa is the percentageof the predicted translation that is the same as the groundtruth.
note that pa is an underestimation because it does notaccount for programs that only differ in writing habits andstyle.
we use this standard to manually judge the correctnessfor the dataset without ground truth.
for our retrieval basedmethod we consider the top retrieved result as the predictedtranslation and report the percentages of search where thecorrect translation was the top ranked result.
note that pa isan underestimation of computational accuracy which evaluates 174whether the translation generates the same outputs as the source program when given the same inputs .
a. comparison with baselines we compare b igpt with one rule based tool j2c and four data driven program translation baselines 1psmt mppsmt tree2tree transcoder which use a translation model to generate the results.
the supervised baselines use matched method pairs as training datato predict the translations for the rest of the programs.
weused the openly available implementation of tree2tree.f o r transcoder we follow their method to pre train the cross language model on the public git archive dataset 30gbof java and c data .
for the program translation baselines1psmt and mmpsmt we report the results from their workon the same dataset as their code and configurations are notavailable.
finally we report the results of two mono languagecode search systems sourcerer codehow and one cross language system rpt .
we generate different versions of b igpt with variations in feature representation and interaction representations we analyze b igpt word 2vec and bigpt code 2vec as two feature representations variations of bigpt that retrieving translation based on word2vec and code2vec respectively.
interactive versions we discuss three different interactiveversions of b igpt.
all of which use the same feature representation and the qtm module.
b igpt noaluses the original qtm that has not been improved by active learning.
bigpt is the default setting of our system.
b igpt fb is the full fledged interactive system when user feedback isavailable as described in table ii.
table ii shows the results and the degree of supervision.
we observe that the full fledged b igpt with at most one user correction per task and optimized qtm outperforms allthe baselines.
the improvement in program accuracy rangesfrom .
to .
.
as expected the mono language codesearch baselines perform poorly because they are designedfor retrieval with more accurate and detailed input than rawcode in another language.
the cross language code searchsystem rpt performs better than other baselines which showsthe feasibility of the translation retrieval methodology.
theresults of partial components of b igpt are encouraging.
bigpt noal which does not leverage any supervision outperforms the tree2tree which shows the effectiveness ofour program feature representation and qtm module.
ourfeature representation equipped with qtm successfully im proves the result by .
compared to rpt showing thatgenerating features in the target language is more promisingthan using features of the source language.
in our defaultsystem b igpt the qtm is further trained by active learning the accuracy can increase by .
.
the table also showsthat the word2vec and code2vec variants of b igpt cannot perform better.
word2vec is designed for natural languages sothat it can not capture the special features of programminglanguages.
although code2vec is designed specifically torepresent code their model can only be trained within thesame programming language which makes it less suitable forcross language similarity comparisons.
we further explored the supervision impact on b igpt.
in this experiment we let the user give at most one correctionto each retrieved task.
with such limited user feedback b igpt fb can still slightly improves on b igpt.
compared to the fully supervised methods 1psmt mmpsmt andtree2tree b igpt and b igpt fb leverage very limited human supervision to achieve better results.
in the first retrieval round where the user does not make corrections to any wrong results b igpt achieves .
accuracy with only labels for qtm.
also the reproduced weakly supervised approach transcoderdoes not achieve better results than b igpt.
we observed that their model often generates invalid translations with regardto grammar.
for example it often mistakes the input typeof a function.
this phenomenon is also acknowledged intheir own paper and can be attributed to the fact that onlytextual features have been used.
b igpt avoids this problem by reusing existing code.
b. evaluation on multiple programming languages we further evaluate b igpt on more languages.
we compare bigpt and the state of the art methods transcoder and rpt on the geeksforgeeks benchmark that contains ground truth for java python and c .
table iii shows that the accuracyof b igpt is significantly higher than transcoder on their own datasets as reported in their own paper.
note that fortranscoder the authors report computational accuracy instead of program accuracy.
as the dataset has ground truth thereported program accuracy for our method which is an underestimation of the possible computational accuracy for b igpt.
we infer that transcoder has two drawbacks transcoderoutputs machine generated translations while b igpt directly retrieves existing programs as translations which makes bigpt always output syntactically correct programs.
transcoder generally considers programming languages asplain text and aims to generate semantically similar programs.the black box neural network model may neglect some non trivial syntactical features of programming languages.
b igpt also outperforms the cross language retrieval system rptwith over .
the reason is that b igpt uses a query that represents the features of the translation rather than the inputprogram.
also the user interaction mechanism can improvethe performance in most cases.
this experiment further showsthat b igpt is generalizable for multiple languages including dynamic languages such as python.
c. influence of user interaction in table ii we showed the influence of a single user correction on the result.
we further investigate the required number of user corrections to retrieve the correct translations.we simulate the user correction with the ground truth in ourparallel dataset and for each returned result we fix the firstdiffering line between true result and returned result.
175table ii comparison of different methods on pa and supervision extent java c genre method description pa supervision extent rule based j2c manually defined translation rules .
fully supervised data drivenprogram translation1psmt phrase based smt .
fully supervisedmmpsmt multi phase phrase based smt .
tree2tree tree to tree neural networks .
transcoder weakly supervised neural translation .
weakly supervised code searchsystemsourcerer lucene based code search free text queries .
no labels directly retrieve translation with input codehow free text queries .
rpt cross language code search .
v ariations of b igptbigpt word 2vec word2vec as queries .
no labels directly retrieve translation with input bigpt code 2vec code2vec as queries .
bigpt noal qtm without active learning .
bigpt the default system .
labels for qtm bigpt fb user feedback is available .
labels for qtm at most correction per task table iii comparison of accuracy on geeksforgeeks c c java java python python java python c python c java transcoder .
.
.
.
.
.
rpt .
.
.
.
.
.
bigpt noal76.
.
.
.
.
.
bigpt .
.
.
.
.
.
bigpt fb .
.
.
.
.
.
fig.
required amount of user feedback figure shows the number of required user corrections to obtain the correct result for all failed translation tasks from the first retrieval round and the improvement inthe overall accuracy.
we observe that in most cases b igpt only requires a single user correction to successfully completethe translation task.
about of the failed retrieval taskscan succeed after user corrections.
considering the averagelength of an input program is lines we can concludethat with a limited number of user corrections the accuracyof b igpt can be significantly improved.
note that we might even achieve better results if we do not restrict the users tofix the first difference each time.
a real user might fix moresignificant errors that lead to faster convergence.
d. evaluation of scalability and efficiency to evaluate b igpt on a larger dataset with multiple languages we run it on the pgas dataset.
as we have to manually judge the correctness we carry out a samplinginspection to make it feasible.
we randomly pick programsand b igpt retrieves the best possible translation from the dataset for each of these programs.
as we spent up to fig.
relationship between the efficiency and database scale minutes on each result to make the judgement as accurate as possible we had to limit the labeling process due to timeand human resources.to show the advantage of b igpt we compare its results to b igpt word 2vec and b igpt code 2vec.
in tables iv we observe that b igpt can successfully retrieve the correct translation for .
programs among fourlanguages.
considering b igpt is positioned as a translation assistant system the accuracy is considerable.
translationtasks that cannot be fully automated can still be assisted by theresults returned by b igpt.
this result shows that it is feasible to scale b igpt to more data volume.
compared to the other program representations our novel representation performssignificantly better.
w2v does not contribute much to theresults and introduces extra model training time compared tothe simple bow model.
in b igpt we retain bow to trade off response time for a slight decrease of accuracy.
code2vec alsoconsiders the structure of programming languages.
however their model can only be trained within the same language which limits its performance in the cross language setting.
we also compare the average runtime of each translation retrieval task between rpt and b igpt.
table v shows that both rpt and b igpt can complete the translation task on average in .2s on the small java c dataset.
on the larger pgas dataset b igpt can retrieve the translation within .57s.
we further investigate how the runtime increases with the scaleof the database.
we randomly pick and of the pgas dataset and run the retrieval process on thesesubsets.
for each subset we retrieve translation candidates 176table iv comparison of program accuracy on the pgas dataset source language js python java c target language c python java js c java js python c js python java bigpt code 2vec .
.
.
.
.
.
.
.
.
.
.
.
bigpt word 2vec .
.
.
.
.
.
.
.
.
.
.
.
bigpt .
.
.
.
.
.
.
.
.
.
.
.
table v efficiency of b igpt runtime per retrieval methodjava c pgas matched methods methods functions rpt .20s .95s bigpt .20s .57s for randomly picked input programs and calculate the average runtime per retrieval task.
including the results on thewhole dataset all the results are shown in figure .
b igpt is here significantly faster than rpt because it filters moreirrelevant programs due to its advanced qtm.
and the runtimeis slowly growing with the database scale with an approximateexponential pattern.
in addition we measured the efficiencyduring user interaction.
the average response time of b igpt to each intermediate user feedback is .4ms which shows that bigpt can respond to corrections in real time.
scope of application.
theoretically b igpt can be applied to all static and dynamic imperative languages.
according to ourexperiments b igpt is more effective in finding translation candidates for grammatically similar programming languages such as c and java.
furthermore when the target languageis a low level programming language such as c the ac curacy is generally higher than python which is a high leveldynamic programming language.
failed cases.
there are cases where our approach fails programs with special structures that do not exist in the target language.
for example deterministic destruction andpointer arithmetic in c cannot be translated to java.
programs with apis that do not exist in the target languagecannot be translated.
short programs i.e.
fewer than lines are highly ambigu ous and lead to more candidates with similar scores.
viii.
c onclusion we presented a novel interactive cross language code retrieval system that can assist program translation by reusingbig code.
we propose a novel cross language program repre sentation with a qtm that can learn a succinct but informativefeature vector to retrieve the possible translation of an inputraw program.
our querying and retrieving mechanism makesthe system scalable and efficient on big code.
further thissuccinct representation can be easily adapted to user correc tions for interactive retrieval improvements.
our experimentsshow that b igpt outperforms existing solutions and requires no parallel training dataset and additional user input.
limitations.
although b igpt s performance is promising it still has some limitations.
a translation task cannot be assisted if there is no potential translation for any subset of the inputcode inside the database.
besides the quality of the data canaffect the results.
for large program inputs a postprocessingstep for verification might be necessary.
b igpt cannot work well on functional programming languages like haskell anderlang because they typically do not contain control flowelements.
future work.
first and foremost there is a potential research direction on creating more convenient and intuitive user interfaces that allow users to make relevant correctionsto translation suggestions and enable the system to convergefaster to the desired result.
second it might also be interestingto look into other forms of user interaction that do not requirethe user to provide corrections in the target language.
third there is still room to explore post processing of cross languagecode retrieval and aggregation of retrieval results to obtaintranslations for larger programs.
finally developing a usererror model that can imitate user errors in code retrievalseems like a promising research direction to better assess thelimitations of retrieval assisted programming.
a cknowledgment this work was funded by the german ministry for education and research as bifold berlin institute for thefoundations of learning and data ref.
01is18025a and ref.01is18037a .
r eferences branching statements.
website.
java nutsandbolts branch.html .
codase.
website.
.
java2csharp.
website.
.
krugle.
website.
.
public git archive.
website.
publicgitarchive .
farouq al omari iman keivanloo chanchal k. roy and juergen rilling.
detecting clones across microsoft .net programming languages.
in19th working conference on reverse engineering wcre .
ieee computer society .
miltiadis allamanis marc brockschmidt and mahmoud khademi.
learning to represent programs with graphs.
in 6th international conference on learning representations iclr .
openreview.net .
miltiadis allamanis hao peng and charles a. sutton.
a convolutional attention network for extreme summarization of source code.
inproceedings of the 33nd international conference on machine learning icml .
jmlr.org .
uri alon meital zilberstein omer levy and eran yahav.
a general path based representation for predicting program properties.
in proceedings of the 39th acm sigplan conference on programming languagedesign and implementation pldi .
acm .
uri alon meital zilberstein omer levy and eran yahav.
code2vec learning distributed representations of code.
proc.
acm program.
lang.
popl .
binger chen and ziawasch abedjan.
rpt effective and efficient retrieval of program translations from big code.
in 43nd international conference on software engineering companion v olume icse .
acm .
xinyun chen chang liu and dawn song.
tree to tree neural networks for program translation.
in 6th international conference on learning representations iclr .
openreview.net .
xiao cheng zhiming peng lingxiao jiang hao zhong haibo y u and jianjun zhao.
mining revision histories to detect cross language clones without intermediates.
in proceedings of the 31st ieee acm international conference on automated software engineering ase .acm .
xiao cheng zhiming peng lingxiao jiang hao zhong haibo y u and jianjun zhao.
clcminer detecting cross language clones without intermediates.
ieice trans.
inf.
syst.
d .
ido dagan and sean p .
engelson.
committee based sampling for training probabilistic classifiers.
in machine learning proceedings of the twelfth international conference on machine learning icml .morgan kaufmann .
behrouz derakhshan alireza rezaei mahdiraji ziawasch abedjan tilmann rabl and v olker markl.
optimizing machine learning work loads in collaborative environments.
in proceedings of the international conference on management of data sigmod .
acm .
jacob devlin ming wei chang kenton lee and kristina toutanova.
bert pre training of deep bidirectional transformers for languageunderstanding.
in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt .
association for compu tational linguistics .
timothy dietrich jane cleland huang and y onghee shin.
learning effective query transformations for enhanced requirements trace re trieval.
in 28th ieee acm international conference on automated software engineering ase .
ieee .
xiaodong gu hongyu zhang and sunghun kim.
deep code search.
inproceedings of the 40th international conference on software engineering icse .
acm .
zack ives yi zhang soonbo han and nan zheng.
dataset relationship management.
in 9th biennial conference on innovative data systems research cidr .
.
zhuoren jiang y ue yin liangcai gao yao lu and xiaozhong liu.
cross language citation recommendation via hierarchical representationlearning on heterogeneous graph.
in the 41st international acm sigir conference on research development in information retrieval sigir .
acm .
toshihiro kamiya shinji kusumoto and katsuro inoue.
ccfinder a multilinguistic token based code clone detection system for large scalesource code.
ieee trans.
software eng.
.
kisub kim dongsun kim tegawend e f. bissyand e eunjong choi li li jacques klein and yves le traon.
facoy a code to code searchengine.
in proceedings of the 40th international conference on software engineering icse .
acm .
nicholas a. kraft brandon w. bonds and randy k. smith.
crosslanguage clone detection.
in proceedings of the twentieth international conference on software engineering knowledge engineering seke .knowledge systems institute graduate school .
jing li aixin sun zhenchang xing and lei han.
api caveat explorer surfacing negative usages from practice an api oriented interactiveexploratory search system for programmers.
in the 41st international acm sigir conference on research development in informationretrieval sigir .
acm .
erik linstead sushil krishna bajracharya trung chi ngo paul rigor cristina videira lopes and pierre baldi.
sourcerer mining andsearching internet scale software repositories.
data min.
knowl.
discov.
.
fei lv hongyu zhang jian guang lou shaowei wang dongmei zhang and jianjun zhao.
codehow effective code search based onapi understanding and extended boolean model e .
in 30th ieee acm international conference on automated software engineering ase .ieee computer society .
lee martie thomas d. latoza and andr e van der hoek.
codeexchange supporting reformulation of internet scale code queries in context t .
in 30th ieee acm international conference on automated software engineering ase .
ieee computer society .
tom as mikolov kai chen greg corrado and jeffrey dean.
efficient estimation of word representations in vector space.
in 1st international conference on learning representations iclr .
tomas mikolov ilya sutskever kai chen gregory s. corrado and jeffrey dean.
distributed representations of words and phrases andtheir compositionality.
in annual conference on neural information processing systems neurips .
anh tuan nguyen tung thanh nguyen and tien n. nguyen.
lexical statistical machine translation for language migration.
in joint meeting of the european software engineering conference and theacm sigsoft symposium on the f oundations of software engineering esec fse .
acm .
anh tuan nguyen tung thanh nguyen and tien n. nguyen.
divideand conquer approach for multi phase statistical migration for sourcecode t .
in 30th ieee acm international conference on automated software engineering ase .
ieee computer society .
anh tuan nguyen zhaopeng tu and tien n. nguyen.
do contexts help in phrase based statistical source code migration?
in ieee international conference on software maintenance and evolution icsme .
ieee computer society .
liming nie he jiang zhilei ren zeyi sun and xiaochen li.
query expansion based on crowd knowledge for code search.
ieee trans.
serv.
comput.
.
v arot premtoon james koppel and armando solar lezama.
semantic code search via equational reasoning.
in proceedings of the 41st acm sigplan international conference on programming language designand implementation pldi .
acm .
razieh rahimi and azadeh shakery.
online learning to rank for crosslanguage information retrieval.
in proceedings of the 40th international acm sigir conference on research and development in informationretrieval shinjuku sigir .
acm .
v eselin raychev martin t. v echev and andreas krause.
predicting program properties from big code .
in proceedings of the 42nd annual acm sigplan sigact symposium on principles of programminglanguages popl .
acm .
steven p .
reiss.
semantics based code search.
in 31st international conference on software engineering icse .
ieee .
baptiste rozi ere marie anne lachaux lowik chanussot and guillaume lample.
unsupervised translation of programming languages.in annual conference on neural information processing systems neurips .
aishwarya sivaraman tianyi zhang guy v an den broeck and miryung kim.
active inductive logic programming for code search.
in proceedings of the 41st international conference on software engineering icse .
ieee acm .
shaowei wang david lo and lingxiao jiang.
active code search incorporating user feedback to improve code search relevance.
inacm ieee international conference on automated software engineer ing ase .
acm .
jingfang xu feifei zhai and zhengshan xue.
cross lingual information retrieve in sogou search.
in proceedings of the 40th international acm sigir conference on research and development in informationretrieval sigir .
acm .
cong yan and yeye he.
synthesizing type detection logic for rich semantic data types using open source code.
in proceedings of the international conference on management of data sigmod .
acm .
cong yan and yeye he.
auto suggest learning to recommend data preparation steps using data science notebooks.
in proceedings of the international conference on management of data sigmod .acm .
pengcheng yin graham neubig miltiadis allamanis marc brockschmidt and alexander l. gaunt.
learning to representedits.
in 7th international conference on learning representations iclr .
openreview.net .
xingquan zhu peng zhang xiaodong lin and y ong shi.
active learning from data streams.
in proceedings of the 7th ieee international conference on data mining icdm .
ieee computer society .