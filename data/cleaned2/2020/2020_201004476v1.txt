modular collaborative program analysis in opal dominik helm helm cs.tu darmstadt.de department of computer science technische universit t darmstadt germanyflorian k bler kuebler cs.tu darmstadt.de department of computer science technische universit t darmstadt germanymichael reif reif cs.tu darmstadt.de department of computer science technische universit t darmstadt germany michael eichberg mail michael eichberg.de department of computer science technische universit t darmstadt germanymira mezini mezini cs.tu darmstadt.de department of computer science technische universit t darmstadt germany abstract current approaches combining multiple static analyses deriving different independent properties focus either on modularity or performance.
whereas declarative approaches facilitate modularity and automated analysis independent optimizations imperative approaches foster manual analysis specific optimizations.
in this paper we present a novel approach to static analyses that leverages the modularity of blackboard systems and combines declarative and imperative techniques.
our approach allows exchangeability and pluggable extension of analyses in order to improve sound i ness precision and scalability and explicitly enables the combination of otherwise incompatible analyses.
with our approach integrated in the opal framework we were able to implement various dissimilar analyses including a points to analysis that outperforms an equivalent analysis from doop the state of the art points to analysis framework.
ccs concepts software and its engineering abstraction modeling and modularity automated static analysis theory of computation program analysis parallel computing models .
keywords static analysis blackboard system modularization composition parallelization acm reference format dominik helm florian k bler michael reif michael eichberg and mira mezini.
.
modular collaborative program analysis in opal.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction solving complex static analysis problems often involves solving several distinct but interdependent sub problems.
analyzing a method s purity e.g.
involves interdependent mutability sub analyses of classes and fields as well as an escape analysis .
traditionally static analyses have been implemented in an imperative monolithic style i.e.
one super analysis computes the results of all sub problems.
not only do monolithic designs become complex when mutually dependent problems are involved .
more importantly individual sub analyses cannot be developed in isolation cannot be reused for other analyses and cannot easily be added removed and exchanged to trade off between precision sound i ness and performance in a fine tuned way i.e.
to enable pluggable precision sound i ness performance.
to address these requirements it is desirable to encode solutions for sub problems of a complex static analysis in separate modules.
however while encoded in independent modules the execution of inter dependent sub analyses needs to be interleaved to enable exchanging intermediate results.
the latter is often necessary for optimal precision as has been proven by the theory of reduced products in abstract interpretation and was more recently demonstrated for other kinds of analyses .
recently declarative approaches to static analysis using the datalog language are gaining increased popularity especially in the area of points to analyses .
such approaches nicely support the requirements stated above.
analyses are implemented as sets of rules that are evaluated by an underlying constraint solver.
thus complex analyses can be broken down into simpler independently developed analyses.
the underlying solver transparently resolves their dependencies and propagates intermediate updates according to the specified rules thus enabling interleaved execution.
moreover the solver can a apply analysisindependent optimizations e.g.
by rearranging the computation order although manual optimization is still necessary and or b automatically parallelize the execution .
however using datalog and giving solvers full control comes with drawbacks in terms of both performance and generality .
first it is not possible to exploit analysis specific knowledge in managing the execution and dependencies of the analyses.
such knowledge can help boost scalability.
for example an imperative purity analysis that determines whether a method is deterministic by amongarxiv .04476v1 oct 2020esec fse november virtual event usa dominik helm florian k bler michael reif michael eichberg and mira mezini others checking the mutability of fields f1 ... f ncould drop further checks as soon as any fiis found to be mutable.
a declarative analysis whose execution is driven by a general purpose solver cannot take this short cut.
analysis specific knowledge is also valuable to correctly compose incompatible optimistic and pessimistic analyses as defined in .
second the datalog solver uses analysis independent data structures and analyses cannot exploit data structures that are tailored for their specific needs.
such optimized data structures like tries can be crucial for achieving performance.
finally the fully declarative approach fosters a one size fitsall style limiting generality.
for instance by relying on relations datalog based approaches support only set based lattices while many common analyses require other kinds of lattices.
constant propagation e.g.
is usually implemented via singleton value based lattices making it infeasible to implement it using datalog .
in this paper we address these issues of declarative approaches without comprimizing on their benefts.
specifically we propose a novel generic approach together with a proof of concept implementation in the opal framework for lattice based fixedpoint computations with support for lattices of any kind including singleton value based interval and set lattices.
like fully declarative approaches it features modular analyses encoded as independently compilable exchangeable and extensible units.
however it does not rely on a general purpose declarative framework and constraint solver.
it offers a specialized approach mixing imperative and declarative styles.
the developer of an opal analysis implements its core functionality imperatively but declaratively specifies its dependencies e.g.
the lattice that the analysis computes and lattices it depends on to do so as well as several constraints regarding its execution.
dependencies and constraints are automatically handled by our custom solver during analysis execution.
our architecture is reminiscent of blackboard systems dependent analyses implemented in decoupled modules coordinate their executions implicitly by writing into and reading from a central data structure the blackboard .
whenever new intermediate results are written to the blackboard the solver automatically and concurrently schedules the execution of dependent analyses to satisfy the declaratively specified dependencies and constraints.
like declarative approaches we decouple mutually dependent analyses enabling their isolated development and interleaved parallel execution out of the box.
at the same time we improve over declarative approaches in two regards.
first beyond automatic and transparent optimizations and parallelization by featuring an imperative programming style within each analysis module our approach enables analysis specific optimizations and data structures.
the possibility to specify fine grained analysis specific constraints enables further optimizations e.g.
suppressing interleaved execution of some analyses to avoid unnecessary intermediate computations.
second with a custom solver that is agnostic of the lattices used by analyses our approach is generic and supports arbitrary kinds of analyses.
using it one can naturally express dataflow and constraint based analyses based on arbitrary lattices.
moreover declarative declarations enable opal to consider analysis specific constraints in managing dependencies.
to the best of our knowledge this is the first approach to correctly compose lazily computed incompatible optimistic and pessimistic analyses.to recap this paper contributes a list requirements on frameworks for collaborative static analysis that is distilled from three case studies section .
a novel approach that satisfies all these requirements section .
it advances the state of the art in implementing modular inter dependent analyses.
a thorough evaluation of the approach that supports our claims on generality showcases its modularity features points out performance improvements over doop the stateof the art declarative framework and provides promising results for parallelization section .
background and terminology in this section we shortly introduce blackboard systems and present terminology used throughout the paper.
blackboard systems use a central data structure the blackboard to coordinate the collaborative work of otherwise decoupled knowledge sources .
the latter contribute partial information to the blackboard towards collaboratively reaching an overall goal.
the blackboard notifies knowledge sources about availability of new information they might require through a control mechanism that decides which knowledge sources should be executed in what order.
the information can then be queried by the knowledge sources which execute and produce further information.
each execution of a knowledge source is called an activation .
entity the term is used to characterize anything one can compute some information for.
entities can be concrete code elements e.g.
classes methods or allocation sites or abstract concepts such as all subtypes of a class.
the set of entities is not necessarily defined a priori and can be created on the fly while analyses execute.
property kind the term characterizes a specific kind of information that can be computed for an entity e.g.
mutability of classes purity of methods or callees of a specific method.
each property kind represents one lattice of possible values.
property the term characterizes a specific value in the lattice of some property kind that is attached to some entity e.g.
a class can be mutable or immutable a method can be pure or impure a specific method may invoke a specific set of methods.
per entity at most one property of a specific kind can be computed.
analysis the term characterizes an algorithm that given an entity computes its property of a certain kind.
we say that an analysis computes a property kind as shorthand for an analysis computes properties of that property kind for a given kind of entity .
analyses are knowledge sources in the sense of the blackboard architecture the properties they compute constitute the information that they contribute to and or query from the blackboard.
case studies we discuss case studies involving several interrelated sub analyses to distill a list of requirements on static analysis frameworks.
during the discussion we emphasize concepts whenever they occur.
the case studies represent very dissimilar kinds of analyses.
in particular they require different kinds of lattices including singleton value lattices e.g.
in .
and set based lattices e.g.
in .
.
this motivatesmodular collaborative program analysis in opal esec fse november virtual event usa the first requirement static analyses frameworks must support varied domain lattices r1 .
.
three address code the first case study is an analysis to produce a three address code representation tac of jvm bytecode presented in more detail in previous work .
in its basic version tac uses def use type and value information including constant propagation provided by an abstract interpretation based analysis ai .
to increase precision ai may be enhanced with analyses that refine type and the value information for method return values and fields.
however such additional analyses may negatively affect the runtime.
hence systematic investigation of the precision performance trade off is needed to decide whether to use such additional analyses on a caseby case basis.
to this end a separation into modules that can be enabled disabled is beneficial.
in general we derive the following requirements regarding support for modular pluggable analyses.
for systematically studying precision soundness performance trade offs static analysis frameworks should support en disabling inter dependent analyses r2 .
to maximize pluggability analyses should be defined in decoupled modules and yet be able to collaboratively compute properties collaborative analyses .
as individual analyses can be disabled it should be possible to specify soundly over approximated fallback values1for the properties they compute in lack of actual results r3 .
moreover an approach for modular collaborative analyses should support their interleaved execution without them knowing about each other s existence r4 .
two analyses are executed interleaved if they can interchange intermediate results .
this is important for optimal precision knowledge gained during the execution of analysisa1may be used by the execution of another analysis a2 on the fly to refine its result and in turn this may enable further refinement for a1.
the precision of field and return value refinement analyses profits from interleaved executions as they depend on each other cyclically.
if a method mreturns the value of a field f then m s return value depends on f s value.
if the value returned by mis written into f then f s value also depends on m s return value.
however interleaved execution must in specific cases be suppressed to ensure correctness.
this is the case for the composition of pessimistic andoptimistic analyses.
pessimistic analyses start with a sound but potentially imprecise assumption and eventually refine it.
optimistic analyses start with an unsound but over precise assumption and progress by reducing over precision towards a sound result.
field and return value refinement analyses are pessimistic the declared return type of method m say list is a sound but eventually imprecise initial value for the return value analysis during the execution the analysis may find out that mactually returns the more precise result say arraylist .
ai is an optimistic analysis it starts with the unsound assumption that all code is dead and refines it by adding statements found to be alive towards a sound but potentially less precise result.
optimistic and pessimistic analyses are incompatible for interleaved execution because they 1to minimize the effect of fallback values on precision it makes sense to compute the fallback by using locally available information e.g.
using declared type information instead of always returning the same over approximated value.refine the respective lattices in opposite directions.
as a result exchanging intermediate results may cause inconsistencies thereby violating monotonicity.
thus the analysis framework must enforce that only final results of pessimistic analyses are passed to dependent optimistic analyses and vice versa avoiding interleaving and suppressing non final updates r5 .
for illustration consider the example of some piece of code say c that contains a call to a method m1that is mutually recursive with a method m2 but is conditioned on a field fbeing an instance oflinkedlist .
to analyze c we combine a field value analysis fa anaianalysis and a call graph construction algorithm cg.
assume that fa which is a pessimistic analysis initially reports the type of the field fto be list .
given this information ai would optimistically consider cto be live and cg hence will consider both m1andm2to be reachable.
because of the mutual recursion and also because of the monotonicity requirement this result cannot be changed later if fafinds out that fcan only contain arraylist s. if however the latter information was present when ai analyzed the code cwould have been marked as dead and cgwould have markedm1andm2as unreachable.
thus the results of this combination of analyses is non deterministic and possibly incorrect imprecise if m1andm2are falsely reported to be reachable .
.
modular call graph construction inter procedural analyses presume a call graph cg given method m cg provides information about a methods that may be invoked at a call site in m callees and b call sites from which mmay be invoked callers .
we use the cg to motivate the need for supporting further kinds of execution interleaving beyond r4 as well as further requirements.
the previous case study illustrated the need for interleaved execution of inter dependent analyses that calculate different properties and operate on different entities composition of analyses for refining field and return values with tac .
the cg use case illustrates two further kinds of interleaved execution.
first we need interleaved execution of multiple instances of the same analysis operating on different code entities to collaboratively compute a single property whereby each instance contributes partial results r6 .
for example different executions of a cg analysis for different callers of a method mneed to contribute their partial results to collaboratively derive all of m s callers computing callers of a method is inherently non local .
second we also need to support interleaving of independent analyses that collaboratively compute a single property r7 .
consider e.g.
the computation of the callees of m. a cg analysis can in principle consider min isolation.
a monolithic analysis for callees is nonetheless not suitable.
it makes sense to distinguish between one sub analysis that handles standard invocation instructions e.g.
cha rta points to based analysis and sub analyses dedicated to non standard ways of method invocation through specific language features e.g.
reflection native methods or functionality related to threads serialization etc.
non standard invocation requires specific handling e.g.
one may deliberately not want to perform reflection resolution or may want to perform it based on dynamic execution traces .
by offering such specialized analyses as decoupled modules they become highly reusable and can be combined with different call graph analysis for standard invocationesec fse november virtual event usa dominik helm florian k bler michael reif michael eichberg and mira mezini instructions.
this makes the call graph construction highly configurable for fine tuning its performance and sound i ness.
hence not only a method s callers but also its callees need to be computed collaboratively.
this time different analyses targeting different language features rather than different executions of the same cg analysis contribute to the same property.
handling special language features may even rely on integrating results of external tools or precomputed values r8 .
for instance one may choose to integrate the results of tamiflex for reflective calls or external tools for analyzing native methods.
the cg case study also motivates support for specifying precise default values r9 in addition to sound fallback values .
consider the case of an unreachable method m. the cg analysis will never compute callees or caller information for m. however this lack of results is an inherent property of the entity and not the result of a missing disabled analysis.
a sound fallback value to compensate the deactivation of the cg module for mmay have to include all methods and hence be too imprecise.
instead analyses depending on the cg should get the information that mis unreachable the precise default value.
the analysis developer knows such information and should be enabled to tell the framework.
another requirement is motivated by the cg.
the cg construction unfolds along the transitive closure of methods reachable from some entry points.
hence it does not make sense to execute the decoupled modules collaboratively constructing the cg each handling a particular language feature globally on all methods of a program.
instead they should be triggered only when the overall analysis progress discovers a newly reachable method.
hence the framework must support triggering analyses once the first intermediate result for a property is recorded r10 .
our previous work provides empirical evidence that encoding an rta sub analysis and sub analyses for language specific features as collaborative interleaved modules results in more sound call graphs and better performance compared to call graph analyses of the soot wala and doop frameworks.
.
mutability escape and purity analysis the analyses in this subsection illustrate the need for further kinds of activation modes in addition to triggered analyses illustrated in the previous subsection a eager analyses which refers to computing an analysis for all entities in the analyzed program and b lazy analyses i.e.
executing an analysis a1only for the entities for which the property that a1computes is queried by some potentially the same analysis a2.
a further requirement shown by analyses in this subsection is that the framework should allow analyses to enforce an execution order that overrides the one determined by the solver.
the use case involves analyses for method purity class and field mutability and escape information .
the latter includes aggregated information on field locality and return value freshness cf.
.
the analyses in this case study interact tightly and compute properties that may be relevant for both end users e.g.
method purity and further analyses e.g.
escape information .
complex dependencies exists between all these analyses.
to finetune the precision performance trade off several analyses for these property kinds with different precision can be exchanged as needed all are optimistic and use tac and or the cg information.table summary of requirements lattices and values r1 support for different kinds of lattices .
.
.
r3 fallbacks of properties when no analysis is scheduled .
.
r9 default values for entities not reached by an analysis .
composability r2 support for enabling disabling individual analyses .
.
.
r4 interleaved execution with circular dependencies .
.
.
r5 combination of optimistic and pessimistic analyses .
r6 different activations contributing to a single property .
r7 independent analyses contributing to a single property .
initiation of property computations r8 precomputed property values .
.
r10 start computation once an analysis reaches an entity .
r11 start computation eagerly for a predefined set of entities .
r12 start computation lazily for entities requested .
.
r13 start computation as guided by an analysis .
since the results of analyses in this case study may be of interest to the end user it is useful to compute them for all possible entities eagerly r11 e.g.
computing the mutability of all fields in the program.
however when the field mutability is only used to support e.g.
the purity analysis it may be beneficial for performance reasons to compute it lazily r12 i.e.
only for the fields for which mutability is queried by the purity analysis.
this illustrates that we need both eager and lazy execution modes.
eager and lazy versions of the same analysis should typically share the code and only be registered with the framework in different ways.
the class mutability analysis also illustrates the need to configure the framework with analysis specific execution orders r13 for performance reasons it makes sense to analyze classes in a program in a top down order starting with parent classes before their children.
our previous work provides empirical evidence for the requirements stated in this section.
an implementation of the purity sub analysis of this case study and through transitive use the mutability and escape sub analyses as collaborative analyses with interleaved execution showed higher precision more fine granular results and similar performance characteristics compared to the then state of the art purity inference tool reim .
.
interim summary table summarizes the requirements along the case studies motivating them.
existing frameworks do not satisfy all of them.
imperative frameworks lack support for modularity especially r5 r6 and r7.
declarative approaches e.g.
doop have other limitations being bound to relations for modeling properties they can not express the range of different analyses represented by our case studies r1 .
they also fail to support sound interactions between incompatible analyses r5 .
by giving the solver full control they do not support different analysis specific activation modes r10 r13 .
approach opal is the first static analysis framework to build upon the concept of blackboard systems static analysis modules correspond tomodular collaborative program analysis in opal esec fse november virtual event usa 1sealed trait classmutability extends propertykind deffallback type theclass mutableclass 4case object immutableclass extends classmutability 5case object mutableclass extends classmutability listing class mutability lattice knowledge sources the store that manages the computed properties corresponds to the blackboard.
opal combines imperative and declarative programming styles for analyses.
the developer of an analysis a a implements the lattice representation of the property values computed by a .
b implements two imperative functions so called initial analysis function iaf respectively continuation function cf .
c declares the property kinds computed by a and properties adepends on .
and d defines how a s results are reported to the blackboard .
.
guided by the declared dependencies the blackboard and its fixed point solver coordinate the execution of the analyses thereby e ensuring all execution constraints .
f performing fixed point computations whenever circular dependencies are involved .
and g automatically scheduling and parallelizing the execution of analyses .
.
.
representing properties values of a property kind constitute a lattice structure.
opal supports singleton value based interval or set based lattices are possible r1 .
a lattice s bottom value models the best possible value e.g.
pure for method purity its top value the sound over approximation e.g.
impure .
lattices must satisfy the ascending descending chain condition to ensure termination of optimistic pessimistic analyses.
when defining a property kind developers can choose the most suitable data structures for efficiency.
developers can also specify fallback and default values.
the blackboard will return the fallback value for some requested property pof kind k if no analysis is available for k r3 .
as it is a sound over approximation the lattice s top value is a good choice however the fallback value can also be provided by a proxy analysis function that does not query the blackboard avoiding cyclic dependencies.
the blackboard will return a default value forp if an analysis is available but did not produce any result for some entity r9 .
for instance call graph analyses only examine methods reachable from entry points for any non reachable method m a default value can be used to state that mis dead and has no relevant callees.
a sound fallback value would include all possible methods as callees of m thus in this case the default value provides more information than a fallback value.
if no default value is declared the fallback value is returned.
developers implement property kinds by specifying an interface which can be used to access and manipulate the property values.
when the propertykind trait is extended the framework assigns an identifier which can be used to query the blackboard for properties of that kind.
listing shows exemplary scala code of a simple class mutability property kind.
lines to define the base trait for the property kind and give a sound fallback value in line .
the two possible property values are defined in lines and .iafsblackboard fixed point solver1.
query dependent properties .
current status .
initial results dependencies cfs4.
repeated invocation on updates .
updated results dependencies figure overview .
analysis structure an overview of opal s analysis structure is shown in figure .
as mentioned the analyses are structured in two parts an initial analysis function iaf and one or more continuation functions cfs .
these functions can be implemented in any way as long as they provide their results as defined by the property kind.
for each entity eto be analyzed by a a s iaf is executed.
the iaf collects information directly from e s bytecode in order to compute its result.
if it needs additional information pertaining to some other entity eor from another analysis that computes a property kind k the iaf queries the blackboard for these dependencies using the identifiers of eandkto find the relevant information arrow .
in figure .
the blackboard will return the currently available value .
.
this value may however not be available or not final either because the respective analysis was not yet executed or because it has dependencies that yet need to be satisfied.
once the iaf completes analyzing the entity it returns to the blackboard a a result computed based on the currently available information and b any remaining dependencies along with a continuation function cf .
.
similar to the solver of declarative frameworks the blackboard resolves dependencies and automatically invokes the cfs whenever updates to these dependencies become available .
.
on completion cfs also return their updated results to blackboard .
potentially triggering the execution of other cfs.
while the iaf is written imperatively dotted queries in figure the subsequent execution is performed similar to declarative frameworks straight lines by having results declare their dependencies and the solver being responsible to satisfy them.
executions of the iafs and cfs are called analysis activations .
to ensure determinism opal executes the activations for a single property sequentially while iafs and cfs for other properties can execute concurrently.
as analyses get notified about dependency updates through the invocation of the cf it is not necessary that dependencies are computed before or when they are queried.
instead they can be computed asynchronously and lazily i.e.
on demand r12 .
this also allows opal to handle cyclic dependencies r4 .
apart from adhering to this basic structure developers may use any suitable strategy to implement an analysis a.amay e.g.
focus on specific statements instead of traversing the entire code of a method opal provides pre analyses to query specific parts of the code e.g.
all statements that access a specific field .
also analyses can internally use any data structure suitable to achieve good performance.
for illustration listing shows an excerpt from a simple class mutability analysis initial analysis function.
the iaf is given the entity to analyze line .
lines to show how to retrieve and handle properties required to compute the iaf s result esec fse november virtual event usa dominik helm florian k bler michael reif michael eichberg and mira mezini 1defanalyze type theclass blackboard.get field fieldmutability match case mutablefield return result theclass mutableclass case dependee immutablefield if !dependee.isfinal dependees field dependee result theclass immutableclass dependees continuation listing class mutability analysis the required property the mutability of an instance field of the analyzed class is queried from the blackboard line and based on the returned value the iaf may compute its result as in line or keep the dependency in a list of dependees line to return it alongside an intermediate result later line .
line also specifies the continuation function to be invoked when any of the properties independees is updated.
we do not show the code for that cf here as its implementation is very similar to lines to i.e.
based on the updated value the intermediate result of the cf is determined.
there are two semantic constraints that the implementations of the analyses must satisfy though.
first they must ensure monotonicity of result updates according to the used lattice.
analyses that optimistically start at a lattice s bottom value may only refine approximations upwards pessimistic analyses only downwards.
opal can automatically check the monotonicity of updates.
monotonicity allows analyses to know which refinements of intermediate results are still possible.
second analyses must be scheduling independent whenever they receive the value of some other property they depend on they must use the information provided by that value to compute the result of the current activation i.e.
they may not defer the incorporation of the newly gained information to a later activation of a continuation function.
this ensures that all available information is used independent of whether the continuation is later scheduled for execution an activation may never occur in case of cyclic dependencies.
for example once the mutability analysis of a class cknows that c s instance field fis mutable it may no longer report that ccould be immutable.
the developer of some analysis amust ensure that ais scheduling independent.
.
declarative specifications on top of the iaf and cf the developer of an analysis aspecifies a the property kinds computed by a b its dependencies c on which entities awill be executed and d when the blackboard should start a sexecution.
these specifications are evaluated when the analysis is registered with the blackboard before the latter takes over control of analysis activation.
when registering analyses developers may also report precomputed values to the blackboard r8 .
the specification of the computed property kinds also states whether intermediate results are optimistic or pessimistic and whether the analyses contributes to a collaborative computation or intends to be the only analysis computing the specified property kinds.
dependency specifications state other property kinds1override def deriveslazily optimistic classmutability 2override def uses set optimistic fieldmutability 3override def register blackboard.set type.object immutableclass valanalysis new classmutabilityanalysis blackboard.registerlazyanalysis this analysis.analyze listing registration of class mutability analysis on which adepends which aqueries and whether acan process optimistic pessimistic intermediate values or final values only.
analyses can eagerly select a set of entities e.g.
all methods of the analyzed program if it is likely necessary to perform the analysis for all of these entities r11 .
this is e.g.
useful for analyses that are of interest to the end user e.g.
if the user is interested in the purity of all methods.
alternatively analyses can be registered to be invoked lazily .
lazy analyses only compute a property if that property is queried r12 by another analysis or by the end user.
finally an analysis can specify a property kind ksuch that it is started for every entity for which khas been computed r10 .
some analyses benefit from enforcing a specific order for computing the properties for different entities r13 .
for instance the class mutability analysis benefits from traversing the class hierarchy downwards such that results for a parent class are available before any subclass is analyzed.
in opal this is supported by enabling the developer of an analysis ato declare a number of computations to be scheduled whenever areturns a result to the blackboard.
for illustration listing shows the registration code for a class mutability analysis.
line declares that the analysis optimistically and lazily derives class mutability.
line declares that in performing its computation it may require field mutability and that it can handle intermediate results for this property if they were computed optimistically.
this declaration is complete no property kinds other than field mutability and class mutability may be queried by this analysis.
line registers a predefined value stating that the base class object is immutable r8 .
the iaf analyze is registered as a lazy analysis in line i.e.
the mutability of a certain class will only be computed on demand e.g.
when a purity analysis queries it.
.
reporting results as already mentioned analyses write intermediate and final results to the blackboard.
they can report results for each single entity individually or for multiple entities at the same time.
a result consists of a single lattice value representing the new value for the property or of an update function uf for updating the property s current value as recorded in the blackboard to incorporate the new result.
a uf is used for properties whose computation is not localized to a specific part of the program e.g.
the callers of a method.
for such properties constraint based analyses have been used in the past declarative analyses also provide such updates called deltas that only specify the change to the property value instead of the full new property value.
the ufmerges the results of one activation to the current state of the property e.g.
add a new caller to an existing set of callers .
this way activations of one or of different analyses can collaboratively contribute to a property r6 r7 .modular collaborative program analysis in opal esec fse november virtual event usa .
execution constraints once the end user chooses a set of analyses to be executed r2 opal uses the declarative specifications section .
to check and automatically enforce restrictions on analyses that can be executed together.
first it ensures that any property kind is computed by at most one analysis or collaboratively this is to avoid that conflicting results are reported to the blackboard.
second if several analyses derive a property kind collaboratively opal ensures that they are all either optimistic or pessimistic.
finally opal ensures that all property kinds required by any analysis are derived by another analysis or there is a fallback value provided this is to ensure that dependencies can be satisfied.
opal s blackboard may run optimistic and pessimistic analyses simultaneously.
but when so it ensures that no intermediate results are propagated between them r5 .
given property kind p that is computed optimistically and pessimistic analysis adepending onp opal does not forward any intermediate values of pto a s cf.
the latter is triggered only when a value of pis submitted marked as final.
we say that the dependency of aonpissuppressed .
there are subtle interactions between dependency suppression and cyclic and collaborative computations which we explain next.
first there can be no cyclic dependencies between pessimistic and optimistic analyses.
the correctness of cyclic dependency resolution relies on the assumption that all intermediate approximations have been processed and no further updates to any property involved in the cycle may happen cf.
section .
.
this obviously is not the case when updates are suppressed.
the interaction between dependency suppression and collaboratively computed properties is more involved.
assume a collaboratively computed property p1that transitively depends on another collaboratively computed property p2and consider the case when one or more of the transitive dependencies between them is suppressed2.
in this case opal must ensure that p2 s values are committed as final before p1 s values can be committed as final too.
this ensures that final values have been propagated along the suppressed dependencies.
to this end opal derives a commit order when checking the execution constraints before executing analyses.
the commit order is a partial order between collaboratively computed property kinds p1must be finalized later than any other collaboratively computed property kind p2on whichp1depends when there is suppression between them.
suppression of intermediate updates can also be used to improve performance consider the relation between tac and ai.
both are optimistic and tac could use intermediate ai results.
but these results are typically not useful hence it can be beneficial to use suppression to avoid costly computation of these intermediate results and instead compute the tac only once on the final ai result.
.
fixed point computation computation is started for the entities selected by eager analyses r11 cf.
section .
.
whenever intermediate values for properties are submitted the blackboard schedules activations of continuation functions distributing updated results to analyses that depend 2on a chain of dependencies more than one may be suppressed.
also if p1depends onp3andp4and each of those depends on p2 there is more than one path between p1andp2 on which dependencies may get suppressed.on them.
additionally the blackboard starts new computations by invoking the initial analysis function for properties that are requested lazily r12 are triggered by some analyses reaching a certain entity r10 or whenever it is guided to do so by running analyses r13 .
this process of scheduling iaf and cf activations is performed until no further updates are generated the blackboard has reached a quiescent state.
at this point however the properties values may not necessarily be final as there still may be unresolved dependencies.
there are three cases to be considered.
first an analysis was scheduled for some property kind p but it did not analyze some entity e for which pwas requested e.g.
becauseewas not reachable in the call graph.
in this case the default value r9 is inserted which may trigger further computations until quiescence is reached again.
second properties that cyclically depend on each other are not finalized yet.
if such properties form a closed strongly connected component i.e.
they do not have any dependees outside of the cycle but other properties may still depend on them they are now finalized to their current value.
by requiring analyses to report their results in a monotonous and scheduling independent way cf.
section .
opal guarantees that the cycle resolution is deterministic and sound.
again further computations may arise from resolving cyclic dependencies including supplying more default values and resolving further cycles until quiescence is reached again.
finally the blackboard finalizes values for collaboratively computed properties.
it respects the commit order computed previously cf.
section .
after finalizing a set of collaboratively computed properties computation is resumed again.
only once quiescence is reached again the next property kinds as given by the commit order are finalized.
this is repeated until all collaboratively computed properties have been finalized.
.
scheduling and parallelization blackboard systems require a control component that upon updates of the blackboard decides which knowledge sources to activate next.
in our case this control component determines the order in which activations of dependent analyses are executed and is called scheduler .
the order in which dependent analyses are activated can have significant effects on performance .
opal allows for the scheduler to be easily exchanged in order to select the best performing one for any chosen set of analyses.
apart from general strategies such as first in first out more specific algorithms may use the dependency structure or the values of intermediate approximations to decide the scheduling order.
this is similar to the control component of blackboard systems asking knowledge sources for an estimated information gain cf.
.
blackboard systems lend themselves well to parallelization.
the individual knowledge sources i.e.
analyses in our case are decoupled and their activations both the initial analysis and the continuations can be executed in parallel on multiple threads.
updates to the blackboard on the other hand can be synchronized on a special thread or if that becomes a bottleneck distributed to several threads based on the property kind and or entity.
a simple implementation may consist of several threads that use a shared data structure holding the property data and use locks or other mechanisms to synchronize accesses to this shared storage.esec fse november virtual event usa dominik helm florian k bler michael reif michael eichberg and mira mezini .
summary our approach fosters strong decoupling of reified lattices choice of data structures analyses choice of algorithm and the solver infrastructure the concrete fixed point solving implementation .
this enables exchanging and optimizing these parts independently.
as reified lattices are the basis for all communication between analyses different versions of analyses can be implemented at different trade offs.
the solver manages execution of analyses tracks dependencies and propagates updates performs monotonicity checks and computes the fixed point solution.
evaluation we evaluate our approach by answering the following questions rq1 does opal support modularization of a broad range of static analysis kinds with varying requirements?
rq2 does exchangeability of analysis modules benefit the end user and the developer?
rq3 can the framework be parallelized?
rq4 what is the benefit of analysis specific data structures?
rq5 how does the performance of opal s analyses compare to state of the art declarative approaches?
we implemented our approach on top of the scala based opal framework for jvm bytecode analysis .
however the approach is framework and language independent.
we answer the above research questions using the case studies of section to analyze the dacapo benchmark .
we choose dacapo because doop which we compare to in section .
has special support for it.
both the implementation of opal as well as the case studies are available in the opal github repository3.
all measurements were performed in a docker container4on a server with two amd r epyc r .
ghz cores threads each cpus and gb ram.
analyses were run using openjdk .
.
bit with gb of heap memory and scala .
.
.
experiments were run seven times and we report their median runtime.
we report only excerpts of the results here5.
.
support for various analyses to answer rq1 we implemented the case studies from section using opal and argue that these are representatives of different analysis kinds.
the first case study represents pessimistic analyses in the context of improving precision of a three address code representation tac it shows how basic analyses can be extended by analyses that are specialized to increase the precision of sub problems solutions.
the modular call graph of the second case study involves tightly interacting yet decoupled analyses e.g.
points to and call graph and demonstrates how one can plug in further modular analyses that handle special cases of java in order to increase the call graph s soundness.
the third case study introduced several exchangeable analyses for different high level properties immutability escape information purity .
the individual analyses are relatively simple and can focus on their respective property but by using the results of other analyses they can be more precise than a corresponding monolithic analysis of medium complexity.
5the entire results can be found here purity results for different configurations hsqldb configuration pure sef other impure analysis pa2 fma e1 .
s pa2 e1 .
s pa2 fma e0 .
s pa2 .
s pa1 fma .
s pa0 fma .
s pa0 .
s as discussed in section to achieve this modularity several requirements need to be satisfied cf.
table .
section already explained how opal supports all of them.
on the contrary as we argue in section .
no current imperative or declarative framework supports all these requirements.
we additionally implemented a solver for inter procedural finite distributive subset problems ifds a well known general framework for dataflow problems based on graph reachability.
similar to other ifds solvers e.g.
heros users provide a domain for their dataflow facts and four flow functions that together specify the ifds problem.
the solver starts one computation per pair of method and entry dataflow fact and these tasks need to communicate their results.
we chose ifds as it is a general framework that allows implementing many dataflow analyses and it is dissimilar from the three case studies analyses.
in particular it shows opal s support for implementing general solvers as individual analyses.
opal s programming model enables the implementation of dissimilar analyses fostering their modularization into a set of comprehensible maintainable and pluggable units.
opal is the only static analysis framework satisfying all requirements from section .
.
.
effects of exchangeability of analyses our approach strictly decouples property kinds from analyses computing them.
thus it can provide different analyses computing the same property kind to cover a wide range of precision sound i ness and performance trade offs.
two experiments examine how this exchangeability fosters rapid probing thus benefiting the analysis developer and end user alike rq2 we explore the impact on precision in one experiment and that on soundness in the second.
in our first experiment we run various configurations of our purity analysis cf.
section .
with different supporting analyses for field mutability or escape information with different precisionscalability trade offs.
no other tool supports similar exchangeability of interacting purity mutability and escape analyses.
table shows the results for hsqldb .
higher indices indicate more precise analyses.
comparing the least precise analysis pa0with the most precise pa2 fma e1 we observe a reduction in the number of reported impure methods by but a runtime slowdown by .6x.
some configurations even have a large impact on runtime for almost no gain in precision e.g.
comparing the most precise one with that using simpler escape analysis e .
in the second experiment we evaluate the rta call graph with different supporting modules for different jvm features.
while doop computes call graphs and offers some modularity e.g.
for reflection no other tool so far includes such fine grained modules formodular collaborative program analysis in opal esec fse november virtual event usa table results for different call graph modules for xalan configuration reachable methods edges analysis rta .
s rta c .
s rta r .
s rta x .
s rta c x .
s rta s t f c x .
s c configured native methods r reflection x tamiflex s serialization t threads f finalizer call graphs.
also doop does not support rta but points to based call graphs only.
results for xalan are shown in table displaying the active modules the number of reachable methods rms call edges and respective construction time.
while some configurations discover more methods edges than others they may discover different sets of methods edges.
a configuration is only guaranteed to be strictly more sound if it uses a strict superset of modules.
compared to the baseline rta with support for preconfigured native methods rta c reaches more methods and more call edges.
reflection support rta r brings over more rms and call edges at the same time construction time increases by about .
using the tamiflex rta x module instead increases call graph size and soundness more but introduces further slowdown.
with all modules enabled we reach more methods and more call edges at the cost of a increased runtime.
moreover the data suggests that different modules benefit different projects.
tamiflex impacted xalan andjython reflection fop and serialization hsqldb .
thus which modules are more relevant than others may differ between different programs and it may be worth investigating tradeoffs even at the level of individual projects.
overall both experiments confirm that opal maintains exchangeability benefits from datalog based analyses while generalizing these results to a broader range of lattices.
opal facilitates systematic investigation of different configurations supporting users and developers in finding the best trade off between precision sound i ness and scalability.
.
parallelization we implemented a proof of concept parallel version of our blackboard control rq3 .
using this we measured the execution time for the points to based call graph with different numbers of threads.
results for five dacapo projects are shown in figure .
the projects were selected to have similar runtime to facilitate graph readability the other projects show similar behavior.
benefits of parallelization over one thread appear at two to four threads and we achieve speedups of up to 2x for threads.
beyond this further improvement is negligible instead it slightly decreases due to growing communication overhead.
these results are encouraging given that the parallel version is not at all optimized.
an optimized version of it is expected to scale better.
designing such an optimized version requires further research to identify the optimal way to parallelize the computation.
opal s computation can be parallelized and that parallelization holds potential for increased performance.
t hreads01020304050runtime s antlr xalan chart eclipse luindexfigure parallel architecture performance table runtime and size of points to based call graphs doop opal opal project compile facts analysis rm runtime rm scala antlr s s s .
s .
s bloat s s s .
s .
s chart s s s .
s .
s eclipse s s s .
s .
s fop s s s .
s .
s hsqldb s s s .
s .
s jython s s s .
s .
s luindex s s s .
s .
s lusearch s s s .
s .
s pmd s s s .
s .
s xalan s s s .
s .
s geo.
.
s .
s .
s .
s .
s .
benefits of specialized data structures to answer rq4 we compare two versions of the same points to based call graph algorithm.
both encode points to caller and callee information as integer values.
the first version uses specialized trie based data structures the second one uses standard scala sets.
results are given in the sixth and last column of table .
due to its high memory consumption we had to run the version using scala s data structures with gb of heap space jython s analysis even required gb.
using tailored data structures opal s runtime decreased by to compared to naively using scala s sets.
selecting suitable data structures adapted to the specific analysis needs is an important factor for analysis performance.
while the analysis developer can freely select optimized data structures in opal strictly declarative approaches do not support such choices.
.
comparison with declarative approaches after evaluating individual unique features of opal in isolation we present the results of an experiment that directly compares the performance of opal with that of doop rq5 a highly optimized state of the art tool for declarative java points to andesec fse november virtual event usa dominik helm florian k bler michael reif michael eichberg and mira mezini call graph analyses on top of the souffl datalog engine.
its declarative approach assembles a fair comparison as it supports similar modularity and configurability and good trade offs between pluggable precision recall.
also doop s and souffl s authors repeatedly claimed its good performance .
specifically we compare our points to based call graph s runtime from section .
to doop s. for better comparability we disabled the reflection support in both tools because the respective approaches are different.
the applications were analyzed together with openjdk .
.0 75 used for the tamiflex data in doop s benchmarks .
minor differences less then difference in the number of rms except for eclipse and xalan remain but these are in doop s favor since they result in more work to be done by opal6.
still the sixth column of table shows that our complete analysis including all preprocessing is often faster than doop s analysis in the geometric mean .
further doop additionally requires time for rule compilation and fact generation.
we used opal s single threaded implementation since it seems that doop is hardly parallelized fact generation was done with threads but did not significantly vary with other values for the fact gen cores parameter and the souffle jobs parameter did not show any effects .
using a parallel version opal should be able to outperform doop even more as shown in section .
.
despite being more general i.e.
not tuned for points to analyses but supporting many different kinds of analyses opal clearly outperforms doop.
related work in this section we discuss several related approaches in various areas of static analysis as well as in blackboard systems.
.
blackboard systems the blackboard metaphor was introduced by newell and implemented for speech recognition in hearsay ii .
blackboard systems were used for image recognition vessel identification or industrial process control .
for these domains no efficient deterministic algorithm is known leading to several problems mentioned by buschmann et al.
nondeterminism making testing difficult no guarantee for good solutions performance suffering from wrong hypotheses and high development effort due to ill defined domains.
as static analyses have a well defined domain and deterministic algorithms these do not apply to our approach.
the structure of blackboard systems is described e.g.
by nii craig and corkill .
corkill also discusses concurrent execution of knowledge sources and the control component similar to opal.
opal resembles a more modern interpretation of blackboard systems its blackboard is not hierarchical and analyses may keep state between activations.
information is however never erased and all communication is done via the blackboard.
brogi and ciancarini used the blackboard approach to provide concurrency for their shared prolog language .
like static analyses this domain is well defined.
their knowledge sources are 6for instance opal does handle some cases of reflection more soundly even with reflection handling disabled in order to process the dacapo benchmark correctly.restricted to be prolog logical programs while opal s analyses can be implemented in a way best suited to the analysis needs.
decker et al.
discuss the importance of heuristics for scheduling concurrent knowledge source activations.
focusing on static analyses and well defined dependency relations opal provides good general heuristics which are agnostic to individual analyses.
.
abstract interpretation cousot et al.
have proven that multiple possibly cheap abstract domains i.e.
analyses can be combined using the reduced product to increase overall precision.
in abstract interpreters such as astr e or clousot dependencies between domains are restricted by the execution order.
thus the same program statement must be analyzed multiple times which is superfluous with opal s explicit dependency management.
also abstract interpretation typically aims to compute abstract approximations of concrete values such as an integer variable s value.
opal further allows natural expression of analyses on all granularity levels.
keidel et al.
provide modular and reusable abstract semantics for different language features allowing soundness proofs from composition of already sound components.
the analyses again approximate single concrete program values.
opal supports analyses to be based on abstract interpretation and includes such analyses but generalizes to a much broader range of static analyses.
.
declarative analyses using datalog datalog is often used to implement static analyses in a strictly declarative fashion .
properties are represented as relations and rules specify how to compute them.
this enables modularization as rules can be easily exchanged and or added e.g.
for new language features .
the doop framework building on top of the highly optimized datalog solver souffl has shown that the rule based approach enables precise and scalable points to analyses.
for this reason doop became the state of theart for such analyses .
datalog based frameworks however are limited in their expressiveness by using relations i.e.
set based abstractions to represent all analysis results.
opal s approach combining imperative and declarative features provides similar benefits as datalog based approaches while allowing for more expressive ways to represent data and to implement analyses.
datalog s limitation to relations has also been pointed out by madsen et al.
.
they propose flix to overcome this using a language inspired by datalog and scala to specify declarative pluggable analyses using arbitrary lattices as in opal.
however flix focuses on verifying soundness and safety properties of static analyses and not on performance.
for instance flix does not allow optimized data structures or scheduling strategies.
we wanted to compare our approach against flix and contacted the authors but they answered that their ifds implementation is dysfunctional now and suggested comparing against doop with the souffl engine which we did in section .
.
szab et al.
also extend datalog to allow arbitrary lattices for static analysis.
their solver inca focuses on incrementalization.
opal allows optimizations e.g.
of used data structures or scheduling strategies.
furthermore analyses coarser granularity compared to individual rules reduces overhead in parallelization.modular collaborative program analysis in opal esec fse november virtual event usa .
attribute grammars attribute grammars used in compilers such as jastadd enable modular inference of program properties by adding computation rules to the nodes of a program s abstract syntax tree ast .
in traditional attribute grammars attributes may only depend on parent sibling and child nodes.
circular reference attribute grammars enable attributes to depend on arbitrary ast nodes and allow circular dependencies.
still analyses are tightly bound to the ast impeding natural expression of analyses based on different structures such as a control flow graph.
similar to opal jastadd enables pluggability for new language features.
however jastadd requires at least one attribute in a cyclic dependency to be marked explicitly while opal handles this transparently.
qvist and hedin proposed concurrent evaluation of low complexity attributes in circular reference attribute grammars.
opal on the other hand supports arbitrary granularity of concurrent computation.
opal s explicit dependency management enables analyses to drop dependencies and commit final results early for improved performance.
finally as memorization of properties is done in opal s blackboard temporary values are garbage collected automatically whereas jastadd requires explicit removal.
.
imperative approaches and parallelization lerner et al.
proposed modularly composed dataflow analyses which communicate implicitly through optimizations of the analyzed code or explicitly through snooping .
a fixed point algorithm repeatedly reanalyzes the code while opal s explicit dependencies avoid reanalysis.
they support only dataflow analyses while opal enables a wide range of analyses including dataflow analyses.
cpachecker is a tool for configurable software verification and analysis.
for any combination of analyses cpachecker requires defining a compound analysis to integrate results of individual analyses and manage their interaction.
for cpa combined analyses must work with the same domain and provide an explicit measure of result precision.
in contrast opal enables tight interaction and interleaved execution of independently developed analyses without requiring a compound analysis or explicit measure of precision.
johnson et al.
present a framework for collaborative alias analysis.
clients ask queries which are processed by a sequence of analyses.
each analysis can answer the query or forward it to the next one.
analyses can also generate additional premise queries.
to ensure termination a complexity metric must be defined and premises must be simpler than the queries they originate from.
therefore cyclic dependencies required for optimal precision and results combined from different analyses are not supported.
parallel execution of static analyses is performed by magellan .
in this framework dependencies are given by the data processed instead of explicitly by the analyses.
haller et al.
concurrently execute tasks based on lattices and apply this to static analysis.
their framework requires dependencies to be managed fully by the client while opal manages them automatically based on declarative specifications.
in recent work we extended this approach to support mutable state and found that exchangeable scheduling strategies significantly impact performance.
both concepts are supported in opal.
threats to validity one threat to the validity of our evaluation is the use of the relatively old and small dacapo benchmark.
it is however widely used to evaluate doop and to compare other approaches with doop .
doop s special support for the benchmark makes it a particularly fair evaluation set.
furthermore our experiment design based on relative comparisons should yield the same results with any well assembled benchmark.
also our results are threatened if our points to analysis is not sufficiently similar to doop.
to achieve comparability we tailored our points to analysis to be as similar as possible i.e.
the call graph derived from the points to results should be almost identical.
in order to ensure this we systematically studied doop s datalog rules validated the resulting call graphs using judge and manually inspected points to sets from deviating call graphs.
conclusion we presented a novel approach for modular collaborative static analyses implemented in the opal framework.
like with declarative frameworks such as doop opal s analyses while developed in isolation can be easily composed to complex analyses by collaboratively computing results during interleaved executions.
subanalyses can be reused in various complex analyses and one can easily exchange sub analyses of a complex analysis for fine tuning precision sound i ness and performance.
but instead of relying on a general purpose solver opal combines imperative and declarative features to overcome limitations of fully declarative frameworks.
individual analyses can be implemented in imperative style making use of whatever data structures and implementation strategies are appropriate for their specific needs.
interdependencies and other characteristics important for guiding their interleaved execution are declaratively specified and automatically managed by a custom solver resembling a blackboard architecture.
due to its approach opal a is more general in terms of the analyses supported it is in particular the first framework to explicitly support lazy collaboration of optimistic and pessimistic analyses and b enables analysis specific optimizations which lead to outperforming state of the art declarative analyses.
we plan to explore several further research questions in the future.
first our evaluation suggests that better parallelization strategies for opal are needed.
second we plan to explore further scheduling strategies both general and analysis specific e.g.
strategies that abort computations whose results are no longer of interest or strategies as well as analyses that adapt their behavior during the execution to increase performance with minimal impact on precision and or soundness.
last but not least we will develop a formal model of opal and formally prove its properties.
for instance we believe that opal s design enables compositional soundness proofs this needs to be proved in the future.