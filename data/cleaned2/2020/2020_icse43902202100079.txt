data driven synthesis of provably sound side channel analyses jingbo wang chungha sung mukund raghothaman and chao wang university of southern california abstract we propose a data driven method for synthesizing static analyses to detect side channel information leaks in cryptographic software.
compared to the conventional way of manually crafting such static analyzers which can be tedious error prone and suboptimal our learning based technique is not only automated but also provably sound.
our analyzer consists of a set of type inference rules learned from the training data i.e.
example code snippets annotated with the ground truth.
internally we use syntax guided synthesis sygus to generate new recursive features and decision tree learning dtl to generate analysis rules based on these features.
we guarantee soundness by proving each learned analysis rule via a technique called query containment checking .
we have implemented our technique in the llvm compiler and used it to detect power side channels in c programs that implement cryptographic protocols.
our results show that in addition to being automated and provably sound during synthesis our analyzer can achieve the same empirical accuracy as two state of the art manually crafted analyzers while being 300x and 900x faster respectively.
i. i ntroduction static analyses are being increasingly used to detect security vulnerabilities such as side channels .
however manually crafting static analyzers to balance between accuracy and efficiency is not an easy task even for domain experts it can be labor intensive error prone and result in suboptimal implementations.
for example we may be tempted to add expensive analysis rules for specific sanitized patterns without realizing they are rare in target programs.
even if the analysis rules are carefully tuned to a corpus of code initially they are unresponsive to changing characteristics of the target programs and thus may become suboptimal over time manually updating them to keep up with new programs would be difficult.
one way to make better accuracy efficiency trade offs and to dynamically respond to the distribution of target programs is to use data driven approaches that automatically synthesize analyses from labeled examples provided by the user.
however checking soundness or compliance with user intent generalization has always formed a significant challenge for example based synthesis techniques .
the lack of soundness guarantees in particular hinders the application of such learned analyzers in security critical applications.
while several existing works try to address this problem rigorous soundness guarantees have remained elusive.
to overcome this problem we propose a learning based method for synthesizing a provably sound static analyzer that detects side channels in cryptographic software by inferring a distribution type for each program variable that indicates if its value is statistically dependent on the secret.
the overall flowfeature synthesis sygus decision tree learningquery containment checking knowledge base kb training programs type annotationslearner proveranalyzer counter examplerverified rrejectedr fig.
.
the overall flow of gps our data driven synthesis method.
of our method named gps is shown in fig.
.
the input is a set of training data and the output is a learned analyzer.
the training data are small programs annotated with the ground truth e.g.
which program variables have leaks.
internally gps consists of a learner and a prover .
the learner uses syntax guided synthesis sygus to generate recursive features and decision tree learning dtl to generate type inference rules based on these features it returns a set rof datalog formulas that codify these rules.
the prover checks the soundness of each learned rule i.e.
it is not only consistent with the training examples but also valid for any unseen programs.
this is formulated by solving a query containment checking problem i.e.
each rule must be justified by existing proof rules called the knowledge base kb .
since only proved rules are added to the analyzer the analyzer is guaranteed to be sound.
if a rule cannot be proved we add a counter example to prevent thelearner from generating it again.
we have implemented gps in llvm and evaluated it on c programs that implement cryptographic protocols and algorithms .
together they have 691k lines of c code.
we compared our learned analyzer with two state ofthe art hand crafted side channel analysis tools .
our experiments show that the learned analyzer achieves the same empirical accuracy as the two state of the art tools while being several orders of magnitude faster.
specifically gps is on average faster than the analyzer from and faster than the analyzer from .
to summarize this paper makes the following contributions we propose the first data driven method for learning a provably sound static analyzer using syntax guided synthesis sygus and decision tree learning dtl .
we guarantee soundness by formulating and solving a datalog query containment checking problem.
we demonstrate the effectiveness of our method for detecting side channels in cryptographic software.
in the remainder of this paper we begin by presenting the technical background in section ii and our motivating example ieee acm 43rd international conference on software engineering icse .
ieee in section iii.
we then describe the learner in section iv and the prover in section v followed by the experimental results in section vi.
finally we survey the related work in section vii and conclude in section viii.
ii.
p reliminaries a. power side channels prior works in side channel security show that variance in the power consumption of a computing device may leak secret information for example when a secret value is stored in a physical register its number of logical bits may affect the power consumption of the cpu.
such side channel leaks are typically mitigated by masking e.g.
using drandom bits r1 rd to split a key bit intod secret shares key r1 keyd rd andkeyd r1 r2 rd key where denotes the logical operation exclusive or xor .
since alld 1shares are uniformly distributed in the f0 1g in theory this order d masking scheme is secure in that any combination of less than dshares cannot reveal the secret but combining all d 1shares key key keyd key recovers the secret.
in practice masking countermeasures must also be implemented properly to avoid de randomizing any of the secret shares accidentally.
consider t tl tr r1 key r1 b key b. while syntactically dependent on the two randomized valuestlandtr tis in fact leaky because semantically it does not depend on the random input r1.
in this work we aim to learn a static analyzer that can soundly prove that all intermediate variables of a program that implements masking countermeasures are free of such leaks.
b. type systems type systems prove to be effective in analyzing power side channels e.g.
by certifying that all intermediate variables of a program are statistically independent of the secret.
typically the program inputs are marked as public inpub secret inkey or random inrand and then the types of all other program variables are inferred automatically.
the type of a variable v denoted type v may be rud sid orukd.
here rud stands for random uniform distribution meaning vis either a random bit or being masked by a random bit.
sid stands for secret independent distribution meaningvdoes not depend on the secret.
while an rud variable is by definition also sid ansid variable does not have to be rud e.g.
variables that are syntactically independent of the secret .
finally ukd stands for unknown distribution or potentially leaky if the analyzer cannot prove vto be rud orsid then it is assumed to be ukd.
type systems are generally designed to be sound but not necessarily complete.
they are sound in that they never miss real leaks.
for example by default they may safely assume that all variables are ukd unless a variable is specifically elevated to sid orrud by an analysis rule.
similarly they may conservatively classify sid variables as ukd or classify rud variables as sid without missing real leaks.
in general the sets of variables that can be marked as the three types form a hierarchy srud ssid sukd.
c. relations a program in static single assignment ssa format can be represented as an abstract syntax tree ast .
static analyzers infer the type of each node xof the program s ast based on various features ofx.
in this work pre defined features are represented as relations .
unary relations inpub x inkey x and inrand x denote the given security level of a program input x which may be public secret or random.
unary relations rud x sid x andinrand x denote the inferred type of a program variable x which may be uniformly random secret independent or unknown.
unary relation op x denotes the operator type of the ast nodex e.g.
op x andor x jxor x where andor x means thatx s operator type is either logical and orlogical or and xor x means that x s operator type is exclusive or binary relations lc x l andrc x r indicate that the ast nodeslandrare the left and right operands of x respectively.
binary relation supp x y indicates that the ast node yis used in the computation of xsyntactically while dom x y indicates that random program input yis used in the computation of xsemantically.
iii.
m otivation consider the program in fig.
2a which computes the function from keccak a family of cryptographic primitives for the sha standard .
it ultimately computes the function n1 i1 i2 i3 where means xor.
unfortunately a straightforward implementation could potentially leak knowledge of the secret inputs i1 i2and i3if the attacker were able to guess the intermediate results i2 and i2 i3via the power side channels .
the masking countermeasures in the implementation therefore use three additional random bits r1 r2andr3to prevent exposure of the private inputs while still computing the desired function.
a. problem setting given such a masked program users want to determine if they succeed in eliminating side channel vulnerabilities in particular if each intermediate result is uniformly distributed rud or at least independent of the sensitive inputs sid .
the desired static analysis thus associates each variable x e.g.
n1 with the elements of a three level abstract domain rud sid orukd indicating that xis uniformly distributed rud secret independent sid or unknown ukd and therefore potentially vulnerable.
the decision tree in fig.
2b represents the desired static analyzer which accurately classifies most variables of the training corpus and is also sound when applied to new programs.
given variablex the decision tree leverages the features of x such as the operator type of x op x andor x jxor x and the 811user label rud b1 rud b2 rud b3 rud b4 sid n9 sid n8 sid n7 rud n6 sid n5 sid n4 rud n3 rud n2 ukd n1 bool mchi bool i1 bool i2 bool i3 bool r1 bool r2 bool r3 bool b1 i1 r1 bool b2 i2 r2 bool b3 i3 r3 bool b4 b2 b3 bool n9 b3 r2 bool n8 r3 r2 bool n7 b2 r3 bool n6 r1 n9 bool n5 n7 n8 bool n4 b2 b3 bool n3 n5 n6 bool n2 n4 b1 bool n1 n2 n3 return n1 a f x true false ukd op x xor x andor x type r type l sid r rud r type l rud rud l rud l ukd type r rud l rud l rud ukd rud r rud r ukd sid fn1 n5gfn1 n5gfn6gfn6gfb1 b2 b3 b4 n2 n3gfn4 n7 n8 n9g b fig.
.
the program on the left is a perfectly masked function from mac keccak.
the decision tree on the right represents the static analyzer that the user would like to synthesize.
here xis a program variable whose type is being computed landrare its left and right operands respectively and f x is a synthesized feature shown in fig.
3a represented by recursive datalog program .
r1 rud x xor x rc x r rud r f x r2 f x lc x l rc x r g1 l rl g2 r rr rl rr r3 g1 r r inrand r r4 g1 x r lc x y g1 y r r5 g1 x r rc x y g1 y r r6 g2 r r inrand r r7 g2 x r lc x y g2 y r xor x r8 g2 x r rc x y g2 y r xor x a excerpt of rules learned by the gps tool.
m1 rud x xor x dom x res res6 m2 supp x x inrand x inkey x inpub x m3 supp x res lc x l rc x r supp l sl supp r sr res sl sr m4 dom x x inrand x m5 dom x inkey x inpub x m6 dom x res xor x lc x l rc x r dom l sdl dom r sdr supp l sl supp r sr res sdl sdr n sl sr b corresponding expert written rules from scinfer .
fig.
.
comparing the rules learned by gps to manually crafted rules from scinfer .
observe that the learned rules are sound i.e.
every variable which potentially leaks information is assigned the distribution type ukd while still managing to draw non trivial conclusions such as rud b4 .
the learned rules r2 r8 in fig.
3a are used to define the new feature f x in fig.
2b.
types ofx s operands e.g.
type l type r and maps xto its corresponding distribution type.
the white nodes of fig.
2b represent pre defined features while the grey nodes represent output classes associated types .
each path from the root to leaf node corresponds to one analysis rule.
the set of pre defined features used in this work is shown in fig.
4a.
designing side channel analyses has been the focus of intense research see for example .
unfortunately it requires expert knowledge in both computer security and program analysis and invariably involves delicate trade offs between accuracy and scalability.
our goal in this work is to assist the analysis designer in automating the development.
this problem has also been the subject of exciting research however these approaches typically eitherrequire computationally intensive deductive synthesis or cannot guarantee soundness and thus produce errors in both directions including false alarms and missed bugs.
in contrast gps combines inductive synthesis from user annotations with logical entailment checking against a more comprehensive known to be correct set of proof rules that form the knowledge base kb .
it takes as input training programs like the one in fig.
2a where the labels correspond to the types of program variables rud sid ukd for intermediate results andinrand inpub inkey for inputs .
the users are free to annotate as many or as few of these types as they wish this affects only the precision of the learned analyzer and not its soundness.
second gps also takes as input the knowledge basekb consisting of proof rules that describe axioms of propositional logic and properties of the distribution types .
in return gps produces as output a set of datalog rules which simultaneously achieves high accuracy on the training data and provably sound with respect to kb.
the proof rules for kb were collected from published papers on masking countermeasures .
we emphasize thatkb is not necessarily an executable static analyzer since repeated application of these proof rules need not necessarily reach a fixpoint and terminate in finite time furthermore even in cases where it does terminate kb may be computationally expensive and infeasible for application to large programs.
as a concrete example we compare excerpts of the rules learned bygps in fig.
3a to the corresponding rules from scinfer a human written analysis in fig.
3b.
lc x l andrc x r arises in both versions indicating that landr are the left and right operands of xrespectively.
specifically in fig.
3b supp x y indicates that yis used in the computation ofxsyntactically while dom x y denotes that random variable yis used in the computation of xsemantically.
observe the computationally expensive set operations in the human written version to the simpler rules learned by gps without loss of soundness or perceptible loss in accuracy.
these points are also borne out in our experiments in table ii where scinfer takes minutes on some keccak benchmarks while our 812op v v xjljr and ornot xor mulleaf type v v ljr rud sid ukd inrand inpub inkey rud x sid x ukd x a op x andor x xor x sid op r leaf r andor r xor r rud type l type l sid l rud l sid rudsid l rud l rud rud?ukdfn4 n7 n8 n9gfn4 n7 n8 n9g fn5gfn5gfn6gfn6gfn2 n3gfn2 n3gfb4 n1gfb4 n1g fb1 b2 b3gfb1 b2 b3g b op x andor x xor x sid op r leaf r andor r xor r rud type l type l sid l rud l sid rudsid l rud l rud f x true false ukd rud c fig.
.
the classifier of fig.
4b is learned only using the features in fig.
4a.
because of the limited expressive power of these features the learned analysis necessarily misclassifies either b4orn1.
fig.
4c denotes the candidate analyzer produced after one round of feature synthesis.
the blue paths corresponds to the rule rud x xor x xor r rud l f x lc x l rc x r .
unfortunately even though this analysis achieves training accuracy the leaf nodes highlighted in red correspond to unsound predictions.
learned analysis takes seconds.
gps consists of two phases first it learns a set of typeinference rules alternatively represented either as datalog programs or as decision trees that are consistent with the training data.
second it proves these rules against the knowledge base.
in the next two subsections we will explain the learning and soundness proving processes respectively.
b. feature synthesis and rule learning the learned analyzer associates each node xof a program s abstract syntax tree ast with an element of the distribution typefukd x sid x rud x g. we may therefore interpret the analyzer as a decision tree that by considering various features of an ast node maps it to a type.
with a pre defined set of features such as those shown in fig.
4a analyzers of this form can be learned with classical decision tree learning dtl algorithms.
fig.
4b shows such an analyzer learned from the labeled program of fig.
2a.
unfortunately the pre defined features may not be strong enough to distinguish between nodes with different training labels e.g.
b4andn1from the training program which have distinct labels rud b4 andukd n1 but after being sifted into the node highlighted in red in fig.
4b cannot be separated by any of the features from fig.
4a.
to ensure soundness the learner would be forced to conservatively assign the label ukd x which sacrifices the accuracy.
gps thus includes a feature synthesis engine triggered whenever the learner fails to distinguish between two differently labeled variables.
in tandem with recursive feature synthesis gps overcomes the limited expressiveness of dtl by enriching syntax space to capture more desired patterns.
observe that paths of a decision tree can be represented as datalog rules e.g.
the red path in fig.
4b is equivalent to ukd x xor x xor r rud l lc x l rc x r viewing this in datalog also allows us to conveniently describe recursive features and reduce feature synthesis to an instanceop x op l op r type l type r f x ce1andor ce xor leaf ce xor xor sid ce xor andor rud ce xor andor sid fig.
.
abstract counter examples produced during the soundness verification of the candidate analyzer shown in fig.
4c.
type l rud l ukd l sid l op x rud op r xor x andor x f x sidandor r xor r sid rud true false ukd rud fig.
.
candidate analysis learned after one round of feedback from the soundness verifier.
the leaves shown in green and red correspond to sound and unsound analysis rules respectively.
of syntax guided synthesis sygus .
syntactically the analysis rules corresponding to new features are instances of a predefined set of meta rules and the target specification is to produce a datalog program for a relation f x that has strictly positive information gain for the variables under consideration see section iv for details .
in our running example the synthesizer produces the feature f x shown in fig.
3a which intuitively indicates that some random input ris used to compute both operands of x. with this new feature the learner can distinguish between b4andn1 and produce the rule shown in fig.
4c which correctly classifies all variables of the training program.
observe that the rules definingf x in fig.
3a involve a newly introduced predicate g x r and recursive structure that can classify variables based on arbitrarily deep properties of the abstract syntax tree.
813c.
proving soundness of learned analysis rules while the learned analysis rules are correct by construction for the training examples they may still be unsound when applied to unseen programs.
we observe this for example in the leaves highlighted in red in fig.
4c.
thus gps tries to confirm their soundness against the domain specific knowledge basekb.
in the context of our running example confirming soundness means proving that every variable xthat is assigned typerud x resp.
sid x by the learned analysis rule is also certified rud x resp.
sid x bykb.
we formalize the soundness proof as a datalog query containment problem and propose an algorithm based on bounded unrolling and k induction to check it.
when applied to the candidate analysis of fig.
4c the check results in the five counter examples ce ce 5with distribution type ukd cei shown in fig.
.
each counterexample indicates the unsoundness of one path from the root of the decision tree to a classification node.
these are abstract counter examples in that they contain missing features and consequently do not define concrete asts.
thus each of these abstract counter examples is a set of feature valuations ff17!v1 f27!v2 fk7!vkgthat the current candidate analysis misclassifies and feeding them back to the learner can prohibit subsequent candidate analyses from classifying variables that satisfy .
with these new constraints from abstract counter examples the learner learns the new candidate analysis shown in fig.
.
this new candidate analysis still has four unsound candidate rules which results in additional abstract counter examples when it is subjected to the soundness check.
we repeat this back and forth between the rule learner and the soundness prover after iterations and after processing counterexamples in all gps learns the rules initially presented in fig.
2b all of which have been certified to be sound.
d. overall architecture of the gps system we summarize the architecture of gps in fig.
.
the learner repeatedly applies dtl and sygus to learn candidate analyses that correctly classify training samples and are consistent with newly added abstract counter examples.
next the prover checks the soundness of the learned analysis.
each subsequent counter example is fed back to the learner which restarts the rule learning process on augmented dataset until either all synthesized rules are sound or a time bound is exhausted.
iv.
l earning the inference rules we formally describe the analysis rule learner in algorithm .
the input consists of a set of labeled examples e and a set of pre defined features f. the outputtis a set of type inference rules consistent with training examples.
each training example x type x 2e consists of an ast node xin a program and its distribution type type x .
at the top level the learner uses the standard decision tree learning dtl algorithm as the baseline.
however if it finds that the current set fof classification features is insufficient it invokes a syntax guided synthesis sygus algorithm dtl e f decision tree learning.
input examples e f x1 type x1 xn type xn g input pre defined features f ff1 f2 f kg output classifiertwhich is consistent with provided examples ifall examples x type x 2e have the same label type x t then returnt leafnode t end if if69f2f such thath ejf h e then f f feature syn e end if t decisionnode f wheref arg minf2fh ejf forvaluationiof featuref do ti dtl ejf x i fnff g add edge from ttotiwith labelf x i end for returnt algorithm to synthesize a new feature fwith strictly positive information gain to augment f. this allows the learner to combine the efficiency of techniques that learn decision trees with the expressiveness of syntax guided synthesis similar ideas have been fruitfully used in other applications of program synthesis see for example .
while the top level classifier e.g.
fig.
2b 4b 4c and has a bounded number of decision points the synthesized features e.g.
fig.
3a may be recursive.
furthermore the newly synthesized features fare inducted as first class citizens off and can subsequently be used at any level of the decision tree see for example fig.
2b and .
next we present the dtl and sygus subroutines respectively.
a. the decision tree learning algorithm recall that our pre defined features include properties of the ast node such as op x and properties referring its left and right children such as op l lc x l .
the choice requires some care having very few features will cause the learning algorithm to fail while having too many features will increase the risk of overfitting.
our synergistic combination of dtl with sygus based on demand feature synthesis can be seen as a compromise between these extremes.
dtl e f is an entropy guided greedy learner where the entropy and conditional entropy of a set defined below are used to measure the diversity of its labels h e p t2typepr type x t log pr type x t h ejf p i2range f h ejf x i algorithm thus divides the set of training examples eusing the feature f f that minimizes the conditional entropy h ejf lines and recursively invokes the learning algorithm on each subset dtl ejf x i fnff g .
observe that h e ifpr type x t meaning purity or all examples x2e share the same type type x t. the difference between h e andh ejf is also referred to as the information gain .
if the learner cannot find a feature with strictly positive information gain line it will invoke the feature synthesis algorithm on line .
814algorithm feature syn e .
input examples e f x1 type x1 xn type xn g output featurefwith positive information gain or ?to indicate failure letsbe the meta rules defined in figure i.e.
the hypothesis space for each relation schema rdefined insdo for each subsetsof meta rules corresponding to the schema do for each choicepin qin and nested relational predicates do letfbe the corresponding instantiation of the meta rules in s ifh ejf h e then returnf end if end for end for end for return?
rf f x pin x f x qin x y f x pin x y qin x y f x qin x y f y f x qin x y pin x f y rg g x y qin x y g x y pin x qin x y g x y qin x z g z y g x y qin x z pin x g z y rh h x f x pin x qin x y h x g x y pin x qin x y h x f x g x y pin x qin x y pin x and x jor x jnot x jxor x jmul x jleaf x jinrand x jinkey x jinpub x jpin pinjpin pinj pin qin x y lc x y jrc x y jx y jqin x y qin x y jqin x y qin x y j qin x y fig.
.
syntax of the dsl for synthesizing new features.
b. the on demand feature synthesis algorithm we represent newly synthesized features as datalog programs.
datalog is an increasingly popular medium to express static analyses and its recursive nature enables the newly learned features to represent arbitrarily deep properties of ast nodes.
a datalog rule is a constraint of the form h x b1 y1 b2 y2 bn yn whereh b1.
.
.bnare relations with pre specified arities and schemas and where x y1.
.
.ynare vectors of typed variables.
each rule can be interpreted as a logical implication if b1.
.
.bn are true then so is h. the semantics of a datalog program is defined as the least fixed point of rule application the solver starts with empty output relations and repeatedly derives new output tuples until no new tuples can be derived.
program synthesis commonly restricts the space of target concepts and biases the search to speed up computation and improve generalization.
one form of bias has been to constrain the syntax this has been formalized as the sygus problem and as meta rules in inductive logic programming .
a meta rule is construct of this form xh x x1 y1 x2 y2 xn yn here xh x1 x2 .
.
.
xnarerelation variables whose instantiation yields a concrete rule.
fig.
shows the meta rulesused in our work.
for example instantiating the meta rule f x qin x y pin x f y withqin x y rc x y andpin x xor x yieldsf x rc x y xor x f y .
there are three variations of the final target relation schema f x g x y andh x wherexandydenote ast nodes.
we formalize the synthesis problem as that of choosing a relationr2ff x g x y h x gand finding a subset pdof its instantiated meta rules from fig.
such that the resulting datalog program pdhas strictly positive information gain on the provided training examples e. algorithm shows the procedure which repeatedly instantiates the meta rules from fig.
and computes their information gain.
it successfully terminates when it discovers a feature that can improve classification.
otherwise it returns failure upon timeout and invokes dtl e f to conservatively classify the decision tree node as being of type ukd.
example iv .
.
givene f b4 rud n1 ukd gshown in fig.
2a the synthesizer may alternatively learn the rules in equations and .
f x inrand x f y lc y x f x f y rc y x f x rud x xor x lc x l rc x r rud l f r g x x inrand x g y z lc y x g x z g y z rc y x g x z h x lc x l rc x r g l xl g r xr xl xr rud x xor x rud l rud r lc x l rc x r h x g x x inkey x g y z lc y x g x z g y z rc y x g x z h x lc x l rc x r g l xl g r xr xl xr rud x xor x rud l rud r h x since the information gain of rule applying to fb4 n1gis zero it gets discarded line in algorithm .
in contrast the information gains of rules and are both positive.
rule intuitively requires that both the left and right operands of x are of type rud and that they do not share any random inputs in computing h x .
rule requires that the same secret key be used in the computation of both operands.
while rule is sound when applied to arbitrary programs rule is unsound.
in the next section we will present an algorithm that can check the soundness of these learned rules.
v. p roving the inference rules we wish to prove that a learned rule denoted never reaches unsound conclusions when applied to any program by showing that it can be deduced from a known to be correct knowledge base kb .
more specifically we wish to confirm that every ast node xmarked as rud orsid by can be certified to be rud orsid bykb.
when both and kb are expressed in datalog the problem reduces to one of determining query containment e.g.
for every valuation of the input relations rud rudkb orsid sidkb .
815b b true b1 b b false b2 b b b3 a b a b b4 a b a b b5 b false b b6 b true true b7 b true b b8 b b b b9 b false false ba b b b bb a a b a bc a a b a bd a b a b a b be a b c a c b bf a b c a c b b10 a b c a b c b11 a b c a b c b12 fig.
.
proof rules for propositional logic to simplify the logic formula and deduce boolean constants true andfalse .
g1 l k1 k1lc r1rc g2 r k2 k2lck1 k2sid x fig.
.
example ast from which is learned.
we will now describe a semi decision procedure to verify the soundness of the learned rules which forms the second phase of the synthesis loop in gps .
a. representation of the learned rule let be a set of datalog rules each of which has a head relation and a body of the following form x x1 x2 n xn it means holds only when all of nhold.
here may be a distribution type e.g.
sid x or a recursive feature g x y e.g.
representing that xdepends on y. b. representation of the knowledge base kb ourkb consists of two sets of proof rules one for propositional logic and the other for distribution types.
proof rules for propositional logic.
fig.
shows the proof rules that represent axioms of propositional logic they can be used to reduce any valid resp.
invalid boolean formula to constanttrue resp.false .
thus they are useful in showing results such as true pandfalse qare secret independent sid for arbitrary logical sentences pandq.
consider the example rule below where g1andg2are synthesized features shown as dashed boxes in fig.
sid x or x lc x l rc x r or l not r g1 l k1 g2 r k2 eq k1 k2 g1 l k1 inkey k1 inrand r1 lc l k1 rc l r1 g2 r k2 inkey k2 lc r k2 sincek1 k2 we transform into an equivalent logic formula sid x eq x k1 r1 k1 rulesb1 b7andbfin fig.
show that k1 r1 k1 is alwaystrue .
thus xis alwaystrue .
sincexis a constant we have sid x meaningxis secret independent.
such sid rules learned by our method automatically and yet overlooked by state of the art hand crafted analyzers can significantly improve the accuracy of side channel analysis on many programs.proof rules for distribution types.
fig.
shows the proof rules that represent properties of the distribution types.
they were collected from published papers that focus on verifying masking countermeasures which also provided the soundness proofs of these rules.
for brevity we omit the detailed explanation.
instead we use rule d2 1as an example to illustrate the rationale behind these proof rules.
in ruled2 thedom x s relation means that variable xis masked by some input from the set sof random inputs.
for example in y x1 x2 wherex1 k r1 r2and x2 b r2 we say that x2is masked by r2 andx1is masked by bothr1andr2.
however since r2 r2 false yis masked only byr1.
thus dom y fr1g holds but dom y fr2g does not hold.
in this sense rule d2 4defines a masking set .
fory it issy fr1 r2g fr2g n fr1 r2g fr2g fr1g which containsr1only.
the masking set defined by d2 4is useful in that as long as the set is not empty the corresponding variable is guaranteed to be of the rud type.
c. proving the soundness of usingkb to prove that for every ast node xmarked as rud x resp.
sid x by it is also marked as rudkb x resp.
rudkb x bykb we show that the following relation ind x is empty for any valuation of the input relations ind x x kb x where the relation may be instantiated to either rud orsid.
in theory this amounts to proving query containment which is undecidable for datalog in general but there is a decidable datalog fragment and our meta rules in fig.
produce rules in this fragment.
first we observe that every tuple t x produced by a datalog program is associated with one or more derivation trees .
the heights of these derivation trees correspond to the depth of rule inlining at which the program discovers t. in particular for each inlining depth k2n each rule h xh x1 x2 n xn is transformed into the rule k xh k x1 k x2 k n xn our insight is to prove that at each unrolling depth k we have k k kb.
thus we define the relation ind k as follows ind k x k x k kb x and prove the emptiness of ind x byk induction .
proposition v .
.
ifind k x is an empty relation for each depthk2n thenind x is an empty relation.
x inrand supp x fxg d1 x inkey supp x fxg d1 x inpub supp x fxg d1 x inrand dom x fxg d2 x inkey dom x d2 x y v s set v rc y x1 lc y x2 supp x1 s1 supp x2 s2 supp y s1 s2 d1 x inpub dom x d2 x y v s set v rc y x1 lc y x2 xor y dom x1 s1 dom x2 s2 dom x s1 s2 s1 s2 d2 x v s set v dom x sx sx6 x rud d3 x inkey s setinkey x s setinkey d4 x v sk setinkey sd setrud ss set v dom x sd sd supp x ss ss sk x sid d5 x1 sid x2 rud s1 s2 set v lc y x1 rc y x2 and y supp x1 s1 supp x2 s2 s1 s2 y sid d6 x1 sid x2 rud s1 s2 set v lc y x1 rc y x2 or y supp x1 s1 supp x2 s2 s1 s2 y sid d7 x1 sid x2 sid s1 s2 set v lc y x1 rc y x2 supp x1 s1 supp x2 s2 s1 s2 y sid d8 x1 sid x2 sid s1 setrud s2 set v and y lc y x1 rc y x2 dom x1 s1 supp x2 s2 s1 s26 y sid d9 x1 sid x2 sid s1 setrud s2 set v or y lc y x1 rc y x2 dom x1 s1 supp x2 s2 s1 s26 y sid da x1 rud x2 rud s1 setrud s2 set v and y lc y x1 rc y x2 dom x1 s1 supp x2 s2 s2 s16 y sid db x1 rud x2 rud s1 setrud s2 set v or y lc y x1 rc y x2 dom x1 s1 supp x2 s2 s2 s16 y sid dc x rud x noukd dd x sid x noukd de x v not y lc y x y v df x bool x true x sid d10 x bool x false x sid d11 x v sk setinkey s set v supp x ss ss sk x noukd d12 x1 rud x2 rud s1 s2 setrud lc y x1 rc y x2 mul y y dom x1 s1 dom x2 s2 s2 s16 y sid d13 x1 rud x2 sid s1 setrud s2 set v lc y x1 rc y x2 mul y dom x1 s1 supp x2 s2 s1 s26 y sid d14 x1 sid x2 rud s1 setrud s2 set v lc y x1 rc y x2 mul y dom x1 s1 supp x2 s2 s2 s16 y sid d15 fig.
.
proof rules for distribution types gathered from prior works .
here vdenotes the type of variable x and is of the following types ukd sid andrud.noukd denotes the secure type either rud orsid .
all the predefined relations in kb are the same as in .
observe that unrolling the rules of a datalog program to any specific depth yields a formula which can be interpreted within propositional logic.
for example unrolling f x from equation at depths 1and2gives us f x inrand x and f y lc y x f1 x rc y x f1 x for any specific value of k we can therefore use an smt solver to verify the emptiness of ind k .
for the induction step in particular we ask the smt solver to check if ind k can be non empty while the ipreceding relationsind k ind k i are assumed to be empty.
here k is expressed recursively using k k i and induction succeeds if there exists such a value for i2n.letv k be free variables introduced by unrolling the rules at depthk.
we assert the non emptiness of ind k below k x2v k ind k x thus we formalize the induction step of the proof by constructing the following formula k k i k k proposition v .
.
if for some i2n the relations ind .
.
.
ind i are all empty the base case and the formula k as defined above is unsatisfiable the induction step then ind k is empty for all k2n.
starting from i we use the smt solver to check proposition v .
for increasingly larger iuntil a timeout is 817reached.
if the smt solver is ever successful in proving the proposition it follows that the learned rule is sound.
d. generating abstract counter examples when the proof fails however we need to prevent the same rule from being learned again to guarantee progress.
let ff1 v1 f2 v2 fk vkgbe the feature valuation in the failing rule r .
we then construct the counter example ce ff7!vj f v g ff7!
1jf2fn g with label ukd ce .
recall thatfis the set of all features currently under consideration.
therefore the feedback ce provided to dtl e f is an abstract counter example with all missing features f2fn set to the unknown value .
consider the subsequent iteration of the decision tree learner dtl e fce g f .
observe that whenever it is in a decision context which is also a prefix preof the counter example ce the information gain of each feature f2 is strictly less than that encountered in the previous invocation.
therefore at some level of the decision tree it will either choose a different feature or invoke the feature synthesis algorithm to grow f. by formalizing this argument we say that proposition v .
.
given a counter example ce to a learned ruler the subsequent invocation of the learner dtl e fce g f is guaranteed to no longer produce r .
before ending this section we stress that the proof rules inkb should not be confused with analysis rules used in the learned analyzer since they are way more computationally expensive.
consider rule d1 whose datalog encoding size forsupp x s would bejvj 2jinj.
for the benchmark named b19 in table i it owns input variables and thereby causing the exponential explosion with .
the learned rule in contrast is much cheaper since it does not rely on these expensive set union and intersection operations.
vi.
e xperiments our experiments were designed to answer the following research questions rqs rq1 how effective is our learned analyzer in terms of the analysis speed and accuracy?
rq2 how effective is our gps method for learning inference rules from training data?
rq3 how effective is our gps method for proving the learned inference rules?
we implemented gps in llvm .
.
gps relies on llvm to parse the c programs and construct the internal representation ir .
then it learns a static analyzer in two steps.
the first step which is sygus guided decision tree learning is implemented in lines of c code.
the second step which proves the learned inference rules is implemented using the z3 smt solver.
furthermore the learned analyzer for detecting power side channels in cryptographic software is implemented in llvm as an optimization opt pass.
we ran all experiments on a computer with .
ghz intel core i5 cpu and gb ram.table i statistics of the benchmark programs in dtest.
name locipubiprivirand name locipubiprivirand b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b11 b12 b13 b14 b15 b16 b17 b18 b19 b20 b21 b22 b23 b24 b25 b26 b27 b28 b29 b30 b31 b32 426k b33 426k b34 426k b35 429k b36 426k b37 442k a. benchmarks our benchmarks are programs with 691k lines of c code in total.
they implement well known cryptographic algorithms such as aes and sha .
some of these programs are hardened by countermeasures such as reordered mackeccak computation masked aes masked s box calculation and masked multiplication to eliminate power side channel leaks.
we partition the benchmarks into two sets dtrain forgps anddtestfor the learned analyzer.
the training set dtrain consists of small programs gathered from various public sources including byte masked aes random reduction of s box common shares and leak examples .
each benchmark is a pair consisting of a program ast and its distribution type i.e the ground truth annotated by developers.
the testing set dtestconsists of large programs whose statistics the number of lines of code and inputs labeled public private and random are shown in table i. since these programs are large it is no longer practical to manually annotate the ground truth instead we relies on the results of published tools a manually crafted static analyzer for b1 b20 and a formal verification tool for b21 b37.
b. performance and accuracy of the learned analyzer to demonstrate the advantage of our learned analyzer answer rq1 we compared our learned analyzer with the two existing tools on the programs in dtest.
only our analyzer can handle all of the programs.
therefore we compared the results of our analyzer with the tool from on b1 b20 and with the tool from on b21 b37.
the results are shown in table ii and table iii respectively.
in both tables columns show the benchmark name and the number of ast nodes.
columns show the existing tool s analysis time and result including a breakdown in three types.
similarly columns show our learned analyzer s time and result.
note that in the ukd sid rud numbers were the number of variables of the llvm ir and thus larger than the number of variables in the original programs.
to be consistent we compared with their results in the same manner in table ii.
the results in table ii and table iii show that our learned analyzer is much faster especially on larger programs such as b20 818table ii comparing the learned analyzer with the tool from .
name astmanually designed analyzer our learned analyzer time s ukd sid rud time s ukd sid rud b1 .
.
b2 .
.
b3 .
.
b4 .
.
b5 .
.
b6 .
.
b7 .
.
b8 .
.
b9 .
.
b10 .
.
b11 .
.
b12 .
.
b13 .
.
b14 .
.
b15 .
.
b16 .
.
b17 .
.
b18 .
.
b19 .
.
b20 .
.
.
seconds versus minutes .
the reason why our analyzer is faster is because the manually crafted analyses rely on evaluating set relations e.g.
difference and intersection of sets of random variables whereas our dsl syntax is designed without set operations to infer the same types thus leading to faster analyses.
although in general the set operation based algorithm is more accurate it has excessive computational overhead.
moreover it does not always improve precision in practice.
furthermore the method in uses an smt solverbased model counting technique to infer leak free variables which is significantly more expensive than type inference.
as shown in table ii and table iii by learning inference rules from data we can achieve almost the same accuracy as manually crafted analysis while avoiding the huge overhead.
given the same definitions of distribution types ukd sid andrud both our learned rules and manually crafted analysis rules can infer the non leaky patterns thus recognizing the variable types correctly under most benchmarks in table ii and table iii except for b4 b6 and b30 where set operations are required to prove the leak freedom of some variables.
recall that losing accuracy here indicates that our learned rules infer the types more conservatively without losing soundness.
nevertheless our analyzer also increased accuracy in some other cases e.g.
b2 due to its deeper constant propagation which led to the proof of more sid variables while the existing tool failed to do so and conservatively marked them as ukd variables.
c. effectiveness of rule induction and soundness verification to answer rq2 and rq3 we collected statistics while applyinggps to the small programs in dtest as shown in table iv.
in total gps took iterations to complete the entire learning process.
column shows the iteration number and column shows the time taken by the learner and the prover together.
columns show the number of inference rules learned during each iteration together with their types ukd sid and rud .
similarly columns show the number of verified inference rules and their types.
the next two columns show the following statistics the size of the learned decision tree tree learn in terms of thetable iii comparing the learned analyzer with sci nfer .
name astthe scinfer verification tool our learned analyzer time s ukd sid rud time s ukd sid rud b21 .
.
b22 .
.
b23 .
.
b24 .
.
b25 .
.
b26 .
.
b27 .
.
b28 .
.
b29 .
.
b30 .
.
b31 .
.
b32 197k .
.4k .4k .
.4k .4k b33 197k .
.8k .4k .6k .
.8k .4k .6k b34 197k .
.2k .4k .2k .
.2k .4k .2k b35 198k .
.6k 8k .8k .
.2k 8k .2k b36 197k .
.8k .4k .6k .
.8k .4k .6k b37 205k .
.6k .6k .6k .
.6k .6k .6k number of decision nodes the number of counter examples cex added by the prover ast cex which are added to the original training programs before the next iteration starts.
the last column shows the number of features generated by sygus these features are also added to the original feature set and then used by the learner during the next iteration.
results in table iv demonstrate the efficiency of both the learner and the prover.
within the learner the number of rules produced in each iteration remains modest on average indicating it has successfully avoided overfitting.
this is because the sygus solver is biased toward producing small features which by occam s razor are likely to generalize well.
furthermore any learned analysis rules have to pass the soundness check and this provides additional assurance against overfitting to the training data.
the prover either quickly verifies a rule or quickly drops it after adding a counter example to prevent it from being learned again.
in early iterations about half of all learned rules can be proved but as more counterexamples are added the quality of the learned rules improves and thus the percentage of proved rules also increases.
d. threats to validity our experimental evaluation focused on cryptographic software which is structurally simple and unlike general purpose software does not exercise complicated language constructs.
it is an interesting direction of future work to extend our techniques to these more general classes of software code.
a notable limitation in our work is the assumption of the knowledge base kb .
while kb is readily available for our application side channel analysis for other applications it might be non trivial to construct.
furthermore an incorrect kb might compromise the soundness of the learned rules although in this work we have carefully mitigated this threat by curating the proof rules from previous papers that have themselves formally verified the validity of these proof rules.
vii.
r elated work generating analyzers from examples.
while there are prior works on learning static analyzers they do not guarantee soundness.
for example the analyzer learned by bielik et al.
is sound with respect to programs in the 819table iv decision tree learning with feature synthesis different iterations with ast .
iteration time s rules learned rules verified treelearn astcex feature syntotal ukd sid rud total ukd sid rud .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
total .
training set not all programs written in the same programming language javascript .
they also need to manually modify the training programs to generate counter examples while our method generates counter examples automatically.
formal specifications.
there are also works on synthesizing static analyzers from formal specifications e.g.
proof rules or second order logic formulas as opposed to training data.
however they restrict the logic used to write the specification and as a result may not be expressive enough to synthesize practical analyzers.
users are also expected to write correct specifications which is a non trivial task.
in addition they cannot exploit the information provided by data.
learning based techniques.
there are several prior techniques using machine learning to conduct static program analyses .
such techniques focus on finding a suitable program tofeature embedding.
however they require the user to perform feature engineering which is known to be laborious.
some of these techniques do not take advantage of new features that may be learned from data instead they build classifiers based solely on existing features.
in contrast our method not only learn new analysis rules from data but also use sygus to synthesize new features automatically.
optimizing an analyzer.
it is possible to optimize an existing static analyzer which can be achieved by adjusting the level of abstraction learn heuristics and parameters make soundness accuracy trade offs or select sound transformers .
however such techniques fundamentally differ from our method because they assume the analyzer is already given and focus on optimizing its performance whereas we focus on synthesizing a new analyzer.
syntax guided synthesis.
since we automatically generate new features our method is related to the large and growing body of work on sygus.
while sygus has been used in variousapplications none of them aims to synthesize a provably sound static analyzer from data.
while some of these existing techniques can synthesize datalog rules the focus has been on efficiency e.g.
pruning the search space based on syntactic structures instead of guaranteeing the soundness of the analyzer.
power side channel analysis.
in this work we use power sidechannel analysis as the application to evaluate our method.
in this sense it is related to the body of work on side channel leak detection as well as mitigation .
while static analysis engines used in these existing works are all hand crafted by domain experts our method aims to synthesize the static analysis from data automatically.
viii.
c onclusions we have presented a data driven method for learning a provably sound static analyzer to detect power side channels in cryptographic software.
it relies on sygus to generate features and dtl to generate analysis rules based on the synthesized features.
it verifies the soundness of these learned analysis rules by solving a query containment checking problem using an smt solver.
we have evaluated our method on c programs that implement well known cryptographic protocols and algorithms.
our experimental results show that the learning algorithm is efficient and the learned analyzer can achieve the same empirical accuracy as state of the art analysis tools while being several orders of magnitudes faster.