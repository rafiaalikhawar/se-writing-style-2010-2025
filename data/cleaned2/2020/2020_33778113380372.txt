lazy product discovery in huge configuration spaces michael lienhardt onera the french aerospace lab france michael.lienhardt onera.frferruccio damiani university of turin italy ferruccio.damiani unito.it einar broch johnsen university of oslo norway einarj ifi.uio.nojacopo mauro university of southern denmark denmark mauro sdu.dk abstract highly configurable software systems can have thousands of interdependent configuration options across different subsystems.
in the resulting configuration space discovering a valid product configuration for some selected options can be complex and error prone.
the configuration space can be organized using a feature model fragmented into smaller interdependent feature models reflecting the configuration options of each subsystem.
we propose a method for lazy product discovery in large fragmented feature models with interdependent features.
we formalize the method and prove its soundness and completeness.
the evaluation explores an industrial size configuration space.
the results show that lazy product discovery has significant performance benefits compared to standard product discovery which in contrast to our method requires all fragments to be composed to analyze the feature model.
furthermore the method succeeds when more efficient heuristics based engines fail to find a valid configuration.
ccs concepts software and its engineering software product lines feature interaction abstraction modeling and modularity software libraries and repositories software creation and management keywords software product lines configurable software variability modeling feature models composition linux distribution acm reference format michael lienhardt ferruccio damiani einar broch johnsen and jacopo mauro.
.
lazy product discovery in huge configuration spaces.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea copyright held by the owner author s .
publication rights licensed to association for computing machinery.
acm isbn .. .
.
introduction highly configurable software systems can have thousands of interdependent configuration options across different subsystems.
in the resulting configuration space different software variants can be obtained by selecting among these configuration options.
the interdependencies between options are typically caused by interaction in the resulting software system.
constructing a well functioning software variant can be a complex and error prone process .
feature models allow us to organize the configuration space and facilitate the construction of software variants by describing configuration options using interdependent features a feature is a name representing some functionality a set of features is called aconfiguration and each software variant is identified by a valid configuration called a product for short .
highly configurable software systems can consist of thousands of features and combine several subsystems each with different features.
the construction and maintenance of feature models with thousands of features for such highly configurable systems can be simplified by representing large feature models as sets of smaller interdependent feature models which we callfragments.
however the analysis of such fragmented feature models usually requires the fragments to be composed to enable the application of existing analysis techniques .
to this aim many approaches for composing feature models from fragments have been investigated .
the analysis of fragmented feature models can be simplified if suitable abstractions can safely replace some of the feature model fragments in the analysis.
this simplification can be realized by means of feature model interfaces .
a feature model interface is a feature model that hides some of the features and dependencies of another feature model thus interfaces are closely related to featuremodel slicing .
an interface can be used instead of a feature model fragment to simplify the overall feature model.
for certain analyses working on the simplified feature model produces results that also hold for the original feature model and for any feature model where the interface is replaced by a fragment compatible with the interface.
this paper addresses automated product discovery in large configuration spaces represented as sets of interdependent feature models.
product discovery sometimes called product configuration is a particular analysis for finding a product which includes a desired set of features .
we aim at automatically discovering a product that contains a given set of features from the feature model fragments ieee acm 42nd international conference on software engineering icse without having to compose all the fragments to apply the analysis.
this work is motivated by our recent experiences in applying techniques for variability modeling to automated product discovery in industrial use cases such as gentoo a source based linux distribution that consists of many highly configurable packages.
the march 1st version of the gentoo distribution comprises features spread across feature models.
gentoo s huge configuration space can be seen as the composition of the feature models for all its packages where package interdependencies are modeled as shared features.
gentoo s official package manager and distribution system portage achieves via its emerge tool efficiency at the expense of completeness i.e.
in some cases this tool fails to discover a product that contains a given set of features although such a product exists.
we show that feature model interfaces which were developed to support analysis reuse for feature model evolution in fragmented feature models do not allow us to reach our aim of complete and efficient automated product discovery.
we propose a novel method for product discovery in sets of interdependent feature models.
the proposed method is lazy in the sense that features are added incrementally to the analysis until a product is found.
we provide a formal account of the method and evaluate it by implementing an efficient and complete dependency solver for gentoo.
in short our contributions are we strengthen feature model interfaces to enable lazy product discovery in sets of interdependent feature models we propose an efficient and complete algorithm for lazy product discovery in huge configuration spaces we provide an open source implementation of the proposed algorithm 1and we evaluate the potential of lazy product discovery in terms of experiments on an industrial size configuration space.
motivation and overall concept a software system like the gentoo distribution comprises configurable packages as of its march 1st version.
the configuration space of each package can be represented by a feature model the overall configuration space of gentoo can then be represented by a feature model that is the composition of the feature models of the packages.
the resulting feature model has features and thus a configuration space with up to solutions.
gentoo s official package manager portage implements an optimized heuristics based product discovery algorithm to find products in this configuration space.
this algorithm is not complete i.e.
it fails to solve some product discovery problems that have solutions.
to the best of our knowledge existing complete productdiscovery approaches need to load the entire feature model to find products.
consequently they do not scale to product discovery problems of the size of gentoo s configuration space.
in this paper we target product discovery in huge configuration spaces such as for gentoo that can be described by a feature model 1the lazy product discovery tool is available at and at archive.softwareheritage.org browse origin 2the evaluation artifact is available at and lazy product discovery algorithm 1inputs set of feature models 2inputc configuration 3vary c 4varm compose pick cut m y m s 5varsolution select m c 6while solution none solution y 7y y solution 8m compose pick cut m y m s solution select m c return solution represented as a set sof feature models with shared features where loading the overall feature model i.e.
the whole set s is too expensive.
we propose lazy product discovery a product discovery method that loads the elements of sincrementally until it finds a product of the overall feature model.
the method relies on the notion of a cut of a feature model mfor a set of features y. this is a feature modelm whose products are products of mand include all the products ofmthat contain a feature in y. the proposed algorithm shown in listing takes as input a setsof feature models with shared features and a set cof features to be included in the discovered product.
after initialization the algorithm incrementally loads cuts until a solution has been found.
letm0denote the composition of the feature models in s. the algorithm returns a not necessarily minimal product of m0which includes all features in c whenever such a product exists otherwise it returns the special value none.
the algorithm relies on the following three auxiliary functions pick cut m y a function that given a feature model m and a set of features y returns a cut ofmfory compose m ... mn a function that given a set of feature modelsm1 ... mn returns the composition of the feature models in the set and select m c a function that given a feature model mand a set of features c returns a product of mcontaining all the features incif it exists and none otherwise.
assuming that the auxiliary functions and work we have that on line the following loop invariants hold inv1 c y. inv2 solution is a product ofm which includes all features in c whenever such a product exists otherwise solution is the special value none.
inv3 ifsolution is a product ofm andsolution y then solution is also a product of m0.
inv4 ifm has no product which includes all features in c then neither doesm0.
checking that inv1 holds is straightforward just observe that on line the variable yis initialized to cand that at each iteration of thewhile loop new features are added to yon line .
checking that inv2 holds is equally straightforward according to the description of the auxiliary functions and the invariant is established on lines and as well as on lines and .
the fact that inv3 and inv4 hold is shown in the proof of theorem in section .
the 1510algorithm terminates because at each iteration of the while loop the size of the set y which by construction only contains features from the features models in s increases.
when the algorithm terminates we have that either solution none ornone solution y. in the first case by inv4 we have thatm0has no product that contains all the features in c while in the second case by inv3 we have that solution is a product ofm0that contains all the features in c. the laziness of this algorithm stems from the fact that it does not need to consider m0at once.
instead the algorithm starts by considering the composition of the cuts of the feature models for y cand then iterates by considering bigger and bigger cuts until the candidate solution is contained in the set y. when this happens we know for the properties of the cut that the found solution is also a solution form0.
the algorithm s efficiency in finding a product with the features inc see lines and of listing compared to executing select m0 c depends on the degree to which the feature models insare such that computing pick cut m y is efficient the feature models m are small compared to m0 select m c performs better than select m0 c and a small number of iterations of the while loop is required.
for the gentoo distribution each feature model miinshas a distinguished feature fisuch that the constraints expressed by miare enabled only if fiis selected see section .
.
this reflects that eachmicorresponds to a gentoo package that is installed if and only iffiis selected.
therefore the function pick cut m y can be efficiently implemented by returning miiffi y and by returning a feature model that expresses no constraints and can therefore be ignored by the composition that builds m otherwise.
the rest of this paper is organized as follows sections provide a formal account of the lazy product discovery method that culminates in the proof that inv3 andinv4 hold section evaluates the performance of the lazy product discovery algorithm by means of experiments and sections and discuss related work and conclude the paper respectively.
a formalization of feature models this section presents a formalization of feature models fm and related notions including feature model interfaces and composition.
.
feature model representations different representations of feature models are discussed e.g.
by batory .
in this paper we will rely on the propositional formula representation of feature models.
in this representation a feature model is given by a pair f where fis a set of features and is a propositional formula where the variables xare feature names x .
a propositional formula over a set of features frepresents the feature models whose products are configurations x1 ... xn f n such that is satisfied by assigning value true to the variablesxi i n and false to all other variables.
example a propositional representation of glibc fm .
gentoo packages can be configured by selecting features called useflags in gentoo which may trigger dependencies or conflicts between packages.
version .
of the glibc library that contains the core functionalities of most linux systems is provided by the package syslibs glibc .
r2 abbreviated to glibc in the sequel .
this package has many dependencies including as expressed in gentoo s notation doc?
sys apps texinfo vanilla?
!sys libs timezone data this dependency expresses that glibc requires the texinfo documentation generator provided by any version of the sys apps texinfo package whenever the feature docis selected and if the feature vanilla is selected then glibc conflicts with any version of the time zone database as stated with the !sys libs timezone data constraint .
these dependencies andconflicts can be expressed by a feature model fglibc glibc where fglibc glibc txinfo tzdata glibc doc glibc v and glibc glibc glibc doc txinfo glibc v tzdata .
here the feature glibc represents the glibc package txinfo represents any sys apps texinfo package tzdata represents any version of the sys libs timezone data package and glibc doc resp.
glibc v represents the glibc s doc resp.
vanilla use flag.
the propositional representation of feature models works well in practice and we shall use it for the evaluation of the proposed method in section .
in contrast to simplify the proofs we follow schr ter et al.
in using an extensional representation of feature models to present our theory.
definition feature model extensional representation .
afeature modelmis a pair f p wherefis a set of features and p 2fa set of products.
example an extensional representation of glibc fm .
let2xdenote the powerset of x. the feature model of example can be given an extensional representation mglibc fglibc pglibc wherefglibc is the same as in example and pglibc glibc glibc txinfo glibc tzdata glibc txinfo tzdata glibc glibc doc txinfo glibc glibc doc txinfo tzdata glibc glibc v glibc glibc v txinfo glibc glibc doc glibc v txinfo txinfo tzdata glibc doc glibc v .
in the description of pglibc the first line contains products with glibc but none of its use flags are selected so texinfo andtzdata can be freely installed the second line contains products with the use flag doc selected in glibc so a package of sys apps texinfo is always required the third line contains products with the use flag vanilla selected in glibc so no package of sys libs timezone data is allowed the forth line contains products with both glibc s use flags selected sosys apps texinfo is mandatory and sys libs timezone data forbidden finally the fifth line represents products without glibc so all combinations of other features are possible including the empty set.
definition empty fm void fms and pre products .
the empty feature model denoted m has no features and has just the empty product .
avoid feature model is a feature model that has no products i.e.
it has the form f for somef.
apre product of a feature modelmis a configuration cthat can be extended to a product ofm more formally c pfor some product pofm .
1511based on the above definition of a pre product we identify two related search problems.
definition feature compatibility product discovery .
consider a feature model mand a set of features cinm.
the featurecompatibility problem forcinmis the problem of determining whethercis a pre product ofm i.e.
whether the features in care compatible with the products in m .
the product discovery problem forcinmis the problem of finding a product of mthat extends c. clearly the feature compatibility problem for cinmhas a positive answer if and only if the product discovery problem for cin mhas a solution.
.
feature model interfaces feature model interfaces were defined by schr ter et al.
as a binary relation expressing that a feature model m is an interface of a feature modelmifm ignores some features of m. definition fm interface relation .
a feature modelm f p is an interface of feature modelm f p denoted as m m ifff f andp p f p p .
note that for all feature models m f p andm if m m then i all products of m are pre products of mand ii m is the only interface of mwhich has exactly the features f i.e.
m is completely determined by f .
example an interface for glibc fm .
the feature model f glibc glibc v p glibc glibc glibc v is the interface of the feature model mglibc from example that is determined by the features glibc andglibc v. the interface relation for feature models is a partial order i.e.
it is reflexive transitive and anti symmetric and the empty feature modelm is an interface of every non void feature model m. moreover mis void if and only if m .
the notion of a feature model interface is closely related to that of afeature model slice which was defined by acher et al.
as a unary operator yrestricting a feature model to a set yof features.
given a feature model m y m is the feature model obtained frommby removing the features not in y. definition fm slice operator .
theslice operator yon feature models where yis a set of features is defined by y f p f y p y p p .
note that for every feature model m f p and set of featuresy the feature model y m f p is the unique interface ofmsuch thatf f y. moreover for every interface m1 f1 p1 ofmit holds thatm1 f1 m .
example a slice of glibc fm .
the feature model interface in example can be obtained by applying glibc glibc v to the feature modelmglibc of example .
.
feature model composition highly configurable software systems often consist of many interdependent configurable packages .
the variabilityconstraints of each of these packages can be represented by a feature model.
therefore configuring two or more packages in such a way that they can be installed together corresponds to identifying a product in a suitable composition of their associated feature models.
in the propositional representation of feature models such composition corresponds to logical conjunction i.e.
the composition of two feature models f1 1 and f2 2 is the feature model f1 f2 1 2 .
in the extensional representation of feature models this form of composition corresponds to the binary operator of schr ter et al.
which is similar to the join operator from relational algebra .
definition fm composition .
the composition of two feature modelsm1 f1 p1 andm2 f2 p2 denotedm1 m is the feature model defined by m1 m f1 f2 p q p p1 q p2 p f2 q f1 .
the composition operator is associative and commutative withm as identity element i.e.
m m m .
composing a feature model with a void feature model yields a void feature model f1 p1 f f1 f2 .
example composing glibc and gnome shell fms .
let us consider another important package of the gentoo distribution gnomeshell a core component of the gnome desktop environment.
version .
.
of gnome shell is provided by the package gnome base gnomeshell .
.
r2 abbreviated to g shell in the sequel and its dependencies include the following statement networkmanager?
sys libs timezone data .
this dependency expresses that g shell requires any version of the time zone database when the feature networkmanager is selected.
thepropositional representation of this dependency can be captured by the feature model fg shell g shell where fg shell g shell tzdata g shell nm and g shell g shell g shell nm tzdata .
the corresponding extensional representation of this feature model ismg shell fg shell pg shell where pg shell g shell g shell tzdata g shell tzdata g shell nm tzdata g shell nm .
here the first line contains products with g shell but none of its use flags are selected tzdata can be freely selected the second line is the product where g shell nm is also selected and tzdata becomes mandatory finally the third line represents products without g shell.
thepropositional representation of the composition is the feature model ffull full where ffull fglibc f g shell glibc txinfo tzdata g shell glibc doc glibc v g shell nm and full glibc g shell glibc glibc doc txinfo glibc v tzdata g shell g shell nm tzdata .
1512theextensional representation of the composition is the feature modelmfull mglibc m g shell ffull pfull where pfull pglibc p g shell txinfo tzdata glibc doc glibc v g shell nm glibc g shell p p txinfo tzdata glibc glibc doc txinfo g shell p p tzdata glibc glibc v g shell p p txinfo glibc g shell g shell nm tzdata p p txinfo glibc glibc doc glibc v txinfo g shell glibc glibc doc txinfo g shell g shell nm tzdata .
here the first line contains the products where glibc andg shell do not interact i.e.
either when they are not installed or only one of them is installed the second line contains the products where both glibc and g shell are installed but without use flags selected so all optional package can be freely selected the third line contains the products with the glibc s use flag docselected so sys apps texinfo becomes mandatory the fourth line contains the products with the glibc s use flag vanilla selected so sys libs timezone data is forbidden the fifth line contains the products with the g shell s use flag vanilla network manager so sys libs timezone data is mandatory the sixth line contains the product with glibc s both use flags selected and the seventh line contains the product with glibc s use flag docand g shell s use flag networkmanager are selected.
problem statement many case studies show that the size of feature models used to model real configuration spaces can be challenging for both humans and machines including the feature model for the source based linux distribution gentoo mentioned above.
the state of the art strategy used to address this challenge is to represent large feature models by sets of smaller interdependent feature models .
the resulting interdependencies between different feature models can be expressed using shared features .
the feature compatibility problem for a given set of features see definition can be decided without first composing the considered feature models when the feature models are disjoint as it suffices to inspect each feature model independently.
namely feature model slices can be used to formulate a feature compatibility criterion for the case with no shared features between the feature models as shown by the following theorem theorem feature compatibility criterion for disjoint fms .
consider the feature models mi fi pi i n with pairwise no shared features i.e.
i j nimpliesfi fj .
then a configuration cis a pre product of the feature model m i nmiif and only if cis a subset of i nfiand for allmi the configuration c fiis a product of c mi .
proof.
letm f p .
case .sincecis a pre product ofm by definition there exist p p such thatc p. hencec f i nfi.
let now consider c mi for any i n by definition p c fiis a product of this feature model and by construction p c fi c fi.
hence c fiis a product of c mi for any i n. case .since for any i n c fiis a product of c mi there existpi pisuch thatc fi pi c. let consider theconfiguration p i npi.
since the feature models mido not share features we have pi fj pj fifor all i j n. hencepis a product ofm.
moreover we have that p c i n pi c i n c fi c i nfi c. hencec pholds which means that cis a pre product of m. unfortunately the feature compatibility criterion of theorem does not work for feature models with shared features.
the problem can be illustrated by the following example.
example feature compatibility with shared features .
consider the two feature models mglibc andmg shell from examples and and the configuration c glibc glibc v g shell g shell nm .
we have c m glibc glibc glibc v glibc glibc v and c m g shell g shell g shell nm g shell g shell nm .
here we have that c f glibc f g shell and it is clear from the previous equation that c f glibc glibc glibc v is a product of c mglibc and thatc fg shell g shell g shell nm is a product of c mg shell .
however cis not a pre product of mglibc m g shell since the use flag g shell nm requires a timezone database to be installed while the use flag glibc v forbids it.
in this paper we address complete and efficient product discovery in sets of interdependent feature models.
to this aim we define a novel criterion which given some selected features enables solving the product discovery problem for a set of feature model fragments with shared features without composing all the fragments.
lazy product discovery we are looking for a product discovery criterion which works for interdependent feature models similar to how the featurecompatibility criterion given in theorem works for disjoint feature models.
the solution lies in a novel criterion based on strengthening the feature model interfaces.
given feature models with shared featuresmi fi pi and a set of selected features c we need feature model interfaces m ithat reflect how cis related to other features inmiin order to guarantee that the interface behaves similarly tomiwith respect to the feature compatibility problem forc.
more formally the interface m imust satisfy the following conditions c mi m i and the products of m iare among the products of mi.
example feature compatibility with shared features continued .
consider feature models mglibc andmg shell and configurationc as discussed in example .
let c1 glibc glibc v and c2 glibc tzdata glibc v .
we can see that the interface m glibc c2 mglibc ofmglibc satisfies with i glibc conditions and above.
since c mglibc c1 mglibc andc2 c1 tzdata this shows that it is important to consider the feature tzdata when checking whether cis a pre product of a composed feature model includingmglibc.
let us now introduce terminology for different restrictions to the interface relation that satisfy one or both of the conditions and given above and investigate some of their properties.
1513definition fm extended slice conservative interface and cut relations .
given a set of features yand two feature modelsm f p andm f p we say that m is an extended slice for yofm denotedm ym iff y m m m holds m is aconservative interface ofm denotedm m iff bothm m andp p hold and m is acut foryofm denotedm ym iffm is both an extended slice for yand a conservative interface.
note that .
the relation is a partial order the feature model is the minimum i.e.
the smallest w.r.t.
both and conservative interface of every void feature model and the empty feature modelm is the minimum conservative interface of every feature model that has the empty product.
the following theorem proves in a constructive way the existence of the minimum cut of mfory for any feature model m f p .
let the minimal products ofmbe the products that are not included in other products and let y f y be the set of features ofmthat occur in y. intuitively the minimum cut of mforyis the feature model obtained from y by incrementally adding all the minimal products of m and their features that contain a feature occurring in the feature model until a fixed point is reached.
theorem characterization of the minimum cut .
for all setsyof features and all feature models m f p let y m be the minimum cut of mfory i.e.
y m min m m ym .
then y m f f y wherefis the function between feature models defined by f f1 p1 f1 p p2p p1 p withp2 p p p p p p p p f .
proof.
letm f y and consider the partially ordered set of feature models s defined by s f p f y f f p p and f1 p1 f p2 iff f1 f and p1 p .
it is straightforward to see that s is a complete lattice with minimumm and maximumm and thatfis monotonic increasing for .
hence by f m exists and is the minimum fixpoint off.
we prove that the fixpoints of fare exactly the cuts of mfory.
let us first consider a feature model my fy py that is a cut ofmfory.
sincemy fy m andpy p for allp p we havep fy p. this implies that for any p p py there exists p pwithp psuch that p p fy .
by definition we havef my p p2p fy py p with p2 p p p p p p p p fy py.
hencef my my.
let us now consider a feature model m y f y p y inssuch thatf m y m y. first it is clear by construction that p y p. moreover if we write p p p p p p p p it is clear from the definition of fthatp p y. suppose that thesetm p p p f y p y is not empty and consider p1 a minimal element of mw.r.t.
.
sincep1 f y by definition of p the setn p p y p p1 is not empty.
consider any maximal element p2ofnw.r.t.
.
sincep1 f y p y we have p1 p2 f y and so the condition p p p p1 p1 p f y holds.
it follows that m yis not a fixpoint off since applying ftom ywould add the product p1 which contradicts the hypothesis.
hence for all p p p f y p y this means thatm y f y m .
since by construction y f f y we have y m m y m m yis a cut ofmfory.
to conclude observe that the orders and are equal on the set of cuts ofmfory.
sincef m is the minimum fixpoint of f w.r.t.
it is also the minimum cut of mfory.
example a minimum cut ofglibc fm .
consider the feature modelmglibc of example and y glibc glibc doc .
the minimal cut y mglibc can be computed by starting with the feature model y and then applying f. in the first application off the setp2collects the products glibc glibc doc and glibc glibc doc txinfo .
the setf1after the first application becomes glibc glibc doc txinfo and therefore in the second application of f the products txinfo glibc txinfo and glibc doc txinfo are added top2.
at this point further applications of fdo not add further products.
in this case the minimum cut y mglibc is different from the slice y mglibc since the cut keeps the information that when glibc andglibc doc are selected then txinfo also has to be selected.
the following theorem proves sufficient criteria to guarantee that a product of the composition of cuts is also a product of the composition of the original feature models and conversely that the original feature model does not have a product that contains a given set of features.
intuitively given a set of features yand a productpof the composition of cuts for y ifpis a subset of y we have that pis also a product of the composition of the original feature models.
moreover if the composition of cuts for yhas no products with the features in a set c y then neither does the the original feature model.
theorem product discovery criterion for interdependent fms .
consider a set yof features a finite set iof indices and two sets of feature models mi fi pi i i and m i f i p i i i such that for all i i m i ymi.
letm f p i imiandm f p i im i. then each product pofm such thatp yis a product ofm and for each set of features c yand for each product pofmsuch thatc p there exists a product qofm such thatc q p. proof.
consider a product p p .
by construction for every i i there exists pi p isuch thatp i ipiand for alli j i pj f i pi f i. by definition for all i i sincepi p i we have that pi pi.
let us now consider i j i. we have that pi fj pi y fj pi f j pj f i pj y fi pj fi.
hence p i ipi p. by definition since m i ymi we have y mi m i. then for all i i there exists yisuch thatc y yiand yi mi m i. consider a product p p such thatc p. by definition for all i i there exists pi pisuch thatp i ipi 1514and for alli j i we havepi fj pj fi.
letq i i pi yi .
clearlyc q p. moreover consider i j i sincepi fj pj fi holds we have pi yi fj yj pi fj yi yj pj fi yi yj pj yj fi yi .
henceq p .
example using the product discovery criterion with glibc and g shell fms .
consider the packages glibc andg shell of example and the set y glibc glibc v tzdata .
it is easy to see that the minimum cut of mglibc foryis y mglibc y 2y y because tzdata can not be selected when glibc andglibc v are selected.
now consider the package g shell instead.
the minimum cut of mg shell foryis y mg shell y 2y .
by the definition of feature model composition we have that y mglibc y mg shell is the same as y mglibc .
now due to theorem we can for example derive that the product glibc tzdata that contains the shared feature tzdata is also a product of the composition of mglibc andmg shell .
note that to discover this fact we avoided computing the composition of the entire feature models and could ignore e.g.
features such as glibc doc andg shell.
the criteria provided by theorem allow us to prove that the lazy product discovery algorithm listing in section is correct and complete.
theorem soundness and completeness of lazy product discovery .
given a finite set iof indices a set of feature models s mi fi pi i i such that all products of miare finite and a finite configuration c the lazy product discovery algorithm listing applied to sandcalways finishes and returns a product of i imithat contains cif and only if such a product exists.
proof.
recall the definitions of auxiliary functions section pick cut m y m for somem s.t.m ym compose m ... mn m1 mn select m c is a product ofmcontaining all the features incif such a product exists none otherwise and the loop invariants inv1 inv4 on line .
in section we have already shown that the invariants inv1 andinv2 hold and that the algorithm always finishes because the set of examined features y which strictly increases during each traversal of the while loop is bounded by i i p pip c which is finite by hypothesis .
we can now conclude the proof by observing that the invariants inv3 andinv4 follow straightforwardly from theorem and theorem respectively.
it is worth observing that a suitable structure of the feature models can enable a particular efficient implementation of the function pick cut m y .
for instance if the feature model mis propositionally represented with a pair of the form f f for some set of featuresf featuref f and formula then whenever f y pick cut m y can return the feature model y 2y withy y f which corresponds to the pair y true in propositional representation.
therefore feature models of the form f f such thatf ycan be filtered away before computing the composition compose pick cut m y m s in lines and of the algorithm.
evaluation with lazy product discovery we aim to efficiently address the product discovery problem in huge configuration spaces consisting of hundreds of thousands of features in tens of thousands of feature models.
therefore we evaluate the performance of the lazy product discovery algorithm introduced in section .
the proposed algorithm loads feature model fragments by need to examine specific features.
a feature is loaded during a configuration process if it occurs in one of the loaded feature model fragments.
in contrast standard product discovery algorithms e.g.
load all the feature models before the product discovery process starts.
we compare the number of loaded features the time and the memory needed to solve a product discovery problem using a lazy and a standard product discovery algorithm.
in detail we investigate the following research questions rq1.
how is the number of loaded features affected by the choice of a lazy or a standard product discovery algorithm?
rq2.
how are the speed and memory consumption of product discovery affected by the choice of a lazy or a standard product discovery algorithm?
in industrial practice product discovery tools are often optimized for efficiency at the expense of completeness.
as a consequence there may be product discovery problems for which solutions exist but no solution is found by the tool.
we compare the lazy product discovery algorithm to one such state of the art tool by looking at the percentage of cases in which no product is found by the state of the art tool although products exists and at the difference in performance for cases when the state of the art product discovery tool return a correct answer that is it either discovers a product or fails when there are no products .
for this purpose we investigate the following research questions rq3.
how often does a state of the art product discovery tool fail because of its incompleteness i.e.
the tool does not discover any product although there is at least one product ?
rq4.
is lazy product discovery a feasible alternative to state of theart product discovery tools in terms of execution time and memory consumption?
.
experimental design and subject to answer these research questions we performed experiments on an industrial system with a huge configuration space.
we chose gentoo a source based linux distribution with highly configurable packages which is among the largest fragmented feature models studied in the literature .
the experiments were performed on the march 1st version of the distribution that contained feature models with features overall.
there are no standard benchmarks for product reconfiguration requests.
therefore we constructed a set of product discovery problems for the evaluation.
the problems were generated by randomly selected a set of features between one and ten such that each of these features requires the installation of a different package.
solving a product discovery problem cin this context amounts to computing a gentoo product that includes any version of the 1515packages associated to the features in cand of other packages such that that all dependencies are fulfilled.
we implemented the algorithm of listing as a tool.
this tool called pdepa targets gentoo s package dependencies which are defined using an ad hoc syntax .
as shown in example gentoo s dependencies can be encoded into feature models where features represent both packages and configuration options called use flags in gentoo .
pdepa parses a package dependency and generates the equivalent propositional formula representing the package feature model.
a particularity of gentoo is that the feature model of a packagefcan be translated into a propositional representation of the form f f where a package selection feature frepresents the package f. the pdepa tool exploits this structure of the feature model in the implementation of the key functions pick cut and compose by using the optimization discussed at the end of section .
specifically pdepa can avoid loading the feature models of packages whose package selection feature is not in the set yof required features when composing cuts listing lines and .
as its solving engine pdepa uses the state of the art smt solver z3 known for its performance and expressivity.
solvers such as z3 allow constraints to be added incrementally reusing part of the search done previously without always restarting the search from scratch.
this is extremely useful for composing cuts listing lines and since the existing constraints can be reused only adding incrementally the new constraints not implied by the existing ones.
although this does not formally reduce the complexity of the algorithm which is np hard in the worst case 3in practice these optimizations enable a significant speed up.
to investigate the research question rq we need to compare pdepa to a standard product discovery algorithm.
unfortunately there is no off the shelf complete product discovery tool for gentoo and therefore we implemented one to establish a baseline for our experiments.
we constructed a software that loads all the feature models of all the gentoo packages and then as done by pdepa calls the smt solver z3 to solve the configuration problem.
we then compared the results of pdepa to the corresponding results of this baseline tool baseline for short in terms of computation time and memory consumption.
to ensure a fair comparison we employ a white box evaluation and both pdepa and the baseline use the same implementation for translating the gentoo dependencies and for loading the feature models.
for research questions rq andrq we compare the results of pdepa to the corresponding results of optimized heuristics based product discovery with emerge the command line interface to gentoo s official package manager and distribution system portage which is not complete i.e.
it fails to solve some product discovery problems that have solutions .
all experiments were performed on virtual machines provided by the iaas openstack cloud of the university of oslo.4every virtual machine had gb of ram vcspus .
ghz intel haswell processors and was running an ubuntu .
operating system.
the gentoo operating system was virtualized by running docker and the image used for the experiments is publicly available.
3the np hardness derives immediately from the np hardness of the problem of finding a valid model for a propositional formula.
results and discussion this section is organized according to research questions rq1 rq4.
to facilitate the discussion of the experiments the figures presenting the different results use a fixed ordering of the product discovery problems we considered along the x axis this ordering is determined by the number of features loaded by pdepa during its computation for a given problem.
each of the experiments was repeated times for pdepa for emerge and for the baseline figures report the mean values for each experiment.
rq .
figure shows the results of the experiments for research question rq and reports on the number of features loaded by pdepa to solve each product discovery problem.
to highlight how lazy product discovery performs compared to standard product discovery which needs to load all features before the analysis can start these numbers are shown as the percentage of features from the full feature model for each of the product discovery problems.
the product discovery problems have been sorted along the xaxis according to this percentage.
the figure shows the loaded features as a full line the mean number for all the product discovery problems as a dashed line and the standard deviation abbreviated to sd in the figures as a the bar.
we see that for the considered product discovery problems the mean number of loaded features is only .
of the overall number of features.
in summary the gain in loaded features when solving each of the considered productdiscovery problems using lazy product discovery over standard product discovery is significant.
rq .
for research question rq we compared the speed and memory consumption of product discovery when using pdepa and the baseline on the defined product discovery problems.
for each problem pdepa loads parts of the fm and calls z3 incrementally until a valid product for the whole fm is found while the baseline first loads the whole fm and then calls z3.
figure shows the computation time for product discovery using pdepa green line and figure shows the computation time for product discovery using the baseline.
the mean execution time for the baseline is seconds compared to seconds for pdepa.
the minimum and maximum execution times of the baseline are .
and .
seconds respectively.
the standard deviation for the baseline is negligible around seconds .
it is worth mentioning that about one third of the execution time is devoted to loading the overall feature model while the remaining time is taken by z3.
the minimum and maximum execution time of pdepa are .
and .
seconds respectively.
the standard deviation is lower than the one for the baseline about seconds.
the maximum computation time ofpdepa is less than one third of the computation time used by the baseline to simply load the overall feature model and it is about the of the minimum execution time of the baseline.
figure shows the memory consumption for product discovery using pdepa green line and figure shows the memory consumption for the baseline.
the mean memory consumption for the baseline is .
mb compared to .
mb for pdepa.
the minimum and maximum memory consumption of the baseline are and mb respectively.
about gb of the used memory here is for the feature model itself.
the standard deviation for the baseline is negligible about .
mb .
the memory consumption .
.
.
.
loaded features sd meanpercentage figure features loaded by pdepa.
.
.
.
.
.
.
.
.226pdepa emerge pdepa mean emerge mean pdepa sd emerge sdseconds figure execution times for pdepa andemerge.
.
.
.
.9baseline mean sdseconds figure baseline execution time.
values that fall outside the standard deviation correspond to the product discovery problems that have no solution.
the minimum and maximum memory consumption of pdepa are and mb respectively.
the standard deviation .
mb is about the same as for the baseline.
the maximum memory consumption of pdepa is about .
of the minimum memory consumption of the baseline.
the experiments show a clear correlation between the time and the memory taken by pdepa to solve a product discovery problem and the number of features loaded by pdepa cf.
figure .
in summary the experiments clearly demonstrate that lazy product discovery allows significant speed up and significant reduction of memory consumption compared to standard product discovery.
rq .
we investigated the failures of a heuristics based incomplete product discovery tool emerge compared to the cases when the complete lazy product discovery algorithm showed that no solution exists for the considered product discovery problems.
figure shows the product discovery problems for which emerge does not find a product red and blue bars .
for the considered0 .
.
.
.
.
.
.371pdepa emerge pdepa mean emerge mean pdepa sd emerge sdmb figure memory consumption for pdepa andemerge.
.
.
.0baseline mean sdmb figure baseline memory consumption.
.
.
no solution exists emerge failureno solution found figure product discovery problems with no solution and emerge failures.
product discovery problems emerge fails to find a valid configuration in .
of the cases.
in of the cases red bars no solution exists.
therefore in of the cases emerge fails to solve a product discovery problem that has a solution.
the experiments show an interesting correlation between the failures of emerge observed in figure and the number of features loaded by pdepa during the product discovery process the failures of emerge occur more frequently as the number of loaded features needed for lazy product discovery increases.
this can be seen since the sorting of thex axis is the same in figures and .
in summary on randomly selected product discovery problems emerge fails to find a solution that exists in around of the cases.
rq .
for research question rq we investigated how well pdepa performs as an alternative to the state of the art configuration tool emerge.
figure shows the time for product discovery using pdepa green line and emerge blue line .
the light green and the light blue bars show the standard deviations and the correspondingly colored dashed lines show the mean times in seconds 1517forpdepa andemerge respectively.
the difference in mean times suggests that pdepa is .
times slower than emerge in average which corresponds to additional seconds.
however as the results for rq above shows that emerge fails for a significant number of the considered product discovery problems lazy product discovery appears to be a feasible alternative to emerge.
figure shows the memory consumption for product discovery using pdepa green line and emerge blue line .
the light green and the light blue bars show the standard deviations and the corresponding colored dashed lines show the mean memory consumption in mb for pdepa andemerge respectively.
the difference in mean times suggests that pdepa consumes four times more memory than emerge in average which amounts to around mb .
in summary lazy product discovery appears as a feasible alternative to emerge if around one order of magnitude additional computation time and four times additional memory consumption are acceptable to always find products when these exist.
.
threats to validity .
.
external validity.
the results of the evaluation strongly depend on the product discovery problems considered in the experiments i.e.
on the feature models of the gentoo packages identified by the features in each product discovery problem.
due to the lack of standard benchmarks we considered product discovery problems that were randomly selected from the features of the march 1st version of the gentoo distribution.
the random selection used the standard random python library that allows to get a set of elements uniformly chosen from a given set.
different product discovery problems could potentially lead to different results.
we plan to investigate other product discovery problems for gentoo and for other domains to get more insights.
in particular it would be interesting to investigate how lazy product discovery performs when varying both the size and the amount of interdependencies of the feature models see section .
.
.
internal validity.
we used prototype implementations of the lazy product discovery algorithm and of the standard productdiscovery algorithm.
both implementations rely on the z3 solver .
z3 was chosen because it is a mature solver and freely available.
the standard product discovery algorithm just performs a call to the z3 solver.
the lazy product discovery algorithm calls the z3 solver whenever a new feature fragment is loaded.
using a different solver than z3 may affect the execution time and memory consumption of both the standard and the lazy product discovery algorithms.
we plan to repeat the experiments using another solver.
introducing optimizations in the lazy product discovery algorithm could potentially reduce the number of loaded features the execution time and the memory consumption for the algorithm.
one possible optimization could be to pre compute at compile time the modal implication graphs of features which could potentially avoid loading feature models that e.g.
are found to be conflicting in the pre analysis.
another possible optimization could be the definition and usage of an ad hoc search strategy for the back end solver instead of using solver s default search strategy.
another threat to validity is that gentoo s package dependencies are not formally specified but only given in a textual representation.
to reduce the probability of errors in the implementation of the lazyproduct discovery algorithm we have used unit tests to compare the results of pdepa with known correct products.
these unit tests were performed by extending the package repository of portage with custom testing and interdependent packages.
possible bugs in gentoo s package manager may also be considered a threat to validity.
when performing the experiments we identified the following surprising behavior in emerge for some sets of packages6 emerge implements a heuristic that only considers the feature model of the most recent package in the set thus forgetting possible solutions.
foremerge to consider a package some part of its feature model must be configured.
specifically some of its features must be selected or deselected such that the constraint identified by the variable required use evaluates to true.
for a given product discovery problem the dependency analysis of emerge considers each package individually.
this can trigger the installation of a package in conflict with the rest of the product discovery problem thus preventing the productdiscovery problem to be solved even if it has a solution.
we reported these issues to the gentoo developer community which replied that they could be considered as bugs of emerge.
we were not able to install the gentoo variants corresponding to the products discovered by pdepa because of bug above.
indeed in many cases emerge s dependency solver triggers the installation of packages that conflict with pdepa s solution.
we plan to overcome this limitation by extending pdepa into a complete package installation tool for gentoo.
related work we discuss related work on interfaces composition and configuration of feature models.
interfaces of feature models.
the feature model cut in this paper strengthens the feature model interfaces introduced by schr ter et al.
which as pointed out in section .
are closely related tofeature model slices introduced by acher et al.
.
in the work of acher et al.
the focus is on feature model decomposition.
in subsequent work acher et al.address evolutionary changes for extracted variability models by using the slice operator in combination with a merge operator and focus on detecting differences between feature model versions during evolution.
instead schr ter et al.
study how feature model interfaces can be used to support evolution for a feature model composed from feature models fragments.
changes to fragments which do not affect their interfaces do not require the overall feature model to be rebuilt by composing the fragments in order to reanalyze it.
challenges encountered to support evolution in software product line engineering have previously been studied by dhungana et al.
.
they use interfaces to hide information in feature model fragments and save a merge history of fragments to give feedback and facilitate fragment maintenance.
no automated analysis is considered.
in contrast to this work on feature model interfaces for evolution the cut in our work is for efficient automated product discovery in huge feature models represented as interdependent feature model fragments.
6these sets consisted of packages with an identical slot .
slots are used in portage to identify which versions of the same package can coexist in one system.
1518feature model views focus on a subset of the relevant features of a given feature model similarly to feature model interfaces.
different views regarding one master feature model are used to capture the needs of different stakeholders so that a product of the master feature model can be identified based on the views partial configurations.
this work on multiple views to a product in a feature model is orthogonal to our work on feature model cuts which targets the efficient configuration of systems comprising many interdependent configurable packages.
composition of feature models.
feature model composition is often used for multi software product lines i.e.
sets of interdependent product lines .
eichelberger and schmid provide an overview of textual modeling languages which support variability model composition like familiar velvet tvl vsl and compare how they support composition modularity and evolution.
acher et al.
compare different featuremodel composition operators by considering possible implementations and discuss advantages and drawbacks.
for the investigation of efficient automated configuration of huge feature models in this paper we use the propositional representation of feature models and a composition operator that corresponds to logical conjunction.
configuration of feature models.
product discovery also called product configuration or product derivation is the process of selecting and deselecting features in a feature model in order to obtain a product .
this is a central and widely studied problem in the field of automated reasoning e.g.
more than different methods for product discovery are discussed in a recent survey .
we are not aware of any method that addresses how complete and efficient product discovery can be achieved in configuration spaces comprising different interdependent feature model fragments without composing all the fragments.
the tool for lazy product discovery is in the class of product discovery tools which automatically produce valid configurations.
automated configuration is supported by a number of tools including featureide gears guidsl ibed hyvarrec satibea s2t2 configurator sip spl conqueror s.p.l.o.t.
and variamos .
however in contrast to our work all these tools are eager and require the building of the global feature model by composing all its fragments.
as such these tools are in line with the standard product discovery algorithm as discussed in section .
some of these standard product discovery tools are interactive i.e.
they support and interact with the user by guiding her in producing a valid configuration or finding one that maximizes her preferences .
our method for lazy product discovery can be exploited to support interactive product discovery either i by requiring the user to enter preferences over different configurations or ii by interacting with the user when deciding what partial configuration should be extended i.e.
when the select function of the algorithm in listing is performed .
an extension of the lazy product discovery algorithm in this direction is left as future work.
different computational techniques can be used to solve the product discovery problem satisfiability solvers constraint programming evolutionary algorithms stochastic algorithms or binary decision diagrams .
due to the np hardness of the configuration problem itself most complete approaches rely onsat solvers but more recently the use of more powerful backend solvers such as constraint solvers and smt solvers are starting to be explored for automatic configuration of feature models .
in our work we have used z3 which is one of the most powerful and mature smt solvers available today.
we would like to remark however that the lazy product discovery method itself is orthogonal to the tool chosen as long as the backend solver allows to implement the pick cut compose and select operations of listing .
conclusion and future work product discovery in huge configuration spaces represented as sets of interdependent feature models is challenging.
standard analysis techniques for fragmented feature models require all the feature models to be composed in order to apply the analysis.
recent work has shown that several analyses of fragmented feature models can be simplified using techniques such as feature model interfaces and slicing however these techniques do not work for product discovery in sets of interdependent feature models.
in this paper we introduce a method for automated product discovery in configuration spaces represented as sets of interdependent feature models.
the method is lazy as features are added incrementally to the analysis until a product is found.
we introduce and formalize the feature model cut and leverage this concept to define a product discovery criterion.
we exploit this criterion to define a complete and efficient algorithm for lazy product discovery in sets of interdependent feature models.
we have evaluated the potential of lazy product discovery on randomly constructed configuration problems for the configuration space of the source based linux distribution gentoo with interdependent feature models and a total of features.
the evaluation has demonstrated significant gains compared to standard product discovery and that the trade off of performance for completeness is reasonable compared to the heuristics based product discovery with emerge the command line interface to gentoo s official package manager and distribution system portage.
we are now investigating different optimizations of the current prototype such as the exploitation of modal implication graphs pre computed at compile time and the usage of ad hoc smt search strategies.
in future work we plan to investigate other productdiscovery problems for gentoo as well as for other domains to gain more insights into lazy product discovery.
while our results make us confident that lazy product discovery is a viable method for product discovery in huge configuration spaces we believe that it may also be used to complement optimized but incomplete algorithms when these fail such as emerge for gentoo.
we also plan to investigate how lazy product discovery can be combined with interactive product discovery.