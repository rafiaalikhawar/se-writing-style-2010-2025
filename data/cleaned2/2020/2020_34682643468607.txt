stateformer fine grained typerecoveryfrom binaries using generativestatemodeling kexinpei kpei cs.columbia .edu columbia university new york usajonasguan jonas cs .toronto.edu universityof toronto toronto canadamatthewbroughton mb4207 columbia .edu columbia university new york usazhongtianchen zc2399 columbia .edu columbia university new york usa songchenyao sy2743 columbia .edu columbia university new york usadavidwilliams king dwk cs.columbia .edu columbia university new york usavikas ummadisetty ummadisettyvikas gmail .com dublin highschool dublin usajunfeng yang junfeng cs .columbia .edu columbia university new york usa baishakhi ray rayb cs.columbia .edu columbia university new york usasumanjana suman cs .columbia .edu columbia university new york usa abstract binarytypeinferenceisacriticalreverseengineeringtasksupporting many security applications including vulnerability analysis forensics and decompilation.
itis adifficult taskbecause sourceleveltypeinformationisoftenstrippedduringcompilation leaving only binaries with untyped memory and register accesses.
existing approaches rely on hand coded type inference rules defined by domainexperts whicharebrittleandrequirenontrivialeffortto maintainandupdate.eventhoughmachinelearningapproaches haveshownpromiseatautomaticallylearningtheinferencerules theiraccuracyisstilllow especiallyfor optimizedbinaries.
wepresent stateformer anewneuralarchitecturethatisadept at accurate and robust type inference.
stateformer follows a twostep transfer learning paradigm.
in the pretraining step the model is trained with generative state modeling gsm a novel task that wedesigntoteachthemodeltostaticallyapproximateexecution effects of assembly instructions in both forward and backward directions.
in the finetuning step the pretrained model learns to use its knowledge ofoperationalsemantics to infertypes.
we evaluate stateformer s performance on a corpus of popular open source software projects containing over .
billion variables of different types.
the programs are compiled with gcc and llvm over optimization levels o0 o3 and obfuscation passes based on llvm.
our model significantly outperforms stateof the art ml based tools by .
in recovering types for both functionargumentsandvariables.ourablationstudiesshowthat gsmimproves type inference accuracyby33 .
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthefirstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
esec fse august 23 28 athens greece copyright heldby the owner author s .
publicationrightslicensed to acm.
acm isbn ... .
.org .
.3468607ccs concepts security and privacy software reverse engineering computingmethodologies machine learning .
keywords typeinference reverseengineering transfer learning machine learningfor program analysis acmreference format kexin pei jonas guan matthew broughton zhongtian chen songchen yao david williams king vikas ummadisetty junfeng yang baishakhi ray and suman jana.
.
stateformer fine grained type recovery frombinariesusinggenerativestatemodeling.in proceedingsofthe29th acm joint european software engineering conference and symposium on thefoundations ofsoftware engineering esec fse august 23 28 athens greece.
acm newyork ny usa 13pages.
.org .
.
introduction recovering source level data types from binaries is very useful for many security criticalsoftware engineering tasks such as vulnerability analysis binary hardening memory introspection and decompilation .
type inference in binaries involves reconstructing source level constructs such as local function variables and data types from untypedbyte addressedmemoryandregisters.thisprocessischallengingbecausethereconstructionisbasedonincompleteinformation mostsource levelinformationisstrippedduringcompilation for optimization andto deterreverseengineering.
traditionalapproachestotypeinferencerelyextensivelyonhandcoded rules defined by domain experts.
these rules facilitate recognizing types directly from specified patterns e.g.
consecutive printablecharactersfordetectingstrings and propagating types from known type sinks e.g.
known string manipulation functions to registers and memory regions storing the source level variables.
unfortunately theserulesarebrittle andrequirecontinuous efforttoadapttonewinstructionsequencesintroducedbycompiler andarchitecture evolution .
esec fse august 23 28 athens greece pei guan broughton chen yao williams king ummadisetty yang ray jana as a result recent years have witnessed a growing interest in data drivenapproachesleveragingmachinelearning ml forbinarytypeinference .theseapproachesmitigatethereliance on hand coded heuristics by learning from a rich training set of diversebinaries.moreover theirlearnedrepresentationshavebeen showntogeneralizeacrossvariouscompilers operatingsystems andarchitecturesandarehighlyefficienttocompute i.e.
theunderlying learning algorithms are amenable to gpu parallelization .
whilepromising existingml basedapproachesstillcannotrecover data types with high accuracy or robustness especially in thepresenceofcompileroptimizations .thetypesareessentially abstractionsdescribing how a data object isexpected to be manipulatedandusedduringexecution .therefore theinherentchallengefacedbyallml basedapproachesistounderstandtheeffects oftheruntimeexecutionofinstructionsinthetargetbinary i.e.
theoperational semantics of codeblocks .
for example on x64 theruntimeeffectofiterativeincrementsofthe rcxregisterby 1togetherwiththeinstruction mov rax isindicative oftraversingan intarray.
unfortunately existing ml based approaches are agnostic to theexecutioneffectsofcodeastheylearnthedirectmappingbetweenstaticcodetokensandcorrespondingtypesinanend to end fashion.
a model trained this way often learns spurious correlations taking shortcuts to leverage simple yet brittle patterns for inferring types.
for example chua et al.
showed that their model eklavya mispredictsthetypeoftheargumenttothefunctionck fopen from diffutils to be integer instead of pointer.
a completely unrelatedinstructionwithin ck fopen namelycallq 0x3fc contributesthemosttothemisprediction.withoutunderstandinghowanintegerisaccessedandmanipulatedduringexecutionandtheeffectsof callq eklavyaestablishesaspurious correlation that the internal call instruction implies an intargumenttock fopen .
inthispaper wepresent stateformer anewneuralarchitecture thatexplicitlylearnstheoperationalsemanticsofassemblyfortype inference.
specifically we design a novel pretraining task to teach thestateformer model the operational semantics of both data and control flow behavior of diverse code blocks and then finetune the pretrainedmodelfor type inference.
learning operationalsemantics.
a human reverse engineer often makes sense of a target binary by following its assembly instructions through mental simulation of their execution.
while the reverseengineermightnotaccuratelyresolveallinvokedbranches byfollowingcontrolfloworcomputetheprecisevaluesofallstates by following data flow duringsimulation she can still get a rough idea of what the code does by approximately following the operational semantics of code blocks.
our key insight is to teach the stateformer model via a novel pretraining task the approximate operationalsemantics of assembly by forcing the modelto predict howdifferentsequencesofinstructions transformtheunderlying program states .
specifically the pretraining task asksthe modelto predictthechangedvaluesofregistersandmemoryafterexecutingeach instruction whichcaptures theoperationalsemantics of assembly code .
thisgives themodel an understanding of the execution effects of code which helps the model to infer thetypesoflow levelregistersandmemoryregionsbasedontheinstructions used to manipulate them without executing any parts ofthe code duringinference.
generative state modeling.
wedesignanovelpretraining task generativestatemodeling gsm wherewetrainaneuralnetwork to reconstruct the complete set of itsexecution states whiletaking the assembly code and a very small subset of its execution states e.g.
register values at specific program points as input.
for example given the instruction sequence inc ecx add ecx xor ecx ecx mov ebx ecx and its corresponding execution states ecx ecx ecx ecx ebx ecx we feed the model with all the instructions and only the executionstate after the secondinstruction i.e.
ecx .ourtrainingprocessforcesthemodel to compute all the preceding and succeeding states ecx ecx andecx ebx ecx .
therefore toachievelow losson the gsmtask themodelneedstounderstandtheoperationalsemantics ofinc add xor andmov.
gsmdynamicallyselectsrandomsubsetsofstatesasinputsacross differenttrainingsamplesand iterations.moreover gsmisfully self supervised implying that we can collect data from an unrestricted number of binaries found in the wild.
as a result gsmcreates diverse prediction tasks that compel the model to approximately reason about the effects of both dataandcontrol instructions inboth forwardandbackward directions acritical capability for type recognition and propagation .
during pretraining with gsm stateformer encodes such a reasoning capabilityaspartofitsnetworkparameters knownasembeddings.
such embeddings can then be finetuned for type inference as a finetuning taskwithafewbinarieswithlabeledtypes.
consideragaintheexampleofinferringthetraversalofan int basedoniterativeincrementsofthe rcxregisterby 1togetherwith the instruction mov rax .
the output of pretrained stateformer willbeasequenceofembeddingsencodingtheeffects ofinc movon other registers and memory locations.
therefore insteadoftrainingonrawcodesequencesfromscratch thefinetuningprocesscaneasilyexploitthelearnedexecutioneffectsof code compressed in these embeddings to predict that rdxcontains the baseaddressofan intarray.
stateformer neuralarchitecture.
to efficiently pretrain with gsm we develop a novel neuralarchitecture specifically designed forlearningoperationalsemanticsofassemblyinstructions.first as themodeltakesasinputboththeprogramcodeandprogramstates we developa multi modalencodingmodule thatcanbetrained on heterogeneous inputsindifferentformats.
second weconstructtwoexplicitobjectivefunctionstojointly optimize the model to understand the operational semantics of bothdataflow andcontrolflow .specifically tohelpthemodellearn about control flow which requires learning operational semantics ofcomparisoninstructions e.g.
cmp weannotatethenon executed pathswithdummyprogramstatestoincorporatepredictingnonexecuted paths as a part of the stateformer s pretraining task.
to helpthemodeltobetterunderstandtheoperationalsemanticsof dataflow whichofteninvolvesassignment e.g.
mov andarithmetic instructions e.g.
add onnumericalvalues weexplicitlymodelthe numerical values in both decimal and hexadecimal formats with a trainable numerical representation module based on the neural arithmetic unit nau .
691stateformer fine grainedtype recovery from binariesusinggenerative statemodeling esec fse august 23 28 athens greece finally as the composite execution effects of a piece of code can resultfromtheinteractionsbetweenfarawayinstructions weleverageself attentionlayersfromtransformer whichisamenable tolearninglong rangedependencieswithoutmanually constructingthedependencies e.g.
graphneuralnet .weshowin section5.5thatsuchadesignindeedachieveshightestingaccuracy ingsmfor unseen program state traces.
result summary.
we evaluate stateformer on a corpus of popularopen sourcesoftwareprojectswith1.67billionsourcevariablesofdifferenttypes.theprogramsarecompiledfor4instruction set architectures x86 x64 arm and mips by compilers gcc andllvm andwith4optimizationlevels o0 o3 and3obfuscationpassesbasedonllvm .bytrainingwith gsm ourmodel outperformsthe state of the artml based toolsbyup to14.
in recoveringtypesforbothfunctionargumentsandvariables.our extensive ablation studies show that stateformer trained with gsmsubstantially boosts thetype inferenceaccuracy by .
we make the following contributions.
weproposeanewpretrainingtask generativestatemodeling gsm toexplicitly learn the operationalsemanticsof assembly code for accurateandrobust type inference.
we develop a novel neural architecture stateformer with speciallydesignedsub modulestolearntheoperationalsemantics ofboth data flowandcontrolflowinstructions.
weevaluate stateformer onanextensivecollectionof33opensourcesoftwareprojectsacrossdifferentarchitectures compilers optimizations andobfuscations.aftertrainingwith gsm stateformeroutperforms the state of the art learning based tools by14.
.ourablationstudiesunveilthattrainingwith stateformerbooststhetypeinferenceaccuracyby33 .werelease the code and datasets of stateformer at .com cumlsec stateformer .
overview thehigh levelworkflowof stateformer followsthegeneraltransferlearningparadigm.asshowninfigure wefirstpretrain stateformerwithgsmbytrainingittoreconstructthemaskedstates grayed out in the trace of program states of various assembly instructions section .
.
we train stateformer to reconstruct both the data and control states section .
.
after pretraining stateformer withgsm we transfer its learned knowledge by finetuning onthe type inference task definedinsection .
.
.
problemdefinition weconsidertheproblemofmappinguntypedlow levelregistersor memoryregions specifiedbymemoryoffsets tothecorrespondingsource leveltypes.
the source leveltypes are associatedwith functionarguments local static andglobalvariables.thegranularity of recovered source level types varies widely across existing works rangingfromprimitive e.g.
int float andaggregate e.g.
struct array typestoclassesinobject orientedprograms andrecursive types such as trees andlists.
wefocusonthecprimitive aggregate andpointertypes.our supportedtypesaremore fine grained thanpriorworks whichonlysupportastrictsubsetofours seesection .4forthe complete list of types .
predicting fine grained types while helpfulto the reverse engineer to better understand the target binary is achallenginglearningtaskthatmustdistinguishbetweensubtly differentaccess patterns of differenttypes .
we formulate type inference asa classificationtask.
specifically givena sequenceofassembly instructions stateformer predicts thetypelabelsforeachoperandintheinstructions.notethat stateformerperforms the type prediction in one shot see section 3for design specifics as opposed to the traditional type propagation approaches that infer the types one by one in a sequence of instructions.asweshowinsection .
thisdesignbringssignificant performance gainsduringinference.
.
understanding operational semantics helps typeinference while reverse engineering types from binaries human analysts needtounderstandwhatatargetfunctioncomputeswithoutexecutingthebinary.oftentheanalystfollowstheassemblyinstructions by simulating the execution in their mind.
without knowing theexactprogramstates duringthefunctioncall theanalystcannotaccuratelyresolvethetakenbranchesorcomputetheprecise valuesduringthesimulation.still theycangetaroughideaofwhat the code does.
this loose approximation of the operational semanticsofthecodeallowstheunderstandingofitsruntimebehavior providingstronghints aboutthe underlying data types .
for instance given a pointer a observing a dereference like a in the execution behavior might imply a byte read of the object at a indicating an intor a pointer type on bit systems.similarly contiguousdereferencingofsequentialaddresses like a a ... suggests that ais likely an array of chars.
examining precise runtime behaviors of a binary over many inputswithhigh coveragedynamicanalysisisprohibitivelyexpensive therefore inthispaper weusemlmodelstolearn approximateoperationalsemanticsofbinariesinadata drivenmanner anduse this knowledge to statically infertypes.
.
learning operational semantics withgsm ourkeymotivationfordeveloping gsmistoteachanmlmodel to approximate operational semantics of code i.e.
its execution effects which wethenexploit fortypeinference.
teachingan ml model the code execution effects is challenging due to many possiblecombinationsofinstructionsthatintroducecomplexdataand controlflowdependencies.therefore itisnotpracticaltomanually engineer input features or target labels to represent the execution effectsand train the modelto understandthem.to thisend gsm exploresaself supervisedapproachthatexploitsalargenumber of traces that can be cheaply generated from many code blocks using under constrained dynamicexecution e.g.
micro execution detailed in section .
toautomate learning diverse instructions executioneffectwithacarefully designedtraining task.
predictingmaskedstates.
thetrainingtaskperformedin gsm requiresaneuralnetworktoreconstructthewholemicro execution traces i.e.
allrecordedprogramstates inthetrainingdatabasedon thecorrespondingcodeblocks.tolearnonahugenumberoftraces we exploit stochasticity to efficiently train a network for gsm.
specifically for eachtrainingsampleineachepoch werandomly masksomestatesinthe traces.suchrandomnessensuresthatthe 692esec fse august 23 28 athens greece pei guan broughton chen yao williams king ummadisetty yang ray jana ... sub ecx add ecx ...... ecx ecx ecx ...code 1partial states ......... mov eax add eax ...... eax eax eax ...code npartial states npretrained model......... ecx ecx ?
?
ecx ?
?
...data ... exec ?
?
exec ?
?
...control ... eax ?
?
eax ?
?
eax ...data ... exec ?
?
exec ?
?
...control i gsm pretraining task pretrained modeltransfer ... mov rax push rax ...code ......... cmp eax ja 0x3c ...code n...... type prediction headpredict types type prediction headpredict types ii type inference finetuning task pretrained model ... mov rax push rax ...codetype prediction headpredict types iii type prediction predict complete states predict complete states n figure stateformer workflow.
we first pretrain stateformer withgsm.
we then stack type prediction heads on top of thepretrainedmodelandfinetuneboththepretrainedmodelandthestackedheadsfortypeinference.finally thefinetuned modelwill onlytaketheprogram codeas input we donotexecute thecodeduringtype inference and predictthe type.
modelcannotconsistentlyachievelowlossbytakingshortcutsthat only work well for afewstates traces orcode blocks.
whiledecidingwhichstatestomask gsmdoesnotfollowthe sequential execution order of states as recorded in the traces.
this designchoiceensuresthatthemodellearnstoreasonaboutboth forward and backward execution effects of a diverse set of code blocks.understandingtheseforwardandbackwarddependencies isknownto be crucial for accuratetype inference .
differencewith masked language models.
whileourstochastic masking setup is inspired by the masked language modeling mlm usedinlearningnaturallanguagesemantics thekeydifference is that natural languages are not stateful i.e.
they have no notion similar to programexecution.therefore the model trained bymlm only uses words inthe neighboring context to predict the maskedwords exploitingthe localwordphrasepatterns .whilein gsm the unmasked states alone provide little observable patterns due to high masking rate the model has to also look at the correspondinginstructions understandtheirexecutioneffectsonthe unmaskedstates inorder to correctlypredict the maskedstates.
.4stateformer architecture learning instruction state dependencies.
achieving low loss ongsm by design requires a neural network to understand the long range dependencies between instructions and unmasked programstates.however standardfully connectedorrecurrentnetworks are inefficient at learning long range dependencies between differentparts ofthe network inputs .
to avoid these issues we develop a hierarchical input embedding module tolearn theinteractions betweenprogramstates and instructions.
specifically we design two input sub networks for learningtwoembeddingsofthebinarycodeandtraces onefor the registers and instruction opcodes and another for the concrete data values.we combine theserepresentations by aggregating the embeddingswithavectoradditionoperationandfeedingtheminto self attention layers that facilitate capturing long range dependencies section .
.finally weusetwooutputsub networkstodecodetheoutputofself attentionlayersfortwodifferentobjectives regressionfor predicting the program datastate and classification for predicting the program control state section .
.
learning representations for numerical values.
typical embeddings for numerical tokens i.e.
register values just like how anydiscretetokenisembedded areknowntofailtoextrapolateto unseen values even on the outputs of simple arithmetic operations like addition .
as understanding data and control flow often requiresreasoningtheexecutioneffectofarithmeticinstructions we use neural arithmetic units naus as part of the subnetwork for data value embeddings.
note that our nau layers unlike the original nau model that directly takes numerical values as input learn to represent the numerical values both decimal and hexadecimal formats as embeddings.
we have done a thorough study and refer interested readers to our supplementary material.
methodology we now provide the details of our methodology including how we collectruntimestatesofbinaryprograms thearchitectureof stateformer andhowwedistillthelearnedknowledgein stateformer from training gsmfor type inference.
.
collecting programstates totrainstateformer withgsm weneedtoobtainruntimeexecutiontracesofbinaryprograms.ideally wewanttocollectdiverse traceswithdifferentinstructionsandcontrolflowtolearnmiscellaneousoperationalsemanticsfortypeinferenceinvariousscenarios.
however the typical dynamic analysis approach is often limited by path coverage resulting in potentially restricted sets of covered instructions.therefore weadoptmicro execution to support tracing arbitrary parts of a binary program without having to find concrete program inputsthat maximizecoverage.
without executing the program from its entry point our executionengineneedstoinitializeintermediateprogramstates i.e.
registers and memory content with randomized values which can be under constrained i.e.
infeasible when executing the program 693stateformer fine grainedtype recovery from binariesusinggenerative statemodeling esec fse august 23 28 athens greece add 0x3 cmp 0x2 jle 0x6 sub 0x1 mov eax 0code add 0x3 cmp 0x2 jle 0x6 sub 0x1 mov 0x0 state trace mov ebp esp mov 0x1c 0x4 figure sample state trace consisting of both the data statesassociatedwitheachinstructionandthecontrolstates indicated by and generated by themicro execution.
normally .inaddition wefocusonprogramstatesthatare explicitly manipulated ininstructions e.g.
we onlylogthevalueof eax insub eax insteadofloggingallregisters flags andvaluesin memory .therefore wecallourcollectedprogramstatesas partial states state implying that they might differ from the genuine program states from actual program executions.
statecollection.
wecollect statetracesandmaskarandomsubsetofthemtotrainthemodeltoreconstructthecomplete state trace.
stateconsistsoftwosourcesofinformation.
theconcrete values of all registers memory addresses and hardcoded offsets that appear in the instruction dubbed datastate.
the boolean annotationindicatingwhethereachinstructioninthecodeisexecuted in a given state dubbed controlstate.
the former appears in bothstateformer s input and output i.e.
subset of datastate as input complete set of datastate as output .
the latter appears onlyinstateformer soutput.section .2elaboratesonhowthese twoparts of stateare usedto train stateformer .
figure2showsanexampleof statetracegeneratedbymicroexecuting a simple code block e.g.
the concrete values of registers are datastateandthe and besideseachinstructionindicates controlstate.
we assign dummy values to all opcode as they do not hold anyvalue during micro execution.
this helps to align stateandtheassemblycodesequence whichmakesitconvenient forstateformer toaggregatethemasnetworkinputs section .
.
to construct controlstate we annotate each instruction with a binary indicator to denotewhether itisexecutedornot.
.2stateformer input andoutput weconstruct5sequencesfor stateformer input namely static assemblycodesequence datastatesequence instruction positionsequence opcode operandpositionsequence and architecture sequence.
each token of the sequences are aligned and embedded into an embedding a low dimensional vector with the samedimensions suchthattheycanbeeasilyaggregatedasasingle sequenceof nembeddings x x1 ... xn .figure3illustratesan example inputof stateformer when training on gsm.
encoding assembly code.
the assembly sequence with length n c add ebp ... nis constructed by tokenizing the assembly instructions.besidestreatingbothopcodesandoperandsastokens we keep punctuations as they provide crucial contextual hints e.g.
thecommadelimitsthesourceanddestination andbrackets indicateadereference ofamemory address.
assemblycodecan have concretenumerical values hardcodedin instructions whichleadstoaprohibitivelylargevocabularysize e.g.
232possiblevaluesinx86 makingitchallengingtoembedall tokens in c. therefore we place the concrete value into datastate and replace all numerical values in both hexadecimal and decimal forms withaspecialtoken hex.thisreducesthevocabularysize ofcacrossallinstructionsetarchitecturestoonly648.wedescribe howwe encode the numerical valuesinthe following.
encoding datastate.
wenormalize datastatesequence vasa two dimensionalarray v vn wherev 0x00 ... 0xff theunionof256bytesandadummytoken .each datastate vican thus be viewed as a sequence of byte tokens v8 where wetransformallthenumericalvaluesintoan8 bytehexadecimal representation.
for example figure 3shows that a datastate 0x6 is padded to .
as each viis aligned with each token ciin code sequence we put s for those cithat do not have dynamic values e.g.
opcode .
such a setting reduces the vocabulary size used to encode all possible numerical values from assume bit architectures toonly257.moreover representinganumericalvaluewithfixed dimensionsmakesiteasytostackasinglelearnablemodule see section3.
tocomputeinter dependenciesbetweendigits learning useful hierarchical knowledge i.e.
an address 0x104cmight be decomposedas asection baseat 0x1000withthe offset 0x4c .
encoding spatial information and syntactic hint.
as we flattenandconcatenatealltheassemblyinstructionsasaplaincode tokensequence theinstructionboundariesandtherelativelocation of tokens within each instruction become ambiguous.
to this end weintroducetwopositionalencodings namelytheinstruction positional encoding and opcode operand positional encoding.
the resultinginstructionpositionsequence p zn andopcode operand position sequence o zn annotate each token in cwith their instruction position and the opcode operand position within each position respectively.figure 3showsthe example of pando.
when training with gsm we mix the training samples from differentinstructionsetarchitectures whichintroducedisparate syntax in their assembly code.
we thus append the architecture sequence ato indicate the architecture which assists the model to transferthelearnedinstructionsemanticsusefulononearchitecture to another e.g.
push eax in x86 hasthe similar semantics to addi sp sp sw t0 sp inmips .
stateformer output.stateformer havedifferentoutputsdependingonthetrainingtasks.whenitisinthepretrainingstage withgsm its output consists of complete state trace including both datastate trace and controlstate trace.
we describe how these outputs participate in the computation of loss functions in section3.
.
when we finetune stateformer for type inference its outputisthe prediction of type labels definedinsection .
.
.
pretrainingwith gsm numerical representation module.
we treat each value viin datastate trace vas an byte sequence.
to learn the interdependencies between high and low bytes in vi we develop a 694esec fse august 23 28 athens greece pei guan broughton chen yao williams king ummadisetty yang ray jana push rbp jmp hexv inst pos op posv1 v2 v3 v4 x64 x64 x64 x64 archcode datastatex1 x2 x3 x4 x1 x2 x3 x4e1 e2 e3 e4 self attention layers input embeddingoutput embedding e1 e2 e3 e4aggregate datastate as embedding v4 0000000000000006neural arithmetic unitinput embedding datastate 0x6input encoding sub networks .
regression on datastate 00000000013bc4a700000000033bc4a7 predictedground truth mean squared error feedforward network2.
classification on controlstate predictedground truth cross entropy feedforward network10output sub networks vector addition figure stateformer s architecture and input output when training with gsm the color consistent with that of figure .
stateformer takes as input the code sequence and a subset of datastate e.g.
v4in the figure .
the other input sequences are described in section .
.
the loss functions measure mean squared error mse between the reconstructed datastate andthegroundtruth and binary cross entropy bce between thepredicted controlstateand the groundtruth.
learnableneuralmodulewithneuralarithmeticunit nau which is shown beneficial to capture the semantics of numerical values involved in arithmetic operations section .
.
formally let vi vi1 ...vi8 denote the byte sequence of vi we denote the aggregatedembedding eviastherepresentationofeach datastate evi nau emb vi1 ... emb vi8 e.g.
emb vi1 denote applying the embedding to the first byte token of vi.
figure3briefly illustrateshowa datastate 0x6getsencodedbynau.notethat viinfigure 3indicates the embedding evi.
sampling subset of datastate.
wesamplearandomsubsetof datastate and replace them with mask e.g.
the grayed out tokens as shown in figure tokens in the model input so that the model is trained to reconstruct the removed datastate.
we define pmaskasthepercentageofthemasked datastateandstudythe effectofdifferent pmaskontype inference insection .
.
multimodal encoding module.
we only apply nau to each datastate sequence v. for other sequences we apply regular embeddings.
we end up with embeddings for each token in each sequence eci evi epi eoi eai.
we then compute the vector sum of embeddings and output a single embedding xi xi sum eci evi epi eoi eai .
the vector sum operation aggregates themultiplemodalities e.g.
instructionandstate ofeachtokeninto asingleembedding.whenwecomputeattentionsbetweenthese embeddings i.e.
dot product the cross modality instructionstate dependencies are naturally computed following the distributive property ofmultiplication xi xj eci ecj ... eai eaj.
lossfunctions.
afterencodingalltheinputsequencesasasingle sequence of embeddings x x1 .. xn we feed xto self attention layers.
the output of self attention layers are known as the contextual embeddings e e1 .. en .
we stack two independent 2layer feedforward networks hvandhf that takes eas input and outputthepredicted datastateand controlstate.formally let f ndenotethe controlstatelabels and masetoflocations in the masked datastate v. we define the pretraining objective as min summationdisplay.
i mmse vi hv ei n summationdisplay.
i 1bce fi hi ei the first part of the objective function aims to minimize the meansquarederror mse betweenthepredicted8 byteandthe groundtruth8 bytefortheonlymasked datastate.notethatmse treats the output byte tokens as numerical values as opposed to categoricalastreatedintheinput .suchasettingencouragesthe loss to penalize predictions far from the groundtruth e.g.
predicts 0x00butthegroundtruthis 0xff .thesecondpartoftheobjective functionaimstominimizethebinarycross entropy bce between thepredicted controlstateandthegroundtruth forallinputtokens.
is the weighting hyperparameter that keeps the scale of bothlossesatroughlythesamemagnitude.asallthemodulesof stateformer aredifferentiable i.e.
nau ffnusedforaggregating input sequences self attention layers and hvandhi optimizing equation 1can be efficiently solvedbygradient descent.
.
transfer learning typeinference afterpretrainingwith gsm wetransfer stateformer slearned knowledgebyfinetuningitfortypeinference.wedefineourconsideredtypesinfigure whichservesasthelabelsfor stateformer to predict.
notably our considered types are much more fine grained thantheexistingml basedtypeinferenceapproaches.forexample eklavya does not distinguish signedness of the primitive types.debin doesnothandlefloatingpoint.andbothworks treat the pointer as a single type ptr without inferring what the pointer refers e.g.
predicting char orstruct .
as discussed in section .
we do not collect state by microexecuting the code during finetuning.
specifically we replace each token invwith the dummy token described in section .
and still follow thesamesteps tocompute theembeddings x.we then stack a new prediction head htype a layer feedforward network that takes as input e the output of the last self attention layers and predicts the type labels defined in figure 4for each input code token.
formally let tidenote the groundtruth type of code tokenci the objective function of finetuning task is defined as the cross entropybetweenthepredictedtype htype ei andtiforeach tokeninaninputsequencewithlength n minn summationtext.
i 1ce ti htype ei .
695stateformer fine grainedtype recovery from binariesusinggenerative statemodeling esec fse august 23 28 athens greece type access no access access prim agg ptr ptr prim agg void prim float double long double sign char sign short sign int sign long sign long long agg struct union enum array sign signed unsigned figure4 thetypes total35typesafterconcretizingtheproduction rule that stateformer predicts.
prim agg and ptr stand forprimitive aggregate andpointertypes.
during finetuning both htypeand the pretrained model weights willbe updatedbygradient descent.
implementation and setup we implement stateformer using the fairseq toolkit based on pytorch .
.
.
all the experiments are run on a linux server withubuntu18.
intelxeon4214at2.20ghzwith48virtualcores 188gbram and4nvidiartx2080 tigpus.toobtaingroundtruth types for training and testing we compile all the software projectswithdebugginginformationandparsethedwarfsections usingpyelftools andghidra .
state collection.
to log the program states state for pretrainingstateformer ongsmtask we implement micro execution using unicorn a cross architecture cpu emulator based on qemu .specifically wemicro executeeachfunctionbinaries collected from the datasets described below times with different randomizedinitialvaluesforregistersandmemory generating9 sets of state for each function binary.
to align the state with the corresponding assembly instructions section .
we leverage capstone to disassemble the function binaries.
metrics.
as described in section we treat type inference as a classificationtask.asthedatasetshavehighly imbalancedlabels wherethemajorityoftokensdonotpossessanytype weuseprecision p recall r andf1scoretomeasuretheactualperformance ofstateformer andallothertools.let tpdenotethenumberof correctlypredictedtypes fpdenotethatofincorrectlypredicted types tndenote the number of correctly predicted no access andfndenote the number of incorrectly predicted no access .
p tp tp fp r tp tp fn andf1 p r p r .
baseline tools.
we compare stateformer with state of the art ml based type inference prototypes eklavya debin andtypeminer .thesetoolshavebeendemonstratedtooutperformtraditionaltypeinferencetechniques.forexample eklavya has been shown to outperform typearmor which is based on principleddataflowanalysissuch as def use andliveness analysis.
eklavya implements the function signature recovery task.
the authorsdefinethetaskaspredictingthetypeoffunctionarguments.
since eklavya does not release their trained model we use their reported numbers and use the same datasets to evaluate stateformer saccuracyinrecoveringtypes for function argument.table the statistics of our datasets categorized by architecture arch optimization opt and obfuscation obf .
arch opt obf variables instructions functions o0 o1 o2 o3 138arm total o0 o1 o2 o3 519mips total o0 o1 o2 o3 561x86 total o0 o1 o2 o3 bcf cff sub 481x64 total total debin recovers both variable types and names.
as we do not studyrecoveringsource level variablenamesbutfocusonobtainingvariabletypes wecomparewithdebin stypepredictiononly.
since debin has released their trained model we run debin on our datasets directlyandcompare against its attainedaccuracy.
typeminerconsidersmuchfiner grainedtypelabelsthantheprevious twoworks.
for example it further distinguishes the pointer type tostructandchar while the former two do not.
as typeminer isnot open sourced we have contacted the authors to obtain their reported f1 scores and compare them to stateformer by runningstateformer ontheir dataset.
these tools vary in their definition of the target types e.g.
eklavyaislimitedtopredictingonlyfunctionargumenttypes and the evaluated architectures e.g.
typeminer only handles x64 eklavyahandlesx86andx64 .hence weadjustoursetupaccordinglywhen comparing withthe baselines.
dataset.
we collect open sourced software projects in their latestversions includingpopularandlargeprojectssuchasopenssl imagemagic andcoreutils.duetothepageconstraints weputthe detailsofthedatasetsinour supplementarymaterial.wecompile these software projects to instruction set architectures including x86 x64 mips and arm each with optimizations i.e.
o0 o3 usinggcc .
and obfuscation strategies including bogus control flow bcf controlflowflattening cff andinstructionsubstitution sub using hikari based on clang .
table1summarizes the statisticsofthe datasets.
pretraining and finetuning setup.
we pretrain stateformer withgsm on all datasets intable .
we sample a random of the functions from the pretraining datasets as the validation set.
wethenpretrainthemodelin10epochsandcheckpointthemodel weights that achieve the lowest validation loss for finetuning.
note thatgsmpretraining task does not have any access to ground truth 696esec fse august 23 28 athens greece pei guan broughton chen yao williams king ummadisetty yang ray jana table stateformer s precision recall f1 for each architecture arch optimization opt andobfuscation obf .
arch opt obf precision recall f1 score o0 .
.
.
o1 .
.
o2 .
.
.4arm o3 .
.
.
mipso0 .
.
.
o1 .
.
.
o2 .
o3 .
.
.
o0 .
.
.
o1 .
.
.
o2 .
.
.8x86 o3 .
.
.
o0 .
.
.
o1 .
.
o2 .
.
.
o3 .
.
.
bcf .
.
cff .
.
.1x64 sub .
.
.
type labels .
therefore we canalways collectarbitrary binariesfor pretraining includingthoseused infinetuningfor type inference.
this isacommon practiceintransfer learning .
we finetune on stateformer our dataset categorized by the architectureandoptimization obfuscation section .
.wepartitionthetrainingandtestingsetbyrandomlyselecting90 ofthe functionsfor training andthe remainderfor testing.
hyperparameters.
we pretrain and finetune stateformer for 10epochsand50epochs respectively.
we set thedefaultmasking percentage pmask .
section3.
and study different choices ofpmaskin section .
.
we choose 4in equation 1such that the mse of predicting datastate and the bce of predicting controlstatearescaledtothesamemagnitude.weperform an extensive evaluation of the properties of nau to understand its capabilityinencodingnumericalvaluesandlearningarithmetics.
duetothespaceconstraints weputthedetailsofthisstudyandthe complete hyperparameter settings inour supplementary material.
evaluation we aim to answer the following researchquestions.
rq1 howaccurateis stateformer intype inference?
rq2 how does stateformer compare to the state of the art ml basedsystems?
rq3 howfast is stateformer comparedto othertools?
rq4 how effective is pretraining with gsmin improving the type inference accuracy?
rq5 how well does stateformer approximate the operational semantics bytraining with gsm?
.
rq1 accuracy we first study the accuracy of stateformer on all binaries.
following the setup described in section we report the results in table2.stateformer achievesanaverage77.
f1scoreacross allarchitecture optimization andobfuscation.
onx86andx64 weobservethat stateformer remainsrelatively robustforbinarieswithhigheroptimizationandobfuscation.for eklavya stateformer o0 o1 o2 o3 .
.
.
.
.00accuracy a x86 o0 o1 o2 o3 .
.
.
.
.00accuracy b x64 figure accuracy of eklavya and stateformer on binaries ofdifferentarchitecturesand optimizations.
example thef1scoreforx86 o3isonly2.
lowerthanthatofx86 o0.thef1scoreforx64 o3isonly3.
lowerthanthatofx64 o1.
regarding the performance across differentarchitectures with all optimizations obfuscations we notice no significant difference on average.theseobservationsindicatethat stateformer isrobust across architectures and optimizations with disparate operational semantics oftheirinstructions.
stateformer achievesanaverage77.
f1scoreacrossallarchitecture optimization andobfuscationandremainsrobustfor binarieswithhigher optimization levels andobfuscations.
.
rq2 comparisonto baseline baselinecomparison.
wecompare stateformer with3state ofthe art type inference tools namely eklavya debin and typeminer asdescribedinsection .tocomparewitheklavya weevaluatestateformer onthesame8projectsconsideredintheirpaper binutils coreutils findutils sg3 utils util linux inetutils diffutils andusbutils.weevaluate stateformer on7typesconsideredin eklavya.eklavyatreatstypeinferenceforeachargument of multiple function arguments as an independent classification task and reportsthe accuracy insteadoff1score .
wethusalsoevaluatestateformer saccuracy definedasthenumberof correctly predictedtypes dividedbyallthe number of tokens.
figure5compares stateformer to eklavya side by side on two architectures i.e.
x86 and x64 and optimizations o0 o3 aseklavyaisevaluatedwiththesesettings.onaverage stateformeroutperformseklavyaby13.
.notably stateformer remainsrobustacrossdifferentoptimizationlevels whileeklavya has aclear dropwhen the optimization level isincreased.
tocomparewithdebin weruntheirreleasedmodelonopenssl whichwehaveconfirmedisnotincludedintheirtrainingset.we compileopensslinto3architectures x86 x64 andarm with4 optimizations o0 o3 .asdebinconsidersonly17types wealso restrictthepredictionof stateformer tothesame17types.figure6shows that stateformer consistently outperforms debin onallarchitecturesandoptimizations achieving14.
higherf1 scores onaverage.we observe debin hasan apparentdrop inf1 scoreswithhigheroptimizations downto46.
forarm while stateformer remainsrobust withat least70 f1 scores.
finally we compare stateformer to typeminer on the same datasets they have considered.
we restrict our test on x64 with o3 as typeminer is evaluated onlyonx64.typeminer treatstype inference as a multi stage classification task trainingindependent 697stateformer fine grainedtype recovery from binariesusinggenerative statemodeling esec fse august 23 28 athens greece debin x86 stateformer x86 debin x64 stateformer x64 debin arm stateformer arm o0 o1 o2 o3 o0 o1 o2 o3 o0 o1 o2 o3 .
.
.
.75f1 score figure6 f1ofdebinand stateformer inrecoveringtypes forbinariesofdifferentarchitecturesandoptimizations.
ptr prim .
.
.
.
.00f1 score typeminer stateformer a task array struct char other ptr .
.
.
.
.8f1 score typeminer stateformer b task int long char double .
.
.
.
.00f1 score typeminer stateformer c task signed unsigned .
.
.
.
.8f1 score typeminer stateformer d task figure stateformer s and typeminer s f1 scores in type inferencetasksdefined insection .
.
classifierstopredicttypesatdifferentlevels.forexample itfirst trainsabinaryclassifiertopredictwhetheravariableisapointer andthentrainsasecondclassifiertopredictthepointertype.since they do not make complete predictions in one shot we compare stateformer on4sub tasksonwhichtypeminerhasbeenevaluated.
specifically typeminer s first prediction task is a binary classificationtaskdecidingwhetheravariablehasapointertype ptr oraprimitivetype prim .itssecondtaskistopredictthe pointertypes including array struct char andother ptr .
itsthirdtaskistopredict the primitivetypes including int long int char anddouble.
its fourth task is to predict the signedness including signedandunsigned .welabelthese4tasksastask1 .
figure7demonstratesthat stateformer outperformstypeminer in4tasksbyanaverage8.
.inparticular typeminersignificantly fluctuates when predicting primitive types task and pointer types task but stateformer ismore robust.
stateformer outperforms eklavya debin and typeminer by13.
.
and8.
respectively and ismore robust than allbaselinesfor differentoptimizations andtype granularity.
.
rq3 inference speed weevaluate stateformer sinferencespeedonbinaryprograms and compare it to debin and ghidra.
specifically we consider software projectswithdifferentsizesonx64compiledwith o0.table3 executiontime seconds of stateformer onboth cpuandgpu debin andghidraon4ofourdatasets with diversenumberofinstructions measured inthousand .
project inst k runtimestateformer speedupstateformerdebinghidracpu gpu imagemagic .
.3n a .
putty .
.
.
.
.
findutils .
.
.
.
.
zlib .
.
.
.
debinterminates abruptly afterrunningone ofthebinariesfor minutes.
epochs .
.
.
.
.
.6f1 score w gsm w gsm only datastate w gsm only controlstate w o gsm a pretraining gsmeffect epochs .
.
.
.
.
.
.7f1 score masking pmask .
masking pmask .
masking pmask .
masking pmask .
b maskingpercentage effect figure left stateformer s testing f1 scores when it is pretrained with gsm pretrained with only predicting datastate pretrained with only predicting controlstate or notpretrained.
right stateformer s validationf1scoreateachfinetuningepochwhenthemaskingpercentages pmaskingsmare .
.
.
or0.
.
table3shows the runtime performance of stateformer debin andghidra.
stateformer basedongpus achieves98.
speedup onaveragethanthesecond besttool.notably whiletheauthorsof debin have tried to optimize their underlying learning algorithms conditionalrandomfield withparallelizedimplementation itperforms1023 and35.
slowerthan stateformer gpuand cpu respectively.
we attribute the speedup of stateformer to its underlying neural architecture which is amenable to gpu acceleration while neither debin s nor ghidra s underlying algorithms can be implementedusing gpuefficiently.
stateformer is98.
faster thanthe second besttool.
.
rq4 effectiveness of gsm inthissection wedigdeeperintotheeffectivenessof gsmpretrainingtaskbyquantifyinghowmuchimprovementthat stateformer achieves when pretrainedwith gsm.
effectiveness of gsm.we compare stateformer s finetuning accuracywhenitis pretrainedwith gsm pretrainedwith partialgsmby only predicting datastate pretrained with partial gsmbyonly predicting controlstate and not pretrained.
figure8ashowsstateformer sf1ateachfinetuningepoch.
it demonstratesthat stateformer pretrainedwith gsmachievesthe bestfinetuningf1scores itreaches71.
f1scorewithinthe epochs.withoutpretrainingwith gsm itonlyachieves38.
f1 698esec fse august 23 28 athens greece pei guan broughton chen yao williams king ummadisetty yang ray jana score.wealsonotethat stateformer pretrainedwithonlypredicting datastateoutperformsthatwithonlypredicting controlstate.
thisisintuitiveaspredicting datastaterequiresunderstanding instructions actual execution effect and computing the concrete values whilepredicting controlstateisonlyabinaryclassification task encoding approximate control flow.
nevertheless we observe even pretraining with predicting only datastate or controlstate is still beneficial for type inference as stateformer pretrained on eitherofthemobtains and62 f1 scores respectively.
masking percentage.
recall in gsm we train stateformer to reconstruct the masked datastate and we use default masking percentage pmask .8throughout our experiments section .
asmaskinglesspercentageof datastatemakesiteasiertotrain ongsm we study how varying pmaskaffects the type inference performance.
figure 8bshows the validation f1 scores achieved bystateformer whenwevary pmask.weobservethatthemore we mask in gsm the better it boosts the type inference performance but the gap of improvement is not significant.
one possible explanation is that even in one example the masked states are less manypretrainingsamplesandthedynamicmaskingstillintroduce diverseenough casesfor learningoperationalsemantics.
stateformer pretrainedwith gsmoutperformsthatwithout pretrainingby inf1 score.masking percentageinpretraininggsmdoes not significantly affect the finetuning results pretraining with masking rate results in decrease in f1 score comparedto pretrainingwith80 maskingrate.
.
rq5 stateformer performance on gsm pretraining losses with gsm.we also study the losses of pretrainingstateformer withgsm.suchastudydirectlyvalidates whether pretraining with gsmindeed helps stateformer to learn operational semantics.
low losses on unseen testing state and functionbinariesindicatesthat stateformer highlylikelylearnsto generalizebasedonitslearnedknowledgeofoperationalsemantics.
figure9shows the training and validation losses in epochs ofpretraining stateformer .thevalidationsetisconstructedby sampling a random functions from the projects used in pretraining as described in section .
specifically figure 9ashows themselossofpredicting datastateandfigure 9bshowsthebce loss of predicting controlstate.
we observe that the validation msedropsto0.
whichtranslatestoaverageabsolutedistance by taking the square root between prediction and groundtruth as .
.
.
.
as we normalize the byte values from into seesection .
.
.8istheactual absoluteerrorbetweenthepredictedbyteandthegroundtruth.the average error within the deviation of only byte indicates that stateformer learns to approximate the executioneffect.
effectsofcontrolanddataflowpretraining.
concurrenttoour work trex alsoleveragestransferlearningtolearnprogram execution semantics.
however trex completely ignores control and data flow modeling as it focuses on binary similarity detection.
incontrast stateformer focusesontypeinference therefore it requiresprecisedataflow typeofoutputofaninstructiondepends on types of operands and control flow it must also infer types of valuesinthe unexecutedportionofthe code .
pretrain epoch .
.
.
.
.00125mse loss training validation a mseinpredicting datastate pretrain epoch .
.
.
.006bce loss training validation b bce inpredicting controlstate figure mse and bce of predicting datastate and controlstate respectively duringpretrainingwith gsm.
epochs .
.
.
.
.
.
.7f1 score stateformer trex figure type inference f1 score between models pretrained by gsmandtrex spretrainingobjective.
because of these differences in the high level requirements of thedownstreamtasks stateformer and trexadoptsignificantly differentpretrainingapproaches i.e.
generatingcontrolanddata flowstate gsm vs.codeandtracetokenclassification.ingeneral it remains an open challenge in transfer learning to determine which pretrainingtaskisthemosteffectiveforwhichdownstreamtask.
part of our contribution in stateformer is to design a pretraining task that makes the downstream task of type inference precise.
for example figure 10shows that stateformer outperforms trex by around .9percentagepointsinf1 score for type inference.
probing stateformer on real world code.
besides quantifyingthepretraininglosses weprobethepretrained stateformer using aconcrete binary example to study howitpredicts state.
consider the example in figure .
we examine how stateformerpredicts registers esp edi ebp andesifrom input datastate in which we mask all registers except for their first appearances.
the accurate prediction of espat line suggests that stateformer is able to associate 0x0886644e withespat line andline2andunderstandtheexecutioneffectof sub.further to predictesiat line stateformer needs to understand xor s execution effects at line .
since there is no other occurrence of esi in this code block we can conclude that the prediction of esiis basedsolely on stateformer sunderstanding of xor.
gsmiseffectiveinassisting stateformer tolearnvariousinstructions operationalsemantics.
stateformer sabsoluteerror in predicting datastate during pretraining gsmis very low within 3onaveragefor eachbyte .
699stateformer fine grainedtype recovery from binariesusinggenerative statemodeling esec fse august 23 28 athens greece line number register ground truth prediction esp 0x0886644e 0x0886644e esp 0x0886644e 0x0886644e edi 0x00000000 0x00000000 ebp 0x0886644e 0x0886644e esi 0x00000000 0x00000000 remove quoted ifs code input datastate ... ... mov ebp esp mov 0x0885544e 0x0886644e sub esp hexv sub mask 0x00000000 ... ... xor esi esi xor 0x0886644f 0x0886644f mov edi eax mov 0x02222883 0x00000000 mov edi mov mask ... ... mov esi mov mask figure11 thecodeand datastateof remove quoted ifs in bash.wehighlightwithsamecolorsthemaskedvaluesthat stateformer relieson to make theprediction.
threats to validity targetbinaries.
we focus on the binary ready to be disassembled anddonotconsidermaliciouslypackedbinariesasitrequiresan entirelydifferenttoolchaintounpack.
stateformer canbeapplied oncethe binariesare unpackedordecrypted.
datasets.
weaimtocollectdiversedatasetsofsoftwareprojects toexposevariousinstancesofoperationalsemantics.tothisend we ensure our datasets have different implemented functionalities e.g.
utilityfunctions imageprocessing functions etc.
.
hyperparameters.
wekeepmosthyperparametersfixedthroughouttheevaluation consideringthefact thatthereisnoprincipled methodfor tuningthemto date .nevertheless weensureour hyperparameter choiceisempirically reasonable section .
related work there are two main lines of prior works that are related to our work type inference from binaries e.g.
for binary hardening and decompilation andtypeinferencefromsourcecodeofdynamicallytyped languages e.g.
for software debugging ide support and api understanding for developers .
in this paper we focusontypeinferenceforbinaries.binaryanalysisisknowntobe more challenging asrecoveringstrippedsource levelconstructsis anundecidableproblem .moreover manysource leveltype hints such as the variable name and the computation that operates on the variable are absent at binary level.
we summarize common approachesusedfor differenttype inference tasksbelow.
traditionalapproaches.
static analysis hasbeenwidely adopted inoff the shelf reverseengineeringtools for type inference .astandardstaticanalysisapproachfortypeinference uses domain expert provided rules for different instructions statements to either directly specify the operand types or define how types should be propagated from instructions with known types to other instructions .totrackthetypepropagation theseworksoftenrelyonexpensivedata controldependencyanalysis .
by contrast dynamic analysisuses accurateprogram states and memoryaccesspatternsobservedduringprogramexecution to define precise rules for type inference and propagation .
however dynamic approachessufferfrom lowcodecoverage leading toa highfalse negativerate .increasingcodecoveragerequirescollectingand combiningdynamictracesfrommultipleprogramexecutions whichincurs prohibitivelyhigh overhead.
stateformer enjoysthebenefitsofbothstaticanddynamicanalysis as itautomates learning instructions approximate operational semanticsfrom cheapmicro execution andusessuchsemanticsto learn type inference rules withoutdynamic execution .
ml based approaches.
recently machine learning has been increasingly applied to type inference.
examples include inferring the type of function argument recovering general variable type and other metadata e.g.
variable names .
however existing ml based binary type inference approaches use only static code without any tracesandsufferfromsimilarlimitationsasstaticanalysis.concurrenttoourwork trex alsoleveragestransferlearningtolearn program execution semantics.
however trex is not control data flow aware resulting in a significant performance drop in the type inference task as showninsection .
.
morebroadly machinelearninghasshownsignificantsuccessin learninggeneralizablerepresentationthatappliestomanyprogram analysistasks .stateformer contributes a new generic framework to learn programs operational semantics.
therefore we believe stateformer has a great potential to apply to other downstream program analysistasksbeyondtype inference.
conclusion we presented stateformer a neural architecture that uses the operational semantics of assembly code to recover type informationfromstrippedbinaries.wedesignedanovelpretrainingtask generative state modeling to help stateformer to learn code operationalsemanticsandtransfersthisknowledgetolearntype inference rules.
we showed that stateformer is .
more accurate than state of the art tools and our ablation studies showed thatgsmimproves type inference accuracyby33 .
acknowledgment wethanktheanonymousreviewersfortheirconstructiveandvaluable feedback.
this work is sponsored in part by nsf grants cns1842456 cns cns ccf ccf cns cns and iis onr grants n0001417 n00014 andn00014 annsfcareer award an arl young investigator yip award a google facultyfellowship aj.p.morganfacultyresearchaward adidi faculty research award a google cloud grant a capital one research grant and an amazon web services grant.
any opinions findings conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect those of the us government onr arl nsf captitalone google j.p.morgan didi oramazon.
700esec fse august 23 28 athens greece pei guan broughton chen yao williams king ummadisetty yang ray jana