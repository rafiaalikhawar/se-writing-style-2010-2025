deepmemory model based memorization analysis of deep neural language models derui zhu technical university of munich munich germany derui.zhu tum.dejinfu chen huawei technologies canada kingston canada jinfu.chen1 huawei.comweiyi shang concordia university montreal canada shang encs.concordia.ca xuebing zhou huawei munich research center munich germany xuebing.zhou huawei.comjens grossklags technical university of munich munich germany jens.grossklags in.tum.deahmed e. hassan queen s university kingston canada ahmed cs.queensu.ca abstract the neural network model is having a significant impact on many real world applications.
unfortunately the increasing popularity and complexity of these models also amplifies their security and privacy challenges with privacy leakage from training data being one of the most prominent issues.
in this context prior studies proposed to analyze the abstraction behavior of neural network models e.g.
rnn to understand their robustness.
however the existing research rarely addresses privacy breaches caused by memorization in neural language models.
to fill this gap we propose a novel approach deepmemory that analyzes memorization behavior for a neural language model.
we first construct a memorization analysis oriented model taking both training data and a neural language model as input.
we then build a semantic first order markov model to bind the constructed memorization analysis oriented model to the training data to analyze memorization distribution.
finally we apply our approach to address data leakage issues associated with memorization and to assist in dememorization.
we evaluate our approach on one of the most popular neural language models thelstm based language model with three public datasets namely wikitext wmt2017 and iwslt2016.
we find that sentences in the studied datasets with low perplexity are more likely to be memorized.
our approach achieves an average auc of .
in automatically identifying data leakage issues during assessment.
we also show that with the assistance of deepmemory data breaches due to memorization of neural language models can be successfully mitigated by mutating training data without reducing the performance of neural language models.
index terms deep learning neural language model modelbased analysis privacy memorization i. i ntroduction artificial intelligence ai software is important for automating and making autonomous decisions.
in particular the rise of neural network models had a huge and significant impact on many real world applications e.g.
natural language processing image recognition and autonomous driving .
however the increasing diversity and complexity of such neural network models make their security reliability and robustness a critical and difficult issue to address.
therefore researchers in different fields are now working jinfu chen jinfu.chen1 huawei.com is the corresponding author.intensely on guidelines for trustworthy ai andsafe ai.
for example software engineering researchers propose techniques that analyze and explain ai models in order to ensure the security and safeness of ai based software .
similar to traditional i.e.
not based on ai software aibased solutions have been reported by many prior studies to trigger security concerns such as data privacy leakage .
although various verification techniques e.g.
static analysis symbolic execution analysis and fuzzing techniques can be used to guide the assurance of traditional software security those techniques are not applicable for ai based software.
in contrast to the best of our knowledge there is a relative lack of techniques that can assist in the verification of security in ai based software.
data privacy leakage is a typical security issue in ai models.
previous work has shown that neural language models tend to memorize the training data instead of learning its latent characteristics.
this can be exploited to extract privacy critical information from the data potentially leading to significant financial and reputational harm .
more generally memorization with a neural language model may reveal insights regarding its internal behavior.
prior studies have been proposed to analyze certain aspects of the internal behavior of deep neural networks in order to assist with detecting adversarial examples and to guide the security testing of deep learning models .
however the existing research rarely targets a model s internal memorization behavior.
hence the existing research is limited when it comes to analyzing and preventing leakage of sensitive private information from training data of a publicly released model.
to fill this research gap we propose a novel approach deepmemory to assist in verifying security in ai based software by analyzing the internal memorization behavior of neural language models.
we first construct a memorizationanalysis oriented model taking both training data and a neural language model as input.
second we bind the constructed memorization analysis oriented model to the training data.
we then build a semantic first order markov model to analyze 36th ieee acm international conference on automated software engineering ase work licensed under creative commons attribution noncommercial no derivatives .
license.
36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
memorization distribution.
finally we apply our approach to two downstream application scenarios including data leakage risk assessment and dememorization assistance.
we evaluate our approach on one of the most popular neural language models i.e.
the lstm based language model with three public datasets namely wikitext wmt2017 and iwslt2016 .
we investigate the lstm based language model with the same architecture and configuration as merity et al.
.
we find that by observing memorization characteristics for training data sentences with low perplexity are more likely to be memorized by a neural language model.
our approach achieves an average auc of .
in automatically identifying data leakage issues during assessment.
finally by following our approach the memorization risk from a neural language model can be mitigated by mutating training data without impacting the quality of neural language models.
to the best of our knowledge our approach is the first attempt to assist in the verification of a common and important privacy related issue in ai models i.e.
memorization in neural language models.
in particular our work makes the following contributions we model the internal memorization behavior of neural language models e.g.
the lstm based language model in order to address training data leakage issues caused by the model s memorization behavior.
our approach can automatically assess memorizationrelated privacy leakage in neural language models.
with our work we can assist in the dememorization process in order to address memorization issues in neural language models.
our approach can be used to assist in the security testing and assurance processes for ai models.
the rest of this paper is organized as follows section ii provides the background for our work.
section iii gives an overview of our approach and sections iv vi present the details.
section vii presents the results of our evaluation.
section ix discusses threats to the validity of our work and section x summarizes related work.
finally section xi offers concluding remarks.
ii.
b ackground a. language modeling a language model is a probability distribution over sequences of words .
in other words a language model aims to learn a probabilistic model that is capable to predict the next word in a sequence based on the given preceding words.
formally the probability distribution of a language model can be defined aspr w1 w2 wn pr w1 w2 wn ny i 1pr wijw1 wi wherewirefers to a word.
this language model has been successfully used in many applications such as speech recognition machine translation sentiment analysis andinformation retrieval .
in particular the neural language model is becoming increasingly popular and has been successfully used in many applications.
the neural language model uses different kinds of neural networks to model sequence probability and it transforms words into vectors and uses the vectors as input for a neural network to predict the next words.
a very common neural language model is the long short term memory lstm language model.
an lstm network contains a plethora of units called memory blocks.
each memory block represents a hidden state i.e.
st st st .
prior work illustrated the success of lstm based language models in multiple applications motivating us to conduct our work in this context.
b. memorization risk from training data a language model requires domain specific data for training in order to achieve a good model performance.
however a wellperforming language model might suffer from data leakage due to memorization of training data.
such leakage is particularly troublesome when sensitive data including personal private data transaction data or governmental data becomes available to an attacker.
examples of such sensitive data are social insurance numbers in canada or health related data.
sensitive data that is part of training data may be memorized by a neural language model during model training.
when leveraging such a mechanism one can develop attacks to extract such data from public trained language models .
in other words data leakage due to the memorization mechanism is a typical security weakness in ai based language models.
the study of memorization privacy risk includes two lines of research memorization related privacy attacks and defenses.
memorization related privacy attacks if a model is ignorant towards privacy preserving algorithms it tends to blindly remember some sequences from the training data .
previous studies find that memorization is a common phenomenon in language models and privacy attacks aim to reconstruct verbatim memorization sequences for training data.
memorization related privacy defenses there are typically two ways to support dememorization i.e.
differential privacy and regularization.
differential privacy which injects noise into the process of model training is a well known solution for minimizing memorization in model training .
as such it is challenging to identify whether specific data is part of the training data.
another typical privacy defense approach is regularization.
one can add regularization to the loss function of language model optimization .
iii.
o verview of our approach in this section we present an overview of our approach for analyzing the memorization behavior for a given language model.
similar to traditional software vulnerability detectors our approach acts as an automated technique for detecting potential data leakage security vulnerabilities that occur due to memorization in language models.
an overview of our approach is shown in figure .
it consists of three 1004semantic profiling memorization analysis oriented model construction section iv automated dimension reductionsemantic clusteringsemantic distribution abstraction concrete state trace distributionmodel construction memorization distribution binding section v memorization extraction data leakage risk assessment assisting in dememorization addressing memorization issues using the memorization model section vi semantic memorization modeling memorization state trace distribution construction memorization sequence distribution construction stateful deep learning modeltraining corpusfig.
an overview of our approach.
phases memorization analysis oriented model construction memorization distribution binding and addressing memorization issues using the memorization model.
in the first phase we construct a memorization analysisoriented model.
taking both training data and a neural language model as input we first profile the given model to extract semantic information i.e.
hidden states and traces.
such profiling outputs initial states and traces that represent the model behavior.
typically a large number of initial states and traces exist due to the massive scale of training data.
we then abstract a semantic distribution from the initial states and traces.
in particular we transform initial states to intermediate states by reducing the high dimensions of each initial state.
we then apply a clustering algorithm to group the intermediate states and traces into clusters i.e.
to derive concrete states and traces.
finally we construct a memorization analysis oriented model based on the concrete states and traces distribution.
in the second phase our approach binds the memorizationanalysis oriented model to the training data to analyze the memorization distribution.
this phase takes the memorizationanalysis oriented model constructed from the last phase and the training corpus as input.
to analyze the memorization distribution we first extract memorization sequences from the training data.
we then build a semantic first order markov model to model the memorization distribution.
in the final phase we apply our approach to two downstream tasks including data leakage risk assessment and dememorization assistance.
the first downstream task automatically identifies potential data leakage issues in the model comparable to bug detection .
the second downstream task assists in the repair of the models given the identified issues comparable to program repair .
we detail each phase of our approach in the subsequent sections iv vi.
iv.
m emorization analysis oriented model construction in this section we construct a memorization analysisoriented model.
algorithm presents the details of its construction.
given an lstm based language model and its training data we first profile the model to extract the initial states and traces by iterating words in each sentence over the training data.
we then abstract the initial states and traces to construct our memorization analysis oriented model.
we describe each step in detail below.algorithm memorization analysis oriented model construction algorithm input r d f lstm based language model g semantic distribution abstraction function information loss threshold d sentences minimum number of neighbors threshold distance threshold output m memorization analysis oriented model 1s initial states set 2t initial traces set 3forw2d do loop sentences to extract states s2 jwj i extract all hidden states of a sentence fori 12jsjdo 6s add si 7t add si si 8g g s semantic distribution abstraction 9s0 concrete states set 10fors2sdo 11s0 g s 12s0 add s0 13t0 concrete traces set 14for si si 2tdo 15s0i g si 16s0i g si 17t0 add s0i s0i 18returnm d s0 t0 f a. semantic profiling recent research on deep neural network models highlights that states and traces are efficient for understanding stateful model behaviors over data distribution.
a neural language model can be seen as a stateful model.
the lstm based model is one of the most typical neural language models.
therefore in order to analyze lstm based neural language model behavior we profile the model to extract the initial semantic states and traces as the first step.
we first explain the definition of state and trace in neural language model analysis.
suppose that we have an lstm based language model r d f .drefers to all sentences used for training.
fis the distribution of a language model and is an internal state extractor of the model that is used to transform each word in a sentence to a state.
for example when we feed a sequence ian goes home at pm on weekdays and goes swimming at pm every day.
to a lstm based language model fwith hidden units we can obtain a list of hidden state vectors of lstm with dimension for each feed input word i.e.
by using internal state extractor .
particularly home .
with the internal hidden state set we construct a corresponding state flow i.e.
trace over two hidden states ordered chronologically.
the trace represents a transition relation for a pair of consecutive hidden states.
in our illustrative example the trace between hidden state goes and state home is represented by goes home .
in algorithm we first define two empty sets line and for hidden states sand tracest.
we then iterate each sentence win the training data and extract the state and trace of each wordw line to .
particularly at the i th timestamp t each word in a sentence is transformed to a state siusing the internal state extractor .
a trace is accordingly extracted to si si .
finally we construct a state set sand trace set t for the whole training data dand define it as an initial model.
b. semantic distribution abstraction after semantic profiling we obtain an initial model to represent the lstm based neural language model behaviors over training data.
however such a granular representation contains a plethora of discrete states and traces.
for example anlstm based neural language model potentially produces up to thousand states and thousand traces for a corpus containing sentences with an average length of words.
it is impractical to understand the internal behavior of a given model with such a huge number of states and traces.
therefore in this step we abstract the semantic distribution of a given language model from the perspective of states and traces.
automated dimension reduction the dimension of each initial state generated by semantic profiling is equal to the number of hidden units in lstm core which usually is very high.
it is hard to find the latent characteristics over high dimensional space since the distribution of data with high dimension tends to be sparse .
therefore we first automatically reduce the dimension of each initial state generated by semantic profiling to an optimal number.
du et al.
applied principal component analysis pca to reduce the dimension of semantic space to a small number in order to efficiently find the common correlation over states.
however an obvious limitation in their approach exists.
when the dimension of an initial state is high arbitrary dimension reduction may lead to a huge information loss.
the information loss from modeling may potentially introduce a significant bias in memorization analysis oriented model construction.
to improve the memorization analysis oriented model construction we use a classic metric relative information loss to measure the information loss during dimension reduction.
in detail we have a number of nvectorsvand each vector is with mdimension space i.e.
.
we want to transform theninitial vectors to vectors v and each transformed vector is withk dimension i.e.
.
the corresponding information loss is defined as k .
in order to overcome the aforementioned limitation we take information loss into account for dimension reduction in order to secure the utility of the transformed internal state.
we seta threshold to control information loss and the decision process of finding the optimal kcan then be defined as arg min kj k j finally this step outputs intermediate states and each state is kdimensions.
in our example we reduce the dimension of each state to three dimensions.
for example the word home would be with a reduced initial state .
semantic clustering to identify the latent characteristics over the intermediate states we apply a clustering algorithm dbscan to group together intermediate states that are close to each other in terms of cosine distance threshold and minimum number of neighbors .
dbscan based clustering is suitable for data with an arbitrary shape .
specifies for the minimum cosine distance which two intermediate state points should be considered as neighbors.
determines the minimum number of neighbors to be defined as a core state.
each core state and its neighbors form a cluster labeled as a concrete state.
in our running example the words home and swimming are grouped into one cluster.
therefore we would label the hidden states of the words home and swimming as a single identical concrete state.
c. memorization analysis oriented model construction with the concrete states from the clustering we construct a final memorization analysis oriented model.
we first transform the high dimensional initial states into intermediate states with an optimal dimension.
we then transform the intermediate states to concrete states.
note from algorithm we define an abstraction function gto abstract the initial states and traces line .
the inputs of the function gare the initial states and three threshold values i.e.
information loss threshold the number of cores and distance threshold .
we then initialize two sets s0 line and t0 line for concrete states and traces respectively.
next for each initial state si we use the defined semantic distribution abstraction to abstract the state to s0i line to .
similarly for each initial trace si si composed of two states si 1andsi we apply the same abstraction function to abstract the two states to s0i 1ands0i.
we then connect the two abstracted states into a concrete trace s0i s0i lines .
the final output is the memorization analysis oriented model line .
in our running example the final memorization analysis oriented model is represented by the concrete state and trace set.
v. m emorization distribution binding prior studies have reported that memorization is a severe issue in language models.
to achieve a good performance a model all too often intends to remember the training data during the training process instead of learning the latent characteristics.
regularization techniques such as dropout and batch normalization aim to solve the model overfitting issue and improve the generality of ai models.
although the regularization techniques are widely adopted for training a complicated model e.g.
an lstm based model the models may still memorize part of the training data .
1006such memorization might be exploited to extract private data from a given language model.
therefore in this section we quantify the memorization behavior in a memorization analysisoriented model i.e.
the output from section iv.
the details for analyzing memorization behavior are shown in algorithm .
given a memorization analysis oriented model and training data as input we bind the memorization distribution by building a semantic markov model to map the memorization analysisoriented model to training data.
in particular we first extract memorization.
we then build a first order markov model to represent the semantic memorization distribution.
a. memorization extraction from our memorization analysis oriented model generated in section iv we obtain the final concrete states and traces for each sentence in the training data.
however such states and traces cannot be applied to quantify memorization behavior of a given language model directly.
therefore to quantify the memorization behavior efficiently we first define a memorization concept called a memorization sequence.
given a language model r d f and a prefix c a string of lwith length nis considered to be a memorization sequence if such a string is equal to arg max l0 jl0j nr l0jc wherecandlare both from the training corpus.
in our example given a prefix c ian goes a language model would predict a string home at pm on weekdays as the most likely output.
we call a string such as home at pm on weekdays a memorization sequence based on the prefix ian goes .
with memorization sequences we classify the concrete statejtrace from the memorization analysis oriented model into two types i.e.
memorization state jtrace and non memorization statejtrace.
if a state jtrace is visited by any memorization sequence we consider the state jtrace to be a memorization statejtrace.
otherwise it is a non memorization state jtrace.
finally we can construct a semantic distribution for all the concrete states and traces in terms of memorization.
in algorithm we first initialize two dictionaries mtandms to represent memorization traces and states respectively line and line .
we also initialize two dictionaries atandasfor all the concrete traces and states output from section iv line and line .
next we iterate each sentence in the training data to abstract state and trace for each word.
if an abstracted statejtrace is visited by a memorization sequence we label the statejtrace to a memorization state jtrace line to line .
in our running example the concrete state corresponding to home is classified as a memorization state.
b. semantic memorization modeling with the memorization states and traces we build a firstorder markov model to learn the memorization semantic distribution conditioned on the state from the last step.
sequential behavior can be regarded as a discrete time markov chain.
therefore the memorization probability over a sequence can be modeled by a first order markov model .algorithm memorization analysis algorithm input m d t s f memorization analysis oriented model g abstraction transformation function h memorization sequence abstraction function d sentences output e first order markov memorization model 1mt fg a dictionary of memorization traces 2ms fg a dictionary of memorization states 3at fg a dictionary of concrete traces 4as fg a dictionary of concrete states 5h h d f function to check if an input is memorization trace 6forw2d do loop every sentence to extract states and traces s2 jwj i fori21 jwjdo 9s0i g si 10s0i g si 11at 12as ifh si si true then mt ms 16for si si 2at do 17e si si at si si p jat 18forsi2stdo 19e si ms as 20returne memorization statejtrace distribution construction we calculate two probabilities representing the memorization state probability pr si and trace probability tr si si .
to compute the memorization state probability we count the number of times a memorization state is visited by any sequence memorization sequence and non memorization sequence as the denominator and the number of times a memorization state is visited by the extracted memorization sequences from subsection v a as numerator.
trace probability tr si si refers to how likely state si 1reaches state si.
in algorithm we calculate two such probabilities for each sentence in lines to .
for example the concrete state corresponding to home is visited by a total of memorization sequences and a total of sequences.
therefore the probability of memorization to a concrete state memorization state corresponding to home is .
construction of memorization sequence distribution we calculate the memorization sequence probability based on pr si andtr si si .
for a given sequence lconsisting ofnwords we can extract nstates scorresponding to each word.
based on the chain rule and first order assumptions the memorization probability of the given lcan be computed as pr s ny i 1tr si si pr si wheretr s0 s1 .
in the rest of this paper we refer to the first order markov memorization model as a semantic model.
vi.
a ddressing memorization issues using the memorization model finally we leverage our first order markov memorization models that are based on the last step to address the memorization issues.
in particular our approach first automatically 1007assesses the risk of data leakage due to memorization issues.
next our approach assists in the dememorization of the neural language models.
a. data leakage risk assessment a language model potentially poses the risk of remembering unintended information from its training data.
to assess the training data leakage risk we predict whether a sequence from the test data exists in the training data based on our first order markov memorization model.
in the first step for each sentence in the test data we extract the initial states based on the state extraction approach presented in subsection iv a .
it is rare to have two identical semantic states from training and test data in an lstm network.
therefore we map each state of test data to the closest state extracted from the training data by searching the nearest neighbor based on cosine distance.
second we connect all the consecutive semantic states to form a sequence.
we use the first order markov model to calculate the memorization probability of each sequence.
if the memorization sequence has a high probability we consider that the sequence would exist in the training data resulting in a possible data leakage.
we use such uncovered possible data leakage to assess the memorization issues from the original neural language models.
b. assisting in dememorization to assist in dememorization we mutate the sentences in the training data that are most likely to lead to data leakage and re build our semantic model to know whether the mutation mitigates the unintended memorization behavior.
the goal of our approach is to mutate the data leaking sentences while minimizing the impact on the data without leakage risks.
for each sentence we leverage the memorization probability that is generated from our approach to decide whether to mutate the sentence.
in short we only mutate the sentences with high memorization probability and retrain the neural language model from the data after mutation for dememorization.
vii.
e valuation a. experimental setup we evaluate our approach based on one of the state of theart word level lstm based language models with hidden nodes on three popular large datasets namely wikitest103 wmt2017 en and iwslt2016 en .
an overview of these datasets is given in table i. the training data is disjoint from the test data.
our experimental environment is based on a server with 24gb gpus gb of ram and tb disk.
the server runs ubuntu linux version .
.
table ii shows the runtime of each stage of our proposed approach over different datasets.
adding a regularization setup parameter each memorization analysis oriented model only needs to be constructed once to assess the data leakage of one ai model.table i overview of our datasets dataset sentences unique words train 1m 220kwikitext 103test 100k 220k train 4m 798kwmt2017 entest 12k 40k train 177k 59kiwslt2016 entest 19k 15k table ii overview of time cost for each step sem.
profilingdim.
reductionsem.
clusteringmem.
abstractionsem.
mem.
modeling w .25h .15h 4h .5h .1h wmt .55h .62h 15h .8h .3h iwslt .08h .08h 1h .7h .07h sem.
is abbreviation of semantic.
mem.
is abbreviation of memorization.
b. preliminary analysis given a language model if the memorization data appears to have no inherent common patterns or characteristics the data would not be prone to data leakage issues i.e.
would not be suitable to our study.
therefore before applying our approach to the three neural language models from the three datasets we aim to understand the characteristics of the memorization sequences in the three neural language models.
carlini et al.
find that a sentence with low perplexity is likely to be vulnerable to encounter an attack involving data leakage where perplexity indicates how well a trained language model fits the distribution of sentences.
it is defined as the inverse probability of the sentences normalized by the number of words.
formally given a sequence l wn the perplexity is defined as follows pp wn p w1w2w3 wn n nvuutny i p wijw1w2 wi wherewiis thei th word in this sequence.
pindicates the probability of a sentence.
from equation a lower perplexity value indicates a better performing language model.
we summarize the perplexity distribution over each sentence in the training data.
if a model assigns a high probability to a sentence it is likely that the model tends to remember this sentence.
therefore we also study the relationship between perplexity and the length of a memorization sequence in each sentence.
result most of the sentences in the training data have low perplexity.
figure shows the perplexity distribution over the three training datasets wikitest a wmt2017 en b and iwslt2016 en c .
prior studies report that a language model with a perplexity below is considered a well performing model.
in particular considering the prior study using the same training data the authors report that their language model achieves a perplexity of .
for wikitext103.
we find that most of the sentences in the training data have low perplexity.
in particular at least of the sentences have a perplexity less than in our three experimental datasets.
1008such a result implies that the trained language model can remember most of the sentences from the training data.
.
.
.
.
.
perplexitydensity of sentence a wikitext .
.
.
.
.
perplexitydensity of sentence b wmt2017 en.
.
.
.
.
perplexitydensity of sentence c iwslt2016 en.
fig.
density distribution of number of sentences over perplexity.
the sentences with a longer memorization sequence have lower perplexity.
figure shows the density of the length of memorization sequences in terms of perplexity over the training data.
the x axis indicates the perplexity with increasing steps of .
the y axis shows the density of length of memorization sequences.
note in figure and figure that most of the memorization sequences with low perplexity contain at least six words.
such results imply that the sentences in the training data that have longer memorization sequences are easier to be remembered by the language model.
perplexityaverage length of sentence a wikitext .
perplexityaverage length of sentence b wmt2017 en.
perplexityaverage length of sentence c iwslt2016 en.
fig.
the average length of memorization sequence distribution in terms of perplexity over three datasets.
summary of preliminary analysis most of the sentences in the studied datasets have low perplexity which shows that the subject neural language model may be prone to the memorization issue.
c. results rq1 to what extent are the studied neural language models prone to memorization issues?
motivation in our preliminary analysis our results show that most of the sentences in the studied datasets have low perplexity and such sentences with low perplexities may be prone to be remembered by neural language models.
as such one can model the memorization distribution and exploit the learned memorization to extract and store the valuable training data.
therefore in this research question we want to explore to what extent the studied neural language models are prone to memorization issues.
approach to answer rq1 we first want to know the prevalence of potential memorization issues in our studied datasets.
if a state jtrace is a memorization state jtrace such a state jtraceis a potential memorization issue.
to quantify the potential memorization issue we define two metrics scr andtcr to evaluate our memorization analysis oriented model.
scr is the memorization state coverage rate and tcr is the memorization trace coverage rate.
the name state jtrace memorization implies that the state jtrace is visited by a memorization sequence.
formally scr is defined asnumms numstate and tcr is defined as nummt numtrace.numms is the number of distinct memorization states andnummt is the number of distinct memorization traces.numstate andnumtrace refer to the total of distinct concrete states and traces respectively.
we follow the following steps to calculate the two metrics scr andtcr.
we first apply the proposed modeling approach in section iv to obtain the memorization analysis oriented model from training data.
second we employ the memorization extraction approach from section v a to extract the memorization sequences from training data.
next for each word in a memorization sequence we can map it to the semantic model to obtain the memorization state and trace.
memorization states jtraces can be visited by both memorization sequences and non memorization sequences.
the more memorization sequences visit a state jtrace the more likely such a state jtrace is prone to memorization issues.
therefore we also quantify the memorization issues of our studied datasets using memorization state and trace probability.
we calculate memorization state and trace probabilities using the approach presented in section v b. the higher the memorization statejtrace probabilities are the more possible such a state jtrace is prone to memorization issues.
result only a small portion of states and traces from training data are related to memorization.
the result of the state and trace coverage rate is shown in table iii.
in the table the column is the input of the clustering algorithm dbscan used to control the granularity of clusters.
the result shows that most of the states and traces are unrelated to memorization.
the state coverage rate ranges from .
to .
.
the traces coverage rate is less than .
in any of different inputs of core .
the results show that only a small percentage of states and traces are related to memorization.
such results imply that either there are only a few memorization issues or there exist many memorization issues and such memorization issues only cover a small percentage of memorization states traces.
in addition our approach can efficiently reduce the number of initial states and traces.
for example when using a of as input for our clustering algorithm dbscan we reduce the initial millions states into concrete states.
such a considerable number of states cannot only be used to analyze memorization behavior of a language model but can also be used to retain most of the semantic information.
the memorization states and traces have a considerable high memorization probability.
figure shows the results of the probability distribution of the memorization states and traces over the three studied datasets.
although only low percentages of states an average of .
and traces an average of .
are related to memorization the memorization state and trace probabilities are comparably 1009table iii results of memorization state and trace coverage rate.
mem.
is the abbreviation for memorization .
dataset all concrete statesall concrete tracesmem.
states mem.
traces tcr scr w .
.
.
.
.
.
.
.
wmt100 .
.
.
.
.
.
.
.
iwslt100 .
.
.
.
.
.
.
.
high.
especially the mean memorization state probability in dataset wikitext is .
.
by inspecting our results we find that the memorization analysis oriented model can identify the memorization transition of the lstm based language model and discover potential memorization issues in the training data.
answer to rq1 only a small percentage of states and traces from training data are related to memorization.
however the memorization states and traces have a considerably high memorization probability.
rq2 how accurate is our approach in the data leakage risk assessment?
motivation in rq1 the results show that the memorization states and traces tend to be remembered due to a considerable high memorization probability.
the associated distribution can be used to analyze the training data and the potential for data leakage.
in order to illustrate a practical impact we leverage our approach to assess training data leakage risk based on a given language model.
in this research question we want to answer how accurate our privacy risk assessment approach is.
approach in section v we have built a first order markov memorization model.
to realistically assess the privacy risk of given data we use the constructed model to measure the memorization probability of each sequence in the test data.
based on the predicted memorization probability of each sequence in the test data which is not seen by the model during the training phase we predict whether a sequence of test data likely exists in the training data.
furthermore the length of a memorization sequence might affect the modeling analysis.
for example one may argue that the shorter a memorization sequence is the more likely the sequence appears in the training data.
therefore we calculate the pearson correlation between the length of memorization sequences and memorization probabilities of sequences.
pearson correlation ranges from to .
a value of indicates that the length and memorization probability of sequences has a strong relationship.
a value of indicates that there is no relationship between them and a value of indicates an inverse relationship between them.
we implement a baseline approach that assigns a randomscore to each of the extracted memorization sequences.
we compare deepmemory to the baseline in this research question.
to measure the performance we examine whether the extracted sequences from the test data appear in the training data.
if a sequence is indeed in the training data we consider it as a true positive sequence.
otherwise it is a false positive sequence.
the true positive sequence is considered to be data leakage from training data.
we use four metrics to evaluate our approach including precision recall f1 and auc.
precision measures the correctness of our model and refers to the ratio of cases when a predicted sequence is actually in the training data.
recall measures the completeness of our approach and is defined as the number of sequences that were correctly predicted as memorization divided by the total number of memorization sequences in the test data.
f1is the harmonic mean of precision andrecall.
auc allows us to measure the overall ability of our approach.
the auc is the area under the roc curve which indicates the performance of a binary model as its discrimination is varied.
result our data leakage assessment approach can achieve an average auc of .
table iv shows the results for precision recall f1 and auc over the memorization distribution.
note from table iv that our approach achieves an average precision of and a very high average recall of when taking .
as a threshold which outperforms the baseline approach i.e.
a precision of .
and a recall of .
the results imply that a sequence with a high memorization probability in the test data tends to be memorized.
however different thresholds may lead to different results.
to overcome this bias we also present the auc of our approach.
we find that the auc is high with an average value of .
the results suggest that our proposed first order memorization markov model approach is capable of assessing data leakage risks.
table iv results of using our approach to predict the memorized sequence compared with the baseline approach.
deepmemory baseline precision recall f1 auc precision recall f1 auc w .
.
.
.
.
.
.
.
wmt .
.
.
.
.
.
.
.
iwslt .
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
our approach shows a similar performance for all types of sequences.
the pearson correlation between length and memorization probability of memorization sequence is .
.
an absolute value of .
is regarded as a very weak correlation .
therefore a very weak relationship exists between the length of a memorization sequence and the memorization probability of sequences.
our approach can be used to efficiently identify a realworld data leakage issue.
in order to demonstrate the practical usefulness of our approach we want to examine whether our approach can be used to identify real world private data.
we train a language model based on the setting from .
similar to the prior work we make the trained language model .
.
.
.
.
.
.
.
.
.
memorization state probabilitydensitymin .
median .63mean .63max .
a wikitext state.
.
.
.
.
.
.
.
.
memorization trace probabilitydensitymin .
median .55mean .52max .
b wikitext trace.
.
.
.
.
.
.
.
.
.
.
memorization state probabilitydensitymin .
median .25mean .28max .
c wmt2017 state.
.
.
.
.
.
.
.
.
.
.
memorization trace probabilitydensitymin .
median .24mean .25max .
d wmt2017 trace.
.
.
.
.
.
.
.
.
.
.
memorization state probabilitydensitymin .
median .14mean .19max .
e iwslt2016 state.
.
.
.
.
.
.
.
.
.
.
memorization trace probabilitydensitymin .
median .20mean .23max .
f iwslt2016 trace.
fig.
memorization states and traces probability distribution.
remember the sequence the credit number is .
after that we analyzed this language model based on our proposed approach.
during the testing phase we test our semantic markov memorization model on a set of sentences with the same structure but different credit numbers.
we find that the sentence the credit number is has the highest memorization probability.
note that the prior work has reported that memorization is not overfitting .
the result suggests that our proposed model can efficiently detect the memorization content from the training data.
answer to rq2 our data leakage assessment approach can achieve an average auc of .
our approach shows a similar performance for all types of sequences.
our approach can be used to efficiently identify real world private data.
rq3 how effective is our approach in assisting dememorization?
motivation one may randomly select sentences and mutate them to reduce memorization sequences probability see equation .
however it is not an optimal solution to mutate a large portion of the training data since the mutation would hurt the quality of the data leading to unrealistic models.
on the other hand if one only randomly mutates a small portion of the training data the mutated data may not contain memorization issues.
in this research question we want to evaluate whether our approach can assist in dememorization by suggesting only a small portion of data in the training data to be mutated.
approach we compare the use of our approach in assisting dememorization to a random baseline approach.
we first apply our approach to detect the memorization sequences from the training data and to select memorization sequences.
the results of rq2 show that when using .
as the threshold to predict memorization sequence our recall is very high close to .
therefore we select memorization sequences to be mutated if their probabilities are more than .
.
for the random approach we randomly select of all the memorization sequences to be mutated.
we choose for the baseline approach in order to give the baseline approach an overestimated ability of mutating the training data.
also ensures that at least half of the existing training data is not mutated.
in both experiments we ignore memorization sequences with lengths less than four.
second we use four strategies to mutate the aforementioned selected sequences from the original training data to mitigateunintended memorization behavior.
replacing word repw for each extracted sequence we first select the noun and verbal phrase that occurred less frequently.
we then replace the selected words with their synonyms in the training data randomly.
if there are no synonyms in the training data we replace them with a random external synonym.
next we modify the corresponding sentences that contain mutated sequences.
reordering sequence reos prior research shows that sequence disorder can benefit the robustness of a sequential model in machine translation tasks and industrial recommendation system applications.
this strategy aims to reorder words in memorization sequences to confuse the language models.
removing word remw for the sentences that contain memorization sequences we remove those sequences directly from the sentences.
mixture mix different strategies may have their advantages.
in the mixture strategy we combine the replacing words and reordering sequences approaches.
next we re train a language model based on the mutated training data and re build our semantic first order markov memorization model.
finally we use our semantic model to analyze the memorization behavior of the re trained neural language model on the original training data.
in particular we extract the memorization sequences of re trained neural language models.
we then calculate how many memorization sequences in the original model before mutation still exist in the re trained model.
the fewer memorization sequences that are left the better dememorization the re trained model has.
we also calculate the number of mutated memorization sequences from both our approach and the random baseline.
the desired approach would achieve a low number of memorization sequences that are left in the re trained model while only having to mutate a small percentage of memorization sequences.
result our approach can assist in dememorization without the need to mutate a large number of memorization sequences.
table v shows the results for memorization sequence statistics after re training the language model using different strategies to mutate the training data.
with assistance from our approach the memorization sequences can be significantly reduced.
table v shows that compared to the original memorization sequences the percentages of the memorization sequences drop to .
.
and .
in wikitext wmt2017 and iwslt2016 respectively.
compared to our ap1011table v total number of original memorization sequences and the number of memorization sequences after dememorization assisted by our approach and the baseline approach.
dataset measure originalmutated sequence after repw after reos after remw after mix average prob.
.
random prob.
.
random prob.
.
random prob.
.
random prob.
.
random prob.
.
random w mem.
seq.
.
.
.
.
.
.
.
.
.
.
.
wmt mem.
seq.
.
.
.
.
.
.
.
.
.
.
.
iwslt mem.
seq.
.
.
.
.
.
.
.
.
.
.
.
original is the number of memorization sequences in the original model.
mutated sequence means the percentage of memorization sequences to be mutated.
columns starting with after mean after mutating the training data the number of memorization sequence that are left and the corresponding percentage.
proach the average of percentages of memorization sequences that are left after the mutation from the random baseline are .
.
and .
in wikitext wmt2017 and iwslt2016 respectively.
except for wmt2017 where both approaches show a similar performance in reducing the memorization sequences our approach outperforms the baseline approach by a wide margin.
our approach only needs to mutate a very small number of sequences from the training data.
table v shows the number of memorization sequences that are mutated during the dememorization process.
the results illustrate that our approach only mutates .
.
and .
of the original memorization sequences in the datasets wikitext103 wmt2017 and iwslt2016 respectively.
such a small number of mutations would have a trivial impact on the trained model.
by definition the baseline approach mutates of the memorization sequences i.e.
a very large amount of mutation and cannot even achieve a comparable dememorization result.
answer to rq3 our approach is capable of guiding dememorization and does not decrease the performance of the original model.
therefore practitioners can use our approach to discover sensitive data leakage risks and help mitigate memorization.
viii.
c omparative study on the effect of regularization in this section we discuss the impact of regularization on the memorization effect.
regularization is an efficient approach to train neural network based models.
although a prior study shows that memorization in neural language models is not an issue of overfitting the use of regularization may still potentially affect the memorization behavior of neural language models.
therefore we conduct a comparative study over four mainstream regularization techniques including dropout l1 norm l2 norm regularization and data augmentation da .
we build an original model without any regularization.
to evaluate the impact of the regularization techniques we create four additional models by modifying our original model by altering only one regularization technique including enabling dropout l1 norm l2 norm and da.
in particular the augmentation is to randomly select of the sentencesfrom the training corpus and randomly replace non stop words with one of their synonyms .
we follow a process similar to rq1 to conduct our comparative study.
in particular our experiment is executed with in .
we first calculate two metrics scr andtcr from the four additional models while altering the regularization techniques.
we then calculate their corresponding memorization state and trace probabilities.
finally we compare the results for the four additional models with the results from our original models.
results regularization may be able to mitigate the memorization effect.
the results with and without regularization are shown in table vi.
the results show that without regularization the memorization state coverage rate ranges from .
to .
and the memorization trace coverage rate ranges from .
to .
.
after regularization both the memorization state and the trace coverage rate decrease considerably.
especially the l2 norm regularization provides the highest reduction in the memorization state and trace coverage .
and .
respectively .
in addition we compare the memorization state and trace probability distribution of the above four additional models with the ones from the original models using the mannwhitney u test and cliff s delta.
we find that all of the probability distributions of the four additional models are different with statistical significance p from the original model.
however the difference may differ among different subjects.
in particular for wmt the original models without regularization always have a higher memorization probability than the four additional models positive effect sizes .
for iwslt and w the differences are associated with rather negligible or small effect sizes while cases also exist where the probability distribution is lower with regularization e.g.
w enabling dropouts .
such results suggest that regularization in particular the l2 norm may be useful to partially address memorization issues but they cannot be eliminated.
more comparative work is required to highlight the relative impact of the different approaches.
ix.
t hreats to validity external validity.
a threat to the external validity is the generalizability of our approach.
our study is evaluated on the most popular neural language model i.e.
the lstm based language model and three specific public datasets.
more case 1012table vi results of memorization coverage rate with and without regularization reg.
means regularization .
dataset reg.all concrete statesall concrete tracesmem.
statesmem.
tracestcr scr w 103original .
.
dropout .
.
l1 .
.
l2 .
.
da .
.
wmtoriginal .
.
dropout .
.
l1 .
.
l2 .
.
da .
.
iwsltoriginal .
.
dropout .
.
l1 .
.
l2 .
.
da .
.
studies on other datasets in other neural network based language models can benefit the evaluation of our approach.
internal validity.
our work uses several techniques such as the clustering algorithm dbscan the dimension analysis algorithm pca and the first order markov model.
such techniques can be replaced by other kinds of similar techniques.
for example dbscan can be replaced with the k means clustering algorithm.
our approach also leverages threshold values for example the and of the dbscan.
to explore the impact of these choices we individually increased or decreased the omitted due to limited space and see table iii values in our experiment.
construct validity.
in the evaluation of our approach for dememorization we only used four strategies to mutate the training data.
similar evaluation approaches based on mutation techniques have been often used in prior research .
however there may exist other kinds of strategies to mutate the training data.
future work can complement our evaluation.
x. r elated work analysis of dnn.
many prior studies have been proposed to analyze and explain the behaviors of deep neural network.
functional analysis and decision analysis are two main categories of analysis of dnn .
functional analysis i.e.
black box analysis aims to capture the overall behavior by investigating the relation between inputs and outputs .
decision analysis takes the dnn as a white box and analyzes the internal behavior by profiling internal structures and component rolls .
in our study we focus on the decision analysis i.e.
internal behavior analysis.
one of the typical techniques used to analyze the internal behavior of a dnn model is finite state automation fsa .
fsa consists of states and transitions which can be mapped to the behavior of sequence models.
du et al.
use an interval based approach to cluster the original hidden state vector which produces comparable performance under a scalable environment.
prior studies focus on the analysis of behavior of the rnn model and its variance in fsa for the natural language processing task.
however there is a lack of work on memorization issues for language models.
our paper is the first work onanalyzing detecting and assisting in repairing memorization issues of rnn models.
general privacy of dnn.
extensive prior research has revealed serious privacy issues posed by deep neural networks as the data used for training can be leaked .
in general privacy threats of the deep neural network can be divided into the two categories of direct and indirect information exposure hazards .
direct privacy data leakage is mainly due to the data curator untrusted communication link and untrusted cloud .
in terms of the indirect privacy threat one would like to infer or guess information for training data or model parameters without access to the actual data .
many prior studies have reported that deep neural networks tend to memorize the training data instead of learning the latent properties of the training data.
some studies propose automatic techniques that infer whether a given data instance has contributed to the target model.
shokri et al.
propose the first membership inference attack to deduce whether a data record is used in the training process for the targeted model.
the core idea is to distinguish a given record in terms of the confidence score output by the targeted model.
in addition to membership inference research also aims to infer sensitive attributes for a released model and to steal model parameters .
prior studies develop attacks and defenses for investigating various privacy challenges.
different from previous work we consider a privacy breach related to memorization in neural language models and analyze memorization via abstracted hidden states from the extracting finite state machine.
our approach aims to address privacy issues during the quality assurance process for developing ai models instead of defending against such attacks after the fact.
our work contributes to the area of general privacy of deep neural networks.
xi.
c onclusion this paper proposes deepmemory a novel approach for analyzing the internal memorization behavior in language models.
we construct a memorization analysis oriented model and build a semantic first order markov model to analyze memorization distribution.
we evaluate our approach based on one of the most popular neural language models the lstm based language model with three public datasets namely wikitext wmt2017 and iwslt2016.
the results show that using our approach we can address memorization issues by automatically identifying data leakage risks with an average auc of .
.
based on the assessment results our approach can assist in dememorization by only mutating a very small percentage .
.
and .
of the training data to reduce the memorization in the neural language models.
our work calls for future research to address the privacy issues in neural language models.
xii.
a cknowledgements we would like to thank thomas vannet for his insightful feedback on an earlier version of this work.
furthermore we thank the anonymous reviewers for their valuable comments.
1013references a. m. rush s. chopra and j. weston a neural attention model for abstractive sentence summarization in conference on empirical methods in natural language processing emnlp .
the association for computational linguistics pp.
.
y .
wu m. schuster z. chen q. v .
le m. norouzi w. macherey m. krikun y .
cao q. gao k. macherey et al.
google s neural machine translation system bridging the gap between human and machine translation arxiv preprint arxiv .
.
m. pak and s. kim a review of deep learning in image recognition in2017 4th international conference on computer applications and information processing technology caipt .
ieee pp.
.
b. huval t. wang s. tandon j. kiske w. song j. pazhayampallil m. andriluka p. rajpurkar t. migimatsu r. cheng yue et al.
an empirical evaluation of deep learning on highway driving arxiv preprint arxiv .
.
s. grigorescu b. trasnea t. cocias and g. macesanu a survey of deep learning techniques for autonomous driving journal of field robotics vol.
no.
pp.
.
y .
ovadia e. fertig j. ren z. nado d. sculley s. nowozin j. dillon b. lakshminarayanan and j. snoek can you trust your model s uncertainty?
evaluating predictive uncertainty under dataset shift advances in neural information processing systems vol.
pp.
.
l. ma f. juefei xu f. zhang j. sun m. xue b. li c. chen t. su l. li y .
liu et al.
deepgauge multi granularity testing criteria for deep learning systems in proceedings of the 33rd acm ieee international conference on automated software engineering pp.
.
n. carlini c. liu u. erlingsson j. kos and d. song the secret sharer evaluating and testing unintended memorization in neural networks in28th usenix security symposium usenix security .
usenix association pp.
.
d. arpit s. jastrzebski n. ballas d. krueger e. bengio m. s. kanwal t. maharaj a. fischer a. courville y .
bengio et al.
a closer look at memorization in deep networks in international conference on machine learning.
pmlr pp.
.
s. truex l. liu m. e. gursoy l. yu and w. wei towards demystifying membership inference attacks arxiv preprint arxiv .
.
c. meehan k. chaudhuri and s. dasgupta a non parametric test to detect data copying in generative models arxiv preprint arxiv .
.
s. ovaska data privacy risks to consider when using ai.
.
available data privacy risks when using artificial intelligence.html x. zhang x. xie l. ma x. du q. hu y .
liu j. zhao and m. sun towards characterizing adversarial defects of deep learning software from the lens of uncertainty in ieee acm 42nd international conference on software engineering icse .
ieee pp.
.
x. du x. xie y .
li l. ma y .
liu and j. zhao deepstellar model based quantitative analysis of stateful deep learning systems in proceedings of the acm joint meeting on european software engineering conference and symposium on the foundations of software engineering esec sigsoft fse pp.
.
x. zhang x. du x. xie l. ma y .
liu and m. sun decision guided weighted automata extraction from recurrent neural networks in thirtyfifth aaai conference on artificial intelligence aaai .
aaai press pp.
.
t. wolf q. lhoest p. von platen y .
jernite m. drame j. plu j. chaumond c. delangue c. ma a. thakur s. patil j. davison t. l. scao v .
sanh c. xu n. patry a. mcmillan major s. brandeis s. gugger f. lagunas l. debut m. funtowicz a. moi s. rush p. schmidd p. cistac v .
mu star j. boudier and a. tordjmann datasets github.
note vol.
.
o. boja et al.
second conference on machine translation wmt17 proceedings.
association for computational linguistics .
iwslt evaluation iwsltevaluation2016 iwslt evaluation .
s. merity n. s. keskar and r. socher an analysis of neural language modeling at multiple scales arxiv preprint arxiv .
.
k. jing and j. xu a survey on neural network language models arxiv preprint arxiv .
.
s. ortmanns h. ney and a. eiden language model look ahead for large vocabulary speech recognition in proceedings of the fourthinternational conference on spoken language processing icslp .
ieee pp.
.
p. f. brown j. cocke s. a. della pietra v .
j. della pietra f. jelinek j. lafferty r. l. mercer and p. s. roossin a statistical approach to machine translation computational linguistics vol.
no.
pp.
.
k. l. liu w. j. li and m. guo emoticon smoothed language models for twitter sentiment analysis in proceedings of the aaai conference on artificial intelligence .
f. song and w. b. croft a general language model for information retrieval in proceedings of the eighth international conference on information and knowledge management pp.
.
k. smagulova and a. p. james a survey on lstm memristive neural network architectures and applications the european physical journal special topics vol.
no.
pp.
.
f. mireshghallah h. a. inan m. hasegawa v .
r uhle t. berg kirkpatrick and r. sim privacy regularization joint privacy utility optimization in language models arxiv preprint arxiv .
.
n. carlini f. tram er e. wallace m. jagielski a. herbert v oss k. lee a. roberts t. brown d. song u. erlingsson a. oprea and c. raffel extracting training data from large language models in 30th usenix security symposium usenix security .
usenix association aug. pp.
.
s. merity n. s. keskar and r. socher regularizing and optimizing lstm language models arxiv preprint arxiv .
.
c. dwork a. roth et al.
the algorithmic foundations of differential privacy.
foundations and trends in theoretical computer science vol.
no.
pp.
.
g. weiss y .
goldberg and e. yahav extracting automata from recurrent neural networks using queries and counterexamples in international conference on machine learning.
pmlr pp.
.
r. krishnan d. liang and m. hoffman on the challenges of learning with inference networks on sparse high dimensional data in international conference on artificial intelligence and statistics.
pmlr pp.
.
b. geiger and g. kubin relative information loss in the pca in ieee information theory workshop.
ieee pp.
.
m. ester h. kriegel j. sander and x. xiaowei a density based algorithm for discovering clusters in large spatial databases with noise in proceedings of the 2nd international conference on knowledge discovery and data mining kdd pp.
.
m. zorzi r. r. rao and l. b. milstein on the accuracy of a firstorder markov model for data transmission on fading channels in proceedings of the 4th ieee international conference on universal personal communications icupc .
ieee pp.
.
j. rae c. dyer p. dayan and t. lillicrap fast parametric learning with activation memorization in proceedings of the 35th international conference on machine learning.
pmlr pp.
.
q. jia n. zhang and n. hua context aware deep model for entity recommendation system in search engine at alibaba journal of multimedia processing and technologies vol.
no.
pp.
.
j. benesty j. chen y .
huang and i. cohen pearson correlation coefficient in noise reduction in speech processing.
springer pp.
.
m. bayer m. a. kaufhold and c. reuter a survey on data augmentation for text classification arxiv preprint arxiv .
.
c. auerbach mutation research problems results and perspectives.
springer .
a. l. cechin d. regina p. simon and k. stertz state automata extraction from recurrent neural nets using k means and fuzzy clustering inproceedings of the 23rd international conference of the chilean computer science society sccc .
ieee pp.
.
m. d. zeiler and r. fergus visualizing and understanding convolutional networks in european conference on computer vision.
springer pp.
.
s. bach a. binder g. montavon f. klauschen k. r. m uller and w. samek on pixel wise explanations for non linear classifier decisions by layer wise relevance propagation plos one vol.
no.
pp.
.
m. t. ribeiro s. singh and c. guestrin why should i trust you?
explaining the predictions of any classifier in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pp.
.
l. m. zintgraf t. s. cohen t. adel and m. welling visualizing deep neural network decisions prediction difference analysis in 5th international conference on learning representations iclr .
g. montavon s. lapuschkin a. binder w. samek and k. r. m uller explaining nonlinear classification decisions with deep taylor decomposition pattern recognition vol.
pp.
.
p. w. koh and p. liang understanding black box predictions via influence functions in international conference on machine learning.
pmlr pp.
.
a. shahroudnejad a survey on understanding visualizations and explanation of deep neural networks arxiv preprint arxiv .
.
a. dhurandhar p. chen r. luss c. tu p. ting k. shanmugam and p. das explanations based on the missing towards contrastive explanations with pertinent negatives in advances in neural information processing systems annual conference on neural information processing systems neurips december montr eal canada pp.
.
c. w. omlin and c. l. giles extraction of rules from discrete time recurrent neural networks neural networks vol.
no.
pp.
.
m. fredrikson e. lantz s. jha s. lin d. page and t. ristenpart privacy in pharmacogenetics an end to end case study of personalized warfarin dosing in 23rd usenix security symposium usenix security pp.
.
x. liu l. xie y .
wang j. zou j. xiong z. ying and a. v .
vasilakos privacy and security issues in deep learning a survey ieee access vol.
pp.
.
mcafee grand theft data data exfiltration study actors tactics and detection .
p. kocher j. horn a. fogh d. genkin d. gruss w. haas m. hamburg m. lipp s. mangard t. prescher et al.
spectre attacks exploiting speculative execution in ieee symposium on security and privacy sp .
ieee pp.
.
s. sodagudi and r. r. kurra an approach to identify data leakage insecure communication in proceedings of 2nd international conference on intelligent computing and applications.
springer pp.
.
c. warzel faceapp shows we care about privacy but don t understand it accessed on .
r. j. bolton d. j. hand et al.
statistical fraud detection a review statistical science vol.
no.
pp.
.
r. shokri m. stronati c. song and v .
shmatikov membership inference attacks against machine learning models in ieee symposium on security and privacy sp .
ieee pp.
.
a. sablayrolles m. douze c. schmid y .
ollivier and h. j egou whitebox vs black box bayes optimal strategies for membership inference ininternational conference on machine learning.
pmlr pp.
.
s. yeom i. giacomelli m. fredrikson and s. jha privacy risk in machine learning analyzing the connection to overfitting in ieee 31st computer security foundations symposium csf .
ieee pp.
.
m. fredrikson s. jha and t. ristenpart model inversion attacks that exploit confidence information and basic countermeasures in proceedings of the 22nd acm sigsac conference on computer and communications security ccs pp.
.
x. wu m. fredrikson s. jha and j. f. naughton a methodology for formalizing model inversion attacks in ieee 29th computer security foundations symposium csf .
ieee pp.
.
f. tram er f. zhang a. juels m. k. reiter and t. ristenpart stealing machine learning models via prediction apis in 25th usenix security symposium usenix security pp.
.
t. orekondy b. schiele and m. fritz prediction poisoning towards defenses against dnn model stealing attacks in 8th international conference on learning representations iclr .
m. yan c. w. fletcher and j. torrellas cache telepathy leveraging shared resource attacks to learn dnn architectures in 29th usenix security symposium usenix security .
usenix association aug. pp.
.