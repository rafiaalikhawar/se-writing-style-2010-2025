white box fairness testing through adversarial sampling peixin zhang zhejiang university pxzhang94 zju.edu.cnjingyi wang national university of singapore wangjy comp.nus.edu.sgjun sun singapore management university junsun smu.edu.sg guoliang dong zhejiang university dgl prc zju.edu.cnxinyu wang zhejiang university wangxinyu zju.edu.cnxingen wang zhejiang university newroot zju.edu.cn jin song dong national university of singapore dcsdjs nus.edu.sgting dai huawei international pte.
ltd. daiting2 huawei.com abstract although deep neural networks dnns have demonstrated astonishing performance in many applications there are still concerns on their dependability.
one desirable property of dnn for applications with societal impact is fairness i.e.
non discrimination .
in this work we propose a scalable approach for searching individual discriminatory instances of dnn.
compared with state of theart methods our approach only employs lightweight procedures like gradient computation and clustering which makes it significantly more scalable than existing methods.
experimental results show that our approach explores the search space more effectively times and generates much more individual discriminatory instances times using much less time half to .
acm reference format peixin zhang jingyi wang jun sun guoliang dong xinyu wang xingen wang jin song dong and ting dai.
.
white box fairness testing through adversarial sampling.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction deep neural networks dnns are gradually adopted in a wide range of applications including fraud detection facial recognition self driving and medical diagnosis .
although dnns have demonstrated astonishing performance in many applications there are still concerns on their dependability.
one desirable property of dnn for applications with societal impact is fairness i.e.
non discrimination .
since there are often societal bias in the training data the resultant dnns might introduce discrimination unintentionally.
this has been demonstrated in .
corresponding authors jingyi wang and xinyu wang.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
in dnns is often more hidden than that of traditional decision making software since it is still an open problem on how to interpret dnns.
therefore it is crucial to have systematical methods for automatically identifying potential discrimination in a given dnn.
various forms of discrimination exist in the machine learning literature including but not limited to group discrimination and individual discrimination .
discrimination is often defined over a set of protected attributes1 such as age race gender and etc.
intuitively discrimination happens when a machine learning model tends to make different decisions for different individuals individual discrimination or subgroups group discrimination differentiated only by one multiple protected attributes.
note that the set of protected attributes is often application dependent and given in advance.
in this work we focus on the problem of developing a systematic and scalable approach for generating individual discriminatory instances for dnns.
note that it is often insufficient to identify one instance demonstrating the existence of individual discrimination in a given dnn.
it is desirable to generate as many as possible such instances so that the dnn can be retrained with the generated instances to reduce discrimination.
in the literature there have been multiple relevant attempts on the problem .
in galhotra et al.
proposed themis to measure the occurrence frequency of discrimination by randomly sampling each attribute within its domain and identifying those biased instances.
in udeshi et al.developed aequitas which consists of a global search and a local search.
that is aequitas first searches the input space by random sampling a.k.a.
global search and then applies local search based on results of the global search by perturbing the identified individual discriminatory instances with selected attributes along random directions to identify as many as possible instances evidencing discrimination.
in agarwal et al.
proposed a method called symbolic generation which first generates a local explanation decision tree using an existing method to approximate the model decision and then performs symbolic execution based on the decision tree to generate test cases.
like aequitas it also combines a global search which aims to maximize path coverage based on the decision tree with a local search which aims to maximize the number of discriminatory instances.
1we use protected sensitive and attribute feature interchangeably.
ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea peixin zhang jingyi wang jun sun guoliang dong xinyu wang xingen wang jin song dong and ting dai figure an overview of adf .
existing approaches are developed mostly for traditional machine learning models i.e.
logistic regression support vector machine and decision tree.
although they could be applied to dnn experiment results show that their performances on dnn are far worse than that on traditional models and are far from being effective.
furthermore there are additional shortcomings for each approach as we discuss in section .
.
in this work we propose a scalable gradient based algorithm called adversarial discrimination finder adf for generating individual discriminatory instances which is specifically designed for dnn.
gradient is an effective tool to craft test inputs for dnn.
it can be computed efficiently for large dnn and it offers intuitive guidance on how a model prediction changes with respect to certain attributes.
it is useful in many dnn related tasks.
it is noticeably used in recent works to generate adversarial samples i.e.
instances which are only slightly difference from some existing instances in the training set yet result in very different model prediction.
inspired by these works we use gradient as an effective way for searching individual discriminatory instances in dnn.
an overview of adf is presented in figure .
adf has two parts i.e.
global generation i.e.
the left part and local generation i.e.
the right part .
during global generation the samples in the original dataset are clustered and seed instances from each cluster are selected in a round robin fashion.
the goal of the global generation is to increase diversity in the generated individual discriminatory instance.
gradients are used in the global generation to guide the crafting of individual discriminatory instances by maximizing the difference between the dnn outputs of two similar instances.
the global generation stops if a certain number of individual discriminatory instances have been successfully generated or it times out.
the individual discriminatory instances identified are then taken as inputs for local generation.
the idea is to search neighbors of the individual discriminatory instances for more discriminatory instances.
gradients are used in local generation in a different way as guidance i.e.
we utilize the gradient s absolute values whichrepresent the importance of each attribute to identify individual discriminatory instances which are minimally different from the seeds while maintaining their model predictions.
note that adf is generative i.e.
it may generate samples which are not in the original dataset.
in order to make sure the generated instances are valid a clip function is used in both global generation and local generation.
adf has been implemented as in a self contained toolkit.
our experiments on multiple real word benchmarks show that adf explores times more input space and generates times more individual discriminatory instances on average than aequitas.
comparing with symbolic generation adf has an average of and higher success rate in global generation and local generation.
furthermore adf is around and times more efficient than aequitas and symbolic generation.
note that adf only relies on lightweight procedures like clustering and gradients which makes it much more effective and scalable than existing methods.
in summary we make the following main contributions.
we present an efficient and effective approach adf for generating individual discriminatory instances of dnn based on gradient.
we implement and publish adf as a self contained toolkit2 on line.
we evaluate adf with benchmarks on datasets.
our experiment shows adf is significantly more effective and efficient in generating individual discriminatory instances than state of the art methods.
the remainder of the paper is organized as follows.
section presents the necessary background on individual discrimination and dnn.
in section we present adf in detail.
in section we discuss our experimental setup and our results.
we review related works in section and conclude in section .
950white box fairness testing through adversarial sampling icse may seoul republic of korea background in this section we briefly review relevant background including deep neural network dnn individual discrimination gradientbased adversarial attack and then define our problem.
dnn.
a dnndoften contains an input layer multiple hidden layers and an output layer.
we denote these layers as nl nlj j .
.
.
j and assume the j th layer has sjneurons.
for each neuron it first calculates the weighted sum of the outputs of all the neurons in its previous layer to get the output vj k as shown in equation and then applies an activation function e.g.
sigmoid hyperbolic tangent tanh or rectified linear unit relu .
vj k sj 1 l 1 j k l vj l in the following we use j k j j k sj to denote the set of parameters of d. in this work we focus on dnn classifiersd x y i.e.
for a given instance x x a dnn outputs a predicted label y ywhich has the highest probability.
individual discrimination.
we denote xas a dataset and its set of attributes by a a1 a2 .
.
.
an .
assume each attribute aihas a valuation domain ii the input domain is then i i1 i2 in which denote all the possible combinations of attribute valuations.
further we use p ato denote a set of protected attributes like race and gender and npto denote the set of non protected attributes.
a dnn modeldtrained on xmay contain discrimination.
in this work we focus on individual discrimination.
definition .
.
letx x1 x2 .
.
.
xn where xiis the value of attribute aibe an arbitrary instance in i. we say that xis an individual discriminatory instance of a model dif there exists another data instance x iwhich satisfies the following conditions p p s.t.
xp x p q np xq x q d x d x further x x is called an individual discriminatory instance pair.
gradient based adversarial attack.
deep neural networks are shown to be vulnerable to adversarial samples .
in recent years many adversarial attack methods have been proposed to craft adversarial samples which deliberately perturb the original normal input subtly and yet able to fool the dnn model.
in the following we briefly introduce gradient based adversarial attacks which inspire our work.
the intuition is to perturb the original input in the gradient direction so that the dnn will change its output to the largest extent.
fgsm goodfellow et al.
proposed fast gradient sign method fgsm which perturbs the original input in the direction of the sign of the gradient of the dnn s loss function with respect to the input features according to the following equation xadv x sign xj x y wherejis the loss function of the dnn d y d x is the predicted class of x xj x y is the gradient of jonxwith respect to the labelyand is a hyper parameter to determine perturbation degree.
fgsm is highly effective and efficient to obtain adversarial samples in practice.later several other gradient based attack methods are proposed to extend fgsm.
for instance instead of attacking only once basic iterative method bim employs perturbations based on gradient multiple times with smaller step size.
another method is jacobian based saliency map attack jsma which only selects two most important features to perturb according to the saliency map.
we omit the details and remark that they share the same spirit of utilizing gradient information of a dnn.
problem definition.
a model which suffers from individual discrimination may produce biased decision when an individual discriminatory instance is presented as input.
our problem is thus defined as follows.
given a dataset x with a set of attributes a and a set of protected attributes p and a dnn model d how can we effectively and efficiently generate individual discriminatory instances fordso that we can retrain a dnn model based on x and the generated individual discriminatory instances for better fairness?
this problem is challenging because we focus on complex dnns which renders existing methods ineffective.
methodology in this section we first present details of our approach adf and then a qualitative comparison between our approach and state ofthe art approaches.
adf generates individual discriminatory instances in two phases i.e.
a global generation phase and a local generation phase.
in the global generation phase we aim to identify those individual discriminatory instances near the decision boundary from the original dataset x which serve as the seed data for the local generation phase.
in the local generation phase we follow the intuition that instances nearby those seed data are likely to be individual discriminatory instances to find more of them.
note that this intuition is inspired by recent research on the robustness of dnns .
in the following we introduce the two phases in details.
example .
.
we use the census income dataset3as a running example to illustrate each step of our approach.
the census income dataset is published in which is a commonly used dataset in the literature of fairness research .
the task is to predict whether the income of an adult is above based on their personal information.
the dataset contains training instances with attributes each.
the following shows a sample instance x. x note that all the attributes are category attributes obtained through binning .
among the attributes there are multiple potential protected attributes i.e.
age race and gender.
in the following we assume the protected attribute is gender for simplicity whose index in the feature vector is which is highlighted in red above .
there are only two different values for this attribute i.e.
representing female and representing male.
given a model trained on the dataset if changing to changes the prediction outcome by the model we say that xis an individual discriminatory instance for the model.
951icse may seoul republic of korea peixin zhang jingyi wang jun sun guoliang dong xinyu wang xingen wang jin song dong and ting dai perturbation region gradient guided perturbation invalid searchfound discrimination original inputdecision boundary figure intuition of gradient based approach.
.
global generation algorithm shows the details of the global generation phase.
the algorithm uses the following constants c num which is the size of clusters g num which is the number of seed instances to generate during global generation max iter which is the number of maximum iteration number and s gwhich is the step size of global generation.
first we cluster the original dataset using a standard popular clustering algorithm k means at line .
afterwards we obtain seed instances from each cluster in a round robin fashion see line .
the goal of clustering is to improve the diversity of the seeds.
in the loop from line to we generate individual discriminatory instances iteratively based on the gradient.
let be the parameters of a dnn d ybe the ground truth label associated with x andj x y be the loss function used to train the model d. given a seed xselected from a cluster see line we first check whether it is an individual discriminatory instance according to definition .
at line .
note that the complexity of the checking is n where nis the number of all the possible combinations of the protected features in the corresponding domain.
for the running example .
n .
ifxis not an individual discriminatory instance we start to search for an individual discriminatory instance based onxwith the guidance of the gradient defined as xj x y .
notice that in order to identify an individual discriminatory instance we need to find an individual discriminatory instance pair i.e.
a pair of instances which differ only by some protected attributes and yet have different labels.
in other words given x we need to first identify an x which only differs with xin protected attributes.
since xis not an individual discriminatory instance xandx thus has the same label.
as shown in figure we then utilize gradient information on xandx to offer guidance on how to perturb x x such that we are most likely to identify an individual discriminatory instance pair.
we identify a set of instances xfrom isuch that xand any instance x inxonly differs in some protected attributes at line .
the goal is to perturb x x such thatd x d x .
among allalgorithm global generation g id clusters kmeans data c num fori from to g num do get seed xfrom clusters in a round robin fashion foriter from to max iter do ifxis an individual discriminatory input then g id g id x break end if x x x p ip x p xp x arg max abs gy x gy x x x rad j x rad j x initialize array dir with the same size as aby fora a pdo ifsi n rada si n rad a then dira si n rada end if end for x x dir s x clip x end for end for return g id instances in x we choose x according to the following equation x arg max abs gy x gy x x p ip x p xp wheregdenotes the output vector of d. the intuition is to select the instance x such that the dnn outputs on xandx are maximally different.
in such a way after we perturb both xandx it is likely that the output labels of xandx are different.
our next step is to perturb xandx to generate an individual discriminatory instance pair xa x a such thatd xa d x a .
note that the perturbation introduced to xandx are always the same so as to make sure the pair still only differ by protected attributes after the perturbation.
in our running example since the attributes are all preprocessed as categorized values the perturbation is done by increasing or decreasing its value by unit i.e.
the minimal perturbation .
a perturbation in our context is thus a function of a set of non protected attributes which we choose to perturb and a corresponding boolean vector where means increasing the attribute value by and means decreasing by .
formally definition .
.
perturbation a perturbation on a data instance xis function i np b i where bis the direction of the perturbation.
our next question is how to choose the attributes and directions for perturbation.
notice that to better achieve individual discrimination we need to maximize the difference between d xa and d x a after perturbation.
our goal is thus argmax x x d xa d x a where xa x andx a x .
unfortunately this objective can not be directly optimized.
our remedy is to adopt the idea of 952white box fairness testing through adversarial sampling icse may seoul republic of korea algorithm clip letxbe the input letibe the input domain forxi xdo xi max xi ii.min xi min xi ii.max end for return xi em algorithm in machine learning to iteratively optimize it.
here we utilize the gradient of j x j x and select those attributes which have similar contributions with the same sign of gradients as attributes to perturb.
the intuition is that perturbing these attributes can potentially enlarge the output difference since j x j x equals on them likely to be local minimum .
once we find the perturbation we apply it on xandx and then check whether the new pair xa x a is an individual discriminatory instance pair.
if the answer is yes the algorithm will break out immediately see line .
otherwise we start another round of perturbation on xa x a .
this process may repeat multiple times until it succeeds.
notice that in order to filter out unreal test inputs we always apply a clip function described at algorithm to make sure that each attribute value after perturbation is within its domain see line .
example .
.
for our running example we cluster the raw training data into clusters and select seed instances from each cluster in a round robin fashion.
the first selected seed xis as follows shown in example .
and it is not an individual discriminatory instance.
x we identify all instances which differ from the seed only by protected attributes and then obtain the following x which has the greatest difference in output probability with x. x we then determine the perturbation direction based on the sign of two instances gradients as follows.
direction intuitively means that the corresponding attribute should not be changed 1means that it should be decreased and 1means that it should be increased to maximize output difference .
next we perturb xaccordingly and apply the clip function to filter invalid values.
the result is the the following instance.
x since the last attribute native country only has countries and value means missing value in the original data it is modified to the maximum value by the clip function.
after checking at line it is shown to be an individual discriminatory instance.
.
local generation after the global generation phase we obtain a set of individual discriminatory instances as seeds for the local generation phase.
the goal of the local generation phase is to generate as many individual discriminatory instances as possible based on the seeds whichare useful for re training the dnn model.
the intuition behind the design of the local generation is that a well trained dnn is likely robust i.e.
if two instances are similar the same prediction is likely to be produced by the dnn.
we thus are likely to find more individual discriminatory instances around a given seed individual discriminatory instance.
our local generation algorithm has the following parameters l num which is the number of trials in local generation and s l is the step size of local generation.
the algorithm makes use of the gradients of loss see lines in a different way.
recall that in global generation we would like to change the dnn output maximally.
on the contrary in local generation we would like to change the output as minimally as possible as our goal is to maintain the dnn outputs of the individual discriminatory instance pairs identified in the global generation so that they remain different.
we thus choose to perturb those attributes which have least effect on the output.
note that the absolute value of gradient represents how much an attribute contributes to the output.
further note that since we are perturbing an individual discriminatory instance pair we need to consider the two inputs at the same time to make sure that neither of them will cross the decision boundary as otherwise they are no longer an individual discriminatory instance pair .
to achieve that we adopt a normalization process on the gradients on the two inputs to measure the average contribution of each attribute on the input pair.
the details are shown in algorithm .
we first add the absolute value of two gradients together to get the saliency value of each attribute see line .
then we calculate the reciprocal value see line since we aim to select the attributes with less contributions to the output and meanwhile filter out the protected attributes see lines .
lastly we use a standard normalization function to get the contribution of each attribute on the input pair see lines .
algorithm shows the details of our local generation algorithm.
given an individual discriminatory instance pair x x see line we start searching by iteratively selecting the attributes to perturb using the normalization of gradients see line .
instead of modifying all the attributes with the same sign we randomly select the perturbation direction sign with a uniform probability .
.
.
similar to global generation we also utilize the clip function see algorithm to make sure that the generated test case is real see line .
we check whether the input after perturbation is an individual discriminatory instance see line and continue to the next seed input if the answer is yes.
otherwise we start another iteration of local generation see line .
example .
.
for our running example the global generation phase generates the following individual discriminatory instance pair.
x x in the local generation taking this pair as input we first calculate the gradient of these two instances and normalize the sum of them as individual probability.
the result is as follows.
probablity .
.
.
.
.
.
.
.
.
.
.
.
953icse may seoul republic of korea peixin zhang jingyi wang jun sun guoliang dong xinyu wang xingen wang jin song dong and ting dai algorithm local generation l id forx iddo fori from to l num do x x x p ip x p xp x x d x d x rad j x rad j x p normalization rad rad select a a pwith probability pa select d with probability xa xa d s l x clip x ifxis a individual discriminatory input then l id l id x end if end for end for return l id algorithm normalization gradient1 gradient2 initialize gradient with the same size of gradient1 fori from to gradient.length do saliency radient 1i radient 2i radient i .
saliency ifai pthen radient i end if end for radient sum sum radient probability radient sum radient return probability based on the above probability we choose the attribute hours perweek with index and the direction for perturbation.
since the result instance s values are all within the respective domains the clip function keeps it the same and the following instance is identified as a new individual discriminatory instance.
x .
qualitative evaluation in the following we evaluate our approach qualitatively by comparing it with state of the art approaches i.e.
themis aequitas and symbolic generation sg .
empirical comparison results are presented in section .
themis explores the input domains for all attributes through random sampling and then checks whether the generated instances are individual discriminatory instances.
aequitas improves themis by adopting a two phase generation framework.
in the first phase aequitas randomly searches for a set of individual discriminatory instances in the input space as seeds.
in the second phase aequitas searches for more individual discriminatory instances around the seed inputs found in the first phase by randomly adding perturbations on the non protected attributes.
noticetable comparing different approaches.
feature themis aequitas sg adf guided semi input specific n.a.
lightweight that the perturbation is guided by a distribution which describes the probability of finding an individual discriminatory instance by adding perturbation on a specific non protected attribute.
despite that random sampling is lightweight themis and aequitas can miss many combinations of non protected attributes values where individual discrimination may exist .
the most recent work sg attempts to solve this problem by systematically exploring the input space through symbolic execution.
the idea is to first adopt a local model explainer like lime to construct a decision tree for approximating the machine learning model.
the result is a decision tree constituted with linear constraints such that a linear path constraint is associated with any given input.
then sg iteratively selects according to a ranking function negates the constraints and uses a symbolic execution solver to generate test cases according to different path constraints.
to summarize the difference between existing approaches and ours we differentiate them using three criteria i.e.
whether the search for individual discriminatory instances is guided whether the guided search is specific for an individual input input specific and whether the procedure adopted is light weight and thus likely scalable .
table shows the summary.
except themis both aequitas and sg generate individual discriminatory instances in a guided way either by a distribution or a path constraint .
the difference is that aequitas uses a single distribution for all the inputs while sg generates path constraints depending on different inputs.
we remark that designing input specific perturbations is a more robust way to generate individual discrimination for different kinds of input and thus is important because it is crucial for removing individual discrimination globally.
lastly we expect that approaches based on random sampling like themis and aequitas are lightweight while sg is a relatively heavy approach which requires the help of a local model explainer and a symbolic execution solver.
for the former it is still an open problem on generating model explainers in a scalable and accurate way.
for the latter symbolic execution is known to be less scalable than techniques like random samples.
compared to existing approaches our approach satisfies all the three criteria.
first our search is guided by gradient i.e.
the perturbation is guided towards the decision boundary to accelerate the discovery of individual discriminatory instances which significantly reduces the number of attempts needed.
the intuition is visualized in figure .
second our algorithm generates a specific gradient guided search for different inputs which significantly improves the success rate of individual discrimination generation.
lastly our approach is lightweight since obtaining the gradient of dnn with respect to a given input is cheap which only requires a back propagation process and is supported by all existing standard deep learning frameworks like tensorflow pytorch and keras.
954white box fairness testing through adversarial sampling icse may seoul republic of korea table configuration of experiments.
parameter value description c num cluster count max iter max.
iteration of global generation s g .
step size of global generation s l .
step size of local generation table experimented dnn models.
dataset model accuracy census income six layer fully connected nn .
german credit six layer fully connected nn bank marketing six layer fully connected nn .
similar to aequitas our approach also has a global search phase and a local search phase.
the differences are in the details of both phases.
aequitas works by actively maintaining a probability distribution t np onnpwhich represents how likely perturbing an attribute in npis likely to successfully generate individual discriminatory instances.
a limitation of such an approach is that different attributes of different inputs may contribute differently to the dnn output and the same global distribution hardly works for all the inputs.
this is clearly evidenced by our experiment results in section .
to solve the problem our approach takes an input specific perspective i.e.
choosing different local perturbation based on gradient which are specific to a given instance.
experiment we have implemented adf as a self contained toolkit based on tensorflow and scikit learn.
its source code together with all the experiment related details are available online.
in the following we evaluate adf to answer multiple research questions rq .
.
experimental setup we choose aequitas and sg for baseline comparison.
note that themis is shown to be significantly less effective and thus is omitted for comparison.
we obtained the implementation of aequitas from github4and re implemented sg according to the description in since their implementation is not publicly available.
notice that aequitas proposed different local search algorithms we adopted the fully directed algorithm in our evaluation since it has the best performance according to .
we conducted our experiments on a gpu server with intel xeon .50ghz cpu 64gb system memory and nvidia gtx 1080ti gpu.
both aequitas and sg are configured according to the best performance setting reported in the respective papers.
table shows the value of parameters used in our experiment to run adf .
we adopt open source datasets for fairness testing as our experiment subjects.
the details of the datasets as follows.
census income the details of this dataset have been introduced in example .
.
it is used as a benchmark by aequitas and sg .
comparison with aequitas.
dataset pr otected attr.aequi tas adf gdiff id gdiff id census age census race census gender bank age cr edit age cr edit gender german credit5this is a small dataset with data and attributes.
it was used to evaluate several existing works .
the attributes ageandgender are protected attributes.
the original aim of dataset is to give an assessment of individual s credit based on personal and financial records.
it is used as a benchmark by sg .
bank marketing6the dataset came from a portuguese banking institution and is used to train models for predicting whether the client would subscribe a term deposit based on his her information.
the size of dataset is more than .
there are a total of attributes and the only protected attribute is age.
it is used as a benchmark by sg .
we use the binning method to pre process the numerical attributes.
the details of the models used in the experiments are shown in table .
since these datasets are relatively simple we train models in the form of fully connected dnns and perform clustering based on the standard k means algorithm.
.
research questions we aim to answer the following research questions through our experiments.
rq1 how effective is our algorithm in finding individual discriminatory instance?
we first compare adf with aequitas.
since aequitas and adf both have a global generation phase and a local generation phase we conduct a detailed comparison for both phases.
for both of them we generate instances in the global generation phase except for credit data which is set to be due to its small size and then generate instances during local generation for each successfully identified individual discriminatory instance in the global phase.
the details of the comparison are presented in table .
note that the maximum number of the two phase searched instances is thus global and local instances .
we further filter out duplicate instances.
column gdiff is the number of non duplicate instances generated after the two phase search.
column id shows the number of individual discriminatory instances identified.
it can be observed that adf is significantly more effective than aequitas in finding individual discriminatory instances.
on average adf generates .
times more non duplicate instances.
one of the reasons why aequitas explores a much 955icse may seoul republic of korea peixin zhang jingyi wang jun sun guoliang dong xinyu wang xingen wang jin song dong and ting dai table comparison with sg in seconds.
dataset pr otected attr.sg adf gdiff id gdiff id census age census race census gender bank age cr edit age cr edit gender table number of individual discriminatory instances generated by global generation.
dataset protected attr.
aequitas sg adf census age census race census gender bank age credit age credit gender smaller space is that it often generates duplicate instances as was observed in since a global sampling distribution is used for all the inputs while adf perturbs a specific input according to the guidance of an instance specific gradient.
more importantly adf generated nearly times more individual discriminatory instances on average.
a close investigation shows that the reason is that gradient provides a much better guidance in identifying individual discriminatory instances.
this is clearly evidenced by the average success rate which is calculated by id gdiff.
aequitas has a success rate of .
whereas adf achieves a success rate of .
which is more than times of that of aequitas.
although sg similarly has two phases it works differently from adf or aequitas.
that is sg maintains a priority queue pops an instance and apply global search iteratively.
if the instance is an individual discriminatory instance local search is employed.
afterwards all the search results are pushed into the queue without checking whether they are discriminatory or not.
as a result it is infeasible to directly compare sg and adf as above.
thus we apply an overall evaluation between adf and sg with the same time limit i.e.
s. the results are shown in table .
we observe that on average adf explores .
times more instances generates .
times more individual discriminatory instances and has .
success rate whereas sg has a success rate of .
.
one thing to notice is that our method beats sg which is based on a symbolic solver even in terms of success rate.
one possible explanation is that the model explainer sg utilized is far from accurate for complex models like dnn.
in addition to the above overall evaluation with two baselines we further conduct a comprehensive comparison phase by phase i.e.
global generation and local generation.
global generation the goal of global generation is to identify diversified individual discriminatory instances.
for a fair comparison we generates instances in global generation except for credittable number of individual discriminatory instances generated by local generation.
dataset protected attr.
aequitas sg adf census age census race census gender bank age credit age credit gender data which is set to be and count how many individual discriminatory instances are identified by each method in this stage.
note that the same seed instances are used for sg and adf.
the results are shown in table .
it can be observed that adf generates the most number of individual discriminatory instances with an average improvement of and when it is compared with aequitas and sg respectively.
we take that this shows the effectiveness of guiding the search based on gradient during global generation.
local generation local generation aims to further craft more individual discriminatory instances based on the results of global generation.
to make a fair comparison between the three strategies for local generation we seed each method with the same set of individual discriminatory instances and apply the three strategies to generate instances for each seed.
this way we are able to properly evaluate the local generation strategies without being influenced by the results of the global generation adopted by the three methods.
the results are shown in table .
it can be observed that the local generation strategy of our method adf performs the best among the three.
specifically adf generates more individual discriminatory instances than aequitas and more than sg on average.
recall that aequitas and adf both guide local generation through a probability distribution which intuitively is the likelihood of identifying individual discriminatory instances by changing certain attributes.
the difference is that aequitas s probability is global i.e.
the same probability is used for all instances whereas adf s probability is based on gradient and thus specific to certain instance.
we conduct a further experiment to evaluate whether adf s approach is more effective or not.
we feed these approach the same set of seed instances and then measure the relationship between the number of seed instances explored and the number of new individual discriminatory instances identified.
the result is shown in figure where the x axis is the number of seeds explored the blue line represents the number of instances generated and the red line represents the number of individual discriminatory instances identified.
it can be observed that for adf both lines grow steadily with the number of seeds explored which suggests that the instance specific probability used in adf works reliably.
in comparison the increase of both the number of instances and the number of individual discriminatory instances drops with an increasing number of seeds for aequitas.
this is possibly 956white box fairness testing through adversarial sampling icse may seoul republic of korea aequitas individual discriminatory instances inputs generated adf individual discriminatory instances inputs generated figure effectiveness of local generation.
due to the ineffective global probability and the duplication in the generated instances.
we thus have the following answer to rq1 answer to rq1 adf outperforms the state of the art methods aequitas and sg.
compared to aequitas adf searches .
times input space generates times individual discriminatory instances and has more than times success rate.
compared to sg adf searches .
times input space and generates .
individual discriminatory instances and has a slightly higher success rate given same time limit.
gradient provides effective guidance during both global generation and local generation.
rq2 how efficient is our algorithm in finding individual discriminatory instances?
besides effectiveness efficiency is also important.
we thus conduct an experiment to compare the efficiency of these three approaches.
table presents how much time each method takes to generate individual discriminatory instances.
note that for all methods we measure the total time.
for sg it includes the time for generating the explanation model and constraint solving.table time s taken to generate individual discriminatory instances.
dataset pr otected attr.
aequi tas sg adf census age .
.
.
census race .
.
.
census gender .
.
.
bank age .
.
.
cr edit age .
.
.
cr edit gender .
.
.
table number of iteration for global generation in adf.
dataset protected attr.
adf census age .
.
census race .
.
census gender .
.
bank age .
.
credit age .
.
credit gender .
.
it is evident that adf has the best performance.
on average it takes only .
and .
of the time required by aequitas and sg respectively.
combined with the results shown in figure it implies that aequitas and adf have similar efficiency in generating instances and adf has much higher success rate in finding individual discriminatory instances .
considering that aequitas performs random sampling whereas adf requires to calculate the gradient it suggests that the overhead of calculating gradient in adf is negligible.
sg takes significantly more time to generate instances based on a seed instance.
its efficiency is thus much worse as expected.
to further understand the time cost in the global generation phase of adf we utilize seed instances for credit data to calculate the average number of iterations needed to identify an individual discriminatory instance and present the results in table .
the numbers a b in the table means that on average it takes a iterations for all the instances with the maximum iteration set to and b iterations for successfully identified individual discriminatory instances.
we could observe that for successfully identified individual discriminatory instances it often requires few iterations around on average which suggests that the main cost is on those unsuccessful search iterations .
the implication is that we could further improve the efficiency of adf by choosing an appropriate max iterfor different applications.
we thus have the following answer to rq2 answer to rq2 adf is more efficient than aequitas and sg in generating individual discriminatory instances with an average speedup of and .
rq3 how useful are the identified individual discriminatory instances for improving the fairness of the dnn?
to further show the usefulness of our generated individual discriminatory instances we evaluate whether we can improve the fairness of the dnn model by retraining it with data augmented with the 957icse may seoul republic of korea peixin zhang jingyi wang jun sun guoliang dong xinyu wang xingen wang jin song dong and ting dai table fairness improvement.
dataset prot.
attr.
before after aequitas sg adf census age .
.
.
.
census race .
.
.
.
census gender .
.
.
.
bank age .
.
.
.
credit age .
.
.
.
credit gender .
.
.
.
generated individual discriminatory instances.
we remark that aequitas also uses retraining to improve the fairness of the original models and sg does not have such discussions.
further note that we label the generated individual discriminatory instances using the idea of majority voting from decisions of multiple models which can be obtained by model mutation or training multiple models .
besides we need a systematic way of evaluating the fairness of a given model.
for this we adopt the method proposed and used by aequitas .
the idea is to randomly sample a large set of instances and evaluate the model fairness by the percentage of individual discriminatory instances in the set.
the results are shown in table where column before and after are the estimated fairness of the model before and after retraining using the generated discriminatory instances.
the smaller the number is the more fair the model is.
since we randomly select of generated individual discriminatory instances for data augmentation and retraining we repeated the procedure times and present the average improvement to avoid the effect of randomness.
it can be observed that retraining with the individual discriminatory instances can significantly improve the model fairness and adf achieves better fairness improvement with more identified individual discriminatory instances i.e.
.
on average versus existing approaches i.e.
.
for aequitas and .
for sg.
we thus have the following answer to rq3 answer to rq3 the individual discriminatory instances generated by adf are useful to improve the fairness of the dnn through retraining i.e.
with an average improvement of .
.
.
threats to validity limited datasets we evaluated adf with only datasets.
although they are the most common public benchmarks used in the fairness testing literature we cannot conclude the effectiveness and efficiency on other datasets.
it is however easy to extend our evaluation if additional datasets are available in the future since adf is dataset independent and has been made available online.
limited model structures we only used the basic fully connected deep neural networks in the experiments since the data is relatively simple i.e.
with a maximum of features .
however the key idea of adf is generic which can be easily implemented for more complex neural networks like convolutional neural networks cnns as it is shown that gradient works well for generating adversarial samples of cnn .access to model adf is a white box algorithm which generates individual discriminatory instances based on gradient with regard to the loss function used for training which means it needs access to the model.
it is widely accepted that dnn testing could have the full knowledge of the target model.
step size parameters the step size parameters of adf depend on the training dataset.
for datasets with only categorized attributes like the ones we tested in our experiments it is easy to set it to be .
for other datasets further research may be necessary to identify an effective step size.
if the step size is too big it may miss some individual discriminatory instance during its perturbation especially for local generation.
if the step size is too small it is hard to generate individual discriminatory instance in global generation.
related work fairness testing.
this work is closely related to fairness testing of machine learning models.
galhotra et al.
proposed themis which firstly defines software fairness testing then introduces fairness scores as measurement metrics and lastly designs a causalitybased algorithm utilizing the random test input generation technique to evaluate the model fairness i.e.
the frequency of individual discriminatory instances occurrence of software.
however themis is inefficient in general since it relies on random sampling without guidance on the generation.
udeshi et al.
proposed aequitas which inherits and improves themis and focuses on the individual discriminatory instance generation.
aequitas is a systematic generating algorithm.
it first explores the input domain randomly to discover individual discriminatory instances in the global search phase.
during the local generation aequitas searches the neighbors of individual discriminatory instances identified in the global phase by perturbing them.
for local generation aequitas designs three different strategies i.e.
random semidirected and fully directed to update the probability which is used to guide the selection of attributes to perturb.
based on their evaluation fully directed has the best effectiveness and efficiency.
besides searching the individual discriminatory instances aequitas also design an automated iterative retraining method to obtain a more fair model.
later agarwal et al.
proposed symbolic generation sg which integrates symbolic execution and local model explanation techniques to craft individual discriminatory instances.
sg relies on the local explanation of a given input which constructs a decision tree utilizing the samples generated randomly by the local interpretable model agnostic explanation lime .
the path of the tree determines all the important attributes leading to the prediction.
the algorithm also contains a global generation phase and a local generation phase.
a detailed comparison between adf and the above approaches are presented in section .
.
gradient based attacks.
this work is also related to research on gradient based adversarial attacks.
a variety of works have been proposed to explore the vulnerability of dnn by crafting adversarial samples.
gradient based adversarial attack is one kind of most effective method.
goodfellow et al.
proposed the first attacking algorithm fast gradient sign method fgsm to generate 958white box fairness testing through adversarial sampling icse may seoul republic of korea adversarial samples by perturbing the original input with the linearization of the loss function used in training process.
fgsm is fast by only attacking once according to the gradient.
later several other attack methods are proposed to extend fgsm.
for instance instead of attacking only once basic iterative method bim employs perturbations based on gradients multiple times often with smaller step sizes and applies a function which performs per attribute clipping to make sure the sample after each iteration is located in the neighborhood of the original sample.
papernot et al.introduced jacobian based saliency map attack jsma an iterative targeted attack method which attempts to force the dnn model to output the attacker desired label with minimal perturbation by utilizing the backward derivative gradient .
it works by first collecting the saliency map based on the jacobian matrix and selecting a pair of features which would cause the most significant change on the desired class followed by increasing the value of the selected two features to the maximum lastly repeat the above three steps until either the number of features perturbed exceeds the bound or achieving a successful adversarial sample.
besides pei et al.
designed an algorithm for maximizing the coverage of neurons as well as model outputs of multiple dnns and solve the optimization function using gradient.
conclusion in this paper we propose a lightweight algorithm adf to efficiently generate individual discriminatory instances for deep neural network through adversarial sampling.
our algorithm combines a global phase and a local phase to systematically search the input space for individual discriminatory instances with the guidance of gradient.
in the global generation adf first locates the individual discriminatory instances near the decision boundary by iteratively perturbing towards the decision boundary.
in the local generation adf again samples according to the gradient to search the neighborhood of a found individual discriminatory instance.
we compare adf with two state of the art fairness testing methods in benchmarks the results show that adf has significantly better performance both in terms of effectiveness and efficiency.