guigan learning to generate gui designs using generative adversarial networks tianming zhao jilin university changchun china zhaotm16 mails.jlu.edu.cnchunyang chen monash university melbourne australia chunyang.chen monash.eduy uanning liu jilin university changchun china lyn jlu.edu.cnxiaodong zhu jilin university changchun china zhuxd jlu.edu.cn abstract graphical user interface gui is ubiquitous in almost all modern desktop software mobile applications and online websites.
a good gui design is crucial to the success of the software in the market but designing a good gui which requires much innovation and creativity is difficult even to well traineddesigners.
besides the requirement of the rapid development of gui design also aggravates designers working load.
so the availability of various automated generated guis can help enhance the design personalization and specialization as they can cater to the taste of different designers.
t o assist designers we develop a model guigan to automatically generate gui designs.
different from conventional image generation models based on image pixels our guigan is to reuse gui components collected from existing mobile app guis for composing a new design thatis similar to natural language generation.
our guigan is based on seqgan by modeling the gui component style compatibility and gui structure.
the evaluation demonstrates that our model significantly outperforms the best of the baseline methods by .
in fr echet inception distance fid and .
in 1nearest neighbor accuracy nna .
through a pilot user study we provide initial evidence of the usefulness of our approach forgenerating acceptable brand new gui designs.
index t erms graphical user interface mobile application gui design deep learning generative adversarial network gan i. i ntroduction graphic user interface gui is ubiquitous in almost all modern desktop software mobile applications and online websites.
it provides a visual bridge between a softwareapplication and end users through which they can interact with each other.
a good gui design makes an application easy practical and efficient to use which significantly affects the success of the application and the loyalty of its users .
forexample computer users view apple s macintosh system ashaving a better gui than the windows system therefore their positive views almost double that of windows users leading to more brand loyalty .
good gui design is difficult and time consuming even for professional gui designers as the designing process mustfollow many design rules and principles such as fluent interactivity universal usability clear readability aestheticappearance and consistent styles .
to follow the fashion trend gui designers have to keep reviewing thelatest hottest mobile apps software or getting inspiration from corresponding author.design sharing sites e.g.
dribbble1 .
considering that each mobile app website software contains tens of different screensand their guis need to be updated iteratively due to the marketpressure designers have to take much innovation extensiveworking load.
unfortunately this design work often awaits just very few designers in a company and software developers have tofill in the gap.
in a survey of more than developers developers reported working on app gui design tasks more so than other development tasks which they had to perform every few days.
however software developers often do not have sufficient professional ui ux design training with art sense.
that is why it is challenging for developers to design the gui only from scratch.
instead when designing the gui for websites or mobile apps developers are very likely to search existing gui designs on the internet as the reference and further implement and customize the gui design fortheir own purposes .
this process usually happens at gui development in small start ups or small scale open sourcesoftware as there are not any professional ui ux designers.
although some studies help with the gui search by attribute filtering or parsing code structure of ui there are three problems with the gui search.
first there is a gapbetween the developers intention in mind and the output textual query and another gap between the textual queryand visual gui design.
due to the gap between the textual and visual information the retrieved gui may not satisfydevelopers requirements.
second the retrieved gui designmay be adopted by other developers resulting in the high similarity to other apps negatively influencing the uniquenessof the app.
directly using others gui may also involve potential intellectual property issues.
third the design styleof some retrieved guis may be out of date and it s hard for developers to keep track of the latest trend of the gui design.
a lot of times people don t know what they want until you show it to them.
steve jobs consequently an automated method for creative gui design generation is terribly needed to alleviate the burden of both novice designers and developers.
with the generated gui ieee acm 43rd international conference on software engineering icse .
ieee ...... gui subtree segmentation ...... ... generator discriminator information of design style and composition structure subtreeserialization input output components recombination gui dataset subtree repository gui generation system based on ganadversarial training generated gui fig.
.
overview of the proposed method.
design developers can further adopt the automated gui code generation for the automated.
in that way the overall gui development process will be significantly simplified.
in this work we develop a deep learning model guigan to automatically generate gui designs based on the existing gui screenshots collected from thousands of mobile apps.
it can provide designers and developers brand new gui designs and they can further customize the generated gui for their own purpose rather than starting from scratch.
although thereare plenty of image generation models like dcgan v aegan cyclegan and wgan they are all basedon plain pixels.
in contrast gui is composed of a set ofdetailed components e.g.
button text images and a good gui design is concerned more with the composition of thesecomponents rather than fine grained component pixels.
due tothe characteristic of gui and inspired by the natural language generation i.e.
selecting a list of words for composing one sentence we formulate our task as selecting a list of existing gui component subtree to compose new gui designs.
an overview of our approach is shown in fig .
first we collect gui screenshots and their corresponding meta information from android apps in categories fromgoogle play and decompose them into component sub trees for re using.
second we develop a seqgan basedmodel.
apart from the default generation and discrimination loss we model the gui component style compatibility and gui layout structure for guiding the training.
therefore ourguigan can generate brand new gui designs for designers inspiration.
the evaluation demonstrates that our model significantly outperforms the best of the baseline methods by .
in fr echet inception distance fid and .
in nearest neighbor accuracy nna .
through a pilot user study we provide the initial evidence of the usefulness of ourapproach for generating acceptable brand new guis.
our contributions in this work can be summarized as follow to the best of our knowledge this is the first study to automatically generate the mobile app gui design which requires much creativity and visual understanding.
we propose a novel deep learning based method to gen erate brand new gui designs composed of subtree sequences from the existing gui designs without additionalmanual presets.
the experimental results based on two specific development conditions show that our method can successfully capture gui design styles and structural features and toolbar textview button textview itemview textview textview layout textview textview textview layout textview textview textview ...layout textview textview textview ... fig.
.
real world data collection of gui subtrees.
automatically generate a new composite gui that con forms to the aesthetic of the consumers and standard gui structure.
ii.
p reliminary in this section we clear our goal and establish the corresponding task and then introduce a deep learning method that our work is based on.
a. task establishment different from the plain image which is made up of pixels one gui design image consists of two types of components i.e.
widgets e.g.
button image text and spatial layouts e.g.
linear layout relative layout .
the widgets leaf nodes areorganized by the layout intermedia nodes as the structuraltree for one gui design as seen in fig .
as most gui designers may re use some of their previous design components in their new design we take the subtree of existing guisas the basic unit for composing a new gui design rather thanplain pixels.
therefore we formulate our task as producing a sequence s t s1 ... s t ...s t st sof gui component subtrees where sis the subtree repository.
it can also be described as generating a new gui by selecting a list of compatible gui subtrees.
to obtain these candidate subtrees from screenshots of the guis we cut them from the original screenshot according tocertain rules.
given one gui design with detailed componentinformation we cut out all the first level subtrees from the original dom tree as seen in fig .
if the width of a subtree exceeds of the gui width we continue to cut it to ......... ... real world data sample structures list image image text image text text subtree embeddings style classification cnn generator lstm network ... samples subtree candidates ... discriminator cnn network layout list image image text text layout link real world structures ... hom med tree s tree r adversarial training fig.
.
the workflow of guigan.
the next level otherwise stop splitting and this subtree is used as the smallest granularity unit.
the procedure will be iterated until the segmentation stops.
finally all the smallest subtrees are given a unique number identification.
we remove the subtrees with duplicate bounds in one gui and keep only one in the process.
based on the collection and observationof the data from our pilot study we remove the subtrees withpartial overlap and preserve those with the aspect ratio between .
to which has a specific structure and can be clippedfrom the original gui screenshot.
b. base model our work is mainly based on the generative adversarial networks gan which consists of a generative network as the generator and a discriminative network as the discrim inator respectively.
the generator learns the features from the real data and generates new samples to fool the discriminator.
the discriminator tries to distinguish the true sample from the fake one.
these two networks are trained in an adversarial mode until the discriminator cannot distinguish the samples generated by the generator.
theguigan is proposed based on seqgan which is a variant of gan.
seqgan is the first work extending gans to generate sequences of discrete tokens.
it solves thecommon problems of traditional gan in dealing with discretedata such as sequences that is the generator is difficult to transfer gradient updates effectively and the discriminator isdifficult to evaluate incomplete sequences.
seqgan combinesthe gan and policy gradient algorithm of reinforcementlearning to guide the training of the generative model throughthe discriminative model.
seqgan uses a long short term memory lstm as the generator a cnn with a highwaystructure as the discriminator and a well trained oracle with the same architecture as the generator to generate samples as the ground truth.
the discriminator updates parametersby distinguishing between real samples and generated ones from the generator in the d step the step of training thediscriminator which belongs to a binary classification task.
the generator uses the monte carlo mc search rewardperformed by the discriminator in combination with the policygradient method to update its parameters in the g step the step of training the generator .
iii.
a pproach we propose a system called guigan that learns to synthesize brand new gui designs for designers by modeling gui component subtree sequences and style compatibility.the approach overview can be seen in fig based on subtrees automatically segmented from the original guis insection ii a we first convert all of them into embedding by modeling their style in section iii a. during the trainingprocess the generator randomly generates a sequence with thegiven length and the discriminator acts as the environment in which the reward can be calculated as the loss gby monte carlo tree search mcts .
we get the homogeneity value of the generated result as loss cin section iii b. by measuring the distance between the generated result and the original gui 750design the model captures the structural information in section iii c with loss scalculated by the minimum edit distance.
by integrating all the loss functions above in section iii d the parameters of the generator are updated with the back propagation algorithm.
a. style embedding of subtree since we are feeding the model with sub images showing gui component subtrees we first convert all of them to an embedding.
similar to natural language sentence the overallgui component layout tree can be regarded as a sentence andthe subtrees obtained from its metadata decomposition is the constituent words of this sentence.
we serialize the subtrees by depth first traversal and map them into embedding spaceto get their vector features as the input of our guigan.
thus we apply a deep learning network to get the feature vector and style embedding of the subtree sequences.
to transform the image from pixel level to vector level we adopt a siamese network to model the gui design with a dual channel cnn structure which maps agui into gui vector space.
we apply a pair of gui images g1 g2 as the input and the goal of the siamese network is to distinguish whether the two images are from the same app.
according to our observation the guis from one app ismore similar in design style than guis from different apps.therefore we set up the learning function to discriminate if two input design images are from one app or not to make the input embedding more meaningful i.e.
representing the design style.
the cnn in the siamese network takes one of the gui screenshot pair as the input.
then the convolution operation is executed with various filters m m matrix to extract the features in the guis.
a relu activation and a max pooling layer follow the convolution operation which canbe considered as a convolutional block and they are stacked repeatedly.
in the end the output of the last convolutional block which represents the embedding in the vector space ofthe guis is flattened into an fc fully connected layer.
the goal of the trained cnn in the siamese network is to convert the gui screenshot image gto n dimensional vector v g. this non linear transformation function fcan be expressed asvg f g where represents the trainable parameters in the network which can be updated by the back propagationalgorithm in the training process.
the weighted l1distance is applied to measure the two feature vectors v g1andvg2 from the two channels and then fed into a sigmoid activation function to calculate the predictive result.
since this task canbe formulated as a binary classification problem i.e.
two inputscreenshots from the same app or not we adopt the binary cross entropy loss function loss x y summationdisplay i xilog y i xi l o g yi wherexis the probability output of the network and yis the target or .the cnns of the different channels use the same weights and learn the ability to obtain the most representative informa tion features in guis which is used to quantitatively comparethe appearance design style similarity between gui images.the pixel information of the intercepted layout subtrees fromthe original gui is fed into the trained cnn and thus weacquire their design embeddings.
b. modeling subtree compatibility we apply the cnn in the siamese network trained in the previous section to help evaluate the aesthetic identity of the generated samples.
in each g step when the generatorgenerates a complete sequence s t which is composed of tsubtrees from different apps subtree repository spliced in order.
according to the metadata of their guis we can acquire the coordinates of each subtree and intercept their images from their original gui images.
then we input theminto the cnn from the siamese network trained before and output their embeddings.
using these embeddings we apply the homogeneity hom to evaluate the aesthetic compatibilityof subtrees in the sequence.
homogeneity hom is the proportion of clusters containing only members of a single class the class here represents the app and when the subtrees all come from the same app they get the highest harmony by h h g c h g where h g c is the conditional entropy of a class with a given cluster assignment and h g c summationtext g g summationtext c c 1ng c nlog ng c nc .h g represents the entropy of gandh g summationtext g g 1n g nlog ng n .nis the total number of samples ngandncbelong to class g and class crespectively and ng c is the number of samples divided from class gto classc.
we expect that the generator can keep learning to make the generated results have higher homogeneity scores which represents better coordination and compatibility.
therefore weintegrate the homogeneity score of the generated result into the training of the generator after the discriminator feeds back thereward value to a complete sequence from the generator.
thenwe calculate the style loss as loss c braceleftbigg exp h if c if c wherecis the number of the app where the subtrees come from.
if the subtrees of a sample are all from the same app loss cbecomes zero.
c. modeling subtree structure in addition to style information another factor to be considered is the structure information corresponding to the gen erated sequence.
the layouts of each actual gui have certaincomposition rules which make a gui not only more logical in appearance but also practical function.
we hope that whengenerating new subtree combination sequences the generator 751can also follow the composition conditions of gui to a certain extent so that these synthetic sequences not only stay diverse but also meet the structural characteristics of the real guis.
for this purpose we use the structure strings of the subtrees from their metadata instead of the gui wireframe images to represent their structures as there is explicit order amongdifferent gui components.
the minimum edit distance med is introduced to quantify the structural similarity between two guis.
the med can be used to evaluate the structuralsimilarity between the generated samples and the real worlddata.
by reducing the structural distance we can optimize the generator so that it learns the reasonable structure combination and order from the real guis which can be expressed as loss s max i j ifmin i j min levsr sg i j levsr sg i j levsr sg i j 1otherwise.
wheresrandsgrepresent the subtree structure strings of real and generated samples levsr sg i j is the distance between the first icharacters in srand the first jcharacters insg.
each character represents a gui component such as listview or framelayout.
d. multi loss fusion there is a large span in the numerical region of the three losses the feedback loss from the discriminator loss g loss c andloss s so that we need to normalize them for subsequent calculation.
by adding the trainable noise parameters the three loss values can be balanced to the same scale and we express the final fusion loss function as loss mul 1loss g 2loss c 3loss s in g step we update the parameters of the generator by minimizing loss mul and we apply adam update algorithm instead of stochastic gradient descent sgd for faster convergence.
arg min gloss mul the challenge of the model is to generate a new gui design with a reasonable structure and compatible style.
we don t want the model to only learn to generate sequences similar tothe real samples.
it promotes guigan to generate new gui designs with authenticity and diversity by fusing the structureand style information simultaneously to the original sequencefeatures.
as shown in fig two samples generated by the guigan are reconstructed by the pieces from the real gui.
iv .
i mplement a tion the data in our experiment comes from the rico opensource online data collection and a part of guis for this article is retained through manual screening.
our modelmainly consists of a seqgan includes an lstm and a cnn and a siamese network.
all networks are implemented on thepytorch platform and trained on a gpu.
a b fig.
.
two example guis generated by guigan with components from corresponding original guis.
a. dataset construction our data comes from rico an open source mobile app dataset for building data driven design applications.
rico isthe largest repository of mobile app designs to date supporting design search ui layout generation ui code generation etc.
rico was built by mining android apps at runtime via human powered and programmatic exploration.
workers spent2 hours using the downloaded apps from the google play store on the platform over five months producing userinteraction traces.
rico contains design and interaction data for uis from apps spanning categories.
based on our observation not all guis from rico datasets can be used in this study so we remove someof them.
first we remove the guis of game apps as gameapp guis is specifically generated by game engine which is different from other general apps.
second we manually remove some low quality guis including large pop up windows advertisements or posters occupying the whole screen size webpage loading page with a progress bar and real scenes incamera.
some examples can be seen in fig and we release all of our datasets at our online gallery .
a b c d fig.
.
examples of guis we removed from our dataset the gui from the game app a pop up window b large picture c waiting page d .
b. model implementation as we take the subtree of the existing guis as the basic unit for composing a new gui design in our paper.
a sample of the real world data is the combination of the subtrees from one single real world gui in order with both the screenshots and structure information based on the metafile.
it is mainly used as the training data for our gan based approach.
we first split those guis collected from real world android apps in google play into subtrees following procedures in section ii a. we then train a siamese network with this data to get subtreestyle embedding in section iii a. the subtree embedding is input to the generator of the guigan for modeling both the style loss loss c and structure loss loss s for generating new subtree sequences which can be used for composing new gui designs.
the generated subtree sequences then inputto the discriminator within the seqgan for training the discriminator to discriminate the real world gui and generatedgui.
after the adversarial training given random noise or prebuilt gui components the guigan can generate a new gui by composing several subtrees from real world guis.
the structure of the generator and the discriminator in seqgan is preserved in guigan which is implemented in pytorch.
we store the start and end subtrees of each realworld gui in the start list and end list respectively.
thelstm randomly takes a subtree in the start list as the initial matrix not the zero matrices in seqgan.
thus the generatorgenerates a sequence of length t the default sequence length is because the gui subtree length in real world data ismostly within .
from the beginning of the first subtree inthe start list if the total height of the later spliced subtree and all previous subtrees exceeds the rated height the subsequentsplicing will be stopped.
if the subtree in the end list is selected the splicing will be stopped directly.
the long short term memory lstm is used as the generative network.
the vector dimension is selected to be32 and the hidden layer feature dimension is selected to be32.
like the discriminative model in seqgan we also use a cnn network that joins the highway architecture.
the batch size is and the learning rate is set to .
.
the siamese network is a two channel cnns with sharedweights.
the positive example is a pair of subtrees from thesame app with a label of and the negative one from twodifferent apps with a label of .
the subtree image is resizedto512 by the nearest neighbor algorithm as the input.
there are conv pool layers blocks in the cnn structure.
the first conv layer use filters and each subsequent conv layer doubles the number of filers.
we set the filter size as10 and 4in different cnn layers and the stride as 1for convolutional layers.
we apply the pooling units of size 2applied with a stride .
we train the siamese network with epochs for about three hours.
v. a utoma ted ev alua tion in this section we prepare gui images collected according to the category and development company of the app as exper imental data based on the statistics and test the performance ofthe proposed model on gui generation.
we use the real world data as the ground truth and introduce wgan gp faceoff and two variations of our guigan as the baseline methods to compare the two metrics of fid and nna.
a. experimental dataset when preparing the experiment dataset we consider two specific usage scenarios when developing app guis.
first most designers and developers have a clear goal for developing the gui of an app in a specific category such as finance education news etc.
since each kind of app category owns its characteristics we try to test our model s capability in capturing that characteristic by preparing a separated dataset for the five most frequent app categories in rico dataset including news magazines books reference shopping communication and travel local as shown in table i. second we notice that designers and developers often refer to the gui design style developed by big companies whendeveloping their own gui.
therefore another experiment isto generate guis by learning gui designs from a specificcompany.
based on their metadata we prepare three kinds of guis from three big companies with most apps in our dataset i.e.
google yinzcam and raycom as shown in table i. t able i gui da t aset by ca tegory or comp any category or company app gui news magazines books reference 394shopping 460communication travel local google yinzcam 136raycom b. evaluation metrics in order to quantify and measure the similarity between the real data distribution prand the generated sample distribution 753pg we introduce fr echet inception distance fid and nearest neighbor accuracy nna as the evaluation metrics.
fid measures the diversity and quality of generatedimages relative to real images and nna is used to analyzethe distribution differences between the two sample sets.
fr echet inception distance fid is a widely used metric recently introduced for measuring the quality and diversity of generated images especially by gan.
fid iscalculated according to inception score is comparing thestatistics of negative samples to positive samples using the fr echet distance between two multivariate gaussians fid p r pg bardbl r g bardbl tr cr cg crcg where rand gare the mean values of the 2048dimensional activations of the inception v3 by asymmetrically decomposing the convolution operation the depth and width of the network pretrained on the imagenet are increasedsimultaneously and the network is widely used to reduce the dimension pool3 layer for real and generated samples randgrespectively while c randcgare the covariances.
a lower fid represents that the two distributions are closer which means that the quality and diversity of the generatedimages are higher.
in our experiments we input the samenumber of real and generated image collections into the inception v3 network to get the fid score.
nearest neighbor accuracy is used in two sample tests to assess whether two distributions are identical.
a well trained nearest neighbor classifier is applied.
the better the performance of the generated model the more difficult it is for the nn classifier to distinguish the true from the false.
therefore the best recognition rate is the worst is .
but if the recognition rate is lower than it means that the model may be overfitting.
this metric is widely used forevaluating the quality of generated images .
c. baseline models according to our query for relevant information there are very little researches on generating gui designs.
but there are some related works about image generation which is roughlysimilar to our task so we use two different kinds of methods as baselines with one from the image generation and the otherone using template search.
the first baseline is wgan gp which is proposed based on wgan .
wgan introduces wasserstein distance and optimizes the implementation process of the algorithm to solve the problem of gradient vanishing in the trainingprocess of traditional gan.
wgan gp introduces a gradientpenalty in wgan which accelerates the convergence of the model and has a more stable training process.
the second baseline is faceoff which parses the dom tree of a raw input website created by a user through measuring the distance of the trees and uses a cnn to learnstyle compatibility to find a similarly well designed web gui.
although faceoff is used to generate a new web gui in this article we modify its raw input to the real world data gui a b c d fig.
.
examples of the guis generated by guigan.
structure of the mobile application as a query and then sort the retrieved results according to the homogeneity score of their subtree combination.
apart from the two baselines mentioned above we also get some derivation baselines from our model by changing themuti loss in the generator.
one guigan style is using only theloss ccorrection and the other one guigan structure with only the loss scorrection within which the generator focuses on either design style or structure characteristics.
in this way we can compare and observe within our model howthese modifications affect the generated results in metric.
d. results table ii and iii show the results of different methods on two metrics in the category and company specific development scenarios.
the results show that our proposed model achievesthe highest scores on fid and nna under both the gui de velopment scenario.
our model achieves a .
and .
boost in fid and nna than the best baselines in the dataset of category and .
and .
increase in the datasetof the company respectively.
fig shows examples of gui images from guigan and for ease of observation we separate different subtrees with thick red lines.
it can be seen that theguis generated from guigan have a comfortable appearance and a reasonable structure composed of different components.
meanwhile it also keeps the overall harmonious design style.
after checking many generated guis we find that both thestructure and style of the guis are also very diverse which can provide developers or designers with different candidates for their gui design.
more generated guis from guigan can be seen in our online gallery .
two baselines including wgan gp and faceoff do not perform well compared with guigan in fid and nna.
according to our observation we can see the overall layoutof generated guis from wgan gp as seen in fig a but very blurred in detail.
that is because wgan gp is a pixelbased approach that cannot accurately model the informationof component based guis.
although it is widely used in the natural image it is not suitable for our artificial gui designimages especially considering the fact that there is not so 754t able ii performance by different app ca tegories categorywgan gp faceoff guigan guigan style guigan structure fid nna fid nna fid nna fid nna fid nna news magazines .
.
.
.
.
.
.
.
.
.
books reference .
.
.
.
.
.
.
.
.
.
shopping .
.
.
.
.
.
.
.
.
.
communication .
.
.
.
.
.
.
.
.
.
travel local .
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
t able iii performance by different app development comp anies companywgan gp faceoff guigan guigan style guigan structure fid nna fid nna fid nna fid nna fid nna google .
.
.
.
.
.
.
.
.
.
yinzcam .
.
.
.
.
.
.
.
.
.
raycom .
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
.
a b c d fig.
.
generated gui examples by wgan gp a and faceoff b c d .
much data in this study.
faceoff is much better than wgangp but there are still some issues with their approach.
first faceoff often chooses the subtrees with the highest back score to accelerate the convergence of the model and only compares the structural similarity between the real guis andthe retrieved template to minimize their distance resulting inthe diversity loss.
however it does not consider the relativeposition of each component especially the specific top downrelationship of the structure in the gui.
therefore most generated guis from faceoff are of very similar structurelike that in fig b and many guis are also of the same color schema as that in fig c and fig d .
the other two derived baselines of our approach guiganstyle and guigan structure explore the impact of style information and structure information on the generated results.
with only modeling the design style information guigan a b c d fig.
.
generated gui examples by guigan style a b and guiganstructure c d .
style can generate gui designs with harmonious color combinations as seen in fig a and b but without very goodstructural designs.
for example the menu tab appears in the middle of the gui in fig a and the login button appears at the top of the gui in fig b .
similar issues also applytoguigan structure with reasonable and diverse layouts of generated guis but terrible color schema as seen in fig c d .
the results from these two baselines demonstrate that thetwo loss function settings in section iii successfully capture the style and structure information.
although most of the samples generated by our model are satisfactory there are still some bad designs.
we manuallyobserve those bad gui designs and summarize some reasons.first due to the default size of components in the subtree some of them are difficult to fit into the generated guis asseen in fig a b .
there is either an overlap between some 755components or one figure taking all the gui space.
second since our model learns the style and structure informationat the same time there may be an imbalance between them for some gui generation.
fig c shows an example withtoo much emphasis on style consistency while ignoring the structural effects.
in contrast fig d has a set of diverse components in good structure but incompatible color schema.
a b c d fig.
.
examples of bad results generated by guigan.
vi.
h uman ev alua tion the target of this work is to automatically generate a list of gui designs for novice designers or developers to adopt.
theautomated experiments above demonstrate the performanceof our model compared with other baselines.
however the satisfactoriness of the gui design can be subjective depending on different users or developers.
to better evaluate the usefulness of guigan we conduct a user study to investigate the feedback from developers in this section.
a. evaluation metrics there are no existing evaluation metrics for mobile gui design in the literature.
but inspired by the web gui evaluation and image evaluation we propose three novel metrics for participants to rate the quality of the gui design from three aspects by considering the characteris tics of the mobile guis.
first design aesthetics is to evaluatethe overall design s pleasing qualities.
second we adopt color harmony which refers to the property that certainaesthetically pleasing color combinations have to evaluate the color schema selection within the gui.
these combinations create pleasing contrasts and consonances that are said to beharmonious.
third structure rationality is used to measure thecomponent layout rationality i.e.
the location of components in the gui and the logic of their combination and sorting.
for each metric the participants will give a score ranging from to with representing the least satisfactoriness while as the highest satisfactoriness.
besides to confirm if guigan implicitly considers the app functionality during the training we further ask participants to manually check if the componentdistribution of generated guis is functionally correct e.g.
themenu bar on the top of the page.
they will mark if the gui components are functionally correctly distributed while forincorrect ones.b.
procedures in real world app development teams often know their target very well.
to mimic that practice we select appcategories same in section v for specific gui generation.for each category we randomly generate gui designs foreach method.
due to the poor performance of wgan gp inthe last experiment we only take faceoff as the baseline.
we recruited five master students majoring in computer science.
they all have several year programming experienceand at least year android development experience mostly about gui implementation and some gui design.
therefore they can be regarded as junior android developers for evaluating whether they are satisfied with our gui design.
first we introduce them to a detailed explanation about the guievaluation metrics.
then they are provided with the generatedgui designs from different methods then give the score of each gui design in three metrics i.e.
design aesthetics colorharmony structure rationality.
note that they do not know which gui design is from which method and all of them willevaluate the gui design individually without any discussion.
after the experiment we tell the participants which gui designs are generated by our model and ask them to leavesome general comments about our guigan.
c. results as shown in table iv the generated gui designs from our model outperform that of faceoff significantly with .
.
and .
which are .
.
and .
increase in overall aesthetics color harmony and structure.
in additionto the average score our model is also better than faceoff in generating gui design for all five app categories in threemetrics.
that result also demonstrates the generalization ofourguigan.
according to the detailed analysis of the experiment result the gui designs with low scores tend to have incomplete structure single content large and abrupt pictures or advertisements.
in contrast the gui with high scoreshas a concise layout slightly rich content and background compatible images.
we also find that users requirements for content richness are much higher than other indicators butthis often goes against the simplicity of layout which needs further research and balance.
to understand the significance of the differences between the two approaches we carry out the mann whitney u test which is specifically designed for small samples only gui design in each category on three metrics.the test results in table iv suggests that our guigan can contribute significantly to the gui design in all three metricswithp value .
orp value .
except the aesthetics and color harmony metrics in the shopping category.
besides the comparison with the baseline we also present participants another dataset mixing with randomly se lected real world gui design images from our dataset and randomly selected gui designs generated by our model for checking the overall gui aesthetics color harmony andstructure.
some generated guis are even rated higher thanreal world ones.
in terms of fig a the five user study 756t able iv performance of human ev alua tion .
denotes p .
and denotes p .
category metricscore faceoff guigan news magazinesaesthetics .
.
harmony .
.
structure .
.
functionality .
.
books referenceaesthetics .
.
harmony .
.
structure .
.
functionality .
.
shoppingaesthetics .
.
harmony .
.
structure .
.
functionality .
.
communicationaesthetics .
.
harmony .
.
structure .
.
functionality .
.
travel localaesthetics .
.
harmony .
.
structure .
.
functionality .
.
a verageaesthetics .
.
harmony .
.
structure .
.
functionality .
.
participants rate the real gui with .
.
and .
in three metrics aesthetics harmony structure on average while .
.
for the generated one.
for fig b they score the real gui with .
.
and and the generated gui with .
.
and which is not that high.
there are several reasonswhy our generated guis get higher scores than some real world guis some generated guis examples in fig are of higher quality than some poorly designed real worldguis.
and note that the score is just .
to .
higher than some real world guis.
there may be the human bias of different raters as different people are of different aesthetic values.
different human raters may also adopt slightly different criteria when rating the gui quality as people vary.
wemitigated those potential bias by not telling them which gui is generated by our model or from real world apps.
the results of functionality can be seen in table iv with the average score as .
which is .
significantlyhigher than that of the baseline.
it shows that the componentdistribution of most of our generated guis is functionally correct.
it also indicates our model implicitly get that appfunctionality during training on a large scale dataset as the gui design is highly related to app functionality though our model does not explicitly consider app functionality.
addi tionally we find that the performance of our model differs indifferent app categories.
guigan achieves an average score of a shopping b travel local fig.
.
in each pair the first image is real world gui while the second one is generated by guigan.
.
in the travel local category but only .
in the books reference category which may be because the former one is of more functional characteristics than the latter one.
after the experiment we tell participants which guis are generated from our tool and get some feedback from them.
they suggest that the padding between components shouldbe adjusted to increase the attractiveness of the gui designs.they also request to add new features that allow manualsettings of result diversity such as a comparison betweencombinations style structure etc.
vii.
r ela ted work gui is crucial for the user experience of modern desktop software mobile applications and online websites.
in thissection we introduce related works about gui design andgui generation.
a. gui design gui design is an important step in gui development.
therefore many researchers are working on assisting designers in the gui design such as investigating the ui design patterns color evolution ui related users review gui code generation website gui generation etc.
liu et al.
follow the design rulesfrom material design to annotate the mobile gui design to represent its semantics.
swearngin et al.
adopt the image processing method to help designs with converting the mobile ui screenshots into editable files in photoshop so that designers can take it as a starting point for further customiza tion.
to render inspirations to the designer chen et al.
propose a program analysis method to efficiently generate the storyboard with ui screenshots given one app executable file.
fischer et al.
transfer the style from fine art to gui.chen et al.
study different gui element detection methodson large scale gui data and develop uied to handlediverse and complicated gui images.
other supporting workssuch as gui tag prediction and gui component gallery construction can enhance designers searching efficiency.
all of these works are targeting at simplifying the design process for professional designers.
in contrast our method is focusing on the initial stage of gui design i.e.
generating diverse gui designs for giving inspirations to novice designers and developers who are of not much gui design training.
757through the gan method in deep learning our model learns the design style and structural characteristics of existing guis to generate diversified new gui for designers reference tolower the gui design barrier.
b. gui generation thanks to the rapid development of deep learning the image generation performance is further improved especially by the generative adversarial network gan and its deviationmodels.
apart from the natural image generation there are also many works on re arranging elements for composing graphic designs with better layouts especially semantic layouts .
sandhaus et al.
present an approach for the automatic layout of photo compositions that incorporates the knowledgeabout aesthetic design principles.
y ang et al.
analyze lowlevel image features and apply high level aesthetic designing principles and predefined templates to the given images and texts thus automatically suggest the optimal template textlocations and colors.
v empati et al.
utilize the maskr cnn object detector to automatically annotate the required objects tags and a genetic algorithm method to generate an optimal advertisement layout for the given image content input components and other design constraints.
a rankingmodel is trained on historical banners to rank the generated captivities by predicting their click through rate ctr .
li et al.
introduce a progressive generative model of image extrapolation with three stages and two important sub tasks.
in the field of text to image synthesis hinz et al.
introduce semantic object accuracy soa to evaluate images given an image caption.
layoutgan is proposed by li etal.
for graphic design and scene generation introducingwireframe rendering for image discrimination.
the generator takes as input a set of vectors and uses self attention modulesto refine their labels and geometric parameters jointly.
jyothi et al.
propose a variational autoencoder based method calledlayoutv ae which allows for generating full image layoutsgiven a label set or per label layouts for an existing image given a new label and has the capability of detecting unusuallayouts.
some works are generating gui test case for checking gui usability accessibility and security .
unlike these works in generating the layout of graphic design like posters advertisements we are the first to work ongui design generation.
different from their tasks in arrangingthe given components our task is more challenging i.e.
selecting components from a repository and compose them into a great gui design by taking the design style and structureinformation into the consideration.
therefore we develop anovel approach for modeling that information.
viii.
c onclusion designing a good gui which requires much innovation and creativity is difficult even for well trained designers.
in this paper we propose a gan based gui design generation method guigan which can assist novice designers and developers by generating new guis by learning the existing appgui screenshots.
the generated gui design can be regardedas the starting point or inspiration for their design work.
we decomposed the filtered guis to form our large scale subtreeretrieval repository then feed these subtrees to our modelfor generating reasonable one dimension sequences which arefurther used for reorganization.
two additional corrections are added to the generator of our model to improve the model in the aspect of design style and structural composition.
theautomated experiments demonstrate the performance of ourmodel and the user study confirms the usefulness of guigan.
to improve the generated guis we will adopt two ways.
first we will improve our model to generate guis with higherquality.
we can summarize a list of issues of the current modelby carrying out a detailed analysis of a bad gui generation of current data.
we then improve our model accordingly and also add a list of rules to post process generated guis e.g.
menubar should be on the top of the gui etc.
second we will buildan ai human collaboration system i.e.
the generated guis in our model are only used for inspiring developers designers.
designers or developers can further select or customize guis according to their purpose.
besides our model can be used to convert designers developers partial gui design to the full one via our model s leveraging the pre built gui components and control though it can be further improved.
some examplescan be seen at our online gallery .
we also plan to combine the current gui code generation works with our gui design generation to fully automate gui development.
ix.
a cknowledgements this work is supported in part by the national natural science foundation of china grant and chunyang chen is partially supported by facebook research award.