understanding neural code intelligence through program simplification md rafiqul islam rabin mrabin uh.edu university of houston houston tx usavincent j. hellendoorn vhellendoorn cmu.edu carnegie mellon university pittsburgh pa usamohammad amin alipour maalipou central.uh.edu university of houston houston tx usa abstract a wide range of code intelligence ci tools powered by deep neural networks have been developed recently to improve programming productivity and perform program analysis.
to reliably use such tools developers often need to reason about the behavior of the underlying models and the factors that affect them.
this is especially challenging for tools backed by deep neural networks.
various methods have tried to reduce this opacity in the vein of transparent interpretable ai .
however these approaches are often specific to a particular set of network architectures even requiring access to the network s parameters.
this makes them difficult to use for the average programmer which hinders the reliable adoption of neural ci systems.
in this paper we propose a simple modelagnostic approach to identify critical input features for models in ci systems by drawing on software debugging research specifically delta debugging.
our approach sivand uses simplification techniques that reduce the size of input programs of a ci model while preserving the predictions of the model.
we show that this approach yields remarkably small outputs and is broadly applicable across many model architectures and problem domains.
we find that the models in our experiments often rely heavily on just a few syntactic features in input programs.
we believe that sivand s extracted features may help understand neural ci systems predictions and learned behavior.
ccs concepts software and its engineering software testing and debugging computing methodologies learning latent representations .
keywords models of code interpretable ai program simplification acm reference format md rafiqul islam rabin vincent j. hellendoorn and mohammad amin alipour.
.
understanding neural code intelligence through program simplification.
in proceedings of the 29th acm joint european software permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction deep learning has increasingly found its use in state of the art tools for code intelligence ci thriving in defect detection code completion type annotation prediction and many more .
this growing body of work benefits from the remarkable generality of deep learning models they are universal function approximators i.e.
they can represent any function not just linear ones.
in practice deep neural network methods appear to learn rich representations of raw data through a series of transforming layers .
this substantially reduces the burden of feature engineering in complex domains including vision natural languages and now software.
given this capacity learned models seem capable of discovering many non trivial properties about the source code even ones that are beyond the reach of traditional sound static analyzers.
as such they can uncover bugs in code that is not syntactically invalid but rather unnatural .
although this may be reminiscent of a software developer s ability to intuit properties about programs there is a sharp contrast in interpretability developers can explain their deductions and formulate falsifiable hypotheses about the behavior of their code.
deep neural models offer no such capability.
rather they remain stubbornly opaque black boxes even after years of research on interpreting their behavior .
this opacity is already a concern in non critical applications where the lack of explainability frustrates efforts to build useful tools.
it is substantially more problematic in safety and securitycritical applications where deep learners could play a key role in preventing defects and adversarial attacks that are hard to detect for traditional analyzers.
at present deep neural models can change their predictions based on seemingly insignificant changes even semantic preserving ones and fail to provide any traceability between those predictions and the code.
in this paper we propose a simple yet effective methodology to better analyze the input over reliance of neural models in software engineering applications.
our approach is model agnostic rather than study the network itself our approach relies on the input reductions using delta debugging .
the main insight is that by removing irrelevant parts to a prediction from the input programs we may better understand important features in the model inference consequently.
given a prediction correct or not we show that model inputs can often be reduced tremendously even by or more while returning the same prediction.
importantly our work is the first toarxiv .03353v2 sep 2021esec fse august athens greece md rafiqul islam rabin vincent j. hellendoorn and mohammad amin alipour original override public void oncreate bundle savedinstancestate super .
oncreate savedinstancestate setcontentview r. layout .
fragmentdetails minimized d oncreate u ve s. oncreate figure example of an original and minimized method in which the target is to predict oncreate as the method name.
the minimized example clearly shows that the model i.e.
code2vec has learned to take short cuts in this case looking for the name in the function s body.
show that these reductions in no way require the inputs to remain natural or depending on the task in any way valid.
this allows us to generate significantly simpler explanations than related work .
our results hold on four neural architectures across two popular tasks for neural ci systems code summarization using code2vec and code2seq and variable misuse detection with rnns and transformers .
we show that our minimal programs are related to but not fully explained by the attention in models that use it.
overall our findings suggest that current models when trained on a single task such as ours have very little care for the readability of functions as a whole rather they overwhelmingly focus on very small simple syntactic patterns that provide salient clues to the required output.
motivating example we first present an example to illustrate our approach and the insights that it can provide about the prediction of neural code intelligence models.
figure shows a code snippet from the methodname dataset.
here the goal is to infer the method name from the method body.
the code2vec model predicts the method name oncreate which is indeed correct.
unfortunately it is hardly clear why code2vec considers dozens to hundreds of paths in the methods ast abstract syntax tree and struggles to identify which ones are most informative indeed in our results we find its builtin attention based explanations to only poorly correlate with essential tokens.
of course a developer looking at this program could provide several explanations why this is intuitively correct.
for one the method invocation super .oncreate strongly suggests that this method overrides a method with the same name in the parent class as such calls are usually made to the overridden method.
this guess does not need to be informed by just that call if the developer is familiar with android development they might recognize this code as working with android apis and perhaps even know that the activity1 in android systems provides oncreate method to initialize a user interface in which developers can place their ui objects on the interface window using the setcontentview method.
turning back to the code2vec model it does not offer any of these explanations but rather an output based on a complex mixture its inputs and millions of parameters.
this lack of transparency is problematic for practitioners and researchers alike who are unlikely to accept recommendations without evidence especially in cases that are not so trivial.
for instance does code2vec note and use the .oncreate invocation?
if so does it specifically leverage the inheritance relation?
and or does it use any of the more android specific reasoning about the presence of the setcontentview invocation?
our approach sivand can better provide compelling answers to these questions.
the second half of figure shows the smallest possible version of this program that is still syntactically valid a necessity for code2vec and yields the same prediction.
any evidence of this method relating to android development has been thoroughly scrubbed all that remains is the single oncreate method call even the mention of super has been minified to a single character s .
table shows some steps that yielded this reduced program in greater detail.
sivand works by iteratively reducing the size of the input program by the code2vec model while preserving the prediction output at every step.
we continue this reduction until the program is either fully reduced to its minimal components depending on the task or any further reduction would corrupt the prediction.
each row in this table shows the intermediate most reduced program that still yields the same prediction as the original input program in the first row as well as the probability score of that prediction for reference.
a close examination of the intermediate results suggests that the savedinstancestate parameter and the setcontentview call indicative of android development is reduced with almost no penalty to the score.
other such cues follow soon after.
in step the super call which represents cues of inheritance is similarly pruned and here too with virtually no penalty to the score.
the subsequent steps largely serve to remove a few miscellaneous characters these compromise the prediction score somewhat perhaps because the method is increasingly irregular.
regardless as long as oncreate is present the score remains high ample to sustain the prediction.
evidently even if the model noticed the android related features it certainly did not need them nor even the mention of super all that remains is the presence of another method call oncreate .
that sivand can elucidate these insights is a mixed blessing and curse its model agnostic approach effectively lets us bypass interpreting the millions of parameters and complex inner workings of the studied models and yields remarkably short programs that make the original predictions easier to comprehend.
the use of delta debugging is a novel approach to such model interpretation in general and its intermediate steps evidently provide useful insights into the model s thinking .
at the same time our approach is a notable departure from more common neural attribution methods e.g.
which typically try to find the part of the input that was most informative but do not necessarily assume that all other parts could be removed entirely.
sivand on the other hand frequently corrupts its input programs almost beyond recognition.
that the assessed models continue to perform so well during this process section across multiple code intelligence tasks is indicative of these models overreliance on small features and lack of holistic interpretation of their input programs.
we discuss various interpretations and implications of this phenomenon in section .understanding neural code intelligence through program simplification esec fse august athens greece table reduction of a program while preserving the predicted method name oncreate by the code2vec model.
step s score code .
override public void oncreate bundle savedinstancestate super .
oncreate savedinstancestate setcontentview r. layout .
fragmentdetails ... .0void oncreate bundle savcestate super .
oncreate sncestate setcontentview r. layout .
fragmentdetails .
void oncreate bundle savcestate super .
oncreate sncestate setcontentview s .
void oncreate bu savate s. oncreate sncestate setcontentview s .
void oncreate bu savate s. oncreate snte view s .
void oncreate bu savate s. oncreate snte s .
id oncreate bu save s. oncreate snte .
d oncreate u ve s. oncreate e ... .
d oncreate u ve s. oncreate 3sivand methodology this section describes sivand a methodology for better understanding and evaluating ci models.
this methodology is model agnostic and can be used for any ci model however in this paper we will focus on the ci models that use neural networks for training as their lack of transparency warrants more attention.
.
notation we usepoto denote the original input to the neural ci model and prto denote the simplified input.
let mbe an arbitrary ci model andpan arbitrary input program then prediction m p denotes the prediction result of mgivenp and p the resultant size of p. conceptually we define size as the number of atomic parts that p has.
in the evaluation of this approach we use token as the atomic part hence p denotes the number of tokens that are returned by the lexer of the language.
the high level goal of sivand is to produce a reduced input programprthat is smaller than the size of po i.e.
pr po and ideally pr po such that if prediction m po r prstill retains the same prediction r prediction m p r r. while such a reduction process can in principle terminate under many kinds of conditions and even return the original input program we are interested in finding the so called minimal input program where no single part of prcan be removed without losing some desired property of po.
.
workflow in sivand figure depicts a high level view of the workflow in the proposed methodology i.e.sivand .
given a input program sivand uses the dd module to reduce the size of the program.
the dd module uses delta debugging to produce various candidate programs by removing various parts of the original input program and iteratively searches for the minimal input program that produces thesame prediction as the original input program.
some of the generated candidates might be invalid programs that is they do not follow the syntax of the programming language that the program is written in.
therefore since some ci models require inputs to be syntactically valid to enhance the efficiency sivand filters out the invalid candidates that do not parse only for those cis.
.
reduction algorithm algorithm high level algorithm for ddmin delta debugging.
the algorithm is initiated by ddmin m po .
input m ci model p input program and n number of partitions.
splitpintonpartitions to build 1 ... n if isuch thatprediction m po prediction m i then ddmin m i else if is.t.prediction m po prediction m p i then ddmin m p i max n else ifn p then ddmin m p min 2n p else returnp end if algorithm describes the delta debugging algorithm proposed by zeller and hildebrandt and later extended by groce et al.
that is adapted to our task for finding minimal input programs.
at a high level the delta debugging algorithm iteratively splits a input program into multiple candidate programs by removing parts of the input program.
it uses nto specify the granularity of parts.
that is for an input program pand granularity level n itesec fse august athens greece md rafiqul islam rabin vincent j. hellendoorn and mohammad amin alipour figure workflow of sivand .
generates 2ncandidates ncandidates by splitting pintonpartitions 1 ... n and another ncandidates by computing p 1 ... p n. at each of these steps the algorithm checks if any resulting candidate program psatisfies the desired property which here means preserving the prediction of the model on the original input program i.e.
prediction m po prediction m wherepo denotes the original input program.
when the algorithm finds a candidate satisfying the property it uses the candidate as the new basepto be reduced further otherwise the algorithm increases the granularity i.e.
nfor splitting until the algorithm determines that thepis minimal.
the time complexity of the delta debugging algorithm is quadratic i.e.
o n2 in the size of the input program.
experimental settings our proposed methodology is task and program agnostic.
we assess these properties by studying two tasks and two models on each of these.
this section outlines all of these.
.
subject tasks we study two popular code intelligence tasks that have gained interest recently method name prediction methodname and variable misuse detection varmisuse .
.
.
methodname .in the method name prediction task the model attempts to predict the name of a method given its body.
we study two commonly used and similar neural approaches for this task code2vec and code2seq .
both these approaches rely on extracting paths in the method s ast that connect one terminal token to another which are mapped to vector embeddings.
these paths are enumerated exhaustively and used by the two models in various ways.
the premise is that these paths consolidate both lexical and syntactic information thus providing more benefit than strictly token based models such as rnns.
incode2vec each path along with its source and destination terminals is mapped into a vector embedding that is learned jointly with other network parameters during training.
the separate vectors of each path context are then concatenated to a single context vector using a fully connected layer.
an attention vector is also learned with the network which is used to aggregate the path context representations into a single code vector representing a method body.
the model then predicts the probability of each target method name given the code vector of the method body via a softmax normalization between the code vector and each of the embeddings of a large vocabulary of possible method names.thecode2seq model uses an encoder decoder architecture to encode paths node by node and generate labels as sequences at each step.
in code2seq the encoder represents a method body as a set of paths in ast where individual paths are compressed to a fixed length vector using a bi directional lstm which encodes paths node by node with splitting tokens into sub tokens.
the decoder again uses attention to select relevant paths while decoding and predicts sub tokens of a target sequence at each step when generating the method name.
.
.
varmisuse .a variable misuse bug occurs when the intended variable used at a particular location of the program differs from the actual variable used.
these mistakes are commonly found in real software development and naturally occur as copy paste bugs .
we specifically experiment with the joint bug localization and repair task of varmisuse given a tokenized sample buggy or correct the task is to predict two pointers into these tokens a localization one pointer to indicate the location of the token that contains the wrong variable or some default token if no bug exists and b repair another pointer to indicate the location of any token that contains the correct variable ignored for bug free samples .
we specifically use the dataset released by hellendoorn et al.
whose repository also includes a number of models that be applied directly to this dataset.2from these we use the following two generic neural approaches rnn and transformer .
the rnn model here is a simple bi directional recurrent architecture that uses gru as the recurrent cell and has layers and hidden dimensions.
the transformer model is based entirely on attention in which the representations of a snippet s tokens are repeatedly improved through combination with those of all others in the functions.
we use the parameters from the original transformer with layers heads and attention dimensions.
.
datasets and models table summarizes the performance characteristics of the ci models that we use in our experiments which is on par with the ones reported in the original studies.
for the methodname task we use the java large dataset3 to train both the code2vec andcode2seq models.
this dataset contains a total of 500java projects from github partitioned into 000projects as training data 200projects as validation data and 300projects to test on.
overall it contains about 16m methods.
neural code intelligence through program simplification esec fse august athens greece table characteristics of the trained models.
model dataset precision recall f1 score code2vec java large .
.
.
code2seq java large .
.
.
model datasetlocalization accuracyrepair accuracyjoint accuracy rnn py150 .
.
.
transformer py150 .
.
.
for the varmisuse task we use the py150 corpus4to train both thernn andtransformer models.
this dataset contains functions from a total of 150k python files from github and are already partitioned into 90k files as training set 10k files as validation set and 50k files as testing set.
each function is included both as a bug free sample and with up to three synthetically introduced bugs yielding about million samples in total.
.
sample inputs to evaluate the effectiveness of sivand we choose both correctly and incorrectly predicted samples from the test set of these datasets.
a correctly predicted samples formethodname task we choose correctly predicted samples for token level reduction where the smallest unit of reduction is a token and around correctly predicted samples for character level reduction where the smallest unit is a character.
running character based reduction was slow as the search space of possible reductions is much larger hence the lower total.
we use the same randomly selected samples for both code2vec andcode2seq models.
forvarmisuse task we choose correctly predicted samples from buggy samples and bug free ones.
for the selected buggy samples models correctly predicted both the location pointer and repair targets.
for the selected non buggy samples models correctly identify as no bug by prediction special index.
we use the same randomly selected samples for both rnn andtransformer models thus ensuring that we selected only samples on which their predictions were both correct.
b wrongly predicted samples formethodname task we choose around samples where the predicted method name is wrong.
we use token level reduction only and randomly select different wrong samples for code2vec andcode2seq separately.
forvarmisuse task we choose buggy samples where the predicted location pointer is wrong but models correctly predict the repair targets.
we randomly select different wrong samples for rnn andtransformer separately.
.
metrics here we define the metrics that we measure in the experiments as follows.
we use size reduction ratio of input programs to evaluate the effectiveness of sivand in reducing the size of the original input figure initial vs. final size of reduced programs in our dataset measured in tokens.
note the log scaling on both axes.
programs.
for the input program po and its reduced counterpart pr size reduction ratio or reduction is calculated as po pr po .
all neural network models in our work are capable of making probabilistic predictions in which a probability distribution is inferred over either tokens for varmisuse or method name parts formethodname .
we can leverage these prediction probabilities to compute a prediction score that indicates the confidence of the model in a particular prediction.
when we reduce the inputs we track changes to these scores on the original targets the reduction is stopped once the model ceases to predict the correct output which generally comes with a drop in the score probability of that target.
to assess whether our models lose certainty during this reduction phase we study the changes in the distribution of these scores relative to program reductions.
hardware.
we used a server with an intel r xeon r .30ghz cpu and a single nvidia tesla p100 gpu with 12gb of memory to run the experiments in this study.
artifacts .the code and data of sivand are publicly available at results in this section we present the results of our experiments where we seek to answer the following research questions.
rq1 how much can typical input programs be reduced?
rq2 what factors influence reduction potential?
rq3 do reduced programs preserve the tokens most used by attention mechanisms?
rq4 what is the cost of running sivand ?
note that for brevity in this section unless otherwise noted the results belong to token level reduction of correct predictions and invarmisuse models buggy programs.
.
rq1 length reduction the goal of sivand is to reduce the size of programs as much as possible to help uncover features that highly impact the prediction.
figure shows its capacity to do so in terms of the size of the original programs versus the reduced programs measured in tokens.
this plot aggregates the results of such reductionsesec fse august athens greece md rafiqul islam rabin vincent j. hellendoorn and mohammad amin alipour figure percentage of tokens reduced vs. initial program size in the various models studied.
note the log scaling on the x axis.
in which each program is reduced until just before its prediction changes.
in this and similar figures we show the loess trend to amortize that large volume of data points also shown with low opacity .
the confidence intervals on these trends are generally very tight and mostly omitted except where informative.
we find this relation to be mostly linear across all our datasets and models with the final program containing to times fewer tokens than the initial one.
figure provides an alternative view showing the achieved reduction as a percentage of the input program size evidently program reduction is somewhat easier on larger programs perhaps because those are more redundant.
these figures alone suggest that the sivand can reduce a large portion of the input programs but we found that the true reduction is more substantial in both datasets the maximum possible reduction is limited in the varmisuse case by the need to preserve all variable occurrences which are error and repair targets and in the methodname case by requiring the program to be syntactically valid which requires keeping at least some syntactic tokens .
the true minimum is less tractable for the latter dataset but when we subtract the irreducible portion from the former we found the relation shown in figure .
interestingly for most programs this trend was nearly constant especially for the rnn which on average preserves just a few extra tokens.
the transformer meanwhile appears to be especially reliant on additional syntax on larger programs although still just a fraction of the original some of non essential tokens .
in other words both models require nearly no additional syntax than just the variable names to make their predictions.
the reduction columns in table provides details of the ratio of reduced tokens to the number of maximum allowed tokens in the models.
it shows that our sivand could remove more than of tokens from each input programs on average it removes .
.
.
and .
of the maximum allowed tokens of the original inputs in code2vec code2seq rnn and transformer models respectively.
also in some cases sivand was able to remove allor almost all of the maximum allowed tokens and isolate the important features.
this substantial reduction allows to better understand and pinpoint important features in the prediction of the models.
figure tokens remaining relative to the minimum possible reduction in the varmisuse task.
note the log scaling on both axes.
observation sivand can reduce the input programs significantly on average more than inmethodname models and more than invarmisuse models.
different neural architectures show slightly different behaviors with transformer permitting less reduction than rnn .
.
rq2 factors impacting reduction we next study a range of factors that impact and are impacted by our program reduction approach to better understand its effect.
impact on prediction score figure tracks the final prediction score against the fraction of the program that was reduced with inter quartile ranges shaded buggy samples only for varmisuse .
recall that the reduction process halts when any further reduction would change its prediction so it is expected that scores remain modestly high although not necessarily above .
the code2vec andcode2seq models and to a lesser extent the transformer model display pronounced tipping point behavior in which the final reduction step still preserved relatively high probabilities while the immediate next step would have to drop at least below .
note that most samples started out with a score of almost exactly regardless of models thus the difference between the initial and final score is not especially informative.
the degree of reduction does not appear to impact the prediction scores much only for the rnn model on the varmisuse task do we see a slight downwards trend among input programs that could not be reduced by much usually smaller ones.
we found similarly little correlation with the input and final reduced program sizes.
this further reinforces that just one or a few tokens make the difference between a confident prediction and a misprediction.
character vs. token level on the methodname dataset we additionally studied character level delta debugging besides the tokenlevel default .
this has the potential to reduce inputs beyond what is possible with token level models and figure confirms that it frequently does this based approach is able to remove another of the characters in the input program thus yielding shorter and potentially more informative reductions such as the one in figure .understanding neural code intelligence through program simplification esec fse august athens greece figure the final score of maximally reduced programs immediately before changing predictions vs. the degree of reduction.
figure character vs. token based model reduction potential on the methodname task both compared in terms of total characters reduced.
note the log scaling on the x axis.
incorrect predictions finally we analyzed whether models could reduce evidence for incorrect predictions as well so far all our analysis focused on correctly predicted samples but of course in practice all these models regularly make mistakes.
we can extract the reduced programs for these mispredictions in much the same way by setting the goal for sivand to preserve the incorrect target while reducing the program.
here we found basically no effect the correct and incorrect predictions could on average across all models be reduced by .
and .
respectively.
similarly this did not seem to correlate much with the initial program size for any of our models.
thus it appears that the models do not differentiate in terms of the evidence required to mispredict correct or not the models base their prediction on small parts of the input program.
.
rq3 similarity between attention reduction attention mechanism in neural networks attempts to capture the important features in the input and directly connect them to the output layer to increase the impact in the prediction and improve the performance of neural networks and accelerate training.
intuitively attention captures the important features in the input programs therefore the number of features shared between sivand and attention can indicate the effectiveness of the approach for retaining important features.
figure percentage of common tokens between attention and reduced input programs in transformer for buggy inputs.
to evaluate the similarity between tokens in reductions and attention we apply the sivand to reduce the input program poto pr.
lettdddenotes the set of tokens in pr andkis the length of tdd.
to evaluate how much the set of features retained by sivand overlap with the features used in the attention mechanism for each po we then collect the set of tokens tattn in the original input programs that receives high attention score.
for a fair comparison we select almost the same size of attention tokens as in the reduced program.
from transformer model we get the attention score for each token hence we select top ktokens astattn.
however for the code2vec andcode2seq models we get an attention score for each path between two terminal nodes in the program s ast.
therefore we instead repeatedly choose high attention paths and collect all nodes until we found kdistinct nodes.
finally we calculate the similarity between the tokens in the reduced input program and the attention mechanism by computing tattn tdd tdd .
figures and show the similarity between the tokens in the attention and reduction in transformer model.
the results suggest that in the majority in more than cases of the non buggy input programstattn andtddfully match and in buggy input programs there is a large overlap majority over match.
according to the column common in table we can see that the code2vec and code2seq also have an average overlap over and respectively.
note that rnn in this experiment does not use attention.
observation on average a large portion of tokens between and are shared between attention tokens and tokens retained by the reduction.
.
rq4 cost of reduction column time in table shows the average time spent on reduction of the input programs.
the average time for reduction of the input programs in varmisuse task was lower than four seconds while the average time needed for reducing an input program in the methodname task was around 60and 107seconds in code2seq andcode2vec models respectively.
figure shows the detailed cost of running sivand plotting the log scaled reduction time inesec fse august athens greece md rafiqul islam rabin vincent j. hellendoorn and mohammad amin alipour table summary of token reduction results on the correctly predicted samples.
task model sample tokens reduction common dd pass average time second min avg max min avg max min avg max total valid correct min avg max methodnamecode2vec100019 .
.
.
.
.
.
.
.
.
.
.
.
.
code2seq .
.
.
.
.
.
.
.
.
.
.
.
.
varmisusernn200013 .
.
.
.
.
.
.
.
.
transformer .
.
.
.
.
.
.
.
.
.
.
figure percentage of common tokens between attention and reduced input programs in transformer for nonbuggy inputs.
figure the reduction time in seconds vs. the number of tokens removed correlate roughly linearly albeit with a substantial overhead on the methodname task due to its parsing requirement.
note the log scaling on both axes.
seconds versus the log scaled total number of tokens reduced on the correctly predicted samples for all models.
in our experiments the models for the methodname task code2vec andcode2seq require a parseable program for making a prediction while the models for the varmisuse task rnn andtransformer can work with any sequence of tokens.
therefore for the methodname task after each step of delta debugging we first create the candidate program from tokens then parse this candidate program to check whether it is valid and finally preprocess it to make a prediction by a model.
this adds significant overhead to sivand s runtime compared to thevarmisuse task.
overall most reductions in methodname finished in less than four minutes while the reductions in varmisusegenerally completed in less than ten seconds.
this aside allmodels scale roughly linearly in reduction time with the number of tokens that are eventually removed which itself correlates strongly with initial size figure .
the individual models within each taskcategory were quite similar in terms of throughput with a minor effect observed based on each model s performance e.g.
rnns are slower than transformers .
the columns under dd pass show that the search for minimal inputs in methodname included creating and trying slightly more than 350intermediate results in the delta debugging wherein on average around 37of them were syntactically correct and produced the same prediction as to the original input program.
similarly in models related to varmisuse the reduction takes between 237and 241on average from which around 30intermediate reduced programs produce predictions similar to the original input program.
observation input program reduction based on tokens is efficient and can reach the minified input with the same prediction in a relatively small number of steps.
discussion the central motivation for using linguistics inspired models on software has long been that source code is in a sense natural that is it contains repetitive predictable patterns much like natural text .
models that leverage this intuition have evolved from simplen gram based models to complex neural networks such as those used in this paper in recent years and become applicable to and successful at a wide range of coding tasks.
yet they are still built and motivated with the same core premise of learning from programs in their entirety punctuation and all and in lexical order.
recent results have already begun to suggest that in practice these models may not be using much of their input programs for at least one task however this still focuses on otherwise natural only simplified programs.
our method makes no attempt at preserving any meaning or validity of the original program we purely focus on the smallest amount of data with which our models could suffice.
this allows us to show a new stronger result these four models across two tasks appear almost entirely indifferent to the naturalness of the provided code snippets remove as much as of tokens and both their predictions and their confidence therein remain almost unchanged figure .
we discuss the reasons and implications of this observation using several examples in the remainder of this section.
.
explanations vs. shortcuts figure shows an example of a code snippet and its minified by an rnn and a transformer model respectively.
in this case understanding neural code intelligence through program simplification esec fse august athens greece original program def modified time self name try file open self .
path mtime mtime float file .
readlines file .
close except mtime none return mtime rnn minimized example self name file self .
path mtime mtime file file mtime return mtime transformer minimized example def modified time self name file self mtime mtime file file mtime mtime figure an example of two models learning different kinds of shortcuts.
the misused variable is mtime in red incorrectly passed in place of name on line of the original snippet.
the synthetically induced error is fairly obvious mtime is passed to the path call where name should be.
if given the task to find such a bug a programmer might notice this particular use beforedefinition bug which could then be explained with a relatively small portion of the function perhaps involving the unused parameter name and mtime s use before its assignment.
looking at the two minified programs we see traces of these same explanations the rnn model which prioritizes lexically local patterns primarily preserves the part of the program where mtime is passed to path an unlikely combination given that name is in the immediate context.
the transformer meanwhile prefers to focus on the subsequent assignment to mtime which is out of place given its preceding use.
however the existence of a short explanation does not naturally imply that much of the program is unnecessary as it apparently does to our models.
the two minified programs are arguably much harder to read than the original especially as our model would see them without the original for reference or the indentation that we added here yet our models seemed to have no more trouble predicting from them than the original.
it is evident that our models have learned to take shortcuts they predominantly leverage simple syntactic patterns quite literally to the exclusion of almost all else.
our broader results show that this behavior is ubiquitous and extends across models and tasks section .
these shortcuts still capture meaningful natural programming patterns.
neither failing to use a parameter nor the assignmentafter use 5are necessarily syntactically invalid but our models have clearly learned that they are sufficiently irregular to contemplate a bug.
what is remarkable about our findings is not the absence of naturalness as a whole but the absence of macro level naturalness .
our models appear to have little to no care for the overall structure and content of the function just for the presence or absence of specific patterns therein.
.
ramifications for deep learning do our results then imply that deep learners are in a sense frauds at least in software engineering applications?
not quite it is well known that machine learners neural networks especially prefer to find simple explanations of their training data which often hinders their generalizability.
the models in our case are nothing less they are presented with a single often obvious task and learn to solve it arguably as efficiently as possible.
that does not invalidate the quality of their learning in practice there are myriad patterns to heed when predicting a vocabulary of hundreds of thousands of method names or finding arbitrary variable misuse bugs in millions of functions.
one interpretation of our results is that our models arrive at a set of simple explanations that encompasses nearly all these cases such as how a variable should not be used before its assignment.
this vocabulary is still large and diverse so it remains a significant challenge for models to discover there are good reasons why model performance can differ vastly even on simple tasks .
we saw these differences in action too transformer models are substantially better at leveraging global information from throughout the function than rnns which are largely lexically local .
correspondingly we saw that the latter permitted substantially more reduction of input programs anecdotally because it mainly preserved features in the immediate context of the bug s location.
note that this is not a strength of the rnn we are reducing programs to find out how much evidence these models used in their predictions.
the ideal is neither the complete program nor virtually none of it.
overall the apparent indifference to macro level naturalness that functions as a whole are complete and well formatted is troubling.
much like prior findings our demonstration that models rely on just a few features of their input naturally implies that they are highly vulnerable to perturbations in those inputs .
a natural question may be what might prompt them to read code more holistically?
one answer may come in the form of multi task learning in which a single model is trained on a wide variety of tasks .
in our analyses we believe the models learned those salient patterns that helped achieve their singular objective a mixture of diverse objectives might prevent such shortcuts altogether.
whether and how this works in practice is an open question.
one risk may be that similar types of shortcuts are useful for many tasks especially those based on synthetically generated flaws a common practice in our field.
if so our approach should be able to elucidate this and may well be able to serve more generally as an indicator of the complexity of a task and or the degree of information used by a model by using visualizations such as figure that show the amount of input data required to accomplish a task on average.
5the variable might be a field.esec fse august athens greece md rafiqul islam rabin vincent j. hellendoorn and mohammad amin alipour .
accessible prediction attribution prediction attribution in deep learners is a fast growing field that attempts to relate the prediction of a neural network to a part of its input .
most current attribution approaches either require access to the trained model in full white box methods including its architecture and hyper parameters or try to approximate them e.g.
.
the methodology based on reduction that we used in this paper does not require any knowledge of either architecture or hyper parameters of the models it can be applied in a complete black box fashion.
this both makes it applicable to cases where internal information of models is unavailable e.g.
proprietary models but also substantially eases its use.
deploying our model to a new pre trained model only required knowing minimal constraints on the input which tokens may not be reduced whether the program must continue to be parseable which are usually readily accessible.
moreover current techniques in attribution and transparency usually require a certain level of knowledge about the characteristics of learning algorithms and reasoning about the model behaviors which the average practitioner may not have sufficient time to acquire.
we envision that the application of sivand if deployed directly to developers would thus be more accessible to software engineers especially those who have prior familiarity with the concept of test reduction for failing test cases in debugging and this knowledge can be easily transferred to reducing input programs insivand .
our results did show differences sometimes significant ones in reduction behavior between various architectures e.g.fig.
which may well be useful for experts to interpret.
but using sivand does not require such knowledge the reduced input programs speak for themselves.
we plan to perform a user study to evaluate the usability of our approach to the average practitioner.
.
limits of extractive interpretation thesivand methodology proposed in this paper is best described as an extractive interpretation approach.
extractive interpretation methods extract and present the important regions of the inputs leaving it to the user to characterize the behavior of the model.
these approaches usually work well in cases where features are sparse and not correlated so that highlighting one or more parts in the input provides enough insights about the model.
the high rate of reduction in most cases in our work may suggest that this approach is indeed applicable here and can provide sufficient insights about the behavior of the models although a user study is needed to validate that further.
in turn the power of this approach would be limited on models that use complex or non syntactic features such as the number of lines of code in programs as the basis for their prediction.
this would prevent the input programs from being reduced significantly or it might be difficult to make sense of the output and pinpoint the underlying important feature in the reduced programs.
.
impact of smallest atomic unit choice choosing different smallest atomic units in the delta debugging algorithm can provide different and potentially complementary insights about the model.
for example code2vec predicts the name of the code snippet in figure as main .
if we use hierarchical delta debugging wherein the smallest atomic unit of reduction isp u b l i c s t a t i c v oid f s t r i n g a r g s system .
s e t p r o p e r t y c o n s t a n t s .
dubbo properties key conf dubbo .
p r o p e r t i e s main .
main a r g s figure code2vec correctly predicts the name of this method as main .
an ast node the result would be void f string args suggesting that the method signature and the argument name have had a large impact on the prediction.
however if we choose characters as the smallest atomic unit and employ delta debugging the result is d f sg r em.s c.d main r which provides a complementary view for the prediction that shows the importance ofmain identifier used in the method body.
future work may extend our approach to new metrics and reduction strategies which may well provide novel insights especially in the future more complex models that are more resistant to such simple reduction.
related work there has been a lot of work in the area of transparent or interpretable ai computer vision and natural language processing that focuses on understanding neural networks.
interpretable transparent machine learning has numerous benefits including making predictions explainable and thus more useful using learned models to generate new insights and improving the quality and robustness of the models themselves .
these objectives are generally studied under the umbrella of explainable ai .
while some work studies the properties of neural models in general many studies are more ad hoc focusing on specific domains and tasks .
.
software engineering there is a growing body of work in the domain of robust neural models for source code or code intelligence in general.
bui et al.
evaluated the impact of a specific code fragment by deleting it from the original program rabin et al.
compared the performance of neural models and svm models with handcrafted features and found comparable performance with a relatively small number of features.
wang et al.
propose a mutate reduce approach to find important features in the code summarization models.
several other studies have evaluated the robustness of neural models of code under semantic preserving program transformations and found that transformations can change the results of the prediction in many cases.
finally sahil et al.
published concurrently with this work present a very similar approach to capturing vulnerability signals from a model s prediction by applying prediction preserving input minimization using delta debugging.
their results are complementary to ours further reinforcing the merit of the proposed method.
.
computer vision and nlp the need for neural model interpretability and robustness was firmly established by goodfellow et al.
who showed that a convolutional neural network could be tricked into changing its image classification into virtually any label by adding an imperceptibleunderstanding neural code intelligence through program simplification esec fse august athens greece amount of targeted noise .
this produced a flood of research on both improving the robustness of neural networks and attacking current architectures often in tandem.
importantly comprehensive studies of robustness and adversarial perturbations have yielded rich scientific insights into the learning behavior of these models both for vision and beyond .
the computer vision community has proposed many ways to visualize what parts of the input are most significant to a neural network both to individual neurons and its output classification.
a popular example and similar to our approach is occlusion analysis in which regions of interest are identified by evaluating the network s prediction when various parts of the input image are occluded.
locations at which prediction accuracy rapidly drops when removed are likely especially integral to the prediction.
for linguistic expressions input perturbations are usually less obvious while certain words such as stop words may safely be removed without altering the meaning of a sentence more general changes quickly risk producing very different inputs.
recent input related methods rely on synonym datasets and swap out similar names to ensure that they generate semantically similar phrases .
our work shows that at least for current models and tasks this is significantly less of a concern in software engineering where many tokens can be removed with little consequence.
threats to validity we evaluated sivand on four neural models and two tasks trained on millions but tested on a few thousands of random samples from their datasets.
as such our findings may naturally not extend to other inputs models and domains.
nevertheless we argue both that our analysis is broad spanning more models and domains than comparable work and that the design of our approach is applicable to many other problem settings in our field which commonly take tokens and asts as inputs to yield a single or few outputs all covered by the models in this work.
to ensure software quality we used the tools and datasets shared by the original developers of the models each from public repositories used by dozens of developers and cited in multiple studies.
for our input reduction we adapted zeller et al.
s implementation of delta debugging which has been widely used in the industry and other research studies over decades.
conclusion we proposed sivand a simple model agnostic methodology for interpreting a wide range of code intelligence models which works by reducing the size of input programs using the well known deltadebugging algorithm.
we apply sivand to four popular neural code intelligence models across two datasets and two tasks showing that our method can significantly reduce the size of input programs while preserving the prediction of the model thereby exposing the most significant input features to the various models.
our results hint at the idea that these models often use just a few simple syntactic shortcuts in their prediction.
this sets the stage for broader use of transparency enhancing techniques to better understand and develop neural code intelligence models.