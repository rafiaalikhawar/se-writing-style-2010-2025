prioritizing test inputs for deep neural networks via mutation analysis zan wang college of intelligence and computing tianjin university tianjin china wangzan tju.edu.cnhanmo you college of intelligence and computing tianjin university tianjin china youhanmo tju.edu.cnjunjie cheny college of intelligence and computing tianjin university tianjin china junjiechen tju.edu.cn yingyi zhang college of intelligence and computing tianjin university tianjin china yingyizhang tju.edu.cnxuyuan dong information and network center tianjin university tianjin china dongxuyuan tju.edu.cnwenbin zhang information and network center tianjin university tianjin china zhangwenbin tju.edu.cn abstract deep neural network dnn testing is one of the most widely used ways to guarantee the quality of dnns.
however labeling test inputs to check the correctness of dnn prediction is very costly which could largely affect the efficiency of dnn testing even the whole process of dnn development.
to relieve the labeling cost problem we propose a novel test input prioritization approach called prima for dnns via intelligent mutation analysis in order to label more bug revealing test inputs earlier for a limited time which facilitates to improve the efficiency of dnn testing.
prima is based on the key insight a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs is more likely to reveal dnn bugs and thus it should be prioritized higher.
after obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input prima further incorporates learning to rank a kind of supervised machine learning to solve ranking problems to intelligently combine these mutation results for effective test input prioritization.
we conducted an extensive study based on popular subjects by carefully considering their diversity from five dimensions i.e.
different domains of test inputs different dnn tasks different network structures different types of test inputs and different training scenarios .
our experimental results demonstrate the effectiveness of prima significantly outperforming the state of the art approaches with the average improvement of .
.
in terms of prioritization effectiveness .
in particular we have applied prima to the practical autonomous vehicle testing in a large motor company and the results on real world scene recognition models in autonomous vehicles further confirm the practicability of prima.
index terms test prioritization deep neural network mutation label deep learning testing i. i ntroduction in recent years deep neural networks dnns have gained great success in many domains e.g.
autonomous vehicles face recognition speech recognition medical diagnosis and software engineering unfortunately like yjunjie chen is the corresponding author.traditional software systems dnns also contain bugs .
due to the popularity and importance of dnns dnn bugs could lead to serious consequences in practice even disasters in safety critical domains.
for example an uber autonomous vehicle killed a pedestrian in tempe arizona in .
therefore it is very critical to guarantee the quality of dnns.
dnn testing is one of the most widely used ways to guarantee the quality of dnns .
in the literature most works on dnn testing focus on proposing various metrics to measure the adequacy of test inputs or designing various approaches to generating test inputs .
however beyond that there is another key challenge in the field of dnn testing it is very costly to label test inputs to check the correctness of dnn prediction which could largely affect the efficiency of dnn testing even the whole process of dnn development .
more specifically the reasons for the labeling cost problem are threefold the test set is large scale the main way of labeling is manual analysis which tends to involve multiple persons to label one test input so as to ensure the labeling correctness domainspecific knowledge is usually required for labeling which makes labeling more expensive by employing professional persons.
according to the existing study this challenge is even more troublesome in practice but currently few efforts have been devoted to solving it.
to relieve this problem it is intuitive to prioritize test inputs so that those test inputs that are more likely to be incorrectly predicted by the dnn under test also called bug revealing test inputs can be labeled earlier.
in this way more bug revealing test inputs can be identified for a limited time and in the meanwhile identifying bug revealing test inputs earlier facilitates to conduct the debugging process earlier which could largely improve the efficiency of dnn testing and shorten the period of dnn development.
indeed some test input prioritization approaches for dnns have been proposed recently to solve ieee acm 43rd international conference on software engineering icse .
ieee the labeling cost problem including coverage based and confidence based test input prioritization approaches.
however these existing approaches either suffer from the effectiveness issue or have limited application scenarios.
more specifically coverage based test input prioritization which prioritizes test inputs based on their neuron coverage by adapting the coverage based test prioritization in traditional software systems has been demonstrated to be not effective compared with confidence based test input prioritization in the existing study .
confidence based test input prioritization i.e.
deepgini is the state of the art approach for dnns which prioritizes test inputs for classification models by measuring the dnn s confidence about its classification for each test input .
that is if a dnn model outputs more similar probabilities for all classes when classifying a test input it means that its confidence for classifying the test input is less and thus the test input should have a higher priority for labeling.
although deepgini has been demonstrated to be effective in some cases it actually suffers from the issues of limited application scenarios deepgini is designed specific to classification models and thus it may not be directly applicable to regression models which are also widely used in practice.
in the meanwhile it has been only evaluated on image classification models and thus it is unclear whether it can still perform well on other domains of test inputs such as sequential data.
the assumption of deepgini that bug revealing test inputs are predicted with similar probabilities for all classes is not tenable in many practical cases.
for example when training data is polluted or the training scenario is transfer learning the confidence of the dnn model for incorrect prediction tends to be strong.
moreover many adversarial input generation methods such as c w aim to generate the test inputs that make the probability of the wrong class large as much as possible which also goes against its assumption.
in these practical cases the performance of deepgini can drop largely which has been demonstrated in our study section iv e .
therefore it is still desired for a more general and better test input prioritization approach for dnns.
to further improve the performance of test input prioritization for dnns in this paper we propose a novel test input prioritization approach for dnns via intelligent mutation analysis which is called prima prioritizing test inputs via intelligent mutation analysis .
the key insights of prima are twofold if a test input can kill many mutated models i.e.
when the prediction result of a test input is different between the original model and the mutated model by slightly changing the original model we regard that the test input kills the mutated model indicating that the test input can test the model sufficiently the test input is more likely to reveal dnn bugs.
that reflects the exploration degree of the test input to the dnn model under test.
if many mutated test inputs by slightly changing an original test input have different prediction results with the original one on the dnnmodel under test indicating that much information of the test input is utilized by the model the test input is more sensitive to capture dnn bugs.
that reflects the exploration degree to the test input itself.
based on the key insights of exploring both dnn models and test inputs we first design a series of model mutation rules and input mutation rules in prima.
based on these mutation rules prima produces a number of mutation results for each test input and then a follow up challenge is how to effectively utilize these mutation results to prioritize test inputs.
in particular different dnn models have different characteristics and thus their ways of utilizing mutation results to achieve the best prioritization effectiveness could be different further indicating the difficulty of solving this challenge.
in this paper prima incorporates learning to rank a kind of supervised machine learning to solve ranking problems to build a ranking model which can effectively prioritize test inputs by intelligently learning how to utilize mutation results for different dnn models.
based on the prioritization result through prima the bug revealing test inputs can be labeled earlier so that the efficiency of dnn testing can be largely improved and the period of dnn development can be effectively shortened.
to evaluate the performance of prima we conducted an extensive study based on popular subjects we call a pair of dataset and dnn model a subject .
in particular we carefully considered the diversity of our subjects from five dimensions including different domains of test inputs i.e.
images text and predefined features different types of test inputs i.e.
natural test inputs and adversarial test inputs generated by different adversarial input generation methods different tasks of dnn models i.e.
classification models and regression models different network structures of dnn models i.e.
cnn and rnn and different training scenarios i.e.
normal training training with polluted training data and transfer learning .
to our best knowledge this is the largest and the most diverse study in this field.
our experimental results show that prima performs stably well on such diverse subjects and significantly outperforms all the compared approaches with the average improvement of .
.
in terms of rauc to be introduced in section iv d .
for example prima performs the best among all the studied approaches in .
cases.
in particular we have applied prima to one of the most influential company for autonomous vehicles all over the world.
due to the company policy we hide its name and call it t. our results on real world scenerecognition subjects in autonomous vehicles further confirm the practicability of prima.
this work makes the following major contributions approach .
we propose a novel test prioritization approach for dnns via intelligent mutation analysis.
in particular we design a series of model and input mutation rules and adopt learning to rank to intelligently combine mutation results for effective test input prioritization.
study .
we conduct the most large scale study based on subjects in which the diversity of subjects is considered carefully from five dimensions demonstrating the effective398ness of our proposed approach.
practical evaluation .
we have applied our proposed approach to one of the most influential company for autonomous vehicles all over the world further confirming its practicability.
ii.
b ackground and related work a. dnn and dnn testing dnn consists of many layers each of which contains a large number of neurons .
the neurons between layers are connected with links each of which is equipped with a weight.
these weights are learned via the training process based on training data.
in general dnn is divided into convolutional neural network cnn and recurrent neural network rnn .
dnn testing is one of the most widely used methods to guarantee the dnn quality .
in dnn testing test inputs refer to the inputs to be predicted by the dnn model under test which can be different forms e.g.
images and text according to different domains.
regarding test oracles in dnn testing it is required for each test input to manually label its ground truth.
then it can determine whether a test input is predicted correctly by the model by comparing the labeled ground truth and the predicted result.
b. test input prioritization for dnns in the literature several test input prioritization approaches for dnns have been proposed to solve the labeling cost problem .
for example the state of the art approach is deepgini which prioritizes test inputs by measuring the dnn s confidence for classifying each test input.
more specifically the test inputs that are predicted with more similar probabilities for all classes are prioritized higher.
moreover they implemented neuron coverage based test input prioritization by adapting coverage based test prioritization in traditional software for comparison in the study.
besides byun et al.
proposed to utilize more advanced metrics i.e.
surprise adequacy including lsa likelihood based surprise adequacy and dsa distance based surprise adequacy to prioritize test inputs for dnns.
lsa refers to the surprise of a test input with respect to the estimated density of each activation value in a set of activation traces for training data while dsa is defined by the euclidean distance between the activation trace for a test input and a set of activation traces for training data.
the surprise based test input prioritization prioritizes the test inputs with larger surprise adequacy values higher.
different from them our work proposes prima a novel and more effective test input prioritization approach for dnns via mutation analysis andlearning to rank .
furthermore some test input selection approaches are also proposed to improve the efficiency of dnn testing which aims to estimate the accuracy of a dnn model by selecting a small set of test inputs.
different from them our work aims to identify more bug revealing test inputs earlier by prioritizing test inputs.c.
mutation based test prioritization for traditional software in the field of test prioritization for traditional software some mutation based test prioritization approaches have been proposed .
for example lou et al.
proposed to prioritize tests according to the number of mutation faults that are killed by each test.
shin et al.
also evaluated the performance of multi objective mutationbased test prioritization which prioritizes tests by considering both killed mutation faults and distinguished mutation faults.
our proposed approach prima mainly has the following differences from them prima considers to mutate both the dnn model under test and the test inputs while traditional mutation based test prioritization approaches mainly mutate the software under test.
dnn models have different characteristics from traditional software and thus they have totally different mutation rules.
prima incorporates learning torank to intelligently utilize mutation results for test input prioritization which is not adopted by traditional mutationbased approaches.
iii.
a pproach to relieve the labeling cost problem we propose a novel test input prioritization approach for dnns via intelligent mutation analysis called prima .
prima considers both model mutation and input mutation based on the following two key insights if a test input can kill many mutated models by slightly changing the model under test indicating that the test input can test the model sufficiently the test input is likely to reveal dnn bugs.
if many mutated test inputs from one test input by slightly changing the test input have different prediction results with the original one on the model under test indicating that much information of the test input is effectively utilized by the model the test input is sensitive to capture dnn bugs.
they reflect the exploration degree of the test input to the dnn model under test and the test input itself respectively.
based on these key insights prima consists of three steps we design a series of model mutation rules and input mutation rules and prima obtains mutation results for each test input section iii a prima extracts a set of features from these mutation results for each test input in order to effectively utilize these mutation results to prioritize test inputs section iii b prima adopts the framework of learning to rank to build a ranking model which is able to intelligently utilize the extracted features for prioritizing test inputs section iii c .
also we present the usage of prima in section iii d. figure shows the overview of prima.
a. mutation rules in prima we design two categories of mutation rules based on the above key insights i.e.
model mutation and input mutation.
model mutation is to slightly change the dnn model under test to produce a mutated model.
if a test input produces different prediction results between the original model and the mutated model we regard that the test input kills the mutated model indicating that the slightly mutated part is effectively tested by the test input.
input mutation is to 399dnn model validation set for the dnn mutation resultsinput mutationmodel mutation feature extractionfeatures test set to be labeled model mutationinput mutation feature extractionlearning to rankranking model prioritized test set developersrankinglabelingfig.
overview of prima slightly change a test input to produce a mutated test input.
if the slight difference between the original test input and the mutated test input leads to different prediction results on the model under test it means that the slightly mutated part is effectively utilized by the model and is contributing to the model.
our designed model mutation rules and input mutation rules will be introduced in detail afterwards.
model mutation rules as presented in section ii a neurons and weights are basic elements in dnns.
thus following the existing work we design four model mutation rules which are operated at the level of neurons rather than layers and thus could conduct more fine grained mutation as presented in the existing work on them as follows neuron activation inverse nai inverts the activation state of a neuron by changing the sign of the neuron output before passing it to the activation function.
neuron effect block neb blocks the effect of a neuron on the next layers by setting the neuron weights to the next layers to be .
gauss fuzzing gf adds noise to the weights of a neuron following gaussian distribution n .
if is large the added noise is large which tends to produce an invalid model.
thus we set to be 0and to be .
.
weights shuffling ws shuffles the weights of a neuron with the previous layers.
in particular prima aims to slightly change the model under test since largely changing the model is likely to produce invalid models slight mutation is better to simulate real bugs slight mutation is helpful to learn which parts in the model are effectively tested by each test input.
therefore instead of changing all the neurons weights prima randomly samplesx neurons weights for each mutation.
input mutation rules the test inputs of dnns can have different forms in different domains such as images sequential data e.g.
text and speech and predefined features like the inputs for traditional machine learning .
to design a general and practical approach prima should be able to handle various domains of test inputs.
in particular prima considers three popular domains including images sequential data i.e.
we use text as the representative and predefined features but it is easy to extend prima to moredomains to be discussed in section vi .
due to their significant differences we design input mutation rules for each of the three domains based on their own characteristics.
even though there are different mutation rules for different domains of test inputs they share the same high level idea of designing input mutation rules changing a small part of basic elements such as pixels for images and characters for text in a test input for each mutation so as to learn how much information in a test input is really contributing to the model under test.
that is each mutation rule is to select a small part of basic elements from a test input to mutate.
in particular the mutation operators for the selected basic elements are adapted from the widely used operators used for generating adversarial inputs in the corresponding domains .
the reason for this idea is that if slight mutation on a test input can change the prediction results it means that the mutated part is indeed utilized by the model and thus it could be sensitive to capture the bugs in the model.
the detailed mutation rules are introduced as follows image mutation rules pixel gauss fuzzing pgf adds noise to the selected pixels following guassian distribution n .
here we set to be and to be .
.
pixels shuffling ps shuffles the selected pixels.
coloring pixel white cpw makes the colors of the selected pixels become white.
coloring pixel black cpb makes the colors of the selected pixels become black.
pixel color reverse pcr reverses the colors of the selected pixels.
text mutation rules character shuffling cs shuffles the selected characters.
character replacement crl replaces the selected characters with other characters randomly selected from the whole set.
character repetition cre repeats the selected characters.
predefined features mutation rules discrete value replacement dvr replaces the selected feature values with the other values randomly selected from the whole set of discrete values.
continuous value modification cvm randomly increases or decreases the selected feature values by e .
to slightly mutate test inputs we set eto be .
according to the designed rules for a given test set and a model under test prima applies each model mutation rule to generatemmutated models and then obtains the prediction results of each test input on all the mutated models and the original model.
also for each test input prima applies each input mutation rule to generate nmutated inputs and then obtains the prediction results of all the mutated inputs and the original input on the model under test.
that is each test input has both model mutation results and input mutation results.
in the future more effective mutation rules could be 400incorporated by prima.
b. feature extraction after obtaining the mutation results for each test input a follow up problem is how to effectively utilize these mutation results to prioritize test inputs.
to intelligently learn how to effectively utilize these mutation results to prioritize test inputs for different models prima adopts the framework of learning to rank to build a ranking model for each dnn model.
since learning to rank requires a set of features like other supervised machine learning we carefully extract a set of features from these mutation results.
in this subsection we present the step of feature extraction in prima.
more details about the step of learning to rank based ranking model building for prioritization can be found in section iii c. intuitively not only whether different prediction results e.g.
different predicted classes for classification tasks are produced between an original model input and a mutated model input should be considered but also the difference degree of the prediction results is also helpful to reflect the bug revealing possibility.
with this intuition we identify two types of features from the mutation results for each test input.
since the outputs of classification models and regression models are different where the former is the probabilities of a test input belonging to each class while the latter is a number we first present the identified features for classification models as below and then adapt them to regression models.
given a set of test inputs t ft1 t2 t sg a classification model under test mwithgclasses denoted as c fc1 c2 c gg a set of mutated models by a mutation rule denoted as r mr fmr mr mr mg a set of mutated inputs from tkby a mutation rule denoted as r tr k ftr k1 tr k2 tr kng the predicted probabilities by tkonmand mr jdenoted asp fp p p gg andp tkmr j fp tkmr j p tkmr j p tkmr j ggand the corresponding predicted classes denoted as c and c tkmr j respectively the predicted probabilities by tr kion mdenoted asp tr kim fp tr kim p tr kim p tr kim gg and the corresponding predicted class denoted as c tr kim the features of a test input tkformare presented as follows f1 features from the mutation results in which c is different from c tkmr j j m orc tr kim i n for each mutation rule fa the number of mutants where c tkmr j orc tr kim is different from c for each model or input mutation rule.
fb the size of the set fc tkmr j orc tr kim c tkmr j orc tr kim is different from c gfor each model or input mutation rule.
this reflects the bugrevealing diversity for the test input.
fc the number of mutants where c tkmr j orc tr kim is the class that the largest number of mutants predicts and is different from c for each model or input mutation rule.
this reflects the distribution of different predicted classes by these mutants to some degree.
f2 features from the difference degree between p tkmr j j m orp tr kim i n andp for each mutation rule fa the average difference degree between p tkmr j orp tr kim andp which is calculated bypm j 1dist p tkmr j p m orpn i 1dist p tr kim p n for each model or input mutation rule where dist is the cosine distance.
fb the distribution of all the difference degrees between p tkmr j orp tr kim andp for each model or input mutation rule.
we split equal intervals in and then count the number of the mutants in each interval according to their difference degrees.
fc the average difference between the probability of c and the probability of this class predicted by mutants for each mutation rule.
the reason for this feature is that the changing probability for the predicted class by the original model and input is more relevant to the bugrevealing possibility.
since regression models output a number rather than a class with a list of class probabilities we cannot extract the features like f1 but only extract the features from the difference degree of the prediction results for each test input including the average difference of the prediction results between the original model and input and the mutants for each mutation rule where the difference is the absolute difference between the prediction results and the distribution of the differences for each mutation rule where we first normalize all the differences to according to the output range of the regression model under test then split equal intervals in and finally count the number of the mutants in each interval according to their normalized differences.
c. learning to rank based ranking model building based on the set of features prima constructs a training set for learning to rank where each instance is an input of the dnn model under test.
for each instance prima extracts the above features from its corresponding mutation results and labels it as or for a classification model according to whether the input is incorrectly predicted by the model under test.
for a regression model prima labels each instance as the absolute difference between its prediction result and the ground truth where the larger difference indicates a larger bug revealing possibility to some degree.
then prima normalizes the features to adjust values measured on different scales to a common scale.
since all the features are numeric type prima normalizes each value of these features into using min max normalization .
suppose the set of training instances is denoted as a fa1 a2 a ugand the set of features is denoted as f ff1 f2 f vg we usexijto represent the value of the feature fjfor the instance aibefore normalization and use x ijto represent the value of the feature fjfor the instance aiafter normalization i uand j v .
formula presents the normalization process.
401x ij xij min fxkjj1 k ug max fxkjj1 k ug min fxkjj1 k ug after obtaining the processed training set prima adopts the framework of learning to rank which is a kind of supervised machine learning and has been widely used to solve ranking problems in many domains such as document retrieval and expert search to build a ranking model for prioritizing test inputs.
in this way the mutation results can be intelligently utilized to achieve the best prioritization effectiveness for different models.
in particular prima adopts the xgboost ranking algorithm which is an optimized distributed gradient boosting learning algorithm to build the learning to rank based ranking model.
the reasons using this learning to rank algorithm are fourfold the labels in our training set for classification models are or which is actually the labels for classification.
in particular the xgboost ranking algorithm is good at handling the classification labels for ranking tasks by effectively searching for optimal splits .
it is able to effectively learn more complex features from basic features using tree ensemble models which matches our problem well.
it has been demonstrated to be effective and efficient compared with other popular learningto rank algorithms .
it makes the ranking results interpretable by measuring the contribution of each feature to the ranking model.
d. usage of prima based on the ranking model prima predicts a score for each test input in a test set and then prioritizes all these test inputs according to the descending order of their scores.
in particular before prediction through the ranking model it is also required to extract features for each test input in the test set from its corresponding mutation results.
during the practical usage of prima we use the validation set for the dnn model under test as the training set for building a ranking model through learning to rank since the ground truth whether each input in the validation set is predicted correctly is known and the validation set is independent with both training and test sets for the dnn model.
after building a ranking model for the dnn model under test based on its validation set prima can use this ranking model to prioritize various test sets for the dnn model.
that is for a dnn model under test the ranking model is built once based on its validation set and then can be constantly used for prioritizing its different sets of test inputs whose stable effectiveness has been demonstrated in our study section iv e .
iv.
e xperimental study design in the study we address two research questions.
rq1 is to investigate the effectiveness of prima compared with the existing approaches.
also we analyzed the contributions of our extracted features to the overall effectiveness of prima in order to further interpret its effectiveness.
rq2 is to investigate the efficiency of prima.
also we explored how to further improve its efficiency.a.
subjects in our study we used pairs of datasets and dnn models as subjects.
table i presents their basic information.
in particular to sufficiently evaluate the effectiveness of prima we carefully considered the diversity of subjects from five dimensions.
to our best knowledge this is the most largescale and diverse study in the field.
different domains of test inputs.
we considered three domains of test inputs including images text and predefined features.
more specifically we collected image datasets i.e.
cifar a class ubiquitous object dataset cifar100 a class ubiquitous object dataset mnist a handwritten digit dataset mnist vs usps a handwritten digit dataset for transfer learning coil a 20class object recognition dataset for transfer learning pie27 vs pie5 a face dataset for transfer learning pie27 vs pie9 a face dataset for transfer learning and driving an autonomous driving dataset provided by udacity text datasets i.e.
trec a question classification dataset imdb a large movie review dataset for binary sentiment classification sms spam a mobile phone spam messages dataset cola a linguistic acceptability dataset and hate speech a hate speech and offensive language collection dataset and one dataset with predefined features i.e.
kddcup99 a network intrusion information dataset provided by a competition in kdd .
different tasks of dnn models.
we considered both classification models id and regression models id .
the number of classes ranges from to across all the classification models.
different network structures of dnn models.
we considered both cnn id and rnn id .
the number of layers ranges from to and the number of weights ranges from 16k to 081k across all the models.
different types of test inputs.
we considered both natural test inputs and adversarial test inputs.
here we adopted three widely used adversarial input generation methods i.e.
c w carlini wagner bim basic iterative methods and jsma jacobian based saliency map attack .
besides the original test set i.e.
the set of natural test inputs following the existing work we also constructed mixed test sets by mixing natural test inputs and adversarial test inputs.
more specifically for each adversarial input generation method we first generated the same number of adversarial test inputs as the corresponding natural test inputs for cifar and cifar respectively.
then we randomly selected half of natural test inputs and half of adversarial test inputs to construct a mixed test set for each of the two datasets under each adversarial input generation method.
that is we had both original test sets and mixed test sets in our study.
in this way we had four different test sets for each of them i.e.
vgg based on cifar resnet based on cifar vgg based on cifar resnet based on cifar100 respectively which can be used to investigate whether prima performs well for different test sets when building the 402table i basic information of subjects id dataset model test type domain cifar vgg original image cifar vgg bim image cifar vgg c w image cifar vgg jsma image cifar resnet original image cifar resnet bim image cifar resnet c w image cifar resnet jsma image cifar vgg original image cifar vgg bim image cifar vgg c w image cifar vgg jsma image cifar resnet original image cifar resnet bim image cifar resnet c w image cifar resnet jsma image mnist lenet original image mnist m1 lenet original image mnist m2 lenet original image mnist m3 lenet original image mnist vs usps lenet original image coil vgg original image pie27 vs pie5 vgg original image pie27 vs pie9 vgg original image driving dave orig original image driving dave drop original image driving dave orig light image driving dave drop light image driving dave orig patch image driving dave drop patch image trec bi lstm original text imdb bi lstm original text sms spam bi lstm original text cola bi lstm original text hate speech bi lstm original text kddcup99 cnn original features ranking model once as presented in section iii d .
besides regarding dave orig and dave drop we also have three test sets respectively i.e.
the original one the patched test sets blocking some parts of each test input and the lighted test sets changing the intensities of lights for each test input .
different training scenarios.
we considered three training scenarios.
the first one is the normal training scenario for a dataset and a dnn model.
the second one is the training scenario with polluted training sets that are mutated by an accident or malicious attack id .
in particular we simulated this training scenario by adopting three polluted training sets from mnist which are widely used by the existing work .
more specifically the three polluted training sets are produced by changing the labels of training inputs i.e.
and respectively.
the third one is the transfer learning scenario id which trains a model based on a training set for solving a problem and then applies this model to a different but related problem.
b. compared approaches in our study we considered three compared approaches i.e.
the state of the art approach deepgini and two surprise based test input prioritization approaches lsa based and dsabased approaches denoted as lsa anddsa in this paper respectively .
more details about these compared approaches canbe found in section ii b. in the existing study neuroncoverage based test input prioritization has been compared with deepgini where the latter outperforms the former.
therefore we did not compare with the former in our study.
however more advanced metrics i.e.
surprise adequacy including lsa and dsa have been used for prioritizing dnn test inputs but the surprise based test input prioritization approaches have not been studied together with deepgini.
therefore we also used the surprise based test input prioritization for comparison.
please note that deepgini and dsa could not directly apply to regression models.
therefore we applied all the compared approaches to classification models and applied only lsa to regression models for comparison.
since lsa crashes when running on subjects due to lacking data points of certain classes which has been confirmed by the authors of lsa we do not include the results of lsa on these subjects.
c. implementation and configuration we implemented prima in python based on keras .
.
and xgboost .
.
and adopted the existing implementations of all the compared approaches which are released by the corresponding work .
regarding our mutation rules we set the number of model mutants mto be and the percentage of neurons weights selected xto be for model mutation.
for input mutation we set the number of selected basic elements to be and the number of input mutantsnto be in the domains of text and pre defined features while we set the number of selected pixels to be .
of the total number of pixels and nto be in the domains of images.
here we selected a relatively small part of basic elements to mutate in order to achieve slight mutation.
we set them by conducting a preliminary study based on a small dataset and found that such settings are effective in general.
also for the xgboost ranking algorithm in prima we set learning rate to be .
colsample bytree to be .
and max depth to be .
in particular the xgboost ranking algorithm is robust to parameter selection .
we also investigated the influence of main parameters on prima which will be presented in section vi b. as presented in section iii d for each subject prima uses the validation set to build a ranking model and then prioritizes test sets using the built ranking model.
for the subjects whose validation sets are not available we simulated a validation set by randomly selecting a set of inputs from the original test set and then prioritized the remaining inputs in the test set for each of these subjects.
our experiments are conducted on the intel xeon silver4214 machine with 128gb ram ubuntu .
and rtx ti gpus in parallel.
our code and experimental data can be found on our project homepage sail repos prima.
d. measurements to measure the prioritization effectiveness following the existing work we transformed the prioritization result produced by a test input prioritization approach for a subject 403to a figure.
for classification models the x axis of the figure is the number of prioritized test inputs and the y axis is the number of bug revealing test inputs while for regression models the x axis of the figure is also the number of prioritized test inputs but the y axis is the accumulated difference between the predicted results and the ground truth.
then we calculated the ratio of the area under curve for the test input prioritization approach to the area under the curve of the ideal prioritization as the metric.
we call this metric rauc .
larger is better.
in particular since labeling test inputs is very costly and the resources are limited it is better if more bug revealing test inputs can be identified when labeling fewer test inputs.
therefore we used rauc n which refers to the rauc for the first nprioritized test inputs as the measurement to measure the prioritization effectiveness in our study.
this measurement reflects the prioritization effectiveness under the given number of test inputs to be labelled.
in our study we considerednto be and respectively since the resources tend to be limited and thus the given number of test inputs to be labelled tends to be small.
here we denote their rauc nasrauc rauc rauc and rauc respectively.
we also presented the prioritization effectiveness on all the test inputs for each subject denoted asrauc all .
in particular the results when nis set to be larger e.g.
can be found on our project homepage.
also we will explain the reason why not use the widely used apfd in traditional test prioritization as the measurement in our study in section vi c. also we measured the prioritization efficiency for each test input prioritization approach.
in our study we used the time spent on test input prioritization as the metric.
e. results and analysis rq1 effectiveness of prima we first studied the prioritization effectiveness of prima.
overall effectiveness .
since the number of subjects used in our study is large and the space is limited we put the detailed comparison results for each subject on our project homepage.
we present the overall comparison results across all the subjects in table ii.
since deepgini and dsa could not directly apply to regression models we separately present the overall comparison results on classification models and regression models.
among subjects metrics cases prima performs the best in .
out of cases where deepgini performs the best in .
out of cases and lsa and dsa cannot perform the best in any case.
in terms of all the used metrics the average results of prima on classification models range from .
to .
with the average improvements of .
.
compared with deepgini .
.
compared with lsa and .
.
respectively.
on regression models the average results of prima range from .
to .
with the average improvements of .
.
compared with lsa.
in particular we conducted statistical analysis to investigate whether prima significantly outperforms all the compared approaches by conducting the wilcoxon signed rank test in terms of each metric at the significance level .
.
we found that all the p values are smaller than .
indicating that prima significantly outperforms all the compared approaches in terms of all the metrics in statistics.
the results demonstrate the effectiveness of prima.
effectiveness on different domains of test inputs .
table iii shows the effectiveness of prima on different domains of test inputs i.e.
images text and predefined features .
we found that prima outperforms all the compared approaches on all the three domains in terms of various metrics.
in particular this is also the first time to study these compared approaches in the domains of text and predefined features.
we found that prima largely outperforms deepgini lsa and dsa with the average improvements of .
.
and .
in terms of rauc in the domain of text.
also the average rauc all value of prima achieves .
while those of deepgini and dsa are only .
and .
respectively in the domain of predefined features.
the results demonstrate the stable effectiveness of prima in various domains.
effectiveness on polluted training and transfer learning .
as presented in section i the assumption of deepgini can be violated in many practical scenarios causing its effectiveness drops largely.
in this paper we are the first to study these approaches in two practical scenarios i.e.
polluted training and transfer learning scenarios whose results are shown in table iv.
we found that prima largely outperforms all the compared approaches in terms of all the metrics in both training scenarios.
in the polluted training scenario the average results of prima in terms of all these metrics range from .
to while those of deepgini only range from .
to .
.
in the transfer learning scenario the average results of prima in terms of all these metrics range from .
to .
while those of deepgini only range from .
to .
.
the results confirm our claims in section i demonstrating that the assumption of deepgini is not tenable in the two practical scenarios especially the polluted training scenario.
moreover the results further demonstrate the stable effectiveness of prima in various practical scenarios.
effectiveness on different types of test inputs .
we further investigated the effectiveness of prima on different types of test inputs i.e.
natural test inputs and adversarial test inputs which can also answer whether once one ranking model through prima is built for a dnn model it can perform well for various test sets of the dnn model.
due to the space limit we used cifar and vgg id as the representative and the conclusion holds for other subjects.
from table v prima performs the best among all the approaches for the four test sets in terms of various metrics.
in particular the rauc values of prima achieve .
.
.
and .
respectively demonstrating the stable effectiveness of prima on different types of test inputs.
that also demonstrates for a dnn model under test the built ranking model through prima is not necessary to be retrained frequently since it can perform rather stably on different test sets which indicates the practicability of prima.
feature contribution .
the ranking model in prima is built 404table ii overall comparison results across all the subjects approach best cases in rauc average rauc impr ovement of prima in rauc100 all all all cdeepgini .
.
.
.
.
.
.
.
.
.
lsa .
.
.
.
.
.
.
.
.
.
dsa .
.
.
.
.
.
.
.
.
.
prima .
.
.
.
.
rlsa .
.
.
.
.
.
.
.
.
.
prima .
.
.
.
.
rows c and r present the overall results on classification and regression models respectively.
columns present the number of subjects where each approach performs the best in terms of each metric columns present the average results across all the subjects in terms of each metric and column present the average improvement of prima over each compared approach in terms of each metric.
table iii comparison on different domains of test inputs domain approachaverage rauc100 all imagedeepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
textdeepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
featuresdeepgini .
.
.
.
.
lsa dsa .
.
.
.
.
prima .
.
.
.
.
table iv comparison on polluted and transfer learning scenario approachaverage rauc100 all polluteddeepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
transferdeepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
via the xgboost ranking algorithm which is interpretable by providing the contribution of each feature to the ranking model.
due to the limited space we show top features with the largest contributions on average across all the imageclassification subjects in table vi.
we found that top features contain both categories of features presented in section iii b and contain both model and input mutation rules demonstrating the rationality of prima in design.
moreover for the image classification subjects model mutation seems to make more contributions than input mutation and neb and nai contribute more than the other model mutation rules while ps and pgf contribute more than the other input mutation rules which can guide us to optimize prima in order to further improve its effectiveness and efficiency.table v comparison on different types of test inputs id approachrauc100 all 9deepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
10deepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
11deepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
12deepgini .
.
.
.
.
lsa .
.
.
.
.
dsa .
.
.
.
.
prima .
.
.
.
.
table vi top features in terms of the average contribution across all the image classification subjects rank rule feature scor e rank rule feature scor e neb fc .
neb fa .
nai fb .
gf fa .
ps fc .
neb fb .
neb fa .
pgf fb .
nai fa .
pgf fc .
fb andfb refer to the feature fb 2in intervals and respectively.
to sum up prima outperforms all the compared approaches in general on different tasks of dnn models different domains of test inputs different training scenarios and different types of test inputs.
rq2 efficiency of prima we investigated the efficiency of prima in table vii.
as deepgini and dsa cannot apply to regression models we compared these approaches in terms of efficiency on all the classification subjects subjects are not included for lsa as explained in section iv b .
the average time spent on test prioritization of prima is .
minutes while that of deepgini is only .
minutes.
although prima is less efficient than deepgini the cost of prima is 405table vii efficiency comparison across all the classification subjects in minutes approach mean std.
min.
max.
deepgini .
.
.
.
lsa .
.
.
.
dsa .
.
.
.
prima .
.
.
.
still acceptable in practice compared with the time consuming and expensive manual labeling which has been confirmed by our industry partners in section v. actually there is a promising direction to further improve the efficiency of prima.
as shown in table vi some mutation rules e.g.
neb and nai tend to make more contributions than others to the effectiveness of prima in general indicating their different influence on prima to some degree.
thus if we only keep the mutation rules making more contributions in prima the effectiveness of prima could be affected slightly while its efficiency could be improved largely.
to explore the feasibility of this direction we conducted a preliminary study by taking subject as an example.
the results show that with only these top mutation rules listed in table vi the efficiency of prima is improved by .
while its effectiveness only decreases less than .
in terms of rauc demonstrating this direction is indeed promising.
to sum up prima is less efficient than deepgini but its cost is still acceptable.
moreover selecting and using the mutation rules making more contributions could improve the efficiency of prima without much effectiveness loss.
v. p ractical evaluation prima has been applied to the practical autonomousvehicle testing in a large motor company which is one of the most influential company for autonomous vehicles all over the world.
due to the company policy we hide the company name and call ittin this paper.
this company has a large number of dnn models built by themselves for autonomous vehicles as well as a large number of test inputs to test these models.
in particular all these test inputs are required to be labeled manually which is very time consuming and expensive.
this company even has established a specialized department to finish the labeling task.
prima aims to solve the labeling cost problem and improve the efficiency of dnn testing which admirably serves their needs.
we have evaluated the effectiveness of prima based on three real world dnn models used for traffic scene recognition in autonomous vehicles in t. for ease of presentation we call the three dnn models m1 m2 and m3respectively.
m1 is a binary classification model detecting lane changing behaviours of ego vehicles with one test set denoted as m1t whose size is larger than 50k.
m2 is a classification model with four classes recognizing different lane changing scenarios and it has one test set denoted as m2 t whose size is nearly 6k.
m3 is a classification model with eight classes recognizing typical behaviours of ego vehicles and it has twotable viii effectiveness on industrial subjects id approachrauc100 all m1tdeepgini .
.
.
.
.
prima .
.
.
.
.
m2tdeepgini .
.
.
.
.
prima .
.
.
.
.
m3t1deepgini .
.
.
.
.
prima .
.
.
.
.
m3t2deepgini .
.
.
.
.
prima .
.
.
.
.
test sets denoted as m3 t1and m3 t2 whose sizes are larger than 10k and 20k respectively.
all of them use the rnn structure and the form of their test inputs is predefined features that are collected from sensors and automatically processed.
due to the company policy we have to hide more details about these models and datasets.
table viii shows the effectiveness of prima on the four industrial subjects.
here we compared prima with the stateof the art approach deepgini which has been demonstrated to be much better than both lsa and dsa on open source subjects .
we found that prima also outperforms deepgini in terms of rauc on all the four industrial subjects.
for example in terms of rauc prima achieves .
.
and on the four subjects with the improvements of .
.
.
and .
respectively.
the results further confirm the effectiveness of prima in practice.
according to the practical evaluation prima has been largely appreciated by the developers in t and in particular they think that the automatic prioritization cost of prima is totally acceptable compared with the timeconsuming and expensive manual labeling further confirming the practicability of prima.
vi.
d iscussion a. generality of prima our study has demonstrated the effectiveness of prima based on a large number of subjects with great diversity indicating the generality of prima to some degree.
besides although we considered three domains i.e.
images text and predefined features for prima in our work prima actually can be applicable to more domains e.g.
speech as long as input mutation rules for the corresponding domains are designed.
as presented in section ii c designing input mutation rules for different domains shares the same highlevel idea i.e.
changing a small part of basic elements in a test input for each mutation by adapting the operators widely used for generating adversarial inputs in the corresponding domains.
with this high level idea it is easy to extend prima to more domains since it is easy to determine the basic elements of test inputs and find the widely used operators for generating adversarial inputs in the corresponding domains.
for example for the speech domain the basic element of a test input is .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
max depthrauc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
colsample bytreerauc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
learning raterauc 100fig.
impact of main parameters in prima phoneme and the widely used adversarial operators include adding noise to audio signal and attacking phonetically similar phrases and thus prima could be easily extended to the speech domain.
that further reflects the generality of prima to some degree.
b. impact of main parameters in prima we investigated the impact of main parameters in prima including max depth the maximum tree depth for each xgboost model colsample bytree the sampling ratio of columns of features when constructing each tree and learning rate the boosting learning rate in the xgboost ranking algorithm.
here we randomly selected six subjects id and for this experiment by considering different domains of test inputs different training scenarios and different types of test inputs.
figure shows the effectiveness of prima under different parameter settings in terms of average rauc across the six subjects.
we found that prima performs stably under different parameter settings and our default settings are also indeed proper.
c. threats to validity the internal threat to validity mainly lies in the implementation of our approach prima all the compared approaches and experimental scripts.
to reduce this threat we adopted the implementations of the compared approaches released by the authors implemented prima based on popular libraries presented in section iv c and carefully checked the code of prima and experimental scripts.
the external threats to validity mainly lie in the subjects used in our study.
to reduce this threat we collected a large number of subjects with great diversity.
theconstruct threats to validity mainly lie in the parameters in prima and the measurements used in our study.
to reduce the threat from the parameters in prima we presented the parameter settings in section iv c and investigated the impact of main parameters in section vi b. regarding the measurements used in our study we measured both prioritization effectiveness and efficiency.
here we used rauc as the metric for effectiveness following the existing work and the prioritization time as the metric for efficiency.
in our study we did not use the widely used metric in traditional test prioritization i.e.
apfd to measure the prioritization effectiveness.
this is because when the accuracy of a model on a test set is low i.e.
there are a large number of bug revealing test inputs the upper bound of apfd could be much smallerthan which may affect the comparison among the subjects with very different accuracy.
vii.
c onclusion to solve the labeling cost problem in dnn testing we propose a novel test input prioritization approach called prima to prioritize bug revealing test inputs higher.
prima is based on mutation analysis and learning to rank.
its insight is that a test input that can kill many mutated models and produce different prediction results with many mutated inputs is more likely to reveal dnn bugs and thus it should have a higher priority for labeling.
after obtaining a number of mutation results via our designed model and input rules prima incorporates learning to rank to build a ranking model by intelligently combining these mutation results for test input prioritization.
our results on diverse subjects demonstrate the effectiveness of prima and it significantly outperforms the state of the art approaches.
moreover prima has been applied to practical autonomous vehicle testing in a large motor company and the results on industrial subjects further demonstrate its effectiveness in practice.
acknowledgment this work is partially supported by the national natural science foundation of china and and intelligent manufacturing special fund of tianjin and the fund no.
jcjq jj .