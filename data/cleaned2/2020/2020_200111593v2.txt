authorship attribution of source code a language agnostic approach and applicability in software engineering egor bogomolov egor.bogomolov jetbrains.com jetbrains research higher school of economics saint petersburg russiavladimir kovalenko vladimir.kovalenko jetbrains.com jetbrains research jetbrains n.v. amsterdam the netherlandsyurii rebryk y.a.rebryk gmail.com higher school of economics saint petersburg russia alberto bacchelli bacchelli ifi.uzh.ch university of zurich zurich switzerlandtimofey bryksin timofey.bryksin jetbrains.com jetbrains research higher school of economics saint petersburg russia abstract authorship attribution i.e.
determining who is the author of a piece of source code is an established research topic.
state of theart results for the authorship attribution problem look promising for the software engineering field where they could be applied to detect plagiarized code and prevent legal issues.
with this article we first introduce a new language agnostic approach to authorship attribution of source code.
then we discuss limitations of existing synthetic datasets for authorship attribution and propose a data collection approach that delivers datasets that better reflect aspects important for potential practical use in software engineering.
finally we demonstrate that high accuracy of authorship attribution models on existing datasets drastically drops when they are evaluated on more realistic data.
we outline next steps for the design and evaluation of authorship attribution models that could bring the research efforts closer to practical use for software engineering.
ccs concepts software and its engineering software maintenance tools software verification and validation security and privacy malware and its mitigation .
keywords copyrights machine learning methods of data collection software process software maintenance security acm reference format egor bogomolov vladimir kovalenko yurii rebryk alberto bacchelli and timofey bryksin.
.
authorship attribution of source code a language agnostic approach and applicability in software engineering.
in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
conference july washington dc usa association for computing machinery.
acm isbn x xxxx xxxx x yy mm.
.
.
.
of acm conference conference .
acm new york ny usa pages.
introduction the task of source code authorship attribution can be formulated as follows given a piece of code and a predefined set of authors to attribute this piece to one of these authors or judge that it was written by someone else.
this problem has been of interest for researchers for at least three decades .
prior research has shown that software engineering tasks such as software maintenance and software quality analysis benefit from authorship information.
since authorship information in software repositories may be missing or inaccurate e.g.
due to pair programming co authored commits and changes made after code review suggestions authorship attribution techniques could help in these tasks.
authorship attribution models output a probability for each of the known developers to be the author of a code fragment.
when the probabilities are significantly higher for a group of developers they are likely to be co authors of the snippet even if this information is missing in the version control system vcs .
on the other hand when the commit was authored by several people but probability is high only for a single person we can derive the main author of the commit.
source code authorship attribution is also useful for plagiarism detection either to directly determine the author of plagiarized code or to ensure that several fragments of code were written by a single author .
importantly while plagiarism detection techniques need a large database of code as a reference when checking for plagiarism authorship attribution approaches can directly identify suspicious code snippets.
plagiarism detection in turn is important in software engineering software companies need to pay extra attention to copyright and licensing issues as they can become liable to lawsuits .
for example developers often use stack overflow to copy and paste code snippets to their projects.
however if developers do not use caution code borrowed from stack overflow can induce licence conflicts on top of complicating maintenance .
recently several pieces of work improved the state of the art in authorship attribution on datasets for three popular programmingarxiv .11593v2 jun 2021conference july washington dc usa egor bogomolov vladimir kovalenko yurii rebryk alberto bacchelli and timofey bryksin languages c python and java.
for c caliskan et al.
reported the accuracy value of .
when distinguishing among potential code authors .
for python alsulami et al.
attributed code of programmers with .
accuracy .
yang et al.
developed a neural network model that achieved .
accuracy for a dataset of java code by authors .
some of the existing approaches use language specific features.
while language specific features can improve the accuracy of authorship attribution for a particular language designing a set of such features is a complex manual task.
to transfer an approach that targets a specific language to another one e.g.
from c to python one either needs to come up with a new set of features or otherwise suffer from a major accuracy drop .
in this study we achieve higher results in authorship attribution accuracy by suggesting two language agnostic models.
both models work with path based representations of code which can be built based on an abstract syntax tree ast for any programming language.
the first model called pbrf path based random forest is a random forest trained on relative term frequencies of tokens and paths in asts.
to the best of our knowledge pbrf is the first model that integrates path based representations into a classical ml model.
the second model named pbnn path based neural network is an adapted version of the code2vec neural network .
we evaluate both models on the datasets of code in three programming languages that were used in previous work.
pbrf improves the state of the art for java c and python datasets even with few available samples per author.
pbnn outperforms pbrf when the number of available samples per author is large.
both models improve state of the art results for java on a dataset of yang et al.
with .
and .
accuracy respectively.
later we evaluate how both models perform on our new collected datasets.
another aspect that we target in this study is data used for evaluation.
existing work on authorship attribution operates with sources of data different from regular software projects examples from books students assignments solutions of programming competitions and open source projects with a single author .
this data is different from code that can be found in real world software projects.
in this study we deeply investigate this difference.
based on the results of this investigation we propose a new data collection technique that can reduce these differences and generate more realistic datasets for the authorship attribution task.
to formalize the differences we suggest the concept of work context i.e.
aspects that can affect developer s coding practices and are specific to the concrete project such as project s domain team internal coding conventions and more.
also we discuss another source of data differences the evolution of programmers individual coding practices over time and changing context of their contributions.
we propose a method to quantitatively measure the impact of work context and evolution of individual coding practices on the accuracy of authorship attribution models.
our evaluation shows that the accuracy of authorship attribution models plunges when models are tested with more realistic data than what is offered by existing datasets.
in particular the model that can distinguish between authors with accuracy in one setup reaches only for developers in another.
this result suggests that before their practical adoption for software engineering existing resultsin the field of source code authorship attribution should be revisited to evaluate robustness of the models and their applicability to more realistic datasets.
we made the artifacts related to this work publicly available on github under mit license.
they contain implementation of the models code for running the experiments and a tool for collection of datasets for the authorship attribution task.
with this work we make the following contributions two language agnostic models that work with path based representation of code.
both models can take as an input any code fragment e.g.
file class method .
these models outperform language specific state of the art models on existing datasets.
an in depth discussion on the limitations of existing datasets supported by quantitative evaluation of effects of these limitations particularly when applied to the software engineering domain.
a novel scalable approach to data collection for evaluation of source code authorship attribution models.
a discussion of the concept of developer s work context and a novel methodology to evaluate its influence on the accuracy of authorship attribution models.
empirical evidence on how the evolution of developers coding practices impacts the accuracy of current authorship attribution models.
background to the best of our knowledge the first work on source code authorship attribution dates back to oman et al.
in .
although the results and approaches for the authorship attribution have changed and improved since then the formulation of the problem did not change and the underlying idea remains to use machine learning based on the features extracted from source code.
in the task of source code authorship attribution a model is given a piece of code and it should attribute the piece of code to one of the known developers or state that it was written by someone else.
the input code can differ by its form the code can be unchanged obfuscated or even decompiled and source of origin.
in previous work four sources of data have been used code examples from books .
used before the era of easily available open source projects for the lack of other sources.
students assignments .
often researchers are not allowed to publish these datasets e.g.
from university courses due to privacy or intellectual property issues.
lack of published data causes problems comparing different methods.
solutions to programming competitions .
this mostly refers to data from google code jam1 gcj an annual competition held by google since .
single author open source projects .
with the increasing popularity of hosting platforms for open source projects e.g.
github they have become a major source of data.
in this case the dataset consists of multiple repositories each developed by a single programmer.
for evaluation the researchers use unseen code snippets from the same repositories.
researchers attribution of source code a language agnostic approach and applicability in software engineering conference july washington dc usa avoid repositories with multiple authors because in this case authorship of even small fragments of code might be shared.
according to the recent survey by kalgutkar et al.
the following are the best results per programming language c caliskan et al.
reported the best results using a random forest trained on syntactical features.
they achieved and accuracy for datasets with and developers respectively.
python alsulami et al.
suggested to use tree based lstm to derive implicit features from asts achieving .
accuracy in distinguishing among authors.
java yang et al.
reported accuracy for a dataset of authors using neural networks.
instead of a commonly used stochastic gradient descent optimizer the authors trained the network with particle swarm optimization improving the accuracy by percent points.
syntactic features derived from ast of code are known to improve the results for authorship attribution as well as for other software engineering tasks such as code summarization method name prediction and clone detection .
compared to real world data where a programmer often works on multiple projects and using multiple languages existing datasets are limited to a single language and one project per author.
to overcome this limitation models applicable in real world environment should work with different programming languages in a consistent manner.
following this idea we decided to build a languageindependent model that is based on syntactic features and works on par with prior studies.
language agnostic models our first goal is to develop an authorship attribution solution that is language agnostic and achieves an accuracy comparable to stateof the art approaches.
to apply machine learning methods to code one should transform code into a numerical form called representation .
while some works use explicitly designed language specific features we represent code using path based representation to work with code in various programming languages in a uniform way.
a common way to use path based representation is the code2vec neural network suggested by alon et al.
for the task of method name prediction.
however code2vec requires a significant number of samples for each author to infer meaningful information due to the large number of trainable parameters.
thus alongside with the neural network we also employ a random forest model trained on similar features.
the random forest model shows a better accuracy for small datasets but generalizes worse for the larger ones.
in the rest of this section we describe our models and define related concepts.
.
definitions abstract syntax tree.
an abstract syntax tree ast is a representation of program s code in the form of a rooted tree.
nodes of an ast correspond to different code constructs e.g.
math operations and variable declarations .
children of a node correspond to smaller constructs that comprise its corresponding code.
different constructs are represented with different node types .
an ast mightomit parentheses tabs and other formatting details.
figure shows an example of a code fragment and a corresponding ast.
int square int x return x x a an example code fragment primitivet ype pt square intmethoddeclaration md simplename sn singlev ariabledeclaration svd primitivet ype pt simplename sn block b returnstatement rs infixexpression ie simplename sn simplename sn x x xint b an ast of this code fragment figure a code example and a corresponding ast path in ast.
apath is a sequence of connected nodes in an ast.
start and end nodes of a path may be arbitrary but we only use paths between two leaves in the ast to conform with code2vec and have the benefit of working with smaller pieces of code that such paths represent.
following alon et al.
we denote a path by a sequence of node types and directions up or down between consequent nodes.
the node in which the path changes direction is called top node .
in figure 1b an example of a path between the leaves of an ast is shown with red arrows.
in the notation of node types and directions this path can be denoted as follows sn md svd sn path context.
path contexts are triplets consisting of a path between two leaves and tokens corresponding to start and end leaves.
a path context represents two code tokens and a structural connection between them.
this allows a path context to capture information about the structure of code.
prior works show that code structure also carries semantic information .
figure 1b highlights the following path context square sn md svd sn x this path context represents a declaration of a function named square with a single argument named x. the path in this pathcontext encodes the following information it contains nodes function declaration as well as single variable declaration and tokens are linked to simple name ast nodes.
path based representation.
a path based representation treats a piece of code as a bag of path contexts.
for larger pieces of code the number of path contexts in the bag might be large and can be reduced by setting a limit on the length i.e.
the number of vertices in the list and width i.e.
the difference in indices between the children of the top node of the paths.
these limits on the length and width are hyperparameters and are determined empirically.
forconference july washington dc usa egor bogomolov vladimir kovalenko yurii rebryk alberto bacchelli and timofey bryksin each piece of code we generate all path contexts that satisfy the limits on the length and width.
however we use a random subset of mined path contexts to train the models as explained later in this section.
to adjust a bag of path contexts comprising the path based representation to the task of training a model we need to transform it into a numerical form.
for the code2vec model an embedding layer translates paths and tokens into numerical vectors.
for the random forest model we cannot use embedding because random forest trains without gradient computation.
in this case the transformation is done by computing relative term frequencies of paths and tokens i.e.
the number of times the token path occurs in a code snippet divided by the total number of tokens paths and storing them in a sparse vector.
since the path based representation does not require any specific properties from the programming language both pbrf and pbnn are language agnostic.
the following subsections cover both cases in more detail.
.
pbrf random forest model the random forest model is designed to work even when the number of samples for each author is rather small starting from a few samples per author where the neural network cannot capture enough information to generalize.
random forest has already proved to be effective in this setup in previous work .
tokens and ast based features have already proved to be effective for authorship attribution in the work by caliskan et al.
.
compared to their work we use more complex ast features ast paths.
random forest does not allow training an embedding of path contexts thus instead of combining paths and tokens into path contexts we use relative term frequencies of tokens and paths as features.
to the best of our knowledge this work is the first to combine path based representations with classical machine learning models such as random forest .
if a set of documents contains ttokens andppaths the random forest model takes a sparse vector of size f p tas an input.
the size of such a vector might be significant up to millions with some features being unimportant for identifying the author.
we employ feature filtering to reduce the effect of this dimensionality problem.
as in previous work on authorship attribution we use filtering based on mutual information mi .
the mutual information of a source code feature fand an author acan be expressed as mi a f h a h a f whereh x is shannon entropy .
for the task of authorship identification we interpret it as follows the higher the mi is i.e.
the lowerh a f is the better one can recognize the author based on the value of the given feature.
feature selection based on the mutual information criteria ranks all the features by their mi with the author label and takes nwith the highest mi value.
nis a hyperparameter determined empirically during the evaluation process by trying various values.
this approach does not account for dependencies among features for example if there are two identical high ranked features we take both and miss some other feature.
to avoid this problem one could add features one by one and compute mutual information aftereach step but on the scale of millions of features this procedure is too costly.
.
pbnn neural network model to achieve a better accuracy for larger datasets we adopted the neural network called code2vec .
it takes a bag of path contexts from a code snippet as an input transforms them into a single numerical vector and predicts a probability for each known developer to be the author of the snippet.
compared to classical machine learning methods neural networks can derive more complex concepts and relationships from structured data when given enough training samples.
a detailed description of the model is available in the original code2vec paper by alon et al.
.
the number of the pbnn s parameters is o t p d .
since the value of t p is usually large from tens of thousands to millions the number of required samples for the model to train is also significant.
evaluation on existing datasets we evaluated the two models presented in the previous section on the publicly available datasets for java c and python used in recent work and compared the accuracy of pbrf and pbnn to the results reported in these papers.
table shows statistical information about the datasets.
table presents the accuracy results of evaluation.
to compare the results of different models when these results are obtained through multiple runs i.e.
folds in cross validation we apply the wilcoxon signed rank test to various accuracy values per run.
when only the mean accuracy is available which is the case for previous work or the number of runs is too small we directly compare the mean values.
table datasets used in previous works.
the number of paths is provided for width 2andlength .
c python java number of authors number of files samples per author to source gcj gcj github unique tokens unique paths table mean accuracy by approach and dataset.
c python java caliskan et al.
.
.
n a alsulami et al.
n a .
n a sgd n a n a .
pso n a n a .
this work pbnn .
.
.
this work pbrf .
.
.985authorship attribution of source code a language agnostic approach and applicability in software engineering conference july washington dc usa .
hyperparameters both our models have parameters that should be fixed before the training phase i.e.
hyperparameters.
these hyperparameters are the number of trees number of features left after feature selection maximum depth of the trees for the random forest section .
size of the embedding vector for the neural network section .
and the limits on length and width of ast paths in path based representation.
we tuned these hyperparameters using grid search .
the plots of the models accuracy dependence on the specific parameters can be found in supplementary materials.
we found that the optimal values are similar across the datasets.
for the random forest model increasing the number of trees improves the accuracy until the number reaches after that the accuracy reaches a plateau.
increasing the maximum tree depth leads to accuracy growth and does not cause overfitting so we removed the limits on the tree depth.
the optimal number of features left after the selection is about of the initial number of features for all datasets.
while a slightly different number of features may result in a higher accuracy the differences are within the standard deviation range.
for the neural network increasing the embedding size from to results in a significant growth in accuracy for the python and c datasets.
for the java dataset we did not notice any significant change even though a larger size does not cause overfitting.
the models show a better accuracy when the limits on the length of paths in an ast are smaller.
we tried the lengths from to and the accuracy is the best for or .
short paths correspond to small repetitive constructs in code which can be typical of a particular developer.
paths of a higher length are less frequent and cause the model to overfit.
the same applies to the limits on the path width increasing the width beyond did not improve the attribution accuracy.
.
evaluation on c the c dataset was introduced by caliskan et al.
.
it contains solutions by each of participants for problems from google code jam competitions from to making it one of the largest experiments with respect to the number of authors.
each author has samples in the dataset corresponding to the solutions of problems.
following the original paper we run a fold crossvalidation.
at each fold the held out set contains a single solution from each author while the training set contains problems from each author.
for comparison we use the average models accuracy over all folds.
caliskan et al.
reported .
mean accuracy after fold cross validation.
pbnn and pbrf achieve .
and .
average accuracy respectively.
the neural model shows a lower accuracy because of overfitting the number of available data points is too small to train a much larger set of the network s internal parameters.
the pbrf s accuracy is higher compared to the caliskan s work .
vs .
and the difference lies out of the standard deviation range computed based on the cross validation which is .
.
we can conclude that pbrf achieves a better accuracy than the previous best result .
.
evaluation on python the python dataset also contains google code jam solutions.
it was collected and introduced by alsulami et al.
and consists of solutions to problems implemented by authors.
as in the c dataset this one contains the same number of samples for each author.
during cross validation the model is trained on problems and validated on other problems that are initially held out.
the best reported average accuracy is .
.
for comparison we use the average accuracy reported by alsulami et al.
for two models.
the first one is a novel model introduced by the authors.
the second one is an adopted version of the model by caliskan et al.
with c specific features removed.
on this dataset our models achieve .
pbnn and .
pbrf accuracy.
similarly to the c dataset pbrf shows a better accuracy compared to pbnn because the number of available samples is too small for efficient training of the neural network.
also in this case pbrf achieves an accuracy that is as good as the previous best result .
.
evaluation on java the java dataset introduced by yang et al.
consists of open source projects each authored exclusively by a single developer.
each project contains from to files with a median value of totaling files overall.
the number of samples per author varies.
for evaluation we split the dataset into folds and perform a stratified cross validation similarly to yang et al.
.
ideally to compare the accuracy of our model to theirs in the most precise manner we would use an identical split of the dataset into folds for our evaluation.
however the original split into folds is not available and we created our own with a fixed random seed.
the model by yang et al.
is a neural network trained either by stochastic gradient descent sgd or particle swarm optimization pso .
when trained by pso the model achieves an average accuracy of .
using fold cross validation.
both our models reach an accuracy of more than .
the previous result lies out of the standard deviation range in both cases thus indicating that the difference is statistically significant.
although the median number of samples per author is only the neural network shows a high accuracy.
even though the average accuracy of the neural network model is slightly higher than that of the random forest the statistical test yields a p value of .
.
also in this case both pbrf and pbnn improve the previous best result .
limitations of current evaluations during the analysis presented in section we realized a number of limitations posed by this evaluation technique.
particularly using a single accuracy metric to compare complex approaches to problems motivated by practical needs such as authorship attribution in software engineering is a one sided solution.
in this section we discuss the limitations of such evaluation.
academic work on authorship attribution is motivated by practical needs such as detection of plagiarism detection of ghostwriting and attribution of malware .
in a perfect world introduced authorship attribution approaches should be evaluated on real world data.
however such real worldconference july washington dc usa egor bogomolov vladimir kovalenko yurii rebryk alberto bacchelli and timofey bryksin data is privacy sensitive and seldom publicly available.
for this reason to show how models behave and compare against each other researchers create datasets from available data sources.
even though these datasets try to mimic the data found in practical applications there exist major differences.
to illustrate them we discuss the concept of developer s work context i.e.
the environment that surrounds programmers when they write code.
the work context includes but is not limited to the following parts of a codebase a codebase of a software project usually contains logically interconnected code that is organized into packages and modules.
this logical connection often implies a lower level connection observed in code calls of methods creation of objects similar names of entities.
project domains the domain of the task e.g.
an android application influences names used libraries implemented features and architectural patterns that are used in such projects more commonly.
projects the project itself might have internal naming conventions and utility components that are called from different parts of the codebase.
moreover some companies have their own style guides for programming languages that affect naming conventions formatting and preferred use of specific language constructions.
an example of this is google style guides .
set of tools integrated development environments ide or text editors version control systems as well as build and deployment tools may influence the way how developers write code.
for example a recent survey of programmers concluded that developers affect their development practices by simply using github in their projects .
this list is not complete and could easily be extended but the effect of even a single of the aforementioned individual examples of work context might be significant for the task of authorship attribution.
when collecting data for evaluation we should take work context into consideration practical applications of authorship attribution often imply that the model should be trained on code written in one context and tested in another or distinguish between developers working in the same context.
however datasets used in prior works do the opposite there is a difference between authors work contexts e.g.
different single authored projects and no difference between work contexts of the same developer in training and evaluation sets.
another concern is that existing datasets do not consider the impact of collaboration on the code.
all of them consist of projects developed by a single programmer.
however projects studied in the software engineering domain are usually developed by teams.
collaborative work is not reflected in prior work but it may introduce additional complexity for the authorship attribution task developers integrate their code into codebases written in collaboration with their colleagues which might make their code harder to distinguish.
developers individual coding practices may change over time .
it is reasonable to think that changes in coding practices e.g.
programmer s style used libraries naming conventions development process can influence the accuracy of the models significantly.
for existing datasets all code written by a single author belongs toroughly the same period of time e.g.
one project one competition or assignments belonging to one course and there is no temporal division between evaluation and training sets.
though for practical problems one might need to train a model on the historical data and apply it to new samples.
this temporal aspect may introduce potential significant difference in individual coding practices between code used for training and testing.
two prior studies consider evolution of programmers style as a challenge for authorship attribution .
burrows et al.
evaluated the difference between six student assignments showing that their coding style changes over time .
caliskan et al.
trained a model on solutions of google code jam gcj and evaluated it on a single problem from gcj .
their experiment did not reveal any major differences in accuracy compared to evaluating on a problem from the same gcj .
these results are contradictory but both studies operated with small datasets and in domains different from real world projects.
thus further research on this topic is required.
we conclude that there is a gap at least theoretical between the existing datasets and what can be collected and used in the context of real world applications.
in particular there is a difference in terms of work context effects of developer collaboration and changes over time.
in the following sections we suggest a novel approach to data collection that allows to quantitatively evaluate the impact of both temporal and contextual issues on the accuracy of authorship attribution models.
collecting realistic data to quantitatively evaluate the impact of dissimilarities between existing datasets and real software projects on the accuracy of authorship attribution techniques we developed a new approach to data collection.
it uses git2repositories as the data source and unlike existing datasets from open source data overcomes the limitation of a single author per project.
.
method of data collection we suggest a new approach to collecting evaluation data for authorship attribution models.
the approach works with any git project without restrictions on the number of developers.
in particular using git as the main data source allows taking data from github the world s largest repository hosting platform with more than million repositories and million users .
open source git repositories and github in particular are a uniquely rich source of data for modern software engineering research efforts .
the atomic unit of contribution in git projects is a commit.
usually a commit has a single author.
this authorship information associated with every change recorded in a git repository makes git a particularly rich source of data for authorship attribution studies.
the first step of our data extraction method is to traverse the history of a repository to gather individual commits.
then we need to identify commits authored by the same developer.
it is not a trivial task because a developer can work within one repository under multiple aliases using different emails.
even though there are prior studies on the problem of alias merging these methods either make assumptions or are probabilistic to some extent.
the attribution of source code a language agnostic approach and applicability in software engineering conference july washington dc usa main benefit of the existing alias merging methods is their complete autonomy which allows working with arbitrary amounts of data.
in this study we processed the intellij idea community repository by merging non stub names and emails that appeared together into groups.
then we accessed github to merge the groups that belong to the same person and contacted the developers to resolve all arguable cases.
we took this approach since we processed a single repository and wanted to avoid any mistakes.
for future researchers who may want to process larger amounts of data autonomously our data preparation pipeline supports using existing entity merging methods.
in the second step we split each commit into changes of fixed granularity e.g.
a change to a single class method or field .
in this study we use a method change as the granularity unit it is hard to precisely track changes of smaller fragments of code e.g.
lines or statements while using class level or higher granularity would leave us with less data points.
also authorship attribution at the method level can be a valuable task in the domain of clone detection where a method is a commonly studied unit .
within the scope of a single commit methods can be renamed or moved to other files.
we use gumtree to precisely track such changes in java code as well as simple changes to a method s body.
as a result we get a set of all method changes made during the project development.
afterwards the extracted data can be grouped into datasets with different properties.
.
collected datasets we implemented an open source tool supporting the described approach to data collection which is available in the supplementary package.
we used the tool to extract data from the intellij idea community edition project the second largest public java project on github.
at the time of processing march 10th the project contained about commits dated from to and authored by developers.
these commits comprise about two million individual method changes.
each change is of one of three types creation of a method deletion or modification of its body or signature.
the latter unlike method creations cannot be processed directly by authorship identification models newly added code fragments might be incomplete and the concrete modifications might be scattered across the method body.
moreover the author of the original code might be different from the one who modifies the method which makes it impossible to define a sole author of the method.
in the datasets designed in this work we only use method creations because they contain new code fragments implemented by a single person and can be labeled accordingly.
however attributing authorship of method modifications is an interesting task for future research which will require changing the models input e.g.
take two versions of code as an input or process the difference between the versions .
in the intellij idea community repository out of all the changes are method creations which is about a third of all the changes.
thus authorship attribution models that operate only with method creations can cover a large part of all the cases.
the developers highly differ by the amount of contributions most active developers created of all methods most active .
to reduce the imbalance of the datasets we split the authors in twogroups authors with at least created samples i.e.
created methods and authors who have between and samples.
in total the groups have about and samples respectively.
to quantitatively evaluate the impact of work context and evolution of coding practices on the quality of authorship attribution we further create several datasets from the collected data.
to make evaluation conditions as close to practical tasks as possible we should have processed several projects and split projects between training and testing sets.
however at this point it is unclear how to define the similarity between work contexts of different projects and we would not be able to run several experiments with an increasing degree of context difference to perform a quantitative evaluation.
thus this left us with one project and multiple datasets.
.
.
datasets with gradual separation of work context .the purpose of these datasets is to measure the influence of variation in developers work context on the quality of authorship attribution.
to achieve this we need multiple pairs of training and evaluation samples that differ only in their work context.
more specifically the pairs should contain the same code fragments but be split differently between the training and testing part.
to control the degree of difference in work contexts we use the project s file tree.
figure shows an example of such a tree.
it consists of folders with edges between a folder and its content.
leaves in the tree correspond to files.
for java code we are interested only in java source files identified by the .java extension.
to reduce the depth of the tree we compress paths of folders with a single sub folder into nodes in figure folders plugins src and main are compressed into a single node plugins src main .
this operation preserves the structure of the tree.
plugins src main root folder platform platform api platform impl p a p b impl a impl b api a api bp d p c figure an example of a project s file tree with similarities between files.
a file tree for a java codebase resembles the structure of its packages.
usually classes in one package are logically connected and refer to each other thus they have similar work contexts.
at a higher level of abstraction this also applies to classes in different sub packages of the same package.
in figure the tree class api a has a work context that is similar to api b because they are in the same package but a less similar context to impl a from platform impl .
nevertheless they are still much closer to each other than to any file from the plugins package since both are used to implement some platform features and may even depend on one another.
from this observation we derive a way to measure the similarity of work contexts of two files it is the depth of the lowest commonconference july washington dc usa egor bogomolov vladimir kovalenko yurii rebryk alberto bacchelli and timofey bryksin algorithm finding split of a dataset with selected separation of work context.
function randomsplit author depth testratio folders allfoldersatdepth depth randomshuffle folders train test emptyset emptyset for each folder folders do samples authorsamples author folder ifgetratio train test testratio then test.insert samples else train.insert samples return train test function splitauthor author depth minr maxr n testr minr maxr bestsplit none for each i range n do split randomsplit author depth testr ratio getratio split ifratio minr maxr then updatebest split bestsplit testr return bestsplit ancestor in the file tree .
in figure similarities between api a and other classes are shown with arrows the depth of the root is considered to be .
by so for a training validation split we can define the similarity of work contexts of these parts as the highest value of pairwise similarity between files in them.
using the maximum value might seem over cautious but in real world applications it is natural to assume that the model will not see any samples from another context during training before receiving them as input.
the subsequent step is to create a sequence of data splits with increasing similarity values or the depth of split.
algorithm shows the pseudocode for creation of such splits.
to preserve the distribution of authors at each level we pick splits for different authors independently and merge them afterwards.
we fix a ratio of evaluation samples the depth of split and an author.
then we collect all folders at the chosen depth and files at this depth or higher.
afterward we greedily divide them into training and evaluation parts trying to get as close to the chosen ratio of testing samples as possible.
when a folder is put into the training or evaluation part all the methods created by the author in the subtree of the folder go into this part.
we repeat the procedure several times for each author and depth and pick the split with the ratio closest to the fixed one.
if there is no split that has a ratio close enough to the fixed one e.g.
because the author worked only in a specific part of the project we remove the author from the dataset.
we created two datasets by applying the described algorithm to the method creations by the developers in two aforementioned groups.
the file tree in the intellij idea project has a depth of .
since the increase in similarity value from d 1todaffects only files with depth dand higher we vary donly in the range which includes over of the files.because of the restrictions on the ratio of training samples for some authors we could not find a suitable separation at every level.
for this reason we filter out samples from these authors.
the filtering left us with authors out of .
finally we obtained a dataset consisting of about samples by authors split at different levels.
at all the levels we have the same set of authors and code snippets with the only difference being the split between training and evaluation part of the dataset.
.
.
dataset with separation in time .these datasets are designed to investigate whether developers coding practices change over time.
the high level idea is as follows we pick a set of method creations from a project intellij idea in our case sort them by time split into ten folds then train a model on one fold and evaluate it on the others.
more specifically we gather all events of method creation generated by the developers in groups with different amounts of samples per developer.
then we sort all the methods written by each author by creation time and divide them into ten buckets of equal size.
to preserve the same distribution of the authors across the buckets we do the division independently for each programmer.
in the end most of the buckets for each developer contain methods written over to months.
we also evaluated an alternative approach of splitting all the methods simultaneously but this approach resulted in adding too much noise to the data in fact some programmers had joined the project later and the model trained on earlier folds did not have any information about them .
the resulting datasets consist of and events of method creation split into ten equal folds.
the events are sorted in time and the difference between the indices of their folds can be used as the temporal distance between them.
the distribution of the authors and the number of samples is uniform for every fold.
.
benefits of the data collection technique the proposed data collection technique enables gathering datasets that have several major benefits over existing datasets and capturing some important effects that are specific to real world data smaller gap between work context of code written by different authors.
while different developers still tend to work in different parts of the codebase naming conventions internal utility libraries and the general domain are the same for everyone since all code originates from the same codebase.
large number of samples available per author.
existing datasets mostly work with up to several hundred code fragments per author.
in the intellij idea dataset collected with our technique developers created more than ten thousand methods each.
the ability to collect multiple contributions for a single author makes the resulting data suitable for studying more fine grained aspects of authorship attribution such as the effect of the changes in coding practices over time on attribution accuracy.
broader domain of application.
since our data collection technique allows one to collect data from any git project it is possible to investigate cross project or cross domain authorship attribution.authorship attribution of source code a language agnostic approach and applicability in software engineering conference july washington dc usa evaluation on collected datasets we evaluate both our models described in section on the datasets that we created from intellij idea source code using the technique described in section .
also we adapt the model by caliskan et al.
to work with java code and refer to it as jcaliskan .
we compare jcaliskan to our models on the new datasets.
we also tried to reimplement the model by yang et al.
since the work lacks an open source implementation.
however we failed to reproduce the results mentioned in the paper .
in our case using particle swarm optimization to train the model led to no improvements and strong overfitting.
since features used in their paper resemble the work by caliskan et al.
we used jcaliskan for comparison.
.
separated work contexts first we work with separated work contexts section .
.
.
these datasets contain nine different training evaluation splits of the same large pool of method creation events labeled with the method s author.
each split is parameterized with a depth value which indicates the maximum possible depth of the lowest common ancestor of file changes in the training and evaluation sets.
if the model is sensitive to the influence of work context it should perform better for higher split depths because with the growth of the split depth training and testing sets become closer from the point of work context.
figure shows the dependency of the accuracy values of all three models on the split depth.
since the number of available samples per author is high and sufficient for proper training the neural network model pbnn outperforms both random forest models pbrf and jcaliskan for each depth split and for both developer groups.
figure shows that the accuracy values increase with the depth and at small depths the accuracy is much lower than in previous experiments see table .
we tried to eliminate possible reasons for that except for the difference in work context the experiments were held on the same data points the sizes of the training sets vary by less than and we train models until convergence.
thus we conclude that work context strongly affects the accuracy of authorship attribution.
.
time separated dataset to see if developers coding practices change in time which might affect the accuracy of authorship attribution models we evaluate our models on two collected datasets with folds separated in time section .
.
.
the datasets contain samples of method creation from developers who did between and method creations and developers who did at least method creations in the intellij idea project.
for each of the developers the data has been divided by time into ten folds of equal size.
this way we preserved the distribution of authors across folds eliminating all differences between folds except for the time when they were written.
we train a separate model on data from each fold except for the last.
then the models are tested on code fragments from subsequent folds.
thus for ten folds we get nine trained models and fold predictions.
we expect a lower accuracy for more distant folds if the developers practices change in time.
split depth0.
.
.
.
.
.
.
.60accuracy pbnn pbrf jcaliskan a authors to samples per author split depth0.
.
.
.
.
.45accuracy pbnn pbrf jcaliskan b authors at least samples per author figure models accuracy on the datasets with separation of work context.
the results for the models are presented in figure and figure .
the neural network pbnn performs on par with random forest models pbrf and jcaliskan for the developers with at least samples but falls behind when the number of samples per author decreases.
the graphs show that the accuracy of all models drops as distance in time grows which confirms our hypothesis evolution of coding practices affects the accuracy of authorship attribution.
.
evaluation on other projects we also held a preliminary evaluation on two other large java projects gradle and mule .
we applied the proposed data collection technique and created context separated and time separated datasets from each project.
the results are consistent with the evaluation on intellij idea models accuracy rapidly drops as we gradually separate contexts.
models accuracy strongly depends on the distance in time between training and evaluation folds.
pbnn outperforms both pbrf and jcaliskan when the number of available samples per author is high.
the detailed results and graphs are available in supplementary materials.conference july washington dc usa egor bogomolov vladimir kovalenko yurii rebryk alberto bacchelli and timofey bryksin evaluation fold number0.
.
.
.
.
.
.35accuracy fold fold fold fold fold fold fold fold fold fold a pbnn s accuracy evaluation fold number0.
.
.
.
.
.
.35accuracy fold fold fold fold fold fold fold fold fold fold b pbrf s accuracy evaluation fold number0.
.
.
.
.
.
.35accuracy fold fold fold fold fold fold fold fold fold fold c jcaliskan s accuracy figure models accuracy on the dataset with separation in time developers at least samples each .
lines are drawn for better readability and do not denote linear approximation.
discussion based on the evaluation results on the collected datasets we conclude that both the difference in work contexts between training and testing data and the evolution of developers coding practices have strong influence on the accuracy of authorship attribution models.
however our results are limited to java language and several large open source projects and need further research.
evaluation fold number0.
.
.
.
.
.30accuracy fold fold fold fold fold fold fold fold fold fold a pbnn s accuracy evaluation fold number0.
.
.
.
.
.30accuracy fold fold fold fold fold fold fold fold fold fold b pbrf s accuracy evaluation fold number0.
.
.
.
.
.30accuracy fold fold fold fold fold fold fold fold fold fold c jcaliskan s accuracy figure models accuracy on the dataset with separation in time developers to samples each .
lines are drawn for better readability and do not denote linear approximation.
.
influence of the work context in section we described an experiment with separation of the work context between training and evaluation sets.
we demonstrated that the model s accuracy decreases as we train and evaluate it on more distant in terms of the codebase s file tree pieces of code for each author.
specifically the accuracy might vary by almost two times as we divide the same samples differently figure .
we conclude that the gap between datasets used in previous work and the data observed in practical tasks is not negligible.
specifically authorship attribution of source code a language agnostic approach and applicability in software engineering conference july washington dc usa a model s accuracy can drop from in one setting to in another figure .
this suggests that for evaluation to provide realistic information about the potential accuracy of a solution on data from conventional software engineering projects researchers should use datasets where training and testing parts for each author belong to different environments.
the proposed dataset with gradual separation of training and evaluation sets can be used by researchers in future to measure their models tendency to rely on context related features rather than individual developers properties.
the models we evaluated turned out to have a strong dependency on work context as well with the accuracy dropping from to pbnn and from .
to .
pbrf for splits at depth and .
to lower the influence of work context on the model s accuracy researchers could design context independent features or add regularization terms.
.
evolution of developers coding practices evaluation on the dataset with samples of code from each developer split in time showed that as programmers coding practices evolve over time learning on older contributions to attribute authorship of the new code leads to a lower accuracy of attribution on a span of several months to years.
to maximize the accuracy in potential real world scenarios we should use training data that is as relevant as possible and re train or fine tune the models as we gather new data samples.
.
threats to validity the proposed technique of data collection does not take into account automatically generated code boilerplate code or code embedded by an ide.
its presence may bias the generated dataset.
in order to mitigate this issue the future work might employ techniques that detect such code and remove it from the dataset.
the collected dataset might contain code copied from thirdparty libraries which was not completely written by the developer stated as the author in the vcs.
recent studies on open source java projects show that up to of methods can be copied from other sources of java projects are clones of other projects and contain more than of cloned code .
it means that for a repository that is not a clone it is reasonable to expect that significantly fewer than of methods are copied from other places.
the rest of the methods should be enough for the models to infer information about the authors.
the experiment with gradual separation of work context relies on the proposed method to measure work context similarity as the depth of the lowest common ancestor in a file tree.
despite the provided rationale on why it is reasonable for some projects the similarity of code might be weakly related to the location in the codebase due to specific codebase organization practices.
we found that the evolution of developers coding practices strongly affects the accuracy of authorship attribution models.
however the observed drop in models accuracy can be caused in part by the evolution of the whole project instead of the individual programmers.
also the observed results are limited to the intellij idea data.
to extend them to a general case one needs to collect a dataset that comprises several projects with overlapping sets of developers with projects divided between training and testing sets.
conclusion authorship attribution of source code has applications in software engineering in tasks related to software maintenance software quality analysis and plagiarism detection.
while recent studies of authorship attribution report high accuracy values they use languagedependent models and do not assess whether their datasets resemble data from real world software projects.
we propose two models for authorship attribution of source code pbnn a neural network and pbrf a random forest model .
both models are language agnostic and work with path based representations of code which can be built for any syntactically correct code fragment.
our evaluation on datasets for c python and java used in recent work shows that the suggested models are competitive with state of the art solutions in terms of accuracy.
in particular they improve attribution accuracy on the java dataset from .
to .
pbnn and .
pbrf .
while demonstrating high accuracy existing work in authorship attribution is evaluated on datasets that might be inaccurate in modeling real world conditions.
this may hinder their adoption in software engineering methods and tools.
we discuss a concept ofwork context the environment that influences the process of writing code such as surrounding files broader codebase or team conventions.
taking work context into account there is significant dissimilarity with previous results.
another concern investigated in this work is the evolution of developers coding practices and its potential impact on accuracy of authorship attribution.
we suggest a novel approach to creation of authorship attribution datasets.
in contrast to prior studies that are limited to projects with a single author our approach can work with any git project.
we use our approach to process history of changes in a large java project intellij idea community repository on github and create several datasets to study the influence of work context and coding practices evolution on the accuracy of authorship attribution.
evaluation of three models on the dataset with separation of work context shows that the accuracy goes down as similarity values decrease.
as we gradually change the similarity level from maximal to minimal the accuracy drops to the low of which is much lower than achieved on the existing dataset of singleauthored projects.
for the experiment with folds divided in time the accuracy drops as the time difference between training and testing folds increases and the drop might also be significant over times for the most distant folds.
we conclude that programmers coding practices evolve over time at least in large projects and their evolution negatively affects quality of authorship attribution.
our study demonstrates that existing solutions to authorship attribution can perform very differently when existing datasets are put into conditions close to real world.
this should be taken into account when evaluating authorship attribution approaches especially during stages of data collection and training testing division.
all the artifacts related to this work are publicly available on github under mit license.