is historical data an appropriate benchmark for reviewer recommendation systems?
a case study of the gerrit community ian x. gauthier mcgill university montreal canada ian.gauthier mail.mcgill.camaxime lamothe polytechnique montreal montreal canada maxime.lamothe polymtl.cagunter mussbacher mcgill university montreal canada gunter.mussbacher mcgill.cashane mcintosh university of waterloo waterloo canada shane.mcintosh uwaterloo.ca abstract reviewer recommendation systems are used to suggest community members to review change requests.
like several other recommendation systems it is customary to evaluaterecommendations using held out historical data.
while history based evaluation makes pragmatic use of available data historicalrecords may be overly optimistic since past assignees mayhave been suboptimal choices for the task at hand or overlypessimistic since incorrect recommendations may have beenequal or even better choices.
in this paper we empirically evaluate the extent to which historical data is an appropriate benchmark for reviewer recom mendation systems.
we replicate the chr evand wlrr ecapproaches and apply them to reviews from the g errit open source community.
we then assess the recommendations withmembers of the g errit reviewing community using quantitative methods personalized questionnaires about their comfort levelwith tasks and qualitative methods semi structured interviews .
we find that history based evaluation is far more pessimistic than optimistic in the context of g errit review recommendations.
indeed while of those who had been assigned to areview in the past felt comfortable handling the review of those labelled as incorrect recommendations also felt thatthey would have been comfortable reviewing the changes.
thisindicates that on the one hand when reviewer recommendationsystems recommend the past assignee they should indeed beconsidered correct.
y et on the other hand recommendationslabelled as incorrect because they do not match the past assigneemay have been correct as well.
our results suggest that current reviewer recommendation evaluations do not always model the reality of software develop ment.
future studies may benefit from looking beyond repositorydata to gain a clearer understanding of the practical value ofproposed recommendations.
i. i ntroduction software repository data is an invaluable resource that accumulates as a software project ages.
version control systems vcss store commits that record changes to codebases.
issue tracking systems itss accrue issue reports that describe product defects enhancement requests and other potentialimprovements.
review tracking systems rtss archive thepeer code reviews that take place during software development.
mining software repositories msr approaches aim to uncover and communicate insights from historical softwarerepository data to support knowledge acquisition and futurestakeholder decisions.
a popular mechanism for communicat ing those insights is through recommendation systems .
for instance recommendation systems have been proposedto aid practitioners with code navigation taskprioritization and task assignment .
reviewer recommendation is a popular and recent variant of the task assignment problem where the recommendationsystem provides a list of potential reviewers ordered by theirrelevance for newly submitted change requests to an rts.
it is challenging to evaluate reviewer recommendation systems in software engineering settings .
an optimalevaluation would be reproducible allowing the research community to compare recommendation approaches in animpartial manner meaningful improving the practical implications of the evaluation and pragmatic making use of available data and resources to the largest extent possible.maximizing all three of these characteristics with a single typeof evaluation is impractical since they present inherent trade offs.
for example a highly meaningful evaluation would inviteactive stakeholders to weigh in on the suggestions that aregenerated by the recommendation system.
however such anevaluation would not be pragmatic since recruiting a largenumber of participants for such a study is difficult.
broadly speaking recommendation systems in software engineering are often evaluated using held out historicaldata .
after being trained using a subset of the availabledata the training corpus the recommendation system is testedusing another subset that was not used for training the testingcorpus .
in the context of reviewer recommendation systems the correct answer that the system is expected to generate isthe list of reviewers who appear in the historical data.
the optimal reviewer recommendation system according to a history based evaluation suggests exactly and only whathappened in the past.
since past decisions may not havebeen optimal relying on such a solution would encouragestakeholders to repeat past mistakes.
moreover kovalenkoet al.
argue that recommending popular past reviewers provides limited value in proprietary settings where suchreviewers are often well known.
in our estimation solelyrelying on historical records to evaluate reviewer recommen302021 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee dation systems suffers from two potential limitations.
first the evaluation may be overly optimistic since the solution that stakeholders applied may have been a suboptimal choice.second the evaluation may be overly pessimistic since the incorrect suggestions may have been as appropriate if notmore so than the solution that stakeholders had applied.
if history based evaluations are overly optimistic or pessimistic performance scores will overestimate or underes timate the value that the reviewer recommendation systemprovides.
therefore we pose the following central question is historical data an appropriate benchmark for reviewer recommendation systems?
as a concrete first step towards addressing this question in this paper we perform a case study of reviewer recommen dation in the context of the g errit open source community.
we conduct our study by a replicating the chr ev and wlrr ec reviewer recommendation approaches and b applying them to review records spanning the last twoyears of g errit development.
we classify the examples in the history based evaluation into correct and incorrect recommendations.
using repre sentative samples from each category we issue personalizedquestionnaires to reviewers in the g errit community.1these questionnaires asked reviewers to assess their comfort levelwith both reviews to which they were assigned i.e.
correct examples and reviews to which they were not assigned butwere recommended by a reviewer recommendation approach i.e.
incorrect examples .
by triangulating observationsfrom this quantitative data with qualitative data collectedfrom semi structured interviews with g errit stakeholders we address the following research questions rq1 optimism how often are past assignees that were deemed correct actually suboptimal?
we find that in the vast majority of cases review ers who were deemed to be correct were actuallycomfortable reviewing the given change.
indeed of reviews received comfort level ratings of either high or expert .
conversely only of the reviewsreceived a comfort level rating of either low or not comfortable .
this suggests that history basedevaluations of reviewer recommendation are notoverly optimistic.
rq2 pessimism how often are incorrect recommendations actually appropriate?we find that of reviews that were initially deemed incorrect received a comfort level rating of either high or expert from respondents.
only ofreviews received a comfort level of either low or no comfort .
these findings suggest that history basedevaluations of reviewer recommendation are overly pessimistic.
1we obtained institutional review board irb approval for this study from the mcgill research ethics board reb .broadly speaking our results suggest that the current method of evaluating reviewer recommendation systems isfar more pessimistic than optimistic.
indeed history basedperformance values are likely to be an under approximation ofthe true value that reviewer recommendation system sugges tions would provide to the g errit community.
in one sense this is a positive development suggesting that in general the performance of reviewer recommendation systems may beunder approximated in the literature.
in another sense under approximated performance scores in the literature may leadpractitioners to draw incorrect conclusions about the qualityof current reviewer recommendation systems.
the remainder of the paper is organized as follows section ii situates this work with respect to the literature onrecommendation systems in software engineering.
section iiidescribes the design of our case study while sections ivand v present the results.
section vi discusses the broaderimplications of our study results.
section vii discusses threatsto the validity of our study.
finally section viii drawsconclusions and proposes directions for future work.
ii.
b ackground andrelated work in this section we provide an overview of recommendation systems in software engineering generally section ii a andreviewer recommendation specifically section ii b .
a. recommendation systems in software engineering at their core most recommendation systems i.e.
systems that suggest to stakeholders future actions to take based on patterns or trends that have been mined from historical data operate in a similar manner.
a corpus of historical examples i.e.
the training corpus is provided as input to a supervised orunsupervised data mining approach e.g.
statistical or machinelearning regression clustering or classification techniques .this step produces a model which can produce recommendations for future examples.
while recommendation systemstake a variety of forms in our estimation the content discoveryand decision support variants which we describe below havereceived plenty of attention within the software engineeringdomain .
content discovery recommendation systems.
content discovery recommendation systems cdrss are at the core of solutions such as bug localization where availableinformation about a bug e.g.
issue report content is providedas a query to an engine that searches for matches in the codebase.
cdrss have also been proposed to guide developers asthey make changes to a code base suggesting other relevantlocations to view modify based on historical co change ten dencies or the recency of viewing modification .
decision support recommendation systems.
software practitioners make decisions to balance trade offs regularly.
although these decisions often impact their organizations itis not uncommon for decisions to be made based on intu ition.
decision support recommendation systems dsrss aid practitioners in making more data grounded decisions.broadly speaking these dsrss fall into two categories.
314beckham mylie ...id reviewer ... carlee ... 20samara molly ... leila ayana ... vincent ... ayana ... devyn ... dana ross ... ross samara ... victor ... juliette ... renee devyn ... 2ayana kenya ... lorelai ... 8londyn devyn ... israel ... 1kenya lorelai ... kenya ...id reviewer ... nick victor ... danica ... generate foldsmodel fitting train modelrecommendation systemid reviewer ... 1kenya lorelai ... 2ayana kenya ... devyn ... 4beckham mylie ... ayana ... dana ross ... danica ... 8londyn devyn ... israel ... nick victor ... lorelai ... leila ayana ... juliette ... kenya ... vincent ... renee devyn ... victor ... 18ross samara ... carlee ... 20samara molly ...model evaluation suggest reviewersid suggestion devyn nick 7danica londyn classify suggestionscorrect id suggestion nick danicaprepared?
incorrect id suggestion devyn londynprepared?
fig.
.
an overview of how historical data is split into testing and training corpora for evaluating a recommendation system.
effort prioritization dsrss help practitioners to allocate constrained resources in a cost effective manner.
for example defect prediction models can be used to prioritize thetesting of areas of the code base that are a most likelyto be defective b estimated to have the most defectsin the future or c estimated to have the highestdefect density .
moreover effort prioritization dsrss havebeen proposed to triage notifications that require stakeholderattention such as posts on a mailing list updates to issuereports or review requests on code review platforms .
expert assignment dsrss strive to support stakeholders in the allocation of tasks to qualified personnel.
expert assign ment dsrss have been proposed to suggest a team membersto whom new issue reports should be assigned b expertswho may be able to answer questions on developer q aforums and c reviewers to whom changelists should beassigned for code review .
evaluation of recommendation systems.
the evaluation of recommendation systems is not simple.
since future examples are not available and in situ evaluation is expensive and impractical recommendation systems in software engineeringare often evaluated using a corpus of held out historical data.figure shows how a corpus of changes is split into trainingand testing corpora for evaluating a reviewer recommendationsystem.
a subset of the data in the case of the figure is held out of the training corpus at the beginning.
thetraining corpus is then used to prepare the recommendationsystem.
each example in the testing corpus is then fed tothe recommendation system to gather its recommendations.if the recommendations do not appear in the actual list ofreviewers the change is considered correct incorrect .
however the examples labelled correct i.e.
the recommendations that appear in the actual list for a given change may not have been an optimal assignee for the task.
likewise the examples labelled incorrect i.e.
the recommendationsthat did not appear within the actual list may have been just ascapable of performing the task as the actual assignee s .
thisleads to four possible categories those considered correct and well suited for the task nick in figure those con sidered correct but not well suited for the task danica infigure those considered incorrect and not well suited forthe task devyn in figure and those considered incorrect but actually well suited for the task londyn in figure .b.
reviewer recommendation in recent years a lightweight and flexible variant of the practice of code review has become popular .
unlikethe rigid formal code inspections of the past modern prac tices embrace the asynchrony of global software developmentteams .
this lightweight variant often referred to as moderncode review revolves around a change based model .
foreach change to a system team members are asked to critiquethe premise structure and content.
investment in this reviewprocess has been shown to pay off in qualitative andquantitative ways .
assigning ill suitedreviewers to review a change can lead to suboptimal reviewoutcomes such as slow or less useful feedback .
considerable research effort has been invested in the problem of reviewer recommendation a variant of expert assignment dsrss which recommend appropriate personnel forreview tasks.
for example reviewbot and revfinder suggest reviewers based on how often they have contributed tothe lines or modules that were modified by the patch respec tively.
correct suggests reviewers with relevant techno logical experience and experience with other related projects.tie combines file location and patch content analyses tosuggest relevant reviewers.
ouni et al.
treat the reviewer recommendation problem as a multi objective optimizationproblem and solve it using evolutionary algorithms.
yanget al.
further specify what role the suggested reviewer should play e.g.
managerial technical .
yu et al.
explore reviewer recommendation in the context of github.
lipcaket al.
further analyze the level of success achieved by reviewer recommendation systems for github projects.
we select reviewer recommendation as an exemplar of a popular research area where history based evaluation iscustomary.
we use the recent chr ev and wlrr ec approaches as concrete reviewer recommendation solutions.
prior work has shown that history based evaluation of reviewer recommendation approaches may yield misleadingresults.
indeed kovalenko et al.
found that in the setting of large software organizations team members often knowwho should review their code limiting the usefulness of therecommendation list.
they argue that in such settings there isa need to move beyond accuracy measures when assessing thevalue of a reviewer recommendation system.
inspired by theirwork we set out to assess the agreement between historicalmeasures and developer perceptions.
32code review repository dc1 extract historical data dc3a train recommendation system pa1 identify candidate participants pa2 issue personalized questionnaires pa3 conduct follow upinterviews dc2 select assigned reviewers dc3b identify suggested reviewersdata collection perception analysis id reviewer ... gary ... sally ... alice ... ... ... ... suggestedassigned assigned reviewers unassigned suggestionsreviewer comfort ... gary ... sally ... alice ... ... ... ... candidate comfort ... bob ... alice ... jerry ... ... ... ...results recommendation system fig.
.
an overview of our study workflow.
iii.
s tudy design in this section we present our rationale for focusing our study on the g errit software community section iii a as well as our approach to the collection section iii b and analysis section iii c of data.
a. subject community we strive to analyze data from a large and active software community that actively invests in the practice of code review.
in selecting our subject community we identified threeimportant criteria that needed to be satisfied criterion track record of code review data.
to perform a robust history based evaluation of a reviewerrecommendation system a large amount of data aboutprior reviews must have accrued.
criterion traceable code review process.
reviewer recommendation systems often rely upon richhistorical context to produce suggestions.
for example revfinder reviewbot and xfinder requireaccess to version control history to compute code ten dency and experience heuristics.
thus the communitythat we select must provide review records that are welllinked to the commits on which they were performed.
criterion responsiveness.
our perception analysis relies on soliciting a large quantity of high quality data fromthe studied community.
an unresponsive community maynot provide a reliable and complete perspective.
we have found that the g errit community and by extension the data derived from the g errit project is able to satisfy all of our evaluation criteria.
in terms of criterion the gerrit project completed reviews from january to december .
in that same period of the commitsin the version control system have an accompanying reviewrecord in the review tracking system satisfying criterion2.
finally since the last author has fostered professionalrelationships and an awareness of code review research withinthe g errit community and because gerrit is a code review platform we believed that the g errit community would be amenable to and welcoming of our study which would in turnlead to a strong response rate satisfying criterion .
b. data collection the first step in our study is to collect data from the g errit community.
figure shows that the process involves the extraction of historical data dc1 from which the assignedreviewers can be selected dc2 .
we can use the historicaldata to train dc3a and evaluate dc3b our selected reviewerrecommendation systems.
we describe each step below.table i an overview of the yearly review activity of the gerrit community .the years with the shaded cell background are selected for our analysis .
year changes reviewed percent reviewed unique reviewers .
.
.
.
.
.
.
.
total analyzed .
dc1 extract historical data.
to obtain a corpus of data with which to train and test our recommendation systems wefirst set out to extract historical review data from the g errit community.
the community practices dogfooding by usingthe g errit software that the community develops.
thus we first set out to extract data from the community s g errit instance using the rest api.2for each change the extracted data includes a a timestamp b the author and reviewers and c the impacted files and lines.
table i provides an overview of the extracted g errit data.
the table shows that the community has been growing over theyears attracting more than contributions per year since2016.
in addition during this time a very large proportion ofthe changes made to the g errit system can also be linked to a review reported within the system.
while this highproportion of linked reviews would allow us to use all of thechanges and reviews available within the g errit system we choose to use only the data from and .3similarly to prior work we consider that more recent changes indicatemore familiarity with a given change.
indeed prior work has posited that the experience of a developer with regards toa given change should be weighted by its age e.g.
developerexperience for a change twice as old would be halved .
wetherefore seek to solicit feedback from participants about datafrom recent changes.
we use a two year period to balance thetradeoff between the quantity of data and limiting experiencedegradation since it may be difficult for the participant torecall their experience for older changes.
thus we chooseto focus our historical analysis on the reviews that occurredduring this two year period.
2gerrit review.googlesource.com 3the bulk of the data collection and analysis for this work was conducted in .
hence data from was not included.
dc2 select assigned reviewers.
history based evaluation of reviewer recommendation systems requires the list of team members who reviewed each change.
to obtain this list wefirst extract the user ids listed as reviewers of each change.
we apply both reviewer and review filtering to clean the raw extracted lists of reviewers prior to training our recommenda tion systems.
first we remove users from reviewer lists whoare known to be accounts that represent automated bots.
forexample the gerritci account is associated with the g errit continuous integration system not a human reviewer.
thefirst author also manually verified the emails first names andlast names within the data to merge any accounts that couldbelong to the same individual no such instances were detected.second table i shows that a small percentage .
of thechanges made to the system during the study period were notreviewed.
since these changes cannot be used to train or testour recommendation system we filter them out of our corpus.a corpus of reviews spanning unique reviewerssurvives the filtering process.
dc3a train recommendation system.
there have been several reviewer recommendation systems proposed in theliterature see section ii .
we choose to re implement chr ev and wlrr ec since they are recently proposed high performance recommendation approaches.
the chr evapproach produces suggestions by computing a user score for each file in the system based on how frequentlythe user has participated in reviews containing the file andhow recently they have done so.
users are given a score basedon the proportion of the total number of comments on a file across all reviews that the user has made.
users are alsoscored based on the number of days in which they have madea comment to a file as a proportion of the total number of daysin which the file has received review comments.
finally usersare scored based on the most recent day that they commentedon a file during a review relative to the most recent commentmade to that file by any user.
meanwhile wlrr ecuses multi objective optimization to consider five input measures code ownership reviewingexperience patch author familiarity review participation rate and a metric representing the review workload.
the first fourmeasures are maximized to increase the chance of participatingin a review and the last measure is minimized to reduce work load.
wlrr ecrecommendations are not ranked.
more details for each approach can be found in the original chr ev and wlrr ec papers.
dc3b identify suggested reviewers.
for chr ev the user scores are used to identify reviewers to suggest.
for eachreview in the testing corpus expertise and recency scoresare computed for each potential reviewer and a ranking isproduced.
for wlrr ec the output is a list of potential reviewers which are all considered to be as equally valid.
to evaluate the performance of these recommendation systems we aim to compare the history based evaluation scoresreported in the literature with those that our models achieve.to do so we need to split our historical data into training andtesting corpora.
the selection of the testing corpus is criticalbecause it is the basis from which we will sample reviewers forour perception analysis.
we allocate the latest changesto the testing corpus of the data set .
rather than randomly sampling reviews we allocate the latest changesbecause this most closely matches the evaluation scenarioin reality.
moreover to avoid other impractical evaluationscenarios only those reviews that were completed prior tothe creation of the review being evaluated are used by therecommendation system.
a random sampling of training andtesting corpora may create an unrealistic scenario where futuredata e.g.
changes from is used to suggest reviewers forpast events e.g.
changes from .
to identify the suggested reviewers for chr ev we need to set a cutoff value kfor the ordered list of suggestions.
to compare with the initial chr evstudy we experiment withk 5settings section iv .
we use the most stringent k setting when selecting reviewers for our perception analysis section v to avoid overburdening oursubject community with unnecessary survey requests.
c. perception analysis after collecting the recommended and assigned reviewers we perform our perception analysis.
figure shows that the analysis is composed of identifying candidate participants pa1 issuing personalized questionnaires pa2 and conduct ing follow up interviews pa3 .
below we describe each step.
pa1 identify candidate participants.
we begin by comparing the recommended reviewers see dc3b to the lists of actual reviewers see dc2 .
we label each review with one oftwo categories.
if the suggested reviewer appears in the listof actual reviewers we label that change and its suggestedreviewer as correct .
alternatively if the suggested re viewer does not appear in the list of actual reviewers we labelthat change and the suggested reviewer as incorrect .
this first step generates a stratified data set from which to sample i.e.
a set of correct changes and a set of592 incorrect changes.
we then apply stratified samplingto each category to select a representative set of changesfor further evaluation.
we draw a representative sample toachieve a confidence level of with a confidence intervalof from each category independently.
a sample size calculation indicates that we need correct changes and233 incorrect changes.
to satisfy our sample size we began by randomly selecting changes from each of the categories.
however because asmall number of reviewers perform a large proportion ofthe reviews we found that random sampling would lead toa sample that over represented and overburdened the mostactive g errit reviewers.
placing too large of a burden on very active reviewers would make them less likely to participate.moreover it increases risk since a lack of response fromsuch a reviewer would produce a large gap in study data.for these reasons we set an upper limit of reviews percandidate participant.
more specifically we continue to drawsamples randomly until we reach our target sample size i.e.
correct and incorrect while ensuring that the 34upper limit is not exceeded for any reviewer.
in the end the samples that we drew include candidate participants ofwhom are present in both correct and incorrect lists andthree and of whom are only present in the correct and incorrect lists respectively.
pa2 issue personalized questionnaires.
with the two samples of changes obtained we create the personalizedquestionnaires for each candidate participant.
to ensure thatthe study is consistent for all reviewers involved we cre ate a template for the questionnaire.
for each change theparticipants are asked how prepared they were or wouldhave been to review the change on a five point semanticscale whether they know of any other reviewers who mighthave been a good reviewer for the given change and if theyhave any specific comments about the change which theyfelt were important to note.
the questionnaire provides theoption to stop and submit the questionnaire after any changein the hope that this will deter candidates from abandoning thequestionnaire without submitting anything.
we emailed eachcandidate participant individually soliciting their feedback viathe personalized questionnaire.
we then use the responses tothese questionnaires to answer our rqs.
a template for ourquestionnaires is available online .
since reviewers with plenty of expertise will generally feel comfortable with most reviews we solicit participation fromusers with varying rates of prior review participation withinthe g errit community.
figure a shows the distribution of the number of reviews that our candidate participants per formed within the two year study period.
the expertise of ourreviewers broadly varies some of whom performed very fewreviews the first quartile is reviews and others performedseveral reviews the third quartile is reviews .
we believethat this provides an accurate cross section of the g errit community and reduces the likelihood of skew due to the over confidence of individual reviewers.
we used a two tailedmann whitney u test to determine whether the distributionsof both populations i.e.
reviewers who performed reviews andrespondents to our survey are equal.
we obtained a p valueof .
and therefore accept the null hypothesis that thesamples are not drawn from statistically distinct populations.in addition figure b shows the distribution of the samemetric for those who responded to our questionnaire.
thecandidate and respondent plots are visually similar leadingus to conclude that reviewers with a similar cross section ofexpertise actually participated in our questionnaire.
pa3 conduct follow up interviews.
having obtained the results of the questionnaire outlined within pa2 we con duct a series of follow up interviews intended to tackle keyemergent themes from within the results of the questionnaire.to this end we reach out to a group of respondents to thequestionnaire who showed interest in the content of the studyand or included responses that we felt would benefit fromfurther elaboration.
in total we contacted nine members ofthe g errit community with six agreeing to participate five over teleconference and one over email.
table ii shows thatwe interviewed participants with different roles within g erritfig.
.
the distributions of the number of reviews that had been performed in the studied period by each of the a candidate respondents and b actualrespondents.
the median first and third quartiles are labelled.
table ii a n overview of the participants in our interviews .
participant participation mediumrole within gerrit p1 email maintainer p2 video call contributor p3 video call contributor p4 video call maintainer p5 video call contributor p6 video call contributor including maintainers and users without a specified position experienced reviewers and new members of the community.
we apply a semi structured approach to our interviews.
the first and last authors conducted and were present for all ofthe interviews.
an outline of key questions and interviewtopics were created prior to the interview session but inter viewers and interviewees had the freedom to follow relevant divergent topics if they arose based on responses.
questionswere initially formulated based on trends in the questionnaireresponses as well as the individual interviewee responses.
the interviews were recorded transcribed and coded to extract themes.
we use these themes to deepen the insightsthat we glean from our questionnaires.
as such we code thetranscripts based on whether the responses relate specificallyto our research questions.
when responses relate directly to aresearch question we check if multiple community membersmention similar points regarding the same topic.
in the cases inwhich they do we report these findings along with the partici pants who stated them participants are labeled as p1 p6 whenoutlining their responses in order to corroborate or contradictthe observations that we draw from the questionnaire.
35iv .
p reliminary ev aluation to test if our recommendation system implementations are on par with the original implementations we evaluate our replicated solutions on a sample of historical data.
a. experimental setup sample size.
we test both approaches on g errit changes to mimic the testing corpus size outlined in the original chr evpaper where three of the four systems tested had sample sizes of roughly changes.
furthermore sampling the most recent changes allows for the largest possiblemodel to be created in each case.
we therefore use changesto provide a fair comparison with the original paper.
evaluation metrics.
to test the recommendation systems in a comparative manner we use the same evaluation metrics as the original chr evpaper i.e.
precision recall f1 score and mean reciprocal rank mrr .
for a given review x the precision recall and f1 score are precision x suggested x actual x suggested x recall x suggested x actual x actual x f1 score x precision x recall x precision x recall x where suggested x is the list of suggested reviewers for x and actual x is the list of team members who reviewed x. precision recall and f1 score are rank agnostic performance measures which ignore the position at which a revieweris suggested.
mrr is a rank aware measure that accounts forthe rank at which correct suggestions appear.
the higher therank of correct suggestions the better larger the mrr value.a perfect mrr of one indicates that for all reviews thefirst suggested reviewer appears in the list of actual reviewers while mrr values that approach zero would imply that the correct suggestions rank near the bottom of the list.
morespecifically mrr is defined as mrr n n summationdisplay i rank i wherenis the number of reviews on which the recommendation systems are evaluated and rank iis the position of the first actual reviewer in the list of suggested reviewers.since wlrr ecdoes not rank candidates this metric is only computed for our chr evimplementation.
b. experimental results table iii shows that our implementation of chr evis able to achieve a similar level of performance to that ofthe original implementation.
for all measures and studiedksettings our implementation achieves performance scores within percentage points of those reported in the original paper suggesting that our replication is largely successful.
similarly table iv shows that our implementation of wlrr ecis able to achieve similar results to those of the originaltable iii comparison of c hr ev on the gerrit value project compared to the original s performance on the android platform .
kprecision recall f1 score mrr value value value value .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table iv comparison of wlrr ec on the gerrit project value compared to the original s performance on the android platform .
evaluation typeprecision recall f1 score value value value actual .
.
.
.
.
.
potential .
.
.
.
.
.
implementation.
the actual and potential reviewers rows reflect the different choices of ground truth data as describedin the original paper .
although the recall values are muchlarger in our actual reviewers ground truth setting webelieve that the consistent values for all other fields suggestthat our replication was successful in this case as well.
webelieve that the differences in results are likely due to thedifference in the average size of the list of actual reviewersfor each change and the differences in datasets.
we find that of the wlrr ecrecommendations overlap with the chr evrecommendations with the k setting.
since higher ksettings reduce the overlap k k k we concentrate our analysis on the k setting.
due to the large overlap in results and because the chr evresults are ranked and therefore allow us to target specific developers from within the recommendation list wechoose to use the recommendations from chr evto select participants for our questionnaires and interviews.
v. s tudy results in this section we describe the results of our study with respect to our research questions.
for each question wepresent our rationale followed by our approach for addressingthe question and finally we present the results.
rq1 optimism how often are past assignees that were deemed correct actually suboptimal?
motivation.
when performing an evaluation of the correctness of a reviewer recommendation system researchers often count a recommendation as correct when the reviewer suggested bythe system for a past change actually performed the review.however the reviewers who reviewed past changes may not bethe best choice to perform the review.
for example reviewersmay participate in a review to learn about a module or tocover for the optimal reviewer who is unavailable at the time ofthe review.
these non technical factors may introduce noise inthe correct signal recorded in past review participation.
this 36table v an outline of the frequency of responses for both correct and incorrect changes comfort scorecomfort score frequency correct changes incorrect changes type of noise would lead history based evaluations to be overly optimistic over estimating the true performance of a system since some of the cases labelled as correct by the evaluationmay actually be incorrect.
hence we set out to quantify andcharacterize the optimism of history based evaluations.
approach.
to address rq1 we select the sample of correct cases i.e.
where the suggested reviewer matches a reviewer who performed a past review.
we follow oursampling strategy from section iii c see step pa1 .
foreach unique reviewer in our sample we create a personalizedquestionnaire which consisted of one page for each review inour sample.
each page asked the participant to a rate howcomfortable they were when performing the review in question on a five point semantic scale b optionally recommendanother team member who may have been able to perform thereview and c optionally provide their rationale for eitheror both of their prior responses free form text .
we use thesemantic scale responses to gain an overview of the optimismof history based recommendation evaluations.
we then codethe free form responses using an open coding approach .we use the most frequent codes to generate hypotheses aboutthe nature of misassigned reviews.
we test the validity of thesehypotheses qualitatively during follow up interviews see steppa3 in section iii c and quantitatively using spearman s rank based correlation coefficient when appropriate.
results.
we contacted candidate participants of various levels of seniority within the g errit community.
we received twelve responses a response rate of .
these responses contained assessments of of the subject changes .
it is rare for past assignees who were deemed correct to have been uncomfortable with their past reviews.
table v shows the distribution of comfort score responses.
the meancomfort level for the changes that received responses is4.
.
moreover in of those cases the level ofconfidence reported by respondents was four or five.
sinceour sample contains instead of cases and our originalpopulation is correct changes with a confidence level of95 our confidence interval unfortunately expands to .
.nevertheless this bodes well for history based evaluationsof review recommendation since our responses suggest thatcurrent ground truth approaches are not overly optimistic.
the follow up interviews further corroborate these findings.
multiple interviewees p5 p6 mentioned that they lookthrough a change s contents and the other reviewers alreadyworking on a change to determine whether they will bringvalue to a review discussion before accepting a review.
onereviewer p6 stated that they often use about a quarter of thetotal time spent on a change figuring out if they will providevalue.
this suggests that when reviewers accept a review theyare rarely a bad candidate for evaluating it.
comfort score values are difficult to estimate.
in their free form responses respondents often mentioned that theyconsider the change size and its type e.g.
merge vs. normalcommit when considering whether they would be an appro priate reviewer.
thus we tested for correlations between sizeheuristics number of changed lines files and change type i.e.
merge commit or not and the comfort scores reported.for the number of lines changed and files changed we observeweak to negligible correlations of .
p .
and .
p .
respectively.
while the number of files has a statistically significant correlation p .
its magnitude is considered weak .
finally ofthe changes with a response were merge commits.
themean comfort score for merge commits was actually slightlyhigher .
than non merge commits .
.
we perform anunpaired two tailed mann whitney u test a non parametricstatistical hypothesis test to compare the comfort scores inthe merge and non merge groups.
the results do not supportthe rejection of the null hypothesis that both groups are drawnfrom the same population p .
.
moreover the cliff s delta an effect size measure is .
which indicates thatthe practical difference between the samples is negligible.
during follow up interviews all participants mentioned encountering trivial changes when filling out the questionnaire.reviewing these trivial changes does not require the samerigour.
however the interviewees did not share a commondefinition of what constitutes a trivial change.
definitionstouched upon a which components a change modifies p1 b how many stakeholders are potentially impacted p1 c the content of the change i.e.
whether it is routinemaintenance which can be reviewed by a larger group thannew features added to the system p6 and d the amount ofdiscussion that is garnered from the community p4 .
intrigu ingly two interviewees felt that merge changes are generallytrivial to review p2 p4 while another interviewee felt thatmerge commits are rarely trivial and often required a carefulreview to catch potential regressions p5 .
this general beliefthat a set of trivial changes exists without a clear definitionconverges with our findings from the questionnaire.
indeed many of the reviewers who participated in the questionnairefelt very comfortable reviewing a large set of changes buta clear pattern of what caused certain changes to be morebroadly reviewable than others did not emerge.
reviewers tend to carefully select the reviews that theyaccept.
thus reviewers were often highly comfortablewith reviews that reviewer recommendation evaluationswould deem correct recommendations.
this suggeststhat optimism is not a serious problem in the context ofthe evaluation of reviewer recommendation systems.
rq2 pessimism how often are incorrect recommendations actually appropriate?
motivation.
reviewer recommendation evaluations often consider a recommendation to be incorrect when the reviewer recommended by the model did not review the change.
how ever the results from rq1 suggest that reviewers who areknowledgeable about a system will likely have many changesfor which they are the most qualified reviewer.
since theymust balance reviewing time with other tasks reviewers maynot be able to participate in the review process of everychange for which they are suitable.
thus relying solely onthe reviewer who performed the review as the ground truthmay underestimate the value of a reviewer recommendationmodel.
to assess this we set out to study if the ground truthdata for reviewer recommendation systems is too pessimistic.
approach.
similar to rq1 we contact the reviewers whose changes had been sampled see section iii c .
however unlike rq1 the changes that we sampled for rq2 are changesfor which the top ranked reviewer suggested by the recom mendation system did not perform the review.
as was done forrq1 we apply an open coding procedure to code the free formsurvey results and then perform quantitative and qualitativeevaluations of the most commonly occurring themes usingcorrelation analyses and follow up interviews respectively.
results.
we contacted reviewers responded .
the responses cover of the targeted changes .
a large proportion of the recommended reviewers that are labelled as incorrect would have been comfortable assessing the given change.
table v shows the distribution of the comfort score responses.
the mean comfort score forthe changes is .
.
moreover of received acomfort score of or .
unfortunately since our sample sizeincludes instead of changes with an initial populationsize of incorrect changes and a confidence level of our confidence interval increases to .
.
nonetheless theresponses still suggest that a large proportion of incorrect reviewer recommendations are actually reviewers who wouldhave been comfortable to assess a given change.
the follow up interviews suggest that reviewers often have reasons other than their comfort level for not performingreviews.
three interviewees p1 p2 p5 mentioned that theyare quite often not able to review all of the changes they wishto due to time constraints.
these interviewees emphasized thata review takes a considerable amount of time to perform welland thus when there are a large number of changes withintheir area of expertise they will not be able to review all ofthem.
this suggests that the current approach to building aground truth for reviewer recommendation is likely overlook ing suitable candidates.
the difference in the comfort scores of correct and incorrect recommendations is not large.
indeed the difference in mean comfort score between correct rq1 and incorrect rq2 cases is .
.
moreover the differencein the proportion of reviews for which the reviewer is com fortable providing a comfort score of or is only twelvepercentage points.
an unpaired two tailed mann whitney u test comparing the comfort scores in the two groups indicatesthat there is a statistically significant difference p .
however the cliff s delta is considered small .
.
while these are non negligible differences their differencein magnitude is exaggerated in the evaluation of reviewerrecommendations where one case will be considered correctand the other incorrect.
the typical ground truth approaches do not account for the way in which reviews are handled in practice.
four of the six interviewees p1 p4 p5 p6 mentioned that thechoice to review a change is not solely about having completeknowledge of a change s content.
they involve themselves inreviews to force themselves to learn more about the codebaseor to steer the evolution of the project.
treating a reviewer as correct or incorrect may overlook such nuanced reasonsfor a community member s participation on a review.
review size does not share a significant correlation with the comfort score.
similar to rq1 we find participants often mentioned that size was a determining factor in theircomfort level.
however when we measured the correlationbetween common size heuristics and the reported comfortscore we do not observe any strong quantitative evidence ofthat relationship.
indeed the number of lines of code changedby a patch and the comfort score share a negligible statisticallyinsignificant negative correlation .
p .
.
moreover the number of files changed and the comfort scoreshare a weak statistically insignificant negative correlation .
p .
.
recommended reviewers are often considered incorrectwhen the suggested reviewer did not perform the review.however we find that in the majority of such cases therecommended reviewers were actually quite comfortableto review those changes.
indeed the difference in thecomfort scores of correct and incorrect recommen dations is small.
vi.
p ractical implications in our estimation our study has three key implications for researchers in the areas of reviewer recommendation andrecommendation systems for software engineering.
a. reviewer recommendation current evaluation procedures systematically underestimate the performance of reviewer recommendation systems.
our findings indicate that reviewers who performed the review rarely felt uncomfortable with the task rq1 .this suggests that the common ground truth for reviewer rec ommendation evaluations i.e.
the reviewers who performedthe task are generally sound.
perhaps more interestingly our findings also indicate that suggested reviewers who didnot perform reviews were rarely uncomfortable with thosetasks rq2 .
this suggests that history based evaluations willunderestimate the performance of a reviewer recommendationsystem since such cases will be labelled as incorrect .
38recommending the correct reviewer may not be the best way to evaluate reviewer recommendation systems.
our participants pointed out that they participate in reviewsfor various reasons that do not relate to being the areaexpert.
for example they use assigned reviews as a forcingfunction to learn about an area of the codebase rq2 .
whilethere is a growing literature illustrating the non technicalbenefits and challenges of code reviewing practices its impact on reviewer recommendation is not yetfully understood.
indeed kovalenko et al.
observe that recommended reviewers rarely provide value for the authorsof changes.
since we observe that reviewer recommendationsystems rarely suggest reviewers who are uncomfortable withthe reviewing task rq1 rq2 we believe that identifyingthe correct reviewer is a relatively easy target to achieve in fact our data suggests performance in the literature islikely being underestimated.
instead we recommend that thenext goal for reviewer recommendation systems should be tobalance the reviewing workload while optimizing for team based constraints.
a recent example of such an approach wasexplored by mirsaeedi and rigby who propose methodsto mitigate turnover risk i.e.
the cost associated with thedeparture of a team member by simulating the reassignmentof reviewers to different reviewing tasks.
b. recommendation systems for software engineering the status quo history based evaluations may not be appropriate.
history based evaluations are almost ubiquitous in evaluating recommendation systems in software engineering.
while our findings are specific to the reviewer recom mendation problem we suspect that they could generalize to at least most other expert assignment dsrss.
nonetheless our findings serve as an existence proof that history basedevaluations are imperfect.
based on our results we recommendthat researchers evaluate the appropriateness of a history based evaluation using qualitative as well as quantitativeevaluations before adopting it.
vii.
t hreats to validity below we discuss the threats to the validity of our study.
a. construct v alidity threats to construct validity jeopardize the certainty that an operationalization measures what it set out to measure.
toconduct our study we reimplemented two reviewer recom mendation approaches.
while we believe that they are validreimplementations we did not have access to the originalsource code and could only implement the systems based onwhat was described in their respective papers .
as aresult it is possible that our reviewer recommendation systemsare not exact replicas.
on the other hand our preliminaryevaluations achieved performance scores that were similar tothose reported in the original papers.
in addition our sampling procedure for our questionnaire analysis has limitations.
first we chose to limit the numberof reviews to which a given contributor was asked to respond.while we took measures to ensure the set of sampled changesmeaningfully represents the overall set of reviewed changes limiting the number of reviews was a hindrance.
second we only evaluated recommendations from the k setting i.e.
quantity of suggested reviewers .
while we understandthat another ksetting may be more reflective of what is done in the gerrit community we selected k to avoid overburdening our participants.
we verified the influence onthe results of the two approaches by testing various kvalues for wlrr ecand chr evand found that larger kvalues still contained high overlap between the two recommendationsystems.
therefore the k setting allowed us to reduce the burden on the community while having a limited impact onthe study scope.
furthermore each unit increase in kwould require more requests for information from reviewersto retain our sample size.
these additional requests riskedantagonizing the community potentially reducing our overallresponse rate.
we attempted to mitigate this issue by not onlyconducting a survey but also conducting developer interviewsto get clear insights from the developers.
we believe that thisdecision was warranted as we have been able to achieve ahigh response rate of reviewers in the optimistic caseand in the pessimistic case participated .
in addition several participants could only respond to a subset of the changes that we selected for them i.e.
of optimistic sampled reviews and of pessimistic sam pled reviews received responses.
while this did expand ourconfidence intervals i.e.
was expanded to .
and .
respectively we were still able to computemeaningful estimates.
we acknowledge that our sample sizescould have been calibrated to reflect prior software engineeringsurvey response rates.
however we believe that this threatwas mitigated by our comparatively high response rate andbecause we were still able to compute meaningful estimatesfrom the data that we did obtain.
finally we rely on the self assessments of the comfort levels of our respondents to estimate their suitability for tasks.
sinceself assessments may not accurately reflect true ability levels these values may be skewed.
b. internal v alidity threats to internal validity pertain to alternative hypotheses that could also explain the results that we observe.
one such threat is that the most experienced reviewers will generallyfeel comfortable reviewing most changes to the g errit code base.
we acknowledge this threat and strive to mitigate it bysoliciting participation from a broad cross section of partici pants with varying expertise levels from within the g errit community see section iii c .
c. external v alidity threats to external validity have to do with the generalizability of our conclusions to other contexts.
due to the practical cost of our analysis procedure we chose to focusour study on one community.
we acknowledge that the specificcharacteristics of the gerrit community may have influenced 39the overall results of the study and thereby affect the generalizability of our conclusions.
however we believe that thefindings presented in this paper apply to a diverse selectionof developers because our questionnaire participants and in terviewees are employed by several organizations includingmultinationals like google apple and ericsson as well assmall to medium sized enterprises like gerritforge.
while theyall participate in the g errit community their perspectives are informed by the internal reviewing processes at their employersites.
nevertheless replication of our study in the context ofother organizations may prove useful.
viii.
c onclusion and future work like any system reviewer recommendation systems require evaluation approaches to understand how well they are able toperform.
the most prominent evaluation scheme for recom mendation systems in software engineering relies heavily onhistorical records to act as a ground truth on which to test.
thisground truth may lead to incorrectly labelled recommenda tions as in the context of reviewer recommendation reviewerswho reviewed past changes may not have been comfortable so.
in addition candidates who were well prepared toreview a change may not have had the opportunity to do so.
hence in this paper we investigate the following question is historical data an appropriate benchmark for reviewer recommendation systems?
through a case study of the g errit community we find that the answer is no because the current ground truth is overly pessimistic and oftenmislabels reviewers as incorrect when they would havebeen comfortable reviewing the change.
the difference in comfort level of reviewers labelled correct and incorrect by the original ground truthis small mean comfort scores of .
vs .
mann whitney u test p .
small effect size .
.
these results suggest that recommending correct reviewers may not be a difficult hurdle to clear.
this might ex plain why reviewer recommendations usually do not providevaluable support for developers in commercial settings .however we do not believe that reviewer recommendationsystems are without an application.
by pivoting the goal ofsuch recommendation systems away from identifying correct reviewers to aiding with other non technical goals such asminimizing the workload of the core team or mitigatingthe risk of knowledge loss we believe that reviewerrecommendation systems may find a more useful niche.
r eferences w. h. a. al zubaidi p. thongtanunam h. k. dam c. tantithamthavorn and a. ghose workload aware reviewer recommendation using a multi objective search based approach in proceedings of the 16th acm international conference on predictive models and data analytics insoftware engineering ser.
promise .
new york ny usa association for computing machinery p. .
j. anvik l. hiew and g. c. murphy who should fix this bug?
in proceedings of the international conference on software engineering pp.
.
v .
balachandran reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommenda tion in 35th international conference on software engineering icse pp.
.
t. baum o. liskin k. niklas and k. schneider a faceted classification scheme for change based industrial code review processes in2016 ieee international conference on software quality reliabilityand security qrs pp.
.
t. baum and k. schneider on the need for a new generation of code review tools in product focused software process improvement pp.
.
o. baysal o. kononenko r. holmes and m. w. godfrey investigating technical and non technical factors influencing modern code review empirical software engineering vol.
no.
pp.
.
c. bird and a. bacchelli expectations outcomes and challenges of modern code review in proceedings of the international conference on software engineering.
ieee may .
.
available a. bosu m. greiler and c. bird characteristics of useful code reviews an empirical study at microsoft in proceedings of the international conference on mining software repositories pp.
.
m. choetkiertikul d. avery h. k. dam t. tran and a. ghose who will answer my question on stack overflow?
in proceedings of the australasian software engineering conference pp.
.
n. cliff dominance statistics ordinal analyses to answer ordinal questions psychological bulletin vol.
no.
p. .
d. cubranic g. c. murphy j. singer and k. s. booth hipikat a project memory for software development ieee transactions on software engineering vol.
no.
pp.
.
m. e. fagan design and code inspections to reduce errors in program development ibm systems journal vol.
no.
pp.
.
j. garcia gathright c. hosey b. s. thomas b. carterette and f. diaz mixed methods for evaluating user satisfaction in proceedings of the 12th acm conference on recommender systems ser.
recsys .new york ny usa association for computing machinery p. .
i. gauthier m. lamothe g. mussbacher and s. mcintosh is historical data an appropriate benchmark for reviewer recommendation systems?a case study of the gerrit community aug .
.
available contribution is historical data anappropriate benchmark forreviewer recommendation systems acase study ofthegerrit community p. j. guo t. zimmermann n. nagappan and b. murphy characterizing and predicting which bugs get fixed an empirical study of microsoftwindows in proceedings of the international conference on software engineering pp.
.
a. e. hassan and r. c. holt the top ten list dynamic fault prediction in proceedings of the international conference on software maintenance pp.
.
w. m. ibrahim n. bettenburg e. shihab b. adams and a. e. hassan should i contribute to this discussion?
in proceedings of the international conference on mining software repositories pp.
.
h. kagdi m. hammad and j. i. maletic who can help me with this source code change?
in ieee international conference on software maintenance pp.
.
y .
kamei and e. shihab defect prediction accomplishments and future challenges in proceedings of the future of software engineering track of the international conference on software analysis evolution and reengineering pp.
.
y .
kamei e. shihab b. adams a. e. hassan a. mockus a. sinha and n. ubayashi a large scale empirical study of just in time qualityassurance ieee transactions on software engineering vol.
no.
pp.
.
m. kersten and g. c. murphy mylar a degree of interest model for ides in proceedings of the international conference on aspectoriented software development pp.
.
b. p. knijnenburg m. c. willemsen z. gantner h. soncu and c. newell explaining the user experience of recommender systems user modeling and user adapted interaction vol.
no.
pp.
oct .
o. kononenko o. baysal and m. w. godfrey code review quality how developers see it in proceedings of the international conference on software engineering pp.
.
v .
kovalenko n. tintarev e. pasynkov c. bird and a. bacchelli does reviewer recommendation help developers?
ieee transactions on software engineering pp.
.
j. lipcak and b. rossi a large scale study on source code reviewer recommendation in 44th euromicro conference on software engineering and advanced applications seaa pp.
.
s. mcintosh y .
kamei b. adams and a. e. hassan the impact of code review coverage and code review participation on software quality a case study of the qt vtk and itk projects 11th working conference on mining software repositories msr proceedings .
t. mende and r. koschke effort aware defect prediction models in proceedings of the european conference on software maintenance andreengineering pp.
.
e. mirsaeedi and p. rigby mitigating turnover with code review recommendation balancing expertise workload and knowledge distribution inproceedings of the international conference on software engineering p. to appear.
r. morales s. mcintosh and f. khomh do code review practices impact design quality?
a case study of the qt vtk and itk projects inproceedings of the international conference on software analysis evolution and reengineering pp.
.
d. mukherjee and m. garg which work item updates need your response?
in proceedings of the international conference on mining software repositories pp.
.
a. ouni r. g. kula and k. inoue search based peer reviewers recommendation in modern code review in ieee international conference on software maintenance and evolution icsme pp.
.
m. m. rahman c. k. roy and j. a. collins correct code reviewer recommendation in github based on cross project and technologyexperience in proceedings of the 38th international conference on software engineering companion ser.
icse .
new york ny usa association for computing machinery p. .
.
available p. c. rigby and c. bird convergent software peer review practices in proceedings of the the joint meeting of the european software engineering conference and the acmsigsoft symposium on the foundations of software engineering esec fse .
acm august preprint available uponrequest to cbird microsoft.com or peter.rigby concordia.ca.
.
available m. p. robillard w. maalej r. j. walker and t. zimmermann eds.
recommendation systems in software engineering.
springer .
a. strauss and j. m. corbin grounded theory in practice.
sage .
r. taylor interpretation of the correlation coefficient a basic review journal of diagnostic medical sonography vol.
no.
pp.
.
p. thongtanunam c. tantithamthavorn r. g. kula n. yoshida h. iida and k. matsumoto who should review my code?
a file location basedcode reviewer recommendation approach for modern code review in2015 ieee 22nd international conference on software analysis evolution and reengineering saner pp.
.
p. thongtanunam s. mcintosh a. e. hassan and h. iida investigating code review practices in defective files an empirical study of the qtsystem in proceedings of the international conference on mining software repositories pp.
.
r. wen d. gilbert m. g. roche and s. mcintosh blimp tracer integrating build impact analysis with code review in proceedings of the international conference on software maintenance and evolution pp.
.
w. e. wong r. gao y .
li r. abreu and f. wotawa a survey on software fault localization ieee transactions on software engineering vol.
no.
pp.
.
x. xia d. lo x. wang and x. yang who should review this change?
putting text and file location analyses together for more accuraterecommendations in proceedings of the international conference on softwar e maintenance and evolution pp.
.
c. yang x. h. zhang l. b. zeng q. fan t. wang y .
yu g. yin and h. m. wang revrec a two layer reviewer recommendation algorithm in pull based development model journal of central south university vol.
pp.
.
y .
yu h. wang g. yin and t. wang reviewer recommendation for pull requests in github what can we learn from code review and bug assignment?
information and software technology vol.
.
m. b. zanjani h. kagdi and c. bird automatically recommending peer reviewers in modern code review ieee transactions on software engineering vol.
no.
pp.
.
t. zimmermann a. zeller p. weissgerber and s. diehl mining version histories to guide software changes ieee transactions on software engineering vol.
no.
pp.
.