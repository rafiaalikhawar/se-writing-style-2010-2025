testing dnn image classifiers for confusion bias errors yuchi tian columbia university yuchi.tian columbia.eduziyuan zhong columbia university ziyuan.zhong columbia.eduvicente ordonez university of virginia vicente virginia.edu gail kaiser columbia university kaiser cs.columbia.edubaishakhi ray columbia university rayb cs.columbia.edu abstract image classifiers are an important component of today s software fromconsumerandbusinessapplicationstosafety criticaldomains.
the advent of deep neural networks dnns is the key catalyst behind such wide spread success.
however wide adoption comes with serious concerns about the robustness of software systems dependentondnnsforimageclassification asseveralsevereerroneous behaviors have been reported under sensitive and critical circumstances.
we argue that developers need to rigorously test theirsoftware simageclassifiersanddelaydeploymentuntilacceptable.
we present an approach to testing image classifier robustness based on class property violations.
we found that many of the reported erroneous cases in popular dnnimageclassifiersoccurbecausethetrainedmodelsconfuse oneclasswithanotherorshowbiasestowardssomeclassesover others.thesebugsusuallyviolatesomeclasspropertiesofoneor moreofthoseclasses.mostdnntestingtechniquesfocusonperimage violations so fail to detect class level confusions or biases.
we developed a testing technique to automatically detect classbasedconfusionandbiaserrorsindnn drivenimageclassification software.weevaluatedourimplementation deepinspect onseveralpopularimageclassifierswithprecisionupto100 avg.
.
for confusion errors and up to .
avg.
.
for bias errors.
deepinspectfoundhundredsofclassificationmistakesinwidelyused models many exposing errors indicating confusion or bias.
ccs concepts software and its engineering software testing and debugging computing methodologies neural networks .
keywords whitebox testing deep learning dnns image classifiers bias both are first authors and contributed equally to this research.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn ... .
introduction imageclassificationhasaplethoraofapplicationsinsoftwarefor safety critical domains such as self driving cars medical diagnosis etc.even day to day consumer software includes image classifiers suchasgooglephotosearchandfacebookimagetagging.image classificationisawell studiedproblemincomputervision wherea model is trained to classify an image into single or multiple predefined categories .
deep neural networks dnns have enabled majorbreakthroughsinimageclassificationtasksoverthepastfew years sometimesevenmatchinghuman levelaccuracyundersomeconditions whichhasledtotheirubiquityinmodernsoftware.
however inspiteofsuchspectacularsuccess dnn basedimage classificationmodels liketraditionalsoftware areknowntohave serious bugs.
for example google faced backlash in due to anotoriouserrorinitsphoto taggingapp whichtaggedpictures of dark skinned people as gorillas .
analogous to traditional software bugs the software engineering se literature denotes theseclassificationerrorsas modelbugs whichcanarisedue to either imperfect model structure or inadequate training data.
at a high level these bugs can affect either an individual image where a particular image is mis classified e.g.
a particular skier is mistakenasapartofamountain oran imageclass whereaclassof imagesismorelikelytobemis classified e.g.
dark skinnedpeople are more likely to be misclassified as shown in table .
the latter bugsarespecifictoawhole classofimagesratherthanindividual images implying systematic bugs rather than the dnn equivalent ofoff by oneerrors.whilemucheffortfromtheseliteratureon neural network testing has focused on identifying individual level violations using white box grey box or concolic testing detection of class level violations remains relativelylessexplored.thispaperfocusesonautomaticallydetecting such class level bugs so they can be fixed.
after manual investigation of some public reports describing the class level violations listed in table we determined two root causes i confusion the model cannot differentiate one class fromanother.forexample googlephotosconfusesskierandmountain .
ii bias themodelshowsdisparateoutcomesbetween two related groups.
for example zhao et al.in their paper men also like shopping find classification bias in favor of women onactivitieslikeshopping cooking washing etc.wefurthernotice that some class level properties are violated in both kinds of cases.
for example in the case of confusion errors the classification errorrate between the objects of two classes say skier and mountain issignificantlyhigherthantheoverallclassificationerrorrateof the model.
similarly in the bias scenario reported by zhao et al.
a ieee acm 42nd international conference on software engineering icse table examples of real world bugs reported in neural image classifiers bug type name report date outcome gorilla tag jul black people were tagged as gorillas by google photo app.
confusion elephant is detected aug image transplantation replacing a sub region of an image by in a room another image containing a trained object leads to mis classification.
google photo dec google photo confuses skier and mountain.
nikon camera jan camera shows bias toward caucasian faces when detecting people s blinks.men like shopping july multi label object classification models show bias towards women on bias activities like shopping cooking washing etc.
gender shades open source face recognition services provided by ibm microsoft and face have higher error rates on darker skin females for gender classification.
dnn model should not have different error rates while classifying the gender of a person in the shopping category.
unlike individual image properties this is a class property affecting all the shopping imageswithmenorwomen.anyviolationofsuchapropertyby definition affects the whole class although not necessarily everyimage in that class e.g.
a man is more prone to be predicted as a woman when he is shopping even though some individual images of aman shopping maystill be predicted correctly.
thus we need a class level approach to testing image classifier software for confusion and bias errors.
the bugs in a dnn model occur due to sub optimal interactions between the model structure and the training data .
to capture suchinteractions theliteraturehasproposedvariousmetricsprimarily based on either neuron activations or feature vectors .
however these techniques are primarily targeted at the individual image level.
to detect class level violations we abstractawaysuchmodel datainteractionsattheclasslevelandan alyzetheinter classinteractionsusingthatnewabstraction.tothis end weproposeametricusingneuronactivationsandabaseline metric using weight vectors of the feature embedding to capture the class abstraction.
for a set of test input images we compute the probability of activation of a neuron per predicted class.
thus for each class we createavectorofneuronactivationswhereeachvectorelementcor respondstoaneuronactivationprobability.ifthedistancebetween thetwovectorsfortwodifferentclassesistooclose comparedto otherclass vectorpairs thatmeansthednnundertestmaynot effectively distinguish between those two classes.
motivated bymode stechnique wefurthercreateabaselinewhereeach class is represented by the corresponding weight vector of the last linear layer of the model under test.
weevaluateourmethodologyforbothsingle andmulti label classificationmodelsineightdifferentsettings.ourexperiments demonstrate that deepinspect can efficiently detect both bias and confusion errors in popular neural image classifiers.
we furthercheck whether deepinspect can detect such classification errorsin state of the art models designed to be robust against norm bounded adversarial attacks deepinspect finds hundreds of errors proving the need for orthogonal testing strategies to detect such class level mispredictions.
unlike some other dnn testing techniques deepinspect does not need to generate additional transformed synthetic images to find these errors.
the primary contributions of this paper are we propose a novel neuron coverage metric to automatically detect class level violations confusion and bias errors in dnnbased visual recognition models for image classification.
weimplementedourmetricandunderlyingtechniquesindeepinspect.
weevaluateddeepinspectandfoundmanyerrorsinwidely useddnnmodelswithprecisionupto100 avg.
.
forconfusion errors and up to .
avg.
.
for bias errors.
our code is available at the errors reported by deepinspect are available at ariselab.info deepinspect.
dnn background deep neural networks dnns are a popular type of machine learning model loosely inspired by the neural networks of human brains.
a dnn model learns the logic to perform a software task from a large set of training examples.
for example an image recognition model learns to recognize cows through being shown trained with many sample images of cows.
a typical feed forward dnn consists of a set of connected computational units referred as neurons that are arranged sequentially in a series of layers.
the neurons in sequential layers are connected to each other through edges.
each edge has a corresponding weight.
each neuron applies anonlinear activation function e.g.
relu sigmoid to the incoming values on its input edges and sends the results on its output edges to the next layer of connected neurons.
for image classification convolutional neural networks cnns a specific type of dnn are typically used.
cnns consist of layers with local spatial connectivity and sets of neurons with shared parameters.
when implementing a dnn application developers typically start with a set of annotated experimental data and divide it into three sets i training to construct the dnn model in a supervised setting meaning the training data is labeled e.g.
using stochastic gradient descent with gradients computed using back propagation ii validation to tune the model s hyper parameters basically configuration parameters that can be modified to better fit the expected application workload and iii evaluation to evaluate the accuracy of the trained model w.r.t.
to its predictions on other annotated data to determine whether or not it predicts correctly.
typically training validation and testing data are drawn from the same initial dataset.
1123forimageclassification adnncanbetrainedineitherofthe following two settings i single label classification.
in a traditional single label classificationproblem eachdatumisassociatedwithasinglelabel lfrom a set of disjointlabels lwhere l .
if l the classification problemiscalledabinaryclassificationproblem if l itisa multi classclassificationproblem .amongsomepopularimage classification datasets mnist cifar cifar and imagenet areallsingle label whereeachimagecanbecategorized into only one class or outside that class.
ii multi labelclassification.
inamulti labelclassificationproblem eachdatumisassociatedwithasetoflabels ywherey l. coco and imsitu are popular datasets for multi label classification.
for example an image from the coco dataset can be labeled as car person traffic light and bicycle.
a multi label classification model is supposed to predict all of car person traffic light and bicycle from a single image that shows all of these kinds of objects.
given any single or multi label classification task dnn classifier software tries to learn the decision boundary between the classes all members of a class say ci should be categorized identically irrespective of their individual features and members of anotherclass say cj shouldnotbecategorizedto ci .thednn represents the input image in an embedded space with the feature vectoratacertainintermediatelayerandusesthelayersafteras a classifier to classify these representations.
the class separation between two classes estimates how well the dnn has learned to separate each class from the other.
if the embedded distance between two classes is too small compared to other classes or lower thansomepre definedthreshold weassumethatthednncould not separate them from each other.
methodology wegiveadetailedtechnicaldescriptionofdeepinspect.wedescribe a typical scenario where we envision our tool might be used in the following and design the methodology accordingly.
usagescenario.
similartocustomertestingofpost releasesoftware deepinspect works in a real world setting where a customer gets a pre trained model and tests its performance in a sample production scenario before deployment.
the customer has white box accesstothemodeltoprofile althoughallthedataintheproduction system can be unlabeled.
in the absence of ground truth labels theclassesaredefinedbythe predictedlabels.thesepredictedlabels areusedasclassreferencesasdeepinspecttriestodetectconfusion andbiaserrorsamongtheclasses.deepinspecttrackstheactivatedneurons per class and reports a potential class level violation if the class level activation patterns are too similar between two classes.such reported errors will help customers evaluate how much they can trust the model s results related to the affected classes.
as elaboratedinsection7 once theseerrorsarereportedbacktothe developers they can focus their debugging and fixing effort on these classes.
figure shows the deepinspect workflow.
.
definitions before we describe deepinspect s methodology in detail we introduce definitions that we use in the rest of the paper.
the following figure deepinspect workflow table shows our notation.
all neurons set n n1 n2 ... activation function out n c returns output for neuron n inputc.
activation threshold th neural path np .for an input image c we define neural path as a sequence of neurons that are activated by c. neural path per class npc .for a class ci this metric represents a set consisting of the union of neural paths activated by all the inputs in ci.
forexample consideraclass cowcontainingtwoimages abrown cow and a black cow.
let s assume they activate two neural paths and .
thus theneural paths forclass cow would be npcow .npcowis further represented by a vector n1 n1 n2 n1 n1 where the superscripts represent the number of times each neuron is activated byccow.thus eachclass ciinadatasetcanbeexpressedwitha neuronactivationfrequencyvector whichcaptureshowthemodel interacts with ci.
neuron activation probability leveraginghow frequently aneuronnjis activatedby allthe members from a class ci thismetric estimatestheprobabilityofaneuron njtobeactivatedby ci.thus we define p nj ci cik cik ci out nj cik th ci wethenconstructa n mdimensional neuronactivationprobability matrix nis the number of neurons and mis the number of classes with its ij th entry being p nj ci .
c1...ci...cm parenlefttpa parenleftexa parenleftexa parenleftexa parenleftexa parenleftbta parenrighttpa parenrightexa parenrightexa parenrightexa parenrightexa parenrightbtan 1p11 p1m ... ... njpj1 ... pjm ... ... nnpn1 pnm thismatrixcaptureshowamodelinteractswithasetofinput data.thecolumnvectors m representtheinteractionofaclass cmwiththemodel.notethat inoursetting csarepredictedlabels.
since neuron activation probability matrix is designed to representeachclass itshouldbeabletodistinguishbetweendifferentcs.next weusethismetrictofindtwodifferentclassesof errorsoftenfoundindnnsystems confusion andbias seetable1 .
.
finding confusion errors in an object classification task when the model cannot distinguish one object class from another confusion occurs.
for example as shown in table a google photo app model confuses a skier with the mountain.
thus finding confusion errors means checking how well the model can distinguish between objects of different classes.
1124an error happens when the model under test classifies an object withawrongclass orformulti labelclassificationtask predicts two classes but only one of them is present in the test image.
we argue that the model makes these errors because during the trainingprocessthemodelhasnotlearnedtodistinguishwellbetweenthetwoclasses say aandb.therefore theneuronsactivated by these objects are similar and the column vectors correspondingtotheseclasses aand bwillbeveryclosetoeachother.
thus we compute the confusion score between two classes as the euclidean distance between their two probability vectors napvd a b a b a b radicaltp radicalbtn summationdisplay.
i p ni a p ni b ifthe valueis lessthan somepre definedthreshold conf th fortwopairsofclasses themodelwillpotentiallymakemistakes in distinguishing one from another which results in confusion errors.this iscallednapvd neuronactivationprobabiliy vector distance .
.
finding bias errors in an object classification task bias occurs if the model under test shows disparate outcomes between two related classes.
for example we find that resnet pretrained by imsitu dataset of ten mis classifies a man with a baby as woman.
we observe that in the embedded matrix baby woman is much smaller than baby man .
therefore during testing whenever the model finds an image with a baby it is biased towards associating the babyimage with a woman.
based on this observation we propose aninter class distance based metric to calculate the bias learned by themodel.wedefinethebiasbetweentwoclasses aandbovera third class cas follows bias a b c c a c b c a c b if a model treats objects of classes aandbsimilarly under the presenceofathirdobjectclass c aandbshouldhavesimilardistancew.r.t.cintheembeddedspace thus thenumeratorofthe above equation will be small.
intuitively the model s output can bemoreinfluencedbythenearerobjectclasses i.e.ifaandbarecloser toc.thus wenormalizethedisparitybetweenthetwodistances to increase the influence of closer classes.
this bias score is used to measure how differently the given model treats two classes in the presence of a third object class.
an average bias abbreviated as avg bias between two objects aand bfor all class objects ois defined as av bias a b o summationdisplay.
c o c nequala bbias a b c theabovescorecapturestheoverallbiasofthemodelbetweentwo classes.if thebiasscore islargerthansome pre definedthreshold we report potential bias errors.
notethat evenwhenthetwoclasses aandbarenotconfused by themodel i.e.
a b conf th they canstill show bias w.r.t.
another class say c i f a c is very different from b c .
thus bias and confusion are two separate types of class level errors that we intend to study in this work.table study subjects dataset model classification cnn reported task name classes models neurons layers accuracy coco 80resnet conv .
multi label coco gender 81resnet conv .
classification imsitu resnet conv .
cifar cnn .
robust 10small cnn .
single label cifar large cnn .
classification resnet .
imagenet 1000resnet conv .
reported in mean average precision reported in mean accuracy using these above equations we develop a novel testing tool deepinspect to inspect a dnn implementing image classification tasksandlookforpotentialconfusionandbiaserrors.weimplemented deepinspectin thepytorch deeplearning frameworkand python .
.
all our experiments were run on ubuntu .
.
with two titan xp gpus.
for all of our experiments we set the activation threshold thto be .
for all datasets and models.
we discuss whywechoose0.5asneuronactivationthresholdandhowdifferent thresholds affect our performance in the section .
experimental design .
study subjects weapplydeepinspectforbothmulti labelandsingle labeldnnbased classifications.
under different settings deepinspect automaticallyinspects8dnnmodelsfor6datasets.table2summarizes our study subjects.
all the models we used are standard widelyusedmodelsforeachdataset.weusedpre trainedmodelsasshown in the table for all settings except for coco with gender.
for cocowithgendermodel weusedthegenderlabelsfrom and trainedthemodelinthesamewayas .imsitumodelisapretrainedresnet 34model .thereareintotal11 538entitiesand roles in the imsitu dataset.
when inspecting a model trained usingimsitu weonlyconsideredthetop100frequententitiesor roles in the test dataset.
among the dnn models three are pre trained relatively more robust models that are trained using adversarial images along with regularimages.thesemodelsarepre trainedbyprovablyrobust trainingapproachproposedby .threemodelswithdifferent network structures are trained using the cifar10 dataset .
.
constructing ground truth gt errors to collect the ground truth for evaluating deepinspect we refer to the test images misclassified by a given model.
we then aggregate thesemisclassifiedimageinstancesbytheirrealandpredictedclasslabels and estimate pair wise confusion bias.
.
.
gt of confusion errors.
confusionoccurswhenadnnoften makes mistakes in disambiguating members of two different classes.
in particular if a dnn is confused between two classes the classification error rate is higher between those two classes thanbetweentherestoftheclass pairs.basedonthis wedefine two types of confusion errors for single label classification and multi label classification separately a confusions distribution b napvd distribution figure2 identifyingtype2confusionsformulti classificationapplications.lhsshowshowwemarkedthegroundtrutherrorsbased on type2 confusion score.
rhs shows deepinspect s predicted er rors based on napvd score.
type1 confusions in single label classification type1 confusion occurs when an object of class x e.g.
violin is misclassified to anotherclass y e.g.
cello .foralltheobjectsofclass xandy itcan be quantified as type1conf x y mean p x y p y x dnn s probability to misclassify class yasxand vice versa and takes the average value between the two.
for example given two classes celloandviolin type1conf estimates the mean probability of violinmisclassified to celloand vice versa.
note that this is a bi directional score i.e.misclassification of yasxis the same as misclassification of xasy.
type2confusions inmulti labelclassification type2confusion occurswhenaninputimagecontainsanobjectofclass x e.g.
mouse andnoobjectofclass y e.g.
keyboard butthemodelpredictsboth classes see figure .
for a pair of classes this can be quantified as type2conf x y mean p x y x p x y y tocomputethe probabilitytodetecttwoobjectsinthepresenceofonlyone.forexample giventwoclasses keyboard andmouse type2conf estimates the mean probability of mousebeing predicted while predicting keyboard and vice versa.
this is also a bi directional score.
we measure type1conf andtype2conf by using a dnn s true classification error measured on a set of test images.
they create thednn strueconfusioncharacteristicsbetweenallpossibleclasspairs.
we then draw the distributions of type1conf andtype2conf .
for example figure 2a shows type2conf distribution for coco .
theclass pairswithconfusionscoresgreaterthan1standarddeviationfromthemean valuearemarkedaspairstrulyconfusedbythemodelandformourgroundtruthforconfusionerrors.forexample in the coco dataset there are classes and thus class pairs class pairs are ground truth confusion errors.
notethat unlikehowabug errorisdefinedintraditionalsoftware engineering our suspicious confusion pairs have an inherent probabilisticnature.forexample evenif aandbrepresentaconfusionpair itdoesnotmeanthatalltheimagescontaining aorb will be misclassified by the model.
rather it means that compared with other pairs images containing aorbtend to have a higher chance to be misclassified by the model.
.
.
gt of bias errors.
a dnn model is biasedif it treats two classes differently.
for example consider three classes man woman andsurfboard .anunbiasedmodelshouldnothavedifferenterror rates while classifying manorwomanin the presence of surfboard .
tomeasuresuchbiasformally wedefine confusiondisparity cd to measure differences in error rate between classes xandzand betweenyandz cd x y z error x z error y z wheretheerrormeasurecanbeeither type1conf ortype2conf asdefinedearlier.
cdessentially estimates the disparity of the model s error between classesx y e.g.
man woman w.r.t.a third class z e.g.
surfboard .
we also define an aggregated measure average confusion disparity avg cd betweentwoclasses xandybysummingupthe bias between them over all third classes and taking the average avg cd x y o summationdisplay.
z o z nequalx ycd x y z .
dependingontheerrortypesweusedtoestimateavg cd wereferto type1 avg cdand type2 avg cd.wemeasureavg cdusingthe true classificationerror ratereported for thetest images.similar toconfusionerrors wedrawthedistributionof avg cdforallpossibleclasspairsandthenconsiderthepairsas trulybiased iftheiravg cd scoreishigherthanonestandarddeviationfromthemeanvalue.
such truly biased pairs form our ground truth for bias errors.
.
evaluating deepinspect we evaluate deepinspect using a set of test images.
error reporting.
deepinspect reports confusion errors based on napvd see equation scores lower napvdindicates errors.
wedrawthedistributionsofnapvdsforallpossibleclasspairs as shown infigure 2b.
classpairs havingnapvd scores lowerthan standard deviationfrom the meanscore aremarked as potential confusion errors.
asdiscussedinsection3.
deepinspectreportsbiaserrorsbased on avg bias score see equation where higher avg bias means class pairs are more prone to bias errors.
similar to above from thedistributionof avg biasscores deepinspectpredictspairswith avg bias greater than standard deviation from the mean score to be erroneous.
note that while calculating error disparity between classesa bw.r.t.c see equation if both aandbare far from c intheembeddedspace disparityoftheirdistances shouldnot reflecttruebias.thus whilecalculating avg bias a b wefurther filter out the triplets where c a th c b th wherethis some pre defined threshold.
in our experiment we remove all the class pairs having larger than standard deviation i.e.th f r o m the mean value of all deltas across all the class pairs.
evaluation metric.
we evaluate deepinspect in two ways precision recall.
weuseprecisionandrecalltomeasuredeepinspect saccuracy.foreacherrortypet supposethateisthenumber of errors detected by deepinspect and a is the the number of true errors in the ground truth set.
then the precision and recall of deepinspect are a e e and a e a respectively.
area under cost effective curve aucec .
similarly to how static analysis warnings are ranked based on their priority levels wealsoranktheerroneousclass pairsidentifiedbydeepinspect based on the decreasing order of error proneness i.e.most error prone pairs will be at the top.
to evaluate the ranking we useacost effectivenessmeasure aucec areaunderthecosteffectiveness curve which has become standard to evaluate rankbased bug prediction systems .
cost effectivenessevaluateswhenweinspect test topn classpairs in the ranked list i.e.inspection cost how many true errors 1126arefound i.e.effectiveness .bothcostandeffectivenessarenormalizedto100 .figure6showscostonthex axis andeffectivenesson the y axis indicating the portion of the ground truth errors found.
aucec is the area under this curve.
baseline.
we compare deepinspect w.r.t.two baselines i mode inspired a popular way to inspect each image is to inspect a feature vector which is an output of an intermediate layer .however abstractingafeaturevectorperimagetothe classlevelisnon trivial.instead foragivenlayer onecouldinspect theweightvector wl w0 l w1 l ... wn l ofaclass say l wherethe superscripts represent a feature.
similar weight vectors are used in mode to compare the difference in feature importance between two image groups.
in particular from the last linear layer before the output layer we extract such per class weight vectorsand compute the pairwise distances between the weight vectors.using these pairwise distances we calculate confusion and bias metrics as described in section .
ii random we also build a random model that picks random class pairs for inspection as a baseline.
for aucec evaluation we further show the performance of an optimal model that ranks the class pairs perfectly if n of all the class pairsaretrulyerroneous theoptimalmodelwouldrankthematthetopsuchthatwithlowerinspectionbudgetmostoftheerrors willbedetected.theoptimalcurvegivesthelowerupperboundof the ranking scheme.researchquestions.
withthisexperimentalsetting weinvestigatethefollowingthreeresearchquestionstoevaluatedeepinspect for dnn image classifiers rq1.can deepinspect distinguish between different classes?
rq2.can deepinspect identify the confusion errors?
rq3.can deepinspect identify the bias errors?
results webeginourinvestigationbycheckingwhetherde factoneuron coverage based metrics can capture class separation.rq1.candeepinspectdistinguishbetweendifferentclasses?
motivation.
the heart of deepinspect s error detection technique liesinthefactthattheunderlyingneuronactivationprobability metric captures each class abstraction reasonably well and thus distinguishes between classes that do not suffer from class level violations.inthisrqwecheckwhetherthisisindeedtrue.wealso checkwhetheranewmetric isnecessary i.e.
whetherexisting neuron coverage metrics could capture such class separations.
approach.
we evaluate this rq w.r.t.the training data since the dnnbehaviorsarenottaintedwithinaccuraciesassociatedwith the test images.
thus all the class pairs are benign.
we evalu ate this rq in three settings i using deepinspect s metrics ii neuron coverage proposed by pei et al.
and iii other neuronactivation related metrics proposed by deepgauge .setting .
deepinspect.
our metric neuron activation probability matrix by construction is designed per class.
hence it would be unfair to directly measure its capability to distinguish between different classes.
thus we pose this question in slightly a different way as described below.
for multi label classification each image contains multiple class labels.
for example an im age might have labels for both mouseandkeyboard .
such coincidence of labels may create confusion if two labels always appeartogether inthe groundtruthset noclassifier candistinguish between them.
to check how many times two labels coincide we define a coincidence score between two labels laandlbas coincidence la lb mean p la lb la p la lb lb .
the above formula computes the minimum probability of labels laandlboccurringtogetherinanimagegiventhatoneofthemis present.notethatthisisabi directionalscore i.e.wetreatthetwo labels similarly.
the meanoperation ensures we detect the least coincidenceineitherdirection.alowvalueofcoincidencescore indicates two class labels are easy to separate and vice versa.
now to check deepinspect s capability to capture class separation wesimplycheckthecorrelationbetweencoincidencescore and confusion score napvd from equation for all possible classlabel pairs.
since only multi label objects can have label coinci dences we perform this experiment for a pre trained resnet model on the coco multi label classification task.
aspearmancorrelation coefficient betweentheconfusionand coincidence scores reaches a value as high as .
showing strong statisticalsignificance.theresultindicatesthatdeepinspectcan disambiguate most of the classes that have a low confusion scores.
interestingly we found some pairs where coincidence score is high but deepinspect was able to isolate them.
for example cup chair toilet sink etc..manuallyinvestigatingsuchcases revealsthatalthoughthesepairsoftenappeartogetherintheinputimages therearealsoenoughinstanceswhentheyappearbythem selves.thus deepinspectdisambiguatesbetweentheseclassesand puts them apart in the embedded space .
these results indicate deepinspect can also learn some hidden patterns from the context and thus can go beyond inspecting the training data coincidence forevaluatingmodelbias confusion whichisthedefactotechnique among machine learning researchers .
.
.
.
.
stop sign cat couch chair cell phone refrigerator cup pizza spoon snowboard class labelsneuron coverage figure distribution of neuron coverage per class label for randomly picked class labels from the coco dataset.
next we investigate whether popular white box metrics can distinguish between different classes.
setting .neuroncoverage nc computestheratioofthe union of neurons activated by an input set and the total number of neurons in a dnn.
here we compute ncper class label i.e.for agivenclass label wemeasurethenumberofneuronsactivated bytheimagestaggedwiththatlabel w.r.t.tothetotalneurons.the 1127activation thresholdwe useis .
.
we perform thisexperiment on coco and cifar to study multi and single label classifications.figure3showsresultsforcoco.weobservesimilarresults for cifar .
each boxplot in the figure shows the distribution of neuron coverage per class label across all the relevant images.
these boxplots visuallyshowthat differentlabels havevery similarncdistribution.
we further compare these distributions using kruskal test whichisanon parametricwayofcomparingmorethantwogroups.
note that we choose a non parametric measure as ncs may not follow normal distributions.
kruskal test is a parametric equivalent of the one way analysis of variance anova .
the result reportsap value .
i.e.somedifferencesexistacrossthese distributions.
however a pairwise cohend s effect size for each class label pair as shown in the following table shows more than and class pairs for cifar and coco have small to negligible effect size.
this means neuroncoverage cannot reliablydistinguish a majority of the class labels.
effect size of neuron coverage across different classes exp setting negligible small medium large coco .
.
.
.
cifar .
.
.
.
setting .deepgauge .
maet al.
arguethateachneuron hasaprimaryregionofoperation theyidentifythisregionbyusingaboundarycondition onitsoutputduringtrainingtime outputsoutsidethisregion low hi h aremarkedas corner cases.
they therefore introduce multi granular neuron and layer level coverage criteria.
for neuron coverage they propose i k multisection coverage to evaluate how thoroughly the primary regionofaneuroniscovered ii boundarycoverage tocomputehow many corner cases are covered and iii strong neuron activation coverageto measure how many corner case regions are covered in hi h region.
for layer level coverage they define iv top k neuroncoverage toidentifythemostactivek neuronsforeachlayer and v top k neuron pattern for each test case to find a sequence of neurons from the top k most active neurons across each layer.
we investigate whether each of these metrics can distinguish between different classes by measuring the above metrics for individualinputclassesfollowingma et al.
smethodology.wefirst profiled every neuron upper and lower bound for each class using the training images containing that class label.
next we computed per classneuroncoverageusingtestimagescontainingthatclass for k multisection coverage we chose k to scale up the analysis.itshouldbenotedthatwealsotried k whichisused in the original deepgauge paper and observed similar results not shown here .
for layer level coverage we directly used the input images containing each class where we select k .
figure4 showstheresults asahistogram oftheabove fivecoveragecriteriaforthecocodataset.forallfivecoveragecriteria there are many class labels that share similar coverage.
for ex ample incoco thereare52labelswithk multisectionneuroncoverage with values between .
and .
.
similarly there are labels with neuron boundary coverage.
therefore none of the fivecoveragecriteriaareaneffectivewaytodistinguishbetween !
figure histogram of deepgauge multi granular coverage per class label for coco dataset differentequivalenceclasses.thesameconclusionwasdrawnfor the cifar dataset.
result1 deepinspectcandisambiguateclassesbetterthan previouscoverage basedmetricsfortheimageclassificationtask.
we now investigate deepinspect s capability in detecting confusion and bias errors in dnn models.rq2.
can deepinspect identify the confusion errors?motivation.
to evaluate how well deepinspect can detect classlevelviolations inthisrq wereportdeepinspect sabilitytodetect the first type of violation i.e.
type1 type2 confusions w.r.t.to ground truth confusion errors as described in section .
.
.
.
.
.
.
type confusion0.
.
.
.
.
.
.30napvd a coco dataset resnet .
.
.
.
type confusion0.
.
.
.3napvd b robust cifar small figure5 strongnegativespearmancorrelation .55and .
between napvd and ground truth confusion scores.
wefirstexplorethecorrelationbetweennapvdandgroundtruth type1 type2confusionscore.strongcorrelationhasbeenfound forall8experimentalsettings.figure5givesexamplesoncoco and cifar .
these results indicate that napvd can be used to detect confusion errors lower napvd means more confusion.
approach.
by default deepinspect reports all the class pairs with napvd scores one standard deviation less than the mean napvdscore as error prone see figure 2b .
in this setting as the resultshown on table deepinspect reports errors at high recall under most settings.
specifically on cifar and robust cifar resnet deepinspect can reporterrors ashigh as71.
and100 respectively.
deepinspect has identified thousands of confusion errors.
1128table deepinspect performance on detecting confusion errors napvd mean 1std top tp fp precision recall tp fp precision recall coco deepinspect .
.
.
mode .
.
.
.
random .
.
.
.
coco gender deepinspect .
.
.
mode .
.
.
.
random .
.
.
.
cifar deepinspect .
.
.
.
mode .
.
.
.
random .
.
.
.
r cifar s deepinspect .
.
mode .
.
random .
.
r cifar l deepinspect .
.
mode .
.
random r cifar r deepinspect .
mode .
.
random imagenet deepinspect .
.
.
.
mode .
.
.
.
random .
.
.
.
imsitu deepinspect .
.
.
.
random .
.
.
.
ifhigherprecisioniswanted ausercanchoosetoinspectonlya smallsetofconfusedpairsbasedonnapvd.asalsoshownintable3 when only the top1 confusion errors are reported a much higher precision is achieved for all the datasets.
in particular deepinspect identifies and confusion errors for the coco model andthecifar 100modelwith100 and79.
precision respectively.
the trade off between precision and recall can be found on the cost effective curves shown on figure which show overall performanceofdeepinspectatdifferentinspectioncutoffs.overall w.r.t.a randombaseline mode deepinspectis gaining aucec performance from .
to .
w.r.t.a mode baseline mode deepinspect is gaining aucec performance from .
to .
.
figure aucec plot of type1 type2 confusion errors in three different settings.
the redvertical line marks standard deviation lessfrommeannapvdscore.deepinspectmarksallclass pairswith napvd scores less than the red mark as potential errors.
figure and figure give some specific confusion errors found by deepinspect in the coco and the imagenet settings.
in particular asshowninfigure7a whenthereisonlyakeyboardbut a keyboard mouse b oven microwave figure7 confusionerrorsidentifiedincocomodel.ineachpair the second object is mistakenly identified by the model.
no mouse in the image the coco model reports both.
similarly figure8ashowsconfusionerrorson cello violin .thereareseveral cellos in this image but the model predicts it to show a violin.
a cello violin b library bookshop figure confusion errors identified in the imagenet model.
for each pair the second object is mistakenly identified by the model.
acrossallthreerelativelymorerobustcifar 10modelsdeepinspect identifies cat dog bird deer and automobile truck as buggy pairs where one class is very likely to be mistakenly classifiedastheotherclassofthepair.thisindicatesthattheseconfusion errorsaretobetiedtothetrainingdata soallthemodelstrained on this dataset including the robust models may have these errors.
theseresultsfurthershowthattheconfusionerrorsareorthogonal tothenorm basedadversarialperturbationsandweneedadifferent technique to address them.
wealsonotethattheperformanceofallmethodsdegradesquite abitonimagenet.imagenetisknowntohaveacomplexstructure andallthetasks includingimageclassificationandrobustimage classification usuallyhaveinferiorperformancecomparedwith simpler datasets like cifar or cifar .
due to such inherent complexity theclassrepresentationintheembeddedspaceisless accurate andthustherelativedistancebetweentwoclassesmay not correctly reflect a model s confusion level between two classes.
result deepinspect can successfully find confusion errors with precision to at top1 for both single and multiobjectclassificationtasks.deepinspectalsofindsconfusionerrors in robust models.
rq3.
can deepinspect identify the bias errors?
motivation.
to assess deepinspect s ability to detect class level violations in this rq we report deepinspect s performance in detecting the second type of violation i.e.
bias errors as described in section .
.
.
approach.
weevaluatethisrqbyestimatingamodel sbias avg bias using equation w.r.t.the ground truth avg cd computed as a coco b cifar figure9 strongpositivespearman scorrelation .76and0.
exist between avg cd and avg bias while detecting classification bias.
in section .
.
.
we first explore the correlation between pairwise avg cd and our proposed pairwise avg bias figure shows the resultsforcocoandcifar .similartrendswerefoundinthe otherdatasetswestudied.theresultsshowthatastrongcorrelation exists between avg cd and avg bias.
in other words our proposed avg bias is a good proxy for detecting confusion errors.
table deepinspect performance on detecting bias errors avg bias mean 1std top tp fp precision recall tp fp precision recall coco deepinspect .
.
.
.
mode .
.
.
.
random .
.
.
.
coco gender deepinspect .
.
.
.
mode .
.
.
.
random .
.
.
.
cifar deepinspect .
.
.
.
mode .
.
.
.
random .
.
.
.
r cifar s deepinspect .
.
mode .
.
random .
.
r cifar l deepinspect .
.
mode .
.
random .
.
r cifar r deepinspect .
.
mode .
.
random .
.
imagenet deepinspect .
.
.
.
mode .
.
.
.
random .
.
.
.
imsitu deepinspect .
.
.
.
random .
.
.
.
as in rq2 we also do a precision recall analysis w.r.t.finding the bias errors across all the datasets.
we analyze the precision and recall of deepinspect when reporting bias errors atthe cutoff top1 avg bias and mean avg bias standard deviation avg bias respectively.theresultsareshownintable4.atcut offtop1 avg bias deepinspectdetectssuspiciouspairswithpre cisionashighas75 and84 forcocoandimsitu respectively.at cutoffmean avg bias standarddeviation avg bias deepinspect has high recall but lower precision deepinspect detects ground truth suspicious pairs with recall at .
and .
for coco and imsitu.deepinspectcanreport657 totaltruebiasbugs across the two models.
deepinspect outperforms the random baselinebyalargemarginatbothcutoffs.asinthecaseofdetecting confusion errors there is a significant trade off between precisionandrecall.thiscanbecustomizedbasedonuserneeds.thecosteffectiveness analysis in figure shows the entire spectrum.
figure bias errors detected w.r.t.the ground truth of avg cd beyond one standard deviation from mean.
asshowninfigure10 deepinspectoutperformsthebaselineby alargemargin.theaucecgainsofdeepinspectarefrom37 .
to .
w.r.t.therandombaselineandfrom6 .
to41 .
w.r.t.the modebaselineacrossthe8settings.deepinspect sperformance isclosetotheoptimalcurveundersomesettings specificallythe aucec gains of the optimal over deepinspectare only .
and .
under the coco and imsitu settings respectively.
inspired by which shows bias exists between men and women in coco for the gender image captioning task we analyze the most biased third class cforaandbbeing men and women.
as showninfigure11 wefoundthatsportslikeskiing snowboarding andsurfboardingaremorecloselyassociatedwithmenandthus misleads the model to predict the women in the images as men.
figure shows results on imsitu where we found that the model tends to associate the class inside with women while associating the class outside with men.
figure11 themodelclassifiesthewomeninthesepicturesasmen in the coco dataset.
we generalize the idea by choosing classes aandbto be any class pair.
we found that similar bias also exists in the single label classificationsettings.forexample inimagenet oneofthehighest biasesisbetweeneskimo dogandrapeseed w.r.t.siberian husky.
the model tends to confuse the two dogs but not eskimo dog and rapeseed.thismakessensesinceeskimo dogandsiberian husk are both dogs so more easily misclassified by the model.
one of the fairness violations of a dnn system can be drastic differences in accuracy across groups divided according to some sensitivefeature s .inblack boxtesting thetestercangetanumber indicating the degree of fairness has been violated by feeding into the model a validation set.
in contrast deepinspect provides a new angle to the fairness violations.
the neuron distance difference 1130figure the model classifies the man in the first figure to be a woman and the woman in the second figure to be a man.
betweentwoclasses aandbw.r.t.athirdclass cshedslightonwhy the model tends to be more likely to confuse between one of them andcthan the other.
we leave a more comprehensive examination on interpreting bias fairness violations for future work.
result deepinspect can successfully find bias errors for both single and multi label classification tasks and even for the robust models from to precision at top1 .
related work software testing verification of dnns.
prior research proposed different white box testing criteria based on neuron coverage and neuron pair coverage .
sunet al.
presentedaconcolictestingapproachfordnnscalleddeepconcolic.theyshowedthattheirconcolictestingapproachcaneffectively increase coverage and find adversarial examples.
odena and goodfellow proposed tensorfuzz which is a general tool that combines coverage guided fuzzing with property based testing to generatecasesthatviolateauser specifiedobjective.ithasapplications like finding numerical errors in trained neural networks exposing disagreements between neural networks and their quantizedversions surfacingbrokenlossfunctionsinpopulargithub repositories and making performance improvements to tensorflow.
there are also efforts to verify dnns against adversarial attacks.
however most of the verification efforts arelimited to small dnns and pixel level properties.
it is not obvi ous how to directly apply these techniques to detect class level violations.
adversarial deep learning.
dnns areknown to be vulnerable towell craftedinputscalledadversarialexamples wherethediscrepanciesareimperceptibletoahumanbutcaneasilymakednns fail .muchwork has been done to defend against adversarial attacks .ourmethodshavepotentialtoidentify adversarial inputs.
moreover adversarial examples are usually out of distribution data and not realistic while we can find both out distributionandin distributioncornercases.further wecan identify a general weakness or bug rather than focusing on crafted attacksthatoftenrequireastrongattackermodel e.g.
theattacker adds noise to a stop sign image .interpretingdnns.
therehasbeenmuchresearchonmodelinterpretabilityandvisualization .acomprehensive study is presented by lipton .
donget al.
observed that instead of learning the semantic features of whole objects neurons tend to react to different parts of the objects in a recurrent manner.our probabilistic way of looking at neuron activation per classaims to capture holistic behavior of an entire class instead of an individualobjectsodiversefeaturesofclassmemberscanbecaptured.closesttooursisbypapernot et al.
whousednearest training points to explain adversarial attacks.
in comparison we analyzethednn sdependenciesontheentiretraining testingdata andrepresentitinneuronactivationprobabilitymatrix.wecan explain the dnn s bias and weaknesses by inspecting this matrix.evaluating models bias fairness.
evaluating the bias and fairnessofasystemisimportantbothfromatheoreticalandapractical perspective .
related studies first define a fairness criteriaandthentrytooptimizetheoriginalobjectivewhilesatisfyingthefairness criteria .theseproperties are defined eitherat individual or group levels .
in this work we propose a definition of a bias error for image classification closely related to fairness notions at group level.
class membership can be regarded as the sensitive feature and the equal itythatwewanttoachieveisfortheconfusionlevelsoftwogroups w.r.t.anythirdgroup.weshowedthepotentialofdeepinspectto detect such violations.
galhotra et al.
first applied the notion of software testing to evaluating software fairness.
they mutate the sensitive features of the inputs and check whether the output changes.
one majorproblem with their proposed method themis is that it assumes themodeltakesintoaccountsensitiveattribute s duringtraining and inference.
this assumption is not realistic since most existing fairness aware models drop input sensitive feature s .
besides themiswillnotworkonimageclassification wherethesensitiveattribute e.g.
gender race isavisualconceptthatcannotbeflipped easily.inourwork weuseawhite boxapproachtomeasurethe biaslearnedbythemodelduringtraining.ourtestingmethoddoes not require the model to take into account any sensitive feature s .
we propose a new fairness notion for the setting of multi object classification averageconfusiondisparity andaproxy averagebias to measure for any deep learning model even when only unlabeled testingdataisprovided.inaddition ourmethodtriestoprovideanexplanationbehindthediscrimination.acomplementaryapproachbypapernot et al.
showssuchexplainabilitybehindmodelbias in a single classification setting.
discussion threats to validity discussion.
in the literature bug detection debugging and repair areusuallythreedistincttasks andthereisalargebodyofworkinvestigatingeachseparately.inthiswork wefocusonbugdetection for image classifier software.
a natural follow up of our work will bedebuggingandrepairleveragingdeepinspect sbugdetection.
we present some preliminary results and thoughts.
acommonlyusedapproachtoimproving i.e.fixing imageclassifiers is active learning which consists of adding more labeleddata by smartly choosing what to label next.
in our case we canuse napvd to identify the most confusing class pairs and thentarget those pairs by collecting additional examples that contain individual objects from the confusing pairs.
we download sampleimagesfromgoogleimagesthatcontainisolatedexamplesof these categories so that the model learns to disambiguate them.
we retrain the model from scratch using the original training data 1131and these additional examples.
using this approach we have some preliminary results on the coco dataset.
after retraining we find that thetype2conf of the top confused pairs reduces.
for example thetype2conf baseball bat baseball glove is reduced from .
to .
and type2conf refrigerator oven is reduced from .
to .
.
unlike traditional active learning approaches that encourage labelingadditionalexamplesnearthecurrentdecisionboundaryof the classifier our approach encourages the labeling of problematic examples based on confusion bugs.
another potential direction to explore is to use deepinspect in tandem with debugging repair tools for dnn models like mode .deepinspectenablestheusertofocusdebuggingeffort onthevulnerableclassesevenintheabsenceoflabeleddata.for instance oncedeepinspectidentifiesthevulnerableclass pairs one canusethegan basedapproachproposedinmodetogenerate more training data from these class pairs apply mode to identify the most vulnerable features in these pairs to select for retraining.
wehavealsoexploredhowtheneuroncoveragethreshold th used in computing napvdaffects our performance in detecting confusion and bias errors.
we studied one multi label classification taskcocoandonesingle labelclassificationtaskcifar .table show how our precision and recall change when usingdifferent neuron coverage thresholds th .
we observed that for cifar and coco that deepinspect s accuracies are overallstable at .
th .
.
with smaller th .
too many neuronsareactivatedpullingtheper classactivation probabilityvectors closer to each other.
in contrast with higher th .
importantactivationinformationgetslost.thus weselect th .
for all the other experiments to avoid either issue.
table deepinspect impact of neuron coverage threshold on detecting confusion errors for coco nc threshold napvd mean 1std top tp fp precision recall tp fp precision recall .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table deepinspect impact of neuron coverage threshold on detecting confusion errors for cifar nc threshold napvd mean 1std top tp fp precision recall tp fp precision recall .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
threatstovalidity.
weonlytestdeepinspecton6datasetsunder settings.
we include both single class and multi class as welltable deepinspect impact of neuron coverage threshold on detecting bias errors for coco nc threshold avg bias mean 1std top tp fp precision recall tp fp precision recall .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table deepinspect impact of neuron coverage threshold on detecting bias errors for cifar nc threshold avg bias mean 1std top tp fp precision recall tp fp precision recall .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
asregularandrobustmodelstoaddressthesethreatsasmuchas possible.
anotherlimitationisthatdeepinspectneedstodecidethresholds forbothconfusionerrorsandbiaserrors andathresholdfordiscardinglow confusiontripletsintheestimationof avg bias.instead of choosing fixed threshold we mitigate this threat by choosing thresholds that are one standard deviation from the corresponding mean values and also reporting performance at top1 .
thetaskofaccuratelyclassifyinganyimageisnotoriouslydifficult.
we simplify the problem by testing the dnn model only for the classes that it has seen during training.
for example while training if a dnn does not learn to differentiate between black vs. brown cows i.e.
allthecowimagesonlyhavelabelcowandthey aretreatedasbelongingtothesameclassbythednn deepinspect will not be able to test these sub groups.
conclusion ourtestingtoolfordnnimageclassifiers deepinspect automatically detects confusion and bias errors in classification models.
we applied deepinspect to six different popular image classification datasets and eight pretrained dnn models including three so calledrelativelymorerobustmodels.weshowthatdeepinspect cansuccessfully detectclass levelviolations forbothsingle and multi label classification models with high precision.