what helped and what did not?
an evaluation of the strategies to improve continuous integration xianhao jin computer science virginia tech xianhao8 vt.edufrancisco servant computer science virginia tech fservant vt.edu abstract continuous integration ci is a widely used practice in modern software engineering.
unfortunately it is also an expensive practice google and mozilla estimate their ci systems in millions of dollars.
there are a number of techniques and tools designed to or having the potential to save the cost of ci or expand its benefit reducing time to feedback.
however their benefits in some dimensions may also result in drawbacks in others.
they may also be beneficial in other scenarios where they are not designed to help.
in this paper we perform the first exhaustive comparison of techniques to improve ci evaluating variants of techniques using selection and prioritization strategies on build and test granularity.
we evaluate their strengths and weaknesses with different cost and time tofeedback saving metrics on real world projects.
we analyze the results of all techniques to understand the design decisions that helped different dimensions of benefit.
we also synthesized those results to lay out a series of recommendations for the development of future research techniques to advance this area.
index terms continuous integration software maintenance empirical software engineering i. i ntroduction continuous integration ci is a software development practice by which developers integrate code into a shared repository several times a day .
however ci gains adoption in practice difficulties e.g.
and pain points e.g.
have been discovered about it.
as software companies adopt ci they execute builds for many of projects and they do so very frequently.
as workload increases two main problems appear the time to receive feedback from the build process increases as software builds often outnumber the available computational resources having to wait in build queues and the computational cost of running builds also becomes very high.
previous studies e.g.
have highlighted the long time that developers have to wait to receive feedback about their builds.
for example at google developers must wait minutes to hours to receive testing results .
even just the dependency retrieval step of ci can take up to an hour per build .
regarding the high cost of running builds that is also highlighted in other studies .
the cost of ci reaches millions of dollars e.g.
at google and microsoft .
while other problems exist for ci we focus on these two because they are the ones that most existing techniques have focused on addressing.
they are also interrelated since cost reduction techniques may also reducetime to feedback e.g.
skipping some tests may cause other tests to fail earlier.
multiple techniques have been proposed to improve ci.
most of them have the goal of reducing either its time tofeedback or its computational cost.
all such techniques consider the observation of build failures to be more valuable than build passes because failures provide actionable feedback i.e.
they point to a problem that needs to be addressed.
timeto feedback reduction techniques aim to observe failures earlier by prioritizing failing executions over passing ones.
these techniques may operate in two different levels of granularity by prioritizing test executions e.g.
or build executions e.g.
.
computational cost reduction techniques aim to observe failures only by selectively executing failing builds only saving the cost of executing passing ones.
they also may operate at two different levels of granularity selecting test executions e.g.
or build executions e.g.
.
to the extent of our knowledge the existing techniques to improve ci have been evaluated under different settings making it hard to compare them.
previous studies used different software projects different metrics and rarely compared one technique to another.
however we expect that different choices of goal granularity and technique design will bring different trade offs.
for example cost reduction techniques at build granularity may be more risky than a test granularity one i.e.
it may save more cost when it skips all the tests in a build but it may also make more mistakes if it skips many failing tests in a build.
however the opposite may be true if test granularity cost reduction techniques also skip a large ratio of full builds i.e.
all the tests in the build .
on another example test selection techniques may be a good alternative to test prioritization techniques that also saves cost as an added benefit or they may instead delay the observation of test failures if they mispredict too many of them.
to the best of our knowledge how these trade offs manifest in practice is still mostly unknown.
empirically understanding these tradeoffs will have valuable practical implications for the design of future techniques and for practitioners adopting them.
in this paper we perform the first evaluation of the existing strategies to improve ci.
we aim to understand the tradeoffs between these techniques for three dimensions d1 computational cost reduction d2 missed failure observation ieee acm 43rd international conference on software engineering icse .
ieee and d3 early feedback.
for this goal we performed a large scale evaluation.
we replicated and evaluated all the existing ci improving techniques from the research literature representing the two goals time to feedback and computational cost reduction and the two levels of granularity build level and test level for which such techniques have been proposed.
we evaluated these techniques under the same settings using the state of the art dataset of continuous integration data travistorrent .
to be able to study all techniques we extended travistorrent in multiple ways mining additional travis logs github commits and building dependency graphs for all our studied projects.
finally we measured the effectiveness of all techniques with metrics in dimensions.
we included every metric that any previous evaluation of our studied techniques used refitted others and designed an additional one.
we analyzed the results obtained by all techniques on all metrics across all dimensions and we synthesized our observations to understand which design decisions helped and which ones did not for each dimension.
finally we further reflect on our results to provide a wide set of recommendations for the design of future techniques in this research area.
the main contributions of this paper are the first comprehensive evaluation of ci improving techniques a collection of metrics to measure the performance of ci improving techniques over various dimensions an extended travis torrent dataset with detailed test and commit and dependencies information the replication of variants of ciimproving techniques evidence for researchers to design future ci improving techniques.
ii.
a pproaches to improve continuous integration we summarize technique families in table i and discuss each technique in detail in iii c. figure depicts a noninterventional example timeline of builds a timeline in which a build selection technique is applied a timeline produced by build prioritization technique a timeline where a testselection technique is applied and a timeline with applying a test prioritization approach.
the example timeline shows a chronological numbered sequence of builds in ci.
each build is made up of at least one test.
we depict each test suite as a rectangle with a test number e.g.
t1 .
failing tests are then highlighted in gray.
the length of the rectangle refers to the time duration for the test to be executed.
we depict skipped tests with a dashed rectangle.
in the most ideal cost saving scenario all of the passing tests would be skipped and all of the failing tests would be observed as soon as possible.
a. computational cost reduction test level granularity test selection techniques aim at automatically detect and label tests that are not going to fail.
these test level approaches collect information from test history and project dependency along with the current commit and use some heuristic models to detect failing tests and skip the others.
figure also illustrates how this type of techniques worksin the simulation timeline.
after a test selection approach is activated it selects a subset of tests e.g.
t2 in build t4 in build that it predicts to have a possibility to fail and decides to skip the others e.g.
t3 in build t1 in build .
for those tests that are not selected in the timeline and get skipped we depict them as dashed rectangles.
in this paper we consider it can skip some builds when it selects no test in those builds.
build level granularity build selection techniques aim at automatically detect and label commits and builds that can be ci skipped.
some approaches try to detect failing builds and skip those passing builds to achieve cost saving.
others aim at identifying commits that can be ci skipped.
figure illustrates how they work in the simulation timeline.
as a build level technique when build selection approach decides to skip a build e.g.
build normally it skips all of the tests in that build.
the inner test sequence is not changed and all of tests are run in an executed build.
b. time to feedback reduction test level granularity test prioritization techniques try to give high priority to tests that are predicted to be failed so that developers could be informed in a shorter time.
this family of approaches normally rearrange the execution order of tests within a build to make predictedto fail tests run earlier by analyzing information such as test failing history and test context.
figure depicts an example of how this type of techniques works in the simulation timeline.
with a test level approach being activated the ci system gives different tests different priorities and firstly executes those tests with a higher priority e.g.
t4 in build t2 in build as well as delays low priority tests e.g.
t1 in build t2 in build .
the sequence of test executions in this timeline gets rearranged and the start time for tests that are more likely to fail move ahead in time.
also all tests are executed at last.
build level granularity build prioritization techniques aim at automatically prioritizes commits that are waiting for being executed.
they favor builds with a larger percentage of test suites that have been found to fail recently and builds including test suites that have not been executed recently as an alternative path.
figure also shows how this family of techniques works in the simulation timeline.
build prioritization techniques will only be activated when there is a collision of builds i.e.
there are multiple builds waiting to occupy the limited resource .
the technique is build level so it will not change the inner order of the test executions and it will normally change the sequence of tests across builds when the approach is activated e.g.
build .
none of tests become dashed in this timeline because they all eventually execute.
iii.
r esearch method in this paper we replicated and evaluated variants of ci improving techniques covering their two goals timeto feedback and computational cost reduction and their two levels of granularity build level and test level with perfect 214no intervention timelinet1t2t3 t4 t1build t2 t1t2t3t4t1t2t4 t1t2t3t4 build selection timelinet1t2t3 t4 t1 t2 t1t2t3t4t1t2t4 t1t2t3t4t1t2t3 t4t2 t1t2t3t4 t1t2t4t1t2t3t4build prioritization timelinet1 t1t2t3 t1 t2 t1t2t3t4t1t2t4 t1t2t3t4 t4test selection timelinetest prioritization timelinet1t2t3 t1 t1t2t4 t4t2 t2t3t4t1 t1t2 t3t4build build build build build build build build build build build build build build build build build build build build build build build 6build build build build build build 6time to feedback reductioncost reductionfig.
example timeline.
failing tests in gray.
build selection runs builds fully when it predicts a failing build.
test selection runs builds partially for tests that would fail .
build prioritization changes the build sequence.
test prioritization changes the test sequence within a build.
technique for the ideal timeline.
we evaluate them over software projects in travistorrent which we extended to be able to run all such kinds of techniques.
our goal is to understand the trade offs between existing ci improving techniques and between the metrics that have been used to evaluate them.
we perform empirical studies to analyze these trade offs for the following dimensions of ci improving techniques using metrics.
we only include selection techniques in empirical study since prioritization techniques have no power in cost saving by nature.
we involve selection and prioritization techniques in empirical study because both of them can have an impact on fault detection e.g.
wrongly skipped failing builds by selection approaches can cause delay in fault detection.
empirical study cost saving d1 computational cost reduction d2 missed failure observation empirical study time to feedback reduction d3 early feedback for each dimension we study rq1 what design decisions helped this dimension?
rq2 what design decisions did not help this dimension?
a. data set we perform our study over the travis torrent dataset which includes projects java projects and ruby projects with data for build instances.
we remove toy projects from the data set by studying those that are more than one year old and that have at least builds and at least lines of source code which is a criteria applied in multiple other works .
to be able to evaluate testgranularity techniques we also filter out those projects whose build logs do not contain any test information.
we focused our study on builds with passing or failing result rather than error or canceled since they can be exceptions or may happen during the initialization and get aborted immediately before the real build starts.
besides in travis a single push or pullrequest can trigger a build with multiple jobs and each jobcorresponds to a configuration of the building step.
we did a preliminary investigation of these builds and found that these jobs with the same build identifier normally share the same build result and build duration.
thus as many existing papers have done we considered these jobs as a single build.
after this filtering process we obtained builds from projects failing builds .
to be able to execute all our studied techniques we extended the information in travistorrent of these projects in multiple ways.
first of all we needed to know the duration of each individual test for the comparison and replication.
also to replicate some techniques e.g.
we needed to capture the historical failure ratio for each individual test.
to obtain these information we built scripts to download the raw build logs from travis and parse them to extract all of the information about test executions such as test name duration and outcome.
some techniques e.g.
require additional information that travistorrent does not provide for builds such as the content of commit messages changed source lines and changed file names.
for that we also mined additional information about commits in the projects code repositories through github.
then we matched each test with its corresponding test file in the project.
finally to be able to run other techniques e.g.
we built a dependency graph for the source code of each project using a static code analysis tool scitool understand to determine the paths between the source files and test files.
b. evaluation process we evaluate the techniques in a real world scenario to understand as best as possible the behavior that the techniques would show in practice.
we take two measures for that.
first we respect the original chronological order of build and test operations when training techniques.
we achieve that by using an fold chronological variant of cross validation.
for each project we split its chronological timeline into folds.
we use the first chronological fold only for testing and we iteratively test the other folds.
for each testing fold 215table i studied techniques.
goal approach granularity studied technique time to feedbackprioritizationtestptmarijan13 ptelbaum14 ptthomas14 build pbliang18 computational costselectionteststgligoric15 stherzig15 stmach19 buildsbhassan17 sbabd19 sbjin20 we train on all the folds that precede it chronologically.
this approach has been used in previous works e.g.
to avoid training with information that would not be available in practice i.e.
it happens in the future.
we follow this approach for all the techniques based on machine learning e.g.
.
for techniques that do not require training e.g.
we simply execute them over the same last folds.
for techniques that train on data from other projects i.e.
for cross project technique variants we also executed them over the same last fold timeline and we divided them into project folds to do cross project cross validation i.e.
for each project the technique is trained on other projects and tested on its last fold data.
second we respect the real world availability of information.
that is for selection based techniques when a build or test is skipped the technique will not know its outcome.
for techniques that rely on the last build or test outcome e.g.
we only inform them of the outcome of the last executed build or test.
additionally when builds are skipped we accumulate their code changes into the subsequent build.
c. replicated techniques we replicated and studied all the techniques that have been proposed to improve ci by reducing the time to feedback or reducing its cost.
in addition to these there are other techniques that were proposed before ci and that could also be applied for these two goals test prioritization techniques and test selection techniques.
therefore we also replicated and studied a state of the art technique in each of these two categories that were not originally proposed for ci.
we summarize all our studied techniques in table i. in total we studied techniques across two goals reducing time to feedback and cost and two granularities test and build levels .
since we also studied multiple variants of some techniques our evaluation included total technique variants.
to provide a reference point we also studied a perfect technique perfect technique.
it achieves the goal of each metric perfectly it predicts which tests or builds will fail with accuracy prioritizing or selecting them perfectly.
we include the detailed description for each technique in iv a and v a.iv.
e mpirical study c ost saving a. studied techniques test selection techniques we replicated all the testselection techniques that were proposed for improving ci stmach19 and st herzig15 .
to provide even more context for our study we also evaluate a state of the art testselection technique st gligoric15 since test selection techniques have also been proposed outside the context of ci e.g.
.
stgligoric15 skips tests that cannot reach the changed files by tracking dynamic dependencies of tests on files.
a test can be skipped in the new revision if none of its dependent files changed.
the rationale is that tests that cannot reach changed files cannot detect faults in them.
stherzig15 is based on a cost model which dynamically skips tests when the expected cost of running the test exceeds the expected cost of removing it considering both the machine cost and human inspection cost .
this technique tends to skip tests that mostly passed in the past or that have long runtime.
stmach19 proposes a machine learning algorithm with combined features of commit changes and test historical information.
we studied two variants of it one is trained in the past builds within the same project in which it is applied st mach19 w and the other is trained in the builds of different software projects than the one in which it will be applied st mach19 c .
it uses the following features file extensions change history failure rates project name number of tests and minimal distance.
build selection techniques we then replicated all buildselection techniques that jave been proposed for improving ci sb abd19 and sb jin20 .
to provide even more context for our study we also replicated a state of the art build prediction technique sb hassan17 .
sbhassan17 predicts every build s outcome based on the information from last build.
builds can be skipped when they are predicted to pass.
in our study information from the previous build is blinded if the build does not get executed.
we study two variants of this technique sb hassan17 wand sbhassan17 c as we did for stmach19.
sbabd19 uses a rule based approach to skip commits that only have safe changes e.g.
changes on configuration or document files.
this technique is expected to capture most failing builds since it only skips builds considered safe to skip.
sbjin20 aims at saving ci cost by skipping passing builds.
their strategy is to capture the first failing build in a subsequence of failing builds and continuously build until a passing build appears.
we replicated this technique under the configuration that provided the optimal effectiveness .
we studied three variants of this technique sbjin20 w sbjin20 cas we did previously and also a rule of thumb variant sb jin20 s that skips builds with 4changed files.
b. d1 computational cost reduction we studied four metrics for d1.
we plot the result of each metric in a box plot where each box represents the distribution 216of values for all the studied projects.
studied metrics build time saved measures the proportion of total build time that is skipped among all build time per project.
it was covered in sb abd19 .
test time saved measures the same as the previous metric but in terms of test time.
the previous work st gligoric15 used this metric in its evaluation.
it shows how much time applying a technique could save during the phase of test executions.
builds number saved measures the proportion of builds that are saved among all builds.
it was studied by sb abd19 and sb jin20 .
it represents how many resources could be saved as the number of builds.
tests number saved measures the same as the previous metric but in term of tests.
previous papers studied this metric.
it represents how many resources could be saved during test executions.
analysis of results comparing metrics.
when we compare the techniques test number vs. test time saved most of them saved a very similar ratio of test time than ratio of tests except st herzig15 .
when comparing build number vs. build time buildgranularity techniques saved a very similar ratio of build time as of builds.
also test granularity techniques saved a larger ratio of build time than of builds.
this means that testgranularity techniques save build time when they skip builds partially when they skipped some of their tests.
when comparing test number vs. build number buildgranularity techniques saved a very similar ratio of builds and tests.
also test granularity techniques saved a much lower ratio of builds than of tests some dramatically so stherzig15 and st mach19 c .
this means that testgranularity techniques saved a low ratio of full builds.
when comparing test time vs. build time build granularity techniques saved very similar ratios of test time and build time.
also test granularity techniques saved a much lower ratio of build time than of test time.
this observation extends our earlier one every build that these techniques did not skip fully and thus did not skip its build preparation time reduced their ability to save build time to an important extent.
comparing granularities.
by comparing test vs. buildgranularity techniques build granularity techniques generally saved higher build time cost except for sbabd19.
build granularity techniques have the advantage of skipping both test execution and build preparation time while testgranularity techniques have the advantage of skipping tests spread over many builds not only on those that get fully skipped.
our observation implies that skipping full builds was a better strategy for saving cost.
comparing techniques.
we first observed that sbmach19 c and sb jin20 c skipped fewer builds than their counterparts that were trained only with data within the same project sb mach19 w sb jin20 w .
after having been trained with a more diverse set of build and tests across many projects these techniques becameless confident to skip them.
st herzig15 saved very low ratio of build time despite saving a large ratio of tests.
this is because it very rarely skips tests that failed many times in the past regardless of the code changes in the build.
so within each build it very rarely skipped the tests with the most past failures thus very rarely skipping builds fully.
sbabd19 saved a median build time which is a relatively high amount considering that it only skipped builds with non executable changes e.g.
that only changed formatting or comments.
st mach19 w and st gligoric15 skipped a relatively high ratio of build time competitively with build selection techniques because they skipped many full builds.
this is because they analyze the relationship between code changes and tests inside a build.
st gligoric15 skips all tests that cannot execute the code changes and stmach19 w considers the distance between the changes and the tests in its predictor.
this allows both techniques to fully skip those builds in which no test can execute the code changes i.e.
when only non executable code was changed or when no tests exist to execute the changes.
sbjin20 w and sb jin20 s saved high ratios of build time since they both focused on skipping full builds.
while sbjin20 s provided higher savings we expect it to also skip a higher ratio of skipped failing builds see iv c sbjin20 s simply skips builds with commits.
finally sbhassan17 w and sb hassan17 c skipped too much build time higher than the perfect baseline .
this is because they mostly rely on the status of the previous build which is unknown if skipped.
so as soon as they observe a passing build they recurrently skip all subsequent builds.
c. d2 missed failure observation studied metrics proportion of skipped failing tests.
this metric measures the undesired side effect of cost saving techniques skipping some of the failing test cases.
it was used by stherzig15 .
proportion of skipped failing builds.
this metric measures the proportion of failing builds that are skipped among all failing builds.
it was covered in sb jin20 .
analysis of results comparing metrics.
all techniques generally skipped a very similar ratio of failing tests than builds with small differences.
stmach19 c st herzig15 st gligoric15 sb jin20 s skipped a slightly higher ratio of failing tests than builds.
this is explained by test granularity techniques skipping partial builds in addition to full builds and thus they also skipped a higher ratio of failing tests.
the case of sb jin20 s is different it skipped a higher ratio of tests because it skipped fewer builds with no failing tests few changed 4files.
sbabd19 sb jin20 c st mach19 w and sb jin20 w skipped a slightly higher ratio of failing builds than tests.
this means that these techniques skipped failing builds with lower than average or no failing tests e.g.
failing due to configuration or compilation errors which amount to of 217perfect st gligoric15st herzig15 st mach19 wst mach19 c sb hassan17 wsb hassan17 csb abd19sb jin20 ssb jin20 wsb jin20 ctests number saved perfect st gligoric15st herzig15 st mach19 wst mach19 c sb hassan17 wsb hassan17 csb abd19sb jin20 ssb jin20 wsb jin20 ctest time saved perfect st gligoric15st herzig15 st mach19 wst mach19 c sb hassan17 wsb hassan17 csb abd19sb jin20 ssb jin20 wsb jin20 cbuilds number saved perfect st gligoric15st herzig15 st mach19 wst mach19 c sb hassan17 wsb hassan17 csb abd19sb jin20 ssb jin20 wsb jin20 cbuild time saved fig.
results for cost saving metrics.
prioritization techniques not included since they do not skip tests builds.
perfect st gligoric15 st herzig15 st mach19 w st mach19 c sb hassan17 w sb hassan17 c sb abd19 sb jin20 s sb jin20 w sb jin20 cproportion ofskipped failing tests perfect st gligoric15 st herzig15 st mach19 w st mach19 c sb hassan17 w sb hassan17 c sb abd19 sb jin20 s sb jin20 w sb jin20 cproportion ofskipped failing builds fig.
results for missed failure observation metrics.
prioritization techniques not included since they do not skip tests builds.
failing builds .
finally sb hassan17 c and sb hassan17 w skipped most failing and passing tests and builds.
comparing granularities.
build granularity techniques generally skipped higher ratios of failing builds and tests than test granularity techniques except for sb abd19.
theygenerally skipped a higher ratio of all tests and builds.
comparing techniques.
if we rank techniques on these two metrics of side effect we observe that they rank almost exactly in the opposite order as they would according to build time saved for d1 .
this shows a clear trade off between costsaving and its side effect of skipping failures.
v. e mpirical study .
d3 t ime to feedback reduction in d3 we study how much prioritization techniques advance the observation of failures and how much the side effect in d2 will influence it.
so we study all the time to feedback and computational cost reduction techniques.
a. studied techniques we only describe here the techniques that we did not describe in earlier sections prioritization techniques.
test prioritization techniques for this family of techniques we replicated all the test prioritization techniques that were proposed for improving ci pt elbaum14 and ptmarijan13 .
to further extend this study we also replicated the state of the art test case prioritization tcp technique.
we chose the technique that provided the highest effectiveness in the most recent evaluation of tcp techniques pt thomas14 .
tcp was a rich research area before ci became a common practice e.g.
.
we apply these techniques to prioritize tests within each build.
ptmarijan13 prioritizes tests that failed recently or have a shorter duration.
tests are ordered based on their historical failure data test execution time and domain specific heuristics.
218perfect pb liang18 st gligoric15 st herzig15 st mach19 w st mach19 c sb hassan17 w sb hassan17 c sb abd19 sb jin20 s sb jin20 w sb jin20 cpositions shifted for treated failing builds perfect pb liang18 st gligoric15 st herzig15 st mach19 w st mach19 c sb hassan17 w sb hassan17 c sb abd19 sb jin20 s sb jin20 w sb jin20 cpositions shifted for all failing builds perfect pb liang18st gligoric15st herzig15st mach19 w st mach19 csb hassan17 w sb hassan17 csb abd19sb jin20 ssb jin20 w sb jin20 cbuild queue length saved perfect pt marijan13 pt elbaum14 pt thomas14 st gligoric15 st herzig15st mach19 w st mach19 cpositions shifted for observed failing tests within a build 50050fig.
results for time to feedback reduction metrics.
ptelbaum14 favors tests that failed either recently or a long time ago.
ptthomas14 uses topic modeling to diversity the tests that get executed earlier.
every prioritized test is selected if it contains the most different topics from the previous test in its identifiers and comments.
the rationale behind this is that similar tests often find similar problems.
build prioritization techniques to the extent of our knowledge only one technique has been proposed to prioritize software builds pb liang18 .
pbliang18 executes builds containing a recently failing and recently non executed test in a collision queue.
we apply pb liang18 to prioritize builds within a build waiting queue as its previous evaluation did .
queues form when build executions overlap in time.
b. studied metrics positions shifted for observed failing tests within a build measures the shifted positions for all observed failing tests prioritized or not .
a similar metric to this one was used in the evaluations of pt marijan13 pt elbaum14 and ptthomas14 .
for test selection techniques we measure the average number of shifted positions for all remaining tests when a test is skipped the next one can now run one position earlier.
positions shifted for treated failing builds measures the number of builds between every treated delayed advanced failing build s original observation position and its new position.
this metric was studied by sb jin20 .
for testgranularity techniques this metric is not impacted since the build is still executed in the same position.
for build selection techniques we consider that when a build is skipped it will run as the next build its tests will run on it .
positions shifted for all failing builds measures the same as the previous one but now across all failing builds.
pbliang18 used a similar metric in its evaluation .
through this metric we can understand the impact of the previous metric over all builds.
build queue length saved this is a metric designed by us to measure how applying a technique could relieve the collision problem when multiple builds are waiting to be executed within a limited resource.
we follow the same configuration in pb liang18 s paper.
the build queue length refers to the median number of builds waiting ahead for each build in each project.
with a pre experiment on all projects we find that for only one project rails rails the median value of every build s waiting queue is bigger than .
thus we only report the result for this metric on that project.
c. analysis of results comparing metrics.
when comparing positions shifted for treated failing builds vs. all failing builds for all techniques the advance pb liang18 or delay others that they introduce in the observation of failing builds is much lower when measured across the whole population of failing builds.
the upside of this is that the undesired effect of most techniques i.e.
delay of failure observation is very low across all failing builds median builds .
the downside is that the desired effect of pbliang18 i.e.
advance of failure observation is also very low across all failing builds median builds .
next we compare the performance of test selection techniques i.e.
the only overlapping technique family in the positions that observed failing tests shifted within a build vs. the positions that failing builds shifted across all builds.
219we observe that test selection techniques provided some advancement in the observation of test failures lower than most test prioritization techniques while introducing a very low delay in observation of build failures median .
comparing granularities.
we did not observe a substantial difference when comparing granularities we observed stronger differences when comparing techniques.
comparing technique strategies.
when comparing technique strategies prioritization vs. selection test selection techniques provided some advancement in the observation of failing tests within a build but test prioritization techniques provided better results overall except pt elbaum14 .
comparing techniques.
ptmarijan3 and pt thomas14 behave very similarly despite their different approaches to prioritization and they are both close to perfect prioritizing most tests correctly.
pt elbaum14 provides a lower advancement of test failures also lower than many testselection techniques since it uses a simpler criterion prioritizing tests that were executed very recently or a long time ago.
all test selection techniques provided a very similar advancement of test failure observation except st herzig15 which was slightly better.
interestingly st herzig15 was one of the techniques with the lowest delay in build failure observation median for all failing builds .
at the buildgranularity pb liang18 had a very low impact in prioritizing builds because builds very rarely occurred concurrently in our dataset only the rails project had a meaningful number of concurrent builds.
an important metric in pb liang18 s original evaluation was the savings in the build queue length.
we plot the results for all techniques for this metric in figure .
interestingly we also observed that test selection and build selection techniques also had a strong impact in this metric less so for test selection techniques and sb abd19 because they skip fewer full builds see iv b2 .
regarding build selection techniques those that saved more builds see iv b2 also saved more in the build queue length metric but also introduced higher delays in build failure observation.
vi.
a nswers for research questions and implications we synthesize our observations and we lay out their implications to advance this area of research.
a. d1 computational cost reduction rq1 what design decisions did not help?
first we report on missed opportunities for saving more computational cost.
cost saving techniques focused on skipping passing builds and tests but they did not specifically target those that would provide the highest savings i.e.
slower tests slower builds or all tests in a build in the case of tests selection .
this is demonstrated by the fact that build granularity techniques saved similar ratios of test number test time build number and build time and that test granularity techniques saved similar ratios of test number and test time and lower ratios of build time than test time.we also learned that training cost saving techniques across projects harmed their predictions.
in other fields training with data from multiple projects is considered to increase the accuracy of predictors.
for cost saving techniques though this exposed the techniques to more diverse sets of failures making more builds tests look like a failure resulting on the predictors saving less cost being less inclined to skip builds and tests .
test selection techniques were also limited in the cost that they could save when they did not target saving full builds stmach19 c and st herzig15 saved very low build time despite saving a high ratio of tests.
an additional aspect that contributed to st herzig15 saving limited build time despite saving high number of tests is that it only used features characterizing the tests but not the code changes in the build e.g.
missing the opportunity to skip full builds for no code changes.
rq2 what design decisions helped?
other design decisions allowed techniques to save high cost.
a particularly useful design decision was trying to predict seemingly safe builds and tests sb abd19 saved builds simply by skipping builds with no code changes and st gligoric15 saved builds skipping tests that did not cover the code changed in the build.
another decision that provided high cost savings was to skip full builds instead of individual tests thus also saving build preparation time.
skipping all tests in a build allows to skip the time to prepare the build i.e.
compilation and other overhead like virtual machine preparation and we observed that build preparation takes a large portion of build time.
an illustrative example is how st gligoric15 and st herzig15 saved about the same ratio of test time but stgligoric15 saved much higher build time because it saved a much higher ratio of full builds.
test selection techniques however performed really well in terms of saving a high ratio of tests by st herzig15 and by st machalica w .
this is because they could save some cost spread out across many builds i.e.
skipping partial builds achieved high cost savings.
however the testselection techniques that skipped full builds also achieved high savings.
intentionally or not st gligoric15 saved many full builds by simply skipping all tests that did not cover the changed code.
st mach19 w also saved many full builds by approximating the same idea one of its predictor s features is the distance between the changed code and the test.
implications for future techniques our results have multiple implications for the design of future techniques.
first we encourage future techniques to consider hybrid approaches to save both full builds and also partial builds i.e.
to save cost at both build and test granularity.
future techniques should also leverage the beneficial factors that we already observed such as skipping full builds with nocode changes or no tests to cover them.
to save more full builds novel prediction features could be designed targeting slower builds if possible which no existing technique attempts.
to save more tests existing techniques already 220provide very useful features saving a high ratio of tests but other new features could be designed to target saving more and slower tests and considering the relationships between the tests and the code changes in the build.
finally our observations also show that build time saved is the metric that most comprehensively shows the cost saved by all existing techniques even though cross referencing multiple metrics allows for additional observations as we did in this study.
b. d2 missed failure observation rq1 what design decisions did not help?
in terms of the proportion of builds and tests that were skipped by costsaving techniques we generally observe that the decisions that made techniques save higher cost also made them make more mistakes i.e.
skip higher ratios of failing builds and tests.
it was also particularly interesting that seeminglysafe techniques sb abd19 and st gligoric15 still showed pretty high ratios of skipped failing builds and tests.
our study thus shows that skipping builds with no code changes or without tests to execute them is not enough to guarantee that they will not fail.
a quick look discovered that the builds and tests skipped by these techniques failed for different reasons such as configuration or compilation errors present in of failing builds .
rq2 what design decisions helped?
one design decision that reduced the skipped failing tests and builds was training techniques across projects.
all thec variants skipped lower ratios than their w counterparts except sbhassan17 c .
also test granularity techniques generally skipped lower ratios of failing tests than buildgranularity techniques did of builds.
implications for future techniques these results imply multiple recommendations for future techniques.
first future techniques should design novel features to predict failures that are caused by no code changes e.g.
configuration changes to avoid assuming that seemingly safe builds will not fail.
second future techniques should attempt to break this trade off between saving cost and skipping failures.
existing techniques generally increase cost savings by also increasing missed failure observations.
future techniques should attempt to improve one of the two dimensions by keeping the other one fixed or optimal .
finally future studies should propose new metrics to better assess the trade off between cost saving and skipped failures of various techniques since most techniques succeed in one at the expense of the other.
sbjin20 proposed the harmonic mean of the two as a balanced metric but further study is granted to understand whether both should be valued equally or in a weighted manner particularly considering the much higher ratio of passes to failures in ci datasets.
c. d3 time to feedback reduction rq1 what design decisions did not help?
unsurprisingly build selection techniques did not advance the observation of build failures at all but at least they introduced very low delays in the observation of failing builds and also savedsome computational cost .
similarly test selection techniques also introduced a small delay in the observation of test failures.
build prioritization also showed very limited advancement in observing failing builds but that was mainly because only one of our studied projects open source had some contention in the build queue.
we expect that industrial software project would obtain a much higher benefit from this approach.
finally we also observed that the build selection techniques that produced higher cost savings also introduced higher delays in build failure observation showing again the tension between both goals.
rq2 what design decisions helped?
the best techniques to provide early feedback were test prioritization techniques.
in fact ptthomas14 provided near perfect results.
we also found that test selection techniques provided lower but competitive advancement of test failure observation while also providing some cost savings.
for example st herzig15 provided high advancement of test failure observation within a build with very low delay of buildfailure observation while also saving some computational cost.
similarly we observed that build selection techniques could also provide reductions in build queue length that were competitive with build prioritization.
implications for future techniques for future techniques we recommend to combine test prioritization with test selection techniques since prioritization techniques could stop after the first failure is identified and save the cost of running the remaining tests.
we found that test prioritization techniques already reached very high results pt thomas14 is near perfect so the features that they use could be also very useful for test selection to save cost.
conversely existing testselection techniques that already perform very well for costsavings e.g.
stherzig15 could be improved in their ability to advance failure observation.
similarly we recommend to further study the application of build selection techniques to provide early observation of build failures by reducing the build queue via skipping builds in industrial projects in which parallel build requests are a larger issue.
finally there is also space to develop new metrics that could capture the balance that techniques provide across all dimensions d1 d3.
d. standing on the shoulders of giants our findings confirm and extend previous work d1 beller et al.
observed that test time is a low proportion of build time.
we extend this observation by finding that our studied test selection techniques infrequently skipped full all tests within builds which strongly limited their costsaving ability.
we thus recommend test selection to incentivize skipping full builds to save higher cost in ci.
d2 jin and servant observed a trade off of higher cost savings incurring more missed build failures in their technique.
we extend this observation by finding that all our studied techniques were affected by that trade off techniques ranked equally by cost savings as by missed failures .
we additionally identified clear strategies that made techniques miss fewer failures training across projects and operating 221at test granularity.
we also observed that a seemingly safe technique still missed a high ratio of failures.
finally we elicited the need for better prediction of safe builds and new metrics to compare trade offs.
d3 herzig et al.
found that their test granularity technique incurs low delay in build failure observations.
we extend this observation by finding that all our other studied test granularity techniques also incur low build failureobservation delay measured across all failing builds.
vii.
t hreats to validity a. internal validity to guard internal validity we carefully tested our evaluation tools on subsets of our dataset while developing them.
our analysis could also be influenced by incorrect information in our analyzed dataset.
for this we studied a popular dataset that is prevalent among continuous integration studies travistorrent .
furthermore many of our studied techniques were originally evaluated on travistorrent projects.
additionally we extensively curated travistorrent removing toy projects following standard practice unusable projects for test granularity techniques and cancelled builds as in past work .
finally we also followed the advice in gallaba et al.
s study to consider the nuance in the travistorrent dataset.
we did so in the following ways we considered passing builds with ignored failures as passing.
developers manually flag such failures to be ignored when they cannot officially support them and thus should not represent the status of the build.
we considered builds that fail after another failure as correctly labeled because they flag an unsolved problem being informative for developers.
we considered failing builds with passing jobs as failing builds.
if at least one job fails it signals a problem informing developers.
our results may also be affected by flaky tests causing spurious failing builds.
however ci systems are expected to function even in the presence of flaky tests since most companies do not consider it economically viable to remove them e.g.
.
besides cross validation may make unrealistic use of chronological events to address this problem we used time based cross validation .
our observed build and test runtimes may have been influenced by the load experienced in the build server at the time.
however we consider this potential impact to be very low since we observed that the standard variance in test duration across builds was .
seconds.
b. external validity to increase external validity we selected the popular dataset travistorrent which has been analyzed by many other research works.
the projects we chose were all java or ruby projects because there are no projects with other programming languages in the data set.
although these two programming languages are popular different ci habits in other languages may provide slightly different results to the ones in this study.
our observations may slightly vary for separate softwareprojects but our goal was to derive general observations for a real world population of software projects.
c. construct validity a threat to construct validity is whether we studied software projects that are similar to those that suffer most accutely from high ci cost and delays in failure observation e.g.
the projects at google and microsoft .
we studied the travistorrent dataset which is the standard dataset used in the literature to evaluate techniques to save cost in ci .
one of our studied projects rails is particularly similar to industrial software projects.
rails was used alongside two other google datasets to evaluate pbliang18 et al.
and it had similar magnitudes of test suites thousands test executions millions and test execution time millions of seconds .
nevertheless early observation or prediction of build failures is beneficial regardless of how much load a project s ci system experiences.
it allows developers to not have to wait for builds to finish which is the motivation of multiple previous works e.g.
.
in particular abdalkareem et al.
found that developers from small projects as small as commits also chose to manually skip commits in ci to save time.
these savings can be substantial for the projects in our studied dataset test suite runtime varies from project to project median .
mins 75th percentile mins but more importantly saving full builds could save much higher cost median mins 75th percentile mins .
also many builds take longer than minutes .
test selection could save higher cost if it leaned harder towards skipping full builds but we found in this study that this incentive is not yet strongly leveraged by our studied test selection techniques.
viii.
r elated work a. empirical studies of ci and its cost and benefit multiple researchers focused on understanding the practice of ci studying both practitioners e.g.
and software repositories .
vasilescu et al.
studied ci as a tool in social coding and later studied its impact on software quality and productivity .
zhao et al.
studied the impact of ci in other development practices like bug fixing and testing .
stahl et al.
and hilton et al.
studied the benefits and costs of using ci and the trade offs between them .
lepannen et al.
similarly studied the costs and benefits of continuous delivery .
felidr eet al.
studied the adherence of projects to the original ci rules .
other recent studies analyzed testing practices difficulties and pain points in ci.
the high cost of running builds is highlighted by many empirical studies as an important problem in ci which reaches millions of dollars in large companies e.g.
at google and microsoft .
people believe that the benefit of ci is mainly lying in the early fault detection.
others find that projects adopting ci are able to adopt pull requests and release in a shorter time.
some also find that ci can help developer team 222in other areas such as providing a common build environment and increasing team communication .
b. approaches to reduce time to feedback in ci a related effort for improving ci aims at speeding up its feedback by prioritizing its tasks.
the most common approach in this direction is to apply test case prioritization tcp techniques e.g.
so that builds fail faster.
these techniques even though not designed to work in ci environment have been claimed to have a potential to provide ci users earlier fault observation.
another similar approach achieves faster feedback by prioritizing builds instead of tests .
their paper grants higher priority to those builds that are more likely to fail according to the historical failing information and works well for those projects that have a ton of collision issues.
naturally these kinds of techniques don t provide benefit in saving the cost.
in this paper we study both test prioritization techniques as well as build prioritization techniques in terms of advancement of failure observation and compare them with selection techniques.
c. approaches to reduce cost of ci a popular effort to reduce the cost of ci focuses on understanding what causes long build durations e.g.
.
thus most of the approaches that reduce the cost of ci aim at making builds faster by running fewer test cases on each build.
it is found that a ton of passing tests could be saved in this way .
some approaches use historical test failures to select tests .
others run tests with a small distance to code changes or skip testing unchanged modules .
recently machalica et al.
predicted test case failures using a machine learning classifier .
these techniques are based on the broader field of regression test selection rts e.g.
.
while these techniques focus on making every build cheaper other work addresses the cost of ci differently by reducing the total number of builds that get executed.
a related recent technique saves cost in ci by not building when builds only include non code changes .
they firstly create a rule based selection technique and then take advantage of machine learning algorithm to improve the accuracy.
then jin and servant propose a build strategy that developing team should skip those less informative passing builds through build outcome prediction.
finally other complementary efforts to reduce build duration have targeted speeding up the compilation process e.g.
or the initiation of testing machines e.g.
.
in this paper we refer cost reduction techniques as selection techniques.
we pick techniques in both build selection techniques and test selection techniques and examines their performance in different costsaving and fault observation metrics.
d. evaluation frameworks for similar techniques multiple research works focus on comparing cross tool performance with an evaluation framework.
zhu et al.
propose a regression test selection framework to check the output against rules inspired by existing test suites for threetechniques.
leong et al.
propose a test selection algorithm evaluation method and evaluate five potential regression test selection algorithms finding that the test selection problem remains largely open.
najafi et al.
studied the impact of test execution history on test selection and prioritization techniques.
luo et al.
conduct the first empirical study comparing the performance of eight test prioritization techniques applied to both real world and mutation faults and find that the relative performance of the studied test prioritization techniques on mutants may not strongly correlate with performance on real faults.
lou et al.
systematically created a taxonomy of existing works in test case prioritization classifying them in algorithms criteria measurements constraints scenarios and empirical studies.
differently to these works our study in this paper specifically targets the context of ci and it has a broader focus than test prioritization or selection.
our study is the first to compare all the techniques proposed to reduce time to feedback or cost in ci including prioritization and selection techniques at test and build granularities.
we performed observations comparing across goals dimensions metrics granularities and techniques.
most of our observations required comparisons at broad scope.
for example we revealed the need for a new incentive in test selection to skip full test suites to also save build preparation time which would not be relevant in studies outside the scope of ci.
ix.
c onclusions and future work in this article we performed the most exhaustive evaluation of ci improving techniques to date.
we evaluated variants of ci improving approaches from families on realworld projects.
we compared their results across metrics in dimensions.
we derived many observations from this evaluation which we then synthesized to understand the design decisions that helped each dimension of metrics as well as those that had a negative impact on it.
finally we provide a set of recommendations for future techniques in this research area to take advantage of the factors that we observe were beneficial and we lay out also future directions to improve on those factors that were not.
we lay out plans to combine approaches at test and build granularities to save further costs and to combine selection and prioritization approaches to improve on the early observation of failures while also saving some cost.
such techniques could consider additional historybased prediction features such as the project s code change history e.g.
since test execution history was beneficial for some techniques e.g.
.
we also discuss the need of future metrics to capture the various characteristics of these techniques in a more holistic way.
in the future we will work on designing a comprehensive technique that combines selection and prioritization as well as build and test granularities to maximize the benefit of ci while reducing its cost as much as possible.
x. r eplication we include a replication package for our paper .
223references r. abdalkareem s. mujahid and e. shihab.
a machine learning approach to improve the detection of ci skip commits.
ieee transactions on software engineering tse .
r. abdalkareem s. mujahid e. shihab and j. rilling.
which commits can be ci skipped?
ieee transactions on software engineering .
j. bell o. legunsen m. hilton l. eloussi t. yung and d. marinov.
deflaker automatically detecting flaky tests.
in ieee acm 40th international conference on software engineering icse pages .
ieee .
m. beller g. gousios and a. zaidman.
oops my tests broke the build an explorative analysis of travis ci with github.
in mining software repositories msr ieee acm 14th international conference on pages .
ieee .
m. beller g. gousios and a. zaidman.
travistorrent synthesizing travis ci and github for full stack research on continuous integration.
inproceedings of the 14th working conference on mining software repositories .
m. beller g. gousios and a. zaidman.
travistorrent synthesizing travis ci and github for full stack research on continuous integration.
inmining software repositories msr ieee acm 14th international conference on pages .
ieee .
n. bettenburg r. premraj t. zimmermann and s. kim.
duplicate bug reports considered harmful.
.
.
really?
in ieee international conference on software maintenance pages .
ieee .
a. celik a. knaust a. milicevic and m. gligoric.
build system with lazy retrieval for java projects.
in proceedings of the 24th acm sigsoft international symposium on foundations of software engineering pages .
acm .
b. chen l. chen c. zhang and x. peng.
buildfast history aware build outcome prediction for fast feedback and reduced cost in continuous integration.
in 35th ieee acm international conference on automated software engineering ase pages .
ieee .
s. elbaum a. g. malishevsky and g. rothermel.
test case prioritization a family of empirical studies.
ieee transactions on software engineering .
s. elbaum g. rothermel and j. penix.
techniques for improving regression testing in continuous integration development environments.
inproceedings of the 22nd acm sigsoft international symposium on foundations of software engineering pages .
w. felidr e l. furtado d. a. da costa b. cartaxo and g. pinto.
continuous integration theater.
in proceedings of the 13th acm ieee international symposium on empirical software engineering and measurement page .
m. fowler and m. foemmel.
continuous integration.
thought works thoughtworks.
com continuous integration.
pdf .
k. gallaba c. macho m. pinzger and s. mcintosh.
noise and heterogeneity in historical build data an empirical study of travis ci.
inproceedings of the 33rd acm ieee international conference on automated software engineering pages .
acm .
a. gambi z. rostyslav and s. dustdar.
improving cloud based continuous integration environments.
in proceedings of the 37th international conference on software engineering volume pages .
ieee press .
a. gautam s. vishwasrao and f. servant.
an empirical study of activity popularity size testing and stability in continuous integration.
in2017 ieee acm 14th international conference on mining software repositories msr pages .
ieee .
t. a. ghaleb d. a. da costa and y .
zou.
an empirical study of the long duration of continuous integration builds.
empirical software engineering pages .
m. gligoric l. eloussi and d. marinov.
practical regression test selection with dynamic file dependencies.
in proceedings of the international symposium on software testing and analysis pages .
f. hassan s. mostafa e. s. lam and x. wang.
automatic building of java projects in software repositories a study on feasibility and challenges.
in acm ieee international symposium on empirical software engineering and measurement esem pages .
ieee .
f. hassan and x. wang.
change aware build prediction model for stall avoidance in continuous integration.
in acm ieee international symposium on empirical software engineering and measurement esem pages .
ieee .
k. herzig m. greiler j. czerwonka and b. murphy.
the art of testing less without sacrificing quality.
in ieee acm 37th ieee international conference on software engineering volume pages .
ieee .
k. herzig and n. nagappan.
empirically detecting false test alarms using association rules.
in ieee acm 37th ieee international conference on software engineering volume pages .
ieee .
m. hilton n. nelson t. tunnell d. marinov and d. dig.
tradeoffs in continuous integration assurance security and flexibility.
in proceedings of the 11th joint meeting on foundations of software engineering pages .
acm .
m. hilton t. tunnell k. huang d. marinov and d. dig.
usage costs and benefits of continuous integration in open source projects.
inproceedings of the 31st ieee acm international conference on automated software engineering pages .
acm .
m. r. islam and m. f. zibran.
insights into continuous integration build failures.
in mining software repositories msr ieee acm 14th international conference on pages .
ieee .
r. jain s. k. singh and b. mishra.
a brief study on build failures in continuous integration causation and effect.
in progress in advanced computing and intelligent engineering pages .
springer .
x. jin and f. servant.
a cost efficient approach to building in continuous integration.
in ieee acm 42nd international conference on software engineering icse pages .
ieee .
x. jin and f. servant.
what helped and what did not?
an evaluation of the strategies to improve continuous integration mar.
.
available at a. labuschagne l. inozemtseva and r. holmes.
measuring the cost of regression testing in practice a study of java projects using continuous integration.
in proceedings of the 11th joint meeting on foundations of software engineering pages .
c. leong a. singh m. papadakis y .
le traon and j. micco.
assessing transition based test selection algorithms at google.
in ieee acm 41st international conference on software engineering software engineering in practice icse seip pages .
ieee .
m. lepp anen s. m akinen m. pagels v .
p. eloranta j. itkonen m. v .
m antyl a and t. m annist o. the highways and country roads to continuous deployment.
ieee software .
j. liang.
cost effective techniques for continuous integration testing.
.
j. liang s. elbaum and g. rothermel.
redefining prioritization continuous prioritization for continuous integration.
in proceedings of the 40th international conference on software engineering pages .
y .
lou j. chen l. zhang and d. hao.
a survey on regression testcase prioritization.
in advances in computers volume pages .
elsevier .
q. luo k. moran d. poshyvanyk and m. di penta.
assessing test case prioritization on real faults and mutants.
in ieee international conference on software maintenance and evolution icsme pages .
ieee .
m. machalica a. samylkin m. porth and s. chandra.
predictive test selection.
in ieee acm 41st international conference on software engineering software engineering in practice icse seip pages .
ieee .
d. marijan a. gotlieb and s. sen. test case prioritization for continuous regression testing an industrial case study.
in ieee international conference on software maintenance pages .
ieee .
a. memon z. gao b. nguyen s. dhanda e. nickell r. siemborski and j. micco.
taming google scale continuous testing.
in ieee acm 39th international conference on software engineering software engineering in practice track icse seip pages .
ieee .
j. micco.
the state of continuous integration testing google.
.
s. mostafa x. wang and t. xie.
perfranker prioritization of performance regression tests for collection intensive software.
in proceedings of the 26th acm sigsoft international symposium on software testing and analysis pages .
a. najafi w. shang and p. c. rigby.
improving test effectiveness using test executions history an industrial experience report.
in ieee acm 41st international conference on software engineering software engineering in practice icse seip pages .
ieee .
a. ni and m. li.
cost effective build outcome prediction using cascaded classifiers.
in ieee acm 14th international conference on mining software repositories msr pages .
ieee .
g. pinto m. rebouc as and f. castor.
inadequate testing time pressure and over confidence a tale of continuous integration users.
in proceedings of the 10th international workshop on cooperative and human aspects of software engineering pages .
ieee press .
m. rebouc as r. o. santos g. pinto and f. castor.
how does contributors involvement influence the build status of an open source software project?
in proceedings of the 14th international conference on mining software repositories pages .
ieee press .
g. rothermel and m. j. harrold.
analyzing regression test selection techniques.
ieee transactions on software engineering .
g. rothermel and m. j. harrold.
a safe efficient regression test selection technique.
acm transactions on software engineering and methodology tosem .
g. rothermel r. h. untch c. chu and m. j. harrold.
prioritizing test cases for regression testing.
ieee transactions on software engineering .
scitools understand.
understand static code analysis tool.
https scitools.com .
.
f. servant.
supporting bug investigation using history analysis.
in 28th ieee acm international conference on automated software engineering ase pages .
ieee .
f. servant and j. a. jones.
history slicing.
in 26th ieee acm international conference on automated software engineering ase pages .
ieee .
f. servant and j. a. jones.
history slicing assisting code evolution tasks.
in proceedings of the acm sigsoft 20th international symposium on the foundations of software engineering pages .
f. servant and j. a. jones.
whosefault automatic developer to fault assignment through fault localization.
in international conference on software engineering pages .
f. servant and j. a. jones.
chronos visualizing slices of sourcecode history.
in first ieee working conference on software visualization vissoft pages .
ieee .
f. servant and j. a. jones.
fuzzy fine grained code history analysis.
in ieee acm 39th international conference on software engineering icse pages .
ieee .
a. shi s. thummalapenta s. k. lahiri n. bjorner and j. czerwonka.
optimizing test placement for module level regression testing.
in ieee acm 39th international conference on software engineering icse pages .
ieee .
d. st ahl and j. bosch.
experienced benefits of continuous integration in industry software product development a case study.
in the 12th iasted international conference on software engineering innsbruck austria pages .
s. w. thomas h. hemmati a. e. hassan and d. blostein.
static test case prioritization using topic models.
empirical software engineering .
m. tufano h. sajnani and k. herzig.
towards predicting the impact of software changes on building activities.
in ieee acm 41st international conference on software engineering new ideas and emerging results icse nier icse .
b. vasilescu s. van schuylenburg j. wulms a. serebrenik and m. g. van den brand.
continuous integration in a social coding world empirical evidence from github.
in ieee international conference on software maintenance and evolution pages .
ieee .
b. vasilescu y .
yu h. wang p. devanbu and v .
filkov.
quality and productivity outcomes relating to continuous integration in github.
in proceedings of the 10th joint meeting on foundations of software engineering pages .
acm .
d. g. widder m. hilton c. k astner and b. vasilescu.
a conceptual replication of continuous integration pain points in the context of travis ci.
in proceedings of the 27th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering pages .
acm .
s. yoo and m. harman.
pareto efficient multi objective test case selection.
in proceedings of the international symposium on software testing and analysis pages .
acm .
s. yoo and m. harman.
regression testing minimization selection and prioritization a survey.
software testing verification and reliability .
l. zhang.
hybrid regression test selection.
in ieee acm 40th international conference on software engineering icse pages .
ieee .
y .
zhao a. serebrenik y .
zhou v .
filkov and b. vasilescu.
the impact of continuous integration on other software development practices a large scale empirical study.
in proceedings of the 32nd ieee acm international conference on automated software engineering pages .
ieee press .
c. zhu o. legunsen a. shi and m. gligoric.
a framework for checking regression test selection tools.
in ieee acm 41st international conference on software engineering icse pages .
ieee .
y .
zhu e. shihab and p. c. rigby.
test re prioritization in continuous testing environments.
in ieee international conference on software maintenance and evolution icsme pages .
ieee .