sat une a study driven auto tuning approach for configurable software verification tools ugur koc austin mordahlyshiyi weiyjeffrey s. fosterzadam a. porter department of computer science university of maryland college park md usa fukoc aporterg cs.umd.edu ydepartment of computer science the university of texas at dallas richardson tx usa faustin.mordahl sweig utdallas.edu zdepartment of computer science tufts university medford ma usa jeffrey.foster tufts.edu abstract many program verification tools can be customized via run time configuration options that trade off performance precision and soundness.
however in practice users often run tools under their default configurations because understanding these tradeoffs requires significant expertise.
in this paper we ask how well a single default configuration can work in general and we propose sat une a novel tool for automatically configuring program verification tools for given target programs.
to answer our question we gathered a dataset that runs four well known program verification tools against a range of c and java benchmarks with results labeled as correct incorrect or inconclusive e.g.
timeout .
examining the dataset we find there is generally no one size fits all best configuration.
moreover a statistical analysis shows that many individual configuration options do not have simple tradeoffs they can be better or worse depending on the program.
motivated by these results we developed sat une which constructs configurations using a meta heuristic search.
the search is guided by a surrogate fitness function trained on our dataset.
we compare the performance of sat une to three baselines a single configuration with the most correct results in our dataset the most precise configuration followed by the most correct configuration if needed and the most precise configuration followed by random search also if needed .
we find that sat une outperforms these approaches by completing more correct tasks with high precision.
in summary our work shows that good configurations for verification tools are not simple to find and sat une takes an important step towards automating the process of finding them.
index terms empirical software engineering software analysis testing verification and validation.
i. i ntroduction static program verification tools are a promising approach for reasoning about the correctness of software.
because the algorithms that back such tools present various precision soundness and performance1tradeoffs program verification tools often include a host of options for tuning the analysis.
however deciding how to set these options can be quite challenging as it may require deep knowledge of the analysis algorithms .
thus in practice many users rely on a default configuration recommended by developers for 1in this work when we speak of performance we are referring to the quality of producing correct results in other words maximizing the number of true positive and true negative results produced.typical scenarios.
unfortunately prior work suggests that the tradeoffs among options depend on the features of the program being analyzed .
in response to these challenges several researchers have explored ways to choose a program verifier or configuration of a verifier to best fit a target program and have selectively applied settings of the analysis configuration options .
other work aims to develop an understanding of certain kinds of configuration options in static analysis frameworks or tools .
to our knowledge the prior work has focused on relatively small and specific configuration or tool spaces.
as a result the effects of configurations on program verification tools is still poorly understood and it remains difficult to tune such tools.
in this paper we aim to address this gap in two steps.
first to better understand the configurability of program verification tools we perform an empirical study in which we construct and analyze a dataset of runs of four popular tools on a range of benchmarks.
second driven by the results of the empirical study we propose sat une a novel technique for automatically tuning the large configuration spaces of software verification tools.
our empirical study examines four tools that participate in the annual software verification competition sv comp cbmc and symbiotic verify c c programs andjbmc and jayhorn verify java programs.
we created a ground truth dataset by running sampled configurations of the four tools on a subset of sv comp benchmarks.
each of the tool configuration benchmark triples in our dataset is labeled as either producing a correct incorrect or inconclusive e.g.
timeout result.
we then analyzed the data to answer two research questions.
first we ask whether for each tool there exists a one size fits all configuration that produces a superset of complete correct results rq1 .
we found that even the most correct config the configuration that produces the most true positive and true negative results for a tool is unable to complete many verification tasks that other configurations could.
second we use statistical analysis to investigate the impact of individual configuration options on the tools performance and precision rq2 .
we found that for each tool at most half of the option settings have a statistically 36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee significant effect on the number of correct or incorrect results produced by a tool.
in other words many option settings do not change the results much.
we also found that the option settings that do have significant effects increase the number of correct results in some programs and decrease it in others suggesting their effects can vary greatly from program to program.
see section ii for details of our empirical study.
overall our study suggests that no single configuration is sufficient for general use and configurations may need to be tuned to the target program.
thus we propose sat une a novel technique that aims to find a good tool configuration for a given target program.
because the tools configuration spaces are very large and complex sat une finds configurations using a meta heuristic search driven by a fitness function where the fitness of a configuration is its predicted likelihood to terminate with a correct result on the target program.
we implement the fitness function as a machine learning model trained on the dataset from the empirical study.
one key feature of sat une is that it is both tool and languageagnostic as the approach only requires a labeled training dataset the ability to run a tool from the command line and the ability to process its output.
section iii describes sat une.
we evaluate sat une by comparing it against three baselines that simulate ways a user might tune a verification tool first using the configuration most correct config that produces the most correct results from our dataset second using the configuration best precision config that is the most precise i.e.
maximizes corrects corrects incorrects and if it does not complete trying most correct config and third using bestprecision config and if it does not complete using random search.
we found that compared to these baselines sat une provides the best balance between precision and number of complete results rq3 .
for example in the best case sat une was able to analyze more programs with higher precision than any baseline.
we also evaluated sat une in terms of run time and found that it was faster than random search and was comparable to the second baseline rq4 .
section iv presents our evaluation of sat une.
in summary our results suggest there is no one size fitsall best configuration for the studied program verification tools and that effects of individual configuration options can vary greatly from program to program.
thus we believe that sat une takes an important first step toward automating the process of finding good program verification tool configurations.
we have made our dataset results and the implementation of sat une publicly available at .
ii.
e mpirical study of configurable verification tools to better understand the configurability of program verification tools we studied four popular tools to examine the effects of both configurations as a whole and of individual configuration options.a.
study setup table i lists the verification tools we used in our study.
we chose these tools because they are among the bestperforming tools in sv comp they come with configuration options that impact the tools performance and precision they provide sufficient configuration option documentation so we can understand what individual option settings do and they target programs written in two widely used programming languages cbmc andsymbiotic verify c programs and jbmc andjayhorn verify java programs.
in our study we focus on the configuration options that affect analysis performance soundness and or precision instead of those that format output or toggle specific checkers.
column in table i shows the number of options we use in each verification tool.
for each option we identified its domain i.e.
the settings it can take on .
in terms of domain there were three different types of options in these verification tools.
first some options may be passed to the tool alone as an argument i.e.
as boolean flags.
inherently a boolean flag option has two possible settings fset unsetg.
for instance cbmc has boolean flag options such as partialloops which allows cbmc to model paths that only partially execute loops rather than fully unwinding them .
second options may have categorical settings.
cbmc has such options such as mm which specifies the memory model for concurrent applications .
this option can take on any of the following settings fsc tso psog.
finally some options take on numerical values.
due to the large number of settings for numerical options we use a set of representative values from their domains.
for instance the unwind option of cbmc which specifies the depth to which loops should be unwound accepts a positive integer we considered the following values for this option f1 100g.
we included all settings of boolean and categorical options.
each specific combination of option settings is a configuration.
column shows the number of possible configurations that can be created with the options and settings that we use.
configuration sampling the large number of options for our subject tools makes it infeasible to study the tools behaviors under all configurations column in table i .
research on combinatorial interaction testing has shown that sampling configuration spaces using covering arrays is an effective way to explore the behavior of configurable software .
furthermore past research also indicates that changes in the behavior of software tend to be caused by interactions of only a few options .
we therefore create a way covering array which is a list of configurations that include all way combinations of configuration option settings for each tool using an existing covering array generator .
column in table i shows the number of sample configurations i.e.
the sizes of the covering arrays.
target programs all of our target programs are taken from the sv comp competition .
to our knowledge the sv comp program set is the largest collection of verification benchmarks for which the ground truths i.e.
whether the benchmark is safe or unsafe according to some property 331table i subject verification tools.
tooltarget of config sample dataset lang.
options space size size size cbmc .
c .
symbiotic .
.
c .
jayhorn .
a java .
jbmc .
java .
are known.
furthermore this program set aggregates multiple benchmarks from across the literature increasing the generality of the conclusions we make.
it consists of over benchmark programs in c and java.
in our study we selected a subset on which we ran the verification tools with each sampled configuration.
for the two java tools we used all benchmark programs from sv comp .
these java programs are written with assertions and the verification tools check if these assertions always hold.
among the programs .
are known to be unsafe.
for c tools the sv comp benchmark has programs in total.2we randomly selected a subset of programs that are subject to only one verification check.
out of the programs we selected there are programs that are subject to concurrency safety verification to memory safety verification to integer overflow verification to reachability verification and to verification of termination.
among the programs are known to be unsafe.
dataset collection we executed each sampled configuration of the subject tools once on each benchmark task to create the dataset for our study.3in each execution we used a minute timeout.
for the purpose of studying the configuration spaces this timeout is sufficient because based on sv comp and results and of the conclusive runs took less than minute for cbmc symbiotic jayhorn and jbmc respectively .
we say a verification run is conclusive if it outputs a judgement that the target program is safe verified or unsafe rejected by the verifier .
in total we performed verification runs.
the sizes of the datasets for each tool range from to runs last columns of table i .
all experiments were conducted on an ubuntu .04lts machine with intel xeon silver cpus .10ghz and 144gb ram.
research questions our study answers two research questions rq1 do the subject tools have any one size fits all configurations?
rq2 what is the impact of individual configuration option settings on the tools results?
rq1 deals with the behavior of configurations as a whole.
we used the dataset to determine whether any subject tool has 2sv comp data was not available when we started this research.
the benchmark set for cis mostly the same between and .
3symbiotic does not check concurrency safety.
thus we did not run symbiotic for the programs that are subject to concurrency safety verification.table ii results of the sample configurations.
tool of verification tasks i.e.
programs allnever solvedcorrect incorrect inconclusive worst most bestprecision correct precision cbmc symbiotic jayhorn jbmc a one size fits all configuration i.e.
a configuration that can complete all verification tasks that were completed by at least one sampled configuration.
rq2 focuses on the effects of individual configuration option settings.
to address this research question we aggregated the number of correct and incorrect verification results for each tool.
we then performed a main effects screening analysis using anov a .
in this analysis we treat the configuration options as cardinal or ordinal factors i.e.
independent variables and the number of correct and incorrect results as the responses i.e.
dependent variables .
we create two models for each tool one for each response using the least square method.
each model shows the effect that each factor has on the response along with standard error and the p value.
in our analysis we consider the factors with statistically significant effects as those with p value .
b. study results rq1 do the subject tools have any one size fits all configurations?
table ii presents results for the subject tools.
columns and show the total number of tasks to verify and the number of tasks the subject tool could not complete within the minute timeout under anyconfiguration.
for each tool we identified the worst precision config the most correctconfig and the best precision config.
the worst precisionconfig is the configuration which had the lowest precision i.e.
corrects corrects incorrects .
the most correct config is the one which classified the most programs correctly and the best precisionconfig is the configuration that had the highest precision.
columns and in table ii show the number of correct incorrect inconclusive results for the worst precision config most correct config and best precision config respectively.
we find that no tool has a single configuration that could correctly verify all tasks other configurations did.
even the most correct config could not correctly verify to of the tasks that other configurations did.
for cbmc there is a large variance in the behavior of different configurations.
the most correct config only completed tasks .
correctly.
however in aggregate of the verification tasks could be completed correctly by some configuration of cbmc.
we observe similar results forjayhorn.
its most correct config verified tasks correctly yet of the tasks were correctly verified by some configuration.
furthermore both configurations produce many more incorrect results than the best precision config for example cbmc s most correct config had incorrect 3321int main 2float x .0f 3float x1 x .5f 5while x16 x x x1 x1 x .5f assert x return a p1 safe define n 2int main 3int i a 4for i i n i a 6for i i n i a 8for i i n i a for i i n i assert a return b p2 unsafe fig.
simplified code examples from the sv comp .
results compared to only incorrect result using its bestprecision config.
depending on the user s requirements this may be an unacceptable tradeoff.
the most correct configs for symbiotic andjbmc are more promising.
symbiotic s most correct config which was also its best precision config correctly verified of tasks and of tasks that could be correctly completed by any configuration.
jbmc also had the same best precision config and most correct config which happened to be its default used in sv comp.
this configuration completed tasks correctly with incorrect results.
jbmc could complete all but i.e.
.
tasks correctly with some configuration.
the above findings suggest that even for the tools with better single configurations there is still significant room for improvement if the right configuration can be identified for a given verification task.
rq2 what is the impact of individual configuration option settings on the tools results?
the results of the main effects screening analysis are summarized in table iii.
column lists the option settings with statistically significant effects on one or both responses.
the number next to each tool s name in column is the number of options with statistically significant settings.
overall for all verification tools at most half of configuration options had at least one setting with a statistically significant effect on the verification results.
furthermore a majority of such option settings presented tradeoffs in that they either increased or decreased both responses together highlighted as underlined blue in table iii .
specifically there were and option settings with statistically significant effects for cbmc symbiotic jayhorn and jbmc respectively.
more notably for cbmc jayhorn and jbmc and of such significant option settings presented tradeoffs.
none of symbiotic s settings presented tradeoffs in our models.
as an example of the tradeoffs a settings can present cbmc s partial loops has an estimated effect of decreasing the number of correct and incorrect results by and respectively.
this option allows partial execution of loops which can make finding counterexamples at small unwinding bounds easier .
the drawback is that it may model spurious paths that do not exist in the original program which couldtable iii configuration option settings with statistically significant effects on the verification results ranked by the size of the estimated effect on the correct response.
options inunderlined blue present tradeoffs i.e.
increase or decrease both corrects and incorrects together .
options without a setting e.g.
partial loops of cbmc are boolean flag options.
tool configuration option settingssignificant effects on response correct incorrect cbmc partial loops .
.
nondet static .
.
paths def .
.
full slice .
.
refine strings .
.
no assumptions .
.
solver z3 .
n a paths fifo .
.
solver boolvector .
n a depth .
.
depth .
.
solver yices .
n a symbiotic overflow with clang .
.
explicit symbolic .
n a no slice .
.
undefined retval nosym .
n a repeat slicing .
.
jayhorn initial heap size .
.
heap mode bounded .
.
heap mode auto .
n a heap limit .
.
solver eldarica .
.
heap limit .
.
initial heap size .
n a inline size .
.
bounded heap size .
n a step heap size n a .
jbmc path def .
.
localize faults .
.
paths fifo .
.
java threading .
n a full slice .
.
slice formula .
.
depth n a .
depth n a .
symex driven lazy load .
n a cause incorrect results.
we use two code examples in figure to illustrate the tradeoff partial loops presents.
both examples were extracted from the sv comp program set.
in figure 1a p1 extracted from the program float divtrue unreach call c is a safe program in that the assertion at line always holds.
this is because the loop at lines keeps dividing xby 5until it reaches a very small number that is below the sensitivity of the float type in c. the loop eventually ends as the pre and post division values become .
in our dataset configurations incorrectly judged p1as unsafe and they all set partial loops.
this is because the analysis insufficiently 333unwinds the while loop and the partial loops option allows the analysis to accept the partial loop execution as a valid path.
to successfully verify p1 a configuration needs to include a sufficient level of loop unwinding and disable partial loops.
on the other hand p2 extracted from the program standard init5 false unreach call ground c is an unsafe program in that the assertion at line in figure 1b never holds.
in our dataset p2was correctly judged as unsafe only by the configurations that set partial loops.
for p2 analyzing all loop iterations is not necessary to determine that the assertion will fail as long as the first iteration of the loop at lines is analyzed.
therefore partially accepting loops is a safe assumption for p2.
the configurations that did not use this option including the best precision config spent too much time in loop unwinding and eventually timed out.
the above examples illustrate that the effectiveness of a tool s configuration options may depend on the target program.
however not all options present tradeoffs.
in table iii some option settings out of have uniform effects.
these option settings can have uniformly positive effects i.e.
they increase the number of corrects and or decrease the number of incorrects or uniformly negative effects i.e.
they decrease the number of corrects and or decrease the number of incorrects .
for example setting explicit symbolic insymbiotic is estimated to produce more correct results without a significant effect on the number incorrect results.
this option makes symbiotic initialize parts of memory with non deterministic values.
without this option evaluation is done with symbolic values a costly step that requires tracking many more execution paths causing symbiotic to timeout.
interestingly all of symbiotic s options had uniform effects.
one can use such uniform options in the configuration if the goal is to increase the likelihood of completing a verification task.
indeed we confirmed that the most correct config set out of of these options consistently with the models estimated effects e.g.
disabling overflow with clang .
however recall our answer to rq1 so still cannot produce a onesize fits all configuration that completes all the verification tasks other configurations did.
in summary not all configuration option settings have significant effects on the verification results.
those that do often present tradeoffs that depend on the target program further supporting our argument that these tools likely do not have any one size fits all configuration that would apply to all target programs.
iii.
t hesat une approach the results of our empirical study motivated us to design an automated approach to tune the configuration spaces of static verification tools so they can successfully verify more programs.
as shown in section ii the four tools under evaluation have very precise best precision configs.
however these configurations can complete fewer tasks relative to the total number of tasks all configurations can complete.
therefore we designed sat une for simulated annealing tune withthe goal of outperforming the tools most correct config in other words to maximize the number of correct results.
at a high level given a tool and target program sat une searches through the tools configuration space to find a configuration that is likely to complete with a correct verification result on the target program.
we made three key design choices based on the results of the study in section ii that differentiate sat une from other approaches.
the first design choice is the adaptation of a meta heuristic search algorithm.
the findings in section ii b demonstrate that there is no one size fits all configuration for any tool.
thus for each target program it is necessary to explore the configuration space for a suitable configuration.
however it is infeasible to explore every configuration of a verification tool with even a modest number of configuration options.
a meta heuristic search algorithm probabilistically explores such search spaces to quickly locate a suitable configuration with which to run the tool for a given verification task.
this choice is critical for both the efficacy and efficiency of sat une.
the second design choice is that of the fitness function f. to perform a meta heuristic search we need a method to determine the fitness of a configuration that way the search knows whether a new candidate configuration is better than the current best configuration.
the only way to know a configuration s true fitness would be to run the verification tool observe whether it completes and validate its result.
however the validation step may not even be possible to perform automatically and even if it were it would be prohibitively expensive to do repeatedly throughout the search.
instead we use f which is a learned model that effectively approximates the fitness of a tool configuration such approximations are commonly known as asurrogate fitness function in the literature .
our model needs to be trained once for each tool but then incurs little overhead when queried during search.
the model steers the search toward configurations that are likely to complete a verification task with a correct result.
the third design choice is in the data we use to train f. as demonstrated in section ii b there are many configuration options that present tradeoffs in terms of producing correct and incorrect results based on the specific target programs.
these options should be evaluated for each verification task individually and set in a way that will increase the chance of getting a correct result.
thus we train fnot only on the results of configurations in the dataset but also on the features of the target program so that it can learn the ways that options interact with program features see section iii b .
figure shows the workflow of sat une.
to use sat une a user provides a target program and a verification tool with an input configuration.4sat une then runs the tool on the target program using the provided input configuration.
if the run produces a conclusive result it is reported to the user in this case sat une incurs no overhead.
if the run does not lead to a conclusive result the meta heuristic configuration search 4in our evaluation we use the best precision config as the input configuration section iv .
334fig.
workflow of the sat une approach.
begins.
we now discuss two key components of sat une the meta heuristic configuration search and the surrogate fitness function.
a. meta heuristic configuration search there exist various meta heuristic search algorithms in the literature such as tabu search hill climbing genetic algorithms and simulated annealing .
among them simulated annealing has been shown to be more effective in finding fitter objects like covering arrays and orthogonal arrays in large and complex combinatorial spaces quickly .
considering the similarity in the search spaces of the mentioned successful applications we decided to derive our meta heuristic configuration search algorithm from simulated annealing .
at a high level simulated annealing is a stochastic search algorithm that iteratively searches for a good solution by altering the current state to generate a new candidate state called a neighboring state.
if the neighboring state is judged to be better than the current state according to some heuristic it is accepted as the current state for the next search iteration.
if the neighboring state is judged to be worse than the current state it may still be accepted probabilistically to help the model avoid becoming stuck in local optima.
the probability of selecting a worse configuration decreases over the search.
this is done through the three control parameters the initial temperature t0 the cooling rate rby which the temperature t is reduced every iteration and the stopping temperature ts.
higher temperatures lead to higher probabilities of accepting inferior candidates i.e.
inferior candidates are more likely to be accepted in early iterations than in later iterations which allows simulated annealing to be more flexible and exploratory early in the search process.
the search ends either when an acceptable solution is found according to some criteria or the temperature falls below ts.
algorithm depicts our meta heuristic configuration search algorithm that adapts simulated annealing.
the inputs to this algorithm are the tool s configuration space cs ho diwhere ois the set of configuration options and dis their domains such that di2dis the set of possible values that option oi2ocould take on sampled for integer domains and exhaustive otherwise the target program p and the surrogate fitness function f. lines perform initialization.
first we initialize the control parameters5ast0 ts r .
then we set the running temperature tto the starting temperature t0 and the isconclusive flag is initialized as ?indicating an inconclusive result.
at line the current configuration cand the best configuration c are both initialized with the default configuration or a randomly generated one if the default is not available .
at line the program representation vector vis initialized with the features extracted from p see section iii b .
at line fis used to compute the cost of c ec using the concatenation of vandcas denoted byhv ci.
ec is the probability of getting either an inconclusive or incorrect result if cwere used to run the verification tool on the target program represented with features v. lines to implement an iterative search process that aims to select a configuration that is likely to complete the verification task with a correct result.
on line ts t checks that the temperature thas not decreased below the stopping temperature ts which is the standard stopping condition for simulated annealing.
in addition the search stops if the verification run is conclusive because our goal is to find a configuration which will produce a conclusive verification result rather than an optimal one.
during each iteration of the inner loop lines the algorithm first generates a new neighboring configuration c0 line .
to generate a new neighboring configuration we change the value of a single option oiin the current configuration to another random value from its domain di.
this simple approach has been shown to be effective for exploring large search spaces in a cost effective manner in similar search problems e.g.
combinatorial testing .
the algorithm then computes the cost of c0using f line and reduces the running temperature by the cooling rate line .
this random configuration generation repeats until one of the acceptance conditions on line is met either e meaning c0is better than caccording to f or the algorithm decides to accept the inferior c0with probability e k e t .
this probability reduces with t. once a state is accepted cand ecare updated line .
if cis the best so far i.e.
ec ec thenc andec are also updated accordingly lines .
when a new c is found we run the verification tool using c for the task p. if the verifier produces a conclusive result the search ends and we return the result line .
otherwise the search continues to the next iteration.
note that running the verification tool is the most expensive step of the algorithm .
in comparison learning f computing 5we empirically determined these values with preliminary experiments that showed that t0andtsdid not impact the performance significantly while r did.
larger rvalues .
.
caused the search end too quickly without sufficient exploration while smaller values caused longer search times.
335algorithm meta heuristic configuration search.
function config search cs ho di p f t0 ts r .control parameters t t0 isconclusive ?
c c getrandomordefault cs v getprogramrepresentation p ec ec f hv ci .cost forhv ci while ts t isconclusive do repeat c0 getneighboringconfig cs c ec0 f hv c0i e ec0 ec t t t r until e 0 rand e k e t c ec c0 ec0 .accept ifec ec then c ec c ec .best config so far isconclusive runv erifier p c returnhisconclusive c i program representations generating random configurations and computing their cost take negligible time.
b. learning the surrogate fitness function we learn the surrogate fitness function fusing the dataset we created for the empirical study in section ii.
in this dataset each data point is of the form hv ci x where x2 fcorrect incorrect inconclusiveg is the verification result and is the concatenation operator.
by including both the program features and configuration in each data point our models can learn from the interactions between them.
f is trained to differentiate between data points with either an incorrect orinconclusive verification result and data points with a correct verification result.
effectively freturns the probability of producing an incorrect or inconclusive for a givenhv ci combination as its cost.
formally ec f hv ci p p recall that algorithm aims at minimizing ec which translates to locating configurations that are more likely to produce conclusive and correct results.
past research has applied a variety of models and features to learn from program code .
in this work we use a simple bag of words model for its simplicity and relative efficacy in representing programs for classifying static analysis results .
we represent a program as a frequency vector by counting the frequencies of program instructions like load store allocate and call and certain constructs like branches loops functions and primitive array pointer compound types.
this simplicity makes our approach somewhat languageagnostic i.e.
extendable to any programming language by identifying the relevant program instructions and constructs .c.
implementation we instantiated sat une to tune the configuration spaces of cbmc symbiotic jayhorn and jbmc .
we learned the fitness functions with a random forest algorithm from weka .
to construct the bag of words model we counted the occurrence of intermediate representation ir instruction types and different program features e.g.
loops branches and function calls in our benchmarks.
the program features were collected via simple static analyses for c and java.
c programs targeted by cbmc andsymbiotic are represented by frequency vectors for program features and llvm ir instructions .
java programs targeted by jayhorn and jbmc are represented by frequency vectors of program features and wala ir instructions .
llvm and wala are popular frameworks for the analysis of c and java programs respectively.
we implemented sat une in600 lines of java code with a command line interface cli .
the sat unecli takes a verification tool a target program to be verified and the initial configuration as input.
iv.
e valuation in this section we discuss the experimental setup for evaluating sat une and answer two research questions on how well it performs.
a. experimental setup we trained sat une s fitness function on the dataset we generated in section ii.
we then evaluated it against other potential strategies for selecting a verification tool configuration in terms of both correctness of the results and running time.
training of sat une s fitness function we split the dataset of each verification tool into five equal partitions that are disjoint by benchmark programs.
four of them are used for training the fitness function f while one partition is held out to evaluate sat une using the finternally .
we then rotated the partitions and repeated this process times to perform fold cross validation with five different random seeds.
since these repetitions allow all of the data to be used for both training and evaluation at different iterations we were able to evaluate sat une on the entire dataset.
in total we trained surrogate fitness functions random seeds fold cross validation tools .
we report the total number of conclusive results across all cross validation sets as sat une s results.
comparison baselines to the best of our knowledge sat une is the first tool and language agnostic approach that automatically configures program verification tools.
we designed three baselines that simulate ways a user might use and tune a verification tool.
the first baseline simulates a user who would only try a single configuration of a tool and accept whatever outcome that configuration gives.
we assume the user does not have extensive domain expertise.
rather than manually tuning the tool for a target program they simply try a configuration that 336table iv results for sat une and three baselines.
the numbers in normal font are the median of runs with different random seeds for sat une andprecision!random and the numbers in the smaller font are the semi interquartile range siqr .
forsymbiotic andjbmc the results of precision!correct are not shown because they are the same as the most correct config.
tool approach correct incorrect inconclusive precision cbmcsat une .
precision!
random .
precision!
correct .
most correct config .
symbioticsat une .
precision!
random .
most correct config .
jayhornsat une .
precision!
random .
precision!
correct .
most correct config .
jbmcsat une .
precision!
random .
most correct config does well overall.
in our evaluation we use the most correctconfig of each tool as this baseline.
the second baseline simulates a user who has more time to try to get a correct result.
in this case the user first tries a highly precise configuration e.g.
best precision config and if the result is inconclusive tries again on a less precise but performant configuration.
in our evaluation we first run each tool s best precision config.
if it produces a conclusive result we report it.
otherwise we fall back to run the most correct config and report the result.
we call this baseline precision!correct .
because the best precision config and most correct config are the same for symbiotic andjbmc section ii the results of precision!correct are the same as most correct config for these tools.
finally the third baseline simulates a user whose target program may be difficult for a verification tool to complete.
the user also has a large amount of time to experiment with different configurations to find one that may work.
in this baseline we start by using the best precision config.
if it fails to complete within the timeout a random search begins based off of the best precision config.
the neighbor generation strategy is the same as sat une in that for each iteration we randomly alter a single setting of a configuration option.
the random search continues until it finds a configuration that finishes within the time limit or it reaches attempts.
we call this baseline precision!random.
metrics we use three metrics in our evaluation the number of correct verification results precision i.e.
the percentage of conclusive results that are correct and the total run time to complete each verification task in minutes.
each experiment was repeated five times and we report the median and semi interquartile range siqr values for the first two metrics.
research questions our evaluation aims to answer two research questions rq3 can sat une correctly verify more programs than baselines?
rq4 how efficient is sat une?
rq3 compares sat une to the three baselines in terms of number of correct results and precision.
due to the randomness in both approaches we also performed statistical analysis to determine whether the results produced by precision!random are significantly different from those produced by satune.rq4 aims to determine how efficient sat une s metaheuristic configuration search is.
we explore the distribution of execution times to determine for each tool how sat une compares to the three baselines.
b. experimental results rq3 can sat une correctly verify more programs?
table iv presents the results of sat une and the three baselines for each tool using median and siqr metrics.
overall we find that compared to the baselines sat une consistently achieves the best balance between the number of correct results and precision.
in all tools but jayhorn sat une completes more tasks correctly than any other baseline strategy.
in symbiotic satune produced and more correct results than mostcorrect config and precision!random respectively without any more incorrect results.
in jayhorn precision!correct completed more tasks correctly than sat une but at the cost of more incorrect results.
notably sat une allowed cbmc to complete every task at the cost of only a single more incorrect result than the next best baseline precision!random .
in terms of precision sat une achieved higher precision than all other baselines for every tool but jbmc .
sat une was still highly precise in jbmc .
but recall that jbmc s best precision config achieved precision.
still sat une was able to complete more verification tasks correctly than most correct config injbmc .
interestingly we found that sat une was able to correctly complete some verification tasks that no single configuration in our study could i.e.
tasks that are in the never solved column of table ii .
sat une correctly completed 337and such tasks for cbmc symbiotic jbmc and jayhorn respectively.
this suggests that sat une s fitness function was able to generalize to configurations it had not previously seen in the training data.
last we discuss the variations in the results using the siqr metric.
overall sat une andprecision!random had small variations in their results while most correct config and precision!correct had none.
it is expected that no variation is present in the most correct config and precision!correct results because both are deterministic.
the variations of satune and precision!random are due to the randomness in their search process.
for sat une its largest variation from thecbmc results is still relatively small accounting for about of the total number of tasks.
rq4 how efficient is sat une?
figure illustrates the distribution of the execution times to verify each program y axis in logarithmic scale as box plots for all of our experiments.
each box plot represents the conclusive verification runs of a tool using sat une or a baseline approach.
the width of the box plots reflects the population size i.e.
the number of correct incorrect results shown in table iv .
of the data points fall inside the box.
the line inside the box is the median and the lower and upper ends of the box correspond to the first and third quartiles respectively.
the red dot and number show the longest time it takes for each approach to complete one task.
the other dots on the central line of each box show the outliers.
we found that all four approaches were relatively fast for most tasks with of tasks being completed in under a minute in all cases.
still we see that even in the worst case red dots sat une was 4x faster than precision!random.
this is attributable to sat une s fitness function specifically because it screens configurations in advance and only runs them if they are judged to be fit.
in jayhorn where sat une had the highest median and maximum run time it generated a median of configurations and only ran of them.
in contrast random search generated and ran a median of configurations.
figure also shows that most correct config was the only baseline that consistently did better than sat une in terms of median execution time.
this is expected since most correctconfig only runs a single configuration with seconds timeout .
more notably is that sat une is comparable to or in the case of cbmc outperforms precision!correct which only runs two configurations.
these findings demonstrate the efficiency of sat une and the significant advantage its fitness function gives it over other search strategies.
v. t hreats to validity here we enumerate the potential threats to the validity of our work and the steps we took to mitigate them.
first the benchmark dataset we used from sv comp is primarily composed of artificial benchmarks.
thus the conclusions we made about the relative quality of sat une compared to other baselines may not generalize to large real world .
.
.
.
.
.
.
.
.
.
.9cbmc symbiotic jayhorn jbmc satune precise random precise correctmost correct configsatune precise randommost correct configsatune precise random precise correctmost correct configsatune precise randommost correct config141632 task completion time in minutes log fig.
execution time of sat une and the baselines.
programs.
unfortunately we are unaware of a large realworld benchmark for which the ground truths are known but we believe the large number of programs we used and the diversity of the sv comp benchmark partially mitigate this potential threat.
second the configuration samples in our empirical study may not be representative of the tools full configuration spaces.
our configuration samples include every three way combinations of configuration option settings and past research in configurable software indicates that the majority of program behaviors are attributable to the interaction of few options .
finally there could be variance in the performance of sat une and precision!random caused by non deterministic operations in machine learning and configuration selection.
we partially mitigated this potential threat by running replications of each experiment and reporting the median and semi interquartile range values which suggested small variations rq3 .
vi.
r elated work to the best of our knowledge this work is the first to use meta heuristic search and machine learning to tune software verification tools with large configuration spaces.
our work is related to work that studies the configuration spaces of analysis tools selects a static analysis tool or a configuration of a static analysis tool that is most suited to a given task selectively applies algorithms in a static analysis uses machine learning models as fitness functions in meta heuristic search and tunes high performance computing systems for a given system architecture and hardware.
studies of tool configuration spaces.
we believe we are the first work to systematically study the configuration 338spaces of static program verification tools.
other work has engaged in similar goals with other types of static analyzers specifically focusing on the tradeoffs presented by different configurations and configuration option settings .
wei et al.
present a study that evaluates the tradeoffs in the different configurations of a numerical static analysis for java programs .
smaragdakis et al.
and lhot ak and hendren instantiate multiple variants of context sensitive pointsto analysis for java to understand the tradeoffs of different design decisions.
the tools we studied present much larger configuration spaces than those in the past studies that required us to apply statistical analysis to understand the impact of configuration options.
configuration and tool selection.
our work is also highly relevant to those that select strategies i.e.
full configurations within a static analysis tool predict or rank which static analysis tool is suitable for a given task and more broadly attempt to learn performance models of tool configuration spaces .
beyer and dangl present a selection approach that uses four manually defined binary program features to select between three manually defined verification strategies for cpa checker .
richter and wehrheim present p esco which uses machine learning to rank five cpa checker verifiers .
sat une differs from these efforts in that it explores large and complex configuration spaces in a tool and language agnostic way using meta heuristics instead of using predefined configurations manually defined heuristics or ranking tools.
other researchers have explored selecting a verification tool or sat solver from a set that would be the most appropriate for a given task.
for example tulsian et al.
present mux a machine learning based approach that uses features extracted from windows device drivers to select the fastest verification tool to analyze them .
xu et al.
developed sat zilla which is the most recent version of the sat zilla tool .
sat zilla given a sat problem and a set of sat solvers as input attempts to select a solver that will perform the best in terms of run time.
our research complements these works by selecting a configuration of a single static analysis tool.
the configuration spaces we select from are much larger than the sets of tools these works used motivating the need for a meta heuristic search strategy.
finally some software product line spl research is closely related to our work in that these works use machine learning or statistical methods to model the effects of setting configurations.
splc onqueror is a well known tool that tries to compute the optimal configurations of a tool given a target metric e.g.
precision or run time .
similarly ha and zhang s deepperf models a tool s configuration space by training deep neural networks .
nair et al.
s what predicts a performance model using a small number of sampled configurations by performing dimensionality reduction on a target program s configuration space .
flash by nair et al.
builds a performance model using sequential model based optimization which allows the model to continue learning about the configuration space as it explores it and producessamples .
finally nair et al.
demonstrated that cheap and inaccurate predictors that rank configurations often perform as well or better than other more expensive approaches .
other researchers have focused not on performance models but rather on evaluating and contributing sampling approaches for spls.
pereira et al.
evaluated six different configuration sampling approaches to determine their relative strengths and weaknesses for selecting representative configurations .
oh et al.
contributed a sampling approach which models feature spaces as counting binary decision diagrams and then produces truly random samples of spl configurations .
while sat une also aims to select configurations it also takes into account the features of the target program which allows it to consider the tradeoffs configuration options of the static program verification tools present.
selective static analysis.
other related work selectively applies static analysis algorithms to parts of the target program.
among analysis algorithms context sensitivity is the most studied .
for example wei and ryder present an adaptive context sensitive analysis that uses eight features extracted from the points to and call graphs of javascript programs .
other algorithms such as flow sensitivity have also been used to develop selective static analysis .
our work similarly studies the relationship between program features and analysis algorithms to achieve a good balance between performance precision and soundness but we consider a wide range of configuration options while being agnostic to the analysis algorithm.
in addition instead of developing new analysis algorithms or tools our approach automatically configures existing verification tools.
ml models as fitness functions.
several researchers explore the use of machine learning models as surrogate fitness functions.
brownlee et al.
demonstrated that a markov network could be an effective surrogate fitness function in genetic algorithms for feature selection in case based reasoning .
jin and sendhoff use ensembles of neural networks to improve the performance of evolutionary algorithms .
singh et al.
evaluated both regression models and radial basis functions as surrogate fitness functions in simulated annealing .
while these papers focus on improving the meta heuristic algorithms we are not aware of any other work that automatically learns surrogate fitness functions for tuning verification tools with large configuration spaces.
high performance computing.
lastly a distantly related line of work includes automatically tuning high performance computing hpc systems for a given system architecture and hardware.
agakov et al.
use machine learning to perform iterative optimization for hpc systems at compile time .
ansel et al.
present an extensible framework o pentuner that enables writing domain specific hpc tuners .
for a more comprehensive review of the literature on hpc system tuning we refer readers to a recent survey by ashouri et al .
our work differs from the work above in that we use a meta heuristic search augmented with a surrogate fitness function for tuning program verification tools which are not hpc systems to get a desired verification outcome.
339vii.
c onclusions and future work in this paper we presented sat une the first auto tuning approach for static program verification tools with large configuration spaces.
the design of sat une is motivated by an empirical study that provided important insights on the characteristics of configuration spaces and the impacts of individual configuration options on the precision and overall correctness of the verification tools results.
first we demonstrated that there is no one size fits all configuration in any tool.
even the most correct config could not complete certain tasks that other configurations did.
second we found that many configuration options present tradeoffs between precision and performance and they should be tuned individually for given target programs to get the most out of the tools capabilities.
sat une is novel in that it uses a simple meta heuristic search algorithm with surrogate fitness functions learned from data to explore large configuration spaces and avoid running configurations that are likely to produce incorrect results.
it is tool and language agnostic.
we applied sat une to four popular verification tools for both c and java programs and evaluated its performance using the ground truth datasets.
the evaluation shows that sat une achieves the best balance between performance and precision improvements compared to the baselines we used.
in future work we will integrate other machine learning techniques such as neural networks into sat une to train the surrogate fitness function.
this will elide the need to manually identify and extract program features and enable sat une to take advantage of more complex structural information that neural networks can potentially learn.
we will also extend the configuration generation step of sat une to incorporate the findings from our empirical study in section ii b. this will enable more effective scanning of the configuration space and help improve the tools precision by better avoiding configurations that are likely to lead to incorrect results.
we will also extend our dataset and apply sat une to additional tools targeting other programming languages.