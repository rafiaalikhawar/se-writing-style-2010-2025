efficient compiler autotuning via bayesian optimization junjie cheny college of intelligence and computing tianjin university tianjin china junjiechen tju.edu.cnningxin xu college of intelligence and computing tianjin university tianjin china xnxdw tju.edu.cn peiqi chen college of intelligence and computing tianjin university tianjin china chenpeiqi tju.edu.cnhongyu zhang the university of newcastle callaghan nsw australia hongyu.zhang newcastle.edu.au abstract a typical compiler such as gcc supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program.
due to the large number of compilation flags and the exponential number of flag combinations it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs.
over the years many compiler autotuning approaches have been proposed to automatically tune optimization flags but they still suffer from the efficiency problem due to the huge search space.
in this paper we propose the first bayesian optimization based approach called boca for efficient compiler autotuning.
in boca we leverage a tree based model for approximating the objective function in order to make bayesian optimization scalable to a large number of optimization flags.
moreover we design a novel searching strategy to improve the efficiency of bayesian optimization by incorporating the impact of each optimization flag measured by the tree based model and a decay function to strike a balance between exploitation and exploration.
we conduct extensive experiments to investigate the effectiveness of boca on two most popular c compilers i.e.
gcc and llvm and two widely used c benchmarks i.e.
cbench and polybench .
the results show that boca significantly outperforms the state of the art compiler autotuning approaches and bayesion optimization methods in terms of the time spent on achieving specified speedups demonstrating the effectiveness of boca.
index terms compiler autotuning bayesian optimization compiler optimization configuration i. i ntroduction compilers e.g.
gcc and llvm are responsible for transforming a source program written in a certain programming language e.g.
c and c into an executable program .
to improve the runtime performance of compiled programs a compiler supports a lot of code optimizations each of which can be enabled or disabled by a compilation flag.
for example gcc has hundreds of compilation flags to support optimizations.
one flag example yjunjie chen is the corresponding author.is finline small functions which can enable the optimization that integrates functions into their callers when their bodies are smaller than the expected function call code.
however since the effect of compiler optimizations depends on the program characteristics e.g.
program structures the same optimizations do not always lead to the same improvement in runtime performance when they are applied to different programs .
moreover due to the large number of optimization flags there are an exponential number of combinations of flags.
it is challenging for users to understand all the flags and their combinations and properly determine which flags should be enabled or disabled in order for the compiled programs to achieve the required runtime performance .
to ease usage of compiler optimizations compiler developers pre define several optimization levels e.g.
o1 o2 o3 in gcc and llvm each of which enables a fixed set of optimization flags and aims to achieve a certain optimization goal in general.
that is users can specify an optimization level to impose a default set of optimization flags for the compiler.
however due to the large differences of program characteristics there is no optimization level that can guarantee to achieve the required runtime performance measured in terms of the execution time of the compiled program for every program.
therefore it is necessary for each specific program to carefully tune the optimization flags in order to achieve required runtime performance instead of directly using the pre defined optimization levels.
this is especially essential for the applications that are sensitive to runtime performance.
in the literature many compiler autotuning approaches have been proposed to automatically tune optimization flags in order to achieve required runtime performance for a given program .
in general they iteratively test various settings of optimization flags we call an optimization flag setting an optimization sequence following the existing work with certain search strategies e.g.
random search or genetic algorithm and then output the best one when reaching the ieee acm 43rd international conference on software engineering icse .
ieee terminating condition.
although these approaches have been demonstrated to be effective to some degree they still suffer from the efficiency problem .
more specifically with tens to hundreds of compiler optimization flags provided by a compiler the number of flag combinations is exponential leading to a very huge search space.
therefore the existing approaches have to compile and execute programs with a large number of optimization sequences during the search process which incurs large cost especially when the compilation and execution are lengthy.
as an example for the program consumer jpeg c with gcc used in our study presented in section iv the state of the art approach irace cannot achieve the required runtime performance within the given time period i.e.
.
hours .
therefore it is very important to improve the efficiency of compiler autotuning.
to solve the problem in this paper we propose the first bayesian optimization based approach for compiler autotuning called boca .
bayesian optimization is recognized as an effective method to optimize an expensive toevaluate objective function .
it uses the accumulated knowledge in the known area of the search space to guide samplings in the remaining area in an iterative process so as to find the optimal sample efficiently.
over the years bayesian optimization has been applied to a wide range of applications such as hyperparameter optimization in deep learning .
however it is hard for traditional bayesian optimization methods to scale to high dimensional data due to their inherent mechanisms e.g.
relying on gaussian process to approximate the objective function .
thus they are inefficient for compiler autotuning where a large number of optimization flags need to be tuned.
some advanced bayesian optimization methods have been proposed to handle high dimensional data .
however since they are not designed for compiler autotuning they do not take the characteristics of compiler autotuning into consideration e.g.
only a small number of optimization flags referred to as impactful optimizations can have noticeable impact on the runtime performance of a specific program .
as a result the direct application of the existing bayesian optimization methods in compiler autotuning is not efficient which will be demonstrated in our study presented in section v b .
in this paper we carefully design boca to improve the efficiency of compiler autotuning.
to make boca scalable to a large number of optimization flags we incorporate random forest a tree based ensemble learning algorithm to build a probabilistic model to approximate the objective function based on the already evaluated samples.
then boca uses the model to predict the quality of unevaluated optimization sequences for the next sampling.
due to the sheer size of optimization sequences it is very costly to predict all of them in each iteration.
therefore we design a selection strategy in boca to select only a subset of candidate optimization sequences so that an optimization sequence that can achieve required runtime performance can be efficiently found by just predicting the subset.
in particular our selection strategy balances between exploitation and exploration exploitation by taking the advantage of flag importance measured based on the tree based model boca identifies the impactful optimizations and then fully enumerates their combinations exploration boca explores a number of settings of the remaining optimizations called less impactful optimizations with a decay function which is helpful to avoid local optimum by exploring less impactful optimizations and further improve the efficiency by utilizing a decay function .
we conducted extensive experiments to evaluate the effectiveness of boca on two most widely used c compilers i.e.
gcc and llvm and on two public c benchmarks i.e.
cbench and polybench widely used in the existing compiler autotuning work .
our experimental results demonstrate that boca does spend less time to achieve the required runtime performance and significantly outperforms the existing compiler autotuning approaches i.e.
one baseline rio and two state of the art approaches ga and irace .
the average improvement in terms of the time spent on achieving the required runtime performance is .
.
.
besides we compared boca with the existing bayesian optimization methods i.e.
one traditional method pal and two advanced methods flash and tpe .
the results show that boca significantly outperforms all of them confirming the necessity of designing a novel bayesian optimization method for compiler autotuning.
furthermore our evaluation confirms the effectiveness of the selection strategy and the decay function used in boca.
our work makes the following major contributions we propose boca the first bayesian optimization based approach for compiler autotuning which significantly improves the efficiency of compiler autotuning.
we design a novel selection strategy in boca to select a subset of candidate optimization sequences by exploiting impactful optimizations with a tree based model and exploring less impactful optimizations with a decay function.
we conducted extensive experiments on two widely used c compilers i.e.
gcc and llvm and two widely used c benchmarks i.e.
cbench and polybench .
the results demonstrate the effectiveness of boca.
ii.
b ackground and related work a. compiler autotuning in the literature a large amount of research work focuses on compiler autotuning .
here we briefly introduce some typical and state of the art approaches which are also used for comparison in our study presented in section iv .
random iterative optimization rio in each iteration it randomly sets each optimization flag enabled disabled and then evaluates the performance of the compiled program under the optimization sequence.
this process is repeated until the terminating condition is reached.
rio has been demonstrated to be effective and is regarded as the baseline in our study.
genetic algorithm based iterative optimization ga ga first constructs an initial set of chromosomes each of which is a random setting of optimization flags.
in each iteration new chromosomes are produced via crossover and mutation operations where the former exchanges the settings of some optimization flags in two chromosomes to produce new chromosomes and the latter randomly flips the setting of an optimization flag in a chromosome to produce a new one.
next new chromosomes are evaluated and the set of chromosomes are updated based on the evaluation results.
this process is repeated until the terminating condition is reached.
ga is a state of the art approach.
irace based iterative optimization irace irace learns the sampling distribution for each optimization flag which is used to set each optimization flag during the iterative process.
it first constructs a set of random settings of optimization flags as the initial set and learns the initial sampling distribution for each optimization flag.
in each iteration it produces new settings of optimization flags according to the learned sampling distributions and then new optimization flag settings are evaluated and the sampling distribution for each optimization flag is updated accordingly.
this process is repeated until the terminating condition is reached.
irace is also a state of the art approach.
besides there are some supervised approaches based on offline learning for compiler autotuning .
here we did not compare with them since their effectiveness heavily depends on the quality of training data used for offline learning and collecting a large amount of training data an instance includes a program a setting of optimization flags and the achieved performance of the compiled program under the setting is very costly in practice.
although the above introduced searchbased approaches have been demonstrated to be effective and do not rely on a large amount of training data they still suffer from the efficiency issue as presented in section i. therefore improving the efficiency of compiler autotuning is very important.
that is it is necessary to propose a novel approach that can spend less time to find an optimization sequence that can achieve required runtime performance.
b. bayesian optimization in this paper we incorporate bayesian optimization to improve the efficiency of compiler autotuning.
bayesian optimization is a method to optimize an objective function f which is expensive to evaluate .
its core idea is to use the accumulated knowledge in the known area of the search space to guide samplings in the remaining area in order to find the optimal sample more efficiently.
it is an on the fly iterative process consisting of two main steps building a surrogate model for approximating the objective function based on already measured samples and guiding the further sampling based on the surrogate model and an acquisition function.
in traditional bayesian optimization gaussian process gp is used to build the surrogate model .
an acquisition function is used to decide where to sample next so that animprovement over the current best observation is likely to be achieved.
the widely used acquisition functions include expected improvement maximum variance and maximum mean .
according to the acquisition function the sample with the best value will be selected and measured.
then the sample with its measured observation will be used to update the surrogate model for the next iteration.
for example pal is recognized as a typical bayesian optimization method which uses maximum variance as the acquisition function.
however traditional bayesian optimization methods cannot scale to high dimensional data e.g.
the large number of optimization flags and thus they have not been applied to the area of compiler autotuning before.
in a similar area i.e.
configurable software systems a state of the art approach called flash is based on bayesian optimization.
to overcome the shortcoming of gp flash replaces gp with cart classification and regression trees and uses maximum mean as the acquisition function since cart outputs only one value.
flash can handle more high dimensional data but it is still costly even unaffordable for compiler autotuning.
this is because the number of flags is large e.g.
the number of tuned flags for gcc in our study is leading to an extremely large number of flag combinations and flash has to enumerate and predict all unevaluated optimization sequences in each iteration.
furthermore compilers are also highly configurable systems.
in the area of configuring software systems many approaches have been proposed to predict performance of a configurable software system by training a model using a sample of configurations .
different from them our work aims to find an optimization sequence under which the compiled program can achieve required runtime performance .
in this paper we design a novel bayesian optimization based approach for efficient compiler autotuning which addresses the limitations of existing bayesian optimization methods.
iii.
a pproach a. problem definition we denote the set of optimization flags supported by a compiler as o fo1 o2 o mg where mis the size of the optimization set and oican be set to 0or1 1refers to disabling enabling it .
a specific setting of optimization flags is called an optimization sequence .
all the possible optimization sequences form the whole optimization space which is denoted as s. since the setting of each flag has two optional values i.e.
0and and the number of optimization flags is m the size of the optimization space is jsj 2m.
in this paper we aim to improve the efficiency of compiler autotuning i.e.
spending less time to find an optimization sequence in sthat can achieve the required runtime performance for a given program.
we call such an optimization sequence desired optimization sequence .
b. approach overview due to the huge optimization space and large evaluation cost it is challenging to efficiently find a desired optimiza1200less impactful optimizations ...impactful optimizations decay exploration... candidate optimization sequences... training setssurrogate model predict outputbest optimization sequencedesired optimization sequence iterationadd tobuild select based on acquisition functionidentify a overview of boca offset decayc1number of combinationsscale iterations b normal decay process of boca fig.
an illustration of boca tion sequence.
in this paper we propose the first bayesian optimization based approach for compiler autotuning named boca since bayesian optimization is well recognized as an efficient method to optimize an expensive to evaluate objective function .
however as presented in section ii b due to the high dimensional data i.e.
the large number of optimization flags in compiler autotuning both traditional bayesian optimization e.g.
pal and the state of the art bayesian optimization e.g.
flash methods used for configuring software systems are not affordable.
thus we carefully design the search process in our approach by incorporating random forest rf and proposing a novel selection strategy for candidate optimization sequences in order to overcome the thorny efficiency issue.
the selection strategy aims to select a subset of candidate optimization sequences which is very likely to include a desired one so that a desired optimization sequence can be found more efficiently by just predicting the subset instead of predicting all unevaluated optimization sequences.
in particular rf is a state of the art machine learning algorithm which is able to effectively learn the interactions and dependencies between optimization flags.
figure 1a shows the overview of boca.
in each iteration boca first learns a surrogate model via random forest based on the training set.
it then selects a set of candidate optimization sequences to be predicted by the surrogate model.
finally it selects the best optimization sequence among these candidates according to a acquisition function which will be evaluated and then used to update the surrogate model.
in the following we introduce our surrogate model and acquisition function in section iii c and present the selection strategy for candidate optimization sequences in section iii d and the overall algorithm of boca in section iii e. c. building a surrogate model there are two requirements for building a surrogate model a training set and a model building algorithm.
in boca a training instance is an optimization sequence and the execution time of a given program after compilation under the optimization sequence.
that is the vector of an optimization sequence is the feature vector of the training instance and the execution time is its label.
there are two sources of training instances.
initially boca randomly selects noptimization sequences and evaluates them to get the corresponding execution time which are used as the initial training set.
in each subsequent iteration boca selects the best optimization sequence from the remaining area of optimization space according to the acquisition function.
it then adds the best one and the associated program execution time to the training set.
based on a training set boca adopts random forest rf to build a surrogate model instead of gp in traditional bayesian optimization.
rf is an ensemble machine learning technique that merges multiple decision trees together to get a more accurate and stable prediction where each decision tree is built based on a subset of training data randomly sampled from the training set and a subset of features randomly sampled from the whole feature set .
the reasons why we replace gp with rf are twofold.
first gp cannot scale to high dimensional data .
in particular it has been found that recent work using gp based bayesian optimization in the se community is limited to around dimensional data i.e.
configuration options which is much smaller than the number of optimization flags.
however rf performs well for high dimensional data .
second gp is very sensitive to its parameters but rf performs stably because of its ensemble strategy.
in particular through such a state of the art machine learning algorithm it can more effectively learn the interactions and dependencies between optimization flags to guarantee the effectiveness of boca.
acquisition function.
boca uses expected improvement ei as the acquisition function since it considers both exploration measured by the standard deviation of a prediction and exploitation measured by the mean of a prediction when determining the next optimization sequence to be measured.
in gp the mean and the standard deviation of a prediction can be directly outputted.
boca uses rf to merge multiple decision trees and thus it acquires the mean and the standard deviation of a prediction based on the prediction of each decision tree.
more specifically given a candidate optimization sequence each decision tree makes a prediction for it and then boca calculates the mean and the standard deviation based on all these predictions made by decision trees.
d. selecting candidate optimization sequences based on the knowledge learnt from the optimization sequences that have already been measured boca searches for an desired optimization sequence in the remaining space.
1201due to the huge optimization space it is extremely costly to predict all the remaining optimization sequences using the surrogate model in order to find a desired optimization sequence.
therefore to reduce the cost boca selects a subset of candidate optimization sequences that is very likely to include a desired one so that the desired one can be efficiently found by just predicting this subset.
however selecting such a subset of candidate optimization sequences is also challenging since the optimization space is extremely huge.
to address this challenge we carefully design a selection strategy.
the key insight is that for a given program only a small number of optimizations can have a large influence on its execution time called impactful optimizations .
thus sufficiently exploiting the impactful optimizations is more likely to find a desired solution.
however precisely identifying those impactful optimizations is difficult.
to reduce the influence of noise it is also necessary to explore other optimizations called less impactful optimizations and the degree of exploration should be decayed with the precision of identifying impactful optimizations improving so as to avoid incurring overmuch cost.
by considering both exploitation and exploration for the optimization space boca selects a subset of candidate optimization sequences that is very likely to include a desired one.
in the following we present the processes of impactful optimization identification section iii d1 and less impactful optimization exploration with decay section iii d2 in detail.
impactful optimization identification.
boca adopts gini importance to measure the impact of optimizations since boca depends on multiple decision trees and gini importance is often used by tree based machine learning techniques to conduct feature selection.
in a decision tree each node has gini importance that measures the total decrease of gini impurity in the node after splitting using a feature referring to an optimization flag in our work .
here gini impurity in a node is a measure of the likelihood of a randomly chosen optimization sequence from the set of optimization sequences in the node would be incorrectly labeled if it is randomly labeled according to the label distribution of the set in the node.
please note that in our problem each feature actually has two optional values i.e.
and and thus a feature can be used for splitting at most one node in a decision tree.
therefore gini importance of a node is equal to gini importance of the feature used for splitting on this node in a decision tree.
more specifically the calculation of gini importance for a node dis shown in formula .
g d nd n where i is gini impurity of a node !left !rightrefers to the proportion of optimization sequences reaching the left right node from a node d ndis the number of optimization sequences in the node and nis the total number of optimization sequences in the whole training set.
further the calculation of gini impurity for a node dis shown in formula .
i d cx i 1p2 i where cis the number of classes in the set and piis the probability of picking an optimization sequence with class ifrom the set.
please note that although boca aims to construct a regression model through rf each decision tree actually splits the whole range into several intervals as classes during training enabling the calculation of gini impurity.
after acquiring gini importance of each optimization flag in each decision tree if the feature is used for splitting in the tree boca further calculates the impact of each optimization flag by merging all these decision trees as shown in formula .
in this formula uis the number of decision trees that use o for splitting on a node tis the total number of decision trees anddiis the node using ofor splitting in the ithdecision tree.
impact o pu i 1g di t by prioritizing optimization flags in the descending order of their impact boca identifies top koptimizations as impactful optimizations.
since impactful optimizations could have large influence on the execution time of the given program boca conducts extensive exploitation of them.
that is boca exploits all the combinations of these impactful optimizations.
here to avoid incurring large costs kshould be small.
we discuss the setting of kin our study section iv c and the impact of kon the results in section vi a. less impactful optimization exploration with decay besides setting these impactful optimizations a complete optimization sequence also needs to set the remaining optimizations less impactful optimizations .
intuitively for a fixed setting of impactful optimizations it is acceptable to randomly set the remaining optimizations to form a complete optimization sequence since the remaining optimizations have a slight influence on the execution time of the compiled program.
however it is difficult to precisely determine the impact of each optimization using any machine learning technique especially when the training set is not large at the first few iterations.
that is the remaining optimizations may also contain really impactful optimizations.
to reduce the influence of noise boca further explores many combinations of the less impactful optimizations for a fixed setting of impactful optimizations.
since it is impossible to explore all the combinations boca randomly selects a certain number of combinations to explore.
another benefit of the random exploration is to avoid local optima.
to further reduce the costs incurred by exploration boca gradually decays the degree of exploration i.e.
the number of randomly selected combinations of less impactful optimizations when the size of the training set is increasing.
this is because as the size of the training set increases the identification of impactful optimizations could become more precise and thus it is not necessary to extensively explore the remaining optimizations.
boca uses a exploration decay function to determine the number of explored combinations of less impactful optimizations for each fixed setting of impactful optimizations at each iteration.
since in the first few iterations the training set size is relatively small it is more likely to have much noise and thus the decay could be slow in the beginning.
subsequently the 1202training set size becomes larger and larger the noise could be reduced and thus the decay could be fast.
with this intuition boca uses a normal decay function as shown in formula .
c i c1 exp max i offset scale2 log decay where c1is the initial number of explored combinations of less impactful optimizations for each setting of impactful optimizations iis the iteration i c i is the number of explored combinations of less impactful optimizations at theithiteration and offset scale and decay control the decay shape of gaussian decay function.
figure 1b depicts the decay process of boca and the meanings of offset scale and decay .
based on the calculated number of explored combinations boca randomly forms c i combinations of less impactful optimizations for each setting of impactful optimizations thus the total number of selected candidate optimization sequences isc i 2kat the ithiteration.
by calculating the ei values of these candidate optimization sequences via the surrogate model boca selects the optimization sequence with the best ei value as the optimal one among the set then measures the execution time of the given program after being compiled under it and adds it to the training set for the next iteration.
e. overall algorithm of boca we formally present boca in algorithm .
for a given program to be compiled algorithm outputs a desired optimization sequence sbestthat makes the program achieve shorter execution time after being compiled under sbest and also the corresponding execution time f sbest .
we use f s to represent the execution time of a program after being compiled under an optimization sequence s. lines construct the initial training set denoted as train by randomly generating size initial optimization sequences and measuring their execution time.
lines conduct the iteration process of boca.
line builds a surrogate model via rf based on the training set.
lines acquire kimpactful optimizations by calculating the impact of each optimization and also treat the remaining optimizations as less impactful optimizations.
line enumerates all the settings of impactful optimizations.
besides setting impactful optimizations a complete optimization sequence also requires a setting of less impactful optimizations.
then lines randomly explore cnon repetitive settings of less impactful optimizations for each setting of impactful optimizations where cis calculated based on the normal decay function.
in this way a set of candidate optimization sequences denoted as allcandidates are obtained by considering both exploitation and exploration.
lines use the surrogate model to predict each candidate optimization sequence so as to obtain the mean and standard deviation of each prediction which are used to calculate the ei value of each candidate.
lines obtain the optimization sequence that has the largest ei value among all the candidates and is not in the training set measure this optimization sequence and then add it and its execution time to the training set for the next iteration.
lines obtain the currently optimal optimizationalgorithm pseudo code of boca input o a list of optimizations k the number of impactful optimizations size initial the size of the initial training set output sbest f sbest a desired optimization sequence the execution time of the given program after compilation under sbest 1train training set to build a surrogate model 2foreach ifrom to size initial do foreach jfrom to mdo s random randomly set or for ojin an optimization sequence s end train .add s f s 7end 8foreach ifrom to iterations do model randomforest train foreach jfrom to mdo importance getimportance oj model get the impact of oj end importantopts getimportantopts importance o k get top kimpactful optimizations unimportantopts set o importantopts importantsettings getallsettings importantopts get all the settings of impactful optimizations allcandidates foreach jfrom to size importantsettings do c normaldecay i get the number of explored settings of less impactful optimizations unimportantsettings getrandomsettings unimportantopts c randomly get cnon repetitive settings of less impactful optimizations allcandidates .add importantsettings unimportantsettings getccandidate optimization sequences by combining importantsettings and each in unimportantsettings end foreach jfrom to size allcandidates do mean std model .predict allcandidates ei ei mean std end bestcandidate getbestcandidate allcandidates ei get the candidate with largest eiand not in train train .add bestcandidate f bestcandidate iff bestcandidate f sbest then sbest f sbest bestcandidate f bestcandidate end 31end 32return sbest f sbest sequence among all the measured ones and line outputs the finally desired one when the terminating condition is reached.
iv.
e valuation in the study we address the following research questions rq1 how does boca perform compared with existing compiler autotuning approaches?
rq2 does boca outperform the existing bayesian optimization methods in compiler autotuning?
rq3 is the selection strategy proposed in boca effective?
a. compilers and programs we used two most popular open source c compilers gcc and llvm as subjects and two widely used c benchmarks cbench and polybench as programs following the existing compiler autotuning work .
following the prerequisites of the benchmarks and existing work we used gcc .
.
and llvm .
for the x86 linux platform.
in our study we tuned optimization flags for gcc and optimization 1203table i basic information of subject programs id program sloc description c1 consumer jpe g c image compression and decompression.
c2 security sha asecure hash algorithm.
c3 automoti ve bitcount testing bit manipulation abilities.
c4 automoti ve susan e image recognition for edges.
c5 automoti ve susan c image recognition for corners.
c6 automoti ve susan s image smoothing.
c7 bzip2e file compression and decompression.
c8 consumer tif f2rgba rgb formatted tiff image conversion.
c9 telecom adpcm c pulse code modulation.
c10 office rsynth text to speech synthesis program p1 2mm 2matrix multiplications.
p2 3mm 3matrix multiplications.
p3 cholesk y cholesk y decomposition.
p4 jacobi 2d d jacobi stencil computation.
p5 lu ludecomposition.
p6 correlation correlation computation.
p7 nussino v dpfor sequence alignment.
p8 symm symmetric matrix multiply.
p9 heat 3d heat equation over 3d data domain.
p10 covariance covariance computation.
flags for llvm including both the optimization flags in o3 the highest optimization level in both gcc and llvm and the optimization flags that have been demonstrated to have large influences on runtime performance of compiled programs .
regarding the used benchmarks we used programs including programs from cbench and programs from polybench in total.
here we did not use all the programs from the two benchmarks since the execution time of the remaining programs cannot be noticeably affected by compiler optimizations although tuning which has been demonstrated by the existing work .
each program is equipped with an input set which can be executed to measure the runtime performance of the compiled program.
table i shows the basic information of the used programs in our study where each column presents the id c is cbench and p is polybench the program name the number of source lines of code sloc and the brief description about the program respectively.
the code of boca the experimental data and the complete list of optimization flags used in our study are available at the project webpage b. compared approaches we considered three categories of compared approaches.
existing compiler autotuning approaches as presented in section ii a we considered three existing compiler autotuning approaches for comparison i.e.
rio ga and irace where rio is regarded as the baseline while ga and irace are the state of the art approaches for compiler autotuning.
existing bayesian optimization methods boca is the first bayesian optimization based approach for compiler autotuning which incorporates a machine learning technique rf and a novel selection strategy for selecting candidate optimization sequences.
in the literature many bayesian optimization methods have been proposed thus it is interesting to investigate whether or not our proposed bayesian optimization based approach boca can outperformthe direct application of existing bayesian optimization methods for compiler autotuning.
here we chose the following bayesian optimization methods for comparison a traditional method pal a state of the art method originally proposed for configuring software systems flash and an advanced general bayesian optimization method tpe .
pal and flash have been presented in section ii b. here we introduce tpe briefly.
different from other bayesian optimization methods the predictive model in tpe does not predict the posterior probability distribution value for f x at a candidate sample x. instead it models both the distribution ofxgiven f x and the distribution of f x and then derives the posterior probability distribution value for f x atxwhen calculating the acquisition function value of x. selection strategy an important component in boca for improving efficiency is the selection strategy for candidate optimization sequences thus it is important to investigate its contribution to boca.
an advanced strategy to construct a set of candidate optimization sequences in bayesian optimization is the local search strategy proposed in smac which conducts local search based on nbbest optimization sequences measured by the acquisition function.
more specifically for each best optimization sequence it first constructs moptimization sequences by flipping one flag setting where m is the number of optimization flags and then uses the optimization sequence with the best acquisition function value as a candidate optimization sequence produced from the best optimization sequence.
also this strategy randomly constructs nrcandidate optimization settings.
that is the total number of candidate optimization settings constructed via this strategy is nb nr.
therefore we compared boca with the variant that replaces the selection strategy used in boca with the local search strategy used in smac.
we call this variant boca s. c. implementations and configurations we implemented boca in python based on scikitlearn and numpy .
we adopted the implementation of rf provided in scikit learn and used its default parameter settings.
by conducting a preliminary study based on a small dataset we set k offset decay scale in boca to be .
and respectively.
we set the initial set size to be following the existing work and the total number of iterations to be .
we also investigated the influence of main parameters on the experimental results in section vi a .
for the compared approaches we adopted their implementations released by the corresponding work .
we also adopted the same parameter settings as the existing work and the same initial set size as boca for fair comparison except ga .
due to crossover in ga we cannot set the initial set size to thus we set it to the closest value i.e.
.
in the existing work the initial set size of ga is set to but it is very costly especially when the compilation and execution time of a program is large thus we did not adopt this setting.
to investigate the effectiveness of ga with a larger initial set size we also tried another initial size of .
we call the two ga implementations ga4and 1204c1 c2 c3 c4 c5 c6 c7 c8 c9 c10p1 p2 p3 p4 p5 p6 p7 p8 p9 p101.
.
.
.
.
.0speedup a gcc c1 c2 c3 c4 c5 c6 c7 c8 c9 c10p1 p2 p3 p4 p5 p6 p7 p8 p9 p101.
.
.
.
.
b llvm fig.
speedup of boca ga10 respectively.
following the existing work we set nbto be and nrto be in boca s. to reduce the influence of machine environment we ran a program under an optimization sequence times and calculated the average time.
to reduce the influence of randomness we ran each approach times and calculated the average results.
our study is conducted on a workstation with 16core intel r xeon r cpu e5 v3 126g memory and centos .
operating system.
d. measurement following the existing work we calculated the speedup of a compiler autotuning approach over o3 i.e.
the highest optimization level of gcc and llvm with the performance goal of minimizing execution time of compiled programs to measure the effectiveness of an approach.
the speedup is calculated by dividing the execution time of the program compiled under o3 by that of the program compiled under the optimization sequence produced by an approach.
as our work aims to improve the efficiency of compiler autotuning we compared these approaches in terms of the time spent on achieving i.e.
first time to reach or exceed a certain speedup.
here we set the total number of iterations for boca to be .
we compared them in terms of the time they spent on achieving the speedup achieved by boca at its 30th 40th 50th 60thiterations respectively.
less is better.
if a compared approach did not achieve a certain speedup within iterations times of the number of iterations of boca we regarded it as timeout .
in section vi b we also discuss the effectiveness of boca with more iterations.
v. r esults and analysis a. rq1 boca v.s.
existing compiler autotuning approaches we first show the speedups of boca within iterations over o3 on both gcc and llvm in figure .
from this figure boca indeed improves the runtime performance of compiled programs compared with the highest vendorprovided optimization level o3.
the average speedups of boca on gcc and llvm are .25x and .13x respectively demonstrating the effectiveness of boca in compiler autotuning.
for example for the program c5 on gcc boca enables optimization flags and disables the other flags which improves the runtime performance of c5 compiled under o3 by .63x.
also compared with o3 the average speedup of boca on gcc improves from .20x at the 20thiteration to1.25x at the 60thiteration and on llvm it improves from .09x at the 20thiteration to .13x at the 60thiteration.
as boca is designed to improve the efficiency of compiler autotuning we extensively compared boca with the existing compiler autotuning approaches i.e.
rio ga ga and irace to investigate how much time they spent in order to achieve the speedup achieved by boca.
the results are shown in table ii where the values represent the time spent on achieving the speedup achieved by boca at its 30th 40th 50th 60thiterations respectively.
we also calculated the results at the 20thiteration which has the same conclusion as those at other iterations and due to the space limit we put them at the project webpage.
the cells with the shading refer to the best result in the corresponding case and the cells marked as7mean that the compared approach does not achieve the corresponding speedup within iterations.
note that the time for completing iterations for all the compared approaches is significantly longer than that for completing iterations for boca.
we calculated the average time spent on completing one iteration for each approach which is and seconds for boca rio ga ga and irace respectively demonstrating the little cost of boca.
although running these compiler autotuning approaches takes some time it is actually acceptable since the given program can be compiled once under the desired optimization sequence and then used all the time.
from table ii among cases programs compilers speedup settings boca performs the best i.e.
requires the shortest time to achieve the corresponding speedup among all the compared compiler autotuning approaches in out of cases.
moreover these compared approaches are even timeout in at least cases.
the results demonstrate that boca does largely improve the efficiency of compiler autotuning.
we further investigated whether boca significantly outperforms all the compared compiler autotuning approaches by conducting the wilcoxon signed rank test for their time spent on achieving the corresponding speedup at the significance level .
.
for the timeout cases we used the time spent on completing iterations for statistical analysis and following calculation.
we found that all the pvalues are smaller than .
indicating that boca significantly outperforms all the compared approaches for each speedup setting.
we also calculated the average improvements of boca over the compared approaches in terms of the time they spent on achieving the speedup that boca achieved at its 30th 40th 50th 60thiterations.
the results are shown in table iii.
the average improvements of boca over all the compared compiler autotuning approaches in terms of the time spent range from .
to .
across all speedup settings.
that further confirms the effectiveness of boca.
b. rq2 boca v.s.
existing bayesian optimization methods to answer rq2 we compared boca with the direct application of existing bayesian optimization methods i.e.
pal flash and tpe in compiler autotuning.
the comparison results between boca and the advanced method tpe are 1205table ii comparison among compared approaches in terms of time seconds app.
idgcc llvmidgcc llvm s30 s40 s50 s60 s30 s40 s50 s60 s30 s40 s50 s60 s30 s40 s50 s60 boca c13531 p11785 rio ga4 ga10 irace tpe boca c23363 p23081 rio ga4 ga10 irace tpe boca c31942 p34191 rio ga4 ga10 irace tpe boca c42961 p41231 rio ga4 ga10 irace tpe boca c52464 p55985 rio ga4 ga10 irace tpe boca c61994 p6745 rio ga4 ga10 irace tpe boca c72101 p72823 rio ga4 ga10 irace tpe boca c83557 p81550 rio ga4 ga10 irace tpe boca c91156 p91824 rio ga4 ga10 irace tpe boca c101599 p101450 rio ga4 ga10 irace tpe shown in tables ii and iii i.e.
row tpe .
table ii shows that boca outperforms tpe in .
out of cases.
table iii shows that the average improvements of boca over tpe range from .
to .
across all speedup settings.
we also conducted the wilcoxon signed rank test between boca and tpe and the results show that boca statistically significantly outperforms tpe.
since pal and flash need to enumerate and predict all the unevaluated optimization sequences in each iteration andthe number of optimization flags is large in our study both of them cannot produce the results of compiler autotuning within acceptable time.
as an example considering the smallest program p4 with gcc when tuning only optimization flags the time spent on completing the first iteration for both pal and flash i.e.
more than seconds has already been larger than the time spent on completing iterations for boca i.e.
seconds .
therefore we cannot directly compare boca with pal and flash based on the whole 1206table iii average improvements of boca over compared approaches in terms of time spent on achieving speedups boca gcc llvm v.s.
s30 s40 s50 s60 s30 s40 s50 s60 rio .
.
.
.
.
.
.
.
ga4 .
.
.
.
.
.
.
.
ga10 .
.
.
.
.
.
.
.
irace .
.
.
.
.
.
.
.
tpe .
.
.
.
.
.
.
.
s10s20s30s40s50s60255075time 2s pal flashboca fig.
boca vs pal vs flash s10s20s30s40s50s60204060time 2s boca bocanodecay fig.
boca vs boca nodecay set of flags used in our study.
we further tried our best to enable comparisons by using a small set of optimization flags.
more specifically we conducted a study by randomly selecting flags for gcc and llvm respectively and randomly selecting four programs i.e.
c4 c7 p7 and p8 as the representatives.
the results are shown in figure where the xaxis represents the speedups and the y axis represents the time spent on achieving the speedups on average across programs and compilers.
we found that even though using a small set of flags boca still spends less time to achieve each specified speedup than both pal and flash.
in summary our results confirm that boca indeed performs better than the existing bayesian optimization methods demonstrating the necessity of designing a novel bayesian optimization method specific to compiler autotuning.
indeed through rf boca can use a relatively small training set to build an effective model.
c. rq3 selection strategy effectiveness we further investigated whether or not our proposed selection strategy is more effective than the existing advanced selection strategy used in bayesian optimization by comparing boca and boca s. figure shows the comparison results in terms of the time spent on achieving the speedup achieved by boca in the 60thiteration.
we found that boca spends less time than boca son achieving the speedup on average for both gcc and llvm.
the average time spent by boca and boca sis and seconds on gcc and and seconds on llvm respectively.
the average improvements of boca over boca sacross all programs are .
and .
on gcc and llvm respectively.
we also conducted the wilcoxon signed rank test and found boca indeed significantly outperforms boca sin statistics.
the results demonstrate the effectiveness of our proposed selection strategy in boca.
we further analyzed the reason why boca outperforms boca s. the latter conducts local search by flipping only one flag and uses the acquisition function to select the final candidates while the former carefully measures the impact of each optimization flag and then exploits boca bocas510152025time 3s a gcc boca bocas510152025 b llvm fig.
comparison between boca and boca s default 16020406080100time 2s a k value comparison default 20020406080100time 2s b scale comparison fig.
parameter evaluation the identified impactful optimizations sufficiently.
the results show that different optimization flags contribute differently to the overall effectiveness of compiler autotuning for a program.
as presented in section iii d to further improve the efficiency of compiler autotuning we designed an exploration process with decay .
it is also interesting to investigate whether such a decay process is really helpful to improve the efficiency.
therefore we conducted a study to compare boca with its variant without the decay process denoted as boca nodecay on the same programs used in figure .
the results are shown in figure .
boca indeed takes less time to achieve the same speedup than boca nodecay in all the settings demonstrating the contribution of the decay process.
vi.
d iscussion a. influence of main parameters in boca we discuss the influence of main parameters in boca based on the same programs used in figure .
in particular we investigated two main parameters in boca k the number of identified impactful optimizations and scale which controls the speed of decay that are described in section iii d. the results are shown in figure where the x axis represents the parameter values and the y axis represents the average time spent on achieving the speedup boca achieves in the 60thiteration across programs and compilers.
from figure 6a our default kvalue i.e.
performs the best.
when kis set to boca cannot be completed within the given time period since exploiting all the combinations of impactful optimizations is very costly.
the result also demonstrates the importance of setting a proper kvalue.
from figure 6b the small values i.e.
and of scale perform better than the large values i.e.
and indicating that relatively quick decay is helpful to improve the efficiency of compiler autotuning.
our current value of scale i.e.
performs slightly better.
1207s10 s20 s30 s40 s50 s60 s70 s80 s90 s100 s110 s1200100200time 2s boca rio ga4 ga10 irace tpe timeoutfig.
comparison with more iterations b. effectiveness of boca with more iterations we investigated the effectiveness of boca with more iterations by completing iterations for boca and iterations for the compared approaches.
the results on program p7 on llvm are shown in figure where the x axis represents the speedups that boca achieves at the corresponding iterations and the y axis represents the time spent on achieving the speedups.
from this figure we can see that boca keeps outperforming all the compared approaches in terms of the time spent with the number of iterations increasing.
moreover the compared approaches are all timeout at a certain point marked by in figure .
c. threats to validity the threats to validity mainly lie in the compilers and programs used in our experiments.
as presented in section iv a to satisfy the prerequisites of the used benchmarks we used relatively old versions of gcc and llvm following existing work .
to reduce the threats we conducted an experiment to investigate the effectiveness of boca on a recent gcc version i.e.
gcc .
.
on three different programs produced by csmith .
csmith is a state of theart c program generator and its produced programs tend to invoke a large amount of compiler optimization code which facilitates the evaluation of compiler autotuning.
the results show that boca achieves .04x .06x speedups over o3 at the 60thiteration on the three programs while all the compared approaches achieve at most .01x speedup even at their 120thiterations demonstrating the effectiveness of boca on a recent compiler and on different programs to some degree.
this speedup is relatively small since the performance of newer versions has been largely improved which causes short execution time of the three small programs under o3.
therefore the improvement room for compiler autotuning becomes small on them.
for larger programs the improvement room could be larger.
due to space constraint we only briefly describe this experiment here.
more detailed results can be found in our project webpage.
in the future we will use more compilers and programs to evaluate the effectiveness of boca to further reduce the threats.
furthermore in our study it is infeasible to know the ground truth global optimum in an enormous search space.
to further investigate whether boca is able to find the global optimum more efficiently we found a small llvm case which only has flags and was completely searched .
we applied boca and the most effective compared approachtpe to it.
boca found the global optimal sequence within iterations while tpe cannot find it within iterations demonstrating the effectiveness of boca in finding the global optimal sequence to some degree.
vii.
c onclusion in this paper we propose boca the first bayesian optimization based approach for efficient compiler optimization.
boca includes a novel searching strategy for bayesian optimization which incorporates the impact of optimization flags measured by a tree based model and a decay function.
we perform extensive experiments on two widely used c benchmarks using gcc and llvm.
the results demonstrate that boca significantly outperforms all the compared approaches in terms of the time spent on achieving specified speedups.
further for compiler writers our work will be useful for developing selftuning compilers and improving compiler code associated with the flags that hamper performance.
acknowledgment this work has been supported by the national natural science foundation of china and .