hisyn human learning inspired natural language programming zifan nan north carolina state university raleigh north carolina usa znan ncsu.eduhui guan university of massachusetts amherst amherst massachusetts usa huiguan cs.umass.eduxipeng shen north carolina state university raleigh north carolina usa xshen5 ncsu.edu abstract natural language nl programming automatically synthesizes code based on inputs expressed in natural language.
it has recently received lots of growing interest.
recent solutions however all require many labeled training examples for their data driven nature.
this paper proposes an nlu driven approach a new approach inspired by how humans learn programming.
it centers around natural language understanding and draws on a novel graph based mapping algorithm foregoing the need of large numbers of labeled examples.
the resulting nl programming framework hisyn using no training examples gives synthesis accuracy comparable to those by data driven methods trained on hundreds of training numbers.
hisyn meanwhile demonstrates advantages in interpretability error diagnosis support and cross domain extensibility.
ccs concepts software and its engineering domain specific languages source code generation .
keywords program synthesis natural language programming acm reference format zifan nan hui guan and xipeng shen.
.
hisyn human learninginspired natural language programming.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
.
introduction recent years have witnessed a growing interest in natural language nl programming where code synthesizer automatically produces programming code based on nl input from users.
it is especially appealing in cases where such an intuitive interface offers conveniences to general users e.g.
iot smarthome and cases where there are many domain specific apis difficult for a programmer to memorize e.g.
python libraries .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
approaches to nl programming fall into two classes data driven and rule driven approaches.
the former features the reliance on many labeled input code pairs as training data to build up some statistical models the latter depends on some predefined domain specific rules.
the rule based approach had shown some success in the early stage of the field development e.g.
smartsynth but have gradually lost attractions due to the lack of robustness and the difficulties in generalizing across domains.
the data driven approach has dominated recent efforts represented by the adoption of deep learning to map nl queries to code via various neural networks e.g.
.
although this approach has shown more promise than the previous rule driven approach its requirement of large numbers of labeled examples hinders its adoptions especially for domains where labeled examples are scarce.
although recent proposals show the possibility of generating examples for a certain domain it is yet unclear how well these methods can generate truly representative examples in complex domains.
in this work we propose nlu driven approach an approach driven by natural language understanding nlu .
it is inspired by how humans code.
rather than going through thousands of examples as a data driven approach does a programmer can start coding after she reads through the documentation of the language or api of interest.
she may check a few examples but the number is far less than what a data driven approach usually needs.
the key is in understanding the language or api the central feature nlu driven nl programming builds on.
more specifically the nlu driven approach features deep processing of programmers intentions and api documents written in natural languages via nlu and the leverage of the deep understanding rather than training examples for code synthesis.
compared to data driven approaches the nlu driven approach has three appealing properties.
first by avoiding the need for a large number of labeled examples it saves users the often heavy burden in collecting generating examples and makes nl programming possible for domains where labeled examples are scarce.
second built on understanding rather than data it avoids the bias that training data examples bring to data driven methods.
finally as a white box approach its better interpretability makes the diagnosis of synthesis errors easier to do.
our exploration leads to hisyn for human learning inspired synthesizer the first nlu driven nl programming framework.
hisyn is equipped with several distinctive features.
deep nl understanding for code synthesis.
a deep natural language understanding is the key to human learning inspired code synthesis.
although nlp has been used in software maintenance its usage in nl programming is still esec fse november virtual event usa zifan nan hui guan xipeng shen preliminary.
unlike prior synthesis studies that use shallow nlp just for assistance hisyn takes modern nlp as the first order tool.
with it hisyn automatically builds up the intermediate representation of input queries and the knowledge base of the programming apis preparing the foundation for synthesis to work on.
it uses nl dependence analysis to capture the deep relations among the different parts of the input query and wordnet a lexical database of english with synsets to capture the semantic associations of english words.
framework architecture design for cross domain extensibility.
domain differences are inherent to natural language programming.
different domains have different terminologies api definitions grammar query patterns and so on.
previous non data driven studies are domain specific rules and elements specific to the target domain are tightly integrated with the synthesizers making them difficult to port to other domains.
hisyn strives to ensure crossdomain extensibility in design.
drawing on inspirations on how modern compilers deal with domain language varieties it creates architecture with the front end producing a unified intermediate representation nl dependence graph for an arbitrary domain on which the back end operates to generate the code in the target api.
neither the front end nor the back end requires changes across domains.
the hisyn design encapsulates domain specific elements into separate modules equipped with an easy to use interface.
for a new domain the developer only needs to use the interfaces to extend those domain specific modules no changes are needed for the hisyn framework.
graph based algorithm for mapping.
based on the intermediate representation hisyn employs grammar graph based translation to generate code to materialize a user s intention in the target programming apis.
the novel algorithm first annotates each node in the ir with candidate apis and then uses path finding on the reverse api grammar graph to identify the appropriate apis their order and assemble them into the final code.
the algorithm bridges the gap between the user s intention and the apis by leveraging both the semantic connections at the natural language level and the syntactical constraints at the api grammar level.
we evaluate hisyn on three domains the domain of text editing the domain of air travel queries and the domain of program source code analysis.
although combing with a few examples could potentially help to examine the potential of pure nlu based approach hisyn is designed to use no examples.
our experimental results show that without any training examples hisyn can produce code as accurately as those by a representative data driven nl programming framework trained on hundreds of examples.
the study validates the cross domain portability of the core of hisyn and demonstrates the large potential of the nlu driven approach for nl programming while saving the burden of collecting large numbers of training examples.
background hisyn employs standard nl processing techniques such as tokenization which splits a piece of text into tokens i.e.
words pos tagging which labels tokens words with their part of speech pos tags lemmatization which reduces a word to its basic form called lemma and named entity recognition ner recognizes namedentities or pre defined categories such as person names organizations quantities and locations.
hisyn in addition heavily leverages dependency parsing which goes a deeper level than those listed nlp techniques analyzing the dependency relations between tokens in a sentence and outputs a dependency graph .
a dependency relation is composed of a subordinate word called dependent a word on which it depends called governor and an asymmetrical grammatical relation between the two words called dependency type .
figure shows the dependency parsing result for an example sentence generated by the stanford corenlp dependency parser1 along with the code to synthesize .
a dependency relation is marked as an arrow pointing from a governor to a dependent and is labeled with the dependency type.
the arrow from flight to cheapest with the label amod indicates that cheapest is an adjective modifier of flight .
all the dependency relations form a directed graph which is called dependency graph.
overall framework the framework design of hisyn is shown in figure .
it has three main components a domain knowledge constructor that processes the domain knowledge to assist code synthesis a front end that transforms an nl based query to a dependency graph which serves as the basis for the intermediate representation that the back end works on a back end that employees grammar graph based translation to generate code based on the ir.
the domain knowledge constructor takes two files as inputs a document that contains all the api and their descriptions a grammar file that contains the context free grammar written in backus naur form bnf .
the constructor parses the input files and generates two outputs an api knowledge base for semantic mapping between words in nl based queries and apis a grammar graph that defines the search space for code generation.
a formal definition of grammar graph will be introduced in section .
.
.
the front end takes an nl based query and applies light regulation first to avoid term confusions for words that have domainspecific meaning.
it then uses multiple nlp techniques including pos tagging lemmatization ner and dependency parsing to produce a dependency graph as an intermediate representation ir .
hisyn prunes non essential words called function words from the ir based on the pos tag and the dependency relations of each word.
figure a shows the pruned dependency graph of figure .
this pruned ir is fed to the back end for code generation.
the back end then employs a novel synthesizing algorithm called grammar graph based translation to generate code according to the pruned ir.
grammar graph based translation first maps each node in the ir to a set of apis based on the lemma and synonyms of words in each api s description.
the apis corresponding to each node are called candidate apis .
one node can be mapped to several candidate apis or no candidate api at all.
in the case where multiple nodes have a mapping to the same candidate api hisyn uses a longest match scheme to group the nodes as a cluster and selects the candidate apis for that cluster instead of each node in that cluster.
grammar graph based translation then translates the ir annotated 76hisyn human learning inspired natural language programming esec fse november virtual event usa figure dependency parsing result from stanfordcorenlp .
the corresponding code is shown in table domain knowledge constructorapi documentationdsl grammar api knowledge grammar graph knowledge basefront endtokenizationpos taggingnerlemmatizationdependency dependency graph ir amended nlp back endpruning semantic mappingreversed all paths searchlongest match local reorderingcandidate paths selection and combinationcode generationgrammar graphbased translation dsl codesenglish queries common knowledge basedomain knowledge baseterm replacement figure framework design.
with candidate apis to dsl codes based on the grammar graph generated by the domain knowledge constructor.
hisyn features a modular design.
by separating domain specific modules from the core it simplifies applications to a new domain.
we next explain each of the main components of hisyn.
nlp and ir this section first introduces the domain knowledge constructor and the front end in the framework and then gives formal definitions of the dependency graph and grammar graph which are two core data structures in hisyn.
.
domain knowledge constructor domain knowledge constructor builds the common knowledge base and the domain knowledge base.
the common knowledge base stores the information that could be used in any domain.
it contains three components wordnet synonym list named entities and preposition dictionary.
the wordnet synonym list is used to help the semantic mapping step in the back end.
the named entities ne are the labels for real world objects such as january s ne is month baltimore s ne is city.
the preposition dictionary is a dictionary we create for mapping a preposition to its semantic related words.
for example the semantic related words of preposition from include start source origin .
the domain knowledge base stores domain specific knowledge.
every domain will have its own knowledge base.
it contains a grammar graph defined in section .
.
and an api knowledge base.
the api knowledge base is generated by parsing the api documentation and contains each api name input output types and its natural language description.
both api documentation and grammar are stored in separate text files.
api documentation is stored in plain text with labeled content e.g.
name forstmt .
the grammar should be in backus naur form .
before synthesizingthe queries hisyn will parse the text files and construct the domain knowledge base.
.
query processing via amended nlp for a given nl query the nlp engine goes through the following steps tokenizing lemmatization pos tagging named entity recognition ner and dependency parsing.
hisyn then represents the result as a dependency graph which is used as the basis for the intermediate representation ir .
one special complexity for code synthesis is terminology confusion.
in a specific domain some terms have a special meaning that confuses the standard nlp parser.
for instance for statement in astmatcher domain is a term indicating a type of statement i.e.
for loops in programming languages.
however the nlp parser will take for as a preposition and provide a parsing result that far away from its meaning in the original query.
we address this issue by letting users add light regulations to the input queries.
specifically hisyn requires all of the domain specific terms to be put inside a pair of punctuation marks double angle brackets as default .
for example find for statement should be written as find for statement .
with the special words explicitly marked hisyn can easily replace them with some common terms mostly noun terms and replace them back after nlp steps.
users can specify their signs if angle brackets are terms of the target domain.
.
pruning in natural language some words in a sentence are function words that express grammatical relationships among other words with little lexical meaning.
these words usually have no mapping to any apis.
thus inside the dependency graph the edges related to these words are trivial edges .
in this stage we prune these trivial edges with function words based on the dependency relation of the edges and the pos tags of the tokens.
77esec fse november virtual event usa zifan nan hui guan xipeng shen the default prunable dependency relation include the following labels det the determiners a an the some etc.
case used for prepositions in stanfordcorenlp enhanced dependency parser prepositions is labeled inside nmod thus prunable.
mark the word introducing a clause subordinate to another clause ref the relative word introducing the relative clause aux a function word associated with a verbal predicate that expresses categories such as tense mood aspect voice or evidentiality cop a copula is the relation between the complement of a copular verb and the copular verb conj and a conjunct is the relation between two elements connected by a coordinating conjunction and or etc.
punct punctuation acl relcl a relative clause modifier of an noun.
the parser also puts other specific relations between the noun and its relative clause thus this edge is prunable.
the default prunable pos tags include the following labels prp the personal pronoun md modal prp possessive pronoun.
wp possessive wh pronoun.
hisyn will traverse the entire graph prune the trivial edges and nodes with dependency relations and pos tags listed above.
figure and figure a show the dependency graph before and after the pruning.
it is worth noting that as a cross domain framework users could configure hisyn to add or delete the dependencies or pos tags to refine the pruning process in new domains.
.
intermediate data structures this section gives the formal definition of two core data structures used in grammar graph based translation dependency graph and grammar graph.
.
.
dependency graph.
dependency graph is the output from the front end of hisyn.
it contains the word form lemma form pos tag and named entity label of each token in a nl based query and the dependency relations among the tokens.
it is used as the basis of the intermediate representation in guiding code generation in the back end.
we give the formal definition of a dependency graph as follows definition dependency graph .a dependency graph is a directed acyclic graph gnl nnl enl where each noden word lemma pos ne nnlcorresponds to a token which includes the word lemma pos tag and named entity tag neis empty if the word is not an named entity.
each edgee ni nj dep enlis a direct edge corresponding to a dependency relation.
the edge points from ni called governor tonj called dependent with the dependency relation dep.
.
.
grammar graph.
a context free grammar cfg is a quadruple t nt s p i.e.
terminal symbols nonterminal symbols start symbol productions .
to enable efficient valid code search we introduce a representation grammar graph to represent a cfg.
a grammar graph defines the search space for code generation.
given a grammar graph the code generation problem is transformed to the problem of finding a subgraph called code generation treefrom the grammar graph.
the definition of a code generation tree will be introduced in section .
.in hisyn the nodes in a grammar graph are of three types nonterminal nodes derivation nodes and api nodes .
edges are of two types or edges and concatenation edges .
we first introduce the definitions of the nodes and edges and then the definition of a grammar graph.
in the definitions below denotes the number of elements in a set.
figure e is a partial grammar graph in the air travel information system atis domain.
we use the following production rules to illustrate a grammar graph query pred set min f ... project min fare min fare col fare pred set pred eq dprt eq arr ... definition non terminal node .a non terminal node nn nnrepresents a unique non terminal symbol in nt.
we usen to represent the set of non terminal nodes derived from a cfg.
the number of non terminal nodes is equal to the number of nonterminal symbols i.e.
nn nt .
for example in figure e the node min fare is a non terminal node.
definition derivation node .a derivation node nd nd corresponds to the string in the right hand side of a production rule in a cfg.ndis used to represent the set of derivation nodes derived from a cfg.
the number of derivation nodes is no more than the number of production rules i.e.
nd p .
this is because different production rules could have the same right hand side string.
an example of derivation nodes in figure e is node min fare c.. p.. c.. and p.. are the abbreviations of the arguments col fare pred set .
definition api node .an api node na nacorresponds to an api function name.
narepresents the set of api nodes derived from a cfg.
the number of api nodes is equal to the number of apis.
in figure e the node min fare is an api node.
definition or edge .an or edge eo eois a directed edge that points from a non terminal node nnto a derivation node nd denoted as eo nn nd .eorepresents the set of or edge derived from a cfg.
an or edge corresponds to a production rule its tail represents a non terminal symbol while its head represents a string of one or more terminal or non terminal symbols.
the number of or edges is equal to the number of production rules i.e.
eo p .
because some production rules can share the same non terminal symbol as their left hand side a non terminal node corresponding to the non terminal symbol can also be tails of more than one or edge.
applying a production rule is equivalent to select one of the or edges.
the edges from the non terminal node query to other nodes in figure e are or edges.
definition concatenation edge .a concatenation edge ec ecis a directed edge that points from a derivation node ndto an api nodenaor a non terminal node nn or from an api node nato a non terminal node nn denoted as ec nd na orec nd nn orec na nn .ecrepresents the set of concatenation edges derived from a cfg.
78hisyn human learning inspired natural language programming esec fse november virtual event usa when a concatenation edge is nd na or nd nn it means that the api represented by naor the non terminal symbol represented bynnis a sub string of the symbol sequence represented by nd.
when a concatenation edge is na nn it means that the nonterminal symbol is one of the arguments of the function represented byna.
in figure e e min fare c.. p.. min fare is a concatenation edge nd na e eq dprt eq arr eq dprt is a concatenation edge nd nn e main fare col fare is a concatenation edge na nn .
a grammar graph contains all three types of nodes and two types of edges.
definition grammar graph .a grammar graph is a directed graph gg ng eg whereng nn nd naandeg eo ec.
all the concatenation edges ecdirected from a ndor annaare ordered and the order corresponds to the order of symbols inside a derivation or the order of arguments of an api.
in figure e the number on each ecindicates the order of the symbols or arguments.
for a non terminal node nnthat has multiple outgoing or edges the first or edge will be marked as the default edge and selected to complete a path during synthesis.
grammar graph based translation grammar graph based translation is the synthesizing algorithm used in the back end of hisyn to generate code in the target programming apis.
the algorithm centers around the dependency graph for the selection and the ordering of apis and relies on the grammar graph for code lowering and correctness.
it takes the ir of an nl based query dependency graph as the input and annotates each node in the ir with candidate apis.
the annotated dependency graph will go through reversed all paths search local reordering candidate paths selection and combination and code generation steps to finalize the dsl code.
we next explain each major step in detail.
.
semantic mapping semantic mapping associates a list of candidate apis to every node and the edges whose dependency relation dep is a preposition relation nmod preposition in a dependency graph.
these lists of candidate apis offer the basis for generating the target dsl code.
node mapping.
a noden word lemma pos ne in a dependency graph falls into three categories based its netag and domain knowledge each category is treated differently to create its candidate api list.
the mapping rules for each category are described below domain term nodes .
adomain specific term is a term that is explicitly marked by a user in the input query.
it is defined in section .
.
a node is a domain node if its word is a domain specific term.
it has a one to one mapping with an api and thus its candidate api list contains only one api.
for example in astmatcher domain the term for statement corresponds to the api forstmt and if statement corresponds to the api ifstmt .
named entity nodes .
a node is a named entity node if its ne tag is not empty.
because a named entity is usually used as an argument of an api hisyn generates a list of candidate api for a named entity node by first identifying the apis related to the node s netag and then using the node s word form as an argument of each api.for example the node baltimore is mapped to in figure a b .
regular nodes .
a node is a regular node if it doesn t fall into the above two categories.
for a regular node a mapping exists if the node s word lemma or one of its synonyms is a token in an api s description.
a regular node can have many or zero mapped candidate apis.
edge mapping.
an edge ni nj dep is apreposition edge if its depis a preposition relation such as nmod from andnmod to.
a mapping from an api to a preposition edge exists if any of the dep s semantic related words is a token in the api s description.
a dep s semantic related words are pre defined in a preposition dictionary.
hisyn only considers preposition edges because prepositions are used to express the temporal or spatial relations between two nouns.
these relations contain important semantic information between the nodes connected by preposition edges.
thus hisyn applies edge mapping to extract such semantics.
we refer to the dependency graph annotated with the lists of candidate apis as an annotated dependency graph .
.
longest match scheme the longest match scheme is designed to handle cases where a phrase more than one word is used to refer to one api on the grammar graph.
for example the numeric letter in text editing refers to the api numbertoken .
but if we map numeric and letter separately there will be two mapped apis numbertoken and chartoken respectively.
inspired by the longest match principle used in many scanners to tokenize strings in compilers we use the longest match scheme to determine the grouping of some nodes in the dependency graph.
a single set of candidate apis will be identified for the group instead of each node within the group.
if two nodes in the dependency graph are connected by a dependency edge with modifier relations amod nmod the edge is called modifier edge and these nodes are within a modifier group .
other nodes connected to a node in a modifier group with modifier edges also belong to the same modifier group.
the node which does not have the governor is the top governor of this group.
for example consider three modifier edges a mod b a mod c c mod d. then these four nodes are in one modifier group and a is the top governor.
at this step each node has its api candidates.
for each api inside the a s candidates hisyn checks how many times this api is also a candidate of other nodes the result is taken as the score of that api.
the apis with the highest score ties can happen are the longest match candidates for the phrase.
then nodes with candidate apis stay inside the modifier group while the other nodes will be treated as regular nodes.
the modifier group will then be treated as one node in dependency graph.
the edges that link to the nodes inside the group will link to the top governor with the directions and dependency relations stay unchanged.
in figure a b flight and cheapest are in one modifier group and the api min fare is the longest match api for this modifier group.
79esec fse november virtual event usa zifan nan hui guan xipeng shen d search resultsnmod from 1longest match step min f are pred set pred set ... pred eq departs eq arrives eq departs eq arrives eq drpt eq arrquery min fare min fare min f are eq dpr t city city cityeq arr city city cityquery min fare pred set ... project min fare pred set ... project min f are pred set ... col fare pred col fare... col f areeq dprt eq arr ... eq dprt eq arr eq dpr t eq arr city time weekday daynum month city ... time ... weekda y ... ... city time weekda ydaynum month ... min f are pred set predset ... eq dep arts min f are pred set pred set ... eq arr predset max max time min time min f are flight cheapest from baltimore to atlanta city baltimore city atlanta query min f are eq arr eq dpr t2 a dependency graph b semantic mapping resultsfind cheapestflightdobj amod baltimore atlantanmod to c reverse all paths search direction e grammar graph f path selection and combinationstep steps step nonterminal node derivation node api nodedependency node dependency edge searching directions or edge concatenation edge edge in the path complimentary edge path connectionsteps 1paths label 1concatenation edge orderstep1 semantic mapping step2 longest matching step3 reversed all paths search step4 local reordering step5 candidates combination and selection min f are c.. p.. pred set ... ... min f are figure running examples for grammar graph based translation .
reversed all paths search reversed all paths search identifies a set of candidate paths in the grammar graph for each dependency edge in the annotated dependency graph.
these sets of candidate paths will be pruned reordered and combined in later steps to determine the orders of candidate apis.
we next introduce several important concepts used in the search algorithm and then describe the algorithm.
areversed grammar graph is a directed graph on the same set of vertices with all of the edges reversed compared to the orientation of the corresponding edges in the grammar graph.
a candidate path is a directed path in the reversed grammar graph or grammar graph.
the root of a candidate path is the node with no incoming edges theleave of a candidate path is the node with no outgoing edges.
both the root and the leave of a candidate path are api nodes.
for each edge governor dependent dep in the annotated dependency graph the algorithm conducts breadth first search bfs on the reversed grammar graph to identify a set of candidate paths.
the search starts from each candidate api of the node dependent and stops when the start symbol of the grammar is reached.
during the search a visited node will not be added to the path again to avoid an infinite loop if recursion exists in the grammar.
if a path reaches one of the candidate apis of the node governor it will be recorded.
the paths that end at the start symbol will also be recorded.
after getting all the candidate paths for each edge we apply two pruning strategies to reduce the number of candidate paths if there exists at least one path that contains any one 80hisyn human learning inspired natural language programming esec fse november virtual event usa declares singlevariabledobj amod initializedacldeclares variablesingleamod dobj initializedacl figure local reordering of the candidate apis for the node governor the paths ending at the grammar s start symbol will be discarded.
if a candidate api of thegovernor is not the destination of any paths from any dependent s candidate api it indicates that this candidate api is not related to the dependent .
this candidate api will be deleted from the candidate api list of the governor .
all the paths starting from the deleted candidate api will also be removed.
the remaining candidate paths are used to generate final dsl codes in later steps.
the algorithm is motivated by our observation that when describing the queries in english people typically follow a gradually refining way.
they state the main interest or purpose first then provide detailed information with dependents.
for example consider the query in figure i would like to find the cheapest flight from baltimore to atlanta.
this query contains the following information it wants to query about a flight this flight has the cheapest fare this flight s departure city is baltimore this flight s arriving city is atlanta.
the edges on the dependency graph show such relations figure a .
inside the graph the words cheapest baltimore and atlanta are all dependents of the flight with nmod nominal modifier relation.
it is consistent with the structure patterns of atis query language which start with the main object or purpose first then use arguments to specify the details.
therefore the direction of edges in the dependency graph pointed out the basic structure of the code the apis mapped from the dependent are inside the arguments of the api mapped from the governor.
correspondingly on the grammar graph the apis mapped from the dependent should be the descendants of the api mapped from the governor.
in figure c we indicate the search direction on the grammar graph.
the numbers label the paths for each start goal pair and figure d shows the search results correspondingly.
.
local edge reordering after the reversed all paths search hisyn applies local edge reordering to fix the structure of the dependency graph.
although the dependency relations in a dependency graph can guide the ordering of the apis when generating dsl code the following two dependency relations are exceptions we observed from our observation of a small set of examples dependency relations related to subject .
the relative order of a subject and a verb in the code snippets can be the opposite of that specified by their dependency relation.
when parsing an english query the nlp engine makes the verb as the governor of the subject.
this structure is sometimes consistent with the code grammar.
for example the code for the description a adds b is add a b .
however exceptions exist.
for example the code for a returns b should be a returns b rather than returns a b .
dependency relations related to modifiers .
a modifier is always the dependent of the noun it modifies.
but an api can modify another api either by using the second api as an argument or being the argument of the second api.
for example in astmatcher domain a declares a single variable corresponds to the code snippet a declstmt hassingledecl vardecl .
because single modifies the variable hassingledecl appears before vardecl .
however the code for a global variable is vardecl hasglobalstorage .
although global modifies variable the api hasglobalstorage is after vardecl .
given an edge e a b dep in a dependency graph whose dep is one of the above two types of dependency relations hisyn conducts reordering if any of the two situations occur there is no path from b s candidate apis to a s candidate apis.
there exists at least one path from a s candidate apis to b s i.e.
reversed direction that is shorter than the shortest path from b s candidate apis to a s. the rationale is that a shorter path on the grammar graph is preferred because the corresponding dependent and governor in the dependency graph are neighbors.
if reordering is needed hisyn changes the positions of the governor and the dependent as shown in figure .
then reversed all paths search is re applied to the reordered edges to find all the candidate paths.
.
candidate paths selection and combination after reversed all paths search and local edge reordering this step combines the sets of candidate paths into code generation trees for generating dsl code.
we first give a formal definition of code generation tree and then describe the algorithm to generate such trees.
definition code generation tree .a code generation tree cgt gt nt et is a subgraph of grammar graph gg ng eg wherent ngandet eg.
a cgt is a directed acyclic graph whose undirected counterpart is a tree.
the root of a cgt is a non terminal node that corresponds to the start symbol.
we define the size of a cgt as the number of api nodes in the tree.
the algorithm first reverses every candidate path so that the direction of each edge in the path is the same as the one in the grammar graph.
it then uses only one of the candidate paths from each edge in a dependency graph and combines the selected candidate paths based on the structure of the dependency graph into a cgt.
because an edge in the dependency graph can have more than one candidate path this step results in many cgts.
there are two cases for candidate paths combination siblings paths combination and parent child paths combination.
siblings paths combination .
two dependency edges are siblings edges if they have the same governor.
two candidate paths are sibling paths if the corresponding dependency edges are siblings.
because a dependency edge can have many candidate paths hisyn generates all the combinations of the path candidates from each sibling edge.
for each combination hisyn combines sibling paths by joining the same nodes in the paths to build a prefix tree.
if two sibling paths do not have any nodes in common they are ignored.
a grammar check will be applied to remove prefix trees that are not correct.
for each node in a prefix tree if it is a non terminal 81esec fse november virtual event usa zifan nan hui guan xipeng shen node hisyn checks if it has only one derivation node as a child if it is an api node hisyn checks if the child nodes are the subset of the api arguments.
the grammar check at this step only checks the existence of the child nodes the order of child nodes is considered in the code generation step.
for edges with no siblings each of the candidate paths will be treated as a prefix tree.
the up right figure labeled path in figure f is one of the prefix trees for paths from min fare to eq dprt and from min fare toeq arr .
parent child paths combination .
a dependency edge e1is the parent of another edge e2ife1 s dependent head is e2 s governor tail.
these two dependency edges are called parent child edges .
two candidate paths are parent child paths if their dependency edges are parent child edges.
hisyn combines a parent path with the child path by adding an edge directed from the leave of the parent path to the root of the child path.
in figure f path and path are connected to prefix tree and prefix tree is connected to path .
after all the siblings paths combination and parent child paths combination the final connected prefix trees become the code generation trees.
the code generation trees then will be transformed to the final code expression in the next step.
.
code generation at this stage hisyn has translated the ir into a set of code generation trees.
this step first selects the minimum cgt and then uses the minimum cgt to generate the correct code expression.
a minimum cgt is the one that has the smallest size i.e.
the smallest number of api nodes .
the rationale of using a minimum cgt for code generation is as follows.
the more apis in a code expression the more information it conveys.
because all the key information contained in a query is already mapped to candidate apis during our synthesis process the smallest cgt is preferred to avoid including redundant or unnecessary apis in the generated code.
a cgt might miss some important nodes for generating the grammar correct code.
we fill those missing nodes by applying two rules if a derivation node is in the cgt then all the non terminal nodes who are children of the derivation node should also be in the cgt.
this is because a derivation node is the right hand side of a production rule and its children are the non terminal symbols or terminal symbols in the right hand side.
if an api node is in the cgt then the descendants of the api nodes should also be in the cgt.
this is because an api s descendants will generate the api s arguments.
we refer to the filled cgt as a completed cgt .
if there are multiple minimum cgts hisyn will complete all of these minimum cgts and choose the minimum completed cgt.
if ties still exist this situation happens when one keyword in a query can be mapped to two or more apis that have similar syntax roles in the grammar that is apis that can be derived from the same non terminal and have the same terminals as input.
then hisyn will select the api with the shortest description all the api descriptions are processed by the nlp engine and the function words are removed when counting the length of the description .
this heuristic assumes that a more complex api needs more wordsto describe it.
if this api is needed the query should be more specific.
thus the limit keywords only lead to an api with less description.
the completed cgt will be transformed into a code expression.
hisyn simplifies the cgt by removing all the non terminal nodes and derivation nodes so that only api nodes are kept in the cgt.
the simplified cgt can then be structured to an ordered apiargument sequence i.e.
dsl code based on the directed edges.
evaluation we conduct a set of experiments to examine the efficacy of the hisyn framework cross domains.
to test the potential of a pure nlu driven approach we use no labeled examples for hisyn.
we use the experiments to answer three questions compared to data driven methods trained on many labeled examples can hisyn produce comparable results without any training example?
how does the length of a query affect the accuracy of hisyn ?
what are the reasons that cause errors?
we describe the experiment settings in section .
report our experiment results and comparisons in sections .
and provide a detailed error analysis in several representative cases in section .
.
.
methodology .
.
dataset.
we use three datasets with different dsls to evaluate hisyn.
the first dataset2is the dsl designed for repetitive text editing tasks.
the second dataset3is the dsl designed for the air travel information system atis .
both of these two datasets come from the work .
the third dataset is the dsl in llvm clang designed for ast node matching.
text editing language is a command language that aims to free office suite application end users from understanding syntax and semantics of regular expressions conditionals and loops.
this dsl has apis in total.
the dataset for text editing includes english queries and dsl pairs.
air travel information system atis is a standard benchmark for querying air travel information.
this atis dsl is designed based around sql style operations and provides support for predicates expressions that correspond to important concepts in air travel queries arrival departure locations times dates prices etc.
it has apis in total.
the dataset for atis includes english queries and dsl pairs.
astmatcher is a tool in clang llvm for constructing ast matching expressions to find code patterns of interest.
it represents a domain with high complexity but scarce labeled examples.
there are a total of astmatcher apis with a full fledged data type hierarchy over types .
we collect astmatcher expressions from clang tidy and let graduate students write the english descriptions independently.
each astmatcher expression is described using a single english sentence from at least two students.
the text editing datasets and atis datasets do not provide the api documentation and grammar.
we manually created these two domain documentations based on the english description and codes.
the astmatcher dataset provides the official documentation.
we generate grammar based on this documentation.
2shorturl.at npfis 3shorturl.at sxys5 82hisyn human learning inspired natural language programming esec fse november virtual event usa table nl queries and codes examples dsl query code astmatcherfind for statements whose init portion declares a single variable which is initialized to the integer literal .forstmt hasloopinit declstmt hassingledecl vardecl hasinitializer integerliteral equals atisi would like to find the cheapest flight from baltimore to atlantaextract row min f col fare atomicrowpredset atomicrowpred eq departs city baltimore any any any any eq arrives city atlanta any any any any text editing insert after 1st word.insert string position after wordtoken integerset integer iterationscope linescope bconditionoccurrence always all .
.
evaluation metrics.
we use all the test cases in each domain in the experiments.
we use dsl codes synthesis accuracy to evaluate the performance of hisyn.
the synthesis accuracy is the ratio between the number of correctly synthesized dsl code expressions and the number of total test cases.
a synthesized dsl code is correctif it is identical to the ground truth code including the apis variables values and their relative order.
.
.
methods for comparison.
we compare hisyn with a datadriven synthesizer in text editing domain and atis domain and a rule driven synthesizer in astmatcher domain.
we choose it for comparison because i as hisyn it is a generic synthesizer that takes nl queries as inputs and produces code expressions for multiple domains.
ii on the available training data in text editing and atis domains it gives accuracy comparable to the existing domain specific nl based synthesizers.
in this prior work the data driven synthesizer builds a generic nl based synthesizer using machine learning.
it takes as input dsl definition and training data consisting of nl dsl pairs and builds a synthesizer by learning the weights and classifiers to rank the output of keywords programming based translation.
the prior work applies to the text editing domain and atis domain but not the astmatcher domain.
it is because this domain has over astmatcher apis.
a large number of apis brings many complexities to this domain.
with various usage conditions a data driven method requires a large number of training examples to train a statistical model that possibly covers most of the use cases of apis.
the real world labeled cases in this domain are yet scarce.
rule based approach works better in this scenario.
we hence build a rule based synthesizer as a comparison in this domain.
it is based on previous work which uses simple nl processing and heavily relies on type checks and domain heuristics.
.
accuracy of hisyn figure a reports the overall accuracy of hisyn.
the accuracies of the data driven method on the first two domains come directly from the previous paper obtained after creating collecting hundreds of labeled examples and using them to train.
hisyn without using any training example achieves comparable accuracies with them .
for text editing and .
for atis.
text editing atis astmathcer domains020406080100accuracy .
.
.
.
.
.
a comparison with other methodshisyn comparison text editing atis astmathcer domains020406080100 .
.
.
.
.
.
b comparison on query difficultieseasy hardfigure accuracy of hisyn in three different domains in the astmatcher domain the rule driven synthesizer achieves accuracy lower than hisyn.
the result shows the benefits of nlu and its mapping algorithm over rule driven methods.
figure b shows the accuracy comparison on queries with different lengths.
the average query lengths of text editing domain atis domain astmatcher domain are and .
hisyn achieves .
.
.
accuracy on shorter queries in three domains and .
.
.
on longer often harder queries.
the much higher accuracy on the easy queries is due to two major reasons given a hard query an nlp engine in the front end is more likely to generate a wrong dependency graph.
in the back end a hard query increases the number of paths to combine and select making it harder to generate the right cgts.
.
error analysis unlike the black box method in neural network based datadriven approaches the interpretable nature of hisyn makes error diagnosis easy to do.
in this section we analyze three major cases where hisyn fails to synthesize the correct code expressions to provide some insights.
both the front end and the back end could cause errors.
the errors in the front end are typically caused by the ambiguity in natural language or incorrect semantic mapping.
the errors in the back end result from wrong decision paths selection and combination steps.
83esec fse november virtual event usa zifan nan hui guan xipeng shen .
.
limitations in nlp engines.
nlp engines may cause errors.
consider the following two test queries query would you tellme the cheapest one way fare from boston to oakland ?
query please give me information concerning a flight from washington dc to philadelphia the earliest one in the morning.
query is an example of lexical ambiguity.
the dependency parser treated oakland as the noun modifier of tell i.e.
tell ... to oakland resulting in the wrong dependency edges in the ir.
query is an example of semantic ambiguity.
one is considered as a number and taken as a part of the phrase one in the morning .
but one is a pronoun in this query.
this semantic ambiguity leads to the mapping of api time 1am which is not the intention of the original query.
when we manually fix the parsing errors hisyn gives correct synthesis results.
as nlp techniques progress nlu driven synthesis is expected to provide even better results.
.
.
incorrect semantic mapping.
this type of errors is caused by passive voice or a word that combines the semantic meaning of several apis.
for example query print any word that is followed by team .
query prepend each line with .
in query is followed here means the position before the word team and it should be mapped to api beforecond .
the document of the api has before relation described and the synthesizer fails in recognizing that follow essentially means that relation but in a different way and hence results in a wrong mapping.
in query4 the word prepend means insert at the beginning which can be mapped to two apis insert andstart start indicates the location of insertion is at the beginning .
however the semantic mapping step only identifies one api insert for each node.
the missing of beginning leads to the error.
applying a deeper semantic analysis for semantic mapping may help this situation.
.
.
wrong decision in code generation step.
in the code generation step we use the minimum cgt to generate the final code expression.
it works in most of the cases because the minimum cgt covers all the information inside a query with the least number of apis.
but exceptions exist.
query in every line delete the text after the correct code expression should be remove selectstring texttoken bconditionoccurrence aftercond string imm all iterationscope linescope bconditionoccurrence always all the synthesized code expression is remove selectstring texttoken bconditionoccurrence always all iterationscope linescope bconditionoccurrence aftercond string imm all the error is the location of the condition after .
after is the condition for deleting the text.
but in synthesized code it is inside the condition for iteration.this error occurs when the following candidate paths from the two dependency edges are combined e1 delete line nmod in ande2 delete string nmod after .e1has one path p11 .e2edge has two paths p21 remove selectstring bconditionoccrance aftercond p22 remove iterationscope bconditionoccrance aftercond .
when combining paths of e1ande2 the cgt combined from p11 andp22will have smaller size apis than the combination of p11 andp21 apis since inp11andp22 will be combined as prefix.
thus the smaller cgt will be chosen to generate the final code expression.
using deeper semantics to guide the paths selection and combination steps could be one of the solutions to avoid this error.
.
threats to validity there are several factors that may threaten the validity of hisyn.
the quality of api documentations.
like in human programming the quality of the documentation of the target api or language is important.
the api description decides which apis will be the candidates for a mapping element in ir.
if the description is not precise an api may not be selected as a candidate which leads to errors in results.
if the description of several apis is similar more unrelated apis will be selected and result in more paths in later steps and cause errors potentially.
the issue can be mitigated by providing some guidelines or even tools to help document developers in ensuring the quality of documentation.
the quality of queries .
the queries are the only source information that describes the user s intention.
thus the quality of queries directly affects the synthesized results.
a query with grammar errors could mislead the nlp engine and result in a wrong dependency graph a query with imprecise descriptions will not describe the intention clearly and could lead to wrong semantic mappings.
the issue can be mitigated by extending the front end of hisyn into an interactive module which may clarify users intentions by interacting with users.
related work nlp has been used for software maintenance and other purposes .
our discussion concentrates on work closely related with code synthesis.
various specifications are used for code synthesis.
a specification can be first order logic expressions a set of examples natural language partial programs or any other form that is easier to write than the expected program.
as hisyn is a natural language based synthesizer we concentrate on prior work on program synthesis from natural language nl .
rule based approaches have been developed to synthesize programs for domain specific tasks such as smartphone automation sql queries and spreadsheet data analysis .
in contrast hisyn is featured by cross domain extensibility.
recent efforts have been spent on machine learning based approaches .
for example desai and others presented a general framework for constructing program synthesizers given a domain specific language dsl definition and training data that contains example pairs of english sentences and the expected programs in the dsl.
84hisyn human learning inspired natural language programming esec fse november virtual event usa quirk uses the semantic mapping approach that learns to map natural language descriptions of if then rules to executable code.
lin leverages recurrent neural networks rnns for nl to code translation.
chen applies lstm based sequence to sequence model with other specifications for code synthesis.
applying these approaches to program analysis would require many training examples to cover the vast space of possible code complexities and situations.
hisyn avoids the barrier by taking full advantage of the domain knowledge nl dependencies and the grammar graphbased translation.
another body of work is api learning.
this work tries to identify some statistical patterns of api usage.
examples include code search tools api usage pattern mining api sequence generation .
these studies rely on statistical machine learning techniques.
they hence require a large set of examples requiring extra efforts when applying to other domains.
discussions our study has focused on the core technical challenges.
some engineering considerations may need to take in the practical usage of hisyn.
for instance even though hisyn gives reasonably accurate results the accuracy could be potentially improved with some interactive features added.
the tool could provide some informative feedback to users such as the dependency graph from the nl query and the semantic mapping between the nodes and the apis like figure b .
that could give programmers insights on the generated code expressions and help them replace erroneous parts.
at programmers demands the tool may also show other candidates not only the minimum heuristic in the semantic mapping list of a node optionally in the order of promise to offer programmers more choices.
prior studies have explored interactivity for nl based code synthesis we decided to focus this work on the core challenges and leave the interactivity as a feature to add in the future.
the nlp tool we select to use is not the one with the most cutting edge nlp techniques but the one with common adoptions for its maturity.
there has been much progress in nlp in recent years which provides promising techniques to improve nlp results.
we foresee that as these techniques become more mature and get integrated into practical nlp tools the accuracy of hisyn could get further improved.
conclusion this paper introduces nlu driven code synthesis a new approach to nl programming.
experiments on framework hisyn demonstrate that nlu driven without using any training example can produce results comparable with data driven methods trained on hundreds of labeled examples.
it saves the burden in example collections avoids dataset caused biases and at the same time gives much better interpretability and support for error diagnosis.
in domains already having many labeled data the nlu driven method could potentially combine with data driven methods to reduce biases and increase interpretability which is left for the future to explore.