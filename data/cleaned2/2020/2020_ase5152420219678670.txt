testing your question answering software via asking recursively songqiang chen y shuo jin y xiaoyuan xie yz school of computer science wuhan university china i9schen gmail.com imjinshuo whu.edu.cn xxie whu.edu.cn abstract question answering qa is an attractive and challenging area in nlp community.
there are diverse algorithms being proposed and various benchmark datasets with different topics and task formats being constructed.
qa software has also been widely used in daily human life now.
however current qa software is mainly tested in a reference based paradigm in which the expected outputs labels of test cases need to be annotated with much human effort before testing.
as a result neither the just in time test during usage nor the extensible test on massive unlabeled real life data is feasible which keeps the current testing of qa software from being flexible and sufficient.
in this paper we propose a method qaasker with three novel metamorphic relations for testing qa software.
qaasker does not require the annotated labels but tests qa software by checking its behaviors on multiple recursively asked questions that are related to the same knowledge.
experimental results show that qaasker can reveal violations at over of valid cases without using any preannotated labels.
diverse answering issues especially the limited generalization on question types across datasets are revealed on a state of the art qa algorithm.
index terms question answering testing and validation recursive metamorphic testing natural language processing i. i ntroduction with the booming development of natural language processing nlp techniques machine has been able to process many tasks.
among them question answering qa is one challenging but attractive goal that requires machine to understand the human language and infer information from it as the human do .
given a question qa software intelligently comprehends the relevant information from a lengthy reference passage or one huge knowledge base and returns the deduced answer.
qa software has been widely used in daily human life now.
for example many intelligent devices are equipped with a virtual assistant such as siri from apple and dueros from baidu which can provide the qa service.
recently we have seen many algorithms being proposed to improve the performance of qa software.
meanwhile various benchmark datasets with distinct topics and task formats have been constructed to evaluate how well machine can answer the questions .
nevertheless the testing methods for qa software are still primitive and thin.
specifically current qa testing practices mainly adopt the reference based paradigm.
and when performing a reference based test the researchers or engineers have to first manually annotate the labels correct answers for the test cases assigned questions which requires yequal contribution and co first authors.zcorresponding author.much human effort .
afterwards qa software is tested by comparing its outputs with the annotated labels.
as a result these testing practices of qa software are mandatorily relying on the existing well annotated datasets.
however the reference based test paradigm has some limitations due to its reliance on the pre annotated labels.
first it cannot support the just in time test for qa software which requires an immediate issue detection on the returned answers to unlabeled questions.
but such a kind of testing is actually inevitable and necessary in the daily usage.
let us consider the common usage scenario of qa software where the user inputs one question to which she is looking for the answer.
after getting an answer from qa software she needs to make a quick decision on whether to trust this answer or not without any pre annotated labels.
this process could be seen as one test execution followed by an immediate issue detection on which the decision is based but the current reference based test paradigm is obviously not designed to support such a process.
and for the real life usage it is also very common to see the users directly trust the qa software to have passed this test and given a reliable answer because they have barely any clues on the correctness of these answers.
however since the reliability of qa software is not always guaranteed because of the complexity and intractability of the neural networks it could be very risky to trust the outputs without any inspection.
secondly the reference based test can only be performed on the existing well annotated benchmarks which may confine the test sufficiency on qa software and hinder the understanding of its real performance.
actually recent studies found that some existing benchmarks introduce bias of topics and task formats because of their limited size and diversity .
as a result some vital functionalities of qa software may not have been well tested yet.
however the only way to perform more reference based tests with the massive and continuously increasing unlabeled real life data is to pay a lot of human effort to annotate them all the time.
this is evidently inefficient and infeasible.
in addition overreliance on the manually annotated labels could also hurt the accuracy of test results because it has been revealed that some of the manually annotated labels that are regarded as the golden