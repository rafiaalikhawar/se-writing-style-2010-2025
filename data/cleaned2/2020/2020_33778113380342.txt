big code !
big vocabulary open vocabulary models for source code rafael michael karampatsis university of edinburgh edinburgh united kingdomhlib babii free university of bozen bolzano bozen bolzano italyromain robbes free university of bozen bolzano bozen bolzano italy charles sutton google research and university of edinburgh mountain view ca united statesandrea janes free university of bozen bolzano bozen bolzano italy abstract statistical language modeling techniques have successfully been applied to large source code corpora yielding a variety of new software development tools such as tools for code suggestion improvingreadability andapimigration.amajorissuewiththese techniquesisthatcodeintroducesnewvocabularyatafarhigher rate than natural language as new identifier names proliferate.
bothlargevocabulariesandout of vocabularyissuesseverelyaffect neural language models nlms of source code degrading their performance and rendering them unable to scale.
in this paper we address this issue by studying how various modelling choices impact the resulting vocabulary on a large scale corpus of projects presenting an open vocabulary source code nlm that can scale to such a corpus times larger than in previouswork and3 showingthatsuchmodelsoutperformthe state of the art on three distinct code corpora java c python .
to our knowledge these are the largest nlms for code that have been reported.
alldatasets code andtrainedmodelsusedinthisworkarepublicly available.
ccs concepts softwareanditsengineering softwaremaintenancetools .
keywords naturalness of code neural language models byte pair encoding acm reference format rafael michael karampatsis hlib babii romain robbes charles sutton and andrea janes.
.
big code !
big vocabulary open vocabulary modelsforsourcecode.in 42ndinternationalconferenceonsoftwareengineering icse may23 seoul republicofkorea.
acm new york ny usa pages.
icse may seoul republic of korea copyright held by the owner author s .
acm isbn .
introduction manyworkshavetakenadvantageofthe naturalness ofsoftware to assist software engineering tasks including code completion improvingcodereadability programrepair identifying buggy code and api migration among many others .theseapproachesanalyzelargeamountsofsourcecode ranging from hundreds to thousands of software projects building machine learning models of source code properties inspired by techniques from natural language processing nlp .
when applying any nlp method to create any type of software development tool a crucial early decision is how to model software svocabulary.thisisallthemoreimportantbecause unlike innaturallanguage softwaredevelopersarefreetocreateany identifierstheylike andcanmakethemarbitrarilycomplex .
because of this fundamental fact any model that is trained on a large scalesoftwarecorpushastodealwithanextremelylargeand sparsevocabulary section2 .rarewordscannotbemodelledeffectively.
furthermore if identifiers were not observed in the training set manyclassesofmodelscannotpredictthem whichisknown as theout of vocabulary oov problem.
hellendoorn and devanbu observe this issue for the task of language modeling showing that aneurallanguagemodel nlm hasdifficultiesscalingbeyondas fewasahundredprojects .giventhatneuralapproachesare thestate of the artinnlp findingwaystoscalethemtoalarger software corpus is a very important goal.
ourfirst contribution is a thorough study of the effects of the vocabularydesignchoicesthatmustbemadewhencreatingany nlp model of software section .
the vocabulary design choices westudyincludehowtohandlecomments stringliterals andwhite space whethertofilteroutinfrequenttokens andwhetherandhowtosplitcompoundtokens suchasnamesthatcontaincamelcaseand underscores.
we examine how these choices affect the vocabulary size whichaffectsthescalabilityofmodels andhowtheyaffectthe oov rate that is how often the vocabulary fails to include names thatappearinnewprojects.wefindthatthechoiceshavealarge impact leadingtovariationsinvocabularysizeofupto threeorders of magnitude.
however we find that the most common ways to reduce vocabulary that were previously considered in the software engineering literature such as splitting identifiers according to underscoresandcase arenotenoughtoobtainavocabularyofa manageablesize advancedapproachessuchasadaptationsofthe byte pair encoding bpe algorithm are needed to reach this goal and deal with the oov problem.
ieee acm 42nd international conference on software engineering icse this work is licensed under a creative commons attribution international .
license thisempiricalstudymotivatesour secondcontribution.drawing on our results we develop a large scale open vocabulary nlm for source code section .
to our knowledge this is the first bpe nlmforsourcecodereportedintheliterature.thisnlmmodel leverages bpe beam search and caching to both keep vocabulary sizelowandsuccessfullypredictoovtokens.weshowthatthis nlm is able to scale we train it on up to software projects yielding the largest nlm trained on source code we are aware of.
finally in our third contribution we extensively evaluate our nlm sections .
we show that the open vocabulary nlm outperformsboth n gramlmsandclosedvocabularynlmsforthetask of code completion for several languages java c and python .
to showthatimprovementinlanguagemodellingtransferstodownstream se tasks we conduct an experiment similar to ray et al .
whoshowedthatlanguagemodelscanbeusedtohighlight buggycode.indeed wefindthatouropen vocabularynlmismore effective than previous lms at highlighting buggy code.
more broadly these contributions may impact future development software tools.
first source code lms have been used in a diverse variety of tools well beyondthe obvious application of autocompletion ranging from code readability to program repair .ourimprovednlmcouldleadtoimprovementstoallofthese tools.
second recent results in nlp show that nlms can be used as upstream tasks in transfer learning leading to stateof the art improvementin downstream tasks for instance a modelcanbepre trainedasannlm andlateronfine tunedasaclassifier.
improved nlm architectures could lead to improved downstream classifiers especiallyif thelabelleddataisscarce.whiletransferlearning from language models has been applied in software en gineering it has not been applied to source code due to the aforementioned vocabulary issues.
finally the general insightsabout vocabulary design that we study are not specific to nlms butarisewheneverwebuilddevelopmenttoolsbyapplyingnlp methods to source code.
we conclude the paper in section and briefly describe the artifacts used in this work and how to obtain them in section .
background and related work we first note that this work is a consolidation of two unpublished worksoriginallyconductedindependently oneworkfocusedonthe impactofvariousvocabularychoicesontheresultingvocabulary size and the training of nlms while the other work investigated the specific vocabulary choice of byte pair encoding andintroduced several improvements to the training procedure .
thispapercontainsjointworkthatimprovesonbothearlierworks by investigating additional characteristics of the vocabulary additional improvements to nlm training an additional use case for nlms and a more thorough empirical evaluation.
.
language modeling in nlp a language model lm estimates the probabilities of sequences of words based on a training corpus.
in nlp these models have been applied to tasks such as speech recognition and machine translation .
early language models were based on n grams the probability of a token is computed based on the n previous tokens in the sequence.
these had success in nlp applications buthavetwoissues.first theyoperateonsmallamountsofpreviouscontext with noftenrangingfrom3to6 e.g.
n 6forjava .
increasing ndoes not scale well if the vocabulary is large for a vocabulary ofsize m thereare mnpossible n grams.second they sufferfromdatasparsity notallpossible n gramsexistinthecorpus.
smoothing alleviates but does not eliminate the issue.
the current state of the art in nlp is neural language models nlm .
nlms represent words in a continuous vector space such that words that are semantically similar are close in vector space allowingthemodeltoinferrelationshipsbetweenwords even if they do not appear in a specific context during training.
this allows these models to better deal with data sparsity leading to better performance.
current nlms are based on architecturessuch as recurrent neural networks rnn long short term memory lstm ortransformer thatmodel longrange dependencies astudyoflstmnlmsshowedthattheyusecontext as large as words much longer than n grams.
.
difficulties with large vocabularies ml models in general and nlms in particular do not handle large vocabularies well.
this is for several reasons scalability.
duringpre processing eachwordisconvertedtoa numerical representation first via one hot encoding producing sparse vectors of length equal to the vocabulary.
nlms then convertthesetowordembeddings densewordvectorsofmuchsmaller dimensions usuallyinthehundreds intheirfirstlayer.foravo cabularyofsize mandembeddingsofsize n theembeddinglayer is a dense matrix of size m n. a large m e.g.
or more affects the memory required by the model as well as the amountof computation required for training.
the output of an nlm is a predictionoverthenexttoken whichisaprobabilitydistribution overtheentirevocabulary.thismustbecomputedonceforeach token in the training corpus many times during training.
this can be prohibitively slow for large vocabularies .
out of vocabulary oov .
in traditional closed vocabulary models the vocabulary must be known in advance and will be builtbasedonthetrainingcorpus.anynewwordencounteredattest time calledout of vocabularywords willnotbeabletobeone hot encoded as the resulting vector would exceed the expected dimensions.
a common workaround is to have a specific unknown token and replace any word not previously seen by this token.
this loses information making the nlm unable to predict any new token which is particularly problematic for source code.
rare words.
deriving meaningful embeddings for rare words is difficult since there is very little data to work with.
gong et al.
showthatthepropertythatsemanticallysimilarwordshavesimilar embeddings does not hold for rare words they hypothesize that since the words are rarely seen the embeddings are rarely updated and thus stay close to their initialized values .
this issue is likely to impact performance a very large vocabulary has been shown to negatively impact it particularly with oov words .
.
handling large vocabularies in nlp anopenvocabularymodel isnotrestrictedtoafixed sizedvocabulary determined at training time.
for instance a character lm 1074predicts each word letter by letter its vocabulary is the set of characters the oov issue vanishes.
however it needs to model longer dependenciesthanawordnlm impactingperformance.models usingsubword units o rsubwords combine the strengths of characterandtokenlms.asubwordunitisasequenceofcharacters that occurs as a subsequence of some token in the training set the modeloutputsasequenceofsubwordunitsinsteadofasequence of tokens.
many nlp models have used linguistically motivated subwords .mikolov etal.foundthatsubwordmodels improved on character models .
sennrich et al.adapt the bytepairencoding bpe algorithmtodecomposewordsinsubwords improving rare word translation .
kimet al.combine a charactercnnwithanlm .vaniaandlopezcomparelms words morphs character n grams bpe on several languages .
another approach to the oov problem are cache models and copymechanisms whichallowthemodeltore usewords thathaveappearedpreviously.thishelpswiththeoovproblem because such models can copy words that are not in their fixed vocabulary butitdoesnothelpthe firsttimeanoovwordappears.
.
language modeling and vocabulary in se languagemodelsinsoftwareengineering se .
seminalstudieshave laidthegroundworkfortheuseoflanguagemodelsonsourcecode gabel and su show that software is very repetitive which motivates the use of statistical modelling for code.
hindle et al .
build language models of source code finding applications in codecompletion.nguyenetal.
augmented n gramlmswith semantic information such as the role of a token in the program e.g.
variable operator etc.
tu et al.
find that software is even more repetitive taking local context into account.
rahman et al.
find that while some aspects of software are not as repetitive as previouslythought non syntaxelements othersareevenmoreso apisequences .othermodelsofsourcecodeincludeprobabilistic higher order grammars phog which use asts and several types of rnns including lstms .
se applications of language models.
probabilistic code models have enabled many applications in software engineering see allamanisetal .
forasurvey .oneexampleisrecommendersystems aiming to aid developers in writing or maintaining code hindle etal.usedatoken levellmforcodecompletion whilelater frankset al.improved on performance with tu s cache and builtacodesuggestiontoolforeclipse .anotherapplication arerecommendationsystemsforvariable method andclassnames thatemployrelevantcodetokensasthelmcontext.campbellet al.
usedn gram language models to detect syntax error locationsinjavacode andlaterusedannlmforthesamepurpose .
rayet al.
showedthat buggycode has onaverage lower probability than correct code and that lms can spot defects as effectively as popular tools such as findbugs.
several approaches use neural machine translation in which an encoderlmispairedtoadecoderlm.examplesincluderecoveringnamesfromminifiedjavascriptcode orfromdecompiledc code .
other applications include program repair learning codechanges orgeneratingsourcecodecomments .guet al.
generate api usage sequences for a given natural language query.theythenlearnjoint semanticrepresentationsofbilingualapicallsequencestosupportapicallmigration .yinetal.
mine pairs of natural language and code from stack overflow to support tasks such as code synthesis from natural language.
large vocabularies in se.
the majority of models of source code usedclosedvocabularymodels.hellendoornanddevanburightly notice that nlms trained on a software corpus would struggle due tovocabularysize becauseidentifiers whicharethebulkof sourcecode canbearbitrarilycomplex andareoftencompound words e.g.
thisidentifierhas6wordsand2numbers causing an explosionofpossibleidentifiers.toproduceannlmthatcanbe trained in a reasonable amount of time hellendoorn and devanbu impose drastic restrictions which would be expected to reduce predictiveaccuracy restrictingthetrainingsetto1 oftheoriginal corpus and thevocabulary toonlyinclude wordswhich occur more than times.
even so the resulting vocabulary size is stillexceeds words.
similarly pradel and sen had a large vocabularyof2.4millionuniquetokens theylimitedittothe10 most common tokens to reduce inaccuracies due to rare words.
to limit this issue previous work has segmented identifiers via a heuristic called convention splitting which splits identifiers on camelcaseandunderscores .eventhoughthissegmentationcan handlesomeoovtokens itislimitedtocombinationsofsubtokens appearing in the training set and thus unable to achieve a truly openvocabulary.additionally manyofthesesubtokensarestillin frequent whichhindersthemodel sabilitytoassignhighscorestotheircompositions.forexample despiteusingconventionsplitting the implementation of code2seq from alon et al.
only keeps the most common vocabulary words.
several studies have empirically compared different techniques forautomaticallysplittingidentifiers .theseworksconsider thesomewhat differentproblemofsplitting identifiersintowords in a way that matches human judgment.
subword units may not necessarily produce words that humans recognize but they can be triviallyreassembledintocompletetokensbeforetheyareshown to a developer.
stemming has also been used to reduce the number of vocabulary words by only keeping their roots this ishowever destructive.
malik et al.combined convention splitting and stemming for type prediction .
few se approaches use caches.
tu et al.
and hellendoorn anddevanbu usen gramcaches.li etal.augmentanrnnwith acopymechanismbasedonpointernetworks toimproveoov codecompletion .whileitcanreuseanoovwordafterseeing it it cannot predict the word s first use learn its representation or learn its dependencies unlike our model.
copy mechanisms were also used for program repair and method naming .
datasets we use code corpora from threepopular programming languages java c and python.
we choose these languages because they have differencesthatcouldaffecttheperformanceoflms.javahasextensively been used in related work .
unlike java c is procedural and makes it possible to write very terse code.1python is a multi paradigm dynamic language with little use of static typing.
for java we used the java github corpus of allamanis et al.
also used in .
the c and python corpora 1for examples see 1075table corpus statistics for each code corpus.
java c python tokens projects tokens projects tokens projects full1.44b .68b .05b small .74m .64m .55m bpe .84m .38m .32m valid.
.83m .97m .65m test .33m .88m .42m were mined following the procedure described in the c corpus was mined in and the python corpus was mined in .
for lexical analysis we used the java lexer implemented in for c andpythonweusedthepygments3library.descriptivestatistics are in table .
for python and c we sampled of the corpus for validation and for testing.
another of the corpus was sampled as a separate data set to learn subword encodings with bpe.
the restofthedatawasusedfortraining.wealsoreportresultsona smaller subset of of our full training set.
for java we used a slightly different procedure to make our experiment comparable toa previous study .
we divide the data into five subsets as in the othertwolanguages.thevalidationandtestsetsarethesameas in and our small train set is the same as their training set.
toobtainthefulljavatrainset wecollectallofthefilesinthejava github corpus that do not occur in the validation or test set.
of these we sampled random projects for the subword encoding data set and the remaining projects were used as the full train set.
inthevocabularystudy bothtrainingsetsandtestsetsareused.
totrainlms wepreprocessthecorporatomatch replacing non asciicharacter sequencessuchas chineseideogramsinside strings with a special token non en removing comments and keeping strings.
note that the lexer in replaces all strings with lengthof15charactersormorewiththeemptystring.inpython we do not add any special tokens to represent whitespace.
modeling vocabulary we study a series of modeling choices for source code vocabulary.
thesechoicesmaybeimplicitlymadebyresearchers withorwithoutevaluatingalternatives theymaynotalwaysbedocumented in their studies.
by making the choices explicit we can study their impactonthevocabulary.wereportresultsonjava similarresults can be observed for c and python.
our evaluation criteria are scalability trainingofmodelsshouldscaletothousandsofprojects.
scalability is influenced by the vocabulary size number ofunique words andthecorpussize numberoftokens .wereportbothmetrics onour fulljava trainingset andcompare themto abaseline with percentages.
for instance .6m and .43b .
information loss modelsshouldbeabletorepresenttheoriginal input as much as possible out of vocabulary oov tokens are particularly undesirable.
we build a vocabulary on the trainingset and compare it with the test set vocabulary we report the percentage of new vocabulary words seen in the test set.
as large smaller vocabulary the75 000mostfrequentwords as in .
to show trends we also plot oov for full vocabulary 200k 100k 75k 50k and 25k.
such as .
word frequency as rare words have worse representations than frequent ones increasing word frequency is desirable.
differentmodellingchoicescanincreaseordecreasethenumberofrare words.
we report the percentage of the vocabulary that has a frequency of or less and plot a bar chart showing the percentage of vocabulary with frequencies of and .
for instance .
baseline .6m r2.43b r42 r83 ourbaselineisavocabularyofunsplittokens exceptstringsand commentsthataresplitbywhitespace not 11millionuniquewordsonjava large.theoovrateonthetest set exceeds with the full vocabulary showing that developers docreatemanynewidentifiers.themostcommonwaytoshrink vocabulary is to replace infrequent tokens with unk .
so further worsens oov issues after reducing the vocabulary to a moremanageable75k closeto80 ofthetestvocabularyisunseen in the training set.
many words are infrequent of vocabulary wordshaveafrequencyof10orless with25 occurringonlyonce.
.
filtering the vocabulary simplestistofiltervocabularyitemsthataredeemedlessimportant.
filtering is destructive it thus needs to be thoroughly justified.
english.
.4m r2.43b r35 r83 source code can contain many non english words in identifiers strings and comments either because developers use other lan guages orfortestingorinternationalizationpurposes.handling multilingual corpora is an nlp research topic in itself we evaluate the simplifying assumption to limit a corpus to english.
this is not trivial dictionary based heuristics have too many false positives e.g.
acronyms .
we use a simple heuristic a word is non english if it contains non ascii characters.
this is imperfect caf na ve or heuristiken are misclassified.
non english words are replaced witha non en placeholder.eventhen thevocabularyshrinksby only while oov drops by only at 75k.
whitespace.
.4m r1.89b r35 r83 some applications e.g.
pretty printers may care about the layoutofsourcecode.othersmaynot givingimportanceonlyto syntactic orsemantic aspects unless code layout issyntactically important suchasinpython .filteringoutwhitespacereducesthe vocabulary only by a handful of tokens but reduces corpus size by .89b tokens .
comments .8m r1.26b r38 r83 comments often contain natural language which is much lessrepetitive than code.
while tasks such as detecting self admittedtechnical debt rely on comments others do not.
replacing comments by placeholder tokens e.g.
comment significantly reducescorpussize afurther26 but itseffectonvocabularyis limited given that comments are already split on whitespace .
strings.
.5m r1.15b r39 r83 similarly string literals canbe filtered replacing themby a placeholder token like string .
this does not reduce corpus size as 1076much a further but shrinks vocabulary a further close to .
million words.
this is still extremely large.
we also evaluate the configuration used in strings are kept unsplit but strings longer than characters are replaced by the empty string.
for consistencywithpreviouswork weuseitas newbaseline .itincreases vocabulary oov and infrequent tokens rate .9m r .15b r39 r84 fulltokenvocabulariesrangeinthemillions andhencedonot scale.
oov and frequency issues are extremely important.
.
word splitting identifiers are the bulk of source code and its vocabulary.
while new identifiers can be created at will developers tend to follow conventions.whenanidentifierismadeofseveralwords inmost conventions thewordsarevisuallyseparatedtoeasereading either incamelcase or in snake case .
thus an effective way to reducevocabularyisto splitcompoundwordsaccordingtothese word delimiters as was done by allamanis et al.
.
the decision whether to split compound words or not has important ramifications.
first it introduces additional complexity the lm can no longer rely on the assumption that source code isa sequence of tokens.
instead compound words are predicted asa sequence of subtokens albeit in a smaller vocabulary.
second subtokensincreasethelengthofthesequences makingitharder torelatethecurrentsubtokenstothepastcontext asitincreases insize.this makestheapproachunviablefor n gramsas nwould need to increase significantly to compensate.
splitting tokens has advantages most obviously the vocabulary canbemuchsmaller.consequently theoovrateisreduced.third a model may infer relationships between subtokens even if thecomposedwordisrare asthesubtokensaremorecommonthanthe composed word.
finally using subtokens allows a model to suggestneologisms tokens unseen in the training data .
splitting.
.27m r1.81b r8 r81 wordsplittingviaconventionsdrasticallyreducesthevocabulary byaclosetoanorderofmagnitude slightlymorethanamillion words at the cost of increasing corpus size by .
the impact on the oov rate is also very large as it decreases by a factor of5 in the unfiltered case for a vocabulary of 75k it is a factor of .
however the effect on word frequency is limited with only more words that are more frequent than occurrences.
case.
.09m r2.16b r9 r83 most commonly words in different case e.g.
value value value will be distinct words for the lm.
this could increase the vocabulary but removing case loses information.
a possible solutionis toencode caseinformation in separatortokens e.g.
valuebecomes upper value valuebecomes upper value .atthecost of increasing sequence length.
case insensitivity does decrease thevocabulary butnotbymuch afurther2 whilecorpussize increases significantly a further .
thus our following configurations do not adopt it our new baseline keeps case.
word splitting is effective but the vocabulary is still large a million words .
oov and frequency issues are still important.
.
subword splitting as splitting on conventions is not enough we explore further.
numbers.
795k r1.85b r6 r72 numeric literals are responsible for a large proportion of the vocabulary yet theirvocabulary is very limited.
thus an alternative tofilteringthemoutistomodelthemasasequenceofdigitsand characters.
this yields a considerable decrease in vocabulary with ourpreviousbaseline foronly2 increaseincorpussize.for oov thereisaslightimprovementfora75kvocabulary as well as for frequency of words occur times or more .
spiral.476k r1.89b r3 r70 several approaches exist that split a token into subtokens butgo beyond conventions by using mining software repositories techniques such as samurai linsen spiral or even neural approaches .
we applied the spiral token splitter which isthestateoftheart.weobservedafurther26 reductionofthe vocabulary fora2 increaseincorpussizecomparedtonumber splitting.spiralwasalsoveryeffectiveintermsofoov with9 of unseen word when the vocabulary is limited to 75k and when unfiltered 476kwords .theimpactonfrequencywaslimited.even if this is encouraging the oov rate is still high.
other approaches.
stemming can reduce vocabulary size but loses information it is not always obvious how to recover the original word from its stem.
we found that applying stemmingcan further reduce vocabulary by which does not appear tobe a worthwhile tradeoff given the loss of information.
anotheroption is character models that achieve an open vocabulary by predicting the source file one character a time.
oov issues vanish but unfortunately this drastically inflates sequence lengths so a character model is not desirable.
while these strategies are effective they do not go far enough vocabulary stays in the hundreds of thousands range.
there are still oov issues for unseen data most words are uncommon.
.
subword splitting with bpe thefinalalternativeweevaluateissubwordsegmentationviabytepairencoding bpe .bpeisanalgorithmoriginallydesignedfor data compression in which bytes that are not used in the data replace the most frequently occurring byte pairs or sequences .
in subword segmentation this corpus is represented as a sequence of subwords.
special end of token t symbols are added to allow us to convert from a sequence of subword units back into a sequenceoftokenswithease.theapproachwasadaptedtobuild nmtvocabularies themostfrequentlyoccurringsequences of characters are merged to form new vocabulary words.
bpe builds up the vocabulary of subwords iteratively at each iterationatrainingcorpusissegmentedaccordingtothecurrent vocabulary.
the initial vocabulary contains all characters in the data set and t and the corpus is split into characters and t .
then allsymbolpairsappearinginthevocabularyarecounted.all theappearancesofthemostfrequentpair s1 s2 arereplacedwith auniquenewsinglesymbol s1s2 whichisaddedtothevocabulary without removing s1ors2 which may still appear alone .
this procedure is called a merge operation.
the algorithm stops after agivenmaximumnumber nofmergeoperations thisistheonly 1077java code public attributecontext m ethodsetter object value this.value value this.setter setter subword units public t attribute con text t t method t set ter t t object t value t t t this t .
t value t t value t t this t .
t set ter t t set ter t t t figure example of java code as a list of subword units.
parameter.thefinaloutputofthealgorithmis thenewvocabulary which contains all the initial characters plus the symbols createdfromthemergeoperations and theorderedlistofmerge operations performed in each iteration.
new data is segmented by splitting it into characters and merging in the same order.
bpehasseveraladvantages.first nowordisoov thevocabularyalwayscontainsallsinglecharacters sounknownwordsat test time can be represented using those subwords if no longer subwords apply.
second the vocabulary dynamically adapts tothe frequency of the sequences common sequences will be rep resented by a single word eg exception while rare ones will be segmented into more common subword units such as roots prefixesandsuffixes thishelps withsparsityissues.finally bpe allows for fine grained control of vocabulary size by tuning the numberofmergeoperations.alargervocabularywillhavemore complete wordsand less sequences smallerones will havelonger sequences.
an example of a java code snippet segmented into subwords is shown in figure .
we computed bpe for 1k 2k 5k 10k and 20k merges on a held out set of 1k project.
bpe subwords.
10k r1.57b r0 r1 weapplybpe 10kmerges toourjavacorpuswithpreprocessed as in which we use as a baseline for comparison.
as expected theoovissuesvanish evenforan extremelysmallvocabulary.the corpussizegrows butnotmorethanpreviouschoicesweexplored.
since bpe merges based on frequency the resulting subtokens no matter their size are frequent more than of the remaining words occur more than times in the corpus with very few wordsthatareinthehundreds and1 lessthanten.loweramounts of merges result in a smaller vocabulary at the cost of a larger corpus size.
our largest bpe vocabulary 20k is times smaller than our initial baseline our smallest is times smaller.
qualitativeexamination.
whilethegoalofbpeisnottoproduce human readable tokens we examine how closely the splits bpe produces match human ones.
we inspected random identifiers and provide anecdotal evidence of the types of splits produced by bpe.
our goal is not to provide strong evidence but rather to give a sense to the reader of what bpe splits look like in practice.
whilesomesubwordsarereadableatbpe1k fileroutputrservice t some subwords are not defaultrmrutr ablertrerernode t but look good at 5k defaultr mutablertreenode t .
bpe handles rare words gracefully producing longer sequences of shorter units as expected.
someexamples include rare words due to typos inrculrdedr 4note that including non ascii characters grows the vocabulary by words in each case a solution is to apply bpe at the bytelevel as done in template t orforeignwords vrormrerrkrmedirenraurfrlisrter t .
some rare words are split in root and suffix gridrify t but some acronyms may be unexpectedly split ibran t .further bpecansplitwordscorrectlywithout case information http rclient rlib t at 5k .
bpeshrinkssourcecodevocabulary veryeffectively.moreover most of the vocabulary is frequent improving embeddings.
neural language model for code we present our nlm for code based on subword units which is based on a recurrent neural network rnn .
rnn lms scan aninput sequence forward one token at a time predicting a distributionovereachtokengivenall of thepreviousones.rnnswith gated units can learn when to forget information from the hidden state and take newer more important information into account .
among various gated units grus have been shown to perform comparably to lstms in different applications .
we intentionally selected a small model as our base model a single layer gru nlm built upon subword units learned from bpe section .
.
for each vocabulary entry we learn a continuous representationof512features whilethegrustateisofthesame size.inallourexperimentsweusedalearningrateof0.
dropoutof0.
andamaximumof50epochsofstochasticgradientdescent with a minibatch size of for the small training sets or for thefull trainingsets .these hyper parametersweretuned onthe small train and validation sets.
after each iteration we measurecross entropy on a validation set section .
if the cross entropy islargerthanthepreviousepochthenwehalvethelearningrate and this can happen for a maximum of times otherwise training stops.duringtrainingoftheglobalmodelweunrollthegrufor 200timesteps following .ourimplementationisopensource githuburlomittedforreview .wealsoexperimentwithlarger capacity models hidden features and gru state .
.
selecting subword units with bpe in our code lm we address vocabulary issues by having the model predictsubwords rather than full tokens at each time step.
subwords are inferred by bpe section .
on a held out dataset of projects that are separate from the training validation and test sets.weexperimentedwiththreeencodingsizes i.e.
themaximum number of merge operations and .
to train the lm wefirstsegmentthetrain validation andtestsetsusingthe learned encoding.
we transform each token into a character sequence adding t after every token.
then we apply in order the merge operations from bpe to merge the characters into subword units in the vocabulary.5as in we do not merge pairs that crosstokenboundaries.finally wetrainandtestthenlmasusual on the data segmented in subword units.
.
predicting tokens from subword units autocompletion algorithms present a ranked list of kpredicted tokens rather than a single best prediction.
with a model based on subwordunits itisnotobvioushowtogeneratethetop kpredictions becauseasingletokencouldbemadefrommanysubword 5we use the bpe implementation from 1078units.
we approximate these using a custom variation of the beam search algorithm.
if the beam is large enough the algorithm can give a good approximation of the top kcomplete tokens.
thenlmdefinesaprobability p s1...sn foranysubwordunit sequence.
the goal of the beam search is given a history s1...sn of subword units that already appear in a source file predict the nextmostlikely completetoken.a completetoken isasequenceof subwordunits w1...wmthatcompriseexactlyonetoken thatis wmendswith t andnoneoftheearliersubwordunitsdo.beam searchfindsthe khighestprobabilitycompletetokens wherewe denoteasingletokenasthesequenceofunits w1...wm thatmaximizethemodel sprobability p w1...wm s1...sn .importantly thelength mofthenewcompletetokenis notfixedinadvance but the goal is to search over complete tokens of different length.
given a value of kand a beam size b the algorithm starts by queryingthemodeltoobtainitspredictionsofpossiblesubword units ranked by their probability.
the algorithm uses two priority queues one called candidates which ranks the sequences of subword units that still need to be explored during the search and one called besttokens which contains the khighest probability complete tokens that have been expanded so far.
each candidate is astructurewithtwofields textwhichistheconcatenationofall the subword units in the candidate and probwhich is the product of the probabilities of each subword unit in the candidate.
both of the priority queues are sorted by the probability of the candidate.
in each iteration the algorithm pops the bbest candidates from thecandidates queue expandsthemwithoneadditionalsubword unit andscorestheirexpansions.ifanexpansioncreatesatoken thenewsubwordunitendswith t thenitispushedontothe token queue and the worst token is popped.
this maintains the invariantthat besttokens hassizek.ifthenewexpansionisnot a complete token then it is pushed onto the candidates queue where it can potentially be expanded in the next iteration.
.
caching we also implement a simple caching mechanism for our nlm to exploit the locality of source code particularly previously defined identifiers.
at test time each time an identifier is encountered the5 tokenhistorythatprecededitisaddedtoacachealongside it.
differently to n grams we do not store probabilities as the nlmwillcomputethem.ifthecurrent5 tokenhistoryexistsinthecache theidentifiersthatfolloweditareretrieved thisisinpractice very small usually or identifiers .
these identifiers are then scoredbythenlm andtheirprobabilitiesarenormalizedto1.the beamsearchdescribedearlieris thenrun andthetwoprobability distributionsaremerged accordingtoacacheweightparameter cache pred cache wei ht beam pred cache wei ht .the top of the merged predictions are then returned.
we set the cache weight to .
.
note that like beam search this is a test time only addition that does not affect training.
.
dynamic adaptation to new projects aglobal lm trained in a cross project setting will perform better if it is adapted to a new project .
lms with n grams also employ caches for this.
simply training an nlm from scratch on a newprojectwillnothaveenoughdatatobeeffective whiletraininga new model on both the original training set and the new project would be impractical and computationally expensive.
instead we use a simple method of dynamically adapting our globalnlmstoanewproject.givenanewproject westartwith theglobalnlmandupdatethemodelparametersbytakingasingle gradient step on each encountered sequence in the project after testingonit.thisseriesofupdatesisequivalenttoasingletraining epoch on the new project.
in our evaluations in section we will splituptheprojectfilesinsuchawaythatwearenevertraining on our test set.
we unroll the gru for time steps instead of as in our global models in order to update the parameters morefrequently.
we apply only one update for two reasons.
first it isfaster allowing the model to quickly adapt to new identifiers in theproject.second takingtoomanygradientstepsoverthenew projectcouldcausethenlmtogivetoomuchweighttothenew project losing information about the large training set.
evaluation intrinsic evaluation language modeling.
a good language modelassignshighprobabilitiestorealsentencesandlowprobabilitiestowrongones.forcode fragmentsthataremorelikelyto occurinhuman writtencodeshouldbeassignedhigherprobability.
precise scoring of code fragments is essential for tasks such as translating a program from one programming language to another code completion and code synthesis from natural language and vice versa .
asinpreviouswork ourintrinsicmetricisthestandardcross entropy.crossentropydefinesascoreoverasequenceoftokens t1 t2 ... t c .foreachtoken ti theprobability p ti t1 ... ti ofeach token is estimated using the model under evaluation.
then the averagepertokenentropyis hp c c summationtext c i 1logp ti t1 ... ti .
cross entropy is the average number of bits required in every prediction lowervaluesarebetter.itnotonlytakesintoaccountthe correctness of the predictions but also rewards high confidence.
ournlmsdefineadistributionoversubwords nottokens.to compute cross entropy for subword nlms we segment each token tiintosubwords ti wi1...wim.thenwecomputetheproduct p ti t1 ... ti producttextm m 1p wim t1 ... ti wi1...wi m where the right hand side can be computed by the subword nlm.
this probability allows us to compute the cross entropy hp c .
extrinsic evaluation code completion.
wereporttheperformance of our lms on code completion which is the task of predictingeachtokeninatestcorpusgivenalloftheprevioustokensinthefile.wemeasureperformancewithmeanreciprocalrank mrr asiscommonincodecompletionevaluation .
eachtimethelmmakesaprediction wegetarankedlistof k predictions.
for each one the reciprocal rank is the multiplicativeinverse ofthe rankof thefirst correctanswer.mrr isthe average of reciprocal ranks for a sample of queries q mrr q q summationdisplay i rank i. a simplified description of mrr is that it averages top kpredictive performance across various k. note that a correct suggestion at rank yields an mrr of at rank .
at rank .
.
thus a 1079smalldifferenceinmrrcouldindicatealargechangeintheranked list especially for higher mrr values.
codecompletionscenarios.
weusethreescenariosfromprevious work eachstatic dynamic and maintenance settings simulates a different way of incorporating nlms in an ide.
the task is always to predict test set tokens but the training sets differ static tests.
the model is trained on a fixed training corpus and later evaluated on a separate test dataset.
this is a cross project setting train validation andtestssetsallcontainseparateprojects.
this simulates a single global lm that is trained on a large corpus of projects and then deployed to clients without adaption.
dynamic tests.
in addition to the training set the model can updateitsparameters afterithasmadepredictionsonfilesinthe testset itnevertrainsontestdata .ournlmsareadaptedusingthe proceduredescribedinsection5.
.aftereachproject werestore the model to the global lm learned from the train set only.
this simulates a setting in which some files from the project of interest are available for dynamic adaptation.
softwaremaintenancetests.
thisscenarioisevenclosertoreal worldusage simulatingeverydaydevelopmentwhereprogrammers make small changes to existing code.
the lms are tested on one fileatatimeinthetestset.foreachtestfile f thetrainsetplusall otherfilesinthetestprojectexcept fisusedastrainingdata.as thisrequiresretrainingthenlmonceperfileinthetestset this scenario was previously deemed infeasible for nlms in .
identifiersonly.
recentworkobservedthatlmsforcompletion perform worse on identifiers than other tokens .
therefore we also report model performance i.e.
entropy and mrr on identifier tokensonly excludingprimitivetypes .toclarifydifferencesbetweenmethods wealsoreport recallatrank1 r thepercentage of all identifier usages which are correctly predicted at rank and similarlyrecallatrank10 r thepercentagewhenthecorrect identifier appears anywhere in the model s top predictions.
research questions rq1.
how does the performance of subword unit nlms compare to state of the artlmsforcode?
wecomparesubwordunitnlmsto standardn gram lms cache lms state of the art n gram lmswithnestedcaching token levelnlms andheuristic splitting nlms .
we do not compare with phog and pointernetworkrnns bothdonothaveafullimplementation available.
we do not evaluate character level nlms as they have not shown benefits for nlp.
rq2.cansubwordunitnlmsscaletolargecodecorpora?doesthe additional training data improve performance?
training on a larger corpus may improve a model s performance but adding more data tends to have diminishing returns.
after some point a model s performancesaturates.weevaluateifnlmscanmakebetteruse of large corpora than n gram models.
moreover training on larger data uses introduces scaling issues.
thus performance in terms of runtime cost memory usage and storage becomes important.
rq3.howdoestheperformanceofsubwordunitnlmsvaryacross programming languages?
in principle the learning methods for nlms are language agnostic however the majority of studies evaluate only on java.
we check if code lms are equally effective onotherprogramminglanguages c sterseness orpython slackof type information could negatively impact an lm s performance.
rq4.isthedynamicupdatingeffectivetoadaptsubwordunitnlms to new projects?
new projects introduce many new identifiers that do not appear even in a large cross project corpus.
an n gram lm can exploit the strong locality that characterises code through caching .
thus we ask whether nlms can also benefit from dynamic adaptation via the procedure presented in section .
.
we compare our dynamic adaption technique against two dynamic n gram models cache lms and nested cache lms .
rq5.
are nlms useful beyond code completion?
nlms in nlp have shown to be useful in a variety of tasks including translation orsummarization theyhavebeenrecentlyshowntobestateofthe artintransferlearning.whiletestingallofthesescenariosvastly exceeds the scope of this paper we test whether nlms improve upon n gram lms in the task of detecting buggy code .
results table presents the evaluation metrics of all scenarios we refer to it continuously.
we used the n gram implementation7u s edi n withthesameparameters n allnlmsareours.wecompute mrr on the first million tokens of the test set as in .
.
rq1.
performance of models because the full data set is so large we compare the different variantsofn grammodelsagainsteachotheronthesmalljavatraining set and then we compare the best n gram lm against our bpe nlmonthelargejavadataset.intable2 weseethatthenested cache model has the best performance of the n gram models with alargeimprovementoverthesimplermodels forexample improving mrr from to on java against the basic n gram model .
this is consistent with the results of .
however our bpe nlm outperformsit.
notethatcachemodelscannotbeevaluatedinthe static scenario since the cache would adapt to the test set .
moving tothelargedataset wefindthatthebpenlmstilloutperforms thenestedcachemodel eventhoughthenestedcachemodelwas specifically designed for code.
while previous work found thatclosednlmsunderperformedonidentifiers wefindthatour bpenlmsdonot.inthedynamicscenario ofidentifiersare predictedwithinthetop10predictions withuptonearly56 in first position.
openvsclosedvocabulary.
tospecificallyevaluatetheeffectof relaxing the closed vocabulary assumption we compare our open vocabulary nlm to two closed vocabulary nlms one that uses full tokens closed nlm and another that splits tokens according to conventions heuristic nlm .
those models have otherwise the same architecture as the open vocabulary.
in both cases we find that the open vocabulary nlm significantly outperforms both closed vocabulary nlms and can be trained even in the maintenance setting unlike the closed versions.
of note our closed vocabulary nlm performs better than the one in as it utilizes afullyconnectedhiddenlayeranddropout.finally intable3we reporttheperformanceoftheopenvocabularynlmswithdifferent 6a naive approach to the software maintenance scenario retrains the model from scratch for every test file which was rightly deemed infeasible for nlms by version .
1080table performance of the various models bold best underlined second best .
modeljava java identifiers c python static dynamic maintenance bugs dynamic static dynamic static dynamic entmrr ent mrr ent mrr ent r r mrr ent mrr ent mrr ent mrr ent mrr smalltrain n gram .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nested .
.
.
.
.
.
.
.
.
.
.
cache .
.
.
.
.
.
.
.
.
.
.
nested cache .
.
.
.
.
.
.
.
.
.
.
closed nlm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
heuristic nlm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bpe nlm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bpenlm cache .
.
.
.
bpe nlm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bpe nlm cache .
.
.
.
largetrain nested cache .
.
.
.
.
.
.
.
.
.
.
bpe nlm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bpenlm cache .
.
.
.
bpe nlm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bpenlm cache .
.
.
.
table3 effectofvocabularysizeonjavaperformanceofour open vocabulary models python and c are similar .
vocab sizestatic dynamic maint.
bugs ent mrr ent mrr ent mrr ent small train .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
large train .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
vocabularysizes obtainedafter2000 and10000bpemerge operations.
we see that performance on the small training set is similar across vocabulary sizes a large vocabulary is not required for good performance.
caches andlargercapacity.
bothourcacheandincreasingmodel capacity from to features are beneficial particularly for the identifiers.
the cache improves mrr by to with moreimprovements for low ranks which is especially important for completion.onthesmallcorpus thelargemodelimprovesmrr bynearly3 asmallerimprovementthanaddingthecache.both improvements are complementary increasing identifier mrr by close to .
openvocabularynlmsareeffectivemodelsofsourcecode even on a small corpus yielding state of the art performance.
.
rq2.
large corpora we contrast performance between small and large training sets.
leveraging data.
when trained on larger corpora the performance of n gram models including nested cache variants gets saturatedandtheyareunabletoeffectivelyleveragetheextrainfor mation .incontrast ourmodelcanbetterleveragetheincrease intrainingdatawhentrainedonthefullcorpus.inthestaticscenario our nlms decrease entropy by about .
bits while mrr increases by about .
more data helps our nlms learn to synthesize identifiers from subwords better and with higher confidence.
theimprovementsaresmallerbutstillexistwhenthenlmsuse dynamic adaptation for all encoding sizes the entropy improvesby .
bits and mrr by to .
in contrast the nested cache ngram model entropy improves by less than .
bits and mrr by lessthan0.
.fromthatweconcludethatsubwordunitnlmscan utilizealargecodecorpusbetterthan n grammodels.asshown in table larger training corpora tend to favor nlms with larger vocabularies particularly in terms of mrr larger models leverage theadditionaldataevenbetter.forallmodels theimprovements are more visible for identifiers the large train alone contributes closeto7 ofmrrforidentifiers versus3 overallforthenlm.
finally largernlms 2048features areevenbetteratleveragingthe additionaltrainingdata due totheirincreasedcapacity.
similarly thecachestillimprovesperformancefurther evenwiththelarge training set both improvements complement each other.
resource usage.
while the nested cache n gram model is competitive with java identifiers this comes at a significant cost resource usage.
disk usage for n gram models range from to mb in the small training set to 6to .5gb in the large training set.
ram usageisevenmoreproblematic asitrangesfromaround5gbin the small training set up to to 60gb in the large training set.
1081this makes the large n gram models unusable in practice as they exceed the memory requirements of most machines.
incontrast thenlmsdonotvarysignificantlywithtrainingset size theirsizeisfixed.theyrangefrom15mb bpe2k to45mb bpe 10k on disk up to 240mb for the large capacity models .
ramusagefornlmsvarybetween2to4gbwhentraining and canbereducedattheexpense ofspeedbyre ducingbatchsize and is considerably lower at inference time for actual code completion rangingfrom250to400mb.
thus ifwecomparepractically applicablemodels thesmallnlmoutperformsthesmallnestedcache n grammodelbyupto5.
inidentifiermrr andupto5.
recall at the large nlm does so by .
mrr and .
recall at .
the open vocabulary makes training nlms on large corpora scalable as vocabulary ceases to grow with corpus size training time scales linearly with added data.
our largest nlm bpe 10k features can process around to hundred thousandtokens per minute roughly to projects per hour depend ing on project size on a consumer grade gpu.
this makes ourdynamic adaptation procedure which trains one project for oneepoch clearly feasible.
training the initial model is still a large upfront cost but it takes from a day small nlm up to two weeks large nlm on our largest dataset and needs to be performed once.
at inference time predicting tokens with beam search takesafractionofasecond fastenoughforactualuseinanide evenwithoutadditionaloptimization.thisisnottruefortheclosed models.
open vocabularynlmscanscale furthermore theyleveragethe increasedtrainingdataeffectively.large n grammodelsdo not scale in terms of resources.
.
rq3.
multiple languages we contrast java performance with python and c. we see interesting differences between java python and c. first n gram models performconsiderablyworseinpython whilenlmsdoverywell.
wehypothesizethatthisisduetothesmallersizeofpythonprojects in our corpus which reduces opportunity for caching the average pythonprojectis2to3timessmallerthantheaveragejavaproject .
cprojects ontheotherhand arecompetitivewithjavaprojects particularly with caching they are on average times larger.
in terestingly the nested and nested cache n gram models performcomparativelyworseincthaninjava cprojectstendtohaveaflatter structure rendering the nesting assumption less effectivein this case.
finally the not applicable in practice large n grammodel outperforms our nlms for c. we observed anectodal evidence that there is considerable duplication in the c corpus which may affect this result .
for nlms the performance is more even across the board with overall slightly worse performance for c and somewhat better performance for python.
our nlm performance results hold for java c and python.
.
rq4.
dynamic adaptation we evaluate the effectiveness of our proposed method for adaption ofnlmsinthedynamicandmaintenancescenarios.thisiscrucialforpracticalusageofnlms becausethedynamicandmaintenancescenariossimulate thesettingwhere thedeveloper ismodifyinga large existingproject.usingwithin projectdataprovidesalarge performanceboost eventhoughwithineachscenario ournlms outperform n grams most n gram models in the dynamic scenario outperformnlmsinthestaticscenario.theimprovementdueto dynamic adaptation is greater than the improvement due to an nlm.ofnote thesituationinthelargetrainingsetisdifferent the staticlargenlmtrainedonthelargetrainingset outperformsthe cachen gramlmsinthedynamicscenario andiscompetitivewith it in the maintenance scenario in other words our large data set is so large that it almostmakes up for not having within project data but within project information is clearly still crucial.
once we apply the dynamic adaptation method to the nlms the picture changes.
with dynamic adaptation our model achieves better cross entropy than the current state of the art making it an effective technique to fine tune an nlm on a specific project.
using this method it is even possible to evaluate nlms on the maintenance scenario which was previously deemed infeasible by sincemultiplemodelshadtobecreated eachtrainedonthe entiretyofthetestsetminusonefile.thisispossibleforusbecause the combination of a small vocabulary size and our finetuning methodrunningforonlyoneepochmakethisscenariomuchfaster.
openvsclosednlms.
interestingly thedifferenceinperformance between the open and closed vocabulary nlms is larger in thedynamic setting.
we hypothesize that dynamic adaptation helps the open vocabulary model to learn project specific patterns about oov words this is not possible for a closed vocabulary nlm.
dynamic adaptation for nlmsyields the state of the art static nlmsarecompetitivewithsomedynamic n grammodels which bodes well for transfer learning.
.
rq5.
bug detection previous work hasobserved that n gram language modelscan detect defects as they are less natural than correct code .
in short defectivelinesofcodehaveahighercross entropythantheircorrectcounterparts.toassesswhetherourcodenlmisapplicable beyond code completion we compare the ability of different language models to differentiate between the two on the well known defects4j dataset .
defects4j contains real world defects from5systems.bothabuggyandacorrectedversionofthesystem areprovidedandthechangedlinescanbeextracted.wecompute the difference in entropy between the buggy and the fixed version foreachofthediffpatchesprovided.theextractedcodesnippets usuallycontainsafewunchangedsurroundinglinesthatprovide useful context for the lms.
we expect a better lm to have a larger entropydifferencebetweenthedefectiveandthecorrectedversion.
we compute these metrics only for lms in a static setting for three reasons we simulated the setting inwhich a bug detector istrainedononesetofprojectsandusedonunseenones itisnot clear how caches would be used in this scenario should the lm know whichfileabugisin?
and3 training two lms for each defect which is very expensive.
the results are shown in the java bugs column in tables and .
as we hypothesized open vocabulary nlms feature a larger entropy drop for clean files than n gram lms or closed nlms.
the 1082dropinentropyis70 to100 forthesmalltrainingset depending onvocabularysizeandmodelcapacity largerisbetter .furthermore thesemodelsbenefitfromalargetrainingset withalarger drop of to .
we hypothesize that beyond data sparsity for identifiers thenlm slongrangedependenciesareespeciallyuseful in this task.
open vocabularynlmarebetterbugdetectorsthan n gramlms particularly when trained on large corpora.
conclusions sourcecodehasacriticaldifferencewithnaturallanguage developers can arbitrarily create new words greatly increasing vocabulary.
thisisagreatobstaclefor closed vocabulary nlms whichdonot scale to large source code corpora.
we first extensively studied vocabularymodellingchoices andshowedthattheonlyviableoption isanopen vocabulary nlm allothervocabularychoicesresultin large vocabularies high oov rates and rare words.
wethenpresentedanewopen vocabularynlmforsourcecode.
bydefiningthemodelonsubwordunits whicharecharactersubse quences of tokens the model is able to handle identifiers unseen intrainingwhileshrinkingvocabularyby threeordersofmagnitude.as aconsequence ournlmcanscaletoverylargecorpora wetrained itondatasetsovera hundredtimeslarger thanhadbeenusedfor previous code nlms.
our nlm also uses beam search dynamic adaptation and cachingto efficiently generate tokens and adapt to newprojects.finally weshowedthatournlmoutperformsrecent state of the artmodelsbasedonaddingnestedcachesto n gram language models for code completion and bug detection tasks in a variety of scenarios and in three programming languages.
of course this study has limitations while we tried to be exhaustive andevaluated a largenumber of scenarios we could not evaluate all the possible combinations hundreds due to the resources needed such as some large models or some large training scenarios.
for this reason we also refrained to evaluate other nlm architectures such as lstms qrnns transformers oradditionalneuralcachevariants .forthesamereason as in we alsolimited mrrto 1million tokens which maycause discrepancies with entropy metrics as they are not evaluated on thesametestset.wealsolimitedourselvestothreelanguages and did not fully evaluate the impact of code duplication .
wealsohopethatthesimplicityandscalabilitywillenablelarge capacitymodelsforcode andthetransferlearningopportunities they bring this has been explored in software engineering albeit not for source code .
improved language models for code havethe potential toenable new toolsfor aiding codereadability programrepair programsynthesis and translation between programming languages .
finally the technique of using subword units is not limited to language modeling but can easily be incorporated into any neural model of code suchasmodelstosuggestreadablenames summarizing sourcecode predictingbugs detectingcodeclones comment generation and variable de obfuscation .table of artifacts used or produced by this work artifact corpus c corpus python corpus java pre processed c pre processed pre processed codeprep openvocabcodenlm trained models artifacts several artifactswere used to conduct this study data source code andmodels.toimprovereplicationofthiswork thespecificversionofeachartifactusedinthisstudycanbereferencedviaa lists the each artifact.
this paper can be referenced when any of these artifacts is used.
datasets.
the datasets described in were published in previous work thejavacorpuswasproducedbyallamanis etal.
andalso usedin .theccorpuswasminedin andthepythoncorpus wasminedin .weusetherawdatasetsforthevocabularystudy butpreprocessthemfornlmtraining.further wedefinedtraining andtestsetsforthecandpythoncorpora anddefinedthelarge training set for the java corpus.
source code.
we implemented the codepreplibrary that supports a variety of pre processing options for source code.
we used codepreptogather thevocabularystatistics presentedinsection .researchers that wish to use the library to pre process source codefor their own study can find the library at giganticode codeprep.
the open vocabulary language model described in as well as the scripts implementing the training procedure and the evaluationscenariosareavailableinthe openvocabcodenlm library.
researchers wishing to extend our model can find it on github at models.themodelsthatweretrainedandevaluatedinsection 8arealsomadeavailableforfurtheruse.eachmodelwastrained ongpusforperiodsrangingfromafewhours uptotwoweeks.
these models can be used as is for inference in a code completion scenario.alternatively theymaybefine tunedforothertasks such as classification .