operation is the hardest teacher estimating dnn accuracy looking for mispredictions antonio guerriero roberto pietrantuono stefano russo dieti universit a degli studi di napoli federico ii via claudio napoli italy fantonio.guerriero roberto.pietrantuono stefano.russo g unina.it abstract deep neural networks dnn are typically tested for accuracy relying on a set of unlabelled real world data operational dataset from which a subset is selected manually labelled and used as test suite.
this subset is required to be small due to manual labelling cost yet to faithfully represent the operational context with the resulting test suite containing roughly the same proportion of examples causing misprediction i.e.
failing test cases as the operational dataset.
however while testing to estimate accuracy it is desirable to also learn as much as possible from the failing tests in the operational dataset since they inform about possible bugs of the dnn.
a smart sampling strategy may allow to intentionally include in the test suite many examples causing misprediction thus providing this way more valuable inputs for dnn improvement while preserving the ability to get trustworthy unbiased estimates.
this paper presents a test selection technique deepest that actively looks for failing test cases in the operational dataset of a dnn with the goal of assessing the dnn expected accuracy by a small and informative test suite namely with a high number of mispredictions for subsequent dnn improvement.
experiments with five subjects combining four dnn models and three datasets are described.
the results show that deepest provides dnn accuracy estimates with precision close to and often better than those of existing sampling based dnn testing techniques while detecting from to times more mispredictions with the same test suite size.
index terms software testing artificial neural networks i. i ntroduction deep neural networks dnn are today integral part of many applications including safety critical software systems such as in the medical and autonomous driving domains .
testing is a crucial activity in the development of such systems for both quality safety reasons to avoid dnn caused catastrophic failures and for cost as well very large samples may be needed to reliably test a dnn .
a significant research effort is currently being put on dnn testing.
a primary goal is to find adversarial examples causing mispredictions namely to expose as many failing behaviours as possible .
several structural coverage criteria have been proposed to drive the automated generation of test inputs and assess the test adequacy neuron coverage kmultisection neuron coverage neuron boundary coverage combinatorial coverage .
it has been argued that these criteria may be misleading because of the low correlation between the number of misclassified inputs in a test set and its this work has been supported by the project cosmic of unina dieti.coverage .
wu et al.
and kim et al.
recently considered discrepancy measures between the training validation data and test data to improve fault detection and to have coverage criteria better correlated to failure inducing inputs.
the output of this type of failure finding testing and then debugging process is an improved dnn with higher accuracy.
this resembles what is called debug testing in the traditional testing literature .
beside differences in testing dnn based and conventional software e.g.
the oracle definition which make dnn testing problematic a further issue is that the so obtained testing results are not necessarily related to the accuracy actually experienced in operation.
in fact testing data may be not representative of the actual operational context .
this may happen when test data are generated artificially like in adversarial examples generation or they differ significantly from input observed in the field.
the number of mispredictions and or the coverage achieved give only an indirect and for what said inaccurate measure of the expected accuracy in operation and ultimately of the confidence that can be placed in the dnn.
a less investigated research path is testing a dnn with the explicit goal of providing a statistical estimate of its expected accuracy in the operational context.
in software reliability engineering this is a well established practice known as operational testing .
the objective is to assess how well a dnn will perform in the intended context using a small yet effective amount of test data.
with a cost effective accuracy estimate testers can establish a threshold based release criterion and correct or tune their artificial networks e.g.
adjust the dnn structure or hyper parameters until the criterion is met.
the reference scenario is the following a dnn model is trained with a set of data training dataset and is meant to operate in a given context operational context .
to test the model an arbitrarily large set of operational data is available operational dataset containing examples whose correct label is unknown the goal of the tester is to select a small subset of the operational data to be manually labelled and used as test cases to estimate the accuracy of the model in operation.
due to manual labelling testers typically look for a trade off between a good estimate and the labelling cost.
this problem has been recently faced by li et al.
.
their selection criterion is to minimise cross entropy between selected test data and the whole set of unlabelled operational data so as to have a sample of tests representative of the operational context.
ieee acm 43rd international conference on software engineering icse .
ieee as this approach does not explicitly look for failing examples it does not give much room for improving the dnn accuracy.
from this viewpoint many tests selected are useless as they are non failing examples that serve only the purpose of having highly confident estimates.
given the above we either have a test suite exposing failures but not representative adversarial like testing or representative tests but few failures exposed operational testing .
this paper targets the drawbacks of both strategies together.
we present deepest deep neural networks enhanced sampler for operational testing an operational testing method for dnn that builds test suites from an operational dataset so as to provide a close and efficient estimate of the expected accuracy and to find at the same time a high number of failing examples.
paraphrasing a famous aphorism by o. wilde experience is the hardest kind of teacher.
it gives you the testfirst and the lesson afterward.
deepest aims to sample from operational data many failing tests to learn from while building a set of tests suited for the dnn accuracy estimate.
with respect to pure operational testing deepest is expected to provide dnn accuracy estimates that are more precise and more efficient for number of tests required plus the practical advantage of enabling debug testing like scenarios improving accuracy through debugging re training .
with respect to adversarial like testing deepest is able to provide accuracy estimates enabling evaluation of alternative designs or dnn fine tuning and establishing a release criterion directly related to what will be observed in operation.
experiments with various dnn and datasets are presented which assess deepest effectiveness sensitivity to the number of tests to select from the operational dataset and dependency on the dataset.
as deepest can exploit various types of auxiliary information to select tests the experimental study considers four variants.
the results show that all the variants produce accuracy estimates similar to those of the state ofthe art technique while detecting from to times more mispredictions with the same sample size.
the paper is structured as follows.
section ii introdcues sampling based operational testing concepts used by deepest.
section iii describes the deepest algorithm.
sections iv and v report the experimentation.
section vi discusses related work.
section vii concludes the paper.
ii.
s ampling based operational testing of dnn s a. operational testing of dnns in the traditional testing literature operational testing refers to the family of techniques that use an operational profile to test a system to estimate its expected reliability i.e.
probability of not failing in operation.
likewise the primary goal of dnn operational testing is to estimate the expected accuracy i.e.
probability of not having mispredictions in a given operational context .1two main challenges arise.
data skew .
the idea of operational testing is that testing should not just care about exposing possible failures but 1we use the terms misprediction and failure interchangeably for dnn.should be able to spot those failure causing inputs that are more likely to occur in operation.
in case of a relevant mismatch between the pre release test data and the post release context the system could be stimulated in operation with inputs never seen during testing with unexpected failures.
data skew is a concern for dnn more than for traditional software systems.
these are expected to work on a range of input data given to functionalities and can be tested on a small carefully selected sample of input data e.g.
obtained by input space partitioning .
dnn are data driven by nature they are constructed around a training dataset and generalising beyond what observed during training is hard.
this data driven approach is governed by a statistical process and due also of the black box nature of dnn it is tricky to identify classes of inputs that homogeneously represent the expected behaviour in operation.
for instance white box partitioning has been shown to be not clearly correlated to the failing behaviour .
thus a drift of the post release operational context from the pre release testing context is more likely to cause unexpected failures compared to traditional software.
the imitation bias of operational testing.
the operational context drift is just a triggering condition for unexpected failures at runtime.
the problem occurs because of the way in which operational testing is conducted.
operational testing selects a small data sample that can accurately represent the population however the mere imitation of the expected input can be inefficient especially in highly reliable systems because many failure free tests are executed to get an acceptable estimate.
a representative sample would roughly contain the same proportion of examples causing misprediction as the operational dataset.
there are two problems with this first in highly accurate dnns the number of examples causing mispredictions is low thus requiring other types of testing activities dedicated to detect mispredictions e.g.
through adversarial like techniques to possibly improve the accuracy.
second just mimicking the expected usage is fine from the estimation point of view as long as the imitation is faithful.
if this is not the case the risk of overestimation increases if we only aim at having a representative sample of tests the actual experienced accuracy may be significantly smaller than the estimated one if the operational context drifts because of the occurrence of unexpected mispredictions.
thus a conventional approach for dnn operational testing allows to obtain the desired accuracy estimate but since it reveals few mispredictions can be ineffective leading to wrong estimates and inefficient requiring further separate testing activities to detect mispredictions .
recent results in operational testing for traditional software show that exposing failures and estimating reliability are not contrasting objectives .
a strategy that actively looks for failures rather than just mimicking the expected usage can lead to accurate and stable estimates and at the same time expose many failures.
our aim is exactly to spot failing examples in a dnn operational dataset while preserving the ability to yield effective small error and efficient low variance estimates.
349b.
sampling based testing deepest uses statistical sampling a natural way to cope with estimation problems it serves to design sampling plans tailored for a population under study providing effective and efficient estimators.
in sampling based testing the sample is the set of ntest casest ft1 tng having a binary outcome pass fail .
test outcomes are a series of independent bernoulli random variables ztisuch thatzti 1if the dnn predicts the correct label for ti zti 0otherwise.
the parameter of our interest is the dnn accuracy we aim to an estimate with two desirable properties unbiasedness i.e.
the expectation of the estimate e should be equal to the true value and efficiency for the given the sample size the variance of should be as low as possible for a highly confident stable estimate .
the probability that zti corresponds to the true unknown proportion pn t 1zti n withnbeing the population size i.e.
the size of the operational dataset .
simple random sampling with replacement srswr is the baseline approach an unbiased estimator of is the observed proportion of correct predictions over the number of trials n srswr pn t 1zti n having assumed independent variables the variance of is v srswr n an improvement is represented by simple random sampling without replacement srswor namely the same test case is not selected twice this reduces the variance to v srswor n n n n while srs keeps the mathematical treatment simple it is unable to exploit additional information a tester might have.
exploiting auxiliary information to modify the sampling scheme is what is done in sampling theory to get more efficient estimators .
the sampling is made proportionally to an auxiliary observable variable assumed to be related to the unknown quantity to estimate the estimator is then adjusted to account for the non uniform sampling so as to preserve unbiasedness.
for instance stratified sampling is a strategy which uses knowledge about which sample units are expected to have homogeneous values and selects units contributing more to lower the estimate s variance.
liet al.
present two sampling strategies for dnn which are to our knowledge the only attempt to dnn operational testing confidence based stratified sampling css and cross entropy based sampling ces the latter being the authors proposal.
both strategies exploit auxiliary information to drive the sampling task.
in css sampling is proportional to the confidence value provided by classifiers when predicting a label examples with higher confidence are more likely to be selected as part of the test suite.
this works well when the classifier is reliable namely if examples with higher confidence are actually thosefor which the prediction is more likely to be correct in other words the model is perfectly trained for the operation context .
whenever the operation context drifts from the training one css exhibits poor performance.
ces attempts to overcome this limitation by using the output of the mneurons in the last hidden layer in the last hidden layer assumed to be more robust to the operation context drift.
it builds the test suite trying to minimize the average cross entropy between the probability distribution of them dimensional representation of the output of neurons computed on the operational dataset and on the selected tests.
to pursue the double objective of sampling cases causing mispredictions and estimating accuracy efficiently deepest adopts an adaptive and without replacement sampling algorithm described in section iii.
c. auxiliary information to look for mispredictions the auxiliary information leveraged by deepest adaptive sampling should represent the belief about some factor s related to the model s in accuracy.
we consider two auxiliary variables related to somehow opposite sources of information the confidence value and thedistance between the operational dataset example and the training dataset.
the latter is based on the result by kim et al.
that inputs more distant from training data are more likely to cause misprediction they show that distance is correlated to mispredictions which is what we look for.
we borrow their distance metrics distance based surprise adequacy dsa and likelihood based surprise adequacy lsa .
these are computed by the activation trace at namely a vector of activation values of each neuron of a certain layer corresponding to a certain input we compute at with reference to the last dnn activation layer.
lsa is defined as the negative log of density computed via kernel density estimation .
dsa is defined as the euclidean distance between the at of a new input and ats observed during training.
the variant using confidence is deepest cs the variants using the distance metrics are deepest dsa and deepest lsa.
performance of an auxiliary variable can strongly depend on the dnn and on the training operation dataset for instance for a distance metric the belief that examples far from the one in the training set are more likely to cause a misprediction is not an absolute truth.
in particular if an example is very similar to many others in the training set according to the distance metric but has a different label distance will be not a good metric to select it.
similarly the confidence is a good proxy ifthe dnn is well trained for the operational context.
in general relying on a single auxiliary variable may work well in some settings and bad in others.
combining multiple beliefs is a choice that is expected to improve the stability of results across multiple settings.
based on this we define a further variant of our algorithm named deepest c considering as auxiliary variable the combination of confidence and distance p pc pd 350wherepcis the confidence value pdis the dsa value normalized 2andpis probability of correct prediction.
the intuition behind is that the probability of a correct prediction is related to both the confidence of the dnn and the drift of the example from what seen during training.
in fact pcis related to the probability that the dnn does a correct prediction according to what seen during training in other words it is the probability of correct prediction with perfect training pdis a proxy for the probability of wrong prediction related to how far the example is from what seen during training hence due to the imperfection of training.
if confidence pcis high andthe example is close to the training dataset i.e.
pd is small there is a high chance of correct prediction.
iii.
t hedeepest algorithm both ces and css select a sample of tests representative of the operational context.
deepest takes a different approach favouring the sampling of failing namely mispredictioncausing tests then used to estimate the dnn accuracy.
while the estimation ability demands for probabilistic hence sampling based selection of examples the very idea of deepest is that since mispredictions are usually rare compared to correct examples looking for failing tests is well handled by adaptive sampling the examples progressively selected may give hints about the probability of finding other failing examples so as to adapt the sampling process accordingly.
given a samples size adaptive sampling implicitly assumes that inputs of interest are not uniformly spread across the input space and adopts a disproportional selection to spot them counterbalanced by the estimator to preserve unbiasedness.
in software testing this is expected to give smaller variance compared to conventional sampling e.g.
srs since the failing inputs are usually not uniformly distributed.
the deepest sampling algorithm is inspired to the adaptive web sampling a flexible design for sampling rare populations.
the above auxiliary variables are used to define a weight wi jbetween any pair of examples iandjof the operational dataset used to explore the example space adaptively.
for instance let us assume to use the normalised dsa distance if the example ihas distance pdi representing the belief that icauses a misprediction and a pre defined threshold is exceeded i.e.
ihas a sufficiently high distance compared to the others then all the wi jvalues 8jof the operational dataset are set to their distance pdj otherwise wi j .
this way a strong enough belief about example icausing misprediction entails the activation of all the weights toward i. the latter are used as explained hereafter for sampling and makes the algorithm follow the distance criterion to spot potential clusters of failing examples.
the deepest sampling strategy is sketched in algorithm .
assuming nexamples to be selected from the operational dataset as test cases the algorithm selects and executes one test case per step.
the first input is selected by simple random 2in deepest c dsa is preferred to lsa since it has been shown to have better performance for the deeper layer .algorithm deepest adaptive sampling data od operational dataset i current sample t test suite n number of tests r probability of wbs 1t 2i srs sampling od first sample by srs 3od odni remove sample from dataset 4t t i add to suite 5y1 labelling and test execution i 6fork k n k do 7rs random ifrs r then 9i wbs sampling od od odni else 11i srs sampling od od odni 12t t i 13yi labelling and test execution i 14zk equation equation compute final estimate sampling namely initially all examples have equal probability of being selected.
then one of two sampling schemes is used to select next test weight based sampling wbs or simple random sampling srs .
example iis selected from the operational dataset at step kwith probability qk igiven by qk i r p j2skwi jp h 2sk j2skwh j r n nsk where r probability of using wbs hence probability of using srs r ris set to .
in our implementation sk current sample all examples selected up to step k wi j weight relating example jinskto examplei nsk size of the current sample sk n size of the operational dataset.
for both wbs and srs the selection is without replacement .
wbs selects an example iproportionally to the sum of weights wi jof already selected examples toward i the chance of takingidepends on the current sample favouring the identification of clusters of failing examples ifthe auxiliary variable hence wi j is well correlated with mispredictions.
as this is not always the case the wbs depth exploration is balanced by srs chosen with probability r for a breadth exploration of the example space.
this diversification in the search is useful to escape from unproductive cluster searches.
the steps are repeated until the testing budget n nis over.
at step the probability that a randomly selected example will cause a misprediction is estimated as the outcome y1of the first test in case of misprediction otherwise .
3without replacement sampling schemes generally give smaller variance than their with replacement counterpart on the same sample size .
351table i experimental dnn and datasets subject training test true dnn dataset classes set size set size accuracy s1 cn5mnist .
s2 lenet5 .
s3 cn12cifar10 .
s4 vgg16 .
s5 vgg16 cifar100 .
at stepk examplei whose outcome is yi is selected with probability qk iaccording to eq.
and the estimator of the misprediction probability is that by hansen hurwitz zk n x j2skyj yi qk i where theyjvalues are the outcome of the tests already selected.zkis an unbiased estimator of the expected misprediction probability at step k the final estimator of the expected accuracy of the dnn is minus the average of the zkvalues n y1 nx k 2zk wherenis the number of tests run.
iv.
e valuation a. experimental subjects the four variants of deepest are evaluated against the srswr scheme as baseline and the mentioned state of theart technique ces.
five experiments are conducted with four dnn models and three popular datasets.
the datasets are mnist a dataset of handwritten digits cifar10 for image processing systems and cifar100 similar to the previous one but with classes .
the chosen dnn models are convnet5 here simply cn5 and lenet5 for mnist classification convnet12 simply cn12 and vgg16 for cifar10 classification vgg16 for cifar100.
table i lists the five subjects dnn dataset pairs the true accuracy is in the last column.
b. research questions and experiment design the evaluation answers the following research questions.
rq1 effectiveness .how does deepest perform in finding inputs causing misprediction i.e.
failing examples and simultaneously estimating a dnn operational accuracy?
to gauge deepest ability to provide effective dnn accuracy estimates with few examples while spotting a high number of failing inputs we set to the number of tests to select then varied to answer rq2 and repeat times the execution of the compared techniques on the subjects.
as for evaluation metrics we compute 4cn5 and cn12 are calibrated in the same way as li et al.
lenet5 is calibrated as kim et al.
for the vgg16 network we considered the weights at accuracy iat thei th repetition and then compute the mean squared error mse as mse 30p30 i i where is the true operational accuracy.
note that for unbiased estimators mse and variance can be considered indistinguishable.
in fact mse variance bias2andbias e .
the precision of the estimator is mse and the relative precision or relative efficiency of estimator a with respect to bis a b mse b mse a a b 1means thatais better than b .
the average number of failures mean i with i being the number of failures in repetition i. for comparison purpose we consider the relative number of failures of technique awith respect to b a b a b a b 1means thatais better than b .
rq2 sensitivity to sample size .how does the performance of deepest vary with the sample size?
it is important to figure out how performance varies with the number of test cases to select from the operational dataset namely with sample size.
indeed deepest aims to perform well especially with a small sample size so as to yield precise estimates with relatively few examples to be manually labelled.
rq3 dataset influence .how is deepest performance affected by the datasets?
in dnn testing results are often heavily dependent on the training and operational datasets.
this rq aims to figure out how these may influence the ability of the auxiliary variables confidence dsa lsa to discriminate failing examples affecting the performance of deepest.
to answer rq3 the test set is completely labeled so as to identify all failures.
c. implementation deepest is implemented mostly in java.5the implementation of the distance metric in deepest dsa and deepest lsa is the same used by kim et al.
we used their python scripts to compute dsa and lsa values.
these are computed considering the last activation layer of each dnn.
the threshold needed for the weights definition is set as follows deepestdsa mean dsa std dsa deepestlsa mean lsa var lsa .
the threshold for confidence used by deepest cs and deepestc is set to assuming that lower confidence values are more related to misprediction i.e.
the weights are activated when the confidence is less than .
in ces the selection exploits the output of the last hidden layer.
we use the same configuration as the original article .
the size of the initial sample is p enlarged by a groupq ofq 5examples at each step.
the number of random groups from which q is selected is l .
for ces and srs we used the python scripts provided by li et al.
.
a subject s1 cn5 mnist b subject s2 lenet5 mnist c subject s3 cn12 cifar10 d subject s4 vgg16 cifar10 e subject s5 vgg16 cifar100 fig.
rq1 effectiveness mean squared error of estimates table ii rq1 effectiveness mean and standard deviation of the number of failing examples detected subjectsrs ces deepest cs deepest dsa deepest lsa deepest c total failing examplesmean mean mean mean mean mean s1 cn5 mnist .
.
.
.
.
.
.
.
.
.
.
.
s2 lenet5 mnist .
.
.
.
.
.
.
.
.
.
.
.
s3 cn12 cifar10 .
.
.
.
.
.
.
.
.
.
.
.
s4 vgg16 cifar10 .
.
.
.
.
.
.
.
.
.
.
.
s5 vgg16 cifar100 .
.
.
.
.
.
.
.
.
.
.
.
v. r esults a. rq1 effectiveness figure plots the mse of the estimated accuracy.
the techniques exhibit comparable performances with deepest c being the best one for of the subjects.
ces has good performance in terms of mse it is the second technique in cases.
considering the single variables confidence dsa or lsa lead to results slightly more variable over the subjects an aspect explored in rq3.
the srs case is interesting too it is never the worst approach and is the best in one case.
table ii reports the average number and the standard deviation of the failing examples detected.
all variants of deepest identify many more failing examples than srs and ces even up to a factor of 30x deepest csvsces for subject s2 and reaching in some cases s1 and s2 deepest cs 5a replication package is at of the total number of failing examples in the datasets last column .
the deepest algorithm leverages the adaptive sampling to spot clusters of failing examples with relatively few tests set to for rq1 .
its performance varies depending on the auxiliary information used but it is always remarkably better than srs and ces.
among the deepest variants confidence deepest cs turns out to be the most effective auxiliary variable in detecting failures showing the best performance for all datasets and models followed by dsa.
ces and srs select the lowest number of mispredicted examples and are close to each other.
considering both the failure detection ability and the estimate accuracy deepest c that combines confidence and dsa the two best auxiliary variables for failing examples detection gives a good trade off since it provides stable across subjects and close to true estimates of the accuracy with many more detected failing examples than ces and srs.
353table iii pairwise comparison of techniques.
a value of r c 1means the technique on the row has a greater precision than that on the column.
similarly for the relative number of detected failures r c a subject s1 cn5 mnist deepest row vs col csdsalsac srs ces r c .
.
.
.
.
r c .
.
.
.
.8929deepestcs r c .
.
.
.
r c .
.
.
.
dsa r c .
.
.
r c .
.
.
lsa r c .
.
r c .
.
c r c .
r c .
b subject s2 lenet5 mnist deepest row vs col csdsalsac srs ces r c .
.
.
.
.
r c .
.
.
.
.4766deepestcs r c .
.
.
.
r c .
.
.
.
dsa r c .
.
.
r c .
.
.
lsa r c .
.
r c .
.
c r c .
r c .
c subject s3 cn12 cifar10 deepest row vs col csdsalsac srs ces r c .
.
.
.
.
r c .
.
.
.
.0522deepestcs r c .
.
.
.
r c .
.
.
.
dsa r c .
.
.
r c .
.
.
lsa r c .
.
r c .
.
c r c .
r c .
d subject s4 vgg16 cifar10 deepest row vs col csdsalsac srs ces r c .
.
.
.
.
r c .
.
.
.
.9106deepestcs r c .
.
.
.
r c .
.
.
.
dsa r c .
.
.
r c .
.
.
lsa r c .
.
r c .
.
c r c .
r c .
e subject s5 vgg16 cifar100 deepest row vs col csdsalsac srs ces r c .
.
.
.
.
r c .
.
.
.
.8323deepestcs r c .
.
.
.
r c .
.
.
.
dsa r c .
.
.
r c .
.
.
lsa r c .
.
r c .
.
c r c .
r c .
f number of wins and losses wins deepest total r vs c ces cs dsa lsac srs wins ces 25deepestcs dsa lsa c srs total cescsdsa lsac srs losses tables iii a e show the results of the pairwise comparison of the techniques.
for the four deepest variants rows and columns headings list in blue the name of the auxiliary variables.
the evaluation metrics are the ratio of the failing examples and the relative precision of the estimators.
values of or greater lower than mean the technique on the row column has better performance.
if a technique is better than the other in a pair for both metrics values coloured in the table we say that it wins.
table iii f summarizes the number of wins and losses .
ces never wins over other approaches while deepest cwins against ces out of times.
srs never wins over deepest while it wins vsces considering vgg16 on cifar100.
deepestcnever looses and collects the highest number of wins showing the best trade off among accuracy of the estimation and number of detected failing examples.the choice of the deepest variant may be determined by which auxiliary information can be collected.
we see that deepestc exploiting a combination of two variables has good and more stable results in terms of mse than the other variants at the expense of a slight decrease of detected failures.
single auxiliary variables are more sensitive to the specific dataset model pair e.g.
confidence works well if the dnn is reliable but expose more mispredictions.
confidence has the advantage of not requiring knowledge of the hidden layers and is easier to compute.
when no information is available or easily computable srs could be a good low cost solution.
b. rq2 sensitivity to sample size to answer this rq experiments are run with the sample sizes considering the subject with the highest accuracy s1 and the one with the lowest accuracy a subject s1 cn5 mnist b subject s5 vgg16 cifar100 fig.
rq2 sensitivity to sample size accuracy for the most a and the least b accurate subjects a subject s1 cn5 mnist b subject s5 vgg16 cifar100 fig.
rq2 sensitivity to sample size mse for the most a and the least b accurate subjects a subject s1 cn5 mnist b subject s5 vgg16 cifar100 fig.
rq2 sensitivity to sample size number of failing examples for the most a and the least b accurate subjects s5 so as to analyze how deepest performs when there are very few and many failing examples in the dataset respectively.
figure shows the mean values of the estimates accuracy over repetitions.
figures and plot the mse and the mean number of detected failures respectively.
expectedly increasing the sample size all techniques exhibit a decreasing trend in mse and an increasing trend in failing examples.
for the subject with highest accuracy s1 we observe the following.
deepest cshows very good performance for mse fig.
a fig.
and it detects on average about six times the failures of srs and ces table ii .
the advantage for mse is more pronounced with the smallest sample sizes which make deepestcparticularly suited when the number of examplesto select and label is very small.
for larger sample sizes the mse is similar but the advantage of deepest cover srs and ces is very pronounced for detected failures fig.
a .
deepestcsis the best among all techniques to detect failures for small sample sizes for sizes and800 the best is deepestdsa fig.
a .
although the estimates are unbiased hence they tend to the true value if we look at the mean estimates over the repetitions figure a ces shows bad performance with up to 200test cases.
srs works well with low budget its good results may be influenced by the very low number of failures repetitions show 0failures and accuracy.
it is interesting to note that in most cases ces and srs overestimate the true accuracy an undesired 355property especially for critical systems.
this is related to the low number of failures detected as discussed in section ii.
for the subject with lowest accuracy s5 ces and srs outperform deepest cfor small sample sizes 50and100 from 200to800 the estimation by ces starts diverging while srs and deepest keep good performance.
the tendency to overestimate the accuracy by ces and srs is confirmed.
performance in failing examples detection is always clearly in favour of all deepest variants.
performance in estimation accuracy is almost specular to what observed with the most accurate model.
a reason is that deepest is a sampling techniques particularly suited for rare populations which is not the case of s5.
as for the ability to detect failing examples confidence is the best auxiliary variable for deepest for subject s5 it presents the best values in all configurations.
c. rq3 dataset influence we have seen in the experiments for rq1 and rq2 that no single auxiliary variable performs best in all situations.
for instance we can consider the confidence a good auxiliary information for subject s1 and a bad choice for s5.
this may depend on several factors assuming a perfect training the confidence could be affected by a bias in the training set or with a perfect training set a wrong training phase e.g.
due to overfitting could generate mispredictions with high confidence.
in some cases the operational dataset could contain examples very similar i.e.
small distance to those in the training set but with a different label affecting the discriminative power of the dsa and lsa metrics.
to analyze how the three datasets influence the ability of auxiliary variables to discriminate failing examples impacting the performance observed in rq1 rq2 we consider the subjects s1 and s5 as for rq2 plus the vgg16 dnn for cifar10.
figures and show the logistic regression for the three datasets.
the curves fit the probabilities for the outcome fail pass to the three predictors confidence dsa and lsa.
consider mnist for which cn5 reaches the highest accuracy among all subjects the probability for a test to fail is very low for values of confidence between .
and figure .
this is clearly not the case for cifar10 figure and especially cifar figure .
in the latter there is a high chance of misprediction even with high values of confidence this could be due to a high skew between training and test data.
the discriminating power of dsa and lsa is clearly greater for mnist as the high slope of the s shaped curves in figures b c suggests compared to corresponding ones in figures and .
in this case it actually happens that the farthest examples have highest probability to be related to a failure with a sharp increase after dsa 9and lsa .
this means that if in operation there are a lot of examples far from what observed in training re training with a more representative dataset can be useful to improve the accuracy.
this behaviour is not observed for cifar10 and cifar100 and dsa and lsa do not seem to be effective in dividing the two sets of examples.
in cifar10 the dsa line is more horizontal meaning that the dsa value does not reflect wellthe failure probability.
the scarcely discriminative power of the auxiliary variables in cifar10 and cifar100 partially explains the smaller gain of deepest over ces and srs especially in terms of mse nevertheless its adaptivity allows spotting many failing examples even in these conditions.
finally it is interesting to highlight the performance of deepestcs based on confidence on mnist in rq2.
the saturation in its failure detection ability figure a can be explained observing that after a number of tests able to spot failures looking at low confidence examples the few remaining ones with high confidence are selected with low probability the sharper discrimination made by dsa and lsa determines a high detection ability even when few failing examples remain.
in summary whenever a tester has good belief evidence about the appropriateness of one of the above auxiliary variables it is a good choice to select the specific deepest variant if not the combined variant deepest chas shown to give the best trade offs in all five experimented cases.
d. threats to validity a threat to the internal validity comes from the selection of the experimental subjects.
to favor the repeatability of the experiments under different possibly influencing factors we have used publicly available networks and pre trained models to avoid incorrect implementation.
the configuration of parameter rin deepest and a different setting of thresholds may also affect the results in terms of efficiency of the estimator hence a fine tuning is suggested before applying the method to other dataset dnns.
although the code developed was carefully inspected a common threat is the correctness of the scripts to collect data and compute the results.
the choice of the sample size influences the effectiveness too.
we ran a sensitivity analysis to show that deepest is still effective compared to both ces and srs under five from to values of the sample size but different values could yield different results.
threats to external validity depend on both the number of models and datasets considered for experimentation.
we strived to control this threat considering different widely used dnn and datasets.
although the results may change with different subjects the diversity and significance of the chosen subjects give confidence to the general considerations.
replicability of the experiments on other subjects is to further mitigate this threat.
vi.
r elated work testing of dnn is a hot research topic.
zhang et al.
present a survey on machine learning testing identifying three main families of techniques mutation testing metamorphic testing and cross referencing .
the former two are for test generation they generate adversarial examples causing mispredictions.
cross referencing can highlight the most interesting test cases when different implementations of a system disagree.
most of these techniques are meant for fault detection rather than for accuracy estimate.
testing dnn for accuracy is the focus of li et al.
who presented ces which deepest has been compared against a confidence b dsa c lsa fig.
rq3 dataset influence mnist samples distribution a confidence b dsa c lsa fig.
rq3 dataset influence cifar10 samples distribution a confidence b dsa c lsa fig.
rq3 dataset influence cifar100 samples distribution as the first approach using operational testing to this aim.
deepest differs from ces in several aspects the key ones being the sampling algorithm and the used auxiliary variables which are conceived to improve failing examples detection while preserving the accuracy estimation power.
operational testing where tests are derived according to the operational profile is an established practice to estimate software reliability .
it was the core technique of cleanroom software engineering and of the software reliability engineering test process .
cai et al.developed adaptive testing still based on the operational profile but foreseeing adaptation in assigning tests to partitions .
recently pietrantuno et al.stressed the use of unequal probability sampling to improve the estimation efficiency to this aim formalizing several sampling schemes .
our proposal goes along this direction we do not sample tests representative of the operational dataset but we alter the selection and counterbalance the uneven selection in the estimator.
unequal probability without replacement and adaptive sampling are the key concepts we borrowed for operational testing of dnns.vii.
c onclusion testing the accuracy of a dnn with operational data aims at precise estimates with small test suites due to the cost for manually labelling the selected test cases.
this effort motivates to pursue also the goal of exposing many mispredictions with the same test suite so as to improve the dnn after testing.
with these two concurrent goals we presented deepest a technique to select failing tests from an operational dataset while ultimately yielding faithful estimates of the accuracy of a dnn under test.
we evaluated experimentally four variants of deepest based on various types of auxiliary information that its adaptive sampling strategy can leverage.
the results with four dnn models and three popular datasets show that all deepest variants provide accurate estimates compared to existing sampling based dnn testing techniques while generally much outperforming them in exposing mispredictions.
practitioners may choose the appropriate variant depending on the characteristics of their operational dataset and on which auxiliary information is available or they can collect.
357references ziad obermeyer and ezekiel j. emanuel.
predicting the future big data machine learning and clinical medicine.
new england journal of medicine .
pmid .
mariusz bojarski et al.
end to end learning for self driving cars.
arxiv .
.
amir efrati.
uber finds deadly accident likely caused by software set to ignore objects on road.
the information may .
jack stewart.
tesla s autopilot was involved in another deadly car crash.
.
available .
yuchi tian kexin pei suman jana and baishakhi ray.
deeptest automated testing of deep neural network driven autonomous cars.
in 40th international conference on software engineering icse pages .
acm .
yaniv taigman ming yang marc aurelio ranzato and lior wolf.
deepface closing the gap to human level performance in face verification.
in ieee conference on computer vision and pattern recognition cvpr pages .
ieee .
kexin pei yinzhi cao junfeng yang and suman jana.
deepxplore automated whitebox testing of deep learning systems.
communications of the acm .
lei ma felix juefei xu fuyuan zhang jiyuan sun minhui xue bo li chunyang chen ting su li li yang liu jianjun zhao and yadong wang.
deepgauge multi granularity testing criteria for deep learning systems.
in 33rd acm ieee international conference on automated software engineering ase pages .
acm .
mengshi zhang yuqun zhang lingming zhang cong liu and sarfraz khurshid.
deeproad gan based metamorphic testing and input validation framework for autonomous driving systems.
in 33rd acm ieee international conference on automated software engineering ase pages .
acm .
lei ma fuyuan zhang minhui xue bo li yang liu jianjun zhao and yadong wang.
combinatorial testing for deep learning systems.
arxiv.org abs .
.
augustus odena and ian goodfellow.
tensorfuzz debugging neural networks with coverage guided fuzzing.
in 36th international conference on machine learning volume of proc.
of machine learning research .
zenan li xiaoxing ma chang xu and chun cao.
structural coverage criteria for neural networks could be misleading.
in 41st international conference on software engineering new ideas and emerging results icse nier pages .
ieee .
weibin wu hui xu sanqiang zhong michael r. lyu and irwin king.
deep validation toward detecting real world corner cases for deep neural networks.
in 49th annual ieee ifip international conference on dependable systems and networks dsn pages .
ieee .
jinhan kim robert feldt and shin yoo.
guiding deep learning system testing using surprise adequacy.
in 41st international conference on software engineering icse pages .
ieee .
phyllis g. frankl richard g. hamlet bev littlewood and lorenzo strigini.
evaluating testing methods by delivered reliability.
ieee transactions on software engineering .
michael r. lyu editor.
handbook of software reliability engineering .
mcgraw hill inc. hightstown nj usa .
zenan li xiaoxing ma chang xu chun cao jingwei xu and jian l u. boosting operational dnn testing efficiency through conditioning.
inproc.
of the 27th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering esec fse pages .
acm .
domenico cotroneo roberto pietrantuono and stefano russo.
relai testing a technique to assess and improve software reliability.
ieee transactions on software engineering .
roberto pietrantuono and stefano russo.
on adaptive samplingbased testing for software reliability assessment.
in 27th international symposium on software reliability engineering issre pages .
ieee .
sharon l. lohr.
sampling design and analysis .
duxbury press .
steven k. thompson.
sampling third edition .
john wiley sons inc. .
daniel g. horvitz and donovan j. thompson.
a generalization of sampling without replacement from a finite universe.
journal of the american statistical association pp.
.
morris h. hansen and william n. hurwitz.
on the theory of sampling from finite populations.
the annals of mathematical statistics .
yann lecun l eon bottou yoshua bengio and patrick haffner.
gradient based learning applied to document recognition.
proceedings of the ieee .
alex krizhevsky.
learning multiple layers of features from tiny images.
technical report tr university of toronto .
jie m. zhang mark harman lei ma and yang liu.
machine learning testing survey landscapes and horizons.
ieee transactions on software engineering pages .
lei ma fuyuan zhang jiyuan sun minhui xue bo li felix juefeixu chao xie li li yang liu jianjun zhao and et al.
deepmutation mutation testing of deep learning systems.
in 29th international symposium on software reliability engineering issre pages .
ieee .
xiaoyuan xie joshua w. k. ho christian murphy gail kaiser baowen xu and tsong yueh chen.
testing and validating machine learning classifiers by metamorphic testing.
journal of systems and software .
siwakorn srisakaokul zhengkai wu angello astorga oreoluwa alebiosu and tao xie.
multiple implementation testing of supervised learning software.
in aaai workshops .
association for the advancement of artificial intelligence .
john d. musa.
software reliability engineered testing.
computer .
harlan d. mills michael dyer and richard c. linger.
cleanroom software engineering.
ieee software .
p. allen currit michael dyer and harlan d. mills.
certifying the reliability of software.
ieee transactions on software engineering se .
richard h. cobb and harlan d. mills.
engineering software under statistical quality control.
ieee software .
richard c. linger and harlan d. mills.
a case study in cleanroom software engineering the ibm cobol structuring facility.
in 12th international computer software and applications conference compsac pages .
ieee .
junpeng lv bei bei yin and kai yuan cai.
on the asymptotic behavior of adaptive testing strategy for software reliability assessment.
ieee transactions on software engineering .
junpeng lv bei bei yin and kai yuan cai.
estimating confidence interval of software reliability with adaptive testing strategy.
journal of systems and software .
kai yuan cai chang hai.
jiang hai hu and cheng gang bai.
an experimental study of adaptive testing for software reliability assessment.
journal of systems and software .
kai yuan cai yong chao li and ke liu.
optimal and adaptive testing for software reliability assessment.
information and software technology .
roberto pietrantuono and stefano russo.
probabilistic sampling based testing for accelerated reliability assessment.
in international conference on software quality reliability and security qrs pages .
ieee .