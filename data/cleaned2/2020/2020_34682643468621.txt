synguar guaranteeinggeneralizationinprogramming by example bowang national universityof singapore singapore bo wang u.nus.eduteodorabaluta national universityof singapore singapore teodora.baluta u.nus.edu aashishkolluri national universityof singapore singapore e0321280 u.nus.eduprateek saxena national universityof singapore singapore prateeks comp.nus.edu.sg abstract programmingbyexample pbe isaprogramsynthesisparadigmin whichthesynthesizercreatesaprogramthatmatchesasetofgiven examples.
in many applications of such synthesis e.g.
program repairorreverseengineering wearetoreconstructaprogramthat isclosetoaspecifictargetprogram notmerelytoproducesome program that satisfies the seen examples.
in such settings we wish thatthesynthesizedprogram generalizes well i.e.
hasasfewerrors as possible on the unobserved examples capturing the target function behavior.
in this paper we propose the first framework called synguar forpbesynthesizersthatguaranteestoachievelowgeneralizationerrorwithhighprobability.
ourmaincontributionisa procedure to dynamically calculate how many additional examples suffice to theoretically guarantee generalization.
we show how our techniques can be used in well known synthesis approaches prose and stun synthesis through unification for common string manipulationprogrambenchmarks.wefindthatoftenafew hundredexamplessufficetoprovablyboundgeneralizationerror below5 with high probability on these benchmarks.
further we confirm this empirically synguar significantly improves the accuracy of existing synthesizers in generating the right target programs.
but with fewer examples chosen arbitrarily the same baseline synthesizers without synguar overfitand lose accuracy.
ccs concepts software and its engineering software notations and tools general programming languages.
keywords program synthesis generalization sample complexity acm reference format bo wang teodora baluta aashish kolluri and prateek saxena.
.
synguar guaranteeing generalization in programming by example.
in proceedings of the 29th acm joint european software engineering conference esec fse august 23 28 athens greece copyright held by the owner author s .
acm isbn .
august 23 28 athens greece.
acm new york ny usa 13pages.
introduction programsynthesisisthegoalofautomaticallygeneratingcomputer programs for a given task.
this vision has existed for over at least four decades .
one of the mainstream approaches towards this goal is programming by example or pbe .
in its simplestform apbesynthesizerisgivenaccesstoan oraclethat can generate correct input output i o examples for the unknown targetprogram.
the synthesizer has to create a candidate program ascloseaspossibletothetargetprogramfromapre specified hypothesisspace i.e.
thespaceofallpossiblecandidateprogramsthat thesynthesizer canreason about.
thenumberof giveni oexamplescan varydependingonthe endapplication butthe fewerthe better.
therein lies the challenge of generalization if examplesare toofew then many possiblecandidate functions satisfy them and picking one arbitrarily might yield a solution that works well only ontheseenexamples.inotherwords thesolutionoverfitstothe seenexamplesandmaynotgeneralizewell.howdoesasynthesizer createprogramsthatareprovablyclosetothetargetprogram?this has been a fundamental question for pbe based program synthesis.
there are several domain specific solutions to generalization.
in program repair as well as in inductive synthesis for example inferring additional specifications from observed examples that must be satisfied by the program is shown to help with generalization .
allowing the synthesizer to use more powerful oracles that adaptively craft examples or logical invariants help to synthesize correct programs .
in neural guided program synthesis machine learning techniques to avoid over fitting such as regularization or structural risk minimization are employed implicitly .
in domains where we have prior knowledge about the likely distribution to which the target program belongs synthesizers can rank solutions guide program search and use generative models for program representation or distributional priors .
some synthesizers favor short programs as per occam srazor .
all of the above approaches while useful require additional knowledge or implicit assumptions about the target program beyondthatcapturedbytheoriginalpbeproblemsetup.withoutsuch assumptions theseapproachesdo notprovideanyformalguarantee 677this work is licensed under a creative commons attribution international .
license.
esec fse august athens greece bowang teodora baluta aashishkolluri andprateek saxena thattheproducedprogramwillbecorrectorgeneralizewellonunseenexamples.itisnaturaltoask canweguaranteegeneralization withoutmakingany additionaldomain specific assumptions?
in this paper we study generalization in pbe from the perspective ofsample complexity how many i o examples should the synthesizer have to see to be confident that its selected solution is close to the target program?
to answer this question the pac learning theory provides a starting point .
a synthesizer generalizeswellwhenthesynthesizedprogramisclosetothetargetprogramwithhighconfidence.thenotionofconfidenceand closeness to the target program can be made formal using pac learningtheory.specifically the synthesizedprogram generalizes if itwill makeno more than a small fraction of errorson unseen examplestakenindependently withhighprobability atleast1 .
ourapproachworksonanydistributionthatthei oexamples are sampled from.
to formally guarantee that generalization is achievedonthedistribution weneedaprincipleddesignforpbe synthesizers.existingpbesynthesizersarenotdesignedtoprovide generalizationguarantees therefore theypickthenumberofi o examplestoworkwithinanad hocfashion.forinstance theymay synthesizeaprogramafterseeingonly2 4examples .this paper seeksto answer the following questions rq1.how many i oexamples would a synthesizer need to see in order to provablygeneralize?
rq2.doexisting synthesizersoverfitwith say 4examples?
as a conceptual contribution we present the first principled framework synguar1 to provide generalization guarantees about the synthesized programs.
we propose a procedure that computes the size of the hypothesis space dynamically during synthesis whichisthenusedtocalculatethesamplesizerequiredtoprovably generalize.
the challenge is therefore two fold.
first while efficiently computing the size of the hypothesis space is easy in some existing pbe synthesizers such as the prose framework for others it requires careful design.
an example of the latter is the synthesisthroughunification stun approach.asourmain technicalcontribution wepresent anexample ofintegrating synguarinto synthesizers based on prose and stun frameworks.
specifically we provide two pbe synthesizers for string manipulation programs that provably generalize one implemented in the prose framework synguar prose and one based on the stun approach synguar stun .tothebestofourknowledge noprior pbesynthesizerclaimssuchstronggeneralizationguaranteesabout the synthesizedprograms.
fromanempiricalperspective ourworkprovidesthefirstexperimentalevidenceforthenumberofexamplessufficienttoguarantee generalization in practice for simple string manipulation tasks.
we runsynguar prose andsynguar stun on two benchmarks in this domain manually designed data wrangling tasks similar to those used in flashfill and the standard sygus benchmark respectively.wefindthatontheirrespectivebenchmarks the tools produce programs which are provably within at most generalizationerrorwithamodestnumberofexamples around samples and examples on average respectively with a highprobability .thisobservationalsosuggeststhatitis 1thetoolisavailablewith canbefoundat strprose input string x start string program recterm stringrecterm catterm concat catterm recterm stringcatterm conststr cs convterm stringconvterm term uppercase term lowercase term stringterm substr x pos1 pos2 int?
pos abspos x ka regpos x kr stringcs constant string intka absolute position tuple regex regex int int kr kr r1 r2 k offset figure1 adslforstringtransformationprograms.
concat returns the string produced by concatenating two strings catterm andrecterm substrreturns the substring between pos1andpos2.
theuppercase andlowercase return the string in upper case and lower case respectively.
absposreturns the absolute position of string x. theregposoperator outputs the kthposition plus an offsetwhere the boundariesofthestringsreturnedbyapplyingregularexpressions r1andr2 respectively on string xmatch.
unlikelyforpbesynthesizerstogeneralizefromjust2 4examples without using some implicit or explicit additional knowledge.
weconfirmthisobservationbyrunningthevanillaversions i.e.
versions with synguar disabled on the same benchmarks with randomly chosen examples or the given seed examples in the benchmark 10insize .wefindthat synguar prose generates thecorrecttargetprogramfor14 16casesfromthedata wrangling task benchmark and synguar stun for cases fromthe sygusbenchmark.
incontrast the vanillaversionsgenerate correct programsfor0 16and36 59cases respectively.thisshowsthat withoutenough examples synthesizedprograms often overfit.
though we focus on string manipulating programs our approach makes minimal additional assumptions and thus can be extendedtootherapplicationdomains suchasprogramrepair invariantdiscovery andsoon.thegeneralizationguaranteefits well in applications such as data cleaning and transformation where a provable accuracy matters or automatic stub writing in symbolicexecution wherethegoalistolearnasymbolic constraint that is approximately close enough to the target.
our experiments suggest that the sample size to achieve generalizationistaskandbenchmarkdependent whichleavesthequestion of how well our presented approach works in other domains or benchmarks open.
theseare grounds for promisingfuture work.
overview in practice it is hard to know how many i o examples suffice to solve a synthesis task.the number of i o examplesacross varioustargetprograms evenforthesamesynthesizer varyinprior worksandarechosensomewhatarbitrarily.forinstance thesygus benchmark hasa dedicated track forthe domain ofstringmanipulating programs.
the benchmark consists of several pbe tasks andeach of themis providedwith a differentnumber of i o examples.
some have as low as examples whereas others have .
678synguar guaranteeing generalizationin programmingby example esec fse august athens greece word w a za z0 digit d .
?
any character concat return substr until the end of 1st word match substr x abspos x regpos x w .
?
concat conststr append concat return substr of 2nd word match until the end of the word substr x regpos x .
?
w regpos x w .
?
concat conststr concat return the substr of 3rd word match until the end of the word substr x regpos x .
?
w regpos x w .
?
concat conststr return the substr from the 4th word until the end of the string substr x regpos x .
?
w abspos x concat return substr of first two characters substr x abspos x abspos x overfits concat conststr concat return the string from the start of the first number offset by till the second last separator substr x regpos x .
?
?
d .
d ?
regpos x .
?
overfits concat conststr concat return the substr from second capital letter offset to offset substr x regpos x .
?
regpos x .
?
overfits concat conststr return the substr from start of second word with offset till the end substr x regpos x w .
?
abspos x overfits figure on the left we show the correct target program tthat tokenizes the string xon the boundaries of the first words.
on theright we show theprogram synthesized on 2examples which overfits at allthefour substroperations.
ex.
input output f 0e e u7kuz85 0e e u7kuz85 j 3bj.
ppm j 3bj ppm tpj av n0d7 6z tpj av n0d7 6z r 6vcs q r 6vcs q m x cskrw ru6 m x cskrw ru6 wk u u nzp x wk u u nzp x gsa ub hn lpa gsa ub hn lpa ro8i3 r sum e ro8i3 r sum e q e ld0 q e ld0 dzpz t.q s dzpz t q s ny e87e lo 0w ny e87e lo 0w fx u p 1fn fx u p 1fn figure the i o examples provided to the synthesis algorithm along with the number of consistent candidates f .
this numberdrops from 1042to18injust12samples.
toillustratetheproblemofoverfittinginpbe basedsynthesis let us consider a data wrangling task of tokenizing a given passageoftextintoindividualwords.thetokenizationtaskinvolves recognizingtheboundariesofthefirst4wordsandreplacingthe characters used to separate them with commas.
the user wishes touseaprogramsynthesistooltolearnaprogramthatperforms thistask.here weuseourtool synguar prose whichisbasedon theproseframework forsynthesizingaprogram.program synthesizersoutputprogramsinthesyntaxofsomepre specified target language or domain specific language dsl .
in order to supportourtask weimplementastringtransformationdslinprose that is similar to the flashfill dsl see figure .
we give a reference implementation of our modified dslin the supplementary material .
the user provides an oracle which can be queried by the synthesizer fori oexamples exemplifying the behavior of thetargetprogram.
eachi oexampleisapairofstrings formedfrom lower andupper caseletters digits spacesandseparators.
given the problem setup as described above the goal of the synthesizerreducestolearningthe4correctsubstringsfromthe providedexamples.letusexaminehowwellthesynthesizerperforms on a few i o examples say .
after running it on the first examples given in figure as expected the output program overfits.
for instance instead of trying to get the first substring untiltheendofthefirstword itjustpickstwocharactersforthe firstwordsinceboththeexampleshaveonlytwocharactersuntil theirrespectivefirstseparators.infact thesynthesizercontinues to overfiteven after being provided 9examples.
however it turns out that for our running example we can theoretically assert a program close to the target will be picked withhighprobabilityafter149examples!ourproposedalgorithmic framework synguar isabletocalculatethisquantityontheflyand stop when enough examples are seen.
to understand how it works consider figure 3that shows the estimated size2of the hypothesis spacethatis consistent ormatches withalltheseenexamplesupto a certain point.
synguar computes this quantity internally which servesasourmaintechnicalinsight.noticethatthespaceofthe consistentprogramsshrinksprogressivelyasmoreexamplesare provided.beforeseeinganyexample thehypothesisspaceisthe set of all the programs our target language can represent and its size can be infinite as the dsl grammar is recursive.
now suppose the synthesizer sees the first i o example then the number of candidateprogramswhichare consistent withthefirstexamplereduces considerably.
the reduction depends critically on the example provided.
for instance the first i o example shown in figure 3will reducethespaceofconsistentprogramsto1042.thisisstillquite large if we arbitrarily pick one program without using any auxiliaryassumptionsorpriorknowledgeaboutthetargetprogram the odds of picking the correct program are negligibly small.
however after seeing examples the consistent program space reduces to 2asound upper bound of the actualsize is calculated.
679esec fse august athens greece bowang teodora baluta aashishkolluri andprateek saxena algorithm1 meta synthesisalgorithm procedure metasyn whilestopping condition do query userfor k examples updatethe hypothesisspace returnnoneif emptyhypothesisspace compute the mexamplesfor guarantee query userfor mexamples updatethe hypothesisspace ifemptyhypothesisspace then returnnone returnfinhypothesisspace 18forourrunningexample.notethatchoosingaprogramoutof these18atrandomdoesnothaveanyguaranteesonitsclosenessto thetargetprogram.foraprogramspaceof18 tochooseaprogram thatisclosetothetargetprogramwithprovablyhighconfidence werequire137moreexamples.
synguar canprovablyassertthat it has seen enough examples to stop and return a program close to the target program after12 examples.
problem setup.
similar to the setup used in existing pbe based synthesizers we are given an oracle to query i o examples and a dsl for representing the output program.
additionally we are given user specified parameters that capture the desired generalization guarantee.
the synthesizer queries the oracle for as many i o examples as it needs and terminates with either none or a synthesized function f. the probability that the synthesizer returns a function fthat might not generalize should be under the given small .
lets x t x be the set of i o examples seen by one invocation of the synthesizer.
here each xis an input drawn independently and identically distributed i.i.d from an unknowndistribution dthattheoraclecaptures.weassumethat f will satisfy all given i o examples x s f x t x .
note that thisisdifferentfromthe best effort orapproximatesynthesis approaches wheretheprogram fisallowedtodifferfrom ton some examples in s. in this setup therefore sis a random variable.
thesynthesizer denotedas a s isalsoarandomvariabledefined over i o samples s drawn from d. we seek to designpbesynthesizers that achieve generalization given by a rigorous pac style guarantee while computing the required sample complexity.
for an synthesizer the generalization error error f prx d is bounded by .
the probability ofgenerating fwitherror f isboundedby .
definition .
synthesizer .
a synthesis algorithm awith hypothesis space his an synthesizer with respect to a target classoffunctionsciffforanyinputdistributions d forallt c given example set sdrawn i.i.d from the d pr the synguar framework weproposeaframeworkwithasimilaralgorithmicmeta structure as that of existing pbe engines.
the overall procedure is shown in algorithm .
instead of synthesizing a program after seeingapre determinednumberofi oexamplestheprocedurequeries an oracle for new examples until it synthesizes a program that generalizes.
there are two key new features in our framework a stopping criterion and a dynamically calculated count of the numberof samplestobe seen.the following classical resultgives usastarting pointto compute the count precisely.
astartingpoint.
the number of examplesprovably sufficient to achievethe generalizationisgivenbyblumeretal.
.we restatethisresult whichcomputessamplecomplexityasafunction of and the capacity or size of any given hypothesis space h. theorem .
sample complexity for synthesis .
for all andhypothesisspace h asynthesisalgorithm a s which outputs functions consistent with mi.i.d samples is an synthesizer if m ln h ln1 theabovetheoremisintuitivelybasedonthefollowinganalysis.
letussaywehavesomeinitialhypothesisspace h.afterseeing onenewi oexample eachhypothesisthatis far from t generalization error becomes inconsistent with some non zero probability and is eliminated.
therefore after seeing sufficiently manynewexamples theprobabilityofany far hypothesisbeing outputfalls below .for details pleasesee the analysis .
.
keyobservations challenges weobservethattheorem .1canbeusedatanypointofthesynthesisprocedure.afterseeingsaythefirst sexamples letthespace of programs consistent with the examples be hs.
we can plug hs intotheorem .1tocomputehowmanymoreexamplesaresufficienttoachievetheguaranteeprovidedindefinition .
.but there are several key technical challenges in utilizing the classical result of theorem .1in providing end to end generalization guarantees.
first applyingthisresultrequiresbeingabletocompute hs .we pointoutthat thishasnotbeenan explicitalgorithmicgoalwhen designingexistingsynthesizers.consequently computing hs is non trivialinsomeofthe existing synthesizers.totackle this we design our own pbe synthesizer based on the stun approach and bottom up explicit search with the ability to compute hs see section4.
.further weshowhowtointegrate synguar inprose a synthesis framework where thesizeofthehypothesis space can be easily computed.
second hs can be large and plugging in its values at the beginning leads to vacuously high sample bounds in practice.
for instance initiallythehypothesissizeinourrunningtaskisinfinite andevenafterseeingoneexample thesizeis1042.therefore instead of naively plugging in values of parameters at the beginning we use the idea that if hs decreases as the synthesizer sees more examples then the estimated sample complexity for generalization reduces as well.
therefore in synguar s design hs is computed ontheflyas thesynthesizerseesmore examples andthestopping condition ensures that the synthesizedprogram generalizes.
lastly thepaclearningtheoryoffersnorecoursetopredicthow fast hs reduceswithmoresamplesinpractice.thisquestion then becomes an empirical one for which programs do we observe that a 680synguar guaranteeing generalizationin programmingby example esec fse august athens greece algorithm synguar synthesis returns a program with error smallerthan withprobability higher than procedure synguar k tunableparameter g pickstoppingcond s s sizeh computesize h n g sizeh whiles ndo s s sample k hs updatehypothesis s sizehs computesize hs returnnoneifsizehs s s k n min n s g sizehs mhs lnsizehs ln1 t sample mhs s s t returnprogram finhsornone if hs smallnumberofexamplesaresufficienttogeneralize?
ourempirical evaluation shows that for several common string manipulation tasks the requirednumber ofexamplesturnoutto be modest.
remark.
itcanbeseenthatoursolutionisamodificationtothe existing pbesynthesis loop which canbe instantiatedfor several programsynthesisengines.ourproofsandanalysisutilizeclassical samplecomplexityarguments togetherwithboundsforhypothesisspacesize.itmaythereforeappearsurprisingthenthatsuch calculationsarenotroutineinpriorprogramsynthesisworksalready despiteaccuracy generalizationbeinganaturalobjective.
we believe that the challenges stated above offer an explanation as towhyapplyingprevioustheoreticalresultsisnotstraightforward and requires a principled approach.
we point out that the subtlety is in our problem formulation itself namely the use of dynamic calculation of the remaining sample size needed for generalization.
.2synguar algorithm we start by addressing the second challenge assuming that hs is computable.
recall that our synthesis algorithm takes as input the error tolerance parameters and the confidence algorithm .
the algorithm follows the structure of algorithm 1and consists of two phases the sampling phase lines and the validation phase lines .
the sampling phase addresses the second challenge by trying to shrink the hypothesis space as much as possible.
in each iteration ksamples are taken before updating the hypothesisspacethatsatisfiesall ksamplesseen.thecrucialpartof the sampling phase is deciding when the number of examples seen sofar storedintheset s andwhosecardinalityis s is enough .
synguar stopssamplingwhenthenumberofexamplesseensofar exceedsastoppingthreshold representedbyvariable n.ineachiteration thestoppingthreshold whichdependsonfinite h initially eitherremainsthesameoritgraduallyshrinkswitheachupdate of the hypothesis space line .
hence synguar dynamically updatesthethresholdbasedonthechangeinthehypothesissize.
tocontrolhowmuchthethresholdvariableshrinkswithrespecttothesizeoftheupdatedhypothesisspace hs synguar picksa function g line3 .
our framework allows using any g n z that is monotonically non decreasing and we provide a sample complexity analysis for such functions.
we propose a particular choiceoffunction gasthedefault g x max ln x ln .
thisghas a useful property the required number of samples it entails in the worst case cannot be more than twice the number ofsamplesthatanoptimalchoiceof gwilltake.wewillformally state andprove this optimalityclaim insection .
.
in the second phase in addition to the samples s synguar samples a fixed number of samples according to theorem .
.synguarthen can return a program fwith provable guarantees line14 .
algorithm 2calls sub procedures updatehypothesis line9 tofindaprogramspaceconsistentwith s andcomputesize line10 tocompute thesize oftheconsistent program space.the sub procedure updatehypothesis can be implemented by any existingpbesynthesisalgorithmwhichreturnhypothesesconsistent withs .section4detailshowtoimplement computesize which isspecific to the underlying updatehypothesis sub procedure.
running example.
consider the example given in section .
first the user inputs respectively.
in the sampling phase theuserisqueriedforoneexampleineachiteration.after the first iteration i.e.
seeing one example the sample size for generalization mhs is2018.instead synguar ssamplingphase stopsafterseeing n 12examplesandtheadditionalsamplesizefor generalization line reduces to mhs .
the total sample size of both phases sums up to examples which is less than the sample complexity after the first iteration.
this is a direct consequenceof synguar dynamically estimating the sample size for generalization.
.
analysis ofthealgorithm synguar s design is motivated by being able to give a formal generalizationguaranteeandaboundedsamplecomplexity.forthis purpose we state andprove the following properties p1 termination synguar alwaysterminatesfor afinite h .
p2 guarantees theprobabilityof synguar returningan fthat is far issmallerthan .
p3 samplecomplexity synguar ssamplecomplexityisalways within ofthe optimal for k 2 ln1 .
theorem .
p1 .
synguar alwaysterminates for afinite h .
proof.itsufficestoprovethatthesamplingphase lines7 ofsynguar terminates in order to show that synguar terminates.
in each iteration of the sampling phase let sibe the queue storing the user provided examples after each iteration ztbe thetth example si si zik ...zik k ands0 .
for each si hsi determinesthesetofconsistenthypothesisthatsatisfy si.letni bethelimitofthenumberofi oexamples nforthesamplingphase after iteration i. for iterations iandjwherei jand g n z such that gismonotonically non decreasing the following holds si sj hsj hsi g hsj g hsi nj min ni sj g hsj ni see line 13inalg.
therefore if n0 g h then the loop will terminate at some iterationpsuch that np sp np k n0 k. 681esec fse august athens greece bowang teodora baluta aashishkolluri andprateek saxena theorem3.
p2 .
theprobabilityof synguar returningasynthesized program fthat is far is smallerthan .
proof.bytheorem .
weknowthatthesamplingphaseterminates with s samples see line .
in lines synguar samplesanadditionalnumberofi oexamplesrequiredtogeneralize and then synthesizes a program after seeing the additional samples.therefore theorem .3followsfrom theorem .
.
in order to prove the last property we define a new quantity q .
it is the smallest sample size taken by synguar for any non decreasing gusedfor asequenceofi o examples q. definition .
smallest dynamic sample size .
for any infinite sampledsequenceofexamples q letprefix q g betheprefixof qat which synguar terminates.then q inf mg g mg prefix q g theorem .
p3 .
synguar uses no more than 2 q examples onanyqwhenthe resultisnotnonewith g x max ln x ln andk 2 ln1 .
duetospacelimit weprovidetheproofofthistheoreminthe supplementary material .
retrofitting synguar into existing synthesizers we now show how to compute hs the size of the consistent programspace.asoundupperboundof hs issafetouse sincein thiscase ouranalysisshows synguar totakemoreexamplesthan thoseneededtoguaranteegeneralization.weshowhowtocompute hs bounds for twowell knownpbe synthesis approaches.
.1synguar fortheproseframework we first apply synguar on top of the prose framework a state of the artpbemeta synthesisframeworkthatgeneratesan inductive synthesizer for a given dsl.
prose allows developers to write dsls and specify witness functions that capture a subset of inversesemanticsforthedsloperators.thesewitnessfunctions arethedriversforthe deductivebackpropagation becausethey specifythe inputs or properties of the input given an i oexample.
we implement a synthesizer named strprose with the dsl in figure1ontopofprosebyspecifyingexecutablesemanticsand witness functions for its operators.
our dsl shares most operators with the dsl of flashfill .
for the operators that differ we detail their executable semantics and witness functions in the supplementarymaterial .notethat synguar workswithanydsl expressible in prose as long as each operator has its semantics andan associatedwitness functionspecified.
prose uses an internal succinct representation of the program space using a data structure called version space algebra vsa which makes it convenient to calculate hs .
a vsa is a directed graph where each node corresponds to a set of programs.
the leaf nodes explicitly represent a set of programs that can be enumerated.
there are two types of internal nodes unionnodesthatrepresentaset theoreticunionand joinnodesthat represent k ary operators whichare definedbythe dsl.computing hs usingvsa.
prosereadilycomputes hs using a bottom up graph traversal on its vsa.
for each leaf node it enumerates and counts the set of programs directly.
for every union node to compute the corresponding number of programs it adds up the count of all child nodes.
for every join node the number of programs is a cross product of all applications of the kary operator to kparameter programs.
this soundly upper bounds hs .
in our implementation we reuse the sizeapi available in prose resultinginthe sizes showninfigure .
scaling to large sample size.
building vsa ona largenumber ofexamplescanbe time consuming.
therefore webuildthevsa onasubsetoftheexampleswhichleadtothesamesetofprograms.
morespecifically wetaketheexamplesonebyoneanddropthe examples that do not decreasethe vsasize.
.2synguar instrstun stunis a well known synthesis approach .
it was originally proposed as an extension of the counter example guided inductive synthesis cegis approach to synthesize program from the specification.
the high level ideaisto synthesize partial solutions satisfyingpartsoftheinputsandunifythem.asaninstantiationof stunforsynthesizingconditionalprogramsunderpbesettings we work with top level3if then else unification operator where theconditioncanbeanybooleanexpressioninthehypothesisspace.
thesubsequentsynthesisalgorithmsfollowingthisapproachdo notcompute hs directly ormakeitstraightforwardtocompute it.wedesignasynthesisalgorithmbasedonthisapproach anda proceduretosoundlycomputeanupperboundon hs .wechoose our target language as the sygus string manipulating program dsl .
our synthesis algorithm isreferredto as strstun .
vanillastrstun overview.
strstun instantiates the previously proposed approach of bottom upexplicit search withobservational equivalence reduction .
its algorithm consists of two phasesatahighlevel anenumerationphaseandaunificationphase.
in the first phase the synthesizer enumerates candidate programs only by repeatedly using function application.
it clusters all candidateprogramswhichhavethesamei obehavioronthegivenexamplesandsavesonlyoneprogramrepresentativeofeachcluster.such enumeratedprogramsmayonlybeconsistentwithsubsetsofthe giveni oexamples.intheunificationphase strstun composes enumeratedprogramswithan if then else unificationoperator.
thefinalsynthesizedprogram p therefore canbeastraight line program obtained by repeated function application or a program withnestedcompositionsoftheform ifp1thenp2elsep3 where p2andp3canbenestedprogramsthemselves.thenestingdepthis boundedinternallytolimitthesearchspace.weexplaintheconstructionaldetailsofthesephasesnext andexplainhowtocompute hs from the internal data structures later.
in what follows we denote inputsandoutputs of thegiveni o examplesasvectors w andorespectively.
vanillastrstun enumeration.
strstun enumeratesallcandidateprogramsinabottom upfashionbygeneratingprograms throughfunctionapplication.westartwiththe smallestsyntactic programs whicharejustsinglecomponents orsyntacticterminals 3such if elseconstructsisrestricted to beingat the topof the function s ast.
682synguar guaranteeing generalizationin programmingby example esec fse august athens greece inthetargetlanguage workinguptoprogramswithmorethanone component.forinstance concat input0 input1 isacandidate program with three components.
the total hypothesis space withoutconditionalsisfixedbasedonauser providedconstraintonthe maximumcomponentsize specifiedasthemaximumnumberof components the straight line program contains.
for each program createdintheenumerationphase wecomputetheoutputsofthe program on the given i o examples.
note that this step does not requireexplicitlyindividuallycreatingandrunningallcandidate programs it ispossibletoevaluateoutputsduringthebottom up construction ofthe programs .
wecompute 2usefuldatastructuresinternallyduringenumeration.
the first is a consistency vector cwhich captures whether anenumeratedprogram pisconsistentwiththegiveni oexamples represented by vector wand vector o. specifically the consistency vector cfor program phas theithelement set to if the p w o namely theoutputof pontheithgiveninputexamplematchesthecorrespondinggivenoutputexample.otherwise c is set to .
this data structure speeds up the search in two ways conceptually.
first it is calculated on top of observational equivalence .ifmanycandidateprogramsgeneratethesameoutputs on the given input examples then they are all observationally equivalent andweonlyneedtokeeponesuchprogramthathasthe outputsforcompleteness thustheconsistencyvectorisonly calculatedonceforthoseprograms.secondly eveniftwoprograms areobservationallynotequivalent andbothgivedifferentincorrect outputs foran input example they will have the samevalue in their consistency vectors.
thus we can effectively cluster manyprogramsthatareobservationallynon equivalentbuthave the same consistency vector to speedupthe nextphase.
thesecondusefuldatastructureisclustermap .itmapsconsistency vectors to sets of programs.
each distinct consistency vector ccomputed during the enumeration phase is mapped to a set of programs c that have i o behaviorcapturedby c. vanillastrstun unification.
in this phase strstun synthesizes programs with nested if then else structures.
the goal is to create programs that are consistent with larger subsets of i o examples than enumerated programs and ideally correct on the full set of i o examples.
the cluster map allows us to quickly find programs which match certain subsets of all the given i o examples specifiedbyaconsistencyvectorvalue.whenaprogram p ifp1thenp2elsep3is synthesized during unification we must carefully construct a semantically correct consistency vector forp usingthoseforsub programs p2andp3.here notethat p1 needs to be a program that evaluates to a boolean value on a given inputexample say w .ifitevaluatesto true then theprogram p2mustbecorrecton w forptobecorrecton w .therefore inthiscase wemark pasconsistentwith w ifandonlyif c forp2hasa .analogously if p1evaluatesto falseonw then c forp3should be set to forpto be marked consistent with w otherwise pismarkedinconsistentwith w .
theabove describedunificationproceduresynthesizesallprograms with a nesting depth of up to a pre configured maximum defaultof .thenestingdepthcontrolsthehypothesisspacedesiredbythe user.theclustermap isupdatedcontinuously withnewconsistencyvectorsdiscoveredintheunificationphase.asuccessfulsolutionisaprogramthatmatchesallgivenexamples i.e.
has aconsistency vector witha for allvalues.
computing the hs .
the vanilla strstun algorithm can be slightly modified and augmented with rules shown in table 1to computethe hs soundly.noticethatinvanilla strstun when employing the observational equivalence a program with larger componentssizemightbediscardedifthereisasmallerprogram thathasthesame valuevector4 .butweneedtocountallprograms at different components sizes.
to do so we store multiple countingvalues andrepresentativeprograms fordifferentcomponent sizesalongwitheachvaluevector.
duringtheenumerationphase programsaresynthesizedbottomup from smallest components size to larger ones.
let tbe the components sizeof aprogram.
we keeptrack of a count v t for each valuevector vcomputedforprogramswithsize t.thecount v t fort smallest base components can be directly enumerated rule in table since these are program input arguments or constant components in our target language.
for t count v t canreturn 0ifthereisnoenumeratedprogramwithcomponents sizetthatoutputsvaluevector v.samevaluevectors vmayhave differentcounts fordifferentcomponentssizes t thus weenumerate on tuple v t rather than just v. whenstrstun uses function applicationtogenerateanewprogram p fromprograms pi rule .1intable thecount for thevaluevectoroftheresulting p is updated by addingthe product of all the counts of its arguments piat their respective components sizes rule .
in table .
this completes all the ways programs are compositionally created in the enumeration phase from component size to the maximum componentsize.
after the enumeration on value vectors is finished we have alsoclusteredobservationallynon equivalentprogramsbasedon aconsistencyvector cin .define asthesetofallthevalue vectors that correspondsto aconsistency vector c we sum upthe count v t for every vin rule in table .
this way we compute counts for consistency vectors.
duringunification programs of the form p ifp1thenp2 elsep3are composed.
here counts of the consistency vectors ofp2andp3have been computed after enumeration if p2andp3 areprogramswithnestingdepthzero.inthiscase the count c of the program pis the the product of p1 all possible p2 condition with p1as the condition and all possible p3 condition withp1as the condition summed over all possible p1.
thus we havecomputed countfortheconsistencyvectorsofprogramswith nesting depthof .
using this we canrecursively compute counts forconsistencyvectorsofprograms withnestingdepth 2ormore rule5 intable .
to optimize we memorize counts ofindividual consistencyvectorsaswellasforsetsofconsistencyvectors.for example the set comprising two consistency vectors and is succinctly represented as and their sum of counts is memorized rule in table .
we prove that the rules in table1provide asound upperbound ofthesizeofthehypothesis space hs insupplementary material .
4the value vector vfor a program pis simply the outputs of pon the given input examples i.e.
v p w for alli.
683esec fse august athens greece bowang teodora baluta aashishkolluri andprateek saxena table count rules for strstun .
we define the set of all boolean value vectors as b and we usecto represent a succinct representation of consistency vectors which can be a singleton or a set of consistency vectors.
the countvalue is calculated differently for value vector v consistency vector c ortheir succinct representation c. countrulesfor enumeration starting pointof enumeration count v number of singlecomponentsthat output v count v t fort .
when program p with valuevector v is enumerated at componentsize t let p f p1 p2 ... pn for some function component fand programs pi.
iftiis the componentsize for pi andviis the valuevector of pi ti i ... n then1 summationtext.1ti t holds and .
count v t count v t n productdisplay.
i 1count vi ti .
countrulesfor clustering count c summationtext.
v summationtext.
t count v t countrulesfor unification cgoal ... the count of hypothesisspace up to kconditions is summationtext.1k i 0count cgoal i count c summationtext.
c ccount c count c i summationdisplay.
b bi summationdisplay.
j count then c b j count else c b i j summationdisplay.
tcount b t where then c b and else c b arealso succinct representation of consistencyvectors and then c b i braceleftbigg ifbi t otherwise else c b i braceleftbigg ifbi f otherwise strstun augmented with synguar .we callstrstun augmentedwith synguar asdescribedinsection 4assynguar stun .
the maximum number of nested conditions during the unification phaseofstrstun is2 andwithoutmuchlossofexpressiveness weonlyallowthe elsebranchtohavenesting.withthissetting the hypothesisspaceof strstun isaunion of h0 the setofstraight lineprograms h1 the programs ofform ifp1thenp2elsep3 h2 the programs ofform ifp1thenp2elseifp3 thenp4elsep5 .
fromh0toh2 thehypothesisspaceisincreasinglyexpressive.
synguar stun invokes the synguar loop with h0first and if it returnsnonethenwith h1 andsooninthatorder.thishasthenice propertythatitwillreturn fconsistentwithexistingexamplesfrom hiwhereiisthesmallestpossible.forcorrectness eachinvocation with a new hypothesis hiuses a failure probability of so the totalfailure probability isboundedby union bound .
evaluation wehave shown thatwhen anexistingpbe synthesizerusing synguarreturns a synthesized program the program generalizes i.e.
itisclosetothetargetwithhighprobability.ourevaluationfocuses ontwoempirical utilitygoalsinstring manipulationtasks accuracy do our theoretical generalization guarantees improve the end accuracyof existing pbe synthesizers?
sample size howmanyexamplesdoes synguar require to achieve provablegeneralization?
recall that synguar primarily extends existing synthesizers to control how many examples the synthesizer sees before stopping.weevaluate a synguar prose whichbuildsontheprose framework and b synguar stun which is implemented on the strstun synthesizer we designed.
the vanilla strstun synthesizer is around lines of c code.
these vanilla versions of pbe synthesizers without the synguar augmentation serve as our baselinesto measure improvements dueto synguar .
we point out that pbe synthesizers for string programs often competeoncomputationaloverheadsreportedforproducing any programthatfitsagivensetofexamples.ouraccuracycriterionand ourobjectivearecompletelydifferent wewanttocheckwhena synthesizerproducesaprogramclosetoafixedtargetprogram the numberofexamplesisnotfixed .thisiswhywedonotcompareto otherbaselinesolverswhichmaybecomputationallyfaster but are not designedto generalize to atarget program.
benchmarks.
forsynguar prose we considered 16common string related programming tasks as target functions to synthesize.
these are of similar style and complexity as those reported in flashmeta paper such as changing the date format extractingnumbersorabbreviatingwords.henceforth werefertothese programsas prose benchmark detailsofwhichareinthesupplementarymaterial .forsynguar stun wetaketheeuphony benchmarkfromthepbe stringstrackofthesygus2019benchmark 5whichcontains 100pbe taskswith 16examples.
the target programs are not available for those 100tasks so we manuallywrotethemfromthegivenexamplesfromthebenchmark.
out of the 100tasks 10are for tasks that output boolean values which are not in the scope of our considered dsl.
further we experimentally observed that our strstun implementation scales uptocomponentsize sizeofthelongeststraight lineprograms beforeunification withinareasonablecomputationofadayforall benchmarkstofinishonourexperimentalsetup largercomponent sizeincreasestheprogramsearchspace .sofortheremaining we filtered out the ones that could not be manually constructed under component size .
this finally results in 59sygus benchmarks whichwe call sygus stun .
the generalization error tolerance for all experiments is set to5 .
and confidence parameter to .
by default.whencomparingsamplesizefordifferent wealsorun the benchmark on .
.
and .
.
.
the defaultstepsize kforthesamplingphaseissetto 1forsynguarproseand20forsynguar stun .
with these parameters it is guaranteedwithprobabilityatleast thatwhen synguar stops itssynthesizedprogramisgoingtohaveageneralizationerrorof at most5 .
we ran all experiments on amazon ec2 ubuntu .
instancewith512gbram core3.1ghzintelxeonprocessors whereeachbenchmarkruns 1core.allourexperimentsfinished within24hours.for synguar prose runsfinishedin1minute 5downloaded from pbe slia track euphony 684synguar guaranteeing generalizationin programmingby example esec fse august athens greece and within hour.
for synguar stun runs finished in minutesand97 within 2hours.
synguar works with any input distribution for creating i o examples.wechooseadistributionthatiseasytogenerateandnot specializedtoeachtargetprogram.specifically wesimulateablackboxfuzzerforstringinputs.allour synguar prose evaluation reports an average over 32trials of each target program and for eachtrial we sample astringinputas follows thestringlengthischosenuniformlyatrandomfrom each character in the string is either chosen uniformly at randomfromthecharacterset c a za z0 .
or chosen as white space withprobability larger than the probability ofany character in c. weruneachsuchinputcreatedonthetargetprogramtocreatethe output.the input output pairsare given to the synthesizer.
we evaluate synguar stun over3trials for each target as theseprogramsarecomputationallyheaviertosynthesize.foreach trial we simulate a basic mutation based fuzzer as the sampling distribution.
specifically we take the input strings provided in the sygus benchmark as seeds and mutate them randomly as follows add a string of randomly chosen length up to size 10at a randomly chosen location of the seed string with each character being aprintablevalue6 or remove arandomly chosencharacter from the seedstring.
for programs with more than one input argument we additionallyaddrulesspecifyingwhetheroneoftheinputsisasubstring of another input argument.
we then randomly choose a substring fromthatinputargumentwhensynthesizingexamples.forinteger inputs werandomlychoseeitheranintegerboundedbythelength of one of the string input arguments or a randomly chosen integer from .
this ensuresthat thetarget program can be runon the mutatedinputswithoutresultingintype errorsorfailure.
.
accuracyimprovement toevaluatewhetherourtheoreticalgeneralizationguaranteestranslate into improved correctness we check that the synthesized programiscorrect i.e.
syntacticallyorsemanticallyequivalent to the targetprogram.oursyntacticequivalenceisconfirmedautomatically andforsemanticequivalence weresorttomanualinspection.
whilesynguar mightproduceprograms that are closetothetarget asper its close guarantee it is difficultto estimate closeness objectively.therefore wetakeaconservativeapproachandonly reportwhether thesynthesizedprogram iscorrect.programsthat are almost or close to correctare reportedas incorrect.
synguar prose synthesizes 16programssemanticallyequivalenttothetargetprograminall 32runsfor .
.02which showsthat synguar prose isusefultosynthesizecommonstring manipulation programs.
for one of the remaining benchmarks the synthesized program is correct on 32runs.
for 1benchmark thesynthesizedprogramiscorrecton 32runs.intotal synguarproseproduces correctprograms in .
runs.
weobservethatthevanilla strprose synthesizer without synguar consistentlyoverfitswhensamplesizesarechosenarbitrarily smallerthanmandatedby synguar prose .forinstance whenwe 6in python weuse string.printable and removethe whitespace charactersp1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p13 p14 p15 p16100200300400500600700800 programin prose benchmarknumberof samples .
.02 .
.02 .
.
figure for most programs in the prose benchmark synguar prose synthesizes programs with provable generalization under 400examples for .
.
.
for .
.
the samplesdropto average .
.
useexactly 4randomlychosenexamplesineachtrial thevanilla strprose synthesizer produces incorrect programs on most of the32runs for all target programs.
most of the synthesized programsoverfittheexamples itisonlycorrecton .
runs in total.
this confirms the importance of synguar s main objective takingenoughexamplesuntilthesynthesizedprogram isguaranteedto generalize withhigh probability.
synguar stun synthesizes 59correctprogramsfromthe sygus stun for .
.02forall3runs.intotal thisleads to159 .
correct runs.
as a point of comparison the vanillaversion strstun synthesizer evaluatedontheexamples providedin the sygus benchmark produces correct programsfor 59ofthe target benchmark.
synguar stun shows a29 improvementoverthevanilla strstun onthesygusbenchmark synthesizingcorrectly an additional 17programs inalltrials.further thissuggeststhata significantnumberof sygus stun programs in the benchmarkdo nothave enough examplesin the benchmark toprovablygeneralize.synthesizers therefore mayneedadditional hints orassumptions to solve themcorrectly.
toanalyzevanilla strstun underthesameinputdistribution assynguar stun we further evaluate it on a fixed number of randomly chosen examples in 3trials.
we use sample size of 4per trialfollowingpriorwork .wefindthatvanilla strstun synthesizes only 59correct programs in all 3runs in total correcton 177runs confirming that itoften overfits.
.
samplesizesufficient forgeneralization our work provides empirical evidence that a modest number of examplessufficeforprovablegeneralizationfortheevaluatedtasks.
figure4shows that we need between 400examples about 197on average to achieve .
.
generalization for thesynguar prose on the16target programs evaluated.
for a smallererrortolerance .
.
theobservedrangeof samplesizebecomes .notethatsamplesizeissensitiveto as is theoretically expected distinguishing between two functions 685esec fse august athens greece bowang teodora baluta aashishkolluri andprateek saxena program in sygus stunnumberof samples .
.
figure for most programs in the sygus stun synguarstunsynthesizes programs with provable generalization under500examples for .
.
.
only11of these programs require 1400examples.
that behave almost identically using random sampling will require manysamples.
figure5shows that we need between 1400examples about 357on average to achieve .
.
generalization for the59target programs evaluated with synguar stun .
most of the programsin the sygus benchmarkrequire under 500samples withonly 11programs requiring 1400i o examples.
.
reductionin programsearch space note that theorem .1doesnotpredict how fast the procedure willconvergetoageneralizedprogram i.e.
howfastthespaceof consistent programs will shrink after each example.
this varies empirically given the task and examples seen.
but synguar internally estimates conservatively how many programs remain consistent after seeing each example.
from this two empirical findings which explain our other observations emerge.
first the programspaceshrinks drastically withthefirstfewexamplesfor nearlyallbenchmarks.second thenumberofexamplesrequired to generalize depends significantly on the expressiveness of the chosenhypothesisspace.
figure6shows the size of the consistent program space for 6representative programs in the prose benchmark computed bysynguar prose .
the y axis is a logarithmic scale.
the full evaluation of the rest of the programs is in the supplementary material .we choose these programsasthey representthe largest smallest andaveragecasesoftheprogramspacesizeafter 25examplesaveragedover 32runs aswellasthelargestandsmallest programspacesizedecreaseafter 5examples.inparticular p 15has the largest decreaseonaverage inthe program space sizeafter examplesfrom 1050to106whilep13hasthesmallestdecreaseafter 5examples from .
103to27.
this observation explains why a smallnumberofexamplesturnouttobesufficientforgeneralizationinthisbenchmark.italsoshowsthatreducingtheconsistent hypothesis space further after the initial quick reduction becomes increasingly difficult withunbiasedsampling.
numberofsamplesprogram space size p2p5p11 p12 p13 p15 figure in prose benchmark the program space shrinks .
onaveragewiththefirst 5examples explainingwhy synguar prose can provablygeneralizeinmodest numberofsamplesformostprograms inthe benchmark.
the sygus benchmark has target programs of different complexity differentnumberofconditionals .for synguar stun the numberofsamplesrequiredtogeneralizedependsontargetprogram s complexity more complex programs require considering a largeroriginalhypothesisspacetoberepresented.however thealgorithmdoesnotknowtheoriginalhypothesisspace.tousefewer samples the algorithm chooses the smallest hypothesis space that stillcontainsprogramsconsistentwithsamples becauseoutputting morecomplexprograms moreconditionals requireschoosinga larger hypothesisspacefor whichmore samples are needed.
we use the target programs in h0as an example to show this phenomenoninfigure7.thefigureshowsforeachtargetprogram thesamplesizesufficientfor generalizationcalculatedby the3parallel synguar instanceson h0 h1 andh2.weshowthatif synguar stun choosesaprogramin h0 thenumberofexamples is141 .ifthetargetprogramisin h0butthesynthesizerchooses a program in h1 the number of samples is larger by .22on average.
for example for program 20in our3runs 213samples aresufficienttopickaprogramin h0 buttoreturnaprogramfrom h1orh2synguar stun requiresaround 1300and2400examples respectively.
this quantitatively shows that the sample size can vary by a large margin when considering more complex programs.
moreover this result explains why choosing a simpler program firstcan require asmallernumber of examples.
related work the overfitting problem in learning programs from examples is known.
many different approaches have been proposed to tackle it see section .
one lineof work proposesconditioning thesearch with program traces rather than just i o examples .
anotherlineofworkimprovestheinputspecification usingdomain specificknowledgeaboutthehypothesisspace.singh et al.
propose to rank the synthesized functions based on distributional priors with machine learning .
similarly several inductive synthesis techniques use deep learning to improve their 686synguar guaranteeing generalizationin programmingby example esec fse august athens greece programin sygus stunnumberofsamples p h0p h1p h2 figure for target programs with no condition t h0 choosing a program p h0 versus a program with one condition h1 or two conditions h2 leads to provable generalization inless numberofsamples.
search .
broadly speaking these approaches are complementary to ours as they either require domain specific priors or theylearnprogrampatternsfromadatasetofsimilarprograms .
moreover none ofthemclaims any generalization guarantees.
several works suggest that larger test harnesses lead to promisingimprovementsinsynthesizedprograms whichisalso observed in related domains of program repair and invariantlearning .ourwork motivatedbythese providestheformal bridgebetweentestharnesssizeandprovablegeneralization.we alsoshowthatittranslatesintosignificantlyimprovedaccuracyon string manipulating tasks directlydueto reducedoverfitting.
establishinggeneralizationguaranteesforsynthesizershasbeen studied for over three decades.
the pac learnability framework has been introduced by valiant foranalyzing generalization fromacomputationalperspective.underthistheory thesample complexity required for generalization has been established for learninginpropositionallogicdomain .theresultshave been extended to learning logic programs i.e.
predicate logic domain .
some oftheabove boundsarelimitedtocertain typesofhypothesisspaces.instead blumeretal.havegiventwo sample complexity bounds union bound and vapnikchervonenkis dimension bound that are more general.
the union bound is used when the hypothesis space is finite and when the vc dimension of a model is difficult to estimate .
whilst the above works have established the bounds in theory their applicability in real world synthesizers have been very limited.
a recent work uses the vc dimension argument for synthesizing linear arithmetic functions for holes given in a sketch .
for many programmingdomains suchas forstring manipulatingprograms itisdifficult to compute vc dimensions.
besidesthepacframework anotherlineofworktowardsgeneralizationisthroughactivelearning.someinteractivesynthesis systemshavequestionselectionmechanismstofinddistinguishing input .ji et al.further approximate optimal questions to resolveambiguityinlessnumberofsamples .however theseapproaches do not give generalization guarantees without assuming the existence of the target programs in the hypothesis space or apriordistributionovertargetprograms sotheyareorthogonalto our approach whichworks underminimal assumptions.
outside of program synthesis generalization has been extensively studied in machine learning.
our work bridges the two lines ofinquirythathaveevolvedinparallel.apartfrompac styledefinitionsbasedonsamplecomplexity generalizationcanbeachievedusingalgorithmicstability .boundshavebeenestablishedforboth convexoptimizationsandnon convexoptimizationalgorithms i.e.
lowsensitivitytosmallchangesininputs .these worksleveragethepropertiesofalgorithmslikestochasticgradient descent sgd andstochasticgradientlangevindynamic sgld in order to estimate the generalization bounds for a given number of samples.
adapting the framework of algorithmic stability to pbe based synthesis is promising future work but it is challenging.
a direct adaptation for example would restrict learnt programstobestable forwhichsmallchangesinoutputsforsmall changes in inputs.
generalization has been explored from other perspectives such as by boundingnetwork capacity and overparametrization in machine learning literature which arealsoalternativestartingpointsforstudyinggeneralizationin program synthesis.
conclusion in this work we exploit the theoretical connection between generalizationandthenumbersofexamplesusedinprogramming byexamplesynthesis.weprovidethefirstprincipledapproachthat guarantees generalization with a modest number of examples in this regime.
key to this result is our mechanism for computing sample complexity on the fly.
we show experimentally that this significantly reduces overfitting and improves accuracy for synthesizing string manipulation programs compared to approaches that use arbitrarilyfewer examples.