early life cycle software defect prediction.
why?
how?
n.c. shrikanth suvodeep majumder and tim menzies department of computer science north carolina state university raleigh usa snaraya7 ncsu.edu smajumd3 ncsu.edu timm ieee.org abstract many researchers assume that for software analytics more data is better.
we write to show that at least for learning defect predictors this may not be true.
to demonstrate this we analyzed hundreds of popular github projects.
these projects ran for months and contained commits median values .
across these projects most of the defects occur very early in their life cycle.
hence defect predictors learned from the first commits and four months perform just as well as anything else.
this means that at least for the projects studied here after the first few months we need not continually update our defect prediction models.
we hope these results inspire other researchers to adopt a simplicity first approach to their work.
some domains require a complex and data hungry analysis.
but before assuming complexity it is prudent to check the raw data looking for short cuts that can simplify the analysis.
index terms sampling early defect prediction analytics i. i ntroduction this paper proposes a data lite method that finds effective software defect predictors using data just from the first of a project s lifetime.
our new method is recommended since it means that we need not always revise defect prediction models even if new data arrives.
this is important since managers educators vendors and researchers lose faith in methods that are always changing their conclusions.
our method is somewhat unusual since it takes an opposite approach to data hungry methods that e.g.
use data collected across many years of a software project .
such datahungry methods are often cited as the key to success for data mining applications.
for example in his famous talk the unreasonable effectiveness of data google s chief scientist peter norvig argues that billions of trivial data points can lead to understanding a claim he supports with numerous examples from vision research .
but what if some software engineering se data was not like norvig s data?
what if se needs its own ai methods based on what we learned about the specifics of software projects?
if that were true then data hungry methods might be needless over elaborations of a fundamentally simpler process.
this paper shows that for one specific software analytics task learning defect predictors we do not need a data hungry approach.
we observe in figure that while the median lifetime of many projects is months most of the defects from those projects occur much earlier than that.
that is very little of the defect experience occurs later in the life cycle.
hence predictors learned after months the vertical greendotted line in figure do just as well as anything else i.e.
learning can stop after just of the life cycle i.e.
months .
that is to say when learning defect predictors of the time we do not want and we do not need data hungry methods .
we stress that we have only shown an early data is enough effect in the special case of a defect prediction for b longrunning non trivial engineering github projects studied here what munaiah et al.
would call organizational projects .
such projects can be readily identified by how many stars approval marks they have accumulated from github users.
like other researchers e.g.
see the tse article by yan et al.
we explore projects with at least stars.
that said even within these restrictions we believe we are exploring an interesting range of projects.
our sample includes numerous widely used applications developed by elastic search engine1 google core libraries2 numpy scientific computing3 etc.
also our sample of projects is written in widely used programming languages including c c java c ruby python javascript and php.
nevertheless in future work we need to explore the external validity of our results to other se tasks other than defect prediction and for other kinds of data.
for example abdalkareem et al.
show that up to of python and javascript packages are trivially small their terminology i.e.
have less than lines of code.
it is an open issue if our methods work for other kinds of software such as those trivial javascript and python packages.
to support such further explorations we have placed all our data scripts on line4.
the rest of this paper is structured as follows.
in we discuss the negative consequences of excessive data collection then in we show that for hundreds of github projects the defect data from the latter life cycle defect data is relatively uninformative.
this leads to the definition of experiments in the early life cycle defect prediction see .
from those experiments in we show that at least for defect prediction a small sample of data is useful but in contrast to norvig s claim more data is notmore useful.
lastly discusses some threats and conclusions are presented in .
4for a replication package see ieee acm 43rd international conference on software engineering icse .
ieee fig.
.
million commits for github projects.
black red shaded clean defective commits.
in this paper we compare a models learned up to the vertical green dotted line to b models learned using more data.
ii.
b ackground a. about defect prediction defect prediction uses data miners to input static code attributes and output models that predict where the code probably contains most bugs .
wan et al.
reports that there is much industrial interest in these predictors since they can guide the deployment of more expensive and time consuming quality assurance methods e.g.
human inspection .
misirili et al and kim et al.
report considerable cost savings when such predictors are used in guiding industrial quality assurance processes.
also rahman et al.
show that such predictors are competitive with more elaborate approaches.
in defect prediction data hungry researchers assume that if data is useful then even more data is much more useful.
for example ..as long as it is large the resulting prediction performance is likely to be boosted more by the size of the sample than it is hindered by any bias polarity that may exist .
it is natural to think that a closer previous release has more similar characteristics and thus can help to train a more accurate defect prediction model.
it is also natural to think that accumulating multiple releases can be beneficial because it represents the variability of a project .
long term jit models should be trained using a cache of plenty of changes .
not only are researchers hungry for data but they are also most hungry for the most recent data.
for example hoang et al.
say we assume that older commits changes may have characteristics that no longer effects to the latest commits .
also it is common practice in defect prediction to perform recent validation where predictors are tested on the latest release after training from the prior one or two releases .
for a project with multiple releases recent validation ignores any insights that are available from older releases.b.
problems with defect prediction conclusion instability if we revise old models whenever new data becomes available then this can lead to conclusion instability where new data leads to different models .
conclusion instability is well documented.
zimmermann et al.
learned defect predictors from pairs of projects project1 project2 .
in only of pairs predictors from project1 worked on project2.
also menzies et al.
studied defect prediction results from recent studies most of which offered widely differing conclusions about what most influences software defects.
menzies et al.
reported experiments where data for software projects are clustered and data mining is applied to each cluster.
they report that very different models are learned from different parts of the data even from the same projects.
in our own past work we have found conclusion instability meaning there we had to throw years of data.
in one sample of github data we sought to learn everything we could from commits.
the web slurping required for that process took nearly days of cpu using five machines with cores over days .
within that data space we found significant differences in the models learned from different parts of the data.
so even after all that work we were unable to offer our business users a stable predictor for their domain.
is that the best we can do?
are there general defect prediction principles we can use to guide project management software standards education tool development and legislation about software?
or is se some patchwork quilt of ideas and methods where it only makes sense to reason about specific specialized and small sets of related projects?
note that if the software was a patchwork of ideas then there would be no stable conclusions about what constitutes best practice for software engineering since those best practices would keep changing as we move from project to project .
such conclusion instability would have detrimental implications for trust insight training and tool development .
trust conclusion instability is unsettling for project managers.
hassan warns that managers lose trust in software analytics if its results keep changing.
such instability prevents project managers from offering clear guidelines on many is449sues including a when a certain module should be inspected b when modules should be refactored and c deciding where to focus on expensive testing procedures.
insight sawyer et al.
assert that insights are essential to catalyzing business initiative .
from kim et al.
perspective software analytics is a way to obtain fruitful insights that guide practitioners to accomplish software development goals whereas for tan et al.
such insights are a central goal.
from a practitioner s perspective bird et al.
report insights occur when users respond to software analytics models.
frequent model generation could exhaust users ability for confident conclusions from new data.
tool development and training shrikanth and menzies warns that unstable models make it hard to onboard novice software engineers.
without knowing what factors most influence the local project it is hard to design and build appropriate tools for quality assurance activities all these problems with trust insight training and tool development could be solved if early on in the project a defect prediction model can be learned that is effective for the rest of the life cycle.
as mentioned in the introduction we study here github projects spanning months and containing commits median values .
within that data we have found that models learned after just commits and four months of data collection perform just as well as anything else.
in terms of resolving conclusion instability this is a very significant result since it means that for of the life cycle we can offer stable defect predictors.
one way to consider the impact of such early life cycle predictors is to use the data of figure .
that plot shows that software employees usually change projects every months either moving between companies or changing projects within an organization .
this means that in seven years months the majority of workers and managers would first appear on a job after the initial four months required to learn a defect predictor.
hence for most workers and managers the detectors learned via the methods of this paper would be fig.
work duration histograms on particular projects from .
data from facebook ebay apple 3m intel and motorola.the established wisdom and the way we do things here for their projects.
this means that a detector learned in the first four months would be a suitable oracle to guide training and hiring the development of code review practices the automation of local bad smell detectors as well as tool selection and development.
iii.
w hyearly defect prediction might work a. github results recently shrikanth and menzies found defectprediction beliefs not supported by available evidence .
we looked for why such confusions exist which lead to the discovery of that pattern in figure of project data changes dramatically over the life cycle.
figure shows data from .2m github commits from popular github projects the criteria for selecting those particular projects is detailed below .
note how the frequency defect data shown in red shaded starts collapsing early in the life cycle.
this observation suggests that it might be relatively uninformative to learn from later life cycle data.
this was an interesting finding since as mentioned in the introduction it is common practice in defect prediction to perform recent validation where predictors are tested on the latest release after training from the prior one or two releases .
in terms of figure that strategy would train on red dots shaded taken near the right hand side then test on the most righthand side dot.
given the shallowness of the defect data in that region such recent validation could lead to results that are not representative of the whole life cycle.
accordingly we sat out to determine how different training and testing sampling policies across the life cycle of figure affected the results.
after much experimentation described below we assert that if data is collected up until the vertical green line of figure then that generates a model as good as anything else.
b. related work before moving on we first discuss related work on early life cycle defect prediction.
in fenton et al.
explored the use of human judgment rather than data collected from the domain to handcraft a causal model to predict residual defects defects caught during independent testing or operational usage .
fenton needed two years of expert interaction to build models that compete with defect predictors learned by data miners from domain data.
hence we do not explore those methods here since they were very labor intensive.
in zhang and wu showed that it is possible to estimate the project quality with fewer programs sampled from an entire space of programs covering the entire project lifecycle .
although we too draw fewer samples commits we sample them early in the project life cycle to build defect prediction models.
in another study about sample size rahman et al.
stress the importance of using a large sample size to overcome bias in defect prediction models .
we find our proposed data lite approach performs similar to data hungry approaches while we do not deny bias in defect 450table i papers discussing different sampling policies.
all papers that utilize all historical data to build defect prediction models shaded in gray .
paper year citations sampling projects paper year citations sampling projects paper year citations sampling projects all release all all release percentage percentage release release all all percentage all month month all all percentage all percentage slice all all all release all percentage release percentage release all all release all all all slice release all all prediction data sets.
our proposed approach and recent defect prediction work handle bias by balancing defective and nondefective samples class imbalance .
recently arokiam and jeremy explored bug severity prediction.
they show it is possible to predict bugseverity early in the project development by using data transferred from other projects .
their analysis was on the cross projects but unlike this paper they did not explore just how early in the life cycle did within project data became effective.
in similar work to arokiam and jeremy in sousuke explored another early life cycle crossversion defect prediction cvdp using cross project defect prediction cpdp data.
their study was not as extensive as ours only releases .
cvdp uses the project s prior releases to build defect prediction models.
sousuke compared defect prediction models trained using three within project scenarios recent project release all past releases and earliest project release to endorse recent project release.
sousuke also combined cvdp scenarios using cpdp approaches to recommend that the recent project release was still better than most cpdp approaches.
however unlike sousuke we offer contrary evidence in this work as our endorsed policy based on earlier commits works similar to all other prevalent policies including the most recent release reported in the literature.
notably we assess our approach on releases and evaluate on seven performance measures.
in summary as far as we can tell ours is the first study to perform an extensive comparison of prevalent sampling policies practiced in the defect prediction space.
iv.
s ampling polices one way to summarize this paper is to evaluate a novel stop early sampling policy for collecting the data needed for defect prediction.
this section describes a survey of sampling policies in defect prediction.
each sampling policy has its way of extracting training and test data from a project.
as shown below there is a remarkably diverse number of policies in the literature that have not been systematically and comparatively evaluated prior to this paper.
in april we found articles in google scholar using the query software and defect prediction and just in time software and defect prediction and fig.
summary of sampling types from table i. sampling policy .
just in time jit defect prediction is a widely used approach where the code seen in each commit is assessed for its defect proneness .
from the results of that query we applied some temporal filtering we examined all articles more recent than ii for older articles we examined all papers from the last years with more than citations per year.
after reading the title abstracts and the methodology sections we found the articles of table i that argued for particular sampling policies.
figure shows a high level view of the sampling policies seen in the table i papers all when the historical data commits files modules etc is used for evaluation within some cross validation study where the data is divided randomly into nbins and the data from bin i2n is used to test a model trained from all other data .
percentage the historical data is stratified by some percentage like .
the minimum we found was .
release the models are trained on the immediate or more past releases in order to predict defects on the current release .
month when or months of historical data is used to predict defects in future files commits or release .
slice an arbitrary stratification is used to divide the data based on a specific number of days like days or six months in .
it turns out figure is only an approximation of the diverse number sampling policies we see in the literature.
a more comprehensive picture is shown in figure where we divide 451fig.
a visual map of sampling.
project time line divided into train commits and test commits .
learners learn from train to classify defective commits in the test .
software releases rithat occur over many months mjinto some train andtestset.
using a little engineering judgment and guided by the frequency of different policies from figure we elected to focus on four sampling policies from the literature and one early stopping policy see table ii .
the share in figure show all and rr are prevalent practices whereas m3 and m6 though not prevalent are used in related literature .
we did not consider separate policies for percentage and slice as the former is similar to all of historical data and the latter is least prevalent and similar to m6 days or six months .
note the magic numbers in table ii months months these are thresholds often seen in the literature.
clean defective commits we arrived at these numbers based on the work of nam et al.
built defect prediction models for using just samples .
sampling at random from the first commits.
here we did some experiments recursively dividing the data in half until defect prediction stopped working.
we will show below that early sampling shown in gray in table ii works just as well as the other policies.
v. m ethods a. data this section describes the data used in this study as well as what we mean by clean and defective commits.
all our data comes from open source os github projects that we mined randomly using commit guru .
fig.
distributions seen in all .
millions commits of all projects median values of commits percent of defective commits life span in years releases and stars .
commit guru is a publicly available tool based on a esec fse paper used in numerous prior works .
commit guru provides a portal where it takes a request url to process a github repository.
it extracts all commits and their features to be exported to a file.
commits are categorized based on the occurrence of certain keywords similar to the approach in szz algorithm .
the defective buginducing commits are traced using the git diff show changes between commits feature from bug fixing commits the rest are labeled clean .
but data from commit guru does not contain release information which we extract separately from the project tags.
then we use scripts to associate the commits to the release dates.
then those codes associated with those changes were then summarized by commit guru using the attributes of table iii.
those attributes became the independent attributes used in our analysis.
note that the use of these particular attributes has been endorsed by prior studies .
se researchers have warned against using all github data since this website contains many projects that might be categories as non serious such as homework assignments .
accordingly following the advice of prior researchers we ignored projects with less than stars less than defects less than two releases less than one year of activity no license information.
less than defective and clean commits.
this resulted in projects developed written in many languages across various domains as discussed in i. figure shows information on our selected projects.
as shown in that figure our projects have median life spans of months with releases the projects have commits min median max with data up to december 452table ii four representative sampling policies from literature and an early life cycle policy the row shown in gray .
policy method all train using all past software commits ri in the project before the first commit in the release under test ri.
m6 train using the recent six months of software commits ri 6months made before the first commit in the release under test ri.
m3 train using the recent three months of software commits ri 3months made before the first commit in the release under test ri.
rr train using the software commits in the previous release ri 1before the first commit in the release under test ri.
e train using early commits clean and defective randomly sampled within the first commits before the first commit in the release under test ri.
table iii commit level features that commit guru tool mines from github repositories dimension feature definition ns number of modified subsystems nd number of modified directories nf number of modified filesdiffusion entropy distribution of modified code across each file la lines of code added ld lines of code deleted size lt lines of code in a file before the change purpose fix whether the change is defect changes that fixing the defect are more likely to introduce more defects fixing ?
ndev number of developers that changed the modified files age the average time interval from the last to the current change history nuc number of unique changes to the modified files before exp developer experience rexp recent developer experience experience sexp developer experience on a subsystem fig.
distributions seen in the first commits of all projects median values of project releases project development months and defective commits median of project commits introduce bugs.
figure focuses on just the data used in the early life cycle e sampler described in figure .
in the median case by the time we can collect commits projects have had five releases in months median value .
b. algorithms our study uses three sets of algorithms the five sampling policies described above the six classifiers described here pre processing for some of the sampling policies.
classifiers after an extensive analysis ghotra et al.
rank over defect prediction algorithms into four ranks.
for our work we take six of their learners that are widely used in the literature and which can be found at all four ranks of the ghtora et al.
study.
those learners were logistic regression lr nearest neighbour knn minimum neighbors decision tree dt random forrest rf na ve bayes nb support vector machines svm pre processers following some advice from the literature we applied some feature engineering to the table iii data.
we normalized la and ld by dividing by lt and normalized lt and nuc by dividing by nf then we dropped nd and rexp since they reported that nf and nd are highly correlated with rexp and exp .
lastly we applied the logarithmic transform to the remaining process measures except the boolean variable fix to alleviate skewness .
in other pre processing steps we applied correlation based feature selection cfs .
our initial experiments with this data set lead to unpromising results recalls less than high false alarms .
however those results improved after we applied feature subset selection to remove spurious attributes.
cfs is a widely applied feature subset selection method proposed by hall and is recommended in building supervised defect prediction models .
cfs is a heuristic based method to find evaluate a subset of features incrementally.
cfs performs a best first search to find influential sets of features that are not correlated with each other however correlated with the classification.
each subset is computed as follows merits krcf p k k k r where meritsis the value of subset swithkfeatures rcfis a score that explains the connection of that feature set to the class r is the feature to feature mean and connection between the items in s wherercfshould be large and r .
another pre processor that was applied to some sampling policies was synthetic minority over sampling or smote.
when the proportion of defective and clean commits or modules files etc.
is not equal learners can struggle to find the target class.
smote proposed by chawla et al.
is often applied in defect prediction literature to overcome this problem .
to achieve balance smote artificially synthesizes examples commits extrapolating using k nearest neighbors minimum five commits required in the data set training commits in our case .
note that we do not apply smote to policies that already guarantee class balancing.
for example our preferred early lifecycle method selects at random defective and nondefective clean commits from the first commits.
also just to document that we avoided a potential methodological error we record here that we applied smote to the training data but never the test data.
c. evaluation criteria defect prediction studies evaluated their model performance using a variety of criteria.
from the literature we used what we judged to be the most widely used measures .
for the following seven criteria nearly all have the range to except initial number of false alarms which can be any positive number four of these criteria need to be minimized d2h ifa brier pf i.e.
for these criteria lessisbetter .
three of these criteria need to be maximized auc recall g measure i.e.
for these criteria more isbetter .
one reason we avoid precision is that prior work shows this measure has significant issues for unbalanced data .
brier recent defect prediction papers measure the model performance using the brier absolute predictive accuracy measure.
let cbe the total number of the test commits.
let yibe for defective commits or otherwise.
let yibe the probability of commit being defective calculated from the loss functions in scikit learn classifiers .
then brier ccx t yi yi initial number of false alarms ifa parnin and orso say that developers lose faith in analytics if they see too many initial false alarms.
ifa is simply the number of false alarms encountered after sorting the commits in the order of probability of being detective then counting the number of false alarms before finding the first true alarm.
recall recall is the proportion of inspected defective commits among all the actual defective commits.
recall true positives true positives false negatives false positive rate pf the proportion of predicted defective commits those are not defective among all the predicted defective commits.
pf false positives false positives true negatives area under the receiver operating characteristic curve auc auc is the area under the curve between the true positive rate and false positive rate.
distance to heaven d2h d2h or distance to heaven aggregates on two metrics recall and false positive rate pf to show how close to heaven recall and pf .
d2h p recall pf p g measure gm a harmonic mean between recall and the compliment of pf measured as shown below.
g measure recall pf recall pf even though gm and d2h combined the same underlying measures we include both here since they both have been used separately in the literature.
also as shown below it is not necessarily true that achieving good results on gm means that good results will also be achieved with d2h.
due to the nature of the classification process some criteria will always offer contradictory results a learner can achieve recall just by declaring that all examples belong to the target class.
this method will incur a high false alarm rate.
a learner can achieve false alarms just by declaring that no examples belong to the target class.
this method will incur a very low recall rate.
similarly brier and recall are also antithetical since reducing the loss function also means missing some conclusions and lowering recall.
d. statistical test later in vi we compare distributions of evaluation measures of various sampling policies that may have the same median while their distribution could be very different.
hence to identify significant differences rank among two or more populations we use the scott knott test recommended by mittas et al.
in tse paper .
this test is a top down biclustering approach for ranking different treatments sampling policies in our case.
this method sorts a list of lsampling policy evaluations with lsmeasurements by their median score.
it then splits linto sub lists m n in order to maximize the expected value of differences in the observed performances before and after divisions.
for listsl m n of size ls ms nswherel m n the best division maximizes e i.e.
the difference in the expected mean value before and after the spit e ms lsabs m l ns lsabs n l we also employ the conjunction of bootstrapping and a12 effect size test by vargha and delaney to avoid small effects with statistically significant results.
important note we apply our statistical methods separately to all the evaluation criteria i.e.
when we compute ranks we do so for say false alarms separately to recall.
e. experimental rig by definition our different sampling policies have different train and different test sets.
but methodologically when we compare these different policies we have to compare results on the same sets of releases.
to handle this we first run all our six policies combined with all our six learners.
this offers multiple predictions to different commits.
next for each release we divide the predictions into those that come from the same learner policy pair.
these divisions are then assessed with statistical methods described above.
vi.
r esults tables iv and v show results when our six learners applied our five sampling policies.
we plot these results into two tables since our policies lead to results with different samples sizes the recent release or rr the policy uses data from just two releases while all uses everything.
in the first row of those tables and denote criteria that need to be maximized or minimized respectively.
within the tables gray cells show statistical test results conducted separately on each criterion .
anything ranked best is colored gray while all else have white backgrounds.
columns one and two show the policy learners that lead to these results.
rows are sorted by how often policy learners win i.e.
achieve best ranks across all criteria.
in tables iv and v no policy learner wins out of times on all criteria so some judgment will be required to make an overall conclusion.
specifically based on results from the multi objective optimization literature we will first remove the policies learners that score worse on most criteria then debate trade offs between the rest.
to simplify that trade off debate we offer two notes.
firstly across all our learners the median value for ifa is very small zero or one i.e.
developers using these tools only need to suffer one false alarm or less before finding something they need to fix.
since these observed ifa scores are so small we say that losing on ifa is hardly a reason to dismiss a learner sampler combination.
secondly d2h and gm combine multiple criteria.
for example winning on d2h and gm means performing well on both recall and pf i.e.
these two criteria are somewhat more informative than the others.
turning now to those results we explore two issues.
for defect prediction rq1 is more data better?
rq2 when is more recent data better than older data?
note that we do explore a third research issue are different learners better at learning from a little a lot or all the available data.
based on our results we have nothing definitive to offer on that issue.
that said if we were pressed to recommend a particular learning algorithm then we say there are no counterexamples to the claim that it is useful to apply cfs lr .
rq1 is more data better?
belief1 our introduction included examples where proponents of data hungry methods advocated that if data is useful then even more data is much more useful.
prediction if that belief was the case then in table iv data hungry sampling policies that used more data should defeat data lite sampling policies.
observation1a in table iv our data hungriest sampling policy all loses on most criteria.
while it achieves thehighest recall it also has the highest false alarm range .
as to which other policy is preferred in the best wins zone of table iv there is no clear winner.
what we would say here is that our preferred data lite method called e that uses defective and non defective commits selected at random from the first commits is competitive with the rest.
hence answer1a for defect prediction it is not clear that more data is inherently better.
observations1b figure of this paper showed that within our sample of projects we have data lasting a median of months.
figure noted that by the time we get to commits most projects are months old median value .
the e results of table iv showed that defect models learned from that months of data are competitive with all the other policies studied here.
hence we say answer1b of the time we do not want and we do not need data hungry methods rq2 when is more recent data better than older data?
belief2 as discussed earlier in our introduction many researchers prefer using recent data over data from earlier periods.
for example it is common practice in defect prediction to perform recent validation where predictors are tested on the latest release after training from the prior one or two releases .
for a project with multiple releases recent validation ignores all the insights that are available from older releases.
prediction2 if recent data is comparatively more informative than older data then defect predictors built on recent data should out perform predictors built on much older data.
observations2 we observe that figure of this paper showed that within our sample of projects we have data lasting a median of months.
figure noted that by the time we get to commits most projects are months old median value .
table v says that e wins over rr since it falls in the best wins section.
hence we could conclude that older data is more effective than recent data.
that said we feel somewhat more the circumspect conclusion is in order.
when we compare e lr to the next learner in that table rr nb we only find a minimal difference in their performance scores.
hence we make a somewhat humbler conclusion answer2 recency based methods perform no better than results from early life cycle defect predictors.
this is a startling result for two reasons.
firstly compared to the rr training data the e training data is very old indeed.
for projects lasting months long rr is trained 455on information from recent few months with e data comes from years before that.
secondly this result calls into question any conclusion made in a paper that used recent validation to assess their approach e.g.
.
vii.
t hreats to validity a. sampling bias the conclusion s generalizability will depend upon the samples considered i.e.
what matters here may not be true everywhere.
to improve our conclusion s generalizability wemined long running os projects that are developed for disparate domains and written in numerous programming languages.
sampling trivial projects like homework assignments is a potential threat to our analysis.
to mitigate that we adhered to the advice from prior researchers as discussed earlier in i and v a. we find our sample of projects have median defects as shown in figure nearly the same as data used by tantithamthavorn et al.
who report median defects.
table iv defect prediction models tested in all applicable project releases.
in the first row and denote the criteria that need to be maximized or minimized respectively.
wins is the frequency of the policy found in the top scott knott rank in each of the seven evaluation measures the cells shaded in gray .
policy classifier wins d2h auc ifa brier recall pf gm m6 nb .
.
.
.
.
.
.
m3 nb .
.
.
.
.
.
.
e lr .
.
.
.
.
.
.
m6 svm .
.
.
.
.
.
.
m3 svm .
.
.
.
.
.
.
all nb .
.
.
.
.
.
.
e knn .
.
.
.
.
.
.
all lr .
.
.
.
.
.
.
m6 lr .
.
.
.
.
.
.
m3 lr .
.
.
.
.
.
.
m6 knn .
.
.
.
.
.
.
all svm .
.
.
.
.
.
.
m3 knn .
.
.
.
.
.
.
m6 rf .
.
.
.
.
.
.
e svm .
.
.
.
.
.
.
all knn .
.
.
.
.
.
.
all dt .
.
.
.
.
.
.
m6 dt .
.
.
.
.
.
.
all rf .
.
.
.
.
.
.
m3 dt .
.
.
.
.
.
.
m3 rf .
.
.
.
.
.
.
e dt .
.
.
.
.
.
.
e nb .
.
.
.
.
.
.
e rf .
.
.
.
.
.
.
key more data all m6 and m3 early e table v defect prediction models tested on project releases.
in the first row and denote criteria that need to be maximized or minimized respectively.
wins is the frequency of the policy found in the top scott knott rank in each of the seven evaluation measures the cells shaded in gray .
policy classifier wins d2h auc ifa brier recall pf gm e lr .
.
.
.
.
.
.
rr nb .
.
.
.
.
.
.
rr lr .
.
.
.
.
.
.
rr svm .
.
.
.
.
.
.
e knn .
.
.
.
.
.
.
rr knn .
.
.
.
.
.
.
rr rf .
.
.
.
.
.
.
e svm .
.
.
.
.
.
.
e dt .
.
.
.
.
.
.
e nb .
.
.
.
.
.
.
e rf .
.
.
.
.
.
.
rr dt .
.
.
.
.
.
.
key recency rr early e 456b.
learner bias any single study can only explore a handful of classification algorithms.
for building the defect predictors in this work we elected six learners logistic regression nearest neighbor decision tree support vector machines random forrest and na ve bayes .
these six learners represent a plethora of classification algorithms .
c. evaluation bias we use seven evaluation measures recall pf ifa brier gm d2h and auc .
other prevalent measures in this defect prediction space include precision.
however as mentioned earlier precision has issues with unbalanced data .
d. input bias our proposed sampling policy e randomly samples commits from early commits.
thus it may be true that different executions could yield different results.
however this is not a threat because each time the early policy e randomly samples commits from early commits to test sizeable releases from table iv and table v across all the six learners.
in other words our conclusions about e hold on a large sample size of numerous releases.
viii.
c onclusion when data keep changing the models we can learn from that data may also change.
if conclusions become too fluid i.e.
change too often then no one has a stable basis for making decisions or communicating insights.
issues with conclusion instability disappear if early in the life cycle we can learn a predictive model that is effective for the rest of the project.
this paper has proposed a methodology for assessing such early life cycle predictors.
define a project selection criteria .
for this paper our selection criteria are taken from related work from recent emse tse papers select some software analytics task.
for this paper we have explored learning defect predictors.
see how early projects selected by the criteria can be modeled for that task.
here we found that defect predictors learned from the first four months of data perform as well as anything else.
conclude that projects matching criteria need more data fortask before time found in step .
in this paper we found that for of the time we neither want nor need data hungry defect prediction.
we stress that this result has only been shown here for defect prediction and only for the data selected by our criteria.
as for future work we have many suggestions the clear next step in this work is to check the validity of this conclusion beyond the specific criteria andtask explored here.
we need to revisit all prior results that used recent validation to assess their approach e.g.
since our rq2 suggests they may have been working in a relatively uninformative region of the data.
while the performance scores of tables iv and v are reasonable there is still much room for improvement.
perhaps if we augmented early life cycle defect predictors with a little transfer learning from other projects then we could generate better performing predictors.
further to the last point another interesting avenue of future research might be hyper parameter optimization hpo .
hpo is often not applied in software analytics due to its computational complexity.
perhaps that complexity can be avoided by focusing only on small samples of data from very early in the life cycle.