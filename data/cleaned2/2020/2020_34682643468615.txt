flex fixing flaky tests in machine learning projects by updating assertion bounds saikat dutta uiuc urbana usa saikatd2 illinois.eduaugust shi the university of texas at austin austin usa august utexas.edusasa misailovic uiuc urbana usa misailo illinois.edu abstract many machine learning ml algorithms are inherently random multiple executions using the same inputs may produce slightly different results each time.
randomness impacts how developers write tests that check for end to end quality of their implementations of these ml algorithms.
in particular selecting the proper thresholds for comparing obtained quality metrics with the reference results is a non intuitive task which may lead to flaky test executions.
we present flex the first tool for automatically fixing flaky tests due to algorithmic randomness in ml algorithms.
flex fixes tests that use approximate assertions to compare actual and expected values that represent the quality of the outputs of ml algorithms.
we present a technique for systematically identifying the acceptable bound between the actual and expected output quality that also minimizes flakiness.
our technique is based on the peak over threshold method from statistical extreme value theory which estimates the tail distribution of the output values observed from several runs.
based on the tail distribution flex updates the bound used in the test or selects the number of test re runs based on a desired confidence level.
we evaluate flex on a corpus of tests collected from the latest versions of ml projects.
overall flex identifies and proposes a fix for tests.
we sent pull requests each fixing one test to the developers.
so far have been accepted by the developers.
ccs concepts software and its engineering software testing and debugging .
keywords flaky tests machine learning extreme value theory acm reference format saikat dutta august shi and sasa misailovic.
.
flex fixing flaky tests in machine learning projects by updating assertion bounds.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction many emerging applications in computer vision natural language processing and medical diagnosis are implemented using machine learning ml algorithms such as deep learning reinforcement learning or probabilistic programming .
the recent pervasiveness of ml algorithms has led to the emergence of general purpose libraries and specialized tools that build on top of these libraries.
many ml algorithms are inherently random each execution of the algorithm may produce a slightly different result.
such randomness has an impact on how to carefully check the implementations of these algorithms because the tests have to account for the variability of computed results from the code under test.
a common class of tests in existing ml projects are integration tests that check for end to end quality of the implementation of an ml algorithm.
such tests typically create a small fixed or randomly generated dataset train the model on the dataset perform inference on the trained model and compute quality metrics and check if they are acceptable.
some common quality metrics include inference accuracy recall and error rate.
when developers write their tests they implement property checks using approximate assertions that compare the metric to an acceptability bound e.g.
assert accuracy .
developers typically choose the bounds based on intuition and experience with the code under test.
these choices are often ad hoc and not well understood especially when the developers are testing implementations of ml algorithms that inherently rely on some degree of randomness.
while randomness in implementations of ml algorithms can be controlled through setting seeds in the underlying pseudo random number generator s so can make the test less effective as it limits possible executions that can potentially help expose real bugs in the implementation .
however by keeping randomness throughout the tests may become flaky test executions can fail non deterministically even when there is no bug in the implementation.
the chance of flaky test failures depends on how tight the developer selected bound is.an important question then becomes how to systematically select such bounds so that test flakiness can be minimized to a desirable level.
our work.
we present flex the first tool for automatically fixing flaky tests due to algorithmic randomness.
flex focuses on tests that use approximate assertions to compare the actual and expected quality of ml algorithm results.
flex transforms the test and systematically selects appropriate assertion bounds that reduce the chance of flaky failures.
the key challenge is to determine how to estimate appropriate assertion bounds with high statistical confidence.
flex s solution is esec fse august athens greece saikat dutta august shi and sasa misailovic based on extreme value theory evt .
evt is a branch of statistics often used in finance and hydrology that can model extreme events such as market risks finance or occurrence of extreme floods hydrology .
given an input sample of measurements of some observed variable evt models the tail of the distribution which can then be used to compute the likelihood of extreme values.
the advantage of using evt is that in the limit the tail distribution will converge to a specific group of probability distributions.
we use the peak over threshold pot method from evt to estimate the tail distribution of a ml algorithm s result quality.
with this method the tail distribution converges in the limit to an instance of the generalized pareto distribution gpd .
gpd is parameterized by a shape parameter which determines if the measured quantity has a tail left or right that is exponentially bounded .
an exponentially bounded tail converges quickly to gpd and can be used to estimate an appropriate bound for the variable in the failing assertion.
on the other hand a heavy tailed distribution cannot provide a reasonable estimate.
in such a case we either collect more samples to get a better estimate or resort to alternative test fixing strategies.
flex records the actual values in the assertion e.g.
the variable accuracy in the example assertion earlier from multiple executions.
it then uses the recorded values to estimate the gpd as representative of the tail distribution.
since the tail distribution converges to gpd only in the limit flex uses statistical methods to find the sufficient number of samples of the output value that leads to convergence.
flex then uses the inferred gpd to determine the likelihood of the extreme values and choose an assertion bound that keeps the chance of the test failure below a pre specified probability c. flex implements several test fix strategies to reduce flakiness update the assertion using a statistical tail bound flex handles two kinds of assertions.
first for assertions that compare the absolute values e.g.
the variables accuracy and from our earlier example assertion flex collects the samples of the actual value accuracy computes the bound satisfying the confidence level using pot and updates the constant with the new bound.
second for assertions that use bounds for differences between two values flex estimates the tail distribution of the differences and updates the bound based on the tail estimate.
update the assertion using an empirical bound flex updates the assertion as in previous strategy but instead of computing gpd it uses an empirical bound computed using bootstrap sampling .
it is used when the pot method fails to compute the tail distribution or produces a heavy tailed distribution.
rerun the test to improve confidence flex does not modify the test body but marks it using the flaky annotation so that the test is re run on failure only declaring true failure if it fails for all re runs this annotation then reduces the chance of a flaky failure stopping a build.
currently developers may use reruns and specify the number of repetitions based on some intuition.
instead flex determines the number either from the estimated gpd when available or using the observed failure rate.
updating the thresholds in the assertions does not change the execution time of the test.
however re running the test can increase the overall execution time as a function of the failure probability .results.
we evaluate flex on a corpus of existing flaky tests collected from the latest versions of projects which use one of six popular machine learning and probabilistic programming frameworks pytorch tensorflow tensorflow probability pyro pymc3 and numpyro .
the dependent projects provide domain specific functionalities and have a wide user base.
flex proposes a fix for tests section .
.
it selected the statistical tail bound strategy in cases empirical bound strategy in cases and re run strategy in cases.
for the remaining tests flex determines that the current assertion bound is looser than what flex suggests.
hence we do not propose fixes for those cases as the flaky failures if they occur are statistically rare.
we sent pull requests each fixing one test to the developers.
so far pull requests have been accepted by the developers are pending and have been rejected.
of the rejected pull requests the developers mostly acknowledged the flakiness and chose to fix the problem in their own way custom to the project.
these results jointly demonstrate that our approach can reduce the flakiness of tests by proposing appropriate assertion bounds for pre specified confidence levels.
contributions.
this paper makes the following contributions we present flex the first technique for automatically fixing tests that are flaky due to algorithmic randomness in ml algorithms.
we present a novel test fixing algorithm that leverages statistical techniques from extreme value theory to guide several test modification strategies.
we evaluate flex on a corpus of flaky tests fixing tests while determining that the rest do not need fixes.
flex is publicly available at example we present an example flaky test whose assertion is not properly bounded leading it to pass and fail non deterministically when run multiple times on the same version of code.
the flaky test is named test ground truth separated modes from icb dcm pypesto a library for parameter estimation that provides state of art algorithms for optimization and uncertainty analysis of black box objective functions .
listing presents the simplified test code.
the test first initializes a sampler using the adaptive metropolis sampling algorithm which is a markov chain monte carlo mcmc method line .
it initializes a dataset for the test which is sampled from a mixture of two gaussian distributions line .
the test then defines the objective function that needs to be optimized.
in this case the objective function measures whether the generated mcmc samples resemble the target mixture distribution using a negative log likelihood metric not shown here .
then the test uses the mcmc sampler to find a solution to the problem that uses iterations for sampling line .
the test compares the results of the sampler with the expected ground truth line using the kolmogorov smirnov ks test a popular statistical procedure used to find the distance between two probability distributions lower is better .
the test checks whether the ks distance statistic is below .
line .
we found that this flaky test fails out of times we run it on the same version of code.
our inspection found that the computed ks statistic varies due to inherent randomness of the code under 604flex fixing flaky tests in machine learning projects by updating assertion bounds esec fse august athens greece 1def test ground truth separated modes sampler sample.adaptiveparalleltemperingsampler internal sampler sample.adaptivemetropolissampler n chains problem gaussian mixture separated modes problem result sample.sample problem n samples sampler sampler x0 np.
array samples result.sample result.trace x rvs1 norm.rvs size loc .
scale np.sqrt .
rvs2 norm.rvs size loc .
scale np.sqrt .
statistic pval ks 2samp np.concatenate samples assert statistic .
assert statistic .
listing fix for test in icb dcm pypesto 1def propose parameter self x np.ndarray x new np.random.multivariate normal x self.
cov return x new listing source of randomness for example flaky test figure distribution of values from example flaky test test such variance in computed values is common in machine learning ml projects .
the source of randomness in this test is in the adaptive metropolis sampling algorithm.
the sampling algorithm makes some random choices during its execution such as choosing the next sample from a distribution for a parameter that is being estimated.
listing shows the corresponding code snippet.
since the sampler runs for a finite number of steps in this case the solution may sometimes be further from the ground truth values than what is expected.
as a result the ks statistic can sometimes exceed .
causing the test to fail.
we collected the actual computed values of the ks statistic at the failing assertion line from several test executions.
figure shows the distribution of the collected samples.
clearly we see that some values exceed the expected bound .
originally set by the developers.
we assume the code under test is implemented correctly so we would then need to repair the test code providing a more reliable assertion bound to ensure it fails less often due to randomness.
to compute a better assertion bound we need to examine the tail of this distribution and also provide statistical confidence in our estimation.
a naive strategy here might be to use the observed extreme value as the new bound .
here .
however this strategy does not give statistical confidence that the execution will never result in an even more extreme value.
another workaround might be to set the bound to a large value say .
.
however so can lead to the test missing bugs which manifest as accuracy regressions.
ideally we want to determine a value that is both large enough as to minimize the flakiness and tight enough as to not miss bugs.
we leverage methods from extreme value theory evt to compute a bound with high statistical confidence section .
these methods take as input a set of samples of the observed variable e.g.
figure estimated tail distribution exponential and corresponding percentile estimates statistic and return a curve representing the tail left right of the distribution.
we can then use the tail distribution to estimate the most probable extreme value max min for a pre specified confidence level.
in this example since we want to find the maximum bound of statistic we need to inspect its right tail.
using evt method peak over threshold we are able to fit an exponential distribution to the tail samples see figure .
we estimate this distribution using only samples collected from executing this test.
to check for goodness of fit and confirm that we do not need more samples we use a sequence of statistical hypothesis tests gpd test .
using this distribution we can ultimately determine that the assertion bound should be .
which ensures the computed values will lead to a passing assertion .
percent of the time the assertion bound is at the .9th percentile for the tail distribution .
we do not choose the .99th percentile .
in this case since it seems to be too extreme.
we sent a pull request that changes the assertion bound to this value to the developers of this project.
the developers accepted and merged this pull request leaving a message thanks for this contribution!
i think checking the test percentiles is the way to go indeed .
further we also collected million samples for this test and observed that our predicted bound indeed matches this empirical percentile.
an alternative strategy to fix such tests might be to fix the seed in the random number generator s rng that are being used which would make the test execution more deterministic.
however setting the seed can also make the test more brittle future changes in code under test or the rng can break the test.
also it can hide potential bugs since the test will always observe the same set of values from the rng.
in this example the developers also agreed on this point saying i think checking the test percentiles is the way to go indeed unless we set the rng which we however rather don t want to atm .
background extreme value theory extreme value theory evt encompasses statistical methods that model the probability of extreme events e.g.
those more extreme than any event observed so far .
we will next describe evt and related statistical methods that we use in our approach.
we will use the standard notation from the probability theory xwill denote a random variable x1 ... xnwill denote random variables each representing observed samples of x andf x x or equivalently f x will denote the cumulative distribution function cdf of the random variable x. it denotes the probability that the value of x is smaller than a constant x. to make distribution parameters explicit we will write f x .
to characterize the probability of extreme events evt studies values which are relatively smaller larger i.e.
belong to the tail 605esec fse august athens greece saikat dutta august shi and sasa misailovic region than the rest of the observations in the sample and uses them to model the tail right left of the distribution.
peak over threshold pot .
for a random variable x the pot method takes as input a set of independent and identically distributed i.i.d.
samples x1 ... xn and outputs a distribution representing the tail of the distribution of x. the pot method uses a user specified threshold tto select a subset of samples that exceed the threshold.
this threshold helps select values from the tail of the distribution.
pot represents the tail of arbitrary continuous distributions using exceedance probability .
given a random variable x with cdffx we define exceedance probability ftas the cdf ofxabove threshold t ft y p x t y x t f t y f t f t where y xf t wherexfis the rightmost endpoint of for infinity.
prior work showed that for a large class of continuous distributions fand larget ftcan be approximated by a generalized pareto distribution gpd i.e.
ft y converges in distribution to g y ast where g y t h y t i if exp y t if here t and correspond to location scale and shape respectively.
these parameters can be estimated using maximum likelihood estimation mle methods .
the shape parameter determines the nature of the tail light exponential or heavy.
figure example cdf plots for light exponential and heavy tailed gpd distributions with .
and .5respectively 0and figure presents an example of how different kinds of tail distributions behave.
the exponential tailed distributions andlight tailed distributions defined as having less probability mass in the tail than exponential converge very fast and can provide reasonable estimates of the extremes.
however the heavy tailed distribution defined as having more probability mass in the tail than exponential converges very slowly.
computing an assertion bound in a high percentile for such a distribution would result in a very extreme value that may be an impractical assertion bound for a test.
estimating parameters of gpd.
given a set of observations s x1 ... xn the location scale and shape parameters of gpd can be estimated using maximum likelihood estimation mle methods.
mle methods compute the point estimate of distribution parameters that maximize the likelihood that distribution produces the observed data.
formally the likelihood function can be defined as p s p x1 p x2 ... p xn n i 1p xi where is the set of parameters of gpd distribution.
mle thenobtains the parameter estimates that maximize this likelihood argmax n i 1p xi .intuitively it selects parameter values such that observed data is most probable.
as the number of observations grows the mle estimates converge in probability to the true values.
goodness of fit vs. samples count.
according to pot the tail distribution is guaranteed to converge to the gpd distribution in the limit .
however it is unknown how many samples may be needed for convergence in practice especially if the distribution has a heavy tail.
the choice of threshold tdetermines a trade off between goodness of fit and minimum samples required for convergence.
researchers have proposed several heuristics for choosing appropriate thresholds.
in this work we adopt the methodology proposed by bader et al.
for automated threshold selection using goodness of fit tests.
the precise problem can be stated as follows given a sequence of samplesx1 ... xnof sizen we want to determine the lowest thresholdtsuch that the gpd fits the exceedances yi xi t adequately.
bader et al.
propose using a sequence of goodness of fit tests for the gpd over each candidate threshold in an increasing decreasing order until the stopping criteria is reached.
for an ordered set of thresholds t1 ... tl let there be zi exceedances i ... l for each threshold.
the sequence of null hypotheses can be stated as hi the distribution of ziexceedances abovetifollows the gpd.
the alternative hypotheses are hi the distribution of ziexceedances above tidoes not follow the gpd.
we use the non parametric anderson darling test as recommended by bader et al.
for this hypothesis test.
to reduce the chances of choosing the wrong threshold by mistake also known as false discovery rate or fdr the authors introduce special stopping criteria when evaluating these hypotheses.
in particular we test each threshold starting from the highest and stop if the following criteria is satisfied exp l j klogpj j k lwhere is the false discovery rate probability of choosing a wrong threshold k ... l is the index of the current threshold and pjis the p value returned by the jth hypothesis test hj .
this technique allows for a principled way to select a reliable threshold and check whether a gpd can be fit.
when one or more of the hypothesis tests pass based on the stopping criteria we say that the samples converged to a gpd and choose the lowest threshold for further analysis.
if all the hypothesis tests fail this means that we may need more samples.
we abstract this check using stoptest function in our algorithm section .
.
box cox transformation.
box cox transformation is a power transform that can create a monotonic transformation of data i.e.
preserves the original order of values .
this transformation is useful in making the data closer to a normal distribution and stabilizing its variance.
normality is a key assumption in many statistical analyses.
hence applying the box cox transformation can enable a broader range of analyses on the data.
the box cox transformation can be described as follows y i y i logyi where is a parameter that can be estimated from the samples using mle methods.
it can only be applied when yi i ... n .
606flex fixing flaky tests in machine learning projects by updating assertion bounds esec fse august athens greece teugels and vanroelen showed that applying box cox transformation can be useful in presence of heavy tails and can lead to faster convergence.
further helsel and hirsch showed that quantiles or percentiles are invariant to monotonic transformations.
hence q f y f q y whereqis the quantile function is any given quantile fis the monotonic transformation and yis the set of samples.
there is no known guarantee that applying the box cox transformation on data will prove to be always useful for any given statistical analysis .
however it is a useful heuristic that can help speed up or even enable finding convergence for a distribution.
flex we propose flex a technique for fixing flaky tests caused by inherent algorithmic randomness in ml projects.
flex assumes that the code under test is implemented correctly and thus considers tests that fail some of the time to be flaky and in need of repair.
given an assertion ain a test tof the form assertx flex performs the following steps collect and pre process the samples x1 ... xnof actual value xfrom several test executions determine the lowest possible threshold tsuch that a gpd gx can be fit toyi xi t i ... n with a confidence of at least using the goodness of fit approach described in section estimate the most probable bound bfromgxforx based on the desired confidence level c as provided by the developer and update the assertion bound to b. for instance if c .99then we determine bsuch that p x b .
.
.
flex algorithm algorithm describes the main flex algorithm.
it takes a test t an assertion ain the test and a confidence threshold cas input and returns the fixed version s of the test t as output.
intuitively the algorithm executes tseveral times and collects the samples from the values being compared in the assertion until either the tail distribution converges to a light or exponential tail or the number of samples collected exceeds the maximum sampling limit max samples and therefore we consider to not converge.
in each iteration of the loop lines we execute the test ntimes and collect samples from the assertion line .
we add the new samples to the existing set samples and check if the tail distribution converges to a light or exponential tail lines .
the estimation algorithm tailboundestimator section .
takes the samples samples assertiona a flagfto enable disable the box cox transformation section and confidence level cas inputs.
when a distribution has a light or exponential tail the distribution has a finite bound and hence can be used to fix the test assertion.
on the other hand if the distribution does not converge or has a heavy tail we might need more samples to get a better estimate.
if we fail to get a bound then we try to get an estimate by enabling the box cox transformation line .
we choose to check convergence first without transforming because the transformation adds extra overhead.
note that box cox can be applied only on positive data.
if all the samples are negative then we change the sign of the values before the analysis and revert the sign of the results if the analysis succeeds.
however if we have a mix of positive and negative values we do not apply this transformation.algorithm flex algorithm input testt assertiona confidence level c output fixed test t procedure flex t a c conv false d samples n initial sample size bound while samples max samples do s testrunner t a n samples samples s conv d bound tailboundestimator samples a false c if not conv or not islightorexp d then enable transform conv d bound tailboundestimator samples a true c end if ifconv and islightorexp d then break end if n next sample size end while return patcher t a samples d bound end procedure we continue the loop until the sample size exceeds a user set limit max samples or if the tail converges to light or exponential distribution lines .
finally flex patches the test using different available fix strategies depending on whether it finds a finite bound or not section and returns the patched test line .
.
estimating the statistical tail bound given a set of samples collected from test executions the tail estimation algorithm applies the peak over thresholds pot method to select values from the tail of the distribution based on a threshold and check if they converge to a tail distribution which belongs to the generalized pareto distribution gpd .
however selecting an appropriate threshold is non trivial and can affect convergence.
in this work we use an automatic threshold selection technique to compare different threshold choices discussed in section goodness of fit and choose the lowest threshold that passes the gpd test meaning it fits adequately to a gpd distribution.
algorithm shows the tail bound estimation algorithm tailboundestimator .
the algorithm takes as input a set of samples s an assertiona a flagfon whether or not to enable the box cox transformation and a confidence level cfor choosing the bound.
for the threshold that the pot method needs we iterate over a set of possible user defined thresholds m line .
any value exceeding a threshold is considered to be part of the tail of the distribution and is used to fit to a distribution that helps compute the bound.
for each threshold t we compute the exceedances line .
we apply the gpd test for convergence and compute the p value p. we also obtain the shape light exp heavy and specification of the distribution dif it converges line .
if the gpd test succeeds i.e.
p significance level and we obtain a light or exponential distribution line then we estimate the bound bby computing the extreme percentile qc for the distribution such as 99th or .99th line .
if the box cox transformation is enabled thecomputeperc method also transforms the bound back to the original scale of the samples.
if we obtain a mildly heavy tail e.g.
for some small we can still approximate it using an exponential distribution in some cases.
we use the likelihood ratio 607esec fse august athens greece saikat dutta august shi and sasa misailovic algorithm tail bound estimation algorithm input sampless assertiona enable transformation f confidence level c output convergence result conv gpd distribution d boundb procedure tailboundestimator s a f c iffthen s transform s end if m getthresholds s d b conv false p fort sorteddescending m do exc x t x t x s pot method if exc min tail samples then continue end if p gpdtest exc convergence test p p p ifp significance level then check if converged d fitgpd s conv true ifislightorexp d then b computeperc d c t f find new bound else d fitwithlrt d approximate to exponential ifd then b computeperc d c t f d d end if end if end if ifstoptest p then stopping criteria for hypothesis test break end if end for return conv d b end procedure test as a hypothesis test to check if original distribution and the exponential distribution obtained by fitting to the samples are not significantly different.
we use the estimate if the hypothesis test passes lines .
the fitwithlrt function line abstracts this test and fitting to exponential distribution.
if the stopping criteria stoptest described in section for the hypothesis tests is satisfied we break out from the loop line .
when considering possible thresholds we iterate through them in descending order because we would like to select the lowest threshold which in turn selects more samples from the tail region to obtain a reliable estimate of the bounds of the distribution.
finally the algorithm returns the status of convergence conv the gpd distribution d and the estimated bound b. .
implementation of flex components we describe details on how we implement the main components for flex.
we implement flex in python.
test runner.
it takes as input a test t an assertion awithin t and the number of times to run n. first test runner instruments testtto log the actual and expected values used in the assertion a. for instance for an assertion of the form assert allclose a b it will instrument the assertion to log values aandbbefore the assertion.
second it will execute the test ntimes parse the values of aandb from the execution logs and return it to the caller.
test runner uses pytest a popular library for executing tests in python projects.tail bound estimator.
it implements the algorithm described in section .
to check whether the tail distribution has converged and to estimate an appropriate bound for the assertion aif the distribution converged and has a light or exponential tail.
we use the eva package in r for the gpd test.
we use the box cox implementation in scipy to transform or inverse transform the samples.
we choose the common significance level of .
for the gpd test.
forstoptest check we use the false discovery rate of .
.
patcher.
the patcher module takes a test t assertionain the test all collected samples samples fitted gpd d and the proposed boundbas input and provides one or more fixed version s of the test as output.
if bis not it updates the assertion in the test accordingly section .
and returns the patched test to the caller.
otherwise it may also propose other fixes for the test.
we discuss each fix strategy in section .
test fixing strategies flex provides three different strategies for automatically fixing and updating a flaky test depending on whether a finite tail bound can be computed using evt and the nature of the assertion sections .
.
.
flex may also choose not to fix a test section .
when it deems that the original bound is already looser than our proposed bound indicating that failures are statistically rare .
when multiple fixes are proposed by flex we first we fix a test using the statistical bound when available.
otherwise we use the empirical bound for the fix.
if the estimated confidence interval for the empirical bound is too high we choose to re run the test instead.
we may also need to adapt our strategy based on the context see section .
.
.
using the statistical tail bound sb if we obtain a light or exponential tailed distribution using algorithm then the distribution has a finite bound.
we then simply compute the extreme percentiles e.g.
q0.99orq0.
based on developer specified threshold c to find a value that is higher or lower than the original bound used in the assertion and update the assertion with the new bound.
the fixed assertion then has a failure probability of approximately c. .
using the empirical bound eb if the tail bound estimation algorithm algorithm fails to converge or provide a finite bound a heavy tail distribution flex estimates an empirical bound from the observed executions.
flex uses bootstrap sampling to re sample with replacement several times from the available samples and compute the extreme max min from each instance of re sampled data.
as a result flex obtains the set of sample extremes e and returns user specified statistic of this set e.g.
q e mean or median as the new empirical bound.
flex also computes the confidence interval q0.
e q0.
e which denotes the variability in the empirical bound a smaller confidence interval indicates the empirical bound is close to the true bound.
.
re running the test rr the flaky plugin for pytest allows the developers to automatically re run the test on failure.
to use this plugin a developer needs 608flex fixing flaky tests in machine learning projects by updating assertion bounds esec fse august athens greece to annotate the test using flaky .
this plugin also allows additional parameters max runs default and min passes default .
the plugin runs the test up to max runs times until it passes max passes times.
flex can annotate the test based on its observed failure rate during its analysis i.e.
re using the observed executions at the end of algorithm .
flex computes the number of re runs in the following two ways flex computes the empirical failure probability of the test p failures runs.
then it computes the number of re runs using n log c logp wherecis the developer provided confidence level as in algorithm for minimum passing probability.
if the distribution converges to a heavy tail we can also compute the probability that a sample exceeds the current bound set in the assertion.
for instance let dbe the tail distribution gpd returned by algorithm and be the current bound used in the test.
then we can compute p x d which is the failure probability of the assertion.
we can then compute the re runs similar to the previous case using this probability.
unlike other approaches re running may increase the average running time of the test.
specifically if the run time of the test is w the expected run time of the test will be n k 1pk p k w. .
not fixing a test nf in some cases flex may propose a bound that is very close to or tighter than the original bound indicating that the assertion bounds are already conservative.
this case indicates that test failures if they occur are extremely rare events.
as such we report but do not propose the fix to the developers.
.
updating assertions we describe how flex updates an assertion when a statistical or empirical bound for an assertion can be computed.
assertions comparing absolute values.
this category includes assertions that either compare with a computed value or with a constant.
some examples include the python assert statement assert and some other apis in unittest e.g.
assertgreater x assertless x and numpy e.g.
assert array less x .
to fix an assertion flex simply replaces with the bound it computes.
listing shows an example of such a fix from the icb dcm pypesto project.
assert statistic .
assert statistic .
listing fix for test in icb dcm pypesto assertions using tolerance thresholds.
some assertions check whether the relative or absolute difference between two floatingpoint values is less than a threshold.
some examples include numpy apis such as assert almost equal a b decimal c and also assert allclose a b rtol c1 atol c2 wherec c1 and c 2are the relative and absolute thresholds respectively.
in these cases flex collects the values of both aandbfrom test executions and computes the absolute or relative difference from each execution.
flex estimates the tail distribution using these differences as samples.
it updates the assertion to either use a lower tolerancethreshold or reduce the decimal places being compared depending on the kind of assertion.
listing shows an example from the microsoft hummingbird project for absolute tolerance fix.
assert allclose model.predict x torch model.predict x rtol 1e atol 1e assert allclose model.predict x torch model.predict x rtol 1e atol 1e listing fix for test in microsoft hummingbird methodology .
projects and flaky tests we follow a similar methodology as dutta et al.
to select machine learning projects for our evaluation.
we start with two popular machine learning libraries pytorch and tensorflow and four probabilistic programming systems pyro numpyro tensorflow probability and pymc3 on github.
we use github s feature to track the projects dependent1on these libraries and also have more than stars as an indication of popularity.
table details of projects used project dependent filtered tensorflow pytorch tensorflow prob numpyro pyro pymc3 total unique successful at testing projects with flaky tests some of these core libraries can have hundreds of dependents so we only select the top dependent projects per library for our study.
table shows all the project details.
overall we select unique projects.
we develop a general installation script to install these libraries which creates a virtual python environment using anaconda and then it installs the library and all its dependencies in the environment along with some libraries for testing such as pytest.
in python libraries developers typically specify all dependencies in the setup.py file which is the main installation module.
they can also specify additional dependencies e.g.
for building documentation and testing in a requirements.txt file.
however in some cases the installation process may not work due to incomplete dependency specifications missing system dependencies such as sql server client or open mpi library or required specialized build testing systems such as bazel .
our installation script installs a general set of system dependencies but relies on pip and pytest to build and test the libraries.
overall we are able to successfully install and test projects.
of the resulting projects we ran their tests using flex s test runner module running only the tests with approximate assertions that we support.
initially we run each test up to times while 1we use only dependent packages as reported by the github api which are projects that can be installed as a library to be used by others.
we use packages because they are more likely to be actively maintained by developers and have reasonable test suites.
609esec fse august athens greece saikat dutta august shi and sasa misailovic recording the actual computed values in each assertion using test runner s instrumentation.
if any assertion s actual values remain exactly the same for all those initial runs we discard those tests from consideration.
for the remaining tests with assertions whose actual values vary we run those tests times while recording test results success failure from each run.
if we detect any failures and at least some passing runs as well we mark the test as flaky and use it for our evaluation.
ultimately we are left with projects with flaky tests as part of our evaluation.
recall flex assumes that the underlying distribution is continuous.
we also included tests with discrete distributions mainly resembling binomial distribution that can be often approximated well with a continuous distribution .
.
flex configuration for our evaluation we configure flex to first collect an initial samples initial sample size from algorithm .
if more samples are needed we configure flex to collect more samples in batches of next sample size in algorithm .
we specify flex to collect at most samples before stopping max samples in algorithm .
we set the minimum number of tail samples when testing for convergence to be min tail samples in algorithm .
we use significance level of .
for the gpd tests.
for the confidence level cin algorithm we configure flex to use 90th 95th 99th .9th and .99th percentiles.
we run all experiments on azure vms standard f32s v2 configuration with .4ghz xeon processor with cores and 64gb memory.
while executing the tests we run threads in parallel as to speed up experiments.
.
reporting to developers for each fix we obtain from running flex on a flaky test we prepare a pull request to send to the developers.
in the process of preparing the pull request we manually inspect the proposed fix es and the surrounding context in the test as to determine if the fix seems reasonable.
for example if the assertion initially checks if some count of values is greater than zero and the fix is to change that assertion bound to instead be a negative number then the fix does not make sense in the context of this test.
we select one of the other available fixes in such a case section based on the context.
for each project in our evaluation we first send a pull request for fixing one test.
we initially send just one pull request as to not bother developers immediately with many pull requests if they are not willing to consider such changes.
if the developers accept the initial pull request we send pull requests for fixing the remaining flaky tests.
we ensure every pull request we send only addresses one flaky test at a time.
as part of a pull request we provide both the proposed fix and the statistical evidence we gathered by running flex on the test.
we present to developers information on the number of times the assertion failed out of how many reruns and we explain how the tail distribution was computed using the actual values from test executions.
we suggest the bounds at either .9th or .99th percentile depending on the test but for completeness we also provide the values for the other percentiles including also 90th 95th and 99th percentiles .
if the developer chooses one of these bounds we adjust the pull request accordingly.
evaluation in this section we address the following research questions rq1 how many flaky tests can flex fix?
which fix strategy does it apply in each case?
how many test runs does it need in each case?
rq2 how do the different fix strategies compare and in what scenarios can each be applied?
rq3 how do developers respond to the fixes?
.
flaky tests fixed by flex we run flex on the flaky tests found in the latest versions of projects from section .
.
table presents the results.
each row represents one flaky test.
column idis a shorthand identifier we give to each test for later reference project presents the name of the project as a github slug test presents the name of the test sha presents the commit sha of the project that we ran flex on samples presents the number of samples flex collected for its analysis conv.
presents whether the tail distribution converged algorithm means yes means no and l epresents whether the distribution had a light or exponential tail when it converges means yes means no means not applicable .
for the final four columns under fixed we mark with the type of fix that flex proposed for the test.
the column sbmeans the test was fixed using a statistical bound estimated using the light or exponential tail distribution computed using pot.
ebmeans the empirical bound strategy is used.
rrmeans re run strategy is used.
by default flex prioritizes the fixes sb eb rr section but adjusts the recommendations based on the context of the test section .
.
nfmeans that flaky test was not fixed because flex s proposed new assertion bound is tighter than the original section .
.
as such these tests would be considered tolerant enough already so flex s proposed assertion bound fix would not make sense.
in sum flex proposes a fix for flaky tests sb eb rr while remain not fixed nf .
we compare the fix strategies in section .
.
overall for tests flex requires only samples the minimum that we collect for convergence showing that our analysis is efficient in most cases.
only for tests does flex require more than samples for convergence.
we apply the gpd test to check if we have enough samples to reliably estimate the tail distribution.
this gives us statistical confidence in our results.
further by considering different thresholds for selecting the tail values we ensure that we can select as many samples from the tail of the distribution for the best possible result.
for the remaining tests which flex chooses not to fix the proposed bound was tighter than original bound.
the box cox transformation helped in early convergence and bound estimation for cases t1 t5 t14 t17 t21 t22 t32 and t34.
.
comparison of fix strategies out of fixed tests flex proposes the statistical bound for tests empirical bound for tests and the re running strategy for tests.
in cases where flex suggests multiple fixes we manually inspect and select the most appropriate fix based on the context.
we next discuss in which scenarios each fix might work.
we observe that flex s statistical tail analysis converges for tests out of which we obtain a light or exponential tail for tests 610flex fixing flaky tests in machine learning projects by updating assertion bounds esec fse august athens greece table results of running flex on flaky tests id github project test sha samples conv.
l efix type sb eb rr nf t1 microsoft coax test update 37c3e6 t2 deepchem deepchem test in silico mutagenesis nonzero 6a535b t3 deepchem deepchem test uncertainty 6a535b t4 deepchem deepchem test restore equivalency 6a535b t5 deepchem deepchem test int sequence 6a535b t6 fastnlp fastnlp test constanttokennumsampler 22c6e6 t7 rlworkgroup garage test update envs env update 1f1742 t8 rare technologies gensim test cbow hs training fromfile cfc9e9 t9 rare technologies gensim test cbow neg training fromfile cfc9e9 t10 rare technologies gensim test sg hs training fromfile cfc9e9 t11 rare technologies gensim test sg neg training fromfile cfc9e9 t12 microsoft hummingbird test tree regressors multioutput regression 9f71c2 t13 microsoft hummingbird test sklearn multioutput regressor 9f71c2 t14 microsoft hummingbird test sklearn regressor chain 9f71c2 t15 kornia kornia test two view cf8e85 t16 magenta magenta teststartcapture callback period b4b9af t17 magenta magenta testwaitforevent signal b4b9af t18 plasticityai magnitude test augmented lstm computes same function as pytorch lstm 7ac0ba t19 plasticityai magnitude test scalar mix layer norm 7ac0ba t20 plasticityai magnitude test multi head self attention respects masking 7ac0ba t21 intellabs nlp architect test tcn adding 728e21 t22 facebookresearch parlai test stochastic fb5c92 t23 pgmpy pgmpy test fit 413c61 t24 pymc learn pymc learn test advi fit returns correct model 4f1ee6 t25 pymc learn pymc learn test advi fit returns correct model 4f1ee6 t26 icb dcm pypesto test ground truth separated modes a34608 t27 tristandeleu pytorch meta test matching log probas 389e35 t28 refnx refnx test all minimisers 34e369 t29 stellargraph stellargraph test poincare ball distance self 1e6120 t30 willianfuks tfcausalimpact test default model sparse linear regression arma data 9fc9e8 t31 google trax test value error high without syncs beaca3 t32 lmcinnes umap test aligned update 05840e t33 lmcinnes umap test neighbor local neighbor accuracy 05840e t34 zfit zfit test onedim sampling a798f9 t35 zfit zfit test sampling a798f9 .
and a heavy tail for one test.
for tests where the analysis does not converge even applying the box cox transformation does not aid the analysis.
these scenarios occur either because either there are very few samples in the tail region or the samples only consist of very few discrete values.
in some cases the variable in the assertion of a test might have a known hard bound such as count orlength that are lower bounded by zero e.g.
assert np.count nonzero scores from deepchem deepchem .
this assertion sometimes fails when the count is zero.
hence this case also does not satisfy flex s requirement of the samples belonging to a continuous distribution.
however this information is not easily interpretable just from the samples that flex s tail analysis collects.
in such cases flex may sometimes propose a negative assertion bound using the inferred tail distribution which is an impractical fix.
further updating the assertion to check for 0also does not make sense.
in these cases re running the test is the only reasonable fix that flex can propose.
we propose the empirical bound fix strategy when we have a large set of samples and can estimate a bound with high confidence i.e.
small confidence interval .
this strategy is useful in scenarios where the tail analysis fails to converge and the quantity of interest does not have a known hard bound like the previous example .
for instance in the microsoft hummingbird project thetesttest tree regressors multioutput regression contains a flaky assertion assert allclose model.predict x torch model.predict x rtol 1e atol 1e flex tracks the maximum absolute difference between the values being compared and obtains a empirical bound of .
.
.
this bound is evidently much higher than the absolute tolerance specified in the test .
flex suggested a fix using this bound to the developers.
in this case however developers found an actual bug in their code which was causing such erroneous executions.
.
developer response to flex s fixes using our methodology for sending pull requests to developers section .
we ultimately sent pull requests for tests for which flex proposes a fix.
table presents the status of our pull requests per project representing the tests that flex can fix.
column a means number of pull requests accepted pmeans number pending rmeans number rejected and umeans number unsubmitted we are waiting initial response from the developer on our first sent pull request .
for pymc learn pymc learn we do not send a pull request since the project has been inactive for the last two years.
the total number of pull requests under column prs matches the number of tests for which we sent fixes.
611esec fse august athens greece saikat dutta august shi and sasa misailovic table pull requests project tests prs a p r u coax dev coax deepchem deepchem fastnlp fastnlp rlworkgroup garage rare technologies gensim microsoft hummingbird plasticityai magnitude intellabs nlp architect facebookresearch parlai pgmpy pgmpy icb dcm pypesto pymc learn pymc learn tristandeleu pytorch meta refnx refnx stellargraph stellargraph lmcinnes umap zfit zfit 21 so far developers accepted pull requests.
pull requests are still pending developer response and pull requests are rejected.
for most of our pull requests we selected the estimate based on the .99th percentile as the new bound of the test.
in some cases we use a different percentile after discussion with developers and we provide the estimates for the other percentiles section .
.
listing shows an example of a fix for a test in zfit zfit .
for this test flex estimates the extreme percentiles as follows 90th 95th 99th and .99th .
the original bound is shown in red .
initially we submitted the pull request with the .99th percentile as the fix shown in blue .
however the developers suggested they would prefer the 99th percentile shown in green to reduce the flakiness to some extent compared to the current rate for now and would later like to investigate into why the computed values are so low.
assert scipy.stats.ks 2samp x xns .pvalue 1e assert scipy.stats.ks 2samp x xns .pvalue 1e assert scipy.stats.ks 2samp x xns .pvalue 1e listing fix for test in zfit zfit of the rejected pull requests the developers accepted different fixes for the tests.
for two of our pull requests to microsoft hummingbird the developers reasoned that our proposed bounds were too large and hence indicative of a real bug in their library.
later on they proposed a global change for fixing several numerical precision issues in their code which impacted such tests.
forrefnx refnx the developer preferred setting the seed instead of changing bounds.
for rare technologies gensim after discussing with the developers we found that the failures were due to a race condition in the code and we proposed a different fix that they accepted .
out of remaining two cases in one case for intellabs nlp architect the developers rejected our pull request without providing any reason.
for zfit zfit the test was already marked flaky and the developers chose not to make any changes.
the positive responses from developers confirm that tuning assertion bounds is a reasonable way to fix flaky tests in these ml projects that deal with randomness e.g.
consider the commentsmentioned in section .
the developers from microsoft hummingbird while accepting one of our initial pull requests also confirmed that they rely on their intuition to manually set such bounds ...for the moment we manually set a reasonable value for the differences but having a more scientific way of finding them will be great!
.
the developers of lmcinnes umap accepted our pull request and commented thanks the non deterministic tests are a little annoying at times.
i appreciate the effort you went to to ensure this won t trip accidentally .
these positive responses show a practical value of flex s systematic approach for determining assertion bounds.
threats to validity the projects we use in our evaluation are only a subset of all machine learning applications.
we selected these projects by starting with the most popular machine learning libraries and finding their dependent projects.
we believe these projects are representative.
we also focus on flaky tests that use approximate assertions found to be a common type of flaky test from prior work .
we detect the flaky tests in these projects through repeated reruns.
we use a similar rerun strategy to detect these flaky tests as prior work .
the flaky tests we use are then a lower bound on the total number of flaky tests as other flaky tests may require even more reruns to observe some failures.
such tests have a higher chance of flakiness and hence are likely the ones that developers would want to focus on.
since flex builds on several statistical methods and heuristics there is a possibility of estimating incorrect bounds.
as a result we may sometimes over estimate the bound which may cause the tests to miss some bugs.
we minimize this risk by using high significance levels both for individual hypothesis tests and for the algorithm for threshold selection.
to increase confidence in the the bug finding ability of the fixed test one can use strategies from the literature e.g.
.
like other prior work on repairing tests we assume code under test to be correct with the implementation matching the intended logic.
ultimately we send the proposed fixes as pull requests to developers providing them the statistical evidence of the fixes.
we allow the developers who are more knowledgeable about the code than us to use the provided evidence to make the final judgment call on how good the proposed fix is.
related work flaky tests.
luo et al.
performed the first empirical study on flaky tests studying open source projects and determining common root causes for flaky tests.
later work would build upon luo et al.
s findings developing techniques to detect specific flaky tests with root causes found from their study such as due to test order dependencies asynchronous waits or unordered collections .
however these prior works focused on flaky tests in traditional software.
dutta et al.
performed an empirical study to find common root causes for flaky tests in ml applications.
they found that a common cause for flakiness in this domain is algorithmic randomness e.g.
calls to random number generators both in theapplication code and the tests.
leveraging these insights they developed flash to detect such flaky tests using convergence testing.
our work shows how to fix such flaky tests using evt and statistical hypothesis tests to update approximate assertion bounds.
612flex fixing flaky tests in machine learning projects by updating assertion bounds esec fse august athens greece flaky test repair.
prior work on test repair generally involves updating assertions after code under test has evolved .
the assumption is that the code under test is correct and so test assertions need to match the current implementation.
we also make this assumption in our work and propose a technique for adjusting assertions that better match the underlying implementation while reducing flakiness.
recently there has been work on repairing specific types of flaky tests such as flaky tests due to test order dependencies or due to unordered collections .
the goal of these techniques is to make flaky tests no longer fail due to their flakiness root cause.
lam et al.
proposed mitigating flakiness due to asynchronous waits by automatically adjusting wait times as to reduce the chance of tests failing due to waits.
we also focus on fixing flaky tests by adjusting assertion bounds reducing the chance of a flaky test though not completely eliminating it .
we focus on flaky tests with approximate assertions that can fail due to inherent randomness in executing code under test.
tera aims to reduce the time of testing ml projects by changing the algorithm hyper parameters which potentially increases the flakiness of tests.
tera is based on bayesian optimization guided by convergence testing.
flex instead changes the assertion bounds to reduce flakiness while not impacting execution time by leveraging distribution estimation from extreme value theory.
extreme value theory evt .
we rely on evt to determine tail distributions of the computed values in approximate assertions.
while we rely on the peak over threshold pot method to apply evt there are other popular methods as well.
block maxima method bmm uses a given block size b selected by user to split the given samples into equally sized blocks and then considers the maximum value in each block.
according to the fisher tippet theorem this distribution is then guaranteed to converge to a generalized extreme value distribution.
the choice of block size bis often not intuitive and can affect the convergence of the distribution.
this method is generally better suited for data with some periodicity e.g.
daily month weather data finance data.
in our case the values in the assertions do not exhibit any such periodicity in general which makes this method less effective.
the pot method on the other hand considers exceedances over some threshold t selected by the user .
these exceedance values from the samples then converge to a generalized pareto distribution gpd .
this method is better suited to our use case.
testing of programs in presence of randomness.
machine learning frameworks like tensorflow and pytorch have led to a surge in machine learning based applications.
probabilistic programming has also been gaining in popularity in recent years leading to the development of numerous probabilistic programming languages .
researchers proposed techniques for testing and debugging probabilistic systems machine learning frameworks and randomized algorithms to complement manual test writing.
researchers have also explored techniques for testing randomized or adaptive software or analyzing robustness of programs .
however the advances in efficient automated test generation for these systems has yet to catch up with the speed of application development while capturing the inherent non determinism and overcoming the lack of reliable oracles in this domain.
conclusion we present flex the first tool for automatically fixing tests from machine learning ml projects that are flaky due to algorithmic randomness.
flex analyzes and transforms tests that use approximate assertions to compare actual and expected values that represent the quality of ml results.
we leverage statistical methods from extreme value theory to determine the appropriate assertion bounds as to reduce the chance of flaky test failures.
we evaluate flex on a corpus of tests collected from the latest versions of ml projects.
overall flex identifies and proposes a fix for tests.
we sent pull requests each fixing one test to the developers.
so far have been accepted by developers.
we envision that many future applications will continue to incorporate a degree of randomness.
our goal is to help developers cope with randomness and overcome the lack of reliable testing oracles both in ml and other domains.