benchmarking automated gui testing forandroid against real worldbugs tingsu shanghaikey laboratory of trustworthycomputing eastchina normaluniversity china tsu sei.ecnu.edu.cnjuewang statekey labfornovel software tech.and dept.
of computersci.and tech.
nanjing university china juewang591 gmail.comzhendongsu departmentof computerscience ethzurich switzerland zhendong.su inf.ethz.ch abstract forensuringthereliabilityofandroidapps therehasbeentremendous continuous progress on improving automated gui testing in thepastdecade.specifically dozensoftestingtechniquesandtools have been developed and demonstrated to be effective in detecting crash bugs and outperform their respective prior work in the number of detected crashes.however an overarching question how effectively and thoroughly can these tools find crash bugs in practice?
hasnotbeenwell explored whichrequiresaground truth benchmark with real world bugs.
since prior studies focus on tool comparisons w.r.t.someselectedapps they cannotprovidedirect in depthanswers to this question.
to complement existing work and tackle the above question this paper offers the first ground truth empirical evaluation of automated gui testing for android.
to this end we devote substantialmanualeffort toset upthe themisbenchmarkset including a carefully constructed dataset with real reproducible crashbugs takingtwoperson monthsforitscollectionandvalidation and aunified extensibleinfrastructure withsix recent state of the art testing tools.
the whole evaluation has taken over 920cpuhours.wefindaconsiderablegapinthesetoolsfinding the collected real bugs bugs cannot be detected by any tool.
our systematic analysis further identifies five major common challenges that these tools face and reveals additional findings such as factors affecting these tools in bug finding and opportunities for tool improvements.
overall this work offers new concrete insights most of which are previously unknown unstated anddifficulttoobtain.ourstudypresentsanew complementary perspective from prior studies to understand and analyze the effectiveness of existing testing tools as well as a benchmark for future researchonthistopic.the themisbenchmarkispubliclyavailable at .
ccs concepts software and its engineering software testing and debugging.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse august 23 28 athens greece associationfor computing machinery.
acm isbn ... .
gui testing android apps crash bugs benchmarking.
acmreference format ting su jue wang and zhendong su.
.
benchmarking automated gui testing for android against real world bugs.
in proceedings of the 29th acm joint european software engineering conference and symposium on thefoundations ofsoftware engineering esec fse august 23 28 athens greece.
acm newyork ny usa 12pages.
.
introduction androidappstypicallyrunincomplexend userenvironmentspostdeployment.
ensuringtheir reliability and correctness avoiding fatal crashes in particular is thus a top priority of any app developmentteam.sincethefirsteffortbyhuandneamtiu in2011 tremendous and continuous efforts have been made to improve automatedguitestingforandroid whichcomplement thecommonly adoptedmanualtestinginthisfield .specifically dozensofautomatedguitestingorfuzzingtools e.g.
havebeendevelopedanddemonstratedto beeffectiveindetectingcrashbugsandoutperformtheirrespective prior work inthe number of detectedcrashes.
thus anoverarchingquestionis howeffectivelyandthoroughly canthesetoolsfindcrashbugsinpractice?
.toanswerthisquestion an ideal approach is to directly assess these tools against a groundtruth benchmark with real world bugs and check how many bugs are found or missed by a given tool.
indeed such a benchmarking approach is well justified and widely adopted in practice for evaluating software testing or analysis tools e.g.
lava defects4j and decapo .
it has two key benefits enabling many direct in depth analyses e.g.
analyzing the false negatives and common weaknesses of tools and consolidating the evaluationvalidity e.g.
avoidingsuchfalsepositivesasbugovercounting duetotheimprecisionofbugde duplicationstrategies .incontrast evaluating testing tools against onlyapps without known bugs is difficult to obtain such benefitsif not impossible.
ontheotherhand noeffortexistsintheliteratureyettoanswer theaforementionedquestionagainstground truth.forexample by investigating the recent literature reviews and relevant publications in this field we identified research papers that proposeautomated testingtechniquesfordetecting crashbugs in androidapps.however noneevaluatedtheproposedtechniques against real world bugs and all compare tools w.r.t.some selected appsalone.similarly allpriorrelevantempiricalstudies in this field also use only apps without known bugs to 119esec fse august athens greece ting su jue wang zhendong su table key differences between the prior relevant studies and ours in evaluating automated testing techniques for android and ?
denotethatthestudycan cannotorcan onlypartially giveanswers respectively and n a isnotapplicable .
studies venueevaluationbasis analysisbasis newstudyinsights tools basishas the ground truth?basisarecrashes known and reproducible?examine tool implementations?discuss confirm withtool authors?quantifying bug findingabilities rq1 common challenges rq2 factorsand opportunities rq3 choudhary etal.
ase multiple apps crashes coverage ?
wangetal.
ase multiple apps crashes coverage zhengetal.
icse seip one one app coverage n a ?
behrangetal.
ase one apps coverage n a ?
our study esec fse multiple realbugs real world bugs evaluate testing tools.
therefore we still do not have direct indepthanswers to the aforementionedquestion.
ourworkaimstofillthisgapbyprovidingthefirstground truth evaluation of existing testing techniques for android.
to clearly present the necessity and novelty of our study table 1summarizes the key differences between the prior relevant studies and ours.
these differences show that our study presents a new complementary perspective from prior studies.
specifically thestudiesbychoudhary et al.
andwang et al.
compare one tool to another based on some selected apps in terms ofthe numbers of found unique crashes and achieved code coverage.
however duetothelackofground truth wecannotanalyzethe falsenegativesofthesetoolsonacommonbasis.asaresult quantifying the degree of effectiveness of these tools becomes difficult.
in practice these two studies face two additional challenges.
first existingtestingtoolsforandroidcannotreliablyprovidereproducible tests for found crashes see section in and section .
in due to the open technical challenges like gui flakiness andlengthytests .asaresult itisdifficulttoanalyzethebug features thusunabletoofferfine grainedanalysesonthetools bug finding abilities.
second existing tools heuristically de duplicate crashes by hashing stack traces which is difficult to make reliable thus likely incurringbug overcounting .
thestudiesbyzheng et al.
andbehrang et al.
investigate thetoollimitationsbyanalyzingtheuncoveredcodeofoneormore apps respectively.butthesetwostudiescannotgivedirectanswers totheraisedquestion becausetheyonlyfocusoncodecoverage whichisaproxyindicatorofbugfindingabilitiesandthecorrelation could be weak our study also observes this in section .
.
moreover they only evaluate one tool monkey .
thus the generabilityofthe identifiedtoollimitationsisunclear.
toachieveourstudy oneimportantstepistosetupagroundtruthbenchmarkwithreal worldbugsbasedonanagreed uponcriterion.tothisend weresorttotheindustrialpractitionersforgaining insights.
specifically we contact senior app testing managers and engineers with years working experience from five well knowncompanies i.e.
google facebook tencent bytedance andtestin amajormobileapptestingserviceproviderinchina withinournetworks.theirteams areresponsiblefor testingtheir own apps like google pay messenger wechat and tiktok which have billions of monthly active users worldwide or the apps from differentvendors.weconductindependenton lineinterviewswith thempercompany andaskthem5preparedandsomefollow up questionsto fully understandtheirtestingpractice.
finally all the interviewees respond that in practice they assign priority labels to the bugs reported by in house testing or app users andtheyprioritize criticalbugs namelyimportantbugs the bugs that break the major app functionalities and affect the largerpercentageofappusers inpractice realtimecrashreportingplatformsareusedtotrackcrashissuesfromend users .inother words criticalbugsaremorelikelytoaffectmoreusersinreality.
alltheintervieweesindicateandagreethattheabilityoffinding critical bugs isan objective metric tomeasure the effectiveness of testing tools in practice.
thus we decide to choose critical bugs astheagreed uponcriteriontosettingupthebenchmark.infact such ability has already been strongly and widely advocated for evaluating testingtoolsinboth industry andacademia .
to this end we take three steps to approach this study.
first wechooseopen sourceappsasthetargetstocollectcriticalbugs because their issuerepositories are public.
specifically we designatetheimportanceofbugs w.r.t.theirissuelabelsassignedbyapp developersthemselves.wecollectthebugswithcriticalissuelabels likehigh priority blocking release p1 urgent .wefinallyconstruct a dataset of real bugs from open source android apps by crawling the issue repositories of android apps.
this processtookussubstantialmanualeffort nearly twoperson months thatcouldnotbeautomated.itinvolvesmanuallyreviewingbug reports locatingbuggycodeversions buildingappbinaries and reproducing andvalidating bugs.section .1details this step.
second werigorouslysetupaunified extensibleexperimental infrastructure andintegrate monkey thestate of the practice testingtool andfivemostrecentstate of the artonesforthorough evaluation namely ape humanoid combodroid timemachine andq testing .
specifically we run these toolsonthecollectedbugs andprofiledifferentmetrics thenumber of bugs they can find i.e.
effectiveness how many times theycan trigger a bug givena number of runs i.e.
stability and howlong theytaketotriggerabug i.e.
efficiency .section .2detailsthisstep.
we name our dataset and infrastructure as the themisbenchmark whichaims for an objective evaluation w.r.t.ground truth.
finally wegivethedetailedquantitativeandqualitativeanalysis onthe testingresults ofthesetoolsbyreviewing the bugfeatures examiningthesetools implementations anddiscussing confirming with the tool authors.
we identify the common challenges that existing tools face and the factors that affect bug finding which have notbeenwell identifiedbythepriorstudies.inparticular weinvestigatedthe following research questions answeredinsection rq1 howeffectivelyandthoroughlycanthesetestingtoolsfind thecollected real world bugs?
rq2 are there any common challenges that allthese tools face in finding thesebugs byanalyzing the common false negatives ?
rq3 arethereanyfactorsaffectthesetoolsinfindingthesebugs bypair wisely analyzing the testing results of these tools ?
what aretheopportunitiesfor improvingthe state of the arts?
summary of main findings .
out of bugs .
bugs cannotbe detected byany testing tool whichindicates thata considerable gap exists between the existing tools and the collected 120benchmarkingautomatedgui testingforandroid against real worldbugs esec fse august athens greece table summary oftheselected automated guitestingtools forandroid inour study.
tool name venue open source maintechniqueneedapp source code?needapp instrumentation?supported sdkstool versionimplementation basis monkey random testing any default ape icse model based .
.
a53e98c monkey based humanoid ase deeplearning based any c494c7d droidbot based combodroid icse model based .
.
567b3f6 monkey based timemachine icse state based .
.
e79beb5 monkey based q testing issta reinforcementlearning based .
.
.
045825a real world bugs.
specifically these bugs impose five common major challenges blocking any tool e.g.
deep use case scenarios changes of system app settings andspecific user interaction patterns .
it indicates that continuous long term research effort is needed to tackle these challenges section .
.
on the other hand the gap is larger when these tools are applied individually they miss a large portion .
.
of bugs although we indeed observe their unique advantages in finding specific bugs section .
.
also we find these tools have obvious randomness in triggering bugs and no one can absolutely outperform the others in bug finding.
by pairwise comparisons we find that their testing results are largelyaffectedbythe guiexplorationstrategies stateabstraction criteria andsmallheuristics whicharetheopportunitiesfortool improvement in the short term section .
.
table6in section .
summarizestheconcretenewinsightsweobtainedfromthisstudy mostofwhichare unknown unstatedanddifficult to obtain.
contributions .our study makesseveral contributions ittakesthe firststeptoconductan empiricalstudy againstrealworld bugs to evaluate gui testing tools for android which presents anew complementary perspective from prior studies.
it carefully setups the themisbenchmark including the first ground truth dataset of real reproducible crash bugs and a unified extensibleinfrastructure to achieve this study.
it gives in depth quantitative and qualitative analysis on the testingresults.itobtainsnewconcretefindings mostofwhich wereunknown statedbefore.italsomotivatesthefutureresearch onthis topic withabenchmark discussedinsection .
.
testingtoolsforourstudy table2liststheselectedtoolsforourstudy.notethatweusethe latest versionsofthesetoolsat the time ofour study.
monkey .monkey isapurerandomtestingtool.inprinciple monkeyemitspseudo randomstreamsofuievents e.g.
touch gestures randomtexts andsomesystemevents e.g.
volume controls navigation .
monkey iswidely usedinindustryforstress testing because it is easy to use and compatible with any android version.
itisapopular baselineto evaluate newtestingtechniques.
ape.ape is a novel model based gui testing tool.
different frompriormodel basedtestingtoolslikestoatwhichusestaticgui abstractioncriteria apeusestheruntimeinformationtodynamically evolve its abstraction criterion via a decision tree which can effectively balance the size and precision of the model.
specifically with this dynamically refined model apegenerates ui events via a randomandgreedydepth firststateexplorationstrategy.moreover apealso internally utilizes monkey to occasionally emits random uieventsandsystemeventsto avoid stucking at local states.
humanoid .humanoid isthefirstdeeplearning basedtesting tool.
the core is a deep neural network model that predicts whichuielementsonthecurrentguipagearemorelikelytobeinteracted with by users and how to interact with it.
the model was trained upon a large scale crowd sourced human interactions dataset.humanoid is expected to drive the gui exploration towardsimportantstatesfasterasitprioritizesuielementsaccording to their importance andmeaningfulness like a human.
humanoid isbuiltondroidbot alightweight model basedguitesting tool which received stars on github at the time of our study.
combodroid .combodroid isanovelmodel basedtesting tool.
its core idea is to generate long and meaningful event sequences by combining a number of short independent use cases to explore deep app states.
combodroid obtains such use cases either from humans or automatically generates from a gui model constructed by gui exploration.
it then analyzes the data flow and gui transition relations between obtained use cases and combines them i.e.
concatenatingusecases inspecificorders to generate final tests.
moreover it works in a feedback loop i.e.
generating additionaluse caseswhen prior tests reachednewapp states.
timemachine .timemachine is a novel state based testing tool.differentfrompriortoolslike sapienz andstoat that evolve event sequences to maximize code coverage timemachine instead evolves a population of states which can be captured upon discoveryandresumedwhenneededforfindingdeeperrors.during testexecution itscoreistotakeasnapshotofevery interestingstate and add into the state corpus and travel back to a most progressive stateand execute next test when the current exploration cannot reachnewinterestingstates.itsuniquenessistheabilitytosnapshot and resume specific app state for further testing via the underlying android basedvirtual machine.
q testing .q testing isareinforcementlearning basedtestingtool.itusesatrainedneuralnetworktocompareguipages.ifa page is similar to any of prior explored gui pages the comparator will give a small reward.
otherwise the comparator will give a large reward.
these rewards are used and iteratively updated to guide the testingto cover more functionalities of apps.
sapienz andstoat.
we also evaluated sapienzandstoat although the tools in table 2outperform them.
sapienzuses genetic algorithms while stoatuses the stochastic model learned from an app to optimize test suite generation.
despite sapienzis closedsource and only compatible with android .
we still include it because itiswell knownandits techniqueisunique.
experimentalsetup .1themis sdataset collect open source apps .
we chose the open source android appsongithubasthemainsourceofcollectingreal worldbugs.to include as many candidate apps as possible we use two strategies we crawled all the apps from f droid the largest opensource app market because most of these apps are maintained on 121esec fse august athens greece ting su jue wang zhendong su github.
we used the keyword android and androidmanifest.xml theuniquefileofanyandroidproject tocollectmissing android apps that are only maintained on github but not released onf droid.we finally got unique android apps ongithub.
filter apps with critical issues .
we designate the importance of bugsw.r.t.the issue labels assigned by developers.
to collect as many critical issues as possible we built a github api based crawler to collect all the issue labels from candidate apps and manually identified different labels denoting critical issues.then weextracted shorten forms ofkeywordsfrom these labels for matching concrete issue labels.
for example we use block to match blocking release blocked sever to match severity high severity crash pri to match high priority priority critical majorpriority urgen tomatch urgency high p1 urgent importan tomatch important!
p2 very important bug high importance etc.
we find these shorten formsofkeywordscaneffectivelyreducethefalsenegativesofcriticalissues.byfilteringthoseappswhoseissuelabelscontainone ofthese12 shortenforms ofkeywords 200valid apps remained.
fromtheaboveresults wefindmanyappsdonothavecritical issuelabels.to furtheravoidmissingcriticalissues wecontinued to scan the remaining apps by checking whether any issue whosetitle bodyorcommentscontainthekeywordssuchas block severe critical major urgent important heavy derived fromthe12shortenformsofkeywords .wegot209validappswith such issues.thus we obtained409 valid apps intotal.
collect raw data of critical issues .
based on the previous data we manually inspected each issue of the apps with explicit criticalissuelabels andtheissuesofthe209appswhichhavematched keywords.specifically acandidateissueforourstudyshouldsatisfythesecriteria wasacrashbugwiththekeywords crash or exception haveexplicitreproducingsteps wassubmitted after 1st jan to avoid apps that could have outdated dependencies.wefinallygot228criticalissuesfrom51apps.
manyissues were excluded because the bug reports do not have clear reproducing steps and thecorrespondingappwasoutdated foralong time.
validateandarchive criticalissues .wemanuallycheckedand validatedeachofthese228criticalissues.thetypicalprocessis reviewingandunderstandingthebugreport locatingthebuggy codeversion buildingandinstrumentingthebuggyappversion reproducingthebug and archivingthebugdata.notethatin practice we often have to iterate between step and .
because many bug reports are not well formatted e.g.
missing buggy code version or code fixing commits we have to manually locate the rightversionbytrialanderroruntilwecanreproducethedescribed bug.moreover buildingappsis verytime consumingbecausewe usuallyhavetoresolveoutdatedormissingdependenciesandsetup necessary building environments e.g.
local servers .
reproducing bugsalsotakestimebecausewehavetolinkthestepstoreproduce in text with the app functionalities in guis.
many bug reports are not well written and many apps do not have clear documentation.
duringthis process anissue wouldbe excludedif we cannot fullyunderstandthebugreport thebuggyappversioncannotbe located thebuggyappversioncannotbebuiltintoanexecutable apk theissuecannotbefaithfullyreproducedonandroid7.
the version supported by the selected tools e.g.
the backendserverwasobsoleted thebugswereconcurrencyorcompatibility issues and the issue is deadly simple e.g.
start up crashes .
in addition we excluded an issue if its corresponding app is not self contained i.e.
testing such an app requires the non trivial collaborationswithhumansorotherdevices.forexample agithub client app was excluded because none of existing gui testing tools can automatically test it without any appropriate complicated app datapreparation e.g.
manuallycreatingasampleprojectrepository withproper code commits issues branches andotherinfo .
inourexperience itusuallytook1 4hourstovalidateoneissue without the guarantee of success.
we spent nearly two personmonthsonvalidatingthe228issues andobtained52validcriticalissuesfrom20apps.foreachsuccessfullyvalidatedissue wearchived itscorrespondingbugdata whichincludes anexecutableapk file jacoco instrumented a bug reproducing video the exception stack trace and other necessary information e.g.
login script .
table 3lists these critical crash bugs.
it gives the app name issueid appfeature codeversion numberofstarsongithub linesofcode loc numberofstepstoreproduce str andother bug information e.g.
whether it needs network access account loginorsystemsettingchangesforreproducing .notethat str denotes the number of shorteststeps observed by us and does not include the steps to login orchangeexternal systemsettings.
discussion .
note that the apps in table 3have diverse features and many of them are highly starred.
thus these apps could serve a good basis for evaluation.
on the other hand all these bugs canbedeterministicallyreproducedbyaguitestinourevaluation setting i.e.
an ideal testing tool could find each of them.
thus these bugs provide a fair basis for all testing tools.
we note that somepriorwork providescrashbugdataset.butwe didnot reusethosedatasets.because thosebugsareselected only based onwhether thebugreports describe bug reproducingsteps rather thanthe agreed uponcriterion of critical bugsin our study.
.2themis sinfrastructure we built a unified extensible infrastructure for our study.
any testing tool can be integrated into this infrastructure and deployed onagiven machine withone lineof command themis themis avd avd name n dev cnt apk apk name o output dir time testing time repeat run cnt tool tool name onecanspecifythetargetdevice avd name sizeofdevicepool dev cnt target app apk name testing time testing time number of runs run cnt the target testing tool tool name automaticlogin via uiautomator basedscripts showinggui screens checking crashesanddumpingcoverageat runtime.
efforts under the hood .
to build this infrastructure we took considerable time to coordinate with the authors of the selected tools to assure correct and rigorous setup.
we tried our best efforts tominimize thebias andensure that eachtoolis at its best state inbugfinding.
we detailour effortsoneachtoolas follows.
ape.wespentslighteffortstosetup ape butaroundtwoweeks tocoordinatewiththetoolauthorstoensureitsusability.forexample weobserve apefrequentlythrows outofmemory andno diskspace errorswhengivenalongrunningtime.toresolvethese 122benchmarkingautomatedgui testingforandroid against real worldbugs esec fse august athens greece table3 listof52real world reproduciblecrashbugs.
str denotesthenumberofshorteststepstoreproduce.
n l and s denote whether reproducingthebugrequiresnetworkaccess account loginandsystem settingchanges respectively.
appname issue id appfeaturecode version github starsloc str nls mahctqsast activitydiary personal diary .
.
activitydiary personal diary .
.
amazefilemanager filemanager .
.
.2k amazefilemanager filemanager .
.
.2k amazefilemanager filemanager .
.
.2k amazefilemanager filemanager .
.
.2k and bible biblestudy .
.
and bible biblestudy .
.
and bible biblestudy .
.
and bible biblestudy .
.
and bible biblestudy .
.
ankidroid flashcardlearning .
.4k ankidroid flashcardlearning .
.4k ankidroid flashcardlearning .
.
.4k ankidroid flashcardlearning .
.4k ankidroid flashcardlearning .
.4k ankidroid flashcardlearning .
.
.4k ankidroid flashcardlearning .
.4k aphotomanager photo manager .
.
collect onlineform .
.
commons wiki media .
.
commons wiki media .
.
commons wiki media .
.
commons wiki media .
.
commons wiki media .
.
firefoxlite webbrowser .
.
firefoxlite webbrowser .
.
firefoxlite webbrowser .
.
frost for facebook facebookwrapper .
.
geohashdroid geohashingapp .
.
materialfbook facebookclient .
.
nextcloud file sharingapp .
.
.3k nextcloud file sharingapp .
.
.3k nextcloud file sharingapp .
.
.3k nextcloud file sharingapp .
.
.3k omni notes notebookapp .
.
.1k open event attendee open event app .
.5k openlauncher home screen app .
.
osmeditor4android mapeditor .
.
.
osmeditor4android mapeditor .
.
phonograph musicplayer .
.
.4k scarlet notes notebookapp .
.
sunflower gallery app .
.
12k wordpress blog manager .
.5k wordpress blog manager .
.4k wordpress blog manager .
.4k wordpress blog manager .
.4k wordpress blog manager .
.4k wordpress blog manager .
.4k wordpress blog manager .
.4k wordpress blog manager .
.4k wordpress blog manager .
.4k total issues we discussed with the tool authors and finally reached the consensus thatallocating 2gbram gbinternal storage and gb external sdcard storage for the android devices could greatly mitigate this issue.
the reason is that apemaintains all gui states in memory and dumps large output files and logs.
thus we also assigned the similar hardware setup for other tools under study to ensure a fair basis.
in addition during the early stage of our study apefrequentlycrashedonanumberofappsinourdataset.thus wereportedalltheencounteredissues andthetoolauthorsfixed allthoseissuesbefore our deployment.
humanoid .
we spent around three days to setup humanoid .
the main effort goes to setting up the compatible tensorflow versionandresolving outdated librarydependencies.
othereffort includes fixing some obvious implementation bugs in droidbot whichhumanoid wasbuilton that affectedthe usability.
combodroid .wespentaroundoneweektocoordinatewith the tool authors to adapt combodroid into our infrastructure.for example to meet our requirements the tool authors modifiedcombodroid to supportrunningmultipletoolinstancesin parallel and provide separate tool modules to support our login scripts.
during the early stage of our study we reported some tool crash issues because combodroid may fail to instrument some apps bysoot.they fixedallthe issuesbefore our deployment.
timemachine .wespentaroundtwoweekstoadaptthetool into our infrastructure.
we made three major modifications which werelaterverifiedbythetoolauthorsbeforeourdeployment.
timemachine requirescodecoverageinformationforrunning.but its original emmabased coverage collection module cannot work on recent android apps created after which only support jacoco basedcoverageprofiling.thus wereplace emmawith jacoco.
because timemachine usesvirtualboxtosnapshotand resumeemulatorstates allthetime sensitiveinformationwillbe lostorimprecise.thus weaddedanadditionalmoduletoenable profiling time sensitive information e.g.
the time duration to triggeracrashbug .
weenhanced timemachine tosupportparallel 123esec fse august athens greece ting su jue wang zhendong su table results of bug finding for the selected tools.
found bugs denotes the total number of bugs found by eachtool.
n 5 n denotesthebreakdown i.e.
which bugswerefoundin nrunsoutofthefiveindependentruns.
mahctqsast foundbugs runningonservermachines automaticloginscripts googleservice apps required by some apps in our dataset and fixed several obvious implementationissuesto improve its usability.
other tools .
it is easy to setup monkey andq testing .
we spent around one week to setup sapienzandstoatby supporting parallelrunning andresolvingsomeusabilityissues.
.
experimentalsetup we deployed our experiment on a bit ubuntu .
machine cores amd 2990wx cpu and 128gb ram .
we evaluated all theselectedtoolsongoogleandroid7.1emulators apilevel25 .
each emulator is configured with 2gb ram 1gb sdcard 1gb internal storage and x86 abi image.
different types of external files including pngs mp3s pdfs txts docxs are stored on the sdcardtofacilitatefileaccessfromapps.weregisteredseparate accountsforeachbugthatrequiresloginandwrotetheloginscripts and during testing reset the account data before each run to avoid possibleinterference.notethatsince sapienzisonlycompatible withandroid4.
wewereunabletorun sapienzonallthe52bugs but only bugs verified to be reproducible on android .
.
the symbol incolumn sa intable 3denotesthatthecorresponding bug is not reproducible on android .
.
for stoat we allocated one hour for modellearningandfive hoursfor modelmutation.
we allocated one device i.e.
one emulator for each bug tool in one run one run required hours and repeated independent runs for each bug tool.
this time setting was decided based on the setupofthesetools intheiroriginalpapers apeuses1hour runs humanoid uses1hour 3runs combodroid uses12hours 3runs timemachine uses6hours 5runs and q testing uses 1hour 4runs andtwopriorstudies choudhary et al.
use1 hour and runs wang et al.
use3 hours and 3runs .thus ourtimesettingislargeenough.thewholeevaluationtookover 920machinehours notincluding sapienz .dueto android slimitation wecanonlyrun16emulatorsinparallelon onephysicalmachine.thus theevaluationtookusaround28days inadditionto around one weekfor deploymentpreparation.
experimentalresults and analysis .
rq1 quantifyingbug findingabilities the ultimate goal of testing tools is to find bugs.
we measured thebugfindingabilitiesoftheselectedtoolsfromthreedifferent perspectives effectiveness howmanybugscanbefoundbythese tools?arethereanydifferencesbetweenthebugsfoundbythese tools?
stability can these tools stably deterministically trigger these bugs across the five runs?
efficiency how many resources e.g.
time are requiredbythesetoolsto trigger thesebugs?effectiveness .intable thelasteightcolumnsgivethetestingresults ofmonkey m ape a humanoid h combodroid c timemachine t q testing q sapienz sa and stoat st on each bug respectively.
the symbol denotes that thetoolfoundthecorrespondingbug.incolumn thesymbol denotesnoneofthetoolscandetectthecorrespondingbug.we can see outof the bugs .
cannot be detected by any tool.itindicatesaconsiderablegapexistsbetweenallthetestingtools and thecollected real world bugs.we will look intothe gap in rq2.
table4summarizesthebugfindingresultsofindividualtools.
therow foundbugs denotesthetotalnumberofbugsthatwere found by individual tools across the five runs.
we can see ape monkey combodroid respectively found and bugs whilehumanoid timemachine q testing found18 and10 bugs respectively.theformerthreetoolsfoundafewmorebugs than the latter three ones.
stoatfound bugs while sapienz only found bugs out of the bugs which it targets.
we find ape the most effective one among these tools only found nearly half of all bugs.monkey ape humanoid combodroid timemachine q testing andstoatmissed30 .
.
.
.
.
.
and .
bugs respectively.itindicatesthegapbecomeslarger i.e.
morebugsweremissed when thesetoolswereapplied individually.
totakeacloselook fig.
a thebottom leftsection reportsthe pairwisecomparisonbetweenthetoolsontheirfoundbugs.the comparisonreportswhichbugswerefoundbybothtools reported ingray andwhichbugswerefoundby onlyoneofthetwotools.
thisprovidesusacloserlookatthebugfindingabilitiesofthese tools.
we can clearly see these tools have obvious differences in thebugsthattheyfound.forexample although monkey ape and combodroid arecloseinthenumbersoffoundbugs eachofthem can still find some bugs that the others cannot.
this phenomenon also applies to those tools that have obvious differences in the number of found bugs e.g.
apeandtimemachine .it indicates that noone canabsolutelyoutperform the others in findingbugs and instead they do complement each other by finding different bugs.
we willanalyzewhichfactorsaffectingthesetoolsinbugfindinginrq3.
.
stability .table4givesthebreakdownofwhichbugsweresuccessfullyfoundinhowmanyruns whichindicatesthestabilityofthese toolsinbugfinding.row n n denoteswhichbugsweretriggeredinnrunsoutofthefiveruns.forexample row 1 5 andcolumn m meansthereare5bugsof monkey weretriggeredinonly onerunoutoffiveruns.thisisanotherimportantmetrictoconsider whenadoptingatestingtool whichindicateshowrandomagui testingtoolcouldbeindetectingbugs.however thismetrichasnot beenreportedbythepriorstudies orbytheauthorsofthese tools.
we can see a non negligible number of bugs were only found in one run but missed in the other four runs see row 1 5 .
for example timemachine andapefound7 and6 bugs respectively inonlyonerun.indetail monkey ape humanoid combodroid timemachine andq testing have22.
.
and .
bugs respectively which were detectedinonly one run.
it indicates that existing tools have obvious randomness in bug finding andanon negligiblenumberofbugswereactuallydetectedbychance.
efficiency .fig.
b givesthebugdetectiontimeofindividualtools ontheirfoundbugs.wecansee ape combodroid andq testing 124benchmarkingautomatedgui testingforandroid against real worldbugs esec fse august athens greece timem .
combo.p value .
d .
timem .
apep value .
d .
timem .
monkey timem .
human.
combo.
timem.
human.
timem.
ape timem.
human.
combo.
ape combo.
ape human.
monkey timem.
monkeycombo.
monkey human.
monkey ape combo.
human.p value .
d .
combo.
ape combo.
monkeyp value .
d .
human.
apep value .35e d .
human.
monkey ape monkeyp value .
d .
p value .
d .
17monkey ape humanoid combodroid timemachine timemachine combodroid humanoid ape monkeyq testing timem .
q test.p value .
d .
combo.
q test.
human.
q test.p value .
d .
ape q test.p value .
d .
monkey q test.
q test.
timem.
q test.
combo.
q test.
human.
q test.
ape1014 q test.
monkey913 q testing a pairwise comparison between pairs of tools timem .
combo.
human.
ape monkey q test.
b bug detection time of individual tools figure1 a pairwisecomparisonofthetoolsintermsoffoundbugsandbugdetectiontime inminutes .theplotsonbottomleft section report the differences of found bugs between the pairs of tools the grey bars shows the common bugs found by bothtools whiletheplotsontop rightsectionreportthebugdetectiontimesbetweenthepairoftoolsonthecommonbugs thep valueandthestandardizedeffectsize darereportedontop rightwithineachplotifthecomparisonresultisstatistically significant .
b the bug detection time in minutes of individual tools on their found bugs.
we did not include sapienz and stoatinfigure because comparing therecenttools listed intable are our mainfocus.
arerelativelyfasterthantheothertoolsinbugfinding.specifically ape combodroid andq testing detect20 21and9 10bugs within the first one hour respectively while monkey humanoid andtimemachine detect and 15bugs respectively.
fig.
a the top right section reports a pairwise comparison betweenthesetoolsinboxplotsonthebugdetectiontime.notethat thecomparisonreportstherunningtimesonthebugsfound byboth tools .
we did not consider the bugs found by only one tool because that is unfair.
the detection time isthe offsetbetween thefirstbugtriggeringtimeandtheexactstartrunningtimeofa tool.forexample timemachine takesaround10minutestocreate and setup the vm image before it actually starts the testing.
we excludedsuchpreparationtimeforanytool.thus thebugdetection timewemeasuredishead to head.wecansee thedetectiontimes ofthesetoolshaveobviousdifferences .tovalidatethesignificance of these differences we used mann whitney u test a nonparametricstatisticalhypothesistestforindependentsamples to comparethedetectiontimesbetweentwotools.wereportthe pvalueandstandardized effect size at the top right corner for any pairwise comparison which is statistically significant.
here the significancelevel issetas0.
i.e.
ifp value .
thedifferenceisbigenoughtobestatisticallysignificant .thestandardizedeffect sizedindicates the magnitude of the difference d .
is small .
d .
is medium d .
is large .
from the results we can seeapeis more efficient than all the other tools in finding bugs.
combodroid ismoreefficientthan humanoid andtimemachine whilemonkey is more efficient than timemachine .
the major reason of such results is due to the differences of testing strategies andtoolimplementations.
.
rq2 common challenges andweaknesses thissectionaimstoidentifythecommonchallengesforexisting gui testingtechniques andtoolsinfindingthe collectedbugs.
analysismethods .
to achieve this analysis we focus on the bugs listed in table which have not been found by any tool.
specifically weusedthefollowinganalysismethodstoidentifythe challenges.
first we carefullyreviewed the18 bugstounderstand their features from both the gui and code levels.
second we examined the implementations of these tools to understand their testing strategies.
third weconductedtheonlinediscussionswiththetool authors we show the bug videos discuss the possible reasons why theirtoolsmiss thesebugs andconfirmour observations.
125esec fse august athens greece ting su jue wang zhendong su table characteristics of the bugs missed by all tools.
str isthenumberofshorteststepsto reproduce.
issueid str distinct transit.text inputssetting changesinteract.
patternsexter.
interact.
analysisresults .table5summarizesthecharacteristicsofthe18 bugsviaouranalysismethods.wedistilledfivemajorchallenges deepusecasescenarios specifictextinputs changingsystemor appsettings specificuserinteractionpatterns and externalapp interactions .
note that one bug may impose multiple challenges at thesametime anyofwhichcouldblockatestingtool.weillustrate thesechallenges as follows.
c1 event trace hard to reach deep use case scenarios .
table5 scolumn distincttransit.
denotesthenumberof distinct guipagetransitions alongthebugtriggeringtrace.thisnumber approximates how deep a bug resides in the app .
we can see that outof18bugs .
canonlybereachedafterbypassingmore than distinct page transitions.
specifically nextcloud s and and bible s 261arethetwobugsthatposethissolechallengefor the selected tools.
for example nextcloud s has distinct page transitions and its search space of event traces is at least eachnumberdenotesthenumber of executable events on one distinct gui page .
this big number blocksany toolfrom findingthe bugwithin six hours.
insight itremainsanopenchallengeforexistingtoolstoreach deepusecasescenarios althoughsometoolslike combodroid and timemachine were designed to reach deep app states humanoid was designed to act like humans to cover more app functionalities.
c2 text inputs no careful design of text input generation .
textinputsareimportanttotriggersomebugsinadditiontothe guiactions.intable 4bugsoutofthe18bugsrequiretextinputs and3outofthese4bugs requirecorner case orinvalid text inputsratherthanmeaningful orvalid ones.indetail ankidroid s requires to input the backslash codes e.g.
bsol osmeditor s requires to fill two invalid length characters and 0 intothetextfieldsof valueandage respectively wordpress s 10876requiresthatthecontentofapostunderwritingisleftas empty onlywordpress s requires to input a valid text not necessarilymeaningful thatcanobtainnon emptysearchentries.
however existingtoolsusuallygeneratepurerandomtextswithout careful designs and thus hard to detect these bugs.
for example combodroid andtimemachine simplyinherit monkey stext generationstrategy whichgeneratesrandomtextsofdigits letters or other symbols apeoptimizes monkey by additionally generating random integer float numbers and time date formatted strings.
humanoid randomly picks textsfrom the training data.insight testing tools should improve the text input generation strategiesforbugfinding .inadditiontogeneratemeaningfultext inputs they should also stress test apps with corner case or invalid text inputs by analyzing app code or the meaning of text fields or defining a list of risky text inputs .note that the prior studies only suggest generating valid text inputs because theyaimforimprovingcode coverage rather thanbug finding.
c3 system appsettings nodedicatedconsiderationofchanging system app settings .
changing system or app settings are common user behaviors .
however we find none of the selected tools dedicatedly considers the necessity of such changes in bugfinding especiallyforsystemsettings becausechangingsystemsettingusuallyrequiresinteractingwithsystemapp settings .
thisleadstotheincapabilityofdetectingsuchbugs.intable bugs out of the bugs involve setting changes and out of these 3bugs .
involvesystemsettings.specifically ankidroid s requires changing the default system language from english to another language andturning on one app preference option commons s 1581requiresthatthesystemlocationserviceisturned offbeforeenteringintothe nearbypageandthenisturnedonto use gps for location and commons s requires turning on the app s night mode theme in the middle of a specific event trace.
none ofthe toolscan detectthesebugs.
insight thekeychallenge ofconsideringsystemorappsettings duringguitestingisthelargespaceofpossibleguitestscausedby twomajorreasons.
onereasonisthediversityofsettingoptions.for example android7.1provides9maincategoriesofsystemsettings with over concrete setting options all of which could affectappbehaviors.butonlylimitedtypesofsystemsettingswere considered before .
another reason is the interleavings between the setting changes and the gui events.
prior work only changes settings before an app starts and does not change settings at runtime.
however all the bugs require changing settings at specific points at runtime.
note that the prior studies have not systematically observed this challenge.
because they analyze the main app code i.e.
java code coverage but we observe not all settingchanges especiallyforsystemsettings willleadtoobvious coverage changes in java code e.g.
changing system languages mainly involves an app s xml resource code .
in addition the implicationfrompriorstudies seetableiiiin togeneratesystem events i.e.
sending broadcast intents cannot work on changing system settings e.g.
security related settings like permissions and location cannotbe changedbysendingintents orapp settings.
c4 interaction patterns no explicit consideration of specific userinteraction patterns .
anothermajor challenge which blocksthesetoolsfromfindingbugsisthelackofgeneratingspecificuserinteractionpatternstoposeadverseconditions.wecan see that out of the bugs .
pose this challenge.
for example wordpress s requires uploading a number of pictures making the uploading takes some time to publish a post and then deleting the post when the uploading is still in progress osmeditor4android s 637requires removingallentriesbutthelast onefromitspageofvalidatorpreference commons s 1385requires a rotation action at one specific page wordpress s requires scrollingdownandback thesitespage revokingthepageloading of new items and select some specific items.
ankidroid s requires putting one specific activity in the background for a while 126benchmarkingautomatedgui testingforandroid against real worldbugs esec fse august athens greece andreturningbacktoit makingtheandroidsystemdestroyand recreate the activity .
despite these bugs seem corner cases the corresponding userinteraction patterns are common inreality.
insight we carefully examined the relevant covered code of these bugs.
it reveals that manifesting these bugs requires exercising specific sequences of callback interactions .
for example wordpress s 6530involvestheinteractionsbetweenthecallbacksofguievents for deleting the post and those of the background thread for uploading the pictures commons s involves the interactions between the lifecycle callbacks of an activity.
however existing tools only focus on maximizing line or activity coverage which ishardtostresstestdifferentcallbackinteractions.oneplausible wayistodesignspecificcoveragecriteria e.g.
callbacksequence coverage ormutationoperators toguidetesting.
notethat thisinsightcannotbeobtainedbypriorstudies becausesuch bugswill not show differences in termsoflineoractivity coverage.
c5 externalinteractions seldomconsidertheinteractions withotherapps .
outofthe18 bugs .
requireinteracting with other apps on the device to obtain the desired data e.g.
a picture file to enable testing the follow up functionalities.
however most tools do not explicitly consider the necessity of these interactionsinbugfinding andinsteadtheyconstrainthetesting effortswithintheappundertest.forexample humanoid willsimplyrestartthetestedappaftercertainstepsofexplorationifitis stillexploringthe otherapps.
c5maybe relatedwithc1andc3.
insight it is much desirable for these testing tools to construct external intents provided with desired data or files to simulate the purpose ofexternal app interactions.
.
rq3 factors andopportunities this section discusses the factors we observed that affect bug finding on the collected real world bugs and the opportunities for tool improvements.specifically weconducttheanalysisbasedonthe testing results of each tool in table 3and the pairwise comparison resultsinfigure .wefollowthesameanalysismethodsinrq2 andsummarize our majorfindingsinthe following aspects.
gui exploration testing strategies affect bug finding .
the toolswestudiedemploydifferentguiexplorationstrategies.
indeed thesestrategiesshowtheiruniqueadvantagesinfindingspecificbugs.
forexample monkey ape combodroid timemachine found4 and1bugs respectively whichthe othertoolscannotfind.
butwealsoobservethattheexplorationstrategieswithmoredirectandfine grainedguidanceseemmoreeffectiveinfindingbugs.
forexample intable ape combodroid andstoatdetectmore bugs than humanoid timemachine andq testing .
specifically bothhumanoid andq testing usetraineddeepneuralnetworkto guideexploration humanoid explores towardshuman preferred pages while q testing prefersexploringpageswithdifferentusagescenarios.
timemachine heuristicallydeprioritizesthosepages that have been visited more times see section .
in .
basically thesethreetoolsareonlyguidedtocovermoreguipages.however thismaynotbedirectlylinkedwithbugfinding.incontrast apedifferentiates andexplores distinctapp states by dynamically refiningstateabstraction combodroid stress teststhedata flow relations at the app code level while stoatoptimizes different event compositions in gui tests via the stochastic model.
thesethree tools are informed by more fine grained analysis and thus are likely to detectmore bugs.
opportunities integrating fine grained program analysis resultsintogui explorationcould be beneficialfor bugfinding.
stateabstractiongranularityaffectsbugfinding .gui layouts are usually used to abstractly represent concrete app states during testing.duetothelargesearchspaceofguipages guistateabstraction strategies or gui comparison criteria are commonly adopted by testing tools to improve testing scalability.
we observe thatthebugfindingabilitiescouldbeaffectedbythestateabstraction granularity which unfortunately has not been well recognized by existingtools.specifically weobservethatthetoolswithmorefinegrainedabstractioncoulddetectmorebugs whichcorroboratesthe preliminaryfindingsof see section6.
.
for example we observe that timemachine andq testing missedsometrivialbugslike wordpress s 11135and nextcloud s .
thetool authorsof timemachine explained to us thatone majorreasoncouldbe timemachine sstateabstractioncriterion istoocoarse.inpractice timemachine usesavariantofthec lv3 abstractioncriterion whichonlyuseslayoutwidgetstoabstract gui states to decide whether a given state is a new interesting state.however thisabstractioncriterioncouldbetoocoarse and timemachine thus fails to identify and snapshot some critical states which are the preconditions of the bugs into its state pool.
asaresult itmaymissthechancetotriggerthebug.
q testing usesamorecoarse grainedabstractioncriterion betweenc lv2 and c lv3 which only differentiates two gui pages if they are from two different app usage scenarios.
in fact timemachine and q testing findtheleastnumbersofbugs comparedtoothertools.
meanwhile alltheaforementionedthreebugscanbedetected byape humanoid andcombodroid .
because combodroid and humanoid usethefine grainedc lv4criterion whichusesboth thelayoutandexecutablewidgetstoabstractstates while apededicatedlyproposesadynamicallyrefinedstateabstractionstrategy to achieve betterbalancebetween state precision andscalability.
on the other hand monkey is pure black box and does not do any abstraction.
it treats everygui page as uniqueand emits gui events at any random screen coordinates and thus sometimes suffers from scalability issues.
for example monkey cannot detect firefoxlite s whichonlyrequires5guievents.thereason isthatthisbugrequiresclickingasmallwidgetatthebottom right corner ofthe first gui page and then clickingone specific setting option among many others on the next page.
as a result monkey has very lowchance to bypassthesetwopagesto trigger the bug.
opportunities defining appropriate state abstraction criterion isimportantforbugfindingbutstillanopenproblem.onepossible solutionistodefinespecificgranularityforspecifictypesofapps or functionalities to reduce the chance of missing important states.
small heuristics affect bug finding .
we find some tools implementedsmallheuristics.
despitetheseheuristicsarenotthefundamentaladvantagesofthecoretestingtechniques they bug finding abilities .
forexample monkey bydefaultdoesnotsupport long touch so it cannot detect amazefilemanager s which requires a longtouchevent.butothermonkey basedtools i.e.
ape combodroid timemachine founditbecause they implemented long touch .
127esec fse august athens greece ting su jue wang zhendong su table6 concretenewinsightsobtainedfromourstudyandthecomparisonwithpriorstudiesonthesenewinsights and ?
denote that thecorresponding studydoes doesnot oronlypartially does obtainthe new insight .
studiesrq1 bug findingabilities rq2 commonchallengesinfindingbugs rq3 factorsand opportunities false negativestesting stabilitytesting efficiencyc1 event trace c2 text inputs c3 system app settings c4 interaction patterns c5 external interactions testing strategiesstate abstractionsmall heuristics choudhary etal.
?
wangetal.
zhengetal.
?
?
behrangetal.
?
ourstudy inaddition apeandcombodroid implementsaspecialstrategy to input texts one of the common user behaviors to input text longtouchthetargettextfieldtoselecttheoriginaltext clearthe wholecontent andthen inputthenewrandomtext.duetothis heuristic only apeandcombodroid foundmaterialfbook s which requires a long touch to invoke the copy paste operation.
alltheothertoolscannotfindthisbugbecausetheyinputtextsvia directlyoverwriting the originaltext.
sometoolsinternallycomplementtheircoretestingtechnique with some heuristics to improve testing effectiveness.
for example apeandcombodroid occasionally invoke the default monkey to do random testing.
as a result they can trigger some bugs that are only likely to be triggered by monkey.
for example monkey may slidedownthenotificationbarbyrandomswipesandchangesome settings therein by random touches.
as a result all monkey based toolscandetect openlauncher s whichrequiresopeningthe do not disturb setting.
humanoid andq testing cannotdetect this bug due to the lack of any monkey like random testing strategies.
opportunities designing and integrating small heuristics by simulatinghuman appinteractionpatterns e.g.
specificuiactions text input styles putting apps in the background and returning back to it can improve bugfinding.
.
discussion newinsightsobtainedfromourstudy .table6summarizesthe concretenewinsightsobtainedfromourstudy.wecanseethatmost of the new insights have not been identified by the prior studies .specifically duetothelackofaground truthbenchmark the studies are difficult to do the in depth analysis like rq1 rq3 while the studies can only identify some or partial insights in rq2 because they identify the tool limitations in achievinghighcodecoverageratherthanbugfinding.wenotethat thepriorstudies identifiedsomeothertoollimitationslike requiringaccount loginandcollaborationwith otherdevices.we excludedsuchlimitationsintheevaluationsetup e.g.
byproviding auto loginscriptsandfocusingon self contained apps because theseare not the limitationsofthe core testingtechniques.
applications of our study .
our study can have three major applications.
first thedetailedanalysisinrq1 rq3distilledmany important findings which can help enhance optimize and extend existing testing tools.
it also pointed out some open research problems e.g.
howtoefficientlyfindsystemsettingrelatedcrashes and betterbalance betweendifferent guiabstractioncriteria.
second thethemisbenchmarkcanbeusedtoquantitativelyandqualitativelyevaluatenewtestingtechniquesforandroidinacontrolled rigorousenvironmentlikedefects4jforjava.forexample anew testingtechniquecouldcompareitselfwiththeresultsofselectedtoolstovalidateitseffectiveness andchallengeitselfwiththe18 critical bugs which no tool can find to prove its advancement.
third theinfrastructurecanbeusedtofacilitateotherresearchlike bugreproducing faultlocalization andprogramrepairfor android.
threats to validity the validity of our study may be subject to somethreats.
onethreat istherepresentativenessofourbugdataset andthegenerabilityofourfindings.toreducethisthreat weinterviewed the industrial practitioners to obtain the agreed upon selectioncriterionofbugsthatconformstorealindustrialpractices.
the data collection is based on a large set of android apps and all the issues with critical labels are assigned by developers.
we carefullyinspectedeachissueandcollectvalidoneswithoutany bias seesection .
.table3showstheappsarediverse andthe analysisinrq2 rq3alsoshowsthebugshavedifferentfeatures.
moreover the interviewees observe that critical bugsdo not have obvious differences from other less important ones in bug manifestation e.g.
thedifficultyofbug triggeringandthetestlength .
thus ourstudyfindingsbasedoncriticalbugscouldbegeneralized to real world bugs.
in the future we could incorporate more bugs tofurthermitigatethisthreat.
anotherthreat isthecorrectnessof evaluation and result analysis.
to counter this we made considerableefforttosetuparigorousexperimentalinfrastructure and resolvedmanytoolissuesbefore the deployment seesection .
.
we carefully examined tool implementations and discussed with thetoolauthorstoanalyzetoolabilitiesandvalidateourobservations.
the experimental data and results were cross checked by the two co authors.
we also made the themisbenchmark publicly available for replication.
conclusion in this paper we take the first step to empirically evaluate automatedguitestingforandroidagainstreal worldbugs.weevaluate severaltestingtoolsonthe52real reproduciblebugs andreveal many new findings.
we find a considerable gap in these tools finding the collected bugs.
we identify five common major challenges thatfutureworkshouldaddress andthefactorsthataffectthese tools in bug finding.
our study provides a new complementary perspective from prior studiesto analyze existing testingtools.