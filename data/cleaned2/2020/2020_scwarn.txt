identifying bad software changes via multimodal anomaly detection for online service systems nengwen zhao tsinghua university bnrist beijing chinajunjie chen college of intelligence and computing tianjin university tianjin chinazhaoyang yu tsinghua university bnrist beijing china honglin wang bizseer beijing chinajiesong li china guangfa bank guangzhou chinabin qiu china guangfa bank guangzhou china hongyu xu china guangfa bank guangzhou chinawenchi zhang bizseer beijing chinakaixin sui bizseer beijing china dan pei tsinghua university bnrist beijing china abstract in large scale online service systems software changes are inevitable and frequent.
due to importing new code or configurations changes are likely to incur incidents and destroy user experience.
thus it is essential for engineers to identify bad software changes so as to reduce the influence of incidents and improve system reliability.
to better understand bad software changes we perform the first empirical study based on large scale real world data from a large commercial bank.
our quantitative analyses indicate that about .
of incidents are caused by bad changes mainly because of code defect configuration error resource contention and software version.
besides our qualitative analyses show that the current practice of detecting bad software changes performs not well to handle heterogeneous multi source data involved in software changes.
based on the findings and motivation obtained from the empirical study we propose a novel approach named scwarn aiming to identify bad changes and produce interpretable alerts accurately and timely.
the key idea of scwarn is drawing support from multimodal learning to identify anomalies from heterogeneous multi source data.
an extensive study on two datasets with various bad software changes demonstrates our approach significantly outperforms all the compared approaches achieving .
f1 score on average and reducing mttd mean time to detect bnrist beijing national research center for information science and technology junjie chen is the corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
.
.
.
in particular we shared some success stories and lessons learned from the practical usage.
ccs concepts software and its engineering maintaining software .
keywords software change anomaly detection online service systems acm reference format nengwen zhao junjie chen zhaoyang yu honglin wang jiesong li bin qiu hongyu xu wenchi zhang kaixin sui and dan pei.
.
identifying bad software changes via multimodal anomaly detection for online service systems.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
introduction for large scale online service systems such as social networking e bank and search engines engineers need to frequently conduct software changes aiming to fix bugs deploy new features adapt to environmental change and improve software performance .
because of importing new code or configurations software changes are more likely to incur service outages user dissatisfaction and huge economic loss .
based on the experience from google sre site reliability engineering about of incidents are related to software changes.
therefore it is essential to avoid incidents and ensure service quality under software changes.
in the literature tremendous efforts have been devoted to ensuring the quality of software changes which can be divided into three categories risk analysis and impact assessment before deployment reliable launching strategy during deployment and monitoring performance and identifying bad changes after deployment .
although each software change mustesec fse august athens greece n. zhao j. chen z. yu h. wang j. li b. qiu h. xu w. zhang k. sui d. pei be rigorously reviewed and extensively tested before deployment e.g.
unit test and integration test errors and bugs could remain uncaught in the real production environment due to the discrepancies between testing and production environment in cluster size complex component interactions resource contention os library versions and unexpected workload .
thus it is necessary to monitor service and identify bad changes in time after deployment which is the target of our work so that proactive actions could be taken to avoid further outages and economic loss.
in the real world it is time consuming and error prone for engineers to check each software change manually.
it is because there are hundreds of changes per day in large systems and numerous monitoring data are involved in each software change.
to overcome it some works are proposed to replace manual inspection with an automated anomaly detection algorithm .
these existing works mainly focus on the behaviors of business kpis key performance indicators e.g.
response time and success rate .
nevertheless it is a little late to identify anomalies from business kpis which may have incurred poor user experience and economic loss.
to better understand the influence and behavior of bad software changes we conduct the first empirical study based on realworld data from a large commercial bank over the last two years and obtain three key findings.
about .
of incidents are caused by changes on average indicating that software change is indeed failure prone rq1 .
abnormal behaviors of bad software changes are very complex and involve heterogeneous multi source data including business kpis characterizing the status of application layer machine kpis characterizing the status of underlay infrastructure like database server and middleware e.g.
cpu usage jvm heap space and logs recording detailed running information of the service .
besides some normal changes could also lead to abnormal but expected behaviors e.g.
resource expansion would bring a decrease in cpu usage and response time rq2 .
the current practice fails to identify abnormal signals earlier hidden in other data sources besides business kpis and ignores the expected changes resulting in unsatisfactory performance rq3 .
to sum up on the one hand this study further motivates the necessity of identifying bad changes earlier on the other hand it provides us some guidelines to design an effective approach.
in this paper we propose a novel approach named scwarn to identifying bad changes and producing warning signals earlier by fusing heterogeneous multi source data.
there are two major challenges when designing such an approach.
the first one is the lack of sufficient abnormal labeled data since systems tend to run stably.
besides different bad changes tend to exhibit various abnormal patterns on different data sources.
thus it is challenging to design the approach in supervised ways.
the second one is how to extract useful information and identify anomalies from heterogeneous multi source data involved in software changes accurately.
to tackle these challenges we leverage the idea of multimodal learning to identify anomalies in an unsupervised manner which has been widely used to learn information from cross modality e.g.
text and images .
more specifically scwarn consists of four main steps i.e.
data preparation .
multimodal anomaly detection .
alerting with analysis report .
and action decision .
.
scwarn first preprocesses raw data to transform heterogeneous data into unified time series based on log parsing .
then dueto the ability of lstm long short term memory to model on temporal data scwarn adopts multimodal lstm to detect anomalies from multi source data .
the core ideas are using lstm to learn the normal pattern in each single source time series and utilizing multimodal fusion to capture the inter correlations among multi source data.
next for online detection equipped with our scenario specific alerting strategy the model could generate alerts with reports to notify engineers to pay attention to suspicious software changes.
finally abnormal but expected changes would be excluded by a policy based strategy based on a knowledge base.
engineers then confirm the result and take corresponding actions e.g.
rollback to prevent further service outages.
we conduct an extensive study to investigate the performance ofscwarn on two widely used benchmark systems i.e.
trainticket and e commerce following the existing work .
specifically based on some common and typical incidents caused by software changes in the real world rq2 we simulate ten types of bad changes to the two systems and construct two datasets which contain a total of bad changes and normal changes.
our experimental results show that scwarn is able to identify bad changes more accurately and earlier.
in detail our approach could achieve .
f1 score on average while the average f1 score of three state of the art compared approaches gandalf funnel and lumos are only .
.
and .
respectively.
the mean time to detect mttd is reduced by .
.
with scwarn .
besides the superiority of our multimodal lstm algorithm is illustrated by comparing it with several related anomaly detection algorithms and the average f1 score is improved by up to .
.
also the contributions and necessity of integrating multi source data are confirmed by comparing with methods using single data source.
to sum up this work makes the following major contributions we perform the first large scale empirical study of incidents induced by bad software changes and obtain some key observations which largely motivate our work.
we propose a novel approach named scwarn to identifying bad software changes more accurately and earlier which could notify engineers to pay attention to bad changes so as to take proactive actions to avoid further service outages in advance.
an extensive study shows that scwarn significantly outperforms all the compared approaches achieving .
f1 score on average and reducing mttd by .
.
.
besides we have released our tool on github for better reproducibility.
background and empirical study .
software change management in online service systems software changes are frequent and inevitable aiming to deploy new features fix bugs adapt to environmental change and improve performance.
figure presents a typical procedure of software change management in industry including the following five steps problem identification .
during the lifecycle of software once a problem is identified by engineers through daily operation monitoring alerts or user complaints they will conduct a change to fix the problem.
the common problems include code bugs poor performance error configurations new requirements etc.identifying bad software changes via multimodal anomaly detection for online service systems esec fse august athens greece changeticketdeveloperoperator .changepreparation3.changereviewteam5.post changemonitoring4.deploymentonlineservicesystems .problemidentification3.ch figure workflow of software change management change preparation .
after problem identification engineers would prepare for a change including proposing a solution to the problem developing code and software testing.
then they will submit a change ticket to service teams which records some detailed information of this change such as service system start time operations involved monitoring data emergency action.
change review .
a group of developers and team leaders will get together to review this change and conduct risk assessment e.g.
how significant this change is how many users it will affect and which components it will affect.
deployment .
after passing the change review engineers would conduct deployment according to the description in the change ticket.
engineers typically deploy software changes using the dark launching strategy.
post change monitoring .
once the deployment is completed engineers need to continuously and closely monitor the service performance so as to identify and mitigate bad changes timely.
in this paper we focus on post change monitoring aiming to identify bad changes timely and prevent further service outages.
.
an empirical study of software changes to better understand the characteristics of bad software changes we conducted the first empirical study based on real world data from a large commercial bank aiming to address the following research questions rqs rq1 what is the percentage of incidents induced by bad software changes?
rq2 what are the root causes and behaviors of bad changes?
rq3 is the current practice of identifying bad software changes good enough?
.
.
study method.
we used five service systems as subjects that are developed and maintained by different product teams.
for each service all incident tickets and change tickets over the last two years are collected and analyzed.
due to the privacy policy we hid some details such as the specific number of collected incidents and changes.
about ticket analysis and labeling three authors manually labeled whether an incident is caused by a change rq1 and the root cause of a bad change rq2 independently through analyzing troubleshooting steps and incident reason description recorded in the ticket under the supervision of cohen s kappa coefficient .
disagreements were discussed with engineers in charge of the tickets to reach a consensus finally.
.
.
rq1 the percentage of incidents induced by software changes.
to answer this rq we counted how many incidents are caused bysoftware changes according to the study method in .
.
.
figure shows the percentage of incidents incurred by software changes in these five services.
clearly we observe that change is an important factor leading to incidents accounting for from to .
on average .
in addition to this study we also investigated some public data.
as stated in and changes account for about and of incidents in google and baidu respectively.
besides we also looked at publicly disclosed recent incidents from google cloud .
each incident report includes a detailed issue summary service impact root cause and remediation action.
we found that of the incidents are incurred by changes accounting for .
to further illustrate the failure proneness of change we counted the number of software changes per day and the number of all incidents per day in service s1during two months as shown in figure .
it is clear that the number of incidents has a positive relationship with the number of changes.
we conducted pearson correlation analysis between incidents and changes.
the pearson coefficient is .
indicating a moderate positive correlation with p value .
which further supports our observation.
in summary software changes are indeed failure prone which could bring great trouble for engineers and customers in software maintenance.
thus it is imperative to design an automated and accurate approach to identifying bad changes timely and reducing the influence of incidents which largely motivates our work.
.
.
rq2 root causes and abnormal behaviors of bad changes.
through analyzing all change tickets we summarized three types of changes i.e.
code change e.g.
adding new features configuration change and infrastructure layer change e.g.
replacing hardware devices .
in particular bad changes may be induced by different factors which we call root causes.
in this rq following the study method stated in .
.
we summarized five types of root causes including code defect configuration error incompatible software version resource contention and some other factors e.g.
error operations .
the percentage of different root causes is shown in figure .
it can be observed that code defect and configuration error account for a large proportion accounting for .
besides to further help understand the behaviors of bad changes we summarize some typical incidents as shown in table the first nine rows .
we can observe that various monitoring data business kpis machine kpis and logs from multiple sources could be influenced by software changes.
taking the code defect leading to memory leak as an example the fullgc log patterns behave abnormally in java gc garbage collection logs cpu usage and memory usage would increase and finally response time would increase.
thus it is essential to integrate heterogeneous multi source data to detect bad changes.
another interesting observation is abnormal data behavior does not necessarily mean that this software change is bad.
it is because some software changes could lead to abnormal but expected behaviors of monitoring data.
for example replacing an old server with a new high performance server could incur a decrease in cpu usage and response time.
table the last three rows presents some common software changes that could lead to abnormal but expected behaviors.
therefore how to filter out expected changes and reduce false alarms should be considered.
.
.
rq3 investigation of current practice.
the current practice of identifying bad changes in the bank we studied adopts esec fse august athens greece n. zhao j. chen z. yu h. wang j. li b. qiu h. xu w. zhang k. sui d. pei s1 s2 s3 s4 s5 service020406080percentage figure percentage of incidents induced by changes aug aug aug sep sep datecount changes day incidents dayfigure incidents per day and changes per day code defectconfiguration errorversion resource contentionothers010203040percentage figure percentage of different root causes table typical bad expected software changes and corresponding behaviors of monitoring data type change operation abnormal behaviors of multi source monitoring data bad software changeconfiguration error wrong ip address error messaegs in network logs e.g.
address conflicted related machine kpis and business kpis behave abnormally configuration error missing modification of correlated configuration error messages in application logs business kpis behave abnormally configuration error deleting white list by mistake business kpis behave abnormally code performance slow sql full table scan and some related database problemsdatabase e.g.
active session lock wait and related machine kpis e.g.
disk space cpu usage behave abnormally response time increases code performance memory leak fullgc log pattern appears frequently in gc log machine kpis e.g.
jvm heap space memory usage behave abnormally response time increases code performance code self loop or dead loop system load cpu usage and other machine kpis behave abnormally response time increases code logic bug wrong database table name error date format error messages in application logs success rate decreases resource contention related machine kpis e.g.
i o wait cpu usage behave abnormally expected software changereplace high performance server resource expansion related machine kpis e.g.
cpu usage memory usage decrease response time decreases traffic switch cpu usage decreases code logic changes e.g.
some new steps are added to transaction process related business kpis behave abnormally e.g.
response time increases strategy to inspect the behavior of business kpis e.g.
response time .
through analyzing incident tickets and discussing with engineers we found that the performance of the current practice is far from satisfactory.
there are three reasons accounting for it.
first as stated in .
.
software changes usually involve multi source data.
thus only focusing on business kpis fails to identify anomalies hidden in other monitoring data leading to long mttd sometimes up to tens of days and some missing alarms.
for example an incident was noticed five days after the change due to the increase in response time.
if integrating multi source data however the incident could be identified earlier from the anomalies of jvm heap space.
second as well as some other anomaly detection methods e.g.
holt winters is not designed especially for identifying bad changes.
thus directly applying it ignores the specific scenario and characteristics of software change e.g.
integrating various data abnormal but expected behaviors and non transient anomalies resulting in some false alarms.
the third reason is the drawback of the anomaly detection algorithm itself since it is hard for and other simple statistical anomaly detection methods to deal with complex data.
although there exist some popular products e.g.
dynatrace and datadog in industry used to monitor service they still fail to overcome the above drawbacks together.
to sum up the current practice of identifying bad changes should be further improved and it is essential to propose an effective approach to tackle the above drawbacks.
.
summary based on the above quantitative and qualitatively analysis we obtain three key findings software change is frequent but failure prone.
thus it is promising to ensure service quality by catching bad changes timely.
software changes usually involve heterogeneous multi source data.
besides abnormal behaviors are not necessarily caused by bad changes.
thus it is crucial to integrate multi source data and exclude noises when designing an approach.
current practice mainly focuses on the behavior of business kpis and fails to filter out expected changes false alarms resulting in poor performance.
these findings support the motivation to identify bad software changes.
thus it is necessary to propose an effective and automated approach to ensuring service quality under software change which can identify bad software changes accurately and earlier based on involved multi source monitoring data and make a decision with timely protective actions to avoid further service outages.
approach the overview of scwarn is presented in figure which includes four main steps data preparation multimodal anomaly detection alerting with analysis report and action decision.
the key idea of scwarn is drawing support from multimodal anomaly detection to deal with heterogeneous multi source data.
specifically the firstidentifying bad software changes via multimodal anomaly detection for online service systems esec fse august athens greece historicaldatadecisionalertingtrainingtestingreal timedata startchangemultimodalmodelanomalyscore incidentsexpectedchangerollbackadaptation thresholdselection finishchangepre processingknowledgebaseanalysisreportmultimodalanomalydetectionalertingwithanalysisreportactiondecisiondatapreparation figure overview of scwarn l1 02t02 systemd reloaded system logging service.l2 02t02 vsftpd ok login client x.x.x.xl3 02t02 dd.forwarder warning transaction.py queue is too big removing old transactions...l4 02t02 vsftpd ok login client x.x.x.xhistoricaltraininglogstimeseriesdata3.logserializationt1 timesystemd reloaded system logging service.
l1 t2 timevsftpd ok login client ip l2 l4 t3 time dd.forwarder warning queue is too big removing old transactions... l3 .logparsinglogtemplatesl1 t1l2 l4 t2l3 t3onlinelogs2.matching figure illustration of log preprocessing step of scwarn is transforming heterogeneous data into unified time seriesa based on log parsing .
.
then we adopt multimodal learning to detect anomalies from multi source monitoring data logs and kpis .
the core ideas are capturing the temporal dependency in each time series via lstm model and encoding the inter correlations among multi source data via multimodal fusion .
.
next once the anomaly score provided by multimodal lstm violates our designed alerting rule a warning signal would be triggered to notify engineers to pay attention to the suspicious change with an interpretable analysis report provided by scwarn .
.
after identifying suspicious changes we design the action decision component to remove expected changes and catch the real bad changes.
finally engineers could take proactive actions to stop bad changes e.g.
rollback and prevent further service outages .
.
in the following we will present each step in detail.
.
data preparation to characterize the behavior of each software change we need to collect all related monitoring data because we do not have prior knowledge about what failure this change will cause.
for example the database change could incur database crash or performance degradation which are reflected in different data sources.
as stated in integrating multi source data not only helps to identify bad changes in advance but also could discover the latent relationship across various data sources so as to obtain more accurate and comprehensive results.
there are three types of data involved in our scenario i.e.
business kpis e.g.
response time machine kpis e.g.
cpu utilization and logs which characterize the running status of the service from multiple aspects.
thus how to fuse these heterogeneous data is a significant challenge.
intuitively kpis are in the format of time series and can be easily handled while logs are usually semi structured or unstructuredtexts which are generated using the print function with a string template and detailed information as parameters.
typically logs should be properly parsed for further analysis .
we adopt the state of the art log parsing algorithm drain to extract log templates and its superiority and efficiency have been demonstrated in .
as shown in figure given historical raw logs we first utilize drain to extract nlog templates the first step in figure and n .
when online logs arrive we could match each log message to the corresponding template the second step and count the number of occurrences of each template per minute and acquire ntemplate time series.
meanwhile the total number of logs and the number of new logs that cannot match the existing templates are also counted the third step .
in general in the normal state most online logs could be matched to templates while the number of new logs would increase in the abnormal state.
thus after log preprocessing raw log messages can be transformed into n 2time series.
in this way all heterogeneous data can be transformed into the format of time series which will be easily handled in the following model.
.
multimodal anomaly detection after data preparation it is still challenging to model the multisource monitoring data since it not only requires capturing the normal pattern of each time series but also needs to encode the inter correlations among multi source data.
another significant challenge is we cannot obtain enough high quality labeled data and tend to adopt unsupervised approaches.
in scwarn we propose to adopt the technique of multimodal learning in an unsupervised manner to overcome the above two challenges.
in recent years the ability of lstm to handle complex temporal or sequential data has ensured its widespread application in domains including natural language processing speech recognition and time series forecasting.
compared with traditional recurrent neural network rnn lstm has shown its strong capability to maintain the memory of long term dependencies due to a contextbased weighted self loop that allows them to forget past information in addition to accumulating it .
therefore it can learn the relationship between past and current data and has shown remarkable performance in various sequential data.
in scwarn we apply lstm to capture the temporal dependency in each time series.
to encode the inter correlations among multi source data we draw support from multimodal learning which is a powerful model for data fusion to acquire the joint representation of different modalities e.g.
images and texts kpis and logs in our problem .
to the best of our knowledge there are three popular types of multimodalesec fse august athens greece n. zhao j. chen z. yu h. wang j. li b. qiu h. xu w. zhang k. sui d. pei businesskpismachinekpislogsmultimodalrepresent tationfusionx!
!
unimodalrepresentationlearningwithlstm lstmlstmlstm prediction receiving block blk 370867src ...x!
!
x!
!
p!
p!
p!
f x!
!
f x!
!
f x!
!
f f f f inputfullyconnectedlayerdatapreparation figure multimodal anomaly detection model fusion in the literature .
early data level fusion integrating multi source data into a single feature vector as the model input.
late decision level fusion aggregating the decisions from multiple models where each model is trained on a separate modality.
intermediate fusion which converts raw inputs to a higher level joint representation by mapping the input through a pipeline of neural network layers .
in particular the majority of existing works adopt intermediate fusion due to its strong ability to learn fused joint information.
inspired by this idea we also utilize intermediate fusion in our approach to capture the hidden correlations existing in multi source data.
overall the core ideas of our multimodal anomaly detection inscwarn are using lstm to learn the temporal dependency in each time series and adopting multimodal fusion to encode the inter correlations among multi source data.
figure displays the detailed network structure of multimodal lstm.
specifically given a window of multi source data xt w t consisting of business kpis xb t w t machine kpis xm t w t and logs xl t w t .
we first apply lstm to each time series separately to learn the unimodal representation is network weight and fis activation function .
then the multimodal joint information is captured by the shared representation layer which is constructed by merging units with connections coming into this layer from multiple modality specific paths.
finally the model output is the predicted value pt 1at time t consisting of pb t pm t 1andpl t .
the anomaly score of data xt can be calculated as the absolute value of the difference between real value and predicted value i.e.
xt pt .
the loss function of multimodal lstm can be calculated by the sum of mean square error mse on each modality which can be formulated as l nn i pb i xb i pm i xm i pl i xl i where nis the number of time steps in the testing set.
the detailed hyperparameters of the network are presented .
.
.
the effectiveness of multimodal lstm algorithm will be illustrated in .
.
.
alerting with analysis report .
.
threshold selection and alerting strategy.
after obtaining anomaly scores a threshold should be selected to decide whether the current time step is abnormal or not.
motivated by during offline training on the monitoring data before the software change we can compute an anomaly score for each time step in training data and obtain a univariate time series astrain s1 s2 sntr ain .
intuitively the values of astrain are not very large and we can determine the threshold based on the distribution of astrain .
specifically we utilize k principle to get the threshold.
if the current anomaly score is larger than k we could declare the current time step is abnormal where and are the mean and standard deviation of astrain respectively.
in general in the field of anomaly detection if the anomaly score exceeds a pre defined threshold an alert will be triggered to notify engineers.
in our scenario however the anomalies induced by software changes are non transient and last for a long time without human intervention.
transient anomalies e.g.
temporary network issues are more likely to be noises.
if these noises are reported it may increase engineers burdens of manual investigation and cause innocent software changes to be stopped.
therefore we adjust the alerting strategy i.e.
an alert is generated for aconsecutive anomalous points exceeding the threshold to remove noises and mitigate false alarms.
the effect of the two parameters kanda on the performance of scwarn will be discussed in .
in detail.
.
.
analysis report.
after generating alerts to notify responsible engineers to pay attention to suspicious software changes it would be better to provide an interpretable analysis report which could enable engineers to have a global view of the software change and inspect related monitoring data conveniently.
figure presents an example of the interpretable report provided by scwarn .
specifically scwarn can provide some information about the software change ticket and generate the overall health score of this change in real time.
the health score is derived from the anomaly score outputted from our multimodal anomaly detection model based on min max normalization which can be computed as follows xand astrain are anomaly scores of current data and training data health score max x min astrain max astrain min astrain furthermore we rank all involved data by their individual anomaly scores prediction error of each time series and display the top k abnormal data which may closely relate to the root cause.
for example since the top abnormal data is cpu utilization in figure we can infer that this alert may be incurred by cpu resource.
also the data comparison of each time series between before and after the change can be presented in a visual manner which is also convenient for engineers to investigate more details for troubleshooting.
.
action decision as introduced in .
.
some successful changes could also result in abnormal but expected behaviors.
based on the observations from the empirical study we could construct a knowledge base of expected change operations and corresponding data behaviors inscwarn e.g.
resource expansion would lead to the decrease of cpu usage and response time .
then we can design a policy based method to distinguish the abnormal behavior is unexpected or not.
if the change operation not in the knowledge base the decision process is conducted by engineers manually and new scenarios can be updated to the knowledge base.
actually despite the result recommended by the policy based method the final decision stillidentifying bad software changes via multimodal anomaly detection for online service systems esec fse august athens greece figure a demo of analysis report provided by scwarn needs to be confirmed by engineers and cannot be entirely replaced by automated algorithms since the cost of false positives innocent changes are stopped and diagnosis time is wasted and false negatives missing failures are huge.
if a software change is confirmed as unexpected engineers would take prompt protective action e.g.
rollback to stop the bad change so as to avoid further service unavailability and economic loss.
if the abnormal behavior is expected we could leverage the state ofthe art adaptation algorithm stepwise to enable real time monitoring system to adapt to new data pattern.
evaluation in this study we aim to address the following research questions rq4 what is the effectiveness of scwarn in identifying bad software changes?
rq5 what is the effectiveness of multimodal lstm?
rq6 what is the time efficiency of scwarn ?
rq7 what is the impact of parameters on the result of scwarn ?
.
experiment setup .
.
datasets and metrics.
to evaluate the performance of scwarn we conducted the study on two widely used benchmark systems.
benchmark systems.
the first system is train ticket an open source microservice system which has been widely applied in the literature .
train ticket serves as a system of selling train tickets containing more than microservices pay price order food etc.
.
the other one is e commerce an end toend application benchmark for e commerce search system .
it covers the major modules and critical paths of an industry scale e commerce provider.
we run the two systems on kubernetes which is an open source system for automating deployment scaling and management of containerized applications.
bad software changes injection.
as presented in .
.
we find that major root causes of bad software changes include code defect error configurations incompatible software versions and resource contention.
based on the key observation to evaluate the effectiveness of scwarn in identifying bad software changes we carefully designed and simulated ten types of bad software change operations in the study where all the above four root causes are involved.
table presents detailed descriptions of bad softwaretable types and descriptions of bad software changes we injected on the benchmark systems for evaluation failure type description code defectf1 create large java objects in program leading to frequent fullgc and outofmemory error f2 inject delay into program to simulate code performance issue f3 sql statement defect leading to slow query configuration errorf4 invalid paths which will be opened or executed f5 unsuitable size of jvm heap memory f6 database port error f7 limited number of database connections f8 non existent database table software version f9 incompatible software version resource contention f10 cpu contention change operations designed by us.
besides we also simulated some normal changes for evaluation including correct configuration modification and importing new code without bugs.
these software changes are injected into different components of each benchmark system ensuring the diversity of our datasets.
data collection.
for each benchmark system kpi data are collected by prometheus and stored in influxdb and logs are collected by logstash and stored in elasticsearch es .
we collected two datasets aandbbased on the two benchmark systems.
dataset acontains bad software changes and normal changes and dataset bcontains bad changes and normal changes.
there are cases for each failure type in each dataset.
for each software change case the involved multi source data including business kpis machine kpis and logs are used for experiments.
metrics.
intuitively identifying bad software changes is a binary classification task.
we run scwarn and compared approaches to see if bad software changes can be caught successfully.
we utilized popular binary classification metrics i.e.
precision recall andf1score as metrics.
besides we also considered the mean time minute to detect bad changes mttd after change deployment which indicates the ability of scwarn to identify bad changes in advance.
.
.
implementations and parameters.
we used two weeks of data before the software change as training data and the online data after the change as testing data.
specifically for multimodal lstm we used one lstm layer sequence length is hidden size is and one fully connected fc layer on each time series separately.
then one fc layer is used to learn the joint representation.
after that two fc layers are used to obtain the final results.
the hidden sizes of all fc layers are set to .
during training adam optimizer with a learning rate of .
is adopted the batch size is and the number of epochs is .
besides we found that the final results are insensitive to hyperparameters within a small range.
for the compared approaches we used the parameters provided by their papers and open source code .
besides we have publicly published our experimental code on github for better reproducibility.
all approaches are implemented by python with widely used libraries including numpy pandas scikitlearn pytorch etc.esec fse august athens greece n. zhao j. chen z. yu h. wang j. li b. qiu h. xu w. zhang k. sui d. pei table precision p recall r f1 score f1 and mttd minutes comparison between scwarn and baselines dataseta datasetb approaches p r f1 mttd p r f1 mttd scwarn .
.
.
.
.
.
.
.
gandalf ad .
.
.
.
.
.
.
.
funnel .
.
.
.
.
.
.
.
lumos .
.
.
.
.
.
.
.
mfunnel .
.
.
.
.
.
.
.
mlumos .
.
.
.
.
.
.
.
.
rq4 performance of identifying bad software changes we compared scwarn with the three following baseline approaches.
gandalf is proposed for end to end safe deployment and contains an anomaly detection ad component based on kpis and logs.
for logs it utilizes error messages clustering to extract error patterns e.g.
code and detects anomalies on each log error pattern via holt winters.
considering the details about kpi anomaly detection in gandalf are unclear we also utilized holt winters on kpis in the experiments.
funnel adopts improved singular spectrum transform isst to detect change point on business kpis after software changes.
lumos uses a b testing to compare the kpi pattern during a time window minutes in our experiments before and after the change by statistical hypothesis testing.
considering that 2 test in is suitable to categorical data we adopted t test in experiments since it is more appropriate and has been widely used to time series .
table displays the results between scwarn and compared approaches.
clearly scwarn performs the best taking all measurements into consideration achieving .
and .
f1 score on two datasets.
in comparison the average f1 score of gandalf funnel and lumos are only .
.
and .
respectively.
the average precision and recall of scwarn achieve .
and .
indicating that scwarn could detect bad changes accurately with few false negatives leading to missing failures and degrading service quality and false positives engineers efforts to diagnose are wasted and innocent changes are stopped .
besides the mttd of scwarn is .
minutes on average reducing by .
.
illustrating that scwarn could identify bad changes earlier.
furthermore considering both funnel and lumos only target at business kpis to compare fairly with our approach we also extended their anomaly detection algorithms to multi source data mfunnel and mlumos in table .
we observe that f1 score and mttd of mfunnel and mlumos are slightly improved compared with raw methods indicating that fusing multi source information is indeed helpful to some degree while they still perform worse than scwarn due to the drawbacks of their algorithms isst and t test as explained below.
we further analyzed the reasons why the compared approaches perform not well.
although gandalf utilizes both kpis and logs it detects anomalies separately on single data source and fails to identify the hidden correlations.
thus its overall performance is weaker than scwarn .
besides it adopts holt winters to detect anomalies on each log error pattern.
on the one hand holt winters is onlyapplicable to seasonal kpi.
on the other hand error message clustering is exclusively designed for logs recording error information which is not generic for all log data.
in comparison our log preprocessing technique has the ability to deal with various logs.
for funnel it uses isst to detect change point after the change while isst is not generic for various time series seasonal variational and stationary .
besides isst requires accumulating enough data leading to long mttd.
in terms of lumos it simply adopts statistical testing for anomaly detection which is only applicable to data with significant change.
besides t test relies on enough new data to compare with old data before the change to obtain accurate results and the window size is set to data points in our experiments.
it is difficult for t test to achieve short mttd and high accuracy at the same time.
therefore scwarn fusing multi source data based on the multimodal anomaly detection could deliver better results.
another interesting observation is scwarn performs differently in different types of bad changes especially in mttd.
specifically the mttd of some failure types f4 f6 f8andf9 is shorter than others.
this is because these bad software changes tend to behave abnormally immediately after the deployment e.g.
error database port would directly cause the service cannot access the database .
in comparison some other failures would perform a slow deterioration process e.g.
creating large java objects would incur jvm heap space to rise slowly until it overflows resulting in longer mttd.
overall taking both f1 score and mttd into consideration scwarn with good interpretability is indeed able to identify bad software changes accurately and timely.
.
rq5 effectiveness of multimodal lstm actually identifying bad changes is an anomaly detection task aiming to detect abnormal patterns after change deployment.
to demonstrate the effectiveness of our multimodal lstm algorithm we compared it with several state of the art anomaly detection methods including donut and lstm for business kpis blstm lstm ndt and omnianomaly for machine kpis and deeplog for logs.
besides we also implemented several methods integrating multi source data based on raw auto encoder m ae raw lstm m lstm and multimodal ae to further compare with our algorithm.
although these algorithms are not specially designed for identifying bad software changes they can be extended to solve the problem.
the performance comparison on two datasets is shown in table and we obtained two key observations.
first multimodal lstm is superior to those approaches only relying on single data source.
it is because incorporating multi source data could identify the failure signals hidden in various data sources in advance.
besides the results provided by multimodal lstm could also be more accurate when fusing comprehensive information.
notice that deeplog performs poorly since it endeavors to detect sequential anomalies of log templates which is unsuitable in our problem.
the second finding is multimodal lstm outperforms other methods m ae m lstm and multimodal ae that also incorporate multi source data.
specifically both directly applying lstm without multimodal fusion average f1 score of m lstm is .
and replacing lstm with ae to model on the single data source in multimodal lstm average f1 score of multimodal ae is .
perform worse thanidentifying bad software changes via multimodal anomaly detection for online service systems esec fse august athens greece table effectiveness of multimodal lstm algorithm compared with existing anomaly detection approaches dataseta datasetb data source approaches p r f1 mttd p r f1 mttd business kpisdonut .
.
.
.
.
.
.
.
b lstm .
.
.
.
.
.
.
.
machine kpislstm ndt .
.
.
.
.
.
.
.
omnianomaly .
.
.
.
.
.
.
.
logs deeplog .
.
.
.
.
.
.
.
multi source datam ae .
.
.
.
.
.
.
.
m lstm .
.
.
.
.
.
.
.
multimodal ae .
.
.
.
.
.
.
.
multimodal lstm .
.
.
.
.
.
.
.
our algorithm .
.
considering that the performance of multimodal lstm has a small difference with m lstm and multimoal ae on datasetb we splitbinto sub datasets based on failure types and calculated f1 score on each sub dataset.
following existing works we conducted a paired sample wilcoxon signed rank test and vargha delaney effect size measure to analyze the difference degree between multimodal lstm and m lstm multimodal ae.
the p values are .
.
and .
.
and the effect size are .
medium difference and .
large difference demonstrating the superiority of multimodal lstm.
.
rq6 time efficiency as an approach to identifying bad software changes time efficiency is a vital factor.
for incoming data if the detection result cannot be provided in time it could cause that engineers cannot identify bad changes and take protective actions immediately.
thus we investigated the efficiency of scwarn and compared approaches including training time and detection time funnel and lumos do not need to train models .
as displayed in table the training time of scwarn is about several minutes which is acceptable compared with omnianomaly and deeplog.
despite lstm model used in scwarn the training cost is relatively short because of the lightweight and effective network structure.
considering numerous software changes per day in large systems short training time could significantly reduce resource overhead.
besides since the training phase is offline training cost cannot lead to the delay in identifying bad software changes.
in terms of the detection time scwarn and most compared approaches could give a result nearly in real time less than second which is negligible.
overall scwarn has acceptable offline training time and negligible online detection time.
.
rq7 parameter sensitivity here we take dataset aas the subject and mainly discuss the effect of two parameters in threshold selection and alerting strategy onscwarn .
as stated in .
.
the value of kink strategy directly influences the threshold selection and final results.
figure a presents the effect of the value of kon the performance of identifying bad changes.
clearly with small k the threshold is relatively loose leading to high recall but low precision.
on the contrary the threshold is strict with large k leading to high precision but low recall.
another parameter we studied is the number of continuoustable training time minutes and detection time seconds comparison dataseta datasetb approaches training detection training detection scwarn .
.
.
.
gandalf ad .
.
.
.
funnel .
.
lumos .
.
donut .
.
.
.
b lstm .
.
.
.
lstm ndt .
.
.
.
omnianomaly .
.
.
.
deeplog .
.
.
.
m ae .
.
.
.
m lstm .
.
.
.
multimodal ae .
.
.
.
value of k0.
.
.
.0metricprecision recallf1 score a kin threshold selection value of a0.
.
.
.0f1 scoref1 score mttdmttd b the number of continuous points figure the effect of parameters anomalous points in alerting strategy.
figure b presents its effect on f1 score and mttd.
intuitively too small will generate some false alarms caused by noises e.g.
temporal network issue .
in comparison although large can achieve satisfactory precision too strict the alerting strategy could result in long mttd.
kand are set to and in our experiments respectively.
in practice parameter selection can be decided based on the validation set.
discussion .
success stories to illustrate the practical effectiveness of scwarn we conducted an informal user study with engineers from different serviceesec fse august athens greece n. zhao j. chen z. yu h. wang j. li b. qiu h. xu w. zhang k. sui d. pei systems in the bank.
these engineers provided several historical cases to us and we reported the results of scwarn to them.
in particular engineers appreciated scwarn from three perspectives.
scwarn could identify bad changes more accurately and earlier than their traditional tool based on .
.
so that protective actions e.g.
rollback can be taken immediately to prevent further economic loss.
engineers confirmed the analysis reports provided by scwarn could assist them in investigating these cases with good interpretability .
taking the case ii below as an example the report displays all abnormal signals of this change and the top abnormal data are heap space gc operation log template and memory usage.
engineers could easily infer that the incident is caused by frequent fullgc and localize the code defect in minutes incorporating java dump files.
besides displaying all related information of this change in a web page could save about minutes for engineers to log in different devices or platforms e.g.
grafana and kibana to inspect data manually.
incorporating multi source data could provide more comprehensive information.
specifically kpis could directly reflect the health status of the change entity and logs could provide clues for further diagnosis.
for example using gc logs could indicate the frequent fullgc problems in case ii using network device logs could help localize the ip address conflict problem incurred by error configuration the appearance of new log pattern address conflict detected .
they also gave us some suggestions to make our tool more userfriendly e.g.
displaying system topology in the user interface and incorporating monitoring data from correlated systems.
in the following we present four typical real cases and anonymize some confidential details.
case i a new database table imported by a software change was not indexed and then a full table scan was conducted.
this incident was noticed by the alert of high response time during busy hours.
with scwarn however the incident can be discovered minutes in advance by some related machine kpis including database metrics lock wait and cpu usage and middleware metrics jdbc connection pool .
in contrast the mttd of gandalf is minutes longer than that of scwarn .
case ii a piece of code imported by a software change was defective and created a large java object incurring frequent fullgc operations and full heap space.
engineers noticed this incident by the alert of high response time.
in comparison scwarn could identify the incident .
hours in advance from gc logs and related machine kpis including jvm metrics heap space and gc count and server metrics cpu usage and memory usage .
besides the details in gc logs could provide clues for further troubleshooting.
case iii after a software upgrade of service a it shared the same server with service b for data synchronization resulting in an efficiency bottleneck of a when b was busy accessing this server.
our approach could discover minutes earlier compared with traditional monitoring tool and locate the problem through related server performance kpis i o wait time is high of service a. case iv the slow sql statement in the new code led to the abnormal behavior of related database performance kpis the number of average active sessions is high .
our approach could capture it accurately and rapidly minutes in advance to avoid further negative impact on service availability.
.
lessons learned generality of our approach .
it is a common phenomenon that bad changes could incur service incidents both in our studied bank and other large companies like google and baidu .
.
.
besides scwarn based on multi source data anomaly detection is generic since most systems include kpis and logs which have been extensively studied in the literature .
thus the problem of identifying bad software changes and scwarn are not limited to the bank we studied.
besides scwarn could also be applied to incident diagnosis and incident prediction.
similar to identifying bad changes these two tasks also aim to detect abnormal signals from monitoring data and existing works mainly focus on single data source independently .
thus fusing kpis and logs may provide more comprehensive information to diagnosis or prediction which can be our future work.
fully automated software change without human involvement is difficult .
although our proposed scwarn could significantly reduce manual efforts on identifying bad changes it is unrealistic to conduct software changes entirely automatically without any human involvement.
it is because only the engineers in charge of the software change have requisite domain knowledge of the expected behavior and can make accurate decisions e.g.
whether the abnormal behavior is expected or not and how to take emergency actions to mitigate unexpected incidents .
actually such domain expertise cannot be replaced by a fully automated algorithm accurately.
multi source data fusion may be helpful to other software engineering tasks.
in our work integrating multi source data in an interpretable and visual manner could help engineers obtain a global view of the software change.
similarly multi source data fusion can also be applied to other tasks aiming to improve accuracy and extend features for example incident linking based on incident tickets and graph dependency alert prioritization using textual alerts and kpis trace anomaly detection combining texts and kpis flaw detection in software programs .
the experience obtained from our work could provide some insights for software testing .
software testing is vital to guarantee code quality and service reliability under development.
in practice engineers could investigate the root cause of the bad software change figure and analyze whether the incident could be avoided by more comprehensive testing so as to obtain some lessons for software testing.
in this way engineers can take effective actions to improve the testing to cover more aspects for example adding stress testing to simulate the impact of the unexpected burst of user requests on the service and testing the associated system to remove the incompatibility or resource contention problem.
.
threats to validity subject systems we conducted the experimental study on two popular benchmark systems and injected various bad software changes .
in fact these failure types we collected from industry may be limited and cannot cover all scenarios.
thus the failure cases in experiments as well as the scale of benchmark systems are threats in our study.
besides the injected changes used for evaluation are synthetic and relatively simple which could introduce bias.
in the future we will further evaluate our approach using more large scale real world software change cases with various failure types.identifying bad software changes via multimodal anomaly detection for online service systems esec fse august athens greece data quality although the two microservice systems used for evaluation are widely applied some hidden bugs existing in systems or other external factors e.g.
network flapping would threaten the quality of collected data.
thus the experimental data may contain some noises and the performance may be affected.
to reduce this threat we have deployed the systems with high availability and checked our data carefully.
evaluation metrics to demonstrate the effectiveness of scwarn we used precision recall f1 score and mttd as measurements which have been widely adopted in existing works.
in the future to reduce this threat we will further analyze the performance of scwarn on different types of bad changes in detail and consider more comprehensive metrics e.g.
fpr tpr to more sufficiently evaluate the effectiveness and efficiency.
.
limitations and future work we describe several limitations of our approach.
one limitation is some silent incidents caused by software changes are challenging to detect.
for example code defects hidden in seldom used functions might go undetected for a long time.
the reason is that scwarn is a data driven approach relying on abnormal patterns on monitoring data.
if some bad changes do not behave abnormally on monitoring data they cannot be caught by scwarn .
another limitation is real large scale deployment.
currently we just reviewed some historical incident cases induced by software changes to illustrate the practical effectiveness.
due to some limitations however we only deployed scwarn in a service system of the bank and large scale deployment in the real world can be our future work.
related work software change.
in the literature considerable efforts have been dedicated to software change in academic and industry which can be divided into three categories impact and risk analysis before change for example sonu et al.
proposed to prevent bugs and misconfigurations via correlated change analysis reliable launching strategy during deployment e.g.
dark launching identifying bad changes after deployment .
in this work we endeavor to tackle the third way because some bugs and errors could remain uncaught due to the discrepancies between testing and the production environment.
thus closely monitoring system behavior after deployment is a vital task.
the core idea of existing works about identifying bad changes is adopting an anomaly detection or change point detection algorithm to detect abnormal behaviors of business kpis after deployment for example isst adopted by funnel cusum used by mercury and a b testing in lumos .
however all of these approaches only utilize single source monitoring data.
thus they fail to discover the failure signals hidden in other data sources and cannot obtain comprehensive results resulting in unsatisfactory performance.
gandalf consumes various data kpis and logs to ensure safe deployment while it adopts a separate anomaly detection model for each single source data and cannot capture the inter correlations among multi source data and the anomaly detection algorithm in gandalf is not generic.
besides the goal of gandalf is to localize which change should be responsible for the abnormal behavior not for identifying bad changes.
in our approach we leverage multimodal data fusion for identifying bad software changes based on heterogeneous multi source data.
the superiority of scwarn compared with existing works has been illustrated in .
.
anomaly detection.
identifying bad changes is also an anomaly detection task.
over the years there has been a great deal of effort spent on anomaly detection of kpis and logs .
about kpi anomaly detection xu et al.
proposed donut to apply variational auto encoder vae to detect anomalies in seasonal kpis.
su et al.
proposed an approach to detect anomalies on multivariate kpis through stochastic rnn.
in terms of logs zhang et al.
proposed logrobust to extract semantic information and utilize an attention based bi lstm model to identify log anomalies.
in industry some products also have the ability to monitor service and detect anomalies from kpis using some statistical methods.
the technique of anomaly detection has also been widely applied in other software engineering fields such as misbehavior detection for autonomous driving systems detecting security issues identifying workflow errors and detecting issues from programming language compilers .
however existing anomaly detection algorithms or tools are not specially designed for identifying bad changes and ignore some characteristics of the problem e.g.
fusing multi source data abnormal but expected behaviors as stated in .
.
.
in our approach through integrating multi source data involved in software changes and adopting multimodal anomaly detection scwarn delivers better performance than existing works.
conclusion in online servicer systems software change is frequent but failureprone.
to better understand bad software changes we conduct the first empirical study based on large scale real world data from a large commercial bank.
our qualitative and quantitative analysis show about .
of incidents are caused by changes on average and the current practice of identifying bad changes is unsatisfactory due to ignoring multi source data involved in the change.
therefore it is necessary to identify bad changes to prevent service outages.
towards this direction we propose a novel approach named scwarn to identifying bad changes accurately and timely.
the core idea ofscwarn is drawing support from multimodal learning to detect anomalies from heterogeneous multi source data.
an extensive study including various bad software changes confirms the effectiveness of scwarn average f1 score is .
with a short mttd which significantly outperforms all the compared approaches.