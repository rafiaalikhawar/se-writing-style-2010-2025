robot robustness oriented testing for deep learning systems 1stjingyi wang zhejiang university wangjyee zju.edu.cn2ndjialuo chen zhejiang university chenjialuo zju.edu.cn3rdyoucheng sun queen s university belfast youcheng.sun qub.ac.uk4thxingjun ma deakin university daniel.ma deakin.edu.au 5thdongxia wang zhejiang university dxwang zju.edu.cn6thjun sun singapore management university junsun smu.edu.sg7thpeng cheng zhejiang university lunarheart zju.edu.cn abstract recently there has been a significant growth of interest in applying software engineering techniques for the quality assurance of deep learning dl systems.
one popular direction is deep learning testing where adversarial examples a.k.a.
bugs of dl systems are found either by fuzzing or guided search with the help of certain testing metrics.
however recent studies have revealed that the commonly used neuron coverage metrics by existing dl testing approaches are not correlated to model robustness.
it is also not an effective measurement on the confidence of the model robustness after testing.
in this work we address this gap by proposing a novel testing framework called robustness oriented testing robot .
a key part of robot is a quantitative measurement on the value of each test case in improving model robustness often via retraining and the convergence quality of the model robustness improvement.
robot utilizes the proposed metric to automatically generate test cases valuable for improving model robustness.
the proposed metric is also a strong indicator on how well robustness improvement has converged through testing.
experiments on multiple benchmark datasets confirm the effectiveness and efficiency of robot in improving dl model robustness with .
increase on the adversarial robustness that is .
higher than the state of the art work deepgini.
i. introduction deep learning dl has been the core driving force behind the unprecedented breakthroughs in solving many challenging real world problems such as object recognition and natural language processing .
despite the success deep learning systems are known to be vulnerable to adversarial examples or attacks which are slightly perturbed inputs that are imperceptibly different from normal inputs to human observers but can easily fool state of the art dl systems into making incorrect decisions .
this not only compromises the reliability and robustness of dl systems but also raises security concerns on their deployment in safetycritical applications such as face recognition malware dongxia wang is the corresponding author.detection medical diagnosis and autonomous driving .
noticeable efforts have been made in the software engineering community to mitigate the threats of adversarial examples and to improve the robustness of dl systems in the presence of adversarial examples .
among them formal verification aims to prove that no adversarial examples exist in the neighborhood of a given input.
substantial progress has been made using approaches like abstract interpretation and reachability analysis .
however formal verification techniques are in general expensive and only scale to limited model structures and properties e.g.
local robustness .
another popular line of work is deep learning testing which aims to generate test cases that can expose the vulnerabilities of dl models.
the test cases can then be used to improve the model robustness by retraining the model however this should not be taken as granted as recent studies have shown that test cases generated based on existing testing metrics have limited correlation to model robustness and robustness improvement after retraining .
in this work we highlight and tackle the problem of effectively generating test cases for improving the adversarial robustness of dl models.
there are two key elements when it comes to testing dl systems.
the first element is the testing metric used to evaluate the quality of a test case or a test suite.
multiple testing metrics including neuron coverage multi granularity neuron coverage and surprise adequacy have been proposed.
the common idea is to explore as much diversity as possible of a certain subspace defined based on different abstraction levels e.g.
neuron activation neuron activation pattern neuron activation conditions and neuron activation vector .
the second key element is the method adopted for test case generation which is often done by manipulating a given seed input with the guidance of the testing metric.
existing test case generation techniques such as deepxplore deepconcolic deephunter and adapt are mostly designed to improve the neuron ieee acm 43rd international conference on software engineering icse .
ieee fig.
overview of robot testing framework.
coverage metrics of the test cases.
while existing testing approaches are helpful in exposing vulnerabilities of dl systems to some extent recent studies have found that neuron coverage metrics are not useful for improving model robustness .
as a consequence unlike in the case of traditional program testing where the program is surely improved after fixing bugs revealed through testing one may not improve the robustness of the dl system after testing.
in this work we address the above mentioned limitations of existing dl testing approaches by proposing a novel dl testing framework called robot i.e.
robustness oriented testing which integrates the dl re training with the testing.
as illustrated in fig.
robot distinguishes itself from existing neuron coverage guided testing works in the following important aspects.
first robot is robustness oriented.
robot takes a userdefined requirement on the model robustness as input and integrates the retraining process into the testing pipeline.
robot iteratively improves the model robustness by generating test cases based on a testing metric and retraining the model.
second in robot we propose a novel set of lightweight metrics that are strongly correlated with model robustness.
the metrics can quantitatively measure the relevance of each test case for model retraining and are designed to favor test cases that can significantly improve model robustness which is in contrast to existing coverage metrics that have little correlation with model robustness.
furthermore the proposed metrics can in turn provide strong evidence on the model robustness after testing.
the output of robot is an enhanced model that satisfies the robustness requirement.
in a nutshell we make the following contributions.
we propose a robustness oriented testing robot framework for dl systems.
robot provides an endto end solution for improving the robustness of dl systems against adversarial examples.
we propose a new set of lightweight testing metrics that quantify the importance of each test case with respect to the model s robustness which are shown tobe stronger indicators of the model s robustness than existing metrics.
we implement in robot a set of fuzzing strategies guided by the proposed metrics to automatically generate high quality test cases for improving the model robustness.
robot is publicly available as an open source selfcontained toolkit .
experiments on four benchmark datasets confirm the effectiveness of robot in improving model robustness.
specifically robot achieves .
more robustness improvement on average compared to state of the art work deepgini .
ii.
background a. deep neural networks in this work we focus on deep learning models e.g.
deep neural networks dnns for classification.
we introduce a conceptual deep neural network dnn as an example in fig.
for simplicity and remark that our approach is applicable for state of the art dnns in our experiments like resnet vgg etc.
dnn a dnn classifier is a function f x y which maps an input x x often preprocessed into a vector into a label in y y. as shown in fig.
a dnn f often contains an input layer multiple hidden layers and an output layer.
we use to denote the parameters of fwhich assigns weights to each connected edge between neurons.
given an input x we can obtain the output of each neuron on x i.e.
f x ne by calculating the weighted sum of the outputs of all the neurons in its previous layer and then applying an activation function e.g.
sigmoid hyperbolic tangent tanh or rectified linear unit relu .
given a dataset d xi yi n i a dnn is often trained by solving the following optimization problem min 1 nn summationdisplay i 1j f xi yi wherejis a loss function which calculates a loss by comparing the model output f xi with the ground truth 301fig.
an example dnn to predict cat or dog.
labelyi.
the most commonly used loss function for multiclass classification tasks is the categorical cross entropy.
the dnn is then trained by computing the gradient w.r.t.
the loss for each sample in dand updating accordingly.
b. deep learning testing most existing deep learning testing works are based on neuron coverage or its variants .
simply speaking a neuronneis covered if there exists at least one test case xwheref x ne is larger than a threshold and thus been activated.
we omit the details of other variants and briefly introduce the following testing methods as representatives.
we also provide pointers for more details.
deepxplore is the first testing work for dnn.
deepxplore proposed the first testing metric i.e.
neuron coverage and a differential testing framework to generate test cases to improve the neuron coverage.
deephunter is a fuzzing framework which randomly selects seeds to fuzz guided by multi granularity neuron coverage metrics defined in .
adapt is another recent work which adopts multiple adaptive strategies to generate test cases which could improve the multi granularity neuron coverage metrics defined in .
adversarial attacks beside the above testing methods traditional adversarial attacks like fgsm jsma c w and pgd attacks are also used to generate test cases in multiple works.
c. problem definition unlike existing coverage guided testing works our goal is to design a robustness oriented testing framework to improve the dl model robustness by testing.
two key problems are to be answered how can we design testing metrics which are strongly correlated with model robustness?
how can we automatically generate test cases favoring the proposed testing metrics?
iii.
the robot framework in this section we present robot a novel robustnessoriented framework for testing and re training dl systems.
the overall framework of robot is shown in figure .
we assume that a requirement on the model robustness section iii a is provided in prior for quality assurancepurpose.
note that the requirement is likely applicationspecific i.e.
different applications may have different requirements on the level of robustness.
robot integrates the dl re training into the testing framework.
it starts from the initial training dataset d0 and trains an initial dnn f0in the standard way.
then it applies a fuzzing algorithm see section iii e which is guided by our proposed testing metrics see section iii c to generate a new set of test cases dt for retraining the model f0to improve its adversarial robustness.
the retraining step distinguishes robot from existing dl testing works and it places a specific requirement on how the test cases in dtare generated and selected i.e.
the test cases must be helpful in improving f0 s robustness after retraining.
we discuss how the test cases are generated in the rest of this section.
robot iteratively generates the test suite dtand retrains the model fnat each iteration.
afterwards it checks whether the robustness of the new model fnis satisfactory using an independent adversarial validation dataset dv subject to an acceptable degrade of the model s accuracy on normal non adversarial data.
if the answer is yes it terminates and outputs the final model fn otherwise robot continues until the model robustness is satisfactory or a predefined testing budget is reached.
in the following we illustrate each component of robot in detail.
a. dl robustness a formal definition although many dl testing works in the literature claim apotential improvement on the dl model robustness by retraining using the test suite generated such a conjecture is often not rigorously examined.
this is partially due to the ambiguous definition of robustness.
for instance the evaluations of are based on accuracy in particular empirical accuracy on the validation set rather than robustness.
in robot we focus on improving the model robustness without sacrificing accuracy significantly and we begin with defining robustness.
definition global robustness gr given an input regionr a dl model f r yis epsilon1 globally robust iff x1 x2 r x1 x2 p f x1 f x2 epsilon1.
global robustness is theoretically sound and yet extremely challenging for testing or verification .
to mitigate the complexity multiple attempts have been made to constrain the robustness into local input space such as local robustness clever and lipschitz constant .
these local versions of robustness are however not ideal either i.e.
they have been shown to have their own limitations .
for instance clever relies on the extreme value theory making it extremely costly to calculate.
in robot we adopt a practical empirical definition of robustness which has been commonly used for model robustness evaluation in the machine learning literature .
302testingpatch testingretraining originalenhancedtraditional softwaredl software fig.
comparison between traditional and deep learning system quality assurance by testing.
definition empirical robustness er given a dl model f x yand a validation dataset dv we define its empirical robustness f dv att as whereatt denotes a given type of adversarial attack and is the accuracy of fon the adversarial examples obtained by conducting att on angbracketleftdv f angbracketright.
intuitively def.
evaluates a model s robustness using its accuracy on the adversarial examples crafted from a validation set dv.
such an empirical view of dl robustness is testing friendly and it facilitates robot to efficiently compare the robustness of the models before and after testing and retraining.
definition is also practical as it connects the dl robustness with many existing adversarial attacks such as as a part of the definition.
in particular for the evaluation of robot in section iv we use two popular attacks i.e.
fgsm and pgd projected gradient descent as att .
b. robot dl testing a general view we first compare and highlight the difference between testing traditional software and deep learning systems in fig.
.
while many testing methods like random testing symbolic execution concolic testing and fuzzing can be applied to identify vulnerabilities or bugs for both the traditional software and the dl systems the workflow differs for the two after testing is done i.e.
the quality of traditional software is enhanced by patching the found bugs whereas deep learning systems are improved via retraining.
arguably the ultimate goal of testing is to improve the system s quality.
such improvement is guaranteed by patching bugs identified through testing in traditional software assuming regression bugs are not frequent i.e.
the usefulness of a bug revealing test for traditional software requires no justification.
it is not obvious for dl systems i.e.
the usefulness of a test case can only be judged by taking into account the retraining step.
nevertheless the retraining phase is largely overlooked so far in the deep learning testing literature.
based on the empirical robustness definition in def.
in alg.
we present the high level algorithmic design of robot for the workflow of dl testing in figure .
the initial trained model f0is given as an input in the algorithm and the testing and retraining iterations inalgorithm robot f0 d dv r t f f0 whileer f dv t rdo dt t f d d d dt updatefby retraining the model with d end while returnf robot are conducted within the main loop lines .
the loop continues until the user provided empirical robustness requirement is satisfied line .
robot aims to bridge the gap between the dl testing and retraining.
let t line denote a fuzzing algorithm to generate test cases guided by certain metrics .
the objective of robustness oriented testing is to improve the model robustness by testing.
formally given a deep learning modelf the goal of robot at each iteration is to improve the following er arg min 1 nn summationdisplay i 1j xi yi d t f d xi yi .
intuitively the testing metric should be designed in such a way that after retraining with the generated test cases the model robustness is improved.
this objective directly links the testing metric to the model robustness.
in the remaining of this section we realize the method in line by answering the question how should we design test metrics that are strongly correlated with the model robustness and how can we generate test cases guided by the proposed metrics?
c. robustness oriented testing metrics our goal is to design testing metrics which are strongly correlated with model robustness.
we note that there have been some efforts in the machine learning community to modify the standard training procedure in order to obtain a more robust model.
for instance the most effective and successful approach so far is robust training which incorporates an adversary in the training process so that the trained model can be robust by minimizing the loss of adversarial examples in the first place min 1 nn summationdisplay i 1max x prime i xi p epsilon1j f x prime i yi .
at the heart of robust training is to identify a strong ideally worst case adversarial example x primearound1a normal example xand train the model so that the loss on the strong adversarial example can be minimized.
robust training has shown encouraging results in training more robust models .
this inspires us to consider deep learning testing analogously in terms of how we generate 1a epsilon1 ball defined according to a certain lpnorm.
303test cases around a normal example and retrain the model with the test cases to improve the model robustness.
the key implication is that when we design robustnessoriented testing metrics to guide testing we should evaluate the usefulness of a test case from a loss oriented perspective.
letx0be the seed for testing.
we assume that a test casextis generated in the neighborhood epsilon1 ball around x0 i.e.
x x x0 p epsilon1 using either a testing method or an adversarial attack.
the main intuition is that a test case which induces a higher loss is a stronger adversarial example which is consequently more helpful in training robust models .
based on this intuition we propose two levels of testing metrics on top of the loss as follows.
a zero order loss zol the first metric directly calculates the loss of a test case with respect to the dl model.
formally given a test case xt generated from seed x a dl model f the loss of xtonfis defined as zol xt f j f xt y whereyis the ground truth label of x. for test cases generated from the same seed we prefer test cases with higher loss which are more helpful in improving the model robustness via retraining.
b first order loss fol the loss of generated test cases can be quite different for different seeds.
in general it is easier to generate test cases with high loss around seeds which unfortunately do not generalize well.
thus zol is unable to measure the value of the test cases in a unified way.
to address this problem we propose a more fine grained metric which could help us measure to what degree we have achieved the highest loss in the seed s neighborhood.
the intuition is that given a seed input the loss around it often first increases and eventually converges if we follow the gradient direction to modify the seed .
thus a criteria which measures how well the loss converges can serve as the testing metric.
a test case with better convergence quality corresponds to a higher loss than its neighbors.
next we introduce first order stationary condition fosc to provide a measurement on the loss convergence quality of the generated test cases.
formally given a seed input x0 its neighborhood area x x x x0 p epsilon1 and a test case xt the fosc value ofxtis calculated as c xt maxx x angbracketleftx xt xf xt angbracketright.
in it is proved that the above problem has the following closed form solution if we take the norm for x. c xt epsilon1 xf xt angbracketleftxt x0 xf xt angbracketright.
however many existing dl testing works are generating test cases from the l2norm neighborhood which makes the above closed form solution for l infeasible.
we thusconsider solving the formulation in eq.
with l2norm and obtain the solution as follows c xt epsilon1 xf xt .
proof according to cauchy schwarz inequality angbracketleftx xt xf xt angbracketright angbracketleftx xt x xt angbracketright angbracketleft xf xt xf xt angbracketright epsilon12 xf xt since there must exist xtsuch thatx xtand xf xt are in the same direction we thus have max angbracketleftx xt xf xt angbracketright epsilon12 xf xt .
thus max angbracketleftx xt xf xt angbracketright epsilon1 xf xt .
note that fosc in both eq.
and eq.
is cheap to calculate whose main cost is a one time gradient computation easy to obtain by all the dl frameworks .
the fosc value represents the first order loss of a given test case.
the loss of a test case converges and achieves the highest value if its fosc value equals zero.
thus a smaller fosc value means a better convergence quality and a higher loss.
c comparison with neuron coverage metrics compared to neuron coverage metrics our proposed loss based metrics have the following main differences.
first both zol and fol are strongly correlated to the adversarial strength of the generated test cases and the model robustness.
thus our metrics can serve as strong indicators on the model s robustness after retraining.
meanwhile our metrics are also able to measure the value of each test case in retraining which helps us select valuable test cases from a large amount of test cases to reduce the retraining cost.
d. fol guided test case selection in the following we show the usefulness of the proposed metric through an important application i.e.
test case selection from a massive amount of test cases.
note that by default we use the fol metric hereafter due to the limitation of zol as described above.
test case selection is crucial for improving the model robustness with limited retraining budget.
the key of test case selection is to quantitatively measure the value of each test case.
so far this problem remains an open challenge.
prior work like deepgini has proposed to calculate a gini index of a test case from the model s output probability distribution .
deepgini s intuition is to favor those test cases with most uncertainty e.g.
a more flat distribution under the current model s prediction.
compared to deepgini fol contains fine grained information at the loss level and is strongly correlated with model robustness.
given a set of test cases dt we introduce two strategies based on fol to select a smaller set ds dtfor retraining the model as follows.
let dt be a ranked list in descending order by fol value i.e.
fol xi fol xi fori .
304algorithm km st dt k n ds letmax andminbe the maximum and minimum fol value respectively equally divide range intoksectionskr foreach fol range r do randomly select n ksamplesdrfromdtwhose fol values are in r ds ds dr end for returnds a k multisection strategy km st the idea of km st is to uniformly sample the fol space of dt.
algo.
shows the details.
assume we need to select ntest cases fromdt.
we equally divide the range of fol into k sections kr at line .
then for each range r kr we randomly select the same number of test cases at line .
b bi end strategy be st the idea of be st is to formdsby equally combining test cases with small and large fosc values.
this strategy mixes test cases of strong and weak adversarial strength which is inspired by a recent work on improving standard robust training .
given a ranked dt we can simply take an equal number of test cases from the two ends of the list to compose ds.
figure shows the loss map of the selected test cases according to different strategies.
we could observe that be st prefers test cases of higher loss km st uniformly samples the loss space while deepgini often prefers test cases with lower loss.
e. fol guided fuzzing next we introduce a simple yet efficient fuzzing strategy to generate test cases based on fol.
note that since we have no prior knowledge of the fol distribution we are not able to design fuzzing strategy for km st. instead we design a fuzzing algorithm for the be st strategy.
the idea is to greedily search for test cases in two directions i.e.
with both small or large fol values.
algo.
presents the details.
the inputs include the modelf the list of seeds to fuzz seedslist the fuzzing region epsilon1 the threshold on the small fol value the number of labels to optimize k a hyper parameter on how much we favor fol during fuzzing and lastly the maximum number of iterations to fuzz for a seed iters .
for each seed in the list we maintain a list of seeds slist at line .
after obtaining a seed xfromslist line we iteratively add perturbation on it from line to line in a way guided by fol.
we set the following objective for optimization line .
obj k summationdisplay i 2p ci p c1 fol x prime algorithm fol fuzz f seedslist epsilon1 k iters letfuzzresult forseed seedslistdo maintain a list slist whileslistis not empty do obtain a seed x slist.pop obtain the label of the seed c1 f x letx prime x foriter toiters do set optimization objective objusing eq.
obtaingrads obj x prime obtainperb processing grads letx prime x prime perb letc prime f x prime letdis dist x prime x iffol x prime folmanddis epsilon1then folm fol x prime slist.append x prime ifc prime!
c1then fuzzresult.append x prime end if end if iffol x prime anddis epsilon1then slist.append x prime ifc prime!
c1then fuzzresult.append x prime end if end if end for end while end for returnfuzzresult whereciis the label with the ithlargest softmax probability off c1with the maximum p c is the softmax output of labelcandkis a hyper parameter.
the idea is to guide perturbation towards changing the original label i.e.
generating an adversarial example whilst increasing the fol value.
we then obtain the gradient of the objective line and calculate the perturbation based on the gradient by multiplying a learning rate and a randomized coefficient .
to .
to avoid duplicate perturbation line .
we run two kinds of checks to achieve the be st strategy at line and line respectively.
if the fol value of the new sample after perturbation x is either increasing line or is smaller than a threshold line we add x primeto the seed list line and line .
furthermore we add x primeto the fuzzing result if it satisfies the check and has a different label with the original seed x line and line .
note that compared to neuron coverage guided fuzzing algorithms which need to profile and update neuron coverage information our fol guided fuzzing algorithm is much more lightweight i.e.
whose main cost is to calculate a gradient at each step.
305be st km st gini strategyminmaxloss mnist be st km st gini strategyminmaxloss fashion mnist be st km st gini strategyminmaxloss svhn be st km st gini strategyminmaxloss cifar10fig.
loss of selected test cases for different datasets using different strategies.
table i datasets and models.
dataset training testing model accuracy mnist lenet .
fashion mnist lenet .
svhn lenet .
cifar10 resnet .
iv.experimental evaluation we have implemented robot as a self contained tookit with about 4k lines of python code.
the source code and all the experiment details are available at .
in the following we evaluate robot through multiple experiments.
a. experiment settings a datasets and models we adopt four widely used image classification benchmark datasets for the evaluation.
we summarize the details of the datasets and models used in tab.
i. b test case generation we adopt two kinds of adversarial attacks and three kinds of coverage guided testing approaches to generate test cases for the evaluation in the following.
we summarize all the configurations of the test case generation algorithms in tab.
ii.
c test case selection baseline we adopt the most recent work deepgini as the baseline of the test case selection strategy.
deepgini calculates a gini index for each test case according to the output probability distribution of the model.
a test case with larger gini index is considered more valuable for improving model robustness.
d robustness evaluation we adopt def.
to empirically evaluate a model s robustness.
in practice we compose a validation set of adversarial examples dvfor each dataset by combining the adversarial examples generated using both fgsm and pgd each .
the attack parameters are the same with tab.
ii.
we then evaluate a model s robustness by calculating its accuracy on dv.
b. research questions rq1 what is the correlation between our fol metric and model robustness?
to answer this question we first select three models with different robustness levels for each dataset.
the first model model is thetable ii test case generation details.
testing method parameter mnist svhn fashion mnist cifar10 fgsm step size .
.
.
.
pgd steps step size .
.
.
.
deepxplore relu threshold .
.
.
.
dlfuzz adapt time per seed 10s s s s relu threshold .
.
.
.
original trained model.
the second model model is a robustness enhanced model which is retrained2by augmenting of the generated test cases and is more robust than model .
the third model model is a robustnessenhanced model which is retrained by augmenting of the generated test cases and is most robust.
then for each model we conduct adversarial attacks to obtain a same number for fgsm and for pgd of adversarial examples.
we show the fol distribution of the adversarial examples for different models in fig.
.
we observe that there is a strong correlation between the fol distribution of adversarial examples and the model robustness.
specifically the adversarial examples of a more robust model have smaller fol values.
this is clearly evidenced by fig.
i.e.
for every dataset the probability density is intensively distributed around zero for model the most robust model while is steadily expanding to larger fol values for model and model with model larger than model .
the underlying reason is that a more robust model in general has a more flatloss distribution and thus a smaller fol value since it is based on the loss gradient .
in addition we also observe that adversarial examples crafted by stronger attacks have smaller fol values.
fig.
shows the fol distribution of adversarial examples from attacking the cifar10 model with fgsm and pgd respectively.
we could observe that adversarial examples from pgd have significantly smaller fol values than fgsm.
the reason is that stronger attacks like pgd are generating adversarial examples that have better loss convergence quality and induce higher loss.
we thus have the following answer to rq1 2retaining in this work takes for cifar10 additional epochs based on the original model.
fol0246810121416kernel density mnist model model model fol02468kernel density fashion minist model model model .
.
.
.
.
.
fol01020304050kernel density svhn model model model fol02468101214kernel density cifar10 model model model 3fig.
fol distribution of adversarial examples for models with different robustness.
fol0510152025kernel density cifar10 fgsm pgd fig.
fol distribution of adversarial examples from fgsm and pgd for cifar10 model.
answer to rq1 fol is strongly correlated with model robustness.
a more robust model have smaller fol values for adversarial examples.
rq2 how effective is our fol metric for test case selection?
to answer the question we first generate a large set of test cases using different methods and then adopt different test case selection strategies i.e.
be st km st and deepgini to select a subset of test cases with the same size to retrain the model.
a selection strategy is considered more effective if the retrained model with the selected test cases is more robust.
we distinguish two different kinds of test case generation algorithms which are both used in the literature i.e.
adversarial attacks and neuron coverage guided algorithms for more fine grained analysis.
for adversarial attacks we adopt fgsm weak and pgd strong attacks to generate a combined set of test cases.
for deepxplore dlfuzz and adapt we generate a set of test cases for each of them.
the parameters used are consistent with tab.
ii.
for each set of test cases we use be st km st and deepgini strategy respectively to select x ranging from to percent of them to obtain a retrained model and evaluate its model robustness.
fig.
shows the results.
we observe that for all the strategies the retrained model obtained improved re silience to adversarial examples to some extent.
besides the model s robustness steadily improves as we augment more test cases from to for retraining.
however we could also observe that in almost all cases except case our fol guided strategies both be st and kmst have significantly better performance than deepgini i.e.
achieving .
.
.
and .
more robustness improvement on average for the four different sets of test cases.
the reason is that fol is able to select test cases which have higher and more diverse loss than deepgini as shown in fig.
previously which are better correlated with model robustness.
meanwhile we observe that the retrained models maintain high accuracy on the test set as well as summarized in tab.
iii .
besides we observe that different test case generation algorithms obtain different robustness improvements.
among deepxplore dlfuzz and adapt adapt and dlfuzz have the highest .
on average and lowest .
on average robustness improvement respectively while deepxplore is in the middle .
on average .
adversarial attacks often achieve higher robustness improvement than all three neuron coverage guided fuzzing algorithms for simpler datasets such as mnist fashionmnist and svhn.
this casts shadow on the usefulness of the test cases generated by neuron coverage guided fuzzing algorithms in improving model robustness and is consistent with .
we further conduct experiments to evaluate and compare how robust the retrained models are when using adversarial examples generated in different ways one from the attacks and the other from different testing algorithms.
we summarize the result in tab.
iv.
we observe that the robustness drops noticeably which is especially the case for cifar10 i.e.
.
.
and .
for deepxplore dlfuzz and adapt each on average compared to the results in fig.
.
nevertheless our test case selection strategies still outperform deepgini in all cases.
this shows that adversarial examples from adversarial attacks alone are insufficient.
it is necessary to improve the diversity of test cases for retraining from a perspective that is well correlated with model robustness.
we thus have the following answer to rq2 .
.
percentage of test cases0.
.
.
.
.
.9robustness mnist attack be st km st deepgini .
.
percentage of test cases0.
.
.
.
.
.
.
.9robustness fashion attack be st km st deepgini .
.
percentage of test cases0.
.
.
.
.
.7robustness svhn attack be st km st deepgini percentage of test cases0.
.
.
.
.
.
.
.55robustness cifar10 attack be st km st deepgini test cases per seed0.
.
.
.
.
.
.
.75robustness mnist deepxplore be st km st deepgini test cases per seed0.
.
.
.
.30robustness fashion deepxplore be st km st deepgini test cases per seed0.
.
.
.
.
.50robustness svhn deepxplore be st km st deepgini test cases per seed0.
.
.
.
.7robustness cifar10 deepxplore be st km st deepgini test cases per seed0.
.
.
.
.
.
.
.28robustness mnist dlfuzz be st km st deepgini test cases per seed0.
.
.
.
.25robustness fashion dlfuzz be st km st deepgini test cases per seed0.
.
.
.
.
.
.50robustness svhn dlfuzz be st km st deepgini test cases per seed0.
.
.
.
.7robustness cifar10 dlfuzz be st km st deepgini test cases per seed0.
.
.
.
.
.
.
.
.85robustness mnist adapt be st km st deepgini test cases per seed0.
.
.
.
.
.6robustness fashion adapt be st km st deepgini test cases per seed0.
.
.
.
.
.
.50robustness svhn adapt be st km st deepgini test cases per seed0.
.
.
.
.
.7robustness cifar10 adapt be st km st deepginifig.
test case selection and robustness improvement with different strategies.
table iii test accuracy of model before and after retraining with percent of generated test cases using adversarial attacks.
dataset original retrained mnist .
.
fashion mnist .
.
svhn .
.
cifar10 .
.
answer to rq2 fol guided test case selection is able to select more valuable test cases to improve the model robustness by retraining.rq3 how effective and efficient is our fol guided fuzzing algorithm?
to answer the question we compare our fol guided fuzzing algorithm fol fuzz with state of the art neuron coverage guided fuzzing algorithm adapt as follows.
we run fol fuzz and adapt for a same period of time i.e.
minutes minutes and minutes to generate test cases.
then we retrain the model with the test cases to compare their robustness improvement.
the hyper parameters for fol fuzz are set as follows k iters learning rate .
.
the parameters for adapt are consistent with tab.
ii.
tab.
v shows the results.
we could observe that within 308table iv robustness performance of models retrained using adversarial examples from attack algorithms against test cases generated by dl testing tools.
deepxplore dlfuzz adapt dataset be st km st deepgini average be st km st deepgini average be st km st deepgini average mnist .
.
.
.
.
.
.
.
.
.
.
.
fashion mnist .
.
.
.
.
.
.
.
.
.
.
.
svhn .
.
.
.
.
.
.
.
.
.
.
.
cifar10 .
.
.
.
.
.
.
.
.
.
.
.
average .
.
.
.
.
.
.
.
.
table v comparison of fol fuzz and adapt.
a b a is the result of fol fuzz and b is the result of adapt.
min min min dataset test case robustness test case robustness test case robustness mnist .
.
.
.
.
.
fashion mnist .
.
.
.
.
.
svhn .
.
.
.
.
.
cifar10 .
.
.
.
.
.
average .
.
.
.
.
.
the same time limit adapt generates slightly more adversarial examples i.e.
compared to of folfuzz.
a closer look reveals that adapt tends to generate a lot of test cases around a seed towards improving the neuron coverage metrics.
however not all these tests are meaningful to improve model robustness.
on the other hand fol fuzz is able to discover more valuable test cases.
we could observe that using fol fuzzed test cases although less than adapt to retrain the model significantly improves the model s robustness than adapt i.e.
.
compared to .
of adapt on average.
we thus have the following answer to rq3 answer to rq3 fol fuzz is able to efficiently generate more valuable test cases to improve the model robustness.
c. threats to validity first our experiments are based on a limited set of test subjects in terms of datasets types of adversarial attacks and neuron coverage guided test case generation algorithms.
although we included strong adversarial attack like pgd and state of the art coverage guided generation algorithm adapt it might be interesting to investigate other attacks like c w and jsma and fuzzing algorithms like deephunter .
second we adopt an empirical approach to evaluate the model robustness which might be different with different kinds of attacks used.
so far it is still an open problem as for how to efficiently measure the robustness of dl models.
we do not use more rigorous robustness metric like clever because it is input specific and has high cost to calculate e.g.
hours for one input .
third our testing framework requires a robustness requirement as input which could be applicationspecific and is relevant to the model as well.
in practice users could adjust the requirement dynamically.v.
related works this work is mainly related to the following lines of works on building more robust deep learning systems.
a deep learning testing extensive dl testing works are focused on designing testing metrics to expose the vulnerabilities of dl systems including neuron coverage multi granularity neuron coverage neuron activation conditions and surprise adequacy .
along with the testing metrics many test case generation algorithms are also proposed including gradient guided perturbation black box and metric guided fuzzing .
however these testing works lack rigorous evaluation on their usefulness in improving the model robustness although most of them claim so and have been shown to be ineffective in multiple recent works .
multiple metrics have been proposed in the machine learning community to quantify the robustness of dl models as well .
however most of them are used to evaluate local robustness and hard to calculate.
thus these metrics are not suitable to test directly.
our work bridges the gap by proposing the fol metric which is strongly correlated with model robustness and integrate retraining into the testing pipeline for better quality assurance.
b adversarial training the key idea of adversarial training is to improve the robustness of the dl models by considering adversarial examples in the training phase.
there are plenty of works on conducting adversarial attacks on dl models of which we are not able to cover all to generate adversarial examples such as fgsm pgd and c w .
adversarial training in general may overfit to the specific kinds of attacks which generate the adversarial examples for training and thus can not guarantee robustness on new kinds of attacks.
later robust training is proposed to train robust models by solving a saddle point problem described in sec.
iii.
dl testing complements these works by generating more diverse adversarial examples.
309vi.
conclusion in this work we propose a novel robustness oriented testing framework robot for deep learning systems towards improving model robustness against adversarial examples.
the core of robot is a metric called fol to quantify both the value of each test case in improving model robustness often via retraining and the convergence quality of the model robustness improvement.
we also propose to utilize the proposed metric to automatically fuzz for more valuable test cases to improve model robustness.
we implemented robot as a self contained open source toolkit.
our experiments on multiple benchmark datasets verify the effectiveness and efficiency of robot in improving dl model robustness i.e.
with .
increasement on the adversarial robustness that is .
higher than the state of the art work deepgini.