machine translation testing via pathological invariance shashij gupta department of computer science and engineering iit bombay india shashijgupta cse.iitb.ac.inpinjia he department of computer science eth zurich switzerland pinjia.he inf.ethz.ch clara meister department of computer science eth zurich switzerland clara.meister inf.ethz.chzhendong su department of computer science eth zurich switzerland zhendong.su inf.ethz.ch abstract machine translation software has become heavily integrated into our daily lives due to the recent improvement in the performance of deep neural networks.
however machine translation software has been shown to regularly return erroneous translations which canleadtoharmfulconsequencessuchaseconomiclossandpolitical conflicts.
additionally due to the complexity of the underlying neural models testing machine translation systems presents new challenges.
to address this problem we introduce a novel methodology called patinv.
the main intuition behind patinvis that sentences with different meanings should not have the same translation.underthisgeneralidea weprovidetworealizationsofpatinv thatgivenanarbitrarysentence generatesyntacticallysimilarbut semantically different sentences by replacing one word in the sentenceusingamaskedlanguagemodelor removingoneword or phrase from the sentence based on its constituency structure.
we then test whether the returned translations are the same for theoriginalandmodifiedsentences.wehaveappliedpatinvtotest google translate and bing microsoft translator using english sentences.
two language settings are considered english !hindi en hi and english !chinese en zh .
the results show that patinvcanaccuratelyfind308erroneoustranslationsingoogletranslate and erroneous translations in bing microsoft translator most of which cannot be found by the state of the art approaches.
ccs concepts software and its engineering software verification and validation computing methodologies machine translation.
keywords testing machine translation pathological invariance permission to make digital or hard copies of all or part of this work for personal or classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn ... .
reference format shashij gupta pinjia he clara meister and zhendong su.
.
machine translation testing via pathological invariance.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa 13pages.
.
introduction duetorecentimprovementsinthequalityoftranslationsfrommachinetranslationsoftware manypeoplehavestartedtorelyonthe technologyintheirdailylives.forexample peopleoftenreadpolitical news or articles from other countries and visit websites with content in various languages.
according to in google translate had million users and translated more than billion words per day.
the improvements that have lead to such widespread usage of machine translation systems are largely due to the advent of deep neural network which are now frequently thecorecomponentofmodernmachinetranslationsoftware.neural machine translation nmt models are becoming as good as a human translator.
many recent nmt systems are approaching human level performance in terms of quality score defined by google and human parity defined by microsoft .
despite these recent improvements nmt systems are not as reliable as one might expect.
deep neural networks are brittle often when a neural network is evaluated on examples that differ intrinsically from the examples it was trained to model it does not perform well .
nmt models are no exceptions they can produce erroneous outputs when inputs are adversarially manipulated such as upper casing some of the letters of the sentence or injecting grammatical errors e.g.
i are studying .
however it is not vital for the sentence to be syntactically wrong to fool annmtmodel.therearenumerouscasesfoundwherenmtmodelsreturnerroneoustranslationsforsyntacticallyandsemantically correct inputs e.g.
in wechat a messenger app with over one billion monthly active users .
when encountered by users incorrect translations can have severe and harmful consequences such asfinancialloss politicalconflicts medicalmisdiagnoses socialissues or personal safety threats .
these side effects motivatetheneedtocreatesystemsforensuringtherobustnessof machine translation software.
863esec fse november virtual event usa shashij gupta pinjia he clara meister and zhendong su table examples of the errors detected by patinv.
the first two are from en hi and the last two are from en zh.
the translation is erroneous for both sentences in the first and the third examples the translation is erroneous for the source sentence in the second example the translation is erroneous for the modified sentence in the fourth example.
source sentence modified sentence translation for both translation meaning the situation at the southern border the situation at the southern border w the situation at the southern border which started as a crisis is now a which started as a crisis is now a which started as a crisis is now a near system wide meltdown .
near system wide development .
nearby system.
i had a story to tell and i wanted i had a story to tell and i wanted i had a story to tell and i wanted to finish it drapersays.
to finish it says.
to finish it.
the south has emerged as a hub of the south has diedas a hub of the south has became a hub of new auto manufacturing by foreign new auto manufacturing by foreign new auto manufacturing by foreign makers thanks to lower manufact makers thanks to lower manufact makers thanks to lower manufacturing costs and less powerful unions.
uring costs and less powerful unions.
uring costs and less powerful unions.
the threatened tariffs led to the the threatened tariffs led to the the threatened tariffs led to the european union pledging counter union pledging countertariffs.
european union pledging countertariffs.
tariffs.
however it can be very difficult to test nmt models.
first testing deep learning models in general is quite different from testing traditionalsoftware inwhichsystems coreconceptsoralgorithms manifest in source code.
rather the output of neural networks depends largely on the millions of parameters it has optimized over during training making these models essentially black boxes.
second recent testing approaches for artificial intelligence ai software of which machine translation software is a subset primarily target models with a small number of potential outputs such as classifiers.
in contrast simply enumerating the possible outputs of mostnmtmodelsisanintractableproblem makingmachine translation systems incredibly difficult to test.
the current standard for automatic evaluation is bleu which is calculated by comparing the word sequences1in the system s output with a set of reference quality translations.
one of bleu s major weaknesses is that it does not truly understand the sentence meanings.
furthermore it is necessary to provide accurate reference translations to determine such a metric for machine translationsystems whichisprohibitiveinmanysituationswhere such resources are limited.
clearly there is need for effective automated systems for testingmachinetranslationsoftware.thispaperproposesanoveltestingmethodology namelypatinv whosemainintuitionis thatsentences of different meanings should not have the same translations.
we use this intuition to formulate approaches which automatically produce syntactically similar sentences with different meanings.
in particular patinv generates sentences of different meaning through two approaches replacing one word in a sentencewithanon synonymouswordand2 removingameaningful word or phrase from the sentence.
the original and the newly generatedsentencesarefedtothetranslationsystemundertest ifthe translations are exactly the same we report them as a suspicious issue.
our practical implementation of patinv uses a masked language model based on bert to perturb words in a sentence and a constituency parser to identify core words and phrases.
thesaurus wordsapi and the nltk library are then used to filter out synonyms and syntactically incorrect sentences.
1a maximum length of four words is common.to evaluate the effectiveness of patinv we use it to test google translate and bing microsoft translator on english sentences fromtwoarticlecategories politicsandbusiness asinputreleased by he et al.
.
without using the optional filtering step i.e.
filtering by sentence embeddings section .
.
patinv successfully reports pathological invariants with .
average precision.
whenthisoptionalfilteringmechanismisemployed patinvcanreport pathological invariants with accuracy.
table 1show some of the erroneous translations found by patinv.
we find that due to its conceptual novelty patinv detects a unique set of errors not found by other approaches.
all the reported pathological invariants and source code have been released .
the main contributions of this paper are as follows we introduce a novel widely applicable black box methodology to validate machine translation software.
wedescribeapracticalimplementationtogeneratesyntacticallysimilarbutsemanticallydifferentsentencesusingbert using various filters to avoid generating invalid test cases.
we evaluate the model on sentences for google translate and bing microsoft translator with two translation settings.
we successfully find erroneous translations in google translate and in bing microsoft translator with high accuracy most of which cannot be found by the state of theart techniques.
a motivating example during the winter olympic games the norwegian team s cookingfacilitiesintendedtoorder1500eggs.thegameswereheld in south korea and thus they need to place an order in korean in a local grocery.
the chefs turned to google translate for help translating their order.
to their surprise a truck load of eggs fell upon their kitchen google translate mistakenly translated eggs into eggs in korean .
2in theory this erroneous translation can be detected if patinv replaces with .
atthemoment thiserrorhasalreadybeenfixed.inpractice itispossiblethat bertdoesnotrecommend asafillwordandthusmaynotfindthetranslation error.
864machine translation testing via pathological invariance esec fse november virtual event usa this is real life translation error which caused inconvenience and could have lead to a huge financial loss.
still translation errors can have much more serious and harmful consequence .
for example in due to a machine translation error israel s prime minister s compliment to israel eurovision winner netta went from you are a real darling to you are a realcow leadingtoembarrassmentandmisunderstanding.in apalestinianmanwasarrestedbypoliceafterposting good morning on facebook in arabic the post was wrongly translated to attack them in hebrew and hurt them in english .
as more and more people have started to rely on machine translation building robust machine translation software is of significant importance.toenhancetherobustnessofmachinetranslationsoftware this paper introduces patinv a novel and widely applicable methodology for testing machine translation.
approach and implementation this section discusses the high level idea of patinv and provides two implementations patinv replace and patinv remove .
recall that the main intuition behind patinv is that sentences with different meanings should not have the same translations.
hence we find issues such that both sentences have different meanings but result in the same translation by the model under test.
the input for patinv is a list of unlabeled monolingual sentences while its output is a list of suspicious issues.
for each input sentence either no issue is detected or a list of suspicious issues is returned.
a suspicious issue consists of the original sentence a generated semantically different sentence and their shared translation.
three types of translation errors can cause an issue the original sentence has an erroneous translation the generated sentence has an erroneous translation both the sentences have erroneous translations.
figure1shows the overview of patinv for both implementations we use a source sentence from our dataset as the example input.
in summary patinv carries out the following four steps generating syntactically similar sentences.
foreachunlabelled sentence wegeneratealistofsyntacticallysimilarsentences by modifying a word or a phrase in the sentence.
filtering via syntactic and semantic information.
wefilterout those test cases in which the newly generated sentence has the same meaning as the original sentence.
collecting target sentences.
wefeedtheoriginalandthenewly generated sentences to the machine translation system under test and collect their target sentences i.e.
translations .
detecting translation errors.
translations of the newly generated sentences are compared with the translation of the originalsentence.ifanytranslationmatchesthatoftheoriginal sentence the pair is reported as a potential issue which may contain an erroneous translation.
.
step generating syntactically similar sentences givenasentenceinasourcelanguage i.e.alanguagewetranslate from weneedtogeneratestructurallysimilarsentencesthathave a different meaning as input for patinv.
specifically we want to generate syntactically similarbut semantically differentsentences.several approaches can be taken to produce a list of variations fromasinglesentence.forexample wecanrandomlyinterchange words.
however sentences generated using this method are not necessarily syntactically similar to the original sentence.
we have found the following two approaches effective for the task replace a word in the original sentence with a word of the same part of speech e.g.
noun or adjective but different meanings patinv replace remove a word or phrase from the original sentence sentence.
patinv remove .
.
patinv replace.
in this approach we replace a single word in our original sentence with another word of the same part ofspeech to generate a new sentence with different meaning.
for example in fig.
1we replace the word completely with the word slightly which leads to a syntactically similar but semantically different sentence.
in practice we replace a given word with k new words to generate knew sentences.
if there are mwords in a sentence that can be replaced our method will produce a list of k mnew sentences.
we narrow the scope of words that can be replaced to avoid strange linguistic phenomenon which can lead to falsepositiveslaterinthetestingprocess.specifically onlynouns verbs adverbs adjectives and possessive pronouns are taken as candidates for replacement.
additionally we elect not to replace stopwords commonly used words e.g.
has were as we empiricallyfindthatreplacingthosewordsoftenleadstosyntacticallyor semantically incorrect sentences.
in our approach we use the set of stopwords provided by nltk .
now we discuss the problem of selecting candidates to replace a given word in the original sentence.
we note that it is important to find words which fit well in the context to ensure that our generated sentence makes sense.
this criterion eliminates the method of choosing words just by high word embedding vector similarity as standard word embeddings do not have any contextual information.
for example back and front have high word embeddingvectorsimilaritybutifwereplace back in looking back at the experience with front in looking front at the experience we end up with a sentence unlikely to occur in the english language.
amodelthatiswellsuitedforthistaskisthemaskedlanguage model mlm inspired by the cloze task .
specifically givenastringcontainingasingle masked wordasinput anmlm predicts what word belongs in the masked position.
the mlm returns a set of words as potential replacements.
the key benefit of using an mlm is that it takes into account the context of the sentencewhenpredictingthemaskedtoken.ifthemlmisgood then it is highly likely that the predicted words will lead to meaningful sentences.
note that we also check whether the words predicted by the mlm belong to nltk stopwords and likewise elect to not use these words as candidate replacements.
formlms thereareseveralout of boxoptionsavailable which are built on top of language representation models such as elmo gpt and bert .
while training one s own mlm is also possible it would require huge amounts of data and vast computationalresourcestoevenmatchthecaliberofasystembuilt 3there is no universally used list of stopwords available 865esec fse november virtual event usa shashij gupta pinjia he clara meister and zhendong su figure overview of patinv english !hindi in google translate on the aforementioned state of the art models with no guarantee ofproducingsomethingbetter.sinceagoodmlmiscriticalforour approach we elect to use the state of the art options.
specifically in our implementation we use bert a language representation modeldevelopedbygoogleai.interestingly themaskedlanguage task was one of the two main tasks used to train the base model givingusfurtherreasontobelievebertiswell suitedforthistask.
.
.
patinv remove.
in this approach we remove a word or a phrase in our original sentence to generate new sentences.
our maingoalistoremovesomethingmeaningfulfromasentence.for example fig.
1shows the complete removal of a word from the sentence.
however choosing words or phrases to remove is not a trivial problem.
a naive implementation of this approach may remove all pairs of consecutive words then triples of consecutive words etc.
while this implementation surely will not miss any error that can be detected with this approach it also leads to many false positives.
identifying meaningful phrases in a sentence can be aided by using its constituency structure i.e.
the syntactic structure of a sentence.
constituency structures show how a word or a group of words form different units in a sentence such as noun phrases or verb phrases.
this structure can be identified using a constituency parser which derives a representative parse tree by splitting a sentence according to a set of phrase structure rules defined by a context freegrammar.thereareseveralapproachesforconstituency parsing currently thestate of the artmodelusesshift reduceparsing .
this model parses grammar non terminals from left to right in a stack like manner to produce a complete set of relations.
an implementation of which we use is available through stanford s corenlp library .
once a sentence is parsed we identify all noun phrases np verb phrase vb prepositional phrase pp and adverb phrase advp as candidates for removal which can be done by recursively moving through the constituency parse tree.
we then form a set of new sentences by removing each word phrase in this set from the original sentence.
specifically each removal corresponds to a new sentence.
.
step filtering by syntactic and semantic information for our two sentence generation approaches we apply filters i.e.
remove some of the generated sentences to reduce potential false positives.
here we explain the two sets of filters.
.
.
patinv replace.
we find that some sentences generated using the masked language model mlm can be too semantically similar to the original sentence or are syntactically incorrect.
to address this issue we introduce three filtering mechanisms filtering out synonyms.
an mlm can predict words that are synonyms of the original word.
as we want to generate sentences with meanings different from that of the original sentence having a word replaced by its synonym e.g.
good talk and nice talk may lead to false positives.
therefore we filter out those caseswherethepredictedwordisasynonymoftheoriginalword.
thisisdoneusingthesaurus.com andwordsapi dictionary.
specifically we crawl thesaurus.com to generate a list of synonyms for each of the predicted words.
we likewise used the api provided by wordsapi to find synonyms.
we note that wordsapi alsoincludeshierarchicalinformation suchasknowingthatahatchback is a type of car a finger is a part of a hand etc.
in our implementation we check whether the predicted word is a synonym or type of the original word.
we filter out these cases.
interestingly there are a few cases where similar words cannot be detected by either of the services mentioned.
for example confirmed and affirmed can have the same meaning in certaincontextsbutneitherthesaurusnorwordsapitagthemassynonyms.
similarly say and said are conjugations of the same verb but are not identified as synonyms.
in general we want to eliminate cases where the base of the predicted word is a synonym of or equivalent to the base of the original word.
as a solutiontothisproblem weusenltk swordnetlemmatizertoget the base form of the verb.
wordnet lemmatizer lemmatizes using wordnet s built in morphy function.
it returns the input word unchanged if it cannot be found in wordnet.
a similar situation occurs for words with the same stem i.e.
select and selective when both are used as adjectives replacing one with the other in a sentence will generally not lead to a different semantic meaning.
866machine translation testing via pathological invariance esec fse november virtual event usa we used nltk s snowball stemmer to identify the word stem and elimante cases where the stem of the predicted and original words are the same.
filtering by constituency structure.
inpractice replacingaword in a sentence with a word of a different part of speech can lead to semantically or syntactically incorrect sentences.
for example in figure 1when completely is replaced by around we end up withagrammaticallyincorrectsentence.suchcasescanbeavoided by ensuring the original and generated sentence have the same constituency structure.
for this task we again use the implementation of available in nltk s corenlpparser to generate a constituency parse.
we then calculate the distance between the constituency parse trees of the original and generated sentence.
we use tree edit distance as our distance measurement as implementedinpython saptedlibrary.betweeneachoriginalandgenerated sentence we ensure tree edit distance is equal to i.e.
only one leaf node the replaced word has changed.
note that a distance of means the part of speech of the replaced word has not changed since part of speech tags correspond to nonterminals in the tree.
filtering by sentence embeddings.4asanadditionalprecaution to ensure generated sentences do not have the same meaning as the original sentence we ensure the distance between sentences is sufficiently large.
to mimic a linguistic notion of distance we take the vector distance between the embedding of the original and generated sentences.
there are several out of the box options available for generating sentence embeddings such as the universal sentence encoder and bert.
while training a sentence encoder would also be possible we note that these state of the art models are readily available and have proven to perform well on the task.
in patinv we used the universal sentence encoder a model for encoding sentences into vectors based on encoders parameterized by deep neural networks.
the universal sentence encoder is freely available on tensorflow hub.
the similarity between two sentences is then calculated as the cosine similarity of the two sentence embeddings.
we filter out the generated sentences with similarity to its original sentence greater than a given threshold which is a tunable parameter.
while there is some redundancy in the above filtering steps using all three ensures a minimal number of false positives which is critical for reducing manual effort.
.
.
patinv remove.
sentences generated using patinv remove may face similar problems i.e.
they may be nonsensical or retain their original meaning.
interestingly we find that removing small words in the original sentence often leads to the latter problem.
this is the case even when the word is not explicitly a stopword.
for example in our experiments we remove the word found from the sentence whether there is a political gain to be found in dishonoring a servant and the translation system returns the same results.
while it can be argued that there is an erroneous translation here the sentence after word removal does not have a clearly different meaning.
we empirically find that removing words of character length only greater than alleviates this issue.
4optional step to increase precision as discussed in section 4tofind a suitable lowerbound wetune this parameter on the politics dataset with en hi language setting from to with step size .
wasfoundtobethelowerboundthatledtodecentprecision and number of pathological invariants.
in our experiments we found that this lower bound also led to high quality results in other experimental settings.
.
step collecting target sentences once modified sentences have been generated we must then feed them to the translation system under test and retrieve their translations.
in the case of google translate and bing microsoft translator which we evaluate patinv on we use the google api and bingapi whichreturnthesameresultsastheirrespectivewebinterfaces.
specifically sentences in a source language are fed to the translation system and their translations in a chosen target language are returned.
we do this for each of our original sentences and all of the generated sentences.
.
step detecting translation errors finally once all translations are collected we must search for erroneous translations.
this component of our methodology is quite straightforward aswehavegeneratedsentencesthatdifferinmeaning from their respective original sentences if the translations of the original and generated sentences are the same we have good reasontosuspectanerroneoustranslation.thischeckcanbedone via a simple string comparison which checks for equality.
fig.
shows an example of such an erroneous translation pair.
evaluation in this section we evaluate patinv by applying both of its implementations patinv replace and patinv remove to google translate and bing microsoft translator with real world sentences as input.
our main research questions are rq1 how effective is patinv at finding erroneous translations?
rq2 where do the false positives and false negatives come from?
rq3 can patinv find erroneous translations that existing approaches cannot find?
rq4 how efficient is patinv?
rq5 how does one tune the parameters of patinv in practice?
inansweringthesequestions weshowthatbothimplementations areeffectiveinfindingerroneoustranslationsandmostoftheerroneoustranslationsfoundcannotbedetectedbyexistingapproaches.
additionally theapproachisefficientandiseasytouseinpractice.
.
experimental setup environment.
allexperimentsarerunondalcor2264l6grackmount server with 2x12 core intel xeon e5 v4 .2ghz processor 256gb ddr4 2400mhz ecc reg server memory and 8x nvidia rtx 2080ti 11gb gpu processor.
dataset.
to evaluate the performance of patinv and align with the sota we use the dataset released in our previous paper .
specifically this dataset contains english sentences crawled 867esec fse november virtual event usa shashij gupta pinjia he clara meister and zhendong su table statistics of input sentences for evaluation.
each corpus contains sentences.
corpus of words average of of words sentence words sentence total distinct politics .
business .
from cnn cable news network .5the dataset consists of articles fromtwocategories politicsandbusiness whereeachdatasetcontains sentences.
more statistics of the released dataset is illustrated in table .
labeling.
the output of our approach is a list of suspicious issues each of which consists of a pair of sentences where the first is the original sentence and the second is our generated one.
we manually label each issue reported by patinv.
two of the authors inspect all the results separately and discuss the labels for all the reportedissuesuntilconvergence.specifically forpatinv replace we first check whether the generated sentence in the issue contains any syntax or semantic errors.
if so the issue will be labeled as a false positive examples are in table .
otherwise we check whether the original sentence and generated sentence are semantically different.
if not the issue will be labeled as a false positive examples are in table .
otherwise we label the issue as a true positive.
then we decide which sentence s in this issue contains translationerror s .forpatinv remove thelabelingprocessissimilartopatinvexceptthatwewillnotlabelanissueasfalsepositive because of syntax semantic errors in the generated sentence.
comparison.
we compared patinv with two state of the art approaches sit and transrepair .
both approaches are based on the intuition that similar source sentences should have similar translations.
for sit we directly used the source code released by the authors.
for transrepair the authors did not release their source code due to industrial confidentiality.
thus we implement transrepair by carefully following the explanations in their paper and consulting the work s main author for key implemenation details.
we re tune the parameters for both sit and transrepairfollowingthesamestrategiesintroducedinthepapers.in .
is used as the minimum cosine similarity of word embeddings to generate word pairs.
in our experiment we lower the threshold to .
because otherwise we were unable to reproduce the quantity of pairs reported in the original paper i.e.
using .
as the threshold yielded two magnitudes fewer word pairs than reported by transrepair.
the implementation of patinv sit and transrepair are all released for reuse .
.
the effectiveness of patinv both of our approaches aim to automatically find erroneous translationsinmachinetranslationsoftwareusingunlabelledsentences.
thus the effectiveness of both of our approaches will be determined by how many erroneous translations an approach can find with what precision an approach can find translation errors.
in this section we show the effectiveness of our model by testing google translate and bing microsoft translator for two a politics dataset b business dataset figure precision v s recall trade off threshold curve for en hi in google translate translation settings english !hindi and english !chinese .
we calculatedourresultsseparatelyforeachdomainintheevaluation dataset.
.
.
effectiveness of patinv replace.
recallthatforpatinv replace weusethesimilaritybetweenthesentenceembeddingvectorsgiven by the universal sentence encoder to check whether the original sentence and newly generated have same meaning.
for different thresholds we filter out issues in which the pair of sentences have similarity less than the chosen threshold we calculate the following metrics for the remaining issues precision the number of erroneous issues divided by total number of reported issues.
recall the number of erroneous issues with similarity less than threshold divided by total number of erroneous issues when there is no threshold.
f1 score the harmonic mean of precision and recall.
868machine translation testing via pathological invariance esec fse november virtual event usa table precision recall and f1 score of patinv replace.
google translate setting dataset tp fp tn fn prec.
rec.
f1 en hi politics .
.
.
en hi business .
.
.
en zh politics .
.
.
en zh business .
.
.
bing microsoft translator setting dataset tp fp tn fn prec.
rec.
f1 en hi politics .
.
en hi business en zh politics .
.
en zh business .
.
table the number of remaining sentences after each step.
a patinv replace step no.
dataset no.
of sentences politics business politics business b patinv remove step no.
dataset no.
of sentences politics business politics business c state of the art approaches toolname dataset no.
of sentences sit politics sit business transrepair politics transrepair business results.
table4 a shows the number of total sentences original sentences newly generated sentences after step and step of patinv replace.
for each translation setting we count the total number of issues reported for each dataset.
we manually check andverifyeachissue reportinghowmanyoftheseissuesareerroneous.
ultimately we were able to achieve precision comparable with those of the state of the art methods.
we categorize erroneous issues by which of the two sentences original or generated contain a translation error.
we categorize non erroneous issues into two categories issues in which both sentences have the same meaning.
issues in which the second sentence is semantically or syntactically incorrect.
for each dataset business and politics we tune patinv s available parameter i.e.
the threshold similarity for sentence embeddings and report the precision recall and f1 score corresponding each threshold.
for the threshold with the best f1 score we report true positives tp false positives fp true negatives tn table result of patinv remove for en hi.
translator dataset tp fp precision google politics .
google business .
bing politics bing business .
false negatives fn .
table 3shows results for google translate and bing microsoft translator.
the results shows that we were able to achieve high recall with precision comparable to the stateof the art approaches as discussed in section .
.
we note that reducingthethresholddoesnotnecessarilyincreasetheprecisionas a lower threshold does not eliminate semantic or syntactic errors this was specifically apparent in the en zh results for the bingbusiness dataset as shown in table .
we additionally find that we can achieve precision given a low enough threshold.
we were able to report erroneous issues in total in this case.
the precision recall trade off curve is illustrated in fig.
.
.
.
effectiveness of patinv remove.
evaluation metric.
likewise the output of this approach consists of a list of issues where the first sentence is the original sentence and the second sentence is a generated one.
the generated sentence is obtained by removing something meaningful a word or a phrase from the sentence.
as in our first approach an issue is only reported when both the original and generated sentences have the same translations.
we notethattherearetwopotentialcasesforeachissue something meaningfulwasremovedfromtheoriginalsentence inwhichcase the issue is a true positive tp nothing important or meaningful was removed from the sentence in which case the issue is a false positive fp .
results.
table4 b shows the number of total sentences original sentences generated sentences after step and step of patinv remove.
for each translation pair we count the total number of issues reported for each dataset and manually verified how many of these issues were erroneous.
table 5shows results for googletranslateandbingmicrosofttranslatorforenglish !hindi.
the results for english !chinese are shown in table where they are compared with state of the art approaches in section .
.
the results shows that we were able to achieve precision comparable to state of the art approaches.
for each erroneous issue we labelwhichsentenceamongthepairhasanerroneoustranslation.
.
false positives and false negatives despite applying multiple filters there were still some false positives in both of our approaches.
specifically we encountered five sources of false positives.
the generated sentence is syntactically or semantically incorrect as shown in table .
note that our approach minimizes these false positives we have used a masked language model built on top of bert a high performing language representation model.
employing this strategy should lead to very few syntactically or semantically incorrect sentences.
the replaced word is a synonym of the original word.
despite combining multiple large online databases to filter out synonyms such cases still occurred.
the main source of such false positives was when 869esec fse november virtual event usa shashij gupta pinjia he clara meister and zhendong su table false positives for patinv replace due to same meaning.
sourceand this would takeus closer to our goal of a truly european banking sector.
newly generatedand this would allowus closer to our goal of a truly european banking sector.
translation en !hi w translation meaningand this would takeus closer to our goal of a truly european banking sector.
sourcenielsen said the most serious cyber threats are those aimedat the heart of democracy.
newly generatednielsen said the most serious cyber threats are those posedat the heart of democracy.
translation en !hi translation meaningnielsen said that the most serious cyber threats are at the heart of democracy.
table syntactically semantically wrong sentences generated by patinv replace.
sourcetheyleftout that the pilots were not trained to handle it.
newly generatedtheyleavesout that the pilots were not trained to handle it.
translation en !hi w translation meaningtheysaidthat the pilots were not trained to handle it.
sourcethe key to accepting praise at work is to show you received it and appreciate it.
newly generatedthe key to accepting praise at work is to take you received it and appreciate it.
translation en !hi translation meaningthe key to accepting praise at work is to receive it and appreciate it.
table false positives generated by patinv remove.
sourcefirst proper training for pilots who are flying new aircraft is crucially important .
newly generatedfirst proper training for pilots who are flying new aircraft is crucially .
translation en !hi w translation meaningfirst proper training for pilots who are flying new aircraft is important .
sourcethreemonths later holmes was indicted on federal wire fraud charges and stepped down from theranos.
newly generatedthree later holmes was indicted on federal wire fraud charges and stepped down from theranos.
translation en !zh theranos translation meaningthreemonths later holmes was indicted on federal wire fraud charges and stepped down from theranos.
our databases did not identify words as synonymous due to differencesintenseorpluralityfromtherelevantwordregisteredinthetable false negatives generated by patinv.
sourcethe key to accepting praise at work is to show you received it and appreciate it.
newly generatedthe key to accepting praise at work is to think you received it and appreciate it.
similarity .
sourceactress jennifer lawrence is also expected to star as holmes in a movie basedon bad blood.
newly generatedactress jennifer lawrence is also expected to star as holmes in a movie titledon bad blood.
similarity .
database.
while we identified word stems with available tools we note that this approach does not always work as expected.
for example nltk s snowball stemmer returns the stems univers for universities and colleg for college whichthencannotbeused with a synonym database as they are not standalone words.
the replaced word in a sentence does not carry much meaning.
we find thatreplacingsomewordsdoesnotchangethemeaningofthesentence because the replaced wordwas not vital for the meanning of thesentence.examplesofsuchcaseshasbeenprovidedintable .
to reduce such cases we remove generated sentences where sentence embedding similarity is high implying semantic similarity between the original and generated sentences.
something meaningful is not removed from the sentence.
we find that only removing words and phrases with length helps to minimize these cases.
the translation system successfully predicts the removed word phrase.
in particular it is possible that something meaningful is removed from the original sentence and a syntactically or semantically incorrect sentence is generated but the translation system still returns the removed word translation for the generated sentence.
examples are illustrated in table.
.
these false positives can be further reduced to make our approaches more efficient.
for example syntactically incorrect sentences can be eliminated by using a good grammar checker.
to eliminate cases of semantically incorrect sentences we can train a corresponding classifier.
we note that there were also some false negatives produced in filtering by sentence embeddings step of patinv replace which are shown in table .
.
errors reported by different approaches in this section we compare patinv with sit and transrepair in termsofprecisionandoverlapoferroneoustranslationsfound.the comparison is done with en zh language setting because sit only provides en zh implementation.
precision.
first we compare the precision of patinv with sit and transrepair because precision is used as a core evaluationmetricinbothpapers.theresultsarepresentedintable .
patinv remove achieves comparable precision compared with existing approaches.
when the threshold for filtering by sentence semantics is .
i.e.
no filtering in this step patinv replace obtains more erroneous issues with slightly lower precision.
when we decrease the threshold more generated sentences are filtered out theprecisionofpatinv replacewillincreaseaccordingly.for 870machine translation testing via pathological invariance esec fse november virtual event usa table precision and the number of erroneous issues using different threshold values en zh patinv replacepatinv remove sit transrepair1.
.
.
.
google politics .
.
.
.
.
.
.
google business .
.
.
.
.
.
.
bing politics .
.
.
.
.
.
.
bing business .
.
.
.
.
.
.
googletranslatepatinvbingmicrosofttranslator4transrepairsit62126280195 patinv6transrepairsit5394483200 figure erroneous translations reported by patinv sit and transrepair en zh example when the threshold is .
patinv replace finds more erroneous issues with comparable precision.
thus consider the performance of patinv replace and patinv remove the effectiveness of patinv on accurately finding erroneous translations is comparable to the state of the art approaches.
overlap of erroneous translations.
fig.3presents the overlapoftheerroneoustranslationsreportedbypatinv sitandtransrepair.thisvenndiagramshowsthatmostoftheerroneoustranslationsreportedbypatinvcannotbefoundbyexistingapproaches.
specifically only .
and .
erroneous translations reported by patinv can be found by either of the existing approaches.
thus we believe patinv complements with sit and transrepair.
.
running time of our approach in this section we present the running time of patinv.
for each experimental setting we run patinv times and report the average timing.
results are shown in table .
note that the running time of patinv replace s filtering by sentence embeddings step is not counted in the total time of patinv replace because it is an optional step.
compared with other steps this optional step is relatively time consuming.
for example our implementation takes around seconds for calculating the similarity of one sentence pair due to expensive api calls.
as we can observe from table most of the time is consumed in the filtering step and the translation step for patinv replace.the filtering step of patinv replace takes time because of the constituency parser used .
seconds per hundred sentences .
the translation step takes time because for each sentence in our experiments we invoke the translators api which include both the translation time and the network communication.
compared with existing approaches patinv replace takes more time because patinv replace generates much more sentences i.e.
for patinv replace for sit and for transrepair and thus patinv replace needs to run the constituency parser and the translator s api much more times leading to longer running time.
however we believe patinv is efficient for practical usage because it is designed to be an offline approach.
moreover with this longer running time patinv replace has reported more erroneous translations in table .
patinv remove and transrepair are much faster because they generate much less sentences .
.
fine tuning with errors reported by patinv in this section we explore whether the reported erroneous translations can be utilized as a fine tuning set for the nmt model to quickly fix translation errors.
fine tuning is a common practice in nmt .
it is often used when developers intend to adapt a trainednmtmodeltoanewdomainofdata e.g.
fromtexton politics to text on business .
to simulate this situation we train a transformernetworkwithglobalattention onthewmt 18zhen chinese to english corpus which contains 20m sentence pairs.
we use the fairseq framework to create the model.
we note that transformers are currently the state of the art modelsfornmt.wereversetheoriginaldirectionoftranslationsothat wecanproperlycomparewithourexperimentsonthegoogleand bing translation systems.
wetestthismodelwithpatinv replaceusingthepoliticsdataset in table .
patinv replace successfully finds erroneous translations.
we manually label them with correct translations and use them as the dataset to fine tune our nmt model.
specifically we train the model for another epochs with only the fixed translations.
as the dataset was small training took 10mins.
after this fine tuning all the erroneous translations are fixed.
meanwhile the bleu score on a held out test set remains the same.
thus theerroneoustranslationsreportedbypatinvcanbeusedto improvetherobustnessofmachinetranslationsoftware.notethat although error fixing for machine translation is an interesting and important topic it is not the focus of this paper.
thus we regard it as a promising future direction.
871esec fse november virtual event usa shashij gupta pinjia he clara meister and zhendong su table running time of patinv google politics hindigoogle business hindigoogle politics chinesegoogle business chinesebing politics hindibing business hindibing politics chinesebing business chinese initialization patinv replace29.
.
.
.
.
.
.
.
pair generation .
.
.
.
.
.
.
.
filtering .
.
.
.
.
.
.
.
translation .
.
.
.
.
.
.
.
detection .
.
.
.
.
.
.
.
total .
.
.
.
.
.
.
.
initialization patinv remove3.
.
.
.
.
.
.
.
pair generation .
.
.
.
.
.
.
.
filtering .
.
.
.
.
.
.
.
translation .
.
.
.
.
.
.
.
detection .
.
.
.
.
.
.
.
total .
.
.
.
.
.
.
.
sit na na .
.
na na .
.
transrepair na na .
.
na na .
.
related work .
robust ai software throughout recent years there has been much work and development in the area of artificial intelligence ai .
ai is used everywhere today such as in autonomous cars and face recognition.
however modern ai models are not as robust as we might hope.
in particular they can return incorrect results e.g.
wrong classification labels that lead to fatal accidents .
recent research has reported a variety of adversarial examples that can foolaisoftware suchasautonomouscars 3dobject classifiers andspeechrecognitionservices .topromote robustness lines of research have focused on testing debugging detecting adversarial examples or training networks in a robust way .
however none of these papers explore machine translation which is the main focus of our paper.
.
robust nlp systems the recent advances in the robustness of computer vision systems have encouraged researchers to investigate the robustness of nlp systemsperformingtaskssuchassentimentanalysis textual entailment and toxic content detection .
however all of these studies work on classification problems which have a much smaller output space than machine translation.
robustness of more complex nlp system has also been explored in a a few recent studies.
in particular jia and liang proposed an adversarial evaluation scheme for the stanford question answering dataset squad which has been widely used for the evaluation of reading comprehension systems.
they found that appending a specially designed sentence to the paragraph can mislead the welltrainedreadingcomprehensionsystems.additionally mudrakarta etal.
successfullyattackedquestionansweringmodels.inparticular they leverage the finding that deep networks often ignore important question terms and thus they perturb the questions accordingly.
compared with machine translation these systems areeasier to verify because the ground truth is unique.
for example the output of reading comprehension could be a specific person name.
for the task of translation there could be multiple correct translationsforagivensentence whichmakesdetectingincorrect outputs incredibly nuanced.
.
robust machine translation machine translation strives to translate text in a source language to text in a target language automatically.
there are primarily two linesofworkontherobustnessofmachinetranslationsoftware adversarial machine learning and machine translation testing.
adversarial machine learning aims at fooling machine translation models with adversarial examples.
most of the existing techniques generate such examples by white box methods in which complete knowledge of network structure and parametersofthemachinetranslationmodelisavailable.unlikethem patinv takes a black box approach.
existing black box techniques perturb or paraphrase sentences that can easily lead to invalid sentences e.g.
syntactic or semantic errors .
in comparison this paper aims at generating syntactically and semanticallycorrect test cases i.e.
source sentences .
machine translation testing aims at finding syntactically and semantically correct sentences that trigger translation errors.
proposedtwospecializedmethodstodetecttwokindsoftranslationerrorsrespectively under translationandover translation .
in contrast patinv is a general methodology that targets general translationerrors.heetal.
andsunetal.
developedmetamorphictestingtechniquesforgeneraltranslationerrorsbasedon the assumption that similar sentences should have similar translations evaluated by sentence structures or existing distance metrics .
sun et al.
also further proposed an automated repair approach.
patinvcomplements these approaches because it is based on a brand new concept named pathological invariance sentences of different meanings should not have the same translation.
our evaluation has also shown that patinv can report many 872machine translation testing via pathological invariance esec fse november virtual event usa erroneoustranslationsthatthestate of the artapproaches cannot report.
thus we believe patinv can complement these approaches.inaddition allexistingworkswereevaluatedbyonelanguage setting en zh while patinv is evaluated by two language settings en hi and en zh .
.
metamorphic testing metamorphic testing techniques find software bugs by detecting the violation of necessary functionalities i.e.
metamorphic relations of software under test .
as an effective approach for addressing the test oracle and test case generation problems metamorphic testing has been adopted in the testing phase of a variety of software systems such as compilers scientific libraries anddatabasesystems .inaddition metamorphic testinghasalsobeenusedinaisoftwaretestingbecauseofitsability to test non testable programmings such as statistical classifiers search engines and autonomous cars .
patinvisanovelmetamorphictestingapproachformachinetranslation software.
conclusion in this paper we present patinv a novel effective and widely applicable methodology for finding translation errors in machine translationsystems.incontrasttoexistingapproachesthatrelyon invariances between translations e.g.
structural invariance patinv is based on pathological invariance sentences of different meaningshaveidenticaltranslation.becauseofthissignificantconceptual difference our implementations of patinv have reported many erroneous translations that cannot be found by existing approaches.inourevaluation patinvsuccessfullyfinds100erroneous translations for en hi and erroneous translations for en zh respectively in google translate and bing microsoft translator.
in particular among the errors reported for en zh out of are uniquetopatinv.inaddition patinvperformsaccuratelyandisespeciallyeffectiveinfindingword phrasemistranslationerrorsand under translation errors.
thus patinv complements with existing approaches.
in the future we will provide more implementations ontopofpatinv scoreidea forexample generatingnewsentences by inserting words to detect over translation errors.
we will also applythegeneralideatovalidateothertextgenerationtasks such as speech recognition and code summarization.