semantic web accessibility testing via hierarchical visual analysis mohammad bajammal university of british columbia vancouver bc canada bajammal ece.ubc.caali mesbah university of british columbia vancouver bc canada amesbah ece.ubc.ca abstract web accessibility the design of web apps to be usable by users with disabilities impacts millions of people around the globe.
although accessibility has traditionally been a marginal afterthought that is often ignored in many software products it is increasingly becoming a legal requirement that must be satisfied.
while some web accessibility testing tools exist most only perform rudimentary syntactical checks that do not assess the more important high level semantic aspects that users with disabilities rely on.
accordingly assessing web accessibility has largely remained a laborious manual process requiring human input.
in this paper we propose an approach called a xeray that infers semantic groupings of various regions of a web page and their semantic roles.
we evaluate our approach on real world websites and assess the accuracy of semantic inference as well as the ability to detect accessibility failures.
the results show that a xerayachieves on average an f measure of for inferring semantic groupings and is able to detect accessibility failures with accuracy.
index terms web accessibility web testing accessibility testing visual analysis i. i ntroduction web accessibility is the notion of implementing web apps in a fashion that allows programmatic access to software functionalities that are otherwise only perceivable through certain senses e.g.
visually .
accessibility has been sometimes dealt with as an afterthought or a nice to have optional feature with many developers and companies ignoring it altogether .
however as shown in figure millions of people around the globe have software related disabilities and are impacted by web accessibility or the lack thereof.
furthermore accessibility is increasingly becoming a legal requirement ratified into laws in many countries.
for instance in the united states the americans with disabilities act requires all government and public agencies as well as certain businesses to make all their software and information technology services accessible.
similar provisions are required by law under the european union s web accessibility directive .
figure shows the increasing number of lawsuits filed in federal us courts against businesses for failing to provide accessibility accommodations.
despite the increasing legal economical and human costs due to lack of accessibility there has been little work in the software engineering research community to automate accessibility testing.
eler et al.
check for missing attribute fields or incorrect attribute values related to accessibility ofweb pages an approach that is also used in a few patents .
the bulk of existing work focuses on topics such as evaluating best practices for conducting empirical accessibility evaluations such as manual checklist walkthroughs or enlisting visually impaired users as part of user studies .
a number of open source tools have been developed to conduct simple syntactic accessibility tests that only check a few attribute values in a web page.
in an audit of of such accessibility testing tools conducted by the united kingdom government s office of digital services these tools found only of a known small set of accessibility violations present in tested web pages.
the aforementioned tools are based on conducting syntactic checks which are simple markup rules e.g.
any aelement must contain a non empty string to check for a minimal level of accessibility.
however none of the aforementioned tools analyze key accessibility requirements related to the semantics of a page such as the high level page structure and the roles or purpose of various elements.
it is these semantic aspects of a page that users with disabilities rely on the most while using web pages but none of the existing tools test for.
accordingly the high level semantic analysis of web accessibility has remained a manual and laborious time consuming process .
to that end we propose an approach that automates testing of a subset of web accessibility requirements pertaining to high level semantic checks that have not been amenable to automation.
the approach is based on a visual analysis of the web page coupled with a few natural language processing nlp steps to conduct a semantic analysis of web pages.
this analysis first identifies major cohesive regions of a page then infers their semantic role or purpose within the page.
subsequently a conformance check is conducted that determines whether the markup of the page correctly corresponds to the inferred semantics.
if the markup contains the same semantic information perceivable by sighted users the page is deemed accessible.
otherwise if semantic information of the page is perceivable only visually but not conveyed in the markup the page would be flagged as failing the accessibility test and the specific reasons are reported.
in this work we focus on vision disabilities as opposed to other forms of disability e.g.
hearing .
the rationale for this is twofold.
first the web is predominantly a visual medium where most of the ieee acm 43rd international conference on software engineering icse .
ieee millions of peoplepopulation of swedenpopulation of canadapopulation of italypopulation of germanyindividuals with accessibility impairments european unionindividuals with accessibility impairments united states figure .
number of people with software related disabilities e.g.
vision hand control cognitive or hearing in the united states and the european union.
data compiled from 2000number of lawsuits year 572628142258figure .
number of software accessibility lawsuits filed in us federal courts per year.
the number of lawsuits increased by around over the four year period .
data compiled from information is accessed visually as opposed to other senses.
second surveys have shown that vision disabilities are the most relevant to web users with disabilities .
this paper makes the following main contributions a novel approach for automatically testing semantic accessibility requirements which is the first to address this issue to the best of our knowledge.
a technique that is based on visual analysis coupled with a few natural language processing steps to infer structure and semantic groupings on a web page for the purpose of accessibility testing.
an implementation of our approach available in a tool called a xeray.
a qualitative and quantitative evaluation of a xeray in terms of its inference accuracy and ability to detect accessibility failures.
the results show that it achieves an f measure of for inferring semantic groupings and is able to detect accessibility failures with accuracy.
ii.
b ackground and motivating example figure shows an example of an inaccessible web page.
in a quick glance at the rendered page in c a sighted user can immediately understand the structure of the page and navigate their way through the various contents of the page.
for instance the user would immediately recognize that they can navigate to other areas of the web site through the navigation menu at the top i.e.
home news faq .
while this happens naturally and instantaneously for sighted users that is not the case for visually impaired i.e.
nonsighted users.
the page structure e.g.
the presence of a navigation bar at the top is communicated exclusively through visual design since the html markup in figure a is simply a collection of div s that do not communicate any semantic functionality.
this implicit visual communication is intuitive and natural for sighted developers and users but is unavailable for users who can not have access to visual information due to disabilities.
accordingly the markup is deemed inaccessible because it is expressed in a fashion that does not provide any semantic information about the page structure.the analysis and conclusion that we just made is currently being done manually since it requires high level semantic analysis of the page.
our goal in this work is to automate the reasoning we have just described in order to be able to automatically reach conclusions about the accessibility of web pages.
a. aria roles the aforementioned lack of semantic markup in the code makes it difficult for non sighted users to navigate the page.
this is because such users rely on screen readers to parse the page for them and present them with the various information or navigation options present in the page.
screen readers are tools that speak out the various options regions or tasks accessible from the page and the non sighted user would then select one of the options they heard from the screen reader.
screen readers can be thought of as web browsers for non sighted users with one major caveat.
while a standard web browser simply renders the page as is and leaves it to the end user to understand what the various elements mean screen readers expect that the page markup contains semantic information about various areas of the page which the reader would then announce audibly giving non sighted users an understanding of the overall semantic structure of the page.
the standard that is used by screen readers during their processing of web pages is the w3c accessible rich internet applications aria semantic markup.
the aria standard specifies a set of markup attributes that should be included in the page s html to make it accessible to screen readers or other assistive technologies.
targeted roles.
aria defines more than attributes spanning various aspects of the page from low level syntax requirements to high level semantic structure.
we therefore have to target a subset of these aria attributes because addressing all attributes in a single academic paper is untractable.
the basis for selecting the targeted roles is that we focus on the most commonly used subset of aria which are landmark roles .
our own experimental results section iv b2 also demonstrate the widespread use of these roles.
the roles div class nv8471 div home div div news div div faq div div contact div div div class hd902 resources div div ... div div class hd902 about us div div ... div .nav8471 background color ffff00 display flex justify content space around font weight bold border 2px solid .
hd902 font size 5vw font weight padding 8vh 2vh a html code b css declaration c rendered page figure .
an example of an inaccessible web page.
identify the major high level regions of the page and specify the semantic role for each of them.
they consist of a set of specific pre defined roles that convey though markup the otherwise only visually perceived role of each region.
for instance the yellow rectangle at the top of figure c is perceivable as being a navigation bar therefore the markup of the element containing that region has to include the landmark attribute role navigation .
when a non sighted user loads the page the screen reader would speak out the presence of a navigation bar and any other semantic regions in the page allowing the user to quickly perceive the high level structure of the page and directly interact with the region they are looking for.
accordingly the absence of semantic markup in figure a makes the page inaccessible because screen readers would be unable to provide alternative non visual means to access the page.
that is the information and cues about the structure and navigation of the page remain locked in the visual design of the page and can not be accessed non visually.
the task of ensuring web accessibility would therefore rely on one key principle any information structure or functionality that is visually perceivable by sighted users must also be available for non sighted users.
when a web page communicates its structure and function using only visual design without providing a programmatic means to access the same information it is deemed inaccessible.
b. existing tools syntax checkers existing accessibility testing tools are based on syntactic checking.
they check the html against certain syntax rules.
for instance one common check is to assert that every img element has an alt text attribute for text descriptions .
another check asserts that input elements must not be descendent of a. another example is checking that every form element has a label attribute.
in general the syntactic checks all follow the form if certain syntax a is present assert that syntax b is true.
while such syntax checks can be useful simple assertions and are easy to automate none of the existing tools perform the more important and more challenging high level semantic analysis of the page s content.
for instance consider figure c where we can see that there is a navigation bar at the top.
now consider the html in a where we observe that the developer did not use the required aria attribute of role navigation .
accordingly because we visually see a navigation region in c but do not find it expressed in the markup in a we conclude that the page has an accessibility failure because we perceived a certain semantic structure as sighted users but it was not expressed in the markup in order to ensure it would be equally available for non sighted users.
that is the the central issue in accessibility making sure that whatever is visually perceived is also expressed in the code.
we now pause to reflect and observe that there is no syntactic check that can automate the conclusion we just reached.
that is the process that we just walked through requires our own visual perception as humans in order to reach the conclusion that there is a mismatch between the perceived page and the markup.
this process can not be put in the form of a syntax check.
in the next section we describe how we construct an approach that automates the accessibility testing procedure that we have just manually conducted.
iii.
a pproach in this paper we propose an approach to automatically test for a subset of web accessibility violations that are pertinent to semantic structure.
we recall that the scope of this work focuses on vision disabilities as opposed to other forms of disability due to the web being a predominantly 1612figure .
overview of the proposed approach.
visual medium and the fact that vision disabilities are the most common software related disability .
figure shows an overview of our proposed approach which is based on the strategy of visually analyzing the web page to infer semantic groupings and their roles and then checking that the html markup matches the inferred semantic roles.
the approach begins by obtaining the document object model dom and screenshot of the web page rendered in a web browser.
next the set of visibly perceivable objects is identified.
this is then used to perform a semantic grouping of the page into a set of semantically coherent regions.
subsequently this information is used in an inference stage where the specific semantic role of each region is detected.
finally the inferred semantics are checked against the markup used in the page and a report is generated as to which parts of the page are inaccessible.
the rationale behind this strategy is to check whether the semantics visually perceivable by sighted users are reflected in the semantics of the html markup thereby ensuring accessibility.
a. visual objects identification in this first stage the goal is to identify objects that are perceivable by sighted users which we refer to as visual objects .
for instance in figure c each item in the top navigation menu would be a visual object.
this step of visual objects identification is the foundation of our overall approach since the identification of visual objects enables checking whether the information and elements that are perceivable by sighted users are also accessible to non sighted users.
objects extraction we begin by taking as input the dom of the page after it is loaded and rendered in a browser.
we then extract from the dom a set of nodes that represent visual content of the page and we refer to each of theseasvisual objects .
we define three types of visual objects textual image and interactive.
textual objects.
the extraction of text content is achieved by traversing text nodes of the dom.
more specifically t fej e e g where tis the set of all visual objects that represent text in the page e2dom is a leaf element iterator of the rendered dom in the browser e is a heuristic predicate that runs a series of checks to detect visually perceivable elements as will be described in section iii a2 and e is a predicate that examines whether there is a text associated with e. more specifically it returns non empty nodes of dom type text which represent string literals.
an example of extracted textual objects would be the resources section in figure c .
we note that the predicate is based on a node type rather than an element i.e.
tag type.
this allows more robust abstraction because the predicate captures any text and does not make assumptions about how developers choose to place their text.
in other words regardless of the tag used for text data e.g.
span div text would still be stored in nodes of type text even for custom html elements.
this helps in making the approach more robust by reducing assumptions about tags and how they are used in the page.
image objects.
subsequently we perform another extraction for image objects.
we define this as follows m fej e e g where mis the set of all visual objects that represent images.
as in the previous case the predicate e examines whether there is any relevant image content associated with e. this has two possibilities a nodes of img svg and canvas elements and b non image nodes with a non null background image.
an example of extracted image objects would be the bell icon in figure c .
we note that this predicate makes the proposed approach more robust by eliminating assumptions about how developers markup images.
if images are contained in standard tags e.g.
img svg then the predicate readily captures them.
however we make no assumptions that this is the only way an image can be included.
for this reason we also capture elements of any type when a non null background image is detected.
interaction objects.
finally we extract the interaction elements as follows i fej e e g where iis the set of all visual objects that represent form elements or similar interactive elements.
these are determined by the predicate e which collects elements such as input fields and drop down menus.
an example of extracted interaction objects would be the email input field in figure c .
visual assertion after the preceding extraction of an initial set of visual objects this stage proceeds by conducting a visual analysis of the objects.
this analysis detects if an object is visually perceivable.
we conduct the visual analysis 1613as follows.
first we obtain the box model of each object.
we use the computed box model in order to faithfully capture the location as finally rendered on screen.
next we obtain a screenshot of the region defined by the box model.
we then analyze the screenshot using the prewitt operator used in computer vision.
this operator applies a set of derivatives or differentiation operations on the image and then typically used to detect salient visual features in the image e.g.
shapes textures .
we therefore use this operator to extract any visual features present in the image regardless of the form of these features.
depending on the presence or absence of visual features the perceptibility state of the object is determined.
if no visual features are detected the object is deemed to be nonperceivable and vice versa.
for example consider figure c .
the navigation region in the top and the main content that follows it are perceivable by sighted users.
however web pages also have spacing elements that do affect the layout but are not individually perceivable themselves.
for instance there can be an element between the navigation bar and the resources section such that a certain vertical distance is maintained below the navigation bar.
while such a spacing element certainly affects the layout and occupies screen space it does not constitute a visual object due to it s imperceptibility.
b. semantic grouping after the visual objects identification is completed we proceed by grouping visual objects into groups representing potential semantically relevant regions on the page.
for instance in figure c one semantic grouping would be the navigation region at the top of the page.
another semantic grouping would be the resources and about us sections representing the main content of the page.
the rationale for this step of the approach is as follows.
we recall that screen readers expect the markup to indicate the major semantic regions of a page.
accordingly in order to automatically assert that any visually perceivable semantic region has been also expressed in the markup we first need a mechanism by which we can detect the semantic regions in the first place.
this is what we aim to achieve in this stage.
here we are only concerned with creating potential semantic groupings while the next stage section iii c infers what exactly is the semantic role if any of each potential grouping.
the grouping uses both structural dom information as well as visual analysis.
the dom is used to generate a large number of potential seed groupings and the visual analysis performs filtering and further analysis to produce a final set of groups.
we adopted this strategy for the following reasons.
we observed that the dom can be used as a source of seed groupings due to its inherently hierarchical nature that also tend to capture the developer s or designer s own intended semantic grouping.
that is the children of a node constitute a rudimentary form of a group which can then be further analyzed merged or divided to create a refined grouping.
subsequently visual analysis filters these initial seed groups and process them to construct semantic groupings.
visual analysis is used because while the dom may provide seedgroupings it does not faithfully represent what the end user is actually observing on the screen.
grouping process.
we now describe the mechanism of the grouping process.
first we obtain one flat non hierarchical set of all dom elements.
for instance in figure a this would be all the div elements in one flat set.
the elements are collected regardless of visibility due to the complex nature of dom and css rendering where non visible nodes can contain visible children.
for this same reason the initial set of elements is flat and non hierarchical because visible children can often be inside non visible nodes and therefore relying on dom hierarchy would yield many false positives and negatives.
instead we build the hierarchy by visually analyzing the collected flat set of elements.
we do this by first collecting the computed box model of each element in the set.
for instance in figure a this would result in a set containing the computed box model of each div element regardless of hierarchy.
next we remove box models that are visually located outside the page boundaries since they are not perceivable to sighted users.
for boxes that are only partially outside the page we trim them to page boundaries.
subsequently we filter equivalent boxes which is when a pair of boxes visually contain the same set of visual objects.
we do this by removing the smaller box in terms of visible area in a pair of equivalent boxes.
next we filter boxes based on how many visual objects are visually contained i.e.
located within them.
we remove each box that visually contains the entire set of visual objects on the page.
for instance in figure a any div that visually contains the entire set of all div s is removed.
this is because such a set does not represent any semantically useful grouping since the entire set of objects is in one group only.
finally we iterate over the set of visual objects.
for each object we find the largest box that visually contains the object.
once this is completed for all visual objects the final result is a set of boxes representing the potential semantic groupings on the page.
c. semantic role inference once semantic grouping is completed we proceed to infer the semantic role of each group.
this step infers one of the pre defined landmark roles section ii a .
an example can be seen in the top navigation bar in figure c indicating the pre defined role of navigation .
however not all roles are relevant to our scope of automated semantic analysis.
for instance region is a generic catch all label that does not convey any specific semantic role and its use is generally discouraged and typically not used by screen readers.
another example is form a label that indicates form regions.
the label is directly associated with html form elements and therefore no semantic analysis or inference is needed for its detection.
accordingly we focus our semantic analysis on the more relevant roles of main navigation contentinfo and search which will be described in the following sections.
main role.
the main aria role indicates a region that contains the main output or results in a web page.
for example 1614on the search results page of a search engine the region containing the list of retrieved search results would be the main region which is then surrounded by other regions such as the navigation bar or footer.
the process by which we infer the role of a group to bemain is as follows.
first we compute a score for each detected group in the page.
the score uses both visual geometrical attributes as well as natural language processing nlp measurements.
more specifically main r a r r whereris a semantic grouping of the page main is the score a r is the visual geometric area for r and r is an nlp metric we define to measure linguistic aspects of the contents ofr.
more specifically r first performs a part of speech pos tagging which is a common nlp analysis than assigns pos labels e.g.
verb noun adjective to each word.
r then measures the variance of the linguistic pos tag frequencies of all textual objects contained in r. we give an example to clarify the various measured values.
consider the rendered page in figure c. rwould represent for instance the region containing the body of the page e.g.
the resources and about us sections .
a r would be the geometric area of that region as visible on the screen.
the rationale is to capture how much would a region occupy the visible space for sighted users.
as for r it first collects all textual objects as explained in section iii a1 within r which would collect all text elements such as resources about us as well as the paragraphs on the page.
for each text object pos tags are collected and then their frequencies i.e.
count of each tag type are computed.
then measures the variance of these pos tag frequencies.
for instance a navigation region rthat has say the textual objects images news and settings has no variance since they all have identical pos tags.
contrast this with the main body of text in a page which contains elements such as such as paragraphs section headings links and much more.
the likelihood of all such content to be linguistically monotonous i.e.
all tags are nouns is practically negligible.
this is why eq.
includes the r factor.
thea r in the equation accounts for the fact that it is unlikely that the main region of the page would be the visually smallest area on the page.
once the score in eq.
is computed for all detected regions we sort the regions by score and select the region with the highest score which is finally reported to be the region having the main role.
navigation role.
the navigation aria role indicates a region in a webpage that allows users to navigate between various pages or views.
the process by which we infer the role of a group to be navigation is as follows.
we first compute a score for each group using the following equation nav r c r h r whereris a semantic group of the page navis the score andc r is a metric that measures the clickables ratio inside the groupr.
this computes the ratio of visual objects thatappear to be clickable to sighted users which we define as any visual object whose onscreen cursor is a hand or a pointer indicating to sighted users that it can be clicked on.
accordingly a group that has high c r is mostly composed of objects that a sighted user can click on which is typically the case for navigation regions.
for example in figure c the yellow navigation region at the top contains elements that all appear as clickables to sighted users.
in contrast a group that does not contain any clickables e.g.
only static texts and images would have a c r equal to zero and therefore is not a navigation region.
this can be seen for instance in figure c in the body of the page below the navigation bar where the body contains only static text paragraphs or images.
h r is a measure of the homogeneity of the contents of r. for semantic groups containing only textual elements h r is the same nlp linguistic variance metric we defined in eq.
.
for all other elements h r represents the dimensional variance of the objects in r. finally a given group is inferred to have a navigation role when navgreater than or equal unity which was determined empirically.
contentinfo footer role.
the contentinfo role also known as the footer role indicates regions of the page that represent complementary content to the parent document.
that is instead of containing the main output of the page or the main navigation elements footer regions serve as complementary content or information that comes after the main content.
in a similar fashion to previous roles we compute a score for each detected grouping using the following equation footer r c r d r a r where as in the previous roles ris a semantic group of the page footer is the score a r is the visual pixel count for r d r is the visual distance from the geometric center of rto the origin of the screen and c r is the clickables ratio in ras defined in eq.
.
as can be observed from the equation the score is mostly concerned with the visual geometric aspects of the region since this aria role is by definition spatial in nature since it refers to a specific spatial visual placement on the page.
accordingly we compute and sort the score for all groups and select the group with the highest score.
if the group is located in the lower half of the page it is reported as a footer.
otherwise no footer regions are reported.
search role.
the search aria role indicates regions in a page that allow users to enter a search query and retrieve items on the page or site.
to infer this role we use a combination of visual analysis a supervised machine learning model as well as linguistic i.e.
keyword techniques.
first we train a convolutional neural network cnn to visually recognize search icons.
we collected and labeled data points representing icon images positive examples and used the inception cnn architecture which has been shown to produce very effective classifications for computer vision machine learning problems .
subsequently we use this model to find search icons on a page.
next we perform a nearest neighbor search to look for text input fields in the 1615spatial vicinity of detected search icons.
if a text input field is found we mark the region containing the search icon and the input field as having a search semantic role.
furthermore we also check for cases where the search input text field has no associated search icons.
in this scenario we extract all text input fields on the page.
we then perform a nearest neighbor search to find any visible label texts in the visual spatial vicinity around the input field.
we then conduct nlp stemming on the label text and find those that include key linguistically significant stem words such as find search and locate.
any detected group that matches any of the above cases is marked as having a search semantic role.
finally we note that due to the non hierarchical nature of our semantic groupings all inferred roles are agnostic to hierarchies and the proposed approach is therefore able to detect hierarchical combinations of the inferred roles e.g.
a navigation region within a footer region .
d. markup conformance this final stage asserts that the source of the page contains markup indicating the presence of the inferred semantic regions and their semantic roles.
for instance in figure c the approach so far would infer that the group of elements at the top of the page represent a coherent semantic grouping and that their semantic role is navigation.
if the html markup corresponding to that area does not contain the aria landmark role of navigation then screen readers will not be able to provide this semantic information to users and we recall that this semantic information is among the most important and widely used of aria roles by users with disabilities .
therefore in such cases where the markup does not conform to the inferred semantic roles we report an accessibility failure and indicate the expected semantic markup and where it should have been expressed in the page.
the mechanism of checking markup conformance is as follows.
first we obtain the semantic groupings and any inferred roles as described in the previous sections.
for each semantic group we identify all dom elements that satisfy two criteria all visual objects of the group are located inside the element s box model and the element s box model is located inside the group s box model.
this process captures all possible dom elements that would qualify as a root for the region without including objects from other regions.
any of these dom elements would therefore have to contain markup indicating the presence of a region and its role.
we then check whether any element in the set meets both of the following requirements the element has a role attribute whose value matches the inferred semantic role of the group.
the element s computed box model visually overlaps the box model of the inferred semantic group.
the rationale for adopting this approach is as follows.
as we noted in section iii b the complex nature of dom and css rendering easily allows cases where non visible non rendered nodes can contain visible rendered children.
a dom based approach e.g.
checking containment by xpath would therefore yieldmany false positives and negatives.
accordingly we use the visual check above for a more robust analysis.
if an element satisfying these requirements is found we log it and move on to the next inferred semantic role and check that it has been correctly expressed in the markup.
the process is repeated for all semantic groupings for which a role has been inferred.
any semantic grouping for which no role has been inferred is discarded.
a report is finally generated indicating all roles that have been correctly expressed in markup and all roles that should have been in the markup but are missing.
e. implementation we implemented the proposed approach in a tool called axeray short for accessibility ray .
it is implemented in java.
we use selenium webdriver to instrument browsers and extract dom information and computed attributes.
we use opencv for computer vision computations deeplearning4j for machine learning operations and the stanford corenlp library for linguistic analysis.
to make the study replicable we made available online a link to our a xeray tool and the anonymized participants responses .
iv.
e valuation to evaluate a xeray we conducted qualitative and quantitative studies to answer the following research questions rq1 how accurate is a xerayin inferring semantic groupings and semantic roles?
rq2 to what extent can a xeraydetect accessibility failures in real world web pages?
in the following subsections we discuss the details of the experiments that we designed to answer each research question together with the results.
a. rq1 semantic grouping and roles inference in this question the objective is to assess how accurate is the semantic grouping and semantic role inference processes.
the rationale for evaluating this aspect is that the approach first performs the grouping and semantics inference and then uses this inference to test for accessibility.
accordingly we first need to assess the inference process itself.
we evaluated this question as follows.
first we collected random subjects from the moz top 5001most popular websites.
we then ran a xerayon each test subject s url and obtained the output groupings and semantic roles.
figure shows an example of the output.
each rectangle represents an inferred grouping together with it s semantic role.
subsequently we recruited human evaluators.
evaluators were recruited from the mturk2crowdsourcing platform.
the qualifications of participants are to be working in the software industry and to have maintained the highest level of accuracy on the mturk platform which is referred to as masters level.
subsequently each human evaluator was presented with the output of a xerayfor all test subjects and asked to assess 1616figure .
sample of the generated accessibility report.
the accuracy of groupings and roles.
more specifically we asked them to identify any output groups that do not represent a meaningful semantic grouping.
this represents the false positives of groupings while the remainder are true positives .
we also asked them to identify any meaningful semantic groupings on the page that were not included in the output.
these are the false negatives .
the same process is repeated for the semantic roles.
results and discussion table i shows the results of evaluating the accuracy of semantic grouping and role inference.
the columns show the precision recall and f measure averaged across evaluators.
the two groups of columns labeled grouping inference and role inference show the accuracy of the proposed approach in inferring semantic groupings and semantic roles respectively.
the highlighted cells show the minimum and maximum values in each column.
the key outcome of this evaluation is the f measures which are at and for grouping and role inference respectively.
these values indicate a rather effective inference process.
the lowest precision was .
this often happens due to a somewhat unusual dom structure where elements in the same region were placed at large tree depth separations from one another.
this resulted in mistakenly grouping a number of elements that should have not been grouped.
as for the recall the lowest performance was at .
such low values often happen in corner cases where elements are falsely excluded from groups due to having an empty box model stemming from complex nested css rules despite being present in the group.
b. rq2 accessibility failures detection while the previous question examined how accurate the inferred semantic groupings and roles are this rq evaluates to what extent the inferred information can be used to reliably detect accessibility failures.
due to the absence of ground truth data we evaluate this rq in two complementary ways using a fault injection experiment and evaluating the output on a large number of real world subjects in the wild.
the rationale for using these two complementary ways is as follows.
in the fault injection experiment explained section iv b1 existing semantic markups if any are removed thereby simulating a fault and a check is made whether the tool is able to detecttable i precision and recall of grouping and role inference .
highlighted numbers are the minimum and maximum values in each column .
grouping inference role inference subject prec.
recall f prec.
recall f wikipedia.org google.com amazon.com stackoverflow.com medium.com khanacademy.org imdb.com cnn.com rt.com booking.com average this removal.
the benefit of this experiment is that it allows automated evaluation without a subjective assessment.
the drawback however is that it is only a lower bound of the actual accuracy since it says nothing about other possible role faults that have not been injected i.e.
due to the absence of some roles in the original markup itself .
for this reason we supplement the fault injection experiment with a manual itemby item evaluation of the tool s output in order to evaluate all true false positive and negative results.
we describe each approach in the following subsections.
subjects.
we conducted the experiments in this rq on a total of real world subjects.
the subjects were collected in two ways.
the first half of subjects were randomly selected from the moz top most popular websites as in rq1.
the second half of subjects were obtained from discuvver.com which is a service that returns a random website from the internet.
fault injection for each subject we inject a random fault in its markup and assess if a xeraywas able to detect the fault.
we recall from section ii a that a web page is deemed inaccessible if there is an absence of semantic roles.
that is the developer did not add the necessary semantic markup to the page.
our goal is thus to simulate this behavior by removing all existing semantic markups on the page and therefore create a new page as if the developer had not included the necessary semantic markup and then check whether our approach can re detect them.
such markup omission by definition is what makes web pages inaccessible 1617table ii results of detecting fault injections .
detected faults subjecttotal injectionsproposed approachbaseline bing.com youtube.com google.com microsoft.com bbc.com amazon.com medium.com n a yahoo.com live.com paypal.com blogger.com netflix.com n a stackoverflow.com imdb.com walmart.com fuelly.com typing.com iconpacks.net expatistan.com n a memrise.com n a retrevo.com startupstash.com eatthismuch.com kdl.org getpocket.com retailmenot.com mailinator.com n a myfridgefood.com joinhoney.com bannereasy.com total .
.
and is therefore the only meaningful fault type.
in other words any mutation of the markup is effectively a markup omission since the exact expected semantic role would become absent from the markup.
misspelled attribute values are also effectively markup omissions.
for instance suppose that a region should have been marked as a navigation region.
whether the navigation role was completely absent from the markup or was misspelled i.e.
nagviton both cases are still effectively markup omissions.
we now describe the injection process.
first we load the subject in an instrumented browser i.e.
via selenium .
we then remove all semantic markups on the page i.e.
landmark role s .
we then apply a xerayon the subject and collect the output.
if after the fault injection i.e.
removal of role s axeraywas able to indicate that there should be a semantic role we conclude that the injected fault has been caught.
otherwise the fault was not detected.
baseline.
in order to have a more thorough evaluation we included a baseline in our experiments.
however since there are no existing tools that perform semantic checking our baseline consists of a simple random selection process.
in this process random regions from the page are selected.
next a random semantic role is assigned to each randomly selected region.
this set of semantic regions and their semantic roles is then taken to be the baseline.results and discussion.
table ii shows the results of evaluating the fault injection experiment.
the first column shows the total number of fault injections performed on each subject.
we recall that this first column is not a number we chose it is rather the total number of injections that were possible since the faults are removals of existing semantic markup.
the second column shows the number of injected faults that were successfully detected by whereas the third column shows the number of injected faults that the tool has failed to detect.
the last row sums up the results across all subjects.
the main result of the evaluation is that a xerayhas detected on average around .
of the injected faults.
while this performance is relatively good given that this sort of analysis hasn t been automated so far we do note this is only a lower bound of the actual accessibility failure detection ability since it says nothing about other possible failures that have not been injected e.g.
where the original markup itself did not include certain semantic roles .
in certain subjects in table ii marked with n a the fault injection process was not possible.
this was because the subject s markup did not contain any of the landmark semantic roles and therefore it was not possible to remove them them and check if our tool was able to detect them back.
despite some of these subjects being top websites the lack of such markup in the subject is an example that illustrates the need for effective and automated accessibility testing.
direct output evaluation in this step we manually evaluate the output on a large number of real world subjects in the wild.
first we loaded each test subject in an instrumented web browser i.e.
via selenium .
page popups or notifications if any were closed.
next we applied a xerayon the subject and collected the generated output as shown in figure .
we then categorized each item in the report into one of the following true positive this represents a true accessibility failure.
this category holds whenever the tool has reported the absence of a correct semantic role that is indeed missing from markup and therefore the reported failure is true.
false positive this is a false accessibility failure.
in this case the tool has either reported an incorrect semantic role or the role is already in the markup but was falsely flagged as missing and therefore the reported failure is false.
false negative this case is a false accessibility pass not failure as in the two previous cases .
this represents cases where a semantic role should have been included the markup but the tool did not report an accessibility failure.
true negative this corresponds to true accessibility pass.
this represents cases where a role is actually semantically not present on the page and the tool did not report a failure.
this also corresponds to cases where a semantic role is present in the markup and the tool has reported that the markup is conforming to the inferred semantic role.
results and discussion.
table iii shows the results of the evaluation.
each row lists the subject the true positive negative and the false positive negative for the subject.
the last row shows the accuracy precision recall and the f measure.
1618table iii direct evaluation of accessibility failure detection on 30real world subjects .
proposed approach sortsite baseline subject tp fp tn fnsemantic issuessyntactic issuestp fp tn fn bing.com youtube.com google.com microsoft.com bbc.com amazon.com medium.com yahoo.com live.com paypal.com blogger.com netflix.com n a n a stackoverflow.com imdb.com n a n a walmart.com fuelly.com typing.com iconpacks.net expatistan.com memrise.com retrevo.com n a n a startupstash.com eatthismuch.com kdl.org getpocket.com retailmenot.com mailinator.com myfridgefood.com joinhoney.com bannereasy.com acc.
prec.
rec.
f1 acc.
prec.
rec.
f1 .
.
.
.
.
.
.
.
the values of the accuracy precision and recall are between and indicating a relatively good performance.
the true positive column represents the true accessibility failures that are indeed present in the subjects.
this certainly does not cover all possible accessibility failures but rather the subset of accessibility issues that we focus on in this work i.e.
the semantic roles .
the true negative column can be thought of as the number of cases where the markup of the subject is in agreement with the tool s output.
in the median two true accessibility failures were detected per subject and two inferred semantic roles per subject were in agreement with what has been expressed in the markup.
for the second half of subjects in table iii i.e.
the random sites have used semantic roles.
in contrast for the subset of top websites i.e.
the first half of subjects have used semantic roles.
of the random websites did not use any semantic roles compared to of the top websites.
this observation is expected since top sites are more likely to have more resources to create better products.
the average execution runtime was seconds.
we then investigated the reasons behind the false positives and negatives.
one common reason is erroneous inference of navigation roles.
this occurred for instance for a region that consisted of weather forecast for the next few days.
from the perspective of our inference procedure this looked like a navigation area since it had a group of links that werecoherent in content and presentation in the sense described in section iii c .
accordingly it was falsely indicated as a navigation and therefore resulted in a false accessibility failure.
another reason involves missing footer roles that should have been reported.
this occurred for instance for a region that was not recognized by the semantic grouping stage and therefore no role was able to be inferred for it.
for comparison table iii also includes an evaluation of sortsite3 which is the best performing state of the art accessibility testing tool .
as mentioned in the introduction sortsite and other state of the art tools only perform syntactic checks and is therefore unable to detect the semantic issues that are the focus of this work.
accordingly we run an evaluation that verifies this empirically.
each subject is fed to sortsite and the output report is saved.
each reported failure is then categorized as either a syntactic issue or a semantic issue.
we recall that as discussed in section ii b a syntactic issue is any failure that only checks the syntax of the html.
examples include checks like each aelement must contain non empty text or input must not appear as a descendant of a .
these are direct checks that are only concerned with syntax.
compare this for instance with the reported failure shown in figure .
here the failure is semantic in nature.
that is the failure is not an application of 1619some syntactic rule.
rather the failure is reported because the markup does not conform to how the page is semantically perceived from a visual perspective not because it didn t conform to a predetermined syntactic rule.
from table iii we observe that sortsite was able to find many syntactic issues rows with n a are cases were the tool was unable to load the subject .
however it did not detect any of the semantic issues.
this is expected as the rationale for this work was the observation that the state of the art only conduct syntactic checks which can not detect the more important and widely used semantic information.
future work in this work we focused on an important subset of accessibility requirements which are the semantic roles as discussed in ii a. therefore as expected our approach can not cover all possible accessibility requirements.
this leaves open a number of avenues for future work to address other accessibility requirements each of which would require a novel technique to address.
the variations in the semantics of various accessibility requirements and the lack of approaches to address them mainly due to the difficulty of performing high level semantic analysis presents a rich and fertile ground to conduct research which has received little attention from the software engineering research community as discussed in the introduction.
for future work we believe it will be a fruitful and interesting pursuit for the research community to explore some of these other accessibility requirements.
threats to validity we chose test subjects i.e.
web sites randomly from the internet with the mentioned criteria in section iv to avoid any selection bias.
plus the participants were selected to be highly qualified evaluators at the crowdsourcing platform mitigating the threats to the internal validity of the study.
the subjects are diverse and complex enough to be representative of real world scenarios mitigating the external validity of the study by making the results generalizable.
to make the study replicable we made available online a link to our a xeraytool and the anonymized participants responses .
v. r elated work accessibility attribute checkers.
existing approaches related to accessibility testing focus on checking syntactical attributes.
eler et al.
and patil et al.
check for missing or wrong ui attributes in android apps such as missing alternative text attributes in images or color attribute values below a certain threshold.
similar checks are also used in other tools such as google s accessibility scanner wa ve and aslint .
the aforementioned papers focus on syntactic checks as opposed to the proposed approach in this work which checks for high level aspects such as page structure and semantic landmarks which are the most important aria roles that users with disabilities rely on .
accessibility guidelines.
the majority of existing work lies within the accessibility research community rather than software engineering.
this research area involves studying certaincategories of websites e.g.
airline websites education portals other categories or certain platforms e.g.
android and then focusing on manually observing how non sighted users would use those apps or websites in order to identify any patterns or trends in accessibility with the purpose of publishing improved accessibility guidelines.
another line of work focuses on researching software development best practices and how do they impact the accessibility of the end product.
for instance sanchez et al.
and bai et al.
examine development practices in agile teams working on accessible software with the goal of proposing a guideline for better agile practices.
krainz et al.
investigates the impact of a model driven approach to development on the accessibility of the created product.
none of the aforementioned works however is concerned with developing an automated approach to test accessibility.
instead their focus is researching best practices or guidelines for developers and designers.
visual analysis.
there exist a few techniques that analyze web applications from a visual perspective.
choudhary et al.
propose an approach that detects cross browser compatibility by examining visual differences between the same app running in multiple browsers.
burg et al.
present a tool that helps developers understand the behavior of front end apps.
it allows developers to specify the element they are interested in then tracks that element for any visual changes to understand code behavior.
bajammal et al.
propose an approach to generate reusable web components by analyzing design mockups.
in contrast to our work none of these works are related to accessibility.
vi.
c onclusion software accessibility is the notion of building software that is usable by users with disabilities.
traditionally software accessibility has often been an afterthought or a nice to have optional feature.
however software accessibility is increasingly becoming a legal requirement that must be satisfied.
while some tools exist to perform basic forms of accessibility checks they focus on syntactic checks as opposed to checking the more critical high level semantic accessibility features that users with disabilities rely on.
in this paper we proposed an approach that automates web accessibility testing from a semantic perspective.
it analyzes web pages using a combination of visual analysis supervised machine learning and natural language processing and infers the semantic groupings present in the page and their semantic roles.
it then asserts whether the page s markup matches the inferred semantics.
we evaluated our approach on real world websites and assessed the accuracy of semantic inference as well as its ability to detect accessibility failures.
the results show on average an f measure of for inferring semantic groupings and an accessibility failures detection accuracy of .