flakeflagger predicting flakiness without rerunning tests abdulrahman alshammari1 christopher morris2 michael hilton2and jonathan bell3 1george mason university fairfax v a usa 2carnegie mellon university pittsburgh pa usa 3northeastern university boston ma usa aalsha2 gmu.edu christom andrew.cmu.edu mhilton cmu.edu j.bell northeastern.edu abstract when developers make changes to their code they typically run regression tests to detect if their recent changes re introduce any bugs.
however many tests are flaky and their outcomes can change non deterministically failing without apparent cause.
flaky tests are a significant nuisance in the development process since they make it more difficult for developers to trust the outcome of their tests and hence it is important to know which tests are flaky.
the traditional approach to identify flaky tests is to rerun them multiple times if a test is observed both passing and failing on the same code it is definitely flaky.
we conducted a very large empirical study looking for flaky tests by rerunning the test suites of projects times each and found that even with this many reruns some previously identified flaky tests were still not detected.
we propose flakeflagger a novel approach that collects a set of features describing the behavior of each test and then predicts tests that are likely to be flaky based on similar behavioral features.
we found that flakeflagger correctly labeled as flaky at least as many tests as a state of the art flaky test classifier but that flakeflagger reported far fewer false positives.
this lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes.
evaluated on our dataset of projects with flaky tests flakeflagger outperformed the prior approach by f1 score on projects and tied on projects.
our results indicate that this approach can be effective for identifying likely flaky tests prior to running time consuming flaky test detectors.
i. i ntroduction regression testing is widely used in quality assurance to determine if recent changes to a codebase re introduce errors.
when a test fails developers typically expect that this failure represents a bug recently introduced.
however a growing and concerning trend is regression tests failures which are not in fact due to recent changes but are instead flaky tests.
flaky tests are non deterministic tests which pass and fail when run on the exact same version of a codebase .
when tests alternate between passing and failing without any code changes developers can be frustrated flaky tests are challenging to debug and a single failing test can halt release cycles.
recent academic and industrial studies have worked towards defining and reporting flaky tests automatically detecting flaky tests and investigating the occurrence of flakiness .
in fact flaky tests appear in most large scale projects.
google has reported that .
of test failures are caused by flaky tests and microsoft s windows and dynamics products report a similar .
flaky test detection is crucial because flakiness reduces tests reliability and frustrates developers.
when flaky tests only fail occasionally these tests can be very hard to reproduce and developers then spend a significant amount of time and resources trying to fix these flaky failures.
because non deterministic failures are not necessarily a result of a regression in the code the very nature of flakiness means developers have a very difficult time locating the source of the flakiness.
hence automatically determining if a test is flaky or not has become a significant topic in recent software testing research .
existing techniques for detecting flaky tests rely on rerunning tests if we can witness two different outcomes passing and failing from the same test on the same version of the codebase then surely that test is flaky.
even state of theart techniques like nondex and idflakies that inject additional non determinism still rely on re running each test case many times.
however rerunning tests can be quite expensive.
most prior academic studies that detected which tests were flaky have re executed test suites a handful of times times times or times .
developers have reported re running suspected flaky tests up to times to increase the likelihood of determining if a test is flaky .
in this paper we describe an empirical study of flaky tests that we conducted by rerunning tests times each which can provide guidance to developers and other researchers on how many times to re run tests to detect flakiness.
while rerunning tests times may be impractical for developers to perform regularly or even for researchers to perform regularly to gather datasets we used the result of this experiment to construct a rich dataset of flaky tests.
this dataset has many uses for instance to study how developers can better identify repair and prevent flaky tests.
a promising alternative approach to detect flaky tests is to create a machine learning classifier that can distinguish between flaky and non flaky tests .
in such an approach developers train a classifier using a known corpus of flaky and non flaky tests and then apply that classifier to a new codebase in order to detect new flaky tests.
alternatively rather than re run alltests in a corpus thousands of times in order to find flaky tests developers or researchers could use ieee acm 43rd international conference on software engineering icse .
ieee this classifier to determine which tests are more likely to be flaky and focus computational resources on re running those tests first.
unfortunately we found that existing approaches are not suitable to this sort of use case in our evaluation a state of the art flaky test classifier had promising recall but extremely low precision .
ideally the features used by the classifier should be applicable to a broad range of projects so that a model built on one set of projects with known flaky tests could be applied to a new project to find new flaky tests.
a recent survey has found factors that increase decrease and otherwise affect the ability to identify flakiness in tests .
these factors include features such as the presence of test smells hard coded values the age of test cases and more.
inspired by this work we collect various features like these for each of the tests in our flaky test dataset and created flakeflagger a tool to automatically predict if a test is flaky or not using these features.
we evaluated flakeflagger on our large dataset of flaky tests and found it significantly more effective than the stateof the art flaky test classifier on the projects evaluated flakeflagger outperformed the prior approach in and underperformed in only projects where two of them have only three flaky tests in total.
the four remaining projects have f1 score in the flakeflagger and the state of the art flaky test classifier evaluation.
section iv presents a complete per project evaluation.
this paper makes the following main contributions dataset a new dataset of flaky tests created by exhaustively executing project s test suites times each orders of magnitude more than prior work .
we show that for many flaky tests it is necessary to rerun them very many times to detect them.
new approach and tool flakeflagger a new hybrid static dynamic approach to collect behavioral features of tests and then predict whether tests are flaky or not.
flakeflagger can be used to guide other flaky test detection tools towards those most likely to be flaky.
evaluation we show that flakeflagger can predict whether a test is flaky or not with far fewer false positives than prior work just false positives compared to from prior work .
we show that test execution time test coverage of source code test coverage of changed lines and usage of third party libraries are effective predictors of flakiness.
we do not find any tokens that commonly exist in flaky tests and we do not find test smells to be a strong indicator of flakiness.
our dataset test running infrastructure and flakeflagger itself are publicly released under a bsd license and can be found on github and in our permanently archived artifact .
ii.
e mpirical study the traditional approach to find flaky tests is to rerun each test many times if a test outcome changes without any change to the code then the test is flaky.
this definition captures both tests that typically pass but infrequently fail and those that typically fail but infrequently pass .
however there islittle guidance from industry or academia in terms of how many times to rerun each test most prior work has considered rerunning times or in limited cases times .
but if each flaky test is flaky for some non deterministic reason what confidence do we have that we will observe its outcome change within those reruns?
if it is feasible to find most flaky tests after only rerunning them a handful of times then perhaps this flaky test rerun problem is not such a big problem rerunning tests only times does take time but it certainly takes less time than rerunning them times.
since we don t know the underlying source of non determinism that causes the flaky test to fail aseparate problem is how much harder is it for a developer to reproduce that flaky failure in a different environment where various uncontrolled factors might vary e.g.
os java version cpu available memory network speed etc.
.
this problem is particularly relevant for developers who are tasked with debugging a flaky test on their local machines which failed on a build server.
to motivate our work in detecting flaky tests we conducted an empirical study investigating these questions mq1 how many flaky tests can be found by rerunning tests given different rerun budgets?
mq2 how hard is it to reproduce a flaky test failure?
these questions are important not only for this work but for any researcher interested in developing new techniques to help developers detect and debug flaky tests.
a. study design to answer these questions we selected projects that have been previously studied in the context of flaky tests .
in comparison the dataset of flaky tests produced by bell et al.
s deflaker was constructed by running tests once on a revision of a project then only rerunning tests that were observed to fail up to times .
lam et al.
s idflakies dataset limited the number of reruns per project to take no more than hours per project resulting in typically under reruns per project and often did not run all tests in every project .
while lam et al.
have also conducted evaluations with more test re runs these evaluations were targeted to only portions of test suites with known flaky tests .
while this approach can reduce the overall amount of computation time needed for an experiment lam et al.
report this experiment consumed days of computation time focusing on rerunning only known flaky tests can bias a dataset to only include those flaky tests that had previously been found through fewer reruns.
instead we ran each project s entire test suite times using over years of computation time in total.
we executed this experiment by creating a queue of jobs where each job represented a single execution of a project s test suite running mvn install .
after running each job we archived all log files and then removed all temporary files and rebooted the machine and then proceeded to run the next test suite in the queue.
this approach was designed to provide reasonable isolation between test runs and simulates how a real build server might compile and test a project.
1573table i flaky tests detected by re running test suites times.
we estimate the percentage of all flaky tests that would be detected if only or reruns had been performed.
color bars are stacked bar charts showing the percentage of tests that failed with a given frequency.
columns deflaker andidflakies show the number of non order dependent flaky tests found in total by those prior works and the number of flaky tests shared by both datasets.
blank cells indicate that a different revision of the project was used due to historical compilation issues.
complete data and results available in our artifact .
flaky by deflaker idflakies of all flaky tests detectable at distribution of failure frequencies as of tests failing project tests reruns shared total shared total reruns reruns reruns runs of spring boot hbase alluxio okhttp ambari hector activiti java websocket wildfly httpcore logback incubator dubbo http request wro4j orbit undertow achilles elastic job lite zxing assertj core commons exec handlebars.java ninja jimfs no flaky tests observed total we considered approaches to increase the likelihood of flaky test detection.
for instance some tests might be flaky because they assume that a standard library method provides deterministic behavior even when it doesn t e.g.
an iterator over a set which might return objects in the same order each time or might not .
other flaky tests may be order dependent which means that when they run in a different order than is expected they can fail be flaky .
rather than simply re run such tests times one could rerun them in a different order each time thereby increasing the likelihood of observing a flaky test .
however there are far more root causes of flaky tests than only these two and we did not want to bias our dataset to include more flaky tests of any single category than another.
for instance in their survey of developer reported flaky tests luo et al.
found that test order dependencies accounted for only of the flaky tests that developers reported in issue trackers .
instead our dataset represents the flaky tests that would be observed in the course of normal development when tests are run on a build server with no artificial non determinism.
we ran these experiments on ubuntu .
vms running openjdk .
.
with cpus and 8gb of ram.
note that perhaps we could also find more flaky tests by using a variety of platforms and computing devices using different versions of java and different operating systems might introduce more non determinism overall finding more flaky tests.
however again our goal in this study is to find the flaky tests that a developer might observe in the normal course of developmentwhen running their test suite on a standard continuous integration build server and notto inject artificial noise.
hence each execution of each test occurs using a standard platform.
b. study results by running each test times we observed flaky tests about .
of the total number of tests were flaky.
the number of observed flaky tests varies from one project to another.
we found projects with less than flaky tests of which have only one and one with none.
on the other hand there are projects which have more than flaky tests.
spring boot has flaky tests of the total observed flaky tests.
table i summarizes these results.
mq1 flaky failures by number of reruns .
while it is not possible to truly state what the probability is of a flaky test failing since we don t know the hidden uncontrolled condition that causes the failure we estimate the probability that each flaky test would have been detected with fewer reruns by assuming that each test failure occurs independently.
overall only roughly a quarter of all of the flaky tests that we found in runs would have been found with reruns roughly half with reruns and roughly two thirds with reruns.
we also visualize this distribution in table i categorizing each flaky test as failing either between and times between and times between and times and finally over times out of the runs .
these sets are represented by colors as shown in table i. the redbar which refers to tests that flake less than or equal project testsrun testsfeatures collection phaselist of featuresanalyze codesource of flaky testslabeledtestsfeature outputs for each testprediction phasesupervised learningdatasplit processingdatainspectionprediction strategychecktpfpfntnprediction resultconfusion matrixof testsfig.
overview of flakeflagger s approach to predict likely flaky tests given a set of known flaky tests.
times takes the majority in projects.
in general we found more than of total flaky tests fail in less than or equal times of flaky tests fail more than and less than or equal out of .
this suggests that researchers building flaky test datasets should consider as many reruns as possible in order to detect flaky tests the chance of observing that a test is flaky might be quite slim indeed.
furthermore we acknowledge that even after re runs it is still possible that we have not detected every flaky test in this dataset.
however this threat is present in anyflaky test dataset and we believe that our empirical design is sufficient for our needs.
mq2 reproducing flaky tests failures.
while mq1 provides insight into the difficulty of identifying flaky tests by re running them on the same platform this does not quite capture the challenge that a developer would face when reproducing those flaky tests in a different environment e.g.
their local machine .
to simulate this activity we compared the set of flaky tests identified from our reruns with those detected by prior researchers on the same versions of the same projects but in different environments.
we specifically chose the projects and revisions of the projects that we studied in order to align with revisions of projects studied by either deflaker or idflakies .
the columns deflaker and idflakies in table i show the total number of flaky tests that that paper reported on that version of that project along with the number of those tests that were also found to be flaky based on our reruns.
a blank entry indicates that the revision of that project that we executed did not have any flaky tests reported by the prior work.
the deflaker dataset contains flaky tests from many revisions of each project but we only studied a single revision of each project and hence it is possible that deflaker had not identified any flaky tests in that revision.
the idflakies dataset consists of both order dependent tests which are detected by shuffling execution orders and nonorder dependent tests which are flaky regardless of execution order .
since we purposefully did not shuffle the execution order of the tests as described above we include only the non order dependent tests from idflakies for comparison.
comparing to deflaker we found flaky tests out of the tests identified as flaky by deflaker.
unfortunately the deflaker authors did not retain the build logs from their test runs so we are unable to diagnose why those tests appeared as flaky to deflaker but not to our reruns.
comparing to idflakies we found flaky tests out of the non order dependent tests that we reran.
in the case of idflakies the authors didretain the build logs that show how these testsfailed and we confirmed by hand that the tests that we missed in our rerun experiment truly were flaky and could have been detected as flaky if we had rerun them more.
these results are indicative of the true non determinism of flaky tests and the difficulties that developers face reproducing them even with reruns we could not detect all flaky tests.
study summary the results from this study demonstrate that flaky tests are extremely difficult to detect.
if developers would like to try to find all tests that are flaky in their test suite they likely should consider re running those tests thousands of times only half of the flaky tests that we found failed more than times out of the runs.
moreover we know that even running tests times will still not guarantee that all flaky tests have been found since we did not succeed in reproducing many flaky test failures observed in prior work.
this study underscores the need for approaches that detect flaky tests without rerunning them.
iii.
a pproach our primary goal with flakeflagger is to create a new approach to proactively identify which tests in a test suite are flaky before they become a nuisance and without rerunning them many times.
for instance when a new test is introduced developers might use flakeflagger to identify if that new test is likely to become flaky since research has shown that most flaky tests are flaky when they are introduced .
alternatively developers might use flakeflagger to aid in a search for flaky tests in a large test suite where developers identify that a portion of the test suite is or is not flaky and use flakeflagger to help label the rest of the tests as flaky or not.
researchers constructing new approaches to detect and repair flaky tests need large datasets of flaky tests and flakeflagger is perfectly suited to help find these flaky tests faster.
instead of re running every test in our dataset times if we already had trained flakeflagger on a subset of the tests or projects we could have greatly reduced the execution time needed to rerun these tests by only re running those reported by flakeflagger as likely to be flaky.
note that flakeflagger is orthogonal to techniques that mutate the execution environment in order to increase the likelihood of a test appearing flaky these tools are still expensive to run and flakeflagger can help focus computing time on the tests most likely to be flaky.
even if flakeflagger presents a false positive i.e.
a test flakeflagger declares to be flaky even though it is not our approach can help developers reduce 1575table ii complete list of features captured for test flakiness prediction.
the covered lines churn feature is represented in multiple forms based on the hvalues number of the past commits .
in our evaluation we considered h 500and feature descriptiontest smellsindirect testing true if the test interacts with the object under test via an intermediary eager testing true if the test exercises more than one method of the tested object test run war true if the test allocates a file or resource which might be used by other tests conditional logic true if the test has a conditional if statement within the test method body fire and forget true if the test launches background threads or tasks.
mystery guest true if the test accesses external resources assertion roulette true if the test has multiple assertions resources optimism true if the test accesses external resources without checking their availability numeric featurestest lines of code number of lines of code in the test method body number of assertions number of assertions checked by the test execution time running time for the test execution source covered lines number of lines covered by each test counting only production code covered lines total number of lines of code covered by the test source covered classes total number of production classes covered by each test external libraries number of external libraries used by the test covered lines churn h index capturing churn of covered lines in past and commits.
each value hindicates that at least hlines were modified at least htimes in that period.
the candidate number of tests they need to investigate via rerunning or manual inspection.
by proactively identifying flaky tests we may also help developers understand why these tests are flaky.
prior work has suggested different properties of tests that might make them more likely to be flaky and flakeflagger can report which of these features are present in each test .
in practice if a feature has a strong correlation with flakiness developers might choose to focus on this feature in their future test maintenance and development activities.
figure shows a high level overview of our approach to detect flaky tests.
first we collect a series of features describing each test in a project.
in a developer s scenario we would assume that some of these tests would be known to be flaky and the developers would be interested in detecting other flaky tests.
for instance the developer might know the flaky tests that exist in their test suite currently and would like to identify if a newly written test is flaky.
we collect behavioral features such as api usage of each test and then construct a classifier to predict which tests are flaky.
in a controlled experiment with known flaky tests we can perform crossvalidation and generate a confusion matrix that describes the performance of the classifier.
in practice without an oracle we would present a report to developers including a list of likely flaky tests.
a. prediction features to develop a list of features that may be predictive of flakiness we look to prior flaky test research .
ahmed et al.
categorized developer reported factors which affect test flakiness.
these features are described by practitioners at a high level and include test case complexity hard coded values and test smells.
eck et al.
interviewed developers about flaky tests and tabulated the frequency of different kinds of flaky tests as well as developers fixes for those flaky tests.
whereas prior flakiness classification approaches used static code level features e.g.
presence of textual tokens in the body of each test method these surveys describe features that are more nuanced.
for instance eck etal.
note that many flaky tests are flaky due to causes not in the test method itself but instead in the production code that is executed by that test .
inspired by previous studies on test flakiness we developed a list of sixteen features some of are based on general studies on the causes of flaky tests while others are defined as bad practices in writing unit tests .
unfortunately some of these flaky test root cases are too complicated to detect without human intervention for instance eck et al s too restrictive range which effectively describes the case where an assertion is wrong .
hence we considered all of the features described in the prior works and then selected only those for which we could write automated detectors.
we implemented detectors for each of the features shown in table ii.
while some of the features can be detected by inspecting the test method statically specifically the conditional logic smell and test line of code the rest of the features require more than static analysis.
for instance existing automated test smell detectors have analyzed only the code that is present in a test method and hence can fail to label tests that are smelly because they invoke a helper method which in turn performs smelly behavior.
for instance the fire and forget smell exists in tests that spawn background threads or tasks a smell detector that considered only direct calls to launch threads in the test method body would not define a test as smelly if it called a helper method to launch that background thread.
since our goal is not to precisely detect test smells as identified by humans but rather to find features that may be representative of flaky tests we decided to expand our definition of many of these smells to be inclusive of all code executed by a test rather than just the code contained in the test method body itself.
we developed a hybrid static dynamic framework to collect the statement coverage of each test and then statically analyze the covered code in order to collect these behavioral features.
for instance we determined that a test had the fire and forget smell feature if anytime during its execution including in the production code that is called by the test that test launched a background thread or task.
we also collect a variety 1576of other features related to the statement coverage of each test such as how many recently changed lines of code are covered.
we implemented this framework as an extension to the maven build system allowing for a zero configuration feature collection process that automatically parses the target project s build scripts to modify them to collect coverage and report features.
due to space limitations we omit additional details on precisely how each feature is detected and make our entire implementation publicly available .
while we have built flakeflagger to analyze java code using the maven build system our approach is sufficiently general to be applied to other build systems or languages.
this list of features is not intended to be complete there may yet be other features that can be easily collected and will be useful for predicting test flakiness.
however we empirically found that this set of features yielded good prediction performance for flakeflagger section iv .
again by making our dataset and infrastructure publicly available we enable future research on behavioral features of flaky tests.
b. flaky tests classifier for our classifier each test instance iis represented as a vectorfx1 x2 x3 x ngwhere each xrepresents a feature value and ncorrespond the total number of features.
to avoid inaccuracies in training we perform an automated data inspection and cleaning process as follows instances with missing values e.g.
missing features can have a negative impact on model performance .
since we use different approaches to collect different features it is possible for some features to be missing for some tests.
for instance if a test crashes in the middle of its execution e.g.
with an unrecoverable out of memory exception we will be unable to collect telemetry from that test and hence unable to calculate several of our features.
some tests are not written in java and hence the feature detectors may not be applicable to them and due to inheritance some tests may not have source code in the project under test with the test residing in a 3rd party library .
we found this occurred fairly infrequently roughly of the tests the difference between tables i and iii and handle tests with at least one missing value by excluding them from our experiment future work could support such tests .
representing the range of numeric values of each feature is also very important.
some features have discrete values for instance each smell related feature has a boolean value smelly or not .
however there are also features with continuous values shown in table ii which do not have a maximum value and may need to be discretized .
from a model learning perspective discretization is crucial because discretized features have a better chance of correlating with the class value as the range of values are limited which helps features to be interpreted .
we considered two main approaches to discretization.
first we considered values as they are without applying any discretization techniques.
we consider this option because we expect to not have long ranges of continuous data for most features.
the second approach weconsidered is to divide continuous data into a fixed number of intervals called bins which is a common techniques to discretize continuous data .
our approach is agnostic to the classification algorithm that we use to build the model.
hence rather than build a single model we designed flakeflagger to construct a set of models based on seven different supervised learning algorithms decisions tree dt random forest rf support vector machine svm multilayer perceptron mlp naive bayes nb adaboosting ada and k nearest neighbor knn using the scikit learn package .
this selection of models builds on flaky test classification prior work that considered dt rf svm knn and nb models only which found that rf performed best .
in our evaluation described in the following section we also found that the random forest had the best performance and report results only for this model but make all models available in our artifact .
we perform a feature selection process using information gain which computes the amount of information that a feature can provide for a classification .
the range of information gain is between and where higher values indicate more predictive power.
imbalanced datasets where there is not an equal number of instances in each class flaky and nonflaky tests in our case usually have very low information gain values.
we compute the information gain for each feature and include only features with an information gain of at least following previous work .
iv.
e valuation to evaluate our classifier we designed experiments to answer the following research questions rq1 how effective is flakeflagger at predicting flaky tests?
rq2 how helpful is each feature in distinguishing between flaky and non flaky tests?
a. experimental design to evaluate flakeflagger we used the extensive dataset described in section ii as a source of flaky tests.
to collect the various features needed to predict flakiness we re executed each test one more time using the feature extractor using the same environment.
machine learning classifiers rely on two data sets one to build the model training and another for testing.
we apply k fold cross validation to evaluate our model.
however k fold cross validation is most applicable to data that is evenly balanced where the proportions of each class in our case flaky and not flaky are similar.
balanced datasets reduce the risk of any of the kfolds having only a single or no instances of one of the classes flaky and not flaky .
since most tests are not flaky we have imbalanced data and hence have designed flakeflagger to use two data sampling techniques smote and random undersampling which we also compare to a baseline without sampling.
we perform this sampling only when training the model and not when testing it to ensure a valid and fair result.
we train and test our models considering all of the tests from all of the projects in a single bucket randomly shuffling which tests 1577from which project appear in each fold.
to evaluate the effect that the size of the testing and training dataset has on the classification results we consider both and testing data sizes.
following past work we consider only features with an information gain of at least .
and report the information gain of each feature.
in our prediction evaluation we label each prediction result as a true positive tp false negative fn false positive fp or true negative tn as follows tp predicted flaky known to be flaky fp predicted flaky not known to be flaky fn predicted not flaky known to be flaky tn predicted not flaky not known to be flaky.
we also evaluate our models using f1 score which is computed using the standard formula based on recall and precision.
lastly we calculate the area under the curve auc a measure of how effective a model is at distinguishing classes in our case flaky and not flaky .
note that in our evaluation false positives represent the number of tests that might erroneously be considered as flaky by developers resulting in excess effort spent re running them to determine if they are flaky or not.
we focus primarily on total positives because we have confidence that the collected flaky tests are indeed flaky but we cannot be confident in our classification of a test as not flaky.
in other words the oracle we use is a result of detecting flaky tests after runs for each test but this does not guarantee that the not flaky tests are really not flaky they may just not have been observed to be flaky.
this approach also allows us to confirm fns are truly flaky tests because they fail at least once during rerun tests.
however because of the inherent non determinism in flaky tests we cannot construct a reliable oracle to evaluate tns and fps but report them as is.
b. rq1 efficacy of our classifier following the evaluation procedure described in the previous section we trained and tested flakeflagger.
we created several models consider approaches to discretization sizes of training sets data balancing approaches and different classification algorithms.
due to space limitations we show only the results of the single best model configuration a random forest model built using the smote technique for balancing the training data and using unbalanced testing data no discretization and a training testing split our artifact includes all results .
we found that smote worked better than other sampling techniques due to the relatively small ratio of flaky tests to the total number of tests.
random forest was far more effective than the other algorithms but we did not find a meaningful difference in performance between a and testing data size.
in order to evaluate the performance of our flaky test classifier we compared its prediction results to the stateof the art flaky test classifier a vocabulary based approach proposed by pinto et al.
.
this approach extract tokens from each test using a simple bag of words model under the theory that flaky tests may use similar apis keywords etc.
and hence tests can be classified as flaky or not based on the presence of various tokens the model also uses onenon token feature the length of the test method .
we also considered a hybrid model that adds the token features to flakeflagger s features.
note that our evaluation methodology differs somewhat from the prior approach in which the authors trained and tested their model on a balanced flaky non flaky dataset with the exact same number of flaky and non flaky tests we consider a more realistic scenario where the model is trained on a balanced flaky non flaky dataset but tested on the complete set of tests which is not balanced .
from a methodological perspective balancing the number of flaky and non flaky tests by ignoring tests from the majority non flaky class may result in over approximating the performance of the model if this balancing is done during the testing phase.
this is a particularly important distinction on projects with very large numbers of tests such as incubator dubbo with tests of which only were flaky our cross validation approach would test the model on a all of the tests whereas the methodology used in would test the model on a much smaller set of non flaky tests.
in our evaluation we use stratified cross validation.
table iii presents the results of this experiment showing the true positives false negatives false positives true negatives precision recall and f1 score for each approach broken down by project.
even though the models are not trained and tested by project i.e.
the project name is not a feature in the model after running the evaluation we mapped each test back to its project to allow us to inspect how the models performed on each project.
we show the aggregate precision recall f1 score and auc.
flakeflagger is a learning based approach and hence we expect that it might perform better on projects that have many flaky tests providing more training data that could help the classifier to better predict flaky tests in that project than those with very few.
to avoid biasing our dataset towards projects with the most flaky tests we did not impose a threshold for a minimum number of flaky tests to include the project in our evaluation including all projects with at lest one flaky test.
we do not include the tests for which we were unable to collect all features typically due to the tests having non java components .
overall flakeflagger and the vocabulary based approach both detected a very similar number of flaky tests and respectively out of a total of flaky tests but the two approaches varied dramatically in terms of precision flakeflagger had a far lower false positive rate with just compared to false positives from the vocabularybased approach.
we found that combining flakeflagger s features with the tokens from the vocabulary based approach yielded slightly improved performance.
the following section explores which features from the vocabulary based approach contributed to this improvement.
considering our initial use case of a researcher or developer using flakeflagger to determine which tests to run timeintensive flaky test detectors on using either flakeflagger or the vocabulary based approach would result in roughly the same number of flaky tests eventually detected that is both have comparable recall .
however the total amount of time 1578table iii prediction performance for flakeflagger the vocabulary based approach and the hybrid combination of both.
the hybrid approach builds a model with both flakeflagger s and the vocabulary based approach s features.
we show the number of true positives false negatives false positives and true negatives precision recall and f1 scores per project.
the auc value is calculated after each fold where the reported value is the overall averages of auc values after all folds.
projects with zero f1 values have very low numbers of flaky tests less than per project and illustrate known limitations of flakeflagger.
flak y by flak eflagger vocabulary based approach combined approach project tests reruns tpfn fp tn pr r f tpfn fp tn pr r f tpfn fp tn pr r f spring boot hbase alluxio okhttp ambari hector activiti java websocket wildfly httpcore logback incubator dubbo http request wro4j orbit underto w achilles elastic job lite zxing assertj core commons e xec handlebars.ja va ninja total auc average per fold needed to run such an experiment would vary dramatically between the two models since flakeflagger had far fewer false positives vs .
assuming that each test would take a comparable amount of time to run flaky test detectors on our developer or researcher would be able to confirm the flakiness of flakeflagger s reported flaky tests tps plus fps in roughly of the time that it would take to confirm the flakiness of the reported flaky by the vocabulary based approach tps plus fps .
we were initially surprised that the precision of the vocabulary based approach s was so much lower than flakeflagger s and indeed lower than reported by the original authors .
we found that the tokens used in pinto et al.
s bag of words model did indeed frequently occur in flaky tests but also occurred quite frequently in non flaky tests.
for example one of the most relevant tokens that the model relied upon both in our study and in was the java throws keyword.
however when examining the entire corpus we discovered that this keyword is used quite frequently in both flaky and non flaky tests and hence is not a very good predictor of flakiness.
flakeflagger s performance varied across projects on some projects e.g.
alluxio we had perfect precision and recall while on others e.g.
okhttp and activiti the approach was less successful.
we investigated more closely the different factors that could cause such a varied performance among different projects.
the first and most obvious factor is the size of the training data our model performed best on the two projectswhich had the most known flaky tests alluxio and spring boot each had more than .
on projects with very few known flaky tests less than flakeflagger did not classify any of the flaky tests as flaky resulting in f1 scores of .
this results from the lack of training data that are representative of the flaky tests in these projects.
however note that even on these projects with so few flaky tests e.g.
zxing with only two known flaky tests ninja with only one even though flakeflagger failed to identify the flaky tests true positives it had far fewer false positives than the other approach.
more broadly speaking we can attribute the variation of prediction performance between projects to the relative generality of our features such as test execution time coverage of recently changed lines etc.
.
each project has its own environmental assumptions development patterns and other unique characteristics that can make it difficult to create a single general purpose approach to classifying tests as flaky or not.
another explanation for why performance varies across projects may be that not all flaky tests have been labeled correctly no rerun based technique can guarantee to find all flaky tests even after reruns .
that is there may be tests that are labeled as not flaky in our dataset that are in fact flaky but we simply did not observe any flaky failure of those tests in our experiments.
the higher number of observed flaky tests in a single project does not guarantee that flakeflagger performs well.
some flaky failures are due to rare dependency conflicts and network failures that are not captured well from our features described 1579table iv list of top features by information gain ig for flakeflagger and the vocabulary based approach.
our models only include features with ig .
vocabulary based features flakeflagger features feature token ig feature ig test lines of code .
execution time .
throws .
source covered lines .
should .
source covered classes .
exception .
covered lines .
mtfs .
covered changes past commits .
runbuildfortask .
covered changes past commits .
tfs .
covered changes past commits .
run .
covered changes past commits .
transitive .
test lines of code .
ioexception .
covered changes past commits .
tachyon .
covered changes past commits .
fileid .
covered changes past commits .
if .
external libraries .
actual .
covered changes past commits .
someinfo .
fire and forget .
testutils .
number of assertions .
writetype .
resources optimism .
some .
mystery guest .
checkspring .
assertion roulette .
testfile .
conditional logic .
createbytefile .
indirect testing .
family .
test run war .
checkcommonslogging .
eager testing .
in table ii.
for example we notice that okhttp has a high number of false positives and false negatives.
with a further inspection on this particular project we found that a group of tests had all failed in the same way due to the same dependency problem in one single run.
however only some tests that used this dependency presented a flaky failure to us and hence we observed many false positives from our model with other not flaky tests that use this same dependency classified to be flaky.
in this same project our model also showed a large number of false negatives tests that are flaky but predicted as not flaky .
upon further investigation we found that these tests were flaky due to transient networkrelated failures that occurred when they were run.
while our model does consider usage of network apis via the mystery guest smell we found that most tests that used network apis were not labeled as flaky in our dataset because we did not witness a network failure in the runs of those tests.
as a result the few tests that didfail due to network failures were typically labeled as not flaky by the model resulting in false negatives.
we discuss these limitations further in section v. c. rq2 how useful is each feature?
to gain more insight into which textual and behavioral features are correlated with test flakiness we report the information gain of each feature in flakeflagger s model and the top features in the model built using the vocabularybased approach.
table iv lists all of flakeflagger s features sorted by information gain along with the top features used by the comparison system based on the vocabulary based approach .
note that we used only features with an information gain of at least 01in our models but we include all top features in this table.
overall we found that features that considered dynamic behavior from each test e.g.
executiontime covered lines and coverage of recently changed lines had a far greater information gain than the tokens that were statically extracted from the test method bodies.
none of the test smells that we collected had a strong information gain which may indicate that test smells are not well correlated with test flakiness all of the smells had an information gain less than our threshold of and in fact eager testing had an information gain of less than .
we found that the top eight flakeflagger features each had a higher information gain than the highest gain vocabulary feature.
the most effective features execution time various forms of coverage size of test and usage of external apis are all measures of test case complexity.
this confirms industry reports that correlate test size with test flakiness .
for instance it is perhaps likely that small unit tests are less likely to be flaky than large integration tests and this behavior is captured by both the execution time of each test and also by the coverage of each test.
due to varying development styles other behavioral features likely have high variance between projects without signaling flakiness .
for instance number of assertions likely varies more between projects because of coding conventions more than it varies between flaky and non flaky tests.
future work might consider ways to normalize these features by project to account for such variance and we make all of our tools and data available to allow other researchers to join in this effort.
in the model built using the vocabulary based approach the features with the highest information gain were test lines of code presence of the throws java keyword and several tokens like should exception and mtfs each with an information gain significantly lower than the top features in flakeflagger s model.
we believe that these tokens might be over fitting to specific patterns in some of the flaky tests in our dataset and indeed these top three tokens differ from those reported by pinto et al.
in their initial work .
table v directly compares the frequency of those three tokens in flaky and other non flaky tests reported by the vocabulary based approach and also their frequencies in the dataset that we used in this study.
while the prior study found that these tokens occurred far more frequently in flaky tests than in nonflaky tests when looking at our corpus we found that these tokens occurred far more frequently in non flaky tests than in flaky tests.
the majority of the flaky tests in the prior study with the job token came from a single project oozie which we did not study in our evaluation.
at the same time the majority of non flaky tests with the token job in our dataset were in the project elastic job lite which was not included in the prior evaluation.
clearly the co occurrence of individual tokens with flaky tests can vary dramatically between projects.
terms that correlate with flakiness in one project can not be expected to correlate with flakiness in other projects this is also evident from the limited number of projects which contain each token.
note that this finding only underscores the need for a large balanced dataset of flaky tests like the one we collected and 1580table v comparison showing the predictive power of the top tokens reported by the vocabulary based approach evaluation on the deflaker dataset compared to this evaluation on our dataset.
for each token we show the information gain ig the number of flaky tests containing that token number of projects containing those tests in parentheses and the number of other non flaky tests containing that token with the number of projects containing those tests .
vocabulary based evaluation this evaluation token ig flaky not flaky ig flaky not flaky job table id described in section ii the deflaker dataset that pinto et al.
used contained more flaky tests than our dataset vs .
however a single project in that dataset oozie contributed more than half of those flaky tests which can make it extremely difficult to draw conclusions that can generalize beyond a single project or beyond the dataset.
we look forward to contribute to new community efforts to build shared flaky test datasets making all of the log files from each of the executions of each test suite in our study publicly available in our artifact .
v. d iscussion and threats to validity while we are confident in our findings it is nonetheless worthwhile to acknowledge various threats to the validity of our conclusions.
for instance the list of projects in our prediction experiment may not be representative of all projects limiting the generalizability of our conclusions.
we reduce this threat by selecting different projects from different domains.
further our project list is based on those studied by others.
rq1 showed that our classifier was able to predict out of flaky tests.
however note that our ground truth is imperfect some of the false positives may in fact be true positives where the failure is in the rerun process to identify that the test is flaky.
unfortunately it may never be possible to provably identify all flaky tests in a project.
as we witnessed in our empirical study section ii the environment in which a test runs may affect if the test appears flaky.
since on the same revisions of the same projects we found a different set of flaky tests than were found by the idflakies and deflaker authors even after re running those tests far more times than the original authors we recognize this as a limitation.
it may be possible to find more flaky tests by injecting artificial non determinism into the execution of each test for instance arbitrarily slowing the execution of each test or shuffling test orders.
these other approaches may find more flaky tests of one particular type for instance reordering tests will expose more order dependent flaky tests perturbing timing may expose more concurrency related flaky tests.
however a significant concern with any such approach is that it will result in a dataset that is overly representative of those specific sources of flakiness.
for example luo et al.
s study found thatjust of flaky tests were caused by order dependencies an approach that built a dataset of primarily tests flaky due to this cause would likely not represent all flaky tests.
similarly we observed that some but far from most tests that made calls through the network could fail due to flakiness in the event of a network failure.
hence one approach to find more flaky tests could be to inject network failures while tests run.
however we speculate that the result of this experiment would likely be that alltests that make network calls will be labeled as flaky.
such an approach would likely not be very useful for developers who presumably are already aware that tests that rely on network resources might fail if the network is unreliable.
this mirrors the sentiment expressed by harman and o hearn suggesting that based on facebook s experiences we should assume that all tests are flaky and google where all tests that execute in more than a single thread are assumed to be potentially flaky .
hence we remain confident that our rerun based flaky test detection approach represents the best possible approach to build our dataset one that includes quite literally the flaky tests that developers might have observed in the course of normal development without injecting additional non determinism.
we considered multiple supervised learning algorithms and found that random forest resulted in the best performance on our dataset.
this result may not hold on other datasets.
similarly we found that some features like execution time and coverage of recently changed lines are more predictive of flakiness than others like test smells and these findings may not hold on other datasets.
we include all results and scripts in our artifact and encourage other researchers to experiment with different configurations of flakeflagger .
our evaluation of flakeflagger presents a direct comparison to a state of the art flaky test prediction approach but does not include a direct comparison with other flaky test detection tools like deflaker idflakies and nondex .
these other approaches rely on repeatedly re running a test perhaps modifying the environment that the test runs in in order to detect if that test is flaky.
section ii describes our empirical study to identify the prevalence of infrequentlyfailing tests and found that hundreds of flaky tests could only be detected after an enormous number of reruns or using a prediction model like flakeflagger.
future work might combine these two approaches using flakeflagger to prioritize which tests to apply these rerun based flaky test detectors to.
to increase our confidence in our implementation we implemented our classifier using the supervised learning algorithms provided by scikit learn .
this well regarded library provides the implementations of different techniques to balance data impute missing values and compute feature correlations.
nonetheless there may be bugs in the tooling or how we used it we make best efforts to check our results and will provide a replication package.
we implemented our own test smell detectors rather than using existing detectors.
this was largely due to the lack of a published experimentally evaluated test smell detector for java at time of writing.
future work might consider extending our test smells detector perhaps integrating 1581the recently released tsdetect smell detector .
even with a replication package some of our our results may be difficult to reproduce since we are purposefully experimenting on non deterministic systems.
in particular when rerunning tests in order to find flaky tests there is no guarantee that a flaky test will fail.
to aid future researchers we collected the build logs of each test execution and include these in our artifact .
vi.
r elated work several recent works have focused on the problem of detecting and managing flaky tests.
luo et al.
provided an empirical study of flaky tests by investigating commits logs of opensource projects .
they presented the most common causes of flaky tests and described possible strategies to fix flaky tests.
unlike our dataset of flaky tests which is constructed by rerunning tests luo et al.
s dataset is constructed by analyzing developer reports.
deflaker is an approach to detect flaky tests immediately after the test fails without rerunning it by monitoring the coverage of the latest code changes .
in contrast flakeflagger s detects flaky tests before they fail.
more similar to our approach idflakies aims to proactively detect flaky tests before they cause unexpected failures by rerunning tests in different orders .
this approach is particularly suited to detect order dependent flaky tests those which can fail deterministically modulo their execution order and reruns tests times.
in comparison flakeflagger aims to find flaky tests regardless of their underlying source of non determinism and in our empirical study to detect flaky tests we reran tests two orders of magnitude more times total than the idflakies work times .
lam et al.
later re ran tests from this dataset times in order to evaluate how difficult these failures were to reproduce finding a mean failure rate of .
.
order dependent flaky tests have received quite a bit of attention in general and several techniques have been proposed to detect them to isolate tests to avoid order dependencies and to repair tests to remove those dependencies .
similarly other specialized kinds of flaky tests like those that are flaky due to commonly incorrect assumptions of api behavior can be detected through specialized means .
lam et al.
studied real flaky test failures at microsoft and found that for the majority of them the failure that occurred on the build server could not be reproduced on a developer machine even after trying to rerun the test times .
this finding is similar to ours we only reproduced half of the flaky test failures that prior work found on the same projects even though we reran the tests times.
strandberg et al.
studied intermittently failing tests for embedded systems finding that in this domain flakiness was typically caused by environmental factors .
eck et al.
analyzed flaky tests by conducting a survey of developers to understand the root cause of flakiness .
this survey helped inspire our feature selection.
harman and o hearn emphasize the importance of flakiness on software testing suggesting that we assume that alltest are flaky and calling for tools and techniques to automaticallyassess the likelihood of a new test becoming flaky in the future.
flakeflagger answers this call directly by helping developers identify which tests are likely to be flaky based on the behavior of other tests.
several recent works have begun study the impact of flakiness on other software testing practices like mutation testing and program repair .
most similar to our approach are prior works that also aim to predict flakiness with the most relevant being bertolino et al.
s flast and pinto et al.
s flaky test vocabulary work .
unlike flakeflagger both flast and pinto et al.
s approach considers only the text in the test method body itself and does not consider any features of the code that is called by that test or code that might exist in setup or teardown methods.
in our evaluation we found that flakeflagger significantly outperformed vocabulary based approaches in predicting flaky tests.
more generally machine learning techniques have been adopted widely in software testing .
durelli et al.
showed that ml algorithms are commonly adopted to automate software testing tasks.
azeem et al.
proposed a systematic literature review on the ml techniques used in code smell detection.
nucci et al.
discussed possible limitations which may result in unexpected result in code smell detection by mainly analyzing fontana et al.
s work detecting code smells using ml.
we did not use ml to detect test smells but used traditional source code parsing to detect smells .
it would be interesting to compare the performance of our hybrid static dynamic test smell detectors with prior techniques using a human oracle to determine valid smells.
vii.
c onclusion flaky tests make testing unreliable because it is hard to say which test failures represent true regressions.
we presented a very large empirical study showing just how difficult it is to proactively detect flaky tests by rerunning them.
flakeflagger is a new approach for automatically classifying tests as flaky or not without reruns and improves on existing classifiers by collecting behavioral features from each test.
developers can use flakeflagger to determine if a newly added test is likely to be flaky or could use it to help classify existing tests.
developers and researchers alike can use flakeflagger to focus more precise and costly flaky test detection efforts on the tests most likely to be flaky.
compared to a state of theart flaky test classifier flakeflagger had similar recall vs but significantly better precision vs .
this improvement translates to a significant reduction in the number of misdiagnosed flaky tests dramatically reducing the number of possibly flaky tests that need to be further investigated.
viii.
d ata availability all tools and data produced for this paper are publicly and permanently archived at .