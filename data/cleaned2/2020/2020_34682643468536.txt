fair preprocessing towards understandingcompositional fairness ofdatatransformersinmachine learningpipeline sumon biswas dept.
of computerscience iowastateuniversity ames ia usa sumon iastate.eduhridesh rajan dept.
of computerscience iowastateuniversity ames ia usa hridesh iastate.edu abstract inrecentyears manyincidentshavebeenreportedwheremachine learningmodelsexhibiteddiscriminationamongpeoplebasedon race sex age etc.
research has been conducted to measure and mitigate unfairness in machine learning models.
for a machine learning task it is a common practice to build a pipeline that includes an ordered set of data preprocessing stages followed by a classifier.
however most of the research on fairness has considered a single classifier based prediction task.
what are the fairness impacts of the preprocessing stages in machine learning pipeline?
furthermore studiesshowedthatoftentherootcauseofunfairness isingrainedinthedataitself ratherthanthemodel.butnoresearch has been conducted to measure the unfairness caused by a specific transformation made in the data preprocessing stage.
in this paper we introduced the causal method of fairness to reason about the fairness impact of data preprocessing stages in ml pipeline.
we leveraged existing metrics to define the fairness measures of the stages.
then we conducted a detailed fairness evaluation of the preprocessing stages in pipelines collected from three different sources.
our results show that certain data transformers are causing the model to exhibit unfairness.
we identified a number of fairness patterns in several categories of data transformers.
finally we showed how the local fairness of a preprocessing stage composes in the global fairness of the pipeline.
we used the fairness compositiontochooseappropriatedownstreamtransformerthat mitigates unfairness in the machine learningpipeline.
ccs concepts softwareanditsengineering softwarecreationandmanagement computingmethodologies machine learning.
keywords fairness machine learning preprocessing pipeline models acm reference format sumon biswas and hridesh rajan.
.
fair preprocessing towards understanding compositional fairness of data transformers in machine learning pipeline.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august 23 28 athens greece.
acm new york ny usa pages.
esec fse august 23 28 athens greece copyright held by the owner author s .
acm isbn .
introduction fairnessofmachinelearning ml predictionsisbecomingmoreimportant with the rapid increase of ml software usage in important decisionmaking andtheblack boxnatureofmlalgorithms .
there isa rich bodyof workon measuring fairness of ml models and mitigate the bias .recentwork has shownthatmoresoftwareengineeringeffortisrequiredtowards detecting bias in complex environment and support developersin building fairer models.
themajorityofworkonmlfairnesshasfocusedonclassification taskwithsingleclassifier .however real worldmachine learning software operate in a complex environment .
inan mltask thepredictionis madeafter goingthroughaseries of stages such as data cleaning feature engineering etc.
which build the machine learning pipeline .
studying only the fairnessoftheclassifiers e.g.
decisiontree logisticregression fails to capture the fairness impact made by other stages in ml pipeline.
in this paper we conducted a detailed analysis on how the data preprocessing stagesaffectfairnessinml pipelines.
prior research observed that bias can be encoded in the data itself and missing the opportunity to detect bias in earlier stage of ml pipeline can make it difficult to achieve fairness algorithmically .
additionally bias mitigation algorithms operatinginthe preprocessingstage were shownto besuccessful .therefore itisevidentthatthepreprocessingstagesofml pipelinecanintroducebias.however nostudyhasbeenconducted tomeasurethefairnessofthepreprocessingstagesandshowhowit impactstheoverallfairnessofthepipeline.inthispaper weusedthe causalmethodoffairnesstoreasonaboutthefairnessimpactofpreprocessing stages in ml pipeline.
then we leveraged existing fairnessmetricstomeasurefairness ofthepreprocessingstages.using themeasures weconductedathoroughanalysisonabenchmark of37real worldmlpipelinescollectedfromthreedifferentsources whichoperateonfivedatasets.thesemlpipelinesallowedusto evaluatefairnessofawideselectionofpreprocessingstagesfrom different categories such as data standardization feature selection encoding over under sampling imputation etc.forcomparative analysis wealsocollecteddatatransformerse.g.
standardscaler minmaxscaler pca l1 normalizer quantiletransformer etc.
from the pipelines as well as corresponding ml libraries and evaluatedfairness.finally weinvestigatedhowfairnessofthesepreprocessing techniques local fairness composes with other preprocessing stages and the whole pipeline global fairness .
specifically we answeredthe following three research questions.
rq1 fairness of preprocessing stages what are the fairness measures of each preprocessing stage in ml pipeline?
rq2 fair 981this work is licensed under a creative commons attribution international .
license.
esec fse august athens greece sumonbiswas andhrideshrajan transformers what are the fair and biased data transformers amongthecommonlyusedones?
rq3 fairnesscomposition how fairnessofdata preprocessing stagescomposes inml pipeline?
howlocal fairnesscompose intoglobalfairness?
does choosing a downstream transformer depend on the fairnessofan upstream transformer?
to the best of our knowledge we are the first to evaluate the fairnessofpreprocessingstagesinmlpipeline.ourresultsshow that by measuring the fairness impact of the stages the developers wouldbeabletobuildfairerpredictionseffectively.furthermore the libraries can provide fairness monitoring into the data transformers similartotheperformancemonitoringfortheclassifiers.
ourevaluationonreal worldmlpipelinesalsosuggestsopportunitiestobuildautomatedtooltodetectunfairnessinthepreprocessing stages andinstrumentthosestagestomitigatebias.wehavemade the following contributionsinthis paper we created a fairness benchmark of ml pipelines with several stages.thebenchmark codeandresultsaresharedinourreplication package1in github repository that can be leveraged in further researchonbuildingfair ml pipeline.
weintroducedthenotionofcausalityinmlpipelineandleveraged existing metrics to measure the fairness of preprocessing stagesinml pipeline.
unfairnesspatternshavebeenidentifiedforanumberofstages.
we identified alternative data transformers which can mitigate biasinthe pipeline.
finally weshowedthecompositionofstage specificfairness intooverallfairness whichisusedtochooseappropriatedownstream transformerthat mitigates bias.
thepaperisorganizedasfollows 2describesthemotivating examples 3describes the existing metrics and our approach.
in 4 we describedthebenchmarkand experiments.
5exploresthe results 6provides a comparative studyamong transformers and 7evaluates the fairness composition.
finally 9describes the threatsto validity 10discussesrelatedwork and 11concludes.
motivation in this section we present two ml pipelines which show that thepreprocessingstageaffectsthefairnessofthemodelanditis importantto study the biasinducedbycertaindata transformers.
.
motivating example yanget al.
studied the following ml pipeline which was originallyoutlinedbypropublicaforrecidivismpredictionon compas dataset .
the goal is predict future crimes based on the data of defendants.
the fairness values in terms of statistical parity difference spd .
and equal opportunity difference eod .
suggest that the prediction is biased towards2caucasian defendants when raceis considered as sensitive attribute.
the pipelineconsistsofseveralpreprocessingstagesbeforeapplying logisticregression classifier.datapreprocessingincludescleaning encodingcategoricalfeatures andmissingvalueimputation.
recentresearch showedthatthedatatransformationinthis 2biastowardsa group connotesthat the prediction favours that group.pipelineisnotsymmetricacrossgendergroupsi.e.
maledefendants arefilteredmorethanthefemale.dothesedatatransformationsintroduceunfairnessintheprediction?ifyes whataretheunfairness measuresofthesetransformers?isitpossibletoleverageexisting metrics to measure the unfairness of each component?
if we can understandtheeffectofeachdatatransformer itwouldbepossible tochoosedatapreprocessingtechniquewiselytoavoidintroducing biasas well as mitigate the inherentbiasindata orclassifier.
1df pd.
read csv f path 2df df df .
days b screening arrest df .
days b screening arrest df .
is recid !
df .
c charge degree !
o df .
score text !
n a 6df df .
replace medium low 7labels labelencoder .
fit transform df .
score text 8impute1 onehot pipeline imputer1 simpleimputer strategy most frequent onehot onehotencoder handle unknown ignore 11impute2 bin pipeline imputer2 simpleimputer strategy mean discretizer kbinsdiscretizer n bins encode ordinal strategy uniform 14featurizer columntransformer transformers impute1 onehot impute1 onehot impute2 bin impute2 bin 17pipeline pipeline features featurizer classifier logisticregression .
motivating example thefollowingmlpipelineiscollectedfromthebenchmarkused by biswas and rajan for studying fairness of ml models.
this pipelineoperateson germancredit dataset.here thegoalistopredictthecreditrisk good bad ofindividualsbasedontheirpersonal datasuchasage sex income etc.inthispipeline beforetraining the classifier data has been processed using two transformers pca forprincipalcomponentanalysis and selectkbest forselecting high scoringfeatures.thefairnessvalue spd .
showsthat predictionisslightlybiasedtowards femalecandidates.however if thetransformersarenotapplied thenpredictionbecomesbiased towardsmale spd .
.
by applying one transformer at a time weobservedthat pcaaloneisnotcausingthechangeoffairness.in thiscase selectbest iscausingbiastowards female whichinturn mitigatingtheoverallfairnessofthepipeline.therefore inaddition to study the fairness of transformers in isolation it is important to understand howfairness of components composesin the pipeline.
1features 2features .
append pca pca n components 3features .
append select best selectkbest 4feature union featureunion features 5estimators 6estimators .
append feature union feature union 7estimators .
append rf randomforestclassifier 8model pipeline estimators 9model .
fit x train y train 10y pred model .
predict x test ourkeyideaistoleveragecausalreasoningandobservefairness impact of a stage on prediction.
to do that we create alternative pipelinebyremovingastage.forexample fromtheabovepipeline we remove the selectkbest and compare the predictions with original pipeline.
we observe that selectkbest is causing .
of the female and .
of the male participants to change predictions from favorable good credit to unfavorable bad credit .since the stage is causing more unfavorable decisions to male the stage is biasedtowardsfemale.thus weusedexistingfairnesscriteriato measure fairnessimpact of astageandpropose novel metrics.
982fair preprocessing towardsunderstandingcompositionalfairness of datatransformersin machinelearning pipeline esec fse august athens greece processed datacustom data filtration s6 pca s1selectbest s2classifier randomforests3s1s3 s5 kbinsdiscretizer f2 hot encoder f1 classifier logisticregression binarize f3 s4 s2 s7 motivating example 1mean imputer f2 model predictiontest data model predictiontest data mode imputer f1 motivating example processed data training data training data custom data filtration figure ml pipelines for the motivating examples having asequenceofpreprocessing stages followed by aclassifier.
methodology inthissection first wedescribethebackgroundofmlpipeline focussingonthedatapreprocessingstages.second weformulatethe method and metrics to measure fairness of a certain preprocessing stagewithrespectto the pipeline itisusedwithin.
.
ml pipeline amershi et al.proposed a nine stage machine learning pipeline withdata oriented collection cleaning andlabeling andmodeloriented model requirements feature engineering training evaluation deployment andmonitoring stages .otherresearch also described data preprocessing as an integral part of the ml pipeline.
the pipelines in the motivating examples are depicted in figure1 whichfollows therepresentationprovidedby yang et al.
.
in this paper we adapted the canonical definition of pipeline fromscikit learnpipelinespecification whichisaligned withthemlmodelsstudiedintheliteratureforfairclassification tasks .
we are interested in investigating the fairnessofthedatapreprocessingstagesinthepipeline whichis depictedwithgrey boxesinfigure .
to summarize a canonical ml pipeline is an ordered set of m stages with a set of preprocessing stages s1 s2 ...sm and a finalclassifier sm .
each preprocessing stage skoperates on the data already processed by preceding stages s1 ...sk .
a data preprocessing stage skcan be a data transformer or a set of customoperations.a datatransformer isawell knownalgorithmor methodto performaspecificoperationsuch asvariableencoding feature selection feature extraction dimensionality reduction etc.
onthedata .forexample inthesecondmotivatingexample two transformers pcaandselectkbest have been used.
custom transformation includesdata task specificcontextualoperationson thedataset.forexample in .
line thedatainstancesthat do not contain a value in the range for the feature days b screening arrest have been filtered.
this means the pipeline ignoredthedataofthedefendantswithmorethan30daysbetween theirscreeningandarrest.thisformulationofmlpipelineallowed ustoevaluatefairnessofthepreprocessingstagesinreal worldml tasks.
.
existingfairnessmetrics wehaveleveragedexistingfairnessmetricstomeasurethefairness ofthewholepipeline.manyfairnessmetricshavebeenproposedin theliteratureformeasuringfairnessofclassificationtasks .
ingeneral thefairnessmetricscomputegroup specificclassification rates e.g.
true positives false positives and calculates thedifferencebetweengroupstomeasurethefairness.inthispaper we adopted the representative group fairness metrics used by .
specifically weleveragedthefollowingmetrics statisticalparity difference spd equal opportunity difference eod average odds difference aod and error rate difference erd .givenadataset dwithninstances let actualclassificationlabelbe y predictedclassificationlabelbe y andsensitive attributebe a.here y 1ifthelabelisfavorabletotheindividuals otherwise y .forexample classificationtaskon german creditdatasetpredictsthecreditrisk good badcredit ofindividuals.
in this case y if the prediction is good credit otherwise y .
suppose for privileged group e.g.
white a and for unprivilegedgroup e.g.
non white a .spdiscomputedby observingtheprobabilityofgivingfavorablelabeltoeachgroup and taking the difference.
eod measures the true positive rate difference between groups.
aod calculates both true positive rate andfalsepositiveratedifferenceandthentakestheaverage.erd calculatesthesumoffalsepositiveratedifferenceandfalsenegative ratedifferencebetweengroups.thedefinitionsofthesemetricsare as follows spd p p eod p p aod p p p p erd p p p p disparateimpact di andstatisticalparitydifference spd both measurethesameratei.e.
probabilityofclassifyingdatainstanceas favorable but di computes the ratio of privileged and unprivileged groups rate whereasspdcomputesthedifference.therefore from di andspd we only usedspd inour evaluation.
.
fairnessofpreprocessing stages suppose pisapipelinewith mstagesandourgoalistoevaluate thefairnessofthestage sk where1 k m.inotherwords we wanttomeasurethefairness impactof skonthe predictionmade byp.toachievethatweappliedthecausalreasoningforevaluating fairness.
the causality theorem was proposed by pearl and furtherstudiedextensivelytoreasonaboutfairnessinmanyscenarios .causalitynotionoffairnesscapturesthat everythingelsebeingequal thepredictionwouldnot bechanged in the counterfactual world where onlyan intervention happens on a variable .
for example galhotra et al .proposed causaldiscriminationscore forfairnesstesting .theauthors createdtestinputsbyalteringoriginalprotectedattributevalues of each data instance and observed whether prediction is changed for those test inputs.
if the intervention causes the prediction to be changed we call the software causally unfair with respect to that intervention.
in our case if a preprocessing stage skbe the intervention to measure the fairness of sk we have to capture the prediction disparity caused by the intervention sk.
this causal reasoningoffairnessisastrongernotionsinceitprovidescausality insoftwarebyobservingchangesintheoutcomemadebyaspecific stageinthe pipeline .
983esec fse august athens greece sumonbiswas andhrideshrajan .
.
causal method to measure fairness of preprocessing stage.
frompipeline p weconstructanotherpipeline p byonlyexcludingthestage skfromp.afterapplyingthestage skinp towhat extent the prediction of p changes and whether the change is favorabletoanygroup?broadly thiscanbemeasuredbyobserving the prediction difference between pandp and computing the fairnessofthesechanges using the fairnessmetrics from .
suppose thepredictionsmadebythetwopipelinesare y p and y p .let ibetheimpactsetfor sk whichdenotestheprediction paritybetween y p and y p suchthatfor ithdatainstance if yi p yi p thenii otherwise .
by causality the fairness ofpreprocessingstage denotedby sf iscalculatedbasedon y p y p i 1withrespecttoafairnessmetric m whichisshownin 2a .
we noticed that a few preprocessing stages specifically the encoderscannotberemovedwithoutreplacingwithanalternative stage.
for such situations we have defined the fairness of skwith reference to anotherstage s k denotedby sf sk s k in 2b .
zelayaalsousedthesimilarmethodforquantifyingthe effect of a preprocessing stage with a goal of computing volatility of a stage .volatility quantifieshow muchimpact apreprocessing stagehasontheoutcomebycomputingtheprobabilityofprediction changes.however itdoesnotcapturethefairnessofthestage since a stage can cause high change in the prediction by maintaining the predictions fair.
next in .
.
we have extended our causality based formulation of 2a for each fairness metric in to capture thefairnessimpactofeachpreprocessingstage.similarto the benefitofthisformulationis themeasuresdonotrequireanoracle sincetheprediction equivalenceofpipelines pandp serves the goal of evaluating fairness of the stage.
note that the rest of the definitionsin .
.2are independent of 2a and 2b .
i braceleftbigg0if yi p yi p 1otherwise for alli .
..n sf sk m i 1wherep p sk 2a sf sk s k m i 1wherep p sk s k 2b .
.
fairness metircsforpreprocessingstage.
we have leveraged the definition of metrics spd eod aod and erd from to capturethe stage specific fairnessof sk.essentially thenewmetrics will identify the disparities between yi p and yi p and use correspondingfairnesscriteriatomeasurehowmuch skfavorsa specific group withrespectto othergroup s .
suppose among ndata instances nuare from the unprivileged group and npfrom the privileged group.
sfcspdcomputes how manyofthedatainstanceshavebeenchangedfromunfavorableto favorable after applying the stage sk.
to do that we count changes in both directions unfavorable to favorable and favorable to unfavorable andtakethedifference.thesignof sfcspdpreserves the direction of changes.
finally the metric sfspdis computed by taking the difference of rates sfrspd between unprivileged and privileged groups.
note that the metric captures fairness by measuringthedifferenceoffavorablechangeratesbetweengroups.
simply counting the mismatches between yi p and yi p could provide degree of changes in sfcspdbut would not capture fairness.
furthermore computing favorable changes to both groups separately and evaluating the disparity between them captures fairnessaccording to the originaldefinitionof spd.sfcispd 1if yi p 1and yi p 1if yi p 0and yi p 0otherwise sfcspd n summationdisplay.
i 1sfcispd sfrspd u sfcsfd u nu sfrspd p sfcspd p np sfspd sfrspd u sfrspd p similarly sfeodis defined using the following equation.
in this case onlythetrue positivechangesareconsideredassuggestedby the definitionofeodfrom .
sfcieod 1ifyi yi p 1and yi p 1if yi p 0andyi yi p 0otherwise sfceod n summationdisplay.
i 1sfcieod sfreod u sfceod u nu y sfreod p sfceod p np y sfeod sfreod u sfreod p sinceaod computes theaverage oftrue positive tp rate and falsepositive fp rate firstthechangesetfortpandfppredictions is computed.
then averaging the probability of changes for tp and fp the change rates are computed for both groups.
finally sfaod iscalculatedbytakingthedifferenceofratesbetweenprivileged andunprivilegedgroups.
sfcitp 1ifyi yi p 1and yi p 1if yi p 0andyi yi p 0otherwise sfcifp 1if yi p 1andyi yi p 1ifyi yi p 0and yi p 0otherwise sfctp n summationdisplay.
i 1sfcitp sfcfp n summationdisplay.
i 1sfcifp sfraod u sfctp u nu y sfcfp u nu y sfraod p sfctp p np y sfcfp p np y sfaod sfraod u sfraod p finally sferdis computed using the change of count in both false positives fp and false negatives fn as mentioned in the definitionoferd in .
sfcifn 1if yi p 0andyi yi p 1ifyi yi p 1and yi p 0otherwise sfcfn n summationdisplay.
i 1sfcifn sfrerr u sfcfp u nu y sfcfn u nu y sfrerr p sfcfp p np y sfcfn p np y sferr sfrerr u sfrerr p thusfar wehavefourfairnessmetrics sfspd sfeod sfaod andsferd to measure the fairness of the stage.
in general the ratescomputed by each metric sfr follow thesamerangeof the originalmetrics .therefore theabove metricshave arange .
positive values indicate bias towards unprivileged group negative values indicate bias towards privileged group and values very close to 0indicatefair preprocessing stage.
984fair preprocessing towardsunderstandingcompositionalfairness of datatransformersin machinelearning pipeline esec fse august athens greece evaluation in this section we describe the benchmark dataset and pipelines thatweusedforevaluation.thenwepresenttheexperimentdesign andresults for answeringthe researchquestions.
.
benchmark we collected ml pipelines used in prior studies for fairness evaluation.first biswasandrajancollectedabenchmarkof40mlmodels collected from kaggle that operate on different datasets e.g.
german credit adult census bank marketing home credit andtitanic .
however the authors did not study the fairness at the component level rather the ultimate fairness of theclassifierse.g.
randomforest decisiontree etc.werevisited thesekagglekernelsandcollectedthepreprocessingstagesused inthepipelines.wenoticedthat homecredit dataset inthis benchmarkisnotunifiedliketheotherdatasets distributedover multiples csv files and the models under this dataset do not operateonthesamedatafiles.hence thesemodels 8outof40 arenot suitable for comparing fairnessofdata preprocessing stages.
second wecollectedthepipelinesprovidedby yangetal .
.
theauthorsreleased3pipelinesontwodifferentdatasets adult censusandcompas.
third zelaya studied the volatility of the preprocessing stages using two pipelines on a fairness dataset i.e.
german credit .
we included these pipelines in our benchmark.
thus wecreatedabenchmarkof37mlpipelinesthatoperateon fivedatasets.thepipelineswiththestagesineachdatasetcategory andtheirperformancesareshownintable .belowwepresenta briefdescriptionofthe datasets andassociatedtasks.
table the preprocessing stages and performance measures accuracy f1 score ofthepipelines inthebenchmark german stages accf1adultstages accf1 gc1 pca sb .
.76ac1ss le .
.
gc2 smote ss .
.81ac2mv .
.
gc3 pca .
.83ac3custom f .
.
gc4 le ss .
.82ac4pca ss .
.
gc5 ss .
.83ac5le .
.
gc6 pca ss le .
.83ac6custom f custom c .
.
gc7 pca sb .
.77ac7pca ss custom f .
.
gc8 ss .
.81ac8ss custom f stratify .
.
gc9 smote .
.77ac9ss .
.
gc10 usamp .
.81acp10impute .
.
bank stages accf1titanicstages accf1 bm1 custom le ss .
.56tt1mv custom f encode .
.
bm2 le .
.61tt2mv custom f .
.
bm3 le ss custom f .
.48tt3custom f impute .
.
bm4 ss .
.33tt4custom f impute rfecv .
.
bm5 ss .
.
tt5custom f .
.
bm6 fs stratify ss .
.58tt6custom f .
.
bm7 fs .
.6tt7ss le custom f .
.
bm8 stratify .
.56tt8custom f .
.
compas stages accf1 cp1 filter impute1 encode impute2 kbins binarize .
.
sb selectbest ss standardscaler le labelencoder usamp undersampling custom f c custom featureengineeringor cleaning fs featureselection mv missing valueprocessing rfecv featureselectionmethod fairness is measured with respect to areferencestage.
german credit.
the datasetcontains data instances and featuresofindividualswhotakecreditfromabank .thetarget isto classifywhether the person has agood badcreditrisk.adult census.
the dataset is extracted by becker from census of united states.
it contains data instances and features including demographic dataof individuals.
the task is to predict whether the person earns over 50kinayear.
bankmarketing.
this dataset contains a bank s marketing campaigndataof41 188individualswith20features .thegoalis to classifywhether aclientwillsubscribe to aterm deposit.
titanic.the dataset contains information about passengers oftitanic .thetaskistopredictthesurvivaloftheindividuals ontitanic.
the sensitive attribute of this dataset is sex.
compas.thedatasetcontainsdataof6 889criminaldefendantsin florida.propublicausedthisdatasetandshowedthattherecidivism predictionsoftwareusedinuscourtsdiscriminatesbetweenwhite andnon white .thetaskistoclassifywhetherthedefendants willre offend where raceisconsideredas the sensitive attribute.
.
experiment design eachpipelineinthebenchmarkconsistsofoneormorepreprocessing stages followed by the classifier.
in this paper our main goal is to evaluate the fairness of different preprocessing stages using the fairnessmetricsdescribedin .thebenchmark codeandresults are releasedinthe replication package .
the experiment designfor evaluating the pipelinesis shownin figure2.
first for each pipeline we identified the preprocessing stages.
for example the pipeline in .1contains six preprocessing stages.
to evaluate the fairness of a stage skin a pipeline p we create an alternative pipeline p by removing the stage sk.
for stages that can not be removed we replaced skwith a reference stages k. among the preprocessing stages shown in table we foundonlytheencoderscannotberemoved.weexperimentedwith all the encoders in scikit learn library i.e.
onehotencoder labelencoder ordinalencoder andfoundthat onehotencoder does not exhibit any bias.
therefore we used onehotencoder as the reference stagefor the encoders inour experiment.
second theoriginaldatasetissplitintotraining andtest set .thentwocopiesoftrainingdataareusedtotrainpipeline pandp .
after training the classifiers two models predict the label for the same set of test data instances.
then similar to the experimentation of for each prediction label we compare the two predictions yi p and yi p with the true prediction label yi.
this comparison providesthenecessarydata to computethefour fairness metrics.
similar to for each stage in a pipeline we run this experiment ten times and then report the mean and standard deviation of the metrics to avoid inconsistency of the randomness in the ml classifiers.
finally we followed the ml best practices sothat noiseisnot introduced evaluatingthefairness of preprocessingstages.forexample whileapplyingsometransformation lack of data isolation might introduce noise in the evaluation e.g.
when applying pcaon dataset it is important to train the pca onlyusingthetrainingdata.ifweusethewholedatasettotrainthe pcaandtransformdata theninformationfromthetest setmight leak.
third since a stage operates on the data processed by the preceding stage s there are interdependencies between them.
we alwaysmaintainedtheorderofthestageswhileremovingorreplacingastage .toobservefairnessofdatatransformerswithout interdependencies we appliedthemonvanilla pipelines .
985esec fse august athens greece sumonbiswas andhrideshrajan sp dataset dsksqprediction spdtrain sqprediction compute fairness dtest train classifier ctrain classifier c modelmodel xtestytestfairness of stage sk train test split figure experiment designto measure fairness ofpreprocessing stages inmachinelearning pipeline.
fairness ofpreprocessingstages in this paper we used a diverse set of metrics to evaluate fairness ofpreprocessingstages.whiledevelopinganmlpipelines ifthe developerhasacomprehensiveideaofthefairnessofpreprocessing stages itwouldbeconvenienttobuildafairpipeline.theevaluation hasbeendoneon69preprocessingstagesin37mlpipelinesfrom5 datasetcategories.for compasdataset wefoundonepipeline .
.
five out of six stages in this pipeline exhibit no bias which has beendiscussed later.forother4datasetcategories theevaluation hasbeenshowninfigure .inthissection first wediscusshow we caninterpret themetrics.second we answerthefirstresearch questionanddiscuss the findingsfrom our evaluation.
.
whatdothemetricsimply?
we investigated the fairness of the preprocessing stages using four metrics sfspd sfeod sfaod andsferd.
these metrics measure thefairnessofthestagesbyusingtheexistingfairnesscriteria e.g.
sfspdmeasures the fairness of a stage with respect to statistical parity difference spd criteria.
these fairness criteria evaluate algorithmic fairness of ml pipelines .
the unfairness characterized by these criteria is measured based on the prediction disparities although the root cause can be the training data orthealgorithm e.g.
datapreprocessing classifier itself.therefore when an ml model is identified as unfair it implies that in the given predictive scenario the outcome is biased.
similarly the metrics proposed in this paper measure algorithmic unfairness caused by a specific preprocessing stage with respect to its pipeline.
for instance in figure pipeline gc4 has two stages labelencoder andstandardscaler .
the fairnessmetricssuggest thatlabelencoder is biased towards unprivileged group positive value and standardscaler is biased towards privileged group negative value .
the stages for which the measures are very close to zero can be consideredas fair preprocessing.
themetricscanprovidedifferentfairnesssignalsforacertain stage.forexample inac4 sferdshowspositivefairness whereas the other metrics suggest negative fairness for both the stages pcaandstandardscaler .thisdisparityoccursbecausedifferent metrics accounts for different fairness criteria.
in this case sferd depends on the false positive and false negative rate difference.
no othermetricisconcernedaboutthefalsenegativeratedifference and hence sferdprovides a different fairness signal than other metrics.inpractice appropriatefairnesscriteriacanvarydepending onthetask usagescenario orinvolvedstakeholders.studysuggests that developers need to be aware of different fairness indicators tobuildfairerpipelines .therefore wedefinedandevaluated fairnessofstageswithrespectto multiple metrics.
.
fairnessanalysis ofstages the pipelines used both built in algorithm imported from libraries i.e.
datatransformers as wellas custompreprocessingstages.
the stages found in each pipeline are shown in table and the fairness measures of those stages are plotted in figure .
although theunfairnessexhibitedbyastageiswithrespecttothepipeline wefoundfairnesspatternsofsomestagesandinvestigatedthem further.ingeneral ourfindingsshowthatthestageswhichchange the underlying datadistributionsignificantly ormodifyminority data are responsible for increasing biasinthe pipelines.
finding data filtering and missing value removal change the datadistribution and hence introduce biasin ml pipeline.
most of the real world datasets contain missing values mv for severalreasonssuchasdatacreationerrors not applicable n a attributes incomplete data collection etc.
in our benchmark adult censusandtitaniccontain mv that required further processing in thepipeline.
.
rowsin adultcensus and20.
rowsin titanic haveatleastonemissingfeatureinthedataset.thepipelineseither remove the rows with mv or apply certain imputation technique that replaces the mv with mean median or most frequently occurredvalues.removalofrowswithmvcansignificantlychange the data distribution which introduces bias in the pipeline.
for example bothtt1andtt2removeddataitemswithmvbyapplying df.dropna method whichintroducesbiasintheprediction figure3 .
research has shown that mv are not uniformly distributed over all groups and data items from minority groups often contain more mv .
ifthose dataitemsare entirely removed therepresentation of minority groups in the dataset becomes scarce.
on the otherhand tt3appliedmean imputationandtt4appliedboth median and mode imputation using df.fillna which exhibits fairnesscomparedtodataremoval.whileourfindingssuggestthat removing data items with mv introduces bias the most popular fairness tools aif aequitas themis ml ignore these data items and remove entire row column.
our evaluation strategyconfirmsthatthetoolscanintegrateexistingimputation methods in the pipeline and allow users to choose appropriate ones.
additionally more research is needed to understand and develop imputation techniques that are fairnessaware.
finding2 newfeaturegenerationorfeaturetransformationcan havelargeimpact onfairness.
we found that most of the feature engineering stages especially thecustomtransformationsexhibitbiasinthepipeline.forexample the pipelines in titanicdataset used custom feature engineering sincethedatasetcontainscompositefeatureswhichmayprovide additional information about the individuals.
for instance tt8 operatesonthefeature nametocreate a newfeature titlee.g.
mr 986fair preprocessing towardsunderstandingcompositionalfairness of datatransformersin machinelearning pipeline esec fse august athens greece pcasb smote ss pcaless ss pcasslepcasb ss smote usamp gc1 gc2 gc3 gc4 gc5 gc6 gc7 gc8 gc9 gc10german creditsf spd sf eod sf aod sf erd custom lessle le ssct f ss ss fs stratify ss fs stratify bm1 bm2 bm3 bm4 bm5 bm6 bm7 bm8bank marketing .
.
.
.
.
.
mvct f encode mvct f imputect f ct f imputerfecvct f ct f sslect f ct f tt1 tt2 tt3 tt4 tt5 tt6 tt7 tt8titanic .
.
.
.
.
.
sslemvct f pcasslect f ct c pcassct f ssct f stratify ss impute ac1 ac2 ac3 ac4 ac5 ac6 ac7 ac8 ac9 ac10adult census figure3 fairnessmeasures y axis ofpreprocessingstagesinpipelines x axis .greylinesabovebarsindicatestandarderror.
mrs dr etc.thistransformationaimsatbetterpredictionofthe survivalofpassengersbyextractingthesocialstatus butcreates high biasbetween maleandfemale.
inadultcensus thefeature eductionofindividualscontainvalues suchaspreschool 10th 1st 4th prof school etc.
whichhavebeen replacedbybroadcategoriessuch dropout highgrad mastersin thepipelineac7.inaddition insteadofusing ageascontinuous value thefeaturehasbeendiscretizedinto nnumberofbins.inboth cases theoriginaldatavalueshavebeenmodified whichhascaused unfairnessinthepipeline.nevertheless somepipelines ac3 ac8 have custom feature transformations that are fair.
previous studies showed that certain features contribute more to the predictive quality of the model .
feature importance in prediction and corelation of features with the sensitive attribute also led to bias detection in ml models.
however does creating new features by removing certain semantics from a potentially biased featureincreasethefairness isanopenquestion.ourmethodto quantify the fairness of such changes can guide further research in this direction.
finding encodingtechniques shouldbe chosencautioslybased ontheclassifier.
two most used encoding techniques for converting categorical featuretonumericalfeatureare onehotencoder andlabelencoder .
onehotencoder createsnnewcolumnsbyreplacingonecolumnfor eachofthe ncategories.
labelencoder doesnotincreasethenumberofthecolumns andgiveseachcategoryanintegerlabelbetween 0and n .inourevaluation wefoundthat labelencoder introducesbiasin germancredit andtitanicdatasetbut onehotencoder doesnotchangefairness.since labelencoder imposesasequentialorderbetweenthecategories itmightcreatealinearrelation with the target value and hence have an impact on the classifier to change fairness.
for example pipelines tt7 creates a new feature calledfamilybased on the surname of the person.
this feature has alargenumberofuniquecategories 667uniqueonesin891datainstances .
therefore the non sparse representation in labelencoder addsadditionalweighttothefeature whichis causingunfairness in tt7.
developers might avoid onehotencoder because it suffers from the curse of dimensionality and the ordinal relation of data is lost.inthatcase developersshouldbeawareofthefairnessimpact oftheencoder.onesolutionmightbeusingpcafordimensionality reduction whichhas been done ingc7.finding4 thevariabilityoffairnessofpreprocessingstagesdepend onthedatasetsizeand overall predictionrateofthe pipelines.
wehaveplottedthestandarderrorofthemetricsaserrorbars in figure .
firstly it shows that the metrics in german credit and titanicdataset are more unstable.
the reason is that the size of these two datasets is less than the other three datasets.
german creditdataset has instances and titanichas instances.
adult census andbank marketing dataset have more than 30k instances.ifthesamplesizeislarge datadistributiontendstobe similar even after taking a random train test split .
however when the dataset size is smaller the distribution is changed among differenttrain testsplitting.furthermore wehavefoundthat sferd is more unstable than other metrics.
sferddepends on the change offalsepositiveandfalsenegativerates.however inmostcases the pipelines are optimized for accuracy and precision since these aresomebestperformingonescollectedfromkaggle.therefore before deploying preprocessing stages it would be desirable to test the stability ofover multiples executions.
finding the unfairness of a preprocessing stage can be dominated bydatasetortheclassifier used in the pipeline.
for thecompasdataset we evaluated the six stages shown in 2. .
all the stages exhibited data filtering show bias.
the data filtering also showed bias close to zero less than .
with respect toallthemetrics.althoughyang etal.
arguedthatthispipeline filters data in different proportions from maleandfemalegroup our evaluation confirms it does not cause unfairness.
this pipeline has been used by propublica to show the bias in the prediction.
therefore itisunderstandablethattheydidnotemployanypreprocessing that introduces bias in the pipelines.
other than that almostallthepreprocessingstagesin bankmarketing pipelinesalso exhibit very little unfairness which suggest that the preprocessing onthis dataset are fair ingeneral.
afewstagesshowdifferentbehaviorwhentheyareusedincomposition with different classifiers.
for example standardscaler has been applied on both gc6 and gc8.
while gc6 employs a randomforest classifier gc8 uses k neighbors classifier.wehave observedtheoppositefairnessmeasuresfor standardscaler inthese twopipelines.
therefore fairnesscanbe dominatedby theunderlyingpropertiesofdataorthepipelinewhereitisapplied.wehave furtherinvestigatedthisphenomenonbyapplyingtransformerson differentclassifiers inthe nextsection.
987esec fse august athens greece sumonbiswas andhrideshrajan .
.
.
.
.
pca sb smote ss pca le ssss pca ss le pca sbss smote usamp ss le mv ct f pca ss le ct f ct c pca ss ct f ss ct f stratify ss impute custom le ss lele ss ct f ssss fs stratify ss fs stratify mv ct f encode mv ct f impute ct f ct f impute rfecv ct f ct f ss le ct f ct f sf spd sf eod sf aod sf erd accuracy f1 score german credit adult census bank marketing titanic gc1gc2gc3gc4gc5gc6gc7gc8gc9gc10ac1ac2ac3ac4ac5ac6ac7 ac8ac9ac10bm1bm2bm3bm4bm5bm6bm7bm8tt1 tt2tt3tt4tt5 tt7tt6 tt8 figure performance changes green are plotted with fairness red of the preprocessing stages.
a positive or negative performance change indicates performanceincrease ordecrease respectively after applying the stage.
.
fairness performancetradeoff inthissection weinvestigatedthefairness performancetradeoff for the preprocessing stages.
the original performances of each pipeline have been reported in table .
to investigate fairness of a stage we created pipeline p by removing the stage from original pipelinep 2 .tounderstandfairness performancetradeoff we evaluatedperformance bothaccuracyandf1score of p andpin the same experimental setup.
then we computed the performance differencetoobservetheimpactofthestageonperformance.for example acc p acc p givestheaccuracyincrease ordecrease if negative after applying a stage.
we plotted the performance impacts ofthe stageswiththeirfairnessmeasures infigure .
first many preprocessingstages have negligible performance impact.
in figure out stages exhibits accuracy and f1 score changeintherange whichindicatesperformance change .
.
we found that in all of these cases except ac9 andac10 thepreprocessingstagesarefairwithaverysmalldegree ofbias.
second tradeoff between performance andfairness is observed forthe stages which improve performance.
stagesimprove accuracy or f1 score more than .
which further exhibits moderate to high degree of bias.
overall the most biased stages tt7 le tt8 ct tt4 ct tt1 mv gc8 ss are improving performance.thisstage specifictradeoffisalignedwiththeoverall performance fairnesstradeoff discussedinpriorwork which can be compared quantitatively by the work of hort et al .
.third wefoundthatsomestagesdecreasetheperformance either accuracy or f1 score.surprisingly mostof these stages also exhibit high degree of bias.
for instance the most performancedecreasing stages bm4 ss ac7 pca gc10 undersampling are showing more bias.
our fairness evaluation would facilitate developers to identifyandremove such stagesinthe pipeline.
fairdata transformers in 5 we found that many data preprocessing stages are biased.
many bias mitigation techniques applied in preprocessing stage have been shown successful .
if we process data with appropriatetransformer thenitmightbepossibletoavoidbiasand mitigateinherentbiasindataorclassifier.evenifadatatransformer isbiasedtowardsaspecificgroup itcouldbeusefultomitigatebias if original data or model exhibits bias towardsthe opposite group.
to that end we want to investigate the fairness pattern of the data transformers.
however in our evaluation figure some transformershavebeenusedonlyinspecificsituationse.g.
smotehastable2 transformerscollectedfrompipelinesandlibraries categories stages transformers mv processing imputation simpleimputer iterativeimputer categorical encodingencoder binarizer kbinsdiscretizer labelbinarizer labelencoder onehotencoder standardizationscaling standardscaler maxabsscaler minmaxscaler robustscaler normalization l1 normalizer l2 normalizer feature engineeringnon lineartransformation quantiletransformer powertransformer polynomialfeaturegenerationpca sparsepca minibatchsparsepca kernelpca featureselection selectkbest selectfpr selectpercentile samplingoversampling smote undersampling allknn stratification random stratified beenonlyappliedon germancredit dataset.whatisthefairnessof thistransformerwhenusedonotherdatasetsandclassifier?inthis section wesetupexperimentstoevaluatethefairnessofcommonly useddata transformersondifferentdatasets andclassifiers.
first wecollectedtheclassifiersusedineachdatasetcategory from the benchmark.
then for each dataset we created a set of vanillapipelines.avanillapipelineisaclassificationpipelinewhich contain only one classifier.
second we found a few categories of preprocessing stages from our benchmark shown in table .
for eachtransformerusedineachstage wecollectedthealternative transformersfromcorrespondinglibrary.forexample inourbenchmark standardscaler fromscikit learnlibraryhasbeenusedfor scalingdatadistributioninmanypipelines.wecollectedotherstandardizingalgorithmsavailableinscikit learn.wefoundthatbesidesstandardscaler scikit learnalsoprovides maxabsscaler minmaxscaler andnormalizer standardize data .
similarly a data oversampling technique smotehas been used in the benchmark we collected another undersamplingtechnique allknnand a combination of over and undersampling sampling technique smotennfromimblearnlibrary .third ineachofthevanilla pipelines weappliedthetransformersandevaluatedfairnessusing themethodusedin .2withrespecttofourmetrics.wefoundthat pipelinesunder titanicusescustomtransformation andmostof the built in transformers are not appropriate for this dataset.
so tobeabletomakethecomparisonconsistent weconductedthis evaluation on four datasets german credit adult census bank marketing compas.
finally we did not use transformers for imputationandencoderstages.encodingtransformers labelencoder onehotencoder havebeenappliedonmostofthepipelinesand 988fair preprocessing towardsunderstandingcompositionalfairness of datatransformersin machinelearning pipeline esec fse august athens greece .
.
.
.
.
d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k smote usamp combined standard minmax maxabs robust normalizer quantiletrans powertrans pca sparsepca kernelpca selectbest selectfpr selectpercentile sampling scaling non linear transfromer pca feature selection .
.
.
.
.
d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k smote usamp combined standard minmax maxabs robust normalizer quantiletrans powertrans pca sparsepca kernelpca selectbest selectfpr selectpercentile sampling scaling non linear transfromer pca feature selection .
.
.
.
.
d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k smote usamp combined standard minmax maxabs robust normalizer quantiletrans powertrans pca sparsepca kernelpca selectbest selectfpr selectpercentile sampling scaling non linear transfromer pca feature selection .
.
.
.
d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k d r x s k smote usamp combined standard minmax maxabs robust normalizer quantiletrans powertrans pca sparsepca kernelpca selectbest selectfpr selectpercentile sampling scaling non linear transfromer pca feature selectionsf spd sf eod sf aod sf erdgerman credit adult census bank marketing compas figure5 fairnessoftransformersonclassifiers d decisiontree r randomforest x xgboost s supportvector k kneighbors their behavior has been understood.
the fairness measures of each transformerondifferentclassifiers have been plottedinfigure .
fairnessamongthedatasetsfollowsasimilarpattern.thisfurther confirms that the unfairness is rooted in data.
the compas dataset shows the least bias.
although racial discrimination has been reported for this dataset this is a more curated dataset than the other three.
by looking at the overall trend of fairness we observe that sampling techniques have the most biased impact on prediction.
otherthan that feature selectiontransformers have more impact thanotherones.
finding among all the transformers applying sampling technique exhibits most unfairness.
sampling techniques are often used in ml tasks when dataset isclass imbalanced.unliketheothertransformers samplingtechniques make horizontal transformation to the training data.
the oversamplingtechnique smotecreates newdatainstances forthe minority class by choosing the nearest data points in the feature space.undersamplingtechniquesbalancedatasetbyremovingdata items from majority class.
although balancing dataset has been showntoincreasefairness ourevaluationsuggestthatinthree outoffourdatasets itincreasesbias.
fromfigure we can see thatsampling techniques exhibitthe mostunfairness.in germancredit dataset differentclassifierreacts differentlywhensamplingisdone.
decisiontree classifierexhibits mostunfairnessforbothoversamplingandundersamplingtowards privilegedgroupi.e.
male.interestingly thecombinationofoverandundersamplingalsofailstoshowfairness.furthermore both german credit andbankmarketing pipelines exhibitbiastowardsunprivilegedgroup whichmightbedesiredwhencomparedtobias towardsprivileged.
finding7 selecting subsetoffeatures oftenincrease unfairness.
selecting thebest performing feature can give performance improvement of the pipeline.
however unfairness can be encoded in specific features .
while selecting bestfeatures somefeatures which encodes unfairness can dominate the outcome.
thus many classifiers in german credit adult census andbank marketing show unfairness because of reduced number of features which hasbeenalsoobservedby zhangandharman .surprisingly selectfpr exhibited very little or no bias compared to the other featureselectionmethods.adetailedinvestigationsuggeststhat selectbest andselectpercentile select only the kmost contributingfeatures.however selectfpr performsfalsepositiverate test on each feature and if it falls below a threshold the feature is removed .
therefore it does not apply harsh pruning which contributesto the fairnessof the prediction.
finding in most of the pipelines feature standardization and non linear transformers are fair transformers.
thesetransformersmodifythemeanandvarianceofthedata by applying linear or non linear transformation.
however they do not change the feature importance on the classifiers.
therefore in most of the cases these transformers especially standardscaler androbustscaler are fair.
some classifiers show bias after applying these transformers such as knc in compas.
the unfairness exhibited by those pipelines are introduced by the classifiers since theseclassifiersshowsimilarbiaspatternforothertransformers 989esec fse august athens greece sumonbiswas andhrideshrajan .
.
.
sslemvcustom f pcasslecustom f custom c pcasscustom f sscustom f stratifyssimputeac1 ac2 ac3 ac4 ac5 ac6 ac7 ac8 ac9 ac10 spdeodaoderd .
.
.
v spd v eod v aod v erdglobal fairness difference local fairness figure comparison of global fairness change and local fairness for adultcensus datasetpipelines.
as well.
the scalers can impact the fairness significantly if there are many outliers in data.
that is why we see more bias for the scalersin germancredit dataset.therefore althoughstandardizing transformers are fair in general they can be biased in composition withspecific classifier ordata property.
fairness composition ofstages from our evaluation we found that many data transformers have fairnessimpactonmlpipeline.inthissection wecomparethe local fairness fairness measures of preprocessing stages with the global fairness fairness measures of whole pipeline .
first we answer whether the local fairness composes in the global fairness.
second we investigateif we canleverage thecompositionto mitigatebias bychoosing appropriate transformers.
.
composition oflocalandglobalfairness weevaluatedtheglobalfairnessof adultcensus pipelines using the four existing metrics from .
we calculated the fairness differenceofthesepipelinesbeforeandafterapplyingthepreprocessingstages.additionally wehaveevaluatedthestage specific fairness metrics.
both the local fairness and difference in global fairnessofthosepipelines have been plottedinfigure .
we can see that local and global fairness follow the sametrend inmostofthepipelines.thisconfirmsthatlocalfairnessisdirectly contributing to the global fairness.
however the global fairness is computedbasedontheoverallchangeintheprediction whereas the local fairness considers the predictions for only those data instances which have been altered after applying a transformer .forexample infigure forsomepipelines e.g.
ac9 ac10 global and local fairness exhibit different trends.
in these cases the overall classification rate difference is not similar to the rate difference of altered labels.
this means that the stages changed the labelssuchthatitshowsbiastowardsprivileged.butwhenthose changes in the labels are considered in addition to all the labels global fairness the bias difference could not capture the actual impactofthatstage.wehaveverifiedthisobservationbymanually inspectingthealteredpredictionlabels.thus wecanconcludethat the local fairness composes to the global fairness.
specifically if a preprocessing stage shows bias for privileged group it pulls the globalfairnesstowardsthefairnessdirectionofprivilegedgroup.
however only observing the global fairness difference we can not measure the fairnessofagiven stageortransformer.
.
bias mitigation usingappropriate transformers for a given transformer in an ml pipeline a downstream transformeroperatesondataalreadyprocessedbythegiventransformer and anupstreamtransformer isappliedbefore thegivenone.
since thefairnessofapreprocessingstagecomposestotheglobalfairness can we choosea downstream transformer to mitigatebias inml pipeline?inthissection weempiricallyshowthattheglobalunfairness can be mitigated by choosing the appropriate downstream transformer.
.
.
usamp ssmm ma ronoqtptk neighbors classifier spdeodaoderd .
.1xgb classifier figure global fairness after applying the upstream transformer left andafterapplyingboththeupstreamandone downstreamtransformer right .
usamp undersampling.
consider the use case of classification task on german credit datasetwithdifferentclassifierssimilartofigure .suppose the original pipeline is constructed using undersampling technique.
sincethispipelineexhibitsbias asshowninfigure canwechoose adownstreamdatastandardizingtransformerthatmitigatesthat bias?inthisusecase undersamplingistheupstreamtransformer and any standardizing transformer is the downstream transformer.
we showed the evaluation for xgb classifier and knc classifier since these two exhibits most bias when the upstream transformer wasappliedin .weplottedtheglobalfairnessafterapplyingonly theupstreamtransformerintheleftoffigure .wealsoreported thelocalfairnessofthestandardizingtransformersintable .now sinceundersamplingmethodexhibitsbiastowardsprivilegedgroup forxgb welookforthetransformerthatisbiasedtowardsprivilegedgroup.infigure amongothertransformers normalizer is the mostsuccessful to mitigatebiasofthe upstream transformer.
similarly for knc the upstream operator exhibits bias towards privilegedgroup.fromtable wecanseethat minmaxscaler is themostbiasedtransformertowardstheoppositedirection.asa result applying minmaxscaler mitigatesbiasthemost.notethat theotherdownstreamtransformersalsofollowthefairnesscompositionwithitsupstreamtransformer.therefore bymeasuring fairnessofthepreprocessingstages developerswouldbeableto instrument the biasedtransformersandbuildfair ml pipelines.
discussion we took the first step to understand the fairness of components in mlpipelines.ourmethodhelpstoprovidecausalityinsoftwareand reasonaboutbehaviorof components based ontheimpactonoutcome.thismethodcanbeextendedfurthertoevaluatethefairness ofothersoftwaremodules inmlpipelineandlocalizefaults .moreover wefoundmostofthestagesexhibitedbias toalow 990fair preprocessing towardsunderstandingcompositionalfairness of datatransformersin machinelearning pipeline esec fse august athens greece table3 localfairnessofstagesasdownstreamtransformer.
model stage sf spd sf eod sf aod sf erd xgbss .
.
.
.
mm .
.
.
.
ma .
.
.
.
ro .
.
.
.
no .
.
.
.
qt .
.
.
.
pt .
.
.
.
kncss .
.
.
.
mm .
.
.
.
ma .
.
.
.
ro .
.
.
.
no .
.
.
.
qt .
.
.
.
pt .
.
.
.
ss standardscaler mm minmaxscaler ma maxabsscaler ro robustscaler no normalizer qt quantiletransformer pt powertransformer orhigherdegree.thefairnessmeasuresofdifferentcomponents canbeleveragedtowardsfairness awarepipelineoptimizationto satisfy fairness constraints.
for example us equal employment commissionsuggestsselection ratedifferencebetweengroupsless than20 .also pipelineoptimizationtechniques e.g.
tpot lara can be potentially utilized for pipeline optimization.
furthermore research has been conducted to understand the impact of preprocessing stages with respect to performance improvement .
this paper will open research directions to develop preprocessingtechniques that improve performance by keeping the fairness intact.
we also reported a number of fairness patternsof preprocessingstagesthat inducing bias in thepipeline suchasmissingvalueprocessing customfeaturegeneration featureselection.moreover instrumentationofthestagescanmitigate theinherentbiasoftheclassifiers.itshowsopportunitiestobuild automated tools for identifying fairness bugs in ai systems and recommending fixes .
finally current fairness tools e.g.
aif aequitas can be augmented by incorporating data preprocessing stages into the pipelines and letting users have control over the data transformers and observe or mitigate bias.
similarly the libraries can provide api support to monitor fairness ofthe transformers.
threats to validity internal validity refers to whether the fairness measures used in this paper actually captures the fairness of preprocessing stages.
to mitigate this threat we used existing concepts and metrics to build new set of metrics.
causality in software has been well studied andcausalreasoninginfairnesshasalsobeenpopular since it can provide explanation with respect to changeintheoutcome.besides thismethoddonotrequireanoracle becausethepredictionequivalencesprovidenecessaryinformation to measure the impact of the intervention .
furthermore in 7 we conducted experiments on local and global fairness to show hownewmetrics composes inthe pipeline.
externalvalidity isconcernedabouttheextentthefindingsof thisstudycanbegeneralized.toalleviatethisthreat weconducted experiments ona largenumberof pipelinevariations.we collected thepipelines fromthree differentsources.
moreover we collected alternative transformers from the ml libraries for comparative analysis.
finally for the same dataset categories we used multiple classifiersandfairnessmetricssothatthefindingsarepersistent.
related works fairness in ml classification.
the machine learning community has defined different fairness criteria and proposed metrics to measure the fairness of classification tasks .followingthemeasurementoffairnessinmlmodels many mitigation techniques have also been proposed to remove bias .
this body of work mostlyconcentratesonthetheoreticalaspectoffairnessinasingle classification task.
recently software engineering community hasalsofocusedonthefairnessinml mostlyonfairnesstesting .
these works propose methods to generate appropriate test data inputs for the model and prediction on those inputs characterizes fairness.
some research has been conducted to build automated tools and libraries for fairness.
in addition empiricalstudieshavebeenconductedtocompare contrast between fairness aspects interventions tradeoffs developersconcerns andhuman aspectsof fairness .
fairness in composition.
dwork and ilvento argued that fairness is dynamic in a multi component environment .
they showed that when multiple classifiers work in composition even if the classifiers are fair in isolation the overall system is not necessarily fair.
bower et al.discussed fairness in ml pipeline where they considered pipelineas sequence of multiple classification tasks .
they also showed that when decisions of fair components are compounded the final decision might not be fair.
for example whileinterviewingcandidatesintwostages fairdecisionineach stagemaynotguaranteeafairselection.
d amour etal.studiedthe dynamicsoffairnessinmulti classificationenvironnementusing simulation .intheseresearch fairnesscompositionisshown over multiple tasks and the authors did not consider fairness of components in single ml pipeline.
we position our paper here to study the impact of preprocessing stages in ml pipeline and evaluate the fairnesscomposition.
conclusion data preprocessing techniques are used in most of the machine learningpipelinesincompositionwiththeclassifier.studiesshowed thatfairnessofmachinelearningpredictionsdependslargelyon the data.
in this paper we investigated how the data preprocessing stages affect fairness of classification tasks.
we proposed the causalmethodandleveragedexistingmetricstomeasurethefairnessofdatapreprocessingstages.theresultsshowedthatmany stages induce bias in the prediction.
by observing fairness of these data transformers fairer ml pipelines can be built.
in addition we showedthatexistingbiascanbemitigatedbyselectingappropriatetransformers.wereleasedthepipelinebenchmark code and results to make our techniques available for further usages.
future research can be conductedtowards developing automated tools to detectbiasinml pipeline stagesandinstrument that accordingly.