cc2vec distributed representations of code changes thong hoang hong jin kang david lo singapore management university singapore vdthoang.
hjkang.
davidlo smu.edu.sgjulia lawall sorbonne university inria lip6 france julia.lawall inria.fr abstract existing work on software patches often use features specific to a single task.
these works often rely on manually identified features and human effort is required to identify these features for each task.
in this work we propose cc2vec a neural network model that learns a representation of code changes guided by their accompanying log messages which represent the semantic intent of the code changes.
cc2vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.
to evaluate if cc2vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches we use the vectors produced by cc2vec for three tasks log message generation bug fixing patch identification and just in time defect prediction.
in all tasks the models using cc2vec outperform the state of the art techniques.
introduction patches used to edit source code are often created by developers to describe new features fix bugs or maintain existing functionality e.g.
api updates refactoring etc.
.
patches contain two main pieces of information a log message and a code change.
the log message used to describe the semantics of the code changes is written in natural language by the developers.
the code change indicates the lines of code to remove or add across one or multiple files.
research has shown that the study of historical patches can be employed to solve software engineering problems such as just in time defect prediction identification of bug fixing patches tangled change prediction recommendation of a code reviewer for a patch and many more.
exploring patches to solve software engineering problems requires choosing a representation of the patch data.
most prior work involves manually crafting a set of features to represent a patch and using these features for further processing .
these features have mostly been extracted from properties of patches such as the modifications to source code e.g.
number of removed and added lines the number of files modified the history of changes e.g.
the number of prior or recent changes to the updated files the record of patch authors and reviewers e.g.
the number of developers or reviewers who contributed to the patch etc.
these permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
can be used as an input to a machine learning classifier e.g.
support vector machine logistic regression random forest etc.
to address various software engineering tasks .
extracting a suitable vector representation to represent the meaning of a patch is certainly crucial.
intuitively the quality of a patch representation plays a major role in determining the eventual learning outcome.
in this paper to boost the effectiveness of existing solutions that employ the properties of patches we wish to learn vector representations of the code changes in patches that can be used for a number of tasks.
we propose a new deep learning architecture named cc2vec that can effectively embed a code change into a vector space where similar changes are close to each other.
as log messages written by developers are used to describe the semantics of the code change we use them to supervise the learning of code changes representations from patches.
specifically cc2vec optimizes the vector representation of a code change in a patch to predict appropriate words extracted from the first line of the log message.
we consider only the first line as it is the focus of many prior works and is considered to carry the most semantic meaning with the least noise.
cc2vec analyzes the code change i.e.
scattered fragments of removed and added code across multiple files.
code removed or added from a file follows a hierarchical structure words form line lines form hunks .
recent work has suggested that the attention mechanism can help in modelling structural dependencies thus we hypothesize that the attention mechanism may be effective for modelling the structure of a code change.
we propose a specialized hierarchical attention network han to construct a vector representation of the removed code and another for the added code of each affected file in a given patch.
our han first builds vector representations of lines these vectors are then used to construct vector representations of hunks and we then aggregate these vectors to construct the embedding vector of the removed or added code.
next we employ multiple comparison functions to capture the difference between two embedding vectors representing removed and added code.
this produces features representing the relationship between the removed and added code.
each comparison function produces a vector and these vectors are then concatenated to form an embedding vector for the affected file.
finally the embedding vectors of all the affected files are concatenated to build a vector representation of the code change in a patch.
after training is completed cc2vec can be used to extract representations of code changes even from patches with empty or meaningless log messages which are common in practice .
cc2vec is also programminglanguage agnostic one can use it to learn vector representations of code changes for any language.
ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea thong hoang hong jin kang david lo and julia lawall the code change representation enables us to employ the power of potentially a large number of unlabeled patch data to improve the effectiveness of supervised learning tasks also known as semisupervised learning .
we can use the code change representation to boost the effectiveness of many supervised learning tasks e.g.
identification of bug fixing patches just in time defect prediction etc.
especially on those tasks for which only a limited set of labeled data may be available.
cc2vec converts code changes into their distributed representations by learning from a large collection of patches.
the distributed representation captures pertinent features of the code changes by considering the characteristics of the whole collection of patches.
such distributed representations can be used as additional features for other tasks.
past studies have demonstrated the value of distributed representations to improve text classification action recognition image classification etc.
unfortunately prior to our work there is no existing solution that can produce a distributed representation of a code change.
to evaluate the effectiveness of cc2vec we employ the representation learned by cc2vec in three software engineering tasks log message generation bug fixing patch identification and just in time defect prediction .
in the first task of log message generation we generate the first line of a log message given a code change.
cc2vec can be used to improve over the best baseline by .
in terms of bleu score an accuracy measure that is widely used to evaluate machine translation systems .
for the task of identifying bug fixing patches cc2vec helps to improve the best performing baseline by .
.
.
and .
in terms of accuracy precision f1 and area under the curve auc .
for just in time defect prediction cc2vec helps to improve the auc metric by .
and .
on the qt and openstack datasets as compared to the best baseline.
the main contributions of this work are as follows we propose a deep learning architecture namely cc2vec that learns distributed representations of code changes guided by the semantic meaning contained in log messages.
to the best of our knowledge our work is the first work in this direction.
we empirically investigate the value of integrating the code change vectors generated by cc2vec and feature vectors used by state of the art approaches on three tasks i.e.
log message generation bug fixing patch identification and justin time defect prediction and demonstrate improvements.
the rest of this paper is organized as follows.
section elaborates the design of cc2vec.
section describes the experiments that demonstrate the value of our learned code change representations to aid in the three different tasks.
section presents an ablation study and some threats to validity.
section describes related prior studies.
we conclude and mention future work in section .
approach in this section we first present an overview of our framework.
we then describe the details of each part of the framework.
finally we present an algorithm for learning effective settings of our model s parameters.
file !
hierarchical attention network code change vector fully connected layer for mapping code change vector a set of words extracted from log messagefeature fusion layers word prediction layerfile input layer feature extraction layerspreprocessing e 1e 2encodingcode changesfigure the overall framework of cc2vec.
feature extraction layers are used to construct the embedding vectors for each affected file from a given patch i.e.
e f1 ef2 etc .
the embedding vectors are then concatenated to build a vector representation for the code change in the patch code change vector .
the code change vector is connected to the fully connected layer and is learned by minimizing an objective function of the word prediction layer.
.
framework overview figure illustrates the overall framework of cc2vec.
cc2vec takes the code change of a patch as input and generates its distributed representation.
cc2vec uses the first line of the log message of the patch to supervise learning the code change representation.
specifically the framework of cc2vec includes five parts preprocessing this part takes information from the code change of the given patch as an input and outputs a list of files.
each file includes a set of removed code lines and added code lines.
input layer this part encodes each changed file as a threedimensional matrix to be given as input to the hierarchical attention network han for extracting features.
feature extraction layers this part extracts the embedding vector a.k.a.
features of each changed file.
the resulting embedding vectors are then concatenated to form the vector representation of the code change in a given patch.
519cc2vec distributed representations of code changes icse may seoul republic of korea feature fusion layers and word prediction layer this part maps the vector representation of the code change to a word vector extracted from the first line of log message the word vector indicates the probabilities that various words describe the patch.
cc2vec employs the first line of the log message of a patch to guide the learning of a suitable vector that represents the code change.
words extracted from the first line of log message can be viewed as semantic labels provided by developers.
specifically we define a learning task to construct a prediction function f p y whereyi y indicates the set of words extracted from the first line of the log message of the patch pi p. the prediction function fis learned by minimizing the differences between the predicted and actual words chosen to describe the patch.
after the prediction function fis learned for each patch we can obtain its code change vector from the intermediate output between the feature extraction and feature fusion layers see figure .
we explain the details of each part in the following subsections.
.
preprocessing the code change of the given patch includes changes made to one or more files.
each changed file contains a set of lines of removed code and added code.
we process the code change of each patch by the following steps split the code change based on the affected files.
we first separate the information about the code change to each changed file into a separate code document i.e.
file1 file2 etc.
see figure .
tokenize the removed code and added code lines.
for the changes affecting each changed file we employ the nltk library for natural language processing nlp to parse its removed code lines or added code lines into a sequence of words.
we ignore blank lines in the changed file.
construct a code vocabulary.
based on the code changes of the patches in the training data we build a vocabulary vc.
this vocabulary contains the set of code tokens that appear in the code changes of the collection of patches.
at the end of this step all the changed files of the given patch are extracted from the code changes and they are fed to the input layer of our framework for further processing.
.
input layer a code change may include changes to multiple files the changes to each file may contain changes to different hunks and each hunk contains a list of removed and or added code lines.
to preserve this structural information in each changed file we represent the removed added code as a three dimensional matrix i.e.
b rh l w wherehis the number of hunks lis the number of removed added code lines for each hunk and wis the number of words in each removed added code line in the affected file.
we use brandbato denote the three dimensional matrix of the removed and added code respectively.
note that each patch may contain a different number of affected files f each file may contain a different number of hunks h each hunk may contain a different number of lines l and each hierarchical attention network eaer comparison layersef feature extraction layersfigure architecture of the feature extraction layers for mapping the code change of the affected file in a given patch to an embedding vector.
the input of the module is the removed code and added code of the affected file denoted by and respectively.
line may contain a different number of words w .
for parallelization each input instance is padded or truncated to the same f h l andw.
.
feature extraction layers the feature extraction layers are used to automatically build an embedding vector representing the code change made to a given file in the patch.
the embedding vectors of code changes to multiple files are then concatenated into a single vector representing the code change made by the patch.
as shown in figure for each affected file the feature extraction layers take as input two matrices denoted by and in figure representing the removed code and added code respectively.
these two matrices are passed to the hierarchical attention network to construct corresponding embedding vectors errepresenting the removed code and earepresenting the added code see figure .
these two embedding vectors are fed to the comparison layers to produce the vectors representing the difference between the removed code and the added code.
these vectors are then concatenated to represent the code changes in each affected file.
we present the hierarchical attention network and the comparison layers in the following sections.
.
.
hierarchical attention network.
the architecture of our hierarchical attention network han is shown in figure .
a han takes the removed added code of an affected file of a given patch as an input and outputs the embedding vector representing the removed added code.
our han consists of several parts a word sequence encoder a word level attention layer a line encoder a line level attention layer a hunk sequence encoder and a hunk attention layer.
suppose that the removed added code of the affected file contains a sequence of hunks h each hunktiincludes a sequence of lines and each line sijcontains a sequence of words .wijkwithk represents the word in the j th line in the i th hunk.
now we describe how the embedding vector of the removed added code is built using the hierarchical structure.
520icse may seoul republic of korea thong hoang hong jin kang david lo and julia lawall w221h221h221uw 221 w222h222h222 w22wh22wh22w 222 22ws21s22h21h21 h22h22 s2lh2lh2lus 21 22 2l t1 t2h1h1 h2h2ut 1 2 h thhhhhe word encoderword attentionline encoderline attentionhunk encoderhunk attention figure the overall framework of our hierarchical attention network han .
the han takes as input the removed added code of the affected file of a given patch and outputs the embedding vector denoted by e of the removed added code.
word encoder.
given a line sijwith a sequence of words wijk and a word embedding matrix w r vc d wherevcis the vocabulary containing all words extracted from the code changes anddis the dimension of the representation of word we first build the matrix representation of each word in the sequence as follows wijk w wherewijk rdindicates the vector representation of word wijk in the word embedding matrix w. we employ a bidirectional gru to summarize information from the context of a word in both directions .
to capture this contextual information the bidirectionalgru includes a forward gru that reads the line sijfromwij1to wijwand a backward gru that reads the line sijfromwijwto wij1.
hijk gru wijk k hijk gru wijk k we obtain an annotation of a given word wijkby concatenating the forward hidden state hijkand the backward hidden state hijkof this word i.e.
hijk is the concatenation operator .
hijksummarizes the word wijkconsidering its neighboring words.
word attention.
based on the intuition that not all words contribute equally to extract the meaning of the line we use the attention mechanism to highlight words important for predicting the content of the log message.
the attention mechanism was previously used in source code summarization and was shown to be effective for encoding source code sequences .
we also use the attention mechanism to form an embedding vector of the line.
we first feed an annotation of a given word wijk i.e.
hijk through a fully connnected layer i.e.
ww to get a hidden representation i.e.
uijk ofhijkas follows uijk relu wwhijk bw where relu is the rectified linear unit activation function as it generally provides better performance in various deep learning tasks .
similar to yang et al.
we define a word context vector uw that can be seen as a high level representation of the answer to the fixed query what is the most informative word over the words.
the word context vector uwis randomly initialized and learned during the training process.
we then measure the importance of the word as the similarity of uijkwith the word context vector uwand get a normalized importance weight ijk through a softmax function ijk exp ut ijkuw kexp ut ijkuw for each line sij its vector is computed as a weighted sum of the embedding vectors of the words based on their importance as follows sij k ijkhijk line encoder.
given a line vector i.e.
sij we also use a bidirectional gru to encode the line as follows hij gru sij j hij gru sij j similar to the word encoder we obtain an annotation of the line sijby concatenating the forward hidden state hijand backward hidden state hijof this line.
the annotation of the line sijis denoted ashij which summarizes the line sijconsidering its neighboring lines.
521cc2vec distributed representations of code changes icse may seoul republic of korea line attention.
we use an attention mechanism to learn the important lines to be used to form a hunk vector as follows uij relu wshij bs ij exp ut ijus jexp ut ijus ti j ijhij wsis the fully connected layer to which we need to feed an annotation of the given line i.e.
sij .
we define usas the line context vector that can be seen as a high level representation of the answer to the fixed query what is the informative line over the lines.
us is randomly initialized and learned during the training process.
ti is the hunk vector of the i th hunk in the removed added code.
hunk encoder.
given a hunk vector ti we again use a bidirectional gru to encode the hunk as follows hi gru ti t hi gru ti t an annotation of the hunk tiis then obtained by concatenating the forward hidden state hiand the backward hidden state hi i.e.
hi .hisummarizes the hunk ticonsidering the other hunks around it.
hunk attention.
we again use an attention mechanism to learn important hunks used to form an embedding vector of the removed added code as follows ui relu whhi bh i exp ut iut iexp ut iut e i ihi whis the fully connected layer used to feed an annotation of a given hunk i.e.
hi .utis the hunk context vector that can be seen as a high level representation of the answer to the fixed query what is the informative hunk over the hunks.
similar to uwandus utis randomly initialized and learned during the training process.
e collected at the end of this part is the embedding vector of the removed added code.
for convenience we denote erandea as the embedding vectors of the removed code and added code respectively.
.
.
comparison layers.
the goal of the comparison layers is to build the vectors that capture the differences between the removed code and added code of the affected file in a given patch.
we use multiple comparison functions to represent different angles of comparison.
these comparison functions were previously used in a question answering task.
the comparison layers take as input the embedding vectors of the removed code and added code denoted byerandea respectively and output the vectors representing the difference between the removed code and the added code.
these vectors are then concatenated to represent an embedding vector erea esim er ea enn er eacosine euclidean a neural tensor network nt b neural network nn c similarity sim ent er easubtraction er eamultiplicationesub emul d element wise subtraction e element wise multiplicationfigure a list of comparison functions in the comparison layers.
of the affected file in the given patch.
figure shows the five comparison functions used in the comparison layers to capture the difference between the removed code and added code.
we briefly explain these comparison functions in the following paragraphs.
a neural tensor network.
inspired by previous works in visual question answering we employ a neural tensor network as follows ent relu et rt ea bnt ti rn nis a tensor and bntis the bias value.
these parameters are learned during the training process.
note that both the removed code and added code have the same dimension i.e.
er rn ea .
b neural network.
we consider a simple feed forward neural network .
the output is computed as follows enn relu w bnn is the concatenation operator the matrix w rn 2n and the bias valuebnnare parameters to be learned.
c similarity.
we employ two different similarity measures euclidean distance and cosine similarity to capture the similarity 522icse may seoul republic of korea thong hoang hong jin kang david lo and julia lawall word prediction layercode changes vector word word word word word vm word vectoref1 ef2 eff hidden layer ep figure the details of the red dashed box in figure .
it takes as input a list of embedding vectors of the affected files of a given patch i.e.
e f1 ef2 ... eff .
epis the vector representation of the code change and is fed to a hidden layer to produce the word vector i.e.
the probability distribution over words .vmis a set of words extracted from the first line of the log messages.
between the removed code and added code as follows esim euc er ea cos er ea euc er ea er ea cos er ea erea er ea euc andcos are the euclidean distance and cosine similarity respectively.
note that esimis a two dimensional vector.
d element wise subtraction.
we simply perform a subtraction between the embedding vector of the removed code and the embedding vector of the added code.
esub er ea e element wise multiplication.
we perform element wise multiplication for the embedding vectors of the removed code and added code.
emul er ea where is the element wise multiplication operator.
the vectors resulting from applying these five different comparison functions are then concatenated to represent the embedding vector of the affected file denoted by efi in the given patch as follows efi ent enn esim esub emul where fiis the i th file of the code change in the given patch.
.
feature fusion and word prediction layers figure shows the details of the part of the architecture shown inside the red dashed box in figure .
the inputs of this part are the list of embedding vectors i.e.
ef1 ef2 ... eff representing the features extracted from the list of affected files of a given patch.
these embedding vectors are concatenated to construct a new embedding vector ep representing the code change in a given patch as follows ep ef1 ef2 eff we pass the embedding vector ep into a hidden layer a fully connected layer to produce a vector h h whep bh where whis the weight matrix used to connect the embedding vector epwith the hidden layer and bhis the bias value.
finally the vector his passed to a word prediction layer to produce the following o hw o where wois the weight matrix between the hidden layer and the word prediction layer and o r vm vmis a set of words extracted from the first line of log messages .
we then apply the sigmoid function to get the probability distribution over words as follows p oi pi exp oi whereoi ois the probability score of the ithword andpiis the patch that we want to assign words to.
.
parameter learning our model involves the following parameters the word embedding matrix of code changes the hidden states in the different encoders i.e.
the word encoder line encoder and hunk encoder the context vectors of words lines and hunks the weight matrices and the bias values of the neural tensor network and the neural net in the comparison layers and the weight matrices and the bias values of the hidden layer and the word prediction layer.
after these parameters are learned the vector representation of the code change of each patch can be determined.
these parameters are learned by minimizing the following objective function o yi y yi log p oi pi yi log p oi pi where p oi pi is the predicted word probability defined in equation yi indicates whether the i th word is part of the log message of the patch pi and are all parameters of our model.
the regularization term is used to prevent overfitting in the training process .
we employ the dropout technique to improve the robustness of cc2vec.
since adam has been shown to be computationally efficient and require low memory consumption we use it to minimize the objective function i.e.
equation .
we also use backpropagation a simple implementation of the chain rule of partial derivatives to efficiently update the parameters during the training process.
experiments the goal of this work is to build a representation of code changes that can be applied to multiple tasks.
to evaluate the effectiveness of this representation we employ our framework namely cc2vec on three different tasks i.e.
log message generation bug fixing patch identification and just in time defect prediction .
523cc2vec distributed representations of code changes icse may seoul republic of korea in the first task of log message generation we use the vector representation of code changes extracted by cc2vec to find a patch that is most similar to another.
for the other two tasks cc2vec is used to extract additional features that are input to the models of bug fixing patch identification and just in time defect prediction.
we compare the resulting performance with and without using our code change vector.
we next elaborate on the three tasks the baselines and results.
.
task log message generation .
.
problem formulation.
while we learn representations of code changes with the aid of log messages we also study the task of generating log messages from code changes.
developers do not always write high quality log messages.
dyer et al.
reported that around of log messages in java projects on sourceforge2 were empty.
log messages are important for program comprehension and understanding the evolution of software therefore this motivates the need for the automatic generation of log messages.
in this task given the code change of a given patch we aim to produce a brief log message summarizing it.
.
.
state of the art approach.
the state of the art approach is nngen which takes as input a new code change with an unknown log message and a training dataset patches and outputs a log message for the new code change.
nngen first extracts code changes from the training set.
each code change in the training set and the new code change are represented as vectors in the form of a bag of words .
nngen then calculates the cosine similarity between the vector of the new code change and the vector of each code change in the training set and selects the top k nearest neighbouring code changes in the training dataset.
from these k nearest neighbours the bleu score is computed between each of the code changes in the top k and the new code change with an unknown log message.
a log message of the code change in the top k with the highest bleu score is reused as the log message of the new code change.
the bleu score is a measure used to evaluate the quality of machine translation systems measuring the closeness of a translation to a human translation.
it is computed as follows bleu bp exp n n nlog pn bp ifc r e r c ifc r nis the maximum number of n grams.
following the previous work we selectn .pnis the ratio of length nsubsequences that are present in both the output and reference translation.
bp is a brevity penalty to penalize short output sentences.
finally c is the length of the output translation and ris the length of the reference translation.
a deep learning approach was previously proposed for this task by jiang et al.
however it underperformed the simpler baseline nngen.
in this study we refer to their work as nmt.
their approach modelled this task as a neural machine translation task translating the code change to a target log message.
like our work they proposed an attention based model however our work differs from theirs as ours incorporates the structure of code changes.
liu performance of each approach on the original and cleaned dataset reported in bleu loggen nngen nmt original .
.
.
clean .
.
.
et al.
investigated the performance of jiang et al.
s attention model they found that once they remove trivial and automaticallygenerated messages the performance of the model decreased significantly suggesting that this model does not generalize.
.
.
our approach.
to use cc2vec for this task we propose loggen.
similar to the nearest neighbours approach used by liu et al.
loggen reuses and outputs a log message from the training set.
however instead of treating each code change as a bag of words loggen uses code change vectors produced by cc2vec.
cc2vec is first trained over the training dataset.
given a new code change from the test dataset with an unknown log message we find the code changes with a known log message that have the closest cc2vec vector.
like liu et al.
after identifying the closest code changes we reuse the log message as the output.
.
.
experimental setting.
the purpose of evaluating cc2vec on this task is to determine if the code change representations received from cc2vec outperform the naive representation used by liu et al.
.
jiang et al.
originally collected and filtered the commits to construct the original dataset.
another version of the dataset was used by liu et al.
who modified the original dataset.
jiang et al.
extracted a total of million patches from the 1k most starred java projects.
they collected the first line of each log message.
to normalize the dataset patch ids and issue ids were removed from the code changes and log messages.
patches were filtered to remove merges rollbacks and patches that were too long.
the log messages that do not conform to verb direct object pattern e.g.
delete a method are also removed.
after filtering the dataset contains 32k patches.
still even with all this cleaning liu et al.
investigated the dataset and found that there were many patches with bot messages and trivial messages.
bot messages refer to messages produced automatically by other development tools such as continuous integration bots.
trivial messages refer to messages containing only information that can be obtained by looking at the names of the changed files e.g.
modify dockerfile .
such messages are of low quality and liu et al.
used regular expressions to locate and remove these patches.
we used the original dataset of jiang et al.
and the cleaned dataset of liu et al.
for evaluation.
while the original dataset consists of a training dataset of 30k patches and a testing dataset of 3k patches the cleaned dataset consists of a training dataset of 22k patches and a testing dataset of .5k patches.
to compare the different approaches we use bleu to evaluate each approach since this was used in both previous works.
.
.
results.
we report the performance of loggen nngen and nmt in table .
loggen outperforms both nngen and nmt.
the clean dataset refers to the dataset which liu et al.
filtered out 524icse may seoul republic of korea thong hoang hong jin kang david lo and julia lawall patches with bot and trivial log messages.
on this dataset loggen outperforms nngen and nmt by a bleu score of .
and .
respectively.
loggen improves over the performance of nngen by .
a greater improvement than nngen s improvement over nmt of .
.
on the original dataset collected by jiang et al.
loggen outperforms nngen and nmt by a bleu score of .
and .
.
these results indicate that loggen can improve over the performance of nngen and nmt by .
and .
in terms of the bleu score respectively.
thus we conclude that the log messages retrieved by loggen are closer in quality to a human translation than those retrieved by nngen and the log messages generated by nmt.
this suggests that cc2vec produces vector representations of patches that correlate to the meaning of the patch more strongly than a bag of words.
.
task bug fixing patch identification .
.
problem formulation.
software requires continuous evolution to keep up with new requirements but this also introduces new bugs.
backporting bugfixes to older versions of a project may be required when a legacy code base is supported.
for example linux kernel developers regularly backport bugfixes from the latest version to older versions that are still under support.
however the maintainers of older versions may overlook relevant patches in the latest version.
thus an automated method to identify bug fixing patches may be helpful.
we treat the problem as a binary classification problem in which each patch is labelled as a bug fixing patch or not.
given the code change and log message we produce one of the two labels as the output.
.
.
state of the art approach.
the state of the art approach is patchnet which represents the removed added code as a three dimensional matrix.
the dimensions of the matrix are the number of hunks the number of lines in each hunk and the number of words in each line.
patchnet employs a 3d cnn that automatically extracts features from this matrix.
unfortunately the 3d cnn lacks a mechanism to identify important words lines and hunks.
to address this limitation we propose a specialized hierarchical attention neural network to quantify the importance of words lines and hunks in our model cc2vec .
another approach was proposed by tian et al.
that combines learning from positive and unlabelled examples lpu and support vector machine svm to build a patch classification model.
unlike cc2vec this approach requires the use of manually selected features.
these features include word features which is a bag of words extracted from log messages and features manually extracted from the code change e.g.
the number of loops added in a patch and if certain words appear in the log message .
.
.
our approach.
cc2vec is first used to learn a distributed representation of code changes on the whole dataset.
all patches from the training and test dataset are used since the log messages of the test dataset are not the target of the task.
next we integrate these vector representations of the code changes with the two existing approaches.
to use cc2vec in patchnet we concatenate the vector representation of the code change extracted by cc2vec with the two embedding vectors extracted from the log message and code change by patchnet to form a new embedding vector.
thetable evaluation of the approaches on the bug fixing patch identification task acc.
prec.
recall f1 auc lpu svm .
.
.
.
.
lpu svm cc2vec .
.
.
.
.
patchnet .
.
.
.
.
patchnet cc2vec .
.
.
.
.
new embedding vector is fed into patchnet s classification module to predict whether a given patch is a bug fixing patch.
for the approach proposed by tian et al.
which uses an svm as the classifier we pass the vectors produced by cc2vec from the code change into the svm as features.
.
.
experimental setting.
the goal of this task is to investigate if cc2vec helps existing approaches to effectively classify bug fixing patches.
we use the dataset of linux kernel bug fixing patches used in the patchnet paper.
this dataset consists of 42k bug fixing patches and 40k non bug fixing patches collected from the linux kernel versions v3.
to v4.
released in july and july respectively.
patches in this dataset are limited to lines of changed code in line with the linux kernel stable patch guidelines.
the nonbug fixing patches are selected such that they have a similar size in terms of the number of files and the number of modified lines as the bug fixing patches.
following the patchnet paper we use fold cross validation for the evaluation.
to compare the performance of the approaches we employ the following metrics accuracy the ratio of correct predictions to the total number of predictions.
precision the ratio of correct predictions of bug fixing patches to the total number of bug fixing patch predictions recall the ratio of correct predictions of bug fixing patches to the total number of bug fixing patches.
f1 harmonic mean between precision and recall.
auc area under the curve plotting the true positive rate against the false positive rate.
auc values range from to with a value of indicating perfect discrimination.
these metrics were also used in previous studies on this task.
.
.
results.
we report the performance of the different approaches in table .
we observe that the best performing approach is patchnet augmented with cc2vec.
for both tian et al.
s model lpusvm and patchnet the versions augmented with cc2vec outperform the original versions.
specifically cc2vec helps to improve the best performing baseline i.e patchnet by .
.
.
and .
in terms of accuracy precision f1 and auc.
cc2vec also helps to improve the performance of lpu svm by .
.
.
.
and .
in accuracy precision recall f1 and auc.
this suggests that cc2vec can learn patch representations that are general and useful beyond the task it was trained on.
.
task just in time defect prediction .
.
problem formulation.
the task of just in time jit defect prediction refers to the identification of defective patches.
jit defect 525cc2vec distributed representations of code changes icse may seoul republic of korea table the auc results of the various approaches qt openstack deepjit .
.
deepjit cc2vec .
.
prediction tools provide early feedback to software developers to optimize their effort for inspection and have been used at large software companies .
we model the task as a binary classification task in which each patch is labelled as a patch containing a defect or not.
given a patch containing a code change and a log message with unknown label we label the patch with one of the two labels.
.
.
state of the art approach.
the state of the art approach is deepjit proposed by hoang et al.
.
deepjit takes as input the log message and code change of a given patch and outputs a probability score to predict whether the patch is buggy.
deepjit employs a convolutional neural network cnn to automatically extract features from the code change and log message of the given patch.
however deepjit ignores information about the structure of the removed code or added code instead relying on cnn to automatically extract such information.
.
.
our approach.
similar to the previous task i.e.
bug fixing patch identification cc2vec is first used to learn distributed representations of the code changes in the whole dataset.
all patches from the training and test dataset are used since the log messages of the test dataset are not part of the predictions of the task.
we then integrate cc2vec with deepjit.
to use cc2vec with deepjit for each patch we concatenate the vector representation of the code change extracted by cc2vec with two embedding vectors extracted from the log message and code change of the given patch extracted by deepjit to form a new embedding vector.
the new embedding vector is fed into deepjit s feature combination layers to predict whether the given patch is defective.
.
.
experimental setting.
the purpose of this task is to evaluate if cc2vec can be used to augment existing approaches in effectively classifying defective patches.
our evaluation is performed on two datasets the qtandopenstack datasets which contain patches collected from the qt and openstack software projects respectively by mcintosh and kamei .
the qt dataset contains 25k patches over years and months while the openstack dataset contains 12k patches over years and months.
and of the patches are defective in the qt dataset and the openstack datasets respectively.
like hoang et al.
we use fold cross validation for the evaluation.
to compute the effectiveness of the approaches we use the area under the receiver operator characteristics curve auc similar to the previous studies.
.
.
results.
the evaluation results for this task are reported in table .
the use of cc2vec with deepjit improves the aucs score of deepjit from .
and .
to .
and .
on the qt and openstack datasets respectively.
specifically cc2vec helps to improve the auc metric by .
and .
for the qt and openstack datasets respectively as compared to deepjit.
thisindicates that cc2vec is effective in learning a useful representation of patches that an existing state of the art technique can utilize.
discussion .
ablation study our approach involves five comparison functions for calculating the difference between the removed code and added code.
to estimate the usefulness of comparison functions see section .
.
we conduct an ablation study on the three tasks log message generation bug fixing patch identification and just in time defect prediction.
specifically we first remove the comparison functions entirely and then remove these functions one by one.
for each task we compare the cc2vec model and its six reduced variants all all omit all comparison functions all nt omit the neural network tensor comparison function all nn omit the neural network comparison function all sim omit the similarity comparison function all sub omit the subtraction comparison function and all mul omit the multiplication comparison function .
table summarizes the results of our ablation test on three different tasks.
we see that cc2vec model always performs better than the reduced variants for all three tasks.
this suggests that each comparison function plays an important role and omitting these comparison functions may greatly affect the overall performance.
all all cc2vec model without using any comparison functions performs the worst.
among the five remaining variants i.e.
all nt all nn all sim all sub and all mul all nt performs the worst.
this suggests that the neural network tensor comparison function is more important the other comparison functions i.e.
neural network similarity subtraction and multiplication .
.
threats to validity threats to internal validity refer to errors in our experiments and experimenter bias.
for each task we reuse existing implementations of the baseline approaches whenever available.
we have double checked our code and data but errors may remain.
threats to external validity concern the generalizability of our work.
in our experiments we have studied only three tasks to evaluate the generality of cc2vec.
this may be a threat to external validity since cc2vec may not generalize beyond the tasks that we have considered.
however each task involves different software projects and different programming languages.
as such we believe that there is minimal threat to external validity.
to minimize threats to construct validity we have used the same evaluation metrics that were used in previous studies.
related work there are many studies on the representation of source code including recent studies proposing distributed representations for identifiers apis and software libraries .
a comprehensive survey of learning the representation of source code has been done by allamanis et al.
.
some studies transform the source code into a different form such as control flow graphs and symbolic traces or collect runtime execution traces before learning distributed representations.
defreez et al.
found function synonyms by learning 526icse may seoul republic of korea thong hoang hong jin kang david lo and julia lawall table results of an ablation study log generation bleu bug fix identification f1 just in time defect prediction auc clean drops by bfp drops by qt drops by openstack drops by all all .
.
.
.
.
.
.
.
all nt .
.
.
.
.
.
.
.
all nn .
.
.
.
.
.
.
.
all sim .
.
.
.
.
.
.
.
all sub .
.
.
.
.
.
.
.
all mul .
.
.
.
.
.
.
.
all .
.
.
.
embeddings through random walks of the interprocedural controlflow graph of a program.
these embeddings are then used in a single downstream task of mining error handling specifications.
henkel et al.
described a toolchain to produce abstracted intraprocedural symbolic traces for learning word embeddings.
they experimented on a downstream task to find and repair bugs related to incorrect error codes.
wang et al.
used execution traces to learn embeddings.
they integrate their embeddings into a program repair system in order to produce fixes to correct student errors in programming assignments.
these studies differ from our work as we leverage natural language data as well as source code.
there have been other studies using deep learning of both source code and natural language data for example joint learning of embeddings for both text and source code to improve code search .
other studies proposed approaches to learn distributed representations of source code on prediction tasks with natural language output.
iyer et al.
proposed a model using lstm networks with attention for code summarization and yin et al.
trained a model to align source code to natural language text from stackoverflow posts.
however unlike our work these studies do not use structural information of the source code.
several studies account for structural information but differ from our work.
hu et al.
proposed an approach to use sequence to sequence neural machine translation to generate method level code comments.
by prefixing the ast node type in each token and traversing the ast of methods such that the original ast can be unambiguously reconstructed they convert the ast of each method into a sequence that preserves structural information.
alon et al.
proposed code2vec which represents code as paths in an ast learning the vector representation of each ast path.
they trained their model on the task of predicting a label such as the method name of the code snippet.
in a later work they proposed code2seq .
instead of predicting a single label they generate a sequence of natural language words.
similar to our work structural information of the input source code is encoded in the model s architecture however in these studies the input code snippet is required to be parseable to build an ast.
as our work focuses on the representation of software patches we deliberately designed cc2vec to not require parseable code in its input.
this is done for two reasons.
firstly a small but still significant proportion of patches may have compilation errors.
a study by beller et al.
on travis ci build failures revealed that about of java project build failures are due to compilation errors .
cc2vec is designed to be usable even for these patches.
secondly parsing will require the entire file with the changed code.
retrieving this information and parsing the entire file will be time consuming.
all the studies above proposed general representations of source code.
the representations they learn with the exception of defreez et al.
are of source code contained in a single function.
in contrast we learn representations of code changes which can contain modifications to multiple different functions across multiple files.
several of the models related to code changes representation were discussed in section .
these models often do not model the hierarchical structure of a code change or require handcrafted features that may be specific to a single task .
two techniques using deep neural networks patchnet and deepjit are most similar to our work.
however as discussed earlier our work differs from theirs in various ways.
a fundamental difference is in the generality of the techniques.
cc2vec is not specific to a single task.
rather cc2vec can be trained for multiple tasks including both generative and classification tasks.
in fact cc2vec is orthogonal to these approaches.
the objective of cc2vec is to produce high quality representations of code changes that can be integrated into patchnet deepjit and similar models.
we showed in section that the performance of these models improves when they are augmented with the code change representation learned by cc2vec.
conclusion we propose cc2vec which produces distributed representations of code changes through a hierarchical attention network.
in cc2vec we model the structural information of a code change and use the attention mechanism to identify important aspects of the code change with respect to the log message accompanying it.
this allows cc2vec to learn high quality vector representations that can be used in existing state of the art models on tasks involving code changes.
we empirically evaluated cc2vec on three tasks and demonstrated that approaches using or augmented with cc2vec embeddings outperform existing state of the art approaches that do not use the embeddings.
finally we performed an ablation study to evaluate the usefulness of comparison functions.
the results show that the comparison functions play an important role and omitting them in part or in full affects the overall performance.
as future work to reduce the threat to external validity we will integrate of cc2vec into other tools and experiments on other tasks involving software patches.
527cc2vec distributed representations of code changes icse may seoul republic of korea package.
the replication package is available at com cc2vec cc2vec.
acknowledgement.
this research was supported by the singapore national research foundation award number nrf2016 nrfanr003 and the anr itrans project.