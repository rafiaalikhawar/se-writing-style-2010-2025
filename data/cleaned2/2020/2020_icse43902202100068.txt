an empirical study on deployment faults of deep learning based mobile applications zhenpeng chen huihan yao yiling lou yanbin cao y yuanqiang liu haoyu wangz and xuanzhe liu key lab of high confidence software technologies peking university ministry of education beijing china ypeking university information technology institute tianjin binhai tianjin china zbeijing university of posts and telecommunications beijing china fczp yaohuihan yiling.lou caoyanbin yuanqiangliu g pku.edu.cn haoyuwang bupt.edu.cn xzl pku.edu.cn abstract deep learning dl is moving its step into a growing number of mobile software applications.
these software applications named as dl based mobile applications abbreviated as mobile dl apps integrate dl models trained using large scale data with dl programs.
a dl program encodes the structure of a desirable dl model and the process by which the model is trained using training data.
due to the increasing dependency of current mobile apps on dl software engineering se for mobile dl apps has become important.
however existing efforts in se research community mainly focus on the development of dl models and extensively analyze faults in dl programs.
in contrast faults related to the deployment of dl models on mobile devices named asdeployment faults of mobile dl apps have not been well studied.
since mobile dl apps have been used by billions of end users daily for various purposes including for safety critical scenarios characterizing their deployment faults is of enormous importance.
to fill in the knowledge gap this paper presents the first comprehensive study to date on the deployment faults of mobile dl apps.
we identify real deployment faults from stack overflow and github two commonly used data sources for studying software faults.
based on the identified faults we construct a fine granularity taxonomy consisting of categories regarding to fault symptoms and distill common fix strategies for different fault symptoms.
furthermore we suggest actionable implications and research avenues that can potentially facilitate the deployment of dl models on mobile devices.
index terms deep learning mobile applications deployment faults i. i ntroduction in recent years deep learning dl has emerged as one of the most popular and promising techniques and has been widely adopted in various applications .
mobile devices are undoubtedly among the most important platforms for running dl based software applications .
these software applications namely dl based mobile applications in short as mobile dl apps integrate dl capabilities to add a wide range of features including object detection image processing natural language processing speech recognition etc.
to achieve this goal developers train dl models using large scale data i.e.
development of dl models and then deploy the obtained dl models on mobile devices for real usage i.e.
deployment of dl models .
in fact development of dl models is a general process for different types of dl based applications and its challenges corresponding author xuanzhe liu xzl pku.edu.cn .have been well studied in the software engineering se research community .
in particular researchers have extensively analyzed faults in the dl programs written based on dl frameworks e.g.
tensorflow tf and keras which encode the structure of desirable dl models and the process by which the models are trained using the training data.
recently the rapid growth of mobile dl apps has posed urgent challenges to the deployment of dl models i.e.
deploying dl models on mobile devices.
for example computation intensive dl models can be executed efficiently on pc server platforms but they cannot be directly deployed and executed on mobile devices with limited computing power .
although major vendors have rolled out specific dl frameworks such as tf lite and core ml to facilitate this deployment process various specific faults are still emerging in this process and frequently asked on stack overflow so one of the most popular q a forums for developers .
moreover previous work has demonstrated that relevant questions are increasing rapidly on so and more difficult to resolve than those related to other aspects of dl based applications.
in addition mobile dl apps are not only used by billions of end users for their daily activities e.g.
speech to text and photo beauty but also reported to be increasingly adopted in various safety critical scenarios e.g.
driver assistance and autonomous vehicles .
therefore the emerging faults related to the deployment of dl models on mobile devices named as deployment faults of mobile dl apps should be carefully addressed.
unfortunately the characteristics of these faults have not been well understood.
to fill in the knowledge gap this paper presents the first comprehensive study on analyzing symptoms andfix strategies of deployment faults of mobile dl apps.
given the surging popularity of mobile dl apps this study is of enormous importance.
it can help in understanding what are the common deployment faults of mobile dl apps and how these faults are resolved in practice so as to provide a high level categorization that can serve as a guide for developers to resolve common faults and for researchers to develop tools for detecting and fixing deployment faults of the increasing mobile dl apps.
ieee acm 43rd international conference on software engineering icse .
ieee we focus our study on the faults that occur during the usage of two representative frameworks specifically designed for deploying dl models on mobile devices i.e.
tf lite and core ml both of which are widely used in industry practice and well adopted in related studies .
specifically we collect a dataset of deployment faults related to their usage from so and github two commonly used data sources for studying software faults .
by manual analysis we qualitatively extract the symptom of each identified fault and construct a hierarchical taxonomy containing symptom categories indicating the diversity of deployment faults of mobile dl apps.
additionally we distill common fix strategies for each symptom category providing insights about deployment fault resolution of mobile dl apps.
based on our results we discuss new directions for future research.
furthermore we offer the scripts and the data used in this study as an additional contribution to the research community for other researchers to replicate and build upon.
ii.
b ackground and research questions we start by introducing current practice of the development of dl models and the deployment of dl models on mobile devices.
fig.
distinguishes the two processes.
development of dl models.
development of dl models is a general process for different types of dl based software applications .
first developers construct structures of desirable dl models and specify run time configuration e.g.
hyper parameters with dl programs written based on stateof the art dl frameworks such as tf and keras.
a dl model consists of multiple layers to convert input to output with each layer containing a set of neurons that accept input from neurons in the preceding layer apply activation function to the input and pass the resulting output to the neurons in the succeeding layer via a set of weighted edges.
the layer used as an entry point into the dl model is called the input layer while the layer that produces the end result is called the output layer.
the input and output layers wrap the input and output tensors multi dimensional arrays of numerical values respectively.
then developers use large scale data to train the dl models during which the weights of edges in the models are adjusted and set to values that minimize the difference between model output and expected output.
finally developers evaluate the performance e.g.
accuracy of the obtained dl models using testing data.
due to space limit we present only the model training phase in fig.
.
deployment of dl models.
dl models which are demonstrated to meet the performance requirements are ready to be deployed on mobile devices for real usage.
the deployment process mainly focuses on platform adaptations.
due to the limited computing power memory size and energy capacity of mobile devices models trained on pc server platforms cannot be directly deployed on them.
to tackle this problem some lightweight frameworks such as tf lite for android and core ml for ios are specifically designed for converting trained dl models to the formats supported by mobile devices.
specifically core ml provides python apis for this task traineddlmodelstrainconverteddlmodelstrainingdata mobiledevicesdevelopmentofdlmodelsdeploymentofdlmodelsconvertintegrate mobileprojectsfig.
.
development and deployment of dl models while tf lite provides both clis and python apis.
it is a common practice in the conversion stage to perform model quantization to reduce precision representations of the weights of edges in trained dl models in order to reduce memory cost and computing overhead.
for example core ml supports converting the weights from bits to bits.
then developers can integrate the converted models into mobile projects with the help of tf lite and core ml.
for instance tf lite provides apis of various programming languages such as java c and python to support the integration.
finally the integrated projects can run on mobile devices and make inference based on input data.
scope and research questions.
we focus our analysis on the process of deploying dl models to mobile devices.
any faults related to this process are within our scope.
however faults that occur during the development of dl models are not considered in this study.
specifically we aim to address two research questions that are concerned with deployment faults of mobile dl apps rq1 symptoms what are the frequent fault symptoms?
rq2 fix strategies what are the common fix strategies for different fault symptoms?
iii.
m ethodology to characterize the deployment faults of mobile dl apps we analyze the relevant questions posted on so and the relevant issues posted on github.
we illustrate the overview of the methodology of our study in fig.
.
refine datasetcollect relevantsoquestionsmanually labelled datacollect relevantgithubissuesrq1 symptoms rq2 fixstrategies fig.
.
overview of the methodology.
a. data collection following previous studies we focus on two representative dl frameworks i.e.
tf lite and core ml that are specially designed for deploying dl models on mobile devices.
since the deployment process is supported by these frameworks we collect the faults that occur in their usage to construct the dataset of interest.
mining so as one of the most popular communitydriven q a websites so s users range from novices to experts increasing the diversity of our collected faults.
in addition developers often post questions on so for the faults that they cannot find solutions quickly leading to more nontrivial faults in our dataset.
we collect the relevant questions on so in the following steps.
download so dataset.
we first download the entire so dataset from the official stack exchange data dump on june .
the dataset covers the so posts generated from july to june .
each so question has one to five tags based on its topics.
extract candidate posts.
we then extract so questions tagged with tf lite and core ml.
in line with previous work we filter out the questions that do not contain any source code because questions about faults usually contain code snippets.
in addition we follow previous studies to exclude questions that do not have an accepted answer ensuring that we consider only questions with a confirmed solution.
as a result we obtain questions for tf lite and questions for core ml.
mining github in addition to so github is also a commonly used data source for studying faults.
following previous work we mine issues in the official github repositories of the selected frameworks to identify faults that occur during their usage.
compared to commits issues contain more fault information that includes original reports and developers discussions .
such a benign characteristic makes issues suitable for studying fault symptoms and fix strategies.
in practice we use the github search api to mine the issues about tf lite and core ml on june .
note that on github issues are used for various purposes including bug report feature request etc.
to categorize the purposes of issues developers often employ repository specific keywords to label issues.
in line with previous work we also employ the issue labels to help us filter out irrelevant issues.
the collection processes for tf lite and core ml are conducted separately as follows.
extract issues for tf lite.
since tf lite has been integrated into the tf ecosystem to obtain issues for tf lite we limit the search to issues in the official tf repository .
the first two authors jointly examine each label in the tf repository to determine which labels can be used for filtering.
then we collect tf lite related issues by extracting issues labeled with comp lite .
moreover we filter out issues labeled with type feature type bug type docs bug type docsfeature or type build install to exclude those about requests for new features bugs in the framework itself documentrelated problems and requests for framework installment build instructions.
to ensure that we consider only issues with a confirmed solution we further exclude those without answers or responses i.e.
those labeled with stalled or stat awaiting response .
overall we obtain issues for tf lite.
extract issues for core ml.
to obtain issues for core ml we first extract all the issues in the official core ml repository .
since labels in this repository are not asabundant as those in the tf repository with the help of issue labels we can filter out only the issues about bugs in the framework itself i.e.
those labeled with label bug .
then similar to the process for tf lite we extract only the closed issues.
overall we obtain issues for core ml.
refining dataset since the extracted posts i.e.
questions and issues may contain some noise that is not about faults e.g.
how to questions on so the third and fourth authors further filter the extracted posts through manual analysis.
specifically they jointly read the extracted posts and exclude any post that either is not related to any issue fixing activity or happens to fix an issue in the framework itself rather than in mobile dl apps.
during this process any conflicts are discussed and resolved by introducing an arbitrator who has three years of experience in deploying dl models on mobile devices and has published several papers related to this topic in top tier conferences.
finally for tf lite we have so questions and github issues for core ml we have so questions and github issues.
b. manual labelling the refined dataset which consists of posts is used for distilling symptoms and fix strategies through manual labelling.
the scale of this dataset is comparable and even larger than those used in existing fault related studies that also require manual inspection.
next we present our procedures of manual labelling.
pilot labelling first we randomly sample of the posts for a pilot labelling.
the first two authors who have five and three years of dl experience respectively jointly participate in the process.
they follow an open coding procedure to inductively create categories for symptoms and fix strategies by analyzing the sampled posts.
the detailed procedures are described below.
the two authors read and reread all the posts to understand the context of faults and assign each post with short but descriptive phrases as initial codes to indicate i the fault symptom that shows what the fault looks like and ii the fix strategy that tells how a fault is fixed.
in this process they take all the contents of each post including the title description code snippets error messages comments answers and even urls mentioned by developers for careful inspection.
then they proceed to construct taxonomies for symptoms and fix strategies respectively.
specifically they group similar codes into categories and the grouping process is iterative in which they continuously go back and forth between categories and posts to refine the taxonomies.
a post is assigned to all related categories if it is related to multiple faults.
in the cases where there is no agreement between the two authors the aforementioned arbitrator is introduced to make discussions and resolve the conflicts.
they follow the procedure until they reach agreement on all posts.
reliability analysis for reliability analysis the first two authors then independently label the remaining posts based on the coding schema generated in the pilot labelling.
specifically they label each post with identified symptom 676and fix strategy categories and add the posts that cannot be classified into the current taxonomies into a new category named pending .
to measure the inter rater agreement during the independent labelling we employ the widely used cohen s kappa as the indicator.
the values obtained for symptoms and fix strategies are .
and .
indicating almost perfect agreement and substantial agreement respectively.
the agreement levels demonstrate the reliability of our coding schema and procedure.
the conflicts of labelling are then discussed and resolved by the aforementioned arbitrator.
for the posts classified as pending we also employ the arbitrator to help us further identify symptoms and fix strategies behind them and determine if new categories need to be added.
as a result we add three new categories into the symptom taxonomy and two new categories into the fix strategy taxonomy and assign all the posts in pending into the taxonomies.
the final labelling results are checked and approved by all participants.
in summary among the posts we identify a total of faults.
the labelling results in pilot labelling and reliability analysis are both included in the final taxonomies.
based on the taxonomies for symptoms and fix strategies we answer therq1 andrq2 raised in section ii respectively.
iv.
rq1 s ymptoms fig.
presents the hierarchical taxonomy of deployment fault symptoms of mobile dl apps.
the taxonomy is organized into three level categories including a root category i.e.
deployment faults five inner categories linked to stages in deploying dl models e.g.
model conversion and specific leaf categories e.g.
model parse failure .
finding we construct a taxonomy of fault symptom categories related to deploying dl models on mobile devices indicating the diversity of deployment faults.
for each category the number in the top right corner refers to the number of faults in it.
due to space limit we address only frequent and non trivial symptoms i.e.
faults .
for data preparation andmodel update we do not present their leaf categories since no frequent symptoms are observed under them.
for the remaining three inner categories faults with infrequent or unclear symptoms are included in the others category.
next we discuss and exemplify each inner category.
a. model conversion as the first stage of deploying dl models model conversion aims to convert dl models into the formats expected by mobile devices.
to implement a converter for model conversion developers need to provide the dl model that is ready to be converted and specify necessary information about the model through apis clis provided by tf lite or core ml.
we observe faults that occur during the model conversion stage accounting for .
of all the identified faults and covering symptom categories.
a large proportion of faults occur when the converter parses the dl model and validates the model information specified by developers such as names and shapes of input outputtensors of the model.
specifically .
of faults in model conversion are triggered when the converter fails in parsing the dl model a. .
moreover when the converter detects missing or incorrect specification of the aforementioned model information developers may encounter tensor error a. and shape size error a. .
furthermore shape size error a. can also be triggered when the converter detects the invalid shape of input output tensors or the dimension size misalignment in the model structure.
in total a.2anda.3account for .
of faults in model conversion .
in addition to the basic model information developers can also specify some information to reduce the precision representations of model weights during the conversion stage so as to reduce the memory cost and computing overhead of dl models on mobile devices.
this process is commonly known as model quantization .
the problematic configuration of quantization related arguments may result in two types of symptoms i.e.
quantization failure a. andunexpected model size a. accounting for out of the faults .
in model conversion .
after parsing the dl model the converter may find that the model uses operations or datatypes that are not supported by tf lite or core ml.
this can result in unsupported operation a. andunsupported datatype a. accounting for .
and .
of faults in model conversion respectively.
in particular a. is the most frequent category in the model conversion stage.
its common occurrence is because that compared to the frameworks used for developing dl models e.g.
tf and keras tf lite and core ml are proposed later and relatively unfledged.
therefore some standard operators functions or layers collectively referred to as operations here used in the model may be not supported by tf lite and core ml.
moreover the dl model may contain some custom operations that cannot be recognized by the converter.
in addition to the symptoms specific to the deployment of dl models we also observe that a portion i.e.
.
of faults in model conversion share common symptoms with general software systems.
for example .
of faults are triggered due to unsuccessful import of dependent modules i.e.
import error a. .
are related to reference to non existent variables or functions i.e.
attribute not found a. and .
are caused by using arguments of api cli incorrectly i.e.
invalid argument a. .
besides the faults with explicit errors thrown during the model conversion stage sometimes developers get unexpected models even after model conversion appears to be successfully done.
for example developers may find that the number shape or format of input output tensors of the model changes.
we classify these cases into the category unexpected model a. accounting for .
faults in model conversion .
finding most i.e.
.
of deployment faults occur during the model conversion stage covering a wide spectrum of symptoms i.e.
categories .
among these categories unsupported operation is the most common accounting for .
of faults in this stage.
dlintegration38 speedissue16 memoryissue12 shape size error24 datatype format error5 unexpected result39 others14 model loading failure9 dependencyresolutionerror13 frameworkloadingfailure5 gpudelegatefailure8 unexpected model6 others3 inference110 datapreparation7 modelupdate2deployment faults304 unsupportedoperation46 shape size error28 tensorerror6 import error7 invalidargument3 attributenotfound13 modelparsefailure14 others13 unexpectedmodelsize3 quantizationfailure3 modelconversion147 unsupporteddatatype5fig.
.
taxonomy of deployment fault symptoms of mobile dl apps.
b. dl integration after the dl model is converted into the expected format developers can integrate it as well as dl frameworks into a mobile app project.
then they can build the project and load the model to make it ready for inference.
faults that appear in this stage are included in the dl integration b category accounting for .
of the deployment faults of mobile dl apps.
dependency resolution error b. is a common fault when building projects accounting for .
of the faults in dl integration .
specifically it refers to failures in preparing necessary dependencies directly or transitively specified by developers.
in these cases projects throw error messages like inability to resolve libraries unsuccessful dependency downloading and undefined reference to objects e.g.
functions and libraries .
after building projects developers can run mobile apps to make it predictable.
however in this phase many developers encounter framework loading failure b. and model loading failure b. which refer to the failures in loading dl frameworks and models respectively and account for a total of .
of faults in dl integration .
what is more developers may configure projects to make it able to use the gpu backend on mobile devices.
however some developers complain that they encounter the gpu delegate failure b. when running mobile dl apps.
b.4represents .
of faults indl integration .
finding faults appearing in the dl integration stage account for .
of the total deployment faults and cover five symptom categories.
a large proportion .
of these faults are thrown with dependency resolution errors.c.
data preparation data preparation c is the stage where a mobile app prepares input data for the next inference stage.
for a mobile dl app input data are usually extracted from user generated data such as camera pictures or typed texts and a data preparation fault often occurs when the app fails to access or process the required user generated data.
note that this type of faults is essentially related to data accessing and processing issues which not only occur in mobile dl apps but also is very common in other mobile apps.
therefore to seek more extensive help developers usually do not describe these problems in the context of mobile dl apps e.g.
on so they prefer not to post their problems with any tag related to dl and thus we observe only a few related cases .
with no frequent symptoms in the collected data.
d. inference inference d consists of faults that occur when a mobile app makes inference based on input data.
.
of deployment faults do not show symptoms until this stage.
a proportion .
of faults in inference appear with explicit errors i.e.
shape size error d. ordatatype format error d. .
they are triggered when the shape size or datatype format of input output arrays used for storing input output data does not align with that of input output tensors of the dl model.
furthermore some developers report that the mobile dl app produces unexpected results i.e.
d. although no errors are thrown.
these cases account for .
in inference .
specifically developers may observe that the mobile dl app produces different results than the original model.
however note that this symptom cannot be always used as the indication of faults especially when model quantization is performed 678during the model conversion stage.
since model quantization reduces the precision representations of model weights it is reasonable to observe the change in model performance.
besides developers also employ some other indications to confirm the existence of unexpected result d. .
for instance the mobile dl app produces the same result for any input or produces different results for the same input.
in addition to the faults that affect the output results there are also .
of faults that have impact on the memory usage and inference speed of mobile dl apps.
we use memory issue d. andspeed issue d. to refer to the two types of faults.
specifically memory issue d. includes symptoms such as out of memory memory leak failures in memory allocation and segment faults speed issue d. is mainly manifested as long latency time of making inference.
finding .
of faults occur when mobile dl apps make inference based on input data covering six symptom categories.
in particular .
of the faults in this stage are captured since developers observe unexpected results.
e. model update once put into real usage mobile dl apps keep receiving feedback from users e.g.
bad cases based on which dl models can further be improved e.g.
updating the weights of models .
instead of re training dl models on pc server platforms and then re deploying the new models again developers can also directly re train the dl models on mobile devices which is the stage model update e .
however since currently on device training requires a large amount of computational resources and is still not widely supported by existing dl frameworks we observe only a few instances .
related to it in our dataset.
f .
distribution of symptoms across frameworks we then further analyze the distribution of the identified fault symptoms across the two selected frameworks i.e.
tf lite and core ml .
we find that the two frameworks share a similar distribution in most categories.
for example of tf lite related issues are included in inference d and for core ml the ratio is comparable.
however at the same time the two frameworks differ obviously in some specific categories.
for instance unsupported operation a. accounts for of tf lite related issues but only of core ml related issues.
v. rq2 f ixstrategies to capture how developers fix different types of deployment faults for each symptom category we summarize its fix strategies in this section.
since data preparation andmodel update contain only a few samples and do not show frequent symptoms here we do not consider them.
for the remaining three inner categories we show the frequency of different fix strategies on their leaf categories in figs.
and respectively.
due to space limit strategies with low frequency i.e.
faults are not shown in the figures.
in each figure x axis represents each leaf category and the letter identifieris consistent with our taxonomy in fig.
y axis shows fix strategies following with their total frequency under the inner category.
next we elaborate the identified fix strategies for frequent symptoms and demonstrate some real world examples of faults and corresponding fixes.
a. fix strategies for faults in model conversion we identify nine frequent fix strategies for faults in model conversion and illustrate the distribution of these strategies on leaf categories in fig.
.
var1var2var3var4var5var6var7var8var9 a.1a.2a.3a.4a.5a.6a.7a.8a.9a.10a.11xy 0510z leafcategoryfix usequantization registeroperator fixtensornamespecification changegraphtype selecttfoperator fixtensorshape sizespecification repairoriginalmodel fixconversionapi cliusage fixframeworkinstallment version a.1a.2a.3a.4a.11a.5a.6a.7a.8a.9a.10var1var2var3var4var5var6var7var8var9 a.1a.2a.3a.4a.5a.6a.7a.8a.9a.10a.11xy 0510z fig.
.
distribution of fix strategies for leaf categories in model conversion .
fix framework installment version.
.
of faults in model conversion are solved by re installing the dl framework or switching the dl framework into a different version.
this strategy covers seven fault symptoms and is especially frequently adopted in the unsupported operation a. and attribute not found a. categories.
for example .
of unsupported operation a. faults are fixed after switching the dl framework into a more recent version with more supported operations.
as for attribute not found a. faults developers often misuse apis in a way unsupported by the current dl framework since apis frequently evolve with dl frameworks.
therefore at most cases developers resolve them by changing the dl framework to another version that supports the reference to specified attributes.
for example a developer reports that she receives an error attributeerror type object tfliteconverter has no attribute from keras model when converting a keras model to the tf lite format tf issue and the corresponding fix is upgrading tf to .x version since from keras model is not supported by .x version.
in addition the framework version issue can also result in some non intuitive symptoms.
for example a developer encounters a shape size error a. during model conversion with the message check failed input shape.dims .size op!size.size vs. so post which leads to a heated discussion.
all the comments suggest that the developer should fix the shape of the input tensor specified during model conversion but none of them work.
finally the developer upgrades tf and successfully resolves the fault.
fix conversion api cli usage.
.
of faults in model conversion involving six frequent symptom categories are fixed by correcting or changing the usage of apis clis for model conversion.
as suggested by previous work so many apis clis provided by existing dl frameworks for model conversion make it difficult for developers to correctly choose or use their desired apis clis meanwhile frequent 679questiondescription how can fix this error when converting from kerasmodel to coreml?
the size of the output layer output in the neural network does not match the number of classes in the classifier.symptom shape sizeerror a. fixstrategy repairoriginalmodelabinaryclassifierimplementedbasedonkeras model sequential ...model.add dense model.add dense model.add activation sigmoid model.add activation softmax model.compile loss binary crossentropy optimizer rmsprop lr .
metrics modelconversionimplementedbasedoncoreml output labels coreml model coremltools.converters.keras.convert cats and dogs.h5 input names class labels output labels image input names image output names example a coremlissue addition deprecation and upgrade of apis clis also make their usage error prone.
repair original model.
repairing the dl model used for conversion fixes .
of faults in model conversion which mainly belong to the shape size error a. andunsupported operation a. categories.
as shown in example a the core ml issue is a real world example on the shape size error a. .
a developer uses keras to implement a binary classifier trains and tests it successfully.
however when she converts the obtained model to the core ml format shape size error a. occurs.
since she specifies two output labels and during model conversion the converter expects a model with a two dimensional output tensor.
however the output of the original model is an one dimensional tensor indicating the probability that the input is classified as label .
to resolve this fault the developer repairs the original model and makes it output a two dimensional tensor with each dimension indicating the probability that the input is classified as one label or .
as for unsupported operation a. developers often i replace it with a supported one ii implement its function outside the model or iii delete it if it is unnecessary.
fix tensor shape size specification fix tensor name specification.
the two strategies fix the specification of the shape size and the name of input output tensors during model conversion respectively.
as described in previous work training dl models can be expensive since it requires a large quantity of computational resources and labelled data that might not be readily available.
therefore developers often directly use pre trained dl models that are available online.
in this case they may have no idea about the model information e.g.
the shape size and the name of input output tensors that needs to be specified during model conversion.
incorrect specification can result in shape size error a. tensor error a. unexpected model a. etc.
therefore we can observe that the two strategies mainly fix faults with these symptoms.
for example a developer reuses an object detection model that she is not familiar with from github and specifies the input tensor as a tensor not contained in the model so post resulting in tensor error a. .
the corresponding solution is fixing tensor name specification.select tf operator register operator.
the two strategies are used to tackle the unsupported operation a. faults that occur when converting dl models into the tf lite format.
selecting tf operators allows dl models to use a subset of tf operators that are not supported by tf lite while registering operators refers to registering unsupported operators in the tf lite run time library so that the run time knows how to map these operators to executable code .
compared to selecting tf operator registering operator can be used not only to support tf operators but also to support operators customized by developers.
change graph type.
this group of fixes changes the type of the model graph e.g.
training graph and evaluation graph used for conversion.
the model graph refers to the computational graph that represents the structure of the dl model.
since operations involved in model training and evaluation are not always the same developers need to construct the training graph and the evaluation graph separately.
the graph used for conversion should be the evaluation graph since developers always aim to make inference rather than training on mobile devices.
when developers use the training graph for conversion some training operations may be unrecognized and unsupported by the converter.
as a result developers would encounter unsupported operation a.
.model parse failure a. is another common symptom that occurs when the incorrect type of model graph is provided.
fix use quantization.
this group of fixes selects a proper quantization method according to developers demand or fixes the incorrect quantization configuration.
naturally it can resolve the quantization failure a. .
in addition since model quantization can reduce the model size while reducing the precision representations of model weights when developers observe that the model size does not change as expected after quantization i.e.
unexpected model size a. there may be a fault in the quantization configuration that needs to be fixed.
finding we identify nine frequent fix strategies for faults in model conversion.
the three most common strategies are fixing framework installment version fixing conversion api cli usage and repairing the original model resolving .
.
and .
of faults in this stage respectively.
b. fix strategies for faults in dl integration as illustrated in fig.
we identify four frequent fix strategies for faults in dl integration .
var1var2var3var4a.1a.2a.3a.4xy 05zvar1var2var3var4a.1a.2a.3a.4xy 05zrepairoriginalmodel fixtensornamespecification fixframeworkinstallment version fixbuildconfiguration b.1b.2b.3b.4leafcategory fig.
.
distribution of fix strategies for leaf categories in dl integration .
fix build configuration.
.
of faults in dl integration are resolved by fixing the build configuration of mobile dl projects including fixing dependency version fixing link configuration fixing option settings etc.
this fix strategy mainly resolves the dependency resolution error b. .
680the remaining three frequent fix strategies have been described in section v a. they are also applicable to some faults indl integration .
fix framework installment version.
when the required dl framework is not successfully installed or the dl model is not incompatible with the framework version used in the project symptoms like framework loading failure b. and model loading failure b. may occur.
in such cases developers need to fix framework installment version.
fix tensor name specification.
when input output tensors are specified incorrectly during model conversion the converted model may not be loaded in mobile projects successfully i.e.
model loading failure b. .
moreover improper specification of input output tensors may cause gpu delegate failure b. .
for instance a developer encounters this failure since some data pre and post processing operators in the original model are not supported by gpu tf issue .
the fixing strategy is re specifying the input and output tensors during model conversion to ensure that the unsupported operators are not between the new input and output nodes thereby not in the converted model.
repair original model.
this strategy can resolve the gpu delegate failure b. .
in fact some operators supported by dl frameworks are not supported by gpu.
in this case developers can repair the original model to remove these operators and implement alternative operations so that the integrated dl models can run on the gpu backend of mobile devices.
finding we identify four frequent fix strategies for faults in dl integration.
the most common one is fixing build configuration which resolves .
of faults in this stage.
c. fix strategies for faults in inference we identify frequent fix strategies for faults in inference and present the distribution of these strategies in fig.
.
usecpuonly fixdatatypeofinput output fixspecificationofinput output gpudelegate fixthreadmanagement fixdatapost processing fixmemorymanagement fix usequantization fixapiusageduringdlintegration fixframeworkinstallment version repairoriginalmodel fixshapeofinput output fixdatapre processing leafcategoryvar1var2var3var4var5var6var7var8var9 a.1a.2a.3a.4a.5a.6a.7a.8a.9a.10a.11xy 0510z d.1d.2d.3d.4d.5var1var2var3var4var5var6var7var8var9var10var11var12var13 a.1a.2a.3a.4a.5xy 0510z fig.
.
distribution of fix strategies for leaf categories in inference .
fix data pre processing fix data post processing.
.
of faults in inference can be resolved by fixing the process of preparing data for model input i.e.
data preprocessing or the process of parsing model output to obtain expected or human readable results i.e.
data post processing .
when developing dl models data pre processing is often considered as an individual stage and thus may not be included inside the model structure.
in this case code for data pre processing needs to be re implemented in the mobile questiondescription java tflite error when allocating memory for runformultipleinputsoutputs.
unexpected failure when preparing tensor allocations.symptom memoryissue d. fixstrategy fixspecificationofinput outputcodeforintegratingdlmodelintomobilesoftwareproject initialize an interpreter with the modelinterpreter tf.lite.interpreter model path f allocatememoryfortheinputandoutputarraysfloat input newfloat float output0 new float float output1 new float float output2 new float float output3 new float specifytheoutputandinputofthemodelobject outputs output0 output1 output2 output3 object inputs input object inputs input ... useinputdatatofilltheinputarrayinterpreter.runformultipleinputsoutputs inputs map of indices to outputs makeinferenceexample b sopost project during the deployment process so as to keep the consistent behaviors of the dl model before and after deployment.
forgetting to implement it or implementing it incorrectly can result in unexpected results.
in addition sometimes the model behaves well and generates the expected output but developers make mistakes in parsing the model output which can also result in unexpected results.
therefore we can find that the two fix strategies mainly tackle the unexpected result d. and .
of faults in this category can be resolved by them.
fix shape of input output fix datatype of input output fix specification of input output.
when integrating dl models into mobile projects developers often need to prepare the input output arrays that are used for storing input output data and specify their shape and datatype.
for example as shown in example b so post a developer integrates a dl model with one input tensor and four output tensors into an android project implemented in java.
first she uses the model to initialize an interpreter.
then she allocates memory and specifies the shape and datatype for input and output arrays and sets these arrays as the model input and output.
finally she uses input data to fill the input array and make inference.
during the above process when the shape of input output arrays are incorrectly specified developers may encounter shape size error d. .
similarly when the specification of the datatype of input output arrays is incorrect developers may encounter datatype format error d. or obtain unexpected result d. .
therefore fixing shape of input output mainly resolves faults in d. while fixing datatype of input output tackles faults in d.2andd.
.
in addition incorrect specification of input output of the model may result in faults such as datatype format error d. and memory issue d. .
for example the symptom of the fault in example b is memory issue d. with an error unexpected failure when preparing tensor allocations .
the corresponding solution is fixing specification of input output.
fix api usage during dl integration.
in the dl integration process as shown in example b developers often misuse 681relevant apis provided by dl frameworks.
the corresponding solution is fixing api usage during dl integration.
fix memory management.
this group of fixes resolves the faults related to memory management during the dl integration stage.
a typical fault is that developers may set the input output arrays before allocating memory for them e.g.
so post which results in memory issue d. .
fix thread management gpu delegate.
the two groups of fixes refer to setting an appropriate number of threads in mobile projects and configuring mobile projects to enable dl models in them to run on the gpu backend respectively.
both of them can reduce the latency during inference and thus resolve of the faults in speed issue d. .
use cpu only.
this group of fixes forces dl models to run on the cpu backend during inference by configuring some settings in mobile projects.
it mainly resolves the shape size error d. .
for example when a developer makes inference with a core ml model an error is thrown reporting that the size of the input sequence exceeds the upper bound so post .
the cause is that a dense operation in the model with a large size of sequences is unable to be performed on a gpu due to the memory constrains.
finally the developer forces the model to use only cpu and resolves the fault.
in addition there are three fix strategies that have been elaborated in sections v a and v b. repair original model.
this strategy mainly resolves the shape size error d. andunexpected result d. .
specifically when the input size expected by dl model is inconsistent with the actual size of data extracted in apps i.e.
shape size error d. one solution is to reshape the original models.
moreover some developers find that the models cannot perform well in real applications i.e.
unexpected result d. and thus choose to refine their original models.
fix framework installment version.
fixing framework installment version can also resolve some faults that occur during inference.
for example a developer gets worse results when she converts a keras model into the tf lite format so post .
the root cause is that an api that she uses during model conversion is problematic in tf .
.
upgrading tf to version .
resolves the fault.
fix use quantization.
the problematic configuration of model quantization can affect performance of the converted model and thus result in unexpected inference results.
moreover since the quantized model is more light than the original one model quantization is also a solution to speed up the inference process.
therefore fixing using quantization mainly resolves the unexpected result d. andspeed issue d. .
finding the fix strategies for faults in inference are diverse.
they cover many stages of the deployment process including fixing data processing fixing the model conversion stage e.g.
fixing using quantization fixing the dl integration stage e.g.
fixing api usage during dl integration etc.
vi.
d iscussion given the rapidly increasing popularity of mobile dl apps our study has timely and immediate implications for devel opers especially novice developers.
specifically our results can help developers more efficiently understand and resolve common deployment faults.
for example a developer may be confused as to how to resolve the unsupported operation a. symptom since the fault may lie in the model development process or any setting configuration in model conversion.
however with our results the developer can know how such faults are usually resolved in practice so that she can find the solution with less trial and error.
nevertheless due to the broad spectrum of deployment faults it is challenging for developers to detect and fix these faults completely manually.
therefore we call on se researchers to develop automated techniques to assist them.
although the combinations of fault symptoms and fix strategies derived in our study can serve as common strategies for the automated techniques we believe that more research efforts are needed to achieve the goal.
next we discuss some implications of our findings on future research.
testing dl models deployed on mobile devices.
as suggested in our study .
of deployment faults e.g.
unexpected model a. unexpected result d. and speed issue d. do not explicitly lead to an error or a crash during deployment and are thus usually exposed relying on developers experience or extra efforts.
this non trivial portion indicates the importance of testing deployed models automatically.
however existing testing efforts are mainly dedicated to the dl models obtained by training rather than the dl models converted and deployed on mobile devices.
unlike testing the trained dl models testing deployed dl models on mobile devices has its unique challenges in i resource limitation and ii undetermined change in model behaviors .
specifically compared to the pc server platforms used for testing trained dl models mobile devices used for testing deployed models have limited resources in terms of computing power and memory size.
in addition in the cases where quantization techniques are employed during model conversion the deployed models should have different behaviors from the original models since quantization techniques reduce the precision representations of model weights.
however it is unclear how differently the models after deployment might behave increasing the difficulty in testing the deployed models.
for example a developer gets worse predictions using a tf lite model converted from a keras model so post .
since she employs quantization techniques during model conversion it is difficult for her to tell whether the performance loss of the model is caused by only the quantization or other bugs in the deployment process.
to the best of our knowledge there is little work focusing on the deployed model testing.
with increasing growth of mobile dl apps we encourage researchers to conduct research in this direction and propose some testing techniques accordingly.
repairing dl models based on deployment faults.
we can find that repairing the original dl models used for deployment is a common fix strategy for faults that occur in model conversion dl integration and inference stages.
specifically it resolves .
of deployment faults cover682ing frequent symptoms.
therefore we believe that this significant fix strategy deserves the attention of researchers.
however existing research efforts focus on repairing dl models in the development process and investigate the correlation between different model repairing patterns and various fault types in the development process including api faults data faults structural faults etc.
by comparison there is little work on repairing dl models based on faults identified in the deployment process.
we call on researchers to develop automated techniques in this direction to facilitate the automated fix of deployment faults of mobile dl apps.
mining api cli usage protocols.
in this study we observe that out of faults are resolved by fixing api cli usage in model conversion and dl integration stages.
mining the api cli usage protocols enforced by dl frameworks is a promising research topic to facilitate the automated detection and fixing of these faults.
specifically researchers can mine these protocols from the official documentation of dl frameworks and relevant projects available on open source code repositories.
in particular the changes in the api cli usage protocols caused by the evolution of dl frameworks need to be highlighted in the mining results.
vii.
t hreats to validity in this section we discuss threats to the validity of our study.
selection of frameworks.
our identification of deployment faults of mobile dl apps is based on two relevant frameworks which may lead to possible selection bias in this study.
to mitigate this threat we select the representative and widelyused frameworks.
on the one hand the selected frameworks are widely used in industry practice and well adopted in related studies .
on the other hand the selected frameworks cover the deployment scenarios of two typical types of mobile apps i.e.
android and ios apps .
selection of data sources.
since there is no list of all mobile dl app projects in the world our study cannot cover all the relevant faults which may lead to a threat to the external validity.
to mitigate this threat we select two representative data sources i.e.
so and github that have been widely used in empirical studies in se .
since previous studies have found that findings derived from so and github posts can be well validated by practitioners we believe that our choice of so and github does not invalidate our results.
however it is still possible that in other contexts developers may encounter faults that are not covered in this study.
in the future we plan to include interviews with researchers and practitioners to further validate our findings.
subjectivity of researchers.
the subjectivity of researchers presents a possible threat to the validity of manual analysis.
to mitigate this threat we ensure that each case is labelled by at least two authors with an experienced arbitrator resolving the conflicts and inspecting all final results.
in addition the inter rater agreement is relatively high which demonstrates the reliability of the labelling schema and procedure.viii.
r elated work in this section we summarize the related work to well position our study within the literature.
challenges that ml dl poses for se.
machine learning ml plays an increasingly significant role in various application domains and poses new challenges for software developers .
to understand these challenges alshangiti et al.
analyzed the ml related questions posted on so and found that these questions are more difficult to answer than other questions.
by further analysis they demonstrated that model deployment is the most challenging across all the ml phases and that dl related topics are the most common in the ml related questions.
in recent years several studies have focused on the challenges in developing dl applications.
for example han et al.
applied an automatic topic modelling technique to the so questions related to three popular dl frameworks and derived the topics contained in these questions.
their results revealed common concerns that developers face when using dl frameworks such as version problems and model training.
similarly zhang et al.
manually analyzed dl related questions on so and found that program crashes model deployment and implementation related questions are the most frequently asked.
recently chen et al.
investigated so questions related to the deployment process of dl based applications.
they derived the topics of the specific challenges that developers face when deploying dl models to server mobile and browser platforms.
in contrast instead of deriving the topics of challenges at a macro level we aim to analyze symptoms and fix strategies of the deployment faults and provide actionable implications for fault detection and fix in mobile dl apps.
in addition we do not limit our analysis to just so and also consider github which ensures comprehensiveness of this study.
empirical study on faults.
there have been a number of empirical studies that focus on faults in different types of software systems.
for example lu et al.
studied concurrency fault characteristics franco et al.
explored real world faults in numerical software gao et al.
conducted an empirical study on recovery faults in large scale distributed systems.
in recent years the rapid development of dl technologies has inspired some empirical studies on characterizing the faults in software applications that make use of dl frameworks.
for example zhang et al.
collected faults in tf programs from so and github.
they categorized the symptoms and root causes of these faults through manual analysis.
following this work humbatova et al.
and islam et al.
extended their scope to the faults in programs written based on five popular dl frameworks to present more comprehensive results.
moreover islam et al.
analyzed the fix strategies of these faults in their follow up work.
recently zhang et al.
studied the program faults of dl jobs running on a remote and shared server platform.
across the existing empirical studies faults are often characterized based on multiple dimensions including types symptoms root causes fix strategies etc.
compared to the prior studies we 683apply these fault characterization methods to the faults in a different domain i.e mobile dl apps.
mobile dl apps.
to make dl models accessible for users developers need to deploy them to different platforms according to various application scenarios.
a popular way is to deploy them on mobile devices.
to facilitate this deployment process researchers have proposed many optimization techniques e.g.
cloud offloading and model compression .
in addition researchers have built numerous dl based applications on mobile devices .
to bridge the knowledge gap between research and practice xu et al.
conducted an empirical study on large scale android apps collected from google play store and demonstrated the increasing popularity of dl in real world mobile apps.
despite this popularity the related techniques for deploying dl models to mobile devices are still not very mature.
recently guo et al.
investigated the performance gap when the trained dl models are migrated from pc to mobile devices with the help of tf lite and core ml.
their findings unveiled that the deployment still suffers from compatibility and reliability issues.
despite these efforts the characteristics of deployment faults of mobile dl apps are still under investigated and thus we aim to fill in this knowledge gap.
ix.
c onclusion in this paper we have presented a comprehensive study of deployment faults of mobile dl apps.
by manual examination of real world faults extracted from so and github we have derived a taxonomy of fault symptoms with categories indicating that the process of deploying dl models on mobile devices stretches over a wide spectrum of faults.
moreover we have analyzed the fixes for the extracted faults and distilled frequent combinations of fault symptoms and fix strategies that can be adopted to facilitate manual and automated fault fix.
finally we have discussed insightful implications for developers and researchers based on our results.
acknowledgment this work was supported by the national key research and development program of china under the grant number 2018yfb1004403 the national natural science foundation of china under the grant number and the beijing outstanding young scientist program under the grant number bjjwzyjh01201910001004.
haoyu wang s work was supported by the national natural science foundation of china under grant numbers and .