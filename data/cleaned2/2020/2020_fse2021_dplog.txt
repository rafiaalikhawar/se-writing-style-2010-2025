would you like a quick peek?
providing logging support to monitor data processing in big data applications zehao wang software peformance analysis and reliability spear lab concordia university montreal canada w zeha encs.concordia.cahaoxiang zhang centre for software excellence at huawei canada haoxiang.zhang huawei.comtse hsun peter chen software peformance analysis and reliability spear lab concordia university montreal canada peterc encs.concordia.cashaowei wang university of manitoba winnipeg canada shaowei.wang umanitoba.ca abstract to analyze large scale data efficiently developers have created various big data processing frameworks e.g.
apache spark .
these big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic.
in traditional software systems developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis.
however due to the abstraction and the peculiarity of big data frameworks there is currently no effective monitoring approach for big data applications.
in this paper we first manually study randomly sampled spark related questions on stack overflow to study their root causes and the type of information if recorded that can assist developers with motioning and diagnosis.
then we design an approach dplog which assists developers with monitoring spark applications.
dplog leverages statistical sampling to minimize performance overhead and provides intermediate information and hint warning messages for each data processing step of a chained method pipeline.
we evaluate dplog on six benchmarking programs and find that dplog has a relatively small overhead i.e.
less than increase in response time when processing 5gb data compared to without using dplog and reduce the overhead by over compared to the baseline.
our user study with developers shows that dplog can reduce the needed time to debug big data applications by and the participants give dplog an average of .
for its usefulness.
the idea of dplog may be applied to other big data processing frameworks and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.
corresponding author tse hsun peter chen peterc encs.concordia.ca permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse august athens greece association for computing machinery.
acm isbn .
.
.
.
concepts general and reference empirical studies information systems mapreduce based systems software and its engineering software testing and debugging.
keywords apache spark logging monitoring acm reference format zehao wang haoxiang zhang tse hsun peter chen and shaowei wang.
.
would you like a quick peek?
providing logging support to monitor data processing in big data applications.
in proceedings of the 29th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse august athens greece.
acm new york ny usa pages.
.
introduction due to advances in data science and technologies the amount of data that is being created and collected is tremendous.
studies estimate that more than of the data in the world has been generated in the past few years.
the vast amount of data once analyzed provides opportunities for governments and companies to make data driven decisions that help improve efficiency and generate revenues.
to analyze such large scale data developers have created various big data processing frameworks such as apache spark hadoop and google s mapreduce .
these big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic.
using these frameworks developers can scale the computation tasks horizontally across clusters of machines with little to no code changes and speed up computation.
in particular spark has become one of the largest and most popular big data processing frameworks due to its intuitive api design and performance .
in traditional software applications developers may use logging frameworks such as log4j to insert logging statements in application source code.
then developers use the generated logs to assist in monitoring testing and debugging .
however due to the abstraction provided by spark there may be peculiar challenges if developers want to add logging statements to monitor big data applications.
first developers often leverageesec fse august athens greece zehao wang haoxiang zhang tse hsun peter chen and shaowei wang method chaining e.g.
filter .dropna .distinct to create a data processing pipeline in spark.
method chaining is one of the core concepts in spark s api design to promote data immutability which helps reduce concurrency issues related to data sharing.
method chaining may introduce challenges if developers need to monitor or understand how the data is transformed in each step as developers can only see the final output.
second spark implements lazy evaluation to optimize the data processing pipeline.
breaking the data processing pipeline to record intermediate information in each step may significantly affect application performance.
the default logging provided by spark only records information related to the spark internals such as cluster resource allocation.
however knowing the states of spark s internals may be not sufficient for developers and there is limited support on recording the execution information on the application side.
due to the importance and complexity of data processing applications there is a need for logging solutions to better monitor data transformation and provide useful system execution information to assist problem diagnosis.
in this paper we focus our study on spark due to its popularity.
however our findings are applicable to other big data frameworks.
to address the above mentioned challenges and assist developers in various tasks related to spark applications e.g.
monitoring and issue diagnosis in this paper we follow a three phase sequential exploratory strategy .
similar to prior research in software engineering our goal is to first identify the challenges that developers encounter and propose an approach to assist them.
first we identify the challenges that developers encounter through a qualitative study.
specifically we study the type of information that may be useful to developers when understanding system execution and diagnosing issues.
we conduct a manual study on spark related questions from stack overflow which reaches a confidence level and a confidence interval.
we found that questions related to data processing and spark api usage are the most common challenges that developers encounter .
in particular most issues are related to not knowing what are the intermediate states of the processed data and improper usage of api that leads to unexpected results.
second we design a logging approach called dplog to provide developers the capability to monitor and understand data processing execution.
dplog leverages statistical sampling to minimize performance overhead and provides intermediate processing information and hint messages in real time for each data processing step of a chained method pipeline.
finally we evaluate dplog by first measuring its performance overhead on six benchmarking programs.
through a user study we also show that the logging information provided by dplog may also assist developers in diagnosing issues in data processing applications.
the contributions of this paper are as follows our empirical study on spark related questions on stack overflow uncovers common challenges that developers encounter.
most of the issues that developers have are related to data transformation and api usage.
in particular developers often have challenges knowing the intermediate data states that lead to unexpected results.
we propose an approach dplog which assists developers with monitoring and understanding data processing in spark.
through an evaluation of six benchmarking programs we find that dplog has a relatively small overhead.
compared to without using dplog the response time when processing 5gb data increases by less than .
dplog reduces the overhead by over compared to the baseline.
we demonstrate the usefulness of dplog through a user study.
our user study with professional developers and graduate students shows that dplog can reduce the needed time to diagnose issues in big data applications by an average of .
on average the participants give dplog .
for its usefulness.
we discuss the implications of our findings and future research opportunities in assisting developers with developing and debugging big data applications.
in summary we proposed a data driven solution i.e.
dplog based on real world spark challenges.
dplog provides support to spark developers to address alleviate such challenges and our evaluation of dplog demonstrates its small performance overhead and its usefulness in helping monitor and diagnose big data applications.
although dplog was implemented for spark applications the idea of dplog can be migrated to other big data frameworks for which method chains are employed and the intermediate information of data is difficult to access.
for instance hadoop also employs method chains on reducers and mappers for data processing jobs so our approach can be migrated to hadoop to monitor the intermediate states of data processing.
paper organization.
section discusses the background of spark and related work.
section presents challenges in developing spark applications that we uncover from stack overflow questions.
section presents the design of dplog.
section evaluates dplog.
section discusses implications and future work.
section discusses threats to validity.
section concludes the paper.
background and related work background of apache spark.
apache spark is a distributed cluster computing framework that can execute the computation in parallel in a cluster.
to assist developers with big data processing spark abstracts the underlying parallel computation and cluster management from developers.
spark provides apis for four programming languages scala java python and r. to process data spark provides three abstractions for distributed data rdd resilient distributed dataset dataset and dataframe.
rdd is an immutable distributed collection of data elements that can be operated in parallel.
after spark .
the spark official guideline suggests replacing rdd with dataset and dataframe which provide richer apis and better performance optimization.
dataset and dataframe both abstract the representations of distributed data whereas the difference is that the data in dataset is strongly typed.
in this paper we implement our logging solution for spark s python api pyspark .
however the concepts are applicable to other programming languages and our prototype solution can be easily extended.
note that pyspark only supports dataframe since objects in python are weakly typed.
below we focus our discussion on dataframe.
spark apis leverage two important concepts in its design method chaining andlazy evaluation .
method chaining is used to ensure data immutability i.e.
dataframe objects are immutable towould you like a quick peek?
providing logging support to monitor data processing in big data applications esec fse august athens greece avoid concurrency issues and allows developers to create a data processing pipeline by chaining multiple data processing methods.
for example developers can call dataframe.filter x .dropduplicates .sort as a chain.
by chaining the three data processing methods i.e.
filter dropduplicates and sort each method would return a new dataframe object that is used as the input for the next method.
the final returned dataframe object will have values larger than three being filtered duplicates removed and sorted.
method chaining also provides an intuitive way for developers to combine multiple data processing methods.
to optimize the performance of the chained methods spark employs lazy evaluation for optimization.
there are two types of methods transformation and action.
for example filter is a transformation method and count is an action method i.e.
return the number of rows in the data .
all the transformation methods are lazily evaluated until an action method is called.
when an action method is called similar to compiler optimization some intermediate data processing steps in spark may be optimized combined or even eliminated to improve performance.
if no action method is called spark would not execute any transformation method.
for example dataframe.filter x will not filter the data unless an action method is called like count .
then spark will start to filter data and calculate the number of rows of the filtered data.
existing logging and monitoring supports for spark.
logging in spark in traditional software applications developers add logging statements in source code to record the program state and application runtime information.
these logs provide valuable information for developers to monitor application status and diagnose issues .
spark is no exception and uses log4j for logging.
all activities that occur inside spark can get logged to the shell console and or the configured underlying storage e.g.
to files on the disk or in databases .
by default logging in spark only records the information about spark s internals and does not record application execution information i.e.
does not show how the data is processed or transformed .
however developers may need to record the intermediate application and data states in a case when there is an issue during program execution or with the final result developers may be left in the dark.
nevertheless adding logging statements to retrieve and record the intermediate information of each data processing step can invalidate lazy evaluation and cause significant performance overhead e.g.
the data needs to be collected from all the worker nodes for each step .
cluster resource monitoring spark provides a web user interface ui that allows developers to monitor the status and resource consumption of a spark cluster.
in the web ui developers can monitor information such as job status and directed acyclic graphs that show how spark schedules and optimizes the data processing methods.
however the web ui only shows the internal execution information of the spark framework.
when there is an unexpected data processing output or error on the application side the web ui cannot provide much useful information.
to assist developers with logging and monitoring big data applications in this paper we first conduct an empirical study on the development challenges that developers encounter.
we manually analyze spark related questions on stack overflow with a focus onunderstanding the real world challenges that developers encounter when running spark application code and what kind of information if recorded is useful for developers to understand the intermediate outcome for supporting the effective development of spark applications.
based on our findings we design a logging solution called dplog which can better monitor data transformation and provide useful system execution information to developers especially supporting the commonly occurred issues that are encountered based on our empirical study.
below we discuss related work on the challenges and supports in developing big data applications.
understanding the challenges of developing big data applications.
bagherzadeh et al.
applied topic modeling i.e.
lda to study the topics of the questions that developers ask on stack overflow.
they find that developers ask questions about mapreduce debugging and basic concepts more frequently than some questions such as performance.
their exploratory study provides a landscape of big data questions that developers ask and is a starting point for future research.
however since the study is entirely quantitative the study provides limited insights on what types of information could be helpful for developing and debugging big data applications.
for example they did not discuss the challenges of using api functions or the data processing problems that developers encounter.
kim et al.
surveyed microsoft data scientists on the common challenges that they encounter.
they find that the most common challenges are related to data quality and the scale of the data.
fisher et al.
also interviewed data analysts at microsoft and they found that debugging in a distributed cloud environment is extremely challenging.
zhou et al.
analyzed issue reports from one of microsoft s big data platforms.
they find that more than of the issues are related to application design and code logic.
in this paper we focus our qualitative study on stack overflow questions related to spark development.
through our qualitative study we observe that questions related to data processing and spark api usage are the most common challenges that developers encounter.
in particular most questions are related to understanding how the data is processed and its intermediate state in a data processing pipeline.
we then design an approach aiming to assist developers in mitigating such challenges.
debugging big data applications.
dave et al.
proposed arthur which is a debugger for hadoop and spark.
arthur enables a user to selectively replay a part of the original computation.
gulzar et al.
developed a series of techniques to support debugging and testing for big data applications.
gulzar et al.
proposed bigdebug which is a debugger for spark applications.
after specifying the breakpoints manually users can use bigdebug to debug spark applications without needing to interrupt or re run spark applications during debugging.
in another work gulzar et al.
developed another debugging tool called bigsift .
given a known error caused by the input data users can specify a predicate that helps flag the problematic data entries.
then bigsift applies delta debugging to find the data entry for which the corresponding output violates the pre defined predicate.
different from prior debugging studies in this paper we focus on providing logging supports to developers.
we first conduct an empirical study to identify the challenges that developers encounter when developing sparkesec fse august athens greece zehao wang haoxiang zhang tse hsun peter chen and shaowei wang applications and identify the types of information that may assist developers in monitoring spark applications.
unlike debuggers logging provides insights on application execution and requires low performance overhead since it is often used in production settings.
debugger on the other hand is used to debug a known issue often makes the application runs hundreds of times slower and is used only in development settings.
challenges in developing spark applications in this section we analyze spark related questions that were asked on stack overflow.
we wish to understand the real world challenges that developers encounter and what kind of logging supports may assist developers with monitoring and developing spark applications.
stack overflow is being widely studied for understanding the challenges in various areas of software engineering such as security mobile development and ai based systems .
similarly we analyze spark related questions on stack overflow to understand the challenges in developing spark applications.
we download the stack overflow data dump that was released in september .
the data dump contains detailed information on every question and answer on stack overflow.
stack overflow requires every question to have at least one tag to illustrate its topic.
we use the tag apache spark to select all spark related posts i.e.
questions and the associated answers .
we follow prior studies to select only the questions that have a score that is higher than zero and has an accepted answer.
moreover we filter out the questions that do not have any code snippets since we wish to study the code snippets to further understand the possible causes of the challenge that the asker encountered.
we collected spark related questions that were asked between to spark .
was released in .
we conduct a qualitative study on a statistically significant sample of questions and their associated answers.
more specifically we randomly sample questions among these questions achieving a confidence level of and a confidence interval of .
we performed a lightweight open coding like process that involves three phases and is performed by two authors i.e.
a1 and a3 in the paper.
we describe the phases to conduct this qualitative study as follows phase i a1 and a3 collaboratively go through questions and their associated answers to derive an initial list of the challenges that developers encounter.
phase ii a1 and a3 independently go through the rest of the posts and assign the derived categories to these posts.
in this phase we did not find any new categories.
phase iii a1 and a3 compare their assigned categories and any disagreement is discussed until a consensus is reached.
the inter rater agreement has a cohen s kappa of .
before the consensus is reached which is a high level agreement .
our manual study result is publicly available .
we find that the most common challenge that developers encounter is related to data processing .
.
in general there are two categories of issues that developers encounter during data processing.
the first issue type is that the application may returnunexpected data processing results e.g.
a bug in the code but developers may have trouble in identifying which data processing method causes the issue.
for example a developer on stack overflow transformed the data by method chaining several data processing methods .
in this question the developer wishes to understand and verify the result of each step for testing purposes.
the suggested answer is to break the chained methods and test them separately.
the second issue type is that due to the vast number of supported frameworks and apis in spark developers may be unfamiliar with some api usage or data format.
without knowing how the data looks like and how it is processed developers may encounter unexpected challenges.
the second most common challenge is related to spark api usage .
.
most of the problems in this category are caused by improper uses of apis which leads to unexpected data processing results without any indication of errors e.g.
no exceptions .
since spark integrates the functional programming paradigm in its api design to abstract big data processing sometimes developers may not be familiar with the working mechanism of an api and can use the api incorrectly.
for example a developer asked a question on stack overflow that the fillna method did not fill the null value as expected .
the developer planned to fill the integer into all the cells that currently have a null value.
however the data type of the column is string while the developer planned to fill the data with integers.
in spark if the data type of the filled value does not match with the data type of a column the replacement would simply have no effect.
there will be no warning messages so it is difficult for developers to notice the issue.
some data processing methods also contain optional parameters that provide different ways to process data but developers may not always be aware of such options.
as an example a developer is confused about the difference between dropduplicates anddistinct .
both methods can remove duplicated data and dropduplicates has an additional parameter that is optional from which the developer can specify the duplicated columns to be removed.
in this case providing some hints on anomalous data processing results and parameter usage may help developers understand how the data is processed.
we also find that developers often encounter challenges in configuring spark .
and its interaction with other data sources .
.
developers often encounter configuration problems due to the variety and flexibility of configuration parameters.
as spark can integrate with a variety of data sources such as databases developers may have problems during this process.
we find that .
of the questions are related to performance and logging issues in spark deployment.
developers have difficulties in configuring spark s default logging monitoring spark execution in the cluster or improving the performance of data processing.
finally there are some questions that we categorize into other category .
which includes known and unresolved bugs in spark or questions that are related to programming language syntax.
in short we find that questions related to data processing and spark api usage are the most common challenges that developers encounter accounting for of the studied questions.
our manual analysis suggests that developers may need to know intermediate results after each data processing method is executed step by step to gain an overview of how their data is processed inwould you like a quick peek?
providing logging support to monitor data processing in big data applications esec fse august athens greece the pipeline.
providing hints or warning messages on api parameter usage and anomalous data processing results may also provide additional support to developers.
such execution information can help developers understand and monitor their applications.
another observation is that in most studied questions developers are more interested in knowing examples of how the data is processed.
therefore showing samples of data processing results may provide values for monitoring purposes.
due to the popularity of these issues we design a logging approach that may assist developers in monitoring data processing.
below we summarize the data processing and spark api usage challenges that we manually uncovered.
challenge data processing in spark usually involves a series of steps to transform the raw data into an understandable usable format.
however due to spark s method chaining and lazy evaluation features it is usually impossible for developers to know the intermediate results or state of the processed data during monitoring.
challenge due to the vast number of apis and their rich options developers may pass incorrect api options and result in unexpected results.
having a warning message on the used api and related options for each intermediate state may provide hints to developers on how the data is processed especially when api methods are chained together to form a complex task.
challenge most of the answers in the studied questions are related to re running the data processing methods separately.
however re running the application when large input data can be time consuming.
there is a lack of tooling supports that allow developers to monitor the data processing details with a reasonable runtime overhead.
to address the above mentioned challenges in the next section we discuss the design of a logging approach that has a small performance overhead and can be easily adapted to existing spark applications.
the design of dplog we present the design of our approach dplog which assists developers with monitoring and understanding the data processing execution.
dplog is a logging approach that provides the intermediate information e.g.
data changes and states and anomalies in the data processing from each of the executed spark methods.
table shows the list of data processing methods that are supported by dplog.
these methods cover all the basic data processing methods provided by pyspark s dataframe .
based on our empirical study results we follow the requirements described below when designing dplog req1 dplog should provide step by step data processing execution information to address challenge .to assist developers with spark development and provide necessary information for monitoring and diagnostic purposes dplog needs to show how the data is transformed processed after calling each method.
req2 dplog should provide hints to developers when a potential issue occurs during data processing to address challenge .
to assist developers with using dataprocessing methods in spark and identify potential issues with either the results or method usage dplog needs to provide some hints to developers to help locate or avoid misuses i.e.
similar to warn level logs in traditional logging .
req3 dplog should be scalable and have a low performance overhead to address challenge .
to assist developers with getting the intermediate information during runtime dplog needs to have a relatively low performance overhead.
below we discuss the design of dplog that fulfills the three above mentioned requirements.
.
req1 step wise application execution information recording intermediate information dplog records information of the data after each data processing method is executed during runtime.
note that there are some technical challenges in obtaining the intermediate results before a data processing pipeline is finished.
as discussed in section spark allows application developers to create a data processing pipeline by chaining methods and leverages lazy evaluation to optimize performance section .
discusses the implementation details of dplog to address the challenges .
in particular dplog records two types of information data state and data processing .
for data state dplog records the information of the data state before and after each method.
dplog records the data state differently for each method.
for example for filter dplog records the number of rows before and after applying filter .
for withcolumn i.e.
for creating a new column dplog records the number of rows and the statistics of the newly added column such as max min mean and standard deviation.
recording such data state information helps gain a high level overview of the data and how it changes.
for data processing dplog records a small sample of the data e.g.
rows for display purposes before and after applying each data processing method for the showcase.
therefore developers can see examples of how the data is transformed and processed through each step of the data processing pipeline.
if there is a logical bug in the data transformation process developers may be able to spot the bug and identify where the bug happens in the pipeline with the information provided by dplog.
.
req2 providing hints on the executed data processing methods to address req2 and provide hints to developers about the usage and potential issue for each data processing method we design dplog to record the anomalous result for each method call and provide possible hint messages.
dplog provides two types of hint messages anomalies in the data processing result and hints on the used values for the optional parameters in the data processing methods.
hints on anomalous data processing results bugs that developers face do not always run into exceptions or failures but may also be related to incorrect calculation or data.
for example if a developer wishes to delete a column in the data but instead the developer gives the name of another column by mistake.
in this case there will be no exceptions but the processed data will be incorrect.esec fse august athens greece zehao wang haoxiang zhang tse hsun peter chen and shaowei wang table the intermediate information and hints of the data processing methods recorded by dplog.
req1 req2 method data state data processing anomalies hint parameter hint filter filter condition number of rows before and after percentage of data changeddisplay samples of the filtered datano data or over are filtered n a dropduplicates number of rows before and after percentage of data changeddisplay samples of the removed datano data or over are deleted use default value for optional parameter distinct number of rows before and after percentage of data changeddisplay samples of the removed datano data or over are deleted n a dropna number of rows before and after number of nulls before and after percentage of data changedthe distribution of null values no data or over are deleted use default value for optional parameter drop drop condition number of columns before and after percentage of data changeddisplay samples of the dropped datanumber of deleted columns is n a not expected fillna number of nulls before and after percentage of data changedthe distribution of null values there are still null values use default value for optional parameter join join condition number of rows before and after percentage of data changedn a n a use default value for optional parameter withcolumn data type of the new column number of columns before and after statistical information about the new column max min mean std dev display samples of the new data n a n a sort sort condition samples of the original data display samples of the sorted datan a n a as shown in table dplog provides hints on anomalous data processing results for various methods.
for filter dropduplicates distinct and dropna dplog provides hints if the resulting data changes significantly or does not change at all either no data or over of the data is removed.
the assumption is that developers often apply the methods to remove some data but if no data or too much data is removed a hint message to warn the developers may be helpful.
note that developers can adjust the threshold value if needed.
for fillna we provide a hint message if there still exist null values in the data after executing fillna .
similarly for drop we provide a hint message if the specified column is not dropped as expected.
hints on method parameters as we found in section developers sometimes may not be familiar with the parameters used in data processing methods.
to assist developers dplog checks the values of the parameters given to the data processing method.
if the parameter value is not given and the default value is used dplog will provide a hint message on the effect of the default parameter value.
for example if the subset parameter of dropduplicates is empty by default spark will apply deduplication to all the columns and the behaviour of dropduplicates becomes the same asdistinct .
in this case dplog will provide a hint message on the effect of not providing the subset parameter.
the rationale is that if the developer has provided a value for the optional parameter the developer likely knows the effect of that parameter value .
in addition to dropduplicates dropna andfillna also have the subset parameter.
similarly dplog will provide a hint message if the value for subset is not provided i.e.
the operation will be applied to columns .
we also provide hints for dropna andjoin .
for example there is an optional parameter how which changes the behaviour of the method.
for dropna when howis set to any it drops a row if it contains any nulls when howis set to all it drops a row if all of its values are null.
similarly the how parameter in join specifies howthe data will be joined e.g.
inner join and left outer join .
dplog will give a hint message on these optional parameters if developers did not provide any value.
for the remaining methods that have no optional parameters the hint messages are not provided.
.
req3 minimizing performance overhead to make dplog practical dplog must be scalable so that it can handle large datasets and dplog must have a reasonable lowperformance overhead.
spark optimizes data processing pipelines i.e.
method chaining of multiple calls to data processing methods using lazy evaluation and other optimization techniques e.g.
removing redundant computation .
therefore if we directly record the intermediate information from every data processing method we would make the optimization techniques invalid and affect the performance.
fortunately many of the big data processing issues that we found during our manual analysis may also happen in a non big data setting.
therefore to minimize the performance overhead of dplog dplog first creates a statistically significant sample of the data and spawns a new spark job that applies the data processing methods step by step.
note that dplog processes the original data and the sampled data simultaneously.
when applying the data processing methods on the sampled data dplog records the intermediate information and provides hint messages.
dplog supports random data sampling and developers can choose the confidence level and confidence interval.
sampling is an effective way to provide a statistically significant representation of the data which is often precise and accurate .
by default dplog applies random sampling with a confidence level and a confidence interval.
.
implementation of dplog to minimize code changes and configurations when using dplog we implement dplog as an independent package.
we implementwould you like a quick peek?
providing logging support to monitor data processing in big data applications esec fse august athens greece hint message data state and processing with dplog.enable as df df.dropna original datasampling based on execute dropna warning over data is deleted!
sampled data no.
row before no.
row after no.
null before no.
null after deleted .
data null value distribution nationality samples of dropped data null ... original data after processingid nationality usa usa usa ... nullsampled data after processing id nationality usa usa usa execute dropna on original dataid nationality usa usa usa ... usaid nationality usa null null ... null figure a working example of dplog.
dplog in python and support pyspark spark s official python apis.
our implementation is based on python .
.
and is evaluated on spark .
.
.
dplog extends the functionality of pyspark but developers do not need to learn the working principle of dplog and hardly need to modify any of the existing code.
developers can import dplog as a package and enable dplog by simply adding with dplog.enable as df as shown in figure .
as shown in table dplog supports the following apis in pyspark filter drop dropduplicates distinct dropna fillna withcolumn sort and join .
dplog covers all the basic data processing methods provided by pyspark s dataframe .
when users read the data into a dataframe object dplog creates a new dataframe object that stores the statistically significant sample of the original data.
dplog processes the original data and the sampled data simultaneously according to the developers source code.
note that if needed developers can also run dplog on the original data even though the overhead will be significant i.e.
similar to the debug level in traditional logging frameworks .
dplog does not modify pyspark s source code.
instead it uses the adapter design pattern to extend pyspark s data processing methods without affecting their original implementations.
therefore even if there is a new release of pyspark or the method implementation is modified dplog can still be applied.
the output of dplog i.e.
intermediate information and hint messages can be saved to the location that the developer specified or be recorded together with spark s default logger.
in addition to the messages dplog will also save the sampled data to further assist monitoring and diagnosis if needed i.e.
developers can load the sampled data and diagnose potential issues .
figure shows a working example of dplog.
first dplog creates a sampled data based on confidence level and confidence interval.
then dplog processes the sampled data and provides both the hint message and data state.
developers only need to add with dplog.enable to enable dplog.
in this example the dataset contains a larger number of null values in the column nationality .
when the method dropna is called dplog provides a hint message that over of the data is deleted and shows the statistics and samples of the dropped data.
since dplogis executed concurrently with the original dataset the final result on the original data is not affected.
evaluation of dplog we evaluate dplog along two dimensions performance overhead and whether the recorded information can assist developers in understanding data processing and diagnosing potential issues.
rq1 what is the performance overhead of dplog?
motivation.
as mentioned in section to make dplog practical and scalable one of the requirements of dplog is to minimize the performance overhead.
therefore in this rq we evaluate the performance overhead and scalability of dplog.
approach.
our goal is to measure both the performance overhead and scalability of dplog.
in particular we implement six spark benchmarking programs for our experiment.
table provides an overview of the programs.
these programs showcase common approaches of how developers use spark for data processing and are similar to the programs used in a prior study .
to minimize possible performance costs related to other non spark code we design the programs so that they only leverage spark apis which is also how big data processing applications are typically designed and implemented .
the implementation of our benchmarking programs is available online .
we measure the original response time without using dplog the overhead of running dplog and the overhead of initializing dplog i.e.
sampling the data and creating a new dataframe .
to evaluate the effectiveness of our sampling mechanism we also measure the response time of running dplog while without sampling as a baseline.
namely the baseline profiles every data processing step.
by default the unmodified spark application would not monitor anything.
to measure the scalability of dplog we run each program using three different levels of data size i.e.
small medium and large 50mb 500mb and 5gb.
the data size is increased tenfold for each level to better illustrate the scalability.
georges et al.
found that performance measurements suffer from instability which may even lead to incorrect results.
to mitigate the issue we follow prioresec fse august athens greece zehao wang haoxiang zhang tse hsun peter chen and shaowei wang table the description of the test programs that are used for performance benchmarking.
description executed spark method p1 drop null value and filter data and add a new column filter dropna based on condition.
withcolumn p2 drop column and remove duplicate data distinct drop p3 drop column and null value and join two dataframe drop dropna join p4 fill null value and drop columns and filter data drop fillna filter p5 remove duplicate data and sort the data.
dropduplicates sort p6 join two dataframes and filter the data join filter studies and repeat each performance measurement times.
we run each program times i.e.
times for each data size and report the mean and standard deviation of the response time in seconds .
we run our experiments on a server with a .
ghz core intel core i7 cpu 16gb ddr4 memory and 500gb ssd disk.
results.
table shows the response time without dplog and with dplog the total response time with dplog after including the data sampling step and the baseline without sampling.
overall we find that the overhead of dplog is consistent across programs.
the runtime overhead w dplog w o dplog of dplog is around to seconds for the programs executed with three different data sizes.
the overhead remains approximately the same even when the data size is increased by folds i.e.
from small to large .
we did a t test to compare dplog and the baseline.
the results are all statistically significant p values are less than .
with large effect sizes.
our finding shows that dplog has good scalability since the overhead is consistently small even as the amount of data increases.
the reason may be that the sample sizes do not increase much especially when the data population is large so the overhead of applying the data processing methods is relatively consistent.
we also find that there is a larger overhead related to the initialization process when sampling the data w sampling overhead w dplog .
to sample the data dplog needs to first decide which data records should be sampled by generating a list of random indices.
then as the data is not indexed dplog needs to scan the entire data to find the corresponding data records which results in a larger sampling overhead.
however dplog only needs to perform sampling once even if there are multiple data processing methods in the pipeline.
furthermore we observe that the overhead of the baseline program is about to times higher than the overhead of dplog and the overhead grows with the data size.
it can be estimated that if the data size increases the overhead of the baseline will increase significantly.
in contrast our sampling mechanism reduces the overhead by at least when the data size is large and the reduction is higher when the data size increases.
finally we examine if the performance overhead of running dplog grows linearly as the data size increases.
we compute the response time ratio between running the programs with dplog and without dplog.
the ratio of the response time of with dplog over response of without dplog against different data sizes.
we observe that the ratios of the average overhead for small medium and large data sizes are .
.
and .
respectively.
namely the ratio of the overhead decreases as the data size increases.
one possible explanation is that as we explained above the sample sizes do not vary much when the data population size is large e.g.
the sample size eventually converges to when theconfidence level is and the confidence interval is no matter how big the data size is so the overhead of applying dplog is relatively consistent rather than growing linearly with the data size.
the overhead of dplog is significantly smaller than the baseline reduced by an average of over .
dplog is also scalable as we find that the relative performance overhead decreases to less than as the data size increases.
rq2 how effective is dplog in assisting developers with issue diagnosis?
motivation.
the execution information provided by dplog may be used for various monitoring tasks.
we also found in section that developers may encounter challenges in diagnosing issues in the data processing pipeline.
as found by beller et al .
most developers rely on logs to examine intermediate application execution state for issue diagnosis.
thus in this rq we investigate the effectiveness of dplog in assisting developers in diagnosing data processing related tasks.
approach.
we design a user study involving participants professional developers and graduate students .
these participants have one to five years of experience in either spark or big data analysis.
we design six issue diagnosis tasks based on the stack overflow questions that we studied in section .
each task involves some data processing code and an injected issue.
we abstract the irrelevant details from stack overflow posts and create a consistent format for the tasks so that developers can focus more on diagnosing the task itself.
the user study also facilitates benchmarking the efficiency improvement by using our tool since in real stack overflow questions developers may need to spend more time to read and comprehend the question.
to ensure the diversity of the selected tasks each task has a different issue either in the data or in the used data processing methods.
the tasks cover all the data processing methods that dplog supports.
the tasks are related to filtering data based on some conditions removing certain columns in the data filtering data removing duplicates and filling or dropping n a values.
the description of the tasks is available online .
each participant is assigned all six tasks and is required to diagnose three tasks with the help of dplog and diagnose another three tasks without using dplog.
we randomize the order of the tasks for each participant to reduce the bias from the learning curve.
note that all the necessary working environments are set up for the participants including the required packages and ide.
we provide detailed instructions on how to use dplog to each participant before starting the user study.
during the experiment each participant is provided with six source code files where each file corresponds to each debugging task.
the participants are allowed to run the program and make any necessary changes to identify the problem.
when the participants believe that they have found the root cause of the problem in the program we stop the timer.
we record the time it takes for each participant to finish each task and ask the participant to rank the usefulness of dplog on a scale from one to five where one is considered as strongly disagree i.e.
not useful and five is considered strongly agree i.e.
extremely useful .would you like a quick peek?
providing logging support to monitor data processing in big data applications esec fse august athens greece table the response time of the studied programs measured in seconds .
the data size is increased by tenfold for each size.
we show the average response time and standard deviation computed from the repeated runs.
w o dplog shows the response time without dplog w dplog shows the response with dplog considering only the runtime overhead w sampling overhead shows the total response time including both the runtime and initialization i.e.
sampling overhead and baseline shows the response time of the baseline without sampling .
small data size medium data size large data size w o dplog w dplog w sampling baseline w o dplog w dplog w sampling baseline w o dplog w dplog w sampling baseline overhead overhead overhead program1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
program2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
program3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
program4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
program5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
program6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
table the average time for the participants to finish the given task with and without dplog.
avg.
time w o dplog min avg.
time w dplog min improvement t1 .
.
t2 .
.
t3 .
.
t4 .
.
t5 .
.
t6 .
.
total .
.
result.
on average dplog reduces the needed time for the participants to diagnose the given tasks by .
table shows the average time it takes for the user to diagnose the programs.
without using dplog on average the participants spent around minutes to point out the potential causes of the issue.
when using dplog the average time reduced significantly to minutes.
for every task using dplog helps reduce the debugging time by to an average of .
for each task in the user study we did a t test to compare the needed time with and without the help of dplog.
the results are statistically significant and all effect sizes are large.
our findings show that dplog is effective in assisting the participants in monitoring and diagnosing data processing issues in spark applications.
participants all agree that dplog is effective in helping with debugging i.e.
the average rating is .
.
of the participants either agree or strongly agree that dplog is effective in assisting them with monitoring and diagnosing data processing in spark.
for example one participant mentioned developers could easily find which step caused wrong data.
this saves a lot of time.
among the participants of them strongly agree that dplog provides the needed support and of them agree that dplog provides assistance in diagnosing issues.
some participants mention that when the data processing pipeline is longer any issue that occurs during the pipeline becomes harder to diagnose and dplog is even more useful when there are more data processing methods in the pipeline .
our user study finds that dplog can reduce the needed time to diagnose the given tasks by an average of .
the participants gave an average rating of .
to dplog.
all of the participants either agree or strongly agree that dplog helps them with monitoring and diagnosing spark applications.
discussion implications of our study.
below we discuss the insights that we observed in our analysis on stack overflow posts and user study.
although the observations are related to data processing in spark our research framework of the empirical analysis and the proposed logging support tool can be easily extended to other big data applications in future research.
knowing the intermediate information of data is important for monitoring and debugging data processing applications.
in our user study for the tasks where the participants are not allowed to use dplog we observe that some participants tried to analyze spark s logs and use spark s cloud resource monitoring tool to debug the programs.
however even if the participants knew that there exist some issues in the program they could not identify the root causes using the existing approaches.
in most cases the participants found that manually printing the data state e.g.
calling print is the only useful approach for debugging.
some participants kept decomposing the chained methods printing the output of each individual method and checking for potential issues.
although we found that there were fewer manually added print statements when the participants use a python debugger they still need to continuously decompose the chained methods to manually debug the result of each data processing method.
more importantly for the tasks in which dplog is not used even after the participants found the issue they still need to conduct extra analysis to find the root cause of the issue in the programs.
different from existing debugging supports dplog provides the intermediate information of data processing methods which helps avoid decomposing the chained methods and reduce debugging effort.
for instance one participant mentioned that the information provided by the tool is very useful and precise.
i can find the reason for the problem much quicker based on the given information.
another participant mentioned that the tool is significantly better than printing information from the code.
the information provided by the tool is quite rich and helpful for locating the problem.
another participant mentioned the tool is very easy to use and provides useful information without manual debugging.
we also observe that in .
of the tasks in which dplog is used participants successfully identified the cause of the issue which is significantly higher than that of the tasks in which dplog is not used .
in other words providing the intermediate information does help participants identify an issue and its root cause.esec fse august athens greece zehao wang haoxiang zhang tse hsun peter chen and shaowei wang providing hints on anomalous data processing results helps identify issues more quickly.
as discussed in section dplog analyzes the execution of data processing methods and the value of their optional input parameters.
if there is an anomalous result dplog would provide a hint message.
in our user study we observe that such hints are useful for participants to notice the existence of an issue in the program.
for example one participant mentioned the most promising advantage of the tool is it can alarm users for anomalous behavior.
another participant mentioned through these hints it is easier for developers to quickly locate the abnormal behavior of the method or abnormal data.
limitations and future work.
our study provides an initial step towards understanding the data processing execution.
even though our findings show that the overhead of dplog is small and it can assist developers with issue diagnosis there are still some limitations and future research opportunities in assisting the development of big data applications.
data visualization and information presentation.
one common suggestion from the participants is related to improving the ui design.
currently dplog records the intermediate information and hint messages in the form of text e.g.
similar to traditional logs recorded by log4j without providing a rich user interface.
a participant said it would be perfect if the tool can finally come as an interactive form.
another participant said it is hard to find the warning message and useful tips.
i would suggest making the warning and tips easier to identify.
in addition to ui design we also observed that visualizing the results of the data processing methods may also help developers quickly identify issues and understand the data state.
for example we find that in certain cases some participants wanted to visualize the data using histograms to understand how the data distribution changes.
therefore future studies may also consider leveraging data visualization to assist developers with monitoring big data applications.
customizable information recording.
currently dplog records all the data processing information that is described in table .
however sometimes developers may already have an idea about possible issues that may occur and they only want to record certain information.
for example in our user study a participant mentioned that there is too much log information and it is not easy to locate the log i need immediately.
the participant is a professional developer who has years of experience in developing spark applications.
even though the participant found dplog to be useful gave dplog in terms of usefulness he suggested a customizable configuration for recording only the needed data processing information.
one approach may be to provide different logging levels such as debug info warn and error .
due to the vast amount of data and the complexity of big data applications future studies may also investigate approaches such as providing a domain specific language that could allow developers to record more customized and focused information to further assist monitoring and debugging.
more advanced debugging assistance.
we uncover common challenges that developers encounter by analyzing questions on stack overflow.
we then design an approach dplog and evaluate it by conducting a user study.
although our user study shows promising results there is still other information that can be added to assist developers.
for example future studies may investigatemore advanced techniques for providing hint messages for anomalous data processing results using machine learning or artificial intelligence.
moreover to reduce the overhead of dplog we apply random sampling to select a statistically significant subset of data.
although sampling is an effective technique to reduce the data size while providing good precision on the original future studies may investigate different sampling techniques and how they affect the effectiveness of debugging big data applications.
threats to validity external validity.
threats to external validity relate to the generalizability of our findings.
in section we studied the spark related questions on stack overflow.
the number of questions is large and it is impossible to study all of the questions qualitatively.
to minimize the bias we randomly sampled statistically representative questions giving a confidence level of and a confidence interval of .
we implement dplog to support only spark s python version.
however our proposed methodology could be applied to the other languages and frameworks.
future research is encouraged to enhance the support for other programming languages.
similarly we do not cover all the apis for data processing.
however in this study we cover all the basic data processing apis for pyspark s dataframe and our user study demonstrates that dplog is effective in helping developers identify issues and their root causes.
future research is encouraged to apply our approach to other data processing apis.
internal validity.
threats to internal validity are related to experimenter errors and bias.
we conducted a qualitative study in section which was performed by humans and bias may be introduced.
to reduce the bias each question is examined by two of the authors individually and discrepancies are discussed until a consensus is reached.
we measured the level of the inter rate agreement in our qualitative study and the agreement value is high i.e.
.
.
conclusion big data technologies have changed how companies and organizations make decisions.
spark as one of the most popular big data processing frameworks on the market has been widely used in developing big data applications.
in this study we analyze the challenges that spark developers encounter and propose dplog to assist developers in monitoring their big data applications.
in short this paper makes the following contributions we conduct an empirical study of spark related questions on stack overflow and identify the major challenges that spark developers encounter unknown intermediate data processing result and no support of warnings on improper api usages.
we propose an approach dplog to help developers monitor and diagnose data processing in spark and implement it as a python package.
dplog has a small runtime overhead.
through a user study we find that dplog effectively reduces the average debugging time by and the participants highly praised the usefulness of dplog.
we discuss the implication of our findings and future research direction that can further help developers develop and debug spark applications.would you like a quick peek?
providing logging support to monitor data processing in big data applications esec fse august athens greece