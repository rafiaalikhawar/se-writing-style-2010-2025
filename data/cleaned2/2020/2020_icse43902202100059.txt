idbench evaluating semantic representations of identifier names in source code yaza wainakh department of computer science tu darmstadt darmstadt germany yaza.wainakh gmail.commoiz rauf department of computer science university of stuttgart stuttgart germany moiz.rauf iste.uni stuttgart.demichael pradel department of computer science university of stuttgart stuttgart germany michael binaervarianz.de abstract identifier names convey useful information about the intended semantics of code.
name based program analyses use this information e.g.
to detect bugs to predict types and to improve the readability of code.
at the core of namebased analyses are semantic representations of identifiers e.g.
in the form of learned embeddings.
the high level goal of such a representation is to encode whether two identifiers e.g.
len andsize are semantically similar.
unfortunately it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers.
this paper presents idbench the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by software developers.
we use idbench to study state of the art embedding techniques proposed for natural language an embedding technique specifically designed for source code and lexical string distance functions.
our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness.
on the downside no existing technique provides a satisfactory representation of semantic similarities among other reasons because identifiers with opposing meanings are incorrectly considered to be similar which may lead to fatal mistakes e.g.
in a refactoring tool.
studying the strengths and weaknesses of the different techniques shows that they complement each other.
as a first step toward exploiting this complementarity we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.
index terms source code neural networks embeddings identifiers benchmark i. i ntroduction identifier names play an important role in writing understanding and maintaining high quality source code .
because they convey information about the meaning of variables functions classes and other program elements developers often rely on identifiers to understand code written by themselves and others.
beyond developers various automated techniques analyze use and improve identifier names.
for example identifiers have been used to find programming errors to mine specifications to infer types to predict the name of a method or to complete partial code using a learned language model .
techniques for this work was supported by the european research council erc grant agreement and by the german research foundation within the concsys and perf4js projects.improving identifier names pinpoint inappropriate names and suggest more suitable names .
the basic idea of all these approaches is to infer the intended meaning of a piece of code from the natural language information in identifiers possibly along with other information such as the structure of code data flow and control flow.
we here refer to program analyses that rely on identifier names as a primary source of information as name based analyses .
most name based analyses reason about names in one of two ways.
first some approaches build upon string distance functions such as the levenshtein distance sometimes in combination with algorithms for tokenizing names e.g.
based on underscore or camel case notation .
given a pair of identifiers e.g.
len andlength a string distance function yields a real valued number that indicates to what extent the character sequences in the identifiers resemble each other.
string distance functions are at the core of name based analyses to detect name related bugs to predict types to improve identifier names or to suggest appropriate names .
second another approach which has become popular more recently are neural network learned embeddings of identifiers .
an embedding maps each identifier into a continuous vector representation so that similar identifiers are mapped to similar vectors.
embeddings implicitly define a similarity function via the cosine similarity of embedding vectors.
for example embeddings of identifiers are at the core of neural program analyses to predict types to detect bugs to de obfuscate code to complete partial code and to map api elements across programming languages .
the common aim of both string distance functions and embeddings of identifiers is to reason about the semantics of identifiers and we hence call both of them semantic representations of identifiers or short semantic representations .
the overall effectiveness of a name based analysis relies on the assumption that the underlying semantic representation encodes some kind of semantic relationship between identifiers.
for example two semantically similar identifiers such as len and length should be closer to each other than two unrelated identifiers such as length andclick .
despite the importance of semantic representations for name based analyses it is currently unclear how well existing ieee acm 43rd international conference on software engineering icse .
ieee approaches actually represent semantic relationships.
specifically we are interested in the following questions a rq how accurately do state of the art semantic representations match the semantic relatedness of identifiers as perceived by software developers?
relatedness here means the degree of association between two identifiers which covers various possible relations between them e.g.
being used in the same application domain or being opposites of each other.
for example top andbottom are related because they are opposites click anddblclick are related because they belong to the same general concept and getborderwidth andgetpadding are related because they belong to the same application domain.
the relatedness of identifiers is relevant for tools that reason about the broad meaning of code elements e.g.
to predict the types of functions .
b rq how accurately do state of the art semantic representations match the semantic similarity of identifiers as perceived by software developers?
similarity here means the degree to which two identifiers have the same meaning in the sense that one could substitute the other without changing the overall meaning .
for example length andsize as well as username anduserid are similar to each other.
the similarity of identifiers is e.g.
relevant for name based bug detection tools .
c rq what are the strengths and weaknesses of the existing semantic representations?
better understanding why particular techniques sometimes succeed or fail to accurately represent identifiers will enable improving the current semantic representations.
d rq do the existing semantic representations complement each other?
if current techniques are complementary it may be possible to combine them in a way that outperforms the individual techniques.
addressing these questions relies on a way to measure and compare the effectiveness of semantic representations of identifiers in source code.
this paper presents idbench the first benchmark for this task which is based on a dataset of developer assessments about the relatedness and similarity of pairs of identifiers.
we gather this dataset through surveys that show real world identifiers and code snippets to hundreds of developers asking them to rate their semantic relationship.
taking the developer assessments as a gold standard idbench allows for evaluating semantic representations in a systematic way by measuring to what extent a semantic representation agrees with ratings given by developers.
moreover inspecting pairs of identifiers for which a representation strongly agrees or disagrees with the benchmark helps understand the strengths and weaknesses of the representation.
applying our methodology to seven widely used semantic representations leads to various novel insights.
we find that different techniques differ heavily in their ability to accurately represent identifier relatedness and similarity.
the best among the studied techniques the cbow variant of fasttext accurately represents the relatedness of identifiers rq but none of the available techniques accurately represents the similarity of identifiers rq .
studying the strengthsand weaknesses of each technique rq shows that some embeddings are confused about identifiers with opposite meaning e.g.
rows andcols about identifiers that belong to the same application domain but are not similar and about synonyms e.g.
file andrecord .
furthermore practically all techniques struggle with identifiers that use abbreviations which are very common in software.
we also find that simple string distance functions which measure the similarity of identifiers without any learning are surprisingly effective and even outperform some learned embeddings for the similarity task.
a close inspection of the results shows that different techniques complement each other rq .
to benefit from the strengths of multiple techniques we present a new semantic representation that combines the available techniques into an ensemble model based on features of identifiers such as the number of characters or whether an identifier contains nondictionary words.
the ensemble model clearly outperforms each of the existing semantic representations improving agreement with developers by and for relatedness and similarity respectively.
in summary this paper makes the following contributions.
a reusable benchmark .
we make available a benchmark of hundreds of pairs of identifiers providing a way to systematically evaluate existing and future embeddings.
to the best of our knowledge this is the first benchmark to systematically evaluate semantic representations of identifiers.
novel insights .
our study reveals both strengths and limitations of current semantic representations along with concrete examples to illustrate them.
these insights provide a basis for future work on better semantic representations.
a technique that outperforms the state of the art .
combining the currently available techniques based on a few simple features yields a semantic representation that clearly outperforms all individual techniques.
ii.
m ethodology to measure and compare the accuracy of semantic representations we gather thousands of ratings from developers section ii a .
cleaning and compiling this raw dataset into a benchmark yields several hundreds of pairs of identifiers with gold standard similarities section ii b .
we then measure the agreement between the gold standard and state of theart semantic representations section ii c where we study two string distance functions and five learned embeddings section ii d .
we apply our methodology to javascript code because recent work on identifier names and code embeddings focuses on this language but our methodology can also be applied to other languages.
a. developer surveys idbench includes three benchmark tasks a relatedness task and two tasks to measure how well an embedding 563identifiers radians angle how related are the identifiers?
unrelated related could one substitute the other?
not substitutable substitutable a direct survey.
which identifier fits best into the blanks?
positions indices opentip.
top topright right bottomright bottom bottomleft left topleft opentip.position ref opentip.
b indirect survey.
fig.
examples of the developer surveys.
reflects the similarity of identifiers a similarity task and a contextual similarity task .
the following describes how we gather developer assessments that provide data for these tasks.
the supplementary material provides additional examples and details of the survey setup.
a direct survey of developer assessments this survey shows two identifiers to a developer and then directly asks how related and how similar the identifiers are.
figure 1a shows an example question from the survey.
the developer is shown pairs of identifiers and is then asked to rate on a five point likert scale how related and how similar these identifiers are to each other.
in total each developer is shown pairs of identifiers which we randomly sample from a larger pool of pairs.
before showing the questions we provide a brief description of what the developers are supposed to do including an explanation of the terms related and substitutable .
the ratings gathered in the direct survey are the basis for the relatedness task and the similarity task of idbench.
b indirect survey of developer assessments this survey asks developers to pick an identifier that best fits a given code context which indirectly asks about the similarity of identifiers.
the motivation is that identifier names alone may not provide enough information to fully judge how similar they are .
for example without any context identifiers idx andhlmay cause confusion for developers who are trying to judge their similarity.
the survey addresses this challenge by showing the code context in which an identifier occurs and by asking the developers to decide which of two given identifiers best fits this context.
if for a specific pair of identifiers the developers choose both identifiers equally often then the identifiers are likely to be similar to each other since one can substitute the other.
figure 1b shows a question asked duringtable i occurrences of idbench identifiers in code corpora of different languages.
total occurrences occurrences of individual identifiers language number perc.
min mean max javascript .
python .
java .
the indirect survey.
as shown in the example for code contexts where the identifier occurs multiple times we show multiple blanks that all refer to the same identifier.
in total we show such questions to each participant of the survey where the identifier pairs are randomly selected from the set of all studied pairs.
the ratings gathered in the indirect survey are the basis for the contextual similarity tasks of idbench.
c selection of identifiers and code examples we select identifiers and code contexts from a corpus of javascript files .
we select pairs made out of identifiers through a combination of automated and manual selection aimed at a diverse set that covers different degrees of similarity and relatedness.
at first we extract from the code corpus all identifier names that appear more than times including method names variable names property names and other types of identifiers.
a naive approach would be to randomly sample pairs among those identifiers.
however this naive approach would result almost only in unrelated and dissimilar identifier pairs.
instead we follow a methodology proposed for natural language which ranks all pairs based on the cosine similarity according to a given embedding and then selects pairs from different ranges in the ranking.
we select pairs using two embeddings .
the fact that these embeddings are later also evaluated with the benchmark does not introduce bias because the ground truth of the benchmark is constructed only from the human ratings not from the embeddings.
in addition to pairs selected as suggested in we manually select some synonym pairs which we observed to lack otherwise and add randomly selected pairs which are likely to be unrelated.
the manual selection was done before evaluating any semantic representations to avoid biasing the benchmark.
to gather the code contexts for the indirect survey we search the code corpus for occurrences of the selected identifiers.
as the size of the context we choose five lines aiming to provide sufficient context to pick the best identifier without overwhelming the study participants with large amounts of code.
for each identifier we randomly select five different contexts.
when showing a specific pair of identifiers to a developer we randomly select one of the gathered contexts for one of the two identifiers.
table i shows how often the selected identifiers occur in the javascript corpus.
overall the identifiers in idbench occur .
million times which covers .
of all identifier occurrences.
even though this was not a criterion when selecting the identifiers the benchmarks covers a non negligible portion 564of real world code.
the table also shows how often individual identifiers occur which is times on average.
to assess whether idbench could also be used for other languages we also measure the occurrences in python and java code corpora with files each.
as shown in table i the identifiers are also frequent in code beyond javascript with an average number of occurrences of and in the python and java corpora respectively.
a manual analysis shows that identifiers that occur across languages cover general programming terminology whereas identifiers that appears in javascript only are mostly specific to the web domain e.g.
tag h4 ordomrange .
to better understand whether idbench covers identifiers that appear in different syntactic roles we measure for each identifier how often it used as a function name variable name or property name.
we then assign each identifier to one of these roles based on whether the majority of its occurrences is in a specific role.
the measurements show that of the identifiers are primarily function names are primarily variables names are primarily property names and the rest is commonly used in multiple roles.
d participants we recruit developers to participate in the survey in several ways.
about half of the participants are volunteers recruited via personal contacts posts in public developer forums and a post in an internal forum within a major software company.
the other half of the participants were recruited via amazon mechanical turk where we offered a compensation of one us dollar for completing both surveys.
on average participants took around minutes to complete both surveys.
that is the offered compensation matches the average salary of software developers in some countries of the world.2in total developers participate in the survey.
most participants live in north america and in india and they have at least five years of experience in software development.
b. data cleaning crowd sourced surveys may contain noise e.g.
due to lack of expertise or involvement by the participants .
to address this challenge we gather at least ten ratings per pair of identifiers and then clean the data based on the inter rater agreement which has been found effective in other crowdsourced surveys .
a removing outlier participants as a first filter we remove outlier participants based on the inter rater agreement which measures the degree of agreement between participants.
we use krippendorf s alpha coefficient because it handles unequal sample sizes which fits our data as not all participants rate the same pairs and not all pairs have the same number of ratings.
the coefficient ranges between zero and one where zero represents complete disagreement and one represents perfect agreement.
for each participant we calculate the difference between her rating and the average of all the other ratings for each pair.
then we average these differences for each rater and discard participants with a difference above developer salarytable ii benchmark sizes and inter rater agreement ira .
size thresholds task relatedness similarity contextual simil.
pairs ira pairs ira pairs small .
.
.
.
medium .
.
.
.
large .
.
.
.
table iii pairs of identifiers with their gold standard similarities.
score identifier identifier related similar contextual ness ity similarity substr substring .
.
.
setminutes setseconds .
.
.
reset clear .
.
.
rows columns .
.
.
setinterval clearinterval .
.
.
count total .
.
.
item entry .
.
.
miny ypos .
.
.
events rchecked .
.
.
re destruct .
.
.
a threshold values given in table ii .
we perform this computation both for the relatedness and similarity ratings from the direct survey and then remove outliers based on the average difference across both ratings.
b removing downer participants as a second filter we eliminate participants that decrease the overall inter rater agreement ira .
we call such participants downers because they bring the agreement level between all participants down.
for each participant p we compute ira simand ira rel before and after removing pfrom the data.
if ira simor ira rel increases by at least then we discard that participant s ratings.
c removing outlier pairs as a third filter we eliminate some pairs of identifiers used in the indirect survey.
since our random selection of code contexts may include contexts that are not helpful in deciding about the most suitable identifier the ratings for some pairs may be misleading.
for example this is the case for code contexts that contain short and meaningless identifiers or that mostly consist of comments unrelated to the missing identifier.
to mitigate this problem we remove a pair if the difference in similarity as rated in the direct and indirect surveys exceeds some threshold values given in table ii .
table ii shows the number of identifier pairs that remain in the benchmark after data cleaning.
for each of the three tasks we provide a small medium and large benchmark which differ in the thresholds used during data cleaning.
the smaller benchmarks use stricter thresholds and hence provide higher agreements between the participants whereas the larger benchmarks offer more pairs.
the thresholds are selected to strike a balance between increasing the overall inter rater 565agreement while keeping enough pairs and ratings to form a representative benchmark.
c. measuring agreement with the benchmark given the ground truth similarities and a semantic representation technique we want to measure to what extent both agree with each other.
a converting ratings to scores as a first step of measuring the agreement with the benchmark we convert the ratings gathered for a specific pair during the developer surveys into a similarity score in the range.
for the direct survey we scale the point likert scale ratings into the range and average all ratings for a specific pair of identifiers.
for the indirect survey we use a signal detection theory based approach for converting the collected ratings into numeric values which has been previously used to create a similarity benchmark for natural languages .
this conversion yields an unbounded distance measure dfor each pair which we convert into a similarity score sby normalizing and inverting the distance s d min d max d min dwheremin dandmax dare the minimum and maximum distances across all pairs.
b examples table iii shows representative examples of identifier pairs and their scores for the three benchmark tasks.
the examples illustrate that the scores match human intuition and that the gold standard clearly distinguishes relatedness from similarity.
some of the highly related and highly similar pairs e.g.
substr andsubstring are lexically similar while others are synonyms e.g.
count andtotal .
the identifiers rows andcolumns are strongly related but one cannot substitute the other and they hence have low similarity.
similarly miny ypos represent distinct properties of the variable y which is why they are related but not similar.
finally some pairs are either weakly or not at all related e.g.
reanddestruct .
c correlation with benchmark we measure the magnitude of agreement of a semantic representation with idbench by computing spearman s rank correlation between the similarities of pairs of identifier vectors according to the semantic representation and our gold standard of similarity scores.
definition correlation with benchmark givennpairs si gi of similarity scores where siis computed by a semantic representation and giis the gold standard let rank si andrank gi be the ranks of siandgi respectively.
the correlation of the semantic representation with the benchmark is r cov rank si rank gi rank si rank gi wherecovand are covariance and standard deviation of the rank variables respectively.
the correlation ranges between perfect agreement and complete disagreement .
for string distance functions we compute the similarity score si dnorm for each pair based on a normalized version dnorm of the distance returned by the string distance function.
we use spearman s rank correlation because directly comparing absolute similarities across different embeddings may be misleading .
the reason is 3the full list of identifiers pairs is available for download as part of our benchmark.that depending on how wide or narrow an embedding space is a cosine similarity of .
may mean a rather high or a rather low similarity.
a rank based comparison as provided by spearman s rank correlation is more robust to different ways of populating the embedding space with identifiers than computing the correlation of absolute similarities.
d. embeddings and string distance functions to assess how accurately existing semantic representations encode the relatedness and similarity of identifiers we evaluate seven semantic representations against idbench two string distance functions and five learned embeddings.
string distance functions use lexical similarity as a proxy for the semantic relatedness of identifiers.
we consider these functions because they are used in name based bug detection tools including a bug detection tool deployed at google to improve identifier names and to suggest appropriate names .
the two string distance functions we evaluate are lv levenshtein s edit distance which is the number of character insertions deletions and substitutions required to transform one identifier into another.
nw needleman wunsch distance which generalizes the levenshtein distance by computing global alignments of two strings.
learned embeddings are popular in recent name based analyses e.g.
for bug detection type prediction and for predicting names and types of program elements .
the five learned embeddings we evaluate are w2v cbow the continuous bag of words variant of word2vec .
w2v sg the skip gram variant of word2vec .
ft cbow the continuous bag of words variant of fasttext a sub word extension of word2vec that represents words as character n grams.
ft sg the skip gram variant of fasttext .
path based an embedding technique specifically designed for code which learns from paths through a structural tree based representation of code .
we train all embeddings on the same code corpus of javascript files .
for each embedding we experiment with various hyper parameters e.g.
dimension number of context words and report results only for the best performing models.4we provide all identifiers as they are to the semantic representations without pre processing or tokenizing identifiers.
the rationale is that such pre processing should be part of the semantic representation.
for example the nw string distance function aligns the characters of identifiers and the fasttext embeddings split identifiers into character n grams which may enable these techniques to reason about subtokens of an identifier.
566lv nw ft cbow ft sg w2v cbow w2v sg pathbased combined similarity functions0.
.
.
.
.8correlation with benchmarklarge benchm.
medium benchm.small benchm.
a relatedness.
lv nw ft cbow ft sg w2v cbow w2v sg pathbased combined similarity functions0.
.
.
.
.8correlation with benchmarklarge benchm.
medium benchm.small benchm.
b similarity.
lv nw ft cbow ft sg w2v cbow w2v sg pathbased combined similarity functions0.
.
.
.
.8correlation with benchmarklarge benchm.
medium benchm.small benchm.
c contextual similarity.
fig.
correlations of embeddings and string distance functions with the small medium and large variants of the benchmark.
iii.
r esults a. rq accuracy of representing semantic relatedness the following addresses the question how accurately the studied techniques represent the relatedness of identifiers i.e.
the degree of association between the two identifiers.
figure 2a shows the agreement of the evaluated semantic representations with the small medium and large variants of the relatedness benchmark in idbench.
all techniques achieve relatively high levels of agreement with correlations between and .
the neurally learned embeddings clearly outperform the string distance based similarity functions vs. showing that the effort of learning a semantic representation is worthwhile.
in particular the learned embeddings match or even slightly exceed the inter rater agreement which is considered an upper bound of how strongly an embedding may correlate with a similarity based benchmark .
comparing different embedding techniques with each other we find that both fasttext variants achieve higher scores than all other embeddings.
in contrast despite using additional structural information of source code path based embeddings score only comparably to word2vec.
a likely reason for the effectiveness of fasttext is that it generalizes across lexically similar names by computing embeddings based on character n grams of an identifier.
e.g.
given the identifier getindex fasttext computes its embedding based on embeddings for its various characters n grams such as index andind allowing the approach to generalize across lexically similar identifiers such as setindex orind.
b. rq accuracy of representing semantic similarity this research question is about the semantic similarity i.e.
the degree to which two identifiers have the same meaning.
figure 2b shows how much the studied semantic representations agree with the similarity benchmark in idbench.
overall the figure shows a much lower agreement with the gold standard than for relatedness.
one explanation is that encoding semantic similarity is a harder task than encoding the less strict 4details on the hyperparameters and how we tuned them are available in the supplementary material.concept of relatedness.
similar to relatedness ft cbow shows the strongest agreement ranging between and .
the results of the contextual similarity task figure 2c confirm the findings from the similarity task.
all studied techniques are less effective than for relatedness and ft cbow achieves the highest agreement with idbench.
a perhaps surprising result is that string distance functions are roughly as effective as some of the learned embeddings and sometimes even outperform them.
the reason is that some semantically similar identifiers are also lexically similar e.g.
len andlength .
one downside of string distance functions is that they miss synonymous identifiers e.g.
count and total .
c. rq strengths and weaknesses of existing techniques to better understand the strengths and weaknesses of the studied semantic representations we inspect various examples section iii c1 and study interesting subsets of all identifier pairs in isolation section iii c2 .
examples to better understand why current embeddings sometimes fail to accurately represent similarities table iv shows the most similar identifiers of selected identifiers according to the ft cbow and path based embeddings.
the examples illustrate two observations.
first fasttext due to its use of n grams tends to cluster identifiers based on lexical commonalities.
while many lexically similar identifiers are also semantically similar e.g.
substr andsubstring this approach misses other synonyms e.g.
item andentry .
another downside is that lexical similarity may also establish wrong relationships.
for example substring and substrcount represent different concepts but fasttext finds them to be highly similar.
second in contrast to fasttext path based embeddings tend to cluster words based on the structural and syntactical contexts they occur in.
this approach helps the embeddings to identify synonyms despite their lexical differences e.g.
count andtotal orfiles andrecords .
the downside is that it also clusters various related but not similar identifiers e.g.
mintext andmaxtext orsubstr and getpadding .
some of these identifiers even have opposing 567table iv top most similar identifiers by the ft cbow and path based models.
identifier embedding nearest neighbors substr ft cbow substring substrs subst substring1 substrcount path based substring getinstanceprop getpadding getminutes floor item ft cbow itemnr itemj iteml itemi itemat path based entry child record targ nextelement count ft cbow counttbl countint countrto countsasnum countone path based total limit minval exponent rate rows ft cbow roworrows rowxs rows l rowsar rowids path based cols cells columns tiles items setinterval ft cbow resetinterval settimeoutinterval clearinterval getinterval retinterval path based clearinterval assume alert nexttick reacttextcomponent mintext ft cbow maxtext minlengthtext microsectext maxlengthtext minutetext path based maxtext displaymsg blanktext disabletext emptytext files ft cbow filesobjs filesgen filesets extfiles libfiles path based records tasks names tiles todos miny ft cbow min y minby minx minpt min z path based minx ymin datamax datamin ymax meanings e.g.
rows andcols which can mislead code analysis tools when reasoning about the semantics of code.
interesting subsets of all identifier pairs to better understand the strengths and weaknesses of semantic representations for specific kinds of identifiers we analyze some interesting subsets of all identifier pairs in more detail.
we focus on four subsets abbreviations .
pairs where at least one identifier is an abbreviation and where both identifiers refer to the same concept e.g.
substr andsubstring orcfg and conf .
since abbreviations are commonly used for concise source code accurately reasoning about them is important.
opposites .
pairs where one identifier is the opposite of the other identifier e.g.
xmin andxmax or setinstanceprop andgetinstanceprop .
since opposite identifiers often occur in similar contexts they may be difficult to distinguish.
synonyms .
pairs that refer to the same concepts e.g.
reset andclear oremptytext andblanktext .
these identifiers often are lexically different but should be represented in a similar way.
added subtoken .
pairs where both identifiers are identical except that one adds a subtoken to the other e.g.
idand sessionid ormaxline andmaxlinelength .
tricky tokenization .
pairs where at least one of the identifiers is composed of multiple subtokens but uses neither camel case nor snail case to combine subtokens e.g.
touchmove andtouchend ornewtext and content .
this and the above subset are interesting because some semantic representations reason about subtokens of identifiers.
to extract pairs into these subsets we inspect all pairs from the small benchmark which yields between and pairs per set.
.
.
.5abbreviationsrelatedness similarity contextual similarity .
.
.5opposites .
.
.5synonyms .
.
.5added subtoken lv nw ft cbow ft sg w2v cbow w2v sg path based0.
.
.5tricky tokenization lv nw ft cbow ft sg w2v cbow w2v sg path based lv nw ft cbow ft sg w2v cbow w2v sg path basedfig.
agreement and disagreement with the benchmark for different kinds of identifiers.
568figure shows how much the different techniques agree or disagree with the benchmark for selected subsets.
as in figure each bar shows the spearman rank correlation between the predicted similarities and the ground truth.
that is higher values are better and negative values indicate a clear disagreement with the ground truth.
the results shows that all techniques are challenged by abbreviations with more than half of the correlations being negative.
the poor performance for abbreviations can be attributed to the fact that fewer characters provide less information and that there may be many variants of the same name.
for opposites and synonyms we find that most techniques and in particular the learned embeddings successfully represent the relatedness of these identifiers.
however almost all techniques clearly fail to capture that opposite identifiers are not similar as one cannot replace the other and to capture that synonyms are similar.
for the subtoken related subsets we find that most techniques are challenged by pairs where one identifier adds a subtoken to the other in particular when reasoning about similarity.
one explanation is that identifiers with an added subtoken tend to be rather specialized and hence occur less frequent which gives less training data to the learning based techniques.
when being faced with identifiers that use nonobvious tokenization most techniques with the exception of needleman wunsch perform relatively well.
we attribute this result to the fact that techniques that reason about substrings of an identifier such as fasttext do not rely on a specific tokenization approach such as camel case or snail case but instead consider character n grams of the given identifier.
d. rq complementarity of existing techniques our inspection of examples and of specific subsets of identifier pairs shows that different semantic representation techniques work well for different kinds of identifiers.
for example some techniques work better for abbreviations than others.
based on this observation we hypothesize that the existing semantic representations complement each other.
if this hypothesis is correct combining techniques in such a way that the most suitable set of techniques is used for a given pair of identifiers could represent similarities more accurately than any of the individual techniques.
to validate this hypothesis we present an ensemble model that combines existing semantic representations.
the key idea is to train a model that predicts the similarity of two identifiers based on the similarity scores provided by the existing semantic representations.
to this end the approach queries each of the seven techniques studied in this paper for a similarity score and provides these scores to the model.
to help the model decide what representations to favor for a given pair of identifiers we also provide to the model a set of features that describe some properties of identifiers.
given two identifiers the features we consider are the length of these identifiers.
the number of subtokens in each of the identifiers based on snail case and camel case conventions.
the number of words among the subtokens that are not in an english dictionary.
the rationale for this feature is to identify abbreviations which usually are not dictionary words.
given the seven similarity scores and the features we train a model that takes the scores and features of a pair as an input and then that predicts a similarity score for the pair.
we train the model in a supervised way using the ground truth provided in idbench as the labels for learning.
we use an off the shelf support vector machine model with the default hyperparameters provided by the underlying library5.
in practice one would train the model with all pairs in our benchmark and then apply the trained model to new pairs.
to enable us to measure the effectiveness of the model we here train it with all but one pair and then apply the trained model to the left out pair.
we repeat this step for each pair and use the score predicted by the model as the score of the combined technique.
figure shows the results of the combined approach.
combining different semantic representations clearly outperforms all existing techniques.
for example for the large benchmark the combined approach increases the relatedness similarity and contextual similarity of the best individual technique by and respectively.
this result confirms our hypothesis that the existing techniques complement each other and shows the benefits of combining them.
iv.
d iscussion this section discusses some lessons learned from our study of semantic representations along with ideas for addressing the current limitations in future work.
a neurally learned embeddings accurately represent the relatedness of identifiers overall all neural embeddings considered in our evaluation provide a high agreement with the ground truth provided by the relatedness scores in idbench.
this result shows that embeddings are effective at assigning similar vector representations to identifiers that occur in the same application domain or that are associated in some other way.
b no existing technique accurately represents the similarity of identifiers while the best available embeddings are highly effective at representing relatedness none of the studied techniques reaches the same level of agreement for similarity.
in fact even the best results in figures 2b and 2c clearly stay beyond the inter rater agreement of our benchmark showing a huge potential for improvement.
for many applications of embeddings of identifiers semantic similarity is crucial.
for example techniques that suggest suitable variable or method names aim for the name that is most similar not only most related to the concept represented by the variable or method.
likewise name based analyses for finding programming errors or variable misuses aim at identifying situations where the developer uses a wrong but perhaps related variable.
improving the ability of 5class sklearn.svm.svr from scikit learn.
569semantic representations to accurately represent the similarity of identifiers will benefit these name based analysis.
c neural embeddings generally outperform string distance functions our results for both relatedness and similarity show that the best available neural embeddings outperform classical string distance functions.
for example for the relatedness benchmark the string distance functions achieve up to correlation whereas embeddings achieve up to correlation.
for the similarity and contextual similarity benchmarks the differences are smaller vs. for similarity and vs. for contextual similarity but still clearly visible.
these results suggest that name based analyses are likely to benefit from using embeddings instead of string distance functions.
d opposite are challenging inspecting examples of in accurately represented pairs of identifiers shows that identifiers that describe opposing concepts are particularly challenging for current semantic representations.
for example both the ft cbow and path based embeddings assign similar vectors tomintext andmaxtext even though these identifiers are clearly not similar but only related.
another example are thesetinterval andclearinterval function names.
table iv shows these and other examples of this phenomenon.
improving semantic representations to better distinguish identifiers with opposing meaning will benefit name based analyses that e.g.
suggest method names or refactorings of identifiers .
e distinguishing singular and plural identifiers is particularly challenging another challenge we observe while inspecting pairs of inaccurately represented pairs of identifiers is to distinguish identifiers of individual items from identifiers of collections of items.
for example ft cbow assigns very similar vectors to substr andsubstrs table iv .
such a conflation of singular and plural concepts may be misleading e.g.
for name based analyses that predicts types .
f shared subword information may be misleading string distance functions and to some extent also subwordbased embeddings such as fasttext rely on the assumption that substrings shared by two identifiers increase the chance that the identifiers are semantically similar.
while a subwordbased approach helps deal with the out of vocabulary problem it may also mislead the semantic representation.
for example the ft cbow embedding assigns similar vectors to mintext andminutetext as well as to setinterval andclearinterval as these identifiers share subwords even though the identifiers refer to clearly different concepts.
g expanding abbreviations may improve semantic representations the finding that practically all existing semantic representations have difficulties with abbreviations raises the question how to address this limitation.
one promising direction is to expand abbreviations into longer identifiers before querying for their relatedness or similarity to another identifier.
several techniques for expanding identifiers have been proposed which could possibly be used as a preprocessing step within semantic representations.h different semantic representations complement each other the availability of different techniques for reasoning about the similarity of identifiers can be exploited by combining multiple such techniques.
our ensemble model section iii d shows the potential of combined approaches.
v. t hreats to validity a. threats to internal validity threats to internal validity are about factors that may influence our results.
the identifiers and the code examples associated with them may not be representative of other code.
to mitigate this threat we gather data from a large and diverse code corpus and we select identifiers that cover semantically similar and dissimilar pairs of identifiers section ii a0c .
the decision to perform our work with code written in a dynamically typed programming language javascript biases our results toward such languages.
the reason for focusing on a dynamically typed language is that such languages are the target of various name based analyses and embedding techniques .
some ratings gathered our surveys may be inaccurate e.g.
because participants may have misunderstood the instructions.
to mitigate this threat we gather at least ten ratings per pair of identifiers and then carefully clean the ratings gathered by developers to remove noise and outliers section ii b .
b. threats to external validity threats to external validity are about factors that may influence the generalizability of our results.
one limitation is that idbench focuses on individual identifiers only.
as a result it is not clear to what extent our evaluation of semantic representations of identifiers allows for conclusions about representations at a larger granularity e.g.
of complex expressions statements or sequences of statements.
we focus on individual identifiers as they are the basic building blocks of code.
recent work on improving name based and learningbased bug detection by aggregating identifiers in complex expressions suggests that improving embeddings for individual identifiers also benefits larger scale code representations .
another limitation is that other string distance functions or other embeddings may perform better or worse than those studied here.
we select semantic representations that have been used in past name based analyses as well as some recent embedding techniques that are state of the art in natural language processing nlp .
by making idbench publicly available we enable others to evaluate future semantic representations.
as any benchmark idbench consists of a finite set of subjects which may not be representative for all others.
the number of pairs of identifiers in the benchmark table ii is in the same order of magnitude as that of word similarity benchmarks used in nlp .
finally we focus on javascript code i.e.
our findings may not generalize to identifiers in other languages.
finally different name based analyses have different requirements on the semantic representations they build upon.
the tasks we present to survey participants may not represent 570all these requirements and hence a semantic representation may perform better or worse in a specific name based analysis than idbench suggests.
vi.
r elated work a name based program analysis various analyses exploit the rich information provided by identifier names e.g.
to find bugs and vulnerabilities to mine specifications to infer types based on identifier names as implicit type hints to predict the name of a method to complete partial code using a learned language model to identify inappropriate names to suggest more suitable names to resolve fully qualified type names of methods variables etc.
in a given code snippet or to map apis between programming languages based on an embedding of code tokens .
a systematic way of evaluating semantic representations of identifiers as provided in this paper helps in further exploiting the implicit knowledge encoded in identifiers and hence will benefit name based program analyses.
b embeddings of identifiers embeddings of identifiers are at the core of several code analysis tools.
a popular approach e.g.
for bug detection type prediction or vulnerability detection is applying word2vec to token sequences which corresponds to the word2vec embedding evaluated in section iii.
train an rnnbased language model and extract its final hidden layer as an embedding of identifiers.
chen et al.
provide a more comprehensive survey of embeddings for source code.
beyond learned embeddings string distance functions are used in other name based tools e.g.
for detecting bugs or for inferring specifications .
the quality of embeddings is crucial in these and other code analysis tools and idbench will help to improve the state of the art.
c embeddings of programs beyond embeddings of identifiers there is work on embedding larger parts of a program.
one approach uses a log bilinear neural language model to predict the names of methods.
other work embeds code based on graph neural networks or sequencebased neural networks applied to paths through a graph representation of code .
code2seq embeds code and then generates sequences of nl words .
for a broader overview and a detailed survey of learning based software analysis we refer the reader to and respectively.
to evaluate embeddings of programs the coset benchmark provides thousands of programs with semantic labels .
another study measures how effective pre trained code2vec embeddings are for different downstream tasks .
one conclusion from kang et al.
s work is that evaluating embeddings on a specific downstream task is insufficient a problem we here address with a task independent benchmark.
both of the above complement idbench because the existing work is about entire programs whereas idbench is about identifiers.
since identifiers are a basic building block of source code a benchmark for improving embeddings of identifiers will eventually also benefit learning based code analysis tools.d benchmarks of word embeddings the nlp community has a long tradition of reasoning about the semantics of words.
in particular that community has addressed the challenge of measuring how well a semantic representation of words matches actual relationships between words through a series of gold standards of words focusing on either relatedness or similarity of words.
these gold standards define how similar two words are based on ratings by human judges enabling an evaluation that measures how well an embedding reflects the human ratings.
unfortunately simply reusing these existing gold standards for identifiers in source code would be misleading.
one reason is that the vocabularies of natural languages and source code overlap only partially because source code contains various terms and abbreviations not found in natural language texts.
moreover source code has a constantly growing vocabulary as developers tend to quickly invent new identifiers e.g.
for newly emerging application domains .
finally even words present in both natural languages and source code may differ in their meaning due to computer science specific terms e.g.
float or string .
this work is the first to address the need for a gold standard for identifiers in code.
e data gathering asking human raters how related or similar two words are was first proposed by and then adopted by others .
our direct survey also follows this methodology.
propose to gather judgments about contextual similarity by asking participants to choose a word to fill in a blank an idea we adopt in our indirect survey.
to choose words and pairs of words prior work relies on manual selection pre existing free association databases e.g.
usf or verbnet or cosine similarities according to pre existing models .
we follow the latter approach as it minimizes human bias while covering a wide range of degrees of relatedness and similarity.
f inter rater agreement validating and cleaning data gathered via crowd sourcing based on the inter rater agreement has been found effective in other crowd sourced surveys .
gold standards for natural language words reach an inter rater agreement of .
and .
.
our small dataset reaches similar levels of agreement showing that the rates in idbench represent a genuine human intuition.
as noted by the inter rater agreement also gives an upper bound of the expected correlation between the tested model and the gold standard.
our results show that current models still leave plenty of room for improvement especially w.r.t.
similarity.
vii.
c onclusion this paper presents the first benchmark for evaluating semantic representations of identifiers names along with a study of current semantic representation techniques.
we compile thousands of ratings gathered from developers into a benchmark that provides gold standard similarity scores representing the relatedness similarity and contextual similarity of identifiers.
using idbench to experimentally compare two string distance functions and five embedding techniques shows that these techniques differ significantly in their agreement 571with our gold standard.
the best available embeddings are effective at representing how related identifiers are.
however all studied techniques show huge room for improvement in their ability to represent how similar identifiers are.
an indepth study of different subsets of identifiers shows the specific strengths and weaknesses of current semantic representations e.g.
that most techniques are challenged by abbreviations opposites and the difference between singular and plural.
to exploit the complementarity of current techniques we present an ensemble model that effectively combines them and clearly outperforms the best individual techniques.
our work will help addressing the limitations of current semantic representations of identifiers.
such progress will benefit downstream developer tools in particular name based program analyses.
more broadly improving semantic representations of identifiers will also contribute toward better learning based program testing and analysis techniques.