aid an automated detector for gender inclusivity bugs in oss project pages amreeta chatterjee1 mariam guizani1 catherine stevens1 jillian emard1 mary evelyn may1 margaret burnett1 iftekhar ahmed2 anita sarma1 1oregon state university corvallis or usa 2university of california irvine irvine ca fchattera guizanim stevecat emardj mayma g oregonstate.edu burnett eecs.oregonstate.edu iftekha uci.edu anita.sarma oregonstate.edu abstract the tools and infrastructure used in tech including open source software oss can embed inclusivity bugs features that disproportionately disadvantage particular groups of contributors.
to see whether oss developers have existing practices to ward off such bugs we surveyed oss developers.
our results show that a majority of developers do not use any inclusivity practices and of respondents cited a lack of concrete resources to enable them to do so.
to help fill this gap this paper introduces aid a tool that automates the gendermag method to systematically find gender inclusivity bugs in software.
we then present the results of the tool s evaluation on github projects.
the tool achieved precision of .
recall of .
an f measure of .
and even captured some inclusivity bugs that human gendermag teams missed.
index terms gender inclusivity automation open source information processing i. i ntroduction a new class of bug is beginning to gain visibility in software engineering literature the inclusivity bug.
an inclusivity bug is software behavior that disproportionately disadvantages a particular group of users of that software.
to date the most studied type of inclusivity bugs are gender inclusivity bugs most but not all of which disproportionately disadvantage women .
some gender inclusivity bugs in software manifest themselves overtly such as with non inclusive language gender stereotyped game characters or genderstereotyped job matches .
detecting overt genderinclusivity bugs is relatively straightforward by definition of the term overt although actually fixing them is not always straightforward .
however non overt gender inclusivity bugs are not so obvious such as software behaviors and workflows that are biased against cognitive or behavioral styles more common in one gender than another.
for example among the software relevant gender differences pointed out in stumpf et al.
s survey are gender clusters of differences in the colors and sounds people can distinguish how people comprehend and interpret verbal written communications their spatial processing their attitudes toward technological risks and how they process information .
the extent to which such non overt gender inclusivity bugs permeate software is large.
the lowest percentage we have been able to locate is one team s report of inclusivity bugs in only of the features they considered other teams have reported much higher rates of inclusivity bugs .
for example in one recent study on a group of oss projects the rate of gender inclusivity bugs reported by oss professionals that were then verified by oss newcomers was .
fortunately there are methods that can help software professionals find fix and or avert inclusivity bugs e.g.
.
one of these is gendermag gender inclusiveness magnifier which provides the foundation of the tool presented in this paper.
gendermag is a method that enables developers to systematically find gender inclusivity bugs in their software or workflow so that they can fix the bugs.
gendermag variants have been used in a variety of domains including digital libraries learning tools and websites machine learning interfaces robotics search engines and oss projects .
unfortunately however using any of these methods can be expensive because they are entirely manual and laborintensive.
for example gendermag s creators recommend that at least evaluators spend hours per session where a session usually covers only use cases.
in fact finding ways to reduce the cost was a major theme in a recent field study of teams work to integrate gendermag into their development processes .
can a software tool help to address this problem to increase the viability of debugging inclusivity bugs that lurk in software?
to find out we conducted a survey of experienced oss developers oss ers to learn what tools and other inclusivity debugging resources developers actually use.
informed by the study we then created aid a tool that automates a vertical slice of gendermag.
we used a vertical slice instead of all of gendermag for two reasons.
first since this is the first of a new class of software tools inclusivity bug detectors we needed a tractable way to investigate whether this class of tools is even possible.
second it enabled us to empirically investigate aid s effectiveness under the strict controls that a single vertical slice affords.
specifically this version of the tool uses a portion of allthe components of gendermag a portion of its scope oss github projects one of its personas abi and one of its cognitive styles the information processing style .
we then empirically investigated ieee acm 43rd international conference on software engineering icse .
ieee table i thefacets a bi s values for each see for the foundations behind each facet abi s values moti vations uses technology to accomplish tasks computer self efficacy low compared to peers attitude towards risk risk a verse information processing style comprehensi ve learning style learns by process thetool s results on oss github projects.
this paper presents aid and reports our empirical findings.
the contributions of this paper are a survey of experienced oss developers inclusivity debugging practices and the tools guidelines they use for these practices aid the first software tool to automate the detection of inclusivity bugs and an empirical investigation of aid s efficacy on oss github projects.
ii.
b ackground thegender magmethod aid is a partial automation of the gendermag method .
gendermag is a process that enables software professionals to systematically locate gender inclusivity bugs in the userfacing portions of their software.
locating these bugs is a prerequisite to fixing them.
gendermag has been used successfully in a variety of domains including university webware educational software machine learning interfaces mobile apps digital libraries search engines and software tools .
at the core of gendermag method are five problem solving styles or facets each backed by extensive foundational research .
the five facets are motivations for using technology computer self efficacy attitude towards risk information processing style and learning style.
each facet has a range of possible values and gendermag uses three multi personas abi pat and tim to bring different facet values to life table i and figure .
each persona embodies a different set of values for the five facets.
facet values statistically more common among women are assigned to abi those more common among men are assigned to tim and a mix of various facet values are assigned to pat.
these facet values are what define each persona s problem solving style.
other persona attributes e.g.
job age hobbies preferred pronouns etc... are customizable.
of particular interest to this paper is the information processing facet as it pertains to the abi persona because the tool we present is a partial automation of that facet from abi s perspective.
the information processing facet describes how a user receives new information to make decisions about actions to take in order to accomplish a task.
the abi persona processes information comprehensively that is by gathering fairly complete information before proceeding.
at the opposite end of the information processing spectrum of values is the tim persona who is more likely to process informationselectively and act upon the first promising information they find then backtrack if needed .
to conduct a gendermag session a software team channels one of these personas chooses a use case a task for the persona to try to accomplish and walks through that use case task using a specialized version of the cognitive walkthrough cw family .
in this process the team answers one question for each subgoal and two questions for each action that the software team is hoping a user will take to accomplish the task.
the action questions are answered before and after performing the action.
the questions are fig.
.
the abi persona with their information processing style facet value enlarged for readability.
see supplemental materials at for the full version.
subgoal question s q will persona have formed this subgoal as a step to their overall goal?
yes no maybe why what facets did you use action question a q will persona know to take the action?
yes no maybe why what facets did you use action question a q if persona takes the action will they know they did the right thing and are making progress toward their goal?
yes no maybe why what facets did you use if the team answers as no maybe to any of these questions and attributes it to one of the persona s facet then and only then it is considered an inclusivity bug.
aid automates the process described above for abi s information processing facet value.
iii.
f ormative study to understand the state of inclusivity evaluations in oss and the challenges behind incorporating such evaluations in the development process we conducted a formative investigation in the form of a survey.
a. survey methodology we recruited oss developers as survey participants as our goals were to learn about inclusivity evaluation practices and needs inside oss.
we aimed to get the perspective of experienced oss developers since in this version of aid we are using github based project pages as our test subjects.
1424table ii inclusivity practice survey data grouped by demographics .
yes of no of respondents respondents respondents size respondents 3months respondents 6months respondents 7mo.
yr. respondents experience 1year respondents paid respondents participation type unpaid respondents when recruiting participants we used the number of followers as a filtering criterion since experienced oss developers are more likely than newcomers to accrue followers.
for example blincoe et al.
found that popular github oss ers influence their followers and attract them to new projects .
we used the number of followers as a filter criterion because experienced oss developers are more likely than newcomers to accrue followers.
for example blincoe et al.
found that popular github oss ers influence their followers and attract them to new projects .
in order to do that we used the github search tool to retrieve usernames that are among the top followed github oss ers.
to get the contacts of this list usernames we modified a tool called zen which obtained email addresses for of these usernames.
we sent the survey to these experienced oss developers.
the survey had a total of eight questions comprising yes no multiple choice and open ended questions.
in addition to collecting demographic information the survey used branching logic when asking about existing inclusivity evaluation practices.
if the participant indicated that their development practices include some form of inclusivity evaluation we asked more about these otherwise we asked what challenges prevent them from so.
in pilot runs the survey took about five minutes to complete.
we emailed this survey to developers.
of the emails bounced or were unreachable which resulted in recipients.
at the end of the two weeks during which the survey remained open we received responses or a response rate of .
.
this response rate compares favorably to the .
survey response rate found in a previous software engineering study .
we removed the survey responses that had partial answers leaving us with responses.
the survey questions and the tool used to retrieve the developers email addresses are provided at .
b. survey results over three fourths of the survey respondents reported that they do not incorporate inclusivity practices into their software development processes.
as table ii s rightmost column shows the majority of every demographic group reported not using inclusivity practices.
when those who responded no were asked why they responded as summarized in figure .
almost half the reasonsrelated to lack of awareness interest.
specifically about of the reasons were a lack of awareness of such practices or of the importance of so another reported lack of support from management and other please specify explained that such practices were up to individual developers or that they were unnecessary unimportant or even harmful p84 inclusivity happens on an ad hoc basis and depends on the individual engineer... p123 everyone i work with uses internet handles.
i have no clue if ladyothelake is a guy gal minority straight gay ... we just code and make good stuff.
p145 open source communities have never prioritized inclusivity.
the above reasons are perhaps unsurprising given that the concept of finding fixing inclusivity bugs within technology itself beyond accessibility e.g.
providing alt text is a relatively recent one in software engineering.
for example to the best of our knowledge the first paper in the software engineering community to even mention the concept of software s inclusivity bugs was only a decade ago and the first software engineering presentation of a systematic process for software developers to find such bugs wasn t until .
reasons like the above are at best only indirectly addressable by creating a new tool.
however the remaining of the reasons survey participants gave lack of specific guidelines the costs of inclusivity evaluation and lack of tools are all potential fodder for software engineering tools.
in fact even the of respondents who did report using inclusivity practices in their development processes did not have much in the way of concrete techniques in their existing inclusivity efforts.
only of these yes respondents less than one third of the yes s reported using a technique of any sort.
of these yes respondents nine use mainly ad hoc inclusivity techniques figure bottom four bars such as inspecting their work for stereotyping language or character depictions relying on user feedback or user study results or following general codes of conduct.
another nine relied on internal custom tools figure second bar from the top .
only four public inclusivity techniques were reported by any respondents gendermag with without the gendermag recorder s assistant respondents public guidelines for inclusive language in code one respondent public guidelines for inclusive documentation one respondent and public guidelines for ethical collection of training data one respondent .
gendermag was the only inclusivity technique used by multiple respondents.
the above respondents the only respondents who reported using any specific techniques tools or guidelines are only of the total respondents.
the other yes s instead described a lack of tools and or guidelines in their inclusivity efforts p299 we don t use specific techniques... p130 lack of knowledge and tools makes developers reinventing the wheels all the time.
1425p60 ...there are no such guidelines or tools ... i hope that just as we have guidelines for code quality we also have inclusivity principles... fig.
.
left no respondents reasons challenges for not using inclusivity as a percentage of all challenges.
note data shown totals due to rounding of each individual bar.
right yes respondents uses of different inclusivity techniques.
iv.
aid even the few survey respondents who reported using a concrete technique had only a little ammunition to address three of the tool relevant challenges alluded to above costs lack of tools and lack of concrete guidelines.
regarding costs gendermag was the only publicly available method used and it is costly requiring teams of multiple software professionals to work together in sessions that can last two or more hours .
regarding tools other than a few mentions of internal tools no respondent reported tool usage other than the recorder s assistant which is simply a note taking organizational aid.
regarding guidelines a few respondents pointed to concrete guidelines but none went beyond language use and stereotyping issues.
together these three categories accounted for of the challenges reported by survey participants who do not use any form of inclusivity evaluation figure and were also pointed out by some participants who douse some kind of inclusivity evaluation.
to overcome these challenges we created aid a tool whose ultimate goal is to automatically detect non overt gender inclusivity bugs like those described in section i. in this paper we report on our first step automating a vertical slice of gendermag inclusivity evaluations toward this ultimate goal.
fig.
.
a snapshot of a spreadsheet with inputs and outputs from a manual gendermag session information processing facet only .
aid uses the subgoal and action columns as input and produces similar outputs as the manual gendermag session.
orange backgrounds use case and subgoal inputs white backgrounds action inputs blue backgrounds gendermag evaluation outputs.
fig.
.
overview of aid s architecture one choice that defined our vertical slice was the type of software we scoped our tool to.
we scoped our tool to use oss platforms which is an important genre for inclusivity bug detection because these platforms are the sole mechanism through which oss community members can interact and oss tools and technology have pervasive genderinclusivity bugs .
another choice in defining our vertical slice was the persona.
we chose gendermag s abi persona as per the abi first practice commonly used because abi has historically provided the most powerful lens for revealing inclusivity bugs .
the third choice was the facet.
we decided on the information processing style facet because oss projects are information rich environments.
for this vertical slice aid takes as inputs the use cases subgoals and actions for a gendermag session as well as the urls of the project s github pages figure .
using these inputs aid evaluates the project s github pages against its model and produces an inclusivity bug report similar to the blue region of figure .
a. deriving the model the question of how to automate gendermag presents a number of challenges.
first there are an infinite number of platforms and uis that could be evaluated each of which have an infinite number of use cases.
second as a member of the cw family gendermag involves simulating other humans a task that some people find difficult to accomplish .
the few researchers who have automated cws for limited purposes e.g.
have done so under the assumption that users are homogeneous and have not attempted to take into account the range in people s cognitive styles.
finally extensive gendermag session data does not yet exist which prevents some kinds of approaches from being viable.
for example a machine learning approach would require training a model using data from millions of gendermag sessions including the context associated with each platform ui elements etc.
since aid is the first investigation into whether an inclusivity bug detector is even possible the tool s featurecompleteness was not a critical consideration.
thus we counteracted the first and second challenges via the vertical slice already discussed.
to address the third challenge we turned to a decision rule approach.
an advantage of decision rules is that beyond statistical patterns they can also harness relevant theories such as those in gendermag s foundational core e.g.
self efficacy theory information processing theory etc.
thus they do not require nearly as much empirical data.
1426that said we still needed some empirical data to ground our decision rules in oss relevant data the inclusivity bugs in real oss projects and why they occur recall figure .
toward that end we obtained gendermag session data from two active oss teams projects f and j that have already used gendermag to improve their projects inclusivity.
project f is a github based platform to help newcomers become familiar with oss it has contributors and .
million lines of code.
project j is a github based data science project with contributors and .
million lines of code.
the project teams shared their gendermag session materials which included their use cases customized gendermag personas and evaluation outputs recall figure .
together these teams data identified inclusivity bugs related to abi s information processing style.
to supplement the data obtained from the external projects we sought out additional github based oss projects to conduct additional gendermag evaluations.
we turned to the classification by fronchetti et al.
which classified oss github based projects into three groups based on the average growth of newcomers logarithmic growth projects linear growth projects and exponential growth projects .
we randomly selected six projects from each group resulting in projects of varying maturity size and domain table iii .
we then manually conducted gendermag sessions on them to produce data about the inclusivity bugs within these projects pertaining to the information processing style facet.
in order to remain consistent with sessions from projects f j we used the same abi persona which was customized to be a newcomer and use cases that they had used table iv .
these sessions covered actions and produced answers to a total of questions two questions per action section ii .
the sessions also produced answers to subgoal questions but since none referred to information processing style we did not include them in the analysis.
the answers to these questions revealed inclusivity bugs steps in the use cases where oss ers with abi s comprehensive information processing style would be disadvantaged.
these inclusivity bugs plus the inclusivity bugs from the projects f j a total of inclusivity bugs comprised the dataset from which we derived the model s decision rules.
b. reliability safeguards we used multiple strategies to safeguard the reliability of the gendermag session outputs see figure used to infer our decision rules and the empirical results from aid.
inter rater reliability of gendermag session outputs the first safeguard was inter rater reliability to ensure consistency of the gendermag session outputs we conducted on the projects.
two pairs of researchers had each conducted gendermag sessions.
to measure consistency of their gendermag session outputs we drew upon inter rater reliability calculations often used with qualitative analysis .
specifically each team independently ran gendermag sessions on of the use cases.
they then compared their outputs usingthe jaccard index to calculate a consensus which resulted in agreement.
given this level of team consensus the two teams divided the remaining projects.
validation against oss teams gendermag session outputs the above is a measure of consistency between the two teams of researchers.
to measure consistency with real oss teams gendermag outputs we validated our research teams outputs against those from projects f j. recall that teams f j conducted the gendermag sessions themselves on their own projects.
we grouped the inclusivity bugs from projects f j into categories for cross validation purposes table viii which will be discussed further in section v through three rounds of negotiated agreement among four researchers.
validating the gendermag sessions we conducted against those of projects f j of the inclusivity bugs from the sessions we conducted matched the inclusivity bug categories of projects f j. validating in the other direction of the inclusivity bugs from projects f j s gendermag sessions matched the inclusivity bugs in the sessions we conducted.
cross validation of tool results finally we crossvalidated each individual inclusivity bug that aid identified for all projects against the inclusivity bugs manually identified in gendermag sessions on all projects.
the results of this cross validation are in section v. c. model decision rules to create the decision rules driving aid the first and second authors analyzed the gendermag data from the projects f j gendermag ed by the research team .
they analyzed each inclusivity bug and the whys behind it as well as the ui.
through inductive reasoning they abstracted the whys into a set of decision rules.
then through three rounds of negotiated agreement they refined and merged similar rules to create the final set of five distinct rules which form the foundation of aid.
aid attempts to emulate a real gendermag session by taking as input a spreadsheet that lists the data that humans would use when performing a gendermag session.
the first three columns are the use case subgoal and action inputs and the fourth column is the webpage url for that step in the cw.
section ii details a gendermag session process.
figure displays the overview of aid s architecture and figure shows an example of an input sent to aid for the use case file an issue .
from this input aid extracts the sentence structure from the text in the subgoal and action.
it does so through a combination of natural language processing nlp techniques such as lexical analysis part of speech tagging and dependency parsing .
overall aid uses python .
.
and associated libraries spacy beautifulsoup gensim xlrd to implement the decision rules.
the code is available at .
the implementation details of each rule are discussed next.
cues to needed information the first rule in our set checks for situations where oss ers like abi would not find all the information they need to complete their task.
for 1427table iii characteristics of the 18projects that researchers evaluated using gender mag name programming language loc contributors age domain imathis octopress ruby application software chaplinjs chaplin coffeescript web libraries and frameworks antirez disque c non web libraries and frameworks sahat satellizer typescript web libraries and frameworkslogarithmic growth winjs winjs javascipt web libraries and frameworks ionic team ionic framework typescript non web libraries and frameworks scalaz scalaz scala non web libraries and frameworks donnemartin system design primer python web libraries and frameworks synrc n2o erlang system software tmux tmux c application softwareexponential growth sbt sbt scala software tools ubernetes kubernetes go software tools rails rails ruby software tools symfony symfony php web libraries and frameworks definitelytyped definitelytyped typescript documentation alpaca lang alpaca erlang software toolslinear growth libuv libuv c system software table iv use cases used for gender mag evaluations use case uc1 use case find an issue to work on uc2 use case file an issue uc3 use case make a documentation contribution fig.
.
aid s approach for capturing inclusivity bugs rule example the wording of the subgoal serves as the information that abi seeks and the words from actions serve as cues to direct abi to a ui action.
without such cues abi would face difficulty finding all the information they need.
for example for project4 uc1 find an issue to work on s1 find the issue list s a ction1 the human team reported project4uc1s1a1 there s nothing about the issue list .
issue is mentioned in one of the sections but in a different context .
rule captures this using the model shown in figure .
rule keywords from subgoals and associated actions should be present on the webpage.
to check if the webpage includes the words from a subgoal and action sequence aid extracts lexical features including the part of speech pos tag of each word using spacy .it then adds the nouns and adjectives from the pos to a list of keywords that it searches for in the webpage.
for example in the third row of table v abi s subgoal s2 is to file a new bug report issue and the associated action a1 is click on the tracker link .
aid parses these sentences to add and to the list of keywords.
the tool does not include verbs e.g.
click since these are usually commands to a human not labels or contentoriented keywords.
we also exclude dom words e.g.
link header footer etc.
and words like information and data from the list of keywords because these words can have overly broad interpretations.
the next step is to look for these keywords in the webpage.
our initial approach performed a simple string search.
however this resulted in keyword matches that were on parts of the webpage that were irrelevant for the specific subgoal.
thus to detect relevant information to the specific subgoal and action under evaluation we retrieve all the sentences on the webpage that contain any of the keywords.
then we use dependency parsing to extract the relationship between the words in each sentence as shown in figure .
the term subtree refers to smaller syntactic units within these sentences.
aid then navigates the dependency parse tree as shown in figure by extracting the subtree of each of the keywords in them.
if one keyword is found in another keyword s subtree it shows a syntactic relationship between them and is considered relevant to the subgoal or action.
for example figure shows the tree structure from a sentence in the webpage of project before submitting a pull request we ask that you please create an issue that explains the bug or feature request .
this results in a dependency parse tree with ask as the root.
we check the subtree of issue and see that bug another keyword is in the list of elements of the tree.
therefore aid determines that this sentence is indeed about issues that are bugs and does not report an inclusivity bug when evaluating the action a1 for subgoal s2 in table v. 1428table v example of input to aid use case subgoal action url uc2 file an issue s1 find information about filing an issue a1 q click on reporting a bug symfon y.com doc current contributing code index.html uc2 file an issue s1 find information about filing an issue a1 q click on reporting a bug symfon y.com doc current contributing code bugs.html uc2 file an issue s2 file a new bug report issue a1 q click on the tracker link symfon y.com doc current contributing code bugs.html uc2 file an issue s2 file a new bug report issue a1 q click on the tracker link github .com symfony symfony issues fig.
.
the dependency parse tree for the sentence before submitting a pull request we ask that you please create an issue that explains the bug or feature request.
the dashed blue border encompasses all the elements in the subtree of the keyword issue .
the keywords issue and bug are bordered in green.
situating abi in the context of the action performed on clicking a link the destination page should offer cues to help abi s understand that they have reached the right place.
if a project page fails to use words similar to what a link label hinted at oss ers like abi could get confused.
for example when the team clicked on a link labeled how to file an issue but didn t find relevant information on the resulting page their inclusivity bug report said project1 uc2s1a3 the link takes to a page that does not really mention how to file issues which is what the link said.
abi might not think that this is where she wanted to go.
this team s observation echoes others recommendations that words in a link s label should be prominent on the link s destination page leading to our second rule rule linked pages should contain keywords from link labels.
aid checks for rule for linked pages and thus the pages that are an input to action question e.g.
... issues in s2 a1 q in table v .
but since it evaluates each page independently it is agnostic of past cw steps it needs to identify the link in the previous step that brought the oss er to this page.
therefore it analyzes the webpage that was an input to the previous cw step s2a1 q ... bugs.html .
it also parses the text of s2a1 q and using the same parsing technique explained above extracts the nouns to create a list of keywords tracker is the only noun in our example .
next it extracts all the link labels in this page s2a1 q ... bugs.html that match the nouns tracker .
note this a step to remove nouns from sometimes wordy action text that do not match the link label.
aid then searches for the fig.
.
aid s approach for capturing inclusivity bugs rule fig.
.
aid s approach for capturing inclusivity bugs rule keywords in the linked page i.e.
s2a1 q ... issues .
if it cannot find that keyword anywhere in the page it reports an inclusivity bug.
figure details the steps in which aid captures inclusivity bugs arising due to a rule violation.
information link navigation oss ers like abi click on a link only after gathering enough information and planning their next step.
labeled links provide abi with information about the webpage they are supposed to visit.
as a team reported for project15 uc1s1a1 it doesn t talk about issue lists and there are a lot of links which don t say where they lead to.
the team felt that abi would not know which link to navigate to since all the links in the page had nondescriptive labels.
other work agrees recommending that links should have descriptive yet unique link labels and begin with keywords .
rule captures this tenet.
rule links should be labeled with a keyword or phrase.
1429aid checks if links on the page it is evaluating are labeled with a keyword or phrase.
to extract the link labels we wrote a custom web scraper using beautifulsoup .
aid then checks the link labels and reports a bug if the url of the link is the same as its label as shown in figure .
cues about issue characteristics our fourth and fifth rules focused on issues in github.
past research has reported that finding a task to work on is especially difficult for some oss ers such as newcomers and mentors .
to address this problem github recommends using issue labels and provides a set of default labels e.g.
bugs documentation good first issue .
figure shows examples of issue labels.
when oss ers like abi come to the issue page they look for cues from the issue labels to gather information about the task e.g.
the type of task which part of the codebase are related to the issue what skills are needed .
when issues are not labeled they can get discouraged.
as pointed out for project7uc1s3a1 neither issue seems to be...very clearly labeled.
so because of her info processing style abi might not be able to gather enough information about the issues listed and give up .
this leads to our fourth rule see figure .
rule issues should have labels.
aid checks for labels on open issues and reports a bug if the issues are unlabeled.
it checks for labels for the first open issues1 which it extracts using the github api.
however simply having labeled issues might not be sufficient.
the cognitive walkthrough at the core of gendermag is about learnability to a first time user i.e.
an oss newcomer in this domain.
thus the humans gendermag sessions reported issues relating to newcomers such as project6 uc1s3a1 the issues are well labeled but there is no sign as to what would be a good one for a first timer like abi she might feel unsure and not choose one.
rule issues should have newcomer friendly labels when appropriate .
rule evaluates the issue label text to check if it is newcomer friendly by string matching against the list of newcomer friendly labels in mungell awesome forbeginners github repository .
this list is curated from projects in programming languages.
we implemented this set of decision rules with some trepidation.
they are text processing rules done statically and derived from gendermag sessions of a relatively small set of projects.
would such rules be able to find the same kinds of inclusivity bugs in arbitrary oss pages that humans bring to gendermag sessions by stepping into the shoes of a persona?
v. aid seffectiveness we ran aid on the projects described above which produced inclusivity bugs.
compared with the inclusivity bugs identified by the humans who had conducted gendermag sessions manually on the same projects and use cases aid s 1github shows issues per page and an oss er newcomer like abi is unlikely to look further if she doesn t see any cues for her task.
fig.
.
aid s approach for capturing inclusivity bugs rule fig.
.
example of an inclusivity bug due to rule violation issue labels are not newcomer friendly precision was .
recall was .
and f measure was .
.
table vi details precision recall and f measures.
a. precision a closer look at false positives aid disagreed with the human evaluators in of the inclusivity bugs it reported.
since false positives play a critical role in developers dissatisfaction with current tools aid s precision a measure of false positive rate could be critical to its acceptance.
in this section we consider which of these extra inclusivity bugs i.e.
those the tool found but the humans did not really were false positives and why.
aid indeed produced false positives for three reasons its use of static analysis instead of dynamic analysis its reliance solely on the html of the webpage and its semantic limitations.
there were of these false positives of the total extra inclusivity bugs .
table vi aid sprecision recall and f measure with human gender mag sessions as the gold standard section iv b .
precision recall f measure rule .
.
.
rule .
.
.
rule .
.
.
rule .
.
.
rule .
.
.
overall .
.
.
1430table vii 61inclusivity bugs where aid was correct and the human teams were incorrect .
category of evidence example internal team activities team mentioned the bug later 27noinformation about contributions on the readme file.
aid reported this bug earlier than a team did but the team found the bug in the next step.
team didn t mention the bug but they fixed it later3unlabelled issues on an issue list.
team didn t report it but we could see from the post fix project version they had fixed it.
team mentioned the bug earlier but not here20tool reported a bug every time it found it but teams did notbother to repeat.
external nielsen s 11unlabelled web link was not in the direct path a team was evaluating.
static analysis analyzing an action like make a change to the readme.md requires human input during the session s walkthrough and that human input affects what gets displayed next during the walkthrough.
because aid uses solely static analysis of the webpage html it cannot take user inputs like the above into account which can generate false positives.
addressing this type of false positive would require some way of gathering user input data either on the spot or from a corpus of user inputs to this question.
there were three instances of this type of false positives.
reliance on text content labels aid does not currently do image analysis.
for example for the action click on pencil icon on the readme page aid looks for keywords pencil and icon .
if the image filename for the pencil icon is something like word pencil or icon e.g.
img src icons pencil.png aid would realize that it relates to the action otherwise it does not.
in contrast the human evaluators easily recognized the icon from its appearance on their screen.
addressing this type of false positive would require adding some form of image processing.
there were false positives of this type.
english semantics aid does not currently address underlying semantics of english language usage.
for example find something to work on would translate to find an issue to work on for many oss ers but the tool does not know this.
some of aid s semantic limitations could be improved though the use of synonym dictionaries or methods for detecting term similarity like latent semantic indexing or tf idf others would require semantic analyses accounting for antecedents referents and or context .
there were false positives from the tool s semantic limitations.
aid was actually right .
the remaining of the extra inclusivity bugs were not false positives.
rather the tool was correct in the three categories evaluator false negatives repetitions of the same inclusivity bug and the tool going beyond the call of duty to find things it didn t need to find.
table vii summarizes them.
we identified the instances where aid was correct in the following manner.
first we looked at every bug aid found that the gendermag teams did not.
when evidence existed that aid was right we associated the evidence with the instances of those bugs as discussed next.
as mahotody et al.
s survey of cognitive walkthroughs shows although humans performing methods in the cwfamily report few false positives or below across numer ous studies their propensity for false negatives has run as high as false negative rates in which the humans simply miss seeing some of the problems.
likewise with gendermag although humans false positive rates are well below human gendermag users tend to miss some inclusivity bugs.
the human evaluators in this investigation were also subject to this phenomenon.
in instances see table vii row2 the tool flagged a bug that the team missed but the team mentioned the bug later in subsequent steps of their gendermag sessions.
in another three inclusivity bugs that aid found the project f team didn t mention the bug but they fixed it later as part of their overall changes see table vii row3 .
this suggests that the project f team eventually realized that it was problematic.
from this we conclude that tool was right in identifying these three bugs.
thus the total of human tool team differences due to humans false negatives was .
there were instances where aid identified bugs that humans did not report because of repetitions.
aid does not realize when it reports the same bug more than once such as if the same bug arises in more than one step of a use case.
in contrast humans do realize it and sometimes do not bother to report the same inclusivity bug if it occurs a second time.
in such cases the team mentioned the bug earlier but not here see table vii row4 .
these repetitions can be potentially annoying but they are not false positives.
finally sometimes aid reported inclusivity bugs that were unlabeled links in locations not directly in the direct path of the use case see table vii row5 .
there would have been no reason for the human teams to find these inclusivity bugs if they strictly followed the use cases.
still that does not mean that such bugs were false positives.
we argue they were true positives that unlabeled links can be inclusivity bugs for comprehensive information processors like abi who want to find all the relevant information.
without a clear label abi s would be unable to predict whether information might lie behind the link.
this may be why the nielsen norman group emphasizes the need for links to use descriptive anchor text that uses keywords to facilitate information processing .
the tool s beyond the call of duty to relax the strict boundaries of the use case sequence resulted in of these inclusivity bugs that the human teams did not find.
b. recall a closer look at false negatives aid s recall rate was extremely high with a total recall of .
.
in rules aid s recall was a perfect .
perhaps because these rules implement straightforward string comparisons.
the tool noticed all instances of rules like these so whenever the humans noticed it aid would too.
the tool s recall gaps all came from rule which produced false negatives.
all of these came from the tool s treatment of subgoals and actions use of verbs.
we decided that when a word was being used as a verb e.g.
click in click on reporting a bug in uc2s1a1 table v the tool would not treat them as keywords to be searched for in the webpage.
1431table viii triangulation of bugs found by teams f j the gender mags we conducted and aid.
o ver of the human reported bugs human fell into f j s original categories .
aid found of the bugs the humans found .
inclusi vity bugs f j 18projects aid notenough information page x readme unclear path to the readmex x readme no info about subgoal x lacks understanding of oss terms can t match to subgoalxx template pull request notenough instructionsxx template filing an issue not enough instructionsx x notenough information to choose optionsx xx notenough information to take actionxxxx issues in issue list not enough information for users to pick a taskx x other total however in the subgoal s1 find information about how to file an issue the verb file describes the process of reporting an issue.
thus although the tool did not consider the word file to be worth looking for here the human teams in their sessions did look for information about the process of filing an issue on the project page project3 uc2s1a1 ...there is nothing that says file a new issue so abi might feel some uncertainty with her action.
this issue accounted for all of aid s false negatives.
c. results triangulation table viii shows the inclusivity bugs reported by all sources in the categories of bugs the humans found triangulated among project f project j the sessions we conducted and aid.
each bug category occupies a row of the table.
the table does not include the bugs that aid found that the humans did not.
aid s low performance on template bugs was due to its static only analysis as discussed earlier.
every category of bug was cross validated by at least three of these four sources and as the table shows aid found of the bugs the humans reported.
vi.
t hreats to validity this section presents the different threats to validity and how we mitigated them.
first two teams of researchers manually identified the inclusivity bugs using the gendermag method so subjectivity of the data can be considered a threat to validity.
to minimize this threat we calculated the inter rater reliability among the two teams and also validated our data against inclusivity bugs found by two external oss teams f j using gendermag.
the second threat to validity of our tool lies in the disagreements between aid s findings and human evaluators findings.
in order to mitigate that we further analyzed these disagreements to understand their origin.
we found different types of disagreements some were linked to the limited scope of our tool when the tool was wrong others where the toolwas more comprehensive than a human evaluator originated from humans not capturing all possible inclusivity issues.
the final threat is the generalizability of aid in its current scope.
aid can be applied to any oss project in github million public repos and other hosting sites gitlab bitbucket .
as a first step aid focused on oss projects hosted on github thus the applicability of our tool might not generalize to other version control platforms or other non oss technology.
however some of aid s rules e.g.
rule1 can be applied on any web app or webpage.
aid also currently automates only a vertical slice of gendermag with the abi persona and their specific information processing style.
this scoping allowed us to investigate aid s feasibility and effectiveness under stricter controls but impacts its generalizability.
most of these threats can be mitigated by future work that addresses the limitations of the tool extending aid to also evaluate for abi s other cognitive facets and to the other gendermag personas pat tim and expanding the scope of software upon which aid runs beyond oss platforms.
vii.
r elated work the nearest neighbours of the gendermag method are the williams recommendations and inclusivemag.
williams pointed out several ways gender inclusivity bugs can arise such as hidden gender bias during the product cycle and women being reluctant to voice their opinions when they are outnumbered in a brainstorming session .
williams also offers concrete recommendations to head off such causal factors such as assembling groups of at least women during brainstorming sessions and having an equal vote distribution by gender when using informal voting systems .
inclusivemag is a meta method that spawns methods like gendermag for under served software users.
it provides a step by step approach for researchers and practitioners to generate methods to evaluate their software.
however neither of these methods have been automated.
gendermag is a member of the cognitive walkthrough family.
mahatody et al.
s comprehensive literature survey of cws describes many cw variations some of which focus on reducing problems with the classic cw such as by reducing the time it requires.
nielsen et al.
recommended that a note taking tool for cws to address issues like these by guiding the analyst through each cw step in order to avoid missing steps and to more accurately record results integrating a cw tool into a prototyping tool .
when cws were first introduced rieman et al.
created a tool to record the results of a human run cw .
the gendermag recorder s assistant is a recent note taking tool to help humans organize gendermag session output their answers to the cw questions additional notes and screenshots pertinent to each question.
these tools are for supporting humans a cw and not the automation of the method itself.
farther afield there is a variety of work relating to automation of usability evaluation.
for example mathur et al.
introduced a usability evaluation framework a model to automate usability evaluation for mobile apps based on 1432prevailing usability guidelines.
baker et al.
created a prototype to automate usability testing for handheld devices .
it measures software usability by recording user actions and compares the user s actions to those expected by the developer.
the webtango prototype predicts user s information seeking behaviour and webpage navigations calculating factors like the time for a user to scan a page based on its complexity.
dingli et al.
proposed a framework and tool that evaluates websites by considering usability guidelines .
however tools like these do not evaluate inclusivity.
various tools exist to help with accessible design.
wa ve is a tool for evaluating accessibility principles.
it indicates if information on a webpage is accessible by visually impaired people by checking for alternative text structural markup and reading order.
additionally it flags audio content for the deaf population.
vischeck is a tool for low vision simulation which uses image processing techniques to create views for low vision users.
aatt provides an accessibility api to test web applications for conformance to the web content accessibility guidelines wcag .
even though these tools assist in the implementation for more inclusive products they do not consider cognitive diversity.
closest to our work is the autocww a tool that automates the cw to identify website navigation problems .
it takes a goal from the user along with a list of webpages the user is supposed to navigate.
it then computes the similarity of the goal text with the headings and link labels of the input pages to see if a user who wanted to find the right link could navigate to the correct webpage.
autocww only evaluates webpages with respect to the overall use case while aid has a more fine grained task oriented approach.
autocww assumes a generic user and not inclusivity of diverse users.
for example it does not take into account different styles of processing information such as abi s comprehensive information processing style.
github oss ers with this style are unlikely to just click on potentially suitable links until after gathering enough information to understand what is available and how they might like to proceed through it.
in contrast aid checks if a user like abi and in future versions like tim or like pat would gather the right amount of information to navigate in the needed direction for their current actions.
viii.
c oncluding remarks the effectiveness that aid showed at automatically detecting these oss project sites inclusivity bugs is promising.
as we have pointed out the five rules at the core of our model used static text processing techniques.
despite this fact even under the conservative assumption that humans were the gold standard aid achieved a precision rate of .
.
further closer scrutiny revealed that more than half of its false positives under this assumption were not actually false positives table vi .
further its recall rate of .
is better than most recall rates by human users of the cw family .
how could the tool have done so well?
researchers have reported the heavy workloads some humans experience using gendermag .
part of their workload is trying to become someone else here one of the gendermag personas which some humans have difficulty e.g.
.
yet the tool did remarkably well with static text analysis without even attempting to model the persona.
we hypothesize that the reason for aid s success lies in the concrete pattern based approach we used to develop the rules.
when humans use gendermag they work from the root of inclusivity bugs problem solving style diversity.
in contrast in developing the rules we worked from patterns of symptoms of the problems the human teams found and encoding them in rules.
our results suggest that the concrete pattern based approach we used is promising.
a goal of this work was to find out whether it was possible for a software tool to automatically detect inclusivity bugs.
as our results with aid show such a tool is both possible and feasible to implement.
despite its concrete rules aid is rooted in theory inherited from the foundations of gendermag.
as shaw and others have argued scientific theory lets technological development pass limits previously imposed by relying on intuition and experience .
for example shaw points out for centuries many architectural structures buildings bridges tunnels canals could be built only by master craftsmen.
not until scientists developed theories of statics and strength of materials were today s extraordinary engineering accomplishments possible such as the hong kong zhuhai macau bridge spanning miles of ocean by ordinary engineers.
in computer science we see the same phenomenon.
for example expert developers once built compilers using only hard won intuitions gained from extensive experience but formal language theory has brought tasks like parser and compiler writing to a level where undergraduate computer science students now routinely build them in their coursework .
aid follows this path through its theory foundations as a step toward enabling ordinary developers to find inclusivity bugs without needing to become experts in gendermag.
our survey s results suggest that tools like this are needed of our respondents lacked any specific tools or approaches they could use to check for inclusivity bugs.
p115 i wasn t aware of methods but i try to get feedback from and build for a diverse set of users.
p156 i can make mistakes ... i strive to not do evil.
ix.
a cknowledgements we thank our survey participants and teams f j. this work is partially supported by the national science foundation under grant numbers and .