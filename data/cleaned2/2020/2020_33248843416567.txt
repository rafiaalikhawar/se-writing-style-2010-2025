seven reasons why an in depth study of the limitations of random test input generation for android farnaz behrang georgia tech atlanta ga usa behrang gatech.edualessandro orso georgia tech atlanta ga usa orso cc.gatech.edu abstract experience paper testing of mobile apps is time consuming and requires a great deal of manual effort.
for this reason industry and academic researchers have proposed a number of test input generation techniques for automating app testing.
although useful these techniques have weaknesses and limitations that often prevent them from achieving high coverage.
we believe that one of the reasons for these limitations is that tool developers tend to focus mainly on improving the strategy the techniques employ to explore app behavior whereas limited effort has been put into investigating other ways to improve the performance of these techniques.
to address this problem and get a better understanding of the limitations of input generation techniques for mobile apps we conducted an in depth study of the limitations of monkey arguably the most widely used tool for automated testing of android apps.
specifically in our study we manually analyzed monkey s performance on a benchmark of apps to identify the common limitations that prevent the tool from achieving better coverage results.
we then assessed the coverage improvement that monkey could achieve if these limitations were eliminated.
in our analysis of the results we also discuss whether other existing test input generation tools suffer from these common limitations and provide insights on how they could address them.
ccs concepts software and its engineering software testing and debugging.
keywords android ui testing test generation empirical study acm reference format farnaz behrang and alessandro orso.
.
seven reasons why an indepth study of the limitations of random test input generation for android.
in 35th ieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
ase september virtual event australia copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
introduction the use of mobile apps has become increasingly widespread.
due to the fierce competition in the mobile app market app quality is a primary concern and an important factor for the success of an app.
like for most software testing is one of the crucial phases of the app development process as it helps developers improve the quality of their app by revealing bugs before the apps are delivered to end users.
because users typically access the features provided by a mobile app through its graphical user interface ui app testing is also usually performed mainly through the ui of the app.
unfortunately testing ui software is time consuming and involves a great deal of manual effort.
for this reason researchers and practitioners alike have proposed a number of techniques and tools for automated test input generation for mobile apps e.g.
.
although effective these techniques and tools are still limited in terms of the coverage they can achieve on the app under test.
after studying these techniques we believe that one overarching and common issue with them is that they tend to focus on improving their exploration strategy and overlook other important aspects of input generation.
in fact despite the large body of research on test input generation for mobile apps little effort has been invested into a broader exploration of the limitations of and improvement opportunities for these techniques.
we believe that a better understanding of these limitations can be extremely beneficial and provide meaningful insight on how to improve input generation for mobile apps.
in this spirit and as a first step in this direction we performed an in depth study of the performance of monkey a tool that automatically tests apps by generating pseudo random streams of events.
we chose monkey as a representative technique because it is arguably the most widely used input generation tool for android apps and because previous studies have shown that it can outperform most other existing input generation tools in both open source and industrial settings.
it is worth noting that in related work zeng and colleagues conducted an empirical study in which they studied monkey s limitations by applying it to wechat a popular messenger app.
their study is limited however by the fact that it focuses on a single app.
in this paper conversely we present the results of an extensive in depth study of monkey s limitations.
the goal of our study is twofold.
first we wanted to identify the inherent limitations of monkey that prevent it from achieving better coverage results.
in particular our study focused on identifying limitations that are common across many apps andgo beyond limitations in the exploration strategies.
second we wanted to assess the coverage improvements that monkey could achieve by eliminating these common limitations.
note that we focused on 35th ieee acm international conference on automated software engineering ase ase september virtual event australia farnaz behrang and alessandro orso coverage alone because coverage is most commonly used as a proxy for the effectiveness of input generation techniques and we wanted to have a more focused analysis and room to present our results in greater detail.
however we plan to investigate other metrics such as fault detection ability and possibly other kinds of coverage in future studies.
in our study we manually analyzed monkey s performance on a benchmark of apps from f droid a widely used free and open source android app repository.
we initially considered apps but of those no longer worked in modern versions of android so we discarded them.
we considered these apps because they have been used extensively in previous research work and have almost become a de facto standard benchmark for evaluating the effectiveness of android test input generation tools.
moreover our analysis involved a considerable amount of manual inspection of the source code of the apps considered and most industrial apps in the google play store are closed source and obfuscated.
it is also worth noting that the set of apps we studied contains some widely used large apps such as wordpress wikipedia k mail and myexpenses which are available in the google play store and have been installed million of times.
whenever possible we used the most recent version of the apps as considering a newer version of an app allows us to get results that reflect recent features and enhancements in the android operating system.
by applying monkey to the apps considered we identified categories of limitations that prevent monkey from achieving better coverage results on of these apps.
the identified categories areexternal apps ui actions domain knowledge content providers input files system events and inputs which prevent monkey from achieving higher coverage for and of the apps respectively.
besides these general categories we also found other limitations specific to individual apps.
we consider these specific limitations less interesting as addressing them is unlikely to provide general improvements.
after identifying and categorizing the above limitations we wanted to investigate whether addressing them would actually benefit input generation.
to do so we semi automatically eliminated these limitations by modifying the apps as explained in section and assessed whether this improved the performance of monkey on the apps affected by the limitations.
so resulted in coverage improvements for of the apps ranging from to .
more precisely for these apps the average median and standard deviation of the coverage improvements were and respectively.
for the remaining two apps we were unable to assess the potential improvements as it was not possible to modify them so as to eliminate the identified limitations.
in summary our main findings are the following inter app communication is common especially across newly developed apps and can considerably limit the effectiveness of input generation tools to explore the apps extensively an input generation tool should go beyond performing simple random clicks should be able to infer the most relevant ui actions and should support performing other actions such as long clicks drag and drop tap and hold touch swipe multi touch and even voice commands domain knowledge is at times necessary to explore some behaviors of the apps under test so input generation techniques must find ways to incorporate such domain knowledge populating contentproviders or generating smart mocks can also be necessary to test some behaviors of the apps input files often have a huge impact on the level of coverage that can be achieved on some apps e.g.
music or video player injecting system events in addition to ui events can considerably improve the effectiveness of input generation tools and specific inputs are at times required to explore particular behaviors of some apps which implies that input generation tools should be able to generate context aware inputs in addition to random ones.
overall we believe that our results support our intuition that there are important limitations for input generation techniques that are orthogonal to the exploration strategies used.
developers of these kinds of techniques should be aware of these limitations and take them into account to improve the effectiveness of their techniques and tools.
in our analysis of the results we also provide insight on whether other existing test input generation techniques and tools consider these common limitations and if they do not how they could address them.
this paper makes the following contributions the first extensive in depth study of the limitations of a representative and commonly used input generation tool for mobile apps.
our results are available at .
the identification of a set of limitations that are common across a majority of the apps considered.
empirical evidence that addressing these limitations can result in better coverage results.
a detailed analysis of our empirical results that provides insights on whether other automated test input generation tools address the limitations we identified and if not how they could do so.
background in this section we present an overview of the android platform android components and android test input generation tools.
.
android platform android apps are mainly written using the kotlin and java languages but there are some platform libraries that allow developers to use c and c to achieve low latency or run computationally intensive apps such as games or physics simulations.
the android sdk tools compile source code along with any data and resource files into an android package kit apk for short .
an apk file contains all the contents of an android app and is used for the distribution and installation of the app.
at runtime java classes are converted into dex bytecode which is then translated to native machine code via art or dalvik two alternative runtimes.
art was introduced in android .
kitkat and has completely replaced dalvik in android .
lollipop .
android .
added to art a jit just in time compiler with code profiling capabilities to improve the performance of android apps as they run.
.
android components components are the essential building blocks of an android app and can be of one of four types activities services broadcast receivers andcontent providers.
1067seven reasons why an in depth study of the limitations of random test input generation for android ase september virtual event australia activities represent the user facing screens of an app.
using activities developers can place and organize ui components.
each activity provides callback methods that allow it to control the user interactions e.g.
clicks with the screens.
services allow apps to perform long running background tasks or to perform work for remote processes without a user interface.
broadcast receivers allow registering for system or app events.
all registered receivers for an event are notified by the android runtime once the event happens.
for instance apps can register for the system event that is fired once the device starts charging.
content providers act as a central repository for storing app data and allowing other apps to access them.
for instance the android system manages contacts through a content provider that can be queried by any app with the proper permissions.
.
android test input generation tools there is a large body of research on test input generation for android apps and researchers have proposed a number of test input generation tools for automating android ui testing e.g.
.
the goal of these tools is to explore as much behavior of apps as possible and reveal faulty behavior.
android apps are event driven programs so their inputs are typically in the form of ui events e.g.
clicks swipe and text inputs or system events e.g.
receiving calls and text messages .
test input generation tools tend to generate such inputs using different exploration strategies namely random systematic and model based.
typically random strategies generate purely random ui and system events.
conversely systematic strategies use more sophisticated techniques e.g.
symbolic execution and evolutionary algorithms to guide the exploration towards previously uncovered code.
finally model based strategies rely on a ui model of the app often consisting of a finite state machine constructed statically or dynamically to guide the exploration of the app and generate events.
empirical study to study the limitations that prevent android test input generation tools from achieving higher coverage we performed an in depth analysis of monkey .
we chose monkey because it is a widely used tool for testing android apps partly because it is part of the android developers toolkit which makes it is easy to use and compatible with different android platforms and application settings .
in addition studies conducted in both open source and industrial settings confirm that monkey performs well when compared with other existing test input generation tools .
we used code coverage as our metric as it is commonly used by similar studies as a proxy for the effectiveness of input generation techniques in future work we plan to investigate additional metrics such as fault detection ability.
our empirical study aims to answer three research questions rq1 what are the general limitations common across apps that prevent monkey from achieving higher coverage?
rq2 how much increase in coverage can monkey achieve by eliminating these common limitations?
rq3 how can other automated test input generation tools address these common limitations?
.
mobile app benchmarks to answer our research questions we chose a benchmark of apps from f droid a popular open source android app repository.
these apps were initially collected by choudhary and colleagues to compare different android test input generation tools.
we chose these apps because they have been widely used in previous research e.g.
and have almost become a de facto standard benchmark for evaluating the effectiveness of the android test input generation tools.
moreover our analysis involved a considerable amount of manual inspection of the source code of the apps considered and most industrial apps in the google play store are closed source and obfuscated.
whenever possible we used the most recent version of the apps as considering a newer version of an app allows us to get results that reflect recent features and enhancements in the android operating system os .
among the apps that we studied we were able to find a newer version for apps .
table lists the apps that we considered in our study.
for each app the first four columns of the table show its name category version used in previous work and new version used in our study.
the cases for which a newer version of the app was not available are indicated with a dash .
.
experiment setup we conducted our study on a bit ubuntu .
physical machine with cores .40ghz intel r cpu and 32gb ram.
to run the apps we used android a set of x86 emulators where each emulator was configured with cpu cores gb of ram gb of sd card and the kitkat version of the android os api level .
if the app under study required a feature not available on the emulators it was tested on a samsung galaxy s6 device running android .
api level .
.
rq1 to answer rq1 we first ran monkey for one hour on each benchmark app and collected line coverage using jacoco java code coverage library .
we chose one hour because previous work found that test input generation tools for android apps typically achieve their maximum coverage within this time limit.
because many testing tools and apps are non deterministic we repeated each experiment times and reported the maximum value across all runs.
note that we chose the maximum value rather than the mean value because our goal is not to compare monkey with other existing tools but rather to identify the limitations that prevent monkey from achieving higher coverage.
the resulting coverage information is shown in column coverage in table .
once we collected coverage information for all benchmark apps we manually analyzed each app along with its source code to assess why monkey was unable to achieve higher coverage on that app.
column in table lists the common monkey limitations that we identified in our analysis.
among the apps that we analyzed no longer worked in newer versions of the android os so we did not further consider them in the study photostream syncmypix aagtl and mirrored .
for the remaining apps we were able to identify categories of common limitations across apps that prevented monkey from 1068ase september virtual event australia farnaz behrang and alessandro orso table list of the apps in our benchmark and of the limitations we identified that prevented monkey to achieve higher coverage on these apps.
a in the new version column indicates the apps for which a newer version was not available.
in the limitations column indicates apps for which either no limitations were identified or the limitations were app specific.
a in the same column indicates apps that no longer worked in modern versions of the android os and were therefore discarded.
name categor y old ne w co verage limitations v ersion v ersion a2dp volume t ransport .
.
.
.
.
.
system events bluetooth aagtl t ools .
.
.
a arddictionary refer ence .
.
.
.
input files dictionary acal pr oductivity .
.
inputs configure server a ddi t ools .
.
domain knowledge matlab commands adsdr oid refer ence .
.
.
.
agr ep t ools .
.
.
input files text file ui actions long click alarm clock pr oductivity .
.
.
ui actions long click alogcat t ools .
.
.
amaze d casual .
.
.
system events screen rotation andr oidomatick.
communication .
any cut pr oductivity .
.
content providers contacts anymemo education .
.
.
.
.
external apps google drive dropbox quizlet a uto answer t ools .
.
system events phone call bluetooth baterr ydog t ools .
.
.
batter y circle t ools .
.
bites lifestyle .
.
system events sms external apps trolly ui actions long click blokish puzzle .
.
domain knowledge game ui actions swipe drag and drop bomb er casual .
domain knowledge game bo okcatalogue t ools .
.
.
.
external apps librarything amazon goodreads zxing pic2shop sharing countdo wntimer t ools .
.
.
domain knowledge letting alarm go off dalvikexplor er t ools .
.
dialer2 pr oductivity .
.
content providers contacts divide conquer casual .
.
domain knowledge game ftp server t ools .
.
.
.
external apps ftp client fileexplor er pr oductivity .
fr ozenbubble puzzle .
.
domain knowledge game external apps google play store gestur es sample .
ui actions touch hndr oid ne ws .
.
.
hotdeath car d .
.
.
domain knowledge game imp ortcontacts t ools .
.
content providers contacts input files vcard file jamendo music .
.
.
external apps sharing ui actions long clicks k 9mail communication .
.
.
content providers email accounts email contacts inputs login k eepassdroid t ools .
.
.
.
.
ui actions long click learnmusicnotes puzzle .
.
domain knowledge game lo ckpatterngen t ools .
lolcatbuilder entertainment .
input files images external apps sharing ui actions drag and drop manpages t ools .
.
mileage finance .
.
.
ui actions long click mininote viewer pr oductivity .
.
mirr ored ne ws .
.
.
.
.
multi sms communication .
.
.
munchlife entertainment .
.
.
myexp enses finance .
.
.
.
.
external apps sharing calendar dropbox mylo ck t ools .
system events phone call ui actions swipe ne ctroid me dia .
.
.
netcounter t ools .
.
.
passw ordmakerpro t ools .
.
.
.
.
p hotostream me dia .
.
quicksettings t ools .
.
.
.
ui actions tap and hold randommusicp lay music .
ringdr oid me dia .
.
.
.
content providers contacts sanity communication .
.
external apps email google play store system events sms phone call bluetooth airplane mode soundboar d sample .
spritemetho dtest sample .
sprite text sample .
syncmypix me dia .
.
tipp ytipper finance .
.
.
t omdroidnotes so cial .0a .
input files tomboy notes t ranslate pr oductivity .
.
external apps email t riangle sample .
w eightchart health .
.
.
external apps email google play store ui actions long click whohasmystuff pr oductivity .
.
.
.
.
content providers contacts external apps calendar ui actions long click wikip edia refer ence .
.
.
.
.
inputs login external apps sharing w ordpress pr oductivity .
.
.
.
inputs login input files images videos external apps social media w orld clock t ools .
.
ui actions long click yahtzee casual .
domainknowledge game zo oborns entertainment .
.
.
.
.
1069seven reasons why an in depth study of the limitations of random test input generation for android ase september virtual event australia achieving higher coverage.
these categories are external apps ui actions domain knowledge content providers input files system events and inputs which affected monkey s performance on and of the apps respectively.
in addition to the categories that we identified we also found other limitations that were more specific to individual apps.
consider for instance tippy tipper which is a tip calculator app that allows for interacting with some ui elements using keyboard keys.
for this app using keyboard keys is required to cover much functionality of the app.
as another example consider munchlife an app for keeping track of character levels while playing a particular card game.
to cover different scenarios and thus achieve higher coverage for the app monkey needs to set the max level in the app preferences to different values.
because these kinds of limitations are very specific to the apps they affect and we were looking for common issues that could benefit input generation tools in a more general way we did not further consider them in our study.
similarly we did not focus on other standard language issues that may affect coverage such as dead code and special cases e.g.
exceptions and catch blocks .
the sixth column in table shows which apps were affected by which of the seven categories of limitations we identified.
we marked the four apps we had to discard with a star whereas a dash indicates those apps whose coverage was not affected by any of these seven categories.
in the rest of this section we discuss the seven identified categories in detail.
.
.
external apps.
communicating with external apps is needed to explore some functionality of apps that we analyzed.
among these apps needed access to at least a sharing app e.g.
messaging apps needed access to the google play store and explicitly needed access to an email app.
some of the apps also needed to communicate with an ftp client app google drive dropbox quizlet librarything amazon zxing or pic2shop barcode scanner apps goodreads or calendar.
specifically the set of apps that need access to sharing apps includes lolcatbuilder an app for sharing photos the user modifies jamendo an app for sharing music wikipedia an app for sharing wikipedia articles myexpenses an app for managing expenses and income including sharing a pdf of transactions via email or dropbox and spreading the word about the app through email and bookcatalogue an app for sharing books and sending information about the app to others through a sharing app.
frozenbubble sanity and weightchart are the apps that need access to the google play store to install other relevant apps or leave comments.
the apps that explicitly need access to an email app are translate sanity and weightchart.
translate is a translation app that uses the google translation service.
the app allows sharing the translations by email.
sanity provides the option to ask questions to the app developer by email.
weightchart is an app for keeping a personal log of body weight and displaying it as a chart.
also in this case the app allows for contacting the app developer.
apps that need to communicate with other external apps are ftp server bites anymemo bookcatalogue wordpress and whohasmystuff.
ftp server is an app that can read write and backup any folders in an android device.
this app needs to communicatewith an ftp client in order for much of its functionality to be explored.
bites a cookbook app relies on trolly a shopping list app for adding recipe ingredients to a shopping list.
anymemo is a flashcard learning app that allows users to download and upload flashcards from to google drive dropbox and quizlet and needs the corresponding apps to do so.
users can synchronize the bookcatalogue app with their goodreads account and also search for data related to books on amazon goodreads and librarything a social cataloging app for storing and sharing book catalogs and various types of book metadata .
the app also supports using two barcode scanner apps zxing and pic2shop to scan books and automatically add them.
the wordpress app can connect to social media services i.e.
facebook linkedin tumblr twitter to automatically share new posts.
the whohasmystuff app also needs access to a calendar app to add events for expected returns of lent items.
.
.
ui actions.
among the apps we analyzed we found apps for which using ui actions not supported by monkey was necessary to achieve higher coverage.
specifically besides clicks tests would need to perform long clicks as well as drag and drop tap and hold touch and swipe actions.
in the lolcatbuilder app while modifying photos captions can be dragged and dropped.
quicksettings is an app that provides quick access to various android system settings such as wifi gps brightness and volume controls.
in this app it is necessary to tap and hold a setting icon to move it between visible and hidden lists or to reorder it.
in the bites app long clicks are needed to modify recipes ingredients and methods.
in the alarm clock app long clicks are needed to change the time and modify alarm notifications.
in world clock an app that shows time in different time zones at once long clicks are needed to modify the time zones to be shown.
in the jamendo app it is necessary to perform long clicks to modify playlists.
gestures a simple app for demonstrating and practicing gestures requires touch events.
in the mylock app swipe actions are required to cover the lock and unlock functionality.
in the agrep app modifying directories requires long clicks.
for the blokish app both swipe and drag and drop actions are required to play the game.
keepassdroid a password manager app for the android platform requires long clicks to modify entries in different kinds of lists.
mileage an app for tracking vehicle mileage requires long clicks to cover functionality related to vehicles and history.
the weightchart app requires long clicks to modify weight entries.
finally the whohasmystuff app also requires long clicks to modify information about lent items.
in general a rich set of ui actions is needed to cover part of the functionality in the apps mentioned above.
the degree to which this issue affects code coverage is different across apps as we discuss in more detail in section .
.
.
.
domain knowledge.
specific domain knowledge is required to achieve higher coverage on nine apps .
among these apps seven are games that require an understanding of the game rules and dynamics divide conquer a game where balls are bouncing and must be constrained into increasingly small areas yahtzee a dice game hotdeath a card game bomber an arcade game blokish a board game frozenbubble a bubble shooting game and learnmusicnotes a simple game for music reading training.
1070ase september virtual event australia farnaz behrang and alessandro orso the other two apps that require domain knowledge to be suitably tested are addi and countdowntimer.
addi creates a mathematical computing environment similar to the one offered by matlab and octave and requires inputs with a specific syntax to exercise any actual functionality.
countdowntimer is an alarm app that requires setting alarms and letting them go off to cover a considerable part of the functionality of the app.
.
.
content providers.
content providers are necessary to cover the behavior of six of the apps considered .
five of these apps require access to phone contacts which are provided by the contacts provider a component that is part of the standard contacts app and that makes data about contacts available to other apps.
these apps are the following anycut an app for creating shortcuts for contacts dialer2 an app for making and managing phone calls importcontacts an app for importing contacts into a device ringdroid an app for associating recordings or specific ringtones to different contacts and whohasmystuff an app that helps to keep track of lent items and leverage contact information gathered from a user s address book.
the sixth app in this group is k 9mail an email client.
to use the various features of the app one needs a valid email account with non empty mailboxes and contacts.
a content provider within another email app e.g.
gmail can be leveraged to get access to this information.
.
.
input files.
input files are needed to achieve higher coverage for six of the apps considered .
the required input files consist of images dictionaries text files vcard files tomboy notes and videos as described below.
lolcatbuilder is an app for adding captions to saving and sharing photos.
it is not possible to exercise the features of this app unless the app has access to image files.
aarddictionary is a dictionary and offline wikipedia reader whose functionality can be explored only if the app has access to one or more dictionaries in a specific format slob which are not provided with the app.
agrep is a text search app similar to the unix utility grep .
to explore the part of the app functionality that searches for patterns within files agrep needs access to text files.
importcontacts the app that we also mentioned in section .
.
needs access to vcard files from which to import contacts.
tomdroidnotes is a tomboy a desktop note taking app client for android.
the app does not create tomboy notes but rather allows for modifying existing notes.
therefore in order to explore the functionality of the app tomboy notes must be present.
wordpress is a website builder and a blog maker app.
some parts of the app functionality related to the use of various media needs input files such as images and videos to be explored.
for these apps not having the required input files can seriously limit the effectiveness of test input generation tools.
moreover in most cases these input files must have a specific format e.g.
vcard tomboy and or specific content e.g.
a string matching a pattern to trigger relevant behavior in the app.
.
.
system events.
for six of the apps considered generating system events is necessary to achieve higher coverage.
although monkey can generate pseudo random streams of system events it only generates limited types of these events e.g.
volume controlevents .
the system events required to suitably exercise these apps are events related to screen rotation bluetooth pairing sending and receiving smss and making and receiving phone calls.
specifically amazed is a game in which screen rotations trigger specific behaviors.
a2dp volume is an app for managing the volume of an audio stream when the device on which the app is running is connected to a bluetooth device and restoring the volume on reconnect.
bites a cookbook app we already described allows users to create recipes and share them via sms.
autoanswer is an app that allows for answering the phone automatically when it rings.
mylock is an app that enables quick unlock in call touchscreen lock and incoming call prompts.
sanity is a phone assistant app with features such as caller announcement and call blocking.
.
.
inputs.
for four of the apps considered specific inputs must be provided to the apps to exercise parts when not most of their functionality.
acal is an app for accessing a calendar on a caldav server and managing tasks and events.
much of the functionality of this app can only be explored if the app has access to a calendar on a caldav server which requires the user to have access to the server and to provide the app with a username the corresponding password and the url of the server.
both the wikipedia and the wordpress apps require users to log into the apps.
in the wikipedia app logging into the app is only needed for accessing some functionality while in the wordpress app it is required for exploring a large portion of the functionality.
finally in the k 9mail app the user can also directly provide login information for an email server instead of accessing email accounts from other email apps through content providers.
summary of rq1.
what are the general limitations common across apps that prevent monkey from achieving higher coverage?
among the apps that we analyzed we were able to identify categories of common limitations that affect the ability of the inputgeneration tool to achieve high coverage.
these limitations affect monkey s performance on of the apps overall and between and of the apps individually.
the identified categories are listed in table along with more details about the apps involved and their versions.
.
rq2 to answer rq2 we attempted to eliminate the common limitations we identified earlier to the extent possible and then collected the coverage after eliminating these common limitations.
for apps that required interactions with external apps we set up those apps before running monkey.
for apps that required specific ui actions to explore part of their functionality we guided monkey to perform those actions on suitable ui elements.
if an app required domain knowledge to cover some specific functionality we provided the app with suitable human created inputs.
specifically for the game apps we helped monkey to reach subsequent levels in the games we provided the addi app which reads commands in matlab format with commands provided as examples in the source code of the app and we simply let alarms go off in the countdowntimer app.
for those apps requiring content providers we suitably populated the required content providers before starting 1071seven reasons why an in depth study of the limitations of random test input generation for android ase september virtual event australia monkey.
for those apps that needed access to particular input files we made different input files available on the device before starting monkey for the apps that required system events to perform part of their functionality we injected the relevant events while monkey was testing the app.
finally for the apps that needed specific inputs we either provided login information or suitably configured servers providing data to the app before running monkey.
figure shows the initial and the improved coverage achieved bymonkey after addressing the common limitations listed in table in the way we just described.
as the figure shows coverage increased for of the apps affected by these limitations with improvements ranging from to .
for these apps the average median and standard deviation of the coverage improvement are and respectively.
for the other two apps anymemo and translate we were unable to achieve improvements despite the fact that we addressed the common limitation relevant for these apps i.e.
accessing external apps .
specifically the open source version of anymemo was unable to connect to the provided external apps whereas translate crashed when trying to open an external email app.
we examined in more detail the apps for which the changes we made in how monkey is run led to considerable coverage improvements.
monkey achieved over increase in coverage for lolcatbuilder after making images available on the device allowing the app to connect to a sharing app and supporting drag anddrop actions.
for two apps k 9mail and learnmusicnotes the coverage increased between and by simply providing login information and domain knowledge.
monkey s coverage increased between and for three other apps dialer2 importcontacts and wordpress by populating content providers providing vcard images and video files along with setting up social media apps and providing login information.
for seven of the apps aarddictionary ftp server bites auto answer yahtzee gestures and keepassdroid coverage increased between and by providing dictionary files injecting sms phone call and bluetooth related system events providing access to an ftp client and the google play store supporting long clicks and touch events and injecting domain knowledge.
it is worth noting that although the bites app needs access to a shopping list app called trolly to keep track of recipe ingredients this app does not seem to exist any longer so we were not able to cover this functionality in the app.
anycut agrep bookcatalogue mileage and sanity are among the apps for which the coverage increased between and .
for these apps the coverage increase is due to the injection of sms phone call and bluetooth related system events enabling disabling of airplane mode populating contacts providing text input files supporting long clicks and setting up the google play store librarything amazon goodreads zxing pic2shop sharing and email apps.
for ten of the apps divide conquer a2dp volume jamendo mylock bomber wikipedia countdowntimer ringdroid tomdroidnotes and weightchart coverage improved between and .
in this case what helped monkey s input generation was the use of domain knowledge injecting bluetooth and phone call related system events supporting long click and swipe actions providing login information and tomboy notefiles populating contacts and setting up the google play store email and other sharing apps.
finally amazed quicksettings alarm clock world clock hotdeath frozenbubble myexpenses and whohasmystuff witnessed coverage increases between and when injecting screenrotation system events adding long click actions using domain knowledge populating contacts and setting up the google play store dropbox calendar and other sharing apps.
summary of rq2.
how much increase in coverage can monkey achieve by eliminating these common limitations?
by eliminating the common limitations we identified in our analysis which are relevant for of the apps considered we were able to increase the coverage achieved by monkey for of these apps.
the coverage improvements range from to with average median and standard deviation of and respectively.
these results clearly indicate that input generation tools can achieve considerable improvements in terms of coverage achieved and justify the investigation of techniques that can automatically address the identified limitations.
.
rq3 automated test input generation techniques and tools for android apps are often focused on improving the ui exploration strategy.
for this research question we aim to identify other possible improvements that are worth pursuing based on our results for rq1 and rq2.
in the rest of this section we discuss whether and how can automated input generation tools other than monkey address the common limitations we identified in section .
.
.
.
external apps.
as shown in our study apps rely extensively on other apps e.g.
sharing apps to implement their functionality.
furthermore some of the apps we considered in our study have been developed a while ago we believe that inter app communications are becoming even more common among newly developed apps.
android apps interact with one another through intents.
whereas existing testing frameworks allow for manually testing this type of interactions e.g.
by supporting the specification of mocks to the best of our knowledge they provide no automated support for generating tests containing inter app interactions.
to be able to test interactions between apps existing tools would have to be extended with the ability to automatically create and consume intents coming from and going to the app under test.
.
.
ui actions.
when interacting with ui elements it is important to identify which actions each ui element can receive and process.
as our study shows random test input generation tools such as monkey might not necessarily perform the ui actions that are required to interact with a particular ui element.
other more sophisticated test input generation tools use both static and dynamic analysis techniques to infer relevant ui actions in a given state or on a given screen.
dynamic analysis techniques typically infer actions by examining the ui hierarchy whereas static analysis techniques tend to rely on the analysis of the event listeners in the source code of the app.
recently approaches based on machine learning e.g.
qlearning based exploration qbe and reinforcement learning have been shown to be effective in identifying relevant ui actions.
1072ase september virtual event australia farnaz behrang and alessandro orso .
.
initial improved coverageamazed .
.
initial improved anycut .
.
initial improved divide conquer .
.
initial improved lolcatbuilder .
.
initial improved quicksettings .
.
initial improved a2dp volume .
.
initial improved coverageaarddictionary .
.
initial improved ftp server .
.
initial improved bites .
.
initial improved addi .
.
initial improved alarm clock .
.
initial improved auto answer .
.
initial improved coverageworld clock .
.
initial improved acal .
.
initial improved jamendo .
.
initial improved yahtzee .
.
initial improved dialer2 .
.
initial improved gestures .
.
initial improved coveragehotdeath .
.
initial improved mylock .
.
initial improved agrep .
.
initial improved k 9mail .
.
initial improved bomber .
.
initial improved coveragefrozen bubble .
.
initial improved anymemo .
.
initial improved blokish .
initial improved importcontacts .
.
initial improved wikipedia .
.
initial improved keepassdroid .
.
initial improved coveragecountdowntimer .
.
initial improved ringdroid .
.
initial improved bookcatalogue .
.
initial improved translate .
.
initial improved tomdroidnotes .
.
initial improved wordpress .
.
initial improved coveragemileage .
.
initial improved sanity .
.
initial improved myexpenses .
.
initial improved learnmusicnotes .
.
initial improved weightchart .
.
initial improved whohasmystuff figur e bar charts showing for the apps affected by the common limitations listed in table the coverage achieved by vanilla monkey and the improved coverage achieved by monkey after we manually addressed these limitations.
in general test input generation tools must go beyond randomly triggering actions to improve their effectiveness.
furthermore in addition to basic actions such as click scroll and swipe test input generation tools should also support multimodal actions such as multi touch actions and voice commands some apps for instance require users to draw a particular pattern on the screen to activate some functionality.
.
.
domain knowledge.
some apps have behaviors that are extremely unlikely to be explored randomly and testing those requires some form of domain knowledge.
in this context one particular class of apps are games.
for testing game apps also test input generation techniques other than random such as search based coverage guided and symbolic execution based techniques are also not particularly effective and fail to explore deep behavior in 1073seven reasons why an in depth study of the limitations of random test input generation for android ase september virtual event australia the apps.
recently there have been promising results in the use of deep reinforcement learning approaches for game testing .
however these results are still preliminary and more research in this direction is needed.
for apps other than games domain knowledge related to the app or even its category could be leveraged to improve the effectiveness of testing tools.
for instance the source code of addi which requires matlab commands as inputs contains examples of such commands as comments that can be used to test the app.
for another example in the case of ringdroid allowing alarms to go off is not specific to this app and is true for all the apps that deal with alarms or timers.
in fact some recent work including our own have started to explore ways to leverage the human knowledge encoded into existing tests or provided by the crowd to improve ui testing e.g.
.
however also in this case existing research is still preliminary and more work needs to be done in this area.
.
.
content providers.
our study shows that it is necessary to have suitably populated content providers in order to cover specific behaviors of some apps.
similar to the case of external apps mock objects can be used to simulate the presence of content providers and test the apps that rely on them.
in order for test input generation tools to effectively test these apps therefore they should be able to automatically mock relevant content providers e.g.
to simulate contacts or gallery images on a device or an emulator without actually adding them .
to the best of our knowledge however none of the existing test input generation tools for android support automatically mocking content providers.
.
.
input files.
it is very common for some app categories such as music or video players to require the presence of input files in order to be suitably tested.
although it would be theoretically possible to store a standard set of files e.g.
jpg mp3 and pdf files on a device so that an app under test could access them if needed some apps require very specific types of files.
aarddictionary and tomdroidnotes for instance need access to particular dictionary files and tomboy note files respectively.
it would not be possible to know which files these apps need without consulting their documentation.
one possible way to address this issue would be to provide a way for developers to specify a minimum testing environment for their apps.
alternatively test input generation tools could be extended to automatically generate mocks that can intercept calls to the filesystem but this type of solution is unlikely to work in the presence of files with structured and complex content.
.
.
system events.
for some apps such as those mainly used for communication system events drive a large fraction of the app behavior.
for some other apps system events play a less central yet still important role.
only a few existing test input generation tools for android have the ability to inject system events in addition to ui events e.g.
.
these tools analyze the apps to find system events the apps handle and generate specific broadcast intents to simulate these events.
as the evaluation of stoat shows injecting this type of events can considerably increase the number of crashes detected by an input generation tool.
.
.
inputs.
as our study shows in some cases specific inputs must be provided to an app to explore parts of its behavior.
to test a browser app for instance valid website addresses should be used.for another example the names of actual movies should be used to test a movie search app.
some existing test input generation tools e.g.
allow users to manually provide inputs that can help testing such as login and password for an app that requires authentication.
some more recent techniques try to go beyond these approaches and automatically generate context aware inputs infer grammars for generating inputs with the right format and allow users to specify simple input generators that can help fuzzing .
in general testing tools should try to go beyond generating purely random inputs and also allow users as a fallback solution to specify specific inputs when needed.
summary of rq3.
how can other automated test input generation tools address these common limitations?
although most existing input generation tools either fail to address the limitations we identified in our study there are ways in which at least some of these limitations can be addressed.
moreover there are recent tools that although still immature aim to mitigate these limitations and represent important steps in the right direction.
threats to validity there are several threats to the external validity of our results.
first our results may not generalize to other apps.
to mitigate this threat we selected benchmarks that were used in previous research and that cover a broad range of app categories second for our experiments we used android .
kitkat api level which is not a recent version of the android os because that allowed us to compile and run most apps.
more empirical studies with additional benchmark apps and newer versions of android would increase confidence in the external validity of the results.
regarding internal validity we only measured coverage of the java code within the apps while ignoring parts written in other languages.
although android apps are written mainly when not exclusively in java this may affect our results.
finally regarding construct validity we could have used different metrics to investigate our research questions.
however we believe that the ones we chose are appropriate for the questions that we were investigating and plan to consider other metrics e.g.
fault detection in future work.
related work there has been a large body of research on automated test input generation for android apps.
in this section we describe the work most closely related to ours grouped by topic.
.
empirical studies choudhary gorla and orso conducted an empirical study on publicly available tools that can automatically generate test inputs for android apps.
their study investigated these tools and their underlying techniques to understand how they compare to one another and which ones may be more suitable in which context.
their study also focused on understanding the ways that the existing tools can be improved or new tools can be developed.
wang and colleagues conducted an empirical study in which they compared existing state of the art android test input generation tools when used on industrial apps in terms of both code coverage and fault detection.
their study reports their experience in applying these tools in an industrial setting and provides insights 1074ase september virtual event australia farnaz behrang and alessandro orso on the strengths and weaknesses of the tools considered and on how combining some of these tools could improve code coverage or fault detection ability.
our empirical study is different from this previous work as our goal is not to compare the existing input generation tools but rather to identify common limitations of random test input generation using monkey as a reference approach and the effect of these limitations in terms of test coverage achieved.
zeng and colleagues conducted a study in which they applied monkey to wechat a feature rich messaging app identified monkey s limitations in this context and reported their findings.
they also developed an approach to address these limitations and achieved consequent improvements in terms of coverage.
our study is different because it focuses on identifying limitations of monkey and similar tools that are not specific to one app but rather common across apps and go beyond the limitations that are only related to exploration strategies.
.
techniques and tools test input generation tools can be classified into four categories based on their exploration strategies random model based systematic and reinforcement learning based.
.
.
random exploration strategy.
these are test input generation tools that employ a random strategy to generate inputs for android apps.
monkey is the most commonly used of these tools.
it is developed and supported by google and is part of the android software development kit sdk .
monkey generates pseudo random streams of user events to test an app.
dynodroid applies random testing but it takes into account the context during exploration.
it also injects system events that are relevant to the app by analyzing its source code.
droidmate also uses a random strategy to explore apps without requiring root access or modifying the os.
intent fuzzer tests how an app can interact with other apps by statically analyzing the source code of the app identifying which intents the app uses and generating intents accordingly.
.
.
model based exploration strategy.
model based strategies construct and use a model of the app under test to systematically generate inputs for the app.
these models are often finite state machines in which nodes represent screens i.e.
activities and menus and edges represent transitions between them.
droidbot dynamically generates a state transition graph and uses the generated graph to guide the exploration of the app under test.
guiripper also builds a model of the app dynamically by crawling it and subsequently uses that model to explore the app.
orbit first statically analyzes the app to extract relevant ui events then uses that information to build a model of the app and then uses the model to test the app.
a3e depth first and a3e target both build a model of the app and then use it to explore the app systematically.
the former builds the model using static dataflow analysis whereas the latter uses dynamic tainting.
swifthand creates a dynamic finite state machine model of the app and refines it to minimize the restarts of the app while exploring it.
puma is a generic framework that allows for implementing dynamic analyses of android apps.
stoat combines static and dynamic analysis to construct a model of the app and then like the other techniques inthis group it uses the generated model to guide the exploration of the app during testing.
unlike other techniques stoat injects in addition to ui events system events so as to improve the effectiveness of test input generation.
.
.
systematic exploration strategy.
systematic exploration is a class of exploration strategies that use more sophisticated techniques such as symbolic execution and evolutionary algorithms to guide the exploration towards previously uncovered code.
evodroid uses an evolutionary algorithm whose fitness function aims to maximize coverage.
acteve aims to explore as many paths as possible using dynamic symbolic execution.
intellidroid is a generic android test input generator that can be configured to produce inputs that target methods that are relevant for a specific dynamic analysis tool.
curiousdroid decomposes the ui of the app under test on the fly and creates a context based model for interactions that are tailored to the current screen layout.
sapienz uses a multi objective search based approach to automatically explore and optimize test sequences minimizing length while simultaneously maximizing coverage and fault detection.
.
.
reinforcement learning based exploration strategy.
koroglu and colleagues use a well known reinforcement learning technique called q learning to explore ui actions within android apps.
esparcia and colleagues also use q learning as a metaheuristic for selecting ui actions in their automatic testing tool.
bauersfeld and vos propose an approach for fully automating robustness testing of complex gui applications that is based on q learning and aims to combine the advantages of random and coverage based testing.
degott borges and zeller combine a crowd based model with a reinforcement learning exploration strategy to automatically learn which interactions can be used for which ui elements.
they then use this information to guide the test generation.
conclusion and future work we presented an in depth empirical study of the limitations of monkey a representative and widely used input generation tool for android apps.
in the study we analyzed monkey s performance on apps and identified general categories of limitations that can prevent monkey from achieving better coverage results on of the considered apps.
we then manually eliminated the identified limitations and reapplied monkey to these apps which resulted in coverage improvements between and for of the apps.
in our analysis of the results we also discussed whether other existing android test input generation techniques can suffer from the same limitations we identified for monkey and if so provided insights on how these techniques could be improved.
in future work we will extend our empirical study by including additional benchmarks and in particular industrial apps so as to confirm and possibly refine our current results.
in future studies we will also consider additional metrics such as fault detection ability and other kinds of coverage and assess how that affects our results.