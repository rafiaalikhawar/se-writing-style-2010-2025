owl eyes spotting ui display issues via visual understanding zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 1laboratory for internet software technologies 2state key laboratory of computer sciences institute of software chinese academy of sciences beijing china 3university of chinese academy of sciences beijing china corresponding author 4monash university melbourne australia liuzhe181 mails.ucas.edu.cn chunyang.chen monash.edu junjie iscas.ac.cn wq iscas.ac.cn abstract graphical user interface gui provides a visual bridge between asoftwareapplicationandendusers throughwhichtheycaninteract with each other.
with the development of technology and aesthetics thevisualeffectsoftheguiaremoreandmoreattracting.
however such gui complexity posts a great challenge to the gui implementation.
according to our pilot study of crowdtesting bug reports display issues such as text overlap blurred screen missing image always occur during gui rendering on different devices due tothesoftwareorhardwarecompatibility.theynegativelyinfluence the app usability resulting in poor user experience.
to detect these issues we propose a novel approach owleye based on deep learning for modelling visual information of the gui screenshot.
therefore owleyecan detect guis with display issues and also locate the detailed region of the issue in the given gui for guiding developerstofixthebug.wemanuallyconstructalarge scalelabelleddatasetwith4 470guiscreenshotswithuidisplayissuesand develop a heuristics based data augmentation method for boosting the performance of our owleye.
the evaluation demonstrates that ourowleyecanachieve85 precisionand84 recallindetecting uidisplayissues and90 accuracyinlocalizingtheseissues.we also evaluate owleyewith popular android apps on google play and f droid and successfully uncover previously undetected ui display issues with of them being confirmed or fixed so far.
keywords ui display mobile app ui testing deep learning acm reference format zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 .
.
owl eyes spotting ui display issues via visual understanding.
in 35th ieee acm international conference on automated software engineering ase september virtual event australia.
acm newyork ny usa 12pages.
introduction graphic user interface gui also short for ui is ubiquitous in almost all modern desktop software and mobile applications.
it permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
a visual bridge between a software application and end users through which they caninteract with each other.
designing a ui requires proper user interaction information architecture and visual effects of the ui.
a good gui design makes an application easy practicalandefficienttouse whichsignificantlyaffectsthe success of the application and the loyalty of its users .
however moreandmorefancyvisualeffectsinguidesignsuch asintensivemediaembedding animation lightandshadowsposta greatchallengefordevelopersintheimplementation.consequently many display issues1such astext overlap missing image blurred screenasseeninfigure1alwaysoccurduringtheuidisplayprocess especially on different mobile devices.
mostofthoseuidisplayissuesarecausedbydifferentsystem settings in different devices especially for android as there are morethan10majorversionsofandroidosrunningon24 distinctdevicemodelswithdifferentscreenresolutions .although the software can still run along with these bugs they negatively influencethefluentusagewiththeapp resultinginthesignificantly bad user experience and corresponding loss of users.
therefore this study is targeting at detecting those ui display issues.
figure examples of ui display issues toensurethecorrectnessofuidisplaying companieshavetorecruitmanytestersforappguitestingorleveragethecrowdtesting.
although human testers can spot these ui display issues there are stilltwoproblemswithsuchmechanism.first itrequiressignificant humaneffortastestershavetomanuallyexploretensofpagesby different interactive ways and also need to check the ui display on differentosversionsanddeviceswithdifferentresolutionorscreen size.second someerrorsintheguidisplay especiallyrelatively minoronessuchas textoverlap componentocclusion aredifficult to spot manually.
to overcome those issues some app development teams adopt the rapid application development rad 1wecallthesebugsasuidisplayissues andwillinterchangablelyuse bugandissuein this paper.
35th ieee acm international conference on automated software engineering ase ase september virtual event australia zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 whichfocusesondevelopingapplicationsrapidlythroughfrequent iterations and continuous feedback.
they utilize users feedback to reveal ui display issues but it is a reactive way for bug fixing which may have already hurt users of the app resulting in the loss of market shares.
incomparisonwithobtainingfeedbackfromusersforreactive app ui assurance we need a more proactive mechanism whichcould check the ui display before the app release automatically spotthepotentialissuesinthegui andremindthedevelopersto fixissuesifany.therearemanyresearchworksonautomatedgui testing by dynamically exploringdifferentpageswithrandomactions e.g.
clicking scrolling filling in the text until triggering the crash bugs or explicit exceptions.
some practical automated testing tools like monkey dynodroid are also widely used in industry.
however these automatedtoolscanonlyspotcriticalcrashbugs ratherthanuidisplayissueswhichcannotbecapturedbythesystem.inthiswork we aim at detecting the ui display issues with the screenshots generated during automatic testing by visual understanding.
tounderstandthecommonuirenderingissues wefirstcarryout a pilot study on non duplicate screenshots from mobile application crowdtesting tasks to observe display issues in these screenshots.
results show that a non negligible portion .
of screenshots are of display issues which can seriously impact theuser experience and degrade the reputation of the applications.
besides we also examine screenshots randomly chosen from the commonly used rico dataset and find .
screenshots having ui display issues.
the common categories of ui display issuesinclude componentocclusion textoverlap missingimage null valueandblurred screen.
considering its popularity and lack of support in current practice of automatic ui testing it would be valuableforclassifyingthescreenshotswithuidisplayissuesfrom the plenty of screenshots generated during ui testing.
inspired by the fact that display bugs can be easily spotted by humaneyes weproposeanapproach owleye2tomodelthevisual informationbydeeplearningtoautomaticallydetectandlocalizeuidisplayissues.our owleyebuildsontheconvolutionalneuralnetwork cnn to identify the screenshots with ui display issues and utilizes gradient weighted class activation mapping grad cam to localize the regions with ui display issues in the screenshotsforguidingdeveloperstofixthebug.toovercomethelackofla beled data for training our model we develop a heuristics based dataaugmentationmethodtogeneratescreenshotswithuidisplay issues from bug free ui images.
we then integrate owleyewith droidbot whichcandynamicallyexploredifferentpagesofthe mobile apps as a fully automatic tool from collecting the screenshotstodetectandlocalizeuidisplayissues.notethatonestrength of our approach over conventional program analysis is that it can be applied to any platform including android ios and it takes the screenshot as theinput which is easy to beobtained in real world practice.
toevaluatetheeffectivenessofour owleye wecarryoutalargescaleexperimenton8 940screenshotsfromcrowdtestingand15 augmented screenshots from android apps.
compared with 2our approach is named as owleyeas it is like owl s eyes to effectively spot ui displayissues.andourmodel nocturnallikeowl cancomplementwithconventional automated gui testing diurnal like eagle for ensuring the robustness of the ui.
state of the art baselines our owleyecan achieve more than and50 boostinrecallandprecisioncomparedwiththebest baseline resulting in precision and recall.
as our owleye can also locate the detailed position of the bug in the ui we carry out a user study to check its accuracy and the results demonstrate that of bug locations are correct.
apart from the accuracyof our owleye we also evaluate the usefulness of our owleyeby applyingitindetectingtheuidisplayissuesinthereal worldapps from google play and f droid.
among apps we find that 57of them are with ui display issues.
we issued bug reports to the development team and are confirmed and fixed by developers.
the contributions of this paper are as follows thisisthefirstworktoconductasystematicalinvestigation ofuidisplayissuesinreal worldmobileapps.wedevelop afirstlarge scaledatasetofappuiswiththatkindofbugs and release it for follow up studies.
based on our pilot findings we propose a novel approach owleye3withcnn basedmodelfordetectingscreenshots with ui display issues and grad cam based model for localizing the buggy region in the ui.
we also propose a heuristics based training data augmentationmethod whichcan automaticallygenerate screenshots of ui display issues with bug free ui images.
motivational study tobetterunderstandtheuidisplayingissuesinreal worldpractice wecarryoutapilotstudytoexaminetheprevalenceoftheseissues.
the pilot study also explores what kindsof ui display issues exist so as to facilitate the design of our approach for detecting uis with display issues.
.
data collection ourexperimentaldatasetiscollectedfromoneofthelargestcrowdtesting platforms4in which crowd workers are required to submit testreportsafterperformingtestingtasks .thedatasetcontains562androidmobileapplicationcrowdtestingtasksbetween january2015andseptember2016.theseappsbelongtodifferent categoriessuchasnews entertainment medical etc.ineachtask crowd workers submit hundreds of testing reports which describe howthetestingisconductedandwhathappenedduringthetest as wellasaccompaniedscreenshotsofthetesting.thereasonwhywe utilizethisdatasetisthatitincludesboththeuiscreenshotsandthe corresponding bug description which facilitates the searching and analysisofuidisplayissues.thisdatasetcontains10 330unique gui screenshots.
.
categorizing ui display issues giventhoseguiscreenshots thefirstthreeauthorsindividually checkeachof themmanuallywithalso itscorrespondingdescriptioninthebugreport.onlyguiscreenshotswiththeconsensusfrom all three human markers are regarded as ones with displayissues.
a total of gui screenshots are determined with ui for the dataset and source code of owleye and the detailed experimental results of this paper.
4baidu baidu.com isthelargestchinesesearchserviceprovider.itscrowdsourcing test platform test.baidu.com is also the largest ones in china.
399owl eyes spotting ui display issues via visual understanding ase september virtual event australia figure examples of five categories of ui display issues display issues which accounts for .
in all screenshots.
this result indicates that the ui display issues account for a non negligible portion of mobile application bugs revealed during crowdtesting and should be paid careful attention for improving the software quality.
during the manually examination process we notice that there are different types of ui issues a categorization of these issues would facilitate the design and evaluation of related approach.
followingthecardsorting method weclassifythoseuiissues into five categories including component occlusion text overlap missing image null value andblurred screen with details as follows componentocclusion asshowninfigure2 a thetextualinformationorcomponentisoccludedbyothercomponents.
itusuallyappearstogetherwithtextvieworedittext.themain reasons are as follows the improper setting of element s height or the adaptive issues triggered when setting a larger sized font.
text overlap as shown in figure b two pieces of textareoverlappedwitheachother.thismightbecausedbythe adaptiveissuesamongdifferentdevicemodels e.g.
whenusinga larger sized font in a device model with small screen might trigger this bug.
notethat fortextoverlapcategory twopiecesoftextaremixed together whileforcomponentocclusion onecomponentcovers part of the other component.
missing image asshowninfigure2 c intheiconposition the image is not showing as its design.
the possible reasons are as follows wrong image path or layout position unsuccessfulloadingoftheconfigurationfileduetopermissions oversized image network connection code logic or picture errors etc.
nullvalue asshowninfigure2 d therightinformation isnot displaying instead nullisshowing incorresponding area.
this category of bugs usually occurs with textview.
the main reasons are as follows issues in parameter setting or database reading andthelengthoftextintextviewexceedingthethreshold etc.
blurred screen as shown in figure e the screen is blurred.
the reason for this bug might because the defects in hardware or the exclusion of hardware acceleration for some cpu or gpu demanding functionalities.
tofurthervalidatethegeneralityofourobservations wealso manuallycheck1 432screenshotsfrom 200random chosenapplications in rico dataset5 which is a commonly used mobile dataset with 66k ui screenshots of android applicationsandwewillfurtherintroducethatdatasetonsection4.we find that uis from apps .
apps are with ui displayissues.notethatnumberishighlyunderestimated asthe collected uis do not cover all pages of the applications and the applications are not fully tested on different devices with different screen resolutions.
.
why visual understanding in detecting ui display issues these findingsconfirm theseverity of uidisplay issues andmotivateustodesignapproachforautomaticallydetectingthesegui issues.
one commonly used practice for bug detection in mobile apps is program analysis but it may not be suitable in this senario.
toapplytheprogramanalysis oneneedtoinstrumentthetarget app develop different rules for different types of ui display issues rewritethecodefordifferentplatforms e.g.
ios android andcustomizetheircodetobecompatibleondifferentmobiledevices e.g.
samsung huawei etc withdifferentscreenresolution whichisex tremelyeffort consuming.specifically itisnottrivialtoenumerate all display issues and develop corresponding rules for detection.
takeninthissense it isworthwhiledevelopinganewefficient and general method for detecting ui display issues.
inspired bythe fact that these display issues can be spotted by human eyes we propose to identify these buggy screenshots with visual understandingtechniquewhichimitatesthehumanvisualsystem.asthe ui screenshots are easy to fetch either manually or automatically andexertnosignificantdifferenceacrosstheappsfromdifferent platforms ordevices ourimage based approach aremore flexible and easy to deploy.
issues detection and localization approach thispaperproposes owleyetoautomaticallydetectandlocalizeui displayissuesinthescreenshotsoftheapplicationundertest as showninfigure3.givenoneuiscreenshot ourcnn basedmodel canfirstclassifyifitrelateswithanydisplayissuesviathevisual understanding.
once the issue is confirmed our model can further localize the detailed issue position on the ui screenshot by grad cam based model for guiding developers to fix the bug.
.
cnn based ui display issues detection as the ui display issues can only be spotted via the visual information we adopt the convolutional neural network cnn 400ase september virtual event australia zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 figure overview of owleye whichhasproventobeeffectiveinimageclassificationandrecognitionincomputervision .figure4showsthestructureof ourclassificationmodelwhichlinkstheconvolutionallayers batch normalization layers pooling layers and fully connected layers.
giventheinputuiscreenshot weconvertitintoacertainimage size with fixed width and height as w h. convolutional layer s parametersconsist ofa setoflearnable filters.the purposeofthe convolutional operation is to extract the different characteristics of the input i.e.
feature extraction .
after convolutional layer the screenshots will be abstracted as feature graph.
inordertoimprovetheperformanceandstabilityofcnn we addbatchnormalization bn afterconvolutionallayer and standardizetheinputlayerbyadjustingandscalingactivation.in aneuralnetwork batchnormalizationisimplementedthrougha normalization step that fixes the mean and variance of each layer s inputs.indetail thestepsforbatchnormalizationareshownbelow y f mean f radicalbig var f considering a batch training we input feature as f then calculate themean mean andvariance var off isaddedinthedenominatorfornumericalstabilityandisanarbitrarilysmallconstant.
figure the architecture of cnn after the bn layer the rectified linear unit relu is added astheactivationfunctionofthenetwork.itincreasesthenonlinearpropertiesofthedecisionfunctionandoftheoverallnetwork withoutaffectingthereceptivefields.reluperformsathreshold operation to eachelement of the input whereany value less than zero is set to zero.the bn layer is then followed by the pooling layer which is to further pick up larger scale detail than just edges and curvesby further distilling the features.
the pooling function uses thetotal statistical characteristics of the adjacent output of a certain locationoftheinputtedimagetoreplacetheoutputofthenetwork at that location and combines the output of one layer of neuron clusterintoasingleneuroninthenextlayertoreducethesizeof data.
max pooling uses the maximumvalue from each of a cluster of neurons at the prior layer.
in a cnn s pooling layers feature maps are divided into rectangular sub regions and the features in eachrectangleareindependentlydown sampledtoasinglevalue commonly by taking their average or maximum value.
in addition toreducingthesizesoffeaturemaps thepoolingoperationgrantsadegreeoftranslationalinvariancetothefeaturescontainedtherein.
the last several layers are fully connected neural networks fc which compile the data extracted by previous layers to form the finaloutput.allinputsfromoneoftheselayersareconnectedto everyactivationunitofthenextlayer.themultiplefullyconnectedrelationshipsincreasethepossibilityoflearningacomplexfunction.
the fully connected layers further encode all features of the ui screenshotintoa k dimensionalvector.finally thedetectionresults are obtained through softmax .
p y b f eftwb summationtext.1k k 1eftwk wherethe k dimensionalvectorarenormalizedintoaprobability distributionwith kprobabilities whichisproportionaltotheindex oftheinputnumber.theinput fisthefeature and p y b f is the predicted probability of fbelonging to category b bug which is similar to the result of the previous layer.
.
grad cam based ui display issues localization although our classification model can check if the given ui screenshot is of display issues some ui display issues may still be too small to spot in a large ui screenshot.
therefore besides the classification model we adopt the feature visualization method to locate the detailed position of the issues for guiding developers to fix the bug.thiscanalsohelpusevaluatewhetherthefeatureextracted by our cnn model is accurate or not.
we apply grad cam model forthelocalizationofuidisplayissues.gradientweightedclass activation mapping grad cam is a technique for visualizing the 401owl eyes spotting ui display issues via visual understanding ase september virtual event australia regionsofinputthatare important forpredictionsoncnn based models .
the final convolutional layer of cnn model contains the spatial and semantic information and this technique uses theclass specificgradientinformationflowingintothefinalconvolutional layer to produce a localization map of the important regions in the inputted image.
the flow of grad cam is shown in figure .
first a screenshot with ui display issue is input into the trained cnn model and the category supervisor to which the image belongs is set to while the rest is .
figure the architecture of grad cam thentheinformationispropagatedbacktotheconvolutional featuremapofinteresttoobtainthegrad campositioning.suppose that the judgment category is b bug the calculation method of the score gradient of bis outputb ak ij whereoutputbis the output ofcategory bbeforesoftmax.throughthefeedbackofglobalaveragepoolingofthegradient theweight b koftheimportanceof neurons is obtained.
this weight captures the importance of thefeature map kof the target class b. by performing the weighted combination of the forward activation graph we can obtain the class discriminative localization map lb grad cam.
b k z summationdisplay.
i summationdisplay.
j outputb ak ij lb grad cam relu summationdisplay.
k c kak finally the point multiplication with the back propagation can obtain the grad cam as the result of ui display issues localization.
.
implementation ourcnnmodeliscomposedof12convolutionallayerswithbatch normalization pooling layers and full connection layers for classifying ui screenshot with display issues.
the size of convolutional kernel in convolutional layer is .
we set up the number of convolutional kernels as for convolutional layer for convolutional layer for convolutional layer and for convolutionallayer9 .themomentuminbnlayerissetas0.
.
forthepoolinglayers weusethemostcommon usedmax poolingsetting i.e.
poolingunitsofsize2 2appliedwithastride .
wesetthenumberofneuronsineachofthefullyconnectedlayers as and respectively.
for data preprocessing werotatesomeuiofthehorizontalscreenstovertical andresize the screens to .
we implement our model based on the pytorch6framework.
heuristic baseddataaugmentation training an effective cnn model for visual understanding requires a large amount of input data.
for example resnet model uses million images from imagenet as training dataset for image classification task.
similarly training our proposed cnn for uidisplayissuesdetectionrequiresabundantofscreenshotswith ui display issues.
however there is so far no such type of open dataset and collecting the related buggy screenshots is quite timeandeffort consuming.therefore wedevelopaheuristic baseddata augmentationmethodforgeneratinguiscreenshotswithdisplay issues from bug free ui images.
thedataaugmentationis basedontherico datasetwhich contains more than 66k unique screenshots from .3k android applications aswellastheiraccompaniedjsonfile i.e.
detailed run timeviewhierarchyofthescreenshot .accordingtoourobservationonsection2 mostuiscreenshotsinthisdatasetareof no dispaly issues.
figure examples of data augmentation algorithm presents the heuristic based data augmentation algorithm.
with the input screenshot and its associated json file the algorithm first locates all the textview and imageview thenrandomly chooses a textview or imageview depending on theaugmented category.
based on the coordinates and size of the textview imageview the algorithm then makes its copy and adjusts its location or size following specific rules to generate the screenshotwithcorrespondinguidisplayissues.figure6demonstratestheillustrativeexamplesoftheaugmentedscreenshotswith ui display issues.
note that among the five categories of ui display issues the category of blurred screen is difficult to be generated following the above idea.
besides preliminary results reveal the proposedapproach can detect this category of issues with relatively high accuracy.hence weleavethiscategoryforfuturework andinthis 402ase september virtual event australia zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 studyweobtainthiscategoryofscreenshotsonlinebysearching blurredscreen .wethenpresentthedetailedaugmentationrulesof the four categories.
algorithm heuristic based data augmentation input scr screenshot without bugs json associated json file category category of generated ui display issue icon pre prepared image icon output augscr augmented screenshot with categorybug 1traverse jsonfile to obtain all textview and imageview 2ifcategory missing image then 3randomly choose an imageview 4else 5randomly choose a textview 6obtain the coordinates of textview imageview x1 y1 x2 y2 coordinate of upper left and lower right 7calculate the width and height of textview imageview w h based on the coordinates 8obtain the text content of textview text 9obtain the background color of textview imageview bg 10ifcategory component occlusion then 11rand random.uniform 12image.new w h rand bg 13ifrand 0then occlude the upper part of component augscr scr.paste image x1 y1 15else occlude the lower part of component augscr scr.paste image x1 y2 h rand 17ifcategory text overlap then 18xrand random.uniform .
w .
w 19augscr scr.write text 20ifcategory missing image then 21image.new w h bg 22scr.paste image x1 y1 23augscr scr.paste icon x1 .
w y1 .
h 24ifcategory null value then 25image.new w h bg 26scr.paste image x1 y1 27augscr scr.write null 28returnaugscr augmentation for component occlusion bug when this category of bug occurs the textual information or component is occludedbyothercomponents.therefore wefirstgenerateacolor block which shares the same background color as the original textview but with a smaller height then put it to cover part of the textview randomly.
augmentation for text overlap bug the textual contents are overlapped with each other when this category of bug occurs.
to augment this category of screenshots we generate a piece oftextwiththesamecontentastheoriginaltextview andoffsetit slightly.
augmentationformissingimagebug wenoticethatwhen this category of bug occurs an image icon would show up to indicate that the area supposes to be an image.
to augment this categoryofscreenshots wefirstdownloadsomefrequently used imageicons online then coverthe originalimagedisplaying area withonerandom chosenimageiconandsetitsbackgroundcolor as the color of its original image.
augmentation for null value bug whenthiscategoryof bugoccurs nullisdisplayedintheareawheresupposestobea piece of text.
we generate this category of screenshots by covering theoriginaltextviewusingacolorblockwhichsharesthesame background color and with nullon it.
note that both component occlusion andtext overlap involves coveringatextview thedifferenceisthattheformeroneutilizesa colorblocktocoverthetextviewsothatitlookslikeacomponent blocks the text while the latter one employs a piece of text to coverthetextviewtomakeitlooklikethetwopiecesoftextare overlapped with each other.
another note is that based on our observation on the screenshots with ui display issues in section whenconductingtheaugmentation thetextviewiscoveredinthe vertical direction in component occlusion while it is covered in the horizontal direction in text overlap.
experiment design .
research questions rq1 issues detection performance how effective of our proposed owleyein detecting ui display issues?
for rq1 we first present some general views of our proposed approach for ui display issues detection and the comparison with commonly usedbaselineapproaches detailsareinsection5.
.we also present the performance comparison among the variations of modelconfiguration e.g.
thenumberofconvolutionallayers to further demonstrate its effectiveness.
besides we also evaluate the contribution of data argumentation by comparing the performance with and without the argumented training data.
rq2 issueslocalizationperformance howeffectiveofour proposed owleyein localizing ui display issues?
forevaluatingtheperformanceofissueslocalization weconduct a user study to check its accuracy.
rq3 usefulnessevaluation howdoesourproposed owleye work in real world situations?
forrq3 weintegrate owleyewithdroidbotasafullyautomatic tooltocollectthescreenshotsanddetectuidisplayissues andthen issue the detected bugs to the development team.
.
experimental setup the experimental dataset comes from two sources.
the first is the screenshotsfromcrowdtesting whichcontains4 470non duplicate screenshotswithuidisplayissuesandequalnumberofbug free non duplicate screenshots see details in section .
the second is the screenshots generated with the data augmentationmethodinsection4.indetail werandomlydownloadone screenshot from each of the random chosen applications in 403owl eyes spotting ui display issues via visual understanding ase september virtual event australia ricodataset andeachscreenshotwouldbeutilizedonceforthedata augmentation.inordertomakethetrainingdatabalancedacross categories we use screenshots for augmenting the component occlusion category whileuse30 screenshotsfordataaugmentation of each of the other three categories.
for the augmented screenshots with ui display issues we first extract their features with orb feature extraction algorithm rank them randomly compute the cosine similarity between a specific screenshot and each of its previous ones and removeitwhenasimilarityvalueabove0.8isobserved.inthisway 800screenshots withuidisplay issuesare remainedandadded into the experimental dataset.
to make the data balanced we then randomlydownloadthescreenshotsfromricoandremovethesimilar ones and a total of bug free screenshots are collected for experiment.forthecategory blurredscreen werandomlydownload 20screenshotswiththisissueonline andrandomlychooseequal number of bug free screenshots from rico.
note that the cosine similaritybetweeneachpairof these40screenshotsisalsobelow .
.
table the number of categories of buggy screenshots categorytraintest valcrowd data aug data component occlusion text overlap missing image null value blurred screen overall in orderto simulate the real worldapplication of ourproposed approach we setup the experiment as follows.
for the screenshots screenshots from crowdtesting apps weutilizethe1 600screenshots 800withuidisplayissues and800without from162appsastestingsettoevaluatetheperformanceof owleye andemployanother1 000screenshots halfof themwithuidisplayissues fromanother50appsasvalidationset to estimate how well the model has beentrained and further tune theparameters.the6 340screenshotsfromtheremaining350apps isutilizedastrainingset.besides allthe15 640screenshots half of them with ui display issues generated with data augmentation is added to the training set to boost the detection performance.
table1presentsthedistributionofscreenshotsintermsofdifferent categories.
the model is trained in a nvidia geforce rtx gpu 16g memory with epochs for about hours.
.
baselines in order to further demonstrate the advantage of owleye we compare it with baselines utilizing both machine learning and deep learningtechniques.the12machinelearningapproachesfirstextract visual features from the screenshots and employ machine learnerfortheclassification.thedeeplearningapproachutilizes artificialneuralnetworkdirectlyonthescreenshotsforclassification.
we first present thethree types offeature extraction method used in machine learning approaches.
sift scale invariant feature transform sift is a common featureextractionmethodtodetectanddescribelocalfeaturesinan image.itcanextracttheinterestingpointsontheobjecttogeneratethe feature description of the object which is invariant to uniform scaling orientation and illumination changes.
surf speed up robot features surf is an improvement ofsift.
surf uses an integer approximation of the determinant of hessian blob detector which can be computed with integer operations using a precomputed integral image.
orb oriented fast and rotated brief orb is a fast feature point extraction and description algorithm.
based on the rapid binary descriptor orb of brief it has rotation invariance and anti noise ability.
with these features we apply four commonly used machine learning approaches i.e.
support vector machine svm knearestneighbor knn naivebayes nb andrandom forests rf for classifying the screenshots with ui display issues.
the combination of three types of image features and four learning algorithms generates a total of baselines.
we also experimentwith multilayerperceptron mlp directly on the screenshots to better demonstrate the superiority of our proposedapproach.indetail mlpisafeedforwardartificialneural network .
the network structure is divided into input layer hidden layer and output layer.
each node is a neuron that uses anonlinearactivationfunction e.g.
correctedlinearunit relu .
it is trained by changing the connection weight according to the outputerrorcomparedwiththegroundtruth.weusedeightlayers of neural network and we set the number of neurons in each layer to and respectively.
.
evaluation metrics in order to evaluate the issues detection performance of our proposedapproach weemploythreeevaluationmetrics i.e.
precision recall f1 score whicharecommonly usedinimageclassification andpatternrecognition .forallthemetrics highervalue leads to better performance.
precision is the proportion of screenshots that are correctly predicted as having ui display issues among all screenshots predicted as buggy precision screenshots correctly predicted as buggy all screenshots predicted as buggy recall is the proportion of screenshots that are correctly predictedasbuggyamongallscreenshotsthatreallyhaveuidisplay issues.
recall screenshots correctly predicted as buggy all screenshots really buggy f1 score f measure or f1 is the harmonic mean of precision and recall which combines both of the two metrics above.
f1 score precision recall precision recall insection6.
weemploykendall sw kendall scoefficientof concordance to assess the agreement of the user evaluated localization results among different practitioners.
it is a commonlyused measurement for the level of agreement between multiple items of multiple raters.
the closer the test outcome is to the higher agreement among the evaluation results of the raters.
404ase september virtual event australia zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 results and analysis .
issues detection performance rq1 we first present the issues detection performance of our proposed owleye aswellastheperformanceintermsoffivecategoriesofui displayissuesintable2.with owleye theprecisionis0.
indicating of the screenshots which are predicted as having ui display issues are truly buggy.
the recall is .
indicating buggy screenshots can be found with owleye.
table issues detection performance rq1 category precision recall f1 score overall .
.
.
component occlusion .
.
.
text overlap .
.
.
missing image .
.
.
null value .
.
.
blurred screen .
.
.
we then shift our focus to the bottom half of table i.e.
the performance in terms of each category of ui display issues.
all the fivecategoriesofuidisplayissuescanbedetectedwitharelative high precision and recall i.e.
mimimum precision and recall are .82and0.80respectively.thecategory missingimage canbedetected with the highest f1 score indicating both precision .
andrecall .
achievearelativelyhighvalue.thismightbecause screenshotswith missingimage bugshaverelativelyfixedpattern and the buggy area is relatively large i.e.
the whole image icon asshowninsection2.incomparison thecategory textoverlap is recognized with the lowest f1 score e.g.
.
precision and .
recall.
this is due to the fact that the pattern of this category is more diversified and the buggy region is much smaller i.e.
the overlappingarea between twopieces of textaccounts for amere of of the text component.
figure examples of bad case in issues detection rq1 wefurtheranalyzethescreenshotswhicharewronglypredicted as bug free with examples in figure .
one common shared by thesescreenshotsisthatthebuggyareaistootinytoberecognized evenwithhumaneye.futureworkwillfocusmoreonimproving the detection performance for these screenshots with attention mechanism and image magnification.
.
.
performance comparison with baselines.
table3shows theperformancecomparisonwiththebaselines.wecanseethat our proposed owleyeis much better than the baselines i.e.
higher in recall compared with the best baseline mlp and higherinprecisionwiththebestbaseline orb nb .thisfurtherindicatestheeffectivenessof owleye.besides italsoimpliesthat owleyeis especially good at hunting for the buggy screenshots from candidate ones i.e.
larger improvement in recall.
table performance comparison with baselines rq1 method precision recall f1 score sift svm .
.
.
sift knn .
.
.
sift nb .
.
.
sift rf .
.
.
surf svm .
.
.
surf knn .
.
.
surf nb .
.
.
surf rf .
.
.
orb svm .
.
.
orb knn .
.
.
orb nb .
.
.
orb rf .
.
.
mlp .
.
.
owleye .
.
.
mlp achieves the highest recall among the baselines indicatingthisdeeplearningapproachisbetteratidentifyingthebuggy screenshots yet with a lower precision.
the machine learning approacheswithorbfeatureachievethehighestf1 score i.e.
.
by orb nb indicating this kind of feature is more suitable fordetecting ui display issues.
this might because orb algorithmis the state of the art feature extraction algorithm and has been proven to be an efficient alternative to sift or surf .
.
.
performance comparison among model configurations.we also conduct experiments to compare the detection performance with different configurations of cnn model.
table showstheperformanceofuidisplayissuesdetectionintermsof different convolutional layers and with without batch normalization bn .
we can see that both the convolutional layers and the batch normalizationcaninfluencetheissuesdetectionperformance.generally speaking when deepening the neural network i.e.
moreconvolutional layers both precision p and recall r would increase.forexample thepr ecisionundergo18 improvementwhen convolutionallayersincreasefrom4to12 withbatchnormalization andtheimprovementofrecallis58 withsameconfiguration changes.
besides the employment of batch normalization can also improvethe performance.therearerespectively18 and47 improvementin precisionandrecallwhen addingthebatchnormalization 12convolutionallayers .thisindicatestheeffectiveness of our applied configurations in owleye.
table performance comparison among model configurations rq1 layer without bn with bn number prf prf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
contribution of data augmentation .weinvestigatethe contribution ofdata augmentationby comparingthe issuesdetectionperformanceontheoveralltrainingdataandonthetraining 405owl eyes spotting ui display issues via visual understanding ase september virtual event australia data by removing the screenshots generated with data augmentation details are in section .
.
from table we can see that both precision p and recall r improve when the augmented screenshotsareaddedtothe trainingdata indicatingthevalueof data augmentation for effective ui display issues detection.
specifically and improvement are observed respectively for precision and recall.
the larger improvement in recall indicates that withtheaugmenteddataset morescreenshotswithuidisplay issuescanbefound.thismightbecausethetrainingsetwiththe argumented data is more diversified in the screenshots thus has greater issues detection capability.
table contribution of data augmentation rq1 categorywithout dataaug with dataaug prf prf overall .
.
.
.
.
.
component occlusion .
.
.
.
.
.
text overlap .
.
.
.
.
.
missing image .
.
.
.
.
.
null value .
.
.
.
.
.
blurred screen .
.
.
.
.
.
wealsopresent theperformanceimprovementintermsoffive categories of ui display issues in table .
results show that issues detectionperformancein nullvalue categoryundergoesthelargest improvement inf1 score.
thismight becausethe trainingdataset beforeaugmentationhasfewscreenshots withthisbug detailsareinsection5.
andthetinyregionfordecidingthis category of bug makes it even difficult for the automatic detection.
after adding the augmented screenshots the diversity of the training screenshots significantly improves the performance.
.
issues localization performance rq2 figure presents the examples of our issues localization whichhighlights the buggy areas.
we conduct a user study to evaluatethelocalizationperformance.werecruitsixsoftwaredevelopers online allofwhommajorincomputerscienceandhavemorethan two years of software development experience.
each of them is presentedwith679correctlydetectedbuggyscreenshotsinrq1 and the accompanied localization results as shown in figure .
they are required to independently evaluate the issues localizationresults andtoanswerthequestionwhethertheyagreewith each of the localization results using likert scale i.e.
strongly agree agree neutral disagree strongly disagree .
the evaluationresultsshouldbereturnedwithineighthourstoensurethe credibility of this study.
table results of issues localization rq2 participant s agree agree neutral disagree s disagree p1 .
.
.
.
.
p2 .
.
.
.
.
p3 .
.
.
.
.
p4 .
.
.
.
.
p5 .
.
.
.
.
p6 .
.
.
.
.
average .
.
.
.
.
asshowninthetable6 thepractitionersstronglyagreeoragree with the ui display issues localization results in an average of figure examples of issues localization rq2 i.e.
screenshots andonlydisagree orstronglydisagree in an average of screenshots.
this indicates the accuracy of our issues localization in the screenshots.
we further calculatethe kendall s w details are in section .
to judge to what extent the evaluation results submitted by the six practitioners are consistentwitheachother.theresultofkendall swis0.
which indicatesahighdegreeofinter agreementontheperformanceof our issues localization.
figure examples of bad case in issues localization rq2 we further analyze the bad case of issues localization as shown infigure9 andfindmostoftheminvolvethescreenshotswith text overlapandcomponent occlusion issues.
as mentioned in previous subsection this might because of the tiny region for localizing the issues which can easily mislead the model.
.
usefulness evaluation rq3 tofurtherassesstheusefulnessofour owleye werandomlysample android applications from f droid7and android applications from google play8.
note that none of these apps appear in our training dataset.
we use droidbot which is a commonly used lightweight androidtestinputgenerator forexploringthemobileappsand take the screenshot of each ui pages.
among the collected 406ase september virtual event australia zhe liu1 chunyang chen4 junjie wang1 yuekai huang1 jun hu1 qing wang1 apps appscanbesuccessfullyrunwithdroidbot add only of the apps can be fetched with more than one screenshot asthey require register orauthenticate to explore more screenshots which cannot be done by droidbot.
for the remaining apps an average of eight screenshots are obtained for eachapp.wethenfeedthosescreenshotsto owleyefordetectingif there are any ui display issues.
once a display issue is spotted we create a bug report by describing the issue attached with buggy ui screenshot.finally wereportthemtotheappdevelopmentteam through issue reports or emails.
table confirmed or fixed issues rq3 app name category source download idstatus perfect piano music google 50m email confirm music player music google 50m email confirm nox security tool google 10m email fixed degoocloud storage tool google 10m email fixed proxynel tool google 10m email confirm secure vpn tool google 10m email confirm thunder vpn tool google 10m email confirm apowermirror tool google 5m email confirm mediafire product google 5m email confirm postegro commun google 500k email fixed deezer music player music google 500k email fixed mtg familiar utilities f droid 500k fixed open food facts health f droid 500k confirm linphone commun f droid 500k 965confirm paytm finance google 100k email confirm transdroid tool f droid 100k 542confirm transistor music f droid 10k fixed onkyo music f droid 10k fixed democracydroid news f droid 10k 51confirm newpipe legacy media f droid 8k fixed lesspass product f droid 5k fixed cetoolbox medical f droid 4confirm opentracks osm health f droid fixed yucata envoy tool f droid n a 3confirm classyshark3xodus tool f droid n a 3confirm vlcfreemote media f droid n a 24confirm table7showsallbugsspottedbyour owleye andmoredetailed information of detected bugs can be seen in our website3.
for fdroid applications ui display issues are detected among which have been fixed and another have been confirmed by the developers.
for google play ui display issues are detected among which have been fixed and another have been confirmed by the developers.thesefixedorconfirmedbugreportsfurtherdemonstratetheeffectivenessandusefulnessofourproposedapproachin detecting ui display issues.
discussion generality across platforms.
almost all the existing studies of gui bug detection are designedfor a specific platform e.g.
android whichlimitsitsapplicabilityinreal worldpractice.in comparison theprimary idea ofour proposed owleyeis todetect uidisplayissuesfromthescreenshotsgeneratedwhenrunningthe applications with visual understanding.
since the screenshots from differentplatforms e.g.
android ios exertalmostnodifference our approach can be generalized for ui display issues detection in otherplatforms.wehaveconductedasmallscaleexperimentfor another popular platform ios and experiment on seven screenshotswithuidisplayissuescollectedinourdaily usedapplications.
resultsshow thatourproposed owleyecanaccurately detectfive of the buggy screenshots.
this further demonstrates the generality of owleye and we will conduct more thorough experiment in future.
generalityacrosslanguages.
anotheradvantageof owleyeis that it can be applied for ui display issues detection in terms of different display languages of the application.
the testing data of the experiment for rq1 contains the screenshots in chinese while the experiment for rq3 relates with the screenshots in english which demonstrates the generality of our approach across languages.
we also collect screenshots with ui display issues in two other languages i.e.
german and korean from the applications in rq3 and runourapproachforbugdetection.resultsshowthatourproposed owleyecan accurately detect of the buggy screenshots which further demonstrates the feasibility of owleye.
potentialwithmoreeffectiveautomatictestingtool.
results in rq3 have demonstrated the usefulness of owleyein real world practicebeingintegratedwithautomatictestingtoolasdroidbot.
however we have mentioned in section .
that some applications can not be run with droidbot and some can only be fetched with one screenshot due to the shortcoming of droidbot both of which limit the full exploration of screenshots.
if armed with a more effectiveautomatictestingtool owleyeshouldplayabiggerrole in detecting ui display issues in real world practice.
related work guiprovidesavisualbridgebetweenapplicationsandusers.therefore manyresearchersareworkingonassistingdevelopersordesignersintheguisearch basedonimage features guicodegeneration basedoncomputer vision techniques.
moran et al.
check if the implemented gui violates theoriginal ui designby comparing theimages similarity with computervision techniques.a follow upwork bythem further detects and summarizes gui changes in evolving mobile applications.differentfromtheseworks ourworksarefocusing on gui testing.
toensurethatguiisworkingwell therearemanystaticlinting tools to flag programming errors bugs stylistic errors and suspiciousconstructs .forexample androidlint reports over260differenttypesofandroidbugs includingcorrectness performance security usabilityandaccessibility.stylelint helps developersavoiderrorsandenforceconventionsinstyles.different from static linting automatic gui testing dynamically exploresguisofanapp.severalsurveys comparedifferent toolsforguitestingforandroidapps.sometestingworksfocus on more specific ui issues such as ui rendering delays and image loading .
recently deep learning based techniques havebeenproposedforautomaticguitesting.unliketraditional guitestingwhichexplorestheguisbydynamicprogramanalysis thesetwotechniquesusecomputervisiontechniquestodetectgui componentsonthescreentodeterminenextactions.inspiredby their works we also adopt the cnn in our study.
butnotethattheseguitestingtechniquesfocusonfunctional testing.
in contrast our work is more about non functional testing i.e.
guivisualissueswhichwillnotcauseappcrash butnegatively influence the app usability.
the ui display bugs detected by our approacharemainly caused bytheappcompatibility due tothedifferentdevicesandandroidversions.itishighlyexpensive 407owl eyes spotting ui display issues via visual understanding ase september virtual event australia and extremely difficult for the developers covering all the popular contexts when conducting testing.
besides different from these works based on static or dynamic code analysis our work only requires the screenshot as the input.
such characteristic enables our light weight computer vision based method and also makes our approach generalised to any platform including android ios or iot devices.
conclusion improving the quality of mobile applications especially in a proactiveway isofgreatvalueandalwaysencouraged.thispaperfocuses on automatic detecting the ui display issues from the screenshots generated during automatic testing.
the proposed owleyeis proventobeeffectiveinreal worldpractice i.e.
26confirmedor fixedpreviously undetecteduidisplayissuesfrompopularandroid apps.
owleyealso achieves more than and boostin recall and precision compared with the best baseline.
as the first work of its kind we also contribute to a systematical investigation of ui display issues in real world mobile apps as well as a large scale dataset of app uis with display issues for follow up studies.
inthefuture wewillkeepimprovingourmodelforbetterperformance in the classification.
apart from the display issue detection wewillfurtherlocatetherootcauseoftheseissuesinourfuture work.thenwewilldevelopasetoftoolsforrecommendingpatches to developers for fixing display bugs.