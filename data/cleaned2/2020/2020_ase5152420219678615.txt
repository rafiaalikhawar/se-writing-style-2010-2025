pynose a test smell detector for python tongjie wang university of california irvine irvine ca united states tongjiew uci.eduyaroslav golubev jetbrains research saint petersburg russia yaroslav.golubev jetbrains.comoleg smirnov jetbrains research saint petersburg state university saint petersburg russia oleg.smirnov jetbrains.com jiawei li university of california irvine irvine ca united states jiawl28 uci.edutimofey bryksin jetbrains research saint petersburg state university saint petersburg russia timofey.bryksin jetbrains.comiftekhar ahmed university of california irvine irvine ca united states iftekha uci.edu abstract similarly to production code code smells also occur in test code where they are called test smells.
test smells have a detrimental effect not only on test code but also on the production code that is being tested.
to date the majority of the research ontest smells has been focusing on programming languages such asjava and scala.
however there are no available automated toolsto support the identification of test smells for python despiteits rapid growth in popularity in recent years.
in this paper westrive to extend the research to python build a tool for detectingtest smells in this language and conduct an empirical analysisof test smells in python projects.
we started by gathering a list of test smells from existing research and selecting test smells that can be considered language agnostic or have similar functionality in python s standardunittest framework.
in total we identified diverse test smells.
additionally we searched for python specific test smells bymining frequent code change patterns that can be considered aseither fixing or introducing test smells.
based on these changes we proposed our own test smell called suboptimal assert .t o detect all these test smells we developed a tool called p ynose in the form of a plugin to pycharm a popular python ide.finally we conducted a large scale empirical investigation aimedat analyzing the prevalence of test smells in python code.
ourresults show that of the projects and of the test suites inthe studied dataset contain at least one test smell.
our proposedsuboptimal assert smell was detected in as much as .
of the projects making it a valuable addition to the list.
index t erms test smells code smells python empirical studies code change patterns mining software repositories i. i ntroduction code smells were introduced to identify potential maintainability issues in software systems however later they have been used as a measure of design quality of softwareprojects .
researchers found that code smells areassociated with bugs fault proneness andmaintainability issues in the code base .
while investigatingthe underlying reasons for introducing code smells researchersattributed various factors to this including developers strug gling with deadlines or not caring about the impact of theapplied design choices .
the first two authors contributed equally to this work.similarly to production code test code can also have code smells in which case they are called test smells.
v an deursen et al.
defined test smells as being caused by poor designchoices similarly to regular code smells when developing testcases.
1just like the code smells test smells make the impacted test code harder to maintain and comprehend .
moreover recent studies have shown that test smells also impact thequality of production code .
since test smells have a negative impact on the quality of production code it is of great interest and importance to studyand detect them.
to date the majority of the research on testsmells has been focusing on statically typed languages likejava and scala .
however inrecent years python has been growing in popularity due tobeing the primary language used in data science and machinelearning in particular .
furthermore despite theempirical evidence against test smells developers tend not tobe aware of the smells that exist in their tests and thelack of efficient tools can be one of the reasons for it.
tothe best of our knowledge there are no works that study theexistence and prevalence of test smells in python code andno tools exist that specifically aim at identifying test smells inthis language.
in this paper we aim to fill these gaps by curating a list of possible test smells for python a tool for their detection and an empirical study of their pervasiveness in python code.we started by conducting a small scale mapping study to finddifferent test smells studied in the literature and selectingtest smells that can be considered language agnostic or haveanalogous functionality in python s standard unittest framework.
in total we identified diverse test smells.
thesetest smells were all adopted from other papers dedicated todifferent programming languages but it is natural to assumethat python has its own specific test smells.
to discoverthem we used a tool called p ython change miner to 1to avoid the ambiguity that exists in testing terminology between languages and frameworks in this paper we will always refer to individual tests or test methods as test cases and to classes that group them as test suites.
36th ieee acm international conference on automated software engineering ase 36th ieee acm international conference on automated software engineering ase .
ieee .
ase51524.
.
.
ieee search for frequent change patterns in test suites.
we manually evaluated patterns that occur in at least three differentprojects and identified possible changes that are related toassert functions in unittest and are aimed at making the tests more specific and simplify the understanding of the testinglogic.
we bundled the less specific versions of these assertionstogether into a single suboptimal assert test smell.
thus a total of smells were identified for python.
we developed p ynose a plugin for pycharm that is able to detect these smells in the python code.
using the tool we performed an empirical study on the prevalence of testsmells in python projects.
our results indicate that testsmells are indeed common in python test code with ofprojects and of test suites having at least one test smell.
overall our contributions are as follows we conducted a small scale mapping study and compileda list of test smells that are applicable to python.
we identified a new python specific test smell by ana lyzing python test code changes.
we developed a tool called p ynose as a plugin for pycharm that can detect test smells from python projectsthat use the standard unittest framework.
p ynose is available for researchers and practitioners on github we report the findings pertaining to the pervasiveness oftest smells from an empirical study conducted on 248python projects.
the rest of the paper is organized as follows.
in section ii we discuss the existing works in the field of test smellsdetection and analysis.
section iii describes the choice oftest smells for python and the search for python specifictest smells.
in section iv we describe the development of p ynose and its evaluation and in section v we describe the empirical study that we conducted using the tool as wellas its results.
in section vi we discuss threats to the validityof our study and finally in section vii we conclude ourpaper and discuss possible future work.
ii.
r ela ted work similarly to production code test code should be designed following proper established programming practices .
v andeursen et al.
defined the term test smells as code smells that are caused by poor design choices when developing testcases and also defined a catalog of test smells.
later severalresearchers extended this catalog .
whilethe majority of the research focused on test smells occurringin java several researchers investigated other languages anddomains.
for example bleser et al.
investigated test smells inscala while peruma et al.
explored unit testsin mobile applications and identified several new test smells.
researchers have also been investigating the negative impacts of test smells on software development .
by conducting two empirical studies bavotaet al.
showed that test smells are widely spreadthroughout software systems and most test smells have astrong negative impact on the comprehensibility of test suitesand production code.
spadini et al.
investigated therelationship between the presence of test smells and thechange and defect proneness of test code as well as thedefect proneness of the tested production code.
they foundthat some test smells are more change prone than others andthey also found that production code tested by smelly tests iscomparatively more defect prone.
tufano et al.
found thattest smells are usually introduced when the corresponding testcode is committed to the repository for the first time and theytend to remain in a system for a long time.
virg nio et al.
investigated correlations between test coverage and test smells and found that test smells influence code coverage.
investigating ways for an automated detection of test smells has also received attention from the research community.
v anrompaey et al.
proposed a set of metrics defined in termsof unit test concepts and compared the proposed detectiontechniques effectiveness with human review.
greiler et al.
analyzed the relationship between the development of a testfixture and possible test smells within it.
they also designeda static analysis tool to identify fixture related test smells andevaluated them by discovering test smells in three industrialprojects.
palomba et al.
developed an automated textual based approach for detecting several types of test smells.compared with the code metrics based techniques proposed by greiler et al.
and v an rompaey et al.
the textual basedtechnique proved to be more effective in detecting certain testsmells.
peruma et al.
recently developed a tool called tsdetect capable of detecting test smells in java.
more recently researchers have been investigating ways to help testers refactor test smells.
lambiase et al.
presentedan intellij based plugin that enables an automated identifica tion and refactoring of test smells using intellij platform sapis.
santana et al.
proposed another tool that can beused in an ide providing testers with an environment forautomated detection of lines of code affected by test smells as well as a semi automated refactoring for java projects.virg nio et al.
presented a tool designed to analyze test suite quality in terms of test smells.
their tool is the firstone that relies on both code coverage and the presence of testsmells to measure the quality of tests.
overall the majority of the mentioned research has been focusing on java.
however in recent years python has beengrowing more popular because of its important role in datascience and machine learning in particular .
to thebest of our knowledge no tools exist that specifically aim atidentifying python test smells.
p ynose addresses this gap.
furthermore there is no large scale analysis regarding theprevalence of test smells in python code and our study isthe first towards filling this gap in research.
iii.
s electing test smells the goal of our study is to build a tool that can identify test smells in python code as well as to assess to whatextent test smells are prevalent in python test suites.
thegeneral pipeline of our study is demonstrated in figure .
insection iii we curate the list of appropriate test smells by u u u uuu u fig.
.
the overall pipeline of the study.
conducting a systematic mapping study section iii a and then augmenting the list by identifying python specific testsmells section iii b .
a. systematic mapping study of test smells as a first step we conducted a small scale systematic mapping study on test smells to curate a list of test smells discussed in the literature.
according to kitchenham et al.
the goalof the mapping study is to survey the available knowledgeabout a topic.
search question.
our search question was phrased as follows what test smells have been studied in literature to date?
search keywords.
to determine the optimal set of search keywords we conducted a pilot search on two well knowndigital libraries ieee and acm.
this process was intendedto identify relevant words utilized in test smell publications.we conducted our query only on the title and abstract of thepublication to avoid false positives.
the finalized search stringis presented below.
title test smell or test smells and abstract test smell or test smells .
data source.
to discover relevant publications we used three of the most popular online paper search engines acmdigital library ieee xplore and scopus.
search period.
to obtain as many related works as possible we queried all related studies before .
this resulted in alist of papers that were published between to .
initial results.
our initial search of the three digital libraries resulted in publications.
to narrow down the search results next we filtered out publications that were not part of ourinclusion criteria.
a summary of the inclusion and exclusioncriteria used to filter the retrieved literature is shown in table i.the filtering process helped us to reduce the number of studiessignificantly however this may have resulted in leaving outsome relevant studies.
thus we conducted backward snow balling i.e.
looking for additional studies in the reference lists of the selected studies as suggested by keele et al.
.in our work we implemented a single iteration of backwardsnowballing.
to ensure the reliability of the selected studies each study was evaluated by three authors of this paper.
eachtable i inclusion and exclusion criteria .
inclusion criteria .
publications that implement software engineering methodologies approaches and practices in test smell detection and refactoring.
.
available in digital format.
exclusion criteria .
publications that are not written in english.
.
websites leaflets and grey literature.
.
published in .
.
full text not available online.
.
tools not associated with peer reviewed papers.
.
duplicated publications.
selected study underwent an agreement process and in caseof uncertainty and disagreement we discussed it until wereached consensus.
we finally ended up with with a set of29 studies.
next we merged the lists of test smells mentionedin these papers which resulted in a list of different testsmells encountered in java scala and android systems.
thefull list of papers and test smells is available online in thesupplementary materials .
next we considered the possibility of implementing each test smell for python.
there were several reasons why someof the test smells could not be implemented the test smell is not applicable to python.
for example the resource optimism test smell in java occurs if a file object is used without checking for its existence.
however in python files always associate with resources because according to the python official documentation open is the standard way to open files for reading and writing withpython .
the test smell detection relies on the production code that is being tested.
for example to identify eager test and lazy test we need to know what the corresponding production files and production classes are.
a lot of recent works studytest to code traceability and a lot of differentapproaches have been suggested.
however reliably makinga strict one to one connection between a test method anda production method in the static analysis environment isdifficult which is why leave the support of such testsmells for future work.
595the test smell detection is possible only when the test is executed.
for example for the test run war it is necessary to actually run the test case which is not possible in a static analysis environment.
even after running identifying such testsmells is non trivial and for practical purposes we had toexclude them.
finally we selected test smells for implementing.
we list them below.
assertion roulette occurs when a test case has multiple non documented assertions.
multiple assertion statementswithout a descriptive message impact the readability under standability and maintainability as it becomes more difficultto understand the reason why this test fails .
conditional test logic runs against the rule that test cases need to be simple and execute all statements in the productioncode.
conditions within the test case alter the behavior of thetest and lead to situations where the test fails to detect defectsin the production code under some conditions .
constructor initialization is made by developers who are unaware of the purpose of the setup method that contains the preparation needed to perform test cases.
as a result theywould define a constructor for the test suite which is not idealin practice .
default test occurs when an ide creates default test suites when the project is created and developers keep the defaultname.
for example pycharm by default names the test suitesmytestcase.
these suites are meant to serve as an examplefor developers when writing unit tests and should be renamed.not renaming them upfront causes developers to start addingtest cases into these files making the default test suite acontainer of all test cases.
this can also cause problems whenthe suites need to be renamed in the future .
duplicate assert occurs when a test case tests for the same condition multiple times .
empty test occurs when a test case does not contain executable statements.
such tests are possibly created fordebugging purposes and then forgotten about or contain com mented out code .
exception handling occurs when passing or failing of a test case is dependent on the production method explicitlythrowing an exception.
instead developers should utilize spe cial functionality of testing frameworks for that such as anassertraises function .
general fixture occurs when a test suite fixture is too general and some test cases only access a part of it.
the fixtureof a test suite is a special method that is executed before thetest cases in the suite and serves as a setup step.
a drawbackof it being too general is that unnecessary work is being donewhen a test suite is run .
ignored test is caused by ignored test cases when it is possible to suppress some test cases from running.
these ig nored test cases add unnecessary overhead by increasing codecomplexity and making comprehension more difficult .
lack of cohesion of test cases occurs if test cases are grouped together in one test suite but are not cohesive.
cohe sion of a class is a metric that indicates how well various partsand responsibilities of a class are tied together.
if test cases ina suite are not cohesive this can cause comprehensibility andmaintainability issues .
magic number test occurs when assert statements in a test case contain numeric literals i.e.
magic numbers as parameters instead of more descriptive constants or variables .
obscure in line setup occurs when the test case contains too many setup steps.
this can hinder inferring the actualpurpose of the assertion in the test.
ideally such preparationshould be moved to a fixture or a separate method .
redundant assertion occurs when a test case contains assertion statements that are either always true or always false and are therefore unnecessary .
redundant print occurs when there is a print statement within the test.
print statements are considered to be redundantin unit tests as unit tests are usually executed as a part of anautomated process with little to no human intervention .
sleepy test occurs when developers need to pause the execution of statements in a test case for a certain duration i.e.
simulate an external event and then continue with theexecution.
explicitly causing a thread to sleep can lead tounexpected results as the processing time for a task can varyon different devices .
test maverick was derived from the general fixture described above.
if the test suite has a fixture with setup buta test case in this suite does not use this setup this test caseis a maverick outlier .
the setup procedure will be executedbefore the test case is executed but it is not needed .
unknown test occurs when the test case has no assertion in it.
it is possible to create a test case that does not useassertions however such a test is more difficult to understandand interpret .
during this selection we also decided to focus specifically on the unittest testing framework that is included into the python standard library.
python also has a lot of popularthird party testing frameworks like pytest and robot however certain test smells would look differently in differentframeworks and it is out of the scope of this paper to supportthem all.
there are two reasons for choosing specificallyunittest.
firstly it remains one of the most popular testingframeworks in python while also being the default one .secondly according to its documentation unittest was originally inspired by junit which allows us to detect some test smells from the literature that were originally proposed forjunit for example fixture related test smells.
additionally several other frameworks support launching test suites fromunittest and can therefore also be detected in this case.
b. identifying python specific test smells in addition to the test smells identified above our goal was to include python specific test smells.
to discover python specific test smells we used a tool called p ython change miner to search for frequent change patterns in the histories of test suites.
we explain the steps of this process in detail in this section.
project selection to carry out this research we needed to collect a dataset of mature open source python projects.
as a starting point we took ghtorrent a large collection ofgithub data more specifically their latest dump at the timeof the compilation compiled in july .
to processit we used a tool called pga create that had beenpreviously used to create public git archive pga .
thistool processes the sql dump to create a csv file with a listof projects that facilitates their convenient filtering.
next weselected all projects with at least stars which allowed us tofilter out toy projects.
we also only considered projects withpython as the main language that are not forks.
this resultedin identifying projects.
of them we randomly selected10 .
the reason for not simply picking top projects by starsis that testing might be organized very differently in projectsof different scale and simply picking the largest or the mostpopular repositories could skew our data towards a specifictype of projects.
next we analyzed the history of the projects to find all commits where at least one python test file was changed.
wedefined a python test file as any file with the .py extension that has the word test in its filename since unittest has a naming convention of having the word test in the name of the test file .
we have conducted a small manual analysisby selecting random python files with the word test in their name and checking whether they are actually relatedto tests.
in this random sample all files were related totesting with explicitly containing test suites and test cases and another containing auxiliary methods and testing utils.
of the projects had at least one commit that changed such files.
as we were looking for code changes we selectedthese projects.
since our goal was to analyze thechanges themselves for practical purposes we decided toselect a smaller set of projects using the criteria recommendedin literature .
we selected projects with at least 000commits contributors years since the first commit andno more than year since the last push.
this resulted in 450projects.
for the purposes of this paper we will call this theprimary dataset the list is available online .
change pattern mining to identify python specific test smells we started by mining the histories of the collectedprojects and finding patterns in the changes made to test filesthat might be considered as either fixing or introducing atest smell.
we extracted all changes made to python test filesfrom the identified projects and processed these files using p ython change miner .
python change miner is a tool that we developed for mining code change patterns in python code.
the tool isbased on the algorithm developed by nguyen et al.
forjava.
the parser in their tool is written specifically for thesyntax of the java language and their tool stores graphs andworks with them as java objects so we could not directlyreuse the tool.
at the same time the algorithm itself is notlanguage specific because it relies only on the abstract syntaxtrees ast of code before and after the change which iswhy we implemented it for python.
the operation processof p ython change miner is similar to that of the tool by nguyen et al.
here we briefly explain the procedure.
python change miner works in two stages building change graphs and mining patterns.
in the first stage theversions of code before and after the change are parsed intoa special representation introduced by nguyen et al.
calledfine grained program dependence graphs fgpdgs .
fgpdgs are graphs with three types of nodes data nodes variables literals constants etc.
operation nodes arithmetic bit wise operations etc.
and control nodes control sequences like if while for etc.
.
these nodes are connected using two types of edges control edges represent a connection between acontrol node and a node that it controls and data edges show the flow of the data in the program such edges also have labelsspecifying the flow of data.
then unchanged nodes in the two fgpdgs of code before and after the change are connected together by special map edges resulting in new graphs called change graphs.
we used gumtree to detect corresponding unchanged nodes in theversions before and after the change and connect them with amap edge.
this is carried out on a function level and therefore this way we obtain a special change graph that represents each change to each testing function from the history of projects inour dataset.
y ou can find an example of fgpdgs and a changegraph in the supplementary materials .
the second stage of p ython change miner involves searching these change graphs for patterns.
this part is alsodone similarly to the work of nguyen et al.
.
first all pairsof nodes representing function calls that are also connectedwith the map edge are considered to be the initial patterns that are then recursively expanded to contain new nodes.
the pat tern is defined by two thresholds minimum size indicating the minimum number of graph nodes in the pattern and minimum frequency indicating the minimum number of repetitions ofthe pattern in the corpus.
changing these parameters influenceswhat is considered to be a pattern and therefore how manypatterns are detected.
this way the patterns are expanding todetect isomorphic subgraphs within our corpus of graphs.
in our work we use the same thresholds as nguyen et al.
minimum size of and minimum frequency of .
it is possible that studying specifically the testing code requiresdifferent thresholds we leave such analysis for future work.we additionally add a maximum size threshold of .
this is done to make the process faster by stopping the patterns fromgrowing too large.
our own preliminary experiments and ouranalysis of the results of nguyen et al.
demonstrated that themajority of discovered patterns are small.
more specifically the depth pattern corpus provided by nguyen et al.
contains a total of patterns of which .
patterns are nodes or smaller.
since smaller patterns aremuch more frequent and are easier to analyze we decided tofocus on them.
an example of a discovered pattern is presentedin figure .
test smells detection in total p ython change miner was able to discover different patterns in the primary dataset.
of them patterns were cross project meaning 597vhoi uhvxowv dvvhuw7uxh lq vhoi uhvxowv dvvhuw q a commit in the obspy project .
vhoi irr ooypbef dvvhuw7uxh lq vhoi irr ooypbef dvvhuw q b commit in the numba project .
vhoi sdvvzrug dffrxqw gdwd dvvhuw7uxh lq vhoi sdvvzrug dffrxqw gdwd dvvhuw q c commit in the reviewboard project .
fig.
.
an example of a change pattern identified in several projects on github.
they were encountered in at least two different projects and appeared in at least three different projects.
three authorsof the paper independently manually labeled all of suchchanges to discover changes that either fix or introduce possi ble test smells.
the reason for focusing on these changes is thatthey are inherently more universal among different developers.along with analyzing the code changes themselves the authorsalso looked at the corresponding commit messages sincecommit messages may contain the rationale for a change.after individual labeling the authors discussed their labelsand reached a perfect agreement.
of the studied patterns constituted various changes to assertion functionality similar to the exampleshown in figure .
three authors of the paper independentlycame to a conclusion that the candidates for possible python specific test smells can be found only within this group because other common changes in testing code correlate tovarious other aspects of software engineering data structures data processing etc.
that are not directly related to testingitself.
for example popular changes include changing the levelof the logger error info debug etc.
or changing the shapeof anumpy array.
such patterns are important but are not directly related to testing or test smells.
we categorized assert related change patterns into three categories which we describe below with specific examples.
i.assertion changes that alter the logic.
often when developers change an assertion in a test case they do it to updatethe logic behind the test.
for example a pattern that occurredin six different projects is changing from assertequal toassertregex.
this way instead of checking for an exact equality between an object and a string a regularexpression is passed that can support variations in strings.
onecommit message reads use a more permissive comparison for jsonschema.v alidationerror messages .
another common pattern involved changing from assertequal toassertin where instead of one correct result there is a list of values.
conversely anothercommon example is changing from assertisnone to another function assertisinstance.
this makes the check more specific the object is not compared to none but rather needs to be an object of a new specific class.
onecommit message conveys a similar idea nullsort instead of none.
a more descriptive placeholder for don t sort .ii.assertion changes that do not alter the logic and use more appropriate functions.
a large portion of the patterns involved keeping the assertion logic the same but replacing the assertion function witha more appropriate one to make the code succinct.
in total eight such patterns were identified.
these changes are python specific in the sense that they rely heavily on a wide range ofassertion functions that unittest supports.
the most popular pattern is shown in figure .
it occurs in seven different projects and moves from usingasserttrue x in y toassertin x y .
one commit message describes this change in great detail use more specific assertions for in checks.
a lot of old code used asserttrue blah in blah or variants on that which didn t tellyou much if there was a failure.
nowadays we have assertinand assertnotin which we can use instead.
this switches ourtests to use these .
this commit message indicates that the original code before the change can be considered a testsmell since using general assertions can make it difficult toinfer the reason of failure by hiding the actual assertion inits body whereas using specific assertions can make it easier.
another change that strives to remove the ambiguity of a general assertion occurs in four repositories and it moves from using assertfalse x y to assertnotequal x y .
sometimes asserttrue is changed to another specific assertion.
for example in threedifferent projects asserttrue x y is changed to unittest sassertlessequal x y .
one commit message expectedly comments this use more specific asserts in unit tests .
in python it is considered bad practice to check the equality of a boolean value when you can check the value itself so in this case a boolean assertion is more correct andmore interpretable which is reflected in a common changepattern where assertequal x false is changed to assertfalse x .
in this section we have given examples of some commit messages that describe the changes along with the changepattern.
we believe that these commit messages justify con sidering the wrong choice of an assertion function in unittest as a test smell.
we called this smell suboptimal assert .
iii.assertion changes that do not alter the logic and use less appropriate functions.
interestingly we also discovered seven change patterns that move from an appropriate assertion function to a more generalone.
following the logic of the previous section these can betreated as introducing a test smell.
the most popular such change is moving from a more specific assertisnotnone x toa more general assertnotequal x none .
one commit message de scribes this change as a fix in test for python .
compatibility .
however the changes in this pattern were made in and since python is deprecated from thisis no longer a problem.
a similar message described commits in two different projects that moved from assertnotin x y 598i u i i u i uh u u i e u iu i u iu fig.
.
the pipeline of p ynose operation in two regimes gui mode and cli mode.
toasserttrue x not in y and two more different projects that moved from assertless x y toasserttrue x y .
the same changes can be found for functions like assertgreater x y and assertisnone x with one commit message saying remove fancy test assertions that are unavailable on .
.
in total we encountered different suboptimal asserts either fixed introduced or both .
we extrapolated them to similar functions and opposite cases where necessary for example if there is a suboptimal assert that contains assertless it can also be formulated for assertgreater etc.
this resulted in a total of different assertions that can beconsidered a part of the suboptimal assert test smell the full list is available online .
iv .
p ynose once we curated the list of test smells explained in section iii our next goal was to implement a tool to identify themin actual python code.
we developed a tool called p ynose that currently identifies test smells language agnosticfrom the existing literature and one python specific elicited byus as described in section iii b and can be run from boththe graphical user interface and the command line.
figure 3shows the operating pipeline of p ynose .
in this section we explain it in greater details.
a. tool internals pynose is implemented as a plugin for pycharm a popular ide for python developed by jetbrains.
the plugin supports two different modes of operation graphical user interface gui mode and command line interface cli mode.
internally p ynose uses program structure interface psi from jetbrains intellij platform that pycharm isbuilt upon to parse python source code and build syntacticand semantic code models for analysis.
when the project isopened and the interpreter is set up the tool uses psi and otherrelated pycharm api to gather all .py files in the project in the form of psifile objects.
next the tool extracts all python classes that are sub classes of unittest.testcase.
with the help of psi p ynose can deal with importing unittest or unittest.testcase under alias or test cases that are not direct sub classes of unittest.testcase.
after collecting individual test suites each detector class correspond ing to each test smell invokes psielementvisitor to create a custom visitor for the necessary psielement which allows p ynose to identify test smells.
for example for the magic number test we use a custom visitor of pycallexpression to find all assertions and then check if one of the provided arguments is apynumericliteralexpression.
if there is a match themagic number test smell is declared to be found.
for the test smells from the literature we implemented their detection in the same way as they are described in theoriginal papers using the mentioned thresholds.
for example we detect obscure in line setup the same way as greiler et al.
by counting the number of local variables in atest case and flagging the case as smelly if this number islarger than a threshold of and detect lack of cohesion of test cases the same way as palomba et al.
by calculating pairwise cosine similarities between test cases.detection rules for all the supported test smells are presentedin table ii the citations mark the works from where thedetection rules were adapted from.
where necessary we usedcode entities analogous to their counterparts in java forexample unittest.skip decorator in the place of the ignore annotation.
if there were several different heuristics to detect the same smell in different papers we selected onebased on its recency and its convenience to implement usingthe psi and the intellij platfrom.
when the analysis is done p ynose can show the detected test smells inside the ide or save them to a json file forfurther analysis.
b. evaluation we conducted an experimental evaluation of the effectiveness of p ynose in correctly detecting test smells.
as there are no existing datasets containing information for all the supported smells we decided to construct our own validation set.we randomly selected eight projects that did not make it intotheprimary dataset.
we then used the definitions of test smells to identify and tag test files with the information regarding thetypes of smells they exhibit.
this process resulted in a totalof annotated files.
the list of projects together with somestatistics about their testing files is shown in table iii.
toensure an unbiased annotation process three authors of thepaper individually did the labelling and discussed their resultsafterwards to reach a consensus.
all the three authors haveexperience with python development ranging from two to fiveyears which includes exposure to developing unit tests.
next we ran p ynose on the same set of projects and compared our results against the oracle.
we calculated precision recall and f1 score for each test smell.
we also calculated 599table ii the detection rules for supported test smells .c ita tions indica te works where the rules were adopted from .
assertion roulette a test case contains more than one assertion statement without an explanation message.
conditional test logic a test case contains one or more control statements i.e.
if for while .
constructor initialization a test suite contains a constructor declaration an init method .
default test a test suite is called mytestcase.
duplicate assert a test case contains more than one assertion statement with the same parameters.
empty test a test case does not contain a single executable statement.
exception handling a test case contains either the try except statement or the raise statement.
general fixture not all fields instantiated within the setup method of a test suite are utilized by all test cases in this test suite.
ignored test a test case contains the unittest.skip decorator.
lack of cohesion of test cases the mean of the pairwise cosine similarities between test cases in a test suite .
.
magic number test a test case contains an assertion statement that contains a numeric literal as an argument.
obscure in line setup a test case contains ten or more local variables declarations.
redundant assertion a test case contains an assertion statement in which the expected and actual parameters of equality are the same e.g.
assertequal x x or the assertion of truth is carried out on the unchangeable object e.g.
asserttrue true .
redundant print a test case invokes the print function.
sleepy test a test case invokes the time.sleep function with no comment.
suboptimal assert a test case contains at least one of the suboptimal asserts.
test maverick a test suite contains at least one test case that does not use a single field from the setup method.
unknown test a test case does not contain a single assertion statement.
table iii eight projects selected for the ev alua tion of pynose .the columns indica te the number of testing files suites and cases with unittest .
project t. files t. suites t. cases ali1234 vhs teletext cea sec ivre davidhalter jedi demisto content justiniso polling lagg steamodd plamere spotipy pygridtools drmaa python total the weighted average of these three metrics for all test smells with the weights being the number of instances of each testsmell in the projects.
the results of the conducted evaluationare presented in table iv.
several test smells were encountered very rarely in the validation projects with three of them having only a singleexample.
this has to do with the fact that these test smells arejust rare in python in general see section v c2 .
however these test smells have very robust definitions that are easyto detect default test requires the tool to simply check the name of the test suite constructor initialization requires the tool to simply check the presence of an init method and sleepy test simply looks for the sleep function in the body of the test case.
as shown in table iv p ynose achieves a high level of correctness with f1 scores ranging from .
to fordifferent test smells.
for the cases where the tool did notachieve we investigated the mismatch.
in one instance assertion roulette was not detected because of a non conventional name of the test case where the namestarted with a symbol instead of the word test as is the convention.
a human rater could tag such a test case astable iv the results of the ev alua tion .inst .stands for instances and indica tes a true number of test suites with a given smell in the v alida tion da taset .
test smell inst.
precision recall f1 assertion roulette .
.
conditional test logic .
constructor initialization default test duplicate assertion empty test exception handling general fixture ignored test lack of cohesion .
.
.
magic number test .
.
obscure inline setup redundant assertion redundant print sleepy test suboptimal assert test maverick unknown test .
.
weighted average .
.
.
having the assertion roulette test smell however p ynose failed to do so.
p ynose also incorrectly identified several conditional test logic test smells.
conditional test logic is detected by the presence of control statements i.e.
if for etc.
irrespective of their impact on the assertion.
for example the for statement can be used simply to assign a variable and such cases are incorrectly tagged as conditional test logic by p ynose .lack of cohesion relies on the cohesiveness of test cases in a test suite.
p ynose measures cohesiveness using cosine similarity whereas human ratersused their subjective judgement which resulted in a mismatchbetween the output of p ynose and the opinion of the human raters in several cases.
several magic number tests were not detected because the comparison to a literal occurred in 600assertions with complex parameters that are not yet supported.
for example assertequal df.shape was tagged as a magic number test test smell by a human rater however p ynose failed to do so because the literal is located in a tuple.
finally two cases of unknown test turned out to be false positives.
the tool considered the test case to not haveassertions when in reality an assertion was present but it wasfrom the unsupported pytest framework.
for all test smells together p ynose achieves the precision of and the recall of .
.
table v shows the comparisonbetween the obtained values and the reported numbers of tsdetect a similar tool for java.
it can be seen that the values are similar however we plan to conduct a morethorough and direct comparison of tools in the future.
table v the comparison of performance between pynose and ts detect .
detector language precision recall f1 tsdetect java .
.
.
pynose python .
.
.
v. p rev alence of test smells after developing and validating p ynose we conducted an empirical study on test smell prevalence in open source pythonprojects.
in this section we present the details and the resultsof this study.
a. selecting projects to analyze the goal of our study was to analyze test smell prevalence in python projects using p ynose .
this was done to increase the subject diversity among the existing empirical studies on test smells as well as to gain an understanding of how testsmells are diffused in python code.
we decided to study thepresence of test smells in the same primary dataset that was used for mining code change patterns.
we decided to do sobecause the primary dataset represents mature open source python projects that use testing within them.
however to make sure that the results of the study are robust and do not depend on the results from section iii b3 we decided to also run the tool on an additional dataset.to gather it we used the same procedure as described insection iii b1 but with one condition being slightly relaxed we gathered projects with the number of commits between 500and instead of at least commits.
this resulted in239 additional projects the full list is available online .
wewill refer to this dataset as the secondary dataset.
while we draw our general conclusions from the primary dataset since it contains more projects with larger histories the purpose ofthesecondary dataset is to make sure that the reported results are unbiased.
b. methodology we ran p ynose on all the projects in the primary and secondary datasets separately.
we dropped the results where not a single test suite was found and only considered testsuites with at least one test case and test files with at least one test suite.
test smells can occur on various levels ofgranularity constructor initialization default test general fixture and lack of cohesion manifest at the level of a test suite as a whole while other test smells such as conditional test logic are formulated at the test case level.
we analyzed the test smells using their appropriate granularity.
a test suite is considered smelly if it contains at least onetest case with a given smell.
a test file can also be considereda valid object for comparison however even though in pythonand in unittest it is possible to have several test suites in one test file this granularity is still largely similar to a test suite and often a test file contains just one or two test suites.
we alsocalculated the distribution of test smells among projects to geta more coarse grained picture of the test smells prevalence.
we studied the most common and the least common test smells as well as the prevalence of the newly proposedsuboptimal assert .
additionally we studied the co occurrence of different test smells in individual test suites and discussedthe correlations between test smells.
c. results in this section we discuss the results of the empirical study of the test smells prevalence in python code.
general information in total at least one unittest test case was found in projects out of the in the primary dataset .
.
from here on out all percentages are calculated based on these projects.
in total in these 248projects p ynose detected test files test suites and test cases.
more detailed statistics are presentedin table vi.
it can be seen from the table that even matureprojects vary greatly by the amount of testing within them.
inour dataset one test file on average had .
test suites andone test suite on average had .
test cases.
table vi the summary of the amount of testing entities per project .
test files test suites test cases minimum mean .
.
.
maximum test smells distribution the distribution of detected test smells is presented in figure .
in general it can be seenthat the studied test smells are prevalent in python code.
thereare only projects that have no smells however all ofthem are very small projects with the largest having only 13test cases.
all the other projects have tests smells inone way or another.
test smells such as assertion roulette and conditional test logic are among the most common test smells and occur in almost of projects that use unittest in theprimary dataset.
also among the most popular test smells aremagic number test general fixture and unknown test.
on the other end of the spectrum we can see test smells that rarely occur in python code.
empty test occurs in just .
of the test suites although interestingly even these instances .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
default testconstructor initializationempty testredundant assertionsleepy testredundant printignored testobscure inline setupexception handlinglack of cohesiontest mavericksuboptimal assertduplicate assertionunknown testgeneral fixturemagic number testconditional test logicassertion roulette of test suites8.
.
.
default testconstructor initializationempty testredundant assertionsleepy testredundant printignored testobscure inline setupexception handlinglack of cohesiontest mavericksuboptimal assertduplicate assertionunknown testgeneral fixturemagic number testconditional test logicassertion roulette of projects primary dataset secondary dataset primary dataset secondary dataset fig.
.
the prevalence of different test smells among all projects and test suites in primary and secondary datasets.
the percentages relate to projects that use unittest the numbers near bars are shown for the primary dataset.
are spread out among as much as .
of the projects.
constructor initialization occurs in .
of the projects and .
of the test suites.
finally the rarest of all test smells thatoccurs in only two projects and only three test suites withinthem is default test.
figure also shows that our introduced suboptimal assert smell constitutes an important addition to python test smells it occurs at least once in .
of theprojects and .
of the test suites.
in comparison with previous works that study java and android code it can be said that the lists of the most populartest smells generally look similar.
it seems that python codehas larger percentages by projects however direct comparisonhere should be carried out in future work.
one specific testsmell that seems to be less prevalent in python is exception handling that occurs in .
of the projects and only in .
of the test suites.
unittest supports a convenient list of assertions like assertraises assertraisesregex and others that may prevent the users from using try except keywords in tests.
it can also be seen that the results for the primary dataset and the secondary dataset are similar to each other with the only noticeable exception being the obscure in line setup which is rarer in the secondary dataset.
the values for suboptimal assert are also similar.
this demonstrates that our results obtained for the primary dataset are unbiased.
overall our results show that various test smells are prevalent in python code even some of the rarer ones still occur inmore than a quarter of all projects.
while some of them can beconsidered more subjective others make it significantly harderto maintain the code base and to interpret the results of testingin case of failure.
we hope that in the future p ynose can be used to help developers and researchers to combat the spreadof test smells in their repositories.
co occurrence of test smells in the previous section we discussed how prevalent different test smells are.
however such an approach considers test smells independently from0123456789 of test suites number of different t est smells in a test suite fig.
.
the distribution of the number of different test smells among individual test suites.
each other and does not fully describe the actual smelliness of code.
to get a better understanding we also analyzed theco occurrence of test smells.
figure shows the distribution of how many different smells co exist within individual test suites.
it can be seen that only16 of all test suites are free from smells.
the remaining of the test suites have at least one smell .
have exactlyone smell .
have two smells .
have three smells and this number gradually decreases with the amount of co occurring test smells.
the highest occurring number in theprimary dataset appears in a single test suite with distinct test smells.
this large test suite with test cases in additionto all the most popular test smells contains commented outempty test cases catching errors with try except instead of using specific assertions of error messages and sleepytests.
it also uses assertequal x true instead of asserttrue x .
we believe that helping developers findsuch suites might be useful for the maintenance of the project.
figure also sheds a new light on the prevalence of test smells in python code.
with more than half of all test suites 602having two different test smells or more their effect on the maintainability of code can become more complex.
we also additionally studied the co occurrence of specific pairs of test smells.
for all pairs of test smells we calculatedthe following value what percentage of test suites that havetest smell x also have test smell y .
two pairs of test smells arecompletely connected.
firstly if the test is empty i.e.
contains no executable statements it is automatically unknown i.e.
has no direct assertions .
secondly if there is a test maverick in a test suite this test suite automatically has a general fixture.
test maverick occurs when the test suite has a setup method with fields and the given test case does not use anyof the fields in it.
of course this automatically means thatthere is at least one method that does not use all of the fields which is the definition of a general fixture.
other strongly connected pairs are all associated with assertion roulette due to its popularity.
if a test suite has a duplicate assert it has anassertion roulette in .
of the cases.
it might not be the case if the duplication has explicit messages becauseassertion roulette is only considered if assertions have no messages but since it is very common to not write errormessages duplicated assertions can become a roulette.
thesame goes for redundant assertion .
of the test suites with which also have an assertion roulette.
this also makes sense because if there is a redundant assertion there probablyshould be some other assertion that is more meaningful.
this co occurrence of test smells demonstrates that test smells have relationship with one another that should beexplored in greater detail in the future.
vi.
t hrea ts to validity while we structured our study to avoid introducing bias and worked to eliminate the effects of random noise it is possiblethat our mitigation strategies may not have been effective.
thissection reviews the threats to validity to our study.
it is possible that during the systematic mapping study of test smells we missed some test smells that are applicableto python.
also python grammar is rather large and isbeing actively updated so p ython change miner does not support all python language constructs and it is possible thatwe may have missed potential test smell changes becauseof this.
we also relied on pattern detection thresholds fromthe original paper by nguyen et al.
while it is possiblethat they could be different for python and for testing code.however the tool supports all the main features of pythonand still produced a large number of code change patterns.
inaddition p ynose is built in such a way that it is simple to add new test smells in the future.
the results of both parts of our study searching for python specific test smells and analyzing the prevalence oftest smells in python code rely on a specific set of open source projects that we selected and might not generalize toall projects including proprietary ones.
however we analyzedtwo moderately large datasets primary and secondary for our tasks that were curated using various conditions suggested inthe literature.
we believe that the similarity of results fromboth datasets demonstrates the reproducibility of the resultsof the empirical study.
it is possible for p ynose to have some unnoticed errors in its implementation.
however we tested the tool rigorously onsynthetic data and performed manual evaluation on real worlddata to minimize the risk as much as possible.
one threat to validity is related to the detection of specific test smells.
some of the implementations of test smells relyon specific thresholds that were picked from the literature.
itis possible that these thresholds are different for python andthis requires further study.
vii.
c onclusions and future work test smells are prevalent in commonly used programming languages such as java and have a detrimental effect not onlyon the quality of test code but also on the production code .
in this work we presented p ynose the first tool for test smell detection in python code that is capable of identifying18 test smells.
out of these test smells were adaptedfrom test smells for other programming languages describedin the literature and we added one test smell called suboptimal assert by analyzing the most frequent changes made to test files in open source python projects.
experiments ona set of eight real world projects showed that p ynose is capable of detecting test smells with precision and .
recall which is on par with other publicly available tools fortest smell detection.
our empirical analysis shows that testsmells are prevalent in python code with of the projectsand of the test suites having at least one test smell inthem.
the most frequent detected test smells were assertion roulette conditional test logic and magic number test.w e also observed that the proposed python specific suboptimal assert smell occurs in the code rather often being present in as much as .
of the projects.
future research directions for this work include supporting more test smells including those that rely onproduction code.
discovering more python specific smells which requiresa specific analysis of the optimal pattern searchingparameters for python.
conducting a more thorough comparison of p ynose to other tools for example to tsdetect that works with java.
it would also be of interest to employ thetools together to carry out a comparison of large pythonand java datasets from the standpoint of test smelldistribution.
analyzing test smell prevalence in python on a largerdataset of projects and in other dimensions for example it would be of great interest to see how test smellscorrelate with test coverage .
p ynose is available on github for use in the ide and for research the research artifacts of this study are also publicly avail able 603references m. fowler and k. beck refactoring improving the design of existing code.
addison wesley professional .
i. deligiannis m. shepperd m. roumeliotis and i. stamelos an empirical investigation of an object oriented design heuristic for maintainability journal of systems and software vol.
no.
pp.
.
w. li and r. shatnawi an empirical study of the bad smells and class error probability in the post release object oriented system evolution journal of systems and software vol.
no.
pp.
.
g. a. oliva i. steinmacher i. wiese and m. a. gerosa what can commit metadata tell us about design degradation?
in proceedings of the international workshop on principles of software evolution.acm pp.
.
s. olbrich d. s. cruzes v .
basili and n. zazworka the evolution and impact of code smells a case study of two open source systems inproceedings of the 3rd international symposium on empirical software engineering and measurement.
ieee computer society pp.
.
t. hall m. zhang d. bowes and y .
sun some code smells have a significant but small effect on faults acm transactions on software engineering and methodology tosem vol.
no.
p. .
n. zazworka m. a. shaw f. shull and c. seaman investigating the impact of design debt on software quality in proceedings of the 2nd workshop on managing technical debt.
acm pp.
.
m. tufano f. palomba g. bavota r. oliveto m. di penta a. de lucia and d. poshyvanyk when and why your code starts to smell bad and whether the smells go away ieee transactions on software engineering vol.
no.
pp.
.
a. v an deursen l. moonen a. v an den bergh and g. kok refactoring test code in proceedings of the 2nd international conference on extreme programming and flexible processes in software engineering xp2001 .
citeseer pp.
.
g. bavota a. qusef r. oliveto a. de lucia and d. binkley an empirical analysis of the distribution of unit test smells and their impacton software maintenance in 28th ieee international conference on software maintenance icsm .
ieee pp.
.
d. spadini f. palomba a. zaidman m. bruntink and a. bacchelli on the relation of test smells to software code quality in ieee international conference on software maintenance and evolution icsme .
ieee pp.
.
g. bavota a. qusef r. oliveto a. de lucia and d. binkley are test smells really harmful?
an empirical study empirical software engineering vol.
no.
pp.
.
m. tufano f. palomba g. bavota m. di penta r. oliveto a. de lucia and d. poshyvanyk an empirical investigation into the nature of testsmells in proceedings of the 31st ieee acm international conference on automated software engineering pp.
.
m. greiler a. v an deursen and m. a. storey automated detection of test fixture strategies and smells in ieee sixth international conference on software testing v erification and v alidation.
ieee pp.
.
j. de bleser d. di nucci and c. de roover assessing diffusion and perception of test smells in scala projects in ieee acm 16th international conference on mining software repositories msr .
ieee pp.
.
s. raschka j. patterson and c. nolet machine learning in python main developments and technology trends in data science machinelearning and artificial intelligence information vol.
no.
p. .
d. sarkar r. bali and t. sharma practical machine learning with python a problem solvers guide to building real world intelligent systems.
berkely apress .
y .
golubev j. li v .
bushev t. bryksin and i. ahmed changes from the trenches should we automate them?
arxiv preprint arxiv .
.
pycharm.
accessed .
.
the python ide for professional developers.
.
available k. reitz and t. schlusser the hitchhiker s guide to python best practices for development.
o reilly media inc. .
g. meszaros xunit test patterns refactoring test code .
pearson education .
b. v an rompaey b. du bois and s. demeyer characterizing the relative significance of a test smell in 22nd ieee international conference on software maintenance.
ieee pp.
.
m. breugelmans and b. v an rompaey testq exploring structural and maintenance characteristics of unit test suites in wasdett 1st international workshop on advanced software development tools andtechniques.
citeseer .
j. de bleser d. di nucci and c. de roover socrates scala radar for test smells in proceedings of the tenth acm sigplan symposium on scala pp.
.
a. peruma k. almalki c. d. newman m. w. mkaouer a. ouni and f. palomba on the distribution of test smells in open sourceandroid applications an exploratory study in proceedings of the 29th annual international conference on computer science and softwareengineering pp.
.
t. virg nio r. santana l. a. martins l. r. soares h. costa and i. machado on the influence of test smells on test coverage in proceedings of the xxxiii brazilian symposium on software engineering pp.
.
b. v an rompaey b. du bois s. demeyer and m. rieger on the detection of test smells a metrics based approach for general fixtureand eager test ieee transactions on software engineering vol.
no.
pp.
.
f. palomba a. zaidman and a. de lucia automatic test smell detection using information retrieval techniques in ieee international conference on software maintenance and evolution icsme .
ieee pp.
.
a. peruma k. almalki c. d. newman m. w. mkaouer a. ouni and f. palomba tsdetect an open source test smells detection tool inproceedings of the 28th acm joint meeting on european software engineering conference and symposium on the foundations of softwareengineering pp.
.
s. lambiase a. cupito f. pecorelli a. de lucia and f. palomba just in time test smell detection and refactoring the darts project inproceedings of the 28th international conference on program compre hension pp.
.
r. santana l. martins l. rocha t. virg nio a. cruz h. costa and i. machado raide a tool for assertion roulette and duplicate assertidentification and refactoring in proceedings of the 34th brazilian symposium on software engineering pp.
.
t. virg nio l. martins l. rocha r. santana a. cruz h. costa and i. machado jnose java test smell detector in proceedings of the 34th brazilian symposium on software engineering pp.
.
b. a. kitchenham d. budgen and p .
brereton evidence based software engineering and systematic reviews.
crc press vol.
.
c. wohlin guidelines for snowballing in systematic literature studies and a replication in software engineering in proceedings of the 18th international conference on evaluation and assessment in softwareengineering pp.
.
s. keele et al.
guidelines for performing systematic literature reviews in software engineering technical report v er.
.
ebse technicalreport.
ebse tech.
rep. .
pynose.
accessed .
.
supplementary materials for this paper.
.
available p .
documentation.
accessed .
.
file and directory access.
.
available a. kicsi l. t oth and l. vid acs exploring the benefits of utilizing conceptual information in test to code traceability in ieee acm 6th international workshop on realizing artificial intelligence synergiesin software engineering raise .
ieee pp.
.
a. kicsi l. vid acs and t. gyim othy testroutes a manually curated method level dataset for test to code traceability in proceedings of the 17th international conference on mining software repositories pp.
.
m. ghafari c. ghezzi and k. rubinov automatically identifying focal methods under test in unit test cases in ieee 15th international working conference on source code analysis and manipulation scam .
ieee pp.
.
unittest.
accessed .
.
unit testing framework for python.
.
available pytest.
accessed .
.
testing framework for python.
.
available r. framework.
accessed .
.
testing framework for python.
.
available g. l. turnquist python testing cookbook.
packt publishing ltd .
g. gousios the ghtorrent dataset and tool suite in proceedings of the 10th working conference on mining software repositories ser.
msr .
piscataway nj usa ieee press pp.
.
.
available g. dumps.
accessed .
.
ghtorrent archive sql dumps.
.
available p .
create.
accessed .
.
a tool to create the pga dataset.
.
available v .
markovtsev and w. long public git archive a big code dataset for all in proceedings of the 15th international conference on mining software repositories pp.
.
e. kalliamvakou g. gousios k. blincoe l. singer d. m. german and d. damian the promises and perils of mining github pp.
.
h. a. nguyen t. n. nguyen d. dig s. nguyen h. tran and m. hilton graph based mining of in the wild fine grained semantic code changepatterns in ieee acm 41st international conference on software engineering icse pp.
.
j. falleri f. morandat x. blanc m. martinez and m. monperrus fine grained and accurate source code differencing in acm ieee international conference on automated software engineering ase v asteras sweden september pp.
.
c. in obspy.
accessed .
.
a diff in the obspy project.
.
available c. in numba.
accessed .
.
a diff in the numba project.
.
available c. in reviewboard.
accessed .
.
a commit message in the reviewboard project.
.
available c. in girder.
accessed .
.
a commit message in the girder project.
.
available c. in beets.
accessed .
.
a commit message in the beets project.
.
available c. in python chess.
accessed .
.
a commit message in the python chess project.
.
available c. in gensim.
accessed .
.
a commit message in the gensim project.
.
available c. in requests.
accessed .
.
a commit message in the requests project.
.
available p .
s. interface.
accessed .
.
program structure interface in intellij platform.
.
available