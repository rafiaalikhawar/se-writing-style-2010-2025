evaluating unit testing practices in r packages melina vidoni rmit university school of computing technologies melbourne australia melina.vidoni rmit.edu.au abstract testing technical debt ttd occurs due to shortcuts non optimal decisions taken about testing it is the test dimension of technical debt.
r is a package based programming ecosystem that provides an easy way to install third party code datasets tests documentation and examples.
this structure makes it especially vulnerable to ttd because errors present in a package can transitively affect all packages and scripts that depend on it.
thus ttd can effectively become a threat to the validity of all analysis written in r that rely on potentially faulty code.
this two part study provides the first analysis in this area.
first systematically selected open source r packages were mined and analysed to address quality of testing testing goals and identify potential ttd sources.
second a survey addressed how r package developers perceive testing and face its challenges response rate of .
.
results show that testing in r packages is of low quality the most common smells are inadequate and obscure unit testing improper asserts inexperienced testers and improper test design.
furthermore skilled r developers still face challenges such as time constraints emphasis on development rather than testing poor tool documentation and a steep learning curve.
i. i ntroduction there are many popular languages tools and environments for statistical computing.
r and its accompanying software environment is a robust open source competitor regardless of how popularity is measured .
r is a packagebased programming ecosystem and provides an easy way to install third party code datasets tests documentation and examples .
the main r distribution installs a few base and recommended packages.
though cran comprehensive r archive network 1distributes r packages many are developed on public version control repositories.
technical debt td is a metaphor used to describe the situation where long term software code quality is traded for a quick short term solution it incurs in future costs of maintenance adaptability and revisions .
it is a metaphor used to reflect the implied cost of additional rework caused by choosing an easy solution in the present instead of using a better approach that would take longer .
testing technical debt ttd occurs due to shortcuts non optimal decisions related to testing and it is the test dimension of td .
this is because not performing unit testing or writing incomplete tests can speed up development at the cost of lowering the quality of the software and increasing the number of faults.
addressing the problem of inadequate software testing requires defining what this means.
since r is a package based programming language errors present in one can transitively all packages and scripts that depend on it effectively becoming a threat to their validity of the analysis written after that.
therefore it is essential to thoroughly test r packages before releasing them.
common r packages for unit testing r code rely on concepts developed for object oriented oo programming .
however r is a dynamic language that combines lazy functional features with minimal oo features .
this combination of paradigms can set it apart from the current oo focused knowledge of ttd.
although this problem has been explored in other domains it has not been addressed in r so far.
despite the growth of the r community there has been to the author s knowledge no research assessing how developers test these packages in practice.
this study is needed to understand how unit testing is approached what are the ttd hot spots in r packages and what challenges developers face.
this manuscript addresses this need through a two part empirical study.
the first part analysed systematicallyselected open source r packages hosted on github the dataset included both popular and newer packages maintained since .
both automated and manual analysis were conducted to address quality of testing to define what is being tested and to identify harmful practices that increase ttd in r packages.
the second part of the study addressed how r developers perceive unit testing it involved contacting the developers of the packages selected before to ask them to complete an anonymous survey.
the survey was sent to developers and received responses response rate of .
.
the main contributions of this study are as follows this is the first study conducted to understand unit testing culture in the r packages development community.
it analyses the extent to which r packages are tested by mining open public r packages repositories hosted in github by performing a number of analysis.
about testing practices findings indicate that few alternative paths are tested and test suits seem to be irrelevant since bugs are often found when tests are passing.
developers lack in depth training expected due to the absence of technical programming background .
moreover there is a striking amount of manual testing potentially due to the statistical nature of r packages and the lack of statistically centred unit testing.
regarding existing testing tools results show that r s unit testing tools may be incomplete lacking assertion functionalities and automatic data initialisation e.g.
junit s beforeall .
ieee acm 43rd international conference on software engineering icse .
ieee it surveys many developers to understand their perspective in testing tools and challenges faced by them when testing their r packages.
this paper is structured as follows.
section ii explains the study design including research questions data collection and survey design.
results are presented in section iii.
after that section iv discusses additional interesting points and describes threats to the validity of the previous sections.
finally section v concludes the paper and mentions future lines of work.
a. related works studies related to r programming practices are scarce.
decan et al.
explored how the use of github influences the r ecosystem both for the distribution of r packages and inter repository package dependency management while ramirez et al.
studied their maintenance to explore change frequency and variability between versions.
furthermore morandat et al.
assessed the success of different r features to evaluate the fundamental choices behind the language design.
furthermore two part studies combining of mining software repositories msr and developer surveys are present in different areas of software engineering research.
for example it has been used to assess the popularity of open source repositories in github stars and watches as well as to evaluate the reasoning behind forking repositories .
in terms of testing it was used to analyse code coverage visualisations in closed source software but also to determine ttd in scala projects and identify potential testing smells .
other authors used it to assess the quality in terms of smells of external contributions in github projects .
finally k rikava and vitek did study unit tests in r packages but with different research questions.
they conducted an msr to inspect r packages source code and propose a tool that automatically generates unit tests.
they reported the implementation and empirical evaluation of the proposed tool.
however they did not focus on other topics related to unit testing cultures such as technical debt constraints or the challenges faced by developers.
ii.
s tudy design this section presents the design of the study.
this includes research questions investigated in this study how data wascollected through an msr approach survey design and distribution.
the methodology used in this manuscript and described in the sections below was approved by rmit university human ethics research committee hrec with project code .
a. research questions the goal of this study is to understand how r packages are tested in order to define best testing practices in this language as well as to identify testing td hot spots.
this leads to the following research questions rqs rq1.
are r packages well tested?
to understand which testing tools are used in r packages identify common practices types of testing and how unit testing tailors to a multi paradigm language like r. rq2.
which are potential testing td weak spots?
to discover and understand negative practices that affect unit testing in r packages.
the long term goal of this is to identify testing smells.
rq3.
how do r package developers perceive unit testing?
part of the msr involved collecting public email addresses of developers disclosed in packages files to send them a structured survey.
questions aimed to understand their subjective perception of testing and the challenges they face.
b. mining r packages part i the first part of the study addresses rq1 and rq2 by mining open source r package repositories hosted in github.
the following inclusion and exclusion criteria were used to search repositories inclusion criteria the repository must be an r package originally posted during or after it needs to show maintenance activity commits in the last two years i.e.
from .
it must have a correct package structure with all dependencies available.
exclusion criteria the repository is an r data package a book or a personal package.
the state of the repository is archived deprecated or outdated.
it is an r package with scripts used in a book.
it has incomplete or missing files i.e.
description namespace or readme files .
it is a fork from another r package.
the gathering followed a systematic selection process summarised in figure .
github s advanced search was used fig.
.
repository selection filtering steps.
1524with the keywords language r package and configured to automatically filter results by the date of the last push as well as by words specifically deprecated outdated archived personal and fork status.
this search produced about packages and were selected for this process at the initial selection step.
this number was a threshold defined by considering how many repositories were used on other msr papers with mostly manual analysis.
for the first screening github metadata was used to exclude repositories not automatically filtered.
the folder structure in the main branch was used to determine a correct package structure.
remaining packages were listed in a text file using their slugs i.e.
username repositoryname .
this was used in the following phase.
the second screening implied automatically downloading the repositories listed in the entry text file running basic tests to determine suitability according to the remaining aspects of the inclusion exclusion criteria.
this was done using covr2andtestthat3.
for each package this required downloading the source from github installing it along with all dependencies running covr to obtain a basic analysis of the tests and devtools test to run all unit testings.
this automated double checking ensured an analysis of the package s integrity but also their test suitability while discarding possible errors in the cloning.
after this process packages remained after all selection steps .
finally the repositories were further filtered according to the results obtained from this analysis table i provides an overview with the number of packages removed at each stage.
table i second screening filtering results with c o v r and t e s t h a t results .
covr testthat result number yes yes both analysis run correctly yes failed covr runs but there are issues with testthat18 na na analysis are unable to run.
empty test structure or manual test cases45 na manual error yes filtered.
covr is unable to complete the analysis.
testthat provides mixed results19 error failed error na in the above table failed means all tests included in the package are unit tests but most of them failed and produced an early stop nais an r specific value that indicates missing results i.e.
tests are not available .
moreover covr has a known bug with dependencies when installing packages in a specific directory4.
though in one case packages had a correct unit testing structure table i row it was not possible to analyse them.
this will be further discussed when addressing the threats to the validity of this study see section iv d .
finally a manual backward check see figure was performed to evaluate repositories discarded by the automated 4see this step ensured that both analyses were running correctly.
no repositories were added after this.
c. developers survey part ii for this part of the study an anonymous online survey was prepared.
table ii details the questions and answers included in it.
this survey was implemented in qualtrics and used its services to distribute it.
email related information was removed to ensure the anonymity of respondents.
questions labelled with indicate a single choice answer.
table ii structure of the survey generated for part ii.
question possible answers how many r packages have you authored?
regardless if they are in cran bioconductor or not packages how many years of experience do you have as an r programmer?
years years how do you test your code?
manually i don t test using testing packages what type of testing do you do?
individual functions only functions clusters using my package externally other if you use testing packages what are the names of them?comment box why do you use testing packages?
generating or executing test cases creating and evaluating results analysing code coverage finding bugs reporting bugs fulfilling cran requirements.
do you face the following challenges during testing?
and if you do how serious are they?likert scale what are the top two things you look for need would like to see?comment box do you use coverage visualisation tools?
always occasionally never name the coverage visualisation tools that you use.comment box does coverage visualisation affect you?
it motivates me it makes me anxious it makes me confident in my code i trust my code is bugfree other did you ever have all tests passing but found a bug in your code?
yes at least once yes more than one time i don t remember never the question regarding challenges included the following options time constraints compatibility issues lack of exposure to tools emphasis on development rather than testing lack of support from their organisation unclear testing benefits poor documentation lack of experience and steep learning curve.
regarding distribution r packages list information about their maintainers in the description file including their email address these emails are publicly shown in cran if the package is available there as well as in the ides integrated development environment upon installation.
because of this an r script was used to extract this information and remove the duplicates obtaining email addresses.
qualtrics distribution tools were used to send the survey to potential candidates.
overall emails bounced and replies were collected response rate of .
.
1525it is worth highlighting that this methodology underwent a thorough ethical review and was approved see section ii .
to preserve the identity of those contacted for the survey as well as avoiding a potential re identification of the responses the names of the packages explored in this manuscript see section ii b are not shared.
as a result the datasets used are not made public.
iii.
r esults this section reports the result of analysing the testing and coverage of systematically selected open source r packages as well as the survey responses.
results are presented by rq to be addressed.
it is worth noticing that during the filtering phase several repositories were excluded due to testing quality see table i .
first repositories were excluded because they had manual testing only.
this implied having correct package structure and testing folders using testthat folder hierarchy but without including asserts in test methods.
thus the tests were manual and packages were incapable of running an automated coverage report.
second repositories had testthat folders with a main testthat.r file but did not include any tests.
as a result r packages were filtered because of lowquality testing by not using automated unit testing frameworks or having incomplete folders that disabled automated checks.
though the following analysis was completed with the repositories that did have automated unit testing the reason to filter these packages is indicative of low testing quality rq1 and two crucial testing smells rq2 lack of unit testing and manual testing.
a. testing quality and smells part i rq1 rq2 the following subsections discuss all the analysis conducted over the packages to answer rq1 and rq2.
the findings and implications relevant to r packages are later summarised on sections iv a and iv b. testing coverage code coverage analysis reveals the areas not exercised by a set of test cases .
there is no proven direct relation that a complete test coverage guarantees a defect free code .
furthermore focusing only on test coverage instead of logic being tested is also a source of ttd .
nonetheless coverage remains a useful tool when analysing testing quality.
this first analysis used the total coverage calculation given bycovr and additionally classified repositories by discipline and type.
this was done by reading their readme.md and description files.
the types identify what tools or algorithms are provided by the package.
figure summarises the averages.
the mean total coverage is .
with the largest distribution of packages having values between and .
overall the coverage of bioinformatics packages varies dramatically by type and software oriented packages which are just tools or data parsing are below the mean.
this can potentially indicate fig.
.
average coverage in selected repositories by discipline and type.
that few methods are thoroughly tested.
relevancy vs tested lines selected packages included lines of code loc of which about were relevant i.e.
r code without comments empty lines or similar .
covr was used to evaluate the proportion of relevant lines that were tested see table iii .
overall only of relevant lines are tested.
to explain these results two in depth analysis paths were followed.
their goal was to understand a what is written in the untested relevant lines and b what proportion of those lines belongs to non exported methods the latter can be thought as the r equivalent of oo s private methods.
this is discussed in the following subsections.
table iii tested and untested relevant lines in selected repositories .
irrelevant lines relevant lines tested lines untested lines total untested lines analysis a sub study was designed to analyse what is written in the untested relevant locs and if those lines belonged to exported or non exported methods.
r s non exported functions are equivalent to oo s private functions as they can only be used internally by the package.
not testing private functions is a conflicting topic discussed among practitioners it often implies a violation of the single responsibility principle requiring the use of techniques such as reflection .
in this line of thought private methods are not testing directly but indirectly through the methods that call them it is not that they are not tested rather than they are tested indirectly .
the analysis of this manuscript adopts this position recommending an indirect test of private nonexported methods or functions.
there are two ways for exporting methods in r i using 1526roxygen2 export annotation5 or ii labelling as notexported all functions whose name starts with a given syntax pattern.
though the former is widely accepted and writes a list of exported functions in the package s namespace file the latter is also common.
a custom r script mined the namespace file of all packages to obtain the names of exported functions covering option i .
to account for functions exported with ii the list was revisited in a second step automatically adding functions exported by name pattern.
this sub study required classifying locs in categories according to their goal returns stops breaks and similar lines that stop a behaviour and go back in the callstack alternatives optional alternatives or switch blocks logs lines that printed information such as warnings errors logs loops r s one liner loops such as lapply do.call and similar wrangling lines that allow reorganising data and online operations such as getting api tokens and similar .
for the repositories the dataset of untested relevant lines consisted of cases.
it was not possible to analyse this automatically since there was no pre classified data and manually sorting the whole dataset was infeasible due to time constraints.
for this reason a representative sample of untested relevant locs urloc was manually classified.
a conservative sample size calculation was performed by selecting from categories with a margin of error of percent at confidence using the approach of thompson .
based on this calculation a random subset of urlocs were sampled from the and then manually classified by looking at the source code in the specific line.
hence the results generated from this sub sampling are considered at least to some extent generalisable.
both datasets manual classification in types and list of exported functions by package were crossed to obtain 5roxygen2 is a package that provides r with functionalities similar to those of javadoc mixed with behaviour from java annotations fig.
.
tested and untested relevant lines in selected repositories.the proportion of exported and non exported urlocs by type figure .
overall .
of urlocs belong to non exported functions.
the most representative group are specific alternatives .
this is quite negative for ttd as it highlights a smell since not all paths are appropriately tested.
other relevant groups are variable preparation and wrangling these represent an essential operation for data science as organising and cleaning data is a critical activity .
such a high number of urlocs in these groups points to another ttd smell lowering the testing quality.
informative asserts assertions are critical elements for unit testing as they evaluate whether a predicate is true or false it compares an expected result to the actual value obtained by the method under testing .
an assert passes when expected and actual are equal.
though not passing asserts symbolise bugs the opposite does not guarantee a defect free system .
asserts are positioned inside of test methods .
furthermore most unit testing tools offer the ability to print a custom message in assertions test methods or both.
confusing ill worded messages are a source of ttd and identified as a testing smell .
r s most common unit testing package testthat locates messages in the test method signature.
an r script preclassified the signatures determining that of them had a written text message while less than .
of test methods stored the messages in a variable.
regarding the former another script counted the length in words and determined that half of them used between and words per message.
nonetheless a second manual sub study was conducted to analyse the clarity of those messages subjectively.
once again given the size of the complete dataset a sub sample was calculated and extracted.
this used the same procedure detailed in section iii a3 also following thompson .
due to this results are also considered somewhat generalisable.
fig.
.
clarity and understandability of messages in test methods.
clarity refers to the use of natural language semantics while understandability concerns grasping what it is being tested 1527fig.
.
proportion of repositories categorised by range of fpr tpf and apt.
for ten categories a confidence level of and a margin of error of percent another sample of test method messages were randomly extracted from the complete dataset.
the ten categories are represented in two likert scales that indicate clarity in terms of natural language semantics and understandability recognising what is being tested .
the latter requires more technical knowledge of r. figure compiles results.
almost messages being labelled very clear likert and only less than are very unclear orunclear likert .
nonetheless about .
are also challenging to understand likert .
furthermore except for clarity likert each category has a larger proportion of mixed tolower understandability likert .
this is quite negative in terms of ttd indicating a code smell.
obscure unit tests that are harder to grasp reduce the maintainability of the code.
test files organisation a custom r script was used to crawl the source code of the selected repositories extracting the signature of test methods assertions and loc position.
this was used to analyse the distribution of test methods and assertions per selected repository.
table iv summarises the distributions in quartiles it is worth noticing that only the first row fpr presents data per repository.
table iv structure of the survey generated for part ii.
criteria code q0 q1 q2 q3 q4 test files per repository fpr .
test methods per test file tpf asserts per test file asserts per test method apt the script identified test methods in the repositories with assertions.
this yields a mean of .
assertions per test method in alignment with the quartiles of apt.
to further this analysis repositories were classified by range offile per repository fpr test methods per test file tpf andassert per test method apt .
the ranges were defined using the quartiles identified in table iv.
therefore figure showcases the proportion of repositories for each combination of ranges.as can be seen most repositories have between and .
test files regardless of their size in terms of loc or r files.
in this group most repositories also include to test methods per test file with a predominant structure of to assertions in each of them.
all groups have a substantial proportion of repositories that have more than one assertion per test method.
having a single assertion per test method has often been identified as a positive practice appropriate initialisation methods should be used to avoid repeating preparation process .
for example the junit framework for java handles this with the beforeall and beforeeach annotations.
nonetheless according to testthat s specification6 this commonly used framework does not provide means to replicate this initialisation behaviour.
asserts type and goals asserts are useful to evaluate the internal state of the program.
most unit testing frameworks provide multiple assertion methods to compare different types of actual vs expected variables.
assessing which assertions are used the most can help understand what is being tested.
since all packages analysed used testthat as a framework for unit testing the assertions extracted by crawling the source code of selected repositories were classified as default if provided by testthat or custom if created by the repository s developers .
this was checked against testthat s specification and automatically classified through an r script.
overall of assertions detected among all selected repositories .
were user defined custom .
there were unique expressions of which were custom.
furthermore testthat provides default assertions but only were found in these repositories those left behind are expect condition expect vector expect reference expect mapequal expect invisible and expect visible .
figure summarises the popularity of assertions across all of the evaluated repositories showcasing the total number of uses.
though some custom assertions have multiple uses this analysis did not validate that implementations across repositories were equivalent.
when designing test cases good practices indicate that developers should test common cases the traditional or 6see 1528fig.
.
top assertions more used in selected repositories classified as default if offered by testthat .
more used path of an algorithm or function as well asedge cases values that require special handling hence testing boundary conditions of an algorithm or function .
additionally dummy tests may also appear in the code these are assertions that test nothing i.e.
asserting if a hard coded true value is true commonly used to rank up the coverage at the expense of correct testing.
besides those three types tests can be automated automatically executed and compared using assertions or manual maybe automatically run but afterwards it needs a manual check by a person.
classifying asserts in these groups can highlight quality deficiencies as well as gaps in the unit testing of packages.
this was evaluated through a third manual sub study that required analysing the code of each assert to classify it in one of the corresponding groups common edge or dummy and automated or manual .
following the same procedure for sub sampling detailed in section iii a3 a third sample of assertions was randomly extracted from the source code of all repositories.
as before results from this analysis are considered to be somewhat generalisable.
fig.
.
manual classification of asserts subsample to determine testing goals.figure summarises the results.
it is quite negative to see that about .
of the asserts were used to assess common cases disregarding the value of edge cases .
this indicates the presence of ttd as the latter should also be evaluated since it often generates most bugs furthermore it can potentially indicate a lack of exception handling in r code but more studies are required to assess this further.
on a positive note only of asserts were manual and upon close inspection all of them evaluated plots.
b. r developers survey part ii rq3 this section presents the results of the survey conducted on r package developers.
structure preparation and distribution of the survey were discussed in section ii c. figure summarises the answers to the demographic questions.
about half respondents have between years of experience as r developers with almost .
having more than ten years.
furthermore about .
had between r packages published and had more than ten packages.
this indicates that the respondents are mostly very experienced developers and it is safe to assume they are well versed in r programming.
fig.
.
self reported demographics of survey participants.
testing practices regarding testing practices about .
said they test their code using unit testing packages while indicated they only test manually.
this is problematic as manual testing indicates an inappropriate structure incurs in automation debt and is unable to work as efficiently as unit testing.
it could also potentially indicate a lack of functionalities provided by standard r tools.
regarding the latter .
of respondents declared using testthat other unit testing packages had less than of use.
table v summarises answers regarding the type of testing performed it is positive to see that developers are interested in assessing functions individually but also in testing the package as a whole.
about of respondents replied to the question about the desired improvement to existing tools.
the most common 1529table v type of testing prioritised by developers .
testing type answers unit evaluating functions individually .
integration studying clusters of functions .
systems using my package externally other .
n a .
reply though worded differently was better documentation tutorials and examples .
due to space limitations the following is a list of highlighted replies the full table of replies to this answer is available as supplementary material7.
honestly anything that would make it faster or automate bits of it.
i do a terrible job of testing and it is largely because it just takes so much time for so little immediate benefit .
a document that would explain how to create meaningful efficient unit tests i.e.
beyond just boosting the code coverage .
i only use testthat maybe a comparison with the different testing packages would be helpful .
understanding of how to build tests for data science and data tables generally .
easier ways to generate test data that s small and easy to include in a package easy documentation of test result changes to track function changes over time in a documented way.
testing challenges figure summarises the challenges screened in the survey and their reported severity.
similar to other languages time constraints is the most popular limitation followed by emphasis on development rather than testing .
on the positive side few respondents less than have trouble discerning the benefits of unit testing tools.
fig.
.
severity of challenges as reported by developers faced regarding to testing r packages.
however almost declared lack of testing experience as a serious very serious issue with similar severity regarding 7see documentation .
furthermore almost still face asteep learning curve for unit testing.
this is especially concerning given that participants self reported a high level of expertise in r package development see figure .
if senior developers still face these challenges at this stage of their careers junior developers may find this even more taxing.
though overall of respondents do not currently face any of the surveyed challenges between still deal with them at different degrees of severity.
furthermore a typical example of incorrect testing practices is when a unit testing suite passing all test cases but developers still find bugs in the code when using the package.
close to participants said this happened to them more than one time and almost estimated at least once almost did not remember .
though the option of this never happened to me was available none of the participants chose it.
though this is quite negative it aligns with the results obtained in part i of this study.
the extensive focus on testing common cases enables test suites that successfully pass all unit testing but that do not wholly test the code.
this supports the existence of ttd with regards to inadequate unit testing.
coverage practices the survey questioned participants about their reliance on coverage tools.
about .
said they use them occasionally.
about said they always use them and another quarter replied the opposite never .
regarding coverage tools agreed on using codecov8 while used the covr r package.
the last question evaluated how developers perceive the coverage of their packages see table vi .
as can be seen many participants could be understanding that greater coverage implies better testing answers like being confident and bugfree when this is not always correct.
this is demonstrated by a large number of tests suites failing to find bugs and the lack of assertions evaluating edge cases.
table vi self reported effects of coverage visualisations .
coverage perception answers it motivates me .
it makes me more confident in my code it makes me anxious there is work left .
i trust my code is bug free .
other none of the above .
na iv.
d iscussion this section discusses the answer to the rqs and results obtained from both parts of the study.
a. rq1 quality testing this section answers question rq1 regarding the correctness of the tests found in r packages.
1530first of all it is worth noticing that during the repository filtering packages were separated due to low quality testing of them had manual testing only and the remaining had empty tests.
furthermore though high coverage values cannot ensure that the code is defect free there is high variability in the coverage of packages with some groups having as little as .
second only about of relevant lines are tested but code inspections showcased that alternative paths are the least explored and covered when unit testing r packages.
moreover the developers survey confirmed that most of them continue to face test suites with all test passing yet still find bugs in their code.
furthermore r unit testing tools may be incomplete.
in particular though some provided asserts are widely used others are not leading many developers to define their custom asserts.
related to this the most common tool testthat see section iii b1 does not provide functions for automated initialisation of test data see section iii a5 .
as a result this may also hinder the quality of unit testing in r packages.
overall it is concerning to see that the unit testing of r packages cannot be considered comprehensive or high quality.
this can be linked to the developers appraisal of potential improvements for existing tools better documentation tutorials and examples including ways to generate small test data.
b. rq2 ttd smells smells are symptoms of poor developing choices and are related to each type of technical debt .
in particular ttd smells have been previously classified and studied by other authors providing a well accepted taxonomy.as a result this section answers rq2 using the information and conclusions extracted from both parts of this study.
it organises current testing concerns into the ttd classifications proposed by samarthyam et al.
.
table vii summarises problematic testing types testing smells and the results of the study that support it.
overall three groups of ttd smells were identified in the mined r packages the most common group are unit testing smells.
these are inadequate cases testing few paths with few relevant tests obscure tests under the premise of unit testing as live documentation and improper asserts nonoptimal usage of asserts focus on coverage rather than common edge cases and so on .
after that issues in exploratory testing are also present due to inexperience testers.
results here are related to self reported challenges regarding unit testing documentation and learning materials survey results .
finally manual testing smells are present due to limited test execution manual testing demands more resources and time also identified as challenges and improper test design requiring manual confirmation and reporting .
c. rq3 developer challenges the main goal of part ii of this study the anonymous survey was to understand developers subjective perception of testing and the challenges they face.
as presented in section iii b see figure the most common concern is regarding time constraints as most developers cannot invest more time in testing but rather develop the package quickly.
furthermore different organisations and employers emphasise development rather than testing reasserting this issue.
table vii types of ttd smells and results showcasing weak spots type smell reason unit testinginadequate unit tests elevated number of relevant lines still untested see table iii and figure .
many alternative paths belonging to exported functions are not being tested see figure .
elevated variability of coverage between packages of the same discipline.
this may indicate incomplete or excess testing see figure .
increased focus on testing common cases with little focus on assessing edge cases see figure .
obscure unit tests though many asserts have messages they are mostly unclear and not understandable see figure .
in average there are too many asserts per test method lowering the readability of automated testing results see table iv and figure .
excessive use of custom asserts may hinder testing understandability see section iii a6 .
improper asserts too many common cases are being tested and few common cases are being evaluated see figure .
excessive use of custom asserts may indicate potential issues with testing frameworks and developers training see section iii a6 .
developers finding bugs regardless of having test suites with all test passing see section iii b2 .
exploratory testing inexperienced testers though most survey participants reported a high level of expertise see figure their main concern in terms of improvement for existing tools was better documentation tutorials and examples as well as guides to create meaningful tests for data science.
this is also supported by the indicated severity medium to high of challenges such as steep learning curve and poor documentation.
manual testinglimited test execution about papers were filtered as they included only manual testing cases with no unit testing.
about of survey participants acknowledged performing only manual testing in their packages see section iii b1 .
improper test design about of asserts were determined to be manual as they were always testing plots.
though the number is small there was also a low amount of plotting related r packages in the selected sample.
as plotting and visualisation are vital for data science better testing tools should be developed.
1531nonetheless even though participants are quite experienced developers the steep learning curve andpoor documentation of testing tools continue to be a challenge.
this is coupled to desired improvements in current testing packages where better documentation and tutorials were the most common request.
it is safe to assume that common challenges can be linked to two potential sources lack of training in developers.
besides self reported issues on the survey carried out in this study previous research also demonstrated that most r programmers come from diverse technical backgrounds not focused on programming .
incomplete tools due to the towering number of custom asserts challenges such as compatibility issues and desired improvements such as better automation test data generation and comparison between testing suits.
this is also supported by the lack of methods that could be used to initialise test data.
d. threats to validity external validity.
these relate to the generalisability of the results.
the dataset consisted of systematically selected open source r packages mined from github.
still it is unclear if the findings would generalise to all r packages.
furthermore the survey respondents sample may not be representative of the entire population of developers and thus results might not generalise to all of them.
attempts to minimise these biases include surveying a large number of r packages almost filtered by maintenance conditions with a large sample of developers.
to the best of the author s knowledge so far this is the first and largest study regarding unit testing in r packages.
internal validity.
these concern the conditions under which experiments are performed.
though packages were systematically selected of them had to be discarded due to being unable to download all required dependencies and run the automated analysis see covr s limitations in table i .
furthermore since there was no baseline data to use as training sets for automated algorithms a set of analysis was conducted on randomised sub samples of data.
though the numbers were statistically defined they may not be entirely generalisable.
it is worth noticing that the manual classification conducted in section iii a i.e.
analysis of untested lines and informative asserts was completed by a single author which may lead to researcher bias.
in each sub study using this approach this threat was minimised by completing the classification twice after the first attempt the author repeated the process.
there was a difference of a week between finishing the first attempt and commencing the second.
for theuntested lines there were only disagreements solved by revising the classification the resolution method used in informative asserts implied averaging the numbers of both attempts only for the disagreements .v.
c onclusion r is a package based programming ecosystem mostly targeted to statistics and data science that provides a simple way to install third party code and extend the language s functionalities.
as a result defects and bugs present in r packages can transitively affect all packages and scripts that depend on it effectively becoming a threat to their validity.
testing technical debt ttd has been identified as shortcuts non optimal decisions related to testing that may reduce the quality of the code produced.
this study reports the current state of testing of almost systematically selected open source r packages available in github.
the source code of the repositories was analysed to determine the quality of testing and possible smells standard across most r packages.
furthermore this study also surveyed the developers of these packages to understand the testing culture and determine the challenges they face while testing.
the findings can be summarised as follows r package testing cannot be considered comprehensive or high quality.
several reasons support this many alternative paths are not being tested there is a highly variable coverage and the occurrence of manual testing.
several ttd smells have been identified by comparing the results of the study to existing ttd smells classifications.
common smells are inadequate and obscure unit tests improper asserts inexperienced testers and improper test design.
r packages developers face numerous challenges.
participants of the survey self reported a high level of expertise.
however they agreed on the following challenges time constraints emphasis on development rather than testing poor documentation of tools steep learning curve and still finding bugs despise of having test suits with allpassing tests.
this study is the first exploratory step to understand unit testing practices in r package programming.
this is considerably novel because r is a multi paradigm language that combines lazy functional features with minimal objectoriented oo features setting it apart from current oofocused knowledge of technical debt.
in the future the authors would like to expand the study by analysing more repositories and surveying more developers.
automated tools can be developed using the manually classified data that was generated in this study opening the door to large scale studies.
also this study focused on r packages but not in r scripts the latter are a completely different way of programming in r fundamental to data science based in this language.
as a result an empirical study on common practices and basic technical debt would be required before addressing ttd in this context.
finally there are also three lines of work related to the results obtained from this study the automated code analysis identified a large number of irrelevant code lines with a majority of them potentially linked to comments.
analysing these lines could highlight 1532the presence of admitted technical debt either generic or ttd .
there is a high variability of coverage in the mined r packages.
though the ideal coverage has been studied in oo centric programming languages this number may vary dramatically for r. further studies could address this area.
the study showcased a large amount of non exported functions being tested.
though this concept is somewhat similar to private methods in oo programming it is not precisely the same.
the impact of testing or not testing non exported functions also requires further analysis.
common unit testing tools used for r packages are incomplete lacking means to handle a systematic initialisation or cleanup of elements to be tested thus effectively adding complexity to the process.
ethical considerations the methodology used in this manuscript and described in the sections below was approved by rmit university human ethics research committee hrec with project code .