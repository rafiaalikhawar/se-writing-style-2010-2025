an investigationofcross project learningin online just intimeso f twaredefect prediction sadiatabassum sxt901 cs.bham.ac.uk universityof birmingham ukleandrol.
minku l.l.minku cs.bham.ac.uk universityof birmingham ukdanyi feng danyi ouchteam.com xiliutech china george g.cabral george.gcabral ufrpe.br federal ruraluniversityof pernambuco brazilliyansong l.song.
cs.bham.ac.uk universityof birmingham uk abstract just in time software defect prediction jit sdp is concerned withpredictingwhethersoftwarechangesaredefect inducingor cleanbasedonmachinelearningclassifiers.buildingsuchclassifiers requires a sufficient amount of training data that is not available at the beginning of a software project.
cross project cp jit sdp can overcome this issue by using data from other projects to build the classifier achieving similar not better predictive performance toclassifierstrainedon within project wp data.however such approacheshaveneverbeeninvestigatedinrealisticonlinelearning scenarios where wp software changes arrive continuously over time and can be used to update the classifiers.
it is unknown to what extent cp data can be helpful in such situation.
in particular it is unknown whether cp data are only useful during the very initialphaseoftheprojectwhen thereislittlewp data orwhether they could be helpful for extended periods of time.
this work thus providesthefirstinvestigationofwhenandtowhatextentcpdata are useful for jit sdp in a realistic online learning scenario.
for that we develop three different cp jit sdp approaches that can operateinonlinemodeandbeupdatedwithbothincomingcpand wptrainingexamplesovertime.wealsocollect2048commitsfrom threesoftwarerepositoriesbeingdevelopedbyasoftwarecompany overthecourseof9to10months anduse19 8468commitsfrom active open source github projects being developed over the course of to years.
the study shows that training classifiers withincomingcp wpdatacanleadtoimprovementsing mean ofupto53.
comparedtoclassifiersusingonlywpdataatthe initial stage of the projects.
for the open source projects which have been running for longer periods of time using cp data to supplementwpdataalsohelpedtheclassifierstoreduceorprevent large drops in predictive performance that may occur over time leading to up to around better g mean during such periods.
corresponding author.
permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation onthefirstpage.copyrights forcomponentsofthisworkownedbyothersthanthe author s mustbehonored.abstractingwithcreditispermitted.tocopyotherwise or republish topostonserversortoredistributetolists requirespriorspecificpermission and or afee.
request permissionsfrom permissions acm.org.
icse may seoul republic ofkorea copyright heldby the owner author s .
publication rightslicensed to acm.
acm isbn ... .
numberofwpdatawerereceived leadingtooverallg meansup to .
betterthanthoseof wp classifiers.
keywords softwaredefectprediction cross projectlearning transferlearning online learning verification latency concept drift class imbalance acmreference format sadiatabassum leandrol.minku danyifeng georgeg.cabral andliyan song.
.aninvestigationofcross projectlearninginonlinejust intimesoftwaredefectprediction.in 42ndinternationalconferenceonsoftwareengineering icse may23 seoul republicofkorea.
acm newyork ny usa 12pages.
introduction the primary objective of software quality assurance activities is to reduce the number of defects in software products .
it is a challenging problem considering the limitation of budget and time allocationforsuchactivities.softwaredefectprediction sdp helps toreducethetimeandeffortrequiredfortestingsoftwareproducts.
differentmachinelearningapproacheshavebeenproposedforsdp .
manystudieshave focusedon identifying defect prone components e.g.
modulesorfiles .recentstudiesinthisareahave beenincreasinglyfocusingonidentifyingdefect inducingsoftware changes.
this is known as just in time software defect prediction jit sdp .advantagesofjit sdpovercomponentlevelsdp include prediction made at an early stage facilitating code inspection finer granularity of the predictions making it easier to find defects and straightforward allocation of developers to inspectthe code.
similartosdpatthemodule filelevel jit sdpclassifiersrequire sufficientamountoftrainingdatawhichisnotavailableduringthe initial phase of a project.
to overcome this problem previous work has proposed cross project cp jit sdp where historical data from other projects are used to train the classifier .
existing cp jit sdp work assumes an offline learning scenario where classifiersarebuiltbasedonapre existingtrainingsetandnever updatedanymore.thismeansthattheclassifierisneverupdated with within project wp data.
however in practice jit sdp is anonline learning problem where bothadditional cp and wp training examples arrive over time.
theroleofcpdatainsuchonlinelearningscenarioisunclear.
cp jit sdphas never been investigatedin online mode before.
in ieee acm 42nd international conference on software engineering icse icse may seoul republicof korea s. tabassum l.l.minku d. feng g.g.cabral l.song particular itisunknownwhethercpdataisonlyhelpfulatthevery early stages of the project when there is little wp data or it brings a prolonged benefit to the predictive performance ofthe classifier.
for instance it may be that cp approaches using an augmented training data stream formed by both wp and cp examples lead to increased predictive performance even at later stages of the project given that classifiers are built using more data than wp classifiers trained only with wp data.
or it may be that cp data causesuchapproachestoobtainworsepredictiveperformancethan wpclassifiersonceenoughwpdataisusedfortraining.besides predictionqualitycanfluctuateduetovariations conceptdrifts in the underlying defect generating process .
concept drift can causethepredictiveperformanceofjit sdpclassifierstodrop .
theuseofcptrainingdatacouldpotentiallyhelptohandleconcept drift.thisisspeciallythecaseconsideringthatjit sdpisaclass imbalance problem where the number of defect inducing software changes is typically much smaller than that of clean changes.
in such typeof problem itwouldtake alotof timeto collect newwp defect inducing examples to recover well from concept drift.
cp trainingexamplescouldpotentiallyhelptorecoverfromconcept drift more quickly.
thispaperthusaimsatinvestigatingwhenandtowhatextent cpjit sdpdatacanbehelpfulinarealisticonlinelearningscenario.
itanswers the following researchquestions rqs rq1can cp data help to improve predictive performance in the initial phase of a project when there is no or little wp training data available?
for howlongandto what extent?
rq2can cp data help to prevent sudden drops in predictive performance which may be caused by concept drift?
to what extent?
rq3what is the overall effect of using cp and wp data together on the predictive performance throughout the development ofsoftwareprojects?inparticular doclassifierstrainedon bothcp andwp data improve deteriorateor retain similar overallpredictive performance to wp classifiers?
it is worth noting that in online scenarios both wp and cp training data arrive over time as their collection can be automated.
if incoming cp data is used for training a cp classifier there is no reason to exclude incomingwp data from the training process of this classifier as such wp data should not hurt its predictive performance.
therefore we use the term cp when referring to onlinelearningapproachesthatmakeuseofbothincomingcpand wpdatafortraining.thismeansthat whenwerefertothebenefits ofcpdata wedonotmeanthebenefitsofcpdatainisolation but the benefits of cp data used along with incoming wp data.
we use the term wp when referring to approaches that only use wp data for training.
to answer the above research questions this paper investigates threecponlinelearningapproaches asingleonlinelearning classifier trained on incoming wp and cp training examples an online learning ensemble where each classifier is trained on incomingtrainingexamplesfromadifferentproject and asingle online learning classifier that filters out cp trainingexamples that are likely to be very different from the recent wp examples.
these approachesareenhancementsoftheapproachesusedinthejit sdp literature sothattheycanoperateinonlinemode.theyarecomparedagainstonlinewpapproaches.ourexperimentsbased on13softwarerepositoriesshowthatthefirstandthirdapproaches arehelpfultoimprovepredictiveperformanceinjit sdpcompared to wp classifiers whilethe secondisnot.
the contributionsofthis work are the following this paper provides the first investigation of cp jit sdp in arealistic onlinelearningscenario.
we show how to adapt cp jit sdp approaches so that they can be usedin an onlinelearningscenario.
werevealthattheuseofcpdatacombinedwithwpdata canimproveoverallpredictiveperformance ratherthanjust achievingsimilarpredictiveperformance comparedtowp learningin jit sdp.
we show that cp data can be helpful for prolonged periods of time rather than only in the beginning of the learning periodas assumedin previous work.
we show that cp data can reduce the negative effect of sudden predictive performance drops in the classifier resulting in more reliable predictions over time.
weshowthatitisbettertousebothcpandwpdatatogether tobuildasingleclassifier ratherthancreatingdifferentclassifiers using disjointsubsetsof the data.
this paper is further organized as follows.
section presents related work.section 3introducesour online jit sdpapproaches.
section4presentsthedetailsoftheinvestigateddatasets.section5 explainstheexperimentalsetupforansweringtherqs.section6 explainstheresultsoftheexperiments.section7presentsthreats tovalidity.section8presentstheconclusionsandimplicationsof this work.
related work therearemanysdpstudies includingrecentstudies investigating class imbalance techniques automated feature engineering ensemble learning among others.
in this section wediscussthreemainresearchareasofsdpthatareclosely relatedtothiswork cpsdpatthecomponentlevel section2.
cp jit sdp section .
andonline jit sdp section .
.
.
cp sdpat thecomponent level there have been several studies on cp sdp at the component level.
an initial study provided guidelines for choosing training projects .
they proposed an approach to identify factors that influence cp prediction success such as data and process factors.
another study showed that carefully selected cp training data may provide better prediction results than training data from the same project.petersetal.
focusedon selecting suitablecp training databasedonthesimilaritiesbetweenthedistributionofthetest and potential training data.
in particular they used similarity measuring and feature subset selection to remove irrelevant training data.
canfora et al.
proposed a multi objective approach for cp defect prediction.
they attempted to achieve a compromise between amount of code to inspect and number of defect prone artifacts.thisapproachperformedbetterthanwpmodels.panichella et al.
analysed the equivalence of different defect predictors and proposed a combined approach codep combined defect predictor that uses machine learning to combine different and 555aninvestigation of cross projectlearning in online just in timeso f tware defectprediction icse may seoul republic of korea complementary classifiers.
this combination performs better than the stand alone cp technique.
nam et al.
applied transfer component analysis tca to cp sdp.
tca is a transfer learningapproachthatmapsthedatatoacommonlatentspacewhere cp and wp data are similar to each other.
they also proposed a newapproachcalledtca whichselectssuitablenormalisation options for tca.
other studies consider class imbalance learning for cp sdp.
for instance ryu et al.
proposed an approach that uses similarity weight drawn from distributional characteristicsandtheasymmetricmisclassificationcosttobalance imbalanceddistributions.
overall these studies demonstrate that data distributional characteristicsareimportantforcpsdp.inparticular theyproposed approachestoselectcpdatathataresimilartowpdata ortomap cpandwpdataintoalatentspacewheretheyaresimilar.however none of these studies were in the context of jit sdp or online sdp.
.
cp jit sdp the first cp jit sdp study was done by kamei et al.
.
they carried out an empirical evaluation of the jit sdp performance by using data from open source projects.
they investigated five cp jit sdp approaches based on project similarity three variations of data merging approaches and ensemble approaches where each model was trained on data from a different project.
all approaches were based on random forests as base learners.
they found that simplemergingofallcpdataintoasingletrainingsetandensemble approaches obtained similar predictive performance to that of wp models.
different from sdp at the component level other more complex approaches including similarity based approaches did not offer any advantagecomparedto these.
another study investigated cp jit sdp in mobile platforms using14appsand42 543commitsextractedfromthecommit.guru platform .
they compared cp performance of four different well known classifiers and fourensemble techniques.
naive bayes performedbestcomparedtootherclassifiersandsomeensemble techniques.
chen et al.
considered jit sdp as a multi objective problem tomaximisethenumberofidentifieddefect inducingchangeswhile minimising the effort required to fix the defects.
they proposed a multi objective optimization based supervised method called multi to build logistic regression based jit sdp models.
they usedsixopensourceprojects.multiwasevaluatedonthreedifferentmodelperformanceevaluationscenarios cross validation cross project validation and timewise cross validation against state of the art supervised and unsupervised methods.
they found that itcan perform significantly better comparedto wp methods.
despiteshowingthatcpjit sdpcanobtainpromisingresults comparedtowpjit sdp noneofthestudiesaboveconsidereda realistic onlinelearningscenario.
.
online jit sdp fewstudies explored jit sdp in online mode.
mcintosh et al.
performed a longitudinal case study of changes from the rapidly evolving qt and openstack systems and found that fluctuationsinthepropertiesoffix inducingchangescanimpactthe performance of jit models.
they showed that jit models typicallylose power after one year.
hence the jit model should be updated withmore recent data.
tan et al.
investigated jit sdp in a scenario where new batchesoftrainingexamplesarrive overtimeand canbeused for updatingthepredictivemodels usingoneproprietaryandsixopen source projects.
they considered the fact that the labels of training datamayarrivemuchlaterthanthecommittime.thisisknown asverificationlatency in the machine learningliterature .they used resampling techniques to deal with the class imbalanced data issueandupdatableclassificationtolearnovertime.however their approach assumes that there is no concept drift i.e.
that the defect generatingprocessdoes not suffer variations over time.
cabral et al.
proposed a method called oversampling online bagging orb to tackle class imbalance evolution in an online jit sdp scenario taking verification latency into account.
class imbalanceevolutionisatypeofconceptdriftwheretheproportionof defect inducing and clean examples fluctuates over time.
orb has anautomaticallyadjustableresamplingratetotackleclassimbalance evolution being able to improve predictive performance over jit sdpapproachesthat assumeafixedlevel of class imbalance.
none ofthe onlinejit sdpstudiesinvestigatedcp jit sdp.
onlinecp jit sdpapproaches inthissection wemodifyandenhancethreecpjit sdpapproaches adopted in to enable them to be applied to online jit sdp.
all ouralgorithmsfullyrespectchronology.inparticular theynever usefuturecp wptrainingexamples futureknowledgeaboutlabels i.e.
defect inducing or clean or test examples for training a modelusedfor testingthe present.
training examples are generated using the online procedure recommendedbycabraletal.
totakeverificationlatencyinto accountforallapproachesstudiedinthispaper.asoftwarechange becomes a training example either when a defect is found to be associated to it or once a pre defined waiting period whas passed whichever is earlier.
this waiting period represents the amount of timethatittakesforonetobeconfidentthatthechangeinquestion isclean.inotherwords ifnodefectisfoundtobeassociatedtothe softwarechangeduringthewaitingperiod atrainingexampleof the clean class is created to represent this software change.
otherwise atrainingexampleofthedefect inducingclassiscreated immediatelyafterthedefectisfound.if afterthewaitingperiod adefectisfoundtobeassociatedtoachangethatwaspreviously consideredclean anewdefect inducingtrainingexampleiscreated forit.trainingexamplesareusedtoupdatetheclassifierassoon as they are created.
it is worth noting that for all our cp approaches classifiers can be trained not only with cp and wp training examples that are madeavailableovertime afterthe firstwp commit butalso with cp training examples producedbefore the firstwp commit.
.
all in oneapproach existing offline jit sdp work assume that cp classifiers are created only with cp data.
unlike offline approaches the all inoneonlineapproachcanusebothcpandwpdatafortraining.all incomingcpandwptrainingdataareconsideredaspartofasingle datastreamoftrainingexamples whichareusedtotrainasingle 556icse may seoul republicof korea s. tabassum l.l.minku d. feng g.g.cabral l.song algorithm all in one approach s streamofincomingchangesfromseveralprojects b index identifying the target project w waiting period initialise predictive model m foreachincoming change xtp sdo xtpis a change arriving from project pat timestamp t ifp bthen y.alt pr edict m xtp end if storextpin aqueue wfl q wfl qis aqueueof incomingexamples waiting to be usedfor training foreach itemqiinwfl qdo ifadefectwaslinkedto qiat atimestamp tthen create adefect inducing trainin afii10069.ital exampleforqi train m trainin afii10069.ital example removeqifromwfl q else ifqiis olderthan wthen create aclean trainin afii10069.ital exampleforqi train m trainin afii10069.ital example removeqifromwfl q storetrainin afii10069.ital exampleincl h cl his a hash of cleantraining examples end if end if end for ifadefectwaslinkedtoa trainin afii10069.ital exampleincl hata timestamp tthen swap the label of trainin afii10069.ital example to defect inducing train m trainin afii10069.ital example removetrainin afii10069.ital examplefromcl h end if end for onlineclassifierassoonastheyareproduced.thedatastreamis in chronological order i.e.
the training examples are sorted based on the unix timestamp of their creation.
algorithm shows the pseudocode for the all in one approach.
thepredictivemodelisinitialisedasanemptymodelthatalways predicts clean .
when a new incoming change xtpis received at timestep t line3 thealgorithmfirstchecksifthischangebelongs to the target project i.e.
to the project whose changes are being predicted line4 .ifitdoes apredictionisprovidedforthischange.
after that xtpis stored in a queue for a pre defined waiting period line .
all changes in this queue are checked to see whether they can be used for training line to .
if a defect is found to be associatedtoagivenchangeinthequeueduringthewaitingperiod lines to a defect inducing training example is created to represent thischange used for training.if a defectis not found by the end of the waiting period of a given change lines to a clean training example is created for this change and used for training.
after that the change is removed from the queue.
thealgorithmalsocheckswhetherthereisanypastchangefoundto bedefect inducing butthatwaspreviouslyconsideredasaclean training example lines to .
if there is the classifier is trained using that changeas adefect inducingtraining example.
the key difference between the proposed approach and the data mergingapproachesusedbykameietal.
isthat in the learningwasoffline withouttakingintoaccountincomingtraining examplesand verification latency and theyused only cp data for training.
in the proposed approach the learning is online takes verification latency into account and the classifier is trained on bothcpandwpdatawhoselabelsareproducedbeforethecurrent timestamp.
.
ensemble approach the ensemble approach uses an ensemble of classifiers rather than asingleclassifier.aseparateclassifierisbuiltfromeachproject s separatetrainingdatastream e.g.
for10projectstherewillbe10 differentclassifiers .thisincludesbothcpandwpdatastreams.
eachchangebelongingtothetargetprojectisthenpredictedbyall theclassifiers andthemeanofthepredictedprobabilitiesretrieved bytheclassifiersiscalculated.thismeanisusedtopredictwhether the change is clean or defect inducing.
the pseudocode for the ensembleapproachissimilartothatoftheall in oneapproach and can be found in the supplementary material .
as with the all in one approach the chronological order of the training examples is alwaysrespected.
inthepreviousofflineensembleapproach kameietal.investigated both simple voting ensembles where equal weight is given to each classifier and weighted voting ensembles where moreweightisgiventoclassifierstrainedonprojectsthataremore similar to the target project .
they showed that weighted voting did not offer any advantage over simple voting.
hence our online ensemble approach uses simple voting.
the key differences betweenourapproachandtheapproachusedbykameietal.
are that our approach is online and our ensemble contains a classifier built from wp training examples that have been labelled up to the currenttimestamp ratherthanusing only cp data classifiers.
.
filteringapproach eventhoughfilteringdidnotimprovepredictiveperformancein offline jit sdp filtering strategies have shown to be very beneficial in offline sdp at the component level .
therefore we investigate whether filtering out software changes that are dissimilarto the target changes could be useful in onlinejit sdp.
weproposedthefollowingfilteringapproachforonlinejit sdp.
first a fixed size window of most recent incoming wp training examplesismaintained.wheneveracptrainingexamplearrives it iscomparedwiththetrainingexamplesinthewpwindow.aswith theall in oneandensembleapproaches thechronologicalorder ofthetrainingexamplesisalwaysrespected.euclideandistances between the input features of the cp training example and each of the wptrainingexamplesin thewindowthat have the samelabel as the cp training example are calculated to check how similar the cptrainingexampleistorecentwpexamples.itisimportanttouse onlytrainingexampleswiththesamelabeltocomputethedistance.
if the labels had been ignored the approach would consider that a 557aninvestigation of cross projectlearning in online just in timeso f tware defectprediction icse may seoul republic of korea algorithm filtering approach s streamofincomingchangesfromseveralprojects b index identifying the test project w waiting period windowsize size of the wp qsliding window k number of top short distances to be used maxdist distance threshold for similarity cpqsize maximum size of the queue cp qof dissimilar cp instancesto be re checkedfor similarityinthe future initialisepredictive model m foreachincoming change xtp sdo xtpis a change arriving from project pat timestamp t ifavgdist trainin afii10069.ital example wp q k maxdist forany trainin afii10069.ital exampleincp qthen train m trainin afii10069.ital example removetrainin afii10069.ital examplefromcp q end if ifp bthen y.alt predict m xtp end if storextpin aqueue wfl q wfl qis aqueueof incomingexamples waiting to be usedfor training foreach itemqiinwfl qdo ifadefectwaslinkedto qiat atimestamp tthen create adefect inducing trainin afii10069.ital exampleforqi else ifqiis olderthan wthen create aclean trainin afii10069.ital exampleforqi storeqiincl h cl his ahash of clean training examples end if end if ifatrainin afii10069.ital examplewascreated for qithen ifavgdist trainin afii10069.ital example wp q k maxdist orthis isawp change then train m trainin afii10069.ital example else addtrainin afii10069.ital exampletocp q end if removeqifromwfl q slidewp qiftrainin afii10069.ital exampleiswp end if end for ifadefectwaslinkedtoa trainin afii10069.ital exampleincl hbefore timetthen swaplabel of trainin afii10069.ital exampleto defect inducing removetrainin afii10069.ital examplefromcl h ifavgdist trainin afii10069.ital example wp q k maxdist then train m trainin afii10069.ital example else addtrainin afii10069.ital exampletocp q end if end if end forcp clean trainingexample described by similar features as a wp defect inducing training example are similar training examples.
however they are differentdueto the differentlabel.
the average of the smallest k distances is calculated.
if this averagedistanceisequaltoorlowerthanamaximumthreshold thecptrainingexampleisallowedtotraintheclassifier.discarded cptrainingexamplesarekeptinafixed sizedqueue.thisqueue is checkedin every iterationto see whetherany old discardedcp trainingexamplehasnowbecomesuitablefortraining.thiscan beusefulincaseconceptdriftcausessuchdiscardedexamplesto become relevant.
algorithm2showsthe pseudocodeforthe filteringapproach.
the predictivemodel is initialisedasan empty modelthat always predicts clean .
when a new incoming change xtpis received at time step t line the algorithm checks whether there are any old cptrainingexamplesthatwerepreviouslynotusedfortrainingdue to their dissimilarity to wp examples but that are now suitable for trainingduetotheirsimilaritytothecurrentwpslidingwindow line to .
then a prediction is given if the change xtpbelongs to target project line to .
the change xtpis then stored in a queue line waiting to be labelled.
all changes in this queue arecheckedtoseewhethertheycanbelabelled lines12to30 .if theycan correspondingtrainingexamplesarecreatedandusedfor training only if they are similar enough to the wp sliding window lines21to23 .iftheyarenotsimilarenoughandarecpexamples they are stored in the queue cp qof discarded cp examples for possiblefutureuse line25 .theslidingwindowisupdated slided ifthetrainingexampleiswp line28 .thealgorithmalsochecks whetheranychangethatwaspreviouslyconsideredascleanhas now been associated to a defect and uses it for training but only if itis similar enough to the wp slidingwindow line31 to .
datasets wehaveextracteddatafromthreeproprietarysoftwaredevelopment project repositories from a chinese software development companyforthepurposeofthisstudy.wehavealsousedtenexisting datasets extracted from open source github projects which weremadeavailablebycabraletal.
at .
some information on the datasets is shown in table .
all datasetswereextractedbasedoncommitguru .thechange metrics include metrics that can be grouped into five types of metrics i diffusionofthechange ii sizeofthechange iii purpose of the change iv history of the change and v experience of the developerthatmade thechange.these changemetricshave been shownto be adequate for jit sdpinprevious work .
itisworthnotingthatthereisevidenceofconceptdriftthatcan be attributed to software engineering in the datasets.
for instance intomcat thenumberofdeveloperschangingthemodifiedfiles associated to a commit increases as the project matures during the first commits then drops.
upon dropping new changes are usually clean different from old ones with similar changemetrics.
to collect the proprietary data for this study all commit messagesfromthethreerepositorieswereextractedusingthegitlog command.achineselanguagenativespeakerknowledgeableof programmingwasaskedtoreadthecommitmessagestoidentify keywordsthatcanbeusedtoidentify corrective commits.keywords representativecommitmessagesofcorrectiveandnon corrective 558icse may seoul republicof korea s. tabassum l.l.minku d. feng g.g.cabral l.song table an overview ofthe projects project total defect inducing defect inducing median defect time period mainlanguage projecttype changes changes changes discovery delay days tomcat .
.
java opensource jgroups .
.
java opensource spring integration .
.
java opensource camel .
.
java opensource brackets .
.
javascript opensource nova .
.
python opensource fabric8 .
.
java opensource neutron .
.
python opensource npm .
.
javascript opensource broadleafcommerce .
.
java opensource c1 confidential confidential .
javascript proprietary c2 confidential confidential .
javascript and 3dstudio proprietary c3 confidential confidential .
python proprietary cases and commit messagesfor whichthe native speaker wasuncertain about were stored in a separate file.
this gave a total of commit messages.
the second author of this paper then went throughthefileprovidinghisindependentclassificationofthecommits as corrective or non corrective by using google translate to translatethecommitmessages.uncleargoogletranslationswere discussedwiththenativespeaker.afterthat thenativespeakerand the second author met to discuss all commit messages.
commit messagesforwhichtherewasstillsomedoubtafterthisdiscussion werefurtherdiscussedwiththe company whoconfirmedwhether theywerecorrectiveornon corrective.thenativespeakerandsecond author then agreed on a list of keywords to identify commits which was presented to the company.
the company confirmed that the listof keywordswasadequate for their projects.
thelistofkeywordswasusedasinputforcommitgurutoidentify corrective commits which were then used to identify which changes are defect inducingorclean to generate thedata sets.
asadataqualityassuranceprocedure allcommitmessagesconsidered by commit guru as corrective and a sample of non corrective commitmessagesweredouble checkedbythenativespeaker giving a total of commit messages.
there was a disagreement between commit guru s classification and the native speaker in only cases and the native speaker was unsure of the correct classification in cases.
therefore commit guru s classification of commits as corrective and non corrective was deemed appropriate.
experimental setup therqsintroducedinsection1willbeansweredbycomparingthe predictiveperformanceoftheall in one ensembleandfiltering approachesagainst wplearning.theanalysis doneforrq1 rq2 and rq3 will concentrate on the predictive performance in the beginning of the projects during periods of time where we can observe sudden drops in predictive performance of the wpapproach and averageacrosstimesteps respectively.we defineatimestepasasequentialnumberindicatingtheorderof wp commits.
each wp commit requires a wp software changeto be predicted as defect inducing or clean.
due to the poor results obtainedbytheensembleapproachontheopensourcedata this approach wasnot run for the proprietary data.
given an open source project repository all other open repositoriesareconsideredasthecpdata.givenaproprietaryrepository two cases were considered the other proprietary repositories are the cp data and all other12 repositories are the cp data.jit sdpisaclassimbalanceproblem andtheopen sourcedatausedinthisstudyareknowntobeclassimbalanced .
therefore learningapproachesneedtouseonlinelearningclassifiers that can deal with this issue.
two state of the art approaches foronlineclassimbalancedlearningareimprovedoversampling onlinebagging oob andimprovedundersamplingonlinebagging uob .also cabraletal.
proposedanewapproach called oversampling rate boosting orb which improves the predictive performance further for jit sdp.
the three approaches are ensemblesofhoeffdingtrees .theseareonlinelearningbase classifiers whichareupdatedincrementallywitheachnewtraining example preservingoldknowledgewithoutrequiringstorageofold examples.
previously oob and uob achieved similar performance tojit sdp andorbperformedthebest.thus onlyooband orb are selectedas the baseclassifiers in this study.
toevaluatetheperformance we adoptrecall onthe clean class recall0 recall on the defect inducing class recall1 and geometric mean of recall0 and recall1 g mean .
they were computed prequentially and using a fading factor to enable tracking changes inpredictiveperformanceovertime asrecommendedforproblems that may suffer concept drift .
if the current example belongs to classi recall t i recall t i 1 y.alt i whereiis zero or one tis the current time step is a fading factor set to .
as in y.altis the predicted class and 1 y.alt iis the indicator function whichevaluates toone if y.alt iand tozerootherwise.if the current example does not belong to class i recall t i recall t i. also g mean t radicalbig recall t r ecall t .
it is worth noting that recall0 falsealarmrate andsofalsealarmsaretakeninto account through both recall0 and g mean.
these metrics were chosenbecausetheyarethemostrecentlyrecommendedforonline class imbalance learning .
thepredictiveperformancesobtainedduringtheinitialphase of the projects and the overall predictive performances calculated using all time steps will be compared across data sets using the scott knott procedure which ranks the models and separates them into clusters.
this test is used to select the best subgroup among different models.
non parametric bootstrap sampling is usedtomakethetestnon parametric asrecommendedbymenzies et al.
.
as explained by demsar non parametric tests are adequate for comparison across data sets.
in addition the scottknotttestadoptedinthispaperusesa12effectsize toruleout anysmalldifferencesinperformance.specifically scott knottonly 559aninvestigation of cross projectlearning in online just in timeso f tware defectprediction icse may seoul republic of korea performed statistical tests to check whether groups should be separated if the a12 effect sizewas mediumor large as recommended in .ifthea12effectsizewasnotmediumorlarge groupswere notseparated.wewillrefertoscott knottbasedonbootstrapsampling and a12 as scott knott.ba12.
we have also included the a12 effectsizesfor eachdataset individuallyto support the analysis.
theparametersforthe filteringapproach werechosenbyperforming grid search on the initial portion commits of the datasetsusingthefollowingsetofvalues whereboldvalueswereselected windowsize k maxdist .
.
.
and cpqsize .
parameters of oob and orb were keptto thesamevaluesasin for open source datasets as theyhavealreadybeentunedforthesedatasets.ensemblesizesand decay factors were further tuned for the proprietary datasets.
the waiting period was for the open source datasets as in and for the proprietary datasets due to their lower defect discovery delay seetable1 .thirtyexecutionsofeachapproachwitheach of the baseclassifiers have been performedoneachdataset.
experimentalresults .
rq1 initialphaseoftheproject wedefinetheinitialphaseoftheopensourceprojectsastheperiod oftimerangingfromthefirsttimestepuntilthetimestepwhere theg meanofthewpapproachreachesthevalueofitsaverage g mean across time steps.
it represents the time it takes for the gmeanofthisapproachtoreachitstypicalvaluesforagivenproject.
fortheproprietarydata thetotalnumberoftimestepsistoosmall to use the average g mean across time steps for this purpose.
in particular hadlongerperiodsoftimebeenobserved theg mean values would be likely to improve further given the trends in gmeanattheendoftheperiodanalyzedfortheseprojects seefig.
whichwill be discussed laterin this section .therefore instead of usingtheaverageg meanacrossalltimesteps wehaveusedthe averageg meanacrossthelast40timestepstodeterminetheinitial phase.
table shows the number of time stepsof theinitial phase ofallprojects aswellastheaverageg meanofeachapproach the effect size a12 against the corresponding wp approach and the scott knott.ba12results duringthis period.
the initial phase of the open source projects lasted from to time steps with a median of and from to time steps with a median of when using oob and orb respectively .
the average g mean of the all in one and filteringapproacheswasfrequentlyhigherduringtheinitialphase of the open source projects than that of the wp approach up to .
higher for brackets using all in one orb .
this is further illustratedbyfigs.1and2 whichshowtheg meansacrosstime steps.
the g means of the wp classifiers were lower than those of all in one and filteringintheinitial phase ofall plots except for figs.
1b 1c and 2b where the g means were similar.
the superiorityofall in oneandfilteringisconfirmedbyscott knott.ba12 which shows that these approaches were better ranked than the othersintermsofg mean.a12effectsizesagainstwplearningfor individual datasets were typically large.
the ensemble approach sometimes achieved better g means than the wp approach at the verybeginningoftheprojects butperformedworsethantheother cp approaches during most of the initial phase .
these a tomcat b jgroups c spring integration d camel e brackets f nova g fabric8 h neutron i npm j broadleafcommerce figur e g mean for all datasets through time using oob.
theverticalredbarindicatesthelasttimestepoftheinitial phase ofthe project shownintable .
results are also supported scott knott.ba12 which shows that the ensemble approach wasbetterrankedthanthe wpapproach but worse rankedthanall in one andfiltering in terms of g mean.
giventhepromisingresultsoftheall in oneandfilteringapproaches weinvestigatedthemfurtherinthecontextoftheproprietarydata.all in onewas investigatedintwodifferentways a combiningbothopensourceandproprietarytrainingdataand b onlywithproprietarydata.theinitialphaseoftheproprietary projectswastypicallymuchsmallerthanthatoftheopensource projects lasting from to time steps and from to timestepsfor oob and orb respectively .
the g means 560icse may seoul republicof korea s. tabassum l.l.minku d. feng g.g.cabral l.song table number of initial time steps of the initial phase and average g means a12 effect sizes and scott knott.ba12 to compare learning approacheson this initial phase oob orb opensourcedata initialtimesteps wpall in one filtering ensemble initialtimesteps wpall in one filtering ensemble tomcat .
.
.
.
.
.
.
.
jgroups .
.
.
.
.
.
.
.
spring integration .
.
.
.
.
.
.
.
camel .
.
.
.
.
.
.
.
brackets .
.
.
.
.
.
.
.
nova .
.
.
.
.
.
.
.
fabric8 .
.
.
.
.
.
.
.
neutron .
.
.
.
.
.
.
.
npm .
.
.
.
.
.
.
.
broadleafcommerce .
.
.
.
.
.
.
.
ranking proprietarydata initialtimesteps wpall in one all in one filtering initialtimesteps wpall in one all in one filtering combined proprietary combined combined proprietary combined c1 .
.
.
.
.
.
.
.
c2 .
.
.
.
.
.
.
.
c3 .
.
.
.
.
ranking top g means for each dataset are in bold.
symbols and represent insignificant small medium and large a12 effect size against the corresponding wp approach wp oob or wp orb .
presence absence of the sign in the effect size means that the corresponding approach was worse betterthanthecorrespondingwpapproach.scott knott.ba12wasrunforalloob andorb basedapproachestogether.foreachperformance metric one test was run for the open source and one test was run for the proprietary data results.
the groups rankings retrieved by scott knott.ba12 are shown in the rankingrows withsmaller numbers indicating better rankings.
were also much lower than for the longer initial phase of the open source projects.
cp approaches helped to improve average g mean for oob based approaches which is confirmed by the scott knott.ba12 results and further illustrated in fig.
.
for instance infig.3a thewpapproachobtainedverylowg meanof8 around time step while all in one combined and filtering obtained a higher g mean of around .
interestingly all in one combined ledtobetterresultsthanall in one proprietary for c1andc2whenusingoobandforc2whenusingorb indicating that open source data can sometimes help to improve jit sdp predictions onproprietary data duringthe initialperiod.
however theuseofcpdatafortheproprietaryprojectswasless helpful when using orb based approaches see table plots in supplementarymaterial .itispossiblethatthewholeperiod of time analyzed for these projects belongs to the initial phase and that the last time step of the initial phase could not be precisely identified due to the lack of information on the typical g means thatwouldbeobtainedbythewpapproachesinprolongedperiods of time as was done for the open source data.
as shown in section .
theg meansobtainedfororb basedcombinedcpapproaches improve when considering the wholeperiodof the projects.
rq1 cpdatawashelpfulintheinitialphaseoftheprojectwhenthere was no or little wp training data available in particular when using all in one and filtering approaches for the open source projects and oob based approaches for the proprietary projects.
this initial phase lasted from to and from to time steps for the open sourceandproprietaryprojects respectively.improvementsinaverage g mean were upto .
avoidingextremely low g means.
.
rq2 periods with sudden dropsin wp classifier s predictiveperformance in some datasets after the initial phase the wp approach suffered periodsoflargedropsing means.someclearcasescanbeobservedfrom time steps to in fig.
1c near time step in fig.
1d around time step in fig.
1j around time step in fig.2a fromtimestep1000to3000infig.2c andaroundtimestep 2100infig.2i.thecpapproachesfrequentlymanagedtoreduce orsometimes even eliminatesuch drops.
for example in fig.
2c we can see that from time steps to thewpclassifierhadalargefallofperformancereducingthe g meantoaround20 .duringthisperiod all in oneandfiltering managed tomaintain a g mean ofaround60 .
the ensemble approachalsomanagedtoavoidthedroping mean butdidnot perform sowell as all in one andfiltering.
wp classifiers may suffer such drops in performance due to changes in the characteristics of wp training data over time.
however in cp learning training data comes from different projects.
someofthecptrainingdatamayhavesimilardistributionasthe targetprojectcurrentlyhas helpingtoreducethenegativeeffectof differences in the distribution over time.
this is a potential reason for cp data to be helpfulincaseof sudden performance drops.
interestingly the filtering approach managed to achieve a more stableg meanthanall in oneforfabric8.thissuggeststhateven though cp data may prevent performance drops resulting from changes in characteristics of the data it might introduce other performance drops due to the use of too dissimilar cp data.
experiments withadditional projectsare neededto confirmthat.
fortheproprietarydata theperiodoftimeanalysedwasnotlong enoughtoidentifylargesuddendropsinpredictiveperformance after the initial phase.
hence to understand whether cp data is helpfultopreventsuddendropsinperformanceforproprietarydata future work onotherproprietary projectsshould be performed.
rq2 cp approaches frequently help to reduce or even prevent sudden drops in performance compared to wp approaches.
in particular the all in oneandfilteringapproachesobtaineduptoaround40 better g mean thanthe wp approach during suchperiods.
561aninvestigation of cross projectlearning in online just in timeso f tware defectprediction icse may seoul republic of korea a tomcat b jgroups c spring integration d camel e brackets f nova g fabric8 h neutron i npm j broadleafcommerce figur e g mean for all datasets through time using orb.
theverticalredbarindicatesthelasttimestepoftheinitial phase ofthe project shownintable .
.
rq3 overallpredictiveperformance according to the scott knott.ba12 test to rank the overall g mean ofalloob andorb basedapproachesacrossopensourcedatasets all in one oob all in one orb filtering oob and filtering orbrankedbest wp orbrankedsecond wp oobranked third ensemble orb ranked fourth and ensemble oob ranked worst.
table3showsthatall in one oob sg meanswereupto13.
better for npm and all in one orb s were up to .
better for spring integration .
the improvements in average g mean a c1 b c2 c c3 figur e g mean for proprietary datasets through time using oob.
the vertical red bar indicates the last time step of the initial phase of the project shownintable .
for the open source data when using all in one oob compared with wp oob had large effect size in out of datasets and the improvements when using all in one orb compared with wp orb hadlarge effectsize inall datasets.
filteringperformedsimilarlytoall in one.table 3showsthat filtering oob sg meanswereupto13.
betterthanwp oob s forspring integration andfiltering orb sg meanswereupto .
betterthanwp orb s forspring integration .theimprovements in average g means when using all in one oob compared with wp oob had large effect size in out of datasets and the improvements when using all in one orb compared with wporb had large effect size in all datasets.
therefore even though filteringimprovedtheresultsoverthewpapproach filteringinstancesdissimilartothetargetprojectdoesnothaveamajorimpact on the performance of the classifier compared to all in one.
as all in one merges all data together to build one classifier it is possiblethatclassifierperformancemainlydependsontheamountand potentiallyvarietyoftrainingdataratherthancpprojectsimilarity.
from that we can see that merging cp and wp data together to train a classifier through all in one or filtering improved overall predictive performance intermsof g mean rather thanmaintaining similar predictive performance as in offline scenarios.
although ensemble approaches performed well in offline mode ourstudy showsthatensemblesof classifierstrainedonseparate projects did not perform well in a realistic online scenario.
ensemble oob s g means were up to .
worse than wp s for jgroups and ensemble orb s were up to .
worse than wp s for jgroups .
a further analysis of the results obtained by each classifier within the ensemble reveals that their individual g means were not high which resulted in the poor g mean of the ensemble as awhole.this could be due to lack of enough training datafortheindividualclassifiers giventhateachclassifierinthe ensemble is trained with data from one project.
this again suggests thatthe larger amount of varieddata was crucialto improve predictive performance in online jit sdp.
it also explains why the ensemble approachworked inoffline modebut not in online mode.
562icse may seoul republicof korea s. tabassum l.l.minku d. feng g.g.cabral l.song table3 overallpredictiveperformance a12effectsizesandscott knott.ba12statisticalteststocomparelearningapproaches dataset approach recall0 recall1 g mean tomcatwp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
jgroupswp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
spring integrationwp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
camelwp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
bracketswp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
novawp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
fabric8wp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
neutronwp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
npmwp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
broadleafcommercewp oob .
.
.
.
.
.
all in one oob .
.
.
.
.
.
filtering oob .
.
.
.
.
.
ensemble oob .
.
.
.
.
.
rankingwp oob all in one oob filtering oob ensemble oob c1wp oob .
.
.
.
.
.
all in one oob combined .
.
.
.
.
.
all in one oob proprietary .
.
.
.
.
.
filtering oob .
.
.
.
.
.
c2wp oob .
.
.
.
.
.
all in one oob combined .
.
.
.
.
.
all in one oob proprietary .
.
.
.
.
.
filtering oob .
.
.
.
.
.
c3wp oob .
.
.
.
.
.
all in one oob combined .
.
.
.
.
.
all in one oob proprietary .
.
.
.
.
.
filtering oob .
.
.
.
.
.
rankingwp oob all in one oob combined all in one oob proprietary filtering oob combined 1dataset approach recall0 recall1 g mean tomcatwp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
jgroupswp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
spring integrationwp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
camelwp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
bracketswp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
novawp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
fabric8wp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
neutronwp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
npmwp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
broadleafcommercewp orb .
.
.
.
.
.
all in one orb .
.
.
.
.
.
filtering orb .
.
.
.
.
.
ensemble orb .
.
.
.
.
.
rankingwp orb all in one orb filtering orb ensemble orb c1wp orb .
.
.
.
.
.
all in one orb combined .
.
.
.
.
.
all in one orb proprietary .
.
.
.
.
.
filtering orb .
.
.
.
.
.
c2wp orb .
.
.
.
.
.
all in one orb combined .
.
.
.
.
.
all in one orb proprietary .
.
.
.
.
.
filtering orb .
.
.
.
.
.
c3wp orb .
.
.
.
.
.
all in one orb combined .
.
.
.
.
.
all in one orb proprietary .
.
.
.
.
.
filtering orb .
.
.
.
.
.
rankingwp orb all in one orb combined all in one orb proprietary filtering orb combined top g means for each dataset are in bold.
standard deviations are shown in brackets.
symbols and represent insignificant small medium and large a12 effect size against the corresponding wp approach wp oob or wp orb .
presence absence of the sign in the effect size means that the corresponding approach was worse better than the corresponding wp approach.
scott knott.ba12 was run for all oob and orb based approaches together.
foreach performancemetric one testwas runfor the open source andone test wasrun forthe proprietary data results.
the groups rankings retrievedby scott knott.ba12 areshown in the rankingrows withsmaller numbers indicating better rankings.
specifically studies in offline scenarios ignore the chronology of theprojects.whenthetargetandotherprojectshaveanoverlap intheirdevelopmentperiod offlinecpapproachestraintheirindividual classifiers witha considerably larger amount ofdatathat would still not have been available for training in practice leading to overoptimistic estimates of predictive performance.
in terms of recalls for the open source datasets wp oob performed generally poorly in terms of recall0 while ensemble oob ensemble orb and wp orb performed generally poorly in terms of recall1.
as a result these approaches ranked worse than the othersontheseperformancemetrics.therecallsoftheapproaches acrossdatasetsareinfluencedbytrade offsbetweenrecall0andrecall1 resulting in several approaches obtaining the same best rankacrossdatasets.thisisbecauseagivenapproachsometimesperforms better in terms of recall0 and sometimes in terms of recall1 resulting in a the same rank among approaches across datasets.
however given the g mean results which combine recall0 and recall1 the trade offsbetweenrecallsobtained byall in one and filtering were better thanthoseobtainedby wp andensemble.
all in one combined andfiltering combined werethebest rankedintermsofg meanfortheproprietaryprojects accordingto scott knott.ba12 .
effect sizes varied from insignificant to large.
in particular all in one oob combined obtained g means upto14.
better forc2 thanthoseofwp oob andfiltering combined obtainedg meansupto18.
better forc2 thanthose of wp orb.
563aninvestigation of cross projectlearning in online just in timeso f tware defectprediction icse may seoul republic of korea interestingly bothall in one oobandall in one orbusing only the proprietary cp data did not perform so well and were rankedsecondintermsofg mean togetherwithwp oob.wporb was the worst ranked approach in terms of g mean.
these results again show that open source data can be helpful for proprietaryjit sdppredictions.theyalsosuggestagainthatthenumber and variety of training examples used for training a classifier is a key factor for obtaining better g means as the all in one approachusingonlythe proprietarycpdatawastrainedon lesscp data.however theg meanvaluesobtainedwhenpredictingthe proprietarydatawereingeneralmuchlowerthanwhenpredicting the open source data even when using all the open source data as cpdata.thissuggeststhathavingagoodamountofwpdataisalso important.theimportanceofthenumberofwptrainingexamples isalsosupportedbythe lower overallg meanduringtheinitial phase of the open source projects against the higher overallg meanacrossthewholeopensourceprojects .so both open source and proprietary projects need a good number of wp training examples to perform well.
intermsofrecalls theresultsfortheproprietarydatawerevaried.
all in one oobtrainedonlywithproprietarycpdatarankedbest in terms of recall1 but worst in terms of recall0.
wp oob and filter orbrankedbestintermsofrecall0 butwereintheworst group interms of recall1.
rq3 mergingcpandwpdatatogether all in oneorfiltering to trainaclassifierachievedupto16.
higheroverallg meanthan wp classifiers for the open source and up to .
higher overall g mean for the proprietary data in an online scenario.
there was no evidencethatfilteringouttrainingexamplesdissimilartotherecent software changes of the target project is helpful to improve overall predictiveperformance.
buildingseparate classifiers from individual projects ensemble approach wasdetrimental.
threats to validity internal validity each approach for each dataset with each classifiers has been executed times to mitigate threats to internal validity.
also results can be influenced by poor parameter choices.
to mitigate this threat a grid search was performed on a set of possiblevaluesforeachparameterbasedonaninitialportionofthe datastream seesection5 .besides allapproachestakeverification latency intoaccount andfully respectchronology.
construct validity the evaluation metrics used in this work are g mean recall0 andrecall1.thesearewidelyusedmetricsappropriate for class imbalance learning .
predictive performance is calculatedinaprequentialwaywithfadingfactortodiscountolder information across time so that plots of predictive performance reflectthevariationsinpredictiveperformanceobservedovertime.
statistical conclusion validity scott knott test was run with nonparametric bootstrap sampling considering a12 effect size to avoid concluding that there is a difference in predictive performance when this difference is likely to be irrelevant due to low effect size.
externalvalidity thisconcernswithgeneralisationof thefindings.
this study used open source projects and proprietary projectsofvariouscharacteristicssuchasprogramminglanguage starting date number of commits per day etc.
the results may not be generalisedfor othertypes of projects.
conclusion thisstudyinvestigatedcplearningforjit sdpinarealisticonline learning scenario using both open source and proprietary data.
in offlinelearning existingcpapproachesforjit sdpdidnotperform betterthanwpapproaches .inonlinelearning weshowedthat cp approaches trained with incoming cp and wp data can help to improve predictive performance over wp approaches trained only with wp data.
the all in one and filtering cp approaches were particularly helpful during the initial phase of the project whenthereisnotenoughwpdataavailable rq1 leadingtoupto .
improvements in g mean.
these approaches also helped to reduce sudden drops in performance of the predictive model rq2 after the initial phase of the project achieving up to around better g mean during such periods of time.
they also improved overallpredictiveperformance rq3 comparedtothewpapproach obtainingupto .
higher overallg mean.
eventhoughtheensembleapproachwasshowntoperformwell inofflinelearning itwastheworstapproachwhenconsidering arealisticonlinelearningscenario obtainingaverageg meansthat were even lower than those of the wp approach.
this indicates thatsplittingdatafromdifferentprojectsmaynotbeappropriate in online scenario.
on the other hand training a single model combiningcpandwptogether all in one significantlyimproved performance hence is more suitable in online jit sdp.
our results indicate that both thenumber of cp andwp training examplesis importantforachievinggoodpredictiveperformanceinjit sdp.
filteringoutverydifferentcpexamplesdidnotimprovethemodels performance significantly comparedto the all in one approach.
our work has practical implications which are described below in online jit sdp if practitioners use cp data along with wpdata thiscanpreventtheperformanceofthemodelto becomeverylowattheinitialphaseofaprojectwhichoften occursduetolackofsufficienttrainingdata.thiswillenable practitioners to use jit sdp earlier during the development of aproject rq1 .
wp models can suffer performance drops which cause them to be unsuitable during certain periods of time.
these drops mean that at any given point in time models may be performingverywellorverypoorly beingunreliableforpractitioners.usingcpdataalongwithwpdatacanovercomethis issuebyhelpingtopreventorreducesuchdrops enabling practitioners to more continuously use jit sdp throughout the lifetimeof the project rq2 .
thecombineduseofbothwpandcpdatathroughall inoneandfilteringimproves overallpredictiveperformance of jit sdp compared to wp classifiers rq3 .
our study indicates the importance of the amount of training data.
practitioners should consider collecting large amounts of both cp andwp training data when adopting jit sdp.
futureworkincludesincorporatingadditionallongerrunningindustrial projects and additional open source projects investigation of different cp approaches for online jit sdp and investigation of methods for automatically adjusting the hyperparameters of the approachesover time.