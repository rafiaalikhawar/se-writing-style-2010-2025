importance driven deep learning system testing simos gerasimou simos.gerasimou york.ac.uk university of york york ukhasan ferit eniser hfeniser mpi sws.org mpi sws kaiserslautern germany alper sen alper.sen boun.edu.tr bogazici university istanbul turkeyalper cakan alper.cakan boun.edu.tr bogazici university istanbul turkey abstract deep learning dl systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation.
nevertheless using dl systems in safety and security critical applications requires to provide testing evidence for their dependable operation.
recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour.
however they are inadequate in capturing the intrinsic properties exhibited by these systems.
we bridge this gap by introducing deepimportance a systematic testing methodology accompanied by an importance driven idc test adequacy criterion for dl systems.
applying idc enables to establish a layer wise functional understanding of the importance of dl system components and use this information to assess the semantic diversity of a test set.
our empirical evaluation on several dl systems across multiple dl datasets and with state of the art adversarial generation techniques demonstrates the usefulness and effectiveness of deepimportance and its ability to support the engineering of more robust dl systems.
ccs concepts deep neural networks test adequacy safety assurance keywords deep learning systems test adequacy safety critical systems acm reference format simos gerasimou hasan ferit eniser alper sen and alper cakan.
.
importance driven deep learning system testing.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
.
work done while at bogazici university.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea association for computing machinery.
acm isbn .
.
.
.
introduction driven by the increasing availability of publicly accessible data and massive parallel processing power deep learning dl systems have achieved unprecedented progress commensurate with the cognitive abilities of humans .
in fact dl systems can solve challenging real world tasks such as image classification natural language processing and speech recognition .
consequently dl systems are becoming key enablers in many applications including medical diagnostics air traffic control malicious code detection and autonomous vehicles .
despite the manifold potential applications using dl systems in safety and security critical applications requires the provision of assurance evidence for their trustworthy and robust behaviour .
vulnerabilities and defects in these systems either originating from systematic errors insufficient generalisation or inadequate training can endanger human lives lead to environmental damage or cause significant financial loss .
preliminary reports from safety advisory boards e.g.
the us transportation board regarding recent unfortunate events involving autonomous vehicles underline not only the challenges associated with using dl systems but also the urgent need for improved assurance evaluation practices.
from a safety assurance perspective testing has been among the primary instruments for evaluating quality properties of software systems providing a trade off between completeness and efficiency .
domain specific standards such as iso26262 and do 178c prescribe testing principles e.g.
adequacy criteria testing properties which should be employed for the verification of applications within the automotive and avionics domains respectively.
evidence collected as a result of testing is typically used to demonstrate compliance with expected quality assurance levels thus manifesting the ability of those systems to operate with an acceptable risk of failure within their lifetime.
however testing dl systems by simply adopting principles recommended by these standards is not straightforward .
the lack of a system specification regulating the inference mechanism to be learnt combined with the data driven programming paradigm makes impossible to explicitly encode the expected dl system behaviour into its control flow structures .
the extremely large configuration spaces of modern dl models deteriorates the issue as it is impossible to determine and calibrate the influence of each configurable parameter in completing a task e.g.
lenet and ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea simos gerasimou hasan ferit eniser alper sen and alper cakan vgg have more than 60k and 100m configurable parameters respectively.
thus traditional software testing techniques and coverage criteria are inapplicable for dl system verification.
driven by the need for providing high quality assurance in dl systems and inspired by traditional software engineering testing paradigms recent research proposes novel testing techniques and coverage criteria see section for an overview of related work .
the core principle underlying those techniques is that for effective dl system testing the test set should be characterised by high diversity thus enabling to exercise different behaviours of the system .
for example deepxplore estimates the diverse dl system behaviour by calculating neuron coverage as the ratio of neurons whose activation values are above a predefined threshold.
similarly the deepgauge multi granularity testing criteria generalise the neuron coverage concept and calculate the ability of the test set to cover i.e.
trigger major and corner case neuron regions given by partitioning the ranges of neuron activation values.
despite their usefulness these criteria are simply an aggregation of neurons or neuron regions whose activation values conform to certain conditions.
by focusing only on these constrained neuron properties and ignoring the overall dl system behaviour the causal relationship between the test set and decision making is uninformative .
also the instantiation of recently proposed techniques depends on user defined conditions number of regions or upper bounds which might not represent the actual behaviour of the dl system adequately.
finally these criteria provide limited information about the testing improvement contributed by individual test inputs as expected for an effective testing adequacy criterion .
in this paper we bridge the gap in existing research by introducingdeepimportance a systematic testing methodology accompanied by an importance driven test adequacy criterion for dl systems based on relevance propagation.
by analysing the activity of a dl system and its internal neuron behaviours deepimportance develops a layer wise functional understanding that signifies the contribution of internal neurons to the output through the layers.
this contribution enables to determine the causal relationship between the neurons and the dl system behaviour as more influential neurons have a stronger causal relationship and can explain which high level features influence more the decision making.
deepimportance establishes this relationship by computing a decomposition of the decision made by the dl system and iteratively redistributing the relevance in a layer wise manner proportional to how prominent each neuron and its connections are .
as we demonstrate in section .
this importance score is quite different from the neuron activation values used by similar dl testing techniques e.g.
.
using those important neurons deepimportance carries out neuron wise quantisation to partition the space of each neuron s activity into an automatically determined finite set of clusters that captures its behaviour to a sufficient level of granularity.
the importance driven adequacy criterion instrumented by deepimportance measures the adequacy of an input set as the ratio of combinations of important neurons clusters covered by the set.
our empirical evaluation using publicly available datasets mnist cifar udacity self driving challenge and dl systems whose models size ranges from small medium lenet camerax1 lidar ir sensorx2 x3h11 h12 h13 h14h21 h22 h23 h24hidden layer 1hidden layer 2input layer y1 y2 y3output layer speed steer ing angle brakefigure a four layer fully connected dl system that receives inputs from vehicle sensors camera lidar infrared and outputs a decision for speed steering angle and brake.
to large e.g.
dave demonstrates the ability of deepimportance to develop a functional understanding of the dl system and evaluate the testing adequacy of a test set.
furthermore the importance driven adequacy criterion is effective in quantifying the ability of a dl system to identify defects as indicated by the coverage difference between the original test set and adversarial examples generated using state of the art adversarial generation techniques .
overall the main contributions of our paper are the deepimportance approach for finding important neurons of a dl system that are core contributors in decision making the importance driven coverage criterion which can establish the adequacy of an input set to trigger different combinations of important neurons behaviours thus enabling software engineers to assess the semantic adequacy of a test set am extensive deepimportance evaluation on three public datasets mnist cifar udacity and three dl systems lenet cifar10 dave showing its feasibility and effectiveness a prototype open source deepimportance tool and a repository of case studies both of which are freely available from our project webpage at to the best of our knowledge deepimportance is the first systematic and automated testing methodology that employs the semantics of neuron influence to the dl system as a means of developing a laywer wise functional understanding of its internal behaviour and assessing the semantic adequacy of a test set.
the remainder of the paper is structured as follows.
section presents briefly dl systems and coverage criteria in traditional software testing.
section introduces deepimportance and section presents its open source implementation.
section describes the experimental setup and evaluation carried out.
sections and discuss related work and conclude the paper respectively.
background .
deep learning systems fig.
shows a typical feed forward dl system consisting of several interconnected neurons arranged into consecutive layers the input 703importance driven deep learning system testing icse may seoul republic of korea trained dnn x1 x3x2 x4y1 y3y2input layerhidden layer 1hidden layer 2output layer neuron importance analysistraining set t importance methodimportant neurons x1 x3x2 x4y1 y3y2hidden layer 2input layerhidden layer 1output layer important neuronimportant neurons clustering clustering methodclustered important neuronsimportancedriven coveragetesting set u coverage results coverage oracle figure deepimportance workflow for determining the importance based testing adequacy of a dl system.
layer the output layer and at least one hidden layer .
each layer within a dl system comprises a sequence of neurons.
a neuron represents a computing unit that applies a nonlinear activation function to its inputs and transmits the result to neurons in the following layer .
commonly used activation functions include sigmoid hyperbolic tangent and relu rectified linear unit .
all neurons except from those in the input layer are connected to neurons in the following layer with weights whose values express how strong are the connections among neuron pairs.
a dl system s architecture comprises the number of layers neurons per layer neuron activation functions and a cost function.
given such an architecture the dl system carries out an iterative training process through which it consumes labelled input data e.g raw image pixels in its input layer executes a set of nonlinear transformations in its hidden layers to extract semantic concepts i.e.
features from the input data and finally generates a decision that matches the effect of these computations in its output layer.
the training process aims at finding weight values that minimize the cost function thus enabling the dl system to achieve high generalisability.
.
coverage criteria in software testing since testing a software system exhaustively is in principle impossible due to its extremely large number of possible inputs coverage criteria are typically employed to quantify how well a test suite exercises the system .
there are several types of coverage criteria with the most widely adopted in industry being statement coverage condition coverage path coverage and branch coverage .
testing techniques and coverage criteria are building blocks of safety standards employed in various safety critical domains such as automotive and avionics e.g.
iso26262 do 178c .
depending on the integrity level associated with a system component different coverage criteria are mandated.
for instance iso26262 requires to demonstrate compliance with statement coverage and mc dc modified condition decision coverage for components whose integrity levels are the lowest a and highest d respectively.
the higher the risk from a component s misbehaviour the higher its integrity level and thus more significant assurance testing effort is required to avoid unreasonable residual risk.
approach deepimportance whose high level workflow is shown in fig.
enables the systematic testing and evaluation of dl systems.
using a pre trained dl system deepimportance analyses the training set tto establish a fundamental understanding of the overall contribution made by internal neurons of the dl system.
this enables to identify the most important neurons that are core contributors to the decision making process section .
.
then deepimportance carries out a quantisation step which produces an automaticallydetermined finite set of clusters of neuron activation values that characterises to a sufficient level how the behaviour of the most important neurons changes with respect to inputs from the training set section .
.
finally deepimportance uses the produced clusters of the most important neurons to assess the coverage adequacy of the test set section .
.
informally the importance driven test adequacy criterion of deepimportance is satisfied when all combinations of important neurons clusters are exercised.
we use the following notations to present deepimportance.
let dbe a dl system with llayers.
each layer li i l comprises li neurons and the total number of neurons in diss l i li .
let also ni jbe the j th neuron in the i th layer.
when the context is clear we use n dto denote any neuron that is a member of d irrespective of its layer.
let xdenote the input domain of dand x xbe a concrete input.
finally we use the function x n r to signify the output of the activation function of neuron n d. .
neuron importance analysis the purpose of importance analysis is to identify neurons within a dl system that are key contributors to decision making.
given an input information within a dl system is propagated according to the strength of connections weights between neurons in successive layers.
as such the activity of some neurons influences more the capabilities of the system to make correct decisions .
although representation learning is a key characteristic of dl systems that eliminates the tedious and potentially erroneous process of manual feature extraction it also means that neurons develop through backpropagation the ability to learn optimal feature transformations for the given setting on their own .
more specifically raw input data passing through the complex architecture of a 704icse may seoul republic of korea simos gerasimou hasan ferit eniser alper sen and alper cakan algorithm neuron importance analysis function importantneuronsanalysis d x m r relevant neurons vector for all x xdo rx x x importance vector rl computevalue d x decision value for all i ll ... l1 do relevance propagation rl relevance li x rl lineurons relevance rx rx rl append to vector rx r r rx collect relevance vectors an analyse d r analyse relevance vectors return top an m select top mneurons modern dl system with many layers many neurons per layer and non linear transformations e.g.
relu activation functions max pooling convolutions yield abstract and discriminative features that enable the system to make effective decisions in the final layer using a log linear model typically softmax .
for instance neurons within the initial hidden layers learn abstract shapes e.g.
edges circles while neurons in deeper layers extract more semantically meaningful features e.g.
faces objects .
using as an analogy a software system whose architecture adopts conventional software engineering principles neurons can be considered as functions that execute a distinct functionality.
irrespective of the position of a function into the control flow graph it receives transformed information from functions preceding in the graph and itself applies function specific transformations before propagating the updated information to subsequent functions in the control flow graph.
we capitalise on this unique characteristic of neurons within a trained dl system to establish the importance of each neuron.
to achieve this we compute a decomposition of the decision f x made by the system for input x xand use layer wise relevance propagation to traverse the network graph and redistribute the decision value in a layer wise manner proportional to the contribution made by each neuron within the layer.
for a fully connected layer i the relevance rijof the j th neuron entails redistributing relevance from neurons in layer i 1which is given by rij k x nij wijk i x nij wijk ri k where ri kis the relevance score of the k the neuron in layer i wijkis the weight connecting neuron jto neuron kand is a small stabilization term to avoid division by zero .
intuitively the relevance attributed to neurons in layer ifrom neurons in layer i 1is proportional to i the neuron activation x nij i.e.
neurons with higher activation values receive a larger relevance contribution and ii the strength of the connection wijk i.e.
more relevance flows through more important connections.
while applies to fully connected layers we refer interested readers to for definitions of redistribution rules for other layer types including pooling activation and normalisation layers.
the redistribution process is underpinned by a relevance conservation property specifying that at every step of the process i.e.
figure input from mnist dataset with the most important pixels contributing to the correct decision highlighted left and difference between neuron activation values and relevance scores right for the same set of neurons.
at every layer li the total amount of relevance i.e.
the prediction is conserved.
no relevance is artificially added or removed.
.
therefore l1 jr1j l4 kr4k l5 lr5l f x .
algorithm shows the high level process for computing the importance scores for neurons of dand selecting the mmost important.
for any given input x x we perform a standard forward pass to compute the decision value i.e.
the magnitude of evidence for a given class before applying softmax line .
next we perform a backward pass lines considering each layer successively during which the relevance is allocated to neurons of the current layer before being backpropagated from one layer to another until it reaches the input layer.
the decomposition is achieved using the layer specific rules in .
the analyse function line analyses the relevance scores of all neurons for all inputs and prioritises them based on a priority criterion e.g.
cumulative relevance normalised relevance .
in our evaluation section we use cumulative relevance.
finally the top mneurons are returned line .
the use of relevance for identifying the most important neurons is a key ingredient of our approach.
building on recent research onexplainability of dl systems which targets the identification of input parts responsible for a prediction deepimportance targets the identification of the most influential neurons these are highrisk neurons that should be tested thoroughly.
albeit being outside the scope of this work we also highlight that other explainabilitydriven techniques could be used for the identification of the most important neurons e.g.
deeplift l2x .
state of the art testing adequacy criteria for dl systems including neuron coverage and k multisection neuron coverage quantify testing coverage solely based on neuron values irrespective of the added value of a neuron to the final decision.
in other words a neuron might contribute to increasing the confidence for classes other than the correct one and this is not distinguished.
deepimportance captures the actual contribution made by each neuron to the decision which in shallow and deeper layers corresponds to raw pixels and concrete features from the input domain respectively.
for instance fig.
left shows the most important pixels and fig.
right shows the difference between the activation values and the relevance scores for the same set of most important neurons within the penultimate layer of a lenet network .
deepimportance exploits this understanding to assess the adequacy 1when neurons with bias contribute to the output the relevance attribution to the bias is redistributed onto each input of the decomposed layer using the method in .
705importance driven deep learning system testing icse may seoul republic of korea of a test set to examine the most critical neurons i.e.
those with the strongest influence on the behaviour of the dl system.
using relevance is also significantly different compared to sensitivity analysis .
while sensitivity analysis cares about what makes more less a labelled input e.g.
a dog to be classified as its target label relevance analysis investigates what actually makes the input to be classified as that label.
the sensitivity scores do not really explain why an input has been predicted in a certain way but rather to which direction in the input space the output is most sensitive.
in contrast relevance scores indicate which neurons inputs are pivotal for the classification.
thus they are a significantly more informative and practicable measure for assessing and explaining the composition about the decision made by the dl systems .
.
important neurons clustering having established the important neurons that are core contributors to the behaviour of the dl system we are now ready to determine regions within their value domain which are central to the dl system execution.
since each neuron is responsible for perceiving specific features within the input domain we argue that for inputs with similar features the activation values of those important neurons are concentrated into specific regions within their value domain.
informally those regions form a pattern that captures the activity of the most influential neurons of the dl system.
the purpose of clustering is threefold.
first compared to which partitions the value range of neuron activation values into k buckets of equal width solely based on a randomly selected number of buckets i.e.
k multisection neuron coverage the clusters generated by our approach correspond to semantically different features of each neuron.
second since the range of neuron activation values x n could in principle be the entire set of real numbers r for relu activation functions the cyclomatic complexity for analysing the dl system is very large.
similar to techniques employed in clustering bucketing in enables to reduce dimensionality and computational cost thus making tractable to test the dl system cf.
section .
finally the identification of clusters for those important neurons could inform the allocation of testing resources to ensure that the regions of those neurons are tested sufficiently thus increasing our confidence for the robust dl system behaviour.
deepimportance employs iterative unsupervised learning to cluster the vector of activation values from the training set for each important neuron and determine sets of values that can be grouped together.
the deepimportance instantiation we present in this research work section employs k means an iterative clustering method that produces kclusters which minimize the withinclass sum of squares.
to this end we segment the activation values of each important neuron into groups clusters so that activation values within the same group are more similar to other activation values in the same group and dissimilar to those in other groups.
determining the optimal number of clusters without analysing the data is not a trivial problem .
we reinforce cluster extraction with the silhouette index thus supporting the automatic identification of a neuron specific optimal strategy for clustering the activation values of each important neuron in dm.
silhouettealgorithm important neurons cluster extraction function clusterimportantneurons dm t c vector for clustered important neurons for all n dmdo n t n t t n th neuron activation values cmaxn arg max c cscore n labels n c n cluster n cmaxn n i cmaxn in n collect cluster vectors return is an internal clustering validation index that computes the goodness of a clustering structure without external information .
as such depending on each neuron s activation values the optimal number of clusters is determined automatically and can be different between the important neurons.
also this strategy addresses the weakness of k means that requires to define the desired number of clusters a priori.
more formally given the n th important neuron n dm and the function c t indicating for each t tthe cluster assigned to t n within the n th neuron s clusters the silhouette score for c n clusters is defined as follows sc n t t t 1b t a t max b t a t where a t c t u n c t t n d u n t n is the intra cluster cohesion given by the average l1distance of activation value t n to all other values in the same cluster and b t min c c t c u n c d u n t n is the inter cluster separation given by the average l1distance between t n and activation values in its nearest neighbour cluster.
maximising the silhouette score gives the optimal clustering strategy and correspondingly the optimal number of clusters for then th important neuron.
therefore the higher the score the better the overall quality of the clustering result in terms of cluster cohesion and cluster separation.
algorithm shows the high level process underpinning deepimportance for quantising the vector of neuron activation values and extracting clusters for the most important neurons.
given as inputs the training set t x the set of possible clusters c n and the set of important neurons dm cf.
section .
deepimportance produces for each neuron n d mthe vector of activation values nfor all training inputs t t line .
then through an iterative cluster analysis strategy using the silhouette index we find the optimal clustering strategy for each important neuron s activation values line .
next we establish the clusters such that n i cmaxn in where inis the vector containing the activation values for th i th cluster line .
we stop when all important neurons have been analysed.
our approach is generic and can support different clustering algorithms including density based grid based and hierarchical clustering .
we emphasise however the importance of using an 706icse may seoul republic of korea simos gerasimou hasan ferit eniser alper sen and alper cakan iterative strategy that enables to determine the optimum number of clusters.
this is an important step that defines the granularity of our importance driven test adequacy criterion cf.
section .
.
investigating the applicability and effectiveness of other clustering algorithms and clustering validity criteria is left for future work.
.
importance driven coverage given an input set y we can measure the degree to which it coversthe clusters of important neurons termed importance driven coverage idc .
since important neurons are core contributors in decision making cf.
section .
it is significant to establish that inputs triggering combinations of activation value clusters of those neurons cf.
section .
have been covered adequately.
this enables to test the most influential neurons thus increasing our confidence in the correct operation of the dl system and reducing the risk for wrong decisions.
the vector of important neurons cluster combinations incc is given by incc n dm centroid i n i n where the function centroid in measures the centre of mass of the i th cluster for the n th important neuron.
we define importance driven coverage to be the ratio of incc covered by all y yover the size of the incc set.
compared to all other elements in incc the j th innc element is covered if there exists an input yfor which the euclidean distance between the activation values of all important neurons n dmand the corresponding neuron s clusters centroids in jis minimised.
formally idc y incc j y y vin incc j mind y n vin incc following from a test input is always mapped to an element of the semantic feature set given by incc .
idc increases only if the mapped incc element tests a new semantic feature set not already covered by existing test suite inputs otherwise the score remains the same.
we provide a proof of idc soundness on deepimportance webpage achieving a high idc score entails a systematically diverse input set that exercises many combinations of important neurons clusters.
the covered combinations do not include only those exercised during training whose activation values have been used for establishing the important neurons but also new and diverse combinations.
these new combinations could represent edge case behaviours for the dl system.
the higher the idc score the more incc combinations have been triggered.
consequently the more confidence we should have in the dl system s operation.
another important characteristic of idc is the layer wise estimation of coverage.
by exploiting the combinations of important neurons clusters given by idc measures how well multiple inputs with semantically different features can trigger those combinations.
as such idc is significantly different to research which focuses on counting how many neurons have at least once been the most active neurons on a given layer .
the granularity with which idc is specified depends on the number of important neurons m cf.
algorithm .
clearly settingmto the number of neurons within any layer results in an unmanageable incc number.
for instance assuming each of the neurons of the penultimate layer of lenet produces two clusters cf.
algorithm the number of combinations given by is incc .9e .
since mis the only idc hyper parameter that affects the combinations of important neurons clusters it enables software engineers to experiment with different testing strategies by specifying how coarse or fine grained the analysis should be.
in safety critical systems for instance we might opt for a fine grained idc coverage hence a large m aiming to cover as many combinations as possible.
we show in our experimental evaluation that the higher the number of m the higher the number of combinations and the more testing budget is required to increase the idc score cf.
section .
investigating training informed ways for the automatic identification of the number of important neurons is part of our future work.
implementation to ease the evaluation and adoption of deepimportance and the importance driven coverage from section we have implemented a prototype tool on top of the open source machine learning framework keras v2.
.
with tensorflow v1.
.
backend .
the open source deepimportance source code the full experimental results summarised in the following section additional information about deepimportance and the case studies used for its evaluation are available at evaluation .
research questions our experimental evaluation answers the research questions below.
rq1 importance can neuron importance analysis identify the most important neurons?
we used this research question to establish if the importance based algorithm underpinning deepimportance for the identification of important neurons comfortably outperforms a strategy that selects such neurons randomly.
rq2 diversity can deepimportance inform the selection of a diverse test set?
we investigate whether software engineers can employ the importance driven coverage to generate a diverse test set that comprises semantically different test inputs.
rq3 effectiveness how effective is deepimportance in identifying misbehaviours in dl systems?
with this research question we examine the effectiveness of deepimportance to detect adversarial inputs carefully crafted by state of the art adversarial generation techniques .
these adversarial inputs should be semantically different than those encountered before thus increasing the importance driven coverage metric.
rq4 correlation how is deepimportance correlated with existing coverage criteria for dl systems?
we analyse the relationship in behaviour between deepimportance and state of the art coverage criteria for dl systems including neuron coverage k multisection neuron coverage and surprise adequacy .
707importance driven deep learning system testing icse may seoul republic of korea rq5 layer sensitivity how is the behaviour of deepimportance affected by the selection of specific neuron layers?
given the layer wise capability of deepimportance we investigate whether performing the analysis on shallow or deeper layers has any impact on the importance driven coverage metric.
.
experimental setup datasets and dl systems.
table shows the datasets and dl systems used in our experimental evaluation.
we evaluate deepimportance on three popular publicly available datasets.
mnist is a handwritten digit dataset with training inputs and testing inputs each input is a 28x28 pixel image with a class label from to .
cifar is an image dataset with training inputs and testing inputs each input is a 32x32 image in ten different classes e.g.
dog bird car .
the udacity self driving car challenge dataset comprises images captured by a camera mounted behind the windshield of a moving car and supported by the steering wheel angle applied by the human driver for each image.
since this is the ground truth the aim for a dl system is to predict the steering wheel angle hence the dl system s accuracy is measured using mean squared error mse between ground truth and predicted steering angles.
the udacity dataset has training and testing inputs.
to enable a systematic and comprehensive assessment of deepimportance we chose dl systems used in related research with different architecture i.e.
number of layers and layer types fully connected convolutional dropout max pooling complexity i.e.
number of trainable parameters and performance.
for mnist we study three dl systems from the le net family i.e.
lenet lenet and lenet trained to achieve over accuracy on the provided test set cf.
table .
for cifar we employ the prototype model in which is a layer convolutional neural network cnn trained to achieve .
accuracy.
for the udacity self driving car challenge we used the pre trained dave self driving car dl system from nvidia.
dave comprises nine layers including five convolutional layers and its mse is .
.
all experiments were run on an ubuntu server with gb memory and intel xeon e5 .20ghz.
table datasets and dl systems used in our experiments.
dataset dl system params performance mnist lenet .
lenet .
lenet .
cifar a layer convnet with max pooling and dropout layers.
.
udacity self driving car challenge dave architecture from nvidia2116983 .
mse the column params shows the number of trainable parameters for each dl model.
the column performance shows the accuracy for for mnist and cifar datasets and mean squared error for the udacity dataset .coverage criteria configurations.
we facilitate a thorough and unbiased evaluation of deepimportance by comparing it against state of the art coverage criteria for dl systems.
to this end we used deepxplore s neuron coverage nc deepgauge s k multisection neuron coverage kmnc neuron boundary coverage nbc strong neuron activation coverage snac and top k neuron coverage tknc and surprise s adequacy distancebased dsc and likelihood based surprise coverage lsc .
for each criterion we use the hyper parameters recommended in its original research.
in particular we set neuron activation threshold to .
innc and k 3andk in tknc and kmnc respectively.
for nbc and snac we set as lower upper bound the minimum maximum activation value encountered in the training set.
the upper bound for dsc and lsc is fixed to and respectively and the number of buckets is set to .
concerning deepimportance unless otherwise stated e.g.
rq5 we always consider the penultimate layer as the subject layer and the number of important neurons m .
when running the experiments we set an upper bound of execution time to three hours.
if a criterion exceeds this threshold we terminate its execution and report that no results have been generated.
we facilitate the replication of our findings by making available the implementation of all those metrics on the project webpage.
synthetic inputs and adversarial examples.
we use both synthetic inputs and adversarial examples to evaluate deepimportance.
synthetic inputs are obtained by applying small perturbations on the original inputs through gaussian like injected white noise .
adversarial examples are carefully crafted perturbations to inputs which albeit being imperceptible to the human lead a dl system to make an incorrect decision .
adversarial examples are typically used to assess the robustness of dl systems.
we employ four widely studied attack strategies to evaluate deepimportance fast gradient sign method fgsm basic iterative method bim jacobian based saliency map attack jsma and carlini wagner c w .
our implementation of these strategies is based on cleverhans a python library for benchmarking dl systems against adversarial examples.
.
results and discussion rq1 importance .
since identifying the most important neurons within a subject layer is a key principle of deepimportance we assess if the neurons identified during neuron importance analysis cf.
algorithm have indeed a significant role in decision making.
to answer this research question we employ deepimportance to find the m 6andm 8most important neurons for the mnist and cifar and udacity datasets respectively.
we select an equivalent number of neurons using a random selection strategy.
next we employed the approach from used in the explainable ai area to highlight input parts responsible for a decision and chose inputs pixels whose score is above the 90th percentile i.e.
among the top .
we then perturbed those pixels setting their value to zero if their score is above a predefined threshold of .
i.e.
they are relevant and to one otherwise.
we limit the magnitude of perturbation to at most of the total number of pixels aiming to keep the perturbed input close to the original.
finally we measured the l2 euclidean distance between the activation values of the 708icse may seoul republic of korea simos gerasimou hasan ferit eniser alper sen and alper cakan table average std dev.
l2 distance of activation values for neurons selected randomly and using deepimportance on mnist lenet cifar and udacity dave .
strategy lenet lenet lenet cifar dave random .
.
.
.
.
.
.
.
.
.
deepimportance .
.
.
.
.
.
.
.
.
.
figure boxplots comparing activation values distance of important and randomly selected neurons between original inputs and those with their most relevant pixels perturbed.
original input and the perturbed input both for deepimportance and random a higher distance signifies a more significant change.
figure and table show boxplots and the average delta standard deviation in parenthesis of activation values for the entire testing set i.e.
for all classes of each dataset respectively.
the reported results are over five independent runs thus mitigating the risk that they have been obtained by chance.
clearly the activation values distance for neurons selected by deepimportance is higher than the equivalent distance for randomly selected neurons.
the difference becomes more evident in lenet and lenet that have and neurons in the penultimate layer respectively with the distance using deepimportance exceeding .
whereas the distance using random is between .
and .
.
similar observations hold for cifar feature maps while the difference is less clear for lenet feature maps .
these observations also provide a useful indication for the number of important neurons mwith regards to the total number of neurons in the subject layer.
we conclude that deepimportance can detect the most important neurons of a dl system and those neurons are more sensitive to changes in relevant pixels of a given input.
rq2 diversity .
a useful coverage criterion for dl systems entails the ability to assign higher coverage for test sets that comprise semantically diverse test inputs .
this is a significant asset for evaluating the ability of a dl system to learn semantically meaningful features for the decision making task rather than memoising or learning irrelevant features i.e.
learn to make decisions by exploiting unintended similarity patterns in the test set .
to answer this research question we measured the idc metric given the original test set uoof each dataset and corresponding dl systems cf.
section .
for multiple values of important neurons m .
then for each test set we created two perturbed versions.
the former is a semantically diverse setudithat consists of inputs whose top pixels identified similarly as in rq1 are perturbed by applying small perturbations to the original inputsthrough gaussian white noise .
the number of perturbed pixels are for mnist for cifar and for udacity.
the other is a numerically diverse setusthat consists of synthetic inputs generated also by injecting gaussian white noise to an equivalent number of randomly selected pixels of original inputs.
for example fig.
shows an original input from the udacity dataset left the perturbed input from the usset centre and the perturbed input within the udiset right .
the modified pixels in the image on the right are the top pixels that lead the car to steer the wheel to the left ground truth .
we add both of these perturbed test sets to the original set and obtain the test sets uo dianduo sand measured their idc metric.
table top rows with idc prefix shows the average idc value for different dl systems and number of important neurons m. as before we reduce randomisation bias by reporting results over five independent runs.
for all datasets and dl systems the idc value for the semantically diverse set column uo di is always higher than that for the numerically diverse set uo scolumn .
in fact the difference becomes more clear for the more complex dl systems e.g.
lenet on average and dave .
on average .
this behaviour is also reinforced by a corresponding reduction in accuracy.
in particular in all instances the prediction confidence for the uo sset is always higher than that of the uo di set.
these observations signify that idc is more sensitive to input features that are important to the decision making task instead of randomly selected features.
another interesting observation from table is that due to the incc number the idc value becomes lower as the number of important neurons mincreases.
considering lenet for instance idc decreases from .
.
to .
.
for uo di uo s when m 6andm respectively.
for these experiments the number of clusters of important neurons extracted from algorithm is between two and four.
this is expected since the combinations of important neurons clusters incc increases exponentially as mincreases e.g.
form 6and form .
software engineers can use this information to adjust the available budget and effort needed to test their dl systems.
for completeness we ran similar experiments using state ofthe art coverage criteria for dl systems cf.
section .
.
table bottom shows their coverage results.
except from lenet i.e.
the dl system with the smallest complexity the coverage results for all other dl systems are smaller for the semantically diverse set udicompared to the numerically diverse setus.
in contrast to idc which is sensitive to perturbations to relevant input features these criteria are also sensitive to perturbations to random input features.
we conclude that deepimportance with its idc coverage criterion can support software engineers to create a diverse test set that comprises semantically different test inputs.
rq3 effectiveness .
building on research in traditional software testing effective coverage criteria for dl systems should be capable of identifying misbehaviours i.e.
failing test cases .
coverage criteria satisfying this property have good fault detection abilities.
thus they can be used to evaluate the adequacy of a test set and provide a quantifiable measurement of confidence in testing .
to assess the effectiveness of deepimportance we compared the idc values between an unmodified test set uoand sets enhanced 709importance driven deep learning system testing icse may seoul republic of korea table average std dev coverage results for importance driven coverage criterion m and other coverage criteria for mnist cifar and udacity datasets the highest value between usandudiis boldfaced t o timeout n a not applicable .
lenet mnist lenet mnist lenet mnist cifar dav e udacity uo uo s uo d i uo uo s uo d i uo uo s uo d i uo uo s uo d i uo uo s uo d i idc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
idc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
idc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
idc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
kmnc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nbc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sna c10.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tknc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dsc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t o to to n a n a n a lsc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t o to to .
.
.
.
.
.
figure example image from the udacity dataset showing the original input left an input from the usset with gaussian noise in random pixels centre and an input from the udiset with gaussian noise to relevant pixels right .
with perturbed inputs using white noise and adversarial inputs carefully crafted using state of the art adversarial generation techniques.
more specifically for each dataset and each dl system we generated four adversarial test sets using fgsm bim jsma and cw and another numerically diverse testus via gaussian white noise with standard deviation .
as in rq1 .
unlike adversarial inputs the set usis correctly classified with accuracy .
on average.
we extended the original set with each of these synthesised test sets and measured their idc values for the corresponding dl systems.
table columns idc 6andidc shows the average idc coverage results for m across the six enhanced test sets of each dl system.
compared to the original test set uo there is a considerable increase in the idc result for the enhanced test sets for all dl systems.
as expected the idc result for m idc is higher than that for m idc as the number of combinations incc grows exponentially with the number of important neurons.
the increase is more significant in test sets involving adversarial inputs than that with gaussian like noisy inputs uo s .
consequently adversarial inputs lead to higher coverage for our idc criterion thus signifying the sensitivity to adversarial inputs and its fault detection abilities conforming to testing criteria in traditional software testing .
we conclude that idc is sensitive to adversarial inputs and is effective in detecting misbehaviours in test sets with inputs semantically different than those encountered before.table effectiveness of coverage metrics.
y means adding y based adversarial inputs to the original test set uo idc 6idc 8nc kmnc nbc snac tknc lsa dsa uo.
.
.
.
.
.
.
.
.
uo s36.
.
.
.
.
.
.
.
fgsm .
.
.
.
.
.
.
.
bim .
.
.
.
.
.
.
.
jsma .
.
.
.
.
.
.
.
lenet cw .
.
.
.
.
.
.
.
uo.
.
.
.
.
.
.
.
.
.
uo s62.
.
.
.
.
.
.
.
.
fgsm .
.
.
.
.
.
.
.
.
bim .
.
.
.
.
.
.
.
.
jsma .
.
.
.
.
.
.
.
.
lenet cw .
.
.
.
.
.
.
.
.
uo.
.
.
.
.
.
.
.
.
.
uo s48.
.
.
.
.
.
.
.
.
fgsm .
.
.
.
.
.
.
.
.
bim .
.
.
.
.
.
.
.
.
jsma .
.
.
.
.
.
.
.
.
lenet cw .
.
.
.
.
.
.
.
.
rq4 correlation .
we report results on how state of the art coverage criteria behave across the six tests sets for mnist in table .
similarly to idc most of the criteria i.e.
kmnc nbc snac lsa dsa experience a similar increase to their coverage results when evaluated using test sets augmented with adversarial inputs e.g.
fgsm bim jsma cw .
as such idc is consistent with criteria 710icse may seoul republic of korea simos gerasimou hasan ferit eniser alper sen and alper cakan table idc coverage results for different layers with the best coverage between the uoanduo disets typeset in bold.
lenet1 idc lenet4 idc lenet5 idc uo uo di uo uo di uo uo di conv1 .
.
.
.
.
.
conv2 .
.
.
.
.
.
fc1 .
.
.
.
fc2 .
.
conv convolutional layer fc fully connected layer lenet has only one fc layer lenet has none.
based on input surprise e.g.
lsa dsa and aggregation of neuron property values e.g.
kmnc nc .
however while the idc result for the test set uo sis always lower than that with adversarial inputs with the exception of bim for idc 8on lenet there are several instances in which uo sproduces higher results than the adversarial augmented sets e.g.
kmnc nbc dsa for lenet .
this is an interesting finding that requires further investigation.
another interesting observation is that nc and tknc are insensitive to either gaussian like noisy inputs or adversarial inputs irrespective of the employed adversarial technique.
the results for nc are not surprising and conform to results reported in existing research .
nevertheless the plateau shown in tknc is particularly important since it is a layer wise coverage criterion like idc.
in contrast to idc tknc measures how many neurons have at least once been the most active kneurons on a target or all layer.
considering these results idc is more informative than tknc.
in general we conclude that idc shows a similar behaviour to state of the art coverage criteria for dl systems hence there is a positive correlation between them.
rq5 layer sensitivity .
since deepimportance operates layerwise we investigated how idc varies for different layers of a dl system.
table shows the coverage results for m across layers ordered by their depth for the three dl systems using the original test set uoand that augmented with semantically diverse inputs uo di.
first we observe that idc value increases when the analysis is performed on deeper instead of shallow layers.
for instance in lenet and the uotest set idc increases from .
in conv1 to .
in conv2 until it reaches .
in fc1.
we consider this observation as a confirmation of the ability of dl systems to extract more meaningful features in deeper layers.
furthermore idc is more sensitive to the test set with semantically diverse inputs uo di .
in fact we can observe a steady increase in the delta in idc values between uo i danduofor more deeper layers.
for lenet for instance the idc delta grows from .
in conv1 to .
and .
in conv2 and fc1 respectively until it reaches .
for fc2.
this behaviour persists despite the slight decrease in idc value between fc1 and fc2 for both uoand uo di.
this observation reasserts our findings in rq2 cf.
table .
overall the chosen target layer affects the result of idc.
since the penultimate layer is responsible to understand semantically important high level features we argue it is a suitable choice to assess the adequacy of a test set using idc.
.
threats to validity we mitigate construct validity threats that could occur due to simplifications in the adopted experimental methodology using widely studied datasets i.e.
mnist cifar and udacity self driving car challenge .
also we employed publicly available dl systems including lenet and dave that have different architectures and achieve competitive performance results .
also we mitigate threats related to the identification of important neurons algorithm by adapting techniques from the explainable aiarea for identifying input parts responsible for a decision .
we limit internal validity threats that could introduce bias when establishing the causality between our findings and the experimental study by designing independent research questions to evaluate deepimportance.
hence we illustrate the performance of deepimportance in rq1 and rq2 for different values of important neurons m and by augmenting the original test sets with both numerically diverse andsemantically diverse perturbed inputs.
the granularity of idc increases exponentially with higher mvalues thus requiring a substantially larger number of inputs to be satisfied.
we also assessed the effectiveness of idc to detect adversarial examples and confirmed its positive correlation with state of the art coverage criteria for dl systems in rq3 and rq4 respectively.
furthermore we investigate the effect of layer selection on idc result in a structured manner in rq5.
finally when randomness can play a factor e.g.
in rq1 and rq2 we reduce threats that the observations might have been obtained accidentally by reporting results over five independent runs per experiment.
we mitigate external validity threats that could affect the generalisation of idc by developing deepimportance on top of the open source frameworks keras and tensorflow which enable whitebox dnn analysis.
we further reduce the risk that deepimportance might be difficult to use in practice by validating it against several dl systems trained on three popular datasets mnist cifar10 udacity .
however more experiments are needed to assess the performance of deepimportance using other techniques to identify the important neurons e.g.
deeplift to extract clusters within important neurons e.g.
hierarchical clustering and to validate the cohesion and separation of those clusters.
these experiments are part of our future work.
related work trustworthiness issues in dl systems urged researchers to develop techniques that enable their effective and systematic testing .
existing research in the area adapts testing techniques and criteria from traditional software engineering e.g.
while other proposes novel test adequacy criteria .
for instance deepxplore introduces neuron coverage for measuring the ratio of neurons whose activation values are above a predefined threshold.
similarly deepgauge introduces a family of adequacy criteria based on a more detailed analysis of neuron activation values.
deepct proposes a combinatorial testing approach while deepcover adapts mc dc from traditional software testing and defines adequacy criteria that investigate the changes of successive pairs of layers.
recent research also proposes testing criteria and techniques driven by symbolic execution coverage guided 711importance driven deep learning system testing icse may seoul republic of korea fuzzing and metamorphic transformations while other research explores test prioritization and fault localisation .
although the objective of existing research is to guide testing of dl systems eventually improving their accuracy and robustness the majority concerns testing adequacy based on neuron level properties.
in contrast deepimportance driven by the fact that the behaviour of a dl system is determined layer wise proposes a layer wise and importance based test adequacy criterion.
in our experimental study cf.
section we compare the performance of idc against other layer wise criteria e.g.
tknc and show that idc is more informative.
the recent research on using surprise adequacy to guide testing is complementary to deepimportance.
a closely related research branch is the provision of guarantees for the trustworthinesss of dl systems via formal verification .
ai2 uses abstract interpretation to verify safety properties while employs abstraction refinement.
other research uses smt solvers to identify safe regions in the input space and thus establish the robustness of dl systems .
instead of smt solvers reluval finds bounds for security properties using interval arithmetic.
finally dlv verifies local robustness based on user defined manipulations.
deepimportance identifies important neurons using techniques from the explainable ai area e.g.
thus it is orthogonal to existing research on dl system verification.
test adequacy is a widely studied topic within traditional software engineering .
interested readers can find comprehensive reviews of relevant research in this area in related surveys and books .
conclusion ensuring the trustworthiness of dl systems requires their thorough and systematic testing.
deepimportance is a systematic testing methodology reinforced by an importance driven idc test adequacy criterion for dl systems.
deepimportance analyses the internal neuron behaviour to create a layer wise functional understanding and automatically establish a finite set of clusters that represent the behaviour of the most important neurons to an adequate level of granularity.
the importance driven adequacy criterion measures the adequacy of a test set as the ratio of combinations of important neurons clusters covered by the set.
our experimental evaluation shows that idc achieves higher results for test sets with semantically diverse inputs.
idc is also sensitive to adversarial inputs and thus effective in detecting misbehaviour in test sets.
our future work involves investigating methods to automatically determine the number of important neurons improving the robustness of idc evaluating deepimportance on other dl systems and datasets and examining how deepimportance results can be incorporated into safety cases .