an empirical analysis of ui based flaky tests alan romano1 zihe song2 sampath grandhi2 wei yang2 and weihang wang1 1university at buffalo suny2university of texas at dallas abstract flaky tests have gained attention from the research community in recent years and with good reason.
these tests lead to wasted time and resources and they reduce the reliability ofthe test suites and build systems they affect.
however most of theexisting work on flaky tests focus exclusively on traditional unittests.
this work ignores ui tests that have larger input spacesand more diverse running conditions than traditional unit tests.in addition ui tests tend to be more complex and resource heavy making them unsuited for detection techniques involvingrerunning test suites multiple times.
in this paper we perform a study on flaky ui tests.
we analyze flaky ui test samples found in projects from both web andandroid environments.
we identify the common underlying rootcauses of flakiness in the ui tests the strategies used to manifestthe flaky behavior and the fixing strategies used to remedy flakyui tests.
the findings made in this work can provide a foundationfor the development of detection and prevention techniques forflakiness arising in ui tests.
i. i ntroduction software testing is a significant part of software development.
most developers write test suites to repeatedly test various indicators of functioning software.
if a test fails developers will analyze the corresponding test to debug and fixthe software.
however not all testing results are fully reliable.sometimes the test may show flakiness and a test showing thisbehavior is denoted as a flaky test.
flaky tests refer to tests with unstable test results.
that is the same test suite sometimes passes and sometimes fails underthe exact same software code and testing code.
the existenceof flaky tests destroys the deterministic relationship betweentest results and code quality.
once a flaky test appears it maylead to tons of efforts wasted in debugging the failed test which leads to delays in software release cycle and reduceddeveloper productivity .
in the past few years researchers have increased efforts to address this problem .
however the existingresearch on flaky tests mostly focuses on unit tests.
comparedwith traditional unit testing the execution environment andautomation process of ui testing are significantly different first many of the events in these tests such as handling userinput making operating system or browser api calls anddownloading and rendering multiple resources such as imagesand scripts required by the interface are highly asynchronousin nature.
this means that user events and various other taskswill be triggered in a non deterministic order.
second compared to traditional unit testing flaky ui tests are more difficult to detect and reproduce.
this is because itis difficult to cover all use case scenarios by simulating userevents to automate ui testing.
ui tests introduce new sourcesof flakiness either from the layer between the user and theui or the layer between the ui and the test application code.moreover the execution speed of the ui test in continuousintegration environments is slow and this difference in execu tion speed makes detecting and reproducing flaky tests moredifficult.
therefore researching flaky ui tests can help weband mobile ui developers by providing insights on effectivedetection and prevention methods.
to further investigate flaky ui tests we collect and analyze real world flaky ui test examples found in popular weband android mobile projects.
for each flaky test example weinspect commit descriptions issue reports reported causes and changed code.
we focus on the following questions andsummarize our findings and implications in table i. rq1 what are the typical root causes behind flaky ui tests?
we examine the collected flaky ui test samples to determine the underlying causes of flaky behavior.
we groupsimilar root causes together into main categories async wait environment test runner api issues and test script logic issues.
rq2 what conditions do flaky ui tests manifest in and how are they reproduced?
in order to understand how users report intermittent behaviors we investigate the commonstrategies used to manifest the flaky ui test samples.
the datareveals strategies used to reproduce and report flaky uitest behavior specify problematic platform reorder prune test suite reset configuration between tests provide code snippet and force environment conditions.
rq3 how are these flaky ui tests typically fixed?
we identify the bug fix applied to each collected flaky ui testsample and group similar ones together.
we find main cate gories for bug fixing strategies delay dependency refactor test and disable features.
we investigate the impacts that these ui specific features have on flakiness and we find several distinctions.
basedon the investigation of above research questions the maincontributions of this study are our study provides guidance for developers to create reliable and stable ui test suites which can reduce theoccurrence of flaky ui tests.
our study summarizes the commonly used manifestation and fix strategies of flaky ui tests to help developerseasily reproduce and fix flaky tests allowing them toavoid wasted development resources.
our study motivates future work for automated detection and fixing techniques of ui based flaky tests.
ieee acm 43rd international conference on software engineering icse .
ieee table i summary of findings and implications findings implications 1of the observed flaky tests collected tests of the .
dataset are caused by an async wait issue.this group represents a significant portion of the dataset collected and highlights the need to take this root cause into consideration when designing ui tests.
2async wait issues are more prevalent in web projects rather than mobile projects w .
vs m .
.the web presents an environment with less stable timing and scheduling compared with a mobile environment so more care must be taken when network or resource loads are used within web ui tests.
3platform issues are happening more frequent on mobile projects rather than web projects w .
vs m .
.it may be caused by android fragmentation problem.
so the android developers should pay more attention to the environment configuration when choosing the test model.
4layout difference cross platform root causes are found more in web flaky test than in mobile flaky tests w .
vs m .
.this difference can be explained by the number of additional platform conditions that web applications can be exposed compared with the conditions found in mobile environment such as different window sizes different browser rendering strategies etc... 5besides removing flaky test the most common fixing strategies are refactoring logic implementations .
and fixing delays .
.
among them refactoring logic implementations can solve most issues caused by wrong test script logic and fixing delay strategy can solve most timing issues.refactoring logic implementations and fixing delays should be the firstconsidered strategies for developers when fixing bugs.
6dependency fixes are more common in mobile projects than web projects w .
vs m .
.this trend can be caused by the android fragmentation problem.
android developers should pay more attention to this problem when designing test suites.
7delay fixes are more common in web projects than mobile projects w .
vs m .
.this phenomenon is related to the most common test framework in android testing espresso which recommends disabling animations during tests.
table ii summary of commit info from ui frameworks ui topic projects commitsflaky keyword filteringui keyword filtering web angular vue react svg bootstrap d3 emberjs total distinct ii.
b ackground a. impacts of flaky ui tests individual test failures the simplest impact that flaky test can have on test suites is that the individual test run will fail.
this flaky behavior leads to a minimal amount of time and resources wasted by attempting to retry the single test.
build failures flaky tests that are part of continuous integration systems can lead to intermittent build failures.
flaky tests in this stage lead to wasted time trying to identify the underlying cause of the build failure only to find out that the failure was not caused by a regression in the code.
ci test timeouts some flaky behaviors do not cause the tests to fail outright.
instead they cause hangups in the ci system that lead to timeouts.
these hangups waste time as the system waits for a process that never finishes causing the ci system to wait until a specified timeout is met.
iii.
m ethodology a. sample collection web in order to collect samples of flaky ui tests we retrieve commit samples from github repositories.
first we obtain a list of the repositories leveraging popular web ui frameworks using the topic keywords react angular vue emberjs d3 svg web and bootstrap .
these keywordsare used with the github search api to identify distinct repositories pertaining to these topics.
from this set of repositories we download all of the commits for these projects giving a total of commits to search.
next we follow a procedure similar to the one used in luo et al.
and search the commit messages for the patterns flak and intermit and the word test to find commits marked as flaky.
this step reduces the number of commits to .
in order to confirm which of these commits were flaky ui tests manual inspection was performed on the commits in the list.
in order to expedite the process another keyword search is performed using the keywords ui gui visual window button display click and animation to prioritize the commits most likely to refer to flaky ui tests.
this final search prioritizes commit messages to search but the full are searched to increase the chance of identifying flaky ui test commits.
after manual inspection and removing duplicate commits the number of verified flaky tests is .
table ii shows the summary of commit information.
android compared with web development android developers are not as consistent with their choice of ui framework.
therefore to find flaky ui tests on the android platform we use the github archive to perform a massive search on all commits and issue reports on github instead of focusing on repositories using popular ui framework.
specifically we limit our search to the list of closed issues since the flaky tests in closed issues are more likely to be fixed than the tests in open issues.
we search with the keywords flak intermit and test which is similar to the patterns used in web searching.
in order to ensure the issues we find are flaky ui tests on the android platform we also add constraints like android ui espresso screen etc.
b. portion of flaky ui tests to other tests we find that flaky ui tests collected in our methodology make up a small portion of all tests available in these repositories.
this small portion can be explained by several 1586table iii top projects containing the most flaky tests projectinspected commitsflaky tests loc waterfox qutebrowser influxdb angular plotly.js material components web components oppia wix style react streamlabs obs material components android focus android rxbinding xamarin.forms firebaseui android fenix detox components mapbox navigation android sunflower reasons.
based on gh archive the number of open issues over containing potential flaky ui tests outnumbers those in closed issues over .
open issues cannot be included in our study however this large number of open issues possibly containing flaky ui tests highlights the significance of ui flakiness.
besides there are flaky ui tests not captured through the keywords.
one example is in the material components web repository .
while our dataset is not exhaustive we believe the results can provide a basis for future work to build on.
c. sample inspection after collecting these commits and issues of flaky tests reports from github we manually inspect the collected samples to identify the information relevant to our research questions.
in particular we analyze the collected flaky tests by first inspecting the commits in the web projects and the issue reports in the android projects for the following traits the root cause of the flakiness how the flakiness is manifested how the flakiness was fixed the test affected the testing environment and the lines of code of the fix.
for the commits we inspect the commit message changed code and linked issues.
for the issue reports we inspect the developer comments and the linked commits.
when available we also inspect the execution logs from the ci.
table iii shows the information of projects containing flaky tests.
through inspection we obtained the sample set of flaky tests of which were from web repositories and were from android repositories.
d. dataset composition our dataset consists of a diverse set of flaky ui test samples.
the languages of the flaky ui tests analyzed are javascript .
typescript .
html .
and others .
for the web projects and java .
kotlin .
and others .
for the android projects.iv .
c ause of flakiness we investigate the collected flaky tests to determine the root cause of the flaky behavior.
we manually inspect the related commits and issues of the test in order to locate the code or condition that caused the flakiness.
we base our root cause categories on those defined by luo et al.
.
we extend the set of categories to include new categories specific to ui flakiness animation timing issue dom selector issue etc... .
the categorization results are summarized in table iv.
table iv summary of root cause categories found root cause categoriesroot cause subcategoriesweb mobile total async wait network resource loading resource rendering animation timing issue environment platform issue layout difference test runner dom selector issue api issue incorrect test runner interaction test script unordered collections logic issue time incorrect resource load order test order dependency randomness total a. categorization after manual inspection of the flaky ui tests we identify four categories that the root causes of flakiness in these tests can fall under timing issue platform issue test runner api issue and test script logic issue.
we describe the categories and provide examples for each below.
async wait we have found the root cause for a significant portion of the flaky tests analyzed arise from issues with async wait mechanisms.
the common cause of such issues is that the tests do not properly schedule fetching and performing actions on program objects or ui elements causing issues with attempting to interact with elements that have not been loaded completely.
the program objects or ui elements can come from network requests the browser rendering pipeline or graphics pipeline.
this improper ordering of events results in an invalid action and causes an exception to be thrown.
among these async wait issues we identified three three subcategories that group similar root causes together.
a network resource loading flaky tests in this category attempt to manipulate data or other resources before they are fully loaded.
attempting to manipulate nonexistent data causes exceptions in the test.
an example is seen in thering ui web component library repository.
this library provides custom branded reusable web components to build a consistent theme across web pages.
in this project some components being tested utilize images that are fetched through network requests however depending on the network conditions the images may fail to load on time or fail to load altogether.
the code snippet in figure shows how the url variable defined in line is url for an image network call to an external web server.
the image is an avatar used by 1587thetag component on line to display on the page.
when the server call occasionally fails to respond in time due to a heavy network load the visual test will intermittently fail as the rendered tag component will be missing the image.
1const url hubconfigureserveruri api rest avatar default?username jet 20brains !
!
3class tagdemo extends react.component 4render return div tag simple tag tag avatar url readonly false with avatar tag div fig.
ring ui network resource loading example.
another example is found in the influxdb project.
this project provides a client side platform meant for storing querying and visualizing time series data.
figure shows a flaky test for the label ui components.
the test checks that labels update properly by first creating a new label and then attempting to modify the label s name and description through the ui.
figure 2a presents the view of the test suite being run on the left with the ui view on the right.
figure 2b shows the code snippet of the test corresponding to the screenshot.
lines create the label to be used in the test.
lines perform assertions on the labels retrieved through a network call.
however due to the execution timing the label is not yet created in the backend store.
the network call returns an empty response which causes the assertion on line to fail.
b resource rendering flaky tests in this category attempt to perform an action on a ui component before it is fully rendered.
this attempt to interact with the missing component leads to visual differences detected by screenshot tests or exceptions thrown by attempting to access elements that have not fully loaded yet.
an example of this is seen in the generator jhipster project.
this project provides a platform to generate modern web application with java.
in this project a test script attempts to click on a button and wait for the button to be displayed instead of the button being clickable.
normally these descriptions refer to the same event but the modal overlay shown in the ui can block the target button from being clickable.
the faulty code snippet is shown in figure .
the waituntildisplayable function on line pauses the execution until the button is displayed on the page.
the test can fail intermittently if another element is still above the button when line is reached such as an element acting as a background shade in a confirmation modal.
this issue also appeared on the android test in the volley project the code snippet in figure leads to flaky behavior because of a short timeout.
the listener occasionally a the test for updating a label first creates a new label through the ui.
in this case the backend had not finished processing the new label so the network call to fetch all labels returns an empty response.
1it can update a label ... const newlabelname attribut const newlabeldescription ... create label cy.get organization org .then id cy.createlabel oldlabelname id description oldlabeldescription verify name descr color cy.getbytestid label card .should have.length cy.getbytestid label card .contains oldlabelname .should be.visible ... modify cy.getbytestid label card .contains oldlabelname .click b influxdb update label test code snippet.
fig.
influxdb network resource loading example.
1const modifieddatesortbutton getmodifieddatesortbutton !
2await waituntildisplayed modifieddatesortbutton !
3await modifieddatesortbutton.click fig.
generator jhipster resource rendering example.
does not finish executing in ms timeout.
this conflicts with the next request for verifying the order of calls.
1verifynomoreinteractions listener 2verify listener timeout .onrequestfinished higherpriorityreq 4verify listener timeout .onrequestfinished lowerpriorityreq fig.
volley resource loading example.
c animation timing issue flaky tests relying on animations are sensitive to timing differences in the running environment and may be heavily optimized to skip animation 1588events.
the sensitivity to scheduling in animations can lead to issues where assertions on the events are used to test for animation progress.
an example of this type of issue is seen in the plotly.js project .
this project provides visualization components such as bar graphs line plots and more for use in web pages.
in the transition tests the developers find that they intermittently fail due to an underlying race condition between the call to transition the axes and the call to transition the bar graphs.
depending on which transition is called first assertions made on the layout of the graph may fail as the bar graph elements are in different positions than expected.
in figure screenshots from a code snippet provided to reproduce the different states of the animation are shown.
in figure 5a we see that graph starts with the first bar is on value the second bar is on value and the third bar is on value .
figure 5b shows the frame immediately after the react step button is clicked changing the values of the bars to and respectively.
in this figure the background lines of the axes have been shifted in order to represent the new scale but the bars scale incorrectly to the new axes values.
finally figure 5c the bars transition to their correct new values on the new axes.
since the bars are not in the expected positions during the transition test the assertions made fail.
another example is seen in the rxbinding project for android s ui widgets.
in the rxswiperefreshlayouttest which is used to test the swipe refresh gesture the call to stop the refresh animation could happen anytime between the swipe release and the actual refresh animation.
the behavior is flaky because the swipe animation timing used in the recorder is unable to catch up to the listener.
environment some flaky tests manifest due to differences in the underlying platform used to run the tests.
the platform can include the browser used for web projects and the version of android ios etc... used for mobile projects.
we found that these issues can also be further divided into two subcategories.
a platform issue these flaky tests suffer from an underlying issue in one particular platform that causes results obtained or actions performed to differ between consecutive runs within that same platform.
in the ring ui project the screenshot tests for a drop down component fail due to a rendering artifact bug present on internet explorer.
this bug causes a slight variation around the drop down border in the screenshots taken that cause the tests to fail when compared.
these tests pass when run on other browsers.
one example on android is about androidx navigation tool .
for some versions of android espresso has flaky behavior when performing a click action on the navigated page because sometimes it cannot close the navigation drawer before the click action.
however on other versions of android this test always passed.
b layout difference flaky tests can fail when the layout is different than what is expected due to differences in the browser environment.
an example is found in theretail ui project .
this project contains a set of reusable components targeted at retail sites.
the screenshot test for its dropdown component fails because different default window sizes across different browsers causes the dropdown box to be cut off in some browsers.
test runner api issue another root cause of flakiness we found involved an issue when interacting with the apis provided by the testing framework that caused it to function incorrectly.
flaky tests with this root cause either use the provided apis incorrectly or the flaky tests manage to expose an underlying issue in the provided api that causes the functionality to differ from what was expected.
we also identify two subcategories among the flaky tests observed.
a incorrect test runner interaction ui tests use apis provided by the test runner to interact with ui elements but these apis can hit unexpected behaviors that cause incorrect behavior.
for example in the android project firefoxlite flakiness appeared because the testing model registered the click action by espresso as a long click.
figure shows the ui layout after performing the click action incorrectly.
a testing site should have opened by clicking sample top site button.
however the remove menu popped up instead because of the long click action on sample top site button.
this behavior difference caused the test to fail.
b dom selector issue flaky tests interacting with dom elements are intermittently unable to select the correct element due to differences in browser implementations or stale elements blocking focus.
an example of the flakiness arising from an incorrect dom element selection is found in the react datepicker project .
this project provides a reusable date picker component for the react library.
the code under the test incorrectly sets two elements on the page to auto focus on causing a jump on the page that results in a visual flicker.
test script logic issue in some flaky tests flakiness arose due to incorrect logic within the test scripts.
the flaky tests may have failed to clean data left by previous tests made incorrect assertions during the test loaded resources in an incorrect order or incorrectly used a random data generator to create incompatible data.
we find that tests in this category fall under one of four subcategories.
a incorrect resource load order flaky tests in this category load resources after the calls that load the tests causing the tested resources to be unavailable when the test is run.
for example in the project mapbox navigation android the test crashed with an exception because they duplicated a resource load call and then initialized a navigation activity.
b time flaky tests can fail when performing a comparison using a timestamp that may have changed from when it is created depending on the execution speed of the test.
an example is found in the react jsonschema form project .
the project generates forms in react code by specifying the fields in json.
in this project a test on its date picker widget intermittently fails due to a strict comparison of time.
figure 7a shows a screenshot of a test failure a starting state b background axes transition c bars transition fig.
an animation timing issue in the plotly.js project.
a presents the initial state of a bar graph using the library.
the bars start at values and respectively.
b the value of the bars are changed to and respectively.
the background axes change scale but the bars are scaled incorrectly.
c the bars then adjust to the correct scale.
fig.
firefoxlite incorrect test runner interaction example.
within a ci system resulting from a strict comparison issue.
figure 7b presents the faulty code snippet of the flaky test that intermittently fails in the ci system.
depending on when the test is run and how quickly the statements in the test execute the date time value retrieved on line with the date time value generated in line can differ by a small amount causing the assertion on line to fail.
c test order dependency flaky tests in this category can interfere with or be influenced by surrounding tests in the test suit.
this interference can be done through shared data stores that are not cleaned well between test runs.
as a result the data stores may contain values from previous tests and produce incorrect values as a result.
one example of this is appeared in android project restmock .
when trying to reset the server between tests the test would sometimes return an exception because there would be requests from the previous test still running as android shares some processes between tests.
d randomness tests can use random data generation but these tests may intermittently fail for certain values of the data generated.
an example of this type of failure is found in thenomad project which provides a platform to deploy and manage containers.
in this project they find that tests utilizing the job factory component to generate fake tasks can intermittently fail when a job given a name or url with spaces is created.
this causes encoding issues later on in the tests.
since spaces are not valid in these fields the spaces generated by the random string generator are edge cases that should have been handled.
a ci system failure in react jsonschema form project when strictly comparing two date time values.
the values only differ by a marginal amount due to the time when the test is executed but since the comparison is strict the test will fail intermittently depending on when it is run.
1it should set current date when pressing the now button !
const node onchange createformcomponent !
schema type string format date time uischema simulate.click node.selector a.btn now const formvalue onchange.lastcall.args .formdata !
const expected todatestring parsedatestring new date .tojson true !
!
expect comp.state.formdata .eql expected b react jsonschema form strict comparison code snippet.
fig.
react jsonschema form strict comparison check example.
b. results from these samples we were able to find characteristics that are particular to flaky ui tests.
the most predominant root cause for these flaky ui tests involved improper handling of asynchronous waiting mechanisms such as the mechanisms used when loading resources.
these resources can include network resources as well as elements that have not yet been loaded in the page.
this behavior resulted in erratic results in the tests such as attempting to click buttons that had not yet opened.
many of these issues were resolved by refactoring the code to include delays when handling a 1590potentially flaky call.
we found that the root cause of the flaky behavior could present a challenge to find and properly fix with some issues spanning over months to fix.
in addition the flaky nature led some of these issue reports to be closed and reopened in another report as many as five times.
other root causes included platform specific behavior layout differences test order dependencies and randomness.
platform specific behavior produces flaky results for different runs in the same platform.
layout differences behavior causes flaky results due to inconsistencies across different platforms.
flakiness resulting from test order dependencies is caused by improper cleanup of data after runs of previous tests.
ui tests involving random data generation can fail intermittently because of the characteristics of the data generated.
v. m anifestation reproducing flaky tests is a challenging task due to their inherit non deterministic behavior.
if developers provide details on how the flaky behavior was initially encountered and subsequently reproduced this information provides possible strategies to apply to similar cases.
we explore the strategies used by developers to manifest the underlying flaky behavior and construct categories for similar manifestations actions taken.
these strategies are important when reporting the flaky test as they are inherently non deterministic in nature so it is challenging to reproduce them compared with regular bugs.
our categories are summarized in table v. table v summary of manifestation categories manifestation category web mobile total unspecified specify problematic platform reorder prune test suite reset configuration between tests provide code snippet force environment conditions total a. specify problematic platform some tests are reported to only manifest on a specific platform.
in this case the author of the report specifies the problematic platform version to reproduce the flaky behavior.
an example of this type of manifestation is found in the waterfox project .
this project is a web browser based on firefox.
in this project an issue involving animation timing only manifests on macosx platforms.
the report provides details on which file to run on this particular platform in order to reliably manifest the flaky behavior seen in the animation test.
another example in an android project is from gutenberg mobile .
this project is the mobile version for gutenberg editor.
figure shows the bug that only appeared on the google pixel device with android .
when deleting the last character the placeholder text should reveal as shown in figure 8b however the text does not popup.
instead the screen appeared as shown in figure 8a.
users would need to add an additional backspace key press to show the placeholder text.
b a fig.
gutenberg mobile specify problematic platform issue example.
b. reorder prune test suite flakiness arising from test order dependencies can be manifested by running the tests without the full test suite.
this includes running tests by themselves running tests in a different order and changing the state between test runs in order to show the flaky behavior.
an example of this manifestation strategy is seen in the influxdb project .
in this project the flaky behavior surrounding table sorting is manifested by running the tests in the test suite independently.
in some cases trying to reset the environment configuration between tests can also lead to flakiness.
in the project restmock the developers tried to reduce flakiness by resetting the server configuration between tests.
however the test became more unstable because some android processes were shared among these tests and the forced reset caused concurrency conflicts.
c. provide code snippet among the bug reports we observed we find that some reports include code snippets.
the code snippets extract a portion of the flaky test into an enclosed sample to make reproducing the flaky behavior more reliable.
an example of this strategy is used in the project plotly.js project .
this projects provides data visualization components for use in web pages.
in this project a test for a treemap component contains a flaky image load.
in order to manifest this more reliably the reporter created a publicly accessible code snippet that runs the component with the flaky loading behavior.
d. force environment conditions flakiness that displays only when run on a specific platform or under certain environment settings can be manifested by forcibly setting these conditions such as environment variables or browser window size during the test run.
an example of this can be found in the react datepicker project .
this project provides a reusable datepicker component for use in react apps.
a test for the calendar component has flaky behavior when run on the first two days of a new month.
this behavior is manifested by setting the time used in the test to be one of these affected dates.
another example on android 1591is a click function in espresso .
if we run an espresso test which calls openactionbaroverfloworoptionsmenu on a slow device a long click action will be accidentally performed.
this bug can be manifested by a short long click timeout.
vi.
f ixing strategy in this section we examine the fixes of the flaky tests.
we identify common fixing patterns and group them into categories.
through comparative analysis of root causes and fixing strategies we find that most async wait issues are fixed by increasing delay or fixing the await mechanism used.
the issues caused by the environments such as platform issues and layout differences normally could not be solved.
the developers prefer to fix these tests by using a workaround or changing the library version.
table vi summarizes the categories and distribution of fixing strategies and are described in the following paragraphs.
table vi summary of fixing categories found categories subcategories web mobile total delayadd increase delay fix await mechanism dependencyfix api access change library version refactor test refactor logic implementation disable features disable animations remove test remove test total a. delay add or increase delay in order to reduce the chance of encountering flaky behavior some tests will add or increase the delay between actions that involve fetching or loading.
this prevents the rest of the test script from executing until the delay is up giving the asynchronous call additional time to complete before moving on.
an example of this fix is used in the next.js project .
this project is used to generate complete web applications with react as the frontend framework.
the patch increases the delays used in multiple steps as shown in figure .
in the figure line loads a new browser instance and navigates to the about page.
lines and get the text on the page and assert that it is equal to the expected value.
lines and manipulate the about page s component file on the filesystem to make it invalid for use.
line was the delay used before of seconds.
if the test is run during a heavy load on the ci the operation in line may take longer than seconds so the fix is to update the wait to seconds shown in line .
finally line makes the assertion that the updated text on the page shown matched the expected error message.
while this does not fix the root cause directly this code patch does decrease the chance of running into a timing issue during testing.1const browser await webdriver context.appport hmr about !
2const text await browser.elementbycss p .text !
3expect text .tobe this is the about page.
4const aboutpage new file join dirname .. pages hmr about.js !
5aboutpage.replace export default export default not a page nexport const fn !
await waitfor await waitfor 8expect await browser.elementbycss body .text !
.tomatch the default export is not a react component !
fig.
next js increase delay example.
fix waiting mechanism in order to fix flaky behavior some tests fix the mechanisms used to wait on an asynchronous call.
this ensures that the call would finish before moving forward in the test script.
an example is seen in thegestalt project .
this project contains a set of reusable components used on the pinterest website.
this test is run using a headless browser and it is accessed through thepage variable.
in figure lines emit an event on the page to trigger the action being tested.
line is supposed to pause the script execution for milliseconds in order for the page to complete the action from the event handler.
however the function page.waitfor returns an asynchronous javascript promise so it requires the await keyword in order to allow the promise to resolve before the lines after the call are run.
the issue is fixed by adding the await keyword where needed.
1it removes all items async await page.evaluate window .dispatchevent new customevent set masonry items detail items page.waitfor await page.waitfor const newitems await page.
selectors.griditem !
assert.ok !newitems newitems.length fig.
gestalt fix waiting example.
another example in an android project is from project rxbinding .
the developers avoided flakiness in this refresh layout test by manually removing the callbacks of stoprefreshing and adding it back after ms delay if the motion action up has been caught.
the code snippet is shown in figure .
swiperefreshlayout .
setid r.id.swipe refresh layout swiperefreshlayout .setontouchlistener new view .ontouchlistener !
override public boolean ontouch view v motionevent event !
if motioneventcompat .getactionmasked event motionevent .action up !
!
handler .removecallbacks stoprefreshing !
handler .postdelayed stoprefreshing !
fig.
rxbinding fix waiting mechanism example.
b. external dependency fix incorrect api access some tests resolved the flakiness by fixing the usage of an incorrect api function.
after switching this function the test script behaved as expected.
an example is shown in the material ui project which provides reusable web components implementing the material design system.
an api function from the testing library used to access dom element children is incorrect.
the code snippet in figure shows how the incorrect api function is fixed by calling the proper getpopperchildren function instead of attempting to get the element s children directly.
the correct function adds additional selection criteria in order to work within the template code generated by the third party popper.js framework.
assert.strictequal wrapper.find popper .childat .hasclass classes.tooltip true function getpopperchildren wrapper return new shallowwrapper wrapper .find popper .props .children popperprops style restprops !
null const popperchildren getpopperchildren wrapper !
assert.strictequal popperchildren.childat .hasclass classes.tooltip true fig.
material ui fix incorrect api example.
another example on the android platform is found in the detox project .
the action to launch an application in an existing instance which has launched an app during initialization can lead to flaky behavior.
launching an app dynamically in uiautomator is performed by moving to the recent apps view and then selecting the app name.
however device .pressrecentapps uiobject recentapp device .findobject selector .descriptioncontains appname !
!
recentapp .click final activity activity activitytestrule .getactivity !
final context appcontext activity .getapplicationcontext !
final intent intent new intent appcontext activity .getclass !
intent .setflags intent .flag activity single top !
launchactivitysync intent fig.
detox fix incorrect api example.
sometimes the recent apps view shows the full activity name e.g.
com.wix.detox.mainactivity instead of app name e.g.
detox which causes flakiness.
to fix this bug developer removed the uiautomator api and created new instances for each launch request.
figure shows the code snippet of this fixing process.
change library version some tests changed the version of a dependency used in the test as the developers found that the new version introduced the flaky behavior.
c. refactor test checks refactor logic implementation some tests made changes to the logic used when performing checks in order to improve the intended purpose of the test while removing the flakiness observed in the test.
an example is found in the react jsonschema form project .
in the repository a check between consecutive timestamps is given an additional error margin to handle the case of slow execution.
figure shows the code snippet changing the exact date time comparison in line to the comparsion with an error margin of seconds in line .
const expected todatestring parsedatestring new date .tojson true !
expect comp.state.formdata .eql expected test that the two datetimes are within seconds of each other.
!
const now new date .gettime const timediff now new date comp.state.formdata !
.gettime expect timediff .to.be.at.most fig.
react jsonschema form refactor logic implementation example.
d. disable features during testing disable animations in order to remove flakiness caused by animation timing some test completely disabled animations during their run.
this change removed the concern of ensuring an animation had completely finished before proceeding with the rest of the script.
an example of this is seen in the the 1593wix style react project where code is added to disable css animations when the test suite is run .
figure shows the disablecssanimation function defined on lines css rules disabling all transitions and animations.
line adds a call to this function before all tests in the test suite are run.
export const disablecssanimation const css webkit transition duration 0s !important !
transition duration 0s !important webkit animation duration 0s !important !
animation duration 0s !important head document .head document .getelementsbytagname head !
style document .createelement style style.type text css style.appendchild document .createnode css head.appendchild style ... beforeall browser.get storyurl browser.executescript disablecssanimation fig.
wix style react disable animations example.
e. removing tests from test suite remove tests in order to fix the test suite runs some projects choose to remove these tests from the suite.
this fix removes the flakiness in the test suite attributed to the flaky test being removed but reduces the code coverage.
mark tests as flaky some tests are not entirely removed from the test suite.
instead they are marked as being flaky which means that if the test fails the entire test suite does not fail.
this allows the test suite to be isolated from the effects of the flaky test without completely removing the coverage it provides.
blacklist tests in order to conditionally prevent some tests from running tests are added to a blacklist.
the test in these blacklists can be skipped from test runs by setting the appropriate options for when the blacklist should be used.
vii.
d iscussion and implications we investigate our collected flaky ui tests to identify relationships between the root causes manifestation strategies and fixing strategies defined in sections iv v and vi respectively.
through our inspection we can identify relationships between the underlying root causes in issues and how the issue was fixed.
these relationships are presented in figure .
the goal of our study on flaky ui tests is to gain insights for designing automated flaky ui test detection and fixing approaches so we analyze our dataset to identify correlations between manifestation strategies and root causes.
however test runner api issue environmentasync w aitdisable animations fix api access fixing strategies root causestest script logic issueanimation t iming issue resource rendering network resource loading incorrect resource load order test order dependency other types dom selector issue incorrect t est runner interaction platform issue layout dif ferencefix await mechanismadd delay change library versionrefactor logic fig.
relationship between root causes and fixing strategies.
we find that no strong correlations between these two groups exist in the dataset.
similarly we could not identify strong correlations between manifestation strategy and fixing strategy.
this leaves the question of detection strategies for flaky ui tests left open for future work to address.
our results do support relationships between root causes and fixing strategies.
if the root cause of a flaky ui test is known the relationships we draw in figure can be used to select an appropriate fixing strategy.
preliminary design ideas can be made for some of the fixing strategies we identify in section vi.
for the add increase delay fixing strategy a possible automated implementation could identify statements that perform the timing delay and increase the amount of time specified.
if there is no delaying statement then a delay can be after asynchronous function calls are performed.
granular details such as the amount of time to add in the delay or reliably identifying asynchronous function calls requires further analysis on the collected samples.
using the relationships found in figure this approach can be used to fix issues caused by resource rendering .
and animation timing issue .
.
for the fix await mechanism fixing strategy an approach for automatic repair would be to identify statements that implement asynchronous wait mechanisms incorrectly.
the details for this approach would be dependent on the language of the project and would require further analysis of the collected samples.
this approach for an automated implementation of the fix await mechanism can be used to fix incorrect resource load order .
animation timing issue .
resource rendering .
and dom selector issue .
.
the disable animations fix can be implemented by configuring the test environment to disable animations globally when setting up.
this approach can be used to fix issues caused by animation timing issue .
resource rendering .
anddom selector issue .
.
the change library version fixing strategy could be automated by methodically switching different versions of the dependencies used in the project.
this approach could be used to address issues caused by animation timing issue .
and test runner api issue .
.
1594viii.
t hreats to validity the results of our study are subject to several threats including the representativeness of the projects inspected the correctness of the methodology used and the generalizability of our observations.
regarding the representativeness of the projects in our dataset we focused on the most popular repositories associated with popular frameworks on web and android.
we restrict the repositories to focus on repositories that impact real applications as opposed repositories under heavy development.
for mobile projects we searched through github database with strict and clear condition settings to ensure that the samples we obtain are targeted and representative.
in respect to the correctness of the methodology used we collect all available commits on github from the repositories related to popular web ui frameworks.
we also leveraged the github archive repository to find all issues related to android ui frameworks.
we filter out irrelevant commits and issues using keywords and then manually inspect the remaining commits and issues in order to verify the relevance to flaky ui tests.
each sample was inspected by at least two people in order to achieve consensus on the data collected.
regarding the generalizability of the implications made we selected flaky test samples from actual projects used in the wild.
in addition the samples do include large scale industrial projects such as the angular framework itself.
we limit numeric implications only to the dataset collected and focus on qualitative implications made on the features of the test samples.
ix.
r elated work empirical studies on software defects.
there have been several prior studies analyzing the fault related characteristics of software systems .
for example in lu et al.
an empirical study was conducted on concurrency bugs.
in sahoo et al.
bugs in server software were studied and in chou et al.
operating system errors were investigated.
studying flaky tests.
flaky tests have gained interest among the academic community.
these tests were first looked at in by luo et al.
.
in this study commits from open source java projects were manually inspected and categorized into categories.
later zhang et al.
performed studied flaky tests specifically caused by test order dependencies.
in goa et al.
conducted a study that concluded that reproducing flaky tests can be difficult.
thorve et al.
studied android projects with commits related to flakiness.
they found three new categories differing from the ones identified in earlier studies dependency program logic and ui.
lam et al.
examine the presence of flaky test in large scale industrial projects and find that flaky test cause a significant impact on build failure rates.
mor anet al.
develop the flakcloc technique to find flaky tests in web applications by executing them under different environment conditions.
eck et al.
survey professional developers from mozillato learn about the perceptions that developers have on the impacts that flaky tests cause during development.
dong et al.
inspect popular android apps and identified flaky tests to develop their flakeshovel technique that controls and manipulates thread execution.
lam et al.
study the lifecycle of flaky tests in large scale projects at microsoft by focusing on the timing between flakiness reappearance the runtime of the tests and the time to fix the flakiness.
detecting and fixing flaky tests.
bell et al.
developed the technique deflaker to detect flaky tests by monitoring the coverage of code changes in the executing build with the location that triggered the test failure.
flaky tests were those that failed without executing any of the new code changes.
lam et al.
develop the framework rootfinder to identify flaky tests and their root causes through dynamic analysis.
the tool idflakies can detect flaky tests and classify the tests into order dependent and non orderdependent categories .
shi et al.
develop the tool ifixflakies to detect and automatically fix orderdependent tests by using code from other tests within a test suite to suggest a patch.
terragni et al.
proposed a technique to run flaky tests in multiple containers with different environments simultaneously.
x. c onclusions this paper performs a study on flakiness arising in ui tests in both web and mobile projects.
we investigated flaky tests collected from web and mobile popular github repositories.
the flaky test samples are analyzed to identify the typical root causes of the flaky behavior the manifestation strategies used to report and reproduce the flakiness and the common fixing strategies applied to these tests to reduce the flaky behavior.
through our analysis we present findings on the prevalence of certain root causes the differences that root causes appear between web and mobile platforms and the differences in the rates of fixing strategies applied.
we believe our analysis can provide guidance towards developing effective detection and prevention techniques specifically geared towards flaky ui tests.
we make our dataset available at xi.
a cknowledgments we thank the anonymous reviewers for their constructive comments.
this research was partially supported by nsf and facebook testing and verification research award .
any opinions findings and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.