using domain specific corpora for improved handling of ambiguity in requirements saad ezzini sallam abualhaija chetan aroraz mehrdad sabetzadehy lionel c. briand y snt centre for security reliability and trust university of luxembourg luxembourg zdeakin university geelong australia yschool of electrical engineering and computer science university of ottawa canada email fsaad.ezzini sallam.abualhaijag uni.lu chetan.arora deakin.edu.au fm.sabetzadeh lbriandg uottawa.ca abstract ambiguity in natural language requirements is a pervasive issue that has been studied by the requirements engineering community for more than two decades.
a fully manual approach for addressing ambiguity in requirements is tedious and time consuming and may further overlook unacknowledged ambiguity the situation where different stakeholders perceive a requirement as unambiguous but in reality interpret the requirement differently.
in this paper we propose an automated approach that uses natural language processing for handling ambiguity in requirements.
our approach is based on the automatic generation of a domain specific corpus from wikipedia.
integrating domain knowledge as we show in our evaluation leads to a significant positive improvement in the accuracy of ambiguity detection and interpretation.
we scope our work to coordination ambiguity ca and prepositional phrase attachment ambiguity paa because of the prevalence of these types of ambiguity in natural language requirements .
we evaluate our approach on industrial requirements documents.
these documents collectively contain more than requirements from seven distinct application domains.
over this dataset our approach detects ca and paa with an average precision of and an average recall of for cases of unacknowledged ambiguity .
the automatic interpretations that our approach yields have an average accuracy of .
compared to baselines that use generic corpora our approach which uses domain specific corpora has better accuracy in ambiguity detection and better accuracy in interpretation.
index terms requirements engineering natural language requirements ambiguity natural language processing corpus generation wikipedia.
i. i ntroduction natural language nl is the de facto medium for specifying requirements in industrial settings.
a key advantage of nl is that it facilitates shared understanding among different stakeholders who may have different backgrounds and expertise .
despite this advantage nl requirements are prone to a variety of quality issues one of the most notable of which is ambiguity .
ambiguity is an inherent phenomenon in nl occurring when a text segment is open to multiple interpretations .
ambiguity in requirements can lead to misunderstandings and inconsistencies among the stakeholders and this can have a potential negative impact on the overall success of a project .
handling ambiguity in requirements is challenging due to two main reasons.
first requirements specifications vary across domains and thus use a domain specific vocabulary .
joint first authorsthis has an impact on the likely interpretations of the requirements and consequently on what can be considered as ambiguous.
second ambiguity can be unacknowledged meaning that multiple readers being unaware of such ambiguity may have different interpretations for the same requirement.
in contrast to acknowledged ambiguity where the reader recognizes ambiguity unacknowledged ambiguity might lead to serious problems due to unconscious misunderstandings .
a fully manual analysis of ambiguity is expensive and also likely to overlook unacknowledged ambiguity.
there is therefore a need for effective automated ambiguity handling approaches that can help companies focus their often limited quality assurance budget on requirements that are more likely to be problematic.
ambiguity has been widely studied in the requirements engineering re literature .
both manual approaches based on reviews and inspections and automated approaches based on natural language processing nlp have been proposed for detecting ambiguity in requirements.
some recent works use domainspecific corpora for detecting terms that are likely to be ambiguous due to different meanings across domains .
current research on ambiguity in re as we elaborate later has three main limitations.
first the research focuses exclusively on detecting ambiguity and does not address automated interpretation for requirements in which no genuine ambiguity exists.
the lack of automated interpretation impedes further automated analysis e.g.
automated information extraction from requirements .
second existing methods for detecting domain specific ambiguity are restricted to identifying merely words with different meanings across domains and further require the domain of interest to be specified a priori.
finally while the negative consequences of unacknowledged ambiguity are known in the re literature the question of how accurately unacknowledged ambiguity can be detected through automated means has never been investigated empirically.
motivated by addressing the above limitations we propose an automated approach for improved ambiguity handling both ambiguity detection and interpretation in nl requirements.
ambiguity detection is concerned with finding the requirements that are genuinely ambiguous.
interpretation in contrast is concerned with providing the most likely meaning where the potential for ambiguity exists but where there is no ambiguity.
our approach incorporates domain knowledge ieee acm 43rd international conference on software engineering icse .
ieee by automatically generating domain specific corpora without any a priori assumption about the domain.
these corpora alongside a set of structural patterns and heuristics are used for handling ambiguity in requirements.
in our evaluation we analyze the impact of domain knowledge on ambiguity handling.
we further assess how well our automated approach can detect unacknowledged ambiguity in different domains.
our work in this paper concentrates on coordination ambiguity andprepositional phrase attachment ambiguity hereafter referred to as ca and paa respectively.
targeting these syntactic ambiguity types is motivated by their prevalence in nl requirements .
in our document collection as we will discuss later in the paper out of requirements are subject to ca analysis and to paa analysis.
within these human annotators acknowledged ambiguity or had different interpretations unacknowledged ambiguity in of the requirements.
coordination is a structure that links together two sentence elements called conjuncts using a coordinating conjunction e.g.
and or or .
ca can potentially occur when the two conjuncts are preceded or followed by a modifier .
the sentence could then be interpretable in two ways depending on whether only the conjunct next to the modifier is being modified or both conjuncts are being modified .
fig.
shows an example requirement r1 with two potential interpretations.
the first interpretation first read hereafter fr occurs when the modifier leo low earth orbit modifies the two conjuncts satellites and terminals fig.
a .
the second interpretation second read hereafter sr occurs when the modifier leo modifies satellites only fig.
b .
a b r1.
service availability shall measure the outage of leo satellites and terminals.
r1.
service availability shall measure the outage of leo satellites and terminals.first read second read leo stands for low earth orbit fig.
.
example of coordination ambiguity ca .
a prepositional phrase pp attachment is a pp preceded by a verb and a noun phrase .
virtually all pp attachments have the potential for paa because they could be interpretable in two ways depending on whether the pp is an adverbial modifier attached to the preceding verb or a noun attribute attached to the preceding noun phrase .
fig.
shows an example requirement r2 with two potential interpretations due to the presence of a pp attachment.
the first interpretation verb attachment hereafter va occurs when the pp with discrete tags is attached to the verb categorize fig.
a .
the second interpretation noun attachment hereafter na occurs when the pp is attached to the noun outages fig.
b .
r1 and r2 have the potential to suffer from ca and paa respectively.
the question is whether these are genuine ambiguities or merely situations that human experts can decisively interpret with little room for divergent interpretations.
existing techniques do not incorporate domain knowledge for providing a likely interpretation of ca instead they rely on frequency b a r2.
the outage management platform shall provide administrators with the ability to categorize outages with discrete tags.
r2.
the outage management platform shall provide administrators with the ability to categorize outages with discrete tags.noun attachmentverb attachmentfig.
.
example of prepositional phrase attachment ambiguity paa .
based computations derived from a generic corpus .
for example using existing techniques attempting to interpret the coordination in r1 would yield fr.
this interpretation is incorrect with domain knowledge the coordination would be interpreted as sr. as for the pp attachment in r2 existing techniques are unable to provide an interpretation although the attachment is interpretable as vawith domain knowledge.
contributions.
we take steps toward addressing the limitations outlined above.
our contributions are as follows we propose an automated approach for handling ca and paa in nl requirements.
our approach uses an ensemble of structural patterns and heuristics.
specifically we match requirements against a set of structural patterns leveraging and enhancing existing patterns in the literature.
in tandem we attempt to interpret all requirements with coordination and ppattachment structures using heuristics that are based on semantic morphological and frequency information.
some of these heuristics are novel others are borrowed from the literature and enhanced where necessary.
by combining these patterns and heuristics we attempt to tell apart the requirements that can be disambiguated via automated interpretation from the requirements that are genuinely ambiguous.
we devise a novel domain specific corpus generator.
without assuming any a priori knowledge about the domain we first automatically extract keywords from an input requirements document.
our corpus generator then assembles a large corpus of wikipedia articles relevant to the terminology and thus the domain of the given requirements document.
this automatically generated corpus is utilized for increasing the accuracy of the heuristics that rely on frequency based information.
for example the occurrence of the word capital in a requirements document within the aerospace domain differs in frequency and co occurring words from the same word occurring in a requirements document within the financial domain.
generating and using a domain specific corpus for ambiguity handling lies at the heart of our proposed approach.
we empirically evaluate our approach on industrial requirements documents.
these documents collectively contain requirements covering seven distinct application domains.
the ground truth for our evaluation was prepared by two trained annotators linguistics experts and non authors .
our results indicate that i our approach detects ca and paa with a precision of and recall of for cases of unacknowledged ambiguity ii the automatic interpretations by our approach have an average accuracy of and iii using domain specific corpora leads to substantial gains in accuracy for ambiguity handling improving detection by an average of and interpretation by an average of .
we have developed a tool named 1486maana which implements our approach for the domainspecific handling of ambiguity.
specifically maana detects requirements that potentially contain ca or paa.
the tool and the non proprietary requirements we use in our evaluation are publicly available at structure.
section ii discusses and compares with related work.
section iii presents our approach.
section iv describes our empirical evaluation.
section v addresses validity considerations.
section vi concludes the paper.
ii.
r elated work we focus on handling ca and paa in nl requirements.
our approach discussed in section iii builds on and further enhances the existing structural patterns and heuristics from the re and nlp literature for ca and paa .
our work to our knowledge is the first to bring these patterns and heuristics together for handling ca and paa.
below we position our work against the related work on ambiguity handling in both the re and nlp communities.
a. ambiguity handling in the re community ambiguity in requirements has been extensively studied from different perspectives including understanding the role of ambiguity in re analyzing the linguistic causes of ambiguity and ambiguity prevention .
automated ambiguity detection solutions in re are mainly based on matching nl requirements against predefined structural patterns using regular expressions nlp or both .
numerous approaches and tools have been proposed to this end .
in addition to these some recent works attempt to detect lexical ambiguity the situation where a word has different meanings depending on the domain by integrating domain knowledge from wikipedia .
ca detection has been investigated to some extent in the re literature.
chantree et al.
address ca detection using structural patterns and frequency based heuristics.
their work has been extended over the years with additional heuristics and for anaphora ambiguity detection i.e.
ambiguity due to multiple interpretations of pronouns.
though considered a prevalent ambiguity type in requirements to our knowledge automated handling of paa has not been previously studied in re.
our work differs from or enhances the above research in several ways.
first none of the existing approaches address the automated interpretation of potentially ambiguous coordination structures.
as for paa the topic has not been tackled in re before.
our approach handles both ca and paa by combining a broad range of structural patterns and heuristics.
second none of the existing approaches evaluate the detection of unacknowledged ambiguity.
we address this gap in our empirical evaluation.
third the existing automated methods for domain specific corpus generation from wikipedia are limited to a pre defined set of domains.
our approach in contrast can generate a corpus based on any given requirements document without knowing the underlying domain in advance.
fourthand finally industrial evaluations of ambiguity handling in re are scarce.
our evaluation contributes to addressing this gap by using a large industrial dataset.
b. ambiguity handling in the nlp community syntactic ambiguity types including ca and paa have been studied for a long time by the nlp community .
in an early work by goldberg ca is handled using conditional probabilities of word co occurrences.
pantel and lin present an unsupervised corpus based method for handling paa through a notion of contextual similarity.
agirre et al.
improve the accuracy of paa handling by integrating semantic similarity with syntactic parsing.
calvo and gelbukh propose querying the web for word co occurrence frequencies and use these frequencies for more accurate ppa handling.
in a similar vein nakov and hearst use structural patterns alongside statistical co occurrence frequencies gathered from the web for handling ca and paa.
in the context of ambiguity handling the use of domain knowledge in nlp is mostly directed at word sense disambiguation wsd in specific domains .
to this end wikipedia is a commonly used source of domain knowledge .
fragolli derives from wikipedia domainspecific corpora as resources for wsd.
similarly gella et al.
map manually defined topics in wordnet to wikipedia for generating domain specific corpora that can in turn be employed for wsd.
we are not aware of any work in the nlp community that uses domain specific corpora for handling either ca or paa.
instead in the existing nlp technologies e.g.
syntax parsing the handling of syntactic ambiguity ca and paa included is tuned over generic texts such as news articles.
these technologies therefore do not provide accurate results for ca and paa in a domain specific context.
as we show in section iv our approach which incorporates domain knowledge for handling ca and paa provides significant improvements over nlp technologies tuned over generic texts.
iii.
a pproach fig.
provides an overview of our approach which is composed of five steps.
the input to the approach is an nl requirements document hereafter rd.
in step we process rdusing an nlp pipeline.
in this step we further identify two subsets of the requirements in rd namely scandsp.
these two subsets contain all the requirements with coordination structures and all the requirements with pp attachments respectively.
in step we match the requirements in sc andspagainst structural patterns that indicate potential ca and paa respectively.
in step we generate a domainspecific corpus for rdby crawling wikipedia.
step can be bypassed if a representative corpus for rd s domain already exists through earlier applications of our approach to other requirements documents in the same domain .
in step we apply a set of heuristics to determine likely interpretations for the requirements in scandsp.
in step we classify into ambiguous and unambiguous the requirements in scandsp 1487ambiguousunambiguous preprocessing pattern matching application of heuristics 4ambiguity handling domain specific corpus generation 3final output nl requirements document rd fig.
.
approach overview.
by combining the results of step and step .
we note that steps and are independent i.e.
the output of neither step is an input to the other .
step is limited to a finite list of ca and paa structural patterns.
as we will explain later in this section the heuristics in step when compared to the patterns in step cover a wider spectrum of structures that have the potential for ca and paa.
combining results from both steps leads to better handling of ambiguity.
below we elaborate each step of our approach.
in the rest of this paper ambiguity refers to ca and paa exclusively.
a. preprocessing the nlp pipeline we use for preprocessing rdis depicted in fig.
.
this pipeline is a sequence of five nlp modules.
the first module in the sequence the tokenizer divides the input text into tokens such as words and punctuation marks.
rd tokenizer pos tagger lemmatizersentence splitter constituency parser preprocessed rd fig.
.
nlp pipeline.the sentence splitter splits the text into sentences.
the pos tagger assigns to tokens part of speech pos tags such as noun verb and adjective.
next is the lemmatizer which identifies the canonical form lemma for each token.
for example the lemma for bought is buy .
finally the constituency parser identifies the structural units of sentences e.g.
noun phrases verb phrases and prepositional phrases.
the results of the nlp pipeline are used in the next steps.
in this step step we further identify the two requirements subsets scandsp that should be subject to ca handling and paa handling respectively.
scis comprised of all the requirements in rdthat contain or and or both.
we note that only these two conjunctions can lead to ca .
spis comprised of all the requirements in rdthat contain a pp attachment .
requirements that contain a conjunction of interest i.e.
and or or as well as a pp attachment are included in both scandsp.
b. pattern matching in this step we analyze scandspto identify requirements that are likely to be ambiguous due to their syntactic structure.
table i lists our patterns for ca and paa.
of these catable i patterns for ambiguity detection ca and paa .
n1 n2 nn noun v verb adv adverb adj adjective dt determiner p preposition or.
v dt adj n1 p dt adj n2 4v dt adj n1 p n2 3v n1 p dt adj n2 v n1 p n2 paadt adj nn p v1 c v220adj1 c adj2 adj nnadv adj1 c adj2 adj1 c adj2 adv21 v1 c v2 p dt adj nn 19ca nn n1 c n2 nnn1 c n2 nnnn n1 c n2 nn p n1 c n2 adj nn n1 c n2 7n1 c n2 p nn v n1 c n24 adj n1 c n2 n1 c n2 v v1 c v2 advn1 c n2 p dt adj nn 17nn p v1 c v2 v1 c v2 p nn12 v1 c v2 to v adv v1 c v2 1814v1 c v2 nn v to v1 c v215 adj adj n1 c n2 10dt n1 c dt n2 p nnnn dt n1 c dt n2nn p dt n1 c dt n224 adj nn dt n1 c dt n2nn dt n1 c dt n2 nn adj dt n1 c dt n227 v n n1 p n25v dt adj n1 p n29v n n1 p dt adj n28v n n1 p dt adj n276 v n1 p dt adj n2 v dt adj n1 p dt adj n210 for ca the two conjuncts are in bold and the modifier is underlined.
for paa the verb and first noun are in bold and the second noun is underlined.
patterns and four paa patterns come from the literature .
the remaining patterns shaded blue in the table i.e.
ca patterns and paa patterns are novel.
the novel patterns were derived by analyzing a subset of the requirements in our dataset as we will precisely define in section iv c. specifically we analyzed the ambiguous requirements in the tuning portion of our dataset.
we match the patterns against the requirements in scand sp.
for pattern matching the unit of analysis is a text segment which is the part of a requirement that matches a given structural pattern from table i. a pattern suggesting ca matches a segment that contains a conjunction denoted as c linking two conjuncts marked in bold with a modifier underlined .
for example the matching segment in r1 corresponds to pattern for ca where leois the modifier and the conjunction andjoins the two conjuncts satellites and terminals.
we recall from section i that ca occurs when it is unclear whether a modifier is attached to both conjuncts fr or only to the closest conjunct sr .
a pattern suggesting paa matches a segment with a verb v followed by a first noun n both marked in bold followed by a pp which consists of a preposition denoted as p and a second noun n2 underlined .
for example the matching segment in r2 categorize outages with discrete tags corresponds to pattern for paa.
again we recall from section i that paa occurs when it is unclear whether the pp in question is an adverbial modifier attached to v va or a noun attribute attached to n1 na .
step identifies the segments from the requirements in sc andsp that match any of the patterns in table i. the matched segments are passed on to step .
c. domain specific corpus generation this step attempts to capture the domain knowledge underlying the input requirements document rd by crawling wikipedia.
fig.
shows the sub steps for generating a domainspecific corpus.
we elaborate these sub steps next.
extract keywords.
step .
builds on an existing automated requirements glossary extraction approach by arora et al.
.
1488wikipedia articlesquery wikipediaextract keywords .1domain specific corpus generation wikipedia .2fig.
.
domain specific corpus generation.
we begin by automatically extracting the list of glossary terms from rd and thereafter select the top k most frequent keywords from the list.
the optimal value of kis tuned in section iv d. for example the keywords extracted from r1 include leo leo satellites satellites and terminals .
these keywords are used in the next step.
query wikipedia.
step .
implements a query engine for identifying wikipedia articles that are relevant to the keywords resulting from step .
.
these articles form the basis of our domain specific corpus.
we begin by retrieving matching wikipedia articles for each keyword.
an article is considered a match if the keyword in question contains or is contained in the title of the wikipedia article.
for instance the wikipedia article titled satellite navigation is a match for the keyword satellite based navigation .
if the above condition is not met no matching wikipedia article is retrieved.
next we broaden the domain information captured in our corpus by taking advantage of the hierarchical category structure of wikipedia .
in wikipedia s hierarchy each category can contain articles and nested sub categories.
for a matching article we retrieve all the articles in the same category and all the articles in the descendant sub categories.
for example the satellite navigation article as shown in fig.
is classified under an identically named wikipedia category https en.wikipedia.org wiki category satellitenavigation accessed .
we retrieve all articles in this category and in all its descendants e.g.
one descendant being geocaching .
so increases topic coherence meaning that the articles included in the corpus are all indeed relevant to the domain under analysis.
next to make our domain specific corpus applicable to other requirements documents from the same domain we attempt to increase the coverage of our corpus.
in particular we consider the categories in the wikipedia category graph that are directly connected to the category of the matching article.
for instance the category navigation in fig.
is directly connected to satellite navigation we therefore include articles listed under navigation and its descendant sub categories.
we note that during the creation of a corpus we consider only the categories whose number of articles is below a threshold this is both to keep the computation time reasonable and to avoid including large and generic categories in the corpus.
we discuss the tuning of in section iv d. the result of this step step .
is a body of raw text from wikipedia articles.
this extracted body of text is our domainspecific corpus hereafter denoted as d. this category has the following sub categories acategory satellite navigation gautomotive navigation systems p geocaching p l location based software c p pages in category satellite navigation the following pages are in this category satellite navigation a automatic v ehicle location c comparison of satellite navigation softwar e d satellite navigation device dilution of pr ecision navigation categories radio navigation navigation satellites matching articlecategory number of sub categories pagessub category with articles directly connected category we refer to a page in wikipedia p as article fig.
.
example of category structure in wikipedia.
d. application of heuristics step attempts to provide likely interpretations for the requirements in scandsp.
we use six heuristics denoted as c1 c6 for interpreting coordination structures and four heuristics denoted asp1 p4 for interpreting pp attachment structures.
out of these ten heuristics eight c c5for ca andp1 p3for paa are borrowed from the literature.
the other two c 6andp4 are novel but based on a very intuitive idea applying constituency parsing which has coordination and pp attachment interpretation built into it.
similar to step section iii b we operate at a segment level.
compared to the patterns in step heuristics cover a wider spectrum of segments that have the potential to be ambiguous.
the heuristics are triggered by the presence of any coordination structure an and or conjunction two conjuncts and a modifier and any pp attachment structure a verb a noun and a pp .
for example had r2 contained an extra adjective categorize outages with standard discrete tags r2 would not have been detected by the patterns of table i but would have been picked up by the heuristics and attempted for interpretation.
several heuristics in our approach are corpus based i.e.
require information about the co occurrence frequencies of the words.
we therefore transform the wikipedia articles from step to an n grams table with n ranging from to .
we set the upper limit to motivated by the use of grams in google s well known web1t database this database is utilized in a wide variety of nlp applications .
1489table ii excerpt of grams table .
grams grams unigrams navigation system satellite orbit 234count words system satellite orbit 26599navigation satellite navigation system low earth orbit ... ... grams 8satellite power system concept development leo sun synchronous receiver satellites8... ... grams89global navigation satellite system geosynchronous satellite launch vehicle27table ii shows a very small excerpt of a grams table generated for the satellite domain.
the frequencies used by the heuristics are the normalized values of the cooccurrence counts listed in the 5grams table .
for example the co occurrence frequency of satellite orbit is computed as .
heuristics for ca.
a segment insccontains a conjunction c two conjuncts conjunct and conjunct and a modifier mod .
ca heuristics attempt to interpret a segment in scas either frorsr.
if a heuristic cannot interpret a segment it returns a designated value not interpretable ni .
as we explain below four of the ca heuristics c 1andc3 c5 require pre defined thresholds denoted as i. these thresholds come from the existing literature.
we tune the thresholds empirically in section iv d. to illustrate the heuristics in this section we already use the tuned ivalues and .
c1 coordination frequency computes the co occurrence frequency of conjunct 1and conjunct 2in our domain specific corpus d .
we consider the co occurrence frequency of the conjuncts irrespective of their order.
for example for r1 we consider among other possible combinations the cooccurrence frequency of terminals and satellites and satellites or terminals .
the intuition is that if the two conjuncts co occur frequently in d they can be regarded as one syntactic unit and thus are both modified by mod in turn favoring fr.c1returns frif the resulting frequency is greater than a threshold and niotherwise.
in r1 c1returns fr.
c2 collocation frequency compares the co occurrence frequency of conjunct 1andmod against the frequency of conjunct 2andmod.
collocation is a recurrent combination of two consecutive words in a large corpus .
for example the words red and wine would be considered collocated while great and wine would not.
the intuition is that a collocation of the mod and the conjunct closer to it is likely to indicate a syntactic unit thus favoring sr. using collocations red wine and cheese can be interpreted as srwhile great wine and cheese would not be interpretable ni .c2returns srif the collocation frequency of mod and the closer conjunct is greater than that of mod and the farther conjunct and ni otherwise.
in r1 c2returns sr. c3 distributional similarity measures the contextual similarity of conjunct 1and conjunct i.e.
how frequently the conjuncts appear in similar contexts.
for example in the context of requirements documents about satellite systems the terms satellite and navigation have a higher distributionalsimilarity than satellite and investment .
the intuition is that conjuncts with high distributional similarity can be regarded as one unit thus favoring fr.c3returns frif the distributional similarity of the conjuncts is greater than and niotherwise.
in r1 c3returns ni.
c4 semantic similarity measures the similarity between conjunct 1and conjunct 2based on their meanings in wordnet.
the intuition is that conjuncts with high semantic similarity can be regarded as one unit thus favoring fr.c4returns fr if the semantic similarity is greater than and niotherwise.
in r1 c4returns fr.
c5 suffix matching examines the number of shared trailing characters suffixes of conjunct 1and conjunct .
for example installation and configuration share five trailing characters.
suffixes are used to change the meaning e.g.
able in noticeable or grammatical property e.g.
ed in closed of a given word .
hence matching suffixes provides a cue about how words are semantically or syntactically related .
the intuition is that conjuncts with the same number of trailing characters are likely to be a single unit thus favoring fr.c5 returns frif the conjuncts share trailing characters greater than and niotherwise.
in r1 c5returns ni.
c6 coordination syntactic analysis is based on applying constituency parsing to the requirement in which the coordination segment of interest appears and then obtaining from the parse tree the interpretation of the parser for the segment.
c6returns frorsras per the parsing results and niif the parser fails to parse the requirement.
in r1 c6returns fr.
heuristics for paa.
a segment in spcontains a verb v and a following noun n followed by a preposition p and another noun n .
paa heuristics attempt to interpret a segment as either vaorna as explained below.
if a heuristic cannot interpret a segment it returns not interpretable ni .
p1 preposition co occurrence frequency compares the frequency of poccurring with vagainstpoccurring with n1.
the intuition is that based on the co occurrence frequency ofv orn1 andp pp can be regarded as an adverbial modifier leading to vaor a noun attribute leading to na.
for example in the segment provide user with a valid option the preposition with frequently follows the verb provide thus leading to a vainterpretation.
precisely p1returns vaif the co occurrence frequency of vandpis strictly larger than that ofn1andp.p1returns naif the converse is true.
if there is a tie between the frequencies e.g.
when the frequencies are zero due tov n1orpbeing absent from the corpus p1returns ni.
in r2 p1returns na.
p2 prepositional phrase pp co occurrence frequency has a similar definition and intuition to p1 the only difference being that we consider the entire pp i.e.
pandn2 instead of justp.
for example consider the segment provide used for illustratingp1.p2would return vabecause the pp with a valid option has a higher co occurrence frequency with v provide than with n1 user .p2 s precise definition is easy to extrapolate from the definition of p1and is omitted for space.
in r2 p2returns na.
p3 semantic class enrichment utilizes the semantic classes in wordnet that group words with similar meanings.
for example wordnet puts scissors and knife under the same semantic class namely tool .
p3is applied after all the segments in sphave been already processed by p1and p2.
specifically p3attempts to find an interpretation for the segments that have been deemed as niby bothp1andp2.
for any such segment x p3checks whether there is some segmentyinspwhich has been interpreted as vaorna by eitherp1orp2 and which shares a semantic class with x. by sharing a semantic class we mean that xandycontain nouns or verbs that fall under the same wordnet semantic class.
if yhas been interpreted as va respectively na and xshares a verb class respectively a noun class with y then p3interpretsxasva respectively na .
the intuition is as follows segments that contain words with similar meanings are likely to have the same interpretation .
for instance a segment x offer operator with a valid option is interpreted as vabyp3if there is a segment y provide user with a valid option already interpreted as va byp2.
this is because the verbs provide and offer have the same wordnet semantic class possession .
p4 attachment syntactic analysis has the same intuition and definition asc6 except that it applies to a pp attachment segment.p4returns vaorna as per the parsing results.
p4 returns niif the parser fails.
in r2 p2returns va. combination of heuristics.
to produce a single interpretation for each segment we combine through voting the results of the heuristics for each ambiguity type c c6for ca andp1 p4for paa .
we consider two voting methods majority voting andweighted voting .
in majority voting all heuristics contribute equally and the resulting interpretation is based on the majority.
in weighted voting the contribution of each heuristic is weighted differently.
the weights are tuned in section iv d. in r1 majority voting yields fr while weighted voting using the tuned weights of section iv d yields sr. we compare the accuracy of both voting methods in section iv.
step partitions scandspinto two subsets each the first subset contains the interpretable segments fr orsrfor segments in sc and vaornafor segments in sp the second subset contains the segments that are not interpretable.
these subsets are passed on to step for ambiguity handling.
e. handling ambiguity in this final step we classify into ambiguous andunambiguous the segments in scandsp.
this classification is based on the results of steps and in our approach see fig.
.
a segment xis classified as ambiguous if either of the following two conditions is met a xmatches some pattern in step or b xis deemed as not interpretable ni in step .
any segment that is not classified as ambiguous would be unambiguous.
we say that a requirement is ambiguous if it has some ambiguous segment otherwise we say the requirement is unambiguous.
our empirical evaluation discussed next is at a segment level rather than a requirement level because eachrequirement may contain multiple segments that are subject to ambiguity analysis.
iv.
e valuation in this section we empirically evaluate our approach.
a. research questions rqs our evaluation addresses four research questions rq1.
what configuration of our approach yields the most accurate results for ambiguity handling?
our approach can be configured in a number of alternative ways the alternatives arise from the choices available for the selection of patterns section iii b the use of default versus optimal thresholds for ca heuristics section iii d and the voting method for combining the heuristics section iii d .
rq1 identifies the configuration that produces the best overall results.
rq2.
how effective is our approach at detecting unacknowledged ambiguity?
as discussed in section i unconscious misunderstandings may occur due to unacknowledged ambiguity.
using the best configuration from rq1 rq2 assesses the effectiveness of our approach in automatically detecting unacknowledged ambiguity.
rq3.
how accurate are the interpretations provided by our approach?
while the exact interpretation of a segment found by our approach fr orsrfor segments in sc and vaornafor segments in sp has no bearing on how we tell apart unambiguous cases from ambiguous ones we want the interpretations to be as correct as possible.
a correct interpretation is important both for reducing manual work in case the analysts choose to vet the automatic interpretations and also for ensuring that any subsequent automated analysis over the requirements e.g.
automated information extraction will produce high quality results.
rq3 examines the accuracy of the interpretations provided by our approach.
rq4.
does our approach run in practical time?
rq4 studies whether the execution time of our approach is practical.
b. implementation we have implemented our approach in java.
the implementation has lines of code excluding comments.
the nlp pipeline of step is implemented using dkpro .
for implementing step we use the english wikipedia dump timestamped .
we access the data in this dump using the jwpl library .
in step we transform the raw text of wikipedia articles into an n grams table using the jweb1t library this enables us to compute our interpretation heuristics more efficiently.
we use stanford parser to obtain the parse trees required by heuristics c6andp4.
forc4 we compute semantic similarity using the resnik measure as implemented by the ws4j library .
for implementation availability please see the footnote on page .
c. data collection our data collection has human experts examine and annotate potential ca and paa in industrial requirements.
we collected 1491table iii data collection results .
sc sptotal domain aerospace automative defense digitalization medicine satellite security total requirements rds unacknowledged acknowledged segments requirements unambiguous unacknowledged acknowledged unambiguous segments requirements our data from requirements documents rds written in english covering seven different application domains.
data collection was performed by two third party annotators nonauthors with expertise in linguistics.
the first annotator anna pseudonym has a masters degree in multilingualism.
anna had previously completed a three month internship in re.
the second annotator nora pseudonym has a masters degree in it quality management.
nora has a professional certificate in english translation.
both annotators underwent a half day training on ambiguity in re.
the two annotators produced their annotations over a six month span during which they declared a total of and hours respectively.
the annotators were then tasked with independently labeling with frorsrall the and or coordination segments inscand labeling with vaornaall the pp attachment segments in sp.
the annotators were specifically instructed to ascribe an interpretation to a segment only when they were confident about their interpretation.
when in doubt the annotators labeled the segment in question as ambiguous.
an agreement between annotators is observed for segment x when both of them either find x ambiguous or interpret x the same way.
any other situation is a disagreement .
using cohen s kappa metric we obtain an inter rater agreement of .
suggesting fair agreement .
to examine the sources of disagreement we further analyze the cases where x is deemed ambiguous i.e.
acknowledged ambiguity .
for these cases we obtain substantial agreement indicating that most disagreements are due to different interpretations i.e.
unacknowledged ambiguity .
as stated earlier in the paper unacknowledged ambiguity is believed to be prevalent in requirements .
the analysis discussed above provides empirical evidence for this belief.
we constructed our ground truth as follows i any segment labeled as ambiguous by at least one annotator is a case of acknowledged ambiguity ii any segment labeled with different interpretations by the annotators is a case of unacknowledged ambiguity and iii any segment labeled with the same interpretation by both annotators is unambiguous.
we motivate our definitions of acknowledged and unacknowledgedambiguity by considering what might happen during a manual inspection where a team would typically be involved.
if a segment is ambiguous enough for someone not necessarily everyone to raise a concern then this segment is likely to be further discussed by the team acknowledged .
the situation is different for unacknowledged ambiguity.
in reality and under time pressure the analysts are unlikely to spell out their interpretations when they feel there is no ambiguity.
consequently the disagreement remains hidden unacknowledged .
table iii provides overall statistics about our data collection showing for each domain the number of rds the total number of requirements and the number of requirements and segments inscandsp.
the table further lists the number of ambiguous segments grouped into acknowledged and unacknowledged and the number of unambiguous segments.
we observe from table iii that out of the total of segments analyzed by the annotators are ambiguous and the remaining are unambiguous.
in the ambiguous segments the proportion of segments with unacknowledged ambiguity is significantly higher than the proportion of segments with acknowledged ambiguity .
we note that repeated segments constitute a relatively small fraction of the ground truth for ca and for paa.
these repetitions are not disproportionately concentrated in one group.
more precisely in the case of ca repetitions are unambiguous are acknowledged and are unacknowledged.
for paa repetitions are unambiguous are acknowledged and are unacknowledged.
since there is no disproportionate concentration of occurrences repetitions have no major bearing on our findings.
we set aside of our ground truth for parameter tuning as we will discuss in section iv d. we refer to this subset of the ground truth as t. the remaining of the ground truth is referred to as e. we useefor answering all the rqs except rq4 which is answered over t e. the tuning set t consists of six rds from six domains with a total of requirements and representing and of the coordination and pp attachment segments respectively.
we selected one rdfrom each domain this was done in a way that the selected document would be as close as possible to containing of the requirements we had in each domain.
we did not select for tuning any documents from the domain of medicine since we had only one rdfrom this domain.
d. parameter tuning tuning involves two groups of parameters parameters for generating a domain specific corpus section iii c and parameters associated with the heuristics section iii d .
both groups of parameters are tuned with the goal of maximizing the overall accuracy of the interpretation heuristics.
note that tuning is performed exclusively over t see section iv c .
parameters for corpus generation.
generating a domainspecific corpus requires tuning the maximum number of keywords k to select from an input rdand the maximum number of articles in a given category in wikipedia.
for each rdint we generate a domain specific corpus.
to tune 1492k we experiment with five values at regular intervals between .
values of koutside this range are undesirable as they result in a corpus that is either too small for k or too large fork .
a suitably large corpus is necessary for accurately estimating the co occurrence frequencies of words in a specific domain .
building and using a corpus that is too large would be time consuming and more importantly would defeat the goal of being domain specific.
using a corpus that is too small would simply be ineffective.
for tuning we experiment with values in the range of in intervals of .
larger categories i.e.
are too generic and smaller ones with are already covered by as is the upper bound for the number of articles in a category.
for optimizing kand we use grid search .
the resulting optimal values are k and .
parameters for heuristics.
applying the interpretation heuristics requires tuning four thresholds 5respectively for heuristicsc1 c3 c5.
for using the weighted voting method we further need to tune the weights of all the heuristics.
we note that the thresholds for the heuristics have been introduced and tuned in the existing literature albeit for generic texts .
we re tune these thresholds to better capture co occurrence frequencies in the context of requirements.
the threshold values from the existing literature are hereafter referred to as default.
we experiment with regular intervals in the range of for tuning and .
to tune we investigate suffixes of lengths to e.g.
the suffix ation has a length of five.
we use random search to optimize the thresholds because the search space is too large for grid search.
the resulting optimal thresholds are and .
for determining the weights of the heuristics we first apply each heuristic individually on t. the weight of a given heuristic is determined by its success in providing interpretations for the segments.
in our experiments the weights of heuristics in descending order for ca are forc5 forc2 forc1 forc4 forc6and forc3 and the weights for paa are forp1 forp2and forp4.
p3is not a standalone heuristic and is thus not weighted.
these weights reflect the contribution of the heuristics in weighted voting to produce a final interpretation for a segment.
e. evaluation procedure we answer our rqs through the following experiments.
expi.
this experiment answers rq1.
we determine the optimal configuration for ambiguity handling by comparing the output of our approach against e. for evaluating the configurations we define a true positive tp as a detected ambiguous segment a true negative tn as an unambiguous segment marked as such a false positive fp as a misclassified unambiguous segment and a false negative fn as a misclassified ambiguous segment.
we compute accuracy a as tp tn tp tn fp fn precision p as tp tp fp and recall r astp tp fn .
we consider eight alternative configurations for our approach denoted as v1 v8.
these alternatives are induced bythree binary decisions.
the first decision is whether to use the collected or the enhanced patterns in step of our approach see table i .
the second decision is whether in step we should use for the thresholds the default or the optimal values from section iv d .
and the third decision is whether the method for combining the heuristics is majority orweighted voting see section iii d .
to analyze the impact of using domain specific corpora we compare our approach against baselines denoted as b1 b8 with similar configurations but using a generic corpus the british national corpus .
to run expi we first need to generate seven corpora one for each application domain in our ground truth see table iii .
six of these corpora are reused from section iv d. the last one for the domain of medicine is generated based on the single rdwe have in our dataset for this domain.
except for the domain of medicine expi provides an implicit assessment of how reusable a domain specific corpus is being generated from one rdand reused for other rds from the same domain.
expii.
this experiment answers rq2.
given the optimal configuration of our approach from expi expii assesses how well our approach can detect unacknowledged ambiguity in different domains.
in expii we compute recall r similar to expi but limiting the evaluation to only the segments with unacknowledged ambiguity in e. the corpora used in expii are the same as those in expi.
expiii.
this experiment answers rq3.
we evaluate the interpretations provided by our approach for the segments classified as unambiguous fr orsrfor segments in sc and vaornafor segments in sp .
specifically expiii compares the interpretations produced by our approach against the interpretations of unambiguous segments in e reporting the ratio of the correctly interpreted segments i.e.
accuracy .
the corpora used in expiii are the same as those in expi and expii.
we further compare our approach against stanford parser one of the commonly used tools for interpreting syntactic ambiguity .
expiv .
this experiment answers rq4 by running the best configuration from rq1 over t e. the experiment is done on a laptop with a .
ghz cpu and 16gb of memory.
f .
answers to the rqs rq1.
table iv shows the results of expi on e .
to determine the optimal configuration of our approach we investigate among all configurations the factors that cause the most variation in accuracy.
we do so by performing regression tree analysis tree not shown .
the most influential factor for both ca and paa as per regression tree analysis is the choice of domain specific versus generic corpus.
the configurations that use a domain specific corpus v1 v8 have an average gain in accuracy of over the configurations that use a generic corpus b1 b8.
this observation clearly highlights the importance of domain knowledge in ambiguity handling.
among v1 v8 using enhanced patterns has a considerable impact on detecting ca.
compared to the configurations with collected patterns v v4 the configurations with enhanced patterns v v8 lead to an average gain of in accuracy 1493table iv results of ambiguity handling rq1 .
v8v4v1 v6v5v2 v3 v7ca paa accuracy a precision p and recall r in percentage domain specific corpus british national corpus .
.
.
optimal .
weighted enhanced .
.
.
enhanced .
.
majority .
.
optimal .
.
.
weighted .
enhanced .
.
default .
.
.
enhanced .
majority .
.
default .2weighted .
collected .
.
.
.
.
optimalcollected optimal .
.
.
.
.
.
majoritycollected .
.
default .
.
.
.
weightedcollected .
.
default .
.
.
.
majority b7b6b5b4b3b2b1 b884.
.6a .
.
.
.
.
majorityenhanceddefault .
.9a .
.
optimalcollected .
.1optimalthresholds p optimal .5weighted .6majority .
.
.
.
.
optimalcollectedpatterns .1majorityweighted .
.
.
.5default default enhanced78.
.0voting r defaultcollected .
.9r .
.
.
.
.
weighted82.
.
.
.9p .
enhanced86.
.
.3majority collected81.
.
weighted .
.
.
.
enhanced table v unacknowledged ambiguity detection using v8 rq2 .
tp fn number of true positives and false negatives r recall in percentage .
.
.
.
.
.
.
.
.8fntpfntp r r .
.
.
.
.8summary domain aerospace automative defense digitalization medicine satellite securityca paa and in recall for a minor drop in precision.
compared to collected patterns enhanced patterns do not improve the detection of paa but do not perform any worse either.
thus we choose the enhanced patterns over the collected ones.
with respect to the thresholds for the heuristics the configurations with optimal thresholds v v8 outperform those with default thresholds v v6 by .
in terms of accuracy.
noting that our parameter tuning used documents from six different application domains we believe that the optimal thresholds are more suitable in an re context than the default ones based on generic texts.
we note that overall the accuracy of ambiguity handling shows little sensitivity to the choice of voting method.
however as highlighted in table iv v8 weighted voting is slightly more accurate than v7 majority voting .
for the subsequent rqs we select v8as the bestperforming configuration of our approach with enhanced patterns optimal thresholds and weighted voting.
to be able to perform a thorough error analysis section iv g we run v8on the entire dataset t e .
this yields a precision and recall of .
and .
for ca and .
and .
for paa respectively.
we observe that for each metric the overall results are only marginally better than what was reported over e. this provides confidence that our tuning section iv d did not overfit.rq2.
the results of expii obtained from running v8 the best configuration from rq1 oneare shown in table v. overall our approach detects unacknowledged ambiguity with an average recall of .
for ca and .
for paa.
our error analysis section iv g examines missed cases of unacknowledged ambiguity in the entire dataset t e .
over the entire dataset v8detects unacknowledged ambiguity with an average recall of .
for ca and .
for paa.
rq3.
the interpretations provided by v8for the segments in scandsp when restricted to e have an average accuracy of .
and .
respectively.
the accuracy of the approach on the entire dataset is marginally higher by an average of .
we examine interpretations errors in section iv g. applying the stanford parser to scandsp when restricted toe yields interpretations with an average accuracy of .
and .
respectively.
in an re context and in comparison to the stanford parser the integration of domain knowledge increases the interpretation accuracy of coordination and ppattachment structures by an average of .
rq4.
executing steps and of our approach takes .
milliseconds per requirement.
step is performed only when a suitable corpus is absent i.e.
when no corpus has been generated before for the domain of a given rd or when the domain of the rdis difficult to ascertain.
across the seven corpora we generated for answering rq1 the average execution time was minutes standard deviation minutes .
to be able to generate corpora there is a one time overhead of hours this is to set up a query engine over wikipedia see step .
in section iii c .
once set up this query engine does not have to be rebuilt unless one wants to switch to a different edition of wikipedia.
with a corpus at hand execution time is dominated by the computation of the frequencies required by the heuristics of step .
this on average takes .
seconds for a requirement in scand .
seconds for one in sp.
processing the requirements in sctakes longer because there are more corpus based heuristics for ca than paa.
non corpus based heuristics take negligible time.
excluding corpus generation the largest document in our dataset took minutes to process.
this document had requirements with coordination and pp attachment segments.
such an execution time is practical for offline e.g.
overnight processing.
with regard to using our approach interactively we observe that at any point in time an analyst likely works on only a small part of a large document.
for interactive use ambiguity handling can be localized to the document fraction e.g.
page that the analyst is reviewing.
g. error analysis in this section we analyze the root causes of the errors made by our approach v on the entire dataset t e .
errors in rq1 and rq2.
out of segments table iii our approach missed ambiguous segments of which are unacknowledged.
these errors can be explained as follows.
coverage of patterns segments do not match any pattern in table i. for example the segment register the 1494microservice in the operations server matches no paa pattern.
one can avoid such errors by expanding the pattern set.
however our experiments indicate that so comes at the cost of a large number of fps and is thus not worthwhile.
nlp errors segments are missed due to mistakes by the nlp pipeline .
for example support in the segment support doctors in the icu is erroneously tagged as a noun this results in the segment to not match any of our patterns.
such nlp mistakes are hard to avoid .
errors in rq3.
we found two causes for interpretation errors.
interpretation errors by the heuristics segments fall under this class of errors having to do with situations where the combination of heuristics provide a wrong interpretation or return not interpretable ni where there is indeed an interpretation.
for example for the segment pulse width and duration the resulting interpretation is sr although it should befr.
one can try to address individual interpretation errors by adjusting the weights of the heuristics.
however so will have a negative overall impact by causing other errors.
document specific abbreviations segments are misinterpreted due to abbreviations.
an abbreviation that is specific to a document can mislead frequency computations if the abbreviation has a homonym or is not found in the corpus at all.
for example moc in moc operator and component stands for monitoring and control in one of our rds from the satellite domain.
this abbreviation however matches mars orbiter camera in the corpus that we generate for this domain.
such mismatches can be reduced through abbreviation disambiguation .
we leave this for future work.
h. discussion about usefulness as shown by table iii ambiguity was acknowledged by the annotators in only of the cases.
the remaining were unacknowledged.
in practice even if the analysts perform a manual review under time pressure and outside an evaluation setting they will likely only examine what at least one analyst finds to be ambiguous and thus miss out on the cases where they unconsciously disagree about the interpretation.
we believe that the main benefit of our automated approach is in bringing of the cases of unacknowledged ambiguity to the attention of the analysts see rq2 .
this is achieved while maintaining a high overall precision meaning that the analysts will spend a small fraction of their manual effort over false positives.
while user studies remain essential for establishing usefulness our good accuracy results suggest that our approach has the potential to be helpful in practice.
v. v alidity considerations internal validity.
bias is a potential threat to the internal validity of our evaluation.
to mitigate this threat the authors had no involvement in the annotation activities.
instead two thirdparty annotators who had no knowledge of our technical approach independently annotated the dataset.
further we made a strict separation between the data used for defining patterns and tuning and the data used for assessing effectiveness.construct validity.
an individual requirement can potentially have multiple instances of ca or paa.
to ensure that this possibility is properly reflected in our metrics we defined accuracy precision and recall at the level of segments rather than whole requirements.
external validity.
our evaluation builds on industrial requirements documents covering seven different domains.
the promising results obtained across these domains provide a measure of confidence about the generalizability of our approach.
this confidence is further strengthened by the fact that our approach can adapt itself to new domains via the automatic generation of domain specific corpora.
due to this characteristic we are optimistic that our approach will be able to achieve comparable results in other domains.
that said future case studies would help further improve external validity.
vi.
c onclusion in this paper we proposed an automated approach for improving the handling of coordination ambiguity ca and prepositional phrase attachment ambiguity paa .
the main novelty of our approach is in automatically extracting domainspecific corpora from wikipedia and utilizing them for increasing the accuracy of ca and paa handling in requirements documents.
we conducted a large scale evaluation of our approach using more than industrial requirements from seven different application domains.
our results indicate that our approach can detect ca and paa with an average precision of and an average recall of .
the results further indicate that employing domain specific corpora has a substantial positive impact on the accuracy of ca and paa handling.
specifically over our dataset we observed a improvement in accuracy when compared against baselines that use generic corpora.
while our work is motivated by improving the quality of systems and software requirements our technical solution is also novel from an nlp standpoint.
our solution thus has the potential to be useful over other types of textual documents within and beyond software engineering.
in future work we would like to integrate our ambiguity handling approach with automated techniques for extracting structured information from requirements specifications.
the motivation for so is to increase the quality of information extraction by more accurately interpreting coordination and prepositional phrase structures.
another direction we would like to explore in the future is to use deep learning to complement or as an alternative to our current approach.
acknowledgement.
this work has received funding from luxembourg s national research fund fnr under the grant bridges18 is and from nserc of canada under the discovery discovery accelerator and crc programs.
we are grateful to the research and development team at qra corp. canada for very valuable insights and assistance.
1495references f. de bruijn and h. dekkers ambiguity in natural language software requirements a case study in proceedings of the 16th working conference on requirements engineering foundation for software quality refsq .
k. pohl requirements engineering 1st ed.
springer .
d. berry e. kamsties and m. krieger from contract drafting to software specification linguistic sources of ambiguity a handbook .
.
available dberry handbook ambiguityhandbook.pdf s. piantadosi h. tily and e. gibson the communicative function of ambiguity in language cognition vol.
no.
.
k. pohl and c. rupp requirements engineering fundamentals 1st ed.
rocky nook .
a. ferrari and a. esuli an nlp approach for cross domain ambiguity detection in requirements engineering automated software engineering vol.
no.
.
f. chantree b. nuseibeh a. de roeck and a. willis identifying nocuous ambiguities in natural language requirements in proceedings of the 14th ieee international requirements engineering conference re .
v .
gervasi a. ferrari d. zowghi and p. spoletini ambiguity in requirements engineering towards a unifying framework in from software engineering to formal methods and tools and back.
springer .
e. kamsties d. berry and b. paech detecting ambiguities in requirements documents using inspections in proceedings of the 1st workshop on inspection in software engineering wise .
n. kiyavitskaya n. zeni l. mich and d. berry requirements for tools for ambiguity identification and measurement in natural language requirements specifications requirements engineering vol.
no.
.
f. dalpiaz i. schalk and g. lucassen pinpointing ambiguity and incompleteness in requirements engineering via information visualization and nlp in proceedings of the 24th working conference on requirements engineering foundation for software quality refsq .
p. spoletini a. ferrari m. bano d. zowghi and s. gnesi interview review an empirical study on detecting ambiguities in requirements elicitation interviews in proceedings of the 24th working conference on requirements engineering foundation for software quality refsq .
h. yang a. de roeck v .
gervasi a. willis and b. nuseibeh analysing anaphoric ambiguity in natural language requirements requirements engineering vol.
no.
.
f. dalpiaz d. dell anna f. aydemir and s. cevikol requirements classification with interpretable machine learning and dependency parsing in proceedings of the 27th ieee international requirements engineering conference re .
s. mishra and a. sharma on the use of word embeddings for identifying domain specific ambiguities in requirements in proceedings of the 27th ieee international requirements engineering conference workshops rew .
d. toews and l. van holland determining domain specific differences of polysemous words using context information.
in proceedings of the 25th working conference on requirements engineering foundation and software quality workshops refsqw .
v .
jain r. malhotra s. jain and n. tanwar cross domain ambiguity detection using linear transformation of word embedding spaces in proceedings of the 26th working conference on requirements engineering foundation and software quality workshops refsqw .
c. arora m. sabetzadeh l. briand and f. zimmer extracting domain models from natural language requirements approach and industrial evaluation in proceedings of the acm ieee 19th international conference on model driven engineering languages and systems models .
a. sleimi n. sannier m. sabetzadeh l. briand and j. dann automated extraction of semantic legal metadata using natural language processing in proceedings of the 26th ieee international requirements engineering conference re .
c. sch utze pp attachment and argumenthood mit working papers in linguistics vol.
no.
.
p. engelhardt and f. ferreira processing coordination ambiguity language and speech vol.
no.
.
b. strang modern english structure 2nd ed.
edward arnold .
f. chantree a. kilgarriff a. de roeck and a. willis disambiguating coordinations using word distribution information in proceedings of the 5th international conference on recent advances in natural language processing ranlp .
m. goldberg an unsupervised model for statistically determining coordinate phrase attachment in proceedings of the 37th annual meeting of the association for computational linguistics acl .
p. resnik semantic similarity in a taxonomy an information based measure and its application to problems of ambiguity in natural language journal of artificial intelligence research vol.
no.
.
p. nakov and m. hearst using the web as an implicit training set application to structural ambiguity resolution in proceedings of the 5th conference on human language technology and empirical methods in natural language processing hlt .
a. de roeck detecting dangerous coordination ambiguities using word distribution in proceedings of the 6th international conference on recent advances in natural language processing ranlp .
s. tjong and d. berry can rules of inferences resolve coordination ambiguity in natural language requirements specification?
in proceedings of the 13th workshop on requirements engineering wer .
h. yang a. willis a. de roeck and b. nuseibeh automatic detection of nocuous coordination ambiguities in natural language requirements inproceedings of the 10th ieee acm international conference on automated software engineering ase .
s. tjong and d. berry the design of sree a prototype potential ambiguity finder for requirements specifications and lessons learned inproceedings of the 19th working conference on requirements engineering foundation for software quality refsq .
a. kilgarriff thesauruses for natural language processing in proceedings of the 1st international conference on natural language processing and knowledge engineering nlpke .
h. yang a. de roeck a. willis and b. nuseibeh a methodology for automatic identification of nocuous ambiguity in proceedings of the 23rd international conference on computational linguistics coling .
a. okumura and k. muraki symmetric pattern matching analysis for english coordinate structures in proceedings of the 4th conference on applied natural language processing anlp .
e. agirre t. baldwin and d. mart nez improving parsing and pp attachment performance with sense information in proceedings of the 46th annual meeting of the association for computational linguistics acl .
h. calvo and a. gelbukh improving prepositional phrase attachment disambiguation using the web as corpus in proceedings of the 8th iberoamerican congress on progress in pattern recognition speech and image analysis ciarp .
m. b. hosseini r. slavin t. breaux x. wang and j. niu disambiguating requirements through syntax driven semantic analysis of information types in proceedings of the 26th working conference on requirements engineering foundation for software quality refsq .
u. shah and d. jinwala resolving ambiguities in natural language software requirements a comprehensive survey sigsoft software engineering notes vol.
no.
.
c. ribeiro and d. berry the prevalence and severity of persistent ambiguity in software requirements specifications is a special effort needed to find them?
science of computer programming vol.
.
f. fabbrini m. fusani s. gnesi and g. lami the linguistic approach to the natural language requirements quality benefit of the use of an automatic tool in proceedings of the 26th annual nasa goddard software engineering workshop sew .
e. kamsties and b. peach taming ambiguity in natural language requirements in proceedings of the 13th international conference on software and systems engineering and applications icssea .
a. massey r. rutledge a. anton and p. swire identifying and classifying ambiguity for regulatory requirements in proceedings of the 22nd ieee international requirements engineering conference re .
l. mich nl oops from natural language to object oriented requirements using the natural language processing system lolita natural language engineering vol.
no.
.
v .
ambriola and v .
gervasi on the systematic analysis of natural language requirements with circe automated software engineering vol.
no.
.
a. mavin p. wilkinson a. harwood and m. novak easy approach to requirements syntax ears in proceedings of the 17th ieee international requirements engineering conference re .
c. arora m. sabetzadeh l. briand and f. zimmer automated checking of conformance to requirements templates using natural language processing ieee transactions on software engineering vol.
no.
.
d. rodriguez d. carver and a. mahmoud an efficient wikipediabased approach for better understanding of natural language text related to user requirements in proceedings of the 39th ieee aerospace conference aeroconf .
b. gleich o. creighton and l. kof ambiguity detection towards a tool explaining ambiguity sources in proceedings of the 16th working conference on requirements engineering foundation for software quality refsq .
h. femmer d. m endez fern andez s. wagner and s. eder rapid quality assurance with requirements smells journal of systems and software vol.
.
b. rosadini a. ferrari g. gori a. fantechi s. gnesi i. trotta and s. bacherini using nlp to detect requirements defects an industrial experience in the railway domain in proceedings of the 23rd working conference on requirements engineering foundation for software quality refsq .
a. ferrari g. gori b. rosadini i. trotta s. bacherini a. fantechi and s. gnesi detecting requirements defects with nlp patterns an industrial experience in the railway domain empirical software engineering vol.
no.
.
g. lami m. fusani and g. trentanni quars a pioneer tool for nl requirement analysis in from software engineering to formal methods and tools and back.
springer .
f. dalpiaz i. van der schalk s. brinkkemper f. aydemir and g. lucassen detecting terminological ambiguity in user stories tool and experimentation information and software technology vol.
.
a. willis f. chantree and a. de roeck automatic identification of nocuous ambiguity research on language and computation vol.
no.
.
k. church and r. patil coping with syntactic ambiguity or how to put the block in the box on the table 1st ed.
mit press .
p. pantel and d. lin an unsupervised approach to prepositional phrase attachment using contextually similar words in proceedings of the 38th annual meeting on association for computational linguistics acl .
e. agirre o. de lacalle c. fellbaum a. marchetti a. toral and p. v ossen semeval task all words word sense disambiguation on a specific domain in proceedings of the 5th workshop on semantic evaluations recent achievements and future directions sew .
m. strube and s. ponzetto wikirelate!
computing semantic relatedness using wikipedia in proceedings of the 21st national conference on artificial intelligence aaai .
e. gabrilovich s. markovitch et al.
computing semantic relatedness using wikipedia based explicit semantic analysis.
in proceedings of the 20th international joint conference on artificial intelligence ijcai .
a. fogarolli word sense disambiguation based on wikipedia link structure in proceedings of the 3rd ieee international conference on semantic computing icsc .
s. gella c. strapparava and v .
nastase mapping wordnet domains wordnet topics and wikipedia categories to generate multilingual domain specific resources in proceedings of the 9th international conference on language resources and evaluation lrec .
g. miller wordnet a lexical database for english communications of the acm vol.
no.
.
c. fellbaum wordnet an electronic lexical database 1st ed.
the mit press .
d. chen and c. manning a fast and accurate dependency parser using neural networks in proceedings of the 18th conference on empirical methods in natural language processing emnlp .
c. arora m. sabetzadeh l. briand and f. zimmer automated extraction and clustering of requirements glossary terms ieee transactions on software engineering vol.
no.
.
d. newman j. lau k. grieser and t. baldwin automatic evaluation of topic coherence in proceedings of the 8th annual conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt .
s. evert google web 1t grams made easy but not for the computer in proceedings of the 8th annual conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt and the 6th web as corpus workshop wac .
c. biemann f. bildhauer s. evert d. goldhahn u. quasthoff r. sch afer j. simon l. swiezinski and t. zesch scalable construction of high quality web corpora.
journal for language technology and computational linguistics vol.
no.
.
t. yen j. wu j. chang j. boisson and j. chang writeahead mining grammar patterns in corpora for assisted writing in proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing proceedings of system demonstrations acl ijcnlp .
t. hawker usyd wsd and lexical substitution using the web1t corpus in proceedings of the 4th international workshop on semantic evaluations semeval .
d. jurafsky and j. martin speech and language processing an introduction to natural language processing computational linguistics and speech recognition 2nd ed.
prentice hall .
c. manning and h. sch utze foundations of statistical natural language processing 1st ed.
mit press .
g. dinu and m. lapata measuring distributional similarity in context inproceedings of the 14th conference on empirical methods in natural language processing emnlp .
l. j. brinton the structure of modern english a linguistic introduction.
john benjamins publishing .
i. witten e. frank m. hall and c. pal data mining practical machine learning tools and techniques 4th ed.
elsevier .
r. eckart de castilho and i. gurevych a broad coverage collection of portable nlp components for building shareable analysis pipelines inproceedings of the workshop on open infrastructures and analysis frameworks for hlt oiaf4hlt .
t. zesch c. m uller and i. gurevych extracting lexical semantic knowledge from wikipedia and wiktionary in proceedings of the 6th international conference on language resources and evaluation lrec .
c. giuliano jweb1t a library for searching the web 1t 5gram corpus last accessed august .
.
available m. zhu y .
zhang w. chen m. zhang and j. zhu fast and accurate shift reduce constituent parsing in proceedings of the 51st annual meeting of the association for computational linguistics acl .
p. resnik using information content to evaluate semantic similarity in a taxonomy in proceedings of the 14th international joint conference on artificial intelligence ijcai .
h. shima ws4j wordnet similarity for java last accessed august .
.
available j. r. landis and g. g. koch an application of hierarchical kappatype statistics in the assessment of majority agreement among multiple observers biometrics vol.
no.
.
j. bergstra and y .
bengio random search for hyper parameter optimization journal of machine learning research vol.
no.
.
g. leech million words of english english today vol.
no.
.
j. hirschberg and c. manning advances in natural language processing science vol.
no.
.
l. breiman j. friedman r. olshen and c. stone classification and regression trees 1st ed.
routledge .
y .
tian and d. lo a comparative study on the effectiveness of part ofspeech tagging techniques on bug reports in proceedings of the 22nd ieee international conference on software analysis evolution and reengineering saner .
j. charbonnier and c. wartena using word embeddings for unsupervised acronym disambiguation in proceedings of the 27th international conference on computational linguistics coling .