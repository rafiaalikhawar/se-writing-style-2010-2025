automatically identifying performance issue reports with heuristic linguistic patterns yutong zhao stevens institute of technology hoboken nj usa yzhao102 stevens.edulu xiao stevens institute of technology hoboken nj usa lxiao6 stevens.edupouria babvey stevens institute of technology hoboken nj usa pouria.babvey gmail.comlei sun stevens institute of technology hoboken nj usa lsun18 stevens.edu sunny wong analytical graphics inc. exton pa usa sunny computer.organgel a. martinez analytical graphics inc. exton pa usa amartinez agi.comxiao wang stevens institute of technology hoboken nj usa xwang97 stevens.edu abstract performance issues compromise the response time and resource consumption of a software system.
modern software systems use issue tracking database to keep track of all kinds of issue reports including performance issues.
however performance issues are largely under tagged in practice since the tagging process is voluntary and manual.
for example the performance tag rate in apache s jira system is below .
this paper contributes a novel hybrid classification approach combining linguistic patterns and machine deep learning techniques to automatically detect performance issue reports.
we manually learn from real life performance issue reports and summarize project agnostic linguistic patterns that recur in the reports.
our approach uses these linguistic patterns to construct the sentence level and issue level learning features for training effective machine deep learning classifiers.
we test our approach on two new datasets each with unclassified issues reports.
we compare our approach with baseline methods.
our approach can reach up to precision and up to recall.
the only comparable baseline method is bert which is still lower in thef1 score.
ccs concepts software and its engineering risk management .
keywords software performance performance optimization software repositories mining acm reference format yutong zhao lu xiao pouria babvey lei sun sunny wong angel a. martinez and xiao wang.
.
automatically identifying performance issue reports with heuristic linguistic patterns.
in proceedings of the 28th permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than acm must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
esec fse november virtual event usa association for computing machinery.
acm isbn .
.
.
.
joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa pages.
.
introduction software performance issues can cause dissatisfaction and drive users to switch to competitor products .
like other types of software bugs performance problems are reported to developers via issue tracking systems.
modern issue tracking systems support tagging a specific issue report as being performance related e.g.
via a label in jira .
this allows product managers to quickly find and prioritize addressing such problems.
however manual tagging of issue reports is tedious and leads to significant under tagging in practice.
we find that only of the largest open source apache projects have over of issue reports tagged as performance issues when empirical studies find that performance issues should be around to .
automatic tagging of performance issue reports is an ideal.
however existing analysis techniques are limited to achieve this goal.
simple techniques such as keyword matching tend to find excessive false positives while machine deep learning methods struggle due to the imbalance of issue types for training.
for example our manual investigation of almost randomly selected issues from apache s jira system finds only performance issues.
with such a significantly unbalanced dataset machine deep learning models tend to miss the few positive cases leading to high precision and low recall.
this paper presents an approach to automatically tag performance issues based on linguistic analysis of the issue description.
our approach stems from the observation that performance related issues often contain similar linguistic characteristics at the sentencelevel agnostic of specific software product.
by manually analyzing almost issues we contribute a set of heuristic linguistic patterns to capture these common linguistic features.
combining these linguistic patterns with state of the art machine deep learning models we offer a method for practitioners to automatically identify performance related issue reports.
the advantage of our approach lies in two aspects the linguistic patterns provide specific learning features to pin point descriptions of performance issues 1esec fse november virtual event usa zhao and xiao et al.
while classic nlp features are mostly based on general statistical models and our sentence level classifier plays an important role in extracting more accurate learning features for issue level classification.
in fact incorporating sentence level analysis improves our recall by on average.
we evaluated our approach on two different datasets each containing unclassified issue reports randomly selected from apache s jira platform.
we compared the sentence level and issue level classifiers in our approach with a total of baseline methods including keyword matching and combinations of different machine deep learning models and classic nlp features.
the results showed that our approach can identify performance related issues with up to precision and recall.
in comparison most stateof the art methods produce only recall.
the rest of this paper is organized as following.
section provides fundamental background information.
section details our approach.
section describes our evaluation design and section explores the evaluation results.
section reviews related prior work and section discusses threats to validity and potential future work.
finally section concludes.
background this paper is tackling a text classification problem.
we will introduce the background information of this problem in this section.
a linguistic patterns linguistic patterns are grammatical rules that allow their users to speak properly in a common language .
previous research has been using heuristic linguistic patterns to classify texts .
for example as a xx... i want to... so that... is a heuristic linguistic pattern which captures how a user story is usually described .
it can be used to automatically match texts that describe users stories in a large corpus.
sometimes the classification is not definite due to the fuzzy nature of the problem.
in these cases researchers combine heuristic linguistic patterns with fuzzy logic .
for example in the study of shi et al.
the authors combine linguistic patterns with fuzzy logic to classify texts in issue reports into different information types including intent benefit drawback example explanation andtrivia .
.
the tricky part is that an input text could reasonably relate to multiple information types without being definite.
thus they assign a confidence value to to each linguistic pattern.
the confidence value models the association between this pattern and a information type.
the linguistic fuzzy model provides interpretability of the classification process .
in this paper we are dealing with a binary classification performance related or not but not in between.
thus fuzziness is not necessary in our case.
the commonality is that we manually learn heuristic linguistic patterns from a large body of developer tagged performance issue reports similar to the practice of shi et al.
.
b machine deep learning .machine learning and deep learning models are commonly used for text classification .
they gain knowledge of classification via large amounts of training based on manually tagged datasets.
an effective classifier relies on extracting relevant features from the data for training.
a common approach is to transform input texts into a numerical representation in the form of vectors andmatrix .
for example count vector works on the frequency of terms.
it is a matrix notation of a corpus where every row represents a document from the corpus every column represents a term from the corpus and every cell represents the frequency count of a particular term in a particular document .
however simply calculating the frequency of terms suffers from a critical problem that all terms are considered equally important when it comes to assessing relevancy on a query.
thus tf idf term frequency inverse document frequency is proposed to scale down the weights of terms with high collection frequency .
tf idf can be generated at different levels of input tokens word level tf idf is a matrix representing scores of every term in different corpus n gram level tf idf is the combination ofnterms together character level tf idf is a matrix representing scores of character level n gram in the corpus .
unlike sparse matrix such as count vector word embedding is a form of representing words and documents using a dense vector representation .
the position of a word within the vector is learned from text and based on the words that surround it.
word embedding can be generated using pre trained embeddings such asglove fasttext and word2vec .
the used model also impacts the accuracy of the classification.
machine learning models have been used for addressing various classification problems.
each model has its unique feature.
naive bayes is a classification technique based on bayes theorem with an assumption of independence among predictors .logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic sigmoid function .support vector machine is a supervised machine learning algorithm which can be used for both classification or regression challenges .decision tree is a flowchart like structure that are commonly used in operations research and management .random forest models are a type of ensemble models particularly bagging models .
extreme gradient boosting is a machine learning ensemble metaalgorithm for primarily reducing bias and also variance in supervised learning and a family of machine learning algorithms that convert weak learners to strong ones .
deep learning modesl has gained popularity in text classification since they are inspired by how human brain works.
there are two main deep learning models convolutional neural networks cnn andrecurrent neural networks rnn .
in addition the bert model is the landmark of multi head attention deep learning models.
it has shown to outperform the previous methods such as cnns rnns and among others for a wide variety of natural language processing nlp tasks .
instead of selecting features bidirectional encoder representations from transformers bert benefits from transfer learning.
in transfer learning the model first is trained on a large text set to solve some general purpose by training the model like language modeling and auto encoding .
this step regarded as pretraining prepares the deep learning model to rapidly learn new downstream tasks.
thus bert does not use a classic method of using features as proxies of contextual information but the model deep learning weights encode a lot of information about the language.
we simply used token embedding as features for the bert model.
2automatically identifying performance issue reports with heuristic linguistic patterns esec fse november virtual event usa c hybrid approaches hybrid approaches combine a base classifier i.e.
the machine deep learning model with a rule based system to improve the classification .
these hybrid systems can be easily fine tuned by adding specific rules for conflicting tags that haven t been correctly modeled by the base classifier.
our approach resides in this category we train the classic machine deep learning models with the heuristic linguistic patterns derived learning features.
to prove the advantage of our approach we will compare with a comprehensive set of baseline methods by combining different nlp features and machine deep learning models discussed in the previous section.
we envision the advantage of our approach lies in that the heuristic linguistic patterns extracted from known performance issues provide more accurate learning features compared to classic nlp features which are based on statistical modeling.
approach our approach is inspired by the observation that performancerelated issues often include sentences that share similar linguistic characteristics.
we capture these common characteristics with heuristic linguistic patterns which we empirically derive from analyzing existing issue reports.
leveraging these linguistic patterns and machine learning techniques we can classify new issues as performance related or not.
therefore our approach consists of two main phases a preparation learning phase to build the linguistics patterns and an issue classification phase.
we first elaborate on linguistic patterns before detailing these two phases below.
.
heuristic linguistic patterns heuristic linguistic patterns allow us to approximate whether some text relates to the topic of performance.
an issue report that describes the system as running slow or inefficient for example would indicate that it likely concerns the system performance.
these patterns offer a project agnostic method for us to identify issue report texts that describe the symptoms root causes solutions and run time measurements of performance issues.
our patterns operate at the sentence level by analyzing the words and grammatical structure of a sentence.
following the approach of shi et al.
we define four types of linguistic patterns lexical profiling structural and semantic .
lexical linguistic patterns extend the basic keyword match concept by additionally considering synonyms negated antonyms and lemmatizations of that keyword phrase.
for example an efficiency lexical pattern would include the such terms as efficient efficiency inefficient and inefficiency .
another example which we call the infinite loop pattern looks for infinite forever orendless followed byloop oriteration .
when a performance issue is identified developers often use a profiler to record performance characteristics of the system.
we define profiling linguistic patterns to capture this information as embedded in an issue report.
usually the matching issue text contains a time or memory usage unit e.g.
milliseconds megabytes or the extent of performance change measured in percentage terms comparing the run time parameters of before and after a code revision.
structural linguistic patterns operate on a higher grammatical level of the sentence structure.
here we are looking for phrase structures that imply a performance issue.
for example we observe that when .
.
.
run execute perform .
.
.
for seconds minutes isa common way in different projects to describe performance issues that happen under a special input.
semantic linguistic patterns extend lexical patterns by also incorporating sentiment analysis to capture linguistic expressions that imply performance issues.
for example our negative necessary pattern searches for a common way of describing the root cause or solution to performance issues that happen under unnecessary conditions.
it searches for the word necessary required oressential in a sentence that has a negative sentiment.
in the following subsection we detail how we derive our linguistic patterns for classifying performance related issues.
.
preparation learning phase the goal is to extract a comprehensive heuristic linguistic pattern setfrom known performance issue reports which can be used to automatically tag more performance issues in a new dataset.
the raw input is the developer tagged performance issue reports on apache s jira system.
the output is a comprehensive heuristic linguistic pattern set .
this part contains five iterative steps.
issue tracking dbpre process manual tagextract hlp hlp in pimerge consolidate saturated?
yes no rank retrieve hlp seti i end issues in pisentences in pii evaluate reflectsentences in p1 to pi added to sentence classifier for each sentencehlp matchersentence hlp vectoryes no issue hlp vectoryes no issue classifiery n?yes sentence taggingissue tagging sentence hlp vectorsissue sentences hlp setml dl models ml dl modelshlp set derivation figure iterative hlp set derivation a rank retrieve .we rank all apache software foundation projects in descending order based on the number of developertagged performance issues.
on apache s jira system developers defined a special label named performance to tag issues.
in each iteration i starting from we retrieve all the developer tagged performance issue reports from the ithproject namely pi.
in each iteration issues from piserve as the source for us to learn how performance issues are described in practice.
we start from projects with the largest number of tagged performance issues to accelerate the building process.
b pre process manual tag .first we pre process the issue reports from project pi.
we use the stanford core nlp1to break each issue report into sentences.
for each sentence we applied lemmatization part of speech pos tagging named entityrecognition ner tagging dependency parsing and sentimental analysis using the annotators of stanford core nlp .
we apply standard data cleaning to remove stop words such as a an the .
next we manually tag each sentence in each issue as either performance related or not performance related.
the reason is that a performance issue usually also contains many sentences that do not carry performance related information.
for example developers may describe the general background information of an issue or include social notes.
thus a sentence must contain description relevant to performance problems such as the symptoms the causes the optimization solutions and performance profiling data to be considered as performance related.
the goal is to identify sentences that contain reusable information that can help identify similar 3esec fse november virtual event usa zhao and xiao et al.
issues in a different context.
a team of five people worked together on the manual tagging including a senior researcher a ph.d candidate a master student with years of previous working experience as product manager and two senior undergraduate students in the software engineering major.
to best avoid bias in the tagging process we divided the team into two groups the ph.d candidate and one senior undergraduate and the master student and the other senior undergraduate.
the senior researcher worked as the mediator in case of conflicts.
the two members in each group tag the same set of sentences and cross validate their results.
each tagger not only tags the sentences as performance related or not but also provides comments that explain why a sentence should be tagged as performance related.
this helps them to be more transparent and definite about their decision.
this also provides reference in the case of disagreement between two taggers.
in the case of disagreements the senior researcher examines the case and makes a final decision based on the grounds of whether general reusable information for tagging other performance issues exists in the sentence.
c heuristic linguistic pattern extraction .based on the tagged performance related sentences and the respective comments provided by the taggers we manually extract and summarize heuristic linguistic patterns that can recur in different contexts for describing performance problems.
this is a combination of automated and manual processes.
for each sentence that is manually tagged as performance related inpi we identify the linguistic properties using the stanford corenlp.
first we use the part of speech tagger pos tagger to assign parts of speech tags to each word such as noun verb adjective etc.
this helps us to extract lexical structural and semantic patterns.
for example load nn is a lexical heuristic linguistic pattern indicating that a sentence must contain keyword load or loads in the form of a noun used in describing computation load s .
similarly nn by nn structural heuristic linguistic pattern captures issues like pixel by pixel byte by byte which is often used to describe a tedious computation process.
meanwhile we use named entity recogonizer tagger ner tagger to capture specific terms for describing time or memory consumption of an issue.
this helps to extract profiling heuristic linguistic patterns such as percentage ner tagger contains percent and duration ner tagger contains duration that describe the profiling measurements.
furthermore stanford core nlp categorizes a sentence in sentiment such as positive neutral and negative.
this helps to extract semantic heuristic linguistic patterns.
figure shows an example of a sentence in issue report kafka matches negative necessary semantic heuristic linguistic pattern since it has keyword unnecessary and its sentiment is categorized as negative.
by the end of this step the output is a set of heuristic linguistic patterns from the project pi.
d merge consolidate .we merge the heuristic linguistic pattern set from project piwith the set built from previous iterations.
the heuristic linguistic patterns from pimay overlap duplicate with the existing heuristic linguistic pattern set.
thus we merge overlapping duplicating or similar patterns together.
for example infinite loop and loop forever can be merged to one heuristic linguistic pattern i.e.
loop infinite forever which means that an infinite a dependencies analysis b sentimental analysis figure an example of extracting a semantic heuristic linguistic pattern loop occurs.
while merging we also consolidate the merged heuristic linguistic pattern through divergent thinking.
that is we add possible variations of identified rules to be inclusive and predictive.
for example the example rule loop infinite forever above can be extended to a more predicable rule loop iteration infinite forever .
e evaluate reflect .the last step of each iteration is to evaluate and reflect the updated heuristic linguistic pattern set.
first we check whether the heuristic linguistic pattern set has grown compared to previous iterations.
if there is no or only a few new patterns added in the current iteration it indicates that the heuristic linguistic pattern set is or close to being saturated.
next we evaluate the precision recall of the heuristic linguistic pattern set using the data from the processed projects.
we use a naive matching approach as long as a sentence matches one of the heuristic linguistic patterns from the set we consider it as performancerelated.
we calculate the precision recall of this naive tagging by comparing with our manual tagging.
if the precision is low it means that the heuristic linguistic patterns can also frequently appear in non performance related sentences implying that the heuristic linguistic pattern set is irrelevant.
if the recall is low it means that the heuristic linguistic pattern set is not comprehensive to capture all different ways that performance issues are presented.
as the heuristic linguistic pattern set grows with iterations the recall should increase gradually and become stable when no more new rules can be found.
we call this status as heuristic linguistic pattern set saturation which means the heuristic linguistic pattern building is complete and no more iterations are needed.
.
issue classification phase in the second part we develop an automatic issue tagging approach by leveraging the heuristic linguistic pattern set from part .
the goal is that given an input issue report our approach automatically outputs yes or no indicating whether this issue is related to performance problems.
our approach works at two progressive levels sentence tagging and issue tagging which depends on the sentence tagging .
a sentence tagging .first given an input sentence our approach automatically tags it as yes or no in terms of performancerelated.
for each sentence in an issue we use a hlp matcher to calculate a sentence hlp vector representing which heuristic linguistic patterns are matched in this sentence.
the sentence hlp vector is a binary vector with ndimensions where nis the total number of heuristic linguistic patterns identified in part .
the ith value in the vector is either or indicating whether the sentence 4automatically identifying performance issue reports with heuristic linguistic patterns esec fse november virtual event usa issue tracking dbpre process manual tagextract rule fuzzy rules in pimerge consolidate saturated?
yes no rank retrieve fuzzy rule seti i end issues in pisentences in pii evaluate reflectsentences in p1 to pi added to sentence classifier for each sentencehlp matchersentence hlp vectoryes no issue hlp vectoryes no issue classifiery n?yes sentence taggingissue tagging sentence hlp vectorsissue sentences hlp setml dl models ml dl modelsfuzzy rule building figure performance issue classification matches the ithheuristic linguistic pattern.
we use the sentence hlp vector as the learning feature to train machine learning and deep learning algorithms.
given any sentence as input we can use the trained model to determine whether it is related to performance problems.
b issue tagging .next we perform the issue level tagging built upon the sentence tagging.
the input is an issue report the output is also yes or no indicating whether this issue report is relevant to performance problems.
towards this we collect the sentence hlp vectors of all the sentences that are tagged as yes above.
we add these vectors to calculate a weighted issue hlp vector which also hasndimensions.
the ithvalue in this vector indicates the number of sentences which are tagged as yes from the previous step and also matches the ithrule.
similarly using the issue hlp vector as the learning feature we train classic machine learning and deep learning models to automatically tag the issue.
c approach variations .the unique contribution of this work is the heuristic linguistic pattern features i.e.
sentence hlp vectorand issue hlp vector .
they can be combined with different machine deep learning models to provide the variations of our approach.
in this study we combine the heuristic linguistic pattern features with classic machine learning models and deep learning models.
thus we have variations in our approach as shown in table .
note that each variation can be applied either at the sentence level by using sentence hlp vector or the issue level by using issue hlp vector .
table sentence issue tagging approach variations type abbre.
model name feature hlp mlhlp nb naive bayes hlp lr logistic regression hlp svm support vector machine sentence hlp vector hlp dt decision tree or hlp rf random forest issue hlp vector hlp xgb extreme gradient boosting hlp dlhlp cnn convolutional neural network hlp rnn long short term memory rnn evaluation design we design our evaluation to answer to following research questions.
rq1 when can the heuristic linguistic pattern set become saturated?
this rq investigates the number of projects we need to review until no more new heuristic linguistic pattern can be found which indicates that the heuristic linguistic pattern set has became saturated.rq2 how accurate is our sentence tagging approach?
we will compare the sentence tagging precision recall and f1score of our approach with that of a comprehensive set of baseline approaches.
rq3 how accurate is our issue tagging approach?
we will compare the issue tagging precision recall and f1 score of our approach with that of a comprehensive set of baseline approaches.
rq4 how much does sentence tagging impact the accuracy of issue tagging?
that is if we directly add the sentence hlp vector of all the sentences in an issue report without tagging the sentences first to calculate the issue hlp vector how much accuracy will be compromised?
.
experiment setup next we talk about the datasets used in this study our experiment setting and the comprehensive set of baseline approaches that we compare our approach against.
a datasets .we extract three datasets containing real life issues from apache s jira system for building heuristic linguistic patterns as well as for training and testing our approach.
table shows basic information of the three datasets.
table datasets id purpose issues p sentences p hlp set building homologous evaluation heterologous evaluation dataset is iteratively collected for building the heuristic linguistic pattern set following section .
.
as we will discuss in more details later there are sentences from issues in projects.
dataset is for evaluating how the heuristic linguistic pattern set can identify more untagged performance issues from the same i.e.
homologous projects.
it contains randomly selected non tagged issues from the same projects of the heuristic linguistic pattern building.
therefore only of issues and of sentences are verified as performance related.
dataset is for evaluating whether the heuristic linguistic patterns are general for tagging issues from heterologous projects independent from the heuristic linguistic pattern building.
it contains issues and sentences from seven projects other than the projects of heuristic linguistic pattern building.
issues and sentences are verified as performance related.
we followed the process in section .
to manually tag each sentence in the three datasets.
for dataset and dataset we also manually tag each issue report following similar practice.
the tagger provides comments for each issue to justify their decision of whether to manually tag this issue as a performance issue or not.
the entire tagging process of three datasets took a total of approximately human hours for the entire group of taggers and mediator.
finally we measured the inter reliability between the two tagging teams using the cohen s kappa agreement.
the agreement is on average .
for tagging the three datasets.
this 5esec fse november virtual event usa zhao and xiao et al.
suggests that the two teams have substantial to almost perfect agreement regarding whether a sentence or an issue report is related to performance problems.
of a particular note we noticed that containing a performancerelated sentence in an issue report does not warrant a performance issue.
the presence of performance related sentences in an issue could just serve as problem context or background information instead of being the main focus purpose of the issue.
issue ignite7849 below is such as an example.
the performance related sentence is underlined above.
however it is just an assumption related to the performance of scanquery .
the main focus of this issue is a functional improvement related to continuous querying .
thus this issue is tag as a no .
ignite currently when we want to use the same predicate for the continuous query and initial query it s easy enough to write something like this.
assumingwearefine with theperformance ofscan query code snip pet .
however this becomes more inconvenient when we want to use sqlquery in the initial query to take advantage of indexing.
this is obviously not ideal because we have to specify the predicate in two different ways.
a quick google revealed that there are products out there that more seamlessly support this use case of continuous querying.
i understand that ignite isn t built on top of sql unlike the commercial rdbmses i found so maybe this is an out of scope feature.
b experiment setting .the iterative process of heuristic linguistic pattern building leads to the dataset .
in answering rq1 we will show how the heuristic linguistic pattern set became saturated with the growth of dataset .
in answering rq2 sentence tagging and rq3 issue tagging we use both the dataset for the homologous evaluation and the dataset for the heterologous evaluation respectively.
for each dataset we run the experiment for times.
each time we randomly divide the dataset into for training and for testing.
the precision recall andf1 score are calculated as the average of the times.
in answering rq4 impact of sentence tagging on the accuracy of issue tagging we implement a variation of our issue tagging approach.
that is instead of adding up the sentence hlp vectors for sentences that are tagged as performance related we directly sum the sentence hlp vectors of all sentences in an issue.
then we compare the precision recall and f1 score of this variation with our original issue tagging approach.
c comparison baselines .to answer rq2 and rq3 we compare our approach with a total of baseline methods grouped into three types of baselines listed in table .
note that each baseline approach can also be applied at the sentence level or issue level.
for fair comparison some baseline methods only compare with a subset of our approach variations.
for example it is not fair to compare a deep learning model with a machine learning model.
thus we will focus on three comparisons ml hlp vs. baseline .
we compare of our approach variations with machine learning models ml with baseline methods.
the baseline contains different methods.
as shown in table from row to row the baseline istable comparison baselines baseline ml models nlp features abbreviation ml model nlp feature nb cv wl ng ch naive bayes lr cv wl ng ch logistic regression count vectors cv svm cv wl ng ch support vector machine word level tf idf wl dt cv wl ng ch decision tree n gram level tf idf ng rf cv wl ng ch random forest character level tf idf ch xgb cv wl ng ch extreme gradient boosting baseline dl models nlp features abbreviation dl models nlp feature bert bert classifier token embedding cnn convolutional neural network word embedding rnn lstm long short term memory rnn word embedding rnn gru gated recurrent units rnn word embedding bi rnn bidirectional rnn word embedding baseline keyword based matching abbreviation method kw lr matching keywords from literature kw hlp matching keywords in hlp set the combination of classic machine learning models and classic nlp features.
for each machine learning model we will compare the accuracy of using the heuristic linguistic pattern features vs. using the nlp features.
dl hlp vs. baseline .
we compare of our approach variations with deep learning models dl with baseline methods.
as shown in table row to row the baseline contains different approaches using deep learning models and nlp features.
ml dl hlp vs. baseline .
as shown in table on the bottom the baseline includes matching common keywords from literature and matching keywords derived from the lexical heuristic linguistic patterns since this type can be used as keywords .
we compare all of our approach variations with baseline for comprehensiveness.
as a particular note for issue tagging our approach build the issue hlp vector by filtering out sentences that are not tagged as performance related.
for a fair comparison in each baseline method for issue tagging we also extract the nlp features based only on sentences that are tagged by this method as performance related.
for instance when using bert for issue tagging the feature token embedding is extracted from sentences that are tagged by bert as performance related.
results .
heuristic linguistic pattern saturation rq1 rq1 when can the heuristic linguistic pattern set become saturated?
we went through iterations to make sure that the heuristic linguistic pattern set was saturated.
table shows the information of the iterations in terms of the project being studied the domain of the project the tag rate of performance issues the number of developer tagged performance issues and the number and percentage of manually tagged performance sentences in each project.
we collected sentences that describe performance related information from totally issues.
overall the performance issue tag rate is only .
and of the sentences are manually verified as performance related.
6automatically identifying performance issue reports with heuristic linguistic patterns esec fse november virtual event usa figure shows how the overall heuristic linguistic pattern set reaches saturation.
in figure 4a the line on the top shows that the entire heuristic linguistic pattern set grows fast in the first iterations and it slows down after iteration and becomes stable in iteration .
we extracted a total of heuristic linguistic patterns with lexical patterns structural patterns semantic patterns and profiling patterns.
the accumulation process of the four types are detailed in the four lines on the bottom of figure 4a.
figure 4b shows the precision and recall of using the updated heuristic linguistic patterns by the end of each iteration to match sentences from all the reviewed data.
if a sentence can match any heuristic linguistic pattern we consider it as performance related.
the precision recall is calculated by comparing to our manual tagging.
the precision remains constantly at indicating that the heuristic linguistic pattern set is precise.
the recall grows from in the first iteration and remains at from iteration to indicating the the heuristic linguistic pattern set becomes comprehensive in iteration .
table heuristic linguistic pattern building iterations iter.
project domain t.rate p.issues p.sentences impala query engine oak repository tool ignite distributed database cassandra database management .
spark analytic engine .
hive database tool .
svn revision control mesos computer cluster .
sling web framework .
pdfbox pdf library .
tapstry web framework lucene retrieval library .
kafka distributed streaming .
total .
answer to rq1 the heuristic linguistic pattern set becomes saturated after iterations containing heuristic linguistic patterns in the type of lexical structural semantic and profiling .
the heuristic linguistic pattern set can match performance related sentences in the rule building dataset with precision of and recall of .
this indicates that the heuristic linguistic pattern set is saturated and can precisely and comprehensively capture the features of how performance issues are described in the heuristic linguistic pattern building dataset.
.
sentence tagging accuracy rq2 rq2 how accurate is our sentence tagging approach?
we evaluated the variations of our approach see table on both dataset and dataset for homologous and heterologous evaluation.
the precision recall and f1 score of the approach variations is shown in figure .
we can make two key observations the performance of the approach variations on dataset and dataset is highly consistent indicating that the heuristic linguistic pattern set is generally applicable to different datasets including those that are independent from the heuristic linguistic pattern building.
a hlps with iterations b precision and recall figure heuristic linguistic pattern saturation with iterations dt hlp rf hlp cnn hlp and rnn hlp can quite precisely to precision and comprehensively to recall capture performance related sentences.
figure sentence tagging precision recall and f1 score next we compare our approach variations with the three baseline approaches as listed in table .
we tested on both dataset and dataset yielding to the same conclusions.
due to space limit we present the details based on dataset in the paper details based on dataset can be found here2.
ml hlp vs. baseline .in figure each sub figure shows the comparison of the precision recall and f1 score of a particular machine learning model when using the sentence hlp vector as learning feature above the horizontal dash line vs. using the four classic nlp features below the horizontal dash line .
we can make the following observations our sentence tagging is .
to times better than most baseline methods based on the f1 score.
for example in figure 6b the f1 score in lr hlp is comparing with the f1 score in lr ng.
the exception is nb cv and svm cv compared to which the f1 score of our approach only outperforms by .
of a particular note our approach always has the highest recall.
it is challenging for machine learning approaches to achieve high recall when using unbalanced data like the performance issues with low positive cases.
for example the recall of most baseline methods is very low between and .
in comparison our dt hlp and rf hlp can have and recall respectively which is significantly higher.
dl hlp vs. baseline .figure shows the comparison between two deep learning models based on the sentence hlp vector above the horizontal dash line vs. five deep learning models based on classic nlp features below the horizontal dash line .
we observe that our approach rnn hlp has very high precision recall and f1 score compared to the five baseline 7esec fse november virtual event usa zhao and xiao et al.
a naive bayes classifier b logistic regression c support vector machine d decision tree e random forrest f extreme gradient boosting figure ml hlp vs. baseline on dataset figure dl hlp vs. baseline on dataset methods.
in particular the classic dl models using the nlp features also suffer from low recall due to the unbalanced nature of the performance issue dataset.
bert is comparable to our approach but still with and lower precision recall and f1 score respectively.
figure ml dl hlp vs. baseline on dataset ml dl hlp vs. baseline .figure compares the variations of our approach above the horizontal dash line with two keyword matching methods below the horizontal dash line .
we observe that the kw matching based on keywords from previous literature yields to very low recall thus it is not a good method to tag performance sentences the kw hlp method based on the lexical heuristic linguistic patterns has pretty good precision and recall indicating that the lexical heuristic linguistic patterns play a significant role in sentence tagging and our approach leveraging all types of heuristic linguistic patterns and combining with ml dl models can optimize the precision and recall by about and compared to using the lexical heuristic linguistic pattern in a simple keyword matching manner.
answer to rq2 four of our approach variations namely dt hlp rf hlp cnn hlp and rnn hlp can precisely to precision and comprehensively to recall capture performance related sentences in different datasets including those that are independent from heuristic linguistic pattern building.
our approach is significantly .
to times better than most baseline methods.
only two baseline methods bert and kw hlp matching keywords from the lexical rules have comparable performance.
however our approach is still to better in precision and to better in recall.
.
issue tagging accuracy rq3 rq3 how accurate is our issue tagging approach?
the issue tagging is built upon the sentence level tagging.
as discussed earlier there are a total of methods for issue tagging variations of our approach keyword matching methods baseline deep learning methods baseline and machine learning methods baseline .
in our approach we build issue hlp vectors as the learning feature see section .
.
in the baseline methods used at the issue level we extract the nlp features from sentences that are tagged by the same method as performance related.
for keyword matching we tag an issue as performance related as long as there is one match.
we repeat our experiment on three datasets dataset dataset and their combination to avoid the interference of particular datasets and generalize our findings.
figure 9a to figure 9c show the precision recall and f1 score of the issue tagging on dataset dataset and the combined dataset respectively.
we ranked different methods by the f1 score.
we observe that 8automatically identifying performance issue reports with heuristic linguistic patterns esec fse november virtual event usa a dataset b dataset c dataset dataset d cross dataset winners for issue tagging figure issue tagging the top five methods based on the three datasets are always from our approach variations with up to precision and up to recall.
as shown in figure 9d the winners in different datasets are highly consistent.
cnn hlp xgb hlp rf hlp and rnn hlp consistently rank in the top five for the three datasets.
on average these four methods have to precision to recall.
this indicates that these three methods can precisely identify the majority of performance issues from different datasets.
the only comparable baseline method for issue tagging is bert whose f1 score is still averagely to lower than cnn hlp xgb hlp rf hlp and rnn hlp.
answer to rq3 four of our approach variations cnn hlp xgb hlp rf hlp and rnn hlp constantly outperform the other methods in the three datasets reaching to precision and to recall.
the only comparable baseline approach is bert whose f1score is still to lower.
.
impact of sentence tagging rq4 rq4 how much does the sentence tagging impact the accuracy of the issue tagging?
this rq examines how the precision recall and f1 score of our approach will be impacted by not performing sentence tagging prior to issue tagging.
that is we calculate the issue hlp vector by adding the sentence hlp vector ofall the sentences in an issue without filtering out sentences that are tagged as not related to performance.
a dataset b dataset figure impact of sentence tagging on issue tagging we conducted two experiments on dataset and dataset and the results are shown in figure 10a and figure 10b respectively.
for both datasets the precision and recall of each approach variation are improved non trivially on average for the precision and for the recall.
of a particular note in the experiment with dataset the precision of rf hlp and dt hlp increased by and with sentence tagging.
this is because the recall in sentence tagging of these two methods have been significantly compromised by and respectively.
therefore the sentence tagging is an important step before the issue tagging for optimizing the recall in tagging unbalanced data with a small percentage of performance issues.
answer to rq4 sentence tagging as a pre step can improve the issue tagging in precision averagely and recall averagely because it ensures that the issuelevel features are built from more accurate input.
related work .
issue categorization software developers testers and customers routinely submit issue reports to the issue tracking systems to record the problems they observe or requests they have.
due to the large amount of issue reports previous research has developed different approaches to automatically categorize issue reports into different types of interests.
this helps developers to prioritize different tasks.
antoniol et al.
showed that automatic classifiers such as decision trees naive bayes and logistic regression are able to distinguish bugs from other kinds of issues such as enhancement and refactoring .
pandey et al.
compared more classifiers for distinguishing bugs from non bugs including naive bayes logistic regression k nearest neighbors support vector machine decision tree and random forest .
they found that random forest performs best in the text mining of bugs .
some prior studies aim at classifying issue reports into more diverse categories.
ohira et al.
manually reviewed issues reports in apache s jira system and classified the bugs impacted on products into security performance and breakage bugs .
limsettho developed an unsupervised framework that can automatically group bug reports together based on their 9esec fse november virtual event usa zhao and xiao et al.
textual similarity .
however their framework labels issue reports using the most frequently appearing words rather than the labels that are already used by labels submitted by users of the issue tracking system.
thung et al.
and chawla et al.
developed automated approaches that can automatically classify issue reports into different types such as functional bugs security bugs refactoring improvements etc.
but performance is not considered as a specific type by them.
in addition some prior studies focused on internal content of issue reports.
shi et al.
leveraged linguistic fuzzy rules to classify sentences in feature requests issue reports into six categories namely intent benefit drawback example explanation andtrivia .
aggarwal et al.
used machine learning models to detect duplicate issue reports .
futhermore some prior studies focused on classifying the severity priority and complexity of issue reports.
lamkanfi et al.
proposed a technique to identify bug severity .
tian et al.
used machine learning classifiers on issue reports as well as code bases to predict bug priorities .
zhang et al.
proposed a markov based method for estimating bug fixing time .
.
performance issue analysis a rich body of prior studies have focused on performance issue analysis from different perspectives .
recent studies that rely on real life performance issues use keyword matching and manual verification to extract dataset.
the most common keywords include fast slow perform latency throughput optimize speed heuristic waste efficient unnecessary redundant too many times lot of time too much time .
however the keyword matching largely compromise the accuracy of the retrieved data.
in addition there could be many different ways to describe performance issues beyond what is captured in known keywords.
thus due to the lack of rigor in the keyword matching approach researchers have to invest a large amount of time on manual verification.
in addition prior studies usually focused on a specific type of performance bugs based on limited number of issue reports.
nistor et al.
studied performance bugs caused by inefficient loop .
yuet al.
studied performance bugs caused by synchronization bottlenecks.
selakovic et al.
presented an empirical study of performance bugs written in javascript language in both .
our study presented an adequate and diversified dataset of more than real life performance issues which can be used as a ground truth dataset for future studies on performance bugs.
discussion a limitations first we did not test our approach on datasets from other than the apache jira platform such as bugzilla and gnats .
however since apache jira is one of the most widely used platforms by real world software projects.
in addition our ground truth data covers various common domains as shown in table while the testing dataset comes from seven new projects and includes three new domains a mobile development framework a messaging server and an enterprise search platform.
therefore we believe our approach is general.
second the heuristic linguistic patterns capture how practitioners describe performance problems in general terms.
if issues are described only in project specific terms understood only by project experts our approach will becompromised.
lastly we have not separately evaluated the accuracy of our approach on datasets of different domains.
we believe that different heuristic linguistic patterns may provide difference accuracy for different domain data.
a possible solution to the last two limitations is to tune the heuristic linguistic patterns with project domain specific concepts.
b validity first we acknowledge that the manual tagging of performance related sentences and issues could pose internal threat to validity.
any manual effort is subjective to bias derived from individual expertise and understanding.
the taggers are not intimately familiar with the reviewed projects but this should not compromise their ability to recognize general performance problems from reading the issue texts e.g.
expensive unnecessary recalculation .
actually it is our intention not to rely on project experts as our goal is to derive general and transferable linguistic patterns.
in addition we tried to mitigate the risk posed by the manual tagging of ground truth dataset by allowing multiple taggers to cross validate results and by asking for tagging comments to increase transparency.
second we cannot guarantee that the heuristic linguistic patterns cover all possible patterns in real life performance issues.
lastly we did not test whether our approach still outperforms the stateof the art approaches when classic data balancing technique such as data swapping and resampling are applied.
however we manually controlled the density of positive issues to be which mimics a balanced dataset.
our experiment shows that the advantage of our approach on balanced dataset lies in the up to higher precision.
we believe that practitioners favor precision over recall in an issue tagging approach as excessive false positives can lead them to stop using it.
c future work we are motivated to optimize our approach on larger scale and more diverse datasets.
in particular we plan to polish our approach leveraging datasets from different issue tracking platforms such as bugzilla andgnats .
we also plan to leverage different types of heuristic linguistic patterns to facilitate the understanding and management of performance issues.
for example profiling linguistic patterns can be used to investigate the dynamic features of different performance issues.
we plan to extend the usage of heuristic linguistic patterns to detect other types of issues such as security issues.
we also plan to explore how the available amount of training data will impact the performance of our approach and the baselines.
conclusion in this paper we contribute a hybrid classification approach combining classic machine deep learning models and the heuristic linguistic patterns to automatically detect performance issues.
we learned and derived a comprehensive heuristic linguistic pattern set with patterns from developer tagged performance issues from the apache s jira platform.
we used the heuristic linguistic pattern set to extract learning features that can accurately pin point performance problem descriptions and use these features train different machine deep learning models.
we evaluated our approach on two different datasets each containing unclassified issue reports randomly selected from apache s jira platform.
the results showed that our approach can identify performance related issues with up to precision and recall which has obvious advantage over baseline methods including bert.
10automatically identifying performance issue reports with heuristic linguistic patterns esec fse november virtual event usa