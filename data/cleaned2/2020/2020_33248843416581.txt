automating just in time comment updating zhongxin liu zhejiang university china liu zx zju.edu.cnxin xia monash university australia xin.xia monash.edu meng yan chongqing university china mengy cqu.edu.cnshanping li zhejiang university china shan zju.edu.cn abstract code comments are valuable for program comprehension and softwaremaintenance andalsorequiremaintenance withcodeevolution.
however when changing code developers sometimes neglect updatingtherelatedcomments bringingininconsistentorobsolete comments aka.
badcomments .suchcommentsaredetrimental since theymay misleaddevelopers andlead tofuture bugs.therefore it is necessary to fix and avoid bad comments.
in this work wearguethatbadcommentscanbereducedandevenavoidedby automaticallyperformingcommentupdateswithcodechanges.we refer to this task as just in time jit comment updating and proposeanapproachnamed cup commentupdater toautomate this task.
cup can be used to assist developers in updating com mentsduringcodechangesandcanconsequentlyhelpavoidthe introduction of bad comments.
specifically cup leverages a novel neural sequence to sequencemodel tolearn comment updatepatternsfromextantcode commentco changesandcanautomatically generate a new comment based on its corresponding old comment and code change.
several customized enhancements such as a special tokenizer and a novel co attention mechanism are introduced in cup by us to handle the characteristics of this task.
we builda dataset with over 108k comment code co change samples and evaluate cup on it.
the evaluation results show that cup outperforms an information retrieval based and a rule based baselines bysubstantialmargins andcanreducedevelopers editsrequired for jit comment updating.
in addition the comments generatedby our approach are identical to those updated by developers in .
testsamples 7timesmorethanthebest performing baseline.
also with ningbo research institute.
also with pengcheng laboratory.
corresponding author.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
concepts softwareanditsengineering softwaremaintenancetools maintaining software software evolution.
keywords comment updating code comment co evolution seq2seq model acm reference format zhongxin liu xin xia meng yan and shanping li.
.
automating justin time comment updating.
in 35th ieee acm international conference on automatedsoftwareengineering ase september21 virtual event australia.
acm new york ny usa pages.
.
introduction code comments are a vital source of software documentation.
developersrecordvariousinformation suchastheintention implementationandusageofacodesegment coderelations andcode evolutions in comments which makes code comments valuableforunderstandingsourcecodeandfacilitatingthecommunicationbetweendevelopers .apriorstudyhasshownthat besidessourcecode commentsareconsideredasthemostessential softwareartifactsforprogramcomprehensionandmaintenance .
despite the value of comments developers may forget or ignoretheupdatesofcommentswhenchangingsourcecode whichmaybringininconsistentorobsoletecomments aka.
bad comments .table1presentsabadcommentexamplein apachekafka .themethodintable1registersthemetricsofthe producer consumer andadminclients whileonlytheconsumer clientsarementionedinitsassociatedcomment.hence thecommentisaninconsistentcomment.beforebeingfixedbydevelopers this comment had existed for over eight months.
during this time it may mislead developers waste their time to double check the implementation complicatecodereviews andresultintheintroduction of bugs .
therefore bad comments have negativeeffectstotherobustnessofasystemandmayincreasethe cost of its development and maintenance.
it is necessary to fix bad comments in time or avoid introducing them.
tofigureouthowandwhenbadcommentsareintroduced we furthercheckedthechangehistoryofthemethodintable1.we found that at the beginning only the consumer clients metrics were registered in this method but two following code changes addedtheproducerandadminclients metrics respectively without 35th ieee acm international conference on automated software engineering ase table a bad comment example public map metricname ?
extends metric metrics ... for finalstreamthread thread threads result.putall thread .producermetrics result.putall thread .consumermetrics result.putall thread.adminclientmetrics ... methodcomment get read only handle on global metrics registry includingstreamsclient sownmetricsplusitsembeddedconsumerclients metrics.
updated comment get read only handle on global metrics registry including streams client s own metrics plus its embedded producer consumerand admin clients metrics.
updatingthecomment.thisfindinginspiresusthatifcomments can be automatically updated with each code change it is possible to reduce and even avoid the introduction of bad comments.
in this work we refer to the task that performs comment updateswithcodechangesas just in time jit commentupdating and propose a novel approach named cup commentupdater to automate this task.
intuitively this task can be automated usingmanuallyderivedpatternsandrules.
however comments are free form texts written in natural languages and are far less formal than source code.
thus it is challenging and time consuming to manually summarize and adaptively apply comment update pat terns.
cup tackles this task in another way.
it leverages a novel neural sequence to sequence seq2seq model to learn the patterns ofcommentupdatesoccurringwithcodechangesandautomaticallygeneratenewcommentsbasedonthecorrespondingoldcommentsandcodechanges.cupcanbeusedtoassistdevelopersinupdating comments during code changes and can consequently help reduce and avoid the introduction of bad comments.
neural seq2seq models have been shown to be effective for many software engineering se tasks .
however the seq2seqmodelsusedinothertaskscannotbedirectlyadoptedto jit comment updating due to two main characteristics of this task first weneedtopreservetheformatofcommentswhiledealing with out of vocabulary oov words.
oov words are pervasive insoftwareartifactsandneedtobecarefullyhandledinmanyse tasks .
furthermore because the goal of this task is to update comments instead of generate them from scratch it is also necessary to keep the format of old and new comments consistent.
second this task takes both code changes and old comments as input.oldcommentsserveasthebasisofupdatesandcodechanges can provide important guidance and clues.
thus this task requires neuralseq2seqmodelstolearntherepresentationsofcodechanges and old comments simultaneously and capture their relationships effectively.
to cope with the first characteristic we propose a simple but effective way to tokenize code and comments which can not only reduceoovwordsbutalsokeeptheformatinformationofcomments.
the copy mechanism is also adopted to copy oov words and format information from the input during generation.
forthesecondcharacteristic ourseq2seqmodelfirstleveragestwodistinctencoderstoencodecodechangesandoldcomments respectively.
then to better capture relationships between code changes and comments we build a unified vocabulary for both code and commenttokens adoptapre trainedfasttextmodeltoobtainword embeddings and integrate a novel co attention mechanism to our seq2seqmodel.theunifiedvocabularyensuresthesametokensappearing in code and comments have identical representations.
the fasttextembeddingsprovideaccuratesyntacticandsemanticinformation of each token.
the co attention mechanism can effectively link and fuse information in code changes and comments.
toevaluateourapproach weextractcodechangesfrom1 popular engineered java projects hosted on github carefully constructing a dataset with 108k code comment co change samples.
an information retrieval based ir based method a rule based methodandaspecialmethodwhichdirectlyoutputsoldcomments are used as baselines.
we evaluate cup and the baselines on our dataset interms of accuracy recall 5and two metrics proposed by us named average edit distance aed and relative edit dis tance red .
the evaluation results show that cup outperforms all baselines in terms of all metrics.
specifically cup can replicate commentupdatesperformedbydevelopersin1612 .
cases whichare7timesmorethanthebest performingbaseline.itisalso theonlyapproachwithredlessthan1 whichindicatesthatitcan reduce developers efforts in jit comment updating.
in summary the contributions of this paper include we propose a novel approach namely cup to automate jit comment updating.
cup is based on a neural seq2seq modelandintroducesseveralcustomizedimprovementsto effectively handle the characteristics of this task.
webuildadatasetwithover108kcode commentco changesamplesforjitcommentupdating.tothebestofourknowledge it is the first large dataset for this task.
we extensively evaluate cup on the dataset using four metrics.
cup is shown to outperform three baselines and can reduce developers efforts in updating comments.
weopensourceourreplicationpackage includingthe dataset the source code of cup our trained model and test results for follow up works.
the remainder of this paper is organized as follows section describesthejitcommentupdatingtaskandtheusagescenarios of our approach.
we elaborate on our approach in section and illustrate how we build our dataset in section .
section presents theproceduresandresultsofourevaluation.insection6 wediscuss the situations where our approach may fail a quality assurancemethod for our approach the performance of our approach in termsbleu 4andmeteorandthethreatstovalidity.afterabrief review of related work in section we conclude this paper and point out future work in section .
problem and usage scenario in this section we formalize the jit comment updating task and describe the usage scenarios of our approach.
.
problem formulation this work targets at automating jit comment updating i.e.
automaticallyupdatingcommentswithcodechanges.thistaskcan figure the overall framework of our approach.
beformalizedasfollows giventhepre andpost changeversions of a code snippet t t primeand the pre and post change versions of its associated comment x y x nequaly find a function fso that f t t prime x y. hereon we refer to t t prime xandyasold code new code oldcomment and newcomment respectively.wetacklethis task by devising and training a neural seq2seq model to approximatef.inaddition sincethegoalistoupdatecommentsinstead of generate them from scratch keeping the format of old and new comments consistent is regarded as an essential requirement.
.
usage scenario our approach namely cup takes a code change and its associated oldcommentasinput aimingtogeneratethecorrespondingnew comment.
its usage scenarios are as follows firstofall cupcanbeusedtoassistdevelopersinperforming jitcommentupdating.whendevelopersmakeacodechange cup can automatically provide update suggestions for the associated comments.ifthecommentsgeneratedbycuparecorrect developers can quickly perform comment updates through one click.
even ifcup ssuggestionsareonlypartiallycorrect theycanalsoreduce developers editsrequiredtoupdatecomments.therefore cupcanhelpimprovedevelopers productivitywithrespecttojitcomment updating and avoid the introduction of bad comments.
cupisalsoabletofixexistingbadcommentswiththehelpof bad comment detection tools.
for example developers can first leverage the tool proposed byliu et al.
to identify comments requiringupdatesineachhistoricalcodechange.then cupcanbe usedtoautomaticallyupdatethedetectedbadcommentsinstead of manually check and modify them.
approach the overall framework of our approach is illustrated in figure .
it consists of three phases i.e.
data flattening model training and comment updating.
specifically we first flatten the code comment co changesamplesextractedfromsourcecoderepositoriesassequences.then ourneuralseq2seqmodelistrainedusingtheflattened data.
finally given a code change and its associated old comment thetrainedmodelcanautomaticallygenerateanewcomment to replace the old one.
in this section we elaborate on the data flattening phase and our neural seq2seq model.
.
data flattening in this phase we convert code changes and comments into sequencessothattheycanbeprocessedbyourneuralseq2seqmodel.ti t prime i aim m equalfiles files equal info insert.
.
equalremove remove equalall delete equalfiles id replace equalmfilesinfo.remove id mfiles.removeall files an edit figure converting a code change to an edit sequence.
.
.
tokenization.
forcomments wefirsttokenizethembyspaces and punctuation marks.
spaces are removed while punctuation marks are reserved.
then compound words which refer to thetokens constructed by concatenating multiple vocabulary wordsaccording to camel or snake conventions are split into multiple tokenstoreduceoovwords.afterthat iftwoadjacenttokensarenot split by space we insert a special token con between them to mark they are concatenated.
asfor code changes eachofthemiscomposedofanold code snippetandanewcodesnippet.thetwosnippetsarefirsttokenized using a lexer.
inner comments and white spaces are removed.
each identifieristokenizedbasedoncamelcasingandsnakecasing and con is also inserted to join the subtokens.
string literals are tokenized like comments.
the key issue in software artifact tokenization is how to deal with compound words.
in the literature the common ways include not changing compound words splitting them and adding a special symbol t at the end of each token beforesplitting e.g.
inputbuffer inputbuffer t input buffer t .
however the first way cannot reduce oov words.
thesecondwaymayloseformatinformation i.e.
atokensequence mayfailtoberecoveredtoitsoriginalsentence.thethirdwaycannot handle the situation wherea subtoken of a compound word is generatedasanindependenttoken.forexample input cannotbegeneratedasanindependentwordifitiscopiedfrom inputbuffer since it does not end with t .
compared to these methods our tokenizercanberegardedas asking theneuralmodeltoalsolearn format information by inserting con to mark concatenation.
simpleisit itcaneffectivelykeepcommentformatconsistent.also topreserveformatinformation wedonotlowercasetokensinboth code and comments.
.
.
code change representation.
after tokenization each code changeisconvertedtotwotokensequences.wecansimplyusetwoencoders to encode them which however makes it hard to capture fine grained modifications between them.
to better represent each codechange wefirstalignitstwotokensequencesusingadifftoolandthenconstructaneditsequencebasedonthealignment similar to as itsrepresentation as shownin figure2.
each element in an edit sequence is a triple ti t prime i ai and is named as an edit.
tiis a token in the old code and t prime iis a token in the new code.
ai istheeditactionthatconverts titot prime i whichcanbe insert delete equalorreplace.if aiisinsert delete ti t prime i willbetheemptytoken .
such edit sequences can not only preserve the information of the old code and the new code but also highlight the fine grained changes between them.
587lstm et prime 1et1 ea1 t1t prime 1a1h prime1 lstm et prime 2et2 ea2 t2t prime 2a2h prime2 lstm et prime 3et3 ea3 t3t prime 3a3h prime3 lstm ex1 x1h1 lstm ex2 x2h2 lstm ex3 x3h3co attentionlstm h1g1lstm h2g2lstm h3g3lstm lstm lstm h prime1 g prime h prime2 g prime h prime3 g prime 3code change attention u prime1 u prime2 u prime3comment attention u1 u2 u3 contextual embed layer embedding layerco attention layermodeling layerlstm e y1 s lstm e y2 y2lstm e y3 y3 u prime3 u3dense softmax s1 s2 s3y1 y2weighted sum pvocab 3y3 c prime3 c3pcode pcmt code change encoder comment encoder decoder figure the architecture of our neural seq2seq model.
.
overview of our neural seq2seq model thearchitectureofourneuralseq2seqmodelispresentedinfigure3.ourmodeltakesasinputaneditsequence e t1 t prime a1 tn t primen an and an old comment x aiming to generate the new comment y .nis the length of theeditsequence.indetail itleveragestwodistinctencoders i.e.
codechangeencoderandcommentencoder toencodetheedit sequenceandtheoldcomment respectively andgeneratesthenew commentthroughanlstm longshort termmemory decoder.
an encoder side co attention mechanism is leveraged to learn the relationships between the code change and the old comment.
two pointer generators are used in the decoder to enable copying tokens from both the new code and the old comment during generation.
.
encoders the two encoders i.e.
code change encoder and comment encoder are nearly the same in structure.
each encoder is composed of four ordered layers an embedding layer a contextual embed layer a co attention layer and a modeling layer.
.
.
the embedding layer.
this layer is responsible for mapping thethreekindsoftokens i.e.
codetokens commenttokens and editactions intoembeddings.thereareonlyfoureditactions so werandomlyinitializeanembeddingmatrixforthemandupdateit during training.
for code and comment tokens we first build a unified vocabulary from all training codeand comment tokens.
then weuseapre trainedfasttextmodel toobtainthewordembedding of each token.
instead of two distinct vocabularies for code andcomments wepreferaunifiedonebecauseitensuresthesame tokensincodeandcommentshavethesameembeddings which can ease the capture of