test automation in open source android apps a large scale empirical study jun wei lin navid salehnamadi and sam malek school of information and computer sciences university of california irvine usa junwel1 nsalehna malek uci.edu abstract automatedtestingofmobileappshasreceivedsignificantattention in recent years from researchers and practitioners alike.
in this paper we report on the largest empirical study to date aimed atunderstanding the test automation culture prevalent among mo bile app developers.
we systematically examined more than .
million repositories on github and identified more than non trivialandreal worldandroidapps.wethenanalyzedthese non trivial apps to investigate the prevalence of adoption oftest automation working habits of mobile app developers in regardstoautomatedtesting and thecorrelationbetweenthe adoption of test automation and the popularity of projects.
among others wefoundthat only8 ofthemobileappdevelopment projectsleverageautomatedtestingpractices developerstend tofollowthesametestautomationpracticesacrossprojects and popular projects measured in terms of the number of contributors stars andforksongithub aremorelikelytoadopttestautomation practices.
to understand the rationale behind our observations we further conducted a survey with professional and experienced developerscontributingtothesubjectapps.ourfindingsshedlight onthecurrentpracticesandfutureresearchdirectionspertaining to test automation for mobile app development.
ccs concepts software and its engineering software testing and debugging.
keywords empirical study automated testing mobile apps android acm reference format jun wei lin navid salehnamadi and sam malek.
.
test automation in open source android apps a large scale empirical study.
in 35th ieee acm international conference on automated software engineering ase september virtual event australia.
acm new york ny usa pages.
introduction testing is an indispensable phase of software development life cycle.
it is the primary way through which quality of software is ase september virtual event australia copyright held by the owner author s .
acm isbn .
reportedtobemoreadvantageousforanumberofreasons such asreliability repeatability andexe cutionspeed especiallyinthe context of continuous integration .
since mobile apps are an integralcomponentofourdailylifeandusedtoperformtasksin criticalfieldssuchasbanking health andtransportation automated testingofmobileappshasreceivedsignificantattentioninrecent years from researchers and practitioners alike.
foranumberofresearchtopicsintheareaofmobilesoftware engineering such as automated program repair automated test transfer mutation testing regression test management andtestrepair understanding theextentmobiletestsexist thetypeandqualityofthesetests and whether the tests are adopted in a particular way is of great importance.forinstance automatedprogramrepairofmobileapps is a plausible idea only if apps come with a substantial number of teststo ensurethe repairsare notbreakingtheir functionality.similarly automated test transfer is going to yield good results only if there is a large number of apps with tests such that tests can be migrated from one app to another.
in addition mobile developers care about why and how to adopt automated testing practicesandparticularly whethersuchadoptionimpactstheover allqualityoftheirappsandwaysinwhichtheirappsareperceived bythedevelopercommunity.therefore aholisticviewregarding practicaladoptionoftestautomationinmobileappdevelopment can contribute to both academia and industry.
to understand the test automation culture prevalent among mobile app developers researchers have investigated the extent towhichtest automationisadoptedin practice .
however thosestudiesarelimitedintermsofbothscaleandqualityofthecurateddataset.first mostpriorworkshaveonlyconsidered hundredsofappsfromasinglesource i.e.
f droid.thefindings andconclusionsdrawnfromarelativelysmallsetofsampleapps may not generalize to the overall app ecosystem.
second previous studies have failed to exclude dummy and invalid tests an important factor that might severely affect theirconclusion.
that is when developers create a new project with android studio the official ide for android app development it generates some example test cases which are irrelevant for the created app.
including these default tests may influence the results ofresearchquestionsastotheadoptionoftestautomationpractices.
finally appropriate and representative subjects are of critical importanceforanempiricalstudy.inthecaseoftestautomation for android apps a practical inclusion criterion is to consider only non trivial apps sinceitisnotcost effectivetowritetestsfortrivial apps such as class assignments tutorials or simple apps with only one component.
studying trivial apps cannot reveal useful insights 35th ieee acm international conference on automated software engineering ase this work is licensed under a creative commons attribution international .
license.
into the adoption of test automation practices.
nevertheless no previous study has focused exclusively on non trivial apps.
in this paper we report on a large scale empirical study on open source android apps from github from three complementaryperspectives apps developers andimpacts.wesystematically examined more than .
million non forked repositories in java andkotlin andinvestigatedmorethan12 000real worldappsto determine theprevalenceoftestautomationinmobileappdevelopment projects working habits of mobile app developers with respect to automated testing and the correlation between theadoptionoftestautomationandthepopularityofprojectsin termsofdifferentmetrics suchascontributorsandstarsongithub and ratings on google play store.
two important contributions of our work are the scale of study andthewaywehavecuratedthedataset.first weconsideredmore than apps across app markets including google play store f droid and playdrone.
we also developed novel heuristics to exclude irrelevant and example tests in data collection and analysis.lastly thesubjectappswereselectedaccordingtoacriteria designatedforidentifyingnon trivialapps detailedinsection4 .
aspresentedinsection5 theseeffortsledtofindingsthatarequite different from prior work.
another contribution of our work is that we considered both unit tests and ui tests.
given the interactive nature of mobile apps ui testing which requires an emulator or a real device to run is the primary way to examine the functionality and usability of mobileapps.therefore inadditiontounittests weareinterestedin whether and how automated ui tests are adopted by mobile developers.
we discuss related research questions such as developers preference for unit and ui testing and their compliance with the testing pyramid practice in section .
to gather a deeper understanding of the underlying reasons for our observations from the source code we further conducted a surveywiththecontributorsofthesubjectapps andendedupwith responses mainly from professional and experienced developers.interestingly withrespecttosomeoftheresearchquestions the results obtained from the analysis of project data and survey responses areinconsistent indicating agap between whatthe developers believe they do versus what they actually do.
overall this paper makes the following contributions wereportonthefirstlarge scaleanalysisfocusingonnon trivial appsinover12 000open sourceprojectsfrom16appmarkets andspanningaperiodof5years toinvestigatehowtestautomation is practically adopted.
we present the working habits of mobile app developers regardingtestautomation suchasthetendencytowritetestsorlack thereof and the compliance with the testing pyramid practice.
wediscusshow thepresenceofautomatedtests anditsextent impact the popularity of apps in terms of different metrics on github and google play store.
wepresentthefindingsofasurveyinvolving148practitioners whodevelopedthesubjectappstounderstandtherationalebehindourobservationsaswellasthechallengesinandroidapp testing.
we create a publicly available dataset for this study .
the dataset was built by referring to multiple data sources includingpublic class exampleinstrumentedtest test public void useappcontext context appcontext instrumentationregistry .getinstrumentation .gettargetcontext assertequals com.example appcontext.getpackagename public class exampleunittest testpublic void addition iscorrect assertequals figure1 exampletestclassesgeneratedbyandroidstudio github googleplaystore f droid andandrozoo.webelieve the dataset can be of great utility for researchers working in the aforementionedresearchareas e.g.
automated programrepair automated test transfer mutation testing that need access to mobile apps with tests.
the remainder of this paper is organized as follows.
section provides a background on mobile app test automation followed by a brief review of prior research efforts in section .
section 4presents our approach for data collection subject selection and developer survey.
section details our findings.
section outlines theimplicationsofthisstudyforresearchersandpractitioners.the paper concludes with a discussion of threats to validity and future work.
test automation in android .
unit and ui tests given the interactive nature of mobile apps there are roughly two types of tests in android unit tests and ui tests.1according to the definition from google unit tests are small tests that validate the app s behavior one class at a time .
in contrast ui tests or endto end tests are medium or large tests that validate user journeys spanningmultiplemodulesoftheapp .thekeydifferencebetween unit and ui tests besides the scope of testing is that unit tests run on a local machine with jvm while ui tests need an emulated or real device to run and almost always use the android os or android framework.
inandroidstudio theofficialideforandroidappdevelopment unit and ui tests are clearly separated they are placed in different directories.
the tests in the testfolder are unit tests that run locally on jvm.
the tests in the androidtest folder are ui tests that require an emulator or real device to run.
these two directories are automatically generated when developers create a new project with android studio.
in this study we consider the tests under the testfolderasunittests andthetestsunderthe androidtest folder as ui tests.
afeatureofandroidstudiohighlyrelatedtoourstudyisthat when developers create a new project it generates not only the folders butalsoexamplesfordifferenttypesoftests.bydefault the 1sometimes they are called local tests and instrumented tests .
1079figure illustration of the testing pyramid practice from testfolder contains a class called exampleunittest.java and the androidtest foldercontainsaclasscalled exampleinstrumentedtest.java as shown in figure .
they are executable examples of unit and ui tests to help developers get started with test automation.
however includingtheseexamplefilesmayresultinoverestimatedconclusions for research questions about the prevalence or adoption of automatedtests becausedevelopersmayaccidentallycommitthese files without an intention to write automated tests.
in this study we exclude these example files when counting number of tests contained in an app.
.
the testing pyramid practice thetestingpyramidisamindsetorpracticetoguidedevelopers in terms of how much effort they should put on creating different kindsofautomatedtests .itessentiallysaysthat developershavetobalancetheirautomatedtestsbyhavingmany morelow levelunitteststhanhigh leveluitests asillustratedin figure .
there are many reasons to follow the test pyramid practice.
first unittestsmakedebuggingeasierbecausetheyfocusonsmall modules that can be tested independently.
when unit tests fail developerscanquicklypinpointtherootcauseoffailureandsavea lot of time.
on the other hand if there is a failure reported by a ui test itusuallymeansthatthecorrespondingunittestsareincorrectormissing.furthermore unittestsaremorerobustandrunfasterin general while ui tests may be subject to flakiness and almost always run slower.
as a result while ui tests are still important to validateend to endworkflows overlyrelyingonthemwillmake testing expensive slow and brittle.
although the proportion of tests for each layer in the testing pyramid varies based on different apps a general recommendation from google is a split unit tests integrationtests and ui tests .
note that while there is a layer of integrationtests andtheycanbeunderstoodasteststhat validatethecollaborationandinteractionofagroupofunits thescope for integration tests is controversial .
in fact these three layers arenottotallyclear cutandsometimesoverlapwitheachother .
in this paper we leverage the characteristics of android apps and androidstudiotoidentifythetwomajortypesoftests unitandui tests.furthermore accordingtotheaboveguideline anappropriate ratio of ui tests could be to of the total number of tests.
related work empiricalstudiesonmobileapptesting.
previously researchers haveinvestigatedhowtestautomationispracticallyadopted .kochharetal.
analyzedover600android apps on f droid to check the presence of test cases and computed the code coverage.
they also conducted surveys to understand the usage of automated testing tools and the challenges faced by developerswhiletesting.cruzetal.
analyzed1 000androidapps onf droidtochecktheirusageofautomatedtestingframeworks and continuous integration tools.
they also found that projects usingautomatedtestinghavemorecontributorsandcommitson github.
recently fabiano et al.
analyzed android apps onf droidtoinvestigatetheprominenceoftestsdevelopedfortheapps aswellasotherqualitymetricsofthetestssuchastestsmells code coverage and assertion density.
our work is different fromtheirs in terms of the scale and data source as we analyzed over apps across app markets.
in addition coppola et al.
analyzed more than apps on github to examine the diffusion evolution and modification causesofuitestsinopen sourceandroidapps.
whiletheirwork ishighlyrelatedtoours thekeydifferenceisthatwefocusononly non trivial apps as they did not factor out toy apps and forks of realappsfromtheirdataset.forexample amongthelistof1 repositories with tests released by the authors2 only of them are considered in our study.
that means our study considers a very different set of apps from theirs.
ontheotherhand toknowthemainchallengesthatdevelopers face while building mobile apps joorabchi et al.
conducted a qualitative study with mobile developers from companies followed by a survey with respondents.
linares v squez et al.
alsoanalyzedresponsesfrom102open sourceandroidapp developers to understand their practices and preferences regarding androidapptesting.unlikeourwork thesepapersdidnotanalyze open sourcedatainthewildandmerelyreliedoninterviewsand survey responses.
finally linares v squez et al.
reviewed the frameworks tools andservicesforautomatedmobiletesting andtheirlimitations.
from a survey they identified several key challenges thatshould be addressed in the near future by the researchers in thearea of mobile test automation.
nevertheless their work did not include any source code analysis or developer survey.
another related topic is the empirical study on automated input generation aig tools .
the work by choudhary et al.
focusedonthecomparisonofdifferentaigtoolsinterms their usability compatibility code coverage and fault detection capability.
another empirical study by wang et al performed asimilarcomparisonofaigtoolsbutfocusedonindustrialapps.
zeng et al.
further investigate the limitations of android monkey the most widely used aig tool in an industrial setting with a popularandcommercialmessengerapp.ourworkdoesnotconsider aig tools rather focuses on automated or scripted test cases created by developers.
empiricalstudiesonopen sourcesoftwaretesting.
anumber of studies investigate the test adequacy in general open source 2we have contacted the authors to ask for the complete list of repositories under their study but get no response.
1080figure3 flowofdatacollectionandanalysisinthisstudy software.
kochhar etal.
studied morethan projects on githubregardingtheir adoption oftesting andthecorrelationof test cases with various project development characteristics such as project size and number of bugs.
to answer questions related totheusage costs andbenefitsofcontinuousintegration hilton et al.
analyzed more than projects on github.
fraser and arcuri empirically evaluated the code coverage ability of evosuite a search based testing tool with public classes retrieved from100javaprojectsfromsourceforge.ontheotherhand belleretal.
reportedafieldstudywith416softwareengineersinwhich theirdevelopmentactivitywasmonitoredwithaneclipsepluginto understandhowandwhendevelopersconducttesting.ourwork complements these studies by providing insights in the context of android app development.
methodology figure3depictstheflowofdatacollectionandanalysisinourstudy.
this study consisted of the following steps we first collected a large list of github repositories from the ghtorrent database we set filtering criteria to identify the repositories representing non trivial android apps we further analyzed the identified repositories to collect their meta data and information about automatedtestsandpopularity weevaluatedthecollecteddatasettoanswerresearchquestionsaboutthetestautomationcultureprevalent among mobile app developers and finally we conducteda survey with the developers of the subject apps to get a deeper understandingoftheunderlyingreasonsforourobservationsfrom the dataset.
we now describe each of these steps in further detail.
.
study subjects and selection criteria theinitiallistofgithubrepositoriesforourstudywasobtained fromtheghtorrentdatabase aresearchprojectthatmonitors thegithubpubliceventtimelineandpopulatesarelationaldatabasewiththecollectedinformation i.e.
meta data.wedownloaded the latest dump of their database and queried the repositories writteninjavaorkotlinthatareneitherforkednordeleted.the query returned a list of more than .
million repositories.
toidentifytherepositoriesofnon trivialandreal worldandroid appsfrom thereturned list weset thefollowing selectioncriteria therepositorymustcontainexactlyone androidmanifest.xml.
themanifestfileisamust haveforeveryandroidapptoprovide essential information about the app to the android build tools .the reason for exactly one manifest file is that the repository containing multiple such files is likely a tutorial or class assignment withmultipledemoapps.weusedgithubapitowalkthroughthe directory tree of the projects to search for the files.
therepositorymustcontain build.gradle withaspecific string com.android.application inside.
android studio uses gradle as its build system and a gradle plugin with this specific stringmeansthatthisprojecthasatasktobuildanandroidapp.weusedgithubapitosearchtheprojectswiththespecifiedcondition.
mostoftherepositorieswerefilteredoutwiththesetwocriteria with about thousand apps left.
at least two components have to be declared in the manifest file.weparsedthemanifestfileandlookedforthedeclarationof four android component types i.e.
activity service broadcast receiver andcontentprovider inside.wesetathresholdof components because we believe it is not cost effective to write tests for a simple app with only one component.
about half of the apps were removed by this step with thousand apps left.
the package name stated in the manifest file must appear in an app market.
we believe that the apps published in app markets especially the markets that charge fees to join such as google playstore aremorelikelybeyondtoyordemoapps becausethe developers want the apps to reach general users and even willing to pay for it .
from the manifest file of each app we retrievedthe package name and tried to match it with apps hosted in the followingappmarkets googleplaystore f droid andthelist of package names and markets provided by androzoo .3this criterion was critical to identify non trivial apps and left us with a list of about thousand apps.
we removed the apps with duplicate package names and ended up with a list of github repositories of non trivial android apps.
the above filtering process took two months primarily because of the rate limit of github api requests per hour .
.
data collection and analysis foreachoftheselectedrepositories weusedgithubapitofurther collect its meta data creation date number of forks number of stars numberofcommits numberofcontributors numberofissues and number of pull requests.
if the app is on google play store we also collected its category and user ratings by crawling the app page.
tocollecttheinformationabouthowtestautomationisadopted intheproject weusedgithubapitowalkthroughthedirectory tree of the project and parsed all the files under the testandandroidtest folders if any exist.
we considered a method as a test case if it is annotated with test .
this annotation is used by junit based testing frameworks including both unit and ui testingframeworkssuchasjunit robolectric mockito and espresso .
a prior study investigating the usage of testing frameworks in apps on f droid shows that of the adoptedunittestingframeworksand97 oftheuitestingframeworks are junit based.
furthermore we classify a test case as a 3a list of app markets considered by androzoo can be found at .
4sometimes two repositories contain the same package name because one is a direct copyoftheother notbyforking .inthissituation wekeeptherepositorywiththe oldest creation date.
1081table distribution of apps by app market market apps google play playdrone fdroid anzhi appchina mi.com virusshare angeeks 1mobile freewarelovers slideme torrents praguard hiapk proandroid apk bang an app may belong to multiple markets table distribution of apps by year created year created apps total unit test if it is under the testfolder and otherwise as a ui test i.e.
underthe androidtest folder .finally asmentionedinsection2.
we excluded the example unit and ui test generated by android studio.
an assumption of our study is that the subject apps were developed with android studio.
because android studio has been the officialideforandroidappdevelopmentsinceitsfirststablerelease in december we further factored out the repositories before from the list described in section .
.
we finally ended upwith12 562repositories appsinourdataset.thedistribution ofappsbyappmarketisshownintable1.whilethemajorityof theappswerepublishedongoogleplaystore thedatasetcovers apps across app markets.
table shows the distribution of apps bytheyeartheywerecreated.fortheappsongoogleplaystore figure shows the distribution by category.
.
survey to complement our findings we conducted an online survey with the developers of the subject apps in our dataset.
in this section we describe the design participant selection and data collection of the survey.
.
.
survey design.
the online survey was designed to understand the rationale behind our findings from the dataset as well as the challenges in android app testing.
we first asked demographic questionstounderstandtherespondents background suchastheir figure distribution of the google play apps by category experiences in terms of the number of years of android app development.
we then asked them about their current practices of android app testing.
for the respondents reporting the use of automatedtests wefurtheraskedthemrelatedquestionssuchasthe preferenceforunitanduitestingandwhethertheyfollowthetestingpyramidpractice andthereasonsfortheirchoices.next we presented some of our findings in the correlation analysis between the adoption of test automation and the popularity of apps and askedfortheiropinionsonpossibleexplanations.finally weaskedtherespondentsforthedifficultiesinadoptingautomatedtestsand general challenges of testing android apps.
for all questions about practicesandopinions weprovidedasetofchoicesidentifiedfrom previous studies as well as an other choice with free form text if none of the provided choices apply.
a sample of the survey can be found at the companion website .
toensurethatthequestionswereclearandthesurveycanbe finished in minutes we conducted a pilot survey with graduate students in computer science who have experience in android app development and survey design.
we rephrased some questions according to the feedback.
the responses from the pilot survey wereusedsolelytoimprovethequestionsandwerenotincluded in the final results.
.
.
participant selection.
from eachsubject appin our dataset wetriedtoretrievetheemailofitsmaincontributorinthefollowing order the email found in the github profile of the repository s owner theemailofthecontributorwhomadethemostcommits and the email of the contributor who made the most recent commit.afterremovinginvalidandduplicatedata weidentified unique email addresses for our survey.
.
.
data collection.
we used qualtrics to distribute the survey to the targeted email addresses and of them bounced.
from the emails successfully sent we received valid and complete responses with a .
response rate.
the responserateisclosetotheresultsofpreviousstudiessuchas2.
reported in and .
reported in o n very similar surveys with mass developers on github.
the received responses are from countries.
the toptwo countrieswheretherespondentsresideareunitedstatesofamerica .
andindia .
.
.
oftherespondentsareprofessional software developers paid by a company and .
of them have more than years of experience in android app development.
1082table distribution of apps in terms of presence of test cases group apps percentage appswith any tests .
apps without tests .
apps with unit tests .
apps with ui tests .
apps with both unit and ui tests .
results in this section we present the results of our study from three complementary perspectives apps developers and impacts.
.
app perspective westartedbyanalyzingourcurateddatasettounderstandthestate of affairs with respect to test automation adoption in open source projects.answerstothesequestionsareimportantforemergingareas of research interest e.g.
automated program repair automated test transfer that rely on the availability of large number of tests.
rq1.
how prevalent is test automation in open source android apps in terms of the presence of unit and ui tests?
table3showsthenumberofrepositoriesgroupedbythepresence of different types of tests.
the results indicate that only .
ofthesubjectappscontaintests andmostofthemarepoorlytested inanautomatedmanner eventhoughtheyarenon trivial.this percentageismuchlowerthanpreviousfindings reportedin reported in and reported in .
there are many possible reasons for the inconsistency between ourresultsandpreviousfindings.first ouranalysisexcludesthe placeholder tests that are automatically generated by android studio as mentioned in section .
.
this check was critical for ourresults since such tests are common in our dataset of the apps examined .
we also manually checked the dataset released by coppola et al.
and found such examples in the reportedtestcases.wearenotabletoverifytheresultsreported by kochhar et al.
because they are not willing to release their dataset.
regarding the results reported by cruz et al.
since theydidnotsearchfortestcases detailedinthenextparagraph we are unable to compare their results with ours.
the way one computes the existence of tests can also influence theresultssignificantly.forexample inthestudybycruzetal.
theyinspectthebuildconfigurationsandlookforimportsrelated totestingframeworkstodeterminethepresenceoftestsinarepos itory.
since having related imports in the build configurations does notnecessarilymeantherearetestcasesintheproject theirfindingsaboutprevalenceoftestautomationispronetooverestimation.
ourinclusioncriterionforsubjectsaredifferentfrompriorstudiestoo.weexcludedthetrivialapps i.e.
simple demoappswith only one component which is not the case with all prior studies.
finally the scale of study might also affect the results.
in the papers by kochhar et al.
and cruz et al.
only and apps from f droid were analyzed respectively.
in contrast our study considers more than apps on github across figure prevalence of test automation of the google play apps by category markets which is substantially different from their works in terms of scale and source of data.
another finding from table is that ui testing is not adopted as extensivelyasunittesting i.e.
vs. .
.wewillfurtherdiscuss this in section .
.
observation only of the non trivial and real world apps have automated tests.
automated ui testing is less adopted than unit testing.
rq2.
istheprevalenceoftestautomationvariedacrossdifferent categories of apps?
tounderstandwhetherthereareanypatternsastotheadoption ofautomatedtestingpracticesacrossdifferentcategoriesofapps for the google play apps with category information in our dataset we reporttheiradoptionofautomatedtestsbycategoryinfigure5.asdepictedinfigure5 whileoveralltheprevalenceoftestautomation is the percentage is substantially higher for some categories of appssuchasfinance andvideoplayers .ontheotherhand some categories of apps such as shopping and dating are poorly tested in an automatic manner.
this variance could be attributed to the quality requirements for different categories.
notethattheobservedpatternsmaynotgenerallyapplytoapps on google play store since many commercial and closed source apps such as popular shopping apps are not included in our study.
the observed patterns have practical implications for both researchers and practitioners.
for instance the fact that certain categoriesofappscontainmoreteststhanothersindicatesthattechniqueslikeautomatedtesttransfer mayworkmuchbetter for apps of a certain category than others.
the results also provide invaluablehintsastowhatarethecustomarydevelopmentpractices for apps of a certain category.
this might help developers set up the right development practice for their open source projects to gain traction and amass contributors.
observation some categories of apps such as finance and videoplayers aremoreextensivelyleveragingtestautomation techniques than others.
1083table the ways of testing android apps by the survey participants way respondents manually with scripted automated tests with dedicated qa team or 3rd party testing services with automatic input generation tools other not at all table the reasons for not adopting test automation by the survey participants difficulty respondents cost to create and maintain automated tests time constraints size or maturity of the app lack of exposure or knowledge of existing frameworks 52cumbersome to use lack of support from management or organization other .
developer perspective in this section we present our findings regarding the associations between developers and test automation including the rationale and preferences reported by the survey participants.
rq3.
howprevalentistestautomationandwhatarethereasons for not adopting it as reported by developers ?
whatarethechallengesintestingofandroidappsin general?
inoursurvey weaskedthedevelopershowtheytesttheirandroidapps andtheywereallowedtoselectalloptionsthatapply.
table shows the results.
interestingly over of the respondentsstatethattheyareusingautomatedtests yetwedonot observethisdegreeoftestautomationadoptionfromthesubject apps they develop.
one possible explanation for this inconsistency isthat theproponentsof testautomationare morewillingto take our survey while the developers not interested in test automation have no incentive to provide feedback.
another reason could be thattheprofessionaldevelopersadoptautomatedtestsatwork but not for their pet projects on github.
finally it is also possible that the developers only uploaded their source code on github without corresponding tests.
to understand why the observed adoption of test automation is low weaskedthedeveloperstospecifythereasonsfornotadopting test automation.
from the results in table we can see the top threereasonsare costtocreateandmaintainautomatedtests e.g.
caused by changing requirements or rapid development time constraints e.g.
because of time to market or customer s schedule and size or maturity of the app e.g.
the app is not big or complex enough to require automated tests.
note that the third reasoncorrespondstoourinsightthatitisnotcost effectivetowrite automated tests for trivial apps and they should be excluded in theempiricalstudy aswehavedone.besides therespondentsalsotable the biggest challengs in testing android apps by the survey participants challenge respondents fragmentation concurrency 66performance 51security energy functionality 43accessibility 35other table the most important or useful criteria for evaluating android app tests by the survey participants criterion respondents faultdetection capability of tests feature or use case coverage of tests 83code coverage of tests code or test case reviews other mentionedotherinterestingdifficultiesinadoptingtestautomation as follows legacycodenotdesignedtobetestedrequireslotsofrefactoring which makes it harder to justify the additional effort to write tests.
...hard to test unexpected gui aspects or unexpected hardware manufactorfirmware issuesor unexpectedpermissionissues orunexpected android behavoir or unexpected 3rd party data formats.
it is worth mentioning that we also asked two general questions to understand the biggest challenges in testing of android apps and themostusefulcriteriaforevaluatingtestsforandroidapps.
theresultsarereportedintables6and7.accordingtotable6 the topthreechallengesare fragmentation e.g.
multipleandroid os or api versions devices with different sizes or resolutions etc.
concurrency e.g.
detecting data races deadlock or violation ofexecutionorderofmethods and performance e.g.
app sresponsivenesssuchasframespersecondforgamingapps.moreover fromtable7wecanseethatthedevelopersdonotconsidercode coverageasthemostimportantcriterionforevaluatingtests whichisinlinewiththepriorstudy .webelievethereportedconcerns call for additional research and development in test automation frameworks and tools.
we take a closer look at the implications of this result in section .
observation of the survey participants reported the use oftestautomation whichvariesdrasticallyfromthatobservedinthedataset.thetopthreedifficultiesinadoptingtestautomation are cost to create and maintain tests time constraints and size or maturity of the app.
rq4.
do the same developers have the same testing habits across apps?
in this section we investigate whether developers are following thesametestautomationhabitsacrossapps.tothatend wefirst clusteredallsubjectappsbytheirowner i.e.
thegithubaccount 1084table8 probabilityofobservingconsistentbehavioronthe apps by the same developers.
sa clusters of apps by the same developers.
sb by different developers setsize clustersprobability p value clusters same behavior sa985 .
.06e 12sb985 .
andobtainedasetof985clusters sa inwhicheachclustercontains two or more apps by the same developer.
next we defined and computed the test adoption rate for each cluster cinsaas follows rate c apps with t est in c apps in c aclusterwitharateof1or0meansthedeveloperhasfollowedthe samebehavioracrossapps.thatis thedevelopereitherwrotetests forallofherappsordidnotwritetestsatall.wefurthercomputed the probability of observing the same behavior in saby dividing thenumberofclustersshowingthesamebehavior i.e.
achievetest adoption rate of or by the size of sa.
moreover to understand if the probability observed in sais high we created another set of clusters sb as a control group.
the number of clusters and the size of each cluster in sbis exactly the same as sa.
however the apps in sbwere randomly selected from the apps not in sa.
we computed the test adoption rate for eachclusterin sbusingthesameequation andtheprobabilityof observing the same behavior in sbaccordingly.
finally to determine if the observed difference between saand sbis statistically significant we applied hypothesis testing on the rate distribution of saandsbusing the non parametric test mannwhitneyu withasignificancelevelof0.
.wechosethemannwhitneyutestbecause saandsbarenotnormallydistributedand did not pass the normality test of shapiro wilk .
the results in table show that in sa the set of clusters in which each cluster consists of the apps by the same developer it ismorelikelytoobserveaclustermanifestingthesamebehavior.
in other words for a group of apps by the same developer theprobability that either all or none of them have tests .
is higherthanagroupofappsbydifferentdevelopers .
.the difference between saandsbis statistically significant because thenullhypothesisthat saandsbarefromthesamedistribution is rejected by the mann whitney u test with a p valueof .
.this findingvouches forthe effectofsoftware engineering educationregardingtestautomation oncelearned developerskeep their habits.
observation app developers tend to follow the same test automation practices across projects.
rq5.
do developers prefer unit or ui testing and why?
fromtable 3insection5.
wesee thattheappsadoptingunit tests .
aremorethanuitests .tovalidateourobservationandunderstandthereasonsbehindthis forthedevelopersreporting theuseoftestautomation wefurtheraskedwhattypeoftesting unit testing or ui testing they do mostly and why.
among the respondents the majority of them prefer unit testing.table the reasons for the preference of unit testing by the survey participants reason respondents speed scope simpleness 28robustness 26other table distribution of the number of tests in the apps with both types of tests.
1q 1stquartile.
2q 2ndquartile median .
3q 3rdquartile.
distribution minmax mean 1q 2q 3q unittests .
.
ui tests .
ratioof ui tests to all tests .
.
.
.
.
.
this is in line with our observation from the dataset.
furthermore oftherespondentshavenopreferenceand7 of them prefer ui testing.
we also asked the proponents of unit testing for their rationale.
table shows that the top three reasons by the developers are speed e.g.
unittestsrunfasterthanuiorend to endtests scope e.g.
unit tests focus on small or independent modules thereby simplifythedebugging and simpleness e.g.
unittestsareeasier to learn and write.
on the other hand developers preferring uitesting indicate that the interactivity is the top reason becauseui or end to end tests can test the app in a more interactive andstraightforwardway.insection6 wediscusshowtheseinsightscould be used for possible improvements of ui testing tools and libraries.
observation majority of the developers prefer unittesting corroborated through both project dataset and survey results.
thetopthreereasonsar espeed scop e and simpleness.
rq6.
is the practice of test pyramid followed by developers?
as mentioned in section .
the testing pyramid practice is a guideline for developers to have a balanced portfolio of differ ent types of automated tests.
to understand if the guideline is appropriatelyfollowedbythedevelopers weanalyzedthe266apps containingbothunittestsanduitestsinourdatasetbycounting the number different types of tests.
furthermore we computedtheratioofthenumberofuiteststothetotalnumberoftestsas follows ui tests un it tests ui tests table shows that the distribution of the numbers of unit and ui tests in the apps are skewed because the averages are much largerthanthemedians i.e.
.75vs.11forunittests and14 .
vs. for ui tests .
that means some apps contain many more tests thanothers.ontheotherhand thethirdquartileshowsthat75 1085of the apps have fewer than .
unit tests and ui tests.
we believe these are reasonable numbers for general apps.
regarding the developers compliance with the test pyramid practice table10showsthatinmorethanhalfoftheapps theratio ofuitestsishigherthan40 whichdiffersfromtherecommended ratio of by google .
in other words the developers putmoreeffortthanrecommendedinwritinguitests.apossible explanation is that the interactive nature of mobile apps drives thedeveloperstowritemoreuitests.however whileuitestsare essential to validate certain types of requirements such as business logicandusability overlyrelyingonthemmaymaketestingand debugging cumbersome as mentioned in section .
.
inoursurvey weaskedtheparticipantswhethertheyarefollowing the testing pyramid practice and of them said no whichisconsistentwithourobservationfromthedataset.aprominent reason from the respondents reporting the non complianceis the lack of exposure or knowledge about the testing pyramidpractice .
other interesting reasons include special needs for myteam or projects and the testing pyramid practice is misleading flawed .
observation6 developers put more effort than recommended inwritinguitests astheaverageratioofuiteststoalltestsis .
.
impact perspective mobile app developers often strive to have their apps become popular.
as members of an open source community developers are pleasedtoseetheirappsreceivemoreattentionfromotherdevelop ersintermsofstars forks contributors etc.ongithub.asproductowners developers want their apps to satisfy the users and receive good ratings and feedback on the market.
while these popularity metrics are not necessarily related to thedevelopment process of apps we would like to investigate whether they are impactedby the adoption of test automation.
specifically we consider thefollowing popularity metrics on github number of stars forks contributors commits issues and pull requests.
moreover we consider user ratings on google play store as the metric of user satisfaction.
these metrics were collected in the manner described in section .
.
table presents the distribution of data in terms of different metrics.
rq7.
how does test automation relate to project popularity?
wewouldliketoknowwhetherappswithtestsaredifferentfrom apps without tests in terms of the popularity metrics on github.
first toeliminatetheeffectcausedbeappsize weexcludedtheappsthathavefewerthan3components the1 stquartile andmorethan 8components the3rdquartile inourdataset endingupwithaset of7 664appsunderconsideration.next weconductedstatistical analysis for each metric with the following steps wedividedthedataintotwodisjointsets rwandr prime.rwconsists ofthemetricvaluesfromtheappswithtests.
r primeconsistofthemetric values form the apps without tests.
5issuesmaybeconsideredasanindicatorofappquality.infact thetopicspostedwith issuescanbeverybroad suchasfeaturerequestorusagediscussion.therefore we consider it as an indicator of popularity.table distribution of the popularity and satisfaction metrics of the apps.
1q 1stquartile.
2q 2ndquartile median .
3q 3rdquartile.
distribution samplesize min max mean 1q 2q 3q stars .
forks .
contributors .
2commits .
55issues .
pull requests .
0ratings .
.
.
.
weappliedthez scoremethod withathresholdofthree times of standard deviation to remove the outliers from both sets.
since the apps without tests are much more than the apps with testsinourdataset rwandr primeareextremelyunbalancedintermsof their sizes.
given that unequal sample sizes maygenerally reduce statisticalpowerof equivalencetests we created rowiththe same size as rwby randomly selecting the values in r prime.
we computed the mean and median of rwand roand the difference between the mean and median.
to determine if the difference observed in rwandrois statistically significant as in section .
we performed hypothesistesting on rwandrousing the mann whitney u test with a significance level of .
.
the null hypothesis on rwandrois that they were selected from populations having the same distribution.
forexample inthecaseofstars thenullhypothesisisthat anapp withtests from rw hasthesamenumberofstarsongithubasan appwithouttests from ro .wechosethemann whitneyutest because rwandroarenotnormallydistributedanddidnotpass the normality test of shapiro wilk.
the above process is repeated for all the popularity metrics.
table shows the results of our statistical analysis.
the statisticalevidenceshowsthattestautomationisassociatedwithall popularity metrics.
namely on average open source android apps withtestsareexpectedtohavemorestars forks contributors commits issues andpullrequestsongithub.ourfindingisnotexactly inlinewiththepriorworkbycruzetal.
inwhichtheyonly foundsuchcorrelationwithcontributorsandcommitsbutnotother metrics.
we believe this inconsistency is caused by similar reasons discussed in section .
.
we presented this correlation to the survey participants and askedfortheiropinionsastothepossibleexplanations.
of the respondents believe that there is a cause and effect relationship between test automation and popularity.
the causation however could be direct reverse bidirectional etc.
as explained by some of the respondents below i would say they have a direct connection since the quality and rigidnessoftheapp scodecandefinitelyinfluenceanapp spopularity.
direct projectscanonlygrowtolargenumbersiftheyarestable.automated testing can ensure this happens to some degree.
direct 1086table impact of having tests on the popularity of apps.
rw apps with tests.
ro apps without tests.
stars forks sizemean median p value size mean median p value rw62910.
.07e .
.59e 12ro629 .
.
.
.
contributors commits sizemean median p value size mean median p value rw6302.
.75e .
.
.53e 67ro630 .
.
.
.
.
issues pull requests sizemean median p value size mean median p value rw63510.
.40e .
.95e 29ro635 .
.
.
.
thedifference is statistically significant.
first you build the app then it gets popular then you get resources motivationtoincrease it squality.that swhenyou gotoui tests.
reverse ithinkbecausetheprojectswerebigtheyweremotivatedtocreate a comprehensive testing suite.
reverse projectsthatbecomepopularendupwritingmoretestsbecause they need to ensure the stability of the project.
as the project becomes more stable due to more testing it provides a positive feedback loop.
theproject inpart ismorelikelytobepopularifitisperceivedas stable and testing helps to increase that stability.
bidirectional ontheotherhand oftherespondentsconsiderthis correlationtobemoreofaconnectionthancausation.forexample the following responses claim common causes for them commoncause experienceddeveloperwhocaresaboutmaking code evolvable.
popular projects are usually bigger with multiple developers and with more management.
tests is just a part of that process.
observation popular projects are more likely to adopt test automation practices.
of the developers believe it implies causality between them.
rq8.
how does test automation relate to user satisfaction?
following the same steps we conducted statistical analysis to investigate whethertest automationrelates touser satisfaction in terms of google play ratings.
as shown in table we do not find the association between them with statistical significance.
surprisingly when we presented this to the survey participants andaskedfortheiropinions oftherespondentsbelieve that test automation and user ratings should be somehow related.namely thedevelopersdonotbelieveourfindingiscorrect.
examples of their reasons are as follows i think it would depend on the type of application.
games and such areharder to testand the qualityof test does notcorrelate withtable13 impactofhavingtestsontheusersatisfactionof apps.
rw apps with tests.
ro apps without tests.
sizemean median p value rw2114.
.
.0689ro211 .
.
.
.
howfunthegameis.forabankingapplicationtestsareessentialand do effect the quality of the final product.
playstoreratingsareanoisymetricofappqualityandoverall userexperience sothenoapparentcorrelationdoesn tconvinceme that app quality isn t impacted at least somewhat by automated testing observation users satisfaction withapps appearstobe unrelated to the adoption of automated testing practices in their development while half of the developers think differently.
discussion automated testing is not widely adopted.
only of the subject apps in our study have adopted automated testing.
as mentionedearlier thisfindingcontradictsearlierstudiesthathavereportedsubstantiallyhigheradoptionrate butitisinline withthegeneralperceptionthatitischallengingtofindcomplex andopen sourceappswithlotsoftestsforresearchpurposes as noted by adamsen et al.
.
nevertheless our study addresses this issuebyprovidingadatasetofreal worldandnon trivialappswith automated tests which can by of significant utility for emerging areasofresearchinterest suchasautomatedprogramrepair automated test transfer and mutation testing .
moreover researchers may hold out hope on specific categories of apps when looking for automated tests for their experiments since our results indicate that the prevalence of test automation is varied across different categories.
note that the focus of our study is on automated or scripted tests.
the subject apps may have gone throughpropermanualtestingbythedevelopers butthatisoutside the scope of this study.
automatedtestingcanbeusefulandimportant.
wefound a strong correlation between the adoption of automated testing practices and the popularity of development projects.
the majority ofthesurveyrespondents believethatthecorrelationis eithercausation ora connection.on theotherhand while users satisfactionappearsunrelatedtotestautomation aconsiderable amount of survey participants think that automated testing contributes to app quality in terms of stability and maintainability andhasimpactsonusers satisfaction.asnotedbypreviousstudies automatedtestingisnotuniversallyapplicable butcan beusefulandimportant especiallyforappsthatupdateregularly and frequently.
automatedtestingneedsmoreattention organizationally and culturally.
despite the benefits of automated testing our study shows that it is not adequately adopted in practice.
many reporteddifficultiesinadoptingtestautomation however canbe addressed from the perspective of organization and culture.
for instance management or organization could provide more support 1087in terms of budget or schedule to allow for the introduction and maintenanceofautomatedtests.inotherwords adoptionoftestautomationinvolvesaculturechange organizationsneedtobewillingto incur the additional cost and effort of setting up test automationpracticesearlyonforthepromiseofproducinghigher qualityapps at a faster pace later on.
toolsandlibrarieshaveroomforimprovement.
oneofthe difficultiesreportedbydevelopersinadoptingautomatedtesting practicesiscumbersometools includingsteeplearningcurve poor documentation usability andcompatibilityissues.apossibleimprovement of such tools is a comprehensive information hub that aggregatesandsummarizesscatteredpiecesofinformationfrom tutorials forums blogs case studies etc.
to make the learning and use of such tools easier.
besides in our study ui testing is less adopted than unit testing and developers have concerns about the speed simpleness androbustnessofuitesting.asaresult current uitestingtoolscouldbeimprovedbyaddressingtheseconcerns.
for example supporting headless mode such as done by robolectric can let developers run ui testswithout an emulator and saveamassiveamountofexecutiontime.inaddition interactive toolssuchasespressotestrecorder canhelpdeveloperscreate ui tests without writing test code.
finally efforts to prevent or resolveflakinessofuitestsmayincreasetherobustnessandattract more users.
awareness matters.
ourstudy indicatesaprimary reasonfor not following specific practices in automated testing is the lackof exposure or knowledge about them.
moreover we found that once developers learn and begin to use test automation techniques theymaintainthathabitacrossotherprojects.therefore raising the developers awareness of existing test automation frameworks tools and practices may increase their adoption.
threats to validity external validity.
the major external validity is the generalization of our findings to all open source android apps.
we mitigated thisthreatbyincludingmorethan 000appsthatvaryinterms of size created year category published market and popularitymetrics on github.
however findings in this study may not be applicabletotrivialappsorcommercialappsdevelopedprivately.
furthermore therespondentsofoursurveymaynotberepresentative of the entire developer community of the subject apps or the globalcommunityofandroidappdevelopers.wetriedtoreduce this threat by collecting the responses of developers from countries withvarious years ofprofessional experience.
thenumber ofresponses toour survey isalso comparable toother similar studies of mobile developers .
internal validity.
weproposedcertainheuristicstoautomaticallyidentifynon trivialapps.whilewemayhavemissedsome complexandpublishedapps e.g.
appswithsingleactivityandmultiplefragments webelievethatthefindingsinthispaperarestill useful for practitioners and researchers regarding test automation.
moreover weautomaticallydeterminethenumberoftestcasescontainedinarepositorybasedontheassumptionthatthetestcases arewritteninjunit basedtestingframeworks.whilejunit based testingframeworksoverwhelminglydominateandroidapptesting e.g.
to100 accordingtoapriorstudy itispossiblethat sometestcasesbuiltontopofothertypesofframeworksarenotincluded in our study.
to mitigate this threat we manually verified a small set of projects in our dataset and did not find any missed test cases.
as a result we argue that such cases are rare and would not significantly impact our conclusions.
conclusion this paper provides a holistic view regarding how and why test automation is practically adopted in open source android apps.
with the analysis of more than non trivial apps on github andasurveyof148developersoftheseapps weinvestigated the prevalence of test automation in mobile app development projects workinghabitsofmobileappdevelopers and thecorrelation between the adoption of test automation and the popularity ofprojects.
among others we found that only of the nontrivialappscontainautomated tests developerstendtofollow the same test automation practices across apps and popularprojects are more likely to adopt test automation practices.
we believethefindingsinthispapershedlightonthecurrentpractices and future research directions pertaining to test automation for mobileappdevelopment.inourfuturework weplantoincorporate additional open source projects such as those hosted on bitbucket andinvestigatenewresearchquestions e.g.
questionsrelatedto the interplay between test automation techniques and continuous integration practices.
acknowledgment thisworkwassupportedinpartbyawardnumber1823262from the national science foundation.