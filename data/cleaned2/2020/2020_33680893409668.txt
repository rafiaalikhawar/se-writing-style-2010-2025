on decomposinga deep neuralnetwork into modules rangeetpan rangeet iastate.edu dept.
of computerscience iowastateuniversity atanasoffhall ames ia usahridesh rajan hridesh iastate.edu dept.
of computerscience iowastateuniversity atanasoffhall ames ia usa abstract deep learning is being incorporated in many modern software systems.deeplearningapproachestrainadeepneuralnetwork dnn modelusingtrainingexamples andthenusethednnmodelfor prediction.
while the structure of a dnn model as layers is observable themodelistreatedinitsentiretyasamonolithiccomponent.
to change the logic implemented by the model e.g.
to add remove logic that recognizes inputs belonging to a certain class or to replacethelogicwithanalternative thetrainingexamplesneedto bechangedandthednnneedstoberetrainedusingthenewsetof examples.
we argue that decomposing a dnn into dnn modules akintodecomposingamonolithicsoftwarecodeintomodules can bring the benefits of modularity to deep learning.
in this work we develop a methodology for decomposing dnns for multi class problems into dnn modules.
for four canonical problems namely mnist emnist fmnist and kmnist we demonstrate that such decompositionenablesreuseofdnnmodulestocreatedifferent dnns enables replacement of one dnn module in a dnn with another without needing to retrain.
the dnn models formed by composing dnn modules are at least as good as traditional monolithicdnns interms oftest accuracyfor our problems.
ccs concepts computingmethodologies machinelearning software andits engineering abstractionandmodularity .
keywords deepneuralnetworks decomposing modules modularity acmreference format rangeet pan and hridesh rajan.
.
on decomposing a deep neural network into modules.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november 8 13 virtual event usa.
acm newyork ny usa 12pages.
introduction a class of machine learning algorithms known as deep learning has receivedmuchattentioninbothacademiaandindustry.thesealgorithmsusemultiplelayersoftransformationfunctionstoconvert inputs to outputs each layer learning successively higher level of permissionto make digitalor hard copies of allorpart ofthis work for personalor classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acm mustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
esec fse november 8 13 virtual event usa associationfor computing machinery.
acm isbn .
inthe data.
the availabilityof large datasets hasmade itfeasibletotrain adjusttheweightsof thesemultiplelayers.since theselayersareorganizedintheformofanetwork thismachine learning model is also referred to as a deep neural network dnn .
while the jury is still out on the impact of deep learning on overall understanding of software s behavior a significant uptick in its usage and applications in wide ranging areas and safety critical systems e.g.
autonomousdriving aviationsystem medicalanalysis etc combine to warrant research on software engineering practices inthe presenceof deep learning.
asoftwarecomponentandadnnmodelaresimilarinspirit bothencodelogicandrepresentsignificantinvestments.theformer isaninvestmentofthedeveloper seffortstoencodedesiredlogicin theformofsoftwarecode whereasthelatterisaninvestmentofthe modeler s efforts an effort to label training data and computation time to create a trained dnn model.
the similarity ends there however.while independentdevelopmentofsoftwarecomponents and a software developer s ability to re use software parts has led to the rapid software driven advances we enjoy today the ability to re use parts of dnn models has not been tothe best of our knowledge attempted before.
the closest approach transfer learning attempts to reuse the entire dnn model for another problem.could we decompose andreuse parts of adnn model?
tothatend weintroducethenovelideaof decomposingatrained dnn model into dnn modules.
once the model has been decomposed the modules of that model might be reused to create a completelydifferentdnnmodel forinstance adnnmodelthatneeds logic present in two different existing models can be created by composing dnn modules from thosetwo models without having toretrain.dnndecompositionalsoenablesreplacement.adnn modulecanbereplacedbyanothermodulewithouthavingtoretrain the dnn.
the replacement could be needed for performance improvement orfor replacing afunctionalitywithadifferentone.
to introduce our notion of dnn decomposition we have focusedondecomposingdnnmodelsformulti labelclassification problems.
we propose a series of techniques for decomposing a dnn model for n label classification problem into ndnn modules one for eachlabel in the original model.
we considereach label as aconcern and view this decomposition as a separation of concerns problem .eachdnnmoduleiscreated duetoitsability to hide one concern .
as expected a concern is tangledwith other concerns and we have proposed initial strategies to identifyandeliminateconcerninteraction.
evaluation.
wehaveevaluatedourdnndecompositionapproach using16differentmodelsforfourcanonicaldatasets mnist fashionmnist emnist andkuzushijimnist .these models use the feedforward fully connected neural networks relu astheactivationfunctionsforhiddenlayers and softmaxasthe esec fse november8 virtualevent usa rangeet pan andhrideshrajan activation function for the output layer.
we have experimented with six approaches for decomposition each successively refining the former.
our evaluation shows that for the majority of the dnn models out of decomposing a dnn model into modules and then composing the dnn modules together to form a dnn model that is functionally equivalent to the original model but moremodular doesnotleadtoanylossofperformanceinterms of model accuracy.
we also examine intra and inter dataset reuse where dnn modules are used to solve a different problem using the same training dataset or entirely different problem using an entirelydifferentdataset.
results.our results show dnn models trained by reusing dnn modulesareatleastasgoodasdnnmodelstrainedfromscratchfor mnist .
fmnist .
emnist .
and kmnist .
wehavealsoevaluatedreplacement whereadnnmodule isreplacedbyanotherandsee similarlyencouragingresults.
outline.in the rest of this paper we describe our initial steps towardachievingbettermodularityfordeepneuralnetworksstarting with a motivating example in related work in methodology in 4and results in .
7concludes.
the code is also publicly accessible at .
whydecomposeadnnintomodules?
achieving decomposition of a dnn into modules has the potential to bring many benefits of modular programming and associated practicestodeeplearning.tomotivateourobjectives considerfigure1thatusesdnnmodelsforrecognizingdigits andletters toillustrate.thetop halfofthefigureshowstworeusescenarios.
if we have a dnn model to recognize can we extract the logic to recognize and build a dnn model for binary digits?
if we have two dnn models for recognizing and a j can we buildadnnmodelforrecognizing0 9ab essentiallyaduodecimalclassifier?thebottom halfofthefigureshowsareplacement scenario.
if we have adnn model thatis a satisfactory jobofclassifying0 4and6 butcouldimproveitsperformance for5 couldwetakethelogicforclassifying5fromanotherdnn modelandreplace the faulty5withanewpart.
dnn model with english letters a j a b c d e f g h idnn model with english digits j a b build a subproblem use and to build a binary digit classifiermerge two problems use and a b to build a duodecimal number classifier remove the faulty part identify a better dnn model replace the faulty partdnn model a dnn model b figure1 examplesoffine grainedreuseandreplacement ifthelogictorecognize0 9anda jwereimplementedassource code modern se practices might consider these reuse and replacement scenarios trivial.
these scenarios would be far from trivial in thecurrentstate of the artindeeplearning.torealizethereusescenarios first the developer will build a model structure for binary digitsfor the firstscenario andduodecimal classifierforthe second scenario.
then the developer will take the training dataset for and partition it to extract labeled training samples for and for the first scenario and a and b for the second scenario.
then thesenewtrainingdatasetswillbeusedtoretrainthenew model structures.
realizing the replacement scenario is more complicated however.
the developer might need to change the model structure of model a to match the structure of model b which alsohasthepotentialtochangemodela seffectivenessfor0 4and .
then they can replace the training samples for used to train model a with those used to train model b. finally the modified modelawouldbetrainedwiththemodifiedtrainingdata.evenfor thesesimplescenarios bothreuseandreplacementarecomplicated.
coarse grained reuse of the entire dnn model as a black box is becoming common.
as modern software continues to integrate dnnmodelsascomponents itisimportanttounderstandwhether fine grained reuse replacement of dnn models canbe improved.
related ideas we are inspired by the vast body of seminal work on decomposing softwareintomodules .webelievethat there are ample opportunities to consider a number of these ideas in the context of dnn but focus primarily on decomposition in this work.
the decomposition of a dnn into modules has not been attempted before but coarse grained reuse at the level of the entire dnn has been.
an approach for achieving reusability in deep learningistransferlearning .inthistechnique adnnmodel is leveraged for a different setting by changing the output layer and input shape of a pre trained network.
transfer learning can be either heterogeneous or homogeneous based ontheproblemdomain.zhou proposedaspecificationbased framework that uniquely identifies the goal of the model.
these models can be advertised in the marketplace so that other developerscantakethismodelasinputandutilizethemtobuildotherdnn modelsontopofthose.li etal.
proposedthatreusabilitycanbe achieved by addingdifferent aucmetrics as tags that canhelpto choose the appropriate model with different parameters.
kirsch et al.
have utilized modular architecture to create a modular layer where nodes are the modules and experiments have shown that using a subset of all modules can outperform the prior results and resultsinlessnoisytraining.comparedtothecoarse grainedreuse ofdnn models the focusof thisworkis onfine grainedreuseand replacementofdnn modules.
decomposingadnn into modules ourapproach illustratedinfigure decomposesatraineddnn model into modules.
in this work we focus on dnn models for multi classclassificationproblems.wewillrefertothesingle blackbox dnn model for all classes as the monolithic model .
our approachdecomposessuchmodelsinto dnnmodules oneforeach label in the original monolithic model.
a dnn module accepts the same inputas the monolithic model but acts as abinary classifier.
in our example in figure a multi class classifier that classifies 890on decomposing adeep neural network intomodules esec fse november8 virtualevent usa ci concern identification ti tangling identification cm concern modularizationconcern tangled concerns0 2concern concern functional concerns0 2concern with tangling identify tangling... 0concern after concern channeling ... 2dnn model with digit 2module decomposition graph based representation 2modules ti cmci figure overview oftheapproachto decompose adnn model.
input into far left is decomposed into three dnn modules far right that classifywhether an inputis0or1or2.
dnndecompositionhasthreesteps concernidentification ci tangling identification ti and concern modularization cm .
by concernhere we mean a specific functionality e.g.
an ability to determinewhetheraninputis0.thismeaningisconsistentwith thepriorworkonmodularity .concern identification ci istheprocessofidentifyingpartsofthemonolithicmodelthatareresponsibleforthatconcern.clearly asfigure 2shows concernsaretangled mixedtogether withinthemonolithic model and parts of the model might contribute to more than one concerns.
once concern identification is complete and parts of the monolithic model that contribute to a concern are identified tanglingidentification ti istheprocessofidentifyelementsamong those parts that are also contributing to other concerns.
finally concern modularization cm is the process of separating the parts of the monolithic model belonging to a concern into its own dnn module.
cm also involves concern channeling where the effects of the non dominant concerns within the module are channeled appropriately.
.
concernidentification ci concernidentificationessentiallyidentifiesthosepartsofthemonolithic model that contribute to specific functionality or concern.
to paraphrasetarr etal.
inordertoachieveabetterdnnquality andimprovethereusability theconcernsofdnnneedtobeseparatedinsuchafashionthateachconcerncanperformaspecific functionalitywithouttheinterventionoftheothers.tountangle theconcernoftheoutputlabel onecouldobtainapieceofthednn that can perform a certain task e.g.
prediction for a single class andcanhidethenon dominantconcern s toseparatethatconcern.
to illustrate consider figure .
here the monolithic model has threetangledconcernsfor0 and2.thegoaloftheconcernidentificationstepistoidentifypartsofthednnthatareresponsible forclassifyinganimageinto0 and2.onceweidentifypartsof the dnn related to a concern those parts still might contribute to otherconcernsaswell.wecallthoseotherconcernsnon dominant concern s .
for example for the concern concerns and are non dominantconcernsinfigure .beforewedecomposethednn intomodules weneedtoidentifytheconcernsinamonolithicdnn andseparatethemtobuildsub networksresponsibleforindividual concerns.
ouralgorithmforconcernidentificationisshowninalgorithm .
we monitor the behavior of the nodes by studying the trainingalgorithm1 concernidentification ci .
procedure ci model input indicator d b x input w b extractwb model extractweightand bias.
x0 x foreachi l 1do xi xi.wi xi xi bi computethe valueof the nodes.
xl xl .wl bl xl softmax xl foreachi l 1do forj 0toj xi do ifxi 0then di updatethevalueoftheedgesto0forinactivenodes.
ifi!
l 1then di bi else ifindicator truethen di wi nochange to edge if indicator is set.
bi bi else fork 0tok wi do updatethe common edges.
ifwi 0then di max di wi ifdi 0then di else di min di wi bi bi forj 0toj xl do ifindicator truethen dl wl bl bl else fork 0tok wl do updatethe output layer edges.
ifwl 0then dl max dl wl else dl min dl wl bl bl returnd b examplesforthatconcern e.g.
trainingexamplesfor0 andadd update removetheedges.thisalgorithmformsasub graphthat can identifythe common edges for asingleoutputlabel.
first the weightandbiasofthednnmodelarestored.inordertoidentifythe edges that are responsible for a particular output label we observe thebehaviorwiththeexamplesthatbelongtothatlabelfromthe trainingdataset.witheachexample weperformtheweightand bias update operation.
in the update operation we provide the dnnmodel model inputexample input theupdatedweight d bias b andanindicator indicator .theweight dandbiasbare 891esec fse november8 virtualevent usa rangeet pan andhrideshrajan initialized with the weight and bias of the monolithic dnn model outside the algo.
.
with everyexample dandbare updated and they are returned as an output.
for the next example the modified dandbwillbe taken as an input.
if for some inputs the common edges for all the examples that belong to a single concern are very low then the graph can become very sparsely connected.
to avoid such circumstances the algorithm stops removingedges from the graphonceitreachesathreshold.wecomputethenon negative edgesoutsidethealgo.
1andsetthethe indicator variable.here we evaluatethe variableand modifies the edges basedonthe value.
the detailed discussion is in in the algo .
.
the edge update is carriedoutinsuchafashionthatremoving updatingtheedgescan helptoidentifyasingleconcernwhileremovingtheotherconcerns outputlabels .first weidentifythevalueofthenodesateachlayer by applying the dense operation in the line .
here ldenotes the total number of hidden and output layers.
for nodes belong to thehiddenlayer wemonitorthevaluebeforeapplyingtherelu operation.
however for the output layer the value computed after applying the softmaxactivation function has been analyzed.
at line10 weiteratethrougheachhiddenlayerandmonitorthevalue ofthenodes.fromline weremovetheincomingedgestothe nodesthat have value 0byapplyingthe reluoperation.
based on the definition of relu any negative value will be updated to zero.
we update the bias of the node to be zero as there will not be anydataflowthroughthatnode.duetothesamereason weremove theoutgoingedgesfromthosenodestothenextlayerexceptthe edges connectingthe last hiddenlayer andtheoutputlayer.ifthe valuecorrespondingtothenodeisapositiveone wevalidatethe indicator.
if the indicator is false the edges are updated by the minimum value of the dnn model edge wi and the updated edge di ifthe value of the edge is a positive one.
if the value of the edgeis negative we performthe maximum operation.
both the operation are carried out to store the semantics of the edges andtheirimpactontheprediction.atline theupdatededges are validated and if the value is negative they are updated as zero basedontheactivationlogic.fromline weperformasimilar operation ontheoutput layer and returnthe updatedweight d andthe bias b .
.
tangling identification ci tanglingidentificationrecognizesthepartsresponsibleforother concerns.
while concern identification can separate the part of the networkthatcontributestoaconcern itmaynotbeabletomakethe separatedpartsfunctional.usingconcernidentification weidentify the edges that are responsible for a particularconcern.
however the remaining network can only classify a single concern as all the edges correspond to the other concerns are removed or updated and the model predicts the dominant concern irrespective of the input.
thus the resulting network becomes a single class classifier.
thisisakintoremovingaconditionalandabranchfromaprogram that results in a subprogram that performs the functionality of the remaining branch but does so unconditionally.
for example in figure1the concerns for and cannot be used as they cannot distinguishbetween differentinputs.
to solve this problem our insight is to identify some edges and nodesbacktotheconcernthathelpsusidentifyinputsthatdon tneedclassificationbythedominantconcern.inourapproach wedo so by adding parts of the non dominant concerns.
by so the problem of classification becomes akin to the one against all oaa classification problem.
in oaa a classifier is built from the scratch thatpredictsaparticularoutputlabel positiveexample andcan stilldetectanynegativeones.thereareafewtechniquesthathave beenproposedbypriorworks thatincludesintroducingan imbalance betweenthe positiveand negative examples punishing the negative example assign higherpriority to the negative exampleswhilemodifyingtheedges.weproposeanothertechniquethat keeps the most relevant edges related to the negative examples.
beforedescribingthesetechniques wediscusstheapproachtoadd edges relatedto non dominantconcernsinalgorithm .
algorithm2 tangling identification ti .
procedure ti model input indicator d b x input w b extractwb model x0 x foreachi l 1do xi xi.wi computethe valueof the nodes.
xi xi bi xl xl .wl bl xl softmax xl foreachi l 1do forj 0toj xi do updatethe negativeedges.
ifxi 0then di di else di wi bi bi forj 0toj xl do updatethe output layer edges.
ifxl .0001then dl wl bl bl returnd b algorithm 2works as follows.
first the value of the nodes is computedsimilarlytothealgorithm .afterthat theedgesincident to the nodes with positive value at lines are added.
for the output layer we reintroduce the edges responsible for the negative outputlabel classification at lines .
next we discuss four different approaches for non dominant edge additiontechniques.
tanglingidentification imbalance ti i .
recallthatconcernidentificationworksusingtrainingexamplesforaparticularconcern.in thisapproach animbalanceisintroducedwhileaddingthenumber ofexamplesfromthepositiveandthenegativeoutputlabels .
inthealgorithm wediscussthestepsfollowedtoincludethepositive and negative examples and carry the edge update operations.
first weight and the bias of the dnn model are extracted at line .
then training examples belonging to the positive output label are filtered.
to identify the concern the positive examples are used to find the common edges that correspond to them based on the approachdepictedinthealgorithm 1ourconcernidentification algorithmreliesonanindicatortohalttheprocessofeliminating edges.atline suchanindicatorhasbeenutilizedthatissetwhen thetotalnumberofnonzeroedges active atthelastlayerisless than10 ofthetotaledges.then negativeexamplesareadded we perform a floor operation to remove the floating point value at lines8 11using the algorithm .
finally the edges corresponding 892on decomposing adeep neural network intomodules esec fse november8 virtualevent usa algorithm3 tangling identification imbalance ti i .
procedure tii model xtrain ytrain class ne ative class w b extractwb model extractweightand bias.
d w b b indicator false xclass xtrain filterpositiveexamples.
fori 0toi xclass do d b ci model xclass indicator d b updatethe positive edges.
ifcountnonzero dl .
dl then indicator false stop the updateif graphisverysparse.
foreachi ne ative classdo updatenegativeedges.
xne ative xtrain fori 0toi xne ative do d b ti model xne ative indicator d b returnd b tothenon dominantconcernsareupdatedoraddedbasedonthe approach discussedinthe algorithm .
tanglingidentification punishnegativeexamples ti pn .
similar to the previous approach of introducing imbalance to tackle the oaa problem here we punish the negative output labels by letting the positive output label update the edges first .
the process is similar to algorithm .
however the input examples are balanced.
in thisapproach if 100examples are utilized for the positiveoutputlabel thenasamenumberofexamplesforthenegative labels are taken e.g.
for an mnist classification problem with ten output labels if we want to decompose a module for label then we choose the ratio being from each negative label .
to stop the negative edges being introduced in a module the approachforidentifyingthecommonedgeshasbeenupdated.in algorithm theedgesbetweentheoutputlayerandthelasthidden layerareupdatedbasedonthevalueoftheweight.inadditionto that validationhasbeenintroducedbeforeline wheretheedges that incident to the nodes at the output layer having a value at least0.
.
asthelastlayerrepresentstheprobabilityvalue have been added.
this helps to remove the edges responsible for thenegativeexamplesleavingmoreedgesrelatedtothepositive outputlabel.
tangling identification higherpriority to negativeexamples tihp .inthisapproach thenegativeoutputlabelsareassignedmore priorityoverthedominantconcernbyswappingtheorderofthe edge update.
for this we update the edges correspond to the dominantconcernfirst lines 12fromalgorithm andthenupdate theedgesresponsibleforthenon dominantconcerns lines .to keeptheratioofpositiveandnegativeexamplesthesame thetotal negativeexamplesareequallydistributedtothenumberofnegative output labels and floor operation has been performed to avoid any floating point number at line 12from algorithm we replace the number10with that is same as the positive examples .
other thanthat everythinghas been kept the same.
tangling identification strong negative edges ti sne .
in this approach the edges related to the positive label are updated then the negative labels.
furthermore the strong negative edges are added by introducing a validation to the algorithm 2at line18by updatingthecheckimposedontheoutputnodevalueassociated withthenegativeexamples.todoso thevalueoftheprobabilityhas been changed from .
to while keeping the other parts ofthe algorithm unchanged.
.
concern modularization cm concern modularization partitions the concerns into parts and builds dnn modulesfor eachconcern.
algorithm4 concernmodularization channeling cm c .
procedure cmc d b class fori 0toi dl do temp dl ifclass 0then assignthe 2nd nodeas the negative.
temp mean temp temp dl mean dl computethe meanof negativeedges.
dl updatealledgesto othernegativenodes.
else assignthe 1st nodeas the negative.
tempw tempb k forj 0toj bl do bl represents the size of the output layer.
ifj!
classthen perform for allnegativenodes.
tempw temp tempb bl k k temp mean tempw computethe meanof negativeedges.
dl mean dl assignthe meanvalue.
forj 1toj bl do updatealledgesto othernegativenodes.
ifj!
classthen temp dl dl temp returnd b concern modularization channeling cm c .
concern modularizationincludestheabstractionofthenon dominantconcerns.
in this approach we propose an approach to abstract the nondominantconcernsbycombiningthenon dominantnodesatthe output layer.
in the algorithm we discuss the methodology to channel the output nodes into one for all the non dominant nodes.
atlines2 theedgesbetweenthelasthiddenlayerandthepositive node at the output layer are updated.
the input classindicates the concerned output label.
the position of the non dominant node at the output label after the concern channeling depends on the positionofthedominantnode.ifthedominantnodeisthefirstnode thenweassignthe2ndnodeattheoutputlabelasthenon dominant node lines .ifnot wechoosethe1stnodeasthenon dominant node and keep the position of the dominant node as it is lines .
all the edges incident on the pre channeling non dominant nodes are replaced by edges directed towards the non dominant nodeafterchannelingwiththevaluebeingtheaverageoftheedges.
theremainingedges incidentonthenon dominantnodesother thanthe channelizedone are updatedto be .
concernmodularization removeirrelevantedges cm rie .
before applying the concern channeling we remove the irrelevant nodesatthelasthiddenlayerthatonlycontributestonon dominant concerns.theedgesthatareconnectedwiththenegativeoutput nodes before channeling the output nodes and not connected weight is zero with the positive output node are combined into onenode.theoutgoingedge s fromthecombinednodetoaparticular negative output node is updated with the average of the all theedgesincidenttothatnegativeoutputnodefromnodesthatare connectedonlywiththenegativeoutputlabels.inthealgorithm 893esec fse november8 virtualevent usa rangeet pan andhrideshrajan algorithm concernmodularization removeirrelevantedges cm rie .
procedure cm rie d b class tempd tempcount fori 0toi dl do temp1 temp2 ifdl 0then forj 0to dl do ifj!
classthen ifdl !
0then identify the irrelevant nodes.
temp1.add dl temp2.add j computethemeanofthenegativeedgesfortheirrelevantnodes.
ifclass 0then dl mean temp1 else dl mean temp1 fork temp2do dl tempd.add i ifclass 0then ne ative node else ne ative node iflen tempd 1then merge the edges if more than irrelevant nodes.
fori tempddo ifclass 0then tempcount dl else tempcount dl dl ne ative node mean tempcount forx tempd do dl updatethe removed edges.
fori 0to dl do compensate the flow.
tempdl forj tempddo tempdl .add dl updatethe merged edges.
dl mean tempdl forx tempd do dl updatethe removed edges.
d b cmc d b class apply the concernchanneling approach.
returnd b we discussthesteps carried out toremove the edges picked based onthefilteringcriteria.atlines theedgesfromthelasthidden tooutputlayerareupdatedifanode nfromthelastlayerisonly connected to the any of the negative output nodes.
these edges are replaced by a single edge with the weight and bias value as the averageoftheedges.ifthenumberofsuchnodesismorethanone we replace all the nodes by one node by removing all the edges from those nodes with one edge with the average weight and bias.
weperformthisoperationatlines .updatingtheedgesand removing the connection to some nodes is similar to changing the flowofthedata.inthisprocess thepaththatdataflowshasbeen updated not the amount of the flow.
since the flow with the edges fromthelasthiddenlayerandtheoutputlayerhasbeenupdated the same flow needs to be adjusted at the preceding layer.
this update operations is shown at lines .
the edges from the precedinglayer tothe lasthidden layerthatincident onthe removed nodes are removed and updated the edges incident to a replaced node at the last hiddenlayer.
in figure an example of such an operation is shown where two nodes are connected with the negative output nodes not with the positive output label.
first edges from the last hidden layer to theoutputlayerarereplacedwithasingleedgeforeachnode.then all such nodes are replaced with a single node with the updated 0module with negative nodesremove edge at the last layer 2remove irrelevant node s figure3 concernmodularization removeirrelevantedges value associated with the edges.
furthermore to compensate for theflow alltheedgesthatareincidentontheremovednodesare removed and the value of the edges to the replaced node at the lasthiddenlayerisupdated.afterthisoperation wechannelthe behavior of the negative output label using the approach described inthe algorithm .
evaluation in this section we first discuss the experimental setup and then answer three research questions.
we discuss how decomposing the dnn models into smaller components or modules perform compared to the monolithic models.
furthermore we compare the performance of reusing the modules from the same dataset and differentdatasetswiththemodelbuiltforasimilarconcern.wealso answer how replacing a module by another module with the same concernanddifferentconcernperforms.finally wesummarizethe key findings.
.
experimentalsetup in this section we discuss the datasets models and the evaluation metricsutilizedtoevaluateourapproach.forconcernidentification algorithm wetakethethresholdvaluetobe10 ofthetotalnumber of edges from the last hidden layer to the output layer i.e.
we stop removingedgesatthatpointtopreventnetworkfrombecomingtoo sparse.fortanglingidentificationimbalance ti i technique we choosetheimbalancetobe100 wherefora10 classclassification problem if1000examplesweretakenfromthepositiveoutputlabel example from each of the other output labels remaining nine labels are taken for modifyingthe edges of the neuralnetwork.
.
.
datasets.
mnist .this dataset comprises of various examples of handwritten digits .
it is divided into the training and testing section.
there are training and testing examples andeachoutputlabel has an equal number of data.
extendedmnist emnist .similartothemnist thisdataset has two dimensional images from the english alphabets a z .
as ourapproachisbasedonthedensehiddenlayer trainingadnn modelwithonlydenselayersdoesnotachievehightestingaccuracy.
to remedy that problem and fixing the number of output labels for all the datasets under experiment a j letters are taken from thedataset.trainingandtestingdatasetcontains48000and8000 examples ofa jletters respectively.
894on decomposing adeep neural network intomodules esec fse november8 virtualevent usa fashion mnist fmnist .this dataset is similar to the structure of the mnist in terms of the training testing example division and the number of output labels.
however this dataset has2dimagesfromdifferentclothes t shirt top trouser pullover dress coat sandal shirt sneaker bag andankleboot.
kuzushiji mnist kmnist .the structure ofthedatasetis similar to the mnist.itcontains imagesfrom japanese digits.
mnist and fmnist have been taken from keras and the othertwodatasets are extractedfrom tensorflow .
.
.
models.
to evaluate our approach we build different models.thesemodelsaretrainedwithcorrespondingdatasetswith 50epochs andtheyhave1 and4hiddenlayers sizeofeach layer is respectively.
the name of the dnn model has been representedas dataset ofhiddenlayers e.g.
forthekmnist dataset with hidden layers the model is referred as kmnist .
for the activation function we used relu for the hidden layer andsoftmaxforthe outputlayer.
for hyperparameters otherthan epoch setto50 anddropout nodropoutused weusedthedefault settings of the keras library.
the testing accuracy of each model is given intable .
.
.
evaluationmetrics.
accuracy .formeasuringtheperformanceofthednnmodel we usetheaccuracymetrics.
however in thecase of thecomposition of the decomposed modules we use the same metrics based onvoting.weexecute ndecomposedmodulesfora nclassification problem in parallel and measure the output of each module.
ifonlyonemodulevotesfortheinput weassignthelabelofthe input based on the dominant output label of the module e.g.
if we execute10modulesforanmnistproblemandforinput module 0onlyvotesforthepositiveoutputlabel thenwelabelthatinput as0.ifmorethanoneornomodulevotesforthepositiveoutput label thenwechoosethemostconfidentonebypickingthemodule with the highest probability for the positive output label.
based on this we compute the accuracy over the test dataset.
we refer to the composed accuracy of the decomposed modules as maand the accuracyofthe trainedmodelas tma.
jaccard index ji .
we use the jaccard index to measure the similarities between the decomposed modules.
first we transform theweightandbiasfromall thelayers intoa1 dimensionalarray and compare the similarities between two modules.
finally we computetheaveragevalueofthemetricandreportintable .if thejaccard index is then thereis nocommonalitybetweentwo comparedobjects andif itis1 then they are exactly the same.
.
results in this section we validate our techniques to decompose dnn into modulesandanswer our researchquestions.
.
.
howefficientarethedecomposedmodules?
toanswerthisresearchquestion weevaluatethefourdifferentproposedtechniques toidentifytanglingconcern.weutilizedthebestapproachbased on the accuracy and the jaccard index and used that technique to apply concern modularization to build modules.
to do so we decomposethednnmodelsintomodulesbeforechannelingthe non dominant concerns and run them in parallel and compute theaccuracy.finally wecomparetheaccuracyamongdifferenttechniques and the pre trained dnn model.
furthermore we compute the average jaccard index ji over all the decomposed modules for eachtechnique.the results have been depictedintable .
we measure the jaccard index to identify tangling concerns amongmodules.wefoundthatutilizingtheti hptechnique where higherpriority hasbeen giventothe negative examples toupdate the edges the lowestjaccard indexcan be achieved.this suggests that the decomposed modules have the least overlap among themselves and are significantly different in structure.
however the composed accuracy of the decomposed modules is very low averageaccuracyis22.
.tounderstandthereason weinvestigate thestructureofthemodulesandfindthattheedgesresponsiblefor predictingthenegativeoutputlabelsareupdatedbythepositive ones.
as the edges related to the negative examples are updated first then the positive ones the edges that are responsible for negative output labels are removed or modified by the edges from the positive output labels.
therefore the network can only classify the dominant output label correctly not the non dominant ones out scenarios have accuracy that explains the decomposed modules are not able to classify the rest of the output labels .
for otherscenarios especiallyformnist 3andemnist whereall theedgesrelatedtothenegativeoutputlabelsarenotupdatedor removed that results inhigher accuracy.
utilizingti pn wherewepunishthenegativeexamplesmore than the positive examples by removing edgesrelated to the negativeoutputlabels achieves alower accuracyfor mostof thecases out scenarios have accuracy .
in comparison to the ti hptechnique weletthenegativeexamplestoaddorupdatethe edgesresponsibleforthenegativeoutputlabelsafterthepositive examples identify the common edges.
here we can see that the jaccardindexishigherthantheti hpthatindicatesthatthereis ahighercommonalityamongthemodules whicharemostlythe negative edges.
however we found that letting the negative examplesaddorupdatealltherelevantedges thecomposedaccuracy of the decomposed modules is significantly lower than the dnn model averagelossofaccuracyis44.
.
this problemhasbeenpartially remedied byallowing onlythe edges that are strongly coupled with the prediction of the negative examples.
if a negative example for a module e.g.
for mnist any input digit other than for a module responsible for has been taken as input to the system that particular model can process the inputandpredictasoneofthenon dominantclassasthestrong edgesstillremainintactinthemodules.however thistechnique increases the jaccard index which depicts that the modules are becoming similar to each other.
this results in increasing the overlap betweentheconcerns.however incomparisontotheotherthree techniques adding the strong negative edges performs the best in termsoftheaccuracy averageloss .
andmedianloss .
.
we move forward with this technique and select this approach for utilizing our concern channeling and remedy the tangling of the concerns.
to do so we update add and remove edges cm c .
the concernchannelizationachievesthebestaccuracyandbetterjaccardindexfor37.
and25 scenarios respectively.however to identifymore tangledconcerns we utilizedthe cm rieapproach whereweremoveirrelevantnodesonlyconnectedwiththenegative output nodes and update the edges to reflect the change at the 895esec fse november8 virtualevent usa rangeet pan andhrideshrajan table accuracyandsimilarityofdecomposed modules acc accuracy ji mean jaccard index modelti i ti pn ti hp ti sne cm c cm rie model accuracy acc jiacc jiacc jiacc jiacc jiacc ji mnist .
.
.
.
.
.
.
.
.
.
.
.
.
mnist .
.
.
.
.
.
.
.
.
.
.
.
.
mnist .
.
.
.
.
.
.
.
.
.
.
.
.
mnist .
.
.
.
.
.
.
.
.
.
.
.
.
fmnist .
.
.
.
.
.
.
.
.
.
.
.
.
fmnist .
.
.
.
.
.
.
.
.
.
.
.
.
fmnist .
.
.
.
.
.
.
.
.
.
.
.
.
fmnist .
.
.
.
.
.
.
.
.
.
.
.
kmnist .
.
.
.
.
.
.
.
.
.
.
.
.
kmnist .
.
.
.
.
.
.
.
.
.
.
.
.
kmnist .
.
.
.
.
.
.
.
.
.
.
.
.
kmnist .
.
.
.
.
.
.
.
.
.
.
.
.
emnist .
.
.
.
.
.
.
.
.
.
.
.
.
emnist .
.
.
.
.
.
.
.
.
.
.
.
.
emnist .
.
.
.
.
.
.
.
.
.
.
.
.
emnist .
.
.
.
.
.
.
.
.
.
.
.
.
table intra dataset reuse.all results in .
mnist and fashion mnist.mn mnist fm fmnist.
mn fmmnmatma matma matma matma matma matma matma matma matma t shirt top .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
trouser .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
pullover .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
dress .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
coat .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sandal .
.
.
.
.
.
.
.
.
.
.
.
.
shirt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sneaker .
.
.
.
.
.
.
.
.
bag .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ankle boot .
.
.
.
.
.
.
.
.
.
.
.
fm t shirt top trouser pullover dress coat sandal shirt sneaker bag blue represents the intramodulecombinationsfor f mnistand yellow represents the the intramodulecombinationsfor mnist.
extendedmnist emnist and kuzushijimnist kmnist .em emnist km kmnist.
em a b c d e f g h i kmemmatma matma matma matma matma matma matma matma matma japanese a97.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese b97.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese c98.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese d97.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese e87.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese f93.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese g97.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese h93.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese i92.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
japanese j97.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
km japanese japanese japanese japanese japanese japanese japanese japanese japanese blue represents the intramodulecombinationsfor emnistand yellow represents the the intramodulecombinationsfor kmnist.
ma decomposed moduleaccuracy tma trained modelaccuracy.
lasthiddenlayer.weapplythistechniquebeforechannelizingnondominantconcerns.fromtable wecanvalidatethatthecm rie decreases the jaccard index from the prior techniques cm c and this approach can produce modules that perform the best in terms ofthe accuracyfor .
9outof16 ofthe cases.
with the accuracy achieved using the cm rie technique we found that the accuracy after decomposing loses .
on average medianis0.
.also in9outof16cases wewereabletoincrease orabletogetthesameaccuracyasthetrainedmodel.tovalidate whether there is a significant difference between the dnn modelsand decomposed modules the average number of edges with value zero inactive have beencomputedforeach scenario.thismetric validates how the edge removal technique to decompose the dnn modelintomodulesperformsinpractice.wefoundthatforcases wheredecomposingadnnmodelintomoduleseithergainaccuracy or remain the same there are on average .
of the edges are inactive.
this result shows that the modules generated are not the same as the dnn model.
896on decomposing adeep neural network intomodules esec fse november8 virtualevent usa table inter datasetreuse.all results are in mnist vs extendedmnist emnist .
mn mnist em emnist em a b c d e f g h i j mnmatmamatmamatmamatmamatmamatmamatmamatmamatmamatma .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist vs kuzushijimnist kmnist .mn mnist km kmnist kmjapanese japanese japanese japanese japanese japanese japanese japanese japanese japanese mnmatmamatmamatmamatmamatmamatmamatmamatmamatmamatma .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ma decomposed moduleaccuracy tma trained modelaccuracy.
however ourdecompositiontechniquecanbemodifiedinthe futuretoincreasetheinactivenodesandremovethetangledconcerns more efficiently.
also to verifywhether changing the hidden layers width affects the performance we altered the width of each hidden layer to from for all the models.
we computed the accuracy of the decomposed modules using the cm rie approach.
wefoundthatthechangeofaccuracyformnist fmnist kmnist and emnist are on average .
.
.
and .
respectively.for furtherrqs we evaluateourproposedapproach with the decomposed modules build with the cm rie technique using models having nodes per hidden layer as discussed in 5. .
.
.
.
does modularity in dnn enable reuse?
in this research question we validate whether fine grained reuse can be achieved by utilizing the decomposed modules.
to validate the reusability and answerthisrq weevaluatedecomposedmodulesfrom16different dnn models andreuse themintwodifferentsettings.
intra datasetreuse.
inthisscenario westudytwomodules decomposedfromthesamednnmodelandexecutetheminparallel to build a smaller problem and validate against a dnn model built with the dominant examples for the picked modules.
in figure wedescribe asimilarexample wherewetake moduleresponsible for the digits and from a pre trained dnn model and reuse them to build a binary classifier.
to validate such scenarios we train a dnn model with the same examples based on the example digit and and the same structure as the dnn model that has beendecomposedtoobtainthemodules.finally wecomparethe composed accuracy of the modules and the accuracy of the trained dnn model.
in table we show various scenarios of intra dataset reuse.
while performing this evaluation we use the modules build from the model with the hidden layers mnist emnist fmnist andkmnist .asthetotalnumberoftheoutputlabels in each dataset is ten choosing two modules responsible for one output label can have scenarios parenleftbig10 parenrightbig .
in table upper half we combine the results from mnist and fmnist datasets.
the cellswithblueandyellowhavebeentakenfromthefmnistand mnist datasets respectively.
we perform similar operations on kmnist and emnist and depict the results in table bottom half .
our results suggests that for out of .
outof45 36outof45 and51.
23outof45 scenariosfor mnist fmnist emnist andkmnistrespectively thecomposed accuracyusingmodulesismoreorthesamecomparedtothetrained dnnmodels.ourresultsuggeststhatthereisnosignificantchange ofaccuracyconsideringallthecases 4datasets .averagegainof accuracyis0.
.
median .
inter dataset reuse.
while the prior experiments were done onthesamedatasetandmodel theseexperimentsarecarriedon differentdatasetswithmodelsbuildwithsimilararchitecture same number of hidden layers .
we evaluate the mnist vs. emnist and mnist vs. kmnist with the same choice of the models.
similar to thepriorexperimentalsetup weevaluatethecomposedaccuracy of two decomposed modules taken from two different datasets and modelsandexecutethemonthedominantexamplesofthemodules e.g.
we takeone modulefrommnist mnist model thatcan classify english and one module from kmnist knist model that isresponsible for japanese .
in this example we validate the accuracy against the inputs and test with the example taken from the test dataset where the output label is either english or japanese .
also we train a model with the same number of hidden layers four with the same output labels english and japanese from the training dataset and compare them.
in table upper half we take two modules for eachexperimentandcomparethetraineddnnmodelbuildfrom 897esec fse november8 virtualevent usa rangeet pan andhrideshrajan table intra datasetreplacement.rm replaced module.
dataset tma prior ma rm0 rm1 rm2 rm3 rm4 rm5 rm6 rm7 rm8 rm9 mnist .
.
.
.
.
.
.
.
.
.
.
.
fmnist .
.
.
.
.
.
.
.
.
.
.
.
kmnist .
.
.
.
.
.
.
.
.
.
.
.
emnist .
.
.
.
.
.
.
.
.
.
.
.
table inter dataset replacement.all results are in mnist vs extendedmnist emnist .
mn mnist em emnist em a b c d e f g h i j mnmatmamatmamatmamatmamatmamatmamatmamatmamatmamatma .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
mnist vs kuzushijimnist kmnist .mn mnist km kmnist kmjapanese japanese japanese japanese japanese japanese japanese japanese japanese japanese mnmatmamatmamatmamatmamatmamatmamatmamatmamatmamatma .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ma decomposed moduleaccuracy tma trained modelaccuracy.
theexamplesfromtheoutputlabels.wereporttheevaluationfor combinations 10x10 of re use from mnist and e mnist modules.
our result suggests that for only six scenarios reusing themodulescanperformthesameasthetrainedmodel.reusing themodulesbetweendifferentdatasetscancausealossofaccuracy .
on average median .
in comparison to dnn models trained with those examples.
similarly we evaluate with mnist andkmnistandreporttheresultsintable bottomhalf .our result suggests that loss of accuracy is .
on average .
median .
also webuilda duodecimalclassifierbasedonour motivating exampledepictedinthefigure whereallthemodulesdecomposed from mnist 4and decomposedmodulesresponsibleforenglish letteraandbrun in parallel and the composed accuracy of the decomposed module is .
.
furthermore a dnn model has been trained based on the and a b and the testing accuracy of that model is .
.
this shows that decomposing the dnn into modules and reusing them to build a problem to recognize the duodecimal 9ab digitsispossible.however theaccuracyofthe compositionofthemodulesislower .
thanthemodeltrained from scratch.
this could be a potential direction for future work.
.
.
doesmodularityindnnenablereplacement?
inthisresearch question weanswerwhetherthedecomposedmodulesdecomposed canbereplacedbyothermodules.replacingamodulefromasetofmodulesbuiltbydecomposingadnnmodelcanhelpineither of these two directions.
first referring to the bottom half of the figure1 where the faulty part of the dnn has been replaced with a better fitted part from a different dnn model trained on the same problem and this scenario we refer to as the intra dataset replacement.second apartofthednnmodelcanbereplacedby apartfromadnnmodelthatrepresentsadifferentconcern.we represent these scenarios as inter problem replacement where the datasetoftheproblemsisdifferent.thesebroadercategoriesofthe replaceability study have discussedinthe nextfewparagraphs.
intra dataset replacement.
in this scenario we replace a modulefromasetofmodulesdecomposedfromadnnmodelwith amodulebuiltonthesamedatasetbutwithdifferentconfigurations decomposedfromadifferentdnnmodel .toevaluatethesescenarios wereplaceeachmodulefromtheleastcomplexmodel based on the number of hidden layers from each dataset and replace thatwithamoduleofsameoutputlabelfromamorecomplexmodel modelwithfourhiddenlayers .finally wecomputethecomposed accuracyofthemodulesandcomparethemwiththeaccuracyof the dnn model from which the modules were decomposed and the prior accuracy of the modules.
table 4depicts the scenarios for four datasets and we report the composed accuracy for mnist1 fmnist kmnist and emnist with modules replaced from mnist fmnist kmnist and emnist respectively.
898on decomposing adeep neural network intomodules esec fse november8 virtualevent usa we found that replacing modules with decomposed modules from morecomplexmodelscanincreasethecomposedaccuracyofthe decomposed modules for out of scenarios.
on average there is an average .
median is .
drop in the accuracy when compared to the composed accuracy of the modules before replacement.furthermore weevaluatetheinterdatasetreplacementon the example depicted in figure .
to make a part of the model faulty we impose bias in the training dataset.
we build a dnn model with training examples for all the output labels except theoutputlabel5 whereweuse500examples.thetraineddnn model achieves testing accuracy of .
.
then we decompose the dnn model intro modules and replace the module with a module responsible for the same output label decomposed from mnist .ourresultshowsthattheaccuracyafterthereplacement is98.
.
.thus wecanconcludethatourapproachcanbe utilizedtoreplaceafaultypieceofdnnmodelwithaparttaken from abetterdnn model.
interdatasetreplacement.
while a module can be replaced with a module with the same concern there can be a situation that needs amodule to be replaced with a different concern.
here wereplaceonemodulefromeachdatasetandreplacethatwitha module taken from a different dataset e.g.
the module responsible for classifying english replaced with a module for classifying japanese .
we validate our approach to replacing the modules from different datasets by conducting the experiments on the modules decomposed from the dnn models with four hidden layers mnist emnist and kmnist .
our scenarios involve replacing modules from mnist with kmnist and modules from mnist with emnist.
our results suggest that the accuracy of the replaced modules perform worse than the dnn models trained withthesamestructure fourhiddenlayers withthetrainingexamples from the same output classes.
in table we depict such scenarios.ourevaluationshowsthatbyreplacingmodulesfrom mnist with extended mnist theaccuracy is decreased .
on average median1.
in comparison to the dnn models trained withthesameconfigurations.in thecaseofsubstitutingmodules from mnist with kuzushiji mnist the accuracy drop is .
median .
compare to the models trainedfrom scratch.
based on the overall evaluation we found that replacing a modulewithsimilarconcernsanddifferentconcernscanbeachieved.
however thereisalossofaccuracyincomparisontothemodels trainedfrom scratch.
.
.
summary.
wefoundthatthecompositionofthedecomposed modules losses only .
accuracy on average and it performs the same or the better for .
out of cases compared to themonolithicmodels.also reusingthemodulesfromthesame problem can gain .
accuracy on average compared to the modeltrainedwiththesamedatasets.however reusingmodules from differentdatasets loss6.
accuracy on average.
we found that replacing a module with another module decomposed from a differentmodel losses0.
and .
for intra datasetand interdataset scenarios respectively.
threats to validity external threats.
while the idea of decomposing deep neural networksintomodulesisgeneral ourdecompositionalgorithmshave utilized certain simplifying assumptions about the input deep neuralnetworks.inthiscurrentwork wehavetackledfeedforward fullyconnectedneuralnetworks thatusereluandsoftmaxasthe activationfunctionforallhiddenlayersandoutputlayer respectively.
further research is needed to generalize these algorithms to other categories of deep neural networks.
in particular concern identification will need to be generalized to other kinds of deep neural network layers.
we anticipate that tangling identification and concern modularization techniques will be applicable to other kindsoflayers but do not have evidenceas of this writing.
internalthreats.
inthisstudy aninternalthreatcanbethechoice of datasets for evaluation.
to alleviate this threat we have used mnistandfashionmnistfromkerasandkuzishijimnistand extendedmnistfrom tensorflow.
conclusion inthiswork weexploredwhetheradnncanbedecomposedso that parts of the dnn which we call dnn modules can be reused to build different dnns or replaced with better dnn modules.
we described our technique that relies on concern identification to identify thesub network identifyingtanglingofother concerns andfinallydecomposingthesub networkintodnn modules.
we described four different techniques for reducing tangling.
we have evaluatedourapproachusingfourcanonicaldatasetsandsixteen differentmodels.oftendecompositionintomoduleshassomecosts butwefindthatthecostsfordnndecompositionisalreadyvery minimal.in56.
ofcases decomposedmodulesareslightlymore accurate .
.
and in remaining cases lose very little accuracy .
.
.
the benefits of decomposition are observed in enablingreuseand replacement.we observethatforour datasets and models both reuse and replacement is possible.
based on these results webelievethatthisworktakesthefirststeptowardenabling more modulardesignsfor deep learning.
there are a number of exciting avenues for future work e.g.
betterdecompositiontechniquesbuildingonthisworkcanbedeveloped to improve the accuracy of dnn modules in reuse and replacementscenarios decompositionforotherkindsofdnnscan bedeveloped unit testingofdnnmodulescouldbeexplored types andcontractsfordnnmodulescouldbedeveloped composition ofdnn moduleswithtraditional functionscould be explored etc.
though our current algorithms are implemented based on a certainassumptions webelievethatthekeyconceptofdecomposinga monolithic model into smaller components can be applied in other kinds of deepneural networks.
for instance for models built with convolutionlayers theconceptofactive inactivenodecancertainly be altered to introduce the sliding window based approach that captures the section ofthe network responsible for aconcern.