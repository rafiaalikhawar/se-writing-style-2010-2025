primers or reminders?
the effects of existing review comments on code review davide spadini d.spadini sig.eu software improvement group delft university of technology amsterdam delft the netherlandsg l alikli gul.calikli gu.se chalmers university of gothenburg gothenburg swedenalberto bacchelli bacchelli ifi.uzh.ch university of zurich zurich switzerland abstract in contemporary code review the comments put by reviewers on a specific code change are immediately visible to the other reviewers involved.
could this visibility prime new reviewers attention due to the human s proneness to availability bias thus biasing the code review outcome?
in this study we investigate this topic by conducting a controlled experiment with developers who perform a code review and a psychological experiment.
with the psychological experiment we find that of participants are prone to availability bias.
however when it comes to the code review our experiment results show that participants are primed only when the existing code review comment is about a type of bug that is not normally considered when this comment is visible participants are more likely to find another occurrence of this type of bug.
moreover this priming effect does not influence reviewers likelihood of detecting other types of bugs.
our findings suggest that the current code review practice is effective because existing review comments about bugs in code changes are not negative primers rather positive reminders for bugs that would otherwise be overlooked during code review.
data and materials ccs concepts software and its engineering software verification and validation .
keywords code review priming availability heuristic acm reference format davide spadini g l alikli and alberto bacchelli.
.
primers or reminders?
the effects of existing review comments on code review.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction peer code review is a well established practice that aims at maintaining and promoting source code quality as well as sustaining permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for third party components of this work must be honored.
for all other uses contact the owner author s .
icse may seoul republic of korea copyright held by the owner author s .
acm isbn .
teams by means of improved knowledge transfer awareness and solutions to problems .
in the code review type that is most common nowadays theauthor of a code change sends the change for review to peer developers also knowns as reviewers before the change can be integrated in production.
previous research on three popular opensource software projects has found that three to five reviewers are involved in each review .
using a software review tool the reviewers and the author conduct an asynchronous online discussion to collectively judge whether the proposed code change is of sufficiently high quality and adheres to the guidelines of the project.
in widespread code review tools reviewers comments are immediately visible as they are written by their authors could this visibility bias the other reviewers judgment?
if we consider the peer review setting for scientific articles reviewers normally judge at least initially the merit of the submitted work independently from each other.
the rationale behind such preference is to mitigate group members influences on each other that might lead to errors in the individual judgments .
it is reasonable to think that also in code review the visibility of existing review comments made by other developers may affect one s individual judgment leading to an erroneous judgment.
an existing comment may prime new reviewers on a specific type of bug due to the availability bias .
availability bias is the tendency to be influenced by information that can be easily retrieved from memory i.e.
easy to recall .
this bias is one of the many cognitive biases identified in psychology sociology and management research .
cognitive biases are systematic deviations from optimal reasoning .
in the cognitive psychology literature kahneman and tversky showed that humans are prone to availability bias .
for example one may avoid traveling by plane after having seen recent plane accidents on the news or may see conspiracies everywhere as a result of watching too many spy movies .
therefore it seems fitting to imagine that a reviewer may be biased toward a certain bug type by readily seeing another reviewer s comment on such a bug type.
this bias would likely result in a distorted code review outcome.
in this paper we present a controlled experiment we devised and conducted to test the current code review setup and reviewers proneness to availability bias.
more specifically we examine whether priming a reviewer on a bug type achieved by showing an existing review comment biases the outcome of code review.
our experiment was completed by developers of which reported to have at least three years of professional development experience.
we required each developer to conduct a code review in which an existing comment was either shown treatment group or ieee acm 42nd international conference on software engineering icse not control group .
we then measured to what extent the reviewers could find in the same code change another bug of the same type as the primed one and a bug of a different type.
we created a setup with two different code changes to review.
based on the availability bias literature we expected the primed participants treatment group to be more likely to find the bug of the same type as it is already available in memory but less likely to find the other bug type since distracted by the comment .
surprisingly instead our results show that for three out of four bugs the code review outcome does not change between the treatment and control groups.
after testing our results for robustness we could find no evidence indicating that for these three bugs the outcome of the review is biased in the presence of an existing review comment priming them on a bug type.
only for one bug type though we have strong evidence that the behavior of the reviewers changed when the previous review comment was about a type of bug that is normally notconsidered during developers coding review practices i.e.
checking for nullpointerexception on a method s parameters the reviewers were more likely to find the same type of bug with a strong effect.
overall we interpret the results of our experiment as an indication that existing review comments do not act as negative primers rather as positive reminders.
as such our experiment provides evidence that the current collaborative code review practice adopted by most software projects could be more beneficial than separate individual reviews not only in terms of efficiency and social advantages but also in terms of its effectiveness in finding bugs.
background and related work in this section we review the literature on human aspects in contemporary code review practices as well as studies on scientific peer review.
subsequently we provide background on cognitive biases in general and present relevant studies in software engineering se .
we also provide a separate subsection on availability bias which consists of some theoretical background and existing research on availability bias in se.
.
human aspects in modern code review past research has provided evidence that human factors determine code review performance to a significant degree and that code review is a collaborative process .
empirical studies conducted at companies such as google and microsoft revealed that besides finding defects and ensuring maintainability motivations for reviewing code are knowledge transfer e.g.
education of junior developers and improving shared code ownership which is closely related to team awareness and transparency.
besides being a collaborative activity code review is also demanding from a cognitive point of view for the individual reviewer.
a large amount of research is focused on improving code review tools and processes based on the assumption that reducing reviewers cognitive load improves their code review performance .
for instance baum et al.
argue that the reviewer and review tool can be regarded as a joint cognitive system also emphasizing the importance of off loading cognitive process from the reviewer to the tool.
ebert et al.
conducted a study to understand the factors that confuse code reviewers through manual analysis of800 comments from code review of the android project and later they built a series of automatic classifiers e.g.
multinomial naive bayes oner for identification of confusion in review comments.
baum et al.
conducted experiments to examine the association of working memory capacity and cognitive load with code review performance.
they found that working memory capacity is associated with the effectiveness of finding de localized defects.
however authors could not find substantial evidence on the influence of change part ordering on mental load or review performance.
spadini et al.
designed and conducted a controlled experiment to investigate whether examining changed test code before the changed production code also known as test driven code review or tdr affects code review effectiveness.
according to the findings of spadini et al.
developers adopting tdr find the same amount of defects in production code but more defects in test code and fewer maintainability issues in the production code.
significantly related to the work we present in this paper is the recent empirical observational study by thongtanunam and hassan .
they investigated the relationship between the evaluation decision of a reviewer and the visible information about a patch under review e.g.
comments and votes by prior co reviewers .
with an observational study on tens of thousands of patches from two popular open source software systems thongtanunam and hassan found that the amount of feedback and co working frequency between reviewer and patch author are highly associated with the likelihood of the reviewer providing a positive vote and that the proportion of reviewers who provided a vote consistent with prior reviewers is significantly associated with the defect proneness of a patch even though other factors are stronger .
these results corroborate the hypothesis that there is some sort of influence generated by the visible information about the change under review on the behavior of the reviewers .
in the work we present in this paper we setup a controlled setting to investigate an angle of this influence further hoping to shed more light on the causal connection between comments visibility and reviewers effectiveness.
.
scientific peer review peer review is the main form of group decision making used to allocate scientific research grants and select manuscripts for publication.
many studies demonstrated that individual psychological processes are subject to social influences .
such finding also points out some issues that might arise during group decision making.
experimental results obtained by deutsch and gerard show that when a group situation is created normative social influences grossly increase leading to errors in individual judgment.
based on the findings of this study it is emphasized that group consensus succeeds only if groups encourage their members to express their own independent judgments.
therefore one of the procedures for peer review of scientific research grant applications is written individual review .
with this review procedure reviewers judge the merit of a grant application in written form independently of one another before the final decision maker approves or rejects an application.
written individual review can mitigate the influence of reviewers on the way to reach a collective judgment.
it is also used in scientific venues to eliminate biases.
there is also another form of review procedure namely panel peer review where a common 1172judgment is reached through mutual social exchange .
in panel peer review a group of reviewers convene to jointly deliberate and judge the merit of an application before the funding decision is made.
however as also emphasized by deutsch and gerard it is crucial to encourage individual members to express their own judgment without feeling under the pressure of normative social influences for proper functioning of group decision making.
.
cognitive biases in software engineering cognitive biases are defined as systematic deviations from optimal reasoning .
in the past six decades hundreds of empirical studies have been conducted showing the existence of various cognitive biases in humans thought processes .
although many theories explain why cognitive biases exist baron stated that there is no evidence so far about the existence of a single reason or generative mechanism that can explain the existence of all cognitive bias types.
some theories see cognitive bias as the by product of cognitive heuristics that humans developed due to their cognitive limitations e.g.
information processing power and time pressure whereas some relate them to emotions.
human cognition is a crucial part of software engineering research since software is developed by people for people.
in their systematic mapping study mohanini et al.
report different cognitive biases that have been investigated by software engineering studies so far.
according to the results of this systematic mapping study the cognitive biases that are most common in software engineering studies are anchoring bias confirmation bias and overconfidence bias.
anchoring bias results from forming initial estimates about a problem under uncertainty and focusing on these initial estimates without making sufficient modifications in the light of more recently acquired information .
anchoring bias has so far been studied in software engineering research within the scope of requirements elicitation pair programming software reuse software project management and effort estimation .
confirmation bias is the tendency to search for interpret favor and recall information in a way that affirms one s prior beliefs or hypotheses .
the manifestations of confirmation bias during unit testing and how it affects software defect density have been widely studied in software engineering literature .
any positive effect of experience on mitigation of confirmation bias has not been discovered so far .
however in some studies participants who have been trained in logical reasoning and hypothesis testing skills were manifested less tendency towards confirmatory behavior during software testing .
ko and myers identify confirmation bias among the cognitive biases that cause errors in programming systems .
van vliet and tang indicate that during software architecture design some organizations assign devil s advocate so that one s proposal is not followed without any questioning .
overconfidence bias manifests when a person s subjective confidence in their judgement is reliably greater than the objective accuracy of such a judgement .
this bias type has been studied within the context of pair programming requirements elicitation and project cost estimation .
availability bias.
availability bias is the tendency to be influenced by information that can be easily retrieved from memory i.e.
easyto recall .
the definition of availability bias was first formulated by tversky and kahneman who conducted a series of experiments to explore this judgemental bias.
however including these original experiments many psychology experiments do not go beyond comparing two groups i.e.
controlled and test group to differ in availability.
to the best of our knowledge in cognitive psychology literature the only experiment providing evidence for the mediating process that manifests availability bias was devised by gabrelcik and fazio who employed memory priming as the mediating process .
availability bias has also been studied in se research.
de graaf et al.
examined software professionals strategies to search for documentation by using think aloud protocols.
authors claim that using incorrect or incomplete set of keywords or ignoring certain locations while looking for documents due to availability bias might lead to huge losses.
mohan and jain claim that while performing changes in design artifacts developers due to availability bias might focus on their past experiences since such info can be easily retrieved from developers memory.
however such information might be inconsistent with the current state of the software system.
mohan et al.
propose traceability among design artifacts as a solution to mitigate the negative effects of the availability bias and other cognitive biases i.e.
anchoring and confirmation bias .
robins and redmiles propose a software architecture design environment reporting that it supports designers by addressing their cognitive challenges including availability bias.
j rgensen and sj berg argue that while learning from software development experience learning from the right experiences might be hindered due to availability bias.
authors suggest retaining post mortem project reviews to mitigate negative effects of availability bias.
overall existing literature points to the potential risks associated with availability bias in se.
as our community has provided evidence that code review is a collaborative and cognitively demanding process and that the collaborative nature of code review also has the potential to affect individual reviewers cognition availability bias could manifest itself during the code review process.
this bias could hamper code review effectiveness.
in our study we aim to explore how existing review comments bias the code review outcome.
experimental design in this section we explain the design of our experiment.
.
research questions and hypotheses the paper is structured along two research questions.
by answering these research questions we aim to understand to what extent contemporary code review is robust to reviewers availability bias depending on the nature of the bug for which a previous comment exists on the code change.
our first research question and the corresponding hypotheses follow.
rq .what is the effect of priming the reviewer with a bug type that is not normally considered?
1173we hypothesize that an existing review comment about a bug type that reviewers do not usually consider such as a null value passed as an argument might prime the reviewers towards this bug type so they find more of these bugs.
also we hypothesize that due to such priming reviewers overlook bugs on which they were not primed.
hence our formal hypotheses are h010 priming subjects with bugs they usually do notconsider does not affect their performance in finding bugs of the same type.
h011 priming subjects with bugs they usually do notconsider does not affect their performance in finding bugs they usually look for.
we also explore how priming on a bug that is usually considered during code reviews affects review performance.
therefore our second research question is rq .what is the effect of priming the reviewer with a bug type that is normally looked for?
we hypothesize that also in the case of an existing review comment about a bug type that reviewers usually consider primes the reviewers towards this bug type so that they find more of these bugs.
also we expect primed reviewers to only look for the type of bugs on which they are primed overlooking others.
hence our formal hypotheses are h020 priming subjects with bugs they usually consider does not affect their performance in finding bugs of the same type.
h021 priming subjects with bugs they usually consider does not affect their performance in finding bugs they usually do not look for.
.
experiment design and structure to conduct the code review experiment and to assess participants proneness to availability bias we extend the browser based tool crexperiment .
the tool allows us to i visualize and perform a code review ii collect data through questions asking for subjects demographics information as well as data consisting of participants interactions with the tool iii collect data to measure subjects proneness to availability bias by using a memory priming set up to trigger subjects use of availability heuristic that is followed by a survey.
both the priming set up and the survey are inherited from a classic experiment in cognitive psychology literature that was designed by gabrielcik and fazio .
code review experiment overview.
for the code review experiment we follow independent measures design augmented with some additional phases.
the following stages in the browser based tool correspond to the code review experiment welcome page the welcome page provides participants with information about the experiment.
this page also aims to avoid demand characteristics which are cues and hints that can make the participants aware of the goals of this research study leading to change in their behaviour during the experiment.
for this purpose we do not inform the participants about the full purpose of the experiment rather they are only told that the experiment aims to compare code review performance under different circumstances.
beforestarting the experiment the subjects are also asked for their informed consent.
participants demographics on the next page subjects are asked questions to collect demographic information as well as confounding factors such as i gender ii age iii proficiency in the english language iv highest obtained education degree v main role vi years of experience in software development vii current frequency in software development viii years of experience in java programming ix years of experience in code reviews x current frequency of code reviews and xi the number of hours subjects worked that day.
it is kept mandatory that subjects answer these questions before proceeding to the next page where they will receive more information about the code review experiment they are about to take part in.
we ask these questions to measure subjects real relevant and recent experience.
collecting such data helps us to identify which portion of the developer population is represented by subjects who take part in our experiment .
actual experiment each participant is then asked to perform a code review and is randomly assigned to one of the following two treatments pr primed the subject is given a code change to review where there exists a review comment made by a previous reviewer about a bug in the code.
the test group of our experiment comprises the subjects who are assigned to this treatment.
npr not primed the subject is given a code change to review.
in the code change there are no comments made by any other reviewers.
the control group of our experiment comprises the subjects who are assigned to this treatment.
more specifically the patch to review contains three bugs two of the same type i.e.
bug a and one of a different type i.e.
bug b .
in the prgroup the review starts with a comment made by another reviewer showing that one instance ofbug ais present.
the participant is then asked to continue the review.
in the npr group the review starts without comments.
the comments shown to the participants in the pr group were written by the authors and the wording was refined with the feedback from the pilots section .
.
each participant is asked to take the task very seriously.
more specifically we ask them to find as many defects as possible and like in real life spend as little time as possible on the review.
however unlike in real life we ask them not to pay attention to maintainability or design issues but only in correctness issues bugs .
for example we discard comments regarding variable namings or small refactorings.
interruptions during the experiment immediately after completing the code review the participants are asked whether they were interrupted during the task and for how long.
follow up questions in the last page of the code review experiment the participants are shown the code change they just reviewed together with the bugs disclosed for each bug we show it and explain why it is a defect and in what cases 1174figure example of a code review using the tool.
it might fail.
then for each bug we ask the participants to indicate whether they captured it in the review if the participants found the bug and they belonged to the prgroup we ask them to what extent the comment of the previous reviewer influenced the discovery of the bug using a point likert scale .
if the participants did not find the bug independently whether they were in the prornpr group we ask them to elaborate on why they think they missed the bug.
assessment of proneness to availability bias.
the code review experiment is followed by a set up that primes participants memory to trigger availability bias.
this set up serves as a mediating process to manipulate availability bias so that we can measure the extent to which each subject is prone to this type of cognitive bias.
to measure this phenomenon we inherited the test part of the controlled experiment of gabrielcik and fazio .
in the original experiment the difference in the results of control and test groups showed that memory priming triggered the participants availability biases.
there are three reasons why we selected this experiment for assessing the proneness to availability bias i to the best of our knowledge it is the only experiment where the underlyingcognition mechanism i.e.
memory priming that triggers availability bias is explicitly devised ii memory priming mechanism is also employed in code review experiment to trigger participants availability bias and iii survey in the original experiment makes it possible to quantitatively assess participants proneness to availability bias.
therefore the remaining stages in the browser based tool comprise the following welcome page we provide a second welcome page in which to avoid demand characteristics the participants are told that they are about to participate in an experiment that aims to explore software engineers attention by testing a set of visual stimuli instead of the actual goal.
warm up session we proceed with a warm up session in which participants are asked to focus on a series of words flashing once each on the screen.
the words are randomly selected from the english dictionary and none of them contain the letter t .
each word flashes for 300ms.
at the end of the warm up we ask the participants to write three words they have seen and recall and to make a guess if they do not remember them.
actual psychology experiment after the warm up we proceed with the actual psychology experiment this time we show two series of words all of them including the 1175letter t .
this time words flash at a faster rate i.e.
150ms to avoid that the participants consciously recognize that the words have the letter t so often which would bias their last task .
after each series we ask the participant to write three words they have seen and recall and to make a guess if they do not remember them.
measuring proneness to availability bias the last task of the participants is to answer questions which ask to compare the frequency words for a given pair of letters in the english dictionary.
for example given the question do more words contain t or s participants responded on a point scale with one end labeled many more contain t and the other many more contain s .
our main goal is to measure the extent to which each subject is prone to availability bias.
hence in of the questions we ask whether in the english dictionary there are more words containing the letter t or another random letter.
as in the experiment of gabrielcik and fazio we expect the participants to indicate that there are more words containing the letter t even though this is not the case since they were primed in step .
the other questions are used to prevent the participants from understanding the actual aim of the study.
.
objects the objects of the study are represented by the code changes or patch for brevity to review and the bugs that we selected and injected which must be discovered by the participants.
patches.
to avoid giving some developers an advantage the two patches are not selected from open source software projects hence they are not known to any of the participants.
to maintain the difficulty of the code review reasonable after all developers are used to review only the codebase on which they work every day we screen many websites that offer java exercises searching for exercises that are neither too trivial nor too complicated based on our experience teaching programming to students self contained and do not rely on special technologies or frameworks libraries.
after several brain storming sessions among the authors only two exercises satisfied these goals and were selected.
defects.
code review is a well established and widely adopted practice aimed at maintaining and promoting software quality .
there are different reasons on why developers adopt this practice but one of the main ones is to detect defects .
hence in our experiment we manually seed bugs functional defects in the code.
more specifically we seed two different types of bugs one that could cause a nullpointerexception bug a and one that could cause the return of a wrong value bug b .
the bugs were injected in the code as follows inpatch we inject two bug aand one bug b the priming is done on bug a inpatch we inject two bug band one bug a the priming is done on bug b thenullpointerexception bug a in the first change was on the passed parameters.
as reported by white and gray literature developers are not used to check for this kind of errors in code review because they expect the caller to make sure the parameters are not null hence we use it as the not normallyconsidered bug that we investigate in rq .
instead bug ain the second change rq does not regard a parameter to make sure that it is bug type that normally developers look for in a review.
.
variables and measurement details we aim to investigate whether participants that are primed on a specific type of bug are more likely to capture only that type of bug.
to understand whether the subjects did find the bug i.e.
the value for our dependent variables we proceed with the following steps the first author of this paper manually analyzes all the remarks added by the participants each remark is classified as identifying a bug or being outside of the study s scope then the authors cross validate the results with the answer given by the participants as explained in section .
after the experiment the participants had to indicate whether they captured the bugs .
in table we represent all the variables of our model.
the main independent variable of our experiment is the treatment prornpr .
we consider the other variables as control variables which also include the time spent on the review the participant s role years of experience in java and code review and tiredness.
finally we run a logistic regression model similar to the one used by mcintoshet al.
and spadini et al.
.
to ensure that the selected logistic regression model is appropriate for the available data we first compute the variance inflation factors vif as a standard test for multicollinearity finding all the values to be below values should be below thus indicating little or no multicollinearity among the independent variables run a multilevel regression model to check whether there is a significant variance among reviewers but we found little to none thus indicating that a single level regression model is appropriate and finally when building the model we added the independent variables step by step and found that the coefficients remained stable thus further indicating little to no interference among the variables.
for convenience we include the script to our publicly available replication package .
availability bias score.
we calculate availability bias scores as in the original experiment by gabrielcik and fazio .
the frequency comparisons on the point scale were scored by assignments of a value between 4and .
positive numbers were assigned for ratings indicating that letter t was contained in more words than the other letter while negative numbers were assigned in favour of the other letter.
we calculated the availability bias score for each participant as the average and also median of values for the relevant questions.
.
pilot runs as the first version of the experiment was ready we started conducting pilot runs to verify the absence of technical errors in the online platform check the ratio with which participants were able to find the injected bugs regardless of their treatment group tune the experiment on the proneness to availability bias in terms of flashing speed and number of words to ask verify the understandability of the instructions as well as the user interface and gather qualitative feedback from the participants.
we conducted three different pilot runs for a total of developers.
the participants were recruited through the professional network of the study authors to ensure that they would take the task seriously and 1176table variables used in the statistical model.
metric description dependent variables foundprimedthe participant found the bug that was primed foundnotprimedthe participant found the bug that was not primed independent variable treatment type of the treatment pr ornpr control variables gender gender of the participant englishlevel english level age age of the participant levelofeducation highest achieved level of education role role of the participant profdevexpyears of experience as professional developer javaexp years of experience in java programpractice how often they program reviewpractice how often they perform code review reviewexp years of experience in code review workedhourshours the participant worked before performing the experiment tiredhow tired was the participant at the moment of taking the experiment stressedhow stressed was the participant at the moment of taking the experiment interruptionsfor how long the participant was interrupted during the experiment totalduration total duration of the experiment psychoexpisprimedwhether the participant was primed in the psychology experiment see figure for the scale provide feedback on their experience.
no data gathered from the participants to the pilot was considered in the final experiment.
after each pilot run we inspected the results and the qualitative feedback we received and discussed extensively among the authors to verify whether parts of the experiment should have been changed.
after the third run the required changes were minimal and we considered the experiment ready for its main run.
.
recruiting participants the experiment was spread out through practitioners blogs and web forums e.g.
reddit and through direct contacts from the professional network of the study authors as well as the authors social media accounts on twitter and facebook.
we did not reveal the aim of the experiment.
to provide a small incentive to participate we introduced a donation based incentive of five usd to a charity per valid respondent.
threats to validity construct validity.
threats to construct validity concern our research instruments.
to measure the extent to which subjects areprone to availability bias we used the memory priming mechanism and the survey that was employed in an experiment designed and conducted by gabrielcik and fazio in cognitive psychology literature .
data obtained from the controlled experiment that gabrielcik and fazio conducted provide direct evidence that memory priming can be a mediating process to trigger availability bias.
the remaining constructs we use are defined in previous publications and we reuse the existing instruments as much as possible.
for instance the tool employed for the online experiment is based on similar tools used in earlier works .
to avoid problems with experimental materials we employed a multi stage process after tests among the authors we conducted three experiments with subjects each time for a total of pilots with external participants.
after each pilot session we made corrections to the experiment based on the feedback from the subjects of the pilot materials were checked by the authors one more time before we launched the actual experiment.
regarding defects and code changes the first author prepared the code changes and corresponding test codes as well as injecting the defects into these code changes.
these were later checked by the other authors.
code change and corresponding test code were on the same page and subjects had to scroll down to proceed to the next page of the online experiment.
in this way we aimed to ensure that subjects saw the test code.
test code were added to make the experiment closer to a real world scenario.
a major threat is that the artificial experiment created by us could differ from a real world scenario.
we mitigated this issue by re creating as close as possible a real code change for example submitting test code and documentation together with the production code and using an interface that is identical to the common code review tool gerrit both our tool and gerrit use mergely to show the diff also using the same color scheme .
internal validity.
threats to internal validity concern factors that might affect the cause and effect relationship that is investigated through the experiment.
due to the online nature of the experiment we cannot ensure that our subjects conducted the experiments with the same set up e.g.
noise level and web searches however we argue that developers in real world settings also have a multi fold of tools and environments.
moreover to mitigate the possible threat posed by missing control over subjects we included some questions to characterize our sample e.g.
experience role and education .
to prevent duplicate participation we adjusted the settings of the online experiment platform so that each subject can take the experiment only once.
to exclude participants who did not take the experiment seriously we screened each review and we did not consider experiments without any comments in the review that took less than five minutes to be completed or that were not completed at all.
furthermore several background factors e.g.
age gender experience education may have impact on the results.
hence we collected all such information and investigated how these factors affect the results by conducting statistical tests.
external validity.
threats to external validity concerns the generalizability of results.
to have a diverse sample of subjects representative of the overall population of software developers who 1177employ contemporary code review we invited developers from several countries organizations education levels and background.
never once a yearonce a monthonce a weekonce a day or morepractice programming reviewing20406080 20406080software development reviewing no experience year or less2 years years years yearsexperience programmer researcher student ph.d.role other89 figure participants characteristics results in this section we report the results of our investigation on whether and how having a comment from a previous reviewer influences the outcome of code review.
.
validating the participants a total of people accessed our experiment environment following the provided link.
from these participants we exclude all the instances in which the code change is skipped or skimmed by demanding either at least one entered remark or more than five minutes spent on the review.
after applying the exclusion criteria a total of participants are selected for the subsequent analyses.
figure presents the descriptive statistics on what the participants reported in terms of their role experience and practice.
the majority of the participants are programmer and reported to have many years of experience in professional software development more than years more than most program daily and review code at least weekly .
table represents how the participants are distributed across the considered treatments and code changes.
the automated assignment algorithm allowed us to obtain a rather balanced number of reviews per treatment and code change.table distribution of participants n across the various treatment groups.
primed pr not primed npr total codechange1 codechange2 total table odds ratio for capturing the primed and not primed bug in the test pr and control npr group.
primed bug npe primed pr not primed npr total found not found odds ratio .
.
.
p .
not primed bug primed pr not primed npr total found not found odds ratio .
.
.
p .
.
rq .
priming a not commonly reviewed bug to investigate our first research question the participants in our test group pr are primed on a nullpointerexception npe bug in a method s parameter.
we expect this type of bug to be missed by most not primed reviewer because normally reviewers would assume that parameters are checked from the calling function .
table reports the results of the experiment by treatment group.
from the first part of the table primed bug we can notice that participants in the prgroup found the other npe bug of the times while participants in the npr group only .
expressed in odds this result means that the npe defect is times more likely to be found by a participant in the prgroup.
the main reasons reported by the participants in the npr for missing this bug are that they were too focused on the logic and not thoroughly enough when it comes the corner cases did not put attention to the fact that integer could be null and that they generally do not check for npe but assume to not receive a wrong object as an input.
as expected even though nullpointerexception has been reported to be the most common bug in java programs developers stated they rarely sanity check the object.
however as shown in table the result drastically changes when a previous reviewer points out that an npe could be raised in this case many of the participants in the prgroup looked for other npe bugs in the code.
when we look at whether the prgroup was primed by the previous reviewer comment hence whether they were able to capture the bug because of they have been primed we have that indicated they were extremely influenced were very influenced and instead were somewhat influenced .
hence the reviewers perceived to have been influenced by the existing comment.
we find a statistically significant relationship p .
assessed using 2 of strong positive strength .
between the 1178table regressions for primed and not primed bugs.
prime dbug notprime d bug estimate s.e.
sig.
estimate s.e.
sig.
inter cept .
.
.
.
isprime d .
.
.
.
totalduration .
.
.
.
.
profde vexp .
.
.
.
programpractice .
.
.
.
reviewexp .
.
.
.
reviewpractice .
.
.
.
.
tired .
.
.
.
workedhours .
.
.
.
interruptions .
.
.
.
... significance codes p .
p .
p .
.
p .
role is not significant and omitted for space reason presence of the comment and whether the same type of bug was found.
therefore we can reject h010.
considering the second part of table we see that the not primed bug was found by both groups prandnpr at similar rate.
for the former participants found it of the times while in the npr they found it of the times.
as shown in the table the difference is not statistically significant p .
.
when looking at the participants comments on why they missed this bug we have that the main reasons are that they forgot to try the specific corner case and that they assumed tests were covering all the corner cases.
the reasons for not capturing the defects were similar in both groups.
given this result we cannot rejecth011.
priming the participants on a specific type of bug did notprevent them from capturing the other type of bug.
in table we show the result of our statistical model taking into account the characteristics of the participants and reviews.
the model confirms the result shown in table even taking into account all the variables the isprimed variable is statistically significant exclusively for the primed bug.
the other variable statistically significant in the model is interruptions that is the number of times the participant has been interrupted during the experiment the estimate has a negative value which means the higher the number of interruptions the lower the number of bugs captured as one can expect.
for the not primed bug instead none of the variables are statistically significant with totalduration and reviewexp are slightly significant with p .
finding .
reviewers primed on a bug that is not commonly considered are more likely to find other occurences of this type of bugs.
however this does not prevent them in finding also other types of bugs.
.
rq2.
priming on an algorithmic bug to investigate our second research question the participants in our test group pr are primed on an algorithmic bug more specifically a corner case cc bug.
the result of this experiment is shown intable odds ratio for capturing the primed and not primed bug in the test pr and control npr group.
primed bug cc primed pr not primed npr total found not found odds ratio .
.
.
p .
not primed bug primed pr not primed npr total found not found odds ratio .
.
.
p .
table .
participants in both groups found the primed bug .
indeed the difference is not statistically significant p .
.
if we consider whether the test group was primed by the previous reviewer comment of the participants reported that they were extremely influenced was somewhat influenced and was slightly or not influenced thus suggesting that even the reviewers noticed a lower influence from this comment even though it was of the same type as one of the other two bugs in the same code change.
among the main reasons for missing the bug participants mainly stated that the tests drove them to not remember that corner case and they focused more on the first one.
hence given this result we can conclude that the participants who saw the review comment didnotfind the similar bug more often than the participants that did not see the review comment.
in the second part of table we indicate whether the participants were able to find the not primed bug.
both the test and control group are very similar in this case too.
indeed in both groups the bug is found around of the times and the difference is not statistically significant.
when looking at the participants comments on why they missed this bug the main reasons they state are that they were too focused on capturing algorithmic bugs without paying attention to npe and that as in the previous rq they did not put attention to the fact that integer could be null.
given these results we cannot reject h020norh021.
in table we show the result when controlling for other variables.
our dependent variable isprimed is not statistically significant.
however we see that totalduration i.e.
the time required by the developer to complete the code review is statistically significant and in the expected direction.
for the npr group the only variable that is significant is reviewpractice i.e.
the average number of time the participant perform code reviews .
both these results are in line with what found in previous research .
finding .
reviewers primed on an algorithmic bug perceive an influence but are as likely as the others to find algorithmic bugs.
furthermore primed participants did not capture fewer bugs of the other type.
1179table regressions for primed and not primed bugs.
primed bug not primed bug estimate s.e.
sig.
estimate s.e.
sig.
intercept .
.
.037e .568e isprimed .
.
.670e .740e totalduration .
.
.561e .976e profdevexp .
.
.
.437e .721e programpractice .
.
.061e .353e reviewexp .
.
.284e .660e reviewpractice .
.
.211e .683e tired .
.
.486e .539e workedhours .
.
.257e .542e interruptions .
.
.331e .630e ... significance codes p .
p .
p .
.
p .
role is not significant and omitted for space reason .
robustness testing in the previous sections we presented the results of our study on whether and to what extent reviewers can be primed during code review by showing an existing code review comment.
surprisingly the results showed that many of our hypotheses were not satisfied in our experiment only in one case primed reviewers captured more bugs than the not primed group in all the other cases reviewers from both groups could capture the same bugs.
to further challenge the validity of these findings in this section we employ robustness testing .
for this purpose we test whether the results we obtained by our baseline model hold when we systematically replace the baseline model specification with the following plausible alternatives.
bugs were too simple or too complicated to find.
choosing the right defects to inject in the code change is fundamental to the validity of our results.
if a defect is too easy to find participants might find the bugs regardless of any other influencing factor even without paying too much attention to the review on the other hand if it is too complicated reviewers might not find any bug and get discouraged to continue .
we measure that of the participants found the three types of defects that we expected them to find thus ruling out the possibility that these bugs were either too trivial or too difficult to find.
people were not primed.
the entire experiment is based on the premise that reviewers in the prgroup were correctly primed.
even though we can not verify this premise the experiment is online hence there is no interaction between the researchers and the participants after the code review experiment the participants had to indicate whether they were influenced by the comment of the previous reviewer in capturing the bug.
as we stated in section .
and section .
of the participants indicated they were extremely or very influenced while only indicated somewhat or slightly influenced were neutral .
this gives an indication that the participants felt they were indeed primed but this did not influence their ability to find other bugs.
nevertheless the reported level of being influenced is subjective so not fully reliable participants could think to have been influenced but were not .
to triangulate this result we test another possibility more specifically one of the possible explanations of why participants may not have been primed is that our sample ofparticipants was immune to priming or very difficult to prime.
indeed there is no study that confirms that developers are as affected by priming as the general population on which past experiment was conducted .
to rule out this possibility we devised the psychological experiment we tested whether developers can also be primed as expected using visual stimuli.
our results show that of the participants were primed as expected.
not enough participants.
another possibility of why we do not find a difference is that we did not have enough participants.
even though participants is quite large in comparison to many experiments in software engineering and we tried to design an experiment that would create a strong signal we cannot rule out that the significance was missing due to the number of participants.
however even if the results were statistically significant assuming we had the same ratios but an order of magnitude more of participants the size of the effect calculated using the coefficient would be none to very negligible .
this suggests that there was no emerging trend and that even having more participants we could have probably obtained a significant yet trivial effect.
some participants did not perform the task seriously.
finally one of the reasons why we did not confirm most of our hypotheses could be that some participants did not take the task seriously hence they might have performed poorly and have altered the results.
having used a random assignment and having a reasonably large number of participants we have no reason to think that one group had more lazy participants than the others.
moreover as we discussed in section to exclude participants who did not take the experiment seriously we filtered out experiments without any comments in the review even if there were comments the first author manually validated them to check whether they were appropriate and they were not capturing a bug we also did not consider reviews that took less than five minutes to be completed or that were not completed at all maybe because the participant left after few minutes .
alternatively it would be possible that participants who were more serious focused more and found more bugs regardless of the priming while less serious ones would just find one and leave the experiment.
to test also this possibility we compared the likelihood of a participant in finding a second bug when a first one was found.
also in this case we did not find any statistically significant effect thus ruling out this hypothesis as well.
discussions we discuss the main implications and results of our study.
robustness of code review against availability bias.
the current code review practice expects reviewers to review and comment on the code change asynchronously and reviewers comments are immediately visible to both the author and other reviewers.
one of the main hypotheses we stated in our study is that the code review outcome is biased because reviewers are primed by the visibility of existing comment on a bug.
indeed if reviewers get primed by previously made comments about some bug s then they could find more bugs of that specific type while overlooking other types of bugs.
this would in turn undermine the effectiveness of the code review process creating a demand for a different approach.
1180to create a different approach one might consider adopting a review method similar to that of scientific venues where reviewers do not see the comments of the other reviewers until they submit their review.
even though this strategy would reduce the transparency of the code review process undermining knowledge transfer team awareness as well as shared code ownership and would probably lead to a loss in review efficiency due to duplicate bug detection it would be necessary if the biasing effect of other reviewers comments would be strong.
our experiment results show that the participants in the test group were positively influenced by the existing comment on the code change so that they could capture more bugs of the same type.
however unexpectedly they were still able to capture the bugs of the different type as the control group did.
like any human reviewers are also prone to availability bias to various extents.
however our results did not find evidence of a strong negative effect of reviewers availability bias.
therefore our data does not provide any evidence that would justify a change in the current code review practices.
existing comments on normally not considered bugs act as positive reminders rather than negative primers.
surprisingly participants in the test group who were primed with the algorithmic bug type more specifically a corner case bug detected the same amount of corner case and nullpointerexception npe bugs as the participants in the control group.
however participants who were primed with a bug that is normally not considered in review i.e.
npe were times more likely to capture this type of bug than the participants of the control group.
this result shows that existing reviewer comments on code change seem to support recalling i.e.
act as a reminder rather than distracting the reviewer.
as previously mentioned in section .
participants in the test group indicated that they were focused on to the corner cases in the code change and did not put attention to the possibility that integer could be null.
such feedbacks are in line with the possible existence of anchoring bias .
it is likely that the existence of a reviewer comment on a uncommon bug had a de biasing effect on the participants in the test group i.e.
mitigated the participants bias .
in software engineering literature there are empirical studies on practitioners anchoring bias.
for instance pitts and brown provide procedural prompts during requirements elicitation to aid analysts not anchoring on currently available information.
according to the findings by jain et al.
pair programming novices tend to anchor to their initial solutions due to their inability to identify such a wider range of solutions.
however to the best of our knowledge there are no studies on anchoring bias within the context of code reviews.
therefore further research is required to investigate underlying cognition mechanisms that can explain why existing reviewer comments on the unexpected bug act as reminders.
conclusions in the study presented in this paper we investigated robustness of peer code review against reviewers proneness to availability bias.
for this purpose we designed and conducted an online experiment with participants including a code review task and a psychological experiment.
with the psychological experiment themajority of the participants i.e.
were assessed to be prone to availability bias median .
max .
however when it comes to the code review our experiment results show that participants are primed only when the existing code review comment is about a type of bug that is not normally considered when this comment is visible participants are more likely to find another occurrence of this type of bug.
hence existing comments on this type of bugs acted as reminders rather than primers.
it is our hope that this study is replicated by other researchers to gain further insights about the extent of robustness of peer code review.