a study on the lifecycle of flaky tests wing lam university of illinois at urbana champaign urbana illinois usa winglam2 illinois.eduk van mu lu microsoft redmond washington usa kivanc.muslu microsoft.com hitesh sajnani microsoft redmond washington usa hitsaj microsoft.comsuresh thummalapenta microsoft redmond washington usa suthumma microsoft.com abstract during regression testing developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality.
thus flaky tests which nondeterministically pass or fail on the same code are problematic because they provide misleading signals during regression testing.
although flaky tests are the focus of several existing studies none of them study the reoccurrence runtimes and time before fix of flaky tests and flaky tests indepth on proprietary projects.
this paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects.
specifically we study the lifecycle of flaky tests in six large scale proprietary projects at microsoft.
we find as in prior work that asynchronous calls are the leading cause of flaky tests in these microsoft projects.
therefore we propose the first automated solution called flakiness and time balancer fatb to reduce the frequency of flaky test failures caused by asynchronous calls.
our evaluation of five such flaky tests shows that fatb can reduce the running times of these tests by up to without empirically affecting the frequency of their flaky test failures.
lastly our study finds several cases where developers claim they fixed a flaky test but our empirical experiments show that their changes do not fix or reduce these tests frequency of flaky test failures.
future studies should be more cautious when basing their results on changes that developers claim to be fixes .
ccs concepts software and its engineering software testing and debugging.
keywords flaky test empirical study lifecycle permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
copyrights for components of this work owned by others than the author s must be honored.
abstracting with credit is permitted.
to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee.
request permissions from permissions acm.org.
icse may seoul republic of korea copyright held by the owner author s .
publication rights licensed to acm.
acm isbn .
.
.
.
reference format wing lam k van mu lu hitesh sajnani and suresh thummalapenta.
.
a study on the lifecycle of flaky tests.
in 42nd international conference on software engineering icse may seoul republic of korea.
acm new york ny usa pages.
introduction developers typically rely on regression testing to ensure that their recent changes do not introduce faults.
ideally test failures during regression testing would reliably signal issues with the developers recent changes.
unfortunately tests that pass and fail nondeterministically on the same code prohibit this ideal.
these tests are commonly called flaky tests and they negatively impact developers by providing misleading signals about the developers recent changes.
flaky tests are a major problem in both industry and research.
in recent years several companies have reported the impact flaky tests have on them.
micco reported that .
of all test runs at google are flaky and that almost of their .2million individual tests fail independently of changes in code or tests.
similarly our previous work reported that about .
of the tests in five microsoft projects are flaky.
at facebook harman and o hearn even proposed to adopt the position that alltests are flaky ataf and that researchers should rethink testing techniques knowing that they will be used in an ataf world.
there are a few studies of flaky tests that focus on common categories of flaky test fixes approaches for detecting flaky tests and automatic approaches for fixing flaky tests .
although this existing work helped substantially advance the topic of flaky tests none of them study the reoccurrence execution time or runtime for short and time before fix of flaky tests which can be relevant to the impact and possible solutions of flaky tests and flaky tests in depth on proprietary projects.
studying flaky tests in depth specifically on the categorization of them on proprietary projects can be important to understand whether new or existing techniques for dealing with flaky tests would work well for both proprietary and open source projects.
to fill this knowledge gap we study the lifecycle of flaky tests on six large scale diverse proprietary projects at microsoft.
specifically we study the overall lifecycle prevalence reproducibility characteristics e.g.
reoccurrence runtimes categories and resolution e.g.
time before fix of flaky tests.
our study of prevalence and reproducibility reveals the substantial negative impact that flaky tests have on developers at microsoft while our study on the ieee acm 42nd international conference on software engineering icse icse may seoul republic of korea wing lam k van mu lu hitesh sajnani and suresh thummalapenta characteristics categories and resolution of flaky tests confirms that some of the findings from studies on open source projects also hold for proprietary projects.
for example similar to two prior studies on flaky tests in open source projects we also find that the most common category of flaky tests in proprietary projects is the async wait category.
tests in this category are flaky because they make asynchronous calls without properly waiting for the call to return.
realizing the substantial impact that flaky tests have on developers at microsoft and how common async wait flaky tests or async wait tests for short are we propose an automated solution to alleviate the negative impact of these tests.
our automated solution called the flakiness and time balancer fatb alleviates the negative impact of async wait tests.
specifically fatb identifies the method calls in the test code that are related to timeouts or thread waits and then calculates the frequency that the flaky test will fail or flaky test failure rate for short .
based on the current flaky test failure rate fatb will then try various time values and outputs the minimum time values that developers should use depending on their tolerance for flaky test failures.
our evaluation of fatb on five versions each of five flaky tests shows that the tests can run up to faster and still achieve the same flaky test failure rate as before.
more importantly we also find that for some tests the developers thought that they fixed the flaky tests by increasing some time values in the tests but our empirical experiments show that these time values actually have no effect on the tests flaky test failure rates.
our finding suggests that what developers claim as fixes for flaky tests in bug reports commit messages etc.
can be unreliable.
although some existing flaky test studies relied on these claims from developers future work should be more cautious when basing their results on changes that developers claim to be fixes .
in summary we present the following main contributions.
we confirm whether the categorization findings on flaky tests of prior studies conducted on open source projects are true for the proprietary projects at microsoft.
we study the lifecycle of flaky tests which helps us understand them and demonstrates the need for our automated solution.
as part of our study we are the first to investigate the reoccurrence runtimes and time before fix of flaky tests.
the data we use for our study is available online .
we propose an automated solution called fatb to balance test flakiness and runtime.
our empirical experiments show that fatb can help tests run up to faster and the tests will still have the same flaky test failure rates as before.
we show that categorization based on changes developers claim as fixes can be unreliable.
our finding invalidates a main assumption of prior studies and reveals the need for future work to more cautiously use such fixes and to confirm flaky test fixes.
background microsoft uses a modern build and test service framework on the cloud called cloudbuild .
cloudbuild is an incremental and distributed system for building code and executing tests similar to other engineering systems such as bazel and buck .
when cloudbuild receives a build request with a change it identifies allmodules that are impacted by the change.
cloudbuild executes the tests only in those impacted modules and skips the remaining modules tests since none of their dependencies changed.
note that within a module cloudbuild always executes all tests in the same order sorted alphabetically .
to address the misleading signals of flaky tests cloudbuild offers a comprehensive flaky test management system called flakes that includes four major features detection reporting suppression and resolution.
the detection feature aims at inferring flaky tests among all tests executed by cloudbuild.
more specifically whenever there is a test failure cloudbuild automatically retries the test once by default and if the retry passes then the test is considered flaky and the build continues.
once a test is considered flaky flakes proceeds to reporting where it reports the flaky test to developers by automatically creating a bug report.
these bug reports help notify developers of the flaky tests and encourages the developers to fix the flaky tests.
note that flakes will link multiple flaky tests to the same bug report by looking for similarities in the flaky tests error messages.
so prevents tests with the same root cause from creating many different bug reports.
forsuppression flakes updates a suppression file that maintains all known flaky tests within the project.
specifically flakes adds information e.g.
error message bug report url code version fullyqualified test name about a test to the suppression file when a test is found to be flaky.
this suppression file is primarily used to suppress future failures of flaky tests since they are known to be flaky already.
flakes can suppress these failures to help reduce developers effort in diagnosing test failures due to flaky tests.
however to discourage developers from fully relying on suppressions to deal with flaky tests flakes simply suppresses the failures for days by default.
finally for resolution when developers close a bug report related to a flaky test flakes automatically removes the test from the suppression file.
if the test is found to be flaky later flakes will reopen a new bug report and repeat all of the steps above.
today flakes is used by projects in total at microsoft.
of these projects flakes has already found at least one flaky test in six of the projects.
across all of these projects flakes has created over bug reports and between may to august flakes suppressed over flaky test failures for these six projects.
study setup this section describes the projects and datasets of flaky tests we use in our study the research questions of our study and how we use the datasets for each question.
.
evaluation projects table provides some statistics collected during july over a day period for six projects that use flakes.
each of these projects has at least one flaky test in it currently or had one sometime in its past.
due to company confidentiality reasons the names of the projects are anonymized.
none of the authors has worked on any of the projects that uses flakes.
in the table column shows the number of distinct tests in each project.
column shows the number of failed builds for each project.
column shows the number of tests executed in all builds.
note that cloudbuild does not execute all tests in each build rather it executes only those tests that are 1472a study on the lifecycle of flaky tests icse may seoul republic of korea table statistics of the projects with flaky tests using flakes during july over a day period .
failed test median build flaky test builds with project tests builds executions time min failures flaky test failures project purpose proja .
ads projb .
cloud computing projc .
engr.
infrastructure projd .
database proje .
engr.
monitoring projf .
search table flaky test statistics of the projects in our study.
projb s pull requests prs are inaccessible for our study.
flaky fixed flaky test project tests flaky tests w prs proja projb projc projd proje projf total within the modules impacted by the change.
column presents the median time of each build in minutes and column shows the number of total flaky test failures suppressed by flakes.
column shows the number and percentage of failed builds that contained at least one flaky test failure suppressed by flakes.
note that each of these builds can have more than one flaky test failure.
finally column shows the purpose of the project.
as this table shows the projects that use flakes and have at least one flaky test are quite diverse.
specifically the median build times for these projects vary from to minutes and they all have distinct purposes.
.
datasets we conduct our study of flaky tests at microsoft using three datasets.
figure shows an overview for how we obtain these three datasets from the six projects that use flakes.
as figure shows we obtain the datasets using three main steps with each subsequent step using some or all of the data in the previous step.
to obtain our datasets we start with all versions of the suppresion files that flakes maintains for each of the six projects.
these suppresion files are version controlled and flakes uses them to keep track of known flaky tests.
having previous versions of these suppresion files consequently allows us to find flaky tests that flakes found in the past regardless of whether these tests are fixed or not.
in total flakes identified flaky tests from the entire history of the six projects shown in table .
we use the suppresion files maintained by flakes for each project to create three datasets labeled as all fixed pull requests and categorized.
dataset all fixed includes all flaky tests that flakes has observed to be fixed and contains flaky tests.
dataset pull requests includes all flaky tests that are fixed and the bug report associated with the flaky test includes a pull request that the developer manually linked to the bug report.
this dataset contains flaky tests.
lastly dataset categorized includes all flaky teststhat have pull requests and upon our manual investigation of the pull requests bug reports and source and test code we categorize these flaky tests with the categories defined in prior studies .
this dataset also contains flaky tests.
to obtain the all fixed dataset the result of step we parse the suppresion files of flakes into a sql like database known as azure data explorer .
we parse the suppresion files from the oldest to the newest version and when we see a flaky test get added to the file we consider that test to be flaky.
on the other hand when we see a flaky test get removed from the file we consider that test to be fixed.1the number of times a flaky test is detected and fixed depends on the number of times the test is added and removed respectively from the suppresion file.
to obtain the pull requests dataset the result of step we join the all fixed dataset with an existing azure data explorer table that keeps track of which pull requests if any are linked to a bug report.
not all closed bug reports are linked to a pull request because developers have to manually link the pull requests themselves.
we join the all fixed dataset with an existing azure data explorer table because flakes keeps track only of the bug report it creates for a particular flaky test and would not otherwise know if a particular bug report has pull request s linked to it.
note that by design projb s bug reports are not accessible through azure data explorer.
therefore all bug report and pull request information for the flaky tests of projb is omitted from our study.
to obtain the categorized dataset the result of step we study the pull request source code and test code of each flaky test in the pull requests dataset.
each flaky test that we categorize is verified by two or more of the authors independently.
our categorization considers four kinds of locations and root cause categories that we obtain from two prior studies on flaky tests.
table summarizes for each project the flaky tests we find in it.
overall flakes identified flaky tests from six projects.
on average flakes has been tracking flaky test information in these six projects for days.
of the flaky tests tests are fixed.
of the flaky tests that are fixed we find that the developers attached a pull request to the bug report for of the tests.
.
research questions to better understand the lifecycle of flaky tests at microsoft we study the prevalence reproducibility characteristics categories and resolution of flaky tests.
more specifically we address the following research questions 1a flaky test can also be removed from the suppresion files because the test is removed from the project.
our all fixed dataset does include such tests.
1473icse may seoul republic of korea wing lam k van mu lu hitesh sajnani and suresh thummalapenta figure overview of how we obtain the datasets used in our study.
rq1 how prevalent are flaky tests and to what extent do they impact developers workflow?
rq2 how many runs are needed to reproduce flaky test failures?
rq3 does test flakiness reoccur after fixes?
if so what are the reasons for it to reoccur?
rq4 how does the runtime of a flaky test differ between passing and failing runs?
rq5 what are the categories e.g.
root cause location of the flaky test fixes?
rq6 how much time do developers take to fix flaky tests?
rq7 how effective are developers at identifying and fixing the timing related async wait issues in flaky tests?
we first address rq1 to understand how problematic flaky tests are at microsoft.
we then address rq2 to understand the difficulty developers may have in debugging and fixing flaky tests.
knowing the difficulty of reproducing flaky test failures we then address rq3 and rq4 to understand the characteristics of these flaky tests.
we then address rq5 to extend our characteristics study by categorizing the location and root cause of the flaky test fixes in our dataset.
lastly we address rq6 and rq7 to understand how effective developers are at fixing flaky tests.
.
methodology all of our research questions use either the all fixed pull requests or categorized datasets of flaky tests described in section .
.
.
.
rq1 prevalence and impact of flaky tests.
for rq1 we use the all fixed dataset our entire dataset of fixed flaky tests.
for this rq we rely on the information collected by microsoft s flakes during july .
specifically we look at the number of failed builds that would have occurred due to flaky test failures if flakes did not suppress such failures from these builds.
.
.
rq2 and rq4 reproducibility and runtime of flaky tests.
for rq2 and rq4 we again start by using the all fixed dataset.
specifically for each flaky test we run the test times in the actual build and testing environment.
we use only three projects projc projd proje because we perform these experiments on real proprietary projects and we cannot interrupt or slow the actual testing environments of the other projects.
.
.
rq3 reoccurrence of flaky tests.
for rq3 we use the allfixed dataset but we filter for tests that have been fixed more than once.
we identify the flaky tests that are fixed more than once by looking for tests that are removed from a project s suppresion file more than once.
for the tests that are fixed more than once we look at the commits bug reports and source and test code to understand why they reoccur.
we use the commits instead of pull requests because not all tests in the all fixed dataset have pull requests linked to the tests bug reports.
indeed we find that for the flaky tests that we study for this rq all of their bug reports do not have pull requests.
we obtain the commits for these tests by using the dates of their bug reports and the version control history of the test code to find the likely commits for these tests.
we then confirm these commits with the developers of the flaky tests.
.
.
rq5 categories of flaky test fixes.
for rq5 we use the pullrequests dataset which contains flaky tests that are fixed and have a pull request associated with the test.
specifically we study where are the changes located in i.e.
source or test code and what root causes of flaky test fixes do these pull requests belong to?
prior studies on flaky tests have found four kinds of locations in which fixes were located in and also identified root causes of flaky test fixes.
for our study we manually label each pull request along with its corresponding bug report and source test code with the same four kinds of locations and root causes as the prior studies.
we decide to use pull requests which consists of one or more commits because pull requests represent a more complete set of changes.
these changes from pull requests generally build without errors and have been tested on the developers machines to ensure that they do not fail any tests.
.
.
rq6 time before fix of flaky tests.
for rq6 we use the allfixed dataset.
specifically for each fixed test we study the bug report linked to the test.
recall that flakes reporting feature as described in section will automatically create a bug report for each test it finds to be flaky.
to obtain the time before fix of flaky tests we study the time the bug reports of these tests took from being created to them being closed.
.
.
rq7 developers effectiveness on identifying and fixing async wait tests.
for rq7 we use the categorized dataset which contains flaky tests that are fixed have a pull request associated with the test and its pull request bug report and code is categorized.
1474a study on the lifecycle of flaky tests icse may seoul republic of korea since in rq5 we find that async wait is the most common category of flaky tests we focus specifically on this category for rq7.
to understand how effective developers are at identifying and fixing async wait tests we sample five async wait tests whose fix by developers is to increase the wait timeout.
we then calculate the flaky test failure rate with the developer suggested fix and measure how the rate changes when the time value increases or decreases.
analysis of the results this section presents the results of our study for the research questions in section .
using the methodology we describe in section .
.
the data we use for our study is available online .
.
rq1 prevalence and impact of flaky tests we begin our study by first investigating how prevalent flaky tests are at microsoft.
from tables and we see that for all projects except for projd the number of flaky tests ever found is only a small fraction of the total number of tests these projects have during the month of july .
however just because a project contains many or few flaky tests it does not necessarily mean that the developers workflow are often or rarely impacted by these tests.
to understand whether these flaky tests do impact developers workflow or not we also show in table the percentage of developers builds in which the build would have failed due to flaky test failures if flakes did not suppress such failures.
one interesting thing to note here is that even though some projects have lots of flaky tests these projects chance for builds to fail due to flaky test failures are not particularly high.
for example projd has flaky tests with over half still not fixed but flaky test failures only affects .
of its builds over a day period while proje has only flaky tests but its builds are affected by flaky test failures .
of the time during the same period.
our results demonstrate that although flaky tests may not always be very prevalent the percentage of builds that are impacted by flaky test failures can still be quite substantial.
.
rq2 reproducibility of flaky test failures one of the biggest challenges developers have when debugging or fixing flaky tests is to reproduce the flaky test failure.
to understand how much of an imposition the reproducibility of flaky test failures may have on developers we study the number of flaky tests in which we can reproduce the flaky test failures and for the tests where we can reproduce the flaky test failure we also study these tests flaky test failure rates.
for each of the flaky tests that we use for this rq we run the test times using the same configuration of the machines and the version of code that flakes detected the test to be flaky on.
table summarizes our results.
we use only a subset of our dataset for this rq because these experiments are performed on real proprietary projects and we could not slow down the actual testing environment of the other projects.
furthermore not all flaky tests can be run again due to problems compiling the version of code that the test was found to be flaky on.
the actual number of flaky tests that we are able to run times for is shown in column .
column shows the number of tests that pass and fail at least once and columns and show the average and mediantable statistics on reproducibility of flaky test failures.
flaky flaky tests average median proj.
tests pass fail fail fail projc .
.
projd .
.
proje .
.
figure percentage rate of flaky test failures.
respectively percentage rate of flaky test failures for the flaky tests that pass and fail at least once.
column of the table shows that flaky test failures are reproducible between to depending on the project.
this finding suggests that there are many flaky tests up to depending on the project where even with runs we cannot reproduce the flaky test failures of these tests.
we also see from column that the median percentage rate of flaky test failures can be quite low particularly for projc and proje.
this finding suggests that even when flaky test failures can be reproduced in runs only a small number of failures are reproduced.
figure shows a box plot for the percentage rate of flaky test failures for the flaky tests that pass and fail at least once in each project.
we can see in this figure that the averages of both projd and proje column in table is quite high due to the percentage rates of outlier tests.
to understand these outliers better we analyze them and find that 9of these tests are likely async wait their names contain async .
of these tests we have the pull request bug report and code for of the tests and while studying the categorization of flaky tests in rq5 section .
we do indeed categorize these tests as async wait tests.
.
rq3 reoccurrence of test flakiness as section .
demonstrates flaky test failures can be quite difficult for developers to reproduce.
this difficulty makes it so that when developers are fixing flaky tests they may just assume that their changes fixed the flaky test failure and they may not actually confirm their assumption.
for our study on the reoccurrence of flaky tests we use the all fixed dataset which contains fixed flaky tests.
of these flaky tests we find that four flaky tests are fixed more than once.
1475icse may seoul republic of korea wing lam k van mu lu hitesh sajnani and suresh thummalapenta if a flaky test is found to be flaky more than once then it is either because the developers initial fix for the flakiness was inadequate or the cause of flakiness was reintroduced.
in either case sufficient time must be given for the developers to notice the reoccurrence and for the test to run many times for it to fail builds again.
on average the fixed flaky tests in our study have been fixed for more than days.
to understand why four flaky tests had to be fixed more than once we manually investigate the the commits bug reports and the source and test code for each flaky test.
we use commits instead of pull requests because not all tests in the all fixed dataset have pull requests and all four tests that are fixed more than once do not have pull requests.
we obtain the commits for these tests by using the time their bug reports closed and the version control history of the test code to find the likely commits for these tests.
we then confirm the likely commits with the developers of the four tests.
our investigation into the four flaky tests that reoccurs more than once reveals that all four reoccurrences are due to case when the developers initial fix was inadequate.
for these flaky tests we can clearly see this being the case since the developers described their initial fix as being inadequate in their latest fix.
for example one developer described his latest fix as increase the wait time for idle timeout test case to ensure that the receive loop exits first.
previously the wait time was second more than the idle timeout which was cutting it too fine .
overall our investigation into the reoccurrence of these four tests finds that developers have the following important sentiments about fixing flaky tests.
developers are rarely able to reproduce the flaky test failures locally on their own machines or on servers.
consequent of developers often resort to making multiple changes to fix test flakiness.
these changes are often either a made by trial and error guessing and the developers rely on the frequent runs of the test on the servers to determine whether the fix was adequate or b made simply to log additional information so that the developers can know more about the flaky test failure before attempting a real fix.
.
rq4 runtime of flaky tests to begin understanding why a test may be flaky we study the runtime of flaky tests.
we study the runtime of flaky tests because one may think that when flaky tests fail they would run faster than their passing runs since the test may have encountered a fault and stopped early.
however flaky tests may also take longer in their failing runs if they are flaky because they time out.
in such cases the flaky test may wait for a callback that simply never happens indicating that these tests likely make asynchronous calls.
similar to rq2 section .
we can use only a subset of our dataset for this rq since we could not slowdown the testing environment of the other projects and the version of code in which some flaky tests were detected on no longer compiles.
table and figure shows the runtime in seconds of the flaky tests that pass and fail at least once in runs.
overall we see that for proje the average and median runtime of passing runs is more than failing runs.
as for projc we see that the average and median runtime of passing runs is about the same as the failing runs.
thistable statistics on runtime in seconds of flaky tests.
test average median proj.
result runs runtime runtime projc pass .
.
projc fail .
.
projd pass .
.
projd fail .
.
proje pass .
.
proje fail .
.
figure runtime in seconds of flaky tests.
result suggests that for these two projects their flaky tests are likely unrelated to asynchronous method calls.
on the other hand we can see that for projd the average and median runtime of failing runs is substantially more than the runtime of passing runs.
this result suggests that projd s flaky tests are likely related to asynchronous method calls.
when we categorize flaky tests in rq5 we do indeed find that the majority of projd s flaky tests are async wait tests.
.
rq5 categories of flaky test fixes to understand the categories of flaky test fixes we use the pullrequests dataset which contains fixed flaky tests that all have an associated pull request.
we categorize these tests by studying their pull requests bug reports and source and test code.
our study focuses on two main questions where were the majority of the changes located in i.e.
source or test code and what root causes of flaky test fixes do these pull requests belong to?
prior studies on flaky tests found four kinds of location in which fixes were located and also identified root causes of flaky test fixes.
table summarizes our findings and those of prior studies.
.
.
location of flaky test fixes.
a prior study on flaky tests identified four kinds of locations for flaky test fixes source code only test code only source and test code and configuration.
we adopt similar kinds of locations for our study.
the only change we make is that configuration is changed to other instead and a fix is considered to be other if it includes changing anything besides source or test code e.g.
test input data configuration .
1476a study on the lifecycle of flaky tests icse may seoul republic of korea table comparison of flaky test fix categories with previous studies .
denotes that such data was not made available in their paper or otherwise.
categories our study location of fixes source only test only test and source other root cause of fixes async wait network concurrency resource leak randomness io time floating point operations test order dependency unordered collections difficult to categorize another flaky test study investigated the categories of flaky tests and how it relates to code smells.
they did not present their findings on how many fixes for flaky tests are in source code or other files.
nevertheless they found that at least of flaky tests could be fixed by manually fixing three code smells in the tests.
overall our findings confirm what prior studies found the majority of fixes for flaky tests are in the test code.
interestingly we find that in of the fixes developers simply removed the test.
our investigation shows that developers sometimes temporarily removed the failing test or claimed that the test is for functionality that is no longer supported.
we also find that about of fixes involve changes to source code.
overall our results show that ignoring flaky tests can be dangerous since they do indicate faults in both source and test code.
.
.
root causes of flaky test fixes.
when performing our study on the categories of flaky test fixes we use the same categories as the prior studies.
we find that the most common category of fixes is async wait with of the fixes belonging to that category.
async wait flaky tests make an asynchronous call and they do not properly wait for the call to return.
the second most common category is network with of the fixes.
note that unlike the prior studies one fix of ours may belong to multiple categories.
also similar to a prior study we find a number of fixes that we could not categorize due to the large number of changes.
specifically in our study these fixes modify an average of 785files.
when we examine such fixes in detail we see that they were a part of a version upgrade or major refactoring.
overall our study differs from prior studies in two main ways.
both prior studies used flaky tests from open source projects while we used flaky tests from proprietary projects at microsoft and we study the pull requests bug reports andtable time given in days for developers to close flaky test or non flaky test related bug reports brs .
projb is omitted because its brs are inaccessible for our study.
flaky test non flaky test proj.
brs median avg.
brs median avg.
proja projc projd proje projf overall source and test code of flaky tests while one prior study studied only commits and another prior study studied only the test code in one version of many projects.
we believe these differences between our studies is responsible for the minor differences in our findings.
one example of how our results differ from prior studies is the percentage of fixes that are categorized as test order dependency.
this difference is likely because the way we run tests at microsoft heavily reduces the chance of test order dependency causing test flakiness.
as explained in section cloudbuild always runs tests in the same order but this requirement is not true for the open source projects in prior studies2.
indeed as table shows none of the flaky tests within the six projects we study are flaky due to test order dependency even though this category is the third most common category of flaky tests in open source projects.
even though the composition of our study differs from prior studies our findings on the location and common root causes of fixes remain largely the same.
specifically we all find that the majority of flaky test fixes are located in test code but a nontrivial amount of them do also involve source code .
also the most common category of flaky test fixes is async wait.
our findings here suggest that solutions like the one we propose in section .
that can help reduce flaky test failures of async wait tests would highly help alleviate the negative impact of flaky tests.
.
rq6 time before fix of flaky tests prior work has highlighted how flaky tests negatively impact developers software development process and how important it is for developers to fix these flaky tests.
to understand whether developers at microsoft understand the importance of fixing flaky tests we study how long developers take on average to fix flaky tests.
more specifically we study the time developers take on average to close the bug report linked to a flaky test.
at microsoft closing a bug report typically means that the bug has been fixed.
for our study we start with the flaky tests in our all fixed dataset which consists of fixed flaky tests.
for the flaky tests in our dataset we find that these flaky tests are linked to bug reports.
as we explain in section multiple flaky tests may be linked to the same bug report if these flaky tests share similar error messages e.g.
two tests are flaky due to the same setup method .
2note that it is still possible for a flaky test to fail due to test order dependency at microsoft because the flaky test could fail when a new test is added and the new test runs before the flaky test.
dually a flaky test could start failing as well when a test that was needed to run before the flaky test is removed from the test suite.
1477icse may seoul republic of korea wing lam k van mu lu hitesh sajnani and suresh thummalapenta table categorization of async wait flaky test fixes.
categories tests timing related fix increase wait timeout add improve callback add improve polling timing unrelated fix removing code mocking async calls difficult to categorize table shows the average and median number of days each project takes to close flaky test and non flaky test related bug reports.
note that projb s results are omitted from table because as we explain in section .
all bug report and pull request information for the flaky tests of projb are inaccessible for our study.
as we show in table developers take on average days with a median of days to close flaky test related bug reports while they take on average days with a median of days to close all non flaky test related bug reports.
the median time developers take to close flaky test and non flaky test related bug reports are the same suggesting that developers consider these bug reports to be of equal importance.
however when we compare the average time developers take to close flaky test related bug reports to the non flaky test ones we see that flaky test related ones take substantially longer than non flaky test ones days for flaky test related ones compared to days for non flaky test related ones .
as table shows part of the reason why the average for flaky test related ones are higher is because the flaky test related bug reports in projc take much longer to close than the non flaky test ones.
when we compare the flaky test and non flaky test related bug reports per project we see that proja s and projf s average time to close non flaky test related bug reports are actually more than the time to close flaky test related ones.
on the contrary we also see that projc s and proje s average time to close flaky test related bug reports are substantially more than the time to close non flaky test related ones.
our findings suggest that although there has been a substantial amount of work from both industry and academia on flaky tests it can still be important to communicate to some developers the importance of fixing flaky tests.
following our study we personally approached a number of teams i.e.
those from projc and proje to better communicate to them this importance.
.
rq7 developers effectiveness on identifying and fixing async wait tests based on the prevalence of async wait tests as described in rq5 section .
and in prior studies on flaky tests we proceed to study developers effectiveness in identifying and fixing async wait tests at microsoft.
to study this rq we first categorize the async wait related flaky test fixes in our dataset.
in total from our work in section .
we find that there are flaky tests that have fixes related to asynchronous method calls.
figure overview of how the flakiness and time balancer fatb works.
prior work on asynchronous tests proposed three main ways one should test asynchronous code.
create a synchronous interface between tests and asynchronous code implement callbacks on all asynchronous code and check or poll frequently on whether an asynchronous service is complete.
when we study the async wait flaky test fixes or async wait fixes for short in our dataset we find that there are no cases in which was done by developers.
our results are likely because requires substantial effort from developers to create and maintain such interfaces.
on the other hand we do see developers using both callbacks of async wait fixes and polling of async wait fixes to fix their async wait tests.
beyond the three categories laid out in this prior study we also find three different categories for these async wait fixes.
specifically we find that the most common category of fix of async wait fixes involves simply increasing the wait time or timeout of asynchronous method calls.
the other two categories are to simply remove code related to the flaky test failure of async wait fixes or to mock the asynchronous calls of async wait fixes .
lastly we find that of the async wait fixes are difficult to categorize since they involve changes to asynchronous method calls but we cannot identify any particular categories for these fixes.
table summarizes the findings from our categorization.
note that the fix for each test may be categorized into one or more categories.
.
.
evaluating and improving developers async wait fixes.
with the majority of the async wait fixes involve simply increasing the wait time or timeout time value for short of an asynchronous call we proceed to study how well these time values set by developers are at reducing flaky test failures and how these time values affect the runtime of these flaky tests.
to evaluate and improve developers async wait fixes we propose the flakiness and time balancer fatb .
fatb first finds the flaky test failure rate of the test using the developers fix.
once fatb obtains the rate associated with the developers fix it then increases or decreases the time value and again measures the flaky test failure rate associated with the new time value.
figure shows an overview for what fatb does.
depending on whether the test is still flaky with the new time value fatb will use that information to either increase or decrease the next time value to try.
on a high level to lower the time value fatb will first use the time value between the developer s fix time and the time set before the fix.
for an example if the developer s 1478a study on the lifecycle of flaky tests icse may seoul republic of korea figure how fatb chooses the next time value an async wait test should try.
fix time was milliseconds ms and the time value before their fix was ms then fatb would first set the time value to be ms. if lowering the time value does not cause flaky test failures in some number of runs by default and for our experiments then fatb will take the time between our current time value and zero e.g.
ms .
if lowering the time value does cause flaky test failures then the next time value will be between the current value and the developer s fix time e.g.
ms .
fatb outputs the observed flaky test failure rate and average test runtime for each time value e.g.
time value 375ms fails runtime 875ms .
as a post processing step fatb will also remove time values that have the same flaky test failure rate choosing to output only the minimum time value and runtime for all observed flaky test failure rates.
this output enables developers to finely balance the trade off of their tests runtime and flaky test failure rate.
the logic fatb uses to generate different time values is shown in figure .
fatb generates time values specific to the machine on which fatb is run on.
to ensure that these generated time values perform well on different machines developers can run small benchmarks on these different machines e.g.
their own development machine and the machines on which they run fatb e.g.
a development server .
the difference in the machines performance on the small benchmarks can then be used to scale the generated time values as needed.
.
.
results.
to evaluate fatb we randomly sample five tests from the 21tests whose fix was to increase the wait timeout.
the results from us applying fatb on these five flaky tests are shown in table .
specifically we use fatb to generate four time values for each test and version represents the value the developers proposed to fix the flaky tests with.
we run the test times for each version to measure that version s flaky test failure rate.
due to confidentiality reasons the names of the tests are anonymized.
we apply step of fatb four times on five flaky tests and find that for four of the flaky tests even when the time value is set to be substantially lower than the value set by the developers to fix the test the tests flaky test failure rates appear to be unaffected.
more specifically for test2 test3 test4 and test5 we see flakytest failure rates even when we substantially decrease the timetable statistics on the results produced by fatb when we apply it to five versions of five async wait tests.
units for time value depend on the test.
runtime is in seconds.
prefix value is the value set before the developer s fix.
version s time value is the value set after the developer s fix.
flaky time average median test version value fails runtime runtime test1 .
.
pre .
.
fix .
.
value .
.
.
.
test2 .
.
pre .
.
fix .
.
value .
.
.
.
test3 .
.
pre .
.
fix .
.
value .
.
.
.
test4 .
.
pre .
.
fix .
.
value .
.
.
.
test5 .
.
pre .
.
fix .
.
value .
.
.
.
value set by the developers.
our finding here further echos the sentiments that we find about fixing flaky tests in section .
.
more specifically we see that the fix employed by the developers for these flaky tests were likely educated guesses that turn out to be unrelated to the flaky test failures of the test.
this finding here is largely related to how truly understanding the root cause of a flaky test is often very complicated.
for these four flaky tests neither we nor the developers were able to actually determine the root cause for the flaky test and consequently neither we nor they are able to fix the flaky test.
future studies on the root causes of flaky tests should be more cautious when basing their results on the changes by developers.
besides the four flaky tests that do not encounter any flaky test failures we see one example test1 that does exhibit flaky test failures once we lower the wait timeout value of the test.
for test1 we can see that once the value goes lower than the value before the developer fixed the flaky test we start observing a high flakytest failure rate e.g.
when set to we see a flaky test failure rate .
however once fatb sets the value to be we see a flaky test failure rate.
when we compare the flaky test failure rate with the developer s fix time value for version to the flaky testfailure rate with the time value for version we see that this flaky test can obtain the same flaky test failure rate when the time value 1479icse may seoul republic of korea wing lam k van mu lu hitesh sajnani and suresh thummalapenta is set to or .
however the lower time value enables this test s average runtime to be about faster than the higher value .
similarly test2 test3 test4 and test5 do not encounter any flaky test failures on all versions and their average runtime can also be faster by about and respectively.
it is surprising that the developers of test2 test3 test4 and test5 would increase the time values of these tests when the values do not appear to empirically affect the tests flaky test failure rate.
our results suggest that there are perhaps some other changes in these pull requests that actually fixes the flaky tests and that the changes in time values were not intended as the fix.
to understand why the developers may have made these time value changes we study the pull request messages of the fixes.
we find that for all four of the flaky tests besides test4 the messages all say that they are increasing the time values as a fix to the flakiness.
for example test3 s message says fix is to wait x time .
on the other hand test4 s message says fix flaky test and then explains how some refactoring was done to some asynchronous code.
from these pull request messages we see that at least for test2 test3 and test5 these developers are purposefully trying to fix their async wait tests by increasing time values.
note that microsoft does not encourage developers to fix their async wait tests by increasing the time value.
however since regulating how developers fix their code would be very costly to do we do plan to use fatb to help developers at microsoft.
with the prevalence of async wait tests and how developers prefer to fix these tests by increasing the time value we suspect that there are many other tests whose collective reduction in test runtime can substantially lessen the time developers spend waiting for test results machine resources needed to run these tests and amount of flaky test failures developers debug.
threats to validity our work contains many of the common threats typically found in empirical studies.
in this section we focus on the issues that are more specific to our study.
subjects of our study.
our study consists of just six projects at microsoft and our findings from studying these six projects may not generalize to other projects or companies.
to avoid any bias in the selection of our projects we include all projects using flakes in our study.
as we describe in section there are a total of projects using flakes and of these projects the six projects we study are the ones where flakes found at least one flaky test.
the projects we study also greatly vary in activity e.g.
number of builds per month and in purpose e.g.
database search .
aside from the projects used in our study our decision to study pull requests bug reports and source and test code to understand the fixes of flaky tests in section .
may also be a threat.
prior studies on flaky tests have used either the commits or just the test code to understand the characteristics of flaky tests.
for our study we use pull requests which consists of one or more commits because we believe that pull requests represent a more complete set of changes made by the developers.
unlike commits changes from pull requests generally build without errors and have been tested locally to ensure they do not fail any tests.
compared against prior work one prior study did not run the tests and observed them to be flaky like we do.
another prior study did run the tests but they run the tests only runs instead of runs like we do and they did not study changes outside of the test code i.e.
bug reports or source code .
metrics used in our study.
the metrics we use in our study poses a potential threat to our findings and results.
for example we use the time a bug report is opened till when it is closed to understand developers sense of urgency in fixing flaky tests.
in reality a developer taking a short or long amount of time to fix flaky tests could be an indicator of how easy or difficult the fix was and would be irrelevant to the developer s sense of urgency.
the timing we report may also be inaccurate since different teams work differently and some teams may close bugs as soon as they are fixed while other teams may only close them at their next team meeting.
flaky tests used in our study.
flaky tests by definition may pass or fail on the same code.
to identify the flaky tests used in our study we rely on flakes which simply reruns failing tests once to see whether it would pass on the rerun.
since there are no guarantees that the rerun would pass if the test is indeed flaky flakes may potentially contain many false negatives in which a test that is flaky is undetected.
nevertheless flakes contains no false positives meaning that all flaky tests detected by flakes must indeed be flaky.
due to this threat the number of flaky tests we report in our study is simply the minimal number of flaky tests in our projects.
our findings in regards to the runtime and reproducibility of flaky tests identifies that there are some patterns.
however more runs of the flaky tests may change our findings.
to mitigate this threat we choose to run each test a high number of runs specifically runs.
running the test runs to identify flaky test failures is substantially more than a prior study on flaky tests which ran the tests for runs.
findings from manual inspection.
certain research questions in our study requires us to manually inspect the information of flaky tests.
specifically for categorizing flaky tests and categorizing async wait tests we minimize the occurrence of miscategorization by having more than one author inspect every pull request every bug report and all source and test code and we discussed our categorizations until everyone agrees.
related work studying flaky tests.
luo et al.
performed the first extensive study on flaky tests.
they manually investigated 201commits from 51open source projects finding that the primary causes for flakiness are async wait concurrency including atomicity violation data races and deadlocks and test order dependency.
palomba and zaidman conducted a similar study on flaky tests in open source projects.
using code smell detectors they also find that async wait is the most common category of flaky tests and that of flaky test fixes can be attributed to three code smells specific to tests.
in comparison to both of these studies our study is based on flaky tests at microsoft and we study flaky tests using pull requests where each test is confirmed to be flaky by our infrastructure.
although our setup and datasets differ we confirm that all three studies have similar findings in that async wait is the most common category of flaky tests and that most flaky test fixes are in the test code.
zhang et al.
reported that test suites can suffer from test order dependency where test outcomes can be affected 1480a study on the lifecycle of flaky tests icse may seoul republic of korea by the order in which the tests are run.
lam et al.
conducted similar work as them and found flaky tests in projects.
of their dataset .
of the flaky tests are due to test order dependency while the remaining are not test order dependency.
in our case cloudbuild always runs tests of a test suite in the same order therefore we do not find any test order dependency tests.
thorve et al.
found additional root causes for flaky tests when studying flaky test commits in android.
similar to luo et al.
their study used commits as opposed to the pull requests we use in our study.
eck et al.
studied developers perception of flaky tests and had developers categorize the patches that developers claim to have fixed flaky tests.
similarly we conduct a study on the pull requests that developers claim to have fixed flaky tests and find that some of these pull requests do not actually fix flaky tests.
as more studies are conducted on the fixes of flaky tests future work should better explore how changes by developers should be used and how one can confirm whether these changes do indeed fix flaky tests.
reproducibility of flaky tests.
luo et al.
s study was the first to report that reproducing flaky test failures is difficult.
since then there has been numerous reports from others on the likelihood to reproduce flaky test failures.
lam et al.
and labuschagne et al.
found that on average .
and .
respectively of builds would fail because of flaky tests.
gao et al.
also observed that it is difficult to reliably reproduce results from tests where a user interacts with a system.
palomba and zaidman found that of all tests they analyzed were flaky.
compared to our work they found these tests to be flaky using only 10runs while our experiments use 500runs.
our study on the reproducibility of flaky tests finds that the likelihood to reproduce flaky test failures can range between to depending on the project.
although studies have shown that ignoring flaky test failures can lead to more crashes in production code developers do ignore flaky tests.
thorve et al.
examined 77commits pertaining to flaky tests from 29android projects and they found that of the commits simply skipped or removed flaky tests.
when we examine the pull requests in our study we find that developers removed the test to fix of the flaky tests.
reducing and removing flaky test failures.
mu lu et al.
proposed a technique to run tests in separate processes and bell and kaiser proposed a technique to run tests in the same process as two different means to remove flaky test failures from test order dependency flaky tests.
shi et al.
proposed ifixflakies a tool to automatically fix test order dependency flaky tests.
at microsoft we rarely have test order dependency flaky tests instead they are mainly async wait tests.
our proposed solution fatb helps reduce the flaky test failure rate of async wait tests.
bell et al.
proposed deflaker a technique that monitors the code coverage of tests in a previous version of code and uses the coverage information to inform developers whether a test failure is due to recent changes in future versions of code.
unlike deflaker fatb directly helps developers prevent flaky test failures altogether.
fowler proposed three main ways in testing asynchronous code creating a synchronous interface between tests and asynchronous code implementing callbacks on all asynchronous code and checking frequently on whether an asynchronous service is complete.
the implementation of and requires substantial effort from developers to setup and maintain.
the implementation of and both still require the developers to provide some timeout value for the asynchronous call which may never complete.
our proposed solution in rq7 fatb can help the developers systematically derive a timeout value that minimizes the runtime and flaky test failure rate.
jagannath et al.
proposed imunit a new language that allows developers to specify the execution flow of tests that make asynchronous method calls.
similarly elmas et al.
proposed concurrit a scripting language that allows developers to control the scheduling of threads to find or reproduce concurrency bugs.
other work proposed tools that help enforce policies specified by the developers.
these policies dictate the scheduling in which threads run and developers have to manually write these policies.
unlike these prior work fatb does not require the developers to provide additional information e.g.
policies in which threads should execute or write their code differently.
instead fatb assists the developers by systematically deriving the time a test should wait for asynchronous calls.
conclusion flaky tests are a major problem in both industry and research.
although flaky tests are the focus of several existing studies none of them study the reoccurrence runtimes and time before fix of flaky tests and flaky tests in depth on proprietary projects.
to fill this knowledge gap we study the lifecycle of flaky tests on six large scale diverse proprietary projects at microsoft.
our study of prevalence and reproducibility reveals the substantial negative impact that flaky tests have on developers at microsoft while our study on the characteristics categories and resolution of flaky tests confirms that some of the findings from studies on open source projects also hold for proprietary projects.
for example similar to two prior studies on flaky tests in open source projects we also find that the most common category of flaky tests in proprietary projects is the async wait category.
to help alleviate the problem of async wait flaky tests we propose the flakiness and time balancer fatb .
fatb identifies the method calls in the test code that are related to timeouts or thread waits and then it calculates the flaky test failure rate of the flaky test.
based on the current flaky test failure rate fatb then tries various time values and outputs the minimum time values that developers should use depending on their tolerance for flaky test failures.
our evaluation of fatb on five versions each of five flaky tests shows that tests can run up to faster and still achieve the same flaky test failure rate as before.
we also find that the developers thought they fixed the flaky tests by increasing some time values in them but our empirical experiments show that these time values actually have no effect on the flaky test failure rates.
our finding suggests that what developers claim as fixes for flaky tests in bug reports commit messages etc.
can be unreliable and future work should be more cautious when basing their results on changes that developers claim to be fixes .