mastering uncertainty in performance estimations of configurable software systems johannes dorn leipzig university johannes.dorn uni leipzig.desven apel saarland university saarland informatics campus apel cs.uni saarland.denorbert siegmund leipzig university norbert.siegmund uni leipzig.de abstract understanding the influence of configuration options on performance is key for finding optimal system configurations systemunderstanding and performance debugging.
in prior research a number of performance influence modeling approaches have been proposed which model a configuration option s influence and a configuration sperformanceasascalarvalue.however thesepoint estimates falsely imply a certainty regarding an option s influencethat neglects several sources of uncertainty within the assessment process suchas measurementbias modelrepresentationandlearningprocess and incompletedata.thisleadstothesituation thatdifferentapproachesandevendifferentlearningrunsassign different scalar performance values to options and interactions amongthem.thetrueinfluenceisuncertain though.thereisno way to quantify this uncertainty with state of the art performance modeling approaches.we propose a novelapproach p4 basedon probabilisticprogrammingthatexplicitlymodelsuncertaintyfor option influences and consequently provides a confidence interval for each prediction of a configuration s performance alongside a scalar.
this way we can explain for the first time why predictions may cause errors and which option s influences may be unreliable.
an evaluation on real world subject systems shows that p4 s accuracy is in line with the state of the art while providing reliable confidence intervals in addition to scalar predictions.
ccs concepts software and its engineering software performance computing methodologies uncertainty quantification keywords probabilistic programming performance influence modeling configurable software systems p4 acm reference format johannes dorn sven apel and norbert siegmund.
.
mastering uncertainty in performance estimations of configurable software systems.
in 35th ieee acm international conference on automated software engineering ase september21 virtualevent australia.
acm newyork ny usa pages.
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation on the first page.
copyrights for components of this work owned by others than acmmustbehonored.abstractingwithcreditispermitted.tocopyotherwise orrepublish topostonserversortoredistributetolists requirespriorspecificpermissionand ora fee.
request permissions from permissions acm.org.
ase september virtual event australia association for computing machinery.
acm isbn ... .
an exemplary option s performance influence modeled by different scalar regression models bars con trasted by p4 s probability density prediction blue curve .
introduction modern software systems are often configurable.
they offer severalconfigurationoptionsthataffectthesystems functionaland non functionalproperties.energyconsumption responsetime and throughputareexamplesofnon functionalproperties whichare commonlysubsumedbytheterm performance.understandingan option s influence on performance and predicting performance for particularconfigurationsiskeywhenitcomestofindingoptimal systemconfigurations.findinganoptimalconfigurationisanessential task because many systems are shipped with a sub optimal defaultconfiguration manuallyexploringconfigurations does not scale and fine grained tuning can improve performanceuptoseveralordersofmagnitude .todeterminethe influence of individual configuration options and their interactions onperformance anumberofmachinelearningapproacheshave beenproposed relyingonrule baseddecisiontrees symbolic regression and artificial neural networks .
to model the influence of options on performance a set of configurations must be sampled and measured from a system s configuration space.
these data are fed into a learning algorithm to fit a model which then allows users to estimate a scalar performance valueforagiven configuration.however this scalarvaluefalsely impliesacertaintythatneglectsseveralsourcesofuncertaintyin the modeling process measurement bias model represen tation and learning process and incomplete data e.g.
due to sampling bias .
without a proper uncertainty measure application engineers may be led to wrong decisions as there is no available information about how certain a learned influence or estimated performanceis.
let us assume that we have trained a model using one of thepreviously mentioned learning approaches.
in the ideal case we 35th ieee acm international conference on automated software engineering ase obtainaninfluenceperconfigurationoptionandinteractionstating theexpected contributionto theoverallsystem performance when de selectinganoptionorcombinationsofoptions.figure 1illustratesthisinfluenceasascalarnumberusingaverticalbar.
each bar represents a different learning approach to determine the scalar representing the option s influence.
as we can see different learning approaches lead to different scalars and even a single approachcanproducesubstantiallydifferentvaluesarisingfrom differentrunsanddifferenthyper parametersettings.lookingat figure it is unclear which actual effect an option has on the system sperformance andthereisnowaytoquantifythisuncertainty with state of the art performance modeling approaches for configurable systems.
likewise applying an optimization approach toautomaticallyfindanoptimalconfigurationusingonlyscalars will yield a single optimal possibly incorrect configuration while there may be other better configurations that the model misjudged due to unconsidered uncertainties.
our key contribution is the following we account for uncertainty about an option s and interaction s true influence on performance that may arise from measurement bias the learning procedure and incompleteness of data .
by making uncertainty explicitacrossthemodelingprocessusinga bayesian ratherthan a frequentist approach we foster model understanding for performance engineers provide clear expectation boundaries for performanceestimatesofsoftwareconfigurations andofferameansto quantify when and where a learned model is inaccurate.
all these pieces of information are absent in current approaches which can harm trust in the models and transfer into practice.
to illustrate our approach we compare the probability distributiondescribingthepossibleperformancevalueofanoption in blue infigure1withthescalarsproducedbythedifferentlearning approaches.consideringthe distributionasawhole wecanstate howlikelytheinfluenceofanoptionorinteractionfallsintoavalue range.thespreadofthedistributionisanimportantindicatorfor the certainty of estimation and whether additional data for thisoption might be necessary.
it also gives confidence intervals for predictions and performance optimizations.
framing the problem of performance modeling in a bayesian setting can be done via probabilistic programming .
it requires the specification of three key components likelihood prior and observations.the likelihood expressesagenerativemodelofhow theobservations i.e.
measured configurations are distributed.
the priorencodes the belief or expectation about each option s and interaction s influence on performance.
this is usually stated by a distribution for a specific value range e.g.
uniform distribution between and seconds .
specifying this distribution requires domainknowledge whichisnotalwaysavailable.akeyelementofourapproachisan automatedpriorestimationalgorithm whichcan be used to learn accurate bayesian performance influence models without domain knowledge.
insummary weproposeanapproachforperformance influence modeling that incorporates and quantifies the uncertainty of influences of configuration options and interactions on performance.
akeyingredientisanautomaticpriorestimationalgorithmthat takestheburdenofguessingpriorsfromtheuser.weconductan evaluation of the reliability of the uncertainty estimates of inferredmodelsandcomparetheaccuracyofourapproachtoastate of theart point estimate model.
we make the following contributions aprobabilisticmodelingapproachforperformanceinfluence modeling of configurable software systems a data preprocessing pipeline to avoid inference failures and to improve model interpretability the tool called p4 which is an open source implementation of our approach an evaluation of p4 s prediction accuracy and anevaluationofthereliabilityoftheuncertaintymeasures of models inferred with p4.
withourapproach weaddtotheimportanttrendonexplainability andinterpretabilityofmachine learningmodels.webelievethat this is especially important in domainssuch as software engineering in which machine learning models must provide insights and explanations to help improving the field.
modeling uncertainty performance influencemodelingentailsdifferentkindsofuncertainty of which we consider aleatoric and epistemic uncertainty inourwork similarto .aleatoricuncertainty resultsfrom errorsinherenttothemeasurementsofthetrainingset epistemic uncertainty expressesdoubtinthemodel sparameters.bothcan be integrated into a bayesian performance model for which we explain the basics in section .
.
.
aleatoric uncertainty performance influence models describe a system s performance in termsofinfluencesofitsconfigurationoptionsandinteractions .
aconfigurationisasetofassignmentstoallavailableoptionsfroma certaindomain e.g.
binaryornumeric thatis c o1 o2 ... on wherenis the number of options and oiis the value assigned to the i th option.
we measure the performance of a configuration by configuring a software system and executing a workload.
formally we denote a configuration s performance as a function that maps a configurationcfrom the set of valid configurations cto its corresponding scalar performance value c mapsto r. for a dbms we could chooseenergyconsumptionasperformance runabenchmark and query an external power meter to determine the energy needed.
however there are two notable sources of error arising from measurement which introduce uncertainty measurement error and representation error.
.
.
measurement error.
typically themeasurementprocess hasaninherenterror whichistypicallyeitherabsoluteorrelative .
absolute errors absaffect all measurements equally c bracketleftbig c abs c abs bracketrightbig by contrast relative errors relare given in percent and affect higher values more severely c bracketleftbigg c rel c rel bracketrightbigg notethat dependingonthecontext thisinterval called confidenceinterval canbedefinedtospanallpossiblemeasurementsfor 685 or altenatively tocontain onlyinafractionofcases e.g.
.
unfortunately this information is rarely available to the user.
the confidence interval of the measurement error constitutes anuncertaintythatcanbereducedbyaggregatingrepeatedmeasurements but it is fixed at modeling time i.e.
the time when we fit the model .
moreover absolute and relative errors are examples forhomoscedastic andheteroscedastic aleatoric uncertainty respectively.thismeansthat inthecaseofrelativemeasurementerror the variance of uncertainty depends on the individual sample heteroscedastic whereas it is constant for the absolute measurement error homoscedastic .
.
.
representation error.
representationofmeasurementdata requires discretization for storage and processing.
we assume a decimalrepresentationforsimplicity astheprecisionoffloating point representationsismorecomplicated1.discretizationcanhappen onthesensorsidebeforewestorethedata.forexample anenergy meter returning only integer watt hour wh values may cause a representation error of .5wh while storing the execution time of a benchmark in seconds with two decimals may yield a representation error of 5ms.
c bracketleftbig c u c u bracketrightbig that is in the general case the performance value at modeling timeliesaroundthemeasuredperformance c within u the unit length of the discretization.
depending on the use case the representation error can induce substantial uncertainty.
.
epistemic uncertainty models ingeneral andperformance influencemodels inparticular nevermatchrealityperfectly.while inourcase aleatoricuncertainty arises from the training data samples epistemic uncertainty stems from the model chosen and the amount of data provided.
let us assume a linear performance model c for a configurable software system with noptions c 0 1 c o1 n c on here c oi returns the value for the i th option of configuration c these values are multiplied with the model parameters where 0is the base performance of the system.
however we can assign different values to to model as a one point estimate.
atypicalusecaseinpracticearelinearregressionmodels which canbefittedtominimizedifferentobjectivefunctions.lasso andridge regressionarealternativestoordinaryleastsquares regression whichcanbe combinedinto anelastic net .their objectivesdifferintheirwayofcomputingthelearningerror l1 andl2normalization .atuningparameterchangeselasticnet s errorcomputationfunctionsuchthatthereisnosinglerightwaytofit a linear model.
as figure shows we obtain different values for the samecoefficient iwhen applyinglasso ridge andordinary leastsquares.hence thefittedvaluefor iisuncertain astheblue curve in figure illustrates.
another reason why can take different values lies in the training data used.
different samples of configurations sampled accordingtodifferentsamplingstrategies leadtodifferent 1see ieee standard for floating point arithmetic for precisionvalues even with the same error function as the literature on samplingapproacheshasdemonstrated .yet evendifferent hyperparameter settings can result in different coefficients dependingonhowstrongwepenalizethelearningerror.inaddition unlessatrainingsetcontainsallvalidsamples weareuncertainwhether is a good fit since increasing the training set size usually improves thepredictionaccuracyofaregressionmodelbyrefining andalso reducesuncertaintyabout .notethatalthoughaddingsamplesto the training set reduces epistemic uncertainty each sample itself is still subject to aleatoric uncertainty.
insteadofspecifyingthemodel sweightsasareal valuedvector rn we can formally incorporate uncertainty into by changing it to a probability vector .
this way each model weight becomes a probability density function that specifies which values for are more probable than others representing the best fit.
thus for gaussian distributed uncertainty we can specify n as a probability vector with rn.
we do not know though whetheruncertaintyisgaussian distributedforreal valuedconfigurablesystemsandwhatarethesettingsfor .todetermine this distribution we need probabilistic programming.
.
probabilistic programming framingtheproblemofperformancemodelinginabayesiansettingcanbedonevia probabilisticprogramming .usersofthis paradigmmustspecifythreekeycomponentswitha probabilistic programminglanguage ppl likelihood prior andobservations.
withthese theppltakes careofbayesianinferenceaccordingto bayes theorem posterior bracehtipdownleft bracehext bracehext bracehtipupright bracehtipupleft bracehext bracehext bracehtipdownright p a b likelihood bracehtipdownleft bracehext bracehext bracehtipupright bracehtipupleft bracehext bracehext bracehtipdownright p b a prior bracehtipdownleft bracehtipupright bracehtipupleft bracehtipdownright p a p b we refrain from explaining bayesian statistics from scratch but explain in what follows the necessary components for inference.
if weassumethat aandbaredistinctevents then p mapsanevent to its probability to occur p gives the conditional probability ofanevent agiventhatanotherevent boccurs.inthecontextof probabilisticprogramming aisavectorof randomvariables that representsmodelparameters whereas brepresentsobservations.
arobability ensity unction pdf is a function of a random variable whose integral over an interval represents the probability of the random variable s value to lie within this interval.
accordingly p maps a random variable to its pdf and p returns the conditionalpdfofarandomvariablegiventhatanotherrandom variable has a certain pdf.
with these definitions we next explain thecomponentsofbayes theoremthatarerelevantforprobabilistic programming.
likelihood parenleftbigp b a parenrightbig.the likelihood specifies the distribution of observations bassumingthatthepdfsformodelparameters aare true.
with probabilistic programming the likelihood is typically specified as a generative model that incorporates random variables.
imagine an example in which we repeatedly toss a coin to find out whether and how it is biased.
we can represent the probabilities of thepossibleoutcomes headsandtails withabernoullidistribution 686b whose parameter p defines the probability of heads.
formally we first let abe a bernoulli distributed random variable and then define the likelihood p b a to be determined by a a b p b a a while this model has only one random variable more complex modelsarepossible ho wever the inferencemaynotbeanalytically solvable requiring approximations such as monte carlo markov chain mcmc sampling .suchagenerativemodelcanmake predictions that are pdfs i.e.
posterior distributions themselves.
prior parenleftbigp a parenrightbig.priors define our belief about the distribution of our random variables before seeing any training data.
choosing priorsnaturallyrequiresdomainknowledgeandiscomparableto selecting a optimization starting point.
an uninformed prior for the coin toss example is a b .
which assumes that both heads and tails are equally probable.
posterior parenleftbigp a b parenrightbigfromobservations b givenalikelihood we can finally update our prior beliefs with observations.
from a machine learning stance observations form the training set.
in caseofthecoin tossexample runningbayesianinferencewith5 observedheadswillyieldanupdatedgenerativemodel theposterior which will give heads a higher probability.
bayesian performance modeling inthissection wedescribeourapproachofincorporatinguncertainty into performance influence models.
figure provides an overviewofallstepsinvolved.inanutshell weperformthefollowingtasks first wepreprocessagivensetofmeasuredconfigurations i.e.
thetrainingset toensurethatinference i doesnotbreakand ii finishesinareasonabletime.second weapplyprobabilistic programmingto buildabayesian modelfor aselectionof options and interactionsthereof.
itis key forscalability thatthis selection comprises the actual set of influencing options and interactions.
third weestimatethepriorsforthe model srandomvariables i.e.
optionsandinteractions andcomputeafittedmodelwithbayesian inference.
.
data preprocessing our approach relies on a training set consisting of a number of sampled configurations that are attributed with their performance.
thus ourapproachcanbecombinedwithanysamplingstrategy suchasfeature wise t wise orrandomsampling .however it is important to process the sample set to avoid inference failures and to promote interpretability as we explain next.
similartoordinaryleastsquares bayesianinferenceisprone tofailureif multicollinearity existsinthetrainingset whichoccurs whenthevaluesofindependentvariablesareintercorrelated .
letusconsiderthefollowingtrainingsetforanexemplarysoftware system with options x y z and m illustrating multicollinearity bxyzm optionbismandatory.itrepresentsthebasefunctionalityofthe system which results from configuration independent parts of the code.options x y andzformanalternativegroup thatis thesystem s constraints enforce that exactly one of them is active in each configuration.
an important insight is that an alternative group introduces multicollinearity to a training set because the selection of any single option is determined by the remaining options for example z x y.multicollinearitynotonlyhindersinference but also interpretability.
consideringthe training set above we seethat the followingperformance influencemodelsare accurate with respect to the measurements but assigning different contributions of individual options c c b c x c y c z c c b c x c y c z c c b c x c y c z becauseexactlyoneoptionofthealternativegroupisactivein each configuration the base performance of a software system can beattributedtothebasefunctionalitybandtheoptionsofanalternative with any ratio.
for example option xcan have an influence of or none depending on how we assign the performance to thesystem sbasefunctionality.therefore performanceinfluence modelsforsuchsystemsaredifficulttocompareandinterpret.here we do not even know whether an option e.g.
x is influential at all.
this is a problem that related approaches share .
choosingadefaultconfiguration providesremedyformulticollinearityinferencefailuresandinterpretabilityproblems.thatis we select a default option for each alternative group using domain knowledgeoratrandom.wethenremovetheseoptionsfromthe training set to achieve the following effects default options performance influences are set to .
multicollinearity arising from alternative groups is reduced sincetheselectionofasingleremainingoptionofanalternative group cannot be determined without the removed defaultoption i.e.
z x ydoesnotholdanymoreif any of these options is removed from the training set .
mandatory options which must be selected in each configuration introduceaspecialcaseofmulticollinearity.option mismandatory andthereforepresentineachconfigurationandindistinguishable from the base influence.
similar to alternative groups a model can splitthebaseinfluencebetweenmandatoryoptionsandthebase influence with any ratio.
moreover we can see that such an option does not contribute any information to the model by computing the shannon information entropy h o summationdisplay.
x 0po x log2 parenleftbigpo x parenrightbig asmisselectedineachconfiguration itsonlyselectionvalue is with selection probability pm .
we see that therefore 687figure workflow of p4 preprocess data compose model from options and interactions estimate priors for random variables infer bayesian performance influence model.
the information entropy of mis h m pm log2 parenleftbigpm parenrightbig pm log2 parenleftbigpm parenrightbig log21 log20 for that reason we can safely remove mandatory options from thetrainingset.thesameappliesfordeadoptions whicharenever active.
notethatoptionsmayonlyappeartobedeadormandatoryasan artifactofthesamplingprocess.thatis itisinsufficienttoquery only the system s variability model for its constraints to detect mandatory or dead options.
hence we perform constraint mining onthesamplesetratherthanthewholesystemtoovercomethis problem.weusetheshannoninformationentropyinequation7 asameanstodeterminedeadoptionsandscanthesetofoptions for combinations that appear to be alternative groups.
.
model composition tobuildabayesianmodelwithprobabilisticprogramming wefirst needtospecifywhichoptionsandinteractionsarepresentinthe model.
subsequently we create random variables from this model structure to account for epistemic and aleatoric uncertainty.
.
.
option and interaction filtering.
composingamodelfrom all options and all potential interactions whose number is exponential in the number of options is impractical for large software systems because models with high numbers of parameters are difficult to interpret and more importantly inference may become computationallyinfeasible .therefore weapply modelselection toconstrainthenumberofparameters.inparticular weusea subset selectionapproach becauseityieldsasubsetofunalteredoptions from a parent set which is not the case for other approaches suchasdimensionalityreduction .webuildtheparentsetof available options sfrom all options oof the system in question as well as all pair wise interactions iwiths o i. we map each pair wiseoption itoavirtualoptionwithrespecttoitsconstituting optionsorandos c on p c or c os with p i r nequals comparedtohigher orderinteractions pair wiseinteractions havebeenfoundtofrequentlyinfluenceperformance andtobe the most common kind of interaction .
however we acknowledgethatconsideringhigher orderinteractionsmayimprovethe accuracy of our approach at the cost of possibly leading to computationally intractable models.
subset selection approaches define a filter function f s mapsto which yield if an option or interaction of the parent set sshould be considered by the model and otherwise.
the result of subset selection consists of filtered options and interactions v braceleftbig s s sandf s bracerightbig similar to previous work we apply lasso regression on the preprocessed training set.
as a result lasso assigns zero performance influence to less and non influential options andinteractions and it distributes the performance influence amongthe remaining elements in v. our lasso filter selects vl v whose performance influence i lasso vl is non zero according to lasso regression flasso vl braceleftbigg 0i lasso vl 1i lasso vl nequal0 .
.
applied probabilistic programming.
we follow related approachesforperformancemodelingofconfigurablesoftwaresystemsandchosean additivemodel tomaketheuncertaintyofthe options and interactions performance influence explicit.
we start withamodelthattakestheformofequation4 whichrepresents the state of the art with two differences instead of scalar influences rn we use a probability vector whose elements each have a pdf and form the coefficients as explained in section .
.
we use the filtered options and interactions vfrom section3.
.1andthusenableourmodeltocapturenon linear performance influence ep c 0 1 c o1 n c on n c on n i c on i to infer the distribution of an option we need to specify a prior distributionfortheprobabilityvector .thisdistributionshould be continuous i.e.
defined over all r and have non zero mass for any r not to exclude certain values entirely.
for performance modeling we choose the normal distribution n .
it has a mode that other than the uniform distribution lets usencode an influence area of high probability.
that is an option s or interaction s influence has a normally distributed probability to fall into an interval to be inferred by probabilistic programming.note that even if a normal distribution is not the best fit for all randomvariables bayesianinferencecanadjustthem.wedescribe how to determine the parameters for chosen prior distributions such as the mean and the standard deviation for the normal distribution n in section .
.
688atthispoint wehaveconstructed ep amodelthatincorporates epistemicuncertaintyin .toaccountforaleatoricuncertainty i.e.
theuncertaintyinthetrainingset weusetwodifferentmodels one for homoscedastic constant variance and one for heteroscedastic variance depending on true performance aleatoric uncertainty.
thesemodelsbuildon ep.weadoptthecommonpriorofanormal distribution for both models.
homoscedasticmodel.
ifweassumethatthevarianceofuncertainty is equal for all training set samples we can complete our bayesian model with a normal distribution around ep c ho c n parenleftbig ep c parenrightbig this normal distribution is modeled as an additional random variable whose parametercapturesthevarianceofabsoluteerrors in training set samples.
heteroscedasticmodel.
toaccountforerrorsinthetrainingset thatarerelativetothetrainingsetsampleperformance weintroduce rel a random variable that captures uncertainty about the errorratio.asanerrorratioisin r i.e.
acontinuous positive variable wechoose the gammadistribution as priorfor rel.
the gammadistributionwithashape aandaspreadparameter bcan take a possibly skewed bell shape with non negative values rel g a b similar to the homoscedastic model we define the heteroscedastic model as a normal distribution around ep c but with the productoftheepistemicperformancepredictionandtherelative error ratio relas standard deviation he c n parenleftbig ep c ep c rel parenrightbig .
prior estimation regularbayesianinferencerequirestheusertoestimatepriordistributions for the model s random variables from domain knowledge or personal experience.
distributions that are too uninformative i.e.
very wide can lead to a holdof the inference whereas distributions thatare too informativewill alsoslowdown inference if theyareimprecise .ourapproachautomaticallychooseswhich optionsandinteractionsaremodeledasrandomvariables suchthat theuserdoesnotneedtoknowwhichrandomvariablesneedpriors beforehand.
for that reason we employ an automatic prior estimationfollowingthe empiricalbayesapproach whichdiffers fromtheregularbayesianapproachinthatitestimatespriorsfrom the training data.
as a result every aspect of bayesian modeling is automated for the user.
.
.
epistemic uncertainty priors.
we capture epistemic uncertaintyinourbayesianmodelinrandomvariablesforthebase influence and the influences for options and interactions whose assumed normally distributed priors rely on means and standard deviations .
we propose a prior estimation algorithm that uses the influence valuesofotheradditivemodelstoestimatepriors.asmodels we use instances of elastic net withrevenly distributed ratios of l1 .
forl1 elastic net behaves like lasso for l1 0i t behaveslikeridgeregressionanditinterpolatestheerrorfunctionsof both approaches for l1 .
we fit elastic nets evenly distributed with l1on the training set.
this way we obtain a set of models mwith different performance influences i for the previouslyselectedoptionsandinteractions.next wedeterminetheempiricaldistributionofinfluencesforeachoptionandinteraction im vl braceleftbig im vl m m bracerightbig we could use the mean and standard deviation of imas prior and foreachoptionandinteraction.however notallmodelsin mwillfitthetrainingdatawell.toreducetheinfluenceforunfit models we weigh each model according to its average error on the training set w mi summationtext.
m j mj barex barex barex m i m wecomputetheweightedmean w t andweightedstandard deviation w t for a specific option or interaction tas follows w t summationtext.
w j 1w j summationdisplay.
i im t w i w t radicaltp radicalbt1 summationtext.
w j 1w j summationdisplay.
i im t w i parenleftbig w t i parenrightbig2 we added the tuning parameter to enable polynomial weighting.
that is the influence of models with the lowest average error is increased for .
in a pre study we empirically evaluated different values for and found that yields accurate priors.
.
.
aleatoric uncertainty priors.
we model aleatoric uncertainty i.e.
uncertainty in each training set sample as a normal distribution for the homoscedastic model hoand as a gamma distributionastherelativeuncertaintyintheheteroscedasticmodel he.
we build the set of all absolute prediction errors for all modelsm mover the samples in the training set and fit a normal distribution using maximum likelihood estimation to estimate a prior for the aleatoric uncertainty in ho.
likewise we estimate a priorforthegammadistributionin ho butwecomputerelative prediction errors instead to model the error ratio cf.
equation .
.
bayesian inference and prediction as discussed in section .
bayesian inference uses prior assumptionsonpdfsofrandomvariablesthatformagenerativemodel called likelihood to compute a posterior that is an updated belief aboutthe randomvariable spdfs.unfortunately theposteriorto many bayesian inference problems cannot be computed directly sorecentresearchinthisfieldhasdevelopedalgorithmsthatcan estimate the posterior approximately.
two notable classes of inference algorithms are variational inference and markov chain monte carlo .
variational inference algorithms tune the prior distribution s parameterswithoutchangingthetypesofthedistributions i.e.
a prior normal distribution stays a normal distribution .
this method is preferred for quick results that do not need to be precise.
markov chain monte carlo mcmc algorithms draw samples fromtheposteriordistributionsandareabletoestimatearbitrary posteriordistributionsintheory apriornormaldistributionmay 689bytransformedtoaskeweddistribution .mcmcalgorithmsare consideredmoreprecise butalsoslowerthanvariationalinference.
wefollowacombinedapproachbyfirstestimatinganapproximate solution with variational inference and subsequently finetunewiththe no u turnsampler nuts anmcmcalgorithm.
we allow .
iterations for variational inference but abort on convergence.nutsusestheintermediateresultofvariationalinference and draws samples from the posterior distributions in total from which are reserved for internal tuning.
in the idealcase weobtain4000samplesofeachrandomvariable sposterior distribution which enables analysis of uncertainty at a high resolution.
prediction.
topredicttheperformanceofaconfiguration c w e insertc soptionselectionvaluesinto c o1 ... c on anddetermine active interactions according to equation .
we can now draw a numberofposteriorsamplestoapproximatethedistributionforthe prediction.
increasing the number of posterior samples makes the approximation more accurate but also slows down prediction.
we draw1000posteriorsamplestoyieldagoodapproximation.with this approximation we can make different kinds of predictions for which we introduce individual notations.
the most informative kindofpredictionisthesampledapproximationitself tildewide .using tildewide wecancomputeaconfidenceintervalforadesiredconfidence ci .
this yields the interval around the mode of predictionoverwhichthepredicteddistributionintegratesto ci.
we use to indicate the confidence interval by default.
we can also use mode of the approximation as a single point estimate prediction dotacc .
evaluation to evaluate our approach we state three research questions that are in line with related work and are also concerned with the new possibilitiesofobtainingaconfidenceintervalforperformancepredictions.
specifically we answer the following research questions rq1 canweaccuratelypredictperformanceasascalarvalue with probabilistic programming?
thisresearchquestionplacesourapproachinrelationtoastate ofthe artapproachthatresortsonlytoascalarvalue.althoughthis is not the main usage scenario we evaluate whether our approach has a comparable accuracy.
rq2 can we accurately predict performance in terms of a confidence interval with probabilistic programming?
rq2referstotheabilitythatuserscanspecifyaconfidenceinterval ofpredictions.thiscansubstantiallyeffectpredictionaccuracyand evaluates the strength of our approach.
rq3 how reliable are predicted confidence intervals?
the third research questions aims at providing a deeper understanding of confidence intervals and incorporated uncertainties in our approach.
we evaluate whether the confidence intervals trulycapture the uncertainty in the predictions.table1 overviewofthesubjectsystemswithdomain number of valid configurations c number of options o and the kind of performance for prediction.
domain c o performance 7z file archive utility compression time bdb c embedded database response timesune multigrid solver solving timehipa ccimage processing solving time hsqldb java based database energy consumptionjavagc garbage collector timellvm compiler infrastructure compilation timelrzip file archive utility compression timepolly code optimizer runtimepsql database system energy consumption vp9 video encoder encoding time energy consumption x264 video encoder encoding time energy consumption .
subject systems forourexperiments weuse12real worldconfigurablesoftware systemsthathavebeenusedintheliterature aspresentedintable1.
weusemeasuredexecutiontimeasperformancefor10subjectsystems from kaltenecker et al .
.
for vpxenc and x264 we have additionally measured energy consumption with a different workload.
in addition we consider energy consumption for two further subjectsystems postgresql shortpsql andhsqldb .a further description of the systems including the used benchmarks is given at our supplementary web site2.
weadopttheprocedureofextractingtrainingandtestsetsfrom each system s measurement data from kaltenecker et al .
.
that is we apply t wise sampling with t to obtain three training test sets t1 t2 t3 of different sizes.
each system s whole population i.e.
all measurements form its test set.
.
setup we implement our approach with the pymc3 framework.
pymc3offersimplementationsformcmc variationalinference as well asconfidence intervalcomputation for model parameters andpredictions.formaximumlikelihoodpriorestimations werely onscipy .
the result is a performance prediction tool based on probabilistic programming p4for short.
toanswerourresearchquestions weinferbayesianmodelswith absolute and relative error with p4 for the chosen subject systems using three training sets t1 t2 t3on a cluster of machines with intel xeon e5 2690v2 cpu and 64gb memory.
for the ten subject systemsbykalteneckeretal .
weusethetrainingsetsprovided attheirsupplementarywebsite.fortheremainingsubjectsystems we sample new training sets with spl conqeror .
fort t wise sampling is equal to option wise sampling which yields n o samples.
since we want to evaluate our approachalsoforlearninginteractionsamongoptions creating n i random variables leads to a modeling problem with more variables dir 5a525f45ec77dbe982081e7f8159e9541391725e 690020406080error in o i 0o i 0o i 0o i 0o i 0o i 0o i 0o i 0o i 35o i 35o i 35o i 357z 0255075100125150o i 0o i 0o i 0o i 0o i 4o i 4o i 4o i 4o i 11o i 11o i 11o i 11berkeleydbc 05101520o i 0o i 0o i 0o i 0o i 23o i 23o i 23o i 23o i 127o i 127o i 127o i 127dune 0102030405060o i 0o i 0o i 0o i 0o i 89o i 89o i 89o i 89o i 185o i 185o i 185o i 185hipacc 0102030405060o i 0o i 0o i 0o i 0o i 66o i 66o i 66o i 66o i 178o i 178o i 178o i 178javagc t02468error in o i 0o i 0o i 0o i 0o i 3o i 3o i 3o i 3o i 3o i 3o i 3o i 3llvm t0204060o i 0o i 0o i 0o i 0o i 15o i 15o i 15o i 15o i 41o i 41o i 41o i 41lrzip t0102030o i 0o i 0o i 0o i 0o i 79o i 79o i 79o i 79o i 97o i 97o i 97o i 97polly t0100200300o i 0o i 0o i 0o i 0o i 99o i 99o i 99o i 99o i 172o i 172o i 172o i 172vp9 t0510152025o i 0o i 0o i 0o i 0o i 0o i 0o i 0o i 0o i 23o i 23o i 23o i 23x264model line color p4 with he relative error spl conqueror error type line style mape mape ci mape ci figure3 scalarmeanabsolutepercentageerror mape of splconqerorcomparedtothemapeandintervalpredictions mapeciforconfidencelevels and95 of p4 withabsoluteerror hoandrelativeerror he fort wisesampledtraining sets.
for each subject system s training set we specify p4 s model size in terms of the number of modeled options o andinteractions i below the system s name.
than observations.
we avoid this situation by excluding interactions from our model for t1.
this might affect prediction accuracy especially compared to other approaches that do not exclude interactions.
we will discuss this in rq .
toaccount forstochasticelements inmcmc we runtheinferenceforeachsystem strainingsetwith5repetitions.theinference took 418s on average with the times ranging from sfor the smallest models and hfor the largest.
among all inferences two failed despite our training set preprocessing in section .
.
.
rq accuracy of scalar predictions .
.
setup.
wechosesplconqerorforcomparisonbecause itsharestheadditivemodelstructurewithourapproachandisused as baseline in the literature .
for comparison we rely on accuraciesof splconqerorasreportedbykalteneckeretal .
.
that is we consider for rq 1the ten subject systems that the original authors have used.
another benefit is that kaltenecker et al .
provided raw measurements of the whole population so we have a reliable ground truth.
weusetheinferredperformance influencemodelstopredictthe performance of the whole populations of our subject systems.
we adoptthe meanabsolutepercentageerror mape fromprevious work toquantifypredictionaccuracy.thatis wefirstcompute the absolute percentage error ape for each configuration c cwith the measured performance true c and predicted scalar performance dotacc c for our models hoand he ape c barex barex true c dotacc c barex barex true c we then compute the mape as the average over all apes mape c summationtext.
c cape c c .
.
results.
asfigure3shows p4achievesmapescorescomparabletosplconqeror.table2providesamorefine grained view.
we see that the overall accuracy is higher when using spl conqeror which is to be expected as only the mode is taken fromtheperformancedistributionprovidedaspredictionsbyour approach.nevertheless weobservethat formanysystems especially when using the relative error he we obtain a similar or even better prediction indicated by underscored values.
the mean error is thus distorted by some larger outliers such as for hipacc and vp9.
these systems have many alternative options so there is a larger uncertainty involved.
since we are not using the provided confidence interval we deprive our approach of its strength.
interestingly comparedto t1 forsomesubjectsystemsp4performs worse on t2.
the reason is that the increased number of randomvariablesinp4 duetotheadditionalmodelingofinteractions requires more measurements as provided by t2to effectively inferperformancedistributions.moreover weseeacleartrendthat withanincreasingnumberofmeasurements p4closesthegap in prediction accuracy with spl conqeror and even outperforms it fort3and hefor out of systems.
toanswerrq ourapproachachievestheaccuracyofstateof the art scalar predictions when a sufficient number of measurements is provided.
in the case of fewer measurements the overhead of learning probability distributions leads to more inaccurate predictions.
691table2 scalarmeanabsolutepercentageerror mape ofsplconqeror shortsplc comparedtothemapeandconfidence intervalpredictionsmape mape ci of p4 withabsoluteerror ho andrelativeerror he fort wisesampledtrainingsets.
best scalar mape values for each training set are shaded light gray best overall mape values are shaded dark gray.
splc mape homape hemape ho mapeci hemapeci t 1t 2t 3t 1t 2t 3t 1t 2t 3t 1t 2t 3t 1t 2t 7z .
.
.
.
.
.
.
.
.
.
.
.
.
.
bdb c .
.
.
.
.
.
.
.
.
.
.
.
.
dune .
.
.
.
.
.
.
.
.
.
.
.
.
.
hipacc .
.
.
.
.
.
.
.
.
.
.
.
javagc .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
llvm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
polly .
.
.
.
.
.
.
.
.
.
.
.
.
.
vp9 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lrzip .
.
.
.
.
.
.
.
.
.
.
.
x264 .
.
.
.
.
.
.
.
.
.
.
.
.
mean .
.
.
.
.
.
.
.
.
.
.
.
.
.
rq accuracy of confidence intervals .
.
setup.
confidenceintervalswithconfidence ci specify a range in which a given pdf integrates to ci.
for predictions a confidence interval specifies a performance range for which the model is confident that it contains the true performance value of the corresponding configuration.
consequently we can expect the true performance to lie outside the confidenceintervalin ofpredictions.although wecanexpect to always capture the true performance with a confidence interval suchanintervalwilllikelyapproach forpdfs that are defined over r. similar to rq we use a relative error metric to answer rq .
however for rq we use p4 to predict confidence intervals as prediction which is the actual strength and novel feature of our approach.
instead of using the apeof a scalar prediction we compute the confidence interval s apeciwith relation to the closest endpoint of the confidence interval for an outlying true performance we define apeci for an confidence interval that includes the measured performance apeci c min c barex barex true c barex barex true c hence the mape ciis the average over all apeci similar to equation21.forourmodels hoand he wereportthemape cifor predictedconfidenceintervalswith ci forhighlyconfident predictionsand ci forlessconfidentpredictions forwhich we expect a narrower interval and consequently a higher error.
.
.
results.
thedotted linesinfigure3 illustrateasubstantialdecreaseinpredictionerrorwhenusingaconfidenceinterval rather than a scalar prediction.
note that we report in figure onlymape ci sfor he weprovidesimilarresultsfor hoatour supplementarywebsite.table2providesfurtherdatafor ho.it revealsthatthepredictedconfidenceintervalsfor7z bdb c lrzip and x264 contain all measuredperformance values when training the absolute model hoont3.table3 fivemostuncertainfeaturesmeasuredbythemean relative confidence interval ?
jaccording to equation of models trained on t1.
values for the variance inflation factor vif larger10areindarkgray highlyproblematic and values between and are in light gray moderately problematic .
files 30 blocksize 1024 were removed from t3.
t1 t3 system option ?vif ?vif vp8 e threads 4 .
.
.
.
vp9 t bitrate 1500 .
.
.
.
7z t files 30 .
.
.
7z t blocksize 1024 .
.
.
vp9 t variablebitrate .
.
.
.
we illustrate how more training samples allow p4 to decrease uncertainty in internal parameters to achieve better prediction accuracy using the variance inflation factor vif .
the vif is an indicator for multicollinearity which can be computed for the activationvaluesofanoption ojinthetrainingset t.itisbasedonthe coefficientofdetermination r2.todetermine r2foranoption oj we fit a linear regression function fjto predict whether ojis active in a configuration c ojwith the remaining options as predictors.
we compute the vif as follows vifj r2 j r2 j summationtext.
c t parenleftbig c oj c oj parenrightbig2 summationtext.
c t parenleftbig c oj fj c oj parenrightbig2 avifof0indicatesanoptionwithnomulticollinearityinthe training set while higher values mark increasingly problematicmulticollinearity.
we adopt the thresholds of and t o indicate moderate and highly problematic multicollinearity respectively.
.
.
.
.
.
model confidence0.
.
.
.
.
.0observed confidencet .
.
.
.
.
model confidencet .
.
.
.
.
model confidencet ci mape in model line color p4 with he relative error p4 with ho absolute error metric line type ci mape in observed confidence figure mape cidepending on model confidence solid versus uncertainty calibration dashed for t wise training sets aggregated over all subject systems.
gray dashed line indicates ideal calibration.
although we could use the vif as a filter for feature selection cf.
section .
to remove options with high multicollinearity in thetrainingset thecomputationaleffortrequiredtocalculateall r2 jmakes it infeasible in practice.
hence we compute the vif only for the most uncertain options in t1to analyze whether multicollinearityisapossiblecauseforuncertaintyofoptions influences.
to compute the uncertainty of an option influence ?
j we use its confidenceinterval jandpointestimate dotacc j.toremovetheinfluence of differing influence scales between software systems we determine the relative confidence interval width as the ratio of the absolute confidence interval width j and the point estimate ?
j j dotacc j looking at table we see that all five options exhibit either a moderate or even a high vif for the training set t1.
this points to a situation in which the learning procedure cannot safely assign a performanceratiotothespecificoption.investigatingthiscloser four options are part of an alternative group despite our efforts toavoidmulticollinearitybyremovingonealternativefromeach alternativegroup.foroption threads 4 wefoundthatitwasactive inalmosteveryconfiguration 13outof16 reducingthecontained information according to equation .
tofurther confirm ourhypothesis that multicollinearitycan be a possible cause we show in table the uncertainty ?
jand the vif for these five options using the larger training set t3.w es eea substantialreductioninuncertaintyforthreeoptionsinlinewith the reduction of the vif.
this strongly indicates that a reduced multicollinearityreducesalsotheuncertaintyofanoption sinfluenceonperformance.options files 30andblocksize 1024 have no uncertainty as they were chosen by p4 to be removed from the alternative group in t3.
overall hoyields better results than hein most cases but both approaches always show substantially lower relative errors than scalarpredictions.
of course itwould be easyfor a modelto predict all performance values correctly with a sufficiently large confidence interval.
however our findings for rq 3demonstrate that p4 s prediction confidence intervals are reliable as we will discuss in section .
.toanswerrq usingconfidenceintervalsto frametheconfidence of predictions substantially reduces the prediction error.
thatis ourapproachisableto modeltheuncertaintyas well as the true performance distributions accurately.
.
rq reliability of confidence intervals .
.
setup.
as predictions our approach can yield confidence intervals with any given confidence level ci .w e call a model s predicted confidence intervals reliable if predictions withan ciconfidenceintervalcontainthemeasuredperformance with a similar observed frequency obs i.e.
obs ci ci .
to compute the observed frequency obs ci for an ciconfidence interval we first define the function within which returns if themeasuredperformance true c liesinapredictedconfidence interval c and otherwise within parenleftbig true c c parenrightbig braceleftbigg 1 true c c else second the observed frequency is computed as the average of within over all configurations of a subject system and their measured performances true c obs ci summationtext.
c cwithin parenleftbig true c c parenrightbig c if ci greatermuch obs ci the predicted confidence interval is inaccuratemoreoftenthanweexpectandshouldhavebeenbroader conversely thepredictedconfidenceintervalshouldbemorenarrow and thus more informative if ci lessmuch obs ci .
since using confidence intervals for performance predictions is novel we have no baseline to which we can compare.
hence we report the ob served frequencies for confidence levels cifrom to in steps of as well as the average error in percentage to answer rq3.inaddition wereportthemape ciforallconfidenceintervals.
.
.
results.
figure4showsacalibrationplotthatcompares ciwith obsusing dashed lines.
a model with ci obsfor all ciwould yield values alongthe dashed gray diagonal.
values abovethediagonalindicatetoobroadconfidenceintervals i.e.
our predictionsaremoreaccuratethantheyshouldbe valuesbelow itsignalconfidenceintervalsthataretoonarrow.thesolidlines 693in figure show the mean mape ciover all subject systems for boththerelativeandtheabsolutemodel.theshadedareaaround it constitutes a confidence interval.
whenanalyzingthedashedlines weseethatusingtheabsolute error hoyieldsintervalsthatareclosertothediagonalthanwhen usingtherelativeerror he.moreover thereisacleartrendthat whenusingmoremeasurements theintervalsbecomeeithernearly perfectly aligned or are underestimating the models prediction accuracy.
hence we see a picture that resembles the picture when usingthemodeforscalarperformanceprediction theapproach requires a certain number of measurements to become accurate but then works robustly.
we can make a further interesting observation when comparing the confidence intervals dashed lines with the mape ci solid lines .
first and most importantly we see that using confidence intervalsof varyingsizes hasa clearmonotonic relationshipwith theprediction error.
thatis increasing theinterval decreases the error.second theerrorsfallrapidly especiallyfor t2andt3 already whenusinganarrowinterval suchas25 .thisisgoodnewsas thisclearlyindicatesthatnarrowconfidenceintervalsyieldaccurate predictions.
third we observe that for the solid lines the uncertaintyishigherwithfewermeasurements asindicatedbythe colored area.
that is the model is aware that the measurements are insufficient to actually make trustworthy predictions.
this is a featuremissinginallscalarpredictionapproaches.forexample for spl conqeror we have no clue whether the model is confident with a certain prediction.
with p4 we have a means to quantify this confidence.
to answer rq with enough measurements our approach yieldsconfidenceintervalsthatcontainthetruevaluewitha frequency that matches the specified confidence.
even with oursmallesttrainingset t1 confidenceintervalswithhigher specified confidence contain the true value more often.
threats to validity threatstointernalvalidityarisefrommeasurementbias.wereusea measurement set from a recent paper whose authors controlled for thisbiasbyrepeatingthemeasurementsseveraltimes .athreat toconstructvaliditymayarisefromthemodelconstructionprocess in pymc3.
we selected probability distributions for the random variables based on typical least squares error distributions and best practices for regression modeling in probabilistic programming.
externalvalidityreferstothegeneralizabilityofourapproach.our data set comprises different subject systems of varying domains andsizes.moreover weassesseddifferentproperties suchasenergy consumption and response time.
we made similar observations for all systems such that we are convinced that our approach worksonalargeandpracticallyrelevantclassofconfigurablesoftware systems.
related work thereisanumberofapproachesinthefieldofperformancemodeling.cart anditsimprovedversiondecart userule based models to accuratelylearn performance models with a smallnumber of samples.
flash is a sequential model based methodthatreliesonactivelearningtofitcart modelsmoreefficiently.
deepperf isadeeplearning basedapproach whichusessparse neural networks for performance estimation.
zhang et al .propose a framework to model performance influence with fourier approximation whereas nair et al .employ spectral learning with dimensionalityreduction .splconqeror learnsan additive model with step wise selection of new terms .
none of the proposedapproachesconsidersuncertaintyinpredictionsandin the internal representation of influences producing only scalar estimates.
notably the need for incorporating uncertainty in performance modelinghasbeenarguedbeforebytrubianiandapel .while there are alreadyconsiderations in other fieldsfor both epistemic andaleatoricuncertainty suchasforcomputervision forsoftware engineering there are only approaches that model some kind of epistemic uncertainty.
antonelli et al .have incorporated uncertaintybyallowingtwoparametersofaperformanceindexforcloudcomputingsystemstobeuncertainandthusadapttochanginghardware .anotherapproachbyarcainietal .transformsafeature modelintotwoqueueingnetworks oneeachforthetwovariants with minimal and maximal performance and thereby represents uncertaintyinperformance .to thebestofourknowledge we arethefirsttofollowtrubianiandapel scalltoincorporateboth epistemic and aleatoric uncertainty in performance modeling of configurable software systems.
summary existing approaches forperformance influence modeling provide onlyscalarpredictionsbasedonmodelinginfluencesofoptionsandinteractions with scalar values.
we argue that these approaches neglect uncertainty arising from the modeling and measurement process.weproposeanovelperformance influencemodelingapproach thatincorporatesuncertaintyexplicitlyandyieldsconfidenceintervalsalongsidescalarpoint estimatepredictions.thisway we provide not only a singular number as a performance estimate but also a posterior distribution and a confidence in which range a performance value lies.
our experiments with real world software systems show that our implementation p4 yields scalar prediction accuracies that match the state of the art when provided with a sufficient number of measurements.
further evaluation shows that theconfidenceintervalsprovidedarereliableand whenusedfor prediction achieve competitive accuracies.
the analysis of our trained models indicates that options that are selected in almost every configuration can reduce the amount ofinformationcontainedinatrainingset renderingtheoption s influenceuncertain.thisobservationcallsforashiftincurrentsampling strategies by taking the information gain more into account ascomparedtocoverageoruniformness.p4showeditspotential especially with pairwise and triple wise sampled training sets.
improving p4 for small training sets hence remains an open issue.
a possibleremedyarep4 soptioninfluenceuncertainties whichmay be facilitated in an active learning setupto learn more efficiently.
acknowledgment siegmund sandapel sworkhasbeenfundedbythegermanresearch foundation si si and ap .
694references fabioantonelli vittoriocortellessa marcogribaudo riccardopinciroli kishors.
trivedi andcatiatrubiani.
.
analyticalmodelingofperformanceindices under epistemic uncertainty applied to cloud computing systems.
future generationcomputersystems .
.
.
paolo arcaini omar inverso and catia trubiani.
.
automated model based performance analysisof software product linesunder uncertainty.
journal of information and software technology ist .
j.infsof.
.
donald e. farrar and robert r. glauber.
.
multicollinearity in regression analysis theproblemrevisited.
thereviewofeconomicsandstatistics .
vibhavgogateandrinadechter.
.
anewalgorithmforsamplingcspsolutions uniformly atrandom.
in principles andpractice of constraint programming cp .
springer .
jianmeiguo krzysztofczarnecki svenapel norbertsiegmund andandrzej wasowski.
.variability awareperformanceprediction astatisticallearning approach.in proceedingsoftheinternationalconferenceonautomatedsoftware engineering ase .
ieee .
jianmei guo dingyu yang norbert siegmund sven apel atrisha sarkar pavel valov krzysztof czarnecki andrzej wasowski and huiqun yu.
.
dataefficient performance learning for configurable systems.
empirical software engineering .
huong ha and hongyu zhang.
.
deepperf performance prediction forconfigurable software with deep sparse neural network.
in proceedings of the international conference on software engineering icse .
ieee .
huonghaandhongyuzhang.
.
performance influencemodelforhighly configurablesoftwarewithfourierlearningandlassoregression.in proceedings oftheinternationalconferenceonsoftwaremaintenanceandevolution icsme .
ieee .
christopherhenard mikepapadakis markharman andyvesletraon.
.
combiningmulti objectivesearchandconstraintsolvingforconfiguringlargesoftwareproductlines.in proceedingsoftheinternationalconferenceonsoftware engineering icse .
ieee acm .
herodotos herodotou harold lim gang luo nedyalko borisov liang dong fatma bilgen cetin and shivnath babu.
.
starfish a self tuning system for big data analytics.
in proceedings of the conference on innovative data systems research cidr .
.
r.carterhillandleec.adkins.
.
collinearity.
in acompaniontotheoretical econometrics.johnwiley sons ltd chapter12 .
.ch13 arthur e. hoerl and robert w. kennard.
.
ridge regression biased es timation for nonorthogonal problems.
technometrics .
https matthew d. hoffman and andrew gelman.
.
the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo.
the journal of machine learning research .
gareth james daniela witten trevor hastie and robert tibshirani.
.an introduction to statistical learning.
springer.
pooyanjamshidiandgiulianocasale.
.
anuncertainty awareapproach to optimal configuration of stream processing systems.
in proceedings of the internationalsymposiumon modeling analysisandsimulationof computerand telecommunication systems mascots .
ieee .
mascots.
.
martin fagereng johansen ystein haugen and franck fleurey.
.
an algorithmforgeneratingt wisecoveringarraysfromlargefeaturemodels.
inproceedings of the international software product line conference splc .
acm .
christian kaltenecker alexander grebhahn norbert siegmund and sven apel.
.
theinterplayofsamplingandmachinelearningforsoftwareperformance prediction.
ieeesoftware .
christian kaltenecker alexander grebhahn norbert siegmund jianmei guo andsvenapel.
.
distance basedsamplingofsoftwareconfigurationspaces.
inproceedings of the international conference on software engineering icse .
ieee .
alex kendall and yarin gal.
.
what uncertainties do we need in bayesian deep learning for computer vision?.
in proceedings of the international conferenceonneural informationprocessingsystems nips .curranassociates inc. .
armen der kiureghian and ove ditlevsen.
.
aleatory or epistemic?
does it matter?structuralsafety .
.
.
sergiykolesnikov judithroth andsvenapel.
.
ontherelationbetween internal and external feature interactions in feature oriented product lines a case study.
in proceedings ofthe internationalworkshopon feature orientedsoftware development fosd .
acm .
alanmiller.
.
subsetselectioninregression.
crcpress.
kevinp.murphy.
.
machinelearning aprobabilisticperspective.
mitpress.
viveknair timmenzies norbertsiegmund andsvenapel.
.
fasterdiscov eryoffastersystemconfigurationswithspectrallearning.
automatedsoftware engineering .
viveknair zheyu timmenzies norbertsiegmund andsvenapel.
.finding faster configurations using flash.
transactions on software engineering .
radfordmneal.
.
probabilisticinferenceusingmarkovchainmontecarlo methods.
department of computer science university of toronto.
robert m. o brien.
.
a caution regarding rules of thumb for variance inflationfactors.
quality quantity .
s11135 jehooh donbatory margaretmyers andnorbertsiegmund.
.findingnearoptimalconfigurationsinproductlinesbyrandomsampling.in proceedings of the joint meeting of the european software engineering conference and the acmsigsoftsymposiumonthefoundationsofsoftwareengineering esec fse .
acm .
herbert e. robbins.
.
an empirical bayes approach to statistics.
in proceedingsofthethirdberkeleysymposiumonmathematicalstatisticsandprobability volume contributions to the theory of statistics.
the regents of the university of california.
geoffreyroeder yuhuaiwu anddavidkduvenaud.
.
stickingthelanding simple lower variance gradient estimators for variational inference.
in proceedings of the international conference on neural information processing systems nips .
curran associates inc. .
john salvatier thomas v. wiecki and christopher fonnesbeck.
.
probabilistic programming in python using pymc3.
peerj computer science e55.
claude e.shannon.
.
a mathematical theoryof communication.
the bell system technical journal .
.tb01338.x norbertsiegmund alexandergrebhahn svenapel andchristiank stner.
.
performance influence models for highly configurable systems.
in proceedings of the joint meeting of the european software engineering conference and the acmsigsoftsymposiumonthefoundationsofsoftwareengineering esec fse .
acm .
norbert siegmund sergiy s. kolesnikov christian kastner sven apel donbatory marko rosenmuller and gunter saake.
.
predicting performance via automated feature interaction detection.
in proceedings of the international conference on software engineering icse .
ieee .
icse.
.
norbertsiegmund markorosenm ller martinkuhlemann christiank stner sven apel and gunter saake.
.
spl conqueror toward optimization of non functional properties in software product lines.
software quality journal .
ralph smith.
.
uncertainty quantification theory implementation and applications.
society for industrial and applied mathematics.
john r. taylor.
.
an introduction to error analysis the study of uncertainties in physical measurements 2nd ed.
.
university science books.
.
roberttibshirani.
.regressionshrinkageandselectionviathelasso.
journal of the royal statistical society.
series b methodological .
catiatrubianiandsvenapel.
.
plus performancelearningforuncertaintyofsoftware.in proceedingsoftheinternationalconferenceonsoftwareengineering new ideas and emerging results.
ieee .
.
dana van aken andrew pavlo geoffrey j. gordon and bohan zhang.
.
automatic database management system tuning through large scale machine learning.
in proceedings of the international conference on management of data sigmod .
acm .
laurensvandermaaten ericpostma andjaapvandenherik.
.
dimensionalityreduction acomparativereview.
journalofmachinelearningresearch .
pauli virtanen ralf gommers travis e. oliphant matt haberland tyler reddy david cournapeau evgeni burovski pearu peterson warren weckesser jonathan bright st fan j. van der walt matthew brett joshua wilson k. jarrod millman nikolay mayorov andrew r. j. nelson eric jones robert kern eric larson cj carey i lhan polat yu feng eric w. moore jake vand erplas denislaxalde josefperktold robertcimrman ianhenriksen e.a.quintero charles r harris anne m. archibald ant nio h. ribeiro fabian pedregosa 695paul van mulbregt and scipy .
contributors.
.
scipy .
fundamental algorithms for scientific computing in python.
nature methods .
niklas werner.
.
energy and performance evolution of configurable systems case studies and experiments.
master thesis.
university of passau.
jeffreywooldridge.
.
introductoryeconometrics a modernapproach 5ed.
.
south western college pub.
tianyinxu longjin xuepengfan yuanyuanzhou shankarpasupathy and rukma talwadker.
.
hey you have given me too many knobs!
understanding and dealing with over designed configuration in system software.
in proceedings of the joint meeting of the european software engineering conference and the acm sigsoft symposium on the foundations of software engineering esec fse .
acm .
yizhang jianmeiguo ericblais andkrzysztof czarnecki.
.
performance prediction of configurable software systems by fourier learning.
in proceedings oftheinternationalconferenceonautomatedsoftwareengineering ase .ieee .
yuqing zhu jianxun liu mengying guo yungang bao wenlong ma zhuoyue liu kunpeng song and yingchun yang.
.
bestconfig tapping the per formance potential of systems via automatic configuration tuning.
in proceedings of the symposium on cloud computing socc .
acm .
https huizouandtrevorhastie.
.
regularizationandvariableselectionviathe elasticnet.
journaloftheroyalstatisticalsociety seriesb statisticalmethodology .