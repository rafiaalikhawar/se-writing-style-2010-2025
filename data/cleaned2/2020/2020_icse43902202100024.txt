are machine learning cloud apis used correctly?
chengcheng wan shicheng liu henry hoffmann michael maire shan lu university of chicago fcwan shicheng2000 hankhoffmann mmaire shanlug uchicago.edu abstract machine learning ml cloud apis enable developers to easily incorporate learning solutions into software systems.
unfortunately ml apis are challenging to use correctly and efficiently given their unique semantics data requirements and accuracy performance tradeoffs.
much prior work has studied how to develop ml apis or ml cloud services but not how open source applications are using ml apis.
in this paper we manually studied representative open source applications that use google or a ws cloud based ml apis and found of these applications contain api misuses in their latest versions that degrade functional performance or economical quality of the software.
we have generalized anti patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ml api misuses.
i. i ntroduction a. motivation machine learning ml provides efficient solutions for a number of problems that were difficult to solve with traditional computing techniques e.g.
object detection and language translation.
ml cloud apis allow programmers to incorporate these learning solutions into software systems without designing and training the learning model themselves and hence put these powerful techniques into the hands of non experts.
indeed there are more than open source projects on github that use google or amazon ml cloud apis to solve a wide variety of problems among which more than were created within the last months.
while these apis make it easy for non experts to incorporate learning into software systems there are still a number of challenges that must be addressed to ensure that the resulting applications are both correct and efficient.
while certain challenges come with the use of any third party api this paper focuses on unique challenges for ml apis that arise due to the nature of learning itself.
complicated data requirements.
machine learning techniques are used to process digitalized real world visual audio and text content.
although such content can be generated by a huge variety of devices and encoding software the suitable input content and format encoding resolution size etc.
for ml apis are rather limited and often uniquely defined by the dnn training process.
for example cameras can produce images in many formats but the image sets on which ml models are trained have a relatively small variety .
thus it is up to the api user to select the input or convert the input into what the api can accept and effectively process.
complicated cognitive semantics.
unlike traditional apis that are coded to perform well defined algorithms ml apisaretrained to perform cognitive tasks whose semantics cannot be reduced to concise mathematical or logical specifications with inevitable overlap between different tasks e.g.
to detect a book in a scene a user might call eitherimage classification orobject detection.
users need a good understanding of these cognitive semantics underlying ml apis to pick the right api for the corresponding software component and usage scenario.
additionally learning models operate in a continuous space even if they ultimately produce a discrete output the discretization is the last step in the model .
thus it is up to users to understand the result of these calls and ensure that they know how to use the result correctly in the context of the software system.
complicated tradeoffs.
while many apis offer tradeoffs between engineering effort and performance e.g.
higher performance apis are more difficult to use ml apis have additional tradeoffs to consider.
the first is accuracy.
as ml apis do not produce discrete correct or incorrect answers it is up to users to understand the probabilistic nature of these api calls how different data transformation and api selection can affect the accuracy and the exact accuracy requirement of the corresponding software component.
furthermore the engineering effort involved in using ml apis is often related to transforming the input data which can have large effects on performance and accuracy.
finally as these apis perform computation in the cloud there is a monetary cost associated with every call which is again affected by data transformation and api selection and is yet another tradeoff to consider.
it is essential that users understand the engineering performance accuracy tradeoffs of every ml api call and ensure that their application s requirements are met.
if ml api users do not address the above challenges their software systems can suffer from inefficiencies in performance or cost and correctness issues.
in addition the fact that these apis do not produce binary correct incorrect outputs means that the resulting performance and accuracy losses can be difficult to diagnose e.g.
in addition to catastrophic failstop failures which are at least easy to notice misunderstanding the api semantics produces lower accuracy and higher cost software.
thus while these apis make it possible for non expert users to incorporate ml into software systems it is still necessary that users understand and avoid api misuses.
prior work studies software development for ml.
for example recent work proposes methods for finding bugs in ml libraries .
other work finds bugs related to designing and training ml models .
however to the best of our ieee acm 43rd international conference on software engineering icse .
ieee knowledge no prior work provides an empirical study detailing the software engineering issues that arise when calling thirdparty ml apis from within software systems.
b. contributions to understand the problems that arise when using ml cloud apis and design appropriate solutions we perform an empirical study of the latest versions as of august of github projects that include non trivial use of google cloud and amazon aws apis the two most popular ai services and cover all the three ml domains offered by them vision speech and language.
our study faces the challenge of lacking existing issuetracking system records about ml api misuses given the short history of ml apis.
consequently we carefully study these projects and discover previously unknown misuses in their latest versions by ourselves.
our study found that misuses of ml apis are widespread and severe out of these applications contain misuses in their latest versions more than half of which contain more than one type of misuse.
these misuses lead to various types of problems including reduced functionality such as a crash or a quality reduced output or degraded performance like an unnecessarily extended interaction latency or increased cost in terms of payment for cloud services.
their root causes are all related to unique challenges for ml apis discussed above which we present in detail in sections iv v and vi.
our study reveals common misuse patterns that are found in many different applications often with simple fixes that avoid failures improve performance and reduce cost.
therefore as a final contribution we design several checkers and small api changes in the form of wrapper functions that both check for and handle common errors.
many more misuses are found by our checkers beyond the projects in the initial study.
we present solutions to some of the problems we have uncovered in section vii.
overall this paper presents the first in depth study of realworld applications using machine learning cloud apis.
it provides guidance to help prevent errors while improving the functionality performance and cost of these applications.
we have released our whole benchmark suite automated checkers and detailed study results online .
ii.
b ackground several companies provide a broad set of machine learning cloud services such as google cloud ai amazon web service aws ai ibm watson and microsoft azure .
these services are built upon pre trained dnns designed to tackle specific problems.
they each offer a set of apis.
by calling these apis inference computations that use industry trained dnns can be conducted on powerful cloud servers without requiring developers to understand details about machine learning or conduct resource provision.
as shown in table i these cloud services cover three ml domains.
vision .
this includes image oriented and videooriented machine learning tasks like detecting objects faces local .takefridgephoto .findingredients3.generaterecipes ... fig.
an example of using ml apis .
landmarks logos text or sensitive content from an image or a video.
language .
this includes natural language processing nlp tasks like detecting or analyzing entity sentiment language or syntax from text inputs.
it also includes translation tasks.
speech .
this includes recognizing text from an audio input and synthesizing an audio from text input.
figure illustrates an example of how applications use ml apis.
it depicts the workflow of whats in your fridge an open source github application for recipe suggestion.
this application uploads a photo taken inside the fridge to the cloud applies a vision api to find out what is inside the fridge and then generates recipes accordingly.
of course as we will discuss later this application actually cannot deliver its functionality due to an api misuse.
iii.
m ethodology a. application selection our work looks at applications that use google cloud ai and amazon ai the two most popular cloud ai services on github with thousands of applications using each type of their ai services as shown in table ii.
our work will target the following two sets of applications all latest versions as of aug. 1st one for all our manual studies and one for our automated checking.
for automated checking we use allthe python applications on github that use google or aws ai service.
for manual studies we collect a suite of non trivial applications that use google amazon ml apis including applications for each of the three major ml domains.
they cover different programming languages python js java and others .
around of these applications use google cloud ai and around use aws ai with using both.
we used fewer applications that use aws ai service as aws lambda a serverless computing platform sometimes makes it difficult for us to judge the exact application workflow.
the sizes of these applications range from to millions lines of code with lines of code being the median size and around of them having more than thousand lines of code.
most of these applications are young created after of them .
they have a median age of around months at the time of our study.
this relatively young age distribution reflects the fact that the power of deep learning has only been recently recognized and yet is being adopted with unprecedented pace and breadth.
126google cloud ai aws ai ibm cloud watson microsoft azure cognitive services visionimage vision airekognitionvisual recognition s computer vision face video video ai video indexer a languagenlp cloud natural language s comprehend natural language understanding s text analytics translation cloud translation s translate s language translator translator speechrecognition speech to text transcribe a speech to text speech to text synthesis text to speech s polly text to speech s text to speech table i ml tasks supported by four popular ml cloud services.
subscript s only a synchronous api is offered for this task subscript a only an asynchronous api is offered no subscript both synchronous and asynchronous apis are offered.
all apps new apps google aws google aws visionimage 7916881842212951video languagenlp translation speechrecognition synthesis total w o duplicates table ii of applications using different types of ml apis on github.
new apps refer to those created after .
since there are many toy applications on github we manually checked about randomly selected applications which use google amazon ml apis to obtain these nontrivial applications.
we manually confirmed they each target a concrete real world problem integrate the ml api s in their workflow and conduct some processing for the input or the output of the ml api instead of simply feeding an external file into the ml api and directly printing out the api result.
we do not have a way to accurately check how seriously these applications have been used in the real world and it is possible that some of these applications have not been widely used.
b. anti pattern identification methodology because of the young ages of ml api services and hence the applications under study we could notrely on known api misuses in their issue tracking systems which are very rare.
instead we must discover api misuses unknown to the developers by ourselves.
since there is no prior study on ml api misuses our misuse discovery can not rely on any existing list of antipatterns.
instead our team including ml experts carefully studies api manuals intensively profiles the api functionality and performance and then manually examines every use of an ml api in each of the applications for potential misuses.
for every suspected misuse we design test cases and run the corresponding application or application component to see if the misuse truly leads to reduced functionality degraded performance or increased cost comparing with an alternative way of using ml apis which we designed.
when one misuse is identified we generalize it and check if there are similar misuses in other applications.
we repeat this process for many rounds until we converge to the results presented in this paper.
during this process we report representative misuses to corresponding application developers receiving confirmation for many cases.
all the manual checking is conducted by twoof the authors with their results discussed and checked by all the co authors.
we identify a wide variety of applications as containing ml api misuses including those both small and large young and old aws and google api based.
this variety of misuses indicates that they are not rare mistakes by individual programmers and do not appear to diminish with software growth age or api provider.
c. profiling methodology in section v we profile several projects to evaluate their performance before and after optimization.
we use real world vision audio or text data that fits the scenario of corresponding software.
we profile the end to end latency for each related module and also the whole process from user input to final output.
by default we run each application under profiling five times for each input and reported the average latency.
all experiments were done on the same machine which contains a core intel xeon e5 v4 cpu .20ghz 25mb l3 cache 64gb ram and 512gb ssd raid .
it has a 1000mbps network connection with twisted pair port.
note that all the machine learning inference is done by cloud apis remotely instead of on the machine locally.
iv.
f unctionality related api m isuses through manual checking we identified three main types of api misuses that commonly affect the functional correctness of applications as listed in table iii white background rows .
they are typically caused by developers misunderstanding of the semantics or the input data requirements of machine learning apis and can lead to unexpected loss of accuracy and hence software misbehavior that is difficult to diagnose.
note that although the high level patterns of these misuses such as calling the wrong api and misinterpreting the outputs naturally occur in general apis the exact root causes code anti patterns and tackling fixing strategies are all unique to ml apis as we discuss below.
a. calling the wrong api unlike traditional apis that are programmed to each conduct a clearly coded task ml apis are trained to perform tasks emulating human behaviors with functional overlap among some of them.
without a good understanding of these apis developers may call the wrong api which could lead to severely degraded prediction accuracy or even a completely wrong prediction result and software failures.
we discuss three pairs of apis that are often misused below.
127what challenges related apis and inputs service impact of problematic apps.
did developers encounter?
provider manual auto should have called a different api complicated cognitive semantic overlap across apistext detection vs. document text detection g low accuracy image classification vs. object detection ag low accuracy sentiment detection vs. entity sentiment detection g low accuracy async vs. sync language nlp a slower complicated tradeoffs input accuracy perf.
async vs. sync speech recognition g slower async vs. sync speech synthesis a slower vision image api vs. annotate image ag slower language nlp api vs. annotate text ag slower unaware of parallelism apis regular api vs batch api ag slower workload dependent should have skipped the api call complicated tradeoffs input performance speech synthesis apis with constant inputs ag slower more cost complicated tradeoffs accuracy performance vision image apis with high call frequency ag slower more cost should have converted the input format complicated data requirements all apis without input validation transformation ag exceptions complicated tradeoffs input accuracy perf.
vision image apis with high resolution inputs ag slower language nlp apis with short text inputs ag more cost complicated tradeoffs input accuracy cost speech recognition apis with short audio inputs ag more cost speech synthesis apis with short audio inputs ag more cost should have used the output in another way complicated semantics about outputs sentiment detection g low accuracy total number of benchmark applications with at least one api misuse ag table iii ml api misuses identified by our manual checking and automated checkers.
a is for aws and g for google.
the s of problematic apps are based on the total of apps using corresponding apis in respective benchmark suite.
note that apps contain more than one type of api misuses the average number of api misuses in each application is .
.
text detection anddocument text detection are both vision apis designed to extract text from images with the former trained for extracting short text and the latter for long articles.
mixing these two apis up will lead to huge accuracy loss.
our experiments using the iam ondb dataset show that text detection has about error rate in extracting hand written paragraphs and can only extract individual sentences not complete paragraphs when processing multi column pdf files yet document text detection makes almost no mistakes for these long text workloads.
this huge accuracy difference unfortunately is not clearly explained in the api documentation and is understandably not known by many developers.
in our benchmark suite applications used at least one of these two apis among which applications use the wrong api.
for example pdf to text uses text detection to process document scans which is clearly the wrong choice and makes the software almost unusable for scans with multiple columns.
image classification andobject detection are both vision apis that offer description tag s for the input image.
the former offers one tag for the whole image while the latter outputs one tag for every object in the image.
incorrectly using image classification in place ofobject detection can cause the software to miss important objects and misbehave an incorrect use along the other direction could produce a wrong image tag.
in our benchmark suite applications use at least one of these two apis among which applications pick the wrong api to use.
for example whats in your fridge is expected to leverage the in fridge camera to tell a user what products are currently inside the fridge.
however sinceit incorrectly applies image classification instead of object detection to in fridge photos it is doomed to miss most items in the fridge a severe bug that makes this software unusable.
similarly phoenix is expected to detect fire in photos and warn users but incorrectly uses image classification .
therefore it is very likely to miss flames occupying a small area.
we have reported this misuse to developers and they have confirmed this bug.
similar problems also exist in language apis.
for example sentiment detection andentity sentiment detection can both detect emotions from an input article.
however the former judges the overall emotion of the whole article while the latter infers the emotion towards every entity in the input article.
mis use between these two apis can lead to not only inaccurate but sometimes completely opposite results severely hurting the user experience.
in our benchmark suite applications used these apis among which applications use the wrong one.
summary above api mis uses form an important and new type of semantic bugs the machine learning component of software suffers unnecessary accuracy losses due to simple api use mistakes which we refer to as accuracy bugs.
accuracy bugs in general are difficult to debug as they are difficult to manifest under traditional testing and developers may easily blame the underlying dnn design without realizing their own easily fixable mistakes.
the particular accuracy bugs discussed here involve some of the most popular apis used by more than half of the applications in our suite and hence are particularly dangerous.
we reported some of these bugs to a few actively maintained applications recently and already got two bug reports confirmed by developers.
one may tackle this problem through a combination of 128response client.analyze sentiment document document encoding type encoding type sentiment response.document sentiment.score ifavg sentiment message your posts show that you might not be going through the best of time.
fig.
misinterpreting outputs in journalbot program analysis testing and dnn design support.
some of these misuses may be statically detected by checking how the api results are used if only one tag or sentiment result is used following a object detection or entity sentiment detection call there is a likely mis use.
mutation testing that targets these misuse patterns could also help we can check whether the software behaves better when replacing one api with the other.
finally it is also conceivable to extend the dnn or add a simple input classifier to check if the input differs too much from the training inputs of the underlying dnn similar to the problem of identifying out of distribution samples tackled by recent ml work .
b. misinterpreting outputs related to the probabilistic nature of cognitive tasks dnn models operate on high dimensional continuous representations yet often ultimately produce a small discrete set of outputs.
consequently ml apis outputs can contain complicated easily misinterpretable semantics leading to bugs.
a particularly common mistake concerns the sentiment detection api from google s nlp service.
this api returns two floating point numbers score andmagnitude .
among them score ranges from 1to1and indicates whether the input text s overall emotion is positive or negative magnitude ranges from 0to 1and indicates how strong the emotion is.
according to google s documentation these two numbers should be used together to judge the sentiment of the input text when the absolute value of either of them is small e.g.
score .
the sentiment should be considered neutral otherwise the sentiment is positive when score is positive and negative when score is negative.
in our benchmark suite applications have used this api among which have used the api results incorrectly .
for example a journal app journalbot figure uses this api to judge the emotion in a user s journal and displays encouraging messages when the emotion is negative.
unfortunately it considers the journal to be emotionally negative checking only that score .
this interpretation often leads to wrong results and hence unfitting messages when themagnitude is small or the score is a small negative value the emotion should be neutral even if score .
we have reported it to developers and they confirmed this bug.
summary incorrectly using ml api results can again lead to accuracy bugs that are difficult to debug.
we reportedsome of these bugs to a few actively maintained applications recently and already got three bugs confirmed by developers.
this above problem about sentiment detection can be alleviated by automatically detecting result misuse through static program analysis which we discuss in section vii.
c. missing input validation inputs to ml apis are typically real world audio image or video content.
these inputs can take many different forms with different resolutions encoding schemes and lengths.
unfortunately developers sometimes do not realize that not all forms are accepted by ml apis nor do they realize that such input incompatibility can be easily solved through format conversion input down sampling or chunking.
as a result lack of input validation and incompatibility handling are very common and can easily cause software crashes.
many ml apis have input requirements and an exception is thrown at an incompatible input.
for example the google speech recognition apis have formatting requirements i.e.
single channel using bit samples for linear pcm and size requirements 1minute for synchronous apis for audio inputs vision apis have size requirements i.e.
mb for aws and 10mb for google for image inputs.
among the benchmark applications choose to use apis that do not require input validation about one third make the effort to guarantee their input validity through input checking and transformation and yet more than half of the applications made no effort to guarantee input compatibility applications .
furthermore none of these applications handle exceptions thrown by api calls and hence can easily encounter software crashes due to incompatible inputs.
for example automatic door takes input camera images and decides to open or close a door using face verification through the aws api compare faces .
since compare faces requires the input image to be smaller than mb without any input checking and transformation this software could be completely unusable if it happens to be deployed with a high resolution camera.
summary input checking and transformation is particularly important for ml apis considering the wide variety of realworld audio and visual content and is unfortunately ignored by developers at an alarming rate out of applications severely threatening software robustness.
this problem can be alleviated by automatically detecting and warning developers of the lack of input validation or exception handling.
even better we can design a wrapper api that automatically conducts input checking and transformation e.g.
image down sampling and audio chunking which we will present in section vii.
v. p erformance related api m isuses through manual checking we identify and categorize main types of ml api mis uses that can lead to huge performance loss and user experience damage see table iii bluebackground rows .
they are typically related to ml apis complicated tradeoffs among input transformation effort performance and accuracy.
129a.
how important are performance anti patterns?
to motivate the study below we first check whether the performance of ml apis matters for software user experience.
first the latency of ml apis are significant ranging from close to one second to several minutes for typical inputs.
based on our profiling in vision tasks most apis takes .
.
seconds to process a low resolution image with 400pixels and almost one full second to process a high resolution image.
in language tasks a character input takes .
.
seconds for synchronous apis and as many as seconds for asynchronous apis.1in speech tasks a second short audio clip takes .
.
seconds with synchronous apis and .
.
seconds with asynchronous apis.
second we find that more than one third of the benchmark applications have soft latency deadlines of a couple of seconds or less with their service quality directly affected by ml apis.
many of them out of involve ml apis in their critical user interactive workflow and hence need the api result to return within a couple of seconds to maintain good software interactivity in addition some applications out of process streaming data audio video and others from a sensor and hence have to finish each api call in less than one second to avoid data loss.
even for those applications that do not have tight deadlines typically one would still hope an output to be generated in a few minutes which could still be challenging as these applications typically feed a large amount of data to ml apis.
clearly inefficient use of ml apis can cause severe damage to user experience as we will see in real examples below.
b. misuse of asynchronous apis the same ml task can often be performed with multiple apis a synchronous version an asynchronous version and sometimes a streaming version see table i .
the different versions have complicated and sometimes counter intuitive tradeoffs between input transformation performance and accuracy that often confuse developers and lead to surprisingly wide spread and severe misuses based on our study.
a common problem is related to asynchronous ml apis.
in many concurrent programs asynchronous functions are used to gain performance through improved concurrency at the cost of extra development effort.
in most ml applications the tradeoff is the opposite asynchronous ml apis are called without improved concurrency and huge performance loss in exchange for less effort in input transformation.
the benefit of asynchronous ml apis is clearly documented they allow much longer audio text inputs than synchronous apis.
for example in google speech recognition service the synchronous api takes audio up to minute long while the asynchronous api can take up to minutes .
the performance downside of asynchronous apis is unfortunately notquantitatively specified in the documentation.
1profiled with aws comprehend on three types of inputs a philosophy text a novel with conversations and a cnn news article.
2profiled with google speech to text on three different inputs a news broadcast an online lecture and a wsj audio.
data format avg std in our profiling synchronous and streaming apis are about twice as fast as asynchronous apis in google speech to text service as shown in figure .a.
the difference is even bigger for aws comprehend service i.e.
nlp .
since its multi file synchronous api has built in parallelism the speed up over asynchronous api can be as many as 400x figure .b .
making things worse most applications call asynchronous ml apis synchronously with the caller blocking itself until the api returns and no other concurrent execution on going and hence has no way to compensate for the poor performance.
among the benchmark applications using google speech recognition apis use the asynchronous api.
out of make the asynchronous call in a synchronous way.
our automated checker confirms this trend out of github applications call this asynchronous api in a synchronous way.
clearly many of these asynchronous apis could be replaced with synchronous or streaming apis with a huge performance improvement up to 400x as profiling shows .
we demonstrate these optimizations using a few benchmark examples below.
replacing with synchronous call.
answering machine applies the asynchronous speech recognition api to every voice mail and then sends specific text messages to slack accounts based on the transcript returned by the api call.
since the typical length of a voice mail is seconds it could have checked the size of every voice mail first which takes .
seconds in our profiling and then used the synchronous api for most of the voice mails with a huge speedup for a .
second voice mail the asynchronous speech to text api takes .
.
seconds and yet the synchronous api takes only .
.
seconds a huge latency improvement.
jiang jung dian is an application that automatically generates meeting reports.
it needs more than minutes i.e.
seconds to process a one minute meeting recording all numbers are averaged based on five runs .
our profiling shows that uploading the audio file and downloading the results together only take .
seconds and yet the majority of the time is spent in an asynchronous speech to text api call and then an asynchronous text comprehend api call with the latter alone taking close to minutes seconds .
if we replace it with aws synchronous multiple file comprehend api the api execution time drops from seconds down to only .
seconds more than 400x speedup!
and hence is no longer a performance bottleneck.
in fact the aws synchronous multifile comprehend api can take in documents at a time with each document containing up to characters big enough to hold the transcript of several hours meetings.
replacing with streaming call.
much real world audio content takes a streaming form and is supported by streaming apis for several audio related ml tasks like the speech recognition service in google cloud aws offers streaming apis but not for python programs .
these streaming apis can either be directly applied to a local audio file which was the setting in figure .a or to a streaming input.
they offer unique benefits for a streaming input they can start processing input and returning inference results before the whole audio finishes they support an unlimited length of a google speech to text b aws comprehend fig.
latency profiling for three different apis of google speech to text synchronous asynchronous and streaming and aws comprehend synchronous one file synchronous multi file and asynchronous .
each point in the figure corresponds to the mean and the error bar corresponds to the standard deviation of five experiments.
note that in b the y axis is broken into two parts with different value ranges.
stream input so that we do not need to worry about chunking a large file or having to make a slow asynchronous call.
unfortunately developers sometimes call non streaming apis to process streaming input causing much performance loss.
in our benchmark suite applications use google synchronous or asynchronous speech recognition apis.
among them applications are actually working on streaming inputs and can be greatly optimized by switching to the corresponding streaming api streaming recognize .
for example potty pot detects offensive language in audio streamed from a microphone.
it repeatedly records the microphone input in a second audio clip feeds it into a synchronous google speech recognition api to look for spotted words which takes around seconds and then records the next second audio clip and so on.
this can lead to severe quality of service problems either a big portion of the microphone audio will not be checked or the users have to carefully pace their speaking pausing seconds after every seconds of speaking.
instead with a streaming api the user experience will be much improved based on our profiling after speaking for seconds the user only needs to wait foran extra .
second for all the checking to finish.
as another example class scribe le records lecture audio and then calls the asynchronous api to generate lecture notes.
as a result after a two minute lecture audio is played one needs to wait for almost minutes for the notes to generate and yet only seconds if a streaming api is used which thus accomplishes most of the work during the lecture time.
summary the complicated tradeoff among synchronous asynchronous and streaming apis has clearly confused many developers.
this leads to a broad misuse of asynchronous apis as quantified in table iii and severe performance loss and user experience damage.
we could create a wrapper api that makes the choice for developers section vii .
c. forgetting parallel apis some ml apis are offered to ease task and data parallelism but are rarely used even when so would require only a simple change to the application.
forgetting task parallelism.
both google and aws offer task parallelism through easy to use apis annotateimage andannotate text .
multiple vision or nlp services can be specified as parameters of these two apis and then each service is applied to the same input in parallel.
unfortunately among the benchmark applications that apply multiple vision nlp apis towards the same input image text only of them use the annotate image annotate text api.
the majority of them completely miss this easy parallelism opportunity.
for example okuninushi a website for japanese wine database applies imageclassification and textdetection to every input image sequentially.
an easy refactoring to use annotate image offers 2x speedup.
we have reported this problem to developers and they have confirmed this bug.
forgetting data parallelism.
google and aws both offer data parallelism through easy to use batching apis which take multiple input files and process them at once.
this offers optimization opportunities for those applications with large inputs the large input can be chunked into multiple smaller pieces and get processed using a batching api.
of course this optimization depends on the specific workload and task.
first the workload should be large enough to amortize the extra input and output processing cost.
second the ml task needs to make sure that the aggregated results from input chunks are mostly the same as the original result from processing one big file.
this works for speech synthesis speech recognition entity detection and syntax analysis tasks as long as the input audio or text is carefully chunked like at the boundaries of pauses sentences or paragraphs.
for example emailclassifier downloads all the emails saved in a database and then applies the aws nlp api to detect sentiments and extract entities from every email.
we can easily chunk long emails by paragraph and then process all paragraphs in parallel using the batching api.
particularly chunking by paragraph typically has no effect to the accuracy of keyword extraction and entity recognition tasks .
the results produced by the synchronous one file api and the 131synchronous multiple files api only have very minor word difference with the latter offering a .5x speedup for a 4500character sample email .
seconds vs. .
seconds .
the total time saving for all the emails will be significant.
samaritan is another example.
it first uses a speech recognition api to get transcript from a doctor s voice message and then uses an nlp api to detect entities from the transcript.
in addition to the entity detection task discussed above the speech recognition task is also suitable for a batching optimization chunking an audio file by silence every seconds typically has minor impact on the output as speech recognition dnns usually are trained on short audio snippets e.g.
vctk dataset mostly consists of second audio clips and google audioset consists of less than second audio clips .
furthermore a doctor s voice message is often long enough to get chunked into multiple second clips which can be processed in parallel.
summary the mentioned parallelism apis are rarely used in our benchmark suite appearing in only out of the applications.
static analysis can be used to identify ml apis sequentially applied to the same input data and suggest or automate an optimization that uses annotate apis.
by dynamically checking the input size to some ml apis like speech recognition and entity detection data parallel optimization can be done by calling batch apis which we have implemented as api wrappers section vii .
d. making skippable api calls sometimes an api call can be skipped at the cost of slightly higher engineering effort or slight but often indiscernible by human functionality difference.
lack of understanding of these tradeoffs leads to some unnecessary api calls.
api calls with constant inputs.
among the benchmark applications that use the speech synthesis api of them call this api with a constant string input and thus could have replaced the api call with a pre recorded audio.
as we will see in section vii our automated checker found that this is indeed a prevalent problem in hundreds of applications.
an example is sounds of runeterra figure a card game extension that improves game accessibility to visually impaired users.
it contains multiple unnecessary calls to google speech synthesis api each generating an audio clip for one constant string e.g.
you won exiting application etc.
replacing each of them with a pre recorded audio clip can save .
seconds and associated monetary cost for each api call.
api calls with excessive frequency.
sometimes a program repeatedly invokes an image processing api at high frequency.
reducing the invocation frequency can lead to huge performance improvement with little to no perceivable output difference to human users.
among vision benchmarks of them fall into this anti pattern.
for example ns tool is a game screen monitoring application.
every second it takes a screenshot of the game and applies the text detection api to check whether the screen is locked if so it sends a message through the def stop self audio self.transform text to audio as bytes io exiting application.
deftransform text to audio as bytes io self string language code default language code voice request build voice request string language code response self.client.synthesize speech voice request.synthesis input voice request.voice config voice request.audio config fig.
skippable call sounds of runeterra internet to the user.
clearly this causes unnecessary waste of computation resources because the auto sleep duration is at least several minutes and a couple of seconds delay in sending out the reminder message would not matter to users.
as another example tags is a video scene detection application.
it applies the image classification api to analyze every frame of the input video it then splits the video into smaller pieces based on where the image classification output changes and eventually outputs the video splits and the label of each split to the user.
clearly we could apply the image classification api at a much sparser rate e.g.
once every other frame or even sparser with big performance improvement and little impact to output quality as most of the adjacent video frames are similar to each other and a miss of a couple of frames is probably un perceivable to human eyes.
summary these problems also occur with other apis as well although not as common as that for speech synthesis and vision image apis.
we reported some of the constantinput speech synthesis problems to a few actively maintained applications recently and already got three bugs confirmed.
we have built a static checker to automatically identify speech synthesis api call with a constant input section vii future research could design a dynamic controller to adjust api call frequency balancing functionality and performance.
e. unnecessarily high resolution inputs vision apis accept inputs with a range of resolutions and impose a complicated tradeoff among input performance and accuracy that is often ignored by developers with higher input resolution the performance degrades greatly while the inference accuracy increases and then saturates quickly.
this tradeoff is not explained clearly in the tutorial aws tutorial did not offer any resolution suggestion google vision apis did suggest image resolution to be x which is ignored by most developers.
to better understand this tradeoff we conducted an experiment with randomly collected high resolution images in four categories dog bufferfly scooper and wardrobe .
we down sampled each image to create more images with different resolutions as shown in figure and then feed them each into the google image classification api.
as shown in the figure the round trip api 132fig.
accuracy and latency with different input resolutions.
time increases greatly with resolution and yet the accuracy saturates at x .
a likely reason is that most vision datasets on which vision dnns are trained contain images with similar resolutions ranging from x to x .
consequently higher resolutions do not lead to higher accuracy.
note that down sampling an image takes only .
seconds on average negligible comparing with the api latency.
due to space constraints we omit the aws results here which have a similar trend.
given this tradeoff developers really should follow the tutorial suggestion in feeding relatively low resolution images e.g.
x into vision apis.
however among the applications in our benchmark suite that use visionimage apis only of them stick to this guideline by downsampling every high resolution user input.
the remaining applications all waste performance without accuracy benefit for any input that has higher than x resolution which unfortunately is the majority today.
summary without a clear understanding about the accuracy performance tradeoff most developers ignore input transformation down sampling .
a static checker could issue warnings for lack of input transformation.
a run time controller can also decide the ideal input size based on applications accuracy and performance constraints.
vi.
c ost related api m isuses every ml api call costs money.
naturally some performance problems particularly all of those skippable calls in section v d also waste money.
in addition the round up charging policy shown in table iv leads to a unique antipattern since every api call is charged based on the input size rounded up calls with very small inputs may be economically sub optimal.
the possibility of combining multiple calls with small inputs creates a complicated tradeoff problem among input transformation accuracy performance and cost.
without knowing the exact input distribution it is difficult to identify applications that fall into this anti pattern.
nevertheless our benchmark suite contains some examples.pricing unit price visionimage image .
.
per unit video minute .
.
per unit languagenlp characters .
per unit translation character per million character speechrecognition seconds per unit synthesis character per million character table iv cost of google cloud ai services.
audio sentence split takes any input audio slices it into to second audio clips based on silence in the audio feeds the clips one by one to the google speech recognition api and finally stores the resulting pairs of clip transcript into a database.
since every api call is charged based on the audio length rounded up to multiple of seconds chunking into 1or second snippets wastes money and likely hurts inference accuracy as well.
a more cost efficient implementation is to feed the whole audio into one api call and then slice the returned transcript and audio in whatever way the application sees fit the returned transcript contains information about the exact audio position matched to each word which makes chunking easy .
for example a second audio could cost around .
in the original implementation and would cost only around .
after applying the proposed fix.
summary.
the round up manner of ml api charging policy creates yet another dimension into the already complicated trade off space.
future work can extend program checkers and run time controllers to consider economical effect as well.
vii.
s olutions we have implemented checkers and wrappers to automatically detect and fix some of the anti patterns introduced in section iv vi.
the auto detection tools are implemented with jedi ast and pygithub library.
a. output misinterpretation checker we have built a static checker to automatically detect misuses of the sentiment detection api s output a type of accuracy bugs discussed in section iv b. our checker first identifies every call site of the api and then examines the data flow graph to see whether both the score field and the magnitude field of the api result are used in later execution.
our analysis is inter procedural and path sensitive.
if the result is used as a parameter of a function call we continue to check how whether the result fields are used inside the callee function if the result is returned by the current function we continue to check how whether the result fields are used in every caller function.
the tracking ends either when we have confirmed that both fields score andmagnitude have been used or when we cannot see both of them being used after checking a threshold number of caller and callee functions.
a bug is reported in the latter case.
among the github python applications that use this api our checker finds of them interpreting the api output incorrectly.
we randomly sampled detected bugs and only found one false positive an application passes the api result to an html template to render a web page which then uses both 133transcribe.start transcription job ... whiletrue status transcribe.get transcription job ... ifstatus in breaktime.sleep operation client.long running recognize config audio result operation.result google cloud speech to textawstranscribefig.
using asynchronous api in synchronously blue lines contain key code structures used by our checker result fields.
unfortunately html code analysis is currently not covered by our checker.
b. asynchronous api call checker as discussed in section v b many applications in our benchmark suite call asynchronous apis in a synchronous blocking way and hence suffer reduced performance for no benefit.
to automatically identify this problem our checker first identifies all the places where an asynchronous api is called and then the application immediately waits on the result following the common api usage patterns shown in figure .
the checker then looks for other concurrent execution.
if not this pattern is tagged as a place for performance optimization.
to accurately identify code snippets that can execute concurrently with an asynchronous api call is difficult.
our checker examines if the function fcalling the asynchronous api or the callers of f ever appears in the same python file with any multi thread and multi process related python apis in which case our checker conservatively thinks that fmay be calling the asynchronous api concurrently with other execution in the program.
otherwise this is reported as a performance problem.
our checker is applied to github python applications using google s speech to text asynchronous api and reports applications that issue at least one asynchronous call while the caller blocks to wait for the result without other concurrent execution in the program.
we manually checked reported problems and found no false positives.
being conservative our checker does have false negatives.
for example our manual checking finds that only of the remaining cases have called the asynchronous api in a concurrent way.
for python applications that use asynchronous aws nlp and speech apis our checker automatically reports applications as having this type of performance problem.
our manual checking finds no false positives out of randomly sampled problem reports.
note that our checker may have more false negatives for aws applications as a number of applications use aws lambda auto scheduler service when making the asynchronous api call which our checker conservatively assumes as having no performance problems.c.
constant parameter api call checker we have implemented a static checker to automatically identify speech synthesis api calls that use constant inputs a type of performance mis use discussed in section v d. our checker starts with every call site and tracks backward along the data dependency graph to see how the parameter of the api call is generated.
specifically the checker keeps a working set that is initialized with the parameter itself p. it first identifies all the passignments that can reach the api call site and replaces pin the working set with all the non constant variables at the right hand side of those assignments.
this back tracking continues until either the working set becomes empty in which case a constant parameter api call problem is reported or our tracking has reached our inter procedural checking threshold configured as levels of function calls in which case we consider this api call as having a variable parameter.
we applied our checker to applications on gibhub that use google s aws s python speech synthesis api.
from them our checker finds applications making the speech synthesis api calls with constant parameters.
we then manually excluded those cases where the problematic calls are inside unit tests and at the end found applications having this performance problem inside their main program.
by manually checking reported applications each from aws and google we found a total of false positives.
in case memoization is actually implemented in the other cases a library call with constant parameters can actually return non constant results which confused our checker.
overall as the number shows this is really a widespread problem in machine learning applications.
d. api wrappers we design api wrappers for all three domains of apis.
in vision tasks our wrapper down samples large images to the suggested size of pixels.
it tackles the anti patterns of missing input validation section iv c and unnecessarily high resolution inputs section v e .
in language tasks the wrapper focuses on entity detection and syntax analysis which allow input chunking with little impact to result accuracy.
our wrapper api takes in one or multiple text strings.
it first concatenates all input strings together which avoid the money wasting problem in section vi.
if the combined string is not too long a synchronous api is called if it is too long it will be chunked and get processed through batching api avoiding the anti patterns of forgetting parallel apis section v c and misuse of asynchronous apis section v b .
the wrapper for speech tasks is similar but only takes one audio as input.
the wrapper uses the synchronous api when the input size allows or streaming api otherwise.
all these wrappers conduct an input validation and in some cases also transformation section iv c .
the source code of all the checkers and wrappers is available online .
134viii.
t hreats to validity internal threats to validity.
the inputs used in our performance profiling and inference accuracy measurement may not represent the exact workload used by real world users.
our static checkers as discussed in section vii can have false positives and false negatives.
external threats to validity.
as discussed in section iii we only studied ml apis offered by google and aws in this work but not those offered by other service providers.
our study only covers cloud apis with pre trained dnns designed for general purpose use and excludes user defined dnns based on their specific needs.
we only study opensource projects on github with no access to those closedsource commercial projects.
the applications in our manual study benchmark suite may not represent all real world applications.
our static analysis tool currently only covers python applications.
ix.
r elated work prior work studies the different phases and different developer roles in large scale development and deployment of mlbased applications .
these applications design their own dnns instead of using existing ml apis.
some work studies dnn deployment challenges caused by different frameworks and platforms and how to address them using techniques like dnn compression and quantization .
some research studies common mistakes in programs that design and train neural networks or other types of machine learning models e.g.
svm and decision tree .
some works focus on testing and fixing neural networks.
all of these studies consider building machine learning models instead of using them.
another line of work focuses on the design and implementation of machine learning apis including machine learning frameworks like tensorflow and pytorch and rest apis for machine learning .
these works do not look at how ml apis are used in larger software systems.
much research has been done for designing and improving faas functions as a service platforms in terms of performance and security .
however these works focus on the server side instead of the client side.
some works also examine the performance of enterprise faas platforms to help developers select service providers.
other works aim to help developers move local computation to the cloud.
these works improve application performance using general faas apis but do not address the unique challenges for ml apis.
x. c onclusion cloud based machine learning apis have become a popular approach for developers to leverage machine learning inference in software.
this paper conducts a comprehensive study to understand the challenges in using these machine learning apis.
by investigating the latest versions of open source applications using google and aws ml cloud apis we have found types of common api misuses that cause functionality performance and service cost problems.
we also develop static checkers to automatically detect some of these problems in a larger set of applications.
the wide presence of these problems motivates future research to further tackle ml api misuses.
data availability we have released our whole benchmark suite automated checkers and detailed study results online .
acknowledgement we thank the reviewers for their insightful feedback.
the authors research is supported by nsf grants ccf cns ccf cns cns iis cns aro grant w911nf1920321 doe grant desc0014195 darpa grant fa8750 the ceres center for unstoppable computing and the marian and stuart rice research award.