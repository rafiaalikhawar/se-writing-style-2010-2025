inductive program synthesis over noisy data shivam handa electrical engineering and computer science massachusetts institute of technology u.s.a. shivam mit.edumartin c. rinard electrical engineering and computer science massachusetts institute of technology u.s.a. rinard csail.mit.edu abstract we present a new framework and associated synthesis algorithms for program synthesis over noisy data i.e.
data that may contain incorrect corrupted input output examples.
this framework is based on an extension of finite tree automata called state weighted finite tree automata.
we show how to apply this framework to formulate and solve a variety of program synthesis problems over noisy data.
results from our implemented system running on problems from the sygus benchmark suite highlight its ability to successfully synthesize programs in the face of noisy data sets including the ability to synthesize a correct program even when every input output example in the data set is corrupted.
ccs concepts theory of computation formal languages and automata theory software and its engineering programming by example computing methodologies machine learning.
keywords program synthesis noisy data corrupted data machine learning acm reference format shivam handa and martin c. rinard.
.
inductive program synthesis over noisy data.
in proceedings of the 28th acm joint european software engineering conference and symposium on the foundations of software engineering esec fse november virtual event usa.
acm new york ny usa 12pages.
introduction in recent years there has been significant interest in learning programs from input output examples.
these techniques have been successfully used to synthesize programs for domains such as string and format transformations data wrangling data completion and data structure manipulation .
even though these efforts have been largely successful they do not aspire to work with noisy data sets that may contain corrupted input output examples.
we present a new program synthesis technique that is designed to work with noisy corrupted data sets.
given programs a collection of programs pdefined by a grammar g and a bounded scope threshold d esec fse november virtual event usa copyright held by the owner author s .
acm isbn .
set a data setd 1 o1 .
.
.
n on of input output examples loss function a loss functionl p d that measures the cost of the input output examples on which pproduces a different output than the output in the data set d complexity measure a complexity measure c p that measures the complexity of a given program p objective function an arbitrary objective function u l c which maps loss land complexity cto a totally ordered set such that for all values of l u l c is monotonically nondecreasing with respect to c our program synthesis technique produces a program pthat minimizes u l p d c p .
example problems that our program synthesis technique can solve include best fit program synthesis given a potentially noisy data setd find a best fit program pford i.e.
a pthat minimizes l p d .
accuracy vs. complexity tradeoff given a data setd find pthat minimizes the weighted sum l p d c p .
this problem enables the programmer to define and minimize a weighted tradeoff between the complexity of the program and the loss.
data cleaning and correction given a data setd find pthat minimizes the loss l p d .
input output examples with nonzero loss are identified as corrupted and either filtered out or replaced with the output from the synthesized program.
bayesian program synthesis given a data setdand a probability distribution p over programs p find the most probable program pgivend.
best program for given accuracy given a data setdand a bound b find pthat minimizes c p subject tol p d b. one use case finds the simplest program that agrees with the data set don at least n binput output examples.
forced accuracy given data setsd d whered d find pthat minimizes the weighted sum l p d c p subject to l p d b. one use case finds a program pwhich minimizes the loss over the data set dbut is always correct for d .
approximate program synthesis given a clean noise free data setd find the least complex program pthat minimizes the lossl p d .
here the goal is not to work with a noisy data set but instead to find the best approximate solution to a synthesis problem when an exact solution does not exist within the collection of considered programs p. .
noise models and discrete noise sources we work with noise models that assume a hidden clean data set combined with a noise source that delivers the noisy data set presented to the program synthesis system.
like many inductive program synthesis systems one target is discrete problems 87this work is licensed under a creative commons attribution international .
license.
esec fse november virtual event usa shivam handa and martin c. rinard that involve discrete data such as strings data structures or tablular data.
in contrast to traditional machine learning problems which often involve continuous noise sources the noise sources for discrete problems often involve discrete noise noise that involves a discrete change that affects only part of each output leaving the remaining parts intact and uncorrupted.
.
loss functions and use cases different loss functions can be appropriate for different noise sources and use cases.
the 1loss function which counts the number of input output examples where the data set dand synthesized program pdo not agree is a general loss function that can be appropriate when the focus is to maximize the number of inputs for which the synthesized program pproduces the correct output.
the damerau levenshtein dl loss function which measures the edit difference under character insertions deletions substitutions and or transpositions extracts information present in partially corrupted outputs and can be appropriate for measuring discrete noise in input output examples involving text strings.
the loss function which is unless pagrees with all of the input output examples in the data set d specializes our technique to the standard program synthesis scenario that requires the synthesized program to agree with all input output examples.
because discrete noise sources often leave parts of corrupted outputs intact exact program synthesis i.e.
synthesizing a program that agrees with all outputs in the hidden clean data set is often possible even when all outputs in the data set are corrupted.
our experimental results section indicate that matching the loss function to the characteristics of the discrete noise source can enable very accurate program synthesis even when there are only a handful of input output examples in the data and all of the outputs in the data set are corrupted.
we attribute this success to the ability of our synthesis technique working in conjunction with an appropriately designed loss function to effectively extract information from outputs corrupted by discrete noise sources.
.
technique our technique augments finite tree automata fta to associate accepting states with weights that capture the loss for the output associated with each accepting state.
given a data set d the resulting state weighted finite tree automata sfta partition the programs pdefined by the grammar ginto equivalence classes.
each equivalence class consists of all programs with identical input output behavior on the inputs in d. all programs in a given equivalence class therefore have the same loss over d. the technique then uses dynamic programming to find the minimum complexity program p in each equivalence class .
from this set of minimum complexity programs the technique then finds the program pthat minimizes the objective function u p d .
.
experimental results we have implemented our technique and applied it to various programs in the sygus benchmark set .
the results indicate that the technique is effective at solving program synthesis problems over strings with modestly sized solutions even in the presence of substantial noise.
for discrete noise sources and a loss functionthat is a good match for the noise source the technique is typically able to extract enough information left intact in corrupted outputs to synthesize a correct program even when all outputs are corrupted in this paper we consider a synthesized program to be correct if it agrees with all input output examples in the original hidden clean data set .
even with the loss function which does not aspire to extract any information from corrupted outputs the technique is typically able to synthesize a correct program even with only a few correct uncorrupted input output examples in the data set.
overall the results highlight the potential for effective program synthesis even in the presence of substantial noise.
.
contributions this paper makes the following contributions technique it presents an implemented technique for inductive program synthesis over noisy data.
the technique uses an extension of finite tree automata state weighted finite tree automata to synthesize programs that minimize an objective function involving the loss over the input data set and the complexity of the synthesized program.
use cases it presents multiple uses cases including best fit program synthesis for noisy data sets navigating accuracy vs. complexity tradeoffs bayesian program synthesis identifying and correcting corrupted data and approximate program synthesis.
experimental results it presents experimental results from our implemented system on the sygus benchmark set.
these results characterize the scalability of the technique and highlight interactions between the dsl the noise source the loss function and the overall effectiveness of the synthesis technique.
in particular they highlight the ability of the technique to given a close match between the noise source and the loss function synthesize a correct program peven when there are only a handful of input output examples in the data set dand all outputs are corrupted.
preliminaries we next review finite tree automata fta and fta based inductive program synthesis.
.
finite tree automata finite tree automata are a type of state machine which accept trees rather than strings.
they generalize standard finite automata to describe a regular language over trees.
definition fta .a bottom up finite tree automaton fta over alphabet fis a tuplea q f qf where qis a set of states qf qis the set of accepting states and is a set of transitions of the form f q1 .
.
.
qk qwhere q q1 .
.
.qkare states f f. every symbol fin alphabet fhas an associated arity.
the set fk fis the set of all k arity symbols in f. arity terms tinf are viewed as single node trees leaves of trees .
tis accepted by an fta if we can rewrite tto some state q qfusing rules in .
the language of an ftaa denoted byl a corresponds to the set of all ground terms accepted by a. example .
consider the tree automaton adefined by states q qt qf f0 true false f1 not f2 and final states 88inductive program synthesis over noisy data esec fse november virtual event usa andtrue not false figure tree for formula and true not false jck c constant jxk x variable jn1k v1jn2k v2.
.
.
jnkk vk jf n1 n2 .
.
.nk k f v1 v2 .
.
.vk function figure execution semantics for program p qf qt and the following transition rules true qt false qf not qt qf not qf qt and qt qt qtand qf qt qf and qt qf qfand qf qf qf or qt qt qt or qf qt qt or qt qf qt or qf qf qf the above tree automaton accepts all propositional logic formulas over true andfalse which evaluate to true.
figure presents the tree for the formula and true not false .
.
domain specific languages dsls we next define the programs we consider how inputs to the program are specified and the program semantics.
without loss of generality we assume programs pare specified as parse trees in a domain specific language dsl grammar g. internal nodes represent function invocations leaves are constants arity symbols in the dsl.
a program pexecutes in an input .jpk denotes the output of pon input j.kis defined in figure .
all valid programs which can be executed are defined by a dsl grammar g t n p s0 where tis a set of terminal symbols.
these may include constants and symbols which may change value depending on the input .
nis the set of nonterminals that represent subexpressions in our dsl.
pis the set of production rules of the form s f s1 .
.
.
sn where fis a built in function in the dsl and s s1 .
.
.
snare non terminals in the grammar.
s0 nis the start non terminal in the grammar.
we assume that we are given a black box implementation of each built in function fin the dsl.
in general all techniques explored within this paper can be generalized to any dsl which can be specified within the above framework.
example .
the following dsl defines expressions over input x constants and and addition and multiplication n x n t n t t t t jtk c qc t q term qos0 q qos0 qf final s f s1 .
.
.sk p qc1s1 .
.
.
qcksk q jf c1 .
.
.ck k c qcs q f qc1s1 .
.
.
qcksk qcs prod figure rules for constructing a cfta a q f qf given input output o and grammar g t n p s0 .
.
concrete finite tree automata we review the approach introduced by to use finite tree automata to solve synthesis tasks over a broad class of dsls.
given a dsl and a set of input output examples a concrete finite tree automaton cfta is a tree automaton which accepts all trees representing dsl programs consistent with the input output examples and nothing else.
the states of the fta correspond to concrete values and the transitions are obtained using the semantics of the dsl constructs.
given an input output example o and dsl g j.k construct a cfta using the rules in figure .
the alphabet of the cfta contains built in functions within the dsl.
the states in the cfta are of the form qcs where sis a symbol terminal or non terminal ingandcis a concrete value.
the existence of state qcsimplies that there exists a partial program which can map to concrete value c. similarly the existence of transition f qc1s1 qc2s2.
.
.qcksk qcs implies f c1 c2.
.
.ck c. theterm rule states that if we have a terminal t either a constant in our language or input symbol x execute it with the input and construct a state qc t where c jtk .
the final rule states that given start symbol s0and we expect oas the output if qos0exists then we have an accepting state.
the prod rule states that if we have a production rule f s1 s2 .
.
.sk s and there exists states qc1s1 qc2s2.
.
.qcksk q then we also have state qcsin the cfta and a transition f qc1s1 qc2s2 .
.
.qcksk qcs.
the language of the cfta constructed from figure is exactly the set of parse trees of dsl programs that are consistent with our input output example i.e.
maps input to output o .
in general the rules in figure may result in a cfta which has infinitely many states.
to control the size of the resulting cfta we do not add a new state within the constructed cfta if the smallest tree it will accept is larger than a given threshold d. this results in a cfta which accepts all programs which are consistent with the input output example but are smaller than the given threshold it may accept some programs which are larger than the given threshold but it will never accept a program which is inconsistent with the input output example .
this is standard practice in the synthesis literature .
.
intersection of two cftas given two cftas a1anda2built over the same grammar g from input output examples 1 o1 and 2 o2 respectively the intersection of these two automata acontains programs which satisfy both input output examples or has the empty language .
given cftasa q f qf anda q f q f a 89esec fse november virtual event usa shivam handa and martin c. rinard q f q f is the intersection of aanda where q q f and are the smallest set such that q c1s qandq c2s q then q c1 c2s q q c1s qfandq c2s q fthen q c1 c2s q f f q c1s1 .
.
.q cksk q c s andf q c 1s1 .
.
.q c ksk q c s then f q c1 c 1s1 .
.
.q ck c ksk q c c s where cdenotes a vector of values and c1 c2denote a vector constructed by appending vector v2at the end of vector v1.
loss functions given a data setd 1 o1 .
.
.
n on and a program p a loss functionl p d calculates how incorrect the program is with respect to the given data set.
we work with loss functions l p d that depend only on the data set and the outputs of the program for the inputs in the data set i.e.
given programs p1 p2 such that for all i oi d jp1k i jp2k i thenl p1 d l p2 d .
we also further assume that the loss function l p d can be expressed in the following form l p d nx i 1l oi jpk i where l oi jpk i is a per example loss function.
definition .
1loss function the0 1loss function l0 p d counts the number of input output examples where pdoes not agree with the data set d l0 p d nx i if oi jpk i else definition .
loss function the0 loss function l0 p d is if pmatches all outputs in the data set dand otherwise l0 p d if o d .o jpk else definition .
damerau levenshtein dl loss function the dl loss function ldl p d uses the damerau levenshtein metric to measure the distance between the output from the synthesized program and the corresponding output in the noisy data set ldl p d x i oi dljpk i oi jpk i oi where la b i j is the damerau levenshtein metric .
this metric counts the number of single character deletions insertions substitutions or transpositions required to convert one text string into another.
because more than of all human misspellings are reported to be captured by a single one of these four operations the dl loss function may be appropriate for computations that work with human provided text input output examples.
complexity measure given a program p acomplexity measure c p ranks programs independent of the input output examples in the data set d. this measure is used to trade off performance on the noisy data set vs. complexity of the synthesized program.
formally a complexity measure is a function c p that maps each program pexpressible in the given dsl gto a real number.
the following cost p complexity measure computes the complexity of given program prepresented as a parse tree recursively as follows cost t cost t cost f e1 e2 .
.
.ek cost f kp i 1cost ei where tand fare terminals and built in functions in our dsl respectively.
setting cost t cost f 1delivers a complexity measure size p that computes the size of p. given an ftaa we can use dynamic programming to find the minimum complexity parse tree under the above cost p measure accepted bya .
in general given an fta a we assume we are provided with a method to find the program paccepted byawhich minimizes the complexity measure.
objective functions given loss land complexity c anobjective function u l c maps l cto a totally ordered set such that for all l u l c is monotonically nondecreasing with respect to c. definition .
tradeoff objective function given a tradeoff parameter the tradeoff objective function u l c l c. this objective function trades the loss of the synthesized program off against the complexity of the synthesized program.
similarly to how regularization can prevent a machine learning model from overfitting noisy data by biasing the training algorithm to pick a simpler model the tradeoff objective function may prevent the algorithm from synthesizing a program which overfits the data by biasing it to pick a simpler program based on the complexity measure .
definition .
lexicographic objective function a lexicographic objective function ul l c l c maps landcinto a lexicographically ordered space i.e.
l1 c1 l1 c2 if and only if either l1 l2orl1 l2andc1 c2.
this objective function first minimizes the loss then the complexity.
it may be appropriate for example for best fit program synthesis data cleaning and correction and approximate program synthesis over clean data sets.
state weighted fta state weighted finite tree automata sfta are fta augmented with a weight function that attaches a weight to all accepting states.
definition sfta .a state weighted finite tree automaton sfta over alphabet fis a tuplea q f qf w where qis a set of states qf qis the set of accepting states is a set of transitions of the form f q1 .
.
.
qk qwhere q q1 .
.
.qkare states f fandw qf ris a function which assigns a weight w q from domain w to each accepting state q qf.
90inductive program synthesis over noisy data esec fse november virtual event usa t t jtk c qc t q term qcs0 q qcs0 qf w qcs0 l o c final s f s1 .
.
.sk p qc1s1 .
.
.
qcksk q jf c1 .
.
.ck k c qcs q f qc1s1 .
.
.
qcksk qcs prod figure rules for constructing a sfta a q f qf w given input per example loss function l and grammar g t n p s0 .
because cftas are designed to handle synthesis over clean noise free data sets they have only one accept state qos0 the state with start symbol s0and output value o .
we weaken this condition to allow multiple accept states with attached weights using sftas.
given an input output example o and per example loss function l o c figure presents rules for constructing a sfta that given a program p returns the loss for pon example o .
the sfta final rule figure marks all states qcs0with start symbol s0as accepting states regardless of the concrete value cattached to the state.
the rule also associates the loss l o c for concrete value cand output owith state qcs0as the weight w qcs0 l o c .
the cfta final rule figure in contrast marks only the state qos0 with output value oand start state s0 as the accepting state.
a sfta divides the set of programs in the dsl into subsets.
given an input all programs in a subset produce the same output based on the accepting state with the sfta assigning a weight w qcs0 l o c as the weight of this subset of programs.
we denote the sfta constructed from dsl g example o perexample loss function l and threshold dasad g o l .
we omit the subscript grammar gand threshold dwherever it is obvious from context.
example .
consider the dsl presented in example .
given inputoutput example x and weight function l c c figure presents the sfta which represents all programs of height less than .
for readability we omit the states for terminals 2and3.
for all accepting states the first number the number in black represents the computed value and the second number the number in red represents the weight of the accepting state.
.
operations over sftas definition intersection .given two sftas a1 q1 f q1 f w1 anda2 q2 f q2 f w2 a sfta a q f qf w is the intersectiona1anda2 if the cfta inais the intersection of cftas of a1anda2 and the weight of accept states inais the sum of weight of corresponding weights in a1anda2.
formally the cfta q f qf is the intersection of cftas q1 f q1 f and q2 f q2 f w q c1 c2s w1 q c1s w2 q c2s forq c1 c2s qf .
x id figure the sfta constructed for example given two sftasa1anda2 a1 a2denotes the intersection ofa1anda2.
definition intersection .given a sfta a q f qf w and a cftaa q f q f a sfta a q f q f w is the intersection ofaanda if the ftaa is the intersection of cfta aanda and the weight of the accepting state ina is the same as the weight of the corresponding accepting state ina.
formally the cfta q f q f is the intersection of ftas q f qf and q f q f w q c1 c2s w q c1 c2s forq c1 c2s q f .
given a sftaaand a cftaa a a denotes the intersection ofaanda .
given a single input output example a cfta built on that example only accepts programs which are consistent with that example.
intersection is a simple method to prune a sfta to only contain programs which are consistent with an input output example.
definition w0 pruned sfta .a sfta a q f q f w is the w0 pruned sfta of a q f qf w if we remove all accept states with weights greater w0from qf.
formally q f q q qf w0 w q and w q w q ifq q f. given a sftaa a w0denotes the w0 pruned sfta ofa.
definition q selection .given a sfta a q f qf w and a accept state q qf the cfta q f q is called the q selection of sfta a. given a sftaa the notationaqdenotes the q selection of sfta a.
91esec fse november virtual event usa shivam handa and martin c. rinard synthesis over noisy data given a data set 1 o1 .
.
.
n on of input output examples and loss function l p d with per example loss function l we construct sftas for each input output example a1 a2 .
.
.an whereai a i oi l .
theorem .
given a sftaa a o l q f qf w for all accepting states q qfand for all programs paccepted by the q selection automata aq l o jpk w q proof.
consider any state q qf.
all programs accepted by state qcompute the same concrete value con the given input .
hence for all programs accepted by the q selection automata aq jpk c. by construction figure w q l c l jpk let sftaa d l be the intersection of sftas defined on input output examples in d. formally a d l a 1 o1 l a 2 o2 l .
.
.a n on l since the size of each sfta a i oi l is bounded the cost of computinga d l iso d .
theorem .
givena d l q f qf w as defined above for all accepting states q qf for all programs paccepted by the q selection automata a d l q l p d w q i.e.
the weight of the state qmeasures the loss of programs by qon data setd.
proof.
consider any accepting state q qf.
sincea d l is an intersection of sftas a1...an whereai a i oi l qi f qf i i wi there exist accepting states q1 qf q2 qf .
.
.qn qf nsuch that all programs p accepted byaqare accepted by a1 q1 a2 q2.
.
.
an qn.
from theorem for all programs paccepted byaq wi qi l oi jpk i from definition of intersection w q nx i 1wi qi nx i 1l oi jpk i l p d algorithm synthesis algorithm input dslg threshold d data setd per example loss function l complexity measure c and objective function u result synthesized program p a d l q f qf w the sfta over data set dand per example loss function l foreach q qfdo pq argminp a d l qc p for each accepting state q find the most optimal program pq end q argminq qfu w q c pq p pq algorithm presents the base algorithm to synthesize programs within various noisy synthesis settings.
theorem .
the program p returned by algorithm is equal to p where p argminp gdu l p d c p proof.
givena d l q f qf w .p returned by algorithm is equal to pq where q argminq qfu w q c pq where pq argminp a d l qc p .
we can rewrite q as argminq qfu w q min p a d l qc p since for any l u l c is non decreasing with respect to c we can rewrite q as argminq qfmin p a d l qu w q c p by theorem for any p a d l q w q l p d q argminq qfmin p a d l qu l p d c p because q is the accepting state of p andp a d l if and only if q qf.p a d l q we can rewrite the above equation as p argminp a d l u l p d c p the set of programs accepted by a d l is the same set of programs in grammar gd.
hence proved.
we next present several modifications of the core algorithm to solve various synthesis problems.
.
accuracy vs. complexity tradeoff given a dsl g a data setd loss functionl complexity measure c and positive weight we wish to find a program p which minimizes the weighed sum of the loss function and the complexity measure.
formally p argminp gd l p d c p where gdis the set of programs in dsl gwith size less than the threshold d. by using the objective function u l c l c we can use algorithm to synthesize program p which minimizes the objective function given above.
.
best program for given accuracy given a dsl g a data setd loss functionl complexity measure cand bound b we wish to find a program p that minimizes the complexity measure cbut has loss less than b. formally p argminp gdc p s.t.l p d b. note that this condition can be rewritten as p argminp a c p wherea a d l b. by the definition of b all accepting states of a have weight less than b. therefore all programs accepted by a have loss less than b i.e.l p d b .
also note that if a program pis not ina then either it has loss greater than bor it is not within the threshold d. 92inductive program synthesis over noisy data esec fse november virtual event usa .
forced accuracy given dsl g a data setd a subsetd d loss functionl complexity measure c and objective function u we wish to find a program p which minimizes the objective function with an added constraint of bounded loss over data set d .
formally p argminp gdu l p d c p s.t.l p d b we first construct a sfta a d l bwhich contains all programs consistent with loss less than or equal to bover data setd .
after constructing a d l as in algorithm we modify a d l by intersectiona d l after dropping the weights of the accepting states with a d l i.e.a d l a d l a d l as in algorithm .
by definition of intersection anda loss of all programs returned by the modified algorithm on d will be less than equal to b. use cases definition .
bayesian program synthesis given a set of input output examples d i oi i .
.
.n dsl grammar g and a probability distribution p is the solution to the bayesian program synthesis problem if p is the most probable program in dsl g given the data set d. formally p argmaxp g p d .
by bayes rule p argmaxp g d p p so p argmaxp gf log d p log p g assuming independence of observations p argmaxp gdf x i oi dlog oi jpk i log p g where oi jpk i denotes the probability of output observation oiin the data setd given a program pwith complexity measure log p per example loss function log oi jpk i given example i oi and the following loss function l p d x i oi dlog oi jpk i the technique in section .
algorithm synthesizes the most probable program p at most kwrong consider a setting in which given a data set a random procedure is allowed to corrupt at most kof these inputoutput examples.
given this noisy data set d our task is to synthesize the simplest program p which is wrong on at most kof these input output examples.
formally given data set d bound k dsl g and a complexity measure c p argminp gc p s.t.l0 p d k wherel0 1is the loss function.
the best program for a given accuracy framework subsection .
allows us to synthesize p subject to a threshold d. experimental results string transformations have been extensively studied within the programming by example community .
we implemented our technique in 6k lines of java code and used it to solve benchmark program synthesis problems from the sygus benchmark suite .
this benchmark suite contains a range of string transformation problems a class of problems that has been extensively studied in past program synthesis projects .string expr e str f concat f e substring expr f conststr s substr x p1 p2 position p pos x k d constpos k direction d start end figure dsl for string transformation is a token kis an integer and sis a string constant we use the dsl from figure with the size complexity measure size p .
the dsl supports extracting and contatenating concat substrings of the input string x each substring is extracted using the substr function with a start and end position.
a position can either be a constant index constpos or the start or end of the kthoccurrence of the match token in the input string pos .
.
implementation optimizations instead of computing individual sftas for each input output example then combining the sftas via intersections to obtain the final sfta our implementation computes the final sfta directly working over the full data set.
the implementation also applies two techniques that constrain the size of the final sfta.
first it bounds the number of recursive applications of the production e concat f e by applying a bounded scope height threshold d. second during construction of the sfta a state with symbol eis only added to the sfta if the length of the state s output value is not greater than the length of the output string plus one.
.
scalability we evaluate the scalability of our implementation by applying it to all problems in the sygus benchmark suite .
for each problem we use the clean noise free data set for the problem provided with the benchmark suite.
we use the lexicographic objective function ul l c with the loss function and the c size p complexity measure.
we run each benchmark with bounded scope height threshold d and and record the running time on that benchmark problem and the number of states in the sfta.
a state with symbol eis only added to the sfta if the length of its output value is not greater than the length of the output string.
because the running time of our technique does not depend on the specific utility function except for the time required to evaluate the utility function which is typically negligible for most utility functions and except for search space pruning techniques appropriate for specific combinations of utility functions and dsls we anticipate that these results will generalize to other utility functions.
all experiments are run on an .
ghz intel r xeon r cpu e52690 v2 machine with 512gb memory running linux .
.
.
with a timeout limit of minutes and bounded scope height threshold of the implementation is able to solve out of the sygus benchmark problems.
for the remaining benchmark problems a correct program does not exist within the dsl at bounded scope height threshold .
table presents results for selected sygus benchmarks.
we omit all long repeat benchmarks.
we also omit all name combine4 phone phone phone phone and univ benchmarks all runs for these benchmarks either do not synthesize a correct program or do not terminate.
the full paper presents results for all sygus benchmarks.1there is a row for each benchmark 1the full paper is available at 93esec fse november virtual event usa shivam handa and martin c. rinard threshold benchmark name time sec sfta size time sec sfta size time sec sfta size time sec sfta size bikes .
.
.
.
.
.
.
.
bikes long .
.
.
.
.
.
.
.
bikes short .
.
.
.
.
.
.
.
dr name x x .
.
.
.
dr name long x x .
.
.
.
dr name short x x .
.
.
.
firstname .
.
.
.
.
.
.
.
firstname long .
.
.
.
.
.
.
.
firstname short .
.
.
.
.
.
.
.
initials x x x x .
.
.
.
initials long x x x x .
.
.
.
initials short x x x x .
.
.
.
lastname .
.
.
.
.
.
.
.
lastname long .
.
.
.
.
.
.
.
lastname short .
.
.
.
.
.
.
.
name combine x x .
.
.
.
name combine long x x .
.
name combine short x x .
.
.
.
name combine x x x x .
.
name combine long x x x x .
.
name combine short x x x x .
.
name combine x x x x .
.
.
.
name combine long x x x x .
.
name combine short x x x x .
.
.
.
reverse name x x .
.
.
.
reverse name long x x .
.
reverse name short x x .
.
.
.
phone .
.
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
phone .
.
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
phone .
.
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
phone .
.
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
phone .
.
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
phone .
.
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
phone .
.
.
.
.
.
.
phone long .
.
.
.
.
.
.
.
phone short .
.
.
.
.
.
.
.
figure runtimes and sfta sizes for selected sygus benchmarks 94inductive program synthesis over noisy data esec fse november virtual event usa benchmark data setnumber of required sizecorrect examples delete dl bikes dr name firstname lastname initials reverse name name combine name combine name combine phone phone phone phone phone phone phone figure minimum number of correct examples required to synthesize correct a program.
problem.
the first column presents the name of the benchmark.
the next four columns present results for the technique running with bounded scope height threshold d and respectively.
each column has two subcolumns the first presents the running time on that benchmark problem in seconds the second presents the number of states in the sfta in thousands of states .
an entry x indicates that the implementation terminated but did not synthesize a correct program that agreed with all provided input output examples.
an entry indicates that the implementation did not terminate.
in general both the running times and the number of states in the sfta increase as the number of provided input output examples and or the bounded height threshold increases.
the sfta size sometimes stops increasing as the height threshold increases.
we attribute this phenomenon to the application of a search space pruning technique that terminates the recursive application of the production e concat f e when the generated string becomes longer than the current output string in this case any resulting synthesized program will produce an output that does not match the output in the data set.
we compare with a previous technique that uses ftas to solve program synthesis problems .
this previous technique requires clean data and only synthesizes programs that agree with all inputoutput examples in the data set.
our technique builds sftas with similar structure with additional overhead coming from the evaluation of the objective function.
we obtained the implementation of the technique presented in and ran this implementation on all benchmarks in the sygus benchmark suite.
the running times of our implementation and this previous implementation are comparable.
.
noisy data sets character deletion we next present results for our implementation running on small few input output examples data sets with character deletions.
we use a noise source that cyclically deletes a single character fromeach output in the data set in turn starting with the first character proceeding through the output positions then wrapping around to the first character again.
we apply this noise source to corrupt every output in the data set.
to construct a noisy data set with k correct uncorrupted outputs we do not apply the noise source to the last koutputs in the data set.
we exclude all long long repeat and short benchmarks and all benchmarks that do not terminate within the time limit at height bound .
for each remaining benchmark we use our implementation and the generated corrupted data sets to determine the minimum number of correct outputs in the corrupted data set required for the implementation to produce a correct program that matches the original hidden clean data set on all input output examples.
we consider three loss functions the 1and dl loss functions section and the following delete loss function which is designed to work with noise sources that delete a single character from the output stream definition .
delete loss function the delete loss functionl1d p d uses the per example loss function lthat is if the outputs from the synthesized program and the data set match exactly if a single deletion enables the output from the synthesized program to match the output from the data set and otherwise l1d p d x i oi dl1d jpk i oi where l1d o1 o2 0o1 o2 1a x b o1 a b o2 x otherwise we use the lexicographic objective function ul l c with c size p as the complexity measure and bounded scope height thresholdd .
we apply a search space pruning technique that terminates the recursive application of the production e concat f e when the generated string becomes more than one character longer than the current output string.
table summarizes the results.
the data set size column presents the total number of input output examples in the corrupted data set.
the next three columns delete dl and present the minimum number of correct uncorrupted input output examples required for the technique to synthesize a correct program that agrees with the original hidden clean data set on all input output examples using the delete dl and loss functions respectively.
with the delete loss function the minimum number of required correct input output examples is always the implementation synthesizes for every benchmark problem a correct program that matches every input output example in the original clean data set even when given a data set in which every output is corrupted.
this result highlights how discrete noise sources produce noisy outputs that leave a significant amount of information from the original uncorrupted output available in the corrupted output and a loss function that matches the noise source can enable the synthesis technique to exploit this information to produce correct programs even in the face of substantial noise.
with the dl loss function the implementation synthesizes a correct program for of the benchmarks when all outputs in the data set are corrupted.
for of the remaining benchmarks the 95esec fse november virtual event usa shivam handa and martin c. rinard technique requires correct input output examples to synthesize the correct program.
the remaining benchmark requires correct examples.
the general pattern is that the technique tends to require correct examples when the output strings are short.
the synthesized incorrect programs typically use less of the input string.
these results highlight how the dl loss function still extracts significant useful information available in outputs corrupted with discrete noise sources.
but in comparison with the delete loss function the dl loss function is not as good a match for the character deletion noise source.
the result is that the synthesis technique when working with the dl loss function works better with longer inputs sometimes encounters incorrect programs that fit the corrupted data better and therefore sometimes requires correct inputs to synthesize the correct program.
with the loss function the technique always requires at least one and usually more correct inputs to synthesize the correct program.
in contrast to the delete and dl loss functions the loss function does not extract information from corrupted outputs.
to synthesize a correct program with the loss function in this scenario the technique must effectively ignore the corrupted outputs to synthesize the program working only with information from the correct outputs.
it therefore always requires at least one and usually more correct outputs before it can synthesize the correct program.
.
noisy data sets character replacements we next present results for our implementation running on larger data sets with character replacements.
the phone long repeat benchmarks within the sygus benchmarks contain transformations over phone numbers.
the data sets for these benchmarks contain input output examples including repeated input output examples.
for each of these phone long repeat benchmark problems on which our technique terminates with bounded scope height threshold section .
we construct a noisy data set as follows.
for each digit in each output string in the data set we flip a biased coin.
with probability b we replace the digit with a uniformly chosen random digit so that each digit in the noisy output is not equal to the original digit in the uncorrupted output with probability b .
we then run our implementation on each benchmark problem with the noisy data set using the tradeoff objective function u l c l cwith complexity measure c size p and the following n substitution loss function definition .
n substitution loss function then substitution loss function lns p d uses the per example loss function lnsthat counts the number of positions where the noisy output does not agree with the output from the synthesized program.
if the synthesized program produces an output that is longer or shorter than the output in the noisy data set the loss function is lns p d x i oi dlns jpk i oi where lns o1 o2 o1 o2 o1 p i 11ifo1 o2 else0 o1 o2 benchmark data set dl output program size loss size size name combine phone phone phone phone figure approximate program synthesis with dl loss.
we run the implementation for all combinations of the bounded scope threshold b .
.
.
and .
.
.
for every combination of band and for every one of the phone longrepeat benchmarks in the sygus benchmark set the implementation synthesizes a correct program that produces the same outputs as in the original hidden clean data set.
these results highlight once again the ability of our technique to work with loss functions that match the characteristics of discrete noise sources to synthesize correct programs even in the face of substantial noise.
.
approximate program synthesis for the benchmarks in table a correct program does not exist within the dsl at bounded scope threshold .
table presents results from our implementation on the clean noise free benchmark data sets with the dl loss function size p complexity measure lexicographic objective function ul ldl p d size p and bounded scope threshold .
the first column presents the name of the benchmark.
the next four columns present the number of input output examples in the benchmark data set the dl loss incurred by the synthesized program over the entire data set the sum of the lengths of the output strings of the data set the dl loss for an empty output would be this sum and the size of the synthesized program.
for the phone benchmarks a correct program outputs the entire input telephone number but changes the punctutation for example by including an area code in parentheses.
the synthesized approximate programs correctly preserve the telephone number but apply only some of the punctuation changes.
the result is 7characters incorrect per output for all but phone which has character per output incorrect.
each output is between 7and 7characters long.
for name combine the synthesized approximate program correctly extracts the last name inserts a comma and a period but does not extract the initial of the first name.
these results highlight the ability of our technique to approximate a correct program when the correct program does not exist in the program search space.
.
discussion practical applicabilty the experimental results show that our technique is effective at solving string manipulation program synthesis problems with modestly sized solutions like those present in the sygus benchmarks.
more specifically the results highlight how the combination of structure from the dsl a discrete noise source that preserves some information even in corrupted outputs and a good match between the loss function and noise source can enable very effective synthesis for data data sets with only a handful of input output examples even in the presence of substantial noise.
even with as generic a loss function as the loss function the technique is effective at dealing with data sets 96inductive program synthesis over noisy data esec fse november virtual event usa in which a significant fraction of the outputs are corrupted.
we anticipate that these results will generalize to similar classes of program synthesis problems with modestly sized solutions within a tractable and focused class of computations.
we note that our current implementation does not scale to sygus benchmarks with larger solutions.
these benchmarks were designed to test the scalability of current and future program synthesis systems.
no currently extant program analysis system of which we are aware can solve these larger problems.
to the extent that the sygus bencharks accurately represent the kinds of program synthesis problems that will be encountered in practice our results provide encouraging evidence that our technique can help program synthesis systems work effectively with noisy data sets.
important future work in this area will more fully investigate interactions between the dsl the noise source the loss function the classes of synthesis problems that occur in practice and the scalability of the synthesis technique.
a full evaluation of the immediate practical applicability of program synthesis for noisy data sets as well as a meaningful evaluation of program synthesis more generally awaits this future work.
noise sources with different characteristics our experiments largely consider discrete noise sources that preserve some information in corrupted outputs.
the results highlight how loss functions like the delete dl and n substitution loss functions can enable our technique to extract and exploit this preserved information to enhance the effectiveness of the synthesis.
the question may arise how well may our technique perform with noise sources that leave little or even no information intact in corrupted outputs?
here the results from the 1loss function which does not aspire to extract any information from corrupted inputs may be relevant if the corrupted outputs considered together do not conform to a target computation in the dsl the technique will in effect ignore these corrupted outputs to synthesize the program based on any remaining uncorrupted outputs.
a final possibility is that the noise source may systematically produce outputs characteristic of a valid but incorrect computation.
here we would expect the algorithm to require a balance of correct outputs before it would be able to synthesize the correct program.
related work the problem of learning programs from a set of input output examples has been studied extensively .
these techniques can be largely broken down into the following four categories synthesis using solvers these systems require the user to provide precise semantics for the operators for the dsl they are using .
our technique in contrast only requires black box implementations of these operators.
a large class of these systems depend on solvers which do not scale as the number of examples increases.
since our techniques are based on tree automata our cost linearly increases as the number of examples increase.
these systems require all input output examples to be correct and only synthesize programs that match all input output examples.
enumerative techniques these techniques search the space of programs to find a single program that is consistent with the given examples .
specifically they enumerate all programs in the given dsl and terminate when they find the correct program.
these techniques may apply different heuristics techniques to prune thesearch space speed up this process .
these techniques require all input output examples to be correct and only synthesize programs that match all input output examples.
vsa based tree automata based techniques these techniques build complex data structures representing all possible programs compatible with the given examples .
current work requires users to provide correct input output examples.
our work modifies these techniques to handle noisy data and to synthesize approximate programs that minimize an objective function over the provided potentially noisy data set.
neural program synthesis ml approaches there is extensive work that uses machine learning deep neural networks to synthesize programs .
these techniques require a training phase and a differentable loss function.
our technique requires no training phase and can work with arbitrary loss functions including for example the 1loss function.
machine learning techniques are incompatible with this type of loss function.
these systems provide no guarantees over the completeness and the optimality of their result whereas our technique due to its property of exploring all programs of size less than a threshold always finds a program within the bounded scope that minimizes the objective function.
data set sampling or cleaning there has been recent work which aspires to clean the data set or pick representative examples from the data set for synthesis for example by using machine learning or data cleaning to select productive subsets of the data set over which to perform exact synthesis.
in contrast to these techniques our proposed techniques provide deterministic guarantees as opposed to either probabilistic guarantees as in or no guarantees at all as in do not require the use of oracles as in can operate successfully even on data sets in which most or even all of the input output examples are corrupted and do not require the explicit selection of a subset of the data set to drive the synthesis as in .
active learning recent research exploits the availability of a reference implementation to use active learning for program synthesis .
our technique in contrast works directly from given input output examples with no reference implementation.
conclusion dealing with noisy data is a pervasive problem in modern computing environments.
previous program synthesis systems target data sets in which all input output examples are correct to synthesize programs that match all input output examples in the data set.
we present a new program synthesis technique for working with noisy data and or performing approximate program synthesis.
using state weighted finite tree automata this technique supports the formulation and solution of a variety of new program synthesis problems involving noisy data and or approximate program synthesis.
the results highlight how this technique by exploiting information from a variety of sources structure from the underlying dsl information left intact by discrete noise sources can deliver effective program synthesis even in the presence of substantial noise.