DeepRover: A Query-Efficient Blackbox Attack
for Deep Neural Networks
Fuyuan Zhangâˆ—
Kyushu University, Japan
fuyuanzhang@163.comXinwen Huâˆ—
Hunan Normal University, China
huxinwen@hunnu.edu.cn
Lei Ma
The University of Tokyo, Japan
University of Alberta, Canada
ma.lei@acm.orgJianjun Zhao
Kyushu University, Japan
zhao@ait.kyushu-u.ac.jp
ABSTRACT
Deep neural networks (DNNs) achieved a significant performance
breakthrough over the past decade and have been widely adopted
in various industrial domains. However, a fundamental problem
regarding DNN robustness is still not adequately addressed, which
can potentially lead to many quality issues after deployment, e.g.,
safety, security, and reliability. An adversarial attack is one of the
most commonly investigated techniques to penetrate a DNN by
misleading the DNNâ€™s decision through the generation of minor
perturbations in the original inputs. More importantly, the adver-
sarial attack is a crucial way to assess, estimate, and understand the
robustness boundary of a DNN. Intuitively, a stronger adversarial
attack can help obtain a tighter robustness boundary, allowing us
to understand the potential worst-case scenario when a DNN is
deployed. To push this further, in this paper, we propose DeepRover ,
a fuzzing-based blackbox attack for deep neural networks used for
image classification. We show that DeepRover is more effective and
query-efficient in generating adversarial examples than state-of-
the-art blackbox attacks. Moreover, DeepRover can find adversarial
examples at a finer-grained level than other approaches.
CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks ;â€¢Software
and its engineering â†’Software testing and debugging .
KEYWORDS
Adversarial Attacks, Deep Neural Networks, Blackbox Fuzzing
ACM Reference Format:
Fuyuan Zhang, Xinwen Hu, Lei Ma, and Jianjun Zhao. 2023. DeepRover : A
Query-Efficient Blackbox Attack for Deep Neural Networks. In Proceedings
of the 31st ACM Joint European Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering (ESEC/FSE â€™23), December
âˆ—Corresponding authors: Fuyuan Zhang, Xinwen Hu
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12. . . $15.00
https://doi.org/10.1145/3611643.36163703â€“9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3611643.3616370
1 INTRODUCTION
Deep neural networks (DNNs) have achieved tremendous success
over the past decades, attaining competitive performance in many
industrial applications and domains, e.g., image classification, ob-
ject detection, and speech recognition. Recently, there has been
a growing trend in deploying DNNs in practical applications and
integrating them into traditional software systems, e.g., unmanned
vehicles. However, the robustness issues of a DNN [ 44] still posts se-
vere concerns and threats, especially in safety- and security-critical
scenarios. Extensive research has been conducted in developing
adversarial attacks aimed at identifying adversarial examples that
can mislead the predictions of a DNN. Importantly, these attacks
provide a means to assess the adversarial robustness of DNNs. A
more effective attack technique could yield a tighter boundary of ro-
bustness, enhancing our understanding of the potential limitations
and risks of a DNN after deployment.
The early stage research in this direction primarily focused on
developing white-box attacks [ 5,12,24,28,30,35]. In the whitebox
setting, attackers can have full access to the internals of DNNs
to find the robustness issues through the feedback of DNNâ€™s in-
ternal behaviors. Various whitebox techniques are proposed and
shown to help detect adversarial examples with high attack suc-
cess rates. However, in typical real-world application scenarios,
the internals of a DNN usually can not be fully accessed, making
the obtained robustness boundary to be misestimated in the op-
erational environment. Therefore, recent research has shifted to
focusing on the blackbox setting [ 1,2,4,7â€“9,15,18,19,29,52],
where attackers cannot access network structures and parameters
of DNNs. The blackbox setting is more realistic as the internal in-
formation of DNNs can not be fully accessed in real-world settings.
Consequently, blackbox attacks are constructed mainly via query-
ing DNNs for their prediction results. As query numbers are viewed
as the main costs of attacks, it has been a de facto standard to assess
the performance of blackbox attacks in query-limited settings.
The main challenge in designing query-efficient blackbox adver-
sarial attacks is to achieve a high attack success rate with a low
number of queries. One is motivated to perturb input images along
gradient descent directions to maintain a high attack success rate.
However, estimating gradient directions in the blackbox setting
costs a large number of queries [ 3,6]. In our previous work, we have
1384
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Fuyuan Zhang, Xinwen Hu, Lei Ma, and Jianjun Zhao
proposed DeepSearch [52], a simple and effective blackbox attack
based on feedback-directed fuzzing. Although conceptually simple,
DeepSearch outperforms state-of-the-art blackbox attacks in both
attack success rate and query efficiency. However, DeepSearch can
only generate adversarial perturbations constrained by the ğ¿âˆdis-
tance metric. Developing query-efficient ğ¿2blackbox attack remains
an open and challenging problem.
In this paper, we develop a fuzzing-based blackbox attack to
better evaluate and understand the adversarial robustness of neu-
ral networks w.r.t the ğ¿2distance metric. We focus on improving
three aspects of blackbox attacks: effectiveness, subtlety, and query
efficiency. Attacks with a high attack success rate can find adversar-
ial examples challenging to be detected by other approaches, thus
increasing the possibility of exposing the severe vulnerability of
neural networks under adversarial perturbations. More subtle at-
tacks can reveal adversarial examples closer to decision boundaries,
which allows a better approximation of the robustness boundaries
of neural networks. Moreover, techniques with a high query effi-
ciency can be further used to better evaluate the cost of constructing
attacks in real-world settings.
Our approach. We propose DeepRover , an effective and query-
efficient blackbox attack for neural networks used for image classi-
fication. The attack of DeepRover is constrained by the ğ¿2distance
metric, constructed via querying a target network for its predic-
tion scores. DeepRover is developed based on feedback-directed
fuzzing. We introduce the design of DeepRover in the following
three aspects of blackbox attacks:
(1)Effectiveness : To achieve a high success rate, DeepRover in-
serts well-designed mutation vectors to input images to push
them towards decision boundaries. Our experiments show
that even defended neural networks are vulnerable to such
perturbations. Consequently, DeepRover is very effective in
generating adversarial examples.
(2)Subtlety : To construct adversarial examples close to decision
boundaries, we extend DeepRover with a refinement step,
adapted from [ 52], and reduce the ğ¿2distance of adversar-
ial examples once they are found. Our experiments show
that the refinement step of DeepRover is indeed effective in
generating subtle adversarial examples.
(3)Query efficiency : To improve query efficiency, we utilize spa-
tial regularities in images and group mutation vectors in
blocks of pixels. We find in our experiments that our group-
ing strategy actually enhanced both the effectiveness and
query efficiency of DeepRover .
We compare DeepRover with two state-of-the-art blackbox ad-
versarial attacks in the score-based blackbox threat model. Attack-
ers have a limited query budget and can only query neural networks
for their prediction scores. We use three well-known datasets in
our evaluation, namely SVHN [ 31], CIFAR- 10[23], and ImageNet
[37]. For SVHN and CIFAR- 10, we evaluate all attacks on both de-
fended and undefended neural networks. The defended networks
are trained with a state-of-the-art adversarial defense [ 54]. Our
experimental results demonstrate that DeepRover has the highest
attack success rate and is more query-efficient than other state-of-
the-art attacks.Contributions. Our contributions are fourfold:
(1)We present a novel fuzzing-based blackbox adversarial attack
for deep neural networks.
(2)We show that DeepRover is more effective than state-of-the-
art blackbox attacks in generating adversarial examples.
(3)We show that the refinement step in DeepRover effectively
reduces the distortion of generated adversarial examples.
(4)We show that DeepRover is query-efficient in finding adver-
sarial examples. Our strategy of grouping mutation vectors
in blocks improves the query efficiency of DeepRover .
Outline. We briefly introduce the relevant background in the
next section. In Sect. 3, we present DeepRover , a novel fuzzing-
based blackbox adversarial attack for deep neural networks. Sect. 4
extends DeepRover with iterative refinement, which can further
reduce the distortion of generated adversarial examples. Our exper-
imental evaluations are presented in Sect. 5. After discussing the
related work in Sect. 6, Sect. 7 concludes and discusses the future
work.
2 BACKGROUND
We use column vectors x=(ğ‘¥1,...,ğ‘¥ğ‘›)ğ‘‡onğ‘›-dimensional vector
space Rğ‘›to represent images. Each coordinate ğ‘¥ğ‘–âˆˆR(1â‰¤ğ‘–â‰¤ğ‘›)
represents an image pixel. Assume that ğ¿ğ‘š={ğ‘™1,...,ğ‘™ğ‘š}are labels
forğ‘šclasses. A deep neural network that classifies input images
intoğ‘šclasses can be represented as a function N:Rğ‘›â†’ğ¿ğ‘š. For
an input image x, the label thatNassigns to xisN(x).
Assume that xis a correctly classified input image and xâ€²is
derived by adding subtle mutations to x. We consider xâ€²as an
adversarial example [44] ifN(x)â‰ N(xâ€²)andxâ€²is very close to x
according to some distance metric. We use the ğ¿2distance metric
in this paper to measure the distance of adversarial examples.
Theğ¿2distance between xandxâ€²is the standard Euclidean
distance between them:
||xâˆ’xâ€²||2=vtğ‘›âˆ‘ï¸
ğ‘–=1(ğ‘¥ğ‘–âˆ’ğ‘¥â€²
ğ‘–)2
For an input xand distance ğ‘‘âˆˆR, the set of input images within
ğ‘‘from xforms anğ‘›-dimensional sphere, and we write S(x,ğ‘‘)to
denote the sphere, i.e., S(x,ğ‘‘)={xâ€²|||xâˆ’xâ€²||2â‰¤ğ‘‘}. A neural
networkNisrobust at input xfor distance ğ‘‘if all images inS(x,ğ‘‘)
are classified into the same class.
Query-limited score-based blackbox attacks. We evaluate
blackbox adversarial attacks in the score-based query-limited set-
ting. We assume that 1)attackers cannot access any internal in-
formation of target neural networks but can learn their prediction
scores, and 2)attackers can only query target neural networks for
a limited time. Hence, the setting of blackbox attacks considered in
this paper is summarized as follows. Given a query budget ğµ, input
xand distance ğ‘‘, attackers are motivated to generate an adversarial
example inS(x,ğ‘‘)by querying target neural networks for their
prediction scores in at most ğµqueries.
1385DeepRover: A Query-Efficient Blackbox Attack for Deep Neural Networks ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
x0x2Dl1S(x0,d)x1x3
Figure 1: DeepRover for DNN classifers
3 FUZZING DEEP NEURAL NETWORKS
In this section, we introduce the technical details of DeepRover to
generate adversarial examples via fuzzing deep neural networks.
3.1 Deep Neural Networks
Adeep neural network classifier (DNN classifier) classifies input
images according to the following definition (from [52]).
Definition 1 (DNN Classifier). Given a classification function
ğ‘“:Rğ‘›â†’Rğ‘š, adeep neural network classifier Nğ‘“:Rğ‘›â†’ğ¿ğ‘šthat
classifies inputs into ğ‘šclassesğ¿ğ‘š={ğ‘™1,...,ğ‘™ğ‘š}is defined as
Nğ‘“(x)=ğ‘™ğ‘—, iffğ‘—=arg maxğ‘–ğ‘“ğ‘–(x),
whereğ‘“ğ‘–:Rğ‘›â†’Ris the evaluation of ğ‘“on theğ‘–th class, i.e.,
ğ‘“(x)=(ğ‘“1(x),...,ğ‘“ğ‘š(x))ğ‘‡.
Intuitively, a DNN classifier Nğ‘“classifies xinto classğ‘™ğ‘—ifğ‘“ğ‘—(x)
is the maximum value among all values ğ‘“ğ‘–(x)for1â‰¤ğ‘–â‰¤ğ‘š.
The decision boundaries ofNğ‘“divide input images into different
classes. For example, the surface of the region Rğ‘™ğ‘–={x|âˆ€1â‰¤
ğ‘—â‰¤ğ‘š,ğ‘—â‰ ğ‘–:ğ‘“ğ‘–(x)âˆ’ğ‘“ğ‘—(x)>0}forms the decision boundaries
ofNğ‘“for classğ‘™ğ‘–, which we denote as Dğ‘™ğ‘–. All images thatNğ‘“
classifies into class ğ‘™ğ‘–are within regionRğ‘™ğ‘–. For an input xâˆˆRğ‘™ğ‘–,
finding adversarial examples within distance ğ‘‘from xis equivalent
to finding an input images in S(x,ğ‘‘)that are across the decision
boundaryDğ‘™ğ‘–.
For example, Fig. 1 shows the decision boundary Dğ‘™1, denoted as
a curved line, of a DNN classifier. Images on the left side of Dğ‘™1are
classified into class ğ‘™1, and images on the other side are classified
into other classes. Assume that input x0is correctly classified and
that we use the dash-dotted circle to represent S(x0,ğ‘‘). Then, input
x1andx2are not adversarial as they are still on the left side of Dğ‘™1.
However, x3is an adversarial example as it has crossed Dğ‘™1.
Objective functions. Assume that xis classified into class ğ‘™ğ‘–. To
find adversarial examples for x, we define the following objective
function and use DeepRover to iteratively minimize the value of
Obj(x)withinS(x,ğ‘‘).
Obj(x)=ğ‘“ğ‘–(x)âˆ’max
ğ‘—â‰ ğ‘–ğ‘“ğ‘—(x)
An alternative choice for defining objective function is Obj(x)=
ğ‘“ğ‘–(x). Intuitively, ğ‘“ğ‘–(x)is the score value for xto be classified toclassğ‘™ğ‘–, and decreasing ğ‘“ğ‘–(x)would implicitly increase the scores
for other classes.
3.2 Mutation Vectors for ğ¿2Attacks
A critical step in our approach is to add well-constructed mutation
vectors , suitable for ğ¿2adversarial attacks, to input images. Deep-
Rover constructs mutation vectors iteratively and adds them to
input images if they can decrease the objective functions.
Intuitively, a mutation vector aims to mutate a small-sized square
block of pixels of input images. The center of the block has the
largest mutation value, and the mutation value decreases as it moves
away from the center. Assume that ğ‘is a square block of pixels of
an image. For ease of presentation, we use ğ‘¥ğ‘–,ğ‘—to represent pixels in
ğ‘, whereğ‘–andğ‘—are coordinates of ğ‘¥ğ‘–,ğ‘—on the images. A mutation
vector added to block ğ‘can be specified by a function that maps
each pixel of ğ‘to a mutation value:
mv(ğ‘¥ğ‘–,ğ‘—)=(
ğ‘1 ifğ‘¥ğ‘–,ğ‘—=ğ‘¥ğ‘–ğ‘,ğ‘—ğ‘ğ‘2
(ğ‘–âˆ’ğ‘–ğ‘)2+(ğ‘—âˆ’ğ‘—ğ‘)2 otherwise,
whereğ‘¥ğ‘–ğ‘,ğ‘—ğ‘is the center pixel on ğ‘, andğ‘1,ğ‘2are two parameters
that control the mutation value added by the mutation vector. In
practice, we have that ğ‘1â‰¥ğ‘2, which guarantees that mv(ğ‘¥ğ‘–ğ‘,ğ‘—ğ‘)â‰¥
mv(ğ‘¥ğ‘–,ğ‘—)forğ‘¥ğ‘–,ğ‘—â‰ ğ‘¥ğ‘–ğ‘,ğ‘—ğ‘.
To improve query efficiency, we exploit spatial regularities in
images and add multiple mutation vectors to improve query effi-
ciency. Instead of adding one mutation vector to an image per query,
DeepRover actually adds multiple mutation vectors simultaneously
to a square block of pixels in each query. Fig. 2a shows a simple
example for illustration. Assume that ğµ1represents a random block
of pixels of an image. Two mutation vectors (represented as blue
squares) are added to random locations in block ğµ1.
3.3 Fuzzing Deep Neural Network Classifiers
DeepRover is an iterative algorithm consisting of two major steps:
initialization and feedback-directed fuzzing . We introduce some
notations below and then explain the two steps in detail.
Assume that ğµrepresents a square block of ğ‘šÃ—ğ‘šneighboring
pixels of an image x. We use xğµto denote the pixel values of ğµin
x, which is a vector that only contains coordinates of pixels in ğµ.
We write x[ğµâ†¦â†’r]for a new image derived by assigning vector
rto corresponding pixels in ğµ. We use Projğ‘‘(x,xâ€²)to denote the
input image derived by projecting xâ€²to the boundary of the sphere
S(x,ğ‘‘). We omit the definition of Projğ‘‘(x,xâ€²)as projecting an
ğ‘›-dimensional vector onto the surface of an ğ‘›-dimensional sphere
is standard.
Initialization. In the initialization step, DeepRover first gen-
erates a number of randomly selected blocks of pixels of an input
image. Then it adds mutation vectors to each of the blocks, which
could potentially decrease the value of objective functions. The re-
sulting image is then projected to the boundary of a corresponding
ğ¿2sphere to ensure that it is still within a predefined ğ¿2distance.
If the derived image is not adversarial, DeepRover will start its
fuzzing step introduced below.
1386ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Fuyuan Zhang, Xinwen Hu, Lei Ma, and Jianjun Zhao
B1B3B2
(a) Initialization
B1B2B3B5B4 (b) Fuzzing
Figure 2: A simple example of initialization and fuzzing.
Feedback-directed fuzzing. In the fuzzing step, DeepRover
iteratively mutates randomly selected blocks of input image pixels
to further decrease the value of objective functions. In each iteration,
DeepRover aims to increase adversarial perturbation on one block
of pixels on the input image. Since the ğ¿2distance for perturbation
is fixed, to remain within the ğ¿2sphere, increasing the mutation
distance on one block of pixels entails decreasing mutations on
other pixels. Consequently, DeepRover performs two operations in
each iteration: mutation insertion andmutation deletion .
In the mutation insertion step, DeepRover inserts multiple muta-
tion vectors on a randomly selected block of pixels ğµ. These vectors
are added at random locations within ğµ. Moreover, we further re-
quire that all inserted mutation vectors point in the same direction,
i.e., either they all increase pixel values or decrease pixel values.
Otherwise, closely located vectors (pointing in opposite directions)
have a chance to cancel the perturbation of each other.
In the mutation deletion step, DeepRover erases existing muta-
tions added to a number of randomly generated small-sized blocks
of pixels on input images. This step aims to reduce the ğ¿2distance
of the existing perturbation so that DeepRover can utilize the re-
duced perturbation distance to add mutation vectors in the mutation
insertion step.
If the resulting image derived by applying the above insertion
and deletion steps can decrease the value of object functions, we
keep all the mutations computed in the current iteration and start
the fuzzing step from the resulting image in the next iteration. Oth-
erwise, we discard the mutations derived in the current iteration.
Example. We give a simple example to illustrate how DeepRover
finds adversarial example for input x0in Fig. 1. DeepRover first
uses its initialization step to generate x1as follows.
DeepRover first randomly selects three blocks of pixels ğµ1,ğµ2
andğµ3on input x0shown in Fig. 2a. Then, DeepRover adds two
mutation vectors at random locations within each block. The added
mutation vectors are represented by blue-colored squares within
ğµ1,ğµ2andğµ3in Fig. 2a. After adding these mutation vectors to
x0, we derive a new image xâ€²
0. Notice that it is possible that the
ğ¿2distance between x0andxâ€²
0is greater than ğ‘‘. To constrain the
distance of initial perturbation, we project xâ€²
0to the boundary of
S(x0,ğ‘‘)to derive an image x1=Projğ‘‘(x0,xâ€²
0), which finishes the
initialization step. Since x1is not across the decision boundary, x1
is not an adversarial example. DeepRover then starts its fuzzing
step from x1as follows.Algorithm 1: DeepRover for DNN classifiers.
Input: input xâˆˆRğ‘›, functionğ‘“:Rğ‘›â†’Rğ‘š, distanceğ‘‘âˆˆR
Output: xâ€²âˆˆS( x,ğ‘‘)
1Function Initialize( x,ğ‘‘)is
2 generateğ‘ blocksB={ğµ1,...,ğµğ‘ }of sizeğ‘¡Ã—ğ‘¡
3 xâ€²=x
4 foreachğµğ‘–âˆˆBdo
5 rğ‘–:=Gen-MutVec( ğµğ‘–,bvn,vs)
6 xâ€²:=xâ€²[ğµğ‘–â†¦â†’xâ€²
ğµğ‘–+rğ‘–]
7 xâ€²:=Projğ‘‘(x,xâ€²)
8 return xâ€²
9
10Function DeepRover( x,ğ‘“,ğ‘‘ )is
11 x0:=xandğ‘˜:=0
12 repeat
13ğµ,ğ‘ğ‘ :=Gen-Block(k, x)
14ğ‘£ğ‘›:=densityÃ—(ğ‘ğ‘Ã—ğ‘ +1)
15 r:=Gen-MutVec(B,vn,vs)
16 xâ€²
ğ‘˜:=Remv-MutVec( xğ‘˜,B)
17 râ€²:=r+xâ€²
ğ‘˜[ğµ]
||xâ€²
ğ‘˜[ğµ]||2
18 râ€²â€²:=||xğ‘˜âˆ’xâ€²
ğ‘˜[ğµâ†¦â†’x[ğµ]]|| 2râ€²
||râ€²||2
19 xâ€²â€²
ğ‘˜:=Projğ‘‘(x,xâ€²
ğ‘˜[ğµâ†¦â†’râ€²â€²])
20 ifObj(xâ€²â€²
ğ‘˜)<Obj(xğ‘˜)then
21 xğ‘˜+1:=xâ€²â€²
ğ‘˜
22 else
23 xğ‘˜+1:=xğ‘˜
24ğ‘˜:=ğ‘˜+1
25 untilNğ‘“(x)â‰ Nğ‘“(xğ‘˜), or query limit is reached
26 return xğ‘˜
First, DeepRover randomly selects a block of pixels ğµ1from x1
shown in Fig. 2b. To create extra perturbation distance for muta-
tion vectors to be added to block ğµ1,DeepRover then randomly
generates four blocks of pixels ğµ2,ğµ3,ğµ4andğµ5and erases ex-
isting mutations on those blocks. Finally, using the perturbation
distance derived from blocks ğµ2,ğµ3,ğµ4andğµ5,DeepRover adds
three mutation vectors (represented as three blue squares) to ğµ1.
After projecting the resulting image to the boundary of S(x0,ğ‘‘),
we derive x2. Since x2is still not adversarial, DeepRover repeats
the above fuzzing process to push further x2towards the decision
boundary. As a result, DeepRover generates image x3, which is an
adversarial example.
DeepRover for DNN classifiers. Alg. 1 shows the DeepRover
algorithm for fuzzing DNN classifiers. We explain the details of the
algorithm as follows.
Function Initialize describes the initialization step of Deep-
Rover . We first generate ğ‘ initial blocks ğµ1,...,ğµğ‘ (line 2), where
each blockğµğ‘–(1â‰¤ğ‘–â‰¤ğ‘ ) contains indices corresponding to a ğ‘¡Ã—ğ‘¡
block of pixels in x. Here, the number of ğ‘ and block size ğ‘¡are
parameters of Alg. 1. Then, function Gen-MutVec generates corre-
sponding mutation vectors for each block ğµğ‘–(line 5). Bothğ‘ğ‘£ğ‘›and
ğ‘£ğ‘ are parameters. Variable ğ‘ğ‘£ğ‘›defines the number of mutation
vectors to be added to block ğµğ‘–. For instance, in the example in
Fig. 2a, variable ğ‘ğ‘£ğ‘›equals to two as DeepRover adds two mutation
1387DeepRover: A Query-Efficient Blackbox Attack for Deep Neural Networks ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
vectors to each block in the initialization step. Variable ğ‘£ğ‘ defines
the size of mutation vectors, i.e., each generated mutation vector
mutates ağ‘£ğ‘ Ã—ğ‘£ğ‘ block of pixels. After adding mutation vectors
rğ‘–to each block ğµğ‘–(line 6) ofx, we rescale the resulting image by
projecting it to the boundary of S(x,ğ‘‘)(line 7).
Function DeepRover summarizes the fuzzing step of DeepRover .
It first assigns to x0an initial image x(line 11), which is supposed
to be the input image after initial perturbation. Then, we generate
a random block ğµusing function Gen-Block (line 13). Here,ğ‘ğ‘
returned by Gen-Block is the percentage of pixels of xcontained in
blockğµ. We generate mutation vectors to block ğµby calling function
Gen-MutVec (line 15), and the derived vector rhas been normalized
to have length 1. Notice that the variable ğ‘£ğ‘›, which defines the
number of mutation vectors for block ğµ, is computed in line 14,
where variable ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ andğ‘ are both parameters. Variable ğ‘ refers
to the number of initial blocks generated in the initialization step.
For instance, in the example in Fig. 2a, variable ğ‘ equals to three as
DeepRover generates three initial blocks in the initialization step.
Variableğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ defines the least number of mutation vectors for
blockğµ. Intuitively, ğ‘ğ‘Ã—ğ‘ roughly estimates how many initial blocks
could have been added to an area as large as block ğµ. The value of
ğ‘£ğ‘›changes when the size of ğµchanges. We use the computation in
line14to control the number of mutation vectors added to images
in each iteration.
The mutation deletion is done by function Remv-MutVec in line
16, and the resulting image is xâ€²
ğ‘˜. We merge the mutation vectors
inrwith the existing perturbation on block ğµofxâ€²
ğ‘˜and derive
the combined mutation râ€²forğµin line 17. In line 18, we compute
the available mutation distance for râ€², rescale it to an appropriate
length, and derive râ€²â€². Finally, we add vector râ€²â€²to blockğµand
project the resulting image to the boundary of S(x,ğ‘‘)(line 19). If
the newly computed image xâ€²â€²
ğ‘˜(after projection) can reduce the
value of objective functions, we use image xâ€²â€²
ğ‘˜for the next iteration
(line 20-23). The iteration continues until DeepRover either finds
an adversarial example or has reached the query limit.
4 ITERATIVE REFINEMENT
Once an adversarial example is found, we are motivated to reduce
its distance from the original image, thus lowering the distortion
of generated adversarial examples. In this section, we adapt the
refinement approach in [ 52] to our setting and extend DeepRover
with an iterative refinement step to generate adversarial examples
with low distortions.
The main idea of our refinement is that we iteratively project
adversarial examples to the boundaries of smaller ğ¿2spheres as long
as the images derived after projection are still adversarial. Once
an adversarial example reaches the decision boundaries of neural
networks, we search for other adversarial examples at the same
distance, i.e., within the same ğ¿2sphere, from which we would
continue our projection step. We illustrate the idea of iterative
refinement through the following example.
Example. Fig. 3 shows the decision boundary Dğ‘™2of a deep
neural network classifier. Images below the decision boundary are
classified into class ğ‘™2, and images above it are classified into other
classes. Assume that input x0is correctly classified into class ğ‘™2and
DeepRover has already found an adversarial example x1within
x2
S(x0,d1)x1x0x3
S(x0,d2)S(x0,d3)x4Dl2Figure 3: DeepRover with iterative refinement
S(x0,ğ‘‘1). Our technique allows finding adversarial examples with
smallerğ¿2distance via iterative refinement starting from input x1.
First, we compute via bisect search the smallest distance ğ‘‘such
that the input Projğ‘‘(x0,x1)is still an adversarial example. Once
such a distance is found, we can generate an adversarial example
closer to x0by projecting x1toS(x0,ğ‘‘). Assume that ğ‘‘2is such a
distance that we find, and as a result, we generate an adversarial
input x2=Projğ‘‘2(x0,x1). Compared to x1, input x2has a lower
distortion as it is closer to x0.
Second, we apply DeepRover to search for other adversarial
examples withinS(x0,ğ‘‘2). The motivation here is to find another
adversarial example such that projecting it onto S(x0,ğ‘‘â€²)for some
ğ‘‘â€²<ğ‘‘2would result in a new adversarial example closer to x0.
Assume that we find x3byDeepRover .
Then, we repeat the bisect search in the first step to find the small-
est distance ğ‘‘such that Projğ‘‘(x0,x3)is still adversarial. Assume
that we find distance ğ‘‘3this time and we derive x4by projecting x3
toS(x0,ğ‘‘3). Assume that DeepRover fails to find another adversar-
ial example inS(x0,ğ‘‘3). As a result, we return x4as the adversarial
example with the lowest distortion.
DeepRover with iterative refinement. The description of
DeepRover with iterative refinement is given in Alg. 2. We give a
brief explanation as follows.
Each iteration starts from a project step (line 3-4). We first com-
pute theğ¿2distance between xandxâ€²(line 3), where xrefers to
the original input image and xâ€²is an adversarial example generated
so far. Then, we use bisect search to find the smallest distance ğ‘‘â€²
such that projecting xâ€²to the boundary of S(x,ğ‘‘â€²)still gives us
an adversarial example (line 4). Since Projğ‘‘â€²(x,xâ€²)is already the
closest adversarial example derived by projection, to continue the
project step in the next iteration, we call DeepRover again (line 5)
to find another adversarial example xâ€²â€²for projection.
On line 6, we check whether the input xâ€²â€²found by DeepRover
withinğ‘‘â€²is adversarial. We start the next iteration if xâ€²â€²is indeed a
new adversarial example. Otherwise, we return the adversarial ex-
ample derived through projection, i.e., Projğ‘‘â€²(x,xâ€²), and terminate
the algorithm.
1388ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Fuyuan Zhang, Xinwen Hu, Lei Ma, and Jianjun Zhao
Algorithm 2: DeepRover with iterative refinement.
Input: input xâˆˆRğ‘›, adversarial input xâ€²âˆˆS( x,ğ‘‘)(ğ‘‘âˆˆR),
functionğ‘“:Rğ‘›â†’Rğ‘š
Output: an adversarial input xâ€²â€²âˆˆS( x,ğ‘‘â€²)(ğ‘‘â€²â‰¤ğ‘‘)
1Function DR-Refinement( x,xâ€²,ğ‘“)is
2 repeat
3ğ‘‘:=||xâˆ’xâ€²||2
4 find the minimum distance ğ‘‘â€²â‰¤ğ‘‘via bisect search such
that input Projğ‘‘â€²(x,xâ€²)is an adversarial example.
5 xâ€²â€²:=DeepRover( x,ğ‘“,ğ‘‘â€²)
6 if xâ€²â€²is an adversarial example then
7 xâ€²:=xâ€²â€²
8 else
9 xâ€²:=Projğ‘‘â€²(x,xâ€²)
10 until xâ€²â€²is not an adversarial example
11 return xâ€²andğ‘‘â€²
5 EXPERIMENTAL EVALUATION
We evaluate the performance of DeepRover1on testing the adver-
sarial robustness of both defended and undefended neural networks.
We compare DeepRover with state-of-the-art blackbox attacks on
neural networks for three well-known datasets. We design our
experiments to answer the following research questions:
RQ1: Whether DeepRover is an effective approach in generating
adversarial examples of deep neural networks?
RQ2: Whether DeepRover is effective in reducing the distortion of
generated adversarial examples?
RQ3: Whether DeepRover is a query-efficient blackbox attack?
RQ4: Whether mutation vectors of DeepRover are more effective
than vectors that mutate all pixels for the same value?
RQ5: Whether grouping mutation vectors in blocks are more ef-
fective than using only one mutation vector by DeepRover ?
5.1 Evaluation Setup
Datasets and network models. We used deep neural networks
on three popular datasets, namely SVHN [ 31], CIFAR- 10[23], and
ImageNet [ 37], to evaluate the performance of DeepRover . For
SVHN and CIFAR- 10, we used 1,000randomly selected correctly
classified images from their test sets to perform adversarial attacks.
For ImageNet, we randomly selected 1,000correctly classified im-
ages from its validation set for evaluation.
For each of SVHN and CIFAR- 10, we trained two wide ResNet
w34-10networks [ 51]. One is a naturally trained undefended net-
work, and the other is trained using a state-of-the-art adversarial
defense [ 54]. For SVHN, we trained an undefended network with
96.68% test accuracy, and the defended network has 93.57% test
accuracy. For CIFAR- 10, the undefended network we trained has
95.47%test accuracy, and the defended network has 84.92%test accu-
racy. For ImageNet, we used two pre-trained undefended networks
(Inception v3 [ 43] and ResNet- 50[17]) in evaluation. Although de-
fense for ImageNet networks are important, we do not consider
defended networks for ImageNet because ImageNet networks using
defense techniques of [54] are not publicly available.
1Source code is available at https://github.com/fuyuan-zhang/DeepRoverExisting blackbox attacks. DeepRover is compared with two
state-of-the-art blackbox adversarial attacks:
â€¢The Bandits attack [ 19] is optimized for both ğ¿2andğ¿âˆ
distance metrics. It is a gradient-estimation-based blackbox
attack that achieves high query efficiency via integrating
gradient priors through bandit optimization. We compare
DeepRover with itsğ¿2attack.
â€¢The Square attack [ 2] is based on the random search, which
iteratively adds square-shaped local updates to input images
as long as the updates can improve its objective function.
The Square attack can perform both ğ¿2andğ¿âˆblackbox
attacks, and we compare DeepRover with itsğ¿2attack.
DeepRover implementation. We give a brief introduction of
some implementation details of Alg. 1. In the fuzzing step, we use
ğµ,ğ‘ğ‘ :=Gen-Block(k, x)to generate random blocks for mutation
insertion. In our implementation, ğµis a square block and its size
decreases as the number of iterations (indicated by ğ‘˜) increases. We
use variable ğ‘ğ‘, the percentage of pixels contained in ğµ, to control
the size ofğµ. In our experiments, we define the initial value of ğ‘ğ‘
using a parameter ğ‘“ğ‘ğ‘and decrease the value of ğ‘ğ‘by half when
ğ‘˜equals to 30,100,300,500,1000,2000,4000,6000, or8000.
In each iteration, after inserting mutation vectors to block ğµ, we
use function Remv-MutVec to delete existing mutations. To achieve
this, we generate a number of square blocks of equal size, e.g.,
ğµ1,...,ğµğ‘™, and erase all previous mutations added to these blocks. In
our implementation, the total number of pixels in blocks ğµ1,...,ğµğ‘™
approximately equals to the number of pixels in block ğµ.
Parameter settings. We scale images to [0,1]for all datasets
and setğ¿2distanceğ‘‘=5.0. The query budget for SVHN and CIFAR-
10networks is 20,000. We set the query budget as 10,000for Ima-
geNet networks, the same as the query budget of [2, 19].
For the Square attack, we use the suggested parameters in their
paper. Their attack has only one parameter and we set ğ‘=0.1for
neural networks on all datasets.
For the Bandits attack, we have tried various parameters to test
its performances on different network models and selected the
parameters that give their best performance for comparison. We
set bandits exploration ğ›¿=1, finite difference probe ğœ‚=5, OCO
learning rate ğœ‚=0.1, image learning rate â„=75, and tile size to 4for
SVHN networks. We set bandits exploration ğ›¿=1, finite difference
probeğœ‚=5, OCO learning rate ğœ‚=0.1, image learning rate â„=50
and tile size to 4for CIFAR- 10networks. For ImageNet ResNet- 50
network, we set bandits exploration ğ›¿=5, finite difference probe
ğœ‚=5, OCO learning rate ğœ‚=0.1, image learning rate â„=50and
tile size to 14. For ImageNet Inception v 3network, we set bandits
exploration ğ›¿=5, finite difference probe ğœ‚=5, OCO learning rate
ğœ‚=0.1, image learning rate â„=50and tile size to 13.
ForDeepRover , we first introduce parameters of mutation vec-
tors. For networks on all datasets, we set ğ‘1=1.5andğ‘2=0.8
to constrain the mutation value of mutation vectors. For SVHN
networks, we set ğ‘£ğ‘ =7to define the size of mutation vectors. For
both CIFAR- 10and ImageNet networks, we set ğ‘£ğ‘ =11to define
the size of mutation vectors.
Other parameters of DeepRover include the followings. We have
three parameters ğ‘ ,ğ‘¡andğ‘ğ‘£ğ‘›in the initialization step. We have
1389DeepRover: A Query-Efficient Blackbox Attack for Deep Neural Networks ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
two parameters ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ andğ‘“ğ‘ğ‘in the fuzzing step, where ğ‘“ğ‘ğ‘
defines the initial value of ğ‘ğ‘. For both defended and undefended
SVHN networks, we set ğ‘ =300,ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ =3,ğ‘¡=12,ğ‘ğ‘£ğ‘›=1, and
ğ‘“ğ‘ğ‘=0.11. For the defended CIFAR- 10network, we set ğ‘ =400,
ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ =2,ğ‘¡=12,ğ‘ğ‘£ğ‘›=3,ğ‘“ğ‘ğ‘=0.11. For the undefended
CIFAR- 10network, we set ğ‘ =400,ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ =1,ğ‘¡=6,ğ‘ğ‘£ğ‘›=1, and
ğ‘“ğ‘ğ‘=0.11. For the ImageNet Inception v3 network, we set ğ‘ =300,
ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ =3,ğ‘¡=60,ğ‘ğ‘£ğ‘›=2, andğ‘“ğ‘ğ‘=0.103. For the ImageNet
ResNet- 50network, we set ğ‘ =300,ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘¦ =3,ğ‘¡=44,ğ‘ğ‘£ğ‘›=1,
andğ‘“ğ‘ğ‘=0.105.
5.2 Evaluation Metrics
We use the following metrics to evaluate the effectiveness, subtlety,
and query efficiency of blackbox attacks.
Attack success rate. We use the attack success rate to evaluate
the effectiveness of adversarial attacks. It computes the percentage
of images for which adversarial examples are discovered. Tech-
niques with higher attack success rates are more effective in gener-
ating adversarial examples. We use genAdv(x)to denote whether
an adversarial example for input xis generated, i.e., genAdv(x)=1
if and only if an adversarial example for xis found. Given a set of
images X={x1,...,xğ‘˜}, we compute the attack success rate of a
blackbox attack by:
ASR(X)=1
ğ‘˜ğ‘˜âˆ‘ï¸
ğ‘–=1genAdv(xğ‘–)
Averageğ¿2distance. We use average ğ¿2distance to measure
the subtlety of adversarial attacks. Assume that X={x1,...,xğ‘˜}
are a set of input images and Xadv={xâ€²
1,...,xâ€²
ğ‘˜}are corresponding
adversarial examples. The average ğ¿2distance between XandXadv
is computed by:
AvgDSğ¿2(X,Xadv)=1
ğ‘˜ğ‘˜âˆ‘ï¸
ğ‘–=1||xğ‘–âˆ’xâ€²
ğ‘–||2
Attacks that can find adversarial examples with lower average
ğ¿2distance rate are more subtle. For attacks with similar success
rates, we further use this metric to distinguish which technique
can find more subtle adversarial examples.
Average queries. We use query numbers to measure the effi-
ciency of blackbox attacks. Blackbox attacks that cost fewer queries
are more efficient in making attacks. For each attack, we compute
the average number of queries needed for conducting successful
attacks. The mean number of queries is also computed in our ex-
periments for reference. Similar to [ 52], we do not count query
numbers in the refinement stage of DeepRover , which starts only
after an adversarial example is generated.
5.3 Experimental Results
We show our experimental results in Fig. 4 and Tabs. 1â€“4. For
the first three research questions, we compare DeepRover with
the Bandits attack and the Square attack. In the last two research
questions, we compare DeepRover with two of its variants denoted
by variant 1and variant 2in the tables.Table 1: Results on SVHN networks.
AttackSuccess
rateAvg.
ğ¿2Avg.
queriesMed.
queries
Undefended network
Bandits 99.9% 4.97 56.1 4.0
Square 100% 4.91 18.6 4.5
DeepRover 100% 2.47 17.0 2.0
Variant 1 100% 4.95 22.3 4.0
Variant 2 100% 4.90 18.2 2.0
Defended network
Bandits 99.9% 4.86 36.9 10.0
Square 100% 4.93 37.0 9.0
DeepRover 100% 2.91 28.7 7.0
Variant 1 100% 4.95 31.0 8.0
Variant 2 100% 4.93 40.0 11.0
Table 2: Results on CIFAR- 10networks.
AttackSuccess
rateAvg.
ğ¿2Avg.
queriesMed.
queries
Undefended network
Bandits 100% 4.93 42.5 4.0
Square 100% 4.86 11.2 2.0
DeepRover 100% 2.21 9.4 2.5
Variant 1 100% 4.93 12.0 4.0
Variant 2 100% 4.89 10.3 3.0
Defended network
Bandits 98.1% 4.74 125.8 32.0
Square 99.9% 4.93 135.9 19.0
DeepRover 100% 3.41 130.9 23.0
Variant 1 99.9% 4.96 137.9 24.0
Variant 2 100% 4.94 136.7 26.0
Table 3: Results on ImageNet Inception v3 network.
AttackSuccess
rateAvg.
ğ¿2Avg.
queriesMed.
queries
Bandits 86.3% 4.91 1137.7 308.0
Square 89.4% 4.99 1062.9 408.0
DeepRover 89.5% 4.17 914.4 307.0
Variant 1 89.0% 4.99 1014.6 383.5
Variant 2 84.3% 4.98 852.6 220.0
Adversarial examples generated by DeepRover for the ImageNet
Inception v 3network are shown in Fig. 5. The first row shows
the original images. Adversarial examples found by DeepRover are
listed in the second row. The third row shows adversarial examples
derived after the refinement step of DeepRover . From Fig. 5, we
can observe that adversarial perturbations constructed by Deep-
Rover , especially those derived after refinement, are hardly visible
to human eyes.
1390ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Fuyuan Zhang, Xinwen Hu, Lei Ma, and Jianjun Zhao
(a) SVHN (defended)
 (b) CIFAR- 10(defended)
 (c) ImageNet (Inception v3)
Figure 4: Results on success rate w.r.t number of queries.
Figure 5: Adversarial examples found by DeepRover on Ima-
geNet Inception v 3network. First row: original images. Sec-
ond row: adversarial examples. Third row: adversarial exam-
ples after refinement.
Table 4: Results on ImageNet ResNet- 50network.
AttackSuccess
rateAvg.
ğ¿2Avg.
queriesMed.
queries
Bandits 67.4% 4.90 929.2 205.0
Square 96.7% 4.99 892.9 305.0
DeepRover 96.7% 3.97 790.9 258.0
Variant 1 95.2% 4.99 996.4 383.5
Variant 2 93.8% 4.98 773.5 216.0
Results on attack success rate (RQ1). Experimental results
demonstrate that DeepRover is an effective approach in generating
adversarial examples on both defended and undefended neuralnetworks. It has achieved a higher attack success rate than the
other two state-of-the-art blackbox attacks.
For SVHN and CIFAR- 10networks, although the attack success
rate of other approaches is also high, DeepRover is the only ap-
proach that has achieved 100% attack success rate on all defended
and undefended neural networks.
For ImageNet ResNet- 50network, DeepRover has achieved 96.7%
attack success rate, which is the same as the Square attack. In
our experiments, the ImageNet Inception v 3network is the most
difficult to attack. DeepRover has achieved an 89.5%attack success
rate, slightly higher than the Square attack.
Answer to RQ1: DeepRover is an effective approach to gener-
ating adversarial examples for both defended and undefended
neural networks.
Results on average distortions (RQ2). For networks on all
three datasets, adversarial examples generated by DeepRover have
the lowest average ğ¿2distance, which means DeepRover can find
adversarial examples more subtly than other approaches. This result
shows that the refinement step of DeepRover effectively reduces
distortions of adversarial examples.
We can use DeepRover to estimate the distance of decision bound-
aries of neural networks. Actually, adversarial examples generated
byDeepRover after refinement are always at the border of decision
boundaries of neural networks. This is because DeepRover always
returns the adversarial example from which it is no longer possi-
ble to reduce its ğ¿2distance by projecting it to a smaller distance,
which can be seen from Alg. 2. For both SVHN and CIFAR- 10, we
can see that the decision boundaries of defended networks are in-
deed pushed further away from the original input images compared
to undefended networks.
Answer to RQ2: DeepRover can find adversarial examples
with low distortions. The refinement step of DeepRover can
effectively reduce the average ğ¿2distance of generated adver-
sarial examples.
Results on query efficiency (RQ3). Experimental results show
thatDeepRover has higher query efficiency than other attacks on all
networks except for the case of the Bandits attack on the defended
CIFAR- 10network.
1391DeepRover: A Query-Efficient Blackbox Attack for Deep Neural Networks ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Although the Bandits attack used fewer average queries, it has
a lower attack success rate than DeepRover , which means some
images that would cost a large number of queries to be attacked are
not counted by the Bandits attack. Otherwise, its average queries
would be much higher. For example, the attack success rate of
Bandits on the defended CIFAR- 10network is 98.1%, which means
there are 19images that Bandits cannot successfully attack within
20,000queries. If it were able to attack even one of the 19images
on the 20,000th query, the average queries of Bandits would have
become 146.0, which is higher than DeepRover .
To have a fairer comparison, we have checked (from our experi-
mental data) the average queries of DeepRover when it reaches the
final success rate of the Bandits attack. On the defended CIFAR- 10
network, the average queries of DeepRover is96.7when it reaches
the success rate of 98.1%, which shows that DeepRover actually has
a higher query efficiency than the Bandits attack.
Answer to RQ3: DeepRover is a query-efficient blackbox
attack in generating adversarial examples.
Results on mutation vectors (RQ4). We perform an ablation
study to show the effectiveness of mutation vectors. Recall that mu-
tation vectors are used to mutate pixels of small-sized blocks, where
pixels near the center of the block are mutated for much larger val-
ues than pixels near the border of the block. To study whether such
a design is effective in improving the attack success rate and query
efficiency, we substitute mutation vectors with vectors that mutate
all pixels in a block for the same distance.
We choose such vectors for comparison because these vectors are
often used for query reduction in related work [ 19,29,52] and are,
therefore, a natural substitute for mutation vectors of DeepRover .
Consequently, we implemented a variant of DeepRover , namely
variant 1, and compared its performance with DeepRover . Since we
focus on effectiveness and query efficiency, the refinement step is
not included in variant 1.
For networks on all datasets, DeepRover has a higher attack
success rate and query efficiency than variant 1. For SVHN and
CIFAR- 10networks, the attack success rate of variant 1is close
toDeepRover . It has achieved a 100% attack success rate on three
networks and a 99.9%success rate on the remaining one. However,
its success rate dropped on ImageNet networks. On the Inception
v3network, the success rate of variant 1is0.5%lower than Deep-
Rover . On the ResNet- 50network, its success rate is 1.5%lower
than DeepRover .
Answer to RQ4: The use of mutation vectors in DeepRover
is effective in improving its attack success rate and query
efficiency. It is more effective than vectors mutating all pixels
in a block for the same value.
Results on grouping mutation vectors (RQ5). To study the
effectiveness of grouping mutation vectors in blocks, we perform
an ablation study and create another variant of DeepRover , namely
variant 2, and compare its performance with DeepRover . Variant 2
is derived by inserting only one mutation vector to input images in
each iteration of the fuzzing step. Similar to variant 1, we do not
include the refinement step in variant 2.For SVHN and CIFAR- 10networks, variant 2has achieved 100%
attack success rate on both defended and undefended networks.
However, it is less query efficient than DeepRover .
On ImageNet networks, the attack success rate of variant 2
dropped significantly. On the Inception v 3network, the success rate
of variant 2is5.2%lower than DeepRover . On the ResNet- 50net-
work, its success rate is 2.9%lower than DeepRover . Although the
average queries of variant 2is lower than DeepRover on ImageNet
networks, the decrease in average queries of variant 2actually
comes from its decrease in attack success rate. Images that cost a
large number of queries to be successfully attacked are not counted
in the average queries of variant 2.
To have a closer comparison between DeepRover and variant 2
regarding query efficiency on ImageNet networks, we have checked
(from our experimental data) the average queries of DeepRover
when it achieves the final attack success rate of variant 2. On the
Inception v 3network, when DeepRover reaches the success rate of
84.3%, its average queries is 585.7, much lower than that of variant
2. On the ResNet- 50network, when DeepRover reaches the success
rate of 93.8%, its average queries is 592.5, which is also much lower
than that of variant 2. This shows that DeepRover is more query
efficient than variant 2on ImageNet networks.
Answer to RQ5: Grouping mutation vectors in blocks can
effectively improve the attack success rate and query efficiency
ofDeepRover .
5.4 Threats to Validity
We introduce the following threats to validity in our experiments.
Selection of datasets and networks. We have only used net-
work models for three datasets, namely SVHN, CIFAR- 10, and Ima-
geNet. Therefore, it is not guaranteed that our experimental results
can be generalized to networks for other datasets. However, the
datasets we have selected are the most commonly used in related
works evaluating adversarial attacks.
Selection of compared approaches. Various blackbox attacks
have been developed to evaluate the robustness of neural networks.
We have selected two approaches from state-of-the-art ğ¿2attacks.
To the best of our knowledge, compared to other state-of-the-art
attacks, they already have a very high attack success rate and query
efficiency.
Selection of parameters. The selection of parameters for com-
pared approaches can affect their performance. In our experiments,
we have used suggested parameters in their papers and tried var-
ious parameters to compare their performance. We select those
parameters that give their best performance on our networks.
Selection of defense techniques. Various defense techniques
have been developed to improve the adversarial robustness of neural
networks. Our experimental results may not generalize to networks
trained with other defense techniques. However, the defense we
have selected is the state-of-the-art defense.
1392ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Fuyuan Zhang, Xinwen Hu, Lei Ma, and Jianjun Zhao
6 RELATED WORK
Our work is related to works on adversarial attacks and the recent
research on developing software engineering techniques for quality
assurance of deep neural networks. As both research areas are
moving fast, we only introduce the research most relevant to us.
Adversarial attacks. Since Szegedy et al. [ 44] first revealed
the existence of adversarial examples in neural networks, various
adversarial attacks have been developed to evaluate the robustness
of neural networks. Whitebox attacks [ 5,12,24,28,30,35] are fast
and highly effective. However, they require full access to network
parameters, which is not realistic in real-world scenarios.
Blackbox attacks have been developed under different assump-
tions of target network models. Papernot et al. [ 33,34] developed a
transfer attack that utilizes the transferability of machine learning.
Scored-based attacks [ 1,2,15,18,19,29,52] require having access
to prediction scores of neural networks. Decision-based attacks
[4,7â€“9] only require knowing the final decision of target networks.
DeepRover is a type of score-based attack, and it is closest to
DeepSearch [52]. Both techniques utilize feedback-directed fuzzing
to generate adversarial examples and reduce the distortions of
adversarial examples through iterative refinement. Their difference
lies in the metrics they use to measure the distance of adversarial
examples. While DeepSearch is anğ¿âˆattack, DeepRover aims to
find adversarial examples constrained by the ğ¿2distance.
Testing and verification of DNNs. Various software engineer-
ing techniques have been developed to facilitate the testing of deep
neural networks. To guide test case generation, researchers have
proposed applicable coverage criteria [ 22,25,26,36,41] of neural
networks, and work in [ 50] has revisited existing coverage criteria
and proposed NeuraL Coverage as a new criterion.
While DNN testing techniques [ 10,14,16,32,42,45,47,55] focus
on detecting bugs in DNNs, formal verification of deep neural net-
works [ 11,13,20,21,38,46,48] aims at providing formal guarantees
of related properties, e.g., safety, robustness and fairness properties
of neural networks. Similar to formal verification of traditional
software, DNN verification techniques also suffer from the state
explosion problem, making it more challenging than DNN testing.
Repairing of deep neural networks. To fix erroneous behav-
iors of DNNs, DNN repair techniques have been proposed recently.
Broadly speaking, existing DNN repair approaches can be cate-
gorized into two groups: methods that directly modify network
parameters [ 39,40,53], e.g., weights or network structures, and
techniques based on retraining the networks with augmented data
[22, 27, 49].
Although various DNN repair techniques are already able to fix
detected erroneous behaviors, it is also essential to ensure that the
repairing process should not introduce unintended effects to DNNs,
e.g., input images that can be correctly classified by DNNs should
not be misclassified after repair.
7 CONCLUSION AND FUTURE WORK
We proposed DeepRover , a fuzzing-based blackbox adversarial at-
tack for deep neural networks. DeepRover is effective and query
efficient in generating adversarial examples. Moreover, adversarialexamples that DeepRover can find are of low distortions. We fore-
see that DeepRover is of great value for evaluating the adversarial
robustness of neural networks in real-world settings.
In our future work, we are motivated to extend the DeepRover
technique to develop attacks constrained by other distance metrics,
e.g., theğ¿0distance metric. We are also motivated to develop defense
techniques to improve the robustness of neural networks against
adversarial perturbations.
ACKNOWLEDGMENTS
We extend our gratitude to the reviewers for their valuable feedback.
This work is supported by JSPS KAKENHI Grant No.JP23K11049,
No.JP21K11841 and No.JP23H03372, JST-Mirai Program Grant Num-
ber JPMJMI20B8, as well as Canada CIFAR AI Chairs Program, the
Natural Sciences and Engineering Research Council of Canada.
REFERENCES
[1]Abdullah Al-Dujaili and Una-May Oâ€™Reilly. 2020. Sign Bits Are All You Need for
Black-Box Attacks. In ICLR .
[2]Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias
Hein. 2020. Square Attack: a query-efficient black-box adversarial attack via
random search. In ECCV . Springer, 484â€“501. https://doi.org/10.1007/978-3-030-
58592-1_29
[3]Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. 2018. Practical Black-Box
Attacks on Deep Neural Networks Using Efficient Query Mechanisms. In ECCV
(LNCS, Vol. 11216) . Springer, 158â€“174. https://doi.org/10.1007/978-3-030-01258-
8_10
[4]Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2018. Decision-Based Ad-
versarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.
InICLR . OpenReview.net.
[5]Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In S&P. IEEE Computer Society, 39â€“57. https://doi.org/10.
1109/SP.2017.49
[6]Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017.
ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural
Networks Without Training Substitute Models. In AISec@CCS . ACM, 15â€“26.
https://doi.org/10.1145/3128572.3140448
[7]Weilun Chen, Zhaoxiang Zhang, Xiaolin Hu, and Baoyuan Wu. 2020. Boosting
Decision-Based Black-Box Adversarial Attacks with Random Sign Flip. In ECCV .
Springer, 276â€“293. https://doi.org/10.1007/978-3-030-58555-6_17
[8]Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, Jinfeng Yi, and Cho-Jui
Hsieh. 2019. Query-Efficient Hard-label Black-box Attack: An Optimization-based
Approach. In ICLR .
[9]Minhao Cheng, Simranjit Singh, Patrick H. Chen, Pin-Yu Chen, Sijia Liu, and
Cho-Jui Hsieh. 2020. Sign-OPT: A Query-Efficient Hard-label Adversarial Attack.
InICLR .
[10] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar: model-based quantitative analysis of stateful deep learning systems. In
ESEC/FSE . ACM, 477â€“487. https://doi.org/10.1145/3338906.3338954
[11] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. AI2: Safety and Robustness Certification of
Neural Networks with Abstract Interpretation. In S&P. IEEE Computer Society,
3â€“18. https://doi.org/10.1109/SP.2018.00058
[12] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In ICLR .
[13] Divya Gopinath, Guy Katz, Corina S. Pasareanu, and Clark W. Barrett. 2018.
DeepSafe: A Data-Driven Approach for Assessing Robustness of Neural Networks.
InATVA (LNCS, Vol. 11138) . Springer, 3â€“19. https://doi.org/10.1007/978-3-030-
01090-4_1
[14] Divya Gopinath, Kaiyuan Wang, Mengshi Zhang, Corina S. Pasareanu, and
Sarfraz Khurshid. 2018. Symbolic Execution for Deep Neural Networks. CoRR
abs/1807.10439 (2018).
[15] Chuan Guo, Jacob R. Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q.
Weinberger. 2019. Simple Black-box Adversarial Attacks. In ICML . PMLR.
[16] Jianmin Guo, Yu Jiang, Yue Zhao, Quan Chen, and Jiaguang Sun. 2018. DLFuzz:
Differential Fuzzing Testing of Deep Learning Systems. In ESEC/FSE . ACM, 739â€“
743. https://doi.org/10.1145/3236024.3264835
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In CVPR . IEEE Computer Society, 770â€“778.
https://doi.org/10.1109/CVPR.2016.90
1393DeepRover: A Query-Efficient Blackbox Attack for Deep Neural Networks ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
[18] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-Box
Adversarial Attacks with Limited Queries and Information. In ICML (PMLR,
Vol. 80) . PMLR, 2142â€“2151.
[19] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2019. Prior Convictions:
Black-box Adversarial Attacks with Bandits and Priors. In ICLR .
[20] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer.
2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks.
InCAV (LNCS, Vol. 10426) . Springer, 97â€“117. https://doi.org/10.1007/978-3-319-
63387-9_5
[21] Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus,
Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, David L.
Dill, Mykel J. Kochenderfer, and Clark W. Barrett. 2019. The Marabou Framework
for Verification and Analysis of Deep Neural Networks. In CAV (LNCS, Vol. 11561) .
Springer, 443â€“452. https://doi.org/10.1007/978-3-030-25540-4_26
[22] Jinhan Kim, Robert Feldt, and Shin Yoo. 2019. Guiding deep learning system
testing using surprise adequacy. In ICSE . IEEE, 1039â€“1049. https://doi.org/10.
1109/ICSE.2019.00108
[23] Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images .
Technical Report. University of Toronto.
[24] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial Examples
in the Physical World. In ICLR . OpenReview.net.
[25] Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang
Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepGauge:
Multi-Granularity Testing Criteria for Deep Learning Systems. In ASE. ACM,
120â€“131. https://doi.org/10.1145/3238147.3238202
[26] Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao, and
Yadong Wang. 2018. Combinatorial Testing for Deep Learning Systems. CoRR
abs/1806.07723 (2018).
[27] Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama.
2018. MODE: automated neural network model debugging via state differential
analysis and input selection. In ESEC/FSE . ACM, 175â€“186. https://doi.org/10.
1145/3236024.3236082
[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In ICLR . OpenReview.net.
[29] Seungyong Moon, Gaon An, and Hyun Oh Song. 2019. Parsimonious Black-Box
Adversarial Attacks via Efficient Combinatorial Optimization. In ICML . PMLR.
[30] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In CVPR .
IEEE Computer Society, 2574â€“2582. https://doi.org/10.1109/CVPR.2016.282
[31] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y Ng. 2011. Reading Digits in Natural Images with Unsupervised Feature
Learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning .
[32] Augustus Odena, Catherine Olsson, David Andersen, and Ian J. Goodfellow. 2019.
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing. In
ICML (PMLR, Vol. 97) . PMLR, 4901â€“4911.
[33] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Trans-
ferability in Machine Learning: From Phenomena to Black-Box Attacks Using
Adversarial Samples. CoRR abs/1605.07277 (2016).
[34] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay
Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks Against Machine
Learning. In AsiaCCS . ACM, 506â€“519. https://doi.org/10.1145/3052973.3053009
[35] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay
Celik, and Ananthram Swami. 2016. The Limitations of Deep Learning in
Adversarial Settings. In EuroS&P . IEEE Computer Society, 372â€“387. https:
//doi.org/10.1109/EUROSP.2016.36
[36] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Au-
tomated Whitebox Testing of Deep Learning Systems. In SOSP . ACM, 1â€“18.https://doi.org/10.1145/3132747.3132785
[37] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Fei-Fei Li. 2015. ImageNet Large Scale Visual Recognition Challenge.
IJCV (2015), 211â€“252.
[38] Gagandeep Singh, Timon Gehr, Markus PÃ¼schel, and Martin T. Vechev. 2019. An
Abstract Domain for Certifying Neural Networks. PACMPL 3 (2019), 41:1â€“41:30.
Issue POPL. https://doi.org/10.1145/3290354
[39] Matthew Sotoudeh and Aditya V. Thakur. 2021. Provable repair of deep neural
networks. In PLDI . ACM, 588â€“603. https://doi.org/10.1145/3453483.3454064
[40] Bing Sun, Jun Sun, Long H. Pham, and Tie Shi. 2022. Causality-Based Neural
Network Repair. In ICSE. ACM, 338â€“349. https://doi.org/10.1145/3510003.3510080
[41] Youcheng Sun, Xiaowei Huang, and Daniel Kroening. 2018. Testing Deep Neural
Networks. CoRR abs/1803.04792 (2018).
[42] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and
Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In ASE.
ACM, 109â€“119. https://doi.org/10.1145/3238147.3238172
[43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbig-
niew Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In
CVPR . IEEE Computer Society, 2818â€“2826. https://doi.org/10.1109/CVPR.2016.308
[44] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks.
InICLR .
[45] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: Automated
Testing of Deep-Neural-Network-Driven Autonomous Cars. In ICSE. ACM, 303â€“
314. https://doi.org/10.1145/3180155.3180220
[46] Caterina Urban, Maria Christakis, Valentin WÃ¼stholz, and Fuyuan Zhang. 2020.
Perfectly parallel fairness certification of neural networks. OOPSLA (2020),
185:1â€“185:30. https://doi.org/10.1145/3428253
[47] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. DeepHunter: A Coverage-Guided
Fuzz Testing Framework for Deep Neural Networks. In ISSTA . ACM, 146â€“157.
https://doi.org/10.1145/3293882.3330579
[48] Xuan Xie, Fuyuan Zhang, Xinwen Hu, and Lei Ma. 2023. DeepGemini: Verifying
Dependency Fairness for Deep Neural Networks. In AAAI . AAAI Press, 15251â€“
15259. https://doi.org/10.1609/aaai.v37i12.26779
[49] Bing Yu, Hua Qi, Qing Guo, Felix Juefei Xu, Xiaofei Xie, Lei Ma, and Jianjun Zhao.
2022. DeepRepair: Style-Guided Repairing for Deep Neural Networks in the
Real-World Operational Environment. IEEE Trans. Reliab. 71, 4 (2022), 1401â€“1416.
https://doi.org/10.1109/TR.2021.3096332
[50] Yuanyuan Yuan, Qi Pang, and Shuai Wang. 2023. Revisiting Neuron Coverage for
DNN Testing: A layer-Wise and Distribution-Aware Criterion. In ICSE. 1200â€“1212.
https://doi.org/10.1109/ICSE48619.2023.00107
[51] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide Residual Networks. In
BMVC .
[52] Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis. 2020. DeepSearch:
A Simple and Effective Blackbox Attack for Deep Neural Networks. In ESEC/FSE .
ACM, 800â€“812. https://doi.org/10.1145/3368089.3409750
[53] Hao Zhang and W. K. Chan. 2019. Apricot: A Weight-Adaptation Approach to
Fixing Deep Learning Models. In ASE. IEEE, 376â€“387. https://doi.org/10.1109/
ASE.2019.00043
[54] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and
Michael I. Jordan. 2019. Theoretically Principled Trade-off between Robustness
and Accuracy. In ICML .
[55] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. DeepRoad: GAN-Based Metamorphic Testing and Input Valida-
tion Framework for Autonomous Driving Systems. In ASE. ACM, 132â€“142.
https://doi.org/10.1145/3238147.3238187
1394