Collaboration Challenges in Building ML-Enabled Systems:
Communication, Documentation, Engineering, and Process
Nadia Nahar
nadian@andrew.cmu.edu
Carnegie Mellon University
Pittsburgh, PA, USAShurui Zhou
University of Toronto
Toronto, Ontario, Canada
Grace Lewis
Carnegie Mellon Software Engineering Institute
Pittsburgh, PA, USAChristian Kästner
Carnegie Mellon University
Pittsburgh, PA, USA
ABSTRACT
Theintroductionofmachinelearning(ML)componentsinsoftware
projects has created the need for software engineers to collabo-
rate withdatascientists and otherspecialists. While collaboration
can always be challenging, ML introduces additional challenges
withitsexploratorymodeldevelopmentprocess,additionalskills
and knowledge needed, difficulties testing ML systems, need for
continuousevolution and monitoring,and non-traditional quality
requirements such as fairness and explainability. Through inter-
views with 45 practitioners from 28 organizations, we identified
key collaboration challenges that teams face when building and
deploying ML systems into production. We report on common col-
laboration points in the development of production ML systems
forrequirements,data,andintegration,aswellascorresponding
teampatternsandchallenges.Wefindthatmostofthesechallenges
centeraroundcommunication,documentation,engineering,and
process, and collect recommendations to address these challenges.
ACM Reference Format:
NadiaNahar,ShuruiZhou,GraceLewis,andChristianKästner.2022.Col-
laborationChallengesinBuildingML-EnabledSystems:Communication,
Documentation, Engineering, and Process. In 44thInternational Conference
on Software Engineering (ICSE ’22), May 21–29, 2022, Pittsburgh, PA, USA.
ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/3510003.3510209
1 INTRODUCTION
Machine learning (ML) is receiving massive attention and funding
inresearchandpractice;itisachievingincredibleadvances,surpass-
ing human-level cognition in many applications, but it is widely
acknowledged that moving from a prototyped machine-learnedmodel to a production system is very challenging. For example,
Venturebeat reported in 2019 that 87 percent of ML projects fail
[
108] and Gartner claimed in 2020 that 53 percent do not make
it from prototype to production [ 70]. While traditional software
projects are already complex, failure prone, and require a broadrange of expertise, the introduction of machine learning raises
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.35102097HDP
,QQHUJURXSV5HVSRQVLELOLW\'DWD&ROODESRLQW6RIWZ(QJ'DWD6FLHQWLVW(QGXVHU,QWHJU3URGXFW	0RGHO7HDP *RYPW
FOLHQW
3E 3F3D
SURGXFW 0/SLSHOLQH LQIHUHQFH
PRGHO
PRQLWRU
0RGHO7HDP 3URGXFW7HDP
3E3D
LQIUDVWU

3URGUHTXLUHPHQWV,QWHJUDWLRQ$3,	4$3XEOLFGDWD  
SURGXFW
0/SLSHOLQH
LQIHUHQFHPRGHO


(QGXVHU0RGHOUHT7UDLQLQJGDWD,QWHJU$3,  2UJDQL]DWLRQ 2UJDQL]DWLRQ
Figure 1: Structure of two interviewed organizations
further challenges, requires additional expertise, and introduces
additionalcollaborationpoints.
Technical aspects such as testing ML components [ 10,20], mis-
use of ML libraries [ 43,45], engineering challenges for developing
MLcomponents[ 3,5,18,27,40,44,60,90],andautomatinglearning
and deployment processes for ML components [ 4,13,29,34,51],
havereceivedsignificantattentioninresearchrecently.However,
humanfactorsofcollaborationduringthedevelopmentofsoftware
products supported by ML components, ML-enabled systems for
short,have receivedless attention,includingthe needtoseparate
and coordinate data science and software engineering work, to ne-
gotiateanddocumentinterfacesandresponsibilities,andtoplanthe
system’soperationandevolution.Yet,thosehumancollaboration
challengesappeartobemajorhurdlesindevelopingML-enabled
systems. In addition, past work has mostly been model-centric,
focusedonchallengesoflearning,testing,orservingmodels,but
rarely focuses on the entire system, i.e., the product with many
non-ML parts into whichthe model is embedded as a component,
whichrequirescoordinatingandintegratingworkfrommultiple
experts or teams.
Tobetterunderstandcollaborationchallengesandavenuesto-
wardbetterpractices,weconductedinterviewswith45participants
4132022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE’22,May21–29,2022,Pittsburgh,PA, USA NadiaNahar,ShuruiZhou,GraceLewis, and Christian Kästner
contributing to the development of ML-enabled systems for pro-
duction use (i.e., not pure data analytics/early prototypes). Ourresearch question is: What are the collaboration points and corre-
spondingchallengesbetweendatascientistsandsoftwareengineers?
Participants come from 28 organizations, from small startups to
large big tech companies, and havediverse roles in theseprojects,
includingdatascientists,softwareengineers,andmanagers.Dur-
ing our interviews, we explored organizational structures (e.g., see
Figure 1), interactions of project members with different technical
backgrounds,and where conflicts arise between teams.
While some organizations have adopted better collaboration
practices than others, many struggle setting up structures, pro-
cesses,andtoolingforeffectivecollaborationamongteammembers
with different backgrounds when developing ML-enabled systems.
Tothebestofourknowledge,andconfirmedbythepractitioners
we interviewed,there islittle systematic orshared understanding
ofcommoncollaborationchallengesandbestpracticesfordevel-
oping ML-enabled systems and coordinating developers with very
different backgrounds (e.g., data science vs. software engineering).
We find that smaller andnew-to-MLorganizations struggle more,
buthave limited advice to draw from for improvement.
Threecollaborationpoints surfacedasparticularly challenging:
(1)Identifyinganddecomposingrequirements,(2)negotiatingtrain-
ing data qualityand quantity, and (3) integrating data science and
software engineering work. We found that organizational struc-
ture,teamcomposition,powerdynamics,andresponsibilitiesdiffer
substantially, but also found common organizational patterns at
specific collaboration points and challenges associated with them.
Overall,ourobservationssuggestfour themesthatwouldbenefit
frommoreattentionwhenbuildingML-enabledsystems: /groupInvest
insupporting interdisciplinaryteams toworktogether(including
educationandavoidingsilos), /file_textPaymoreattentiontocollabora-
tion points and clearly document responsibilities and interfaces,
/cogsConsider engineeringwork asakeycontribution totheproject,
andὌ5Investmore into process and planning.
In summary, we make the following contributions: (1) We iden-
tify three core collaboration points and associated collaboration
challenges based on interviews with 45 practitioners, triangulated
with a literature review, (2) We highlight the different ways in
whichteamsorganize,butalsoidentifyorganizationalpatternsthat
associate with certain collaboration challenges, and (3) We identify
recommendations to improve collaboration practices.
2 STATE OF THE ART
Researchers and practitioners have discussed whether and how
machine learning changes software engineering with the introduc-
tion of learned models as components in software systems [e.g.,
1,5,42,69,81,83,90,103,111].Tolaythefoundationforourinter-
view study and inform the questions we ask, we first provide an
overview of the related work and existing theories on collabora-
tion in traditional software engineering and discuss how machine
learningmaychangethis.
CollaborationinSoftwareEngineering. Mostsoftwareprojects
exceed the capacity of a single developer, requiring multiple devel-
opers and teams to collaborate (“work together”) and coordinate
(“aligngoals”).Collaborationhappens acrossteams,ofteninamoreformal and structured form, and withinteams, where familiarity
withotherteammembersandfrequentco-locationfostersinformal
communication[ 63].Atatechnicallevel,toallowmultipledevelop-
ers to work together, abstraction and adivide and conquer strategy
are essential. Dividing software into components (modules, func-
tions,subsystems)andhidinginternalsbehind interfaces isakey
principle of modular software development that allows teams to
divide work, and work mostly independently until the final system
is integrated [62, 72].
Teamswithinanorganizationtendtoalignwiththetechnical
structure of the system, with individuals or teams assigned to com-
ponents[ 30],hencethe technicalstructure(interfacesand depen-
denciesbetweencomponents)influencesthepointswhereteams
collaborate and coordinate. Coordination challenges are especially
observedwhenteamscannoteasilyandinformallycommunicate,
often studied in the context of distributed teams of global corpora-
tions[38,68] andopen-source ecosystems [16, 95].
More broadly, interdisciplinary collaboration often poses chal-
lenges. It has been shown that when team members differ in their
academicandprofessionalbackgroundsandpossessdifferentexpec-
tations on the same system, communication, cultural, and methodi-
calchallengesoftenemergewhenworkingtogether[ 21,73].Key
insights are that successful interdisciplinary collaboration depends
on professional role, structural characteristics, personal character-
istics, and a history of collaboration; specifically, structural factors
such as unclear mission, insufficient time, excessive workload, and
lackof administrative support are barriers to collaboration [24].
Thecomponent interfaceplaysakeyroleincollaborationasa
negotiation and collaboration point. It is where teams (re-)negotiate
how to divide work and assign responsibilities [ 19]. Team mem-
bers oftenseek informationthat maynot becaptured ininterface
descriptions,asinterfacesarerarelyfullyspecified[ 32].Inanide-
alizeddevelopmentprocess,interfacesaredefinedearlybasedon
what is assumed to remain stable [ 72], because changes to inter-
faceslaterareexpensiveandrequiretheinvolvementofmultiple
teams.Inaddition,interfacesreflect keyarchitecturaldecisions for
thesystem,aimed to achieve desired overall qualities [11].
Inpracticethough,theidealizeddivide-and-conquerapproach
followingtop-downplanningdoesnotalwaysworkwithoutfriction.
Notallchangescanbeanticipated,leadingtolatermodifications
andrenegotiationofinterfaces[ 16,31].Itmaynotbepossibleto
identify how to decompose work and design stable interfaces until
substantial experimentation has been performed [ 12]. To manage,
negotiate,andcommunicatechangesofinterfaces,developershave
adopted a wide range of strategies for communication [ 16,33,97],
often relying on informal broadcast mechanisms to share planned
or performed changes with other teams.
Softwarelifecyclemodels [22]alsoaddressthistensionofwhen
and how to design stable interfaces: Traditional top-down mod-
els (e.g., waterfall) plan software design after careful requirements
analysis;the spiralmodel pursuesarisk-firstapproachinwhichde-
velopersiteratetoprototyperiskyparts,whichtheninformsfuture
systemdesigniterations; agileapproachesde-emphasizeupfront
architectural design for fast iteration on incremental prototypes.
Thesoftwarearchitecturecommunityhasalsograppledwiththe
questionofhowmuchupfrontarchitecturaldesignisfeasible,prac-
tical,ordesirable[ 11,107],showingatensionbetweenthedesire
414CollaborationChallengesin Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
for upfront planning on one side and technical risks and unsta-
ble requirements on the other. In this context, our research explores
howintroducing machinelearning intosoftwareprojects challenges
collaboration.
SoftwareEngineeringwithMLComponents. InaML-enabled
system, machine learning contributes one or multiple components
toalargersystemwithtraditionalnon-MLcomponents.Wereferto
thewholesystemthatanenduserwoulduseasthe product.Insome
systems,thelearned modelmaybearelativelysmallandisolated
additiontoalargetraditionalsoftwaresystem(e.g.,auditpredictionintaxsoftware);inothersitmayprovidethesystem’sessentialcore
withonlyminimalnon-MLcodearoundit(e.g.,asalesprediction
systemsendingdailypredictionsbyemail).Inadditiontomodels,
an ML-enabled system typically also has components for training
and monitoring the model(s) [ 42,51]. Much attention in practice
recentlyfocusesonbuildingrobustML pipelines fortrainingand
deploying models in a scalable fashion, often under names suchas“AIengineering,”“SysML,” and“MLOps” [
51,59,67,90].Inthis
work, we focus more broadly on the development of the entire
ML-enabled system, including both ML and non-ML components.
Compared to traditional software systems, ML-enabled systems
require additional expertise in data science to build the models and
mayplaceadditionalemphasisonexpertisesuchasdatamanage-
ment,safety,andethics[ 5,49].Inthispaper,weprimarilyfocuson
therolesof softwareengineers anddatascientists,whotypicallyhave
different skills and educational backgrounds [ 48,49,84,111]: Data
science education tends to focus more on statistics, ML algorithms,
andpracticaltrainingofmodelsfromdata(typicallygivenafixeddataset,notdeployingthemodel,not buildingasystem),whereas
softwareengineeringeducationfocusesonengineeringtradeoffs
with competing qualities, limited information, limited budget, and
the construction and deployment of systems. Research shows that
software engineers who engage in data science without further
educationareoftennaivewhenbuildingmodels[ 111]andthatdata
scientistsprefertofocusnarrowlyonmodelingtasks[ 84]butare
frequentlyfacedwithengineeringwork[ 106].Whilethereisplenty
ofworkonsupportingcollaborationamongsoftwareengineers[ 26,
33,85,115] and more recently on supporting collaboration among
datascientists[ 105,114],wearenotawareofworkexploringcollab-
orationchallengesbetween these roles, which we do in this work.
The software engineering community has recently started to
exploresoftware engineering for ML-enabled systems as a research
field,withmanycontributionsonbringingsoftware-engineering
techniques to ML tasks, such as testing models and ML algorithms
[10,20,28,110],deployingmodels[ 4,13,29,34,51],robustnessand
fairnessofmodels [ 81,94,101],lifecycles forMLmodels[ 1,5,34,
61,74],andengineeringchallengesorbestpracticesfordeveloping
ML components [ 3,5,18,27,40,44,60,90]. A smaller body of
workfocusesontheML-enabledsystembeyondthemodel,such
as exploring system-level quality attributes [73, 93], requirements
engineering[ 103],architecturaldesign[ 113],safetymechanisms
[17,83], and user interaction design [ 7,25,112].In this paper, we
adopt this system-wide scope and explore how data scientists andsoftware engineers work together to build the system with ML and
non-MLcomponents.3 RESEARCH DESIGN
Because there is limited research on collaboration in building ML-
enabledsystems,weadoptaqualitativeresearchstrategytoexplore
collaborationpoints andcorresponding challenges,primarilywith
stakeholderinterviews.Weproceededinfoursteps:(1)Weprepared
interviewsbasedonaninitialliteraturereview,(2)weconducted
interviews, (3) we triangulated results with literature findings, and
(4)wevalidatedourfindingswiththeinterviewparticipants.We
baseourresearchdesignon StraussianGroundedTheory [98,99],
which derives research questions from literature, analyzes inter-
views with open and axial coding, and consults literature through-
out the process. In particular, we conduct interviews and literature
analysis in parallel, with immediate and continuous data analysis,
performingconstantcomparisons,andrefiningourcodebookand
interview questions throughout the study.
Step1:Scopingandinterviewguide. Toscopeourresearchand
prepare for interviews, we looked for collaboration problems men-
tionedinexistingliteratureonsoftwareengineeringforML-enabled
systems (Sec. 2). In this phase, we selected 15 papers opportunis-
tically through keyword search and our own knowledge of the
field. We marked all sections in those papers that potentially relate
to collaboration challenges between team members with differ-
entskillsoreducationalbackgrounds,followingastandardopen
codingprocess[ 99].Eventhoughmostpapersdidnottalkabout
problems in terms of collaboration, we marked discussions thatmay plausibly relate to collaboration, such as data quality issues
between teams. We then analyzed and condensed these codes into
nineinitialcollaborationareasanddevelopedaninitialcodebook
andinterview guide (provided in Appendix of arXiv version [66]).
Step 2: Interviews. We conducted semi-structured interviews
with45participantsfrom28organizations,each30to60minutes
long.Allparticipantsareinvolvedinprofessionalsoftwareprojects
using machine learning that are either already or planned to be
deployedinproduction.InTable1,weshowthedemographicsof
theinterviewparticipantsandtheirorganizations.Detailscanbe
foundin the Appendix of our arXiv version [66].
Wetriedtosampleparticipantspurposefully(maximumvaria-
tion sampling [ 36]) to cover participants in different roles, types of
companies,andcountries.Weintentionallyrecruitedmostpartic-
ipants from organizations outside of big tech companies, as they
representthevastmajorityofprojectsthathaverecentlyadopted
machine learning and oftenface substantially different challenges
[40]. Where possible, we tried to separately interview multiple
participantsindifferentroleswithinthesameorganizationtoget
different perspectives. We identified potential participants through
personalnetworks,ML-relatednetworkingevents,LinkedIn,and
recommendationsfrompreviousintervieweesandlocaltechlead-
ers. We adapted our recruitment strategy throughout the research
based on our findings, at later stages focusing primarily on spe-
cific roles and organizations to fill gaps in our understanding, until
reachingsaturation.Forconfidentiality,werefertoorganizations
bynumberand toparticipantsbyPXy whereXreferstothe orga-
nization number and ydistinguishes participants from the same
organization.
415ICSE’22,May21–29,2022,Pittsburgh,PA, USA NadiaNahar,ShuruiZhou,GraceLewis, and Christian Kästner
Table 1: Participant and Company Demographics
Type Break-down
ParticipantRole(45) ML-focused (23), SE-focused (9), Manage-
ment(5),Operations(2),DomainExpert(2),
Other(4)
ParticipantSeniority(45) 5 years of experience or more (28), 2-5
years (9), under 2 years (8)
CompanyType (28) Bigtech(6),NonIT(4),Mid-sizetech(11),
Startup(5),Consulting(2)
CompanyLocation (28) North America(11), SouthAmerica (1), Eu-
rope (5), Asia (10), Africa (1)
We transcribed and analyzed all interviews. Then, to map chal-
lenges to collaboration points, we created visualizations of orga-
nizationalstructureandresponsibilitiesineachorganization(we
showtwoexamplesinFigure1)andmappedcollaborationproblems
mentionedintheinterviewstocollaborationpointswithinthese
visualizations. We used these visualizations to further organize our
data; in particular, we explored whether collaboration problems
associatewithcertaintypes of organizational structures.
Step 3: Triangulation with literature. As we gained insights
from interviews, we returned to the literature to identify related
discussions and possible solutions (even if not originally framed in
termsofcollaboration)totriangulateourinterviewresults.Relevant
literature spans multiple research communities and publication
venues, including machine learning, human-computer interaction,
software engineering, systems, and various application domains
(e.g., healthcare, finance), and does not always include obvious
keywords; simply searching for machine-learning research yields a
far too wide net. Hence, we decided against a systematic literature
review and pursued a best effort approach that relied on keyword
search for topics surfaced in the interviews, as well as backward
andforwardsnowballing.Outofover300papersread,weidentified
61 as possibly relevant and coded them with the same evolving
codebook.ThecompletelistcanbefoundinourarXivversion[ 66].
Step4: Validity check with interviewees. For checking fit and
applicability as definedby Corbin and Strauss [ 99] and validating
our findings, we went back to the interviewees after creating a full
draft of this paper. We presented the interviewees both a summary
and the full draft, including the supplementary material, along
withquestionspromptingthemtolookforcorrectnessandareas
of agreement or disagreement (i.e., fit), and any insights gained
fromreadingaboutexperiencesoftheothercompanies,roles,or
findings as a whole (i.e., applicability). Ten interviewees responded
withcommentsandallindicatedgeneralagreement,someexplicitly
reaffirmedsomefindings. Weincorporatedtwominorsuggested
changesabout details of two organizations.
Threatstovalidity andcredibility. Ourworkexhibitsthetyp-
ical threats common and expected for this kind of qualitative re-search. Generalizations beyond the sampled participant distribu-tion should be made with care; for example, we interviewed fewmanagers, no dedicated data experts, and no clients. In several
organizations, we were only able to interview a single person, giv-
ing us a one-sided perspective. Observations may be different inorganizationsinspecificdomainsorgeographicregionsnotwell
represented in our data. Self-selection of participants may influ-
enceresults;forexampledevelopersingovernment-relatedprojects
more frequentlydeclined interviewrequests. Asdescribed earlier,
we followed standard practices for coding and memoing, but, as
usual in qualitative research, we cannot entirely exclude biases
introduced by us researchers.
4 DIVERSITY OF ORG. STRUCTURES
Throughout our interviews, we found that the number and type
of teams that participate in ML-enabled system development dif-fers widely, as do their composition and responsibilities, powerdynamics, and the formality of their collaborations, in line with
findingsbyAhoetal.[ 1].Toillustratethesedifferences,weprovide
simplified descriptions of teams found in two organizations in Fig-
ure1.Weshowteamsandtheirmembers,aswellastheartifactsfor
whichtheyare responsible,suchas,whodevelopsthe model,who
builds a repeatable pipeline, who operates the model (inference ),
who is responsible for or owns the data, and who is responsible
for the final product. A team often has multiple responsibilities and
interfaceswithotherteamsatmultiplecollaborationpoints.Where
unambiguous, we refer to teams by their primary responsibility as
product team ormodel team.
Organization3(Figure1,top)developsanML-enabledsystemfor
agovernmentclient.Theproduct(healthdomain),includinganML
modeland multiplenon-ML components,is developedbya single
8-person team.The team focuseson traininga model first,before
buildingaproductaroundit.Softwareengineeringanddatascience
tasks are distributed within the team, where members cluster into
groupswithdifferentresponsibilitiesandroughlyequalnegotiation
power. A single data scientist is part of this team, though they
feelsomewhatisolated.Dataissourcedfrompublicsources.The
relationshipbetweentheclientanddevelopmentteamissomewhat
distant and formal. The product is delivered as a service, but the
teamonly receives feedback when things go wrong.
Organization7(Figure1,bottom)developsaproductforin-house
use(qualitycontrolforaproductionprocess).Asmallteamisdevel-
opingandusingtheproduct,butmodeldevelopmentisdelegated
to an external team (different company) composed of four data sci-
entists, of which two have some software engineering background.
Theproductteaminteractswiththemodelteamtodefineandrevise
modelrequirementsbasedonproductrequirements.Theproduct
teamprovidesconfidentialproprietarydatafortraining.Themodel
team deploys the model and provides a ready-to-use inference API
totheproductteam.Therelationshipbetweentheteamscrosses
company boundaries and is rather distant and formal. The product
teamclearlyhas thepower in negotiations between the teams.
Thesetwoorganizationsdifferedalongmanydimensions,andwe
foundnoclearglobalpatternswhenlookingacrossorganizations.
Nonethelesspatternsdidemergewhenfocusingonthreespecific
collaborationaspects, as we will discuss in the next sections.
5 COLLABORATION POINT: REQUIREMENTS
AND PLANNING
In an idealized top-down process, one would first solicit product re-
quirements and then plan and design the product by dividing work
416CollaborationChallengesin Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
into components (ML and non-ML), deriving each component’s re-
quirements/specifications from the product requirements. In this
process,collaborationisneededfor:(1)productteamneedstone-
gotiateproductrequirementswithclientsandotherstakeholders;
(2) product team needs to plan and design product decomposition,
negotiating with component teams the requirements for individ-
ual components; and (3) product project manager needs to planand manage the work across teams in terms of budgeting, effort
estimation,milestones,and work assignments.
5.1 CommonDevelopment Trajectories
Feworganizations,ifany,followanidealizedtop-downprocess,and
it may not even be desirable, as we will discuss later. While we did
notfindanyglobalpatternsfororganizationalstructures(Sec.4),
there are indeed distinct patterns relating to how organizations
elicitrequirementsanddecomposetheirsystems.Mostimportantly,
we see differences in terms of the orderin which teams identify
product and model requirements:
Model-firsttrajectory: 13ofthe28organizations(3,10,14–17,
19, 20, 22, 23, 25–27) focus on building the model first, and build
aproductaroundthemodellater.Intheseorganizations,product
requirements are usually shaped by model capabilities after the
(initial) model has been created, rather than being defined upfront.
Inorganizationswithseparatemodelandproductteams,themodel
teamtypically startsthe project and the product team joins later
withlow negotiating power to build a product around the model.
Product-first trajectory: In13organizations(1,4,5,7–9,11–
13, 18, 21, 24, 28), models are built later to support an existing
product. In these cases, a product often already exists and product
requirementsarecollectedforhowtoextendtheproductwithnew
ML-supportedfunctionality.Here,the modelrequirementsarede-
rived from the product requirements and often include constraints
on model qualities, such as latency, memory and explainability.
Parallel trajectory: Two organizations (2, 6) follow no clear
temporalorder; model and product teams work in parallel.
5.2 Product and Model Requirements
Wefoundaconstanttensionbetweenproductandmodelrequire-
ments in our interviews. Functional and nonfunctional product
requirements set expectations for the entire product. Model re-
quirementssetgoalsandconstraintsforthemodelteam,suchas
expected accuracy and latency, target domain, and available data.
Product requirements require input from the model team
(/group,Ὄ5).Acommonthemeintheinterviewsisthatitisdifficultto
elicitproductrequirementswithoutagoodunderstandingofMLca-
pabilities, which almost always requires involving the model team
and performing some initial modeling when eliciting product re-
quirements. Regardless of whether product requirements or model
requirementsareelicitedfirst,datascientistsoftenmentionedbeing
faced with unrealisticexpectations about model capabilities.
Participants that interact with clients to negotiate product re-
quirements (which may involve members of the model team) indi-
catethattheyneedtoeducateclientsaboutcapabilitiesofMLtech-
niquesto setcorrectexpectations (P3a,P6a,P6b,P7b,P9a,P10a,P15c,
P19b,P22b,P24a).ThisneedtoeducatecustomersaboutMLcapabil-
itieshasalsobeenraisedintheliterature[ 1,17,44,49,100,103,106].For many organizations, especially in product-first trajectories,
the model team indicates similar challenges when interacting with
theproductteam.Iftheproductteamdoesnotinvolvethemodel
teaminnegotiatingproductrequirements,theproductteammay
notidentifywhatdataisneededforbuildingthemodel,andmay
commit to unrealistic requirements. For example, P26a shared “For
thisproject,[theprojectmanager]wantedtoclaimthatwehaveno
false positives and I was like, that’s not gonna work.” Members of
the model team often report lack of ML literacy in members of
theproductteam andproject managers (P1b,P4a, P7a,P12a, P26a,
P27a) and a lack of involvement (e.g., P7b: “The [product team]
decidedwhat typeof datawould makesense. Ihad nosay onthat.” ).
Usually the product team cannot identify product requirements
alone, instead product and model teams need to interact to explore
whatis achievable.
In organizations with a model-first trajectory, members of the
modelteamsometimesengagedirectlywithclients(andalsoreport
having to educate them about ML capabilities). However, when
requirements elicitation is left to the model team, members tend to
focusonrequirementsrelevantforthemodel,butneglectrequire-
ments for the product, such as expectations for usability, e.g., P3c’s
customers“werekindofhappywiththeresults,but weren’thappy
with the overall look and feel or how the system worked.” Several re-
searchpaperssimilarlyidentifiedhowthegoalsofdatascientistsdi-
vergefromproductgoalsifproductrequirementsarenotobviousat
modeling time, leading to inefficient development, worse products,
or constant renegotiation of requirements, especially [67, 73, 112].
Modeldevelopmentwithunclearmodelrequirementsiscom-
mon(/file_text). Participantsfrom model teams frequently explain how
they are expected to work independently, but are given sparse
model requirements. They try to infer intentions behind them, but
are constrained by having limited understanding of the product
that the model will eventually support (P3a, P3b, P16b, P17b, P19a).
Model teams often start with vague goals and model requirements
evolve over time as product teams or clients refine their expec-tations in response to provided models (P3b, P7a, P9a, P5b, P19b,
P21a).Especiallyinorganizationsfollowingthe model-firsttrajec-
tory, model teams may receive some data and a goal to predictsomething with high accuracy, but no further context, e.g., P3a
shared“thereisn’talwaysanactualspecofexactlywhatdatathey
have,whatdatatheythinkthey’regoingtohaveandwhattheywant
themodeltodo.” Severalpaperssimilarlyreportprojectsstarting
withvaguemodel goals [50, 77, 83, 111].
Eveninorganizationsfollowinga product-firsttrajectory,product
requirementsareoftennottranslatedintoclearmodelrequirements.
For example, participant P17b reports how the model team wasnot clear about the model’s intended target domain, thus could
notdecidewhatdatawasconsideredinscope.Asaconsequence,
modelteamsusuallycannotfocusjustontheircomponent,buthave
to understand the entire product to identify model requirementsin the context of the product (P3a, P10a, P13a, P17a, P17b, P19b,
P20b,P23a),requiringinteractionswiththeproductteamoreven
bypassingtheproductteamtotalkdirectlytoclients.Thedifficulty
of providing clear requirements for an ML model has also beenraised in the literature [
49,55,80,92,104,111], partially arguing
thatuncertaintymakesitdifficulttospecifymodelrequirements
417ICSE’22,May21–29,2022,Pittsburgh,PA, USA NadiaNahar,ShuruiZhou,GraceLewis, and Christian Kästner
upfront[1,44,50,69,106].Ashemoreetal.reportmappingproduct
requirements to model requirements as an open challenge [10].
Providedmodelrequirementsrarelygobeyondaccuracyand
data security (/cogs, /file_text).Requirements given to model teams pri-
marilyrelatetosomenotionofaccuracy.Beyondaccuracy,require-mentsfordatasecurityandprivacyarecommon,typicallyimposed
by the data owner or by legal requirements (P5a, P7a, P9a, P13a,
P14a,P18a,P20a-b,P21a-b,P22a,P23a,P24a,P25a,P26a).Literature
also frequently discusses how privacy requirements impact and
restrict ML work [15, 41, 43, 55, 56, 78].
We rarely heard of any qualities other than accuracy. Some par-
ticipantsreportthatignoringqualitiessuchaslatencyorscalabilityhasresultedinintegrationandoperationproblems(P3c,P11a).Ina
fewcasesrequirementsforinferencelatencywereprovided(P1a,
P6a,P14a)andinonecasehardwareresourcesprovidedconstraints
on memory usage (P14a), but no other qualities such as traininglatency,modelsize,fairness,orexplainabilitywererequiredthat
couldbe important for product integration and deployment.
Whenprompted, veryfew ofourinterviewees reportconsider-
ations for fairness either at the product or the model level. Only
two participants from model teams (P14a, P22a) reported receiving
fairnessrequirements,whereasmanyothersexplicitlymentioned
thatfairnessisnotaconcernforthemyet(P4a,P5b,P6b,P11a,P15c,
P20a,P21b,P25a,P26a).Thelackoffairnessandexplainabilityre-
quirements is in stark contrast to the emphasis that these qualities
receive in the literature [e.g., 7, 15, 25, 39, 40, 57, 89, 92, 109, 114].
Recommendations. Ourobservationssuggestthatinvolvingdata
scientists early when soliciting product requirements is important
(/group) and that pursuing a model-first trajectory entirely without
consideringproduct requirements is problematic (Ὄ5). Conversely,
model requirements are rarely specific enough to allow data scien-
tiststoworkin isolationwithoutknowingthebroadercontext of
the system and interaction with the product team should ideally
be planned as part of the process. Requirements form a key col-
laboration point between product and model teams, which should
be emphasized even in more distant collaboration styles (e.g., out-
sourcedmodeldevelopment).Thefeworganizationsthatusethe
paralleltrajectory reportfewerproblemsbyinvolvingdatascien-
tistsinnegotiatingproductrequirementstodiscardunrealisticonesearly on (P6b). Vogelsang and Borg also provide similar recommen-
dationstoconsultdatascientistsfromthebeginning tohelpelicit
requirements [ 103]. While many papers place emphasis on clearly
definingML use cases and scope [ 49,93,100], several others men-
tion how collaboration of technical and non-technical stakeholders
suchas domain experts helps [73, 89, 104, 106].
MLliteracyforcustomersandproductteamsappearstobeim-
portant ( /group). P22a and P19a suggested conducting technical ML
trainingsessionstoeducateclients;similartrainingisalsouseful
for members of product teams. Several papers argue for similar
trainingfornon-technicalusersof ML products [44, 89, 103].
Most organizations elicit requirements only rather informally
and rarely have good documentation, especially when it comesto model requirements. It seems beneficial to adopt more formal
requirementsdocumentationforproductandmodel( /file_text),asseveral
participants reported that it fosters shared understanding at this
collaborationpoint(P11a,P13a,P19b,P22a,P22c,P24a,P25a,P26a).Checklists could help to cover a broader range of model quality
requirements, such as training latency, fairness, and explainability.
Formalismssuchasmodelcards[ 64]andFactSheets[ 8]couldbe
used as a starting point for documenting model requirements.
5.3 Project Planning
ML uncertainty makes effort estimation difficult (/group). Irre-
spective of trajectory, 19 participants (P3a, P4a, P7a-b, P8a, P14b,
P15b-c, P16a, P17a, P18a, P19a-b, P20a, P22a-c, P23a, P25a) men-
tioned that the uncertainty associated with ML components makes
it difficult to estimate the timeline for developing an ML compo-
nent and by extension the product. Model development is typically
seen as a science-like activity, where iterative experimentation and
exploration is needed to identify whether and how a problem can
be solved, rather than as an engineering activity that follows a
somewhat predictable process. This science-like nature makes itdifficultfor themodel team to set expectations or contracts with
clientsortheproductteamregardingeffort,cost,oraccuracy.While
data scientists find effort estimation difficult, lack of ML literacy
in managers makes it worse (P15b, P16a, P19b, P20a, P22b). Teams
report deploying subpar models when running out of time (P3a,
P15b,P19a),orpostponingorevencancelingdeployments(P25a).
These findings align with literature mentioning difficulties associ-
atedwitheffortestimationforMLtasks[ 1,9,61,106]andplanning
projectsinastructuredmannerwithdiversemethodologies,with
diverse trajectories, and without practical guidance [ 1,17,61,106].
Generally,participantsfrequentlyreportthatsynchronization
betweenteamsischallengingbecauseofdifferentteampace,differ-
ent development processes, and tangled responsibilities (P2a, P11a,
P12a,P14-b, P15b-c, P19a; see also Sec. 7.2).
Recommendations. Participants suggested several mitigation
strategies:keepingextrabuffertimesandaddingadditionaltime-
boxesforR&Dininitialphases(P8a,P19a,P22b-c,P23a; Ὄ5),contin-
uouslyinvolvingclientsineveryphasesothattheycanunderstand
the progression of the project and be aware of potential missed
deadlines(P6b,P7a,P22a,P23a; /group).Fromtheinterviews,wealso
observe the benefits of managers who understand both software
engineeringandmachinelearningandcanalignproductandmodel
teamstoward common goals (P2a, P6a, P8a, P28a; /group).
6 COLLABORATION POINT: TRAINING DATA
Data is essential for machine learning, but disagreements and frus-
trations around training data were the most common collaboration
challenges mentioned in our interviews. In most organizations, theteam that is responsible for building the model is not the team that
collects, owns, and understands the data, making data a key collab-
oration point between teams in ML-enabled systems development.
6.1 CommonOrganizationalStructures
Weobservedthreepatternsarounddatathatinfluencecollaboration
challengesfrom the perspective of the model team:
Provided data: The product team
hastheresponsibilityofprovidingdata
tothemodelteam(org.6–8,13,18,21,
23).Theproductteamistheinitialpoint
418CollaborationChallengesin Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
of contactfor all data-relatedquestions from themodel team. The
product teammay ownthe dataor acquireit froma separatedata
team(internalorexternal).Coordinationregardingdatatendsto
be distant and formal, and the product team tends to hold more
negotiationpower.
External data: The product team
doesnothavedirectresponsibilityfor
providingdata,butinstead,themodel
teamreliesonexternaldataproviders.
Commonly, the model team (i) uses publicly available resources
(e.g.,academicdatasets,org.2–4,6,19)or(ii)hiresathirdpartyfor
collectingorlabelingdata(org.9,15–17,22,23).Intheformercase,
the model team has little to no negotiation power over data; in the
latter, it can set expectations.
In-housedata: Product,model,and
data teams are all part of the same or-
ganization and the model team relies
on internal data from that organization
(org. 1, 5, 9–12, 14, 20, 24–28). In these cases, both product and
modelteamsoftenfinditchallengingtonegotiateaccesstointernal
dataduetodifferingpriorities,internalpolitics,permissions,and
security constraints.
6.2 NegotiatingData QualityandQuantity
Disagreementsandfrustrationsaroundtrainingdatawerethemost
common collaboration challengesin our interviews. Inalmost ev-
ery project, data scientists were unsatisfied with the quality andquantity of data they received at this collaboration point, in line
with a recent survey showing data availability and management to
be the top-ranked challenge in building ML-enabled systems [5].
Provided and public data is often inadequate (/file_text, /group).In or-
ganizations where data is provided by the product team, the model
team commonly statesthatit is difficult to getsufficient data (P7a,
P8a, P13a, P22a, P22c). The data that they receive is often of low
quality,requiringsignificantinvestmentindatacleaning.Similarto
the requirements challenges discussed earlier, they often state that
the product team has little knowledge or intuition for the amount
and quality of data needed. For example, participant P13a stated
that they were given a spreadsheet with only 50 rows to build a
modelandP7areportedhavingtospendalotoftimeconvincing
theproductteamoftheimportanceofdataquality.Thisalignswith
past observations that software engineers often have little appreci-
ation for data quality concerns [ 49,54,65,77,84] and that training
datais often insufficient and incomplete [6, 43, 56, 77, 83, 93, 106].
When the model team uses public data sources, its members
also have little influence over data quality and quantity and report
significant effort for cleaning low quality and noisy data (P2a, P3a,
P4a, P3c, P6b, P19b, P23a). Papers have similarly questioned the
representativeness and trustworthiness of public training data [ 34,
103,109]as “nobody gets paid to maintain such data” [104].
Training-servingskew isacommonchallengewhentrainingdata
is provided to the model team: models show promising results,but do not generalize to production data because it differs from
providedtrainingdata(P4a,P8a,P13a,P15a,P15c,P21a,P22c,P23a)
[9,23,55,56,77–79,84,100,109,116]. Our interviews show that
this skew often originates from inadequate training data combinedwithunclearinformationaboutproductiondata,andthereforeno
chancetoevaluatewhetherthetrainingdataisrepresentativeof
production data.
Dataunderstandingandaccesstodomainexpertsisabottle-
neck (/file_text, Ὄ5).Existingdatadocumentation(e.g,dataitemdefini-
tions,semantics,schema)isalmostneversufficientformodelteams
to understand the data (also mentioned in a prior study [ 46]). In
the absence of clear documentation, team members often collect
informationandkeeptrackofunwrittendetailsintheirheads(P5a),
knownasinstitutionalortribalknowledge[ 5,40].Dataunderstand-
inganddebuggingofteninvolvemembersfromdifferentteamsand
thuscausechallengesat this collaboration point.
Model teams receiving data from the product team report strug-
glingwithdataunderstandingandhavingadifficulttimegetting
helpfromtheproductteam(orthedatateamthattheproductteam
works with) (P8a, P7b, P13a). As the model team does not have
directcommunicationwiththedatateam,dataunderstandingis-
suesoftencannotberesolvedeffectively.Forexample,P13areports
“Ideally,forusitwouldbesogoodtospendmaybeaweekortwowith
one person continuously trying to understand the data. It’s one ofthe biggest problems actually, because even if you have the person,
if you’re not in contact all the time, then you misinterpreted some
thingsandyoubuildonit.” Thelownegotiationpowerofthemodel
teamin these organizations hinders access to domain experts.
Modelteamsusingpublicdatasimilarlystrugglewithdataun-
derstanding and getting help (P3a, P4a, P19a), relying on sparse
datadocumentation or trying to reach any experts on the data.
Forin-houseprojects,inseveralorganizationsthemodelteam
reliesondatainshareddatabases(org.5,11,26,27,28),collectedby
instrumentinga productionsystem, butsharedby multipleteams.
Several teams shared problems with evolving and often poorly
documenteddatasources,asparticipantP5aillustrates “[datarows]
can have 4,000 features, 10,000 features. And no one really cares.
Theyjustdumpfeaturesthere.[...]Ijustcannottrack10,000features.”Modelteamsfacechallengesinunderstandingdataandidentifyingateamthatcanhelp(P5a,P25a,P20b,P27a),aproblemalsoreported
in a prior study about data scientists at Microsoft [49].
Challengesinunderstandingdataandneedingdomainexperts
are also frequently mentioned in the literature [ 13,40,41,46,49,
65,77,84], as is the danger of building models with insufficient
understanding of the data [ 34,103]. Although we are not aware of
literature discussing the challenges of accessing domain experts,papers have shown that even when data scientists have access,
effective knowledge transfer is challenging [71, 91].
Ambiguitywhenhiringadatateam( /file_text).Whenthemodelteam
hiresanexternaldatateamforcollectingorlabellingdata(org.9,
15, 16, 17, 22, 23), the model team has much more negotiation
power over setting data quality and quantity expectations (though
Kim et al. report that model teams may have difficulty gettingbuy-in from the product team for hiring a data team in the first
place [49]). Our interviews did not surface the same frustrations as
withprovideddataandpublicdata,butinsteadparticipantsfrom
these organizations reported communication vagueness andhidden
assumptions askeychallengesatthiscollaborationpoint(P9a,P15a,
P15c, P16a,P17b, P22a,P22c, P23a). Forexample, P9arelated how
419ICSE’22,May21–29,2022,Pittsburgh,PA, USA NadiaNahar,ShuruiZhou,GraceLewis, and Christian Kästner
differentlabellingcompaniesgiventhesamespecificationwidely
disagreed on labels, when the specification was not clear enough.
Wefoundthatexpectationsbetweenmodelanddatateamsare
often communicated verbally without clear documentation. As
a result, the data team often does not have sufficient context to
understand what data is needed. For example, participant P17b
states“Data collectors can’t understand the data requirements all the
time. Because, when a questionnaire [for data collection] is designed,
the overview of the project is not always described to them. Evenif we describe it, they can’t always catch it.” Reports about low
quality data from hired data teams have been also discussed in the
literature [10, 43, 55, 84, 103, 106].
Need to handle evolving data (/cogs, /group).In most projects, mod-
els need to be regularly retrained with more data or adapted to
changes in the environment (e.g., data drift) [ 42,55,84], which is a
challengefor manymodel teams (P3a, P3c, P5a, P7a-b, P11a, P15c,
P18a,P19b,P22a).Whenproductteamsprovidethedata,theyoften
haveastaticviewandprovideonlyasinglesnapshotofdatarather
than preparing for updates, and model teams with their limited
negotiationpowerhaveadifficulttimefosteringamoredynamic
mindset(P7a-b,P15c,P18a,P22a),asexpressedbyparticipantP15c:
“People don’t understand that for a machine learning project, data
has to be provided constantly.” It can be challenging for a model
teamto convincethe productteam toinvest incontinuousmodel
maintenanceand evolution (P7a, P15c) [46].
Conversely, if data is provided continuously (most commonly
withpublicdatasources,in-housesources,andowndata teams),
model teams struggle with ensuring consistency over time. Data
sources can suddenly change without announcement (e.g., changes
toschema,distributions,semantics),surprisingmodelteamsthat
makebutdonotcheckassumptionsaboutthedata(P3a,P3c,P19b).
Forexample,participantsP5aandP11areportsimilarchallenges
with in-house data, where their low negotiation power does not
allow them to set quality expectations, but they face undesired and
unannounced changes in data sources made by other teams. Most
organizations do not have a monitoring infrastructure to detect
changesin data quality or quantity, as we will discuss in Sec. 7.3.
In-houseprioritiesandsecurityconcernsoftenobstructdata
access (Ὄ5). In in-house projects, we frequently heard about the
productormodelteamstrugglingtoworkwithanotherteamwithin
the same organization that owns the data. Often, these in-house
projects are local initiatives (e.g., logistics optimization) with more
or less buy-in from management and without buy-in from other
teamsthathavetheirownpriorities;sometimesotherteamsexplic-
itly question the business value of the product. The interviewed
model teams usually have little negotiation power to request data
(especiallyifitinvolvescollectingadditionaldata)andalmostnever
get an agreement to continuously receive data in a certain format,
quality,orquantity(P5a,P10a,P11a,P20a-b,P27a)(alsoobservedin
studies at Microsoft, ING and other organizations [ 34,49,65]). For
example, P10a shared “we wanted to ask the data warehouse team to
[provide data], and it was really hard to get resources. They wouldn’t
do that because it was hard to measure the impact [our in-house
project]hadonthebottomlineofthebusiness.” Modelteamsinthese
settingstend to work with whatever data they can get eventually.Security and privacy concerns can also limit access to data (P7a,
P7b, P21a-b, P22a, P24a) [ 46,55,56,65,77], especially when data
is owned by a team in a different organization, causing frustra-
tion,lengthynegotiations,and sometimesexpensive data-handling
restrictions (e.g., no use of cloud resources) for model teams.
Recommendations. Data quality and quantity is important to
model teams, yet they often find themselves in a position of low
negotiationpower,leadingtofrustrationandcollaborationineffi-
ciencies.Modelteamsthathavethefreedomtosetexpectationsand
hiretheirowndatateamsarenoticeablymoresatisfied.Whenplan-
ningtheentireproduct,itseemsimportanttopayspecialattention
tothiscollaborationpoint,andbudgetfordatacollection,accessto domain experts, or even a dedicated data team (
Ὄ5). Explicitly
planningtoprovidesubstantialaccesstodomainexpertsearlyin
theproject was suggested as important (P25a).
We found it surprising that despite the importance of this col-
laborationpointthereislittlewrittenagreementonexpectations
and often limited documentation ( /file_text), even when hiring a dedi-
cateddatateam—instarkcontrasttomoreestablishedcontractsfor
traditionalsoftwarecomponents.Notallorganizationsallowthe
moreagile,constantclosecollaborationbetweenmodelanddata
teams that some suggest [ 77,79]. With a more formal or distant
relationship (e.g., across organizations, teams without buy-in), itseems beneficial to adopt a more formal contract, specifying data
quantity and quality expectations, which are well researched inthe database literature [
58] and have been repeatedly discussed
inthe contextof ML-enabledsystems[ 43,46,49,56,91].This has
also been framed as data requirements in the software engineering
literature[ 83,100,103].Whenworkingwithadedicateddatateam,
participants suggested to invest in making expectations very clear,
forexample,byprovidingprecisespecificationsandguidelines(P9a,
P6b, P28a), running training sessions for the data collectors and
annotators(P17b,P22c),andmeasuringinter-rateragreement(P6b).
Automatedchecksarealsoimportantasdataevolves( /cogs).For
example,participantP13amentionedproactivelysettingupdata
monitoringtodetectproblems(e.g.,schemaviolations,distribution
shifts)atthiscollaborationpoint;apracticesuggestedalsointhe
literature[ 53,56,77,79,84,89,100]andsupportedbyrecenttooling
[e.g.,47,79,86]. The risks regarding possible unnoticed changes to
data make it important to consider data validation andmonitoring
infrastructure asakeyfeatureoftheproductearlyon( /cogs,Ὄ5),as
also emphasized by several participants (P5a, P25a, P26a, P28a).
7 COLLABORATION POINT:
PRODUCT-MODEL INTEGRATION
As discussed earlier, to build an ML-enabled system both ML com-
ponents and traditional non-ML components need to be integrated
anddeployed,requiringdatascientistsandsoftwareengineersto
worktogether,typicallyacrossmultipleteams.Wefoundmanycon-
flicts at this collaboration point, stemming from unclear processes
and responsibilities, as well as differing practices and expectations.
7.1 CommonOrganizationalStructures
We saw large differences among organizations in how engineering
responsibilitieswereassigned,mostvisibleinhowresponsibility
formodel deployment and operation is assigned, which typically
420CollaborationChallengesin Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
involves significant engineering effort for building reproducible
pipelines, API design, or cloud deployment, often with MLOps
technologies. We found the following patterns:
Shared model code: In
some organizations (2, 6, 23,
25),themodelteamisrespon-
sible only for model develop-ment and delivers training code (e.g., in a notebook) or modelfiles to the product team; the product team takes responsibility
for deployment and operation of the model, possibly rewriting the
training code as a pipeline. Here, the model team has little or no
engineeringresponsibilities.
ModelasAPI: Inmostorgani-
zations (18 out of 28), the model
teamisresponsiblefordeveloping
anddeployingthemodel.Hence,
themodel teamrequires substantialengineering skillsin addition
to data science expertise. Here, some model teams are mostly com-
posedofdatascientistswith littleengineeringcapabilities(org.7,
13,17,22,26),someconsistmostlyofsoftwareengineerswhohave
picked up some data science knowledge (org. 4, 15, 16, 18, 19, 21,
24), and others have mixed team members (org. 1, 9, 11, 12, 14, 28).
ThesemodelteamstypicallyprovideanAPItotheproductteam,
orreleaseindividualmodelpredictions(e.g.,sharedfiles,email;org.
17,19,22)or install models directly on servers (org. 4, 9, 12).
All-in-one: If only few people
work on model and product, some-
times a single team (or even a sin-
gleperson)sharesallresponsibilities
(org.3,5,10,20,27).Itcanbeasmallteamwithonlydatascientists
(org.10,20,27)ormixedteamswithdatascientistsandsoftware
engineers(org. 3, 5).
Wealsoobservedtwooutliers:Onestartup(org.8)hadadistinct
model deployment team, allowing the model team to focus on
data science without much engineering responsibility. In one large
organization (org. 28), an engineering-focused model team (model
as API) was supported by a dedicated research team focused on
data-scienceresearch with fewer engineering responsibilities.
7.2 ResponsibilityandCulture Clashes
Interdisciplinary collaboration is challenging (cf. Sec. 2). We ob-
served many conflicts between data science and software engineer-ingculture,madeworsebyunclearresponsibilitiesandboundaries.
Team responsibilities often do not match capabilities and
preferences (/cogs). When the model team has responsibilities re-
quiringsubstantialengineeringwork,weobservedsomedissatis-
faction when its members were assigned undesired responsibilities.
Data scientists preferred engineering support rather than needing
to do everything themselves (P7a-b, 13a), but can find it hard toconvince management to hire engineers (P10a, P20a, P20b). For
exampleP10adescribes “Iwasstrugglingtochangethemindsetof
the team lead, convincing him to hire an engineer...I just didn’t want
this to be my main responsibility.” Especially in small teams, data
scientists report struggling withthe complexity of the typical ML
infrastructure (P7b, P9a, P14a, P26a, P28a).Incontrast,whendeploymentistheresponsibilityofsoftware
engineersintheproductteamorofdedicatedengineersin all-in-
oneteams, some of those engineers report problems integrating
themodelsduetoinsufficientknowledgeonmodelcontextordo-
main, and the model code not being packaged well for deployment
(P20b, P23a, P27a). In several organizations, we heard about soft-
wareengineersperformingMLtaskswithouthavingenoughML
understanding(P5a,P15b-c,P16b,18b,19b,20b).Mirroringobser-
vations from past research [ 111], P5a reports “there are people who
are ML engineers at [company] , but they don’t really understand
ML.Theywereactuallysoftwareengineers...theydon’tunderstand
[overfitting, underfitting, ...]. They just copy-paste code.”
Siloing data scientists fosters integration problems ( /group,Ὄ5).
Weobserveddatascientistsoftenworkinginisolation—knownas
siloing—inalltypesoforganizationalstructures,evenwithinsingle
small teams (see Sec. 4) and within engineering-focused teams.
Insuch settings,datascientists oftenworkin isolationwithweak
requirements(cf.Sec.5.2)withoutunderstandingthelargercontext,
seriouslyengagingwithothersonlyduringintegration(P3a,P3c,
P6a,P7b,P11a,P13a,P15b,P25a)[ 41],whereproblemsmaysurface.
Forexample,participantP11areportedaproblemwhereproduct
and model teams had different assumptions about the expected
inputs and the issue could only be identified after a lot of back and
forthbetween teams at a late stage in the project.
Technicaljargonchallengescommunication(/group). Participants
frequentlydescribedcommunicationissuesarisingfromdiffering
terminologyusedbymembersfromdifferentbackgrounds(P1a-b,
P2a,P3a,P5b,P8a,P12a,P14a-b,P16a,P17a-b,P18a-b,P20a,P22b,
P23a),leadingtoambiguity,misunderstandings,andinconsistent
assumptions (on top of communication challenges with domain
experts)[ 1,46,76,104].P1breports, “Therearealotofconversations
inwhichdisambiguationbecomesnecessary.Weoftenusedifferent
kindsofwordsthatmightbeambiguous.” Forexample,datascien-
tists may refer to prediction accuracy as performance, a term many
software engineers associate with response time. These challenges
can be observed more frequently between teams, but they even
occur within a team with members from different backgrounds
(P3a-c,P20a).
Code quality, documentation, and versioning expectationsdiffer widely and cause conflicts ( /group,/cogs).
Many participants
reported conflicts around development practices between data sci-
entists and software engineers during integration and deployment.
Participants report poor practices that may also be observed in
traditionalsoftwareprojects;butparticularlysoftwareengineers
expressedfrustrationininterviewsthatdatascientistsdonotfollowthesamedevelopmentpracticesorhavethesamequalitystandards
whenitcomestowritingcode.Reportedproblemsrelatetopoor
code quality (P1b, P2a, P3b, P5a, P6a-b, P10a, P11a, P14a, P15b-c,
P17a,P18a,P19a,P20a-b,P26a)[ 9,27,34,37,75,87,106],insufficient
documentation (P5a-b, P6a-b, P10a, P15c, P26a) [ 8,46,64,114], and
notextendingversioncontroltodataandmodels(P3c,P7a,P10a,
P14a, P20b). In two shared-model-code organizations, participants
report having to rewrite code from the data scientists (P2a, P6a-b).
Missingdocumentation for ML code and models is considered the
causefordifferentassumptionsthatleadtoincompatibilitybetween
ML and non-ML components (P10a) and for losing knowledge and
421ICSE’22,May21–29,2022,Pittsburgh,PA, USA NadiaNahar,ShuruiZhou,GraceLewis, and Christian Kästner
eventhemodelwhenfacedwithturnover(P6a-b).Recentpapers
similarlyholdpoor documentation responsible for team decisions
becoming invisible and inadvertently causing hidden assumptions
[34,40,43,46,76,114].HopkinsandBoothcalledmodelanddata
versioning in small companies as desired but “elusive” [40].
Recommendations. Many conflicts relate to boundaries of re-
sponsibility (especially for engineering responsibilities) and to dif-
ferent expectations by team members with different backgrounds.
Better teamstend to define processes, responsibilities, and bound-
aries more carefully ( Ὄ5), document APIs at collaboration points
between teams ( /file_text), and recruit dedicated engineeringsupport for
model deployment ( /cogs), but also establish a team culture with mu-
tual understanding and exchange ( /group). Big tech companies usually
have more established processes and clearer responsibility assign-
ments than smaller organizations and startups that often follow
ad-hoc processes or figure out responsibilities as they go.
TheneedforengineeringskillsforMLprojectshasfrequently
been discussed [ 5,67,87,90,96,112,116], but our interviewees
differ widely in whether all data scientists should have substantial
engineering responsibilities or whether engineers should support
datascientistssothattheycanfocusontheircoreexpertise( /cogs).
Especially interviewees from big tech emphasized that they expect
engineering skills from all data science hires (P28a). Others empha-
sizedthatrecruitingsoftwareengineersandoperationsstaffwith
basic data-science knowledge can help at many communication
and integration tasks, such as converting experimental ML codefor deployment (P2a, P3b), fostering communication (P3c, P25a),
and monitoring models in production (P5b). Generally, siloing data
scientists is widely recognized as problematic and many intervie-
wees suggest practices for improving communication ( /group), such as
trainingsessionsforestablishingcommonterminology(P11a,P17a,
P22a, P22c, P23a), weekly all-hands meetings to present all tasks
andsynchronize(P2a,P3c,P6b,P11a),andproactivecommunica-
tiontobroadcastupcomingchangesindataorinfrastructure(P11a,
P14a, P14b). This mirrors suggestions to invest in interdisciplinary
training[5,48, 49,69,76, 112]andproactive communication [54].
7.3 QualityAssurance for Model and Product
During development and integration, questions of responsibility
forqualityassurancefrequently arise, oftenrequiring coordinationandcollaborationbetweenmultipleteams.Thisincludesevaluating
components individually (including the model) as well as their
integrationand thewholesystem, oftenincludingevaluating and
monitoringthe system online(in production).
Model adequacy goals are difficult to establish (/file_text, /group).Off-
line accuracy evaluation of models is almost always performed by
themodelteamresponsibleforbuildingthemodel,thoughoften
theyhavedifficultydecidinglocallywhenthemodelisgoodenough
(P1a, P3a, P5a, P6a, P7a, P15b, P16b, P23a) [ 34,44]. As discussed
in Sec. 5 and Sec. 6, model team members often receive little guid-
anceonmodeladequacycriteriaandareunsureabouttheactual
distribution of production data. They also voice concerns aboutestablishing ground truth, for example, needing to support data
for different clients,and hence not beingable to establish (offline)
measuresformodelquality(P1b,P16b,P18a,P28a).Asqualityre-
quirementsbeyondaccuracyarerarelyprovidedformodels,modelteamsusuallydonotfeelresponsiblefortestinglatency,memory
consumption,orfairness(P2a,P3c,P4a,P5a,P6b,P7a,P14a,P15b,
P20b).Whereasliteraturediscussedchallengesinmeasuringbusi-
nessimpactofamodel[ 10,14,43,49]andbalancingbusinessgoals
with model goals [ 73], interviewed data scientists were concerned
about this only with regards to convincing clients, managers or
product teams to provide resources (P7a-b, P10a, P26a, P27a).
Limited confidence without transparent model evaluation
(/file_text).Participants in several organizations report that model teams
donotprioritizemodelevaluationandhavenosystematicevalua-
tionstrategy(especiallyiftheydonothaveestablishedadequacy
criteria they try to meet), performing occasional “ad-hoc inspec-
tions” instead (P2a, P15b, P16b, P18b, P19b, P20b, P21b, P22a, P22b).
Without transparency about their test processes and test results,
other teams voiced reduced confidence in the model, leading to
skepticismto adopt the model (P7a, P10a, P21b, P22a).
Unclear responsibilities for system testing (Ὄ5). Teams often
strugglewithtestingtheentireproductafterintegratingMLand
non-MLcomponents.Modelteamsfrequentlyexplicitlymentioned
thattheyassumenoresponsibilityforproductquality(including
integration testing and testing in production) and have not been
involved in planning for system testing, but that their responsibili-
ties end with delivering a model evaluated for accuracy (P3a, P14a,
P15b,P25a,P26a). However,insev eralorganizations,productteams
also did not plan for testing the entire system with the model(s)
and, at most, conducted system testing in an ad-hoc way (P2a, P6a,
P16a,P18a,P22a).Recentliteraturehasreportedasimilarlackof
focus on system testing in product teams [ 13,114], mirroring also
a focus in academic research on testing models rather than testing
the entire system [ 10,20]. Interestingly, some established software
developmentorganizationsdelegatedtestingtoanexistingseparate
qualityassuranceteamwithnoprocessorexperiencetestingML
products (P2a, P8a, P16a, P18b, P19a).Planning for online testing and monitoring is rare (Ὄ5, /cogs,
/group).
Duetopossibletraining-servingskewanddatadrift,literature
emphasizestheneed foronline evaluation [4, 10, 13, 14,23, 42, 44,
47,51,65,87,88,90,103].Withcollectedtelemetry,onecanusually
approximate both product and model quality, monitor updates,
and experiment in production [ 14]. Online testing usually requires
coordinationamongmultipleteamsresponsibleforproduct,model,
andoperation.Weobservedthatmostorganizationsdo notperform
monitoringoronlinetesting,asitisconsidereddifficult,inaddition
to lack of standard process, automation, or even test awareness
(P2a, P3a, P3b, P4a, P6b, P7a, P10a, P15b, P16b, P18b, P19b, 25a,
P27a).Only11 out of 28 organizations collected any telemetry; it is
most established in big tech organizations. When to retrain models
isoftendecidedbasedonintuitionormanualinspection,though
many aspire to more automation (P1a, P3a, P3c, P5a, P10a, P22a,P25a, P27a). Responsibilities around online evaluation are often
neitherplanned nor assigned upfront as part of the project.
Most model teams are aware of possible data drift, but many do
nothaveanymonitoringinfrastructurefordetectingandmanaging
drift in production. If telemetry is collected, it is the responsibility
of the product or operations team and it is not always accessible to
the model team. Four participants report that they rely on manual
feedback about problems from the product team (P1a, P3a, P4a,
422CollaborationChallengesin Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
P10a). Atthe sametime, others reportthat productand operation
teams do not necessarily have sufficient data science knowledge to
provide meaningful feedback (P3a, P3b, P5b, P18b, P22a) [82].
Recommendations. Qualityassuranceinvolvesmultipleteams
andbenefitsfromexplicitplanningandmakingitahighpriority
(Ὄ5).Whiletheproductteamshouldlikelytakeresponsibilityfor
productqualityandsystemtesting,suchtestingofteninvolvesbuild-
ing monitoring and experimentation infrastructure ( /cogs), which re-
quiresplanningandcoordinationwithteamsresponsibleformodel
development,deployment,andoperation(ifseparate)toidentify
therightmeasures.Modelteamsbenefitfromreceivingfeedback
ontheirmodelfromproductionsystems,butsuchsupportneeds
to be planned explicitly, with corresponding engineering effort as-
signed and budgeted, even in organizations following a model-first
trajectory.Wesuspectthateducationaboutbenefitsoftestingin
production and common infrastructure (often under the label Dev-
Ops/MLOps [ 59]) can increase buy-in from all involved teams ( /group).
Organizations thathave establishedmonitoring and experimenta-
tioninfrastructure strongly endorse it (P5a, P25a, P26a, P28a).
Definingclearqualityrequirementsformodelandproductcan
helpall teamstofocus theirquality assuranceactivities(cf. Sec.5;
/file_text).Evenwhenitischallengingtodefineadequacycriteriaupfront,
teams can together develop a quality assurance plan for model and
product.Participants andliterature emphasizedtheimportance of
human feedback to evaluate model predictions (P11a, P14a) [ 88],
which requires planning to collect such feedback ( Ὄ5). System and
usability testing may similarly require planning for user studies
withprototypes and shadow deployment [89, 100, 109].
8 DISCUSSION AND CONCLUSIONS
Throughourinterviewsweidentifiedthreecentralcollaboration
points where organizations building ML-enabled systems face sub-
stantialchallenges:(1)requirementsandprojectplanning,(2)train-
ing data, and (3) product-model integration. Other collaboration
points surfaced, but were mentioned far less frequently (e.g., inter-
action with legal experts and operators), did not relate to problems
betweenmultipledisciplines(e.g.,datascientistsdocumentingtheir
work for other data scientists), or mirrored conventional collabora-
tion in software projects (e.g., many interviewees wanted to talk
about unstable ML libraries and challenges interacting with teams
building and maintaining such libraries, though the challenges
largelymirrored those of library evolution generally [16, 31]).
Data scientists and software engineers are certainly not the first
to realize that interdisciplinary collaborations are challenging and
fraught with communication and cultural problems [ 21], yet it
seemsthatmanyorganizationsbuildingML-enabledsystemspay
littleattentionto fostering better interdisciplinary collaboration.
Organizationsdifferwidelyintheirstructuresandpractices,and
someorganizationshavefound strategies that work for them(see
recommendation sections). Yet, we find that most organizations do
notdeliberatelyplantheirstructuresandpracticesandhavelittle
insight into available choices and their tradeoffs. We hope that this
work can (1) encourage more deliberation about organization and
processatkeycollaborationpoints,and(2)serveasastartingpoint
forcatalogingand promoting best practices.Beyond the specific challenges discussed throughout this paper,
we see four broad themes that benefit from more attention both in
engineering practiceandin research:
/groupCommunication: Manyissuesarerootedinmiscommunica-
tion between participants with different backgrounds. To facilitate
interdisciplinarycollaboration,educationiskey,includingMLliter-
acy for software engineers and managers (and even customers) but
also training data scientists to understand software engineering
concerns. The idea of T-shaped professionals [ 102] (deep expertise
inonearea,broadknowledgeofothers)canprovideguidancefor
hiringandtraining.
/file_textDocumentation: Clearlydocumentingexpectationsbetween
teamsisimportant.Traditionalinterfacedocumentationfamiliar
to software engineers may be a starting point, but practices for
documenting model requirements (Sec. 5.2), data expectations (Sec.
6.2), and assured model qualities (Sec. 7.3) are not well established.
Recent suggestions like model cards [ 64], and FactSheets [ 8]a r e
a good starting point for encouraging better, more standardized
documentation of ML components. Given the interdisciplinary na-
ture at these collaboration points, such documentation must be
understoodbyallinvolved–theoriesof boundaryobjects [2]may
helpto develop better interface description mechanisms.
/cogsEngineering: With attention focused on ML innovations,
many organizations seem to underestimate the engineering ef-fort required to turn a model into a product to be operated and
maintainedreliably.Arguablyadoptingmachinelearningincreases
software complexity[ 48,69,87] andmakes engineeringpractices
suchasdataqualitychecks,deploymentautomation,andtestingin
production even more important. Project managers should ensure
that the ML and the non-ML parts of the project have sufficient
engineeringcapabilitiesandfosterproductandoperationsthinking
from the start.
Ὄ5Process: Finally,machinelearningwithitsmorescience-like
process challenges traditional software process life cycles. It seems
clear that product requirements cannot be established without in-
volving data scientists for model prototyping, and often it may
be advisable to adopt a model-first trajectory to reduce risk. But
while a focus on the product and overall process may cause delays,
neglectingitentirelyinvites thekind ofproblems reportedby our
participants. Whether it may look more like the spiral model or
agile [22], more research into integrated process life cycles for ML-
enabled systems (covering software engineering and data science)
isneeded.
Acknowledgements. Kästner’sandNahar’sworkwassupported
in part by NSF grants NSF award 1813598 and 2131477. Zhou’s
workwassupportedinpartbyNaturalSciencesandEngineering
Research Council of Canada (NSERC), RGPIN2021-03538. Lewis’
work was funded and supported by the Department of Defense
under Contract No. FA8702-15-D-0002 with Carnegie Mellon Uni-
versity (CMU) for the operation of the Software Engineering Insti-
tute (SEI), a federally funded research and development center. We
would thank all our interview participants (K M Jawadur Rahman,
MiguelJette,andanonymousothers)andthepeoplewhohelped
us connect with them.
423ICSE’22,May21–29,2022,Pittsburgh,PA, USA NadiaNahar,ShuruiZhou,GraceLewis, and Christian Kästner
REFERENCES
[1]Aho, T., Sievi-Korte, O., Kilamo, T., Yaman, S. and Mikkonen, T., 2020. Demysti-
fyingdatascienceprojects:Alookonthepeopleandprocessofdatascience
today.InProc.Int’lConf.Product-FocusedSoftwareProcessImprovement,153-167.
[2]Akkerman, S.F. and Bakker, A. 2011. Boundary Crossing and Boundary Objects.
Review of educational research. 81, 2, 132–169.
[3]Akkiraju, R., Sinha, V., Xu, A., Mahmud, J., Gundecha, P., Liu, Z., Liu, X. and
Schumacher,J.2020.CharacterizingMachineLearningProcesses:AMaturity
Framework. BusinessProcess Management, 17–31.
[4]Ameisen, E. 2020. Building Machine Learning Powered Applications: Going from
Ideato Product. O’Reilly Media, Inc.
[5]Amershi, S. et al. 2019. Software Engineering for Machine Learning: A Case
Study.In Proc. of 41st Int’l Conf.on Software Engineering: Software Engineering
in Practice (ICSE-SEIP), 291–300.
[6]Amershi, S., Chickering, M., Drucker, S.M., Lee, B., Simard, P. and Suh, J. 2015.
ModelTracker: Redesigning Performance Analysis Tools for Machine Learning.
In Proc. of 33rd Conf. on Human Factors in Computing Systems, 337–346.
[7]Amershi, S. et al. 2019. Guidelines for Human-AI Interaction. In Proc. of CHI
Conf.on Human Factors in Computing Systems, 1–13.
[8]Arnold,M.etal. 2019.FactSheets:IncreasingtrustinAIservicesthrough sup-
plier’s declarations of conformity. IBM Journal of Research and Development,
63.
[9]Arpteg,A.,Brinne,B.,Crnkovic-Friis,L.andBosch,J.2018.SoftwareEngineering
Challenges of Deep Learning. In Proc. Euromicro Conf. Software Engineering and
Advanced Applications (SEAA), 50–59.
[10]Ashmore,R.,Calinescu,R.andPaterson,C.2021.AssuringtheMachineLearningLifecycle:Desiderata,Methods,andChallenges. ACMComputingSurveys(CSUR),
54 (5): 1-39.
[11]Bass, L., Clements, P. and Kazman, R. 1998. Software Architecture in Practice.
Addison-Wesley Longman Publishing Co., Inc.
[12]Bass, M., Herbsleb, J.D. and Lescher, C. 2009. A Coordination Risk AnalysisMethod for Multi-site Projects: Experience Report. In Proc. Int’l Conf. Global
Software Engineering, 31–40.
[13]Baylor, D., Breck, E., Cheng, H.T., Fiedel, N., Foo, C.Y. et al. 2017. TFX: A
TensorFlow-Based Production-ScaleMachineLearningPlatform. InProc.Int’l
Conf.KnowledgeDiscovery and Data Mining, 1387-1395.
[14]Bernardi, L., Mavridis, T. and Estevez, P. 2019. 150 successful machine learning
models.In Proc. Int’l Conf. Knowledge Discovery & Data Mining, 1743-1751.
[15]Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A., Jia, Y., Ghosh, J., Puri, R.,
Moura,J.M.F.andEckersley,P.2020.Explainablemachinelearningindeploy-
ment.In Proc. of Conf. on Fairness, Accountability, and Transparency, 648–657.
[16]Bogart, C., Kästner, C., Herbsleb, J. and Thung, F. 2021. When and how to make
breakingchanges:Policiesandpracticesin18opensourcesoftwareecosystems.
ACM Transactions on Software Engineering and Methodology. 30, 4, 1–56.
[17]Borg, M. et al. 2019. Safely Entering the Deep: A Review of Verification and
Validation for Machine Learning and a Challenge Elicitation in the Automotive
Industry. Journalof Automotive Software Engineering. 1, 1, 1–9.
[18]Bosch,J.,Olsson,H.H.andCrnkovic,I.2021.EngineeringAISystems:AResearch
Agenda.ArtificialIntelligenceParadigmsforSmartCyber-PhysicalSystems .IGI
Global.1–19.
[19]Boujut, J.-F. and Blanco, E. 2003. Intermediary Objects as a Means to FosterCo-operation in Engineering Design. Computer Supported Cooperative Work
(CSCW). 12, 2, 205–219.
[20]Braiek,H.B.andKhomh,F.2020.Ontestingmachinelearningprograms. Journal
of Systems and Software. 164, 110542.
[21]Brandstädter,S.andSonntag,K.2016.InterdisciplinaryCollaboration. Advances
in Ergonomic Design of Systems, Products and Processes, 395–409.
[22]Braude, Eric J and Bernstein, Michael E. 2011. Software Engineering: Modern
Approaches 2nd Edition. Wiley. ISBN-13:978-0471692089.
[23]Breck,E.,Cai,S.,Nielsen,E.,Salib,M.andSculley,D.2017.TheMLtestscore:
ArubricforMLproductionreadinessandtechnicaldebtreduction. InProc.of
Int’lConf.on Big Data (Big Data), 1123–1132.
[24]Brown,G.F.C.1995. Factorsthatfacilitateorinhibitinterdisciplinarycollaboration
withina professional bureaucracy. University of Arkansas.
[25]Cai,C.J.,Winter,S.,Steiner,D.,Wilcox,L.andTerry,M.2019.“helloAI”:Uncov-eringtheonboardingneedsofmedicalpractitionersforhuman-AIcollaborative
decision-making. In Proc. Human-Computer Interaction. 3, CSCW, 1–24.
[26]Cataldo,M.etal.2006.IdentificationofCoordinationRequirements:Implications
fortheDesignofCollaboration andAwarenessTools. InProc.Conf.Computer
Supported Cooperative Work (CSCW), 353–362.
[27]Chattopadhyay,S.,Prasad,I.,Henley,A.Z.,Sarma,A.andBarik,T.2020.What’s
Wrong with Computational Notebooks? Pain Points, Needs, and Design Oppor-
tunities.In Proc. Conf. Human Factors in Computing Systems (CHI), 1–12.
[28]Cheng, D., Cao, C., Xu, C. and Ma, X. 2018. Manifesting Bugs in Machine
Learning Code: An Explorative Study with Mutation Testing. In Proc. Int’l Conf.
Software Quality, Reliability and Security (QRS), 313–324.[29]Chen, Z., Cao, Y., Liu, Y., Wang, H., Xie, T. and Liu, X. 2020. Understanding
ChallengesinDeployingDeepLearningBasedSoftware:AnEmpiricalStudy.
arXiv2005.00760.
[30] Conway, M.E. 1968. How Do Committees Invent? Datamation. 14, 4, 28–31.
[31]Cossette, B.E. and Walker, R.J. 2012. Seeking the Ground Truth: A Retroac-
tiveStudyontheEvolutionandMigrationofSoftwareLibraries. InProc.Int’l
SymposiumFoundationsof Software Engineering (FSE), 1–11.
[32]Curtis,B.,Krasner,H.andIscoe,N.1988.Afieldstudyofthesoftwaredesign
process for large systems. Communicationsof the ACM. 31, 11, 1268–1287.
[33]Dabbish,L.,Stuart,C.,Tsay,J.andHerbsleb,J.2012.SocialCodinginGitHub:
Transparency and Collaboration in an Open Software Repository. In Proc. Conf.
ComputerSupported Cooperative Work (CSCW), 1277–1286.
[34]Haakman, M., Cruz, L., Huijgens, H. and van Deursen, A. 2021. AI Lifecycle
ModelsNeedToBeRevised.AnexploratorystudyinFintech. EmpiricalSoftware
Engineering. 26, 5, 1–29.
[35]Haakman, M., Cruz, L., Huijgens, H. and van Deursen, A. 2020. AI Lifecycle
Models Need To Be Revised. An Exploratory Study in Fintech. arXiv 2010.02716.
[36]Harsh, S. 2011. Purposeful Sampling in Qualitative Research Synthesis. Qualita-
tive Research Journal. 11, 2, 63–75.
[37]Head,A.,Hohman,F.,Barik,T.,Drucker,S.M.andDeLine,R.2019.Managing
messes in computational notebooks. In Proc. Conf. Human Factors in Computing
Systems(CHI), 1-12.
[38]Herbsleb, J.D. and Grinter, R.E. 1999. Splitting the Organization and Integrating
the Code: Conway’s Law Revisited. In Proc. Int’l Conf. Software Engineering
(ICSE), 85–95.
[39]Holstein,K.etal..2019.ImprovingFairnessinMachineLearningSystems:What
Do Industry Practitioners Need? In Proc. Conf. Human Factors in Computing
(CHI) Systems, 1–16.
[40]Hopkins, A. and Booth, S. 2021. Machine learning practices outside big tech:
How resource constraints challenge responsible development. In Proc. Conf. on
AI,Ethics,and Society, 134-145.
[41]Hukkelberg,I.andRolland,K.2020.ExploringMachineLearninginaLargeGov-
ernmentalOrganization:AnInformationInfrastructurePerspective. European
Conf.on Information Systems, 92.
[42]Hulten, G. 2019. Building Intelligent Systems: A Guide to Machine Learning
Engineering. Apress.
[43]Humbatova, N. et al. 2020. Taxonomy of real faults in deep learning systems. In
Proc. Int’l Conf. on Software Engineering (ICSE), 1110-1121.
[44]Ishikawa, F. and Yoshioka, N. 2019. How do engineers perceive difficulties
inengineeringofmachine-learningsystems?-Questionnairesurvey. InProc.
Int’lWorkshop onConducting EmpiricalStudies inIndustry(CESI) andSoftware
EngineeringResearch and Industrial Practice (SER&IP), 2-9.
[45]Islam, M.J., Nguyen, H.A., Pan, R. and Rajan, H. 2019. What Do Developers
Ask About ML Libraries? A Large-scale Study Using Stack Overflow. arXiv
1906.11940.
[46]Kandel, S., Paepcke, A., Hellerstein, J.M. and Heer, J. 2012. Enterprise data
analysisandvisualization:Aninterviewstudy. IEEETransactionsonVisualization
and ComputerGraphics. 18, 12, 2917–2926.
[47]Kang, D., Raghavan, D., Bailis, P. and Zaharia, M. 2020. Model Assertions for
MonitoringandImprovingMLModels. InProc.ofMachineLearningandSystems,
2, 481-496.
[48]Kästner,C.andKang,E.2020.TeachingSoftwareEngineeringforAl-Enabled
Systems.InProc.Int’lConf.SoftwareEngineering:SoftwareEngineeringEducation
and Training (ICSE-SEET), 45–48.
[49]Kim, M., Zimmermann, T., DeLine, R. and Begel, A. 2018. Data Scientists in
Software Teams: State of the Art and Challenges. IEEE Transactions on Software
Engineering. 44, 11, 1024–1038.
[50]Kuwajima,H.,Yasuoka,H.andNakae,T.2020.Engineeringproblemsinmachine
learningsystems. MachineLearning, 109, no 5, 1103-1126.
[51]Lakshmanan, V., Robinson, S. and Munn, M. 2020. Machine Learning Design
Patterns. O’Reilly Media, Inc.
[52]Lewis, G.A., Bellomo, S. and Ozkaya, I. 2021. Characterizing and DetectingMismatch in Machine-Learning-Enabled Systems. In Proc. Workshop on AI
Engineering-Software Engineering for AI (WAIN), 133-140.
[53]Lewis,G.A.,Ozkaya,I.andXuX.2021.SoftwareArchitectureChallengesfor
ML Systems. In Proc. Int’l Conf. on Software Maintenance and Evolution , 634-638.
[54]Li, P.L., Ko, A.J. and Begel, A. 2017. Cross-Disciplinary Perspectives on Collab-
orations with Software Engineers. In Proc. Int’l Workshop on Cooperative and
HumanAspects of Software Engineering (CHASE), 2–8.
[55]Lwakatare,L.E.,Raj,A.,Bosch,J.,Olsson,H.H.andCrnkovic,I.2019.Ataxonomy
of software engineering challenges for machine learning systems: An empirical
investigation. In Proc. Int’l Conf. Agile Software Development, 227–243.
[56]Lwakatare, L.E., Raj, A., Crnkovic, I., Bosch, J. and Olsson, H.H. 2020. Large-
scalemachinelearningsystemsinreal-worldindustrialsettings:Areviewof
challengesand solutions. Informationandsoftware technology . 127, 106368.
[57]Madaio,M.A.etal.2020.Co-DesigningCheckliststoUnderstandOrganizational
Challenges and Opportunities around Fairness in AI. In Proc. Conf. Human
Factorsin Computing Systems (CHI), 1–14.
424CollaborationChallengesin Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
[58]Mahanti,R.2019. DataQuality:Dimensions,Measurement,Strategy,Management,
andGovernance. Quality Press.
[59]Mäkinen, S., Skogström, H., Laaksonen, E. and Mikkonen,T. 2021. Who Needs
MLOps:WhatDataScientistsSeektoAccomplishandHowCanMLOpsHelp? In
Proc. Workshop on AI Engineering-Software Engineering for AI (WAIN), 109-112.
[60]Martínez-Fernández, S., Bogner, J., Franch, X., Oriol, M., Siebert, J., Trendowicz,
A., Vollmer, A.M. and Wagner, S. 2021. Software Engineering for AI-Based
Systems:A Survey. arXiv2105.01984.
[61]Martinez-Plumed, F., Contreras-Ochando, L., Ferri, C., Hernandez Orallo, J.,
Kull, M., Lachiche, N., Ramirez Quintana, M.J. and Flach, P.A. 2021. CRISP-DM
twentyyearslater:Fromdataminingprocessestodatasciencetrajectories. IEEE
Transactions on Knowledge and Data Engineering. 33, 8, 3048–3061.
[62] Meyer, B. 1997. Object-Oriented Software Construction. Prentice-Hall.
[63]Mistrík,I.,Grundy,J.,vanderHoek,A.andWhitehead,J.2010. Collaborative
Software Engineering. Springer.
[64]Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B.,
Spitzer,E.,Raji,I.D.andGebru,T.2019.ModelCardsforModelReporting. In
Proc. Conf. Fairness, Accountability, and Transparency, 220–229.
[65]Muiruri, D., Lwakatare, L. E., K Nurminen, J. and Mikkonen, T. 2021. Practices
and Infrastructures for ML Systems–An Interview Study. TechRxiv 16939192.v1.
[66]Nahar, N., Zhou, S., Lewis, G. and Kästner, C. 2021. Collaboration Challenges in
Building ML-Enabled Systems: Communication, Documentation, Engineering,
andProcess. arXiv2110.10234.
[67]O’Leary, K. and Uchida, M. 2020. Common problems with creating machine
learning pipelines from existing code. In Proc. Conf. Machine Learning and
Systems(MLSys).
[68]Ovaska, P., Rossi, M. and Marttiin, P. 2003. Architecture as a coordination tool
in multi-site software development. Software Process Improvement and Practice.
8, 4, 233–247.
[69]Ozkaya, I. 2020. What Is Really Different in Engineering AI-Enabled Systems?
IEEESoftware. 37, 4, 3–6.
[70]Panetta, K. 2020. Gartner Identifies the Top Strategic Technology Trends for
2021.URL: https://www.gartner.com/smarterwithgartner/gartner-top-strategic-
technology-trends-for-2021.
[71] Park, S., Wang, A., Kawas, B., Vera Liao, Q., Piorkowski, D. and Danilevsky, M.
2021.FacilitatingKnowledgeSharingfromDomainExpertstoDataScientists
forBuildingNLPModels. InProc.26thInt’lConf.onIntelligentUserInterfaces,
585-596.
[72]Parnas, D.L. 1972. On the Criteria to be used in Decomposing Systems into
Modules. Communicationsof the ACM. 15, 12, 1053–1058.
[73]Passi,S.,andPhoebeS.2020.MakingDataScienceSystemsWork. BigData&
Society7 (2): 1-13.
[74]Patel, K., Fogarty, J., Landay, J.A. and Harrison, B. 2008. Investigating statistical
machine learning as a tool for software development. In Proc. Conf. Human
Factorsin Computing Systems (CHI), 667–676.
[75]Pimentel, J.F., Murta,L., Braganholo, V. and Freire, J. 2019.A large-scale study
about quality andreproducibility of Jupyter notebooks. In Proc. 16thInt’l Conf.
on Mining Software Repositories (MSR), 507-517.
[76]Piorkowski, D. et al. 2021. How AI Developers Overcome Communication Chal-
lenges in a Multidisciplinary Team: A Case Study. In Proc. ACM on Human-
ComputerInteraction, 5, (CSCW1), 1-25.
[77]Polyzotis, N., Roy, S., Whang, S.E. and Zinkevich, M. 2018. Data Lifecycle Chal-
lengesin Production Machine Learning: A Survey. SIGMODRec. 47, 2, 17–28.
[78]Polyzotis,N.,Roy,S.,Whang,S.E.andZinkevich,M.2017.DataManagement
Challenges in Production Machine Learning. In Proc. Int’l Conf. on Management
of Data, 1723–1726.
[79]Polyzotis, N., Zinkevich, M., Roy, S., Breck, E. and Whang, S. 2019. Data valida-
tionformachinelearning. In Proc. Machine Learning and Systems, 334–347.
[80]Rahimi, M., Guo, J.L.C., Kokaly, S. and Chechik, M. 2019. Toward Requirements
Specification for Machine-Learned Components. In Proc. Int’l Requirements
EngineeringConf.Workshops (REW), 241–244.
[81]Rakova,B.,Yang,J.,Cramer,H.andChowdhury,R.2021.WhereResponsibleAI
meetsReality:PractitionerPerspectivesonEnablersforShiftingOrganizational
Practices. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, 1–23.
[82]Ré, C., Niu, F., Gudipati, P. and Srisuwananukorn, C. 2019. Overton: A data sys-
tem for monitoring and improving machine-learned products. arXiv 1909.05372.
[83]Salay,R.,Queiroz,R.andCzarnecki,K.2017.AnAnalysisofISO26262:Using
MachineLearningSafely in Automotive Software. arXiv1709.02435.
[84]Sambasivan, N. et al. 2021. “Everyone wants to do the model work, not the data
work”: Data Cascades in High-Stakes AI. In Proc. Conf. on Human Factors in
ComputingSystems(CHI). 1–15.
[85]Sarma,A.,Redmiles,D.F.andvanderHoek,A.2012.Palantir:EarlyDetectionof
Development Conflicts Arising from Parallel Code Changes. IEEE Transactions
on Software Engineering. 38, 4, 889–908.
[86]Schelter,Setal.2018.AutomatingLarge-scaleDataQualityVerification. Proc.
VLDBEndowmentInt’lConf.Very Large Data Bases. 11, 12, 1781–1794.
[87]Sculley, D. et al. 2015. Hidden Technical Debt in Machine Learning Systems.
Advances in Neural Information Processing Systems 28. 2503–2511.[88]Sculley,D.,Otey,M.E.,Pohl,M.,Spitznagel,B.,Hainsworth,J.andZhou,Y.2011.
Detecting adversarial advertisements in the wild. In Proc. Int’l Conf. Knowledge
Discovery and Data Mining, 274–282.
[89]Sendak, M.P. et al. 2020. Real-World Integration of a Sepsis Deep Learning
Technology Into Routine Clinical Care: Implementation Study. JMIR medical
informatics. 8, 7, e15182.
[90]Serban, A., van der Blom, K., Hoos, H. and Visser, J. 2020. Adoption and Ef-
fects of Software Engineering Best Practices in Machine Learning. In Proc. Int’l
Symposiumon Empirical Software Engineering and Measurement, 1–12.
[91]Seymoens,T.,Ongenae,F.andJacobs,A.2018.Amethodologytoinvolvedomain
experts and machine learning techniques in the design of human-centered
algorithms. In Proc. IFIP Working Conf. Human Work Interaction Design, 200-214.
[92]Shneiderman, B. 2020. Bridging the gap between ethics and practice. ACM
Transactions on Interactive Intelligent Systems. 10, 4, 1–31.
[93]Siebert, J., Joeckel, L., Heidrich, J., Nakamichi, K., Ohashi, K., Namba, I., Ya-
mamoto, R. and Aoyama, M. 2020. Towards Guidelines for Assessing Qualities
ofMachineLearningSystems. InProc.Int’lConf.ontheQualityofInformation
and CommunicationsTechnology, 17–31.
[94]Singh, G., Gehr, T., Püschel, M. and Vechev, M. 2019. An abstract domain for
certifyingneuralnetworks. Proc. ACM Program. Lang. 3, POPL, 1–30.
[95]Smith,D.,Alshaikh,A.,Bojan,R.,Kak,A.andManesh,M.M.G.2014.Overcoming
barrierstocollaborationinanopensourceecosystem. TechnologyInnovation
ManagementReview.4 ,1 .
[96]d. S. Nascimento, E. et al. 2019. Understanding Development Process of Ma-
chine Learning Systems: Challenges and Solutions. In Proc. Int’l Symposium on
EmpiricalSoftware Engineering and Measurement (ESEM), 1–6.
[97]de Souza, C.R.B. and Redmiles, D.F. 2008. An Empirical Study of SoftwareDevelopers’ Management of Dependencies and Changes. In Proc. Int’l Conf.
Software Engineering (ICSE), 241–250.
[98]Strauss,A.andCorbin,J.1994.Groundedtheorymethodology:Anoverview.
Handbook of qualitative research. N.K. Denzin, ed. 273–285.
[99]Strauss, A. and Corbin, J.M. Basics of Qualitative Research: Grounded Theory
Procedures and Techniques. SAGE Publications.
[100]Studer, S. et al. 2021. Towards CRISP-ML(Q): A Machine Learning Process
Model with Quality Assurance Methodology. Machine Learning and Knowledge
Extraction, 3(2), 392-413.
[101]Tramèr, F. et al. 2017. FairTest: Discovering Unwarranted Associations in Data-
DrivenApplications. InProc.EuropeanSymposiumonSecurityandPrivacy(EuroS
P), 401–416.
[102]Tranquillo, J. 2017. The T-Shaped Engineer. Journal of Engineering Education
Transformations. 30, 4, 12–24.
[103]Vogelsang,A.andBorg,M.2019.RequirementsEngineeringforMachineLearn-
ing:PerspectivesfromDataScientists. InProc.Int’lRequirementsEngineering
Conf.Workshops (REW), 245–251.
[104] Wagstaff, K. 2012. Machine Learning that Matters. arXiv1206.4656.
[105]Wang,A.Y.,Mittal,A.,Brooks,C.andOney,S.2019.HowDataScientistsUse
Computational Notebooks for Real-Time Collaboration. Proc. Human-Computer
Interaction. 3, CSCW, 39.
[106]Wan, Z., Xia, X., Lo, D. and Murphy, G.C. 2019. How does Machine Learn-ing Change Software Development Practices? IEEE Transactions on Software
Engineering, 47(9), 1857-1871.
[107]Waterman, M., Noble, J. and Allan, G. 2015. How Much Up-Front? A Grounded
theory of AgileArchitecture. In Proc.Int’l Conf. Software Engineering, 347–357.
[108]Staff, V. B. 2019. Why do 87% of data science projects never make it into pro-
duction? URL:https://venturebeat.com/2019/07/19/why-do-87-of-data-science-
projects-never-make-it-into-production/ .
[109]Wiens, J., et al. 2019. Do no harm: A roadmap for responsible machine learning
for healthcare. Nature medicine. 25, 9, 1337–1340.
[110]Xie,X.,Ho,J.W.K.,Murphy,C.,Kaiser,G.,Xu,B.andChen,T.Y.2011.Testing
and Validating Machine Learning Classifiers by Metamorphic Testing. Journal
of Systems and Software. 84, 4, 544–558.
[111]Yang,Q.,Suh,J.,Chen,N.-C.andRamos,G.2018.GroundingInteractiveMachineLearningToolDesigninHowNon-ExpertsActuallyBuildModels. InProc.Conf.
DesigningInteractive Systems, 573–584.
[112]Yang, Q. The role of design in creating machine-learning-enhanced user experi-
ence.In Proc. AAAI Spring Symposium Series, 406-411.
[113]Yokoyama,H.2019.MachineLearningSystemArchitecturalPatternforImprov-ingOperationalStability. InProc.Int’lConf.onSoftwareArchitectureCompanion
(ICSA-C), 267–274.
[114]Zhang, A.X., Muller, M. and Wang, D. 2020. How do data science workers
collaborate? Roles, workflows, and tools. Proc. Human-Computer Interaction.4 ,
CSCW1,1–23.
[115]Zhou,S.,Vasilescu,B.andKästner,C.2020.HowHasForkingChangedinthe
Last 20 Years? A Study of Hard Forks on GitHub. In Proc. Int’l Conf. Software
Engineering(ICSE), 445–456.
[116]Zinkevich,M.2017.Rulesofmachinelearning:BestpracticesforMLengineering.
URL:https://developers.google.com/machine-learning/guides/rules-of-ml .
425