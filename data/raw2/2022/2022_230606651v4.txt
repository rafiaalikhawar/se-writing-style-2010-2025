Predicting Software Performance with Divide-and-Learn
Jingzhi Gong
Loughborough University
Leicestershire
United Kingdom
j.gong@lboro.ac.ukTao Chenâˆ—
University of Birmingham
Birmingham
United Kingdom
t.chen@bham.ac.uk
ABSTRACT
Predicting the performance of highly configurable software systems
is the foundation for performance testing and quality assurance. To
that end, recent work has been relying on machine/deep learning to
model software performance. However, a crucial yet unaddressed
challenge is how to cater for the sparsity inherited from the config-
uration landscape: the influence of configuration options (features)
and the distribution of data samples are highly sparse.
In this paper, we propose an approach based on the concept of
â€œdivide-and-learnâ€, dubbed DaL. The basic idea is that, to handle
sample sparsity, we divide the samples from the configuration land-
scape into distant divisions, for each of which we build a regularized
Deep Neural Network as the local model to deal with the feature
sparsity. A newly given configuration would then be assigned to
the right model of division for the final prediction.
Experiment results from eight real-world systems and five sets
of training data reveal that, compared with the state-of-the-art ap-
proaches, DaLperforms no worse than the best counterpart on 33
out of 40 cases (within which 26 cases are significantly better) with
up to 1.94Ã—improvement on accuracy; requires fewer samples to
reach the same/better accuracy; and producing acceptable train-
ing overhead. Practically, DaLalso considerably improves different
global models when using them as the underlying local models,
which further strengthens its flexibility. To promote open science,
all the data, code, and supplementary figures of this work can be
accessed at our repository: https://github.com/ideas-labo/DaL.
CCS CONCEPTS
â€¢Software and its engineering â†’Software performance .
KEYWORDS
Configurable System, Machine Learning, Deep Learning, Perfor-
mance Prediction, Performance Learning, Configuration Learning
ACM Reference Format:
Jingzhi Gong and Tao Chen. 2023. Predicting Software Performance with
Divide-and-Learn. In Proceedings of the 31st ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE â€™23), December 3â€“9, 2023, San Francisco, CA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616334
âˆ—Tao Chen is the corresponding author
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.36163341 INTRODUCTION
â€œWhat will be the implication on runtime if we deploy
that configuration?â€
The above is a question we often hear from our industrial part-
ners. Indeed, software performance, such as latency, runtime, and
energy consumption, is one of the most critical concerns of soft-
ware systems that come with a daunting number of configuration
options, e.g., x264 (a video encoder) allows one to adjust 16 options
to influence its runtime. To satisfy the performance requirements, it
is essential for software engineers to understand what performance
can be obtained under a given configuration before the deployment.
This not only enables better decisions on configuration tuning [ 11]
but also reduces the efforts of configuration testing [16].
To achieve the above, one way is to directly profile the software
system for all possible configurations when needed. This, however,
is impractical, because (1) the number of possible configurations
may be too high [ 10,23,31,53]. For example, HIPAğ‘ğ‘(a compiler
for image processing) has more than 10,000 possible configurations.
(2) Even when such a number is small, the profiling of a single
configuration can still be rather expensive [ 10,31]: Wang et al. [62]
report that it could take weeks of running time to benchmark and
profile even a simple system. Therefore, an accurate performance
model that can predict the expected performance of a newly given
configuration is of high demand.
With the increasing complexity of modern software, the number
of configurable options continues to expand and the interactions be-
tween options become more complicated, leading to significant diffi-
culty in predicting the performance accurately [ 8,54]. Recently, ma-
chine learning models have been becoming the promising method
for this regression problem as they are capable of modeling the com-
plex interplay between a large number of variables by observing
patterns from data [23, 26, 52, 53, 55, 61, 63].
However, since machine learning modeling is data-driven, the
characteristics and properties of the measured data for configurable
software systems pose non-trivial challenges to the learning, pri-
marily because it is known that the configuration landscapes of
the systems do not follow a â€œsmoothâ€ shape [ 31]. For example, ad-
justing between different cache strategies can drastically influence
the performance, but they are often represented as a single-digit
change on the landscape [ 7,10,44]. This leads to the notion of
sparsity in two aspects:
â€¢Only a small number of configuration options can signif-
icantly influence the performance, hence there is a clear
feature sparsity involved [23, 30, 53, 61].
â€¢The samples from the configuration landscape tend to form
different divisions with diverse values of performance and
configuration options, especially when the training data isarXiv:2306.06651v4  [cs.SE]  4 Feb 2024ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Jingzhi Gong and Tao Chen
limited due to expensive measurementâ€”a typical case of
sample sparsity [27,38,51]. This is particularly true when
not all configurations are valid [53].
Existing work has been primarily focusing on addressing feature
sparsity, through using tree-liked model [ 21]; via feature selec-
tion [ 8,20,36]; or deep learning [ 17,23,52]. However, the sample
sparsity has almost been ignored, which can still be a major obstacle
to the effectiveness of machine learning-based performance model.
To address the above gap, in this paper, we propose DaL, an
approach to model software performance via the concept of â€œdivide-
and-learnâ€. The basic idea is that, to handle sample sparsity, we
divide the samples (configurations and their performance) into
different divisions, each of which is learned by a local model. In
this way, the highly sparse samples can be split into different lo-
cally smooth regions of data samples, and hence their patterns and
feature sparsity can be better captured.
In a nutshell, our main contributions are:
(1)We formulate, on top of the regression of performance, a
new classification problem without explicit labels.
(2)We extend Classification and Regression Tree (CART) [ 39] as
a clustering algorithm to â€œdivideâ€ the samples into different
divisions with similar characteristics, for each of which we
build a local regularized Deep Neural Network (rDNN) [ 19].
(3)Newly given configurations would be assigned into a division
inferred by a Random Forest classifier [ 28], which is trained
using the pseudo labeled data from the CART. The rDNN
model of the assigned division would be used for the final
prediction thereafter.
(4)Under eight systems with diverse performance attributes,
scale, and domains, as well as five different training sizes, we
evaluate DaLagainst four state-of-the-art approaches and
with different underlying local models.
The experiment results are encouraging: compared with the best
state-of-the-art approach, we demonstrate that DaL
â€¢achieves no worse accuracy on 33 out of 40 cases with 26 of
them being significantly better. The improvements can be
up to 1.94Ã—against the best counterpart;
â€¢uses fewer samples to reach the same/better accuracy.
â€¢incurs acceptable training time considering the improve-
ments in accuracy.
Interestingly, we also reveal that:
â€¢DaLcan considerably improve the accuracy of an arbitrarily
given model when it serves as the local model for each divi-
sion compared with using the model alone as a global model
(which is used to learn the entire training dataset). However,
the original DaLwith rDNN as the local model still produces
the most accurate results.
â€¢DaLâ€™s error tends to correlate quadratically with its only
parameterğ‘‘that sets the number of divisions. Therefore, a
middle value (between 0 and the bound set by CART) can
reach a good balance between handling sample sparsity and
providing sufficient training data for the local models, e.g.,
ğ‘‘=1orğ‘‘=2(2 or 4 divisions) in this work.This paper is organized as follows: Section 2 introduces the
problem formulation and the notions of sparsity in software per-
formance learning. Section 3 delineates the tailored problem for-
mulation and our detailed designs of DaL. Section 4 presents the
research questions and the experiment design, followed by the
analysis of results in Section 5. The reasons why DaLworks, its
strengths, limitations, and threats to validity are discussed in Sec-
tion 6. Section 7, 8, and 9 present the related work, conclude the
paper, and elaborate data availability, respectively.
2 BACKGROUND AND MOTIVATION
In this section, we introduce the background and the key observa-
tions that motivate this work.
2.1 Problem Formulation
In the software engineering community, the question introduced
at the beginning of this paper has been most commonly addressed
by using various machine learning models (or at least partially) [ 9,
25,44,58,65], in a data-driven manner that relies on observing the
softwareâ€™s actual behaviors and builds a statistical model to predict
the performance without heavy human intervention [1].
Formally, modeling the performance of software with ğ‘›configu-
ration options is a regression problem that builds:
P=ğ‘“(ğ’®),PâˆˆR (1)
whereby ğ’®denotes the training samples of configuration-performance
pairs, such that xâˆˆğ’®.xis a configuration and x=(ğ‘¥1,ğ‘¥2,Â·Â·Â·,ğ‘¥ğ‘›),
where each configuration option ğ‘¥ğ‘–is either binary or categori-
cal/numerical. The corresponding performance is denoted as P.
The goal of machine learning-based modeling is to learn a re-
gression function ğ‘“using all training data samples such that for
newly given configurations, the predicted performance is as close
to the actual performance as possible.
2.2 Sparsity in Software Performance Learning
It has been known that the configuration space for software systems
is generally rugged and sparse with respect to the configuration
options [ 6,10,23,31] â€”feature sparsity , which refers to the fact
that only a small number of configuration options are prominent to
the performance. We discover that, even with the key options that
are the most influential to the performance, the samples still do not
exhibit a â€œsmoothâ€ distribution over the configuration landscape.
Instead, they tend to be spread sparsely: those with similar charac-
teristics can form arbitrarily different divisions, which tend to be
rather distant from each other. This is a typical case of high sample
sparsity [27,38,51] and it is often ignored in existing work for
software performance learning.
In Figure 1, we show examples of the configuration samples
measured from four real-world software systems. Clearly, we see
that they all exhibit a consistent pattern1â€”the samples tend to form
different divisions with two properties:
â€¢Property 1: configurations in the same division share closer
performance values with smoother changes but those in-
between divisions exhibit drastically different performance
and can change more sharply.
1Similar pattern has been registered on all systems studied in this work.Predicting Software Performance with Divide-and-Learn ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
(a)BDB-J
 (b)x264
(c)HSMGP
 (d)VP8
Figure 1: Projection of configurations in the landscape with
respect to the performance and two most important options
(the divisions are circled).
â€¢Property 2: configurations in the same division can have a
closer value on at least one key option than those from the
different divisions.
In this regard, the values of performance and key configuration
options determine the characteristics of samples. In general, such
a high sample sparsity is caused by two reasons: (1) the inherited
consequence of high feature sparsity and (2) the fact that not all
configurations are valid because of the constraints (e.g., an option
can be used only if another option has been turned on) [ 53], thereby
there are many â€œempty areasâ€ in the configuration landscape.
When using machine learning models to learn concepts from the
above configuration data, the model needs to (1) handle the complex
interactions between the configuration options with high feature
sparsity while (2) capture the diverse characteristics of configura-
tion samples over all divisions caused by the high sample sparsity,
e.g., in Figure 1, where samples in different divisions have diverged
performance ranges. For the former challenge, there have been
some approaches proposed to target such, such as DeepPerf [23]
andPerf-AL [52]. However, very little work has aimed to address
the latter which can be the main obstacle for a model to learn and
generalize the data for predicting the performance of the newly-
given configuration. This is because those highly sparse samples
increase the risk for models to overfit the training data, for in-
stance by memorizing and biasing values in certain respective divi-
sions [ 27], especially considering that we can often have limited
samples from the configuration landscape due to the expensive
measurement of configurable systems. The above is the main mo-
tivation of this work, for which we ask: how can we improve the
accuracy of predicting software performance under such a high
sample sparsity?
division1division2divisionNClassifierNew config.  Performance
CART
rDNNrDNN
rDNNData samples    â€¦â€¦â€¦Dividing/training flowPredicting flowDividingTraining
Predicting
Figure 2: The architecture of DaL.
3 DIVIDE-AND-LEARN FOR PERFORMANCE
PREDICTION
Drawing on our observations of the configuration data, we propose
DaLâ€” an approach that enables better prediction of the software
performance via â€œdivide-and-learnâ€. To mitigate the sample spar-
sity issue, the key idea of DaLis that, since different divisions of
configurations show drastically diverse characteristics, i.e., rather
different performance values with distant values of key configura-
tion options, we seek to independently learn a local model for each
of those divisions that contain locally smooth samples, thereby the
learning can be more focused on the particular characteristics ex-
hibited from the divisions and handle the feature sparsity. Yet, this
requires us to formulate, on top of the original regression problem
of predicting the performance value, a new classification problem
without explicit labels. As such, we modify the original problem
formulation (Equation 1) as below:
ğ’Ÿ=ğ‘”(ğ’®) (2)
âˆ€ğ·ğ‘–âˆˆğ’Ÿ:P=ğ‘“(ğ·ğ‘–),PâˆˆR (3)
Overall, we aim to achieve three goals:
â€¢Goal 1: dividing the data samples into diverse yet more
focused divisions ğ’Ÿ(building function ğ‘”) and;
â€¢Goal 2: training a dedicated local model for each division
ğ·ğ‘–(building function ğ‘“) while;
â€¢Goal 3: assigning a newly coming configuration into the
right model for prediction (using functions ğ‘”andğ‘“).
Figure 2 illustrates the overall architecture of DaL, in which there
are three core phases, namely Dividing ,Training , and Predicting . A
pseudo code can also be found in Algorithm 1.
3.1 Dividing
The very first phase in DaLis to appropriately divide the data into
more focused divisions while doing so by considering both the
configuration options and performance values. To that end, the
key question we seek to address is: how to effectively cluster the
performance data with similar sample characteristics ( Goal 1 )?
Indeed, for dividing the data samples, it makes sense to consider
various unsupervised clustering algorithms, such as ğ‘˜-mean [ 40],ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Jingzhi Gong and Tao Chen
Algorithm 1: Pseudo code of DaL
Input: The expected depth ğ‘‘extracted from CART; a new configuration cto
be predicted
Output: The predicted performance of c
1ifâ„³=âˆ…then
2 /* dividing phase. */
3ğ’®â†randomly sample a set of configurations and their performance
4Tâ† trainCART( ğ’®)
5ğ‘‘â€²=1
6 whileğ‘‘â€²â‰¤ğ‘‘do
7 ifğ‘‘â€²<ğ‘‘then
8 ğ’Ÿâ†extract all the leaf divisions of samples from Tat the
ğ‘‘â€²th depth
9 else
10 ğ’Ÿâ†extract all divisions of samples from Tat theğ‘‘â€²th depth
11ğ‘‘â€²=ğ‘‘â€²+1
12 /* training phase. */
13 forğ·ğ‘–âˆˆğ’Ÿdo
14 â„³â†trainRegularizedDNN( ğ·ğ‘–)
15/* predicting phase. */
16ifFhas not been trained then
17ğ’°â†Removing performance data and labeling the configurations based
on their divisions in ğ’Ÿ
18ğ’°â€²â†SMOTE(ğ’°)
19Fâ† trainRandomForest( ğ’°â€²)
20ğ·ğ‘–=predict(F,c)
21M=get the model from â„³that corresponds to the predicted division ğ·ğ‘–
22return predict(M,c)
BIRCH [ 66] or DBSCAN [ 14]. However, we found that they are
ill-suited for our problem, because:
â€¢the distance metrics are highly system-dependent. For ex-
ample, depending on the number of configuration options
and whether they are binary/numeric options;
â€¢it is difficult to combine the configuration options and per-
formance value with appropriate discrimination;
â€¢and clustering algorithms are often non-interpretable.
As a result, in DaL, we extend Classification and Regression Tree
(CART) as the clustering algorithm (lines 3-11 in Algorithm 1) since
(1) it is simple with interpretable/analyzable structure; (2) it ranks
the important options as part of training (good for dealing with
the feature sparsity issue), and (3) it does not suffer the issues
above [ 5,8,21,22,43,44,49]. As illustrated in Figure 3, CART is
originally a supervised and binary tree-structured model, which
recursively splits some, if not all, configuration options and the
corresponding data samples based on tuned thresholds. A split
would result in two divisions, each of which can be further split. In
this work, we at first train the CART on the available samples of
configurations and performance values, during which we use the
most common mean performance of all samples for each division
ğ·ğ‘–as the prediction [21, 22]:
ğ‘¦ğ·ğ‘–=1
|ğ·ğ‘–|âˆ‘ï¸
ğ‘¦ğ‘—âˆˆğ·ğ‘–ğ‘¦ğ‘— (4)
in whichğ‘¦ğ‘—is a performance value. For example, Figure 3 shows a
projected example, in which the configuration that satisfies â€œ rtQua-
lity=true â€ and â€œ threads=3 â€ would lead to an inferred runtime
of 112 seconds, which is calculated over all the 5 samples involved
using Equation 4.
# Samples = 18rtQuality?# Samples = 10# Samples = 8(runtime = 32s)# Samples = 5(runtime = 122s)# Samples = 5(runtime = 154s)threads > 5?d=1d=2falsetruefalsetruertQualityPerformancethreadsPerformancethreadsdivision1division2division2rtQualitydivision1division3DivisionSplitFigure 3: Projection of CART for VP8 showing the possible
divisions with different colors under alternative depth ğ‘‘.
By choosing/ranking options that serve as the splits and tuning
their thresholds, in DaL, we seek to minimize the following overall
loss function during the CART training:
L=1
|ğ·ğ‘™|âˆ‘ï¸
ğ‘¦ğ‘—âˆˆğ·ğ‘™(ğ‘¦ğ‘—âˆ’ğ‘¦ğ·ğ‘™)2+1
|ğ·ğ‘Ÿ|âˆ‘ï¸
ğ‘¦ğ‘—âˆˆğ·ğ‘Ÿ(ğ‘¦ğ‘—âˆ’ğ‘¦ğ·ğ‘Ÿ)2(5)
whereğ·ğ‘™andğ·ğ‘Ÿdenote the left and right division from a split,
respectively. This ensures that the divisions would contain data
samples with similar performance values ( Property 1 ) while they
are formed with respect to the similar values of the key configu-
ration options as determined by the splits/thresholds at the finest
granularity ( Property 2 ), i.e., the more important options would
appear on the higher level of the tree with excessive splitting.
However, here we do not use CART to generalize prediction
directly on new data once it is trained as it has been shown that
the splits and simple average of performance values in the division
alone can still fail to handle the complex interactions between the
options, leading to insufficient accuracy [ 23]. Further, with our loss
function in Equation 5, CART is prone to be overfitting2especially
for software quality data [ 34]. This exacerbates the issue of sample
sparsity [ 27] under a small amount of data samples which is not
uncommon for configurable software systems [23, 52].
Instead, what we are interested in are the (branch and/or leaf)
divisions made therein (with respect to the training data), which
enable us to use further dedicated and more focused local models
for better generalizing to the new data (lines 6-11 in Algorithm 1).
As such, the final prediction is no longer a simple average while
we do not care about the CART overfitting itself as long as it fits
the training data well. This is similar to the case of unsupervised
clustering for which the clustering is guided by implicit labels (via
the loss function at Equation 5). Specifically, in DaLwe extract the
data samples according to the divisions made by the ğ‘‘th depth of
the CART, including all the leaf divisions with depth smaller than
ğ‘‘. An example can be seen from Figure 3, where ğ‘‘is a controllable
parameter to be given. In this way, DaLdivides the data into a range
of[ğ‘‘+1,2ğ‘‘]divisions (ğ‘‘â‰¥1), each of which will be captured by a
local model learned thereafter. Note that when the number of data
2Overfitting means a learned model fits well with the training data but works poorly
on new data.Predicting Software Performance with Divide-and-Learn ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
samples in the division is less than the minimum amount required
by a model, we merge the two divisions of the same parent node.
As a concrete example, from Figure 3, we see that there are two
depths: when ğ‘‘=1there would be two divisions (one branch and
one leaf) with 10 and 8 samples respectively; similarly, when ğ‘‘=2
there would be three leaf divisions: two of each have 5 samples and
one is the division with 8 samples from ğ‘‘=1as it is a leaf. In this
case, CART has detected that the rtQuality is a more important
(binary) option to impact the performance, and hence it should
be considered at a higher level in the tree. Note that for numeric
options, e.g., threads , the threshold of splitting ( threads >5) is
also tuned as part of the training process of CART.
3.2 Training
Given the divisions produced by the Dividing phase, we train a
local model for the samples from each division identified as part of
Goal 2 (lines 12-14 in Algorithm 1). Theoretically, we can pair them
with any model. However, as we will show in Section 5.2, the state-
of-the-art regularized Deep Neural Network (rDNN) [ 23] (namely
DeepPerf ), published at ICSEâ€™19, is the most effective one under
DaLas it handles feature sparsity well for configurable software.
Indeed, Ha and Zhang [ 23] showed that rDNN is more effective than
the others even with small data samples when predicting software
performance (in our study, we also evaluate the same systems with
small training sample sizes as used in their work). Therefore, in
DaLwe choose rDNN as the underlying local model by default.
In this work, we adopt exactly the same structure and training
procedure as those used by Ha and Zhang [ 23], hence we kindly
refer interested readers to their work for the training details [ 23].
Since the local models of the divisions are independent, we utilize
parallel training as part of DaL.
3.3 Predicting
When a new configuration arrives for prediction, DaLchooses a
model of division trained previously to infer its performance. There-
fore, the question is: how to assign the new configuration to the
right model ( Goal 3 )? A naive solution is to directly feed the con-
figuration into the CART from the dividing phase and check which
divisions it associates with. Yet, since the performance of the new
configuration is unforeseen from the CARTâ€™s training data, this so-
lution requires CART to generalize accurately, which, as mentioned,
can easily lead to poor results because CART is overfitting-prone
when directly working on new data [34].
Instead, by using the divided samples from the Dividing phase
(which serves as pseudo labeled data), we train a Random Forestâ€”a
widely used classifier and is resilient to overfitting [ 4,48,59]â€”to
generalize the decision boundary and predict which division that
the new configuration should be better assigned to (lines 15-21 in
Algorithm 1). Again, in this way, we are less concerned about the
overfitting issue of CART as long as it matches the patterns of train-
ing data well. This now becomes a typical classification problem
but there are only pseudo labels to be used in the training. Using
the example from Figure 3 again, if ğ‘‘=1then the configurations
in the 10 sample set would have a label â€œdivision1â€ ; similarly, those
in the 8 sample set would result in a label â€œdivision2â€ .However, one issue we experienced is that, even with ğ‘‘=1, the
sample size of the two divisions can be rather imbalanced, which
severely harms the quality of the classifier trained. For example,
when training BDB-C with 18 samples, the first split in CART can
lead to two divisions with 14 and 4 samples, respectively. Therefore,
before training the classifier we use Synthetic Minority Oversam-
pling Technique (SMOTE) [ 2] to pre-process the pseudo label data,
hence the division(s) with much less data (minority) can be more
repeatedly sampled.
Finally, the classifier predicts a division whose local model would
infer the performance of the new configuration.
3.4 Trade-off with the Number of Divisions
Since more divisions mean that the sample space is separated into
more loosely related regions for dealing with the sample sparsity,
one may expect that the accuracy will be improved, or at least, stay
similar, thereby we should use the maximum possible ğ‘‘from CART
in the dividing phase . This, however, only exists in the â€œutopia caseâ€
where there is an infinite set of configuration data samples.
In essence, with the design of DaL, the depthğ‘‘will manage two
conflicting goals that influence its accuracy:
(1)greater ability to handle sample sparsity by separating the
distant samples into divisions, each of which is learned by
an isolated local model;
(2)and a larger amount of data samples in each division for the
local model to be able to generalize.
Clearly, a greater ğ‘‘may benefit goal (1) but it will inevitably
damage goal (2) since it is possible for CART to generate divisions
with imbalanced sample sizes. As a result, we see ğ‘‘as a value that
controls the trade-off between the two goals, and neither a too
small nor too large ğ‘‘would be ideal, as the former would lose the
ability to deal with sample sparsity while the latter would leave
too little data for a local model to learn, hence produce negative
noises to the overall prediction. Similar to the fact that we cannot
theoretically justify how much data is sufficient for a model to learn
the concept [ 46], it is also difficult to prove how many divisions are
sufficient for handling the sample sparsity in performance modeling.
However, in Section 5.3, we will empirically demonstrate that there
is a (upward) quadratic correlation between ğ‘‘value and the error
incurred by DaLdue to the conflict between the above two goals.
4 EXPERIMENT SETUP
Here, we delineate the settings of our evaluation. In this work,
DaLis implemented based on Tensorflow and scikit-learn . All
experiments were carried out on a machine with Intel Core i7 2GHz
CPU and 16GB RAM.
4.1 Research Questions
In this work, we comprehensively assess DaLby answering the
following research questions (RQ):
â€¢RQ1: How accurate is DaLcompared with the state-of-the-
art approaches for software performance prediction?
â€¢RQ2: Can DaLbenefit different models when they are used
locally therein for predicting software performance?
â€¢RQ3: What is the sensitivity of DaLâ€™s accuracy to ğ‘‘?ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Jingzhi Gong and Tao Chen
Table 1: Details of the subject systems. ( |â„¬|/|ğ’©|) denotes the
number of binary/numerical options, and |ğ’|denotes the
number of valid configurations (full sample size).
System|â„¬|/|ğ’©|Performance Description |ğ’|Used by
Apache 9/0 Maximum load Web server 192 [22, 23, 52]
BDB-C 16/0 Latency (ms) Database (C) 2560 [22, 23, 52]
BDB-J 26/0 Latency (ms) Database (Java) 180 [22, 23, 52]
x264 16/0 Runtime (ms) Video encoder 1152 [22, 23, 52]
HSMGP 11/3 Runtime (ms) Compiler 3456 [23, 53]
HIPAğ‘ğ‘31/2 Runtime (ms) Compiler 13485 [23, 53]
VP8 9/4 Runtime (ms) Video encoder 2736 [47]
Lrzip 9/3 Runtime (ms) Compression tool 5184 [47]
â€¢RQ4: What is the model building time for DaL?
We ask RQ1 to assess the effectiveness of DaLunder different
sample sizes against the state-of-the-art. Since the default rDNN in
DaLis replaceable, we study RQ2 to examine how the concept of
â€œdivide-and-learnâ€ can benefit any given local model and whether
using rDNN as the underlying local model is the best option. In
RQ3 , we examine how the depth of division ( ğ‘‘) can impact the
performance of DaL. Finally, we examine the overall overhead of
DaLinRQ4 .
4.2 Subject Systems
We use the same datasets of all valid configurations from real-world
systems as widely used in the literature [ 22,23,47,52,53]. To re-
duce noise, we remove those that contain missing measurements
or invalid configurations. As shown in Table 1, we consider eight
configurable software systems with diverse domains, scales, and
performance concerns. Some of those contain only binary configu-
ration options (e.g., x264 ) while the others involve mixed options
(binary and numeric), e.g., HSMGP , which can be more difficult to
model [23].
The configuration data of all the systems are collected by prior
studies using the standard benchmarks with repeated measure-
ment [ 22,23,47,52,53]. For example, the configurations of Apache â€”
a popular Web serverâ€”are benchmarked using the tools Autobench
andHttperf , where workloads are generated and increased until
reaching the point before the server crashes, and then the maximum
load is marked as the performance value [ 22]. The process repeats
a few times for each configuration to ensure reliability.
To ensure generalizability of the results, for each system, we
follow the protocol used by existing work [ 23,52,55] to obtain five
sets of training sample size in the evaluation:
â€¢Binary systems: We randomly sample ğ‘›,2ğ‘›,3ğ‘›,4ğ‘›, and
5ğ‘›configurations and their measurements, where ğ‘›is the
number of configuration options [23, 52].
â€¢Mixed systems: We leverage the sizes suggested by SPLCon -
queror [55] (a state-of-the-art approach) depending on the
amount of budget.
The results have been illustrated in Table 2. All the remaining
samples in the dataset are used for testing.Table 2: The training sample sizes used. ğ‘›denotes the number
of configuration options in a binary system.
System Size 1 Size 2 Size 3 Size 4 Size 5
Apache ğ‘› 2ğ‘› 3ğ‘› 4ğ‘› 5ğ‘›
BDB-C ğ‘› 2ğ‘› 3ğ‘› 4ğ‘› 5ğ‘›
BDB-Jğ‘› 2ğ‘› 3ğ‘› 4ğ‘› 5ğ‘›
x264ğ‘› 2ğ‘› 3ğ‘› 4ğ‘› 5ğ‘›
HSMGP 77 173 384 480 864
HIPAğ‘ğ‘261 528 736 1281 2631
VP8 121 273 356 467 830
Lrzip 127 295 386 485 907
4.3 Metric and Statistical Validation
4.3.1 Accuracy. For all the experiments, mean relative error (MRE)
is used as the evaluation metric for prediction accuracy, since it
provides an intuitive indication of the error and has been widely
used in the domain of software performance prediction [ 22,23,52].
Formally, the MRE is computed as:
ğ‘€ğ‘…ğ¸ =1
ğ‘˜Ã—ğ‘˜âˆ‘ï¸
ğ‘¡=1|ğ´ğ‘¡âˆ’ğ‘ƒğ‘¡|
ğ´ğ‘¡Ã—100% (6)
wherebyğ´ğ‘¡andğ‘ƒğ‘¡denote theğ‘¡th actual and predicted performance,
respectively. To mitigate bias, all experiments are repeated for 30
runs via bootstrapping without replacement. Note that excluding
replacement is a common strategy for the performance learning
of configuration as it is rare for a model to learn from the same
configuration sample more than once [18].
4.3.2 Statistical Test. Since our evaluation commonly involves
comparing more than two approaches, we apply Scott-Knott test [ 42]
to evaluate their statistical significance on the difference of MRE
over 30 runs, as recommended by Mittas and Angelis [ 42]. In a nut-
shell, Scott-Knott sorts the list of treatments (the approaches that
model the system) by their median values of the MRE. Next, it splits
the list into two sub-lists with the largest expected difference [ 64].
For example, suppose that we compare ğ´,ğµ, andğ¶, a possible split
could be{ğ´,ğµ},{ğ¶}, with the rank ( ğ‘Ÿ) of 1 and 2, respectively. This
means that, in the statistical sense, ğ´andğµperform similarly, but
they are significantly better than ğ¶. Formally, Scott-Knott test aims
to find the best split by maximizing the difference Î”in the expected
mean before and after each split:
Î”=|ğ‘™1|
|ğ‘™|(ğ‘™1âˆ’ğ‘™)2+|ğ‘™2|
|ğ‘™|(ğ‘™2âˆ’ğ‘™)2(7)
whereby|ğ‘™1|and|ğ‘™2|are the sizes of two sub-lists ( ğ‘™1andğ‘™2) from
listğ‘™with a size|ğ‘™|.ğ‘™1,ğ‘™2, andğ‘™denote their mean MRE.
During the splitting, we apply a statistical hypothesis test ğ»to
check ifğ‘™1andğ‘™2are significantly different. This is done by using
bootstrapping and Ë†ğ´12[60]. If that is the case, Scott-Knott recurses
on the splits. In other words, we divide the approaches into different
sub-lists if both bootstrap sampling and effect size test suggest that
a split is statistically significant (with a confidence level of 99%) and
with a good effect Ë†ğ´12â‰¥0.6. The sub-lists are then ranked based
on their mean MRE.Predicting Software Performance with Divide-and-Learn ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 3: The median and interquartile range of MRE, denoted as Med (IQR), for DaLand the state-of-the-art approaches for all
the subject systems and training sizes over 30 runs. For each case, green cells mean DaLhas the best median MRE; or red cells
otherwise. The one(s) with the best rank ( ğ‘Ÿ) from the Scott-Knott test is highlighted in bold.
ApproachApache BDB-C BDB-J x264 HSMGP HIPAğ‘ğ‘VP8 Lrzip
ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR) ğ‘Ÿ Med (IQR)
DeepPerf 3 20.19 (6.34) 1 43.66 (42.88) 3 2.98 (3.34) 2 8.04 (3.06) 3 7.09 (3.04) 2 9.70 (1.28) 2 4.68 (3.27) 2 35.40 (16.59)
DECART 2 19.44 (6.48) 3 51.88 (45.91) 12.36 (0.75) 3 9.27 (2.11) N/A N/A N/A N/A N/A N/A N/A N/A
Perf-AL 4 33.57 (11.34) 4 82.38 (57.27) 5 37.45 (2.45) 4 37.00 (9.43) 4 66.70 (14.05) 4 31.99 (0.06) 4 60.05 (2.03) 3 58.45 (0.12)
SPLConqueror 114.44 (0.00) 4 86.85 (0.00) 4 12.29 (0.00) 17.01 (0.00) 2 15.04 (0.00) 3 16.68 (0.00) 3 29.39 (0.00) 4 123.7 (0.00)Size 1
DaL 2 21.02 (6.64) 141.77 (32.54) 2 3.14 (2.90) 2 8.04 (1.30) 14.66 (1.32) 19.07 (1.18) 11.59 (0.36) 126.40 (6.94)
DeepPerf 1 9.79 (4.56) 2 18.17 (8.11) 2 1.91 (0.68) 13.17 (1.00) 2 3.69 (0.61) 2 6.89 (1.44) 2 2.39 (1.21) 2 21.46 (4.53)
DECART 2 9.77 (5.36) 113.37 (7.25) 11.84 (0.17) 2 6.29 (1.72) N/A N/A N/A N/A N/A N/A N/A N/A
Perf-AL 4 32.97 (6.24) 4 73.99 (22.51) 5 37.90 (1.67) 4 34.98 (7.81) 4 66.67 (0.11) 4 31.98 (0.06) 4 60.04 (0.24) 3 58.46 (0.18)
SPLConqueror 3 13.88 (0.00) 2 162.91 (0.00) 4 24.85 (0.00) 3 6.63 (0.00) 3 22.63 (0.00) 3 16.96 (0.00) 3 35.34 (0.00) 4 171.85 (0.00)Size 2
DaL 2 9.82 (5.39) 3 17.22 (15.67) 3 1.90 (0.37) 1 3.21 (1.95) 12.66 (0.68) 15.55 (0.60) 11.21 (0.09) 115.37 (6.04)
DeepPerf 2 7.98 (1.91) 2 12.47 (4.08) 3 2.01 (1.10) 2 2.23 (0.90) 2 2.28 (0.37) 2 4.68 (0.90) 2 1.93 (0.72) 2 18.68 (3.20)
DECART 2 8.07 (1.05) 1 7.56 (5.07) 1 1.64 (0.24) 3 4.57 (1.03) N/A N/A N/A N/A N/A N/A N/A N/A
Perf-AL 4 31.73 (4.66) 2 69.74 (4.33) 5 37.19 (1.70) 5 36.25 (7.73) 4 66.63 (0.18) 4 31.99 (0.06) 4 60.02 (0.29) 3 58.44 (0.21)
SPLConqueror 3 13.99 (0.00) 3 193.89 (0.00) 4 25.88 (0.00) 4 6.31 (0.00) 3 32.46 (0.00) 3 17.18 (0.00) 3 36.26 (0.00) 4 177.61 (0.00)Size 3
DaL 17.17 (2.67) 15.74 (6.84) 2 1.61 (0.31) 11.66 (0.57) 11.56 (0.20) 14.39 (0.41) 11.12 (0.08) 111.83 (4.53)
DeepPerf 1 6.85 (1.60) 2 9.06 (9.76) 3 1.70 (0.37) 2 1.74 (0.98) 2 2.23 (0.39) 2 3.58 (1.00) 2 1.54 (0.40) 2 15.55 (1.71)
DECART 2 7.47 (0.72) 1 5.17 (5.07) 11.47 (0.12) 3 3.66 (1.42) N/A N/A N/A N/A N/A N/A N/A N/A
Perf-AL 4 30.67 (6.84) 3 69.50 (2.49) 5 37.74 (2.59) 5 36.41 (6.47) 4 66.63 (0.14) 4 31.98 (0.06) 4 59.99 (0.27) 3 58.48 (0.21)
SPLConqueror 3 13.84 (0.00) 4 231.68 (0.00) 4 29.01 (0.00) 4 6.32 (0.00) 3 33.73 (0.00) 3 17.42 (0.00) 3 37.14 (0.00) 4 236.48 (0.00)Size 4
DaL 16.59 (1.77) 13.68 (2.78) 2 1.61 (0.29) 11.13 (0.63) 11.49 (0.15) 13.22 (0.35) 11.10 (0.06) 110.10 (3.23)
DeepPerf 2 6.66 (1.61) 2 7.08 (8.03) 2 1.69 (0.48) 2 1.44 (0.90) 2 1.94 (0.68) 2 2.82 (0.57) 2 1.45 (0.23) 2 10.17 (0.91)
DECART 3 7.14 (0.89) 1 3.54 (1.92) 11.37 (0.26) 3 2.15 (1.05) N/A N/A N/A N/A N/A N/A N/A N/A
Perf-AL 5 30.45 (4.91) 3 69.29 (0.69) 4 35.76 (4.47) 5 36.20 (3.75) 4 66.59 (0.20) 4 31.97 (0.14) 4 59.95 (0.68) 3 58.39 (0.21)
SPLConqueror 4 14.06 (0.00) 4 296.68 (0.00) 3 29.87 (0.00) 4 6.36 (0.00) 3 35.86 (0.00) 3 17.69 (0.00) 3 36.21 (0.00) 4 208.08 (0.00)Size 5
DaL 15.96 (2.07) 12.15 (1.52) 1 1.46 (0.26) 10.79 (0.33) 11.16 (0.08) 12.39 (0.22) 11.07 (0.05) 16.60 (1.34)
5 EVALUATION
5.1 Comparing with the State-of-the-art
5.1.1 Method. To understand how DaLperforms compared with
the state-of-the-art, we assess its accuracy against both the standard
approaches that rely on statistical learning, i.e., SPLConqueror [55]
(linear regression and sampling methods) and DECART [22] (an im-
proved CART), together with recent deep learning-based ones, i.e.,
DeepPerf [23] (a single global rDNN) and Perf-AL [52] (an adver-
sarial learning method). All approaches can be used for any type
of system except for DECART , which works on binary systems only.
Following the setting used by Ha and Zhang [ 23],SPLConqueror3
andDECART use their own sampling method while DaL,DeepPerf
andPerf-AL rely on random sampling. Since there are 8 systems
and 5 sample sizes each, we obtain 40 cases to compare in total.
We use the implementations published by their authors with the
same parameter settings. For DaL, we setğ‘‘=1orğ‘‘=2depending
on the systems, which tends to be the most appropriate value based
on the result under a small portion of training data (see Section 5.3).
We use the systems, training sizes, and statistical tests as described
in Section 4. All experiments are repeated for 30 runs.
5.1.2 Results. The results have been illustrated in Table 3, from
which we see that DaLremarkably achieves the best accuracy on
31 out of 40 cases. In particular, DaLconsiderably improves the
accuracy, e.g., by up to 1.94Ã—better than the second-best one on
Size 1 ofVP8. The above still holds when looking into the results of
the statistical test: DaLis the only approach that is ranked first for
26 out of the 31 cases. For the 9 cases where DaLdoes not achieve
the best median MRE, it is equally ranked as the first for two of
them. These conclude that DaLis, in 33 cases, similar to (7 cases) or
3Since SPLConqueror supports multiple sampling methods, we use the one (or combi-
nation for the mixed system) that leads to the best MRE.significantly better (26 cases) than the best state-of-the-art for each
specific case (which could be a different approach).
For cases with different training sample sizes, we see that DaLper-
forms generally more inferior than the others when the size is too
limited, i.e., Size 1 andSize 2 for the binary systems. This is expected
as when there are too few samples, each local model would have a
limited chance to observe the right pattern after the splitting, hence
blurring its effectiveness in handling sample sparsity. However,
in the other cases (especially for mixed systems that have more
data even for Size 1 ),DaLneeds far fewer samples to achieve the
same accuracy as the best state-of-the-art. For example, on Lrzip ,
DaLonly needs 386 samples ( Size 3 ) to achieve an error less than
15% while DeepPerf requires 907 samples ( Size 5 ) to do so.
Another observation is that the improvements of DaLis much
more obvious in mixed systems than those for binary systems. This
is because: (1) the binary systems have fewer training samples as
they have a smaller configuration space. Therefore, the data learned
by each local model is more restricted. (2) The issue of sample
sparsity is more severe on mixed systems, as their configuration
landscape is more complex and comes with finer granularity.
As a result, we anticipate that the benefit of DaLcan be amplified
with more complex systems and/or more training data.
To summarize, we can answer RQ1 as:
RQ1: DaLperforms similar or significantly better than the
best state-of-the-art approach in 33 out of 40 cases, with up to
1.94Ã—improvements. It also needs fewer samples to achieve
the same accuracy and the benefits can be amplified with
complex systems/more training samples.
5.2 DaLunder Different Local Models
5.2.1 Method. Since the idea of â€œdivide-and-learnâ€ can be appli-
cable to a wide range of underlying local models of the divisionsESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Jingzhi Gong and Tao Chen
Table 4: The Scott-Knott ranks ( ğ‘Ÿ) on the MRE of DaLunder different local models and their global counterparts. The green cells
denote the best rank. Raw MRE results can be accessed at: https://github.com/ideas-labo/DaL/blob/main/Table4_full.pdf.
System DaL rDNN DaLğ‘…ğ¹ RF DaLğ¶ğ´ğ‘…ğ‘‡ CART DaLğ¿ğ‘… LR DaLğ‘†ğ‘‰ğ‘… SVR DaLğ¾ğ‘…ğ‘… KRR DaLğ‘˜ğ‘ğ‘ğ‘˜NN
ApacheSize 1 1 2 1 3 2 2 1 5 4 4 6 7 2 2
Size 2 2 1 2 2 2 1 3 5 6 7 8 9 4 6
Size 3 1 2 3 2 4 4 1 7 6 9 10 11 5 8
Size 4 2 2 3 2 5 4 1 8 7 9 10 11 6 8
Size 5 2 4 4 3 5 5 1 9 7 10 11 12 6 8
BDB-CSize 1 2 2 2 6 1 1 3 8 3 4 2 7 3 5
Size 2 1 1 3 4 1 2 7 6 5 5 4 6 5 6
Size 3 2 5 4 4 3 1 6 9 8 7 6 9 8 9
Size 4 1 3 3 4 1 2 5 9 6 6 5 8 6 7
Size 5 1 3 2 4 1 1 4 9 6 7 5 9 6 8
BDB-JSize 1 2 2 2 3 1 1 9 8 6 4 3 7 5 5
Size 2 4 3 1 1 3 2 4 10 8 5 4 9 6 7
Size 3 2 3 1 1 2 2 3 8 6 5 3 7 4 4
Size 4 2 4 1 1 3 3 4 9 8 8 5 9 7 6
Size 5 2 4 1 1 3 3 4 9 7 8 5 9 6 6
x264Size 1 1 1 3 2 3 3 4 5 5 9 8 6 4 7
Size 2 1 1 3 3 3 3 2 4 7 9 4 5 6 8
Size 3 1 2 5 5 4 4 2 5 6 9 5 7 8 9
Size 4 1 2 5 6 3 4 3 7 7 10 6 8 9 11
Size 5 1 2 5 6 3 4 6 8 8 11 7 9 10 12
HSMGPSize 1 1 8 6 5 7 7 2 11 4 5 3 10 8 9
Size 2 1 2 5 6 8 8 2 10 7 8 4 10 9 9
Size 3 1 2 4 5 7 7 2 10 6 9 3 10 8 8
Size 4 1 2 3 4 7 8 4 11 6 10 5 11 9 9
Size 5 1 2 3 3 7 6 10 9 4 8 5 10 9 9
HIPAğ‘ğ‘Size 1 1 2 3 3 4 4 9 8 5 7 4 9 6 8
Size 2 1 2 3 3 3 3 6 10 5 6 5 8 4 7
Size 3 1 2 3 3 3 3 5 11 5 8 6 10 4 9
Size 4 1 2 4 4 3 3 8 11 7 10 8 11 6 5
Size 5 1 2 3 2 2 2 10 12 7 8 9 11 5 6
VP8Size 1 1 5 4 6 2 3 5 7 9 10 7 12 10 11
Size 2 4 5 3 4 1 2 4 8 6 11 7 12 9 10
Size 3 1 2 2 2 2 3 5 11 5 9 6 10 8 8
Size 4 1 2 2 2 2 2 3 8 4 6 3 7 5 6
Size 5 1 2 3 4 2 3 6 12 7 10 5 11 8 9
LrzipSize 1 3 4 2 3 1 2 5 10 5 5 7 9 6 5
Size 2 3 4 2 2 1 2 5 8 6 6 7 9 5 5
Size 3 3 4 2 1 1 2 6 9 7 6 8 10 5 5
Size 4 3 4 4 7 1 2 8 10 2 3 6 9 5 8
Size 5 3 4 1 3 1 2 7 9 7 8 9 11 5 6
Average 1.63 2.78 2.90 3.38 2.95 3.15 4.63 8.58 6.00 7.48 5.85 9.13 6.25 7.35
identified, we seek to understand how well DaLperform with dif-
ferent local models against their global model counterparts (i.e.,
using them directly to learn the entire training dataset). To that
end, we run experiments on a set of global models available in
scikit-learn and widely used in software engineering tasks to
make predictions directly [ 18,37], such as CART, Random Forest
(RF), Linear Regression (LR), Support Vector Regression (SVR), Ker-
nel Ridge Regression (KRR), and ğ‘˜-Nearest Neighbours ( ğ‘˜NN). We
used the same settings as those for RQ1 and all modelsâ€™ hyperpa-
rameters are tuned in training. For the simplicity of exposition, we
report on the ranks ğ‘Ÿproduced by the Scott-Knott test.
5.2.2 Result. From Table 4, we can obtain the following key ob-
servations: firstly, when examining each pair of the counterparts,
i.e.,DaLğ‘‹andX,DaLcan indeed improve the accuracy of the local
model via the concept of â€œdivide-and-learnâ€. In particular, for simple
but commonly ineffective models like LR [ 23],DaLcan improve
them to a considerable extent. Yet, we see that DaLdoes not often
lead to a significantly different result when working with CART
against using CART directly. This is as expected, since using dif-
ferent CART models for the divisions identified by a CART makes
little difference to applying a single CART that predicts directly.
Interestingly, we also see that our model performs better than the
traditional ensemble learning: DaLğ¶ğ´ğ‘…ğ‘‡ â€”a CART-based â€œdivide-
and-learnâ€ model performs generally better than RF, which uses
CART as the local model and combines them via Bagging.Secondly, the default of DaL, which uses the rDNN as the local
model, still performs significantly better than the others. This aligns
with the findings from existing work [ 23] that the rDNN handles
the feature sparsity better. Indeed, deep learning models are known
to be data-hungry, but our results surprisingly show that they can
also work well for a limited amount of configuration samples. The
key behind such is the use of regularization, which stresses addi-
tional penalties on the more important weights/options. This has
helped to relieve the need for a large amount of data during training
while better fitting with the sparse features in configuration data. A
similar conclusion has also been drawn from previous studies [ 23].
Therefore, for RQ2 , we say:
RQ2: Thanks to the concept of â€œdivide-and-learnâ€, DaLis
able to significantly improve a range of global models when
using them as the underlying local model.
5.3 Sensitivity to the Depth ğ‘‘
5.3.1 Method. To understand RQ3 , we examine different ğ‘‘values.
Since the number of divisions (and hence the possible depth) is
sample size-dependent, for each system, we use 80% of the full
dataset for training and the remaining for testing. This has allowed
us to achieve up to ğ‘‘=4with 16 divisions as the maximum possible
bound. For different ğ‘‘values, we report on the median MRE togetherPredicting Software Performance with Divide-and-Learn ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Figure 4: The median MRE (s), its IQR (area), and the average smallest training size of the divisions ( â˜…) achieved by
DaLunder different depths ( ğ‘‘values)/number of divisions over all systems and 30 runs. ğ‘Ÿ=1means rank 1 in the Scott-Knott
test on MRE. The best-ranked ğ‘‘is marked asâˆ—.
with the results of Scott-Knott test for 30 runs. We also report the
smallest sample size from the divisions, averaging over 30 runs.
5.3.2 Results. From Figure 4, we see that the correlation between
the error of DaLandğ‘‘value is close to quadratic: DaLreaches its
best MRE with ğ‘‘=1orğ‘‘=2. At the same time, the size of train-
ing data for a local model decreases as the number of divisions
increases. Since ğ‘‘controls the trade-off between the ability to han-
dle sample sparsity and ensuring sufficient data samples to train
all local models, ğ‘‘=1orğ‘‘=2tends to be the â€œsweet pointsâ€ that
reach a balance for the systems studied. After the point of ğ‘‘=1
orğ‘‘=2, the MRE will worsen, as the local modelsâ€™ training size
often drops dramatically. This is a clear sign that, from that point,
the side-effect of having too less samples to train a local model
has started to surpass the benefit that could have been brought by
dealing with sample sparsity using more local models.
Whenğ‘‘=0, which means only one division and hence DaLis
reduced to DeepPerf that ignores sample sparsity, the resulted MRE
is the worst on 4 out of 8 systems; the same applied to the case when
ğ‘‘=4. This suggests that neither too small ğ‘‘(e.g.,ğ‘‘=0with only
one division) nor too larger ğ‘‘(e.g.,ğ‘‘=4with up to 16 divisions,
i.e., too many divisions) are ideal, which matches our theoretical
analysis in Section 3.4.
Therefore, we conclude that:
RQ3: The error of DaLhas a (upward) quadratic correlation
toğ‘‘. In this work, ğ‘‘=1orğ‘‘=2(2 to 4 divisions) reaches a
good balance between handling sample sparsity and providing
sufficient training data for the local models.
5.4 Overhead of Model Building
5.4.1 Method. To study RQ4 , we examine the overall time required
and the breakdown of overhead for DaLin various phases. As someTable 5: The overhead ranges across all systems and sizes.
Approach Overhead (min) Restriction and Prerequisite
DeepPerf 3 to 60 None
DECART 0.07 to 0.5 does not work on mixed systems
Perf-AL 0.08 to 1 None
SPLConqueror 4Ã—10âˆ’4to 5Ã—10âˆ’3needs to select sampling method(s)
DaL 6 to 56 needs to set the depth ğ‘‘
â€”DaL(dividing ) 9Ã—10âˆ’4to 0.18 None
â€”DaL(training ) 4 to 52 None
â€”DaL(predicting ) 1.3 to 5 None
baselines, we also illustrate the model building time required by
the approaches compared in RQ1 .
5.4.2 Result. From Table 5, DaLincurs an overall overhead from 6
to 56 minutes. Yet, from the breakdown, we note that the majority
of the overhead comes from the training phase that trains the local
models. This is expected, as DaLuses rDNN by default.
Specifically, the overhead of DaLcompared with DeepPerf (3
to 60 minutes) is encouraging as it tends to be faster in the worst-
case scenario while achieving up to 1.94Ã—better accuracy. This is
because (1) each local model has less data to train and (2) the parallel
training indeed speeds up the process. In contrast to Perf-AL (a
few seconds to one minute), DaLappears to be rather slow as the
former does not use hyperparameter tuning but fixed-parameter
values [ 52]. Yet, as we have shown for RQ1 ,DaLachieves up to a few
magnitudes of accuracy improvement. Although SPLConqueror
andDECART have an overhead of less than a minute, again their
accuracy is much more inferior. Further, SPLConqueror requires a
good selection of the sampling method(s) (which can largely incur
additional overhead) while DECART does not work on mixed systems.
Finally, we have shown in RQ3 that DaLâ€™s MRE is quadratically
sensitive toğ‘‘(upward), hence its value should be neither too small
nor too large, e.g., ğ‘‘=1orğ‘‘=2in this work.ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Jingzhi Gong and Tao Chen
(a) prediction by DaL
 (b) prediction by DeepPerf
Figure 5: Example run of the actual and predicted perfor-
mance by DaLand DeepPerf forVP8.
In summary, we say that:
RQ4: DaLhas competitive model building time to DeepPerf
and higher overhead than the other state-of-the-art ap-
proaches, but this can be acceptable considering its improve-
ment in accuracy.
6 DISCUSSION
6.1 Why does DaLWork?
To provide a more detailed understanding of why DaLperforms
better than state-of-the-art, in Figure 5, we showcase the most com-
mon run of the predicted performance by DaLandDeepPerf against
actual performance. Clearly, we note that the sample sparsity is
rather obvious where there are two distant divisions. DeepPerf ,
as an approach that relies on a single and global rDNN, has been
severely affected by such highly sparse samples: we see that the
model tries to cover points in both divisions, but fails to do so as it
tends to overfit the points in one or the other. This is why, in Fig-
ure 5b, its prediction on some configurations that should lead to low
runtime tend to have much higher values (e.g., when rtQuality=1
andthreads=1 ) while some of those that should have high runtime
may be predicted with much lower values (e.g., when rtQuality=0
andthreads=1 ).DaL, in contrast, handles such a sample sparsity
well as it contains different local models that particularly cater to
each division identified, hence leading to high accuracy (Figure 5a).
6.2 Strengths and Limitations
The first strength of DaLis that the concept of â€œdivide-and-learnâ€,
paired with the rDNN, can handle both sample sparsity and feature
sparsity well. As from Section 5.1 for RQ1 , this has led to better
accuracy and better utilization of the sample data than the state-of-
the-art approaches.
The second strength is that, as from Section 5.2 for RQ2 ,DaLcan
improve different local models compared with when they are used
alone as a global model. While we set rDNN as the default for the
best accuracy, one can also easily replace it with others such as LR
for faster training and better interoperability. This enables great
flexibility with DaLto make trade-offs on different concerns of the
practical scenarios.
A limitation of DaLis that it takes a longer time to build the
model than some state-of-the-art approaches. On a machine withCPU 2GHz and 16GB RAM, DaLneeds between 6 and 56 minutes
for systems with up to 33 options and more than 2,000 samples.
6.3 Whyğ‘‘âˆˆ{1|2}is Highly Effective?
We have shown that the setting of ğ‘‘inDaLshould be neither too
small nor too large; the key intention behind the ğ‘‘is to reach a
good balance between handling the sample sparsity and providing
sufficient data for the local models to generalize. This is especially
true when the CART might produce divisions with imbalanced
sample sizes, e.g., we observed cases where there is a division with
around 500 samples while one other has merely less than 10. Our
experimental results show that such â€œsweet pointsâ€ tend to be ğ‘‘=1
orğ‘‘=2for the cases studied in this work.
However, the notion of â€œtoo smallâ€ and â€œtoo largeâ€ should be
interpreted cautiously depending on the systems and data size.
That is, although in this study, setting ğ‘‘=1orğ‘‘=2appears to be
appropriate; they might become â€œtoo smallâ€ settings when the data
size increases considerably and/or the system naturally exhibits
well-balanced divisions of configuration samples in the landscapes.
Yet, the pattern of quadratic correlation between ğ‘‘and the error of
DaLshould remain unchanged.
6.4 Using DaLin Practice
Like many other data-driven approaches, using DaLis straightfor-
ward and free of assumptions about the software systems, data, and
environments. We would recommend setting ğ‘‘=1orğ‘‘=2by
default, especially when the data sample size is similar to those we
studied in this work. Of course, it is always possible to fine-tune the
ğ‘‘value by training DaLwith alternative settings under the configu-
ration samples available. Given the quadratic correlation between ğ‘‘
and the error, it is possible to design a simple heuristic for this, e.g.,
we compare the accuracy of DaLtrained with ğ‘‘=ğ‘–andğ‘‘=ğ‘–+1
starting from ğ‘‘=1and finally selecting the maximum ğ‘‘valueğ‘˜
such that DaLwithğ‘˜+1is less accurate than DaLwithğ‘˜.
6.5 Threats to Validity
Internal Threats. Internal threats to validity are related to the
parameters used. In this work, we set the same setting as used in
state-of-the-art studies [ 21,23,52,55]. We have also shown the
sensitivity of DaLtoğ‘‘and reveal that there exists a generally best
setting. We repeat the experiments for 30 runs and use Scott-Knott
test for multiple comparisons.
Construct Threats. Threats to construct validity may lie in
the metric used. In this study, MRE is chosen for two reasons: (1)
it is a relative metric and hence is insensitive to the scale of the
performance; (2) MRE has been recommended for performance
prediction by many latest studies [22, 23, 52].
External Threats. External validity could be raised from the
subject systems and training samples used. To mitigate such, we
evaluate eight commonly used subject systems selected from the
latest studies. We have also examined different training sample
sizes as determined by SPLConqueror [55]â€”a typical method. Yet,
we agree that using more subject systems and data sizes may be
fruitful, especially for examining the sensitivity of ğ‘‘which may
lead to a different conclusion when there is a much larger set of
training configuration samples than we consider in this study.Predicting Software Performance with Divide-and-Learn ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
7 RELATED WORK
We now discuss the related work in light of DaL.
Analytical model. Predicting software performance can be
done by analyzing the code structure and architecture of the sys-
tems [ 12,61]. For example, Marco and Inverardi [ 12] apply queuing
network to model the latency of requests processed by the soft-
ware. Velez et al. [61] use local measurements and dynamic taint
analysis to build a model that can predict performance for part of
the code. However, analytical models require full understanding
and access to the softwareâ€™s internal states, which may not always
be possible/feasible. DaLis not limited to those scenarios as it is a
data-driven approach.
Statistical learning-based model. Data-driven learning has
relied on various statistical models, such as linear regressions [ 33,
53,55,57], tree-liked model [ 29,44,49], and fourier-learning mod-
els [24,68],etc. Among others, SPLConqueror [55] utilizes linear
regression combined with different sampling methods and a step-
wise feature selection to capture the interactions between configu-
ration options. DECART [22] is an improved CART with an efficient
sampling method [ 68]. However, recent work reveals that those
approaches do not work well with small datasets [ 23], which is
rather common for configurable software systems due to their ex-
pensive measurements. This is a consequence of not fully handling
the sparsity in configuration data. Further, they come with various
restrictions, e.g., DECART does not work on mixed systems while
SPLConqueror needs an extensive selection of the right sampling
method(s). In contrast, we showed that DaLproduces significantly
more accurate results while does not limit to those restrictions.
Ensemble model. Models can be combined in a shared man-
ner to predict software performance. For example, Chen and Bah-
soon [ 8] propose an ensemble approach, paired with feature selec-
tion for mitigating feature sparsity, to model software performance.
Other classic ensemble learning models such as Bagging [ 3] and
Boosting [ 50] (e.g., RF) can also be equally adopted. Indeed, at a
glance, our DaLdoes seem similar to the ensemble model as they
all maintain a pool of local models. However, the key difference is
that the classic ensemble models will inevitably share information
between the local models at one or more of the following levels:
â€¢At the training level, e.g., the local models in Boosting learn
the same samples but with a different focus; the Bucket of
Models (i.e., what Chen and Bahsoon [ 8] did) builds local
models on the same data and uses the best upon prediction.
â€¢At the model prediction level, e.g., Bagging aggregates the
results of local models upon prediction.
DaL, in contrast, has no information sharing throughout the
learning as the samples are split and so does the prediction of the
local models. This has enabled it to better isolate the samples and
cope with their inherited sparsity, e.g., recall from RQ2 , the overall
accuracy of DaLğ¶ğ´ğ‘…ğ‘‡ is better than RF (they both use CART as the
local models but learn with and without sharing information).
Deep learning-based model. A variety of studies apply neural
network with multiple layers and/or ensemble learning to predict
software performance [ 15,23,32,35,41,45,52,56,67].DeepPerf [23]
is a state-of-the-art DNN model with ğ¿1regularization to mitigate
feature sparsity for any configurable systems, and it can be more
accurate than many other existing approaches. The most recentlyproposed Perf-AL [52] relied on adversarial learning, which con-
sists of a generative network to predict the performance and a
discriminator network to distinguish the predictions and the ac-
tual labels. Nevertheless, existing deep learning approaches capture
only the feature sparsity while ignoring the sample sparsity, caus-
ing serve risks of overfitting even with regularization in place.
Compared with those, we have shown that, by capturing sample
sparsity, DaLis able to improve the accuracy considerably with
better efficiency and acceptable overhead.
Hybrid model. The analytical models can be combined with
data-driven ones to form a hybrid model [ 13,26,63]. Among others,
Didona et al. [13] use linear regression and ğ‘˜NN to learn certain
components of a queuing network. Conversely, Weber et al. [63]
propose to learn the performance of systems based on the parsed
source codes from the system to the function level. We see DaLas
being complementary to those hybrid models due to its flexibility
in selecting the local model: when needed, the local models can be
replaced with hybrid ones, making itself a hybrid variant. In case
the internal structure of the system is unknown, DaLcan also work
in its default as a purely data-driven approach.
8 CONCLUSION
This paper proposes DaL, an approach that effectively handles the
sparsity issues in configurable software performance prediction. By
formulating a classification problem with pseudo labels on top of
the original regression problem, DaLextracts the branches/leaves
from a CART which divides the samples of configuration into dis-
tant divisions and trains a dedicated local rDNN for each division
thereafter. Prediction of the new configuration is then made by the
rDNN of division inferred by a Random Forest classifier. As such,
the division of samples and the trained local model handles the
sample sparsity while the rDNN deals with the feature sparsity.
We evaluate DaLon eight real-world systems that are of diverse
domains and scales, together with five sets of training data. The
results show that DaLis:
â€¢effective as it is competitive to the best state-of-the-art
approach on 33 out of 40 cases, in which 26 of them are
significantly better with up to 1.94Ã—MRE improvement;
â€¢efficient since it often requires fewer samples to reach the
same/better accuracy compared with the state-of-the-art;
â€¢flexible given that it considerably improves various global
models when they are used as the local model therein;
â€¢robust because, given the quadratic correlation, a middle
ğ‘‘value(s) (between 0 and the bound set by CART) can be
robust and leads to the best accuracy across the cases, e.g.,
ğ‘‘=1orğ‘‘=2under the sample sizes in this work.
Mitigating the issues caused by sparsity is only one step towards
better performance prediction, hence the possible future work based
onDaLis vast, including multi-task prediction of performance under
different environments and merging diverse local models (e.g., a
mix of rDNN and LR) as part of the â€œdivide-and-learnâ€ concept.
Consolidating DaLwith an adaptive ğ‘‘is also within our agenda.
9 DATA AVAILABILITY
Data, code, and supplementary figures of this work can be found at
our repository: https://github.com/ideas-labo/DaL.ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Jingzhi Gong and Tao Chen
REFERENCES
[1] Ethem Alpaydin. 2004. Introduction to machine learning .
[2]Rok Blagus and Lara Lusa. 2013. SMOTE for high-dimensional class-imbalanced
data. BMC Bioinform. 14 (2013), 106. https://doi.org/10.1186/1471-2105-14-106
[3] Leo Breiman. 1996. Bagging predictors. Machine learning 24 (1996), 123â€“140.
[4]Junjie Chen, Ningxin Xu, Peiqi Chen, and Hongyu Zhang. 2021. Efficient Compiler
Autotuning via Bayesian Optimization. In 43rd IEEE/ACM International Conference
on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021 . IEEE, 1198â€“
1209. https://doi.org/10.1109/ICSE43902.2021.00110
[5]Tao Chen. 2019. All versus one: an empirical comparison on retrained and
incremental machine learning for modeling performance of adaptable software.
InProceedings of the 14th International Symposium on Software Engineering for
Adaptive and Self-Managing Systems, SEAMS@ICSE 2019, Montreal, QC, Canada,
May 25-31, 2019 , Marin Litoiu, SiobhÃ¡n Clarke, and Kenji Tei (Eds.). ACM, 157â€“168.
https://doi.org/10.1109/SEAMS.2019.00029
[6]Tao Chen. 2022. Lifelong Dynamic Optimization for Self-Adaptive Systems: Fact
or Fiction?. In IEEE International Conference on Software Analysis, Evolution and
Reengineering, SANER 2022, Honolulu, HI, USA, March 15-18, 2022 . IEEE, 78â€“89.
https://doi.org/10.1109/SANER53432.2022.00022
[7]Tao Chen. 2022. Planning Landscape Analysis for Self-Adaptive Systems. In
International Symposium on Software Engineering for Adaptive and Self-Managing
Systems, SEAMS 2022, Pittsburgh, PA, USA, May 22-24, 2022 , Bradley R. Schmerl,
Martina Maggio, and Javier CÃ¡mara (Eds.). ACM/IEEE, 84â€“90. https://doi.org/10.
1145/3524844.3528060
[8]Tao Chen and Rami Bahsoon. 2017. Self-Adaptive and Online QoS Modeling for
Cloud-Based Software Services. IEEE Trans. Software Eng. 43, 5 (2017), 453â€“475.
https://doi.org/10.1109/TSE.2016.2608826
[9]Tao Chen, Ke Li, Rami Bahsoon, and Xin Yao. 2018. FEMOSAA: Feature-Guided
and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software. ACM
Trans. Softw. Eng. Methodol. 27, 2 (2018), 5:1â€“5:50.
[10] Tao Chen and Miqing Li. 2021. Multi-objectivizing software configuration tuning.
InESEC/FSE â€™21: 29th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, Athens, Greece, August
23-28, 2021 , Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massim-
iliano Di Penta (Eds.). ACM, 453â€“465. https://doi.org/10.1145/3468264.3468555
[11] Tao Chen and Miqing Li. 2023. Do Performance Aspirations Matter for Guiding
Software Configuration Tuning? An Empirical Investigation under Dual Perfor-
mance Objectives. ACM Transactions on Software Engineering and Methodology.
32, 3, Article 68 (apr 2023), 41 pages. https://doi.org/10.1145/3571853
[12] Antinisca Di Marco and Paola Inverardi. 2004. Compositional generation of soft-
ware architecture performance QN models. In Proceedings. 4th Working IEEE/IFIP
Conference on Software Architecture . 37â€“46.
[13] Diego Didona, Francesco Quaglia, Paolo Romano, and Ennio Torre. 2015. Enhanc-
ing performance prediction robustness by combining analytical modeling and
machine learning. In Proceedings of the 6th ACM/SPEC international conference
on performance engineering . 145â€“156.
[14] Martin Ester, Hans-Peter Kriegel, JÃ¶rg Sander, and Xiaowei Xu. 1996. A Density-
Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
InProceedings of the Second International Conference on Knowledge Discovery and
Data Mining, Portland, Oregon, USA . AAAI Press, 226â€“231. http://www.aaai.org/
Library/KDD/1996/kdd96-037.php
[15] Thomas L. Falch and Anne C. Elster. 2017. Machine learning-based auto-tuning
for enhanced performance portability of OpenCL applications. Concurr. Comput.
Pract. Exp. 29, 8 (2017). https://doi.org/10.1002/cpe.4029
[16] Vincenzo Ferme and Cesare Pautasso. 2017. Towards Holistic Continuous Soft-
ware Performance Assessment. In Companion Proceedings of the 8th ACM/SPEC
on International Conference on Performance Engineering, ICPE 2017, Lâ€™Aquila, Italy,
April 22-26, 2017 , Walter Binder, Vittorio Cortellessa, Anne Koziolek, Evgenia
Smirni, and Meikel Poess (Eds.). ACM, 159â€“164. https://doi.org/10.1145/3053600.
3053636
[17] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep Sparse Rectifier
Neural Networks. In Proceedings of the Fourteenth International Conference on
Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April
11-13, 2011 (JMLR Proceedings, Vol. 15) , Geoffrey J. Gordon, David B. Dunson, and
Miroslav DudÃ­k (Eds.). JMLR.org, 315â€“323. http://proceedings.mlr.press/v15/
glorot11a/glorot11a.pdf
[18] Jingzhi Gong and Tao Chen. 2022. Does Configuration Encoding Matter in
Learning Software Performance? An Empirical Study on Encoding Schemes. In
19th IEEE/ACM International Conference on Mining Software Repositories, MSR
2022, Pittsburgh, PA, USA, May 23-24, 2022 . ACM, 482â€“494. https://doi.org/10.
1145/3524842.3528431
[19] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT
press.
[20] Johannes Grohmann, Simon Eismann, Sven Elflein, JÃ³akim von Kistowski, Samuel
Kounev, and Manar Mazkatli. 2019. Detecting Parametric Dependencies for Per-
formance Models Using Feature Selection Techniques. In 27th IEEE International
Symposium on Modeling, Analysis, and Simulation of Computer and Telecommuni-
cation Systems, MASCOTS . 309â€“322.[21] Jianmei Guo, Krzysztof Czarnecki, Sven Apel, Norbert Siegmund, and Andrzej
Wasowski. 2013. Variability-aware performance prediction: A statistical learning
approach. In 2013 28th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2013, Silicon Valley, CA, USA, November 11-15, 2013 . IEEE, 301â€“
311. https://doi.org/10.1109/ASE.2013.6693089
[22] Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel
Valov, Krzysztof Czarnecki, Andrzej Wasowski, and Huiqun Yu. 2018. Data-
efficient performance learning for configurable systems. Empirical Software
Engineering 23, 3 (2018).
[23] Huong Ha and Hongyu Zhang. 2019. DeepPerf: performance prediction for
configurable software with deep sparse neural network. In Proceedings of the
41st International Conference on Software Engineering, ICSE 2019, Montreal, QC,
Canada, May 25-31, 2019 , Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.).
IEEE / ACM, 1095â€“1106. https://doi.org/10.1109/ICSE.2019.00113
[24] Huong Ha and Hongyu Zhang. 2019. Performance-Influence Model for Highly
Configurable Software with Fourier Learning and Lasso Regression. In 2019
IEEE International Conference on Software Maintenance and Evolution, ICSME
2019, Cleveland, OH, USA, September 29 - October 4, 2019 . IEEE, 470â€“480. https:
//doi.org/10.1109/ICSME.2019.00080
[25] Xue Han and Tingting Yu. 2016. An Empirical Study on Performance Bugs for
Highly Configurable Software Systems. In Proceedings of the 10th ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement,
ESEM 2016, Ciudad Real, Spain, September 8-9, 2016 . ACM, 23:1â€“23:10. https:
//doi.org/10.1145/2961111.2962602
[26] Xue Han, Tingting Yu, and Michael Pradel. 2021. ConfProf: White-Box Per-
formance Profiling of Configuration Options. In ICPE â€™21: ACM/SPEC Interna-
tional Conference on Performance Engineering, Virtual Event, France, April 19-
21, 2021 , Johann Bourcier, Zhen Ming (Jack) Jiang, Cor-Paul Bezemer, Vittorio
Cortellessa, Daniele Di Pompeo, and Ana Lucia Varbanescu (Eds.). ACM, 1â€“8.
https://doi.org/10.1145/3427921.3450255
[27] Michael Heck, Nurul Lubis, Carel van Niekerk, Shutong Feng, Christian
Geishauser, Hsien-Chin Lin, and Milica Gasic. 2022. Robust Dialogue State
Tracking with Weak Supervision and Sparse Data. CoRR abs/2202.03354 (2022).
arXiv:2202.03354 https://arxiv.org/abs/2202.03354
[28] Tin Kam Ho. 1995. Random decision forests. In Third International Conference on
Document Analysis and Recognition, ICDAR 1995, August 14 - 15, 1995, Montreal,
Canada. Volume I . 278â€“282. https://doi.org/10.1109/ICDAR.1995.598994
[29] Chin-Jung Hsu, Vivek Nair, Vincent W. Freeh, and Tim Menzies. 2018. Arrow:
Low-Level Augmented Bayesian Optimization for Finding the Best Cloud VM.
In38th IEEE International Conference on Distributed Computing Systems, ICDCS
2018, Vienna, Austria, July 2-6, 2018 . IEEE Computer Society, 660â€“670. https:
//doi.org/10.1109/ICDCS.2018.00070
[30] Ling Huang, Jinzhu Jia, Bin Yu, Byung-Gon Chun, Petros Maniatis, and Mayur
Naik. 2010. Predicting Execution Time of Computer Programs Using Sparse
Polynomial Regression. In Advances in Neural Information Processing Systems
23: 24th Annual Conference on Neural Information Processing Systems. , John D.
Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and
Aron Culotta (Eds.). 883â€“891. https://proceedings.neurips.cc/paper/2010/hash/
995665640dc319973d3173a74a03860c-Abstract.html
[31] Pooyan Jamshidi and Giuliano Casale. 2016. An Uncertainty-Aware Approach to
Optimal Configuration of Stream Processing Systems. In 24th IEEE International
Symposium on Modeling, Analysis and Simulation of Computer and Telecommuni-
cation Systems, MASCOTS 2016, London, United Kingdom, September 19-21, 2016 .
IEEE Computer Society, 39â€“48.
[32] Andreas Johnsson, Farnaz Moradi, and Rolf Stadler. 2019. Performance Prediction
in Dynamic Clouds using Transfer Learning. In IFIP/IEEE International Symposium
on Integrated Network Management, IM 2019, Washington, DC, USA, April 09-11,
2019, Joe Betser, Carol J. Fung, Alex Clemm, JÃ©rÃ´me FranÃ§ois, and Shingo Ata
(Eds.). IFIP, 242â€“250. http://dl.ifip.org/db/conf/im/im2019/189279.pdf
[33] Yong-Bin Kang, Shonali Krishnaswamy, Wudhichart Sawangphol, Lianli Gao, and
Yuan-Fang Li. 2020. Understanding and improving ontology reasoning efficiency
through learning and ranking. Inf. Syst. 87 (2020). https://doi.org/10.1016/j.is.
2019.07.002
[34] Taghi M. Khoshgoftaar and Edward B. Allen. 2001. Controlling Overfitting in
Classification-Tree Models of Software Quality. Empir. Softw. Eng. 6, 1 (2001),
59â€“79.
[35] Yeseong Kim, Pietro Mercati, Ankit More, Emily Shriver, and Tajana Rosing. 2017.
P4: Phase-based power/performance prediction of heterogeneous systems via
neural networks. In 2017 IEEE/ACM International Conference on Computer-Aided
Design, ICCAD 2017, Irvine, CA, USA, November 13-16, 2017 , Sri Parameswaran
(Ed.). IEEE, 683â€“690. https://doi.org/10.1109/ICCAD.2017.8203843
[36] Junnan Li, Zhihui Lu, Yu Tong, Jie Wu, Shalin Huang, Meikang Qiu, and Wei Du.
2019. A general AI-defined attention network for predicting CDN performance.
Future Gener. Comput. Syst. 100 (2019), 759â€“769. https://doi.org/10.1016/j.future.
2019.05.067
[37] Ke Li, Zilin Xiang, Tao Chen, Shuo Wang, and Kay Chen Tan. 2020. Understanding
the automated parameter optimization on transfer learning for cross-projectPredicting Software Performance with Divide-and-Learn ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
defect prediction: an empirical study. In ICSE â€™20: 42nd International Conference
on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020 . ACM, 566â€“577.
https://doi.org/10.1145/3377811.3380360
[38] Guodong Liu, Hong Chen, and Heng Huang. 2020. Sparse Shrunk Additive Models.
InProceedings of the 37th International Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research,
Vol. 119) . PMLR, 6194â€“6204. http://proceedings.mlr.press/v119/liu20b.html
[39] Wei-Yin Loh. 2011. Classification and regression trees. Wiley interdisciplinary
reviews: data mining and knowledge discovery 1, 1 (2011), 14â€“23.
[40] James MacQueen et al .1967. Some methods for classification and analysis of
multivariate observations. In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability , Vol. 1. Oakland, CA, USA, 281â€“297.
[41] Aniruddha Marathe, Rushil Anirudh, Nikhil Jain, Abhinav Bhatele, Jayaraman J.
Thiagarajan, Bhavya Kailkhura, Jae-Seung Yeom, Barry Rountree, and Todd Gam-
blin. 2017. Performance modeling under resource constraints using deep transfer
learning. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, SC 2017, Denver, CO, USA, Novem-
ber 12 - 17, 2017 , Bernd Mohr and Padma Raghavan (Eds.). ACM, 31:1â€“31:12.
https://doi.org/10.1145/3126908.3126969
[42] Nikolaos Mittas and Lefteris Angelis. 2013. Ranking and Clustering Software
Cost Estimation Models through a Multiple Comparisons Algorithm. IEEE Trans.
Software Eng. 39, 4 (2013), 537â€“551. https://doi.org/10.1109/TSE.2012.45
[43] Vivek Nair, Tim Menzies, Norbert Siegmund, and Sven Apel. 2018. Faster discov-
ery of faster system configurations with spectral learning. Autom. Softw. Eng. 25,
2 (2018), 247â€“277.
[44] Vivek Nair, Zhe Yu, Tim Menzies, Norbert Siegmund, and Sven Apel. 2020. Finding
faster configurations using FLASH. IEEE Transactions on Software Engineering
46, 7 (2020).
[45] Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky, Os-
man S. Unsal, and AdriÃ¡n Cristal. 2017. A Machine Learning Approach for
Performance Prediction and Scheduling on Heterogeneous CPUs. In 29th Inter-
national Symposium on Computer Architecture and High Performance Computing,
SBAC-PAD 2017, Campinas, Brazil, October 17-20, 2017 . IEEE Computer Society,
121â€“128. https://doi.org/10.1109/SBAC-PAD.2017.23
[46] Taiwo Oyedare and Jung-Min Jerry Park. 2019. Estimating the Required Training
Dataset Size for Transmitter Classification Using Deep Learning. In 2019 IEEE
International Symposium on Dynamic Spectrum Access Networks, DySPAN 2019,
Newark, NJ, USA, November 11-14, 2019 . IEEE, 1â€“10. https://doi.org/10.1109/
DySPAN.2019.8935823
[47] Kewen Peng, Christian Kaltenecker, Norbert Siegmund, Sven Apel, and Tim
Menzies. 2021. VEER: Disagreement-Free Multi-objective Configuration. CoRR
abs/2106.02716 (2021). arXiv:2106.02716 https://arxiv.org/abs/2106.02716
[48] Rodrigo Queiroz, Thorsten Berger, and Krzysztof Czarnecki. 2016. Towards
predicting feature defects in software product lines. In Proceedings of the 7th In-
ternational Workshop on Feature-Oriented Software Development, FOSD@SPLASH
2016, Amsterdam, Netherlands, October 30, 2016 , Christoph Seidl and Leopoldo
Teixeira (Eds.). ACM, 58â€“62. https://doi.org/10.1145/3001867.3001874
[49] Atri Sarkar, Jianmei Guo, Norbert Siegmund, Sven Apel, and Krzysztof Czar-
necki. 2015. Cost-Efficient Sampling for Performance Prediction of Configurable
Systems (T). In 30th IEEE/ACM International Conference on Automated Software
Engineering, ASE . 342â€“352.
[50] Robert E Schapire. 2003. The boosting approach to machine learning: An overview.
Nonlinear estimation and classification (2003), 149â€“171.
[51] Atsushi Shibagaki, Masayuki Karasuyama, Kohei Hatano, and Ichiro Takeuchi.
2016. Simultaneous Safe Screening of Features and Samples in Doubly Sparse
Modeling. In Proceedings of the 33nd International Conference on Machine Learn-
ing, ICML 2016, New York City, NY, USA, June 19-24, 2016 (JMLR Workshop and
Conference Proceedings, Vol. 48) , Maria-Florina Balcan and Kilian Q. Weinberger
(Eds.). JMLR.org, 1577â€“1586. http://proceedings.mlr.press/v48/shibagaki16.html
[52] Yangyang Shu, Yulei Sui, Hongyu Zhang, and Guandong Xu. 2020. Perf-AL:
Performance Prediction for Configurable Software through Adversarial Learning.
InESEM â€™20: ACM / IEEE International Symposium on Empirical Software Engi-
neering and Measurement, Bari, Italy, October 5-7, 2020 , Maria Teresa Baldassarre,
Filippo Lanubile, Marcos Kalinowski, and Federica Sarro (Eds.). ACM, 16:1â€“16:11.
https://doi.org/10.1145/3382494.3410677
[53] Norbert Siegmund, Alexander Grebhahn, Sven Apel, and Christian KÃ¤stner. 2015.
Performance-influence models for highly configurable systems. In Proceedingsof the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE .
284â€“294.
[54] Norbert Siegmund, Sergiy S. Kolesnikov, Christian KÃ¤stner, Sven Apel, Don S.
Batory, Marko RosenmÃ¼ller, and Gunter Saake. 2012. Predicting performance
via automated feature-interaction detection. In 34th International Conference
on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland , Martin
Glinz, Gail C. Murphy, and Mauro PezzÃ¨ (Eds.). IEEE Computer Society, 167â€“177.
https://doi.org/10.1109/ICSE.2012.6227196
[55] Norbert Siegmund, Marko RosenmÃ¼ller, Martin Kuhlemann, Christian KÃ¤stner,
Sven Apel, and Gunter Saake. 2012. SPL Conqueror: Toward optimization of
non-functional properties in software product lines. Softw. Qual. J. 20, 3-4 (2012),
487â€“517. https://doi.org/10.1007/s11219-011-9152-9
[56] Muhammad Umer Sohail, Hossein Raza Hamdani, Asad Islam, Khalid Parvez,
Abdul Munem Khan, Usman Allauddin, Muhammad Khurram, and Hassan Elahi.
2021. Prediction of Non-Uniform Distorted Flows, Effects on Transonic Com-
pressor Using CFD, Regression Analysis and Artificial Neural Networks. Applied
Sciences 11, 8 (2021). https://doi.org/10.3390/app11083706
[57] Jingwei Sun, Guangzhong Sun, Shiyan Zhan, Jiepeng Zhang, and Yong Chen.
2020. Automated Performance Modeling of HPC Applications Using Machine
Learning. IEEE Trans. Computers 69, 5 (2020), 749â€“763. https://doi.org/10.1109/
TC.2020.2964767
[58] Paul Temple, Mathieu Acher, Gilles Perrouin, Battista Biggio, Jean-Marc JÃ©zÃ©quel,
and Fabio Roli. 2019. Towards quality assurance of software product lines with
adversarial configurations. In Proceedings of the 23rd International Systems and
Software Product Line Conference, SPLC . 38:1â€“38:12.
[59] Pavel Valov, Jianmei Guo, and Krzysztof Czarnecki. 2015. Empirical comparison of
regression methods for variability-aware performance prediction. In Proceedings
of the 19th International Conference on Software Product Line, SPLC 2015, Nashville,
TN, USA, July 20-24, 2015 , Douglas C. Schmidt (Ed.). ACM, 186â€“190. https:
//doi.org/10.1145/2791060.2791069
[60] AndrÃ¡s Vargha and Harold D. Delaney. 2000. A Critique and Improvement of the
CL Common Language Effect Size Statistics of McGraw and Wong.
[61] Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven Apel, and Christian
KÃ¤stner. 2021. White-Box Analysis over Machine Learning: Modeling Perfor-
mance of Configurable Systems. In 43rd IEEE/ACM International Conference on
Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021 . IEEE, 1072â€“1084.
https://doi.org/10.1109/ICSE43902.2021.00100
[62] Tiantian Wang, Mark Harman, Yue Jia, and Jens Krinke. 2013. Searching for
better configurations: a rigorous approach to clone evaluation. In Joint Meeting of
the European Software Engineering Conference and the ACM SIGSOFT Symposium
on the Foundations of Software Engineering, ESEC/FSEâ€™13 . 455â€“465.
[63] Max Weber, Sven Apel, and Norbert Siegmund. 2021. White-Box Performance-
Influence Models: A Profiling and Learning Approach. In 43rd IEEE/ACM Interna-
tional Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May
2021. IEEE, 1059â€“1071. https://doi.org/10.1109/ICSE43902.2021.00099
[64] Tianpei Xia, Rahul Krishna, Jianfeng Chen, George Mathew, Xipeng Shen, and
Tim Menzies. 2018. Hyperparameter Optimization for Effort Estimation. CoRR
abs/1805.00336 (2018). arXiv:1805.00336 http://arxiv.org/abs/1805.00336
[65] Tingting Yu, Wei Wen, Xue Han, and Jane Huffman Hayes. 2019. ConPredictor:
Concurrency Defect Prediction in Real-World Applications. IEEE Trans. Software
Eng. 45, 6 (2019), 558â€“575.
[66] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. 1996. BIRCH: An Efficient
Data Clustering Method for Very Large Databases. In Proceedings of the 1996
ACM SIGMOD International Conference on Management of Data, Montreal, Quebec,
Canada, June 4-6, 1996 , H. V. Jagadish and Inderpal Singh Mumick (Eds.). ACM
Press, 103â€“114. https://doi.org/10.1145/233269.233324
[67] Wei Zhang, Zhihui Lu, Ziyan Wu, Jie Wu, Huanying Zou, and Shalin Huang.
2018. Toy-IoT-Oriented data-driven CDN performance evaluation model with
deep learning. J. Syst. Archit. 88 (2018), 13â€“22. https://doi.org/10.1016/j.sysarc.
2018.05.005
[68] Yi Zhang, Jianmei Guo, Eric Blais, and Krzysztof Czarnecki. 2015. Performance
prediction of configurable software systems by fourier learning (t). In 2015 30th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 365â€“373.
Received 2023-02-02; accepted 2023-07-27