Multilingual CodeCo-evolution using LargeLanguageModels
Jiyang Zhang
UTAustin
USA
jiyang.zhang@utexas.eduPengyu Nie
UTAustin
USA
pynie@utexas.eduJunyiJessyLi
UTAustin
USA
jessy@austin.utexas.eduMilosGligoric
UTAustin
USA
gligoric@utexas.edu
ABSTRACT
ManysoftwareprojectsimplementAPIsandalgorithmsinmultiple
programming languages. Maintaining such projects is tiresome,
as developers have to ensure that any change (e.g., a bug /f_ix or
anewfeature)isbeingpropagated,timelyandwithouterrors,to
implementations in other programming languages. In the world of
ever-changingsoftware,usingrule-basedtranslationtools(i.e.,tran-
spilers)ormachine learningmodels fortranslating code fromone
language to another provideslimited value. Translatingeach time
theentirecodebasefromonelanguagetoanotherisnotthewayde-
veloperswork.Inthispaper,wetargetanoveltask:translatingcode
changesfromoneprogramminglanguagetoanotherusinglarge
language models(LLMs).We design and implement the /f_irst LLM,
dubbedCodeditor , to tackle this task. Codeditor explicitly mod-
els code changes as edit sequences and learns to correlate changes
acrossprogramminglanguages.Toevaluate Codeditor ,wecollect
a corpus of 6,613 aligned code changes from 8 pairs of open-source
software projects implementing similar functionalities in two pro-
gramming languages (Java and C#). Results show that Codeditor
outperformsthestate-of-the-artapproachesbyalargemarginon
allcommonlyusedautomaticmetrics.Ourworkalsorevealsthat
Codeditor is complementary to the existing generation-based
models,andtheircombination ensures even greaterperformance.
CCSCONCEPTS
•Computing methodologies →Machine learning ;•Software
and its engineering →Software evolution .
KEYWORDS
Language models,code translation,software evolution
ACM ReferenceFormat:
JiyangZhang,PengyuNie,JunyiJessyLi,andMilosGligoric.2023.Multi-
lingual Code Co-evolution using Large Language Models. In Proceedings of
the31stACMJointEuropeanSoftwareEngineeringConferenceandSympo-
siumontheFoundationsofSoftwareEngineering(ESEC/FSE’23),December
3–9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.
https://doi.org/10.1145/3611643.3616350
ESEC/FSE ’23,December 3–9, 2023, SanFrancisco, CA, USA
©2023 Copyright held bytheowner/author(s).
ACM ISBN979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.36163501 INTRODUCTION
To ensure /f_lexibility and a wide adoption of their software, compa-
niesprovideapplicationprogramminginterfaces(APIs)fortheirser-
vices in several programminglanguages. Services, such as Google
Cloud [20] and MongoDB [ 23], oﬀer APIs written in most popular
programming languages, including C++, C#, Java, and Python. Fur-
thermore,popularsoftwarepackages,likeAntlr[ 43]andLucene[ 47],
have options to target diﬀerent programming languages for the
purpose ofbeingusedacross various platforms easily.
Maintainingsoftwarethatoﬀersthesamefunctionalityinmulti-
ple programming languages is challenging. Any code change, due
to a feature request or a bug /f_ix, has to be propagated timely to
all programming languages. At present, developers have to manu-
allyco-evolve code.Thisrequiresdeveloperstomanually/f_indthe
correspondence between code snippets and apply necessary edits.
Therehasbeenworkthatcould,intheory,helpwithtranslation.
Rule-based migration tools [ 3,18,50] have been designed to trans-
late between high-level programming languages (e.g., Java and C#).
However,rule-basedsystemsrequiredeveloperswhohaveexper-
tisewithbothprogramminglanguagestomanuallywriterulesto
specifythetranslationmappings.Andtherulesneedtobeupdated
with the evolution of programming languages themselves; they
quickly become outdated [ 3,9]. Recent work on automatic code
translation [ 27,33,46,49,60] aim to directly translate between a
source and a target programming language with the help of LLMs,
whicharepretrainedonmultipleprogramminglanguages.While
these techniques could be used to produce code snippets that look
correct, they make irrelevant changes that deviate substantially
from the newly introduced features in the source programming
language, or they fail to precisely infer the project-speci/f_ic data
types andclass names.
Figure1illustrates the limitation of existing models. Developers
changedPdfException toLayoutExceptionMessageConstant in
methoddocWithInvalidMapping02 in the Java project itext/-
itext7. In a later commit in the corresponding C# project itext/-
itext7-dotnet,developersrevisedmethod DocWithInvalidMapp-
ing02with exactly the same edits while keeping other parts of the
methodunchanged.WeprovidetheJavacodechange,thepredic-
tionofanexistinglargelanguagemodel,CodeT5[ 55],/f_ine-tuned
for code translation, and the correct C# code change in Figure 1.
The addedlines ofcode are highlighted in green and the removed
ones are highlighted in red. Although the existing model is able
tocorrectlytranslatetheupdatedexceptiontypefromJavatoC#,
it misses the class name for the /f_ield HtmlRoles and incorrectly
infers the function call Assert.Catchas it does notuse the prior
version ofC#code forreference.
Tobuildmorerobustandaccuratetechniquesthathelpsoftware
developers co-evolve projects implemented in diﬀerent languages,
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
695
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiyangZhang,PengyuNie,JunyiJessy Li, andMilos Gligoric
1@Test
2public void docWithInvalidMapping02() throwsIOException {
3...
4customRolePara.getAccessibilityProperties().setRole(HtmlRoles.p);
5Exception e = Assert.assertThrows(PdfException. class,()->document.add(
customRolePara));
6- Assert.assertEquals(MessageFormat.format( PdfException .
ROLE_IS_NOT_MAPPED_TO_ANY_STANDARD_ROLE, "p"), e.getMessage());
7+ Assert.assertEquals(MessageFormat.format( LayoutExceptionMessageConstant .
ROLE_IS_NOT_MAPPED_TO_ANY_STANDARD_ROLE, "p"), e.getMessage());
8} JavaChangeMadeby Developers
1[NUnit.Framework.Test]
2publicvirtual voidDocWithInvalidMapping02() {
3...
4- customRolePara.GetAccessibilityProperties().SetRole(
LayoutTaggingPdf2Test .HtmlRoles.p);
5+ customRolePara.GetAccessibilityProperties().SetRole(HtmlRoles.p);
6- Exception e = NUnit.Framework.Assert. Catch(typeof(PdfException),()=>
document.Add(customRolePara));
7+ Exception e = NUnit.Framework.Assert. IsThrows (PdfException. class,()=>
document.Add(customRolePara));
8- NUnit.Framework.Assert.AreEqual(String.Format( PdfException .
ROLE_IS_NOT_MAPPED_TO_ANY_STANDARD_ROLE, "p"), e.Message);
9+ NUnit.Framework.Assert.AreEqual(String.Format(
LayoutExceptionMessageConstant .
ROLE_IS_NOT_MAPPED_TO_ANY_STANDARD_ROLE, "p"), e.Message);
10} C#ChangePredicted by Existing Generation-based Model
1[NUnit.Framework.Test]
2publicvirtual voidDocWithInvalidMapping02() {
3...
4customRolePara.GetAccessibilityProperties().SetRole(
5LayoutTaggingPdf2Test.HtmlRoles.p);
6Exception e = NUnit.Framework.Assert.Catch(typeof(PdfException),()=>
document.Add(customRolePara));
7- NUnit.Framework.Assert.AreEqual(String.Format( PdfException .
ROLE_IS_NOT_MAPPED_TO_ANY_STANDARD_ROLE, "p"), e.Message);
8+ NUnit.Framework.Assert.AreEqual(String.Format(
LayoutExceptionMessageConstant .
ROLE_IS_NOT_MAPPED_TO_ANY_STANDARD_ROLE, "p"), e.Message);
9}C#ChangeMadeby Developersand Predicted by Our Codeditor✓
Figure1:ExampleofusingLLMstohelpdevelopersco-evolve
code in two programming languages. The top box shows
developer-made changes in a Java project itext/itext7,
whichneedstobepropagatedtothecorrespondingC#project
itext/itext7-dotnet.Themiddleboxshowstheprediction
byanexistinggeneration-basedlargelanguagemodel,which
incorrectly changes irrelevant parts of the code. The bottom
box shows the correct prediction by our model, Codeditor .
weexplicitlymodelthe changesthatneedtobemade.Weformu-
late a novel task: automatically updating code snippets in a target
programminglanguage,basedonthe changesmadeinthesource
programminglanguage.
Most of the existing models implicitly tackle the code evolution
tasks by generating tokens one by one in accordance with the
underlyinglearnedprobabilityinsteadoffocusingonhowthecode
shouldbe modi/f_iedorretained.Priorwork[ 10,11,15,41,51,57,59]
have shown that standard generation-based models underperform
models that explicitly modelthe editsonsoftware-editing tasks.
To model code evolution across programming languages, we
design an LLM, dubbed Codeditor , which learns to align the edits
acrossprogramminglanguagesandexplicitlyperformseditsonthe
oldversionofthecodeinatargetprogramminglanguage.Following
prior work [ 15,41,48,59], we enable the model to reason aboutnecessary edits and learn to apply them by directly generating an
editsequence.
For training and evaluation, we collect the /f_irst dataset with
aligned Java and C# code changes on the methods with similar
functionality and implementations. Speci/f_ically, we extract 6,613
pairsofcodechangesfrom8open-sourceJavaprojectsandthecor-
responding C# projects on GitHub by mining the commit histories.
Thisisthe/f_irstdatasetcontainingparallelcodechangesoftwopro-
gramming languages. We conduct the evaluation in two directions,
updatingC#methodbasedontheJavachanges(sourcelanguageis
JavaandtargetlanguageisC#)andupdatingJavamethodbasedon
the C# changes (source language is C# and target language is Java).
Our results show that Codeditor outperforms all existing mod-
els across all the chosen automatic metrics, including the large
pretrained generative models Codex [ 12] under few-shot setting
and ChatGPT [ 40] under zero-shot setting. Codeditor achieves
96 (out of 100) CodeBLEU score on the task of updating C# code
based on Java changes, which is more than 25% higher than the
large pretrainedgeneration-basedmodel/f_ine-tunedonthis task.
Further,we/f_indthat Codeditor andgeneration-basedmodels
are complementary to each other as Codeditor is better at up-
dating longer code snippets while generation model is better at
handling the shorter ones. Thus, we combine the two models by
choosingeithermodel’spredictionbasedonthesizeoftheinput
code.Ourresultsshowthatthecombinationcanfurtherimprove
ourCodeditor model’sexact-match accuracyby6%.
The main contributionsof this paper include:
•Task. We formulate a novel task of automatically updating code
writteninoneprogramminglanguagebasedonthechangesin
the corresponding code inanotherprogramming language.
•Model. We design and implement Codeditor , the /f_irst LLM for
this task which learns to align the edits across programming
languages and explicitly performs edits on the old version of the
code intarget programming language.
•Dataset.Wecreatethe/f_irstdatasetwithalignedcodechangesfor
two programminglanguages(JavaandC#)from8open-source
projectpairs.
•Results.Weshowthat Codeditor signi/f_icantlyoutperformsthe
existing LLMs /f_ine-tuned for code translation on exact-match
accuracyby77%.Wealsoshowthat Codeditor iscomplemen-
tary to generation-based LLMs and the combination can further
improveCodeditor ’sexact-match accuracyby6%.
Codeditor andour corpusare publiclyavailable onGitHub:
https://github.com/EngineeringSoftware/codeditor .
2 TASK
At a high level, we work on a system that is triggered when a
softwaredeveloper,whomaintainsprojectswritteninmultiplepro-
gramming languages,makeschangestoonemethod inone ofthe
languages, i.e., the “source” language. The system would automati-
callysuggestupdatestothemethodswithidenticalfunctionality
in other language(s), i.e., the “target” language(s). To scope our
work inthis paper,we focuson Javaas the source language, and
C#asthetargetlanguage.Weleaveevaluationthattargetsother
programming languagesas future work.
696Multilingual Code Co-evolution usingLarge LanguageModels ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 1: The mappings between concise edit sequence and
unambiguous editsequence.
Edit Concise Unambiguous
Insertion <Insert><ReplaceKeepBefore>
<ReplaceKeepAfter>
Deletion <Delete><Delete>
<ReplaceKeepBefore>
<ReplaceKeepAfter>
Replacement <Replace><Replace>
<ReplaceKeepBefore>
<ReplaceKeepAfter>
In Figure 1, consider a method /u1D440/u1D446;/u1D45C/u1D459/u1D451(docWithInvalidMapp-
ing02)writteninthesourcelanguage /u1D446andamethod /u1D440/u1D447;/u1D45C/u1D459/u1D451(Doc-
WithInvalidMapping02 ) written in the target language /u1D447with
identicalfunctionality(hencesimilarimplementation).Giventhe
updated method /u1D440/u1D446;/u1D45B/u1D452/u1D464in/u1D446, we de/f_ine the task to generate the
new method /u1D440/u1D447;/u1D45B/u1D452/u1D464in/u1D447leveraging context provided by the code
changes/u1D438/u1D446, such thatits functionality is consistent with /u1D440/u1D446;/u1D45B/u1D452/u1D464.
Namely,we modelthe conditional probability distribution
/u1D443(/u1D440/u1D447;/u1D45B/u1D452/u1D464|/u1D440/u1D447;/u1D45C/u1D459/u1D451, /u1D440/u1D446;/u1D45B/u1D452/u1D464,/u1D438/u1D446)
andgenerate /u1D440/u1D447;/u1D45B/u1D452/u1D464bysampling from the distribution.
3 MODEL
We present the overview of the proposed Codeditor model in
Figure2.Codeditor is built upon the encoder-decoder framework
whichconsistsofatransformer-basedencoderandatransformer-
based decoder [ 53]. Many conditional generation tasks, includ-
ing code summarization andtranslation, are being addressed with
encoder-decoder models [ 1,21,37,54,55].
Weinitialize Codeditor ’sparameterswiththepretrainedlan-
guagemodelCoditT5[ 59].CoditT5hasshownpromisingresults
onvarious software-related editing tasks in a singleprogramming
language, but nonetheless would provide us with a “warm-start”
that carries the necessary inductive biases towards modeling edits.
To adapt to the multilingual co-editing task, we then /f_ine-tune the
Codeditor modelexploringtwokeycomponents:(i)thecontext
fedintothe model; (ii)the outputformat ofthe model.
To encourage our Codeditor model to leverage the (synchro-
nous) code change histories of multiple programming languages
in itstraining data, we provide the model with context from three
sources as shown inFigure 2: (i)code changes on source program-
ming language ( /u1D438/u1D446); (ii) old version of the code written in target
programming language ( /u1D440/u1D447;/u1D45C/u1D459/u1D451); (iii) new version of the code writ-
teninsourceprogramminglanguage( /u1D440/u1D446;/u1D45B/u1D452/u1D464).
Weexploretwoformatstorepresentthegeneratedcodechanges:
(i) the code edits in the target programming language ( /u1D438/u1D447); (ii) a
metaeditsequencethattranslatesthecodeeditsfromthesource
programminglanguagetothetargetprogramminglanguage,fol-
lowed by the code edits in the target programminglanguage (this
is similartotheoutput format ofCoditT5). In bothcases,we then
applythegeneratedcodeeditsinthetargetprogramminglanguage
(/u1D438/u1D447)totheoldversionofthecode( /u1D440/u1D447;/u1D45C/u1D459/u1D451)toobtainthenewversion
ofthe code ( /u1D440/u1D447;/u1D45B/u1D452/u1D464).3.1 EditRepresentations
3.1.1 Concise Edit Sequence. We /f_irst represent edits using a se-
quenceofeditsidenticaltothatusedinCoditT5[ 59],whichwecall
concise editsequence.Eacheditisrepresentedas:
<Operation> [token span] <OperationEnd>
Here,<Operation> iseitherInsert,DeleteorReplace.Notethat
theReplaceis represented in a slightly diﬀerent structure since
wemustspecifyboththeold contentstobereplacedandthenew
contentsto replace with:
<ReplaceOld> [old contents] <ReplaceNew>
[new contents] <ReplaceEnd>
Forexample,inFigure 1,thecodechange ontheoldJavamethod
canbe represented by“ <ReplaceOld> PdfException <Replace-
New> LayoutExceptionMessageConstant <ReplaceEnd> ”.
Weusedifflib[17]tocomputethesetofminimaleditsequence
from the old andnewversionsof code.
3.1.2 UnambiguousEditSequence. OnedrawbackofCoditT5[ 59]’s
representation speci/f_ied above is that the concise edit sequence
can be ambiguous due to the absence of positional information.
Forexample,theJavacodechangeinFigure 1canberepresented
usingReplace as: “<ReplaceOld> PdfException <ReplaceNew>
LayoutExceptionMessageConstant <ReplaceEnd> ”.Withoutfur-
ther speci/f_ication, the edit does not contain any clues regarding
whichPdfException should be replaced as there are two occur-
rencesof PdfException intheold codesequence.For similarrea-
sons,Insertisalwaysambiguousbecauseofnotindicatingwhere
toaddthenewcontentsand Deleteisambiguousincaseswhere
multiple occurrences oftoken spans can be removed.
Toeliminatethepotentialambiguityintheconciseeditsequence,
wedesigntheformatofunambiguouseditsequencebyadjusting
the condensed edit sequence proposed by Panthaplackel et al . [41],
whichuses anchor tokens to specifythe location to perform edits.
Insertion . We do not use Insertsince it will always introduce
ambiguitywithoutlocationinformation.Torepresentinsertion,we
/f_irst /f_ind unique anchor tokens that are the shortest span of tokens
thatiseitherbeforeoraftertheeditlocationandisuniqueinthe
inputsequence.Thenweuse ReplaceKeepBefore orReplaceKeep -
After,whichrepresentsreplacingtheanchortokenswiththein-
serted contents and the anchor tokens. For example, in Figure 1,
supposetheJavacodechangeentailsaddingablankreturnstate-
ment after the assertEquals statement on line 7. The token span
“getMessage ());” will serve as the minimal span of anchor tokens
becauseitisuniqueamongtheoldJavacodesequence,anditoccurs
right before the edit to be performed. We disambiguate the edit
sequence:
<Insert> return; <InsertEnd>
withthe unambiguous editsequence:
<ReplaceOldKeepBefore> getMessage());
<ReplaceNewKeepBefore> getMessage()); return;
<ReplaceEnd>
This edit sequence indicates that “ getMessage ());” should be re-
placedwith“ getMessage ());return;”.Weintroduce ReplaceKeep -
Beforewherethetokensthatfollowsthe <ReplaceOldKeepBefore>
697ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiyangZhang,PengyuNie,JunyiJessy Li, andMilos Gligoric
<ReplaceOldKeepBefore>
format(PdfException
<ReplaceNewKeepBefore> format(
LayoutExceptionMessageConstant
<ReplaceEnd>/u1D438/u1D446
...
String.Format(PdfException.../u1D440/u1D447;/u1D45C/u1D459/u1D451
...
MessageFormat.format(
LayoutExceptionMessageConstant../u1D440/u1D446;/u1D45B/u1D452/u1D464EditsTranslation
MetaEdits<ReplaceOldKeepBefore>
Format(PdfException
<ReplaceNewKeepBefore> Format(
LayoutExceptionMessageConstant
<ReplaceEnd>/u1D438/u1D447
<ReplaceOld> format
<ReplaceNew> Format
<ReplaceEnd>meta edit sequence<ReplaceOldKeepBefore>
Format(PdfException
<ReplaceNewKeepBefore> Format(
LayoutExceptionMessageConstant
<ReplaceEnd>/u1D438/u1D447...
String.Format(
LayoutExceptionMessageConstant../u1D440/u1D447;/u1D45B/u1D452/u1D464
Figure2:Work/f_lowof Codeditor formultilingualco-editing. Codeditor leveragesthecontextofcodechangehistoriesof
multiple programming languages from three sources: code changes on the source programming language ( /u1D438/u1D446), the old version
ofcodeinthetargetprogramminglanguage( /u1D440/u1D447;/u1D45C/u1D459/u1D451),andthenewversionofcodeinthesourceprogramminglanguage( /u1D440/u1D446;/u1D45B/u1D452/u1D464).
Codeditor hastwovariantsthatbothgeneratethecodechangesinthetargetprogramminglanguage( /u1D438/u1D447)butindiﬀerent
formats:EditsTranslationdirectlygeneratesthecodechanges;MetaEditsgeneratesthemetaeditplanwhichedits /u1D438/u1D446to/u1D438/u1D447,
followed by the code changes. Finally, we apply the code changes ( /u1D438/u1D447) on the old version of code ( /u1D440/u1D447;/u1D45C/u1D459/u1D451) to obtain the new
versionofcode( /u1D440/u1D447;/u1D45B/u1D452/u1D464) inthetargetprogramming language.
should be removed and the tokens following <ReplaceNewKeep-
Before> shouldbeinserted.Diﬀerentfrom Replace,thereissome
overlap between the tokens to be removed and tokens to be in-
serted.Ifanchortokensdonotexistbeforetheeditlocation,weuse
ReplaceKeepAfter with thetokens after theeditlocation instead.
Replacement . If the span of tokens to be replaced is unique in
the old sequence, regular Replace sequence is suﬃcient and deter-
ministic; in that case we will keep using it. Otherwise, it is unclear
whichoccurrenceoftokenspanshouldbereplaced.Asanexample,
in Figure 1, the Java code change is changing from PdfException
toLayoutExceptionMessageConstant intheassertEquals state-
mentonline6.Thereplacementintheconciseeditsequenceisam-
biguous because there are two usages of PdfException (on lines 5
and 6) in the old Java code sequence after tokenization. To address
this,similartotheinsertioncase,wesearchfortheminimalanchor
tokensbeforeoraftertheeditlocationthatcanformauniquespan
inthe old sequence.For example,the concise editsequence:
<ReplaceOld> PdfException <ReplaceNew>
LayoutExceptionMessageConstant <ReplaceEnd>
canbedisambiguateintothefollowingunambiguouseditsequence:
<ReplaceOldKeepBefore> format(PdfException
<ReplaceNewKeepBefore> format(
LayoutExceptionMessageConstant <ReplaceEnd>
Deletion .Similartoreplacement,ifthespanoftokenstobedeleted
is unique across the old sequence, we will keep using Delete
because it is unambiguous. Otherwise, it will be transformed to
ReplaceKeepBefore orReplaceKeepAfter .Forexample,suppose
the token “ PdfException .” should be removed from the old Java
methodonline6inFigure 1.The concise editsequence:
<Delete> PdfException. <DeleteEnd>willbe transformedto:
<ReplaceOldKeepBefore> format(PdfException.
<ReplaceNewKeepBefore> format( <ReplaceEnd>
This edit sequence indicates that “ format(PdfException .” should
be replaced with “ format(”, unambiguously implying the deletion
of“PdfException .”.
To summarize, the unambiguous edit sequence contains 4 types
of edits:<Replace> ,<Delete> ,<ReplaceKeepBefore> and<Re-
placeKeepAfter> .Themappingsbetweenconciseeditsequence
and unambiguousedit sequenceare summarized inTable 1. Given
the unambiguous edit sequence, we can apply it to the old input
sequenceto derive the neweditedsequencedeterministically.
3.2 Model Input
Weaimtobuildperformantmachinelearningmodelsforthemulti-
lingual co-editing task by providing the model with code evolution
information,namelytherevisionsofcodeofbothsourceandtarget
programminglanguages.Insteadofdirectlytranslatingtheentire
codesnippetbetweenprogramminglanguages, Codeditor trans-
latesthe code changesbetween programming languages.
3.2.1 SourceCodeEdits. Toencouragethemodeltolearnthealign-
mentbetweendeveloper-madechangesacrossprogramminglan-
guages, we provide Codeditor with code changes in the source
programming language. To maintain both precision and concise-
ness of the edits, we adopt the unambiguous edit sequence (Sec-
tion3.1.2) to represent the code changes. As shown in Figure 2,
the Java code changes ( /u1D438/u1D446) of replacing the PdfException with
LayoutExceptionMessageConstant isstructuredinthe form of
698Multilingual Code Co-evolution usingLarge LanguageModels ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
<ReplaceOldKeepBefore> format(PdfException
<ReplaceNewKeepBefore> format(
LayoutExceptionMessageConstant <ReplaceEnd>
3.2.2 History-Related Context. In addition to the learned repre-
sentationofcodechangesinsourceprogramminglanguage( /u1D438/u1D446),
weprovide Codeditor withtheoldcodeintargetprogramming
language( /u1D440/u1D447;/u1D45C/u1D459/u1D451)tobetterhelpthemodeltoinferthecorrelated
code changes in the target programming language. The intuition is
thatthemodelwillreasonabouthowtotransferandtunetheedits
insourceprogramminglanguagegroundingthespeci/f_icimplemen-
tation ofthe methodintarget programming language.
Furthermore,weappendthenewcodeinsourceprogramming
language ( /u1D440/u1D446;/u1D45B/u1D452/u1D464) as one of the contexts. We believe this will give
the model more context to understand the edits in source program-
minglanguageandpromotetheconsistencyoftheupdatedmethods
intwoprogramminglanguages.
Tosumup,wecombinehistory-relatedcontextfromthreesources:
codechangesinthesourceprogramminglanguage( /u1D438/u1D446),oldcode
in the target programming language ( /u1D440/u1D447;/u1D45C/u1D459/u1D451), and new code in the
sourceprogramminglanguage( /u1D440/u1D446;/u1D45B/u1D452/u1D464).Weconcatenatetheminto
asequenceseparatedbyaspecial SEPtoken as the modelinput.
3.3 Model Output
Weproposetwoformatsasthemodel’stargetoutputwhichlead
to two modes of Codeditor :EditsTranslation andMetaEdits . Both
modes usethe same input andbothmodes’target outputs entaila
sequenceofeditsonthe target programming language.
EditsTranslation . The output of EditsTranslation mode is the un-
ambiguouseditsequenceintargetprogramminglanguagewhich
suggests how the code in target programming language should
bechanged.Notethatthemodel-generatedunambiguouseditse-
quence can be parsed and applied to old version of codedetermin-
istically. EditsTranslation essentially learns to translate the code
edits from the source programming language ( /u1D438/u1D446) to the target
programming language ( /u1D438/u1D447) grounding the code history context.
EditsTranslation mode’s target output forthe C# example inFig-
ure1is:
<ReplaceOldKeepBefore> Format(PdfException
<ReplaceNewKeepBefore> Format(
LayoutExceptionMessageConstant <ReplaceEnd>
MetaEdits .Inthismode,weadopttheoutputformatofCoditT5[ 59]
formultilingualco-editingsinceourmodelisbuiltuponCoditT5,
and it had showed promising performance on software editing
tasks.CoditT5ispretrainedtogeneratethefollowingoutputfor-
mat: “[Edit Plan] <SEP> [Target Sequence]”. The edit plan is a
concise edit sequence that represents the steps to edit the input
sequence;thetargetsequenceistheeditedsequenceafterapplying
theproceedingeditplan.Wetailoredthisformattothemultilingual
co-editing task; the edit plan represents the edits between the code
edits on source programming language ( /u1D438/u1D446) and target program-
ming language ( /u1D438/u1D447) which we call the meta edit sequence . And the
/f_inal target sequence should be the unambiguous edit sequence on
thetargetprogramminglanguage( /u1D438/u1D447).FortheexampleinFigure 1,
the expected meta edit sequence that converts Java edit to C# edit
isthe following:Table2:Open-sourceprojectsusedinourdatasetandnumber
ofexamples fromeachproject.
JavaProject C#Project Count
antlr/antlr4 tunnelvisionlabs/antlr4cs 12
apache/lucene apache/lucenenet 40
apache/poi nissl-lab/npoi 5
eclipse/jgit mono/ngit 808
formicary/fpml-toolkit-java formicary/fpml-toolkit-csharp 20
itext/itext7 itext/itext7-dotnet 5,121
quartz-scheduler/quartz quartznet/quartznet 17
terabyte/jgit mono/ngit 590
SUM 6,613
Table 3: Statistics of our dataset. Number of examples of
training, validation and test data; average number of tokens
in the old version of method and new version of method;
averagenumberofeditsforthecodechange;averagenumber
ofadded anddeleted tokens.
Train Val Test
Count 4,391 623 1,599
JavaAvg.len(/u1D440/u1D45C/u1D459/u1D451)193.05 192.88 159.06
Avg.len(/u1D440/u1D45B/u1D452/u1D464)195.99 192.36 159.37
Avg.#edits 2.71 2.68 2.43
Avg.#add. tks 19.57 16.64 10.90
Avg.#del.tks 16.62 17.16 10.59
C#Avg.len(/u1D440/u1D45C/u1D459/u1D451)200.37 199.71 168.60
Avg.len(/u1D440/u1D45B/u1D452/u1D464)203.49 199.47 169.22
Avg.#edits 2.73 2.75 2.47
Avg.#add. tks 20.30 17.69 11.86
Avg.#del.tks 17.18 17.92 11.25
<ReplaceOld> format <ReplaceNew> Format <ReplaceEnd>
The target sequenceafter applyingthe meta editsequenceis:
<ReplaceOldKeepBefore> Format(PdfException
<ReplaceNewKeepBefore> Format(
LayoutExceptionMessageConstant <ReplaceEnd>
Notethatduringinference,weonlyusethetargetunambiguousedit
sequence to get the updated code in target programming language
as MetaEdits mode’sprediction.
4 DATASET
Thisisthe/f_irstworktoconsiderthehistoryofsoftwareprojects
in a multilingual task; hence, we also created a new dataset that
includesalignedcodechangesbetweenprogramminglanguages.
As the /f_irst step, we build the dataset by mining histories of the
open-source Java and C# projects. We /f_irst collect the changed
methods from the commits of the Java and C# projects. We then
design heuristics to pair (i.e., align) those changes on methods
with similar implementations and functionalities. We consider two
directions on our dataset: J2CS (updating C# method based on Java
changes)andCS2J(updatingJavamethodbasedonC#changes).In
this section, we describe the approach we use to collect the data
(Section4.1), split and preprocess data (Section 4.2), and /f_inally
present the statisticsofour dataset (Section 4.3).
699ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiyangZhang,PengyuNie,JunyiJessy Li, andMilos Gligoric
4.1 Data Collection
Tobuild thedataset,we extractalignedJavaand C#codechanges
atthemethodlevelastuples(Javaoldmethod;Javanewmethod,
C#oldmethod;C#newmethod).Thecodechangesareminedfrom
the git commits. We consider 8 open-source projects as listed in
Table2whichhavebothJavaandC#implementationsandareused
inpriorwork[ 13,33,36].Alltheprojectswere/f_irstdevelopedin
Java andthen portedto C#.
To collect the paired changes, we /f_irst assign a unique identi/f_ier
toeachmethodintheprojects(forbothJavaandC#projects)based
on the method signature, class name and path to the /f_ile where
the method is de/f_ined. Similar to the strategy used by Lu et al .
[33],we thenpairtheJavamethodsandC#methodsaccordingto
thesimilarityoftheiruniqueidenti/f_iers.Thisstrategyiseﬀective
becausetheportedC#projecthasverysimilarstructureandnaming
rulesfor classesandmethodsto the corresponding Java project.
We use the following rulesto extractthe alignedcode changes:
(1)ForeachJavamethodchange,weextractthecodechangesin
the paired C# method that happen no later than 90 days of the
Java change as the possible matched code change . We use the
commit dateas the time ofthe change.
(2)To/f_ilterunrelatedcodechanges,wecomputetheJaccardsim-
ilarity [24] between C# and Java added and deleted lines. We
further re/f_ine the/f_iltering by sub-tokenizing these linesbased
on camelCase conventions (e.g., lastModified tolast modi -
fied) and compute Jaccard similarity only for the added and
deletedtokens.Weonlykeeppossiblematchedcodechanges
that have the token-level Jaccard similarity higher than 0.4 and
the line-level Jaccardsimilarityhigher than0.5.
(3)For each Javacode change and C# code change, we only select
themostsimilarcorrespondingcodechangeiftherearemultiple
possible matchedcode changes.
4.2 Data Preprocessing andSplitting
ForbothJavaand C#methods,we remove the inline naturallan-
guage comments and tokenize the method into tokens using the
language-speci/f_ic lexersgeneratedbyAntlr [ 43].
We envision the following use case for the machine learning
model:wheneveradevelopermakesachangeintheprojectwritten
in the source programming language, the developer will use the
model trained on the existing historical aligned code changes to
migratethatchangetoprojectswritteninothertargetprogramming
languages.Toevaluatethemodelsunderthisusecase,following
the recommendations from prior work [ 38], we split the dataset
into training, validation and test sets using the time-segmented
approach.Namely,thechangesinthetrainingsettookplacebefore
thechangesinthevalidationset,whichinturntookplacebefore
thechangesinthetestset.Morespeci/f_ic,foreachJavaandC#code
changepair,we/f_irstcollectthetimeoftheC#commitandthensort
the code change pairs in chronological order. We then select the
oldest70%ofthecodechangepairsfromeachprojectastraining
data, next oldest 10% as validation data, the remaining as test data.
To morerigorouslyassess thegeneralizationcapabilitiesof the
models,wealsoevaluatedthemwhensplittingthedatasetusingthe
cross-project approach [ 38], which is frequently used in prior workonmachinelearningmodelsforcode.Speci/f_ically,thealignedcode
changes in the training set are from diﬀerent projects compared to
thoseinthe validation andtest sets.
4.3 Statistics
The statistics of the collected dataset are shown in Table 3. We
present the number of examples in the training, validation, and
test dataset using time-segmented split approach. We show the
average number of tokens in the old methods (Avg. len( /u1D440/u1D45C/u1D459/u1D451)) and
new methods (Avg. len( /u1D440/u1D45B/u1D452/u1D464)) after tokenization by the lexers. To
measure the size of the code changes, we calculate the average
numberofaddedtokens(Avg.#add.tks) anddeletedtokens(Avg.
# del. tks) in the changed Java and C# methods as well as the
average number of edits (Avg. # edits) needed for those changes.
Forcomputingtheseedit-relatedstatistics,werepresentthecode
changes using concise editsequences(Section 3.1.1).
ForbothJavaandC#codechanges,thediﬀerencebetweenav-
erage number of added tokens and deleted tokens is usually small,
fewer than 4 tokens. Similarly, we /f_ind that the average number of
editsneededisfewerthan3andtheeditshappenedinthenewer
commits are generally smaller than prior ones. This is expected as
the software projects are becoming more stable as they evolve, and
thus there will be smaller code changes to be made. For evaluation,
werunallthemodelsandbaselinesonthisdatasetintwodirections:
(1) updating C# method based on Java changes, and (2) updating
Java method based on C# changes. We denote the former one as
J2CSandthe latter one as CS2J.
5 EXPERIMENTS
Inthissection,wedescribethebaselineswecomparetowithour
Codeditor model(Section 5.1),theevaluationmetrics(Section 5.2)
andthe detailedexperiment setup(Section 5.3).
5.1 Baselines
We evaluate our approach against rule-based models, pretrained
encoder-decodermodels,thestate-of-the-artcode-editingmodel
(whichtargets asingleprogramminglanguage), andlargegenera-
tive models pretrainedonbillionsof linesof code.
Copy. This is a rule-based model which copies the old code in
target programming language ( /u1D440/u1D447;/u1D45C/u1D459/u1D451) as the prediction. This is
not a trivial baseline since there are quite a few examples in the
dataset that entail small edits between two versions. We include
this to benchmarkthe models that actually updatethe code.
CopyEdits . Based on our observations, there are cases where the
codechangeinsourceprogramminglanguage( /u1D438/u1D446)isexactlythe
same as the change in target programming language ( /u1D438/u1D447) , such
aschangingthevariablenameorupdatingthelogmessage.This
rule-based model copies the /u1D438/u1D446and directly applies it to the old
code intarget programming language( /u1D440/u1D447;/u1D45C/u1D459/u1D451).
CodeT5-Translation .Weconsiderastate-of-the-artmodelthat
does not have access to the code change history. Namely, a code
translationmodelthattranslatescodebetweentheprogramming
languages(from /u1D440/u1D446;/u1D45B/u1D452/u1D464to/u1D440/u1D447;/u1D45B/u1D452/u1D464).We useCodeT5[ 55], anLLM
pretrainedonlargeamountofdeveloper-writtencodefromGitHub,
whichwe /f_ine-tuneonour constructeddataset.
700Multilingual Code Co-evolution usingLarge LanguageModels ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
CodeT5-Update .ThismodelhasthesamearchitectureasCodeT5-
Translation except that we supply it with code change history.
The model input is the same as for our Codeditor models, i.e.,
with extra context of the old code in target programming language
(/u1D440/u1D447;/u1D45C/u1D459/u1D451)andthecodechangeinsourceprogramminglanguage( /u1D438/u1D446).
Diﬀerent from Codeditor model, it is trained to directly generate
the newcode intarget programminglanguage( /u1D440/u1D447;/u1D45B/u1D452/u1D464).
CoditT5. This is the state-of-the-art model for software editing
tasks [59]. It has the same model architecture and input as Coded-
itor, while the output consists of the edit plan to represent the
edits on the target programming language and the target sequence
whichrepresentstheupdatedcode( /u1D440/u1D447;/u1D45B/u1D452/u1D464)afterapplyingtheedit
plan.
Codex-few-shot[ 12].Largepretrainedgenerativemodelssuch
asGPT-3[ 8]haveshownimpressiveresultsunderthecontextof
few-shot learning or evenzero-shot learning on various generation
tasks. They are ableto generalize to new tasksthey have not seen
duringpretrainingwithonlyafeworevennolabeledexamples.To
compare the /f_ine-tuned Codeditor model with generative models,
we include Codex, a large generative model built on GPT-3 and
is further pretrained on billions of GitHub data. Following prior
work [2,26], for each example in test data, we randomly select
severallabeledexamplesinthetrainingdataasthecontext.Note
that the labeled examples are selected from the same project as
the test data. For J2CS dataset, each labeled example is formed
as: “Java: /u1D440/u1D446;/u1D45C/u1D459/u1D451=>/u1D440/u1D446;/u1D45B/u1D452/u1D464C#:/u1D440/u1D447;/u1D45C/u1D459/u1D451=>/u1D440/u1D447;/u1D45B/u1D452/u1D464” to inform the
modelthealignedupdatesbetweentwoprogramminglanguages.
Thedesignedpromptforinferenceis“Java: /u1D440/u1D446;/u1D45C/u1D459/u1D451=>/u1D440/u1D446;/u1D45B/u1D452/u1D464C#:
/u1D440/u1D447;/u1D45C/u1D459/u1D451=>”.Themodeloutputisthepredictionforthenewcodein
targetprogramminglanguage( /u1D440/u1D447;/u1D45B/u1D452/u1D464).Toconformtotherequired
inputlength limit,we include 2labeledexamples inthe prompt.
ChatGPT-zero-shot [ 40]. ChatGPT is an upgraded version of
GPT-3 model and is further /f_ine-tuned for conversation generation
following human instructions with the help of supervised and rein-
forcement learning methods. It has showed strong performance on
codecompletionbenchmarkslikeHumanEvalandMBPP[ 4,12,39].
For each example in test data, we provide instructions including
both theprevious and theupdated versions ofthecodewritten in
the source programming language, subsequently prompting Chat-
GPT to update the old code in the target programming language
accordingly. For J2CS dataset, the prompt is formed as: “The de-
veloper updates the Java method from: /u1D440/u1D446;/u1D45C/u1D459/u1D451to:/u1D440/u1D446;/u1D45B/u1D452/u1D464. Please
update the C# method accordingly. This is the old C# method:
/u1D440/u1D447;/u1D45C/u1D459/u1D451.”
5.2 EvaluationMetrics
Following priorwork [ 1,41,55,59],we usemetricsforevaluating
thequalityofcodegeneration:BLEU[ 42],CodeBLEU[ 45],xMatch,
andmetricsforevaluatingthequalityofsoftwareediting:SARI[ 56]
andGLEU[ 35].Notethatforallthemetricswereportinthispaper,
they range from 0to 100andhigher scores are better.
xMatch. When the generated code matches exactly with the ex-
pected code in target programming language, this metric is 100;
otherwise, this metric is 0. This metric re/f_lects the percentage of
exact matches among the models’predictions ontest data.BLEU. It is a widely used metric originally proposed for evalu-
ating the quality of machine translation. It measures the n-gram
overlap between the generated sequence and the expected one.
Concretely,wereportthe1 ∼4-gramsoverlapbetweenthetokens
inthe predictions andtokens inthe ground truth.
CodeBLEU .Themetricisproposedforevaluatingthequalityof
code generation. In addition to measuring the n-gram overlap, it
considerstheoverlapoftheAbstractSyntaxTree(AST)anddata-
/f_lowgraph between generatedcode andthe expectedcode.
SARI.Itmeasuresqualityofthesystemsthataredesignedtomake
edits. Speci/f_ically, it is computed as the average of the F1 score
for kept and inserted spans of tokens, and the precision of deleted
spans oftokens.
GLEU. It is a variant of BLEU. It was originally proposed for gram-
matical error correction and designed for rewarding the correct
editswhilepenalizingthe incorrectones.
5.3 ExperimentalSetup
We run all experiments on machines with 4 NVidia 1080-TI GPUs,
Intel(R)Xeon(R)CPUE5-2620v4@2.10GHzfortraining.Weim-
plement our models using PyTorch 1.9.0. All the hyper-parameters
oftheCodeT5andCoditT5baselinesaresettothesamevaluesasin
priorwork[ 55,59].ForCodeditor ,CodeT5-Translation,CodeT5-
Update, and CoditT5, we early stop the training when the BLEU
score onthe validation set doesnot improve for 5 epochs,and use
beam search with beam size 20 during inference. For Codex and
ChatGPT,we settemperature to 0.2 duringinference.
NotethatCodexandChatGPTareclosed-sourceandmaybeup-
dated/deprecated over time. We used the code-davinci-002 version
of Codex when performing experiments in the time-segmented
split; however, OpenAI deprecated Codex in March 2023 before we
could complete our experiments in the cross-project split, as such
we didnot include Codex inthis part of results.
6 RESULTS
We organize our evaluation around three main research questions:
RQ1:What is the bene/f_it of using code change history in multilin-
gual co-editing?
RQ2:How does our edit-based model, Codeditor , compare to
generation-basedmodels for the multilingualco-editing?
RQ3:How can a generation-based model complement Codeditor
modelto further improve the performance?
6.1 QuantitativeAnalysis
In tables 4-7, we present results for baselines and our proposed
Codeditor models on J2CS, CS2J for both time-segmented and
cross-project splits. We conducted statistical signi/f_icance testing
throughbootstraptests [ 6]undercon/f_idence level 95%.
RQ1: Conntribution of code change histories. We divide mod-
els into two categories with respect to whether a model has access
to the information on code change histories: Copy and CodeT5-
Translation are history-agnostic models, and the remaining are
history-awaremodels.Overall,thehistory-awaremodelsoutper-
formthehistory-agnosticones.Therule-basedmodelCopyEdits,
701ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiyangZhang,PengyuNie,JunyiJessy Li, andMilos Gligoric
Table 4:Results on theJ2CS dataset. The results with thesamesuﬃxes (e.g., /u1D6FD) are NOTstatistically signi/f_icantly diﬀerent.
Models xMatch BLEU-4 CodeBLEU SARI GLEU
Copy 0.00 83.11 90.42 30.68 74.58
CopyEdits 38.21/u1D6FD90.29/u1D6FC/u1D71291.34 76.92 87.93
CodeT5-Translation 38.02/u1D6FD87.45 77.15 83.77 85.59
CodeT5-Update 60.41/u1D71690.00/u1D712/u1D70276.63 80.11 88.72
CoditT5 60.29/u1D71689.84/u1D6FC/u1D70275.20 80.99 89.29
Codex-few-shot 48.84 80.71 59.63 72.80 79.74
ChatGPT-zero-shot 29.52 85.60 73.00 68.44 84.74
Codeditor (MetaEdits) 63.48 94.55 94.78 85.63 93.20
Codeditor (EditsTranslation) 67.23 95.44 96.02 87.23/u1D6FF94.21
Hybrid 71.79 96.12 96.09 87.08/u1D6FF95.07
Table 5:Results on theCS2J dataset. The results with thesamesuﬃxes (e.g., /u1D6FD) are NOTstatistically signi/f_icantly diﬀerent.
Models xMatch BLEU-4 CodeBLEU SARI GLEU
Copy 0.00 83.06 89.82 30.66 74.55
CopyEdits 38.15 89.36/u1D6FC90.31/u1D6FD75.86 87.02/u1D712
CodeT5-Translation 40.21 89.10/u1D6FC77.99/u1D6FD83.99 87.21/u1D712
CodeT5-Update 55.97 90.62 76.38/u1D6FE79.65 89.72
CoditT5 60.98 90.88 75.87/u1D6FE81.41 90.15
Codex-few-shot 55.53 82.54 60.35 76.23 82.13
ChatGPT-zero-shot 32.52 86.95 76.01 69.05 86.33
Codeditor (MetaEdits) 68.61/u1D716/u1D70293.98 94.43 85.74 92.61
Codeditor (EditsTranslation) 67.92/u1D6FF/u1D71695.29 94.83 86.2494.23
Hybrid 67.67/u1D6FF/u1D70296.44 95.36 84.4695.75
Table6:Resultsonthecross-projectsplitusingJ2CSdataset.Theresultswiththesamesuﬃxes(e.g., /u1D6FD)areNOTstatistically
signi/f_icantly diﬀerent.
Models xMatch BLEU-4 CodeBLEU SARI GLEU
Copy 0.00 79.96/u1D6FC89.08 30.53 71.89
CopyEdits 14.19 87.95 89.73 67.58 85.55
CodeT5-Translation 10.64 77.34 64.35 71.73 74.16
CodeT5-Update 29.38 80.56/u1D6FC66.15 64.70 79.65
CoditT5 34.59 81.59 65.17 83.29 80.91
ChatGPT-zero-shot 39.58 86.97 74.90 70.15 86.14
Codeditor (MetaEdits) 38.36 90.79/u1D6FD91.60 73.45 88.94/u1D712
Codeditor (EditsTranslation) 41.91 90.86/u1D6FD91.35 74.59 88.94/u1D712
Hybrid 43.35 92.51 91.70 89.13 91.18
which directly applies the code change in source programming
language ( /u1D438/u1D446) to the old code in target programming language
without any adaptation, has comparable performance to the ma-
chine learning history-agnostic model CodeT5-Translation. This
emphasizesthe importance of contextual information providedby
codechangehistoriesinmultilingualco-editing.Interestingly,we
/f_ind that Codex-few-shot, which is used under the few-shotlearn-
ing setting without /f_ine-tuning, performs better than /f_ine-tunedCodeT5-Translation on xMatch, while worse than other history-
aware /f_ine-tunedmachine learning models. This again underlines
thevalueofcodechangehistoriesandsuggeststhat/f_ine-tuningwill
give better performance by leveraging more code history contexts
inthe training data.
RQ2:Codeditor vs. generation-based models. Among all the
history-awaremodels,machinelearningmodels,suchasCodeT5-
UpdateandCoditT5,achievemuchhigherperformancethanthe
702Multilingual Code Co-evolution usingLarge LanguageModels ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table7:Resultsonthecross-projectsplitusingCS2Jdataset.Theresultswiththesamesuﬃxes(e.g., /u1D6FD)areNOTstatistically
signi/f_icantly diﬀerent.
Models xMatch BLEU-4 CodeBLEU SARI GLEU
Copy 0.00 80.02 88.60 30.51 71.94
CopyEdits 13.86 86.99 88.70 66.50 84.14
CodeT5-Translation 6.21 76.84 62.27 67.37 69.81
CodeT5-Update 31.60 81.98 65.82 65.49 81.07
CoditT5 35.81 82.89 65.20 83.27 81.67
ChatGPT-zero-shot 39.80 89.35 76.69 72.36 88.62
Codeditor (MetaEdits) 41.91 91.54/u1D6FC91.21 73.46 89.36/u1D6FD
Codeditor (EditsTranslation) 40.35 91.63/u1D6FC90.99 74.15 89.58/u1D6FD
Hybrid 46.34 93.17 91.32 89.62 91.24
0 125 250 375 500
# subtokens0.00.20.40.60.81.0xMatch
CodeT5-Update
CODEDITOR(EditsTranslation)
Figure 3: Average percentage of model’s predictions that ex-
actlymatchthegroundtruthonexamplesthathavediﬀerent
number of subtokens. The bands represent the 95% con/f_i-
denceinterval.
0 50 100 150 200 250 300 350 400 450 500
# subtokens050100150200250300350400Count
CodeT5-Update
CODEDITOR(EditsTranslation)
Figure 4: Distribution of number of sub tokens in models’
targetoutputs.
rule-based CopyEdits, which demonstrates that the machine learn-
ingmodelseﬀectivelylearnstoreasonaboutthecorrelatedcode
changes and adjust them to the target programming language. We
observethat Codeditor (inbothEditsTranslationandMetaEdits
modes), which is trained to /f_irsttranslate codechanges onsource
programming language to target programming language and then
apply the edits to the old code in target programming language,achieve even higher performance across all the metrics than the
largepretrainedgeneration-basedmodel(CodeT5-Update)which
directlygeneratesthenewcodeintargetprogramminglanguage
from scratch. This highlights that the models that are trained to
explicitlyperformeditsbypredictingtheeditsequencearebetter
suited for editing tasks in the software domain than generation-
basedmodels.
To further investigate the advantages of Codeditor over the
best generation-based model (CodeT5-Update), we break down
theperformanceofEditsTranslationandCodeT5-Updateoneach
example in the test data of J2CS. In Figure 3, we show the average
percentage of Codeditor (EditsTranslation) and CodeT5-Update’s
predictionsthatexactlymatchthegroundtruthwithrespecttothe
number of sub-tokens in the input old code ( /u1D440/u1D447;/u1D45C/u1D459/u1D451). Note that the
codearesubtokenizedusingtheRobertatokenizer[ 31],whichis
usedbyallmachinelearningmodels.Weexcludetheexamplesthat
have more than 500 sub-tokens from being shown in this /f_igure
as those outliers only account for less than 5% of the test data. We
canseethattheperformanceofCodeT5-Updatedrasticallydrops
with the increase of number of sub-tokens in the code to be edited
(/u1D440/u1D447;/u1D45C/u1D459/u1D451),butEditsTranslation’sperformanceisratherstable.This
illustrates another bene/f_it of Codeditor in accurately handling
longerinput,becauseoffocusingontransformingtheeditsinstead
ofgeneratingthe entire newcode like CodeT5-Update.
Meanwhile, most of the existing transformer-based models have
alengthlimitfortheinputsequencebecausethenaiveself-attention
has quadratic complexity with regard to the input length. In Fig-
ure4,wepresentthedistributionofthenumberofsub-tokensin
themodels’targetoutputsfor Codeditor (EditsTranslation)and
CodeT5-UpdateonthetestdataofJ2CS.Weonlyshowthedistri-
bution of target outputs with fewer than 500 sub-tokens for the
same reason described in the previous paragraph. Most of Codedi-
tor’stargetoutputs(thesequenceofeditoperations)areshorter
than CodeT5-Update’s output (new code in target programming
language). This might explain why Codeditor achieves better
performance than generation-based models on longer code as gen-
eratinglongersequencearegenerallymorechallengingtomachine
learningmodels.Recentstudies[ 5,7,14]havefocusedonexploring
approachestoaddressthelimitationofthemodel’sinputcontext
703ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiyangZhang,PengyuNie,JunyiJessy Li, andMilos Gligoric
1public static Document parseBodyFragment(String bodyHtml, String baseUri) {
2...
3List <Node> nodeList = parseFragment(bodyHtml, body, baseUri);
4Node[] nodes = nodeList.toArray( newNode[0]);
5-for(inti = nodes.length - 1; i > nodeList.size() ; i--) {
6+for(inti = nodes.length - 1; i > 0; i--) {
7 nodes[i].remove();
8}
9...
10}JavaChange
1public static Document ParseBodyFragment(String bodyHtml, String baseUri) {
2...
3IList <iText.StyledXmlParser.Jsoup.Nodes.Node> nodeList = ParseFragment(bodyHtml, body, baseUri);
4iText.StyledXmlParser.Jsoup.Nodes.Node[] nodes = nodeList.ToArray( newiText.StyledXmlParser.Jsoup.Nodes.Node[nodeList.Count]);
5for(inti = nodes.Length - 1; i > nodeList.Count; i--) {
6 nodes[i].Remove();
7}
8...
9}
1...
2-for(inti = nodes.Length - 1; i > nodeList.Count ; i--) {
3+for(inti = nodes.Length - 1; i > 0; i--) {
4...
1...
2- iText.StyledXmlParser.Jsoup.Nodes.Node[] nodes = nodeList.ToArray( newiText.StyledXmlParser.Jsoup.Nodes.Node[ nodeList.Count ]);
3+ iText.StyledXmlParser.Jsoup.Nodes.Node[] nodes = nodeList.ToArray( newiText.StyledXmlParser.Jsoup.Nodes.Node[ 0]);
4-for(inti = nodes.Length - 1; i > nodeList.Count ; i--) {
5+for(inti = nodes.Length - 1; i > 0; i--) {
6...
1...
2-iText.StyledXmlParser.Jsoup.Nodes. Node[] nodes = nodeList.ToArray( newiText.StyledXmlParser.Jsoup.Nodes. Node[nodeList.Count ]);
3+ Node[] nodes = nodeList.ToArray( newNode[0]);
4-for(inti = nodes.Length - 1; i > nodeList.Count ; i--) {
5+for(inti = nodes.Length - 1; i > 0; i--) {
6...C#OldMethod
Codeditor (EditsTranslation) Prediction
CodeT5-UpdatePrediction
CodeT5-TranslationPrediction
Figure 5:Qualitative analysis ofallthemodels on oneexample inthetestdata ofJ2CS dataset.
window size. Future research should examine the performance dif-
ference between translating edit sequences and generating entirely
newcode using models capableofhandling longer context.
RQ3:Combininggeneration-basedmodelwith Codeditor .
To exploit the superiority of generation-based model on short code
snippets, we combine our strongest generation model—CodeT5-
Update—with the strongest Codeditor mode—EditsTranslation—
based onthesizeof thecodesnippet. Speci/f_ically, we useCodeT5-
Updateifthecodetobeupdatedhasfewersub-tokensthanathresh-
oldanduse Codeditor (EditsTranslation)otherwise.Topickthe
thresholdforcombiningtwomodels,weperformedagrid-search
onthevalidationsetandselectedtheonethatgivesoptimalxMatch
score. We refer to the combined model as the Hybridmodel and
provideitsresultsonthebottomrowofTable 4toTable7.Bycom-
bininggeneration-basedmodelwith Codeditor ,wecanachieve
improved performance on most of the reported automatic metrics.
6.2 QualitativeAnalysis
Figure5shows an example in J2CS dataset and the models’ predic-
tions. We show the code changes from Java project itext/itext7
inthemethod( parseBodyFragment ).Thenewlyaddedcodeishigh-
lightedingreenandremovedcodeishighlightedinred.Wealso
presenttheoldversionofthecorrespondingC#method( ParseBody -
Fragment ) fromitext/itext7-dotnet, and the predicted codechangesfromthreemodels: Codeditor (EditsTranslation),CodeT5-
Update, CodeT5-Translation. Note that CodeT5-Translation only
has access to the newversionof Java method.
AlthoughCodeT5-Translationisabletocorrectlytranslatethe
code change in Java, it fails to infer the full name of the type Node
and makesan irrelevantedit, because it does not have the context
of the old version of C# code. CodeT5-Update correctly captures
the Java change while making an extra irrelevant edit on the C#
code. Our proposed model, Codeditor (EditsTranslation) accu-
ratelyidenti/f_iesthepositionintheC#methodtomakeeditsand
correctlyadjusts the Java edits.
7 LIMITATIONS
Studiedprogramminglanguages ..Westudythetranslationof
code changesbetween two programming languages. In this paper,
we focus on open-source Java and C# projects due to the ease
oflocating correspondingchangesusing heuristics.Nevertheless,
it is important to note that our approach can be applied to other
programminglanguagepairsaswell,andweleavetheinvestigation
ofsuch pairsfor future research.
Correspondencebetweenprogramminglanguages .Ourmodel,
Codeditor , is intended for developers to migrate code changes
fromaprojectwritteninasourceprogramminglanguagetoprojects
704Multilingual Code Co-evolution usingLarge LanguageModels ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
writtenintargetprogramminglanguages,leveragingknowncor-
respondences(e.g.,methodswithsimilarfunctionalities)between
the source and target programming languages. In this work, we
adoptasimilarstrategyusedin[ 33]tomatchJavaandC#methods.
In practice, a code retrieval system can be used as a /f_irst step to
identifythelocationswherethecodechangesshouldbepropagated.
We leave the combination of code retrieval tool and Codeditor as
future work.
Empiricalevaluation .Thispaperpresentstheempiricalstudyre-
sultsforinternalmetricsthatareofinteresttoresearchers.However,
theexternalmeasurementsoftheimpactonsoftware engineering
eﬀort are not included in this study. These measurements could be
addressedbyconductinguserstudies.
8 RELATED WORK
In this section, we describe related work on the rule-based code
translation tools, existing machine learning models designed for
codetranslation,andthemachinelearningmodelsthatareproposed
for accelerating software evolution.
Rule-based code translation . Researchers and practitioners have
designed rule-basedtools fortranslatingthesourcecodebetween
programminglanguages.Suchtools,usuallycalledtranspilers,were
built for pairs like Java and C# [ 3], C and Rust [ 18], C and Go [ 16].
Nguyen et al .[36]proposed PBSMT, a phrase-based statistical ma-
chine translation models for source code translation. Gyori et al .
[22]proposed LambdaFicator totranslateimperativeJavacodeto
using the functional Stream APIs. Radoi et al . [44]presented the
rule-basedmodeltotranslatesequentialJavacodetoMapReduce
framework. Prior work [ 34] has shown that existing rule-based
code refactoring tools can only deal with stylized code snippets
over common code patterns.
Learning-based code translation . Researchers have proposed
variousmachinelearningmodelsforthecodetranslationtask.Chen
et al.[13]proposed a tree-to-tree neural network with a tree-RNN
encoder and a tree-RNN decoder. Motivated by the success of large
pretrained LLMs for many Natural Language Processing tasks,
domain-speci/f_ic models that are pretrained on source code and
technical text have emerged. Researchers have applied them to the
codetranslationtask.Luetal .[33]proposedCodeXGLUE,abench-
mark including the code translation dataset consisting of Java and
C#methodswithequivalentfunctionality.They/f_ine-tunedandeval-
uated CodeBERT on the translation dataset. Results showed that it
produced the best results among all the existing baselines. LLMs
thatarebuiltontheencoder-decoderparadigmandpretrainedwith
generalunsuperviseddenoisingauto-encodingobjectivesshowed
promisingresultsonwiderangeofcodegenerationtasksincluding
code translation. Such models include CodeT5 [ 55], PLBART [ 1],
and UniXcoder [ 21]. For the comparison of Codeditor with state-
of-the-art code translation models, we include two variants ofthe
CodeT5-basedtranslationmodels(withhistorycontextandwith-
out) inour evaluation.
ResearchersdesignedLLMswhicharepretrainedwiththeobjec-
tive tailored for code translation. Tipirneni et al . [49]introduced
tasksonpredictingASTpathsanddata/f_lowsduringpretraining.
Lachaux et al .[27]proposed TransCoder which is pretrained to do
code translation with back-translation objective. To improve thequality of pretraining data, Roziere et al . [46]leveraged an auto-
mated unit-testing system to /f_ilter out invalid generated programs
during back-translation. Zhu et al .[60]proposed MuST, which is a
multilingualcodesnippettranslationpretrainingobjective.None
of the above work leverages the code change history, which is the
main contribution of our paper. We leave improving Codeditor
withpretrainingobjectivestailoredforcodetranslationasfuture
work.
Software evolution and machine learning . New research initia-
tiveshaveemergedaroundbuildingandevaluatingmodelsthataid
theprocessofsoftwareevolution.Priorwork[ 19,29,30,32,41]pro-
posedto updatethecomment giventhe changesinthe associated
method,e.g.,Panthaplackeletal .[41]builtamodelthattakesthe
codechangeascontexttomakeeditsontheoutdatedcomment.Nie
etal.[38]presentdiﬀerentapproachestosplitdatasetintotraining,
validationandtestsetsandstudiedhowdiﬀerentapproachesaﬀect
theevaluationofmachinelearningmodels.Kamezawaetal . [25]
presented a dataset, RNSum, which consists of release notes and
the associated commit messages collected from GitHub reposito-
ries and designed models to generate release notes based on the
commitmessages.Zhangetal . [59]proposedanovelpretraining
objective designed for software editing tasks and built CoditT5.
CoditT5was/f_ine-tunedonthreedownstreamtasksrelatedtothe
software evolution. Li et al . [28], Tufano et al . [52], Zhang et al .
[58]proposed models that targeted various tasks through the code
reviewprocess.Themodelsaretrainedonthehistoricaldataand
evaluated on the new pull requests submitted for code review. Our
Codeditor model incorporates the context from the code changes
in source programming language and the old version of method in
target programming languages toimprove itsperformance on the
multilingual co-editing task, which helps developers co-evolve the
projectsimplementedindiﬀerentprogramming languages.
9 CONCLUSION
In thispaper, we formulateda new task:translatingcode changes
acrossprogramminglanguageswiththegoaltosynchronizeprojects
that provide the same APIs or implementations in multiple pro-
gramminglanguages.Weproposed Codeditor ,amodelwhichuses
code change history as contextual information and learns to make
edits on the existing version of code written in the target program-
minglanguage.Weshowedthatourmodeloutperformsexisting
code translation models and is better than the generation-based
modelseveniftheyusehistoricalcontext. Codeditor isasigni/f_i-
cant advancement in supporting developers with the maintenance
oftheirprojectsthatincrementallyprovideidenticalfunctionalities
inmultiple programming languages.
ACKNOWLEDGMENTS
We thank Nader Al Awar, Yu Liu, Sheena Panthaplackel, Aditya
Thimmaiah,ZhiqiangZang,andtheanonymousreviewersfortheir
comments and feedback. We acknowledge the Texas Advanced
Computing Center (TACC) at The University of Texas at Austin
for providing HPC resources that have contributed to the research
results reported within this paper. This work is partially supported
by the US National Science Foundation under Grant Nos. CCF-
2107291, IIS-2145479,CCF-2217696andCCF-2313027.
705ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA JiyangZhang,PengyuNie,JunyiJessy Li, andMilos Gligoric
REFERENCES
[1]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021.
Uni/f_ied Pre-training for Program Understanding and Generation. In Conference
oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
Human Language Technologies . 2655–2668.
[2]Tou/f_iqueAhmedandPremkumarDevanbu.2022. Few-ShotTrainingLLMsfor
Project-Speci/f_icCode-Summarization. In Automated SoftwareEngineering . 1–5.
[3]ChristianMauceriAlexandreFAU.2013. Java2csharp. http://sourceforge.net/
projects/j2cstranslator/
[4]Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,
et al.2021. Program synthesis with large language models. arXiv preprint
arXiv:2108.07732 (2021).
[5]IzBeltagy,MatthewEPeters,andArmanCohan.2020. Longformer:Thelong-
documenttransformer. arXiv preprint arXiv:2004.05150 (2020).
[6]Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An Empirical
Investigation of Statistical Signi/f_icance in NLP. In Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language
Learning. 995–1005.
[7]Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. 2023.
Unlimiformer: Long-range transformers with unlimited length input. arXiv
preprint arXiv:2305.01625 (2023).
[8]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
PrafullaDhariwal,ArvindNeelakantan, Pranav Shyam, GirishSastry,Amanda
Askell,etal .2020. Languagemodelsarefew-shotlearners. Advancesinneural
information processingsystems 33(2020), 1877–1901.
[9]NghiDQBuiandLingxiaoJiang.2018. HierarchicalLearningofCross-Language
MappingsThroughDistributedVectorRepresentationsforCode.In International
Conference onSoftwareEngineering, NIER . 33–36.
[10]Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray.
2020. Codit: Code Editing with Tree-based Neural Models. Transactions on
SoftwareEngineering 4 (2020), 1385–1399.
[11]SaikatChakrabortyandBaishakhiRay.2021. OnMulti-ModalLearningofEditing
Source Code. In AutomatedSoftwareEngineering . 443–455.
[12]MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveira
Pinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,
et al.2021. Evaluating Large Language Models Trained on Code. arXiv preprint
arXiv:2107.03374 (2021).
[13]Xinyun Chen, ChangLiu, and Dawn Song. 2018. Tree-to-TreeNeural Networks
forProgramTranslation.In AdvancesinNeuralInformationProcessingSystems ,
Vol. 31.
[14]Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashat-
tention: Fast and memory-eﬃcient exact attention with io-awareness. Advances
inNeural InformationProcessingSystems 35(2022), 16344–16359.
[15]YangruiboDing,BaishakhiRay,PremkumarDevanbu,andVincentJHellendoorn.
2020. PatchingasTranslation:theDataandtheMetaphor.In AutomatedSoftware
Engineering . 275–286.
[16]Elliot Chance et al. 2021. A tool for transpiling C to Go. https://github.com/
elliotchance/c2go
[17]Python Software Foundation. 2023. diﬄib — Helpersfor computing deltas. Re-
trieved February2,2023 from https://docs.python.org/3/library/diﬄib.html
[18] Galoisand Immunant. 2023. C2Rust. https://github.com/immunant/c2rust
[19]Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann. 2021.
Automating the removal of obsolete TODO comments. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe FoundationsofSoftwareEngineering . 218–229.
[20] Google. 2023. Google Cloud. https://cloud.google.com/
[21]Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022.
UniXcoder:Uni/f_iedCross-ModalPre-trainingforCodeRepresentation.In Annual
Meetingofthe Associationfor ComputationalLinguistics . 7212–7225.
[22]AlexGyori,LyleFranklin,DannyDig,andJanLahoda. 2013. CrossingtheGap
fromImperativetoFunctionalProgrammingThroughRefactoring.In Interna-
tional Symposiumonthe FoundationsofSoftwareEngineering . 543–553.
[23] MongoDB Inc. 2023. MongoDB. https://www.mongodb.com/
[24]Paul Jaccard. 1912. The Distribution of the Flora in the Alpine Zone. New
phytologist (1912), 37–50.
[25]HisashiKamezawa,NorikiNishida,NobuyukiShimizu,TakashiMiyazaki,and
HidekiNakayama.2022. RNSum:ALarge-ScaleDatasetforAutomaticRelease
Note Generation via Commit Logs Summarization. In Annual Meeting of the
Associationfor ComputationalLinguistics . 8718–8735.
[26]Junaed Younus Khan and Gias Uddin. 2022. Automatic Code Documentation
GenerationUsing GPT-3. In AutomatedSoftwareEngineering . 1–6.
[27]Marie-AnneLachaux,BaptisteRoziere,LowikChanussot,andGuillaumeLample.
2020. Unsupervised Translation of Programming Languages. In Advances in
Neural InformationProcessingSystems . 20601–20611.
[28]Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep
Majumder,JaredGreen,AlexeySvyatkovskiy,ShengyuFu,etal .2022.AutomatingCode Review Activities by Large-Scale Pre-Training. In Joint European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
1035–1047.
[29]Bo Lin, Shangwen Wang, Kui Liu, Xiaoguang Mao, and Tegawendé F Bissyandé.
2021. AutomatedCommentUpdate:HowFarareWe?.In InternationalConference
onProgramComprehension . 36–46.
[30]Bo Lin, Shangwen Wang, Zhongxin Liu, Xin Xia, and Xiaoguang Mao. 2022.
Predictivecommentupdatingwithheuristicsandast-path-basedneurallearning:
Atwo-phaseapproach. IEEETransactionsonSoftwareEngineering 49,4(2022),
1640–1660.
[31]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
Robustly Optimized Bert Pretraining Approach. arXiv preprintarXiv:1907.11692
(2019).
[32]Zhongxin Liu, Xin Xia, David Lo, Meng Yan, and Shanping Li. 2021. Just-in-
time obsolete comment detection and update. IEEE Transactions on Software
Engineering (2021).
[33]Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .2021.
CodeXGLUE: A Machine Learning BenchmarkDataset for Code Understanding
and Generation. arXiv preprint arXiv:2102.04664 (2021).
[34]Benjamin Mariano, Yanju Chen, Yu Feng, Greg Durrett, and Isil Dillig. 2022.
Automated Transpilation of Imperative to Functional Code using Neural-Guided
Program Synthesis. In International Conference on Object-Oriented Programming,
Systems,Languages, and Applications . 1–27.
[35]CourtneyNapoles,KeisukeSakaguchi,MattPost,andJoelTetreault.2015.Ground
TruthforGrammaticalErrorCorrectionMetrics.In AnnualMeetingoftheAssoci-
ation for Computational Linguistics and International Joint Conference on Natural
Language Processing . 588–593.
[36]Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. 2015. Divide-
and-ConquerApproachforMulti-PhaseStatisticalMigrationforSourceCode.In
AutomatedSoftwareEngineering . 585–596.
[37]Pengyu Nie. 2023. Machine Learning for Executable Code in Software Testing and
Veri/f_ication . Ph.D. Dissertation. The Universityof Texas at Austin.
[38]PengyuNie,JiyangZhang,JunyiJessyLi,RaymondJ.Mooney,andMilosGligoric.
2022. ImpactofEvaluationMethodologiesonCodeSummarization.In Annual
Meetingofthe Associationfor ComputationalLinguistics . 4936–4960.
[39] OpenAI. 2023. GPT-4Technical Report. arXiv: arXiv:2303.08774
[40] OpenAI. 2023. Introducing ChatGPT. https://openai.com/blog/chatgpt
[41]Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, and Raymond
Mooney. 2020. Learning to Update Natural Language Comments Based on Code
Changes. In Annual Meeting of the Association for Computational Linguistics .
1853–1868.
[42]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:a
Method for Automatic Evaluation of Machine Translation. In Annual Meeting of
the Associationfor ComputationalLinguistics . 311–318.
[43]Terence J. Parr and Russell W. Quong. 1995. ANTLR: A Predicated-LL (k) Parser
Generator. Software:Practice and Experience 25,7 (1995), 789–810.
[44]CosminRadoi,StephenJFink,RodricRabbah,andManuSridharan.2014.Translat-
ingImperativeCodetoMapReduce.In InternationalConferenceonObject-Oriented
Programming, Systems,Languages, and Applications . 909–927.
[45]ShuoRen,DayaGuo,ShuaiLu,LongZhou,ShujieLiu,DuyuTang,NeelSundare-
san,MingZhou,AmbrosioBlanco,andShuaiMa.2020. CodeBLEU:aMethodfor
Automatic Evaluation of Code Synthesis. arXiv preprint arXiv:2009.10297 (2020).
[46]BaptisteRoziere,JieMZhang,FrancoisCharton,MarkHarman,GabrielSynnaeve,
andGuillaumeLample.2021. LeveragingAutomatedUnitTestsforUnsupervised
CodeTranslation. arXiv preprint arXiv:2110.06773 (2021).
[47]Apache Software. 2022. Apache Lucene. Retrieved March 2, 2022 from https:
//lucene.apache.org/
[48]Felix Stahlberg and Shankar Kumar. 2020. Seq2Edits: Sequence Transduction
Using Span-level Edit Operations. In Empirical Methods in Natural Language
Processing . 5147–5159.
[49]SindhuTipirneni,MingZhu,andChandanKReddy.2022. StructCoder:Structure-
AwareTransformerforCodeGeneration. arXivpreprintarXiv:2206.05239 (2022).
[50]MarcoTrudel,ManuelOriol,CarloAFuria,andMartinNordio.2011. Automated
Translation of Java Source Code to Eiﬀel. In International Conference on Objects,
Models,Components, Patterns . 20–35.
[51]MicheleTufano,JevgenijaPantiuchina,CodyWatson,GabrieleBavota,andDenys
Poshyvanyk. 2019. On Learning Meaningful Code Changes via Neural Machine
Translation. In InternationalConference onSoftwareEngineering . 25–36.
[52]RosaliaTufano,SimoneMasiero,AntonioMastropaolo,LucaPascarella,Denys
Poshyvanyk, and Gabriele Bavota. 2022. Using Pre-Trained Models to Boost
CodeReviewAutomation.In InternationalConferenceonSoftwareEngineering .
2291–2302.
[53]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
You Need.In AdvancesinNeural InformationProcessingSystems . 5998–6008.
706Multilingual Code Co-evolution usingLarge LanguageModels ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[54]YueWang,HungLe,AkhileshDeepakGotmare,NghiD.Q.Bui,JunnanLi,and
Steven C. H. Hoi. 2023. CodeT5+: Open Code Large LanguageModelsforCode
Understandingand Generation. arXiv preprint (2023).
[55]YueWang,WeishiWang,Sha/f_iqJoty,andStevenCHHoi.2021.CodeT5:Identi/f_ier-
awareUni/f_iedPre-trainedEncoder-DecoderModelsforCodeUnderstandingand
Generation. In EmpiricalMethodsinNatural Language Processing . 8696–8708.
[56]Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-
Burch. 2016. Optimizing Statistical Machine Translation for Text Simpli/f_ication.
Transactions ofthe Associationfor ComputationalLinguistics 4 (2016), 401–415.
[57]Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, and Graham Neubig. 2021.
Learning Structural Edits via Incremental Tree Transformations. In International
Conference onLearning Representations .[58]Jiyang Zhang, Chandra Maddila, Ram Bairi, Christian Bird, Ujjwal Raizada,
Apoorva Agrawal, Yamini Jhawar, Kim Herzig, and Arie van Deursen. 2023. Us-
ing Large-scale Heterogeneous Graph Representation Learning for Code Review
RecommendationsatMicrosoft. InternationalConferenceonSoftwareEngineering,
SEIP(2023).
[59]Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos
Gligoric. 2022. CoditT5: Pretraining for Source Code and Natural Language
Editing.In AutomatedSoftwareEngineering . 1–12.
[60]Ming Zhu, Karthik Suresh, and Chandan K Reddy. 2022. Multilingual Code
Snippets Training for Program Translation. In AAAI Conference on Arti/f_icial
Intelligence . 11783–11790.
Received 2023-02-02; accepted 2023-07-27
707