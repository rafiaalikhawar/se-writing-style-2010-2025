Bridging Pre-trained Models and Downstream Tasks for Source
Code Understanding
Deze Wang
National University of Defense
Technology, China
wangdeze14@nudt.edu.cnZhouyang Jiaâˆ—
National University of Defense
Technology, China
jiazhouyang@nudt.edu.cnShanshan Liâˆ—
National University of Defense
Technology, China
shanshanli@nudt.edu.cn
Yue Yu
National University of Defense
Technology, China
yuyue@nudt.edu.cnYun Xiong
Fudan University
Shanghai, China
yunx@fudan.edu.cnWei Dong
National University of Defense
Technology, China
wdong@nudt.edu.cn
Xiangke Liao
National University of Defense
Technology, China
xkliao@nudt.edu.cn
ABSTRACT
With the great success of pre-trained models, the pretrain-then-
finetune paradigm has been widely adopted on downstream tasks
for source code understanding. However, compared to costly train-
ing a large-scale model from scratch, how to effectively adapt pre-
trained models to a new task has not been fully explored. In this
paper,weproposeanapproachtobridgepre-trainedmodelsand
code-related tasks. We exploit semantic-preserving transformation
to enrich downstream datadiversity, and help pre-trained models
learn semantic features invariant to these semantically equivalent
transformations.Further,weintroducecurriculumlearningtoor-
ganizethetransformeddatainaneasy-to-hardmannertofine-tune
existing pre-trained models.
We apply our approach to a range of pre-trained models, and
they significantly outperform the state-of-the-art models on tasks
for source code understanding, such as algorithm classification,
code clone detection, and code search. Our experiments even show
thatwithoutheavypre-trainingoncodedata,naturallanguagepre-
trained model RoBERTa fine-tuned with our lightweight approach
could outperform or rival existing code pre-trained models fine-
tuned on the above tasks, such as CodeBERT and GraphCodeBERT.
Thisfindingsuggeststhatthereisstillmuchroomforimprovement
in code pre-trained models.
âˆ—Zhouyang Jia and Shanshan Li are the corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510062CCS CONCEPTS
â€¢Computing methodologies â†’Supervised learning ;Artificial
intelligence .
KEYWORDS
fine-tuning,dataaugmentation,curriculumlearning,test-timeaug-
mentation
ACM Reference Format:
Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong,
and Xiangke Liao. 2022. Bridging Pre-trained Models and Downstream
TasksforSourceCodeUnderstanding.In 44thInternationalConferenceon
Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510062
1 INTRODUCTION
Large-scale models, such as BERT [ 9], RoBERTa [ 26], GPT-3 [ 5],
T5 [36], and BART [ 23], have greatly contributed to the develop-
ment of the field of natural language processing (NLP), and gradu-
ally form the pretrain-then-finetune paradigm. The basic idea of
thisparadigmis tofirstpre-trainamodelon largegeneral-purpose
datasetsbyself-supervisedtasks, e.g.,maskingtokensintraining
dataandaskingthemodeltoguessthemaskedtokens.Thetrained
model is then fine-tuned on smaller and more specialized datasets,
eachdesignedtosupportaspecifictask.Thesuccessofpre-trained
modelsinthenaturallanguagedomainhasalsospawnedaseries
of pre-trained models for programming language understanding
andgeneration,includingCodeBERT[ 11],GraphCodeBERT[ 13],
PLBART[ 2],andtheusageofT5tosupportcode-relatedtasks[ 28],
improving the performance of a variety of source code understand-
ing and generation tasks.
However, pre-training a large-scale model from scratch is costly.
Additionally, along with an increasing number of pre-trained mod-
els,howtoeffectivelyadaptthesemodelsforanewtaskisnotfully
exploited. In this paper, we try to take the first step to bridge large
pre-trained models and code-related downstream tasks. Moreover,
despite the success of existing pre-trained models for code-related
2872022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao
tasks,thesemodelshavetwopotentialissues.First,thesemodels
graftNLPpre-trainingtechniquestounderstandthesemanticsof
sourcecode,h owever,the semanticsofprogramminglanguageand
naturallanguageareessentiallydifferent,andsemanticallyequiv-
alentsourcecodemaybeinvarioussyntacticforms.Thesecond
issueisthatpre-trainedmodelstypicallyhaveatleastmillionsof
parameters, so when a pre-trained model is applied to downstream
taskswithspecializeddatasets,thereisariskofoverfittingbecause
the model is over-parameterized for the target dataset. Many stud-
ieshavealsofoundthatwhenthetestsetisdifferentfromtheactualsceneorthetestsetisslightlyperturbed,variousmodelsforsource
code would make mistakes [34, 37, 53].
Toaddresstheaboveissues,wedesignalightweightapproach
on top of the existing pre-trained language model fine-tuning para-
digm, that satisfies (1) extracting code semantic knowledge embed-
ded in diverse syntactic forms and complementing it to pre-trainedmodels,(2)reducingoverfittingtothetargetdatasetandbeingmorerobustintesting.Inordertoincorporatesemanticknowledgeofthe
programminglanguagesintomodels,weemploydataaugmenta-
tion, which is mainly used to enrich the training dataset and make
it as diverse as possible. There are many successful applications of
dataaugmentationinthefieldofimageprocessing,suchasrandom
cropping [ 21], flipping [ 42] and dropout [ 44]. For code data, this
paperconsiders semantic-preservingtransformation.Anexample
ofcodetransformationisshowninFig.1,wherethesameprogram
is transformed three times successively, keeping the semantics un-
changed.Sincethesemanticsoftheoriginalprogramarepreserved,
it is logical that the model should have the same behavior as the
original program for the program generated by the transformation
techniques. Moreover, it is cheap to leverage a source-to-source
compiler[ 3]toperformsemantic-preservingtransformationson
source code.
In this paper, we build our approach on a series of large-scale
pre-trained models, including natural language pre-trained model
RoBERTa and code pre-trained models CodeBERT and GraphCode-
BERT, to bridge pre-trained models with downstream tasks forsource code. We first construct semantic-preserving transforma-
tionsequencesandapplythemtooriginaltrainingsamples,asin
Fig.1,togeneratenewtrainingdataandintroducecodesemantic
knowledge into models. The transformation sequences make code
transformationsmorecomplicatedandcouldguidemodelstobetterlearn the underlying semantics of the code. These training data are
then fed to pre-trained models to fine-tune the models. Finally, we
augmentthetestsetswiththesameaugmentationtechniquesas
thetrainingsetstoobtainmultipletransformedtestsets.Tofurther
reduce overfitting from the training process, we average the model
performance on these test sets. Since our method averages the pre-
dictionsfromvarioustransformationversionsforanycodesnippet
intestsets,thefinalpredictionsarerobusttoanytransformation
copy.
The transformed data significantly increase the data diversity,
however, they can also be considered as adversarial examples com-
paredtotheoriginaldata[ 35,37].Fig.1showstheoriginalprogram
and programs after multiple code transformations. As the number
oftransformationsincreases,newtokensandsyntacticformsare
introduced,andthedistributionoftransformeddatabecomesmore
distinct from that of original data, making it more difficult to learn.To solve this issue, we introduce Curriculum Learning (CL) [ 29]
and present training examples in an easy-to-hard manner, instead
ofacompletelyrandomorderduringtraining.Manystudieshave
shown that it benefits the learning process not only for humans
butalsoformachines[ 10,22].ThekeychallengeofCLishowto
defineeasyandhardsamples,andinthispaperweproposetwohy-pothesesandexperimentallyverifythemtodeterminethelearning
order.
Inourexperiments,basedonpre-trainedmodelsCodeBERTand
GraphCodeBERT,ourmethodsignificantlysurpassesthestate-of-
the-art performance on algorithm classification, code clone detec-
tion and code search tasks. In the algorithm classification task, our
approach improves 10.24% Mean Average Percision (MAP) com-
pared to the state-of-the-art performance, and in the code clone de-tectiontask,usingonly10%oftherandomlysampledtrainingdata,
codepre-trainedmodelCodeBERTfine-tunedwithourapproach
outperforms the state-of-the-art model GraphCodeBERT normally
fine-tunedwithalltrainingdata.Inthecodesearchtask,ourmethodimprovesthestate-of-the-artperformanceto0.720MeanReciprocalRank(MRR).Moreimpressively,totestwhetherourapproachintro-
duces additional semantic knowledge of source code for the model,
we apply our approach to natural language pre-trained model
RoBERTa and find that it even outperforms CodeBERT with 3.88%
MAP on algorithm classification task and RoBERTa pre-trained
with code on code search task, and has the same performance as
CodeBERToncodeclonedetectiontask.Thedata,pre-trainedmod-
els andimplementationofour approach are publiclyavailable at
the link: https://github.com/wangdeze18/DACL.
The main contributions of our paper are as follows:
â€¢We design a lightweight approach on top of the existing
pre-trained language model fine-tuning paradigm, to bridge
pre-trainedmodelsanddownstreamtasksforsourcecode.
To the best of our knowledge, it is the first work in this
direction.
â€¢We apply our method to pre-trained models CodeBERT and
GraphCodeBERT,andtheaugmentedmodelsdramatically
outperformthestate-of-the-artperformanceonalgorithm
classification,code clone detection and code search tasks.
â€¢Our study reveals that for code-related tasks, without the
needforheavypre-trainingoncodedata,naturallanguage
models(e.g. RoBERTa)easilyoutperformthesamemodels
pre-trained with code, as well as the state-of-the-art codepre-trained models (e.g. CodeBERT) with the help of our
approach.
2 PRELIMINARIES AND HYPOTHESES
2.1 Data Augmentation
DataAugmentation(DA)isatechniquetocreatenewtrainingdata
fromexistingtraining dataartificially.Transformationswereran-
domly applied to increase the diversity of the training set. Data
augmentation is often performed with image data, where copiesof images in the training set are created with some image trans-
formationtechniquesperformed,suchaszooms,flips,shifts,and
more. In fact, data augmentation can also be applied to natural lan-
guageandcodedata.Inthispaper,ourpurposeofintroducingdata
augmentation is to learn code semantics from semantic-preserving
288
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
   int maxn = 0, secMaxn = 0; 
   for (int count = 1; count < SIZE;  count++){
       if (numbers [count ] > maxn ){ 
           secMaxn = maxn ; 
           maxn = numbers [count ]; 
       } 
       else  if (numbers [count ] > secMaxn ){ 
           if (numbers [count ] != maxn )
               secMaxn = numbers [count ]; 
       }
   } 
   cout <<maxn <<" "<<secMaxn <<endl;-  int maxn = 0, secMaxn = 0 ; 
+ int count = 1, maxn = 0, secMaxn = 0;
-  for  (int count = 1; count < SIZE ; count ++){ 
+ while  (count < SIZE ) { 
       if (numbers [count ] > maxn ){ 
           secMaxn = maxn ; 
           maxn = numbers [count ]; 
       } -    else  if (numbers [count ] > secMaxn ){ 
+    else  if (numbers [count ] > secMaxn
-         if (numbers [count ] != maxn ) 
+    && numbers [count ] != maxn )
          secMaxn = numbers [count ]; 
+     count ++; 
   } 
   cout <<maxn <<" "<<secMaxn <<endl;   int count = 1, maxn = 0, secMaxn = 0; 
   int nonSense , temp ; 
   while  (count < SIZE ) { 
        if (numbers [count ] > maxn ){ 
            secMaxn =  maxn ; 
            maxn = numbers [count ]; 
       }  
       else if (numbers [count ] > secMaxn  
       &&  numbers [count ] != maxn ){
           secMaxn = numbers [count ]; 
       }
       count++;
   } 
-  cout <<maxn <<" "<<secMaxn <<endl;
+ printf ("%d %d\n" , maxn , secMaxn );    int count = 1, maxn =  0, secMaxn = 0; 
+ int nonSense , temp ; 
   while (count < SIZE ) { 
       if (numbers [count ] > maxn ){ 
           secMaxn = maxn ; 
           maxn = numbers [count ]; 
       }        else if (numbers [count ] > secMaxn 
-      && numbers [count ] != maxn ) 
+     &&  numbers [count ] != maxn ){
           secMaxn =  numbers [count ];
+     } 
       count ++; 
   } 
   cout <<maxn <<" "<<secMaxn <<endl;Original program 1-Transformation program 2-Transformation program 3-Transformation program
Figure 1: An example of code transformation. ð‘˜âˆ’ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› program represents the result of the original program after ð‘˜
transformations.All four programs implement the function to find the maximum and second largest values in the array.
transformation, more specifically, to assist models in extracting
and learning features in a way that are invariant to semantically
equivalent declarations, APIs, control structures and so on.
Inthispaper,weexploitdataaugmentationnotonlyforthetrain-
ingsetbutalsoforthetestset.Theapplicationofdataaugmenta-
tiontothetestsetiscalledTest-TimeAugmentation(TTA)[ 30,42].
Specifically,itcreatesmultipleaugmentedcopiesofeachsample
in the test set, has the model make a prediction for each, and then
returns an ensemble of those predictions. The number of copies of
thegivendataforwhichamodelmustmakeapredictionisoften
small.Inourexperiment,werandomlysamplethreesamplesfor
eachpieceofdatafromtheiraugmentedcopies,taketheaveragere-sultsastheresultoftheaugmentedperspectiveandaddtheresults
on the original dataset as the final results.
2.2 Curriculum Learning
The learning process of humans and animals generally follows the
order of easy to difficult, and CL draws on this idea. Bengio et
al.[4] propose CL for the first time imitating the process of human
learning,andadvocatethatthemodelshouldstartlearningfrom
easy samples and gradually expand to complex samples. In recent
years, CL strategies have been widely used in various scenariossuch as computer vision and natural language processing. It has
shownpowerfulbenefitsinimprovingthegeneralizationabilityand
accelerating convergence of various models [ 14,18,33,46]. At the
same time, it is also easy-to-use, since it is a flexible plug-and-play
submodule independent of original training algorithms.
There are two key points of CL, one is the scoring function and
the other is the pacing function. The scoring function makes it
possible to sort the training examples by difficulty, and present to
the network the easier samples first. The pacing function deter-
mines the pace by which data is presented to the model. The main
challenge is how to obtain an effective scoring function without
additional labelling of the data.
2.3 Hypotheses
We formulate two hypotheses about the scoring functions to de-
termine the order of learning and conduct experiments to verify
them.Many studies have shown that deep models for source code are
vulnerabletoadversarialexamples[ 34,37,53].Slightperturbations
to the input programs could cause the model to make false predic-
tions.Therefore,itisnaturalforustoformulatethefirsthypothesis
thatthe augmented data are more challenging to learn than
the original data for general models. We design an experiment
toverifythishypothesisdirectly,asshowninAlgorithm1.Itshows
the pseudocode to verify the impact of code transformation bycomparing the performance of the model on a range of trainingset variants. The training set variants are generated by iterating
the transformation functions on the original training set. (line 4-7)
Afterthemodelistrainedonthewholetrainingsetincludingall
trainingsetvariants,weevaluatethemodelondifferenttraining
set variants.
We apply Algorithm 1 to the state-of-the-art model CodeBERT
withbenchmarkdatasetPOJ104[ 31](willbeexplainedin4.1).Fig.2
shows the performance of CodeBERT for these training set vari-ants. The model performs best on the original training set. The
performancegetsprogressivelyworseasthenumberoftransfor-
mationsontheoriginaldatasetincreases,whichverifiesthatdata
augmentation would increase the difficulty of the training set and
experimentally supports our hypothesis.
92.95
91.22
89.12 
87.63 
85.99
8587899193
Original 1-Trans 2-Trans 3-Trans 4-TransPrecision@R(%)    
Figure 2: The performance of CodeBERT on both original
and augmented training sets for POJ104 dataset.
Astheaugmenteddataaremoredifficulttolearn,itisnaturalto
letthemodellearntheoriginaldatafirstandthentheaugmented
data from easy to hard. We detail our curriculum learning strategy
based on this hypothesis in the next section.
289
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao
Algorithm 1 Validation Algorithm for Hypothesis 1
Input:Trainingset ð·,transformationfunctions ð‘‡1,...,ð‘‡ ð‘˜,model
ð‘€
1:Î“â†{ð·}, a set of training set variants
2:Î©â†{ }, a set of experimental results
3:ð·/prime=ð·
4:fortransformation ð‘¡â†1...ð‘˜do
5:ð·/prime=ð‘‡ð‘¡(ð·/prime)
6:Î“â†Î“âˆª{ð·/prime}
7:end for
8:Train model ð‘€with the whole training set Î“
9:fordatasetð‘¥inÎ“do
10:Calculate results on the model ð‘€,ð‘€(ð‘¥)
11: Î©â†Î©âˆª{ð‘€(ð‘¥)}
12:end for
13:return Î©
The second hypothesis we propose is to solve the multiclass
classificationtask.Imageclassification,textclassificationlikenews,
andalgorithmclassificationareallclassicalmulticlassclassification
tasks. The task is quite difficult, and a common simplification isto split the multiclass classification task into easily solvable sub-
tasks withfewer classes.Hence, weformulate thehypothesis that
forthemulticlassclassificationtask,itismoreeffectiveto
determine the learning order of the model from a class per-
spective. Based on this hypothesis, the optimization goal of the
model gradually transitions from a classification problem with few
classes to a classification of multiple classes during the entire train-
ing process. Intuitively, the task is much easier to solve under this
settingcomparedtoastraightforwardsolution.Wenextconduct
an experiment to verify the hypothesis.
The difficulty of code data may be reflected in the length of the
code, the use of rare tokens, the complexity of logic, etc. Although
these heuristics are reasonable for people, they are not necessarily
thecaseformodels.Therefore,unlikethepreviousvalidationexper-
imentthatusescodeaugmentationtechniquestodistinguishthe
difficultyof thesamples artificially,we letthemodel itselfgive an
evaluationofthedataasthedifficultyscores,asshowninAlgorithm
2.
The purpose of Algorithm 2 is to get the average difficulty score
of each class on the training set. To get the difficulty score of each
sampleonthetrainingset,weapplytheleave-one-outstrategy, i.e.,
whenwecomputethedifficultyscoresforapartofthesamples,we
train the model with all the other data. (line 4-9) Then we compute
the average difficulty scores on each class. (line 11-14)
To have a comparison with the learning order under the first
hypothesis,wealsoapplyAlgorithm2tothestate-of-the-artmodel
CodeBERT with POJ104 dataset. POJ104 dataset contains many
classes,andthetaskofPOJ104datasetistopredicttheclassfora
given program. We apply Algorithm 2 to both the original training
set and the augmented training set. We sort their average difficultyscores of each class according to the scores on the original training
set, as shown in Fig. 3.0.300.400.500.600.700.800.901.00
0 1 02 03 04 05 06 0Precision@R
Class ID Results on Original Dataset Results on Augmented Dataset
Figure 3: Visualization of average performance across train-
ing classes for POJ104 dataset by CodeBERT.
Algorithm 2 Validation Algorithm for Hypothesis 2
Input:Training set ð·, the entire dataset ð‘†, modelð‘€
1:ð¶â†{ }, a set of difficulty scores
2:Î˜â†{ }, a set of average difficulty scores on classes
3:Split training set ð·uniformly as { ð·ð‘–:ð‘–=1...ð‘}
4:forð‘–â†1...ð‘do
5:Calculate the difference set of ð·ð‘–overð‘†,ð‘†âˆ’ð·ð‘–
6:Train model ð‘€withð‘†âˆ’ð·ð‘–and get model ð‘€ð‘–
7:Evaluate ð·ð‘–withð‘€ð‘–and obtainthe experimental results of
each sample as the difficulty score set ð¶ð‘–
8:ð¶â†ð¶âˆªð¶ð‘–
9:end for
10:Train model ð‘€with training set ð·
11:forclassð‘¥inð·do
12:Calculate average difficulty scores on class ð‘¥,ðœ‡(ð¶,ð‘¥)
13: Î˜â†Î˜âˆª{ðœ‡(ð¶,ð‘¥)}
14:end for
15:return Î˜
FromFig.3itcanbefoundthattheperformanceofthemodel
on various classes varies greatly. The experimental performance
reflects the difficulty of classes; the better the experimental perfor-
mance,thelowerthedifficulty,andviceversa.Also,wefindthat
the performance on the augmented dataset is almost always lower
than that on the original dataset, further validating our previous
hypothesis. At the same time, Fig. 3 shows that the performance of
themodelontheaugmenteddataset,althoughdecreasing,isalways
distributed around the performance of the same class on the origi-
naldataset.Therefore,weconcludethatformulticlassclassification
tasksorganizingthedatabyclasscanyielddatawithmorestable
gradients than artificially differentiating the data by augmentation
techniques.Itmotivatesustoexposemodelstotheeasierclasses
first and then gradually transition to the harder classes.
3 PROPOSED APPROACH
Inthissection,wedescribethedetailsofourapproach.Ourmethod
is built on the fine-tuning paradigm and adapts pre-trained models
to downstream tasks. Given pre-trained models and datasets of
downstreamtasks,weexploitthepotentialofpre-trainedmodels
on these tasks by acting on the data only.
290
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Training 
set
Augmented
datasetOrdered
datasetModel ResultsDeclaration
API
ControlOriginalAugmentation technique
Test-time augmentation
NTi/g166
Figure 4: Overview of our proposed method.
3.1 Approach Overview
Fig. 4 presents an overview of our approach. Our approach mainly
consists of three components.
â€¢Augmentation for training data that transforms given
programsintosemanticallyequivalentprogramsandbuild
augmented dataset to make training data more diverse.
â€¢Curriculum strategy that organizes augmented dataset
into the ordered dataset in an easy-to-hard order. The order
is determined by scoring functions.
â€¢Test-time augmentation that yields transformed versions
of programs for prediction. The results are the fusion of
results of original programs and transformed programs of
different transformation types.
Table 1: Code Transformation Techniques
Transformation
FamilyC/C++ Java
Controlfor/while/if
transformerfor/while/if_else
transformer
APIinput/output
c/cpp_style
transformerequal_loc/equal_func/
add_assign transformer
Declaration
and otherunused_decl/brace/
return transformerstmt_sort/merge/divide
transformer
3.2 Augmentation for Training Data
Inordertohelpmodelslearncodefeaturesinawaythatareinvari-
ant to semantically equivalent programs, we construct semantic-
preserving transformations for code data. The lexical appearances
andsyntacticalstructuresaredifferentbeforeandaftertransforma-
tions, but the semantics of programs are identical.
Various languages apply different transformation techniques
duetospecificlanguagecharacteristics.Inthispaper,weusethe
same transformation techniques for data in the same language
whichdonotrelyonpriorknowledgefromtasksordatasets.There
are two programming languages in our experiments. For C/C++,
we modify the work from Quiring et al.[34]. For Java, we apply
theSPATtool[ 39].Weapply tentransformationsforC/C++ and
nine transformations for Java. The specific transformations areshown in Table 1. These techniques are grouped by the granu-larity of their changes. They change the control structure, API
and declaration, respectively, to help models extract and learn thecorresponding features, while ensuring that the semantics remainunchanged.TakingthetransformationsinFig.1asanexample,the
ð‘“ð‘œð‘Ÿtransformer is applied to transform the original program to
the1âˆ’ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› programandconvertsthe ð‘“ð‘œð‘Ÿstructureto
ð‘¤â„Žð‘–ð‘™ð‘’.Thistypeoftransformationenablesthemodeltounderstand
variouscontrolstructures.From1 âˆ’ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› programto
2âˆ’ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› program, ð‘¢ð‘›ð‘¢ð‘ ð‘’ð‘‘_ð‘‘ð‘’ð‘ð‘™andð‘ð‘Ÿð‘Žð‘ð‘’transformer
are applied. This type of transformations could also generate di-
verseandequivalentdeclarationstatementsbymerging,splitting
and swapping declaration statements, helping the model to ignore
theinterferenceofsyntacticformalsandfocusonsemantics.Inthelasttransformationto3
âˆ’ð‘¡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› program,theoutputAPI
ð‘ð‘œð‘¢ð‘¡is converted to ð‘ð‘Ÿð‘–ð‘›ð‘¡ð‘“. The API transformation exploits the
fact that thesame function can be implementedby different APIs.
These transformation techniques would also work in combination
to make the dataset more diverse.
3.3 Curriculum Strategy
Thekeychallengeofcurriculumlearningishowtodefineeasy/difficult
examples.Inthispaper,weproposetwodifficultyscoringfunctions
based on the hypotheses presented in Section 2.3.
Augmentation-based Curriculum Strategy. The previous section
hasintroduceddataaugmentationtechniquesforcodedata,anditis
cheap to generate diverse data through transformations. However,
compared with original data, the augmented data can be regarded
asperturbationsoradversarialexamplesoforiginaldata[ 37,53],
and they should be more difficult to learn as verified in Section 2.3.
Therefore,wedesignanaugmentation-basedcurriculumstrat-
egy. We first train on only the original data, and then gradually
increasetheproportionoftheaugmenteddata,ensuringthatthe
modelisexposedtomoredataandthedifficultygraduallyincreases
during the training process.
In particular, it should be noted that in the process of learn-
ing the augmented data we do not strictly follow the order of
1âˆ’ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› programsto ð‘šð‘¢ð‘™ð‘¡ð‘–âˆ’ð‘‡ð‘Ÿð‘Žð‘›ð‘ ð‘“ð‘œð‘Ÿð‘šð‘Žð‘¡ð‘–ð‘œð‘› programs,
since wefind that some programshave far more transformed pro-
gramvariantsthanothersandmultipletransformationscouldcause
the data to be unbalanced. Therefore, we sample an equal number
ofaugmentedsamplesfromthetransformedprogramvariantsof
eachsampleintheoriginaltrainingsetforlearning,andthedatastatisticsareshowninTable2.Thismethodiseasytoimplement
on general models, and we illustrate its effects in the following
experiments.
Class-based Curriculum Strategy. Especially for multiclass classi-
ficationtasks,basedonthehypothesisverifiedinSection2.3,we
propose a class-based curriculum strategy.
Specifically,theleave-one-outstrategyisemployedtoobtainthe
difficulty scores on the entire training set, and then the average
difficultyscoreoneachclassiscalculated.Thesamplesinthesameclasstaketheaverageclassdifficultyscoreastheirdifficultyscores.Inthetrainingprocess,thissettingallowsthemodeltolearneasierclassesfirst,andthentomoredifficultclasses.Obviously,themodel
needs toextract and learn morefeatures to deal withincreasingly
difficult tasks.
Once the scoring function is determined, we still need to de-
finethepaceatwhichwetransitionfromeasysamplestoharder
291
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao
samples.Withreferencetothework[ 32],whenselectingandap-
plyingdifferentpacingfunctions,weensurethatthemodelhasa
numberofsamplestolearnwhenthetrainingiterationbegins,and
gradually gets in touch with difficult samples until all samples are
available.Weimplementarangeofpacingfunctionsaccordingto
Penhaet al.[32] and illustrate its effects in Section 5.3.
3.4 Test-Time Augmentation
We also apply augmentations to the test set. These are the same
astheaugmentationtechniquesappliedonthetrainingset.TTA
neither modifies trained models nor changes test distribution. It
performspredictionsonthesametestdataasthegeneraltestproce-
dure,exceptthatthepredictionforeachtestinputistheaggregation
of predictions on multiple transformed versions.
Tofurthereliminateoverconfidentincorrectpredictionsdueto
overfitting [ 48], for each sample in the test set we sample three
augmented copies from its transformed candidates. Sampling more
samplesforpredictionmaymaketheresultsmorerobust,butwould
increase the predictiontime proportionally. Asshown in the right
part of Fig. 4, the final experimental performance is the sum ofresults on the original test set and results in the augmented per-
spective, which are the average of the results on augmented copies.
As a result, incorrect prediction on a single test case by the model
is corrected by combining multiple perspectives to make a final
prediction.
4 EXPERIMENTS
In this section, we conduct experiments to verify whether our
methodiseffectiveindifferenttasks,includingalgorithmclassifi-
cation, code clone detection and code search tasks.
Table 2: Data Statistics
Dataset Original training set Augmented training set
POJ104 30815 123058
CodeCloneBench 901028 3362570
CodeSearchNet 164923 331533
4.1 Data preparation
Inthissubsection,wepresentbenchmarkdatasetsforthreetasks
from CodeXGLUE [ 27]: POJ104, BigCloneBench [ 45] and Code-
SearchNet[ 16]anddescribehowtosimplyadaptdataofvarious
tasks to our approach.
POJ104datasetiscollectedfromanonlinejudgeplatform,which
consistsof104programclassesandincludes500student-written
C/C++programsforeachclass.ThetaskforPOJ-104datasetisto
retrieve other programs that solve the same problem as a given
program.Wesplitthedatasetaccordingtolabels.Weuse64classes
of programs for training, 24 classes of programs for testing, and 16
classesofprogramsforvalidation.Fordataaugmentation,tosuc-
cessfullycompiletheprograms,â€œ#includeâ€statementsareprepended
beforetheprograms.Thisprocessdoesnotintroducedifferences
since added statements are the same for all programs. As some
programs cannot be compiled, we further use regular expressions
tocorrectprogramswithsimplegrammaticalerrors,andremovetherestwithseriousgrammaticalandsemanticproblems.Atotalof
1710programswereremoved,accountingforabout3%(1710/52000).
To guarantee the fairness of the experiments, we also evaluate the
baseline models on both the original dataset and the normalized
dataset. For test-time augmentation, the results of the original and
augmented versions of the same program are merged to make a
prediction.
BigCloneBenchdatasetcontains25,000Javaprojects,cover10
functionalities and including 6,000,000 true clone pairs and 260,000falseclonepairs.ThedatasetprovidedbyWang etal.[
49]isfiltered
by discarding code fragments without any tagged true or false
clone pairs, leaving it with 9,134 Java code fragments. The dataset
includes901,028/415,416/415,416pairsfortraining,validationand
testing, respectively. This dataset has been widely used for thecode clone detection task. For code augmentation, since the datais in the form of code pairs, we replace any original program in
clone pairs with augmented programs to form new pairs. For test-
time augmentation, all versions of a code pair are considered to
determine whether it is a clone pair.
CodeSearchNet contains about 6 million functions from open-
source code spanning six programming languages. In this paper,
weusethedatasetinJava.Givenanaturallanguagequeryasthe
input, the task is to find the most semantically related code from a
collection of candidate programs. According to the state-of-the-art
model GraphCodeBERT [ 13], we expand 1000 query candidates to
thewholecodecorpus,whichisclosertothereal-lifescenario.The
answer of each query is retrieved from the whole validation and
testing code corpus instead of 1,000 candidate programs. For code
augmentation in the training set, since the data are pairs of naturallanguagequeriesandprogramminglanguagefragments,wereplace
originalprogramswithaugmentedprogramsandformnewpairs
withtheirnaturallanguagequeries.Whendoingtest-timeaugmen-
tation, it is different from the previous two tasks. Since the test set
is the set of natural language queries, we apply code augmentation
techniquestothecodebasecorrespondingtothesequeries.Fora
queryandeachcodeincodebase,wecalculatesimilarityofthecodeanditsmultipletransformedversionstothequery,respectively.We
use the average of similarity for sorting and evaluation.
Theoriginalandaugmenteddatastatisticsoftheabovetasksare
shown in Table 2 and the augmented datasets contain the original
data. We release all data for verification and future development.
Theoretically,moreaugmenteddatacanbeobtained,however,more
data to train would bring larger time overhead. To trade off theexperimental performance and time overhead, we use a limitedamount of augmented data, and we apply curriculum learningstrategy where the model is trained from a smaller data size and
the overhead is further reduced.
4.2 Experimental Setups
To illustrate the effectiveness of our method on code-related tasks,
we build our approach on code pre-trained models CodeBERT and
GraphCodeBERT. To illustrate the applicability of our method, we
alsoevaluateourmethodonnaturallanguagepre-trainedmodel
RoBERTa[ 26]thathasnotbeenexposedtocodeatall.Inreplica-
tionexperiments,wefollowthedescriptionintheiroriginalpapers
and released code. For parameter settings, to ensure fairness, we
292
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
keepallparametersconsistentwiththeirreleasedcodeincluding
random seeds except for the warmup step and epoch. The warmup
stepparameteradaptstotheincreaseofthedataset,anditsvalue
is adjusted from the original dataset size to the augmented dataset
size.Alsoduetotheincreaseindatasizeandthe progressivecur-
riculum learning, we increase the epoch and set it to 20, 10, and 15
on POJ104, BigCloneBench, and CodeSearchNet, respectively. We
replicateCodeBERTandGraphCodeBERTwiththesameparameter
settings.Theresultsreportedintheoriginalpapersandourrepli-
cated results are not much different, and we present all the results.
For data augmentation, we implement augmentaion techniques on
the top of Clang [ 1] for C/C++. With respect to pacing function,
the hyperparameters are set according to Penha et al.[32].
4.3 Algorithm Classification
Metrics and Baselines. We use precision and MAP as the evalua-
tionmetricsofthealgorithmclassificationtask.Precisionisdefined
as the average precision score and MAP is the rank-based mean of
averageprecisionscore,eachofwhichisevaluatedforretrieving
most similar samples given a query. We apply RoBERTa and the
state-of-the-art model CodeBERT as baseline methods. RoBERTa is
apre-trainedmodelonnaturallanguage.CodeBERTisapre-trained
model on code data. It combines masked language modeling [ 9]
with replaced token detection objective [ 8] to pre-train a Trans-
former [47] encoder.
Table 3: Algorithm Classification Comparison
Model Precision MAP
RoBERTa 82.82 80.31(76.67)
RoBERTa + DA + CL 88.15 86.55
CodeBERT 85.28 82.76(82.67)
CodeBERT + DA + CL 93.63 92.91
Results.We compare with and without our method (DA + CL)
for these pre-trained models. Table 3 summarizes these results. For
baseline methods, all experimental results are evaluated on our
normalizeddataset,exceptforresultsofMAPinparentheses.These
resultsarereportedintheoriginalpaperofbaselinemethodsand
MAP is their only metric for algorithm classification task. Natural
language pre-trained model RoBERTa fine-tuned with our method,
achieves88.15%onprecision,86.55%onMAP.Ourmethodimproves
itsperformancenoticeablyby5.33%onprecision,6.31%onMAPand
9.88% compared to the results reported in the original paper. Code
pre-trainedmodelCodeBERTfine-tunedwithourmethod,achieves
93.63% precision and 92.91% on MAP. Our method substantially
improves8.35%onprecision,10.15%onMAP,and10.24%compared
totheoriginalresult.Notably,withourmethod,RoBERTamodel
withoutbeingpre-trainedoncodedataoutperformstheexisting
state-of-the-art model CodeBERT fine-tuned on this task by 3.79%
MAP.
4.4 Code Clone Detection
Metrics and Baselines. We use precision, recall and F1 score as
the evaluation metrics of the code clone detection task. In ourexperiments, we compare a range of models including the state-of-
the-art model GraphCodeBERT. GraphCodeBERT is a pre-trained
modelforcodewhichimprovesCodeBERTbymodelingthedata
flowedgesbetweencodetokens.CDLH[ 50]learnsrepresentations
of code fragments through AST-based LSTM. ASTNN [ 56] encodes
ASTsubtreesforstatementsandfeedstheencodingsofallstatement
treesintoanRNNtolearnrepresentationforaprogram.FA-AST-
GMN [49] leverages explicit control and data flow information and
uses GNNs over a flow-augmented AST to learn representation for
programs. TBCCD [ 54] proposes a tree convolution-based method
to detect semantic clone, that is, using AST to capture structural
informationandobtainlexicalinformationfromtheposition-aware
character embedding.
Table 4: Code Clone Detection Comparison
Model Precision Recall F1
CDLH 0.92 0.74 0.82
ASTNN 0.92 0.94 0.93
FA-AST-AMN 0.96 0.94 0.95
TBBCD 0.94 0.96 0.95
RoBERTa(10% data) 0.966 0.962 0.964(0.949)
RoBERTa(10% data) + DA + CL 0.973 0.957 0.965
CodeBERT(10% data) 0.960 0.969 0.965
CodeBERT(10% data) + DA + CL 0.972 0.972 0.972
GraphCodeBERT 0.973 0.968 0.971
Results.Table4showsresultsforcodeclonedetection.Ourre-
produced results are mostly consistent with results reported in
originalpapers,exceptfortheF1scoreof0.964forRoBERTa,whichishigherthantheoriginalresultof0.949.Weimplementourmethod
onRoBERTaandCodeBERT.Experimentsshowthatmodelswith
our method consistently perform better than the original mod-
els.Notably,withourmethod,RoBERTaperformscomparablyto
CodeBERT, and CodeBERT outperforms the state-of-the-art model
GraphCodeBERT.Moreimportantly,followingtheoriginalsettings
ofCodeBERT,CodeBERTonlyrandomlysamples10%ofthedata
fortrainingcomparedtoGraphCodeBERT.EventhoughweexpandthedatausingdataaugmentationintheexperimentforCodeBERT,thedatausedbyCodeBERTarestillmuchlessthandataforGraph-
CodeBERT.
4.5 Code Search
Metrics andBaselines. For codesearch task,weuse MRRas the
evaluation metric. MRR is the average of the reciprocal rank ofresults of a set of queries. The reciprocal rank of a query is the
inverse of the rank of the first hit result.
Table5showstheresultsofdifferentapproachesontheCode-
SearchNet corpus. The first four rows are reported by Husain et
al.[16].NBOW,CNN,BIRNN andSELFATTrepresentneuralbag-
of-words [ 40], 1D convolutional neural network [ 19], bidirectional
GRU-based recurrent neural network [ 7], and multi-head atten-
tion [47], respectively.
Results.Table5showsresultsofdifferentapproachesforcode
search. RoBERTa (code) is pre-trained on programs from Code-
SearchNet with masked language modeling while maintaining the
293
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao
Table 5: Code Search Comparison
Model MRR
NBow 0.171
CNN 0.263
BiRNN 0.304
SelfAtt 0.404
RoBERTa 0.599
RoBERTa(code) 0.620
RoBERTa + DA + CL 0.635
CodeBERT 0.676
CodeBERT + DA + CL 0.697
GraphCodeBERT 0.696(0.691)
GraphCodeBERT + DA + CL 0.720
RoBERTa architecture. Our reproduced result 0.696 of GraphCode-
BERTisslightlydifferentlyfromtheoriginallyreportedresult0.691.
Weimplementour methodonRoBERTa,CodeBERT andthestate-
of-the-artmodelGraphCodeBERTforcodesearch.Theresultsshow
thatnaturallanguagepre-trainedmodelRoBERTawithourmethodoutperformsRoBERTa(code),whichisthesamemodelarchitecturepre-trainedoncodedata.CodeBERTwithourmethodoutperforms
the original state-of-the-art model GraphCodeBERT. The perfor-mance of GraphCodeBERT with our method reaches 0.720 MRR,
surpassing the original result 0.691 MRR.
4.6 Summary
On above tasks and their benchmark datasets, our method substan-
tiallyimprovestheperformanceofarangeofpre-trainedmodels,
achievingthestate-of-the-artperformanceonalltasks.Forthenatu-rallanguagepre-trainedmodelwithnoexposuretocodeatall,with
thehelp ofourapproach,it isabletomatch orevensurpassexist-
ing code pre-trained models normally fine-tuned to corresponding
tasks. In the code search task, RoBERTa pre-trained with natu-
rallanguageandfine-tunedwithourmethod,surpassesthesame
architecture pre-trained with code data and fine-tuned with the
general method. These all illustrate the strong bridging role of our
method between pre-trained models and code-related downstream
tasks by introducing semantic knowledge for downstream tasks
into pre-trained models.
Forcode-relatedtasks,applyingourapproachtoapre-trained
modelatthefinetunestagewitharelativelysmallcostispreferable
to pre-training a more complicated model from scratch with huge
resources. It illustrates the superiority of our method, but this isnottonegatetheworkofcodepre-trainedmodelseither.Infact,our approach achieves better results when applied to a superior
pre-trained model. Probably, the research of pre-trained models for
source code has much work to do in terms of data diversity and
conjunction with downstream tasks.
5 ANALYSIS
This section analyzes the effects of different parameters on the
performance of tasks in our experiment.5.1 Ablation Study
Thissectioninvestigateshowdataaugmentationandcurriculum
learning affect the performance of models, respectively. The fol-
lowing subsectionsshow theseresults for algorithmclassification,
code clone detection and code search task.
Table 6: Ablation Study on Algorithm Classification
Model Precision MAP
CodeBERT 85.28 82.76
CodeBERT + DA + CL 93.63 92.91w/o DA-Training 91.90 90.79w/o TTA 88.76 87.21
w/o CL 92.55 91.52
Algorithm Classification. For algorithm classification task, we
conduct experiments without augmention on training set (DA-
Training),test-timeaugmentationorcurriculumlearning.There-
sultsareshowninTable6.Thefirstrowshowstheresultsofthe
baselinemodel.Thesecondrowpresentstheresultsofthebaseline
model with our full method. The third row removes augmentation
onthetrainingset.Thefourthrowpresentstheresultsofremov-
ing test-time augmentation. The results of removing curriculumlearning strategy are shown in the last row. As seen from the re-
sults,removinganyofthecomponentsleadstoadropofthemodel
performance,andtheremovaloftest-timeaugmentationleadsto
a significant performance degradation, indicating that all three
componentsarenecessarytoimproveperformance,andtest-time
augmentaion contributes the most to the improvements. We be-lieve that for clustering tasks similar to algorithm classification,
integratingmultipleperspectivesinadataaugmentationmanner
during testing could be a huge boost to model performance.
Table 7: Ablation Study on Code Clone Detection
Model Precision Recall F1
CodeBERT 0.963 0.965 0.964
CodeBERT + DA + CL 0.972 0.972 0.972w/o TTA 0.971 0.972 0.971
w/o CL 0.976 0.965 0.970
w/o DA-Training + CL 0.964 0.965 0.964
CodeCloneDetection. Forcodeclonedetectiontask,wealsocon-
duct experiments without augmention on training set, test-time
augmentationorcurriculumlearning.Unlikealgorithmclassifica-
tion,weapplyaugmentation-basedcurriculumlearningforcode
clone detection task. The removal of augmentation on the training
set means that the CL component also does not work, and onlytest-time augmentation component works. The experimental re-sults in Table 7 show that the combination of augmentation onthe training set and CL component has the largest performance
improvement,andtest-timeaugmentationhasnosignificantper-
formance improvement, but the model can still benefit from it.
294
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 8: Ablation Study on Code Search
Model MRR
GraphCodeBERT 0.696
GraphCodeBERT + DA + CL 0.720
w/o TTA 0.707
w/o CL 0.708
w/o DA-Training + CL 0.710
Code Search. With the same ablation experimental setups as for
the code clone detection task, we conduct experiments on the code
searchtask.AsshowninTable8,weconcludethatallthreecom-
ponentsarenecessaryfortheimprovements.Thelastrowshows
the resultusing only test-timeaugmentation, which isable to sig-
nificantly exceed the original state-of-the-art performance with-
out training with additional augmentation data. We speculate that
test-timeaugmentationis abletocombinemultiple augmentation
copiesinthecoderetrievalprocesstomakejudgmentsandelim-inate overconfident incorrect predictions on the original test set.
Thepenultimaterowshowstheexperimentalresultofremoving
CLcomponent.Inotherwords,itisobtainedbythecombination
of augmentation on the training set and test-time augmentation
actingonthemodel.Comparedtotheresultofapplyingtest-time
augmentationcomponentonlyinthelastrow,wefindthatmore
augmenteddatausedfortrainingmayresultinnegativegains.One
possiblereason isthat theaugmented dataintroduces morenoise,
causing the model to choose from more candidates for the same
query during training. These results further illustrate the necessity
of curriculum learning on augmented data.
Table 9: Effects of Augmentation Types on Algorithm Classi-
fication
Model Precision MAP
All 93.63 92.91
w/o Declaration 92.13 90.88w/o API 92.35 91.24
w/o Control 94.17 93.41
5.2 Effects of Augmentation Type
Sincethispaperconsidersmultipleaugmentationtechniques,inthissection we explore the effects of augmentation techniques at differ-entgranularitiesontheexperimentalresults.Webuildtransformeddatasetsofthesamesizeusingaugmentationtechniquesofdifferent
granularitiesand trainCodeBERT separatelyon thesedatasets for
algorithmclassificationtask.ResultsareshowninTable9.Thefirst
rowshows theresultsusingallaugmentation techniques ofthree
granularities, while the second to fourth rows show the results
withouttheaugmentationtechniquesforthedeclaration,API,or
controlstucturegranularity,respectively.Fromtheresults,itcan
be seen that not using the augmentation techniques of declaration
or API granularity leads to a decrease in results, while not usingthe augmentation techniques of control sturcture leads to an in-
crease.ThisindicatesthattheaugmentationofdeclarationandAPIcontributemoretotheimprovements, however,thecontrolstruc-
ture augmentation introduces more noise than contribution. We
speculate that changing the control structure has a greater impact
onthetokenorderandcontextrelativetotheothertwogranular-
itiesofaugmentationtechniques,andpre-trainedmodelsweuse
are based on masked language modeling and are context sensitive.
These reasons make it more difficult for the models to learn the
knowledge and features introduced in the process of changing the
controlstructure.Thisfindingalsoencouragesthecodepre-trained
modeltofurtherexploitstructuralinformationofsourcecodein
order to better understand the program semantics.
Table 10: Effects of Pacing Function on Algorithm Classifica-
tion
Model Precision MAP
Random(baseline) 85.28 82.76
Anti 81.87 78.73
Linear 86.94 84.96
Step 86.39 84.35
Geom_progression 85.97 83.70Root_2 86.98 84.96
Root_5 86.11 83.95
Root_10 87.38 85.33
5.3 Effects of Pacing Function
To understand how the model is impacted by the pace we go from
easytohardexamples,weevaluatetheeffectsofdifferentpacing
functions on the experimental results, as shown in Table 10. We
conduct experiments onPOJ104 dataset in thealgorithm classfica-
tion task. The learning order is determined by the scoring function
described in Section 3.3. The baseline model CodeBERT is trained
in a random order and the Anti method orders training samples
from hard to easy. The other methods learn training samples from
easy to hard, with the difference that at each epoch a different pro-
portion of the training data are fed to the model as determined by
theirfunctions.Webrieflyintroducedifferentpacingfunctionsand
thedetailsaredescribedinPenha etal.[32].Theð¿ð‘–ð‘›ð‘’ð‘Žð‘Ÿfunction
linearlyincreasesthepercentageoftrainingdatainputtothemodel.
ð‘†ð‘¡ð‘’ð‘functiondividestrainingdataintoseveralgroups,andafter
fixed epoches a group of training samples will be added for model
training. ð‘…ð‘œð‘œð‘¡_ð‘›andðºð‘’ð‘œð‘š_ð‘ð‘Ÿð‘œð‘”ð‘Ÿð‘’ð‘ ð‘ ð‘–ð‘œð‘› functions correspond to
two extremecases. ð‘…ð‘œð‘œð‘¡_ð‘›functionfeeds themodelwith alarge
numberof easysamples andthen slowlyincreases theproportion
of hard samples, while ðºð‘’ð‘œð‘š_ð‘ð‘Ÿð‘œð‘”ð‘Ÿð‘’ð‘ ð‘ ð‘–ð‘œð‘› function does the oppo-
site.Inthe ð‘…ð‘œð‘œð‘¡_ð‘›function, ð‘›isthehyperparameter,andthelarger
the value of ð‘›, the more training data are fed to the model at the
beginning. All these functions are fed with the same training data
at the final stage of training.
InTable10,wecanseethatfeedingdatafromeasytohardhas
a certain performance improvement, while the performance of in-
puttingtrainingsamplesfromhardtoeasyissignificantlyworse
than the baseline in a random order. These results illustrates the
effectiveness of our curriculum learning strategy and scoring func-
tions. Comparison of different pacing functions shows that ð¿ð‘–ð‘›ð‘’ð‘Žð‘Ÿ
295
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao
andð‘†ð‘¡ð‘’ð‘functions achieve similar results as ð‘…ð‘œð‘œð‘¡_2 function. The
ð‘…ð‘œð‘œð‘¡functions obviouslyoutperformthe ðºð‘’ð‘œð‘š_ð‘ð‘Ÿð‘œð‘”ð‘Ÿð‘’ð‘ ð‘ ð‘–ð‘œð‘› func-
tion,whichisconsistentwiththefindingsofSohrmann etal.[43]
andPenha etal.[32].Thereasonsarethattherootfunctiongives
themodelmoretimetolearnfromharderinstancesandisbetter
than noCL in terms ofstatistical significance. Inour experiments,
we used ð‘…ð‘œð‘œð‘¡_10 function for algorithm classification task, and
sincewedidnotperformablationstudyonthedatasetsoftheother
two tasks, we use ð¿ð‘–ð‘›ð‘’ð‘Žð‘Ÿfunction by default. The performance on
these two tasks could probably be further improved with different
pacing functions, and we leave it for future work.
6 RELATED WORK
6.1 Data Augmentation
Data augmentation aims to increase the data diversity and thus
thegeneralizationabilityofthemodelbyvarioustransformation
techniques. This approach is widely used in the computer vision
domain[41,51,58].Inrecentyears,researchersapplydataaugmen-
tation to code data as well [ 34,35,37,53,55]. A series of studies
are motivated by the fact that existing models are vulnerable to
adversarialexamples,andtheydesignmethodstoexposethevul-
nerability of models and improve the robustness of models. Our
aimistomakethemodelsmoregeneralizableandperformbetter
on real data, unlike the methods described above. Jain et al.[17]
improveaccuracyincodesummarizationandtypeinferencetask
basedon equivalent datatransformationsand unsupervisedaux-
iliary tasks. Nghi et al.[6] propose a self-supervised contrastive
learning framework for code retrieval and code summarizationtasks. Our aim is similar to these studies, but we do not need to
designtheobjectivefunctionormodelarchitecture.Withoutthe
needforcomplicatedmodeldesign,ourapproachaccomplishesthesamegoalbyactingonthedataonly.Wesimplyaugmentsthedata
and feeds the augmented data into the model in an easy-to-hard
manner.Therefore,ourlightweightmethodcanbeeasilyapplied
over existing models and various downstream tasks.
6.2 Curriculum Learning
Learning educational material in order from easy to difficult is
very common in the human learning process. Inspired by cogni-
tivescience[ 38],researchershavefoundthatmodeltrainingcan
also benefit from a similar curriculum learning setting. Since then,
CL has been successfully applied to image classification [ 12,15],
machine translation [ 20,33,57], answer generation [ 24] and infor-
mation retrieve [32].
The core of CL lies in the design of the scoring function, that is,
how to define easy and hard samples. A straightforward approach
istostudythedatatocreatespecificheuristicrules.Forexample,
Bengioetal.[4]useimagescontaininglessvariedshapesaseasy
examples to be learned first. Tay et al.[46] use paragraph length
as an evaluation criterion for difficulty in the question answer
task. However, these are highly dependent on the task dataset and
cannot be generalized to general tasks. Guo et al.[14] examine
the examples in their feature space, and define difficulty by the
distribution density,which successfully distinguishesnoisy images.
Xuetal.[52]generallydistinguisheasyexamplesfromdifficultones
on natural language understanding tasks by reviewing the trainingset in a crossed way. In this paper, similar to Xu et al.[52], we also
utilizecrossvalidationtomeasuredatadifficultybymodelitself,but
wealsotaketheclassdistributionintoconsideration.Weintuitively
solve the multiclass classification problem from a class perspective
by first transforming it into a classification of fewer easy classes
andthengraduallyincreasingthenumberofdifficultclasses.Atthesametime,wecombinecurriculumlearninganddataaugmentation
toovercometheproblemthataugmenteddataismoredifficultto
learn. We first learn the original data, then gradually transitionto augmented data, and experimentally illustrate and verify the
effectiveness of the design.
7 THREATS TO VALIDITY
There are several threats to validity of our method.
â€¢Duetotheuseoftest-timeaugmentationinourmethod,thiscomponentcannotbeeasilyappliedtocodegenerationtasks.
Augmentationonthetrainingsetandcurriculumlearning
arestillapplicable,e.g.,Jain etal.[17]haveachievedgood
performance on the code summarization task using code
augmentation.
â€¢Thetransformationtechniquesweusearenotrepresentative
of the whole. Due to the characteristics of various tasks and
datasets,sometransformationsmayleadtolargeimprove-
ments and some may bing no improvements. Therefore, we
release the datasets for replication and reducing experimen-
talbias.Ourapproachisdesignedtobealightweightcom-
ponentthatgeneralizestomultipledownstreamtasks.For
specific downstream tasks, new augmentation techniques
can also be applied to optimize the performance.
â€¢Due to limited computed resource, we did not explore theperformance of our approach for the code clone detectiontask on GraphCodeBERT or conduct ablation stuies on all
threetasksregardingthepacingfunctionandtransformation
type. In fact, there should be room for improvement andinterestingconclusionstobeexplored.Weshallgetbetter
results by searching for more suitable pacing functions and
transformation types for the other two tasks. We leave it for
future works.
8 CONCLUSION
In this paper, we focus on bridging pre-trained models and code-
relateddownstreamtasksandproposealightweightapproachon
the fine-tuning paradigm, which is easy to implement on top of
variousmodels.Webuildourapproachoncodepre-trainedmodels
ofCodeBERTandGraphCodeBERT,andthesemodelssubstantially
outperform original models and achieve the state-of-the-art per-formance on algorithm classification, code clone detection and
codesearch.Moreover,weapplyourmethodtonaturallanguage
pre-trainedmodelRoBERTaanditachievescomparableorbetter
performancethanexistingstate-of-the-artcodepre-trainedmodels
fine-tuned on these tasks. This finding reveals that there is stillmuch room for improvement in existing pre-trained models for
source code understanding.
This paper focuses on code discriminative tasks. It is more chal-
lenging to apply our approach to code generation tasks. However,
generation tasks are data-hungry and may require more diverse
296
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
data for learning, such as code generation where multiple code
candidates are expected to be generated. In the future, it wouldbe interesting to combine our approach and prompt-based learn-
ing[25]tofurtherexploitthepotentialofgenerativepre-trained
models on code generation tasks.
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for
their insightful comments. This work was substantially supported
by National Natural Science Foundation of China (No. 61690203,
61872373, 62032019, and U1936213). This work was also supported
by the Major Key Project of PCL.
REFERENCES
[1] [n.d.]. Clang: a C language family frontend for LLVM. https://clang.llvm.org/
[2]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2021.
Unified Pre-training for Program Understanding and Generation. In NAACL.
[3]A. Aho, M. Lam, R. Sethi, and J. Ullman. 2006. Compilers: Principles, Techniques,
and Tools (2nd Edition).
[4]Yoshua Bengio, J. Louradour, Ronan Collobert, and J. Weston. 2009. Curriculum
learning. In ICML â€™09.
[5]TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,J.Kaplan,Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child,
A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDario
Amodei.2020. LanguageModelsareFew-ShotLearners. ArXivabs/2005.14165
(2020).
[6]Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised Contrastive
Learning for Code Retrieval and Summarization via Semantic-Preserving Trans-
formations. Proceedings of the 44th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (2021).
[7]KyunghyunCho,B.V.Merrienboer,Ã‡aglarGulÃ§ehre,DzmitryBahdanau,Fethi
Bougares,HolgerSchwenk,andYoshuaBengio.2014. LearningPhraseRepresen-
tations using RNN Encoderâ€“Decoder for Statistical Machine Translation. ArXiv
abs/1406.1078 (2014).
[8]Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.
ELECTRA:Pre-trainingTextEncodersasDiscriminatorsRatherThanGenerators.
ArXivabs/2003.10555(2020).
[9]J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In
NAACL-HLT.
[10]J.Elman.1993. Learninganddevelopmentinneuralnetworks:theimportanceof
starting small. Cognition 48 (1993), 71â€“99.
[11]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, X. Feng, Ming Gong, Linjun
Shou, Bing Qin, Ting Liu, Daxin Jiang, and M. Zhou. 2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. ArXivabs/2002.08155
(2020).
[12]Chen Gong, D. Tao, S. Maybank, W. Liu, Guoliang Kang, and Jie Yang. 2016.Multi-Modal Curriculum Learning for Semi-Supervised Image Classification.
IEEE Transactions on Image Processing 25 (2016), 3249â€“3260.
[13]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, LongZhou, Nan Duan, Jian Yin, Daxin Jiang, and M. Zhou. 2021. GraphCodeBERT:
Pre-trainingCodeRepresentationswithDataFlow. ArXivabs/2009.08366(2021).
[14]S. Guo, Weilin Huang, H. Zhang, Chenfan Zhuang, Dengke Dong, M. Scott,
andDinglongHuang.2018. CurriculumNet:WeaklySupervisedLearningfrom
Large-Scale Web Images. ArXivabs/1808.01097(2018).
[15]Guy Hacohen and D. Weinshall. 2019. On The Power of Curriculum Learning in
Training Deep Networks. ArXivabs/1904.03626(2019).
[16]H. Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. ArXivabs/1909.09436(2019).
[17]ParasJain,Ajay Jain,TianjunZhang,P.Abbeel,J. Gonzalez,andI.Stoica.2020.
Contrastive Code Representation Learning. ArXivabs/2007.04973(2020).
[18]LuJiang,DeyuMeng,T.Mitamura,andA.Hauptmann.2014. EasySamplesFirst:
Self-pacedRerankingforZero-ExampleMultimediaSearch. Proceedingsofthe
22nd ACM international conference on Multimedia (2014).
[19]Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In
EMNLP.
[20]Tom Kocmi and Ondrej Bojar. 2017. Curriculum Learning and Minibatch Bucket-
ing in Neural Machine Translation. In RANLP.[21]A. Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classifica-
tion with deep convolutional neural networks. Commun. ACM 60 (2012), 84 â€“
90.
[22]K. Krueger and P. Dayan. 2009. Flexible shaping: How learning in small steps
helps.Cognition 110 (2009), 380â€“394.
[23]M.Lewis, YinhanLiu,NamanGoyal, MarjanGhazvininejad,AbdelrahmanMo-
hamed,OmerLevy,VeselinStoyanov,andLukeZettlemoyer.2020. BART:De-
noising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. ArXivabs/1910.13461(2020).
[24]Cao Liu, Shizhu He, Kang Liu, and Jun Zhao. 2018. Curriculum Learning for
Natural Answer Generation. In IJCAI.
[25]Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, andGraham Neubig. 2021. Pre-train, Prompt, and Predict: A Systematic Survey
ofPromptingMethodsinNaturalLanguageProcessing. ArXivabs/2107.13586
(2021).
[26]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:
ARobustlyOptimizedBERTPretrainingApproach. ArXivabs/1907.11692(2019).
[27]ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
Blanco,ColinClement,DawnDrain,DaxinJiang,DuyuTang,GeLi,LidongZhou,
LinjunShou,LongZhou,MicheleTufano,MingGong,MingZhou,NanDuan,
NeelSundaresan,ShaoKunDeng,ShengyuFu,andShujieLiu.2021. CodeXGLUE:
AMachineLearningBenchmarkDatasetforCodeUnderstandingandGeneration.
ArXivabs/2102.04664(2021).
[28]A.Mastropaolo,SimoneScalabrino,N.Cooper,DavidNader-Palacio,D.Poshy-
vanyk, R. Oliveto, and G. Bavota. 2021. Studying the Usage of Text-To-Text
Transfer Transformer to Support Code-Related Tasks. 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE) (2021), 336â€“347.
[29]TambetMatiisen,A.Oliver,T.Cohen,andJ.Schulman.2020. Teacherâ€“Student
CurriculumLearning. IEEETransactionsonNeuralNetworksandLearningSystems
31 (2020), 3732â€“3740.
[30]Nikita Moshkov, Botond Mathe, Attila KertÃ©sz-Farkas, RÃ©ka Hollandi, and P.
HorvÃ¡th.2020.Test-timeaugmentationfordeeplearning-basedcellsegmentation
on microscopy images. Scientific Reports 10 (2020).
[31]Lili Mou, Ge Li, L. Zhang, T. Wang, and Zhi Jin. 2016. Convolutional Neural
NetworksoverTreeStructuresforProgrammingLanguageProcessing.In AAAI.
[32]GustavoPenhaandC.Hauff.2020. CurriculumLearningStrategiesforIR. Ad-
vances in Information Retrieval 12035 (2020), 699 â€“ 713.
[33]EmmanouilAntoniosPlatanios,OtiliaStretcu,GrahamNeubig,B.PÃ³czos,and
TomMichael Mitchell. 2019. Competence-based Curriculum Learningfor Neural
Machine Translation. In NAACL-HLT.
[34]Erwin Quiring, A. Maier, and K. Rieck. 2019. Misleading Authorship Attribution
of Source Code using Adversarial Learning. In USENIX Security Symposium.
[35]Md Rafiqul Islam Rabin, Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang, and M. A.
Alipour.2020. OntheGeneralizabilityofNeuralProgramAnalyzerswithrespect
to Semantic-Preserving Program Transformations. ArXivabs/2008.01566 (2020).
[36]Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
MichaelMatena,YanqiZhou,W.Li,andPeterJ.Liu.2020. ExploringtheLimitsof
TransferLearningwithaUnifiedText-to-TextTransformer. ArXivabs/1910.10683
(2020).
[37]Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, S. Jha,and T. Reps. 2020. Semantic Robustness of Models of Source Code. ArXiv
abs/2002.03043(2020).
[38]DouglasL.T.RohdeandD.Plaut.1999. Languageacquisitionintheabsenceof
explicit negative evidence: how important is starting small? Cognition 72 (1999),
67â€“109.
[39]SantiagoMunz. 2021. Semantic Preserving Auto Transformation. https://github.
com/SantiagoMunz/SPAT
[40]ImranA.Sheikh,I.Illina,D.Fohr,andG.LinarÃ¨s.2016.LearningWordImportance
with the Neural Bag-of-Words Model. In Rep4NLP@ACL.
[41]Connor Shorten and T. Khoshgoftaar. 2019. A survey on Image Data Augmenta-
tion for Deep Learning. Journal of Big Data 6 (2019), 1â€“48.
[42]K. Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks
for Large-Scale Image Recognition. CoRRabs/1409.1556 (2015).
[43]M.Sohrmann,C.Berendonk,M.Nendaz,R.Bonvin,andSwissWorkingGroup
ForProfilesImplementation.2020. Nationwideintroductionofanewcompetency
framework for undergraduate medical curricula: a collaborative approach. Swiss
medical weekly 150 (2020), w20201.
[44]Nitish Srivastava, Geoffrey E. Hinton, A. Krizhevsky, Ilya Sutskever, and R.Salakhutdinov.2014. Dropout:asimplewaytopreventneuralnetworksfrom
overfitting. J. Mach. Learn. Res. 15 (2014), 1929â€“1958.
[45]Jeffrey Svajlenko, Judith F. Islam, I. Keivanloo, C. Roy, and Mohammad Mamun
Mia. 2014. Towards a Big Data Curated Benchmark of Inter-project Code Clones.
2014 IEEE International Conference on Software Maintenance and Evolution (2014),
476â€“480.
[46]Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C. Phan, Xingdi Yuan, J.Rao, S. C. Hui, and A. Zhang. 2019. Simple and Effective Curriculum Pointer-
GeneratorNetworksforReadingComprehensionoverLongNarratives. ArXiv
297
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao
abs/1905.10847(2019).
[47]Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you
Need.ArXivabs/1706.03762(2017).
[48]Guotai Wang, Wenqi Li, M. Aertsen, J. Deprest, S. Ourselin, and Tom
Kamiel Magda Vercauteren. 2019. Aleatoric uncertainty estimation with test-
timeaugmentationformedicalimagesegmentationwithconvolutionalneural
networks. Neurocomputing 335 (2019), 34 â€“ 45.
[49]Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting Code Clones
withGraphNeuralNetworkandFlow-AugmentedAbstractSyntaxTree. 2020
IEEE 27th International Conference on Software Analysis, Evolution and Reengi-
neering (SANER) (2020), 261â€“271.
[50]HuihuiWeiandMingLi.2017. SupervisedDeepFeaturesforSoftwareFunctional
Clone Detection by Exploiting Lexical and Syntactical Information in Source
Code. InIJCAI.
[51]Jerry W. Wei, A. Suriawinata, L. Vaickus, Bing Ren, Xiaoying Liu, Jason Wei, and
S.Hassanpour.2019. GenerativeImageTranslationforDataAugmentationin
Colorectal Histopathology Images. Proceedings of machine learning research 116
(2019), 10â€“24.[52]Benfeng Xu, L. Zhang, Zhendong Mao, Q. Wang, Hongtao Xie, and Yongdong
Zhang.2020. CurriculumLearningforNaturalLanguageUnderstanding.In ACL.
[53]Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of
code.Proceedings of the ACM on Programming Languages 4 (2020),1â€“3 0 .
[54]HaoYu,WingLam,LongChen,GeLi,TaoXie,andQianxiangWang.2019. NeuralDetectionofSemanticCodeClonesViaTree-BasedConvolution. 2019IEEE/ACM
27th International Conference on Program Comprehension (ICPC) (2019), 70â€“80.
[55]HuangzhaoZhang,Z.Li,GeLi,L.Ma,YangLiu,andZhiJin.2020. Generating
AdversarialExamplesforHoldingRobustnessofSourceCodeProcessingModels.
InAAAI.
[56]J.Zhang,XuWang,HongyuZhang,HailongSun,KaixuanWang,andXudong
Liu.2019. ANovelNeuralSourceCodeRepresentationBasedonAbstractSyntax
Tree.2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE)
(2019), 783â€“794.
[57]XuanZhang,ManishKumar,HudaKhayrallah,KentonMurray,JeremyGwinnup,
Marianna J. Martindale, P. McNamee, Kevin Duh, and Marine Carpuat. 2018. An
Empirical Exploration of Curriculum Learning for Neural Machine Translation.
ArXivabs/1811.00739(2018).
[58]Zhun Zhong, L. Zheng, Guoliang Kang, Shaozi Li, and Y. Yang. 2020. Random
Erasing Data Augmentation. In AAAI.
298
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:52:58 UTC from IEEE Xplore.  Restrictions apply. 