Pitfalls in Experiments with DNN4SE:
An Analysis of the State of the Practice
Sira Vegas
sira.vegas@upm.es
Universidad Polit√©cnica de MadridSebastian Elbaum
selbaum@virginia.edu
University of Virginia
ABSTRACT
Software engineering techniques are increasingly relying on deep
learning approaches to support many software engineering tasks,
from bug triaging to code generation. To assess the efficacy of such
techniques researchers typically perform controlled experiments.
Conducting these experiments, however, is particularly challenging
given the complexity of the space of variables involved, from spe-
cialized and intricate architectures and algorithms to a large number
of training hyper-parameters and choices of evolving datasets, all
compounded by how rapidly the machine learning technology is
advancing, and the inherent sources of randomness in the training
process. In this work we conduct a mapping study, examining 194
experiments with techniques that rely on deep neural networks
appearing in 55 papers published in premier software engineering
venues to provide a characterization of the state-of-the-practice,
pinpointing experiments common trends and pitfalls. Our study
reveals that most of the experiments, including those that have
received ACM artifact badges, have fundamental limitations that
raise doubts about the reliability of their findings. More specifi-
cally, we find: 1) weak analyses to determine that there is a true
relationship between independent and dependent variables (87% of
the experiments), 2) limited control over the space of DNN relevant
variables, which can render a relationship between dependent vari-
ables and treatments that may not be causal but rather correlational
(100% of the experiments), and 3) lack of specificity in terms of what
are the DNN variables and their values utilized in the experiments
(86% of the experiments) to define the treatments being applied,
which makes it unclear whether the techniques designed are the
ones being assessed, or how the sources of extraneous variation are
controlled. We provide some practical recommendations to address
these limitations.
KEYWORDS
deep learning, machine learning for software engineering, software
engineering experimentation
ACM Reference Format:
Sira Vegas and Sebastian Elbaum. 2023. Pitfalls in Experiments with DNN4SE:
An Analysis of the State of the Practice . In Proceedings of The 31st ACM
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA
¬©2023 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXXJoint European Software Engineering Conference and Symposium on the Foun-
dations of Software Engineering (ESEC/FSE 2023). ACM, New York, NY, USA,
12 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
The application of deep learning (DL) techniques across the soft-
ware development life cycle is becoming a thriving research thread
in the software engineering (SE) community. Such emerging tech-
niques, often grouped under labels such as DL4SE or DNN4SE, have
rendered promising results supporting the automation of activities
ranging from requirements engineering to code maintenance [ 6].
Similar to other DL application areas, the maturation of frameworks
and tools that lowered the bar for the adoption for such technology
has facilitated their application in the SE domain. In addition, our
community is in an advantageous position in that we can tap into a
continuously increasing number of public repositories with various
types of software artifacts such as code and tests that constitute
rich data sets on which DL techniques can thrive.
To assess such techniques, researchers perform experiments in
which variables are manipulated in a controlled environment to
investigate their impact over response variables [ 15]. Conducting
such experiments, however, can be extremely challenging given the
number and complexity of variables that may affect a technique
that relies on DL. Tens of variables play a fundamental role in how
a deep neural network (DNN) is set up as part of an experiment.
Some of these variables are inherently complex as they point to
optimization procedures that contain their own set of parameters.
Other variables like those associated with datasets or competing
models often point to online resources that may unsuspectingly
evolve. Other variables, like those affecting the sample used by
gradient descent to set the network weights or the proportions
of data used for training and testing, are deceptively simple, yet
they constitute sources of randomness that will impact the DNN‚Äôs
performance. Yet other variables that may not be explicitly defined,
like the ones defining termination criteria, can have subtle interac-
tions with other variables undermining the implementation of the
intended experimental constructs. The key takeaway is that when
evaluating the application of a DL technique to a problem through
an experiment, the lack of careful consideration of a complex set of
variables can dramatically impact the findings.
Thegoal of this paper is to begin understanding the extent
to which experiments on DNN4SE techniques are addressing the
distinct experimental challenges introduced by DNNs.
In pursuing that goal we make four contributions.
I) We contribute a characterization and analysis of the state-of-
the-practice of experimentation with DNN4SE by addressing a fun-
damental question: RQ1: To what extent are DNN4SE experi-
ments specified in papers? To answer that question, in Section 3,arXiv:2305.11556v1  [cs.SE]  19 May 2023ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA Sira Vegas and Sebastian Elbaum
we present a systematic mapping study [ 17] of 55 papers from ICSE,
FSE, and TSE from 2018-2021 that apply DL techniques to auto-
mate SE tasks. Building on a cause-effect model of the experimental
space and the variables relevant to DNNs, we determine the degree
to which the variable space in the experiments was specified by
each paper. We find that while most experiments clearly identify,
for example, the response variables (76%) and training data (69%),
none describe their complete space of variables. Furthermore, most
experiments lack in critical aspects like the choice for experimental
design to control the sources of variability (30%) and the use of even
descriptive statistics as part of the results analysis and interpreta-
tion (56%). This lack of specificity is not just an under-reporting
issue, but it reflects a limited consideration of fundamental experi-
mental aspects that threaten the validity of the findings.
II) Given the community ongoing efforts for sharing artifacts [ 30],
we extend the previous characterization through RQ2. Do shared
artifacts improve the specifications of DNN4SE experiments
provided in the papers? Section 4 contributes an analysis of the
artifacts associated with the subset of papers that earned ACM
artifact badges, increasing the depth of analysis to include code,
data, and documentation. As expected, artifacts complement some
but not all aspects presented in the papers, especially the definition
of variables and the training and test data, all of which are necessary
to operationalize the experiments. However, we also find that 68%
of the experiments reported in the artifacts present inconsistencies
when compared with the corresponding paper, ranging from the
loss function to the testing data being used. This is problematic
because the additional effort invested to prepare artifacts to further
support the experiments often raises doubts about which portions
of the papers and the artifacts are to be trusted.
III) We contribute an analysis of why these findings matter through
RQ3. What are the implications of the previous findings about
the under-specification of DNN4SE experiments? Section 5
summarizes these implications. First, by failing to clearly define fac-
tors and treatments in 86% of the experiments, it is unclear whether
most experimental results are caused by the intended constructs
or by other variables that were not operationalized correctly. In
the best of cases, one could argue that those unspecified variables
in the papers are controlled when the experiments are performed.
However, our analysis of artifacts reveals that that is rarely the case.
Second, even when variables are specified it is often unclear how
they are controlled to establish causality. We find that 62% of ex-
periments account for sources of randomness related to the dataset,
and none controlled for other sources of training randomness by,
for example, performing multiple training runs or varying the DNN
initial weights. Third, we find that 56% of experiments identify rela-
tionships between independent and dependent variables based on
single observations which is suspect as it ignores any experimental
fluctuation.
IV)Recommendations. We are not the first community chal-
lenged by the DNN complexity. The AI community has devel-
oped various checklists to mitigate common ML experimental pit-
falls [ 1,18,22]. Similarly, the SE community has developed a body
of knowledge to assess and improve the quality of the experiments
we conduct (see related work in Section 8). However, as it shall be-
come clear from our RQ1-RQ2-RQ3 findings, there is a distinct and
Figure 1: A cause-and-effect diagram for 5 groups of vari-
ables in experiments with DNNs.
urgent need for the SE community to become much more cognizant
of how to manage the space of variables particular to the DNN
domain. Towards that end, we recommend actionable practices to
manage the challenges in DNN4SE experimentation (Section 7) that,
if adopted, can alleviate many of the concerns we encountered. For
example, simply conducting multiple DNN training runs to control
for randomness could benefit almost all experiments, performing
more meaningful comparisons over multiple observations to ac-
count for experimental variability in DNNs could benefit from 56%
to 87% of the experiments, and standardizing a minimal specifica-
tion of the space of DNN training variables and providing partial
automation for synchronizing the paper and artifact content of
DNN4SE experiments could benefit 96% of the experiments.
2 DNNS‚Äô EXPERIMENTAL VARIABLES
Machine learning (ML) is a subfield of Artificial Intelligence that
aims to enable computers to learn from experience [ 11,19]. ML
algorithms build a model based on sample (training) data to make
predictions without being explicitly programmed to do so [ 26]. DL
is a type of ML technique supported by neural networks that have a
deep architecture as per their constituting layers [ 11]. The training
of these DNNs consists of adjusting its model parameters , using a
deep learning algorithm controlled by a set of training hyper-
parameters andmodel hyperparameters , using a dataset [11].
We will later use these 5 groups of variables associated with DNNs,
illustrated through a cause-effect diagram in Figure 1, as a basis for
the analysis of experiments.
The DNN overall architecture is defined by the model hyperpa-
rameters and includes 5 variables. DNNs consist of interconnected
neurons grouped in layers . There is always an input layer that ac-
cepts inputs and an output layer that provides the output, and at
least two hidden layers between them. There are different layer
types , and how those layers are connected define the higher-level
architecture of the DNN (e.g., feed-forward, CNN, RNN, LSTM).
Every connection between 2 neurons has a weight which regulates
how much of the initial value will be forwarded to a given neuron.
Each neuron has an associated value, called the bias. Weights and
biases need to be initialized prior to training. The sum of the prod-
ucts of the inputs and respective weights, plus the bias, are then
provided to an activation function to produce a neuron‚Äôs output.
A forward pass is the set of calculations that take place when the
input travels through the DNN to the output.
The neuron weights andbiases constitute the two variables defin-
ing the DNNs model parameters . They are initialized before train-
ing and reset during the subsequent phases of the training process.Pitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA
Adataset is a collection of inputs and outputs. At least two
types of datasets are required: training andtest. The training set is
used to adjust the model parameters during training. The test set is
used to check how well the algorithm performs on data that has
not seen before, and it is intended to estimate the generalization
error. The training set can be further divided into a training and a
validation set . This validation set can be used to get an estimate of
model skill while tuning its hyperparameters.
TheDL algorithm is defined through 4 variables [ 11]: arep-
resentation for encoding the elements in the dataset, a function
measuring the error between the value predicted by the model and
the real value, an optimization procedure to minimize the training
error (e.g. stochastic gradient descent, Nesterov momentum, Adam),
andregulatization strategies to reduce the generalization (test) error
(e.g. dropout, data augmentation, early stopping).
During training, the DL algorithm‚Äôs behaviour is controlled
through 5 training hyperparameters [11]. The batch size defines
the number of training samples to consider per training iteration .
Depending on the batch size, multiple iterations will be needed to
go through the entire training set. The number of epochs defines
how many times the algorithm will go through a dataset. The train-
test split defines on what portion of the data training is performed.
Given a batch, the network performance (measured as a function
of error/cost/loss) is used to drive the backpropagation (the reverse
of a forward pass using gradient descent) to update the network
weights and biases to minimize this error. The learning rate specifies
how much to update the model in response to the estimated error.
Albeit simplified and limited for exposition, this section high-
lights the vast space of variables involved in training a DL system,
where each one can take an increasing number of values. These
variables also have many subtle interdependencies (e.g., the batch
and epoch size often depend on the parameter initialization, the loss
function depends on the architecture, the architecture depends on
the data dimensionality). Confounded with the multiple sources of
randomness involved in the DL training process (e.g., different train-
test partitions, different sample batches being selected, different
portions of the network being targeted for regularization, different
supported hardware), defining and conducting robust experiments
is intrinsically challenging.
3 ANALYSIS OF PAPERS
In this section we answer RQ1 by providing an overview of the
state of the practice in performing experiments where DNNs are
utilized to address SE challenges (DNN4SE). We characterize the
growing number of experiments being carried out in this domain
and identify some overarching limitations across those experiments.
3.1 Scope of Analysis
We have performed a semi-automated search of papers reporting
experiments with DNNs developed to solve SE tasks. Figure 2 sum-
marizes the search and selection process. We have shared in the
paper repository the outputs of each step.
In a first automated step, during January 2022, we searched
SCOPUSTMusing the string ‚Äúdeep OR neural‚Äù in all fields. The
search was limited to full papers from the technical track of the
International Conference on Software Engineering (ICSE) and the
Figure 2: Paper search and selection process.
Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE), and papers
published in IEEE Transactions on Software Engineering (TSE),
covering the period 2018-2021. We decided to favor the flagship
conferences ICSE and FSE because we believe they include the latest
work in DL and appear at the top of various ranks1. Similarly, we
selected TSE because it has the highest impact factor among SE
journals2. The search resulted in 444 out of 1154 published papers.
Next, we examined the papers to exclude the ones that did not
cover techniques using DNNs to address software engineering chal-
lenges. The examination was conducted by one of the senior re-
searchers authoring this paper with expertise in empirical software
engineering and DNN development. This process led to the exclu-
sion of 276 papers that did not include a DNN (e.g. a DNN-solution
is part of the related work in a paper), 68 papers that focus on
improving the engineering of DNN-solutions (e.g. testing of DNN-
solutions), and 24 papers that use ML mechanisms but not DNNs
(e.g., shallow networks). When papers that did not clearly fit in
existing categories were found, they were examined and discussed
jointly by both authors. The remaining 76 papers report experi-
ments with DL-based software to address SE challenges.
Table 1 shows the paper count distribution over the years and
venues. We can see that the number of papers in this area is steadily
increasing over the last few years, from 6 papers in 2018 to 41
papers in 2021. For the subsequent analysis we selected every paper
identified as within scope from 2018 to 2020, and given the larger
number of relevant papers published in 2021 (from 16 in 2020 to 41
in 2021), we randomly sampled 20 papers from 2021. This sampling
was necessary to control the cost of the study given that just data
extraction time per paper was approximately 4 hours per person
(we later describe the analysis costs per paper and artifact). This
gave us a total of 55 papers spanning four years to analyze.
3.2 Analysis Process
To have a consistent data extraction process from the papers, we
defined a set of scoping and analysis guidelines.
1https://csrankings.org, http://portal.core.edu.au/conf-ranks
2https://www.scimagojr.com, https://jcr.clarivate.com/jcr/homeESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA Sira Vegas and Sebastian Elbaum
Table 1: Number of papers within scope analyzed / pub-
lished, and experiments analyzed (in parenthesis)
ICSE ESEC/FSE TSE Total
2018 2/2 (5) 4/4 (11) 0 (0) 6/6 (16)
2019 8/8 (27) 4/4 (9) 1/1 (6) 13/13 (42)
2020 7/7 (25) 6/6 (21) 3/3 (14) 16/16 (60)
2021 7/15 (21) 7/15 (32) 6/11 (23) 20/41 (76)
Total 24/32 (78) 21/29 (73) 10/15 (43) 55/76 (194)
First, for each paper, we initially considered just the contents
of the published paper. At this early examination stage we did
not peak into artifacts that may be associated with the paper like
code repositories as we wanted to have a common baseline of
materials among all covered papers. This also made the analysis
cost more viable at the first stage of the study. In addition, for each
experiment in a paper we bounded the analysis to the DNN portions.
That is, when we found experiments comparing the performance of
DNNs against other type of approaches that employ traditional SE
approaches or humans, we deemed those portions of the experiment
as already understood by the community and only focused on the
portions including DNNs.
Second, we controlled for three common sources of uncertainty
we faced when analyzing the papers. To control for different report-
ing styles, we examined the papers in their totality as we often found
portions of the experiments distributed and modified throughout
the paper. For example, we found instances where the experimental
designs are sprinkled through background, approach, study design,
and results. To control for DNN usage types, we only considered
papers that use DNNs to perform either complex data encodings
or function as a model. Third, to control for different levels of de-
tail across experiments, we decided to account for all experiments
mentioned in the paper, even if marginally reported.
Given the previous guidelines, the analysis process started with
both authors jointly developing an initial characterization schema
for the experiments. This schema is based on the steps of the ex-
perimental process [ 10,20,31], although we adapted those steps
to account for the types of variables found in DNNs such as the
model hyperparameters, the training hyperparameters, the DL al-
gorithm, the dataset(s), and the model parameters (as per Figure 1).
Then, both authors conducted a refinement and calibration cycle
by extracting the information from all the experiments reported
in the 17 ICSE papers from 2018 to 2020 according to the schema.
This resulted in a refined schema and a more consistent evalua-
tion process. Finally, each remaining paper was examined by just
one author. However, when experiments did not fit the schema,
introduced new DNN elements, or had ambiguous specifications,
they were examined and discussed jointly by the researchers. There
were 8 of such joint examinations, lasting between 1-3 hours, which
often triggered the re-examination of previously evaluated papers
to ensure their consistent analysis.
Table 2 exemplifies the analysis we performed for an experi-
ment. The goal of this paper [ 51] is to perform log-based anomaly
detection. The proposed DNN receives as input a sequence of log
events and predicts whether the sequence is an anomaly. The ex-
periment evaluates the performance of the proposed DNN in terms
of precision, recall, and F1, comparing it against 4 other approaches(none are DNNs). Column 1 and 2 show the steps and aspects of
the experimental process, and column 3 assesses to what extent
the information has been identified (Fully, Partially, or Missing).
The last column provides an explanation of what is lacking. For
this experiment, we have been able to find all information related
to research hypotheses, DL algorithm, response variables and test
set characteristics. For this reason, their final assessment is ‚ÄúFully‚Äù
addressed. We have not been able to find any information related
to model parameters nor statistics, and therefore, their final as-
sessment is ‚ÄúMissing‚Äù. For the rest we have not able to find some
information, therefore, the final assessment is ‚ÄúPartially‚Äù addressed.
A detailed description of the classification criteria and its appli-
cation to all the 55 analyzed papers is available in the repository
(Section 10).
3.3 Findings
Table 3 summarizes the findings for the 194 experiments analyzed
across the 55 identified target papers. It is encouraging to find
that most experiments specify at least to some extent the response
variables, the research hypotheses, and the training and test set data.
However, the rest of the experimental aspects tend to be under-
specified. We find that 50% (7 out of 14) of the aspects are partially
addressed, while another 21% (3 out of 14) of the aspects are missing
among the experiments detailed in the papers.
We find that essential aspects are missing in most experiments.
For example, for the model parameters to be fully addressed, we
required a pointer to a repository where they could be found. Such
pointer was lacking for 98% of the experiments. For the choice of
(experimental) design to be fully addressed we required a descrip-
tion of what variables are manipulated or controlled and how, yet
30% of the experiments did not have it. For the analysis and inter-
pretation (S6) to be fully addressed we required descriptive and
inferential statistics, yet they were missing for 56% and 87% of the
experiments respectively. These results at least raise doubts about
whether most of the papers are: 1) implementing the construct
they are intending, 2) performing meaningful assessments given
the experimental noise that is not accounted for by the analysis
and interpretation, and 3) establishing causality given the limited
amount of control over the large and complex space of variables to
be specified.
4 ANALYSIS OF ARTIFACTS
In Section 3 our analysis of papers revealed that the under-specification
of experiments with approaches that use DL to address SE problems
is pervasive. Still, given our community growing practice towards
artifact sharing [ 30] and the nature of DL experiments (i.e., large
open datasets, common architectures, standard APIs), it seems rea-
sonable to ask whether the missing portions of the experiments
specifications appear in the shared artifacts. This is also important
as it may let us understand if the problem is just one associated
with how experiments are reported or if there is a deeper concern
about how the experiments are being conducted.
We begin to answer RQ2 through an analysis of the artifacts
associated with those papers to assess the degree to which the
under-specification in the papers is complemented by the associatedPitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA
Table 2: Assessing of a sampled experiment [51] specification in terms of Fully addressed, Partially addressed, or Missing.
Step Aspect Assessment What is lacking
S1. Hypotheses formulation Research hypotheses Fully
S2. Variables identification Model hyperparameters Partially Missing hyperparameters for initialization
Model parameters Missing Missing a pointer to where they can be found
DL algorithm Fully
Training hyperparameters Partially Missing train-test split and learning rate
Training data Partially No information about a dataset for confidentiality reasons
S3. Operationalization Factors and treatments Partially Some model and training hyperparameters are missing, not all
training data available, and model parameters are missing
Response variables Fully
S4. Design Choice of design Partially No analysis of sources of randomness, whether they have been
controlled, and if so, the mechanism used
Instrumentation Partially One test set is missing due to confidentiality issues. Software
environment is not defined. Measuring instruments and
procedure can be deduced but are not defined
S5. Objects selection Test set chars. Fully
S6. Analysis & interpretation Descriptive statistics Missing No descriptive statistics reported
Inferential statistics Missing No inferential statistics reported
S7. Validity evaluation Validity threats Partially Missing internal, construct and conclusion
Table 3: Characterization of 194 experiments with DNNs
Step Aspect Full Partial Missing
S1 Research hypotheses 76% 0% 24%
S2 Model hyperparam. 7% 85% 8%
Model parameters 2% 0% 98%
DL algorithm 26% 72% 2%
Training hyperparam. 19% 73% 8%
Training data 69% 27% 4%
S3 Factors and treatments 14% 82% 4%
Response variables 76% 18% 6%
S4 Choice of design 0% 70% 30%
Instrumentation 2% 97% 1%
S5 Test set characteristics 59% 19% 22%
S6 Descriptive statistics 10% 34% 56%
Inferential statistics 12% 1% 87%
S7 Validity threats 2% 79% 19%
artifacts, and whether the design and analysis limitations identified
are mitigated by the artifacts.
4.1 Scope of Analysis
Forty-eight out of 55 papers point to some kind of external arti-
fact. A cursory analysis of those artifacts reveals that their content
(from just readmes plus code to experimental results and even new
experiments), availability (from broken links to pointers to private
repositories or Zenodo), and quality (from a model dump without
any explanation to those including a code base to reproduce the
results in the paper) had too much variance to define a standardized
analysis that would render meaningful findings. This finding is
consistent with recent reports on artifact quality [30].
Thus, to get a more precise estimate of the degree of under-
specification when considering artifacts, we reduce the scope of
analysis to the artifacts associated with the 9 papers (including a
total of 44 experiments) that earned at least one of the ACM artifact
badges3[8]. This reduced scope allows us to focus more deeply on
3ACM defines three badges: Artifacts Evaluated (successfully completed an indepen-
dent audit, with two levels: Functional and Reusable), Artifacts Available (available for
retrieval), and Results Validated (results obtained by a team other than the original,papers vetted by a conference committee according to established
guidelines regarding their completeness and quality.
4.2 Analysis Process
We analyzed all artifacts with the following process. First, we ex-
amined the readme files and other introductory documentation to
get a broad sense of what the artifact was meant to provide. Sec-
ond, we systematically explored the artifact directories and their
contents to identify the resources of information to collect the data
required for Table 2. Third, we analyzed the code broadly construed
to include Python or C code, configuration files, and batch scripts.
The analysis was first meant to map each experiment reported
in the paper to the items in the artifact. Although conceptually
simple, this analysis process was nothing but straight-forward as
the artifact structure rarely matched that of the paper (where the
experiments are reported). In most cases, we had to recover por-
tions of one or multiple experiments from undocumented code.
This required multiple inspections of the code, running portions
of it to confirm that what was learned from the static inspections,
and referencing back the findings to the information in the paper.
Fourth, for each experiment identified in the artifact, we collected
metadata such as the one reported in Table 2 (more details about
the information collected are provided in the repository described
in Section 10). During this step we also determined whether the
artifact improved or complemented the information provided in
the paper, and recorded any inconsistencies we found between
them. These steps required approximately 8 hours per paper ([41]
was an exception given the number of experiments reported). The
difficulties in this process, particularly in the third and fourth steps,
and the time allocated per paper, forced us to be conservative in our
assessment, only judging an artifact experiment to be incomplete
or inconsistent with the paper when we had a high certainty that
that was the case. Still, these sources of uncertainty in our analysis
with two levels: Results Reproduced and Results Replicated). The 9 papers we analyzed
earned the Artifacts Available badge, and three of them also earned the Artifacts
Evaluated (two Reusable [36, 41] and one Reusable and Functional [43]).ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA Sira Vegas and Sebastian Elbaum
Table 4: Characterization of (44) experiments that earned
ACM Artifact Badges.
Improvements Constant
Step Aspect PAFA M PA M FA PA PA M PA FA
S1 Research hypotheses 0% 0% 0% 0% 18% 0% 82%
S2 Model hyperparam. 68% 0% 7% 0% 0% 5% 20%
Model parameters 0% 0% 9% 0% 82% 0% 9%
DL algorithm 39% 0% 7% 0% 0% 13% 41%
Training hyperparam. 59% 0% 7% 0% 0% 14% 20%
Training data 7% 0% 7% 0% 2% 4% 80%
S3 Factors & treatments 0% 2% 5% 52% 0% 30% 11%
Response variables 2% 3% 9% 0% 2% 0% 84%
S4 Choice of design 0% 0% 0% 0% 39% 59% 2%
Instrumentation 2% 7% 0% 5% 2% 75% 9%
S5 Test set chars. 0% 0% 7% 0% 43% 30% 20%
S6 Descriptive statistics 0% 2% 0% 0% 64% 23% 11%
Inferential statistics 0% 0% 0% 0% 95% 0% 5%
S7 Validity threats 0% 0% 0% 0% 27% 71% 2%
constitute a threat to the validity of our findings (Section 6) that
we mitigate by sharing our data (Section 10).
4.3 Findings
Table 4 summarizes our findings for the 44 experiments from papers
that earned ACM badges. The columns under Improvements contain
the % of experiments exhibiting gains across the specification levels
(i.e.,ùëÉùê¥‚Üíùêπùê¥means improvement from partially addressed to fully
addressed), while the columns under Constant show the aspects of
the experiments that remained unchanged.
Overall, and as expected, we find that considering the artifact
consistently improves the specification of some portions of the
experiments but not others. The improvement is particularly no-
ticeable in the variable identification step (S2) where many experi-
ments that were Partially Addressed (PA) become Fully Addressed
(FA). More specifically, the DL algorithm, model and training hy-
perparameters and the training data become fully addressed in 87%,
95%, 86% and 94% of the experiments, respectively4. The model
parameters (also part of S2) show a modest 9% gain caused by the
artifact for just one of the papers ([ 36]). Under operationalization
(S3), the response variables also improve, becoming full for 95%
of the experiments, while factors and treatments show some im-
provement for 59% of the experiments but still remains partially
addressed for 84% of the experiments. These operationalization
improvements were also expected as the code must assign values to
the independent variables and measure the dependent variables to
assess the experimental outcome. The rest of the aspects, which are
more closely associated with the experimental design and analysis
than the implementation, showed slight or no improvement. The
instrumentation showed an improvement for 14% of the experi-
ments, test set characteristics for 7%, descriptive statistics for 2%,
and research hypotheses, choice of design, inferential statistics, and
validity threats showed no improvement. In summary, considering
the artifacts improved the aspects associated with S2, but the rest
of weak spots identified in the papers remain.
4The exceptions are two optimization experiments missing from the artifact‚Äôs code
(E4 [ 16] and E1 [ 47]), and 4 experiments in a paper that are missing the training
code [36].Our inspection also reveled several incomplete artifacts. We
found that papers pointing to a piece of information that is not
accessible in the artifact, either because it is missing from the arti-
fact (e.g., paper [ 50] mentions that the artifact includes ‚Äúall model
information‚Äù, but the model parameters are missing) or because it
requires special permissions or has broken links (e.g., paper [ 41]
contains dropbox links to training data that need permission).
More problematic, however, the inspection of the artifacts re-
vealed many cases where the experiments in the artifact and the
experiments reported in the paper are inconsistent . We found that
most artifacts contained pieces of code representing variations
of the experiments reported in the paper. This in itself is not a
major source of concern as one may conjecture that these varia-
tions corresponded to different configurations explored during the
investigation and development of the proposed techniques, config-
urations that perhaps were not properly labeled or cleaned from
the shared code base. What is concerning, however, are the cases
where the artifact does not have a single experiment variant that
matches the experiment reported in the paper.
When comparing papers and artifact content, we find that 78%
of the papers and 68% of the experiments show inconsistencies.
For example, [ 12] mentions that the loss function used is binary
cross-entropy, while the sigmoidal cross-entropy function is used in
the artifact code. Paper [ 43] mentions the programs used as test sets
for the paper, but the artifact contains a different set of programs.
Paper [ 44] makes a reference to grid search, which is absent in the
artifact. Paper [ 55] mentions that the Adam optimizer is used, but
the code also contains AdamW. Again, our analysis was conserva-
tive and the time dedicated to explore the artifacts was bounded,
so it is reasonable to expect the inconsistencies found are likely
an underestimate of the ones present. We also found artifacts that
were at times inconsistent with themselves. For example, [ 41] pro-
vides generous supplementary information in the form of an online
appendix that contains information related to experiments that are
not reported in the paper, but these show the same inconsistencies
with the code that the paper has regarding model hyperparameters
and training data. Similarly, [ 47] does not mention in the paper
the number of epochs used, and there are two values for it in the
configuration file contained in the artifact.
It is important to emphasize that the analysis of the artifacts
provides further evidence that the limitations we have identified in
these experiments go beyond under-reporting problems. The lack of
specificity in fundamental experimental design and implementation
details reflect deficiencies that can have severe implications for the
findings. We delve into these implications next.
5 IMPLICATIONS
The previous sections characterized the degree of under-specification
in DL experiments to address SE problems when considering pa-
pers and artifacts. We found that the most affected experimental
aspects are the analysis and interpretation of results, the design and
the operationalization of factors and treatments. In this section we
answer RQ3 by deriving the implications of under-specifying those
aspects from the perspective of conclusion, internal, and construct
validity of the experimental findings [27].Pitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA
5.1 Is there a Relationship between the
Response Variable and the Factor(s)?
(Conclusion Validity)
The experiments analyzed include 3 types of analysis to determine
if there is a relation between the factors and the response variable.
We have found that 56% of the reviewed experiments resort to
comparing single data points . This is problematic because it as-
sumes that a single observation on the effect of the treatment will
be a good estimate of the mean effect of that treatment, basically
ignoring fluctuations due to experimental errors (this will be fur-
ther discussed in Section 5.2). For example, [ 54] proposes a DNN
that given as input a set of may links between communicating ob-
jects in two Android applications, outputs the probability that such
links exists. The proposed approach is compared against 3 simpler
DNN architectures as baselines. Based on the comparison of the
values obtained from the test set for the four alternatives, the paper
concludes that the best option is their proposed model (the most
complex one), with response variables values of 0.931 (F1), 0.991
(AUC) and 0.992 (Kruskal‚Äôs Œ≥). However, the results of the second
best option are 0.920, 0.988 and 0.989 respectively. Note that a mere
standard deviation of 0.0155, 0.0045 and 0.0045 in the response vari-
ables (assuming a sample size of 30) will invalidate the conclusion.
Note that often the single point comparison is the result of a prob-
lem with the design of this experiment, which does not control, for
example, for random sources of variation that would have required
multiple runs and hence resulted in multiple values to perform a
statistical comparison that accounts for variability. A variant of this
problem is manifested in [ 11], which proposes a DNN that receives
a code function as input and predicts whether it is vulnerable. The
paper computes the performance of three techniques over multiple
Android applications in terms of precision, recall, F-measure, and
AUC. However, it then resorts to count the number of projects in
which each technique has shown better results and compares those
single values losing an opportunity to perform a more meaningful
comparison.
We find that 31% of the experiments perform a comparison
of means . This is stronger than using single data points, but still
insufficient to guarantee that the differences found in the sample
can be extrapolated to the population the sample represents. For
example, [ 5] proposes a DNN that takes as input color pictures of
source code files to predict whether they contain a fault. It uses
10 test sets corresponding to open source projects to assess the
proposed approach against 4 existing techniques as per their mean F-
measure for the different projects, and concludes that ‚Äúthe proposed
DTL-DP shows significant improvements on the state-of-the-art
in cross-project defect prediction‚Äù. Yet, there is no analysis that
considers the variability observed on the collected measures, even
though the F1 values showed large variability. To better understand
the implications of this oversight we perform a statistical analysis
with the data reported in Table 4 of the paper. Let‚Äôs assume that
the statistical null hypothesis (H0) is: ‚ÄúThere is no difference in F-
measure between the different approaches examined‚Äù, and that the
design is a 1-factor 5-levels experiment (inferred from the design
description). The 1-way repeated measures ANOVA shows that we
can reject the null hypothesis (p <0.01). The follow-up Bonferroni
multiple comparisons test shows that the proposed approach has aTable 5: Extraneous variables and how to deal with them
Characteristics Mechanism
Case Known Measurable Controllable
I No - - Randomization
II Yes No - Case I + Replication
III Yes Yes No Case II + Statistical adjustment
IV Yes Yes Partially Case III + Blocking
V Yes Yes Yes Case IV + Held-constant
Incorporate as factor
better performance than three of the competing ones, but similar
to one of them (DBN-CP, Cohen‚Äôs d=0.3). This example illustrates
that relationships identified through means may not necessarily be
generalizable to the population.
Only 13% of the papers we reviewed identify the potential rela-
tionship through inferential statistics , meaning that the obtained
results can be generalized from the experiment sample to the popu-
lation it represents. For example, [ 33] proposes a DNN that given
as input a code snippet that needs to be logged, suggests which
variables should be logged. Their proposed approach is compared
against 5 baselines for 9 different projects in terms of accuracy,
mean reciprocal rank and mean average precision. Data is analyzed
with a Wilcoxon signed-rank test (considering the 9 scores, one per
project), and Cliff‚Äôs Delta effect size is computed. In all cases, the
improvement of the proposed approach is statistically significant,
with a large effect size.
5.2 Is the Relationship Causal? (Internal
Validity)
In an experiment, the extent to which extraneous variables are
accounted for in the design will define the strength of the causal-
ity link [ 15]. Table 5 shows some of the established recommended
mechanisms to deal with extraneous variables [ 2,15,20,31], which
depend on the nature of the extraneous variable being controlled.
For example, when the variable is known, measurable, and con-
trollable, then we can address it either holding it constant or by
incorporating it an experimental factor (e.g. dataset); and when the
variable is known and measurable but not controllable we can use
blocking to control its impact (e.g. random training/test split). Such
mechanisms naturally apply to DL.
However, DL systems can be particularly challenging in that
they have variables that use sources of randomness to improve the
performance of the model [ 9]. In some cases, these variables are easy
to identify and set (e.g. random weights initialization, batch size),
in others they are easy to identify but difficult to anticipate their
impact (e.g., data shuffling, dropout), and in other cases they are not
even easily identifiable (e.g. more obscure options of core libraries).5
Traditionally, the ML community has focused on classical notions
of variance associated to the dataset variables, mostly ignoring the
other types [ 5]. We now analyze whether such trend also applied
to the DNN4SE experiments we analyzed. Since all experiments we
have studied neither explicitly analyze the sources of randomness
present in the experiment, discussing how they have incorporated
them into the design, nor provide the experimental design and its
5For a detailed analysis see [23, 29, 32].ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA Sira Vegas and Sebastian Elbaum
rationale in the paper, the results presented here are deduced from
the papers.
Our findings confirm that most experiments (62%) acknowledge
theclassical ML random variables related to the dataset. For exam-
ple papers [ 10], [26], and [ 19] use several test sets. While paper [ 13]
uses k-fold-cross-validation. However, this leaves free other sources
of randomness. We also find that some experiments (38%) neglect to
mention any kind of source of randomness , approaching their
experimental design with the assumption that all variables can
be held constant. All these papers train the DL algorithm once,
measuring the response variable(s) for the test set. An example
is [54] mentioned in the previous section and also [45] which pro-
poses a DNN that automatically applies code changes implemented
by developers during pull requests (PRs). None of the papers we
analyzed deal with random variables extrinsic to the dataset .
One example of this deficiency is how all experiments train the
DNN only once for a given combination of hyperparameters. For
example, In the optimization experiment reported in [ 16], Xavier
initialization of parameters is used. However, since the DNN is
trained just once, it is impossible to know if the best configuration
is due to the combination of levels of factors or just a fortuitous
(random) selection of initial weights.
It is important to note that all previous instances deal with known
sources of randomness (cases II-V from Table 5). But there might
beunknown sources of randomness in an experiment (case I). The
ML community has not fully acknowledged the existence of these
variables, but it would be valuable for the experiments designs to
safeguard against them. These variables are typically addressed by
randomly assigning the order in which the experimental runs will
take place. Imagine a situation where caching is in effect for the
non-initial runs. If the runs are not randomly executed, and there
is not enough of them, the first runs could behave differently from
the rest. If we are comparing 2 DNNs and we plan all the runs for
one of them first, this could be affecting the results.
5.3 Does the (Cause) Operationalization
Accurately Represent its Construct?
(Construct Validity)
A construct validity is an assessment of how well researchers trans-
late their ideas into specific factors and treatments, and response
variables [ 31]. Since the experiments we have analyzed operational-
ize well their response variables (76% fully address it), we will focus
on factors and treatments.
The positive news is that only 4% of experiments have a defi-
nition of factors that is incomplete . This is the case for many
hyperparameter optimization experiments, which are often not
fully acknowledged in the papers. For example, in [ 53], the hyper-
parameters fine-tuning optimization experiment is mostly absent.
The paper briefly mentions the range of hyperparameters, and gives
some examples, but the listing is not exhaustive so in the end it is
not known what factors were explored.
On the negative side, 82% of experiments define their factors
properly, but their treatment definitions are incomplete . For
example, the optimization experiment in [ 49] mentions that the
hyperparemeters to be fine-tuned are embedding size, number of
hidden states, batch size, maximum number of iterations, optimizer,learning rate, beam size and lambda. However, it does not specify
the range of values that have been explored. In [ 50], the regulariza-
tion term, the number of iterations, or the topology of the proposed
DNN are not reported. In the experiment in [ 5], the treatments
are defined at the architectural level (a deep adaptation network
is compared against a deep belief network, a LSTM, and a CNN).
However, specific implementations of these architectures are being
compared, overlooking the fact that other non-specified variables
like the model hyperparameters, the DL algorithm or the data rep-
resentation could be the underlying causes for the performance
gain, and not the architecture.
This issue gets magnified as the limitations propagate across
papers. For example, [ 53] has an ambitious agenda to compare the
proposed technique against three other state-of-the-art approaches,
but none of them are easily and reliably available. For one of them,
the stable version in Github is available, but it may be different
from the one in the paper (according to our results from Section 4).
For another, the authors of the paper had to resort to reimplement
the approach following the original paper where it is proposed,
which may implement a different technique. For the third one, the
performance numbers reported in the original paper are used, but
even if the training has ben imitated, those may have suffered from
extraneous variables that are unstated. The limited availability of
high-quality artifacts remains an ongoing challenge.
Finally, 14% of experiments had all factors and treatments
fully operationalized . For example, paper [ 26] specifies that the
factors are: Word2vec vector length (with values 100, 50, 120), learn-
ing rate (with values 0.001, 0.005, 0.01) and epoch size (with values
100, 200, 300).
5.4 Characterization of Experiments and
Implications
We now proceed to analyze the distribution of experiments‚Äô im-
plications to better understand how often they occur and what
combinations are the most common. We utilize the parallel cate-
gories diagram in Figure 3 to facilitate the exposition. The sets of
nodes being considered are associated with the three implication
types analyzed in the previous sections. That is, for relationship
exploration (conclusion validity) we consider comparisons among
single values, means, and inferential; for causality (internal validity)
we consider when none, classical, and other sources of randomness
are controlled; for construct validity we consider when none, fac-
tors, or both treatments and factors are defined. In the figure, the
nodes on the left correspond to comparisons, the ones on the center
to causality, and the ones on the right to constructs issues.
We have not found any experiment that properly addresses all
types of validity threats discussed under the implications. The best
conducted experiments, 11% of the ones examined, perform infer-
ential analysis, control classical sources of randomness, and specify
factors. The majority of experiments (61%), however, have at least
one critical issue (either compare single values, do not control any
single source of randomness, or specify neither factors nor treat-
ments). Even though most experiments specify at least factors, they
perform comparisons based on single values and/or do not control
any variables (45%).Pitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA
Figure 3: Distribution of experiments‚Äô implications.
6 VALIDITY THREATS
We briefly discuss the main limitations arising from the scope,
design, and implementation of our study [17].
Theexternal validity of our study is determined by the eligibil-
ity criteria we chose. We concentrated on the ‚Äòtop‚Äô conferences and
journals (Section 3.1) with the expectation that the findings would
constitute an upper bound for the average quality of experiments
appearing in other venues. The time period covered (2018‚Äì2021)
allowed us to determine the status of recent research in the topic
(2022 papers were not examined as the search was performed in
1/2022, and the analysis was conducted during 2022). Given the
number of relevant papers in 2021, we randomly selected a subset to
be examined. The same quality-driven and cost-control reasoning
applies for us to target the artifacts with an ACM badge (Section 4.1).
Internal validity . Our source of information (Section 3.1) to
identify the papers‚ÄîSCOPUS‚Äîincluded the chosen venues in the
time period covered; this reduced the possibility of omitting po-
tentially relevant studies. The search strategy we followed is also
repeatable. The paper selection process required for each paper to
be examined by one of the two authors; however, joint checks and
discussion of papers that did not fit in existing filtering criteria
reduced the chances of missing potentially relevant papers.
Due to the high data extraction costs from papers, the data col-
lection process (Section 3.2 and Section 4.2) considered all 17 ICSE
papers 2018‚Äì2020, which were jointly examined by both authors to
ensure that the collection strategies and results were aligned, while
the remaining papers were examined by just one author. Again,
joint checks and discussions of studies that did not fit the schema,
introduced new ML elements, or had ambiguous specifications re-
duced possible researcher bias. Finally, doing a critical appraisal of
individual sources of evidence , we note that analyzing papers was
challenging given the diversity of presentation styles, the number
and complexity of the variables to check, and the increasing rich-
ness of the DNN domain. Furthermore, analyzing artifacts was a
consistently arduous re-engineering process. The nature and magni-
tude of these analyses may have introduced errors in our measures.
We attempted to control these internal threats by sharing all the
intermediate results of the study with the community.
Construct validity . The characterization schema ( data items
defined in Section 3.2) was specifically developed for this research.
We created it starting from the steps of the experimental process
and the aspects of the experiments that have to be covered duringeach step. Beginning with the generic definitions given by the ex-
perimental software engineering literature, the authors iteratively
and systematically tailored it to the DL domain. We believe this
provides a reasonable operationalization, one that is transparent
as well for others to assess, refine, and reuse. A simpler assess-
ment would just analyze the validity threats reported by the papers.
However, the description of threats is typically ad-hoc and often
incomplete [ 3,28]. For this reason, we decided to assess the validity
of the experiments from their description (and code artifacts in
some cases). The syntheses of results made in Section 3.3 and Sec-
tion 4.3 allow identifying the validity level of the results reported
in the studies.
7 RECOMMENDATIONS
Failing to address the limitations we identified in the state of the
practice could undermine much of the research devoted to DNN4SE.
Thus, we propose three actionable recommendations that have the
potential to address most of the pressing concerns we discovered.
Rec#1: Perform Multiple DNN Training Runs to Control
for Randomness. Experiments must strive to control the random-
ness of the DNN training process. This process can introduce vari-
ous sources of randomness, and a fundamental one is the random
data selection and shuffling that occurs iteratively to compute the
gradient over the DNN, which means that the resulting values may
change over different runs. Yet, none of the papers reported to make
multiple training runs to control for this intrinsic source of DNN
randomness. This raises questions about whether most results are
caused by just a fortuitous or unfortunate sample selection while
searching for the gradient. There are other sources of randomness
to consider (e.g., the initialization weights, the data splitting) but
based on our findings we argue that simply conducting multiple
runs of the DNN training process would enable the control of a
sizable portion of the randomness we observed. Furthermore, given
the size of the experiments we analyzed and the magnitude of free
computing resources available, we found no compelling argument
for not running an experiment multiple times to account for the
randomness in the DNN training process. This recommendation
could benefit almost all experiments we analyzed.
Rec#2: Compute Statistics Over Multiple Runs and Data
Partitions. Experiments are meant to establish relationships be-
tween factors and response variables. Our analysis, however, found
that 56% of experiments identified a relationship based on single
observations. Some of those studies had multiple observations gath-
ered over multiple units of analysis (i.e., projects, releases, apps);
in such cases it is difficult to justify why select a single data point
to compare treatments. For the rest of the cases, however, there
are plenty of opportunities to collect multiple observations. For
instance, recommendation Rec#1 for conducting multiple training
runs will enable the collection of multiple observations. A second
easily accessible source of observations for most of the papers we
analyzed are the multiple partitions of the dataset used as part of
the training. Given the number of sources of randomness in DNN
training rendering multiple observations, we find no compelling
reason not to require at least a comparison of means from such
observations, and if there are enough observations computer infer-
ential statistics to judge whether the results generalize from theESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA Sira Vegas and Sebastian Elbaum
sample to the population. This recommendations could benefit from
56% to 87% of the experiments we analyzed.
Rec#3: Specify DNN Training Parameters Treatment Space
and Check for Paper Consistency Against Artifact. We have
already described the large DNN configuration space and how dif-
ferent instantiations of it can dramatically impact the performance
of DNN4SE techniques. Yet, most papers fail to provide a specifi-
cation of even some of the basic DNN parameters in that space.
That lack can be mitigated by artifacts with code implementing
the DNN training process. However, re-engineering the experimen-
tal design from such artifacts puts an undue load on the reader
and it is fault prone (we have done 44 of them to attest to that!).
Furthermore, some experiments are often missing in the artifacts
and it is common to find inconsistencies between papers and their
corresponding artifacts. We recommend that papers shall provide a
tabular description of the DNN configuration space explored for
each experiment (as we have done for each experiment analyzed ‚Äì
see Appendix for samples). We also recommend for the adoption of
ML experiment management tools (e.g., jupiter, mlflow, DVC) to
track the DNN experiments, how they evolve, and also to control
how they are shared in the papers and in the artifacts to facili-
tate the detection of inconsistencies. This recommendation could
benefit 86% of the analyzed experiments.
Deploying Mediums. The previous recommendations can be
implemented through different mediums. They can go directly to
authors as part of a call for papers checklists [ 1,18,22], be integrated
as a part of the artifact verification process, be provided to reviewers
to help them judge a paper soundness and verifiability, become part
of broader guidelines such as the recently introduced empirical
processes guidelines [ 24], or serve as instructions for newcomers to
the area. Given the increasing number of DNN4SE papers (the trend
from Table 1 indicates that they are likely to become a dominant
research thrust in the venues we studied for years to come) and
the pitfalls we observed and quantified, pursuing several of these
mediums seems warranted.
Periodic Checks of DNN4SE Paper Experiments. Quantifi-
cations and reflections of where we stand as a community, like we
have completed here, are an essential measurement stick to judge
progress. Given the issues we found and the nature of DNN4SE
that includes rapidly evolving technology, researchers, and method-
ologies, follow up checks seem required to at least determine the
trends over the concerns. To reduce the cost of such checks, the
framework we have defined in our evaluation could be reused and
a smaller sample of the yearly experiments could be analyzed.
8 RELATED WORK
A series of studies have analyzed the quality of SE experiments .
Table 6 shows the number of papers examined, the period covered,
whether all or just a subset of papers are examined, the population
the papers belong to (selected journals and/or conferences, par-
ticular conferences, or all), the sampling performed (Exhaustive
or Random), the type of experiments included (human-oriented,
technology-oriented, or both), and the quality aspect under exam-
ination for each of those studies. These studies differ primarily
from ours in that they focus mostly on a single quality aspect at a
high-level of abstraction that is common across multiple softwareTable 6: Studies analyzing the quality of SE experiments.
Study Size Period Population Sample Type Aspect
[7] 103 93‚Äì02 Selected Js&Cs E Both Statistical power
[16] 103 93‚Äì02 Selected Js&Cs E Both Effect size
[12] 103 93‚Äì02 Selected Js&Cs E Both Theory
[14] 150 02‚Äì12 All R Both Researcher and
publication bias
[25] 51 06‚Äì15 ICSE R Both Correctness of
analysis
[28] 83 15‚Äì19 Selected Js E Human Construct validity
engineering domains, while we performed a deeper specialized
analysis on more quality aspects but focused on a single domain.
To improve the quality of experiments, the SE community has
developed an extensive body of knowledge, some of which has re-
sulted in guidelines for running and reporting experiments. Some
of the guidelines are general enough to apply to any SE experi-
ment [ 13,15,31], and therefore served as a starting point to charac-
terize our experiments (Section 3). However, such general guidelines
do not address the specific challenges associated with experiments
in the DNN4SE domain which is rapidly evolving and acquiring
a critical momentum in the SE community. Other guidelines are
specific . For example, there are domain-specific guidelines for the
analysis of randomized testing algorithms [ 4], for addressing the
diversity of the projects from which to get the dataset to be used in
MSR studies [ 21], and there are guidelines that are specific to con-
ducting human-based experiments [ 24, Experiments] or to perform
benchmarking [ 24, Benchmarking]. Again, although helpful, they
are not addressing specific concerns raised when conducting exper-
iments in the DNN domain. This is the first paper that characterizes
the state of the practice in DNN4SE experimentation.
9 CONCLUSIONS
The SE community is increasingly developing techniques based
on DNNs to solve software engineering problems. Performing ex-
periments to assess such techniques is challenging given DNNs‚Äô
inherent complexity involving many subtle and interdependent
training variables, sources of randomness, and rapid technological
evolution. Our examination of 194 experiments in 55 papers is the
first to quantify these challenges. We find that 87% of experiments
are missing inferential statistics and 56% are missing even basic
descriptive statistics, 4% are not stating the experimental factors
and 82% only do so partially, and 38% do not specify even the ba-
sic elements of the experimental design to control any source of
randomness while the rest only control for the classical sources
of randomness. These findings‚Äô trends only mildly change when
artifacts are provided as part of such experiments, and what is
more concerning is that we find that most artifacts are not fully
consistent with their corresponding paper.
These findings are problematic because they imply that: 1) there
is weak support to determine that there is a true relationship be-
tween independent and dependent variables that did not take place
by happenstance, 2) there is limited control over the space of DNN
relevant variables, which can render a relationship between depen-
dent variables and treatments that may not be causal but rather
correlational, and 3) there is a lack of specificity in terms of what
are the DNN variables and their values utilized in the experimentsPitfalls in Experiments with DNN4SE: An Analysis of the State of the Practice ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA
to define the treatments being applied, which makes it unclear
whether the techniques designed are the ones being assessed. We
have proposed a series of actionable recommendations addressing
the most critical findings we uncovered and will push forward to
have them become a part of our community practices.
10 DATA AVAILABILITY
The data of our analyses is currently in an anonymous repository.
11 ACKNOWLEDGEMENTS
This work was supported in part by NSF grant #1900676. The au-
thors are thankful to the reviewers for their feedback and to the
authors of the papers and artifacts we analyzed.
REFERENCES
[1]AAAI 2022. The 37th AAAI Conference on Artificial Intelligence Reproducibility
Checklist . accessed August 26, 2022.
[2]Naomi Altman and Martin Krzywinski. 2021. Sources of variation. Nature Methods
12 (2021), 5‚Äì6.
[3]Apostolos Ampatzoglou, Stamatia Bibi, Paris Avgeriou, Marijn Verbeek, and
Alexander Chatzigeorgiou. 2019. Identifying, categorizing and mitigating threats
to validity in software engineering secondary studies. Information and Software
Technology 106 (2019), 201‚Äì230.
[4]Andrea Arcuri and Lionel Briand. 2011. A Practical Guide for Using Statistical
Tests to Assess Randomized Algorithms in Software Engineering. In Proceedings
of the 33rd International Conference on Software Engineering, ICSE‚Äô11, May . 1‚Äì10.
[5]Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichy-
poruk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan,
Vikram Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Tal Arbel, Chris Pal,
Gael Varoquaux, and Pascal Vincent. 2021. Accounting for Variance in Machine
Learning Benchmarks. In Proceedings of Machine Learning and Systems . 747‚Äì769.
[6]Prem Devanbu, Matthew Dwyer, Sebastian Elbaum, Michael Lowry, Kevin Moran,
Denys Poshyvanyk, Baishakhi Ray, Rishabh Singh, and Xiangyu Zhang. 2020.
Deep Learning & Software Engineering: State of Research and Future Directions.
arXiv:2009.08525 [cs.SE]
[7]Tore Dyb√•, Vigdis By Kampenes, and Dag I. K. Sj√∏berg. 2006. A systematic review
of statistical power in software engineering experiments. Information and Software
Technology 48, 8 (2006), 745‚Äì755.
[8]Association for Computing Machinery. 2020. Artifact Review and Badging . https:
//www.acm.org/publications/policies/artifact-review-and-badging-current
[9]Claudio Gallicchio, Jos√© Mart√≠n-Guerrero, Alessio Micheli, and Emilio Olivas.
2017. Randomized Machine Learning Approaches: Recent Developments and
Challenges. In Proceedings of the 25th European Symposium on Artificial Neural
Networks (ESANN) .
[10] Omar S Gomez, Natalia Juristo, and Sira Vegas. 2014. Understanding replication
of experiments in software engineering: A classification. Information and Software
Technology 56, 8 (2014), 1033‚Äì1048.
[11] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . The
MIT Press.
[12] Jo Erskine Hannay, Dag I. K. Sj√∏berg, and Tore Dyb√•. 2007. A Systematic Review
of Theory Use in Software Engineering Experiments. IEEE Trans. Software Eng.
33, 2 (2007), 87‚Äì107.
[13] Andreas Jedlitschka, Marcus Ciolkowski, and Dietmar Pfahl. 2008. Reporting
Experiments in Software Engineering. In Guide to Advanced Empirical Software
Engineering , Forrest Shull, Janice Singer, and Dag I.K. Sj√∏berg (Eds.). Springer,
Chapter 8, 201‚Äì228.
[14] Magne J√∏rgensen, Tore Dyb√•, Knut Liest√∏l, and Dag I. K. Sj√∏berg. 2016. Incorrect
results in software engineering experiments: How to improve research practices.
J. Syst. Softw. 116 (2016), 133‚Äì145.
[15] Natalia Juristo and Ana M Moreno. 2011. Basics of software engineering experi-
mentation . Springer Science & Business Media.
[16] Vigdis By Kampenes, Tore Dyb√•, Jo Erskine Hannay, and Dag I. K. Sj√∏berg. 2007.
A systematic review of effect size in software engineering experiments. Inf. Softw.
Technol. 49, 11-12 (2007), 1073‚Äì1086.
[17] B. Kitchenham, L. Madeyski, and D. Budgen. Early Access. SEGRESS: Software
Engineering Guidelines for REporting Secondary Studies. IEEE Transactions on
Software Engineering (Early Access). https://doi.org/10.1109/TSE.2022.3174092
[18] machine 2020. The Machine Learning Reproducibility Checklist v2.0 . accessed
August 26, 2022.
[19] Tom Mitchell. 2019. Machine Learning . McGraw-Hill Education.
[20] Douglas C Montgomery. 2019. Design and Analysis of Experiments . John Wiley
& Sons Inc.[21] Meiyappan Nagappan, Thomas Zimmermann, and Christian Bird. 2013. Diversity
in software engineering research. In 9th joint meeting on foundations of software
engineering . 466‚Äì476.
[22] neurips 2022. The 36th Conference on Neural Information Processing Systems
PaperChecklist Guidelines . accessed August 26, 2022.
[23] Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan
Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems
and Opportunities in Training Deep Learning Software Systems: An Analysis
of Variance. In Proceedings of the 35th IEEE/ACM International Conference on
Automated Software Engineering . 771‚Äì783.
[24] Paul Ralph, Nauman bin Ali, Sebastian Baltes, Domenico Bianculli, Jessica Diaz,
Yvonne Dittrich, Neil Ernst, Michael Felderer, Robert Feldt, Antonio Filieri, Breno
Bernard Nicolau de Fran√ßa, Carlo Alberto Furia, Greg Gay, Nicolas Gold, Daniel
Graziotin, Pinjia He, Rashina Hoda, Natalia Juristo, Barbara Kitchenham, Valentina
Lenarduzzi, Jorge Mart√≠nez, Jorge Melegati, Daniel Mendez, Tim Menzies, Jefferson
Molleri, Dietmar Pfahl, Romain Robbes, Daniel Russo, Nyyti Saarim√§ki, Feder-
ica Sarro, Davide Taibi, Janet Siegmund, Diomidis Spinellis, Miroslaw Staron,
Klaas Stol, Margaret-Anne Storey, Damian Tamburri, Marco Torchiano, Christoph
Treude, Burak Turhan, Xiaofeng Wang, and Sira Vegas. 2021. Empirical Standards
for Software Engineering Research. arXiv:2010.03525v2 [cs.SE]
[25] Rolando Reyes, √ìscar Dieste, Efra√≠n R. Fonseca, and Natalia Juristo. 2018. Statis-
tical errors in software engineering experiments: a preliminary literature review.
In2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE) .
1195‚Äì1206.
[26] Arthur Samuel. 1959. Some studies in machine learning using the game of
checkers. IBM Journal of research and development 3, 3 (1959), 210‚Äì229.
[27] William R. Shadish, Thomas D. Cook, and Donald T. Campbell. 2002. Experimental
and Quasi-Experimental Designs for Generalized Causal Inference . Wadsworth,
Cengage Learning.
[28] Dag I.K. Sjoberg and Gunnar R. Bergersen. 2022. Construct Validity in Software
Engineering. IEEE Transactions on Software Engineering (2022), Early Access.
https://doi.org/10.1109/TSE.2022.3176725
[29] Cecilia Summers and Michael J. Dinneen. 2021. Nondeterminism and Instability
in Neural Network Optimization. In Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event . 9913‚Äì9922.
[30] Christopher S. Timperley, Lauren Herckis, Claire Le Goues, and Michael Hilton.
2021. Understanding and Improving Artifact Sharing in Software Engineering
Research. Empirical Software Engineering 26, 4 (2021).
[31] Claes Wohlin, Per Runeson, Martin H√∂st, Magnus C Ohlsson, Bj√∂rn Regnell, and
Anders Wessl√©n. 2012. Experimentation in software engineering . Springer Science
& Business Media.
[32] Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and Sara Hooker. 2022.
Randomness In Neural Network Training: Characterizing The Impact of Tooling.
InProceedings of the 5th Conference on Machine Learning and Systems .
ANALYZED PAPERS
[AP1] Kaibo Cao, Chunyang Chen, Sebastian Baltes, Christoph Treude, and Xiang
Chen. 2021. Automated Query Reformulation for Efficient Search Based on
Query Logs From Stack Overflow. In Proceedings of the 43rd ICSE .
[AP2] Preetha Chatterjee, Kostadin Damevski, and Lori Pollock. 2021. Automatic Ex-
traction of Opinion-Based Q&A from Online Developer Chats. In Proceedings
of the 43rd ICSE .
[AP3] Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu.
2018. From UI Design Image to GUI Skeleton: A Neural Machine Translator
to Bootstrap Mobile GUI Implementation. In Proceedings of the 40th ICSE .
[AP4] Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu,
Guoqiang Li, and Jinshui Wang. 2020. Unblind Your Apps: Predicting Natural-
Language Labels for Mobile GUI Components by Deep Learning. In Proceedings
of the 42nd ICSE .
[AP5] Jinyin Chen, Keke Hu, Yue Yu, Zhuangzhi Chen, Qi Xuan, Yi Liu, and Vladimir
Filkov. 2020. Software Visualization and Deep Transfer Learning for Effective
Software Defect Prediction. In Proceedings of the 42nd ICSE .
[AP6] Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu,
Liming Zhu, and Guoqiang Li. 2020. Object Detection for Graphical User
Interface: Old Fashioned or Deep Learning or a Combination?. In Proceedings
of the 28th ESEC/FSE .
[AP7] Yanju Chen, Ruben Martins, and Yu Feng. 2019. Maximal Multi-Layer Specifi-
cation Synthesis. In Proceedings of the 27th ESEC/FSE .
[AP8] Yujun Chen, Xian Yang, Hang Dong, Xiaoting He, Hongyu Zhang, Qingwei
Lin, Junjie Chen, Pu Zhao, Yu Kang, Feng Gao, Zhangwei Xu, and Dongmei
Zhang. 2020. Identifying Linked Incidents in Large-Scale Online Service
Systems. In Proceedings of the 28th ESEC/FSE .
[AP9] Zhenpeng Chen, Yanbin Cao, Xuan Lu, Qiaozhu Mei, and Xuanzhe Liu. 2019.
SEntiMoji: An Emoji-Powered Learning Approach for Sentiment Analysis in
Software Engineering. In Proceedings of the 27th ESEC/FSE .ESEC/FSE 2023, 3 - 9 December, 2023, San Francisco, USA Sira Vegas and Sebastian Elbaum
[AP10] Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya
Ghose, and Tim Menzies. 2019. A Deep Learning Model for Estimating Story
Points. IEEE Transactions on Software Engineering 45, 7 (2019), 637‚Äì656.
[AP11] Hoa Khanh Dam, Truyen Tran, Trang Pham, Shien Wee Ng, John Grundy, and
Aditya Ghose. 2021. Automatic Feature Learning for Predicting Vulnerable
Software Components. IEEE Transactions on Software Engineering 47, 1 (2021),
67‚Äì85.
[AP12] Davide Fucci, Alireza Mollaalizadehbahnemiri, and Walid Maalej. 2019. On Us-
ing Machine Learning to Identify Knowledge in API Reference Documentation.
InProceedings of the 27th ESEC/FSE .
[AP13] Jian Gao, Yu Jiang, Zhe Liu, Xin Yang, Cong Wang, Xun Jiao, Zijiang Yang, and
Jiaguang Sun. 2021. Semantic Learning and Emulation Based Cross-Platform
Binary Vulnerability Seeker. IEEE Transactions on Software Engineering 47, 11
(2021), 2575‚Äì2589.
[AP14] Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann.
2021. Automating the Removal of Obsolete TODO Comments. In Proceedings
of the 29th ESEC/FSE .
[AP15] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep Code Search. In
Proceedings of the 40th ICSE .
[AP16] Huong Ha and Hongyu Zhang. 2019. DeepPerf: Performance Prediction for
Configurable Software with Deep Sparse Neural Network. In Proceedings of
the 41st ICSE .
[AP17] Marlo Haering, Christoph Stanik, and Walid Maalej. 2021. Automatically
Matching Bug Reports With Related App Reviews. In Proceedings of the 43rd
ICSE .
[AP18] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, and Miltiadis Allamanis.
2018. Deep Learning Type Inference. In Proceedings of the 26th ESEC/FSE .
[AP19] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec:
Distributed Representations of Code Changes. In Proceedings of the 42nd ICSE .
[AP20] Qiao Huang, Xin Xia, David Lo, and Gail C. Murphy. 2020. Automating
Intention Mining. IEEE Transactions on Software Engineering 46, 10 (2020),
1098‚Äì1119.
[AP21] Reyhaneh Jabbarvand, Forough Mehralian, and Sam Malek. 2020. Automated
Construction of Energy Test Oracles for Android. In Proceedings of the 28th
ESEC/FSE .
[AP22] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural
Machine Translation for Automatic Program Repair. In Proceedings of the 43rd
ICSE .
[AP23] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code
Prediction by Feeding Trees to Transformers. In Proceedings of the 43rd ICSE .
[AP24] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A Neural Model
for Generating Natural Language Summaries of Program Subroutines. In
Proceedings of the 41st ICSE .
[AP25] Jaeseong Lee, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. 2020. On the
Naturalness of Hardware Descriptions. In Proceedings of the 28th ESEC/FSE .
[AP26] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: Context-Based Code
Transformation Learning for Automated Program Repair. In Proceedings of
the 42nd ICSE .
[AP27] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. A Context-Based Auto-
mated Approach for Method Name Consistency Checking and Suggestion. In
Proceedings of the 43rd ICSE .
[AP28] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Fault Localization with Code
Coverage Representation Learning. In Proceedings of the 43rd ICSE .
[AP29] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Vulnerability Detection with
Fine-Grained Interpretations. In Proceedings of the 29th ESEC/FSE .
[AP30] Hui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang. 2021.
Deep Learning Based Code Smell Detection. IEEE Transactions on Software
Engineering 47, 9 (2021), 1811‚Äì1837.
[AP31] Kui Liu, Dongsun Kim, Tegawend√© F. Bissyand√©, Taeyoung Kim, Kisub Kim,
Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning to Spot and
Refactor Inconsistent Method Names. In Proceedings of the 41st ICSE .
[AP32] Kui Liu, Dongsun Kim, Tegawend√© F. Bissyand√©, Shin Yoo, and Yves Le Traon.
2021. Mining Fix Patterns for FindBugs Violations. IEEE Transactions on
Software Engineering 47, 1 (2021), 165‚Äì188.
[AP33] Zhongxin Liu, Xin Xia, David Lo, Zhenchang Xing, Ahmed E. Hassan, and
Shanping Li. 2021. Which Variables Should I Log? IEEE Transactions on
Software Engineering 47, 9 (2021), 2012‚Äì2031.
[AP34] Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu Zhang,
and Lingming Zhang. 2021. Boosting Coverage-Based Fault Localization via
Graph-Based Representation Learning. In Proceedings of the 29th ESEC/FSE .
[AP35] Suyu Ma, Zhenchang Xing, Chunyang Chen, Cheng Chen, Lizhen Qu, and
Guoqiang Li. 2021. Easy-to-Deploy API Extraction by Multi-Level Feature
Embedding and Transfer Learning. IEEE Transactions on Software Engineering
47, 10 (2021), 2296‚Äì2311.
[AP36] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: Inferring
JavaScript Function Types from Natural Language Information. In Proceedings
of the 41st ICSE .[AP37] Forough Mehralian, Navid Salehnamadi, and Sam Malek. 2021. Data-Driven
Accessibility Repair Revisited: On the Effectiveness of Generating Labels for
Icons in Android Apps. In Proceedings of the 29th ESEC/FSE .
[AP38] Facundo Molina, Renzo Degiovanni, Pablo Ponzio, Germ√°n Regis, Nazareno
Aguirre, and Marcelo Frias. 2019. Training Binary Classifiers as Data Structure
Invariants. In Proceedings of the 41st ICSE .
[AP39] Kevin Moran, Carlos Bernal-C√°rdenas, Michael Curcio, Richard Bonett, and
Denys Poshyvanyk. 2020. Machine Learning-Based Prototyping of Graphical
User Interfaces for Mobile Apps. IEEE Transactions on Software Engineering
46, 2 (2020), 196‚Äì221.
[AP40] Thanh Nguyen, Ngoc Tran, Hung Phan, Trong Nguyen, Linh Truong,
Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2018. Com-
plementing Global and Local Contexts in Representing API Descriptions to
Improve API Retrieval Tasks. In Proceedings of the 26th ESEC/FSE .
[AP41] Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen Yao,
David Williams-King, Vikas Ummadisetty, Junfeng Yang, Baishakhi Ray, and
Suman Jana. 2021. StateFormer: Fine-Grained Type Recovery from Binaries
Using Generative State Modeling. In Proceedings of the 29th ESEC/FSE .
[AP42] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. 2020. Type-
Writer: Neural Type Prediction with Search-Based Validation. In Proceedings
of the 28th ESEC/FSE .
[AP43] Dongdong She, Rahul Krishna, Lu Yan, Suman Jana, and Baishakhi Ray. 2020.
MTFuzz: Fuzzing with a Multi-Task Neural Network. In Proceedings of the 28th
ESEC/FSE .
[AP44] Lin Shi, Mingzhe Xing, Mingyang Li, Yawen Wang, Shoubin Li, and Qing Wang.
2020. Detection of Hidden Feature Requests from Massive Chat Messages via
Deep Siamese Network. In Proceedings of the 42nd ICSE .
[AP45] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and
Denys Poshyvanyk. 2019. On Learning Meaningful Code Changes via Neural
Machine Translation. In Proceedings of the 41st ICSE .
[AP46] Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020. Deep Semantic
Feature Learning for Software Defect Prediction. IEEE Transactions on Software
Engineering 46, 12 (2020), 1267‚Äì1293.
[AP47] Shangwen Wang, Ming Wen, Bo Lin, and Xiaoguang Mao. 2021. Lightweight
global and local contexts guided method name recommendation with prior
knowledge. In Proceedings of the 29th ESEC/FSE .
[AP48] Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys
Poshyvanyk. 2020. On Learning Meaningful Assert Statements for Unit Test
Cases. In Proceedings of the 42nd ICSE .
[AP49] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-Based Neural Source Code Summarization. In Proceedings of the 42nd
ICSE .
[AP50] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and
Xudong Liu. 2019. A Novel Neural Source Code Representation Based on
Abstract Syntax Tree. In Proceedings of the 41st ICSE .
[AP51] Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,
Chunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, Junjie Chen, Xiaoting He,
Randolph Yao, Jian-Guang Lou, Murali Chintalapati, Furao Shen, and Dongmei
Zhang. 2019. Robust Log-Based Anomaly Detection on Unstable Log Data. In
Proceedings of the 27th ESEC/FSE .
[AP52] Dehai Zhao, Zhenchang Xing, Chunyang Chen, Xin Xia, and Guoqiang Li. 2019.
ActionNet: Vision-Based Workflow Action Recognition from Programming
Screencasts. In Proceedings of the 41st ICSE .
[AP53] Gang Zhao and Jeff Huang. 2018. DeepSim: Deep Learning Code Functional
Similarity. In Proceedings of the 26th ESEC/FSE .
[AP54] Jinman Zhao, Aws Albarghouthi, Vaibhav Rastogi, Somesh Jha, and Damien
Octeau. 2018. Neural-Augmented Static Analysis of Android Communication.
InProceedings of the 26th ESEC/FSE .
[AP55] Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong,
and Lu Zhang. 2021. A Syntax-Guided Edit Decoder for Neural Program Repair.
InProceedings of the 29th ESEC/FSE .