NatGen: GenerativePre-trainingby‚ÄúNaturalizing‚Äù Source Code
SaikatChakraborty
Columbia University
New York, NY, USA
saikatc@cs.columbia.eduToufiqueAhmed
Universityof California,Davis
Davis, CA,USA
tfahmed@ucdavis.eduYangruiboDing
Columbia University
New York, NY, USA
yrbding@cs.columbia.edu
PremkumarT.Devanbu
Universityof California,Davis
Davis, CA,USA
ptdevanbu@ucdavis.eduBaishakhi Ray
Columbia University
New York, NY, USA
rayb@cs.columbia.edu
ABSTRACT
Pre-trained Generative Language models ( e.g.,PLBART, CodeT5,
SPT-Code)forsourcecodeyieldedstrongresultsonseveraltasks
in the past few years, including code generation and translation.
Thesemodelshaveadoptedvaryingpre-trainingobjectivestolearn
statistics of code construction from very large-scale corpora in a
self-supervisedfashion;thesuccessofpre-trainedmodelslargely
hinges on these pre-training objectives. This paper proposes a new
pre-training objective, ≈ÇNaturalizing≈æ of source code, exploiting
code‚Äôsbimodal,dual-channel(formal&naturalchannels)nature.
Unlike natural language, code‚Äôs bimodal, dual-channel nature al-
lows us to generate semantically equivalent code at scale. We in-
troduce six classes of semantic preserving transformations to in-
troduce un-natural forms of code, and then force our model to
produce more natural original programs written by developers.
Learning to generate equivalent, but more natural code, at scale,
over large corpora of open-source code, without explicit manual
supervision, helps the model learn to both ingest & generate code.
We fine-tuneour modelinthree generative Software Engineering
tasks: code generation, code translation, and code refinement with
limited human-curated labeled data and achieve state-of-the-art
performance rivaling CodeT5. We show that ourpre-trained model
isespeciallycompetitiveatzero-shotandfew-shotlearning,and
betterat learningcode properties(e.g.,syntax, data flow).
CCS CONCEPTS
¬∑Softwareanditsengineering ‚ÜíLanguagefeatures ;¬∑Comput-
ing methodologies ‚ÜíKnowledge representation and reason-
ing.
KEYWORDS
Source Code Pre-training, Neural Network, Source Code Trans-
former,Semantic PreservingTransformation
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11...$15.00
https://doi.org/10.1145/3540250.3549162ACMReference Format:
SaikatChakraborty, ToufiqueAhmed,YangruiboDing, PremkumarT. De-
vanbu, and Baishakhi Ray. 2022. NatGen: Generative Pre-training by ≈ÇNat-
uralizing≈æ Source Code. In Proceedings of the 30th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Soft-
wareEngineering(ESEC/FSE‚Äô22),November14≈õ18,2022,Singapore,Singapore.
ACM,NewYork,NY,USA, 13pages.https://doi.org/10.1145/3540250.3549162
1 INTRODUCTION
Statistical models of the ≈Çnaturalness" of code [ 33] have proven
useful for a range of Software Engineering tasks [ 7,50], includ-
ing code generation [ 10], repair [ 15,61], summarization [ 40], re-
trieval[47],andclonedetection[ 20,66].Theearlierworkinthis
area trained models directly on tasks, including the early work
on type recovery [ 8,31], de-obfuscation [ 55,62], repair [ 30], and
summarization [ 2,35]. Training on-task requires a lot of labeled
data. While labeled data is abundant for tasks like code completion
(where the corpus inherently provides supervision), other tasks
like code generation, translation, summarization, repair, etc., re-
quirewell-curated,high-qualitydata.Simplygrabbingdatafrom
Github might yield poor-quality [ 27], highly-duplicated data [ 5].
Withincreasingmodelcapacity(hundredsofmillions,evenbillions
ofparameters,are pretty common;largermodels tend to perform
better [17,64]), this unacceptable disparity between vast model
capacityandthelimitedavailabilityofwell-curated,high-quality,
labeleddata has increasedandwilllikely worsen.
Thisshortageofhigh-qualitylabeleddataforon-tasktrainingis
notuniquetoSoftwareEngineering(SE),althoughitiscomplicated
herebytheincreased,specializedskillrequiredforlabelingSEdata.
Toaddresstheissueoftraininglargemodelsinthepresenceofdata
scarcity,suchmodelsareoftenpre-trainedonsomegenerictasks,
whichrelatetoactualdownstreamtasks.Forexample,considertwo
SE tasks: code generationandcode translation.Both tasksrequire
MLmodels tolearnhowtogeneratenatural,syntactically,andse-
mantically correct code. This commonality across tasks motivates
aquestforbetterpre-trainedmodels,usingaself-(orun-)super-
vised task which transfers well to other downstream tasks. Such
pre-trainedmodelscanalsolearnagenericrepresentationofthe
inputdata,which,inturn,transfers to diversedownstream tasks.
A popular approach for dealing with this problem involves
derivativesofBERTstylemodels, e.g.,CodeBERT[ 22],GraphCode-
BERT[28],etc.Thesemodelsaregoodatcapturinggenericcode
representations.Forcodegenerationtasks,GPT-3orBART-style
models(e.g.,Codex,CodeT5,PLBART,SPTCode,etc.[ 3,17,46,64])
18
ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore S.Chakraborty, T. Ahmed, Y. Ding, P. Devanbu, B.Ray
arepopular.Theimportantinsighthereisthatindependentoffi-
naltasks,when veryhighcapacitymodelsaretrainedwithhuge
code corpora to learn simple, self-supervised, ≈Çbusy work≈æ, they
stilllearn general syntactic and semantic constraints of writing code .
Different approaches adopt different techniques to train the model
to write code. For instance, GPT-style models ( e.g.,Codex) learn to
generate code sequentially, mimicking the left-to-right language
model. CodeT5masks out some tokens andasks the model to gen-
erateonlythosemaskedtokens.Ontheotherhand,PLBARTand
SPT-Code present the model with erroneous code (with deleted
or masked tokens) and ask the model to generate the corrected,
completecode.Themodels‚Äôabilitytogeneratecodedependsmainly
onthe pre-training objective that the modelisoptimizedfor.
We propose a novel pre-training task: we ask the model to ≈Çnat-
uralize"code, i.e.,take ≈Çweird",syntheticcodeasinputand output
semantic equivalent, ≈Çnatural" code that a human developer would
havewritten.Thisisaverydemandingpre-trainingtask√êthemodel
has to learn both code naturalness andcode semantics. We were
inspiredby notingtheworkofhumanEditors(of books,journals,
newspapers): they digest imperfectly written but mostly correct
text, understand the intent, and then produce more perfect text
withprettymuchthesamemeaning.Editingis hard:askilledEditor
has to have very high levels of language comprehension, to under-
stand given, potentially badly-written text, and then deploy very
high-level writing skills to generate well-formed text. If Editing
could be used as an at-scale pre-training task, the learned model
wouldpresumablyhaveexcellentlanguagecomprehensionandalso
generate excellent text. However, it‚Äôs not obvious how to generate
at-scale training data for this ≈ÇEditing"task,say,for English.
a. Natural Code
Scanner sc =newScanner (...);
while(sc.hasNext ()) {
String ln =sc.next();
...
}
...b. Un-naturalcode
Scanner sc =newScanner (...);
for( ;sc.hasNext ();) {
String ln =sc.next();
...
}
...
Figure 1: Example of a natural code fragment written by
developersandits‚Äòun-naturally‚Äôtransformedcounterpart.
If theinitialization andupdatepart of the forloop were
to left empty,developers would writethe whileloop.
Butourconcernhereiscode,notnaturallanguage.Westartwith
the argument that, because of thebimodal, dual-channel nature of
code[12],itisindeedpossibletogenerateat-scaletrainingdatafor
theEditingtask(a.k.a.refactoringinSoftwareEngineeringtermi-
nology).Codehasaformalchannel,withwell-definedsemantics;
becauseofthis,it‚Äôspossibletotransformcodeintoendlessforms,
allmeaning-equivalent . Essentially, we can deploy a set of meaning
preserving transformations to rewriteexisting code from widely-
usedGitHubprojects(whichpresumablyhavegood-qualitycode
thathaspassedhumancodereview).Theserewrites,( e.g.,Figure1),
preserve meaning but will make the code into an artificial, often
unnatural form1.
1Studies,withhuman-subjects[ 13,14]suggestthathumansfindsuchrewrittenbut
semanticallyidenticalformsharderto read and understand.Nevertheless,wenowhave amatchedpairoftwosemantically
equivalent forms of code: a ≈Çde-naturalized" form and the original
≈Çnatural"form.Furthermore,wecanproducethesepairsat-scale,
and then pre-train on a code ≈ÇNaturalization" task. By analogy
with human Editors as described above, such pre-training forces
themodeltolearntwohardthings:1)capturethemeaningofthe
input code,and 2) generate an output that more closely resembles
human-written code. We hypothesize that the resulting model will
bothlearnbettermeaningrepresentations, andalsogeneratebetter
code.
Tothisend,wepre-trainedour NatGenmodel,using≈ÇCodeNat-
uralizing≈æ task. NatGenis based on a transformer-based sequence-
to-sequence model, and learns to ≈Çnaturalize" artificially generated
≈Çde-naturalized"codebackintotheformoriginallywrittenbyde-
velopers. We emphasize thatNatGenlearns to generate the whole
code;thislearnedskilltransferstodownstreamfine-tuningtasks
that require code generation. We show that our pre-training objec-
tive helps model generate more natural code (complete code, with
high syntactic and semantic similarity with the original human-
written code). With proper fine-tuning, NatGen achieves state-
of-the-artperformanceinvariousdownstreamfine-tuningtasks,
includingcodegeneration,codetranslation,bugfix,thatdemand
codegeneration.Wealsoshowthat NatGen isspeciallyeffective
when labelleddata isscarce.
We summarize our main contributions.
(1)Weintroducetheideaof"Codenaturalization"asapre-training
task.
(2)UsingcodefromGithub,andcustomtooling,wehavegenerated
and released a large dataset for pre-training models on the
Naturalization task.
(3)WehavebuiltandreleasedalargeSequence-to-Sequencemodel
pre-trainedonNaturalization.
(4)We show that (when appropriately fine-tuned) NatGenoutper-
forms SOTA onseveral settings.
We publish our source code and data download script for pre-
trainingNatGen anonymously in https://github.com/saikat107/
NatGen.Wealsosharethepre-trainedmodelin https://bit.ly/natgen-
pre-trained-models and all the finetuned model in https://bit.ly/
natgen-fine-tuned-models .
2BACKGROUND & PROBLEM FORMULATION
This section presents the relevant technical background that leads
to this work andan overviewof the main research questions.
2.1 The Dual Channels ofCode
Humanscanreadandwritebothnaturallanguagesandcode.How-
ever,unlikenaturallanguage,sourcecodeinvolves twochannelsof
information: formal & natural [ 14]. The formal channel, unique to
code,affordsprecise,formalsemantics;interpreters,compilers,etc.,
use this channel. On the other hand, the natural channel (perhaps
moreprobabilisticandnoisy)reliesonvariablenames,comments,
etc., and is commonly used by humans for code comprehension
and communication [ 13,14]. The formal channel‚Äôs precision en-
ables semantic preserving code transformation, which supports
staticanalysis,optimization,obfuscation, etc.Forinstance,major
refactoringofasourcecodemaydrasticallychangethesyntactic
19NatGen: Generative Pre-trainingby ≈ÇNaturalizing≈æSource Code ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
structure while preserving the semantics [ 20,23]. However, not all
thesemanticallyequivalentcodeis≈Çnatural"[ 32]√êtheusualway
developerswritecodeandthus,amenabletostatisticalmodels[ 32].
In fact, deviation from such ≈Çnaturalness" may lead to unintended
bugs[54],andincreasedifficultyofhumancomprehension[ 13,14].
Weleveragethenatural/formaldualityforourpre-trainingob-
jective in this work. We keep the formal channel constant (not
changing the meaning) for a given code and modify the syntax
by creating ≈Çunnatural≈æ code. Then we train the model to take
the ≈Çunnatural" code as input and do what a human Editor does
withnaturallanguagetext:understandthe≈Çunnatural"codeand
generate more natural code that a developer would write. Thus,
the model simultaneously learns to both comprehend code, and
generate≈Çnatural≈æ code.
2.2 ≈ÇNaturalizing" vs.De-noising
Naturalizing pre-training essentially follows in the tradition of
denoising pre-training , although,arguably, theformer ismore sub-
tle and challenging. Denoising pre-training [ 3,38,39] is a well-
establishedpre-trainingstrategyforencoder-decodermodels:the
encoder is presented with a noised-up input, and the decoder is
asked to generate the original, noise-free input. By training the
modeltoidentify&remove≈Çnoise≈æinanoisyoutput,(intheory)
one teaches it to reason about andcorrectly generate text.Exactly
what a model learns largely depends on the noise types. For in-
stance,PLBART[ 3]usessyntacticnoise2(i.e.,tokenmasking,token
deletion, etc.). Thus, denoising pre-training enables PLBART to
learn both about the syntax of input source code, andlearn to gen-
eratesyntacticallycorrectcode.Naturalizingpre-training,onthe
otherhand,beginswithsyntacticallycorrectbutartificially-created
unnatural source code and forces the model to generate correct
semantically equivalent natural code that is just what a human
originallywrote.Suchpre-trainingrequiresmoresubtlechangesto
thecode.Wehypothesizethatthisprovidesamoredemandingpre-
training setting, which will lead to better on-task code generation
performance.
2.3 Research Questions
Ourhypothesisisthatour naturalizing task(seeSection 3.1)endows
ourpre-trainedmodelwiththeabilitytogeneratesyntacticallyand
semantically correct, andnaturalcode.This leads to several RQs.
RQ1. Does ≈ÇNaturalization≈æ help to improve code genera-
tion?
In contrast to existing de-noising techniques [ 3] that help the
model learn lexical & syntactic structure, the naturalizing task,
which is arguably more demanding than de-noising, forces Nat-
Gengenerating better code with higher syntactic and semantic
correctness.
The pre-training data we use (in NatGen) challenges the model
tonaturalizecodethatwas≈Çde-naturalized"inseveralways,such
as dead-code inserted, variable renamed, etc. We investigate the
relative performance underdifferentnaturalizationchallenges.
2Noisethat breaks the syntax structureof codeRQ2.Howdodifferentcomponentsin NatGen contribute
to codegeneration?
We evaluate the performance under different challenges on a
held-out validation dataset.This datasetissampledwiththe same
distributionofde-naturalizingtransformsasthetrainingdataset
(Dùë°); on this set, the model to reconstruct the original code. Our
exploratory investigation reveals that Variable Renaming is the
hardest transformation to undo: the model reconstructs original
codewithonly 40%accuracy.DeadCode,ontheotherhand,isthe
easiestwith 99%accuracy.
Wefurtherinvestigate NatGen‚Äôsperformancefordownstream
sourcecode generationtasks.
RQ3. How effective is NatGen when fine-tuned for differ-
entgenerative tasksinsource code?
We fine-tune the pre-trained NatGen on task-specific train-
ing dataset for a certain time budget and evaluate the fine-tuned
model on the benchmark testing dataset for corresponding task.
Thesetasksincludesourcecode(java)generationfromtext,code
translation(from JavatoC#andC# toJava), andBugfixing. After
fine-tuning, NatGenachievesthestate-of-the-artperformancein
allthesetasks.Inaddition,wealsodiscoverthat,codegenerated
byNatGen aresyntacticallyandsemanticallymoreclosertothe
expectedcode.
We observe that training a model for a complex task requires
sufficient labeled data. However, for most software engineering
tasks, finding labeled data is a significant challenge [ 4]. We investi-
gate potential scenario where size of the training data is extremely
small.
RQ4.Howwelldoes NatGen ‚Äôspre-traininghelpintasks
where labelled data is scarce?
We simulate training data scarcity in two different ways ≈õ Zero-
shotlearning ,andFew-shotlearning .For≈ÇZero-shot≈ælearning,we
evaluatethepre-trained NatGenindifferenttasks withoutanytask
specific fine-tuning. For ≈Çfew-shot≈æ setting, we simulate training
data scarcity by sub-sampling the benchmark training datasets.
We fine-tune the pre-trained NatGen on these limited training
examplesandmeasuretheperformance.Weobservethat NatGenis
veryefficientinlow-datatraining.Since NatGenlearnstogenerate
syntactically and semantically correct code as part of pre-training,
itfaces less burden whilelearninginlow-data training.
3 METHODOLOGY
Our approach comprises three steps: (i) ≈ÇDe-Naturalize≈æ source
code to accumulate pre-training data for NatGen (ƒü3.1); (ii) pre-
trainNatGen using this data for naturalization task (ƒü 3.2); (iii)
Fine-tunepre-trained NatGenwithtaskspecific dataset (ƒü 3.3).
3.1 De-naturalizing SourceCode
Forthefirststepabove,weusesixrulestotransformanaturalcode
intoitsunnaturalcounterpart.Thesetransformationsaresemantic-
preserving but rewrite an original, natural, (human-) written code
toanartificialform.Givenanaturalcodeelement,wedeployan
20ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore S.Chakraborty, T. Ahmed, Y. Ding, P. Devanbu, B.Ray
1intsearch(int[]arr,intkey,intlow,inthigh){
2 while(low<=high) {
3 intmid=low+ ((high-low) / 2);
4 if(arr[mid] ==key) {return mid; }
5 else{high=mid+ 1; }
6 }
7 return -1;
8}
(a)Original Code1intsearch(int[]arr,intkey,intlow,inthigh){
2 for(;low<=high;) {
3 intmid=low+ ((high-low) / 2);
4 if(arr[mid] ==key) {return mid; }
5 else{high=mid+ 1; }
6 }
7 return -1;
8}
(b) Loop Transformation
1intsearch(int[]arr,intkey,intlow,inthigh){
2 while(low<=high) {
3 intmid=low+ ((high-low) / 2);
4 while ( i< i ) {
5 high=mid +1;
6 }
7 // ... Rest of the Code
8 }
9 return -1;
10}
(c) DeadCodeInsertion1intsearch(int[]arr,intkey,intlow,inthigh){
2 while(high >= low ) {
3 intmid=low+ ((high-low) / 2);
4 if(arr[mid]!=key) {
5 high = mid + 1;
6 }
7 else{ return mid; }
8 }
9 return -1;
10}
(d)Blockand Operand Swap
1intsearch(int[]arr,intkey,intlow,inthigh){
2 while(low<=high) {
3 intmid=low+ ((high-low) / 2);
4 if(arr[mid] ==key) {return mid; }
5 else{
6 high=mid+ +;
7 }
8 }
9 return -1;
10}
(e)Inserting confusing codeelement1intsearch(int[]var_1,intkey,intlow,intvar_2){
2 while(low<=var_2) {
3 intmid=low+ ((var_2-low) / 2);
4 if(var_1[mid] ==key) {return mid; }
5 else{var_2=mid+ 1; }
6 }
7 return -1;
8}
(f) Variable Renaming
Figure 2:Semanticpreserving transformationused to prepare thepre-training data for NatGen .
appropriatetransformation,basedonitsASTstructureandrewrite
the code to ≈Çde-naturalize≈æit.
3.1.1 Designing Transformation Rules. We use six classes of de-
naturalizing transformations. These transformations are motivated
bypriorworkonfunctionalreasoningaboutsourcecode[ 20,25,26]
andsemantic bug-seeding [ 48]. Figure2showthe details.
Loop Transformation (Figure 2b).This rule modifies for
loopsintoequivalent whileloopandvice-versa.Werewritea while
loopoftheform while ( condition ){ loop-body } intoafor
loop asfor ( ;condition ;){ loop-body } . Likewise, to
transforma forloop into a whileloop, we move theinitializer of
thefor(ifany)beforetheloop,andtheupdate expression (ifany)
oftheforloopasthelast statement intheloop.Wealsoaddthis
updatestatementbeforeanyloopbreakingstatement( i.e.,break,
continue ).Forexample,wetransform≈Ç for(int i = 0; i < 10;
i++){ if(i){ foo(); continue;} bar(); } ≈æ as ≈Çint i = 0;
while(i < 10){ if(i){ foo(); i++;continue;} bar();
i++;}≈æ.
DeadCodeInjection(Figure 2c).Weinjectblocksofdeadcode
atrandom positionsintheoriginalcode.By≈Çdeadcode"wemeancode that appears in the source but is never executed. In Figure 2c,
we inject the code block high=mid+ 1;at line 4 of the original
code(Figure 2a).Toaddchallengetothemodel,wetransplantthese
insertedstatementsfromthesameinputcode.Toensurethe"death"
of inserted code, we put the inserted statements in a block headed
byeitheralooporabranch,guardedbyaunsatisfiableconditionso
that thecode inside theblock will never execute.In Figure 2c,the
condition i < iis always false; and the code in line 5 is quite
dead.
BlockSwap(Figure 2d).Hereweswapthe≈Çthen"blockofacho-
senifstatementwiththecorresponding elseblock.Topreserve
semantic equivalence, we negate the original branching condition.
For instance, Figure 2dreplaces the ifblock (line 4 in Figure 2a)
with the elseblock (line 5 in Figure 2a). We negate the original
condition ( arr[mid] ==key) as (arr[mid] !=key).
Operand Swap (Figure 2d).Here, we swap the operands of
binarylogicaloperations.Forinstance,wechangetheexpression
low<=highwithhigh>=lowinline2inFigure 2d.Whenswap-
ping the operands of a logical operator, we change the operator to
makesure themodifiedexpression isthe logicalequivalentto the
onebeforemodification.Incaseofasymmetricinequalityoperators
21NatGen: Generative Pre-trainingby ≈ÇNaturalizing≈æSource Code ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
for try
if
Step2: Select TransformationAST
Extractionfor 1.
if 2.Locating
Transformation
sites
Filter applicable
transformationTransformation
PoolApplicable 
Transformations Random
Selection
Apply
  TransformationStep1: Find Tranformation Location
Step3: Apply TranformationNatural Code
while (...)
{...}
  try{
   if(...){
} }while try
ifwhile
Context
Adaptation
Un-Natural CodeCode
Regenerationfor(...)
{...}
  try{
   if(...){
} }
Figure 3:≈ÇDe-Naturalization≈æworkflow in NatGen .
(>,<,>=,<=), we change the direction ≈õ keep as is for symmetric
operators ( i.e.,==,!=).
Confusing Code Insertion (Figure 2e).We introduce con-
fusing code patterns in the code as outlined by Gopstein et al .
[25,26]. In particular, we introduce two forms ofconfusing code.
First, we modify the of the form {i=j;j+= 1;}toi=j++;.
Second,we introduce ternaryoperator as applicable. For example,
we transform the code if(x!= 0){y=p;}else{y=q;}to
y= (x!= 0)?p:q;.
Variable Renaming (Figure 2f).We rename some variables
toVAR_i. While renaming a variable, we analyze the dataflow of
that variable and rename all occurrences of that variable in the
entire code. From all the variables used in the code, we change
just a certain percentage. For instance, in Figure 2f, we renamed
variablearrtovar_1, and variable hightovar_2, leaving all
othervariablesunchanged.Notethat,unlikeothertransformations,
variablerenamingdoesnotcreateASTofDataflowgraphdifference.
However, this challenging task [ 9] forces the model to learn to
generatenaturalvariablenames.Thisresemblesthede-obfuscation
pre-training taskof[ 58].
3.1.2 Applying Transformation. Assume a set of transformation
rulesŒ¶={ùúô1,ùúô2,ùúô3,...}. Given original code ùëêùëñ,ùúôùëó(ùëêùëñ)transforms
thecode,changingthestructurewhilepreservingsemantics.Fig-
ure3shows how to apply such transformation to ùëêùëñ. It works in
three steps:
‚Ä¢FindTransformationLocation. Givenapieceofsourcecode( ùëêùëñ),
we first use tree-sitter3to parse out the AST ( ùëáùëêùëñ). From the
AST, we extract potential locations for de-naturalization. These
locations are nodes ( ùëõùëò) inùëáùëêùëñ. While choosing location ùëõùëòfrom
3https://tree-sitter.github.io/tree-sitter/ùëáùëêùëñ, we consult Œ¶≈õ we extract the nodes where at least one of
ùúôùëó‚ààŒ¶isapplicable.
‚Ä¢Select Transformation Rule. Once we have a set of such nodes,
we filter out the transformation rules that cannot be applied
to any node of in ùëáùëêùëñ. After such a filtration, we have a set of
transformations Œ¶ùëé‚äÜŒ¶.Atthisstage,werandomlyselectone
transformationpattern ùúôùëó‚ààŒ¶ùëétoapplyatanapplicationloca-
tion(AST node) ùëõùëò.
‚Ä¢Apply Transformation. We apply ùúôùëótoùëõùëòto get the transformed
nodeùëõ‚Ä≤
ùëò. We then structurally match ùëõ‚Ä≤
ùëòwith the original AST
ùëáùëêùëñ,specifically ùëõùëò.Weadaptthecontextof ùëõùëòtothetransformed
node‚Äôs (ùëõ‚Ä≤
ùëò) context. In that way, we get the transformed AST
(ùëá‚Ä≤ùëêùëñ), whichwe then translate to getthe transformedcode ùëê‚Ä≤
ùëñ.
We designed the transformation function ùúôùëóand subsequent
context adaptation in such a way that preserves the meaning or
functionality of the original code. We use AST analysis and (ap-
proximated)data flowanalysisoncode AST.
3.2 Pre-training
Once we have a pool of ≈Çunnatural≈æ code using the transformation
in Section 3.1(i.e.,transform code ùëêùëñas ‚Äòun-natural‚Äô code ùúôùëó(ùëêùëñ)),
we use a neural sequence-to-sequence translation model ( M) to
reconstruct ùëêùëñfromùúô(ùëêùëñ),i.e.,we wantM(ùúôùëó(ùëêùëñ))to approximate
ùëêùëñ.Inparticular,givenatrainingdataset Dùë°={ùëê1,ùëê2,...}consisting
ofdeveloperswrittencode,setof≈Çde-naturalizing≈ætransformations
Œ¶={ùúô1,ùúô2,ùúô3,...}, we optimize the following function to learn
M‚Äôsoptimal parameter Œò.
Œò=argmin
ùúÉ‚àëÔ∏Å
ùëêùëñ‚ààDùë°ùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶/parenleftbigM/parenleftbigùúôùëó(ùëêùëñ)/parenrightbig,ùëêùëñ/parenrightbig(1)
3.3 Fine-Tuning
Theobjectiveofourpre-trainingistolearntobothcomprehendand
generate general-purpose source code. However, different tasks re-
latedtosourcecodegeneration( e.g.,texttocodegeneration,codeto
codetranslation,bugfixing)callfortask-specifictrainingofthepre-
trainedmodel.Thistrainingphaseonapre-trainedmodelisknown
as fine-tuning [ ?]. We consider the fine-tuning in NatGen as a
translationtaskandfollowthestandardtransformerbased-machine
translationprocedure[ 63].First,theencodergeneratestheencoded
representation ùëÖ(ùëã)given the input ùëã=[ùë•1,ùë•2,...,ùë•ùëõ]. The de-
coder then sequentially generates the output ùëå=[ùë¶1,ùë¶2,...,ùë¶ùëö].
Whileencodinganinputtoken ùë•ùëò,theencoderlearnstheattention
matrixw.r.t.every token in the input, including ùë•ùëò. Such attention
matrixisknownas self-attention .Whilegeneratinganoutputto-
kenùë¶ùëö, the decoder learns the attention matrix with all previously
generated tokens [ùë¶1,ùë¶2,...,ùë¶ùëö‚àí1]throughself-attention and the
encodergeneratedrepresentation ùëÖ(ùëã)throughcross-attention .We
refertoVaswanietal .[63]formoredetailabouttransformer-based
translation.
4 EXPERIMENTALSETUP
This section details the experimental designof NatGen.
Pre-training data. Following prior works [ 22,28,64], we primar-
ily use CodeSearchNet [ 34] dataset for the pre-training purpose.
CodeSerachNetisapubliclyavailabledatasetwithsixlanguages:
22ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore S.Chakraborty, T. Ahmed, Y. Ding, P. Devanbu, B.Ray
Java,Python,Go,JavaScript,Ruby,andPHP.InadditiontoCode-
SearchNet, CodeT5 uses additional data for C and C#. We also use
1MfunctionseachforCandC#.Forthesetwoadditionallanguages,
we collected 5000 active projects from GitHub and randomly se-
lected 1M functions considering the maximum sequence length of
the model.
Table 1: Statistics of fine-tuning datasets.
Task Dataset Train# Dev# Test#
Text‚àí ‚ÜíCode Generation [ 36] Concode 100000 2000 2000
Code‚àí ‚ÜíCode Translation [ 42] CodeXGLUE 10300 500 1000
Text+code ‚àí ‚ÜíCode BugFix[ 60]Small 46628 5828 5831
Medium 53324 6542 6538
Fine-tuningdata. Weevaluatedifferentvariationsofthreebench-
mark tasks related to source code generation. The first task is Text
toCodegeneration ,wheretheinputisanNLdescriptionofaJava
method, and the output is the code. The second task is Code Trans-
lationbetweenJavatoC# andC#toJava .Forthistask,weevaluate
Java-C#paralleldatasetproposedbyLuetal . [42].Thethirdand
final task is Bug Fix, where the given a buggy code and a summary
of the fix model generates the fixed code. For this task, we used
thetwodifferentversionsofthedataset(small,withlessthan50
tokens and medium with up to 100 tokens) proposed by Tufano
etal.[60].Notethat,similartoMODIT[ 16],weevaluateon concrete
versionoftherefinementdatasets.Table 1showsthedatasetsand
their statistics. For Text to Code Generation and Code Translation,
wereusethesamesplitfromCodeXGLUE[ 42],andforBugFix,we
reuse the same split as MODIT.
Pre-training Model Configurations. We use 12 layer transformers
with 12 attention heads on both encoder and decoder following
the CodeT5 [ 64] architecture. As discussed in Section 3, we use de-
naturalizationgenerativeobjectivesforpre-training.Weinitialize
ourmodelwithCodeT5‚Äôs[ 64]releasedparameters.Inparticular,we
initializeNatGenwith≈ÇCodeT5-base≈æmodel.Wepre-train NatGen
on 2 Nvidia GeForce RTX 3090 GPUs for 25K steps, maintaining
the effective batch size at 1080 with learning rate 5e-5. We train
NatGenfor approximately168hours.
EvaluationMetric. Throughouttheexperimentsinthiswork,we
evaluate accuracies w.r.t. exact match (EM), Syntax match (SM),
Dataflow match (DM), and CodeBLEU (CB) [ 56]. SM is the propor-
tionofmatchingsubtreesbetweenoutputcodeandtadgetcode‚Äôs
ASTsw.r.t.number of all possible subtrees in the target code‚Äôs AST.
DMisthepercentageofmatched(withtargetcode)anonymized
dataflowedge(def-useedge)ofoutputcode w.r.t.alldataflowedges
in the target code. Note that, both the SM and DM are components
of CB. We explicitly evaluate these for understanding the syntactic
and semantic correctness of generated code. We reuse Microsoft
CodeXGLUE tool[ 44]to compute SM,DM, andCB.
Baselines. Whilecomparingtheevaluationresultsfordifferent
tasks, we compare with large scale pre-trained models, includ-
ing GPT-2 [ 51], CodeGPT [ 42], PLBART [ 3], SPT-Code [ 46] and
CodeT5[64].Mostofourfine-tuningevaluationisonbenchmarkeddataset; thus, we report the available results from CodeXGLUE
leaderboard [ 45]. There are some task specific baselines, which we
discuss whiledescribingcorresponding task.
5 EMPIRICAL RESULTS
Weevaluate NatGen on(i)pre-trainingand(ii)threefine-tuning
tasks. We also check NatGen‚Äôs effectiveness in zero-shot and few-
shotsettings.
5.1NatGen ‚ÄôsEffectiveness on Pre-training
RQ1.Does ≈ÇNaturalization≈æhelpto improvecodegeneration?
Motivation. Weinvestigatewhetherpre-trainingonnaturalizing
taskhelpsthemodelgeneratecorrectandnaturalcode(codethat
issyntactically andsemantically similar to the originalcode).
Experimental Setup. We compare three large scale pre-trained
models: (i) CodeT5 [ 64], (ii) PLBART [ 3], and (iii) NatGen. Note
that, since PLBART is only pre-trained on Java and Python, we
comparePLBARTonlyforthoselanguages,withthecorresponding
resultsofothermodels.Weaskeachofthesemodelstoreconstruct
developers‚Äô written codefromits de-naturalized (but semantically
identical,seeƒü 3.1&ƒü3.1.1)variants.Weusetheheld-outvalidation
datafromourtrainingprocedureforthisevaluation.Weevaluate
the models for generating the Exact Match (EM), Syntax Match
(SM)andDataflowMatch (DM).
Table 2: Evaluation of NatGen for code generation task. CS
isthepercentageofexampleswhereoutputisdirectlycopied
from source, and ED is the median edit distance between
input codeandoutputcode.
Eval Data Model EM SM DM CB CS ED
FullCodeT5 0 13.93 19.86 9.74 0% 60
NatGen 70.39 98.78 97.69 97.31 0.01% 8
Java &PyCodeT5 0 13.83 23.67 10.87 0% 65
PLBART 0 73.17 75.95 74.56 7.05% 3
NatGen 64.13 98.16 96.85 96.82 0.01% 10
Results.Table2showsthe evaluation results.
¬∑SyntaxMatch. WefindthatthecodegeneratedbyPLBARTand
NatGenare mostly syntactically correct. However, CodeT5‚Äôs does
not always generate syntactically valid code, suggesting an advan-
tagefornaturalizationpre-training.Forinstance,Figure 4shows
code generated by different models from the given input. As we
cansee,CodeT5generatesasyntacticallyerroneousfragment.In
contrast, PLBART made a minor edit on the input code, just re-
movingthe protected keyword.BothPLBARTand NatGen are
pre-trainedtogeneratecompletecoderatherthanfragments(which
isthecaseofCodeT5[ 52]);thus,theformertwogenerallydobetter
at generatingsyntactically correctcode.
¬∑Semantic Match. NatGen is effective at recovering developers‚Äô
writtencodefromitsde-naturalizedsemanticvariants√êaround70%
ofthegeneratedcode(CodeBlue=97%) exactlymatches theoriginal
code. PLBART, which deploys syntactic denoising, is at the second
position interms ofCodeBlue.
23NatGen: Generative Pre-trainingby ≈ÇNaturalizing≈æSource Code ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
1.Input
protected SDV iam (SDV in,...){
if(i<i){
return new IAM(...);
}
return new IAM(...);
}2.PLBARToutput
SDV iam (SDV in, ...){
if(i<i){
return new IAM(...);
}
return new IAM(...);
}
3.NatGenoutput
protected SDV iam (SDV in,...){
return new IAM(...);
}4.CodeT5output
if(in) {
return
}}
Figure4:Exampleofinputgeneratedcodebydifferentpre-
trained models (slightly simplified).
NatGen also dominates the other two models in generating
syntactically (SM) & semantically (DM) valid code. While PLBART
appearstogeneratesyntacticallycorrectcode,itmostlycopiescode
fromtheinput√êmedianeditdistancefromPLBART‚Äôsinputandthe
generatedcodeis3(seeTable 2).Infact,in7.05%ofcases,PLBART
just copies the input! By contrast, NatGenlearns to generate vari-
antsoftheinputcode,withonly0.01%directcopyandamedian
editdistanceof10.SincePLBARTistrainedtoremovesyntaxerrors
from the input, we conjecture that itdoes not inherentlylearn the
semantic variation of the code. By contrast, we expose NatGento
semanticcodevariations,forcingittolearntogeneratecodethat
isboth more natural andsemantically equivalent.
¬∑Closer look into CodeT5. UnlikeNatGenand PLBART, CodeT5
is not explicitly trained to generate complete code. During pre-
training, CodeT5 learned to ≈Çunmask≈æ masked token sequences.
Thus, to better measure CodeT5‚Äôs generation capacity, we conduct
another experiment where we replaced all occurrences of some of
the variable names in code with a special MASK1,MASK2tokens and
asked CodeT5 to generate. This is one of the objectives (masked
identifiersprediction)CodeT5ispre-trainedtooptimize. Wetake
the CodeT5‚Äôs output and identify all potential identifiers4. Sur-
prisingly, in only0.27% of the cases, could CodeT5 generate all
the variables, and in 0.61% of cases halfof the masked variables.,
whileNatGen successfully translates 40.45% of those examples
backtoitsoriginalcode,includingcorrectlypredictingthereplaced
variable names. In addition, CodeT5‚Äôs generated token sequence
contained a lot of other tokens than the variable names (Figure 4.4,
for example).
Result1:Naturalizationenables NatGentoreasonaboutcode
semantics and thus help generate more natural code variants than
existingpre-training modelsand pre-training objectives.
We also did an ablation study evaluating the effect of NatGen‚Äôs
differentcomponentsonthe results.
RQ2.Howdodifferentcomponentsin NatGen contributetocodegen-
eration?
Motivation. InthisRQ,westudyhowdifferenttransformation
rules (see ƒü 3.1)contribute to learn generating natural code from
different semantic variants . We also evaluate how well NatGen
learns that in different programming languages over training time.
4weuseregex "[A-Za-z_]+[A-Za-z0-9_]*" to find identifiers.Experimental Setup. While pre-training, we checkpoint the Nat-
Genmodelevery1ktrainingsteps,forafullrunof25ksteps.At
each checkpoint,we evaluatethenaturalization taskperformance.
Before training, we held out 0.1% of the total data as validation
data. Notethat, since our goalin this experiment is to understand
NatGen‚Äôspre-trainingbetter,we≈Çde-naturalized"thevalidation
datausingthesametrainingdatadistribution.Thissettinggives
usacontrolledenvironment for experimentation.
Figure 5: Performance of NatGen pre-trained model under
differentcodetransformations.
Results.Figure5showsNatGen‚Äôsperformance underdifferent
types of semantic variants. Results show that NatGen has most
troublerecreatingtheoriginalcode(just40%ExactMatch)withthe
variablerenamingtask.Variablerenamingischallengingevenfor
human developers [ 6]√êdifferent developers may propose different
names for the same object. Nevertheless, on this task, NatGen
achieves good syntax and dataflow match (99% and 92% respec-
tively),indicatingthat NatGen preservessyntaxorsemanticsin
mostcaseswhilegeneratingcode withrenamedvariables.
On the other hand, NatGencan eliminate Dead Code with 99%
accuracy. This result may be an artifact of our specific implementa-
tion of this transformation. Our dead-code insertion rule is simple,
andformulaic;sothe NatGenquicklylearnstoidentifyandremove
such dead code. A more complex pattern of dead code may chal-
lenge the model more, and help make it more robust; we leave this
forfuturework.Fornaturalizingothertransformations, NatGen
achieves more than 80% exactmatch accuracy forBlock swap and
Confusion removing, and more than 75% exact match accuracy for
therest.Inallcases,syntaxmatch,dataflowmatch,andCodeBLEU
are well above 90%.
Figure6shows how validation performance improves for differ-
entlanguages,withmoretrainingsteps.Acrossallthelanguagesthe
performance rapidly increases over the first few thousand training
steps.Infact,atthebeginningof(step0)of NatGen‚Äôspre-training,
theoverallexactmatchis0,syntaxmatchis13.93%,dataflowmatch
is19.86%andCodeBLEUis9.74%(seeTable 2fordetails5).However,
afterjust1000stepsoftraining,theexactmatchrisesto61%,syntax
matchto97%,dataflowmatchto94%,andCodeBLEUto95%.These
metricscontinueimprovingastrainingprogresses.Theseresults
confirmthatacrossallthelanguages NatGen graduallylearns to
generatemore naturalcode from semantic variants.
5NatGen‚Äôs pre-training start from CodeT5-base. Thus, CodeT5-base is NatGen‚Äôs
checkpointat step0.
24ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore S.Chakraborty, T. Ahmed, Y. Ding, P. Devanbu, B.Ray
Figure 6: Progression of CodeBLEU of different language in
Validationdatasetovernumberpre-training steps.
Result2:pre-training performance depends on the types of se-
mantic variants√êwhile variable renaming seems the most difficult
(‚àº40% accuracy), dead-code elimination appears to be an easier
task(‚àº99%accuracy)to learn.
5.2NatGen ‚ÄôsEffectiveness on Fine-Tuning
Tasks
This section evaluates NatGen‚Äôs performance on three benchmark
sourcecode generative tasks.
RQ3. How effective is NatGenwhen fine-tuned for different generative
tasks in source code?
Table3:ResultsofTexttoCodeGeneration.‚Äò-‚Äôimpliesthat
those results arenot reported by corresponding approaches.
Mùëôùëéùë†ùë°isthemodelaftercompletingthefintuning,and Mùëèùëíùë†ùë°
is the intermediate model with best validation performance.
Approach EM SM DM CB
Seq2Seq 3.05 - - 26.39
Guo et al. [ 29]10.05 - - 29.46
Iyeret al. [ 36]12.20 - - -
GPT-2 17.30 - - 29.69
CodeGPT 20.10 - - 35.98
PLBART 18.75 - - 38.52
CodeT5-base22.30 - - 43.20(reported)
CodeT5*Mùëôùëéùë†ùë°21.85 44.34 44.52 41.75
Mùëèùëíùë†ùë°21.55 41.08 43.71 38.30
NatGenMùëôùëéùë†ùë°22.2545.59 46.87 43.73
Mùëèùëíùë†ùë°22.3044.38 45.64 42.44
* Ourreproduced result using CodeT5‚Äôspubliclyavailable pre-trained model.
Baselines. In addition to the baselines discussed in Section 4, for
theTextto Java Code generation task, we compare with a group of
baselines with no pre-training involved. These baselines include
LSTMbasedSequencetosequencemodels,Guoetal .[29]‚Äôs,andIyer
et al. [36]‚Äôs proposed techniques. We also report our reproduced
version of CodeT5 resultsin differenttasks,slightly differentfromwhattheyreported.Forboththe BugFixtask,wecomparewiththe
reportedresultsofMODIT[ 16]andourreproducedCodeT5result.
Results.
Text to Code Generation. Table3shows evaluation results for
text to code generation. We trained for 30 epochs. We stopped the
trainingisthevalidationperformancedoesnot increaseformore
thanthree(3)consecutiveepochs.ForbothCodeT5and NatGen,
we report the performance of final model after the fine-tuning
terminated ( Mùëôùëéùë†ùë°) and the performance of the model with best
validation perfomance ( Mùëèùëíùë†ùë°). Interestingly, for both CodeT5 and
NatGen, theMùëôùëéùë†ùë°model performs better than the corresponding
Mùëèùëíùë†ùë°model. The result shows that NatGen‚Äôs generated code
aremoresyntacticallyandsemanticallyclosertothetargetcode.
TheMùëôùëéùë†ùë°model of NatGenoutperforms CodeT5‚Äôs Mùëôùëéùë†ùë°model
by 2.8% inSM, 5.28%inDM and 4.74% inCB. We conjecture that
NatGen‚Äôspre-trainingwith≈Çnaturalization≈æhelpgeneratemore
naturalcode.
Table4:CodeTranslationresults.‚Äò-‚Äôimpliesthatthoseresults
are notreported by corresponding approaches.
ApproachJava‚àí ‚ÜíC# C#‚àí ‚ÜíJava
EM SM DM CB EM SM DM CB
PBSTM 12.5 - - 42.7 16.1 - - 43.5
CodeBERT 59.0 - - 85.1 58.8 - - 79.4
SPT-Code 64.1 - - - 60.2 - - -
PLBART 64.6 - - 87.9 65.0 - - 85.3
CodeT565.9 - - - 66.9 - - -(reported)
CodeT5* 65.9 90.4 91.9 87.8 66.0 90.4 88.9 84.4
NatGen 66.2 91.0 92.0 88.1 67.3 91.0 89.8 85.2
* Ourreproduced result using CodeT5‚Äôspubliclyavailable pre-trained model.
Code Translation. Table4shows the results of NatGen and
different baselines for Code Translation. For Java to C# translation,
NatGen achievesexactmatchaccuracyof66.2%whileCodeT5‚Äôs
accuracyis65.9%.InC#toJavatranslation, NatGenachieves67.3%
exactmatchaccuracy,whichCodeT5achieves66.0%.Inaddition,
the syntactic match (SM), Dataflow match, and CodeBLEU are also
higher thanthat ofCodeT5.
Table 5:ResultofBug fix(Top1 fixaccuracy).
ApproachBugFix ùë†ùëöùëéùëôùëô BugFix ùëöùëíùëëùëñùë¢ùëö
Unimodal Multimodal Unimodal Multimodal
MODIT 20.35 21.57 8.35 13.18
CodeT5 21.79 22.97 12.59 14.94
NatGen 22.26 23.43 13.32 14.93
Bug Fix. Similar to MODIT, we evaluate the top-1 accuracy of the
generatedfixedcode.Wealsoevaluateuni-modalsettings,where
the fix description is unavailable, and multi-modal settings, where
wehaveaccesstothefixdescription.Table 5showstheresultsof
Bug Fix. For the BugFix ùë†ùëöùëéùëôùëôdataset,NatGen outperforms both
CodeT5 and MODIT in both unimodal and multi-modal settings.
25NatGen: Generative Pre-trainingby ≈ÇNaturalizing≈æSource Code ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
(a)Java to C# Translation
 (b) C#to JavaTranslation
 (c)Textto Code Generation
 (d)BugFix(small,multimodal)
Figure 7:Zero-shot transfer learning capability of NatGen infordifferenttasks.
(a)Java to C# Translation
 (b)C# to Java Translation
 (c)Textto Code Generation
 (d)BugFix(small,multimodal).
Figure 8: Few shot Learning evaluation of NatGen . In each case, the pre-trained model is fine-tuned on 200 training examples
for10epochandtheresultison thefulltestset.
(a)Javato C#Translation.
 (b) C#to JavaTranslation.
 (c) Text to CodeGeneration.
 (d)BugFix (small,multimodal).
Figure 9: NatGen ‚Äôsresults on differenttaskswith Few shot settings.X-axisshowsnumberoftraining examples.
ForFortheBugFix ùëöùëíùëëùëñùë¢ùëödataset,NatGen performsbetterthan
CodeT5andMODITinunimodalsettingandslightlyworsethan
CodeT5 inthe multi-modalsetting.
Result3:NatGenperforms better than most of the existing
baselines. NatGen‚Äôs improvement in Syntax match and Dataflow
match signifies NatGen‚Äôs ability to generate code syntactically
and semantically closer to targetcode.
Finally,weevaluate NatGen‚Äôsperformanceinthepresenceof
data scarcity.
RQ4. How well does NatGen‚Äôs pre-training help in tasks where labelled
datais scarce?
Motivation. Learningtogeneratecodeusuallyrequiresalarge
amount of annotated training data. A lot of time and effort goes
into curatinghigh-qualitytrainingdata [ 4,38]. Unsupervised pre-
training endowsmachinelearningmodels withnecessarydomain
knowledge about the task [ 21]. In practice, this knowledge appears
to transfer across multiple tasks. Such pre-training reduces the
effort to learn each different task. We therefore study the effective-
ness ofNatGen‚Äôs domain knowledge about source code syntax
and semantics. In particular, we stress test whether the knowledgeNatGenlearnedduringpre-trainingisusefulfordownstreamtasks,
bylimitingavailable task-specific training data.
Experimental Setup. We evaluate NatGen‚Äôs over several data-
limited tasks: Text to Code generation ,Code Translation , andBug
Fix. We consider two different settings. First, we consider zero-
shot [57,67] evaluation. Here we evaluate different pre-trained
modelswithoutanytask-specifictraining.Naturally,wedon‚Äôtsee
goodperformanceinthissetting.Nevertheless,thisstress-testmea-
suresthecodegenerationabilityofmodels.Second,wetryfew-shot
learning [ 53,59,65]. We randomly choose a few training examples
for eachtask and fine-tune thepre-trained models on those exam-
ples, and evaluate their performance. We gradually increase the
number oftraining examples over several few-shotsettings.
Results.Figure7showsthe NatGen‚ÄôsandCodeT5‚Äôszero-shot
performance.Lackingtask-specifictraining,wecanseeherehow
much transferable knowledge each model learned just during pre-
training.Therearelargedifferencesinallthetasksbetween Nat-
GenandCodeT5acrossSyntaxMatchandDataflowMatch.Itsigni-
fiesNatGenlearnstogeneratebothsyntacticallyandsemantically
correct code during pre-training, which CodeT5 rarely can do. Fig-
ure8showstheperformanceof NatGenandCodeT5whentrained
26ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore S.Chakraborty, T. Ahmed, Y. Ding, P. Devanbu, B.Ray
on 200 training examples. NatGen also has an advantage over
CodeT5 here.
We notea largerperformance gap in the Translation tasks(Fig-
ure7a&7b) and Bug Fix (Figure 7d) tasks, compared to Text to
Code Generation task (Figure 7c) in both the zero-shot and the few
shot(Figure 8)experiments.Weconjecturethatsuchdiscrepancyis
theartifact ofthenatureofthetasks.The cross-lingual alignment
between NL and Java code is the key factor in generating text to
code.Incontrast,boththeinputandoutputaretheprogramming
language in the translation and bug fix task. Thus, we hypothe-
sizethatNatGen leveragesitssharedknowledgeacrossdifferent
programminglanguageslearnedduringthe pre-training.
We further stress test NatGen‚Äôs with few-shot learning; we
graduallyincreasedthenumberoftrainingexamplesandtrained
bothCodeT5and NatGen.Figure9showstheperformanceprogress
as the number of training examples increase. For all four tasks,
NatGensignificantlyimprovesoverCodeT5 when thenumberof
trainingexamplesisminimal.Withincreasingtrainingexamples,
theperformancegap graduallydecreases.Arguably, withenough
labeleddata and enough resources, all high-capacity models will
get better at generating source code. Nevertheless, we learn two
criticallessonsfrom NatGen‚Äôsbetterperformanceinzero-shotand
few-shotlearning.First, NatGen‚Äôsbetterperformanceacrossall
tasks suggests that that the coding knowledge it learns from the
naturalization task is more generic and transferable. Second, for
anypre-trainedmodeltobeeffectiveincodegeneration,especially
inalimitedtrainingdatascenario,thepre-trainingshouldexplicitly
teachthemodelhowtowritecode.Otherwise,wehypothesizethat
abigchunkoffine-tuningresourceswillbespentonthemodels‚Äô
learningto writecode.
Result4:NatGenis very effective in source code generative
tasks whenminimal training resource isavailable. Since NatGen
explicitly learns to generate code during pre-training, it can avoid
learning suchduringfine-tuning saving fine-tuning resource.
6 LIMITATIONS& THREATS
Biasintroducedby‚Äòde-naturalizing‚Äôtransformations. In
Section3.1,wedescribedoursixtransformationsto≈Çde-naturalize"
sourcecode.The NatGenmodellearnstorevertonetransformation
at a time. In fact, we found empirically that, when given code
with more than one ‚Äòde-naturalization‚Äô transformation applied, the
model reverses only one of them. There is thus a threat our limited
applicationofde-naturalizationlimitstheabilityofour NatGen.
Regardless,weconsider NatGenasaproof-of-conceptandthefirst
work towards teaching a model to write natural code. We leave the
investigationmorenaturalcodepatternsandtheireffectoncode
generationas apotentialfuture work.
Table 6:NatGen ‚ÄôsperformanceinCodesummarization
Approach Go Java JS Python Php Ruby Overall
PLBART 18.91 18.45 15.56 19.30 23.58 14.11 18.32
CodeT5 19.5620.3116.16 20.01 26.0315.24 19.55
NatGen 19.4320.3816.00 20.09 26.0015.38 19.55KnowledgeretentionfromCodeT5. AsmentionedinSection 4,
westartNatGen‚Äôspre-trainingfromCodeT5-basemodel[ 64].Start-
ing further pre-training from an existing pre-trained checkpoint is
very common in large-scale pre-training. For instance, GraphCode-
BERT [28] is pre-trained based on CodeBERT [ 22] model, which
was pre-trained based on RoBERTa [ 41] model. Both the Open
AI-CodeX [ 17] and GitHub Copilot [ 24] models are further pre-
trained in OpenAI-GPT3 [ 11]. Nevertheless, when we further train
a pre-trained model on different tasks, it is subject to ≈Ç catastrophic
forgetting ≈æ [37] of the knowledge learned in the base model. In
ordertotestwhether NatGen isforgettingCodeT5‚Äôsknowledge
aboutnaturallanguagegeneration,wealsoevaluate NatGen for
Codesummarization.Heretheinputissourcecode,andtheoutput
isNaturallanguage.Afterfine-tuning NatGen‚ÄôsoverallBLEUin
19.547 while CodeT5‚Äôs was 19.551, suggesting that NatGenmostly
retainsCodeT5‚Äôscapacity togenerateNL(see Table 6fordetailed
results).
Fair Comparison with CodeT5. We initialize NatGen with
pre-trainedcheckpointfromCodeT5(alreadypre-trained75Ksteps
with their objective) and train NatGenfor 25K steps with ‚Äònatural-
code‚Äôwritingobjective.Askepticreaderwouldwanttoknowwhat
happenswhen we pre-train CodeT5for 25K more steps with their
trainingobjective.Wearguethatsincethepre-trainingobjective
doesnotexplicitlyaccountforgeneratingcode(Seesection3.2of
CodeT5‚Äôsoriginalpaper),furthertrainingwiththeCodeT5objective
doesnotnecessarilyincreaseitscodegenerationcapacity.Wedo
acknowledgeCodeT5‚Äôsabilitytounderstandandreasonabout input.
Sincethepre-traininglargemodelisextremelyexpensive(ƒü 4)6;we
leverage such knowledge by initializing NatGen from CodeT5‚Äôs
publicly available pre-trained model. Moreover, CodeT5 release
neithertheircodeforpre-training(onlyforfine-tuning),norany
earlierorlatercheckpointsforustocarryoutfurtherinvestigation.
≈ÇNaturalization≈æwithprogram-analysis. NatGenisaproto-
typeofagenerativepre-trainedmodelwith≈ÇNaturalization≈ætask,
trained to revert six classes of de-naturalization transformations
(seeFigure 2).However,perfectperformance w.r.t.thesetransforma-
tion isnotthe main objective of this research. Tools to accomplish
≈Çnaturalization" couldsurelybe builtusingtraditionalrefactoring
methods; however, our goal is to train NatGenso that it learns to
generatenaturalcode withthe helpof this ≈ÇNaturalization≈ætask.
NatGen as ≈ÇCode-Refactoring≈æ tool. NatGen suggests the
promise of neural transformers to build meaning-preserving code-
refactoringtools.However,torealizeamoreaccurateandpowerful
neuralre-factoringtool,moretrainingdata,withalargervarietyof
transformations, wouldbe required.We leave this as future work.
7 RELATED WORKS
Theapproachofpre-traininglargeTransformerswithouthuman
labels started in NLP domain with BERT [ ?], which introduces
two pre-training objectives (i.e., Mask Language Modeling and
NextSentencePrediction).Later,Liuetal.showthatRoBERTa[ 41]
outperforms BERT only using Mask Language Modeling (MLM)
with new training strategies and hyper-parameter tuning. MLM is
6CodeT5waspre-trainedon16 NVIDIA A100s,with40Gmemory each,for12 days!
One mightreasonably assumeit was alreadywell-trained onthe originalobjective
27NatGen: Generative Pre-trainingby ≈ÇNaturalizing≈æSource Code ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
a self-supervised task that the model randomly masks or modifies
acertainnumber oftokens andtries to recover them.
Following the success of the pre-trained model in the NLP do-
main, researchers applied these models to code related tasks. Code-
BERT is one of the earliest that was specially trained for code
andrelevantnaturallanguagedescriptions.Itispre-trainedwith
twoobjectives(i.e., MLM andReplacedToken Detection[ 18])and
demonstrated pre-training‚Äôs effectiveness for code. Later, an archi-
tecturallyequivalentmodel,GraphCodeBERT,wasintroduced;it
improvedoverCodeBERTonmosttasksbyincorporatingdata-flow
information.
Though CodeBERT [ 22] & GraphCodeBERT [ 28], DietCode-
BERT[68]dowellatcodeunderstandingtasks,thesemodelsare
not as good at generative tasks. Both models are encoder-only
and have to start with an untrained decoder in fine-tuning for
generativetasks,suchascoderepair,codegeneration,codesum-
marization,andcodetranslation.Toaddressthislimitation,Ahmad
etal.introducedPLBART[ 3],pre-trainedasagenerativedenois-
ing autoencoder. A specific set of noises is introduced to code and
relevant natural language description and used as the input to
the model. The model‚Äôs objective is to encode the noisy input in
the encoder and generate noise-free code or text in the decoder.
PLBART(buildsonBART[ 39])outperformsbothCodeBERT[ 22]
andGraphCodeBERT[ 28]onbothunderstandingandgenerative
taskswithapre-trainedencoderanddecoder[ 3].DOBF[ 58]uses
de-obfuscation (recovering variable names) as their pre-training
task; however, rather than generating code, they just generate a
dictionary ofrecoverednames.
CodeT5[64](basedT5[ 52])isthelatestdenoisingmodel.CodeT5
usesthedeveloper-assignedidentifiersincode,addingtwocode-
specificpre-trainingobjectivestotheoriginalT5,identifiertagging
and masked identifier prediction. CodeT5 is an encoder-decoder
model and excels at both understanding and generative tasks com-
pared to other models. Similar to CodeT5, [ 43,49] are also built
based on T5 architecture and perform reasonably well in the dif-
ferent downstream tasks. NatGen has a similar architecture to
CodeT5;butrather thanCodeT5‚Äôspre-trainingobjectives, we≈Çde-
naturalize"code,usingtheformalchannelofcodetoinjectmeaning-
preserving transforms, and then force NatGen to recreate, the
original, ≈Çnatural" code. Rewriting semantically equivalent code
requiressemanticunderstanding,andthatcanbeappliedtocode
only because of its dual-channel nature. Our evaluation shows
thatrewritingsemanticallyequivalentprogramsinthepre-training
stageresultsinperformancegainsinatleastthreepopularSoftware
Engineeringtasks.
8 CONCLUSION
We introduce the≈ÇCode-Naturalization≈æpre-training objective for
generative models of code. As proof-of-concept we pre-trained our
NatGen towrite‚Äònatural‚Äôsourcecode from‚Äòun-natural‚Äôcounter-
part. With this pre-training, NatGenlearns to write code syntacti-
callyandsemanticallyclosertodevelopers‚Äôwrittencode.We≈Çde-
naturalize≈æ existing developers‚Äô code, using six kinds of ≈Çsemantic-
preserving≈æ transformations. We further fine-tune the NatGenon
different variations of three downstream tasks that require code
generation. NatGenachievesstate-of-the-artperformanceinthesedownstream tasks, and NatGen‚Äôs generated code are syntactically
and semantically closer to the target code. Our pre-training on the
‚Äònaturalizing‚Äô task is especially effective in resource-constrained
settingi.e.,zero-shot,andfew-shottransfer learning.
9 DATA AVAILABILITYSTATEMENT
We publicly code, and all processing scripts of NatGen‚Äôs pre-
training[ 1].NatGen pre-trainedmodelisalsoavailablethrough
https://huggingface.co/saikatc/NatGen .
ACKNOWLEDGMENTS
ThisworkissupportedinpartbyNSFgrantsCCF-2107405,SHF-
1845893, SHF-1414172, SHF-2107592, IIS-2221943, and IBM faculty
award.Anyopinions,findings,conclusions,orrecommendations
expressed herein are those of the authors and do not necessar-
ilyreflectthoseoftheUSGovernment,NSF,orIBM.Ahmedwas
also supported by UC Davis College of Engineering Dean‚Äôs Distin-
guishedFellowship.
REFERENCES
[1]2022. NatGen: Generative Pre-training by ≈ÇNaturalizing≈æ Source Code - Code
and scriptsfor Pre-Training. https://doi.org/10.5281/zenodo.6977595
[2]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.
A Transformer-basedApproachfor SourceCode Summarization.In Proceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL) .
[3]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2021.
Unified Pre-training for Program Understanding and Generation. In 2021 Annual
ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics (NAACL) .
[4]WasiUddinAhmad,MdGolamRahmanTushar,SaikatChakraborty,andKai-Wei
Chang. 2021. AVATAR: A Parallel Corpus for Java-Python Program Translation.
arXiv:2108.11590 [cs.SE]
[5]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learningmodelsofcode.In Proceedingsofthe2019ACMSIGPLANInternational
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software. 143≈õ153.
[6]MiltiadisAllamanis,EarlTBarr,ChristianBird,andCharlesSutton.2015. Sug-
gestingaccuratemethodandclassnames.In Proceedingsofthe201510thJoint
MeetingonFoundationsofSoftwareEngineering . 38≈õ49.
[7]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys(CSUR) 51,4 (2018), 1≈õ37.
[8]MiltiadisAllamanis,EarlTBarr,SolineDucousso,andZhengGao.2020. Typilus:
Neuraltype hints.In Proceedingsof the41stacmsigplan conferenceon program-
minglanguagedesignand implementation . 91≈õ105.
[9]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2017. Learning
to representprogramswith graphs. arXiv preprint arXiv:1711.00740 (2017).
[10]MatthewAmodio,SwaratChaudhuri,andThomasWReps.2017. Neuralattribute
machinesfor programgeneration. arXiv preprint arXiv:1705.09231 (2017).
[11]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
PrafullaDhariwal,ArvindNeelakantan, Pranav Shyam, Girish Sastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
arXiv:2005.14165 [cs.CL]
[12]Casey Casalnuovo, Earl T Barr, Santanu Kumar Dash, Prem Devanbu, and Emily
Morgan. 2020. A theory of dual channel constraints. In 2020 IEEE/ACM 42nd
InternationalConferenceonSoftwareEngineering:NewIdeasandEmergingResults
(ICSE-NIER) . IEEE,25≈õ28.
[13]CaseyCasalnuovo,KevinLee,HulinWang,PremDevanbu,andEmilyMorgan.
2020. Do programmers prefer predictable expressions in code? Cognitive science
44,12(2020), e12921.
[14]Casey Casalnuovo, E Morgan, and P Devanbu. 2020. Does surprisal predict
code comprehension difficulty.In Proceedings of the42nd Annual Meeting ofthe
CognitiveScienceSociety . CognitiveScience SocietyToronto, Canada.
[15]Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi Ray.
2020. CODIT:Code EditingwithTree-Based NeuralModels. IEEE Transactions
onSoftwareEngineering 1 (2020), 1≈õ1.
28ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore S.Chakraborty, T. Ahmed, Y. Ding, P. Devanbu, B.Ray
[16]SaikatChakrabortyandBaishakhiRay.2021. OnMulti-ModalLearningofEditing
Source Code. In 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE) . 443≈õ455. https://doi.org/10.1109/ASE51524.2021.
9678559
[17]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
OliveiraPinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov,AletheaPower,LukaszKaiser,MohammadBavarian,ClemensWinter,
Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,Shan-
tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam,VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,Miles
Brundage,MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained onCode. arXiv: 2107.03374 [cs.LG]
[18]Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.
ELECTRA:Pre-trainingTextEncodersasDiscriminatorsRatherThanGenerators.
InInternational Conference on Learning Representations .https://openreview.net/
pdf?id=r1xMH1BtvB
[19]]devlin2018bert Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.[n.d.]. BERT:Pre-trainingofDeepBidirectionalTransformersfor
LanguageUnderstanding.In Proceedingsofthe2019ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies,Volume 1 (Long and Short Papers) .
[20]Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray,
and Saikat Chakraborty. 2022. Towards Learning (Dis)-Similarity of Source
CodefromProgramContrasts.In Proceedingsofthe60thAnnualMeetingofthe
Associationfor ComputationalLinguistics (Volume 1: Long Papers) . 6300≈õ6312.
[21]Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. 2010. Why
doesunsupervised pre-traininghelp deeplearning?. In Proceedingsofthethir-
teenth international conference on artificial intelligence and statistics . JMLR Work-
shopand ConferenceProceedings, 201≈õ208.
[22]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
APre-TrainedModelforProgrammingandNaturalLanguages.In Findingsofthe
Associationfor ComputationalLinguistics: EMNLP 2020 . 1536≈õ1547.
[23]Martin Fowler. 2018. Refactoring: improving the design of existing code . Addison-
WesleyProfessional.
[24] GitHub. 2022. GitHubCopilot( https://copilot.github.com/ ).
[25]DanGopstein,Anne-LaureFayard,SvenApel,andJustinCappos.2020. Thinking
aloudaboutconfusing code: Aqualitative investigationof programcomprehen-
sion and atoms of confusion. In Proceedings of the 28th ACM Joint Meeting on
EuropeanSoftware Engineering Conference and Symposium onthe Foundations of
SoftwareEngineering . 605≈õ616.
[26]Dan Gopstein, Hongwei Henry Zhou, Phyllis Frankl, and Justin Cappos. 2018.
Prevalenceofconfusingcodeinsoftwareprojects:Atomsofconfusioninthewild.
InProceedingsofthe15thInternationalConferenceonMiningSoftwareRepositories .
281≈õ291.
[27]David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to
Comment ≈ÇTranslation≈æ: Data, Metrics, Baselining & Evaluation. In 2020 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE,746≈õ757.
[28]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou,NanDuan,JianYin,DaxinJiang,etal .2021. GraphCodeBERT:Pre-training
CodeRepresentationswithDataFlow.In InternationalConferenceonLearning
Representations .
[29]Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin. 2019. Coupling
RetrievalandMeta-LearningforContext-DependentSemanticParsing.In Pro-
ceedings of the 57th Annual Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics, Florence, Italy, 855≈õ866.
https://doi.org/10.18653/v1/P19-1082
[30]Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common CLanguage Errorsby DeepLearning..In AAAI. 1345≈õ1351.
[31]VincentJHellendoornandPremkumarDevanbu.2017. Aredeepneuralnetworks
thebestchoiceformodelingsourcecode?.In Proceedingsofthe201711thJoint
MeetingonFoundationsofSoftwareEngineering . ACM,763≈õ773.
[32]AbramHindle,EarlTBarr,MarkGabel,ZhendongSu,andPremkumarDevanbu.
2016. Onthe naturalness of software. Commun. ACM 59,5 (2016), 122≈õ131.
[33]AbramHindle,EarlTBarr,ZhendongSu,MarkGabel,andPremkumarDevanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
SoftwareEngineering (ICSE) . IEEE,837≈õ847.
[34]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
codesearch. arXivpreprintarXiv:1909.09436 (2019).https://arxiv.org/abs/1909.
09436
[35]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers) . 2073≈õ2083. https://doi.org/10.18653/v1/P16-1195
[36]SrinivasanIyer,IoannisKonstas,AlvinCheung,andLukeZettlemoyer.2018.Map-
ping language to code in programmatic context. arXiv preprint arXiv:1808.09588
(2018).
[37]James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska,etal .2017. Overcomingcatastrophicforgettinginneural
networks. Proceedingsofthenationalacademyofsciences 114,13(2017),3521≈õ
3526.
[38]Marie-AnneLachaux,BaptisteRoziere,LowikChanussot,andGuillaumeLample.
2020. Unsupervised Translation of Programming Languages. arXiv preprint
arXiv:2006.03511 (2020).
[39]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.2019.Bart:Denoising
sequence-to-sequencepre-trainingfor naturallanguagegeneration, translation,
and comprehension. arXiv preprint arXiv:1910.13461 (2019).
[40]ShangqingLiu,YuChen,XiaofeiXie,JingkaiSiow,andYangLiu.2020. Retrieval-
augmentedgenerationforcodesummarizationviahybridgnn. arXivpreprint
arXiv:2006.05405 (2020).
[41]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. RoBERTa:A
RobustlyOptimizedBERTPretrainingApproach. arXivpreprintarXiv:1907.11692
(2019).https://arxiv.org/abs/1907.11692
[42]Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .2021.
CodeXGLUE: A Machine Learning BenchmarkDataset for Code Understanding
and Generation. arXiv preprint arXiv:2102.04664 (2021).https://arxiv.org/abs/
2102.04664
[43]Antonio Mastropaolo, SimoneScalabrino,NathanCooper, David NaderPalacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE,
336≈õ347.
[44]Microsoft. 2021. CodeBLEU calculator ( https://github.com/microsoft/
CodeXGLUE/tree/main/Code-Code/code-to-code-trans/evaluator/CodeBLEU ).
[45]Microsoft. 2022. CodeXGLUE Leaderboard ( https://microsoft.github.io/
CodeXGLUE/ ).
[46]ChanganNiu,ChuanyiLi,VincentNg,JidongGe,LiguoHuang,andBinLuo.2022.
SPT-Code: Sequence-to-Sequence Pre-Training for Learning the Representation
of Source Code. arXiv preprint arXiv:2201.01549 (2022).
[47]Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-WeiChang.2021. RetrievalAugmentedCodeGenerationandSummarization.
arXiv preprint arXiv:2108.11601 (2021).
[48]JibeshPatraandMichaelPradel.2021. Semanticbugseeding:alearning-based
approachforcreatingrealisticbugs.In Proceedingsofthe29thACMJointMeeting
on European Software Engineering Conference and Symposium on the Foundations
ofSoftwareEngineering . 906≈õ918.
[49]LongPhan,HieuTran,DanielLe,HieuNguyen,JamesAnibal,AlecPeltekian,
andYanfangYe.2021. CoTexT:Multi-taskLearningwithCode-Text Transformer.
arXiv preprint arXiv:2105.08645 (2021).
[50]MichaelPradelandSatishChandra.2021. Neuralsoftwareanalysis. Commun.
ACM65,1 (2021), 86≈õ96.
[51]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI
blog1,8 (2019), 9.
[52]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-
its of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683 (2019).
[53]SachinRaviandHugoLarochelle.2016. Optimizationasamodelforfew-shot
learning. (2016).
[54]Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli,andPremkumarDevanbu.2016. Onthe"naturalness"ofbuggycode.
In2016IEEE/ACM38thInternationalConferenceonSoftwareEngineering(ICSE) .
IEEE,428≈õ439.
[55]VeselinRaychev,MartinVechev,andEranYahav.2014. Codecompletionwith
statisticallanguagemodels. In Acm Sigplan Notices , Vol. 49.ACM,419≈õ428.
[56]ShuoRen,DayaGuo,ShuaiLu, LongZhou,ShujieLiu,DuyuTang,Ming Zhou,
Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU: a Method for Automatic
Evaluation of Code Synthesis. arXiv preprint arXiv:2009.10297 (2020).https:
//arxiv.org/abs/2009.10297
[57]Bernardino Romera-Paredes and Philip Torr. 2015. An embarrassingly simple
approachtozero-shotlearning.In Internationalconferenceonmachinelearning .
PMLR, 2152≈õ2161.
[58]Baptiste Roziere,Marie-Anne Lachaux, Marc Szafraniec,and Guillaume Lample.
2021. DOBF: A deobfuscation pre-training objective for programming languages.
29NatGen: Generative Pre-trainingby ≈ÇNaturalizing≈æSource Code ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
arXiv preprint arXiv:2102.07492 (2021).
[59]Qianru Sun,YaoyaoLiu, Tat-SengChua,andBerntSchiele.2019. Meta-transfer
learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on
Computer Visionand PatternRecognition . 403≈õ412.
[60]MicheleTufano,JevgenijaPantiuchina,CodyWatson,GabrieleBavota,andDenys
Poshyvanyk. 2019. On Learning Meaningful Code Changes via Neural Machine
Translation. arXiv preprint arXiv:1901.09102 (2019).
[61]MicheleTufano,Cody Watson,GabrieleBavota, Massimiliano Di Penta,Martin
White, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing
patchesinthewildvianeuralmachinetranslation. ACMTransactionsonSoftware
Engineering and Methodology (TOSEM) 28,4 (2019), 1≈õ29.
[62]BogdanVasilescu,CaseyCasalnuovo,andPremkumarDevanbu.2017.Recovering
clear,naturalidentifiersfromobfuscatedJSnames.In Proceedingsofthe201711th
jointmeetingonfoundations ofsoftwareengineering . 683≈õ693.
[63]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
youNeed.In Advancesin Neural InformationProcessingSystems 30 .5998≈õ6008.
[64]YueWang,WeishiWang,ShafiqJoty,andStevenCHHoi.2021. Codet5:Identifier-
awareunifiedpre-trainedencoder-decodermodelsforcodeunderstandingandgeneration. arXiv preprint arXiv:2109.00859 (2021).
[65]YaqingWang,QuanmingYao,JamesTKwok,andLionelMNi.2020. Generalizing
fromafewexamples:Asurveyonfew-shotlearning. ACMcomputingsurveys
(csur)53,3 (2020), 1≈õ34.
[66]MartinWhite,MicheleTufano,ChristopherVendome,andDenysPoshyvanyk.
2016. Deeplearningcodefragmentsforcodeclonedetection.In Proceedingsof
the31st IEEE/ACM InternationalConference onAutomatedSoftwareEngineering .
ACM,87≈õ98.
[67]Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. 2018.
Zero-shot learning√êa comprehensive evaluation of the good, the bad and the
ugly.IEEEtransactionsonpatternanalysisandmachineintelligence 41,9(2018),
2251≈õ2265.
[68]ZhaoweiZhang,HongyuZhang,BeijunShen,andXiaodongGu.2022. DietCode
is Healthy: Simplifying Programs for Pre-Trained Models of Code. In Proceed-
ings of the 2022 The ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering (Singapore, Singapore)
(ESEC/FSE 2022) .
30