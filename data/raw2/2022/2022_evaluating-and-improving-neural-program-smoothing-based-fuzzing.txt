Evaluating and Improving
Neural Program-Smoothing-based Fuzzing
Mingyuan Wuâ€ 
Southern University of Science and
Technology, Shenzhen, China and the
University of Hong Kong, Hong Kong,
China
11849319@mail.sustech.edu.cnLing Jiang, Jiahong Xiang,
Yuqun Zhang*
Southern University of Science and
Technology
Shenzhen, China
{11711906,11812613,zhangyq}@mail.sustech.edu.cnGuowei Yang
The University of Queensland
Brisbane, Australia
guowei.yang@uq.edu.au
Huixin Ma, Sen Nie, Shi Wu
Tencent Security Keen Lab
Shanghai, China
{huixinma,snie,shiwu}@tencent.comHeming Cui
The University of Hong Kong
Hong Kong, China
heming@cs.hku.hkLingming Zhang
University of Illinois
Urbana-Champaign, USA
lingming@illinois.edu
ABSTRACT
Fuzzingnowadayshasbeencommonlymodeledasanoptimization
problem,e.g.,maximizingcodecoverageunderagiventimebudget
via typical search-based solutions such as evolutionary algorithms.
However, such solutions are widely argued to cause inefficientcomputing resource usage, i.e., inefficient mutations. To addressthis issue, two neural program-smoothing-based fuzzers, Neuzz
andMTFuzz, have been recently proposed to approximate pro-
gram branching behaviors via neural network models, which input
byte sequences of a seed and output vectors representing program
branching behaviors. Moreover, assuming that mutating the bytes
with larger gradients can better explore branching behaviors, they
developstrategiestomutatesuchbytesforgeneratingnewseeds
as test cases. Meanwhile, although they have been shown to beeffective inthe originalpapers, theywereonly evaluatedupon alimited dataset. In addition, it is still unclear how their key tech-nical components and whether other factors can impact fuzzingperformance. To further investigate neural program-smoothing-based fuzzing, we first construct a large-scale benchmark suite
with a total of 28 popular open-source projects. Then, we exten-
sivelyevaluate NeuzzandMTFuzzonsuchbenchmarks. Theeval-
uation results suggest that their edge coverage performance canbeunstable.Moreover,neitherneuralnetworkmodelsnormuta-
tion strategies can be consistently effective, and the power of their
gradient-guidancemechanisms havebeencompromised.Inspired
â€ Mingyuan Wu is also affiliated with the Research Institute of Trustworthy Au-
tonomous Systems, Shenzhen, China.
* Yuqun Zhang is the corresponding author. He is also affiliated with the Research
Institute of Trustworthy Autonomous Systems, Shenzhen, China and Guangdong
Provincial Key Laboratory of Brain-inspired Intelligent Computation, China.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510089bysuchfindings,weproposeasimplistictechnique, PreFuzz,which
improvesneuralprogram-smoothing-basedfuzzerswitha resource-
efficientedgeselectionmechanism toenhancetheirgradientguid-
anceanda probabilisticbyteselectionmechanism tofurtherboost
mutationeffectiveness.Ourevaluationresultsindicatethat PreFuzz
can significantly increase the edge coverage of Neuzz/MTFuzz, and
also reveal multiple practical guidelines to advance future research
on neural program-smoothing-based fuzzing.
ACM Reference Format:
MingyuanWuâ€ ,LingJiang,JiahongXiang,YuqunZhang*,GuoweiYang,
Huixin Ma, Sen Nie, Shi Wu, Heming Cui, and Lingming Zhang. 2022.
EvaluatingandImprovingNeuralProgram-Smoothing-basedFuzzing.In
44thInternationalConferenceonSoftwareEngineering(ICSEâ€™22),May21â€“
29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510089
1 INTRODUCTION
Fuzzing [ 44] nowadays has been widely adopted to detect soft-
ware bugs or vulnerabilities via feeding invalid, unexpected, or
randomdata asinputsfor executingprogramsunder test.Todate,
manyexistingapproaches model fuzzingasanoptimizationprob-
lem and attempt to solve it by augmenting code coverage via
mutating program seed inputs under a given time budget. Such
coverage-guided fuzzingtaskscanbetypicallyresolvedbyapplying
search-based optimization algorithms such as evolutionary algo-
rithms [13,15,42,49,51]. Specifically, test inputs are iteratively fil-
tered,mutated,andexecutedsuchthatthetestresultscanapproach
the optimal solutions to satisfy the fitness functions of the adopted
evolutionary algorithms, which are usually designed to maximize
code coverage. However, evolutionary fuzzers have been arguedthat they fail to â€œleverage the structure (i.e., gradients or higher-
orderderivatives)oftheunderlyingoptimizationproblemâ€[ 41].To
addresssuchissue,neuralprogram-smoothing-basedtechniques,
e.g.,Neuzz[41]andMTFuzz[40],havebeenrecentlyproposedto
exploit the usage of gradients for fuzzing via neural network mod-
els. Specifically, they first adopt a neural network which, given the
byte sequence of a seed as input, outputs a vector representing its
associatedprogrambranchingbehaviors.Next,theycomputethe
8472022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA M. Wu et al.
gradients of the collected output vectors with respect to the bytes
ofthegivenseed.Accordingly,theysorttheresultinggradientsand
developstrategiestomutatethebyteswithlargergradientsmore
aggressively.Eventually,alltheresultingmutantsareusedastest
casesforfuzzing.Notethat MTFuzzfurtherattemptstooutperform
Neuzzby leveraging the power of multi-task learning and adopts a
dynamicanalysismoduletoaugmentthemutationstrategy.Intheir
original papers, Neuzzoutperforms 10 existing coverage-guided
fuzzerson10real-worldprojectsbyatleast3Xmoreedgecoverage
over24-hourrunsandfurtherdetects31previously-unknownbugs.Comparedto Neuzzandfourotherstate-of-the-artfuzzers, MTFuzz
achieves 2Xto 3Xedge coverageupon allthe benchmarkprojects
andexposes11previously-unknownbugswhichcannotbedetected
by the other fuzzers.
Despite the effectiveness shown in their original papers, the
evaluation on NeuzzandMTFuzzcan be potentially biased due
totheirlimitedbenchmarksuitewithonly10projects.Moreover,
NeuzzandMTFuzzadopt a different edge coverage metric from
many existing fuzzers [ 4,9,27,31,51] that can potentially bias the
performance comparison. Furthermore, the investigation on the
factors that canimpact their edge coverage performanceis rather
limited, i.e., they only simply presented the overall effectivenessof the techniques without investigating the contributions made
byindividualcomponents,e.g.,themodelstructure,thegradient
guidance mechanism, and the mutation strategy.
In this paper, to enhance the understanding of the effectiveness
andefficiencyofprogram-smoothing-basedfuzzing,wefirstcon-
structalarge-scalebenchmarkbyretainingalltheprojectsadoptedintheoriginal NeuzzandMTFuzzpapers(exceptonethatwefailto
run)andadding19additionalopen-sourceprojectsthatwerefre-
quently adopted in recent fuzzing research work. We then conduct
an extensive evaluation for NeuzzandMTFuzzaccordingly. The
evaluationresultsuggestswhile NeuzzandMTFuzzcanoutperform
AFL on all the studied benchmark projects by 10.5% and 8.9% on
averageintermsofedgecoveragerespectively, MTFuzzdoesnotal-
waysoutperform Neuzzandboththeiredgecoverageperformances
arehighlyprogram-dependent.Wealsofindneithertheirmutation
strategies nor neural network models can be consistently effective.
Meanwhile, although the gradient guidance mechanisms can be
promising, their strengths have not been fully leveraged.
Inspiredbythefindingsofourstudy,weproposeanimproved
technique, namely PreFuzz[38], upon neural program-smoothing-
based fuzzing. In particular, we develop a resource-efficient edge
selection mechanism to facilitate the exploration on unexplored
edges rather than the already covered edges. Moreover, we also
applyaprobabilisticbyteselectionmechanism guidedbygradient
informationto NeuzzandMTFuzztofurtherboostedgeexploration.
Ourevaluationresultssuggestthat PreFuzzcansignificantlyout-
performNeuzzandMTFuzz, i.e., 43.1% more than Neuzzand 45.2%
more than MTFuzzaveragely in terms of edge coverage.
To conclude, this paper makes the following contributions:
â€¢Dataset. A dataset including 28 real-world projects that can
be used as the benchmarks for future research on fuzzing.
â€¢Study.An extensive study of neural program-smoothing-
basedfuzzersonthelarge-scalebenchmarksuite,withde-
tailed inspection of both their strengths and limitations.â€¢Technicalimprovement. Atechniqueimprovingneural
program-smoothing-based fuzzers by combining a resource-
efficient edge selection mechanism and aprobabilistic byte
selection mechanism .
â€¢Practicalguidelines. Multiplepracticalguidelinesforad-
vancing future program-smoothing-based fuzzing research.
2 BACKGROUND
2.1 Coverage-guided Fuzzers
Coverage-guided fuzzers nowadays widely adopt evolutionary al-
gorithms[ 49]formutationstrategiessincetheycanbeadvancedin
discovering program vulnerabilities without prior program knowl-
edge. In this section, we first introduce the basic framework for
evolutionaryalgorithms,andthenillustratehowatypicalcoverage-
guided fuzzer AFL integrates evolutionary algorithms.
2.1.1 EvolutionaryAlgorithm. Tosolveanoptimizationproblem,
anevolutionaryalgorithm(EA)adoptsoperationssuchasmutat-
ingtheexistingsolutionstogeneratenewsolutions.Amongsuch
generated solutions, an EA applies a fitness function to filter them
based on their quality such that the remaining ones are retained
as one population. Such process is iterated until hitting the preset
time budget with the final population returned as the solutions for
the optimization problem.
2.1.2 Integrating fuzzing with EA. Coverage-guided fuzzers often
useincreasedcodecoverageasthefitnessfunctions.Specifically,
they usually adopt edge coverage (where an edgerefers to a basic-
block-wise transition, e.g., a conditional jump in programs) to rep-
resent code coverage and retain only the seeds that can trigger
newedgecoverageforfurthermutations.Forinstance,American
Fuzzy Lop (AFL) [ 51], a widely-used coverage-guided fuzzer, is
launched by instrumenting programs such that it can acquire and
store the edge coverage of each program seed input at runtime.
Subsequently, AFL iterates and mutates each seed input according
to its adopted evolutionary algorithm. Like most coverage-guided
fuzzers [4,9,27,31], when running a seed increases edge cover-
age,AFLidentifiessuchseedasanâ€œinterestingâ€seedandretains
it for further mutations. Note that the mutations in AFL consist
of two stages: the deterministic stage (AFL ğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ ) and the
havoc stage (AFL ğ»ğ‘ğ‘£ğ‘œğ‘). In particular, AFL ğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ applies a
fixed set of mutators, e.g., the bitflip,arithmetic, and interesting
valuemutators, for respectively mutating the bits of each existing
â€œinterestingâ€ seed deterministically. After AFL ğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ , all the
collected â€œinterestingâ€ seeds are used to launch AFL ğ»ğ‘ğ‘£ğ‘œğ‘where
randommutations,i.e.,randomlychosenmutators,areiteratively
applied to the randomly selected bits of the seed inputs.
2.2 Neural Program-smoothing-based Fuzzers
Programsmoothingreferstosettingupasmooth(i.e.,differentiable)
surrogatefunctiontoapproximateprogrambranchingbehaviors
with respect to program inputs [ 41]. While traditional program
smoothing techniques [ 7,8] can incur substantial performance
overheads due to heavyweight symbolic analysis, integrating such
concept with neural network models can be rather powerful since
theycanbeusedtocopewithhigh-dimensionaloptimizationtasks,
848
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Evaluating and Improving
Neural Program-Smoothing-based Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
i.e.,toresolve(approximate)complexandstructuredprogrambe-
haviors.Tothisend, Neuzz[41]andMTFuzz[40]areproposedto
smooth programs via neural network models and guide mutations
by yielding the power of their gradients. Specifically, to formulate
theoptimizationproblemforfuzzing,theprogrambranchingbehav-
iors are defined as a function ğ¹(ğ‘¥), whereğ‘¥represents a seed input
in terms of byte sequence and the solution is a vector representing
its associated branching behaviors. For instance, a solution vector
[1,0,1,...]indicates that the first and the third edges have been
accessed/explored while the second one has not. Since ğ¹(ğ‘¥) is typi-
cally discrete, smoothing programs, i.e., making ğ¹(ğ‘¥) differentiable,
is essential to cope with the usage of gradients.
We then illustrate the rationale behind NeuzzandMTFuzz. Note
that a program execution path, i.e., a sequence of edges, can be
determinedbythebytesequenceofaseedinput.Accordingly,an
edgecanbeaccessed/exploredwhenthevalueofitscorresponding
bytessatisfiesitsaccesscondition.Otherwise,oneofitsâ€œsiblingâ€
edges(i.e.,edgesunderonesharedprefixedge)canbealternativelyaccessed.Forinstance,inFigure1,edge
ğ‘’0canbeaccessedwhenthe
valueofğ‘ ğ‘’ğ‘’ğ‘‘[ğ‘–]satisfiestheaccessconditionfor ğ‘’0,i.e.,ğ‘ ğ‘’ğ‘’ğ‘‘[ğ‘–]<1.
Hence,mutatingsuch ğ‘ ğ‘’ğ‘’ğ‘‘[ğ‘–]canleadtoexploringanewbranching
behavior, i.e., accessing ğ‘’0â€™s â€œsiblingâ€ edge ğ‘’1instead of ğ‘’0.
1. x = 1; 
2. z = x + y; 
4. if(x > seed[i]){ 
5.   y = z - x; 
6.   x++; 
8. }else{ 
9.   x = y; 
10.  y--;  
11.} 
13.x = y - z;x = 1; 
z = x + y; if(x > seed[i]){
y = z - x; x++;  x = y;  y--;
x = y - z;e0 e1seed i  + 1i â€¦  - 1i
Figure1:Anexampleofneuralprogram-smoothingrationale
NeuzzandMTFuzzassumethatneuralnetworkmodelscaniden-
tifytheâ€œpromisingâ€byte(s)(i.e.,thebyte(s)correspondingtotheac-
cess condition) for a previously explored edge. Specifically, the gra-
dient of such byte(s) (e.g., ğ‘ ğ‘’ğ‘’ğ‘‘[ğ‘–]in Figure 1) to the explored edge
is supposed to be larger than other bytes after training (illustrated
inSection2.2.1).Accordingly,mutatingsuchbyte(s)canindicate
thattheaccessconditionofthecorrespondingedgemaynotbesat-isfied,i.e.,potentiallyexploringnewâ€œsiblingâ€edges.Tosummarize,
NeuzzandMTFuzzlearntoextracttheexistingbranchingbehav-
iors to explore new edges rather than predicting â€œpromisingâ€ bytes
forunseenedges.Inparticular,theirmechanismscommonlycon-
sistoftwosteps:neuralprogramsmoothingandgradient-guided
mutations as shown in Figure 2.
2.2.1 Neural Program Smoothing. NeuzzandMTFuzzadopt an it-
erativetraining-and-mutationprocess.Undereachiteration,they
train neural network models using â€œinterestingâ€ seed inputs col-
lected in real-time (out of the â€œSeed Corpusâ€ in Figure 2). Note that
Figure 2 also shows that NeuzzandMTFuzzadopt different neural
network models which will be further illustrated in Section 2.2.3.
â€¦â€¦
âˆ‚e0
âˆ‚b0â‹¯âˆ‚e0
âˆ‚bm
â‹®â‹±â‹®
âˆ‚en
âˆ‚b0â‹¯âˆ‚en
âˆ‚bm
Gradient  
Calculation &. Sorting
Gradient-Guided 
Mutation
 CrackContext SensitiveEdge Cov.
Approch Sensitive
Seed Corpus
Mutants
MutantsNeuzz &. MTFuzz
MTFuzz
MTFuzz
MTFuzz Neuzz &. MTFuzzStage I: Neural program smoothing
Stage II: Gradient-guided mutationUpdate Seed Corpus
Figure 2: Framework of NeuzzandMTFuzz
2.2.2 Gradient-guidedMutations. Afterobtainingtheneuralnet-
workmodels, NeuzzandMTFuzzrandomlyselectadeterministic
number of the â€œinterestingâ€ seeds and the explored edges. For each
selected seed, they calculate the gradients of the selected edges
vectors with respect to all the bytes. Furthermore, all such bytes
are sorted according to their corresponding gradient rankings and
then aggregated as one vector for further mutations. In particular,
NeuzzandMTFuzzsegment each selected seed such that the bytes
inthefrontsegmentshavelargergradientsthanthebytesinthe
backsegmentsandthefrontsegmentsincludefewerbytesthanthe
back segments. Accordingly, the â€œpromisingâ€ bytes are expected to
be located in the front segments. For any segment ğ‘ ğ‘’ğ‘”, all its bytes
aresimultaneouslymutatedfor255times.Asaresult, Neuzzand
MTFuzzcan explore more mutation space of the front segments
than the back ones, i.e., mutating the more â€œpromisingâ€ bytes moreaggressively,forexploringnewbranchingbehaviors.Eventually,alltheresultingseedsaftertheiterativetraining-and-mutationprocess
are used as test cases for fuzzing.
2.2.3 MTFuzzvs.Neuzz. Figure2alsodemonstratesthat MTFuzz
differs from Neuzzby adopting multi-task learning technique and
a dynamic analysis module to augment its mutation strategy.
Inadditiontothewidely-usededgecoverage, MTFuzzadoptstwo
additionaltasksâ€”theapproach-sensitiveedgecoverage,i.e.,howfar
offanunexplorededgeisfromgettingtriggered,andthecontext-
sensitive edge coverage, i.e., the context for an explored edge, to
construct the multi-task neural network model for smoothing pro-
gramsandfurtherguidingfuzzing.Moreover, MTFuzzadoptsan
independentmodule,namely Crackinitsimplementation,which
uses dynamic programanalysis to explore new edges withoutgra-
dient information. Specifically, Crackiterates each byte of the seed
inputandmutatesittoobservewhetherthevariablesassociated
with an unexplored branch can be also changed. If so, such byte is
identified as a â€œpromisingâ€ byte to be mutated for 255 times.
3 EXTENSIVE STUDY
3.1 Benchmarks
Although NeuzzandMTFuzzhave been shown to outperform the
existing fuzzers in terms of the edge coverage in the original pa-
pers[40,41],suchresultscanbepossiblybiasedbytheusedsubject
849
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA M. Wu et al.
Table 1: Statistics of the studied benchmarks
Benchmark Class LOC Package
bison LEX&Y A C C 18,701 3.7
xmlwf XML 6,871 expat-2.2.9
mupdf PDF 123,562 1.12.0
pngimage PNG 11,373 libpng-1.6.36
pngfix PNG 12,173 libpng-1.6.36
pngtest PNG 11,323 libpng-1.6.36
tcpdump PCAP 46,892 4.99.0
nasm ASM 18,941 nasm-2.15.05
tiff2pdf TIFF 17,272 libtiff-4.2.0
tiff2ps TIFF 16,177 libtiff-4.2.0
tiffdump TIFF 15,113 libtiff-4.2.0
tiffinfo TIFF 15,014 libtiff-4.2.0
libxml XML 73,239 2.9.7
listaction SWF 6,278libming-0.4.8
listaction_d SWF 6,272libming-0.4.8
libsass SCSS 14,638 libsass-3.6.5
jhead JPEG 1,886 3.04
readelf ELF 72,111 Binutils2.30
nm ELF 55,212 Binutils2.30
strip ELF 65,683 Binutils2.30
size ELF 54,463 Binutils2.30
objdump ELF 74,710 Binutils2.30
libjpeg JPEG 8,856 9c
harfbuzz TTF 9,853 1.7.6
base64 FILE 40,332 LAVA-M
md5sum FILE 40,350 LAVA-M
uniq FILE 40,286 LAVA-M
who FILE 45,257 LAVA-M
projects. For example, 10 popular real-world projects are the main
experimentalsubjectsforboth NeuzzandMTFuzz;ho w e v er ,itis
notclearhowsuch10projectsareselectedandwhethertheexperi-
mental findings can generalize to other real-world projects.
To reduce such threat, we extend the benchmark for evaluat-
ingNeuzzandMTFuzz. In particular, in addition to retaining the
adopted9projectsintheoriginalpapers(wecouldnotsuccessfully
runproject Zliboutofthe10originalprojects),wealsoadoptaddi-
tional19projectsforourextendedevaluations.Morespecifically,to
extend our benchmark projects, we first investigate all the fuzzing
papers published in ICSE, ISSTA, FSE, ASE, S&P, CCS, USENIXSecurity, and NDSS in year 2020 and collect all their benchmark
projects.Next,wesortthecollectedbenchmarkprojectsintermsof
their usage in all the collected papers (presented in [ 38]). We then
collectthetop30mostusedbenchmarkprojectsandsuccessfully
run19ofthemwhichareeventuallyincludedinourextendedbench-
marks (thefailedexecutions aremainly causedby environmental
inconsistencies and unavailable dependencies). Table 1 presents
the statistics of our adopted benchmarks. Specifically, we consider
our benchmark to be sufficient and representative due to followingreasons:(1)tothebestofourknowledge,thisisaratherlarge-scale
benchmark suite compared with prior work; (2) the 28 collected
benchmarkscover12differentfileformatsforseedinputs,e.g.,ELF,
XML, and JPEG; and (3) the LoC of each program, ranging from
1,886 to over 120K, represents a wide range of program sizes.
3.2 Evaluation Setups
We conduct all our evaluations on Linux version 4.15.0-76-generic
Ubuntu18.04withRTX2080ti.Followingtheevaluationsetupsof
NeuzzandMTFuzz,foreachselectedbenchmarkproject,wefirst
run AFL-2.57b on a single CPU core for 1 hour to initialize ourseedcollection andthenrun Neuzz,MTFuzzandall theirvariants
(introducedin latersections)uponthe collectedseedswith atime
budgetof24hours.Notethatalltheedgeswithinthe1-hourinitial
seedcollectionareexcludedfromtheevaluationresultsinthere-
mainingsessions.Moreover,werunourexperimentsfor5timesforeachfuzzerandpresenttheaverageresultswithcloseperformance
underdifferentruns.Notethatweinstrumentallthebenchmark
projects with afl-gcc to acquire runtime edge coverage.
In addition to studying NeuzzandMTFuzz, we also include AFL
asabaselinetechniquethroughoutourextensiveevaluationsbe-cause (1) AFL is widely adopted as baseline by many fuzzing ap-
proaches[ 3,4,28,31,50]andfrequentlyupgradedforimproving
its performance; and (2) Neuzzadopts multiple concepts originated
from AFL for its implementation [39].
3.3 Research Questions
Weinvestigatethefollowingresearchquestionstoextensivelystudy
neural program-smoothing-based fuzzing.
â€¢RQ1:HowdoNeuzzandMTFuzzperformonalarge-scale
dataset?ForthisRQ,weinvestigatetheireffectivenessand
efficiencyofedgeexplorationunderourlarge-scalebench-
mark suite.
â€¢RQ2:How do the key components of NeuzzandMTFuzzaf-
fectedgeexploration?ForthisRQ,weattempttoinvestigate
howexactlytheiradoptedgradientguidancemechanisms,
neuralnetworkmodels,andmutationstrategiescanaffect
edge exploration.
3.4 Results and Analysis
3.4.1 RQ1: performance of Neuzz and MTFuzz on a large-scale
dataset.We first investigate the edge coverage performance of all
thestudiedfuzzers.Inthispaper,followingmanyexistingcoverage-
guided fuzzers [ 4,9,27,31,51], we determine to adopt the number
oftheedgesvia afl-showmap asourdefaultedgemetric.Moreover,
note that the edge metric of the original NeuzzandMTFuzzpapers
can be potentially biased since it counts the byte number of the
trace_bits structureimplementedbyAFLandthusisinconsistent
with the results provided by the guidance function (i.e., defining
â€œinterestingâ€seedsmentionedinSection2.1.2)intheirimplemen-
tations.Nevertheless,asacomprehensivestudy,wealsoevaluate
all the studied fuzzers in terms of the edge metric of the original
NeuzzandMTFuzzpapers.
Table2presentstheedgecoverageresultsofourextensivestudy
forNeuzzandMTFuzzunder bothadopted metrics. For instance,
forAFLunder bison,10,374correspondstoourdefaultedgemetric
and 308 corresponds to the original metric in the Neuzz/MTFuzz
papers. For our default edge metric, we can observe that Neuzz
significantly outperforms AFL by 10.5% (22,395 vs. 20,265 explored
edges)intermsofedgecoverageonaverage.Comparedwiththe
performanceadvantage claimedinits originalpaper(i.e., 2.7X), it
isclearlydegraded.Wetheninvestigatetheperformancedifferenceamongbenchmarkprojects.Interestingly,wecanobservethattheir
performance advantage is rather inconsistent, i.e., ranging from
-31.2%to180.5%.Moreover, NeuzzonlyoutperformsAFLupon10
outof19extendedprojects.Suchresultssuggestthat Neuzzcannot
850
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Evaluating and Improving
Neural Program-Smoothing-based Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 2: Edge coverage results of all the studied approaches
Benchmarks AFL Neuzz Neuzz ğ‘…ğ‘’ğ‘£ MTFuzz MTFuzz ğ‘…ğ‘’ğ‘£ MTFuzz ğ‘‚ğ‘“ğ‘“ Neuzz ğ¶ğ‘ğ‘ Neuzz ğ‘…ğ‘ğ‘ Neuzz ğµğ‘…ğ‘ğ‘
bison 10,374(308) 12,260(432) 12,218(264) 13,812(599) 12,799(470) 12,801(524) 12,375(475) 12,592(429) 12,444(499)
xmlwf 13,729(3,272) 10,499(2,200) 9,192(1,644) 10,853(509) 10,532(457) 10,752(476)10,872(2,794) 11,465(2,891) 11,469(2,831)
mupdf 13,665(361) 16,705(795) 17,664(654) 16,603(328) 16,348(237) 16,522(334) 16,853(796) 16,921(792) 16,889(804)
pngimage 4,077(201) 3,369(324) 2,522(199) 2,347(200) 2,172(198) 2,373(194) 2,946(241) 2,553(197) 3,011(323)
pngfix 7,134(135) 5,181(85) 4,564(71) 5,767(80) 5,350(71) 5,737(73) 5,157(74) 5,194(74) 5,247(76)
pngtest 3,185(68) 2,828(29) 2,933(8) 3,166(56) 2,719(45) 3,016(81) 3,074(46) 3,271(58) 3,103(42)
tcpdump 12,434(3,525) 18,293(4,310) 18,566(5,005) 17,026(5,512) 15,097(3,715) 17,463(4,621) 18,091(4,495) 18,910(4,635) 19,411(4,581)
nasm 33,633(1,768) 34,788(1,654) 33,838(1,652) 34,958(1,754) 34,451(1,722) 33,907(1,786) 35,375(1,791) 34,528(1,695) 35,009(1,899)
tiff2pdf 45,183(4,844) 47,109(4,365) 42,519(3,993) 44,765(4,355) 38,449(3,676) 44,230(4,044) 46,934(3,971) 44,617(3,765) 50,347(4,580)
tiff2ps 20,862(3,621) 23,705(3,634) 21,063(3,420) 22,671(3,131) 16,700(2,535) 21,817(3,194) 23,931(3,743) 21,322(3,841) 23,160(3,791)
tiffdump 2,416(8) 3,239(52) 3,117(52) 2,617(64) 2,262(38) 2,509(61) 3,124(46) 2,962(47) 3,052(43)
tiffinfo 11,964(2,440) 15,853(2,618) 15,742(2,407) 13,785(2,799) 10,249(1,431) 12,394(1,904) 14,698(2,788) 13,239(2,649) 15,569(2,379)
libxml 20,064(541)31,340(1,553) 32,075(1,765) 29,236(1,635) 29,162(1,553) 27,902(1,205) 31,421(1,687) 31,774(1,695) 31,731(1,649)
listaction 21,340(3,151) 17,945(2,893) 14,969(2,328) 13,382(622) 12,257(559) 12,356(591)17,743(2,791) 17,562(2,822) 17,073(2,770)
listaction_d 31,617(2,728) 25,006(4,604) 18,643(3,460) 26,629(3,376) 21,644(2,554) 23,619(2,780) 25,869(5,036) 28,436(5,271) 23,622(4,784)
libsass 198,976 (10,385) 162,717(8,492) 158,800(8,438) 132,972(7,373) 132,491(7,232) 132,644(7,106) 154,793(8,742) 160,318(8,902) 163,492(8,527)
jhead 2,082(28) 1,433(24) 1,566(27) 1,268(18) 1,215(16) 1,273(16) 1,327(22) 1,560(22) 1,502(23)
readelf 14,329(898)40,186(6,059) 34,994(4,868) 42,173(6,684) 35,178(5,799) 40,005(6,161) 42,889(6,257) 44,389(6,159) 32,796(5,314)
nm 11,154(1,351) 16,159(2,226) 13,505(2,015) 31,402(3,707) 28,027(3,128) 22,149(3,045) 18,070(2,312) 20,724(3,132) 16,007(4,099)
strip 20,536(1,409) 32,791(3,254) 31,604(3,348) 41,520(5,172) 35,649(5,699) 32,072(3,674) 33,549(3,649) 33,816(3,916) 33,348(3,753)
size 10,730(1,188) 14,197(2,450) 12,414(2,036) 18,675(3,872) 16,525(3,397) 12,623(2,087) 14,488(2,606) 14,254(2,540) 12,284(2,480)
objdump 15,492(247)31,808(2,358) 28,617(1,850) 31,507(2,007) 27,227(2,117) 30,074(2,270) 31,176(2,954) 33,165(2,639) 33,486(3,062)
libjpeg 8,197(266)16,037(1,600) 13,460(1,566) 9,038(876) 8,446(797) 8,255(487)16,576(1,729) 16,859(1,713) 17,566(1,892)
harfbuzz 26,420(3,107) 35,502(5,982) 28,037(5,606) 44,342(6,268) 37,821(4,930) 44,364(6,155) 45,179(7,542) 48,959(7,656) 50,911(7,764)
base64 1,344(12) 1,202(0) 987(0) 935(0) 912(0) 819(0) 1,247(0) 1,226(0) 1,243(0)
md5sum 2,871(33) 3,168(131) 3,004(37) 3,101(35) 3,036(35) 3,044(35) 3,097(33) 3,241(33) 3,163(35)
uniq 713(2) 756(2) 750(2) 725(0) 728(0) 716(0) 755(2) 754(2) 752(2)
who 2,917(14) 2,973(17) 2,919(17) 2,680(15) 2,753(17) 2,720(17) 2,997(17) 3,031(17) 3,026(17)
Average 20,265(1,640) 22,395(2,219) 20,724(2,026) 22,070(2,180) 20,007(1,872) 20,648(1,890) 22,665(2,380) 23,130(2,414) 22,883(2,429)
(a)Neuzz/AFL
 (b)MTFuzz/AFL
Figure 3: Edge coverage advantage of the fuzzers over AFL
always outperformAFL andthe performance advantageof Neuzz
over AFL can be program-dependent.
Wealsoobservethat Neuzzoutperforms MTFuzzby1.5%(22,395
vs. 22,070 explorededges) averagely in termsof edge coverage on
all benchmark projects. While on 11 of 28 total projects, MTFuzz
outperforms Neuzzby 20.8% averagely, Neuzzoutperforms MT-
Fuzzby 17.7% on the other 17 projects. Furthermore, even AFL
outperforms MTFuzzby33.6%averagelyonatotalof11projects.
Suchresultsindicatethatsimilarto Neuzz,MTFuzzcannotperform
consistently either.
We then attempt to reveal the characteristics of how the edge
coverageperformancevariesamongthestudiedprojects.Tothis
end, we delineate the correlation between the edge coverage ad-
vantage of the studied fuzzers compared with AFL and the size
of their studied projects via the Pearson Correlation Coefficient
analysis[ 1].Figure3presentssuchresultsof NeuzzandMTFuzz.In
each subfigure, the horizontal axis denotes the LoC of each project
andtheverticalaxisdenotestheratioasdividingtheedgecoverage
result of each studied approach by the edge coverage result of AFL.
We can observe that overall, the correlation is rather strong (at the
significance level of 0.05), i.e., all the studied approaches can result
inlargeredgecoverageimprovementoverAFLuponlargerprojectsthansmallerones.Suchresultsclearlydemonstratethatprogram
size can significantly impact the edge coverage performance of
neural program-smoothing-based fuzzers.
Weobservesimilardatatrendsintermsoftheedgemetricinthe
originalNeuzz/MTFuzzpapers.Inparticular, Neuzzcanoutperform
AFL by 35.3% (2,219 vs. 1,640 explored edges) and can outperform
MTFuzzby1.8%(2,219vs.2,180explorededges).Notethatunder
such measure, for certain projects, e.g., base64,NeuzzandMTFuzz
explore zero edges after excluding the edges from 1-hour initial
seedcollection.Suchresultscouldbemisleadingthatthestudied
fuzzersperformequallypoorin base64,whilesuchperformance
gaps can be clearly presented by our default edge metric.
Finding 1: The performance of Neuzz and MTFuzz canbe largely program-dependent. Interestingly, such program-
smoothing-based fuzzers tend to perform better on larger pro-
grams.
Notethatrandomnessisinjectedtomanyexistingfuzzers[ 4,27,
31,50] for selecting bytesto guide mutations, e.g., AFLğ»ğ‘ğ‘£ğ‘œğ‘.H o w -
ever,NeuzzandMTFuzzutilize only deterministic mutation strate-
gies, i.e., adopting no randomness for selecting bytes which can be
deterministically identified based on their corresponding gradient
ranking. Therefore, we further investigate the edge exploration
efficiencyofrandombyteselectiontoinferwhetherincludingthem
inNeuzzandMTFuzzcan be potentially beneficial. Specifically, we
involveAFLinafine-grainedmanner,i.e.,itsdeterministicstage
AFL ğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ and the havoc stage AFL ğ»ğ‘ğ‘£ğ‘œğ‘(i.e., essentially
therandombyteselectionmechanism)bothofwhichenablenon-
deterministic execution time, for performance comparison with
NeuzzandMTFuzz.
Figure 4 presents our evaluation results in terms of the explored
edge number per second, namely Edge Discovery Rate (EDR) in this
851
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA M. Wu et al.
paper,ofNeuzz,MTFuzz,AFL,AFLğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ andAFLğ»ğ‘ğ‘£ğ‘œğ‘.W e
canobservethatoverall, NeuzzandMTFuzzcanoutperformAFLby
10.2%and8.5%respectively.Interestingly, AFL ğ»ğ‘ğ‘£ğ‘œğ‘achievesthe
highest EDR, i.e., 21.8X larger than AFL ğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ , 7.7X larger
thanNeuzz,and7.8Xlargerthan MTFuzzaveragelyonallthebench-
marks. Accordingly, we can derive that AFLğ»ğ‘ğ‘£ğ‘œğ‘can significantly
augment edge exploration, i.e., it promptly explores edges upon
thelimitedseedinputsprovidedby AFL ğ·ğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘ .Suchresult
is enlightening that applying random byte selection mechanism
to neural program-smoothing fuzzers can potentially boost edge
exploration.
Finding 2: AFL ğ»ğ‘ğ‘£ğ‘œğ‘dominates the efficiency of edge ex-
ploration, indicating that it is promising to augment edge
exploration by adopting random byte selection mechanism.
00.81.62.43.2
EDR EDR EDR EDR EDR0.255 0.2592.256
0.0990.235
AFL AFLDeterministic AFLHavoc NeuzzEdge  Discover  Rate
MTFuzz
Figure 4: EDR of the studied approaches
3.4.2 RQ2: Effectiveness of the key components.
Gradientguidance. Theinconsistenciesbetweenourfinding
andthedeclaredresultsintheoriginalpapers(i.e.,finding1)inspire
us to further investigate the performance impact of the adopted
mechanisms of NeuzzandMTFuzz. To this end, we determine to
firstinvestigatetheeffectivenessoftheirdominatingfactor,i.e.,thegradient guidance mechanism. In particular, since such mechanismis proposed to facilitate the mutations on the â€œpromisingâ€ bytes foredgeexplorationviagradientcomputation,ourpurposeistoinves-
tigate whether their derived gradients can locate such bytes. More
specifically,weproposeanintuitivegradientguidancemechanismâ€”
instead of aggressively mutating the bytes with larger gradients in
the original NeuzzandMTFuzz, we aggressivelymutate the bytes
withsmallergradients.Suchmechanismisinjectedto Neuzzand
MTFuzztoform theirvariants Neuzz ğ‘…ğ‘’ğ‘£andMTFuzz ğ‘…ğ‘’ğ‘£.Wethus
evaluateNeuzz ğ‘…ğ‘’ğ‘£andMTFuzz ğ‘…ğ‘’ğ‘£to observe their performance
differencefromtheoriginal NeuzzandMTFuzztoinvestigatethe
effect of the gradient guidance mechanisms.
WecanobservefromTable2that Neuzzcanexplore8.1%(1,671)
moreedgesthan Neuzz ğ‘…ğ‘’ğ‘£andMTFuzzcanexplore10.3%(2,063)
more edges than MTFuzz ğ‘…ğ‘’ğ‘£on average. Such consistent results
suggest that larger gradients can be a better indicator to promising
bytes, i.e., the derived gradients can reflect promising bytes.
Interestingly, Neuzz ğ‘…ğ‘’ğ‘£can outperform Neuzzon 5 out of 28
projects,i.e., libxml,mupdf,jhead,tcpdumpandpngtest.Mean-
while,MTFuzz ğ‘…ğ‘’ğ‘£can outperform MTFuzzunder uniqandwho.
Suchresultsalsoindicatethatthepowerofthegradientguidance
inNeuzzandMTFuzzhas not been completely leveraged.Finding 3: Although the gradient guidance mechanisms
adoptedbyNeuzzandMTFuzzareoveralleffectiveforiden-
tifyingthepromisingbytes,theirperformancecanberather
unstable on some programs.
DNNmodels. Nowthatthegradientsderivedby NeuzzandMT-
Fuzzcan be proven to be effective in reflecting promising bytes for
mutations, we further investigate how their corresponding neural
networkmodelsimpactedgeexploration.Specifically,sincecom-
pared toNeuzz,MTFuzzenables the independent dynamic analysis
moduleCrackto augment their mutation strategy, we turn it off
andformitsvariant MTFuzz ğ‘‚ğ‘“ ğ‘“,i.e.,applyingthemutationstrat-
egyofNeuzzinMTFuzz,suchthattheyonlydifferintheadopted
neural network models. Moreover, we also include the Convolu-
tional Neural Network (CNN) [ 26] model and two commonly-used
Recursive Neural Network (RNN) [ 14] models, i.e., LSTM [ 24] and
Bi-LSTM [ 22], and adopt them in the original Neuzzto form its
variantsNeuzz ğ¶ğ‘ğ‘,Neuzz ğ‘…ğ‘ğ‘, andNeuzz ğµğ‘…ğ‘ğ‘. Note that we
investigatemore RNN-based modelssincetheyare typicallyused
inlearning thedistribution over asequenceto predictthe future
symbolsequence[ 10](e.g.,forspeechrecognition)andexpectedto
better match the program input features than CNN-based models.
Eventually, we determine to evaluate Neuzzand all the variant
techniquestodetecthowmultipleneuralnetworkmodelsimpact
the edge exploration of program-smoothing-based fuzzers. Notethat their hyper-parameter setups are introduced in our GitHub
page [38].
We can observe from Table 2 that overall, all our studied ap-
proaches perform similarly in terms of edge coverage. Specifically,
Neuzzslightlyoutperforms MTFuzz ğ‘‚ğ‘“ ğ‘“by8.5%(22,395vs20,648ex-
plorededges), underperforms Neuzz ğ¶ğ‘ğ‘by1.2% (22,395vs.22,665
explored edges), Neuzz ğ‘…ğ‘ğ‘by 3.3% (22,395 vs. 23,130 explored
edges) and Neuzz ğµğ‘…ğ‘ğ‘by 2.2% (22,395 vs. 22,883 explored edges).
Meanwhile,wecanalsoobservethatnoneofthestudiedapproachescandominateontopofallthestudiedprojects,i.e., Neuzzdominates
7,MTFuzz ğ‘‚ğ‘“ ğ‘“dominates 2, Neuzz ğ¶ğ‘ğ‘dominates 4, Neuzz ğ‘…ğ‘ğ‘
dominates 9, and Neuzz ğµğ‘…ğ‘ğ‘dominates 6. Therefore, we derive
that upgrading neural network models cannot significantly impact
the performance of edge exploration.
Finding4:Differentneuralnetworkmodelshavelimitedim-
pactontheeffectivenessofprogram-smoothing-basedfuzzing.
Mutation Strategies. We then investigate the impact from the
mutation strategy of the neural program-smoothing-based fuzzers.
Specifically,since MTFuzzdiffersfrom Neuzzmainlybyenabling
Crackformutationsandtheirrespectiveneuralnetworkmodelsdo
not significantly impact the edge exploration (reflected by Finding
4), we concentrate our investigation on the impact from Crack.T o
this end, we evaluate MTFuzzandMTFuzz ğ‘‚ğ‘“ ğ‘“. Table 2 demon-
strates that overall, MTFuzzcan outperform MTFuzz ğ‘‚ğ‘“ ğ‘“by 6.9%
(22,070vs.20,648explorededges).However,suchadvantagecanbe
rather inconsistent, ranging from -2.5% to 47.9% upon individual
projects.Ontheotherhand,applying Crackcanbepotentiallycost-
ineffectivesinceitisquiteheavyweight.Therefore,itisessential
852
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Evaluating and Improving
Neural Program-Smoothing-based Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
to consider whether it is worthwhile in applying such technique
for neural program-smoothing-based fuzzing.
Finding 5: The dynamic analysis module Crack adopted by
MTFuzz can be cost-ineffective.
3.5 Discussion
We first discuss why neural network models do not significantly
impact the edge coverage performance. To this end, we ought to
understand the effect of the adopted neural network models ofNeuzzandMTFuzz. In particular, note that neural networks are
usually used for data prediction, i.e., learning and generalizinghistorical data to predict unseen data. Accordingly, researchers
havedevelopedmanyneuralnetworkmodelstostrengthentheir
generalization and prediction capabilities. Therefore, one may mis-
understand that NeuzzandMTFuzzattempt to use neural network
models to predict the bytes corresponding to unexplored edges.
Instead, as a matter of fact, NeuzzandMTFuzzleverage neural net-
workmodelswhichcomputethegradientstoreflecttherelations
between explored edges and seed inputs, i.e., mutating the byte
correspondingtoalargergradientcanbemorelikelytoexplorea
newedgeotherthantheexistingedgeunderonesharedprefixedge.
As a result, any neural network model can be applied as long as it
cansuccessfullydelivergradientstoreflectsuch explorededgeâ€”seed
inputrelations, i.e., how its generalization or prediction capability
doesnotquitematterundersuchscenarios.Therefore,itisquite
likely that a simplistic model (e.g., feed-forwarded network model
adopted by Neuzz) can perform similarly as fine-grained models
(e.g.,multi-tasklearningmodeladoptedby MTFuzzandtheRNN
models adopted by the studied Neuzzvariants).
We then attempt to illustrate why NeuzzandMTFuzzcannot al-
waysbeeffective.Notethateventhough NeuzzandMTFuzzenable
gradientguidancemechanismstoexplorenewedges,theiriterative
training-and-mutationstrategyviarandomlyselectingedgesand
seeds in the beginning can nevertheless select existing edges otherthan unexplored edges to compute gradients (illustrated in Section
2.2.2), i.e., they still allow inefficient mutations. Specifically for the
smaller programs where NeuzzandMTFuzzcannot outperform
AFL, their edge exploration converges faster than larger programs
due to the limited number of edges, i.e., they have a higher chance
to select an existing edge whose â€œsiblingâ€ edges have already been
exploredbyotherseedsforgradientcomputation.Thus,itcanbe
difficult to mutate its â€œpromisingâ€ bytes for exploring new edges.
4PREFUZZ
Our findings reveal that we can possibly leverage the power of the
gradientguidancemechanismtoenhancetheedgeexplorationof
neural program-smoothing-based fuzzers. To this end, we propose
PreFuzz(Probabilistic resource-efficientprogram-smoothing-based
Fuzzing).Figure5presentstheworkflowof PreFuzz.PreFuzzfirst
trains a neural network model by applying all the existing seedsas the training set. Next, PreFuzzadopts a resource-efficient edge
selectionmechanism toselectedgesforgradientcomputation.Then,
thegradientinformationisutilizedtogeneratemutantsforfuzzing.Notethatamutantwhichexploresnewedgescanbeusedasaseedâ€¦â€¦
Gradient  
Calculation &. Sorting
Seed Corpus
Mutants
M tants
Gradient-Guided 
Mutation
PBS
PreFuzz Neuzz &. MTFuzzStage I: Neural program smoothing
Stage II: Gradient-guided mutationUpdate 
 Seed Corpus
âˆ‚e0
âˆ‚b0â‹¯âˆ‚e0
âˆ‚bm
â‹®â‹±â‹®
âˆ‚en
âˆ‚b0â‹¯âˆ‚en
âˆ‚bm
Resource-Esosoce
ff
ï¬ï¬
f
fffent
ciece
Edge Selection Mechanism en
Mec i
NN Training
Figure 5: Framework of PreFuzz
for further edge exploration. Meanwhile, PreFuzzadoptsprobabilis-
ticbyteselectionmechanism (PBSinFigure5)tofacilitatemutations.
4.1 The Details
4.1.1 Resource-EfficientEdgeSelectionMechanism. Thepurposeof
theresource-efficient edge selection mechanism is to prevent explor-
ingtheexistingbranchingbehaviors(i.e.,edges).Tothisend,our
mechanism is designed to identify the edge worthy being explored
for later selecting and mutating its corresponding byte. Intuitively,
when one edge can identify the number of its â€œsiblingâ€ edges (as
defined in Section 2.2),such edge number can be a potentialindica-
tor whether the given edge should be included for further gradient
computation. More specifically, the more â€œsiblingâ€ edges have beenexplored,thelesslikelynewâ€œsiblingâ€edgescanbeexploredviathe
gradient computation for the given edge.
Algorithm 1 presents the details of the resource-efficient edge
selection mechanism . First, it is quite essential to acquire the run-
time edge exploration states, e.g., the number of â€œsiblingâ€ edges of
agivenedgeandhowmanyhavebeenexplored(lines2to3).To
thisend,wedecompiletheassembly-levelprograms,parsethem
to the instructions via AFL-specific instrumentation, and construct
theedgeexplorationstatesviastaticallyanalyzingtheparsedin-
structions. Next, given one edge, we derive the ratio of its explored
â€œsiblingâ€ edgenumber overits totalâ€œsiblingâ€ number (lines5 to9).
Ifsuchratioislowerthanapresetthreshold,weretainthegiven
edgeandstoresitina CandidateEdgeSet wherewelaterrandomly
select such edges for further gradient computation (lines 10 to 12).
We use Figure 1 to further illustrate such algorithm. Assuming
thatğ‘’0canbeexploredgiventheâ€œseedâ€inFigure1,mutatingthe
byte of the given seed corresponding to the access condition of
ğ‘’0canexploreitsâ€œsiblingâ€edge ğ‘’1.WhileNeuzzandMTFuzzare
designedtoperformsuchmutationforedgeexploration, ğ‘’1could
have nevertheless been explored already due to the randomnessinjected to their mechanisms (illustrated in Section 2.2.2). Thus,
the effectiveness of the gradient guidance mechanism may be com-
promised. However, our resource-efficient edge selection mechanism
cancollecttheexplorationinformationoftheâ€œsiblingâ€edgeof ğ‘’0,
i.e.,ğ‘’1, before computing the gradient for ğ‘’0. If it finds out that
ğ‘’1has already been explored, it would not select ğ‘’0for gradient
computation in the first place to save the computing resource.
853
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA M. Wu et al.
(a)Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› /Neuzz
 (b)Neuzz ğ‘ƒğ‘Ÿğ‘œğ‘/Neuzz
 (c)PreFuzz/Neuzz
Figure 6: Edge coverage ratio upon Neuzz
Algorithm 1 Candidate Edge Set Construction
Input: threshold, exploredEdge
Output:selectedEdges
1:function CONSTRUCT_CANDIDATE_EDGE_SET
2:candidate â†set()
3:correspRelation â†getEdgeRelation ()
4:foredge in exploredEdge do
5: explored â†0
6: siblingsâ†|correspRelation[edge]|
7: forneighbour in correspRelation[edge] do
8: ifneighbour in exploredEdge then
9: explored â†explored + 1
10: ifexplored/siblings <threshold then
11: candidate.add(edge)
12:selectedEdges â†randomlySelectFromSet (candidate)
13:returnselectedEdges
Table 3: Edge coverage results of PreFuzz
Benchmarks AFL Neuzz MTFuzz Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› Neuzz ğ‘ƒğ‘Ÿğ‘œğ‘ PreFuzz
bison 10,374 12,260 13,812 13,003 13,744 15,078
xmlwf 13,729 10,499 10,853 12,290 16,960 21,009
mupdf13,665 16,705 16,603 17,002 19,995 21,203
pngimage 4,077 3,369 2,347 2,883 2,838 4,876
pngfix 7,134 5,181 5,767 7,307 7,422 7,930
pngtest 3,185 2,828 3,166 3,993 4,199 4,703
tcpdump 12,434 18,293 17,026 19,764 30,767 34,947
nasm 33,633 34,788 34,958 37,534 41,973 43,628
tiff2pdf45,183 47,109 44,765 51,506 50,555 57,172
tiff2ps 20,862 23,705 22,671 23,649 24,247 29,332
tiffdump 2,416 3,239 2,617 3,590 3,554 3,888
tiffinfo 11,964 15,853 13,785 17,157 17,572 21,839
libxml 20,064 31,340 29,236 32,161 40,935 47,689
listaction 21,340 17,945 13,382 20,208 29,161 32,447
listaction_d 31,617 25,006 26,629 26,351 40,355 46,762
libsass 198,976 162,717 132,972 169,936 215,510 218,130
jhead 2,082 1,433 1,268 1,952 2,463 2,464
readelf 14,329 40,186 42,173 40,396 47,727 53,859
nm 11,154 16,159 31,402 19,040 22,605 30,709
strip 20,536 32,791 41,520 33,864 37,705 42,943
size 10,730 14,197 18,675 14,734 15,261 19,231
objdump 15,492 31,808 31,507 34,036 38,983 43,195
libjpeg 8,197 16,037 9,038 17,192 22,615 24,365
harfbuzz 26,420 35,502 44,342 47,877 45,412 60,333
base64 1,344 1,202 935 1,454 1,631 1,644
md5sum 2,871 3,168 3,101 3,415 3,580 3,518
uniq 713 756 725 792 794 795
who 2,917 2,973 2,680 3,262 3,255 3,491
Average 20,265 22,395 22,070 24,155 28,636 32,042
4.1.2 Probabilistic Byte Selection Mechanism. Inspired by Finding
2, we further inject an additional nondeterministic stage to neural
program-smoothing fuzzers. To this end, we develop a ProbabilisticByteSelectionMechanism andappenditto Neuzztoexpandedge
exploration. Note that the probabilistic byte selection mechanism
utilizes the gradient information generated by the resource-efficient
edgeselectionmechanism ,andgetsactivatedafterthemutationstage
inherited from Neuzz. This stage contains three steps: (1) dividing
each seed input into segments, (2) selecting segments by gradient-
based probability distribution, and (3) randomly selecting bytes
from the selected segment for mutation via AFL ğ»ğ‘ğ‘£ğ‘œğ‘mutators.
UnlikeAFL ğ»ğ‘ğ‘£ğ‘œğ‘which randomly selects bytes from the whole
seed, we first divide a seed into a constant number (8 by default in
ourpaper)ofequal-lengthsegments.Wethenselectseedsegments
based on their probabilities. Note that while intuitively leveraging
byte-wiseprobabilitydistributionforbyteselectionismorenatural,
this is essentially deterministic and excludes the benefits of ran-
domness(asinFinding2).Therefore,ourprobabilitydistributionis
established uponseed segments ratherthan individualbytes so as
to leverage the power of randomness and AFL ğ»ğ‘ğ‘£ğ‘œğ‘.
Next, we calculate the fitness score for each segment, presented
inEquation1,where/summationtext.1ğ‘ ğ‘’ğ‘”ğ‘–
ğ‘—=1ğ‘”ğ‘Ÿğ‘ğ‘‘ ğ‘—denotesthegradientsumforall
thebyteswithinagivensegment ğ‘ ğ‘’ğ‘”ğ‘–,ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘ ğ‘’ğ‘”ğ‘–)denotesitsbyte
number,andthefitnessscoreforagivensegment ğ‘ ğ‘’ğ‘”ğ‘–iscomputed
as the average gradient of all the bytes within ğ‘ ğ‘’ğ‘”ğ‘–.
ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  ğ‘ ğ‘’ğ‘”ğ‘–=/summationtext.1ğ‘ ğ‘’ğ‘”ğ‘–
ğ‘—=1ğ‘”ğ‘Ÿğ‘ğ‘‘ ğ‘—
ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„(ğ‘ ğ‘’ğ‘”ğ‘–)(1)
Accordingly,theprobability ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘ ğ‘’ğ‘”ğ‘–forselectingasegment ğ‘ ğ‘’ğ‘”ğ‘–
for mutation is presented in Equation 2, i.e., the ratio of the fitness
score ofğ‘ ğ‘’ğ‘”ğ‘–over the total fitness scores of all the segments.
ğ‘ƒğ‘Ÿğ‘œğ‘ ğ‘ ğ‘’ğ‘”ğ‘–=ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  ğ‘ ğ‘’ğ‘”ğ‘–/summationtext.1ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
ğ‘—=1ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  ğ‘ ğ‘’ğ‘” ğ‘—(2)
Finally, we apply AFL ğ»ğ‘ğ‘£ğ‘œğ‘to mutate the selected segments. In
particular, AFLğ»ğ‘ğ‘£ğ‘œğ‘randomly selects a byte from the segment for
mutationbasedonitsmechanism.Notethatifthemutantsarealso
â€œinterestingâ€,theyareretainedforfurthergradientcomputationand
theprobabilistic byte selection mechanism . Such process is iterated
until hitting the time budget.
4.2 Performance Evaluation
Weattempttoevaluatetheperformanceof PreFuzzanditstechnical
components respectively. To evaluate the usage of the resource-
efficientedgeselectionmechanism andtheprobabilisticbyteselection
mechanism , we form two Neuzzvariants, i.e., Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
854
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Evaluating and Improving
Neural Program-Smoothing-based Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
whichinjects resource-efficientedgeselectionmechanism toNeuzz
andNeuzz ğ‘ƒğ‘Ÿğ‘œğ‘which appends the probabilistic byte selection mech-
anismtoNeuzz.Notethatweretain Neuzz,MTFuzz,andAFLasour
baselines for performance comparison. The experimental setups in
thissectionfollowthesamesettingsinSection3.2.The threshold
for Algorithm 1 is set to 0.41.
4.2.1 Edgeexplorationeffectiveness. Table3presentstheexperi-
mentalresultsofedgeexplorationeffectiveness.Wecanfindthat
overall,PreFuzzoutperforms all the existing baselines in terms
of edge coverage averagely, e.g., PreFuzzcan outperform AFL by
58.1%(32,042vs.20,265explorededges)and Neuzzby43.1%(32,042
vs.22,395 explorededges).Notethat undertheoriginally adopted
metric of edge coverage, PreFuzzalso outperforms NeuzzandMT-
Fuzzby34.3%and36.7%.Suchresultssuggestthatcombiningthe
resource-efficientedgeselectionmechanism andtheprobabilisticbyte
selectionmechanism forNeuzzcanberatherpowerful.Moreover,
Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› outperforms Neuzzby 7.9% (24,155 vs. 22,395
explored edges) and MTFuzzby 9.4% (24,155 vs. 22,070 explored
edges). Specifically, Neuzzobtains 271more edges averagelythan
Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› on2projectswhile Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› obtains
1,917 more edges averagely than Neuzzon the rest 26 projects.
Such results indicate that the resource-efficient edge selection mech-
anismcan enhancethe overalleffectiveness of Neuzz. Inaddition,
Neuzz ğ‘ƒğ‘Ÿğ‘œğ‘alsooutperforms Neuzzby27.9%(28,636vs.22,395ex-
plored edges) and MTFuzzby 29.8% (28,636 vs. 22,070 explored
edges). Such results demonstrate that introducing randomness can
alsosignificantlyincreasetheedgecoverageoftheneuralprogram-
smoothing-based fuzzers.
Figure 6 presents the correlation between the edge coverage ad-
vantageof Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ,Neuzz ğ‘ƒğ‘Ÿğ‘œğ‘,PreFuzzoverNeuzzby
dividing their corresponding edge coverage results and the LoC of
the studied benchmark projects. Interestingly, we can observe that
the correlation is rather weak, i.e., all presented ğ‘values (0.0688,
0.2211 and 0.1602) fail to reach the significance level of 0.05. It in-
dicatesthattheedgecoverageadvantageovertheoriginal Neuzzis
notaffectedbytheprogramsize.Moreover,suchadvantageisrather
consistent.Specifically,wedeterminetouseCoefficientofVariation
(CV) [5], a widely-used metric for measuring the dispersion of a
probability distribution [35,37, 48], to measure theconsistency of
their performance improvement. Note that a lower CV indicates a
more consistent performance improvement. As a result, PreFuzz,
Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› , andNeuzz ğ‘ƒğ‘Ÿğ‘œğ‘can achieve 19.6%, 11.5%, and
17.4%ofCVfortheirp erformanceimprovementover Neuzz,which
areallsignificantlyreducedcomparedwiththeCVof Neuzz(37.6%)
for its improvement overAFL. Therefore, we summarize thatour
proposedmechanismscansignificantlyandconsistentlystrengthentheneuralprogram-smoothing-basedfuzzers.Notethatwefindun-dertheedgecoveragemetricadoptedintheoriginal Neuzz/MTFuzz
papers, the performance gain of PreFuzzoverNeuzzis 34.3% (2,981
vs. 2,219 explored edges) which is also rather significant.
Figure7presentstheaveragetimetrendofedgecoveragewithin
24hoursforAFL, MTFuzz,NeuzzandPreFuzzamongallthebench-
mark projects. We can observe that at any time being, PreFuzzcan
outperform other fuzzers significantly in terms of edge coverage.
1WealsoevaluatemorethresholdsetupsandpresenttheresultsinourGitHublink[ 38]
which indicate that changing threshold setups incurs limited performance impact.10,00016,25022,50028,75035,000
0 4 8 12 16 20 24AFL Neuzz MTFuzzAverage Edge Coverage
Time (hours)Prefuzz
Figure 7: Edge coverage of PreFuzz in terms of time
4.2.2 In-depth Ablation Study. In this section, we further perform
in-depth ablation studies to investigate the efficacy of our resource-
efficient edge selection mechanism andprobabilistic byte selection
mechanism respectively. Specifically for the resource-efficient edge
selection mechanism , we find that overall, 24.0% edges do not need
tobeexploredbyapplying Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› undereachiteration
averagely (1,230 vs. 935 edges). Moreover, the probabilistic byte
selectionmechanism inPreFuzzismoreefficientwhencombining
withtheresource-efficientedgeselectionmechanism sincePreFuzz
explores averagely 11.9% more edges than Neuzz ğ‘ƒğ‘Ÿğ‘œğ‘(32,042 vs.
28,636explorededgesinTable3).Suchresultsindicatethatapplying
theresource-efficient edge selection mechanism can significantly
savetheeffortonexploringtheedgeswhichcannotcontributeto
increasing edge coverage.
We further investigate the probabilistic byte selection mecha-
nismintermsof EdgeDiscoveryRate .Tothisend,wealsoinclude
AFL ğ»ğ‘ğ‘£ğ‘œğ‘,Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› , the gradient-guided mutation stage
ofPreFuzz,andtheprobabilisticbyteselectionstageof PreFuzz(rep-
resented as PreFuzz ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡andPreFuzz ğ‘ƒğ‘Ÿğ‘œğ‘, respectively) for per-
formance comparison. Note that PreFuzz ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡andPreFuzz ğ‘ƒğ‘Ÿğ‘œğ‘
resultsareextractedfromthetwostagesofacomplete PreFuzzrun,
e.g.,PreFuzz ğ‘ƒğ‘Ÿğ‘œğ‘utilizes the resource-efficient edge selection mecha-
nismtoselectedgesforcomputingtheirgradientswhile Neuzz ğ‘ƒğ‘Ÿğ‘œğ‘
randomly selects explored edges for gradient computation. Fig-
ure 8 presents our evaluation results. We can observe that overall,
PreFuzz ğ‘ƒğ‘Ÿğ‘œğ‘cansignificantlyoutperformalltheotherstudiedap-
proaches on top of all the studied benchmarks, e.g., PreFuzz ğ‘ƒğ‘Ÿğ‘œğ‘
can be 62.0% more efficient than AFL ğ»ğ‘ğ‘£ğ‘œğ‘(3.656 vs. 2.256 EDR).
Accordingly, we can infer that the gradient guidance adopted by
PreFuzzcanprovide moreâ€œhigh-qualityâ€ seedsand moreefficient
guidance(i.e.,gradients)forlaunchingits probabilisticbyteselection
mechanism to explore more new edges than AFL ğ»ğ‘ğ‘£ğ‘œğ‘. Further-
more, we can observe that the EDR of PreFuzz ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡can also
outperform the original Neuzz ğ¸ğ‘‘ğ‘”ğ‘’ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› by 91.4%. Therefore,
we also infer that PreFuzz ğ‘ƒğ‘Ÿğ‘œğ‘can advance the edge exploration
efficiency of PreFuzz ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡. To summarize, combining the two
improvements can mutually advance their edge exploration.
4.2.3 Crashes. Table 4 presents the unique crashes exposed by
Neuzz,MTFuzzandPreFuzzin the studied benchmarks. Overall,
PreFuzzexplores the most unique crashes by outperforming Neuzz
by 62% (149 vs. 92), and MTFuzzby 80% (149 vs. 83). In addition,
PreFuzzdominates the number of the exposed unique crashes in
eachbenchmark. Furthermore,the crashesexposed by Neuzzand
855
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA M. Wu et al.
Table4:Uniquecrashesfoundby Neuzz,MTFuzz andPreFuzz
Benchmarks Neuzz MTFuzz PreFuzz
size 5 9 7
readelf 15 7 37
libjpeg 2 0 5
objdump 0 0 1
who 2 0 4
bison 15 18 20
jhead 8 7 12
listaction 25 16 31
listaction_d 7 8 17
nm 3 3 3
strip 10 15 12
Total 92 83 149
01.534.56
0.5360.283.656
2.256
AFLHavoc PreFuzzProbNeuzzEdgeSelection PreFuzzGradientEdge  Discover  Rate
Figure 8: Edge Discovery Rate of different PreFuzz stages
MTFuzzarealsodetectedby PreFuzzinourevaluation.Suchresults
suggest that PreFuzzcan also be more effective than Neuzzand
MTFuzzin terms of exposing potential vulnerabilities.
4.3 Implications
Based on our findings in this paper, we propose the following
implications for advancing the future research on fuzzing.
Simplisticneuralnetworkmodelsmaysuffice. Ourstudy
resultsrevealthattheedgecoverageperformancecanbeessentially
impacted by how the resulting gradients of the adopted neural net-
work models reflect the relations between explored edges and seed
inputs rather than their generalization or prediction capabilities.
Thatsaid,simplisticneuralnetworkmodelsmayalreadysufficefor
program-smoothing-based fuzzing.
Think twice before applying dynamic analysis. Our evalua-
tionsindicatethatthedynamicanalysismoduleadoptedin MTFuzz
can be quite effective on large programs. However, executing such
module can be rather heavyweight, similar as manyother program
analysis techniques [ 6,12,19]. Therefore, we recommend to think
carefully before adopting dynamic analysis techniques to enhance
neural program-smoothing-based fuzzing.
Edge selection? Yes! Gradient computation? Maybe. Our
evaluationsrevealthatselectingâ€œpromisingâ€edgesformutations
can be quite effective in increasing the edge coverage performance
onprogramsofvaryingsizes.Meanwhile,onequestioncanbeasked:
is it necessary to bind such powerful mechanism with gradient
guidance? Especially when we realize that the power of neural
networkscanbearguedtobeâ€œunderusedâ€(i.e.,theirgeneralization
and prediction capabilities are underused), such question can then
be transformed as â€” is it necessary to use neural networks for
computinggradientstorepresenttherelationsbetweenexplorededgesandseedinputs?Toanswersuchquestion,itisworthwhileto
attemptotherlightweightalternativestorepresentsuchrelations
as potential future research directions.
Probabilistic search helps. Our study results indicate that the
edgecoverageperformanceoftheneuralprogram-smoothing-basedfuzzerscanbesignificantlyenhancedbyappendingthe probabilistic
byteselectionmechanism .Intuitively,wesuggesttheuserstodesign
suchprobabilisticsearchstrategywithmoreguidancetoanyoftheir
adopted fuzzers when possible. Accordingly, one possible research
directioncanbehowtointegratesuchprobabilisticsearchstrategy
with diverse fuzzers for optimizing the edge coverage performance.
5 THREATS TO VALIDITY
Threatstointernalvalidity. Thethreattointernalvaliditylies
in the implementation of the studied fuzzing approaches in the
experimentalstudy.Toreducethisthreat,wereusedthesourcecodeofNeuzzandMTFuzzwhenweimplemented PreFuzz.Meanwhile,to
implementthe probabilisticbyteselectionmechanism ,wealsoreused
suchcodefromtheoriginalAFLforthe PreFuzzimplementation.
Moreover, all the student authors manually reviewed PreFuzzcode
carefully to ensure its correctness and consistency.
Threats to external validity. The threat to external validity
mainlyliesinthebenchmarksused.Toreducethisthreat,weadopt
the original benchmarks used by NeuzzandMTFuzz, and add 19
more projects widely used for the evaluations in many popular
fuzzers [3, 4, 28, 31, 50] published recently.
Threats to construct validity. The threat to construct validity
mainly lies in the metrics used. While the edge coverage metrics
adopted by NeuzzandMTFuzzare not widely used by the existing
fuzzers and can be arguably limited to reflect edge coverage, toreduce this threat, we determine to follow the majority by usingthe AFL built-in tool afl-showmap for measuring edge coverage
whilealsopresentingpartialresultsintheoriginalmeasureaswell.
Notably while under our metric, the performance advantages of
NeuzzandMTFuzzare reduced, our PreFuzzcan incur quite strong
performance gain under both metrics.
6 RELATED WORK
Asthisworkmainlystudiesdeeplearning-basedfuzzingapproaches,
we are going to discuss closely related work in the following three
dimensions: the existing fuzzing approaches (Section 6.1), the deep
learning-based fuzzing techniques (Section 6.2), and the existing
studies on fuzzing (Section 6.3).
6.1 Fuzzing
To date, various fuzzing techniques have adopted evolutionary
algorithmstoimprovetheperformanceoffuzztesting.BÃ¶hmeet
al. [4] proposed AFLFastwhich designs a seed selection strategy
toweighseedsviaMarkovChainontopoftheoriginalAFL[ 51].
They also proposed AFLGo[3] to take advantages in weighting
seeds based on edge structures to explore the target point specifiedbyusers.Lemieuxetal.[
27]proposed FairFuzztoincreasegreybox
fuzz testing coverage by fuzzing rare branches of program. ManÃ¨s
et al. [32] proposed Ankou, a grey-box fuzzing solution based on
differentcombinationsofexecutioninformation.Fioraldietal.[ 16]
introduced WEIZZto automatically generate and mutate inputs
856
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. Evaluating and Improving
Neural Program-Smoothing-based Fuzzing ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
for unknown chunk-basedbinary formats. Similar toour PreFuzz,
manyworksalsoutilizedlight-weightprogramanalysistofacilitate
fuzzing efficacy. Rawat et al. [ 36] proposed VUzzerto leverage
control-anddata-flowfeaturesbasedonstaticanddynamicanalysis
toinferfundamentalpropertiesoftheapplicationwithoutanypriorknowledgeorinputformat.Mathisetal.[
33]presentedatechnique
to learn program tokens by tainting for fuzzing. Padhye et al. [ 34]
automatically guided QuickCheck-like random input generators
to semantically analyze test programs for generating test-oriented
Java bytecode. Chen et al. [ 9] introduced Angora, a mutation-based
fuzzerthatsolvespathconstraintwithoutsymbolicexecutionby
taint checking and searching. Furthermore, new guidance other
than code coverage are proposed to fuzz specific software systems.
Wuetal.[ 46]proposed Simuleetoparseconstraintsofinputsfroma
givenGPUkernelfunctionandmutatetheinputsguidedbymemory
access conflict to fuzz CUDA programs. Accordingly, they further
proposed AuCS[47] to repair the detected synchronization bugs.
Wenetal.[ 43]proposedamemory-usage-guidedfuzzertogenerate
excessive memory consumption inputs and trigger uncontrolled
memory consumption bugs. Zhao et al. [ 53] synthesized programs
for testing JVMs based on the ingredients extracted from JVM
historical bug-revealing tests.
6.2 Deep Learning on Fuzzing
Sheetal.proposed Neuzz[41],thefirstneuralprogram-smoothing-
based fuzzer using neural network models to discover â€œpromis-
ingâ€ bytes for a previously explored edge. They [ 40] also proposed
MTFuzzto fuzz a system more efficiently via a multi-task neural
network. Meanwhile, deep learning is also used in evolution-based
fuzzing.Zongetal.[ 55]proposed FuzzGuard,adeep-learning-based
approachtohelpevolution-basedfuzzerspredictthereachability
ofinputsbeforeexecutingprograms.Moreover,researchershave
also utilized deep learning to learn how to generate valid inputs
fordeeplyfuzzingasystem.Lyuetal.[ 30]introduced SmartSeed
whichusedGenerativeAdversarialNetworks[ 21]togenerateseeds
from learning the patterns of valuable existing seeds. Liu et al. [ 29]
proposed DeepFuzz to automatically and continuously generate
C programs by a generative Sequence-to-Sequence model [ 11].
Godefroid et al. [ 20] divided fuzzing tasks into two categories, i.e.,
learning input format to fuzz deeper and breaking input formatto trigger defects. Zhang et al. [
52] proposed DeepRoad to auto-
matically generate driving scenes to fuzz image-based autonomousdrivingsystems.Zhouetal.[
54]furthergeneratedrealisticandcon-
tinuous physical-world images to fuzz such systems. In this paper,
we propose PreFuzzwith theresource-efficient edge selection mecha-
nismand theprobabilistic byte selection mechanism to improve the
performance of neural program-smoothing-based fuzzers.
6.3 Studies on Fuzzing
Theempiricalstudiesonfuzzinggivemanyimplicationsforfurther
research. Klees et al. [ 25] provided guidelines on evaluating the
effectiveness of fuzzers by assessing the experimental evaluations
carriedoutbydifferentfuzzers.Gavrilovetal.[ 17]proposedanew
metric consistently with bug-based metrics by conducting a pro-
gram behavior study during fuzzing. BÃ¶hme et al. [ 2] summarized
thechallengesandopportunitiesforfuzzingbystudyingexistingpopular fuzzers. Geng et al. [ 18] performed an empirical study on
multiple artificial vulnerability benchmarks to understand howclose these benchmarks reflect reality. Herrera et al. [
23] investi-
gated and evaluated how seed selection affects a fuzzerâ€™s ability to
findbugsinreal-worldsoftware.Wuetal.[ 45]studiedthefeatures
ofthehavocmechanismadoptedbymanyfuzzersincludingAFL
and found it is already a powerful fuzzer which outperforms many
existingones.Inthispaper,weconductanempiricalstudytoinves-tigatethepowerandlimitationofneuralprogram-smoothing-based
fuzzing and reveal various findings/guidelines for future learning-based fuzzing research.
7 CONCLUSION
Inthispaper,weinvestigatedthestrengthsandlimitationsofneural
program-smoothing-based fuzzing approaches, e.g., MTFuzzand
Neuzz.Wefirstextendedourbenchmarksuitebyincludingaddi-
tional projects that were widely adopted in the existing fuzzing
evaluations.Next,weevaluated NeuzzandMTFuzzontheexten-
sive benchmark suite to study their effectiveness and efficiency.Inspired by our study findings, we proposed PreFuzzcombining
two technical improvements, i.e., the resource-efficient edge selec-
tion mechanism and theprobabilistic byte selection mechanism . The
evaluationresultsdemonstratethat PreFuzzcansignificantlyout-
performNeuzzandMTFuzzintermsofedgecoverage.Furthermore,
our results also reveal various findings/guidelines for advancing
future fuzzing research.
8 ACKNOWLEDGEMENT
ThisworkispartiallysupportedbytheNationalNaturalScience
Foundation of China (Grant No. 61902169), Guangdong Provincial
Key Laboratory (Grant No. 2020B121201001), and Shenzhen Pea-
cockPlan(GrantNo.KQTD2016112514355531).Thisworkisalso
partiallysupportedbyNationalScienceFoundationunderGrant
Nos. CCF-2131943and CCF-2141474,as well as Ant Group.
REFERENCES
[1]JacobBenesty,JingdongChen,YitengHuang,andIsraelCohen.2009. Pearson
correlation coefficient. In Noise reduction in speechp rocessing. Springer, 1â€“4.
[2]MarcelBÃ¶hme,CristianCadar,andAbhikRoychoudhury.2020. Fuzzing:Chal-
lenges and Reflections. IEEE Software (2020).
[3]MarcelBÃ¶hme,Van-ThuanPham,Manh-DungNguyen,andAbhikRoychoudhury.
2017.Directedgreyboxfuzzing.In Proceedingsofthe2017ACMSIGSACConference
on Computer and Communications Security. 2329â€“2344.
[4]Marcel BÃ¶hme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
BasedGreyboxFuzzingasMarkovChain.In Proceedingsofthe2016ACMSIGSAC
Conference on Computer and Communications Security (CCS â€™16) . Association for
ComputingMachinery,NewYork,NY,USA,1032â€“1043. https://doi.org/10.1145/
2976749.2978428
[5]Charles E Brown. 1998. Coefficient of variation. In Applied multivariate statistics
in geohydrology and related sciences. Springer, 155â€“157.
[6]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDIâ€™08). USENIX Association, USA, 209â€“224.
[7]SwaratChaudhuriandArmandoSolar-lezama.2010. Smoothinterpretation.In
In PLDI.
[8]Swarat Chaudhuri and Armando Solar-Lezama. 2011. Smoothing a Program
SoundlyandRobustly.In ComputerAidedVerification-23rdInternationalCon-
ference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings. 277â€“292.
https://doi.org/10.1007/978-3-642-22110-1_22
[9]Peng Chenand HaoChen. 2018. Angora: Efficientfuzzing byprincipled search.
In2018 IEEE Symposium on Security and Privacy (SP). IEEE, 711â€“725.
[10]Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
857
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA M. Wu et al.
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[11]Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[12]Leonardo De Moura and Nikolaj BjÃ¸rner. 2008. Z3: An Efficient SMT Solver.InProceedings of the Theory and Practice of Software, 14th International Con-
ference on Tools and Algorithms for the Construction and Analysis of Systems
(TACASâ€™08/ETAPSâ€™08). Springer-Verlag, Berlin, Heidelberg, 337â€“340.
[13]Jared DeMott, Richard Enbody, and William F Punch. 2007. Revolutionizing the
field of grey-box attack surface testing with evolutionary fuzzing. BlackHat and
Defcon(2007).
[14]JeffreyLElman.1990. Findingstructureintime. Cognitivescience 14,2(1990),
179â€“211.
[15]ShawnEmbleton,SherriSparks,andRyanCunningham.2006. Sidewinder:An
Evolutionary Guidance System for Malicious Input Crafting. Black Hat USA
(2006).
[16]AndreaFioraldi,DanieleConoDâ€™Elia,andEmilioCoppa.2020.WEIZZ:Automatic
Grey-Box Fuzzing for Structured Binary Formats. In Proceedings of the 29th
ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA
2020). Association for Computing Machinery, New York, NY, USA, 1â€“13. https:
//doi.org/10.1145/3395363.3397372
[17]M. Gavrilov, K. Dewey, A. Gro ce, D. Zamanzadeh, and B. Hardekopf. 2020. A
Practical, Principled Measure of Fuzzer Appeal: A Preliminary Study. In 2020
IEEE20thInternationalConferenceonSoftwareQuality,ReliabilityandSecurity
(QRS). 510â€“517. https://doi.org/10.1109/QRS51102.2020.00071
[18]SijiaGeng,YuekangLi,YunlanDu,JunXu,YangLiu,andBingMao.2020. An
empiricalstudyonbenchmarksofartificialsoftwarevulnerabilities. arXivpreprint
arXiv:2003.09561 (2020).
[19]Patrice Godefroid, Michael Y Levin, David A Molnar, et al .2008. Automated
whitebox fuzz testing.. In NDSS, Vol. 8. 151â€“166.
[20]Patrice Godefroid, Hila Peleg, and Rishabh Singh. 2017. Learn&fuzz: Machine
learningforinputfuzzing.In 201732ndIEEE/ACMInternationalConferenceon
Automated Software Engineering (ASE). IEEE, 50â€“59.
[21]IanJGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
networks. arXiv preprint arXiv:1406.2661 (2014).
[22]Alex Graves and JÃ¼rgen Schmidhuber. 2005. Framewise phoneme classification
withbidirectionalLSTMandotherneuralnetworkarchitectures. Neuralnetworks
18, 5-6 (2005), 602â€“610.
[23]AdrianHerrera,HendraGunadi,ShaneMagrath,MichaelNorrish,MathiasPayer,and Tony Hosking. 2021. Seed Selection for Successful Fuzzing. In Proceedings of
the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA 2021).
[24]SeppHochreiterandJÃ¼rgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735â€“1780.
[25]GeorgeKlees,AndrewRuef,BenjiCooper,ShiyiWei,andMichaelHicks.2018.
Evaluating fuzz testing. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security. 2123â€“2138.
[26]Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E
Howard,WayneHubbard,andLawrenceDJackel.1989. Backpropagationapplied
to handwritten zip code recognition. Neural computation 1, 4 (1989), 541â€“551.
[27]Caroline Lemieux and Koushik Sen. 2018. Fairfuzz: A targeted mutation strategyforincreasinggreyboxfuzztestingcoverage.In Proceedingsofthe33rdACM/IEEE
International Conference on Automated Software Engineering. 475â€“485.
[28]Yuwei Li, Shouling Ji, Chenyang Lv, Yuan Chen, Jianhai Chen, Qinchen Gu, and
Chunming Wu. 2019. V-fuzz: Vulnerability-oriented evolutionary fuzzing. arXiv
preprint arXiv:1901.01142 (2019).
[29]Xiao Liu, Xiaoting Li, Rupesh Prajapati, and Dinghao Wu. 2019. Deepfuzz:
Automatic generation of syntax valid c programs for fuzz testing. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 33. 1044â€“1051.
[30]ChenyangLyu,ShoulingJi,YuweiLi,JunfengZhou,JianhaiChen,andJingChen.
2018. Smartseed: Smart seed generation for efficient fuzzing. arXiv preprint
arXiv:1807.02606 (2018).
[31]ChenyangLyu,ShoulingJi,ChaoZhang,YuweiLi,Wei-HanLee,YuSong,and
RaheemBeyah.2019. {MOPT}:Optimizedmutationschedulingforfuzzers.In
28th{USENIX}Security Symposium ( {USENIX}Security 19). 1949â€“1966.
[32]ValentinJ.M.ManÃ¨s,SoominKim,andSangKilCha.2020. Ankou:GuidingGrey-
Box Fuzzing towards Combinatorial Difference. In Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering (ICSE â€™20) . Association for
ComputingMachinery,NewYork,NY,USA,1024â€“1036. https://doi.org/10.1145/
3377811.3380421
[33]BjÃ¶rn Mathis, Rahul Gopinath, and Andreas Zeller. 2020. Learning input to-
kens for effective fuzzing. In Proceedings of the 29th ACM SIGSOFT International
Symposium on Software Testing and Analysis. 27â€“37.
[34]Rohan Padhye, Caroline Lemieux, Koushik Sen, Mike Papadakis, and YvesLe Traon. 2019. Semantic fuzzing with zest. In Proceedings of the 28th ACMSIGSOFT International Symposium on Software Testing and Analysis. 329â€“340.
[35]Rahul Potharaju and Navendu Jain. 2013. Demystifying the dark side of the
middle:Afieldstudyofmiddleboxfailuresindatacenters.In Proceedingsofthe
2013 conference on Internet measurement conference. 9â€“22.
[36]Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida,and Herbert Bos. 2017. VUzzer: Application-aware Evolutionary Fuzzing.. In
NDSS, Vol. 17. 1â€“14.
[37]Yanyan Ren, Shriram Krishnamurthi, and Kathi Fisler. 2019. What Help Do
Students Seek in TA Office Hours. In Proceedings of the 2019 ACM Conference on
International Computing Education Research. Association for Computing Machin-
ery, 41â€“49.
[38]Github Repository. 2021. Program smoothing fuzzing. https://github.com/
PoShaung/program-smoothing-fuzzing.
[39]DongdongShe.2020. neuzzrepository. https://github.com/Dongdongshe/neuzz.
[40]Dongdong She, Rahul Krishna, Lu Yan, Suman Jana, and Baishakhi Ray. 2020.MTFuzz: fuzzing with a multi-task neural network. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 737â€“749.
[41]DongdongShe,KexinPei,DaveEpstein,JunfengYang,BaishakhiRay,andSuman
Jana.2019. NEUZZ:Efficientfuzzingwithneuralprogramsmoothing.In 2019
IEEE Symposium on Security and Privacy (SP). IEEE, 803â€“817.
[42]AriTakanen,JaredD Demott,Charles Miller,andAtte Kettunen.2018. Fuzzing
for software security testing and quality assurance. Artech House.
[43]Cheng Wen, Haijun Wang, Yuekang Li, Shengchao Qin, Yang Liu, Zhiwu Xu,
Hongxu Chen, Xiaofei Xie, Geguang Pu, and Ting Liu. 2020. MemLock: Memory
UsageGuidedFuzzing. In Proceedings ofthe ACM/IEEE42nd InternationalConfer-
ence on Software Engineering (ICSE â€™20). Association for Computing Machinery,
New York, NY, USA, 765â€“777. https://doi.org/10.1145/3377811.3380396
[44]Wikipedia. 2020. Fuzzing. en.wikipedia.org/wiki/Fuzzing. Online; accessed
27-Jan-2020.
[45]MingyuanWu,LingJiang,JiahongXiang,YanweiHuang,HemingCui,Lingming
Zhang, and Yuqun Zhang. 2022. One Fuzzing Strategy to Rule Them All. In 2022
IEEE/ACM 44th International Conference on Software Engineering (ICSE).
[46]Mingyuan Wu, Yicheng Ouyang, Husheng Zhou, Lingming Zhang, Cong Liu,and Yuqun Zhang. 2020. Simulee: Detecting cuda synchronization bugs viamemory-access modeling. In 2020 IEEE/ACM 42nd International Conference on
Software Engineering (ICSE). IEEE, 937â€“948.
[47]MingyuanWu,LingmingZhang,CongLiu,ShinHweiTan,andYuqunZhang.
2019. Automating CUDA Synchronization via Program Transformation. In 2019
34thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE).
748â€“759. https://doi.org/10.1109/ASE.2019.00075
[48]Yalong Yang, Bernhard Jenny, Tim Dwyer, Kim Marriott, Haohui Chen, and
Maxime Cordeil. 2018. Maps and globes in virtual reality. In Computer Graphics
Forum, Vol. 37. Wiley Online Library, 427â€“438.
[49]Xin Yao, Yong Liu, and Guangming Lin. 1999. Evolutionary programming made
faster.IEEE Transactions on Evolutionary computation 3, 2 (1999), 82â€“102.
[50]InsuYun,SanghoLee,MengXu,YeongjinJang,andTaesooKim.2018. {QSYM}:A
practicalconcolicexecutionenginetailoredforhybridfuzzing.In 27th{USENIX}
Security Symposium ( {USENIX}Security 18). 745â€“761.
[51] MichaÅ‚ Zalewski. 2020. American Fuzz Lop. https://github.com/google/AFL.[52]
Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-shid. 2018. DeepRoad: GAN-Based Metamorphic Testing and Input Valida-tion Framework for Autonomous Driving Systems. In 2018 33rd IEEE/ACM
International Conference on Automated Software Engineering (ASE). 132â€“142.
https://doi.org/10.1145/3238147.3238187
[53]Yingquan Zhao, Zan Wang, Junjie Chen, Mengdi Liu, Mingyuan Wu, YuqunZhang, and Lingming Zhang. 2022. History-Driven Test Program Synthesisfor JVM Testing. In 2022 IEEE/ACM 44th International Conference on Software
Engineering (ICSE).
[54]HushengZhou,WeiLi,ZelunKong,JunfengGuo,YuqunZhang,BeiYu,LingmingZhang,andCongLiu.2020. DeepBillboard:SystematicPhysical-WorldTestingof
AutonomousDriving Systems.In 2020IEEE/ACM 42ndInternationalConference
on Software Engineering (ICSE). 347â€“358.
[55]Peiyuan Zong, Tao Lv, Dawei Wang, Zizhuang Deng, Ruigang Liang, and KaiChen. 2020. Fuzzguard: Filtering out unreachable inputs in directed grey-box
fuzzingthroughdeeplearning.In 29th{USENIX}SecuritySymposium( {USENIX}
Security 20). 2255â€“2269.
858
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. 