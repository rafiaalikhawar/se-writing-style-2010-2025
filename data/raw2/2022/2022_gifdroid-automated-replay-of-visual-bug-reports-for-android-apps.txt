GIFdroid: Automated Replay of Visual Bug Reports for Android
Apps
Sidong Feng
Monash University
Melbourne, Australia
sidong.feng@monash.eduChunyang Chenâˆ—
Monash University
Melbourne, Australia
chunyang.chen@monash.edu
ABSTRACT
Bug reports are vital for software maintenance that allow users
to inform developers of the problems encountered while usingsoftware. However, it is difficult for non-technical users to write
cleardescriptionsaboutthebugoccurrence.Therefore,moreand
more users begin to record the screen for reporting bugs as it is
easy to be created and contains detailed procedures triggering the
bug. But it is still tedious and time-consuming for developers to
reproduce thebug due to thelength and unclearactions within therecording.Toovercometheseissues,wepropose
GIFdroid ,alight-
weightapproachtoautomaticallyreplaytheexecutiontracefrom
visualbugreports. GIFdroid adoptsimageprocessingtechniques
to extract the keyframes from the recording, map them to states in
GUITransitionsGraph,andgeneratetheexecutiontraceofthose
statestotriggerthebug.Ourautomatedexperimentsanduserstudydemonstrateitsaccuracy,efficiency,andusefulnessoftheapproach.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging.
KEYWORDS
bug replay, visual recording, android testing
ACM Reference Format:
Sidong Feng and Chunyang Chen. 2022. GIFdroid: Automated Replay of
VisualBugReportsforAndroidApps.In 44thInternationalConferenceon
Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510048
1 INTRODUCTION
Software maintenance activities are known to be generally expen-
sive and challenging [ 60] and one of the most important mainte-
nance tasks is to handle bug reports [ 17]. A good bug report is
detailed with clear information about what happened and what
theuserexpectedtohappen.Itgoesontocontainareproduction
step or stack trace to assist developers in reproducing the bug, and
âˆ—Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510048supplement information such as screenshots, error logs, and en-
vironments. As long as this bug report is accurate, it should be
straightforward for developers to reproduce and fix.
Bugs are often encountered by non-technical users who will
document a description of the bug with steps to reproduce it. How-
ever, clear and concise bug reporting takes time, especially for
non-developer or non-tester users who do not have that expertise
and are not willing to spend that much effort [ 18,25]. A poorly
writtenreportwouldbeevenpoorlyinterpreted[ 24,26,36],which
oftendevolvesintothefamiliarrepetitivebackandforthandthe
need to nag that comes with every single bug. That effort may
further prohibit usersâ€™ contribution to bug reporting.
Compared to writing it down with instructions on how to repli-
cate,video-basedbugreportssignificantlylowerthebarfordocu-
mentingthebug.First,itiseasytorecordthescreenasthereare
many tools available [ 3,13], some of which are even embedded
intheoperatingsystembydefaultlikeiOS[ 10]andAndroid[ 12].
Second,videorecordingcanincludemoredetailandcontextsuchas
configurations,andparameters,henceitbridgestheunderstanding
gap between users and developers. That convenience may even
betterengageusersinactivelyprovidingfeedbacktoimprovethe
app.
Despitetheprosofthevideo-basedbugreport,itstillrequires
developerstomanuallycheckeachframeinthevideoandrepeatit
intheirenvironment.Accordingtoourempiricalstudyof13,587
bug recordings from 647 Android apps in Section 2, one video isof 148.29 frames on average with a varied resolution for manual
observation.Inaddition,only6.8%ofvideorecordingsstartfrom
the app launch and most recordings begin 2-7 steps before the bug
occurrence,indicatingthatdevelopersneedtoguessstepstothe
entryframeofthevideobythemselves.Therefore,itisnecessarytodevelopanautomatedbugreplaytoolfromvideo-basedbugreports
to save developersâ€™ effort in a bug fix.
Therearemanyrelatedworksonbugreplaybutrarelyrelated
to visual bug reports. Some researchers [ 37,65,70,83,84] leverage
the natural language processing methods with program analysis togenerate the test cases from the textual descriptions in bug reports.
However,thoseapproachesdonotapplytovideo-basedbugreports.
There are platforms providingboth video recording and replaying
functionalities[ 1,23,45,62]whichalsostorethelow-levelprogram
execution information. They require the framework installation or
app instrumentation which is too heavy for end users. Users tend
tousegeneralrecordingtoolstogetthevideothatonlycontains
the visual information, according to our observation in Section 2.2.
Automation for processing general recordings to reproduce bugs is
necessary and would help developers shift their focus toward bug
fixing.
10452022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Feng et al.
Toautomatetheanalysisofbugrecordings,weintroduce GIFdroid ,
a light-weight image-processingapproach to automatically replay
thevideo(GIF)basedbugreportsforAndroidapps.First,weextract
keyframes(i.e.,fully renderedGUIs)ofarecordingby comparing
the similarity of consecutive frames. Second, a sequence of located
keyframes is then mapped to the GUI states in the existing UTG
(UI Transition Graph) of the app by calculating image similarity
based on pixel and structural features. Third, given the mapped
sequence, we propose a novel algorithm to not only address the de-
fective mapped sequence, but also auto-complete the missing trace
betweenapplaunchtotheentryframeofthevideo,resultinginan
optimal execution trace to automatically repeat the bug trigger.
Toevaluatetheeffectivenessofourtool,wecreatethegroudtruth
by manually labelling 61 video recordings from 31 apps. As our
approachconsistsofthreemaincomponents,weevaluatethemone
byoneincludingkeyframelocation,GUImapping,andtracegen-
eration.Inalltasks,ourapproachsignificantlyoutperformsother
baselines and successfullyreproduce82% video recordings. Apart
from the accuracy of our tool, we also evaluate the usefulness of
ourtoolbyconductingauserstudyonreplayingbugsfrom10real-
world videorecordings inGitHub. Throughthe study, weprovide
theinitialevidenceoftheusefulnessof GIFdroid forbootstrapping
bug replay.
The contributions of this paper are as follows:
â€¢The first light-weight image-processing based approach,
GIFdroid ,toreproducebugsforAndroidappsdirectlyfrom
the GIF recordings with code released for public1.
â€¢A motivational empirical study of large-scale recordings
within real-world issue reports from GitHub including their
content, length, etc.
â€¢A comprehensive evaluation including automated experi-
ments and a user study to demonstrate the accuracy, effi-
ciency and usefulness of our approach.
2 MOTIVATIONAL MINING STUDY
Whilethemainfocusandcontributionofthisworkisdeveloping
an approach to automatically replay the visual bug reports, we
still carry out an empirical study to understand the characteristics
ofvisualbugreports.Theoverallstatusofvisualbugreportscan
clearly frame the context and motivation of this work and theircharacteristics will be taken into consideration in our approach
design.Butnotethatthismotivationalstudyjustaimstoprovidean
initialanalysistowardsdeveloperssupportingvisualbugreports,
and a more comprehensive empirical study would be needed to
deeply understand it.
2.1 Data Collection
We choose F-droid [ 5] as the source of our study subjects, as it
containsalargesetofapps(1,274atthetimeofourstudy)covering
diversecategoriessuchasconnectivity,financial,multimedia.All
appsareopen-sourcehostedonplatformslikeGitHub[ 8]which
makesitpossibleforustoaccesstheirissuerepositoriesforanalysis.
We built a web crawler to automatically crawl the visual bug re-
portsfromissuerepositoriesoftheseappscontainingvisualrecord-ings(e.g.,animation,video)withsuffixnameslike.gif,.mp4,etc.,or
1https://github.com/sidongfeng/gifdroid
Figure1:NumberofrecordingsinGitHubfrom2012to2020
Figure 2: Example of the issue reports for Bug Replay. Thereporters provide a visual recording (i.e., gif) to help devel-opers understand the bug report more clearly.
URLfromvideosharingplatforms(e.g.,GIPHY2[7],YouTube[ 15]).
It took us two weeks to scan 408,272 issues from 1,274 apps and
finally mined 13,587 visual recordings from 7,230 issues (647 apps).
The dataset consists of 8,698 GIFs and 4,889 videos. As shown in
Figure 1, attaching visual recording in the issue report is more and
morepopular.Withinthevideo,thereisalwayssomeextrainfor-
mation (e.g., caption, whole screen of the PC, face of speaker) inadditiontothe appusagescreen.Sincetherearemore GIFsthan
videos, we focus on the GIF visual recording for brevity in this
paper.
2GIPHY is the most popular animated GIF sharing platform, serving 700 million users
1046
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. GIFdroid: Automated Replay of Visual Bug Reports for Android Apps ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: Recording resolution.
Ratios Occurrences Resolution
16:9 288 (48%)1920Ã—1080
1280Ã—720
960Ã—540
5:3 63 (10.5%)1280Ã—800
800Ã—480
3:2 21 (3.5%)900Ã—600
480Ã—320
Figure 3: Number of frames of each recording.
2.2 What are in Visual Recordings?
To understand the content within these visual recordings, the first
two authors manually check 1,000 (11.5%) GIFs and their corre-
sponding descriptions in the issue report. Following the Card Sort-
ing [67] method, we classify those GIFs into three categories:
(i)BugReplay(60%).Mostscreenrecordingsareaboutthebugre-
playincludingdetailedproceduresintriggeringthebugs,especially
forthoserequiringcomplicatedactionswithoneexampleseenin
Figure 23. The visual recordings bridge the lexical gap between
users and developers and help developers read and comprehendthe bug report reasoning about the problem from the high-level
description.
(ii) Issue Fixed (17%). Once the issue is fixed, developers tend
to record the screen to display the update changes, along witha checklist for project owners to approve the pull request, Also,
those recordings are used for app introduction or answering usersâ€™
feature requests.
(iii) Feature Request (23%). Users may use recordings to ask
formissing functionality(e.g.,providedby otherapps)or missing
content(e.g.,incataloguesandgames),andsharingideasonhowto
improve the app in future releases by adding or changing features.
2.3 What are Characteristics of Bug
Recordings?
Sincethetargetofthisworkistoauto-replaythebugrecording,we
further analyze the characteristics of 600 bug recordings classified
in Section 2.2. Due to the difference of devices or usersâ€™ resize ofthevideo,theremaybedifferentresolutionsandaspectratiosassummarized in Table 1. About half of bug recordings are of 16:9
aspect ratio with varied resolution from high-quality 1920x1080 to
960x540.Therearealsosomeminoraspectratiosandresolutions
3https://github.com/fr3ts0n/AndrOBD/issues/144as users may post-process the recording by resizing or cropping
arbitrarily, according to our observation.
Figure3depictsthenumberofframesinvisualbugrecording.
On average, there are 148.29 frames per video and some of them
areevenwithmorethan500frameswhichrequiredevelopersto
manually check and replay in their own settings. We also find that
only 41 recordings (6.8%) start from the launch of the app, while
mostrecordings startfrom 2-7steps beforethebug occurrence.It
indicates that there is no explicit hint to guide developers to the
entry frame of the recording and that barrier may negatively influ-
ence developersâ€™ efficiency. As the important information to show
usersâ€™actions,somerecordingscontaintouchindicators(e.g.,circle
or arrow of the touch position) which is animportant resource to
replaythebug[ 23].However,thereareonly155recordings(25.8%)
enabling the â€œShow Touchesâ€ option in our dataset. Developers
especially noviceones haveto guess andtry thepotential actions
to trigger the target page from the current page.
Summary : By analyzing issue reports from 1,274 existing apps
crawled from F-Droid, 60% of them are with recordings for bug
replay. A large number of frames and varied resolution make
itdifficultfordeveloperstoreplaythemintheirsetting.Such
phenomenonisfurtherexacerbatedas74.2%ofrecordingsare
without touch indicators on the screen. These findings confirm
thenecessityanddifficultyofthereplayofvisualbugreports,
andmotivateourapproachdevelopmentforautomaticreplaying
the recordings for developers and testers.
3 APPROACH
Given an input bug recording, we propose an automated approach
to localize a sequence of keyframes in the GIF and subsequentlymap them to the existing UTG (UI Transition Graph) to extractthe execution trace. The overview of our approach is shown in
Figure 4, which is divided into three main phases: (i) the Keyframe
Location phase, which identifies a sequence of keyframes of an
input visual recording, (ii) the GUI Mapping phase that maps each
located keyframe to the GUIs in UTG, yielding an index sequence,
and(iii)the ExecutionTraceGeneration phasethatutilizestheindex
sequence to detect an optimal replayable execution trace.
3.1 Keyframe Location
Note that GUI rendering takes time, hence many frames in the
visual recording are showing the partial rendering process. The
goal of this phase is to locate keyframes i.e., states in which GUI
are fully rendered in a given visual recording.
3.1.1 Consecutive Frame Comparison. Inspiredbysignalprocess-
ing, we leverage the image processing techniques to build a per-ceptual similarity score for consecutive frame comparison basedon Y-Difference (or Y-Diff). YUV is a color space usually used in
video encoding, enabling transmission errors or compression arti-
factsto bemoreefficientlymasked bythehuman perceptionthan
using a RGB-representation [ 28,68]. Y-Diff is the difference in Y
(luminance)valuesoftwoimagesintheYUVcolorspace.Weadopt
theluminancecomponentasthehumanperception(a.k.ahuman
vision) is sensitive to brightness changes. In the human visual sys-
tem, a majority of visual information is conveyed by patterns of
1047
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Feng et al.
Figure 4: The overview of GIFdroid.
contrastsfromitsbrightnesschanges[ 79].Furthermore,luminance
is a major input for the human perception of motion [ 50]. Since
peopleperceiveasequenceofgraphicschangesasamotion,con-
secutive images are perceptually similar if people do not recognize
any motions from the image frames.
Consider a visual recording/braceleftbig
ğ‘“0,ğ‘“1,..,ğ‘“ğ‘âˆ’1,ğ‘“ğ‘/bracerightbig, whereğ‘“ğ‘is
the current frame and ğ‘“ğ‘âˆ’1is the previous frame. To calculate the
Y-Diff of the current frame ğ‘“ğ‘with the previous ğ‘“ğ‘âˆ’1, we first
obtain the luminance mask ğ‘Œğ‘âˆ’1,ğ‘Œğ‘by splitting the YUV color
spaceconvertedbytheRGBcolorspace.Then,weapplythepercep-
tual comparison metric, SSIM (Structural Similarity Index) [ 73], to
produceaper-pixelsimilarityvaluerelatedtothelocaldifferencein
the average value, the variance, and the correlation of luminances.
Indetail,theSSIMsimilarityfortwoluminancemasksisdefined
as:
ğ‘†ğ‘†ğ¼ğ‘€(ğ‘¥,ğ‘¦)=(2ğœ‡ğ‘¥ğœ‡ğ‘¦+ğ¶1)+(2ğœğ‘¥ğ‘¦+ğ¶2)
(ğœ‡2ğ‘¥+ğœ‡2ğ‘¦+ğ¶1)(ğœ2ğ‘¥+ğœ2ğ‘¦+ğ¶2)(1)
whereğ‘¥,ğ‘¦denote the luminance masks ğ‘Œğ‘âˆ’1,ğ‘Œğ‘, andğœ‡ğ‘¥,ğœğ‘¥,
ğœğ‘¥ğ‘¦arethemean,standarddeviation,andcrosscorrelationbetween
the images, respectively. ğ¶1andğ¶2are used to avoid instability
when the means and variances are close to zero. A SSIM score is
a number between 0 and 1, and a higher value indicates a strong
level of similarity.
3.1.2 Keyframe Identification. Tomakedecisionsonwhetherthe
frame is a keyframe, we look into the similarity scores of consecu-
tiveframesinthevisualrecordingasshowninFigure5.Thefirst
step is to group frames belonging to the same atomic activity ac-
cording to a tailored pattern analysis. This procedure is necessary
because discrete activities performed on the screen will persist
Figure 5: An illustration of the Y-Diff similarity scores of
consecutive frames in the visual recording.
acrossseveralframes,andthus,needtobegroupedandsegmentedaccordingly. There are 3 types of patterns, i.e., instantaneous transi-
tions,animation transitions, and steady.
(1) instantaneous transitions : As shown in Figure 5 Activity 1
(clicking a button), the similarity score starts to drop drastically
whichrevealsaninstantaneoustransitionfromonescreentoan-
other. In addition, one common case is that the similarity scorebecomes steady for a small period of time
ğ‘¡ğ‘ between two drasti-
cally droppings as shown in Figure 5 Activity 3. The occurrenceof this short steady duration
ğ‘¡ğ‘ is because GUI has not finished
loading.WhiletheGUIlayoutofGUIrenderingisfast,resources
loading may take time. For example, rendering images from the
web depends on device bandwidth, image loading efficiency, etc.
(2)animationtransitions :Figure5Activity2showsthesimilarity
sequenceofthetransitionswhereanimationeffectsareused,forexample, the "scrolling" event. That is, over a period of time, the
similarity score continues to increase slightly.
1048
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. GIFdroid: Automated Replay of Visual Bug Reports for Android Apps ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure6:AnexampleofpartialUTGonmobileappGUIpro-
ceeding from state GUI-1 to state GUI-3.
(3)steady :Figure5givesanexampleofaGUIsteadystatewhere
the consecutive frames are similar for a relatively long duration.
Wehaveempiricallyset0.954asthethresholdtodecidewhether
two frames are similar, and 5 frames as the threshold to indicate a
steady state.
3.2 GUI Mapping
It is easy for developers to get the UTG of their own app [ 31,
77]. Therefore, instead of inferring actions from the recording [ 23,
80], we directly map keyframes extracted from the recording to
states/GUIswithintheUTG.Toachievethis,wecomputetheimagesimilaritybetweenthekeyframeandeachGUIscreenshotbasedon
both pixel and structural features. The GUI screenshot that has the
highest similarity score is regarded as the index of the keyframe.
3.2.1 UTG Construction. A GUI transitions graph (UTG) of an An-
droidappiswidelyusedtoillustratethetransitionsacrossdifferent
GUIs triggered by typical elements such as toasts (pop-ups), text
boxes,textviewobjects,spinners,listitems,progressbars,check-
boxes. In Figure 6 we illustrate how a UTG emerges as a resultof user interactions in an app. Starting with the GUI-1, the user
â€œscrolls downâ€ to the bottom of the page (GUI-2). When the user
â€œtapsâ€abutton,theGUItransitstotheGUI-3.Therearemanytools
to construct a UTG, either manually [ 11] or automatically [ 19,48].
Inthispaper,weadopttheFirebase[ 6],awidely-usedautomated
GUIexplorationtooldevelopedbyGoogle,whileothertoolscan
also be used.
3.2.2 Feature Extraction. Thebasicneedforanyimagesearching
techniquesistodetectandconstructadiscriminativefeaturerepre-
sentation[ 34,35].Aproperconstructionoffeaturescanimprove
the performance of the image searching method [ 47]. According to
our observation, in addition to the pixel features as that in natural
images,GUIscreenshotsarealsoofadditionalstructuralfeatures
i.e., thelayout ofdifferent components inone page.Therefore, we
adoptahybridmethodbasedontwotypesoffeatures,SSIM(Struc-
turalSimilarityIndex)[ 73]andORB(OrientedFASTandRotated
BRIEF)[63],forsearchingthemappingGUIscreenshotsintheUTG
for the keyframe.
WhileSSIMdetectsthefeatureswithinpixelsandstructures(i.e.,
adetaileddescriptionisdemonstratedinSection3.1.1),itstillhas
several fundamental limitations that exist in visual recordings, e.g.,
imagedistortion[ 46,71].Toaddressthis,wefurthersupplement
a local invariant feature extraction method, ORB. Given an image,
4We set up that value by a small-scale pilot study.
Figure 7: Illustration of the execution trace generation. In-dex sequence 2,3 indicate two types of defective sequences,i.e., missing {ğ·}and wrong mapping to {ğµ}, respectively.
ORB first detects the interest points, indicating at which the direc-
tion of the boundary of the object changes abruptly or intersection
point between two or more edge segments. Then, ORB converts
eachinterestpointintoann-bitsbinaryfeaturedescriptor,which
acts as a â€œfingerprintâ€ that can be used to differentiate one feature
from another. A feature descriptor of an interest point is computed
by an intensity difference test ğœ:
ğœ(ğ‘;ğ‘¥,ğ‘¦)=/braceleftBigg
1 if p(x) <p(y)
0 otherwise(2)
whereğ‘(ğ‘¥),ğ‘(ğ‘¦)areintensityvaluesatpixel ğ‘¥,ğ‘¦aroundtheinterest
point. Due to the characteristic of local feature extraction, ORB
features remain invariant of scale, brightness, and also maintain a
certain stability of affine transformation, and noise.
3.2.3 Similarity Computation. Based on the features extracted by
SSIM and ORB, we compute a similarity value ğ‘†ğ‘ ğ‘ ğ‘–ğ‘šandğ‘†ğ‘œğ‘Ÿğ‘,
respectively. To compute ğ‘†ğ‘œğ‘Ÿğ‘, we adopt the Brute Force algo-
rithm [63], which compares the hamming distance between fea-
ture descriptors. We compute the ğ‘†ğ‘ ğ‘ ğ‘–ğ‘šusing Equation 1 based on
similarityonluminance,contrast,andstructure.Wethenfurther
determine the similarity ğ‘†ğ‘ğ‘œğ‘šğ‘between the keyframe and states in
UTG by combining two feature similarities score:
ğ‘†ğ‘ğ‘œğ‘šğ‘=ğ‘¤Ã—ğ‘†ğ‘œğ‘Ÿğ‘+(1âˆ’ğ‘¤)Ã—ğ‘†ğ‘ ğ‘ ğ‘–ğ‘š (3)
whereğ‘¤isaweightfor ğ‘†ğ‘ ğ‘ ğ‘–ğ‘šandğ‘†ğ‘œğ‘Ÿğ‘,takingavaluebetween0
to 1. Smaller ğ‘¤value weights ğ‘†ğ‘ ğ‘ ğ‘–ğ‘šmore heavily, and larger value
weightsğ‘†ğ‘œğ‘Ÿğ‘moreheavily.Weempiricallychoose0.5asthe ğ‘¤value
for the best performance.
Based on the combined similarity between the keyframe and
each GUI screenshot, we select the highest score to be the index of
the keyframe. Consequently, a sequence of keyframes is converted
to a sequence of the index in the UTG.
3.3 Execution Trace Generation
After mapping keyframes to the GUIs in the UTG, we need to
go one step further to connect these GUIs/states into a trace toreplay the bug. Howev er, this process is challenging due to two
reasons. First, the extracted keyframe (Section 3.1) and mapped
GUIs(Section3.2)maynotbe100%accurate,resultinginamismatch
of the groundtruth trace. For example in Figure 7, {ğ·}is missed in
theindexsequence2andthesecondkeyframeiswronglymapping
1049
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Feng et al.
to{ğµ}intheindexsequence3.Second,differentfromtheuploaded
GIF which may start the recording anytime, the recovered trace in
our case must begin from the launch of the app.
Therefore,thetracegenerationalgorithmneedstoconsiderboth
the wrong extraction/mapping in our previous steps, and the miss-
ingtracebetweentheapplaunchandfirstkeyframeinthevisual
bug report. To overcome these issues, our approach first generates
all candidate sequences in UTG between the app launch to the
last keyframe from GIF. By regarding the extracted keyframes as a
sequence,ourapproachthenfurtherextractstheLongestCommon
Subsequence (LCS) between it and all candidate sequences.
TheoverflowofourapproachcanbeseeninAlgorithm1.Given
an index sequence ğ‘‹={ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›}(extracted by keyframe map-
ping) where ğ‘¥ğ‘›is the last node, a UTG graph ğº, and an app launch
nodeğ‘ .Tofindacyclicpaths(avoidingdeadloops)fromapplaunch
node(ğ‘ )tolastindexnode( ğ‘¥ğ‘›),weadoptDepth-FirstSearchtraver-
sal,thattakesapathon ğºandstartswalkingonitandcheckifit
reaches the destination then count the path and backtrack to take
anotherpath.Toavoidcyclicpath,werecordallvisitednodes(Line
5),sothatonenodecannotbevisitedtwice.Theoutputofthetraver-
salisasetofacyclicpath ğ‘†ğ¸ğ‘„={ğ‘¦1,ğ‘¦2,...,ğ‘¦ ğ‘š}whereğ‘¦1=ğ‘ and
ğ‘¦ğ‘š=ğ‘¥ğ‘›. We regard those acyclic paths as the candidate sequences
forexecution.Forexample,inFigure7,therearethreecandidate
sequences from launch node {ğ´}to last index node {ğ¹}are found
(1){ğ´,ğµ,ğ¶,ğ·,ğ¸,ğ¹ }, (2){ğ´,ğµ,ğ¶,ğ»,ğ¼,ğ¹ }, (3){ğ´,ğµ,ğ¶,ğ»,ğ¼,ğ¸,ğ¹ }. Note
that{ğ´,ğµ,ğ¶,ğ·,ğ¸,ğ·,ğ¸,ğ¹ }is omitted due to cyclic.
Next,foreachcandidatesequence ğ‘†ğ¸ğ‘„,weadoptthedynamic
programming algorithm to find the LCS to the index sequence
ğ‘‹(Lines 17-32), in respect to detect how many index nodes are
coveredinthiscandidatesequence.Therecursivesolutiontodetect
the LCS for each candidate sequence ğ‘†ğ¸ğ‘„to index sequence ğ‘‹can
be defined as:
ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—]=â§âªâªâª â¨
âªâªâªâ©0i fğ‘–=0o rğ‘—=0
com[i-1,j-1]+1 ifğ‘¥[ğ‘–]=ğ‘ ğ‘’ğ‘[ğ‘—]
max(com[i,j-1],com[i-1,j]) if ğ‘¥[ğ‘–]â‰ ğ‘ ğ‘’ğ‘[ğ‘—](4)
whereğ‘ğ‘œğ‘štobeatablefor ğ‘‹andğ‘†ğ¸ğ‘„(e.g.,ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—]istherelation
between ğ‘‹ğ‘–andğ‘†ğ¸ğ‘„ ğ‘—). A LCS can be found by tracing the table
ğ‘ğ‘œğ‘š. We omit the details of tracing the table as ğ¹ğ‘–ğ‘›ğ‘‘ğ¿ğ¶ğ‘†(ğ‘ğ‘œğ‘š)due
tospacelimitations.Notethatweinitializefirstrowandcolumn
ofğ‘ğ‘œğ‘šas zero to prevent the occurrence of NULL (Line 19). For
example,inFigure7,theLCSsbetweenindexsequenceandthree
candidate sequences are (1) {ğ¶,ğ¸,ğ¹}(2){ğ¶,ğ¹}(3){ğ¶,ğ¸,ğ¹}, respec-
tively.
Once the LCSs are detected, we select the candidate sequence
thathasthelongestLCSastheexecutiontraceduetoitreplaysmostkeyframes(orindexnodes)inthevisualrecording.Besides,ourgoal
istohelpdevelopersreproducethebugwiththeleastamountof
time/steps. Therefore, we choose the optimal execution trace with
theshortestsequence.Forexample,inFigure7,(2) {ğ´,ğµ,ğ¶,ğ»,ğ¼,ğ¹ }is
omittedduetoitdoesnotreplaythemostindexnodes,(i.e.,itsLCS
{ğ¶,ğ¹}is not the longest). (3) {ğ´,ğµ,ğ¶,ğ»,ğ¼,ğ¸,ğ¹ }is omitted due to it
is not the optimal trace (i.e., not the shortest sequence). Therefore,
theoptimal executiontrace (1) {ğ´,ğµ,ğ¶,ğ·,ğ¸,ğ¹ }is generated based
on the index sequence, even defective.Algorithm 1: Execution Trace Generation
Inputs : ğ‘‹: index sequence {ğ‘¥1,ğ‘¥2,ğ‘¥3,...,ğ‘¥ ğ‘›};
ğº: UTG graph;
ğ‘ : starting node in UTG;
Output:execution trace
/* Find the candidate sequences ( ğ‘†ğ¸ğ‘„ğ‘  ) from app launch node
to last index node */
1ğ‘†ğ¸ğ‘„ğ‘ â†[] ;
2ğ‘†ğ¸ğ‘„â†[] ;
3ğ‘£ğ‘–ğ‘ ğ‘–ğ‘¡ğ‘’ğ‘‘â†false for all nodes in UTG;
4DFS(ğ‘ ,ğ‘¥ğ‘›,ğ‘£ğ‘–ğ‘ ğ‘–ğ‘¡ğ‘’ğ‘‘,ğ‘†ğ¸ğ‘„ )
5visited[ğ‘ ]â†true ; // prevent acyclic sequence
6ğ‘†ğ¸ğ‘„.ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ (ğ‘ );
7ifğ‘ ==ğ‘¥ğ‘›then
8 ğ‘†ğ¸ğ‘„ğ‘ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ (ğ‘†ğ¸ğ‘„);
9else
10 foreachğ‘£âˆˆğº.Adj[ğ‘ ]do
11 ifvisited[ğ‘£] == false then
12 DFS(ğ‘£,ğ‘¥ğ‘›,ğ‘£ğ‘–ğ‘ ğ‘–ğ‘’ğ‘‘,ğ‘†ğ¸ğ‘„ )
13 end
14end
/* Backtrack to take another path */
15ğ‘†ğ¸ğ‘„.ğ‘ğ‘œğ‘();
16visited[ğ‘ ]â†false ;
/* Find the ğ¿ğ¶ğ‘†ğ‘  between index sequence and each candidate
sequence */
17foreachğ‘†ğ¸ğ‘„âˆˆğ‘†ğ¸ğ‘„ğ‘ do
18letğ‘ğ‘œğ‘š[0..ğ‘š,0..ğ‘›]be new table ;
19initialize ğ‘ğ‘œğ‘šâ†0; // prevent NULL sequence
20forğ‘–â†1toğ‘‹.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ do
21 forğ‘—â†1toğ‘†ğ¸ğ‘„.ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ do
22 ifğ‘¥[ğ‘–]==ğ‘†ğ¸ğ‘„[ğ‘—]then
23 ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—]â†ğ‘ğ‘œğ‘š[ğ‘–âˆ’1,ğ‘—âˆ’1]+1
24 else ifğ‘ğ‘œğ‘š[ğ‘–âˆ’1,ğ‘—]â‰¥ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—âˆ’1]then
25 ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—]â†ğ‘ğ‘œğ‘š[ğ‘–âˆ’1,ğ‘—]
26 else
27 ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—]â†ğ‘ğ‘œğ‘š[ğ‘–,ğ‘—âˆ’1]
28 end
29 end
30end
31ğ¿ğ¶ğ‘†ğ‘ â†save (ğ¹ğ‘–ğ‘›ğ‘‘ğ¿ğ¶ğ‘† (ğ‘ğ‘œğ‘š))
32end
/* Find the execution trace with longest LCSs and shortest
sequences */
33ğ‘‡ğ‘Ÿğ‘ğ‘ğ‘’â†ğ‘†ğ¸ğ‘„ğ‘ [ğ‘šğ‘ğ‘¥(ğ¿ğ¶ğ‘†ğ‘ )&ğ‘šğ‘–ğ‘›(ğ‘†ğ¸ğ‘„ğ‘ )];
34returnğ‘‡ğ‘Ÿğ‘ğ‘ğ‘’
4 AUTOMATED EVALUATION OF GIFDROID
In this section, we describe the procedure we used to evaluate
GIFdroid in terms of its performance automatically. We manually
construct a dataset as the groundtruth for evaluating each step
within our approach, instead of using the real-world bug record-
ingsduetotworeasons.First,manyreal-worldbugreportshave
1050
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. GIFdroid: Automated Replay of Visual Bug Reports for Android Apps ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
been fixed and the app is also patched, but it is hard to find the
correspondingpreviousversionoftheappforreproduction.Second,
the replay of some bug reports (e.g., financial, social apps) requires
much information like authentication/database/hardware to gener-
atetheUTGwhicharebeyondthescopeofthisstudy.Therefore,
we collect 61 visual recordings from 31 open-source Android apps
which were used in previous studies [ 23,26,55]. They are also
top-rated on Google Play covering 14 app categories (e.g., devel-
opment, productivity, etc.). To make these recordings as similar
toreal-worldbugreportsaspossible,weadoptdifferentwaysfor
generatingtherecordingincludingdifferentcreationtools(32from
video conversion, 22 from mobile apps, 7 from emulator screen
recording), varied resolutions (27 1920 Ã—1080, 23 1280 Ã—800, 11
900Ã—600), diverse length (30-305 frames), and differed playing
speed(7-30 framespersecond).
For each app, we also collect its UTG by Google Firebase. Since
our approach consists of three main steps, we evaluate each phase
ofGIFdroid , including Keyframe Location (Section 3.1), GUI Map-
ping (Section 3.2), and Execution Trace Generation (Section 3.3).
Therefore, we ask two experienced developers to manually label
keyframesfromrecordings,GUImappingbetweenrecordingand
UTG, and real trace in the UTG as the groundtruth for each phase.
Note that each human annotator finished the labelling individually
and they discussed the difference until an agreement was reached.
4.1 Accuracy of Keyframe Location
Ground Truth: To evaluate the ability of keyframe location to ac-
curatelyidentifythekeyframespresentinthevisualrecordings,we
manually generated a ground truth for the keyframe. We recruited
two paid annotators from online posting who have experiencein bug replay. To help ensure the validity and consistency of the
groundtruth,wefirstaskedthemtospendtwentyminutesreading
throughthevideoannotationguidelines(i.e.,TRECVID[ 58])and
get familiar with the widely-used annotation tool VirtualDub [ 14].
Then, we assigned the set of visual recordings to them to anno-
tate keyframes independently without any discussion. After the
annotation, the annotators met and sanity corrected the subtle dis-
crepancies(i.e., Â±3frames).Anydisagreementwouldbehanded
over toone authorfor the finaldecision. In total,we obtained289
keyframesfor61recordings,4.73perrecordingonaverage,tallying
with our observations for real-world recordings in Section 2.3.
Metrics: We employ three evaluation metrics, i.e., precision,
recall, F1-score, toevaluate the performance ofkeyframe location.
The predicted frame which lies in the ground truth interval is
regarded as correctly predicted. Since there should be only onekeyframe per interval, if two or more keyframes are localized ina single interval, only the first keyframe is counted as correct.
Precisionistheproportionofframesthatarecorrectlypredictedas
keyframes among all frames predicted as keyframes.
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =#ğ¹ğ‘Ÿğ‘ğ‘šğ‘’ğ‘ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘™ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘ ğ‘˜ğ‘’ğ‘¦ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘ 
#ğ´ğ‘™ğ‘™ ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘  ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘  ğ‘˜ğ‘’ğ‘¦ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘ 
Recall is the proportion of frames that are correctly predicted as
keyframes among all keyframes.
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™=#ğ¹ğ‘Ÿğ‘ğ‘šğ‘’ğ‘ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘™ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘  ğ‘˜ğ‘’ğ‘¦ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘ 
#ğ´ğ‘™ğ‘™ ğ‘˜ğ‘’ğ‘¦ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘ Table 2: Performance comparison for keyframe location.
Method Precision Recall F1-score
Hecate [66] 0.174 0.247 0.204
Comixify [59] 0.244 0.516 0.332
ILS-SUMM [64] 0.255 0.685 0.371
PySceneDetect [9] 0.418 0.550 0.475
GIFdroid 0.858 0.904 0.880
F1-score (F-score or F-measure) is the harmonic mean of precision
and recall, which combine both of the two metrics above.
ğ¹1âˆ’ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’=2Ã—ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› Ã—ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› +ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™
For all metrics, a higher value represents better performance.
Baselines: We set up four state-of-the-art methods which are
widely used for keyframe extraction as the baselines to compare
withourmethod. Comixify [59]isan unsupervisedreinforcement
learningmethodtopredictforeachframeaprobabilitytoextract
keyframesinthevideos. ILS-SUMM [64]formulatesthekeyframe
extraction as an optimization problem, using a meta-heuristic opti-
mization framework to extract a sequence of keyframes that has
minimumfeaturedistance. Hecate[66]isatooldevelopedbyYahoo
thatestimatesframequalityontheimageaestheticsandclusters
them to select the centroid as a keyframe. PySceneDetect [9]i sa
practical tool implemented as a Python library that is popular in
GitHub.ThecoretechniqueforPySceneDetecttodetectkeyframes
isacontent-awaredetectionbyanalyzingthecolor,intensity,mo-tion between frames.
Results:
Table 2 shows the performance of all approaches. The
performance of our method is much better than that of other base-
lines, i.e., 32%, 106%, 14% boost in recall, precision, and F1-score
compared with the best baseline (ILS-SUMM, PySceneDetect). The
issueswiththesebaselinesarethattheyaredesignedforgeneral
videoswhichcontainmorenaturalsceneslikehuman,plants,an-
imals etc. Howev er, different from those videos, our visual bug
recordings belong to artificial artifacts with different rendering
processes. Therefore, considering the characteristics of visual bugrecordings, our approach can work well in extracting keyframes.
Ourapproachalsomakesmistakesinkeyframeextractiondue
to three reasons. First, within some apps, the resource loading is
so slow that the partial GUI may stay for a relatively long time, be-
yond our threshold setting in Section 3.1. So, that frame is wrongly
predicted as akeyframe. Second, some usersmay click the button
beforetheGUIisfullyrenderedtothenextpage.Thatshortperiodmakesourapproachmissthekeyframe.Third,someGUIsmaycon-tainanimatedappelementssuchasadvertisementormovieplaying,
whichwillchangedynamically,resultinginnosteadykeyframes
being localized.
4.2 Accuracy of GUI Mapping
Ground Truth: To evaluate the ability of GUI mapping to accu-
ratelysearchthenearestGUIscreenshotintheUTG,wefirstcol-
lectedtheUTGandtheirGUIscreenshotsforthoseapps,following
the procedure outlined in Section 3.2.1. Given the keyframes (i.e., a
framefromthegroundtruthinterval),weselectedthemostsimilar
1051
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Feng et al.
Figure 8: Examples of bad cases in GUI mapping.
GUIscreenshotintheUTGasthegroundtruthforGUImapping.
Notethat labellingwas finishedby twoannotatorsindependently
without any discussion, and any disagreement would be handed
over to one author for the final decision. Thus, we obtained 289
ground truth pairs of keyframes and GUI screenshots.
Metrics: WeformulatetheproblemofGUImappingasanimage
searching task (i.e., search the most similar GUI screenshot), so we
adoptPrecision@ktoevaluatetheperformanceofGUImapping.
Thehighervalueofthemetricis,thebetterasearchmethodper-
forms.Precision@kistheproportionofthetop-kresultsforaquery
GUI that contains the grouptruth one. Note we only consider ğ‘˜in
therange1-3,asdevelopersrarelycheckalongrecommendation
list.
Baselines: Tofurtherdemonstratetheadvantageofourmethod,
we compare it with 10 image processing baselines, including pixel
level (e.g, euclidean distance [ 33], color histogram [ 72], finger-
print[16]),andstructurallevel(e.g.,SSIM[ 73],SIFT[52],SURF[21],
ORB [63]).
Results: Table 3shows theoverall performance ofall methods.
Incontrastwithbaselines,ourmethodoutperformsinallmetrics,
85.4%, 90.0%, 91.3% for Precision@1, Precision@2, Precision@3
respectively. We observe that the methods based on structural fea-
tures perform much better than pixel features due to the reason
that the pixel similarity suffers from the scale-invariant as the res-
olutions for visual recordings varies. Our method that combines
SSIMandORBleadstoasubstantialimprovement(i.e.,9.7%higher)
overanysinglefeature,indicatingthattheycomplementeachother.
In detail, ORB addresses the image distortion that causes false GUI
mapping considering only SSIM.
Albeitthegoodperformanceofourmethod,westillmakewrong
mapping for some keyframes. We manually check those wrongmapping cases, and find that one common cause is the dynami-
callyloadedcontent.Forexample,asseeninFigure8,someGUIs
mappings look visually very different as the pop-up window or
imagesloadedfromtheinternetmayvaryalotatdifferenttime.As
the dynamic content takes over a large area of the whole GUI, our
approach based on visual features cannot accurately locate them.
4.3 Performance of Trace Generation
Ground Truth: To evaluate the ability of trace generation to ac-
curatelygeneratethereplayableexecutiontrace,welabeltheex-
ecution trace that can replay the visual recording from the app
launchasthegroundtruth.Notethattheremaybemultipleground
Figure 9: Illustration of bad casefor trace generation. Blackline represents ground truth. Red line represents the gener-ated execution trace.
truthtracescomplyingwiththeoptimaldefinition(i.e.,theshort-
est trace to replay the recording) in Section 3.3. Therefore, we
manually check and label all the ground truths by two annotators
independentlywithoutanydiscussion.Toensurethequality,any
disagreement would be handed over to one author for the finaldecision. Totally, we obtained 61 execution traces, including 539
reproduction steps.
Metrics: To measure the similarity of the groundtruth trace
andpredictedreplaysequence,wefirstgettheLCSbetweentwo
sequences. We then calculate the similarity [ 75] following2Ã—ğ‘€
ğ‘‡
whereğ‘€isthelengthofLCS,and ğ‘‡isthelengthofthesumofboth
sequences. It gives a real value with a range [0,1] and is usually
expressedasapercentage.Thehigherthemetricscore,themore
similar the generated execution trace is to the ground truth. If the
generated trace exactly matches the ground truth, the similarity
value is 1 (100%).
Baselines: Wesetuponestate-of-the-artscenariogeneration
method (V2S) and an ablation study of GIFdroid without LCS algo-
rithm(GIFdroid-LCS)asourbaselines. V2S[23]isthefirstpurely
graphicalAndroidrecord-and-replaytechnique.V2Sadoptsdeep
learning models to detect user actions via classifying touch indica-
torsforeachframeinavideo,andconvertstheseintoascriptto
replaythescenarios.NotethatV2Shasstrictrequirementsabout
theinputvideoincludinghighresolution,framespersecondand
touchindicatorforinferringdetailedactions.Therefore,inaddition
to testing V2S in all our datasets, we also test its performance inthe part of our dataset (i.e., 19 recordings) which contain touch
indicatorscalled V2S+Touch.Totesttheimportanceoftheproposed
algorithm that addresses defectiveindexsequence in Section 3.3,
wealsoaddanotherablationstudycalled GIFdroid-LCS whichdoes
not contain that component.
Results: Table4showstheperformancecomparisonwiththe
baselines. Our methodachieves 89.59% sequence similarity which
is much higher than that of baselines. Note that due to the strictrequirement of input recordings, V2S does not work well in allour datasets, but performs well in our partial dataset with touch
indicators.Evenforrecordingswithtouchindicators,theextracted
trace is still not that accurate as it could not recover the trace from
the app launch to the entry in the recording. In a word, the hard
requirementsstilllimititsgeneralityintherealtestingenvironment
especially those open-source software development. In addition,
addingLCScanmitigatetheerrorsintroducedinthefirsttwostepsinourapproach,resultinginaboostofperformancefrom82.63%to89.59%.Although applyingLCStakes abitmoreruntime (i.e.,13.25
seconds on average), it does not influence its real-world usage as it
can be automatically run offline.
1052
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. GIFdroid: Automated Replay of Visual Bug Reports for Android Apps ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Performance comparison for GUI Mapping
MetricPixel Stucture GIFdroid
Euclidean Histogram pHash dHash aHash SSIMSIFTSURFORBSSIM+ORB
Precision@1 0.0% 30.6% 46.6% 50.6% 38.6%75.7%63.6%67.8%75.7% 85.4%
Precision@2 1.3% 47.7% 58.6% 67.8% 61.3%81.3%81.3%77.3%83.7% 90.0%
Precision@3 1.3% 55.7% 63.6% 75.7% 66.6%85.3%89.3%82.6%89.3% 91.3%
Table 4: Performance comparison of execution generation.
Metric V2SV2S+Touch GIFdroid-LCS GIFdroid
Similarity 7.17% 63.19% 82.63% 89.59%
Time (sec) 791.23 856.91 84.66 97.91
Table 5 shows detailed results of the success rate for each visual
recordings, where each app, execution trace (number of steps), and
successfullyreplayedstepsaredisplayed.Greencellsindicateafully
reproduced execution trace from the video recording, Orange cells
indicate more than 50% of steps reproduced, and Red cells indicate
less than 50% of reproduced steps. GIFdroid fully reproduces 82%
(50/61) of the visual recordings, signals a strong replay-ability. We
manuallychecktheinstanceswhereourmethodfailedtoreproduce
scenarios. Instances where slightly biased (orange cells) are largely
due to inaccuracies in keyframe location (i.e., in Figure 9(a), {ğµ}is
missing) and GUI mapping (i.e., in Figure 9(b), {ğµ}is incorrectly
mappingto {ğ·}).Instancesthatfailedtoreproduce(redceils)are
duetotheinaccuraciesonthelastindexasourmethoddependson
thelastindextoendthesearch(i.e.,inFigure9(c),wesearchthe
execution trace on {ğ¸}).
5 USEFULNESS EVALUATION
Weconductauserstudytoevaluatetheusefulnessofthegenerated
execution trace for replaying visual bug recording into real-world
developmentenvironments.Werecruit8participantsincluding6
graduate students(4 Master, 2Ph.D) and 2software developers to
participateintheexperiment.Allstudentshaveatleastone-year
experienceindevelopingAndroidappsandhaveworkedonatleast
one Android apps project as interns in the company. Two software
developers are more professional and work in a large company
(Alibaba) about Android development.
Procedure: Wefirstgivethemanintroductiontoourstudyand
alsoa realexampleto try.Eachparticipant isthenasked torepro-
duce the same set of 10 randomly selected visual bug recordingsfrom GitHubwhichareofdiversedifficultyranging from6 to11
stepsuntiltriggeringbugs.Detailedexperimentdatasetcanbeseen
in our online appendix5. The study involves two groups of four
participants: the experimental group ğ‘ƒ1,ğ‘ƒ2,ğ‘ƒ3,ğ‘ƒ4who gets help
with the generated execution trace by our tool, and the control
groupğ‘ƒ5,ğ‘ƒ6,ğ‘ƒ7,ğ‘ƒ8who starts from scratch. Each pair of partici-
pants/angbracketleftğ‘ƒğ‘¥,ğ‘ƒğ‘¥+4/angbracketrighthavecomparabledevelopmentexperience,sothat
the experimental group has similar capability to the control group
intotal.Notethatwedonotaskparticipantstofinishhalfofthe
5https://sites.google.com/view/gifdroidtasks with our tool while the other half without assisting tool to
avoidpotentialtoolbias.Ourtooltakesabout138.08sonaverage
to generate execution trace for the average 10.59s bug recording as
seen in the Table 6. We only record the time used to reproduce the
visualbugrecordingsinAndroid,astheexecutiontracegeneration
canbefinishedoffline,especiallyforlongrecordingswhichrequire
much processing time. Since our approach is fully automated, our
model can automatically deal with thebug video recording imme-
diately once uploaded. Participants have up to 10 minutes for each
bug replay.
Results: Table 7 shows the experiment result. Although most
participantsfrombothexperimentalandcontrolgroupscansuccess-fullyfinishthebugreplayontime,theexperimentgroupreproducesthevisualbugrecordingmuchfasterthanthatofthecontrolgroup
(with an average of 171.4 seconds versus 65.0 seconds). In fact, the
average time of the control group is underestimated, because three
bugs fail to be reproduced within 10 minutes, which means that
participantsmayneedmoretime.Incontrast,allparticipantsinthe
experimentgroupfinishallthetaskswithin2minutes.Tounder-
standthesignificanceofthedifferencesbetweenthetwogroups,
wecarryouttheMann-WhitneyUtest[ 54](specificallydesigned
for small samples) on the replaying time. The testing result sug-gests that our tool can significantly help the experimental group
reproduce bug recordings more efficiently ( ğ‘âˆ’ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ <0.01).
We summarise two reasons why it takes the control group more
time to finish the reproduction than the experiment group. First,
some visual recording is quite complicated which requires partici-
pantsinthecontrolgrouptowatchthevisualrecordingsseveral
times for following procedures. The GUI transitions within therecording may also be too fast to follow, so developers have to
replay it. Second, it is hard to determine the trigger from one GUI
tothenextone.AsillustratedinSection2.3,only25%ofvideosarerecorded with the touch indicator, resulting in developersâ€™ guess of
theactionfortriggeringthenextGUI.Thattrialanderrormakes
the bug replay process tedious and time-consuming. It is especiallysevereforjuniordeveloperswhoarenotfamiliarwiththeappcode.
6 THREATS TO VALIDITY
We had discussed the limitations of our approach at the end of
eachsubsectionoftheevaluationinSection4,suchaserrorsduetoslowrenderinginkeyframelocation(Section4.1),dynamicloadingcontentinGUImapping(Section4.2),etc.Inthissection,wefurther
discuss the threats to validity of our approach.
Internal Validity. In our automated experiments evaluating
GIFdroid , threats to internal validity may arise from human an-
notatorsandartificialrecordings.Tohelpmitigatethisthreat,we
1053
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Feng et al.
Table 5: Detailed results for execution trace generations. Green cells indicate fully reproduced recordings, orange cells >50%
reproduced, and red cells <50%.
Table 6: The execution time of GIFdroid for real-world bug recordings
AppNameGif GIFdroid Execution
Duration (sec) Keyframe Location (sec) GUI Mapping (sec) Trace Generation (sec) Total (sec)
AnkiDroid-1 16.7 28.05 174.96 0.58 203.59
AnkiDroid-2 9.5 13.95 84.80 1.12 99.87
KISS-1 13.8 16.74 148.51 0.13 165.38
NeuroLab-1 9.6 14.76 71.05 0.01 85.82
NeuroLab-2 4.5 4.76 77.32 0.13 82.21
BeHe-1 11.8 7.10 168.77 0.11 175.98
BeHe-2 6.0 3.73 64.80 0.60 69.13
AndrOBD-1 7.9 10.00 115.56 0.41 125.97
PDFConverter-1 17.4 36.22 191.03 2.32 229.57
PDFConverter-2 8.7 9.99 131.51 1.84 143.34
Average 10.59 14.53 122.83 0.72 138.08
Table7:Performancecomparisonbetweentheexperimental
and control group.âˆ—denotes p<0.01.
AppNameControl Group Experimenal Group
Success Time (sec) Success Time (sec)
AnkiDroid-1 2/4 473 4/4 102
AnkiDroid-2 3/4 334 4/4 95
KISS-1 4/4 121 4/4 82
NeuroLab-1 4/4 90 4/4 73
NeuroLab-2 4/4 366 4/4 62
BeHe-1 4/4 77 4/4 55
BeHe-2 4/4 94 4/4 51
AndrOBD-1 4/4 63 4/4 56
PDFConverter-1 4/4 71 4/4 47
PDFConverter-2 4/4 25 4/4 24
Average 3.7/4 171.4 4/4 65.0âˆ—
recruited two paid annotators from online posting who have expe-
rienceinvideoannotation.Tomitigateanypotentialsubjectivity
orerrors,weaskedthemtoannotateindependentlywithoutany
discussion,andthenmetandsanitycorrectedthediscrepancies.An-
other potential threat concerns the selection of optimal execution
trace.Tomitigatethisthreat,wechosetheshortestcandidatese-
quenceastheoptimalexecutiontracetohelpdevelopersreproduce
the bug with the least amount of time/steps.
External Validity. The main threat to external validity arises
fromthepotentialbiasintheselectionofexperimentalappsusedin
our automated evaluation. To help mitigate this threat, we utilized
theappsusedinpreviousstudies[ 23,26,55],whichhaveundergoneseveralfilteringandqualitycontrolmechanismstoensurediversity.
One more potential external threat concerns the generalizability
on the manual creation of visual recordings for validating the per-
formanceofourapproach.Forexample,therearemanydifferent
types of devices with different screens especially in Android which
mayresultindifferentGUIrendering.Tomitigatethisthreat,we
took care to generate the recordings as general as possible, includ-
ingdifferentcreationtools,variedresolutions,diverselengthand
differedrecor dingspeed.
7 RELATED WORK
Agrowingbodyoftoolshasbeendedicatedtoassistinginrecording
and replaying bugs in mobile and web apps. We introduce related
works of bug replay based on different types of information includ-
ing the app running information, textual description, and visual
recording in this section.
7.1 Bug Record and Replay from App Running
Information
Nurmuradov et al. [ 57] introduced a record and replay tool for An-
droid applications that captures user interactions by displaying the
device screen in a web browser. It used event data captured during
therecordingprocesstogenerateaheatmapthatfacilitatesdevelop-
ers understanding of how users are interacting with an application.
Additional tools including ECHO [ 69], Reran [ 39], Barista [ 42],
andAndroidBotMaker[ 2]areprogram-analysisrelatedapplica-
tions for the developer to record and replay interactions. However,
they required the installation of underlying frameworks such as
replaykit [ 4], troyd [40], or instrumenting apps which is too heavyAppName Rep. Trace AppName Rep. Trace AppName Rep. Trace AppName Rep. Trace
Token 8/8 5/5 TimeTracker 10/10 8/8 TrackerControl 23/24 12/12 YoloSec 9/9 8/10
DeadHash 25/25 18/18 GNUCash 9/9 7/7 aFreeRDP 11/11 10/10 AntennaPod 7/7 8/9
ProtonVPN 7/7 9/9 FastNFitness 8/8 8/10 JioFi 6/6 8/8 WiFiAnalyzer 9/9 5/5
PSLab 8/9 6/6 DroidWeight 6/6 7/7 openScale 18/18 3/18 KeePassDX 6/6 7/7
Trigger 7/7 12/12 ADBungFu 2/8 21/21 EteSyncNotes 11/11 12/12 PortAuthority 8/8 12/12
ATime 7/7 8/8 AdAway 11/11 19/19 StinglePhoto 7/7 6/6 InviZible 6/6 10/10
1054
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. GIFdroid: Automated Replay of Visual Bug Reports for Android Apps ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
for end users. In contrast to these approaches, our GIFdroid is
rather light-weight which just requires the input recording and
UTG which can be generated by existing tools.
7.2 Bug Replay from Textual Information
Tolowertheusagebarrierofrecordandreplaythatrequiresframe-
works, many of the studies [ 22,37,41,55,74] facilitate the bug
replay base on the natural language descriptions in bug report
which contains immediately actionable knowledge (e.g., reliable
reproduction steps),stack traces, test cases,etc. For example,ReC-
Droid [84] leveraged the lexical knowledge in the bug reports and
proposed a natural language processing (NLP) method to auto-
maticallyreproducecrashes.However,ithighlydependedonthe
descriptionwritinginthebugreportincludingformatting,word
usage, granularity [ 24,36,43] which limit its deployment in the
real world. Many works that assist developers in writing bug re-
ports[51,53,55,56]stillcannotdirectlybenefitit.Differentfrom
textual bug reports, visual recordings contain more bug details and
can be easily created by non-technical users. Therefore, our work
focuses on the automated bug replay from these visual bug reports.
7.3 Bug Replay from Visual Information
Insoftwareengineering,therearemanyworksonvisualartifacts
including recording, tutorials, bug reports [ 27,29,32,38,61,78,
82] with some of them specifically for usability and accessibility
testing [30,49,76,81]. In detail, Bao et al. [ 20] focused on the
extractionofuserinteractionstofacilitatebehavioralanalysisofdevelopers during programming tasks, such as code editing, text
selection,andwindowscrolling.Krieteretal.[ 44]analyzedevery
singleframeofavideotogeneratelogfilesthatdescribewhateventsarehappeningattheapplevel(e.g.,the"k"keyispressed).Different
fromtheseworksinextractingdevelopersâ€™behavior,ourworkis
concerned with the automated bug replay from the recording in
the bug report.
Bernal et al [ 23] proposed a tool named V2S which leverages
deep learning techniques to detect and classify user actions (e.g., a
touchindicatorthatgives visualfeedbackwhenauserpresseson
thedevicescreen)capturedinavideorecording,andtranslateitintoareplayabletestscript.Althoughthetargetofthisworkisthesame
asours,therearetwomajordifferencesbetweenthem.First,V2S
requireshigh-resolutionrecordingwithtouchindicatorswhichis
hardtogetinreal-worldbugreportsaccordingtoouranalysis(less
than25.8%)inSection2.2.Second,itrequirescompleterecording
fromtheapplaunchtothebugoccurrencewhilemostrecordings
(93.2%) in the real world do not start from the app launch, but just
2-7 steps before the bug page. In contrast, our approach designisfullydrivenbythepracticaldata, henceitdoes nothavethose
requirements.Theuserstudyofreal-worldbugreplayalsoconfirms
the usefulness and generality of our approach.
8 CONCLUSION
The visual bug recording is trending in bug reports due to its easy
creation and rich information. To help developers automatically
reproduce those bugs, we propose GIFdroid , an image-processing
approach to covert the recording to executable trace to trigger thebug in the Android app. Our automated evaluation shows that our
GIFdroid can successfully reproduce 82% (50/61) visual recordings
from31Androidapps.Theuserstudyonreplaying10real-world
visualbugrecordingsconfirmstheusefulnessofour GIFdroid in
boosting developersâ€™ productivity.
In the future, we will keep improving our method for better per-
formance in term of keyframe extraction, GUI mapping, and trace
generation. For example, the traditional image processing methods
may not robust to minor GUI changes such as configuration and
parameterchanges.Wewillfurtherimproveourapproachtolocatemorefine-grainedparameterinformation.Tomake
GIFdroid more
usable,wewillalsotakethehumanfactorintotheconsideration.Astheautomatedapproachmaynotbeperfect,wewillfurtherexplore
how human can collaborate with the machine for replaying the
bugsinthevisualbugreport.While GIFdroid isfullyautomated,
can run in the background, the execution overhead is not ideal.Inthefuture,wewillimprovetheefficiencyofourapproach,for
example, accelerating by more advanced hardware and algorithm.
REFERENCES
[1]2021. Android Developer: Playback capture. https://developer.android.com/
guide/topics/media/playback-capture.
[2]2021. Bot Maker for Android - Apps on Google Play. https://play.google.com/
store/apps/details?id=com.frapeti.androidbotmaker.
[3] 2021. BugClipper. https://bugclipper.com/.
[4]2021. Commandlinetoolsforrecording,replayingandmirroringtouchscreen
events for Android. https://github.com/appetizerio/replaykit.
[5] 2021. F-Droid. https://f-droid.org/en/packages/.[6] 2021. Firebase - Build and Run Successful Apps. https://firebase.google.com/.[7]
2021. GIPHY: Search All the GIFs, Make Your Own Animated GIF. https://giphy.
com/.
[8] 2021. GitHub. https://github.com/.[9]
2021. Python and OpenCV-based scene cut/transition detection program &
library. https://github.com/Breakthrough/PySceneDetect.
[10]2021. Record the screen on your iPhone, iPad, or iPod touch. https://support.
apple.com/en-us/HT207935.
[11] 2021. Sketch â€” The digital design toolkit. https://www.sketch.com/.[12]
2021. Take a screenshot or record your screen on your Android device. https:
//support.google.com/android/answer/9075928?hl=en.
[13] 2021. TestFairy. https://www.testfairy.com/.[14] 2021. VirtualDub.org. https://www.virtualdub.org/index.[15] 2021. YouTube. https://www.youtube.com/.[16]
MohammadAAlsmirat,FatimahAl-Alem,MahmoudAl-Ayyoub,YaserJararweh,andBrijGupta.2019.Impactofdigitalfingerprintimagequalityonthefingerprintrecognitionaccuracy. MultimediaToolsandApplications 78,3(2019),3649â€“3688.
[17]John Anvik, Lyndon Hiew, and Gail C Murphy. 2005. Coping with an open bug
repository. In Proceedings of the 2005 OOPSLA workshop on Eclipse technology
eXchange. 35â€“39.
[18]Jorge Aranda and Gina Venolia. 2009. The secret life of bugs: Going past theerrors and omissions in software repositories. In 2009 IEEE 31st International
Conference on Software Engineering. IEEE, 298â€“308.
[19]TanzirulAzimandIulianNeamtiu.2013. Targetedanddepth-firstexploration
for systematic testing of android apps. In Proceedings of the 2013 ACM SIGPLAN
international conference on Object oriented programming systems languages &
applications. 641â€“660.
[20]LingfengBao,JingLi,ZhenchangXing,XinyuWang,XinXia,andBoZhou.2017.
Extracting and analyzing time-series HCI data from screen-captured task videos.
Empirical Software Engineering 22, 1 (2017), 134â€“174.
[21]Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. 2006. Surf: S peeded up robust
features. In European conference on computer vision. Springer, 404â€“417.
[22]Jonathan Bell, Nikhil Sarda, and Gail Kaiser. 2013. Chronicler: Lightweightrecording to reproduce field failures. In 2013 35th International Conference on
Software Engineering (ICSE). IEEE, 362â€“371.
[23]Carlos Bernal-CÃ¡rdenas, Nathan Cooper, Kevin Moran, Oscar Chaparro, An-drian Marcus, and Denys Poshyvanyk. 2020. Translating video recordings of
mobileappusagesintoreplayablescenarios.In ProceedingsoftheACM/IEEE42nd
International Conference on Software Engineering. 309â€“321.
[24]Nicolas Bettenburg, Sascha Just, Adrian SchrÃ¶ter, Cathrin Weiss, Rahul Premraj,
andThomasZimmermann.2008. Whatmakesagoodbugreport?.In Proceedings
of the 16th ACM SIGSOFT International Symposium on Foundations of software
1055
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Feng et al.
engineering. 308â€“318.
[25]NicolasBettenburg,RahulPremraj,ThomasZimmermann,andSunghunKim.
2008. Extractingstructuralinformationfrombugreports.In Proceedingsofthe
2008 international working conference on Mining software repositories. 27â€“30.
[26]OscarChaparro,CarlosBernal-CÃ¡rdenas,JingLu,KevinMoran,AndrianMarcus,
MassimilianoDi Penta, DenysPoshyvanyk, andVincent Ng. 2019. Assessingthe
qualityofthestepstoreproduceinbugreports.In Proceedingsofthe201927th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering. 86â€“96.
[27]Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu. 2018.
Fromuidesignimageto guiskeleton:aneuralmachinetranslatortobootstrap
mobileguiimplementation.In Proceedingsofthe40thInternationalConferenceon
Software Engineering. 665â€“676.
[28]HuChen,MingzheSun,andEckehardSteinbach.2009. CompressionofBayer-
patternvideosequencesusingadjusted chromasubsampling. IEEEtransactions
on circuits and systems for video technology 19, 12 (2009), 1891â€“1896.
[29]Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xin Xia, Liming Zhu, JohnGrundy, and Jinshui Wang. 2020. Wireframe-based UI design search through
image autoencoder. ACM Transactions on Software Engineering and Methodology
(TOSEM) 29, 3 (2020), 1â€“31.
[30]Jieshan Chen,Chunyang Chen,Zhenchang Xing,Xiwei Xu,Liming Zhut,Guo-
qiangLi,andJinshuiWang.2020. Unblindyourapps:Predictingnatural-languagelabelsformobileguicomponentsbydeeplearning.In 2020IEEE/ACM42ndInter-
national Conference on Software Engineering (ICSE). IEEE, 322â€“334.
[31]SenChen,LinglingFan,ChunyangChen,TingSu,WenheLi,YangLiu,andLihua
Xu. 2019. Storydroid: Automated generation of storyboard for Android apps.
In2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).
IEEE, 596â€“607.
[32]Nathan Cooper, Carlos Bernal-CÃ¡rdenas, Oscar Chaparro, Kevin Moran, and
Denys Poshyvanyk. 2021. It Takes Two to Tango: Combining Visual and Textual
InformationforDetectingDuplicateVideo-BasedBugReports. arXivpreprint
arXiv:2101.09194 (2021).
[33]Per-Erik Danielsson. 1980. Euclidean distance mapping. Computer Graphics and
image processing 14, 3 (1980), 227â€“248.
[34]BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,
YangLi,JeffreyNichols,andRanjithaKumar.2017. Rico:Amobileappdataset
forbuildingdata-drivendesignapplications.In Proceedingsofthe30thAnnual
ACM Symposium on User Interface Software and Technology. 845â€“854.
[35]Biplab Deka, Zifeng Huang, and Ranjitha Kumar. 2016. ERICA: Interaction
miningmobileapps.In Proceedingsofthe29thAnnualSymposiumonUserInterface
Software and Technology. 767â€“776.
[36]MonaErfaniJoorabchi,MehdiMirzaaghaei,andAliMesbah.2014. Worksforme!
characterizing non-reproducible bugreports. In Proceedings of the11th Working
Conference on Mining Software Repositories. 62â€“71.
[37]Mattia Fazzini, Martin Prammer, Marcelo dâ€™Amorim, and Alessandro Orso. 2018.
Automatically translating bug reports into test cases for mobile apps. In Proceed-
ingsofthe27thACMSIGSOFT InternationalSymposiumonSoftwareTestingand
Analysis. 141â€“152.
[38]ChristianFrisson,SylvainMalacria,GillesBailly,andThierryDutoit.2016. In-
spectorWidget:ASystemtoAnalyzeUsersBehaviorsinTheirApplications.In
Proceedingsofthe2016CHIConferenceExtendedAbstractsonHumanFactorsin
Computing Systems. 1548â€“1554.
[39]Lorenzo Gomez, Iulian Neamtiu, Tanzirul Azim, and Todd Millstein. 2013. Reran:
Timing-andtouch-sensitive recordand replayforandroid. In 201335th Interna-
tional Conference on Software Engineering (ICSE). IEEE, 72â€“81.
[40]Jinseong Jeon and Jeffrey S Foster. 2012. Troyd: Integration testing for android.
Technical Report.
[41]FitsumMesheshaKifetew,WeiJin,RobertoTiella,AlessandroOrso,andPaolo
Tonella.2014. Reproducingfieldfailuresforprogramswithcomplexgrammar-
based input. In 2014 IEEE Seventh International Conference on Software Testing,
Verification and Validation. IEEE, 163â€“172.
[42]AndrewJKoandBradAMyers.2006. Barista:Animplementationframework
for enabling new tools, interaction techniques and views in code editors. InProceedings of the SIGCHI conference on Human Factors in computing systems.
387â€“396.
[43]AGunesKoruandJeffTian.2004. Defecthandlinginmediumandlargeopen
source projects. IEEE software 21, 4 (2004), 54â€“61.
[44]PhilippKrieter andAndreas Breiter. 2018. Analyzing mobileapplicationusage:
generating log files from mobile screen recordings. In Proceedings of the 20th
internationalconferenceonhuman-computerinteractionwithmobiledevicesand
services. 1â€“10.
[45]Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo,
Peng Yan, Yuetang Deng, and Tao Xie. 2017. Record and replay for android: Are
we there yet in industrial cases?. In Proceedings of the 2017 11th joint meeting on
foundations of software engineering. 854â€“859.
[46]Daeho Lee and Sungsoo Lim. 2016. Improved structural similarity metric for the
visible quality measurement of images. Journal of Electronic Imaging 25, 6 (2016),
063015.[47]Toby Jia-Jun Li, Lindsay Popowski, Tom M Mitchell, and Brad A Myers. 2021.
Screen2Vec:SemanticEmbeddingofGUIScreensandGUIComponents. arXiv
preprint arXiv:2101.11103 (2021).
[48]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2017. Droidbot: a
lightweightui-guidedtestinputgeneratorforandroid.In 2017IEEE/ACM39th
International Conference on Software Engineering Companion (ICSE-C). IEEE, 23â€“
26.
[49]Zhe Liu, Chunyang Chen, Junjie Wang, Yuekai Huang, Jun Hu, and Qing Wang.
2020. Owl eyes: Spotting ui display issues via visual understanding. In 2020 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 398â€“409.
[50]Margaret Livingstone and David H Hubel. 2002. Vision and art: The biology of
seeing. Vol. 2. Harry N. Abrams New York.
[51]Rafael Lotufo, Zeeshan Malik, and Krzysztof Czarnecki. 2015. Modelling the
â€˜hurriedâ€™bugreportreadingprocesstosummarizebugreports. EmpiricalSoftware
Engineering 20, 2 (2015), 516â€“548.
[52] DavidGLowe. 2004. Distinctiveimagefeaturesfromscale-invariantkeypoints.
International journal of computer vision 60, 2 (2004), 91â€“110.
[53]Senthil Mani, Rose Catherine, Vibha Singhal Sinha, and Avinava Dubey. 2012.
Ausum: approach for unsupervised bug report summarization. In Proceedings of
the ACMSIGSOFT 20th InternationalSymposium onthe Foundations ofSoftware
Engineering. 1â€“11.
[54]Henry B Mann and Donald R Whitney. 1947. On a test of whether one oftwo random variables is stochastically larger than the other. The annals of
mathematical statistics (1947), 50â€“60.
[55]KevinMoran,MarioLinares-VÃ¡squez,CarlosBernal-CÃ¡rdenas,andDenysPoshy-
vanyk.2015.Auto-completingbugreportsforandroidapplications.In Proceedings
of the 2015 10th Joint Meeting on Foundations of Software Engineering. 673â€“686.
[56]LauraMorenoandAndrianMarcus.2017. Automaticsoftwaresummarization:
the state of the art. In 2017 IEEE/ACM 39th International Conference on Software
Engineering Companion (ICSE-C). IEEE, 511â€“512.
[57]DmitryNurmuradovandReneeBryce.2017. Caret-HM:recordingandreplaying
Android user sessions with heat map generation using UI state clustering. In
Proceedingsofthe26thACMSIGSOFTInternationalSymposiumonSoftwareTesting
and Analysis. 400â€“403.
[58]PaulOver,GeorgeAwad,MartialMichel,JonathanFiscus,GregSanders,BarbaraShaw, Wessel Kraaij, Alan F Smeaton, and Georges QuÃ©ot. 2013. Trecvid 2012-an
overview of the goals, tasks, data, evaluation mechanisms and metrics. (2013).
[59]MaciejPesko,AdamSvystun,PaweÅ‚Andruszkiewicz,PrzemysÅ‚awRokita,and
TomaszTrzciÅ„ski.2019. Comixify:TransformVideoIntoComics. Fundamenta
Informaticae 168, 2-4 (2019), 311â€“333.
[60]Strategic Planning. 2002. The economic impacts of inadequate infrastructure for
software testing. National Institute of Standards and Technology (2002).
[61] Luca Ponzanelli,GabrieleBavota,AndreaMocci,Massimiliano DiPenta,Rocco
Oliveto, Mir Hasan, Barbara Russo, Sonia Haiduc, and Michele Lanza. 2016. Too
long; didnâ€™t watch! extracting relevant fragments from software development
video tutorials. In Proceedings of the 38th International Conference on Software
Engineering. 261â€“272.
[62]Zhengrui Qin, Yutao Tang, Ed Novak, and Qun Li. 2016. Mobiplay: A remote
execution based record-and-replay tool for mobile applications. In Proceedings of
the 38th International Conference on Software Engineering. 571â€“582.
[63]Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. 2011. ORB: An
efficientalternativetoSIFTorSURF.In 2011Internationalconferenceoncomputer
vision. Ieee, 2564â€“2571.
[64]Yair Shemer, Daniel Rotman, and Nahum Shimkin. 2019. ILS-SUMM: Iter-ated Local Search for Unsupervised Video Summarization. arXiv preprint
arXiv:1912.03650 (2019).
[65]Yang Song and Oscar Chaparro. 2020. BEE: a tool for structuring and analyzing
bugreports.In Proceedingsofthe28thACMJointMeetingonEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering.
1551â€“1555.
[66]YaleSong, MiriamRedi,JordiVallmitjana,and AlejandroJaimes.2016. Toclick
or not to click: Automatic selection of beautiful thumbnails from videos. InProceedings of the 25th ACM International on Conference on Information and
Knowledge Management. 659â€“668.
[67]DonnaSpencer.2009. Cardsorting:Designingusablecategories. RosenfeldMedia.
[68]RamadassSudhirandLtDrSSanthoshBaboo.2011. AnefficientCBIRtechnique
with YUV color space and texture features. Computer Engineering and Intelligent
Systems2, 6 (2011), 78â€“85.
[69]YuleiSui,YifeiZhang,WeiZheng,ManqingZhang,andJinglingXue.2019. Event
trace reduction for effective bug replay of Android apps via differential GUIstate analysis. In Proceedings of the 2019 27th ACM Joint Meeting on European
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering. 1095â€“1099.
[70]SureshThummalapenta,SaurabhSinha,NimitSinghania,andSatishChandra.
2012. Automating test automation. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 881â€“891.
1056
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. GIFdroid: Automated Replay of Visual Bug Reports for Android Apps ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[71]ShiqiWang,AbdulRehman,ZhouWang,SiweiMa,andWenGao.2011. SSIM-
motivated rate-distortion optimization for video coding. IEEE Transactions on
Circuits and Systems for Video Technology 22, 4 (2011), 516â€“529.
[72]Xiang-Yang Wang, Jun-Feng Wu, and Hong-Ying Yang. 2010. Robust image
retrieval based on color histogram of local feature regions. Multimedia Tools and
Applications 49, 2 (2010), 323â€“345.
[73]Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image
qualityassessment:fromerrorvisibilitytostructuralsimilarity. IEEEtransactions
on image processing 13, 4 (2004), 600â€“612.
[74]MartinWhite,MarioLinares-VÃ¡squez,PeterJohnson,CarlosBernal-CÃ¡rdenas,
andDenysPoshyvanyk.2015. Generatingreproducibleandreplayablebugre-
portsfromandroidapplicationcrashes.In 2015IEEE23rdInternationalConference
on Program Comprehension. IEEE, 48â€“59.
[75]KrzysztofWoÅ‚kandKrzysztofMarasek.2014. Asentencemeaningbasedalign-
ment methodfor parallel text corporapreparation. In New Perspectivesin Infor-
mation Systems and Technologies, Volume 1. Springer, 229â€“237.
[76]Bo Yang,Zhenchang Xing, XinXia, Chunyang Chen,Deheng Ye,and Shanping
Li.2021. Donâ€™tDoThat!HuntingDownVisualDesignSmellsinComplexUIs
againstDesignGuidelines.In 2021IEEE/ACM43rdInternationalConferenceon
Software Engineering (ICSE). IEEE, 761â€“772.
[77]Shengqian Yang, Haowei Wu, Hailong Zhang, Yan Wang, Chandrasekar Swami-
nathan,DacongYan,and AtanasRountev. 2018. Staticwindowtransitiongraphs
for Android. Automated Software Engineering 25, 4 (2018), 833â€“873.[78]ShengchengYu,ChunrongFang,YexiaoYun,andYangFeng.2021. Layoutand
ImageRecognitionDrivingCross-PlatformAutomatedMobileTesting.In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
1561â€“1571.
[79] Semir Zeki. 1993. A vision of the brain. Blackwell scientific publications.
[80]Dehai Zhao, Zhenchang Xing, Chunyang Chen, Xin Xia, and Guoqiang Li. 2019.
ActionNet:Vision-basedworkflowactionrecognitionfromprogrammingscreen-
casts. In2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). IEEE, 350â€“361.
[81]DehaiZhao,ZhenchangXing,ChunyangChen,XiweiXu,LimingZhu,Guoqiang
Li,andJinshuiWang.2020. Seenomaly:vision-basedlintingofGUIanimation
effects against design-donâ€™t guidelines. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE). IEEE, 1286â€“1297.
[82]Tianming Zhao, Chunyang Chen, Yuanning Liu, and Xiaodong Zhu. 2021.
GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Net-
works.In 2021IEEE/ACM43rdInternational Conferenceon SoftwareEngineering
(ICSE). IEEE, 748â€“760.
[83]Yu Zhao, Kye Miller, Tingting Yu, Wei Zheng, and Minchao Pu. 2019. Automati-
callyExtractingBugReproducingStepsfromAndroidBugReports.In Interna-
tional Conference on Software and Systems Reuse. Springer, 100â€“111.
[84]Yu Zhao, Tingting Yu, Ting Su, Yang Liu, Wei Zheng, Jingzhi Zhang, and
William GJ Halfond. 2019. Recdroid: automatically reproducing android applica-
tion crashes from bug reports. In 2019 IEEE/ACM 41st International Conference on
Software Engineering (ICSE). IEEE, 128â€“139.
1057
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. 