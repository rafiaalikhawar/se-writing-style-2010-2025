PerfSig: Extracting Performance Bug Signatures via
Multi-modality Causal Analysis
Jingzhu He
ShanghaiTech University
Shanghai, China
hejzh1@shanghaitech.edu.cnYuhang Lin
North Carolina State University
Raleigh, NC, USA
ylin34@ncsu.eduXiaohui Gu
North Carolina State University
Raleigh, NC, USA
xgu@ncsu.edu
Chin-Chia Michael Yeh
Visa Research
Palo Alto, CA, USA
miyeh@visa.comZhongfang Zhuang
Visa Research
Palo Alto, CA, USA
zzhuang@visa.com
ABSTRACT
Diagnosing a performance bug triggered in production cloud envi-
ronments is notoriously challenging. Extracting performance bug 
signatures can help cloud operators quickly pinpoint the problem 
and avoid repeating manual efforts for diagnosing similar perfor-
mance bugs. In this paper, we present PerfSig, a multi-modality 
performance bug signature extraction tool which can identify prin-
cipal anomaly patterns and root cause functions for performance 
bugs. PerfSig performs fine-grained anomaly detection over various 
machine data such as system metrics, system logs, and function call 
traces. We then conduct causal analysis across different machine 
data using information theory method to pinpoint the root cause 
function of a performance bug. PerfSig generates bug signatures 
as the combination of the identified anomaly patterns and root 
cause functions. We have implemented a prototype of PerfSig and 
conducted evaluation using 20 real world performance bugs in six 
commonly used cloud systems. Our experimental results show that 
PerfSig captures various kinds of fine-grained anomaly patterns 
from different machine data and successfully identifies the root 
cause functions through multi-modality causal analysis for 19 out 
of 20 tested performance bugs.
KEYWORDS
Debugging, Bug signatures, Software reliability, Performance
ACM Reference Format:
Jingzhu He, Yuhang Lin, Xiaohui Gu, Chin-Chia Michael Yeh, and Zhong-
fang Zhuang. 2022. PerfSig: Extracting Performance Bug Signatures via 
Multi-modality Causal Analysis. In 44th International Conference on Soft-
ware Engineering, May 21‚Äì29, 2022, Pittsburgh, PA. ACM, New York, NY, 
USA, 12 pages. https://doi.org/10.1145/3510003.3510110
1 INTRODUCTION
Cloud systems are becoming increasingly complex, which  dramati-
cally increase the occurrence chance of various software bugs. In
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510110thiswork,wefocusonthoseperformancebugs[ 23,33]whichcause
cloud systems to get stuck in a hang state or experience perfor-
manceslowdown.Performancebugstriggeredinproductioncloud
environmentsarenotoriouslydifficulttodiagnoseandfixduetothe
lack of diagnostic information. When a performance bug occurs in
productioncloudenvironments,systemoperatorsanddevelopers
often need to put a lot of manual efforts to diagnose and fix theproblem under time pressure. For example, it took more than 12
hoursforAmazontorecoveritsmembershipserviceoutagecaused
by a performance bug [ 4]. The bug was triggered by a limit on the
allowablethreadcount,thatis,theserverhungwhenthenumber
of server threads exceeded its pre-defined limit.
During our empirical bug study using popularbug repositories
such as Jira and Bugzilla [ 2,5], we observe that many performance
bugs repeatedly occur in different versions of open source systems,
which causes the community to perform redundant debugging
over thesame bug.Moreover, micro-servicesusing containers[ 6]
make the bug replication easier than ever ‚Äì the same bug occurs inmultiplecontainersthatarecreatedfromthesamecontainerimage.
Tothisend,webelievecreatingsignaturesfordifferentperformance
bugscanhelpsystemoperatorsquicklyidentifyrecurrentbugsandexpeditedebuggingprocess.Aperformancebugsignatureuniquelycharacterizesaperformancebuginbothsymptoms(i.e.,anomalous
resource usages and/or abnormal log sequences) and root cause
functions.
Previousworkonperformancebugdetectionanddiagnosis(e.g.,
[14,19,20,23,25,57,59])hastwomajorlimitationswhenapplying
to the production cloud environment. First, previous work (e.g.,
[14,19,20,23]) mainly focuses on depicting performance bugs via
analysis over single data type such as system metrics, system calls,
system logs, or performance counters. However, a performance
bugmaymanifestasanomaliesindifferentdatatypes.Forexam-
ple, an infinite loop bug maycause a persistently highCPU usage
while a timeout bug can cause abnormal log sequences. Thus, it is
likelythatwemayfailtoextractbugsignaturesforsomeperfor-
mancebugsifweonlyfocusonanalyzingonedatatype.Moreover,
extracting anomalies alone often cannot uniquely characterize a
performance bug because different performance bugs may exhibit
similar anomaly patterns in one data modality. For example, dif-
ferent infinite loop bugs can all show increased CPU consumption.
Thus, it is necessary to perform multi-modality analysis to extract
representativesignaturesfordifferentperformancebugs.Second,
16692022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Jingzhu He et al.
//RPC class
341 public static <T> ProtocolProxy<T>
waitForProtocolProxy(...) throws IOException {
- return waitForProtocolProxy(..., 0,...);
+ return waitForProtocolProxy (...
+ ,getRpcTimeout( conf),...);
346 }
+ public static int getRpcTimeout
(Configuration conf) {
+ return conf.getInt(Com monConfigurationKeys
+ .IPC_CLIENT_RPC_TIMEOUT_KEY,
+ CommonConfigurationKeys
+ .IPC_CLIENT_RPC_TIMEOUT_DEFAULT);
+}
Figure1ThecodesnippetoftheHadoop-11252Bug.Thebuggy
code is invoked at the DataNode.
)	
"#'#%#% !%#!#"!#% 		
)	
	"#'##'#$"! #$%#% 
)	
	"#'##'#$% #! 		$%#%  
)	
"#'#%!"" $#'#! 		
)	
"#'#%!"" #'#$% #! 		
)	
"#'#%!"" #'#$"! #$$  
&(#& 
Figure2LogsgeneratedbyHadoop-11252bug.Thelogsarepro-
duced at the NameNode.
the existingtools [ 25,57,59] arenot application-agnostic. Theex-
isting tools often require domain knowledge extracted from the
sourcecodeorbinarycode.However,suchinformationisnoteasily
accessibleinproductionsystems.Therefore,itisessentialtodesign
a light-weight performance bug signature extraction tool without
requiring domain knowledge.
1.1 A Motivating Example
WeuseHadoop-11252[ 1]bugtoillustratehowaperformance
bug happens and how it manifests in different machine data types.
TherootcauseofHadoop-11252bugisthattheDataNodedoesnot
properly timeout the connection with the NameNode. Timeout is a
commonlyusedfailovermechanismtoclosethebrokenconnection.
AsshowninFigure1,therootcausefunctionis waitForProtocolP
roxyfunctionwhichpasses0timeoutvalue(0meansnevertimeout)
tothetimeoutconfigurationincorrectlyattheDataNodeside.When
the NameNode experiences some unexpected problems such as
network outage, theDataNode hangs on waitingfor the response
fromtheremoteserverwithoutproducinganyerrorinformation.As
showninFigure2,weobservethatthelogentriesthataretypically
produced by server stopping are missing at the NameNode side.
Even if developers can discover the missing log anomaly at the
NameNode side, it is still difficult for them to pinpoint the root
cause function which is actually located at the DataNode side.
1.2 Contribution
In this paper, we present PerfSig, an automatic performance bug
signature extraction tool which performs multi-modality analysis
acrossdifferentmachinedataincludingsystemmetrics,systemlogs,
and function call traces. When a performance alert or service level
objective(SLO)violationisdetected,PerfSigistriggeredtoanalyze
a time window of recent machine data. PerfSig first employs signalprocessingtechniquesandunsupervisedmachinelearningmethods
to identify fine-grained anomaly patterns in various machine data.
For example, for system metrics such as CPU usage time series, we
employfastFouriertransform(FFT)andtimeseriesdiscordmining
toidentifyanomalypatternssuchasfluctuationpatternchanges,
persistentincrease,andcycleperiodchanges.Forsystemlogs,we
identify abnormal log sequences such as missing log entries in
a certain common sequence or overly long time span for certain
sequences.Next,PerfSigperformscausalanalysisbetweenabnor-
mal metric/log patterns and function call traces using information
theorymethodmutualinformation(MI)[ 42].Ourcausalanalysis
reveals the Granger causality (i.e., dependencies) [ 26] between the
anomaliesdetectedindifferentmonitoringdata(e.g.,systemmet-
rics, system logs, function call traces). The goal is to identify the
root cause function which is the top contributor to the metric or
log anomaly. PerfSig outputs the performance bug signature as the
combinationofthedetectedanomalypatternandthepinpointed
root cause function.
Specifically, this paper makes the following contributions.
‚Ä¢Wepresentanewmulti-modalityperformancebugsignature
extractionframeworkwhichcanpreciselydepictaperfor-
mance bug using both fine-grained anomaly patterns and
root cause functions.
‚Ä¢Wedescribeasetoffine-grainedanomalydetectionmethods
to capture specific manifestation of a performance bug in
system metrics or logs.
‚Ä¢Weintroduceaninformationtheorybasedcausalanalysis
approach to pinpointing root cause functions by discover-
ing the causal relationship between function call traces and
anomaly patterns of system metrics or logs.
‚Ä¢We have implemented a prototype of PerfSig and evaluated
it over 20 real-world bugs that are discovered in six com-
monly used cloud systems. The results show that PerfSig
can produce precise signatures for 19 out of 20 performance
bugs.
Therestofthepaperisorganizedasfollows.Section2discusses
thesystemdesigndetails.Section3presentstheexperimentalevalu-
ation.Section4discussesthethreatstovalidity.Section5discusses
the related work. Section 6 concludes the paper. Section 7 presents
the data availability.
2 SYSTEM DESIGN
Inthissection,wedescribethesystemdesignofthePerfSigsystem
indetails.Wefirstgiveanoverviewofthesystem.Next,wepresent
howweidentifyvariousfine-grainedanomalypatternsinsystem
metricandsystemlogdata,respectively.Finally,wediscusshow
weperform causalanalysisbetweensystemmetric/log anomalies
andfunctioncalltracestoidentifytherootcausefunctionwhich
contributes to the anomaly.
2.1 Approach Overview
PerfSig adopts a two-phase approach to extracting signatures for a
performance bug, shown by Figure 3. When a performance alert is
generated or a service level objective (SLO) violation is detected,
PerfSigistriggeredtoextractperformancebugsignatureson-the-fly
by analyzing a recent time window of system metrics, system logs,
1670PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Figure 3 The architecture overview of PerfSig.
andfunctioncalltraces.Duringthefirstphase(PhaseI),PerfSigem-
ploysvarioussignalprocessingandmachinelearningtechniquesto
extract fine-grained anomaly patterns to capture the manifestation
of the performance bug in either system metrics or system logs.
Specifically, for system metrics, PerfSig uses time series analysis
schemes to extract principal features such as fluctuation pattern
changes, persistent increases, and cycle period changes (Section
2.2). For system logs, we leverage classification and frequent se-
quenceminingtoextractanomalouslogpatternssuchasmissing
certainlogentriesoroverlylongtimespanofcertainlogsequences
(Section2.3).Duringthesecondphase(PhaseII),PerfSigusesin-
formation theory approach to performing causal analysis between
functioncalltracesanddetectedabnormalsystemmetricorsystem
log patterns to pinpoint root cause functions (Section 2.4). Per-
formancebugsoftenmanifestasabnormalresourceconsumption
and/orsystemlogoutputs(i.e.,performancebugsymptoms),which
are typically caused by the root cause functions (e.g., infinite loops,
missing timeout, costly operations). Therefore, we leverage the
Granger causality between anomaly patterns and function time
spananomaliestoidentifytherootcausefunctions.Combiningthe
phaseIandPhaseIIresults,PerfSigoutputstheperformancebug
signature as the combination of the fine-grained anomaly pattern
and the pinpointed root cause function.
2.2 System Metric Anomaly Pattern Detection
Manyperformancebugscanmanifestaschangesinsystemmetrics
such as CPU utilization, memory utilization, and network traffic.
However,differentperformancebugscanexhibitdifferentanomaly
patterns.Forexample,Figure4showsdifferentCPUusageabnormal
patterns for three real performance bugs. To extract distinctive
signatures for different performance bugs, we need to not only
detect anomalies but also extract fine-grained anomaly patterns.
WeobservethatthesystemmetricssuchasCPUconsumption
are inherently fluctuating. In order to extract principal anomaly
patterns,wefirstleveragelowpassfilterstoremoverandomnoises
from the raw system metric time series. The low pass filter per-
forms data denoising by filtering out high frequency signals in
original system metric time series. The rationale is that random
fluctuations usually manifest as the high frequency signals. We
transformthetimeseriestothesignalsinfrequencydomainanddrop high frequency signals. Note that we use relational values
instead of absolute values, which avoids setting manual thresh-
olds. If we choose a too large filtering percentage, we filter out
toomanysignalswhichmightincludeanomalies.Ifwechoosea
too small filtering percentage, we cannot filter out noises. In our
experiment, we filter out top 50% high frequency signals. After
that, we transform the signals in the frequency domain back to
the time series. We conduct extensive experiments to compare the
time series patterns before and after performing low pass filters.
Theresultsshowthattheanomalypatternsbecomemoresalient
after filtering. For example, Figure 4b and Figure 9a show the same
CPU usage patterns before and after the filtering, respectively. We
can see the anomaly pattern is much clearer in Figure 9a. Next,
we employ signal processing methods over denoised time series to
extract principal anomaly patterns.
Fluctuationpatternchanges. Fordynamicdata-intensivecom-
puting systems such as Hadoop, CPU utilization usually has pe-
riodical large fluctuations during normal run. It is because the
application workload contains different types of interleaving jobs.
For example, Figure 4a shows the CPU utilization‚Äôs fluctuation
change when Hadoop-15415 bug occurs. During the normal run
(thefirsthalfofthefigure),weobservelargefluctuations.Afterthe
bug is triggered (the second half of the figure), the system hangs
inside an infinite loop which fully consumes one CPU core and
thenCPUutilizationstaysatasteadyvalue.Weobservethatmany
hang bugs in dynamic data-intensive systems often manifest as
fluctuation pattern changes which refer to the cases when the sys-
tem usage changes from normal fluctuating patterns caused by
dynamic workloads during normal runs tonearly non-fluctuating
patternscausedbythehangbugsduringbuggyruns.Tocapture
this anomaly pattern, PerfSig calculates the standard deviations
of a moving window in the system metric time series and identify
the time when the moving window standard deviation experiences
significant changes (e.g., dropping from a large value to a small
value).
Persistentincreases. Besidessoftwarehangbugs,slowdown
bugsareanothercommoncategoryofperformancebugs.Weob-
serve slowdown bugs caused by code inefficiency often consumes
alargeamountofcomputationalresources(e.g.,CPU)duringthe
abnormalperiod.Forexample,Figure4bshowstheCPUincrease
1671ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Jingzhu He et al.
(a)Hadoop-15415bug‚Äôsfluctuationpatternchange.
The bug starts at 10:33:02.
(b)Hadoop-6133bug‚Äôspersistentincrease.Thebug
starts at 00:02:54 and ends at 00:03:30.
(c)Cassandra-7330bug‚Äôscyclicpatternchange.The
bug starts at 01:48:50.
Figure 4 Three commonly seen system metric anomaly patterns.
caused by the Hadoop-6133 bug. PerfSig detects such abnormal
pattern using time series discord [ 56]. A sliding window is applied
onthefilteredsystemmetrictimeserieswherethenearestneigh-
bor distance [ 56] between the previous sliding windows and the
current sliding window is computed. PerfSig detects the persis-
tent increase pattern when the nearest neighbor distance shows
significant increases.
Cyclicpatternchanges. Manyproductionserversystemsex-
hibitcyclicresourceconsumptionpatterns.Itisbecauseproduction
server resource usage patterns are typically drivenby production
workloadpatterns.Whentheproductionworkloadexhibitsregu-
lar patterns, the corresponding system usage patterns show cyclic
patterns. We observe that when a performance bug is triggered,
the workload changes, leading to the cyclic resource usage pattern
changes. Figure 4c shows the CPU cyclic pattern change caused
bytheCassandra-7330bug.Duringnormalrun,CPUshowsacy-
cleofnineseconds,whileduringbuggyrun,CPUdoesnotshow
cyclic pattern. To detect such cyclic pattern changes, we employ
fast Fourier transform (FFT) algorithm on a sliding window of sys-
temmetrictimeseriestoextractthedominatingfrequencieswhose
magnitude values are in the top rank list. We detect the cyclic
pattern change when the top frequency values experience changes.
2.3 System Log Anomaly Pattern Detection
We now describe how PerfSig extracts anomaly patterns from sys-
tem logs. Much existing work focuses on detecting abnormal error
logs that only appear in a buggy run. However, we observe that
when a performance bug is triggered, the system usually does not
produce any error log message. Instead, some log entries included
inthenormalrunaremissingduringthebuggyrun,whicharequite
common among software hang bugs. In other cases like slowdown
bugs,somelogsequencescouldexhibitlongertimespan(i.e.,the
timedurationfromthestarttimeofthefirstlogentryinasequence
totheendtimeofthelastlogentryinasequence)duringthebuggy
run when compared with normal run.
Previousworkinreconstructingexecutionpathfromthesystem
logs contains three limitations: 1) focusing on sequential task exe-
cution [25]; 2) is combined with domain knowledge extracted from
binarycode[ 59];and3)isnotgenericforallthesystems[ 57].Com-
pared with the existing work, PerfSig considers concurrent task
executionandonlytakesthelogentriesandvectortimestampas
the input. PerfSig does not require any application-specific knowl-
edge.Todiscoverthelogsequencesfrominterleavinglogentries,
PerfSigfirstclassifiesthelogentriesgeneratedbydifferenttasks.	




	


Figure 5 The log entry classification framework.
After the task classification is done, we further separate the log
entriesbasedonthetimegaps.Thenweperformfrequentsequence
mining to extract the log sequences.
Semantics-based Grouping. After we collect logs from dis-
tributedhosts,PerfSigclassifieslogsgeneratedbydifferenttasks.
The idea is that logs generated by the same task have similar se-
manticmeanings.Toachievethisgoal,weusethewordembedding
vector [45] to represent each word‚Äôs contextual meaning. After
that, we use the average of the word embedding vectors of the
words‚Äôtorepresenteachlogentry,whichiscommonforgenerating
representation for natural language sentences [ 39]. Intuitively, log
entrieswhichhavesimilarmeaningshavesimilarwordembedding
vectors. Therefore, we can apply clustering algorithm to grouping
the similar log entries together. Figure 5 summarizes the classifi-
cation procedures. Specifically, PerfSig pre-processes the logs to
splitthelogentriesintowords,extractswordembeddingvectors
foreachword,buildslogentryrepresentationbyaggregatingthe
word embeddings associated with each log entry, and classifies the
entries generated by different tasks with the Self-Organizing Map
(SOM) algorithm [36].
First, we pre-process each log entry to extract the words for
learningwordembeddingvectors.Thefirststepistosplitthelog
entries based on brackets and parentheses, because content inside
a pairofbrackets andparentheses oftenrepresents onecommand
oroperation,e.g.,databasequerycommand.Afterthat,wesplitthe
logentrybasedoncomma,fullstop,colon,andsemi-colon.Then
we further separate them according to the spaces between words.
Oncewehaveextractedwordsfromthelogentries,wetreateach
entry as a sentence and learns the word embedding representation
foreachword.Themajoradvantageofusingwordembeddingis
that it considers words‚Äô semantic meaning in the contexts. It is
basedonthehypothesisthatwordsoccurredinthesimilarcontexts
tend to be semantically similar. We choose to use word embedding
asthefeaturevectorbecauselogentriesgeneratedbythesametask
typicallyuseshortsentencewithsimilarterms.Weuseword2vec
with Continuous Bag of Words (CBoW) [ 45] to learn the word
embedding vector to represent each word‚Äôs meaning. Each word
embeddingisinitializedasa ùëõ-dimensionalvector.Ineachiteration,
1672PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA

	
!
!
!
!

!
!


	
 
 
 
	 
 
 
Figure6Thelogentries‚Äôembeddingvectorsformstwoclusters,
one for each task.
wetraineachword‚Äôsembeddingusingthesumof ùëösurrounding
context words‚Äô embedding vectors. We update each word‚Äôs embed-
ding vector until the convergence is reached. In our experiment, ùëõ
is set to 100 and ùëöis set to five.
Weconstructeachlogentry‚Äôsembeddingvectorbytakingthe
average of all the words‚Äô embedding vectors in the log entry. After
that, we apply the SOM clustering algorithm [ 36] to clustering log
entriesthatbelongtodifferenttasks.Comparedwiththetraditional
distance-based methods, SOM model has better performance to
cluster high-dimensional vectors.
Figure6showsanexampleofclassificationresults.Thelogen-
triesaresplitintotwoclustersandeachclusterrepresentsonetype
oftask.Forexample,logclusterArepresentsHadoopsystemestab-
lishes IPC connection between different processes. Log cluster B
represents Hadoop system runs the map and reduce computational
jobs.
FrequentSequence Mining: After task classification,we suc-
cessfullyseparateinterleavinglogsintologentryclusters,repre-
sentingdifferenttasks.Thegoalofthisstepistoextractfrequent
systemlogsequenceswitheachlogcluster,whichoftenrepresenta
setofexecutionsuchasclient-serverconnectionorthreadscommu-
nication.Performancebugsmanifestasmissing logentriesinthe
logsequenceorthelogsequencehasabnormaltimespan.Forexam-
ple, Figure 2 shows a complete log sequence. When Hadoop-11252
bughappens,thelastthreelogentriesaremissing.Ourideaisto
only extract the constants (log keys) from the log entries. Then we
split thelog entriesbased on thetime gapsand extract frequently
occurred log sequences.
The log entries contain variables like socket reader number and
portnumber,whicharedifferentineachIPCconnection.Weextract
the constants (log keys) by adopting simple regular expressions.
Afterwesplitthelogentriesintowords,wekeepthewordsthat
only contain alphabetic letters and replace other words with ‚Äú ‚àó‚Äù.
Then we concatenate the words with a space as the log key. For
eachlogentrycluster,weseparateitbasedonthetimegaps.The
rationale is based on the observation that the same log sequence
occursafterlongtimegapcomparedwithitstimespan.Wecalculate
thetimegapsbetweeneachtwoconsecutivelogentriesandderives
theaveragevalueandstandarddeviationofeachcluster.Ifthetime
gapbetweentwoconsecutivelogentriesexceedstheaveragevalue
+ 2.0√óstandard deviation, we separate the log cluster.
After we get separated log clusters and replace each log en-
try with the log key, we perform frequent sequence mining to# 
"
	! 
# 
"
	! 
# 
"
	! 
# 
"
	! 	


 	
Figure 7 Logs generated by HDFS-4301 bug.
extract log sequences from normal run data. We use PrefixSpan
[28]toperformfrequentsequenceminingbecauseitismoreeffi-
cient compared with other methods. After we extract top frequent
logsequencesfromnormalrun,weusethemtodetectanomalies
duringbuggyrun.Specifically,wedetecttwoanomalypatterns,i.e.,
missing logs and abnormal log sequence time span.
Missing log entries anomaly pattern : During the anomaly
detection phase, we first perform semantics-based grouping. In
eachcluster,weextractlogkeysandcheckwhetherthelogkeys
belongtoanyextractedlogsequence.Iftheanswerisyes,wecheck
whetherotherlogkeysofthelogsequencealsoappearinthelog
cluster. For example, Figure 2 shows how we detect Hadoop-11252
bug. During normal run, we extract the complete log sequence
from starting the socket reader to stopping the server responder.
During buggy run, we perform semantic-based grouping to cluster
logentriesgeneratedfromIPCconnectiontaskstogether.Forthe
extracted the log sequence, We find the log keys to start the server
but we cannot find the log keys to stop the server.
Excessive time span anomaly pattern : Similar to missing log
patterndetection,weperformsemantics-basedgroupingandlog
key extraction. If all the log keys of one particular log sequence
appearinonecluster,PerSigextractsthelogsequencetimespan
whichstartsfromthefirstlogkey‚Äôsoccurringtimetothelastlog
key‚Äôs occurring time. If the time span is excessive long, PerSig
reports the abnormal log sequence time span pattern. For example,
Figure 7 shows how we detect HDFS-4301 bug. During normal run,
weextractthelogsequenceandthelogsequencespanssixseconds.
Duringbuggyrun,thelogsequencespans97secondsandPerSig
reports the abnormal log sequence time span pattern.
2.4 Root Cause Analysis via Multi-modality
Causal Analysis
Afterweidentifytheanomalypatterns,weperformcausalanalysis
to localize the root cause function.
2.4.1 Causal Analysis between System Metrics and Function Call
Traces.After we identify three anomaly patterns of the system
metric time series, we retrieve a window of the filtered time series
which contains the anomaly patterns. Suppose the bug happens
betweentime [ùë°1,ùë°2],then weretrievethe systemmetricbetween
[ùë°1‚àíùë§,ùë°2+ùë§],whereùë§istheparametertocontrolthewindow
size.Thenweretrieveallthefunctioncalltraceswhichoccursin
1673ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Jingzhu He et al.
{"i":"296 0606995495142", "s":"ab3c06709e 3bbab4",
"b":1622001774031 ,"e":1622001774058,
"d":"DFSInputStream#byteArrayRead",
"r":"YarnChild", "p":[],
"n":{"path":".../job_1621397508582_0005/job.split"}}
...
{"i":"37dada 28edec1ea0" ,"s":"5cbe565 f9c5a3c8b",
"b":1622001775036 ,"e":1622001810393,
"d":"conf.getClassByName", "r":"Test","p":[]}
Figure8Theextractedfunctioncalltraces.‚Äúb‚Äùrepresentsfunc-
tion‚Äôsstarttimestamp,‚Äúe‚Äùrepresentsfunction‚Äôsendtimestamp
and‚Äúd‚Äùrepresentsthefunctionname.Allthetraceshavethestart
time no earlier than 00:02:54 and end time no later than 00:03:30.
thetimewindow,i.e, [ùë°1‚àíùë§,ùë°2+ùë§].Forexample,Figure9ashows
the CPU utilization time series. It contains the CPU spike occurred
between00:02:54and00:03:30.Weset ùë§tooneminute,therefore,
we includethe normalrun data from00:01:54 to 00:02:54and data
from 00:03:30 to 00:04:30, because we want to include the sharp
CPUchangesat00:02:54and00:03:30.Includingnormalrundata
can increase the accuracy of causal analysis. If we only consider
the buggy run data, the frequently used functions also occur in
the buggy run and may infer a high causality score. However, if
we consider both normal run and buggy run, the frequent used
functionsalwaysappear,whichdecreasesthecausalrelationship
betweenthesystemmetricsandthem.Figure8showstheretrieved
function call trace snippet. All the function call traces are in the
chronologicalorder.Weextractallthefunctioncalltracesoccurred
between00:02:54and00:03:30,i.e‚Äûtheirstarttimesarenoearlier
than 00:02:54 and end time no later than 00:03:30. We extract three
kinds of information, i.e, beginning timestamp, ending timestamp
andfunctionname,toformulatethefunctioncalltraceintotime
series.
Next,weformulatethefunctioncalltracesintothetimeseries.
we extract the aggregated time span of each function between two
consecutivetimepointsofsystemmetrictimeseries.IfCPUutiliza-
tion are sampled at ùë°1,ùë°2,...,ùë°ùëÅ, then the function time series can
beformulatedas ùëá[ùë°1,ùë°2),ùëá[ùë°2,ùë°3),...,ùëá[ùë°ùëÅ,ùë°ùëÅ+1),whereùëá[ùë°ùëñ,ùë°ùëñ+1)
represents the function‚Äôs aggregated time span during [ùë°ùëñ,ùë°ùëñ+1)
period.Forexample,Figure9bshowsthe conf.getClassByName
functiontimeseries.Duringthetime[00:02:25,00:02:26),thebug
happens and conf.getClassByName is invoked for all the one sec-
ond period.Therefore, conf.getClassByName time serieshas the
value of 1000 milliseconds at the time point 00:02:25. If ùëÄfunc-
tions are invoked during the whole time window, then there are ùëÄ
function time series. In this way, the function time series is aligned
withsystemmetrictimeseries,andwecanapplycausalanalysis
algorithm on them. We extract the time span as the function call‚Äôs
feature because performance issues such as hang or slowdown,
usually manifest as the changes in function time spans.
After we get all the function time series, we normalize each
functiontimeseriesandthesystemmetrictimeseries.Afternor-
malization,thesumofallthedatapointsinonetimeseriesisequal
to one.Weadopt information theoreticmethod, i.e.,mutual infor-
mation(MI)[ 3,42,44],toinferthecausalrelationshipbetweeneach
function‚ÄôstimeseriesandCPUtimeseries.Becauseperformance
bugsoftenmanifestasabnormalsystemusagesand/orabnormal
(a)TheCPUutilizationtimeseriesafterfiltering.Thebugstartsat00:02:54and
endsat00:03:30.Weretrieveoneminutenormalrundatabeforeandafterbug
is triggered.
(b)ThefunctioncalltimeseriesinHadoop-6133bug. conf.getClassByName is
therootcausefunctionand DFSInputStream#byteArrayRead isannon-rootcause
function.
Figure 9 Hadoop-6133 bug‚Äôs time series.
systemlogoutputs,weleveragetheGrangercausalitybetweensys-
temanomaliesandfunctiontimespananomaliestoidentifyroot
cause functions. Mutual information is one of the entropy-based
methods which are important methods to perform exploratory
causal analysis for time series data [ 42,52]. Compared with linear
correlationmethodssuchasPearsonandSpearmancoefficient,mu-
tualinformationcapturesnon-linearcausalrelationshipbetween
two time series. Mutual information measures how much knowing
one of the two time series reduces uncertainty about the other.
Mutualinformationnotonlyconsidertheabsolutevalueofeach
data point, but also consider the data distribution across the whole
time series. Therefore, we can filter out the frequently invoked
functions. It is because the frequently invoked functions have long
time span during both buggy run and normal run. Considering the
data distribution, they cannot have a larger mutual information
than those functions which are only frequently invoked during
buggyrun.Incomparison,thefrequentinvokedfunctionscanhave
highcorrelationscoreswithsystemmetrictimeseriesbecausethey
have long time spans during buggy run.
Mutual information between two time series ùëãandùëåis defined
as:
ùëÄùêº(ùëã,ùëå)=/summationdisplay.1
ùë•,ùë¶ùëù(ùë•,ùë¶)log(ùëù(ùë•,ùë¶)
ùëù(ùë•)ùëù(ùë¶)) (1)
whereùëù(ùë•)andùëù(ùë¶)representtheprobabilitiesof ùëãandùëåoccurred
atthesamplingpoint. ùëù(ùë•,ùë¶)representstheprobabilitythat ùëãand
ùëåoccur at the same time.
We calculate the mutual information between each function
time series and the system metric time series. We rank all the
candidate functions based on the mutual information scores. We
1674PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
(a) The log sequence time series. The bug is triggered at 12:05:58.
(b)Thefunctioncalltimeseries. RPC.waitForProtocolProxy istherootcause
function and ClientProtocol.create is an non-root cause function.
Figure 10 Hadoop-11252 bug‚Äôs time series.
then determine the function that contributes to the anomaly most
as the one with the largest mutual information.
2.4.2 CausalAnalysisbetweenSystemLogsandFunctionCallTraces.
Weperformcausalanalysisonfunctiontimeseriesandlogsequence
timeseriesto localize therootcausefunctionthat contributesto
theloganomaly.Eventhoughsystemlogscontainrichinformation,
e.g., the class name that generates the logs, it is still essential to
perform causal analysis, because the function that generates the
log sequence is usually not the root cause function.
Afterweidentifythemissinglogorlongerexecutionpatterns,
we retrieve the log sequence‚Äôs information, i.e., beginning time
and end time, during both normal run and buggy run. The start
timeisthefirstlogentry‚Äôsinvokingtimeandtheendtimeisthe
last log entry‚Äôs invoking time. For example, in Figure 2, the log
sequence start time is 12:04:53 and end time is 12:04:56. If missing
logpatternhappens,weregarditstimespanasinfinitybecausethe
log sequenceneverends. Besidesthat, weretrieve allthe function
call trace within the same time window, i.e, during the period that
the log sequences are produced.
We formulate the log sequences and the function call traces
into time series. We sample them every one second and collect the
aggregatedtimespanduringtheonesecondinterval.Itissimilar
to how we formulate the function time series in Section 2.4.1. Note
thatthefunctiontimeseriesstillneedstobealignedwiththelog
sequencetimeseries.Figure10ashowsthelogsequencetimeseries
and Figure 10b shows the function call time series. The root cause
function has similar shape with the log sequence.
After generatinglog sequence and functiontime series, we cal-
culate the mutual information between each function‚Äôs time series
and the log sequence to identify the root cause function, similar to
Section 2.4.1.3 EXPERIMENTAL EVALUATION
In this section, we first present the evaluation methodology fol-
lowed by the experimental results. Next, we present several real
bugexamplesincludingonenegativecasestudywherePerfSigfails
to extract a signature for the bug.
3.1 Evaluation Methodology
Cloudsystems: Westudied20realperformancebugsfromsixcom-
monlyusedopen-sourcecloudsystems:Hadoopcommonlibrary,
HadoopMapReducebigdataprocessingframework,HadoopHDFS
filesystem,HadoopYarnresourcemanagementservice,HBasedata-
basesystem,andCassandradatabasesystem.FourHadoopsystems
aresetupindistributedmodestoevaluatePerfSig‚Äôseffectiveness
over distributed system performance bugs.
Benchmarks: We use the ‚Äúhang‚Äù, ‚Äústuck‚Äù, ‚Äúblock‚Äù, ‚Äúlog‚Äù, ‚ÄúCPU‚Äù,
‚Äúperformance‚Äùand‚Äúslowdown‚Äùkeywordstosearchforperformance
bugs. We manually examine each bug to determine whether it is a
real performance bug triggered in production environments and
whether it is reproducible in deterministic ways. To the best of our
efforts, we successfully reproduced 20 bugs in six cloud systems.
Table 1 shows our collected bug benchmark.
Setup:All the experiments wereconducted in our lab machine
withanInteli7-4790Octa-core3.6GHzCPU,16GBmemory,running
64-bit Ubuntu v16.04 with kernel v4.4.0.
3.2 Implementation
Functioncalltracing: Thefunctioncalltracesarecollectedusing
GoogleDapperframework.Dapperhasvariousimplementations
ondifferentproductionsystems.Forexample,animplementation
ofDapper,HTrace[ 7]isintegratedintoHadoopsinceversion2.7.0.
Another implementation of Dapper, Zipkin [ 9] is integrated into
Hadoop, HBase, and Cassandra. Those implementations collect
tracesforerror-pronefunctionsincloudsystems.Wecanconfigure
theparametersforDappertracingintheconfigurationfilesdirectly
and deploy the production systems to trace the function calls.
Anomalypatternanalysis: Weimplementtheanomalypat-
tern detection in Python 3.9. We use scikit-learn [ 46] package to
implementFFTanalysisandlogclassification.WeusetheWord2vec-
CBoW implemented in Gensim [47] for embedding learning.
Hyperparameter in word embedding learning: We use the
Word2vec-CBoW algorithm [ 45] with the embedding vector size
of 100 and the window size of 5 words. The SOM is set to be 5x5
grid map and the weight vector length in SOM is set to be 100, the
same as the embedding vector size.
3.3 Alternative Approaches
Performance bug signature is quite new. Previous tools only ad-
dressed partial problems. PerfSig first provides a comprehensive
end-to-endsolutiontoextractbothsymptomsandrootcausesas
thebugsignatures.TocomparewithPerfSig,weimplementseveral
alternative approaches to perform log analysis and causal analysis.
3.3.1 Logs Anomaly Pattern Detection. For system log anomaly
pattern detection, we implement four alternative approaches to
compare with PerfSig.
1675ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Jingzhu He et al.
Table 1 Bug benchmark. bug ID‚àórepresents it is a distributed system performance bug.
Bug ID Version Symptom Description
Cassandra-7330 2.0.8 Hang The corrupted InputStream returns error code, causing an infinite loop.
Cassandra-9881 2.0.8 Hang Improper exception handling skips loop index-forwarding API, causing an infinite loop.
Hadoop-11252‚àó2.5.0 Hang the RPC connection timeout is missing, leading to system hanging.
Hadoop-15415 2.5.0 Hang Misconfigured parameter indirectly affects loop index, causing an infinite loop.
Hadoop-5318 0.19.0 Slowdown The AtomicLong operations cause contention with multiple threads.
Hadoop-6133 0.20.0 Slowdown Extra calls cause 80x performance slowdown.
Hadoop-8614 0.23.0 Hang Skipping after EOF returns error code, affecting loop stride.
Hadoop-9106‚àó3.0.0-alpha1 Slowdown IPC connection timeout is hard-coded, causing a much longer failure recovery time.
MapReduce-5066‚àó2.0.3-alpha Hang Timeout is missing when JobTracker calls a URL, causing system hanging.
MapReduce-4089 2.0.0-alpha Hang Task status updates cause hung task never timeout.
MapReduce-3862 0.23.1 Hang NodeManager hangs on shutdown due to struggling DeletionService threads.
MapReduce-7089 2.5.0 Hang Misconfigured variable causes loop stride to be set to 0.
MapReduce-6990 2.5.0 Hang Skipping on a corrupted InputStream returns error code, affecting loop stride.
HDFS-1490‚àó2.0.2-alpha Hang Timeout is missing for image transfer operations, causing system hanging.
HDFS-4301‚àó2.0.3-alpha Slowdown Timeout value on image transfer operation is large.
HDFS-7005‚àó2.5.0 Hang DFS input streams do not timeout.
HBase-17341 1.3.0 Slowdown Timeout is missing for terminating replication endpoint.
Yarn-163 2.0.0-alpha Hang Skipping on a corrupted FileReader returns error code, affecting loop stride.
Yarn-1630 2.2.0 Hang YarnClient endlessly polls the state of an asynchronized application.
Yarn-2905 2.5.0 Hang Skipping on a corrupted aggregated log file returns error code, affecting loop stride.
DBScan-embedding: We usethe same embedding representa-
tion. However, instead of the SOM clustering algorithm, we use
DBScan[ 49]togroupthelogentriesgeneratedbydifferenttasks.
DBScan is a popular non-parametric density-based clustering al-
gorithm. It automatically determines the number of clusters when
trained.
SOM-TFIDF: We use Term-Frequency-Inverse-Document-Freq-
uency (TFIDF) [ 34] to extract features from each log entry. Term
frequency (TF) captures the frequent words in a log entry. Inverse
documentfrequency(IDF)isusedtoweightdownthefrequently
used meaningless words such as ‚Äúthe‚Äù, ‚Äúan‚Äù, and ‚Äúon‚Äù. The SOM
algorithm is used to classify the TFIDF vectors associated with
different log entries into different task groups.
DBScan-TFIDF: We use TFIDF to represent each log entry and
DBScan [49] to classify the log entries into different task groups.
Topic-LDA: Latent Dirichlet Allocation (LDA) [ 16] is a popular
technique to analyze the topics of natural language documents.
When log entries are fed into the LDA algorithm, the algorithm
estimatestheprobabilityofeachlogentrybelongingtodifferent
topics. We choose the topic with the largest probability as its topic.
Thenweclassifythelogentrieswiththesametopicintoonetask
group.
3.3.2 Causal Analysis. We implement two alternative approaches
forcausalanalysisforcomparisonwithPerfSig.Bothalternative
approaches use correlation coefficients to predict causality. Specifi-
cally, we test the Pearson Correlation Coefficient [ 27] and Spear-
man‚Äôs Rank Correlation Coefficient [ 27]. Pearson Correlation Coef-
ficient measuresthe correlation using normalized co-variances of
absolutevalues.Spearman‚ÄôsRankCorrelationCoefficientmeasures
the correlation using normalized co-variances of ranked values. A
larger causal score means the two time series are strongly corre-
lated.
3.4 Results
Table 2 shows the results of signature extraction for bugs which
manifest as system metric anomalies. PerfSig can identify threedifferent anomaly patterns, i.e, fluctuation pattern changes, persis-
tent increases and cyclic pattern changes, from all the 11 tested
bugs. Moreover, PerfSig is capable of detecting the true root cause
functions for all the tested bugs.
Wealsoevaluatedthreedifferentcausalanalysismethods(i.e.,
MI, Pearson and Spearman) under two different settings: with low-
passfilterandwithoutlow-passfilter.Overall,thelow-passfilter
improves the quality of causal analysis result, no matter which
causal analysis method is used. It is because smoothing the time
series reduces irrelevant fluctuations brought by dynamic work-
loads and makes the anomaly pattern more salient. For example, in
Hadoop-15415 bug, the hflushfunction ranks the highest when
filtering is not employed. The hflushfunction is CPU-intensive,
whichismis-detectedastherootcausefunctionduetoalowcausal
score.Inourexperiments,weobservethatsettinga50-90%filter-
ingthresholdproducesthesameresults,sowechoose50%asour
default filtering threshold.
Whencomparingthethreedifferentcausalanalysismethods,we
canseethattheproposedMIschemeoutperformsthealternative
Pearson and Spearman methods. The experimental results show
that good causal analysis techniques needs to consider data dis-
tribution across the whole time series. Moreover, entropy based
method such as MI is better than co-variance based methods.
Table 3 shows the results of bug signature extraction results for
bugs which manifest as log anomalies. Specifically, We compare
PerfSig(i.e.,SOM-embedding)withfourotheralternativedesigns:
DBScan-embedding,SOM-TFIDF,DBScan-TFIDF,andTopic-LDA.
Overall, PerfSig is capable of detecting the anomaly pattern for
eight out of nine bugs which show log anomalies.
First,wecomparePerfSigwithDBScan-embeddingwherethe
workload classification method, SOM, is replaced with DBScan.
PerfSig outperforms DBScan-embedding because the word embed-
dingvector is100-dimensional.DBScanhas poorperformanceon
high-dimensional vectors due to the curse of dimensionality [37].
If we replace the embedding representation with the TFIDF rep-
resentation, we can see PerfSig outperforms both alternative ap-
proacheswithTFIDF,i.e,SOM-TFIDFandDBScan-TFIDF.Itisbe-
causeTFIDFonlyconsidersindividualwordoccurrenceandfailsto
1676PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table2Signatureextractionresultsforbugswhichhavesystemmetricanomalies.Pearsonùëìispearsonmethodwithfilter.Similarly,
Spearmanùëìis spearman method with filter. For each method, we present the root cause function‚Äôs rank using causal analysis.
Bug ID Anomaly Pattern Root Cause FunctionNumber of
Candidate
FunctionsPerfSig PearsonùëìSpearmanùëìMIPearson Spearman
Hadoop-8614Fluctuation
pattern changeIOUtils#skipFully 91 1 31 1 614 12
Hadoop-15415Fluctuation
pattern changeIOUtils#copyBytes 91 1 3 1 613 20
Yarn-163Fluctuation
pattern changeContainerLogsPage#printLogs 91 1 24 2 820 81
Yarn-2905Fluctuation
pattern changeAggregatedLogsBlock
#readContainerLogs91 1 37 2 646 35
Yarn-1630Fluctuation
pattern changeYarnClientImpl#submitApplication 91 1 2 1 12 10 21
MapReduce-7089Fluctuation
pattern changeReadMapper#doIO 91 1 4 1 5 6 48
Mapreduce-6990Fluctuation
pattern changeTaskLog#Reader 91 1 41 1 258 20
Hadoop-5318 Persistent increase FSDataOutputStream#write 188 1 1 1 1 1 18
Hadoop-6133 Persistent increase conf#getClassByName 115 1 1 1 21 24 32
Cassandra-9881Cyclic pattern
changeScrubber#scrub 41 1 1 1 1 1 1
Cassandra-7330Cyclic pattern
changeStreamReader#drain 41 1 1 1 1 1 1
Table3Signatureextractionresultsforbugswhichhavesystemloganomalies. representstheanomalypatternandtherootcausefunc-
tion cannot be identified. For each method, we present the root cause function‚Äôs rank using causal analysis.
Bug ID Signature (Anomaly Pattern, Root Cause Function)Number of
Candidate
FunctionsPerfSigDBScan-
embeddingSOM
-TFIDFDBScan
-TFIDFTopic-LDA
Hadoop-11252Infinite log sequence timespan due to missing closing server log,
RPC.waitForProtocolProxy91 1 1 1 1 
MapReduce-5066Infinite log sequence timespan due to missing closing server log,
JobEndNotifier.localRunnerNotification81 1   3 
MapReduce-4089Infinite log sequence timespan due to missing stopping service log,
PingChecker.run79 1 1 1 1 1
Mapreduce-3862Infinite log sequence timespan due to missing stopping service log,
DeletionService.delete81 1 1 1 1 
HDFS-7005 ‚Äì ‚Äì     
HDFS-1490Infinite log sequence timespan due to missing closing server log,
TransferFsImage.getFileClient93 1  1 1 
Hadoop-9106Abnormal log sequence timespan,
Client.call()84 1 1   
HBase-17341Abnormal log sequence timespan,
ReplicationSource.terminate()8 1 1   
HDFS-4301Abnormal log sequence timespan,
TransferFsImage.getFileClient()93 1  1 1 
capturethesemanticsofeachword.Theembeddingrepresentation
converselycapturesthesemanticinformationbymodelingthecon-
textual surrounding words. As system logs are written in a human
readable format for developers to diagnose problems of the system,
the advantage of embedding representation over TFIDF is clear
and also is observed from the experimental results. We observe
semantic grouping works well because the system log semantics
are much simpler than natural language textual data. SOM and
DBScan clustering have similar performance on TFIDF representa-
tion,becausenumberofTFIDFdimensionsisnotlarge.Logentries
typically are short sentences with limited key words.
Next, we replace the classification framework with topic extrac-
tion model (i.e., Topic-LDA), which is another kind of semantic
analysis. We observe a much worse performance compared with
the other methods.Topic-LDAhas poor performance becausethe
log entries are usually very short. According to a study on textdocumentsfrommicro-bloggingplatformTwitter[ 8,32],theper-
formanceofTopic-LDAdegradeswhentheinputtextdocuments
are short. According to [ 32], it is hard for Topic-LDA to extract
semantics from short documents as Topic-LDA cannot obtain suffi-
cient statistics from short documents.
PerfSig considers all functions invoked around the performance
alert detection time as candidate root cause functions and ranks all
candidaterootcausefunctionsbasedontheMIscores.Asshownin
Table2andTable3,for80%bugs,thenumberofcandidatefunctions
exceeds 70. In our experiments, the true root cause functions have
the highest MI scores (102% higher than the second rank candidate
functions on average) in 19 out of 20 bugs.
1677ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Jingzhu He et al.
Table4Theruntimeoverheadanddiagnosistime.Hadooprep-
resentsthefourHadoopsystem,i.e.,HadoopCommon,Hadoop
MapReduce, Hadoop HDFS and Hadoop Yarn.
System WorkloadTracing
OverheadMetric
Anomaly
Detection
TimeLog
Anomaly
Detection
TimeCausal
Analysis
Time
Hadoopùúã
calculation0.18% 0.13¬±0.01s0.22¬±0.26s 0.28¬±0.16s
HBasedatabase
query0.67% 0.13¬±0.01s0.17¬±0.01s0.02¬±0.002s
Cassandradatabase
query1.7% 0.13¬±0.01s0.36¬±0.03s 0.15¬±0.01s
//ReflectionUtils class
104 public static <T> T newInstance(...) {
117 setConf(result, conf);
119 }
59 public static void setConf(...) {
64 setJobConf( theObject, conf);
66 }
74 private static void setJobConf(...) {
80 Class<?> jobConfClass =
81 conf. getClassByName(
"org.apache.hadoop.mapred.JobConf" );
95 }
//Configuration class
761 public Class <?> getClassByName(...)
throws ClassNotFoundException {
762 return Class.forName(name, true, classLoader);
/* duplicated costly operations to search for
the same configuration class */
763 }
Figure 11 The code snippet of the Hadoop-6133 Bug.
3.5 Overhead and Diagnosis Time
Table4showstheruntimeoverheadanddiagnosistime.Therun-
timetracingoverheadisbelow2%.Thediagnosistimeareallless
than one second. Note that although log anomaly detection uses
thedeeplearningmodel,i.e.,wordembeddingmodel,thediagno-
sis time is still very short because log entries are typically short
sentences with repeated words.
3.6 Case Studies
WehavedescribedHadoop-11252bug‚ÄôsrootcauseinSection1.1.
PerfSigidentifiesthelogsequencewhichstartsfromthefirstlog
entryofstartingservertothelastlogentryofstoppingserveras
in Figure 2. PerfSig identifies the bug as the missing log pattern.
PerfSig extracts the log sequence time series during normal run
and regard the log sequence‚Äôs time span as infinity due to missing
logduringbuggyrun.PerfSigperformscausalanalysisonallthe
function call time series and the log sequence time series. The time
series are shown in Figure 10. PerfSig determines the root cause
functionas RPC.waitForProtoProxy becauseithasthelargestMI
value.
Hadoop-6133 bug is caused by duplicated costly operations. As
showninFigure11,whenweuse ReflectionUtils.newInstance
function to initialize a new reflection instance at line 104, setConf
functionisinvokedatline117tosetthejobconfiguration. setConf
callssetJobConf functionatline64,thencallsthe conf.getClass#
	    

#
#
	   

#
#
	 	 !
#
	  "
	 
#
	  "
 
	


Figure 12 Logs generated by HDFS-7005 bug.
ByNamefunctionatline81tofindaparticularconfigurationclass.
However,theJDKfunction Class.forName invokedby conf.getCl
assByName iscostly.Whentherearemultiplethreadswhichinitial-
izetheinstances, Class.forName isfrequentlyinvokedtosearch
for the same configuration class, which is unnecessary. When
the bug is triggered, we observe 80x slowdown in the system.
PerfSig identifies the bug anomaly pattern as the persistent in-
creases, because Class.forName consumes a lot of CPU resources,
as shown in Figure 9a. PerfSig then performs causal analysis on
the CPU utilization time series and all the function time series. As
showninFigure9b,PerSigdeterminestherootcausefunctionas
conf.getClassByName because it has the largest MI value.
Negative Case Study : HDFS-7005 bug is caused by missing
timeout for DFSClient#newConnectedPeer . When this bug hap-
pens,theResourceManagerhangsonwaitingresponsefromthe
HDFS cluster. We observe that the logs of shutting down HDFS
clusterismissingasshowninFigure12.PerfSigfailstoclassifythe
logentriestothesamecluster.Forexample,thefirsttwologentries
are grouped in cluster A which represents DataNode‚Äôs tasks. Since
the log entries of the log sequence are classified to different tasks,
we cannot extract the right log sequence from the log clusters.
4 THREATS TO VALIDITY
Benchmark bias: For the experimental evaluation,we have repro-
duced20performancebugs,whichareallthebugswecanreproduce
within a time limit. Up to the submission, we have exhaustively
searchedthebugreportsinsixcommoncloudsystemsfromJIRA
[5], selected all the true performance bugs and tried our best to
reproduce them.
Parameter bias: The choice of several hyperparameters in the
designcanintroducebiasonthesystemefficacy,suchasthethresh-
oldofthelowpassfilter.Inourexperiment,weadjustthehyperpa-
rameters several times and choose the best one. The experimental
resultsshowthatourdesignislesssensitivetothehyperparameters
than the alternative approaches.
5 RELATED WORK
In this section, we discuss the existing work in the literature.
Single-modality data analysis : Previous work has worked
on bug diagnosis by performing single modality data analysis. Co-
henetal.[ 19,20]leveragedTree-Augmented NaiveBayesmodels
and clustering methods to extract signatures from system metrics.
PerfScope[ 23],PerfCompass[ 24]andTScope[ 30]diagnosedper-
formancebugsbyperformingunsupervisedmachinelearningon
systemcalltraces.Stitch[ 58]andlprof[ 59]reconstructedthedo-
mainknowledgeandsystemmodelfromthelogs.CloudSeer[ 57]
reconstructedtheexecutionworkflowentirelyfrominterleaving
1678PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
OpenStacklogs.PLELog[ 55]performedsemi-supervisedlearning
combiningHDBScanclusteringandprobabilisticlabelestimationto
detect log anomalies. LogFaultFlagger [ 10] extracted vectors based
on TF-IDF method and applied the kNN classifier to identifying
abnormal logs. Deeplog [ 25] applied a deep neural network model,
i.e., Long Short-Term Memory (LSTM), to detect anomalies from
the system logs. The mystery machine [ 18] analyzed logs from
Internet service to diagnose Facebook request latency. CSight [ 15]
modeled the system behavior in the form of CFSM from system
logs.Kabinnaetal.[ 35]appliedmachinelearning-basedmethods
to determine the change risk of system logs based on certain met-
rics such as file ownership and log density. Li et al. [ 40] adopted
the LDA method to extract topics from source codes automatically
and studied the likelihood of topics to be logged. Lou et al. [ 43]
constructed program workflow from event traces to understand
system behaviors and verify system executions. PBI [ 14] and REPT
[21]leveragedhardwaretraces,i.e.,performancecountersandIntel
ProcessorTrace,tounderstandthesoftwarebugs.PerfSigperforms
multi-modality analysis to address the limitation that performance
bugsmanifestindifferentdatatypescomparedwiththeexisting
work.
Performance bug diagnosis and fixing : Previous work has
proposeddetectionandfixingsolutionsforperformancebugs.Hang
doctor[17]detectedsofthangsatruntimetoaddressthelimitations
of offline detection. PerfChecker [ 41], and HangWiz [ 53] automat-
ically detected soft hang bugs by searching the application code
for known blocking APIs. Yang et al. [ 54] and He et al. [ 29]p r e -
sented comprehensive empirical studies and detection solutions
for two kinds of performance bugs, i.e., database-backed web appli-
cation performance bugs and configuration-related performance
bugs.PerfDebug[ 51]appliedadataprovenance-basedtechnique
to diagnose performance issues in applications that exhibit compu-
tation skew. DScope [ 22] and HangFix [ 31] adopted pattern-driven
approachestodiagnosingandfixingsoftwarehangbugsincloud
systems.PerSigcomplementstheexistingworkinprovidingamulti-
modality signature extraction framework to depict performance
bugs in a comprehensive fashion.
Causal analysis : Causal analysis attracts much attention in
softwaredebuggingrecently.Forexample,UniVal[ 38]transformed
branchandlooppredicatesintovariablesandappliedstatisticcausal
analysis to infer the faulty component. REPTRACE [ 48] performed
causality analysis on system call traces to identify the execution
dependencies. Compared with the existing work, PerfSig performs
multi-modality causal analysis among different types of data to
extract the root cause functions of performance bugs, taking a
further step in applying causal analysis in software debugging.
Causalanalysisontimeseriesdatatypicallyfocusedonidenti-
fying Granger-causal relationship among different time series [ 11,
13,50,52].McCracken[ 44]presentedacomprehensivereviewto
performexploratorycausalanalysis.Thecausalanalysistechniques
couldbedividedintotwocategories,i.e.,regression-basedandin-
formationtheory-basedmethods.Fortheregression-basedmethod,
Arnold et al. [ 13] adopted linear lasso regression for identifying
Granger-causal relationship among time series. Tank et al. [ 50]
explored the possibility of detecting non-linear Granger-causal re-
lationship among time series by training deep learning models (i.e.,multi-layer perceptrons and recurrent neural networks) with spar-
sity constrain. For the techniques using information theory, the
Granger-causalrelationshipwasdetectedbyentropy-basedmea-
sures [11,52]. Existing work leveraged different kinds of measures
likedirectedinformationtheory[ 11,12]ortransferentropy[ 52].
PerfSigmakesthefirststeptoapplytheinformationtheorymethod
mutual information to performance bug signature extraction.
6 CONCLUSION
Inthispaper,wepresentPerfSig,anautomaticperformancebugsig-
natureextractiontool.PerfSigcananalyzevariouskindsofmachine
data including system metric, system logs, and function call traces
to identify principal anomaly patterns and root cause functions as
unique signature patterns for representing performance bugs. We
have implemented a prototype of PerfSig and conducted extensive
evaluationsusing20realworldperformancebugsonsixcommonly
used cloud systems. Our results show that PerfSig can successfully
extract unique signatures for 19 out of 20 tested performance bugs.
PerfSig imposes low overhead to the cloud system, which makes it
practical for production environments.
7 DATA AVAILABILITY
Thedata andtheimplementation ofPerfSigare publiclyavailable
at https://github.com/jhe16/PerfSig.
ACKNOWLEDGEMENTS
Wewouldliketothanktheanonymousreviewersfortheirvaluable
comments.This workwas sponsoredin partbyNSF CNS1513942
grant,NSFCNS1149445grant,andNSAH98230-17-D-0080grant.
Anyopinionsexpressedinthispaperarethoseoftheauthorsanddo
notnecessarilyreflecttheviewsofNSF,NSA,orU.S.Government.
REFERENCES
[1] 2016. Hadoop-11252. https://issues.apache.org/jira/browse/HADOOP-11252.
[2] 2018. Bugzilla. https://www.bugzilla.org.
[3]2018. Java Information Dynamics Toolkit (JIDT). https://github.com/jlizier/jidt.
[4]2020.AWSoutage.https://www.techrepublic.com/article/amazon-reveals-reason-
for-last-weeks-major-aws-outage/.
[5] 2021. Apache JIRA. https://issues.apache.org/jira.
[6] 2021. Docker. https://www.docker.com/.
[7] 2021. HTrace. http://htrace.incubator.apache.org/.
[8] 2021. Twitter. https://twitter.com.
[9] 2021. Zipkin. https://zipkin.io/.
[10]AnunayAmarandPeterCRigby.2019. Mininghistoricaltestlogstopredictbugs
andlocalizefaultsinthetestlogs.In 2019IEEE/ACM41stInternationalConference
on Software Engineering (ICSE) . IEEE, 140‚Äì151.
[11]Pierre-Olivier Amblard and Olivier JJ Michel. 2011. On directed information
theory and Granger causalitygraphs. Journal ofcomputational neuroscience 30, 1
(2011), 7‚Äì16.
[12]Pierre-OlivierAmblardandOlivierJJMichel.2013. TherelationbetweenGranger
causalityanddirectedinformationtheory:Areview. Entropy15,1(2013),113‚Äì
143.
[13]Andrew Arnold, Yan Liu, and Naoki Abe. 2007. Temporal causal modeling with
graphicalgrangermethods.In Proceedingsofthe13thACMSIGKDDinternational
conference on Knowledge discovery and data mining . 66‚Äì75.
[14]JoyArulraj,Po-ChunChang,GuoliangJin,andShanLu.2013. Production-run
software failure diagnosis via hardware performance counters. Acm Sigplan
Notices48, 4 (2013), 101‚Äì112.
[15]Ivan Beschastnikh, Yuriy Brun, Michael D Ernst, and Arvind Krishnamurthy.
2014. Inferringmodelsofconcurrentsystemsfromlogsoftheirbehaviorwith
CSight.In Proceedingsofthe36thInternationalConferenceonSoftwareEngineering .
468‚Äì479.
[16]DavidMBlei,AndrewYNg,andMichaelIJordan.2003.Latentdirichletallocation.
the Journal of machine Learning research 3 (2003), 993‚Äì1022.
1679ICSE‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Jingzhu He et al.
[17]Marco Brocanelli and Xiaorui Wang. 2018. Hang doctor: runtime detection and
diagnosis of soft hangs for smartphone apps. In Proceedings of the Thirteenth
EuroSys Conference . 1‚Äì15.
[18]Michael Chow, David Meisner, Jason Flinn, Daniel Peek, and Thomas F Wenisch.
2014. The mystery machine: End-to-end performance analysis of large-scale
internetservices.In 11thUSENIXSymposiumonOperatingSystemsDesignand
Implementation (OSDI) . 217‚Äì231.
[19]I.Cohen,M.Goldszmidt,T.Kelly,J.Symons,andJ.S.Chase.2004. Correlating
instrumentation data to system states: A building block for automated diagnosis
and control. In OSDI.
[20]Ira Cohen, Steve Zhang, Moises Goldszmidt, Julie Symons, Terence Kelly, and
Armando Fox. 2005. Capturing, indexing, clustering, and retrieving system
history.ACM SIGOPS Operating Systems Review 39, 5 (2005), 105‚Äì118.
[21]WeidongCui,XinyangGe,BarisKasikci,BenNiu,UpamanyuSharma,Ruoyu
Wang, and Insu Yun. 2018. REPT: Reverse debugging of failures in deployed
software.In 13thUSENIXSymposiumonOperatingSystemsDesignandImplemen-
tation (OSDI) . 17‚Äì32.
[22]Ting Dai, Jingzhu He, Xiaohui Gu, Shan Lu, and Peipei Wang. 2018. Dscope:
Detecting real-world data corruption hang bugs in cloud server systems. In
Proceedings of the ACM Symposium on Cloud Computing . 313‚Äì325.
[23]Daniel J Dean, Hiep Nguyen, Xiaohui Gu, Hui Zhang, Junghwan Rhee, Nipun
Arora, and Geoff Jiang. 2014. PerfScope: Practical Online Server Performance
Bug Inference in Production Cloud Computing Infrastructures. In SOCC.
[24]DanielJDean,Hiep Nguyen, PeipeiWang,XiaohuiGu,AncaSailer,andAndrzej
Kochut. 2015. Perfcompass: Online performance anomaly fault localization and
inference in infrastructure-as-a-service clouds. IEEE Transactions on Parallel and
Distributed Systems 27, 6 (2015), 1742‚Äì1755.
[25]Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. 2017. Deeplog: Anomaly
detection and diagnosis from system logs through deep learning. In Proceedings
ofthe2017ACMSIGSACConferenceonComputerandCommunicationsSecurity .
1285‚Äì1298.
[26]Michael Eichler. 2012. Causal Inference in Time Series Analysis. Causality:
Statistical Perspectives and Applications (2012), 327‚Äì354.
[27]IsabelleGuyonetal .2008.Practicalfeatureselection:fromcorrelationtocausality.
Mining massive data sets for security: advances in data mining, search, social
networks and text mining, and their applications to security (2008), 27‚Äì43.
[28]Jiawei Han, Jian Pei, Behzad Mortazavi-Asl, Helen Pinto, Qiming Chen, Umesh-
war Dayal, and Meichun Hsu. 2001. Prefixspan: Mining sequential patterns
efficiently by prefix-projected pattern growth. In proceedings of the 17th interna-
tional conference on data engineering . Citeseer, 215‚Äì224.
[29]Haochen He, Zhouyang Jia, Shanshan Li, Erci Xu, Tingting Yu, Yue Yu, Ji Wang,
andXiangkeLiao.2020. CP-detector:usingconfiguration-relatedperformance
properties to expose performance bugs. In 2020 35th IEEE/ACM International
Conference on Automated Software Engineering (ASE) . IEEE, 623‚Äì634.
[30]JingzhuHe,TingDai,andXiaohuiGu.2018. Tscope:Automatictimeoutbugiden-
tification for server systems. In 2018 IEEE International Conference on Autonomic
Computing (ICAC) . IEEE, 1‚Äì10.
[31]JingzhuHe,TingDai,XiaohuiGu,andGuoliangJin.2020. HangFix:automatically
fixing software hang bugs for production cloud systems. In Proceedings of the
11th ACM Symposium on Cloud Computing . 344‚Äì357.
[32]Liangjie Hong and Brian D Davison. 2010. Empirical study of topic modeling in
twitter. In Proceedings of the first workshop on social media analytics . 80‚Äì88.
[33]Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. 2012.
Understanding and detecting real-world performance bugs. ACM SIGPLAN
Notices47, 6 (2012), 77‚Äì88.
[34]Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its
application in retrieval. Journal of documentation (1972).
[35]Suhas Kabinna, Cor-Paul Bezemer, Weiyi Shang, Mark D Syer, and Ahmed E
Hassan. 2018. Examining the stability of logging statements. Empirical Software
Engineering 23, 1 (2018), 290‚Äì333.
[36]Teuvo Kohonen. 1990. The self-organizing map. Proc. IEEE 78, 9 (1990), 1464‚Äì
1480.
[37]MarioKoppen.2000. Thecurseofdimensionality.In 5thOnlineWorldConference
on Soft Computing in Industrial Applications (WSC5) , Vol. 1. 4‚Äì8.
[38]Yiƒüit K√º√ß√ºk, Tim AD Henderson, and Andy Podgurski. 2021. Improving fault
localizationbyintegratingvalueandpredicatebasedcausalinferencetechniques.
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE) .
IEEE, 649‚Äì660.
[39]Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and
documents. In International conference on machine learning . PMLR, 1188‚Äì1196.
[40]HengLi,Tse-HsunPeterChen,WeiyiShang,andAhmedEHassan.2018.Studying
software logging using topic models. Empirical Software Engineering 23, 5 (2018),
2655‚Äì2694.
[41]Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and detect-
ing performance bugs for smartphone applications. In Proceedings of the 36th
international conference on software engineering . 1013‚Äì1024.
[42]Joseph T Lizier. 2014. JIDT: An information-theoretic toolkit for studying the
dynamics of complex systems. Frontiers in Robotics and AI 1 (2014), 11.[43]Jian-GuangLou,QiangFu,ShengqiYang,JiangLi,andBinWu.2010. Miningpro-
gram workflow from interleaved traces. In Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery and data mining . 613‚Äì622.
[44]James M McCracken. 2016. Exploratory Causal Analysis with Time Series Data.
Synthesis Lectures on Data Mining and Knowledge Discovery 8, 1 (2016), 1‚Äì147.
[45]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimationofwordrepresentationsinvectorspace. arXivpreprintarXiv:1301.3781
(2013).
[46]Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel,
BertrandThirion,OlivierGrisel,MathieuBlondel,PeterPrettenhofer,RonWeiss,
Vincent Dubourg, et al .2011. Scikit-learn: Machine learning in Python. the
Journal of machine Learning research 12 (2011), 2825‚Äì2830.
[47]Radim ≈òeh≈Ø≈ôek and Petr Sojka. 2010. Software Framework for Topic Modelling
withLargeCorpora.In ProceedingsoftheLREC2010WorkshoponNewChallenges
for NLP Frameworks . ELRA, Valletta, Malta, 45‚Äì50. http://is.muni.cz/publication/
884893/en.
[48]ZhileiRen,ChanglinLiu,XushengXiao,HeJiang,andTaoXie.2019. Rootcause
localization for unreproducible builds via causality analysis over system call
tracing. In 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE, 527‚Äì538.
[49]ErichSchubert,J√∂rgSander,MartinEster,HansPeterKriegel,andXiaoweiXu.
2017. DBSCANrevisited,revisited:whyandhowyoushould(still)useDBSCAN.
ACM Transactions on Database Systems (TODS) 42, 3 (2017), 1‚Äì21.
[50]AlexTank,IanCovert,NicholasFoti,AliShojaie,andEmilyFox.2018. Neural
granger causality for nonlinear time series. arXiv preprint arXiv:1802.05842
(2018).
[51]Jason Teoh, Muhammad Ali Gulzar, Guoqing Harry Xu, and Miryung Kim. 2019.
Perfdebug: Performance debugging of computation skew in dataflow systems. In
Proceedings of the ACM Symposium on Cloud Computing . 465‚Äì476.
[52]Raul Vicente, Michael Wibral, Michael Lindner, and Gordon Pipa. 2011. Transfer
entropy‚Äîamodel-freemeasureofeffectiveconnectivityfortheneurosciences.
Journal of computational neuroscience 30, 1 (2011), 45‚Äì67.
[53]XiWang,ZhenyuGuo,XuezhengLiu,ZhileiXu,HaoxiangLin,XiaogeWang,
and Zheng Zhang. 2008. Hang analysis: fighting responsiveness bugs. ACM
SIGOPS Operating Systems Review 42, 4 (2008), 177‚Äì190.
[54]Junwen Yang, Cong Yan, Pranav Subramaniam, Shan Lu, and Alvin Cheung.
2018. Hownottostructureyourdatabase-backedwebapplications:astudyof
performancebugsinthewild.In 2018IEEE/ACM40thInternationalConferenceon
Software Engineering (ICSE) . IEEE, 800‚Äì810.
[55]Lin Yang, Junjie Chen, Zan Wang, Weijing Wang, Jiajun Jiang, Xuyuan Dong,
and Wenbin Zhang. 2021. Semi-supervised log-based anomaly detection via
probabilistic label estimation. In 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE) . IEEE, 1448‚Äì1460.
[56]Chin-ChiaMichaelYeh,YanZhu,LiudmilaUlanova,NurjahanBegum,YifeiDing,
HoangAnhDau,DiegoFurtadoSilva,AbdullahMueen,andEamonnKeogh.2016.
Matrix profile I: all pairs similarity joins for time series: a unifying view that
includesmotifs,discordsandshapelets.In 2016IEEE16thinternationalconference
on data mining (ICDM) . Ieee, 1317‚Äì1322.
[57]Xiao Yu, Pallavi Joshi, Jianwu Xu, Guoliang Jin, Hui Zhang, and Guofei Jiang.
2016. Cloudseer:Workflowmonitoringofcloudinfrastructuresviainterleaved
logs.ACM SIGARCH Computer Architecture News 44, 2 (2016), 489‚Äì502.
[58]XuZhao,KirkRodrigues,YuLuo,DingYuan,andMichaelStumm.2016. Non-
intrusive performance profiling for entire software stacks based on the flow
reconstructionprinciple.In 12thUSENIXSymposiumonOperatingSystemsDesign
and Implementation (OSDI) . 603‚Äì618.
[59]Xu Zhao, Yongle Zhang, David Lion, Muhammad Faizan Ullah, Yu Luo, Ding
Yuan, and Michael Stumm. 2014. lprof: A non-intrusive request flow profiler for
distributedsystems.In 11thUSENIXSymposiumonOperatingSystemsDesignand
Implementation (OSDI) . 629‚Äì644.
1680