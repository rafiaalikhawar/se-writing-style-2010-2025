On the Reliability of Coverage-Based Fuzzer Benchmarking
Marcel BÃ¶hme
MPI-SP, Germany
Monash University, AustraliaLÃ¡szlÃ³ Szekeres
Google, USAJonathan Metzman
Google, USA
ABSTRACT
Givenaprogramwherenoneofourfuzzersfindsanybugs,howdo
we know which fuzzer is better? In practice, we often look to code
coverageasaproxymeasureoffuzzereffectivenessandconsider
the fuzzer which achieves more coverage as the better one.
Indeed,evaluating10fuzzersfor23hourson24programs,we
findthatafuzzerthatcoversmorecodealsofindsmorebugs.There
isaverystrongcorrelation betweenthecoverageachieved andthe
numberofbugsfoundbyafuzzer.Hence,itmightseemreasonable
to compare fuzzers in terms of coverage achieved, and from that
derive empirical claims about a fuzzerâ€™s superiority at finding bugs.
Curiously enough, however, we find no strong agreement on
which fuzzer is superior if we compared multiple fuzzers in terms
of coverage achieved instead of the number of bugs found. The
fuzzer best at achieving coverage, may not be best at finding bugs.
ACM Reference Format:
Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman. 2022. On the Relia-
bilityofCoverage-BasedFuzzerBenchmarking.In 44thInternationalConfer-
enceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA.
ACM,NewYork,NY,USA, 13pages.https://doi.org/10.1145/3510003.3510230
1 INTRODUCTION
In the recent decade, fuzzing has found widespread interest. In
industry,wehavelargecontinuousfuzzingplatformsemploying
100k+machinesforautomaticbugfinding[ 23,24,46].Inacademia,
in2020alone,almost50fuzzingpaperswerepublishedinthetop
conferences for Security and Software Engineering [62].
Imagine,wehaveseveralfuzzersavailabletotestourprogram.
Hopefully,noneofthemfindsanybugs.Ifindeedtheydonâ€™t,we
might have someconfidence in the correctness of the program.
Thenagain,evenaperfectly non-functionalfuzzerwouldfindno
bugs in our program. So, how do we know which fuzzer has the
highest â€œpotentialâ€ of finding bugs? A widely used proxy measure
offuzzereffectivenessisthecodecoveragethatisachieved.After
all,a fuzzer cannot find bugs in code that it does not cover.
Indeed, in our experiments we identify a very strong positive
correlation between the coverage achieved and the number of bugs
found by a fuzzer. Correlation assesses the strength of the associa-
tionbetweentworandomvariablesormeasures.Weconductour
empiricalinvestigationon10fuzzers Ã—24Cprograms Ã—20fuzzing
campaignsof23hours( â‰ˆ13CPUyears).Weusethreemeasuresof
coverage and two measures of bug finding, and our results suggest:
As the fuzzer covers more code, it also discovers more bugs.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
For all other uses, contact the owner/author(s).
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.351023012345678910
123456789 1 0
Fuzzer Ranks by avg. #branches coveredFuzzer Ranks by avg. #bugs discovered
02468 1 0#benchmarks
(a)1 hour fuzzing campaigns ( ğœŒ=0.38).12345678910
123456789 1 0
Fuzzer Ranks by avg. #branches coveredFuzzer Ranks by avg. #bugs discovered
02468 1 0#benchmarks
(b)1 day fuzzing campaigns ( ğœŒ=0.49).
Figure 1: Scatterplot of the ranks of 10 fuzzers applied to 24
programs for (a) 1 hour and (b) 23 hours, when ranking each
fuzzer in terms of the avg. number of branches covered (x-
axis) and in terms of the avg. number of bugs found (y-axis).
Hence, it might seem reasonable to conjecture that the fuzzer
which is better in terms of code coverage is also better in terms
of bug findingâ€”but is this really true? In Figure 1, we show the
rankingofthesefuzzersacrossallprogramsintermsoftheaverage
coverageachievedandtheaveragenumberofbugsfoundineach
benchmark. The ranks are visibly different. To be sure, we also
conducted a pair-wise comparison between any two fuzzers where
the difference in coverage andthe difference in bug finding are
statistically significant. The results are similar.
We identify nostrong agreement on the superiority orranking
ofafuzzerwhencomparedintermsofmeancoverageversusmean
bug finding. Inter-rater agreement assesses the degree to which
tworaters,here bothtypesofbenchmarking,agreeon thesuperi-
ority or ranking of a fuzzer when evaluated on multiple programs.
Indeed, two measures of the same construct are likely to exhibit a
highdegreeofcorrelationbutcanatthesametimedisagreesub-
stantially [ 41,55]. We evaluate the agreement on fuzzer superiority
when comparing any two fuzzers where the differences in terms of
coverage andbugfindingarestatisticallysignificant.Weevaluate
the agreement on fuzzer ranking when comparing all the fuzzers.
Concretely, our results suggest a moderate agreement. For fuzzer
pairs,wherethedifferencesintermsofcoverage andbugfinding
is statistically significant, the results disagree for 10% to 15% of
programs.Onlywhenmeasuringtheagreementbetweenbranch
coverageandthenumberofbugsfoundandwhenwerequirethe
differences to be statistically significant at ğ‘â‰¤0.0001 for coverage
andbugfinding,dowefindastrongagreement.However,statistical
significanceat ğ‘â‰¤0.0001onlyintermsofcoverageisnotsufficient;
we again find only weak agreement. The increase in agreement
with statistical significance is notobserved when we measure bug
findingusingthetime-to-error.Wealsofindthatthevarianceofthe
agreementreducesas moreprogramsareused, andthatresultsof
1h campaigns do not strongly agree with results of 23h campaigns.
16212022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman
In summary, this paper makes the following contributions :
â˜…We introduce a novel methodology to evaluate proxy measures
of fuzzer (or test suite) effectiveness. Specifically, we suggest
evaluating agreement instead of correlation, and propose a bug-
based evaluation without pre-determined ground-truth.
â˜…Weprovidethefirstevidenceonthereliabilityofcoverage-based
benchmarking for the evaluation of fuzzer effectiveness. We
confirm a very strong correlation and a moderate agreement.
â˜…Weexploreaninterpretationofourresultsfor reachingafault
versusexposingabug(Section6)anddiscussourresultsinthe
largercontextoffuzzerbenchmarking,wherewemakeconcrete
recommendations for future evaluations (Section 7).
â˜…Wepublishalldata,theanalysis,andthevirtualexperimental
infrastructure.We provide preciseinstructions toreproduceand
extend our experiments: https://doi.org/10.5281/zenodo.6045830
2 RELATED WORK
Codecoveragehaslongbeenusedasaproxymeasureofthebug
finding ability of a test suite. Fortunately, in practice the most com-
monsituationisthatthetestsuite detectsnobugs.Now,ifalltest
cases pass, how do we assert whether the test suite is effective?
Practitionersoftenrelyoncodecoverageinstead[ 33].Underpin-
ning coverage as a proxy measure is the insight that a test suitecannot find bugs in code that it does not cover. However, recent
empirical studies on the correlation between code coverage and
bug finding identify different degrees of correlation [9].
Thecode coverage of a test suite or fuzzer can be measured, e.g.,
asthenumberofprogrambranchesthatareexercisedbythetest
suiteorfuzzer,respectively.The bugfindingability ofatestsuite
(orfuzzer)canbemeasured,e.g.,asthenumberofbugsfoundor
thetimeittooktofindthefirstbug.The correlation betweentwo
random variables measures the strength of their association and
the direction of their relationship.
Using artificially injected bugs and developer-generated test
suites,InozemtsevaandHolmes[ 32]findaweakcorrelation between
coverage and test suite effectiveness when the size of the test suite
iscontrolled for(and amoderate tostrongcorrelation iftest suite
sizeisignored).However,Chenetal.[ 9]raiseconcernsaboutthe
experimental methodology (i.e., the stratification of test set size)
posingasignificantthreattothevalidityoftheresults.Gopinath
etal.[26]identifieda strongcorrelation betweencodecoverageand
testsuiteeffectivenessfordeveloper-providedtestsuitesandfound
the impact of test suite size neglible. For auto-generated test suites
the correlation was moderate to strong ; however, the majority of
auto-generated test suites covered less than 20% of code while the
coverage values for developer-generated test suites had a much
wider spread, and they might have been written specifically for
detectingthesebugs.Gligoricetal.[ 22]findaverystrongcorrelation
between coverage and bug finding using different measures of
correlation. In contrast to this line of work, we use real bugs instead
of artificially injected bugs (i.e., mutants). Mutants may or may
not be representative of real bugs [ 9,35,50]. Instead of developer-
providedtestsuites,ourstudyisconcernedwith"testsuites"that
wereauto-generatedbyvariousfuzzers.Inourstudy,testsuitesize
is nota concern, either,as we explicitlycontrol for themethod by
which the test suite (i.e., seed corpus) is generated.Usingrealbugsandauto-generatedtestsuites(generatedbyone
fuzzer), Wei et al. [ 61] observe that the majority of bugs (>50%) are
found in the last two thirds of the campaign when branch cover-
age increases only slightly from 90% to 94%. Along this qualitative
reasoning, they conclude that "there is weak correlation between
number of faults found and coverage". Kochhar et al. [ 37] find a
strong correlation between coverage and bug finding for one pro-
gramanda moderatecorrelation foranother.However,Chenetal.
[9]raiseconcernsaboutthecorrelationmeasurethatwasusedand
note that the association is likely stronger than indicated. More
generally, Chen et al. expose several flaws in experimental method-
ologies of previous work and highlight common pitfalls in the
statistical evaluation. Their own experiments indicate a very strong
correlation between coverage and bug finding.
Inourstudy, wecanconfirmaverystrongcorrelation. However,
in contrast to all previous work, we suggest the use of agreement
insteadofcorrelationforempiricalinvestigationsoftestsuiteef-
fectiveness.The agreement betweentwomeasuresquantifiesthe
degree to which both measures would agree on the relative perfor-
manceoffuzzers.Wedefinetwotypesofagreement:agreementonsuperiority,whichconcernstwofuzzers;andagreementon ranking,
which concerns more than two fuzzers. We say that two measures
agree on superiority if both measures consider the same fuzzer bet-
ter performing than the other, and the difference is statistically
significant. We say that two measures agree on ranking if both
measuresordermorethantwofuzzersaccordingtotheiraverage
performance the same way, not considering statistical significance.
Counterintuitively to the strong correlation result, we find that the
agreement, both on superiority and ranking, is moderate.
Benchmarkingbugfindingtoolsisdifficult.Forstaticanalysis
tools, Dwyer, Person, and Elbaum [ 14] show that even small varia-
tions in the toolâ€™s configuration can give rise to a very large varia-
tion in the toolâ€™s bug finding effectiveness. For fuzzing, Gavrilov et
al.[19]startfromtheobservationthat"bug-based metricsareim-
practicalbecause(1)thedefinitionofâ€˜bugâ€™isvague,and(2)mappingbug-revealinginputstobugsrequiresextensivedomainknowledge".Infact,wewillelaborateonthechallengesofbug-basedevaluation
inSection7.Insteadofcountingthenumberofbugs,Gavrilovet
al. [19] propose to measure the number of changes in program
behavior over time that a fuzzer can detect.
Tothebestofourknowledge,ourworkisthefirsttoevaluate
whether coverage-based fuzzer benchmarking is reliable: Does the
ranking of two or more fuzzers in terms of coverage agree withtheir ranking in terms of bug finding? The current guideline on
sound fuzzer evaluation suggests that coverage-based benchmark-
ing alone may be insufficient (referring to the contentious study[
9] by Inozemtseva and Holmes [ 32] which suggests a weak cor-
relation). Our study provides the first empirical evidence on the
reliability of coverage-based fuzzer benchmarking.
3 EXPERIMENTAL SETUP
3.1 Research Questions
Our objective is to evaluate the degree to which a coverage-based
andabug-basedbenchmarkingagreeonfuzzerperformance.We
aim to answer the following research questions.
1622On the Reliability of Coverage-Based Fuzzer Benchmarking ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
RQ.1 Correlation. Howstrongistheassociationbetweenthecov-
erage achieved by a fuzzer and its ability to find bugs?
RQ.2 Agreement. Howstrongistheagreementontheranksor
thesuperiorityofthefuzzersincoverage-basedversusabug-
based benchmarking?
RQ.3 Campaign Length. Doestheagreementbetweencoverage-
based and bug-based benchmarking increase with the length
of the fuzzing campaign? (Our default is 23 hours).
RQ.4 Campain Trials. Does agreement between coverage- and
bug-basedbenchmarkingincreasewiththenumberofcam-
paignsper{fuzzer Ã—program})?(Ourdefaultis20campaigns
per combination).
RQ.5 Extrapolation Withinonetypeofbenchmarking,howstrong
is the agreement on the ranks or superiority of the fuzzers
running 23 hour campaigns versus shorter campaigns?
RQ.6 Mitigation of Threats to Validity. (a) How strong is the
agreement between two randomized rounds of coverage-
based benchmarking? (b) How strong is the agreement be-
tween different measures of bug finding or between different
measuresofcoverage?(c)Howdoesagreementvaryasthe
number of available programs increases?
3.2 Experimental Design
We evaluate these research questions using a post hoc bug identifi-
cationinstead of a pre-determined ground truth. While it requires
substantiallymoreeffort,theposthocidentificationallowsusto
avoid some of the pitfalls of ground-truth based benchmarking, as
discussed in Section 7.1In our design, after conducting the fuzzing
campaigns, we employ a process of automatic and manual dedu-plicationtoidentifytheuniquebugsthateachfuzzerdiscovered.
Fuzzing campaigns may produce many bug reports, some of which
actually pertain to the same unique bug. So, we sorted them out.
Our experiments generated 341,595 bug reports; too many for
ustomanuallydeduplicate.WeusedavariantoftheClusterfuzz
deduplicationapproachtoautomaticallygroupbugreports.After
that, we manually deduplicated the 409 automatically deduplicated
bugsto get235unique bugs.Twoprofessional softwareengineers
labeled the bugs to find duplicates. We note that our experimental
design is indeed not very economically. Our fuzzersdid not find a
singlebugin30%(7/24)oftheselectedprogramsdespitesubstantial
fuzzingeffort(Fig. 2).However,itallowsustomitigateanumber
of threats to validity of a ground-truth based evaluation (Sec. 7).
3.3 Fuzzers and Programs
Benchmark Details. The 24 benchmark programs we used are
listed inFigure 2. Many of the programs are popular and well-
maintained open-source software libraries that are widely usedtosupport criticalservicesin theinternet.For instance,
libxml2
is a popular parser library for XML-documents, phpis the inter-
preter for websites written in the PHP programming language,
andwireshark is a popular network protocol analyzer. The set of
benchmark programs ranges from parser libraries, protocol imple-
mentations, and implementations of compression algorithms allthe way to OS service managers, interpreters, and platforms for
1Examples are survivorship bias, observer-expectancy bias, and selection bias.Name Size Harness Name #Branches #Known Bugs
libhevc 252.3k LoC hevc_dec_fuzzer 54.7k 11
ndpi 58.0k LoC fuzz_ndpi_reader 42.9k 15
libhtp 19.3k LoC fuzz_htp 10.2k 1
aspell 28.1k LoC aspell_fuzzer 28.4k 1
grok 26.6k LoC grk_decompress_fuzzer 45.0k 4
matio 35.0k LoC matio_fuzzer 46.7k 49
stb 70.2k LoC stbi_read_fuzzer 6.7k 11
njs 89.0k LoC njs_process_script_fuzzer 34.0k 10
zstd 93.2k LoC stream_decompress 22.6k 1
openh264 140.1k LoC decoder_fuzzer 39.3k 22
libgit2 224.9k LoC objects_fuzzer 114.2k 3
poppler 225.6k LoC pdf_fuzzer 184.7k 17
libxml2 505.1k LoC xml_reader_file_fuzzer 101.8k 3
arrow 769.4k LoC parquet-arrow-fuzz 473.9k 34
php 2.6M LoC fuzz-execute 324.5k 9
php 2.6M LoC fuzz-parser-2020-07-25 436.8k 8
wireshark 4.3M LoC fuzzshark_ip 526.3k 10
proj4 168.2k LoC standard_fuzzer 92.2k 36
tpm2 48.6k LoC execute_command_fuzzer 20.8k 11
file 16.7k LoC magic_fuzzer 8.6k 1
muparser 34.2k LoC set_eval_fuzzer 7.6k 0
usrsctp 92.0k LoC fuzzer_connect 53.6k 0
libarchive 166.5k LoC libarchive_fuzzer 38.7k 10
systemd 588.4k LoC fuzz-varlink 63.1k 1
Total 13.2M LoC 2.7M 268
Figure 2: Details about benchmark programs. In our data
analysis, we excluded programs below the line because nomore than two (of ten) fuzzers found a least one bug in at
least one campaign.
AFL [63] AFL++ [15] AFLSmart [53] AFLFast [4]
FairFuzz [38] Eclipser [10] MOPT [40] Honggfuzz [58]
LibFuzzer [56] Entropic [3]
Figure 3: The fuzzers available in Fuzzbench, that we used in
our experiments.
in-memoryanalytics. Outof these24 benchmark programs,there
are seven (7) programs containing bugs that could not be found by
anyfuzzer,four(4)programswherebugswereveryhardtofind,
andthree(3)programswherenomoremorethantwofuzzerscould
find bugs in at least one campaign.
Benchmark selection. Our benchmark programs have been
randomly selected from programs in OSS-Fuzz that have histori-
cally contained a relative high number of bugs. OSS-Fuzz [24]i sa
service that provides fuzzing for 500 open source projects. Integra-
tions are usually performed by project maintainers and/or security
researcherswhowritefuzztargetsandcompileseedcorporaand
dictionariesfortheprojects.Thismeansthatourbenchmarkpro-
grams have been prepared for fuzzing by the maintainers and not
by us, which reduces experimenter bias. OSS-Fuzz automatically
reportseachbugitfindstogetherwiththefirstandlastprogram
version in which the bug exists. Most program versions do not
contain any bugs, which is why a random selection of program
versions from OSS-Fuzz would be prohibitively expensive. Most
fuzzers would not find any bugs. Hence, we use the information
fromOSS-Fuzztorandomlychoseourbenchmarkprogramsfrom
programversionswhichareknowntohaveanincreasednumberofbugs.SimilarlytoOSS-Fuzz,allprogramsareinstrumentedwithAd-
dressSanitizer [ 57] as oracle to detect bugs. All fuzzing campaigns
are started from an initial seed corpus provided from OSS-Fuzz.
1623ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman
Fuzzers. Figure 3shows the list of fuzzers we used. We chose
these fuzzers basedon their importance and ease-of-use. Entropic,
libFuzzer, Honggfuzz, AFL, and AFL++ are widely-used in industry
while AFLSmart, AFLFast, FairFuzz, Eclipser, and MOpt-AFL are
important academic works and extensions of AFL.
3.4 Variables and Measures
Our objective is to evaluate the degree to which a coverage- and a
bug-based benchmarking agree on fuzzer performance. We have
onemainandtwosupplementarymeasuresofcoverageplustwo
main and one supplementary measure of bug finding.
MeasuresofCoverage.Ourmainmeasureofcoverageis branch
coverage,i.e.,thenumberofbranchesintheprogramthatthefuzzer
has exercised until this point in the campaign. Branch coverage
captures the control-flow in a program, subsumes statement cover-
age [22], is considered to be the most effective proxy measure of
bugfinding[ 20,22],andistheconventionalmeasureofcoverage
to evaluate coverage-guided greybox fuzzing [ 36]. Fuzzbench mea-
sures "region coverage" [ 12] in 15-minute intervals on a dedicated
measurer instance2using clangcompiler flags -fprofile-instr-
generate and-fcoverage-mapping and the llvm-cov tool [12].
As supplementary measures of coverage, we also analyze the
numberofuniquepathsandthenumberofuniqueedgesasmea-
sured by the AFL-fuzzer. The number of unique paths (#paths) con-
tinuestobeacommonperformancemeasureforgreyboxfuzzers
[17,18,68]despiteitsobviousflaws[ 36,38].Thenumberofunique
edges(#edges), reported as map size by AFL-based fuzzers, is often
used as a proxy for branch coverage. AFL maintains a fixed-size
hashmap containing an entry for every tuple of conditional jumps
that are sequentially exercised in the program. For all measures of
coverage,wedirectlyevaluatecoverageonthebuggyprogramto
avoid the clean program assumption [8].
Measures of Bug Finding. Our main measures of bug finding
arebug coverage, i.e., the number of bugs that the fuzzer has found
until a given point in the trial, and the time-to-error, i.e., the length
of the fuzzing campaign when the first bug was found. In order
tocountthenumberofbugs(#bugs)ataparticularpointintime,
we execute all bug-revealing inputs and remove all duplicates. Our
method of deduplicating bugs is similar to ClusterFuzzâ€™s. For each
crash reported in a trial, we take the crash type (e.g., â€œHeap-buffer-
overflowâ€) and the top three symbolized stack frames reported
byAddressSanitizerorUndefinedBehaviorSanitizer.Crasheswith
the same type and stack frames are considered duplicates, and
only one of them is counted. To further improve the quality of the
deduplication, we manually removed the remaining duplicates. In
ordertomeasurethetime-to-error(TTE),wereportthelengthof
the fuzzing campaign when the first crashing input was generated.
As supplementary measure of bug finding, we also count the
number of unique crashes (#crashes), i.e., the number of "unique
paths"thatareexercisedbycrashinginputs.Thenumberofunique
crashes, similar to the number of unique paths, is a standard but
contentious measure of bug finding. Crashes are flagged as suchby standard code sanitizers, such as ASAN [
57]. For both mea-
sures of bug finding, we directly evaluate bug finding on programs
containing real bugs to mitigate threats to construct validity.
2Each fuzzing campaign runs on separate compute instance, called runner.Spearmanâ€™s ğœŒInterpretation
0.00 - 0.09 Neglible correlation
0.10 - 0.39 Weak correlation0.40 - 0.69 Moderate correlation0.70 - 0.89 Strong correlation0.90 - 1.00 Very strong correlation
(a)Taken from Schober et al. [55]Cohenâ€™s
ğœ… Interpretation
0.00 - 0.20 No agreement
0.21 - 0.39 Minimal agreement0.40 - 0.59 Weak agreement0.60 - 0.79 Moderate agreement0.80 - 0.90 Strong agreement0.91 - 1.00 Almost perfect agreement
(b)Taken from McHugh [42].
Figure 4: Interpretation of Spearmanâ€™s ğœŒand Cohenâ€™s ğœ….
3.5 Statistical Analysis
Inordertoinvestigatetherelationshipbetweencoverage-basedand
bug-finding based measures of fuzzer performance, we compute
correlation and agreement.
Correlation [55] assesses the strength of the association and
the direction of the relationship between two random variables.
We assess the correlation between a measure of coverage and a
measure of bug finding using Spearmanâ€™s rank correlation.W eu s e
Spearmanâ€™s instead of the more common Pearsonâ€™s correlation
asPearsonâ€™sassumesalinearrelationshipwhileourscatterplots
inFigure 6indicate an exponential one. Since both variables are
continuous,represent pairedobservations,andtheirrelationshipis
monotonic, the assumptions for Spearmanâ€™s correlation are met.
The interpretation of Spearmanâ€™s ğœŒis shown in Figure 4.a.
Inter-raterAgreement [59]assessesthedegreeofagreement
between two raters of the same phenomenon. In our case, we mea-
sure the agreement between a coverage-based measure of fuzzer
performance and a bug-finding-based measure of fuzzer perfor-
manceontherankingorsuperiorityofafuzzer.Sincecoverageand
bugfindingmeasurethesameconstruct,i.e.,fuzzerperformance,
the assumption forassessing agreement is met. Schoberet al. [ 55]
note that "two variables can exhibit a high degree of correlation
but can at the same time disagree substantially". Bland and Altman
[41]suggestthatanytwomeasuresofthesameconstructshould
necessarily be strongly correlated, but may not strongly agree.
Agreement on Rank. In order to benchmark multiple fuzzers
simultaneously, it might seem reasonable to establish a ranking,
wherethebestfuzzeraccordingtosomemeasureisrankedhighest
(cf.Fig.1).Afuzzerâ€™srankingforaprogramandtimestampisbased
on the corresponding average for that measure across all (twenty)
trials.Wemeasuretheagreementonthecoverage-basedandbug-
finding-based ranks of a fuzzer using Spearmanâ€™s correlation [55].
Agreement on Superiority. Unlike a pair-wise comparison, a
rankingdoesnotconsiderthestatisticalsignificanceofthediffer-
encebetweenanytwofuzzers.Hence,wealsomeasuretheagree-
mentonthe superiority ofafuzzeroveranotherwhensuperiority
is established according toa measure of coverage versus a measure
ofbugfinding.UsingCohenâ€™skappa ğœ…anddisagreementpropor-
tionğ‘‘, we measure agreement on superiority for pairs of fuzzers
onlywhere the difference in terms of bothmeasures is statistically
significant ( ğ‘â‰¤{0.05,0.001,0.0001}) for at least 10% of the pro-
grams(>=3).Webelievethereisinsufficientevidenceforfuzzer
pairs where differences are statistically significant for the less than
10% of programs. Given a fuzzer pair, the coverage- and bug-based
evaluationeach"rates"whichfuzzerissuperior.Wemeasurethe
agreementontheseratingsacross(atleastthree)benchmarkpro-
grams. We also consider a third method using Spearmanâ€™s ğœŒ.
1624On the Reliability of Coverage-Based Fuzzer Benchmarking ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Disagreementproportion ğ‘‘iseasytointerpret.Givenapairof
fuzzers where the differences in terms of coverage andbug finding
are statistically significant for at least 10% of the programs, the
disagreement proportion ğ‘‘gives the proportion of programs where
both fuzzers are considered superior according to coverage or bug
finding, respectively: ğ‘‘=(1âˆ’ğ‘ğ‘œ).
Cohenâ€™s kappa ğœ…a standard, more robust measure of inter-
rater agreement which also takes into account the possibility of
theagreementoccurringbychance[ 42,59].Giventhesamepair
of fuzzers, Cohenâ€™s kappa ğœ…is computed as the difference between
therelativeobservedagreementonthesuperiorityofafuzzer ğ‘ğ‘œ
andthehypotheticalprobabilityofchanceagreement ğ‘ğ‘’divided
by the complement of the probability of chance agreement: ğœ…=
(ğ‘ğ‘œâˆ’ğ‘ğ‘’)/(1âˆ’ğ‘ğ‘’). Cohenâ€™s interpretation is shown in Figure 4.b.
Spearmanâ€™srho ğœŒallowsustouse alldatapointsusinganordi-
nal rather than a binary variable for superiority: 1for the superior
fuzzer, -1for the inferior fuzzer, and 0where the difference is not
statistically significant according to the given ğ‘-value.
StatisticalSignificance.Theevaluatethestatisticalsignificance
ofthedifferencebetweentwofuzzers,wereportMannâ€“Whitney
ğ‘ˆtestâ€”following the recommendations byArcuri et al. [ 1]o nt h e
evaluation of randomized algorithms and Klees et al. [ 36] on the
evaluationof fuzzers. Mann-Whitney ğ‘ˆisa nonparametrictest of
thenullhypothesisthat,forrandomlyselectedvalues ğ‘‹andğ‘Œfrom
two populations, the probability of ğ‘‹being greater than ğ‘Œis equal
to the probability of ğ‘Œbeing greater than ğ‘‹.
3.6 Experiment Infrastructure
We used the FuzzBench fuzzer evaluation platform [ 45] to conduct
ourexperiments.Thesystemconsistsofadispatcher(theâ€œbrainâ€
of an experiment) and workers. The dispatcher dispatches jobs
(a) to build fuzzers and benchmarks, (b) to start separate worker
machines,eachofwhichrunsonefuzzingcampaignforone{fuzzer
Ã—program}combination,(c)tomeasuretheresultsofthefuzzing
campaigns and save results to a central SQL database, and (d) to
generate reports based on the measurement results.
Ameasurement consistsofmeasuringcodecoverageandcrashes.
Manycrashinginputsmayrevealessentiallythesamebug.Forthis
reason, we employ a simple deduplication strategy to assign crash-
ing inputs to bugs they reveal (cf. Section 3.4). For more details on
FuzzBench,werefertheinterestedreadertothearticlebyMetzman
et al. [45] which introduces the FuzzBench infrastructure.
Each fuzzing campaign is run inside an Ubuntu 16.04 docker
container on an n1-standard-1 virtual machine instance running
on Google Cloud. Each instance has 1 virtual CPU core, 3.75 GB of
RAM, and 30 GBof disk space available to use.By default, we run
20campaignsof23hoursforeach{fuzzer Ã—program}-combination.
3.7 Reproducibility
TheFuzzBenchfuzzerevaluationplatformwasdesignedtofacili-
tateopenscience,rigorousevaluation,andreproducibility. Figure5
shows the identifiers of the FuzzBench experiments for this paper.
We also link the exact commit hash (i.e., version) of FuzzBench
which fixes the exactversions of all fuzzers, all benchmark pro-
grams, and the entire experimental platform that was used for our
experiments.EachFuzzBenchreport(availableatthelinkbelow)
describes precisely how our empirical analysis can be reproduced.Experiment FuzzBench
Identifier Commit Description
2021-02-17-bug-paper 38e344fe 20 runs of 23 hours, all fuzzers, all subjects
2021-08-19-crash-s db192b60 30 runs of 23 hours, all fuzzers, 11 subjects
2021-08-19-crash-s2 db192b60 30 runs of 23 hours, all fuzzers, 11 subjects
Figure 5: Reproducibility. Our experiments can be repro-duced using the exact same settings and version of our ex-
periment infrastructure.
Data Availability. The data used for our evaluation can be
downloaded at from the corresponding FuzzBench reports at
â€¢https://www.fuzzbench.com/reports/<experiment-id>
where the experiment identifier is given in Figure 5.
DataAnalysis.Wemakeourdataanalysisscriptavailableas
Jupyter notebook together with all generated tables and images at:
â€¢https://github.com/icse22data
Archival.Forlong-termarchival,wealsopublishthedataand
analysis script at the Zenodo research artifact archival platform.
â€¢https://doi.org/10.5281/zenodo.6045830
4 EXPERIMENTAL RESULTS
RQ1. Correlation
Weinvestigatewhetherafuzzerthatcoversmorecodeisalsobetter
at bug finding. We ask whether the coverage that a fuzzer achieves
is also a good predictor of the number of bugs found.
Methodology.3Toassessthecorrelationbetweencoverageand
bug finding, we prepare a scatter plot of the mean branch coverage
over the mean number of bugs found across all fuzzing campaigns
foreachprogramatanypointintime(Fig. 6).Forallthreemeasures
ofcoverage,wealsocomputeSpearmanâ€™srankcorrelationbetween
themeancoverageachievedandthemeannumberofbugsfound
duringtheaveragefuzzingcampaignforeach{programxfuzzer}
combination at any point in time (Fig. 7).
Results. Figure 6visually depicts the relationship between both
coverage and bug finding. The scatter plots often show an al-
most straight line, suggesting a very strong correlation. In fact, the
strength of the association between coverage and bug finding is
confirmed in Figure 7. For all three measures, we see an average
Spearmanâ€™s rank correlation above 0.90, which we interpret as a
very strong correlation (cf.Figure 4).
There is a strong correlation between the coverage a fuzzer
achieves and the number of bugs it finds in a program. As a
fuzzer covers more code, it also finds more bugs.
Thescatterplotin Figure6alsoprovideshintsastothefunctional
relationship. The linear increase with the log-scale y-axis seems
to suggests an exponential relationship : A linear increase in branch
coverage yields an exponential increase in the number of bugs
found.Whilecounterintuitiveatfirst,itisnotactuallysurprisingif
weconsiderthatmostofthecodehasalreadybeencoveredevenat
thestartofthecampaign.Eachfuzzingcampaignstartswithaseed
corpusthatalreadycoversmuchoftheprogram,andwe measure
3Theexactprocedurecanbefoundinourdataanalysisscriptwhichwehavemade
publically available.
1625ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman
zstdphpâˆ’parser poppler stb wiresharkndpi njs openh264 phpâˆ’executelibhevc libhtp libxml2 matioarrow aspell grok libgit2
8.5k 8.8k 9.0k 9.2k 9.5k43.6k 44.0k 44.4k 44.8k 34.0k 35.0k 36.0k 37.0k 2.1k 2.2k 2.3k 2.4k 2.5k 360.0k 380.0k 400.0k 420.0k 440.02.0k 3.0k 4.0k 6.6k 6.8k 7.0k 7.2k 13.2k 13.4k 13.6k 13.8k 14.0k 130.0k 140.0k 150.0k 160.0k 170.0k4.0k 6.0k 8.0k 6.4k 6.5k 6.5k 6.5k 14.0k 16.0k 18.0k 2.0k 2.2k 2.4k 2.6 k4.0k 4.2k 4.4k 4.6k 4.8k 5.2k 5.3k 5.4k 5.5k 8.0k 8.2k 8.4k 8.6k 8.8k 2.4k 2.4k 2.4k 2.4k0.91.01.11.21.31.41.5
1.52.02.53.0
0.10.30.5
0.10.31.00.10.31.0
0.30.51.0
0.31.03.0
2.53.03.50.30.51.0
0.30.51.0
0.010.030.10
1.01.21.415202530
0.030.050.10
0.50.71.0
0.30.51.0
0.010.030.10
Avg. #Branches CoveredAvg. #Bugs Discovered
Figure 6: Scatter plot of the mean number of bugs found
(onthelog-scale)asthemeannumberofcoveredbranches
increases in the average fuzzing campaign for a benchmark.
the first data point after the first 15-minute interval, when most
ofthe"shallow"brancheshavealreadybeencovered.Thiscanbe
verifiedbylookingatthestartofthex-axisforeachbenchmark.We
canseethatthemajorityofbrancheswhicharecoveredin23hours
have already been covered in the first 15 minutes. Covering a new
branch gets harder over time. Even if coverage is fully saturated
and not a single new branch can be covered, a fuzzer might still
findnewbugs.Thisinterpretationagreeswiththeobservationby
Wei et al. [ 61] who found, for random testing, that the majority of
bugs(>50%)werediscoveredinthelasttwothirdsofthecampaign,
when branch coverage increased only slightly from 90% to 94%.
In our study, there appears to be an exponential relationship
between branch coverage and the number of bugs found.
RQ2. Agreement: Coverage versus Bug Finding
A strong correlation between two variables does not necessarily
implythattheystronglyagree[ 41,55].Weinvestigatethedegree
towhichtheresultsofcoverage-basedbenchmarkingagreewith
the results of bug-based benchmarking.
Methodology (Ranking).Forevery{program Ã—fuzzerÃ—time
stamp}-combination, we have twenty data points / trials. For every
measure and for every {program Ã—time stamp}-combination, we
compute the fuzzer ranks by ordering all ten fuzzers accordingto the average measured value across all twenty trials. For every
measureofcoverageandeverymeasureofbugfinding,respectively,
wecomputetheagreementbetweenthecoverage-basedandbug-
based ranking (in terms of Spearmanâ€™s ğœŒ).#Branches #Paths #Edges
arrow0.999269 0.999276 0.999277
matio0.990898 0.990896 0.990892
ndpi0.888853 0.888625 0.888602
njs0.918627 0.918636 0.918627
openh264 0.969526 0.969552 0.969522
poppler 0.949209 0.949217 0.949210
wireshark 0.888212 0.888212 0.888212
aspell0.988724 0.988689 0.988703
grok0.880887 0.880876 0.880710
libgit20.605309 0.600231 0.602031
libhevc 0.959148 0.959149 0.959147
libhtp0.974873 0.965578 0.975135
libxml2 0.932176 0.932191 0.932172
php-execute 0.834285 0.834286 0.834285
php-parser 0.989402 0.989377 0.989400
stb0.951317 0.951294 0.951250
zstd0.830236 0.830244 0.830233
Average 0.914762 0.913902 0.914553
Figure7:Averagecorrelation( ğœŒ)betweencoverageand#bugs
found for all programs where at least one bug was found.
Methodology (Superiority). From ten fuzzers, we can construct
45 unique pairs of fuzzers. For each fuzzer pair, each program, and
every measure, we determine effect size and statistical significance
betweenbothfuzzersintermsofmeanandmedianofthatmeasure
across 20 trials of 23h. For each fuzzer pair, if the difference interms of the coverage andin terms of bug finding is statistically
significant at ğ‘<{0.05,0.001,0.0001}, for at least 10% of programs,
we compute the agreement on superiority for this pair.
Results. Figure8.ashowstheagreementonrankingandsupe-
riorityofafuzzerin23hourscampaigns.Intermsof ranking,w e
observe a moderate agreement between coverage and bug finding.4
InFigure1themoderateagreementisillustratedbythelargespread.
In terms of superiority, for Cohenâ€™s ğœ…we observe a weak to moder-
ate agreement for the average pair of fuzzers where the superiority
alongbothmeasuresisstatisticallysignificant.Acrossallmeasures,
if we benchmark the average fuzzer pair using a coverage- versus
bug-basedapproach,resultsdisagreefor10%to20%ofprograms.
Figure15 intheappendixshowsamuchloweragreementifweuse
thedifferencein medianinsteadofthemeantoestablishsuperiority.
Onlyifthedifferenceintermsofbranchcoverage andthediffer-
enceintermsofthenumberofbugsfoundisstatisticallysignificant
atğ‘â‰¤0.0001 (i.e., for 11 of 45 fuzzer pairs [24%]), we observe a
strongagreement onthesuperiorityofafuzzer( ğœ…=0.872).Inthis
case, a coverage-based and a bug-based evaluation of those eleven
fuzzerpairsdisagreesonlyforonebenchmark(4.3%),onaverage.
However,statisticalsignificanceat ğ‘â‰¤0.0001onlyofthedifference
incoverageis insufficient,weagainonlyobserveaweakagreement
(seeFigure 14 andFigure 9.d). The increase in agreement with sta-
tistical significance is notobserved when we measure bug finding
using the time-to-error (TTE).
Weobservea moderateagreement betweenacoverage-basedand
a bug-based benchmarking of fuzzer performance. For fuzzer
pairs, where thedifferences in terms of coverage andbug find-
ing is statistically significant, the results usually disagree for
10%to20%ofprograms.Onlyfor#branchesversus#bugs,the
agreementonsuperiorityincreasesasthestatisticalsignificance
forbothdifferences increases.
4The interpretation of these values can be found in Figure 4.
1626On the Reliability of Coverage-Based Fuzzer Benchmarking ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
#Bugs Time-to-Error
#Branches #Edges #Paths #Branches #Edges #Paths
Ranking ğœŒ=0.498 0.376 0.376 0.373 0.313 0.329
Superiority (ğ‘â‰¤0.05)ğœ…=0.533(33%)0.323(27%)0.535(38%)0.488(31%)0.322(36%)0.154(33%)
Superiority (ğ‘â‰¤0.001)ğœ…=0.327(24%)0.450(24%)0.235(24%)0.612(24%)0.542(22%)0.485(24%)
Superiority (ğ‘â‰¤0.0001)ğœ…=0.872(24%)0.428(20%)0.239(24%)0.553(20%)0.630(22%)0.496(22%)
Superiority (ğ‘â‰¤0.05)ğ‘‘=0.113(33%)0.213(27%)0.162(38%)0.211(31%)0.201(36%)0.360(33%)
Superiority (ğ‘â‰¤0.001)ğ‘‘=0.141(24%)0.144(24%)0.246(24%)0.177(24%)0.146(22%)0.166(24%)
Superiority (ğ‘â‰¤0.0001)ğ‘‘=0.043(24%)0.150(20%)0.262(24%)0.204(20%)0.082(22%)0.183(22%)
Superiority (ğ‘â‰¤0.05)ğœŒ=0.524 0.374 0.424 0.367 0.421 0.274
Superiority (ğ‘â‰¤0.001)ğœŒ=0.437 0.397 0.319 0.375 0.376 0.379
Superiority (ğ‘â‰¤0.0001)ğœŒ=0.448 0.384 0.253 0.366 0.412 0.314
(a)Agreementontherank(firstrow)andsuperiorityofafuzzerin23hrcampaignsintermsofCohenâ€™skappa(followingthreerows),disagreement
proportion(middlethreerows),andSpearmanâ€™scorrelation(lastthreerows).Eachcellshowsthemeasureofagreementandsomecells,inparenthesis,
the proportion of fuzzer pairs where the differences are statistically significant at the corresponding p-value ( ğ‘â‰¤{0.05,0.001,0.0001}).0.000.250.500.751.00
0 6 12 18 24
Campaign lengths (in hours)Agreement on Rank (Spearman)
bugs tte
(b)Agreement on ranksbetween measures of bug
finding and #branches after a campaign of ğ‘¥hours.
Figure 8: Agreement on ranking and superiority: Coverage versus Bug Finding
We also observe that the agreement on superiority is smallest
for path coverage versus the number of bugs found, particularly
forhighsignificancethresholds.Pathcoveragehasbeenacommon
performance measure for greybox fuzzers [ 17,18,68] despite its
obviousflaws[ 36,38].Theminimalagreementsuggestsabandoning
path coverage as performance measure.
RQ3. Agreement Over Campaign Length
We investigate whether there is a suitable campaign length where
a coverage-based and a bug-based evaluation maximally agree.
Methodology. To compute the agreement on ranking and supe-
riority of a fuzzer over time, we followed the same methodology
specified in the discussion for RQ2 for every of the 92 time stamps.
Results. Figures 8.b and9show the agreement on ranks and
superiorityovertime,respectively.Intermsof ranking,theagree-
mentremainsmoderateovertheentireduration.Inthefirstnine
hours,weobservean increasein agreementbetweenanevaluation
based on branch coverage versus one based on the number of bugs.However, the agreement on ranks decreases again, remaining mod-erate overall. In terms of superiority,w ed onot observe an increase
inagreement(oradecreaseindisagreement)overtimeforallthree
levelsof statistical significance. The agreementbetween coverage-
and bug-based benchmarking appears to decrease slightly. The
differences are statistically significant for 20-30% of fuzzer pairs.
Inourstudy,wedo notobserveanincreaseinagreement(nor
a decrease in disagreement) over time.
RQ4. Agreement Over Campaign Trials
We investigate whether there is a suitable number of campaigns
per{fuzzer Ã—program}-combinationwheretheacoverage-based
and a bug-based evaluation maximally agree.
Methodology. Allresults reported aboveare derivedfrom our
defaultsetup wherewerun20campaignsoftwentythreehoursfor
each{fuzzer Ã—program}-combination.Inordertoinvestigate,the
agreement as the number of trials increases, we run an additional
40campaignsforasubset5ofthebenchmarkprogramsforatotal
5Benchmarkprogramswith60trialsforeachofthe10fuzzers:arrow,libarchive,matio,
ndpi, njs, openh264, poppler, proj4, tpm2, and wireshark.of60campaignsoftwentythreehoursforeach{fuzzer Ã—program}-
combination.Fromthissetof60trials,werandomlysample ğ‘›trials
without replacement for each combination, where ğ‘›âˆˆ(1,59), and
computeagreementforthosetrialsusingthemethodologyspecified
in RQ2. To account for the randomness in the sampling, we repeat
this experiment 50 times.
Results.Figure10 showstheagreementonfuzzerrankingasthe
numberof trialsincreases.For thefirst20trials in Figure10.a, we
canclearlyseeanincreasingtrend.Asthenumberoftrialsincreases,theagreementincreasesaswell.However,from Figure10.b,itseems
that there is not much benefit in running more than 20 trials as the
agreement increases only ever so slightly.
The agreement between coverage-based and bug-based bench-
marking increases as the number of campaigns increases. How-
ever,theredoesnotseemtobemuchbenefitinrunningmore
than 20 campaigns per {fuzzer Ã—program}-combination.
RQ5. Agreement with Shorter Trials
Weinvestigatethedegreetowhichtheresultsof(coverage-based
or bug-based) benchmarking using shorter campaigns (say 1 hour)
agree with the results of benchmarking using 23 hour campaigns.
Methodology.Wemeasure theagreement onthe rankingof a
fuzzer when ranked at the end of the campaign versus earlier in
the campaign, following the methodology we specified for RQ3.
Results.A sw ec a ns e ei nFigure 11, there is a substantial dif-
ference in ranking when we rank fuzzers in a 1 hour campaign
versus a 23 hour campaign. In fact, there is only moderate agree-
mentbetweentheresultsofabug-basedbenchmarkingat1hour
versusthoseofabug-basedbenchmarkingat23hours.However,
as we expect, the agreement increases with campaign length. Inthe bottom left of Figure 11.b, we can see that 15 minutes before
the end of the 23 hour campaign, the ranks very strongly agree.
Thebenchmarkingresultsforrathershortfuzzingcampaigns
may not strongly agree with results of sufficiently long cam-
paigns. However, in our study, the benchmarking results for 12
hourcampaignsdoalready verystronglyagree withbenchmark-
ing results of 23 hour campaigns.
1627ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman
(a)Agreement on superiority between measures
of bug finding and #branches after a campaign
ofğ‘¥hours. Difference significant at ğ‘<0.05.
0.00.20.40.60.81.0
0 6 12 18 24
Campaign lengths (in hours)Agreement on Superiority (Cohen)
0% 20% 40%
bugs tte(b)Agreement on superiority between measures
of bug finding and #branches after a campaign
ofğ‘¥hours. Difference significant at ğ‘<0.001.
0.00.20.40.60.81.0
0 6 12 18 24
Campaign lengths (in hours)Agreement on Superiority (Cohen)
0% 20% 40%
bugs tte(c)Agreement on superiority between measures
of bug finding and #branches after a campaign
ofğ‘¥hours. Difference significant at ğ‘<0.0001.
0.00.20.40.60.81.0
0 6 12 18 24
Campaign lengths (in hours)Agreement on Superiority (Cohen)
0% 20% 40%
bugs tte(d)Agreementonsuperioritywhenweonlyrequire
the difference in coverage(but not bug finding) to
be statistically significant at ğ‘<0.0001.
0.00.20.40.60.81.0
0 6 12 18 24
Campaign lengths (in hours)Agreement on Superiority (Cohen)
0% 20% 40%
bugs tte
0.0%10.0%20.0%30.0%40.0%
0 6 12 18 24
Campaign lengths (in hours)Disagreement on Superiority (%Programs)
0% 20% 40%
bugs tte
0.0%10.0%20.0%30.0%40.0%
0 6 12 18 24
Campaign lengths (in hours)Disagreement on Superiority (%Programs)
0% 20% 40%
bugs tte
0.0%10.0%20.0%30.0%40.0%
0 6 12 18 24
Campaign lengths (in hours)Disagreement on Superiority (%Programs)
0% 20% 40%
bugs tte
0.0%10.0%20.0%30.0%40.0%
0 6 12 18 24
Campaign lengths (in hours)Disagreement on Superiority (%Programs)
0% 20% 40%
bugs tte
Figure9:Agreementonsuperiorityovercampaignlength.Weshowagreementwhenevaluatingfuzzerperformancebased
onbranchcoverageversusthenumberofbugs(solidline)andbranchcoveragedversusthetime-to-error(dashedline).The
colorshowsthepercentageoffuzzerpairsforwhichthedifferencesarestatisticallysignificantatthecorresponding ğ‘-value
(ğ‘â‰¤{0.05,0.001,0.0001}).
0.00.20.40.60.81.0
0 4 8 12 16 20
Number of Campaigns (Trials)Agreement on Rank (Spearman)
bugs tte
(a)Agreement over 20 trials for all programs.0.00.20.40.60.81.0
0 1 02 03 04 05 06 0
Number of Campaigns (Trials)Agreement on ranking (Spearman)
(b)Agreement over 60 trials for a subset.
Figure 10: Agreement as the number of trials increases. The
solid line showstheaverageagreementontherankingofa
fuzzer when ranked using branch coverage versus the num-
ber of bugs found. The dashed line shows the average agree-
mentontherankingofafuzzerwhenrankedusingbranch
coverage versus the time it takes to find the first bug (TTE).
RQ6. Mitigations of Threats to Validity
We investigate several possible concerns and threats to validity.
(a) Baseline Agreement. A valid concern is that the results of
coverage- and those of bug-based benchmarking may not agree
simply because of some randomness in the measurement or bro-
ken measures of agreement. To investigate this concern, we check0.20.40.60.81.0
0 6 12 18 24
Campaign length (in hours)Agreement on Rank (Spearman)shape
Bugs
Edges
(a)Agreement on ranks between a campaign of
lengthğ‘¥hoursandoneoflength23hours.Forin-
stance,intermsofthenumberofbugsfound,the
rankofafuzzerafter1hourmoderatelyagrees
with the rank of that fuzzer after 23 hours.Spearman's p = 0.4
Spearman's p = 0.91Spearman's p = 0.7
Spearman's p = 16 22.750.25 1
123456789 1 0 123456789 1 012345678910
12345678910
Rank at {0.25, 1, 6, 22.75} hoursRank at 23 hours
(b)Scatter plot of fuzzer ranks by #branches be-
tweenacampaignoflength {0.25,1,6,22.75}
hours(facets)andacampaignoflength23hours.
Here,ğ‘¥are the ranks at the given campaign
length and ğ‘¦are the ranks at 23 hours.
Figure 11: Agreement withincoverage- or bug-based bench-
marking as campaign length increases.
thebaselineagreementbetweentworandomroundsofcoverage-
based benchmarking. From the 60 trials per {fuzzer Ã—program}-
combination generated for RQ4, we randomly sample 2 Ã—20 trials
without replacement and compute agreement on ranks as specified
in RQ2. To account for randomness, we repeat this experiment
50 times. To discharge the concern, we expect a high agreement.
Aswecanseein Figure12.a,weobservea verystrongagreement
on the rank of a fuzzer between two rounds of coverage-based
benchmarking for every campaign length.
1628On the Reliability of Coverage-Based Fuzzer Benchmarking ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
0.00.20.40.60.81.0
0 6 12 18 24
Campaign lengths (in hours)Agreement on Rank (Spearman)
(a)Agreement between two randomized rounds
of coverage-based benchmarking.0.00.20.40.60.81.0
0369 1 2 1 5
Number of ProgramsAgreement on Rank (Spearman) bugs tte
(b)Agreement as the number of benchmark
programs increases.
Figure 12: Investigating threats to validity.
#Bugs
Time-to-Error #Crashes
Ranking ğœŒ=0.671 0.645
Superiority (ğ‘â‰¤0.05)ğœ…=0.868(31%) 0.893(38%)
Superiority (ğ‘â‰¤0.001)ğœ…=0.875(18%) 1.000(20%)
Superiority (ğ‘â‰¤0.0001)ğœ…=0.833(13%) 1.000(20%)
Superiority (ğ‘â‰¤0.05)ğ‘‘=0.040(31%) 0.030(38%)
Superiority (ğ‘â‰¤0.001)ğ‘‘=0.042(18%) 0.000(20%)
Superiority (ğ‘â‰¤0.0001)ğ‘‘=0.056(13%) 0.000(20%)
Superiority (ğ‘â‰¤0.05)ğœŒ=0.663 0.757
Superiority (ğ‘â‰¤0.001)ğœŒ=0.634 0.658
Superiority (ğ‘â‰¤0.0001)ğœŒ=0.556 0.647#Branches
#Edges #Paths
ğœŒ=0.735 0.647
ğœ…=0.785(44%)0.647(42%)
ğœ…=0.746(40%)0.786(38%)
ğœ…=0.666(33%)0.721(33%)
ğ‘‘=0.094(44%)0.083(42%)
ğ‘‘=0.096(40%)0.065(38%)
ğ‘‘=0.114(33%)0.067(33%)
ğœŒ=0.666 0.670
ğœŒ=0.670 0.703
ğœŒ=0.626 0.669
Figure13:Agreementamongmeasuresofbugfinding(Col-
umn #Bugs) and measures of coverage (Column #Branches).
(b)AgreementbetweenMeasures.Asdiscussedin Section3.4,
wehaveseveralmeasuresofbugfindingandseveral(supplemen-
tary) measures of code coverage. For a sound empirical analysis,
we would expect that all measures of bug finding strongly agree
andalso thatallmeasures ofcodecoveragestronglyagree along
all our measures of agreement. As we can see in Figure 13, there
is astrong agreement on superiority and ranking of a fuzzer when
comparing fuzzers in terms of time-to-error versus counting the
number of bugs found. Between measures of coverage, we identify
a strong correlation in most cases, as well.
(c)AgreementOver Programs.Despitethis being oneofthe
largest empirical studies on the relationship between coverage and
bug finding, a valid concern might be that the number of bench-
mark programsis relativelysmall. Toinvestigate thisconcern, we
randomly chose ğ‘›programs without replacement out of the 17
programs where our fuzzers find bugs, and we compute agreement
according tothe methodologyspecified inRQ3, for ğ‘›âˆˆ(1,17).T o
accountforrandomness,werepeatthisexperiment50times. Fig-
ure 12.b shows the scatter plot for the agreement on the randomly
chosenprogramsasthenumber ğ‘›ofprogramsincreases(greydots
and triangles), and the average agreement on fuzzer rank (solidand dashed line). As expected the average agreement is approxi-
matelyconstantasthenumberofprogramsincreases.However,the
varianceissubstantial,rangingbetweenneglibleandverystrong
agreementwhenonly ğ‘›=5benchmarksarechosen.However,at
ğ‘›=16benchmarks,theagreementrangesonlywithinthemoderate
agreement band.Repeating thisexperiment bychosing programs
withreplacement gives similar results.(a)Theeffectofrandomnessontherankingwithincoverage-
basedbenchmarkingisneglible.(b)Eventhoughmeasuresof
coveragedonotagreewithmeasuresofbugfinding,themea-
suresagreewithincoverage-basedandwithinbug-basedbench-
marking, respectively. (c) The number of benchmark programs
hasasubstantialimpactonourresult.However,thevariancein
agreementisreasonablysmallforourbenchmarksizetosup-
port our conclusion, we would suggest. We do notrecommend
using less than 10 benchmark programs for coverage-based
fuzzer evaluation.
5 THREATS TO VALIDITY
As for any empirical study, there are various threats to the validity
of our results and conclusions.
One concern is internal validity, i.e., the degree to which our
study minimizes systematic error. For our selection of fuzzers and
benchmark programs there is a risk of experimenter bias, selec-tion bias, survivorship bias, and confirmation bias. To minimize
experimenter andconfirmationbias,fuzzersandprogramswerepre-
paredbyindependentdevelopers.Wepickedprogramsrandomly
from the largest publicly available collection of fuzzer harnessesfor 500 open source projects. Each harness was prepared by the
correspondingmaintainer.Eachfuzzerwasdevelopedandaddedto
FuzzBencheitherbythefuzzerdeveloperortheFuzzBenchteam
longbeforeourstudystarted.However,apossiblecauseof survivor-
shipandselectionbias isthatâ€“tokeepexperimentcostreasonableâ€“
the benchmark programs were selected from OSS-Fuzz such that a
largenumberofbugscanbefound.Manyofthosebugswerefound
by a subset of the evaluated fuzzers (e.g., AFL, AFL++, libFuzzer,
Honggfuzz).However,ourstudyis notconcernedwithestablishing
thestate-of-the-art(findingwhichfuzzeristhebest).Instead,we
areinvestigatingthereliabilityofcoverage-basedbenchmarking,
which mitigates most risk of selection and confirmaion bias.
Another concern is external validity, i.e., the degree to which
ourstudycanbegeneralizedtoandacrossotherprograms,fuzzers,
bugs, and measures. To the best of our knowledge, ours is the
largest study across all these dimensions. We chose a large variety
of widely-used open-source C programs from different domains.
GiventheresulsinRQ6,weareconfidentthatourresultsgeneralizetomanymoreopen-sourceCprograms.Weconductourevaluation
on a large number of actual bugs that these program contained
organicallysometimeinthepast.Wechosevarious,verysuccessful
greyboxfuzzers whichare usedatGoogle [ 23,24],Microsoft [ 46],
othercompaniesandmanyindependentsecurityresearchers[15].
However, there is no guarantee that our results extend to (bugs in)
programs written in other programming languages or fuzzers that
arefundamentallydifferentfromgreyboxfuzzers.Eventhoughour
benchmark programs contain more known bugs than any other
bug-based benchmark to-date, the number of bugs however is still
lowcomparedtoe.g.,themillionsofbranchesinourbenchmark
programs (Figure 2). The sensitivity analysis in RQ6 on the impact
ofthenumberofprogramschosenfortheevaluationprovidessomeconfidencethatourresultextendstoother,similarbugs.Therefore,
it will be useful to replicate this study with other set of subjectprogramswithreal-bugsinthem,preferablywithanevenlarger
and more diverse set of bugs.
1629ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman
Athirdconcernis constructvalidity,i.e.,thedegreetowhichour
study measures what it purports to be measuring. In this paper, we
areinterestedinâ€œfuzzereffectivenessâ€,andoneofthemainques-
tionswewouldliketoansweriswhethercodecoverageisagood
metricforassessingit.Wedothisbycomparingcoveragemetricsto
bug-finding metrics, i.e., two of them: â€œnumber of bugs foundâ€ and
â€œtimetofirstbugfoundâ€.Ourassumptionisthatthesebugbased
metricsaretheonesthatreallycapturefuzzereffectiveness.Among
thesetwowebelievethatnumberofbugsisthemorerobustmetric,
as it is a more granular, give that it considers multiple bug datapoints,notjustasingleone.Itisstillpossible,however,thatdueto our limited benchmark program set, which contains a limited
setofbugs,thenumberofbugsthatafuzzerfindsin thissetisan
imperfect metric (as discussed for the threat of the number of bugs
on external validity). More specifically we measure â€œnumber of
uniquebugsfoundâ€, whereâ€œuniqueâ€doesnot haveanoperational
oruniversaldefinition.WerelyontheOSS-Fuzzcrashdeduplica-
tion algorithm for this, which has been successfully field testedover many years. Our results for RQ6, where we assess baseline
agreementandtheagreementbetweenmeasuresprovidefurther
confidenceinconstructvalidity.WedonotmaketheCleanProgram
Assumption[ 8]sincecoverage-basedandbug-basedbenchmarking
are conducted on the same program version.
Finally,conclusionvalidity relatestothereliabilityofourmea-
surementsandthevalidityofourstatisticaltests.Wehaveaddressedtheseissuesbyusingwellestablishedstandardmethodstocompute
correlation,agreementandstatisticalsignificance.Totriangulate,
weusemultiplemeasures(Section3.4).Wealsocarriedoutvarious
sanity checks regarding agreement in Section 4 under RQ6.
6 DISCUSSION: REACHING A LOCATION
VERSUS EXPOSING A BUG
The underpinning assumption of coverage-based benchmarking is
thatbugsthatliveincodethatisnotcoveredcanalsonotbeexposed.However, we find that the results of coverage-based benchmarking
maynotreliablyindicatetheresultsofbug-basedbenchmarking.
So, how is reaching a certain location related to exposing a bug?
Inourexperiments,weusecodesanitizers[ 11,57]todetectbugs.
During compilation, a code sanitizer injects assertions into the pro-
gram binary that failwhen, e.g., a memory safetyissue occurs. So,
coveringthoselocationsshouldbeenough,right?Indeed,asZhangand Mesbah [
67] find that assertion coverage is strongly correlated
with test suite effectiveness. Ã–sterlund et al. [ 49] demonstrate that
a fuzzer that focusses on the coverage of sanitizer instrumenta-
tion outperforms existing fuzzers. Now, branch coverage subsumes
"sanitizercoverage".Then, whydowenotsee astrongagreement
between results of coverage-based and bug-based benchmarking?
If fuzzers were guaranteed to detect the bug when they reached
the corresponding code location, then evaluating fuzzers based on
code coverage would be equivalent to evaluating them based on
bugsfound.However,simplyreachingagivenbranchorstatement
isofteninsufficienttotriggerabug.Therootcauseofabugmay
not be localized in a single statement, but a certain sequence ofstatements may need to be executed throughout the code before
the bug is exposed [ 6]. On the other hand, triggering the bug may
be as hard as covering that program branch which reports that thebug has been triggered. Like bugs that cannot be exposed upon
coveringabranch,thecoverageofthatbranchitselfmayalready
require a certain program state.
Onehypothesis[ 64]isthatfaultscouldbeempiricallydistributed
inanon-uniformmanneracrossthecodebase[ 47].Asfuturework,
itwillbeinterestingtoinvestigatethisandotherhypotheses.Maybe
wecanfindspecificpropertiesordifferencesbetweenthetypical
programlocation(orbranch)andfaultlocationsorerrorconditions
more generally. It would be interesting whether achieving these
error conditions (versus achieving code coverage) require different
capabilities from a fuzzer.
Yet,westillbelievethatcodecoverageisanexcellentmeasurable
objectivefunctionforafuzzer.Coverageguidancehasbeenthekey
to the recent success of greybox fuzzers [ 2]. Maximizing coverage
is the key measurable objective in search-based software testing
[43,44].Bugsaresimplytooraretobecomeanexplicitobjective
or to provide a reasonable signal during fuzzing.
Inourresults,weseethatthefuzzerthatisbetterinachieving
coverage may still be worse in finding bugs. The goal of this paper
istoinvestigatehowoftenwecanobservethisâ€œasymmetryâ€.Ifthis
happensrarely,thatmeansthatfuzzerscanbesoundlyevaluated
solelybasedoncodecoverage.Ifthishappensoftenontheother
hand, then it is recommended to use both code coverage and bugs
to evaluate fuzzers.
7 FUZZER BENCHMARKING: CHALLENGES
AND RECOMMENDATIONS
In2020alone,almost50fuzzingpaperswerepublishedinthetop
conferences for Security and Software Engineering [ 62]. To ensure
arealisticassessmentof progressinthefield,weneed soundmea-
sures of fuzzer effectiveness. Only if our measures reflect a fuzzerâ€™s
truebugfindingability,canweproperlyevaluatenewtoolsagainst
thestate-of-the-art.Indeed,whileimprovementsmightseemrea-
sonable,onlyarigorousevaluationwilltellforsure.Forinstance,
ForAllSecure,thewinningteamattheDARPACyberGrandChal-
lenge, burned one CPU-year every night to assess the previous
dayâ€™s improvements [ 48]. Nighswander adds that "many times â€˜ob-
viousâ€™ changes made things worse and stupid things helped. Stats
are vital". Towards this end, large benchmarking platforms have
beenbuilt[ 28,39,45];e.g.,FuzzBench[ 45]hasfacilitatedrapidand
dramaticadvancesamongthemostsuccessfulfuzzers[ 31].How-
ever, according to a recent survey of researchers and practitioners,
sound fuzzer benchmarking remains a key open challenge [2].
In this paper, we provide the first empirical evidence that the
resultsofacoverage-basedevaluationarenotstronglyindicative
ofthefuzzersâ€™relativebugfindingability.However,asweshallsee
next, a rigorous bug-based evaluation is not without perils, either.
7.1 Challenges of Bug-Based Benchmarking
Economic considerations. The most effective fuzzer finds thelargest number of bugs. To evaluate the effectiveness of a fuzzer,in the perfect world, we would select a random, representative
sample of programs (where we do not know whether any bugs can
be found). However, we would quickly find that bugs are sparsein the typical program, and that the cost for experiments with a
reasonable statistical power would be prohibitive.
1630On the Reliability of Coverage-Based Fuzzer Benchmarking ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Syntheticbugs.Tomakebug-basedbenchmarkingmoreeco-
nomical, researchers have proposed to articially inflate the number
of bugs in these programs using synthetic bugs [7,13,51,52,54].
However,itisnofinalconsensusonwhetherthesyntheticbugsare
realisitic [ 7,21,25]. Infact, as futurework, wesuggest to conduct
a similar analysis of agreement, as proposed in this work, between
benchmarking based on artificial bugs versus real bugs.
Ground truth. Alternatively, researchers have been curating
realbugs thatwerehistoricallyfoundinprograms[ 5,6,16,27,28,
34,60]. While this approach is both economical and provides a
more representative, objective ground truth, it is subject to several
threats to validity that might not be obvious to the uninformed
experimenter. (a)Evaluatingfuzzersbasedonpreviouslydiscovered
bugsintroducesa survivorshipbias :Fuzzersthatarebetteratfinding
previously undiscovered bugs may appear worse than they are. On
theotherhand,fuzzersthatcontributedtotheoriginaldiscovery
of some of the ground truth bugs may appear better than they are.
(b)Toincreasethenumberofbugsinaprogram(andtoreducethe
benchmarkingcost),curatorsmay"front-port"severaloldbugsinto
oneversion.Thisintroducesartificial bugmaskingandinteraction
effects, posing a threat to construct valdity. (c)To simplify bug
countingandtoprovidethesamebugoracletoallfuzzers,curators
maymanuallytranslateeachbugintoalocalizedif-statement.This
introducesan observer-expectancybias.Forinstance,inthiswork,
the relationship between coverage and bug finding is precisely the
subject of our study (Section 6)?
Overfitting.Givenagroundtruthbenchmark,researchersmight
be enticed to iteratively and unknowingly tune their fuzzer imple-
mentationtothebugsinthebenchmark.Zelleretal.[ 65,66]identify
aparticularlyseverecaseofthisconfirmationbiaswhichinvalidates
some empirical evidence in a well-cited paper. They recommendto augment bug-based evaluation with a coverage-based evalua-
tion: "During testing, executing a location is a necessary condition
forfindingabuginthatverylocation.Sincewearestillfarfrom
reachingsatisfyingresultsincoveringfunctionality,improvements
in code coverage are important achievements regardless of bugs
being found" [65].
7.2 Recommendations
Forfutureevaluationsoffuzzerperformance,basedontheseresults
and our experience [ 45], we make the following recommendations.
In the order of their appearance in the benchmarking process:
R1Ifpossible,selectatleast10respresentativeprograms.Foreach
fuzzer-programcombination,conductatleast10(better20)
campaigns of at least 12 (better 24) hours. Increasing these
valuesimproves generalityandstatisticalpoweroftheresults.
R2Select"real-worldprograms"thatrepresentprogramsthatare
typically fuzzed in practice. Select "real-world bugs" that rep-
resent the set of bugs which are typically found in programs
used in practice.6Improving the representativeness of the
benchmark increases the external validity of the results. If ex-
periment cost are a concern, authors can prioritize programs
that (are likely to) contain a large number of bugs.
6Asfuturework,wesuggesttoevaluatetherepresentativenessofsyntheticbugsusing
a similar experimental setup as presented in Section 3.R3Select asbaselinethe fuzzer that was extended to implement
the technical contributions and make sure that the configura-
tions (parameters, initial seeds, dictionaries, etc.) are equiva-
lent.Forinstance,todemonstratetheadvantagesofstructure-
aware fuzzing [ 53], we would implement structure-aware
fuzzingintoastructure-un awarefuzzerandcomparetheex-
tendedagainstthebaselinefuzzer.Thisimprovesconstruct
validity and allows to attribute precisely the observed perfor-
manceimprovementstotheproposedtechnicalcontributions.
Acomparisontootherfuzzersmaybeconductedoptionallyif
the authors wish to establish the new fuzzer as the new state-
of-the-art. However, note that the observed improvements
maybelargelyduetodesignandengineeringdifferences(e.g.,
Honggfuzz versus AFL).
R4Consider using a "training set" as benchmarks during the
fuzzerdevelopmentanda"validationset"possiblyusingan
independent benchmarking platform for the actual empiri-
calevaluation.Thisallowsauthorstoreduceoverfittingand
confirmation bias.
R5Measure and report both, coverage- and bug-based metrics toprovide a holistic assessment of fuzzer performance. Use clas-
sicalmeasuresofcoveragetofacilitate(future)comparisons
across various fuzzers. Do not use fuzzer-specific measures
(such as AFLâ€™s number of paths). Use the same measurement
tooling and procedure across all fuzzers and programs to
increaseinternalvalidity.Considerusingaposthocbugiden-tification(Section3.2)ratherthangroundtruthbugstoreduce
threats to internal validity, such as survivorship bias.
R6Assess and report various, non-parametric measures of effect
sizeandstatisticalsignificance,suchasVargha-Delaneyâ€™s Ë†ğ´12
and Mannâ€“Whitney ğ‘ˆtest, respectively [ 1]. This allows to
quantifythemagnitudeofthedifferencesandthedegreeto
which the differences can be explained due to randomness.
R7Discusspotentialthreatstovalidityandyourstrategiestomiti-gatetheidentifiedthreats.Forinstance,discussyourstrategies
to mitigate selection, survivorship, observer-expectancy, and
confirmationbias.Ifindicated,conductanempiricalevalua-
tion of potential threats to validity.
R8Reportallspecificparametersoftheexperimentalsetup(in-
cluding how the programs, bugs, and initial seed corpus were
chosen[30]),publishthetools(fuzzerandbaseline)andthe
benchmark (programs and bugs) to faciliate the reproducibil-
ityoftheresults.Publishdata,analysis,andfigurestofacilitate
open access. Upload all artifacts to an open-access repository
likeZenodoforlong-termarchival[ 29].Reproducibilityisthe
foundation of sound scientific progress.
ACKNOWLEDGMENTS
We gratefully acknowledge the contributions of the Fuzzbench
team. We also thank Stephan Lipp (TU Munich), Adrian Herrera
(ANU),MathiasPayer(EPFL),andRahulGopinath(CISPA)fortheir
feedback on earlier versions of this paper.
REFERENCES
[1]AndreaArcuriandLionelBriand.2014. AHitchhikerâ€™sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
1631ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Marcel BÃ¶hme, LÃ¡szlÃ³ Szekeres, and Jonathan Metzman
#Bugs Time-to-Error
#Branches #Edges #Paths #Branches #Edges #Paths
Superiority (ğ‘â‰¤0.05)ğœ…=0.374(44%)0.329(44%)0.418(44%)0.371(44%)0.217(44%)0.234(44%)
Superiority (ğ‘â‰¤0.001)ğœ…=0.360(40%)0.240(40%)0.315(42%)0.305(42%)0.255(38%)0.177(38%)
Superiority (ğ‘â‰¤0.0001)ğœ…=0.388(33%)0.204(33%)0.320(42%)0.336(42%)0.247(38%)0.101(38%)
Superiority (ğ‘â‰¤0.05)ğ‘‘=0.227(44%)0.241(44%)0.248(44%)0.251(44%)0.330(44%)0.313(44%)
Superiority (ğ‘â‰¤0.001)ğ‘‘=0.208(40%)0.247(40%)0.261(42%)0.241(42%)0.271(38%)0.304(38%)
Superiority (ğ‘â‰¤0.0001)ğ‘‘=0.179(33%)0.235(33%)0.273(42%)0.241(42%)0.265(38%)0.321(38%)
Figure 14: Agreement on superiority when only requiring the difference in coverage to be statistically significant.
#Bugs Time-to-Error
#Branches #Edges #Paths #Branches #Edges #Paths
Ranking ğœŒ=0.492 0.378 0.361 0.376 0.315 0.324
Superiority (ğ‘â‰¤0.05)ğœ…=0.312(33%)0.038(27%)0.378(38%)0.233(31%)0.245(36%)0.151(33%)
Superiority (ğ‘â‰¤0.001)ğœ…=0.260(24%)0.359(24%)0.102(24%)0.521(24%)0.331(22%)0.485(24%)
Superiority (ğ‘â‰¤0.0001)ğœ…=0.621(24%)0.428(20%)0.025(24%)0.553(20%)0.476(22%)0.496(22%)
Superiority (ğ‘â‰¤0.05)ğ‘‘=0.291(33%)0.328(27%)0.293(38%)0.301(31%)0.309(36%)0.404(33%)
Superiority (ğ‘â‰¤0.001)ğ‘‘=0.189(24%)0.166(24%)0.309(24%)0.200(24%)0.203(22%)0.184(24%)
Superiority (ğ‘â‰¤0.0001)ğ‘‘=0.097(24%)0.150(20%)0.316(24%)0.204(20%)0.150(22%)0.183(22%)
Figure 15: Agreement on ranking and superiority when considering the difference in terms of Medianinstead of the Mean.
VerificationandReliability 24,3(2014),219â€“250. https://doi.org/10.1002/stvr.1486
[2]MarcelBÃ¶hme,CristianCadar,andAbhikRoychoudhury.2021. Fuzzing:Chal-
lenges and Opportunities. IEEE Software (2021), 1â€“9. https://doi.org/10.1109/MS.
2020.3016773
[3]Marcel BÃ¶hme, Valentin ManÃ¨s, and Sang Kil Cha. 2020. Boosting Fuzzer Effi-
ciency: An Information Theoretic Perspective. In Proceedings of the 14th Joint
meetingoftheEuropeanSoftwareEngineeringConferenceandtheACMSIGSOFT
Symposium on the Foundations of Software Engineering (ESEC/FSE). 970â€“981.
https://doi.org/10.1145/3368089.3409748
[4]Marcel BÃ¶hme, Van-Thuan Pham, and Abhik Roychoudhury. 2019. Coverage-
BasedGreyboxFuzzingasMarkovChain. IEEETransactionsonSoftwareEngi-
neering45, 5 (2019), 489â€“506. https://doi.org/10.1109/TSE.2017.2785841
[5]MarcelBÃ¶hmeandAbhikRoychoudhury.2014.CoREBench:StudyingComplexity
of Regression Errors. In Proceedings of the 2014 International Symposium on
Software Testing and Analysis (San Jose, CA, USA) (ISSTA 2014) . Association for
ComputingMachinery,NewYork,NY,USA,105â€“115. https://doi.org/10.1145/
2610384.2628058
[6]Marcel BÃ¶hme, Ezekiel O. Soremekun, Sudipta Chattopadhyay, EmamurhoUgherughe, and Andreas Zeller. 2017. Where is the Bug and How is It Fixed?
An Experiment with Practitioners. In Proceedings of the 2017 11th Joint Meet-
ing on Foundations of Software Engineering (Paderborn, Germany) (ESEC/FSE
2017). Association for Computing Machinery, New York, NY, USA, 117â€“128.
https://doi.org/10.1145/3106237.3106255
[7]JoshuaBundt,AndrewFasano,BrendanDolan-Gavitt,WilliamRobertson,and
Tim Leek. 2021. Evaluating Synthetic Bugs. In Proceedings of the 2021 ACM Asia
ConferenceonComputerandCommunicationsSecurity(ASIACCSâ€™21).Association
for Computing Machinery, New York, NY, USA, 716â€“730. https://doi.org/10.
1145/3433210.3453096
[8]Thierry Titcheu Chekam, Mike Papadakis, Yves Le Traon, and Mark Harman.
2017. AnEmpiricalStudyonMutation,StatementandBranchCoverageFault
Revelation That Avoids the Unreliable Clean Program Assumption. In 2017
IEEE/ACM 39th International Conference on Software Engineering (ICSE) . 597â€“
608.https://doi.org/10.1109/ICSE.2017.61
[9]Yiqun T. Chen, Rahul Gopinath, Anita Tadakamalla, Michael D. Ernst, Reid
Holmes, Gordon Fraser, Paul Ammann, and RenÃ© Just. 2020. Revisiting the Rela-
tionshipbetweenFaultDetection,TestAdequacyCriteria,andTestSetSize.In
Proceedingsofthe35thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (Virtual Event, Australia) (ASE â€™20). Association for Computing Ma-
chinery, New York, NY, USA, 237â€“249. https://doi.org/10.1145/3324884.3416667
[10]Jaeseung Choi, Joonun Jang, Choongwoo Han, and Sang Kil Cha. 2019. Grey-Box Concolic Testing on Binary Code. In Proceedings of the 41st International
Conference on Software Engineering (Montreal, Quebec, Canada) (ICSE â€™19). IEEE
Press, 736â€“747. https://doi.org/10.1109/ICSE.2019.00082
[11]LLVM Developers. 2021. Clang 13 documentation - UndefinedBehaviorSanitizer
(UBSan). https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html. [Online;
accessed 16.Aug.21].
[12]LLVMDevelopers.2022. Clang15documentation-Source-basedCodeCover-
age.https://clang.llvm.org/docs/SourceBasedCodeCoverage.html#interpreting-
reports. [Online; accessed 10.Feb.22].
[13]Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti,
Wil Robertson, Frederick Ulrich, and Ryan Whelan. 2016. LAVA: Large-scaleAutomatedVulnerabilityAddition.In ProceedingsoftheIEEESecurityandPrivacy
(S&Pâ€™16). 1â€“10. https://doi.org/10.1109/SP.2016.15
[14]Matthew B. Dwyer, Suzette Person, and Sebastian Elbaum. 2006. Controlling
Factors in Evaluating Path-Sensitive Error Detection Techniques. In Proceedings
of the 14th ACM SIGSOFT International Symposium on Foundations of Software
Engineering (FSEâ€™06).AssociationforComputingMachinery,NewYork,NY,USA,
92â€“104. https://doi.org/10.1145/1181775.1181787
[15]Andrea Fioraldi, Dominik Maier, Heiko EiÃŸfeldt, and Marc Heuse. 2020. AFL++ :
Combining Incremental Steps of Fuzzing Research. In 14th USENIX Workshop on
Offensive Technologies (WOOT 20) . USENIX Association. https://www.usenix.
org/conference/woot20/presentation/fioraldi
[16]Gordon Fraser and Andrea Arcuri. 2012. Sound Empirical Evidence in Software
Testing.In Proceedingsofthe34thInternationalConferenceonSoftwareEngineering
(Zurich, Switzerland) (ICSE â€™12). IEEE Press, 178â€“188.
[17]ShuitaoGan,ChaoZhang,PengChen,BodongZhao,XiaojunQin,DongWu,and
ZuoningChen.2020. GREYONE:DataFlowSensitiveFuzzing.In 29thUSENIX
Security Symposium (USENIX Security 20). USENIX Association, 2577â€“2594.
[18]ShuitaoGan,ChaoZhang,XiaojunQin,XuwenTu,KangLi,ZhongyuPei,and
Zuoning Chen. 2018. CollAFL: Path Sensitive Fuzzing. In 2018 IEEE Symposium
on Security and Privacy (SP). 679â€“696. https://doi.org/10.1109/SP.2018.00040
[19]Miroslav Gavrilov, Kyle Dewey, Alex Groce, Davina Zamanzadeh, and Ben Hard-
ekopf. 2020. A Practical, Principled Measure of Fuzzer Appeal: A PreliminaryStudy. In 20th IEEE International Conference on Software Quality, Reliability
and Security, QRS 2020, Macau, China, December 11-14, 2020. IEEE, 510â€“517.
https://doi.org/10.1109/QRS51102.2020.00071
[20]GregoryGay.2017.Generatingeffectivetestsuitesbycombiningcoveragecriteria.
InInternationalSymposiumonSearchBasedSoftwareEngineering .Springer,65â€“
82.
[21]SijiaGeng,YuekangLi,YunlanDu,JunXu,YangLiu,andBingMao.2020. An
EmpiricalStudyonBenchmarksofArtificialSoftwareVulnerabilities. (March
2020).https://arxiv.org/abs/2003.09561v1
[22]MilosGligoric,AlexGroce,ChaoqiangZhang,RohanSharma,MohammadAmin
Alipour, and Darko Marinov. 2015. Guidelines for Coverage-Based Comparisons
of Non-Adequate Test Suites. ACM Transaction on Software Engineering and
Methodology 24, 4, Article 22 (Sept. 2015), 33 pages. https://doi.org/10.1145/
2660767
[23]Google. 2021. ClusterFuzz. https://google.github.io/clusterfuzz/. [Online; ac-
cessed 16.Aug.21].
[24]Google.2021. OSS-Fuzz:ContinuousFuzzingforOpenSourceSoftware. https:
//github.com/google/oss-fuzz. [Online; accessed 16.Aug.21].
[25]Rahul Gopinath, Philipp GÃ¶rz, and Alex Groce. 2022. Mutation Analysis: An-
swering the Fuzzing Challenge. arXiv:2201.11303 [cs.SE]
[26]RahulGopinath,CarlosJensen,andAlexGroce.2014. CodeCoverageforSuite
Evaluation by Developers. In Proceedings of the 36th International Conference on
Software Engineering (Hyderabad, India) (ICSE 2014). Association for Computing
Machinery,NewYork,NY,USA,72â€“82. https://doi.org/10.1145/2568225.2568278
[27]PÃ©terGyimesi,BÃ©laVancsics,AndreaStocco0001,DavoodMazinanian,ÃrpÃ¡d
BeszÃ©des, Rudolf Ferenc, and Ali Mesbah 0001. 2019. BugsJS: a Benchmark of
JavaScriptBugs.In Proceedingsofthe12thInternationalConferenceonSoftware
Testing,VerificationandValidation.IEEE,90â€“101. https://doi.org/10.1109/ICST.
1632On the Reliability of Coverage-Based Fuzzer Benchmarking ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
2019.00019
[28]Ahmad Hazimeh, Adrian Herrera, and Mathias Payer. 2020. Magma: A Ground-
Truth Fuzzing Benchmark. In Proceedings of the jointinternational conference on
Measurementandmodelingofcomputersystems (SIGMETRICSâ€™20).Association
for Computing Machinery, New York, NY, USA, 29 pages. https://doi.org/10.
1145/3428334
[29]Ben Hermann, Stefan Winter, and Janet Siegmund. 2020. Community Expec-
tations for Research Artifacts and Evaluation Processes. In Proceedings of the
28thACMJointMeetingonEuropeanSoftwareEngineeringConferenceandSympo-
sium on the Foundations of Software Engineering (Virtual Event, USA) (ESEC/FSE
2020). Association for Computing Machinery, New York, NY, USA, 469â€“480.
https://doi.org/10.1145/3368089.3409767
[30]AdrianHerrera,HendraGunadi,ShaneMagrath,MichaelNorrish,MathiasPayer,
and Antony L. Hosking. 2021. Seed Selection for Successful Fuzzing. In Proceed-
ingsofthe30thACMSIGSOFT InternationalSymposiumonSoftwareTestingand
Analysis(Virtual, Denmark) (ISSTA2021).Association forComputing Machinery,
New York, NY, USA, 230â€“243. https://doi.org/10.1145/3460319.3464795
[31]Marc Heuse. 2020. Twitter. https://twitter.com/hackerschoice/status/
1302514351811842056. [Online; accessed 16.Aug.21].
[32]Laura Inozemtseva and Reid Holmes. 2014. Coverage is Not Strongly Correlated
with Test Suite Effectiveness. In Proceedings of the 36th International Conference
on Software Engineering (Hyderabad, India) (ICSE 2014) . Association for Comput-
ingMachinery,NewYork,NY,USA,435â€“445. https://doi.org/10.1145/2568225.
2568271
[33]Marko Ivankovic, Goran Petrovic, RenÃ© Just, and Gordon Fraser. 2019. Code
coverageatGoogle.In Proceedingsofthe201927thACMJointMeetingonEuropean
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (ESEC/FSEâ€™19).AssociationforComputingMachinery,NewYork,
NY, USA, 955â€“963.
[34]RenÃ© Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Databaseof Existing Faults to Enable Controlled Testing Studies for Java Programs. In
Proceedings of the 2014 International Symposium on Software Testing and Analysis
(SanJose,CA,USA) (ISSTA2014).AssociationforComputingMachinery,New
York, NY, USA, 437â€“440. https://doi.org/10.1145/2610384.2628055
[35]RenÃ©Just,DarioushJalali,LauraInozemtseva,MichaelD.Ernst,ReidHolmes,and
Gordon Fraser. 2014. Are Mutants a Valid Substitute for Real Faults in Software
Testing?.In Proceedingsofthe22ndACMSIGSOFTInternationalSymposiumon
Foundations of Software Engineering (Hong Kong, China) (FSE 2014). Association
for Computing Machinery, New York, NY, USA, 654â€“665. https://doi.org/10.
1145/2635868.2635929
[36]GeorgeKlees,AndrewRuef,BenjiCooper,ShiyiWei,andMichaelHicks.2018.EvaluatingFuzzTesting.In Proceedingsofthe2018ACMSIGSACConferenceon
ComputerandCommunicationsSecurity (CCSâ€™18).AssociationforComputing
Machinery, 2123â€“2138. https://doi.org/10.1145/3243734.3243804
[37]Pavneet Singh Kochhar, Ferdian Thung, and David Lo. 2015. Code coverageand test suite effectiveness: Empirical study with real bugs in large systems.
In2015IEEE22ndInternationalConferenceonSoftwareAnalysis,Evolution,and
Reengineering (SANER). 560â€“564. https://doi.org/10.1109/SANER.2015.7081877
[38]CarolineLemieuxandKoushikSen.2018. FairFuzz:ATargetedMutationStrat-
egy for Increasing Greybox Fuzz Testing Coverage. In Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software Engineering (ASE2018). Association for Computing Machinery, New York, NY, USA, 475â€“485.
https://doi.org/10.1145/3238147.3238176
[39]StephanLipp,DanielElsner,ThomasHutzelmann,SebastianBanescu,Alexan-
der Pretschner, and Marcel BÃ¶hme. 2022. FuzzTastic: A Fine-grained, Fuzzer-agnostic Coverage Analyzer. In Proceedings of the 44th International Confer-
ence on Software Engineering Companion (ICSEâ€™22 Companion). 1â€“5. https:
//doi.org/10.1145/3510454.3516847
[40]ChenyangLyu,ShoulingJi,ChaoZhang,YuweiLi,Wei-HanLee,YuSong,and
Raheem Beyah. 2019. MOPT: Optimized Mutation Scheduling for Fuzzers. In
Proceedingsofthe28thUSENIXConferenceonSecuritySymposium (SantaClara,
CA, USA) (SECâ€™19). USENIX Association, USA, 1949â€“1966.
[41]J.MartinBlandandDouglasG.Altman.1986. Statisticalmethodsforassessing
agreement between two methods of clinical measurement. The Lancet 327,
8476 (1986), 307â€“310. https://doi.org/10.1016/S0140-6736(86)90837-8 Originally
published as Volume 1, Issue 8476.
[42]Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia
medica22, 3 (2012), 276â€“282.
[43]P.McMinn.2004. Search-basedsoftwaretestdatageneration:asurvey:Research
Articles. Software Testing, Verification & Reliability 14 (2004), 105â€“156.
[44]Phil McMinn. 2011. Search-Based Software Testing: Past, Present and Future. In
2011 IEEE Fourth International Conference on Software Testing, Verification and
Validation Workshops. 153â€“163. https://doi.org/10.1109/ICSTW.2011.100
[45]Jonathan Metzman, LÃ¡szlÃ³ Szekeres, Laurent Maurice Romain Simon, Read Trev-
elin Sprabery, and Abhishek Arya. 2021. FuzzBench: An Open Fuzzer Bench-
marking Platform andService. In Proceedingsof the29th ACMJoint Meetingon
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE 2021). Association for Computing Machinery,New York, NY, USA, 1393â€“1403. https://doi.org/10.1145/3468264.3473932
[46]Microsoft.2021. OneFuzz:Aself-hostedFuzzing-As-A-Serviceplatform. https:
//github.com/microsoft/onefuzz. [Online; accessed 16.Aug.21].
[47]Stephan Neuhaus, Thomas Zimmermann, Christian Holler, and Andreas Zeller.
2007.PredictingVulnerableSoftwareComponents.In Proceedingsofthe14thACM
ConferenceonComputerandCommunicationsSecurity (Alexandria,Virginia,USA)
(CCSâ€™07).Association forComputing Machinery,NewYork, NY,USA, 529â€“540.
https://doi.org/10.1145/1315245.1315311
[48]Tyler Nighswander. 2020. Twitter. https://twitter.com/tylerni7/status/
1374519171413766145. [Online; accessed 16.Aug.21].
[49]SebastianÃ–sterlund,KavehRazavi,HerbertBos,andCristianoGiuffrida.2020.
ParmeSan:Sanitizer-guidedGreyboxFuzzing.In 29thUSENIXSecuritySymposium
(USENIX Security 20). USENIX Association, 2289â€“2306.
[50]Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae. 2018. Are
Mutation Scores Correlated with Real Fault Detection? A Large Scale Empirical
Study on the Relationship between Mutants and Real Faults. In Proceedings of
the 40th International Conference on Software Engineering (Gothenburg, Sweden)
(ICSE â€™18). Association for Computing Machinery, New York, NY, USA, 537â€“548.
https://doi.org/10.1145/3180155.3180183
[51]Jibesh Patra and Michael Pradel. 2021. Semantic Bug Seeding: A Learning-Based Approach for Creating Realistic Bugs (ESEC/FSE 2021). Association for
ComputingMachinery,NewYork,NY,USA,906â€“918. https://doi.org/10.1145/
3468264.3468623
[52]Jannik Pewny and Thorsten Holz. 2016. EvilCoder: Automated Bug Insertion
(ACSACâ€™16).AssociationforComputingMachinery,NewYork,NY,USA,214â€“225.
https://doi.org/10.1145/2991079.2991103
[53]Van-ThuanPham,MarcelBÃ¶hme,AndrewE.Santosa,AlexandruR.CÄƒciulescu,
andAbhikRoychoudhury.2019. SmartGreyboxFuzzing. IEEETransactionson
Software Engineering (2019), 1â€“17.
[54]Subhajit Roy, Awanish Pandey, Brendan Dolan-Gavitt, and Yu Hu. 2018. BugSynthesis: Challenging Bug-Finding Tools with Deep Faults. In Proceedings of
the201826thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposium on the Foundations of Software Engineering (ESEC/FSE 2018).
Association for Computing Machinery, New York, NY, USA, 224â€“234. https:
//doi.org/10.1145/3236024.3236084
[55]PatrickSchober,ChristaBoer,andLothar ASchwarte.2018. Correlationcoeffi-
cients: appropriate use and interpretation. Anesthesia & Analgesia 126, 5 (2018),
1763â€“1768.
[56]KostyaSerebryany.2021. libFuzzer-alibraryforcoverage-guidedfuzztesting.
https://llvm.org/docs/LibFuzzer.html. [Online; accessed 16.Aug.21].
[57]Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry
Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker. In Proceed-
ings of the 2012 USENIX Conference on Annual Technical Conference (Boston, MA)
(USENIX ATCâ€™12). USENIX Association, USA, 28.
[58]RobertSwiecki.2021. Honggfuzz. https://github.com/google/honggfuzz. [Online;
accessed 16.Aug.21].
[59]Howard E.A. Tinsley and David J. Weiss. 2000. 4 - Interrater Reliability and
Agreement. In Handbook of Applied Multivariate Statistics and Mathematical
Modeling, Howard E.A. Tinsley and Steven D. Brown (Eds.). Academic Press, San
Diego, 95â€“124. https://doi.org/10.1016/B978-012691360-6/50005-7
[60]David A. Tomassi, Naji Dmeiri, Yichen Wang, Antara Bhowmick, Yen-Chuan
Liu,PremkumarT.Devanbu,BogdanVasilescu,andCindyRubio-GonzÃ¡lez.2019.
BugSwarm: mining and continuously growing a dataset of reproducible failures
and fixes. In ICSE. IEEE / ACM, 339â€“349.
[61]Yi Wei, Bertrand Meyer, and Manuel Oriol. 2012. Is Branch Coverage a Good
Measure of Testing Effectiveness? Springer Berlin Heidelberg, Berlin, Heidelberg,
194â€“212. https://doi.org/10.1007/978-3-642-25231-0_5
[62]Cheng Wen. 2021. Recent Papers Related To Fuzzing. https://github.com/
wcventure/FuzzingPaper. [Online; accessed 16.Aug.21].
[63]MichaÅ‚Zalewski. 2021. american fuzzy lop (2.52b). https://lcamtuf.coredump.cx/
afl/. [Online; accessed 16.Aug.21].
[64]Andreas Zeller. 2022. A Tweet. https://twitter.com/AndreasZeller/status/
1468142858553200644. [Online; accessed 10.Feb.22].
[65]Andreas Zeller, Sascha Just, and Kai Greshake. 2019. When Results Are All
That Matters: Consequences. https://andreas-zeller.blogspot.com/2019/10/when-
results-are-all-that-matters.html. [Online; accessed 16.Aug.21].
[66]Andreas Zeller, Sascha Just, and Kai Greshake. 2019. When Results Are All
That Matters: The Case of the Angora Fuzzer. https://andreas-zeller.blogspot.
com/2019/10/when-results-are-all-that-matters-case.html. [Online; accessed
16.Aug.21].
[67]Yucheng Zhang and Ali Mesbah. 2015. Assertions Are Strongly Correlated
with Test Suite Effectiveness. In Proceedings of the 2015 10th Joint Meeting on
FoundationsofSoftwareEngineering (Bergamo,Italy) (ESEC/FSE2015).Association
for Computing Machinery, New York, NY, USA, 214â€“224. https://doi.org/10.
1145/2786805.2786858
[68]Xiaogang Zhu, Shigang Liu, Xian Li, Sheng Wen, Jun Zhang, Seyit Ahmet
Ã‡amtepe,andYangXiang.2020. DeFuzz:DeepLearningGuidedDirectedFuzzing.
CoRRabs/2010.12149 (2020).
1633