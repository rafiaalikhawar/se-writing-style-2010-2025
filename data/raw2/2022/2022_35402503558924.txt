MpBP: VerifyingRobustness ofNeural Networks with
Multi-path Bound Propagation
Ye Zheng
zhengyeah@foxmail.com
ShenzhenUniversity
Shenzhen,ChinaJiaxiang Liu‚àó
jiaxiang0924@gmail.com
ShenzhenUniversity
Shenzhen,ChinaXiaomu Shi
xshi0811@gmail.com
ShenzhenUniversity
Shenzhen,China
ABSTRACT
Robustness of neural networks need be guaranteed in many safety-
criticalscenarios,suchasautonomousdrivingandcyber-physical
controlling. In this paper, we present MpBP, a tool for verifying
the robustness of neural networks. MpBPis inspired by classi-
cal bound propagation methods for neural network verification,
and aims to improve the effectiveness by exploiting the notion
ofpropagationpaths.Specifically, MpBPextends classicalbound
propagation methods, including forward bound propagation, back-
ward bound propagation, and forward+backward bound propa-
gation, with multiple propagation paths. MpBPis based on the
widely-usedPyTorchmachinelearningframework,henceprovid-
ing efficient parallel verification on GPUs and user-friendly us-
age. We evaluate MpBPon neural networks trained on standard
datasets MNIST, CIFAR-10 and Tiny ImageNet. The results demon-
strate the effectiveness advantage of MpBPbeyond two state-of-
the-art bound propagation tools LiRPA and GPUPoly, with compa-
rableefficiencytoLiRPAandsignificantlyhigherefficiencythan
GPUPoly. A video demonstration that showcases the main fea-
tures ofMpBPcan be found at https://youtu.be/3KyPMuPpfR8 .
Source code is available at https://github.com/formes20/MpBP and
https://doi.org/10.5281/zenodo.7029261 .
CCSCONCEPTS
‚Ä¢Software and its engineering ‚ÜíFormal methods ;Formal
softwareverification .
KEYWORDS
formalverification,neuralnetworks,boundpropagation
ACM Reference Format:
YeZheng,JiaxiangLiu,andXiaomuShi.2022. MpBP:VerifyingRobustnessof
Neural Networks with Multi-path Bound Propagation. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE ‚Äô22), November 14≈õ
18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 5pages.https:
//doi.org/10.1145/3540250.3558924
‚àóCorresponding author
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Copyright heldby theowner/author(s).
ACM ISBN 978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.35589241 INTRODUCTION
Deep neural networks (DNNs) have been increasingly deployed in
manyrealisticscenarios[ 1,17,21,27].Nevertheless,thesafetyof
neural networks is difficult to be guaranteed. Especially, there are
many signs that DNNs are not robust[26]. That is, the output of
a DNN may be unstable against small perturbations to the input.
For instance, some dust on a ≈Çstop≈æ traffic sign that is negligible
for humans,maylead to amisclassificationof the signinto an≈Ç80
km/h≈ætrafficsignbyanauto-drivingsystem,whichisintolerant.
The lack of robustness hinders the deployment of DNNs in such
safety-criticalscenarios.
Atypicalrobustnessevaluationtechniqueistesting,whichchecks
whetheraperturbedinputinducesanincorrectoutput.Alargebody
of techniques are developed to test the robustness of DNNs [ 33].
Theyareeffectivetofindadversarialexamplesviolatingrobustness,
butcannotformally guaranteetherobustness.Ontheotherhand,
formalverification(e.g.[ 9,11,12,20,25,29≈õ31])isabletoformally
provetherobustnessofagivenDNNagainstperturbedinputs,thus
providingaformalguarantee for safety-criticalapplications.
Bound propagation [29] plays an important role in DNN veri-
fication. It is a method for calculating provable lower and upper
bounds of each neuron in a DNN, according to the input region
generatedbyperturbations.Theprovableboundsofoutputneurons
arethenusedtodeterminewhetheramisclassificationmayexist.If
no,therobustnessw.r.t.theinputregionisguaranteed.Duetoits
effectivenessandefficiency,boundpropagationisoftenusedasa
stand-alone verification method [ 18,23,24,29,30], but also can be
utilized as an essential component in other verification techniques,
such as the Branch-and-Bound (BaB) method [ 8,28,30]. It is the
mainstream choice for bounding in BaB methods, which can be
usedtodeterminethe nextbranching.
Tightness is the most crucial measurement for the effectiveness
of bound propagation methods. Calculating tighter bounds means
thatamethodcandealwithmorerobustnessverificationproblems.
Accordingtopropagationstrategies,boundpropagation isfurther
categorized into backward bound propagation (BBP),forward bound
propagation (FBP),forward+backward bound propagation (FBBP)
andintervalboundpropagation (IBP),withdifferenttrade-offsbe-
tweentightnessandefficiency[ 29]. Manyeffortshave beenmade
to improve the tightness [ 8,25,28,30,31]. Our recent work pro-
posesthenotionof propagationpaths forBBP[34].Withthisnotion,
existing BBPmethods[ 24,29,32]canbe seen asemployingonly
one singlepropagationpath,whichis a specialcase of multi-path
BBP.Theexperimentalresultsin[ 34]showthatleveragingmultiple
pathsinBBPtightenstheoutputboundseffectively.Furthermore,
the notion ofpropagation paths is suitable for parallelization.
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1692
ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Ye Zheng,Jiaxiang Liu, andXiaomu Shi
.pth .onnx  modelsspecfication/gid00049/gid00032/gid00045/gid00036/gid00414/gid00032/gid00031
reachable region/gid00048/gid00041/gid00038/gid00041/gid00042/gid00050/gid00041
/gid00020/gid00021/gid00016/gid00017/gid00429/gid00421
¬ã¬Ä¬Ä/gid00020/gid00021/gid00016/gid00017/g3/g3/g3/g71/g14
0x/gid00014/gid00043/gid00007/gid00003/gid00017 /gid00014/gid00043/gid00003/gid00003/gid00017
/gid00014/gid00043/gid00007/gid00003/gid00003/gid00017 /gid00010/gid00003/gid00017
PyTorch frameworkconstraint generatorverification methodsM/g3465BP
path configuration
Figure 1:Architecture andWorkflow of MpBP
Inthispaper,wepresentaDNNverificationtool MpBP,whichex-
tendstheideaofmulti-pathBBPto Multi-pathBoundPropagation.
MpBPis implemented based on the widely-used machine learning
frameworkPyTorch[ 4].Itprovidesefficientparallelverification,es-
peciallyinthepresenceofGPUs.Specifically,thetwomainfeatures
ofMpBPare as follows:
‚Ä¢It is a multi-path bound propagation tool. MpBPsupports
fourboundpropagationtechniques:multi-pathBBP,multi-
path FBP, multi-path FBBP, and classical IBP. They offer
varioustrade-offs betweenverificationeffectivenessandef-
ficiency,whichcanaccommodatedifferentapplicationsce-
narios.
‚Ä¢It is a PyTorch based verification tool. With efficient parallel
tensorcomputationsprovidedbyPyTorch,boundpropaga-
tion through multiple paths spends almost the same time as
thesingle-pathboundpropagation.Ontheotherhand,for
the users who are already familiar with PyTorch, we believe
thatMpBPprovideseffortless andmore flexibleusage.
2 THE MPBPTOOL
Thearchitectureandworkflowof MpBParedepictedinFigure 1.
MpBPacceptsasinputsaDNNmodel,aspecification,andapath
configuration,then outputswhetherthegiven modelsatisfiesthe
expected specification ( verified) orunknown . The core of MpBP
consists of four modules: MpFBP,MpBBP,MpFBBPandIBP, which
calculate the output bounds of the given DNN via, respectively,
multi-pathFBP,multi-pathBBP,multi-pathFBBP,andtheclassical
IBP techniques. These modules inherit the different trade-offs from
theircorrespondingtechniques.Userscanchooseoneofthemto
fittheirrequirements.Theoutputboundscomputedbythespeci-
fied module constitute the output reachable region. The reachable
region is then compared against the unsafe region (the red square
in Figure 1), which is constructed by the constraint generator with
the input specification. According to the comparison, MpBPout-
puts an answer verified, indicating that the input specification (e.g.
robustness) issatisfiedbythe inputDNN, or unknown .
2.1 Input andOutput
MpBPsupportsfeedforwardfully-connectedneuralnetworks(FFNNs)
and convolutional neural networks (CNNs) with the most com-
monlyusedReLUactivationfunction.Thenetworkcanbespecified
in the PyTorch model format .pthor the.onnxformat. The input
specification identifies the properties that users want to verify. Itconsists of an input x0to the DNN, a perturbation threshold ùõøand
a set of safe/unsafe labels. It specifies that all perturbed inputs,
which are in the neighborhood of x0w.r.t.ùêøùëù-norm distance ùõø, are
classified into safe labels (equivalently, not classified into unsafe
labels)bytheDNN.ForexampleinFigure 1,ifwewanttoverify
whethera≈Çstop≈æsignimagepreservestheprediction≈Çstop≈æeven
withperturbations under threshold ùõø,we use theimageas x0and
set the ≈Çstop≈æ label as safe label (hence others are unsafe). MpBP
supportsthree typesof ùêøùëù-normsto measurethedistancefor per-
turbation: ùêø‚àû-,ùêø1-,andùêø2-norms.Finally,inthe pathconfiguration ,
userscanspecifythenumberofpropagationpathsemployedfor
verification.Morepathsbringtighterbounds.Buttheimprovement
is not permanent. We suggest a default path number 4, which is
empirically chosenthroughsomepreliminary experiments.
Afterboundcalculationwiththespecifiedmodule, MpBPoutputs
the reachable region of the given model w.r.t. x0andùõø. Meanwhile,
it also returns the verified/unknown answer, indicating whether
theinput modelsatisfies theinput specification.For the unknown
answer, users may try to change the configurations (path and/or
verification technique) to perform amore accuratecalculation.
Besidestheaforementionedinputs, MpBPprovidesoptions --bp
toselecttheboundpropagationtechnique,and --verbose tochoose
between differentkindsof outputinformation.
2.2 Multi-path BoundPropagation
InDNNverification,thegiveninput x0andtheperturbationthresh-
oldùõøconstitutetheinputregion,usuallyrepresentedbyintervals.
Therefore,thevalueofeachneuroninthenetworkrangeswithinan
interval instead of being a single value. A comprehensive introduc-
tiontoboundpropagationcanbefoundin[ 29].Generallyspeaking,
backwardboundpropagationcomputestheneuronboundsviaa
depth-2nestingloop.Theouterloopiteratesthroughallneurons
from the input to the output layer by layer. For each neuron, the
innerloopcalculatesitslowerandupperboundfunctions,byiterat-
ing,conversely,towardtheinputlayerusingallneuronboundsthat
arealreadycomputed.Thedirectionoftheinnerloopdefinesthe
notion ≈Çbackward propagation≈æ. The two bound functions are then
concretized by the input intervals to obtain the concrete bounds
of the neuron. These bounds are again leveraged to compute the
bound functionsfor neuronsinthe nextlayers inthe network.
Classical BBP tools, like ERAN[15] andCROWN [32], maintain
onlyasinglepairoflowerandupperboundfunctionsforeachneu-
ron.Thenotionofpropagationpathsregardsthespecifictechnique
1693MpBP: Verifying Robustness of Neural NetworkswithMulti-pathBound Propagation ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
forcomputingtheboundfunctionsasaspecificpath[ 34].Itisthen
natural to synergize different techniques to achieve better bounds.
Multi-pathBBPextendsclassicalBBPwithmultiplepropagation
paths to obtain multiple pairs of lower and upper bound functions
for each neuron. Multiple concrete bounds for a neuron can be
derived from these bound functions. Since they are all provable
bounds, their intersection provides a tighter, or at least identical,
provablebound. The possibly betterboundwill benefit thebound
computationfor neuronsinnextlayers.
Figure2exemplifies two-path BBP on a simple DNN. Assume
thatallneuronsinthenetworkhavethebias 0.Consideringtheneu-
ronùë•ùëñ+1,1,twolowerboundfunctions ùëô1(ùë•0,1,ùë•0,2)andùëô2(ùë•0,1,ùë•0,2)
areobtainedfromtwoover-approximationtechniquesforbound
propagation [ 24]:ùë•ùëñ+1,1‚â•ùë•ùëñ,1andùë•ùëñ+1,1‚â•0, respectively. These
two techniques constitute two propagation paths. By concretizing
ùëô1(ùë•0,1,ùë•0,2)andùëô2(ùë•0,1,ùë•0,2)with input intervals, we get two con-
crete lower bounds ùëôùëè1andùëôùëè2ofùë•ùëñ+1,1. Takingùëôùëè=max{ùëôùëè1,ùëôùëè2}
guarantees apossiblytighterlower bound of ùë•ùëñ+1,1.
‚ãØ
‚ãØ ‚ãØ‚ãØ
‚ãØ1/g16
1/g16111,1ix/g16
1,2ix/g16 ,2ix,1ix1,1ix/g14
,1ReLU( )ix1,10ix/g14/g1161,10ix/g14/g1161,1 ,1i ix x/g14/g1161,1 1,2 1,1i i ix x x/g14 /g16 /g16/g116 /g16 Path 1
Path 2 
0,1x
0,2x1,1 1 0,1 0,2( , )il x x x/g14/g116
0 2 1,1 0,1 ,2( , )il x x x/g14/g116
Figure 2:AnExample ofTwo-path BBPforNeuron ùë•ùëñ+1,1
Although the notion of propagation paths is proposed when
considering BBP, it is applicable as well when discussing other
bound propagationmethodslike FBPandFBBP.
ForwardboundpropagationisanalternativetoBBP.UnlikeBBP,
FBPcalculatestheneuronboundsfromtheinputtotheoutputlayer
by layer, without performing backward propagation (i.e., the inner
loop). Therefore, it is extremely efficient, with time complexity
O(ùëÅ), whereùëÅis the number of neurons in the DNN. As a conse-
quence, thebounds obtained by FBP are looserthan those by BBP.
ComparedtoBBP,FBPsacrificestightnessforefficiency.Butitis
an important alternative in the applications like BaB. In the BaB
method,boundpropagationisoftenthechoicehelpingtodecide
the next branching. It requires the bound propagation technique to
be fast enough with acceptable tightness, making FBP an excellent
choice. Like BBP, existing FBP techniques employ only a single
propagationpath.WeextendtheclassicalFBPtomulti-pathFBPin
MpBPusing multiple propagation paths. It guarantees tighter, at
leastidentical,bounds thanthe classicalsingle-path FBP.
Forward+backwardboundpropagationisacompromisebetween
FBPandBBP.Itcomputestheneuronbounds layerbylayer with-
out backward propagation like FBP until the last layer, but then
performs backward propagation for the neurons in the last layer
to obtain their bound functions. FBBP bridges the tightness gap
and efficiency gap between FBP and BBP. We extend it to multi-
path FBBP, which also bridges the tightness gap and efficiency gap
between multi-path FBPandmulti-path BBP.Note that in Figure 2, besides ùë•ùëñ+1,1‚â•ùë•ùëñ,1andùë•ùëñ+1,1‚â•0, it
holds that ùë•ùëñ+1,1‚â•ùúÜùë•ùëñ,1for anyùúÜ‚àà [0,1]. We take the following
strategy [ 34] to prescribe the propagation paths in MpBP. The
first propagation path utilizes the heuristics in [ 24], adopting a
strategy to minimize local over-approximation errors. When the
path number ùëö=2, besides the first path, we take ùúÜ=0when
computing bounds as the second propagation path. For ùëö=3, the
third propagation path takes ùúÜ=1. Whenùëö‚â•4, along with the
abovefirstthreepaths,wefurtheraddthe (ùëö‚àí2)-sectionpoints
of the interval [0,1]as theùúÜ‚Äôs for the other (ùëö‚àí3)propagation
paths. For instance, ùúÜ4=1
2for the 4th path when ùëö=4;ùúÜ4=1
3
andùúÜ5=2
3for the 4th and5th pathsrespectivelywhen ùëö=5.
The computations by different propagation paths are easy to
parallelize.Theoretically,themulti-pathboundpropagationtech-
niques can improve tightness without sacrificing efficiency via
parallelization,comparedtotheirclassicalsingle-pathcounterparts.
We implement the multi-path bound propagation techniques in Py-
Torch.Byplacingdifferentpathsintoanewdimensionoftensors,
it enables the same time cost as the single-path bound propagation.
3 EXPERIMENTS
We evaluate MpBPagainst two state-of-the-art bound propagation
tools LiRPA [ 29] and GPUPoly [ 19]. LiRPA is the essential com-
ponent performing bound propagation in ùõº-ùõΩ-CROWN [3], the
winner of the 2nd International Verification of Neural Networks
Competition [ 2]. GPUPoly is the GPU implementation of the main
boundpropagationenginein ERAN[15],anactivelyupdatedverifi-
cationtool.TheevaluationisconductedinthreeDNNs:(i)anFFNN
trained by the MNIST dataset [ 16], which contains 800 neurons;
(ii)aCNNtrainedbytheCIFAR-10dataset[ 14],withabout 2.8√ó105
neurons; and (iii) a CNN trained by the Tiny ImageNet dataset [ 5],
consisting ofabout 4.7√ó105neurons, whichis thelargestamong
thethree.AllexperimentsarerunonaLinuxserverwithtwoIn-
telXeonGold6132CPUs,aNVIDIATeslaV100GPUand256GB
memory. MpBPsetsthepathnumberto4,whiletheperturbation
thresholds are allmeasuredw.r.t. ùêø‚àû-norm.
Effectiveness. To evaluate effectiveness, we compare MpBPwith
LiRPA andGPUPoly on all three DNNs with different bound prop-
agation techniques. The task is to verify whether the first 100
correctly-classifiedimagesineachdatasetarerobustagainstvar-
ious perturbation thresholds. The results are depicted in Table 1.
It shows the numbers of robustness problems successfully verified
bythetools.Alargernumberindicatesthatthecorrespondingtool
is more effective. Overall, it is easily observed that, with the same
boundpropagationtechnique,ourtool MpBPismoreeffectivethanthe
other two toolsonall networksagainst every perturbationthreshold.
OntheMNISTFFNN,wecomparethemulti-pathFBPandmulti-
path FBBP techniques in MpBPwith the classical FBP and FBBP
techniquesinLiRPA,respectively.GPUPolyisnotinthecomparison
sinceitdoesnotsupportFBPorFBBP.With(multi-path)FBP,we
can see that MpBPverifies more problems than LiRPA for each
perturbationthresholdinTable 1.Thebestimprovementappears
whenùõø=0.0026,MpBPsuccessfullyverifying7( =40‚àí33)more,out
of100,problemsthanLiRPA.Usingthemoreeffectivetechnique
FBBP, both tools are able to verify more problems. Still, MpBP
outperforms LiRPA byleveragingmultiple pathsinFBBP.
1694ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Ye Zheng,Jiaxiang Liu, andXiaomu Shi
Table 1: Effectiveness Evaluation: Numbers of verified prob-
lems are shown.Larger numbermeansmore effective.
Tools ModelsandPerturbationThresholds ùõø
MNIST FFNN
0.0014 0.0018 0.0022 0.0026
FBPMpBP 73 62 51 40
LiRPA 69 59 48 33
FBBPMpBP 86 78 69 58
LiRPA 83 77 66 56
CIFAR-10 CNN Tiny ImgNet CNN
0.0010 0.0014 0.0010 0.0014
BBPMpBP 61 38 27 22
LiRPA 56 36 25 19
GPUPoly 56 36 - -
ForthetightesttechniqueBBP,wecompare MpBPwithLiRPA
andGPUPolyonthetwolargeCNNs.OntheCIFAR-10CNN,Table 1
demonstrates that GPUPoly achieves the same results as LiRPA,
andMpBPverifies more problems, 5 and 2 respectively, for both
perturbation thresholds. As for the largest network Tiny ImageNet
CNN,the≈Ç-≈æinthetableindicatesthatGPUPolycurrentlydoesnot
supporttheTinyImageNetdataset.Lessproblemsareverifiedby
bothMpBPandLiRPAforthislargestnetwork.Notwithstanding,
MpBPrevealstheadvantagesbeyondLiRPA,verifying2and3more
problems respectivelyfor the twoperturbation thresholds.
In summary, the evaluation demonstrates that MpBPis more
effectivethanLiRPAandGPUPoly,thankstothemulti-pathbound
propagationtechniques.
Efficiency. Toevaluateefficiency,wechoosetheMNISTFFNN
andtheperturbationthreshold ùõø=0.0026,andtrytoverifythesame
100robustnessproblemsasabove.Wecomparetheverificationtime
spent by all three tools, configured with each supported technique.
WedidnotevaluateonthetwoCNNsbecauseFBPandFBBPcan
hardlyverifyany problems for theselarge networks.
Figure3illustrates the verification time by each tool with differ-
enttechniquestoverifyallthe100robustnessproblems.Thex-axis
listsallthetoolswithdifferentconfiguredtechniques,whichare
groupedw.r.t.thetypesofboundpropagation(drawnindifferent
colors). The y-axis refers to the verification time (in seconds) spent
by the tools. The results demonstrate that each configuration of
MpBPis slightly slower than the corresponding configuration of
LiRPA, but the difference is negligible. Using BBP, MpBPis even
much faster ( ‚âà212√ó) than GPUPoly, which is also accelerated by
GPUs. On the other hand, we can see from the figure that multi-
path FBBP does bridge the efficiency gap between multi-path FBP
andmulti-path BBP.
In summary, the evaluation shows that MpBPis much more
efficient thanGPUPoly,whilecomparable to LiRPA.
4 RELATED WORK
A growing attention has been paid to DNN verification. Earlier
worksreduceDNNverificationproblemstoconstraintsolvingprob-
lems,e.g.SMTproblems[ 9,12,13]andLP/MILPproblems[ 6,7,22].00.050.0310.0520.220.250.2859.60
0.26
0.20.35960
M/g3465BP(/gid00014/gid00043/gid00007/gid00003/gid00017)
LiRPA(FBP)LiRPA(FBBP)M/g3465BP(/gid00014/gid00043/gid00007/gid00003/gid00003/gid00017)
LiRPA(BBP)M/g3465BP(/gid00014/gid00043/gid00003/gid00003/gid00017)
GPUPolyTime (s)FBP
FBBP
BBP
Figure 3:Efficiency: ComparisonofVerification Time
Thesemethodsaremostlysound(i.e.nofalsenegative)andcom-
plete(i.e.,nofalsepositive).Buttheysufferfromlimitedscalabil-
ity due to the computational complexity. Bound propagation is
a lightweight method in terms of complexity. It is sound but in-
complete, because of the over-approximation techniques that have
beenused[ 18,24,29].Itturnsouttobeeffectiveandefficient,thus
applicabletolarge-scalenetworks.Ourworkaimstoimprovethe
effectivenessofboundpropagationwithoutsacrificingitsefficiency.
Many efforts have been made to improve the tightness, hence
effectiveness, of bound propagation. A line of work focuses on de-
signingnovelover-approximationtechniquestobalancetightness
with efficiency, e.g. [ 10,23,24,32]. With the notion of propagation
paths,theseworksdesignbetterpropagation paths,whichcanbe
easilyintegratedin MpBP.Ontheotherhand, ùõº-CROWN [30]lever-
ages optimization techniques to achieve better over-approximation
for neurons in the DNNs, while the authors in [ 25] and [31] ex-
ploit LP or MILP solving to calculate tighter bounds of neurons for
propagation. These works introduce constraint solving into bound
propagation,makingitmoreeffectivebutconsiderablylessefficient.
Instead,MpBPimprovestheeffectivenesswithmerelynegligible
extratimecost.Anotherimportantmethodtotightenthebounds
is the BaB method, where bound propagation is usuallychosen as
the bounding component [ 8,28,30].MpBPis orthogonal to BaB
methods,andcan be usedinBaBfor boundingas well.
5 CONCLUSION
Thispaperpresents MpBP,aDNNverificationtoolthatleverages
multi-pathboundpropagationtechniques. MpBPshowsitseffec-
tivenessadvantagesovertwostate-of-the-artboundpropagation
tools,withcompetitive efficiency.BasedonPyTorch, MpBPoffers
friendlyandflexibleusage.Moreover,modeltrainingandtesting
canalso beconducted within MpBP.The training-verifyingwork-
flowissupportedas well.
ACKNOWLEDGMENTS
The authors are funded by the Natural Science Foundation of
GuangdongProvince(2022A1515011458,2022A1515010880),theNa-
tionalNaturalScienceFoundationofChina(61836005,62002228),
and the Shenzhen Science and Technology Innovation Program
(JCYJ20210324094202008, 20200810045225001).
1695MpBP: Verifying Robustness of Neural NetworkswithMulti-pathBound Propagation ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
REFERENCES
[1]2012. Deep Neural Networks for Acoustic Modeling in Speech Recognition: The
Shared Views of Four Research Groups. IEEE Signal Process. Mag. 29, 6 (2012),
82≈õ97.https://doi.org/10.1109/MSP.2012.2205597
[2]2021.2nd International Verification of Neural Networks Competition (VNN-
COMP‚Äô21) . Retrieved 2021 from https://sites.google.com/view/vnn2021
[3] 2022. alpha-beta-CROWN:AFastand Scalable Neural Network Verifier with Effi-
cient Bound Propagation .https://github.com/huanzhang12/alpha-beta-CROWN
[4] 2022. PyTorch.https://pytorch.org/
[5] 2022. TinyImageNet .http://cs231n.stanford.edu/tiny-imagenet-200.zip
[6]Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and
Juan Pablo Vielma. 2020. Strong mixed-integer programming formulations
for trained neural networks. Math. Program. 183, 1 (2020), 3≈õ39. https:
//doi.org/10.1007/s10107-020-01474-5
[7]Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth
Misener. 2020. Efficient Verification of ReLU-Based Neural Networks via Depen-
dencyAnalysis.In TheThirty-FourthAAAIConferenceonArtificialIntelligence,
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference,IAAI2020,TheTenthAAAISymposiumonEducationalAdvancesin
ArtificialIntelligence,EAAI2020,NewYork,NY,USA,February7-12,2020 .AAAI
Press,3291≈õ3299. https://ojs.aaai.org/index.php/AAAI/article/view/5729
[8]Rudy Bunel,Jingyue Lu, IlkerTurkaslan,Philip H.S.Torr,Pushmeet Kohli,and
M.PawanKumar.2020. BranchandBoundforPiecewiseLinearNeuralNetwork
Verification. J.Mach.Learn.Res. 21(2020),42:1≈õ42:39. http://jmlr.org/papers/
v21/19-468.html
[9]R√ºdiger Ehlers. 2017. Formal Verification of Piece-Wise Linear Feed-Forward
NeuralNetworks.In AutomatedTechnologyforVerificationandAnalysis-15th
InternationalSymposium,ATVA2017,Pune,India,October3-6,2017,Proceedings
(Lecture Notes in Computer Science, Vol. 10482) , Deepak D‚ÄôSouza and K. Narayan
Kumar (Eds.). Springer, 269≈õ286. https://doi.org/10.1007/978-3-319-68167-2_19
[10]TimonGehr, MatthewMirman,DanaDrachsler-Cohen,PetarTsankov, Swarat
Chaudhuri, andMartin T.Vechev. 2018. AI2: Safety and RobustnessCertification
of Neural Networks with Abstract Interpretation. In 2018 IEEE Symposium on Se-
curityandPrivacy,SP2018,Proceedings,21-23May2018,SanFrancisco,California,
USA. IEEE Computer Society, 3≈õ18. https://doi.org/10.1109/SP.2018.00058
[11]Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety
VerificationofDeepNeuralNetworks.In ComputerAidedVerification-29thInter-
nationalConference,CAV2017,Heidelberg,Germany,July24-28,2017,Proceedings,
PartI (LectureNotesinComputerScience,Vol.10426) ,RupakMajumdarandViktor
Kuncak(Eds.).Springer, 3≈õ29. https://doi.org/10.1007/978-3-319-63387-9_1
[12]GuyKatz,ClarkW.Barrett,DavidL.Dill,KyleJulian,andMykelJ.Kochenderfer.
2017. Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks. In
ComputerAidedVerification-29thInternationalConference,CAV2017,Heidelberg,
Germany, July 24-28, 2017, Proceedings, Part I (Lecture Notes in Computer Science,
Vol. 10426) , Rupak Majumdar and Viktor Kuncak (Eds.). Springer, 97≈õ117. https:
//doi.org/10.1007/978-3-319-63387-9_5
[13]Guy Katz, Derek A. Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus,
RachelLim,ParthShah,ShantanuThakoor,HaozeWu,AleksandarZeljic,DavidL.
Dill,MykelJ.Kochenderfer,andClarkW.Barrett.2019. TheMarabouFramework
for Verification and Analysis of Deep Neural Networks. In Computer Aided
Verification-31stInternationalConference,CAV2019,NewYorkCity,NY,USA,July
15-18, 2019, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 11561) , Isil
DilligandSerdarTasiran (Eds.). Springer, 443≈õ452. https://doi.org/10.1007/978-
3-030-25540-4_26
[14]AlexKrizhevsky,GeoffreyHinton,etal .2009. Learningmultiplelayersoffeatures
from tinyimages. (2009).
[15]SRI Lab. 2022. ETH Robustness Analyzer for Neural Networks (ERAN) .https:
//github.com/eth-sri/eran
[16]Yann LeCun. 1998. The MNIST database of handwritten digits .http://yann.lecun.
com/exdb/mnist/
[17]GeertLitjens,ThijsKooi,BabakEhteshamiBejnordi,ArnaudArindraAdiyoso
Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A. W. M. van der Laak,
BramvanGinneken,andClaraI.S√°nchez.2017. Asurveyondeeplearningin
medical imageanalysis. MedicalImageAnal. 42(2017), 60≈õ88. https://doi.org/10.
1016/j.media.2017.07.005
[18]Matthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Differentiable
AbstractInterpretationforProvablyRobustNeuralNetworks.In Proceedingsofthe
35thInternationalConferenceonMachineLearning,ICML2018,Stockholmsm√§ssan,
Stockholm,Sweden,July10-15,2018 (ProceedingsofMachineLearningResearch,
Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.). PMLR, 3575≈õ3583. http:
//proceedings.mlr.press/v80/mirman18b.html
[19]ChristophM√ºller,Fran√ßoisSerre,GagandeepSingh,MarkusP√ºschel,andMar-
tin T. Vechev. 2021. Scaling Polyhedral Neural Network Verification on GPUs. In
Proceedings of Machine Learning and Systems 2021, MLSys 2021, virtual, April 5-9,
2021, AlexSmola,AlexDimakis, and Ion Stoica(Eds.).mlsys.org.
[20]LucaPulinaandArmandoTacchella.2010. AnAbstraction-RefinementApproach
toVerificationofArtificialNeuralNetworks.In ComputerAidedVerification,22ndInternational Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings
(Lecture Notes in Computer Science, Vol. 6174) , Tayssir Touili, Byron Cook, and
Paul B. Jackson (Eds.). Springer, 243≈õ257. https://doi.org/10.1007/978-3-642-
14295-6_24
[21]OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein,
AlexanderC.Berg,andFei-FeiLi.2015. ImageNetLargeScaleVisualRecognition
Challenge. Int. J. Comput. Vis. 115, 3 (2015), 211≈õ252. https://doi.org/10.1007/
s11263-015-0816-y
[22]Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang.
2019. AConvexRelaxationBarriertoTightRobustnessVerificationofNeural
Networks. In Advances in Neural Information Processing Systems 32: Annual Con-
ferenceonNeuralInformationProcessingSystems2019,NeurIPS2019,December
8-14,2019,Vancouver,BC,Canada ,HannaM.Wallach,HugoLarochelle,Alina
Beygelzimer, Florence d‚ÄôAlch√©-Buc, Emily B. Fox, and Roman Garnett (Eds.).
9832≈õ9842.
[23]GagandeepSingh,TimonGehr,MatthewMirman,MarkusP√ºschel,andMartinT.
Vechev. 2018. Fast and Effective Robustness Certification. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al, Canada ,
Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol√≤
Cesa-Bianchi, and Roman Garnett (Eds.). 10825≈õ10836. https://proceedings.
neurips.cc/paper/2018/hash/f2f446980d8e971ef3da97af089481c3-Abstract.html
[24]Gagandeep Singh, Timon Gehr, Markus P√ºschel, and Martin T. Vechev. 2019. An
abstract domain for certifying neural networks. Proc. ACM Program. Lang. 3,
POPL(2019), 41:1≈õ41:30. https://doi.org/10.1145/3290354
[25]Gagandeep Singh, Timon Gehr, Markus P√ºschel, and Martin T. Vechev. 2019.
Boosting Robustness Certification of Neural Networks. In 7th International Con-
ferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,
2019. OpenReview.net. https://openreview.net/forum?id=HJgeEh09KQ
[26]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
IanJ.Goodfellow,andRobFergus.2014. Intriguingpropertiesofneuralnetworks.
In2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada,April14-16,2014,ConferenceTrack Proceedings ,Yoshua BengioandYann
LeCun(Eds.). http://arxiv.org/abs/1312.6199
[27]Chris Urmson and William Whittaker. 2008. Self-Driving Cars and the Urban
Challenge. IEEEIntell.Syst. 23,2(2008),66≈õ68. https://doi.org/10.1109/MIS.2008.
34
[28]Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and
J. Zico Kolter. 2021. Beta-CROWN: Efficient Bound Propagation with Per-
neuron Split Constraints for Neural Network Robustness Verification. In Ad-
vances in Neural Information Processing Systems , M. Ranzato, A. Beygelzimer,
Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran As-
sociates, Inc., 29909≈õ29921. https://proceedings.neurips.cc/paper/2021/file/
fac7fead96dafceaf80c1daffeae82a4-Paper.pdf
[29]KaidiXu,ZhouxingShi,HuanZhang,YihanWang,Kai-WeiChang,MinlieHuang,
BhavyaKailkhura,XueLin,andCho-JuiHsieh.2020. AutomaticPerturbation
Analysis for Scalable Certified Robustness and Beyond. In Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle,
Marc‚ÄôAurelioRanzato,RaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin
(Eds.).
[30]Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and
Cho-Jui Hsieh. 2021. Fast and Complete: Enabling Complete Neural Network
Verification with Rapid and Massively Parallel Incomplete Verifiers. In 9th Inter-
nationalConferenceonLearningRepresentations,ICLR2021,VirtualEvent,Austria,
May3-7,2021 .OpenReview.net. https://openreview.net/forum?id=nVZtXBI6LNn
[31]PengfeiYang,RenjueLi,JianlinLi,Cheng-ChaoHuang,JingyiWang,JunSun,
BaiXue,andLijunZhang.2021. ImprovingNeuralNetworkVerificationthrough
SpuriousRegionGuidedRefinement.In ToolsandAlgorithmsfortheConstruction
and Analysis of Systems - 27th International Conference, TACAS 2021, Held as Part
of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021,
LuxembourgCity,Luxembourg,March27-April1,2021,Proceedings,PartI (Lecture
NotesinComputerScience,Vol.12651) ,JanFrisoGrooteandKimGuldstrandLarsen
(Eds.).Springer, 389≈õ408. https://doi.org/10.1007/978-3-030-72016-2_21
[32]Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
2018. EfficientNeuralNetworkRobustnessCertificationwithGeneralActivation
Functions. In Advances in Neural Information Processing Systems 31: Annual
ConferenceonNeuralInformationProcessingSystems2018,NeurIPS2018,December
3-8,2018, Montr√©al,Canada ,SamyBengio,Hanna M.Wallach,HugoLarochelle,
Kristen Grauman, Nicol√≤ Cesa-Bianchi, and Roman Garnett(Eds.).4944≈õ4953.
[33]Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2022. Machine Learning
Testing: Survey, Landscapes and Horizons. IEEE Trans. Software Eng. 48, 2 (2022),
1≈õ36.https://doi.org/10.1109/TSE.2019.2962027
[34]Ye Zheng, Xiaomu Shi, and Jiaxiang Liu. 2022. Multi-path Back-propagation for
NeuralNetworkVerification(inChinese). RuanJianXueBao/JournalofSoftware
33,7 (07 2022),2464≈õ2481. https://doi.org/10.13328/j.cnki.jos.006585
1696