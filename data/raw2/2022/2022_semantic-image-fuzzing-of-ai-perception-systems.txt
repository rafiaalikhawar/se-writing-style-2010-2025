Semantic Image Fuzzing of AI Perception Systems
Trey Woodlief
University of Virginia
Charlottesville, Virginia, USA
adw8dm@virginia.eduSebastian Elbaum
University of Virginia
Charlottesville, Virginia, USA
selbaum@virginia.eduKevin Sullivan
University of Virginia
Charlottesville, Virginia, USA
sullivan@virginia.edu
ABSTRACT
Perceptionsystems enableautonomoussystemstointerpretrawsen-
sorreadingsofthephysicalworld.Testingofperceptionsystems
aimstorevealmisinterpretationsthatcouldcausesystemfailures.
Current testing methods, however, are inadequate. The cost of hu-
man interpretation and annotation of real-world input data is high,
somanualtestsuitestendtobesmall.Thesimulation-realitygap
reduces the validity of test results based on simulated worlds. And
methods for synthesizing test inputs do not provide corresponding
expected interpretations. To address these limitations, we devel-
opedğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ , a new approach to fuzz testing of perception
systems based on semantic mutation of test cases that pair real-
world sensor readings with their ground-truth interpretations. We
implemented our approach to assess its feasibility and potentialto improve software testing for perception systems. We used itto generate 150,000 semantically mutated image inputs for five
state-of-the-artperceptionsystems.Wefoundthatitsynthesized
tests with novel and subjectively realistic image inputs, and that it
discoveredinputsthatrevealedsignificantinconsistenciesbetween
thespecifiedandcomputedinterpretations.Wealsofoundthatit
producedsuchtestcasesatacostthatwasverylowcomparedto
that of manual semantic annotation of real-world images.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging;â€¢Computing methodologies â†’Perception ; Vision for
robotics;â€¢Computersystemsorganization â†’Embeddedand
cyber-physical systems.
KEYWORDS
semantic fuzzing, autonomous systems, perception
ACM Reference Format:
Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan. 2022. Semantic Im-
age Fuzzing of AI Perception Systems. In 44th International Conference on
Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510212
1 INTRODUCTION
The perception-implementing layers of software in autonomous
systems (ASs) are responsible for mapping raw sensor inputs to
semantic interpretations that can inform decisions and actions in the
This work is licensed under a Creative Commons Attribution-NonCommercial-
ShareAlike International 4.0 License.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510212
Figure 1: Autonomous System Perception Pipeline
physical world. Figure 1 illustrates a simplified AS pipeline where
the world is sensed through a camera. The resulting image is then
processed by ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘to generate an interpretation that the AS will
use to inform navigation and other control decisions.
Faultyperceptionsystemscanmisinterpretthephysicalworld,
leading to dangerous, even deadly, actions. For example, Tesla has
recently admitted that its ASs sometimes misinterpret parked cars,
and the US National Highway Traffic Safety Administration has
now opened an investigation into almost a dozen instances where
aTesla vehiclehascrashedintoaparkedemergencyvehiclewith
its lights flashing. Those mishaps have resulted in at least one
fatalityandmultipleseriousinjuries[ 3,35].Otherexamplesinclude
miscalculatingtheexistenceorlocationofavehicle,orfailingto
detect people in the planned trajectory of a vehicle [1, 12, 18, 36].
Such mishaps point to fundamental shortcomings in current
methods of testing AS perception software. A key problem is that
thesemachine-learnedcomponentstendtobetrainedusingdata
fromnormaldriving,withfewifanyopportunitiestolearnfrom
rare but safety-critical events. When such events do occur, percep-
tion systems mustaccurately interpret the physical world.
The rarity of safety-critical but infrequent events then entails
that real-world test driving will never be adequate from a test-
ing perspective. A study by the RAND Corporation found that
autonomousvehicleswouldhavetodrivehundredsofmillionsof
milestodemonstratereliability,andthatdoingsowouldtakeexist-ingfleetstensorhundredsofyears[
16].Alongsimilarlines,Waymo
Corporation safety validation methods convey opportunities to
scaleuptestingusingsimulation,butultimatelyconcludethatre-
liance on real-world road driving for validation is required [ 21].
Augmentation of data captured in the real world has emerged as
a potentially viable alternative to reduce the need for real-worlddriving[
10,26,30,37].However,these techniquesoperateatthe
pixel-level,missinganopportunitytotrulyexploreimagesthatare
semantically interesting in challenging the perception pipeline.
What this work proposes is an approach to testing percep-
tion systems using sensor-reading/expected-interpretationtest case pairs derived from real-world AS perception testcases (for which we already have ground truth interpreta-tions), by mutating both sensor readings (e.g. images) andtheirexpectedinterpretationsinacoordinatedmanner
.Our
long-term aim is to generate test cases that focus on rare, safety-
criticalinputsthatinclude,forexample,vehiclesthatarecrashed,
crossing into oncoming traffic, overturned, etc.
19582022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan
(a) Run the sensor in the
real world with human annotations
(b) Use simulation to create new test cases
(c) Low-level Mutation of previous tests
(d) Semantic Mutation of previous tests
(our approach ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ )
Figure 2: Overview of test case generation approaches for perception layers of autonomous systems
Thispapertakesthefirststep towardsourlong-termvisionby
proposingandevaluatinganapproachfornon-guidedtestgener-
ationthatcan,forexample,incorporatevehiclesandpeopleinto
images.Theapproachleveragespriortestcasesasrawmaterials,
mutatingexistingtestinputstoautomaticallyproducerealisticand
novel synthetic sensor data, and deriving interpretations by also
mutating their existing interpretations. Testing with our approach
theninvolvescomparingthesepredictedoutputswiththeactual
interpretation outputs produced by a given system under test (SUT).
Our contributions are as follows:
(1)ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ , a new concept in testing of perception soft-
ware for safety-critical autonomous systems, based on se-
manticmutationsappliedto(sensorreading,ground-truth
interpretation) test case pairs.
(2)ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ ,ademonstrationsystemtargetingcamera-based
autonomous vehicle perception.
(3)Experiments using our approach to test five state-of-the-art
perception systems, with results showing that our approach
canproducerealisticinputsandoveralltestcasepairsthat
reveal problematical perception errors in these systems.
2 MOTIVATION AND RELATED WORK
Basic definitions. Atest case,ğ‘¡=(ğ‘Ÿ,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)for the perception
systemğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘of an AS is a set of input sensor readings, ğ‘Ÿ, paired
with a valid output interpretation, ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘, forğ‘Ÿ. Figure 3a shows
an example of a single sensor reading ğ‘Ÿfor an autonomous vehicle
perceptionsystemandbelowit,Figure3fshowsthecorresponding
ground-truth interpretation ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘. The systemâ€™s performance is
thenjudgedbasedonanoraclethatcompares ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘(ğ‘Ÿ)andğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘.
Conformity. ASs operate in the physical world and thus a sen-
sor reading ğ‘Ÿused in a test case ğ‘¡must conform to the constraints
oftherealworld.If ğ‘Ÿcouldhavebeenacquiredfrom(andisthus
sufficiently realistic with respect to) some configuration of the real
world,ğ‘¤,wesaythat ğ‘Ÿconforms withğ‘¤.Thisnotionisimportant
as a failing ğ‘¡with nonconforming ğ‘Ÿis likely a false-positiveâ€”a
failure that would not occur in the real world.
Figure 2 outlines three common existing procedures to generate
asuiteof ğ‘¡andourapproach ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ ,whichwenowexplore.
2.1 Testing in the Real World
One common procedure, outlined in Figure 2a, is to operate the
AS in the physical world while recording the sensor readings. This
procedureisadvantageousinthatitcangeneratealargedatasetreflecting typical operating conditions. However, it is limited in
thatproducingspecificdesiredconditions(e.g.,haveacarsuddenly
turn in front of our fast moving vehicle) in the real world can be
impractical, dangerous, and expensive [ 21]. Crafting the ground-
truthinterpretationalsoincursgreatexpense,requiringahumanto
manually annotate the sensor readings. In prior studies, producing
high-qualityannotationsofcameraimagesforadrivingbenchmark
requiredonaverageover90minutesofhumaneffortperimage[ 7].
2.2 Testing in Simulation
Anothercommonprocedure,showninFigure2b,usesamodelof
the world embedded in a simulator to replace operating in the real
world.Usingthesimulatorâ€™smodeloftheworldtoautomatically
producesensor readingsand ground-truthinterpretations enables
constructing arbitrary simulated worlds at a lower cost [ 27,31,38].
However, this approach suffers from the simulation-reality fidelity
gap asğ‘¤â‰ ğ‘¤ğ‘ ğ‘–ğ‘š[15]. This can diminish the value of the tests as
the results may be simulation-specific [ 40], which is why they are
oftenrevalidatedinthephysicalworld[ 21].Thatis,thesetestsmay
not conform to any configuration of the real world.
2.3 Generating Tests with Low-level Mutations
A third procedure mutates collected sensor readings to produce
(ğ‘Ÿ,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)andisillustratedinFigure2c.Naivetechniquesfollow
in the vein of standard data augmentation techniques, performing
global mutations such as affine transforms or adding noise [ 10,
26,42].Forexample,Figure3banditsinterpretationinFigure3g
showhorizontallymirroringtheimageanditsinterpretation,while
Figures3cand3hshowtheuseofamaskthatobscurespartsofthe
image.Moreadvancedstrategiessuchasaddingweather,shown
inFigures3dand3i,aredomain-specificbutstillgloballyapplied
acrossallpixels[ 30,37,42].Mutation-basedprocedurescanquickly
generatemanyteststhat,ifthemutationsaredesignedcarefully,
canalsobeconforming.Thatismuchlesslikely,forexample,for
theboxadditionsin3cthanforthefogadditionin3d.Furthermore,
to automatically provide an oracle, this kind of procedure tends to
notsemantically affecttheimage. Forexample, addingfogshould
not affect the entities identified in the imageâ€”the interpretation of
Figure3iisthesameastheoriginalinFigure3f.Additionally,recent
work has examined ways to use mutation strategies to generate
adversarial changes to the scene that do not change the semantics
but are likely to cause changes in the ASâ€™s perception [ 2,17,43].
When these mutations do affect the image semantically, like in 3c,
1959Semantic Image Fuzzing of AI Perception Systems ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(a) Original Image
 (b) Affine Edit
 (c) Coarse Edits
 (d) Global Edits (Weather)
 (e) Our Edits (Car Added)
(f) Original interp
 (g) Affine Edit Interp.
 (h) Coarse Edits Interp.
 (i) Global Edits Interp.
 (j) Our Interp. (Car Added)
Figure 3: Overview of image mutation techniques (best viewed on a screen)
the interpretation of those changes is unknown as shown in 3h
where several people are cut off.
2.4 Generating Tests with Semantic Mutations
Inthisworkweenvisionamoresophisticatedkindofmutationthat
incorporatesworldsemanticsandiscognizantofwhatthereadings
mean (i.e., what the pixels in an image actually represent in the
world).Suchamutationcould,forexample,addacardrivingonthe
streetasshowninFigure3ewithacorrespondinginterpretationin
Figure 3j. We believe and later show how such semantic mutations
havethepotentialtocreatetestcasesthatareconformingbutdo
notoccurinexistingtestsuitesduetotheiruncommonoccurrences
in the physical world.
Figure 2d shows how our proposed approach, ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ , dif-
fers from prior mutation techniques in the semantic nature of
the mutations. Performingthesemutationsrequiresthatmu-
tatedsensorreadingscontinuetoconformtotheconstraints
oftherealworld ,leadingtoseveralchallenges.Ontheinterpreta-
tion side, we must craft domain-specific rules to validate potential
mutations and determine what types of mutations are likely to
yield perception failures, e.g. adding a pedestrian to a roadway.
Furthermore,wemustincreaseconformitylikelihoodbycrafting
preconditions based on the sensor modality. For example, for cam-
era images, this may include not just that an entity is in a viable
position,but alsothat theperspective, lighting,and shadows are
conforming. Sensor reading mutations must be associated with
interpretation mutations that will serve as an oracle. Section 3
describes how the approach tackles these challenges.
3 APPROACH
We describe our approach for using semantic mutations to test AS
perceptionsystemsregardlessofsensormodality.Weformalizethe
problemdefinition,presentthegeneralapproach,andinstantiate
theapproachforcamera-basedperceptionsystemsforautonomous
vehicles.
3.1 Problem definition
Asemantic mutation ,ğ›¿=(ğ›¿ğ‘Ÿ,ğ›¿ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘),transforms a test case,
ğ‘¡=(ğ‘Ÿ,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘),whereğ‘Ÿconformswith someconfigurationof the
world,ğ‘¤,intoanewtestcase, ğ‘¡/prime=ğ›¿(ğ‘¡)=(ğ›¿ğ‘Ÿ(ğ‘Ÿ),ğ›¿ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘(ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘))=
(ğ‘Ÿ/prime,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘/prime), whereğ‘Ÿ/primealso conforms with some world configura-
tion,ğ‘¤/prime,andğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘/primeisavalidinterpretationof ğ‘Ÿ/prime.Forexample,amutation, ğ›¿ğ‘ğ‘‘ğ‘‘ğ¶ğ‘ğ‘Ÿmay add a car to an image while mutating its
interpretation to indicate that a car is now there.
We say that ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘passesa mutated test case ğ›¿(ğ‘¡), if and only
ifğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘(ğ›¿ğ‘Ÿ(ğ‘Ÿ))=ğ›¿ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘(ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘), highlighting the symmetry of
themutationfunctions.Figure4illustratesthisprocess.Inpractice,
onemightneedto(andinourexperimentswedo)useapproximate
equality: ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘(ğ›¿ğ‘Ÿ(ğ‘Ÿ))âˆ’ğ›¿ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘(ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)<ğœ–forasuitablysmall
ğœ–. At a conceptual level, our approach thus constitutes a form of
metamorphictesting[ 32]whereexistingtestcasesareconverted
into new ones in which the correct outputs for transformed inputs
are deduced using knowledge of properties of the physical world.
ğ‘¡â§âªâªâªâªâªâª â¨
âªâªâªâªâªâªâ©ğ‘Ÿğ›¿ğ‘Ÿâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’ â†’ğ‘Ÿ/prime
Annotationâââ /arrowbtâââ /arrowbtğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘
ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ğ›¿ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’ â†’ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘/prime?=ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘(ğ‘Ÿ/prime)â«âªâªâªâªâªâª â¬
âªâªâªâªâªâªâ­ğ‘¡/prime
Figure4:SemanticMutationofTestsforPerceptionSystems
3.2 Semantic Mutations with ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§
A key insight driving ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ is that prior captured data sets
can be leveraged as a source of initial test cases and of resources to
designsemanticmutationsthataremorelikelytoresultinconform-
ing tests. That insight guides the scope of mutations considered in
any givendomain, includingmeans toensure reasonableconfor-
mity of mutated inputs with possible real worlds.
First,wescopethespaceofallmutations Î”basedontheentities
that appear in the ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘s of the available tests. For example, if the
interpretationsmarkcarsandpeople,weonlyallowmutationsthat
add, remove, or change cars and people. We argue that since those
arethecriteriabywhichexistingtestsarejudged,itissensibleto
focus just on those to try to find unexpected ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘behaviors.
Second, weassociate aset of preconditions ğ‘ƒğ‘Ÿğ‘’ğ‘with eachmu-
tationğ›¿, specified in terms of the ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘that defines whether ğ›¿
isapplicabletoagiven ğ‘¡.Ifğ‘ƒğ‘Ÿğ‘’ğ‘(ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)isnotsatisfied,then ğ›¿is
not applicable to that test. For example, for a mutation that adds
a car to an image, ğ‘ƒğ‘Ÿğ‘’ğ‘may be the existence of a road in ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘;
for changing the color of a car, ğ‘ƒğ‘Ÿğ‘’ğ‘would be the existence of at
least one car in ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘. The idea is to leverage an existing interpre-
tation to determine mutation applicability. This is a key distinction
1960ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan
compared to other strategies that allows for our semantic muta-
tionswhilerespectingconformity.Insteadofcraftingmutationsfor
sensorinputsandpropagatingthemutationtotheinterpretation
without respect for changing semantics, we craft our mutations for
the interpretation and then propagate to the sensor domain.
Third,weenabletheparameterizationof ğ›¿ğ‘Ÿtoemployrealsensor
datatoincreasethelikelihoodofconformity.Forexample,when
addingacarto ğ‘¡ğ‘–=(ğ‘Ÿğ‘–,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘–),ifthereexistsa ğ‘¡ğ‘—=(ğ‘Ÿğ‘—,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘—)
that contains a car constrained by a similar context as per their
ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘s, thenğ›¿ğ‘ğ‘‘ğ‘‘ğ¶ğ‘ğ‘Ÿğ‘Ÿcould put the car from ğ‘Ÿğ‘—intoğ‘Ÿğ‘–. In the case
ofimages,weusetheentitypose,perspective,lighting,andadja-
cententitiesintheinterpretationtodefinethecontext.Notethat
context is sensor modality and domain dependent, and that the
richness of the resource set will affect the diversity of generated
testcases.Additionally,weexploretheintegrationofdiscriminators
to determine conformity. We discuss this further in Section 4.4.4.
Allofthemechanismsinourapproachmakeaconscioustrade
off between conformity and a smaller space of available mutations.
3.3 Semantic Mutation of Images by ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§
To further explore our approach, we now focus onthe perception
systemofanautonomousvehiclethatcontainsasinglefront-facing
cameraforsensing,andweconcretize ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ tothisspecific
sensormodality.Werefertothisspecificapplicationas ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ .
AsillustratedinFigure3,forthesesystemsthesensorinputconsists
ofasinglecameraimageandtheoutputconsistsofaninterpretation
of the world in the form of a per-pixel semantic annotation.
ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ requiresadatasetofreal-worldcameradatatobuild
its resource set to serve as the basis of its mutations. Camera based
perceptionsystemsarewidelystudied,withseveralavailablebench-
marks [5,7,11] consisting of thousands of test cases of image and
interpretation pairs. Each benchmark targets a different level of
precision which affects the strength of the resource set.
We now explore the space of semantic mutations, Î”, for the
domainofcamerasystemsforautonomousvehicles.Wefirstdefine
thesemanticentitytypes thatcanappearinimages,e.g.car,bicyclist,
etc. Each such entity has a state, e.g., orientation, color, position,
lighting,etc.Finallywedefineasetofsemanticactions:e.g.,add
entity, remove entity, change entity state. The space of mutations,
Î”,is then the set of actions, each parameterized by an entity and a
state, e.g. add (action) a car (entity) at location ğ‘™(state).
In practice, semantically mutating images can be difficult due
totheconformityrequirement.Forexample,addingacartoalo-
cation in an image requires verifying that a car can exist in that
location: e.g., the car cannot intersect another car. Removing an
entity requires generating conforming image data with a known
interpretation for the vacated space, a process known as semantic
inpainting [ 20,39]. Changing the pose of an entity requires render-
ing the entity in a conforming manner from a different perspective
than it appeared in the original test case.
4 IMPLEMENTATION FOR THE
AUTONOMOUS VEHICLE DOMAIN
Tostudytheapplicabilityof ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ ,wecreatedanextensible
pipelineinPythonforfuzzingperceptionsystemsofautonomousvehicles with a single front-facing camera, ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ , that imple-
mentsthreemutations:changingthecolorofcars,addingcars,and
adding pedestrians1. We begin with the system architecture and
then discuss detailed implementations for the chosen mutations.
4.1ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ Architecture
Figure5outlines ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ â€™scomponentsandflow.Givenamu-
tationğ›¿,ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ selectsatestcase ğ‘¡andqueriestheresource
set for viable resources to perform ğ›¿. It then selects a resource and
checks whether mutatingusing ğ‘¡,ğ›¿, and theselected resource will
generateaconformingtest.Ifso,itappliesthemutationtogenerate
ğ‘¡/prime, otherwise it selects another resource and repeats.
ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ requires a prior data set as a basis for mutation.
In the autonomous vehicle domain, there are several such data
sets, includingKITTI[ 11], nuScenes[ 5], andCityscapes [ 7]. Each
provides(ğ‘Ÿ,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)testcaseswhere ğ‘Ÿisacameraimageand ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘
isaper-pixelannotationthatprovidesaground-truthinterpretation
basedonthecategoriesprovidedinthedataset.Asapreprocessing
step,ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ parses the prior test set to build a resource set of
available data, e.g. a list of all cars and their poses.
Eachdatasethasadifferentlistoftrackedcategorieswithdiffer-
inglevelsofprecisionanddesignchoicesforhowtotreatcertain
entities.Forexample,nuSceneshas7labelsforhumansdifferentiat-
ing between, e.g. adults and children, while Cityscapes has 2 labels
that distinguish between a pedestrian and a person riding a bike or
motorcycle. We developed ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ using the Cityscapes data
setduetoitspopularityinrelatedliterature,thewidenumberof
perception systems that have been developed to target the data set,
and the availability of open sourced tools to evaluate performance.
Wenote,however,thatthedesignchoicesandavailabledataina
data set influence the richness and precision of ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ â€™s re-
source set and thus the mutations available. For example, when
we aim to develop a mutation for adding a vehicle that has been
involved in a crash to a test case, under the theory that a deformed
vehicle would be more difficult to perceive, the data setâ€™s lack of
thislabelwillprevent ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ fromleveragingsuchamutation.
WeimplementedeachmutationusingNumpy[ 13]alongwith
OpenCVforPython[ 4]andPillow[ 6],twocommonimageprocess-
inglibrariesforPython.Wecreatedtheframeworkinamodular
fashion to easily incorporate additional mutations or data sets.
Forourfirstimplementationof ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ ,wechosetoexplore
twomutations:changingthecolorsofentitieswithinthesceneand
addingentitiestothescene.Wechosethesemutationsforinitial
study ofğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ due to our ability to perform those mutations
using available image manipulation techniques. Although these
mutations are relatively simple, we conjecture that if the simple
mutations yield interesting test cases, then this will encourage
further research to enable more advanced techniques which can
then be incorporated. We describe the implementation details of
each mutation in the following sections.
4.2 Changing Object Color
Thesimplestmutationweimplementedchangesthecolorofasingle
entity in the test case and was designed with the goal of changing
1Codeandhigh-levelalgorithmdescriptionsareavailableathttps://github.com/less-
lab-uva/perception_fuzzing
1961Semantic Image Fuzzing of AI Perception Systems ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 5: Pipeline for Semantic Mutation with ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§
(a) Original Car
 (b) Mutated Car
Figure 6: Applying color change mutation
(best viewed on a screen)
the color of vehicles to mirror the physical parallel of repainting a
vehicle.Inthecontextofthe SUTsweareexamining,thecolorof
the vehicle should not change the target perception category, and
soğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘/prime=ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘. Thus, we examine how to produce ğ‘Ÿ/primefromğ‘Ÿ.
4.2.1 Implementation. For this mutation, the test selection step
chooses a test case containing a car as the base test ğ‘¡for mutation,
andneedsnoadditionalresources.Toaffectthecolorchange,we
manipulatethecolorintheHSLcolorspace,whichseparatesthe
components into hue, saturation, and lightness in a way that is
similar tohuman perception ofcolor [ 14]. Atthe most basiclevel,
thismutationperformsahueshiftontheentity,changingittoa
different color. However, this leads to two concerns. First, if we
only want to change the color of the vehicleâ€™s paint, how do we
prevent changing the color of the vehicleâ€™s windows? In this color
model, high lightness corresponds to white and low to black. This
meansthatsincemostwindowsappeardarkinimages,theywill
beunaffectedbyahueshift.Thisleadstothesecondissue:altering
the color of white vehicles. To facilitate this, ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ checks
for high lightness pixels and decreases the lightness and increases
the saturation so that the hue shift applied to the entire vehicle
produces another color. Specifically, if the average lightness of the
vehicleisover100,thenallpixelswithalightnessvalueover100
have their lightness decreased by a random amount between 20
and 50. Then, to prevent the colors from appearing faded, pixels
withalightnessover100andasaturationlessthan50havetheir
saturation increased. Figure 6 showcases an example application of
this mutation changing the carâ€™s color from red to blue. As shown
in Figure 6, the change object color mutation can render results
that appear very realistic.
4.2.2 Potential for False Positives. As with all mutations, this oper-
ationmustensurethat ğ‘Ÿ/primeconformstosomeworld ğ‘¤/prime.Ingeneral,
sinceğ‘Ÿconformedtosomeworld ğ‘¤,thenthereexistsa ğ‘¤/primewiththe
(a) Original Car
 (b) Mutated Car
Figure 7: Nonconforming color change mutation
(best viewed on a screen)
vehiclepaintedthenewcolor.However,thecolorshiftcaninadver-
tentlyaffectotherpartsofthevehicleandmayleadtofalsepositive
test cases in which there is no such ğ‘¤/prime. The very observant reader
may have identified in Figure 6 that not only does the car body
appear repainted blue but so do the brake lights. In the physical
world, it is possible for a car to have blue brake lights; however,
thisislikelynotplausibleduetoregulationsgoverningthecolor
ofthebrakelight.Moreadvancedversionsofthismutationcould
considerrefinementsinthisarea.Anotherissue,showninFigure7,
occurswhenthereishighglareonthevehicle,causingpixelated
distortions and thus not conforming with any world ğ‘¤/prime.
4.3 Adding an Entity
Addingentitiestoscenesisoneoftheadvancesof ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ .The
goalofthismutationistoaddanentitytoasceneinawaythatmay
impacttheperceptionsystemsuchasaddingavehicleorpedestrian.
Adding an entity to the sensor input produces a corresponding
addition in the interpretation, and allows us to examine the SUTâ€™s
performance at the semantic level.
4.3.1 Implementation. For this mutation, the test selection step
choosesanytestcontainingaroadasthebasetest ğ‘¡formutation.
Theresourcequerystepselectsallpotentialentitiesofthespecified
class,e.g.allcars,andfiltersoutthosethatareoccludedbyother
entities.Toincreasethelikelihoodthataddingtheentityaffectsthe
SUT,vehiclesmusthaveaboundingrectanglethatisatleast100
pixelsonitslongestside,or50pixelsforpedestrians. ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§
then randomly selects an entity and checks if adding that entity
toğ‘¡wouldresultinaconformingtestbycheckingforcompatible
perspective,semantics,andlightingbetweentheentityand ğ‘¡.These
are nontrivial conformity checks, as we explore below.
Theresourcesetprovidesacollectionofimageswiththepixel
boundariesgivingthelocationandoutlineofisolatedentities.When
1962ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan
(a) Source Image
 (b) Base Image (ğ‘Ÿ)
(c) Isolated Entity in
Resources
(d) Mutated Image with
Car Added (ğ‘Ÿ/prime)
Figure 8: Applying add car mutation
(best viewed on a screen)
adding the entity, the mutation takes the isolated portion of the
imageandoverlaysit atthesamepixelcoordinates onthebaseimage.
Figure 8 demonstrates this process.
Maintainingaconsistentperspectiveisonekeytoconformity.
If done improperly, the added entity will appear out of proportion
and misaligned with other image features. To this end, we add
theconstraintthattheentitymustbeplacedatthesamerelative
physical coordinates to the camera in the new test case as it was in
the source test case. If we want to add a car with pose ğ‘relative to
the camera from ğ‘¡ğ‘ to test case ğ‘¡to generate ğ‘¡/prime, the resulting ğ‘¡/primewill
consistof ğ‘¡withthecarfrom ğ‘¡ğ‘ addedatpose ğ‘relativetotheits
camera.Retaining thesame poseincreases thelikelihoodthat the
added car will appear in the correct perspective in ğ‘¡/prime.
However,addingtheentityatthesamepixelcoordinatesdoes
not guarantee that the entity is added at the same physical co-
ordinates relative to the camera. The relationship between pixel
coordinatesandphysicalcoordinatesdependsonthecharacteris-
tics of the camera, the vanishing point of the scene, and the size
of the object. We assume that the entire resource set consists of
images taken with cameras that have consistent characteristics.
Additionally, since the entity is always added so that it occupies
thesameregionofpixels,ithasthesamesize.Thus,ifthesource
and destination images have the same vanishing point, they will
be compatible because adding at the same pixel coordinates will
resultinthesamephysicalcoordinatesrelativetothecamera.How-
ever, finding pixel-perfect matches on the vanishing point between
images is extremely unlikely due to potential number of scenes. In-
stead,wedividetheimagesintoquadrantsandconsidertwoimages
to be compatible if their vanishing points are in the same quadrant.
Increasingtheprecisionwhenmatchingvanishingpointsincreases
thelikelihoodthattheperspectivewillbeadequatelymaintained,
butatthecostofallowingfewerpossiblemutationsandmoreex-
pense in searching for compatible images during mutation. We
utilize an implementation [ 41] of an algorithm that uses the in-
tersection of Hough lines to determine which region of the image
likelycontainsthevanishingpoint[ 22].Iftheimagethattheentity
issourcedfromand ğ‘¡anddonothavecompatiblevanishingpoints,
it fails the conformity check.
The new location of the entity must be physically feasible in
ordertoconformtoaworld ğ‘¤/prime.Tovalidatethis, ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ checks
if the lower left corner of the entityâ€™s bounding box is on the road.Lighting also plays an important role in the conformity of a
mutatedimage,especiallythebrightness.Forexample,acarfrom
an image with bright sunlight cannot be added in the shade. To
addressthis, ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ takestheregionofpixelsinthebaseimage
that the entity would be placed over and the pixels of the entity
and converts them to the HSV (hue, saturation, value) color space.
In HSV, the value corresponds to the brightness of the pixels. To
ensure that the lighting conditions are similar, if the entity does
not have a median value within 5 units ( âˆ¼2%) of the median value
of the base image target area, then it fails the conformity check.
Onceğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ chooses a base image and entity to add, the
images are combined into a new test image. Similarly, the interpre-
tationofthebaseimageiseditedtoincludetheproperclassification
oftheentityatitsposition.ThisisshowninFigures3eand3j,where
the car has been added both in the image and the interpretation.
4.3.2 Potential for False Positives. The previous techniques im-
prove the likelihood to generate images with conforming perspec-
tive and brightness, but as seen in Figure 9 there are several condi-
tions that can lead to false positives.
Whileğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ takesstepstoincreasethelikelihoodofmatch-
ing consistent perspectives, this does not always happen. Figure
9a shows an example of a car that has been added to an image
withaperspectivemismatch,causingtheaddedcartoappearmuch
smaller than it should compared to the entities around it. Another
issue related to entity placement is overlap. Figure 9b shows an
instance where a car has been added on top of a person walking
their dog. At first it seems that this could have been avoided by
checkingforoverlappingpixels.However,thiswouldoverlycon-
strainthemutationandp reventmanyinteresting testcasesbecause
pixel-level occlusion does not imply that the entities overlap in the
physical world. Determining overlap involves reasoning about the
physicalsizesofentitiesandtheirdistancefromthecamera,which
future work could explore.
Anotherconsiderationisconsistentlighting.Inmostautonomous
vehicledrivingscenes,thepositionoftheSunalongwithanyocclu-
sions like cloud coverdetermine entity brightness and reflections.
Fortheimagetoconformtoaconfigurationoftherealworld,these
effects must be consistent with a single lighting configuration. For
example,addingacarwithabrightreflectiontoacloudyimagewill
result in a nonconforming image as shown in Figure 9c. Further, in
mostlightingconditionsallentitiesinascenewillcastashadow.
Determining the characteristics of the shadows requires a detailed
model of the scene with information about the physical location of
all entities and light sources. This level of data is not present in the
resource set derived from Cityscapes, andso ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ does not
attempttoaddashadowwhenaddinganentity.Thiscanleadto
nonconforming images, as shown in Figure 9d.
4.4 Challenges and Vision for Other Mutations
Currently, ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ providesonlytheaforementionedmutations
relatedtoentityrecoloringandadditionandthuslacksmutationsin
two key areas: entity repositioning and removal. We now examine
wherethestate-of-the-artincomputergraphicsandvisionislimited
to support such advanced mutations, and we comment on our
preliminary prototypes and results.
1963Semantic Image Fuzzing of AI Perception Systems ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(a) Incorrect Perspective
 (b) Overlapping Entities
(c) Inconsistent lighting
 (d) Missing shadow
Figure 9: Nonconforming add car mutations
(best viewed on a screen)
Renderingarealisticimageofascenefromaspecificperspective,
is a widely studied area of computer graphics, encompassing all
mannerofimagesynthesis.However,thegoalofourmutationsisto
renderarealisticimageofasceneusingreal-worlddata.Todosowe
need a technique to convert a scene into a rendered image that: (1)
is sufficiently novel from prior data, (2) has a known interpretation
and (3) conforms to a world ğ‘¤. We find that although state-of-the-
arttechniqueshavemadestridesinalloftheseareas,thereisno
technique that meets all three criteria.
4.4.1 Repositioning Entities. Recent machine learning research at-
tempts to render novel scene compositions distinct from that of
sampled data. Originally used to render the same entity from a
different perspective [ 23], the current state-of-the-art can remove
orslightlyrepositionentitiesinthescene[ 28].Whilethisapproach
can render very realistic and conforming images with known in-
terpretationusingpriordata,itislimitedinhowâ€œfarawayâ€from
the original scene it can operate. Entities can only be removed if
they did not occupy that space in a different video frame and repo-
sitioningis limitedtoa fewmetersand degreesdifferent thanthe
original scene. While this area is promising, it currently does allow
for sufficient variation from prior data to use in ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ .
4.4.2 RemovingEntities. Anotherrelevantthreadofgraphicsre-
search is the problem of inpainting, the task of filling in regions of
animagesuchthatthenewimageisconforming[ 20,39].Forexam-
ple,removingacarfromthescenecanbeachievedbyinpainting
over the region of the car with empty road. Although inpainting
has potential to produce conforming images for use in mutation,
in practice such tools satisfy neither our interpretation nor confor-
mityrequirements.Theinterpretationoftheinpaintedregionisnot
knownandalthoughthesystemistrainedonrealdata,itsability
toproduceconformingimagesdecreasesastheinpaintedregion
grows.Wedevelopedaprototypeusing[ 39]toperformamutation
toremoveacarfromanimage.Figure10showsanexampleoutput
of using inpainting to remove acar from an image taken from the
nuScenes [ 5] data set. As shown in Figure 10b, although the car
itselfisnolongervisible,theregionexhibitsseveralnonconforming
distortions and lighting effects which hint at the prior existence of
thecar.Advancementsin inpaintingtomakethe proceduremore
robust would greatly expand the space of feasible mutations.
(a) Original Car
 (b) Car Inpainted
Figure 10: Inpainting to Remove Entities
(best viewed on a screen)
4.4.3 DirectImageSynthesis. Directimagesynthesissystemswork
as the inverse to the perception system of an AS, taking in an
annotatedinterpretationofanimageandrenderingacameraim-
age that is consistent with that interpretation [ 29]. This would
remove the need to perform mutations on the sensor inputs, allow-
ingğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ tomutatetheinterpretationdirectlyandusedirect
image synthesis to create a matching sensor reading. This process
canrichlycombinedatatoproducenovelscenes,butcurrentim-
plementationsstill donotsatisfy theinterpretationandconformity
requirements. While interpretation is considered during synthesis,
someregionsareleftunconstrainedandaredelegatedtoinpainting,
suffering from the shortcomings outlined above.
4.4.4 Discriminators for Conformity Checking. In prior sections,
wedescribethepossibilitiesforfalsepositivesarisingfromnoncon-
forming test cases. Machine learning discriminators present a way
toidentifysuchnonconformingtests[ 25].Thesediscriminatorsare
aformofbinaryclassifierthatseektodetermineifaninputbelongs
to a distribution, so they could preemptively reject mutations that
are nonconforming with the real world defined by a training set.
Weexploredthisapproach,trainingabinaryclassifierbasedona
CNNusingover48,000imagestakenfromboththeCityscapesdata
set and those generated by ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ . The discriminator learned
to differentiate between the classes, but could not differentiate
conforming versus nonconforming images. In part, the problem is
that the generated images contain conforming and nonconforming
images,butchecking24,000ofthemwasprohibitivelyexpensive.
Moregenerally,abroaderchallengeisthatdiscriminatorstendto
require much larger data sets even for much smaller images. We
suspect that the discriminator requires much more training data to
targetaresolutionsuitableforASs,whichisnotfeasibleduetothe
cost to obtain real-world data. Future research in this direction has
the potential to reduce the false positive rate of ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ .
5 DISCUSSION OF TRADE-OFFS
ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ provides a framework for testing AS perception sys-
tems distinct from the prior approaches described in Section 2. We
noteherethatthesepriorapproachesarenotsuitablefordirectcom-
parisonduetothedifferinggoalsandintendedoutputs.Simulation,
low-levelmutation,and ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ providedifferenttrade-offs
intheirabilitytogeneratenoveloutputs,costtogeneratetests,and
conformity of outputs. We now discuss those trade-offs.
In terms of ability to generate novel inputs, simulation provides
the highest level of utility, allowing for practically unlimited en-
vironmentgeneration.Attheotherend,low-levelmutationtech-
niques provide a narrow range of possible test cases based on prior
data and the global mutation strategies employed, being limited
1964ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan
inthenoveltyofthemutateddatacomparedtotheoriginaldata.
ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ builds from the low-level mutation strategy in terms
ofutility,providingnewdimensions forthenoveltyof themutated
data. Although still limited by the availability of prior data, the
richer semantic mutation strategies allow for the generation of
more meaningful novel test cases that contain a substantive and
human-understandable semantic change.
Although simulation provides the highest level of utility for
generatingnovel inputs,itsefficiency ismore nuanced.Simulated
environments may be re-used between testing different systems,
butbuildinganewenvironmentcomesatahighcostintermsof
timeandexpertiserequired.Onceanenvironmentisinplacefor
testing, adapting the environment can be achieved at a lower cost.
For example, given an existing scenario, the simulator CARLA [ 8]
providesanautomatedwaytovaryweatherconditions.Forboth
low-level mutation and ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ , there is an initial cost to col-
lect the baseline data to mutate; however, readily available data
sets [5,7,11] can ameliorate these costs to the end-user. Using
available data, the low-level mutation strategies are likely the most
efficient in producing new test cases. Given the additional machin-
ery thatğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ uses to leverage multiple facets of the prior
datawhilealsoperformingconformitychecks,thecomputational
cost is likely significantly higher than the low-level approaches.
AsdiscussedinSection2,oneofthekeylimitationsofsimulation
testingisthesimulation-realityfidelitygapthatcandiminishthe
applicability of the tests and require revalidation in the physical
world as they may not be conforming with any real world [ 15,
21,40]. The mutation-based strategies are designed to avoid this
limitation by using real-world data as the baseline and ensuring
thatthemutationspreservetheconformityoftheoriginaldata.We
note that ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ specifically identifies this need to preserve
conformity as a fundamental part of the framework.
ASsarecomplexsystemsthatrequireavariedcomplementof
testingtechniques.ThevalidationprocessforanyASshouldinclude
multipletypesoftesting. ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ providesavaluableaddition
tothevalidationtoolboxbyprovidingsemanticmutationsabsent
in the low-level mutation strategies, while maintaining conformity
to ensure it avoids the pitfall of the simulation-reality gap.
6 EVALUATION
We have developed ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ , a test generation tool that can
efficientlyidentifyperformanceinconsistenciesinASperception
systems using semantic mutations. To evaluate the potential of our
approach, ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ ,in thesedimensions weseek toanswerthe
following research questions:
RQ1.How effective is our technique in uncovering inconsisten-
cies defined at different levels of severity?
RQ2.Howefficientisourtechniqueintermsofthetimetaken
to generate the mutation and to detect the inconsistencies?
RQ3.Which mutations are the most effective?
6.1 SUTs
To assess ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ , we evaluate it on five highly competitive
perception systems submitted to the Cityscapes benchmark for the
â€œPixel-Level Semantic Labeling Taskâ€ [ 7]. These SUTs were the five
highestperformingSUTsforwhichcodeandpre-trainedmodelsTable 1: Cityscapes benchmark scores for SUTs evaluated
Rank YearSUT IoU
Overall With Code
5 12020NVIDIA SemSeg [34]285.4
17 32021EfficientPS [24]384.2
23 62020DecoupleSegNet [19]483.7
26 72018SDCNet [44]583.5
29 92019HRNetV2+OCR [33]683.3
were publiclyavailable. Each projectwas forked fromthe original
repository,editedifnecessarytoprovideaconsistentoutputformat,
andpackaged torunin aDocker containerforreplicability.Table1
listsSUTsrankedbytheirperformance(overallandamongtheones
withcode)ontheoriginalCityscapesdatasetasperthedefaultIoU
Class metric. Intuitively, this is the percentage of correctly labeled
pixels,withaminimumscoreof0andamaximumof100.Pixelsthat
do not belong to a class are marked as â€œdo not careâ€ (like the hood
of the ego vehicle) meaning that they are excluded from scoring
regardless of the SUT assigned label.
6.2 Tests Generated
WeusedtheCityscapesdatasetasthebasisforourtesting,serving
as the original test suite and basis for building the resource set.
We pruned testcases thataretoo difficultfor the SUTsto prevent
mutating test cases where the mutation will not be the focus of the
test;iftheSUTstruggleswiththeimageasawhole,themutation
will not provide any additional utility. To perform this filter, we
ran the highest performing SUT on the baseline, as determined by
itsrankingontheCityscapesleaderboard[ 7],ontheoriginaltest
cases in the data setto establish the baseline performance of each
test case. Any test case on which the best SUT performs below the
thresholdisremovedfromconsideration.Forourtesting,wesetthis
minimumscoreparameterat95%resultingintheremoval42ofthe
total 3,475 images in the data set. ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ then performs a one-
timepreprocessinganalysisoftheremainingtestcases,asdescribed
inSection4,tobuildtheresourcesetbyfindingallavailableentities
across the test cases to include in future mutations.
Forevaluation,weraneachSUTon150,000testsgeneratedby
ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ comprised of 50,000 â€œAdd Carâ€ mutations, 50,000 â€œAdd
Personâ€ mutations, and 50,000 â€œChange Car Colorâ€ mutations.
6.3 Metrics
An effective mutation, one that finds a potential SUT failure mode,
rendersaconformingimagethatcausestheSUTtoperformpoorly.
Tocapture thatnotionwe relyonthe Cityscapesbenchmarkeval-
uationtool( ğ‘’ğ‘£ğ‘ğ‘™)whichscoreseachSUTâ€™sperformancebasedon
thepercentageofcorrectlyclassifiedpixels.Weevaluatetheper-
formance of the mutation strategies by calculating the percentage
point (p.p.)difference betweenthe SUTâ€™s scoreon the originalim-
age and its score on the mutated image. Any drop in the SUTâ€™s
2https://github.com/NVIDIA/semantic-segmentation/tree/7726b14
3https://github.com/less-lab-uva/EfficientPS
4https://github.com/less-lab-uva/DecoupleSegNets
5https://github.com/less-lab-uva/semantic-segmentation/tree/sdcnet
6https://github.com/less-lab-uva/HRNet-Semantic-Segmentation
1965Semantic Image Fuzzing of AI Perception Systems ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 11: Inconsistencies found per SUT
score on the mutated image compared to the original image was
induced directly by the mutation itself, allowing us to gauge the
strength of the mutations to find inconsistencies.
Moreformally,foragivenSUT ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘,testcase (ğ‘Ÿ,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘),and
corresponding mutation (ğ‘Ÿ/prime,ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘/prime), the drop is given by
ğ‘‘ğ‘Ÿğ‘œğ‘=ğ‘’ğ‘£ğ‘ğ‘™(ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘(ğ‘Ÿ),ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)âˆ’ğ‘’ğ‘£ğ‘ğ‘™(ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘(ğ‘Ÿ/prime),ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘/prime)
The larger the value, the more error the mutation induced in the
SUT.Wejudgedropsoflessthan1p.p.tobeinthenoiseasmost
tests result in SUTs misclassifying a few pixels around the edges
of objects, leading to small drops compared to the baseline. We
select 1 p.p. as the cut off to reduce the likelihood that we deem an
inconsistency noteworthy when it is not, and as a filter to main-
tain the feasibility to complete the manual reviews we conduct for
evaluation.We categorizedrops between1 and5p.p. asmoderate
inconsistencies, between 5 and 10 as significant, and those greater
than10asextremeinconsistencies.Foradditionalcontext,thehood
of the car visible at the bottom of all images in the Cityscapes data
set (see Fig. 3), occupies roughly 4 .3% of the image; a drop of more
than5p.p.meansanarealargerthanthehoodwasmisclassified.
We note that there is no additional filtering at this step based on
howwelltheSUTperformedontheoriginaltestcase.Forexample,
iftheSUTscoredonly85%ontheoriginaltestandthenscored78%
onthemutatedtest,thenthiswouldresultinadropof7p.p.and
be classified as a significant inconsistency. Future work may exam-
ine usingspecificperformance thresholdsinstead ofperformance
drops, specifically as this technique applies to testing a single SUT.
However, by measuring deterioration of performance, we are able
to use this metric to compare across the various SUTs even though
they show different levels of absolute performance.
Wealsomeasurethetimetoperformeachmutationandthetime
to run the SUTs to assess efficiency.
6.4 Results
6.4.1 RQ 1 Results: Finding Inconsistencies. Figure 11 shows the
counts of inconsistencies found in each category per SUT; note
the log scale on the y-axis. We first remark that ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ foundModerate
1.37 p.p. Drop
(a) Orig. ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘1Significant
5.25 p.p. Drop
(b) Orig. ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘2Extreme
23.3 p.p. Drop
(c) Orig. ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘3
(d) Mutation 1
 (e) Mutation 2
 (f) Mutation 3
(g)ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘1
 (h)ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘2
 (i)ğ‘ƒğ‘’ğ‘Ÿğ‘ğ‘’ğ‘3
Figure 12: Visualizing SUT Inconsistencies across Severities
(best viewed on a screen)
1210 SUT inconsistencies resulting from 884 mutations and that
eachSUThadover100inconsistencies.Further,eachoftheSUTs
exhibited at least one significant inconsistency and 3 of the 5 SUTs
combined to exhibit a total of 20 extreme inconsistencies.
The distribution of inconsistencies among the SUTs is unex-
pected. NVIDIA SemSeg [ 34] and EfficientPS [ 24], the two highest
scoringontheCityscapesbenchmark,hadthehighestnumberof
inconsistencies in all three categories with 211 and 477 respec-
tively. EfficientPS revealed more than three times the number of
inconsistencies of the SUT with the fewest inconsistencies.
For contextualizing the magnitude of the inconsistencies, Figure
12 shows three mutations produced by ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ and their inter-
pretationsbytheEfficientPSSUTbeforeandafterthemutation.The
leftmost column shows a moderate inconsistency; an added person
occludesabus,causingEfficientPStothenclassifythebusasatrain.
Themiddlecolumnshowsasignificantinconsistency;anaddedcar
occludes a train, causing EfficientPS to correctly identify only part
ofthetrain,labelingtherestasâ€œdonotcareâ€.Therightmostcolumn
shows an extreme inconsistency; an added car occludes a truck,
causingEfficientPStomisclassifythetruck,labellingportionsas
â€œdo not careâ€ and the rest as building.
We also assess effectiveness of ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ in terms of incon-
sistencies found over time. Figure 13 shows the number of sig-
nificantandextremeinconsistenciesfoundversusthenumberof
mutated tests executed. While ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ generated 150,000 tests,
thegraph showstheaverage of10permutationsto controlforpa-
rameter selection randomness. Figure 13 shows that the number of
inconsistenciesfoundcontinuestoincreaseevenat150,000tests,
indicatingthatfuzzinghasnotsaturated.Againwenotethatthe
twostrongestSUTsonthebenchmarkyieldsignificantandextreme
inconsistencies at a much higher rate than the other SUTs. Further
analysisofthespecificSUTsisneededtounderstandthefactorsin-
volved in this performance, but these data suggest that the highest
performing SUTs may be more brittle under certain conditions.
Sinceğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ usesrandomtestandresourceselection,we
computed the number of duplicate tests generated as an indication
ofsaturation.Only1228 (0.82%)ofthe150,000testswereduplicates,
1966ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan
Figure 13: Inconsistencies found over time
Table 2: False Positive Rate for Inconsistencies Found
SUT False Positive Rate
[1,5)[5,10)[10,100]
NVIDIA SemSeg [34] 11%67% 50%
EfficientPS [24] 43%47% 53%
DecoupleSegNet [19] 38%75% â€”
SDCNet [44] 42%0% 0%
HRNetV2+OCR [33] 28%67% â€”
suggesting that fuzzing was not near saturation. This further high-
lightstheabilityofourapproachtogenerateordersofmagnitude
moredatathantheoriginaltestsuitewhichcontained3,433tests
suitable for mutation rendering a more than 40-fold increase.
We have shown that ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ can generate test cases that
induce inconsistencies in the SUTs. However, one potential issue is
thepresenceoftestcasesthatarenonconforming,whichleadsto
false inconsistencies. As highlighted in Section 4, there are several
factors that can result in a nonconforming test case, and determin-
ing conformity is subjective. Still, to gain a better grasp on the rate
offalsepositiveswemanuallyinspectallthegeneratedtestcases
that led to 5+ p.p. drop, and sample 10% of the test cases that led
to a drop between 1 and 5 p.p.. The process entailed each author
examiningeachimageandclassifyingthemaseitheratruepositive
or false positive. If any of the three authors deemed an image a
false positive, it was conservatively recorded as such.
Tofurtherconveythequalityofourassessment,Figure14show-
cases 5 true positive and 5 false positive test cases that induced
inconsistencies in our study. The false positive rates are shown in
Table2.Thehighfalsepositiveratereflectsthechallengingapplica-
tion domain to achieve conformity. Still, this high rate is mitigated
bythenumberofinstancesonwhichitapplies.Generating150,000
test cases produces a few hundred tests with inconsistencies of
interest which developers can examine to understand performance
orselectfuturetestingdirections.Inthisexamination,determining
ifanimageisnonconformingtakesafewsecondsoftime,meaning
encounteringafalsepositiveincursarelativelylowcost.Further-
more, the number of inconsistencies found per SUT is small and
can be prioritized by the drop measure.6.4.2 RQ2Results:Efficiency. Weconsiderefficiencybycomparing
thetimefor ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ tomutateatestwiththetimefortheSUTs
to run a test. Table 3 shows the average time to generate each type
of mutation (2nd row) compared to the average time to run each
SUT on those tests (3rd-7th row) in milliseconds, with standard
deviationinparentheses.Foreachtablecell,wegenerated100tests
10 times, and averaged the times across these trials.
The color change mutation was the fastest, taking less than 20%
thetimeoftheaddmutations.Thisisbecausetheconformitycheck
for the color change mutation always passes, so it spends less time
selecting viable resources. The add car mutation was slightly faster
than the addperson mutation. This is likelybecause it is easier to
satisfy the conformity constraintsâ€”it is more likely for a randomly
sampledcartoappearontheroadthanarandomlysampledper-
son, meaning fewer samples are needed to find a suitable car. As
expected,wefoundthatforeachSUTthetimeittakestoexecute
the test is not different based on the mutation. While the fastest
SUT, HRNetV2+OCR [ 33], takes about as much time to execute
a test as ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ does to generate a test, we note that three
of the SUTs take more than double that time, and one, NVIDIA
SemSeg[34],takesmorethan5timesaslong.Further,onceasetof
mutationshavebeengenerated,theycanbeusedinthefutureto
testadditionalrevisionsoftheSUTatnoadditionalcosttoprepare.
Table 3: Average time to generate and execute a test in mil-
liseconds, with the standard deviation in parenthesis.
Activity Add Add Color
Car Person Car
Test Generation 593 (19.9) 642 (42.7) 105 (3.06)
NVIDIA SemSeg [34] 3421 (29.5) 3418 (22.9) 3404 (31.5)
EfficientPS [24] 837 (7.22) 837 (5.88) 837 (7.41)
DecoupleSegNet [19] 1426 (2.24) 1425 (2.60) 1423 (2.48)
SDCNet [44] 1328 (3.09) 1328 (2.81) 1327 (8.33)
HRNetV2+OCR [33] 600 (5.89) 605 (4.23) 605 (2.89)
6.4.3 RQ 3 Results: Mutation Types. In this section we examine
more carefullythe resultsper mutationtype. Figure 15shows the
inconsistenciesfoundforeachSUTbasedonthemutationtype;note
the log scale on the y-axis. The add car mutation induced the most
inconsistencies(877),followedbytheaddpersonmutation(280),
andthenthemutationtochangethecarcolor(53).Theseresults
reflect what we may expect; adding a car affects a large portion of
theimage,leadingtoahigherlikelihoodofthemutationyielding
an inconsistency. H owever, even though recoloring a car affects
the same number of pixels as adding that car to another image,
we find that the SUTs are robust against changing the color of the
car. This suggests that editing large regions of the image, even in a
conformingmanner,isinsufficient.Thisfurthersupportsournotion
thathighlevelsemanticmutationssuchasaddingacararerequired
to exercise these perception systems and find inconsistencies.
6.5 Threats to Validity
The external validity of our findings for ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ are affected
byourchoiceofdatasetandSUTs.WeselectedCityscapesforits
1967Semantic Image Fuzzing of AI Perception Systems ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(a) True Positive 1
 (b) True Positive 2
 (c) True Positive 3
 (d) True Positive 4
 (e) True Positive 5
(f) False Positive 1
(Overlapping)
(g) False Positive 2
(Lighting)
(h) False Positive 3
(Incorrect Coloring)
(i) False Positive 4
(Perspective & Overlap)
(j) False Positive 5
(Incomplete Entity)
Figure 14: Sample of True and False Positives (best viewed on a screen)
Figure 15: Inconsistencies by mutation type
popularity as a benchmark of perception systems, and for SUTs we
selectedthetopfivethatmadetheirsourceavailableforreuseinthe
Cityscapescompetition.Extendingthescopetodatasetssuchas
nuScenes[ 5],KITTI[ 11],andWaymo[ 9]wouldhelpgeneralizethe
findings.Morebroadly,theproposedapproachismoregeneralthan
ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ , being applicable to other sensor data such as the com-
monplace LIDAR (light detection and ranging) for remote sensing.
This level of generalization remains to be tested empirically.
The internal validity of our findings may be affected by several
factors.Topamongthemistheimplementationofthemutations,
which is complex, and includes external components and many
parameters.Inspiteofourvalidationefforts,theycouldhavefaults.
Wesharethecodetomitigatethatthreat.Also,byrestrictingthe
initial tests to thoseon which the SUTs didwell we helped isolate
the effect of the mutations. However, this constraint may have
leftoutopportunitiestomaketeststhatrenderpoorresultseven
worse. We also attempted to control for the randomness of several
nondeterministic components through repeated executions.
Intermsofconstructvalidity,althoughwehavequantitatively
shown that ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ can generate many test cases that yield
inconsistencies, our examination of false positives for conformity
exhibits inherent bias. We share the code to reproduce the test
suite to mitigate this threat. Further, the inconsistencies we found
in terms of the percentage point drop may not extend to causefailures in real ASs. While Section 6.4 showcases several serious
inconsistencies,furtherstudyisneededtounderstandifandhow
these inconsistencies would affect the entire AS.
7 CONCLUSION AND FUTURE WORK
We introduced a novel approach that automatically generates test-
caseswithsensorreadingandground-truthinterpretationpairsfor
AS perception systems. The approach leverages domain-specific
semanticsandpriortestcasesbasedonreal-worldsensordatato
generatemutatedsensorreadingsthatstillconformtothephysical
world. Our experimental prototype for images showed that low-
cost and high-level semantic mutations such as adding a car can
uncover inconsistencies in state-of-the-art perception systems.
ğ‘ ğ‘’ğ‘šğ‘†ğ‘’ğ‘›ğ‘ ğ¹ğ‘¢ğ‘§ğ‘§ and our implementation ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ set several
directions for future work. Following standard software fuzzing,
wewillexaminehowtouseSUTperformancetoguidemutation
type and parameterselection to more quickly findinconsistencies.
We willalso investigatehow to expandto other sensormodalities
andaggregatereadingsfrommultiplesensorstomoreholistically
test AS perception systems. This work also encourages several
directions of research in computer graphics, advances in which
would likely translate quickly into improvements in ğ‘ ğ‘’ğ‘šğ¼ğ‘šğ¹ğ‘¢ğ‘§ğ‘§ .
TestingASperceptionsystems requiresexaminingrare,safety-
critical scenarios which cannot be obtained practically from the
realworld.Wewilldevelopmoreadvancedmutationsthatincor-
porate these elements, such as cars driving in the wrong direction,
damagedcars,orpeoplesittingoncars.Althoughthecurrentimple-
mentation has the opportunity to generate some of these elements,
e.g.acardrivingthewrongdirection,makingsuchmutationsan
explicitdesign goalwill requiretheintegration ofricher datasets
andthedevelopmentofmoresophisticatedmechanismstocheck
conformity. The ability to systematically generate these scenarios
willfurtherbolstertheutilityofourapproachbygeneratingcon-
formingsensorreadingsofeventsthatareimpracticaltootherwise
obtain and have exhibited problems for real-world deployed ASs.
ACKNOWLEDGEMENTS
ThisworkwassupportedinpartbyfundsprovidedbyNSF#1924777
and NSF#1909414. Trey Woodlief was supported by a University
of Virginia SEAS Fellowship. We are thankful to David Luebke
of NVIDIA for his help in understanding the state-of-the-art in
computer graphics and vision.
1968ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Trey Woodlief, Sebastian Elbaum, and Kevin Sullivan
REFERENCES
[1]2019. Uber in fatal crash had safety flaws say US investigators. BBC(Nov 2019).
https://www.bbc.com/news/business-50312340
[2]Adith Boloor, Xin He, Christopher Gill, Yevgeniy Vorobeychik, and Xuan Zhang.
2019. Simple physical adversarial examples against end-to-end autonomous
driving models. In 2019 IEEE International Conference on Embedded Software and
Systems (ICESS) . IEEE, 1â€“7.
[3]Neal E Boudette and Niraj Chokshi. 2021. U.S. Will Investigate Teslaâ€™s Autopilot
System Over Crashes With Emergency Vehicles. New York Times (Aug 2021).
https://www.nytimes.com/2021/08/16/business/tesla-autopilot-nhtsa.html
[4]G. Bradski. 2000. The OpenCV Library. Dr. Dobbâ€™s Journal of Software Tools
(2000).
[5]Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Li-
ong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
2019. nuScenes:Amultimodaldatasetforautonomousdriving. arXivpreprint
arXiv:1903.11027 (2019).
[6]Alex Clark. 2015. Pillow (PIL Fork) Documentation. https://buildmedia.
readthedocs.org/media/pdf/pillow/latest/pillow.pdf
[7]MariusCordts,MohamedOmran,SebastianRamos,TimoRehfeld,MarkusEn-
zweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. 2016.
The Cityscapes Dataset for Semantic Urban Scene Understanding. In Proc. of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .
[8]Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.2017. CARLA:Anopenurbandrivingsimulator.In Conferenceonrobot
learning. PMLR, 1â€“16.
[9]Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao,
SabeekPradhan,YuningChai,BenSapp,CharlesQi,YinZhou,ZoeyYang,Au-
rÃ©lien Chouard,Pei Sun, Jiquan Ngiam,Vijay Vasudevan, Alexander McCauley,
Jonathon Shlens, and Dragomir Anguelov. 2021. Large Scale Interactive Motion
Forecasting for Autonomous Driving: The Waymo Open Motion Dataset. arXiv
preprint arXiv:2104.10133 (2021).
[10]XiangGao,RiponKSaha,MukulRPrasad,andAbhikRoychoudhury.2020. Fuzz
testing based data augmentation to improve robustness of deep neural networks.
In2020 IEEE/ACM42nd International Conferenceon Software Engineering(ICSE) .
IEEE, 1147â€“1158.
[11]Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. 2013. Vision
meets Robotics: The KITTI Dataset. International Journal of Robotics Research
(IJRR)(2013).
[12]Isobel Asher Hamilton. 2019. Tesla is being sued again for a deadly Autopilot
crash.Insider(Aug 2019). https://www.businessinsider.com/tesla-sued-family-
jeremy-beren-banner-autopilot-crash-2019-8
[13]Charles R. Harris, K. Jarrod Millman, StÃ©fan J. van der Walt, Ralf Gommers,
Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg,
Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van
Kerkwijk, Matthew Brett, Allan Haldane, Jaime FernÃ¡ndez del RÃ­o, Mark Wiebe,
Pearu Peterson, Pierre GÃ©rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren
Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. 2020.
Array programming with NumPy. Nature585, 7825 (Sept. 2020), 357â€“362. https:
//doi.org/10.1038/s41586-020-2649-2
[14]NoorAIbraheem,MokhtarMHasan,RafiqulZKhan,andPramodKMishra.2012.
Understanding color models: a review. ARPN Journal of science and technology 2,
3 (2012), 265â€“275.
[15]Nick Jakobi, Phil Husbands, and Inman Harvey. 1995. Noise and the reality
gap:Theuseofsimulationinevolutionaryrobotics.In EuropeanConferenceon
Artificial Life . Springer, 704â€“720.
[16]NidhiKalraandSusanM.Paddock.2016. DrivingtoSafety:HowManyMilesof
Driving Would It Take to Demonstrate Autonomous Vehicle Reliability? RAND
Corporation, Santa Monica, CA. https://doi.org/10.7249/RR1478
[17]Zelun Kong, Junfeng Guo, Ang Li, and Cong Liu. 2020. Physgan: Generating
physical-world-resilient adversarial examples for autonomous driving. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
14254â€“14263.
[18]Tom Krisher. 2018. Feds: Tesla accelerated, didnâ€™t brake ahead of fatal crash. AP
News(Jun2018). https://apnews.com/article/north-america-us-news-mi-state-
wire-ca-state-wire-transportation-8c833b3e5d9c49cf97a10974126daad9
[19]Xiangtai Li, Xia Li, Li Zhang, Cheng Guangliang, Jianping Shi, Zhouchen Lin,
Yunhai Tong, and Shaohua Tan. 2020. Improving Semantic Segmentation via
Decoupled Body and Edge Supervision. In ECCV.
[20]Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, and Shinâ€™ichi Satoh. 2020.
Guidance and evaluation: Semantic-aware image inpainting for mixed scenes. In
European Conference on Computer Vision . Springer, 683â€“700.
[21]Waymo LLC. 2020. Waymoâ€™s Safety Methodologies and Safety Readiness
Determinations . Technical Report. 30 pages. https://storage.googleapis.com/sdc-
prod/v1/safety-report/Waymo-Safety-Methodologies-and-Readiness-
Determinations.pdf
[22]Andrea Matessi and Luca Lombardi. 1999. Vanishing point detection in the
hough transformspace.In European Conferenceon Parallel Processing . Springer,987â€“994.
[23]BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,Ravi
Ramamoorthi,andRenNg.2020. Nerf:Representingscenesasneuralradiance
fields for view synthesis. In European conference on computer vision . Springer,
405â€“421.
[24]Rohit Mohan and Abhinav Valada. 2020. EfficientPS: Efficient Panoptic Segmen-
tation.International Journal of Computer Vision 129 (2020), 1551 â€“ 1579.
[25]Huy H Nguyen, T Ngoc-Dung Tieu, Hoang-Quoc Nguyen-Son, Vincent Nozick,
JunichiYamagishi,andIsaoEchizen.2018. Modularconvolutionalneuralnetwork
fordiscriminatingbetweencomputer-generatedimagesandphotographicimages.
InProceedingsofthe13thinternationalconferenceonavailability,reliabilityand
security. 1â€“10.
[26] Augustus Odena,CatherineOlsson,DavidAndersen,andIanGoodfellow.2019.
Tensorfuzz: Debugging neural networks with coverage-guided fuzzing. In Inter-
national Conference on Machine Learning . PMLR, 4901â€“4911.
[27]Matthew Oâ€™Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, and John C
Duchi. 2018. Scalable end-to-end autonomous vehicle testing via rare-event
simulation. Advances in neural information processing systems 31 (2018).
[28]Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. 2021.
Neuralscenegraphsfordynamicscenes.In ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition . 2856â€“2865.
[29]Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun. 2018. Semi-parametric
imagesynthesis.In ProceedingsoftheIEEEConferenceonComputerVisionand
Pattern Recognition . 8808â€“8816.
[30]Christos Sakaridis, Dengxin Dai, and Luc Van Gool. 2018. Semantic foggy scene
understanding with synthetic data. International Journal of Computer Vision 126,
9 (2018), 973â€“992.
[31]Hans-PeterSchÃ¶ner.2018. Simulationindevelopmentandtestingofautonomous
vehicles. In 18. Internationales Stuttgarter Symposium . Springer, 1083â€“1095.
[32]Sergio Segura, Gordon Fraser, Ana B Sanchez, and Antonio Ruiz-CortÃ©s. 2016. A
survey onmetamorphic testing. IEEE Transactionson softwareengineering 42, 9
(2016), 805â€“824.
[33]KeSun,BinXiao,DongLiu,andJingdongWang.2019. DeepHigh-Resolution
Representation Learning for Human Pose Estimation. In CVPR.
[34]AndrewTao, KaranSapra, andBryanCatanzaro. 2020. Hierarchicalmulti-scale
attention for semantic segmentation. arXiv preprint arXiv:2005.10821 (2020).
[35]Brad Templeton. 2019. NTSB Report On Tesla Autopilot Accident
Shows Whatâ€™s Inside And Itâ€™s Not Pretty For FSD. Forbes(Sep 2019).
https://www.forbes.com/sites/bradtempleton/2019/09/06/ntsb-report-
on-tesla-autopilot-accident-shows-whats-inside-and-its-not-pretty-for-
fsd/?sh=6270d8234dc5
[36]Brad Templeton. 2020. Tesla In Taiwan Crashes Directly Into Over-
turned Truck, Ignores Pedestrian, With Autopilot On. Forbes(Jun 2020).
https://www.forbes.com/sites/bradtempleton/2020/06/02/tesla-in-taiwan-
crashes-directly-into-overturned-truck-ignores-pedestrian-with-autopilot-
on/?sh=20a7458f58e5
[37]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. Deeptest:Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th international conference on software engineering . 303â€“314.
[38]ChristopherStevenTimperley,AfsoonAfzal,DeborahSKatz,JamMarcosHer-
nandez, and Claire Le Goues. 2018. Crashing simulated planes is cheap: Can
simulation detect robotics bugs early?. In 2018 IEEE 11th International Conference
on Software Testing, Verification and Validation (ICST) . IEEE, 331â€“342.
[39]JiahuiYu,ZheLin,JimeiYang,XiaohuiShen,XinLu,andThomasSHuang.2018.
Generativeimageinpaintingwithcontextualattention.In ProceedingsoftheIEEE
conference on computer vision and pattern recognition . 5505â€“5514.
[40]Juan CristÃ³bal Zagal and Javier Ruiz-Del-Solar. 2007. Combining simulation and
realityinevolutionaryrobotics. JournalofIntelligentandRoboticSystems 50,1
(2007), 19â€“39.
[41]Sebastian Zanlongo, Matthew Turk, and Sanjay Parajuli. 2019. vanishing-point-
detection. https://github.com/SZanlongo/vanishing-point-detection.
[42]Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khur-
shid. 2018. DeepRoad: GAN-based metamorphic testing and input validation
frameworkforautonomousdrivingsystems.In 201833rdIEEE/ACMInternational
Conference on Automated Software Engineering (ASE) . IEEE, 132â€“142.
[43]HushengZhou,WeiLi,ZelunKong,JunfengGuo,YuqunZhang,BeiYu,Lingming
Zhang, and Cong Liu. 2020. Deepbillboard: Systematic physical-world testing of
autonomousdrivingsystems.In 2020IEEE/ACM42ndInternationalConferenceon
Software Engineering (ICSE) . IEEE, 347â€“358.
[44]Yi Zhu, Karan Sapra, Fitsum A Reda, Kevin J Shih, Shawn Newsam, Andrew
Tao, and Bryan Catanzaro. 2019. Improving semantic segmentation via video
propagationandlabelrelaxation.In ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition . 8856â€“8865.
1969