Automatic Detection of Performance Bugs
in Database Systems using Equivalent Queries
Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
‚àóGeorgia Institute of Technology, Atlanta, GA, USA; liuxy@gatech.edu, arulraj@gatech.edu, orso@cc.gatech.edu
‚Ä†Meta, Seattle, W A, USA; zhouqi@fb.com
Abstract
Because modern data-intensive applications rely heavily on database
systems (DBMSs), developers extensively test these systems to elim-
inate bugs that negatively affect functionality. Besides functionalbugs, however, there is another important class of faults that nega-
tively affect the response time of a DBMS, known as performance
bugs. Despite their potential impact on end-user experience, perfor-
mance bugs have received considerably less attention than functional
bugs. To Ô¨Åll this gap, we present AMOEBA , a technique and tool
for automatically detecting performance bugs in DBMSs. The core
idea behind A MOEBA is to construct semantically equivalent query
pairs, run both queries on the DBMS under test, and compare their
response time. If the queries exhibit signiÔ¨Åcantly different response
times, that indicates the possible presence of a performance bug in
the DBMS. To construct equivalent queries, we propose to use a set
of structure and expression mutation rules especially targeted at un-
covering performance bugs. We also introduce feedback mechanisms
for improving the effectiveness and efÔ¨Åciency of the approach. We
evaluate AMOEBA on two widely-used DBMSs, namely PostgreSQL
and CockroachDB, with promising results: AMOEBA has so far dis-
covered 39 potential performance bugs, among which developers
have already conÔ¨Årmed 6 bugs and Ô¨Åxed 5 bugs.
CCS Concepts
‚Ä¢Software and its engineering ‚ÜíMaintaining software; Soft-
ware veriÔ¨Åcation and validation;‚Ä¢ Information systems ‚ÜíQuery
optimization.
Keywords
Differential testing, database testing, query optimization
ACM Reference Format:
Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó. 2022. Automatic De-
tection of Performance Bugs in Database Systems using Equivalent Queries.
In44th International Conference on Software Engineering (ICSE ‚Äô22), May
21‚Äì29, 2022, Pittsburgh, PA, USA. ACM, New Y ork, NY , USA, 12 pages.
https://doi.org/10.1145/3510003.3510093
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00
https://doi.org/10.1145/3510003.35100931 Introduction
Database management systems (DBMSs) play a critical role in mod-
ern data-intensive applications [ 17,33]. For this reason, developers
extensively test these systems to improve their reliability and ac-curacy. For instance, they leverage tools such as
SQLSMITH [4]
and SQLancer [37‚Äì39] to discover crash-inducing or logic bugs in
DBMSs. However, the same level of scrutiny has not been applied
toperformance bugs‚Äîbugs that affect the time taken by the DBMS
to process certain queries. Detecting performance bugs is just as
crucial as detecting functional bugs, as delayed responses from the
DBMS can dramatically affect the user experience [32, 44].
CHALLENGES .To retrieve the results for a given SQL query,
the DBMS invokes a pipeline of complex components (e.g., query
optimizer, execution engine) [ 22,34]. The overall performance of the
DBMS may be reduced due to sub-optimal decisions taken by any of
these components and the complex interactions among them [ 8,9].
Therefore, performance testing on individual components of the
DBMS is in general insufÔ¨Åcient to detect performance bugs [ 21,24,
29,30]. Another key challenge for detecting performance bugs in
DBMS is deÔ¨Åning a test oracle that speciÔ¨Åes the correct behavior
(i.e., response time) of a performant DBMS for a given SQL query.
There are two lines of research that attempt to address this chal-
lenge, both focusing on performance regressions. One approach uses
a pre-determined performance baseline as the oracle [ 35,45,46]
and reports a performance bug if there is a signiÔ¨Åcant deviation.
While potentially effective in detecting some performance bugs, this
approach is human-intensive and error prone, as it is challengingto construct an accurate performance baseline and to account for
variability in DBMS performance (to reduce false positives) [
28].
Furthermore, this approach relies on a Ô¨Åxed, limited set of queries
from standard benchmarks that only cover a subset of the SQL input
domain [42].
The second approach leverages differential testing to discover
performance regressions [ 26] by using an oracle to compare the exe-
cution time of the same query on two versions of the DBMS. While
this technique does not require a developer-provided, pre-determined
baseline, it is only able to detect regressions, as (1) it requires two
versions of the DBMS, with and without the performance bugs,
and (2) focuses on structurally simple queries specially tailored for
uncovering regressions.
OURAPPROACH .To address the limitations of existing techniques,
we present AMOEBA , a new approach for discovering performance
bugs in DBMSs. AMOEBA addresses the challenges discussed above
along three dimensions. First, it constructs a performance oracle by
comparing the execution time of semantically equivalent queries
(i.e., queries that always return the same result) [ 19,48]. When
2252022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
the target DBMS exhibits a signiÔ¨Åcant difference in execution time
on a pair of semantically equivalent queries, this may indicate the
presence of a performance bug. Second, it constructs queries tai-
lored to the discovery of performance bugs, by supporting complex
structures and computationally expensive SQL operators. Further,
because of the large space of SQL queries that AMOEBA can explore,
we introduce a feedback mechanism that lets it focus on the subset
of the query space that is more likely to uncover performance bugs.
Third, it introduces two types of semantic preserving query muta-
tion rules that are also tailored to performance bugs detection: (1)structural mutations, which transform an input query using a setof query rewrite rules derived from the query optimization litera-
ture [ 23], and (2) expression mutations, which modify expressions
within an input query without changing their semantics.
To evaluate our technique, we implemented it and applied it to two
widely-used DBMSs: CockroachDB and PostgreSQL . Our results
are promising, in that AMOEBA found 39 potential performance
bugs, among which developers have conÔ¨Årmed 6 bugs and Ô¨Åxed5 bugs. We also compared
AMOEBA against two other sources of
equivalent queries that could be used for detecting performance bugs:
a manually-written test suite in a widely-used query optimization
framework, and the Ternary Logic Partitioning (TLP) approach [ 38].
Our results show that the equivalent queries generated by AMOEBA
are more likely to detect performance bugs.
CONTRIBUTIONS .This paper makes the following contributions:
‚Ä¢A performance bug detection technique with three new aspects:
‚ÄìThe use of query equivalence to generate performance oracles.
‚ÄìTwo types of query mutations that preserve the semantics of
queries: structural mutations and expression mutations.
‚ÄìA feedback mechanism that improves the effectiveness and
efÔ¨Åciency of the approach.
‚Ä¢An implementation of the technique that is publicly available [ 12].
‚Ä¢An evaluation of the technique that shows that it can detect real
and relevant previously-unknown performance bugs in two widely-
used DBMSs.
2 Motivating Example
Figure 1 shows a motivating example that we use to illustrate how
semantically equivalent queries can be leveraged for detecting perfor-
mance bugs in DBMSs and show the signiÔ¨Åcant impact performance
bugs can have on the end-user experience.
The example consists of a pair of equivalent queries, Q1 (Fig-
ure 1a) and Q2 (Figure 1b), that our technique actually generatedbased on the SCOTT schema [
11] and that detected a real perfor-
mance bug. We also show, in Figure 2, the logical query plans for Q1and Q2 (i.e., the sequence of logical operations performed when exe-cuting the two queries). Although Q1 and Q2 are equivalent, Q1 runs
1,444√óslower than Q2 on the same database in CockroachDB [ 6]
(v20.2.0-alpha).
The difference in performance in the two cases is caused by how
theemp table, which contains 10 million rows, is processed. For Q1,
the DBMS ignores that the maximum number of result tuples is 13
and processes the entire emp table anyway. For Q2, conversely, the
DBMS considers the LIMIT directive, processes the table by row,
and stops after fetching the Ô¨Årst qualifying 13 entries. As a result,
while Q1 takes 13 seconds to execute, Q2 only takes 9 milliseconds.SELECT job, deptno FROM emp WHERE job = 'Technical '
GROUP BY job, deptno LIMIT 13;
(a)Original query Q1, execution time = 13 s
SELECT CAST ('Technical 'AS VARCHAR (10)) AS"job" , deptno
FROM emp WHERE "job" ='Technical '
GROUP BY job, deptno LIMIT 13;
(b) Mutated query Q2, execution time=9m s
Figure 1: Query Rewriting ‚Äì Example of application of the projection
column mutation rule.
(a)Logical query plan for Q1
 (b) Logical query plan for Q2
Figure 2: Logical Query Plans ‚Äì sequence of logical operations performed
when executing the queries in Figure 1.
The developers have acknowledged that this is a previously-unknown
performance bug and have produced a Ô¨Åx for it.
AMOEBA can detect this performance bug because it uses the
execution time of equivalent queries as performance oracles. Further-
more, by doing so, AMOEBA can provide a concrete performance
bug report, which allows developers to reproduce and investigate the
potential performance bug.
3 Background and Terminology
This section provides some relevant background information and
introduces the terminology used in the paper.
PERFORMANCE BUGS AND RELATED CONCEPTS .Aperfor-
mance bug is a bug that affects the time taken by the DBMS to
process certain queries. Before reporting it to the DBMS developers,
we refer to a performance discrepancy identiÔ¨Åed by AMOEBA as a
potential performance bug (PPB), which can be unique or not, de-
pending on whether its root cause differs from that of other bugs dis-
covered by AMOEBA (based on manual analysis). After we report
a PPB, depending on the feedback we receive from the developers,
we classify the PPB according to the following taxonomy:
‚Ä¢ConÔ¨Årmed: The developers acknowledge that the issue reported
indicates an actual performance bug. ConÔ¨Årmed performance
bugs can further be classiÔ¨Åed as either previously unknown, if the
developers do not refer us to a previous/duplicate bug report or an
already planned Ô¨Åx, or previously known, otherwise. A conÔ¨Årmed
performance bug, whether previously known or not, can also be
classiÔ¨Åed as Ô¨Åxed, if the developers plan to Ô¨Åx the it, already Ô¨Åxed
it, or have a Ô¨Åx in progress, or not Ô¨Åxed, otherwise.
‚Ä¢Backlogged: If the developers respond to the report and state that
they will analyze the PPB at a later time.
‚Ä¢UnconÔ¨Årmed: If the developers do not acknowledge that the issue
reported is a performance bug. This can happen for two reasons.
Either the developers disagree that two reported queries are equiv-
alent, in which case we refer to the issue as a false positive,o r
they acknowledge the performance discrepancy but consider it a
future/missing optimization, rather than a bug.
‚Ä¢Unknown: If the developers ignore the report.
226
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automatic Detection of Performance Bugs
in Database Systems using Equivalent QueriesICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA
Figure 3: Architecture of AMOEBA ‚ÄìGENERA TOR constructs a set of base SQL queries based on feedback from other components. MUTA TOR performs
semantic-preserving structural and expression mutations on the base queries. V ALIDA TOR executes a pair of semantically equivalent queries using the target
DBMS and reports query pairs that exhibit a signiÔ¨Åcant difference in runtime performance.
SEMANTIC EQUIV ALENCE .Two queries are semantically equiv-
alent if they always produce the same result on any input database
instance. Query equivalence is a well-studied topic used in many ap-
plications, such as testing DBMSs for correctness [ 38,40], educating
developers [ 27], and automatically grading student assignments [ 16].
Unlike prior efforts, we seek to leverage semantic equivalence of
queries to Ô¨Ånd performance bugs in DBMSs.
QUERY REWRITING .AMOEBA constructs equivalent queries by
rewriting them using a set of rules that preserve equivalence [ 23]. For
example, a rule could mutate projection columns (i.e., the columns
the query should return) using information from the Ô¨Ålter predicate
(i.e., the predicate that speciÔ¨Åes which rows are to be returned).
We illustrate how this rule transforms its projection columns while
preserving semantic equivalence using query Q1 (Figure 1a) and
its logical query plan (Figure 2a). Since the Ô¨Ålter clause ( ùúé)i n
Q1 selects tuples such that the job attribute is equal to a speciÔ¨Åc
value, the rule replaces the Ô¨Ånal projection column job with a literal
column that takes the same value. Figure 1b and Figure 2b show the
transformation result, that is, Q2 and its logical query plan.
4 The A MOEBA Technique
4.1 System Overview
AMOEBA helps developers uncover performance bugs in a DBMS.
The key idea is to compare the runtime performance of the DBMS
on two semantically equivalent queries, which we would normally
expect the DBMS to execute in a similar amount of time. If thatis not the case, and the difference in the query execution time ex-
ceeds a developer-speciÔ¨Åed threshold (e.g., 2 √ó), then AMOEBA has
found a PPB. Such a performance oracle allows us to detect perfor-
mance bugs in a single DBMS (i.e., without resorting to comparative
analysis against another DBMS).
Figure 3 illustrates the architecture of A MOEBA , which contains
three main components: (1) GENERA TOR , (2) MUTA TOR , and (3)
VA L I DAT O R .
1GENERA TOR leverages a domain-speciÔ¨Åc fuzzing technique
to generate SQL queries from scratch based on a database schema.
We refer to these queries as base queries. GENERA TOR is tailored to
generate queries that are more likely to trigger performance bugs in
DBMSs. In particular, it receives feedback from the latter compo-
nents of A MOEBA to guide the query generation process.
2MUTA TOR takes a base query as input and seeks to generate
equivalent queries by applying a set of semantics-preserving query-
rewriting rules to the query. We refer to the resulting set of equivalentqueries as mutant queries. The output of this component is the base
query and a set of equivalent mutant queries.
3VA L I DAT O R takes a set of equivalent queries as input and
generates a list of performance bug reports. It runs each pair ofequivalent queries on the target DBMS and observes whether any
pair exhibits a signiÔ¨Åcant difference in runtime performance. If so,
it Ô¨Årst veriÔ¨Åes whether this behavior is reproducible across mul-tiple runs. If it can conÔ¨Årm the discrepancy, it generates a reportthat consists of: (1) the pair of equivalent queries that exhibit the
performance discrepancy, and (2) their query execution plans.
We next present the three components of A MOEBA in detail.
4.2 Query Generator
AMOEBA uses a grammar-aware GENERA TOR that randomly con-
structs, given a database schema, a set of base queries from scratch
(i.e., without using any seed query). As shown in Algorithm 1,
GENERA TOR takes a target database as input, generates base queries
as output, and uses two key procedures for generating queries that aremore likely to trigger performance bugs: (1) G
ENERA TE QUERY (¬ß4.2.1)
uses a top-down, grammar-aware approach to generate queries that
are compatible with the schema of the input database, and (2)
UPDA TE PROB TABLEWITH FEEDBACK (¬ß4.2.2) leverages feedback
from prior runs of MUTA TOR (¬ß4.3) and VA L I DAT O R (¬ß4.4) to guide
the G ENERA TE QUERY procedure. AMOEBA relies on this feedback
mechanism to improve the probability of generating queries that
trigger performance bugs. Next, we provide more details about these
two procedures.
4.2.1 Grammar-Aware Query Generation. Researchers have
extensively explored techniques for grammar-aware query genera-
tion [ 4,13,14,47].AMOEBA ‚Äôs query generation approach differs
from prior work in that it is geared towards generating queries that
aremore likely to trigger performance bugs in DBMS. This part of
the approach is based on two main components: (1) a grammar for
generating queries with different structures and operators, and (2) a
probability table deÔ¨Åned with respect to the grammar to guide the
query generation process.
GRAMMAR .AMOEBA uses a grammar based on the SQL-92 stan-
dard [ 1]. The grammar is expressed in Backus‚ÄìNaur Form (BNF),
which consists of both terminal and non-terminal symbols. We show
a subset of the grammar in Table 1. For instance, the non-terminal
symbol table refmay either be a base table (i.e., table simple ) from the
target database or a derived table (i.e., table joined ) resulting from a
JOIN operator.
227
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
Table 1: SQL Grammar ‚Äì A subset of the SQL grammar that allows
AMOEBA to generate queries with a variety of structures and operators.
query_spec /Colonequal SELECT <column ref> FROM <table ref> <group_clause> <limit_clause>
table ref /Colonequal <table simple > | <table joined >
table joined /Colonequal <table ref> <join_spec> <table ref>
join_spec /Colonequal <join type > ON <join cond >
join type /Colonequal LEFT | CROSS | INNER
join cond /Colonequal <bool expr >|T R U E
Table 2: Probability Table ‚Äì Probability values that AMOEBA uses to gen-
erate table references and join conditions.
(a) Table References
table simpletable joined
LEFT CROSS INNER
0.5 0.16 0.17 0.17(b) Join Conditions
bool expr TRUE
0.5 0.5
PROBABILITY TABLE .As shown in Table 2, AMOEBA maintains
a table that contains the probability of using each non-terminal and
terminal symbol when generating queries following the grammar.
Thus, this table determines the likelihood of a SQL structure or
clause to appear in the generated query. For all symbols that stem
from a given non-terminal symbol, the probabilities sum up to one.
For instance, Table 2b speciÔ¨Åes that there is an equal chance of
generating a non-terminal symbol, the JOIN condition, using either
a boolean expression (e.g., t1.k = t2.k) or the keyword T RUE .
Next, we present the algorithm of G ENERA TE QUERY , which is
shown in Algorithm 1. First , the algorithm acquires information that
it needs to generate queries (line 1). SpeciÔ¨Åcally, it performs the
following steps: (1) it randomly samples a small dataset from the
target database, so as to be able to generate queries with meaningful
predicates and a variety of selectivity. (2) it collects table schemas
of the target database (i.e., table names, column names, and column
types), which it needs to create valid expressions, such as SQL func-tion calls and column comparisons. Second, the algorithm initializes
with default values the probability table it uses for guiding the query
generation procedure (line 2). Third, it invokes the B UILD SPECIFI -
CA TION function to construct a query speciÔ¨Åcation based on (1) the
SQL grammar and (2) the collected meta-data (line 6). Finally, the
algorithm translates the speciÔ¨Åcation into a well-formed query for
the target DBMS (line 7).
4.2.2 Feedback from Mutator and Validator. We now discuss
how GENERA TOR updates the probability table based on the feed-
back from MUTA TOR and V ALIDA TOR (line 9).
FEEDBACK FROM MUTATOR .GENERA TOR uses the feedback
from MUTA TOR to improve the likelihood of generating base queries
that can be successfully mutated. Since A MOEBA relies on the gen-
eration of semantically equivalent queries, this feedback mechanism
indirectly increases the likelihood of discovering PPBs. In proce-dure U
PDA TE PROB TABLE WITH MUTA TOR FEEDBACK (line 13),
GENERA TOR updates the probability table when a base query that it
generates is successfully transformed by MUTA TOR (line 12). First, it
extracts SQL entities from the base query. For example, since Q1 inour motivating example (Section 2) can be successfully mutated intoQ2,
GENERA TOR extracts the following entities from Q1: table simple ,
GROUP BY, and L IMIT . The rationale for this part of the approach
is that these entities are correlated with successful mutations. Then,Algorithm 1: Algorithm for generating SQL queries
Input : database: database under test
Output : base queries: random SQL queries
1meta-data ‚ÜêRetrieveMetaData(database);
2prob_table ‚ÜêInitProbabilityTable();
3rule_activation_frequency_table ‚ÜêInitRuleActivationFrequencyTable();
4Procedure GenerateQuery(meta-data, prob_table)
5 while True do
6 speciÔ¨Åcation ‚ÜêBuildSpecification(meta-data, prob_table) ;
7 base query ‚ÜêSpecToQuery(speciÔ¨Åcation, dialect );
8 return base query ;
9 prob_table ‚ÜêUpdateProbTablewithFeedback(base query, prob_table) ;
10
11Procedure UpdateProbTablewithFeedback(base query, prob_table)
12 ifMutateQuery(base query) is True then
// If the base query can be mutated to generate equivalent
queries, update the probability table so that it is more
likely to generate queries containing SQLentities that
the base query contains.
13 UpdateProbTableWithMutatorFeedback(base query, prob_table,
applied_rules) ;
14 ifTriggerPerformanceBug(base query) is True then
// If the base query can trigger a performance bug, update
the probability table so that it is more likely togenerate queries containing SQLentities that the base
query contains.
15 UpdateProbTableWithValidatorFeedback(base query, prob_table) ;
16
GENERA TOR updates the probability table by increasing the values
corresponding to these extracted entities and decreasing the values
of the other entities. For example, given the entities extracted from
Q1, GENERA TOR would update the probability table by (1) increas-
ing the probabilities associated with table simple ,G ROUP BY, and
LIMIT , and (2) decreasing the probabilities of LEFT, CROSS, and
INNER After this update, GENERA TOR will generate base queries
that are more likely to contain the G ROUP BYand L IMIT clauses.
GENERA TOR also seeks to avoid generating queries that only trig-
ger a limited set of mutation rules, which improves the computational
efÔ¨Åciency of AMOEBA (¬ß5.4). To accomplish this, it keeps track of
the frequency with which each mutation rule has been Ô¨Åred (line 3).
For instance, when a base query triggers a less-frequently triggered
mutation rule, GENERA TOR increases the probabilities of the en-
tities extracted from that query by a larger amount. In this way,
GENERA TOR is more likely to construct queries that trigger a wide
variety of mutation rules.
FEEDBACK FROM VALIDATOR .GENERA TOR utilizes informa-
tion about discovered PPBs to increase the likelihood of generat-
ing base queries that uncover them. When VA L I DAT O R discovers
a base query that eventually leads to Ô¨Ånding a performance dis-
crepancy (line 14), GENERA TOR invokes the U PDA TE PROB TABLE -
WITH VALIDA TOR FEEDBACK procedure to suitably update the prob-
ability table (line 15). Similar to U PDA TE PROB TABLE WITH MUTA -
TOR FEEDBACK , this procedure Ô¨Årst extracts the SQL entities from
the base query, and then updates the probability table accordingly.
4.3 Query Mutator
MUTA TOR takes a base query as input and seeks to generate mutant
queries that are semantically equivalent to the base query.
QUERY EQUIV ALENCE .
MUTA TOR leverages and adapts query rewrite rules from the query
optimization literature [ 23] to transform queries while preserving
their semantics. We now discuss its mutation rules and algorithm.
228
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automatic Detection of Performance Bugs
in Database Systems using Equivalent QueriesICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA
Table 3: Illustrative List of Mutation Rules.
Category Rule Idx Transformation Working Example
Structure 13 Push Ô¨Ålter through GROUP BY (t1.c GROUP BY t1.c) WHERE t1.c > 0 ‚Üí(t1.c WHERE t1.c > 0) GROUP BY t1.C
59 Propagate LIMIT through LEFT JOIN (t1 LEFT JOIN t2) SORT t1.c ‚Üí((t1 SORT t1.c) LEFT JOIN t2) SORT t1.c
Expression 15 Convert EXTRACT into date range comparison EXTRACT (YEAR FROM c1) < 2021 ‚Üíc1 < TIMESTAMP ‚Äô2021-01-01 00:00:00‚Äô
54 Reduce expression in FILTER WHERE c1 = CAST(10/2) as INT ‚ÜíWHERE c1 = 5
(a)Rule condition
 (b) Transformation
Figure 4: Illustrative Rule Condition and Transformation (Rule 59).
4.3.1 Mutation Rules. MUTA TOR currently supports 75 muta-
tion rules that fall into two categories: (1) rules for mutating query
structure (67 rules), (2) rules for mutating SQL expressions (8 rules).
Each rule is represented by a triple:
<id, condition, transformation> [23].
The idis a unique identiÔ¨Åer for the rule. The condition is a precondi-
tion that the original query must satisfy for the rule to be applicable
and semantic preserving; it is expressed in terms of the meta-data
of the database under consideration, including the data type, PRI-
MARY KEY, UNIQUE, and NOT NULL information for each
column of the database. Finally, the transformation is a function that
takes the original query as input and returns a mutated query.
Table 3 presents a subset of these rules to illustrate their variety.
Due to space limitations, the table does not include the precise condi-
tion for each rule. A complete list of rules and their implementation
are available in our supplementary materials [12].
RULES FOR MUTATING QUERY STRUCTURE .These rules mod-
ify the query‚Äôs structure based on a relational algebraic transfor-mation that preserves equivalence [
15]. As an example, Figure 4
illustrates the condition and transformation associated with rule 59
in Table 3. This rule modiÔ¨Åes the query structure by propagating the
SORT operator below the LEFT JOIN operator. As shown in Fig-
ure 4a, this rule is only triggered when the SORT operator is the
parent of the LEFT JOIN operator. If the input query satisÔ¨Åes this
condition, then this rule transforms it into a structurally-different
query shown in Figure 4b by pushing a copy of the SORT operator
and its argument below the LEFT JOIN operator.
RULES FOR MUTATING EXPRESSION .These rules rewrite the
expression in the query without altering the query structure. Rule 15
in Table 3, for instance, mutates the comparison between a constant
value and the result of the EXTRACT function (e.g., EXTRACT
(YEAR FROM c1)) < 2021) by replacing the original expression
with a new one that compares the attribute against a timestamp.
4.3.2 Mutation Algorithm. We outline the algorithm of MUTA TOR
in Algorithm 2. MUTA TOR takes a base query and the metadata of
the target database as input and returns the base query and its se-
mantically equivalent mutant queries as output. It Ô¨Årst preprocesses
the base query and generates its logical query plan tree, Rorigin ,o n
which it applies the mutation rules (line 2). It attempts to mutateAlgorithm 2: Procedure for mutating SQL queries
Input : base query; meta-data: metadata for the target database
Output : base query and mutant queries
1Procedure MutateQuery(base query, meta-data)
2 Rorigin‚ÜêPreprocess(base query) ;
3 transformed_trees ‚ÜêEmptySet() ;
4 mutant queries ‚ÜêEmptySet() ;
5 forùëò‚Üê1to number_of_attempts do
// randomly select a list of mutation rules
6 mutate_rules ‚ÜêRulesInitialization() ;
7 Rnew‚ÜêMutateTree(R origin , mutate_rules, meta_data) ;
8 ifRnew‚àâtransformed_trees then
9 new query ‚ÜêTranslateToQuery(R new , dialect );
10 Update(transformed_trees, mutant queries) ;
11 return base query, mutant queries ;
12Procedure MutateTree(R origin , mutate_rules, meta_data)
13 target_expr ‚ÜêRorigin ;
14 forrule‚ààrewrite_rules do
15 target_expr ‚ÜêApplyRule(target_expr , rule, meta_data) ;
16 iftarget_expr ‚â†Rorigin then
17 return target_expr ;
Rorigin for a total of number_of_attempts times (line 5).
In each iteration, MUTA TOR performs the following three steps:
1It randomly initializes an ordered list of mutation rules (mu-
tate_rules), which it then applies on the Rorigin in a sequential
manner (line 6). In doing so, MUTA TOR increases the likelihood
of uncovering different compositional effects of mutation rules on
the input query.
2It invokes procedure M UTA TE TREE to transform Rorigin using
mutate_rules (line 7). Within this procedure, MUTA TOR uses the
database meta-data to check whether the rule condition for perform-
ing the transformation is met (line 15). Procedure M UTA TE TREE
returns the resulting plan tree, Rnew, only if Rnew is different from
Rorigin (line 17).
3After getting Rnew from procedure M UTA TE TREE ,MUTA TOR
checks whether it is different from trees constructed in prior mutationattempts (line 8). If so,
MUTA TOR translates Rnew into a well-formed
SQL query, new query, based on the target DBMS‚Äôs dialect and
appends it to mutant queries (line 9, line 10). Finally, the algorithm
returns the base query and mutant queries as its output.
4.4 Validator
VA L I DAT O R takes a set of pairs of semantically-equivalent queries
as input and generates performance bug reports as output. To do so,
it compares the execution time of the queries within each pair. If a
pair of equivalent queries consistently exhibit signiÔ¨Åcant difference
in their runtime performance, V ALIDA TOR generates a performance
bug report that consists of: (1) the pair of queries, and (2) theirexecution plans. Before presenting the algorithm of
V ALIDA TOR ,
we discuss two challenges associated with discovering performance
229
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
Algorithm 3: Procedure for detecting performance bugs
Input : base query mutant queries
validation_attempt: # of additional runs threshold: least time difference
Output : bug reports: each report contains a query pair and execution plans
1ifCheckPlanDiff(base query, mutant queries) then
// Run equivalent queries if they have different execution plans
2 time_list‚ÜêRunQuery(base query, mutant queries) ;
3 ifMax(time_list )> threshold ‚àóMin(time_list )then
// Rerun queries for validation_attempt times to confirm the
difference is consistent
4 ifConfirm(base query, mutant queries) then
5 GenBugReport(base query, mutant queries) ;
6
7Procedure CheckPlanDiff(base query, mutant queries)
8 cost_list‚ÜêEstimateCost(base query, mutant queries) ;
9 ifCount(Set(cost_list ))>1then
10 return True ;
1112
Procedure Confirm(base query, mutant queries, validation_attempts)
13 difference_count ‚Üê0;
14 forùëò‚Üê1to validation_attempts do
15 time_list‚ÜêRunQuery(base query, mutant queries) ;
16 ifMax(time_list )> threshold * Min(time_list )then
17 difference_count += 1;
18 ifdifference_count =validation_attempts then
19 return True ;
20
bugs based on query equivalence and how we address them in the
algorithm.
EQUIV ALENT EXECUTION PLANS .A pair of semantically equiv-
alent queries with different syntax (i.e., structural difference or predi-
cate difference) may reduce to the same query execution plan. In thiscase, the DBMS will execute these queries in the same way and there
will not be any difference in runtime performance. Such queries are
not useful for discovering performance bugs. Therefore, to improve
the computational efÔ¨Åciency of AMOEBA ,VA L I DAT O R focuses on
equivalent queries that have different execution plans. In particular,
before executing a set of equivalent queries and comparing theirruntime performances, it Ô¨Årst compares their plans and skips this
query pair if they have the same plan.
FALSE POSITIVES .The execution time of a query may be af-
fected by system-level factors (e.g., caching behavior of concurrent
queries) [ 24]. To avoid false positives due to these factors, before
reporting a PPB to the developers, VA L I DAT O R veriÔ¨Åes that the dif-
ference is consistently reproducible by re-executing the same query
pair multiple times in isolation and in different execution orders.
VALIDATOR ALGORITHM .Algorithm 3 presents the algorithm of
VA L I DAT O R . The algorithm Ô¨Årst invokes procedure C HECK PLAN -
DIFF to Ô¨Ålter out equivalent queries that lead to equivalent execution
plans (line 1). V ALIDA TOR assumes that two queries have the same
execution plan if their estimated costs are the same. SpeciÔ¨Åcally,given two equivalent queries, it utilizes the EXPLAIN feature of
DBMSs (line 8) [ 41] to compute the estimated cost of each query in
the pair. If the estimated plan costs are the same, V ALIDA TOR con-
siders the two query plans to be equivalent and skips them (line 9).
After discarding pairs of equivalent queries deemed to have iden-
tical execution plans, V ALIDA TOR runs the remaining pairs on the
DBMS and records their execution time (line 2). Then, within theresulting set of execution times,
V ALIDA TOR checks whether the
ratio of the longest to the shortest query execution time exceeds adeveloper-speciÔ¨Åed threshold (line 3). If the ratio exceeds this thresh-
old, V ALIDA TOR invokes procedure C ONFIRM to check whether the
runtime performance difference is consistently reproducible (line 4).
Procedure C ONFIRM re-executes these queries on the DBMS
for multiple runs in random orders and monitors whether the exe-cution time difference still holds (line 14‚Äì19). If so,
V ALIDA TOR
automatically generates a performance bug report (line 5).
5 Evaluation
To evaluate the effectiveness and generality of AMOEBA , we investi-
gated the following questions:
RQ1. Can A MOEBA Ô¨Ånd performance bugs in DBMSs? (¬ß5.3)
RQ2. How efÔ¨Åcient is A MOEBA ? (¬ß5.4)
RQ3. Are all mutation rules created equal with respect to discover-
ing performance bugs? (¬ß5.5)
RQ4. How does AMOEBA compare against other techniques for
Ô¨Ånding performance bugs? (¬ß5.6)
RQ5. How do the base queries in AMOEBA compare against those
in Calcite? (¬ß5.7)
5.1 Implementation
QUERY GENERATION .GENERA TOR aims to construct queries
that (1) are likely to be syntactically correct and (2) cover a widely-
supported subset of SQL constructs [ 1]. To this end, we implement
GENERA TOR based on SQLA LCHEMY [7], a SQL toolkit and Ob-
ject Relational Mapper (ORM).
QUERY MUTATION .We build MUTA TOR on top of the Calcite [5]
query optimization framework. Calcite transforms queries by iter-
atively applying a set of query rewrite rules [ 23] and works well
with SQLA LCHEMY , in that they both cover a widely-supported
subset of SQL constructs and dialects. As a consequence, mostof the semantically-valid queries constructed by
GENERA TOR can
be processed by the MUTA TOR . Furthermore, by leveraging the
SQLA LCHEMY and Calcite frameworks, AMOEBA can be easily
extended.
IMPLEMENTATION SCOPE .AMOEBA supports queries with four
data types: integer, double, datetime, and string. The queries may
use several SQL constructs (e.g., GROUP, DISTINCT, ORDER
and UNION) and functions (e.g., AVG and SUM). We present
a detailed list of supported SQL constructs in our supplementary
materials [12].
SCHEMA .AMOEBA currently generates queries using the SCOTT
schema [ 11] and runs them on a database based on the same schema.
(We made this decision because we seek to compare AMOEBA
against the manually-crafted Calcite test suite, which is based on
this schema.) It is worth noting that the SCOTT schema is compara-
tively simple: only contains three tables with two primary keys and
one foreign key. We conÔ¨Ågured the size of the database to 30 MB .
Because the query execution time is proportional to the size of the
database, we wanted to achieve a balance between discovering re-
producible bugs (which requires a larger database) and limiting the
computational cost of AMOEBA (which requires a smaller database).
230
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automatic Detection of Performance Bugs
in Database Systems using Equivalent QueriesICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA
(a)CockroachDB (b) PostgreSQL
Figure 5: Runtime Impact of Potential Performance Bugs ‚Äì Figures (a)
and (b) show the slow-down of queries caused by PPBs for CockroachDB
and PostgreSQL, respectively.
5.2 Evaluation Setup
Our evaluation focused on two DBMSs: (1) CockroachDB (v20.2.0-
alpha), and (2) PostgreSQL (v12.3). We ran all experiments on a
server with two Intel(R) Xeon(R) E5649 CPUs (24 processors) and
236 GB RAM. We manually examined the bug reports generated by
AMOEBA and reported them to the developers for feedback.
5.3 RQ1 ‚Äî Performance Bugs Detection
AMOEBA found 25 and 14 PPBs in CockroachDB and PostgreSQL ,
respectively. Figure 5 summarizes the impact of the discovered
performance discrepancies (i.e., the performance gap between equiv-
alent query pairs in the bug report).
RUNTIME IMPACT .While PPBs found in CockroachDB exhibit a
slow-down ranging from 1.9 √óto 669.1√ó, those found in PostgreSQL
exhibit a slow-down between 1.9 √óto 555.6√ófor equivalent queries .
DEVELOPERS ‚ÄôFEEDBACK .Overall, developers have conÔ¨Årmed
6 bugs and Ô¨Åxed 5 among those that we reported. However, the
reaction was different between the developers of PostgreSQL and
CockroachDB . Of the 7 PPBs we reported to them, the PostgreSQL
developers considered 4 to be future/missing optimizations that they
did not plan to support at the moment, and 1 to be a false positive. Of
the remaining 2, they did not respond to 1, and the other 1 matched
a planned Ô¨Åx, which should at least indicate that it was considered
a critical optimization worth addressing. Notably, the PostgreSQL
developer who responded to our reports recommended that we listed
the future/missing optimizations that we identiÔ¨Åed and reported on
their ofÔ¨Åcial wiki page, which serves as a collaboration area for
PostgreSQL developers and users [ 10]. At the time of this writing,
we are in the process of creating and submitting this page.
The CockroachDB developers reacted more positively to our re-
ports, conÔ¨Årming 6 as performance bugs, assigning 18 reports totheir backlog, and classifying 1 report as a false positive. Amongthe conÔ¨Årmed reports, the
CockroachDB developers have Ô¨Åxed 5
(acknowledging 2 that matched planned Ô¨Åxes).
In summary, and according to the terminology we introduced in
Section 3, AMOEBA identiÔ¨Åed 1 unconÔ¨Årmed, previously known,
and Ô¨Åxed PPB,15 unconÔ¨Årmed PPBs, 1 of which is a false positive,
and 1 unknown PPB for PostgreSQL ; for CockroachDB ,AMOEBA
identiÔ¨Åed 6 conÔ¨Årmed performance bugs (among which 3 were
previously unknown and Ô¨Åxed, 2 were previously known and Ô¨Åxed,
1As we discussed above, this is a somehow peculiar case, as the developers did not
conÔ¨Årm that the reported PPB was an actual performance bug, but they had a Ô¨Åx for it
in the works.and 1 was previously unknown and not Ô¨Åxed), 18 backlogged PPBs,
and 1 unconÔ¨Årmed PPB, which is a false positive. We provide details
on all the reports submitted and on the developers‚Äô reactions to each
of them in our supplementary materials [12].
DESCRIPTION OF BUGS .We now discuss a subset of the PPBs
found by A MOEBA to illustrate the types of bugs it can Ô¨Ånd.
EXAMPLE 1: E XPRESSION SIMPLIFICATION .The pair of equiv-
alent queries below exhibit a 3.2√ó slow-down in CockroachDB.
/* [First query, 75 milliseconds] */
SELECT Max (emp.sal)
FROM dept INNER JOIN emp ONename NOT LIKE name
WHERE name = 'ACCT ';
/* [Second query, 238 milliseconds] */
SELECT Max (emp.sal)
FROM dept INNER JOIN emp ONename NOT LIKE name
WHERE name = 'ACCT 'ISTRUE ;
The performance difference is caused by the way the Ô¨Ålter predicate
is processed. For the Ô¨Årst query, the DBMS leverages information
from the predicate to simplify the JOIN condition, by replacing the
variable name with the value ‚ÄòACCT‚Äô. Because of the simpliÔ¨Åed
JOIN condition, the DBMS only needs a partial scan of the table
emp. Conversely, with the second query, the DBMS decides thatit cannot leverage the predicate information to simplify the JOIN
condition and scans the entire table emp. After analyzing this bug
report, the CockroachDB developers realized that a critical predicate
normalization rule was missing in their query optimizer. In particular,
if an expression within the predicate guarantees to yield a non-null
result (e.g., in our example, the non-nullable column name compares
with a string value), it is safe to reduce operations on top of it that
still take null value into consideration [ 36]. With the second query,
this rule would remove the ISTRUE check on top of the comparison
clause, which would lead to a more efÔ¨Åcient query execution plan.
The developers quickly Ô¨Åxed this performance bug due to its broad
impact on query performance.
EXAMPLE 2: S UB-QUERIES RETURNING A SCALAR .The pair
of equivalent queries below contain predicates that rely on results of
the same subquery.
/* [First query, 7 milliseconds] */
SELECT sal FROM emp LEFT OUTER JOIN (SELECT job FROM
bonus LIMIT 1)AStON true
WHERE t.job IS NOT DISTINCT FROM 'ACCT ';
/* [Second query, 211 milliseconds] */
SELECT sal FROM emp WHERE (SELECT job FROM bonus LIMIT
1) IS NOT DISTINCT FROM 'ACCT ';
However, when the predicate is false ( i.e., the Ô¨Årst tuple of job is not
‚ÄòACCT‚Äô), CockroachDB spends 30 √ómore time to execute the second
query compared to the Ô¨Årst one. The performance difference stems
from how the predicate is processed. For the Ô¨Årst query, the DBMS
realizes that the predicate in the JOIN operator evaluates to false, and
thus skips scanning the emp table and executing the JOIN operation.
For the second query, however, the DBMS ignores the predicate
result and scans the entire emp table anyway. The developers quickly
conÔ¨Årmed that this performance bug lies in the query executionengine. They also acknowledged that the bug belongs to a moreimportant limitation in the query optimizer, in that it cannot re-
optimize the main query based on the results of the sub-query. They
plan to Ô¨Åx this issue in the near future.
231
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
EXAMPLE 3: H ANDLING AGGREGATE OPERATORS .The pair of
equivalent queries below trigger a 2.9 √óexecution time difference on
PostgreSQL , which exposes a suboptimal behavior when handling
an unnecessary GROUP BY operator.
/* [First query, 25 milliseconds] */
SELECT emp_pk FROM emp WHERE emp_pk > 100;
/* [Second query, 72 milliseconds] */
SELECT emp_pk FROM emp WHERE emp_pk > 100 GROUP BY
emp_pk;
SpeciÔ¨Åcally, both queries request the DBMS to fetch the emp_pk
column based on the same predicate. The second query also appends
a GROUP BY operation before returning the Ô¨Ånal table. Since
emp_pk is the primary key of the emp table, the emp_pk column
takes unique values, thereby rendering the GROUP BY operation
unnecessary. For the second query, however, the DBMS performs the
GROUP BY operation anyway, which leads to a slower execution
time than in the case of the Ô¨Årst query. While the developers classiÔ¨Åed
this performance discrepancy as a missing optimization, rather than
an actual performance bug, they also mentioned that they were inthe process of producing a Ô¨Åx for it. As we mentioned above, this
seems to indicate that this performance issue was considered relevant
enough to be addressed.
DISCUSSION .The empirical results of applying AMOEBA to test
CockroachDB and PostgreSQL show that AMOEBA can effectively
detect PPBs. SpeciÔ¨Åcally, the semantically equivalent queries gen-
erated by AMOEBA do trigger different runtime behaviors in the
DBMSs considered, thereby allowing us to use them as a differential
performance oracles for Ô¨Ånding PPBs. In addition, we found that
the format of our bug reports (i.e., a pair of equivalent queries and
their execution plans) seems to provide sufÔ¨Åcient information for the
DBMS developers to reproduce and investigate the PPB.
By using the runtime performance of semantically equivalentqueries as a performance oracle,
AMOEBA discovered 39 PPBs
spread across different components of two DBMSs.
5.4 RQ2 ‚Äî EfÔ¨Åciency
To answer RQ2, we examined the computational efÔ¨Åciency of AMOEBA ,
as well as whether AMOEBA ‚Äôs feedback mechanisms increase the
probability of generating queries that discover PPBs.
In this part of the evaluation, we ran AMOEBA onCockroachDB
and PostgreSQL in four different conÔ¨Ågurations and with a timeout
of Ô¨Åve hours per conÔ¨Åguration:
(1) A MOEBA none : both feedback mechanisms disabled,
(2) A MOEBA validator : feedback from VA L I DAT O R enabled,
(3) A MOEBA mutator : feedback from MUTA TOR enabled, and
(4) A MOEBA both both feedback mechanisms enabled.
Figure 6 presents the results of this study, which consist of the
number of total and unique PPBs that A MOEBA discovered in each
conÔ¨Åguration. We manually mapped each performance bug report to
a corresponding unique bug based on the developers‚Äô feedback.
BUGREPORTS ,UNIQUE BUGS ,AND FALSE POSITIVES .We
examine the overall efÔ¨Åciency of AMOEBA by averaging the bug-
Ô¨Ånding results across four runs. As shown in Figure 6, AMOEBA
generated an average of 19 performance bug reports (corresponding
to 9 unique PPBs) for CockroachDB and an average of 46 PPBs(a)CockroachDB (b) PostgreSQL
Figure 6: EfÔ¨Åciency of AMOEBA ‚ÄìFigures (a) and (b) show the number
of total PPBs and unique PPBs that AMOEBA discovers after Ô¨Åve hours in
CockroachDB and PostgreSQL, respectively.
(corresponding to 7 unique PPBs) for PostgreSQL . Finally, among
the 182 reports generated for PostgreSQL , 5 are false positives (cor-
responding to 1 unique PPB).
IMPACT OF FEEDBACK MECHANISMS .To understand the im-
pact of the feedback mechanisms ( i.e., feedback from validator
and feedback from mutator) on the effectiveness of AMOEBA ,w e
compared the total and unique number of PPBs that AMOEBA dis-
covered in the four different conÔ¨Ågurations considered. As Figure 6
shows, for both DBMSs, the feedback from validator and mutator
increased both the total and unique performance discrepancies that
AMOEBA discovered. On CockroachDB , while A MOEBA none only
discovered 10 total performance discrepancies and 6 unique perfor-
mance discrepancies, the other conÔ¨Ågurations (i.e., A MOEBA validator ,
AMOEBA mutator , and A MOEBA both ) discovered 25, 22, and 19 total
performance discrepancies, respectively, which correspond to 12,
11, and 8 unique performance discrepancies. On PostgreSQL , while
AMOEBA none only discovered 20 total performance discrepancies
and 4 unique performance discrepancies, the other conÔ¨Ågurations dis-covered 82, 40, and 40 total performance discrepancies, respectively,
which correspond to 9, 7, and 8 unique performance discrepancies.
Based on these results, validator-only feedback seems to be more
effective than a combination of validator-and-mutator feedback (es-
pecially for CockroachDB ). One possible explanation is that lever-
aging both feedback mechanisms tends to reward a larger number
of features and result in more complex queries, which can make it
challenging for the DBMS to signiÔ¨Åcantly optimize either query in a
pair.
RUNTIME PERFORMANCE .We also counted the number of se-
mantically equivalent queries that AMOEBA examined across four
runs. On average, AMOEBA generated and examined a pair of seman-
tically equivalent queries per 1.5 and 0.8 seconds for CockroachDB
and PostgreSQL , respectively. The overall runtime performance of
AMOEBA was dominated by the runtime of the tested DBMS. To
further improve the runtime performance of AMOEBA , it would be
possible to deploy it on multiple servers to parallelize its execution.
AMOEBA detects a large number of PPBs in both CockroachDB
and PostgreSQL within the given time limit of 5 hours. On both
DBMSs, feedback from validator and mutator both had a signiÔ¨Å-
cant impact on the number of PPBs discovered.
232
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automatic Detection of Performance Bugs
in Database Systems using Equivalent QueriesICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA
Figure 7: Impact of rules on query performance and frequency of bug-
revealing query pairs.
5.5 RQ3 ‚Äî Effect of Individual Mutation Rules
To answer RQ3, we performed an in-depth analysis of mutation
rules used by AMOEBA and their importance in discovering PPBs
using the performance bug reports presented in ¬ß5.4. SpeciÔ¨Åcally, we
examined a dataset of 76 and 182 query pairs that triggered PPBs in
CockroachDB and PostgreSQL , respectively. We investigates each
mutation rule along two dimensions:
Impact on query performance: If a rule generated a query pair
that exhibited a signiÔ¨Åcant runtime performance difference, we mea-
sured the speed-up (ratio >1) or slow-down (ratio <1).
Frequency of generating bug-revealing query pairs: We counted
the number of times each rule generated a query pair that triggered a
PPB, normalized by the total number of bug-revealing pairs.
Figure 7 presents the results for both measures for CockroachDB .
The impact on query performance is shown using a box plot and
the y-axis on the left. The frequency of bug-revealing query pairs is
shown using a bar chart and the y-axis on the right. The results for
PostgreSQL are analogous.
As the Ô¨Ågure shows, not all mutation rules are equally useful in
discovering PPBs. Among 75 mutation rules that AMOEBA uses for
generating query pairs, only 39 and 44 rules can generate query pairs
that trigger PPBs in CockroachDB and PostgreSQL , respectively.
These subsets include both structure and expression mutation rules.
With respect to impact on performance, we found that while some
rules always had the same effect on query performance (i.e., either
speed-up or slow-down), others exhibited different effects ( i.e., both
speed-up and slow-down) in different cases. Since AMOEBA seeks
to mutate each query by applying a sequence of rules, these patterns
result from the compositional effects of mutation rules (¬ß4.3.2): (1)
contention between mutation rules, that is the performance penalty
caused by one rule is damped by the gains from another rule, and
vice versa; (2) enabling effect of mutation rules, that is a rule itself
may not affect the query performance but may transform the query
into a form that makes it suitable for mutation by other rules (with
an effect on performance).
We further studied our results to identify the characteristics of
the mutation rules that generate query pairs that can reveal PPBs, as
these may indicate query characteristics that make them challenging
for a DBMS to optimize and execute. For both DBMSs, we found
that these ‚Äúeffective‚Äù rules mostly re-arrange or eliminate expensive
operators ( UNION, GROUP BY, and JOIN) in a given query.
Conversely, other rules that manipulate the projection and sorting
operators ( SELECT and ORDER BY) are less likely to generate
query pairs that trigger PPBs. Improving how the DBMS handles
these operators would therefore enhance their robustness.Table 4: Comparative Analysis of AMOEBA ‚ÄìNumber of PPBs discovered
by the set of query pairs in each baseline and by A MOEBA .
BenchmarkPPBs Found
Cockroach PostgreSQL
Benchmark 1 (TLP) 1 1
Benchmark 2 (Calcite) 4 4
Benchmark 3 (Calcite+A MOEBA )4 6
AMOEBA 25 14
Both structural and expression mutation rules generate queries thatcan reveal PPBs. Rules differ in their impact on query performance,and some rules that do not directly affect query performance enablethe application of other rules. Rules that transform expensive oper-
ators seem to be effective in generating queries that trigger PPBs,
which highlights opportunities for improving future versions of
the tested DBMSs.
5.6 RQ4 ‚Äî Comparative Analysis
Query equivalence has been leveraged before in related work [ 16,
27,40], albeit not to uncover performance bugs. To answer Q4, we
compare AMOEBA to three baseline based on two of these existing
approaches: Calcite [19, 48] and TLP [38].
BASELINE 1: Q UERY PAIRS FROM TLP. TLP [ 38] construct
equivalent queries to discover logic bugs in DBMSs. It is based
on the observation that any predicate in SQL evaluates to TRUE,
FALSE, or NULL [ 38]. Accordingly, TLP constructs a mutant
query that is equivalent to the base query by (1) dividing the basequery into three partition queries, wherein each predicate is con-
structed based on the value of the overall base query‚Äôs predicate and
(2) concatenating these partition queries using the UNION oper-
ator. Our Ô¨Årst baseline consists of 2000 pairs of equivlent queries
generated using TLP .
BASELINE 2: Q UERY PAIRS FROM CALCITE .The Calcite test
suite [ 19,48] consists of tests manually crafted to ensure the correct-
ness of Calcite‚Äôs query transformation rules; each test transforms an
input query using a set of transformation rules and examines whether
the resulting query is correct. Our second baseline consists of 373
pairs of equivalent queries from the Calcite test suite, in which we
use the input query as the base query and the transformed query as
the mutant query. This is an appropriate and challenging baseline
because (1) the input queries are manually created to cover a wide
range of SQL operators, and (2) the mutant queries are generated
using the same set of transformation rules as A MOEBA .
BASELINE 3: Q UERY PAIRS FROM CALCITE USING AMOEBA
MUTATOR .Our Ô¨Ånal baseline consists of 373 pairs of equivalent
queries generated by applying AMOEBA ‚ÄôsMUTA TOR on the base
queries from the Calcite test suite.
COMPARISON .Once selected these three baselines, we ran each set
of baseline query pairs on our SCOTT-based database and comparedthe number of PPBs discovered by each baseline to those discovered
byAMOEBA . Our results are shown in Table 4. As the Ô¨Ågure shows,
AMOEBA discovered signiÔ¨Åcantly more PPBs than the baselines
in both DBMSs. We next analyze the factors that contribute to the
efÔ¨Åcacy of A MOEBA .
233
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
MUTATION RULES AND ALGORITHM .TLP uses a different set
of mutation rules than AMOEBA for generating equivalent queries,
which were not designed to detect performance bugs. The queries
mutated by TLP consistently take longer to execute than their cor-
responding base query, with an average slow-down of 17 √ó. This
happens because the mutated queries force the DBMS to perform ad-
ditional operations (i.e., fetching the tuples for each partition query
and combining the partial results). Given this inherent overhead,
TLP is limited in the kinds and number of PPBs it can Ô¨Ånd.
While Calcite and AMOEBA use the same set of mutation rules,
they differ in the way they leverage these rules. Calcite ‚Äôs tests trans-
form the base queries using a small set of mutation rules applied in a
speciÔ¨Åc order. Conversely, AMOEBA ‚Äôs automated mutation strategy
exploits all available mutation rules and their compositional effects
to generate equivalent query pairs, which increases the chances of
generating queries that can discover PPBs (¬ß4.3.2). The effect of
this different approach can be observed in the second and third rows
of Table 4, for PostgreSQL , where AMOEBA ‚Äôs mutation strategy
discovers two more PPBs than the manual mutation in Calcite.
For both DBMSs considered, AMOEBA discovered considerably
more PPBs than the baselines based on Calcite and TLP . Two
reasons for this better performance are that AMOEBA (1) uses
a rule mutation approach tailored to discovering PPBs and (2)performs a broad exploration of the equivalent query space by
leveraging all available rules and their compositional effects.
5.7 RQ5 ‚Äî Analysis of Base Queries
As shown in Table 4, the base queries derived from the Calcite test
suite allowed for discovering fewer PPBs than those generated by
AMOEBA . To understand why, we compared a set of 2000 base
queries generated by A MOEBA to the 373 base queries in Calcite.
SINGLE -CLAUSE ANALYSIS .We Ô¨Årst examined the SQL single-
clause and type coverage in the two sets. We found that the base
queries for AMOEBA and Calcite cover almost the same set of SQL
types and operators (e.g., join operators and common keywords).
Since Calcite ‚Äôs base queries are less effective than AMOEBA ‚Äôs, we
inferred that targeting high single-clause coverage when generating
queries is not sufÔ¨Åcient for detecting PPBs. We then investigated the
importance of considering combinations of different SQL clauses.
TWO-CLAUSE COMBINATION ANALYSIS .We examined two-
clause combination coverage for both sets of base queries ( e.g., exis-
tence of base queries with both GROUP BY and LEFT JOIN). T o
do this, we constructed a co-occurrence matrix using the base query
dataset and SQL clauses supported by both AMOEBA and Calcite .
For each base query dataset, we counted the frequency of queries
with two SQL clauses, normalized the count by the number of total
queries, and plotted the result in a heatmap, shown in Figure 8. Addi-
tionally, we identiÔ¨Åed clause-pair combinations within base queries
that triggered performance bugs (¬ß5.4), and highlighted those com-
binations that are only found by AMOEBA using a‚àómarker in the
heatmap. We only present the heatmap for CockroachDB , as the
one for PostgreSQL is similar. Because AMOEBA -generated base
queries lead to the discovery of more performance discrepancies,
we believe these highlighted combinations may represent interesting
two-clause combinations that are more likely to trigger PPBs.
Figure 8: Two-Clause Combinations ‚Äì AMOEBA -generated base queries
cover more clause pair combinations and interesting patterns (highlighted
with the‚àómarker) than Calcite.
The results in the heatmap show that, for both DBMSs considered,
the base queries generated by AMOEBA explore a signiÔ¨Åcantly larger
space of two-clause combinations than Calcite ‚Äôs base queries. While
base queries from Calcite ‚Äôs tests cover 103 and 106 clause pair
combinations for CockroachDB and PostgreSQL , base queries from
AMOEBA cover 208 and 200 combinations for the two DBMSs.
POSSIBLE IMPLICATIONS FOR DBMS DEVELOPERS .Our re-
sults also show that the base queries from Calcite missed a signiÔ¨Åcant
amount of interesting clause-pair combinations that lead to discover
PPBs. SpeciÔ¨Åcally, they missed 58 and 52 clause pair combinations
for CockroachDB and PostgreSQL . We summarize the character-
istics of these clause pairs, as they may reÔ¨Çect query patterns thatare challenging for DBMS to optimize but are neglected by man-ual testing efforts: (1) Ô¨Ålter clauses (i.e., WHERE and HA VING),
when combined with expensive operators such as JOIN, GROUP
BY, and UNION, can have a signiÔ¨Åcant effect on query execution
time; (2) the LIMIT clause can also affect query execution time.
Because LIMIT requests a smaller set of results, an optimal plan
should either scan a partial table or terminate expensive operations
early. Improving how the DBMSs handle these operations and their
combinations may enhance the robustness of its performance.
An additional reason why AMOEBA outperforms the baselines
considered, in addition to those discussed in ¬ß5.7, is that it covers
a wider range of clause-pair combinations that may be challenging
for the DBMS to optimize.
6 Limitations
In this section, we discuss the main limitations of AMOEBA and
present possible ways to address and mitigate them in future work.
RELIANCE ON AN EXISTING OPTIMIZATION FRAMEWORK .
Because AMOEBA is based on the widely-used query optimization
framework Calcite , it inherits Calcite ‚Äôs limitations. First, Calcite
only focuses on a widely-supported subset of SQL operators and
functions, so AMOEBA focuses on the same subset. However, since
Calcite is an extensible framework, it is feasible to add support
234
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automatic Detection of Performance Bugs
in Database Systems using Equivalent QueriesICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA
for additional SQL features. Second, AMOEBA leverages rewrite
rules from Calcite . Although these rules are designed to preserve
query semantics [ 23],AMOEBA could generate false positives if
for some reason Calcite ‚Äôs rules failed to satisfy this property. To
mitigate this threat, AMOEBA generates a bug report only if the two
(supposedly) equivalent queries return the same set of output tables,
as it is unlikely that queries that are not equivalent would generate
the same results.
PERFORMANCE BUGS VS MISSING OPTIMIZATIONS .While
AMOEBA can identify relevant cases that developers should consider,
it is ultimately the developers‚Äô decision whether to support a given
optimization. In fact, as discussed in ¬ß5.3, the PostgreSQL develop-
ers considered most of the the reported issues missing optimizations
that they decided to ignore. In some cases, this happens because
Ô¨Ånding the optimal execution plan is an NP-hard problem [ 25], so
a DBMS may select a sub-optimal plan to reduce computationalcost. In other cases, it is simply a design decision. Nevertheless,
we believe it is useful to have an automated tool that can report to
developers cases that may need consideration and assists them in
pinpointing the root cause of a performance discrepancy.
Related to this point, further analysis of the responses from the
PostgreSQL developers made us realize that a possible limitation of
this approach is that the automated query generation and mutation
may result in queries that look artiÔ¨Åcial and may therefore alienate
the developers rather than compel them to investigate and Ô¨Åx thecorresponding issues. To address this problem, in future work, we
will investigate ways to simplify the automatically generated query
pairs and make them more representative of queries that can be
encountered in practice.
Finally, it is also worth noting that a byproduct of this work is
that it shows that SQL is not fulÔ¨Ålling one of its key promises‚Äîthat
developers can write queries in any form and leave optimization to
the DBMS.
7 Related Work
In this section, we present prior work on testing DBMSs with an
emphasis on DBMS performance.
FUZZING DBMS S.Given the large state space of possible SQL
queries, fuzzing has been applied to Ô¨Ånd crash bugs and security
vulnerabilities in DBMSs [ 2‚Äì4]. Researchers have improved the
efÔ¨Åcacy of the fuzzing loop by taking the feedback from the tested
DBMS into consideration [ 13,47]. While AMOEBA is also a fuzzing
tool equipped with a feedback mechanism, it differs from prior work
in that it focuses on generating semantically equivalent query pairs
that trigger different runtime performance.
DIFFERENTIAL AND METAMORPHIC TESTING .To circumvent
the oracle problem associated with automated testing, researchers
have applied differential and metamorphic testing techniques for dis-
covering logic bugs in DBMSs [ 18,31].RAGS discovers logic bugs
by executing the same query on different DBMSs and comparing theresults [
42]. Waas et al. propose a framework for validating the query
optimizer by executing alternative execution plans for the input query
and comparing their results [ 43]. TLP is the state-of-the-art tool for
discovering logic bugs in DBMS using metamorphic testing [ 38].
However, as discussed in ¬ß5.6, TLP is not suitable for discoveringperformance bugs. Unlike this previous work, AMOEBA is a meta-
morphic testing technique tailored for discovering performance bugs
in DBMS.
PERFORMANCE TESTING .Researchers have presented techniques
for Ô¨Ånding performance bugs by executing the DBMS on pre-deÔ¨Åned
workloads and comparing their behavior against performance base-
lines [ 26,35,45,46]. These techniques detect performance re-
gressions caused by DBMS upgrades and conÔ¨Åguration changes.
AMOEBA differs from these approaches in that it does not require a
pre-deÔ¨Åned baseline for Ô¨Ånding performance bugs. Instead, it lever-
ages the tested DBMS‚Äôs runtime behaviors on equivalent queries as
a performance oracle.
OPTIMIZER TESTING .Researchers have proposed techniques
for testing the query optimizer‚Äôs ability to Ô¨Ånd the best executionplan [
21,24]. Li et al. propose a benchmark for assessing the efÔ¨Å-
ciency of a query optimizer (i.e., optimization time) [ 30]. Leis et al.
investigate the impact of the components of the query optimizer on
runtime performance [ 29]. These efforts are geared towards quanti-
fying the quality of an optimizer. Another line of research focuses on
developing frameworks for testing the correctness of query transfor-
mation rules in the query optimizer [ 20,43]. This work requires an
in-depth knowledge about the tested query optimizer. AMOEBA com-
plements these efforts by taking a black-box approach and facilitates
a more extensive testing of optimizers.
Acknowledgments
This work was partially supported by NSF, under grants CCF-1563991, CCF-
0725202, IIS-1850342, and IIS-1908984, DARPA, under contract N66001-
21-C-4024, ONR, under contract N00014-18-1-2662, DOE, under contract
DE-FOA-0002460, Adobe, the Alibaba Innovative Research Program, Cisco,
Facebook, Google, IBM Research, Intel, and Microsoft Research. We thank
the developers at CockroachDB and PostgreSQL for their useful feedback
on our bug reports.
8 Conclusion
We presented AMOEBA , a new approach for detecting performance
bugs in DBMSs. The key idea behind AMOEBA is to construct two
semantically equivalent queries and then compare the time it takes
the DBMS under test to execute the two queries. If the execution timefor the two queries is signiÔ¨Åcantly different, that indicates a potential
performance bug in the DBMS. In order to boost the effectiveness
and efÔ¨Åciency of AMOEBA , we also deÔ¨Åned a query generation and
two feedback mechanisms that allow it to focus on the subset of the
query space that is more likely to uncover performance bugs. To
assess our approach, we implemented AMOEBA and evaluated it on
two widely-used DBMSs with encouraging results. AMOEBA was
able to discover 39 potential performance bugs. Developers already
conÔ¨Årmed 6 of these bugs and Ô¨Åxed 5 of them.
In future work, we plan to apply AMOEBA to additional DBMSs
and to improve our approach based on our current and future Ô¨Åndings.
Our current results, for instance, highlight relevant query patternsthat DBMS may have difÔ¨Åculties processing efÔ¨Åciently. We canuse this information to improve
AMOEBA by adding to it more
rules that focus on such patterns. We will also investigate debuggingtechniques that can help DBMS developers investigate the root cause
of a performance bug after it has been reported.
235
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, P A, USA Xinyu Liu‚àó, Qi Zhou‚Ä†, Joy Arulraj‚àó, Alessandro Orso‚àó
References
[1] 1992. Database Language SQL. http://www.contrib.andrew.cmu.edu/~shadow/sql
/sql1992.txt.
[2] 2015. AFL: American Fuzzy Lop. http://lcamtuf.coredump.cx/afl/.
[3] 2016. OSS-Fuzz: Continuous Fuzzing for Open Source Software. https:
//github.com/google/oss-fuzz.
[4] 2016. SQLSmith. https://github.com/anse1/sqlsmith.[5] 2020. Apache Calcite. https://calcite.apache.org/.[6] 2020. CockroachDB. https://www.cockroachlabs.com/docs/releases/v20.2.0.[7] 2020. SQLAlchemy. https://www.sqlalchemy.org/.[8]
2021. CockroachDB Performance Bug Reports. https://github.com/cockroachdb
/cockroach/issues?q=performance.
[9] 2021. PostgreSQL Performance Bug Reports. https://www.postgresql.org/searc
h/?m=1&q=performance&l=8&d=-1&s=r.
[10] 2021. PostgreSQL Wiki. https://wiki.postgresql.org/wiki/Main Page.
[11] 2021. SCOTT schema. https://www.orafaq.com/wiki/SCOTT.[12] 2022. Supplementary material. https://bit.ly/3I995jL[13]
Hardik Bati, Leo Giakoumakis, Steve Herbert, and Aleksandras Surna. 2007. A
genetic approach for random testing of database systems. In VLDB. 1243‚Äì1251.
[14] Nicolas Bruno, Surajit Chaudhuri, and Dilys Thomas. 2006. Generating Queries
with Cardinality Constraints for DBMS Testing. In TKDE. 1721‚Äì1725.
[15] Stefano Ceri and Georg Gottlob. 1985. Translating SQL into relational algebra:
Optimization, semantics, and equivalence of SQL queries. In TSE. 324‚Äì345.
[16] Bikash Chandra, Bhupesh Chawda, Biplab Kar, KV Maheshwara Reddy, Shetal
Shah, and S Sudarshan. 2015. Data generation for testing and grading SQL queries.
InVLDB Journal. 731‚Äì755.
[17] CL Philip Chen and Chun-Y ang Zhang. 2014. Data-intensive applications, chal-
lenges, techniques and technologies: A survey on Big Data. In Information sci-
ences. 314‚Äì347.
[18] Tsong Y Chen, Shing C Cheung, and Shiu Ming Yiu. 2020. Metamorphic testing: a
new approach for generating next test cases. In arXiv preprint arXiv:2002.12543.
[19] Shumo Chu, Chenglong Wang, Konstantin Weitz, and Alvin Cheung. 2017.
Cosette: An Automated Prover for SQL.. In CIDR.
[20] Hicham G Elmongui, Vivek Narasayya, and Ravishankar Ramamurthy. 2009. A
framework for testing query transformation rules. In SIGMOD. 257‚Äì268.
[21] Leo Giakoumakis and C√©sar A Galindo-Legaria. 2008. Testing SQL Server‚Äôs
Query Optimizer: Challenges, Techniques and Experiences. In Data Engineering
Bulletin. 36‚Äì43.
[22] Goetz Graefe. 1993. Query Evaluation Techniques for Large Databases. In CSUR.
73‚Äì169.
[23] Goetz Graefe. 1995. The cascades framework for query optimization. In Data
Engineering Bulletin. 19‚Äì29.
[24] Zhongxian Gu, Mohamed A. Soliman, and Florian M. Waas. 2015. Testing the
accuracy of query optimizers. In DBTest. 1‚Äì6.
[25] Toshihide Ibaraki and Tiko Kameda. 1984. On the optimal nesting order for
computing n-relational joins. In TODS. 482‚Äì502.
[26] Jinho Jung, Hong Hu, Joy Arulraj, Taesoo Kim, and Woonhak Kang. 2019. Apollo:
Automatic detection and diagnosis of performance regressions in database systems.
InVLDB. 57‚Äì70.[27] R Kearns, Stephen Shead, and Alan Fekete. 1997. A teaching system for SQL. In
ACSE. 224‚Äì231.
[28] D. Lee, S. K. Cha, and A. H. Lee. 2012. A Performance Anomaly Detection and
Analysis Framework for DBMS Development. In TKDE. 1345‚Äì1360.
[29] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper,
and Thomas Neumann. 2015. How good are query optimizers, really?. In VLDB.
204‚Äì215.
[30] Zhan Li, Olga Papaemmanouil, and Mitch Cherniack. 2016. OptMark: A Toolkit
for Benchmarking Query Optimizers. In CIKM. 2155‚Äì2160.
[31] William M McKeeman. 1998. Differential testing for software. In Digital Techni-
cal Journal. 100‚Äì107.
[32] Rasha Osman and William J. Knottenbelt. 2012. Database System Performance
Evaluation Models: A Survey. In Performance Evaluation. 471‚Äì493.
[33] Jaroslav Pokorn `y. 2015. Database technologies in the world of big data. In
CompSysTech. 1‚Äì12.
[34] Raghu Ramakrishnan and Johannes Gehrke. 2003. Database Management Sys-
tems.
[35] Kim-Thomas Rehmann, Changyun Seo, Dongwon Hwang, Binh Than Truong,
Alexander Boehm, and Dong Hun Lee. 2016. Performance Monitoring in SAP
HANA ‚Äôs Continuous Integration Process. In PER. 43‚Äì52.
[36] Raymond Reiter. 1986. A Sound and Sometimes Complete Query Evaluation
Algorithm for Relational Databases with Null V alues. In J. ACM . 349‚Äì370.
[37] Manuel Rigger and Zhendong Su. 2020. Detecting Optimization Bugs in Database
Engines via Non-Optimizing Reference Engine Construction. In FSE.
[38] Manuel Rigger and Zhendong Su. 2020. Finding Bugs in Database Systems via
Query Partitioning. In OOPSLA.
[39] Manuel Rigger and Zhendong Su. 2020. Testing Database Engines via Pivoted
Query Synthesis. In OSDI.
[40] Shetal Shah, S Sudarshan, Suhas Kajbaje, Sandeep Patidar, Bhanu Pratap Gupta,
and Devang Vira. 2011. Generating test data for killing SQL mutants: A constraint-
based approach. In ICDE. 1175‚Äì1186.
[41] Hitesh Kumar Sharma, Mr SC Nelson, et al .2017. Explain Plan and SQL Trace
the Two Approaches for RDBMS Tuning. In Database Systems Journal. 31‚Äì39.
[42] Donald R Slutz. 1998. Massive Stochastic Testing of SQL. In VLDB. 618‚Äì622.
[43] Florian Waas and C√©sar Galindo-Legaria. 2000. Counting, Enumerating, andSampling of Execution Plans in a Cost-Based Query Optimizer. In SIGMOD.
499‚Äì509.
[44] Gerhard Weikum, Axel Moenkeberg, Christof Hasse, and Peter Zabback. 2002.
Self-Tuning Database Technology and Information Services: From Wishful Think-
ing to Viable Engineering. In VLDB. 20‚Äì31.
[45] Khaled Y agoub, Peter Belknap, Benoit Dageville, Karl Dias, Shantanu Joshi, and
Hailing Y u. 2008. Oracle‚Äôs SQL Performance Analyzer.. In Data Engineering
Bulletin. 51‚Äì58.
[46] Jiaqi Y an, Qiuye Jin, Shrainik Jain, Stratis D Viglas, and Allison Lee. 2018.
Snowtrail: Testing with Production Queries on a Cloud Database. In DBTEST.
[47] Rui Zhong, Y ongheng Chen, Hong Hu, Hangfan Zhang, Wenke Lee, and Dinghao
Wu. 2020. SQUIRREL: Testing Database Management Systems with Language
V alidity and Coverage Feedback. In CCS. 955‚Äì970.
[48] Qi Zhou, Joy Arulraj, Shamkant Navathe, William Harris, and Dong Xu. 2019.
Automated veriÔ¨Åcation of query equivalence using satisÔ¨Åability modulo theories.
InVLDB. 1276‚Äì1288.
236
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. 