Preempting Flaky Tests via Non-Idempotent-Outcome Tests
Anjiang Wei
Stanford University
anjiang@stanford.eduPu Yi
Peking University
lukeyi@pku.edu.cnZhengxi Li
University of Illinois
zli89@illinois.edu
Tao Xie
Peking University
taoxie@pku.edu.cnDarko Marinov
University of Illinois
marinov@illinois.eduWing Lam
George Mason University
winglam@gmu.edu
ABSTRACT
Regression testing can greatly help in software development, but it
can be seriously undermined by flaky tests, which can both pass
and fail, seemingly nondeterministically, on the same code commit.
Flaky tests are an emerging topic in both research and industry.
Priorworkhasidentifiedmultiplecategoriesofflakytests,devel-
oped techniques for detecting these flaky tests, and analyzed some
detected flaky tests.
To proactively detect, i.e., preempt, flaky tests, we propose to
detectnon-idempotent-outcome(NIO) tests,anovelcategoryrelated
to flaky tests. In particular, we run each test twice in the same
test execution environment, e.g., run each Java test twice in thesame Java Virtual Machine. A test is NIO if it passes in the first
run but fails in the second. Each NIO test has side effects and â€œself-
pollutesâ€thestatesharedamongtestruns.Weperformexperiments
on both Java and Python open-source projects, detecting 223 NIO
Java tests and 138 NIO Python tests. We have inspected all 361
detected tests and opened pull requests that fix 268 tests, with 192
already accepted, only 6 rejected, and the remaining 70 pending.
ACM Reference Format:
AnjiangWei,PuYi,ZhengxiLi,TaoXie,DarkoMarinov,andWingLam.
2022. Preempting Flaky Tests via Non-Idempotent-Outcome Tests. In 44th
InternationalConferenceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,
Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.
1145/3510003.3510170
1 INTRODUCTION
Nondeterministic tests that can pass or fail for the same version of
thecodeundertestareknownbymultiplenames.Practitionersand
researchers most often call these tests â€œflakyâ€ [ 37,44,65,77,91]
but also call them â€œflappersâ€ [ 33], â€œunreliable testsâ€ [ 40], â€œbrittle
assertionsâ€[ 48],â€œnondeterministictestsâ€[ 35],â€œerratictestsâ€[ 70],
andmore.Inthispaper,weusetheterm flakytests.Flakytestshave
beenreportedasanimportantprobleminacademicresearch(e.g.,atleastsevenpapersfrom2021analyze[
27,30,39,85]anddetect[ 21,
62,84] flaky tests) and in both â€œgrey literatureâ€ (e.g., blogs from
Gradle [94], Salesforce [ 33], and Thoughtworks [ 35]) and research
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510170papersbyvariouscompanies(e.g.,byApple[ 55],Facebook[ 34,44],
Google [19,38,69,71,100], Huawei [ 49], Microsoft [ 46,47,56,57],
and Mozilla [83, 90]).
One well-studied category [ 23,36,40,48,58,65,76,88,99]o f
flaky tests are order-dependent (OD) tests [99], whose outcome
depends on the order in which tests are run; OD tests occur in
twomajorsituations.First,testingframeworks,suchasJUnit,do
notmandatetheorderinwhichtestsarerun,andtestsuitesthat
pass in one order can start failing when run in another order. A
notoriousexampleoccurredwhentheJavastandardlibrarychanged
from Java 6 to Java 7: many test suites that used to pass started
failing, resulting in many publicly reported complaints [ 52,53,66].
Second,testscanrunindifferentordersduetotheuseofregression
testing techniques [ 59], such as test prioritization [ 32,64,86], test
selection [45, 96, 97], and test parallelization [26, 50, 54, 89].
The terminology on OD tests is somewhat confusing as prior
papers[23,36,40,48,58,65,76,88,99]usedthesametermwithdif-
ferentmeaningsorintroducednewtermsforsame/similarconcepts.
Wefollowthemostrecentlyusedterminology[ 39,88].Following
Shietal.[ 88],wecallatesta victimforagiventestsuite(e.g., t1in
Figure1)ifthetest failswhenrun afteranothertest,calleda polluter,
inthesametestsuite(e.g., t2inFigure1)but passeswhenrun before
that other test. The victim fails because the tests share some state
(xin Figure 1), and the polluter modifies (i.e., â€œpollutesâ€ [ 40,88])
thesharedstate.HuoandClause[ 48]calledthetestassertionsthat
dependonthesharedstateâ€œbrittleassertionsâ€.Eachvictimhasat
leastonebrittleassertion,butnotalltestswithabrittleassertion
are victims (e.g., t3in Figure 1 has a brittle assertion, but no test
pollutes z). We call a test a latent-victim if it has a brittle assertion
but may or may not be a victim in the current test suite.
Note that a polluter is defined with respect to a given test suite,
where the test suite has a victim. Gyori et al. [ 40] used the term
â€œpolluterâ€torefertoanytestthatchangessomesharedstateevenif
ithasnovictiminthecurrenttestsuite(e.g., t4inFigure1modifies
ybut no test fails because of that). To avoid confusion, we use
latent-polluter torefertoatestthatmodifiesthesharedstatebut
mayormaynothaveavictiminthecurrenttestsuite.Following
Musuvathi et al. [ 74], a latent-polluter can be also called a â€œnon-
idempotent-statetestâ€,becausethetestdefinitelymodifiesthestate,butrunningthetesttwicemayormaynothaveadifferentbehavior.
To reduce the risk that flaky tests fail at inopportune times,
practitioners[ 44,90]andresearchers[ 40,48,61]haveadvocated
for proactively detecting potential flaky tests, i.e., preempting them
from becoming flaky. For example, to preempt OD-related tests,Huo and Clause [
48] proposed using dynamic taint analysis to
detect latent-victims, and Gyori et al. [ 40] proposed monitoring
17302022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Pu Yi, Zhengxi Li, Tao Xie, Darko Marinov, and Wing Lam
1// shared variables x, y, z, w are initialized to 0
2voidt1() { assert x == 0; } // victim
3voidt2() { x = 1; } // polluter
4voidt3() { assert z == 0; } // latent-vi ctim
5voidt4() { y = 1; } // latent- polluter
6voidt5() { assert w == 0; w = 1; } // NIO
Figure 1: Example test suite containing different kinds of
victims and polluters, including an NIO test.
the shared heap state and file system to detect latent-polluters.
However,whiledetectinglatent-victimsandlatent-polluters,the
keyistobalancedetectingasmanytestsaspossiblewithdetecting
tests that are worth fixing. For example, Gyori et al. [ 40, Figure
4] reported many false positives: they automatically found 575
latent-polluters, manually inspected all and filtered out 381 tests
that cannot reasonably become polluters (e.g., modify state that
cannotbeobservedviaanypublicAPIbutonlythroughreflection),
and did not fix any of the remaining 194 tests.
Todetectlatent-victimsandlatent-pollutersthatareworthfixing,
we propose to focus on non-idempotent-outcome (NIO) tests, which
are related to OD tests [ 99] and similar to â€œunrepeatable testsâ€ [ 70].
A test is an NIO test if the test outcome (pass or fail) changes after
repeated test runs, due to the changes of the state shared amongruns of the NIO test (e.g.,
t5in Figure 1). For a test ğ‘¡to be an
NIO test, ğ‘¡must write and read some shared state ( win Figure 1).
Detecting ğ‘¡canbehelpfulbecauseanODtestcanemergewhen ğ‘¡is
run together with anothertest (from the current or future versions
of theğ‘¡â€™s test suite), where that other test writes or reads a part of
thestatesharedwith ğ‘¡.Iftheothertestwritestothesameshared
state, then ğ‘¡can become flaky/victim; if the other test reads the
shared state, then ğ‘¡can become a polluter with the other test as
flaky/victim. Figure 2 shows the relationship of multiple categories
of OD-related tests.
NIO tests are important to detect because they may be more
worthyfixingthanotherlatent-victimsandlatent-polluters,con-
sidering that NIO tests are bothlatent-victims andlatent-polluters
at the same time. In contrast to many false positives (66%) that
Gyori et al. [ 40] reported for latent-polluters, with no pull requests
opened, we find that developers are receptive to our NIO test fixes.
While debugging (NIO tests in) unfamiliar projects can be time-
consuming,wehavefixedmanyNIOteststhatwehavedetected
in open-source projects. Specifically, we have opened pull requests
for 268 tests. Developers have accepted fixes for 192 of the tests
withonly6rejected,andtheremaining70pending.(Oneproject
is an outlier as we have opened fixes for 120 NIO tests, and the
developers have accepted our fixes for all 120 tests.)
To detect NIO tests, we use a simple idea: run each test twice
in the same test execution environment to check whether thetest passes in the first run but fails in the second run. As prior
work on flaky tests has been mostly for Java [ 77] and recently for
Python[39],weperformourevaluationonopen-sourceJavaand
Pythonprojects;thegeneralprincipleeasilyextendstootherlan-
guages.Weruneachtesttwiceinthesameexecutionenvironment,
aJavaVirtualMachine(JVM)oraPythoninterpreterforJavaorPython tests, respectively. A test is categorized as NIO if the test
outcomechanges deterministically fromâ€œpassâ€toâ€œfailâ€.Morefor-
mally, we require that there existsa test order /angbracketleft...,ğ‘¡,ğ‘¡,... /angbracketrightsuch
that running the test ğ‘¡results in the outcomes â€œpassâ€ and â€œfailâ€.
Figure2:Relationshipofpolluters,victims,latent-polluters,latent-victims, and NIO tests.
TodetectNIOtestsforJavaprojects,weevaluatethreemodes
thatruntestsinisolationortogether.Onemodererunsonly onetest
methodin thesame execution environment, anothermode reruns
test methods from only one class, and the third mode reruns all
test methods from the entire test suite. We modify the iDFlakies
tool [58] to support running the same test twice withoutchanging
thetestcode.Bydefault,JUnitdoes notrunthesametest method
twice. Hence, prior tools for flaky tests (including DTDetector [ 99],
ElectricTest [ 24], iDFlakies [ 58], iFixFlakies [ 88], and PraDet [ 36])
do not contain this feature.
Ourevaluationon127testsuitesfromopen-sourceJavaprojects
detects 223 NIO tests in 34 of the test suites. The three modesdetect only slightly different tests, but rerunning all tests from
theentiretestsuiterunsthefastest.Weapplythatmodeon1006
Python projects and detect 138 NIO tests in 90 projects. Of the 361
detectedNIOtests,42arenotNIOinthelatestversion(i.e.,already
fixed or deleted). We have inspected in detail all remaining 319
testsandopenedpullrequestsfor84%(268/319)ofthetests.Each
test requires building the project, running the test, and debugging
thesharedstateâ€”usuallyrequiringatleastanhourfrommultiple
authors. Section 4.4 discusses our experience of fixing NIO tests
and Section 5 presents multiple real cases of our fixes.
In this paper, we make the following main contributions:
â€¢NIO Tests. We define NIO tests as tests that deterministi-
callychangeoutcomefromâ€œpassâ€toâ€œfailâ€whenruntwice
in the same execution environment; NIO tests are in the
intersection of latent-polluters and latent-victims.
â€¢Effective Detection. We propose three modes to detect
NIO tests by repeatedly running tests, either in isolation ortogether, in the same execution environment.
â€¢Empirical Evaluation.
We evaluate all modes on 127 Java
test suites and detect 223 NIO tests. We also evaluate themost effective mode on 1006 Python projects and detect
additional 138 NIO tests.
â€¢Real Cases. WepresentmultiplerealcasesofNIOteststo
illustratethespecificcausesthatmakethemNIOanddiscuss
our experience in fixing NIO tests.
â€¢Well-Accepted Fixes. We have opened pull requests that
fix 268 tests, with 192 of them accepted, only 6 rejected, and
the remaining 70 pending.
Our dataset and scripts are publicly available [18].
2 BACKGROUND, MODES, AND EXAMPLES
NIOtestsarerelatedtoODtests,whichcanpassorfailbasedon
the order of the tests in the test suite. Section 1 has introduced
the most common kinds of OD-related tests. We introduce one less
common kind here. OD tests deterministically fail when run in
some pre-states. (A test that fails for all pre-states is a broken, not
1731
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. Preempting Flaky Tests via Non-Idempotent-Outcome Tests ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
flaky, test.) Beyond victims and polluters (defined in Section 1), Shi
et al.[88] defineda brittleas atest thatfails evenwhen run inthe
starting execution environment state (i.e., a brittle ğ‘fails in the
order/angbracketleftğ‘/angbracketright) and has at least one test ğ‘ in the test suite such that ğ‘ 
sets the state for the brittle to pass (i.e., ğ‘passes in the order /angbracketleftğ‘ ,ğ‘/angbracketright).
In contrast, victims pass in the starting state (i.e., a victim ğ‘£passes
intheorder /angbracketleftğ‘£/angbracketright)butfailafterapolluter ğ‘(i.e.,ğ‘£failsintheorder
/angbracketleftğ‘,ğ‘£/angbracketright); victims are much more common than brittles. In all prior
work [77], various tests in the same order were all differenttests
because each test was always run only once in a test order.
Incontrast,NIOtestsstemfromrepeatingthe sametest.Inthe
simplestcase, onlyonetest ğ‘¡isrun twice,i.e.,the orderis /angbracketleftğ‘¡,ğ‘¡/angbracketright.If
thefirstrunfails,thetestisabrittle.However,ifthefirstrunpasses
and the second run fails, the test is NIO. Note that each NIO test
is, by definition, botha latent-victim and a latent-polluterâ€”it â€œself-
pollutesâ€ the state on which it depends. Moreover, some NIO tests
maybevictimsorpollutersintheirtestsuite,butourevaluation
for Java tests shows that most of the NIO tests, 87.4% (195/223), are
neitherpolluters norvictims in their test suite.
2.1 Three Modes for Detecting NIO Tests
Weevaluatethree modestodetectNIOtests.Eachmoderepeatedly
runs (1) just a particular test method from a test class (isolated-
method), (2) all the test methods from a test class (isolated-class ), or
(3) all the test methods from a test suite (entire-suite ). For example,
consideratestsuitewithtwotestclasses, ğ¶andğ·,andthreetests1
ğ¶.ğ‘¡,ğ¶.ğ‘¢,andğ·.ğ‘£withtheirexplicitlylistedclasses:isolated-method
repeatedly runs each test in its own VM2twice, e.g., /angbracketleftğ¶.ğ‘¡,ğ¶.ğ‘¡/angbracketright;
isolated-class runs all tests from each test class in one VM, e.g.,
/angbracketleftğ¶.ğ‘¡,ğ¶.ğ‘¡,ğ¶.ğ‘¢,ğ¶.ğ‘¢ /angbracketright;entire-suiterunsalltestsfromthetestsuitein
one VM, e.g., /angbracketleftğ¶.ğ‘¡,ğ¶.ğ‘¡,ğ¶.ğ‘¢,ğ¶.ğ‘¢,ğ·.ğ‘£,ğ·.ğ‘£ /angbracketright.
Different modes for detecting NIO tests could have trade-offs in
termsoftheteststhattheydetectormiss,andhowfasttheyrun.
Comparedtoisolated-classandentire-suite,isolated-methodwould
miss detecting a test ğ‘¡as NIO if another test, ğ‘¡/prime, sets/pollutes the
statesothatrunning /angbracketleftğ‘¡/prime,ğ‘¡,ğ‘¡/angbracketrightmakesthesecond ğ‘¡fail,whilerunning
just/angbracketleftğ‘¡,ğ‘¡/angbracketrightmakes both runs pass. In our experiments (Section 4),
we find that isolated-method does notmiss detecting any NIO
testthatisolated-classandentire-suitedetect.However,isolated-
method needs to create a new VM for every test, so this mode
can run substantially slower than the other modes. Some prior
projects [ 22,75] did compare running Java tests isolated in JVM vs.
alltogetherinoneJVM,butthoseprojectswere notrepeatingtests
trying to detect NIO tests.
On the other hand, isolated-class and entire-suite would also
miss detecting a test ğ‘¡as NIO if some other test, ğ‘¡/prime/prime, sets/cleans the
state so that running /angbracketleftğ‘¡/prime/prime,ğ‘¡,ğ‘¡/angbracketrightmakes both ğ‘¡pass or both fail, while
runningjust /angbracketleftğ‘¡,ğ‘¡/angbracketrightmakesthefirst ğ‘¡passandthesecond ğ‘¡fail.Inour
experiments,wefindthatentire-suitemisses13(of223)NIOtests
detected by isolated-method. Of the 13 tests, 10 are not detectedbecause they fail in both runs, which should prompt developersto inspect them already. The remaining 3 tests are not detected
because they pass in both runs. Section 4.2 presents more details.
1We use the term â€œtestâ€ to refer to a test method, following the JUnit terminology and
how the test code is organized into test classes that contain test methods.
2Weusethetermâ€œVMâ€torefertoanexecutionenvironment,suchasaJavaVirtual
Machine or a Python interpreter.1static AtomicInteger counter = newAtomicInteger();
2classCommand...Exception {
3public void execute(...) {
4 counter.incrementAndGet();
5 throw new ActivitiException( "");
6}
7}
8@Test
9public void testRetryInterceptor() {
10...// setup retryInterceptor and processEngine
11try{
12 processEngine.getManagementService().
executeCommand( newCommand...Exception());
13 Assert.fail( "...");
14}catch(ActivitiException e) {
15 ...// assert what number of retries failed
16}
17Assert.assertEquals(retryInterceptor.
getNumOfRetries() + 1, counter.get());
18}
19@After
20public void shutdownProcessEngine() {
21processEngine.close();
22+counter.set(0);
23}
Figure 3: Our fix for an example NIO test detected by allmodes in
activiti [20].
Toillustratethedifferencesandsimilarities,wenextshowtwoex-
amplesofrealNIOteststhatwedetectinopen-sourceJavaprojects:
(1)atestdetectedbyallthreemodesand(2)atestdetectedbyonly
isolated-method. These examples come from popular Java projects,
showing that even mature, well-tested projects can have NIO tests.
Section 5 discusses more examples of NIO tests.
2.1.1 NIO Test Detected by All Modes. Figure 3 shows an NIO test
detected by all three modes. This test is from the project Activ-
iti[20], which is a light-weight workflow and Business Process
Management platform.
ThetestRetryInterceptor teststartsbysettingupa retryInter-
ceptor, which is used to set up a processEngine (Line 10). The test
thenrunsacommandwiththe processEngine (Line12)beforeas-
sertingthatsomenumberofretriesareperformed(Lines15and17).
By default, the retryInterceptor is set to retry a command three
times if it fails. Specifically, the command object used by the test is
Command...Exception , which simply increments the sharedcounter
before throwing an exception (Lines 4â€“5).
This test is NIO because of the shared countervalue. The test
asserts (Line 17) that the number of retries recorded by retryIn-
terceptor is the same as the value of the counter. In the first test
run, the retryInterceptor ensures that the command (Line 12) is
retriedthreetimes( +1forthefirsttry),andthetestpassesasthe
executemethod(Line3)willhaverunfourtimes,setting counter
tofour.However,inthesecondtestruninthesameJVM,the retry-
Interceptor is reinitialized and starts with zero retry, while the
counteris notreinitializedandwillalreadybefourfromthefirst
run of this test. Indeed, the exception for the test failures in thesecond run is that
retryInterceptor.getNumOfRetries()+1 is four,
whilecounter.get() is eight from the two runs of the test.
We prepare a fix by resetting the counterto 0 in the @After
methodoftheclass(Line22).Ourpullrequest[ 1]forthisfixhas
been accepted by the developers. (An alternative fix would have
1732
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Pu Yi, Zhengxi Li, Tao Xie, Darko Marinov, and Wing Lam
1@Test
2public void testSize() throws InterruptedException {
3...// create an object i n InternalThreadLocal
4Assert.assertTrue( "size method is wrong!" ,
InternalThreadLocal. size() == 1);
5...// create an object i n InternalThreadLocal
6Assert.assertTrue( "size method is wrong!" ,
InternalThreadLocal. size() == 2);
7+ InternalThreadLocal.removeAll();
8}
9@Test
10public void testSetAndGet() {
11...// setup testVal and internalThreadLocal
12...// create an object i n InternalThreadLocal
13Assert.assertTrue( "set is not equals get" , Objects.
equals(testVal, internalThreadLocal.get()));
14}
Figure4:OurfixforanexampleNIOtest( testSize)detected
by only the isolated-method mode in dubbo[28].
addedcounter = new AtomicInteger(); atthestartofthetest.)The
testRetryInterceptor test isneithera victim nor a polluter.
2.1.2 NIO Test Detected by One Mode. Figure 4 shows a test de-
tectedasNIOinonlytheisolated-methodmode.Thistestisfrom
the project Dubbo[28], which is a high-performance remote proce-
dure call framework.
ThetestSize test checks whether the size()method defined
inInternalThreadLocal correctly returns the total number of local
InternalThreadLocal objects bound to the current thread. The test
first creates a thread local object and asserts that size()returns
1 (Line 4). The test then creates another thread local object and
asserts that size()returns 2 (Line 6).
Thistestfirstpassesandthenfailswhenruntwiceintheisolated-
method mode, because the test does not remove the two created
objectsbetweenthetwotestruns.Specifically,duringthesecond
run of this test in the same JVM, three objects are bound to the
currentthread(twoobjectsfromthepreviousrunofthetestand
one new object from the current run ofthe test), while during the
firstrunonlyoneobjectisbound.Therefore,inthesecondrun,the
test fails the first assertion (Line 4).
Incontrast, testSize failsin bothrunsof theisolated-classand
entire-suitemodesandis,thus,notreportedasNIO.Inthesetwo
modes,testSize runs after testSetAndGet (Line 10), which also
createsathreadlocalobject(Line12)anddoesnotremoveit.Essen-
tially,testSize is not only NIO when run in the isolated-method
mode but also a victim with testSetAndGet being the polluter.
Although testSize isnotreportedasNIOintheisolated-class
and entire-suite modes, the test does fail in both modes, and devel-
operswouldideallyfixallfailingandNIOtests.Section4.2describes
aninterestingcasewhereatestpassesinbothisolated-classand
entire-suite modes, but is detected as NIO in isolated-method.
Our fix simply adds InternalThreadLocal.removeAll(); at the
end of the test (Line 7). Our pull request [ 2] for this fix has been
accepted by the developers.
3 RESEARCH QUESTIONS AND SETUP
To improve the understanding of NIO tests, we investigate the
following research questions (RQs):RQ1:How prevalent are NIO tests in projects with flaky tests?
RQ2:How do different running modes affect NIO test detection?
RQ3:How do the runtimes of detection modes differ?
RQ4:How do developers respond to proposed fixes for NIO tests?
RQ5:How do NIO tests compare to other OD-related tests?
WeempiricallyaddresstheseRQsonJavaandPythonprojects.
We first describe how we select the projects for our evaluation. We
use Java projects for all five RQs but Python projects for only RQ1
andRQs4-5.WedonotusePythonprojectsforRQs2-3toreducethe
machinecosts;ourevaluationonJavaprojectsfindstheentire-suitemodetobethebesttrade-off.Wenextdescribehowweuse/modify
some testing tools forour evaluation. We finally describe how we
confirm the detected NIO tests.
3.1 Projects
ForJava,weusetheprojectsfromourrecentstudies[ 61,92]onflaky
tests.Thestudiesfoundatleastoneflakytestin55open-sourceJava
projectsobtainedfromGitHub.Foreachproject,weusethesame
Gitcommitasthestudies.Weusethesameprojectsandcommits
becausethestudiesdetectedvictimsandpollutersinthespecificproject commits, thereby allowing us to compare NIO tests that
we detect to previously detected tests (Section 4.5). However, these
projectcommitsaresomewhatolder,sosomedetectedNIOtests
may be already fixed or deleted in the latest project version.
AllselectedprojectsusetheMavenbuildsystem[ 67],soeach
testsuiteinourstudy,asinpriorstudies[ 61,92],isaMavenmodule.
Maven-basedJavaprojectsareorganizedinasetofmodulesthatcaneachhavetheirowncodeundertestandatestsuite.Ourstudyuses
127 of the 130 modules from the recent studies. We omit 3 modules
because we have trouble running their tests. All projects use JUnit,
the most popular testing framework for Java. Most Maven plugins
runthesameoperationoneachmoduleintheproject.Surefire[ 68]
isthedefaultMavenpluginforrunningtests;executing mvn test
at the top level of a Maven-based project runs Surefire for eachmodule in the project. Surefire then finds all test classes in themodule and passes them to JUnit, which for each test class findsall test methods and runs them withoutrepetition. As described
inSection3.2,weadapttheiDFlakiestool[ 58]toenablerunning
various modes withtest repetition.
For Python, we use the dataset from a recent study by Gruber et
al. [39]. The dataset has 1006 projects, each of which is reported to
haveatleastoneflakytest.TodetectNIOtests,weruneachproject
onthecommitinwhichthedatasetreportsatleastoneflakytest.
BuildingPythonprojectscanbedifficultduetodependency-related
errors [73]. We use FlaPy [ 39], the infrastructure released with the
dataset for building the projects and running the tests.
3.2 Tools for Detecting NIO Tests
ForJava,wemodifyaresearchtestingtool,i.e.,iDFlakies[ 58],to
enable repeating tests in one execution without any changes tothe test code. We choose iDFlakies because we are familiar withthe tool, and it works for the Java projects that we select [
58].
Our extension is relatively simple as iDFlakies already treats the
inputasa listoftestsandallowsrepetition.However,theoutput
that iDFlakies produced for multiple test runs would overwrite the
resultsofearlierrunswiththelaterrunsbecausethetestnamewas
1733
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. Preempting Flaky Tests via Non-Idempotent-Outcome Tests ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
used as a key to the run result. To support running test(s) multiple
times in one JVM, we change how iDFlakies reports results and
have added our change to iDFlakies.
ForPython,thedatasetthatweusecomeswithaninfrastructure
to run tests using pytest[80], the most popular testing frame-
work in Python. To run each test multiple times in one Python
interpreter,weusea pytestplugincalled pytest-repeat [81].We
invokepytest-repeat with--count=2 ,withnomodificationstothe
test code. From the output, we exclude parameterized unit tests
(whose name includes a square bracket) and UnitTest style tests
(whose class extends unittest.TestCase ) because pytest-repeat
cannotrunthemcorrectly[ 81].Ifpytest-repeat didnothavethese
limitations, we could have detected even more NIO tests in the
projects that we evaluate. To ensure that our experiments finish in
areasonableamountoftime,weseta10-secondtimeoutforeach
test. Only 486 of 34,446 tests time out.
WerunsetupsandteardownsduringNIOdetectionforbothJava
andPython.Wedonotcleanthediskstatebetweenrunsbecause
the cleaning would be too expensive.
3.3 Confirming Detected NIO Tests
Runningatesttwiceandobservingthefirstrunpassandthesecond
run fail does notguarantee that the test is NIO, because some tests
arenon-deterministic[ 60],i.e.,theycanpassandfailinrepeated
runsevenforthesametestorder.Moreover,consideringthatiD-
Flakiesor pytest-repeat mayhavebugs,wewanttocheckwhether
all NIO tests detected by these two specialized tools can also be
detectedusingthetoolsthatdevelopersmorecommonlyusetorun
tests, such as Surefire and JUnit for Java and pytestfor Python.
For both Java and Python, we confirm whether the detected
testsareNIObyaddingacopyofthetestandrunningtogetherthe
originalandthecopy,thuseffectivelyrunningthetesttwice.Forex-ample,foraJavatest
@TestpublicvoidtestOriginal(){/*body*/} ,
wecanadd @Test public void copy() {testOriginal();} .Running
a test twice by adding a copy is fairly reliable and robust.3Using a
copy to confirm the detected NIO tests, we remove the tests that
arebrittles[ 88]ornon-deterministic.Beyondconfirmingeverytest
fromTable2byrunningacopy,wemanuallyinspectthecodeto
identify the shared state. Our manual inspection and running of
theoriginalandcopyoftheNIOtestsconfirmthat everytestinour
evaluation is indeed NIO (no false positives).
Anotherbenefitofaddingatestcopyistoshowthatoneneed
notusespecializedtools,suchasiDFlakiesand pytest-repeat ,to
detecttheNIOtests.ForJava,wealsoconsiderconfirmingtheNIOtestswiththe
@Ruleand@ClassRule annotationfromJUnit.Wefind
these annotations to be worse than adding a copy, because they do
not work for some older versions of JUnit or when the test class
uses some specialized test runners.
4 RESULTS
4.1 RQ1: Prevalence of NIO Tests
ForJava,wedetectatotalof223NIOtestsin34modules.Weapply
ourthreedetectionmodesto127modulesanddetectatleastone
3Running a test copy, however, can mask the confirmation of NIO tests when the test
name affects thetest behavior, e.g., the test name matchessome external resource in
the file system. We do not observe such a problem in our additions of copy.1public class SearchQueryTest extends ... {
2@Override public void setUp() {
3 super.setUp();
4 createUser( "Bob", ...);
5 createUser( "Barbara" , ...);
6 createUser( "Anton" , ...);
7 createUser( "Robert" , ...);
8 createUser( "John", ...);
9 Session session = getSession();
10 session.flush();
11 session.getTransaction().commit();
12 session.beginTransaction();
13}
14@Test
15public void no_where() {
16 assertEquals(5, query().fetch(). size());
17}
18}
Figure 5: NIO test ( no_where)i n querydsl [82].
NIO test in 26% of modules. These 127 modules have a total of
40,019 tests, so NIO tests are over 0.5% of all tests in these modules.
ModuleM20,with122NIOtests,isanoutlier.Evenignoringthis
module,westillfindtheratioofNIOteststobenon-negligible,over
0.2% (101 out of 39,536) of all tests, in the remaining 126 modules.
For Python, we detect a total of 138 NIO tests in 90 projects. We
apply the entire-suite mode to 1006 projects and detect at least one
NIOtestinabout9%ofprojects.Thismoderunsatotalof34,446
tests in these 1006 projects, so NIO tests are over 0.4% of all tests.
Table 1 shows the statistics of the NIO tests detected in 34 Java
modules.Duetolimitedspace,weomitadetailedbreakdownfor90
Pythonprojects.ForeachJavamoduleinwhichourexperiments
detect at least one NIO test, we tabulate the GitHub slug (user-
name/project) and module name, the number of NIO tests that are
detected for isolated-method (IM), isolated-class (IC), and entire-
suite (ES) modes, the time to run our experiments for the threedifferent modes, and the time ratios. In Section 4.5, we compare
NIO tests to other kinds of OD-related tests in these Java modules
and Python projects.
A1:NIO tests are currently prevalent enough that every project
should run NIO detection at least once.
4.2 RQ2:NIOTestsDetectedinDifferentModes
Section 2 has introduced our three modes to detect NIO tests. In
our experiments, the majority (210) of NIO tests are detected by
allthreemodes.Alltestsdetectedbyentire-suitearedetectedby
isolated-class,andalltestsdetectedbyisolated-classaredetected
by isolated-method. In contrast, isolated-method detects 11 and
13 tests that are not detected by isolated-class and entire-suite,
respectively. Our inspection finds that 8 (resp. 10) tests are notdetected by isolated-class (resp. entire-suite), because they havepolluters in the test class (resp. test suite). These polluters run
beforetheNIOtests,makingthemfailtwice,intheisolated-class
orentire-suitemode.Section2.1.2presentsanexampleofoneofthese tests where a polluter makes the example test fail in both
runs of the isolated-class and entire-suite modes.
The remaining 3 out of 13 tests, which pass in the first run
and fail in the second run in isolated-method, interestingly pass
in both runs in the isolated-class and entire-suite modes. All three
tests are from the module M22 ( querydsl-hibernate-search ), and
1734
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Pu Yi, Zhengxi Li, Tao Xie, Darko Marinov, and Wing Lam
Table 1: Statistics about the NIO tests detected and the time taken in various modes (IM: isolated-method; IC: isolated-class;
ES: entire-suite). Overhead shows the ratio of runtime for various modes.
# NIO Tests Time to Run [s] Overhead
IDProject User/Name - Module IMICESIM ICESIM/ICIM/ES
M1activiti/activiti - activiti-engine 22215729 4694 6043.426.0
M2activiti/activiti - activiti-spring-boot-starter 7667263692292.03.2
M3apache/hadoop - hadoop-hdfs-httpfs 3322981 9045963.35.0
M4apache/hadoop - hadoop-hdfs-nfs 1001290 8807491.51.7
M5apache/hadoop - hadoop-mapreduce-client-app 3336087225814002.74.3
M6apache/hadoop - hadoop-mapreduce-client-core 31136891617 8412.34.4
M7apache/hadoop - hadoop-mapreduce-client-jobclient 5552935419702 72001.54.1
M8apache/hbase - hbase-server 131313180778 5520710320 3.317.5
M9apache/incubator-dubbo4- dubbo-cluster 3339274502212.14.2
M10apache/incubator-dubbo - dubbo-common 6553341 7512924.411.4
M11apache/incubator-dubbo - dubbo-config-api 2212184 4112155.310.2
M12apache/incubator-dubbo - dubbo-monitor-default 1112582492551.01.0
M13apache/incubator-dubbo - dubbo-remoting-netty 1113743563101.11.2
M14apache/incubator-dubbo - dubbo-rpc-api 4445773822361.52.4
M15apache/incubator-dubbo - dubbo-rpc-rest 2223152562161.21.5
M16eclipse-ee4j/tyrus - non-deployable 1114332011122.23.9
M17elasticjob/elastic-job-lite - elastic-job-lite-core 4442890 9641883.015.4
M18looly/hutool - hutool-core 1112967 651 854.634.9
M19orbit/orbit - actor-tests 1118518 75227811.330.6
M20pholser/junit-quickcheck - core 1221221222687 6711014.026.6
M21pholser/junit-quickcheck - generators 44462552294 1562.740.1
M22querydsl/querydsl - querydsl-hibernate-search 3003863723161.01.2
M23spring-projects/spring-boot - spring-boot 222148990 12299 38412.1388.0
M24spring-projects/spring-boot - spring-boot-actuator 12111121821 5416 3684.059.3
M25spring-projects/spring-boot - spring-boot-actuator-autoconfigure 22221866 9357 4292.351.0
M26spring-projects/spring-boot - spring-boot-test 11123505 5623 2424.297.1
M27spring-projects/spring-boot - spring-boot-test-autoconfigure 44464684358 5381.512.0
M28spring-projects/spring-ws - spring-ws-core 11145291247 2233.620.3
M29undertow-io/undertow - servlet 10022891133 2972.07.7
M30vmware/admiral - kubernetes 1116843081822.23.8
M31vmware/admiral - common 4446682851422.34.7
M32vmware/admiral - request 10039951833 9952.24.0
M33wildfly/wildfly - server-integration 1116904572711.52.5
M34zalando/riptide - riptide-spring-boot-starter 1114943211341.53.7
SumÃ—3|Arith. Mean Ã—3|Geo. Mean Ã—2 22321221014963 4030 8572.58.4
all three tests have the same root cause. One of the three tests
(SearchQueryTest.no_where ) is shown in Figure 5.
UnlikethetwoexamplesdescribedinSection2,thepollutedstate
that causes no_where to fail is not on the heap but in a database
stored in the file system. Specifically, the test adds five entries to a
databaseandthencheckswhetherthedatabasecontainsfiveentries
(Line 16). The entries added to the database are saved to the file
systemonlyafterallofthetestsfinishandtheJVMexits.Therefore,
in the isolated-class and entire-suite modes, even when this test is
runmultipletimes,andthe setUp()methodisrunmultipletimes,
all of the runs use a new (empty) database and all of the runs
pass. However,we find no_where to pass inthe first run and failin
the second run in the isolated-method mode because we already
run (twice) another test in the SearchQueryTest class, saving the
database to the file system before no_where runs. The first run of
4apache/incubator-dubbo nowredirectsto apache/dubbo ,butwekeeptheoldname
to be consistent with prior work.no_where in the isolated-method mode passes even with a polluted
database because the database is loaded asynchronously, and if the
database is not ready by the first run, then it simply uses a new
databasefrommemory.Ontheotherhand,thesecondruntypically
usesthepolluteddatabasefromthefilesystemandconsequently
fails. Section 5.1 describes more details about this test.
A2:Allthreemodesdetectsimilartests,butisolated-methoddetects
slightly more than isolated-class, which detects slightly more than
entire-suite.
4.3 RQ3: Runtime of Different Modes
Section 2 has discussed how the three different modes can vary in
the runtime due to the number of JVMs that the modes need to
run. In all three modes, the total number of test runsis exactly the
same, but the number of JVM runs differs greatly. Each JVM run
hastostartaJVMandloadtherequiredclasses,takingnontrivial
time.ThenumberofJVMrunsneededisthesameasthenumber
1735
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. Preempting Flaky Tests via Non-Idempotent-Outcome Tests ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
of tests, the number of test classes, and one in the isolated-method,
isolated-class, and entire-suite modes, respectively.
Table1presentsthetimetorunthethreedetectionmodes.As
expected,theisolated-methodmodeistheslowestamongthethree
modes.Specifically,the(geo.mean)overheadoftheisolated-methodmodeis8.4xand2.5xovertheentire-suiteandisolated-classmodes,
respectively. These numbers confirm that the overhead for each
JVM run is nontrivial [ 22,75]. Our results also show that the M23
module has a much higher overhead for isolated-method/entire-
suitethanothermodules.Wefindthatthehigheroverheadisbe-
causeM23hasthehighestnumberoftests(2,108)ofallthemodules,
andconsequently,isolated-methodruns2,108JVMs,whileentire-
suite runs only 1 JVM. As the entire-suite mode runs substantially
fasterthantheothertwomodesandyetmissesdetectingonly5.8%
(13/223) of NIO tests, we recommend that developers regularly run
theentire-suitemodeandonlyrarelyruntheisolated-methodmode
to detect NIO tests that the entire-suite mode may miss. Follow-
ingourownrecommendation,werunonlytheentire-suitemode
forPythonprojectsbutconfirmthedetectedPythonNIOtestsby
running each test twice in isolation.
A3:The most cost-beneficial mode is entire-suite; we suggest run-
ning entire-suite periodically and isolated-method for only newly-
added or directly-modified tests [61].
4.4 RQ4: Experience with Fixing NIO Tests
We fix and open pull requests for 268 NIO tests. 192 of them are
accepted, only 6 rejected, and the remaining 70 pending. Table 2
showsthestatisticsaboutourpullrequests.Therowsâ€œJavaâ€and
â€œPythonâ€showthesumforallJavamodulesandPythonprojects,
respectively.Toillustratediversity,weshowdetailsforeachJava
module. Due to space limit, we show only the sum for Python.
Our experiments use an older version of projects (to compare
withvictimsandpolluters;Section4.5),buttestsareworthfixing
only in the latest version. The table marks as â€œN/Aâ€ 42 tests thatarenotNIOinthelatestversion,i.e.,fixed,deleted,ignored(e.g.,
annotated with @Ignorein Java) or archived.
We inspect all361 NIO test that we detect, even â€œN/Aâ€. For each
NIO test, we inspect for at least one hour before giving up and
proceeding to the next test. Each NIO test that we do not fix in our
first iteration is reinspected later. In the end, we do not fix 51 tests
(17 Java and 34 Python) for three reasons: (1) we cannot localize
the pollution (for 3 Java and 16 Python tests); (2) we localize the
pollutionbutitisdifficulttoclean(for13Javaand13Pythontests);and(3)wedonotfixteststhatarespecifiedtoruninspecificorders
(for1Javaand5Pythontests,e.g.,annotatedwith
@TestMethodOrder
in Java) because developers are likely already aware of the statepollution, thus unlikely to want to clean the state pollution that
makesothertestspass(inotherwords,theNIOtestsetsstatefor
some brittle test [88]).
Anexampletestthatwedonotfixbecausewecannotlocalize
thestatepollutionis AuthUtilsTest.testValidateSessionData from
M31.Thetesttriestocreateanewusereachtime,checksseveral
functionalities of the session, and then clears the session. In the
secondrun,thetestfailstheassertion assertEquals(authCtxUser,
getOp.getAuthorizationContext()) , because the call getAuthoriza-
tionContext() returnsnullinstead of the expected object. We findTable2:Statisticsaboutourpullrequests(PRs)forNIOtests;
â€œN/Aâ€markstestsnotavailableinthelatestprojectversion.
# NIO Tests # NIO Tests in Our PRs
ID detected N/Aopened accepted rejected
M1 20 2 2 0
M2 77N/A 0 0
M3 30 3 0 1
M4 10 1 1 0
M5 30 3 1 0
M6 30 3 1 0
M7 50 5 1 0
M8 133 7 1 0
M9 30 2 2 0
M10 60 6 6 0
M11 21 1 1 0
M12 10 0 0 0
M13 11N/A 0 0
M14 40 4 4 0
M15 22N/A 0 0
M16 10 1 1 0
M17 40 4 4 0
M18 11N/A 0 0
M19 11N/A 0 0
M20 122 2120 120 0
M21 40 4 4 0
M22 30 3 3 0
M23 21 1 0 0
M24 120 2 0 0
M25 20 2 0 0
M26 10 1 0 1
M27 41 3 0 3
M28 10 1 0 0
M29 10 1 0 0
M30 10 1 0 0
M31 40 3 0 0
M32 10 1 0 0
M33 10 0 0 0
M34 10 1 1 0
Java 22320 186 153 5
Python 13822 82 39 1
Total 36142 268 192 6
thatnullisreturnedbecauseattheendofthefirstrun,theregu-
laruserâ€œlogsoutâ€via AuthUtils.cleanupSessionData(getOp) .How-
ever,aftercarefulinspection,wefindthatseeminglyallthevariables
involved in the test code are newly initialized, and therefore, wecannot easily identify the exact global variable that is shared be-tween the two runs. We envision that future research can apply
program analysis techniques to help developers localize the shared
state for NIO tests.
An example test that we do not fix because we cannot clean the
pollution is WebMvcMetricsFilterTests.regexBasedRequestMapping
from M24. In the second run, the test fails the assertion assert-
That([...].timer().count()).isEqualTo(1L) because count()re-
turns2Linstead of the expected 1L. It is obvious that we need
toresetthetimer(ortheobjectthatstoresthetimer)tocleanthe
1736
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Pu Yi, Zhengxi Li, Tao Xie, Darko Marinov, and Wing Lam
pollution. However, after extensive code search, we cannot find
a reasonable cleaning method. Moreover, the receiver object for
timer()iscreatedbyaclassimportedfromalibrarydependency,
preventing us from easily implementing the functionality to reset
the timerâ€™s state in the module M24 itself.
Overall,wefix268(oftheavailable319)testsbyrevising/adding
codetocleanthesharedstateoftheNIOtests,therebymakingthem
idempotent.The number ofopenedpullrequests (PRs)isâ€œN/Aâ€if
allNIOtestsdetectedinamodulearenolongerapplicableinthe
latest version. Our fixes cover at least one test in 82 out of 109
componentsâ€”27/29 Java modules and 55/80 Python projectsâ€”that
have at least one NIO test remaining in the latest version.
Basedontheacceptancerateanddevelopersâ€™comments,mostof
our fixes have been appreciated by developers. Ignoring the outlier
M20,ofalltheacceptedtests,40%forJava(19%forPython)were
acceptedwithoutcomments,while60%(81%)wereacceptedwith
developer compliments (e.g., â€œnice catchâ€, â€œthanksâ€, â€œlgtmâ€); only 2
for Java (5 for Python) tests had some discussions with developers
(e.g., asking us to modify â€˜@Beforeâ€™). Out of the 268 fixed tests,
only6(5forJavaand1forPython)hadourPRsrejected.Forthe
rejected tests in M3 and M26, the developers believed that thepolluted state would affect only the test itself and not any othertest, and thus claimed that no fixes were needed [
3,4]. For the
three rejected tests in M27, the developers believed that our fix
waspotentiallyâ€œmaskingâ€theproblem[ 5].FortherejectedPython
test,thedeveloperappreciatedourfindingbyconfirmingthatthe
NIOtestisâ€œcertainlyavalidissueâ€,butrejectedourPRbecausethe
developer wanted to completely refactor the code instead [6].
ThemoduleM20isabigoutlierwith120tests.Wepointoutthat
the fixes for these 120 tests do have some diversity, e.g., modifying
eightdifferentsetsofdatastructures.Moreover,eachtestrequiresadifferentlinetobefixed,sofixingallthetestsisnotsimplychanging
one line that is shared across all the tests.
ReflectingonourPRsandourlimiteddiscussionswiththedevel-
opers, the lessons learned are that providing (1) steps to reproduce
testfailuresand(2)explanationsofwhyfixingNIOisbeneficialcan
improve the likelihood that PRs get accepted. For example, for one
PR [7], we had initially reported the failure message from running
the test twice without providing (1) or (2). The developer com-
mentedâ€œIâ€™mnotsurehowthiswouldsolvethe[failure],andIâ€™ve
never come across itâ€ and closed our PR. After we provided (1) and
(2),including â€œcleanstate pollutionso thatsomeother testswonâ€™t
fail in the futureâ€, the developer promptly reopened and merged
ourPR,replyingâ€œThanksfortheexplanation.ThePRmakessenseâ€.
Our recent PRs include both (1) and (2).
A4:Developers are generally positive about fixes for NIO tests;
providing reproducing steps and explaining the motivation help.
4.5 RQ5: NIO â€“ Victim â€“ Polluter Comparison
Every NIOtest isboth latent-victimand latent-polluter,and some
NIO tests may be victims or polluters in their test suite. To un-derstand how the NIO tests that we detect relate to OD-relatedtests detected in prior work, we intentionally use the same pro-jects/modules and commits as prior studies for Java [
61,92] and
Python[39]thathavereportedsomevictimsorpolluters.Detecting
victims is expensive and typically requires running many random
Figure 6: Number of (fixed) Java and Python NIO tests andtheir overlap with victim and (for Java only) polluter tests.
orders of test suites or sophisticated analyses of test dependen-
cies[36,58,92,99].Detecting(all)pollutersisevenmoreexpensive
asthedetectioncanrequirecheckingalltestpairs[ 36,88,92].In
fact,thePythondataset[ 39]thatweusereportsonlyvictimsbut
does not report polluters.
Figure 6 (top row) shows Venn diagrams relating the number
ofNIO,victim,and(forJavaonly)pollutertestsinthesedatasets.
Of223JavaNIOtests,only13arevictimsbutnotpolluters,7are
pollutersbutnotvictims,and8arebothvictimsandpolluters.Of
138 Python NIO tests, only 36 are victims. We find that a large
number of victims and polluters detected in prior work are notNIO tests. On the other hand, despite the relation of victims and
polluters to NIO tests, the majority of the NIO tests that we detect
werenotdetected before.
Figure6(bottomrow)showsVenndiagramsfocusingonthe268
NIOteststhatwefix.Of186fixedJavaNIOtests,only12arealso
victims,7arealsopolluters,and1isbothvictimandpolluter.Of
82 fixed Python NIO tests, only 16 are victims. The Venn diagrams
show some of the diversity of the NIO tests that we fix.
A5:NIOtestsarerelatedtobutnotsubsumedbyODtests;detecting
NIO tests can be an effective way to preempt OD tests.
5 CASE STUDIES
We next discuss some interesting examples of NIO tests that we
inspectorfix.WhileallNIOtestspollutesomepartoftheshared
state,differenttestspollutedifferentparts.Theseexamplesillustrate
various parts of the shared state that cause NIO tests.
5.1 Java â€“ Database
Three tests from querydsl [82] (Section 4.2) are NIO because of the
statepollutedthroughadatabasestoredonthedisk.Specifically,the test fails an assertion (Line 16 in Figure 5) with the message
expected:<5> but was:<10> .Thetestclass SearchQueryTest extends
AbstractQueryTest whose@Before public void setUp() method
insertsfivenewusersintoadatabase(Line8).WheneverJUnitruns
the test class, it invokes the setupmethod that inserts five more
users into the database. Indeed, if we run the test in two JVMs, the
failure in the second JVM is due to every user being added twice.
Our inspection reveals that each JVM run adds some files in the
directory querydsl-hibernate-search/target/lucene/indexes/com.
querydsl.hibernate.search.User/ .Thesefilesaredatabase-related,
persist across different JVMs, and make the test NIO. Interestingly,
these files are only persisted afterthe JVM finishes. Therefore,
1737
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. Preempting Flaky Tests via Non-Idempotent-Outcome TestsICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
1@Test
2public void testAsync() {
3RpcContext rpcContext = RpcContext.getContext();
4Assert.assertFalse(rpcContext.isAsyncStarted());
5rpcContext.setAsyncContext( newAsyncContext ...);
6...// checks no asyncContext has started
7RpcContext.startAsync();
8Assert.assertTrue(rpcContext.isAsyncStarted());
9asyncContext.write( newObject());
10...// assert something was done by asy ncContext
11rpcContext.stopAsync();
12Assert.assertTrue(rpcContext.isAsyncStarted());
13+RpcContext.removeContext();
14}
Figure 7: Developer fix for NIO test in dubbo[28].
runningthesametesttwiceorevenmoretimesinthesameJVM,
from a clean disk state, does notlead to any test failure.
The fix for these tests is simple, just correcting a typo: changing
FileUtils.delete(new File("target/lucene3")) toFileUtils.del-
ete(new File("target/lucene")) in the setup method. This project
usesTravisCIforcontinuousintegration,butthisissueis notde-
tected inCI becauseit runs theentire testsuite (without repeated
tests)inoneJVMalwaysfromacleandiskstate.Incontrast,running
thetestsuitemultipletimesonthesamemachine(e.g.,adeveloperâ€™s
laptop) would have detected the issue. We opened a PR [ 8] with
our fix, and the developers accepted it and replied â€œThanks! /thumbs_up_altâ€.
5.2 Java â€“ File System
The test TestViewfsWithNfs3.testNfsRenameSingleNN fromhadoop
checks whether it can rename a file represented by an HdfsFile
object.This testisNIO becauseofdiskupdates. Thetestfirst gets
theHdfsFile that it tries to rename and checks the status of this
HdfsFile. The test then renames that HdfsFile and checks its sta-
tus after renaming. In the second run on the same JVM, this testraises a
NullPointerException , specifically from invoking status-
BeforeRename.isDirectory() .Beforerenaming,thetestchecksthat
theHdfsFile is not a directory. The problem is that the test gets
theHdfsFileStatus objectfor statusBeforeRename basedonthefile
name, but the name has been changed in the first run, so statusBe-
foreRename becomes nulland causesthe exception.Our proposed
fix [9] renames the HdfsFile again back to its original name at the
end of the test, and was accepted with â€œMerged. Thank youâ€.
In the same project, the test TestTaskProgressReporter.test-
BytesWrittenRespectingLimit writes some bytes to the local file
system. It also increments some counters that are written to thefile system. However, after the test finishes, the counters are not
reset,makingoneassertionfailwhenthetestrunsforthesecond
time. Our fix [ 10] invokes FileSystem.clearStatistics() to reset
the counters at the end of the test, and was also accepted.
5.3 Java â€“ Heap Reachable from Static Fields
The mostcommoncase forNIO testsis heapâ€œpollutionâ€:either the
staticfieldsthemselvesortheobjectsreachablefromthestaticfields
arepolluted.Figure7showsanexampleNIOtestfrom Dubbo[28].
Thetest testAsync startsbygettingaRemoteProcedureCallcon-
text(Line 3).The testaims tocheckwhether thecontext properly
exercisessometaskasynchronously.Indoingso,thetestalsochecks1@Test
2public void testSigTermedFunctionality() throws ... {
3AppContext mockContext = Mockito. mock(AppContext.
class);
4JHEventHandlerForSigtermTest jheh =
5 newJHEventHandlerForSigtermTest(mockContext, 0);
6// adds some jobId to the static fileMap
7jheh.stop();
8// adds some jobId to the static fileMap
9jheh.stop();
10// assertions at the end of the test
11}
12// a method to execute jheh.stop()
13@Override
14protected voidserviceStop() throws Exception {
15// log the info
16for(Map.Entry<JobId,MetaInfo> jobIt : fileMap.
entrySet()) {
17 JobId toClose = jobIt.getKey();
18 // log the info
19 finalJob job = context.getJob(toClose);
20 intsuccessfulMaps = job.getCompletedMaps()
21 - job.getFailedMaps() - job.getKilledMaps();
22 // NullPointerExcep tion rai sed in the second run
23 // stop the job
24}
25...
26// helper class for testSigTermedFunctionality
27classJHEventHandlerForSigtermTest extends
JobHistoryEventHandler {
28public JHEventHandlerForSigtermTest(AppContext
context, intstartCount) {
29 super(context, st artCount);
30+JobHistoryEventHandler.fileMap.clear();
31}
32}
Figure 8: Our fix for NIO test in hadoop[43].
four times (Lines 4, 6, 8, and 12) whether the async task in rpcCon-
texthasstarted.Thefirsttwochecksbeforethe rpcContext starts
the async task (Line 7) are expected to be false, while the later
two checks are expected to be true. The test is NIO because the
rpcContext (fromLine3)is sharedinallrunsofthistest.Therefore,
thefirstcheck(Line4)inthesecondrunfails,becausetheasync
taskhasalreadystartedduringthefirstrun.Notethateventhough
Line 11 stops the async task, the check on Line 12 still passes inthe first run (while Line 4 fails in the second run) because
rpc-
Context.isAsyncStarted() is simply checking whether the async
taskhasstartedbefore,andnotwhetheritisstillongoing.Inthe
latestversion,thedevelopershavecleanedthestatepollutionbyadding
RpcContext.removeContext(); at the end (Line 13), so the
test is â€œN/Aâ€ in Table 2.
TestJobHistoryEventHandler.testSigTermedFunctionality from
hadoopisNIO.Figure8showstherelevantcodesnippet.Theroot
causeofthefailureisthatthistestaddssomeentriestothestatic
fieldJobHistoryEventHandler.fileMap ,whichissharedamongtests,
and does not remove the entries from the map. JobHistoryEven-
tHandler.fileMap has as keys the ids of the jobs that have been
created. The call jheh.stop() (Line 7) in the test calls another
method serviceStop (Line 14), which iterates over the JobHisto-
ryEventHandler.fileMap togetthejobids,thengetseach Jobobject
byitsid,andfinallystopsit.Afterthefirsttestrun,thecreated Job
objects are cleaned, but their stale ids remain in JobHistoryEven-
tHandler.fileMap .Therefore,inthesecondrun,thetestgets null
from these stale ids (Line 19) and throws NullPointerException
(Line22).Ourfixistoclear JobHistoryEventHandler.fileMap atthe
1738
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Pu Yi, Zhengxi Li, Tao Xie, Darko Marinov, and Wing Lam
1defcmd_mock():
2 def_cmd_mock( name:str):
3 cmd.__overrides__[ name] = [ '/bin/true' ]
4 yield _cmd_mock
5-cmd.__overrides__ =[]
6+cmd.__overrides__ ={}
Figure 9: Our fix for NIO test in benchbuild [25].
end of the constructor of class JHEventHandlerForSigtermTest ,a
helperclassofthistest(Line30).Infact,theconstructorofanother
helperclass, JHEvenHandlerForTest ,inthesamefileclears JobHisto-
ryEventHandler.fileMap ,sothesamecleanupneedstotakeplace
in the constructor of JHEventHandlerForSigtermTest . Our PR [ 11]
for this test has been merged by the developers.
MetricsTest.shouldRecordCircuitBreakers fromriptideis an-
other test that is NIO due to the pollution of the shared static field.
This test adds new timersto theSimpleMeterRegistry but does not
clear them. In the second test run, the assertion assertEquals(4,
timer.count()) attheendfailsbecausethe timer.count() hasbeen
incrementedto 8aftertwo runs.Interestingly, thisstatepollution
not only makes the test fail in the second run but also causes an-other test,
shouldRecordRequests in the same test class, to fail. In
other words, shouldRecordCircuitBreakers is a polluter for the vic-
timshouldRecordRequests . Because this issue affects multiple tests,
we do not fix just one test body but add to the class a teardown()
method,whichcleans SimpleMeterRegistry aftereachtestrun.Our
PR[12]forthistestwaspromptlyaccepted(within10minutes),and
the developers gave a thumb up and thanked for our contribution.
5.4 Java â€“ System Property
SecurityUtilsTest.testEnsureTrustStoreSettings inadmiraltests
whetheritcanproperlysetsomesystemproperties.Forexample,
forSECURITY_PROPERTIES , the test starts by getting the value of the
system property with System.getProperty(SECURITY_PROPERTIES) .
Thenitchecksthatthispropertyhasnotbeenset,bycomparingthe
valueto nullandanemptystring.Thischeckpassesinthefirsttest
run.Thetestthenruns System.setProperty(SECURITY_PROPERTIES,
...);andassertEquals(...); tosetanothervaluetothatsystem
propertyand toassertthat ithasbeenproperly set.Thetest isNIO
because it does not reset this system property at the end. In the
secondrunofthetestonthesameJVM,thefirstassertion(checking
that this property has not been set) fails.
Thefixforthistestistocleanthepollutedsystemproperties,e.g.,
weaddSystem.clearProperty(SECURITY_PROPERTIES); .OurPR[ 13]
was pending review before the project got archived.
5.5 Python â€“ Buggy Cleaning
Thetest test_cli_slurm.test_slurm_command frombenchbuild [25]
isNIOduetoaninterestingstatepollution:developershavecode
tocleanthestatebutmistakenlypollutethetypeofavariableso
that the test fails on the second run. This type mistake is more
likelytoappearinadynamicallytypedlanguage.Figure9shows
therelevantcodeandourfix.Thetestcallsthefunction cmd_mock
thatitselfreturnsafunction _cmd_mock thatcanaddthecorrespond-
ing value of the key nameto a dictionary called cmd.__overrides__ .
Notethat cmd.__overrides__ isaglobalvariablesharedamongtest1defto_zero(tvd, northing, easting, surface_northing,
surface_easting):
2 # perform somechecking
3-northing-=surface_northing
4-easting-=surface_easting
5+northing=northing-surface_northing
6+easting=easting-surface_easting
7 return tvd, northing, easting
8
9# initialization for global variables g1, g2, ..., g5
10g1 = ...
11deftest_zero():
12 # global variables passed in as arguments
13 v1, v2, v3 = to_zero(g1, g2, g3, g4, g5)
14 np.testing.assert_equal(...) # assertion
Figure 10: Our fix for NIO test in wellpathpy [93].
1deftest_celery_integration():
2 server_address = ( "", 8080)
3 server = HTTPServer (server_address, Handler)
4 # perform some assertions
5+server.socket.close()
Figure 11: Our fix for NIO test in pybrake[79].
runs. In the second run, the test fails reporting TypeError: list
indices must be integers or slices, not str . The root cause
iscmd.__overrides__ = [] that sets the global variable to be an
emptylist.Whiledevelopershadthoughttocleanthestate,they
mistakenly wrote the wrong cleaning code. Our fix changes the
empty list to the empty dictionary. The developers accepted our
PR [14] and said â€œThanks, good catch!â€.
5.6 Python â€“ Function Side Effect
Thetest test_location.test_zero fromwellpathpy [93]isNIOdue
to state pollution stemming from the side effects in the function
under test. Figure 10 shows the test and the function to_zero. The
test calls to_zeroby passing 5 global variables (of type numpy
arrays) initialized outside test_zero . The second run of the test
fails, causing AssertionError when executing Line 14. The root
cause is that to_zeromodifies the data in the numpy arrays passed
in (namely g2andg3). Within to_zero,northing andeastingpoint
to thesame numpy arraysas g2andg3, respectively.A discussion
ofsuchaliasingfornumpyarraysisonStackOverflow[ 15].Ourfix
replacestheoperator -=thatmodifiesarraydata inplacewithan
assignmentthatcreatesnewarraysanddoesnotmodifythearrayspassedin.ThedevelopersmergedourPR[
16]andcommentedâ€œThis
is a good changeâ€.
5.7 Python â€“ Network Related
The test test_celery_integration.test_celery_integration from
pybrake[79]isNIOduetostatepollutionrelatedtonetwork.Fig-
ure 11 shows the relevant code and our fix. The second test run
throwsOSError: [Errno 98] Address already in use .Thereason
is that the test does not release the network resource at the endof the execution, and therefore, the second run cannot initialize
theserver usingthesameaddress.Our fixistoclose theserverto
make the address reusable after the test execution. The developers
merged our PR [17].
1739
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. Preempting Flaky Tests via Non-Idempotent-Outcome Tests ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
6 DISCUSSION
Motivation and cost for rerunning a passing test. One could
question why rerun tests twice when it is not usually done and
takes time. The cost of rerunning tests twice to detect NIO tests
hasthebenefitto preemptorder-dependent(OD)tests,aprominent
category of flaky tests [ 31,58,65,99]. As a cost-effective approach
to proactively detect OD-related tests, instead of rerunning all
tests twice all the time, developers could rerun (1) only sometimes:
all tests periodically (e.g., every weekend) or (2) only some tests:
newly-added or recently-modified tests for all regression runs. We
previouslyproposedthesetwooptionsforotherflaky-test-detection
approaches [61] but not for rerunning tests twice.Evidence of NIO tests becoming polluters or victims.
We de-
scribethehistoryoftwoexamplesfrom hadoop.AnexampleNIO
testthatbecameapolluteris TestJobHistoryEventHandler.testSig-
TermedFunctionality ,whichonedeveloperhadadded(in 5f52156a)
and later became a polluter when another developer added (in
64e4fb98) three victims ten months later. For this NIO test, the
developers accepted our fix [ 11] with compliments. Had the first
developerusedourapproachtodetectNIOtests,thetestcouldhavebeenfixedbeforethevictimswereadded.AnexampleNIOtestthat
became a victim is TestTaskProgressReporter.testTaskProgress ,
whichonedeveloperhadaddedfirst(in 7e6f384d)andthenbecame
a victim when another developer added (in cb26cd4b) a polluter
aboutsevenmonthslater.Yetagain,usingourapproachcouldhave
prevented the later polluter-victim pair.
7 THREATS TO VALIDITY
SomekeythreatstovalidityaretheruntimeratiosandwhethertheteststhatwedetectarereallyNIOandworthfixing.Ourcomparison
of the runtime for different modes (Section 4.3) can be affected by
thenoiseinthemeasurementoftime.Tomitigatethisthreat,we
run the experiments on isolated Azure machines, and claim only a
general trend of the overhead of different modes.
As our evaluation for NIO tests involves rerunning the tests,
our results could be affected by flakiness itself. For example, a test
mayappearNIO(firstrunpasses,secondrunfails),althoughthe
test is actually idempotent and happens to exhibit NIO-like results
becauseofsomenondeterminism.Tomitigatethisthreat,wererun
the NIO tests with various tools and additionally manually inspect
all detected NIO tests to obtain higher confidence in the tests that
we study. In fact, the projects under evaluation likely have more
NIO tests that are not detected because of tool limitations (e.g.,
pytest-repeat does not run certain kinds of Python tests).
We also use many existing tools and modify some to detect NIO
tests.Inprinciple,manyofthesetools,suchasMavenSurefire[ 68],
JUnit[51],pytest[80],orpytest-repeat [81],couldhavebugsthat
impact our results. We mitigate this threat by choosing some of
themostwidelyusedbuildsystemsandtestingframeworks.OurownmodificationstoiDFlakies[
58]aremorelikelytohavebugs.
We mitigate this threat by having multiple authors check iDFlakies
modificationsandmanuallyinspectvariousresults.Finally,thebest
waytoalleviateconcernsaboutusefulnessofNIOtestsistoprovide
fixesthatdeveloperslargelyaccept:paraphrasingthesayingâ€œthe
proofisinthepuddingâ€,wecouldsayâ€œtheproofisinPRingâ€,i.e.,
opening pull requests that get accepted.8 RELATED WORK
Arecentsurvey[ 77]reviewsmanypapersthathavestudiedvarious
causes and categories of flaky tests [ 21,24,29,31,36,39,42,48,57,
58,60,65,72,78,85,87,88,99]. These papers focus on tests that
can pass or fail when running each test in a test suite only once. In
contrast,wearethefirsttoinvestigateNIOtests,whichpassand
fail when run twice in the same VM.
Tohelpwiththeproblemofflakytests,varioustoolshavealso
beenproposedtohelpdetectthesetests[ 24,36,48,58,78,99].Most
tools require running the tests and observing whether a test can
pass in some runs and fail in other runs. Similar to these tools, our
detectionofNIOtestsisbasedonwhetheratestcanpassandfailin
various runs. Unlike these prior tools, we run the tests twice in the
same VM. Our findings for NIO tests are particularly important to
the topic of flaky-test detection, because NIO tests can pollute the
state used by other tests andthey can fail themselves depending
on the state set by other tests. Much of prior work on flaky-test
detectionhasbeenonorder-dependenttests,whichpassorfaildue
to the pollution of othertests.
Beyond detecting flaky tests, related work has also proposed
automatically fixing flaky tests [ 30,63,88,98], tolerating flaky
testsbyresettingstate[ 23],accommodatingtestdependencies[ 24],
generatingorder-dependentflakytestsusingmutations[ 41],and
accommodating order-dependent tests in regression testing [ 59].
The ideas from these projects could help automatically fix or ac-
commodate NIO tests in the future.
NIOtestsarebothlatent-victimandlatent-pollutertests.Oracle-
Polish[48]andPolDet[ 40]usesophisticatedtechniquestodetect
latent-victim and latent-polluter tests, respectively. However, they
report many more tests that developers consider false positives.
Morerecently,additionaltoolshavebeenproposedtodetectpol-
luters[88,95].Wereportthe importantintersection oflatent-victim
and latent-polluter tests. Our approach is simplebut effective at
preempting polluters and victims. Our approach is also portable,
e.g., we evaluate on both Java and Python tests.
9 CONCLUSION
This paper has focused on NIO tests, which pass in the first run
but fail in the second run in the same VM. We have proposed
threemodes todetectNIO tests;thesemodes detect223NIO Java
tests, and the most practical mode detects 138 NIO Python tests.
These NIO tests are mostly new and have notbeen detected by
prior research on flaky tests. We have opened pull requests for 268
NIO tests and developers have accepted many of them. We hope
that our promising results on NIO tests and our publicly availabledataset [18] can spur more research on this topic.
ACKNOWLEDGMENTS
WethankYangChen,RuixinWang,SatvikEltepu,ReedOei,and
JonathanSteinfortheirhelp. Thisworkwaspartiallysupported
byUSNSFgrantsCCF-1763788andCCF-1956374,andbyNatural
Science Foundation of China (Grant No. 62161146003), and the
XPLORER PRIZE. Tao Xie (the corresponding author) is also with
the Key Laboratory of High Confidence Software Technologies
(PekingUniversity),MinistryofEducation,China.Weacknowledge
support for research on flaky tests from Facebook and Google.
1740
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Pu Yi, Zhengxi Li, Tao Xie, Darko Marinov, and Wing Lam
REFERENCES
[1] 2022. https://github.com/Activiti/Activiti/pull/3488
[2] 2022. https://github.com/apache/dubbo/pull/6936[3] 2022. https://github.com/apache/hadoop/pull/2482[4] 2022. https://github.com/spring-projects/spring-boot/pull/25435[5] 2022. https://github.com/spring-projects/spring-boot/pull/27664[6] 2022. https://github.com/josiest/geom/pull/1[7] 2022. https://github.com/mtik00/yamicache/pull/10[8] 2022. https://github.com/querydsl/querydsl/pull/2658[9] 2022. https://github.com/apache/hadoop/pull/2724
[10] 2022. https://github.com/apache/hadoop/pull/2500[11] 2022. https://github.com/apache/hadoop/pull/2499[12] 2022. https://github.com/zalando/riptide/pull/1020[13] 2022. https://github.com/vmware/admiral/pull/319[14] 2022. https://github.com/PolyJIT/benchbuild/pull/425[15]
2022. https://stackoverflow.com/questions/11585793/are-numpy-arrays-
passed-by-reference
[16] 2022. https://github.com/Zabamund/wellpathpy/pull/50[17] 2022. https://github.com/airbrake/pybrake/pull/163[18] 2022. NIO Tests. https://sites.google.com/view/nio-tests[19] 2022. TotT:Avoidingflakeytests. http://googletesting.blogspot.com/2008/04/
tott-avoiding-flakey-tests.html
[20] Activiti 2022. https://github.com/activiti/activiti[21]
Abdulrahman Alshammari, Christopher Morris, MichaelHilton, and Jonathan
Bell. 2021. FlakeFlagger: Predicting flakiness without rerunning tests. In ICSE.
[22]JonathanBell.2014. Detecting,isolating,andenforcingdependenciesamong
and within test cases. In FSE Doctoral Symposium.
[23]Jonathan Bell and Gail Kaiser. 2014. Unit test virtualization with VMVM. In
ICSE.
[24]JonathanBell,GailKaiser,EricMelski,andMohanDattatreya.2015. Efficient
dependency detection for safe Java test acceleration. In ESEC/FSE.
[25] BenchBuild 2022. https://github.com/PolyJIT/benchbuild[26]
Jeanderson Candido, Luis Melo, and Marcelo dâ€™Amorim. 2017. Test suite paral-
lelization in open-source projects: A study on its usage and impact. In ASE.
[27]ZhenDong,AbhishekTiwari,XiaoLiangYu,andAbhikRoychoudhury.2021.
Flaky test detection in Android via event order exploration. In ESEC/FSE.
[28] Dubbo 2022. https://github.com/apache/dubbo[29]
Saikat Dutta, August Shi, Rutvik Choudhary, Zhekun Zhang, Aryaman Jain,and Sasa Misailovic. 2020. Detecting flaky tests in probabilistic and machine
learning applications. In ISSTA.
[30]Saikat Dutta, August Shi, and Sasa Misailovic. 2021. FLEX: Fixing flaky tests in
machine learning projects by updating assertion bounds. In ESEC/FSE.
[31]Moritz Eck, Fabio Palomba, Marco Castelluccio, and Alberto Bacchelli. 2019.
Understanding flaky tests: The developerâ€™s perspective. In ESEC/FSE.
[32]SebastianElbaum,Alexey G.Malishevsky,andGregg Rothermel.2000. Priori-
tizing test cases for regression testing. In ISSTA.
[33]LamyaaEloussi.2016. Flakytests(andhowtoavoidthem). https://engineering.
salesforce.com/flaky-tests-and-how-to-avoid-them-25b84b756f60
[34]Facebook testing and verification request for proposals 2019. https:
//research.fb.com/programs/research-awards/proposals/facebook-testing-
and-verification-request-for-proposals-2019
[35]MartinFowler.2011.Eradicatingnon-determinismintests. https://martinfowler.
com/articles/nonDeterminism.html
[36]Alessio Gambi, Jonathan Bell, and Andreas Zeller. 2018. Practical test depen-
dency detection. In ICST.
[37]Zebao Gao, Yalan Liang, Myra B. Cohen, Atif M. Memon, and Zhen Wang. 2015.
Making system user interactive tests repeatable: When and what should we
control?. In ICSE.
[38]Google.2008. Avoidingflakeytests. http://googletesting.blogspot.com/2008/
04/tott-avoiding-flakey-tests.html
[39]Martin Gruber, Stephan Lukasczyk, Florian KroiÃŸ, and Gordon Fraser. 2021. An
empirical study of flaky tests in Python. In ICST.
[40]AlexGyori,AugustShi,FarahHariri,andDarkoMarinov.2015. Reliabletesting:
Detecting state-polluting tests to prevent test dependency. In ISSTA.
[41]Sarra Habchi, Maxime Cordy, Mike Papadakis, and Yves Le Traon. 2021. On the
use of mutation in injecting test order-dependency. In MSR.
[42]Sarra Habchi, Maxime Cordy, Mike Papadakis, and Yves Le Traon. 2021. A
replicationstudyontheusabilityofcodevocabularyinpredictingflakytests.
InMSR.
[43] Hadoop 2022. https://github.com/apache/hadoop[44]
MarkHarmanandPeterOâ€™Hearn.2018. Fromstart-upstoscale-ups:Opportu-
nities and open problems for static and dynamic program analysis. In SCAM.
[45]MaryJeanHarrold,JamesA.Jones,TongyuLi,DonglinLiang,AlessandroOrso,
Maikel Pennings, Saurabh Sinha, S. Alexander Spoon, and Ashish Gujarathi.
2001. Regression test selection for Java software. In OOPSLA.
[46]Kim Herzig, Michaela Greiler, Jacek Czerwonka, and Brendan Murphy. 2015.
The art of testing less without sacrificing quality. In ICSE.[47]KimHerzigandNachiappanNagappan.2015. Empiricallydetectingfalsetest
alarms using association rules. In ICSE.
[48]ChenHuoandJamesClause.2014. Improvingoraclequalitybydetectingbrittle
assertions and unused inputs in tests. In FSE.
[49]HeJiang,XiaochenLi,ZijiangYang,andJifengXuan.2017. Whatcausesmy
test alarm? Automatic cause analysis for test alarms in system and integration
testing. In ICSE.
[50]James A. Jones, Mary Jean Harrold, and John Stasko. 2002. Visualization of test
information to assist fault localization. In ICSE.
[51] JUnit 2022. https://junit.org[52]
JUnitandJava 72012. http://intellijava.blogspot.com/2012/05/junit-and-java-
7.html
[53]JUnit test method ordering 2022. http://www.java-allandsundry.com/2013/01/
junit-test-method-ordering.html
[54]Taesoo Kim, Ramesh Chandra, and Nickolai Zeldovich. 2013. Optimizing unit
testexecutioninlargesoftwareprogramsusingdependencyanalysis.In APSys.
[55]Emily Kowalczyk, Karan Nair, Zebao Gao, Leo Silberstein, Teng Long, and Atif
Memon. 2020. Modeling and ranking flaky tests at Apple. In ICSE SEIP.
[56]WingLam,PatriceGodefroid,SumanNath,AnirudhSanthiar,andSureshThum-
malapenta. 2019. Root causing flaky tests in a large-scale industrial setting. In
ISSTA.
[57]Wing Lam, KivanÃ§ MuÅŸlu, Hitesh Sajnani, and Suresh Thummalapenta. 2020. A
study on the lifecycle of flaky tests. In ICSE.
[58]Wing Lam, Reed Oei, August Shi, Darko Marinov, and Tao Xie. 2019. iDFlakies:
A framework for detecting and partially classifying flaky tests. In ICST.
[59]Wing Lam, August Shi, Reed Oei, Sai Zhang, Michael D. Ernst, and Tao Xie.
2020. Dependent-test-aware regression testing techniques. In ISSTA.
[60]Wing Lam, Stefan Winter, Angello Astorga, Victoria Stodden, and Darko Mari-
nov. 2020. Understanding reproducibility and characteristics of flaky tests
through test reruns in Java projects. In ISSRE.
[61]Wing Lam, Stefan Winter, Anjiang Wei, Tao Xie, Darko Marinov, and Jonathan
Bell. 2020. A large-scale longitudinal study of flaky tests. In OOPSLA.
[62]Johannes Lampel, Sascha Just, Sven Apel, and Andreas Zeller. 2021. When life
givesyouoranges:DetectinganddiagnosingintermittentjobfailuresatMozilla.
InESEC/FSE.
[63]Chengpeng Li, Chenguang Zhu, Wenxi Wang, and August Shi. 2022. Repairing
order-dependent flaky tests via test generation. In ICSE.
[64]Jingjing Liang, Sebastian Elbaum, and Gregg Rothermel. 2018. Redefining
prioritization: Continuous prioritization for continuous integration. In ICSE.
[65]Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An
empirical analysis of flaky tests. In FSE.
[66]Maintaining the order of JUnit3 tests with JDK 1.7. 2013. https://coderanch.
com/t/600985/engineering/Maintaining-order-JUnit-tests-JDK
[67] Maven 2022. https://maven.apache.org[68]
MavenSurefireplugin2022. https://maven.apache.org/surefire/maven-surefire-
plugin
[69]Atif Memon, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob Siem-
borski, and John Micco. 2017. Taming Google-scale continuous testing. In ICSE
SEIP.
[70] Gerard Meszaros. 2007. xUnit Test Patterns: Refactoring Test Code.
[71]JohnMicco.2017. ThestateofcontinuousintegrationtestingatGoogle.In ICST.
https://bit.ly/2OohAip
[72]Rashmi Mudduluru, Jason Waataja, Suzanne Millstein, and Michael D. Ernst.
2021. Verifying determinism in sequential programs. In ICSE.
[73]SuchitaMukherjee, AbigailAlmanza,and CindyRubio-GonzÃ¡lez. 2021. Fixing
dependency errors for Python build reproducibility. In ISSTA.
[74]MadanMusuvathi,ShazQadeer,andThomasBall.2007. CHESS:Asystematic
testing tool for concurrent software. Technical Report MSR-TR-2007-149.
[75]Pengyu Nie, Ahmet Celik, Matthew Coley, Aleksandar Milicevic, Jonathan Bell,
and Milos Gligoric. 2020. Debugging the performance of Mavenâ€™s test isolation:
Experience report. In ISSTA.
[76]OwainParry,GregoryM.Kapfhammer,MichaelHilton,andPhilMcMinn.2020.
Flakeitâ€™tillyoumakeit:Usingautomatedrepairtoinduceandfixlatenttest
flakiness. In ICSE (Workshops).
[77]Owain Parry, Gregory M Kapfhammer, Michael Hilton, and Phil McMinn. 2021.
A survey of flaky tests. TOSEM(2021).
[78]Gustavo Pinto, Breno Miranda, Supun Dissanayake, Marcelo dâ€™Amorim,Christoph Treude, and Antonia Bertolino. 2020. What is the vocabulary of
flaky tests?. In MSR.
[79] pybrake 2022. https://github.com/airbrake/pybrake[80] pytest 2022. https://docs.pytest.org/en/6.2.x[81] pytest-repeat 2022. https://pypi.org/project/pytest-repeat[82] Querydsl 2022. https://github.com/querydsl/querydsl[83]
Md Tajmilur Rahman and Peter C. Rigby. 2018. The impact of failing, flaky, and
high failure tests on the number of crash reports associated with Firefox builds.
InESEC/FSE.
[84]Maaz Hafeez Ur Rehman and Peter C. Rigby. 2021. Quantifying no-fault-found
test failures to prioritize inspection of flaky tests at Ericsson. In ESEC/FSE.
1741
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. Preempting Flaky Tests via Non-Idempotent-Outcome Tests ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[85]Alan Romano, Zihe Song, Sampath Grandhi, Wei Yang, and Weihang Wang.
2021. An empirical analysis of UI-based flaky tests. In ICSE.
[86]Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary Jean Harrold.
2001. Prioritizing test cases for regression testing. TSE(2001).
[87]August Shi, Alex Gyori, Owolabi Legunsen, and Darko Marinov. 2016. De-
tecting assumptions on deterministic implementations of non-deterministic
specifications. In ICST.
[88]AugustShi,WingLam,ReedOei,TaoXie,andDarkoMarinov.2019. iFixFlakies:
Aframeworkforautomaticallyfixingorder-dependentflakytests.In ESEC/FSE.
[89]FriedrichSteimann,MarcusFrenkel,andRuiAbreu.2013. Threatstothevalidity
and value of empirical assessments of the accuracy of coverage-based fault
locators. In ISSTA.
[90]Testverification2022. https://developer.mozilla.org/en-US/docs/Mozilla/QA/
Test_Verification
[91]Swapna Thorve, Chandani Sreshtha, and Na Meng. 2018. An empirical study of
flaky tests in Android apps. In ICSME.
[92]AnjiangWei,PuYi,TaoXie,DarkoMarinov,andWingLam.2021. Probabilistic
andsystematiccoverageofconsecutivetest-methodpairsfordetectingorder-
dependent flaky tests. In TACAS.[93] wellpathpy 2022. https://github.com/Zabamund/wellpathpy[94]
EricWendelin.2022. Introducingflakytestmitigationtools. https://blog.gradle.
org/gradle-flaky-test-retry-plugin
[95]Pu Yi, Anjiang Wei, Wing Lam, Tao Xie, and Darko Marinov. 2021. Finding
polluter tests using Java PathFinder. SEN(2021).
[96]ShinYooandMarkHarman.2012. Regressiontesting minimization,selection
and prioritization: A survey. STVR(2012).
[97]LingmingZhang,DarkoMarinov,LuZhang,andSarfrazKhurshid.2012. Re-
gression mutation testing. In ISSTA.
[98]Peilun Zhang, Yanjie Jiang, Anjiang Wei, Victoria Stodden, Darko Marinov, and
August Shi. 2021. Domain-specific fixes for flaky tests with wrong assumptions
on underdetermined specifications. In ICSE.
[99]SaiZhang,DarioushJalali,JochenWuttke,KÄ±vanÃ§MuÅŸlu,WingLam,MichaelD.
Ernst, and David Notkin. 2014. Empirically revisiting the test independence
assumption. In ISSTA.
[100]Celal Ziftci and Jim Reardon. 2017. Who broke the build?: Automatically iden-
tifying changes that induce test failures in continuous integration at Google
scale. InICSE.
1742
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:34:15 UTC from IEEE Xplore.  Restrictions apply. 