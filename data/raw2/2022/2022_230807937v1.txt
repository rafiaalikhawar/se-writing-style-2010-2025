Automated Testing and Improvement of Named Entity
Recognition Systems
Boxi Yu
boxiyu@link.cuhk.edu.cn
School of Data Science, The Chinese
University of Hong Kong, Shenzhen
(CUHK-Shenzhen)
ChinaYiyan Hu
yiyanhu@link.cuhk.edu.cn
School of Science and Engineering,
The Chinese University of Hong
Kong, Shenzhen (CUHK-Shenzhen)
ChinaQiuyang Mang
qiuyangmang@link.cuhk.edu.cn
School of Data Science, The Chinese
University of Hong Kong, Shenzhen
(CUHK-Shenzhen)
China
Wenhan Hu
wenhanhu@link.cuhk.edu.cn
School of Data Science, The Chinese
University of Hong Kong, Shenzhen
(CUHK-Shenzhen)
ChinaPinjia Heâˆ—
hepinjia@cuhk.edu.cn
School of Data Science, The Chinese
University of Hong Kong, Shenzhen
(CUHK-Shenzhen)
China
ABSTRACT
Named entity recognition (NER) systems have seen rapid progress
in recent years due to the development of deep neural networks.
These systems are widely used in various natural language pro-
cessing applications, such as information extraction, question an-
swering, and sentiment analysis. However, the complexity and
intractability of deep neural networks can make NER systems un-
reliable in certain circumstances, resulting in incorrect predictions.
For example, NER systems may misidentify female names as chem-
icals or fail to recognize the names of minority groups, leading to
user dissatisfaction. To tackle this problem, we introduce TIN, a
novel, widely applicable approach for automatically testing and
repairing various NER systems. The key idea for automated testing
is that the NER predictions of the same named entities under similar
contexts should be identical. The core idea for automated repairing
is that similar named entities should have the same NER prediction
under the same context. We use TIN to test two SOTA NER models
and two commercial NER APIs, i.e., Azure NER and AWS NER. We
manually verify 784 of the suspicious issues reported by TIN and
find that 702 are erroneous issues, leading to high precision (85.0%-
93.4%) across four categories of NER errors: omission, over-labeling,
incorrect category, and range error. For automated repairing, TIN
achieves a high error reduction rate (26.8%-50.6%) over the four
systems under test, which successfully repairs 1,056 out of the 1,877
reported NER errors.
KEYWORDS
Metamorphic testing, named entity recognition, software repairing,
AI software
1 INTRODUCTION
Named entity recognition (NER) systems have become popular in
recent years and are widely used to enhance natural language pro-
cessing (NLP) applications such as information retrieval, machine
âˆ—Pinjia He is the corresponding author.translation, and text classification. With the development of the In-
ternet, mountains of data accumulate every day. Common Crawl,1
a nonprofit organization that crawls the web and freely provides its
archives and datasets to the public, has collected 380 TB of data and
3.15 billion pages by October 2022. As of 30 December 2022, there
are 6,594,544 articles in the English Wikipedia and it contains over
4 billion words [ 4]. Being able to identify the semantics of interest
in unstructured texts, NER systems play a significant role in several
downstream applications. For instance, chatbots like ChatGPT and
Googleâ€™s Bard employ NER to identify and categorize entities in
user queries, enhancing response accuracy, while in finance, NER
algorithms sift through reports and online mentions, extracting
and classifying data for in-depth profitability analysis and real-time
stock market trend monitoring.2News providers, such as publish-
ing houses, can harness NER to sift through and categorize their
abundant daily content. By detecting crucial entities like people,
organizations, and places in articles, they can seamlessly tag and
organize them, optimizing content discovery for readers.3
Though achieving shining F1-Score on multiple NER benchmarks
(e.g., CoNLL03 [ 31] and OntoNotesv5 [ 36]), current NER systems
are far from perfect, and errors produced by these systems largely
dissatisfy the users and could lead to detrimental influence. In
NER systems, errors pertain to either the incorrect delineation
of named entities within a text corpus or the misclassification of
such identified entities into inappropriate categories. In Table 1,
we present multiple instances of NER errors, encompassing four
categories: omission, over-labeling, incorrect category, and range
error. For example, in the first sentence, the name "Paul" is clearly
identifiable as a person. However, the NER system fails to recognize
any word as a named entity, leading to an omission error. The
inaccuracies stemming from NER errors can reverberate across a
broad spectrum of domains. For example, in the report by Mishra
et al. [ 26], some NER models are better at identifying White names
across all datasets with higher confidence compared with other
1https://commoncrawl.org/
2https://www.techtarget.com/whatis/definition/named-entity-recognition-NER
3https://towardsdatascience.com/named-entity-recognition-applications-and-use-
cases-acdbf57d595earXiv:2308.07937v1  [cs.CL]  14 Aug 2023ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, and Pinjia He
demographics, such as Black names. In addition, Zhao et al. [ 39]
found that some NER systems are prone to identifying female names
as chemicals, and most NER systems perform better on male-related
data than female-related data.
Although the reliability of NER systems is of great importance,
there is a dearth of general methods for automatically testing and
repairing various NER systems since it is quite challenging. First,
unlike traditional code-based software, NER systems are mainly
based on deep neural networks with millions of parameters. There-
fore, many testing techniques that perform well on code-based
software may not be suitable for testing NER systems. Second, it is
laborious to manually construct the test oracle, i.e., large amounts
of text data with labels indicating the presence of named entities,
such as people and organizations. Third, NER systems have mul-
tiple standards for identifying and categorizing named entities in
text, which further compounds the difficulty of designing a general
method for testing and repairing various NER systems.
Traditional approaches for repairing AI-based systems either
adopt data augmentation [ 13,14,38] or algorithm optimization.
Thus, these approaches need manual labeling of the data or modifi-
cation of the model architecture, which often incurs a high cost. In
addition, these traditional methods need to access the model in a
white-box manner, while a black-box testing and repair approach
would be much more general.
To address the challenges, we propose TIN, the first approach
for automatically testing and repairing NER systems, which is ap-
plicable to NER systems with various standards. We design three
transformation schemes for test case generation, including similar
sentence generation, structural transformation, and random entity
shuffle. Meanwhile, we design the corresponding metamorphic rela-
tions for each of the transformation schemes. Then we construct the
test inputs consisting of the original sentence, the mutant sentence,
and their NER predictions. The test input is reported as a suspicious
issue if it does not satisfy the corresponding metamorphic relation.
After receiving the suspicious issues from the automated testing,
our location algorithm would be used to locate the suspicious en-
tity that is prone to NER errors. For each suspicious entity, we use
BERT [ 12] to generate similar named entities and a novel repairing
algorithm to predict the correct NER category of the suspicious
entity by leveraging the prediction of similar entities under the
same context.
We apply TIN to test two SOTA NER models from Flair [ 2] (a
widely-used NLP library), and two commercial APIs from Azure and
AWS. The two SOTA models Flair-CoNLL and Flair-Ontonotes are
trained with the NER standard of CoNLL03 [ 31] and OntoNotesV5
[36], respectively. For the commercial APIs, we denote Azure Cog-
nitive Services for Language as "Azure NER", and AWS Sagemaker
as "AWS NER" for simplicity. To verify the effectiveness of TIN,
we manually inspect part of the results for testing and repairing.
TIN successfully reports 702 erroneous issues out of 784 suspicious
issues with a high precision ranging from 85.0% to 93.4% on the four
NER systems under test. The detected NER errors in the erroneous
issues include omission, mislabeling, incorrect category, and range
error. In terms of NER repairing, TIN improves predictions with a
high rate ranging from 48.1%. to 52.2%, while only downgrading
predictions in only a low ratio from 12.1% to 19.5%. TIN also ex-
hibits a high error reduction rate (26.8%-50.6%) on the four NERsystems, demonstrating its effectiveness in repairing NER systems
and reducing NER errors. All the source code and data in this work
will be released for reuse.
This paper makes the following main contributions:
â€¢It introduces TIN, a novel, widely-applicable approach for
automatically testing and repairing NER systems.
â€¢TIN provides three transformation schemes for test case
generation and the corresponding metamorphic relations for
NER error detection.
â€¢TIN contains a novel repairing algorithm that can effectively
fix the reported NER errors.
â€¢It presents the evaluation of TIN against SOTA models and
commercial APIs, achieving a high precision of 85.0%-93.4%
for reporting erroneous issues, and a high error reduction
rate of 26.8%-50.6% for repairing the NER errors.
2 PRELIMINARIES
2.1 Named Entity Recognition
NER systems are essential tools in NLP because they allow ma-
chines to extract structured information from unstructured text.
NER systems are used in a variety of applications, including infor-
mation extraction, machine translation, and question answering.
For example, a NER system might be used to extract the names of
people, organizations, and locations from a news article. This infor-
mation can then be used to build a database of entities and their
relationships, which can be queried and analyzed to gain insights
or to perform tasks such as information retrieval or summarization.
Overall, NER systems are significant because they enable machines
to understand and make sense of large volumes of unstructured text,
which is an increasingly important task in todayâ€™s digital world.
NER task has multiple standards, e.g., the NER dataset CoNLL03
[31] and Ontonotesv5 [ 36] have different NER standards. In addition,
the four NER systems have different NER standards, which only
share the common categories of PERSON (PER),LOCATION (LOC),
andORGANIZATION (ORG) (in Table 2). The various identification
standards further exacerbate the difficulty of manual test oracle
construction.
2.2 Constituency Parser
A constituency parser is a natural language processing tool that
is used to analyze and understand the syntactic structure of sen-
tences. It works by breaking down a sentence into its constituent
parts, known as constituents, and arranging them in a tree-like
structure known as a phrase structure tree or a constituency tree.
The tree represents the syntactic relationships between the words
in a sentence and the grammatical roles they play. In the process
of analyzing sentences and producing phrase structure trees, con-
stituency parsers use context-free grammars (CFGs [ 5]). CFGs are
a set of rules that specify how words can be combined to form
phrases and sentences. The parser uses these rules to identify the
constituents of a sentence and the relationships between them.
In TIN, we adopt the constituency parser implemented in Stan-
ford CoreNLP [ 1] to parse the sentences. In the constituency parser,
a non-terminal node is a node in the phrase structure tree that repre-
sents a syntactic category, such as a noun phrase (NP), verb phrase
(VP), and sentence (S). Non-terminal nodes are used to represent theAutomated Testing and Improvement of Named Entity Recognition Systems ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA
Table 1: Example of the NER errors
Error Type Sentence Predicted Entities Target Entities
Omission Sir Paul â€™s command of the stage is so casual that he makes it look easy ( Flair-Ontonotes ). NULL ["Paul", PER]
Over-labeling Elon Musk is having a similar effect on the platform ( Azure ).["Elon Musk", PERSON ]
["Platform", LOCATION ]["Elon Musk", PERSON ]
Incorrect CategoryNorrie believes securing Unesco status could offer new opportunities
in sustainable tourism and branding of local produce, while at the
same time highlighting the environmental value of the peatland ( Flair-Conll ).["Norrie", PER]
["Unesco", MISC ]["Norrie", PER]
["Unesco", ORG]
Range Error Det Supt Rance said the investigation remained active ( AWS ).["Det", PERSON ]
["Supt Rance", PERSON ]["Det Supt Rance", PERSON ]
Table 2: NER systems with different standards
NER Systems NER Categoreis
Flair-CoNLL PERSON ,ORGANIZATION ,LOCATION , MISCELLANEOUS NAMES
Flair-Ontonotes PERSON ,ORGANIZATION ,LOCATION , CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, MONEY, NORP, ORDINAL, PERCENT, PRODUCT, QUANTITY, TIME, WORK-OF-ART
Azure NER PERSON ,ORGANIZATION ,LOCATION , PERSONTYPE, EVENT, PRODUCT, SKILL, ADDRESS, PHONENUMBER, EMAIL, URL, IP, DATETIME, QUANTITY
AWS NER PERSON ,ORGANIZATION ,LOCATION , COMMERCIAL ITEM, DATE, EVENT, OTHER, QUANTITY, TITLE
internal structure of the sentence and the relationships between its
constituents. A terminal node is a leaf node in the phrase structure
tree that represents an individual word or punctuation mark in the
sentence. Terminal nodes are used to represent the basic building
blocks of the sentence, and they do not have any children.
2.3 Problem Definition
2.3.1 NER Testing. We denote the NER systems under test as ğ‘,
and the input sentence as ğ‘ . TIN adopts metamorphic testing to
alleviate the test oracle problem. ğ‘(ğ‘ )represents the output of the
NER systems, which contains a list of NER predictions with the
corresponding NER categories, e.g., PERSON orORGANIZATION . TIN
transforms the original sentence ğ‘ to obtain the mutant sentence
ğ‘ â€²through transformation ğ‘ â€²=ğ‘‡(ğ‘ )by utilizing the structure of
the sentence ğ‘ and the NER output ğ‘(ğ‘ ). We then design the cor-
responding metamorphic relation (MR) for the NER output pair
(ğ‘(ğ‘ ),ğ‘(ğ‘ â€²)), and report the(ğ‘(ğ‘ ),ğ‘(ğ‘ â€²))along with(ğ‘ ,ğ‘ â€²)as a
suspicious issue if the metamorphic relation is unsatisfied.
2.3.2 NER Repairing. NER repairing aims to repair the NER errors
and gives the correct NER prediction. We design a repairing system
ğ‘…to repair the NER predictions of the suspicious issues reported by
the testing part of TIN. We repair both the NER prediction of the
original sentence and the mutant sentence and obtain the repaired
NER predictions ğ‘…(ğ‘ )andğ‘…(ğ‘ â€²).
3 APPROACH AND IMPLEMENTATION
3.1 Overview
The overview framework of TIN is shown in Fig 1, which consists of
three main components, which are test case generation, detecting
suspicious issues and automated repairing, respectively.
The first part of TIN is test case generation. TIN first feeds the
original sentences into the NER systems and gets the NER prediction
of the original sentences, which are used to assist the automated
generation of test cases. After generating multiple mutant candi-
dates, we use sentence filters to filter out low-quality sentences.
The filtered mutants are then used for automated testing.
The second part of TIN is detecting suspicious issues. We con-
struct the test inputs, which consist of the sentence and the NERpredictions of the original and mutant sentences. We design the
metamorphic relation for each of the transformation schemes and
report the suspicious issues that may contain NER errors if the
metamorphic relations are violated.
The third part of TIN is automated repairing. After we obtain
the suspicious issues from the second part of TIN, we start to locate
the suspicious entities that may lead to the NER errors. We use
BERT [ 12] to generate mutant named entities with similar semantics
to replace the suspicious entities. We then filter them through entity
filtering to obtain the filtered mutants. We feed the filtered mutants
to the NER systems and obtain a list of NER predictions for repairing
the suspicious entities. In practice, we develop a scoring system to
leverage the entity predictions to repair the suspicious entities.
3.2 Automatically Generating Test Cases
To automatically generate test cases, we design three transforma-
tion schemes to generate mutant sentences from the original sen-
tence. The transformation schemes consist of: (1) similar sentence
generation, (2) structural transformation, and (3) random entity
shuffle. We will introduce each of these transformation schemes
and their implementations in the following.
3.2.1 Similar Sentence Generation. The core idea of generating
similar sentences is to substitute the words or phrases in the sen-
tences with the ones that have similar semantics. In this procedure,
we avoid modifying named entities of the original sentences pre-
dicted by the NER systems. We implement two methods for similar
sentence generation, including token-level and phrase-level similar
sentence generation, which are explained as below.
1) Token-level similar sentence generation: After we obtain
the NER prediction of the original sentence ğ‘(ğ‘ ), we mask the
token that does not belong to a named entity in turn (one word
being masked each time) and feed the masked sentence into BERT
to generate a list of candidate words. We select the top-K most
predictive candidate words and use them to replace the original
ones and generate mutant sentences. During this process, we only
use verbs and adjectives as candidate tokens to avoid grammatically
strange or incorrect sentences. In addition, we ensure the token toESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, and Pinjia He
Original sentenceNER predictionsNoSatisfied?
SentencemutationNER systems
Mutant candidatesSentencefilteringSuspiciouslocationNER systems
FixedpredictionEntityfiltering
FilteredmutantsNER systemsNER predictionsNewpredictionOriginalpredictionSuspicious issuesMutant candidatesSuspiciousentitiesFilteredmutants
OriginalsentenceMutantsentenceNewpredictionOriginalpredictionTest inputsOriginalsentenceMutantsentenceMRCheckingNER predictionsRepairingsystem
Test case generationDetecting suspicious issuesAutomated repairingæ€»æ¡†æ¶å›¾ï¼šåˆ†æˆä¸‰ä¸ªéƒ¨åˆ†overall
Sentence transformationEntitymutation
Figure 1: Overview of TIN
But NHS England says emergency care will continue to be provided.S
CC NP VP .
VBZ SBAR
S
NP VPBut NHS England
says
emergency care will continue be 
provided.
NP
medical care
Insert Delete
Figure 2: Phrase-level similar sentence generation
be replaced keeps the identical part-of-speech, by using the NLTKâ€™s
POS tagging tool [3].
2) Phrase-level similar sentence generation: Besides token-
level replacement, we design a phrase-level sentence transformation
scheme by utilizing the constituency parser. Specifically, we only
modify the noun phrase to avoid grammar errors. As shown in Fig. 2,
we use a constituency parser to parse the original sentence and find
NP nodes where there are no other NP nodes in their subtree, i.e.,
the NP nodes with the smallest unit. Then we use sense2vec [ 35]
to generate similar noun phrases to replace the content in the NP
nodes we find in turn (one NP node being replaced each time).
As shown in Fig. 2, the NP node with " emergency care " has been
replaced by the NP node with " medical care ". Finally, we obtain a
list of mutant sentences, each with one noun phrase replaced.3.2.2 Structural Transformation. The general idea of structural
transformation is changing the syntactic structure of the original
sentence as well as ensuring the new sentences have a similar
context to the original sentence. Specifically, we provide three
transformation implementations of sentence syntactic structure to
transform the declarative sentence into the interrogative sentence.
We first use a constituency parser to parse the original sentence
and obtain a parse tree. Then we find the sentence node S that
satisfies the CFGs grammar of " Sâ†’NP VP ", and check the first
child VP node of S that modifies the NP node. There are different
ways to handle the first child VB node of the VP node, and we use
a specific transformation method for each case.
1)If the VB node is a "be verb" that serves as a "normal verb" and
directly modifies a noun phrase, e.g., the sentence " He is a student. ",
we will transform the sentence structure from <subject><verb><(obj
ect)*> to <verb><subject><(object)*>, and get a sentence " Is he a
student? ". Please notify we use * here to represent that the object is
not necessary. As shown in Fig. 3 (A), we will move the VB node to
the position of the first child of node S. As a result, the declarative
sentence " Twitter was the obvious solution " is transformed into the
interrogative form " Was twitter the obvious solution ".
2)If the VB node is a "normal verb" that is not a "be verb", e.g.,
the sentence with the transitive verb " I eat a burger " or the sentence
with the intransitive verb " He cried ", we will transform the sentence
structure from <subject><verb><(object)*> to <Aux><subject><ver
b><(object)*>. As shown in Fig. 3 (B), we will insert a VB node with
an "auxiliary verb" at the position of the first child of node S.
3)If the VB node is an "auxiliary verb", e.g., the sentence " He
has faced floods ", we will transform the sentence structure from
<subject><aux><verb><(object)*> to <aux><subject><verb><(objAutomated Testing and Improvement of Named Entity Recognition Systems ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA
S
NP VP .
NPTwitter Was
the obvious solution?VBSubject (Object)* Verb
S
NP VP .
VB NPTwitter
was the obvious solution.Subject (Object)* Verb
Twitter was the obvious solution. Was Twitter the obvious solution?A
S
NP VP .
VB NPBelarus
shares a border with Russia
as well as Ukraine.Subject (Object)* Verb
Belarus shares a border with Russia as well as Ukraine .BNP VP .
VB NPBelarus
share a border with Russia
as well as UkraineSubject (Object)* Verb
?VB
Does Belarus share a border with Russia as well as Ukraine ?Aux
S
S
NP VP .
VB VPTravellers
have.Subject (Object)* Verb
C
VB NP
faced Hogmanay disruption on Scotland 's 
railways after Friday's floods
Travellers have faced Hogmanay disruption on 
Scotland 's railways after Friday's floods.S
NP VP . VB
VPtravellers
VB NP
faced Hogmanay disruption on Scotland 's 
railways after Friday's floods
Have travellers faced Hogmanay disruption on 
Scotland 's railways after Friday's floods?Have ?Does
Aux Aux Subject Verb (Object)*
Figure 3: Structural transformation
ect)*>. As shown in Fig. 3 (C), we will move the VB node to the
position of the first child of node S.
After the movement or insertion, we replace the "." at the end of
the sentence with "?".
3.2.3 Random Entity Shuffle. We propose to use random entity
shuffle to generate mutant sentences and test whether the NER
predictions keep the same. We first use the NER systems to obtain
the NER predictions and replace the original named entities with
the placeholders of their own type, e.g., < ORG> and < PER>. Then
we randomly shuffle the order of the named entities of the same
category within the sentence and place the named entities into the
corresponding placeholders to generate mutant sentences.
For example, the sentence " Spotify, Apple Music, and Deezer all
said the track was their top performer of the year, beating competi-
tion from Ed Sheeran, Drake, and Taylor Swift. " will be modified to
"<ORG>, <ORG>, and < ORG> all said the track was their top performer of
the year, beating competition from < PER>, <PER>, and < PER>". After
we random shuffled and filled named entities for each NER category
respectively, mutant sentences such as " Apple Music, Spotify, and
Deezer all said the track was their top performer of the year, beat-
ing competition from Ed Sheeran, Taylor Swift, and Drake. " will be
generated.
3.3 Filters to Improve the Quality of Test Cases
To improve the quality of the test cases generated by TIN, we adopt
a semantic filter to ensure the semantic similarity between the
original sentence and mutant sentence, and a syntactic filter to
filter out the sentences that are prone to grammar errors.
3.3.1 Semantic Filter. In automated testing, we have two transfor-
mation methods for similar sentence generation, including token-
level and phrase-level substitution. As the same named entity may
have different NER categories under various contexts, we need toensure that these two transformation methods only change the
semantics of the sentence subtly. In practice, we use BERT to obtain
the context-aware embedding of the word or phrase and compute
the cosine similarity between the mutant and the original one. To
avoid ambiguity, we denote BERT _MLM as the function we use to
generate words by using the BERT masked language model and
BERT _Embedding as the function that we use BERT to compute the
context-aware embedding. The context-aware embedding â„of the
wordğ‘¤in a sentence ğ‘ is denoted as â„=BERT _Embedding(ğ‘¤,ğ‘ ).
For a phrase that contains multiple words, we use the average of
the context-aware embedding vectors in the phrase as its context-
aware embedding. Given a context-aware embedding vector â„and
another context-aware embedding vector â„â€², the semantic similarity
is defined as:
CosSim(ğ’‰,ğ’‰â€²)=ğ’‰ğ‘»Â·ğ’‰â€²
âˆ¥ğ’‰âˆ¥âˆ¥ğ’‰â€²âˆ¥, (1)
which is the cosine similarity between â„andâ„â€².
The semantic filter will filter out the mutant sentences where
the semantic similarity CosSim is less than a threshold ğ‘†ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ .
Specifically, we set ğ‘†ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ as 0.65 for token-level replacement
and phrase-level replacement based on experience, which achieves
good performance on four NER systems under test.
3.3.2 Syntactic Filter. When we transform the original sentence
to generate mutant sentences, there may include grammar errors,
punctuation errors, or rarely used expressions. After the transfor-
mation of the sentences, it is significant to filter out these test
cases, which rarely appear in the real world. To this end, we use
the syntactic evaluator SynEval implemented by AEON to compute
the syntactic score of our mutant sentences, which evaluates the
naturalness through the perplexity of the Pre-trained Language
model. Specifically, we calculate the difference in the syntactic score
between the original sentence ğ‘ and the mutant sentence ğ‘ â€², which
is defined as:
ğœ–=SynEval(ğ‘ )âˆ’SynEval(ğ‘ â€²). (2)
We adopt the syntactic filter for all the transformation approaches,
and use a threshold ğ‘†ğ‘¦ğ‘›ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ to filter out the low-quality
mutant sentences with ğœ–greater than ğ‘†ğ‘¦ğ‘›ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ . We use grid
search to find the best ğœ–value between 0 and 0.05, balancing quan-
tity and quality of test cases. For Flair-CoNLL, which only contains
four NER categories, we set ğœ–as 0.02 for structural transformation,
and 0.01 for other sentence transformation. For the other three NER
systems, which have much more NER categories, we set ğœ–as 0 for
all transformation methods to achieve stricter filtering.
3.4 Detecting Suspicious issues
After we generate the mutant sentences and use the filters to select
the test cases with high quality, we begin to construct the test inputs
for TIN. The test inputs contain the original and mutant sentences
and their predictions by the NER systems. As shown in Fig. 1, we
examine the metamorphic relations of the test inputs. We check if
the first metamorphic relation, ğ‘€ğ‘…1, is met for generating similar
sentences.ğ‘€ğ‘…1requires:
ğ‘(ğ‘ ,ğ‘’)=ğ‘(ğ‘ â€²,ğ‘’),âˆ€ğ‘’, ğ‘ .ğ‘¡.,ğ‘’âˆˆğ‘ andğ‘’âˆˆğ‘ â€², (3)ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, and Pinjia He
Named entityPrediction 1Prediction 2Appear in both sentenceSuspicious entity or notUSLOCLOCYESNOAmericanNULLMISCNONOBBC News RussianORGMISCYESYESSentence[1]:Some of those supporting theUS (LOC)had posed asindependent mediaoutlets and some had tried to pass off content from legitimate outlets,such asBBC News Russian (ORG), as their own.Sentence[2]:Some of those supporting theUS (LOC)had posed asAmerican (MISC)mediaoutlets and some had tried to pass off contentfrom legitimate outlets, such asBBC News Russian (MISC), as their own.
Figure 4: Suspicious entities location
whereğ‘(ğ‘ ,ğ‘’)andğ‘(ğ‘ â€²,ğ‘’)represents the NER prediction of ğ‘’in the
original sentence ğ‘ and the mutant sentence ğ‘ â€². In this case, a token
or phrase is changed in the mutant sentence ğ‘ â€², which may result
in a new named entity that does not exist in the original sentence.
Thus, we would not check the named entities that do not exist in
both sentences, since the NER predictions of these named entities
are not defined in both sentences.
However, structural transformation and random entity shuffle do
not involve new named entities, which only change the order of the
named entities or the syntactic structure of the sentence. Therefore,
we check whether the test inputs satisfy the second metamorphic
relationğ‘€ğ‘…2, defined as below:
ğ‘(ğ‘ )=ğ‘(ğ‘ â€²). (4)
If a test input does not satisfy the metamorphic relations, TIN will
report the test input as a suspicious issue.
3.5 Automatically Repairing the Named Entity
Recognition Errors
After a suspicious issue is raised, we will repair both sentences
ğ‘ andğ‘ â€²to obtain fixed NER predictions ğ‘…(ğ‘ )andğ‘…(ğ‘ â€²). For the
original sentence ğ‘ , ifğ‘(ğ‘ )â‰ ğ‘…(ğ‘ ), we will report a repairing
attempt replacing ğ‘(ğ‘ )withğ‘…(ğ‘ ), and for the mutant sentence ğ‘ â€², if
ğ‘(ğ‘ â€²)â‰ ğ‘…(ğ‘ â€²), we will report a repairing attempt replacing ğ‘(ğ‘ â€²)
withğ‘…(ğ‘ â€²). The process of black-boxed repairing consists of the
following two parts, i.e., suspicious entity location and relabeling.
3.5.1 Suspicious Entity Location. A suspicious issue consists of
two sentences and their NER predictions. However, we do not
know which named entity contains the NER errors and needs to
be repaired. To ensure the quality of the fixed NER prediction, we
need to locate the suspicious entities that are prone to NER errors.
We compare the contents and the NER predictions between the
original sentence ğ‘ and the mutant sentence ğ‘ â€²to locate suspicious
entities. For each sentence, an entity ğ‘’is a suspicious entity if both
of the following conditions are satisfied.
â€¢The entity exists in both sentences: ğ‘’âˆˆğ‘ andğ‘’âˆˆğ‘ â€²
â€¢The NER prediction of this entity is different between the
two sentences: ğ‘(ğ‘ ,ğ‘’)â‰ ğ‘(ğ‘ â€²,ğ‘’)
We then add all satisfied ğ‘’to a list as the suspicious entities of both
sentences.
For example, as shown in Fig. 4, we first iterate all the named en-
tities predicted by the NER systems and obtain their NER categories.
The information on these named entities is shown in the table. " BBC
News Russian " appears in both sentences, and the predictions of its
Evaluation FunctionBBC News is an operational business division of the BBC.
Suspicious Entity : "BBC News " -LOC
[MASK] News is an operational business division of the BBC.
BBC [MASK] is an operational business division of the BBC.
BERT Masked Language Model"CNN News "
"Fox News "
"BBC Newspaper "
"BBC company "
CNN News is an operational business division of the BBC.
Fox News is an operational business division of the BBC.
BBC Newspaper is an operational business division of the BBC.NER System
"CNN News "-ORG -0.3
"Fox News "-ORG -0.4
"BBC Newspaper "-MISC -0.3ORG -0.7, MISC -0.3Relabeled Named Entity
BBC News -ORG -0.7
Select the NER type with
the maximum score 
Filtering
Sentence GenerationFigure 5: Suspicious entity relabel
NER category are different between the two sentences, thus it is
reported as a suspicious entity located.
3.5.2 Suspicious Entity Relabeling. Given a suspicious entity, we
would relabel the suspicious entity by our repairing algorithm in
turn (one suspicious entity being relabeled each time). Similar to
the automated testing part, we first generate and filter the mutant
entities which have similar semantics to the suspicious entity. Then
we feed the mutant entities into a scoring system to obtain the fixed
NER prediction.
1) Similar entity generation: Given an input sentence ğ‘ and a
suspicious entity ğ‘’ğ‘ , TIN masks each word/subword (a fragment of
a word that might not stand alone as a full word in the language)
tokenized by BERT ofğ‘’ğ‘ in turn (one word or subword being masked
each time) and feeds the masked sentences into BERT_MLM to
generate top-K mutant entities with predictive logit.
2) Similar entity filter: Next, to ensure high quality of the
similar entities, we adopt another threshold ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ and filter
out mutant entities with a predictive logit ğ‘lower thanğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ .
To ensure the mutant entities have consistent semantics with the
suspicious entity, we then compute the context-aware embedding
for the suspicious entity and the mutant entities. For each mutant
entityğ‘’ğ‘–, we filter out ğ‘’ğ‘–if the predefined semantic similarity be-
tween the embedding of ğ‘’ğ‘ and embedding of ğ‘’ğ‘–is lower than a
thresholdğ‘†ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ . We also check the format consistency be-
tween the suspicious entity and the mutant entities, and remove
those with inconsistent formats. For example, the case of the first
letter of the generated word must remain unchanged. In practice, we
setğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ as 5.5 andğ‘†ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ as 0.45 based on experience,
which outperforms well on the four NER systems.
3) Scoring system: After entity generation and filtering, we re-
place the suspicious entity with the filtered mutant entities ğ‘’â€²
1,ğ‘’â€²
2,...
ğ‘’â€²ğ‘šin turn to generate mutant sentences ğ‘ â€²
1,ğ‘ â€²
2,...ğ‘ â€²ğ‘š, which will
be fed into the NER system to obtain the NER prediction of each
mutant entityâˆ€1â‰¤ğ‘–â‰¤ğ‘šğ‘(ğ‘ â€²
ğ‘–,ğ‘’â€²
ğ‘–). Specifically, we add a new NER
category named NULL to indicate that the words should not be
recognized as a named entity. For each NER category, we add a
score calculated by an evaluation function ğ¹(ğ‘,ğ‘ ğ‘–ğ‘š)to it in the
scoring system, where ğ‘denotes the predictive logit of the mutant
entity andğ‘ ğ‘–ğ‘šdenotes the semantic similarity. For the masked
word/subword ğ‘¤, mutantğ‘’â€²and mutant sentence ğ‘ â€², we define theAutomated Testing and Improvement of Named Entity Recognition Systems ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA
At the moment , Mr Moodley uses YouTube clips of test  matches to collect footage, but he hopes to deal with Cricket South Africato secure more videos.Suspicious Entities: "Cricket South Africa", "South Africa"Relabeled Prediction"Cricket South Africa"-ORG-0.911"South Africa"-LOC-0.503Suspiicous Entity Relabeling
Fixed Named Entity Prediction"Cricket South Africa"-ORG-0.911"Cricket South Africa""South Africa"are conflicted in rangeLOC-0.503 < ORG-0.911Range Conflict DetectingDeprecate"South Africa"
Figure 6: Repairing of range conflict
evaluation function as below:
ğ¹(ğ‘,ğ‘ ğ‘–ğ‘š)=ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³ğœ†Â·ğ›¼Â·ğ‘exp(ğ‘˜Â·ğ‘ ğ‘–ğ‘š)ğ‘(ğ‘ â€²,ğ‘’â€²)=NULL,ğ‘¤is subword,
ğ›¼Â·ğ‘exp(ğ‘˜Â·ğ‘ ğ‘–ğ‘š)ğ‘(ğ‘ â€²,ğ‘’â€²)=NULL,ğ‘¤is word,
ğœ†Â·ğ‘exp(ğ‘˜Â·ğ‘ ğ‘–ğ‘š)ğ‘(ğ‘ â€²,ğ‘’â€²)â‰ NULL,ğ‘¤is subword,
ğ‘exp(ğ‘˜Â·ğ‘ ğ‘–ğ‘š)ğ‘(ğ‘ â€²,ğ‘’â€²)â‰ NULL,ğ‘¤is word
(5)
whereğ‘˜is a constant to balance the weights between the predic-
tive logit and the semantic similarity, ğ›¼,ğœ†are constant coefficients
between(0,1]used for controlling the influence of the newly added
NER category NULL and the subword tokenized by BERT , respec-
tively. In practice, we set ğ‘˜=2.5,ğ›¼=0.2, andğœ†=0.5for all NER
systems we adopt TIN to repair. Finally, the NER category with
the maximum score in the scoring system will be selected as the
relabeled NER prediction of the suspicious entity.
The overall process of the relabeling is detailed by Algorithm 1.
We define a function Relabel where the input consists of a suspicious
entityğ‘’ğ‘ and the sentence ğ‘ . The output of the function is the
relabeled NER category ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› and a scoreğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ for the NER
category. We first define an empty dict ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ to store the score of
each NER category during the process (Line 2). Then we enumerate
each word/subword ğ‘¤ğ‘inğ‘’ğ‘ (Line 4) and mask ğ‘¤ğ‘by a special mark
"[MASK]" to obtain a masked sentence ğ‘ ğ‘šğ‘ğ‘ ğ‘˜ (Line 5). The masked
language model function BERT _MLM generates a list of candidate
wordsğ¶ğ‘¤with the corresponding predictive logits ğ‘and returns
the top-K candidate words with the highest predictive logit (Line 6).
The embedding function BERT _Embedding extracts the context-
aware embedding of the suspicious entity and the mutant entities
(Lines 3 and 12). We enumerate each word ğ‘¤ğ‘and its predictive
logitğ‘inğ¶ğ‘¤(Line 7) and filter them through the introduced filters:
format Consistency, predictive logit, and semantic similarity (Lines
8 to 14). After that, we generate the mutant entity ğ‘’ğ‘šand the
mutant sentence ğ‘ ğ‘šwith the filtered ğ‘¤ğ‘, which will be fed into
the NER system to obtain the NER prediction of ğ‘’ğ‘š(including
the newly added category NULL ), denoted as ğ‘(ğ‘ ğ‘š,ğ‘’ğ‘š). Then we
use the evaluation function ğ¹(ğ‘,ğ‘ ğ‘–ğ‘š)to compute the score for the
ğ‘’ğ‘š, and add the score to ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’[ğ‘(ğ‘ ğ‘š,ğ‘’ğ‘š)](Line 15). Finally, we
select the NER category with the maximum score in the dict as
the relabeled NER category ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› (Line 18) and its score as
ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (Line 19).There is an example using TIN to relabel the suspicious entity
"BBC News " -LOCin the sentence " BBC News is an operational
business division of the BBC. ", as shown in Fig. 5. TIN first masks each
word/subword in the " BBC News " in turn to generate mutant entities
"CNN News ", "Fox News ", "BBC Newspaper ", and " BBC company "
through the BERT. Then TIN filters out the mutant entity " BBC
company " and feeds the mutant sentences into the NER system to
obtain the NER predictions " CNN News " -ORG, "Fox News " -ORG,
and " BBC Newspaper " -MISC . The evaluation function of TIN will
calculate the score for each filtered mutant entity and aggregate the
score to the corresponding NER category: ORG(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ =0.7),MISC
(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ =0.3). After that, TIN selects the NER category ORGthat has
the maximum score as the relabeled result of the " BBC News ".
4) Repairing of range conflict: As the relabeling process is
independent for each suspicious entity, there are potential range
conflicts among the relabeled named entities, which will be solved
by TIN using ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ . In practice, we enumerate all pair (ğ‘’ğ‘,ğ‘’ğ‘)of
relabeled named entity within ğ‘ and check whether there is a range
conflict between ğ‘’ğ‘andğ‘’ğ‘. If there is a range conflict among two
relabeled named entities, TIN will deprecate the relabeled named
entity with lower ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ betweenğ‘’ğ‘andğ‘’ğ‘.
For example, as shown in Fig 6, there is a range conflict between
two suspicious entities " Cricket South Africa " and " South Africa " in
the sentence. After the relabeling process, the ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ of the former
is0.911whereas the latter is 0.503, by which TIN will deprecate the
relabeled named entity " South Africa ".
Finally, we obtain a fixed NER prediction ğ‘…(ğ‘ )without range
conflicts. If the fixed NER prediction differs from the original ğ‘(ğ‘ ),
the NER system is repaired using ğ‘…(ğ‘ ).
Algorithm 1: Black-box relabeling the suspicious entity
Data:ğ‘ : a sentence input; ğ‘’ğ‘ : the suspicious entity to relabel;
Result:ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› : the relabeled NER type of the suspicious
entity;ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ : the score of the relabeled NER type.
1Function Relabel(ğ‘ ,ğ‘’ğ‘ ):
2ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ =Dict()
3â„ğ‘ =BERT _Embedding(ğ‘ ,ğ‘’ğ‘ )
4 foreachğ‘¤ğ‘âˆˆğ‘’ğ‘ do
5ğ‘ ğ‘šğ‘ğ‘ ğ‘˜ =Replace(ğ‘ ,ğ‘¤ğ‘,[MASK])
6ğ¶ğ‘¤=BERT _MLM(ğ‘ ğ‘šğ‘ğ‘ ğ‘˜)
7 foreach(ğ‘¤ğ‘,ğ‘)âˆˆğ¶ğ‘¤do
8 ifFormat _Incosistent(ğ‘¤ğ‘,ğ‘¤ğ‘)then continue ;
9 ifğ‘<ğ‘ƒğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ then continue ;
10 ğ‘ ğ‘š=Replace(ğ‘ ,ğ‘¤ğ‘,ğ‘¤ğ‘)
11 ğ‘’ğ‘š=Replace(ğ‘’ğ‘ ,ğ‘¤ğ‘,ğ‘¤ğ‘)
12 â„ğ‘š=BERT _Embedding(ğ‘ ğ‘š,ğ‘’ğ‘š)
13 ğ‘ ğ‘–ğ‘š=CosSim(â„ğ‘ ,â„ğ‘š)
14 ifğ‘ ğ‘–ğ‘š<ğ‘†ğ‘‡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ then continue ;
15 ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’[ğ‘(ğ‘ ğ‘š,ğ‘’ğ‘š)]+=ğ¹(ğ‘,ğ‘ ğ‘–ğ‘š)
16 end
17 end
18ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =arg maxğ‘˜ğ‘’ğ‘¦âˆˆNER_Category(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’[ğ‘˜ğ‘’ğ‘¦])
19ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ =max(ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’.ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ğ‘ )
20 returnğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›,ğ‘ _ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’
21End FunctionESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, and Pinjia He
Table 3: Precision of TIN on BBC News dataset
NER systems Token-level Replacement Phrase-level Replacement Structural Transformation Random Entity Shuffle TIN Overall
Flair-CoNLL 86.0% (43/50) 90.0% (45/50) 91.7% (33/36) 80.0% (40/50) 86.6% (161/186)
Flair-Ontonotes 82.0% (41/50) 90.0% (45/50) 90.0% (45/50) 78.0% (39/50) 85.0% (170/200)
Azure NER 90.0% (45/50) 92.0% (46/50) 98.0% (49/50) 92.0% (46/50) 93.0% (186/200)
AWS NER 96.0% (48/50) 96.0% (48/50) 93.8% (45/48) 88.0% (44/50) 93.4% (185/198)
Table 4: Precision comparison between TIN and baseline on CoNLL03 (tested on Flair-CoNLL)
TIN SeqAttack
Methods Token-level Replacement Phrase-level Replacement Structural Transformation Random Entity Shuffle TIN Overall BERT-Attack Clare
Precision 96.0% (48/50) 96.0% (48/50) 92.9% (26/28) 98.0% (49/50) 96.1% (171/178) 24.0% (12/50) 28.0% (14/50)
4 EVALUATION
â€¢RQ1: How accurate is TIN at finding erroneous issues?
â€¢RQ2: What categories of named entity recognition errors
can TIN find?
â€¢RQ3: How effective is TIN at repairing NER errors?
4.1 Experiment Setup
To verify the results of TIN, we manually inspect the results both
for the automated testing and the automated repairing part of TIN.
For the automated testing part of TIN, we collectively decide: (1)
whether the issue contains NER errors; and (2) if yes, what cate-
gory of NER errors it contains. For the automated repairing part of
TIN, we collectively decide whether the suspicious entity has been
repaired correctly. It is noticeable that manual inspection is only
used to verify the effectiveness of TIN, which is totally automated.
Specifically, we evaluated 784 suspicious issues for NER testing and
3,828 suspicious entities for NER repairing. All experiments are
conducted on a Linux (Ubuntu 20.04.2 LTS) workstation with 64GB
Memory and GeForce RTX 3090 GPU.
4.2 Dataset
We use two datasets in this paper, the BBC News dataset and
CoNLL03 dataset. We collect the sentences on BBC news with
multiple news categories to construct the BBC News dataset, which
contains 3,223 sentences without NER labels. We use this dataset to
evaluate TINâ€™s performance to test and repair NER systems on gen-
eral text data. The CoNLL03 dataset is a widely used NER dataset
that contains the ground truth label. As the baselines rely on ground
truth label to generate test cases, we compare TIN and the baselines
on the test dataset of CoNLL03.
4.3 Comparison
TIN is the first black-box testing and repairing approach for general
NER systems, and there is a dearth of baselines for testing ap-
proaches. Alternatively, we choose two popular adversarial attack
approaches as our baseline methods Bert-Attack [ 22] and Clare [ 20],
and compare TIN with the baseline methods on the CoNLL03 test
dataset [ 31]. We use TIN and the baselines to test the Bert-Base-
NER [ 12] fine-tuned on CoNLL03 dataset, and evaluate the precision
of the reported suspicious issues.4.4 Evaluation Metric
As TIN consists of automated testing and automated repairing for
NER systems, we use different evaluation metrics for these two parts.
For NER automated testing, the output is a list of suspicious issues,
each containing (1) original sentence ğ‘ and its NER predictions ğ‘(ğ‘ ),
(2) mutant sentence ğ‘ â€²and its NER predictions ğ‘(ğ‘ â€²). We define the
Precision of NER automated testing as the percentage of erroneous
issues in suspicious issues, which have NER prediction errors in
at least one sentence. We use Precision to evaluate how precise
TIN reports the suspicious issues. Explicitly, for a suspicious issue
ğ‘ğ‘¡, we setğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘¡(ğ‘ğ‘¡)toTrue ifğ‘(ğ‘ )orğ‘(ğ‘ â€²)have NER prediction
errors. Otherwise, we will set ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘¡(ğ‘ğ‘¡)toFalse . Given a list of
suspicious issues, the Precision is calculated by:
Precision =Ã
ğ‘ğ‘¡âˆˆğ‘ƒğ‘‡1{ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ(ğ‘ğ‘¡)}
|ğ‘ƒğ‘‡|, (6)
whereğ‘ƒğ‘‡is the suspicious issues returned by TIN and |ğ‘ƒğ‘‡|is the
number of suspicious issues from automated NER testing.
For automated NER repairing, there are four possible outcomes:
ğ‘‡ğ¹,ğ¹ğ‘‡,ğ¹ğ¹,ğ‘‡ğ‘‡ . We useğ‘‡andğ¹to represent whether the NER pre-
diction is correct or incorrect, and ğ‘‡ğ¹,ğ¹ğ‘‡,ğ¹ğ¹,ğ‘‡ğ‘‡ present the tran-
sition through the repairing, e.g., ğ¹ğ‘‡means the repairing process
successfully fixes the NER systems by changing incorrect predic-
tion to correct. The number of incorrect NER predictions before
repairing is ğ‘ğ‘¢ğ‘šğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ =ğ¹ğ‘‡+ğ¹ğ¹and the number of correct NER
predictions before repairing is ğ‘ğ‘¢ğ‘šğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ =ğ‘‡ğ‘‡+ğ‘‡ğ¹. Three
evaluation metrics are used to evaluate the effectiveness of NER
repairing:
â€¢ğ¸ğ‘Ÿğ‘Ÿ2ğ¶ğ‘œğ‘Ÿ measures the probability of changing incorrect
NER predictions to correct and is calculated as ğ¸ğ‘Ÿğ‘Ÿ2ğ¶ğ‘œğ‘Ÿ=
ğ¹ğ‘‡
ğ‘ğ‘¢ğ‘šğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ.
â€¢ğ¶ğ‘œğ‘Ÿ2ğ¸ğ‘Ÿğ‘Ÿmeasures the probability of changing correct NER
predictions to incorrect and is calculated as ğ¶ğ‘œğ‘Ÿ2ğ¸ğ‘Ÿğ‘Ÿ=
ğ‘‡ğ¹
ğ‘ğ‘¢ğ‘šğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡.
â€¢ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘’ measures the ability to reduce NER errors and
is calculated as ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘’ =ğ¹ğ‘‡âˆ’ğ‘‡ğ¹
ğ‘ğ‘¢ğ‘šğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ.
4.5 RQ1: Precision of Finding Erroneous Issues
We adopt TIN to test four NER systems and report suspicious is-
sues, where the effectiveness lies in how precise the reported issues
are. The evaluation result on BBC News Dataset is shown in Ta-
ble 3, where TIN achieves a high average precision ranging fromAutomated Testing and Improvement of Named Entity Recognition Systems ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA
85.0% to 93.4% over the four NER systems under test. Among all
the transformation methods on the four NER systems under test,
TIN achieves a high precision from 78.0% to 98.0%. On BBC News
Dataset, TIN successfully reports 702 erroneous issues out of 784
suspicious issues we sample. The evaluation result on BBC News
Dataset has shown that TIN achieve a good performance on the
dataset without ground truth label. We also compare TIN with the
baselines on ConLL03, where the result is shown in Table 4 We can
observe that TIN achieves much higher precision than the baselines,
ranging from 92.9% to 98.0%. While BERT-Attack and Clare only
achieve a low precision of 24.0% and 28.0%, respectively. Therefore,
TIN has a significant advantage over the baselines for reporting
NER errors, even though it does not need the ground truth label to
generate the test cases.
RQ1 Answer : TIN achieves a high average precision on the BBC
News Dataset without ground truth label, ranging from 85.0% to
93.4% over the four NER systems under test. For the comparison
between TIN and the baselines on CoNLL03, TIN has achieved a
much higher precision than the baselines.
4.6 RQ2: Categories of the Reported NER Errors
TIN is capable of finding NER errors of diverse kinds. In our ex-
perimental results, we mainly found 4 categories of NER errors:
omission, over-labeling, incorrect category, and range error, where
the categories are concluded during our manual inspection. We will
show examples of the reported NER errors with respective to each
category of NER error in the following.
Omission An omission error indicates that the NER systems fail to
recognize the named entity in a sentence. For example, in the first
sentence of Table 5, Flair-CoNLL fails to detect the named entity
"ESA".
Over-labeling An over-labeling error indicates that the NER sys-
tems label the words that do not belong to any NER category. For
example, in the second sentence of Table 5, Flair-Ontonotes mis-
takenly label the " halfway " as the NER category CARDINAL .
Incorrect category An incorrect category error indicates that the
NER systems incorrectly predict the NER category of the named
entity. For example, in the third sentence of Table 5, Azure NER
predicts " Mekelle " asPERSON , which is indeed a LOCATION .
Range error A range error indicates the NER systems only label
part of the named entity or involve unnecessary elements besides
the named entity. For example, in the fourth sentence of Table 5,
AWS NER erroneously predicts the two words " Project " and " Atra-
tum" of the named entity " Project Atratum " (a specific project name
which belongs to OTHER in AWS NER categories) as OTHER and
TITLE , which overlooks the whole meaning of the named entity
and leads to the range error.
We calculate the distribution of the four NER categories on a
random sample of 468 NER errors found by TIN, where omission,
over-labeling, incorrect category, and range error, each accounts
for 16.9% (79/468), 19.6% (92/468), 34.2% (160/468), 29.3% (137/468).RQ2 Answer : TIN can successfully find four categories of NER
errors, including omission, over-labeling, incorrect category, and
range error, which occupies 16.9% 19.6%, 34.2% and 29.3% on an
evaluation of 468 NER errors.
4.7 RQ3: Repairing Performance of TIN
We evaluate a total of 3,828 suspicious entities for assessing TINâ€™s
ability to repair the NER errors. The results of the automated re-
pairing are shown in Table 6, where we can observe that a big ratio
of the suspicious entities contains NER errors, from 47.8% to 51.6%
among the four NER systems under test. We have found 356, 549,
498, and 474 NER errors for Flair-CoNLL, Flair-Ontonotes, AWS
NER, and Azure NER, respectively. By utilizing TIN to repair these
NER errors, 192/356, 264/549, 275/498, and 325/474 of the NER er-
rors have been repaired. The ğ¸ğ‘Ÿğ‘Ÿ2ğ¶ğ‘œğ‘Ÿin Table 6 is used to show
TINâ€™s ability to repair the NER errors, where a high ratio from 48.1%
to 68.6% of the NER errors have been repaired to be correct NER
predictions among the four NER systems under test. However, TIN
may sometimes turn the correct NER prediction into an incorrect
prediction. The ğ¶ğ‘œğ‘Ÿ2ğ¸ğ‘Ÿğ‘Ÿin Table 6 is used to assess the extent
where TIN would mislead the NER systems. We can observe that
only 12.1% to 19.5% of the correct NER predictions have been misled
to the incorrect NER predictions, which is much lower than the
ratio that the NER errors are repaired. ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘’ considers both
the errors being repaired and the correct NER predictions being
misled. We can observe that there is a great ratio of error reduction
after the automated repairing, from 26.8% to 50.6%. Therefore, we
can see that the automated repairing effectively decreases the NER
errors of the four NER systems under test.
As shown in Table 5, TIN can repair four categories of NER errors
it finds:
Repairing of Omission Error: In the first sentence of Table 5,
TIN locates the suspicious entity " ESA" which is an abbreviation of
the organization name " Environmental Services Association ". In the
original prediction, the NER system misses the NER prediction of
"ESA", where there is an error of omission. TIN successfully predicts
the correct NER category ORGof "ESA" in the fixed prediction.
Repairing of Over-labeling Error: In the second sentence of
Table 5, TIN locates the suspicious entity " halfway ", which does
not belong to any NER category under the rule of Flair-Ontonotes.
In the original prediction, the NER system recognizes the NER
prediction of " halfway " asCARDINAL , where there is an error of
over-labeling. TIN successfully deprecates the NER prediction of
"halfway " in the fixesd prediction.
Repairing of Incorrect Category: In the third sentence of Table 5,
TIN locates the suspicious entity " Mekelle ", which is a special zone
and capital of the Tigray Region of Ethiopia. In the original predic-
tion, the NER system recognizes the NER prediction of " Mekelle " as
incorrect NER category PERSON , where there is an error of incor-
rect category. TIN successfully predicts the correct NER category
LOCATION of "Mekelle " in the fixed prediction.
Repairing of Range Error: In the fourth sentence of Table 5,
TIN locates the suspicious entities " Project ", "Stratum " and " Project
Stratum ", where " Project Stratum " is a broadband infrastructure
project to extend access to superfast broadband services acrossESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, and Pinjia He
Table 5: Example of four categories of NER errors and repairing of them by TIN
Sentence Suspicious Entities Original Prediction Fixed Prediction
Ben Johnson , from the Environmental Services Association (ESA),
told BBC News more and more people were putting devices
containing these batteries in with household rubbish or
mixing them with other recycling. ( Flair-CoNLL )["ESA"]["Ben Johnson", PER]
["Environmental Services Association", ORG]
["BBC News", ORG]
Error Category: Omission["Ben Johnson", PER]
["Environmental Services Association", ORG]
["BBC News", ORG]
["ESA", ORG]
The halfway point affords us an opportunity to step back and
then look at what our margins are and where we could be a
little smarter to buy down risk and understand the spacecraftâ€™s
performance for crewed flight on the very next mission. ( Flair-Ontonotes )["halfway"]["halfway", CARDINAL ]
Error Category: Over-labeling["halfway", NULL ]
They say the only positive thing the federal authorities have done
is to return electricity to Mekelle. ( Azure NER )["Mekelle"]["authorities", PERSONTYPE ]
["Mekelle", PERSON ]
Error Category: Incorrect Category["authorities", PERSONTYPE ]
["Mekelle", LOCATION ]
Fibrus is delivering a similar scheme in Northern Ireland known as
Project Stratum. ( AWS NER )["Project"]
["Stratum"]
["Project Stratum"]["Fibrus", ORGANIZATION ]
["Northern Ireland", LOCATION ]
["Project", OTHER ]
["Stratum", TITLE ]
Error Category: Range Error["Fibrus", ORGANIZATION ]
["Northern Ireland", LOCATION ]
["Project Stratum", OTHER ]
Table 6: Improvement based on manual inspection
NER Systems ğ‘‡ğ‘‡ ğ‘‡ğ¹ ğ¹ğ‘‡ ğ¹ğ¹ ğ‘ğ‘¢ğ‘šğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ ğ‘ğ‘¢ğ‘šğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ğ¸ğ‘Ÿğ‘Ÿ2ğ¶ğ‘œğ‘Ÿ ğ¶ğ‘œğ‘Ÿ 2ğ¸ğ‘Ÿğ‘Ÿ ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘’
Flair-ConNLL 286 (41.4%) 48 (7.0%) 192 (27.8%) 164 (23.8%) 356 (51.6%) 334 (48.4%) 53.9% 14.4% 40.4%
Flair-Ontonotes 483 (42.7%) 117 (10.3%) 264 (23.3%) 285 (25.2%) 549 (47.8%) 600 (52.2%) 48.1% 19.5% 26.8%
AWS NER 456 (44.8%) 63 (6.2%) 275 (27.0%) 223 (21.9%) 498 (49.0%) 519 (51.0%) 55.2% 12.1% 42.6%
Azure NER 413 (42.5%) 85 (8.7%) 325 (33.4%) 149 (15.3%) 474 (48.8%) 498 (51.2%) 68.6% 17.1% 50.6%
Northern Ireland. In the original prediction, the two elements of
"Project Stratum ", i.e., " Project " and " Stratum ", are recognized as
OTHER andTITLE , where there is an error of range. TIN successfully
predicts the correct NER category OTHER of "Project Stratum " and
deprecates the NER predictions of " Project " and Stratum in the fixed
prediction.
RQ3 Answer : The evaluation results show the great ability of
TIN to repair the NER systems, where ğ¸ğ‘Ÿğ‘Ÿ2ğ¶ğ‘œğ‘Ÿis much higher
thanğ¶ğ‘œğ‘Ÿ2ğ¸ğ‘Ÿğ‘Ÿ, and there is a high ratio of ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘’ from
26.8% to 50.6% on the four NER systems under test. TIN can
repair four categories of NER errors it finds.
5 DISCUSSION
5.1 False Positives
In the manual inspection of the experimental results for TINâ€™s
testing part, false positives refer to the suspicious issues that do
not have NER errors. In addition, if only the mutant sentence in a
suspicious issue contains the NER errors while it is "unnatural", i.e.,
grammatically incorrect or difficult to understand, we also regard
the suspicious issue as false positive. This is because NER errors
detected on the "unnatural" mutant sentence contribute little to
improving real-world NER systems. In practice, we use a semantic
filter and a syntactic filter to reduce the amounts of "unnatural"
mutant sentences.
It is worth noting that the accuracy of NER systems does not
impact the false positives of TINâ€™s testing part, even though we use
the NER prediction of the original sentence from NER systems to
generate mutant sentences. Specifically, if the original sentenceâ€™s
NER prediction is incorrect, it may lead to unexpected sentencetransformation, e.g., replacing named entity in similar sentence
generation. However, when TIN reports suspicious issues in this
case, it would not result in false positives since the suspicious issues
contain at least an original NER prediction error.
In the manual inspection of the experimental results for TINâ€™s
repairing part, false positives represent the case that TIN turns
the correct NER prediction to incorrect NER prediction, which is
pre-defined as ğ‘‡ğ¹in Section 4.4. However, ğ‘‡ğ¹only ranges from
6.2% to 10.3% on the repairing results, which is much lower com-
pared with the true positive indicator ğ¹ğ‘‡. Despite the presence of
false positives, TIN can improve the performance of NER systems,
reducing errors from 26.8% to 50.6%.
5.2 Threat to Validity
The main threats to external validity lie in the selection of the NER
systems and the dataset for testing. There are over hundreds of NER
systems designed for multiple purposes and mountains of text data
on the Internet, thus we can not evaluate TIN on all of them. To en-
rich the diversity of the test data, we collect the BBC News Dataset
containing multiple news categories. In addition, we evaluate TIN
on two typical SOTA NER systems in Flair [ 2], and two famous
commercial NER APIs to verify its generalization ability. Threats to
internal validity are factored into our experimental methodology,
and they may affect our results. Even though TIN automatically
tests and repairs the NER systems, we need to manually inspect
the results to assess the performance of TIN, which is potentially
error-prone. To minimize this threat, two people performed man-
ual inspection separately and collectively decided whether TIN
succeeded in reporting erroneous issues and repairing NER errors.Automated Testing and Improvement of Named Entity Recognition Systems ESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA
6 RELATED WORK
6.1 Robustness of NLP Systems
Deep neural networks have boosted the performance of multiple
NLP tasks, including sentiment analysis [ 8,17,21], code analy-
sis [7,16,29], reading comprehension [ 10,11], and machine trans-
lation [ 13â€“15,33,34]. However, there are still many bugs during
the usage of the SOTA NLP systems. Many researchers researched
on the robustness of NLP systems, which unveiled bugs proposed
by the neural networks used for various NLP systems [ 8,9,17,18,
21,27,28,30]. For example, Jia et al. [ 18] proposed an adversarial
evaluation scheme for the Stanford Question Answering Dataset
(SQuAD), which found seventeen SOTA modelsâ€™ performance is
primarily undermined by the adversarial sentence inserted into the
paragraph. Similar to these works, TIN focuses on the validation
and improvement of the robutsness for NER systems. Although
some of TINâ€™s mutation operators have already been utilized in
existing NLP testing approaches [ 14,33] (e.g., word replacement),
these established NLP testing tools cannot be seamlessly adapted
for testing NER systems. This is due to the need to avoid substi-
tuting named entities with non-named entities, and the absence
of named entity labels in real-world text. However, TIN leverages
the predictions of NER systems and performs mutations accord-
ingly, eliminating the dependence on named entity labels for test
case generation. TIN approach effectively addresses the challenges
posed by traditional methods. Furthermore, the testing framework
employed by TIN can be adopted to test other NLP systems, offering
the advantage of finely controlling the components of the generated
test cases.
6.2 Automated Improvement of NLP Systems
Beyond the attack and test, improving the performance of the NLP
systems is the ultimate goal. Some researchers [ 13â€“15] relabeled
the reported bugs of the NLP systems and fine-tuned the model
on the relabeled dataset to fix the bugs. Though effective, this
method demands a significant amount of human resources. Other
researchers [ 33,34] developed automated repairing techniques to
fix the machine translation bugs. For example, CAT [ 34] and Tran-
sRepair [ 33] are word replacement methods that provide automatic
fixing of revealed bugs without model retraining. Inspired by CAT
and TransRepair, we design a repairing system to fix the NER errors
found by TIN, which is the first black-box repairing algorithm to
repair the general NER systems.
6.3 Robustness of NER Systems
Although NER systems have gained significant progress and bene-
fited from deep neural networks, it is not a solved task yet. Some
researchers use adversarial methods [ 24,32] to attack the NER
systems, which undermines the performance of the NER systems
under the perturbed datasets. The adversarial attack method needs
to access the parameters of the NER models, thus it can not be con-
veniently adopted to test the commercial NER APIs. Xu et al. [ 37]
adopted two specialized metamorphic relations to test an industrial
NER system called Litigant. This approach can only be used to test
Litigant because it relies on the characteristics (e.g., plaintiff and
defendant roles in two companies) and application contexts (e.g.,the transformation between parent and sub-company names via
extension) of Litigant. Thus, we did not include it in our experi-
ments. We proposed TIN, the first black-box approach for testing
and repairing general NER systems, which can be easily adapted to
test various NER systems with different NER categories.
6.4 Metamorphic Testing
Metamorphic testing is a technique for testing software systems
where the input and output of a test case are related in some way,
such that if the input is modified in a specific way, the output should
also be modified in a specific way. As an effective approach to
address the test case generation problems and test oracle problems,
metamorphic testing is widely adopted to test a variety of systems,
including: (1) traditional software, such as compilers [ 19,23], and
database systems [ 25], and (2) AI-based systems, such as machine
translation [ 13,14,33,34] and image captioning systems [ 38]. TIN
is the first metamorphic testing approach for validating general
NER systems with different NER categories.
7 CONCLUSION
This paper presents TIN, the first approach that automatically tests
and improves the general NER systems, which is black-box and
widely applicable to various NER systems. We have proposed three
transformation schemes to generate test cases and two metamor-
phic relations to test the NER systems, which achieve a high average
precision from 85.0% to 93.4% for reporting erroneous issues over
the four NER systems under test. TIN have successfully found a
diversity of NER errors among the erroneous issues, including
omission, mislabeling, incorrect category, and range error. The au-
tomated repairing of TIN can fix all four categories of NER errors
and achieve a high rate of ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘…ğ‘’ğ‘‘ğ‘¢ğ‘ğ‘’ from 26.8% to 50.6% on the
four NER systems under test.
8 DATA AVAILABILITY
Codes and data of TIN can be found at [6].
ACKNOWLEDGMENTS
We thanks the anonymous ESEC/FSE reviewers and Xinwen Zhang
for their valuable feedback on the earlier draft of this paper. This
paper was supported by the National Natural Science Foundation
of China (No. 62102340) and Shenzhen Science and Technology
Program.
REFERENCES
[1] 2020. Stanford CoreNLP . https://stanfordnlp.github.io/CoreNLP
[2]2022. Flair: A very simple framework for state-of-the-art NLP . https://github.com/
flairNLP/flair
[3] 2022. NLTK: . https://www.nltk.org/
[4]2022. Wikipedia:Size of Wikipedia . https://en.wikipedia.org/wiki/Wikipedia:
Size_of_Wikipedia#cite_note-2
[5]2023. Context-free grammar . https://en.wikipedia.org/wiki/Context-free_
grammar
[6]2023. TestNER: A toolkit for testing and improving named entity recognition .
https://github.com/RobustNLP/TestNER
[7]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing Distributed Representations of Code. Proceedings of the ACM on Programming
Languages POPL (2019).
[8]Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava,
and Kai-Wei Chang. 2018. Generating Natural Language Adversarial Examples. InESEC/FSE 2023, 11 - 17 November, 2023, San Francisco, USA Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, and Pinjia He
Proc. of the 2018 Conference on Empirical Methods in Natural Language Processing
(EMNLP) .
[9]Isaac Caswell, Onkur Sen, and Allen Nie. 2015. Exploring adversarial learning
on neural network models for text classification. (2015).
[10] Danqi Chen. 2018. Neural Reading Comprehension and Beyond . Ph. D. Dissertation.
Stanford University.
[11] Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A Thorough
Examination of the CNN/Daily Mail Reading Comprehension Task. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (ACL) .
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[13] Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su. 2020. Machine trans-
lation testing via pathological invariance. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering . 863â€“875.
[14] Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-invariant testing for
machine translation. In 2020 IEEE/ACM 42nd International Conference on Software
Engineering (ICSE) . IEEE, 961â€“973.
[15] Pinjia He, Clara Meister, and Zhendong Su. 2021. Testing machine translation
via referential transparency. In 2021 IEEE/ACM 43rd International Conference on
Software Engineering (ICSE) . IEEE, 410â€“422.
[16] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (ACL) .
[17] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversar-
ial Example Generation with Syntactically Controlled Paraphrase Networks. In
Proc. of the 16th Annual Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies (NAACL-HLT) .
[18] Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading
Comprehension Systems. In Proc. of the 2017 Conference on Empirical Methods in
Natural Language Processing (EMNLP) .
[19] Vu Le, Mehrdad Afshari, and Zhendong Su. 2014. Compiler Validation via Equiv-
alence Modulo Inputs. In ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI) .
[20] Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun,
and Bill Dolan. 2020. Contextualized perturbation for textual adversarial attack.
arXiv preprint arXiv:2009.07502 (2020).
[21] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger:
Generating Adversarial Text Against Real-world Applications. In Proc. of the 26th
Annual Network and Distributed System Security Symposium (NDSS) .
[22] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020. Bert-
attack: Adversarial attack against bert using bert. arXiv preprint arXiv:2004.09984
(2020).
[23] Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F. Donaldson.
2015. Many-Core Compiler Fuzzing. In ACM SIGPLAN Conference on Program-
ming Language Design and Implementation (PLDI) .
[24] Bill Yuchen Lin, Wenyang Gao, Jun Yan, Ryan Moreno, and Xiang Ren. 2021.
RockNER: A simple method to create adversarial examples for evaluating the
robustness of named entity recognition models. arXiv preprint arXiv:2109.05620(2021).
[25] Mikael Lindvall, Dharmalingam Ganesan, Ragnar Ãrdal, and Robert E Wiegand.
2015. Metamorphic model-based testing applied on NASA DATâ€“An experience
report. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engi-
neering , Vol. 2. IEEE, 129â€“138.
[26] Shubhanshu Mishra, Sijun He, and Luca Belli. 2020. Assessing demographic bias
in named entity recognition. arXiv preprint arXiv:2008.03415 (2020).
[27] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2017. Adversarial Train-
ing Methods for Semi-Supervised Text Classification. In Proceedings of the 5th
International Conference on Learning Representations (ICLR) .
[28] Pramod K. Mudrakarta, Ankur Taly, Mukund Sundararajan, and Kedar Dhamd-
here. 2018. Did the Model Understand the Question?. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (ACL) .
[29] Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to
Name-based Bug Detection. Proceedings of the ACM on Programming Languages
OOPSLA (2018).
[30] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically
Equivalent Adversarial Rules for Debugging NLP Models. In Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (ACL) .
[31] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recognition. arXiv preprint cs/0306050
(2003).
[32] Walter Simoncini and Gerasimos Spanakis. 2021. SeqAttack: On adversarial
attacks for named entity recognition. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing: System Demonstrations . 308â€“
318.
[33] Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
Automatic testing and improvement of machine translation. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering . 974â€“985.
[34] Zeyu Sun, Jie M Zhang, Yingfei Xiong, Mark Harman, Mike Papadakis, and Lu
Zhang. 2022. Improving machine translation systems via isotopic replacement.
InProceedings of the 2022 International Conference on Software Engineering, ICSE .
[35] Andrew Trask, Phil Michalak, and John Liu. 2015. sense2vec-a fast and accurate
method for word sense disambiguation in neural word embeddings. arXiv preprint
arXiv:1511.06388 (2015).
[36] Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Prad-
han, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, et al .2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium,
Philadelphia, PA 23 (2013).
[37] Yezi Xu, Zhi Quan Zhou, Xiaoxia Zhang, Jing Wang, and Mingyue Jiang. 2022.
Metamorphic testing of named entity recognition systems: A case study. IET
Software (2022).
[38] Boxi Yu, Zhiqing Zhong, Xinran Qin, Jiayi Yao, Yuancheng Wang, and Pinjia
He. 2022. Automated testing of image captioning systems. In Proceedings of the
31st ACM SIGSOFT International Symposium on Software Testing and Analysis .
467â€“479.
[39] Xingmeng Zhao, Ali Niazi, and Anthony Rios. 2022. A Comprehensive Study
of Gender Bias in Chemical Named Entity Recognition Models. arXiv preprint
arXiv:2212.12799 (2022).