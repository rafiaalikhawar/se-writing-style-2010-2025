DeepInfer : Deep TypeInference from SmartContract Bytecode
KunsongZhao
The Hong Kong Polytechnic
University
China
kunsong.zhao@connect.polyu.hkZihaoLi
The Hong Kong Polytechnic
University
China
cszhli@comp.polyu.edu.hkJianfeng Li
Xi’anJiaotong University
China
j/f_li.xjtu@gmail.com
He Ye
KTHRoyalInstitute of Technology
Sweden
heye@kth.seXiapuLuo∗
The Hong Kong Polytechnic
University
China
csxluo@comp.polyu.edu.hkTingChen∗
Universityof Electronic Scienceand
Technology of China
China
brokendragon@uestc.edu.cn
ABSTRACT
SmartcontractsplayanincreasinglyimportantroleinEthereum
platform. It provides various functions implementing numerous
services,whosebytecoderunsonEthereumVirtualMachine.To
use services by invoking corresponding functions, the callers need
to know the function signatures. Moreover, such signatures pro-
videcrucial informationformany downstream applications, e.g.,
identifyingsmartcontracts,fuzzing,detectingvulnerabilities,etc.
However, it is challenging to infer function signatures from the
bytecode due to a lack of type information. Existing work solv-
ing this problem depended heavily on limited databases or hard-
coded heuristic patterns. However, these approaches are hard to be
adaptedtosemanticdiﬀerencesindistinctlanguagesandvarious
compilerversionswhendevelopingsmartcontracts.Inthispaper,
weproposeanovelframework DeepInfer that/f_irstleveragesdeep
learning techniques to automatically infer function signatures and
returns.Thenoveltiesof DeepInfer are:1)DeepInfer liftsthe byte-
code into the Intermediate Representation (IR) to preserve code
semantics;2) DeepInfer extractsthetype-relatedknowledge(e.g.,
critical data /f_lows, constant values, and control /f_low graphs) from
theIRtorecoverfunctionsignaturesandreturns.Weconductex-
periments on Solidity and Vyper smart contracts and the results
show that DeepInfer performs faster and more accurate than exist-
ing tools, while being immune to changes in diﬀerent languages
andvariouscompilerversions.
CCS CONCEPTS
•Securityandprivacy →Software reverseengineering .
KEYWORDS
SmartContract, Type Inference,DeepLearning
∗Corresponding authors.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci/f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12...$15.00
https://doi.org/10.1145/3611643.3616343ACMReference Format:
Kunsong Zhao, Zihao Li, Jianfeng Li, He Ye, Xiapu Luo, and Ting Chen.
2023.DeepInfer : Deep Type Inference from Smart Contract Bytecode. In
Proceedings of the 31st ACM Joint European Software Engineering Conference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’23),
December 3–9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA,
13pages.https://doi.org/10.1145/3611643.3616343
1 INTRODUCTION
Cryptocurrencieshaveshownaprevalenttrendinbothindustry
andacademiainrecentyears.WiththeemergenceofEthereum[ 45],
oneofthelargestdecentralizedplatform,programmablecryptocur-
rency services enter a new era [ 11,12,29,34,62]. Smart contracts,
as the key component of Ethereum, enable developers and users
releasecryptocurrencies,deployapplications,andutilizeservices
ontheEthereumblockchainwithouttheinterventionofthetrusted
third parties [ 2,33,59,70]. A smart contract is implemented by
high-levellanguages (e.g., Solidity [ 52] and Vyper [ 58]), thencom-
piled into the bytecode executed on Ethereum Virtual Machine
(EVM),and/f_inallydeployedonEthereum.Thefunctionsofasmart
contractwillberegisteredastheApplicationBinaryInterface(ABI),
making others easily invoke and implement their functionalities
[9,53].
Table 1: An overview of existing studies for recovering func-
tion signaturesandreturnsinsmart contracts.
ApproachScalability Accuracy
TechniquesLanguages Compilers Signature Return
EBD[43] - - FSD
Eveem[17] FSD+SA
Gigahorse[ 21] SA
SigRec [9] SA
DeepInfer DL
FullPartial Nosupport
FSD:Ethereumfunction signaturedatabase, SA: staticanalysis, DL:deep learning
TheABIconsistsoffunctionsignaturesandreturns.Whenin-
voking a function, users need to know the signatures because it
de/f_ines the calling rules [ 53]. Function signatures comprise a func-
tion id and a list of parameter types in which function id is derived
from the /f_irst 4 bytes of the Keccak-256 hash result of a function
name and the corresponding list of parameter types in source code
745
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KunsongZhao,ZihaoLi, JianfengLi, He Ye,Xiapu Luo, andTing Chen
[53]. Moreover, the function returns specify the format of return
values of the function [ 53]. Identifying function signatures and re-
turnsisacrucialsteptoanalyzethebehaviorofafunctioninsmart
contracts because one needs to /f_irst know how a function is called
andwhat isreturnedbefore the evaluation. For example,previous
studies adopted function signatures to recognize diﬀerent types of
smartcontracts[ 5,13,16,19,24] andutilized functionarguments
togeneratemoremutanttestcasestofacilitatethefuzzingtoolsfor
detecting more vulnerabilities [ 9,23,27]. Moreover, checking re-
turnsofafunctioncanavoidvulnerabilities,suchastheunchecked
callreturn valueweakness [ 54].
Problem: function signatures and returns are black boxes
for users. This is because nearly 99% of the deployed contracts
are closed-source [ 18,26]. The arguments of function signatures
and returns arerepresented asthe256-bit wordswithoutthe type
informationand debuginformation inthebytecode,whichmakes
it hard to be recovered [ 1]. As shown in Table 1, some studies
contributetorecoveringfunctionsignaturesinsmartcontracts.For
example, EVM Bytecode Decompiler (EBD) [ 43] searches function
signaturesfromtheirEthereumFunctionSignatureDatabase(FSD).
Eveem[17]andGigahorse[ 21]relyonthehard-codedheuristicsto
restorefunctionsignaturesinwhichEveemalsoincorporatesthe
knowledge from the FSD. Chen et al. [ 9] proposed a static analysis-
based tool SigRec that manually designed 31 heuristic patterns
accordingtotheaccessrulesfordiﬀerentparametertypesinthe
EVMandemployedthesymbolicexecutiontechniquetorecover
function signatures from the bytecode ofasmart contract.
Unfortunately,theseapproachesstillsuﬀerfromthefollowing
limitations.
Limitation 1 (L1) : The existing methods for the function sig-
nature recoveryheavilydepend oneither anincomplete database
involvingafractionoffunctionsinthewild[ 9]orrestrictedmatch-
ing patterns designed by human experts. The limited database is
hard to cover all the functions on Ethereum, where as the /f_ixed
matching patterns will be invalid when the smart contracts are
developedbynewprogramminglanguageswiththeevolutionof
the Ethereumecosystems.
Limitation2(L2) :Despitetheexistingtooldesigningasetof
arguments access rules that has shown the ability to recovering
types of arguments, its accuracy is still less than satisfactory, espe-
ciallywhentheaccesspatternsencounterevenaslightchangedue
to the compilerversionupgrades.
Limitation 3 (L3) : All the existing studies merely focus on
recovering function signatures but the inference of the returns of a
functionis ignored. Since theaim of smart contractsis to execute
transactionsonEthereumplatform,boththesignaturesandreturns
of a contract function are indispensable components for analyzing
thefunctionalityandcheckingwhethersomedesiredoperations
are performedcorrectly[ 39,42].
With deep learning techniques achieving promising results over
traditionalpattern-basedmethodsinmanysoftwareengineering
tasks [49,61,68], one can leverage these data-driven techniques to
learnimplicitknowledgetoinferfunctionsignaturesandreturns.
However, it is challenging to design deep learning-based inference
models for this purpose.
Challenge 1 (C1) : The inference model needs to automatically
learn how diﬀerent types of arguments are operated in the EVMbytecoderatherthanponderouslydependsonthefunctiondatabase
or manually extracts a limited set of access patterns focusing on
speci/f_ic languages. This requires that the model actually grasps
the diﬀerences in diﬀerent types of arguments which are language-
independent.
Challenge 2 (C2) : As the compiler version upgrades, the pat-
ternstoaccesstheargumentswillbechanged.Thisrequiresthat
themodelisscalable,i.e.,itneedstobefreefromthecharacteristics
ofvariouscompilerversions.
Challenge3(C3) :Forfunctionsignatureinference,wecanforce
the model to understand the access patterns of diﬀerent types of
argumentsand thenusesuch knowledgetopredictthesignatures
of other functions from a prepared type list. Unfortunately, it is
infeasibleforinferringsomecomplextypes(e.g.,array)becausewe
cannot determine how the number of nesting layers or dimensions
thereshouldbeandrestrictittoalimitedsetoftypesinadvance.
Thatis,itwillsuﬀerfromanout-of-vocabulary(OOV)issue[ 22,25].
This requiresthe modelto decidewhichtypes needto beinferred
from a /f_inite set of types and which types need to dynamically
determinethe nestingdepthordimensionsduringthe inference.
Challenge 4 (C4) : Recovering returns is more diﬃcult than
inferring signatures, because the return values are stored in the
memoryandreturnedattheendoffunctionexecution[ 42,52].This
requires the model to be able to understand the semantic of the
whole function rather than a speci/f_ic part. Despite there also exist
some studies for function signature inference in other scenarios
[3,41,47],noneofthemsupportstypeinferencefromthebytecode
ofsmart contracts because they target the sourcecode.
In this paper, we present DeepInfer , a novel deep learning-based
frameworktoautomaticallyrecoverfunctionsignaturesandreturns
fromtheEVMbytecodewithoutanyhumanintervention.Tomake
the model deal with various languages, DeepInfer /f_irst lifts the
bytecodecompiledfromdiﬀerentlanguagesintotheIRinwhichthe
language-speci/f_icandcompiler-speci/f_icoperationsarestripped( C1,
C2).Then,itconductsade/f_inition/useanalysistoextractcritical
information(e.g.,data/f_lowsandconstantvalues)thatarehighly
relevantfortheinferenceofsignaturesfromtheIR(§ 3.3).Tomake
themodelhavethepotentialtorecovervarioustypesofarguments,
we design a two-stage inference framework in which the basic
and complex types are handled individually (§ 2.2). Speci/f_ically,
DeepInfer recognizes the basic and complex types according to the
knowledgeextractedandlearnedfromtheIR.Forthebasictypes,
DeepInfer recoverstheactualtypebyselectingtheonewhohasthe
maximum probability among thelist ofprede/f_ined types collected
from the oﬃcial documentations because of the limited number of
types(§3.4).Sincethecomplextypescannotberestrictedina/f_inite
set due to the above-mentioned OOV issue, DeepInfer employs a
sequencegenerationmodeltodynamicallygeneratethepossible
structure of such types according to the knowledge learnt from
thecriticalinformation(§ 3.5)(C3).Torecoverreturns, DeepInfer
constructs the control /f_low graph (CFG) that captures the function
semanticbymeansofthestructuredgraphrepresentationfromthe
IR andemploysthegraphneural networktoexcavatethe implicit
semanticknowledge,aimingatpromotingthemodeltounderstand
the functionality(§ 3.6) (C4).
Weconductexperimentsonopen-sourcesmartcontractswrit-
ten bySolidity and Vyper. We collectunique functionsfrom these
746DeepInfer : Deep Type Inference from SmartContract Bytecode ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
open-sourcesmartcontractsandusethemtoevaluatetheaccuracy
ofDeepInfer .Theexperimentalresultsshowthat,overall, DeepInfer
obtainsthetop-5accuracyof0.980and0.937forsignatureinference
inbothSolidityandVypersmartcontracts,respectively.Compared
with the baselines in Table 1,DeepInfer obtains an average accu-
racy improvement by 117.1% for recovering function signatures.
Moreover, DeepInfer achieves the top-5 accuracy of 0.968 and 0.924
for recovering function returns, respectively. Further experiments
show that DeepInfer performs more than 24 times faster than base-
line methods. Meanwhile, DeepInfer is not aﬀected by diﬀerent
languagesandvariouscompilerversions.
Insummary,thispapermakesthefollowingmajorcontributions:
•Wedevelop DeepInfer ,anovelframeworkthatextractsandlearns
functionaccess-relatedinformationfromtheliftedthree-address
code to recover function signatures. Moreover, DeepInfer is able
to understandcode semantics for recoveringfunction returns.
•We conduct comprehensive experiments on real-world smart
contractswrittenbySolidityandVyper,andevaluatetheaccuracy
ofDeepInfer . The results show that DeepInfer obtains an average
accuracy improvement by 117.1% compared to the existing tools
forrecoveringfunctionsignatures. DeepInfer alsohastheunique
potentialtorecoverfunction returnsand isimmunetochanges
indiﬀerentlanguagesandvariouscompilerversions.
•DeepInfer isthe/f_irstworkonrecoveringfunctionsignaturesand
returnsfromEVMbytecodebasedondeeplearninginference,to
thebestofourknowledge.Onthecontrarytothestate-of-the-art
work[9],ourprocessisdoneinafullyautomaticmannerwithout
any human intervention.
2 BACKGROUND
2.1 Ethereum & SmartContracts
Ethereum[ 45]isasecond-generationblockchain-basedplatform
thatprovidesmore/f_lexibledistributedcomputingabilitiesbyincor-
poratingsmartcontracts[ 28].Therearetwotypesofaccountsin
Ethereum blockchain: External Owned Account (EOA) that can be
treatedasawalletkeepingassets(e.g.,Ether)andsmartcontract
whichiscreatedbyeitherEOA orothersmart contracts [ 14].
Smart contracts are executable programs that can run on the
Ethereum blockchain, which implement various functionalities
satisfying the requirements of end-users. It can be executed in
a completely decentralized manner and does not depend on any
trusted third parties [ 36]. Smart contracts are written by high-level
programming languages (e.g., Solidity and Vyper) and compiled
intothe bytecode executingonthe EVM.
2.2 FunctionInvocation in EVM
TheEOAandCOAaccountscancallasmartcontractfunctionby
sendinganinvocationmessagethatconsistsoftheaddressofthe
smart contract where the invoked function is located and a special
/f_iledcalldatawhichcarriesinformationaboutthefunctioninvoked
and actual arguments [ 9]. Thecall data/f_ield appears as a sequence
ofbyteswhose/f_irst4bytesrefertothefunctionidandtherestis
thearguments [ 53].For example, to invoke the functionshown in
Listing1, thecall data/f_ield starts from the corresponding function
id0xea7cabdd followedbytwospeci/f_ictypesofarguments: _tokenId
(aone-dimensional dynamicarray) and owner(an address).There are two instructions read arguments from the call data
/f_ieldto EVM, including CALLDATALOAD andCALLDATACOPY .CALLDAT-
ALOAD/f_irstloadsthetopelementinthestackofEVMandusesitas
the oﬀset to locate data in the call data/f_ield. It then loads 32 bytes
data starting from the oﬀset from the call data[45]. Eventually, the
loadeddataisputintothestackofEVM.Diﬀerentfrom CALLDAT-
ALOADthatreadsa/f_ixed-sizedataintothestack, CALLDATACOPY loads
avariable-sizedatafrom calldatatothememoryofEVM[ 45].It
/f_irstloadsthreeelementsfromthetopstack,includingthememory
locationforstoringthedata,thelocationof calldataforloadingthe
data,andthelengthofdatatobeloaded.Therearemanysupported
typesinSolidityandVyperintheiroﬃcialdocumentations[ 52,58],
whichcanbetreatedasbasictypesandcomplextypesaccordingto
whetheronetypeisenumerable.Thebasictypesincludeaddress,
string,struct,bool,bytes,(u)int /u1D440(where/u1D440∈{8,16,...,256}),and
bytes/u1D441(where/u1D441∈{1,2,...,32}). The complex types contain vari-
ousarraysthatarederivedfromthebasictypesexceptforstruct,
butcanhavevariablenestingdepthandsizes,suchasone/multi-
dimensional array with compile-time /f_ixed or dynamic sizes and
thenestedarray[ 52,58].Readerscanseemoreaccessdiﬀerences
ofthesetypes inthe literature [ 9,53].
1function checkAllOwner( uint256 [] _tokenId, address owner) public
view returns (bool) {
2......
3for(uinti=0;i<_tokenId. length;i++){
4 if(owner != zombieToOwner[_tokenId[i]]){
5 return false ;
6 }
7}
8return true ;
9}
Listing 1: A function sample with two arguments and one
return whose function id is 0xea7cabdd .
2.3 Motivating Example
Listing1illustratesanexampleofafunctiondeployedinEthereum.
The function consists of two arguments and one return value in
whichthe/f_irstparameteristhedynamicarrayandthesecondoneis
theaddress.Thisfunctioncheckswhetheralltokensinthedynamic
array_tokenIdbelongtoaparticular ownerandreturnstrueifso,
false otherwise. There are diﬀerent EVM instructions to load these
two typesofparameters.For thesecondparameter,theEVM /f_irst
usesaCALLDATALOAD instructionwhoseoperandisthestartpoint
of this parameter in the call data/f_ield. Then it loads a 32-byte data
andusesan ANDinstructionformaskingtoa/f_ixed-length(i.e.,32
bytes) [9,52]. The masked result is the actual parameter value that
canbeusedbyfollowinginstructions.Torecoverthetypeofthis
parameter,wecanmakethemodellearntheoperationinstructions
involvedandconstantvaluesthatareusedformasking,andthen
selectthetypewiththemaximumprobabilityfroma/f_ixedtypelist
because the basic types are limitedas mentionedabove.
However, thetypeinference forthe /f_irst parameteris diﬃcultbe-
cause there is no dimension information of the array in the bytecode
and thus is unknown during the compilation. Assume the actual
parameters of _tokenId in thecall data is [0xa, 0xb, 0xc] and the
arrangement of the data is displayed in Fig. 1. The /f_irst 4 bytes
refer to the function id and the oﬀsetrecords the start point of
this array. Moreover, numis the size of the actual elements in this
array, followed by their individual real data values (i.e., 0xa, 0xb,
and 0xc). Hence, to access an array element (e.g., 0xb), the EVM
747ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KunsongZhao,ZihaoLi, JianfengLi, He Ye,Xiapu Luo, andTing Chen
4 bytes 32 bytes 32 bytes
id offset num 0xa 0xb 0xc
Figure1: Thedataarrangementofadynamicarrayuint256[].
uses two CALLDATALOAD instructions to load the items oﬀsetand
numto identify the start point of this array, followed by another
CALLDATALOAD instruction to obtain the actual value 0xb according
to its address. To infer such types of arguments, DeepInfer designs
agenerativedeeplearningmodelwhichhasthepotentialtoauto-
matically generate nested structures by learning critical data /f_lows
andinvolvedconstantvalues.Thisisbecausetheformsofarrays
arevarious,suchasone/multi-dimensionalstatic/dynamicarrays
and nested arrays, whose nested depths and dimensions cannot be
limitedintoa/f_ixedset.
On the other hand, recovering the type of the return value is
diﬃcult because they are stored in memory during EVM running.
Diﬀerentfromthetyperecoveryofsignaturesthatholdsexplicit
operationinstructionstoloadandaccessthefunctionarguments,
return values are stored into or loaded from the memory. This
prevents the model from collecting obvious instruction operations.
To infer the return type, we make the model to understand the
functionality of the contract function and determine what is the
type ofthe return value.
3 FRAMEWORK
3.1 Overview
Fig.2demonstratestheoverallframeworkof DeepInfer thattakesas
input the EVM bytecode of smart contracts and /f_inally outputs the
signatures and returns of each function in it. Speci/f_ically, DeepInfer
/f_irst lifts the bytecode into IR in which the complex stack opera-
tionsarestrippedbuttheusefulinstructionoperationsarereserved
in the form of readily comprehensible three-address code. Then,
DeepInfer recognizes all functions from the generated IR according
to the function boundaries (§ 3.2). After that, DeepInfer extracts
function access-related information (such as data/f_low features and
constant values) and constructs control /f_low graphs (CFG). This
isbecausethefunctionaccess-relatedinformationindicateshow
each parameter is loaded and used in the EVM for recovering func-
tion signatures whereas the CFG is related to understanding the
functionalityforrecoveringfunctionreturns(§ 3.3).Basedonthese
information, DeepInfer trains the deep learning models to recover
thebasictypesofargumentsbybuildingtheclassi/f_icationmodel
(§3.4),generatesthecomplextypesofargumentsbytrainingthe
sequence generation model (§ 3.5), and infers the function returns
byunderstanding the wholefunctionality(§ 3.6).
3.2 Lifting & FunctionRecognition
FortheinputEVMbytecodeofasmartcontract, DeepInfer /f_irstuses
Gigahorse[ 21]toparsethebytecodeintotheregister-basedIR(i.e.,
the three-address code) that consists of a clause <opcode, operand 1,
operand 2, ..., operand /u1D45B, result>(/u1D45B>0), in which the resultis the
output of the instruction opcodewith the operands (e.g., operand 1).
Ifavariableappearsin result,weconsideritavariablede/f_initionoperation.Moreover, ifavariable appearsin operand,we consider
it is used. Note that each variable can be de/f_ined only once, i.e.,
it holds a unique value, but can be used multiple times [ 21]. We
use the IR rather than smart contract bytecode in the following
steps ofDeepInfer because it simpli/f_ies stack operations to clauses,
whichmakesiteasytoanalyzeandextracttheaccesspatternsof
function arguments. More speci/f_ically, DeepInfer /f_irst recognizes
the JUMP instructions to /f_ind the boundaries of basic blocks and
then conducts the context-sensitive and /f_low-sensitive analysis
to process the register-based IR at the contract level. Besides, it
speculatesthe entrance and exitof each basic block to recognize
the function boundaries and generates the register-based IR at
the function level. Each basic block is connected to one or more
precursorandsuccessorbasicblocksexceptfortheentranceand
exit.The precursorsrefer tothe ones thatmay be executed before
the execution of a basic block. The successors means the ones
that may be executed after the execution of a basic block. After
generatingtheregister-basedIRforasmartcontract, DeepInfer uses
aregularexpressiontoextractallthepublicfunctionsbymatching
the functiondeclarations.Next,we introduce howdoes DeepInfer
extract criticalfeatures from such IR for modeltraining.
3.3 FeatureExtraction
After obtaining the three-address code of each function, DeepInfer
extractsfunctionaccess-relatedfeaturesfromit.Thekeyinsights
herelieinthreeaspects.First,inordertoinferthetypesofargu-
ments,DeepInfer intensively learns how each parameter is loaded
andusedbytheinstructions.Thisisanecessarystepbecausedif-
ferent types of parameters are operated by distinct opcodes. For
example,two ISZEROinstructionswill beusedwhen theloadedpa-
rameter is a boolwhereas only the instruction BYTEcan be used
toassesseachbytedataforaparameterwiththetype byte32[9].
Thus,DeepInfer proactivelylearnssuchaccesspatternsautomati-
callyaccordingtotheinstruction/f_lows.Second,sometypes(e.g.,
int)arewithdiﬀerentbitsizeswhicharerepresentedasthemask
value de/f_ined by the constant values in bytecode. For example, the
typeuint8requires 31 bits of zeros to mask the left side of the data
whereas another type uint248simply needs 1 bit. Thus, DeepIn-
feris carefully designed to collect such constant values from the
de/f_initionofthem(§ 3.2). Third,unlike thearguments thatutilize
diﬀerent instructions to access distinct types of parameters, the
returnvaluesarestoredinthememoryandreturnedafterthefunc-
tionisexecuted,whichmeansthattherearenospeci/f_icinstruction
operations.Thus, DeepInfer needstounderstandthewholefunc-
tionalitytoinferthereturnsratherthanlearntheaccesspatterns
from the instruction/f_lows.
3.3.1 Definition/UseAnalysis. Theaimofde/f_inition/useanalysisis
totraceasetofrelationsbetweenvariablesinwhichonevariableis
usedbyothers.Asaforementioned(§ 2),aparameterisaccessedby
usingeither CALLDATALOAD orCALLDATACOPY instruction[ 9],thus,we
do not need to analyze the de/f_initions and uses of all variables. On
the contrary, we fully focus on analyzing the variables containing
the de/f_initions or uses of each of the parameters in a function.
We canachievesuchvariableanalysisbyusingthede/f_inition/use
analysis based on the parsed IR. Thanks to such IR that assigns
a unique variable name to represent the execution result of each
748DeepInfer : Deep Type Inference from SmartContract Bytecode ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
......
Begin block 0x14e
......
0x14e: v14e(...) = CONST  
..... 
Begin block 0x152  
......
0x158: v158 = CALLDATALOAD  v140 
0x159: v159(...) = CONST  
0x16e: v16e = AND  v159, v158 
......
Bytecode0x608080604052 
60043610156100 
1357600080fd5...Lifting & Function
Recognition (3.2)Feature  
Extraction (3.3)
Signature/ReturnFunction Return
Recovery (3.6)
PreprocessingFunction Signature
Recovery (3.4 & 3.5)
Recovery
Constant AttentionSoftmax Ed
c1 c2 c3 ckEc+
TypeSignature  
Inferenceoutputcls
Ed c1 c2 c3 cks1out1
<SOS>out2<EOS>
s2 s3 stout3copygeneration
Encoder Decoder......
...Input 
Probability Vocabulary 
Probability ...Signature  
Generation
Ed
HFLM+ local weightsGlobal Encoder
Local Encoder+
hijCDF Embedding
global weights
CFG EmbeddingFunction ReturnsSoftmax
Function Return RecoveryCV EmbeddingMarked as  
Array?
Control Flow Graph
Recognition 
Constant Tracing
Definition/Use Analysis
Figure 2: The overallframeworkof DeepInfer .
instruction, DeepInfer cansearchwhichvariablede/f_initionscontain
the two instructions and treat these variables as the start points,
aimingatcollectingtheparameteraccess-relatedpaths.Sincethe
clauses in the IR will use the assigned variables as the operands,
DeepInfer can recursively look for the clauses whose operands
depend on the variable de/f_initions of the two instructions. As a
result,DeepInfer canobtainasetofinstructionsequenceswhere
each sequence starts from either CALLDATALOAD orCALLDATACOPY
instruction and we call each instruction sequence the Critical Data
Flow(CDF)inthe following.
3.3.2 Constant Tracing. The goal of constant tracing is to collect a
setofconstantvaluesthatarerelevanttotypesoffunctionargu-
ments. The analysis of de/f_initions and uses can generate a set of
CDFstarting from CALLDATALOAD orCALLDATACOPY .However,only
using CDFs to build the signature inference model is inadequate
becausesometypesnotonlyusetype-relatedaccessinstructions
but depend on values to determine their sizes (e.g., uint8) or the
dimensions (e.g., address[2] ). Such values are also de/f_ined in the IR
with the keyword CONST. One intuitive solution is to collect all the
de/f_initions that contain CONST. However, not all the values de/f_ined
byCONSTare related to the parameter access operations because
some conditional jumps (e.g., JUMPI) or operations that consume
constantvalues(e.g., CALLDATALOAD )canalsoinvolvesuchvalues.Tosolvethisissue,westartwitheachCDFandcollectthevariables
thatappearintheoperands.Foreachvariable,werecursivelyback-
track to where it is de/f_ined until the initial clause whose opcodeis
the keyword CONSTis found. Thus, we collect the corresponding
operandinthisclauseastheconstantvalue(CV).Asaresult,we
canobtainasetofCVsforeachCDF.BytraversingalltheCDFs,we
cancollectalltheconstantvaluesassociatedwiththearguments
access operations.
3.3.3 Control Flow Graph Construction. The above features can
be used to determine the access patterns and the corresponding
sizesordimensionswhilerecoveringfunctionsignatures.However,
while recovering function returns, there is no related operation
manifestationintheIRbecauseallthereturnvaluesarestoredin
the memory, i.e., there is a lack of explicit instructions for identify-
ing speci/f_ic operations about types that contained in returns. This
requires/f_indingarepresentationthatnotonlycontainstheimplicit
characteristicsofeachfunctionbutcanalsobelearnedbythedeep
learning model. As an alternative, CFG abstractly represents the
possible /f_lows of all basic blocks in a function using the structured
graph representation. It can re/f_lect how each statement is executed
duringtheprogramrunning[ 44,71].Besides,thestructuredpre-
sentation of a CFG also supports the use of the learnable model
to dig for implicit features (i.e., its functionality). These bene/f_its
749ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KunsongZhao,ZihaoLi, JianfengLi, He Ye,Xiapu Luo, andTing Chen
encourage us to extract the CFG from the IR. Because the IR has
beenclearlydividedintothebasicblocks(§ 3.2),wecanuseregular
expressionmatchingtoretrievethesebasicblocksandtreateach
basicblockasthenodeofaCFG.Besides,wegeneratetheedgesofa
CFGbymatchingtheprecursorsandsuccessorsofeachbasicblock.
As a result, we can obtain the structured CFG for each function in
whichnodesrepresentthebasicblocksandedgesrefertothejump
relationships.
3.4 FunctionSignatureInference
One aim of DeepInfer is to recover function signatures by learning
how diﬀerent types of arguments are operated in the bytecode.
Therearemanytypesofargumentsusedinthehigh-levellanguages
(e.g.,SolidityandVyper)inwhichsomebasictypes(e.g.,uint,int,
string, bool, etc) can be restricted into a /f_ix-length set because they
areenumerablebutothercomplextypes(i.e.,array)cannotbecause
they can hold unlimited dimensions and arbitrary sizes [ 53,58].
Thus,wecannotsimplytreatthesignaturerecoveryasatraditional
classi/f_icationtask.Tosolvethisproblem,weproposeatwo-stage
framework that determines diﬀerent types using distinct strategies.
Speci/f_ically, we /f_irst mark types that are not enumerable with a
generalsign(i.e.,Array)andallthetypescanberestrictedintoa
limited type list. Thus, DeepInfer trains a classi/f_ication model to
determine the type of a parameter by selecting the one with the
maximal prediction probability from this type list. Then, for the
typesthataremarkedasArray, DeepInfer trainsanothergenerative
model to generate their actual types. In this subsection, we will
introduce how to construct the classi/f_ication model to learn the
parameteraccesspatternsandleavethedetailofconstructingthe
generative modelinthe nextsubsection (§ 3.5).
3.4.1 CDFEmbedding. AfterextractingasetofCDFsandCVsfrom
theIRofafunction, DeepInfer trainsamodeltolearnaccesspatterns
ofdiﬀerent typesofarguments.Forthe collectedCDFs,we retain
theopcodesformodeltrainingbecauseopcodespreservethereal
operationsduringthecodeexecution.Sincetheobtainedopcodes
arenotnumericandcannotbeusedastheinputoftyperecovery
model,DeepInfer /f_irst trains a Word2Vec model based on these
opcodes to produce the initial embedding vectors. Speci/f_ically, it
treatstheopcodesextractedfromaCDFasasentenceandemploys
the Continuous Bag of Word (CBOW) technique [ 40] that predicts
a word (i.e., opcode) using its contextual words to generate an
embedding mapping matrix W.
SincetherearemultipleCDFsineachfunctionandallofthem
together form the access patterns of arguments, inspired by pre-
vious work [ 66],DeepInfer designs the Hierarchical Flow Learn-
ing Mechanism (HFLM) to learn the access patterns among the
opcodes in these CDFs. Speci/f_ically, assume the set of opcode se-
quences extracted from the CDFs is Θ={/u1D7031,/u1D7032,...,/u1D703/u1D45A}where
/u1D703/u1D456={/u1D703/u1D4561,/u1D703/u1D4562,...,/u1D703/u1D456/u1D45B} is the/u1D456-th sequence. /u1D45Aand/u1D45Brepresent the
numberofCDFs andthelengthoftheCDF, respectively.Foreach
opcode,DeepInfer /f_irst initializes it with the embedding matrix, i.e.,
/u1D452/u1D456/u1D457=W/u1D703/u1D456/u1D457.Then,foreach /u1D703/u1D456,itusesthebi-directionalLSTM[ 73]
toincorporatethecontextualoperationsfromtwodirectionsand
produce the hiddenembedding vectors:
/uni210E/u1D456/u1D457=[→
/u1D43F/u1D446/u1D447/u1D440(/u1D452/u1D456/u1D457)|←
/u1D43F/u1D446/u1D447/u1D440(/u1D452/u1D456/u1D457)] (1)where/uni210E/u1D456/u1D457isthe hidden embeddingvector ofthe /u1D457-thopcode in /u1D703/u1D456
and[·|·]refersto the concatenate operation.
Local Encoder . After obtaining the vector of each opcode, DeepIn-
dernext generates the embedding vector of a CDF. Since there
exist instructions that merely use the variable de/f_inedby previous
instructions as the target address and the type-related instructions
will occur after such instructions, simply summing or averaging
operation over the opcodes will introduce some irrelevant noise
information.Tomakethemodelpaymoreattentiontotype-related
operations,weemploytheattentionmechanismtocalculatelocal
weights from these opcodes and aggregate them into the represen-
tation ofthe CDF:
/u1D452/u1D456=/summationdisplay.1
/u1D45E/uni210E/u1D456/u1D457[/u1D452/u1D465/u1D45D/parenleftbig/u1D453/u1D459/parenleftbig/uni210E/u1D456/u1D457/parenrightbig/parenrightbig//summationdisplay.1
/u1D45E/u1D452/u1D465/u1D45D/parenleftbig/u1D453/u1D459/parenleftbig/uni210E/u1D456/u1D457/parenrightbig/parenrightbig](2)
where/u1D452/u1D456istheembeddingvectorof /u1D456-thCDF; /u1D452/u1D465/u1D45D()referstothe
exponential function; /u1D453/u1D459represents the linear layer followed by the
ReLU activation function [ 20]. We call the above operations the
local encoder as shown in Fig. 2, because they focus on a single
CDFinafunction.
GlobalEncoder .Accordingtotheaboveoperations,wecanobtain
the embedding vector for each CDF. Then, we introduce how to
generatetheoverallembeddingvectoratfunctionlevelbecauseour
aimistorecoverthesignaturesforafunction.Therearemultiple
CDFscanbeextractedfromafunctionandtheyareaccessedinthe
sameorderastheargumentsthatappearinthefunctiondeclaration.
Thus, we treat the CDFs as a sequence and use their embedding
vectors(i.e., /u1D452/u1D456)asthemodelinput.Wealsousethebi-directional
LSTM to updatethe embedding vectors of eachCDF:
ˆ/u1D452/u1D456=[→
/u1D43F/u1D446/u1D447/u1D440(/u1D452/u1D456)|←
/u1D43F/u1D446/u1D447/u1D440(/u1D452/u1D456)] (3)
whereˆ/u1D452/u1D456is the hidden embedding vector of /u1D456-th CDF generated by
the model.
Similarly,notall CDFsplaythesameimportance inrecovering
thetypeofoneparameterbecausetheEVMwilluseoneormore
diﬀerent instructions to access distinct types of parameters. Hence,
DeepInfer usestheattentionmechanismtocalculateglobalweights
fromthe embeddingvectors ofall the CDFsandincorporate them
into an overall representation, forcing the model to concentrate
on the parts of interested CDFs while recovering diﬀerent types of
parameters:
/u1D438/u1D451=/summationdisplay.1
/u1D45Dˆ/u1D452/u1D456[/u1D452/u1D465/u1D45D/parenleftbig/u1D453/u1D454(ˆ/u1D452/u1D456)/parenrightbig//summationdisplay.1
/u1D45D/u1D452/u1D465/u1D45D/parenleftbig/u1D453/u1D454(ˆ/u1D452/u1D456)/parenrightbig] (4)
where/u1D438/u1D451refers to the overall representation vectors of CDFs; /u1D453/u1D454
representsthelinearlayerfollowedbytheReLUactivationfunction.
By using theabove equations(1) - (4), DeepInfer can output the
overall CDF embedding vector for each function. Next, we will
introduce howto dealwithconstant values.
3.4.2 CV Embedding. As for constant values, since there is no
orderrelationsbetweenthem, DeepInfer buildsalookuptable /u1D44A/u1D450
in which each constant is initialized randomly and updated during
themodeltraining.Assumeasetofconstantvaluescollectedinthe
IRofeachfunctionis Φ={/u1D7191,/u1D7192,...,/u1D719/u1D458}inwhich/u1D458isthenumber
of unique constant values. DeepInfer embeds each constant into its
initialembeddingvectorbyinquiringaboutthelookuptable,i.e., /u1D450/u1D458
=/u1D44A/u1D450/u1D719/u1D458.Wewillexplainhowtheseconstantvaluesareaggregated
750DeepInfer : Deep Type Inference from SmartContract Bytecode ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
intotheCDFembeddingvectorstoenhancethemodelinference
abilitywhenbuildingtheclassi/f_icationmodelandthegenerative
modelinthe following.
3.4.3 ClassificationModel. Asmentionedabove,wehaverestricted
allthetypesintoalimitedlist.Thus,wecannaturallyregardthesig-
nature inference as a classical multi-classi/f_ication task. Speci/f_ically,
given the embedding vectors of CDFs /u1D438/u1D451and CVs/u1D4501,/u1D4502,...,/u1D450/u1D458, we
expectthemodeltolearnnotonlytheaccessoperationsfromCDFs
but also the possible sizes from CVs. Thus, DeepInfer /f_irst uses a
linear function /u1D453/u1D450to map each constant embedding /u1D450/u1D456into the hid-
denvector ˆ/u1D450/u1D456andthenemploystheattentionmechanismtolearn
topaymoreattentiontotheconstantvaluethatarerelevanttoa
speci/f_ic type,i.e.,constant attention:
/u1D438/u1D450=/summationdisplay.1
/u1D458[/u1D452/u1D465/u1D45D(ˆ/u1D450/u1D456)//summationdisplay.1
/u1D458/u1D452/u1D465/u1D45D(ˆ/u1D450/u1D456)]/u1D450/u1D456 (5)
where/u1D438/u1D450istheoverallrepresentation oftheCVsineachfunction.
To learn the operation patterns and the constant information si-
multaneously, DeepInfer concatenatesthesetwoembeddingvectors
to form the representation vector for eachfunction:
/u1D438/u1D450/u1D459/u1D460=[/u1D438/u1D451|/u1D438/u1D450] (6)
According to the representation /u1D438/u1D450/u1D459/u1D460,DeepInfer can infer the
most possible type for the arguments by selecting the one that has
the maximal prediction probability over the type list:
/u1D45C/u1D462/u1D461/u1D45D/u1D462/u1D461/u1D450/u1D459/u1D460=/u1D44E/u1D45F/u1D454/u1D45A/u1D44E/u1D465(/u1D44A1/u1D438/u1D450/u1D459/u1D460+/u1D44F1) (7)
where/u1D45C/u1D462/u1D461/u1D45D/u1D462/u1D461/u1D450/u1D459/u1D460is the predicted type of a parameter; /u1D44A1and/u1D44F1
represent the trainable weightmatrixandbias,respectively.
Recall that we mark all the types of arrays as the general signal
Array.IfDeepInfer predictsaparameterasthetypeArray,itsactual
type still needs to be further determined. We will introduce the
details inthe nextsubsection.
3.5 FunctionSignatureGeneration
Diﬀerentfromenumerabletypes,typesthataremarkedasArray
canincludein/f_initenestedstructureandarbitrarysize,whichmakes
it impossible to maintain a limited type list holding all the possible
situations.Tosolvethisproblem, DeepInfer designsasequencegen-
erationmodeltodynamicallygeneratethepossiblearraytypeby
learning the type-related information extracted from the IR. Specif-
ically,DeepInfer followsthearchitectureofsequencetosequence
learningwhichconsistsofanencoderandadecodertoachievethis
goalas showninFig. 2.
Encoder .Diﬀerentfromtheclassicalsequencelearningencoder
that takes the tokenized sequence asinputs and makes each token
learnthecontextualtokeninformation, DeepInfer directlytreatsthe
embedding vector of CDFs (i.e., /u1D438/u1D451) and all the embedding vectors
of CVs (i.e., /u1D450/u1D458) as the input, but doesn’t require them to recognize
the context because there is no order relationships between the
inputs.DeepInfer utilizes such an input form because we expect
itcannotonlypredictthebasictypesofanarray(e.g.,uint,bool,
address,etc)accordingtoimplicitknowledgefromCDFsbutalso
determine the dimensions according to the CVs. Thus, DeepInfer
adopts the input embedding vectors X={/u1D4500,/u1D4501,/u1D4502,...,/u1D450/u1D458}(/u1D4500=
/u1D438/u1D451) to guide the type generationinthe decoding procedure.Decoder.DeepInfer uses vanilla Recurrent Neural Network (RNN)
as the basic architecture of the decoder which reads an input to-
kenandcombinesitwithembeddingvectorsfromtheencoderto
generate the target token. Assume the vocabulary of basic types is
V={/u1D4631,...,/u1D463/u1D45F}where/u1D45Frepresentsthevocabularylength, Deep-
Infercan predict the basic type of an array from this vocabulary
accordingtothe input information. However,thenested depthor
dimension size of an array cannot be foreseen because such values
needtobedetermineddynamicallyaccordingtotheinputs.That
is, it suﬀers from the out-of-vocabulary (OOV) issue [ 22,25]. To
deal with this situation, DeepInfer introduces the copy mechanism
[22,51]thatcan dynamically copy orgenerate the mostpossible
token as the decoder output.
Given the encoder output {/u1D4500,/u1D4501,/u1D4502,...,/u1D450/u1D458}and the current (i.e.
step/u1D461) decoder input hidden status /u1D460/u1D461,DeepInfer employs the atten-
tionmechanismto calculatethe currentcontextvector:
/u1D438/u1D450/u1D461/u1D465=/u1D458/summationdisplay.1
/u1D456=0/u1D6FC/u1D456/u1D450/u1D456 (8)
/u1D6FC/u1D456=/u1D460/u1D461/u1D44A/u1D454/u1D450/u1D456//summationdisplay.1
/u1D456/u1D460/u1D461/u1D44A/u1D454/u1D450/u1D456 (9)
where/u1D6FC/u1D456is the weight score and /u1D44A/u1D454is the trainable weight matrix.
Then,DeepInfer predictstheprobabilityoftheoutputtokenat
currentstepbyeitherselectingthemostpossibletokenfromthe
vocabularyVorcopyingthe token from the inputsequence X:
/u1D45C/u1D462/u1D461/u1D461=/u1D45D/u1D454/u1D454/u1D452/u1D45B/u1D452/u1D461+(1−/u1D45D/u1D454)/u1D450/u1D45C/u1D45D/u1D466.alt/u1D461 (10)
wherethe /u1D45C/u1D462/u1D461/u1D461isthe/f_inalprobabilitydistributionatstep /u1D461;/u1D45D/u1D454refers
totheprobabilityatokenneedstobegenerated; /u1D454/u1D452/u1D45B/u1D452/u1D461and/u1D450/u1D45C/u1D45D/u1D466.alt/u1D461
meanstheprobabilitydistributionsoverthevocabulary Vorthe
inputsequenceX,respectively:
/u1D45D/u1D454=/u1D70E/parenleftbig/u1D44A/u1D450/u1D461/u1D465/u1D438/u1D450/u1D461/u1D465+/u1D44A/u1D460/u1D460/u1D461+/u1D44A/u1D45D/u1D460/u1D461−1+/u1D44F/u1D454/parenrightbig(11)
/u1D454/u1D452/u1D45B/u1D452/u1D461=/u1D460/u1D45C/u1D453/u1D461/u1D45A/u1D44E/u1D465(/u1D44A/u1D449[/u1D460/u1D461|/u1D438/u1D450/u1D461/u1D465]+/u1D44F/u1D449) (12)
/u1D450/u1D45C/u1D45D/u1D466.alt/u1D461=/summationdisplay.1
/u1D456:/u1D465/u1D456=/u1D466.alt/u1D44E/u1D461/u1D461/u1D461
/u1D456(13)
where/u1D70Eis an activation function; /u1D44A/u1D450/u1D461/u1D465,/u1D44A/u1D460,/u1D44A/u1D45D,/u1D44F/u1D454,/u1D44A/u1D449, and/u1D44F/u1D449
are the trainable parameters; /u1D466.altis a token and /u1D465/u1D456∈X;/u1D44E/u1D461/u1D461/u1D461
/u1D456refers to
the attentive probability distributionover Xat step/u1D461.
Thedecoderstartswiththetoken<SOS>andoutputsallpossible
tokensstepbystepuntilaspecialtoken<EOS>appearedfollowing
the process in previous work [ 22,51]. To make the model better
learnthecorrectoutputtokensequence, DeepInfer introducesthe
teacher forcing technique [ 69] during the model training. This
techniqueemploysthescheduledsampling[ 8]thatmakestheinput
distribution between training and generation as similar as possible
to forcethe outputtokens being subjectto truly array types.
3.6 FunctionReturn Recovery
Asdescribedabove,argumentsaccesshasexplicitinstructionop-
erations in the EVM bytecode whereas the returns of a function
is stored into the memory. Thus, the model needs to understand
thefunctionalityforinferringfunctionreturns.CFGre/f_lectshow
each statement is executed during the program running [ 44][71]
andweexpectthemodeltounderstandreturn-relatedknowledge
fromtheCFG.GraphNeuralNetwork(GNN),duetoitspowerful
751ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KunsongZhao,ZihaoLi, JianfengLi, He Ye,Xiapu Luo, andTing Chen
structurallearningability,hasbeenwidelyusedinmanycodecom-
prehension related tasks [ 4][67]. Because of the graph structure
representationoftheCFG,itisnaturalthatusingtheGNNmodel
to learn its implicit information. In this work, DeepInfer adopts the
GraphAttention Network(GAT)to learntheknowledge fromthe
CFG, because not all the execution paths are associated with the
returnvaluesandGATcanautomaticallygivemorefocusonthe
executionpathsrelatedto the functionreturns.
Concretely,assumethenodesetofaCFGis Ω={/u1D7141,/u1D7142,...,/u1D714/u1D707}
where/u1D707represents the number of basic blocks. We extract the
instruction sequencefromeach basic blockandusetheiropcodes
to train another CBOW model for producing a new embedding
matrixW′similar to § 3.4.1. We don’t use the previous mapping
matrixWbecause our aim here is tomake theembedding ofeach
opcode capture its contextual operations at basic block level rather
than the signature-related CDFs. For each basic block, DeepInfer
usesW′toinitialize theopcodesincluded in itand averages them
to produce the node embedding vectors.
Next,DeepInfer employs the GAT to update the embedding vec-
tors of each node by incorporating its neighborhood nodes with
diﬀerentweights:
/uni210E∗
/u1D707=/summationdisplay.1
/u1D708∈N/u1D707/u1D711/u1D707/u1D44A/u1D45F/uni210E∗
/u1D708 (14)
where/uni210E∗/u1D707istheupdatedembeddingvectorofthe /u1D707-thnodeandN/u1D707
means the neighborhood nodes of the /u1D707-th node. /u1D44A/u1D45Fis trainable
parameters. /u1D711/u1D707isthe weightscore across N/u1D707:
/u1D711/u1D707=exp/parenleftBig
LeakyReLU/parenleftBig
/u1D44A′/u1D45F[/uni210E∗/u1D707|/uni210E∗/u1D708]/parenrightBig/parenrightBig
/summationtext.1
/u1D708∈N/u1D707exp/parenleftBig
LeakyReLU/parenleftBig
/u1D44A′/u1D45F[/uni210E∗/u1D707|/uni210E∗/u1D708]/parenrightBig/parenrightBig (15)
where LeakyReLU [ 37] is the activation function and /u1D44A′/u1D45Fis a train-
ableparameter.
After the node embedding vectors are updated by multi-layer
GAT,DeepInfer aggregatesall thenodeembedding vectorstopro-
duce the representation vectors /u1D43B/u1D43Afor a CFG using the similar
operation as Eq. (2) and Eq. (4). Finally, DeepInfer uses a softmax
layer to outputthe probability distribution.
4 EVALUATION
4.1 ExperimentalSetup
4.1.1 Dataset. To evaluate the accuracy of DeepInfer , we follow
the previous work [ 10] to instrument an Ethereum node to ob-
tain the runtime bytecode from deployed smart contracts. As a
result,wecollect47,598,631bytecodeforSoliditysmartcontracts
and 75,627 bytecode for Vyper smart contracts. We merely keep
theopen-sourcesmartcontractsbecausetheground-truthcanbe
obtainedfromtheirsourcecode.Weremovetheduplicatedsmart
contracts and use the Etherscan API [ 6] to collect their ground-
truth. According to the statistic, 99.9% functions with equal or less
thannineargumentsandreturns.Followingtheprocessingprin-
ciple in previous work [ 15], we focus on the functions with no
morethannineargumentsorreturns.Asaresult,weobtain292,064
unique functions for Solidity contracts and 5,003 unique functions
for Vyper contracts, respectively. For each language, we randomly
select80%ofthecollecteddataasthetrainingsetandtheremainder
istreatedas the test set.4.1.2 Implementation. We implement DeepInfer in about 2,100
lines of code using Python 3. The IR isparsed from the bytecode
using the Gigahorse tool [21]. We employ multiprocessing package
with 64 processesto parallelize the data processing pipeline. Since
there are multiple arguments or returns for each function, we /f_irst
trainamodeltopredictthenumberofargumentsorreturnsand
separately train the models for arguments or returns on each posi-
tion.Weiterativelytrainthemodelstorelievethedatade/f_iciency
issueandenhancetheknowledgelearnedbythemodel[ 63–65,72].
Forthemodelconstruction,weadoptPytorchframework[ 48]to
implementourHFLMcomponentinwhichtwobi-directionalLSTM
layersareused.Weembedeachinputtokenintoa100-dimensional
embeddingvectorandthesizeofhiddenlayerissetas200.Toopti-
mize themodel parameters, we selectthe Adam algorithm [ 30] as
theoptimizertoupdatethegradient.Duringthegenerationprocess,
weadoptbeamsearchtechnique[ 55]withabeamwidthof10to
producetheoutputsequence.Tounderstandthefunctionalityfrom
theCFG,weusetwolayersoftheGATtolearnandupdatenode
embedding vectors.
Table 2:EvaluationResults of DeepInfer
Compiler MetricSignature Return
Number TypeNumber Type
SolidityTop-1 0.983 0.835 0.871 0.749
Top-3 0.995 0.944 0.971 0.889
Top-5 0.998 0.966 0.988 0.943
VyperTop-1 0.721 0.634 0.800 0.535
Top-3 0.958 0.840 0.990 0.782
Top-5 0.994 0.897 0.998 0.841
4.2 RQ1: Howistheaccuracy of DeepInfer ?
4.2.1 Motivation. The aimof DeepInfer istorecoverthe function
signatures and returns by learning the access patterns or under-
standingthefunctionalityfromEVMbytecode,respectively.This
questionisdesignedtoexploretowhatextent DeepInfer canrecover
the function signatures orreturns.
4.2.2 Approach. Toanswerthisquestion,wetraindiﬀerentmodels
todealwithdistincttasks.For function signature inference, Deep-
Infertrainsamodeltopredictthenumberofargumentsandthen
other models are trained to determine the type at each position
over a limited type list (§ 3.4). For the types that are marked as
Array,sincethein/f_initenestingdepthandarbitrarysizeofthem,
DeepInfer trainsgenerativemodelstorecovertheactualtype(§ 3.5).
Forfunctionreturninference, DeepInfer trainsclassi/f_icationmod-
els that can understand the functionality to predict the number
and types of returns (§ 3.6). We evaluate the accuracy of DeepInfer
with all the collected functions in open-source Solidity and Vyper
contracts, respectively. We report the average top- /u1D458accuracy of
DeepInfer , which is the ratio of correct inference occurred in the
most probable /u1D458(/u1D458∈{1,3,5}) candidates to the total number of
unique ground truths.
4.2.3 Results. Table2illustratestheaccuracyof DeepInfer for re-
coveringthenumberandtypesofsignaturesandreturns,respec-
tively. When considering the top-1 suggestion, DeepInfer achieves
752DeepInfer : Deep Type Inference from SmartContract Bytecode ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
the accuracy of 0.983, 0.835, 0.871, and 0.749 for the number and
type inference of arguments and returns in Solidity, respectively. It
obtainsthe top-1 accuracy of 0.721, 0.634, 0.800,and 0.535 inVyper
smart contracts, respectively. When taking the top-5 suggestion
intoaccount,theaccuracygoesupto0.998,0.966,0.988and0.943for
Solidity, and 0.994, 0.897, 0.998 and 0.841 for Vyper, respectively. It
showstheaverageaccuracyimprovementsinrecoveringsignatures
and returns by 13.3% and 38.6% inSolidity and Vyper, respectively.
We manually investigate the incorrect inference and summarize
the causes offailures as follows.
1function register( bytes_domain, address _address) external {
2......
3addresses[_domain] = _address;
4}
Listing 2:Aninaccurate recovery incase1
Case 1:DeepInfer will confuse thetypes bytesandstringwhen
there is no operation on the parameter itself in smart contracts. As
shownin Listing 2, despiteDeepInfer correctly predict thesecond
parameterasthetype addressbutitinaccurately predictsthe/f_irst
parameter as stringinstead of bytes. This is because both bytesand
stringareloadedbythesameinstruction CALLDATACOPY .Theydiﬀer
only in that the former can be accessed by the BYTEinstruction
butthelattercannot.However,inthiscase,the/f_irstparameteris
used as the index without any manipulation for itself, i.e., the BYTE
instructionisabsent inthe corresponding IR.
1function getContractVersionCount( bytes32 _name) external ... {
2......
3return addressStorageHistory[_name]. length;
4}
Listing 3:Aninaccurate recovery incase2
Case 2:DeepInfer will confuse the types bytes32anduint256.
When loading the byte sequence, it will be masked by zeros on
the low-order side. Instead loading the unsigned integer will result
in the masking by zeros on the high-order side. However, when
loading the types bytes32oruint256, they have already reached the
maximum length (i.e., 32 bytes) so no masking operation is needed,
i.e., they have the same loading instructions. The diﬀerences be-
tweenthemarethattheformercanbeaccessedby BYTEinstruction,
whereasthelattercanbeusedforarithmeticoperations.Asaresult,
asshowninListing 3,DeepInfer incorrectly predictthe bytes32as
theuint256,becausetherearenobyteaccessorarithmeticopera-
tionsandthis parameterisonly usedas the index.
1function verify( address [2] tokens, uint256 [8] args) external {
2address depositToken = tokens[0];
3address issueToken = tokens[1];
4uint256 totalIssueAmount = args[0];
5uint256 interestRate = args[1];
6uint256 maturity = args[2];
7uint256 issueFee = args[3];
8uint256 minIssueRatio = args[4];
9......
10}
Listing 4:Aninaccurate recovery incase3
Case 3: There are some missing constant values due to the
compilation optimization of smart contracts, which misleads the
predictionof DeepInfer .AsshowninListing 4,theargumentsofthe
function are one-dimensional static array whose sizes needed to
be inferred by incorporating the information from constant values.
However, sincetheoptimization operationis activated duringthe
compilation, the constant values related to the size are missing. As
aresult,DeepInfer inaccuratelyinfersthe size ofthe array.1function getRiskAndValue( bytes32 _result) public returns (uint80
,uint128 ) {
2bytesmemory riskb = sliceFromBytes32(_result, 0, 16);
3bytesmemory valueb = sliceFromBytes32(_result, 16, 32);
4return (uint80(toUint128(riskb)), toUint128(valueb));
5}
Listing 5:Aninaccurate recovery incase4
Case 4:DeepInfer fails to infer some rare types. As shown in
Listing5, very few functions adopt the type uint80as the return
type. This causes the model can simply learn extremely limited
knowledge about this type during the model training process. As a
result,DeepInfer outputs an incorrectinference.
AnswertoRQ1 :Thetop-5accuracyof DeepInfer is0.974
for Solidityand0.933 for Vyper.
4.3 RQ2: Howdoes DeepInfer perform
compared with existingtools?
4.3.1 Motivation. Sincethere are someexisting tools thatdecom-
pile bytecode into human-readable pseudocode ordirectly recover
function signatures from the bytecode, this question is designed
to explore whether DeepInfer performs better than existing tech-
niques.
4.3.2 Approach. Wecompared DeepInfer withthreestate-of-the-
art decompilers (including EBD [ 43], Eveem [ 17], and Gigahorse
[21])andonesymbolicexecution-basedstaticanalysistechnique
SigRec [9] in terms of signature recovery. All the four baseline
approachessupportthebytecodeofsmartcontractsasinput.Ifone
method can correctly infer the types of signatures or returns at
aposition,wetreatthisasasuccessfulprediction.Wereportthe
average accuracy of these baselines in Solidity and Vyper smart
contracts, respectively.
4.3.3 Results. Table3presents the average results for DeepInfer
andbaselinemethods.Fromthistable,wecan/f_indthattheaverage
top-1 accuracy of DeepInfer achieves average improvements by
85.9% and 148.3% compared with the baselines while recovering
function signatures in Solidity and Vyper, respectively. Despite
DeepInfer obtainingnearlythesametop-1accuracyasSigRecwhen
predicting the types of arguments in Solidity, it achieves an im-
provement by 8.4% when considering the top-5 suggested types.
Besides, the top-1 accuracy of DeepInfer achieves an improvement
by 13.8% compared with SigRec when dealing with Vyper Smart
contracts. This is because SigRec depends on the static heuristic
rulesdesignedbyhumanexperts,whichlimits itsabilitytoexpand
to cover newly developed and compiled contractsinvolving new
access patterns.Instead, DeepInfer can automatically learn theac-
cess patterns of arguments from the bytecode without any human
intervention. We can see from this table that none of the baselines
support recovering function returns. Instead, DeepInfer achieves
the top-5 accuracy of 0.968 and 0.924 while recovering function
returns inSolidityandVyper,respectively.
753ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KunsongZhao,ZihaoLi, JianfengLi, He Ye,Xiapu Luo, andTing Chen
Table 3:Average Results for DeepInfer andBaselines
Compiler Approach EBD Eveem Gigahorse SigRecDeepInfer
Top-1 Top-3 Top-5
SoliditySignature 0.459 0.521 0.327 0.904 0.900 0.967 0.980
Return - - - - 0.818 0.936 0.968
VyperSignature 0.208 0.271 0.216 0.589 0.670 0.888 0.937
Return - - - - 0.674 0.891 0.924
AnswertoRQ2: Thetop-1accuracyof DeepInfer forrecov-
ering functionsignatures achieves average improvements
by 85.9% and 148.3% across the baselines in Solidity and
Vyper, respectively. In addition, only DeepInfer can recover
function returns with the top-5 accuracy by 0.968 for Solid-
ity and0.924 for Vyper.
4.4 RQ3: Howeﬃcientis DeepInfer ?
4.4.1 Motivation. Asthestaticanalysistechniquesforfunctionsig-
naturerecovery(e.g.,SigRec)aretime-expensiveduetotheprogram
simulation executionand path exploration[ 7],DeepInfer depends
onwell-traineddeeplearningmodelstodirectlypredictfunction
signatures. This question is designed to evaluate how eﬃcient is
DeepInfer whilerecoveringfunctionsignatures andreturns.
4.4.2 Approach. To explore what is the performance overhead of
DeepInfer , we execute and compare it with other baselines. We
excludetheapproachEBDbecauseitdependsonalimitedFSDand
searches the function signatures from this database if exists, which
just consumes a negligible amount of time. For DeepInfer , since
the training process can be done oﬄine, we compare its predictive
time cost. We report the average time consumption for recovering
signatures andreturns ofeachfunctioninseconds.
Table 4:Average Time Consumption
Approach DeepInfer /u1D446SigRec Gigahorse Eveem Speedup DeepInfer /u1D445
Time(s) 0.08 0.40 0.57 5.09 24.25x 0.003
DeepInfer /u1D446andDeepInfer /u1D445refer to theinference for signatures andreturns,respectively.
4.4.3 Results. Table4elaborates the average time consumption of
DeepInfer andotherthreebaselinesforrecoveringfunctionsigna-
tures.DeepInfer onlyspends0.08secondsforrecoveringsignatures
of each function, which is on average over 24 times faster than
otherbaselines. Besides, DeepInfer spends 0.003 seconds forrecov-
eringfunctionreturnsonaverage.Thisisbecausediﬀerentfrom
the static analysis-based techniques that run on central processing
units,DeepInfer adoptsdeeplearningastheinfrastructurewhich
naturallysupportstheoperation accelerationofthegraphicspro-
cessing units.In addition, DeepInfer can be fullytrainedoﬄineand
then usedto predict online.
Answer to RQ3: DeepInfer is over 24 times faster than the
baselinesonaverage.
4.5 RQ4: Howdoesthecompiler version aﬀect
DeepInfer ?
4.5.1 Motivation. The compiler versions are frequently upgraded
which will introduce the new characteristics and operations toavoid potential vulnerabilities [ 56]. This question is designed to
explore howthe changes of compilerversionsimpact DeepInfer .
4.5.2 Approach. Toevaluatetheimpactofvariouscompilerver-
sions onDeepInfer , we categoryall theopen-source Solidity smart
contracts according to their main versions. We train the models
on the lower versions and test whether DeepInfer still works on
thehighversionthatisnotvisibleduringthemodeltraining.For
example,we trainthe models onthesmart contracts withthe ver-
sions 0.4.x to 0.7.xand assess its accuracy on the high version 0.8.x.
We select the smartcontractswhose versionislarge than0.4.x be-
causethecounterpartistoolittletosupportthemodeltraining(less
than 1%). We report the average accuracy of DeepInfer on diﬀerent
compilerversions.
Top-1 Top-3 Top-50.70.80.91.0Accuracyv0.5.x v0.6.x v0.7.x v0.8.x
(a) SignatureTop-1 Top-3 Top-50.70.80.91.0Accuracyv0.5.x v0.6.x v0.7.x v0.8.x
(b) Return
Figure 3: Evaluation results of DeepInfer under various com-
piler versions.
4.5.3 Results. Fig.3presentstheaverageaccuracyof DeepInfer un-
derdiﬀerentcompilerversions.Wecan/f_indthat DeepInfer achieves
nearly the same performance for recovering function signatures
and returns across various compiler versions, respectively. Despite
thestate-of-the-arttoolSigRec hasthe abilitytorecover thefunc-
tion signatures as shown in § 4.3, it will be completely disabled
when thecompiler version islarger than 0.8.0.This isbecause the
design of SigRec relies on the access patterns of compiler versions.
Instead,DeepInfer is built upon the IR which strips the compiler-
related operations. Such design makes DeepInfer insensitive to the
changesofcompilerversions.Italsodemonstratesthat DeepInfer
actuallylearnstheimplicitknowledgethatisrelevanttofunction
signatures andreturns.
Answer to RQ4: DeepInfer is immune to the changes in
compilerversions.
5 DISCUSSION
In this section, we will discuss the limitations of DeepInfer and
potentialthreatsto validity encountered.
First,DeepInfer cannot recover the function signatures if the
IR is not correctly parsed during the phase of lifting. DeepInfer
adopts the state-of-the-art lifting tool Gigahorse [ 21] to convert
thebytecodeintotheregister-basedIR.IfGigahorsecrashes,theIR
cannotbeproperlylifted, DeepInfer willnotbeabletoextractthe
validaccesspatterns,resultinginafailedrecovery.Similarsituation
will also occur while recovering returns. But we found that such
situation is rare (nearly 0.2%), which implies that DeepInfer will
hardly be aﬀected. We will try to extend such tool to support valid
754DeepInfer : Deep Type Inference from SmartContract Bytecode ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
lifting operation, especially for the Vyper smart contracts in the
future.
Second, the accuracy of inferring some rare types is not as high
asthatofinferringothertypes.Thisisbecause DeepInfer designs
adeeplearning-basedarchitecturewhich isinherentlylimited by
the training data itself due to the data-hungry characteristics of
deep learning techniques [ 31,60]. One solution is to employ some
data-enhancementtechniquesthatpreservethesemanticsofthe
bytecode andwe leave this as the immediate future work.
On the other hand, there are some possible threats to validity
duringourexperiments.Thethreatstointernalvaliditylieinthe
tuning of hyper-parameters in the models. To relieve this validity,
we /f_ine-tune the batch size from 16 to 128 and the learning rate
from1e-5to1e-3tomakethemodelbefullytrained.Inaddition,
wesetthehiddensizeas200dimensionsandadoptsomedefault
settings(i.e.,stackingtwoLSTMorGATlayers)forexperiments.
Otherparametercombinationsmayalsoimprovetheaccuracyof
DeepInfer andwe leave this explorationas the future work.
6 RELATED WORK
6.1 SignatureInference in SmartContracts
Abi-guesser [ 50]wasdeveloped to infer thetypes ofABI-encoded
data. Chen et al. [ 9] was the /f_irst to develop a static analysis-based
tool called SigRec to recover function signatures in smart con-
tracts.Speci/f_ically,SigRec/f_irstdisassembledthebytecodeandthen
proposedtype-awaresymbolicexecutionthatexploredhowapa-
rameterwasmanipulatedinEVMinstructions.Itdesigneddiﬀerent
patternsfordiﬀerenttypesaccordingtothespeci/f_ictype-related
operations in EVM and symbolically executed EVM instructions to
recover parametertypes.
However,abi-guesser can onlydeal with very simple data for-
mats, which limits the ability in real-world smart contracts. In
addition,sincetheaccessingrulesusedbySigRecheavilyrelyon
the expert knowledge, itis hard to be extended. Thus, we propose
the/f_irstdeeplearningbasedmethodthatcanautomaticallylearn
theaccessingrulesorunderstandfunctionalitiestorecoverfunc-
tionsignaturesandreturnsfromthebytecodeofsmartcontracts
withoutany manual eﬀorts.
6.2 DeepLearning forSignatureInference
Although very little work focusedon functionsignature inference
in smart contract, this topic been explored much more in other
scenarios with the help of deep learning techniques. Chua et al.
[15] was the /f_irst to introduce deep learning into function type
recovery from C/C++ binaries. They regarded instructions as a
sequence and employed the RNN model to learn the semantics
to recover types. However, some important information related
to function signatures was stripped oﬀ during the compilation
process. To deal with this limitation, Lin et al. [ 35] proposed ReSIL
that injected compiler-optimization-related domain information
intotheinstructionstoassisttheinferenceoffunctionsignatures.
Peietal.[ 46]pretrainedamodeltolearntheoperationalsemantics
from asssembly instructions with generative state modeling and
thenusedthemodeltoinferthetypesforC/C++binaries.Lehmann
et al. [32] tried to recover types from WebAssembly binaries. They
de/f_inedgrammarsofhigh-leveltypelanguagesandemployedtheneural machine translation architecture to recover these high-level
types basedonthe DWARFdebugginginformation.
Inadditiontotheabovestudiesrecoveringsignaturesfrombi-
naries, there also exist work focused on signature recovery from
sourcecode.Maliketal.[ 38]developedaLSTM-basedmodelcalled
NL2Type that utilized the code comments, function names and pa-
rameternames fromthesourcecodetopredictthefunctiontypes
inJavaScript.Allamanisetal.[ 3]developedagraphneuralnetwork
basedmethodequippedwithanoveltripletlossfunctionwhichcan
learn the syntactic and semantic from source code to predict types
in Python functions. Mir et al. [ 41] /f_irst parsed Python source code
intoASTsandthenextractedidentifers,contextualinformationand
visibletypehints.Theyincorporatedcodesemanticslearntbyusing
a RNN model from code contexts with type hints and employed
kNN algorithm to infer types. Peng et al. [ 47] proposed HiTyper
thatcombineddeeplearningwithstaticanalysisfortypeprediction
inPython. They/f_irst constructedatypedependencygraph(TDG)
fromsourcecodeandthenconductedforwardstatictypeinference
along the TDG according to type dependencies. For some variables
that would impact the types of many others, HiTyper trained a
similarity-based deep learning model to recommend possible types.
Diﬀerentfromtheabovestudies,wefocusonrecoveringfunction
signaturesandreturnsfromthebytecodeofsmartcontractswithout
any human intervention. Since the smart contracts are designed
toperformespecialactionsontheblockchain(e.g.,transactions),
theabove-mentionedtoolscannotcapturesomedomain-speci/f_ic
operations, such as CALLDATALOAD andCALLDATACOPY . Thus, it is
necessarytodevelopanewtoolforthebytecodeofsmartcontracts.
To the best of our knowledge, we are the /f_irst to design such an
automaticdeeplearningmodelfor this purpose.
7 CONCLUSION
Wepresent DeepInfer ,anoveldeeplearning-basedframeworkto
automatically recover function signatures and returns from the
bytecodeofSolidityandVypersmartcontractswithoutanyhuman
intervention. The experimental results demonstrate that DeepInfer
is more accurate and eﬃcient than existing tools under diﬀerent
languages and distinct compiler versions. In the future, we plan
to conduct a deep analysis on each component of the DeepInfer
framework, and explore other alternative model components (such
asTransformers)andparametercombinationstoimprovethemodel
performance. In addition, we will extend our tool to support smart
contracts running onotherblockchains.
8 DATA AVAILABILITY
Ourexperimentalmaterialsareavailableat[ 57]orhttps://github.
com/sepine/DeepInfer .
ACKNOWLEDGMENTS
We sincerely thank the anonymous reviewers for their construc-
tive comments. This work is partly supported by Hong Kong RGC
Projects (No. PolyU15219319,PolyU15222320, PolyU15224121) and
National NaturalScienceFoundation(No.62202405).
755ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA KunsongZhao,ZihaoLi, JianfengLi, He Ye,Xiapu Luo, andTing Chen
REFERENCES
[1]ElviraAlbert, JesúsCorreas,PabloGordillo,GuillermoRomán-Díez,andAlbert
Rubio. 2019. SAFEVM: a safety veri/f_ier for Ethereum smart contracts. In Proceed-
ingsofthe28thACMSIGSOFTInternationalSymposiumonSoftwareTestingand
Analysis(ISSTA) . 386–389. https://doi.org/10.1145/3293882.3338999
[2]MaherAlharbyandAadVanMoorsel.2017. Blockchain-basedsmartcontracts:
A systematic mappingstudy. arXiv preprint arXiv:1710.06372 (2017).
[3]Miltiadis Allamanis, Earl T Barr, Soline Ducousso, and Zheng Gao. 2020. Typ-
ilus: Neural type hints. In Proceedings of the 41st Acm SIGPLAN Conference
on Programming Language Design and Implementation (PLDI) . 91–105. https:
//doi.org/10.1145/3385412.3385997
[4]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-
erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).
[5]MonikadiAngeloandGernotSalzer.2020. Characterizingtypesofsmartcon-
tractsin theethereumlandscape.In InternationalConference onFinancialCryp-
tography and Data Security . 389–404.
[6] EtherscanAPI.2019. Etherscan documentation. https://docs.etherscan.io/ .
[7]Roberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil Demetrescu, and
IreneFinocchi.2018.Asurveyofsymbolicexecutiontechniques. ACMComputing
Surveys(CSUR) 51,3 (2018), 1–39. https://doi.org/10.1145/3182657
[8]Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled
samplingforsequencepredictionwithrecurrentneuralnetworks. Advancesin
Neural InformationProcessingSystems(NeurIPS) 28(2015).
[9]Ting Chen, Zihao Li, Xiapu Luo, Xiaofeng Wang, Ting Wang, Zheyuan He,
KezhaoFang,YufeiZhang,HangZhu,HongweiLi,etal .2021. Sigrec:Automatic
recoveryoffunctionsignaturesinsmartcontracts. IEEETransactionsonSoftware
Engineering (TSE) 48, 8 (2021), 3066–3086. https://doi.org/10.1109/TSE.2021.
3078342
[10]TingChen,ZihaoLi,YufeiZhang,XiapuLuo,AngChen,KunYang,BinHu,Tong
Zhu,ShifangDeng,TengHu,etal .2019. Dataether:Dataexplorationframework
forethereum.In 2019IEEE39thInternationalConferenceonDistributedComputing
Systems(ICDCS) . 1369–1380. https://doi.org/10.1109/ICDCS.2019.00137
[11]Ting Chen, Zihao Li, Yufei Zhang, Xiapu Luo, Ting Wang, Teng Hu, Xiuzhuo
Xiao,DongWang,JinHuang,andXiaosongZhang.2019. Alarge-scaleempirical
study on control /f_low identi/f_ication of smart contracts. In Proceedings of the
2019 ACM/IEEE International Symposium on Empirical Software Engineering and
Measurement (ESEM) . 1–11.https://doi.org/10.1109/ESEM.2019.8870156
[12]TingChen,ZihaoLi,YuxiaoZhu,JiachiChen,XiapuLuo,JohnChi-ShingLui,
XiaodongLin,andXiaosongZhang.2020. Understandingethereumviagraph
analysis. ACM Transactions on Internet Technology (TOIT) 20, 2 (2020), 1–32.
https://doi.org/10.1109/INFOCOM.2018.8486401
[13]TingChen,YufeiZhang,ZihaoLi,XiapuLuo,TingWang,RongCao,Xiuzhuo
Xiao, and Xiaosong Zhang. 2019. Tokenscope: Automatically detecting incon-
sistent behaviors of cryptocurrency tokens in ethereum. In Proceedings of the
2019ACMSIGSACConference onComputerandCommunicationsSecurity(CCS) .
1503–1520. https://doi.org/10.1145/3319535.3345664
[14]Weimin Chen, Xinran Li, Yuting Sui, Ningyu He, Haoyu Wang, Lei Wu, and
Xiapu Luo. 2021. Sadponzi: Detecting and characterizing ponzi schemes in
ethereum smart contracts. Proceedings of the ACM on Measurement and Analysis
ofComputingSystems 5,2 (2021), 1–30. https://doi.org/10.1145/3460093
[15]ZhengLeongChua,ShiqiShen,PrateekSaxena,andZhenkaiLiang.2017. Neural
netscanlearnfunctiontypesignaturesfrombinaries.In 26thUSENIXSecurity
Symposium(USENIXSecurity) . 99–116.
[16]Monika Di Angelo and Gernot Slazer. 2020. Wallet contracts on Ethereum. In
2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC) . 1–2.
https://doi.org/10.1109/ICBC48266.2020.9169467
[17] Eveem. 2019. Eveem. https://eveem.org .
[18]Michael Fröwis and Rainer Böhme. 2017. In code we trust? In Data Privacy
Management,Cryptocurrenciesand Blockchain Technology . 357–372.
[19]MichaelFröwis,AndreasFuchs,andRainerBöhme.2019.Detectingtokensystems
on ethereum. In International Conference on Financial Cryptography and Data
Security. 93–112.
[20]XavierGlorot,AntoineBordes,andYoshuaBengio.2011. Deepsparserecti/f_ier
neuralnetworks. In Proceedingsofthe14th InternationalConferenceonArti/f_icial
Intelligenceand Statistics . 315–323.
[21]Neville Grech, Lexi Brent, Bernhard Scholz, and Yannis Smaragdakis. 2019.
Gigahorse: thorough, declarative decompilation of smart contracts. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE) . 1176–
1186.https://doi.org/10.1109/ICSE.2019.00120
[22]JiataoGu,ZhengdongLu,HangLi,andVictorOKLi.2016. Incorporatingcopying
mechanisminsequence-to-sequencelearning. arXivpreprintarXiv:1603.06393
(2016).
[23]JingxuanHe,MislavBalunović,NodarAmbroladze,PetarTsankov,andMartin
Vechev. 2019. Learning to fuzz from symbolic execution with application to
smart contracts. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
andCommunicationsSecurity(CCS) .531–548. https://doi.org/10.1145/3319535.3363230
[24]ZheyuanHe,ShuweiSong,YangBai,XiapuLuo,TingChen,WenshengZhang,
PengHe,HongweiLi,XiaodongLin,andXiaosongZhang.2022. TokenAware:
Accurate and Eﬃcient Bookkeeping Recognition for Token Smart Contracts.
ACM Transactions on Software Engineering and Methodology (TOSEM) (2022).
https://doi.org/10.1145/3560263
[25]Ziniu Hu, Ting Chen, Kai-Wei Chang, and Yizhou Sun. 2019. Few-Shot Rep-
resentation Learning for Out-Of-Vocabulary Words. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics (ACL) .http:
//dx.doi.org/10.18653/v1/P19-1402
[26]JianjunHuang,SongmingHan,WeiYou,WenchangShi,BinLiang,JingzhengWu,
and Yanjun Wu. 2021. Hunting vulnerable smart contracts via graph embedding
basedbytecodematching. IEEETransactionsonInformationForensicsandSecurity
(TIFS)16(2021), 2144–2156. https://doi.org/10.1109/TIFS.2021.3050051
[27]Bo Jiang, Ye Liu, and Wing Kwong Chan. 2018. Contractfuzzer: Fuzzing
smart contracts for vulnerability detection. In 2018 33rd IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE) . 259–269. https:
//doi.org/10.1145/3238147.3238177
[28]Jiao Jiao, Shuanglong Kan, Shang-Wei Lin, David Sanan, Yang Liu, and Jun
Sun.2020. Semanticunderstandingofsmartcontracts:Executableoperational
semantics of solidity. In 2020 IEEE Symposium on Security and Privacy (S&P) .
1695–1712. https://doi.org/10.1109/SP40000.2020.00066
[29]Jerome Kehrli. 2016. Blockchain 2.0-from bitcoin transactions to smart con-
tract applications. Niceideas, November. Available at: https://www. niceideas.
ch/roller2/badtrash/entry/blockchain-2-0-frombitcoin (2016).
[30]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[31]AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,IvanKrasin,Jordi
Pont-Tuset,ShahabKamali,StefanPopov,MatteoMalloci,AlexanderKolesnikov,
et al.2020. The open images dataset v4. International Journal of Computer Vision
128, 7 (2020), 1956–1981.
[32]DanielLehmannandMichaelPradel.2022. Findingthedwarf:recoveringprecise
types from WebAssembly binaries. In Proceedings of the 43rd ACM SIGPLAN
InternationalConferenceonProgrammingLanguageDesignandImplementation
(PLDI). 410–425. https://doi.org/10.1145/3519939.3523449
[33]ShipengLi,JingweiLi,YuxingTang,XiapuLuo,ZheyuanHe,ZihaoLi,XiCheng,
YangBai,TingChen,YuzheTang,etal .2023.BlockExplorer:ExploringBlockchain
Big Data via Parallel Processing. IEEE Trans. Comput. (2023).https://doi.org/10.
1109/TC.2023.3248280
[34]ZihaoLi,JianfengLi,ZheyuanHe,XiapuLuo,TingWang,XiaozeNi,Wenwu
Yang, Chen Xi, and Ting Chen. 2023. Demystifying DeFi MEV Activities in
FlashbotsBundle.In Proceedingsofthe2023ACMSIGSACConferenceonComputer
and Communications Security(CCS) .
[35]Yan Lin, Debin Gao, and David Lo. 2022. ReSIL: Revivifying Function Signature
Inference using Deep Learning with Domain-Speci/f_ic Knowledge. In Proceedings
ofthe12thACMConferenceonDataandApplicationSecurityandPrivacy .107–118.
https://doi.org/10.1145/3508398.3511502
[36]Lu Liu, Lili Wei, Wuqi Zhang, Ming Wen, Yepang Liu, and Shing-Chi Cheung.
2021. Characterizing Transaction-Reverting Statements in Ethereum Smart
Contracts.In 202136thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (ASE) . 630–641. https://doi.org/10.1109/ASE51524.2021.9678597
[37]Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al .2013. Recti/f_ier non-
linearities improve neural network acoustic models. In Proceedings of the 30th
InternationalConference onMachineLearning (ICML) , Vol. 30.3.
[38]Rabee SohailMalik, Jibesh Patra,andMichael Pradel. 2019. NL2Type:inferring
JavaScript function types from natural language information. In 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE) . 304–315. https:
//doi.org/10.1109/ICSE.2019.00045
[39]Jan Midtgaard and Thomas P Jensen. 2012. Control-/f_low analysis of function
calls and returns by abstract interpretation. Information and Computation 211
(2012), 49–76.
[40]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. 2013. Eﬃcient
estimationofwordrepresentationsinvectorspace. arXivpreprintarXiv:1301.3781
(2013).
[41]Amir M Mir, Evaldas Latoškinas, Sebastian Proksch, and Georgios Gousios. 2022.
Type4Py: practicaldeep similarity learning-basedtypeinference forpython. In
Proceedingsofthe44thInternationalConferenceonSoftwareEngineering(ICSE) .
2241–2252. https://doi.org/10.1145/3510003.3510124
[42]Benoît Montagu and Thomas Jensen. 2021. Trace-based control-/f_low analysis. In
Proceedings of the 42nd ACMSIGPLAN International Conferenceon Programming
LanguageDesign andImplementation(PLDI) .482–496. https://doi.org/10.1145/
3453483.3454057
[43] MrLuit. 2019. EVMbytecodedecompiler. https://github.com/MrLuit/evm .
[44]AnimeshNandi,AtriMandal,ShubhamAtreja,GargiBDasgupta,andSubhra-
jit Bhattacharya. 2016. Anomaly detection using program control /f_low graph
mining from execution logs. In Proceedings of the 22nd ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining . 215–224. https:
//doi.org/10.1145/2939672.2939712
756DeepInfer : Deep Type Inference from SmartContract Bytecode ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[45]Ethereum Yellow Paper. 2022. Ethereum: A Secure Decentralised Generalised
Transaction Ledger. https://ethereum.github.io/yellowpaper/paper.pdf .
[46]Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen Yao,
David Williams-King, Vikas Ummadisetty, Junfeng Yang, Baishakhi Ray, and
Suman Jana. 2021. StateFormer: /f_ine-grained type recovery from binaries using
generative state modeling. In Proceedings of the 29th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations
ofSoftwareEngineering(ESEC/FSE) .690–702. https://doi.org/10.1145/3468264.
3468607
[47]Yun Peng, Cuiyun Gao, Zongjie Li, Bowei Gao, David Lo, Qirun Zhang, and
Michael Lyu. 2022. Static inference meets deep learning: a hybrid type infer-
ence approach for python. In Proceedings of the 44th International Conference on
SoftwareEngineering(ICSE) .2019–2030. https://doi.org/10.1145/3510003.3510038
[48]PyTorch.2022. PyTorchDocumentation. https://pytorch.org/docs/stable/index.
html.
[49]BaptisteRoziere,Marie-AnneLachaux,LowikChanussot,andGuillaumeLample.
2020. Unsupervised translation of programming languages. Advances in Neural
InformationProcessingSystems(NeurIPS) 33(2020), 20601–20611.
[50] Samczsun. 2022. Abi-guesser. https://github.com/samczsun/abi-guesser .
[51]Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point:
Summarizationwithpointer-generatornetworks. arXivpreprintarXiv:1704.04368
(2017).
[52]Solidity. 2022. Solidity Documentation v0.8.17. https://docs.soliditylang.org/en/
v0.8.17.
[53]ContractABISpeci/f_ication.2022. SolidityDocumentationv0.8.17. https://docs.
soliditylang.org/en/v0.8.17/abi-spec.html .
[54]SWC-104.2020. Uncheckedcallreturnvalue. https://swcregistry.io/docs/SWC-
104.
[55]Transformers. 2022. Hugging Face Transformers. https://github.com/
huggingface/transformers .
[56]Solidity v0.8.0. 2020. Solidity v0.8.0 Breaking Changes. https://docs.soliditylang.
org/en/v0.8.17/080-breaking-changes.html .
[57]Paper Artifact V1.0. 2023. Paper Artifact. https://doi.org/10.6084/m9./f_igshare.
23993463.v1 .
[58] Vyper. 2022. Vyper Documentation. https://vyper.readthedocs.io/en/stable/ .
[59]Zhiyuan Wan, Xin Xia, David Lo, Jiachi Chen, Xiapu Luo, and Xiaohu Yang.
2021. Smartcontract security:Apractitioners’ perspective.In 2021IEEE/ACM
43rd International Conference on SoftwareEngineering(ICSE) . 1410–1422. https:
//doi.org/10.1109/ICSE-Companion52605.2021.00104
[60]XiaozhiWang,ZiqiWang,XuHan,WangyiJiang,RongHan,ZhiyuanLiu,Juanzi
Li, Peng Li, YankaiLin, and Jie Zhou. 2020. MAVEN: A massive general domain
eventdetectiondataset. arXiv preprint arXiv:2004.13590 (2020).
[61]HuihuiWeiandMingLi.2017. SupervisedDeepFeaturesforSoftwareFunctional
Clone Detection by Exploiting Lexical and Syntactical Information in Source
Code..In ProceedingsoftheInternationalJointConferenceonArti/f_icialIntelligence
(IJCAI). 3034–3040.
[62]Pengcheng Xia, Haoyu Wang, Bingyu Gao, Weihang Su, Zhou Yu, Xiapu Luo,
ChaoZhang,XushengXiao,andGuoaiXu.2021. Tradeortrick?detectingandcharacterizingscamtokensonuniswapdecentralizedexchange. Proceedingsof
the ACM on Measurement and Analysis of Computing Systems (SIGMETRICS) 5, 3
(2021), 1–26. https://doi.org/10.1145/3491051
[63]Zhiwen Xie, Runjie Zhu, Kunsong Zhao, Jin Liu, Guangyou Zhou, and Jimmy Xi-
angji Huang. 2021. Dual gated graph attention networks with dynamic iterative
training for cross-lingual entity alignment. ACM Transactions on Information
Systems(TOIS) 40,3 (2021), 1–30. https://doi.org/10.1145/3471165
[64]ZhiwenXie,RunjieZhu,KunsongZhao,JinLiu,GuangyouZhou,andXiangji
Huang.2020. Acontextualalignmentenhancedcrossgraphattentionnetworkfor
cross-lingualentityalignment.In Proceedingsofthe28thInternationalConference
onComputationalLinguistics (COLING) . 5918–5928. http://dx.doi.org/10.18653/
v1/2020.coling-main.520
[65]KunXu,LinfengSong,YansongFeng,YanSong,andDongYu.2020. Coordinated
reasoning for cross-lingual knowledge graph alignment. In Proceedings of the
AAAI conference onArti/f_icial Intelligence(AAAI) , Vol. 34.9354–9361.
[66]Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard
Hovy. 2016. Hierarchical attention networks for document classi/f_ication. In
Proceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies(ACL-HLT) .1480–
1489.http://dx.doi.org/10.18653/v1/N16-1174
[67]Zeping Yu, Wenxin Zheng, Jiaqi Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2020.
Codecmr: Cross-modal retrieval for function-level binary source code matching.
AdvancesinNeuralInformationProcessingSystems(NeurIPS) 33(2020),3872–3883.
[68]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In 2020 IEEE/ACM 42nd
International Conference on Software Engineering (ICSE) . 1385–1397. https:
//doi.org/10.1145/3377811.3380383
[69]Wen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. 2019. Bridging
the gap between training and inference for neural machine translation. arXiv
preprint arXiv:1906.02448 (2019).
[70]PeilinZheng,ZibinZheng,andXiapuLuo.2022.Park:acceleratingsmartcontract
vulnerability detection via parallel-fork symbolic execution. In Proceedings of
the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA). 740–751. https://doi.org/10.1145/3533767.3534395
[71]Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019.
Devign:Eﬀectivevulnerabilityidenti/f_icationbylearningcomprehensiveprogram
semantics via graph neural networks. Advances in Neural Information Processing
Systems(NeurIPS) 32(2019).
[72]Hao Zhu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2017. Iterative entity
alignmentviaknowledgeembeddings.In ProceedingsoftheInternationalJoint
Conference onArti/f_icial Intelligence(IJCAI) .
[73]XiaodanZhu,ParinazSobihani,andHongyuGuo.2015. Longshort-termmemory
overrecursivestructures.In InternationalConferenceonMachineLearning(ICML) .
1604–1612.
Received 2023-02-02; accepted 2023-07-27
757