2022 IEEE/ACM 44th International Conference onSoftwareEngineering (ICSE)
Adaptive Test Selection forDeep Neural Networks
Xinyu Gao
xinyugao@smail.nju.edu.cn
State Key Laboratory forNovel
Software Technology
Nanjing University
Nanjing 210023, China
Zixi Liu
zxliu@smail.nju.edu.cn
State Key Laboratory forNovel
Software Technology
Nanjing University
Nanjing 210023, China
ABSTRACTYang Feng*
fengyang@nju.edu.cn
State Key Laboratory forNovel
Software Technology
Nanjing University
Nanjing 210023, China
Zhenyu Chen
zychen@nju.edu.cn
State Key Laboratory forNovel
Software Technology
Nanjing University
Nanjing 210023, China
KEYWORDSYining Yin
ynyin@smail.nju.edu.cn
State Key Laboratory forNovel
Software Technology
Nanjing University
Nanjing 210023, China
BaowenXu
bwxu@nju.edu.cn
State Key Laboratory forNovel
Software Technology
Nanjing University
Nanjing 210023, China
Deep neural networks (DNN) have achieved tremendous develop-
ment inthepast decade. While many DNN-driven software appli-
cations have been deployed tosolve various tasks, they could also
produce incorrect behaviors andresult inmassive losses. Toreveal
theincorrect behaviors andimprove thequality ofDNN-driven ap-
plications, developers often need rich labeled data forthetesting
and optimization ofDNN models. However, inpractice, collecting
diverse data from application scenarios andlabeling them properly
isoften ahighly expensive and time-consuming task.
Inthis paper, weproposed anadaptive test selection method,
namely ATS, fordeep neural networks toalleviate this problem.
ATS leverages thedifference between themodel outputs tomea-
sure thebehavior diversity ofDNN testdata. And itaims atselect-
ingasubset with diverse tests from amassive unlabelled dataset.
Weexperiment ATSwith four well-designed DNN models andfour
widely-used datasets incomparison with various kinds ofneuron
coverage (NC). The results demonstrate that ATS cansignificantly
outperform alltestselection methods inassessing both fault detec-
tion andmodel improvement capability oftestsuites. Itispromis-
ingtosave thedata labeling and model retraining costs fordeep
neural networks.
CCS CONCEPTS
•Software and itsengineering --+Software testing and debug-
ging.
'Yang Feng isthecorresponding author.
Permission tomake digital orhard copies ofallorpart ofthis work forpersonal or
classroom useisgranted without feeprovided that copies arenotmade ordistributed
forprofit orcommercial advantage and that copies bear this notice and thefullcita-
tion onthefirst page. Copyrights forcomponents ofthiswork owned byothers than
theauthor(s) must behonored. Abstracting with credit ispermitted. Tocopy other-
wise, orrepublish, topost onservers ortoredistribute tolists, requires prior specific
permission and/or afee.Request permissions from permissions@acm.org.
leSE '22,May 21-29, 2022, Pittsburgh, PA,USA
©2022 Copyright held bytheowner/author(s). Publication rights licensed toACM.
ACM ISBN 978-1-4503-9221-1/22/05 ...$15.00
https://doi.org/1O.1145/3510003.3510232
73deep learning testing, deep neural networks, adaptive random test-
ing' testselection
ACM Reference Format:
Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu, Zhenyu Chen, and Baowen
Xu. 2022. Adaptive Test Selection forDeep Neural Networks. In44th In-
ternational Conference onSoftware Engineering (ICSE '22), May 21-29,2022,
Pittsburgh, PA,USA. ACM, New York, NY, USA, 13pages. https:lldoi.orgl
10.1145/3510003.3510232
1INTRODUCTION
Deep neural networks (DNN) have been deployed inmany fields
toassist insolving various tasks, such asmedical image diagno-
sis[28], autonomous driving [9],customer services [55], machine
translations [6]and soon.AstheDNN-driven software demon-
strates such fantastic performance onwell-defined tasks, influenc-
ingourdaily activities andlives, their quality and reliability have
raised wide concerns. DNN-driven software, essentially isonekind
ofsoftware program, could also suffer from software defects that
may cause monetary andeven human lifelosses [1-3]. Therefore,
forDNN-driven software, quality assurance techniques have be-
come exceedingly demanded.
However, assuring thequality ofDNN-driven software isachal-
lenging task, duetothenatural differences between DNN models
and conventional software systems. While conventional software
systems often rely ondevelopers toconstruct thebusiness logic
manually, DNN models, which form thekernel part andempower
theDNN-driven software, employ adata-driven programming par-
adigm that needs tolearn theinternal logic from massive data [36].
This feature notonly makes thebehavior oftheDNN model diffi-
cult tointerpret and analyze butalso disables theapplication of
many conventional quality assurance methods. With millions or
even billions ofneurons, co=ections, and activation functions,
DNN models construct complex nonlinear transformations tomap
input features into theproper labels. Thus, itishard fordevelopers
tooptimize theDNN-driven software bymanually tuning thein-
ternal parameters oftheDNN model. Inpractice, developers often
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu,Zhenyu Chen, and Baowen Xu
1https:llgithub.comiSATE-Lab/ATS2BACKGROUND
This section includes several basic knowledge ofDNN, neuron cov-
erage, and testselection methods.
2.1 The Architecture ofDeep Neural Network
The architecture ofthedeep neural networks (DNN) canberepre-
sented asacomposite function chain that maps theinput data xto
theoutput result y.
(2)
(3)(1) y=f(x) =lO) (J(l) (...(J(D) (x»»
Y={yly ERn,Ilylll =1/\Vi,y(i) ?:O}
L={III EY,::Ii,I(i) =I}Thecontributions ofthispaper could besummarized asfollows:
•Method. Wemodel thefault pattern and design ametric
toevaluate their differences forDNN models. Based onthe
fault pattern metric, wepropose ATS, thefirst adaptive ran-
dom testselection method forDNN models.
•Tool. Weimplement ATS into atool that could help DNN
developers toselect testcases from massive unlabelled data.
Wehave released thesource code ofthetoolandexperimen-
taldatasets online 1.
•Study. Weconduct anextensive experiment toinvestigate
theperformance ofATS method, coverage-guided selection
methods, and testprioritization methods. The results show
that ATS cansignificantly outperforms other test selection
methods and efficiently enhances theDNN model.
And thesetofallground truth isdenoted asLinthispaper:Take ann-category classifier asanexample, thegoal ofadeep neu-
ralnetwork istoapproximate ytothetheoretical classifier y*.In
other words, byadjusting theinside weights oftheDNN model,
wehave y""y*which gives anoutput vector y=f(x) closeto the
one-hot vector (i.e., corresponding ground truth) I=1*(x).The fi-
naloutput result iscalculated through thesoftmax function. Itcan
beinterpreted asaprobability (allelements arepositive, and the
sum is1).Thus theoutput domain Ysatisfies theconstraint that:
AnlOng them, thelength Dofthechain means thedepth ofthe
network. The function inthemiddle ofthechain, i.e.,fU),j=
{I,2,...,D-I}, isthehidden layer oftheDNN. Each hidden layer
fU)maps theforward input into avector, and each element of
thevector isanindependent unit (called aneuron) representing a
vector-to-scalar function. Thedimensionality Dofthehidden layer
fU)means thewidth ofthenetwork. Inside each layer, allneurons
ofthis layer areindependent and actinparallel.
Therefore, foraD layer deep neural network model Mwith
fixed-width W,ifaninput date xiscalculated bythemodel DNN,
weusehiERWtorepresent thei-layer vector value. Combin-
ingallhidden layers vector, wecould getahidden layer matrix
HER WxD-l ,which contains allthehidden neuron values ofan
input data.
Given aDNN model DNN andaninput data x,denote theset
Neuron(DNN, x)torepresent thehidden output values whenDNNretrain theDNN model with rich data [23,40,43]tofixtheincor-
rect behaviors and improve theperformance ofDNN-driven soft-
ware. Inthisprocess, they need tocollect agreat many ofdata from
application scenarios andhire alarge workforce tolabel them.
Under this situation, identifying and selecting themost repre-
sentative data become critical forimproving theeffectiveness and
efficiency ofquality assurance tasks ofDNN-driven software. In-
spired bythegreat success that thestructural code coverage cri-
teria achieved inconventional software programs, some prior re-
search hasproposed structural neuron coverage tomeasure thead-
equacy ofDNN testing [32,41,49]. These researches have demon-
strated theeffectiveness ofstructural neuron coverage ondistin-
guishing thegeneral mutation tests andadversarial samples. How-
ever, similar totheconventional code coverage, structural neuron
coverage also requires anextremely high overhead inthecollec-
tion process, thus itisdifficult toapply them onthelarge-scale
models [30]. Besides, several independent research groups also re-
veal that some neuron coverage criteria could rapidly reach the
maximum coverage with very few tests, which may restrict their
effectiveness asaguidance criterion fortest selection [14, 23,39].
Ontheother hand, another family oftechniques isproposed topri-
oritize testcases based onsome rules [23]. Test prioritization tech-
niques often value testcase that hasahigh probability ofdetecting
DNN incorrect behaviors. These techniques have demonstrated the
high effectiveness ofcollecting aportion oftests from alarge size
ofthedataset; however, they failtoconsider therelationship and
distribution ofselected data, andthus cannot diversify thedetected
incorrect behaviors. This feature may fundamentally hinder their
applications, especially when there areplenty ofsimilar, ordupli-
cate, tests inthecandidate set.
This paper extends theabove techniques andalleviates their lim-
itations. Weexplore analternative solution toassist thetestselec-
tion ofDNN models. Wefirst employ theoutput vector ofaDNN
model torepresent themodel behaviors ofthegiven input. Then,
wedefine thelocal domain oftheDNN model output todescribe
thefault pattern. Byprojecting thebehavior information into dif-
ferent local domains, wecanevaluate thefault pattern ofthetest
cases through extended operation, which values both themodel un-
certainty and behavior diversity. Further, wedesign afitness met-
rictomeasure thefault pattern difference between thecandidate
test and theselected set.Based ontheabove design, wepropose
ATS, thefirst adaptive test selection method fordeep neural net-
works, toselect more diverse tests from thecandidate set.ATS can
select asubset that reveals more different faults intheDNN-driven
software and reduces thelabeling efforts fortheoptimization pro-
cess.
Tovalidate theeffectiveness ofATS, weconduct experiments for
ATS and baseline methods with four well-designed DNN models
andfour widely-used datasets. Wealso realize different pollutions
ofunfiltered datasets inreality and simulate them inourexper-
iments. The experimental results demonstrate that ATS performs
well indefect detecting tasks. Moreover, wealso prove that our
adaptive test selection can select diverse defects faster than pri-
oritization techniques which blindly select thetest cases without
considering differences. Finally, wedemonstrate that ATS ismore
effective than both thecoverage-guided testselection methods and
testprioritization methods forDNN-driven system optimization.
74
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. Adaptive Test Selection forDeep Neural Networks leSE '22,May 21-29,2022, Pittsburgh, PA,USA
isexecuted with x: 2.3 Test case selection methods
And thefinal output result calculated bysoftmax function isde-
noted as:
2.2 Neuron Coverage Criteria
Inspired bytheeffectiveness ofstructural coverage onguiding the
testing ofconventional software applications, researchers have pro-
posed many testing criteria based onthestructural neuron cover-
agetomeasure thetestadequacy ofdeep neural network systems.
With theguidance ofstructural neuron coverage, several testgen-
eration techniques [26,49,57,59, 63,67]have been proposed toim-
prove theperformance ofDNN models. Inthis section, webriefly
introduce those structural neuron coverages.
Neuron Activation Coverage(NAC(k». Astheearliest neu-
ron coverage criterion [49], theprimary assumption ofNAC(k)
built upon isthat themore neurons areactivated, indicates more
states ofDNN areexplored. The computation process ofNAC(k)
requires collecting each neuron's output value and counting neu-
rons ascovered iftheir outputs exceed thethreshold k.The NAC( k)
coverage ofatestiscomputed astheratio ofthenumber ofcovered
neurons tothetotal number ofneurons.
k-MuItisection Neuron Coverage (KMNC(k». Based onthe
NAC(k) assumption about theDNN states, theresearchers further
assume that instead oftreating theneuron asavalue with only two
states (activated andinactivated), buttreats theoutput ofaneuron
asarange ofvalues [41]. Inother words, suppose that theoutput of
aneuron 0onthetraining setisintheinterval [Iowa, highaL and
divide them equally into ksegments. The goal oftheKMNC(k)
criterion istomake theneuron cover each segment ofksegments.
Neuron Boundary Coverage (NBC). Different from KMNC( k),
neuron boundary coverage (NBC) focuses onwhether thecorner
regions (-00, Iowa] and [high a,(0)arecovered bytestcases [41].
Strong Neuron Activation Coverage (5NAC(k». Some stud-
iespoint outthat strongly activated neurons may have additional
value forDNN, so5NAC(k) was proposed [41]. Strong Neuron
Activation Coverage (5NAC) isasimplification ofNBC, which
only collects theratio ofsome neurons that cover theupper bound
[high a,(0)toallneurons.
Top-k Neuron Coverage (TKNC(k». Top-k Neuron Coverage
(TKNC(k» focuses onthemost active kneurons ineach layer [41].
Itiscomputed bytheratio ofthetotal number oftop-k neurons on
each layer tothetotal number ofneurons inaDNN.
Modified Condition/Decision Coverage (MC/DC). Similar
totheconcept ofconventional software testing, MC/DCcriteria of
neuron networks [58] models theneuron output value (orsign) as
adecision, andalltheprevious layer co=ected neurons aremod-
eled asconditions. Neuron network MC/DC consists offour im-
plementations, namely 55,5V,V5,andVV-coverage, andallofthe
above-mentioned neuron coverage criteria could beregarded asa
specific situation oftheoriginal MC/DC neuron coverage [58].Inthissection, weintroduce several widely used testcase selection
methods. Foranytestselection method, thegoal istoobtain afixed
size(N)subsetXs from thecandidate setX c,i.e.,wehaveXs ~Xc
and IXsl =N.
2.3.2 Prioritization testselection. Generally, foragiven candidate
setXc, prioritization test selection methods compute aweight p
foreach test case xinthecandidate set.The weight prepresents
thetestsignificance oftestx.Inother words, pisthepossibility of
revealing model errors. Itcould bedenoted asfollows:
(6) p=Priority(x) xEXc
The testcase with higher priority pissupposed tohave ahigher
value ofdetecting faults andenhancing theDNN model. Therefore,
theprioritization testselection canberepresented asfollows:
Xs= argmax I IPriority(x)1 (7)
Xs';;XcAIXsl=N XEXs
Simply speaking, thepriority method oftestcase selection isto
select thetestcases with top-N weights toform atestsuite ofthe
required size.
Here weintroduce four different prioritization selection meth-
ods, which come from different fields ofAIsoftware testing and
traditional AIresearch.
DeepGini. Recently, Feng etal.[23] propose atest prioritiza-
tion technique based onastatistical perspective ofDNN, named
DeepGini. Ittakes theuseoftheGini coefficient tomeasure the
likelihood oftestcase xbeing misclassified.
LSA. Kim etal.[32] propose atest criterion towards DNN test-
ing, called SADL (Surprise Adequacy forDeep Learning). Inthe
research, LSA (Likelihood-based Surprise Adequacy) isproposed
tomeasure how close totheclass boundary thenew inputs are.
The higher LSA value ofthetest means itismore surprising to
theDNN. Thus itcould beregarded asapriority weight fortest
selection.
CES. Lietal.[40] propose atestselection method based oncon-
ditioning, which istoassess thenew precision onoperational envi-
ronments. The experiment results show that CES (Cross Entropy-
based Sampling) estimator outperforms random sampling inallex-
periments.
Maxp. Maxp isarepresentative uncertainty sampling strategy
ofactive learning [50].Itemploys themaximal prediction probabil-
ityoftheclassification task toindicate theprediction confidence of
theclassification model and thus prioritizes theinput oftheleast
prediction confidence.2.3.1 Coverage-guided testselection. Inthis section, weintroduce
asetoftestselection methods guided bycoverage metrics. Incon-
ventional software testing, testers tend toselect atest case that
covers more different code lines [65]. This kind ofselection method
follows abasic assumption that early reaching themaximum cov-
erage would lead tothehigher capability ofdefect detection [22].
Forany coverage metric, ineach iteration, itfollows anaddi-
tional greedy algorithm toselect thenext test according tothe
feedback from thepreviously selected set,i.e.,select thetestcases
that covers themaximum number ofuncovered area ofthegiven
coverage criterion.(5)(4)
Run(DNN,x) =yEYNeuron(DNN,x) =H=[hI,..., hD-l] ERWxD-l
75
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA
3METHODOLOGY
Thekeyfeature ofourselection method istoselect testcases with
diverse failure directions andhigher failure probabilities. First, we
introduce amotivating example foradaptive testselection ofDNN
testing. Then, wepropose amapping relationship, which converts
theoutput domain into asetofintervals todescribe thefault pat-
tern ofagiven test orset.After that, wepropose afitness metric
tomeasure thedifference between thecandidate test and these-
lected set.Finally, wepropose theadaptive test selection method
ATS based onfault pattern and fitness metric.
3.1 Motivation and Inspirations
Fortheconventional software program, there hasbeen some work
discussing theshape andthelocation offaults from theperspective
oftheinput domain [4,8, 12,24,61]. Toachieve aneven spread of
test cases within theinput domain, Chen etal.[15] propose adap-
tive random testing (ART) based ontheanalysis offault patterns.
Even though inthepast decade, plenty oftechniques have been
proposed forimproving ART andextending itsapplication scenar-
ios[7,11,16, 31], itisdifficult toapply them totheDNN model
testing tasks because oftheir working nature andinput features.
The input data ofmodern DNN models areoften ofvarious
types, such asimages, point clouds, texts, speech signals, and so
on.This variance makes itdifficult forustomeasure thefault pat-
tern from theinput domain. Nevertheless, foragiven testinput, the
DNN model often produces avector containing theprobability of
labels andthus determines thefinal output based ontheprobability
distribution. Thus, based ontheprobability distribution, selecting
thetestwith higher uncertainty may obtain ahigher probability
ofdetecting DNN faults. Foragiven output vector yERn, thepro-
cess could beformalized asRun(DNN, x)=y.Foraclassification
model, weregard aninput data xisclassified asi-thcategory, iff
thei-thelement ofyisthemaximum element (i=argmaxi y(i».
Iftheoutput probability oftheinput data ismore concentrated
(max(y(i» iscloser to1),weassume that themodel hashigher
confidence forclassification results.
Further, thetestcases which aredifferent from each other could
reveal more diverse faults ofDNN models. Formultiple testcases
with thesame degree ofuncertainty, asimilar testmay correspond
tothesame fault, and thetest cases that differ from each other
could better reveal different faults ofthemodel. Forexample, if
wehave atest setX={Xl, x2, x3, X4}. The output vectors are:
Y1=<0.9,0,0.1 >,Y2=<0.6,0,0.4 >,Y3=<0.2,0.4,0.4> and
Y4=<0.59,0,0.41 >.Thus, wecanregard theDNN model asmore
confident fortheclassification result ofXl,and {X2, X4} aremore
likely tobesimilar. Above observations inspire ustointroduce a
metric tomeasure thedifference offaults from theperspective of
theoutput domain. Based onthismetric, wedesign andimplement
ATS toguide thetestselection ofDNN models.
3.2 Fault Pattern Computation
Wedesignthe fault pattern mapping Pattern, which converts the
output vector into several intervals tosatisfy theinsights intro-
duced above. The sizeand location ofthesubsets reflect theinfor-
mation ofthemodel's uncertainty and test case x'sfault pattern.
76Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu,Zhenyu Chen, and Baowen Xu
.,1,...,0)
li4E~--......I­
-i'
Figure 1:AsiInplified figure toillustrate Step.3&4. (Project
and Extend).
Such amapping assists usinanalyzing andextracting thefault pat-
tern distribution corresponding toeach test case from theoutput
domain. Ontheone hand, wedefine theuncertainty oftest case
XETiasUncertain(x) =1-Run(DNN,x)(i). Forexample, ifwe
have anoutput vector y=<0.3,0.3,0.4 >,then theuncertainty is
denoted as1-max(y(i» =0.6. Ontheother hand, weexpress
thedirection astheline from theprediction one-hot vector Lito
theoutput vector y.Forexample, fortheoutput vector ydenoted
above, itsprediction vector isdenoted asL3=<0,0, 1>,thus the
direction isdenoted asl;y=<0.3,0.3, -0.6 >.Finally, wedesign
thefault pattern based onboth direction and uncertainty.
3.2.1 Fault Pattern afTest Case. Next, weintroduce themain steps
ofcomputing thefault pattern. ForatestsetT,thefault pattern of
each test XEXiscalculated byfour steps.
Stepl. Test SetClustering: Foreach test case XET,wecluster
thetestcase into nsubset ofTbased onitsprediction cate-
gory, i.e.,wehave xETdffi =argmaxi(Run(DNN,x)(i» =
argmaXi y(i).Clustering based ontheprediction category
ensures that wecananalyze testcases with similar results.
Step2. Local Domain Determining: Foreach test case xinclus-
terhweconstruct anindex setInd(i)={(i,p,q)Ip*-q*­
i/\1~P<q~n/\p,qEN}. Each element inInd(i) repre-
sents alocal domain, which isaplane spanned byLiLpand
LiLq.Byanalyzing each local domain onebyone, wecould
extend fault pattern information totasks with anynumber
ofcategories (n2':3).
Step3. Project Operation: This step aims toextract local infor-
mation oftest case x'soutput vector y.Refer toFig. 1,the
goal ofProject istofind they'Ef'..LiLpLqsubject to(y'-
y)J..span(Lilp, Lilq). Specific calculation formula can re-
ferto[10,56]. Through calculation, wecould getthelocal
information y'corresponding toindex (i,p, q).
Step4. Extend Operation: Extend thesubspace vector y'deter-
mined by(i,p,q)toaninterval denoted asPattern x(i,p,q)=
[a,b].Refer toFig. 1,weimplement theoperation inthetri-
angle f'..LiLpLq'Wedetermine theintersection point byex-
tending L"(y' toLpLq.The intersection point represents the
local direction ofthetest case x.After that, thelocal fault
pattern isdesigned asasegment GKinline LpLq.The mid-
point ofsegment GK istheintersection, and thelength is
determined bytheuncertainty ofthetestcase x.Finally, we
normalize theLpLqtotheinterval [0,1], and consider the
local fault pattern asthenormalized interval [a,b]~[0,1]
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. Adaptive Test Selection forDeep Neural Networks leSE '22,May 21-29,2022, Pittsburgh, PA,USA
ofsegment GK. Thenormalized length of[a,b]iscontrolled
1-y(i)by-ry-' where I]isahyperparameter tocontrol thegran-
ularity. Werecord thelocal fault patterns oftestxETicor-
responding toindex (i,p,q)asPattern x(i,p,q)=[a,b].
StepS. Local Pattern Gathering: Foreach test case xincluster
hwecollect allofitslocal fault patterns Patternx(i,p, q),
where (i,p,q) isinInd(i), thesetoflocal patterns isre-
garded astestcase x'sfault pattern. Inother words, wehave
that Pattern x=U(i,p,q)Elnd(i){(i,p,q): [a,b]}.When test xbelongs tocluster hthefitness metric could be
expressed asfollows:.() I IPatternx(i,p, q)\Pattern5(i,p, q)1Fitness x,5 =..I Pattern x(i,p,q)UPatterns (i,p,q)I(l,p,q) Elnd(l)
(9)
Noted that, theoperator \and UinEq.9arebasic setopera-
tors [25], and I.Irepresent thelength ofcorresponding set.The
Fitness metric reflects thedifference between thetest xand the
selected set5.
Table 1:Anexample toillustrate fault pattern.Algorithm 1:ATS test selection Alg.
3.3.1 Fitness Metric. The fitness metric aims toprovide anormal-
ized value toquantify thefitness ofeach candidate xfrom thecan-
didate setCagainst theselected set5,denoted asFitness(x, 5).3.3 Fault Pattern Fitness Metric Design
With thehelp offault pattern, wepropose afitness metric that
assesses thedifference between testcase xand theselected testset
5.Based onthismetric, wecould migrate thebasic idea ofadaptive
random testing (ART) into DNN test case selection.3.2.2 Fault Pattern ofTest Set. Toobtain thefault pattern ofa
given testsetT,weneed tomerge thefault patterns calculated by
each testcase. Inother words, forcluster Ti,wetake theunion ofall
corresponding local fault patterns under each specific local domain
(i,p,q),denoted asPatternYi=U(i,p,q) Elnd(i) {(i,p,q):5}.Noted
that,S isasubset ofnormalized segment interval [0,1],which is
obtained by5=UxETiPatternx(i, p,q).
Finally, wegather together fault patterns ofallclusters toget
thefault pattern ofthegiven testsetT:
Patterny =U U {(i,p,q):5} (8)
Yi,i=l, ..,n(i,p,q) Elnd(i)Example 1.Here weintroduce a4-category classification exam-
pletoillustrate thefault pattern introduced above. Forthetestcase
xand itsoutput result Run(DNN, x)=y=<0.5,0.2,0.2, 0.1>.
First, wedetermine i=argmaxiy(i) =1.(Step!.) Second, when
n=4,theindex setofcluster Tiisconstructed asInd(i)={(1, 2,3),
(1,2,4), (1,3,4)}. (Step2.) After that, thelocal information y'of
output yiscalculated (Step3.), which isshown incolumn 1,2,3,4.
Based ontheExtend operation (Step4.), theintersection points are
shown inIntersect column. Thelength oflocal fault pattern iscal-
culated by(l-Yl) /1]=0.1(I]=5inthisexample), andthecovered
interval of(i,p, q)isshown inthelast column. Finally, wecould
gather thefault pattern oftestcase x(StepS.). Pattern x={(1, 2,3):
[0.45, 0.55], (1,2,4) :[0.59, 0.69], (1,3,4) :[0.59, 0.69]}.
26
2725
28
29return selected list:5;1Procedure ATS(DNN, C~X,N);
2collect output vectors: Run(DNN,X);
3determine output vector dimension n;
4cluster Cinto nsubset: C,(i=1, ,n);
5construct index sets: Ind(i) (i=1, ,n);
6foreach testxEC,calculate fault pattern: Pattern x;
7initial selected subset: 51,...,5n<--0;
8initial selected testcase list:5<--0;
9forj=1,2,...,L!ifJdo
//evenly select from each cluster
10 fori=1,2,...,ndo
11 ifC, '"0then
12 forxEC,do
13 Lcalculate Fitness(x, 5,);
14 ifmax xFitness(x, 5,)>0then
15 x=argmax xFitness(x, 5,);
//select test xwith maximum fitness
16 5.append(x);
//Added inorder
17 5,<--5,U{x};
18 C,<--C,\{x};
3.3.2 Overall procedure. Wedivide thewhole selection procedure
into two parts: cluster bycluster selection and total selection. From
Line 9toLine 17,wetend toevenly select test cases which have
maximum fitness metric Fitness(x,5i) according totheselected
subset. Thus, wecould select thetest case with different fault pat-
terns and higher uncertainty ineach cluster. Ifthecandidate set
isunbalanced, there may exist asituation that wecannot select19C<--U'=l .....nC,;
20while size(5) <Ndo
21 forxECdo
22Lcalculate Fitness(x, 5);
23 ifmax(Fitness(x, 5»>0then
24lx=argmax xFitness( x,5);
//select test xwith maximum fitness
else
lx=argmax x(L:(i.P.q) Elnd(i) 1Patternx(i, p,q)I;
//select test xwith higher uncertainty
5.append(x);
C<--C\{x};[0.45,0.55]
[0.59,0.69]
[0.59,0.69]234 lInter Local Fault
sect PatternIndex
Pattern x(1,2,3) 0.530.230.23 0 0.5
Pattern x(l,2,4) 0.570.27 00.16 0.64
Pattern x(l,3,4) 0.57 00.270.16 0.64Run(DNN,x) =yll0.5 0.20.2 0.1I
77
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA
enough test cases forsome clusters. From Line 20toLine 28,we
select therestpart oftheselected setS.First, weprefer test cases
with higher fitness metric results. InLines 25-26, ifalltest cases
inthecandidate sethave thesame fitness scores, then wesimply
select thenext case based onitsuncertainty.
Table 2:AsiInplified example ofATS.Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu,Zhenyu Chen, and Baowen Xu
generation methods, baseline approaches, and research questions.
Toconduct theexperiments, weimplement ourapproach aswell as
various NC-guided testselect methods upon Keras 2.3.1 with Ten-
sorFlow 1.13.1. Allexperiments areperformed onaUbuntu 18.04.3
LTS server with two NVIDIA Tesla V100 GPU, one 10-core pro-
cessor "lntel(R) Xeon(R) Gold 6248 CPU @2.50GHz", and 120GB
memory.
Candidate
testCase IOri Round 1ISet
Pattern Pattern Fitness PatternRound 2
Fitness4.1 Datasets and DNN Models
Xl [0.45,0.55] 0.05/0.35 0.05/0.53
X2 [0.2,0.4] 0/0.3 [0.2,O.5]U 0.0/0.48
x3 [0,0.12] [0.2,0.5] 0.12/0.42 [0.7,0.88] 0.12/0.6
x4 [0.7,0.88] 0.18/0.48
x5 [0.46,0.66] 0.16/0.46 0.16/0.64
Example 2.Inthispart, weintroduce asimplified example ofATS.
Weomit theprocess ofclustering (Step1.), local domain determin-
ing(Step2.) and local fault patterns gathering (StepS.). Thus only
thecore part oftheselection procedure isintroduced. InRound 1,
weshow thefitness between original fault pattern [0.2,0.5] and
h£ul fh. IPattern x\Patterns 1 1tea tpattern 0eac test, l.e., IPatternxUPatterns I'Forexamp e,
fttF't ([02 05]) -1[0.45,0.55]\ [0.2,0.5] 1-orescase Xl, Iness Xl,.,. - 1[0.45,0.55]U[0.2,0.5] 1-
I[~:~:~:~~J I=0.05/0.35. Based onLine 15inAlg. 1,weselect out
test x4inthefirst round. And incolumn SetPattern, weupdate
thefault pattern oftheselected set. Inthenext round (column
Round 2),wecalculate thefitness between each testincandidate
set{Xl, x2, x3, X5}and [0.2,0.5] U[0.7,0.88]. Finally, inRound 2,
weselect outthemaximum fitness testcase X5.
3.4 Enhancing theDNN model with ATS
Both testing and enhancing theDNN-driven system need torely
onmanually labeled data. Collecting plenty ofunlabeled data is
usually easy toachieve. However, compared with thecost ofdata
collection, thecost ofmanual labeling ismuch greater. Fordata
with strong expertise knowledge, such asmedical image data, itis
unrealistic toblindly label allcollected data. Therefore, thesignifi-
cance ofDNN testselection istoreduce thelabeling cost. Such an
idea isalso inspired byactive machine learning [50], which aims
toselect data near thedecision boundary (uncertainty). Program-
mers suppose that these uncertain data could beused tooptimize
thedecision boundary ofDNN more efficiently.
Besides, ATS also payattention tothedistributionselected data
subset. The success ofadaptive random testing (ART) proves that
theselection strategy based onpreviously selected data feedback
ismeaningful. From theperspective oftheDNN model, thecom-
plexity ofthedecision boundary makes uncertainty prioritization
prefers aspecific fault pattern.
Tosum up,ATS could notonly select testcases todetect model
faults more efficiently butalso construct amore proper subset for
DNN optimization through retraining.
4EXPERIMENT DESIGN
This section introduces our experimental settings, including the
data setand DNN model used intheexperiment, select data set
78Table 3:Dataset and DNN models.
Dataset Description IDNN Model #Neurons Layers
28x28 hand- ILeNet-1 42 5
MNIST written digits LeNet-5 258 7
32x:2 colored IResNet-20 698 20
CIFAR-10 lIllages VGG-16 7274 21
28x28 gray- LeNet-1 42 5
Fashion scale images ResNet-20 698 20
street view LeNet-5 258 7
SVHN numbers VGG-16 7274 21
Intheresearch, theexperiments areconstructed onfour well-
known publicly available DNN datasets: MNIST, CIFAR-10, SVHN,
andFashion. Table 3presents thestatistical details onthese datasets.
MNIST [64] dataset isahandwritten digits dataset with 10labels.
Itcontains 70,000 input data intotal, ofwhich 60,000 aretraining
data, and 10,000 aretest data. CIFAR-10 [33, 34]dataset consists
of60,000 32x32 colour images in10classes, with 6,000 images per
class. CIFAR-10 isdivided into 50,000 training and 10,000 testing
images. Fashion [62] isadataset ofZalando's article images con-
sisting ofatraining setof60,000 examples and atestsetof10,000
examples. Each example isa28x28 grayscale image associated with
alabel from 10classes. SVHN [47] isareal-world image dataset
that canbeseen assimilar toMNIST. Butitincorporates anorder
ofmagnitude more labeled data (over 600,000 digit images). Itiscol-
lected from house numbers inGoogle Street View images. Atthe
same time, weselect four different scale DNN models toensure the
universality ofourexperiments, which areLeNet-1, LeNet-5 [37],
VGG-16 [54] and ResNet-20 [28]. Foreach data set,weselect two
different DNN models toensure thecriterion result isstable and
excellent ondifferent combinations.
4.2 Candidate SetConstruction
Tosimulate thedata mutation inrealistic settings, wechoose tofol-
low theprior research convention [46,53] and employ seven well-
designed benign perturbations rather than adversarial examples
generators fordata augmentation. Wemake this choice isbecause
adversarial examples areoften generated from carefully designed
algorithms [35,48],they cannot represent data collected from the
real-world application scenario andmay lead tounreliable conclu-
sions [39].
Foreach dataset, wegenerate thetestdata based onseven well-
used benign perturbations toretain itsoriginal label, including
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. Adaptive Test Selection forDeep Neural Networks
shift, zoom, brightness, rotation, shearing, blur, and contrast ra-
tio[46, 53].When theoriginal test size isN,foreach augmenta-
tion operator, wegenerate thesame amount ofdata. Wedivide the
original test setand each generation setinto two parts, one for
constructing thecandidate setand theother forconstructing the
new test set.FortheMNIST dataset, wefinally constructed acan-
didate setand anew test setwith thesame size 40000, i.e.,5000
original testdata, and5000 generated data forseven augmentation
operators.
Furthermore, considering that unfiltered candidate data may in-
clude pollution and invalid data. Foreach generated candidate set,
wealso design different data pollution scenarios tocover differ-
enttypes ofinvalid data that may exist inanunfiltered dataset. In
addition tothepure candidate set,weconstruct four polluted can-
didate sets with 20% additional invalid data, including irrelevant
data, meaningless synthetic data, repeat data, and crashed data.
Tosum up,weconstruct five candidate sets foreach dataset,
including apure valid data setand four unfiltered datasets with
part ofinvalid data.
4.3 Baseline Approaches
During thetestselection, wetake theRandom Sampling asabase-
line method naturally. Wedenote this baseline asRS. RSdraws
samples randomly from thecandidate testsetaccording tothetar-
getsize.
Next, weintroduce other baseline approaches used inexperi-
ments. The baseline selection methods aredivided into two types:
coverage-guided testselection andprioritization testselection. We
choose four typical techniques foreach type.
4.3.1 Coverage-guided TestSelection. Tocompared with coverage-
guided test selection methods, weselect 4well-known DNN neu-
ron coverage criteria (NAC [49], NBC, TKNC and SNAC [41])
toguide thetest selection procedure introduced inSec. 2.3.1. For
themore specific introduction, refer toSec. 2.2.Noted that lim-
ited bythecomputation resource, weabandoned KMNC [41] and
MC/DC [58] asthebaseline because even forasimple DNN model
(LeNet-5), they take more than 24hours toselect 10% test cases.
The configurable parameters oftheneuron coverage criteria fol-
low theauthors' suggested settings oremploy default settings of
theoriginal papers [41,49].
4.3.2 Prioritization TestSelection. Wealso choose four prioritiza-
tion selection methods introduced inSec. 2.3.2 tocompare theef-
fectiveness with widely-used prioritization methods. More specifi-
cally, weconduct theexperiments with DeepGini [23], LSA [32],
CES [40]andMaxp [38]. Noted that forDeepGini, weabbreviated
itasGini intherest ofthepaper.
4.4 Research Q.Yestions
ATS isdesigned toadaptively select thenext testcase according to
thepreviously selected set.Itaims toselect anappropriate subset
formodel faults detection and DNN model optimization. Based on
thegoals oftest selection, weempirically explore thefollowing
research questions (RQ).
4.4.1 RQl: Fault Detection. Can ATS detect more faults than
baseline approaches?
79leSE '22,May 21-29, 2022, Pittsburgh, PA,USA
Similar toconventional software testing, foragiven test selec-
tion method, aselected setthat can trigger more faults means it
could reveal more defects inthesoftware. Therefore, wefirst com-
pare thefault detection capabilities ofATSandbaseline approaches.
Ineach DNN model &Dataset, weselect 10%sizeofeach candidate
set,filter theinvalid data, and collect thecorresponding fault de-
tection rate. Foraselected test setX,thefault detection rate is
defined asfollows:
. IXwronglFault_DetectlOn_Rate(X) = IXI (10)
where IXiIdenotes thenumber oftest cases being misclassified,
and IXIdenotes thesize oftheselected set.
4.4.2 RQ2: Fault Diversity. Can ATS select testcases that cover
more diverse faults?
For traditional software testing, Chan etal.[15, 18]have ob-
served that failure-causing inputs usually arevery dense andclose
tooneanother. Such insight could bemigrated toDNN model test-
ing,i.e.,similar faults may reflect thesame defect inDNN. Inorder
toanalyze themodel more comprehensively, wehope thetest se-
lection methods could notonly detect more faults butalso detect
more diverse faults efficiently.
Weuseaconcept offault type toanswer this question. Fora
given testcase xbeing misclassified, itsfault type isdefined as:
Fault_Type(x) =(Label(x)* --->Label(x» (11)
where Label(x)* denotes theground-truth label, and Label(x) de-
notes theprediction label calculated byDNN model. Forexample,
ifahandwritten digits xwith true label "7"ismisclassified into "I",
then thefault type ofxisdenoted as:Fault_Type(x) =(7--->1).
Foreach candidate setwith tencategories, thenumber offault
types is10X9=90.Therefore, foreach DNN model and dataset
combination, weaggregate thefault types ofthefive candidate
sets, thetotal number offault types foraspecific DNN&Dataset is
90X5=450. Wecollect thetheoretical maximum number offault
types and compare thechanging trend ofthenumber ofdetected
fault types.
Toquantify thecapability ofselecting diverse faults, wecollect
thecumulative sum ofthefault types found byspecific test se-
lection methods and compute thecorresponding RAUC (ratio of
area under thecurve) between theselection method and theoreti-
calcurve.
4.4.3 RQ3: OptiInization Effectiveness. Does thetestcases se-
lected byATS guide theretraining more effectively?
Different from thetraditional software, theDNN model could
not beenhanced directly. Therefore, wepropose RQ3 tofurther
evaluate theeffectiveness ofretraining theDNN model with se-
lected testcases. Toanswer RQ3, wechoose four selected setwith
different sizes (2.5%, 5%,7.5%, 10%) toadd back into theoriginal
training setand retrain theDNN model. And theeffectiveness of
model optimization iscompared onthenewly constructed testset
introduced inSec. 4.2.
The experiment isrepeated twice inallcombinations toavoid
random errors intheprocess ofretraining. Furthermore, wenot
only compute theaverage accuracy improvement butalso imple-
ment theWilcoxon rank -sum test[21] tocheck whether theATS are
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA
statistically significant outperform other baseline approaches at
thesignificance level of0.05.
5RESULT ANALYSIS
Inthissection, wepresent theresults offault detection (RQl), fault
diversity (RQ2) and further analyze whether ATS could optimize
theDNN model more effectively (RQ3).
5.1 Answer toRQ1: Fault Detection
Asshown inTable 4,wecompare theaverage fault detection rate
between ATS and baseline methods. Limited bythespace, weonly
display two selection ratio results (5%&10%). Other results follow
thesame trend.
First, wefound that allcoverage-guided testselection methods
have apoor performance indetecting faults. Such afinding iscon-
sistent with theformer research [14,27,39].Compared with some
existing works that demonstrate thefault detection capability of
neuron coverage inthetestsetwith adversarial examples [39,41],
weassume that thefaults generated bydata mutation inrealistic
settings aremore natural yetchallenging tobedetected. One of
thepossible illustrations totheineffectiveness ofneuron coverage-
guided test selection may bethat, compared toadversarial exam-
ples, benign perturbations could notlead tomany different neuron
activation states. Besides, wealso find outthat theneuron cov-
erage iseasy toreach themaximum during theselection process
and nolonger beincreased, which isalso supported byexisting
researches [23].
Compared with random sampling (RS) baseline, both ATS and
most prioritization selection methods show ahigher capability of
detecting more faults. ATS hasthebest result inmost DNN model
&dataset combinations. Weconclude that ATS hasanoutstanding
capability ofdetecting more faults within alimited selection size.
5.2 Answer toRQ2: Fault Diversity
Asshown inFig. 2,foreach Dataset &DNN model combination,
wedraw thecumulative sum curve ofthefault types foreach selec-
tion method. Note that thedark blue curve onthetoprepresents
thetheoretical maximum number offault types that could bein-
cluded inthecorresponding subset size. Besides, wealso compute
theratio ofarea under thecurve (RAUe) between theoretical and
each selection method. The closer theRAUC isto1,means thecor-
responding selection method performs better indetecting more di-
verse faults. The results ofRAUC areshown inTab. 5.Compared
with random sampling (RS) baseline, ATS, Gini, Maxp, and TKNC
show better results consistently.
However, different from theresults shown inRQl, ATS achieved
thebest results under alldataset&DNN model combinations. For
example, asforFashion &LeNet-l, both Gini and Maxp show a
higher fault detection rate than ATS inTab. 4.InRQ2, theRAUC
ofGini and Maxp areonly 79.48% and 81.51%, respectively, while
theRAUC ofATS is91.88%. This phenomenon means that although
prioritization test selection methods can sometimes detect more
faults, thetestcases selected bythese methods may have anuneven
distribution. Inother words, prioritization test selection methods
may prefer aspecific type offault. There does notexist anadaptive
adjustment step forthepriority listtoadjust theweight based on
80Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu,Zhenyu Chen, and Baowen Xu
selected setfeedback. Such selection methods areeasy toselect a
subset with limited fault types, which may have anegative impact
onmodel optimization.
5.3 Answer toRQ3: Optimization Effectiveness
The final goal ofDNN testing istodetect faults and optimize the
DNN model toimprove thegeneralization oftheDNN model. Thus
weevaluate theeffectiveness ofDNN optimization byadding back
valid test data into theoriginal training set.Wecollect theaccu-
racy improvement results offour different selection ratios (2.5%,
5%,7.5%, 10%). The concrete accuracy improvement results canbe
found inTab. 6.
From theperspective ofselectionmethods, theaverage accu-
racy improvement results ofmost selection techniques perform
better than random sampling. Although LSA displays thebest ac-
curacy improvement under thecombination ofMNIST &LeNet-5,
theretraining results under other combinations guided byLSA are
even worse than random sampling thesame size test cases forre-
training. Toevaluate theeffectiveness more precisely, weusethe
Wilcoxon rank-sum test [20] tocheck whether ATS outperforms
other baseline approaches. Weimplement theone-side Wilcoxon
rank-sum testtocheck whether thebaseline method isgreater or
worse than theATS. Ifthep-value isless than 0.05, wereject the
null hypothesis Hoand accept thealternative hypothesis HIthat
onemethod isstochastically greater than another. Otherwise, we
regard there isnosignificant difference between thetwo methods.
The results show that nobaseline exceeds ATS statistically. Only
invery few configurations, there aresome prioritization methods
that show similar performance toATS.
Furthermore, considering theFashion &LeNet-l discussed above,
ATS achieves asignificant retraining improvement over Gini and
Maxp. Such aresult supports ourassumption that, instead ofse-
lecting more faults with less diversity, ATS could select asubset
with enough faults with more fault types, which ismore effective
toenhance theDNN model.
6DISCUSSION
This section further discusses theATS with adaptive random test-
ingand active learning and also demonstrates thethreats tothe
validity ofthispaper.
6.1 The Effectiveness ofATS
From allexperimental results, ATS ismore effective than other
existing test selection methods. Byanalyzing theresults ofother
baselines, weobserve that although structural coverage criteria are
effective inclassic testing applications, neuron coverage criteria
seem ineffective tobeaguide criterion forDNN testselection. Be-
sides, compared with existing prioritization methods, ATS shows a
better andmore stable result.
Similar totheidea ofclassic adaptive random testing (ART) [15,
18], ATS selects thetest case based onthefitness between each
candidate cfrom thecandidate setCagainst theexecuted setE.
However, atthesame time, ART cannot bedirectly applied toDNN
testing. One reason isthat different from thelow dimensional in-
putdomain discussed inART, theinput data dimension oftheDNN
model ispretty large [68]. The curse ofdimensionality lead tothe
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. Adaptive Test Selection forDeep Neural Networks ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA
Table 4:The average fault detection rate foreach configuration.
Selecting 5%tests II Selecting 10% tests
Fault
DataDetect(%) I I Coverage-Guided IPrioritization IIIICoverage-Guided IPrioritization I
Model RS NAC NBC SNAC TKNC Gini CES LSA Maxp ATS RS NAC NBC SNAC TKNC Gini CES LSA Maxp ATS
LeNet5 8.48.8 9.4 8.48.6 23.9 10.6 40.8 24.8 42.5 8.58.7 9.2 8.68.5 23.7 10.6 40.4 24.7 40.9
MNISTLeNetl 9.9 10.1 9.79.7 10.0 36.6 8.8 32.738.0 48.7 9.89.9 9.7 9.8 9.7 36.4 8.7 32.6 37.8 47.0
FashionLeNetl 18.718.7 18.418.5 20.9 53.2 16.1 31.3 54.8 53.2 18.618.618.618.3 20.3 52.5 16.0 31.3 53.9 51.5
ResNet20 18.818.9 19.0 17.6 19.6 53.1 17.9 23.9 52.1 57.4 18.618.7 19.1 18.019.3 52.1 18.0 24.6 51.4 55.7
CIFARVGG16 19.217.019.2 18.5 22.3 50.4 25.9 13.5 51.0 56.4 19.3 17.4 19.018.5 21.6 49.6 25.8 13.6 50.1 55.0
ResNet20 13.5 13.8 14.1 13.6 14.1 43.3 15.1 13.8 43.8 52.4 13.3 13.8 14.013.513.6 42.6 15.1 13.7 43.4 51.2
LeNet5 16.4 15.716.3 16.215.9 53.8 14.9 24.4 54.3 53.1 16.315.7 16.1 15.915.9 52.7 14.8 24.1 53.0 49.4
SVHNVGG16 7.1 7.37.57.2 8.4 28.5 12.5 6.5 30.0 44.3 7.4 7.5 7.8 7.5 8.6 28.9 13.2 6.5 30.9 40.1
-ATO
-Glnl
-CES
-~---"'"-NBC
-SNAC---"-Theo
1Z3 4 5 6 1•
!'eraont4lgeof5elect.dTiKtsl"J-ATO
-Glnl
-CES
-~-,""-""-NBC-'''''-~,-"--12 3 4 5 6 18
Ptlraontag. orS,lectlId T,sts 1'1'1-ATO
-Glnl-'"-~---"'"-,,,-''''''--,-"--123 4 5 6 1•
Perl;enlilgeofSfllectlldTest5I"1!300
!200-ATO
-Glnl-'"-~
-~.-"'"-,,,-''''''-~,-"-Th~
12 3 4 5 6 78
PlIn:.ntilg.or5fl~T~{"'1'"'
~
IZOll
(a)MNIST &LeNet-5 (b)MNIST &LeNet-! (c)Fashion &LeNet-! (d)Fashion &ResNet-20
-ATO
-Glnl
-CES
-~-,""-"'"-NBC-,"'"-~,-"-",",-ATO
-Glnl
-CES
-~---"'"-NBC
-SNAC-"'"-"-Thea-ATO
-Glnl
-CES
-~
-~.
-,~-,,,-....,
-~,-"-Thm-An
-Glnl-'"-~
-~.-"'"-,,,-,""-~,-"-Thm
1Z 3 4 567I
I'ercentageafSelectl!!dnstsl'"12] 4 5 6 18
Pl!!rcenta\Jl!of5electedTestsC'J'l12] 4 5 i 7•
Pl!!rcentageof5eIl!ctedTests(%)12] 4 561I
PercentageofSl!lO!!rtedTesI!I("1
(e)CIFAR&VGG-!6 (f)CIFAR &ResNet-20 (g)SVHN &LeNet-5 (h)SVHN &VGG-!6
Figure 2:The cumulative sum ofthefault types found byspecific test selection methods.
Table 5:When selecting 10% tests, theratio ofarea under the
curve between each selection method totheoretical.
RAVCI MNIST I Fashion I CIFAR I SVHN
(%) LeNet5 LeNetlLeNetl ResNet20 VGG16 ResNet20 LeNet5 VGG16
RS 162.97 65.97 162.8761.25 170.41 63.90 191.34 77.67
NAC 65.13 66.3962.83 61.38 68.1267.3892.16 75.84
NBC 67.74 63.91 57.53 60.52 70.05 62.97 92.1278.15
SNAC 61.9267.06 60.41 61.38 70.08 66.05 88.87 76.42
TKNC 63.95 68.15 65.5863.4277.2062.7091.03 80.57
Gini 55.73 82.57 79.48 89.97 92.66 92.0098.18 88.94
CES 67.82 60.84 62.58 57.41 79.45 65.5790.19 85.37
LSA 80.92 79.18 77.01 60.69 55.60 61.51 89.72 41.15
Maxp 59.75 83.00 81.51 90.0692.5091.34 98.4392.71
ATS 189.32 91.48191.88 92.20 195.6095.43 198.80 97.52invalidation offitness metric. Thus wechoose todesign ATS inthe
output domain. The other reason isthat thetarget between ran-
dom testing and DNN testing isdifferent. ART aims toreduce the
resource cost ofconstructing test cases randomly. Itmeans that
onlyifthecomputational cost ofART ismuch lighter than RT
then wecan consider ART isaneffective and efficient test selec-
tion method [5,60]. However, forDNN testing, thecost ofmanual
labeling isoften extremely expensive, especially when thelabeling
task requires professional knowledge [51]. Inthis case, thecontro-
versy ofcomputational overheads isnolonger essential forDNN
test selection because theresource cost intest execution and data
selection ismuch cheaper incomparison with theconsumption of
manually labeling.
Tofurther discuss thesignificance ofATS forDNN testing, on
thebasis ofRQ3, weobtain theaccuracy improvement when re-
training themodel with thewhole candidate set.First, inRow Ori
Ace, wegive theoriginal accuracy ofeach model onthenewly
81
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. ICSE '22,May 21-29,2022, Pittsburgh, PA,USA Xinyu Gao, Yang Feng, Yining Yin,Zixi Liu,Zhenyu Chen, andBaowen Xu
Table 6:The DNNs' accuracy iInprovement value after retraining with theselected tests.
II I Coverage-Guided IPrioritization III ICoverage-Guided IPrioritization I
Dataset Model ATS NAC NBC SNAC TKNC Gini CES LSA Maxp RS ATS NAC NBC SNAC TKNC Gini CES LSA Maxp RS
LeNet5 2.38 1.39 1.831.601.631.42 1.65 1.42 4.09 2.802.972.70 2.94 2.46 2.73 4.16 2.68 2.55
MNISTLeNetl 2.37 1.57 1.67 1.551.65 1.78 1.08 1.43 3.97 2.97 2.85 2.90 2.81 3.46 2.00 3.313.66 2.51
t" LeNetl 1.53 0.920.79 0.820.91 1.14 0.70 0.84 t"2.82 1.931.661.89 1.95 2.32 1.40 1.92 2.45 1.72
u;FashionResNet20u;
N 3.34 2.86 2.862.83 2.75 3.22 2.57 2.66 ,..;4.96 4.174.304.08 4.15 4.86 3.883.57 4.944.04- VGG16 2.36 1.851.83 1.84 1.98 2.27 1.95 1.86-3.73 2.61 2.672.672.78 3.48 2.75 1.79 3.52 2.65 u u
..!<CIFAR ..!<v ResNet20 0.99 0.45 0.42 0.50 0.31 0.88 0.40 0.38v1.68 0.96 0.95 1.071.03 1.52 1.00 0.55 1.49 0.94 (j) (j)
LeNet5 1.74 0.90 0.95 0.93 0.84 1.49 0.99 1.14 3.17 1.731.901.77 1.81 2.90 1.71 2.03 2.96 1.90
SVHNVGG16 2.09 1.121.171.101.231.521.37 1.09 2.95 1.711.821.75 1.83 2.32 2.12 0.95 2.48 1.73
LeNet5 3.52 2.28 2.432.292.46 1.96 2.26 3.59 2.16 2.08 4.53 3.20 3.31 3.163.38 2.99 2.94
MNISTLeNetl 3.46 2.422.28 2.34 2.31 2.74 1.58 2.882.94 2.05 4.30 3.383.193.383.30 3.95 2.87
:;,FashionLeNetl 2.40 1.48 1.46 1.44 1.631.711.07 1.57 1.96 1.41 t"3.15 2.24 2.192.292.33 2.75 2.11
ResNet20 4.36 3.68 3.663.76 3.74 4.25 3.41 2.96 4.23 3.51 ;:;5.30 4.62 4.784.49 4.72 5.37 4.360VGG16 3.08 2.252.29 2.322.42 2.88 2.42 1.58 2.932.28 "E4.13 2.94 2.91 2.93 3.163.99 2.95 vvCIFARResNet20 3l1.83(j) 1.44 0.75 0.720.91 0.79 1.30 0.79 0.40 1.30 0.72 1.21 1.09 1.211.15 1.69 1.11
LeNet5 2.64 1.33 1.41 1.36 1.35 2.31 1.36 1.67 2.41 1.47 3.59 2.16 2.24 2.072.27 3.47 2.21
SVHNVGG16 2.72 1.471.54 1.55 1.58 1.97 1.82 0.88 2.10 1.49 3.18 1.94 2.02 1.991.97 2.61 1.95
1.Colored baseline blocks represent theaccuracy improvement between ATS andbaseline issimilar statistically, anduncolored blocks represent
ATSisgreater than baseline incorresponding configurations.
2.Foreach selection ratio, webold themaximum accuracy improvement value ofeach DNN &dataset combination.
Table 7:Model optiInization effect compared with retraining
with allcandidate tests.
Dataset IMNIST Fashion CIFAR SVHN
Model LeNet5 LeNetlLeNetl ResNet20 VGG16 ResNet20 LeNet5 VGG16
OdAceI91.01 89.95 79.4079.7878.32 85.35 82.39 92.30
100% testsI6.486.30 5.15 8.42 8.59 2.76 7.25 4.40
10%ATS 4.534.30 3.15 5.30 4.13 1.83 3.59 3.18
Imp% I69.9% 68.3% 61.2%62.9% 48.1% 66.2% 49.5% 72.1%
constructed test set.The accuracy improvement isshown inTab. 7
Row 100% tests. Refer toTab. 6,weshow theaccuracy improve-
ment when use ATS select 10% test cases from thecandidate set,
denoted as10% ATS. And inthelast row, wecalculated theim-
provement ratio of10% ATS to100% tests. The results show that
forDNN testing, anappropriate test case selection method could
optimize theDNN model with amuch lighter labeling cost.
Based ontheexperimental results and discussion, wedraw the
conclusion that theadaptive testselection method ATS could select
test cases from amassive unlabeled dataset automatically. The test
setselected byATS contains numerous faults with diverse types.
From theresults ofRQ3, wefound that such atest setcould en-
hance theDNN effectively and efficiently.
6.2 Comparison with Active Learning
Active learning (AL) isasubfield ofmachine learning (ML) inwhich
alearning algorithm aims toachieve good accuracy with fewer
training samples byinteractively querying theoracles tolabel new
data points [50,66]. Thus, both ALand test case selection attempt
toovercome thelabeling bottleneck byselecting data from anun-
labeled candidate set.
82Although thedesign motivation ofthetwo technologies over-
laps slightly, itisworth emphasizing that there areinherent differ-
ences between test selection and active learning. The kernel pur-
pose ofAListoobtain amodel ofbetter performance with less
ground truth query efforts. However, from theperspective ofDNN-
driven system testing, ATS focuses onthetesting and debugging
process ofapre-trained model, which aims toexpose unpredicted
behaviors and toenhance themodel. Tothis end, weimplement
theresearch experiments offaults detection rate and faults diver-
sity inRQ1&RQ2.
Furthermore, inevaluation, wechoose arepresentative ALmethod,
namely Maxp, asoneofthetestprioritization baseline [19,52]. Itis
almost themost commonly applied uncertainty-sampling strategy
inactive learning [66], which employs thesame way ofATS tomea-
sure input uncertainty. The experimental results show that com-
pared with random sampling, Max-p iseffective asaselection method.
Meanwhile, ATS outperforms Maxp under most ofthemodel-dataset
combinations because wedesign anadaptive selection strategy to
overcome theweakness ofpriority-based selection.
6.3 Threats toValidity
Subject selection. The selection oftraining datasets and DNN
models could beathreat tovalidity. Wealleviate this threat byem-
ploying large-scale datasets and four well-designed DNN models
intheexperiment. Further, foreach studied dataset, weemployed
two DNN models with different numbers ofneurons and architec-
ture toevaluate theperformance ofATS. However, some oftheex-
periment results may notbeperfectly generalized toother datasets
and DNN models.
Data siInulation. Further, weemploy augmented data tosimu-
late theunseen inputs forDNNs, which may cause another threat.
Although theaugmented operators arecommon data noises inthe
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. Adaptive Test Selection forDeep Neural Networks
virtual environment, itisimpossible toguarantee that thedistri-
bution ofthereal unseen input isthesame asoursimulation. Ad-
ditional experiments based onreal unseen inputs needs tobecon-
ducted infuture work.
7RELATED WORKS
This section introduces therelated works ontwo aspects: thetest-
ingofdeep learning systems and adaptive random testing.
7.1 The Testing ofDeep Learning Systems
Tomeasure thetest adequacy ofdeep learning systems, several
testcriteria have been proposed. Peietal.[49] proposed thefirst
white-box testing framework DeepXplore, which aims toidentify
and generate thecorner-case inputs that may induce different be-
haviors over different DNNs. Maetal.[41] further refined theidea
ofcoverage and proposed fine-grained multi-layer testing criteria
toguarantee thequality ofthemodel entirely. Based onthemu-
tation test, Maetal.[29, 42]proposed DeepMutation and Deep-
Mutation++ toevaluate thequality ofdatasets with model-level
mutation operators.
Based ontheabove coverage criteria, different DNN test appli-
cation techniques areproposed. Tian etal.[59]presented DeepTest
togenerate testcases bymaximizing thenumber ofactivated neu-
rons. Similarly, Zhang etal.[67] give anadversarial network to
generate different weather conditions driving scenes toincrease
thediversity ofdatasets.
However, aspointed outintheintroduction section, many exist-
ingtesting techniques give less consideration tothecharacteristic
oftheDNN-driven system, which results inthedifference between
theimprovement oftheDNN system and therepair ofthetradi-
tional software system. This iswhy recent studies [23, 27]argue
theguidance ofexisting testing criteria, especially theneuron cov-
erage criteria. Inaddition, other test case generation techniques
orprioritization techniques aredifficult togive asuitable testing
criterion orarebased onalow-guidance (neuron coverage) test
adequacy criterion. Inthis paper, themethod ATS isintended to
break outofthescope ofneuron coverage design and start from
thesight oftheDNN -driven software system toidentify andselect
testsuites ofhigh model improvement capability.
7.2 Adaptive Random Testing
Test selection isaclassic research topic ofsoftware testing. There
areplenty oftechniques and methods areproposed forconven-
tional software systems.
Among them, one ofthewell-known test selection method is
Adaptive Random Testing (ART). Chen etal.[15]proposed thefirst
specific algorithm ofthismethod (FSCS-ART). The core idea isto
choose anew test case, kcandidates arerandomly generated. For
each candidate Ci,theclosest previously executed test islocated,
andthedistance diisdetermined. Thecandidate with thelargest di
isselected, andtheother candidates arediscarded. Besides, another
testselection technique, Antirandom testing [44] isalso based on
theconcept ofdistance todistribute test cases. Itisalmost deter-
ministic, which means itrequires thenumber oftest cases tobe
chosen inadvance. The core ofthetechnology toimprove random
testing efficiency istoachieve even spread test case distribution.
83ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA
Therefore, anumber ofdifferent methods using different intuitions
toachieve this goal have been investigated intheliterature. Anex-
ample isRestricted Random Testing (RRT) [13], which isbased on
thenotion ofadaptive exclusion, and thegoal ofthismethod isto
select thetest case outside oftheexclusion zones. ART byParti-
tioning[18] uses arather different intuition, partitioning theinput
domain inessence, and allocating test cases evenly topartitions,
achieves even spread. Besides, other attempts totake advantage
offailure region contiguity, butusing various other intuitions to
achieve theeven spreading oftestcases, including Q!1asi-Random
Testing [17], and Lattice-Based ART [45].
Inthetesting ofDNN-driven software systems, itiseasy toget
enough unlabeled testcases. However, choosing proper data tola-
belmanually isalways difficult. Our work aims tohelp testers se-
lectvaluable tests from massive unlabeled tests efficiently.
8CONCLUSION
Inthispaper, wepropose ATSanadaptive testselection method for
Deep Neural Networks. Wedesign afitness computation method
toadaptively determine which test inthecandidate setismore
suitable tobelabeled manually. Theexperimental results inthispa-
perdemonstrate that ATS caneffectively help testers choose more
valuable testcases toimprove thequality ofthemodel. Different
from current testselectionmethods, ATS isguided byfault pattern
design and candidate fitness metric fortestselection ofDeep Neu-
ralNetworks. Weprovide analternative view foridentifying and
selecting thetests. Weexpect that ATS can inspire testers tose-
lect test suites with enough diversity and faults detection ability,
which canefficiently improve thesystem.
ACKNOWLEDGEMENT
Wewould like tothank anonymous reviewers fortheir insight-
fuland constructive comments. This project was partially funded
bytheNational Natural Science Foundation ofChina under Grant
Nos. 62002158, 61832009, and 61932012, and theScience, Technol-
ogy and I=ovation Commission ofShenzhen Municipality (No.
CJGJZD202006171 0300 1003).
REFERENCES
[1][n.d.]. Amazon promises fixforcreepy Alexa laugh -BBC News. https://www.
bbc.comlnews/technology-43325230. (Accessed on08/23/2021).
[2][n.d.]. AGoogle self-driving carcaused acrash forthefirst time -The
Verge. https://www.theverge.coml2016/2/29/11134344/google- self- driving- car-
crash-report. (Accessed on09/04/2021).
[3][n.d.]. Tesla' sLatest Autopilot Death Looks Just Like aPrior Crash
IWIRED. https://www.wired.comlstory/teslas-Iatest-autopilot-death-looks-
like-prior-crash!. (Accessed on09/04/2021).
[4]Paul Eric Ammann andJohn CKnight. 1988. Data diversity: Anapproach to
software fault tolerance. Ieee transactions oncomputers 37,4 (1988), 418-425.
[5]Saswat Anand, Edmund KBurke, Tsang Yueh Chen,]ohn Clark, Myra BCohen,
Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, Phil McMinn, Antonia
Bertolino, etal.2013. Anorchestrated survey ofmethodologies forautomated
software testcase generation. Journal ofSystems and Software 86,8 (2013), 1978-
2001.
[6]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma-
chine translation byjointly learning toalign and translate. arXiv preprint
arXiv:1409.0473 (2014).
[7]Arlinta CBarns, Tsang Yueh Chen, Fei-Ching Kuo, Huai Liu,Robert Merkel, and
Gregg Rothermel. 2016. Acost-effective random testing method forprograms
with non-numeric inputs. IEEE Trans. Comput. 65,12 (2016), 3509-3523.
[8]Peter GBishop. 1993. Thevariation ofsoftware survival time fordifferent oper-
ational input profiles (orwhy you canwait along time forabigbug tofail). In
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. ICSE '22,May 21-29, 2022, Pittsburgh, PA,USA
FJCS-23 TheTwenty-Third International Symposium onFault-Tolerant Computing.
IEEE, 98-107.
[9]MariuszBojarski, Davide DelTesta, Daniel Dworakowski, Bernhard Firner, Beat
FIepp, Prasoon Goyal, Lawrence DJackel, Mathew Monfort, UrsMuller, Jiakai
Zhang, etal.2016. End toend learning forself-driving cars. arXiv preprint
arXiv:1604.07316 (2016).
[10] Otto Bretscher. 1997. Linear algebra with applications. Prentice Hall Eaglewood
Cliffs, N].
[11] Paulo MSBueno, Mario Jino, andWEric Wong. 2014. Diversity oriented test
data generation using metaheuristic search techniques. Information Sciences 259
(2014),490-509.
[12] FTChan, Tsong Yueh Chen, IKMak, andYuen-Tak Yu.1996. Proportional sam-
pling strategy: guidelines forsoftware testing practitioners. Information and
Software Technology 38,12(1996), 775-782.
[13] KpChan, TyChen, and DTowey. 2006. Restricted random testing: Adaptive
random testing byexclusion. International Journal ofSoftware Engineering &
Knowledge Engineering 16,4(2006),553-584.
[14] Junjie Chen, Ming Yan, Zan Wang, Yuning Kang, and Zhuo Wu. 2020. Deep
neural network test coverage: How fararewe? arXiv preprint arXiv:201O.04946
(2020).
[15] Tsong Yueh Chen, Hing Leung, andI.K.Mak. 2004. Adaptive Random Testing.
(2004).
[16] Tsong Yueh Chen, RMerkel, PKWong, andGEddy. 2004. Adaptive random test-
ingthrough dynamic partitioning. InFourth International Conference onQuality
Software, 2004. QSIC 2004. Proceedings. IEEE, 79-86.
[17] Tsong Yueh Chen and Robert G.Merkel. 2007. Q1lasi-Random Testing. IEEE
Transactions onReliability 56(2007),562-568.
[18] T.Y.Chen,R. G.Merkel, P.K.Wong, andG.Eddy. 2004. Adaptive random testing
through dynamic partitioning. InQuality Software, 2004. QSIC 2004. Proceedings.
Fourth International Conference on.
[19] Aron Culotta and Andrew McCallum. 2005. Reducing labeling effort forstruc-
tured prediction tasks. InAAAI, Vol. 5.746-751.
[20] Jack Cuzick. 1985. AWilcoxon-type testfortrend. Statistics inmedicine 4,1
(1985),87-90.
[21] LDeCapitani andDDeMartini. 2011. Onstochastic orderings oftheWilcoxon
rank sum teststatistic-with applications toreproducibility probability estima-
tion testing. Statistics &probability letters 81,8(2011), 937-946.
[22] Daniel DiNardo, Nadia Alshahwan, Lionel Briand, and Yvan Labiche. 2013.
Coverage-based test case prioritisation: Anindustrial case study. In2013 IEEE
Sixth International Conference onSoftware Testing, Verification and Validation.
IEEE, 302-311.
[23] Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, and Zhenyu
Chen. 2020. DeepGini: prioritizing massive tests toenhance therobustness of
deep neural networks. InProceedings ofthe29thACM SIGSOFJlnternational Sym-
posium onSoftware Testing and Analysis. 177-188.
[24] George BFinelli. 1991. NASA software failure characterization experiments.
Reliability Engineering &System Safety 32,1-2(1991), 155-169.
[25] Abraham Adolf Fraenkel, Yehoshua Bar-Hillel, and Azriel Levy. 1973. Founda-
tions ofsettheory. Elsevier.
[26] Jianmin Guo, YuJiang, Yue Zhao, Q1lan Chen, andJiaguang Sun. 2018. DLFuzz:
differential fuzzing testing ofdeep learning systems. InProceedings ofthe2018
26th ACM Joint Meeting onEuropean Software Engineering Conference and Sym-
posium ontheFoundations ofSoftware Engineering. 739-743.
[27] Fabrice Harel-Canada, Lingxiao Wang, Muhammad AliGuIzar, Q1lanquan Gu,
and Miryung Kim. 2020. Isneuron coverage ameaningful measure fortesting
deep neural networks? InProceedings ofthe28thACM Joint Meeting onEuropean
Software Engineering Conference and Symposium ontheFoundations ofSoftware
Engineering. 851-862.
[28] Kaiming He,Xiangyu Zhang, Shaoqing Ren, andJian Sun. 2016. Deep residual
learning forimage recognition. InProceedings oftheIEEE conference oncomputer
vision andpattern recognition. 770-778.
[29] Qiang Hu,LeiMa,Xiaofei Xie, BingYu, Yang Liu,andJianjun Zhao. 2019. Deep-
Mutation++: AMutation Testing Framework forDeep Learning Systems. In
2019 34th IEEE/ACM International Conference onAutomated Software Engineer-
ing(ASE). IEEE, 1158-1161.
[30] Gunel Jahangirova and Paolo Tonella. 2020. AnEmpirical Evaluation ofMuta-
tion Operators forDeep Learning Systems. In2020 IEEE 13th International Con-
ference onSoftware Testing, Validation and Verification (ICST).
[31] BoJiang, Zhenyu Zhang, Wing Kwong Chan, andTHTse. 2009. Adaptive ran-
dom testcase prioritization. In2009 IEEE/ACM International Conference onAuto-
mated Software Engineering. IEEE, 233- 244.
[32] Jinhan Kim, Robert Feldt, andShin Yoo. 2019. Guiding Deep Learning System
Testing Using Surprise Adequacy. In2019 IEEE/ACM 41st International Confer-
ence onSoftware Engineering (ICSE).
[33] A.Krizhevsky and G.Hinton. 2009. Learning multiple layers offeatures from
tiny images. Handbook ofSystemic Autoimmune Diseases 1,4(2009).
[34] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2009. CIFAR1O. https:
//www.cs.toronto.edu/~kriz/cifar.html.
84Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu,Zhenyu Chen, and Baowen Xu
[35] Alexey Kurakin, IanGoodfellow, Samy Bengio, etal.2016. Adversarial examples
inthephysical world.
[36] Hugo Larochelle, Yoshua Bengio, J<'r6me Louradour, andPascal Lamblin. 2009.
Exploring strategies fortraining deep neural networks. Journal ofmachine learn-
ingresearch 10,1(2009).
[37] Yann LeCun, Leon Bottou, Yoshua Bengio, andPatrick Haffner. 1998. Gradient-
based learning applied todocument recognition. Proc. IEEE 86,11 (1998), 2278-
2324.
[38] David DLewis andWilliam AGale. 1994. Asequential algorithm fortraining
text classifiers. InSIGIR' 94.Springer, 3-12.
[39] Zenan Li,Xiaoxing Ma, Chang Xu,and Chun Cao. 2019. Structural coverage
criteria forneural networks could bemisleading. In2019 IEEE/ACM 41st Interna-
tional Conference onSoftware Engineering: New Ideas andEmerging Results (ICSE-
NIER). IEEE, 89-92.
[40] Zenan Li,Xiaoxing Ma, Chang Xu,Chun Cao, Jingwei Xu,andJian Lii.2019.
Boosting operational DNN testing efficiency through conditioning. InProceed-
ingsofthe2019 27th ACM Joint Meeting onEuropean Software Engineering Con-
ference and Symposium ontheFoundations ofSoftware Engineering. 499-509.
[41] LeiMa, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, BoLi,Chun-
yang Chen, Ting Su,LiLi,Yang Liu, etal.2018. Deepgauge: Multi-granularity
testing criteria fordeep learning systems. InProceedings ofthe33rd ACMlIEEE
International Conference onAutomated Software Engineering. 120-131.
[42] LeiMa, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, BoLi,Felix Juefei-Xu, Chao
Xie, LiLi,Yang Liu,Jianjun Zhao, etal.2018. Deepmutation: Mutation testing
ofdeep learning systems. In2018 IEEE 29th International Symposium onSoftware
Reliability Engineering (ISSRE). IEEE, 100-111.
[43] Wei Ma,Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, andYves LeTraon.
2021. Test selection fordeep learning systems. ACM Transactions onSoftware
Engineering and Methodology (TOSEM) 30,2 (2021), 1-22.
[44] Y.K.Malaiya. 1995. Antirandom testing: getting themost outofblack-box test-
ing.InInternational Symposium onSoftware Reliability Engineering.
[45] Johannes Mayer. 2005. Lattice-based adaptive random testing. InIEEE/ACM In-
ternational Conference onAutomated Software Engineering.
[46] Agnieszka Mikolajczyk andMichal Grochowski. 2018. Data augmentation for
improving deep learning inimage classification problem. In2018 international
interdisciplinary PhD workshop (IIPhDW). IEEE, 117-122.
[47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, BoWu, and An-
drew YNg. 2011. Reading digits innatural images with unsupervised feature
learning. (2011).
[48] Nicolas Papernot, Patrick McDaniel, SomeshJha, Matt Fredrikson, ZBerkay Ce-
lik,andAnanthram Swami. 2016. Thelimitations ofdeep learning inadversarial
settings. In2016 IEEE European symposium onsecurity andprivacy (EuroS&P).
IEEE, 372-387.
[49] Kexin Pei,Yinzhi Cao, Junfeng Yang, andSuman Jana. 2017. DeepXplore: Auto-
mated White boxTesting ofDeep Learning Systems. Getmobile Mobile Comput-
ing&Communications 22,3(2017).
[50] Burr Settles. 2009. Active learning literature survey. (2009).
[51] Burr Settles. 2011. From Theories toQ1leries: Active Learning inPractice. In
Active Learning andExperimental Design workshop Inconjunction with AISTATS
2010 (Proceedings ofMachine Learning Research), Isabelle Guyon, Gavin Cawley,
Gideon Dror, Vincent Lemaire, andAlexander Statnikov (Eds.), Vol. 16.PMLR,
Sardinia, Italy, 1-18. https://proceedings.mlr.press/v16/settles11a.html
[52] Burr Settles and Mark Craven. 2008. Ananalysis ofactive learning strategies
forsequence labeling tasks. InProceedings ofthe2008 Conference onEmpirical
Methods inNatural Language Processing. 1070-1079.
[53] Connor Shorten andTaghi MKhoshgoftaar. 2019. Asurvey onimage data aug-
mentation fordeep learning. Journal ofBigData 6,1(2019), 1-48.
[54] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional net-
works forlarge-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[55] Satinder PSingh, Michael] Kearns, Diane JLitman, andMarilyn AWalker. 2000.
Reinforcement learning forspoken dialogue systems. InAdvances inNeural In-
formation Processing Systems. 956-962.
[56] Gilbert Strang, Gilbert Strang, Gilbert Strang, andGilbert Strang. 1993. Introduc-
tion tolinear algebra. Vol. 3.Wellesley-Cambridge Press Wellesley, MA.
[57] Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill,
andRob Ashmore. 2019. DeepConcolic: Testing anddebugging deep neural net-
works. In2019 IEEE/ACM 41st International Conference onSoftware Engineering:
Companion Proceedings (ICSE-Companion). IEEE, 111-114.
[58] Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill,
andRobAshmore. 2019. TestingDeep Neural Networks. arXiv:cs.LG/1803.04792
[59] Yuchi Tian, Kexin Pei,Suman Jana, and Baishakhi Ray. 2017. DeepTest: Auto-
mated Testing ofDeep-Neural-Network-driven Autonomous Cars. (2017).
[60] Dave Towey. 2007. Adaptive random testing; ubiquitous testing tosupport ubiq-
uitous computing. InProceedings oftheKorea Society ofInformation Technology
Applications Conference. TheKorea Society ofInformation Technology Applica-
tions, 138-138.
[61] LeeJWhite andEdward ICohen. 1980. Adomain strategy forcomputer program
testing. IEEE transactions onsoftware engineering 3(1980), 247-257.
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. Adaptive Test Selection forDeep Neural Networks
[62] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST:
aNovel Image Dataset for Benchmarking Machine Learning Algorithms.
arXiv:cs.LG/cs.LG/1708.07747
[63] Xiaofei Xie, LeiMa, Felix ]uefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu,]ian-
junZhao, BoLi,]ianxiong Yin, andSimon See. 2019. DeepHunter: acoverage-
guided fuzz testing framework fordeep neural networks. InProceedings ofthe
28th ACM SIGSOFJ International Symposium onSoftware Testing andAnalysis.
146-157.
[64] Christopher].C. Burges Yann LeCun, Corinna Cortes. 1998. MNIST. http://yann.
lecun.comlexdb/mnist!.
[65] Shin Yoo andMark Harman. 2012. Regression testing minimization, selection
and prioritization: asurvey. Software testing, verification and reliability 22,2
85leSE '22,May 21-29,2022, Pittsburgh, PA,USA
(2012),67-120.
[66] Xueying Zhan, Huan Liu, Qing Li,andAntoni BChan. [n.d.]. AComparative
Survey: Benchmarking forPool-based Active Learning. ([n.d.]).
[67] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, andSarfraz Khur-
shid. 2018. DeepRoad: GAN-based metamorphic testing and input validation
framework forautonomous driving systems. In2018 33rd IEEE/ACM Interna-
tional Conference onAutomated Software Engineering (ASE). IEEE, 132-142.
[68] Arthur Zimek, Erich Schubert, andHans-Peter Kriegel. 2012. Asurvey onunsu-
pervised outlier detection inhigh-dimensional numerical data. Statistical Anal-
ysisandData Mining: TheASA Data Science Journal 5,5(2012), 363-387.
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:50:18 UTC from IEEE Xplore.  Restrictions apply. 