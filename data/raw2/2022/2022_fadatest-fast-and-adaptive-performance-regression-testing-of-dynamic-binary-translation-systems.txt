FADATest: Fast and Adaptive Performance Regression Testing of
Dynamic Binary Translation Systems
Jin Wu
Harbin Institute of Technology
ChinaJian Dong
Harbin Institute of Technology
ChinaRuili Fang
University of Georgia
USA
Wen Zhang
University of Georgia
USAWenwen Wang
University of Georgia
USADecheng Zuo
Harbin Institute of Technology
China
ABSTRACT
Dynamic binarytranslation (DBT) is thecornerstone of many im-
portant applications. In practice, however, it is quite difficult to
maintain the performance efficiency of a DBT system due to its
inherentcomplexity.Althoughperformanceregressiontestingisaneffectiveapproachtodetectpotentialperformanceregressionissues,
itisnoteasytoapplyperformanceregressiontestingtoDBTsys-
tems, because of the natural differences between DBT systems and
common software systems and the limited availability of effective
testprograms.Inthispaper,wepresent FADATest ,whichdevises
severalnoveltechniquestoaddressthesechallenges.Specifically,
FADATest automatically generates adaptable test programs from
existingrealbenchmarkprogramsofDBTsystemsaccordingtothe
runtime characteristics of the benchmarks. The test programs can
thenbeusedtoachievehighly efficientandadaptiveperformance
regression testing of DBT systems. We have implemented a proto-
typeofFADATest .Experimentalresultsshowthat FADATest can
successfullyuncoverthesameperformanceregressionissuesacross
the evaluated versions of two popular DBT systems, QEMU and
Valgrind, as the original benchmark programs. Moreover, the test-
ing efficiency is improved significantly on two different hardwareplatforms powered by x86-64 and AArch64, respectively.
CCS CONCEPTS
â€¢Software and its engineering â†’Source code generation ;
Software performance; Simulator / interpreter.
KEYWORDS
Performance regression testing, DBT, Test program generation
ACM Reference Format:
JinWu,JianDong,RuiliFang,WenZhang,WenwenWang,andDecheng
Zuo.2022. FADATest :FastandAdaptivePerformanceRegressionTesting
ofDynamicBinaryTranslationSystems.In 44thInternationalConferenceon
Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM,
New York, NY, USA, 13pages.https://doi.org/10.1145/3510003.3510169
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35101691 INTRODUCTION
Dynamic binary translation (DBT) is a key enabling technology
for many important applications, such as whole program analy-
sis[6,33],hardwaresimulation[ 21,32],heterogeneouscomputa-
tion offloading [ 38], and software fuzz testing [ 9,23]. In essence, a
DBTsystemdynamicallytranslatesexecutablebinarycodefrom
aguestinstructionsetarchitecture(ISA)toa hostISA,whichcan
bedifferentfromorsameastheguestISA.Byexecutingthegen-
eratedhost codeon aphysical hostmachine, theDBT systemcan
eitheremulatethesemanticsoftheguestapplicationorenhance
its functionality, e.g., execution tracing for performance analysis.
DespitethevitalimportanceofDBT,itisstillquitechallengingto
developandmaintainan efficientDBTsysteminpractice.Typically,
thetranslationprocessinaDBTsystemmapsaguestinstruction
into one or more host instructions, which together emulate the se-
mantics of the guest instruction. Given the semantic gaps between
differentISAs,thetranslatedhostbinarycodeoftensuffersfrom
significant code explosion. For example, an x86-64 instruction may
be translated into dozens of AArch64 instructions due to the differ-
encesbetweenthetwoISAs.Therefore,ingeneral,theperformance
of a guest application running with DBT is remarkably worsethan
itsnativeperformance.Besides,tosupportcodetranslationsacross
variousISAsinonesystem,DBTdevelopersusuallyhavetocreatea
hugecodebase.Evenworse,tokeeppacewiththerapidlyevolving
hardware architectures,existing DBT systemsneed to beupdated
frequently to support emerging machine instructions. These fac-tors inevitably render the inherent difficulty of maintaining the
performance efficiency of a DBT system.
Togiveanexample,QEMU[ 3]isawell-knownDBTsystemand
has been widely used in many research projects and real-world
products[ 29].Ithasaround2.8Mlinesofsourcecode.Everyday,
anaverageof10commitsareaddedtothecodebasebydifferent
developers, embodying 25 source lines revised in each commiton average. As a consequence, a tiny and seemingly innocuous
modificationtothelargecodebasemayintroduceanunexpected
impactontheperformanceofthesystem.Indeed,wehaveobserved
multiple performance regression issues between two successive
releaseversions,whichencloseanaverageof 2188codecommits.
This apparently makes it quite difficult and time-consuming to
debug and fix the performance issues.
Therefore, in this paper, we advocate that it is necessary and im-
perative to conduct performance regression testing during the daily
developmentofaDBTsystem.ThiswillallowDBTdevelopersto
notice unexpected performance regressions at a very early stage
8962022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng Zuo
andthusreducethetremendousengineeringeffortneededtofixthe
problems. Performance regression testing has been demonstrated
tobeapracticalapproachtodetectpotentialperformanceinconsis-
tencies between two different versions of a software system. It has
beenextensivelyadoptedduringsoftwaredevelopmentcyclesin
commercialcompanies [ 1,11,17],and alargeamount ofresearch
work has been devoted to studying and enhancing performance
regression testing [8, 13,14,16,22,24â€“27,30,35].
However, unfortunately, performance regression testing of DBT
systems faces several unique challenges. First, the performance of
a DBT system is usually evaluated using industry-standard and
classicalbenchmarksuites,suchas SPECCPU2017[34]andPAR-
SEC[5].Duetotheaforementionedperformanceoverheadincurred
byDBT,ittakesan extremelylong time,evenwithmostrecentDBT
optimizationsapplied[ 15,36,37,40â€“42],tocompletethetestingof
anentirebenchmarksuite.Forinstance,QEMUneedsmorethan
three hours to finish the execution of a singlebenchmark program
in SPEC CPU 2017. Note that, although these benchmark suites are
shipped with input data sets in different scales, the smaller data
sets are primarily used for correctness verification rather than per-
formance testing. Second, different from most software systems, a
DBTsystemtakesasinput executablebinarycode insteadofregular
program data. This makes it extraordinarily difficult to generate
effectivetestinputsforperformanceregressiontesting,aseachtest
inputneedstobeabinarycodeinguestISAcompiledfromatest
program.Simplyusingartificialtestprogramswouldnothelpbe-
causetheywill notbeabletoexposethesameperformanceissuesas
real benchmark programs. Last but not least, DBT systems usually
runondiverseplatforms,rangingfromservers,todesktops,mobile
and embedded devices. Given the dramatically different computing
power ofthese platforms, it isobviously unreasonable touse a set
offixedtestprogramstoconductperformanceregressiontesting
ofaDBTsystemonalloftheseplatforms,becausethismayleadto
either inaccurate testing results on high-performance platforms or
poor testing efficiency on low-power platforms.
To address the above challenges and make DBT performance
regression testing possible and practical, we propose FADATest in
thispaper. FADATest aimstorealize fastandadaptiveperformance
regression testing of DBT systems. To this end, FADATest first
intelligently captures runtime characteristics of real benchmarkprograms through dynamic program profiling. Based on the col-
lected information, FADATest then automatically generates adapt-
abletestprogramstopreservethecharacteristicsofthebenchmark
programs. The generated test programs are able to accurately sim-
ulate the behaviors and performance results of original benchmark
programs,whiletheexecutiontimesofthetestprogramsaresig-
nificantly shorter. Therefore, the test programs can replace the
original benchmark programs to achieve efficient performance re-
gression testing of DBT systems. Furthermore, the test size of each
testprogramgeneratedby FADATest canbeeasilyscaledup/down
on demand to fit the target hardware platform of a DBT system.
We have implemented a research prototype of FADATest .T o
evaluate the effectiveness of FADATest , we employ two major per-
formancebenchmarksuitesofDBTsystems,SPECCPU2017and
PARSEC. Specifically, we utilize both the original benchmark pro-
grams and the generated test programs to test the performance of
twowidely-usedDBTsystems,QEMUandValgrind,acrossdifferentreleaseversions.Experimentalresults showthattheperformance
resultsofthegeneratedtestprogramsstronglymatchwiththoseof
the original benchmark programs. In other words, all performance
regression issues in the evaluated versions that can be detected by
the original benchmark programs are also successfully discovered
by the test programs generated by FADATest . More importantly,
the testing efficiency is enhanced significantly, compared to the
original benchmark programs. This demonstrates the capability of
FADATest to conduct efficient performance regression testing of
DBT systems. In addition, the evaluation results on a low-power
AArch64 platform show that the adaptability of the generated test
programs allows FADATest to achieve the high testing efficiency
on this hardware platform without loss of testing accuracy.
In summary, this paper makes the following contributions:
â€¢We present FADATest , whichintegrates noveltechniques to
realizeefficientandadaptiveperformanceregressiontestingofDBTsystems.
FADATest automaticallygeneratestestpro-
gramsforperformanceregressiontestingaccordingtothe
dynamic characteristicsof real benchmarks.
â€¢Weimplementaresearchprototypeof FADATest .Thepro-
totype supports two mainstream ISAs on the market, x86-64
andAArch64.Ourimplementationalsoovercomesseveral
technical obstacles caused by executing special hardware
instructions in the generated test programs.
â€¢Weconductcomprehensiveexperimentstoevaluatetheef-
fectivenessof FADATest .Theevaluationresultsshowthat
FADATest can successfully reveal performance regression
issuesofDBTsystems.Comparedtotheoriginalbenchmark
programs, the testing efficiency is significantly improved.
2 BACKGROUND AND MOTIVATION
How DBT Works? Ingeneral,a DBTsystemtranslatesguestbi-
nary code at the basic block (or block) granularity. Each block con-
tains a sequence of guest instructions with at most one branchinstruction at the end of the block. That is, a block has only one
entryandoneexitandtheexecutionofablockissequential.The
translated host binary code is saved to a software code cache to
mitigate the translation overhead as a block may be executed mul-
tipletimesinthesameexecution.Oncethetranslationofablock
is completed, the execution flow is transferred to the code cache
so that the translated host binary code can be executed. Therefore,
the execution of a DBT system is typically interleaved by the code
translation and the execution of the translated host binary code,
until all dynamically discovered blocks are translated.
HighPerformanceOverheadofDBT. Due to the significant se-
manticgapsbetweendifferentISAs,thetranslatedhostbinarycode
oftensuffersfromsubstantialcodeexplosion,whichundoubtedly
leadstoheavyperformanceoverhead.Togiveanexample,Figure 1
showsthenormalizedexecutiontimesofQEMU(version5.0.0)and
Valgrind (version 3.16.0) with benchmark programs in PARSEC
ontwodifferenthardwareplatforms:x86-64andAArch64.More
detailsabouttheconfigurationofthetwoplatformscanbefound
in Section 5. The performance baseline is the execution time of the
corresponding benchmarks running natively on the x86-64 plat-
form. As we can see from the figure, QEMU introduces an average
of 6Ã—and 36Ã—performance slowdown for the x86-64 and AArch64
897
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FADATest ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
 1 4 16 64 256 1024 4096
blackscholesbodytrackcannealdedup
fluidanimatefreqmine
streamclusterswaptionsvipsgmeanNormalized Execution Timex86-64-native
AArch64-native
Valgrind-x86-64QEMU-x86-64
QEMU-AArch64
Figure1:NormalizedexecutiontimesofQEMUandValgrind
onx86-64andAArch64platforms.Thebaselineisthenativebenchmark execution time on the x86-64 platform.
platforms,respectively.Similarly,Valgrindintroducesanaverage
of28Ã—performanceslowdownonthex86-64platform.Wefailed
to collect the data of Valgrind on the AArch64 platform due to the
unbearably long execution time. This result demonstrates the slow
executionofprogramsrunningatopaDBTsystem.Fromthisfigure,
wecanfurtherconcludethattheperformanceofthebenchmarksis
much worse on the AArch64 platform (i.e., QEMU-AArch64). This
is mainly because the computing power of the AArch64 processoris much lower than the x86-64 processor on our platforms. On the
other side, this shows the difficulty to achieve a similar testing
efficiencywhenusingthesameprogramstotesttheperformance
of a DBT system on different hardware platforms.
PerformanceRegressions ofDBT. Given the inherent complex-
ity of modern ISAs, developers have to manually create a huge
code base to support various guest and host instructions in a DBT
system.Forexample,oneofthesourcefilesrelatedtotranslating
AArch64instructionsinQEMUcontainsaround15Klinesofsource
code.Besides,toimprovethetranslationquality,aswellassupport
emerginghardwareinstructions,suchassingleinstructionmultiple
data(SIMD)instructions,frequentchangestothesourcecodeare
very commonduring the development and maintenanceprocess.
This unavoidably leads to performance variance across different
versions of a DBT system. For example, Figure 2illustrates the
performance results of different release versions of QEMU withthe
swaptions benchmark in PARSEC under different numbers
of threads. As shown in the figure, the performance of QEMU is
not consistent across different versions, e.g., the performance is de-
creased significantly from 2.12.1 to 3.0.0. Though the performance
is increased later on in 4.0.1, it is not clear whether the previous
performancedegradationisfixedornot.Therefore,inordertomon-
itor and maintain the performance efficiency of a DBT system, it is
necessary and urgent to conduct performance regression testing
during the daily development of the DBT system.
3 FADATEST
In general, there are several principles for designing a practical
performance regression testing approach for DBT systems:
â€¢High Testing Efficiency . Given the heavy performance
overhead incurred by DBT systems, the testing time should 0 1 2 3 4
2.3.0 2.5.1 2.9.1 2.12.1 3.0.0 4.0.1 4.1.1 4.2.0 5.0.0 5.2.0Normalized Execution Time1-thread
2-thread4-thread
8-thread16-thread
Figure 2: Normalized execution times of different versionsof QEMU with the swaptions benchmark in PARSEC under
different numbers of threads. The baseline is the executiontime of QEMU (version 2.3.0) of each thread configuration.
be sufficiently short. This allows the testing approach tobe used during the daily development of a DBT system to
rapidly discover performance regression issues.
â€¢High Testing Accuracy . The testing approach is expected
to have a strong capability to accurately uncover poten-
tialperformanceregressionsofaDBTsystem.Thismeans
representativeDBTperformancebenchmarksuitescanbe
replaced to avoid extremely long testing times.
â€¢High Testing Adaptability . Since a DBT system may run
on a broad range of hardware platforms, it is necessary and
requiredthatthetestingapproachcanbeadaptedseamlesslytotheactualtargetplatformoftheDBTsystemwithoutloss
of the high testing efficiency and accuracy.
â€¢Low Manual Effort . It is typically not acceptable if the
testingapproach needs toputadditional engineeringeffort
onDBTdevelopers.Moreover,thetestingapproachshould
beveryeasytouseandhelpfulsothatDBTdeveloperswould
like to adopt it in their development cycles.
The key to realize practical performance regression testing of
DBTsystemsistogenerateshortyeteffectivetestprograms,whichcanbeusedtotestDBTperformanceregularly.AnaÃ¯veapproachistorandomlysampletheexecutionofaselectedbenchmarkprogram
to collect a list of basic blocks and then assemble them together to
form atest program.Though thisapproach canpotentially shorten
the testing time, it does not comply with the above principles. For
example,itishardtodetermineanappropriatesamplingratesothatthegeneratedtestprogramcanachieveacceptabletestingefficiencyandaccuracy.Also,thetestprogramsgeneratedusingthisapproach
arefixedandthushardtobescaledup/downtofitdifferenthard-
ware platforms. In contrast, the design of FADATest is compliance
with these principles. It combines several novel techniques to real-
izefast,accurate,andadaptiveperformanceregressiontestingof
DBTsystems.Next,wepresentahigh-leveloverviewof FADATest ,
followed by a detailed description of each component.
3.1 System Overview
FADATest takes several steps to generate a test program from a
seed DBT performance benchmark program. The seed benchmark
programmaycomefromanexistingbenchmarksuite,e.g.,SPEC
898
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng Zuo
6HHG%HQFKPDUN
3URJUDP
([HFXWLRQ
3URILOLQJ7HVW
3URJUDP
6FDOLQJ8S 6FDOLQJ'RZQ
+LJK3HUIRUPDQFH
&RPSXWHU
(PEHGGHG
/RZ3RZHU'HYLFH%ORFN
+RWQHVV7KUHDG
$FWLYLWLHV
'LVWULEXWLRQ3DWWHUQ
$QDO\VLV7HVW3URJUDP
*HQHUDWLRQ
'%7
6\VWHP'%7
6\VWHP
7HVWLQJ 7HVWLQJ
Figure 3: System overview of FADATest.
CPU 2017 or PARSEC, or provided by DBT developers with spe-
cificperformancetestingpurposes.Figure 3showsthehigh-level
workflow of FADATest.
As shown in the figure, FADATest first collects runtime charac-
teristics of the seed benchmark program through online execution
profiling. This enables FADATest to understand the dynamic be-
havior of the seed program. Next, FADATest performs an in-depth
analysis on the gathered information to determine the distribu-
tion patterns of the program. This allows FADATest to generate
adistributionpatterntosummarizehowthebasicblocksaredis-tributed across different threads and how the number of threads
influencessuchadistribution.Withthisdistributionpattern,the
finalstepof FADATest istogeneratethetestprogram.Morespecifi-
cally,FADATest includesallblocksthatmayaffecttheperformance
oftheDBTsysteminthetestprogram.Besides,thetestprogram
isparameterizedintwodimensions.First,thedynamicexecution
counters of blocks can be adjusted to increase/decrease the testing
time.Second,thenumberofthreadscanbechangedtotestspecificthreadsettingsoftheDBTsystem.Bothofthemcanbeeasilytuned
through the options of the test program provided to users. This
way, the generated test programs can test DBT performance on
different hardware platforms, as shown in the figure.
3.2 Program Execution Profiling
In theory, we can extract a test program statically from a seed
benchmarkprogram,ateithersourcecodelevelorbinarycodelevel.However,thisapproachmayleadto inaccurate testingresultsdueto
the lack of the important dynamic execution characteristics of the
seedbenchmarkprogram.Toavoidthisproblem,thefirststepof
FADATest is to collect the execution profile of the seed benchmark
program. To this end, FADATest executes the seed program with
thestandardinputdatasetprovidedbythebenchmarksuite.For
example, FADATest uses thereference input to collect the runtime
characteristics of benchmark programs in SPEC CPU 2017. Note
that SPEC CPU 2017 also provides the testandtraininputs, which
are smaller than reference, but they are generally not intended for
performancetestingandthusnotconsideredby FADATest .During
the execution of the seed program, FADATest collects two major
types of profiling information: block hotness andthread activities.7

07

:7

,7

07

:7

:7

,7

07

:7

:7

:7

:7

,QXPBWKUHDGV   QXPBWKUHDGV   QXPBWKUHDGV  
Figure4:Theactualnumberofthreadscreatedbythebench-mark bodytrack in PARSEC is different from the number
of threads specified by the input parameter ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ .M :
main thread, W: worker thread, I: I/O thread.
Block Hotness Profiling.
Here, the â€œhotnessâ€ of a basic block
meansthe timestheblockis executeddynamicallyunderthe pro-
videdinputdataset.Thereasonwhy FADATest collectsthehotness
information at the basic block level is because basic block is the
translationunitofDBTsystems.A hotbasicblocktypicallyimplies
that the block is executed frequently and thus more likely to ac-
count for a higherproportion of the entire execution time of the
DBT system, compared to a coldblock.FADATest creates a hotness
counterfor every basic block translated by the DBT system and
increasesthecounterbyoneeachtimewhenthetranslatedhostbi-narycodeoftheblockisexecuted.Notethatthehotnesscounteris
thread private, which means each thread has its own block hotness
data. This allows FADATest to perform a thread-aware distribution
pattern analysis in the following step.
Thread Activity Profiling. Thread activity information includes
thespecifictimepointsatwhichathreadiscreatedandterminated.Insteadofusingtheabsolutetime,
FADATest employsrelativetimes,
whicheliminateanypotentialinaccuraciescausedbyuncertainty
factors,suchasthreadsynchronizationandscheduling.Specifically,
when a thread is created/terminated, FADATest marks down the
correspondingexecutionpointofitsparentthread,i.e.,thenumber
of basic blocks that have been executed in the parent thread. With
this thread activity information, the test program generated by
FADATest is able to exhibit a similar thread behavior to the seed
program. This is important as the generated test program maybe used to test DBT performance with various thread settings,
whichcannotbeprofiled completely inadvance.Forexample,atest
program generated based on the execution profiles of 1, 2, and 4
threads may be used to test the performance of 8 threads or more.
3.3 Distribution Pattern Analysis
The purpose of the distribution pattern analysis is to determine
howtheexecutionofabasicblockisdistributedamongdifferent
threads, so that the generated test program can faithfully simulate
the thread behaviors of the seed benchmark program. In particular,
the distribution pattern analysis aims to answer the following two
questions. First, how is the execution of a basic block changed
whenthenumberofthreadsisscaledup/down?Second,whatisthe
exact distribution of a basic block among different threads under a
specific number of threads? To answer these questions, FADATest
conductsacomprehensivedistributionpatternanalysisbasedon
the profiled block hotness and thread activity information.
899
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FADATest ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Algorithm 1: Thread Grouping Analysis
Input:ğ»ğ‘œğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ - The list of block hotness arrays, where each
array corresponds to a block and is indexed by thread id
Output: ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘  - The result thread groups
1ğ‘ğ‘¢ğ‘šğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  â†ğºğ‘’ğ‘¡ğ‘ğ‘¢ğ‘šğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  (ğ»ğ‘œğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ );
2ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘  â†<<1,2,...,ğ‘ğ‘¢ğ‘šğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  >>;
3foreachhotness array ğ»ğ´inğ»ğ‘œğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ do
4ğ»ğ‘ğ‘ ğ‘ğ‘’ğ‘Ÿğ‘œ â†ğ¹ğ´ğ¿ğ‘†ğ¸;
5ğ»ğ‘ğ‘ ğ‘ğ‘œğ‘›ğ‘’ğ‘ğ‘’ğ‘Ÿğ‘œ â†ğ¹ğ´ğ¿ğ‘†ğ¸;
6foreachgroupğ‘”inğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘  do
7 forthread id ğ‘¡ğ‘–ğ‘‘inğ‘”do
8 ifğ»ğ´[ğ‘¡ğ‘–ğ‘‘]=0then
9 ğ»ğ‘ğ‘ ğ‘ğ‘’ğ‘Ÿğ‘œ â†ğ‘‡ğ‘…ğ‘ˆğ¸;
10 else
11 ğ»ğ‘ğ‘ ğ‘ğ‘œğ‘›ğ‘’ğ‘ğ‘’ğ‘Ÿğ‘œ â†ğ‘‡ğ‘…ğ‘ˆğ¸;
12 end
13 end
14 ifğ»ğ‘ğ‘ ğ‘ğ‘’ğ‘Ÿğ‘œ andğ»ğ‘ğ‘ ğ‘ğ‘œğ‘›ğ‘’ğ‘ğ‘’ğ‘Ÿğ‘œ then
15 (ğ‘”1,ğ‘”2)â†ğ‘†ğ‘ğ‘™ğ‘–ğ‘¡ğºğ‘Ÿğ‘œğ‘¢ğ‘ (ğ‘”);
16 ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ .ğ‘…ğ‘’ğ‘šğ‘œğ‘£ğ‘’ (ğ‘”);
17 ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ‘ .ğ´ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ (ğ‘”1,ğ‘”2);
18 end
19end
20end
21returnResultGroups ;
ThreadGroupingAnalysis. Generally,amulti-threadedprogram
exports to users an adjustable parameter, e.g., ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ , which
allows users to specify the number of threads for the execution
oftheprogram.Intuitively,thevalueof ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ isthesame
astheactualnumberofthreadscreatedbytheprogram.But,our
observations on the multi-threaded benchmark programs from
PARSECshowthattheactualnumberofthecreatedthreadsisprob-
ablygreaterthan the specified value of ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ . For example,
bodytrack isaPARSECbenchmark.Whenwerunthisbenchmark
with a specified number of threads 1, i.e., ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘  =1, the
actual number of the created threads is 3. Similarly, if the specified
number of threads is 2 and 4, the actual number of the created
threads is 4 and 6, respectively. Figure 4illustrates this behavior.
Furtherinvestigationshowsthatthisbenchmarkalwayscreatesa
main thread, an I/O thread, and ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ worker threads. In
other words, the main thread and the I/O thread do notscale when
the value of ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ increases. Therefore, the first step of the
distributionpatternanalysisin FADATest istounderstandhowthe
threadsof theseed benchmarkprogram aregrouped andhowthe
number of threads in each group scales.
To this end, FADATest analyzes the profiled basic block hotness
datatoidentifythethreadgroupingpattern.Thisisinspiredbythe
key observation that the threads in the same group should execute
same/similar basic blocks. By searching for group-private blocks,
which are executed by one group but not others, FADATest can
rapidly identify thread groups in an execution profile. FADATest
represents the thread grouping pattern using a two-dimensional
array:<<ğ›¼1,ğ›½1>,<ğ›¼2,ğ›½2>,...,<ğ›¼ğ‘›,ğ›½ğ‘›>>,where ğ›¼ğ‘–Ã—ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ +
ğ›½ğ‘–is the actual number of threads in the group ğ‘–,1â‰¤ğ‘–â‰¤ğ‘›.Table 1: Thread grouping patterns identified by FADATest
for seed benchmark programs in PARSEC.
Thread Grouping Pattern
blackscholes <<0, 1>, <1, 0>>
bodytrack <<0, 1>, <1, 0>, <0, 1>>
canneal <<0, 1>, <1, 0>>
dedup <<0, 1>, <1, 0>, <1, 0>, <1, 0>, <1, 0>, <0, 1>>
fluidanimate <<0, 1>, <1, 0>>
freqmine <<0, 1>, <1, -1>>
streamcluster <<0, 1>, <1, 0>, <1, 0>, <1, 0>, <1, 0>, <1, 0>, <0, 1>>
swaptions <<0, 1>, <1, 0>>
vips<<0, 1>, <1, 0>>
Algorithm 1describeshow FADATest identifiesthreadgroups.
Initially,thereisonlyonethreadgroup.Theanalysisattemptsto
split the initial group into multiple groups by scanning the hotness
data of all basic blocks to identify group-private blocks. A group is
split into two groups if at least one thread in the group executessome blocks that are not executed by any other thread(s). By ap-plying this algorithm to multiple execution profiles of the same
seedbenchmarkprogramwithdifferentnumbersofthreads,e.g.,
1,2,and 4, FADATest canobtainmultiple threadgroupingresults.
Withthesegroupingresults, FADATest canfurtherinferthethread
groupingpatternbyanalyzingtherelationshipbetweenthetotal
number of threads in each group and the value of ğ‘›ğ‘¢ğ‘š_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ .
Recall the bodytrack benchmark in Figure 4, the thread group-
ingpatternidentifiedby FADATest is<<0,1>,<1,0>,<0,1>>.We
further list the thread grouping patterns identified by FADATest
for seed benchmark programs from PARSEC in Table 1. The ta-
bleshowsthateverybenchmarkhasmorethanonethreadgroup.
streamcluster evenhassevengroups.Thisexplainsthenecessity
of the thread grouping analysis. Indeed, without this information,
it will be extremely hard, if not impossible, to correctly scale up
thenumberofthreadsinthegeneratedtestprogramandmatchthe
thread behaviors of the seed benchmark program.
Block Hotness Pattern Analysis. Withthethread groupingin-
formation, FADATest next analyzes the distribution patterns of
basic block hotness. FADATest achieves this through two steps.
First,FADATest identifies the block hotness patterns at the thread
group level, i.e., the relationship between the total execution coun-
ters of a basic block inside a group and the number of threads in
the group. Second, FADATest figures out the distribution patterns
of basic block hotness among threads inside a group. Note that
FADATest analyzes the hotness distribution pattern for eachbasic
block, as different blocks often exhibit different behaviors.
Given a thread group ğºwithğ‘šthreads,FADATest creates a
simple yet accurate model to compute the total execution count of
a block in ğº:ğœƒÃ—ğ‘š+ğ›¿, whereğœƒandğ›¿are the model parameters
and can be calculated based on the method of least squares approx-
imation using the profiled execution counts of the block in ğºwith
differentthreadnumbers.Supposetheactualexecutioncountofthe
blockisğ¶1andthecountcomputedbythemodelis ğ¶2.Theerror
rateofthemodelisdeterminedby|ğ¶1âˆ’ğ¶2|
ğ¶1.Ourexperienceswith
standardmulti-threadedDBTperformancebenchmarkprograms
show that, within an error rate less than 10%, this model is able to
900
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng Zuo
 0 10 20 30 40 50 60 70
1 2 4 8 16Executed Times(k)By each thread
 0 10 20 30 40 50
1 2 4 8 16By each thread
Figure5:Hotnessdistributionpatternsoftwodifferentbasic
blocks of bodytrack in the same thread group. The x-axis is
the number of threads in the group.
predict the execution counts of more than 90% basic blocks. For
the remaining blocks, they often have very small execution counts
andthuscanbeexcludedfromthegeneratedtestprogram.Tosum-
marize,FADATest canaccuratelycomputethehotnesscountofa
basic block for each thread group through this model.
Next,FADATest analyzesthedistributionpatternofablockin-
side a group. It is worth pointing out that this pattern typically
doesnotchangewhenthenumberofthreadsinthegroupchanges.
FADATest uses a vector to denote the pattern: < ğ›¾1,ğ›¾2, ...,ğ›¾ğ‘š>,
whereğ‘šis the number of threads in the group and/summationtext.1ğ‘š
ğ‘–=1ğ›¾ğ‘–=1.
To simplify the design, FADATest heuristically treats a distribu-
tion pattern < ğ›¾1,ğ›¾2, ...,ğ›¾ğ‘š> as an evenly distributed pattern, i.e.,
ğ›¾1=ğ›¾2=...=ğ›¾ğ‘š=1
ğ‘š,i fâˆ€ğ‘–âˆˆ{1,...,ğ‘š},|ğ›¾ğ‘–âˆ’1/ğ‘š|
1/ğ‘š<0.1. This
allowsFADATest toaccuratelycapturethepatternformorethan
95% basic blocks.
Figure5showstwodifferentdistributionpatternsanalyzedby
FADATest for two basic blocks of the same thread group in the
bodytrack benchmark. As shown in the figure, the total execution
countofthefirstblockdoesnotchangeasthenumberofthreadsin
the group increases, while the total execution count of the second
block increases. But for both blocks, the total execution counts are
evenly distributed among different threads in the group.
3.4 Test Program Generation
FADATest adopts the C language for the test program as it is an
efficientsystemlanguageanditisquitestraightforwardtointegrate
theassemblycodeofbasicblocksintothetestprogram.Toallow
users to easily scale up/down the execution time of the test pro-
gram,FADATest generates each test program with a scaling factor
ğœ‚,whichisusedtocalculatetheactualexecutioncounterofabasic
blockğµğµfor each thread ğ‘‡:
ğ´ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™ğ¸ğ‘¥ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘’ğ‘Ÿ (ğµğµ,ğ‘‡)=ğ»ğ‘œğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘’ğ‘Ÿ (ğµğµ,ğ‘‡)
ğœ‚(1)
Here, the hotness counter of ğµğµforğ‘‡is calculated using the dis-
tribution pattern derived from the previous analysis. The value of
ğœ‚can be tuned each time when the test program is used to test
DBT performance on a new hardware platform. In case the actual
execution countof abasic blockbecomeszero,i.e., ğœ‚is higher than
its hotness counter, the block will be skipped in the testing.
For a multi-threaded test program, a simple execution mecha-
nismistolaunchallthreadsatthebeginningoftheprogram.How-
ever,thismayintroducepotentialinaccuratetestingresults.Thereasonisthattheexecutionofamulti-threadedprogramusuallyAlgorithm 2: Test Program Generation
Input:ğ»ğ‘œğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ - The list of block hotness arrays
ğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ¿ğ‘–ğ‘ ğ‘¡ - The specification of thread groups
ğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ğ¿ğ‘–ğ‘ ğ‘¡ - The specification of thread activities
Output: ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’- The generated source code of the test program
1ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘ğ‘ˆğ¿ğ¿;
2foreachthread group id ğ‘‡ğºğ¼ğ·inğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğºğ‘Ÿğ‘œğ‘¢ğ‘ğ¿ğ‘–ğ‘ ğ‘¡ do
3ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’+ğºğ‘’ğ‘›ğ¶ğ‘œğ‘‘ğ‘’ğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ¹ğ‘¢ğ‘›ğ‘ğ»ğ‘’ğ‘ğ‘‘ (ğ‘‡ğºğ¼ğ·);
4foreachbasic block ğµğµinğ»ğ‘œğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ do
5 ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’+ğºğ‘’ğ‘›ğ‘ƒğ‘ğ‘¦ğ‘™ğ‘œğ‘ğ‘‘ğµğµ (ğµğµ,ğ‘‡ğºğ¼ğ· );
6end
7ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’+ğºğ‘’ğ‘›ğ¶ğ‘œğ‘‘ğ‘’ğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ¹ğ‘¢ğ‘›ğ‘ğ‘‡ğ‘ğ‘–ğ‘™ (ğ‘‡ğºğ¼ğ·);
8end
9ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’+ğºğ‘’ğ‘›ğ¶ğ‘œğ‘‘ğ‘’ğ¶ğ‘¡ğ‘Ÿğ‘™ğ¹ğ‘¢ğ‘›ğ‘ğ»ğ‘’ğ‘ğ‘‘ ();
10foreachthread group id ğ‘‡ğºğ¼ğ·inğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ğ¿ğ‘–ğ‘ ğ‘¡ do
11ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’+ğºğ‘’ğ‘›ğ¶ğ‘œğ‘‘ğ‘’ğ‘‡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ (ğ‘‡ğºğ¼ğ·);
12end
13ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’â†ğ‘‡ğ¶ğ‘œğ‘‘ğ‘’+ğºğ‘’ğ‘›ğ¶ğ‘œğ‘‘ğ‘’ğ¶ğ‘¡ğ‘Ÿğ‘™ğ¹ğ‘¢ğ‘›ğ‘ğ‘‡ğ‘ğ‘–ğ‘™ (ğ‘‡ğºğ¼ğ·);
14returnTCode;
interleavesmulti-threadedexecutionphaseswithsingle-threaded
executionphases.Hence,thetotalexecutiontimeoftheprogramshouldcombinetheexecutiontimesofallphases.Inotherwords,
the test program generated by FADATest should preserve both
multi-threadedphasesandsingle-threadedphasesintheseedbench-
mark program. Therefore, FADATest integrates the profiled thread
activity information into the generated test program. In particular,
the relative execution points of thread creation and termination
events are the same as those in the seed benchmark program. This
way, the multi-threaded test program generated by FADATest can
achieve more accurate performance testing results.
Algorithm 2shows the process of generating a test program in
FADATest . An example of the generated test program is illustrated
in Figure 6. The major components of the test program include
several functions: init,run_thread_0 ,fini, andmain. Themain
function serves as the driver of the test program. The initand
finifunctions allocate and deallocate memory resources required
to run the test program, respectively. FADATest analyzes the in-
structionsineachbasicblocktodeterminetheexactmemoryre-
sourcerequiredbytheblock(seeSection 4formoredetails).The
run_thread_0 function is the test function, which contains the
assemblycodeofbasicblocks.Here,â€œ0â€istheindexofthethread
group. That means, FADATest generates a test function for each
thread group, rather than each thread. This design choice allows
FADATest to limit the size of the test program and reduce the addi-
tionalpressureonthecodecacheofthetargetDBTsystem,asall
executed blocks will be translated and stored in the code cache.
4 IMPLEMENTATION
Wehaveimplementedaprototypeof FADATest withthesupport
oftwomainstreamISAsonthemarket:x86-64andAArch64.The
execution profiler is implemented at the binary code level, using
theDBTsystemQEMU[ 3].Notethat FADATest isnottiedtoany
specific DBT system, and, in fact, the generated test programs
canbe usedtotest variousDBT systems.Thedistribution pattern
901
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FADATest ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
1#include ...
2#define ...
3structtimeval *tv_begin, *tv_end; unsigned char *ptr_mem_base;
4voidrun_thread_0(unsigned int *percent);
5...//other declarations
6voidinit(void){
7ptr_mem_base =alignment16(malloc(1858758 * sizeof(uint64_t));
8ptr_double_float =malloc(sizeof (double));
9tv_begin =calloc(sizeof (struct timeval));
10...//more initialization
11}
12voidrun_thread_0(unsigned int *percent) {
13//executes basic blocks according to hotness and scaling factor
14 for(long i=0;i<*percent; i++){
15 asm volatile(
16 "movq %[input_mem_base], %%r15 \n\t"//prepares memory
17 "movq $1363, %%r10\n\t" //repeats 1363 times per iteration
18 "loop_0_0: movl 0x2f(%%r15), %%edx\n\t"
19 "cmpl %%ebx, %%edx\n\t"
20 "jne jmp_hit_0_0\n\t" //branches to a provided target
21 "test %%r15, %%r15\n\t"
22 "jmp_hit_0_0: dec %%r10\n\t"
23 "test %%r10, %%r10\n\t"
24 "jnz loop_0_0\n\t"
25 :
26 :[input_mem_base] "m"(ptr_mem_base)
27 :"rdx","r10","r15"
28); ... //more basic blocks
29}} ... //more threads
30voidfini(void) { ... /*releases memory*/}
31intmain(int argc, void*argv) {
32init();
33/*runs threads according to thread activity profiling*/
34run_thread_func_0(80); //executes first 80% of main thread.
35 for(int i=1;i<n_thread;i++){ //runs threads
36pthread_create(&(tid[i-1]), NULL,\
37 (void*)run_thread_func_1, &percent);
38}
39 for(int i=1;i<n_thread;i++){
40pthread_join(&(tid[i-1]), NULL);
41}
42run_thread_func_0(20); //executes remaining 20% of main thread.
43fini();return 0;
44}
Figure 6: A test program generated by FADATest. The code
is simplified for demonstration.
analyzerandthetestprogramgeneratorareprimarilydevelopedinPython(
âˆ¼2.4KLoC).Themajorobstacleweencounteredduringthe
implementationwas howtolegitimately executetheinstructions
in the basic blocks included in the generated test programs. We
next elaborate our solutions for each type of instructions.
Memory Access Instructions. To correctly execute a memory
access instruction,we needto passan accessiblememory address
to the instruction. In general, the memory address accessed by an
instruction is encoded through four elements and calculated based
ontheformula: ğ‘ğ‘ğ‘ ğ‘’+ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥Ã—ğ‘ ğ‘ğ‘ğ‘™ğ‘’+ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡.Here,ğ‘ğ‘ğ‘ ğ‘’isrequired
whileğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥andğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡areoptional. ğ‘ ğ‘ğ‘ğ‘™ğ‘’isaconstantof1,2,4,or
8. Besides, ğ‘ğ‘ğ‘ ğ‘’is typically provided through a register. Thus, our
implementationallocates amemoryregionin advanceandpasses
appropriatevaluestorelatedregisterssothatthecalculatedaddress
fallsintotheregion.Notethatmemoryaccessinstructionsinthe
sameblockcanshareonememoryregiontoreducetheoverhead
caused by memory allocation and data preparation.
Direct Branch Instructions. Since the generated test program
does not recover the control flow of the seed program, we need
to update the target of a branch instruction. Otherwise, the ex-ecution of the branch instruction may experience an error if the
branchtargetisunreachable.Hence,werevisethetargetofabranch
instruction to the instruction immediately following the branch
instruction. Furthermore, several instructions are inserted betweenthebranchandthetargetincasethebranchisaconditionalbranch.IndirectBranchInstructions. Ourimplementationalsoneedsto
take care of indirect branches. This is achieved by first placing a
reachable code address into a memory location and then replacing
the operand of an indirect branch with the address of the location.
This allows the indirect branch to be executed correctly.
Call/Ret Instructions. In our implementation, call/ret instruc-
tions are not executed directly. Instead, a call instruction is de-composed into two instructions. The first one saves the returnaddress and the second one branches to the call target, which isreplaced with a reachable code address. Like most DBT systems,
ret instructions are handled in the same way as indirect branches.
Floating-Point Instructions. Inappropriate floating-point opera-
tions may produce floating-point exceptions, e.g., division by zero.
Giventhataprogramwillbeterminatedonceafloating-pointex-
ception is triggered, we need to avoid such operations. To this end,
our implementation analyzes all floating-point instructions in a
basic block and carefully prepares the right floating-point data for
them so that no exception will be triggered.SIMDInstructions.
Similar to floating-point instructions, our im-
plementation feeds SIMD instructions with appropriate floating-
pointvaluestoavoidpotentialfloating-pointexceptions.Inaddition,
SIMDinstructionsgenerallyaccessmultiplememoryitemssimulta-
neously. This requires our implementation to pay special attention
to calculate the actual size of the required memory and provide
aligned addresses for SIMD instructions.
SystemCalls. Ifablockcontainsasystemcall,ourimplementation
firstly analyzes the block to determine whether the system call
numberisdefined.Incasetheinstructionthatdefinesthesystem
callnumberisnotfoundinthisblock,anadditionalinstructionwill
be inserted before the system call instruction to define the system
call number. We use the getpidsystem call because it does not
require a parameter and also has no impact on the system state of
thehostplatform.Forsimplicity, FADATest doesnotensurethatthe
systemcallparametersarepreparedcorrectly.Thisisreasonable
because a system call may fail even in a normal execution.ISA-Specific Instructions.
Some ISA-specific instructions may
have special semantics and access implicit operands. For example,
the x86-64 CPUIDinstruction takes as input the value in RAXand
returnstheCPUfeaturesto RAX,RBX,andRCX.Ourimplementation
ensuresthecorrectexecutionofsuchaninstructioninabasicblock
by analyzing the instructions before it and inserting additional
instructions to initialize the input value if necessary.
5 EXPERIMENTAL RESULTS
Inthis section,we evaluate FADATest . Theevaluationaims toan-
swer the following research questions: i) How efficient are the test
programs generated by FADATest when used to test DBT perfor-
mance? ii) Can the test programs generated by FADATest uncover
the same performance regression issues of a DBT system as the
originalseedbenchmarkprograms?iii)Howefficientisthetestpro-
gramgenerationprocess?Thedataandsourcecodeareavailable
athttps://github.com/fadatestdbt/fadatest.git.
ExperimentalSetup. Inourevaluation,weuse FADATest togen-
erate test programs from benchmark programs in two standard
902
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng Zuo
100101102103104
blackscholesbodytrackcannealdedup
fluidanimatefreqmine
streamclusterswaptionsvipsgmeanTesting Performance SpeedupQEMU-AArch64-FADATest
QEMU-x86-64-FADATest
Valgrind-x86-64-FADATest
QEMU-x86-64-RdmSmpl
Figure 7: Testing performance speedup achieved by the
test programs generated by FADATest and random sam-pling (RdmSmpl). The performance baseline is the testingtimes of original benchmarks. Higher is better.
benchmarksuites:SPECCPU2017andPARSEC(version3.0),whichhavebeenextensivelyusedtotesttheperformanceofDBTsystems.ForeachSPECCPU2017benchmark,
FADATest onlyneedstorunit
oncetocollecttheexecutionprofile,asSPECCPU2017benchmarksaresingle-threaded.ForPARSECbenchmarks,
FADATest runsthem
with 1, 2, and 4 threads to collect multiple execution profiles for
distributionpatternanalysis,butteststhemwith8threadstoshow
the capability of FADATest to scale up the number of threads. The
generatedtestprogramsareemployedtotesttheperformanceof
two representative DBT systems: QEMU and Valgrind, with 8 and
5 historical versions, respectively.
Apart from comparing with the testing results of the original
benchmarkprograms,wealsoimplementarandomsamplingmech-
anism and compare it comprehensively with FADATest . However,
ifthesamplingfrequencyistoohigh,itwillsignificantlyprolong
thesampling process.Moreover,itneeds extrastorageto savethe
sampledbasicblocks.Hence,weadoptasamplingintervalof 10,000
basicblocksandlimittheentiresamplingprocessto10hoursfor
a benchmark program. Even with such a long time, we still can
onlycollectblocksforthreePARSECbenchmarks.Thisalsodemon-
strates the impracticabilityof the random sampling approach.
To demonstrate the adaptability of the generated test programs,
the evaluation covers two hardware platforms, powered by x86-64
andAArch64,respectively.Thex86-64platformisequippedwith
an Intel i9-9900 CPU at 3.1GHz and 32GB main memory, while the
AArch64platformhasaRockchipRK3399CPUat2.0GHzwith4GB
memory.Onthex86-64platform,QEMUtakesasinputAArch64
guestbinaries,whileontheAArch64platform,itrunsx86-64guest
binaries. However, QEMU fails to run x86-64 binaries of original
SPEC CPU 2017 benchmarks on the AArch64 platform. Also, there
is no data for Valgrind on AArch64 due to the significantly long
executiontimeofValgrind.Ittakeslessthan3minutestomanually
tunethescalingfactor ğœ‚foratestprogramononeplatform.The
twoplatformsareoccupiedexclusivelybyourevaluationtoreduce
any potential impactsof random factors. Inaddition, we run each
test 10 times and use the average value of them as the final result.5.1 Testing Efficiency
Figure7showsthetestingperformancespeedupachievedbythe
test programs generated by FADATest when used for testing the
performance of QEMU and Valgrind on the two hardware plat-
forms. The performance baseline is the testing time of the original
benchmark programs. The results of randomsampling are also in-
cludedforreference.SincetherearemultipleversionsofQEMUand
Valgrind, we measure the testing performance speedup of every
version for each benchmark and use the geometric mean as the
final speedup result of the benchmark. As shown in the figure, the
testing performance is significantly improved by the test programs
generatedby FADATest comparedtotheoriginalbenchmarks,with
an average speedup of 248 Ã—for QEMU-AArch64, 156 Ã—for QEMU-
x86-64,and96 Ã—forValgrind-x86-64.Itisnotasurprisethatrandom
sampling can also achieve testing performance speedup, as it skips
theexecutionofmanybasicblocks.Onaverage,itonlytakesaround
35secondstocompletethetestingofatestprogramgeneratedby
FADATest .This showsthecapability of FADATest tooffer ahigh
testing efficiency for DBT performance testing.
5.2 Testing Effectiveness
QEMU.Figure8shows the performance testing results of differ-
entversionsofQEMUonthex86-64platformusingeachoriginal
benchmarkprograminSPECCPU2017andPARSECandthecorre-
spondingtestprogramgeneratedby FADATest .Thetestingtimes
are normalized to the version 2.9.1, so both of the two lines start
from1.Duetothespacelimitation,weomitthetestingresultsof
QEMU-AArch64, which are very similar to QEMU-x86-64.
From Figure 8, we can make two observations. First, the perfor-
mance of QEMU-x86-64 changes frequently across different ver-
sions. For some benchmarks, such as perlbench andleela, there
is a clear performance regression from the version 3.0.0 to the ver-
sion4.0.1.Forsomeotherbenchmarks,e.g., canneal andvips,the
performanceregressionstartsearlier,i.e.,fromtheversion2.12.1to
the version 3.0.0. This phenomenon again suggests that it is highly
necessary to conduct performance regression testing for DBT sys-
tems.Moreover,acomprehensivetestsuiteisrequiredtoexpose
theperformanceregressionsasearlyaspossible.Second,thetesting
results of the test programs generated by FADATest highly match
withthoseoftheoriginalbenchmarkprograms.Inparticular,the
performance changes uncovered by testing the original benchmark
programs can also be captured by testing the generated test pro-
grams.Thisshowstheeffectivenessofthetestprogramsgenerated
by FADATest for DBT performance regression testing.
Valgrind. Figure9shows the performance testing results of Val-
grind using the original benchmark programs and the correspond-
ing test programs generated by FADATest . Similarly, the testing
times of the version 3.14.0 are used as the performance baseline.
As shown in the figure, Valgrind also suffers from performance
regressions, e.g., from the version 3.17.0 to the version 3.18.0 for
perlbench .Furthermore,the testingresults ofthetest programs
generated by FADATest perfectly match with those of the original
benchmark programs. This further demonstrates the effectiveness
andtheportabilityof FADATest forDBTperformanceregression
testing across different DBT systems.
903
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FADATest ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
 0 2 4 6
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0blackscholes
 0 1 2 3 4 5 6
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0bodytrack
 0 1 2 3
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0canneal
 0 1 2 3
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0dedup
 0 2 4 6 8
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0fluidanimate
 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0freqmine
 0 3 6 9
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0streamcluster
 0 1 2 3 4 5 6
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0swaptions
 0 1 2 3 4 5 6
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0vips
Original
FADATest 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0perlbench
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0gcc
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0mcf
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0omnetpp
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0xalancbmk
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0Normalized Execution Timex264
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0deepsjeng
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0leela
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0exchange2
 0 1 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0xz
Figure 8: Testing effectiveness of the test programs generated by FADATest with different versions of QEMU-AArch64.
 0.5 1 1.5
3.14.03.15.03.16.03.17.03.18.0Normalized
Execution Timeperlbench
 0.5 1 1.5
3.14.03.15.03.16.03.17.03.18.0omnetpp
Original FADATest 0.9 1 1.1
3.14.03.15.03.16.03.17.03.18.0dedup
Figure 9: Testing effectiveness of the test programs gener-
ated by FADATest with different versions of Valgrind.
 0.5 1 1.5 2 2.5 3
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0Normalized
Execution Timecanneal
 1 1.5 2 2.5
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0dedup
Original FADATest RdmSmpl 1 1.5 2
2.9.12.12.13.0.04.0.14.1.14.2.05.0.05.2.0freqmine
Figure 10: Test effectiveness of the test programs gen-erated by FADATest and the random sampling mecha-nism (RdmSmpl) with different versions of QEMU.
ComparingwithRandomSampling.
Wenextcomparethetest-
ing results of the test programs generated by FADATest and those
generatedusingtherandomsamplingapproach.Wealsoinclude
thetestingresultsoforiginalbenchmarksasreference.Figure 10
shows the results of some benchmarks for QEMU. From the figure,
we can clearly see that random sampling cannot achieve testing
resultsthatalignwiththose oforiginalbenchmarkprograms.For
example,randomsamplingfailstouncovertheperformancefluc-
tuation after the version 4.0.1 for the benchmark canneal. The100101102103104105
blackscholesbodytrackcannealdedup
fluidanimatefreqmine
streamclusterswaptionsvips
averageAbsolute Execution Time(s) Online-AArch64
Offline-AArch64Online-x86-64
Offline-x86-64
Figure 11: Absolute execution times (in seconds) of the two
stages of the test program generation process in FADATest:online profiling and offline analysis and generation.
reason is that the test programs generated by random samplingcannot preserve the program-specific runtime characteristics of
originalbenchmarkprograms.Incontrast, FADATest generatestest
programsbasedontheprofiledblockhotnessandthreadactivity
information, which enables it to achieve a testing effectiveness
similar to original benchmark programs.
5.3 Test Program Generation Process Study
PerformanceOverhead. Therearetwomajorstagesin FADATest
togenerateatestprogramfromabenchmarkprogram:onlineprofil-ingandofflineanalysisandgeneration.Figure 11showstheabsolute
904
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng Zuo
Table 2: Sizes of log files generated by FADATest (in bytes).
â€œA64:â€ AArch64; â€œx64:â€ x86-64.
Benchmark A64 x64 Benchmark A64 x64
blackscholes 327K 510K freqmine 437K 638K
bodytrack 1.9M 2.0M streamcluster 1.9M 2.9M
canneal 578K 826K swaptions 377K 592K
dedup 1.3M 1.8M vips 2.8M 2.8M
fluidanimate 510K 759K average 1.5M 1.2M
Table3:Comparisonsofthetestprogramgenerationprocess
between FADATest and random sampling (RdmSmpl).
canneal dedup freqmine
Online FADATest 882s 116s 2188s
Profiling RdmSmpl 5157s 1453s 33465s
Offline FADATest 1.6s 4.63s 1.95s
Analysis RdmSmpl 69.79s 69.94s 173.41s
Log SizeFADATest 578KB 1.3MB 437KB
RdmSmpl 262MB 78MB 1.8GB
times required by the two stages for each benchmark and the two
ISAs.WeomittheresultsofSPECCPU2017benchmarksduetothe
space limitation. For online profiling, the time is closely correlated
totheoriginalexecutiontimeofeachbenchmarkprogram.Onaver-
age, the execution time of online profiling in FADATest , compared
totheoriginalbenchmarkexecutiontime,isaround2 .4Ã—.Wethink
this performance overhead is reasonable and also acceptable, as
the test programs only need to be generated once. Compared to
the online profiling stage, the offline analysis and generation stage
takesmuchlesstime.Asshowninthefigure,forallbenchmarks,
the generation time is less than 30 seconds. This demonstrates the
highefficiencyoftheanalysisandgenerationprocessin FADATest .
StorageCost. We also study the cost of storing the log files gener-
ated byFADATest during the online profiling stage. Table 2shows
the detailed sizes of the log files generated for each benchmarkand the two ISAs. As shown in the table, for all benchmarks, the
size is less than 3MB. On average, it only costs around 1.5MB for a
benchmark and one ISA. We believe this storage cost is acceptable
in practice, given that the capacity of a typical storage device is up
to several gigabytes even on embedded devices.
Comparing with Random Sampling. We further compare the
test program generation process of FADATest with the random
sampling approach. Table 3shows the results. As shown in the
table,ittakesamuchlongertimeforrandomsamplingtofinishthe
generation process. This is because the sampled blocks need to be
savedduringthesamplingprocess.Moreover,thesizesofthelog
files generated by random sampling are much larger. These results
show the impracticability of the random sampling approach.
6 CASE STUDY
In this section, we show two representative case studies on QEMU
to demonstrate the practicability and necessity of conducting per-
formance regression testing through FADATest during the daily
development of a DBT system. 8.8 9 9.2 9.4 9.6
 0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700Absolute Testing Time (s)
Commit #, from old to newCommit 888ea96a
Figure 12: Performance regression by a single commit.
A performance regression caused by a performance bug. In
thiscasestudy,wefirstobservedaperformanceregressionissueof
theperlbench benchmarkunder the reference inputbetween two
commits: eabb7b91 (04/21/2016)and 757e725b (01/26/2016),which
include1764othercommitsbetweenthem.Thatmeans,tofigureout
whichcommit(s)leadtotheregression,itisnecessarytoconduct
comprehensive performancetesting on individualcommits. Given
thelargenumberofthecommits,itwillbeverytime-consuming
toconductsuchtestingusingtheoriginalbenchmarkbecauseof
the long execution time of QEMU. Note that we didnâ€™t observe a
performancedifferenceifthe testinputofperlbench isusedforthe
testing. Also, it may take more than 300 hours to test the commits
with thetraininput. In contrast, with the FADATest generated test
program,wecompletetheperformancetestingofthe1764commits
in around 13 hours. Figure 12shows the testing result.
Fromthefigure,wecanclearlyseethatthisperformanceregres-
sion issue is caused by a single commit, i.e., 888ea96a (02/16/2016).
Furtherinvestigationshowsthatthiscommitessentiallyremoved
the manuallyenforced __always_inline__ attribute because the
developeroveroptimisticallyassumesthatâ€œcompilershaveimproved... andweareprobablybetterofftrustingthecompilerratherthantry-ingtoforceitshand.â€Unfortunately,however,thecompilerisnotassmartashumandeveloperstomakeanoptimaldecisiononwhether
a function should be inlined or not. On the other hand, inlining is
an important compiler optimization that can enable many other
optimizations,e.g.,deadcodeeliminationandconstantpropagation.
That is why this commit leads to a performance regression.
A performance regression caused by extended functionali-
ties.This case study centers on the performance regression of
QEMU for the perlbench benchmark between the two release ver-
sions:4.0.1(10/17/2019)and3.0.0(08/14/2018),asshowninFigure 8.
Thereare5655commitsbetweenthesetwoversions.Byconducting
performancetestingusingthetestprogramgeneratedby FADATest ,
two commits that caused the regression are identified in around
42hours: f7b78602 (01/29/2019)and c47eaf9f (02/05/2019).The
first commit added additional code to check whether a translation
blockisvalidfordifferentclusters.Sincethecodewasaddedonthe
criticalpathofthetranslationprocess,itintroducedasubstantial
performance slowdown. The second commit mainly added the sup-
portofARMpointerauthenticationinstructions,whichweresimplyignoredbeforethiscommit.Asaresult,itwouldinevitablydecrease
the performance if the guest application has such instructions.
905
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FADATest ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Summary. The above case studies clearly demonstrate the benefit
ofthehightestingefficiencyofferedby FADATest onidentifying
problematiccommitsthatcauseperformanceregressions.Atthe
same time, it shows the necessity of conducting performance re-gression testing during the daily development of a DBT system.
Here,wewouldliketoemphasizethatthehightestingefficiency
ofFADATest also provides a performance-centric view for devel-
opers oneach commit. This isimportant even though thecommit
istoaddanewfeaturetotheDBTsystemandexpectssomeper-formance slowdown, as the testing will tell developers the exactperformanceslowdownandallowsthemtoimmediatelyremedy
the performance loss if necessary.
7 RELATED WORK
Performance RegressionTesting. A considerable amount of re-
search work has been conducted to enhance both effectivenessand efficiency of performance regression testing. Some research
work leverages automated workload generation [ 4,10] to produce
proper input data sets for performance regression testing. Someresearch work creates selective strategies to attain better testing
efficiency [ 19,30,31]. Bagherzadeh et al. analyze the performance
resultsofsystemcallsinhistoricversionsoftheLinuxkernel[ 2].
WISE presents an automated test generation techniques to find
performancebugs[ 7].Theresearchworkin[ 18]proposestouti-
lize performance counters to detect performance regressions. Toobtainafasttestingreport,someresearchworkstudiestestcaseprioritization approaches [
20,26]. PerfScope [ 14] improves the
testing efficiency by conducting performance risk analysis for pri-
oritization. To test the performance of multi-threaded programs,
SpeedGun generates multiple threads from a single-threaded appli-
cation [28]. PerfImpact sends the same input to two releases and
automatically mines the corresponding execution traces to rank
the impacts of code changes [ 24]. FOREPOST detects performance
issuesbyadoptingmachinelearningtechniquestoselectinputdata
to cover computationally intensive program paths [25].
Though performance regression testing has been explored com-
prehensively in common software systems, it is still very challeng-
ingtodirectlyapplyexistingperformanceregressiontechniques
to DBT systems. There are several reasons. First, different from
commonsoftwaresystems,DBTsystemstakeasinputexecutable
binaries, which make it quite difficult to automatically and flexibly
generate representative inputs to conduct performance regression
testingofDBTsystems.Second,duetothelowefficiencyofDBT
systems, it takes extremely long time to complete the testing of
standard benchmark programs on DBT systems. Third, the diverse
hardware platforms of a DBT system make it hard to use the same
benchmarkprogramstotesttheperformanceoftheDBTsystemon
different hardwareplatforms, especially whenthe platformshave
significantlydifferentcomputingpower.Toovercomethesechal-
lenges,FADATest generates test programs from real benchmark
programsbasedontheexecutioncharacteristicsofthebenchmarks.
The generated test programs can achieve efficient and adaptive
performance regression testing of DBT systems.
GuestProgramGenerationforDBTSystems. PerfDBT[ 39]at-
temptstogenerateguestprogramstotesttheperformanceofaDBT
system. However, PerfDBT has several fundamental limitationswhen using the generated guest programs for DBT performance
regressiontesting.First,itisdesignedspecificallyforx86-64plat-
forms, and thus cannot be applied to DBT systems running on a
different hardware platform. Second, multi-threaded benchmarkprograms are not supported by PerfDBT. As a consequence, the
multi-threadingperformanceofthetargetDBTsystemcannotbe
tested. Third, PerfDBT assigns too many memory objects in the
generated guest programs, which may introduce a negative impact
on the performance testing results. In contrast, FADATest does not
havetheselimitations.Itisdesignedspecificallyforefficientand
adaptive performance regression testing of DBT systems.
CodeGenerationforHardwareSimulators. To generate input
programs for simulators, SimPoint samples instructions during the
execution of a program [ 12]. The sampling policy is created based
on the phase information of the original program. Though the
selected instructions can be used to test architecture simulators,
theyarenotsuitableforDBTsystems.ThisisbecauseDBTsystems
aretypicallydevelopedtorunreal-worldapplications,insteadof
smallpiecesofinstructions.Moreover,SimPointdoesnotsupport
multi-threaded programs. That means the instructions extractedby SimPoint cannot recover the thread behaviors of the original
programs.ComparedtotheinstructionsextractedbySimPoint,the
test programs generated by FADATest are more appropriate for
testing the performance of DBT systems, as they can preserve the
performance characteristics of original programs.
8 CONCLUSION
Although DBT is a key enabling technology, developing and main-
taining a real-world DBT system is not easy. An important method
toconstantlymaintaintheperformanceefficiencyofaDBTsystem
is to frequently conduct performance regression testing. However,
applying performance regression testing to DBT systems suffersfrom several practical challenges, such as the natural differencesbetween DBT systems and regular software systems, the limited
availabilityoftestprograms,andvarioushardwareplatformstar-
getedby aDBTsystem.This paperpresents FADATest toaddress
these challenges. FADATest employs several novel techniques to
generate adaptable test programs, which can be used to achieve
efficientandadaptiveperformanceregressiontestingofDBTsys-
tems. We also implement a prototype of FADATest and use it to
generatetestprogramsfrombenchmarkprogramsinSPECCPU2017 and PARSEC. Experimental results show that the test pro-
grams can uncover the same performance regression issues across
differentversionsofQEMUandValgrindastheoriginalbenchmark
programsandthetestingefficiencyisimprovedsignificantly.We
anticipatethat FADATest willbeintegratedintothedevelopment
cycles of existing DBT systems by automatically launching the
performance regression testing before each code commit.
ACKNOWLEDGMENTS
Weareverygratefultothereviewersfortheirvaluablefeedback
and comments. This work was done while Jin Wu was visiting the
UniversityofGeorgia.ThisworkwassupportedinpartbytheM.G.MichaelAwardfundedbytheFranklinCollegeofArtsandSciences
at the University of Georgia and a faculty startup funding of the
University of Georgia.
906
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Jin Wu, Jian Dong, Ruili Fang, Wen Zhang, Wenwen Wang, and Decheng Zuo
REFERENCES
[1]Adam Henson. 2019. Automatic Website Performance Regression Test-
ing.https://www.freecodecamp.org/news/automatic-website-performance-
regression-testing-4e30e6bf5cd.
[2]Mojtaba Bagherzadeh, Nafiseh Kahani, Cor-Paul Bezemer, Ahmed E Hassan,
JuergenDingel,andJamesRCordy.2018. AnalyzingadecadeofLinuxsystem
calls.Empirical Software Engineering 23, 3 (2018), 1519â€“1551.
[3]FabriceBellard.2005. QEMU,aFastandPortableDynamicTranslator.In Proceed-
ings of the Annual Conference on USENIX Annual Technical Conference (Anaheim,
CA)(ATC â€™05) . USENIX Association, USA, 41.
[4]AbderrahmaneBenbachir,IsnaldoFranciscoDeMelo,MichelDagenais,andBram
Adams. 2017. Automated Performance Deviation Detection across Software
Versions Releases. In 2017 IEEE International Conference on Software Quality,
Reliability and Security (QRS). IEEE, 450â€“457.
[5]Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai Li. 2008. The
PARSECBenchmarkSuite:CharacterizationandArchitecturalImplications.In
Proceedings of the 17th International Conference on Parallel Architectures and
CompilationTechniques (Toronto,Ontario,Canada) (PACTâ€™08).Associationfor
Computing Machinery, New York, NY, USA, 72â€“81. https://doi.org/10.1145/
1454115.1454128
[6]DavidBrumley, IvanJager,ThanassisAvgerinos,andEdwardJ.Schwartz. 2011.
BAP: A Binary Analysis Platform. In Proceedings of the 23rd International Confer-
enceonComputerAidedVerification (Snowbird,UT) (CAVâ€™11).Springer-Verlag,
Berlin, Heidelberg, 463â€“469.
[7]Jacob Burnim, Sudeep Juvekar, and Koushik Sen. 2009. WISE: Automated test
generation for worst-case complexity. In 2009 IEEE 31st International Conference
on Software Engineering. IEEE, 463â€“473.
[8]JieChen,DongjinYu,HaiyangHu,ZhongjinLi,andHuaHu.2019. Analyzing
Performance-AwareCodeChangesinSoftwareDevelopmentProcess.In Proceed-
ingsofthe27thInternationalConferenceonProgramComprehension (Montreal,
Quebec, Canada) (ICPC â€™19). IEEE Press, 300â€“310. https://doi.org/10.1109/ICPC.
2019.00049
[9]AndreaFioraldi,DanieleConoDâ€™Elia,andEmilioCoppa.2020.WEIZZ:AutomaticGrey-Box Fuzzing for Structured Binary Formats. In Proceedings of the 29th ACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis (VirtualEvent,
USA)(ISSTA 2020). Association for Computing Machinery, New York, NY, USA,
1â€“13.https://doi.org/10.1145/3395363.3397372
[10]KingChunFoo,ZhenMingJiang,BramAdams,AhmedEHassan,YingZou,and
ParminderFlora.2015. Anindustrialcasestudyontheautomateddetectionof
performanceregressionsinheterogeneousenvironments.In 2015IEEE/ACM37th
IEEE International Conference on Software Engineering, Vol. 2. IEEE, 159â€“168.
[11]Gunnar Morling. 2020. Towards Continuous Performance Regression Test-ing. https://www.morling.dev/blog/towards-continuous-performance-
regression-testing.
[12]GregHamerly,ErezPerelman,andBradCalder.2004. Howtousesimpointto
picksimulationpoints. ACMSIGMETRICSPerformanceEvaluationReview 31,4
(2004), 25â€“30.
[13]Christoph Heger, Jens Happe, and Roozbeh Farahbod. 2013. Automated Root
CauseIsolationofPerformanceRegressionsduringSoftwareDevelopment.In Pro-
ceedingsofthe4thACM/SPECInternationalConferenceonPerformanceEngineering
(Prague, Czech Republic) (ICPE â€™13). Association for Computing Machinery, New
York, NY, USA, 27â€“38. https://doi.org/10.1145/2479871.2479879
[14]PengHuang,XiaoMa,DongcaiShen,andYuanyuanZhou.2014. Performance
RegressionTestingTargetPrioritizationviaPerformanceRiskAnalysis.In Pro-
ceedings of the 36th International Conference on Software Engineering (Hyderabad,
India)(ICSE â€™14). Association for Computing Machinery, New York, NY, USA,
60â€“71.https://doi.org/10.1145/2568225.2568232
[15]Jinhu Jiang, Rongchao Dong, Zhongjun Zhou, Changheng Song, Wenwen Wang,
Pen-Chung Yew, and Weihua Zhang. 2020. More with Less â€“ Deriving More
Translation Rules with Less Training Data for DBTs Using Parameterization.
In2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO). 415â€“426. https://doi.org/10.1109/MICRO50266.2020.00043
[16]Ivo Jimenez, Noah Watkins, Michael Sevilla, JayLofstead, and Carlos Maltzahn.
2018. Quiho: Automated Performance Regression Testing Using Inferred Re-source Utilization Profiles. In Proceedings of the 2018 ACM/SPEC International
Conference on Performance Engineering (Berlin, Germany) (ICPE â€™18). Association
for Computing Machinery, New York, NY, USA, 273â€“284. https://doi.org/10.
1145/3184407.3184422
[17]JoelBealesandJeffreyDunn.2018. MobileLab:Highlyaccuratetesting toprevent
mobileperformanceregressions. https://engineering.fb.com/2018/10/19/android/
mobilelab.
[18]TomasKalibera,LubomirBulej,andPetrTuma.2005. Automateddetectionof
performance regressions: The Mono experience. In 13th IEEE International Sym-
posiumonModeling,Analysis,andSimulationofComputerandTelecommunication
Systems. IEEE, 183â€“190.
[19]Rafaqut Kazmi, Dayang NA Jawawi, Radziah Mohamad, and Imran Ghani. 2017.
Effective regression test case selection: A systematic literature review. ACMComputing Surveys (CSUR) 50, 2 (2017), 1â€“32.
[20]Muhammad Khatibsyarbini, Mohd Adham Isa, Dayang NA Jawawi, and Rooster
Tumeng.2018. Testcaseprioritizationapproachesinregressiontesting:Asys-
tematic literature review. Information and Software Technology 93 (2018), 74â€“93.
[21]Stephen Kyle, Igor BÃ¶hm, BjÃ¶rn Franke, Hugh Leather, and Nigel Topham.2012. Efficiently Parallelizing Instruction Set Simulation of Embedded Multi-
Core Processors Using Region-Based Just-in-Time Dynamic Binary Translation.
InProceedings of the 13th ACM SIGPLAN/SIGBED International Conference on
Languages,Compilers,ToolsandTheoryforEmbeddedSystems (Beijing,China)
(LCTES â€™12). Association for Computing Machinery, New York, NY, USA, 21â€“30.
https://doi.org/10.1145/2248418.2248422
[22]PhilippLeitnerandCor-PaulBezemer.2017. AnExploratoryStudyoftheStateof
Practice of Performance Testing in Java-Based Open Source Projects. In Proceed-
ingsofthe8thACM/SPEConInternationalConferenceonPerformanceEngineering
(Lâ€™Aquila,Italy) (ICPEâ€™17).AssociationforComputingMachinery,NewYork,NY,
USA, 373â€“384. https://doi.org/10.1145/3030207.3030213
[23]Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu,
andAlwenTiu.2017. Steelix:Program-StateBasedBinaryFuzzing.In Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering (Paderborn,
Germany) (ESEC/FSE 2017). Association for Computing Machinery, New York,
NY, USA, 627â€“637. https://doi.org/10.1145/3106237.3106295
[24]Qi Luo, Denys Poshyvanyk, and Mark Grechanik. 2016. Mining Performance
Regression Inducing Code Changes in Evolving Software (MSR â€™16). Association
for Computing Machinery, New York, NY, USA, 25â€“36. https://doi.org/10.1145/
2901739.2901765
[25]Qi Luo, Denys Poshyvanyk, Aswathy Nair, and Mark Grechanik. 2016. FORE-
POST: A Tool for Detecting Performance Problems with Feedback-Driven Learn-
ing Software Testing. In Proceedings of the 38th International Conference on Soft-
ware Engineering Companion (Austin, Texas) (ICSE â€™16). Association for Comput-
ingMachinery,NewYork,NY,USA,593â€“596. https://doi.org/10.1145/2889160.
2889164
[26]Shaikh Mostafa, Xiaoyin Wang, and Tao Xie. 2017. PerfRanker: Prioritization of
PerformanceRegressionTestsforCollection-IntensiveSoftware.In Proceedingsof
the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis
(SantaBarbara,CA,USA) (ISSTA2017).AssociationforComputingMachinery,
New York, NY, USA, 23â€“34. https://doi.org/10.1145/3092703.3092725
[27]Thanh H.D. Nguyen, Bram Adams, Zhen Ming Jiang, Ahmed E. Hassan, Mo-
hamed Nasser, and Parminder Flora. 2012. Automated Detection of Performance
RegressionsUsingStatisticalProcessControlTechniques.In Proceedingsofthe
3rdACM/SPECInternationalConferenceonPerformanceEngineering (Boston,Mas-
sachusetts,USA) (ICPEâ€™12).AssociationforComputingMachinery,NewYork,
NY, USA, 299â€“310. https://doi.org/10.1145/2188286.2188344
[28]Michael Pradel, Markus Huggler, and Thomas R Gross. 2014. Performance
regression testing of concurrent classes. In Proceedings of the 2014 International
Symposium on Software Testing and Analysis. 13â€“25.
[29]QEMU wiki. 2021. Projects using the QEMU code. https://wiki.qemu.org/Links#
Projects_using_the_QEMU_code.
[30]David Georg Reichelt and Stefan KÃ¼hne. 2018. How to Detect Performance
ChangesinSoftwareHistory:PerformanceAnalysisofSoftwareSystemVersions.
InCompanion of the 2018 ACM/SPEC International Conference on Performance
Engineering (Berlin, Germany) (ICPE â€™18). Association for Computing Machinery,
New York, NY, USA, 183â€“188. https://doi.org/10.1145/3185768.3186404
[31]Gregg Rothermel and Mary Jean Harrold. 1996. Analyzing regression test selec-
tion techniques. IEEE Transactions on software engineering 22, 8 (1996), 529â€“551.
[32]Daniel Sanchez and Christos Kozyrakis. 2013. ZSim: Fast and Accurate Mi-croarchitectural Simulation of Thousand-Core Systems. In Proceedings of the
40th Annual International Symposium on Computer Architecture (Tel-Aviv, Israel)
(ISCA â€™13). Association for Computing Machinery, New York, NY, USA, 475â€“486.
https://doi.org/10.1145/2485922.2485963
[33]DawnSong,DavidBrumley,HengYin,JuanCaballero,IvanJager,MinGyung
Kang, Zhenkai Liang, James Newsome, Pongsin Poosankam, and Prateek Saxena.
2008. BitBlaze:ANewApproachtoComputerSecurityviaBinaryAnalysis.In
Proceedingsofthe4thInternationalConferenceonInformationSystemsSecurity
(Hyderabad, India) (ICISS â€™08). Springer-Verlag, Berlin, Heidelberg, 1â€“25. https:
//doi.org/10.1007/978-3-540-89862-7_1
[34]Standard Performance Evaluation Corporation. 2020. SPEC CPU 2017. https:
//www.spec.org/cpu2017.
[35]AmandaSwearngin,MyraB.Cohen,BonnieE.John,andRachelK.E.Bellamy.
2013. HumanPerformanceRegressionTesting.In Proceedingsofthe2013Interna-
tionalConferenceonSoftwareEngineering (SanFrancisco,CA,USA) (ICSEâ€™13).
IEEE Press, 152â€“161.
[36]Wenwen Wang. 2021. Helper Function Inlining in Dynamic Binary Translation.
InProceedings of the 30th ACM SIGPLAN International Conference on Compiler
Construction.AssociationforComputingMachinery,NewYork,NY,USA,107â€“118.
https://doi.org/10.1145/3446804.3446851
[37]WenwenWang,Pen-ChungYew,AntoniaZhai,andStephenMcCamant.2020. Ef-ficientandScalableCross-ISAVirtualizationofHardwareTransactionalMemory.InProceedingsofthe18thACM/IEEEInternationalSymposiumonCodeGeneration
907
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. FADATest ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
andOptimization.AssociationforComputingMachinery,NewYork,NY,USA,
107â€“120. https://doi.org/10.1145/3368826.3377919
[38]Wenwen Wang, Pen-Chung Yew, Antonia Zhai, Stephen McCamant, Youfeng
Wu, and Jayaram Bobba. 2017. Enabling Cross-ISA Offloading for COTS Bi-
naries. In Proceedings of the 15th Annual International Conference on Mobile
Systems, Applications, and Services (Niagara Falls, New York, USA) (MobiSys
â€™17). Association for Computing Machinery, New York, NY, USA, 319â€“331.
https://doi.org/10.1145/3081333.3081337
[39]JinWu,JianDong,RuiliFang,WenwenWang,andDechengZuo.2020. PerfDBT:
Efficient Performance Regression Testing of Dynamic Binary Translation. In
2020 IEEE 38th International Conference on Computer Design (ICCD) . 389â€“392.
https://doi.org/10.1109/ICCD50377.2020.00071
[40]Jin Wu, Jian Dong, Ruili Fang, Ziyi Zhao, Xiaoli Gong, Wenwen Wang, andDecheng Zuo. 2021. Effective Exploitation of SIMD Resources in Cross-ISAVirtualization. In Proceedings of the 17th ACM SIGPLAN/SIGOPS International
Conference on Virtual Execution Environments (Virtual, USA) (VEE 2021) . Associa-
tionforComputingMachinery,NewYork,NY,USA,84â€“97. https://doi.org/10.
1145/3453933.3454016
[41]ZiyiZhao,ZhangJiang,YingChen,XiaoliGong,WenwenWang,andPen-Chung
Yew. 2021. Enhancing Atomic Instruction Emulation for Cross-ISA Dynamic
Binary Translation. In 2021 IEEE/ACM International Symposium on Code Genera-
tionandOptimization(CGO).351â€“362. https://doi.org/10.1109/CGO51591.2021.
9370312
[42]Ziyi Zhao, Zhang Jiang, Ximing Liu, Xiaoli Gong, Wenwen Wang, and Pen-Chung Yew. 2020. DQEMU: A Scalable Emulator with Retargetable DBT onDistributed Platforms. In 49th International Conference on Parallel Processing -
ICPP(Edmonton, AB, Canada) (ICPP â€™20). Association for Computing Machinery,
NewYork,NY,USA,Article7,11pages. https://doi.org/10.1145/3404397.3404403
908
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:36 UTC from IEEE Xplore.  Restrictions apply. 