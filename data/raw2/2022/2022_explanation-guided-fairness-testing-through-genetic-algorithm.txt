Explanation-Guided Fairness Testing through Genetic Algorithm
Ming Fan
mingfan@mail.xjtu.edu.cn
Xiâ€™an Jiaotong University, ChinaWenying Wei
waving@stu.xjtu.edu.cn
Xiâ€™an Jiaotong University, ChinaWuxia Jin
jinwuxia@mail.xjtu.edu.cn
Xiâ€™an Jiaotong University, China
Zijiang Yang
zijiang@xjtu.edu.cn
Xiâ€™an Jiaotong University, ChinaTing Liuâˆ—
tingliu@mail.xjtu.edu.cn
Xiâ€™an Jiaotong University, China
ABSTRACT
The fairness characteristic is a critical attribute of trusted AI
systems. A plethora of research has proposed diverse methods
for individual fairness testing. However, they are suffering from
threemajorlimitations,i.e.,lowefficiency,loweffectiveness,and
model-specificity. This work proposes ExpGA, an explanation-
guided fairness testing approach through a genetic algorithm (GA).
ExpGA employs the explanation results generated by interpretable
methods to collect high-quality initial seeds, which are prone to
derivediscriminatorysamplesbyslightlymodifyingfeaturevalues.ExpGAthenadoptsGAtosearchdiscriminatorysamplecandidates
byoptimizingafitness value.Benefitingfromthiscombinationof
explanationresultsandGA,ExpGAisbothefficientandeffectiveto
detect discriminatory individuals. Moreover, ExpGA only requires
prediction probabilities of the tested model, resulting in a bettergeneralization capability to various models. Experiments on
multiplereal-worldbenchmarks,includingtabularandtextdatasets,show that ExpGA presents higher efficiency and effectiveness than
four state-of-the-art approaches.
CCS CONCEPTS
â€¢Software and its engineering â†’Software creation and
management.
KEYWORDS
Explanation result, fairness testing, genetic algorithm
ACM Reference Format:
Ming Fan, Wenying Wei, Wuxia Jin, Zijiang Yang, and Ting Liu. 2022.
Explanation-GuidedFairnessTestingthroughGeneticAlgorithm.In 44th
International Conference on Software Engineering (ICSE â€™22), May 21â€“29,
2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510137
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35101371 INTRODUCTION
ArtificialIntelligence(AI)systemshavebeenpenetratingvarious
aspects of our lives. Except for the benefits brought by AI systems,
however, an AI system would harm the fairness of our society if it
makes biased decisions and thus presents a discriminatory nature.
Recentstudies[ 18,38]pointedoutthatsomefamiliarAIsystems
behavewithdeviationsingenderandskincolor.Forexample,inface
recognition systems developed by IBM, Microsoft, and face++, the
detection accuracy for white-skin men is much higher (sometimes
20% higher) than that of black-skin women.
Fairness characteristic is critical for trusted AI systems, and
fairness testing has attracted much attention recently in both
industrial (e.g., IBMâ€™s AI Fairness 360 [ 6], Googleâ€™s ML-Fairness-
Gym[9])andacademiccommunities[ 29,36,39].TheGDPR[ 10],a
dataprivacylawacrossEurope,claimsthatâ€œthepersonaldatashall
be processed fairly in relationto the data subjectâ€. Definitions of
fairness in existing work fall into two categories [ 29]: 1) individual
fairness,i.e.,similarindividualsaregivensimilardecisions;2)group
fairness, i.e., equal decisions are made for different groups. Most
research on software fairness, including this work, has focused on
individual fairness [17].
Individual fairness testing mutates the input samples to
generate sufficient testing samples and checks whether these
samples are individual-discriminatory. Typical approaches include
THEMIS[ 24],AEQUITAS[ 40],SG[15],ADF[42],andMT-NLP[ 28].
In general, these fairness testing methods first construct a set of
seed samples that are suspiciously discriminatory using random or
clusteringtechniques.Then,theysearchformorediscriminatory
samples around the seeds through different strategies, such as
symbolicexecutionandgradient-basedmethods.Afterthat,they
expand the original dataset by modifying the labels of detecteddiscriminatory samples and finally retrain to improve the model
fairness.Priorindividualfairnesstestingapproacheshaveachieved
relatively good performance;however, theystillsuffer fromthree
major limitations.Low efficiency
:Whenhandlingcomplexdeeplearningsystems,
SG[15]isinefficientduetotheuseofsymbolicexecution.Symbolic
execution,whichtypicallyrequiresSMTsolvers,iscomputationally
expensive to generate particular test cases.Low effectiveness
: THEMIS [ 24], AEQUITAS [ 40], and MT-
NLP [28] adopt unguided or semi-guided search strategies, leading
to a low success rate of the discriminatory sample generation.Model-specificity
: ADF [42], a lightweight approach, requires
gradientinformation.Suchinformationisextractedfromthewhite-
box DNN model but unavailable in black-box models.
8712022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ming Fan and Wenying Wei, et al.
Label: Incomeàµ50KLabel: Incomeàµ‘50K
same samenative_country sexâ€¦â€¦ marital-status
Figure 1: An example pair of discriminatory samples
To address these limitations, this work proposes ExpGA,
a model-agnostic fairness testing approach. ExpGA employs
explanation results and the genetic algorithm (GA), thus detecting
discriminatory samples both efficiently and effectively.
Our ExpGA first leverages the explanation results generated
by interpretable methods to identify a set of seed samples. Seed
samples are prone to derive discriminatory samples by slightlymodifying their feature values. Then, using seed samples asthe input, ExpGA employs the GA to produce large amount of
discriminatoryoffspring.InExpGA,theexplanationresultsfavor
high-quality seeds. GA can guide the search process towardsdiscriminatory sample solutions by optimizing the fitness value.Furthermore, ExpGA is able to handle black-box models since it
only requires prediction probabilities from the models.
To evaluate the efficiency and effectiveness of our ExpGA,
we compare ExpGA with four state-of-the-art discriminatory
testingapproaches,includingAEQUITAS,SG,ADF,andMT-NLP.
Using five diverse training models, we conduct comprehensiveexperiments on three tabular dataset benchmarks (handled by
AEQUITAS, SG, and ADF) and two text dataset benchmarks
(handledbyMT-NLP).Thedataofourexperimentscanbefound
on github1.
The results indicate that ExpGA can detect discriminatory sam-
plesmoreefficientlyandeffectivelythanthebaselineapproaches.
Inparticular,ontabulardatasets,ExpGAconsumeslessthan0.2s
toidentifyadiscriminatorysamplewithasuccessrateof49%on
average,outperformingAEQUITAS,SG,andADFinbothefficiency
and accuracy. On text datasets, ExpGA is five times more efficient
andtwicemoreeffectivethanMT-NLP.Inaddition,theperformance
ofExpGAismorestableinvariousmodelsthanthefourbaseline
approaches, indicating a better generalization capability of ExpGA.
In summary, our work makes the following contributions:
(i)We propose a model-agnostic fairness testing approachcalled ExpGA. It can identify individual discriminatorysamples for both tabular data and text data by combining
the local explanation and GA.
(ii)We conduct extensive experiments on three tabular datasets
andtwotextdatasetstoevaluateExpGA.Theresultsshow
that ExpGA outperforms four state-of-the-art approaches in
terms of both efficiency and effectiveness.
In the rest of this paper: Section 2 introduces the background
andtheproblemdefinition.Section3illustratesExpGAandSection
4reports experimentalresults.Section5 and6discussthe threats
to validity and related work. Section 7 draws conclusions.
1https://github.com/waving7799/ExpGA2 BACKGROUND AND PROBLEM
DEFINITION
2.1 Individual Discrimination
There exists individual discrimination in a classifier if some
protected attributes explicitly play important roles in the decision-
making process [ 20,31]. The protected attributes are predefined in
specific domains such as gender,country, and age.
Formally, given a classifier ğ‘“=<ğ‘‹,ğ´>,ğ‘‹denotes the input
domain,and ğ´=ğ´1Ã—ğ´2Ã—Â·Â·Â·Ã—ğ´ğ‘›representsthefeaturespaceof ğ‘‹.
Each input sample ğ‘¥âˆˆğ‘‹is represented as a ndimensional feature
vector, i.e., ğ‘¥={ğ‘1,ğ‘2,...,ğ‘ğ‘›}. Considering a set of protected
attributes ğ‘ƒ,aclassifier ğ‘“presentsindividualdiscriminationifthere
existsapairofsamples, ğ‘¥andğ‘¥/prime,satisfyingthefollowingEq.(1)-(3),
whereğ‘¥={ğ‘1,ğ‘2,...,ğ‘ğ‘›}andğ‘¥/prime={ğ‘/prime
1,ğ‘/prime
2,...,ğ‘/prime
ğ‘›}. In this case, ğ‘¥
andğ‘¥/primeareapairofdiscriminatorysamplesin ğ‘“.Hereweusethe
symbols âŠ¿and/trianglerightequaltodenotethatonefeatureiscloselyrelatedand
unrelated to a specific protected attribute, respectively.
âˆƒğ‘ğ‘–âˆˆğ‘¥,ğ‘ğ‘–âŠ¿ğ‘âˆˆğ‘ƒ,ğ‘ğ‘–â‰ ğ‘/prime
ğ‘– (1)
âˆ€ğ‘ğ‘—âˆˆğ‘¥,ğ‘ ğ‘—/trianglerightequalğ‘âˆˆğ‘ƒ,ğ‘ ğ‘—=ğ‘/prime
ğ‘— (2)
ğ‘“(ğ‘¥)â‰ ğ‘“(ğ‘¥/prime) (3)
Fig.1presentsanexamplepairofdiscriminatorysamplesthat
areclassifiedintodifferentclasses.Theonlydifferencebetweenthe
twosamplesistheirvaluesofsexfeature,whichisrelatedtothe
protectedattribute gender,indicatingtheunfairnessoftheclassifier
withregardto gender.Notethattheapproachofignoringcertain
protectedattributestomitigatemodelbiasisnoteffectivedueto
the presence of redundant encoding in the training dataset [32].
2.2 Locally Interpretable Method
The drawbacks of invisible black-box models raise a series of
security issues; many studies have proposed interpretable methods
to measure the credibility of the prediction process.Among them,
the locally interpretable methods [ 25â€“27,34,35] are the most
prominent. They interpret the predictions of black-box classifier
models by providing potentially important features that dominate
the decision results. By approximating the decision boundaryof the classifier model, the generated interpreter learned withinterpretable methods can explain the prediction specificallyfor a single sample. Concretely, given a test sample
ğ‘¥, and its
classification result ğ‘¦=ğ‘“(ğ‘¥), an interpreter ğ‘”will output
explanation results ğ‘’, a sorted set of features ranking based on
their importance to this decision result.
2.3 Genetic Algorithm
The genetic algorithm (GA) is commonly used to search for
approximate optimal solutions [ 30,37,43]. GA first randomly
initializes a population with potential solutions to the targetproblem. It then approximates the optimal solution iteratively
throughthreebiologicallyinspiredoperators,includingselection,
crossover, and mutation.
Specifically,GAencodeseachsolutionintheinitialpopulation
into a set of â€œgeneâ€ sequences and designs a fitness function tomeasure the qualities of current solutions. In the selection step,solutions with better fitness scores tend to be selected into the
872
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. Explanation-Guided Fairness Testing through Genetic Algorithm ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Model
DatasetInitialization Phase
PreprocessingExplanation
GenerationConstruction of 
Seed Sample Set rankNum< ß
Selection Crossover Mutation
İİİÜ«àµİˆ
Optimization PhaseDiscriminatory 
Samples
Figure 2: The overview of ExpGA.
newpopulation.Inthecrossoverstep,twoparentsolutionsâ€™gene
fragmentsareexchangedtoproducechildrensolutions.Afterthat,a
part of gene fragments of children solutions are mutated randomly,
probably breaking out of the local search space and enhancing the
diversityofsolutions.GArepeatsthewholeprocessuntila desirable
solutionisfoundorthenumberofiterationsreachesthemaximum
as configured.
2.4 Problem Definition
For a given model, if the input is identified as an individual dis-
criminatory sample, this model would be suffered from individual
discrimination and may produce a biased decision. In this software
fairness testing work, we assume that the dataset ğ‘‹for training
a model, the corresponding feature space ğ´, and the protected
attribute set ğ‘ƒare provided. The problem is that, given a black-
box model ğ·trained using dataset ğ‘‹, can we effectively and
efficiently detect individual discriminatory samples for ğ·?B y
supplementingthediscoveredindividualdiscriminatorysamplesto
ğ‘‹,themodelfairnessisdesiredtohaveanimprovementthrough
retraining.Wehavenopriorknowledge(i.e.,trainingalgorithms
andparameters)abouttheblack-boxmodel,makingexistingwhite-boxbasedmethodsthatrequiremodelinnerinformationineffective.
3 METHODOLOGY
Inthispaper,weproposetheExpGA,alightweightmodel-agnostic
software fairness testing method. Fig. 2 illustrates the overallframework of ExpGA. The inputs are a dataset and its trained
model. In the initialization phase, ExpGA employs an interpretable
method to search for seed samples in the entire feature domain.
Seed samplesare more likely to derivediscriminatory samples by
slightly modifyingfeature values thanother samples. Usingseeds
asinputs,the optimizationphase employsGAtoefficientlygenerate
a large number of discriminatory samples through the selection,
crossover, and mutation operators.
To illustrate the generalizability of ExpGA, we present two
diversescenariosthatExpGAisabletohandle:theinputdatasetis a tabular form or a set of text paragraphs. Fig. 3 shows thetwo corresponding examples. The first example is selected from
thecensusincomeclassificationdataset[ 4],asshowninFig.3(a).
Each sample includes features of marital status ,sex,occupation,Sample 1:
Sample 2:no       male    lawyer       1         50h
yes      male   teacher      3        36h
â€¦â€¦Indeed, in my opinion, the movie itself rates as one the all-
time great experiences of silent cinema. A creative artist of th e 
first rank, robertson is a master of pace, camera angles and montage. He has also drawn brilliantly natural performances 
from all his playersâ€¦â€¦(a) An example in the census income classification dataset
(b) An example in the sentiment analysis datasetLabel: Income>50k 
Label: Income àµ‘50k
Label: Positive Features:
marital status
sexoccupation
educationhours per week
Figure3:Twoexamplesintabulardatasetandtextdatasetscenarios.
education, hours per week, etc. The labels of sample 1 and sample 2
areIncome >50kandIncomeâ‰¤50k, respectively. Another example is
selected from the sentiment analysis dataset for movie comments
(i.e.,IMDBdataset[ 8]).AsshowninFig.3(b),eachsampleinthis
dataset is a text paragraph. Although the samples in the above two
differentscenariosareheterogeneous,theybothcanbeconsistently
represented as feature vectors, ğ‘¥={ğ‘1,ğ‘2,...,ğ‘ğ‘›}. For tabular
samples, ğ‘ğ‘–denotes the ğ‘–-th feature value; for text samples, ğ‘ğ‘–
denotes the ğ‘–-th word. For convenience, we will uniformly use
ğ‘ğ‘–to denote the ğ‘–-th word of sample ğ‘¥.
3.1 InitializationPhase
Theinitializationphasefirstidentifieswhetherasamplecontains
sensitive words that are related to the given protected attributes
ğ‘ƒ. Then it learns an interpreter by employing the interpretable
methods, outputting the words that dominate decision results. The
samples whose sensitive word ranks high in these explanation
results are selected as the seed samples. Concretely, this phasecontains three steps, i.e., prepossessing, explanation generation,
and construction of the seed sample set.
3.1.1 Preprocessing. This step will identify the sensitive words
that are related to the protected attributes. Considering that the
protectedattributeis {ğ‘”ğ‘’ğ‘›ğ‘‘ğ‘’ğ‘Ÿ},wecaneasilyidentifythattheword
â€œsexâ€inFig.3(a)isdefinitelyclosetotheattributesincetheypresent
a similar semantic meaning. However, in some cases like Fig. 3(b),
it is difficult to literally decide the relationships between the words
and protected attributes due to the diversity of the language. Inthis example, the word â€œmasterâ€ might be related to the gender
information implicitly.
Tosolvethisproblem,wemanuallyconstructaknowledgegraph
containing the relations between sensitive words and protected
attributes. At first, we obtain the protected attributes following an
existing work [ 11,20,23]. Then, we construct a set of general
relations, such as â€œIsAâ€, â€œRelatedToâ€, â€œDistinctFromâ€, accordingto the ConceptNet [
5], a widely used, open, and multilingual
knowledgegraph.Afterthat,weexpandthegraphbyaddingsimilar
wordswithawordembeddingtoolsuchas Glove[33].Foranew
word, if there exists a similar word in the graph of which the
similarity surpasses the configured threshold (i.e., 0.7), it will be
873
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ming Fan and Wenying Wei, et al.
Knowledge 
Graphgender
has
countryhasreligionhas
...hasmaster father motherDistinctFrom
IsA
Christianity
Catholicism 
â€¦Islamism IsA America
Spanish 
â€¦African IsAâ€¦
agepersonhas has
Figure 4: A partial prior knowledge graph.
Algorithm 1: Initialization phase
Input:ğ‘‹,ğ‘ƒ, Classifier ğ‘“, Interpreter ğ‘”,ğ‘ ğ‘’ğ‘’ğ‘‘_ğ‘›ğ‘¢ğ‘š,ğœ–,
Output:Seed Sample Set ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡
1ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡ =âˆ…
2foreachsampleğ‘¥inğ‘‹do
3if|ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡|â‰¤ğ‘ ğ‘’ğ‘’ğ‘‘_ğ‘›ğ‘¢ğ‘šthen
4 ğ‘™ğ‘ğ‘ğ‘’ğ‘™=ğ‘“(ğ‘¥)
5 ğ‘’=ğ‘”(ğ‘¥,ğ‘“,ğ‘™ğ‘ğ‘ğ‘’ğ‘™ )
6 foreachğ‘ğ‘–inğ‘¥do
7 ifğ‘ğ‘–âŠ¿ğ‘,ğ‘âˆˆğ‘ƒthen
8 ğ‘Ÿ=ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘ğ‘¢ğ‘š (ğ‘ğ‘–,ğ‘’)
9 ifğ‘Ÿâ‰¤ğœ–then
10 ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡ =ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡ âˆª{ğ‘¥}
11returnğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡
addedtothegraph.Finally,onceawordhasanaccessiblepathto
the protected attribute in the graph, it is regarded as a sensitive
word that is related to the specific protected attribute. Note that
the knowledge graph only needs to be constructed once.
Fig. 4 depicts a sub-graph of the knowledge graph we have
constructed.Thesub-graphcontainsasubsetofprotectedattributes,
such asgender,age,country, and religion. An edge of this graph
indicates a relation from a subject to an object. For instance,
America â€œIsA" a country. Based on this graph, for the example
shown in Fig. 3(b), we can identify that â€œmasterâ€ is a person and it
hasthegenderattribute.Thus,itisidentifiedasasensitiveword
related to gender, that is, ğ‘šğ‘ğ‘ ğ‘¡ğ‘’ğ‘Ÿ âŠ¿ğ‘”ğ‘’ğ‘›ğ‘‘ğ‘’ğ‘Ÿ.
3.1.2 Explanation Generation. Our assumption of using inter-
pretable methods is that non-sensitive words should have a
synergistic influence, dominating the classification of samples
withoutdiscrimination.Onthecontrary,fordiscriminationsamples,
their sensitive words would rank relatively high in explanationresults. As a result, a slight perturbation of sensitive words ondiscrimination samples might dramatically change the predictedlabel. Furthermore, we rely on the model-agnostic interpretable
methodssincetheyaregoodattraininganinterpreterforanopaque
model, thus benefiting the generalization of ExpGA to different
models such as CNN, MLP, and SVM.
Fig.5presentstheexplanationresultsforthetwoexamplesin
Fig.3.Thewordsâ€œmaleâ€andâ€œmasterâ€aremarkedwithredcolor
sincetheyareidentifiedassensitivewordsduringtheprepossessing
step. The darker blue shading words play more essential roles in
the classification process than the lighter shading words.Sample 1:
Sample 2:no male lawyer       1        50h
yes      male teacher      3        36h
(a) Explanation result of the first example
(b) Explanation result of the second exampleLabel: Income>50k 
Label: Income àµ‘50k
Label: Positive â€¦â€¦Indeed, in my opinion, the movie itself rates as one the all-
time great experiences of silent cinema. A creative artist of th e 
first rank, robertson is a master of pace, camera angles and 
montage. He has also drawn brilliantly natural performances 
from all his playersâ€¦â€¦
Figure 5: Explanation results of the two examples.
3.1.3 ConstructionofSeedSampleSet. Thisstepconstructsaset
of seed samples, from which more discriminatory samples will
be derived by the genetic algorithm. We first rank the explanation
resultğ‘’basedontheimportancescoreassignedbytheinterpretable
method.Thenwedesignafunction ğ‘Ÿğ‘ğ‘›ğ‘˜ğ‘ğ‘¢ğ‘š (ğ‘ğ‘–,ğ‘’)toobtainthe
rank number of a word ğ‘ğ‘–inğ‘’. By iteratively obtaining the rank
number of each sensitive word, we select the highest number to
checkwhetheritishigherthanathresholdvalue ğœ–.Ifhigher,the
sample will be added to the seed sample set.
ğœ–controls the balance between the number and quality of the
selected seed samples. In Fig. 5, given that ğœ–=2, all samples are
selectedasseedsamplessincetheranknumbersofsensitivewords
containedinthesesamplesare2.However,if ğœ–=1,noneofthem
wouldbeselected.Itcanbeseenthatabigger ğœ–wouldresultinmore
seed samples; however, the rank number of sensitive words might
be lower, making the sensitive words less important. Therefore,
the quality of these samples would be poor. On the contrary, asmaller
ğœ–can result in high-quality seed samples with a smaller
number,whichmightaffecttheefficiencyofthegenerationoffinal
discriminatory samples.
Algorithm1formalizestheinitializationimplementations.The
inputsincludethedataset ğ‘‹,protectedattributeset ğ‘ƒ,aninterpreter
ğ‘”constructed by leveraging the existing interpretable method, and
the target number of seed samples ( ğ‘ ğ‘’ğ‘’ğ‘‘_ğ‘›ğ‘¢ğ‘š). The output is a set
of seed samples, denoted as ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡. Each sample is first fed to the
classifierandinterpretertogenerateitsexplanationresults(lines
2-5).Thesample,whichcontainsanysensitivewordwithasmaller
ranking number than ğœ–, will be added to ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡(lines 6-10).
3.2 OptimizationPhase
The optimization phase leverages the biologically inspired GA
to generate discriminatory samples, taking advantage of its highconvergence speed and strong local searching ability. Here weuse Algorithm 2 to help understand the order of the threeoperators. At first, we will construct the initial population basedon the
ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡input (line 1). Then, the complete process of the
selection, crossover, and mutation (lines 4-6) will be repeated ğ‘™
times, as set by users. After finishing each iteration, we leverage
ğ·ğ‘–ğ‘ ğ‘ğ‘Ÿğ‘›ğ‘–ğ‘šğ‘–ğ‘›ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦ğ¶â„ğ‘’ğ‘ğ‘˜ ()to check whether a candidate sample ğ‘¥
is atruediscriminatory sample. If the output set ğ·ğ‘–ğ‘ ğ‘†ğ‘’ğ‘¡does not
874
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. Explanation-Guided Fairness Testing through Genetic Algorithm ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Algorithm 2: Optimization phase
Input:ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡,ğ‘™
Output:Discriminatory Sample Set ğ·ğ‘–ğ‘ ğ‘†ğ‘’ğ‘¡
1ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ¼ğ‘›ğ‘–ğ‘¡ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘†ğ‘’ğ‘’ğ‘‘ğ‘†ğ‘’ğ‘¡)
2ğ‘›ğ‘¢ğ‘š=0
3whileğ‘›ğ‘¢ğ‘šâ‰¤ğ‘™do
4ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› )
5ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ‘œğ‘£ğ‘’ğ‘Ÿ (ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› )
6ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘œğ‘Ÿ (ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› )
7foreachsampleğ‘¥inğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› do
8 ifğ‘‡ğ‘Ÿğ‘¢ğ‘’==ğ·ğ‘–ğ‘ ğ‘ğ‘Ÿğ‘›ğ‘–ğ‘šğ‘–ğ‘›ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘¦ğ¶â„ğ‘’ğ‘ğ‘˜ (ğ‘¥)then
9 ğ·ğ‘–ğ‘ ğ‘†ğ‘’ğ‘¡=ğ·ğ‘–ğ‘ ğ‘†ğ‘’ğ‘¡âˆª{ğ‘¥}
10returnğ·ğ‘–ğ‘ ğ‘†ğ‘’ğ‘¡
containthesample, ğ‘¥willbeaddedtotheset(lines7-9).Thedetails
of the above steps are introduced below.
3.2.1 ConstructionofInitialPopulation. Thisstepconstructsthe
initialpopulationbasedonseedsamples.However,itisdifferent
to construct the initial populations for the tabular dataset and the
text dataset.
For seed samples from a tabular dataset, all of them will be
added to the initial population since they have the same number of
features.
For seed samples from a text dataset, it is unfeasible to initialize
the population by simply aggregating all seed samples since the
lengths of paragraphs in different text samples are diverse. Asimple replacement of a word
ğ‘ğ‘–in one sample with that word
ğ‘/prime
ğ‘–in another sample (in the next crossover step) would lead to
a new sentence with syntax errors or incorrect semantic. In this
work, we construct an initial population correspondingly from
each seed sample. Specifically, for a given seed sample, we first
randomly select ğ‘˜non-sensitive words from it. Correspondingly, ğ‘˜
newwordsthathavesimilarmeaningswiththose ğ‘˜selectedwords
canbeobtainedthroughthesimilaritycomputationbytheword
embeddingtool Glove.Afterthat,wereplacetheoriginallyselected
words with these new words, producing ğ‘˜new samples derived
fromtheseedsample.Thesetofthese ğ‘˜+1sampleswillforman
initial population. Our work sets ğ‘˜=20 according to experimental
experience.
3.2.2 Selection. This step selects high-quality samples to generate
the next population through optimizing a fitness function. We
designafitnessfunction ğ‘“ğ‘–ğ‘¡(ğ‘¥)forasample ğ‘¥.ğ‘“ğ‘–ğ‘¡(ğ‘¥)quantifiesthe
extentofachangefromthepredictionprobabilityof ğ‘¥/primetothatof
ğ‘¥/prime/prime.ğ‘¥/primeandğ‘¥/prime/primeare derived from ğ‘¥. We assume that ğ‘¥with a higher
fitness score is more likely to be mutated into a discriminatory
sample.ğ‘“ğ‘–ğ‘¡(ğ‘¥)can be obtained with Eq. (4):
ğ‘“ğ‘–ğ‘¡(ğ‘¥)=|ğ‘ƒğ‘Ÿğ‘œğ‘(ğ‘¥/prime,ğ‘™)âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘(ğ‘¥/prime/prime,ğ‘™)| (4)
Wenowintroducehowtoderive ğ‘¥/primeandğ‘¥/prime/primefromğ‘¥.Giventhat ğ‘¥
contains a sensitive word ğ‘ğ‘–âŠ¿ğ‘, we construct the </tildewideğ‘ğ‘–,Â¬/tildewideğ‘ğ‘–>that
isapairofsensitivewordsbutwithoppositesemanticmeanings.
We then substitute /tildewideğ‘ğ‘–andÂ¬/tildewideğ‘ğ‘–for the original ğ‘ğ‘–, producing two
newsamples, ğ‘¥/primeandğ‘¥/prime/prime,respectively. ğ‘ƒğ‘Ÿğ‘œğ‘(ğ‘¥/prime,ğ‘™)andğ‘ƒğ‘Ÿğ‘œğ‘(ğ‘¥/prime/prime,ğ‘™)
correspondtothepredictionprobabilityof ğ‘¥/primeandğ‘¥/prime/primeforthespecific
labelğ‘™.Algorithm 3: Selection of new population
Input:ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
Output:ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
1ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =âˆ…
2ğ‘ ğ‘¢ğ‘šğ¹ğ‘–ğ‘¡=/summationtext.1
ğ‘¥âˆˆğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘“ğ‘–ğ‘¡(ğ‘¥)
3Î”(ğ‘‹)=âˆ…
4foreachsampleğ‘¥inğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› do
5ğ›¿(ğ‘¥)=ğ‘“ğ‘–ğ‘¡(ğ‘¥)/ğ‘ ğ‘¢ğ‘šğ¹ğ‘–ğ‘¡
6 Î”(ğ‘‹)=Î”(ğ‘‹)âˆªğ›¿(ğ‘¥)
7while|ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› |<|ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› |do
8ğ‘§=ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡(ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, Î”(ğ‘‹))
9ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆªğ‘§
10returnğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
Algorithm 4: Crossover and Mutation
Input:ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ,ğ‘ğ‘Ÿ,ğ‘šğ‘Ÿ
Output:ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
1foreachsampleğ‘¥inğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› do
2ğ‘¥/prime=ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ (ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› )
3 <ğ‘ğ‘–,...,ğ‘ ğ‘—>=ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ (ğ‘¥,ğ‘ğ‘Ÿ)
4ğ‘“ğ‘Ÿğ‘ğ‘”(ğ‘¥,ğ‘–,ğ‘—)=<ğ‘ğ‘–,...,ğ‘ ğ‘—>
5ğ‘¥=ğ‘¥.ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ (ğ‘“ğ‘Ÿğ‘ğ‘”(ğ‘¥,ğ‘–,ğ‘—),ğ‘“ğ‘Ÿğ‘ğ‘”(ğ‘¥/prime,ğ‘–,ğ‘—))
6ğ‘¥/prime=ğ‘¥/prime.ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’(ğ‘“ğ‘Ÿğ‘ğ‘”(ğ‘¥/prime,ğ‘–,ğ‘—),ğ‘“ğ‘Ÿğ‘ğ‘”(ğ‘¥,ğ‘–,ğ‘—))
7foreachsampleğ‘¥inğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› do
8foreachğ‘ğ‘–inğ‘¥do
9 ifğ‘ğ‘–/trianglerightequalğ‘then
10 ///trianglerightequaldenotes that ğ‘ğ‘–is not related to ğ‘
11 ğ‘/prime
ğ‘–=ğ‘ğ‘ğ‘™ğ‘†ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘Šğ‘œğ‘Ÿğ‘‘ (ğ‘ğ‘–,ğ‘šğ‘Ÿ)
12 ğ‘¥=ğ‘¥.ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ (ğ‘ğ‘–,ğ‘/prime
ğ‘–)
13returnğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
For example, given ğ‘ğ‘–=â€œactorâ€ and ğ‘=gender, /tildewideğ‘ğ‘–is the same as
ğ‘ğ‘–, andÂ¬/tildewideğ‘ğ‘–would be â€œactressâ€. Another example is that, when ğ‘ğ‘–=
â€œmaster", ğ‘=gender, and ğ‘ğ‘–has a relationship with ğ‘, we would set
/tildewideğ‘ğ‘–=â€œmale master" and Â¬/tildewideğ‘ğ‘–=â€œfemale master" based on the prior
knowledge graph we have constructed in Section 3.1.1.
Algorithm 3 shows the selection in terms of ğ‘“ğ‘–ğ‘¡(ğ‘¥). The input
is current population ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› and the output is a new
population ğ‘ğ‘’ğ‘¤ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› afterselection. ğ‘ ğ‘¢ğ‘šğ¹ğ‘–ğ‘¡denotesthesum
of all fitness scores of the samples in ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› .ğ›¿(ğ‘¥)denotes
the probability value of ğ‘¥that might be selected into the new
population. Î”(ğ‘‹)isasetofall ğ›¿(ğ‘¥)ofsamplesin ğ¶ğ‘¢ğ‘Ÿğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› .
Finally, selecting based on the probability distribution indicated
fromÎ”(ğ‘‹),wecanconstructanewpopulationthathasthesame
sample size as ğ¶ğ‘¢ğ‘Ÿğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› .
3.2.3 Crossover and Mutation. We conduct the crossover and
mutation operators on the selected population to generate more
varieties, expanding the search space of discriminatory samples.
Algorithm4illustratesthecrossover(lines1-6)andthemutation
(lines7-12).Thetwoparameters, ğ‘ğ‘Ÿandğ‘šğ‘Ÿ,denotethecrossover
rate and mutation rate.
Werandomlychoosepairsofsamplesasthecrossoverparents
fromthecurrentpopulationwithaprobabilityof ğ‘ğ‘Ÿ.Forapairof
samples, ğ‘¥andğ‘¥/prime, fragments <ğ‘ğ‘–,...,ğ‘ğ‘—>and<ğ‘/prime
ğ‘–,...,ğ‘/prime
ğ‘—>are
extracted from ğ‘¥andğ‘¥/primerandomly. The two samples will exchange
their fragments with each other.
During the mutation, each word of a sample in the current
populationisreplacedwithitssimilarwordwithaprobabilityof ğ‘šğ‘Ÿ.
875
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ming Fan and Wenying Wei, et al.
Algorithm 5: Checking of discriminator sample
Input:ğ‘¥, Classifier ğ‘“,ğ‘ƒ
Output:ğ·ğ‘–ğ‘ 
1ğ·ğ‘–ğ‘ =ğ‘“ğ‘ğ‘™ğ‘ ğ‘’
2ifâˆƒğ‘ğ‘–âˆˆğ‘¥,ğ‘ğ‘–âŠ¿ğ‘,ğ‘âˆˆğ‘ƒthen
3 </tildewideğ‘ğ‘–,Â¬/tildewideğ‘ğ‘–>=ğ‘”ğ‘’ğ‘¡ğ‘ƒğ‘ğ‘–ğ‘Ÿ(ğ‘ğ‘–)
4ğ‘¥/prime=ğ‘¥.ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ (ğ‘ğ‘–,/tildewideğ‘ğ‘–)
5ğ‘¥/prime/prime=ğ‘¥.ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ (ğ‘ğ‘–,Â¬/tildewideğ‘ğ‘–)
6ifğ‘“(ğ‘¥/prime)â‰ ğ‘“(ğ‘¥/prime/prime)then
7 ğ·ğ‘–ğ‘ =ğ‘¡ğ‘Ÿğ‘¢ğ‘’
8returnğ·ğ‘–ğ‘ 
Sample 1:
Sample 2:no male lawyer       1        50h
yes      male teacher      3        36hcrossoverno male teacher       3 50h
yes      male lawyer      1 36hmutation doctor 5
no 40h 2
â€¦â€¦Indeed, in my opinion , the movie itself rates as one
the all-time marvelous experiences of silent cinema .A
creative artist of the first rank, robertson is a master of
pace, camera angles and montage. He has also drawn
excellent natural performances from all his players â€¦â€¦
â€¦â€¦Indeed, in my opinion ,t h ef i l m itself rates as one
the all-time great experiences of silent theater .A
creative artist of the first rank, robertson is a master of
pace, camera angles and montage. He has also drawn
brilliantly natural performances from all his players â€¦â€¦â€¦â€¦Indeed, in my opinion, the film itself rates as one
the all-time great experiences of silent theater .A
creative artist of the first rank, robertson is a master of
pace, camera angles and montage. H e has also drawn
brilliantly natural performances from all his playersâ€¦â€¦â€¦â€¦Indeed, in my opinion, the movie itself rates as one
the all-time marvelous experiences of silent cinema .A
creative artist of the first rank, robertson is a master of
pace, camera angles and montage. He has also drawn
excellent natural performances from all his playersâ€¦â€¦Sample 1: 
Sample 2: crossover viewfeels
famous participants mutation(a) Crossover and mutation steps of tabular dataset samples
(b) Crossover and mutation steps of text dataset samples
Figure 6: Examples for the crossover and mutation steps.
Note that the mutation excludes the sensitive words. Concretely, 1)
categoricalwordsuchasâ€œnoâ€andâ€œteacherâ€willbereplacedwith
similarwordsinthesamecategorysuchasâ€œyesâ€andâ€œlawyerâ€;2)
text word such as â€œgreatâ€ and â€œbrilliantlyâ€ will be replaced with
similar words using the tool Glove; 3) numeric word such as â€œ1â€
and â€œ36hâ€ can be replaced with numeric data like â€œ2â€ and â€œ40hâ€.
Fig. 6(a) shows the crossover and mutation for the tabular
samplesandFig.6(b)forthetextsamples.InFig.6(a),thecrossover
fragmentsare <â€œlawyerâ€,â€œ1â€ >insample1and <â€œteacherâ€,â€œ3â€ >in
sample 2. Then the words of â€œteacherâ€, â€œ3", â€œyes", â€œ1", and â€˜"36h"are mutated with â€œdoctorâ€, â€œ5", â€œnoâ€, â€œ2", and â€œ40h". In 5(b), the
different words between the two samples are underlined. The two
sub-sentencesintheorangeshadingareexchangedwitheachother.
Afterthat,somewordslikeâ€œexcellent"willbemutatedwiththeir
similar words like â€œfamous".
3.2.4 CheckingofDiscriminatorySample. Aftereachiterationof
theselection,crossover,andmutation,wecheckwhetheragiven
sample is a discriminatory one by identifying their label difference
using Algorithm 5. Specifically, once a sample contains a sensitive
wordğ‘ğ‘–, we leverage the function ğ‘”ğ‘’ğ‘¡ğ‘ƒğ‘ğ‘–ğ‘Ÿto return </tildewideğ‘ğ‘–,Â¬/tildewideğ‘ğ‘–>.
Then, we replace the word ğ‘ğ‘–with the new words and obtain
two new samples. Once the two new samplesâ€™ predicted labels
aredifferent,thegivensamplewillbecheckedasadiscriminator
one.4 EVALUATION
In this section, we evaluate the performance of ExpGA on boththe tabular datasets and text datasets. We first introduce our
experimentalsetup(Section4.1).Thenweconducttheevaluation
by answering the three research questions.
RQ1:WhatistheperformanceofExpGAinfindingdiscriminatory
samples on tabular datasets? (Section 4.2)
RQ2:WhatistheperformanceofExpGAinfindingdiscriminatory
samples on text datasets? (Section 4.3)
RQ3:Towhatextentdothediscriminatorysamplesgeneratedby
ExpGA improve the model fairness through retraining? (Section 4.4)
4.1 Experimental Setup
4.1.1 Datasets. We conduct the evaluation on fivepopularpublic
benchmark datasets, including three tabular datasets and two text
datasets. Their descriptions are listed below:
â€¢Census Income Dataset[4] labels whether the income of
anadultisover$50Kornot.Thereare32,561samplesand
13 features, involving three protected attributes, i.e., gender,
age, andrace.
â€¢German Credit Dataset [7] is used to assess the credit
level of applicants (i.e., good or bad) based on their personal
conditions. There are 600 samples depicted by 20 diverse
features. The protected attributes are genderandage.
â€¢Bank Marketing Dataset [3] is related to the direct
marketing campaign of a Portuguese banking institution.
Theobjectiveistopredictwhethertheclientwouldsubscribe
toatermdepositbasedontheirinformation.Thereare16
features and 45,211 samples. The protected attribute is age.
â€¢IMDBDataset[8] isalargemoviereviewdatasetforbinary
sentimentclassification.Theoriginaldatasetcontains50,000
samples, where each sample contains 210 words on average.
The protected attributes are genderandcountry.
â€¢SST Dataset [13] is also used to classify whether a movie
comment is positive or negative. The original dataset
contains 9,613 samples, where each one has about 19 words
onaverage.Theprotectedattributesarethesameasthose
of the IMDB dataset.
For convenience, we use Cencus, Credit, and Bank to denote the
abbreviations of the three tabular datasets.
4.1.2 BaselineApproaches. Onthetabulardatasets,weselectthree
state-of-the-artapproaches,i.e.,AEQUITAS,SG,andADF,asthe
baseline approaches. Their descriptions are listed below:
â€¢AEQUITAS [40] : It is a two-phase method to search for
individual discriminatory samples, where the search is
random in its global phase and is directed in its local phase.
AEQUITAS proposes three schemes, i.e., random scheme,
semi-directedscheme,andfully-directedscheme,toguide
thelocalstage.Afully-directedschemeperformsbest,which
is adopted in our comparative experiment.
â€¢SG [15]: SG is based on program symbolic execution
and interpretable method. Instead of directly obtainingexplanation results for a single sample, SG uses LIME to
generate local perturbation samples to builda decision tree
and generates test inputs by solving path constraints.
876
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. Explanation-Guided Fairness Testing through Genetic Algorithm ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: Training parameters of different models.
Dataset ModelTraining Parameters
Cencus SVMrbf kernel
Credit RF100 trees
Bank MLPfive-layer fully-connected NN
(128 neurons each layer)
IMDB CNNone convolution layer,one pooling layer,one fully-connected NN layer
SST LRl2 penalty
â€¢ADF [42] :It searchesforseed discriminatorysamplepairs
along with the gradient direction and generates morediscriminatory samples locally guided by gradients in an
opposite way.
Inourexperiments,weimplementAEQUITASandADFbasedon
their public repositories published on Github [ 1,2]. We implement
SG based on the refactored version available in the ADF repository.
The parameter settings in experiments are the same as the optimal
settings given in their papers.
On the text datasets, we use MT-NLP as the baseline approach.
â€¢MT-NLP [28] producestestinputsaccordingtoMetamor-
phic Testing (MT). MT-NLP uses a knowledge graph to
locatesensitivewordsinasentence.Afterward,itemploys
two mutation schemes to generate perturbed sentences,
followedbyadiscriminatorycheckingonalltheseperturbed
sentences.
Given that our proposed approach and the baselines contain
random factors when selecting seed samples, we conduct the
experiment30timesandusetheaveragevaluesontabulardatasets.NotethatfortheMT-NLP,sinceitssourcecodeisnotavailable,we
conduct the comparison using its paper results.
4.1.3 TrainedModelsandInterpreters. Toevaluatethegeneraliz-
ability of ExpGA, we conduct ExpGA on various models, which
are also used in the baseline approaches. As shown in Table 1, we
train the SVM, Random Forest(RF), and MLP models on tabulardatasets, and train the CNN and Logistic Regression(LR) models
on text datasets. Here the traditional machine learning models, i.e.,
RF, SVM, and LR models, are trained via scikit-learn [ 12] library,
and the deep learning models such as MLP and CNN are trained
via TensorFlow [ 14]. Table 1 lists key parameters for training these
models.Theotherparametersabsentinthistableusethedefault
values configured by scikit-learn and TensorFlow.
Based on the trained models, we leverage existing locally
interpretable methods, LIME [ 34] and SHAP [ 27], to generate
corresponding interpreters. Recent empirical studies [ 22,41]o n
differentinterpretablemethodspresentthatLIMEperformsbeston
tabular datasets and SHAP outperforms the others on text datasets.
Consistently, our experiments use LIME for tabular datasets and
SHAP for text datasets by default.
4.1.4 Measurement Metrics. We use DSS and SUR metrics to
measure the efficiency and effectiveness of discriminatory sample
detectionbydifferentmethods.WeassumethatadiscriminatoryTable 2: Parameter settings in different datasets.
Dataset ğœ–ğ‘ ğ‘Ÿğ‘š ğ‘Ÿ
Cencus 7 0.9 0.05Credit 14 0.9 0.05Bank 9 0.9 0.05IMDB 20 0.5 0.05SST 20 0.5 0.05
sample detection method presents a better performance if it
achieves a smaller value of DSS and a larger value of SUR.
DSS denotes the average consumed time for generating a
discriminatory sample,
ğ·ğ‘†ğ‘†=ğ‘‡ğ‘–ğ‘šğ‘’
ğ·ğ‘†ğ‘(5)
whereğ‘‡ğ‘–ğ‘šğ‘’andğ·ğ‘†ğ‘denotetheconsumedtimeandthenumber
ofdetecteddiscriminatory samples,respectively.SURdenotes the
success rate for generating discriminatory samples,
ğ‘†ğ‘ˆğ‘…=ğ·ğ‘†ğ‘
ğ‘‡ğ‘†ğ‘(6)
where TSN is the total number of generated testing samples. It
is worth noting that when calculating the number of generatedtestingsamples,weonlyconsiderthesamplesthatarefedtothe
classifiers to perform discriminatory checking.
4.1.5 Parameters Setting. Recall that ExpGA involves three impor-
tant parameters, including ğœ–for seed sample selection, ğ‘ğ‘Ÿfor the
crossover, and ğ‘šğ‘Ÿfor the mutation. Table 2 lists these parameter
values configured for each dataset.
To configure an appropriate ğœ–value for each dataset, we first
randomly select 100 samples from each dataset and generateexplanation results. Then, we rank these samples in increasing
orderaccordingtotheirsensitivewordrankingnumbers. ğœ–valueis
setastherankingnumberofthe20ğ‘¡â„sample,indicatingthatabout
20% of the samplesare regarded as seed samples. We configure ğ‘ğ‘Ÿ
andğ‘šğ‘Ÿaccording to our experimental experience.
OurexperimentsareconductedonaserverwithUbuntu18.04
operatingsystem,IntelXeon2.50GHzCPU,NVIDIARTXGPU,and
128GB system memory.
4.2 Performanceon Tabular Datasets
ToanswerRQ1,wecompareExpGAwithAEQUITAS,SG,andADF
on three tabular datasets. For the Census and Bank datasets, we
first randomly select 1,000 samples from each as the input. For the
Creditdatasetthatcontains600 samples,weinputallthesamples
into the methods. Then, we insert a timer in each method to count
the generated discriminatory samples every five minutes during
aperiodofonehour.Afterthat,wemeasureTSN,DSN,DSS,and
SUR for each method.
Table3,Table4,andTable5presentperformancemeasurements
of the four methods based on MLP, RF, and SVM models,
respectively. Since the ADF method requires gradient information
which is unavailable in RF and SVM models, ADFâ€™s results are
absent in Table 4 and Table 5. Four main observations are listed
below.
877
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ming Fan and Wenying Wei, et al.
Table 3: The performance of detecting discriminatory samples for four approaches using MLP model.
DatasetExpGA AEQUITAS SG ADF
TSN DSN DSS SUR TSN DSN DSS SUR TSN DSN DSS SUR TSN DSN DSS SUR
Censusgender 491,323 141,049 0.03 28.70% 85,629 1,797 2.00 2.10% 7,070 871 4.13 12.31% 116,119 24,974 0.14 21.51%
age 43,465 29,467 0.12 67.79% 12,243 1,107 3.25 9.04% 4,190 1,530 2.35 36.51% 40,135 21,795 0.17 54.30%
race 90,303 30,171 0.12 33.41% 22,053 551 6.53 2.50% 4,355 588 6.12 13.50% 47,296 8,097 0.44 17.12%
Creditgender 281,130 65,009 0.06 23.12% 38,096 1,596 2.26 4.19% 7,569 1,321 2.73 17.45% 62,242 8,052 0.45 12.93%
age 46,357 34,048 0.11 73.44% 11,756 5,146 0.70 43.77% 7,699 4,659 0.77 60.5% 21,649 5,957 0.60 27.51%
Bank age 46,802 27,099 0.13 57.90% 14,808 2,915 1.23 19.68% 3,776 2,592 1.39 68.63% 27,325 11,852 0.30 43.37%
Table 4: The performance of detecting discriminatory samples for three approaches using RF model.
DatasetExpGA AEQUITAS SG
TSN DSN DSS SUR TSN DSN DSS SUR TSN DSN DSS SUR
Censusgender 274,521 156,259 0.02 56.92% 56,776 1,210 2.98 2.13% 10,906 595 6.05 5.46%
age 51,192 46,035 0.08 89.93% 21,330 13,472 0.27 63.16% 17,400 12,441 0.29 71.50%
race 78,349 47,810 0.08 61.02% 29,309 362 9.94 1.24% 12,572 894 4.03 7.11%
Creditgender 20,295 4,247 0.85 20.93% 3,245 148 24,32 4.56% 7,269 2,192 1.64 30.15%
age 10,183 3,673 0.98 36.07% 869 153 23.53 17.60% 6,349 2,654 1.36 41.80%
Bank age 13,284 8,885 0.41 66.99% 3,952 941 3.83 23.81% 3,871 2,530 1.42 65.36%
Table 5: The performance of detecting discriminatory samples for three approaches using SVM model.
DatasetExpGA AEQUITAS SG
TSN DSN DSS SUR TSN DSN DSS SUR TSN DSN DSS SUR
Censusgender 804,328 145,226 0.02 18.06% 109,706 386 9.33 0.35% 1,119 0 - 0%
age 124,816 78,064 0.05 62.54% 81,275 2,489 1.45 3.06% 2,998 624 5.77 20.81%
race 226,434 78,627 0.05 34.72% 62,954 277 13.0 0.44% 1,141 0 - 0%
Creditgender 118,428 47,570 0.08 40.17% 152,432 3,683 0.98 2.42% 9,204 1,912 1.88 20.77%
age 21,058 9,377 0.38 44.53% 53,344 8,419 0.43 15.78% 7,952 1,918 1.88 24.12%
Bank age 42,746 29,574 0.12 69.19% 31,276 41 87.80 0.13% 744 248 14.52 33.33%
â€¢FortheTSN,ExpGAoutperformsthebaselineapproaches
significantly, indicating that the size of search space via
ExpGA is much bigger than other approaches. The main
reason is the crossover and mutation operators employed in
ExpGA,benefitingan efficientgenerationoflargenumbers
of new testing samples and avoiding a localized search.
â€¢FortheDSNandDSS,ExpGAcangenerateabout4timesthe
discriminatory samples compared with ADF. The advantage
ofExpGAisprominentwhencomparedwithAEQUITASand
SG. For example, in the Census dataset with genderas the
protected attribute, ExpGA requires only 0.03s (indicated by
DSS) to generate a discriminatory sample while AEQUITAS
and SG require 2s and 4.13s.
â€¢For the SUR, in most datasets, ExpGA presents the besteffectiveness, except for a few cases where SUR value by
ExpGAisslightlysmallerthanthatofSG.Forexample,on
theBankdataset,theSURbyExpGAis57.90%whileSURbySGis68.63%.Wefigureoutthatthisexceptionalobservation
is caused by the time period setting. That is, SUR by ExpGA
has not achieved a stable value within a time of one hour
while SG achieves its highest SUR value. After we extended
the time to two hours, the SUR of ExpGA surpasses SG.
â€¢We observe the stability of methods with changes ofmodels. For all the three models, ExpGA can achieve a
Figure 7: Comparison of the diversity for four approaches.
relativelymorestableperformancewithasmallerDSSand
a bigger SUR than other approaches. The results indicate
abettergeneralizabilityofExpGAondifferentmodels.For
AEQUITAS,intheBankdatasetusingtheSVMmodel,the
SURisonly0.13%,muchlowerthanthose(sometimes63.16%)inMLPandRFmodels.ForSG,intheCensusIncomedataset
using the SVM model, there exist DSN values equal to 0.
878
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. Explanation-Guided Fairness Testing through Genetic Algorithm ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 6: The performance of ExpGA using CNN model.
DatasetExpGA
TSN DSN DSS SUR
IMDBgender 44,381 2,477 16.42 5.60%
country 24,405 357 118.8 1.46%
SSTgender 4,579 105 14.6 2.29%
country 4,669 252 4.8 5.40%
WealsocomparetheGAtorandomexploration,keepingseed
exploration the same. The results show that without GA, all the
SUR values are not higher than 6% and the DSS values are 6 times
ofthoseusingGA,indicatingthatGAissignificantingenerating
discriminatory samples. The details can be found on our github.
Insummary,eventhoughAEQUITASisthefirstapproachthat
proposes a two-phase framework and inspires the following work,
itsrandomlyglobalsearchschemeleadstodetectioninefficiency.
SG has superior effectiveness than ADF in general and sometimes
outperforms ExpGA. However, SG relies on the heavyweight
symbolicexecutionthatrequiresasubstantialtimeconsumption
when generating testing samples. ADF appears to be a good
choice for white-box deep learning models. However, the gradient
information required by ADF is often unavailable in most practical
scenarios.
Furthermore, we measure the diversity of generated discrim-
inatory samples for the above four approaches. Specifically, we
first randomly select 1,000 discriminatory samples on the MLP
modelforeachapproach.Then,wetranslatetheirfeaturesintotwodimensions using PCA. As illustrated in Fig. 7, we can observe that
thecoverageareaofExpGAislargerthanothers,indicatingthat
the discriminatory samples generated by ExpGA present the most
diversity among the four approaches.
Finally,consideringtherandomfactorsintheaboveapproaches,
weusestatisticalteststomeasuretheirsignificanceofDSSinterms
of two metrics, i.e., p-value and effect size [ 16]. To calculate the
p-values,weapplythenon-parametricMann-Whitney-Wilcoxon
tocomparetheDSSofExpGAwiththoseofotherthreebaseline
approaches.Then,tocalculatetheeffectsizemetric,weemploythe
non-parametric Vargha and Delaneyâ€™s /hatwideğ´12statistics. The results
demonstratethatallthep-valuesare0,andalltheeffectsizesare
lowerthanthestandardthresholdvalue,i.e.,0.29,indicatingthat
the DSS of ExpGA is significantly smaller than the others.
Answering RQ 1: Compared with three baseline ap-
proaches, our ExpGA presents the best efficiency andeffectiveness in detecting discriminatory samples from
tabulardatasets.Onaverage,ExpGArequireslessthan0.2s
(indicated by DSS) to detect a discriminatory sample with
about49%(indicatedbySUR)successrate.Moreover,the
performanceofExpGAondiversemodelsismorestable
than those of other approaches, indicating that ExpGAis model-agnostic and can effectively handle black-box
models.Table 7: The performance of ExpGA using LR model.
DatasetExpGA
TSN DSN DSS SUR
IMDBgender 77,898 1,292 13.29 1.66%
country 17,245 257 46.76 1.49%
SSTgender 10,977 620 2.39 5.65%
country 4,932 234 4.90 4.74%
4.3 Performanceon Text Datasets
ToanswerRQ2,werandomlyselect1,000samplesfromtheoriginal
datasetsasourinput.Notethatwesettheiterationparameterfor
the text datasets ğ‘™as 20 to stop our algorithm.
Table 6 and Table 7 list the discriminatory sample detection
results by ExpGA using CNN and LR models, respectively. From
the results, we can draw two main conclusions.
â€¢The results show that ExpGA can effectively detect discrim-
inatory samples. However, the performance measured by
DSSandSUR ontextdatasets isworsethanthat ontabular
datasetsshowninRQ1.Theperformancedifferenceisdue
to that the word replacement in ExpGA employs different
strategiesfortextandtabulardatasets.Ontabulardatasets,a
wordcanbereplacedbymanycandidates.Ontextdatasets,aword only can be finitely replaced by its synonyms to retain
thesyntaxandsemanticcorrectnessofasentenceafterword
substitution.Asaresult,DSNcalculatedontextdatasetsis
smaller than that on tabular datasets.
â€¢We observe that DSS values on the IMDB dataset are bigger
than those on the SST dataset. It indicates that the detection
ontheformerdatasetrequiresagreatertimeconsumption
than that on the latter. The average length of samples inthe IMDB dataset is ten times larger than that in the SSTdataset. The longer the samples, the larger the detection
space, leading to a higher time cost in every iteration.
Moreover,wecomparetheperformanceofourapproachwith
MT-NLP based on CNN and LR models. Since the source code ofMT-NLP is unavailable, we directly use the performance resultsevaluated by the work of [
28]. Table 8 presents the comparison
results on IMDB dataset with genderprotected attribute. Although
ExpGA shows smaller TSN than MT-NLP, its DSS value is about
fivetimessmallerthanMT-NLP,andtheSURvalueisatleasttwice
larger than MT-NLP.
Answering RQ 2: ExpGA outperforms the MT-NLP
baseline,withatleastfivetimestheefficiencyandtwice
theeffectivenessinfindingdiscriminatorysamplesontext
datasets.
4.4 Fairness Improvement through Retraining
To answer RQ3, we employ a data augmented method, used in
AEQUITAS [ 40] and ADF [ 42], to investigate the model fairness
improvement imposed by the discriminatory samples detected by
ExpGA. Following the data augmented method, given a target
model,weexecuteExpGAtogeneratediscriminatorysamplesfrom
879
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ming Fan and Wenying Wei, et al.
Table 8: Comparison results of detecting discriminatory samples between ExpGA and MT-NLP on IMDB dataset.
Dataset ModelExpGA MT-NLP
TSN DSN DSS SUR TSN DSN DSS SUR
IMDB genderCNN44,3812,477 16.42 5.60% 174,183 1,111 90.73 0.64%
LR77,8981,292 13.29 1.66% 174,183 1,322 76.25 0.76%
Table 9: Comparison results of detecting discriminatory samples before and after retraining.
DatasetSample AddedNormal Sample
Testing AccuracyDiscriminatory Sample
Testing PercentageDSS SUR
Before After Before After Before After Impr Before After Impr
Cencusgender 10% 84.65% 84.41% 100% 0.1% 0.03 0.37 12.3x 28.70% 11.14% 61.12%
age 10% 84.65% 83.30% 100% 3.9% 0.12 0.23 1.9x 67.79% 19.87% 70.69%
race 10% 84.65% 84.60% 100% 1.7% 0.12 0.17 1.4x 33.41% 22.50% 32.65%
Creditgender 10% 74.17% 74.22% 100% 2.3% 0.06 0.18 3.0x 23.12% 13.97% 39.57%
age 10% 74.17% 75.81% 100% 7.8% 0.11 0.15 1.4x 73.44% 53.62% 26.98%
Bank age 10% 89.52% 89.40% 100% 8.4% 0.13 0.33 2.5x 57.90% 41.65% 28.07%
IMDBgender 2.58% 87.55% 86.24% 100% 0.67% 16.42 211.48 12.8x 5.60% 0.48% 91.43%
country 0.72% 87.55% 85.70% 100% 0%118.8 334.07 2.8x 1.46% 0.45% 69.18%
SSTgender 7.31% 79.50% 78.82% 100% 0%14.6 107.72 7.4x 2.29% 1.16% 49.35%
country 1.82% 79.50% 77.70% 100% 5.7% 4.8 576.56 120x 5.40% 0.31% 94.26%
adataset.Aportionofthesediscriminatorysamplesareretained
in the original dataset, constructing a new dataset. Based on the
updateddataset,weretrainthemodelandre-executeourExpGA
to test the model fairness.
To verify the fairness of retrained model, we randomly select
1,000discriminatorysamplesastheground-truth.SinceRQ2shows
that the DSNon text datasets ismuch smaller than thatof tabular
datasets,weusehalfofthegenerateddiscriminatorysamplesfor
retraining and the remaining for testing.
TheDatasetcolumn and Sample Added column in Table 9 list
the datasets with the protected attributes, and the percentage of
generateddiscriminatorysamplesthatremainedforconstructing
new datasets in this experiment. On tabular datasets, 10% Ã—
|ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ ğ‘’ğ‘¡ |discriminatorysamples arereservedforretrain-
ing.Ontextdatasets,thepercentagesarecalculatedbasedonthe
size of generated discriminatory samples.
The Columns 3-12 in Table 9 show experiment results, i.e.,
the model accuracy and performance of ExpGA before and after
retrainingmodels. NormalSampleTestingAccuracy indicatesthe
model accuracy calculated against normal samples in the datasets,
andDiscriminatory Sample Testing Percentage is the corresponding
accuracy against the ground-truth discriminatory samples. DSS
and SUR show the efficiency and effectiveness of ExpGA. Impr
columnsdenotetheDSS(orSUR)improvementsof AfterDSS(SUR)
vs.BeforeDSS (SUR). These measurements show that:
â€¢Themodelaccuraciesofdetectingnormalsamplesarenearly
unchangedafterretrainingmodelsondatasetsaugmented
with the discriminatory samples generated by ExpGA. This
observationindicatesthat thedataaugmented methodhas
littleinfluenceontheclassificationperformanceofnormal
samples.â€¢Afterretraining,onaverage,morethan97%oftheoriginal
discriminatorysamplesarenotbiasedtonewmodels,indi-
cating that model fairness has been enhanced significantly.
â€¢TheImprofDSSpresentsthatamuchlongertimeisrequired
todetectadiscriminatorysamplebyExpGAforre-trained
models. For example, in SST dataset using countryas the
protectedattribute,aftersearchingaboutthreehours,only
18 discriminatory samples are detected.
â€¢The SUR values drop byan average of 56% (up to 94.26% in
SSTdataset),indicatingthegreaterdifficultyforasuccessful
generation of a discriminatory sample.
Answering RQ 3: After retraining models via a data aug-
mentedmethodusingdiscriminatorysamplesgenerated
by our ExpGA, the model fairness can be considerably
improved,i.e.,morethan97%oftheoriginaldiscriminatory
samples are not biased to new models. Furthermore, the
efficiencyandeffectivenessof generatingdiscriminatory
samples on new models become much lower compared
with the original models while keeping the testing
accuracies on normal samples unaltered.
5 THREATS TO VALIDITY
5.1 Threats to Internal Validity
Interpretable Methods : Our ExpGA relies on interpretable
methods, such as LIME [ 34] and SHAP [ 27], to obtain explanation
results for an initial seed selection. In practice, it is difficult for
interpretable methods to guarantee perfect explanation results.Consequently, the probably missing or mismatching sensitive
880
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. Explanation-Guided Fairness Testing through Genetic Algorithm ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
featuresmightaffectourseedsamplequality.Tomitigatethisthreat,
our future work will explore more interpretable methods.
Retraining to Improve Model Fairness : In RQ3, we test the
model fairness using a data augmented method by imposing
discriminatorysamplesdetectedbyExpGA.Althoughthemodel
fairness has been improved a lot, we are not claiming that ExpGA
radically combs out the model fairness issue. In the future, a
consideringofthefairnesscharacteristicduringdatapreparation
andmodeltrainingshouldbeapromisingexplorationdirection[ 21].
5.2 Threats to External Validity
Limited Experiment Datasets : We evaluate ExpGA on five
benchmark datasets that are used by baseline approaches. The
resultsdemonstratedthatExpGAoutperformsbaselinesondiverse
benchmarks, though we cannot conclude that ExpGA would
achieveagoodperformanceonotherdatasetswithotherprotected
attributes.Oncenewdatasetsareavailable,wewillfurtherverify
ExpGA.Construction of Knowledge Graph
: We construct a prior
knowledge graph to identify the words related to protected
attributes. Currently, this graph only involves a limited number of
protected attributes but sufficient for our experimental evaluation.
WhenusingExpGAtotestmodelfairnessregardingotherprotected
attributes,theknowledgegraphshouldbeextendedtoexpressmore
domains.
6 RELATED WORK
FairnessTesting : Recently, software fairness testing has received
muchattention.Galhotra etal.[24]pioneeredafairnesstestmethod
namedTHEMIS.THEMISdistinguishesthegroupdiscrimination
andindividualdiscriminationforsoftwarefairnesstesting.How-
ever, the strategy of randomly generating test cases in THEMIS is
inefficient. Following the definitions coined by THEMIS, Udeshi
etal.[40]proposedAEQUITAS,achievingabetterefficiencybya
directionalsearchingscheme.ThissearchschememakesitdifficultforAEQUITAStocoversamplesindiversedistributions.Agarwal et
al.[
15]proposedSGbasedonprogramsymbolicexecution.SGuses
LIMEtogeneratelocalperturbationsamplestobuildadecisiontree
and then analyzes each tree path to generate test inputs. Similar to
SG, our ExpGA also leverages the interpretable method LIME. The
difference is that ExpGA uses explanation results to identify high-
qualityseedsamples.ADF[ 42]isadetectionmethodspecifically
designed for DNN networks. ADF requires gradient informationfrom the model architecture, thus restricting its practical usage.
Chakraborty et. al[19] thought that data imbalance and improper
data label are two main reasons for model biased, thus they
proposed Fair-SMOTE to achieve group fairness. Fair-SMOTE first
syntheticallygeneratesnewdatapointsforallthesubgroupsexcept
the subgroup having the maximum number of data points to solve
dataimbalanceintrainingdata.Then,itfindoutandremovebiasedsamplesintrainingdatatoeliminatebiasedlabels.Itisworthnoting
that our work focuses on individual fairness, while Fair-SMOTE
focuses on group fairness. Therefore, we do not compare our work
with Fair-SMOTE in this paper.Insummary,comparedwiththeseapproaches,ourExpGAisa
lightweightmodel-agnosticindividualfairnesstestingmethod,able
to handle diverse and large-scale scenarios.Interpretable Methods
: This work is also related to researches
related to machine learning interpretable methods. Ribeiro et al.
proposed a fast and effective method LIME [ 34] that fits a local
decision boundary with a simple linear regression model. The
weightofthelinearmodelrepresentstheimportanceoffeatures.
Lundberg and Lee [ 27] proposed SHAP, generates a set of shap
values based on linear functions to represent the coefficients of
eachfeature.UnlikeLIME,SHAPdefinesmultipleSHAPkernelsasaweightfunction.Atthesametime,inspiredbygametheory,SHAP
defines three properties to constrain the fitting process. Guidotti et
al.[25]proposedarule-basedblackboxmodelinterpretablemethod
calledLORE,whichfirstusesgeneticalgorithmstogeneratetwo
sample sets with completely different labels. Then, it constructsa decision tree and extracts a set of rules from the tree as a localexplanation. Considering that a single linear function cannot fit
thehighlynonlineardecisionboundarywell,Guo etal.[26]used
multiple linear models to approximate the local decision boundary
andchosetheonewiththehighestaccuracyasinterpretationresult.
7 CONCLUSION
ThisworkproposesExpGA,anexplanation-guidedmethodthrough
theGAforsoftwarefairnesstesting.ThenoveltyofExpGAisthe
combinationofexplanationresultsandGA,thusdeterminingits
highefficiencyandeffectivenessindiscriminatorysampledetection.
ExpGA is also model-agnostic and can handle black-box models
in diverse scenarios. The evaluation experiments demonstrate that
ExpGAcandetectdiscriminatorysamplesmuchfasterwithahighersuccessratethanfourstate-of-the-artmethods,bothonthetextand
tabular benchmarks. Augmented with the discriminatory samples
generatedbyExpGA,thefairnessoftestedmodelshasasubstantial
improvement through retraining.
ACKNOWLEDGMENT
This work was supported by National Key R&D Program of
China (2018YFB1004500), National Natural Science Foundationof China (61902306, 62002280, 61632015, 61602369, U1766215,
61772408,61702414,61833015),ChinaPostdoctoralScienceFoun-
dation(2019TQ0251,2020M673439,2020M683507),InnovativeRe-
search Group of the National Natural Science Foundation of
China(61721002),MinistryofEducationInnovationResearchTeam
(IRT_17R86), Youth Talent Support Plan of Xiâ€™an Association for
Science and Technology (095920201303).
REFERENCES
[1] 2018. AEQUITAS Github Project. https://github.com/sakshiudeshi/Aequitas.
[2] 2020. ADF Github Project. https://github.com/pxzhang94/ADF.[3]
2021. Bank Marketing Dataset. https://archive.ics.uci.edu/ml/datasets/Bank+
Marketing.
[4]2021. Census Income Dataset. http://archive.ics.uci.edu/ml/datasets/Census+
Income.
[5]2021. ConceptNet:Anopen,multilingualknowledgegraph. https://conceptnet.
io/.
[6]2021. Fairness 360: Understand and mitigate bias in ML models. https://ai-
fairness-360.org/.
[7]2021. German Credit Dataset. https://archive.ics.uci.edu/ml/datasets/statlog+
(german+credit+data).
[8] 2021. IMDB Dataset. https://www.imdb.com/interfaces/.
881
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ming Fan and Wenying Wei, et al.
[9]2021. ML-fairness-gym: A Tool for Exploring Long-Term Impacts of Machine
Learning Systems. https://ai.googleblog.com/2020/02/ml-fairness-gym-tool-for-
exploring-long.html.
[10]2021. Principle (a): Lawfulness, fairness and transparency. https://ico.org.uk/for-
organisations/guide-to-data-protection/guide-to-the-general-data-protection-
regulation-gdpr/principles/lawfulness-fairness-and-transparency/.
[11]2021. The Role of Protected Attributes in AI Fairness. https://www.trustscience.
com/blog/the-role-of-protected-attributes-in-ai-fairness.
[12] 2021. Scikit-Learn. https://scikit-learn.org/stable/user_guide.html.
[13]2021. Stanford Sentiment Treebank Dataset. http://nlpprogress.com/english/
sentiment_analysis.html.
[14] 2021. TensorFlow. https://github.com/tensorflow/tensorflow.[15]
Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2019. Blackboxfairnesstestingofmachinelearningmodels.In Proc.FSE.625â€“
635.
[16]Andrea Arcuri and Lionel Briand. 2011. A practical guide for using statistical
teststoassessrandomizedalgorithmsinsoftwareengineering.In Proc.ICSE.IEEE,
1â€“10.
[17]YuriyBrunandAlexandraMeliou.2018. Softwarefairness.In Proc.FSE.754â€“759.
[18]JoyBuolamwiniandTimnitGebru.2018. Gendershades:Intersectionalaccuracy
disparities in commercial gender classification. In Proc. FAT. 77â€“91.
[19]Joymallya Chakraborty, Suvodeep Majumder, and Tim Menzies. 2021. Biasin Machine Learning Software: Why? How? What to do? arXiv preprint
arXiv:2105.12195 (2021).
[20]Sam Corbett-Davies and Sharad Goel. 2018. The measure and mismeasure of
fairness:Acriticalreviewoffairmachinelearning. arXivpreprintarXiv:1808.00023
(2018).
[21]Brian dâ€™Alessandro, Cathy Oâ€™Neil, and Tom LaGatta. 2017. Conscientiousclassification: A data scientistâ€™s guide to discrimination-aware classification.
Big data5, 2 (2017), 120â€“134.
[22]MingFan,WenyingWei,XiaofeiXie,YangLiu,XiaohongGuan,andTingLiu.
2020. CanWeTrustYourExplanations?SanityChecksforInterpretersinAndroid
Malware Analysis. IEEE Transactions on Information Forensics and Security 16
(2020), 838â€“853.
[23]James R Foulds, Rashidul Islam, Kamrun Naher Keya, and Shimei Pan. 2020. An
intersectional definition of fairness. In Proc. ICDE. 1918â€“1921.
[24]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing:
testing software for discrimination. In Proc. FSE. 498â€“510.
[25]RiccardoGuidotti,AnnaMonreale,SalvatoreRuggieri,DinoPedreschi,Franco
Turini, and Fosca Giannotti. 2018. Local rule-based explanations of black box
decision systems. arXiv preprint arXiv:1805.10820 (2018).
[26]Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu Xing.
2018. Lemna: Explaining deep learning based security applications. In Proc. CCS.
364â€“379.
[27]ScottMLundbergandSu-InLee.2017. Aunifiedapproachtointerpretingmodel
predictions. In Proc. NeurIPS. 4765â€“4774.
[28]PingchuanMa,ShuaiWang,andJinLiu.[n.d.]. Metamorphictestingandcertified
mitigation of fairness violations in nlp models. In Proc. IJCAI. 458â€“465.
[29]Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan.2019. Asurveyonbiasandfairnessinmachinelearning. arXivpreprint
arXiv:1908.09635 (2019).
[30]Deepti Bala Mishra, Rajashree Mishra, Arup Abhinna Acharya, and Kedar Nath
Das. 2019. Test data generation for mutation testing using genetic algorithm. In
Soft Computing for Problem Solving. 857â€“867.
[31]Shira Mitchell, Eric Potash, Solon Barocas, Alexander Dâ€™Amour, and KristianLum. 2018. Prediction-based decisions and fairness: A catalogue of choices,
assumptions, and definitions. arXiv preprint arXiv:1811.07867 (2018).
[32]Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. [n.d.]. Discrimination-
aware data mining. In Proc. KDD. 560â€“568.
[33]JeffreyPennington,RichardSocher,andChristopherDManning.2014. Glove:
Global vectors for word representation. In Proc. EMNLP. 1532â€“1543.
[34]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. why should i
trustyou?:Explainingthepredictionsofanyclassifier.In Proc.KDD.1135â€“1144.
[35]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2018. Anchors:High-
precision model-agnostic explanations. In Proc. AAAI. 1527â€“1535.
[36] Nripsuta Ani Saxena. 2019. Perceptions of Fairness. In Proc. AIES. 537â€“538.
[37]Akshat Sharma, Rishon Patani, and Ashish Aggarwal. 2016. Software testing
usinggeneticalgorithms. InternationalJournalofComputerScience&Engineering
Survey7, 2 (2016), 21â€“33.
[38]RachaelTatman.2017. GenderanddialectbiasinYouTubeautomaticcaptions.
InProc. EthNLP@EACL. 53â€“59.
[39]FlorianTramer,VaggelisAtlidakis,RoxanaGeambasu,DanielHsu,Jean-Pierre
Hubaux, Mathias Humbert, Ari Juels, and Huang Lin. 2017. Fairtest: Discoveringunwarrantedassociationsindata-drivenapplications.In Proc.EuroS&P.401â€“416.
[40]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
directed fairness testing. In Proc. ASE. 98â€“108.
[41]Alexander Warnecke, Daniel Arp, Christian Wressnegger, and Konrad Rieck.2020. Evaluating explanation methods for deep learning in security. In Proc.EuroS&P. 158â€“174.
[42]PeixinZhang,JingyiWang,JunSun,GuoliangDong,XinyuWang,XingenWang,
JinSongDong,andTingDai.2020.White-boxfairnesstestingthroughadversarial
sampling. In Proc. ICSE. 949â€“960.
[43]Ridong Zhang and Jili Tao. 2017. A nonlinear fuzzy neural network modeling
approach usingan improved geneticalgorithm. IEEE Transactionson Industrial
Electronics 65, 7 (2017), 5882â€“5892.
882
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:19:35 UTC from IEEE Xplore.  Restrictions apply. 