Unleashing the Power of Compiler Intermediate Representation
to Enhance Neural Program Embeddings
Zongjie Li, Pingchuan Ma, Huaijin Wang,
Shuai Wangâˆ—
The Hong Kong University of Science and Technology
Hong Kong SAR
{zligo,pmaab,hwangdz,shuaiw}@cse.ust.hkQiyi Tang, Sen Nie, Shi Wu
Tencent Security Keen Lab
China
{dodgetang,snie,shiwu}@tencent.com
ABSTRACT
Neuralprogramembeddingshavedemonstratedconsiderablepromise
in a range of program analysis tasks, including clone identification,program repair, code completion, and program synthesis. However,
most existing methods generate neural program embeddings di-rectly from the program source codes, by learning from features
such as tokens, abstract syntax trees, and control flow graphs.
Thispapertakesafreshlookathowtoimproveprogramembed-
dings by leveraging compiler intermediate representation (IR). We
firstdemonstratesimpleyethighlyeffectivemethodsforenhancingembeddingqualitybytrainingembeddingmodelsalongsidesource
code and LLVM IR generated by defaultoptimization levels (e.g.,
-O2).WethenintroduceIRGen,aframeworkbasedongeneticalgo-
rithms(GA),toidentify(near-)optimalsequencesofoptimization
flags that can significantly improve embedding quality.
We use IRGen to find optimal sequences of LLVM optimization
flags by performing GA on source code datasets. We then extend a
popularcodeembeddingmodel, CodeCMR,byaddinganewobjec-
tivebasedontripletlosstoenableajointlearningoversourcecodeandLLVMIR.Webenchmarkthequalityofembeddingusingarep-
resentativedownstreamapplication,codeclonedetection.When
CodeCMRwas trained with source code and LLVM IRs optimized
by findings of IRGen, the embedding quality was significantly im-
proved,outperformingthestate-of-the-artmodel, CodeBERT ,which
wastrainedonlywithsourcecode.Ouraugmented CodeCMRalso
outperformed CodeCMR trained over source code and IR optimized
withdefaultoptimizationlevels.Weinvestigatethepropertiesof
optimization flags that increase embedding quality, demonstrate
IRGenâ€™sgeneralizationinboostingotherembeddingmodels,and
establishIRGenâ€™suseinsettingswithextremelylimitedtraining
data.Ourresearchandfindingsdemonstratethatastraightforward
additiontomodernneuralcodeembeddingmodelscanprovidea
highly effective enhancement.
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510217ACM Reference Format:
Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen
Nie,ShiWu.2022.UnleashingthePowerofCompilerIntermediateRepresen-
tationtoEnhanceNeuralProgramEmbeddings.In 44thInternationalConfer-
enceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA.
ACM,NewYork,NY,USA, 13pages.https://doi.org/10.1145/3510003.3510217
1 INTRODUCTION
Recentdevelopments indeep neuralnetworks(DNNs) havedeliv-
ered advancements in computer vision (CV) and natural language
processing (NLP) applications. We have noticed lately an increase
ininterestinusingDNNstosolveavarietyofsoftwareengineering
(SE) problems, including software repair [ 37,89], program synthe-
sis[14,54,71],reverseengineering[ 78],malwareanalysis[ 15],and
programanalysis[ 80,103].SimilartohowDNNsunderstanddis-
cretenaturallanguagetext,nearlyallneuralSEapplicationsrequire
computing numeric and continuous representations over software,
whicharereferredtoasprogramembeddingsorembeddingvectors.
The common procedure for generating code embeddings is to
processaprogramâ€™ssourcecodedirectly,extractingtokensequences,
statements, or abstract syntax trees (ASTs) to learn program repre-
sentations[ 9,11,17,37,66].Althoughsomepreliminaryapproaches
have attempted to extract semantics-level code signatures, such ap-
proachesarelimitedbyuseofsemanticfeaturesthataretoocoarse-
grained [ 73], low code coverage (due to dynamic analysis) [ 89],
or limited scalability [ 90]. To date, learning from code syntactic
andstructuralinformationhasremainedthedominantapproach
inthisfield,andaspreviousworkhasargued[ 27,82,88,90],the
use of features at this relatively â€œshallowâ€ level is likely to degrade
learning quality and produce embeddings with low robustness.
For some CV and NLP tasks, data augmentation has been pro-
posed as a tool to improve the quality of learned embedding repre-
sentations[ 30,79].Theseapproachestypicallyincreasetheamount
of training data by adding slightly modified copies of already exist-
ingdata orby creatingnew piecesofsynthetic datafrom existing
data. Thus, embedding models can be trained on larger numbers
of data samples, resulting in higher-quality embedding representa-
tions. Previous research has shown the value of data augmentation
approaches in increasing embedding quality [29, 53,57,64].
Thiswork investigatesusingcompiler intermediaterepresenta-
tions(IR)toaugmentcodeembedding.Moderncompilersinclude
numerous optimization flags that can seamlessly convert a piece of
source code into a range of semantically identical but syntactically
distinct IR codes. From a comprehensive standpoint, we argue that
ourtechnique canboostprogram embeddingontwo fundamental
levels.First, the translation of a single piece of source code into
22532022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen Nie, Shi Wu
severalvariantsofIRcodewiththesamefunctionalitysignificantly
increases the diversity of available training data. As previously
noted, such augmented data can commonly improve the quality of
learned embeddings. Second, although programs with the same
functionalitymayappearsyntacticallydistinctassourcecode,they
arelikelytobecomemoresimilarafterpruningandrearrangement
by optimizations. This alleviates the difficulties imposed by syntax
changes, as the optimizations regulate syntactic characteristics.
Webeginbyillustratingthatusingdefaultcompileroptimization
levels, such as -O2of LLVM, can produce IR code that significantly
improves the embedding quality of a popular embedding model,
CodeCMR [100],andoutperformsthestate-of-the-art(SOTA)model,
CodeBERT [31], trained on source code alone. However, despite
the promising potential in this â€œmisuseâ€ of compiler optimizations,
the high number of available optimization flags and the conse-
quently large search space present a challenge for identifying well-
performing optimizationsequencesto augmentembeddingmodels.
We propose IRGen, a framework that uses genetic algorithms
(GA)tosearchfor(near-)optimaloptimizationsequencesforgener-
ation of IR code to augment program embedding models. Compiler
optimizationflagsaretypicallycombinedtogeneratemachinein-
structions with high speed or small size. In contrast, IRGen targets
optimizationsequences,generatingIRcodethatis structurallysimi-
lartotheinputsourcecode.Thispreventsover-simplificationofthe
IR code, which is undesirable in our task since overly-simplified IR
oftenbecomes lessâ€œexpressive.â€Additionally,tomaximizelearning
efficiency, we limit overuse of out-of-vocabulary (OOV) terms (our
definition of OOV follows ncc[16]; see Sec. 4.3).
Wepresentasimpleyetunifiedextension,throughtripletloss[ 94],
to enable embedding models to learn from source code and LLVM
IR.Forevaluation,weusedIRGentoanalyzeIRsgeneratedfrom
the POJ-104 [ 66] and GCJ [ 72] datasets, which include a total of
299,880C/C++programs.After143to963CPUhoursofsearch(we
use a desktop computer to run IRGen), IRGen was able to form
optimization sequences with high fitness scores from the 196 opti-
mizationflagsavailableinthex86LLVMframework(ver.11.1.0).
To evaluate the quality of embedding, we setup a representative
downstreamtask,codeclonedetection.When CodeCMR wastrained
with IR code generated by the identified optimization sequences,
embedding quality (in terms of code clone detection accuracy) sig-
nificantly improved by an average of 11.66% (peaking at 15.46%),
outperformingtheSOTAmodel CodeBERT trainedwithonlysource
code(for12.02%)or CodeCMRjointlytrainedwithsourcecodeandIR
emitted by default optimizations (for 5.94%). We also demonstrate
that IRGen is general to augment other neural embedding models
and show that IRGen can almost doublethe quality of learned em-
beddings in situations with limited data (e.g., 1% of training dataavailable). We characterize optimization flags selected by IRGenand summarize our findings. This work can help users take use
ofcompileroptimization,an out-of-the-box amplifier,toimprove
embedding quality. In summary, our contributions are as follows:
â€¢We advocate the use of compiler optimizations for software
embedding augmentation. Deliberately-optimized IR code
can principally improve the quality of learned program em-beddings by extending model training datasets and normal-
izing syntactic features with modest cost.â€¢We build IRGen, a practical tool that uses GA algorithmsto iteratively form (near-)optimal optimization sequences.
Additionally,wepresentasimpleyetgeneralextensionover
modern code embedding models to enable joint learning
over source code and IR.
â€¢Our evaluation demonstrates highly promising results, with
our augmented model significantly outperforming SOTA
models.Wefurtherdemonstratethegeneralizationof IRGenanditsmeritinaugmentingverylimitedtrainingdata.IRGen
is released at [1].
2 PRELIMINARY
Neural code embedding, as in Fig. 1, converts discrete source code
to numerical and continuous embedding vectors, with the end goal
of facilitating a variety of learning-based program analysis. We
introduce program representations in Sec. 2.1. We examine alterna-
tivemodeldesignsinSec. 2.2andtheconceptofdataaugmentation
for neural (software) embedding in Sec. 2.3.
	
 

 	
 




	
	
Figure 1: Common neural program embedding pipeline.
2.1 Input Representation
Code can be expressed as text and processed using existing NLP
models. However, it would be costly and likely ineffective because
programming languages usually contain a wealth of explicit and
sophisticatedstructuralinformationthatisdifficultforNLPmodels
to comprehend [ 66]. Therefore, modern code embedding models
often learn programembeddings using code structural representa-
tionswhich areinformative.For instance,theabstract syntaxtree
(AST)isusedtorepresentcodefragmentsforprogramembeddings.
Once a code snippetâ€™s AST has been generated, there are several
methodsforextractingdiscretesymbols(e.g.,ASTnodes)foruse
in the subsequent learning process. For example, code2vec [11]
andcode2seq [10]extractacollectionofpathsfromASTtoform
embeddings, as discussed below.
Control flow graphs (CFG) are also used to form input repre-
sentation, especially when analyzing assembly code. Two repre-sentative tools, asm2vec [
27] and BinaryAI [ 99], construct CFGs
overassemblycodeandcombinebasicblock-levelembeddingsinto
the programâ€™s overall embedding. Recentresearch [ 8,16,36] has
explored the use of hybrid representations that incorporate datafrom different layers. For instance,
ncc[16] extracts a so-called
contextualflowgraphfirst,whichsubsumesinformationfromboth
control flow graph and data flow graph.
2.2 Neural Model Learning Procedure
NLPmodelsaregenerallydesignedtoprocessinfinitesequences
oftokens,whereassoftwareisstructured.Hence,theneuralcode
2254
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
embedding learning process can be divided into two broad cate-
gories:1)decomposingprogram(structural)representations(e.g.,
AST or CFG) into one or multiple token sequences that are then
processed by NLP models; and 2) attempting to initiate an â€œend-
to-endâ€procedurefordirectlylearningstructuralrepresentations
using advanced neural models like graph neural networks (GNN).
CodeBERT is a large-scale SOTA code embedding model that
primarily learns from token-level software representations. It is
inspiredbyBERT[ 26],afamousbidirectionalnaturallanguageem-
beddingmodel. CodeBERT constructslearningobjectivesusingboth
maskedlanguagemodeling(MLM)andreplacementtokendetec-
tion.Usingtheseobjectives,itistrainedtopredicttokensthathave
beenrandomlymaskedoutoftheinputsuntilsaturationaccuracyisreached.Anothermodel,
asm2vec [27],usesMLMs,particularlyan
extendedPV-DMmodel[ 52],toembedx86instructionsatthetoken
level. Token sequences can be extracted from tree or graph rep-
resentations.Forexample, code2vec [11]and code2seq [9]break
ASTintopaths,transformpathstoembeddingsusingLSTM[ 42],
and finally aggregate path-level embeddings to produce the ASTâ€™s
embedding. The structure-based traversal method [ 43] converts
ASTs into structured sequences.
TBCNN [ 67], Great [ 41] and BinaryAI [ 99] leverage advanced
models,such asGNNs,todirectlyprocess programstructuralrep-
resentations. BinaryAI [ 99], for example, uses standard GNNs to
propagate and aggregate basic block embeddings into CFG em-
beddings.BesidesCFGs,neuralmodels cancreatestructureswith
richer information. ncc[16] forms a contextual flow graph with
control- and data-flow information. Each node in the contextual
flow graph contains a list of LLVM IR statements, which nccthen
transforms into vectors. It further uses a GNN to aggregate thenode embeddings into an embedding of the entire program. As
with ncc,MISIMbeginsbyconstructinganovelcontext-awarese-
manticstructure(CASS)fromcollectionsofprogramsyntax-and
structure-levelproperties.ItthenconvertsCASSintoembedding
vectors using GNNs. It outperforms prior AST-based embedding
tools, including code2vec andcode2seq [9].
2.3 Data Augmentation
Images can be rotated while retaining their â€œmeaningâ€ (e.g., viaaffine transformations [
104]). Similarly, we can replace words in
natural language sentences with their synonyms, which should
notimpairlinguisticsemantics.Dataaugmentationleveragesthese
observations to create transformation rules that can enlarge model
training data.
Itisworthnotingaconventionaltechnique,namelyfeatureengi-
neering[105],cangenerallyhelpdatascienceandmachinelearning
tasks.Featureengineeringfacilitatestoeliminateredundantdata
that can reduce overfitting and increase accuracy. Nevertheless, in
theeraofdeeplearning,itgraduallybecomeslessdesirabletoman-
uallyâ€œpickusefulfeatures,â€giventhatweneedtofrequentlydeal
with high-dimensional data like image, text, video, and software.
Howtopickusefulfeaturesisoftenobscurewhenlearningfrom
thosecomplexhigh-dimensionaldata.Infact,ithasbeendemon-
strated that data augmentation generally and notably improves
deeplearning modelperformance androbustness, andit hasbeen
frequently employed as a routine technique to enhance modernTable 1: MAP scores of CodeCMR on POJ-104 [ 66] for different
input setup.
Setup MAP(%) Setup MAP(%)
Source 76.39 Source + LLVM IR -O2 84.29
Source + LLVM IR -O0 82.90 Source + LLVM IR -O3 84.21
Source + LLVM IR -O1 83.37 Source + LLVM IR -Os 83.81
Source + LLVM IR Optimized by Optimization Sequences Found by IRGen 89.18
deep learning models in a variety of domains [ 60,61,70,75,77,91,
93,101,102,104].
Standard data augmentation approaches, however, are notdi-
rectly applicable to enhance program embeddings. Augmentingneural program embeddings is challenging and under-explored.Due to the synthetic and semantic constraints of programming
languages, arbitrary augmentation can easily break a well-formed
program.Thispaperexploresbringingdataaugmentationtosource
code. In particular, we advocate employing compiler optimizations
to turn a same piece of source code into semantically identical but
syntacticallydiverseIRcode.Notethatwedonotneedtoâ€œreinvent-
ing the wheelâ€ to develop extra semantics-preserving source code
transformations[ 44].Instead, wedemonstratehow amaturecom-
pilercanfacilitateeffectivedataaugmentationsimplybyexploiting
optimizations developed over decades by compiler engineers.
3 MOTIVATION
TheLLVMcompilerarchitecturesupportshundredsofoptimiza-
tion passes, each of which mutates the compiler IR in a unique
way.Tomakecompileroptimizationmoreaccessibletousers,the
LLVMframeworkoffersseveraloptimizationbundlesthatauser
can specify for compilation, for example, -O2,-O3, and -Os. The
first two bundles combine optimization passes for fast code execu-
tion,whereas -Osaimstogeneratethesmallestexecutablepossible.
OurpreliminarystudyshowsthatbyincorporatingoptimizedIR
codeintoembeddinglearning,theembeddingqualitycanbesub-
stantiallyenhanced.Thissectiondescribesourpreliminaryfinding,
which serves as an impetus for the subsequently explored research.
LearningOverSourceCode. WeusePOJ-104[ 66],acommonly
used dataset containing 44,912 C programs written for 104 tasks.
Thisdatasetissplitintothreeprogramsets:onefortraining,onefor
validation,andonefortesting(seeSec. 6).Wetrained CodeCMR[100],
onepopularcodeembeddingtool,onthetrainingsplit,andthen
performmulti-labelclassificationonthetestingsplit. CodeCMR gen-
eratescodeembeddingsbyfirstconvertingsourcecodetoacharac-
ter sequence and then computing character-level embeddings. The
embeddingsarefedtoastackofPyramidConvolutionalNeuralNet-
work (DPCNN) [ 45], in which an average pooling layer constructs
the programâ€™s embedding. DPCNN has been shown as powerful at
embedding programs. In our evaluation on POJ-104, we observe
promisingaccuracyintermsofMAP[ 68]score,asshownin Source
of Table1. MAP is a commonly used metrics in this field, and a
higherMAPscoreindicatesagreaterqualityofcodeembeddings.
As willbe shownin evaluation(Table 4), thisresult iscomparable
to those obtained using the SOTA model, CodeBERT.
Findings. Despite the decent results, we find that CodeCMR fails
togroupquiteanumberofPOJ-104programswhobelongtothe
same class. The POJ-104 C code and corresponding LLVM IR are
2255
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen Nie, Shi Wu
toolengthytofitinthepaper;wepresentsomeveryreadablecases
at [3] and summarize our key observations below.
C/C++programsimplementingthesamefunctionalitycanex-
hibit distinct syntactic appearance. For instance, at [ 3], we present
a case where two programs, ğ‘1andğ‘2, are implementing the same
â€œdaysbetweendatesâ€task.Wefindthat ğ‘1usesone switchstate-
ment, whereas ğ‘2uses a sequence of ifstatements. Further, ğ‘1
uses many local variables to encode #days in each month, while
thoseinformationin ğ‘2arehardcodedinconstants.Thisway, ğ‘1
andğ‘2, differ from both control- and data-flow perspectives.
Nevertheless,wefindthattheLLVMIRcodecompiledfromthese
two programs are much closer in both control- and data-flows. Let
ğ‘™1andğ‘™2(see [3]) be two LLVM IR programs compiled from ğ‘1and
ğ‘2andoptimizedwithoptimizationlevel-O3.Wefindthat ğ‘™1andğ‘™2
preservesmostofthestructureofthesourcecode.Morecrucially, ğ‘™1
andğ‘™2both use a LLVM IR switchstatement to encode the control
structures. Data usage is also regulated, where both local variables
inğ‘1and the constants in ğ‘2become integers hardcoded in IR
statements.TheinducedIRprograms ğ‘™1andğ‘™2are(visually)very
similar,revealingthetruesemantics-levelequivalenceof ğ‘1andğ‘2.
Wethussuspectthat CodeCMRisindeedhamperedbytooflexible
code representationin C programs.In other words,it is shownas
demanding to explore extracting more robust features from C/C++
code to enhance the learning quality.
Learning over Code Structure or Semantics. As previously
stated, CodeCMR learnsonthecharacter(token)sequence.Thisindi-
cates that CodeCMR is less resilient against changes at the syntactic
level. Graph-level embeddings might be more robust to token-level
changes, given their reliance on the rich structural information
containedintheprogram.Nonetheless,inreal-lifecodesamples,
manychangescanalsooccuratthegraphlevel,andasshownin
Sec.6,representativegraph-levelembeddingmodelsalsoperform
poorly on diverse and large-scale datasets, such as POJ-104.
Some readers may wonder if learning directly from code seman-
tics, such as input-output behaviors captured by dynamic analy-sis [
88,90], is possible. While dynamic analysis can precisely de-
scribe code behaviors (on the covered paths), it suffers from low
coverage. Symbolic execution (SE) [ 20] is used to include a greater
amount of program activity in applications such as code similar-ity analysis [
59]. Nonetheless, SE is inherently inefficient, where
trade-offs are made to launch SE over real-world software [ 19,85].
Learning over IR Code. This paper advocatesusing compiler IR
toextendmodeltraindatasetandenhancecodeembeddingmodels.
However, we do notsuggest learning solely from IR for two rea-
sons. First, compiler optimizations such as static single-assignment
(SSA) [25] result in LLVM IR codes that typically have ten times as
manylinesofcode(LOC)asthecorrespondingCsourcecode.This
providesasignificantimpedimenttotrainingembeddingmodels.
In our preliminary study, we find that training embedding models
using LLVM IR code alone resulted in significantly inferior perfor-
manceacrossmultipletasksanddatasets.Second,whenoutputting
IR code, the LLVM compiler prunes or â€œobfuscatesâ€ certain source
code features such as string and variable names. Note that variable
names and constants are generally crucial to improving embed-dingquality.Similarly,inLLVMIRcode,callsites,particularlyto
standard libraries like glibc, areoften modified.For example, the
callsite statement in set<int> row; row.insert(x); would beconverted to a complex function name with additional prefixes.
Notably,weshouldavoidtweakingwithordisablingcertainâ€œan-
noyingâ€ optimization passes (for example, the SSA pass), as many
optimization flags assume the existence of other flags.
LearningOverSourceCodeandIRCode. Weextended CodeCMR
toprocessIRcodeemittedby clang.Weaugmentedthefrontend
ofCodeCMRto process LLVM IRs. We also extended the learning
objectives by requiring CodeCMR to minimize the distance between
thesourcecodeandcorrespondingIRusingtripletloss(seeSec. 4.4).
We compiled each test program in the POJ-104 training set into
LLVMIRtotrainthe CodeCMR,andthenbenchmarktheMAPscore
using the same setting.
AsseeninTable 1,usingLLVMIRinthelearningprocesssignif-
icantlyimprovedembe ddingperformance.Forinstance,whencom-
pilingthesourcecodeintoLLVMIRwithnegligibleoptimization
(-O0),thejointlearningenhancedtheMAPscorebyapproximately
6%. Note that jointly training over source code and IR ( -O0) has
already outperformed the SOTA model, CodeBERT (82.7%). More
importantly, it is seen that compiler optimizations can notably im-
prove CodeCMRâ€™s performance. We observe that as compared with
-O0, using optimization levels -O2,-O3, and -Osproduces MAP
scores greater than 84%.
We regard the above findings as encouraging and intuitive: they
demonstrate the possibility and benefit of learning jointly fromsource code and IR code (which are more regulated) rather than
fromsourcecodealone.Inevaluation(Sec. 6.3),wediscussoptimiza-
tionflagsfurtherwithcasestudiestorevealthattheycaneffectively
regulatecodegenerationpatterns,removesuperfluouscodefrag-
ments, and generate more consistent IR code in the presence ofsyntactic or structural changes in the source code. We therefore
summarize the key findings of this early investigation as follows:
Launchingajointtrainingusingbothprogramsourcecodeand corresponding IR can notably improve the embedding
quality.
Limitation of Standard Optimization Levels. Despite the en-
couraging results, we note that these default optimization levelsare selected by compiler engineers with differentfocus, e.g., pro-
ducing smallest possible executable or fast execution. However,
we explore a different angle, where optimizations are â€œmisusedâ€
to generateLLVM IRcode to augment programneuron embedding.
In that regard, it is possible to suspect the inadequacy of utiliz-ing simply the default optimization levels. For instance, certain
CPU flags in -Osand-O3are aggressive in shrinking IR code (e.g.,
-aggressive-instcombine ),which,mightnotbepropersinceem-
beddingmodelsgenerallyprefermoreâ€œexpressiveâ€inputs.Inevalua-tion,wefindthataggressiveflagssuchas
-aggressive-instcombine
arenotpickedbyIRGen.Wealsofindthatoptimizationflagsshould
be adaptive to source code of different complexity, whereas default
optimization levels are fixed. Sec. 6.3compares optimization flags
selected by IRGen when analyzing different datasets.
We introduce IRGen, an automated framework to determine
(near-)optimalsequencesofoptimizationflagsforeachparticular
dataset. To compare with standard optimization levels, the finalrow of Table 1presents the improved performance of
CodeCMR
2256
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
when using IRGen-selected optimization flags. These results show
that IR code optimized using IRGen-formed optimization sequence
significantly improved the accuracy.

	

 	

		
	

		
		

 





Figure 2: The workflow of IRGen.
4 DESIGN OF IRGEN
Fig.2depicts IRGenâ€™s workflow. Given a dataset of C source code,
wefirstformavalidationdataset ğ‘ƒwherefitnessscoresarecom-
puted from (see Sec. 4.3). Our observation shows that the size of ğ‘ƒ
does not need to be large (otherwise the GA procedure becomes
slow). For the current implementation, we randomly select 5% pro-
grams from the training dataset (POJ-104 or GCJ; see details in
Sec.6)o f CodeCMR.
IRGen initializesthe firstgeneration ofoptimization sequences
(Sec.4.1), and then launches the GA-based search to iteratively
modify and select sequences giving high fitness scores (Sec. 4.2).
The GA process is repeated ğ‘times until termination ( ğ‘is 800
currently).Aftertermination,weselectthe ğ¾sequenceswiththe
top-ğ¾highest fitness scores across the entire GA search. Using
these optimization sequences, each piece of C/C++ code in the
trainingdatacanbecompiledinto ğ¾syntacticallydistinctpiecesof
LLVM IR code. The resultingaugmented code dataset can be used
toempowerneuralembeddingmodels,byincorporatingtripletloss
as an extra learning objective (Sec. 4.4).
Application Scope. This research mainly focuses on the LLVM
compiler framework given its popularity. LLVM provides a total of
196 optimization flags that are applicable on x86 platforms. IRGen
traverses the entire search space to identify sequences expected to
improveembeddingquality.IRGenâ€™siterativemethodis orthogonal
to the LLVM framework and can therefore be extended to support
othercompilers,suchas gcc(tomanipulateitsGIMPLEIR),without
incurring additional technical difficulties.
Thecurrentimplementationof IRGenprimarilyenhancesC/C++
codeembedding.C/C++programscanbe compiledintoLLVMIR
and optimized accordingly. Moreover, most security related em-
beddingdownstreamapplications(e.g.,CVEsearch[ 27])concern
C/C++ programs. Nevertheless, we clarify that code embedding
has its wide application on other languages such as Java/Python.
ItisworthnotingthatIRGenreliesonarichsetofcompileropti-
mizationstogeneratediverseIRcode.Java/Pythoncompilers/in-
terpreters provide fewer optimization passes, and they leave many
optimizationsatruntime.Asaresult,thesearchspaceforIRGento
explore would be much smaller. We also have a concern on the ex-
pressivenessofJava/PythonbytecodeincomparisonwithLLVMIR.
Theirbytecodeseemsverysuccinct,potentiallyunderminingthe
SOTA embeddingmodels.Overall, weleaveit asonefuture work
to explore extending IRGen to enhance Java/Python embedding.IRGenâ€™s present implementation does not consider the order of
optimizations in a sequence. We also assume each flag can onlybe used once. This enables a realistic and efficient design whenGA is used; similar design decisions are also made in relevant
works [55,74]. Taking orders or repeated flags into account would
notably enlarge the search space and enhance the complexity of
IRGen.Wereservethepossibilityofusingmetaheuristicalgorithms
withpotentiallygreatercapacity,suchasdeepreinforcementlearn-
ing [65], for future work. See Sec. 7for further discussion.
Design Focus. IRGenâ€™s GA-based pipeline was inspired by lit-
eratures in search-based software engineering, particularly us-ing GA for code testing, debugging, maintenance, and harden-
ing[5,33,63,69,74,92].Sec.8furtherreviewsexistingstudies.Our
evaluation will show that the GA method, when combined withour well-designed fitness function, is sufficiently good at select-
ing well-performingoptimization sequences.Further enhancement
may incorporate other learning-based techniques; see Sec. 7.
4.1 Genetic Representation
FollowingcommonGApractice,werepresenteachoptimization
sequenceasaone-dimensionalvector ğ‘£=(ğ‘“1,ğ‘“2,...,ğ‘“ğ¿),whereğ¿
isthetotalnumberofoptimizationflagsofferedbyLLVMforx86
platforms.Each ğ‘“ğ‘–isabinarynumber(0/1),denotingwhetherthe
corresponding flag, ğ‘ğ‘–, is enabled or not on sequence ğ‘–. As standard
setup, we initialize ğ‘€instances of vector ğ‘£, by randomly setting
elementsinavector ğ‘£as1.Theserandomlyinitializedsets,referred
to as a â€œpopulationâ€ in GA terminology, provide a starting point to
launch generations of evolution. Here, ğ‘€is set as 20.
4.2 Modification and Selection
Ateachgeneration ğ‘¡,weemploytwostandardgeneticoperators,
Crossover and Mutation, to manipulate all 20 vectors in the pop-
ulation.Giventwoâ€œparentâ€vectors, ğ‘£1andğ‘£2,twooffspringsare
generated using ğ‘˜-point crossover: ğ‘˜cross-points are randomly
selected on ğ‘£1andğ‘£2, and the content marked by each pair of
cross-points is swapped between them. Here, ğ‘˜is set as 2, and the
chance of each potential crossover is set as 0.4. We also use flip
bit mutation,another common method,to diversify vectors inthe
population. We randomly mutate 1% of bits in vector ğ‘£. After these
mutations,thepopulationsizeremainsunchanged(20vectors),butsomevectorsaremodified.EachvectorisassessedusingthefitnessfunctiondefinedinSec. 4.3.Allmutatedandunmutatedvectorsare
thenpassedintoastandardroulettewheelselection(RWS)mod-ule,wherethechanceforselectingavectorisproportionaltoits
fitness score. This way, a vector with a higher fitness score is more
likely to be selected into the next generation. The RWS procedure
is repeated 20 times to prepare 20 vectors for generation ğ‘¡+1.
4.3 Fitness Function
Given a vector, ğ‘£, denoting a sequence of optimization flags, fit-
ness function Fyields a fitness score as an estimation of ğ‘£â€™s merit.
Specifically, for each ğ‘£, we compile every program ğ‘in the vali-
dation dataset ğ‘ƒusing optimizations specified in ğ‘£to produce IR
programs ğ‘™âˆˆğ¿. ForaC program ğ‘andits compiledIR ğ‘™,wefirst
compute the following fitness score:
2257
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen Nie, Shi Wu
ğ¹ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ _ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘,ğ‘™=ğ‘ ğ‘–ğ‘šğºÃ—ğ‘¢ğ‘›ğ‘˜_ğ‘Ÿğ‘ğ‘¡ğ‘’0
ğ‘¢ğ‘›ğ‘˜_ğ‘Ÿğ‘ğ‘¡ğ‘’ğ‘™
whereğ‘ ğ‘–ğ‘šğºdenotes the graph-level similarity between the ğ‘™andğ‘.
Thevalueof ğ‘¢ğ‘›ğ‘˜_ğ‘Ÿğ‘ğ‘¡ğ‘’0denotesthenumber of#OOVcasesfound
in IR code ğ‘™0when compiling ğ‘with -O0(i.e., the baseline), and
ğ‘¢ğ‘›ğ‘˜_ğ‘Ÿğ‘ğ‘¡ğ‘’ğ‘™stands for #OOV cases found in ğ‘™. Then, Fis acquired
by averaging the above fitness score for all programs ğ‘âˆˆğ‘ƒ.
Graph Similarity. The graph similarity metric quantifies the simi-
laritybetweentheoriginalsourcecodeandthecompiledIRcodeat
the CFG level. This provides a high-level assessment of the created
IR codeâ€™s quality. More importantly, this condition prevents exces-
sive reduction of the code by the compiler optimizations, ensuring
that the IR code reasonably preserves the original source codeâ€™s
structure-level features.
Wetentativelyassessedthreegraphsimilaritycomputationmeth-
ods: 1) kernel methods, 2) graph embedding [ 35,97], and 3) tree
edit distance. Graph embedding methods often require to fine-tune
alargenumberofhyper-parameterswhichisgenerallychalleng-
ing. We also find that tree edit distance algorithms had limited
capacity to process the very complicated CFGs created for our test
cases. IRGenâ€™s present implementation therefore uses a classic and
widely-used kernel method, shortest-path kernels [ 18], to quantify
thestructuraldistancesbetweensourcecode ğ‘anditsoptimized
IR codeğ‘™. Overall, kernel methods, including shortest-path kernels,
denote a set of methods originated from statistical learning theory
tosupportpatternanalysis[ 21].Kernelmethodsareshowntobe
effective in various tasks such as classification and regression. For
our scenario, we feed the employed kernel function with a pair of
CFGderivedfrom sourcecode ğ‘andthecorrespondingIR ğ‘™,and
the kernel function returns a score ğ‘ ğ‘–ğ‘šğº.
OOVRatio. Inembeddinglearning,OOVsrepresenttokensthat
arerarelyobservedandarenotpartofthetypicaltokenvocabulary.
Weclarifythatwefollow ncc[16]todefinevocabulary.Particularly,
our vocabulary denotes a bag of IR statements, and therefore, IR
code is represented by a list of statement embeddings. Accordingly,
â€œOOVâ€ in our context denotes a new statement never occurring
inthebaselinevocabulary.Suchnewstatementscorrespondtoa
specialembeddingnotedasâ€œ[unknown]â€inourimplementation,
degrading the learning quality.
A high #OOV is discouraged in the fitness function. That is, we
leverage the OOV ratio to punish an optimization sequence if it
results in an IR code with too many OOV cases. To this end, a
â€œbaselineâ€ is first computed, recording the #OOV encountered in IR
code generated by compiling ğ‘with -O0. Then, given optimization
sequence ğ‘£, we count the #OOV cases identified in its optimized IR
codeğ‘™, and compute the relative OOV ratio.
We clarify thatit ispossible to avoidtoken-level OOVissue by
leveragingsub-tokenizationtechniqueslikeBPE[ 46].Giventhat
said,inthecurrentsetting,anIRstatementisrepresentedbyasingleembeddingvector,whereasBPErepresentsastatementbymultiple
vectors ofsub-tokens. Theextra overhead dueto multiple vectors
isseenasacceptableforsourcecodebutunaffordableforIRcode,
which is orders of magnitude longer. In fact, our preliminary study
explored using BPE: we report that BPE would result in 16 Ã—and
30Ã—longer vectors on our test datasets, POJ-104 [ 66] and GCJ [ 72].4.4 Learning Multiple IRs using Triplet Loss
Insteadofkeepingsinglesequencewiththehighestfitnessscore,
IRGenretainsthetop- ğ¾sequencesfromeachgeneration,asranked
by their fitness scores. We find that it is beneficial to perform aug-
mentation with multiple LLVM IR codes generated by the top- ğ¾
optimization sequences (see results in Sec. 5). Given the GA proce-
dure, these top- ğ¾sequences will evidently share some overlapping
optimization flags. However, we find that when a source program
iscompiledinto ğ¾LLVMIRprogramsusingthesetop- ğ¾sequences,
theseğ¾IR programs are still distinct (see cases at [ 2]), although
they share regulated code structures that are correlated with thereference source code. Hence, we anticipate that the augmented
datasetwillbediverse,whichhasbeengenerallyshowntobeuseful
in enhancing embedding learning quality [ 50,56,96].ğ¾denotes a
hyper-parameterof IRGen.Webenchmarktheaccuracychanges
in terms of different ğ¾in Sec.5.

	
	
	 

		 	




	 

	  
	  
  
 

 	
 	 


Figure 3: Learning from IR code with Triplet Loss.
Fig.3depicts an efficient and general extension over program
embeddingmodelstosubsumemultipleIRcode.Asexpected,we
first extend a code embedding model ğ‘€to process LLVM IR. Then,
we employ a popular learning objective, namely triplet loss [ 94],
asthelossfunctionof ğ‘€.Thetriplet,whichconsistsofapositive
sample,anegativesample,andananchor,isusedastheinputfor
tripletloss.Ananchorisalsoapositivesample,whichisinitially
closertosomenegativesamplesthanitistosomepositivesamples.
Theanchor-positivepairsarepulledcloserduringtraining,whereas
theanchor-negativepairsarepushedapart.Inoursetting,apositivesamplerepresentsaprogram
ğ‘,anchorrepresentsIRcodeproduced
fromğ‘,andnegativesamplesrepresentotherunrelatedsourcecode.
Notethat ğ‘€isnotnecessarily CodeCMR.Othernon-trivialsource
code embedding models can serve ğ‘€in this pipeline; see our eval-
uation on generalization in Sec. 6.4. Further, while we adopt Fig. 3
to enhance ğ‘€, we clarify that there may exist other augmenta-
tion pipelines. We provide proposals of other pipelines at [ 4] for
information of interested audiences.
5 IMPLEMENTATION
IRGeniswrittenprimarilyinPythonwithabout9Klinesofcode.
ThisprimarilyincludesourGApipeline(Sec. 4)andextensionof
CodeCMR(seebelow).IRGenisbasedonLLVMversion11.1.0[ 51].
WealsotentativelytestedLLVMversion7.0,whichworkssmoothly.
IRGenisbuiltinafullyautomatedandâ€œout-of-the-boxâ€manner.
UsersonlyneedtoconfigureIRGenwiththepathoftheirLLVM
toolchain. We release IRGen and data (e.g., augmented models)
2258
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
at[1].Ourresultscanbereproducedusingourreleasedartifacts.
We pledge to keep IRGen up to date to support future study.
" !
"!(	
"!(	
"!(	
""!"""!
 !*$#&
%'"")! #! !
"'$"
Figure 4: The main structure of CodeCMR.
Enhancing CodeCMR.As stated in Sec. 3, IRGen is currently im-
plemented with CodeCMR[100], which is a SOTA code embedding
modelthathasbeenthoroughlytestedonreal-worldC/C++pro-
grams.Wefindthatitscodeisstraightforwardtouseandofhigh
quality.WeemphasizethatIRGenisorthogonaltotheparticular
codeembeddingmodelsused.Weassessthegeneralizationof IR-
Gen using another embedding tool, ncc, which directly computes
embeddingsfromLLVMIR;seeSec. 6.4.Weextendedtheofficial
version of CodeCMR to jointly compute embeddings using C source
code and LLVM IR code. We also implement a C/C++ parser based
on treesitter [ 86] and a LLVM IR parser (extended from ncc), as
we need to compare distance of C/C++ and IR code using kernel
methods.Fig. 4depictsthemainnetworkstructureofourextended
CodeCMR.CodeCMRisavariantofDPCNN,whichhasbeenshown
to efficiently represent long-range associations in text. As shown
in Fig.4, the key building block, a word-level convolutional neural
network (CNN), can be duplicated until the model is sufficiently
deeptocaptureglobalinputtextrepresentations.Giventhat(IR)
programs are typically lengthy and contain extensive global infor-
mation, CodeCMRexhibits promising accuracy.
Table 2: Augmentation using ğ¾collections of optimized IR.
ğ‘˜=1ğ‘˜=2ğ‘˜=3ğ‘˜=4ğ‘˜=5ğ‘˜=6ğ‘˜=7
MAP 86.34 88.03 88.96 89.71 90.16 91.12 90.00
Tuningğ¾.RecallingthatIRGengenerates ğ¾collectionsofIRcodes
by returning the top- ğ¾sequences, we now compare the effect of ğ¾
onlearningquality.Weranourexperimentsusing CodeCMR trained
on POJ-104 and measured the MAP accuracy for different ğ¾in
Table2. Overall, although increasing ğ¾can continuously extend
thetrainingdata,thelearningaccuracyreacheditspeakvaluewhen
ğ¾=6. We interpret these results to confirm another important (and
intuitive) observation:
Aligned with data augmentation on natural language or
image models, involving multiple diverse IR code collec-
tions in training datasets augments the learning quality of
code embedding.We provide samples on [ 2] to illustrate how source code can be
compiledinto ğ¾piecesofdiverseIRcode.Inourevaluation(Sec. 6),
wechose ğ¾=6asthedefaultoption. However,w eclarifythatthe
best value of ğ¾, as a hyper-parameter, can be influenced by both
the specific dataset and the neural embedding model. We therefore
recommend users to tune ğ¾for their specific learning task.
6 EVALUATION
Our evaluation aims to answer the following research questions:RQ1: How does
CodeCMR, after enhanced by IRGen, perform in
comparison to other relevant works on code clone detection? RQ2:
How accurate is the genetic algorithm (GA) adopted by IRGen?RQ3: What are the most important optimization flags and their
characteristics?Doesthe optimalsequenceofflagschange ondif-
ferent datasets? RQ4: What is the generalization of IRGen with
respect to other models and different learning algorithms? RQ5:
CanIRGenstillachievepromisingaugmentationwhenonlylimited
source code samples are available? Before reporting the evaluation
results, we first discuss the evaluation setup as follows.
Dataset. WeusedthePOJ-104[ 66]andGCJ[ 72]datasetsforour
evaluations. Table 3reports the summary statistics of these two
datasets.AsmentionedinSec. 3,thePOJ-104datasetcontains44,912
C/C++ programs that implement entry-level programming assign-
ments for 104 different tasks (e.g., merge sort and two sum). TheGoogle Code Jam (GCJ) is an international programming compe-tition that has been run by Google since 2008. The GCJ dataset
contains the source code from solutions to GCJ programming chal-
lenges. The GCJ dataset is commonly used and contains 260,901C/C++ programs. Compared to POJ, the GCJ files are longer and
morenumerous.WefindthatGCJfilescontaincomplexusageof
Cmacros.Asdescribedlaterintheevaluation,wefoundthatthe
morelengthyGCJcodeanditsrelativelycomplexcodestructures
had notable impacts on the optimization sequences selected by the
GAprocedure.Forbothdatasets,weusedthedefaultsettingtosplitthemfortrainingandtesting.Wedidnotusetheirvalidationsplits.
Foreachdataset,weusedIRGentoselectthetop- ğ¾optimization
sequencesretainedbytheGAprocess.WethencompiledeachC
source code in the training datasets into ğ¾pieces of LLVM IR code
to extend the training datasets.
Ourmethodrequiresthat thetrainingcodesbe compilable.W e
indeedexploredsomeotherdatasetssuchasDevign[ 106].However,
wefoundthatmanyofitscasescannotbecompiled.Fixingthese
issues would have required considerable manual effort. Another
practicalconcernis cost;assoonreportedin Cost,training CodeCMR
on GCJ already takes over 90 hours on 16 Tesla V100 GPU cards.
Consideringlargerdatasetsisoutofscopeforthisresearchproject.
Table 3: Statistics of the dataset used in evaluation.
Split GCJ POJ-104
Classes in training data 237 64
Programs in training data 238,106 28,103
Classes in test data 31 24
Programs in test data 22,795 10,876
Programs with macro 80,432 10
Average lines of C code 71.19 35.97
Average lines of LLVM IR code 1659.50 238.51
2259
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen Nie, Shi Wu
Table4:Accuracyofall(augmented)models.Foreachmetrics,wemark best models whentrainingwithCsourcecode.We
also mark the best models when training with C code and LLVM IR code optimized following different schemes. IRGen
denotes training CodeCMRusing source code and six collections of LLVM IR optimized by sequences formed by IRGen.
MethodGCJ POJ-104
MAP@R(%) AP(%) MAP@R(%) AP(%)
code2vec 7.76 (-0.79/+0.88) 17.95 (-1.24/+1.76) 1.90 (-0.43/+0.38) 5.30 (-0.80/+0.60)
code2seq 11.67 (-1.98/+1.73) 23.09 (-3.24/+2.49) 3.12 (-0.45/+0.67) 6.43 (-0.37/+0.48)
ncc 17.26 (-1.11/+0.57) 31.56 (-1.11/+1.46) 39.95 (-2.29/+1.64) 50.42 (-2.98/+1.61)
ncc-w/o-inst2vec 34.88 (-5.72/+7.63) 56.12 (-7.63/+9.96) 54.19 (-3.18/+3.52) 62.75 (-5.49/+4.42)
Aroma-Dot 29.08 42.47 52.08 45.99
Aroma-Cos 29.67 36.21 55.12 55.40
CodeCMR 64.86(-1.49/+0.72) 98.52(-0.16/+0.12) 76.39(-0.55/+1.30) 77.18(-2.95/+1.92)
MISIM-GNN 74.90(-1.15/+0.64) 92.15(-0.97/+0.7) 82.45 (-0.61/+0.40) 82.00 (-2.77/+1.65)
CodeBERT 68.95(-0.91/+0.37) 81.34(-1.29/+0.36) 82.67(-0.42/+0.33) 85.73(-1.14/+2.13)
CodeCMR-O0 81.08(-1.03/+0.58) 96.31(-0.34/+1.11) 82.90(-1.24/+0.97) 84.95(-2.53/+1.03)
CodeCMR-O1 83.87(-0.77/+0.24) 97.10(-0.27/+0.54) 83.37(-0.97/+0.31) 86.61(-1.35/+0.78)
CodeCMR-O2 82.60(-0.81/+0.19) 96.28(-0.57/+0.27) 84.29(-1.24/+0.53) 85.96(-1.18/+0.91)
CodeCMR-O3 82.67(-1.13/+0.69) 96.77(-0.44/+0.64) 84.21(-0.43/+0.98) 85.06(-0.83/+0.39)
CodeCMR-Os 85.17(-0.24/+0.38) 98.02(-0.31/+0.13) 83.81(-0.93/+0.24) 85.07(-0.72/+1.21)
CodeCMR-demix 84.93(-1.44/+0.73) 98.02(-0.49/+0.31) 85.14(-0.71/+0.76) 88.58(-0.93/+0.44)
IRGen 86.48(-1.13/+1.57) 99.94(-0.07/+0.02) 89.18(-0.33/+0.61) 93.24(-0.21/+0.09)
BaselineModels. Tocomparewith CodeCMRaugmentedbyIRGen,
we configure seven embedding models, including CodeBERT [31],
code2vec [11],code2seq [9],ncc[16],Aroma[58],CodeCMR and
MISIM[98].CodeCMR was introduced in Sec. 3.CodeBERT ,ncc, and
MISIMwere introduced in Sec. 2.2.
nccis a unique and extensible code embedding framework that
learns directly from LLVM IR code. As expected, ncccan be aug-
mented with LLVM IR optimized by IRGen (see Sec. 6.4). When
using ncc,weassessedtwovariants, nccandncc-w/o-inst2vec .
The latter model omits the standard ins2vec model [16] for IR
statement-level embedding and instead uses a joint learning ap-
proachtosimultaneouslycomputeandfine-tunethestatementand
graph-levelembeddings.For MISIM,weleverageditsprovidedvari-
ant, referred to as MISIM-GNN , that leverages GNNs in the learning
pipeline and has been shown to outperform other MISIMvariants.
AromawasreleasedbyFaceboo ktofacilitatehigh-speedquery
matching from a database of millions of code samples. Aromadoes
notperform neural embedding but instead contains a set of con-
ventional code matching techniques (pruning, clustering, etc.). We
selected AromaforcomparisonbecauseitisaSOTAproductiontool
that also features code clustering and similarity analysis. Hence,
Aromaandneuralembeddingtoolscanbecomparedonanequiv-
alent basis, demonstrating the strength of SOTA neural embed-ding tools, particularly after augmentation using IRGen. The of-ficial codebase of
Aromaprovides two variants, Aroma-Dot and
Aroma-Cos. We benchmarked both variants.
Cost.OurlearningandtestingforGAwereconductedonadesktop
machine with two Intel Core(TM) i7-8700 CPU and 16GB RAM.
The machine was running Ubuntu 18.04. IRGen takes averaged
143.52 and 963.41 CPU hours to finish all 800 iterations of GA
procedureforPOJ-104andGCJ,respectively.DespitethehighCPU
hours, we clarify that the wall-clock time can be largely reduced
viaparallelism.Weexploredtore-runtheGAprocedureona64-
core CPU server. We report that it takes about 25 wall-clock hoursfor POJ-104, and about 81 wall-clock hours for GCJ. Setting this
parallelismchangesabout60LOCinIRGen;seeourcodebaseat[ 1].
When needed, it is also possible to optimize GA with subsampling
for extremely large datasets.
Training embedding models are usually very costly. We employ
aGPUserverfortrainingforallinvolvedmodels.Theserverhas
twoIntel(R)Xeon(R)Platinum8255CCPUsoperatingat 2.50GHz,
384 GB of memory and 16 NVIDIA Tesla V100 GPU, each with32GB RAM. The learning rate is 0.001 and the repeat number ofresidual blocks is 11; other settings of our extended
CodeCMRare
the same with the standard CodeCMR setting. In total, over 120
epochs took approximately 15.9 and 91.3 hours for POJ-104 and
GCJ, respectively.
6.1 Accuracy of IRGen
We first answer RQ1using Table 4. For neural embedding models,
welauncheachexperimentsforthreetimesandreporttheaverage,
aswellastheminimumandmaximumscoresinparentheses.Ta-
ble4reportstheevaluationresultsofbaselinemodelsinlines3â€“11.
In accordance with our research motivation (Sec. 3), we also report
results using CodeCMR augmented with IR code optimized by stan-
dard optimization levels (-O0, -O1, -O2, -O3, -Os). CodeCMR-demix
representstraining CodeCMRbyusingsourcecodeandfivesetsof
IR compiled by all five default optimization levels. The last row in
Sec.3reports the performance metrics for CodeCMR augmented by
sixcollections ofLLVM IRsoptimizedusing sequencesgenerated
by IRGen. For both the POJ-104 and GCJ datasets, in addition to
MAP ,w eusedAP[ 13]asthemetric.Bothmetricsarecommonly
usedinrelevantresearchtoassessperformanceofembeddingmod-els. AP stands for Average Precision, a method combines recall and
precision for ranked retrieval results. For both metrics, a higher
score indicates better performance.
2260
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Our results show that modern embedding models, including
CodeBERT ,CodeCMR, and MISIM-GNN , can largely outperform con-
ventional code matching tools suchas Aroma-Dot andAroma-Cos .
WhenlearningoverCsourcecode,wefoundthat CodeBERT wasthe
bestperformingmodelforthePOJ-104dataset,whereas MISIM-GNN
delivered the best performance for the GCJ dataset. In contrast,
CodeCMRperformedlesswellthaneitheroftheSOTAmodelsacross
all of the evaluated metrics. code2vec andcode2seq shows rela-
tively lower accuracy compared with others. Since we run each
evaluationthreetimes,wefindthattheiraccuracyscoresareun-
stable.Suchobservationisalsoconsistentlyreportedinprevious
works [98]. Nevertheless, even the â€œpeakâ€ accuracy scores of them
are still much lower than that of the SOTA models.
WhenlearningfromIRoptimizedusingstandardoptimization
levels, CodeCMRoutperformed SOTA model MAP scores by more
than10%ontheGCJdataset.Evaluationofthisformof CodeCMR
training on the POJ-104 dataset showed consistently promising
enhancementrelativetotheSOTAmodelsinmostcases.Also,com-
paring with augmenting CodeCMR with one collection of optimized
IR code, the CodeCMR-demix setting shows (slightly)better perfor-
mance, particularly for the POJ-104 setting. This also reveals the
strength of training with multiple diverse sets of IR code.
We found that CodeCMR, when augmented by findings of IRGen
(thelastrowofTable 4),constantlyandnotablyoutperformedall
the other settings. We interpret the evaluation results as highly
encouraging,showingthatIRGencangeneratehigh-qualityLLVM
IR code that enhances CodeCMR to significantly outperform the
SOTAmodels( CodeBERT andMISIM-GNN )onallmetrics.Again,we
note that IRGen is not limited to enhance CodeCMR: we present
evaluation of enhancing nccin Sec.6.4.
Figure5:Smoothedfitnessscoreincreasesover800iterations.
6.2 Fitness Function
RQ2assessestheefficiencyofourfitnessfunction.Fig. 5reports
thefitnessscoreincreasesfromall800IRGeniterationsacrosseach
GA campaign. The test cases, despite their diverse functionality,
manifested encouraging and consistent trends during optimization
searching. The fitness scores kept increasing and were seen to
reach saturation performance after around 410 to 600 iterations.
Weinterpretthatundertheguidanceofourfitnessfunction,IRGen
can find well-performing sequences for both datasets.Figure 6: Ordered contributions of each optimization flag.
6.3 Potency of Optimization Flags
This section answers RQ3by measuring the potency of selected
optimizationflags(selectedflagsarefullylisted at[ 1]).We report
thatforthePOJ-104dataset,thetop-1sequence ğ‘†havingthehighest
fitnessscorecontains49flags.Tomeasuretheircontribution,we
first train CodeCMRusing C source code and LLVM IR optimized
usingsequence ğ‘†andrecordthebaselineaccuracyas ğ‘ğ‘ğ‘.Then,we
iteratively discard one optimization flag ğ‘“fromğ‘†and measure the
augmentationeffectivenessof usingtheremainingsequencewith
48 flags. The accuracy drop reveals the contribution of flag ğ‘“.
Fig.6orders the contribution of each flag in ğ‘†. Overall, we in-
terpret that no â€œdominatingâ€ optimization flags are found in this
evaluation. In other words, we interpret that all these 49 flags man-
ifestreasonablecontributiontothemodelaugmentation,andthe
top-10 flags contributes in total 34.38%. We thus make our first
important observation w.r.t. RQ3:
Instead of identifying one or few dominating flags that
significantlycontributetoenhancingcodeembedding,it
is rather the formed sequence of optimization flags that is
important.
Thisevaluationshowsthatasequenceofflagsworkstogetherto
producehigh-qualityIR,insteadofoneorafewâ€œfranchiseplayersâ€
that can largely outperform other flags. In other words, the GA
processconductedbyIRGenis criticaltothisresearch,becauseit
offersageneralwaytoconstructsuchasequencewithmodestcost.
Wenowconsiderthecharacteristicsofthetenhighestpotency
flags. We put these flags into three categories as follows:
Simplify an IR Statement. Optimization flags, including -dce,
-early-cse ,-reassociate ,-bdceand-loop-deletion ,simplify
IR statementsvia various dataflow or controlflow analysis meth-
ods. For instance, -early-cse regulates IR statements by elimi-
nating common subexpression eliminations, and -reassociate
reassociatescommutativeexpressionstosupportbetterconstant
propagation. In all, these optimization can make two syntactically
distinct pieces of source code more similar in IR.
2261
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen Nie, Shi Wu
MakeIRStatementSequencesClosertoSourceCode. Flags,in-
cluding -mem2reg ,-instcombine ,and -dse,cansimplifythecom-
piledIRcode,makingit moresimilartothesourcecode.Forinstance,
-mem2reg promotes memory references to be register references.
This prunes verbose memory usage in IR code and generates IR
codesimilarwithsourcecodeintermsofmemoryreference.Other
flags, such as -instcombine and-dse, combine and simplify in-
structionstoformfewerandmoresuccinctinstructionsequences
that are generally closer to source code.
SimplifyCFG. Optimizationflags,including -break-crit-edges ,
-simplifycfg , and -loop-rotate , perform more holistic transfor-
mations to simplify the IR code CFGs. For instance, -simplifycfg
performs dead code elimination and basic block merging by elimi-
natinguselessbasicblocksandtheirassociatedLLVMPHInodes.
ByregulatingCFGs,theseoptimizationsdeliversimilarIRsfrom
two semantically similar but syntactically different source codes.
Our analysis identified numerous optimization flags that signifi-
cantlyimprovedthetrainingIRsforembeddinglearning.Thisis
intuitive,giventhattheylaunchtransformationfromdifferentgran-
ularities. More importantly, we make the following observation to
characterize important optimization flags:
Optimizationpassesthat simplifyandregulateIRcode,ei-
therattheIRstatementlevelortheCFGlevel,aregenerally
desirable.
Many of these flags are often employed as cleanup passes to run
after compiler inter- or intra-procedural optimizations.
Optimization Passes on GCJ
We further analyze the top-1 optimization sequences found by
IRGenfortheGCJdataset.Thistop-1sequencecontains50flags.
Given that the top-1 sequence found over POJ-104 contains 49
flags,we furthermeasurethe agreementofthese twosequencesby
counting the number of flags appeared in both sequences. These
two sequences agree on 28 flags. The top-3 flags in the POJ-104sequence all exist in the intersection set, and five of the top-10flags in the POJ-104 sequence exist in the intersection set. With
respecttothe(dis)agreement,weconductamanualinvestigation
and summarize our findings as follows:
Agreement. We found that these 29 overlapping flags primar-
ily serve the purpose of simplifying and regulating IR code indifferent ways. For instance,
-reassociate and -deadargelim
simplifyIRstatementsanddeletedeadarguments.Therestover-
lapping flags are used as utilities for other passes (e.g., -lcssa
servesloop-orientedoptimizations)orforanalysispurposes(e.g.,
-block-freq ).Overall,weinterpretthatcodecleanupandregula-
tion are generally applicable to enhancement of learning quality.
Disagreement. Given that POJ-104 test cases are relatively suc-
cinct, we find that IRGen tended not to select flags that focus onshrinking the code size. In contrast, GCJ contains much lengthyC/C++ code, whose derived LLVM IR code is even more lengthy.
Hence,IRGenadaptivelyprioritizesmoreflagstoreducethesize.
Overall, we find that whether IRGen inclines to â€œshrinkâ€ code size
is dataset-dependent. IR compiled from POJ is generally shorter
thanthatofGCJ;therefore,ithasfewerOOVissuesandtheneedforshrinkingislessfrequent.GCJhaslengthyIRandmoreOOVIR
statements; it is demanding to shrink IR to avoid OOV. In addition,
GCJfeaturesmorefloatingnumberrelatedprogrammingtasks,and
accordingly, floating number related flags, such as -float2int ,
are involved to turn floating numbers into integers and effectively
reducethe#OOVcases.Incontrast,POJ-104datasetdoesnotpri-
marily involve floating number-related computations.
Overall,we interpretthese resultsas promising:wefound that
over half of the optimization flags selected by IRGen (29;29/49=
59.2%ofallflagsselectedoverPOJ-104)wereselectedacrosstwo
datasets of different complexity without using any pre-knowledge
orin-depthprogramanalysis.Theseoverlappingflagsfurtherhigh-lighttheimportanceofcleaningupandregulatingIRcodetomake
neural embedding models more robust. Moreover, the 42.9% dis-
agreement, to some extent, shows that IRGen enables the selection
of more diverse flags adaptive to different datasets.
Table 5: Augment nccover the POJ-104 dataset.
Model MAP@R(%)
ncc 39.95
ncc-random 40.34
ncc-IRGen 56.07
ncc-w/o-inst2vec 54.19
ncc-w/o-inst2vec -random 55.10
ncc-w/o-inst2vec -IRGen 60.46
6.4 Generalization
Asaforementioned,augmentation(includingthefitnessfunction;
see Sec.4.3) delivered by IRGen is independent from particular em-
beddingmodeldesign.Toanswer RQ4,wedemonstratethegeneral-
izationof IRGenbyaugmentinganotherpopularneuralembeddingmodel,
ncc.Aspreviouslydescribed, nccperformsembeddingover
LLVMIRcode.Therefore,wedidnotneedtochangetheimplemen-tationof
ncc.The nccaugmentationevaluationresultsarereported
in Table5. To compare with optimization sequences formed by IR-
Gen, we also prepare a â€œbaselineâ€, denoting a sequence containing
randomlyselected49optimizationflags.Thesetwobaselineresults
are reported in third and sixth rows.
As expected, augmentation notably improved the quality of ncc
and ncc-w/o-inst2vec . Particularly, the MAP score of the lat-
ter model is improved to 60.46%, which is even higher than thescores achieved by two variants of Aroma. In contrast, we find
thattworandomschemesshownegligibleenhancement.Overall,
this evaluation indicates that IRGen delivers general augmentation
for neural code embedding models, without consideration of the
specific model designs.
6.5 Augmentation with Small Training Data
RQ5considersascenariowhereprogramembeddingisinhibited
byalimitedamountoftrainingdata.Wearguethatthisisacom-
mon situation, such as in vulnerability analysis where only limited
vulnerablecodesamplesareavailable.Inthesesituations,wean-
ticipate the feasibility and usefulness of extending training dataset
and augmenting embedding models using optimized IR code.
Fig.7presentstheevaluationresultsofaugmentingasmalltrain-
ingdataset.Specifically,werandomlyselected1%oftheCsource
2262
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 7: Performance with different size of training data.
codes from the POJ-104 training data to train CodeCMR and mea-
sured the resulting embedding accuracy, which was quite low (the
first bar in Fig. 7; MAP = 24.08%). However, after using different
standardoptimizationlevelsandoptimizationsequencesselected
byIRGen,theMAPaccuracyincreasedtoover40%,almost doubling
theoriginalMAPscore.Inadditiontoanextreme1%sampletest,
wealsorandomlyselected5%,20%,and50%ofthePOJ-104train-
ing data and re-trained CodeCMR. As shown in Fig. 7, we further
augmentedthemodelforeachsubsetbyusingthesamestandard
optimization flags and flags selected by IRGen. We consistently
achievedpromisingaugmentationresults.Inparticular,optimiza-
tion flags found by IRGen outperformed all other augmentation
schemes in all of these small training data settings. These intuitive
andhighlypromisingevaluationresultsindicatethatIRcodecan
beleveragedtosignificantlyandreliablyaugmentcodeembedding,
particularly when only very limited data are available. Comparing
with standard optimization levels, optimization sequences selected
by IRGen can result in even higher enhancement.
7 DISCUSSION
GeneralizabilityofDownstreamApplications. Thisworkfo-
cusesonarepresentativedownstreamtaskofcodeembeddingâ€”
codeclonedetection.Toclarifythegeneralizabilitybehindâ€œcode
cloneâ€:programembeddingscanbeusedasthebasisofavarietyof
downstream tasks like malware clustering, vulnerability detection,
and code plagiarism detection. Holistically, many of these applica-
tions are based on deciding two softwareâ€™s similarity. Therefore,
weviewâ€œcodecloneâ€detectionasacorebasistoassessthoseap-
plications. Nevertheless, the augmentation pipeline of IRGen is
generally orthogonal toaparticulardownstreamtask.Weleaveitas
onefutureworktobenchmarktheaugmentationcapabilityoffered
byIRGentowardother importantdownstream applications,such
as vulnerability detection and program repair. We envision to have
consistently promising observations.
Conflicts Between Optimization Flags. gccdocuments a set of
constraintsbetweenoptimizationpasses[ 32]whereinusingtwo
conflicting passes can result in compilation errors. However, to
our knowledge, the LLVM compiler framework does not explicitly
document any â€œconflictingâ€ flags. We are also not aware of any
compilationerrorscausedbyusingtwoconflictingLLVMpasses.
Incaseswhereoneflaghasconflictswithotherflags,suchinforma-
tion can be explicitly encoded as constraints to validate generated
optimization sequences.Other Learning Methodologies. Production compiler frame-
works like gccand LLVM provide considerable optimization flags,
formingalargesearchspaceinourresearchscenario.IRGenconsti-
tutesaGA-basedapproachtosearchfor(near-)optimaloptimizationsequences.Ourempiricalevaluationshowsthattheproposedlearn-
ingprocessis sufficient toidentifyhigh-performingoptimization
sequences. We note that there are more advanced (evolutionary)optimization algorithms available. In particular, the two fitnessobjectivescouldhavebeenoptimizedseparately(i.e.,withmulti-objective optimization). Also, from a holistic view, searching for
optimization sequences is a Markov Decision Process (MDP). Com-
plex MDPs (e.g., auto-driving) can be likely addressed with rein-
forcementlearning(RL)techniques.FutureworkmayexploreusingadvanceddeepRLmodels,whichhaveachievedprominentsuccess
in solving real-world challenges in autonomous driving [ 28,84]
and video games [65].
8 RELATED WORK
WereviewedprogramembeddingfromvariousperspectiveinSec. 2.
The development of IRGen was inspired by existing works in
search-based software engineering [ 39,40] and search-based it-
erativecompilationtechniques[ 12,22,24,48,49].Ingeneral,many
tasks in software engineering require exploration of (optimal) solu-
tionsunderarangeofconstraintsandtrade-offsbetweenresources
andrequirements.Tothisend,metaheuristicalgorithms,suchas
local search, simulated annealing (SA) [ 47], genetic algorithms
(GAs) [95], and hill climbing (HC) [ 76] are frequently used to ad-
dress these challenges. Typical applications include testing and de-
bugging[ 38,62,63],verification[ 5â€“7,23,34],maintenance[ 69,81],
and software hardening [33, 33,83].
Alineofrelevantandactively-developedresearchaugmentssoft-
wareobfuscationbycombiningobfuscationpasses.Liuetal.[ 55]
searchforasequenceofobfuscationpassestomaximizeobfuscation
effectiveness(andthusmakesoftwaremoresecure).Amoeba[ 92]
empiricallydemonstratedthatcombiningobfuscationpasses,though
enhancingobfuscationpotency,oftencarrieshighcosts.Wanget
al. [87] trained a reinforcement learning model to explore optimal
obfuscation combinations by taking both cost and effectiveness
intoaccount.BinTuner[ 74]usesaguidedstochasticalgorithmto
explore how combinations of compiler optimization passes can
obfuscate software.
9 CONCLUSION
Existing neural program embedding methods are generally limited
to processing of program source code. We present simple, yet ef-fective, strategies to improve embedding quality by augmenting
trainingdatasetswithcompiler-generatedIRcode.Inadditiontouseofdefaultcompileroptimizationlevels,wepresentIRGen,asearch-
basedframeworktofindcustomizedoptimizationsequencesthat
produceIRsthatcansubstantiallyimprovethequalityoflearnedem-
beddings. In evaluation, these models outperformed others trained
usingonlysourcecodeorIRgeneratedwithdefaultoptimization
combinations.Ourstudyprovidesinsightsandguidanceforusersaiming to generate higher quality code embeddings.
2263
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang and Qiyi Tang, Sen Nie, Shi Wu
ACKNOWLEDGEMENTS
ThisworkwassupportedinpartbyCCF-TencentOpenResearch
Fund.Wearegratefultotheanonymousreviewersfortheirvaluable
comments.
REFERENCES
[1] [n.d.]. IRGen Website. https://sites.google.com/view/irgen .
[2][n.d.]. OneSourceCodeCompiledintoDiverseIRPrograms. https://sites.google.
com/view/irgen/main-page/diverse-ir-code-example .
[3][n.d.]. POJ104 Code Sample and LLVM IR. https://sites.google.com/view/irgen/
main-page/motivation-code-example .
[4][n.d.]. Proposals of Other Pipelines. https://sites.google.com/view/irgen/main-
page/proposals-of-other-pipelines.
[5]EnriqueAlbaandFranciscoChicano.2007.ACOhg:Dealingwithhugegraphs.In
Proceedingsofthe9thannualconferenceonGeneticandevolutionarycomputation.
10â€“17.
[6]Enrique Alba and Francisco Chicano. 2007. Ant colony optimization for
model checking. In International Conference on Computer Aided Systems Theory.
Springer, 523â€“530.
[7]Enrique Alba and Francisco Chicano. 2007. Finding safety errors with ACO. In
Proceedingsofthe9thannualconferenceonGeneticandevolutionarycomputation.
1066â€“1073.
[8]Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learn-
ingtoRepresentProgramswithGraphs.In InternationalConferenceonLearning
Representations.
[9]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).
[10]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A general path-based representation for predicting program properties. ACM SIGPLAN Notices
53, 4 (2018), 404â€“419.
[11]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. Code2Vec:Learning Distributed Representations of Code. Proc. ACM Program. Lang. 3,
POPL (Jan. 2019).
[12]Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-Kelley,
JeffreyBosboom,Una-MayOâ€™Reilly,andSamanAmarasinghe.2014. Opentuner:
An extensible framework for program autotuning. In Proceedings of the 23rd
internationalconference on Parallel architectures and compilation . 303â€“316.
[13]Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press / Addison-Wesley.
[14]Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and
Daniel Tarlow. 2016. DeepCoder: Learning to Write Programs. In Proceedings of
4th InternationalConference on Learning Representations .
[15]TalBen-Nun,AliceShoshanaJakobovits,andTorstenHoefler.2018.NeuralCode
Comprehension:A LearnableRepresentationofCodeSemantics. In Advances
in Neural Information Processing Systems. 3585â€“3597.
[16]Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural
CodeComprehension:ALearnableRepresentationofCodeSemantics. Advances
in Neural Information Processing Systems 31 (2018), 3585â€“3597.
[17]Sahil Bhatia and Rishabh Singh. 2016. Automated correction for syntax errors
in programmingassignments usingrecurrent neural networks. arXiv preprint
arXiv:1603.06129 (2016).
[18]KarstenMBorgwardtandHans-PeterKriegel.2005. Shortest-pathkernelson
graphs.In FifthIEEEinternationalconferenceondatamining(ICDMâ€™05).IEEE,
8â€“pp.
[19]Cristian Cadar. 2015. Targeted program transformations for symbolic execu-tion. InProceedings of the 2015 10th Joint Meeting on Foundations of Software
Engineering. 906â€“909.
[20]Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al .2008. KLEE: unas-
sisted and Automatic Generation of High-Coverage Tests for Complex Systems
Programs. In OSDI, Vol. 8.
[21]Colin Campbell.2002. Kernel methods: asurvey of currenttechniques. Neuro-
computing 48, 1-4 (2002), 63â€“84.
[22]YangChen,YuanjieHuang,LievenEeckhout,GrigoriFursin,LiangPeng,Olivier
Temam,andChengyongWu.2010. Evaluatingiterativeoptimizationacross1000
datasets.In Proceedingsofthe31stACMSIGPLANConferenceonProgramming
Language Design and Implementation. 448â€“459.
[23]FranciscoChicanoandEnriqueAlba.2008. FindinglivenesserrorswithACO.
In2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on
ComputationalIntelligence) . IEEE, 2997â€“3004.
[24]Keith D Cooper, Devika Subramanian, and Linda Torczon. 2002. Adaptive
optimizing compilers for the 21st century. The Journal of Supercomputing 23, 1
(2002), 7â€“22.
[25]Ron Cytron, Jeanne Ferrante, Barry K Rosen, Mark N Wegman, and F Kenneth
Zadeck. 1991. Efficiently computing static single assignment form and thecontroldependencegraph. ACMTransactionsonProgrammingLanguagesand
Systems (TOPLAS) 13, 4 (1991), 451â€“490.
[26]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
CoRRabs/1810.04805(2018).
[27]S. H. Ding, B. M. Fung, and P. Charland. 2019. Asm2Vec: Boosting Static Repre-
sentationRobustnessforBinaryCloneSearchagainstCodeObfuscationand
Compiler Optimization.In IEEE S&P.
[28]Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun. 2017. CARLA: An open urban driving simulator. In Conference on robot
learning. PMLR, 1â€“16.
[29]Alhussein Fawzi, Horst Samulowitz, Deepak Turaga, and Pascal Frossard. 2016.
Adaptive data augmentation for image classification. In 2016 IEEE international
conference on image processing (ICIP). Ieee, 3688â€“3692.
[30]StevenYFeng,VarunGangal,JasonWei,SarathChandar,SoroushVosoughi,
Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation
approaches for nlp. arXiv preprint arXiv:2105.03075 (2021).
[31]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, MingGong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al
.2020. CodeBERT:
A pre-trained model for programming and natural languages. arXiv preprint
arXiv:2002.08155 (2020).
[32]GCC.2021. OptionsThatControlOptimization. https://gcc.gnu.org/onlinedocs/
gcc/Optimize-Options.html.
[33]Shadi Ghaith and Mel O CinnÃ©ide. 2012. Improving software security using
search-based refactoring. In International Symposium on Search Based Software
Engineering. Springer, 121â€“135.
[34]Patrice Godefroid. 1997. Model checking for programming languages using
VeriSoft.In Proceedingsofthe24thACMSIGPLAN-SIGACTsymposiumonPrinci-
ples of programming languages. 174â€“186.
[35]PalashGoyalandEmilioFerrara.2018. Graphembeddingtechniques,applica-
tions, and performance: A survey. Knowledge-Based Systems 151 (2018), 78â€“94.
[36]DayaGuo,ShuoRen,ShuaiLu,ZhangyinFeng,DuyuTang,LIUShujie,Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al .2020. GraphCode-
BERT: Pre-training Code Representations with Data Flow. In International Con-
ference on Learning Representations.
[37]Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. Deep-fix: Fixing common c language errors by deep learning. In Thirty-First AAAI
Conference on Artificial Intelligence.
[38] Mark Harman, Yue Jia, and Yuanyuan Zhang. 2015. Achievements, open prob-
lems and challenges for search based software testing. In 2015 IEEE 8th Interna-
tionalConferenceonSoftwareTesting,VerificationandValidation(ICST).IEEE,
1â€“12.
[39]Mark Harman and Bryan F Jones. 2001. Search-based software engineering.
Information and software Technology 43, 14 (2001), 833â€“839.
[40]MarkHarman,SAfshinMansouri,andYuanyuanZhang.2012. Search-based
software engineering: Trends, techniques and applications. ACM Computing
Surveys (CSUR) 45, 1 (2012), 1â€“61.
[41]Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, andDavid Bieber. 2019. Global relational models of source code. In International
conference on learning representations.
[42]Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory.
Neural computation 9, 8 (1997), 1735â€“1780.
[43]XingHu,GeLi,XinXia,DavidLo,andZhiJin.2018. Deepcodecommentgenera-tion.In2018IEEE/ACM26thInternationalConferenceonProgramComprehension
(ICPC). IEEE, 200â€“20010.
[44]ParasJain,AjayJain,TianjunZhang,PieterAbbeel,JosephGonzalez,andIon
Stoica.2021. ContrastiveCodeRepresentationLearning.In EMNLP.5954â€“5971.
[45]Rie Johnson and Tong Zhang. 2017. Deep pyramid convolutional neural net-
worksfortextcategorization.In Proceedingsofthe55thAnnualMeetingofthe
Association for Computational Linguistics (Volume 1: Long Papers). 562â€“570.
[46]Rafael-MichaelKarampatsis,HlibBabii,RomainRobbes,CharlesSutton,and
Andrea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for
source code. In ICSE. IEEE, 1073â€“1085.
[47]Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. 1983. Optimization by
simulated annealing. science220, 4598 (1983), 671â€“680.
[48]PeterMWKnijnenburg,ToruKisuki,andMichaelFPOâ€™Boyle.2001. Iterative
compilation.In InternationalWorkshoponEmbeddedComputerSystems.Springer,
171â€“187.
[49]PrasadKulkarni,StephenHines,JasonHiser,DavidWhalley,JackDavidson,and
Douglas Jones. 2004. Fast searches for effective optimization phase sequences.
ACM SIGPLAN Notices 39, 6 (2004), 171â€“182.
[50]Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data Augmentation
using Pre-trained Transformer Models. CoRRabs/2003.02245(2020).
[51]ChrisLattnerandVikramAdve.2004. LLVM:Acompilationframeworkforlife-
long program analysis & transformation. In Code Generation and Optimization,
2004. CGO 2004. InternationalSymposium on . IEEE, 75â€“86.
[52]QuocLeandTomasMikolov.2014. Distributedrepresentationsofsentencesand
documents. In International conference on machine learning. PMLR, 1188â€“1196.
2264
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[53]Zhenhao Li and Lucia Specia. 2019. Improving neural machine translation
robustness via data augmentation: Beyond back translation. arXiv preprint
arXiv:1910.03009 (2019).
[54]Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017.
Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak
Supervision. In Proceedings of the 55th Annual Meeting of the Association for
ComputationalLinguistics (Volume 1: Long Papers) , Vol. 1. 23â€“33.
[55]HanLiu,ChengnianSun,ZhendongSu,YuJiang,MingGu,andJiaguangSun.
2017. Stochastic optimization ofprogram obfuscation. In 2017 IEEE/ACM 39th
InternationalConference on Software Engineering (ICSE) . IEEE, 221â€“231.
[56]Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, and Soroush
Vosoughi. 2020. Data Boost: Text Data Augmentation Through Reinforcement
Learning Guided Conditional Generation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020. Association for Computational Linguistics, 9031â€“9041.
[57]Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin D Cubuk.
2019. Improvingrobustnesswithoutsacrificingaccuracywithpatchgaussian
augmentation. arXiv preprint arXiv:1906.02611 (2019).
[58]SifeiLuan,DiYang,CelesteBarnaby,KoushikSen,andSatishChandra.2019.
Aroma: Code recommendation via structural code search. Proceedings of the
ACM on Programming Languages 3, OOPSLA (2019), 1â€“28.
[59]Lannan Luo, Jiang Ming, Dinghao Wu, Peng Liu, and Sencun Zhu. 2014.
Semantics-basedObfuscation-resilientBinaryCodeSimilarityComparisonwith
Applicationsto Software Plagiarism Detection. In FSE.
[60]Pingchuan Maand Shuai Wang.2022. MT-Teql: Evaluatingand Augmenting
Neural NLIDB on Real-world Linguistic and Schema Variations. In PVLDB.
[61]Pingchuan Ma, Shuai Wang, and Jin Liu. 2020. Metamorphic Testing and Certi-
fied Mitigation of Fairness Violations in NLP Models. In IJCAI. 458â€“465.
[62]Phil McMinn. 2004. Search-based software test data generation: a survey. Soft-
ware testing, Verification and reliability 14, 2 (2004), 105â€“156.
[63]Phil McMinn. 2011. Search-based software testing: Past, present and future. In
2011IEEEFourthInternationalConferenceonSoftwareTesting,Verificationand
Validation Workshops. IEEE, 153â€“163.
[64]JunghyunMin,RThomasMcCoy,DipanjanDas,EmilyPitler,andTalLinzen.
2020. Syntactic data augmentation increases robustness to inference heuristics.
arXiv preprint arXiv:2004.11999 (2020).
[65]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[66] Lili Mou,Ge Li, LuZhang, TaoWang, andZhi Jin.2016. Convolutional neural
networksovertreestructuresforprogramminglanguageprocessing.In Thirtieth
AAAI Conference on Artificial Intelligence.
[67] Lili Mou,Ge Li, LuZhang, TaoWang, andZhi Jin.2016. Convolutional neural
networks over tree structures for programming language processing. (2016).
[68]Kevin Musgrave, Serge J. Belongie, and Ser-Nam Lim. 2020. A Metric Learning
RealityCheck.In ComputerVision-ECCV2020-16thEuropeanConference,Glas-
gow,UK,August23-28,2020,Proceedings,PartXXV (LectureNotesinComputer
Science, Vol. 12370). Springer, 681â€“699.
[69]MarkOâ€™KeeffeandMelOCinnÃ©ide.2008. Search-basedrefactoringforsoftware
maintenance. Journal of Systems and Software 81, 4 (2008), 502â€“516.
[70]QiPang,YuanyuanYuan,andShuaiWang.2021. MDPFuzzer:FindingCrash-
Triggering State Sequences in Models Solving the Markov Decision Process.
arXiv preprint arXiv:2112.02807 (2021).
[71]EmilioParisotto,AbdelrahmanMohamed,RishabhSingh,LihongLi,Dengyong
Zhou, and Pushmeet Kohli. 2016. Neuro-Symbolic Program Synthesis.
[72]Juraj PetrÃ­k. 2020. Google Code Jam programming competition. https://github.
com/Jur1cek/gcj-dataset .
[73]Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran
Sahami,andLeonidasGuibas.2015. Learningprogramembeddingstopropagate
feedback on student code. In ICML.
[74]Xiaolei Ren, Michael Ho, Jiang Ming, Yu Lei, and Li Li. 2021. Unleashing the
hidden power of compiler optimization on binary code difference: an empirical
study(PLDI).
[75]MarcoTulioRibeiro,CarlosGuestrin,andSameerSingh.2019. Areredroses
red? evaluating consistency of question-answering models. In ACL. 6174â€“6184.
[76]Bart Selman and Carla P Gomes. 2006. Hill-climbing search. Encyclopedia of
cognitive science 81 (2006), 82.
[77]Ramprasaath R Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,Marco Tulio Ribeiro, Besmira Nushi, and Ece Kamar. 2020. SQuINTing at
VQA Models: Introspecting VQA Models With Sub-Questions. In CVPR.
[78]Eui Chul Richard Shin, Dawn Song, and Reza Moazzezi. 2015. Recognizing
functionsinbinarieswithneuralnetworks.In InProceedingsofthe24thUSENIX
Security Symposium. 611â€“626.
[79]Connor Shorten and Taghi M Khoshgoftaar. 2019. A survey on image data
augmentationfor deep learning. Journal of Big Data 6, 1 (2019), 1â€“48.
[80]XujieSi,Hanjun Dai,MukundRaghothaman,MayurNaik, andLeSong.2018.
Learning Loop Invariants for Program Verification.. In Advances in Neural
Information Processing Systems (NeurIPS).[81]ChrisSimons,JeremySinger,andDavidRWhite.2015. Search-basedrefactoring:
Metricsarenotenough.In InternationalSymposiumonSearchBasedSoftware
Engineering. Springer, 47â€“61.
[82]YuleiSui,XiaoCheng,GuanqinZhang,andHaoyuWang.2020.Flow2Vec:Value-
flow-basedprecisecodeembedding. ProceedingsoftheACMonProgramming
Languages 4, OOPSLA (2020), 1â€“27.
[83]JulianThomÃ©,AlessandraGorla,andAndreasZeller.2014.Search-basedsecurity
testing of web applications. In Proceedings of the 7th International Workshop on
Search-Based Software Testing. 5â€“14.
[84]Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. 2020. End-to-end
model-free reinforcement learning for urban driving using implicit affordances.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition. 7153â€“7162.
[85]David Trabish, Andrea Mattavelli, Noam Rinetzky, and Cristian Cadar. 2018.
Choppedsymbolicexecution.In Proceedingsofthe40thInternationalConference
on Software Engineering. 350â€“360.
[86]treesitter-release[n.d.]. Tree-sitter. https://github.com/tree-sitter/tree-sitter.
https://tree-sitter.github.io/tree-sitter/.
[87]Huaijin Wang, Shuai Wang, Dongpeng Xu, Xiangyu Zhang, and Xiao Liu.
2020. GeneratingEffectiveSoftwareObfuscationSequenceswithReinforcement
Learning. IEEE Transactions on Dependable and Secure Computing (2020).
[88]Ke Wang, Rishabh Singh, and Zhendong Su. 2017. Dynamic neural program
embedding for program repair. In ICLR.
[89]Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Dynamic Neural ProgramEmbeddings for Program Repair. In 6th International Conference on Learning
Representations.
[90]Ke Wang and Zhendong Su. 2019. Learning blended, precise semantic program
embeddings. In POPL.
[91]Shuai Wang and Zhendong Su. 2020. Metamorphic Object Insertion for Testing
Object Detection Systems. In ASE.
[92]ShuaiWang,Pei Wang, andDinghaoWu.2017. Compositesoftwarediversifica-
tion(ICSME).
[93]Jason Wei and Kai Zou. 2019. Eda: Easy data augmentation techniques for
boostingperformanceontextclassificationtasks. arXivpreprintarXiv:1901.11196
(2019).
[94]Kilian Q Weinberger and Lawrence K Saul. 2009. Distance metric learningfor large margin nearest neighbor classification. Journal of machine learning
research10, 2 (2009).
[95]Darrell Whitley. 1994. A genetic algorithm tutorial. Statistics and computing 4,
2 (1994), 65â€“85.
[96]Cameron R.Wolfe and KeldT. Lundgaard. 2019. Data Augmentationfor Deep
Transfer Learning. CoRRabs/1912.00772(2019).
[97]Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4â€“24.
[98]Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marucs, Nesime Tatbul,
Jesmin Jahan Tithi, Paul Petersen, Timothy Mattson, Tim Kraska, Pradeep
D u be y ,e ta l .2020. MISIM: An end-to-end neural code similarity system. arXiv
preprint arXiv:2006.05265 (2020).
[99]Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and Shi Wu. 2020.
OrderMatters:Semantic-AwareNeuralNetworksforBinaryCodeSimilarity
Detection. (2020).
[100]ZepingYu,WenxinZheng,JiaqiWang,QiyiTang,SenNie,andShiWu.2020.
CodeCMR: Cross-Modal Retrieval For Function-Level Binary Source Code
Matching. Advances in Neural Information Processing Systems 33 (2020).
[101]Yuanyuan Yuan, Qi Pang, and Shuai Wang. 2021. Enhancing Deep Neural
Networks Testing by Traversing Data Manifold. arXiv preprint arXiv:2112.01956
(2021).
[102]Yuanyuan Yuan, Shuai Wang, Mingyue Jiang, and Tsong Yueh Chen. 2021.
Perception Matters: Detecting Perception Failures of VQA Models Using Meta-
morphic Testing (CVPR).
[103]Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William E. Byrd,Matthew Might, Raquel Urtasun, and Richard Zemel. 2018. Neural Guided
Constraint Logic Programming for Program Synthesis.
[104]XiangZhang,JunboZhao,andYannLeCun.2015. Character-levelconvolutional
networks for text classification. Advances in neural information processing
systems28 (2015), 649â€“657.
[105]AliceZhengandAmandaCasari.2018. Featureengineeringformachinelearning:
principles and techniques for data scientists. " Oâ€™Reilly Media, Inc.".
[106]YaqinZhou,ShangqingLiu,JingKaiSiow,XiaoningDu,andYangLiu.2019. De-
vign:EffectiveVulnerabilityIdentificationbyLearningComprehensiveProgram
Semanticsvia GraphNeural Networks.In Advancesin NeuralInformation Pro-
cessing Systems 32: Annual Conference on Neural Information Processing Systems
2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada. 10197â€“10207.
2265
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:42 UTC from IEEE Xplore.  Restrictions apply. 