Automated Testing of Software that Uses Machine Learning APIs
Chengcheng Wan
University of Chicago
cwan@uchicago.eduShicheng Liu
University of Chicago
shicheng2000@uchicago.eduSophie Xie
Whitney Young High School
sxie2@cps.eduYifan Liu
University of Chicago
liuyifan@uchicago.edu
Henry Hoffmann
University of Chicago
hankhoffmann@uchicago.eduMichael Maire
University of Chicago
mmaire@uchicago.eduShan Lu
University of Chicago
shanlu@uchicago.edu
ABSTRACT
Anincreasingnumberofsoftwareapplicationsincorporatemachine
learning (ML) solutions for cognitive tasks that statistically mimic
human behaviors. To test such software, tremendous human effort
isneededtodesignimage/text/audioinputsthatarerelevanttothe
software, and to judge whether the software is processing these
inputsasmosthumanbeingsdo.Evenwhenmisbehaviorisexposed,
it is often unclear whether the culprit is inside the cognitive ML
API or the code using the API.
This paper presents Keeper, a new testing tool for software that
uses cognitive ML APIs. Keeper designs a pseudo-inverse function
for each ML API that reverses the corresponding cognitive task in
anempiricalway(e.g.,animagesearchenginepseudo-reversesthe
image-classificationAPI),andincorporatesthesepseudo-inverse
functions into a symbolic execution engine to automatically gener-
ate relevant image/text/audio inputs and judge output correctness.
Once misbehavior is exposed, Keeper attempts to change how ML
APIsareusedinsoftwaretoalleviatethemisbehavior.Ourevalu-
ationonavarietyofopen-sourceapplicationsshowsthatKeeper
greatly improves the branch coverage, while identifying many pre-
viously unknown bugs.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging;‚Ä¢Computing methodologies ‚ÜíMachine learning;‚Ä¢
Information systems ‚ÜíRESTful web services.
KEYWORDS
software testing, machine learning, machine learning API
ACM Reference Format:
ChengchengWan,ShichengLiu,SophieXie,YifanLiu,HenryHoffmann,
Michael Maire, and Shan Lu. 2022. Automated Testing of Software that
UsesMachineLearningAPIs.In 44thInternationalConferenceonSoftware
Engineering(ICSE‚Äô22),May21‚Äì29,2022,Pittsburgh,PA,USA. ACM,New
York, NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510068
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35100681 INTRODUCTION
1.1 Motivation
Machine learning (ML) offers powerful solutions to cognitive tasks,
allowingcomputerstostatisticallymimichumanbehaviorsincom-
puter vision, language, and other domains. To facilitate easy use
of these ML techniques, many cloud providers offer well-designed,
well-trained, and easy-to-use cognitive ML APIs [ 1‚Äì5]. Indeed,
many software applications in a variety of domains are incorpo-
rating ML APIs [ 6,7]. Thus, effectively testing these applications‚Äî
which this paper refers to as ML software ‚Äîhas become urgent.
Tobetterunderstandthistestingtask,considerPhoenix[ 8],afire-
alarmapplication.AsshowninthetophalfofFigure1,Phoenixuses
the Google label_detection API to perform image classification
on aninput photo, and then triggersan alarm if anyof the top-3
classificationlabelsreturnedbytheAPIincludesthekeyword"fire".
This simple demo application turns out to be difficult to test.
First,randominputsworkpoorly,astheyrarelycontainfireandhence cannot exercise the critical
alarm()branch. Second, even
with carefully collected image inputs, manual checking is likely
neededtojudgetheexecutioncorrectness(i.e.,whetheranalarm
shouldbetriggered).Finally,evenafterafailedtestrun‚Äîe.g.,the
picture on the right of Figure 1 fails to trigger the alarm‚Äîit is
difficult to know whether the failure is due to the statistical nature
oflabel_detection ,whichhastobetolerated,ortheapplication‚Äôs
incorrect use of the API, which has to be fixed. In fact, this case
belongstothelatter:therightfigureactuallyhasatop-3label‚Äúflame‚Äù
returned by label_detection ; not checking for the ‚Äúflame‚Äù label,
this application may miss fire alarms in many critical situations.
 ! 
 
 
!

 !       





    
	   
 
 	
Figure 1: An example of using ML Cloud APIs [8].
This example has demonstrated several open challenges in test-
ing ML software.
1)Infinite,yetsparseinputspaces.Thespacesofimages,texts,
oraudios‚ÄîtypicalinputformsofcognitiveMLAPIs‚Äîareinfinitelylarge,yet realisticinputsthatare relevanttothesoftware-under-test
2122022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA C. Wan, S. Liu, S. Xie, Y. Liu, H. Hoffmann, M. Maire, and S. Lu
are spread sparsely throughout this space. For example, only a tiny
portion of real-world images contain fire and are relevant to the
fire alarm software.
Existinginputgenerationtechniquesareineffectivehere.Ran-
dom input generators cannot produce realistic inputs through
random-pixel images or random-character strings. Fuzzing tech-
niquesthatapplyperturbations(whitenoises[ 9],blockreplacement
[10],ormapping[ 11])toseedinputstendtoproduceinputsthatare
either unrealistic or similar with the seed. For example, no fuzzing
canturntheleftphotointotherightphotoinFigure1.Symbolicex-
ecution techniques also do not work, as it is difficult to express the
inputrealismasasolvableconstraint.Furthermore,noneofthese
techniquessolvestherelevancechallenge.Totellwhichimagesarerelevantforafirealarmapplicationrequiresbothanunderstanding
ofthesoftwarestructure(i.e.,knowingthatabranchpredicateis
aboutfireintheinput)andtheabilitytoperformtheverycognitive
task we need to test (i.e., judging whether a photo contains fire).
2)Outputcorrectnessrelyingonhumanjudgement.Cogni-
tive ML APIs are designed to statistically mimic human behaviors,
e.g., identifying the objects in an image, interpreting the emotionalsentimentinasentence,etc.Consequently,tojudgethecorrectness
of ML software, ideally, we want to ask many people to process
the same set of inputs and see if their decisions statistically match
withthesoftwareoutputs‚Äîa processthatisinherentlydifficultto
automate.Forexample,itisdifficulttotellwhetherthefirealarm
should be triggered or not without manual inspection (Figure 1).
In traditional testing, the execution correctness often can be
checkedautomaticallyusingthemathematicalrelationshipbetween
theinputsandtheoutputsorcertaininvariantsexpectedtohold
by the execution. These techniques are still useful for the non-
cognitivepartsoftheMLsoftware,butcannothelpthecognitive
parts. Previous work generated test oracles for domain-specificapplications, like an image dilation software [
12], a blood-vessel
categorizer[ 13],animageregiongrowthprogram[ 14],abiomedical
textprocessor[ 15].Theirdesigneachtargetsaparticularcognitive
task and cannot be applied for general ML software.
3)Probabilisticincorrectnessthatisdifficulttodiagnose .
When ML software produces outputs that differ from most human
beings‚Äô judgement, which we refer to as an accuracy failure, de-
velopers must attribute this failure to either the ML API or the
surrounding software‚Äôs use of the ML API. This attribution is diffi-
cultasMLAPIsusestatisticalmodelstoemulatecognitivetasks,
andareexpectedtoproduceincorrectoutputsfromtimetotime.In
other words, developers need to distinguish failures caused by the
probabilistic nature of the ML API, which simply must be tolerated
as part of using this specific ML API, from a misuse of the API,
which represents a bug and must be fixed by the developer.
Again,thissituationisdifferentfromthatintraditionalsoftware
testing, where a test failure like a crash indisputably points out
something incorrect with the software that needs to be fixed.
Note that, much recent work studies how to test [ 9,16‚Äì41] and
fix[42‚Äì45]neuralnetworks.However,theyfocusonimprovingthe
accuracy, fairness, and security of the neural network itself; e.g.,
makingsurethenetworkisrobustagainstadversarialsamplesor
does not contain certain biases, etc. They do notconsider how the
neural network is usedin the context of an application and do not
test how well the application using the neural network functions.1.2 Contributions
This paper proposes Keeper, a testing tool designed for software
that uses cognitive machine learning APIs (ML software).
Totackletheuniqueinputspaceandoutputoraclechallenges,
Keeper designsa set ofpseudo-inverse functions forcognitive ML
APIs1. For an API ùëìthat maps inputs fromdomain Ito outputs in
domain O, its pseudo-inverse function ùëì/primereverses this mapping at
the semantic level. We make sure that the mapping by ùëì/primehas been
confirmed by many people to have high accuracy. For example, the
Bing image search engine is a pseudo-inverse function of Google‚Äôs
image classification API.
Keeper then integrates the pseudo-inverse functions with sym-
bolicexecutiontoreachthesparseprogram-relevantinputspace.
Specifically,Keeperfirstusessymbolicexecutiontofigureoutwhat
valuesanML-APIoutputcantaketofulfillbranchcoverage(e.g.,
‚Äúfire‚Äù==labels[0].desc inFigure1).Keeperthenautomatically
generates realistic inputs that are expected to produce the desired
ML-APIoutputs,leveragingpseudo-inversefunctions.Forexample,
thetwoimagesshownin Figure1areamongtheimagesreturnedby a Bing image search with the keyword ‚Äúfire‚Äù.
Keeperalsomakespseudo-inversefunctionsaproxyofhuman
judgement and automatically judges the correctness of software
outputsthatarerelatedtocognitivetasks.Sinceourpseudo-inverse
functions are notanalytically inverting ML APIs (i.e., ùëì/prime(ùëì(ùëñ))‚â†ùëñ
is possible), a test input generated by Keeper may not cover thetargeted software branch, like the right image in Figure 1 failingtocoverthe
alarmbranch.Atthesametime,sincethesepseudo-
inverse functions have been approved by many human beings,
Keeperreportsan accuracyfailure whenoverathresholdportion
of inputs fail to cover a particular target branch.
Ofcourse,Keeperalsomonitorsgenericfailuresymptomslike
crashes during test runs, and helps expose bugs in code regions
that require specific ML inputs to exercise.
Finally, to help developers understand the root cause of an accu-
racyfailure,KeeperexploresalternativewaysofusingMLAPIsand
informsthedevelopersofanycodechangesthatcanalleviatethe
accuracy failure. For the example in Figure 1, Keeper would inform
developers that comparing the returned labels with not only ‚Äúfire‚Äù
butalso‚Äúflame‚Äùwouldmakethesoftwarebehaviormoreconsistent
with common human judgement.
Puttingthesealltogether,wehaveimplementedKeeperthatcan
beusedeitherthroughacommand-linescriptoraplug-ininside
the VScode IDE [ 46]. Given a software application, Keeper first
highlights all the functions that directly or indirectly call ML APIs.
Foranyfunctionthatdeveloperswanttotest,Keeperautomatically
generatesmanytestcasestothoroughlytesteverybranchinthe
specified function and its callees. Keeper analyzes the test runs
andreports anyfailures, aswellas potentialpatchesfor accuracy
failures, to developers.
We evaluate Keeper on the latest version of 63 open-source
Python applications that cover different problem domains and ML
APIs. Due to the relatively young age of ML APIs, these 63 applica-
tionsaremostlyresearch projects,hackathonproducts,anddemo
programs. Keeper achieves 91% branch coverage on average for
1ThecurrentimplementationofKeepersupportsGoogleCloudAIAPIsandcanbe
easily extended to support similar APIs from other service providers.
213
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automated Testing of Software that Uses Machine Learning APIs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
theseapplications.Intotal,Keepercovers21‚Äì38%morebranches
thanalternativetechniquesthatdirectlyusemachinelearningtrain-
ingdatasetorrandomfuzzing.Keeperexposes35uniqueaccuracy
and crash failures from 25 out of these 63 applications.
2 BACKGROUND AND DEFINITIONS
ThissectionprovidesabriefoverviewofMLAPIs,theirinputsand
outputs, and how they are typically used in software.
MLCloudAPIs. MLAPIsofferedbydifferentserviceproviders
all cover three main categories of machine learning tasks: visiontasks, language tasks, and speech tasks. Keeper handles all the
commonlyusedAPIsinthesethreefamilies,asshowninTable1.
KeepercurrentlydoesnothandleVideoIntelligenceAPIs(fromthe
vision family), Translation APIs (from the language family), and
SpeechSynthesisAPIs(fromthespeechfamily ),astheyareused
much less frequently in open-source applications [7].
Inadditiontoimage/text/audioinputs,someMLAPIsalsotakein
configuration parameters. For example, analyze_sentiment takes
in not only a text string, but also configurations like language, en-
coding, and input type, as shown in Figure 2. These configurations
are set to constant values, mostly the default values offered by
Google,inalloftheMLsoftwarewehavechecked.Therefore,in
this paper, Keeper focuses on generating image/text/audio inputs.
1document = { "content" : text_content, "type_" : Type.
PLAIN_TEXT, "language" :"en"}
2response = client.analyze_sentiment(request= { 'document' :
document, 'encoding_type' : EncodingType.UTF8}
Figure 2: An example of Google Cloud API with text input.
TheoutputofaMLAPImayincludemultiplerecords,likemulti-
ple classification results, multiple objects detected, and so on. Each
recordtypicallycontainsakeyresultfieldoftenofastringoranenum type, like the classification label of an image, the emotion
ofaface,andsoon,andaconfidencescorefield,whichindicates
how likely this result is correct. Unless otherwise specified, the
remaining paper refers to these key result fields as ML API output,
as summarized in Table 1. Note that, some of these APIs do output
other auxiliary information. For example the face detection APIalso outputs the bounding box of each face detected in the inputimage. These auxiliary result fields may be used to make control
flow decisions, although such usage has not been observed in anyof the 360 applications collected by a previous ML API study [7].
ML software. Sometimes, ML APIs are only loosely connected
with the remaining part of the software, with their output directly
printedoutwithoutfurtheruseinthesoftware.Testingthistype
ofsoftwaresimplyneedstotestMLAPIsandtheremainingpart
of the software separately and hence is not the target of Keeper.
In some other cases, ML APIs are more closely connected, with
their results used to impact the control flow of the software exe-
cution.Thesecasespresentnewchallengestosoftwaretestingas
discussed in Section 1 and hence is the focus of this paper.
3 TEST INPUT GENERATION
Keeperisatestingtoolforsoftwarewhosecontrolflowisinfluenced
by ML APIs. As shown in Figure 3, Keeper includes two major		
	

	
	

	

	


	







Figure 3: An overview of Keeper.
components:test-inputgeneration,whichwepresentinthissection,
and test-output processing, which we present in Section 4.
Keeper‚Äôs input generation is built upon an existing symbolic
execution engine, DSE [ 47]. Given a function ùêπto test2and all the
function parameters represented as symbolic variables, a symbolic
path constraint is generated for every branch; solving all the path
constraints produces a test suite that offers full branch coverage.
Inthis section,weexplain howKeeperhandles caseswhenML
APIsarepartofthepathconstraintsandgeneratesinputsforML
APIs, which are not handled by existing techniques.
A naive solution is to symbolically execute ML APIs‚Äô imple-
mentation. Unfortunately, this is too expensive to carry out for
state-of-the-artdeepneural networks(DNN).Notto mentionthat
theexactDNNsusedbyMLAPIprovidersareunknown.Forexam-
ple, a state of the art image classification network, EfficientNet-L2
[48], has480 million parameters.It takes in a224 x 224pixel image
and generates the output after about 50 billion floating point oper-
ations.Solvingapathconstraintthatinvolvesthisnetworkwith
more than 50,000 (224 x 224) symbolic variables would take days.
Keeper decomposes the problem of generating inputs for ML
APIs into two parts: first, it identifies the ML-API outputs that are
neededtosatisfypathconstraintsusingsymbolicexecution(Section
3.1);andthensynthesizestheML-APIinputsthatareexpectedto
produce those outputs using carefully designed pseudo-inverse
functions (Section 3.2). As we will see, this decomposition not only
avoidsthecomplexityofdirectlyapplyingsymbolicexecutionto
DNNs, but also help judge the execution correctness (Section 4).
3.1 Identifying relevant ML outputs
To identify the desired ML-API outputs, Keeper makes its symbolic
execution skip any statement that calls an ML API and instead
marks API output that is used by following code as symbolic. This
way, the output, instead of input, of ML APIs will be part of the
path constraints, and by solving the constraints, Keeper obtainstheAPI-outputvaluesthatareneededtoexercisecorresponding
branches.
The only tweak Keeper makes here is to have the symbolic exe-
cutionenginesometimesgeneratingonepathconstraintforeach
branch sub-condition, instead of the whole branch. Specifically,a common code pattern that we have observed is to decide the
2Users of Keeper can choose any function to test, including the mainfunction.
214
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA C. Wan, S. Liu, S. Xie, Y. Liu, H. Hoffmann, M. Maire, and S. Lu
ML Task Main Output Constraint Example Pseudo-inverse Function
VisionImage classification image class class=="fire" [8] Search on internet, keyword: [image class]
Object detection object name object=="tableware" [49] Search on internet, keyword: [object name]
Face detection face emotion emotion =="joy" [50] Search on internet, keyword: [emotion] + "human face"
Text detection extracted text text=="3923-6625" [51] Print [extracted text] on an image
LanguageDocument classification document class class=="food" [52] Search on internet, keyword: [document class]
Sentiment detection score, magnitude score<0 [53] Select tweets from Sentiment140 dataset [54]
Entity detection entity name, type type=="Person" [55] Use text generation technique, seed: [name] or [type]
Speech Speec hrecognition transcript text=="turn on the light" [56] Usespeec h synthesize technique on [transcript]
Table 1: Different ML APIs handled by Keeper and their pseudo-inverse functions.
1defsmart_can(img):
2labels = client.label_detection(image=img)
3classes = [x.desc forxinlabels]
4forcinclasses:
5ifc= ="food":
6 return "organic"
7ifc= ="paper" orc= ="aluminum" :
8 return "recyclable"
9return "non-recyclable
Figure 4: A smart can application, Heap-Sort-Cypher [57]
execution path based on whether or not an ML API outputs a la-
bel that belongs to a pre-defined set. For example, the smart-canapplication in Figure 4 executes the
recyclable path when the
output of label_detection contains a label that is either paper
oraluminum .Sincedifferentlabelsoftenrepresentdifferenttypes
ofreal-worldinputs,Keeperwillgenerateonepathconstraintfor
every condition clause, instead of one for the whole branch. For
example,fortheline-7branchinFigure4,Keepergeneratestwocon-
straints(‚Äúpaper‚Äù ‚ààclasses)and(‚Äúaluminum‚Äù ‚ààclasses),which
then prompts Keeper to generate two separate sets of images to
satisfy these two constraints.
In our implementation, this is accomplished by enabling a corre-
sponding feature of the underlying symbolic execution engine. For
example, for a branch condition ‚ÄúA or B or C‚Äù, four constraints will
be formed representing (1) A is True, (2) B is True, (3) C is True,
and(4)noneofA,B,CisTrue.Solvingtheseconstraintsleadsto
four inputs or input sets that satisfy these constraints separately.
3.2 Identifying ML API inputs
Given an ML API ùëìand an output ùëú, Keeper aims to automatically
generateasetofinputs ùêºsothatùëì(ùëñ),ùëñ‚ààùêºisexpectedtoproduce
ùëúaccordingtocommonhumanjudgement.Forexample,thetwo
images in Figure 1 are expected to make label_detection output
‚Äúfire‚ÄùandtheimagesineverycolumnofFigure5areexpectedto
makelabel_detection output the corresponding column-header.
To achieve this, Keeper designs a pseudo-inverse function ùëì/primefor
every API ùëì, so thatùëì/prime(ùëú)will produce the input set ùêºforùëì.W e
wantùëì/primeto have the following properties.
First,ùëì/primeis not an analytical inversion of ùëì. Ideally,ùëì/primeshould be
built independently from ùëì(e.g., not based on the same training
data set), so that ùëì/primecan help not only input generation but also
failure identification in a way similar to N-version programming.
Second,ùëì/primeshould be a semantic inverse of ùëì, reversing the cog-
nitive task performed by ùëìin a way that is consistent with most  	

 
Figure 5: Keeper-generated test cases for Figure 4
humanbeings.Thisway,testinputsgeneratedbyKeepercanex-
pect to cover most of the software branches, unless the ML API is
unsuitable for the software or is used incorrectly.
Third,ùëì/primeshould producemore thanone outputfor each inputit
takesin.ThiswillallowKeepertogeneratemultipleinputsfor ùëìto
exercise a corresponding branch, and get a statistically meaningful
test result given the probabilistic nature of ML APIs.
With these goals in mind, we have designed three types of
pseudo-inverse functions as summarized in Table 1.
3.2.1 Search-based pseudo inversion. For many vision and lan-
guageAPIs,searchenginesoffereffectivepseudoinversion:they
take in a keyword and return a set of realistic images/texts thatreflect the keyword. Search engines have several properties that
serve Keeper‚Äôs testing purposes. First, they offer great semantic in-
version,astherearemultiplesearchenginesthathavebeenusedby
hundredsofmillionsofusersformanyyearswithhighsatisfaction
[58]. Theirtop search resultstypically match the commonhuman
judgement.Second,theyarenotananalyticalinversionofMLAPIs,
and we will use non-Google engines to minimize potential correla-
tions.Third,theyacceptawiderangeofsearchwordsandproduce
many ranked results, which means a large number of high-quality
testinputsforKeeper.Specifically,Keeperusesdifferentengines
and search keywords for different ML APIs:
Vision tasks. Image-classification and object-detection APIs
returnstringlabelsthatdescribetheimageandtheobjectsinsidetheimage,respectively.ForbothAPIs,KeeperusestheBing[
59]image
searchengineandusesthedesiredlabeldescriptionorobjectname
as the search keyword. For example, the images in each columnof Figure 5 were the top-3 search results returned by Bing using
215
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automated Testing of Software that Uses Machine Learning APIs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
the keywords listed atop. The only exception is the last column:
when there is no specific keyword requirement (like c != food
and c!= paper and c != aluminum ), Keeper uses a blank image
and images generated by a random-image generator [60].
Theface-detectionAPIdetectshumanfacesinan image.Some
MLsoftwareusesthereturnedemotionstringassociatedwitheach
face(e.g.,‚Äújoy‚Äù,‚Äúsorrow‚Äù,etc.)todecideexecutionpath.Togenerate
corresponding images, Keeper uses ‚Äú[emotion] human face‚Äù as a
keyword to search the Bing image.
Languagetasks. Document-classificationAPIsprocessadoc-
ument andreturn categoriesbased onthe documentcontent, like
‚Äúpets‚Äù, ‚Äúhealth‚Äù, ‚Äúsports‚Äù, and others. Keeper uses the desired cat-
egory name as keyword and searches it at (1) knowledge graph
websites, Wikipedia [ 61] and Britannica [ 62]; and (2) Bing web
searchengines.Keeperthenusesthetextextractedoutfromeach
returned web page as the ML API input.
3.2.2 Synthesis-based pseudo inversion. The semantic inversion of
some ML APIsdoesnot match the functionality ofsearch engines.
Fortunately, we find ways to synthesize inputs for them.
Thetext-detection API extracts printed or handwritten text
from an image. Unfortunately, image search engines tend to return
imageswhosecontentreflectsthesearchkeyword,insteadofim-
ages that contain the keyword as text within the image. Therefore,
givenatextstring,Keeperprintsitonabackgroundimageusing
the Python pillow library [ 63]. Keeper adopts both printed and
hand-writing fonts; different font settings produce different test
images.Todecidethebackgroundimage,Keepercheckswhether
thetext-detection API shares its input image with another vi-
sion API. If so, the test images Keeper generated for the other API
willbeusedasthebackground;otherwise,ablankimageandsome
randomimageswillbeused.Figure6showssomeofthetestimages
that Keeper generates for application wanderStub [ 64], which has
a branch checking if the input image contains "Total".
Figure 6: Test inputs generated for wanderStub [64].
Theentity-detection APIinspectstheinputsentenceforknown
entities‚Äîthereareintotal13entities,suchasADDRESS,DATE,etc.
Since the search engines usually return long documents, Keeperinstead uses a popular language model GPT-2 [
65] to synthesize
anynumberofsentencesthatstartwithapre-definedword/phrase
that corresponds to the desired entity type.
Thespeech-recognition APItranscribestheinputaudioclip
and outputs the transcript. Keeper uses speech synthesis tools,
particularlythepyttsx3[ 66]Pythonlibrary,togeneratethedesired
audioclipsbasedona giventranscript.Keepergeneratesmultipleaudio clips using different voice settings supported by this library.3.2.3 ML benchmarks for pseudo inversion. Thesentiment detec-
tionAPIpresentstwo challenges.First,althoughthisAPI aimsto
identifytheprevailingemotionalopinionwithinthetext,itdoesnot
directly output a categorical result. Instead, it returns two floating-
point numbers, scoreandmagnitude , for developers to derive
emotioncategoriesfrom.Thereisnoperceivablewaytogenerate
textthatcanoffertheexact scoreormagnitude .Second,evenif
we just hope to generate text that contains positive or negative
emotion, no search engine or synthesizer can accomplish this.
Facing these challenges, Keeper resorts to the Sentiment140
dataset [54], which contains 1,600,000 tweets, manually labelled as
positive, negative, and neutral. Keeper randomly samples the same
numberofpositive,negative,andneutraltweetsas test inputsfor
any sentiment-detection API called inside an ML software, with
theexpectationthatthesetweetswillhelpcoverdifferentbranches
in the software that are designed for different emotions.
Notethat,wetreatMLbenchmarksasthelastresortformultiple
reasons.First,thelabelsassociatedwithdatainsideMLbenchmarks
either have few categories or have limited quality. For example,ImageNet [
67] contains 1000 manually labeled image categories,
which is too few compared with the 20,000 labels of Google Vision
AI.On thecontrary, OpenImagehas 9millionimages with20,000
labels.Howeve r89%ofthelabelsaregeneratedbyDNNs,and53%of
thehuman-verifiedonesareincorrect[ 68].Second,MLbenchmarks
are built with pre-processed real-world data. Such "clean" data has
less variety, as they share similar size, resolution, and encoding
format.Third,somebenchmarksmaybepartofthetrainingdata
setofGoogleMLAPIs,whichmakesthetestinputsbiasedtowards
theonesAPIscanperformwellonandhencelesslikelytoreveal
problems.Finally,GenerativeAdversarialNetworksynthesizesnew
data following the distribution of the training set [ 69]. It covers
different domains, including generating images from text [ 70]. We
donotuseit,asthisapproachrequiresmuchtrainingdataandends
up generating non-real-world data that has similar distribution
with the training set, whose limitations we discussed earlier.
3.3 Putting everything together
Overall, Keeper generates test inputs for any function ùêπin the
followingsteps.First,itssymbolicexecution(Section3.1)generates
a set of inputs Ithat offer full branch coverage unless some path
constraintsareun-satisfiable.Ifnobranchin ùêπoritscalleesdepends
ontheoutputofanMLAPI,theinputgenerationisdone.Otherwise,
if there is such an ML-dependent branch ùëè, those inputs that are
expectedtocover ùëè,denotedas Iùëè‚äÇI,containfieldsthatrepresent
the desired outputs of ML APIs and require further processing.
Next, foreach desired output ùëúof anML API ùëì, Keeperapplies
ùëì‚Äôs pseudo-inverse function ùëì/primeonùëúto generate a set of image/tex-
t/audio inputs for ùëì(Section 3.2). If ùëì‚Äôs input is exactly an input
of the function under test ùêπ(i.e., it is not derived from an input of
ùêπthrough pre-processing), the input generation is done. Keeper
updateseveryinputin Iùëèwiththeimage/text/audioinformation.If
there were ùëòinputs in Iùëè, Keeper now gets ùëò√óùëÅùëèinputs, with ùëÅùëè
being the number of image/text/audio inputs Keeper generated for
the ML API ùëìto exercise ùëè. Developers can configure ùëÅùëè, or the to-
talnumberoftestinputstogenerate.Keeperwillthencompute ùëÅùëè,
216
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA C. Wan, S. Liu, S. Xie, Y. Liu, H. Hoffmann, M. Maire, and S. Lu
so that every ML-dependent branch (sub-)condition gets exercised
by about the same number of inputs.
Ifùëì‚Äôsinputisderivedfromaninputoffunction ùêπthroughpre-
processing,Keeperrunssymbolicexecutiononthatpre-processing
code to figure out the desired input of ùêπand finishes the input
generation. For example, if a function deletes the first character of
astringparameterandfeedstheresultingstringtoanMLAPI ùëì,
Keeper will add a character to the beginning of every input gener-
atedforùëìtogetthestringparameterofthisfunction.Thesymbolic
execution engine used by Keeper can handle pre-processing re-
latedto text(i.e.strings),but notthoserelated toimagesoraudio,
such as image/audio clipping. Future work can extend Keeper with
common image/audio transformation routines.
Finally, these test inputs generated by Keeper are ready to be
executed. Particularly, in order for a software to consume a test
imageoraudiofile ùëìùëñùëôùëígeneratedbyKeeper,Keeperchangesthe
file path embedded in the software to a path that points to ùëìùëñùëôùëí.
4 TEST OUTPUT PROCESSING
Once all the test inputs are generated and executed, Keeper works
on failure identification and attribution.
4.1 Failure identification
Keeper looks for three types of failure symptoms: (1) low accuracy,
(2) dead code, and (3) generic failures like crashes.
4.1.1 Low-accuracyfailures. Whensoftwareincorporatescogni-
tiveMLAPIsinitscomputation,judgingtheoutput‚Äôscorrectness
becomeschallenging:(1)bydefinitionofcognitivetasks,thisoutput
needstobecheckedwithmanypeopletoseeifitmatcheswithcom-
monhumanjudgement;(2)duetotheprobabilisticnatureofML
APIs, an occasional mismatch is expected. Of course, frequent mis-
matchesare un-acceptableandseverely hurt userexperience,like
nottriggeringfirealarmswhenneeded(Figure1)orconsistently
categorizing garbage incorrectly (Figure 4).
Totacklethefirstchallenge,Keeperusespseudo-inversefunc-
tions as an approximation of common human judgement; to tackle
the second challenge, Keeper considers the software to suffer from
a low-accuracy failure, or an accuracy failure for short, only when
overathresholdportionofinputsofaparticulartypehaveproduced
outputs that are inconsistent with common human judgement.
Specifically, for all the inputs Iùëèthat are generated to cover a
branchùëè, Keeper checks which of them exercise ùëèat run time,
denoted as Isucc
ùëèand calculates the recallofùëè(i.e.,|Isucc
ùëè|
|Iùëè|). If the
recall drops below a threshold ùõº, 75% by default. Keeper reports an
accuracyfailureassociatedwith ùëè.Thesettingof ùõºcanbeadjusted,
butshouldnotbe100%,asMLAPIsareprobabilisticandpseudo-
inverse functions cannot guarantee to be correct all the time.
Forthefire-alarmexampleinFigure1,Keeperidentifiesanac-
curacy failure associated with the ‚Äúfire‚Äùbranch, as its recall is
41%; for the smart-can example in Figure 4, Keeper identifies an
accuracy failure as the recall of the recyclable branch is only 13%.
Forabranch ùëèthatdependsontheoutputofasentiment-detection
API, Keeper identifies failures slightly differently as inputs are
generatedforsentiment-detectionAPIdifferentlyasdiscussedin
Section 3.2.3. During test runs, Keeper checks all the inputs that1labels = client.label_detection(image=img)
2temp = label[0].desc + label[1].desc + label[2].desc
3if"fire" intempor"flame" intempor"ash"intemp:
4alarm()
Figure 7: A fixed version of Figure 1 suggested by Keeper.
exerciseùëètoseewhatportionofthemarelabeledashavingpositive
emotionandwhatportionarelabeledasnegative.Ifbothgoabovea
threshold, indicating that branch ùëèis not accurately differentiating
inputs with different emotions, Keeper reports an accuracy failure.
Rootcausesofaccuracyfailures. Notethat,theseaccuracy
failures are notequivalent with low precision or low recall of the
MLAPIitself.Thelatterisjustoneofthepossiblerootcausesof
the former. Keeper intentionally does not calculate the precision or
recall of any ML API, but instead focuses on the overall software.
Onepossiblecauseisthatdevelopersmissedsomerelatedlabels
in a branch condition, which we refer to as an incomplete label
problem. Forexample, the label_detection API doesnot return
‚Äúfire‚Äùasatop-3labelformanytopfireimagesreturnedbytheBing
image search. This by itself is notconsidered a failure by Keeper. If
thesoftwareusestheAPIproperly,likeraisingafirealarmupon
not only a ‚Äúfire‚Äù label but also a ‚Äúflame‚Äù label and an ‚Äúash‚Äù labelas shown in Figure 7, no accuracy failure would be reported, asthe recall of the alarm-related branch is as high as 85% and the
precision is 100% in our experiments.
Another possible cause is that developers used a non-existing
label,whichdoesnotexistintheAPI‚Äôslabelsetandcanneverbe
theoutput.Thisisnotasurpriseasthelabelsthatcanbeoutputby
GoogleVisionAPIaretoomany(19,985)fordeveloperstomemo-
rize. For example, an applicationcompares the label_detection
output with ‚Äúclothes‚Äù and ‚Äúpants‚Äù [ 57], which are non-existing
labels. Instead, ‚Äúclothing‚Äù and ‚Äútrousers‚Äù are valid labels.
4.1.2 Dead-codefailures. Theseoccurwhenabranchisnotcov-
ered after all the testing runs. They happen under two scenarios.
OnescenarioisthatKeepergeneratesasetoftestinputs Iùëèex-
pectedtocoverabranch ùëè,andyetùëèisnotexercisedbyanyinputin
Iùëè.Suchanextremecaseoflowbranchrecall(i.e.,0)isoftencaused
by the branch comparing a ML API output with a non-existing
label.If thiscomparisonisone ofmultiplebranch sub-conditions,
an accuracy failure would likely occurr (i.e., a low but non-zero
recall); if itis the only conditionclause, a deadcode failure occurs.
Forexample,asmartphotoapplicationFESMKMITL[ 71]checks
the output of label_detection against the string ‚Äúface". Unfortu-
nately, among the 20,000 category labels that could be output by
this API, none of them is ‚Äúface‚Äù. Instead, ‚Äúhuman face‚Äù is one of the
valid labels for this API, which the developers should have used.
The other scenario is that Keeper fails to generate any inputs
to cover a branch, which triggers a dead-code failure report before
any test runs. Sometimes, this is caused by a typo in the branch
condition.Forexample,KeeperexposessuchafailureinVerlan[ 72].
Verlanuses object-detection tojudgewhetheranimagecontains
an animal or not. Unfortunately, it wrongly uses "animal" instead
ofobj.name == ‚Äúanimal‚Äù in its branch condition, making the
if-statementalways True.Itwillregard everyimagethatcontains
at least one object as an animal image!
217
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automated Testing of Software that Uses Machine Learning APIs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
1object = client.object_detection(image=img)
2forobjinobjects:
3ifobj.name== "dog"or"animal" :
4do_A()
Figure 8: Dead-code bugs in Verlan [72]
4.1.3 Generic failures. These have symptoms like crashes that do
not require special techniques to observe. Comparing with tradi-
tional testing techniques, Keeper offers extra benefit in two sce-narios.(1)ThefailuresarecausedbybugslocatedonapaththatrequiresspecificMLAPIinputstotrigger.Keepercontributesby
generating the needed ML API inputs to exercise the path. (2) The
failures are directly relatedto the corner casesof ML API inputs,such as blank images that cause
label_detection to return no
labels.AnexampleofsuchabugexposedbyKeeperisillustrated
in Figure 9.
1text = client.text_detection(image=img)
2labels = text[0].descr iption.split( '\n')
3forlabelinlabels:
4do_something()
Figure 9: Crash failure in FortniteKillfeed [ 73]:a blank image
returns an empty array textand trigger an index-out-of-range.
4.2 Failure attribution
Tohelpdevelopersunderstandandtackleaccuracyfailures,Keeper
attempts to automatically patch the software by changing how ML
APIs‚Äôoutputisused.Keepersuggeststhechangetodevelopersand
ifallattemptsfailed,Keepersuggestsdeveloperstoconsiderusingadifferent,moreaccurateMLAPI,oraddingextrainputscreeningor
pre-processing. Specifically, Keeper attempts two types of changes
to the branch ùëèwhere the failure is associated with.
Labelchanges. Whenbranch ùëècomparesaMLAPIoutputwith
a set of labels, Keeper tries to expand the set of labels with three
goals in mind. (1) Recall goal: more test inputs that are expected to
exerciseùëècan now satisfy ùëè‚Äôs condition; (2) Precision goal: most
inputs that are not expected to exercise ùëèshould continue to fail
theconditionof ùëè;(3)Semanticgoal:theaddedlabelsarerelatedto
the original label(s) in ùëèin terms of natural language semantics.
Withoutlossofgenerality,imaginethat ùëètakestheformof if
o == label0 ,withobeingtheoutputofanMLAPI ùëì.Keeperfirst
collectsthesetoflabels ùêøoutputby ùëìforeveryinputin Ifail
ùëè,the
set of inputs that are expected to exercise ùëèbut fail to do so.
Then, considering the semantic goal, Keeper filters out every
labelinùêøthatisneitheradjacenttonorsharingacommonneighbor
withlabel0in the wikidata knowledge graph [ 74]. For example,
‚Äúamber‚ÄùisprunedoutbyKeeperwhileprocessingtheaccuracyfail-
ure in Figure 1, because it is far away from ‚Äúfire‚Äù in the knowledge
graph. Instead, ‚Äúflame‚Äù and ‚Äúash‚Äù both remain, as they are both
adjacent to ‚Äúfire‚Äù on the graph.
Next,Keeperusesagreedyalgorithm to iterativelyexpandthe
set of labels compared with oinùëè. Every time, Keeper adds to
the set a label l‚ààùêøso thatloffers the biggest improvement in
ùëè‚Äôs recall without reducing ùëè‚Äôs F1-score (i.e., the harmonic mean
oftheprecisionandtherecall).Here,theprecisionofbranch ùëèiscomputedas|Iùë†ùë¢ùëêùëê
ùëè|
|Iùë†ùë¢ùëê|:amongalltheinputsthatexercise ùëè,howmany
ofthemareexpectedtodoso.Thisprocedurecontinuesuntilthe
recall ofùëègoes above the accuracy failure threshold or when there
is no eligible candidate label remaining in ùêø.
Exactly through this process, Keeper suggests to the developers
that the alarm branch in Figure 1 should check more labels like
that in Figure 7, as by checking more labels the branch‚Äôs recall can
increase from 40% to 85% on those test cases generated by Keeper.
This suggestion is proposed through a text description instead of a
code patch‚Äî"If you additionally check flame and ash in the branch
condition on Line 3, your program will agree with most human
beings‚Äôjudgementfor85%oftestinputs,animprovementfrom40%
of your original code".
Threshold changes. As discussed earlier, an accuracy failure is
reportedwhenabranch ùëè,whichchecksthe scoreand/ormagnitude
outputofasentiment-detectionAPI,getsexercisedbymanyinputs
labeled ashaving positive emotions and alsomany inputs labeled
as having negative emotions. Keeper applies logistic regression to
theseinputtexts,withthe{ score,magnitude }outputofeachinput
asfeaturevectorsandthelabeledemotionasaclass.Keeperthen
suggests the linear formula of logistic regression as a new branch
checking threshold to developers, letting them know that this new
formula can better differentiate text inputs with different emotions.
5 IMPLEMENTATION
We have implemented Keeper for Python applications that use
GoogleCloudAIAPIs[ 1],themostpopularcloudAIserviceson
Github [7]. The core algorithm of Keeper is general to other lan-
guages and ML Cloud APIs. Keeper uses dynamic symbolic execu-
tionframeworkPyExZ3[ 47],whichimplementstheDSEalgorithm,
andusesCVC4[ 75]forconstraintsolving.KeeperusesPythonbuilt-
intracebacktool[ 76]tocheckbranchcoverage,andPyan[ 77]and
Jedi[78]forcallgraphandprogramdependencyanalysis.Keeper
uses Python scikit-learn[79] library for linear regression models.
Figure 10: Keeper IDE plugin interface
WehaveimplementedanIDEpluginforvisualizedinteraction
withKeeper,asthedebuggingandfixingofaccuracyfailurespar-
ticularly requires developers‚Äô participation, as illustrated in Figure
10. The plugin is an extension in Visual Studio Code [ 46], a pop-
ular code editor supporting multiple languages. For any Python
software,Keeper firstidentifiesallfunctions thatinvokeML APIs
218
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA C. Wan, S. Liu, S. Xie, Y. Liu, H. Hoffmann, M. Maire, and S. Lu
Failure type Root Cause Related ML Task Keeper RReal RReal+Noise Fuzz.
Crash failuresOut-of-bound accesses Text detection, entity detection 6 5 5 4
Missing input validation* Document classification 1 - - -
Missing type conversion - 1 1 1 1
Accuracy failuresImproper labels Image classi., object detect., document classi. 9 - - -
API limitations Image classification, object detection 6 - - -
Improper threshold Sentiment detection 9 - - -
Dead-code failuresTypos Image classification, text detection 2 - - -
Non-existing label Image classification 1 - - -
Table 2: Unique failures exposed by Keeper. (*: This crash disappeared later with the most recent version of Google API.)
directlyorindirectlythroughcallees,anddisplaysthemontheside
bar,under‚ÄúRELEVANTFILESANDCODES‚ÄùinFigure10.Fromthatlist,developerscanselectthefunctiontotest.Oncetheyhavemadetheselection,theywillbeaskedtoprovidetypeinformationoffunc-tionparameters,asPythonisadynamicallytypedlanguage.Keeper
will then start the testing. At the end of the testing, which usually
takes1‚Äì2minutes,anyexecutionfailurethathasbeenexposedis
listed in the side bar, right under ‚ÄúFUNCTIONS WITH FAILURES‚Äù
in the figure. Source code related to each failure is highlighted,
togetherwithahoveringwindowthatoffersdetailedinformation
like failure description, triggering inputs, and patch suggestions. A
demo of the Keeper plugin can be found at our artifact [80].
6 EVALUATION
Our evaluation aims to answer several questions:
(1) Does Keeper help improve the branch coverage in testing?(2) Is Keeper able to find bugs during its testing?(3) Is Keeper able to suggest fixes for accuracy failures?
6.1 Methodology
6.1.1 Applications. We evaluate Keeper using 63 Python appli-
cations that are from two sources. 1) From the 360 open-sourceapplications assembled by a previous study of ML APIs [
7], we
found45PythonapplicationsthatuseMLAPIsinanon-trivialway
(i.e.,theAPIoutputaffectscontrolflow).2)Weadditionallychecked
about100randomPythonapplicationsonGitHubthatuseMLAPIs
and found 18 applications that use ML APIs in a non-trivial way.
These 63applications usea rangeof MLAPIs, includingVision
(32 apps), Language (23 apps), and Speech (8). Their sizes range
from54linesofcodetomorethan100,000linesofcode,with582
lines of code being the median3. They have a median age of 18
months at the time of our study (Apr. 1st, 2021). Among these 63
applications,16applicationshavereceived1ormultiplestarson
Github; the other 47 applications have not received any stars. The
details of each application, including the link to each Github code
repository, are included in Table 4.
Despiteourbesteffortinapplicationcollection,unfortunately,
mostofthese63applicationsseemtoberesearchprojects,hackathon
products,ordemoprograms,basedontheirlimitedpopularityin
Github. This is probably due to the young age of ML APIs. Con-sequently, our evaluation results may not generalize to mature
software that has a solid user base.
3Filesfromtemplates, frameworks,and librariesarenotincluded intheLoCcounting.Vision App. Language App. SpeechApp.
Keeper 91.9% 91.5% 89.7%
Random-Real 74.5% 85.0% 54.3%
Random-Real-Noise 73.0% 65.2% 54.3%
Fuzzing 44.4% 74.0% 24.9%
Table 3: Average branch coverage across 63 applications.
For more than half of the applications (35), we simply specify
mainas the function to test. In other cases, the function under test
is the entry function to the software feature related to ML APIs.
The average number of branches in these functions-to-test is 13.
6.1.2 Baselines. WecompareKeeperwith3othertechniques.Each
technique generates 100 test inputs for each function under test.
(1)RandomReal :werandomlypickinputsfromwellestablished
data sets, including ImageNet [ 67] that contains 14 million images,
Twitter US Airline Sentiment [ 81] that contains 15,000 tweets, and
a set of audio clips synthesized for 115 daily sentences [82].
(2)Random Real + Noise : we add random noise to inputs picked
byRandomReal.For animage,werandomlyadded noisesfollow-
ing Gaussian distribution; for an text input, we randomly decide
whether to add noise and if so, randomly changed the word orders.
For audio input, we do not add noise here, as we found that addingsmallnoisesdoesnotaffectMLAPIandyetaddingbignoiseswould
turn the audio clip into what the third approach will generate.
(3)Fuzzing: we use a coverage-based fuzzing tool pythonfuzz
[83] to generate images, text, and audio. For every image input, we
useanintegerlisttofillitsRGBmatrixinarepeatedway.Forevery
text inputs, we generates ASCII character sequences. For audio
inputs, we directly generates the audio data.
6.2 Software testing evaluation
6.2.1 Branchcoverage. Foreachofthe63functionsspecifiedtotest,
eachfromoneapplicationinourbenchmarksuite,wecomputetheaccumulativebranchcoverageachievedbythe100inputsgenerated
by each testing technique. Table 3 shows the overall results.
Acrossdifferenttypesofapplications,Keeperconsistentlyachieves
high branch coverage, around 90% on average. The uncoveredbranches are either related to dead-code failures that Keeper dis-
covers,orrelatedtocodethatourunderlyingsymbolicexecution
engine cannot handle. In comparison, the fuzzing technique per-
formedtheworst,coveringlessthan50%ofthebranchesforvision
andspeech applications, confirmingour intuition thatit is impor-
tant to use realistic inputs to test ML APIs.
219
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automated Testing of Software that Uses Machine Learning APIs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Application w/ link Description Stars LOCBranch Coverage Keeper Exposed Failures
#Branches Keeper RReal Rreal+Noise Fuzz Accuracy Crash Dead-code
selfmailbot Telegram bot 114 911 1090% 70% 10% 10%
FortniteKillfeed Game assistant 5 1440 28100% 0% 0% 0% 1
FB_MMHM Meme inspector 3 2850 3494% 88% 91% 53%
Audio-SentenceSplit Audio splitter 3 304 4100% 50% 50% 50%
calbot Nutrition tracker 2 653 8100% 100% 100% 25%
Hapi Produce analyzer 2 261 12100% 100% 92% 92%
stockmine Investment helper 2 1079 1090% 80% 70% 50% 1
Tone Smart music player 2 12709 4100% 100% 25% 25% 1
UOttaHack-2019 Speechemotion detector 2 107 6100% 100% 83% 83% 1
BlindHandAssistance Blind assistant 2 708 2227% 18% 18% 14%
IngredientPrediction Recipe recommender 1 142 12100% 58% 58% 58% 3
recipeGO Recipe recommender 1 22457 4100% 100% 100% 25%
devfest Public opinion analyzer 1 206 10100% 80% 50% 80% 1
Klassroom Note taker 1 17250 14100% 100% 100% 86%
uofthacks6 News summerizer 1 495 4896% 79% 79% 75% 1
HackThe6ix Insurance manager 1 65944 4687% 72% 70% 63%
Average branch coverage / Total failures exposed by Keeper 93% 75% 62% 49% 72 0
Aander-ETL Smart album 0 471 1681% 75% 81% 63% 3
Alpr License recognization 0 89 4100% 75% 75% 50%
artificial_intelligence Calorie calculator 0 401 2681% 35% 35% 4%
emotion2music Smart music player 0 777 10100% 70% 70% 10%
Experiments Product info analyzer 0 2500 18100% 100% 100% 6%
FESMKMITL Emotion tagger 0 1024 863% 63% 63% 63% 1
heapsortcypher Garbage classifier 0 85 12100% 83% 92% 75% 3
Image-analyzer-chat-bot Chat bot 0 163 12100% 100% 92% 33%
ns_online_toolkit Game assistant 0 9907 8100% 88% 88% 25%
Phoenix Fire alarm 0 284 6100% 83% 83% 83% 1
ResearchSpring2019 Prescription reader 0 131065 6235% 0% 0% 0% 1
SeeFarBeyond Coin finder 0 280 5470% 39% 39% 39% 22
smart_can Garbage classifier 0 1750 14100% 100% 100% 79%
SnapCal Smart calendar 0 233 475% 75% 75% 50%
twimage-search Landmark recognizer 0 107 26100% 100% 88% 46%
WanderStub Exchange convertor 0 54 2100% 0% 0% 0% 1
Verlan Animal finder 0 73 475% 75% 75% 25% 1
thgml Calorie calculator 0 76476 8100% 38% 38% 13%
SBHacks2021 Smart camera 0 234 10100% 100% 100% 70%
flood_depths Flood monitor 0 203 4100% 100% 100% 50%
image_tagging Fruit checker 0 749 4100% 100% 100% 100%
shecodes-hack Clothes checker 0 7052 4100% 75% 100% 75%
SunHacks2019 Blind assistant 0 40071 14100% 100% 79% 7% 1
SnapTrack_HACK112 Nutrition tracker 0 1096 4100% 100% 100% 100% 1
lahacks-quaranteen Image checker 0 4080 2100% 100% 50% 50%
plant-watcher Plant manager 0 183 875% 75% 75% 75% 1
senior-project Smart album 0 582 1090% 90% 90% 70%
animal_analysis Image sharing platform 0 355 1090% 80% 60% 50%
calhacksv2 Movie review analyzer 0 17105 10100% 100% 90% 80%
carbon-hack-sentiment Public opinion analyzer 0 222 8100% 100% 88% 88% 1
Cloud-Computing Food delivery 0 26914 4100% 100% 25% 100% 1
EC601_twitter_keyword Investment helper 0 2563 6100% 100% 83% 83% 1
ElectionSentimentAnalysis Tweet analyzer 0 1801 8100% 100% 88% 100%
GeoScholar Scholar database 0 268 4100% 100% 100% 100% 1
JournalBot Journel manager 0 295 6100% 100% 83% 100% 1
noteScript Note taker 0 886 1688% 44% 44% 44% 11
Sarcatchtic-MakeSPP19 Text tone checker 0 259 8100% 100% 100% 88% 1
Twitter_Mining_GAE Disaster news analyzer 0 822 667% 67% 67% 67%
BadGIF Discord bot 0 19747 4100% 50% 50% 75%
Mind_Reading_Journal Journel manager 0 558 6100% 100% 67% 83%
newsChronicle Timeline generator 0 346 4100% 100% 100% 100%
ocr-contratos Contract analyzer 0 22931 683% 83% 67% 67%
most_anoying_app_ever Smart music player 0 176 8100% 63% 63% 38%
PottyPot Swear remover 0 135 14100% 71% 71% 14%
ReadingMachine Book reader 0 209 4100% 75% 75% 25%
SwearRemoval Swear remover 0 203 10100% 30% 30% 20%
TRANSLATOR Consecutive interpreter 0 10700 14100% 57% 57% 29%
Average branch coverage / Total failures exposed by Keeper 93% 78% 72% 55% 17 6 3
Table4:Informationandresultsof63applications. (EachapplicationnamecontainsahyperlinktoitsGitHubrepository;theLOC
numbersreferonlytotheactualapplicationcode,notlibraries,templates,orotherfilesintherepository;#Branches:thenumberofbranches
in the function under test; 0% coverage: all test cases crash the program execution before reaching any branches.)
220
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA C. Wan, S. Liu, S. Xie, Y. Liu, H. Hoffmann, M. Maire, and S. Lu
RandomRealperformsbetterthanfuzzing,butstillfailstocover
aboutaquarterofbranchesinvisionapplicationsandhalfofthe
branches in speech applications. Adding random noises to random
realistic inputs does not help. Keeper covers 23% and 59% more
branches than Random-Real for vision and speech applications,
respectively,asKeeperleveragessymbolicexecutionandpseudo-
inverse functions to generate inputs targeting different branches.
Applications that use language APIs appear to be the easiest
to cover‚Äîeven fuzzing achieves 74% coverage. This is probably
because language APIs‚Äô output, like document type or entity name,
has much less variation than that of vision and speechAPIs.
Table4showstheexactbranchcoverageofferedbyeachtech-
niqueforeachbenchmarkapplication.Aswecansee,Keeperoffers
the highest branch coverage for all 63 applications.
6.2.2 Failureexposingandattribution. AsshowninTable2,Keeper
exposedmanyfailuresbyrunningthose100testinputsitgenerated:35 failures from the latest version of 25 applications. These failures
cover a range of symptoms and root causes. Except for one failure
caused by missing type conversion, the others are all related to
different types of cognitive ML tasks, as shown in the table.
In comparison, alternative testing techniques missed 2‚Äì3 crash
failurescaughtbyKeeper.Furthermore,unlikeKeeper,theycannot
automatically recognize accuracy failures and dead-code failures.
Accuracy failures. Among the 24 accuracy failures exposed
by Keeper, 15 of them are related to label checking for vision APIs
and document-classification API, and 9 are related to threshold
checking for the sentiment detection API.
For all of the 9 failures related to sentiment detection, Keeper
manages to suggest better checking threshold that fixes the failure.
There are 9 accuracy failures that Keeper manages to fix by
making the failure branch check for 1‚Äì3 extra labels. The failure in
Figure 1 is one such example. As another example, one application
checksiftheoutputof label_detection containseither‚Äúbuilding‚Äù
or ‚Äúestate‚Äù or ‚Äúmansion‚Äù. This branch‚Äôs recall is very low: 33%.
Keepersuggestsadding‚Äúhouse‚Äù,‚Äúarchitecture‚Äù,and‚Äúwindow‚Äùto
the label set, which would improve the recall to be above 75%.
Fortheremaining6vision-relatedaccuracyfailures,codechanges
byKeepercanalleviatetheproblembutcannotpushtherecallof
the related branch to be above 75%, suggesting fundamental API
limitations. Two of these cases actually involve non-existing labels.
For example, the ‚Äúaluminum‚Äù in line 7 of Figure 4 is actually a non-
existing label. Keeper suggests checking ‚Äúmetal‚Äù instead, which
increases the branch‚Äôs recall to close to 40%, but still below 75%.
Deadcodefailures occurredin3applications.Oneofthemis
duetonon-existinglabels.Twoarebecauseoftyposinbranches
that process ML API output, like the one in Figure 8.
Crashfailures aremainlycausedbyout-of-boundaccessesto
listsreturnedbyMLAPIs,asshowninFigure9.Onecrashiscausedbybuggycodeinsideabranchbodythathandlesimageswithcoins
inside. This failure cannot be exposed by other testing techniques,
as they did not produce images with coins inside.
Falsepositives. Keeperhastwofalsepositivesintotal(theyare
not included in Table 2). One application tries to detect sensitive
documentbycheckingifanyoutputofthedocument-classification
APIcontainsa‚Äúensitive‚Äùsub-string.Keeperfeedsitspseudo-inverse
function with ‚Äúensitive‚Äù and fails to get any test inputs, and hence
Figure 11: End-user preference: Original vs. Keeper version.
incorrectly reports a dead-code failure. The other application has a
branchthatgetscoveredonlywhenanMLAPIgeneratesaspecific
outputwithlowconfidence.Keeperisnoteffectiveatgenerating
low-confidence inputs and wrongly reports an accuracy failure.
Thresholdsetting. AsdiscussedinSection4.1,therecallthresh-
oldùõºissetto0.75bydefaultwhendetectingaccuracyfailures.Nat-
urally,morefailureswouldbereportedwhen ùõºislarger.Increasing
ùõºto0.95,whichisunreasonablyhigh,wouldcreates5morefailure
reports; decreasing ùõºto 0.6 would have 2 fewer failure reports.
Resultsacrossapplications. AsshowninTable4,the35fail-
uresexposedbyKeeperarefrom25differentapplications.These
includebothapplicationsthathavereceivedstarsonGithuband
thosethathavenot;boththesmallestapplicationinourbenchmark
suite, WanderStub, and the largest one, ResearchSpring2019.
6.3 User studies
To better evaluate the accuracy failures and the code changes sug-
gestedbyKeeper,werecruited100participantsonAmazonMechan-icalTurk(Mturk)forasoftware-usersurvey.Thesurveyincludes4
applicationsfromourbenchmarksuites:2image-relatedapplica-
tionsand2text-relatedapplications.Oneachsurveypage,abrief
description is given for an application and user-study participants
are told to review how two versions of this application perform
onasetofinputs.Then,thewebpagedisplaysanumberofinput
images/textandthecorrespondingoutputsofapplicationversion-1
andapplicationversion-2.Thesetwoversionsaretheoriginalap-
plication and the application with suggested code changes from
Keeper(referredtoas fixedinFigure11);werandomlydecidewhich
one of them is version-1 and which is version-2 on each survey
pagetoreducepotentialbias.Eachparticipantisaskedtoanswer
questions about (1) for each input, which version‚Äôs output they
prefer; and(2) whichversion they thinkis betterwith everything
considered. Participants were compensated $5 after the survey.
AsummaryoftheuserstudyresultsisshowninFigure11.As
we can see, in all cases, a dominate portion of end-users prefer
the version with changes suggested by Keeper over the original
version,supportingKeeper‚Äôsjudgementaboutaccuracyfailuresand
Keeper‚Äôsattemptinfixingtheaccuracyproblems.Atthesametime,
we also noticed that there are 20‚Äì26% of user-study participantswho prefer the original software and 12‚Äì27% who feel the two
versions are about the same. These results confirm the fact that
cognitivetasksareinherentlysubjective‚Äîevenhumanbeingsoften
do not agree with each other on these tasks.
221
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automated Testing of Software that Uses Machine Learning APIs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
7 THREATS TO VALIDITY
Internal threats to validity. Keeper assumes that search engines‚Äô
top results are mostly consistent with human judgement, which
could be incorrect. The failure identification and fixing attempts
inKeeperareinherentlyprobabilistic.TherecallthatKeepercal-
culated for each branch could vary depending on the test inputs.
More test inputs would make the testing procedure more robust.
Some inputs generated by Keeper may not be the inputs that
the software aims to handle, like the image being a photo taken
indoorandyetthesoftwaremeanttobeusedoutdoor.WhenKeeper
expands a branch‚Äôs comparison label set, the increase of the recall
sometimes comes with the decrease of the precision (i.e., more
inputsnotexpectedtoexercisethebranchdoesexercise).AlthoughKeeper usesthe F1-scoreto balance precisionand recall, ultimatelydevelopersneedtomakethecodechangedecision.Weimplemented
Keeper IDE plug-in, aiming to help developers make informed
decision about how their software uses ML APIs.
WhenaninputexpectedbyKeepertocoverabranch ùëèfailsto
doso,thisinputmaycoveranotherbranch ùëè/primewhosebodyconducts
the same computation as ùëè. This would confuse Keeper‚Äôs failure
identification, although we have not observed such situations.
Externalthreatstovalidity. Mostofapplicationsinourbench-
mark suite, including those used as examples in the paper, are
research applications, hackathon projects, or demo programs. Con-
sequently, observations and results obtained from them might not
generalizetomorewidelyused,real-worldapplications.Ourtool
is only tested with python applications using Google AI, not other
ML Cloud API services.
8 RELATED WORK
ML-related software. Prior work studied development phases
[5,84‚Äì87] of software that contains machine learning components.
They do not look at how to test such software.
Arecentstudymanuallyidentifiedanti-patterns[ 7]fromsoft-
warethatusesMLAPIs.Keeperdiffersfromthisstudybyproposing
testing techniques that can automatically expose failures and at-
tributefailurecauses.Onthecontrary,thisrecentstudyobtained
all its anti-patterns through manual code inspection. It managedto build automated detectors for some performance-related anti-patterns, like repeatedly calling a ML API with a constant input,but does not have automated bug detection or testing solutions
for any correctness-related anti-patterns.Furthermore, due to the
different design goals, the type of failure root causes covered by
Keeperalsodiffersfromthepreviousstudy.Inthe45applications
thatareevaluatedbothbyKeeperandthepreviousstudy,Keeper
automaticallyexposed32failures,amongwhichonly3werealso
identified by the previous study.
Another line of work [ 88‚Äì91] studies testing autonomous sys-
tems. They are tailored for the characteristics of autonomous driv-
ing and spatial-temporal data, and thus not applicable to most ML
software targeted by Keeper.
ML-relatedtesting. Muchresearchhasbeendonefortesting[ 9,
17‚Äì41,92]andfixing[ 42‚Äì45]neuralnetworks,intermsofaccuracy,
fairness, and security. Other work studies implementation bugs of
neuralnetworkarchitectures[ 93,94]andothermachinelearning
models [95, 96]. They are orthogonal to Keeper.As discussed in Section 1, some previous work looked at how to
test specific software that contains ML components [ 12‚Äì15]. Un-
fortunately, their solutions do not apply to general ML software.For example, one work trained a SVM classifier to judge the cor-
rectness of an image dilation program, leveraging the fact that the
input image and the output image should contain the same objects
[12].Totestablood-vesselimagecategorizer,previouswork[ 13]
generates blood-vessel images with certain density, branches, and
other features, and use these features to generate output ground
truth. Previous work [ 14,15] uses metamorphic approaches to test
entity detection and image region growth programs. They require
application-specific rules about inputs and outputs relationship
(e.g., after we concatenate inputs of entity detection, the output
becomes the concatenation of individual outputs [15]).
Priorwork studiesautomatictestingand bugdetectionofma-
chinelearningAPIs,includingframeworksforimplementingneural
networks[ 97‚Äì103]andRESTAPIsthatprovidemachinelearning
solutions [ 104‚Äì106]. They focus on the implementation inside ML
APIs, not how they interact with other software components.
Test generation using search engines. Previous work [ 107,
108] explored using search engines to generate string inputs for
software under test.Specifically,when a program identifier corre-
spondstoacommonconcept,suchas emailAddress ,thisidentifier
can be used as a keyword to search for related web pages. The
resulting web pages can then be processed to help generate related
stringinputs(e.g.,arealisticemailaddress).Clearly,Keepertackles
fundamentally different problems from previous work, although
Keeper also leverages search engines.
9 CONCLUSION
It is challenging to efficiently and effectively test software contain-
ing machine learning components. We present Keeper, an auto-mated coverage-guided testing framework that helps developersto detect bugs and provide fixing suggestions for their softwareimplementation. Keeper automatically generates test cases via anoveltwo-stagesymbolicexecutionandKeeper-designedMLin-
verse functions. We evaluate Keeper with a variety of open-source
machinelearningapplicationsandachievehighcodecoveragewith
a small set of test cases. It identifies bugs that leads to software
crash, lower inference accuracy, or dead code.
10 DATA AVAILABILITY
We release our benchmarks, the tool source code, experimental
results, and user study results online [80].
ACKNOWLEDGEMENT
We thank the reviewers for their insightful feedback. The authors‚Äô
researchissupportedbyNSF(CNS1764039,CNS1956180,CCF1837120,
CCF2119184, CNS1952050,CCF1823032), ARO (W911NF1920321),
and a DOE Early Career Award (grant DESC0014195 0003). Ad-ditional support comes from the CERES Center for UnstoppableComputing, CDAC Summer Lab, the Marian and Stuart Rice Re-search Award, Microsoft research dissertation grant, UChicago
College Research Fellow Grant, and research gifts from Facebook.
222
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA C. Wan, S. Liu, S. Xie, Y. Liu, H. Hoffmann, M. Maire, and S. Lu
REFERENCES
[1]Google, ‚ÄúGoogle cloud ai.‚Äù Online document https://cloud.google.com/products/
ai, 2020.
[2]Amazon, ‚ÄúAmazon artificial intelligence service.‚Äù Online document https://aws.
amazon.com/machine-learning/ai-services, 2020.
[3]Microsoft, ‚ÄúMicrosoft azure cognitive services.‚Äù Online document https://azure.
microsoft.com/en-us/services/cognitive-services, 2020.
[4] IBM, ‚ÄúIbm watson.‚Äù Online document https://www.ibm.com/watson, 2020.
[5]S.Amershi,A.Begel,C.Bird,R.DeLine,H.Gall,E.Kamar,N.Nagappan,B.Nushi,andT.Zimmermann,‚ÄúSoftwareengineeringformachinelearning:Acasestudy,‚Äù
inICSE-SEIP, pp. 291‚Äì300, IEEE, 2019.
[6]K.DasandR.N.Behera,‚ÄúAsurveyonmachinelearning:concept,algorithms
and applications,‚Äù International Journal of Innovative Research in Computer and
Communication Engineering, vol. 5, no. 2, pp. 1301‚Äì1309, 2017.
[7]C. Wan, S. Liu, H. Hoffmann, M. Maire, and S. Lu, ‚ÄúAre machine learning cloud
apisusedcorrectly?,‚Äùin 43thInternationalConferenceonSoftwareEngineering
(ICSE‚Äô21), 2021.
[8]Phoenix, ‚ÄúA fire-detection application.‚Äù https://github.com/yunusemreemik/
Phoenix.
[9]A.Odena,C.Olsson,D.Andersen,andI.Goodfellow,‚ÄúTensorfuzz:Debugging
neural networks with coverage-guided fuzzing,‚Äù in ICML, 2019.
[10]X. Xie, L. Ma, F. Juefei-Xu, H. Chen, M. Xue, B. Li, Y. Liu, J. Zhao, J. Yin, and
S. See, ‚ÄúDeephunter: Hunting deep neural network defects via coverage-guided
fuzzing,‚ÄùarXiv preprint arXiv:1809.01266, 2018.
[11]P. Ma, S. Wang, and J. Liu, ‚ÄúMetamorphic testing and certified mitigation of
fairness violations in nlp models.,‚Äù in IJCAI, pp. 458‚Äì465, 2020.
[12]T. Jameel, L. Mengxiang, and L. Chao, ‚ÄúAutomatic test oracle for image process-
ing applications using support vector machines,‚Äù in 2015 6th IEEE International
Conference on Software Engineering and Service Science (ICSESS), pp. 1110‚Äì1113,
IEEE, 2015.
[13]M.C.J√∫nior,R.A.Oliveira,M.A.Valverde,M.P.Jackowski,F.L.Nunes,andM.E.
Delamaro, ‚ÄúFeature-based test oracles to categorize synthetic 3d and 2d images
ofbloodvessels,‚Äùin Proceedingsofthe2ndBrazilianSymposiumonSystematic
and Automated Software Testing, pp. 1‚Äì6, 2017.
[14]C.Jiang,S.Huang,andZ.Hui,‚ÄúMetamorphictestingofimageregiongrowthpro-
gramsin imageprocessingapplications,‚Äùin 2018IEEE InternationalConference
on Software Quality, Reliability and Security Companion (QRS-C), 2018.
[15]M. Srinivasan, M. P. Shahri, I. Kahanda, and U. Kanewala, ‚ÄúQuality assurance of
bioinformatics software: a case study of testing a biomedical text processing
toolusingmetamorphictesting,‚Äùin Proceedingsofthe3rdInternationalWorkshop
on Metamorphic Testing, pp. 26‚Äì33, 2018.
[16]J. Wang, G. Dong, J. Sun, X. Wang, and P. Zhang, ‚ÄúAdversarial sample detection
for deep neural network through model mutation testing,‚Äù in ICSE, 2019.
[17]K. Pei, Y. Cao, J. Yang, and S. Jana, ‚ÄúDeepxplore: Automated whitebox testing of
deep learning systems,‚Äù in ASPLOS, 2017.
[18]Y.Tian,K.Pei,S.Jana,andB.Ray,‚ÄúDeeptest:Automatedtestingofdeep-neural-
network-driven autonomous cars,‚Äù in ICSE, 2018.
[19]X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao, B. Li, J. Yin, and
S. See, ‚ÄúDeephunter: A coverage-guided fuzz testing framework for deep neural
networks,‚Äù in ISSTA, pp. 146‚Äì157, 2019.
[20]S. Ma, Y. Liu, W.-C. Lee, X. Zhang, and A. Grama, ‚ÄúMode: automated neural
network model debugging via state differential analysis and input selection,‚Äù in
ESEC/FSE, 2018.
[21]S. Ma, Y. Aafer, Z. Xu, W.-C. Lee, J. Zhai, Y. Liu, and X. Zhang, ‚ÄúLamp: data
provenanceforgraphbasedmachinelearningalgorithmsthroughderivative
computation,‚Äù in FSE, 2017.
[22]N.D.Bui,Y.Yu,andL.Jiang,‚ÄúAutofocus:interpretingattention-basedneural
networks by code perturbation,‚Äù in ASE, 2019.
[23]R. B. Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, ‚ÄúTesting vision-based
control systems using learnable evolutionary algorithms,‚Äù in ICSE, 2018.
[24]L.Ma,F.Zhang,J.Sun,M.Xue,B.Li,F.Juefei-Xu,C.Xie,L.Li,Y.Liu,J.Zhao,
etal.,‚ÄúDeepmutation:Mutationtestingofdeeplearningsystems,‚Äùin ISSRE,2018.
[25]M.Zhang,Y. Zhang,L.Zhang,C.Liu,andS. Khurshid,‚ÄúDeeproad:Gan-based
metamorphictestingandinputvalidationframeworkforautonomousdriving
systems,‚Äù in ASE, 2018.
[26]A.Dwarakanath, M.Ahuja, S.Sikand, R.M. Rao,R. J.C.Bose, N.Dubash, and
S.Podder,‚ÄúIdentifyingimplementationbugsinmachinelearningbasedimage
classifiers using metamorphic testing,‚Äù in ISSTA, 2018.
[27]S. Galhotra, Y. Brun, and A. Meliou, ‚ÄúFairness testing: testing software for
discrimination,‚Äù in FSE, 2017.
[28]R.Angell,B. Johnson,Y.Brun,and A.Meliou,‚ÄúThemis:Automatically testing
software for discrimination,‚Äù in ESEC/FSE, 2018.
[29]S. Amershi, M. Chickering, S. M. Drucker, B. Lee, P. Simard, and J. Suh, ‚ÄúModel-
tracker: Redesigning performance analysis tools for machine learning,‚Äù in CHI,
2015.
[30]S.Yan,G.Tao,X.Liu,J.Zhai,S.Ma,L.Xu,andX.Zhang,‚ÄúCorrelationsbetween
deepneuralnetworkmodelcoveragecriteriaandmodelquality,‚Äùin ESEC/FSE,2020.
[31]F. Zhang, S. P. Chowdhury, and M. Christakis, ‚ÄúDeepsearch: A simple and
effective blackbox attack for deep neural networks,‚Äù in ESEC/FSE, 2020.
[32]F.Harel-Canada,L.Wang,M.A.Gulzar,Q.Gu,andM.Kim,‚ÄúIsneuroncoverage
a meaningful measure for testing deep neural networks?,‚Äù in ESEC/FSE, 2020.
[33]V. Riccio and P. Tonella, ‚ÄúModel-based exploration of the frontier of behaviours
for deep learning system testing,‚Äù in ESEC/FSE, 2020.
[34]S. Gerasimou, H. F. Eniser, A. Sen, and A. Cakan, ‚ÄúImportance-driven deep
learning system testing,‚Äù in ICSE, 2020.
[35]B. Paulsen, J. Wang, and C. Wang, ‚ÄúReludiff: Differential verification of deep
neural networks,‚Äù in ICSE, 2020.
[36]X. Zhang, X. Xie, L. Ma, X. Du, Q. Hu, Y. Liu, J. Zhao, and M. Sun, ‚ÄúTowardscharacterizing adversarial defects of deep learning software from the lens of
uncertainty,‚Äù in ICSE, 2020.
[37]D. Berend, X. Xie, L. Ma, L.Zhou, Y. Liu, C. Xu, and J. Zhao, ‚ÄúCats are not fish:
Deep learning testing calls for out-of-distribution awareness,‚Äù in FSE, 2020.
[38]Y. Feng, Q. Shi, X. Gao, J. Wan, C. Fang, and Z. Chen, ‚ÄúDeepgini: prioritizingmassive tests to enhance the robustness of deep neural networks,‚Äù in ISSTA,
2020.
[39]S. Dutta, A. Shi, R. Choudhary, Z. Zhang, A. Jain, and S. Misailovic, ‚ÄúDetecting
flaky tests in probabilistic and machine learning applications,‚Äù in ISSTA, 2020.
[40]S. Lee, S. Cha, D. Lee, and H. Oh, ‚ÄúEffective white-box testing of deep neural
networks with adaptive neuron-selection strategy,‚Äù in ISSTA, 2020.
[41]A.SharmaandH.Wehrheim,‚ÄúHigherincome,largerloan?monotonicitytesting
of machine learning models,‚Äù in ISSTA, 2020.
[42]H. Zhang and W. Chan, ‚ÄúApricot: a weight-adaptation approach to fixing deep
learning models,‚Äù in ASE, 2019.
[43]Z.Li,X.Ma,C.Xu,J.Xu,C.Cao,andJ.L√º,‚ÄúOperationalcalibration:Debugging
confidence errors for dnns in the field,‚Äù in ESEC/FSE, 2020.
[44]Z.Sun,J.M.Zhang,M.Harman,M.Papadakis,andL.Zhang,‚ÄúAutomatictesting
and improvement of machine translation,‚Äù in ICSE, 2020.
[45]M. J. Islam, R. Pan, G. Nguyen, and H. Rajan, ‚ÄúRepairing deep neural networks:
Fix patterns and challenges,‚Äù in ICSE, 2020.
[46]Microsoft,‚ÄúVisualstudiocode.‚ÄùOnlinedocumenthttps://code.visualstudio.com/,
2021.
[47]M. Irlbeck et al., ‚ÄúDeconstructing dynamic symbolic execution,‚Äù Dependable
Software Systems Engineering, vol. 40, p. 26, 2015.
[48] H. Pham, Z. Dai, Q. Xie, and Q. V. Le, ‚ÄúMeta pseudo labels,‚Äù in CVPR, 2021.
[49]recipeGo,‚ÄúAreciperecommendationapplication.‚Äùhttps://github.com/Reckonzz/
recipeGO.
[50]emotion2music, ‚ÄúA smart music player application.‚Äù https://github.com/
varnachandar/emotion2music.
[51]NsTool, ‚ÄúA monitor application.‚Äù https://github.com/clarkwkw/ns_online_
toolkit.
[52]noteScript, ‚ÄúA lecture note application.‚Äù https://github.com/GalenWong/
noteScript.
[53]stockmine,‚ÄúAstockpredictionapplication.‚Äùhttps://github.com/nicholasadamou/
stockmine.
[54]A.Go,R.Bhayani,andL.Huang,‚ÄúTwittersentimentclassificationusingdistant
supervision,‚Äù CS224N project report, Stanford, vol. 1, no. 12, p. 2009, 2009.
[55]Klassroom, ‚ÄúA lecture note application.‚Äù https://github.com/dev5151/Klassroom.
[56]TRANSLATOR, ‚ÄúA smart light application.‚Äù https://github.com/mubeenafatima/
TRANSLATOR.
[57]HeapSortCypher, ‚ÄúA garbage classification application.‚Äù https://github.com/
matthew-chu/heapsortcypher.
[58]D.Chaffey,‚ÄúSearchenginemarketingstatistics2020.‚Äùhttps://www.smartinsights.
com/search-engine-marketing/search-engine-statistics/.
[59]M. Bing, ‚ÄúBing image search.‚Äù https://www.bing.com/images/trending?FORM=
ILPTRD.
[60] ‚ÄúLorem picsum.‚Äù https://picsum.photos.[61] ‚ÄúWikipedia.‚Äù https://en.m.wikipedia.org/.[62] ‚ÄúEncyclopedia britannica.‚Äù https://www.britannica.com/.[63] ‚ÄúPillow: Python imaging library.‚Äù https://pypi.org/project/Pillow/.[64]
WanderStub, ‚ÄúAn exchange conversion application.‚Äù https://github.com/
richardjpark26/WanderStub.
[65]A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, ‚ÄúLanguagemodels are unsupervised multitask learners,‚Äù OpenAI blog,v o l .1 ,n o .8 ,p .9 ,
2019.
[66] ‚Äúpyttsx3: Text-to-speechlibrary for python.‚Äù https://pypi.org/project/pyttsx3/.
[67]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet: A large-scale
hierarchical image database,‚Äù in CVPR, 2009.
[68]A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Ka-
mali,S.Popov,M.Malloci,A.Kolesnikov, etal.,‚ÄúTheopenimagesdatasetv4,‚Äù
International Journal of Computer Vision, pp. 1‚Äì26, 2020.
[69]J. Gui, Z. Sun, Y. Wen, D. Tao, and J. Ye, ‚ÄúA review on generative adversarial
networks:Algorithms,theory,andapplications,‚Äù arXivpreprintarXiv:2001.06937,
2020.
223
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. Automated Testing of Software that Uses Machine Learning APIs ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
[70]A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
and I. Sutskever, ‚ÄúZero-shot text-to-image generation,‚Äù arXiv preprint
arXiv:2102.12092, 2021.
[71]FESMKMITL,‚ÄúAsmartcameraapplication.‚Äùhttps://github.com/matthewjmc/
FESMKMITL.
[72] Verlan, ‚ÄúA pet application.‚Äù https://github.com/sarvesh-tech/Verlan.[73]
FortniteKillfeed,‚ÄúArealtimetrackerapplication.‚Äùhttps://github.com/Godsinred/
FortniteKillfeed.
[74] ‚ÄúWikipedia.‚Äù https://www.wikidata.org/.[75]
C. W. Barrett, C. L. Conway, M. Deters, L. Hadarean, D. Jovanovic, T. King,
A. Reynolds, and C. Tinelli, ‚ÄúCVC4,‚Äù in Computer Aided Verification - 23rd Inter-
national Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings
(G. Gopalakrishnan and S. Qadeer, eds.), vol. 6806 of Lecture Notes in Computer
Science, pp. 171‚Äì177, Springer, 2011.
[76]‚ÄúPython system-specific parameters and functions.‚Äù https://docs.python.org/3/
library/sys.html#sys.settrace.
[77]D. Marby and N. Yonskai, ‚ÄúPyan3: Offline call graph generator for python 3.‚Äù
https://github.com/davidfraser/pyan.
[78]D.Halter,‚ÄúJedi:anawesomeauto-completion,staticanalysisandrefactoring
library for python.‚Äù Online document https://jedi.readthedocs.io.
[79] ‚Äúscikit-learn: Machine learning in python.‚Äù https://scikit-learn.org/stable/.[80]
C.Wan,S.Liu,S.Xie,Y.Liu,H.Hoffmann,M.Maire,andS.Lu,‚ÄúProjectWebpage:
AccurateLearningforEneRgyandTimelinessinSoftwareSystem.‚Äùhttps://alert.
cs.uchicago.edu/#release.
[81]Kaggle,‚ÄúTwitterusairlinesentiment.‚Äùhttps://www.kaggle.com/crowdflower/
twitter-airline-sentiment.
[82]‚Äú100englishdailysentencesfordailyuse.‚Äùhttps://englishspeakingcourse.net/
100-english-sentences-for-daily-use/.
[83]Fuzzit.dev, ‚ÄúPythonfuzz: coverage-guided fuzz testing for python.‚Äù https://gitlab.
com/gitlab-org/security-products/analyzers/fuzzers/pythonfuzz.
[84]C. Hill, R. Bellamy, T. Erickson, and M. Burnett, ‚ÄúTrials and tribulations of
developers of intelligent systems: A field study,‚Äù in VL/HCC, 2016.
[85]M.Kim,T.Zimmermann,R.DeLine,andA.Begel,‚ÄúTheemergingroleofdata
scientists on software development teams,‚Äù in ICSE, 2016.
[86]M.Kim, T.Zimmermann, R.DeLine, andA.Begel, ‚ÄúDatascientists insoftware
teams: State of the art and challenges,‚Äù TSE, 2017.
[87]X. Zhao and X. Gao, ‚ÄúAn ai software test method based on scene deductive
approach,‚Äù in 2018 IEEE International Conference on Software Quality, Reliability
and Security Companion (QRS-C), pp. 14‚Äì20, IEEE, 2018.
[88]P.Helle,W.Schamai,andC.Strobel,‚ÄúTestingofautonomoussystems‚Äìchallenges
and current state-of-the-art,‚Äù in INCOSE international symposium, 2016.
[89]T. Linz, ‚ÄúTesting autonomous systems,‚Äù in The Future of Software Quality Assur-
ance, pp. 61‚Äì75, Springer, Cham, 2020.
[90]M. Lindvall, A. Porter, G. Magnusson, and C. Schulze, ‚ÄúMetamorphic model-based testing of autonomous systems,‚Äù in 2017 IEEE/ACM 2nd International
Workshop on Metamorphic Testing (MET), 2017.
[91]H.KhosrowjerdiandK.Meinke,‚ÄúLearning-basedtestingforautonomoussys-
temsusingspatialandtemporalrequirements,‚Äùin Proceedingsofthe1stInter-
national Workshop on Machine Learning and Software Engineering in Symbiosis,
2018.
[92]H. Zhu, D. Liu, I. Bayley, R. Harrison, and F. Cuzzolin, ‚ÄúDatamorphic testing: A
methodfortestingintelligentapplications,‚Äùin 2019IEEEInternationalConference
On Artificial Intelligence Testing (AITest), 2019.
[93]Y. Zhang, L. Ren, L. Chen, Y. Xiong, S.-C. Cheung, and T. Xie, ‚ÄúDetecting nu-
merical bugs in neural network architectures,‚Äù in ESEC/FSE, 2020.
[94]G.Jahangirova,N.Humbatova,G.Bavota,V.Riccio,A.Stocco,andP.Tonella,
‚ÄúTaxonomy of real faults in deep learning systems,‚Äù in ICSE, 2020.
[95]Y. Tao, S. Tang, Y. Liu, Z. Xu, and S. Qin, ‚ÄúHow do api selections affect the
runtime performance of data analytics tasks?,‚Äù in ASE, 2019.
[96]D. Cheng, C. Cao, C. Xu, and X. Ma, ‚ÄúManifesting bugs in machine learningcode:Anexplorativestudywithmutationtesting,‚Äùin 2018IEEEInternational
ConferenceonSoftwareQuality,ReliabilityandSecurity(QRS),pp.313‚Äì324,IEEE,
2018.
[97]H. V. Pham, T. Lutellier, W. Qi, and L. Tan, ‚ÄúCradle: cross-backend validation to
detect and localize bugs in deep learning libraries,‚Äù in ICSE, 2019.
[98]L.Buitinck,G.Louppe,M.Blondel,F.Pedregosa,A.Mueller,O.Grisel,V.Nic-
ulae, P. Prettenhofer, A. Gramfort, J. Grobler, et al., ‚ÄúApi design for machine
learning software: experiences from the scikit-learn project,‚Äù arXiv preprint
arXiv:1309.0238, 2013.
[99]E. R. Sparks, A. Talwalkar, V. Smith, J. Kottalam, X. Pan, J. Gonzalez, M. J.Franklin, M. I. Jordan, and T. Kraska, ‚ÄúMli: An api for distributed machine
learning,‚Äù in ICDM, 2013.
[100]S. Bahrampour, N. Ramakrishnan, L. Schott, and M. Shah, ‚ÄúComparative study
of deep learning software frameworks,‚Äù arXiv preprint arXiv:1511.06435, 2015.
[101]M.NejadgholiandJ.Yang,‚ÄúAstudyoforacleapproximationsintestingdeep
learning libraries,‚Äù in ASE, 2019.
[102]Q. Guo, X. Xie, Y. Li, X. Zhang, Y. Liu, L. Xiaohong, and C. Shen, ‚ÄúAudee:
Automated testing for deep learning frameworks,‚Äù in FSE, 2020.[103]S. Tizpaz-Niari, P. Cern `y, and A. Trivedi, ‚ÄúDetecting and understanding real-
world differential performance bugs in machine learning libraries,‚Äù in ISSTA,
2020.
[104]F.Petrillo,P.Merle,N.Moha,andY.-G.Gu√©h√©neuc,‚ÄúArerestapisforcloudcom-
putingwell-designed?anexploratorystudy,‚Äùin ICSOC,pp.157‚Äì170,Springer,
2016.
[105]E. Gossett, C. Toher, C. Oses, O. Isayev, F. Legrain, F. Rose, E. Zurek, J. Carrete,
N. Mingo, A. Tropsha, et al., ‚ÄúAflow-ml: A restful api for machine-learning
predictions of materials properties,‚Äù Computational Materials Science, 2018.
[106]P.Godefroid,D.Lehmann,andM.Polishchuk,‚ÄúDifferentialregressiontesting
for rest apis,‚Äù in ISSTA, 2020.
[107]P. McMinn, M. Shahbaz, and M. Stevenson, ‚ÄúSearch-based test input generation
forstringdatatypesusingtheresultsofwebqueries,‚Äùin 2012IEEEFifthInter-
national Conference on Software Testing, Verification and Validation , pp. 141‚Äì150,
IEEE, 2012.
[108]M. Shahbaz, P. McMinn, and M. Stevenson, ‚ÄúAutomatic generation of valid and
invalid testdata for stringvalidation routines using websearches and regular
expressions,‚Äù Science of Computer Programming, vol. 97, pp. 405‚Äì425, 2015.
224
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:51:55 UTC from IEEE Xplore.  Restrictions apply. 