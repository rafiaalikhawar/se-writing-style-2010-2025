This is a repository copy of A Generative and Mutational Approach for Synthesizing Bug-
exposing Test Cases to Guide Compiler Fuzzing .
White Rose Research Online URL for this paper:
https://eprints.whiterose.ac.uk/202769/
Version: Accepted Version
Proceedings Paper:
Ye, G., Hu, T., Tang, Z. et al. (5 more authors) (2023) A G enerative and Mutational 
Approach for Synthesizing Bug-exposing Test Cases to Guide Compi ler Fuzzing. In: 
Proceedings of the 31st ACM Joint European Software Engine ering Conference and 
Symposium on the Foundations of Software Engineering. ES EC/FSE: The 31st ACM Joint 
European Software Engineering Conference and Symposium on t he Foundations of 
Software Engineering, 03-09 Dec 2023, San Francisco, USA.  Association for Computing 
Machinery (ACM) , pp. 1127-1139. ISBN 9798400703270 
https://doi.org/10.1145/3611643.3616332
Â© 2023 Copyright held by the owner/author(s). This is an aut hor produced version of a 
conference paper accepted for publication in Proceedings of ESEC/FSE 2023, made 
available under the terms of the Creative Commons Attributio n License (CC-BY), which 
permits unrestricted use, distribution and reproduction in a ny medium, provided the 
original work is properly cited.
eprints@whiterose.ac.uk
https://eprints.whiterose.ac.uk/
Reuse 
This article is distributed under the terms of the Creat ive Commons Attribution (CC BY) licence. This licence 
allows you to distribute, remix, tweak, and build upon t he work, even commercially, as long as you credit the 
authors for the original work. More information and t he full terms of the licence here: 
https://creativecommons.org/licenses/ 
Takedown 
If you consider content in White Rose Research Online to b e in breach of UK law, please notify us by 
emailing eprints@whiterose.ac.uk including the URL of th e record and the reason for the withdrawal request. A Generativeand MutationalApproachforSynthesizing
Bug-exposingTestCasesto Guide Compiler Fuzzing
GuixinYe
NorthwestUniversity, China
gxye@nwu.edu.cnTianmin Hu
NorthwestUniversity, China
hutianmin@stumail.nwu.edu.cnZhanyongTangâˆ—
NorthwestUniversity, China
zytang@nwu.edu.cn
Zhenye Fan
NorthwestUniversity, China
fanzhenye@stumail.nwu.edu.cnShin Hwei Tan
ConcordiaUniversity, Canada
shinhwei.tan@concordia.caBoZhang
Tencent Inc., China
cradminzhang@tencent.com
Wenxiang Qian
Tencent Inc., China
saplasqian@tencent.comZheng Wang
Universityof Leeds, UnitedKingdom
z.wang5@leeds.ac.uk
ABSTRACT
Randomtestcasegeneration,or fuzzing,isaviablemeansforun-
covering compiler bugs. Unfortunately, compiler fuzzing can be
time-consuming and inefficient with purely randomly generated
testcasesduetothecomplexityofmoderncompilers.Wepresent
ComFuzz ,afocusedcompilerfuzzingframework. ComFuzz aimsto
improve compiler fuzzing efficiency by focusing on testing compo-
nentsandlanguagefeaturesthatarelikelytotriggercompilerbugs.
Our key insight is human developers tend to make common and
repeaterrorsacrosscompilerimplementations;hence,wecanlever-
agethepreviouslyreportedbuggy-exposingtestcasesofaprogram-
ming language to test a new compiler implementation. To this end,
ComFuzz employs deep learning to learn a test program generator
from open-source projects hosted on GitHub. With the machine-
generated test programs in place, ComFuzz then leverages a set
ofcarefullydesignedmutationrulestoimprovethecoverageand
bug-exposingcapabilitiesofthetestcases.Weevaluate ComFuzz
on11compilersfor JSandJavaprogramminglanguages.Within
260hoursofautomatedtestingruns,wediscovered33uniquebugs
across nine compilers, of which 29 have been confirmed and 22,
includinganAPIdocumentationdefect,havealreadybeenfixedby
thedevelopers.Wealsocompared ComFuzz toeightpriorfuzzers
onfourevaluationmetrics.Ina24-hourcomparativetest, ComFuzz
uncoversatleast 1.5Ã—morebugsthanthestate-of-the-artbaselines.
CCS CONCEPTS
Â·Software and its engineering â†’Software testing and de-
bugging;Compilers ; Â·Computing methodologies â†’Artificial
intelligence .
âˆ—Z.Tang is the corresponding author. G.Yeand T. Hu areco-first authors.
ESEC/FSE â€™23, December 3Å›9, 2023, San Francisco, CA,USA
Â©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
Thisistheauthorâ€™sversionofthework.Itispostedhereforyourpersonaluse.Not
for redistribution. The definitive Version of Record was published in Proceedings of
the31stACMJointEuropeanSoftwareEngineeringConferenceandSymposiumonthe
Foundations of Software Engineering (ESEC/FSE â€™23), December 3Å›9, 2023, San Francisco,
CA,USA, https://doi.org/10.1145/3611643.3616332.KEYWORDS
Fuzzing,Historicalbug,Guidedtesting,Deep learning,Compiler
ACMReference Format:
GuixinYe,TianminHu,ZhanyongTang,ZhenyeFan,ShinHweiTan,Bo
Zhang, Wenxiang Qian, and Zheng Wang. 2023. A Generative and Muta-
tionalApproachforSynthesizingBug-exposingTestCasestoGuideCom-
piler Fuzzing. In Proceedings of the 31st ACM Joint European Software Engi-
neering Conference and Symposium on the Foundations of Software Engineer-
ing (ESEC/FSE â€™23), December 3Å›9, 2023, San Francisco, CA, USA. ACM, New
York, NY, USA,13pages. https://doi.org/10.1145/3611643.3616332
1 INTRODUCTION
Compilersplayakeyroleinsoftwaredevelopment[ 37].Mostappli-
cation developerstreatcompilersas blackboxesandhaveto trust
thecompiler-generatedcode.However,moderncompilersareintri-
cate software systems with large codebases consisting of hundreds
of thousandsoflines, andlike manylarge-scalesoftwareprojects,
compiler bugs are inevitable and often manifest in the deployment
environment[ 78].Unfortunately,detectingcompilerbugscanbe
challenging,yettheirpresencecansignificantlyimpedesoftware
development, leading to runtime crashes and even catastrophic
consequenceswhen applicationsare deployed[70,78].
Automatedtestcodegenerationtechnique-orfuzzing-hasbeen
awell-establishedandeffectivewaytodetectcompilerbugs[ 24].
Compiler fuzzing techniques include generation-based [ 29,85,87]
and mutation-based [ 25,26,50,88] methods, which are typically
usedwithdifferentialtesting[ 77,91].Thisisachievedbyexecuting
a randomly generated test program on multiple compiler test beds
andobservingtheoutputsofthecompilerandexecutablebinary.An
anomalous behavior like compiler crashing, freezing, compilation
timeout,orabinaryexecutionresultthatdeviatesfromthemajority
ofthe outputs indicates apotentialcompilerbug.
A fundamental challenge for fuzzing techniques is generating
test cases that can quickly expose buggy behaviors [ 16]. Exist-
ingapproachestypicallyemployagenerationormutation-based
approach. Generation-based techniquesconstructtestcasesbyus-
ing either manually designed grammar rules [ 44,85], generation
templates [ 89], or by restructuring code snippets extracted from
a program seeds pool [ 40,65]. In contrast, mutation-driven tech-
niques leverage pre-designed rules to synthesize test cases [ 83].ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA G.Ye, T. Hu,Z.Tang,Z.Fan, S.Tan, B.Zhang,W. Qian, andZ.Wang
Thesefuzzingtechniqueshaveprovenuseful;however,theyhave
afundamentallimitationwhenappliedtocompilerswithalarge
codebase.Thesetechniquesoftenrelyonrandomtestgenerationto
achievecoverage,whichisunlikelytobeeffectiveduetotheuneven
distributionofsoftwarebugsacrosscomponents[ 78].Inpractice,it
iscommonforafewmodulestoaccountformostbugs[ 55].Conse-
quently,arandomtestgenerationapproachmaydisproportionately
allocatetestingeffortstomoduleslessprone tobugs.Therefore,a
more efficient and effective strategy for compiler fuzzing should
directfuzzing efforts toward modules more likely tocontain bugs.
Byfocusingonthesespecificmodules,wecanmaximizetheimpact
ofourtestingeffortsandincreasethechancesofuncoveringcritical
vulnerabilitieswithin agiven test time budget.
Wepresent ComFuzz ,anewcompilerfuzzingframeworkthat
combinesgenerativeandmutationaltechniqueswhileimproving
thecompilertestingefficiency.Unlikepreviousapproachesthatrely
on random test case generation for achieving coverage, which is
challenginginthecontextofcompilertesting[ 29,36,88],ComFuzz
isspecificallydesignedtotargetcompilercomponentsthataremore
likelytocontainbugs.Toachievethis, ComFuzz leverageshistorical
testprogramsobtainedfromProofs-of-Concept(PoCs)ofCommon
Vulnerabilities and Exposures (CVEs) and compiler test suites. Our
generative approach is motivated by two key observations: bugs
areprevalentinasmallfractionofcodewithinsoftware[ 55,78],
and the fixing of historicaldefectsoften introduces new bugs [ 55].
Insteadofsolelyusinghistoricaltestcasesasseedprogramswitha
random mutation strategy to test compilercomponents uniformly,
ComFuzz conducts intensive fuzzing tests on modules that have
previouslyexhibitedbugs.Thesetargetedmodulesareknowntobe
error-prone andcan potentiallycontain bugsintroducedbyfixes.
To reduce the developerâ€™s efforts in building the test program
generator, we harness the potential of deep-learning-based genera-
tive techniques[ 71].Particularly, we employaTransformer-based
model[71]toinferfeaturesandconstructsofthetargetprogram-
minglanguage.Tocreatetestinputs(programsinourcase),weuse
the historical code as seed input for our trained model, which then
producesnewtestprograms.Sincetheseprogramsarederivedfrom
bug-exposingtestcases,theyareapttoembodyspecificfeatures
like standard library calls or language constructs while exhibiting
new behaviors introduced by the program generator. Therefore,
these generated programs can effectively guide the fuzzing efforts
towardtestingthe error-prone componentsofthe compiler.
Our approach is among the recent efforts of synthesizing test
programs by reassembling the code ingredients extracted from the
historicaltestcases[ 50,91].However,existingapproachesfailedto
conducthigh-intensitytestingforabuggycompilercomponentdue
to the randomness of the assembled test cases. Our key conceptual
insight is that there can be residual bugs in previously buggy mod-
ules,andwecanleverageasetofbug-guidedmutatorstofindthese
residual bugs. For example, to find residual API misuse bugs, we
definedSIM, a mutatorthat replaces the original API with another
similarAPItoexposemoreresidualbugs.Duringeachtestingitera-
tion,abug-guidedmutatorisselectedtomutateatestprogramthat
has been shown to expose anomalous compiler behaviors. We also
designed five general-purpose mutators to improve the diversity
of the generated test cases. We use general-purpose mutators tocreatenewtestcodesincaseswherethedevelopedtestprograms
fail to expose bugsorcannotimprove the code coverage.
Weevaluated ComFuzz on11JavaScript( JS)andJavacompilers.
In260hoursofautomatedtestingruns, ComFuzz reportedatotalof
33uniquebugsacrossninetestedcompilers,covering26previously
unknown bugs. Of the 33 submitted bugs, 29 have been confirmed,
and22bugsÅ›includinganOpenJ9documentbugÅ›havebeenfixed
by developers. Our extensive evaluations show that ComFuzz is
highlyeffectiveingeneratingbug-exposingtestcases.Comparedto
eightstate-of-the-art(SOTA)fuzzers[ 25,27,40,50,65,86,89,91],
ComFuzz uncovers at least 1.5Ã—more bugsthanprior methods.
In summary,this paper makes the following contributions:
â€¢We propose a new compiler fuzzing technique by combining the
historical test programs and bug-guided mutators, which can
quickly cover the defective compiler components and achieve
focusedintensive testing;
â€¢We present an extensible test generation scheme that can be
easilyportedtotestcompilersforotherprogramminglanguages;
â€¢Weevaluatedtheeffectivenessof ComFuzz bycomparingitwith
SOTAfuzzersthatutilizehistoricaltestcasesforsoftwaretesting.
2 BACKGROUNDAND MOTIVATION
2.1 Compiler TestingandChallenges
Prior work for compiler testing includes generation-based [ 14,33,
49,85]andmutation-guided[ 22,80,84,88]methods.Whilepromis-
ing,priormethodsstill sufferfrom the followingtwochallenges:
1)How to generate bug-exposing test cases that can quickly cover
the defective component of a compiler? Although many methods
havebeendevotedtogenerating validtestcases[ 14,49,53,59,89],
all of them are randomly generated so that they fail to quickly
locatethebuggycomponentsofacompilerduringtheearlytesting
phase. Furthermore, a recent study stated that bugs in software
do not conform to a uniform distribution, and only 40% of code
will have bugs [ 55]. This means randomly synthesized test cases
in prior work may be wasting most of the time testing the benign
compiler modules. Even if a test case triggers a compiler bug, prior
work cannot continuously test the buggy compiler module in adja-
cent iterative tests, which is bound to decrease testing efficiency.
Thus,anotherchallengeis howtoconductfocusedandhigh-intensive
testing for a buggy compiler component in adjacent testing iterations.
Recent studies indicate that one major cause of bugs is the in-
complete or incorrect repair of historical bugs [ 55,91]. Meanwhile,
bugsthatarethesameorsimilartothehistoricalonesoftenarise
during software evolution [ 31,39,82]. Furthermore, recent work
hasshownthatusinghistoricaltestprogramsasseedtestprograms
canimprovefuzzingefficiency[ 40,44,50,65,91].Inspiredbythese
studies, we derived two key intuitions: (1) historical test programs
(e.g.,testsuitesorPoCs)canbeusedtogeneratebug-exposingtests
for quickly covering buggy compiler components, and (2) buggy
compilercomponentscanbecontinuouslyandintensivelytested
by mutating the generated bug-exposing tests. These intuitions
helpto addressthe twochallenges above andmotivate our work.
Unlike prior work, our work aims to balance reusing existing
codefragmentsfromhistoricalprogramsandexploringnewpro-
gramstatestoidentifybugs.Thekeyquestionsare:(1)howtoobtain
high-quality seeds and (2) how to leverage historical programs forA Generative andMutationalApproachforSynthesizingBug-exposing TestCases to Guide CompilerFuzzing ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA
1public class JVMTest {
2/*--COMFUZZ-generated args via bug-guided mutators--*/
3 static int i = 999374098;
4 static int limit = 0;
5 static int[] arr = { -1 };
6 int[] test2(int i, int limit, int[] arr) {
7 /*--COMFUZZ-generated code segment via DL-model--*/
8 while(i++ != 0) {
9 if(arr[arr.length - 1] >= limit)
10 break; }
11 return arr; }
12 public static void main(String[] args) throws Exception{
13 newJVMTest.test2(i, limit, arr); }}
Figure 1: A ComFuzz -generated test case, obtained from a
historical testcaseandexposed anew bugofOpenJ9.
exploring new program states. Our approach differs from other
DL-based methods by reusing historical tests as the model seed in-
put for test program generation. We extract randomly cut-out code
blocks to create a high-quality initial pool of seed programs. We
thencombineagenerationmodelwithcarefullydesignedmutators
to explore newprogram states andexpose bugs.
2.2 MotivationExample
Figure 1 shows a bug-exposing test case generated by ComFuzz
during iterative testing, which uncovers a new performance bug of
OpenJ9.Forthistestcase,OpenJ9failstoenableJIToptimizationfor
themainloopinlines8Å›10.Specifically,this while-loop appearsat
the entry of the test2function. It has a walk.bytecodePCOffset
of 0 in OpenJ9, which disables the JIT optimization of OpenJ9,
leadingtosignificantperformancedegradation.Specifically,OpenJ9
takes over 60 seconds to execute the test program, while other JVM
engines like HotSpot take 7 milliseconds. This performance bug
wasfixedbythe OpenJ9 developers.
ComFuzz generatesthisbug-exposingtestprogrambysetting
theappropriatevariables(lines3Å›5)tomanifesttheperformance
bug.Thesevariablesaregeneratedusingourbug-guidedmutator,
whereas the code block ( test2function in lines 6Å›11) is created
from ahistoricalbug-exposing test case[8].
ComFuzz achieves this by first building a DL-based program
generator.Then,thelearnedgeneratorisappliedtoproducenew
test cases by taking as input a randomly chosen seed generation
header (e.g., line 6). Unlike prior work [ 29,86], our seed genera-
tion headers are extracted from the historical test programs, e.g.,
JDK test suites and PoCs collected from CVE. Our goal is to fo-
cus on compiler components that are likely to contain bugs. The
bug-triggeringvariables,e.g., limit = 0 atline4,arecriticalfor
manifesting this performance bug as using a small value makes
theperformancedifferencenegligible.Togeneratesuchvariables,
we study the historical test cases to design BOUN, one of our five
bug-guided mutators. Finally, ComFuzz assembles the generated
program,thebug-triggeringvariabledeclarationstatements,and
thenecessarystartupfunctionintoacomplete,executabletestcase.
2.3 AutomatedProgramGeneration
The rapid advances in deep learning (DL) promote the automated
programmodelingmethods[ 11,19,93],whichhavebeenwidely
usedinprogram-relatedtasks,suchascodeoptimization[ 30,64,87],
programgeneration[ 47],andvulnerabilitydetection[ 52,82].Since
no expert knowledge is required, many DL-based compiler fuzzershave been proposed for automated test program generation [ 29,
34,41,57]. Newer approaches [ 50,86] subsequently employ more
powerfulneuralnetworkstofurtherimprovethecorrectnessofthe
generatedtestcases.Inspiredbypriorwork,weuseanadvanced
neuralnetworktomodelprograms,aimingtoautomaticallygen-
erate test cases. Unlike existing DL-based fuzzers that randomly
createtestcases,wefeedhistoricaltestsintothemodeltogenerate
bug-exposing test cases,achievingabetterbug-exposing ability.
3 OURAPPROACH
Figure 2 provides ComFuzz overview, which uses historical pro-
gramsandbug-guidedmutatorsforfocusedcompilertesting.Tothis
end, weestablish atest programgenerator based ona pre-trained
model[71](Section3.1).Thebuiltgeneratorisusedtogeneratebug-
exposingtestcasesbyfeedingitwiththehistoricaltestprograms
(Section 3.2). The generated test cases are applied to test target
compilersthroughdifferentialtesting(Section3.3),whichkeepsthe
interesting testcasesthatdiscovernewcompilerbranchesortrigger
inconsistentdifferentialoutcomesand apply them to focused and
guidedtests using bug-guidedmutators(Section 3.4).
3.1 ProgramGenerator Construction
Most existing compiler testing approaches utilize domain-specific
languagemodels(e.g.,grammarortemplate-basedgenerators)to
generatetestprograms.Thesemethodsrequireexpertknowledgeto
designthe grammaticalrulesor the generationtemplates, making
these approaches hard to extend for new programming languages.
Instead, our program generator is built upon a Transformer-based
model [71], which is easy to generalize to other programming lan-
guages byfeeding itwiththe target training samples.
Data collection and preprocessing. Since our program gener-
ator is built by fine-tuning a pre-trained model, the fine-tuning
process requires massive training samples. To collect enough sam-
ples, we respectively scraped the top 10k open-source JSandJava
projects ranked by stars hosted on GitHub. For each project, we
extracted function-level code snippets as training samples. Specifi-
cally,ourapproachfirstremovesallcomments.Then,weextractthe
function-level code segments from the programs using parsers (i.e.,
Esprima [ 2]forJSand JavaParser [ 75] forJava). Toimprove the
correctnessofextractedcodesnippets,wealsoextracttheexpected
globalvariablesandinsertthemintothebodyofthecodesegments.
Finally, we utilize syntax analysistools (e.g., JSHint [ 1] forJSand
JavaCompiler [ 4] forJava) to ensure the syntax correctness of the
code segments andstore theminacodebase.
Language model. Our language model is constructed based on a
Transformer-basedneuralnetwork[ 71].Itisessentiallyanencoder-
decodernaturallanguagegenerationmodelwithamulti-headat-
tentionmechanism.Boththeencoderanddecoderarecomposed
of a stack of six identical blocks. Specifically, an encoder block
consists of a multi-head self-attention layer and a position-wise
fully connected feed-forward layer. A decoder block consists of an
encoderblockandamulti-head attentionlayer. For eachnetwork
layer,we employ aresidual connection andanormalization layer.
Fine-tuning. Thepre-trainedmodelisrefinedusingcollectedtrain-
ing programs. During the fine-tuning phase, we encode each train-
ingsampleasasuitablevectorforinputintoneuralnetworks.ToESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA G.Ye, T. Hu,Z.Tang,Z.Fan, S.Tan, B.Zhang,W. Qian, andZ.Wang
CodeSource
Train
TrainingSamples ProgramGeneratorTC
39Test262 Test
Suites
TestCasesHistorical
TestProgramsSeedsPool
Generation
headers
TargetCompilers
V8
JSC
Chakraâ€¦JavaScript Java
HotSpotOpenJ9
GraalVM
â€¦FilteringDuplicate
Miscompilations PotentialBugCodeCoverageMutation
Rules
Bug-guidedMutating
 NewTestCases
GenerateNext
iterative
testing
Figure 2:Overview of ComFuzz ,which combineshistorical testcasesandbug-guided mutatorsforfocused intensive fuzzing.
do so, we employ Byte Pair Encoding (BPE) [ 74], a subword-based
tokenization algorithm. BPE constructs a vocabulary dictionary by
iterativelymergingthemostfrequentpairsofcharactersorcharac-
tersequencesinagivencorpusintosubwords.Thisprocessensures
that each vocabulary item is represented as a subword based on
its frequency in the training set. Using BPE, we create a vocab-
ulary that captures common subword units in the corpus. When
processing a trainingsample, we map eachsubwordto an integer
byreferencing the vocabulary dictionary.Thismapping allowsus
totransformthetrainingsample(i.e.,acodesnippetinthiswork)
intoasequenceofintegers.Bycollectingalltheintegervaluesasso-
ciatedwiththesubwordsinthesample,weobtainarepresentation
vector that effectivelyencodes the sampleâ€™sinformation.
Givenaninputtingvector ğ‘£,whichisfirstfedintothepre-trained
modeltoobtaintheactivationvalueofthelasttransformerblock
ğ‘ğ‘š
ğ‘™.Then,itispassedthroughaclassificationlayertopredictthe
nexttoken. The processcan be representedas follows:
ğ‘ƒ(ğ‘¦|ğ‘£1,...,ğ‘£ğ‘š)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ğ‘š
ğ‘™ğ‘Šğ‘¦) (1)
wheresoftmax representstheclassificationlayer, ğ‘¦isthepredicted
token,and ğ‘Šğ‘¦istheweightmatrix.Sincethepre-trainedmodelhas
billions of parameters, the former layers are language-independent
features. Thus, the fine-tuning process only trains the last few
layers.This isdone byupdatingthe weightsofthe lastfew layers
while keeping others unchanged during fine-tuning. The objective
isto maximizethe following function:
L=âˆ‘ï¸
ğ‘£,ğ‘¦ğ‘™ğ‘œğ‘”ğ‘ƒ(ğ‘¦|ğ‘£1,...,ğ‘£ğ‘š) (2)
Toacceleratemodelconvergence,wefine-tunedourlanguage
modelusingtheAdamoptimizer[ 46]for200epochs.Re-training
took around 40 hours using three NVIDIA GTX 3080Ti GPUs,
whichwasaone-offcost.Thehyperparametersweusedinclude:
temperature=0.75, response length=500, Top P=9 ,andoth-
ersaredefault.Oncetrained,ourlanguagemodelcancontinuously
generatetest programs byfeeding the seedgenerationheaders.
3.2 TestCaseGeneration
Figure 1 shows that a test case contains three ingredients: (1) a
mainfunction (lines 12Å›13), (2) a test program (lines 6Å›11), and (3)
its arguments (lines 3-5). We synthesized test programs by feeding
the generationheadersintoarefinedlanguagemodel.
Generation header extraction. We feed the generation model
withaseedcodeinput(generationheader)extractedfromahistori-
cal test program (e.g., Å‚ int[] test2(int i, int limit, int[]
arr)Å¾ in Figure 1). As the seed input determines the starting point
ofatestprogram,agoodgenerationheaderplaysanimportantrolein generating bug-exposing programs. To obtain high-quality gen-
eration headers, we first collected as many historical test programs
aspossible.For Java,wecollectedthehistoricaltestprogramsfrom
the test suites of HotSpot, OpenJ9, Kona, and GraalVM. For JS, we
obtainedthetestprogramsfromTest-262[ 10],anofficialJavaScript
languageconformancetestsuite.AllPoCsarecollectedfromthe
CVE database. We then extract all function-level code blocks for
each gathered historical test program by parsing it into an AST.
Thegenerationheadersareextractedbyrandomlycuttingoffthe
former lines of the function-level code block. Note that we also
collect 20k ordinary generation headers that are extracted from
open-sourceprojectsforeachprogramminglanguageinorderto
increasethe diversityofgeneratedtest cases.
Test program generation. The test program is synthesized by
feedingthegenerationmodelwitharandomlychosenseedgenera-
tion header. During testing, the generation model first randomly
selectsagenerationheaderfromtheseedpool,anditthenyieldsthe
probabilisticvector ofthenext token (e.g., a subword-based token
encoded by BPE). Differ from natural language generation tasks
that output the token with the highest probability, we employ a
Markov chain Monte Carlo (MCMC) algorithm [ 32], a probabilistic
sampling scheme where a token with a higher prediction proba-
bility is more likely to be chosen to sample the next token. This
processcanimprovethediversityofthegeneratedtestprograms.
Next,thegeneratedtokenisappendedtotheoriginalgeneration
header, which is fed to the generation network to produce the next
token repeatedly. This synthesized process terminates when the
generationnetwork produces thetermination symbol Å‚<EOF>Å¾ or
a bracket â€˜}â€™ that indicates the end of a program method or exceeds
the maximum token length ğœ–.Hereğœ–issetto be 6,000.
Argumentsgeneration. Incompilertesting,ahigh-qualitytest
casenotonlycontainsabug-exposingtestprogrambutalsocon-
tainstheargumentsexpectedbythetestprogram.Tosynthesizethe
desired arguments, we designed several heuristic rules for all basic
datatypes,e.g.,Integer,Double,String,Array,Object,etc.For Java,
the variable type is determined according to the parameter type;
forJS,theargumenttypesareinferredusinganexistingwork[ 86].
3.3 Differential Testing
Weemploytheestablisheddifferentialtestingmechanism[ 61]to
expose compiler bugs. A majority voting scheme is utilized to cap-
turetheanomalouscompilerbehaviorswhichareyieldedbythe
minoritycompilers.Theanomalouscompilationsneedtobefurther
confirmedmanually after filtering duplicatemiscompilations.
Anomalouscompilerbehaviors. Acompilertypicallyconsists
of three components: a parser that checks if a program is correctlyA Generative andMutationalApproachforSynthesizingBug-exposing TestCases to Guide CompilerFuzzing ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA
ATestCase
Parsing
PhaseAll
compilers
parsed?NoParsing
Error
Optimization
PhaseAll
compilers
passed?YesNoOptimization
Error
YesExecution
Phase
Successful?YesPass
NoWrong
Result
Exception
Timeout
Crash
Figure 3: Possible outcomes of differential testing. An anomalous behavior deviating from most compiler outcomes indicates a
potential bug. Our current do not consider Å‚Wrong ResultÅ¾ and Å‚TimeoutÅ¾ at the parsing and optimization stages because it is
hard to establish an oracle for the intermediate results and attribute timeout to intermediate compilation stages, respectively.
coded without syntax and semantic errors; an optimizer that aims
at optimizing code at a high-level intermediate representation (IR),
andagenerator(alsoknownasbackend)whichisresponsiblefor
translatingtheIRintobinarycode.Thus,theincorrectimplementa-
tion of any of the aforementioned three components may produce
an anomalous behavior, indicating a possible compiler bug. This
happens when the outcome for a given test input compiled using a
compiler differs from the outcome from most of the tested compil-
ersforthesameinput,e.g.,wrongresult,exception,timeout,and
crash. These anomalies can manifest at various stages, including
parsing, optimizing, and runtime. However, ComFuzz does not de-
tectÅ‚WrongResultsÅ¾andÅ‚TimeoutÅ¾attheparsingandoptimization
stages because (1) the intermediate outcome during parsing and
optimization istypicallyimplementation-dependent,and (2)it is
hard to attribute compilation timeout to individual stages since we
treatthetestedcompilerasablackbox.Figure3showssevenpo-
tential outcomes when executing a test case. All outcomes (except
for the Å‚PassÅ¾) represent anomalous behaviors that necessitate sub-
sequent manual analysis. A Å‚PassÅ¾ outcome signifies that all tested
compilersyieldidenticaloutcomeswithoutanyabnormalbehavior.
As this outcome aligns withthe expectedresult, itisdisregarded.
Suppose the test case does not trigger any anomalous outcomes
during the parsing and optimization phases, the compilerbackend
proceeds to translate the optimized code into machine instructions
tobeexecutedonthetestedplatform.However,whenexecutingthe
compiled binary, there are potentialscenarios where fourdistinct
anomalousbehaviors mayarise duringruntime. Firstly,a Å‚Wrong
ResultÅ¾occurswhenthebinaryproducesanoutputthatdeviates
from the outputs generated by most tested compilers. This discrep-
ancyindicatesaninconsistencyorerrorwithinthecompiledbinary.
Secondly, anÅ‚ExceptionÅ¾ is encounteredwhenthe execution of the
binary results in a thrown exception, while the execution given
byothercompilersdoesnotexhibitthisbehavior.Thispointsto-
wards an exception-handling flaw. Thirdly, a Å‚TimeoutÅ¾ occurs if a
program fails to terminate within the specified time limit, while bi-
nariesgeneratedbyothercompilersterminatebeforethetimelimit.
This usually indicates an optimization bug, leading to prolonged
executiontimes.Lastly,aÅ‚CrashÅ¾canmanifestifthebinaryitself
orthe compiler (e.g., forinterpret execution mode)crashes during
execution.This occurrence suggestsapotentialcompilerbug.
Identifying anomalousbehaviors. Among sixtypes of anoma-
lous behaviors, CrashandTimeoutare of immediate interest, in-
dicating the erroneous compiler implementation. Following the
practices in prior work [ 25,40], we set the timeout threshold for
runtime execution to 30 seconds. We consider an erroneous behav-
ioroccurswhenanybinarygivenbyacompilerhasanexecution
time exceeding 30 seconds, whereas binaries generated by otherException in thread â€œ main â€ java.lang.StringIndexOutOfBoundsException : String index out of range: 1
at java.lang.AbstractStringBuilder.deleteCharAt (AbstractStringBuilder.java:824)
at java.lang.StringBuilder.deleteCharAt(StringBuilder.java:253)
â€¦â€¦
Exception in thread â€œmainâ€ java.lang. StringIndexOutOfBoundsException : index 1, length 1
at java.base/java.lang.String.checkIndex(String.java:3278)
at java.base/java.lang. AbstractStringBuilder.deleteCharAt (AbstractStringBuilder.java:916)
at java.base/java.lang.StringBuilder.deleteCharAt(StringBuilder.java:297)
â€¦â€¦HotSpot
OpenJ9
Figure4:DifferentiatedanomalousbehaviorsthatHotSpot
andOpenJ9 threw,which indicatethe same JVMexception.
compilers for the same input complete their execution within 30
seconds.Inourevaluation,wedonotencounteranyfalsepositives
whenusingthisthreshold,sowedonotfinditbeneficialtoincrease
the threshold. For the other four anomalous behaviors, a majority
votingschemeisappliedtoidentifyifacompilercontainspotential
defeatsbycomparing the compilation andexecutionresults.
Since all compilers may not have the same error or exception
messages,directlycomparingtheiroutcomescanresultinahigh
false positive rate. Figure 4 shows one of such examples, where
bothHotSpotandOpenJ9threwan Out-of-Bounds exceptionwith
thesamelanguagesemanticsbutdifferentmessages(highlighted
with a dark background). If we were to compare the contents di-
rectly,thiswouldmistakenlybecategorizedastwodistincttypesof
differential behavior. We propose using a key information extractor
to minimize false positives. The extractor first eliminates compiler-
specificimplementations,suchasthelocationorvariable-related
information from the stack trace generated by the target compilers.
It then extracts the essential information, such as the exception
typeandaffectedAPIs(highlightedinboldredfontinFigure4),and
storestheminanunorderedlistforeachcompileroutput.Listswith
the same elements indicate the same anomalous compiler behavior.
Inaddition,theextractedkeyinformationisalsousedtofilter
outduplicatemis-compilations.Specifically,weextendedthetree-
basedclassifierproposedbyComfort[ 86]tobuildourfilter.Unlike
Comfort,whichconsistsofthreedecisionlayers,ouraugmented
filteraddsanewlayeratthesecondlayerofComfort.Thedecision
nodes in the new layer correspond to standard exit codes (also
known as return codes) that are returned by the operating system.
3.4 Mutation forFocusedTesting
Test case mutation is a powerful way to improve code coverage.
Priormutationalapproaches[ 50,88]randomlychoosepre-designed
operators to mutate the interesting test cases, incurring extraor-
dinarily costly and time-consuming testing. This is because the
random mutants fail to focus on testing a specific compiler compo-
nentinsuccessivefuzzing.Todoso,wedesignedseveralbug-guided
mutators to generate new bug-exposing test cases by mutating the
interesting test casethat has triggeredthe anomalousbehaviors.ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA G.Ye, T. Hu,Z.Tang,Z.Fan, S.Tan, B.Zhang,W. Qian, andZ.Wang
Algorithm1 MutatorSchedulingPolicy
Input:
ğ‘¡ğ‘ğ‘Ÿğ‘œğ‘” //Atest programneeded to bemutated
ğ‘‚ğ‘€ //Thecollectionofbug-guided mutators
ğºğ‘€ //Thecollectionofgeneral-purpose mutators
Output: ğ‘ƒğ‘›ğ‘’ğ‘¤ //Alist thatstores the mutated test programs
1:Letğ‘‚ğ‘€â†{Å‚ğ‘†ğ¼ğ‘€â€,Å‚ğ‘‰ğ‘ˆğ¿â€,Å‚ğ¼ğ‘ğ‘†ğ¿â€,Å‚ğ‘†ğ‘ğ¼ğ‘ƒâ€,Å‚ğµğ‘‚ğ‘ˆğ‘â€};
2:Letğºğ‘€â†{Å‚ğ‘…ğ¸ğ‘ƒğ‘‚â€,Å‚ğºğ¸ğ‘ğ‘ƒâ€,Å‚ğ¶ğ‘‚ğ‘ğ¹â€,Å‚ğ¼ğ‘ğ‘†ğ¶â€,Å‚ğ·ğ¸ğ¿â€};
3:Letğ‘ƒğ‘›ğ‘’ğ‘¤,ğ‘€bethe empty lists
4:ğ‘¡ğ‘ğ‘ ğ‘¡â†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘–ğ‘›ğ‘”ğ‘‡ğ‘œğ´ğ‘†ğ‘‡(ğ‘¡ğ‘ğ‘Ÿğ‘œğ‘”);
5:ifğ‘¡ğ‘ğ‘Ÿğ‘œğ‘”is an interesting test case then
6:ğ‘€â†ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘“ğ‘¦ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  (ğ‘¡ğ‘ğ‘ ğ‘¡,ğ‘‚ğ‘€);
7:else
8:ğ‘€â†ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘“ğ‘¦ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  (ğ‘¡ğ‘ğ‘ ğ‘¡,ğºğ‘€);
9:end if
10:ğ‘šâ†ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘  (ğ‘€);
11:ğ‘¡ğ‘›ğ‘’ğ‘¤â†ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¡ğ‘ğ‘Ÿğ‘œğ‘”,ğ‘š);
12:ğ‘ƒğ‘›ğ‘’ğ‘¤.ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘(ğ‘¡ğ‘›ğ‘’ğ‘¤);
13:returnğ‘ƒğ‘›ğ‘’ğ‘¤;
Mutationoperators. Wedesignedtenkindsofmutationoperators,
includingfivebug-guidedandfivegeneral-purposemutators.All
ourmutatorscanbefoundat[ 7].Toobtainasetofbug-guidedmu-
tatorsforfindingresidualbugs,werefertotheexistingliteratureon
frequently-occurringbugs[ 12,50,73].Thisleadstofivebug-guided
mutatorsrepresentingfivecommonclassesofbugs(i.e.,APImisuse,
security, performance, incomplete bug fixes, and missing boundary
check).Thebug-guidedmutatorsaimtoproducenewbug-exposing
test programs based on interesting test cases for focused testing,
whereas the general-purposemutators are applied to improve the
diversity of the mutant programs to avoid convergence during the
testingprocess.We describe bug-guidedmutatorsbelow:
â€¢Similar API Replacement (SIM): Replace an original API call
with one that has similar semantics or the same return values.
This mutatorisinspiredbyprior work onAPI misuse [12].
â€¢Vulnerability Rules (VUL): Mutate the target test case with
vulnerabilityrulesmanually designedaccording to PoCs.
â€¢InsertLoopStatement(INSL): InsertLoopStatement(e.g., for,
while) into the target test program. This is motivated by a prior
empirical study onperformance issues[73].
â€¢Snippet Replacement (SNIP): Replace a basic code block with
astructurally-similarone.Thisisinspiredbypriorwork[ 50]that
observedthatmorethan95%ofcodefragmentsoverlapbetween
the historicaltest programs dueto incompletebugfixes.
â€¢BoundaryValues(BOUN): Generateboundaryvalues(e.g., 0,
OXFF,NULL) for arguments passedto the function calls.
Given an interesting test case, mutator SIMis responsible for
replacing an existing API with a new one with similar functions
or the same types of return values. For example, the Javafunc-
tionlastIndexOf(String str) will be replaced with the simi-
lar method lastIndexOf(String str, Int fromIndex) . Such
mutation is able to reach deeper code branches of the method
lastIndexOf() and continuously test the String Class ofJVM.
FortheJSlanguage,therearealsomanysimilar-semanticfunction
calls,suchas String() v.s.toString() .SimilarAPIsareautomati-
callycollectedusingascripttoparsethelanguagespecificationdoc-
ument.VULaimsat mutatingthetest casebyusing pre-designedvulnerability patterns. Specifically, we implemented three patterns
thatcoverthreetypesofvulnerabilities,includingCWE-1321,CWE-
915,andCWE-843.Wechosethembecausetheyarethetop-3most
severevulnerabilities(wecounttheseveritybycalculatingthenum-
berofrelevantCVEslabeledasHIGHorCRITICALtothetotalnum-
berofCVEs).Thefirstpatternisaboutprototypepollutionvulnera-
bility,whichisachievedbymodifyingtheprototypechainattributes
through__proto__ orObject.setPrototypeOf and declaring a
newobjectaccordingly.Thesecondpatternisrelatedtotheremote
code execution vulnerability, which applies the getter/setter or
__defineGetter__() and__defineSetter__() to modify the at-
tributesofthetargetobjects.Thethirdpatternabouttypeconfusion
vulnerabilityreplacesafunctioncallwithmultiplecalls,meanwhile
changingtheobjecttype.Thetestcasesthatconformtoanyoneof
the vulnerability rules will be mutated. We would like to note that
theabovetwomutators SIMandVULarelanguage-specific,and
they need to mutate the interesting test cases according to the pro-
gram context of a specific programminglanguage. This,we think,
is inevitable due to the nature of different programming languages.
INSLoperator inserts the loop statement, e.g., fororwhile,
into the test case to enrich the control dependencies of the mutant
program. This operator creates a hot code region to activate the
just-in-timecompilationmoduleoftestedcompilersforexploring
the performance issue. The SNIPoperator replaces a basic code
block in the original test program with a similar one. To do so, we
firstextractthecodeblocksfromgatheredprograms,andeachcode
blockisacompletefragment(e.g., ifstatements)thatisrecorded
with ablock assembly constraint , which is represented as a tu-
ple:<pre-constraint,post-constraint> .Differfrompre/post-
condition in Hoare logic [ 42], thepre-constraint marks its re-
quired variables or statements, and the post-constraint labels
the return values that are required to be defined to execute the
code block without a runtime error. The collected code blocks with
theirblock assembly constraint are stored in a JSON file. The SNIP
operatorwillfirstselectthecodeblockswithexpected blockassem-
bly constraint from the JSON file and then randomly selects a code
block to replace the original one. This aims to cover deep branches
for tested components. The last operator BOUNis to generate the
boundary values for the test case. For the Java test program, we
defined 23 boundary values such as 0,1,-1,NaN,NULL,0xFFF, and
Undefined , etc., which are from the historical test cases that ex-
posedcompilerdefectsorvulnerabilities.Forthe JSprogram,we
utilize Comfort [ 86] to generate the boundary values according to
the ECMA-262specification.This operationcancontinuously test
an API andcover its deeper code branches.
To increase the diversity of the test cases, we also designed five
general-purposemutationoperators.Theyaredescribedasfollows:
â€¢Replace Operator (REPO): Randomly replace a binary or a
unaryoperatorwithanothercorrespondingone.e.g.,replaceÅ‚--Å¾
withÅ‚++Å¾,orreplace Å‚+Å¾ withÅ‚-Å¾.
â€¢Generate Parameters (GENP): Randomly generate parameters
withprimary andreference types.
â€¢Change Control Flow (CONF): Change the control flow by
replacing aconditional statement,e.g.,replace ifwithswitch.
â€¢InsertConditionalStatement(INSC): Insertconditionalstate-
ments(e.g., if...else ) intothe originaltest case.A Generative andMutationalApproachforSynthesizingBug-exposing TestCases to Guide CompilerFuzzing ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA
Table 1:Target compilerswe have tested.
Category Compilers Versions BuildNo. ReleaseDate
V8 v9.9.1 8a05d7a Dec.2021
v1.11.24 a75335b Dec.2020ChakraCorev1.13.0.0-beta 418a27c Jun.2022
SpiderMonkey C96.0 fd8da16 Jan. 2022
JavaScriptCore v286936 - Dec.2021
GraalJS v21.3.0 ede7e2b Oct. 2021
JerryScript v3.0.0 6fe763f1 Sep.2022
Hermes v0.10.0 7d3e091 Aug. 2022JS
QuickJS v2021-03-27 b5e6289 Mar. 2021
v8u332-b02 1e4e4ae Feb. 2022OpenJ9v11.0.15+1 c902226 Feb. 2022
V8.0.332 e1f6c13 Feb. 2022HotSpotV11.0.14 b8cdf1a Jan. 2022JVM
GraalVM v22.0.0.2 bd6570e Jan. 2022
Total 11 14 14 -
â€¢DeleteCodeSnippet(DEL): Randomlydeleteabasiccodeblock
from the originaltest case.
Thegeneral-purposemutatorschangethedataandcontrolde-
pendencies that significantly deviate from the original programs
to guide towards testing more uncovered compiler components.
Specifically,mutators REPOandGENPchangethedatadependen-
cies while CONFandINSCalter the control dependencies. The
mutatorDELcan changeboth the data andcontroldependencies.
Mutatorscheduling policy. Algorithm1 presentsour mutation
scheduler.Theschedulertakesinthemutatorsandthetestprogram
required to be mutated, producing a list of new mutated test cases.
Givenatestprogram ğ‘¡ğ‘ğ‘Ÿğ‘œğ‘”,ourschedulerfirstdeterminesifitisan
interesting test program. Here the interesting test programs refer
to those that have ever triggered anomalous compiler behaviors or
discoveredthenewbranchesofthetestedcompilers.Ourinsight
isusing both codecoverageandanomalous compiler behaviorsas
guidance can help to discover more new code branches. If ğ‘¡ğ‘ğ‘Ÿğ‘œğ‘”is
aninteresting test case, the scheduler then identifies which bug-
guided mutators are suitable for mutating (lines 5Å›6); otherwise,
theappositegeneral-purposemutatorswillbechosen(lines7Å›8).
Todetermine ğ‘€,wefirstparse ğ‘¡ğ‘ğ‘Ÿğ‘œğ‘”intoanAST(line4)andsearch
thepotentialmutationpositionsbytraversingtheparsedAST.A
mutation position essentially refers to an AST node whose context
programconformsto the patternofanyofourtenmutators.For
example, the test2function in Figure 1 expects two parameters of
typeinteger, which can be generated by the bug-guided mutator
BOUN. Note that the mutator determination process may produce
multiplemutators. The schedulerwillrandomlychoosenomore
thanğ‘€.ğ‘€ğ´ğ‘‹mutatorstogeneratenewtestprograms(lines10Å›12).
4 EXPERIMENTALSETUP
Target Compilers. We apply ComFuzz to testJSandJVMcom-
pilers. Table 1 lists the tested compilers and the versions used.
Specifically, we apply ComFuzz to 8JSand 3JVMcompilers us-
ing their latest trunk branches. In total, we have tested 14 target
compiler-versionconfigurations.Table 2:Statistics forexposed bugspertargetcompiler.
Compiler #Submitted #Confirmed #Fixedof #Conf.
ChakraCore 5 4 4
SpiderMonkey 2 1 1
GraalJS 3 3 3
JerryScript 4 3 1
Hermes 3 3 1
QuickJS 1 0 0
OpenJ9 12 12 10
HotSpot 2 2 1
GraalVM 1 1 1
Total 33 29 22
CompetitiveBaselines. Wechooseeightpriormethods,covering
bothgeneration-andmutation-based fuzzersfor compilertesting.
Specifically,wecompare ComFuzz withfourfinegenerativefuzzers:
Comfort[ 86]andPolyGlot[ 27]fortheJSengine;JavaTailor[ 91]
andJAttack[ 89],thelatesttwotestprogramsynthesizersfor JVM
testing.Wealsocompare ComFuzz withfourmutationalfuzzers:
CodeAlchemist [ 40], DIE [65], and Montage [ 50] forJSengines as
they represent the SOTA methods;andClassming [25] for JVM.
Implementation and Evaluation Platforms. Our program gen-
erator is built upon a Transformer architecture [ 71] in PyTorch
v1.11.0. Our mutators are implemented in JSandJava. The differ-
ential testing engine is written in Python. Our evaluation platform
isamulti-coreserverwitha3.6GHz8-core(16threads)IntelCorei7
CPU, four NVIDIA GTX 3080Ti GPUs, and 64GB of RAM, running
Ubuntu 18.04 operating system with Linux kernel 4.15. All DNN
models run onthe native hardware using GPUs.
5 EXPERIMENTALRESULTS
5.1 Bug Summary
Thissubsectionexhibitsthenumberofidentifiedbugsandpresents
their various summary statistics for the purpose of evaluating the
ability of ComFuzz to discover previously unknown bugs. The
experiment started with testing JSengines first in November 2021
and then extending our testing framework to JVMin April 2022.
The total testing time is about 260 hours on approximately 400k
test cases for JSand 200k test cases for Javathat are generated
from around 20khistoricaltest programs.
NumberofBugs. Table2givesthedistributionofthe ComFuzz -
exposedbugsaccordingtothetestedcompilers. ComFuzz discov-
ered bugsinall the tested compilers exceptfor V8 andJavaScript-
Core. Among the confirmed bugs, we found a total of three unique
bugsexposedbythesametestcases.Listing3showsoneofthese
bugs. This implies that bugs are prevalent in different compilers.
Overall, as of February 2023, we have reported 33 unique bugs. To
date, 29 bugs have been confirmed, of which 22 have been fixed
by the developers. For the remaining four reported JSbugs, two
bugs were rejected due to the special design of the compiler; the
other two bugs are waiting to be verified. In addition to the above
fourbugs,threemorebugsweresilentlyfixedinthebetaversion
of the tested compilers after submitting our bug reports. In total,
ComFuzz exposed26outof33bugsthatwerepreviouslyunknown.ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA G.Ye, T. Hu,Z.Tang,Z.Fan, S.Tan, B.Zhang,W. Qian, andZ.Wang
5 
3 8 
4 6 10 
5 5 10 
JS JVM Total0 2 4 6 8 10 #Confirmed Bugs 
Tested Compilers 
 Parser 
  Optimizer 
  Backend 
Figure 5:#Confirmed bugspercompiler component.
It is worth mentioning that for OpenJ9, ComFuzz found 12 bugs,
farmorethanthenumberofbugsexposedinothercompilers.This
is mainlybecauseOpenJ9 introduces many optimizationschemes
thatarepronetodefectsduetoincorrectimplementation.Herefive
such bugswere foundby ComFuzz inthe optimizerofOpenJ9.
Affected compiler components. As discussed in Section 3.3, a
compiler is composed of a parser, an optimizer, and a backend.
Each of the three components inevitably has defects due to wrong
implementations. To assess how ComFuzz performs in covering
thesethreecomponents,wegroupedthe ComFuzz -discoveredbugs
into three categories: Parser,Optimizer , andBackend, according
to the phase where the bug is caused. Figure 5 gives the number
of confirmed bugs discovered by ComFuzz for each component.
NotethattheOpenJ9documentbug(seeListing4)isexcludedfrom
Figure 5. For JSengines,ComFuzz exposed around 4Å›5 bugs in
the three components, indicating that bugs are prevalent in dif-
ferent components of a compiler. For JVM, the most error-prone
componentis Optimizer ,whichhasexposed6bugs,followedby
theBackend andParser.Overall,bugsin Optimizer areprevalent
- 6JVMand 4JSbugs belong to this group. According to the devel-
opersâ€™feedback,this isoftendue toerroneousimplementationof
the optimization schemes. This is in line with the current research
trend that mainstream compiler vendors are striving to enhance
the optimization level anddepth.
5.2 Ablation Study
Recallthat ComFuzz consistsofthreecomponents:(1)ageneration
model that leveragesthe historical test programs(see Section 3.2);
(2) the bug-guided or (3) general-purpose mutators that mutate in-
terestingtestcases(seeSection3.4).Toillustratehowtheyperform
inbug-exposingcapability,weevaluatetheireffectsin ComFuzz
with an ablation study. In ComFuzz-M andComFuzz-A , we re-
movedthemutationandgenerationpartandkeptothermodules
unchanged, respectively. Likewise, in ComFuzz-G andComFuzz-P ,
we respectively remove the general-purpose mutators and bug-
guidedmutatorsandkeepothercomponents.Wecompared Com-
Fuzzwith all four variants with a test time budget of 48 hours. All
thevariantsareevaluatedusingthesameseedprogramstoavoid
the test biascausedbythe randomnessofthe test casegenerator.
Figure6reportsthenumberofbugsdiscoveredbyeachimple-
mentation variant. ComFuzz-M discovered four bugs for JSand
fivebugsfor JVM,respectively.Thisconfirmsthatourgeneration
modeliseffectiveingeneratingbug-exposingtestcases.Further-
more,ComFuzz-A respectively exposed five and six bugs for JS
andJVM,indicatingtheeffectivenessofourmutationstrategyalone.
By augmenting the generation model with bug-guided mutators,
ComFuzz-G improves ComFuzz-M by exposing four more bugs.
Likewise,comparing ComFuzz-P withComFuzz-M ,wecanseethat
withgeneral-purposemutators, ComFuzz-P discoveredtwomore
9 10 19 
4 5 9 
6 7 13 
6 5 11 
5 6 11 
JS JVM Total 0 5 10 15 20 #Comfirmed Bugs 
Tested Compilers 
 COMFUZZ 
  COMFUZZ-M 
  COMFUZZ-G 
 COMFUZZ-P 
  COMFUZZ-A Figure 6:#Confirmed bugsfoundby ComFuzz variants.
bugs, suggesting a better bug-exposing capability. This indicates
the usefulness of our bug-guided and general-purpose mutators in
augmentingourgenerationmodelforexposingcompilerbugs.The
ablationstudy alsoshowsthat comparedtoitsvariants, ComFuzz
achievesthebestperformancebygivingatleast 1.5Ã—improvements
in bug detection. This indicates the effectiveness of ComFuzz in
combining the generative andmutational techniques.
5.3 Bug Examples
ComFuzz is capable of finding diverse kinds of bugs on tested
compilersaccordingtothehistoricaltestprogramsandbug-guided
mutators.Toprovideaconvincingglimpseofthediversityofthe
exposed bugs, we give four ComFuzz -generated test cases that
expose the JSandJVMcompilerbugs.
1public class JVMTest {
2public static void main( String [] args ) throws
Exception {
3 StringBuilder sb = newStringBuilder () ;
4 sb . append( 'J') ;
5 sb . deleteCharAt (1) ;
6 System . out . println ( sb . toString () ) ; } }
Listing 1: OpenJ9 for JDK 8 fails to throw an OutOfBound
exceptionforthistestcode.
OpenJ9Parserbug. Thebug-exposingtestcodeinListing1isa
program that throws an OutOfBounds exception because it deletes
anelementthatdoesnotexistatline5.Whenexecutingthistest
code,OpenJ9forJDK8failstothrowanexception.Therootcauseis
thatOpenJ9incorrectlyreturnstheboundaryvaluewhencallingthe
innerfunction StringBuilder delete (int start, int end) .
Thisbug-exposingtestcodeismutatedviaourbug-guidedmutator
SIM(see Section 3.4) by replacing the delete() function with the
deleteCharAt() function. This bug was quickly confirmed and
addedto the repairlistfor the nextrelease version.
1public class JVMTest {
2 static void foo () {
3 Integer i = 100;
4 do{
5 return;
6 } while( i++ < 10) ; }
7 public static void main( String [] args ) {
8 foo () ; }}
Listing 2: HotSpot for JDK 8 throws an AssertionError
exceptionwhile thecodeis correct.
HotSpot Backend bug. The tested HotSpot for JDK 8 threw an
AssertionErrorexception whenexecutingthetest caseshownin
Listing 2.As thetest codeis syntacticallycorrect, HotSpotshould
compile the code successfully and transform the test code into a
bytecode.TherootcauseofthisbugisthattheHotSpotbackend
incorrectlymapsflat do...while loopstatementsintothebytecode.A Generative andMutationalApproachforSynthesizingBug-exposing TestCases to Guide CompilerFuzzing ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA
ComFuzz producesthisbug-exposingtestprogrambyapplyingthe
bug-guidedmutator INSLtoinsertthe do...while loopstatement
into the body of the foofunction at lines 4-6. The bug has been
confirmedandassignedfor repair.
1varfoo =function ( t1 ) {
2 t1 . sort ( function (a , b) {
3 return aâˆ’b; }) ;
4 print ( t1 ) ; }
5varParameter1 = [ 'a','b'];
6 foo ( Parameter1 ) ;
Listing 3:Ittriggers an optimizer bugofSpiderMonkey.
SpiderMonkey Optimizer bug. This test program in Listing 3
containsa sortfunctionthatinvokesaninlinecomparisonfunction
(atline2),whichissynthesizedviareplacingtheoriginalbodyof
thefoofunction with sortfunction call1by using our bug-guided
mutatorSNIP(Section3.4).Thecorrectoutputsofthistestprogram
shouldbeÅ‚a,bÅ¾becausethe JSspecification,ECMA-262statesthat
thesortfunctionshouldreturntheoriginalarray t1(HereisÅ‚a,
bÅ¾)whenthevalueofthestatement a-bintheinlinecomparison
function equals NAN. While SpiderMonkey yields Å‚b, aÅ¾, the wrong
results. The root cause is that SpiderMonkey misused a specific
optimizationschemeforthecomparisonfunctioninsteadofactually
callingthecomparatorfunction,leadingtoanincorrectresult.This
bug is immediately verified and classified as P1priority - the most
urgent level that should be fixed soon, as Bugzilla states. Moreover,
asimilar test casealsoexposedaconfirmedbugofJerryScript.
1public class JVMTest {
2boolean testLatin1 () {
3 try{
4 StringBuilder sb = newStringBuilder () ;
5 System . out . println ( sb . capacity () ) ;
6 sb . ensureCapacity ( Integer .MAX_VALUE/2+1) ;
7 } catch(OutOfMemoryError oom) {
8 oom. printStackTrace () ; }
9 return true ; }
10public static void main( String [] args ) throws
Exception {
11 newJVMTest () . testLatin1 () ; }}
Listing 4: The test program that revealed the incomplete
documentation probleminOpenJ9.
IncompleteOpenJ9documentation. TheComFuzz -generated
testprogrampresentedinListing4triggeredananomalousbehavior
ofOpenJ9forJDK11(i.e.,catchingan OOMexception).Suchbehavior
is expected because the -XX:+CompactStrings option responsible
for causing this exception is disabled by default in OpenJ9 for JDK
11, while it is enabled in HotSopt and OpenJ9 for JDK 8. During
the manual analysis, we found that the OpenJ9 documentation had
nodescription forthisoption.Wehave reportedthisdefecttothe
developer,anditwasfixedquicklyafter reporting.
5.4 EvaluationofDifferential Testing
Toquantifytheroleofourdifferentialtestingmodule,wecountthe
numberofconfirmedbugsexposedby ComFuzz anddividethem
intoeither crash(bugsthatleadtoaruntimecrash)or inconsistency
1Thesortfunction is originated from the link [9].Table3:Thenumberof ComFuzz -uncoveredbugsfoundby
execution crashesordifferentialtesting.
Compilers #Crash #Inconsistency Ratio of Inconsis.
JS 3 15 83.3%
JVM 2 13 86.7%
Total 5 28 84.8%
24 48 72 96 120 144 168 0 10 20 30 40 Percentage (%) 
Testing Time (hours) 
 FPR  
  FNR 
(a) JS (b) JVM 24 48 72 96 120 144 168 0 10 20 30 40 Percentage (%) 
Testing Time (hours) 
 FPR  
  FNR 
Figure7:Howthefalsenegativerate(FNR)andfalsepositive
rate (FPR)change as we increase the testingtime.
(bugsdiscoveredbydifferentialtesting).AsshowninTable3,ap-
proximately 85% of the bugs were uncovered due to inconsistency,
showing the importanceof employingdifferential testing.
Recallthatourdifferentialtestingmethodologyincorporatesa
filteringmechanism designedto duplicate mis-compilation behav-
iors.Weconsidertwometricstoassessthefilterâ€™seffectiveness:the
falsepositiverate(FPR)andthefalsenegativerate(FNR).Inthis
context, a false positive refers to the number of cases mistakenly
classifiedasbugs,whilea falsenegative representsthenumberof
actual inconsistentresultsthat were erroneouslyfiltered.Figure 7
showshowFNRandFPRchangethroughoutthetestingprocess.We
observe that throughout the entire testing period, the FPR remains
consistentlylow(below10%).Asthetestingprogresses,weseea
gradualdecreaseintheFNRofourfilteringmechanism,showingits
increasing efficiencyin accurately identifying inconsistent results.
5.5 Compareto PriorCompiler Fuzzers
Weusethefollowingmetricstocompare ComFuzz againsteight
baselines[25,27, 40, 50, 65, 86, 89, 91]introducedinSection 4:
Bug exposing capability. This metric quantifies the number of
anomalous compiler behaviors. Note that we have checked and
removed all the duplicate anomalous behaviors; hence, each of
themindicatesapotentialcompilerbugthatneedstobeverified
by developers. For a fair comparison, we tested each fuzzer for
24 hours of consecutive testing runs using the ComFuzz â€™s seed
programs andseedprograms from the baselines,respectively.
Syntaxpassingrate. Itmeasurestheratioofthegeneratedtest
cases that are syntactically correct. For each fuzzer, we leverage
50k-generatedtest casesto compute the syntax passingrate.
Code coverage. We use three widely used coverage criteria: state-
ment coverage, function coverage, and branch coverage for the
comparison.To collectthe coverageinformation, we useGcov[ 3]
and Lcov [ 5] forJVM, and llvm-cov [ 6] forJSengine, the code
profiling toolsfor instrumentingC code in JSandJVMcompilers.
Throughput. Following the practices in [ 13,92], we compute
the fuzzing throughput by measuring the number of test cases
processed per minute. This is computed by applying each fuzzer to
the same 10ktest casesusing their defaultsettings.ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA G.Ye, T. Hu,Z.Tang,Z.Fan, S.Tan, B.Zhang,W. Qian, andZ.Wang
16 
8 
2 5 6 6 
13 
0 8 
0 
JS 0 5 10 15 20 #Anomalous behaviors 
Tested Compilers 
 COMFUZZ  
  Comfort 
 Montage       
  CodeAlChemist 
 DIE               
  PolyGlot 
JVM 0 5 10 15 20 
#Anomalous behaviors 
 COMFUZZ    
  Classming 
 JavaTailor      
  JAttack 
(a)w/ComFuzz â€™sseed programs
7 
5 
4 4 
2 
0 
7 
1 2 
0 
JS 0 5 10 #Anomalous behaviors 
Tested Compilers 
 COMFUZZ  
  Comfort 
 Montage       
  CodeAlChemist 
 DIE               
  PolyGlot 
JVM 0 5 10 
#Anomalous behaviors 
 COMFUZZ    
  Classming 
 JavaTailor      
  JAttack (b)w/baselinesâ€™ seed programs
Figure 8:Anomalousbehaviorsfoundby differentfuzzers.
JS 0% 20% 40% 60% 80% 100% Percentage 
Tested Compilers 
 COMFUZZ        
  Comfort  
  Montage 
 CodeAlChemist 
  DIE         
  PolyGlot 
JVM 0% 20% 40% 60% 80% 100% 
Percentage 
 COMFUZZ    
  Classming 
 JavaTailor      
  JAttack 
Figure 9:Comparisonresults ofsyntaxpassingrate.
5.5.1 Bug-exposingcapability. Figure8showsthat ComFuzz ex-
posed more unique anomalous behaviors than other individual
fuzzers, either using ComFuzz â€™s or baselinesâ€™ seed programs. With
ComFuzz â€™sseedprograms, ComFuzz discovered16anomalousbe-
haviorsfortarget JSengines,achievinganaverageimprovementof
270%thanthenumberofanomalousbehaviorsdiscoveredbyother
fuzzers.For JVM,ComFuzz foundatotalof13anomalousbehaviors,
1.5Ã—over the number of anomalous behaviors found by JavaTailor.
Amongall29anomalousbehaviorsdiscoveredby ComFuzz ,15were
foundbythetestcasesgeneratedfromhistoricaltestprograms,and
6werediscoveredbyourbug-guidedmutators.Likewise, ComFuzz
exposed a total of 14 anomalous behaviors with baselinesâ€™ seed
programs, alsoachievinga 1.5Ã—more thanthat ofotherbaselines.
This demonstrates ComFuzz â€™sbug-exposing capability.
5.5.2 Syntaxpassingrate. Figure9showshowmanyautomatically
generatedtestprogramscanpassthesyntaxchecks. ComFuzz gives
an average passing rate of 82%, achieving a 10% improvement over
mostalternativemethods.Amongthesyntacticallyincorrecttest
cases generatedby ComFuzz , nearly90% of themwere created by
general-purposemutators,whichareerror-proneastheyrandomly
mutate the test cases without any syntax guidance. In contrast,
JAttack and JavaTailor applied well-designed grammatical rules to
synthesize test cases, reaching higher passing rates of 100% and
98.4%,respectively.However,thegrammaticalruleslimittheirbug-
exposingabilities.AsweshowinSection5.5.1, ComFuzz discovered
at least1.5Ã—more anomalousbehaviors thanany ofthe baselines.
5.5.3 Code coverage. Figure 10 presents the comparison results of
codecoverage,where ComFuzz givesthebeststatementandbranch
coveragecomparedtoallevaluatedfuzzers.Theresultsdemonstrate
thatusinghistoricaltestprogramsforgeneratingtestcasesismore
helpful in covering deeper code of the tested compiler. For the
JSengine,MontageandCodeAlChemistachievehigherfunction
coverage than ComFuzz . The reason is that their seed programs
covermore JSfunctions,buttheygivealowerstatementandbranch
coverage than ComFuzz due to the low syntax passing rate of their
generated tests. This also illustrates Montage and CodeAlChemist
have lower bug-exposing capabilities than ComFuzz .
5.5.4 Throughput. Table4 compares the fuzzing throughput com-
puted as the number of test cases processed per minute. We ran
Statement Cover. Function Cover. Branch Cover. 20% 30% 40% 50% Percentage 
JS 
 COMFUZZ         
  Comfort     
  Montage 
 CodeAlChemist  
  DIE            
  PolyGlot 
Statement Cover. Function Cover. Branch Cover. 20% 30% 40% 50% 
Percentage 
JVM 
 COMFUZZ    
  Classming 
 JavaTailor      
  JAttack Figure 10:Compared results ofcodecoverage.
all fuzzers with the same 10k test cases and then calculated the
throughput.Comparedtootherfuzzers,Classmingtakesthelongest
timetofuzzatestcase.Thegenerationtimeaccountsforamuch
largerpartofthis,asClassmingneedstogetthescopeofthelive
codebyexecutingtestcases(bothoriginandmutateprograms).As
forComFuzz ,itgeneratestestcaseswithlargeloopswhenusing
INSLmutator, leading to a long run time in the total test and a
lower throughput than other baselines. Nonetheless, the fuzzing
throughputof ComFuzz iscomparable to otherfuzzers.
6 DISCUSSIONSAND THREATS TO VALIDITY
Our work focuses on the context of compiler testing by combining
historicaltest casesandbug-guidedmutationrules. ComFuzz pro-
videsafocusedandefficientcompilerfuzzingframework,achieving
a higher bug-exposing capability than SOTA solutions. We em-
phasizethat ComFuzz isnotdesignedtoreplaceexistingfuzzers.
Instead, we aim to generate bug-exposing test cases for quickly
discovering buggy compiler behaviors. Hence, we employ a DL-
based model to learn a test program generator from historical test
cases. Unlike JAttack, our program generator cannot guarantee the
correctsyntaxofallsynthesizedtestcasesduetotheusageofprob-
abilistic predictionmechanisms during the sampling process. Still,
the efficiency in generating syntactically valid programs would
remainlargelyunchangedcomparedtoJAttack.Inthefuture,we
will try to employ more powerful neural networks with a larger
numberoftrainingsamples.Unlikeexistingfuzzers,ourworkdoes
notpursuefullcodecoverageforthetestedcompiler.Incontrast,
wefocusoncoveringthebuggycompilercomponentsviaasetof
carefullydesignedbug-guidedmutators.
Threats. Ourexperimentsmaynotgeneralizebeyondtheevaluated
fuzzers and languages beyond JavaandJS. We mitigate this by
evaluatingeightSOTAs.Portingourtechniquetoanewprogram
languagewouldrequiretheDL-basedtestprogramgeneratoron
new historical test cases collected for the targeting language, re-
designsomeofthemutationrulesandkeyinformationextractorfor
differentialtestingbutthemodeltrainingcanbelargelyautomated.
7 RELATED WORK
Generativefuzzers. Generation-basedtestingoftenutilizessto-
chastic grammar rules [ 14,15,33,35,49,58,59] or generation tem-
plates[20,21,45]tosynthesizetests.Therepresentativemethods
areEdSketch[ 45]andjsfunfuzz[ 72].EdSketchisanopen-source
template-based JVMfuzzerthatuseshand-writtengenerationtem-
platestosynthesize Javaprograms.Similarly,jsfunfuzzemploys
pre-definedcontext-freegrammarstogenerate JStestcases.Sub-
sequent studies have proposed increasingly complex grammar and
templates to improve the syntactic or semantic passing rates of
generated test programs [ 62,63,76]. JAttack [ 89] provides cus-
tomized generation templates where developers can encapsulateA Generative andMutationalApproachforSynthesizingBug-exposing TestCases to Guide CompilerFuzzing ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA
Table 4:Fuzzingthroughput( #test cases/minute ).
Fuzzers ( JS) Throughput Fuzzers ( JVM) Throughput
ComFuzz 15.58ComFuzz 17.00
Comfort 9.58 Classming 2.19
Montage 24.39JavaTailor 18.07
CodeAlchemist 29.85 JAttack 20.91
DIE 19.35
POLYGLOT 10.45
the expected code features for JVMtesting. SPE [ 90] introduces a
syntactictemplatethatconsistsofaskeletalprogramandvariables
set. It generates random equivalent C programs by enumerating
the combinations of the skeletal program and the variables. CL-
Smith [53], the extended method of CSmith [ 85], added multiple
generation options to generate OpenCL kernels for covering more
compiler features. However, these approaches pursue full coverage
oftargetcompilers,leadingtoaninefficientbug-revealingability.
By contrast, ComFuzz is devoted to quickly exposing the buggy
compiler component by generating bug-revealing test cases based
onhistoricaltest programs. We are the firstto do so.
Mutational fuzzers. Mutational testing aims to improve the code
coveragefortargetcompilers,whichisachievedbyreassembling
or modifying a set of seed programs [ 22,23,44,80,84]. EMI [48]
and its subsequent works [ 28,54,56] are among the representa-
tive mutation-based fuzzers. They generate semantic-equivalent
test cases by performing equivalent mutations. LangFuzz [ 44] uses
and recombines code fragments that previously exposed bugs to
generate random JSprograms. SYMFUZZ [ 22] mutates parent test
programsbasedontheoptimalmutationratiothatisdetermined
bywhite-boxsymbolicanalysis.IFuzzer[ 81]utilizesgeneticpro-
gramming techniques to generate unusual input code fragments
fortesting JSengines.CodeAlChemist[ 40]anditsimprovement
workDIE[ 65]breaksthehistorical JSPoCsintocodesegmentsand
reassemblesthesesegmentsintonewtestprograms.Classming[ 25]
mutates the parent test cases by introducing a live bytecode mu-
tation technique for JVMtesting. Differ from the aforementioned
fuzzers,ComFuzz utilizes the bug-guided mutators to generate
newtest cases bymutatingtheparent bug-revealing testprograms,
whichcan coverthedeepercodebranchesandimplementhighly-
intensivetestingforthebuggycompilercomponent.Thismutation
insightcould be valuable for mutation-guidedcompilertesting.
Guidedcompilertesting. Sincerandomtestcasegenerationmeth-
odsforcompilertestingareblindandtime-consuming,recentstud-
ies have proposed a set of guided fuzzers. AFL [ 88] is the first
coverage-guidedtestingframework,anditemployscompile-time
instrumentation and genetic algorithms to assist in generating ran-
domtestcasesforcoveringmorecodebranches.Thesubsequent
works [17,38,51,60,66,69,84] further improve code coverage
for domain-specific testing by mutating the seed programs. Poloto
etal. [67] proposed aninterpreter-guided unittesting solution on
the JIT compiler. It employs concolic testing to explore all possible
executionpathsandthecorrespondingvaluesofaninterpreterand
usestheseconcretevaluestoimplementdifferentialunittestingon
multiple JIT compilers. Classfuzz [ 26] is a coverage-guided method
onJVMcompilers, it employs MCMC sampling to guide mutator
selection.Confuzzion[ 18]introducesamutationalfeedback-guidedfuzzer on JVMfor exposing the type of confusion vulnerabilities. It
uses historical execution information to randomly select mutation
methods to generate new test cases. JavaTailor [ 91] is a closely
related work that produces randomly generated tests by mutat-
inghistoricaltestprograms.Thekeydifferenceisthat ComFuzz -
generated test cases are bug-directed that can perform focused and
highlyintensivetestingforabuggycompilercomponent,leading
to more effective testingthanotherguidedfuzzers.
DL-basedtesting. Toreducehumaninvolvement,deep-learning
models have been used to generate test cases. DeepSmith [ 29] and
Learn&fuzz [ 34] start with the recurrent neural network (RNN)
to generate test code, opening the DL-based testing trend. The
subsequentwork[ 50,57,86]exploreddifferentdeeplearningarchi-
tectures, e.g., LSTM [ 43], Seq2Seq [ 79], and GPT-2 [ 68], to improve
thesyntacticpassingrateofthegeneratedtestcodes.Inspiredby
existingmethods, ComFuzz also uses thedeep learning model for
test code generation, but it focuses on a directed generation by
feeding the neuralnetwork withhistoricaltest cases.
8 CONCLUSIONS
We have presented ComFuzz , a fuzzing framework for detecting
compiler bugs. ComFuzz leverages historical bug-exposing test
programs to generate test cases. This strategy increases the test
coverageofcompilercomponentsthatarelikelytocontainbugs.
Ratherthansolelydependingonpasttestcasesandapplyingran-
dom mutations across all compiler components, ComFuzz focuses
inonmodulespreviouslyknownforbugs.Suchmoduleshavebeen
historically prone to errors and canpotentially contain bugs intro-
ducedbyfixes.Auniquefeatureof ComFuzz isitsuseofbug-driven
mutatorstouncovertheseresidualbugs.Tofurtherenrichourtest-
ingdiversity,weincorporatedfivemultipurposemutatorsdesigned
toproducenewtestcaseswhenexistingonesfallshortinrevealing
bugsorenhancingcode coverage.
Weevaluate ComFuzz on11distinctcompilers,coveringboth JS
andJava.In260testinghours,itunveiled33distinctbugsinnineof
those compilers. Of these detected issues, 29 were verified, with 22
being rectified by the developers. Compared to eight prior fuzzers,
ComFuzz uncovers at least 1.5Ã—more bugsthanits counterparts.
DATA AVAILABLE
Thedata andcode associatedwith thispaper areopenly available
at https://github.com/NWU-NISL-Fuzzing/COMFUZZ.
ACKNOWLEDGEMENTS
WethanktheESEC/FSEanonymousreviewersfortheirconstruc-
tive feedback. We also thank all the JVMandJSdevelopers for
analyzing and replying to the bugs we reported. This work was
supportedinpartbytheNationalNaturalScienceFoundationof
China (NSFC) under grant agreements 62102315 and 61972314, the
China Postdoctoral Science Foundation Fellowship (2022M712575),
the Shaanxi International Science and Technology Cooperation
Program (2023-GHZD-04), the Shaanxi Province Å‚Engineers + Sci-
entistsÅ¾ Team Building Program (2023KXJ-055), and a CCF-Tencent
Open Fund. For the purpose of open access, the author has applied
a Creative Commons Attribution (CCBY) license to any Author
AcceptedManuscript versionarising from this submission.ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA G.Ye, T. Hu,Z.Tang,Z.Fan, S.Tan, B.Zhang,W. Qian, andZ.Wang
REFERENCES
[1] [n.d.]. https://jshint.com/Accessed onAugust 2023.
[2][n.d.]. Esprima:ECMAScriptparsinginfrastructureformultipurposeanalysis.
https://esprima.org/Accessed onAugust 2023.
[3][n.d.]. Gcov. https://gcc.gnu.org/onlinedocs/gcc/Gcov.html Accessed on August
2023.
[4][n.d.]. JavaCompiler. https://docs.oracle.com/javase/8/docs/api/javax/tools/
JavaCompiler.htmlAccessed onAugust 2023.
[5][n.d.]. Lcov. https://github.com/linux-test-project/lcov Accessed on August
2023.
[6][n.d.]. llvm-cov. https://www.llvm.org/docs/CommandGuide/llvm-cov.html
Accessed onAugust 2023.
[7][n.d.]. Mutators of COMFUZZ. https://github.com/NWU-NISL-Fuzzing/
COMFUZZ/blob/main/docs/ComFuzz-mutators.md Accessed onAugust 2023.
[8][n.d.]. OpenJDKTest7052494. https://github.com/openjdk/jdk7u/blob/master/
hotspot/test/compiler/7052494/Test7052494.java#L56Accessed onAugust 2023.
[9][n.d.]. The sort function. https://github.com/oowuyue/notebook/blob/
9efd633366c4d2721d76bb9ce7f95b89cc1260cb/js/js-gist/fortune500.js#L166Ac-
cessed onAugust 2023.
[10][n.d.]. Test262:ECMAScriptTestSuite(ECMATR/104). https://github.com/tc39/
test262 Accessed onAugust 2023.
[11]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learn-
ing to Represent Programs with Graphs. In International Conference on Learning
Representations .
[12]Sven Amann, Hoan Anh Nguyen, Sarah Nadi, Tien N Nguyen, and Mira Mezini.
2018. Asystematicevaluationofstaticapi-misusedetectors. IEEETransactions
onSoftwareEngineering 45,12(2018), 1170Å›1188.
[13]Anastasios Andronidis and Cristian Cadar. 2022. SnapFuzz: high-throughput
fuzzingofnetworkapplications.In Proceedingsofthe31stACMSIGSOFTInter-
national Symposium on Software Testing and Analysis (ISSTA) . ACM, 340Å›351.
https://doi.org/10.1145/3533767.3534376
[14]Osbert Bastani, Rahul Sharma, Alex Aiken, and Percy Liang. 2017. Synthesizing
programinputgrammars. Proceedingsofthe38thACMSIGPLANConferenceon
ProgrammingLanguageDesignandImplementation(PLDI) 52,6(2017),95Å›110.
https://doi.org/10.1145/3062341.3062349
[15]Pavol Bielik, Veselin Raychev, and Martin Vechev. 2016. PHOG: probabilistic
modelforcode.In ProceedingsoftheInternationalConferenceonMachineLearning
(ICML). 2933Å›2942.
[16]Marcel Boehme, CristianCadar,and AbhikRoychoudhury.2021. Fuzzing: Chal-
lengesand Reflections. IEEE Software 38,3 (2021), 79Å›86.
[17]Marcel BÃ¶hme, Van-Thuan Pham, and Abhik Roychoudhury. 2017. Coverage-
basedgreyboxfuzzingasmarkovchain. IEEETransactionsonSoftwareEngineering
45,5 (2017), 489Å›506. https://doi.org/10.1109/TSE.2017.2785841
[18]William Bonnaventure,AhmedKhanfir,AlexandreBartel,Mike Papadakis, and
Yves Le Traon. 2021. CONFUZZION: A Java Virtual Machine Fuzzer for Type
Confusion Vulnerabilities. In 2021 IEEE 21st International Conference on Software
Quality,Reliability and Security(QRS) . IEEE,586Å›597.
[19]Alexander Brauckmann, AndrÃ©s Goens, Sebastian Ertel, and Jeronimo Castril-
lon. 2020. Compiler-based graph representations for deep learning models of
code.InProceedingsofthe29thInternationalConferenceonCompilerConstruction .
201Å›211.
[20]AndreaCalvagna,AndreaFornaia,andEmilianoTramontana.2014. Assessing
thecorrectnessofJVMimplementations.In 2014IEEE23rdInternationalWETICE
Conference . IEEE,390Å›395.
[21]Andrea Calvagna and Emiliano Tramontana. 2013. Automated conformance
testing of Java virtual machines. In 2013 Seventh International Conference on
Complex,Intelligent,and SoftwareIntensiveSystems . IEEE,547Å›552.
[22]SangKilCha,MaverickWoo,andDavidBrumley.2015. Program-adaptivemu-
tationalfuzzing.In ProceedingsoftheIEEESymposiumonSecurityandPrivacy
(S&P). IEEE,725Å›741. https://doi.org/10.1109/SP.2015.50
[23]StefanosChaliasos,ThodorisSotiropoulos,DiomidisSpinellis,ArthurGervais,
Benjamin Livshits, and Dimitris Mitropoulos. 2022. Finding Typing Compiler
Bugs.InProceedingsofthe43rdACMSIGPLANInternationalConferenceonPro-
gramming Language Design and Implementation . 183Å›198. https://doi.org/10.
1145/3519939.3523427
[24]JunjieChen,JibeshPatra,MichaelPradel,YingfeiXiong,HongyuZhang,Dan
Hao,andLuZhang.2020. ASurveyofCompilerTesting. ACMComputingSurveys
(CSUR)53(2020). https://doi.org/10.1145/3363562
[25]Yuting Chen, Ting Su, and Zhendong Su. 2019. Deep differential testing of JVM
implementations. In 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE) . IEEE,1257Å›1268.
[26]Yuting Chen, Ting Su, Chengnian Sun, Zhendong Su, and Jianjun Zhao. 2016.
Coverage-directeddifferentialtestingofJVMimplementations.In proceedings
of the 37th ACM SIGPLAN Conference on Programming Language Design and
Implementation . 85Å›99.
[27]Yongheng Chen, Rui Zhong, Hong Hu, Hangfan Zhang, Yupeng Yang, Dinghao
Wu,andWenkeLee.2021. Oneenginetofuzzâ€™emall:Genericlanguageprocessortestingwithsemanticvalidation.In 2021IEEESymposiumonSecurityandPrivacy
(S&P). IEEE,642Å›658.
[28] ShafiulAzamChowdhury,Sohil LalShrestha,Taylor TJohnson,and Christoph
Csallner.2020. SLEMI: Equivalencemoduloinput(EMI) basedmutationof CPS
modelsforfindingcompilerbugsinSimulink.In 2020IEEE/ACM42ndInterna-
tional Conference onSoftwareEngineering (ICSE) . IEEE,335Å›346.
[29]ChrisCummins,PavlosPetoumenos,AlastairMurray,andHughLeather.2018.
Compiler fuzzing through deep learning. In Proceedings of the 27th ACM SIG-
SOFT InternationalSymposium on Software Testing and Analysis (ISSTA) . 95Å›105.
https://doi.org/10.1145/3213846.3213848
[30]Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. 2017.
End-to-enddeeplearningofoptimizationheuristics.In 201726thInternational
Conference on Parallel Architectures and Compilation Techniques (PACT) . IEEE,
219Å›232.
[31]MarcoDâ€™AmbrosandMicheleLanza.2006. Softwarebugsandevolution:Avisual
approachtouncovertheirrelationship.In ConferenceonSoftwareMaintenance
and Reengineering (CSMRâ€™06) . IEEE,229Å›238.
[32]Walter R Gilks, Sylvia Richardson, and David Spiegelhalter. 1995. Markov chain
Monte Carlo inpractice . CRC press.
[33]Patrice Godefroid, Adam Kiezun, and Michael Y Levin. 2008. Grammar-based
whitebox fuzzing. In Proceedings of the 29th ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI) . 206Å›215. https:
//doi.org/10.1145/1375581.1375607
[34]Patrice Godefroid, Hila Peleg, and Rishabh Singh. 2017. Learn&fuzz: Ma-
chine learning for input fuzzing. In Proceedings of the 32nd IEEE/ACM Inter-
national Conference on Automated Software Engineering (ASE) . IEEE, 50Å›59.
https://doi.org/10.1109/ASE.2017.8115618
[35]RahulGopinath,BjÃ¶rnMathis,MathiasHÃ¶schele,AlexanderKampmann,and
AndreasZeller.2018. Sample-freelearningofinputgrammarsforcomprehensive
softwarefuzzing. arXivpreprintarXiv:1810.08289 (2018). https://doi.org/arXiv:
1810.08289
[36]Samuel GroÃŸ. 2018. FuzzIL: Coverage guided fuzzing for JavaScript engines . Ph.D.
Dissertation. Masterâ€™s thesis, KarlsruheInstituteof Technology.
[37]Dick Grune, Kees Van Reeuwijk, Henri E Bal, Ceriel JH Jacobs, and Koen Lan-
gendoen.2012. Modern compiler design . SpringerScience & BusinessMedia.
[38]Tao Guo, Puhan Zhang, Xin Wang, and Qiang Wei. 2013. Gramfuzz: Fuzzing
testing of web browsers based on grammar analysis and structural mutation.
InProceedingsofthe2thInternationalConferenceonInformatics&Applications
(ICIA). IEEE,212Å›215. https://doi.org/10.1109/ICOIA.2013.6650258
[39]VarunaGupta,NGaneshan,andTarunKumarSinghal.2015.Determiningtheroot
causesofvarioussoftwarebugsthroughsoftwaremetrics.In 20152ndInterna-
tional Conference on Computing for Sustainable Global Development (INDIACom) .
IEEE,1211Å›1215.
[40]HyungSeok Han, DongHyeon Oh, and Sang Kil Cha. 2019. CodeAlchemist:
Semantics-Aware Code Generation to Find Vulnerabilities in JavaScript Engines.
InProceedingsofthe26thAnnualNetworkandDistributedSystemSecuritySym-
posium(NDSS) . https://doi.org/10.14722/NDSS.2019.23263
[41]JingxuanHe,MislavBalunoviÄ‡,NodarAmbroladze,PetarTsankov,andMartin
Vechev.2019. Learningtofuzzfromsymbolicexecutionwithapplicationtosmart
contracts.In Proceedingsof the ACM SIGSAC Conference onComputer and Com-
municationsSecurity(CCS) . 531Å›548. https://doi.org/10.1145/3319535.3363230
[42]Charles Antony Richard Hoare. 1969. An axiomatic basis for computer program-
ming.Commun. ACM 12,10(1969), 576Å›580.
[43]SeppHochreiterandJÃ¼rgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735Å›1780. https://doi.org/10.1162/NECO.1997.9.8.1735
[44]Christian Holler, KimHerzig, and Andreas Zeller.2012. Fuzzingwith code frag-
ments. In Proceedingsofthe21stUSENIX SecuritySymposium(USENIXSecurity) .
445Å›458. https://doi.org/10.5555/2362793.2362831
[45]Jinru Hua and Sarfraz Khurshid. 2017. EdSketch: Execution-driven sketching for
Java.InProceedingsofthe24thACMSIGSOFTInternationalSPINSymposiumon
ModelChecking ofSoftware . 162Å›171.
[46]Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A Method for Stochastic
Optimization.In ProceedingsoftheInternationalConferenceonLearningRepre-
sentations (ICLR) . https://doi.org/arXiv:1412.6980
[47]Triet HM Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep learning for
sourcecodemodelingandgeneration:Models,applications,andchallenges. ACM
ComputingSurveys(CSUR) 53,3 (2020), 1Å›38.
[48]Vu Le, Mehrdad Afshari, and Zhendong Su. 2014. Compiler Validation via
Equivalence modulo Inputs. In Proceedings of the 35th ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation (PLDI) . 216Å›226.
https://doi.org/10.1145/2594291.2594334
[49]Xuan-Bach D Le, Corina Pasareanu, Rohan Padhye, David Lo, Willem Visser,
andKoushikSen.2019. SAFFRON:Adaptivegrammar-basedfuzzingforworst-
case analysis. ACM SIGSOFT Software Engineering Notes 44, 4 (2019), 14Å›14.
https://doi.org/10.1145/3364452.3364455
[50]SuyoungLee,HyungSeok Han,SangKil Cha, andSooelSon. 2020. Montage:A
Neural Network Language Model-Guided JavaScript Fuzzer. In Proceedings of the
29thUSENIXSecuritySymposium(USENIXSecurity) . USENIX, 2613Å›2630.A Generative andMutationalApproachforSynthesizingBug-exposing TestCases to Guide CompilerFuzzing ESEC/FSE â€™23, December3â€“9, 2023,San Francisco, CA, USA
[51]Caroline Lemieux and Koushik Sen. 2018. Fairfuzz: A targeted mutation strat-
egy for increasing greybox fuzz testing coverage. In Proceedings of the 33rd
ACM/IEEE International Conference on Automated Software Engineering (ASE) .
475Å›485. https://doi.org/10.1145/3238147.3238176
[52]Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng,andYuyiZhong.2018. Vuldeepecker:Adeeplearning-basedsystemfor
vulnerability detection. Network and DistributedSystemSecuritySymposium .
[53]Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F. Donald-
son. 2015. Many-Core Compiler Fuzzing. In Proceedings of the 36th ACM SIG-
PLANConferenceonProgramming LanguageDesign andImplementation(PLDI) .
https://doi.org/10.1145/2737924.2737986
[54]Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F Donaldson.
2015. Many-corecompiler fuzzing. ACMSIGPLAN Notices 50,6 (2015), 65Å›76.
[55]Bingchang Liu, Guozhu Meng, Wei Zou, Qi Gong, Feng Li, Min Lin, Dandan Sun,
Wei Huo,and Chao Zhang.2020. A large-scale empirical study on vulnerability
distribution within projects and the lessons learned. In 2020 IEEE/ACM 42nd
InternationalConference onSoftwareEngineering (ICSE) . IEEE,1547Å›1559.
[56]Jiawei Liu, Yuxiang Wei, Sen Yang, Yinlin Deng, and Lingming Zhang. 2022.
Coverage-guided tensor compiler fuzzing with joint IR-pass mutation. Proceed-
ingsofthe ACMonProgrammingLanguages 6,OOPSLA1 (2022), 1Å›26.
[57]Xiao Liu, Xiaoting Li, Rupesh Prajapati, and Dinghao Wu. 2019. Deepfuzz:
Automatic generation of syntax valid c programs for fuzz testing. In Proceed-
ings ofthe AAAIConference onArtificialIntelligence (AAAI) ,Vol.33.1044Å›1051.
https://doi.org/10.1609/aaai.v33i01.33011044
[58]Rupak Majumdar and Ru-Gang Xu. 2007. Directed test generation using sym-
bolic grammars. In Proceedings of the 22nd IEEE/ACM International Conference on
AutomatedsoftwareEngineering(ASE) .134Å›143. https://doi.org/10.1145/1321631.
1321653
[59]BjÃ¶rnMathis,RahulGopinath,MichaÃ«lMera,AlexanderKampmann,Matthias
HÃ¶schele, and Andreas Zeller. 2019. Parser-directed fuzzing. In Proceedings
of the 40th ACM SIGPLAN Conference on Programming Language Design and
Implementation(PLDI) . 548Å›560. https://doi.org/10.1145/3314221.3314651
[60]BjÃ¶rn Mathis, Rahul Gopinath, and Andreas Zeller. 2020. Learning input to-
kens for effective fuzzing. In Proceedings of the 29th ACM SIGSOFT Interna-
tional Symposium on Software Testing and Analysis (ISSTA) . 27Å›37. https:
//doi.org/10.1145/3395363.3397348
[61]W. M. Mckeeman. 1998. Differential Testing for Software. Digital Technical
Journal10,1 (1998), 100Å›107.
[62]Facundo Molina, Marcelo dâ€™Amorim, and Nazareno Aguirre. 2022. Fuzzing Class
Specifications. https://doi.org/10.48550/ARXIV.2201.10874
[63]Rohan Padhye, Caroline Lemieux, Koushik Sen, Mike Papadakis, and Yves
Le Traon. 2019. Semantic Fuzzing with Zest. In Proceedings of the 28th ACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis(ISSTA) .ACM,
329Å›340. https://doi.org/10.1145/3293882.3330576
[64]Eunjung Park, John Cavazos, and Marco A Alvarez. 2012. Using graph-based
program characterization for predictive modeling. In Proceedings of the Tenth
InternationalSymposiumonCodeGeneration and Optimization . 196Å›206.
[65]SoyeonPark,WenXu,InsuYun,DaeheeJang,andTaesooKim.2020. Fuzzing
JavaScriptEngineswithAspect-preservingMutation.In Proceedingsofthe41st
IEEESymposiumonSecurityandPrivacy(S&P) .1629Å›1642. https://doi.org/10.
1109/SP40000.2020.00067
[66]Chao Peng and Ajitha Rajan. 2020. Automated Test Generation for OpenCL
Kernels Using Fuzzing and Constraint Solving. In Proceedings of the 13th Annual
Workshop on General Purpose Processing Using Graphics Processing Unit (San
Diego, California) (GPGPU â€™20) . Association for Computing Machinery, New
York, NY, USA,61Å›70. https://doi.org/10.1145/3366428.3380768
[67]Guillermo Polito, StÃ©phane Ducasse, and Pablo Tesone. 2022. Interpreter-guided
differentialJITcompilerunittesting.In Proceedingsofthe43rdACMSIGPLAN
InternationalConferenceonProgrammingLanguageDesignandImplementation .
981Å›992.
[68]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI
Blog1,8 (2019), 9.
[69]Alexandre Rebert, Sang Kil Cha, Thanassis Avgerinos, Jonathan Foote, David
Warren,GustavoGrieco,andDavidBrumley.2014. Optimizingseedselectionfor
fuzzing.In Proceedingsofthe23rdUSENIXSecuritySymposium(USENIXSecurity) .
861Å›875. https://doi.org/10.5555/2671225.2671280
[70]AlanRomano,XinyueLiu,YonghwiKwon,andWeihangWang.2021. AnEm-
pirical Study of Bugs in WebAssembly Compilers. In 2021 36th IEEE/ACM In-
ternationalConferenceonAutomatedSoftwareEngineering(ASE) .42Å›54. https:
//doi.org/10.1109/ASE51524.2021.9678776
[71]Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distil-
BERT, a distilled version of BERT: smaller, faster, cheaper and lighter. In NeurIPS
EMC2 Workshop .[72] Mozilla Security. 2007. funfuzz. https://github.com/MozillaSecurity/funfuzz.
[73]Marija Selakovic and Michael Pradel. 2016. Performance issues and optimiza-
tions in javascript: an empirical study. In Proceedings of the 38th International
Conference onSoftwareEngineering . 61Å›72.
[74]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine
translation of rare words with subword units. In Proceedings of the 54th An-
nualMeetingof theAssociationforComputational Linguistics(ACL) .1715Å›1725.
https://doi.org/10.18653/v1/P16-1162
[75]Nicholas Smith, Danny Van Bruggen, and Federico Tomassetti. 2017. Javaparser:
visited.Leanpub,oct. de (2017).
[76]Prashast Srivastava and Mathias Payer. 2021. Gramatron: Effective Grammar-
AwareFuzzing (ISSTA2021) .AssociationforComputingMachinery,NewYork,
NY, USA,244Å›256. https://doi.org/10.1145/3460319.3464814
[77]Chengnian Sun, Vu Le, and Zhendong Su. 2016. Finding and analyzing compiler
warning defects. In Proceedingsof the38th IEEE/ACM International Conference
on Software Engineering(ICSE) .IEEE, 203Å›213. https://doi.org/10.1145/2884781.
2884879
[78]Chengnian Sun, Vu Le, Qirun Zhang, and Zhendong Su. 2016. Toward under-
standingcompilerbugsinGCCandLLVM.In Proceedingsofthe25thInternational
SymposiumonSoftwareTestingand Analysis . 294Å›305.
[79]Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learn-
ing withneural networks. Advancesin neural informationprocessing systems 27
(2014).
[80]Michael Sutton, Adam Greene, and Pedram Amini. 2007. Fuzzing: brute force
vulnerabilitydiscovery . Pearson Education.
[81]Spandan Veggalam, Sanjay Rawat, Istvan Haller, and Herbert Bos. 2016. Ifuzzer:
An evolutionary interpreter fuzzer using genetic programming. In Proceedings of
the European Symposiumon Research inComputer Security (ESORICS) . Springer,
581Å›601. https://doi.org/10.1007/978-3-319-45744-4_29
[82]HuantingWang,GuixinYe,ZhanyongTang,ShinHweiTan,SongfangHuang,
DingyiFang,YansongFeng,LizhongBian,andZhengWang.2020. Combining
graph-basedlearningwithautomateddatacollectionforcodevulnerabilitydetec-
tion.IEEETransactionsonInformationForensicsandSecurity 16(2020),1943Å›1958.
https://doi.org/10.1109/TIFS.2020.3044773
[83]Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. 2019. Superion: grammar-
aware greybox fuzzing. In Proceedings of the 41st International Conference on
SoftwareEngineering (ICSE) . 724Å›735. https://doi.org/10.1109/ICSE.2019.00081
[84]Maverick Woo, Sang Kil Cha, Samantha Gottlieb, and David Brumley. 2013.
Scheduling black-box mutational fuzzing. In Proceedings of the ACM SIGSAC
Conference on Computer and Communications Security (CCS) . 511Å›522. https:
//doi.org/10.1145/2508859.2516736
[85]Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and un-
derstanding bugs in C compilers. In Proceedings of the 32nd ACM SIGPLAN
conferenceonProgrammingLanguageDesignandImplementation(PLDI) .283Å›294.
https://doi.org/10.1145/1993498.1993532
[86]GuixinYe,ZhanyongTang,ShinHweiTan,SongfangHuang,DingyiFang,Xi-
aoyang Sun, Lizhong Bian, Haibo Wang, and Zheng Wang. 2021. Automated
conformancetestingforJavaScriptenginesviadeepcompilerfuzzing.In Proceed-
ingsofthe42ndACMSIGPLANInternationalConferenceonProgrammingLanguage
Designand Implementation . 435Å›450. https://doi.org/10.1145/3453483.3454054
[87]GuixinYe,ZhanyongTang,HuantingWang,DingyiFang,JianbinFang,Songfang
Huang, and Zheng Wang. 2020. Deep Program Structure Modeling Through
Multi-Relational Graph-based Learning. In Proceedings of the ACM International
Conference on Parallel Architectures and Compilation Techniques (PACT) . 111Å›123.
https://doi.org/10.1145/3410463.3414670
[88]MichalZalewski. 2014. American FuzzyLop. https://lcamtuf.coredump.cx/afl/.
[89]ZhiqiangZang,NathanielWiatrek,MilosGligoric,andAugustShi.2022.Compiler
Testing using Template Java Programs. In International Conference on Automated
SoftwareEngineering . Toappear. https://doi.org/10.1145/3551349.3556958
[90]Qirun Zhang, Chengnian Sun, and Zhendong Su. 2017. Skeletal program enu-
meration for rigorous compiler testing. In Proceedings of the 38th ACM SIGPLAN
Conference onProgrammingLanguage Designand Implementation . 347Å›361.
[91]Yingquan Zhao, Zan Wang, Junjie Chen, Mengdi Liu, Mingyuan Wu, Yuqun
Zhang,andLingmingZhang.2022. History-driventestprogramsynthesisfor
JVMtesting.In 2022IEEE/ACM44thInternationalConferenceonSoftwareEngi-
neering (ICSE) . IEEE,1133Å›1144.
[92]Yaowen Zheng, Ali Davanian, Heng Yin, Chengyu Song, Hongsong Zhu, and
LiminSun.2019. FIRM-AFL:High-ThroughputGreyboxFuzzingofIoTFirmware
via Augmented Process Emulation.. In USENIX Security Symposium . 1099Å›1114.
[93]Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Effective vulnerability identification bylearning comprehensive program
semantics via graph neural networks. Advances in neural information processing
systems32(2019).
Received 2023-02-02; accepted 2023-07-27