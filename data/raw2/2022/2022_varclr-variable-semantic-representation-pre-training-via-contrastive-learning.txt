VarCLR: Variable Semantic Representation Pre-training via
Contrastive Learning
Qibin Chen
qibinc@cs.cmu.edu
Carnegie Mellon UniversityJeremy Lacomis
jlacomis@cs.cmu.edu
Carnegie Mellon UniversityEdward J. Schwartz
eschwartz@cert.org
Carnegie Mellon University Software
Engineering Institute
Graham Neubig
gneubig@cs.cmu.edu
CarnegieMellon UniversityBogdan Vasilescu
bogdanv@cs.cmu.edu
Carnegie Mellon UniversityClaire Le Goues
clegoues@cs.cmu.edu
Carnegie Mellon University
ABSTRACT
Variable names are critical for conveying intended program behav-
ior.Machinelearning-basedprogramanalysismethodsusevariable
name representations for a wide range of tasks, such as suggesting
newvariablenamesandbugdetection.Ideally,suchmethodscould
capture semantic relationships between names beyond syntactic
similarity,e.g.,thefactthatthenames averageand meanaresimi-
lar.Unfortunately,previousworkhasfoundthateventhebestof
previousrepresentationapproachesprimarilycaptureâ€œrelatednessâ€
(whether two variables are linked at all), rather than â€œsimilarityâ€
(whether they actually have the same meaning).
We propose VarCLR, a new approach for learning semantic rep-
resentations of variable names that effectively captures variable
similarity in this stricter sense. We observe that this problem is an
excellent fit for contrastive learning , which aims to minimize the
distancebetweenexplicitlysimilarinputs,whilemaximizingthe
distancebetween dissimilarinputs. Thisrequires labeled training
data, and thus we construct a novel, weakly-supervised variable
renaming dataset mined from GitHub edits. We show that VarCLR
enables the effective application of sophisticated, general-purpose
languagemodelslikeBERT,tovariablenamerepresentationand
thus also to related downstream tasks like variable name similarity
search or spelling correction. VarCLR produces models that sig-
nificantly outperform the state-of-the-art on IdBench, an existing
benchmarkthatexplicitlycapturesvariablesimilarity(asdistinct
from relatedness). Finally, we contribute a release of all data, code,
andpre-trainedmodels,aimingtoprovideadrop-inreplacementfor
variable representations used in either existing or future program
analysesthatrely on variable names.
CCSCONCEPTS
â€¢Softwareanditsengineering â†’Softwarelibrariesandrepos-
itories;â€¢Computingmethodologies â†’Learninglatentrep-
resentations; Naturallanguageprocessing ;Neuralnetworks .
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510162ACM Reference Format:
Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan
Vasilescu,andClaireLeGoues.2022.VarCLR:VariableSemanticRepresenta-
tion Pre-training via Contrastive Learning. In 44th International Conference
on Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA.
ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/3510003.3510162
1 INTRODUCTION
Variable namesconveykey informationaboutcodestructure and
developer intention. They are thus central for code comprehen-
sion, readability, and maintainability [ 7,46]. A growing array of
automatic techniques make use of variable names in the context
of tasks like but not limited to bug finding [ 64,68] or specifica-
tionmining[ 88].Beyondleveragingtheinformationprovidedby
names in automated tools, recent work has increasingly attempted
to directly suggest good or improved names, such as in reverse
engineering [35, 45]or refactoring [3, 5, 52].
Developing (and evaluating) such automated techniques (or
name-basedanalyses [75])reliesinlargepartontheabilitytomodel
andreasonabouttherelationshipsbetweenvariablenames.Forcon-
creteness, consider an analysis for automatically suggesting names
in decompiled code. Given a compiled program (such that variable
names are discarded) that is then decompiled (resulting in generic
names like a1,a2), a renaming tool seeks to replace the generic
decompiler-provided identifiers with more informative variable
names for the benefit of reverse engineers aiming to understand it.
Goodnamesinthiscontextarepresumablycloselyrelatedtothe
namesusedintheoriginalprogram(beforethedeveloper-provided
names were discarded). A variable originally named max, for exam-
ple, and then decompiled to a2, should be replaced with a name at
leastcloseto max,like maximum.Modelingthisrelationshipwellis
key for both constructing and evaluating such analyses.
Accurately capturing and modeling these relationships is dif-
ficult. A longstanding approach has used syntactic difference â€”
likevariousmeasuresofstringeditdistanceâ€”toestimatethere-
lationshipbetweentwovariables(suchasforspellchecking[ 18]).
However,syntacticdistanceisquitelimitedincapturingunderly-
ing name semantics. For example, the pairs ( minimum,maximum) and
(minimum,minimal) are equidistant syntactically â€” with a Leven-
shteindistanceof two â€” but maximumand minimumare antonyms.
Morerecentworkhassoughttoinsteadencodevariablename
semanticsusingneuralnetworkembeddings,informingavarietyof
23272022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSEâ€™22,May21â€“29,2022,Pittsburgh,PA, USA Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues
name-based analyses [ 29,65,78]. Unfortunately, although state-of-
the-art techniques for variable name representation better capture
relatedness ,theystillstruggletoaccuratelycapturevariablename
similarity , in terms of how interchangeable two names are. Vari-
ables may be related for a variety of reasons. While maximumand
minimumare highly related, they certainly cannot be substituted
foroneanotherin a codebase. minimumand minimal, onthe other
hand,arebothrelatedandverysimilar.Inrecentwork,Wainakh
etal.[75]presentedanoveldataset,IdBench,basedonahuman
survey on variable similarity and interchangeability, and used it to
evaluatestate-of-the-artembeddingapproaches.Theyempirically
establishedthatthereremainssignificantroomforimprovement
in terms of capturing similarity rather than merely relatedness.
In this paper, we formulate the variable semantic representation
learning problem as follows: given a set of variable data, learn a
function ğ‘“thatmaps avariablenamestring toalow-dimensional
densevectorthatcanbeusedinavarietyoftasks(likethetypesof
name-basedanalysesdiscussedabove).Tobeuseful,suchamap-
ping function should effectively encode similarity , i.e., whether
two variables have the same meaning. That is, ğ‘“(minimum)and
ğ‘“(minimal)should be close to one another. Importantly, however,
the function should also ensure that variable names that are not
similar(regardlessofrelatedness!)are farfromoneanother.That
is,ğ‘“(minimum)andğ‘“(maximum)should be distant.
Our first key insight is that this problem is well suited for a
contrastive-learningapproach[ 14,30,63,82].Conceptually,con-
trastive learning employs encoder networks to encode instances
(in this task, variables) into representations (i.e., hidden vectors),
withagoalofminimizingthedistancebetween(therepresentation
of)similarinstancesandmaximizingthedistancebetween(therep-
resentation of) dissimilar instances. Contrastive learning requires
asinputasetofâ€œpositivepairâ€examplesâ€”ofsimilarvariables,in
our caseâ€”fortraining.
Oursecondkeyinsightisthatwecanconstructasuitableweakly-
supervised dataset of examples of similar variables by taking ad-
vantage of large amounts of source control information on GitHub.
Following the definition of â€œsimilarityâ€ from prior work [ 60,75],
weconsidertwovariablenamesaresimilariftheyhavethesame
meaning,orare interchangeable .Wethereforeautomaticallymine
sourcecontroleditstoidentifyhistoricalchangeswheredevelopers
renamedavariablebutdidnototherwiseoverlymodifythecode
in which it was used. Although potentially noisy, this technique
matchesanintuitiveunderstandingofvariablenamesimilarityin
terms of interchangeability, and allows for the collection of a large
dataset,whichwe call GitHubRenames.
Finally, we observe that the variable semantic representation
learningproblemrequiresmorepowerfulneuralarchitecturesthan
word2vec -based approaches [ 8,59,75].1Such approaches are
limited both empirically (as Wainakh et al. showed) and conceptu-
ally; notefor example that theycannot capture componentorder-
ing, such as the difference between idx_to_word andword_to_idx .
Meanwhile, Pre-trained Language Models (PLMs) [ 9,20,67] based
onthepowerfulTransformerarchitecture[ 74]haveachievedthe
state-of-the-art on a wide range of natural language processing
1word2vec [ 59] is an embedding algorithm based on the distributional hypothesis ,
which assumes words that occur in the same contexts tend to have similar meanings.tasks, including text classification [ 20], question answering and
summarization[ 48],anddialogsystems[ 1].PLMstailoredspecif-
ically for programming languages such as CodeBERT [ 22] and
Codex [12] are useful in a variety of tasks such as code completion,
repair,andgeneration[ 12,55],thoughnotyetforvariablenamerep-
resentation. Encouragingly, previouswork showsthat contrastive
learning can strongly improve BERT sentence embeddings for tex-
tualsimilaritytasks[ 24].And,contrastivelearninghasbeenshown
to benefit from deeper and wider network architectures [13].
WecombinetheseinsightstoproduceVarCLR,anovelmachine
learningmethodbasedoncontrastivelearningforlearninggeneral-
purposevariablesemanticrepresentationencoders.InVarCLR,the
contrastivelearningelementservesasapre-trainingstepforatradi-
tionalencoder.WhilepowerfulmodernapproacheslikeCodeBERT
performpoorlyonthevariablerepresentationproblemoff-the-shelf,
we show that VarCLR-trained models dramatically outperform the
previous state-of-the-art on capturing both variable similarity and
relatedness.VarCLRisdesignedtobegeneraltoavarietyofuseful
downstreamtasks;wedemonstrateitseffectivenessforboththeba-
sic variable similarity/relatedness task (using the IdBench dataset
asagoldstandardbaseline)aswellasforvariablesimilaritysearch,
and spelling error correction.
To summarize, our main contributions are as follows:
(1)VarCLR, a novel method based on contrastive learning that
learns general-purpose variable semantic representations
suitablefor a variety of downstream tasks.
(2)Anewweaklysuperviseddataset,GitHubRenames,forbet-
ter variable representation learning consisting of similar
variablenames collected from real-world GitHub data.
(3)ExperimentalresultsdemonstratingthatVarCLRâ€™smodels
significantlyoutperformstate-of-the-artrepresentation ap-
proaches on IdBench, an existing benchmark for evaluat-
ing variable semantic representations. These results further
substantiate the utility of more sophisticated models like
CodeBERT,withlargermodelcapacity,inplaceofthepre-
vious word2vec -basedmethodsforlearningvariablerep-
resentations, while showing that the contrastive learning
pre-trainingstepiscriticaltoenablingtheeffectivenessof
suchmodels.
(4)Experimental results that demonstrate that both unsuper-
vised pre-training and our proposed weakly-supervised con-
trastivepre-trainingareindispensablepartsforadvancing
towards the state-of-the-art, for the former takes advantage
of greater data quantity by leveraging a huge amount of
unlabeled data, while the latter takes advantage of better
dataquality with ournew GitHubRenamesdataset.
Finally, we contribute a release of all data, code, and pre-trained
models,aimingtoprovideadrop-inreplacementforvariablerepre-
sentationsusedineitherexistingorfutureprogramanalysesthat
rely on variable names.2
2 PROBLEM DOMAIN
Variable names critically communicate developer intent and are
thusincreasinglyusedbyavarietyofautomatedtechniquesasa
centralsourceofinformation.Suchtechniquesincreasinglyrelyon
2Code,data,andpre-trainedmodelavailableathttps://github.com/squaresLab/VarCLR.
2328VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
machine learning and embedding-based representation approaches
to encode variable name meaning for these purposes. However,
recent work [ 75] shows that while neural embeddings based on
techniques like word2vec do a better job of capturing relation-
ships between variables than syntactic edit distance does, they
stillstruggletocaptureactualvariablesimilarityintermsoftheir
interchangeability. In this paper, we show that this problem is
amenable to a contrastive learning approach, enabling accurate
general-purposerepresentations of variable name semantics.
We define the variable semantic representation learning problem
as follows: given a collection of suitable variable data, learn a func-
tionğ‘“thatmapsavariablenamestringtoalow-dimensionaldense
vectorthatcanbeusedtobenefitvariousdownstreamtasks(like
variablesimilarityscoringin the simplest case, or arbitrarily com-
plexname-basedanalyses).Agoodmappingfunction ğ‘“forvariable
namerepresentations should:
(1)Capture similarity. ğ‘“should encode similarnames such that
theyareclosetooneanother.Twonamesaresimilarwhen
they have similar or generally interchangeable meanings,
likeavgandmean. This isespecially important for variables
thatarerelatedbutnotsimilar ,suchas maximumandminimum.
Indeed,antonymsareoftencloselyrelatedandcanappear
in similar contexts ( maxand minfor example may be used
togetherin loops finding extrema).
(2)Capture component ordering and importance. Variables often
consist of component words or sub-words. We observe that
the order of such components can affect meaning. For ex-
ample, idx_to_word andword_to_idx contain the same sub-
words, but have different meanings. Moreover, the impor-
tanceofdifferentcomponentwordsinavariablecanbediffer-
ent and the importance of the same word can vary between
variables. Forexample, invariables onAddand onRemove,on
is less important, while addandremoveare more important.
InturnOnandturnOff,onandoffare more important than
turn. A good mapping function ğ‘“should be able to cap-
ture these differences, instead of treating variables as an
unordered bag of sub-words.
(3)Transferability. Therepresentationshouldbegeneral-purpose
andusableforawiderangeoftasks.Benefitsofatransfer-
able,sharedrepresentationincludetheabilityto(1)improve
accuracy on unsupervised or data-scarce tasks, where it can
be hard to obtain high-quality variable representations from
scratch, and (2) for complex tasks consisting of many sub-
tasks,makebetteruseoflabeleddatafrommultiplesub-tasks
via multi-tasklearning.
Thisformulationoftheproblemmotivatesouruseof contrastive
learning, which is an effective way to learn similarity from labeled
data.Conceptually,givenanencodernetwork ğ‘“ğœƒandasetofsimilar
â€œpositive pairsâ€, contrastive learning returns a newencoder that at-
temptstolocatesimilarâ€œpositivepairâ€instancesclosertogetherand
dissimilarâ€œnegativepairâ€instancesfartherapart.Inpractice,this
can be accomplished by re-training the original encoder on a new
pre-training task: instance discrimination [ 82]. Instance discrim-
ination casts the contrastive learning problem as a classification
problem where only the â€œpositive pairâ€ instances are equivalent.
Ratherthanexplicitlyadjustingthedistancesbetweenpoints,the7RNHQL]HU Variable   VARCLR
(QFRGHU
9DULDEOH
5HSUHVHQWDWLRQ'RZQVWUHDP7DVNV
6LPLODULW\6FRULQJ
6LPLODULW\6HDUFK
6SHOOLQJ(UURU&RUUHFWLRQ5HODWHGQHVV6FRULQJ*LW+XE5HQDPHV
Variable
Pairs
EncoderVARCLR
&RQWUDVWLYH
3UHWUDLQLQJ
fÎ¸
fÎ¸/prime
Figure 1: Conceptualoverview of VarCLR.
encoderâ€™s parameters are trained to optimize its performance at
discriminating similar instance from dissimilar instances. This nat-
urallyadjuststheparametersoftheencodersuchthatsimilarin-
stances are moved closer together (and vice-versa for dissimilar
instances). The actual output of the contrastive learning process is
a newencoder ğ‘“ğœƒ/primethatisidentical to theoriginal encoder inneural
architecture, but has a different set of parameters ğœƒ/primeresulting from
training on the instance discrimination task.
There are two central design choices in applying contrastive
learning, however. First, Which neural architectures should beused
forğ‘“ğœƒ?This is usually decided by the problem domain in question.
For example, in computer vision, ResNet [ 31] for learning image
representations[ 13,30,63];innaturallanguageprocessing,Simple
word embedding orBERT [ 20,53] for learningsentence represen-
tations [24,80]; and in data mining, Graph Neural Network [ 44]
forlearninggraphrepresentations[ 66].Second, Howtoconstruct
similar (positive) and dissimilar (negative) training pairs? Unsuper-
viseddataaugmentationlikecroppingorclippinghasbeenusedto
create different â€œviewsâ€ of the same image as similar pairs in im-
age processing [ 63,72]; word dropout can augment text sentences
for natural language processing [ 24]. For supervised contrastive
learning, positive pairs can be created from labeled datasets di-
rectly[24],orviasamplinginstancesfromthesameclass[ 42].Note
thatdissimilarpairstypicallyneednotbeexplicitlydefined.Instead,
in-batch negatives [63] can be sampled from instance pairs that are
not explicitly labeled as positive.
Thechoiceofsimilarinstancesisveryimportant,asitinfluences
the learned similarity function and impacts downstream effective-
ness [73]. For example, consider how training can lead to uninten-
tionalpropertiesofalearnedsimilarityfunctionfor word2vec .At
a highlevel, word2vec [59] canbe viewed asa form ofunsuper-
visedcontrastivelearning.Itemploysawordembeddinglayeras
the encoder, and treats words co-occurring in the context window
as similar pairs, while treating other words in the dictionary as
dissimilarones.3Duetoitschoiceofâ€œsimilarinstancesâ€,itlearns
moreofassociation(orrelatedness)betweenwords,insteadofsimi-
larityin terms of how interchangeable two words are. For example,
word2vec embeddings of cohyponym words such as red, blue,
white, green are very close. While this might not be a problem
inNLPapplications, word2vec leadstounsatisfactorybehavior
3Weleaveouttheminordifferencethat word2vec producestwosetsofembeddings,
whilecontrastive learning usually uses a unified representation.
2329ICSEâ€™22,May21â€“29,2022,Pittsburgh,PA, USA Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues
whenappliedtovariablenames[ 75],e.g.,byidentifying minLength
and maxLength as similar.
3 METHOD
Figure1showsahigh-levelconceptualoverviewofVarCLR,our
frameworkfor learningeffective semanticrepresentations ofvari-
able names. VarCLR consists of a contrastive pre-training phase
that takes two inputs: (1) a positive set of similar variable name
pairs,and(2)aninputencoder.Thesetofsimilarvariablesiscrucial
forVarCLRâ€™sperformance.WethusproduceGitHubRenames,a
novelweakly-superviseddatasetconsistingofpositiveexamples
of similar variables by examining large amounts of source code
historyavailablefromGitHub(Section3.1).Thesevariablesmust
be suitably tokenized for encoding in a way that captures and
retains relevant information (Section 3.2), both for pre-training
and for downstream tasks. VarCLR also takes an input encoder
ğ‘“ğœƒwithlearnableparameters ğœƒ(Section3.3).Thisencoderisthen
trained using contrastivelearning (Section 3.4). The outputof our
framework is a contrastively-trained VarCLR encoder that con-
verts tokenized variables into semantic representations suitable for
a variety of tasks and name-based analyses, including similarity
scoringor spelling error correction, among others.
3.1 Similarvariables: GitHubRenames
Ahigh-leveldefinitionofâ€œsimilarityâ€[ 60,75],isthedegreetowhich
twovariableshavethesamemeaning.Contrastivelearningrequires
positiveexamplesfortraining,andthusweneedasetofappropriate
positive pairs of similar variable names. As discussed in Section 2,
theseneednotbemanuallyconstructed.AlthoughIdBench[ 75]
providescuratedsetsofhuman-judgedâ€œsimilarâ€variables,theyare
too small for training purposes (the largest set, has 291 variable
pairs).Thismotivatesanautomatedmechanismforconstructing
trainingdata,withtheaddedbenefitthatweneednotbeconcerned
about training and testing on the same dataset (as we use IdBench
for evaluation).
Instead, we observe that one way to define variable similarity is
toconsiderthedegreetowhichtwovariablesareexplicitly inter-
changeable in code (close to IdBenchâ€™s definition of â€œContextual
similarityâ€). We therefore collect a weakly supervised dataset of
interchangeablevariablenamesbyminingsourcecontrolversion
historiesforcommitswherevariablenameschange.Thesevariable
pairsareconsideredsimilarbecausetheyappearinterchangeable
in the same code context.
Concretely,webuiltuponexistingopen-sourcedatasetcollection
code used to mine source control for the purpose of modeling
changes[ 84].4Givenarepository,thiscodeminesallcommitsof
lessthansixlinesofcodewhereavariableisrenamed.Theintuition
is to look for commits that do not make large structural changes
thatmightcorrespondtoamajorchangeinavariableâ€™smeaning.
We applied dataset collection to an expanded version of the list of
repositories used in ref [ 84], consisting of 568 C# projects.5The
final GitHubRenames dataset contains 66,855variable pairs, each
consisting of a variable name before and after a renaming commit.
4https://github.com/microsoft/msrc-dpu-learning-to-represent-edits
5https://github.com/quozd/awesome-dotnetTheGitHubRenamesdatasetisonlyweaklysupervisedsince
developers were not asked to label variable pairs explicitly. The
dataset may thus be noisy,and in particular we did not attempt to
filter out renames corresponding to bug fixes. Indeed, we note that
anumberofpairsinGitHubRenamescorrespondtofixingspelling
mistakes (Section 4.4). Overall, however, we note that our method
transfers well to the IdBench validation set, and expect that more
datawill onlyimprove VarCLRâ€™s effectiveness.
3.2 Inputrepresentation
Avariablenameasatextstringmustbepreprocessedtobeused
as input to a neural network encoder. We observe two interest-
ing aspects of variable names that inform our preprocessing. First,
variablenamesareoftencomposedofmultiplewordswithinter-
changeablecasestyles,e.g., max_iteration vsmaxIteration .S ec -
ond, variable names are sometimes composed of short words or
abbreviations,withoutanunderscoreoruppercasetoseparatethem.
e.g., filelist,sendmsg.
Forthefirstproblem,weapplyasetofregexrulestocanonicalize
variable names into a list of tokens, e.g., ["max", "iteration"] .
Thesecondproblemismorechallenging,andcouldcauseOut-of-
vocabulary (OOV) problems. To solve this, we use the pre-trained
CodeBERTtokenizer[ 22],whichisunderlyingaBytePairEncoding
(BPE) model [ 70] trained on a large code corpus based on token
frequencies.Whenencounteringanunknowncompositevariable
name such as sendmsg, it is able to split it into subword tokens , e.g.,
["send", "##msg"] , where "##"means this token is a suffix of the
previous word.
3.3 Encoders
Generally, a neural encoder takes the input sequence, and encodes
and aggregates information over the sequence to produce a hid-
denvector.Thatis,givenasequenceoftokens ğ‘£=(ğ‘£1,ğ‘£2,...,ğ‘£ğ‘›)
corresponding to a tokenized variable name, an encoder outputs
a hidden vector ğ’‰âˆˆRğ‘‘, whereğ‘‘is the dimension of the hidden
representation:
ğ’‰=ğ‘“ğœƒ(ğ‘£), (1)
ğ‘“ğœƒdenotes the encoder withlearnableparameters ğœƒ.
NotethatVarCLRisapplicabletoanyencoderwiththisform.
In this paper, we instantiate it specifically for Word Embedding
Averaging(VarCLR-Avg),theLSTMEncoder(VarCLR-LSTM),and
BERT(VarCLR-CodeBERT).
Word Embedding Averaging. Averaging the embeddings of input
tokens is a simple but effective way to represent a whole input
sequence,givensufficientdata[ 80,81].Therefore,weconsiderthis
asasimplebaselineencoder.Formally,giventhetokenizedvariable
nameğ‘£=(ğ‘£1,ğ‘£2,...,ğ‘£ğ‘›),ğ‘£ğ‘–âˆˆV, and a word embedding lookup
tableğ¿ğœƒ:Vâ†’Rğ‘‘:
ğ’‰=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ¿ğœƒ(ğ‘£ğ‘–), (2)
whereVis the vocabulary, i.e., the collection of all tokens the
model can handle, ğœƒâˆˆR|VÃ—ğ‘‘|is the learnable embedding matrix.
Although simple and efficient, this Word Embedding Averaging
encodersuffersfrom two issues: 1) Order. The averaging operator
2330VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
9DULDEOH v1,i Encoder
fÎ¸
 Encoder
fÎ¸9DULDEOHv2,i
v2,j
v2,kFRV
G6FRUHV F5HSUHVHQWDWLRQV&RQWUDVWLYH
/RVVv1,iv2,i
v2,j v1,j
v2,k v1,k
*LW+XE5HQDPHVVLPLODU
GLVVLPLODUqi
D9DULDEOH3DLUV E0LQLEDWFKÅ LNCEki
kj
kk
Figure2: Overviewof VarCLRâ€™scontrastivepre-trainingmethod.a) GitHubRenames containsinterchangeablevariablepairs.b)Ateachtraining
step,sampleamini-batchofvariablepairs,andaimtopullclosethevariablesrepresentationswithinapair,e.g., ğ‘£1,ğ‘–andğ‘£2,ğ‘–,whilepushing
awaytherepresentationsofothervariables,e.g., ğ‘£1,ğ‘–andğ‘£2,ğ‘—.c)Toachievethis,anencoder ğ‘“ğœƒwithlearnableparameters ğœƒ,isadoptedtoencode
thevariablestringtohiddenvectors.d)contrastivelossiscalculatedbasedonthesimilarityscoresasthecosinedistancebetweenencoded
hiddenvectors; the encoder ğ‘“ğœƒis optimized with gradient descent.
discards word orderinformation inthe input sequence,and thus
poorlyrepresentsvariablenameswherethisorderisimportant,e.g.,
idx_to_word andword_to_idx .2)Tokenimportance .Anunweighted
average of word embeddings ignores the relative importance of
words in a variable name, as well as the fact that the importance of
a word can vary by context.
LSTM Encoder. Recurrent Neural Networks (RNNs) [ 69] general-
izefeed-forwardneuralnetworkstosequences.Giventhetokenized
variablename ğ‘£=(ğ‘£1,ğ‘£2,...,ğ‘£ğ‘›),astandardRNNcomputesase-
quence of hidden vectors (ğ’‰1,ğ’‰2,..., ğ’‰ğ’).
ğ’‰ğ’•=sigmoid/parenleftBig
ğ‘Šhxğ¿ğœƒğ‘’(ğ‘£ğ‘¡)+ğ‘Šhhğ’‰ğ’•âˆ’1/parenrightBig
, (3)
whereğ‘Šhx,ğ‘ŠhhâˆˆRğ‘‘Ã—ğ‘‘areweightmatrices,and ğœƒğ‘’istheembed-
ding matrix (as in Equation (2)). RNNs process the input sequence
by reading in one token ğ‘£ğ‘¡at a time and combining it with the
pastcontext â„ğ‘¡âˆ’1.Thiscapturessequentialorderinformation.After
processing all input tokens, we can average the hidden states at
each step to output a representation of the original variable:
ğ’‰=1
ğ‘›ğ‘›/summationdisplay.1
ğ‘–=1ğ’‰ğ’Š. (4)
We use bi-directional Long Short-Term Memory (LSTM) mod-
els [34], a variant of RNNs widely used in natural language pro-
cessing.LSTMsintroduceseveralnewcomponents,includingthe
input and forget gates, controlling how much information flows
from the current token, and how much to keep from past contexts,
respectively. This better handles the token importance problem by
dynamically controlling the weight of the input token at each step.
BERT.Transformer-basedmodels[ 74]typicallyoutperformLSTMs
andareconsideredtobethebetterarchitectureformanyNLPtasks.
Pre-trainedLanguageModels(PLMs),builtuponTransformers,can
leverage massive amounts of unlabeled data and computational
resources to effectively tackle a wide range of natural language
processing tasks. Useful PLMs for programming languages includeCodeBERT [ 22] and Codex [ 12] PLMs not only capture compo-
nent ordering and token importance that LSTMs do, but provide
additionalbenefits:1)BERT-basedmodelsarealreadypre-trained
withself-supervisedobjectivessuchasMaskedLanguageModeling
(MLM)[20]onalargeamountofunlabeleddata.Itprovidesagood
initialization to the model parameters and improves the modelâ€™s
generalizationability, requiringfewer data to achievesatisfactory
performance[ 9].2)Transformerencodersaremuchmorepowerful
thanpreviousmodelsthankstothemulti-headself-attentionmech-
anism,allowingforthemodeltobemuchwideranddeeperwith
more parameters. We therefore propose to use PLMs for programs
as our most powerful choice of variable name encoder.
Effectiveness versus efficiency. Although BERT has the largest
model capacity ofthese encoders,it alsorequires highercomputa-
tion cost for both training and inference, and suffers from a longer
inference latency. The trade-off posted between effectiveness and
efficiency can vary according to different downstream applications.
Therefore,wefinditmeaningfultocompareallencodersinVar-
CLR. Different or better encoder models can be directly plugged
into the VarCLR framework in the future. We omit further interior
technicaldetailsofbothLSTMandBERTmodelsastheyarebeyond
the scope of this paper.
3.4 Contrastive Learning Pre-training
VarCLRimplementsthedesignchoicesforinputdata,variabletok-
enization, andinput encoderin a contrastivelearning framework.
Figure 2 provides an overview. Conceptually, contrastive learning
uses encoder networks to encode instances (in this task, variables)
intorepresentations(i.e.,hiddenvectors),andaimstominimizethe
distance between similar instances while maximizing the distance
between dissimilar instances.
Specifically,givenachoiceofencoderandsetoflabeledâ€œpositive
pairsâ€ of variable names, we use instance discrimination [ 82]a s
ourpre-trainingtask,andInfoNCE[ 63]asourlearningobjective.
Given a mini-batch of encoded and L2-normalized representations
2331ICSEâ€™22,May21â€“29,2022,Pittsburgh,PA, USA Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues
ofğ¾similarvariablepairs/braceleftbig
(ğ‘£1,ğ‘–,ğ‘£2,ğ‘–)|ğ‘–=1,...,ğ¾/bracerightbig
,wefirstencode
themto hidden representations:
ğ’’ğ’Š=ğ‘“ğœƒ(ğ‘£1,ğ‘–)/bardblex/bardblexğ‘“ğœƒ(ğ‘£1,ğ‘–)/bardblex/bardblex2, (5)
ğ’Œğ’Š=ğ‘“ğœƒ(ğ‘£2,ğ‘–)/bardblex/bardblexğ‘“ğœƒ(ğ‘£2,ğ‘–)/bardblex/bardblex2, (6)
where/bardblÂ·/bardbl2isâ„“2-norm,ğ‘“ğœƒdenotes the encoder. Then, we define
the InfoNCEloss as:
LNCE(ğ’’,ğ’Œ)=âˆ’E/parenleftBigg
logğ‘’ğ’’ğ’Š/latticetopğ’Œğ’Š/ğœ
/summationtext.1ğ¾
ğ‘—=1ğ‘’ğ’’ğ’Š/latticetopğ’Œğ’‹/ğœ/parenrightBigg
, (7)
whereğœis the temperature hyperparameter introduced by [ 82].
Intuitively, this objective encourages the model to discriminate the
corresponding similar instance ğ‘£2,ğ‘–of an instance ğ‘£1,ğ‘–from other
instances in the mini-batch ğ‘£2,ğ‘—. This learning objective is very
similar tothe cross-entropyloss forclassification tasks,while the
difference is that instead of a fixed set of classes, it treats each
instance as a distinct class. Following [ 26], we further make the
loss symmetricand minimizethe followingobjective function:
L=1
2LNCE(ğ’’,ğ’Œ)+1
2LNCE(ğ’Œ,ğ’’). (8)
In our task, this objective encourages the encoder to push the
representations of a pair of similar variables to be close to each
other, so that they can be discriminated from other variables.
We refer to this process as pre-training in the sense that the
training is not intended for a specific task but is learning a general-
purpose variablerepresentation.
4 EXPERIMENTS
In this section, we evaluate VarCLRâ€™s ability to train models for
variable representation along several axes. Section 4.1 addresses
setup,datasets,andbaselinescommontotheexperiments.Then,we
begin by addressing a central claim: How well do VarCLR models
encode variable similarity , as distinct from relatedness ? We answer
this question by using pre-trained VarCLR models to compute
similarity (and relatedness, resp) scores between pairs of variables,
andevaluatetheresultsonhuman-annotatedgoldstandardground
truthbenchmark (Section 4.2).
Next,weevaluateVarCLR-trainedmodelsontwootherdown-
streamtasks,demonstratingtransferability:variablesimilaritysearch
(Section4.3),and variable spelling error correction (Section 4.4).
Finally,weconductanablationstudy(Section4.5)lookingatthe
influence of training data size, pre-trained language models, and
pre-trained embeddings from unsupervised learning contribute to
VarCLRâ€™s effectiveness.
4.1 Setup
Pre-training. ForVarCLR-AvgandVarCLR-LSTM,weusethe
Adam optimizer [ 43] withğ›½1=0.9,ğ›½2=0.999,ğœ–=1Ã—10âˆ’8,a
learningrateof0.001,andearlystopaccordingtothecontrastive
lossonthevalidationset.Weuseamini-batchsizeof1024.Theinput
embedding and hidden representation dimensions are set to 768
and150respectively.wealsoinitializetheembeddinglayerwiththe
CodeBERTpre-trainedembeddinglayer.ForVarCLR-CodeBERT,
weusetheAdamWoptimizer[ 54]withthesameconfigurationandlearningrate,andamini-batchsizeof32.6WeusetheBERTmodel
architecture [ 20] and initialize the model with pre-trained weights
from CodeBERT [ 22]. For all three methods, we apply gradient
normclippingintherange [âˆ’1,1],andatemperature ğœof0.05.A
summary of the hyper-parameters can be found along with our
data, code, and pre-trained models at https://bit.ly/2WIalaW.
Dataset.While we use the GitHubRenames for training Var-
CLR, we use the IdBench [ 75] dataset for evaluation.7IdBench is
a benchmark specifically created for evaluating variable semantic
representations.Itcontains pairsofvariablesassignedrelatedness
andsimilarityscoresbyreal-worlddevelopers.IdBenchconsists
of three sub-benchmarksâ€” IdBench-small, IdBench-medium, and
IdBench-large, containing 167, 247, 291 pairs of variables, respec-
tively. Ground truth scores for each pair of variable are assessed
by multiple annotators. Pairs with disagreement between anno-
tatorsexceedingaparticularthresholdareconsidereddissimilar;
the three benchmarks differ in the choice of threshold. The smaller
benchmark provides samples with higher inter-annotator agree-
ment, while the larger benchmark provides more samples with
commensuratelyloweragreement.Themediumbenchmarkstrikes
abalance.WedescribecustomizationsoftheIdBenchdatasetto
particulartasks in their respective sections.
Baselines. We compare VarCLR models to the previous state-of-
the-art as presented in IdBench [ 75]. We reuse the baseline results
providedbytheIdBenchframework.TheIdBenchpaperevaluates
anumberofpreviousapproachesaswellasanewensemblemethod
thatoutperformsthem;we includeas baselines asubset of those
previoustechniques,andtheensemblemethod.Ofthestringdis-
tance/syntacticfunctions(stillbroadlyusedinvariousname-related
applications[ 51,68]),weinclude LevenshteinEditDistance(LV)
(the number of single-character edits required to transform one
string into the other); it performs in the top half of techniques
on scoring similarity, and is competitive with the other syntac-
tic distance metric [ 62] on relatedness. Of the embedding-based
single models, we include FastText CBOW (FT-cbow) and SG
(FT-sg)[8],extensionsof word2vec thatincorporatesubwordin-
formation,tobetterhandleinfrequentwordsandnewwords.These
were the best-performing embedding-based methods on both relat-
edness and similarity.
Finally, we include two combined models. IdBench [ 75]p r o -
posesanensemblemethodthatcombinesthescoresofallmodels
and variable features. For each pair in IdBench, the combined
model trains a Support Vector Machine (SVM) classifier with all
otherpairs,thenappliesthetrainedmodeltopredictthescoreof
the left-out pair. Note that this approach is trained on the IdBench
benchmarkitselfandis notdirectlycomparabletoothermethods.
For comparison, we add VarCLR-Avg, VarCLR-LSTM, VarCLR-
CodeBERT scores as additional input features to the combined
approach, and report the results for Combined-VarCLR.
6Larger mini-batch sizes make the contrastive learning task more challenging and
improvethequalityoflearnedrepresentation,asshownin[ 13]andourpreliminary
experiments. We use batch size of 32 for VarCLR-CodeBERT due to GPU memory
limitations.
7The IdBench evaluation scripts were updated after publication, leading to minor
differencesinevaluationscores.WeusetheirlatestcodeasofMay1st,2021toevaluate
thebaselinesandour models.
2332VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
4.2Variable Similarity and Relatedness Scoring
Our central claim is that VarCLR is well-suited to capturing and
predictingvariablesimilarity.Formally,giventwo variables ğ‘¢and
ğ‘£, we obtain variable representations with pre-trained VarCLR
encoderğ‘“ğœƒ/primeand compute the variable similarity score as the cosine
similaritybetween the two vectors:
ğ’‰ğ’–,ğ’‰ğ’—=ğ‘“ğœƒ/prime(ğ‘¢),ğ‘“ğœƒ/prime(ğ‘£) (9)
Ë†ğ‘ (ğ‘¢,ğ‘£)=ğ’‰ğ’–Â·ğ’‰ğ’—
/bardblğ’‰ğ’–/bardbl2/bardblğ’‰ğ’—/bardbl2, (10)
whereË†ğ‘ (ğ‘¢,ğ‘£)denotes the VarCLRâ€™s predicted similarity score. Fol-
lowing IdBench [ 75], we then compare the similarity scores of
pre-trainedVarCLRrepresentationswithhumanground-truthsim-
ilarityscoresbycomputingSpearmanâ€™srankcorrelationcoefficient
between them. This correlation coefficient falls in the range [-1,
1],where1indicatesperfectagreementbetweentherankings;-1
indicatesperfect disagreement; and 0 indicates no relationship.
Note that the VarCLR pre-training task is explicitly optimiz-
ingthedistancebetweensimilarvariablepairs.Thus,thevariable
similarity scoring task only really evaluates the performance of
the pre-training itself. To more fully evaluate whether our method
leadstobetterrepresentationsthatcantransfer,wealsoevaluate
on the variable relatedness scoring task.
Results.Table1showsthemodelsâ€™performanceonthesimilarity
and relatedness tasks in terms of Spearmanâ€™s rank correlation with
groundtruth.Table1ashowsthatVarCLR-CodeBERTimproves
overthepreviousstate-of-the-artonallthreeIdBenchbenchmarks,
withanabsoluteimprovementof0.18onIdBench-smalland0.13on
IdBench-largecomparedtothepreviousbestapproach,FT-cbow.
Thisshows thatVarCLRalignsmuchbetterwith humandevelop-
ersâ€™ assessment of variable similarity than any of the previously
proposed models. Interestingly, VarCLR-Avg also outperforms FT-
cbow by a large margin (+0.12 on IdBench-small). This suggests
that most of our gains do not come from the use of a more power-
fulencoderarchitecturesuchasBERT.Instead,weconcludethat
theGitHubRenamesdatasetiseffectiveatprovidingsupervision
signalsofvariablesimilarity,andthecontrastivelearningobjective
is effective. Although their architectures are very similar, VarCLR-
Avg outperformsFT-cbow.
That said, the improvements in VarCLR-CodeBERT (+0.06) and
VarCLR (+0.03) over VarCLR-Avg verify our assumption that pow-
erful models with larger representational capacity are necessary
for learning better variablerepresentations, since they are able to
capture and encode more information (e.g., sequential order and
tokenimportance)thanthe embedding averaging methods.
Table1bshowsthatVarCLRalsoachievesthestate-of-the-art
performance on IdBench in terms of relatedness prediction. It
surpasses the previous best by 0.07 on IdBench-small and 0.07
on IdBench-large. This is noteworthy because VarCLR training
does not explicitly optimize for relatedness. This suggests that the
VarCLR pre-training task learns better generic representations,
ratherthanoverfittingtothetargettask(i.e.,variablesimilarity).
Thisisveryimportant,andsupportsourmajorcontribution:Bypre-
trainingforthesimilaritylearningtaskonGitHubRenameswith
a contrastive objective, VarCLR achieves better representations
whichcan be applied to general tasks.Table 1: Spearmanâ€™s rank correlation with IdBench-small,
IdBench-medium, IdBench-largeofsinglemodels(top)and
ensemblemodels (bottom), by increasing performance.
(a) Similarity scores
Method Small Medium Large
FT-SG 0.30 0.29 0.28
LV 0.32 0.30 0.30
FT-cbow 0.35 0.38 0.38
VarCLR-Avg 0.47 0.45 0.44
VarCLR-LSTM 0.50 0.49 0.49
VarCLR-CodeBERT 0.53 0.53 0.51
Combined-IdBench 0.48 0.59 0.57
Combined-VarCLR 0.66 0.65 0.62
(b) Relatedness scores
Method Small Medium Large
LV 0.48 0.47 0.48
FT-SG 0.70 0.71 0.68
FT-cbow 0.72 0.74 0.73
VarCLR-Avg 0.67 0.66 0.66
VarCLR-LSTM 0.71 0.70 0.69
VarCLR-CodeBERT 0.79 0.79 0.80
Combined-IdBench 0.71 0.78 0.79
Combined-VarCLR 0.79 0.81 0.85
4.3 Variable Similarity Search
We next evaluate our learned representations in the context of
a more applied downstream application: similar variable search.
Similar variable search identifies similar variable names in a set of
namesgivenaninputquery.Thiscanbeusefulforrefactoringcode,
orforassigningvariablesmorereadablenames(e.g.,replacing fd
with file_descriptor ). For a given set of variables Vand a pre-
trained VarCLR encoder ğ‘“ğœƒ/prime, we compute representation vectors
K={ğ‘“ğœƒ/prime(ğ‘£)|ğ‘£âˆˆV}.Foraqueryvariable ğ‘¢,wefindtop- ğ‘˜similar
variablesin Vwith thehighestcosine similarityto ğ‘“ğœƒ/prime(ğ‘¢).
Toquantitativelyevaluateeffectivenessinfindingsimilarvari-
ables, we created a new mini-benchmark VarSim from the original
IdBenchbenchmark.Weselectvariablepairswhichhavehuman-
assessed similarity scores greater than 0.4 in IdBench. This leaves
uswith100â€˜similarâ€™variablepairsfromall291variablepairsinthe
IdBench-largebenchmark.Weusethevariablecollectionprovided
inIdBenchcontaining208,434variablesastheoverallcandidate
pool.WeuseHit@Kasourevaluationmetric,computingthecosine
similarityoftherepresentationsofaqueryvariable ğ‘¢andallthe
variables in the candidate pool. We select the top-K variables with
the highest similarity scores and check whether the corresponding
similar variable ğ‘£is in the top-K list. We choose K to be 1, 5, 10, 25,
50, 100,250,500, 1000.
Results.AsshowninFigure3a,VarCLR-CodeBERTachievesthe
best similaritysearch performance, with47% at K=100 and76% at
K=1000, compared to FT-cbow (37% at K=100, 68% at K=1000). This
2333ICSEâ€™22,May21â€“29,2022,Pittsburgh,PA, USA Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues
Table 2: Variable Similarity Search. Top-5 most similar variables found by the IdBench method and VarCLR-CodeBERT .
Variable Method Top 5 Similar Variables
substrFT-cbow substring substrs subst substring1 substrCount
VarCLR-CodeBERT subStr substring substrs stringSubstr substrCount
itemFT-cbow itemNr itemJ itemL itemI itemAt
VarCLR-CodeBERT pItem itemEl mItem itemEls itemValue
countFT-cbow countTbl countInt countRTO countsAsNum countOne
VarCLR-CodeBERT sCount countOf counts countInt countTh
rowsFT-cbow rowOrRows rowXs rows_1 rowsAr rowIDs
VarCLR-CodeBERT drows allrows rowsArray ows nRows
setIntervalFT-cbow resetInterval setTimeoutInterval clearInterval getInterval retInterval
VarCLR-CodeBERT pInterval mfpSetInterval setTickInterval clockSetInterval iInterval
minTextFT-cbow maxText minLengthText microsecText maxLengthText minuteText
VarCLR-CodeBERT minLengthText minContent maxText minEl min
filesFT-cbow filesObjs filesGen fileSets extFiles libFiles
VarCLR-CodeBERT filesArray aFiles allFiles fileslist filelist
minyFT-cbow min_y minBy minx minPt min_z
VarCLR-CodeBERT ymin yMin minY minYs minXy
1 10 50 250 1000
Top-K0%20%40%60%80%100%Hit@KVarCLR-Avg
VarCLR-LSTM
VarCLR-CodeBERT
FT-cbow
(a) Similarity Search
1 5 10 25 50 100
Top-K20%40%60%80%Hit@KAvg
LSTM
CodeBERT
(b) Spelling Error Correction
Figure 3: Hit@K score comparison on VarSimandVarTypo.
indicatesthatourmethodiseffectiveatfindingsimilarvariables,
able to distinguish the most similar variable to the query vari-
ableoutof200distractorsaround76%ofthetime.8Interestingly,
8Since we evaluate the Hit@1000 score in a candidate pool of size âˆ¼200,000, the
â€œresolutionâ€ of this retrieval task is1000
200000=1
200. Although inspecting the top 1000
may not be practical as an real-world application itself, it is still an informative metric
of the representation quality, and may indicate effectiveness in other settings, e.g.,VarCLR-Avg and VarCLR-LSTM are less effective at similarity
search than FT-cbow, even though they outperform FT-cbow by
a large margin in the similarity scoring task. Embedding-based
methods are still a strong baseline for variable similarity search.
However, contrastive methods still amplify the effectiveness of
unsupervised embedding methods.
Similarity scoring and similarity search are distinct tasks, and
so it is not unexpected that techniques will be equally effective
on both. For example, word2vec tends to put the embeddings
ofsimilarrarewordsclosetosomecommonfrequentword.This
behavior does not affect the similarity search effectiveness because
the rare words are able to find each other, and the frequent word is
closeenoughto itssimilarwordthantotheserarewords.However,
this will hurt similarity scoring between the rare words and the
frequentvariable,sincetheyareactuallynotsimilar.Incomparison,
VarCLR is able to avoid these kinds of scoring mistakes.
CaseStudy. Wedemonstrateourresultsqualitativelybychoos-
ingthesamesetofvariablesusedtodemonstratethistaskinthe
IdBenchpaper,anddisplayingthecomparativeresultsinFigure3a.
For space, we omit two of the variables ( rowsand count)i nt h e
set; the two methods perform comparably (such as on substr). We
observe that the overall qualities of the two methodsâ€™ results are
similar.Thisisunderstandablesincethegapbetweenthetwometh-
ods on variable similarity search is relatively small as shown in
Table 2.
Meanwhile, it is worth noting that VarCLR-CodeBERT is bet-
ter at penalizing distractive candidates that are only related but
not similar. For example, for minText, VarCLR-CodeBERT ranks
minLengthText ,minContent before maxText, while FT-cbow sug-
gests the opposite. For miny, VarCLR-CodeBERT ranks ymin,yMin,
minYastop-3,whileFT-cbowsuggestsrelatedbutdissimilarvari-
ablessuchas minByandminx.Thisprovidesadditionalevidencethat
a developer looking at the top 5 similar variables from a limited 1,000 candidates,
which has the same requirement on resolution. Another possible application is to use
VarCLR to retrieve a large candidate pool as the first stage to other methods, e.g.,
naturalvariablenamesuggestion.
2334VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
our method is able to better represent semantic similarity rather
than pure relatedness.
4.4 Variable Spelling Error Correction
Spelling Error Correction is a fundamental yet challenging task
innaturallanguageprocessing [ 37].Weexplorethepossibilityof
applyingVarCLRmodelstoperformspellingerrorcorrectionon
variable names. If the representations of misspelled variable names
are close to their correct versions, corrections may be found via
nearest neighbor search. Fortunately, the GitHubRenames dataset
enablesthisgoal,becauseaportionofrenamingeditsinGitHubRe-
names are actually correcting spelling errors in previous commits.
We can therefore reformulate this problem as a variable similarity
search task, since our method treats these misspelled names as
similarto their corrected versions.
We create a new synthetic variable spelling error correction
dataset, VarTypo, with 1023 misspelled variables and their cor-
rections. Specifically, we create this dataset by sampling variables
fromthe208,434variablepoolfromIdBench,andusethe nlpaug9
package[ 56]tocreatemisspelledvariablesfromthecorrectones.
We use KeyboardAug which simulates typo error according to
charactersâ€™distanceonthekeyboard.Thistaskischallengingbe-
cause our method does not leverage any external dictionary or
hand-craftedspellingrules.Meanwhile,althoughstringdistance
functions such as Levenshtein distance can potentially perform
better, these functions require expensive one-by-one comparisons
betweenthequeryvariableandeveryvariableinthepool,whichis
very time consuming, while our method uses GPU-accelerated ma-
trix multiplication to compute all cosine distances at once and can
potentially adopt an even more efficient vector similarity search li-
brarysuchas faiss.Therefore,webelieveitisstillaninformative
benchmark for evaluating variable representations.
Results.Similar to variable similarity search, we evaluate the
effectiveness as the Hit@K score of using the representation of
misspelled variables to retrieve the corresponding correct variable.
AsshowninFigure3b,VarCLRcansuccessfullycorrectthe29.4%of
thetimeatTop-1,and73.6%ofthetimeatTop-100.Oneinteresting
observation we find is that in this task, the gap (-4.5% at Top-1
and -1.0% at Top-100) between VarCLR-Avg and the other two
powerful encoders is relatively small. It even outperforms VarCLR-
CodeBERT after K=25. One possible explanation is that fixing a
typo requires neither word sequential order or word importance
information, i.e., being able to model the variable as a sequence
insteadof a bag of words does not benefit this task.
Case Study. For illustration, we randomly select misspelled vari-
able names and use our VarCLR to find the most similar correct
variablenames.AsshowninTable3.,ourmodelisabletocorrect
some of the misspelled variables, including insertions, deletions,
andmodifications,whilefailingtorecoverothers.Notably,variable
names consisting of multiple words such as minSimilarity can be
corrected successfully.
9https://github.com/makcedward/nlpaugTable3: Thetop-3mostsimilarvariablestomisspelledvariables,
foundby VarCLR.
Variable Top 3 Similar Variables
temepratures temperatures, temps, temlp
similar lity similarity, similarities, similar
minSimilar lity minSimilarity, similarity, minRatio
program _able programmable, program, program6
supervis ior superior, superview, superc
produc itons obligations, proportions, omegastructors
trans altion transac, trans, transit
4.5 AblationStudies
So far we have demonstrated the importance of both contrastive
learning and sophisticated models like CodeBERT for VarCLR per-
formance.Here,performablationstudiestomeasuretheeffectof
additionaldesigndecisionsinVarCLR:oftrainingdatasize,ofusing
pre-trained language models, and of using pre-trained embeddings
from unsupervised learning.
4.5.1 Effect of Data Size on Contrastive Pre-training. Pre-training
VarCLRrequiresweakly-superviseddatascrapedfrompublicrepos-
itories. Thus, we evaluate how much data is required to train an
effective model, to elucidate data collection costs. To evaluate this,
we train VarCLR-Avg, VarCLR-LSTM, VarCLR-CodeBERT on 0%,
0.1%, 1%, 3.16%, 10%, 21.5%, 46.4%, 100% percent of the full dataset,
measuringthe similarityscore on IdBench-medium.
Figure4showstheresults.ForallthreeVarCLRvariants,training
data size has a significant positive effect on effectiveness. This is
especiallytrueforVarCLR-CodeBERT,butperformanceflattens
andconvergesastrainingdatasizeapproaches100%.Thissuggests
that GitHubRenamesis of an appropriate size for this task.
Another interesting observation is that VarCLR-Avg outper-
forms VarCLR-LSTM with smalleramounts of training data. This
indicatesthemorepowerfulLSTMmodeldoesnotsurpassasimple
one until the data size reaches a critical threshold. This is likely
becauseamorecomplexmodelhasmoreparameterstotrainandre-
quires more data to reach convergence. With sufficient data, larger
models win, thanks to their representational capacity. This sug-
gests a caveat in applying representation learning models: it is
importanttochooseamodelwithanappropriatecomplexitygiven
the amount of available data, rather than defaulting to the best-
performing model overall.
4.5.2 Using a Pre-trained Language Model. Before contrastive pre-
training on GitHubRenames, VarCLR-CodeBERT is initialized
withamodel(pre-)pre-trainedonalargecodecorpus.Theeffectof
thispre-trainingisalsoillustratedinFigure4.AlthoughVarCLR-
CodeBERThasamuchlargernumberofparameters,itoutperforms
VarCLR-Avg and VarCLR-LSTM after contrastive pre-training on
only 1% of GitHubRenames. While this seems to contradict the
conclusion reached in the comparison between VarCLR-LSTM and
VarCLR-Avg, it displays the benefit of initialization with a pre-
trainedmodel.ComparedtoVarCLR-LSTM,whichcontainsran-
domlyinitializedparametersthathavetobetrainedfromscratch,
VarCLR-CodeBERT parameters produce reasonable representa-
tions from the start. Therefore, it requires less data to converge,
2335ICSEâ€™22,May21â€“29,2022,Pittsburgh,PA, USA Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues
0% 0.1% 1% 10% 100%
Percent of Trainin gData0.20.30.40.5Spearmanâ€™s Rank Correlation
Avg
LSTM
CodeBERT
Figure 4: Effect of contrastive pre-training data size on learned
VarCLRrepresentations, evaluated on IdBench-medium.
Table 4: Effect of pre-trained CodeBERT embeddings on similarity
score effectiveness (Spearmanâ€™s). Models are either randomly ini-
tialized and contrastively pre-trained (Contrastive), initialized with
CodeBERT embeddings (CodeBERT), or both ( VarCLR).
Method Small Medium Large
Contrastive-Avg 0.34 0.33 0.30
CodeBERT-Avg 0.44 0.43 0.40
VarCLR-Avg 0.47 0.45 0.44
Contrastive-LSTM 0.35 0.33 0.30
CodeBERT-LSTM 0.36 0.36 0.36
VarCLR-LSTM 0.50 0.49 0.49
and thanks to its large model capacity, ultimately outperforms the
othertwo variants by a large margin.
Despite the fast convergence, directly applying CodeBERT with-
out contrastive pre-training leads to poor performance (0.13 at
0% data). One possible reason is that CodeBERT was originally
trained for whole-program representations, and using it with vari-
able names as inputs leads to a problematic divergence from its
trainingdata distribution.
4.5.3 Effect of Pre-trained CodeBERT Embeddings. Both VarCLR-
AvgandVarCLR-LSTMareinitialized withthe wordembeddings
fromCodeBERTbeforecontrastivepre-training.Tostudytheeffect
of these pre-trained embeddings, we measure the Spearmanâ€™s cor-
relation coefficient of the similarity scores of the models modified
in two ways: one with randomly-initialized embeddings that is
thencontrastivelypre-trained(â€œContrastiveâ€inTable4),andone
thatisinitializedwithCodeBERTembeddingsbut notcontrastively
pre-trained (â€œCodeBERTâ€ in Table 4).
The results show that pre-trained CodeBERT embeddings are
essentialtotheperformanceofVarCLR-AvgandVarCLR-LSTM.
However, directly adopting the pre-trained embeddings alone is
stillinsufficient,especiallyforLSTMs.Thisimpliesthatbothun-
supervised pre-training and weakly supervised pre-training are
indispensableforusefulvariablerepresentations: the former takesadvantageof dataquantity byleveragingahugeamountofunla-
beleddata,whilethelattertakesadvantageof dataquality using
the weakly supervisedGitHubRenamesdataset.
5 RELATED WORK
VariableNamesandRepresentations. Variablenamesareimpor-
tant for source code readability and comprehension [ 25,46]. Be-
causeofthis,therehasbeenrecentworkfocusingonautomatically
suggesting clear, meaningful variable names for tasks such as code
refactoring [3, 5, 52] and reverse engineering [35, 45].
A common approach involves building prediction engines on
top of learned variable representations. Representation learning is
acommontaskinNaturalLanguageProcessing(NLP),andthese
techniquesareoftenadaptedtosourcecode.Simplerapproaches
model variable representations by applying word2vec [58]t o
codetokens[ 8,16,59,75],whilemoreadvancedtechniqueshave
adapted neural network architectures [ 41] or pre-trained language
models [12]. Source code representation is a common enough task
thatresearchershavedevelopedbenchmarksspecificallyforvari-
able [75]and program representations [76].
SimilarityandRelatedness. Afundamentalconcernwithexisting
variablerepresentationsandsuggestionenginesisthedifferencebe-
tween â€œrelatedâ€ and â€œsimilarâ€ variables [ 60,75]. â€œRelatedâ€ variables
referencesimilarcoreconceptswithoutconcernfortheirprecise
meaning,whileâ€œsimilarâ€variablesaredirectlyinterchangeable.For
example, minWeight andmaxWeight arerelatedbutnotsimilar,while
avgand meanare both. Unlike state-of-the-art techniques, which
only model relatedness, VarCLR explicitly optimizes for similarity
by adapting contrastive learning techniques from NLP and com-
puter vision research. In NLP, systems are often designed to focus
ontextrelatedness[ 10,23,85],similarity[ 32],orboth[ 2].While
document search might only be concerned with relatedness [ 23]
similarityis particularly important in systems designed for para-
phrasingdocuments [79, 81].
VarCLR relies on contrastive learning to optimize for similarity.
Contrastive learning isparticularly useful for learning visual repre-
sentations without any supervision data [ 11,13,14,26,30,72,82],
but has also been used for NLP [ 61]. Recent work has applied con-
trastive learning to the pre-training of language models to learn
text representations [ 17] and, similar to our task, learn sentence
embeddingsfortextualsimilaritytasks[ 24].Contrastivelearning
has also been used for code representation learning [ 36] where
source-to-source compiler transformation is applied for generating
different views of a same program. Different from this work, we
focus on learning representations for variable names, and leverage
additionaldatafrom GitHub for better supervision.
String similarity and spelling errors. Efficient string similarity
searchremainsanactiveresearcharea[ 6,19,49,87].Mostofthese
methodscanbecategorizedas sparseretrieval methods,focusing
ondistancefunctionsontheoriginalstringorn-grams.Theseal-
gorithmsdependonthelexicaloverlapbetweenstringsandthus
cannotcapturethesimilaritybetweenvariablespairssuchas avg
andmean. More recently, dense retrieval methods have been shown
effectiveinNLPtasks[ 39,47].Thesemethodsperformsimilarity
search inthespace oflearnedrepresentations,so thatsequences
2336VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
with similar meanings but low lexical overlap can be found. Mean-
while,extremelyefficientsimilaritysearchframeworksfordense
vectorssuchas faiss[38]canbeapplied.VarCLRintroducesthe
conceptofdenseretrievalintothevariablenamesdomain,enabling
moreeffectiveandefficientfindingofalistofcandidatesthatare
similarto a given variable name.
Neuralmodels forspelling errorcorrection usuallyrequire par-
allel training data which are hard to acquire in practice [ 28,86].
Recentworkadoptsdifferentmechanismstocreatesyntheticparal-
leldata,includingnoiseinjection[ 37],andback-translationmod-
els[27].Weleaveadetailedcomparisontofuturework,butnote
that VarCLR shows promise without expensive training data.
Name-andMachineLearning-basedProgramAnalyses. Ourdown-
stream tasks are examples of program analyses based on infor-
mationgatheredwithmachinelearning(ML).Name-basedbased
program analyses predicated on machine learning have been used
in many contexts. In the context of code modification, they have
been used for variable name suggestion from code contexts [ 5],
method and class name rewriting [ 52] and generation [ 4], code
generation directly from docstrings [ 12], and automated program
repair[15,77].Theyhavealsobeenusedfortypeinferencefrom
naturallanguageinformation[ 57,83],detectingbugs[ 40,64,65,68],
anddetectingvulnerabilities[ 29].VarCLRcanserveasadrop-in
pre-trainingstepfor suchtechniques,enablingmoreeffective use
ofthesemanticinformationcontainedinvariablenamesforawide
range of such analyses.
6 DISCUSSION
In this paper, we study variable representation learning, a problem
with significant implications for machine learning and name-based
programanalyses.Wepresentanovelmethodbasedoncontrastive
learning for pre-training variable representations. With our new
weakly-supervisedGitHubRenamesdataset,our methodenables
theuseofstrongerencoderarchitecturesinplaceof word2vec -
based methods for this task, leading to better generalized repre-
sentations.OurexperimentsshowthatVarCLRgreatlyimproves
representation quality not only in terms of variable similarity, but
alsoforotherdownstreamtasks.Whilethesedownstreamtasksmay
notbeimmediatelypracticalthemselves,ourapproachispromising
as a drop-in pre-training solution for other variable name-based
analysistasks,whichwehopeotherswillattemptinfuturework.
For example, VarCLR can replace the the word2vec-CBOW em-
beddings used in a name-based bug detector [ 64], or the n-gram
based language model used as a similarity scoring function for
name suggestion [ 3]. Existing dictionary-based IDE spell-checkers
may also benefit from using VarCLR to rank suggestions based on
the pretrained semantic similarity.
Wenotelimitationsandpossiblethreatsinourstudy.Ourdataset
is automatically constructed from git commits from GitHub, and
likely contains noise that can harm contrastive learning perfor-
mance [50]. However, our results show that despite this noise, our
models transfer well, and our evaluation is based on an entirely
distinct test set. Knowledge distillation and self-training meth-
ods [21,33] such as momentum distillation [ 50] can be applied
to deal with the noise in weak supervision data [50, 71].In this work, we applied VarCLR exclusively to unsupervised
downstreamtasks.Fine-tuningVarCLRmodelswithlabeleddata
might further enable significant performance improvements for
more complicated tasks, like natural variable name suggestion [ 3].
Beyond constructing similar variable names, it is also conceptually
possibletoconstructsimilarpairsof largercodesnippetsfromgit
diffs describing patches. Applying contrastive learning on these
pairscanpotentiallyimproveCodeBERTcode representationand
understanding, which could benefit tasks well beyond variable
similarity, such as code search. Finally, we used instance discrimi-
nation[82]toguideourcontrastivelearningapproach,withpromis-
ing results. This suggests that more advanced contrastive learning
methodssuchasMoCo[ 30],BYOL[ 26],SwAV[ 11]beadaptedto
this taskforbetter representation learning in general.
ACKNOWLEDGMENTS
The authors would like to thank Michael Pradel and the authors
of IdBenchforproviding us with data for our experiments. This
material is based upon work supported in part by the National
ScienceFoundation(awards 1815287 and 1910067).
REFERENCES
[1]Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel,
Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al.2020. Towards a human-like open-domain chatbot. arXiv preprint
arXiv:2001.09977 (2020).
[2]Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and
Aitor Soroa. 2009. A study on similarity and relatedness using distributional and
wordnet-based approaches. (2009).
[3]Miltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2014. Learn-
ingNaturalCodingConventions.In SymposiumontheFoundationsofSoftware
Engineering (FSE) . 281â€“293.
[4]MiltiadisAllamanis,EarlTBarr,ChristianBird,andCharlesSutton.2015. Sug-
gestingaccuratemethodandclassnames.In Proceedingsofthe201510thJoint
Meeting on Foundations of Software Engineering . 38â€“49.
[5]R.Bavishi,M.Pradel,andK.Sen.2017. Context2Name:ADeepLearning-Based
Approach to Infer Natural Variable Names from Usage Contexts . Technical Report.
TU Darmstadt, Department of Computer Science.
[6]Roberto J Bayardo, Yiming Ma, and Ramakrishnan Srikant. 2007. Scaling up
allpairssimilaritysearch.In Proceedingsofthe16thinternationalconferenceon
World Wide Web . 131â€“140.
[7]Dave Binkley, Marcia Davis, Dawn Lawrie, Jonathan I Maletic, Christopher
Morrell, and Bonita Sharif. 2013. The impact of identifier style on effort and
comprehension. EmpiricalSoftware Engineering 18,2 (2013), 219â€“276.
[8]Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-
riching Word Vectors with Subword Information. Transactions of the Association
forComputationalLinguistics 5 (2017), 135â€“146.
[9]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 (2020).
[10]EliaBruni,Nam-KhanhTran,andMarcoBaroni.2014. Multimodaldistributional
semantics. Journalof artificial intelligence research 49 (2014), 1â€“47.
[11]Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin. 2020. Unsupervised learning of visual features by contrasting
clusterassignments. arXivpreprint arXiv:2006.09882 (2020).
[12]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared
Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al .2021.
Evaluatinglargelanguagemodelstrainedoncode. arXivpreprintarXiv:2107.03374
(2021).
[13]TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton.2020. A
simpleframeworkfor contrastive learning ofvisual representations. In Interna-
tionalconferenceon machine learning . PMLR, 1597â€“1607.
[14]Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representation
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition . 15750â€“15758.
[15]Zimin Chen and Martin Monperrus. 2018. The Remarkable Role of Similarity in
Redundancy-based Program Repair. arXivprepring arXiv:1811.05703 (2018).
2337ICSEâ€™22,May21â€“29,2022,Pittsburgh,PA, USA Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues
[16]Zimin Chenand MartinMonperrus.2019. Aliterature study ofembeddings on
source code. arXivpreprint arXiv:1904.03061 (2019).
[17]Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019.
ELECTRA:Pre-trainingTextEncodersasDiscriminatorsRatherThanGenerators.
InInternationalConferenceon Learning Representations .
[18]FredJ.Damerau.1964. ATechniqueforComputerDetectionandCorrectionof
Spelling Errors. Commun.ACM (1964),171â€“176.
[19]Dong Deng, Guoliang Li, Jianhua Feng, and Wen-Syan Li. 2013. Top-k string
similaritysearchwithedit-distanceconstraints.In 2013IEEE29thInternational
Conferenceon Data Engineering (ICDE) . IEEE, 925â€“936.
[20]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand
ShortPapers) .AssociationforComputationalLinguistics,Minneapolis,Minnesota,
4171â€“4186. https://doi.org/10.18653/v1/N19-1423
[21]JingfeiDu,Ã‰douardGrave,BelizGunel,VishravChaudhary,OnurCelebi,Michael
Auli,VeselinStoyanov,andAlexisConneau.2021. Self-trainingImprovesPre-
trainingfor NaturalLanguageUnderstanding.In Proceedings of the 2021 Confer-
enceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies . 5408â€“5418.
[22]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages.In Proceedings of the
2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings .
1536â€“1547.
[23]Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan,
Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept
revisited.In Proceedingsofthe10thinternationalconferenceonWorldWideWeb .
406â€“414.
[24]TianyuGao,XingchengYao,andDanqiChen.2021. SimCSE:SimpleContrastive
Learningof Sentence Embeddings. arXivpreprint arXiv:2104.08821 (2021).
[25]EdwardM.GellenbeckandCurtisR.Cook.1991. AnInvestigationofProcedure
andVariableNamesasBeaconsDuringProgramComprehension . TechnicalReport.
Oregon State University.
[26]Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre H
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-
hanDanielGuo,MohammadGheshlaghiAzar,etal .2020. Bootstrapyourownla-
tent:Anewapproachtoself-supervisedlearning. arXivpreprintarXiv:2006.07733
(2020).
[27]JinxiGuo,TaraNSainath,andRonJWeiss.2019. Aspellingcorrectionmodelfor
end-to-ends peechrecognition.In ICASSP2019-2019IEEEInternationalConference
on Acoustics, S peechand SignalProcessing(ICASSP) . IEEE, 5651â€“5655.
[28]MasatoHagiwaraandMasatoMita.2020. GitHubTypoCorpus:ALarge-Scale
MultilingualDatasetofMisspellingsandGrammaticalErrors. In Proceedingsof
the12thLanguageResourcesandEvaluationConference . 6761â€“6768.
[29]Jacob A Harer, Louis Y Kim, Rebecca L Russell, Onur Ozdemir, Leonard R Kosta,
Akshay Rangamani, Lei H Hamilton, Gabriel I Centeno, Jonathan R Key, Paul M
Ellingwood,etal .2018. Automatedsoftwarevulnerabilitydetectionwithmachine
learning. arXivpreprint arXiv:1803.04497 (2018).
[30]KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossGirshick.2020. Momen-
tumcontrastforunsupervisedvisualrepresentationlearning.In Proceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition .9729â€“9738.
[31]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
visionandpatternrecognition . 770â€“778.
[32]Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating
semantic models with (genuine) similarity estimation. Computational Linguistics
41,4 (2015), 665â€“695.
[33]Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXivpreprint arXiv:1503.02531 (2015).
[34]SeppHochreiterandJÃ¼rgenSchmidhuber.1997. Longshort-termmemory. Neural
Computation 9, 8 (1997), 1735â€“1780.
[35]AlanJaffe,JeremyLacomis,EdwardJ.Schwartz,ClaireLeGoues,andBogdan
Vasilescu.2018. MeaningfulVariableNamesforDecompiledCode:AMachine
Translation Approach. In International Conference on Program Comprehension
(ICPCâ€™18) . 20â€“30.
[36]Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E Gonzalez, and
Ion Stoica. 2020. Contrastive code representation learning. arXiv preprint
arXiv:2007.04973 (2020).
[37]Sai Muralidhar Jayanthi, Danish Pruthi, and Graham Neubig. 2020. NeuSpell:
A Neural Spelling Correction Toolkit. In Proceedings of the 2020 Conference on
Empirical Methodsin NaturalLanguage Processing:System Demonstrations . 158â€“
164.
[38]Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity
search with gpus. IEEETransactions on Big Data (2019).[39]Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) . 6769â€“6781.
[40]Sayali Kate, John-Paul Ore, Xiangyu Zhang, Sebastian Elbaum, and Zhaogui Xu.
2018. Phys:probabilisticphysicalunitassignmentandinconsistencydetection.In
Proceedingsofthe201826thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering . 563â€“573.
[41]DeborahS.Katz,JasonRuchti,andEricSchulte.2018. UsingRecurrentNeural
NetworksforDecompilation.In InternationalConferenceonSoftwareAnalysis,
Evolution and Reegnineering (SANER â€™18) . 346â€“356.
[42]PrannayKhosla,PiotrTeterwak,ChenWang,AaronSarna,YonglongTian,Phillip
Isola,AaronMaschinot,CeLiu,andDilipKrishnan.2020. Supervisedcontrastive
learning. arXivpreprint arXiv:2004.11362 (2020).
[43]Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXivpreprint arXiv:1412.6980 (2014).
[44]Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
GraphConvolutionalNetworks.In ICLRâ€™17.
[45]JeremyLacomis, PengchengYin,Edward Schwartz, Miltiadis Allamanis, Claire
Le Goues, Graham Neubig, and Bogdan Vasilescu. 2019. Dire: A neural approach
to decompiled identifier naming. In 2019 34th IEEE/ACM International Conference
on Automated Software Engineering (ASE) . IEEE, 628â€“639.
[46]DawnLawrie,ChristopherMorrell,HenryFeild,andDavidBinkley.2006. Whatâ€™s
inaName?AStudyofIdentifiers.In InternationalConferenceonProgramCom-
prehension (ICPC â€™06) . 3â€“12.
[47]Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent re-
trievalforweaklysupervisedopendomainquestionanswering. arXivpreprint
arXiv:1906.00300 (2019).
[48]Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation,andComprehension.In Proceedingsofthe58thAnnualMeetingof
theAssociation for Computational Linguistics . 7871â€“7880.
[49]ChenLi,JiahengLu,andYimingLu.2008. Efficientmergingandfilteringalgo-
rithmsforapproximatestringsearches.In 2008IEEE24thInternationalConference
on Data Engineering . IEEE, 257â€“266.
[50]JunnanLi,RamprasaathRSelvaraju,AkhileshDeepakGotmare,ShafiqJoty,Caim-
ing Xiong, and Steven Hoi. 2021. Align before Fuse: Vision and Language Repre-
sentation Learning with Momentum Distillation. arXiv preprint arXiv:2107.07651
(2021).
[51]HuiLiu,QiurongLiu,Cristian-AlexandruStaicu,MichaelPradel,andYueLuo.
2016. Nomen est omen: Exploring and exploiting similarities between argument
and parameter names. In Proceedings of the 38th International Conference on
Software Engineering . 1063â€“1073.
[52]Kui Liu, Dongsun Kim, TegawendÃ© F BissyandÃ©, Taeyoung Kim, Kisub Kim, Anil
Koyuncu,SuntaeKim,andYvesLeTraon.2019. Learningtospotandrefactor
inconsistent method names.In 2019 IEEE/ACM 41st InternationalConferenceon
Software Engineering (ICSE) . IEEE, 1â€“12.
[53]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[54]IlyaLoshchilovandFrankHutter.2018. DecoupledWeightDecayRegularization.
InInternationalConferenceon Learning Representations .
[55]ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
Blanco,ColinClement,DawnDrain,DaxinJiang,DuyuTang,GeLi,LidongZhou,
Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan,
NeelSundaresan,ShaoKunDeng,ShengyuFu,andShujieLIU.2021.CodeXGLUE:
AMachineLearningBenchmarkDatasetforCodeUnderstandingandGeneration.
InThirty-fifth Conference on Neural Information Processing Systems Datasets and
BenchmarksTrack (Round 1) . https://openreview.net/forum?id=6lE4dQXaUcb
[56]Edward Ma. 2019. NLP Augmentation. https://github.com/makcedward/nlpaug.
[57]Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: inferring
JavaScript function types from natural language information. In 2019 IEEE/ACM
41stInternationalConferenceon Software Engineering (ICSE) . IEEE, 304â€“315.
[58]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffery Dean. 2013.
Distributed Representations of Words and Phrases and their Compositioniality.
ComputingResearch Repository (CoRR) abs/1310.4546(2013).
[59]TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems . 3111â€“3119.
[60]George AMillerand WalterG Charles.1991. Contextual correlatesof semantic
similarity. Languageandcognitive processes 6, 1 (1991), 1â€“28.
[61]Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings ef-
ficiently with noise-contrastive estimation. In Advances in neural information
processing systems . 2265â€“2273.
[62]Saul B Needleman and Christian D Wunsch. 1970. A general method applicable
to the search for similarities in the amino acid sequence of two proteins. Journal
2338VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
of molecular biology 48,3 (1970), 443â€“453.
[63]Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
withcontrastive predictive coding. arXivpreprint arXiv:1807.03748 (2018).
[64]Michael Pradel and Thomas R Gross. 2011. Detecting anomalies in the order
of equally-typed method arguments. In Proceedings of the 2011 International
Symposiumon Software Testing and Analysis . 232â€“242.
[65]Michael Pradel and Koushik Sen. 2018. Deepbugs: A learning approach to name-
based bug detection. Proceedings of the ACM on Programming Languages 2,
OOPSLA(2018),1â€“25.
[66]Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,
Kuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph
neuralnetworkpre-training.In Proceedingsofthe26thACMSIGKDDInternational
Conferenceon Knowledge Discovery & Data Mining . 1150â€“1160.
[67]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits
ofTransferLearningwithaUnifiedText-to-TextTransformer. JournalofMachine
LearningResearch 21 (2020), 1â€“67.
[68]Andrew Rice, Edward Aftandilian, Ciera Jaspan, Emily Johnston, Michael Pradel,
and Yulissa Arroyo-Paredes. 2017. Detecting argument selection defects. Pro-
ceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1â€“22.
[69]DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams.1986. Learning
representations by back-propagating errors. nature323,6088(1986),533â€“536.
[70]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
TranslationofRareWordswithSubwordUnits.In Proceedingsofthe54thAnnual
Meeting ofthe Associationfor Computational Linguistics(Volume1: Long Papers) .
1715â€“1725.
[71]Antti Tarvainen and HarriValpola.2017. Mean teachersare better role models:
Weight-averaged consistency targets improve semi-supervised deep learning
results.Advances in Neural Information Processing Systems 30 (2017).
[72]Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiview
coding.In ComputerVisionâ€“ECCV2020:16thEuropean Conference,Glasgow,UK,
August 23â€“28, 2020, Proceedings, Part XI 16 . Springer, 776â€“794.
[73]YonglongTian,ChenSun,BenPoole,DilipKrishnan,CordeliaSchmid,andPhillip
Isola. 2020. What makes for good views for contrastive learning? arXiv preprint
arXiv:2005.10243 (2020).
[74]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
y o uN eed .I n Advances in Neural Information Processing Systems 30: Annual Con-
ferenceon Neural Information Processing Systems 2017 . 6000â€“6010.
[75]YazaWainakh,MoizRauf,andMichaelPradel.2021. IdBench:EvaluatingSeman-
ticRepresentationsofIdentifierNamesinSourceCode.In 2021IEEE/ACM43rd
InternationalConferenceon Software Engineering (ICSE) . IEEE, 562â€“573.
[76]KeWangandMihaiChristodorescu.2019. Coset:Abenchmarkforevaluating
neuralprogram embeddings. arXivpreprint arXiv:1905.11445 (2019).
[77]Martin White, Michele Tufano, MatÃ­as MartÃ­nez, Martin Monperrus, and Denys
Poshyvanyk.2018. Sortingand Transforming Program Repair Ingredients via
Deep Learning Code Similarities. arXivpreprint arXiv:1707.04742 (2018).
[78]Martin White, Michele Tufano, Matias Martinez, Martin Monperrus, and Denys
Poshyvanyk.2019. Sortingandtransformingprogramrepairingredientsviadeep
learning code similarities. In 2019 IEEE 26th International Conference on Software
Analysis, Evolution and Reengineering (SANER) . IEEE, 479â€“490.
[79]JohnWieting,MohitBansal,KevinGimpel,andKarenLivescu.2015. Towards
universal paraphrastic sentence embeddings. arXiv preprint arXiv:1511.08198
(2015).
[80]JohnWieting,KevinGimpel,GrahamNeubig,andTaylorBerg-Kirkpatrick.2019.
SimpleandEffectiveParaphrasticSimilarityfromParallelTranslations.In Proceed-
ingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics .
4602â€“4608.
[81]JohnWieting,KevinGimpel,GrahamNeubig,andTaylorBerg-Kirkpatrick.2021.
ParaphrasticRepresentationsat Scale. arXivpreprint arXiv:2104.15114 (2021).
[82]Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised
featurelearning vianon-parametricinstancediscrimination.In Proceedingsof
theIEEEconferenceon computer vision and pattern recognition . 3733â€“3742.
[83]ZhaoguiXu,XiangyuZhang,LinChen,KexinPei,andBaowenXu.2016. Python
probabilistic type inference with natural language support. In Proceedings of
the201624thACMSIGSOFTinternationalsymposiumonfoundationsofsoftware
engineering . 607â€“618.
[84]PengchengYin,GrahamNeubig,MiltiadisAllamanis,MarcBrockschmidt,and
AlexanderLGaunt.2018. LearningtoRepresentEdits.In InternationalConference
on Learning Representations .
[85]Torsten Zesch, Christof MÃ¼ller, and Iryna Gurevych. 2008. Using Wiktionary for
ComputingSemanticRelatedness..In AAAI, Vol. 8. 861â€“866.
[86]ShaohuaZhang,HaoranHuang,JicongLiu,andHangLi.2020. SpellingError
Correction with Soft-Masked BERT. In Proceedings of the 58th Annual Meeting of
theAssociation for Computational Linguistics . 882â€“890.
[87]Zhenjie Zhang, Marios Hadjieleftheriou, Beng Chin Ooi, and Divesh Srivastava.
2010. Bed-tree: an all-purpose index structure for string similarity search based
oneditdistance.In Proceedingsofthe2010ACMSIGMODInternationalConferenceon Management of data . 915â€“926.
[88]Hao Zhong, Tao Xie, Jian Pei, and Hong Mei. 2009. MAPO: Mining and Rec-
ommending API Usage Patterns.In European Conference on Object-Oriented Pro-
gramming (ECOOP) . 318â€“343.
2339