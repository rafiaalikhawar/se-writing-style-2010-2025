WhatImproves Developer ProductivityatGoogle?
CodeQuality
LanCheng
Google
UnitedStates
lancheng@google.comEmersonMurphy-Hill
Google
UnitedStates
emersonm@google.comMarkCanning
Google
UnitedStates
argusdusty@google.com
Ciera Jaspan
Google
UnitedStates
ciera@google.comCollinGreen
Google
UnitedStates
colling@google.comAndrea Knight
Google
UnitedStates
aknight@google.com
NanZhang
Google
UnitedStates
nanzh@google.comElizabeth Kammer
Google
UnitedStates
eakammer@google.com
ABSTRACT
Understanding what affects software developer productivity can
help organizations choose wise investments in their technical and
social environment. But the research literature either focuses on
whatcorrelateswithdeveloperproductivityinecologicallyvalid
settingsorfocusesonwhatcausesdeveloperproductivityinhighly
constrainedsettings.Inthispaper,webridgethe gapbystudying
software developers at Google through two analyses. In the first
analysis,weusepaneldatawith39productivityfactors,findingthat
code quality, technical debt, infrastructure tools and support, team
communication, goals and priorities, and organizational change
andprocessareallcausallylinkedtoself-reporteddeveloperpro-
ductivity. In the second analysis, we use a lagged panel analysis to
strengthenourcausalclaims.Wefindthatincreasesinperceived
code quality tend to be followed by increased perceived developer
productivity,butnotviceversa,providingthestrongestevidence
todate that code quality affects individualdeveloperproductivity.
CCSCONCEPTS
Â·Softwareanditsengineering â†’Softwarecreationandman-
agement ;Software development process management ;Â·General
and reference â†’Empirical studies .
KEYWORDS
Developerproductivity,code quality,causation,paneldata
ACM ReferenceFormat:
Lan Cheng, Emerson Murphy-Hill, Mark Canning, Ciera Jaspan, Collin
Green, Andrea Knight, Nan Zhang, and Elizabeth Kammer. 2022. What
ImprovesDeveloperProductivityatGoogle?CodeQuality.In Proceedings
ESEC/FSE â€™22,November 14Å›18, 2022, Singapore, Singapore
Â©2022 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3558940ofthe30thACMJointEuropeanSoftwareEngineeringConferenceandSympo-
siumontheFoundationsofSoftwareEngineering(ESEC/FSEâ€™22),November
14Å›18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 12pages.
https://doi.org/10.1145/3540250.3558940
1 INTRODUCTION
Organizationswanttomaximizesoftwareengineeringproductivity
so that they can make the best software in the shortest amount
oftime.WhilesoftwareengineeringproductivityÅ‚isessentialfor
numerousenterprisesandorganizationsinmostdomainsÅ¾[ 50]and
canbeexaminedthroughmultiplelenses[ 29],understandingthe
productivity of individual software developers can be especially
fruitful because it has the potentialto be improved through many
actions(e.g.fromtoolingtoprocesschanges)andbymanystake-
holders(fromindividualdeveloperstoexecutives).However,itis
difficulttoknowwhichactions willtrulyimproveproductivityin
an ecologically valid setting,thatis,in a way thataccurately char-
acterizesproductivityinarealisticsoftwaredevelopmentcontext.
This motivates ourresearch question:
RQ:Whatcausesimprovementstodeveloperproduc-
tivity in practice?
A wide spectrum of prior research provides some answers to
this question, but with significant caveats. For example, at one end
of the research spectrum, Ko and Myersâ€™ controlled experiment
showed that a novel debugging tool called Whyline helped Java de-
velopers fix bugs twice as fast as those using traditional debugging
techniques [ 30]. While this evidence is compelling,organizational
leadersarefacedwithmanyopenquestionsaboutapplyingthese
findingsinpractice,suchaswhetherthedebuggingtasksperformed
in that study are representative of the debugging tasks performed
by developers in their organizations. At the other end of the spec-
trum,Murphy-Hillandcolleaguessurveyeddevelopersacrossthree
companies,findingthatjobenthusiasmconsistentlycorrelatedwith
highself-ratedproductivity[ 36].Butyetagain,anorganizational
leaderwouldhaveopenquestionsabouthowtoapplytheseresults,
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1302
ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore L.Cheng,E. Murphy-Hill,M. Canning,C.Jaspan, C.Green, A.Knight, N.Zhang,E. Kammer
Table 1:Hypotheticalcross sectionalsurvey response data.
Productivity Rating CodeQualityRating
ArujSomewhat productive Medium quality
Rusla Extremelyproductive Extremelyhigh quality
Table 2: More hypothetical survey responses, collected 3
monthsafterthedatainTable 1.Plusses(+)andminuses(Å›)
indicatethedirection ofthechange sincethepriorsurvey.
Productivity Rating CodeQualityRating
ArujHighly productive (+) High Quality (+)
Rusla Somewhat productive (Å›) High Quality (Å›)
such as whether some unmeasured third variable causes bothhigh
productivityandhigh jobenthusiasm.
More broadly, theseexamples illustrate thefundamental limita-
tions of prior approaches to understanding developer productivity.
Ononehand,softwareengineeringresearchthatusescontrolledex-
periments canhelpshowwithahigh degreeofcertaintythatsome
practices and tools increase productivity, yet such experiments are
by definition highly controlled, leaving organizations to wonder
whether the results obtained in the controlled environment will
alsoapplyintheirmorerealistic,messyenvironment.Ontheother
hand,researchthatusesfieldstudiesÅ›oftenwithcrosssectional
datafromsurveysortelemetryÅ›canproducecontextuallyvalidob-
servations about productivity, but drawing causal inferences from
fieldstudiesthatrivalthosedrawnfromexperimentsischallenging.
Our studybuilds onthe existingliterature aboutdeveloper pro-
ductivity, contributing the first study to draw strong causal conclu-
sions in an ecologically valid context about what affects individual
developer productivity.
2 MOTIVATION
The paperâ€™s main technical contribution Å› the ability to draw
stronger causal inferences about productivity drivers than in prior
workÅ›isenabledbytheuseofthe paneldataanalysis technique[ 25].
In this section, we motivate the technique with a running example.
Much of the prior work on developer productivity (Section 3)
reliesoncross-sectionaldata .Toillustratethelimitationsofcrosssec-
tionaldata,letusintroduceanexample.Considerasurveythatasks
aboutrespondentsâ€™productivityandthequalityoftheircodebase.
Thesurveyisdistributedatalargecompany,andtwodevelopers
respond, Aruj and Rusla. Letâ€™s assume their responses are repre-
sentative of thedeveloper population. Their survey responses are
showninTable 1.
From this survey, we see that productivity correlates with code
quality.Butwecannotconfidentlysaythathighcodequalitycauses
high developer productivity, due in part to the following confound-
ingexplanations [ 1]:
â€¢Time-invariant effects. These are effects that have the
same influenceover time.For example,if Rusla went to col-
lege and Aruj did not, from cross-sectional data, we cannot
distinguish between the effect of college and the effect of
code qualityonproductivity.â€¢Respondent-independenttimeeffects. Theseareeffects
that influence all respondents uniformly, such as seasonal
effects or company-wide initiatives. For example, prior to
the survey, perhaps all engineers were given their annual
bonus, artificiallyraising everyoneâ€™sproductivity.
â€¢Non-differentiated response effects. These are effects
whererespondentswillgivethesameorsimilarresponsesto
everysurveyquestion,sometimesknownasstraightlining.
For example, perhaps Aruj tends to choose the middle op-
tiontoeveryquestionandRuslatendstoanswerthehighest
optionfor every question.
We use panel analysis to address these confounds, enabling
strongercausalinferencethanwhatcanbeobtainedfromcrosssec-
tionaldata[ 25].Thepowerofpaneldataisthatitusesdatacollected
at multiple points in time from the same individuals, examining
howmeasurements changeover time.
Toillustratehowpaneldataenablesstrongercausalinference,
letusreturntotherunningexample.Supposewerunthesurvey
again, three months later,andobtain the data showninTable 2.
OneinterestingobservationisthatifweanalyzeTable 2inisola-
tion, we notice that thereâ€™snot acorrelationbetween productivity
and codequalityÅ› bothrespondentsreportthesamecodequality,
regardlessoftheirproductivity.Butmoreimportantly,lookingat
thechangesin responsesfrom Table 1andTable 2,we see produc-
tivitychangesarenowcorrelated:Arujâ€™sincreasingproductivity
correlates with increasing code quality, and Ruslaâ€™s decreasing pro-
ductivitycorrelates withdecreasing code quality.
Panel analysis rules out the three confounding explanations
present inthe cross-sectionalanalysis:
â€¢In the cross-sectional analysis, we could not determine if
Ruslaâ€™s high productivity was driven by her college edu-
cation. But in this panel analysis, we can rule out that ex-
planation,becausecollegeisatimeinvariantexposureÅ›it
theoreticallywouldhavethesameeffectonherproductivity
in the first survey as in the second survey. This ability to
rule out other potential causes that are time invariant exists
whether or not the researcher can observe those potential
causes.Whilewithcross-sectionalanalysis,researchersmay
be able to control for these potential causes using control
variables, researchers have to anticipate and measure those
controlvariablesduringanalysis.Thisisunnecessarywith
panel analysis because time invariant factors are eliminated
bydesign.
â€¢In the cross-sectional analysis, we could not determine if
productivitywasdrivenbyarecentannualbonus.Thisex-
planation is ruled out in the panel analysis. If the annual
bonushadaneffect,thechangeinproductivityscoresacross
both participants wouldbe uniform.
â€¢In the cross-sectional analysis, we could not determine if
respondents were just choosing similar answers to every
question. This explanation is also ruled out with panel anal-
ysis. If respondents were choosing similar answers, there
would be no change in productivity scores, and thus we
wouldnot see acorrelation among the changes.
1303What ImprovesDeveloperProductivityatGoogle? Code Quality ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
The ability of panel analyses to draw relatively strong causal
inferences makes it a quasi-experimental method, combining some
ofthe advantagesofexperiments withthoseoffield studies[ 24].
3 RELATED WORK
Toanswerresearchquestionssimilartoours,severalresearchers
previously investigated what factors correlate with developer pro-
ductivity.Petersenâ€™ssystematicmappingliteraturereviewdescribes
seven studies that quantify factors that predict software developer
productivity [ 39], factors largely drawn from the COCOMO II soft-
ware cost driver model [ 10]. For instance, in a study of 99 projects
from 37 companies, Maxwell and colleagues found that certain
toolsandprogrammingpracticescorrelatedwithprojectproduc-
tivity, as measured by the number of lines of written code per
month [33]. More broadly, a recent study explored what factors
correlate withindividualdevelopersâ€™ self-reportedproductivityat
threecompanies[ 36].Incontrasttoourstudy,thesepriorstudies
report correlations withrelativelyweakcausalclaims.
Otherresearchershavebeenabletomakestrongercausalclaims
aboutprogrammerproductivitybyrunningcontrolledexperiments.
For instance, when Tosun and colleagues asked 24 professionals to
completeasimpleprogrammingtask,eitherusingtest-drivendevel-
opment(treatmentgroup)oriterativetest-lastdevelopment(control
group),theyfoundthattreatmentgroupparticipantsweresignif-
icantly more productive than control group participants, where
productivity was measured as the number of tests passed in a fixed
amount oftime [ 49]. Suchrandomized controlled experiments are
consideredaÅ‚goldstandardÅ¾becausetheycanmakeverystrong
causalclaims[ 8]. The challengewithsuch studies isthat theyare
expensive to run with high ecological validity. Consequently, such
studies typically use students as participants rather than profes-
sionals(e.g.[ 13,44,46]),usesmallproblemsandprogramsrather
than more realistic ones (e.g. [ 5,35,46]), and can only vary one or
two productivity factors per experiment (e.g. [ 14,31,42]). While
the studypresentedhere cannotmakeasstrongcausalclaimsas
experiments, the present field study has higher ecological validity
thanexperimental studies.
Toaddressthesechallenges,softwareengineeringresearchers
have been using causal inference techniques in field studies, where
stronger causal claims can be made than in studies with simple
correlations. The core of such studies is analyses that leverage
time series data, rather than cross-sectional data. For instance,
Wang and colleagues use Grangerâ€™s causality test [ 20] to infer that
womenâ€™spullrequestscauseincreasesinthosewomenâ€™sfollower
counts [51]. As another example, using the Bayesian CausalImpact
framework[ 6],Martinandcolleaguesshowthat33%ofappreleases
caused statistically significant changes to app user ratings [ 32].
Thesepapersusedfine-grainedtimeseriesdata,whichisnotpossi-
bleforthetypeofdatadescribedinthispaper,andtoourknowledge,
has not been appliedto studiesofdeveloper productivity.
Panel analysis, another causal inference technique, has been
usedbypriorsoftwareengineeringresearchers.Qiuandcolleagues
used GitHub panel data to show that Å‚social capital impacts the
prolongedengagementofcontributorstoopensourceÅ¾[ 40].Islam
and colleagues used panel data to show that distributed version
controlsystemsÅ‚reducetheprivatecostsforparticipantsinanOSSprojectandthusincreasesthenumberofparticipants,butdecreases
theaveragelevelofcontributionbyindividualparticipantsÅ¾[ 26].
Likethesepapers,weusepaneldatatomakecausalinferences,but
inour case,the inferences are aboutdeveloper productivity.
4 PANEL ANALYSIS: METHODS
Towards answering our research question, we next describe our
data sources, dependent variables, independent variables, panel
data,andmodelingdesign.
4.1 Data Sources
The data of this study comes from two sources: Google engineersâ€™
logs data and a company-wide survey at Google. Neither source
wasbuiltspecificallyfortheresearch wedescribehere,andsowe
considerthis opportunisticresearch.
4.1.1 Logs Data. We collected a rich data set from engineersâ€™ logs
from internal tools, such as a distributed file system that records
developersâ€™ edits, a build system, and a code review tool. This data
contains fine-grained histories of developersâ€™ work, enabling us to
make accuratemeasurements ofactual workingbehavior,such as
the time developers spend actively writing code. The data helps
uscharacterizedevelopersâ€™workpractices,suchaswhatkindsof
developmenttasks theyaredoing,how longthosetaskstake, and
how long they are waiting for builds and tests to complete. Details
onthesetools,howdataisaggregatedintometrics,measurement
validation,andethicalconsiderationsofdatacollectioncanbefound
elsewhere[ 27].WedescribetheexactmetricsweuseinSection 4.3.
4.1.2 EngSat. TheEngineeringSatisfaction(EngSat)Surveyisa
longitudinal program to: understand the needs of Google engi-
neers; evaluate the effectiveness of tools, process, and organization
improvements; and provide feedback to teams that serve Google
engineers. Every three months, the survey is sent out to one-third
of eligible engineers Å› in one of five core engineering job roles,
havebeenatGoogleâ€™sparentcompanyforatleast6months,and
below the Å‚directorÅ¾ level. The same engineers are re-surveyed
every three quarters, and a random sample of one-third of new
engineers is added each quarter so that all engineers are invited.
The survey questions cover a range of topics, from productivity to
tool satisfaction to team communication. Respondents are asked
todescribetheirexperienceinthe3monthperiodpriortotaking
the survey. Before beginning the survey, respondents are informed
howthe data isusedandthat participationisvoluntary.
While EngSat response rates are typically between 30% and
40%, response bias does not appear to be a significant threat. We
know this because we analyzed two questions for response bias,
one on productivity and one on overall engineering experience
satisfaction. We found that EngSat tends to have lower response
rates for technical leads and managers, those who have been at
Googleforalongerperiod,andforengineersfromtheSanFrancisco
BayArea,whereGoogleisheadquartered.Toestimatetheimpact
ofnon-responsebiasonametricderivedfromEngSatresponses,
wecompareabias-correctedversionofthemetrictoitsuncorrected
versionandcheckforthedifference.Thebias-correctedmetricis
calculated by reweighting EngSat responses with the propensity
score (a similarity measure [ 21]) of responding to EngSat, which is
1304ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore L.Cheng,E. Murphy-Hill,M. Canning,C.Jaspan, C.Green, A.Knight, N.Zhang,E. Kammer
Figure1:Quantitativefeaturesthatpredictedself-ratedpro-
ductivity.
estimatedbasedonfactorssuchasdevelopertenure,worklocation,
andcodinglanguageandtools.Wefindthataftercorrectingforthis
non-response bias using propensity score matching, the percent of
engineers who responded favorably did not change significantly
foreitherquestion.Forinstance,adjustingfornon-responsebias,
productivity decreases relatively by 0.7%, which is too small to
reach statistical significance at the 95% level. These results were
consistent acrossthe three rounds ofEngSatthat we analyzed.
4.2 DependentVariable: Productivity
Weuseself-ratedproductivityfromoursurveyasourdependent
variable. Whilesubjective and objective measuresofproductivity
each have advantages and disadvantages, we chose a subjective
measure of productivity here both because it is straightforward
tomeasureinsurveyformandbecauseitisusedbroadlyinprior
research[ 15,34,36].
TheEngSatsurveyasksthequestion: Overall,inthepastthree
months how productive have you felt at work at Google/Alphabet?
Respondentscanchoose"Notatallproductive","Slightlyproduc-
tive", "Moderately productive", "Very productive", or "Extremely
productive".Wecodedthisvariablefrom1(Notatallproductive)
to 5(Extremelyproductive).
Prior software engineering research has shown that subjec-
tive productivity correlates with objective measures of produc-
tivity [36,37] as a wayto establish convergentvalidityof question-
basedproductivitymetrics(thatis,howtheyrelatetoothermea-
sures of the same construct [ 7]), We sought to do the same by
correlating our subjective productivity measure below with sev-
eral objective measuresof productivity. Ratherthan using a linear
correlationasusedinpriorwork,wewereopentothepossibility
that relationships were non-linear, and thus we selected a random
forestas aclassifier.
First, we created a simple random forest to predict a binary ver-
sion of self-rated productivity, where we coded Å‚Extremely produc-
tiveÅ¾ and Å‚VeryproductiveÅ¾ as productive, and the othervalues as
notproductive.Wethenpredictedthisbinarymeasureofself-rated
productivity using six quantitative productivitymetrics measured
overathreemonthperiod.Twoofthemeasurescapturetheamount
outputproducedover the fixedperiod:
â€¢TotalNumber ofChangelists. Thisrepresents thenumberof
changelists(CLs)thatanengineermerged,aftercodereview,
intoGoogleâ€™smain code repository.
â€¢Total Linesof Code. AcrossallCLs anengineermerged,the
totalnumber oflinesofcode changed.Twomeasurescapturetheamountoftimeittakesanengineerto
produce one unitofoutput(achangelist):
â€¢Median Active Coding Time. Across every CL merged, the
median time an engineer spent actively writing code per
CL[27].
â€¢MedianWall-ClockCodingTime. Themedianwall-clocktime
an engineer spent writing code per CL, that is, the time
elapsed between when the engineer starts writing code and
when they request the code be reviewed.
The remainingtwomeasures capturednon-productive activities,
that is, how much time an engineer spends waiting per unit of
output(achangelist):
â€¢MedianWall-ClockReviewTime. Themedianwall-clocktime
an engineer spentwaiting for code reviewper CL.
â€¢Median Wall-Clock Merge Time. The median wall-clock time
an engineer waited between approval for merging and actu-
allymergingper CL.
We gathered the above data over 6 consecutive quarters from
2018Q1to2019Q2.Foreachquarter,welinkedanengineerâ€™ssubjec-
tivemeasureofproductivitytotheabovefivequantitativemeasures.
Sinceengineersareinvitedtotakeoursurveyonceevery3quarters,
a single engineer may be represented at most twice in this data set.
In total, we had1958 engineer data pointsfor our model.
Afterrandomlyselecting10%ofthedataforvalidation,themodel
had 83% precision and 99% recall, suggesting a substantial relation-
shipbetweenquantitativeandqualitativeproductivitymeasures.
Lookingattheimportanceofeachquantitativemetricinclassify-
ing developersinthe model (Figure 1),we see that MedianActive
Coding Time was the most predictive quantitative feature. This
alignswithMeyerand colleaguesâ€™findingthatMicrosoftengineers
viewcoding as theirmostproductive activity[ 34].
4.3 IndependentVariables
To predict the dependent variable, we started with 42 independent
variablesÅ›reducedto39afteramulticolinearitycheck(Section 4.6)
Å›availablefromthesurveyandlogsdata.Sincesurveyrespondents
are asked to report on their experiences from the three months
prior to the survey, we collected log data for the corresponding
three month period. While many metrics could be analyzed, we
selected metrics that were relatively straightforward to collect and
thatappearedplausiblyrelatedtoindividualproductivity,basedon
consultationwithinternalsubjectmatterexpertswithinGooglethat
were experiencedwithbuilding anddeployingdeveloper metrics.
Below,wegroupindependentvariablesintosixcategories,de-
scribe each variable, and link them to prior work. We give each
variableashortname(inparentheses)tomakereferencingthem
easier in the remainder of the paper. Full survey questions and
response scalesare available inthe Appendix.
4.3.1 Code Quality & Technical Debt. The first category of poten-
tial drivers of productivity are those relating to code quality and
technicaldebt.Basedonexperience,DeMarcoandListerclaimthat
software quality, generally speaking, Å‚is a means to higher pro-
ductivityÅ¾[ 11].Inanexperiment,Schankinandcolleaguesfound
that participants found errors 14% faster when descriptive iden-
tifier names were used [ 45]. Studying small industrial programs
1305What ImprovesDeveloperProductivityatGoogle? Code Quality ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
written in the 1980s, Gill and Kemerer found that code complex-
ity correlates withsoftware maintenance productivity[ 17]. Based
oninterviewsandsurveyswithprofessionalsoftwaredevelopers,
Besker and colleagues found that technical debt correlates nega-
tivelywithdeveloper productivity[ 3,4].
We measured code quality and technical debt with 5 subjective
factors from our survey:
â€¢Code Quality Satisfaction (sat. withproject code quality,sat.
withdependency code quality)
â€¢Code Technical Debt (projecttechdebt)
â€¢DependencyTechnical Debt (dependency techdebt)
â€¢Technical DebtHindrance (techdebthindrance)
4.3.2 InfrastructureTools& Support. The next categoryofpoten-
tialdriversofproductivityareissuesrelatingtotoolsandinfrastruc-
ture. Prior work showed that using Å‚the best tools and practicesÅ¾
was the strongest correlate of individual productivity at Google,
though not a significant correlate at two other companies [ 36].
Storey and colleagues also found that Microsoft developersâ€™ pro-
cesses andtoolscorrelatedwithindividualproductivity[ 47].
This category had6objective and12 subjective measures:
â€¢Tools, infrastructure and service satisfaction (sat. with infra &
tools)
â€¢Toolsand infrastructurechoice (choicesofinfra &tools)
â€¢Toolsandinfrastructureinnovativeness (innovationofinfra
&tools)
â€¢Toolsand infrastructureease (easeofinfra &tools)
â€¢Tools and infrastructure frustration (frustration of infra &
tools)
â€¢Developer stackchange (changeoftoolstack)
â€¢Internaldocumentation support (doc. support)
â€¢Internaldocumentation hindrance (doc. hindrance)
â€¢Build&test cycle hindrance (build&test cyclehindrance)
â€¢Buildlatencysatisfaction (sat. withbuildlatency)
â€¢50thand90thpercentileofbuildduration (p50buildtime,p90
buildtime)
â€¢%oflong builds per week (%oflongbuilds)
â€¢50th and 90th percentile of test duration (p50 test time, p90
test time)
â€¢%oflong testsper week (%oflongtests)
â€¢Learninghindrance (learninghindrance)
â€¢Migration hindrance (migration hindrance)
4.3.3 TeamCommunication. Thenextcategoryofdriversofpro-
ductivity are issues relating to team communication. In a survey
of knowledge workers, Hernaus and MikuliÄ‡found that socialjob
characteristics(e.g. groupcooperation) correlatedwith contextual
jobperformance[ 23].Morespecifically,insoftwareengineering,
ChatzoglouandMacaulayinterviewedsoftwaredevelopers,finding
that most believed that communication among team members was
very important to project success [ 9]. Studying communication
networksquantitatively,KidaneandGloorfoundthatintheEclipse
project,ahigherfrequencyofcommunicationbetweendeveloper
correlatedpositivelywithperformance andcreativity[ 28].Tomeasureteamcommunicationinourstudy,weexamined9
objective measures and1subjective measure:
â€¢50th and 90th percentile of rounds of code review (p50 code
reviewrounds, p90 code reviewrounds)
â€¢50th and 90th percentile of total wait time of code review (p50
code reviewwaittime,p90 code reviewwaittime)
â€¢50th and 90th percentile of code reviewersâ€™ organizational dis-
tances from author (p50 review org distance, p90 review org
distance)
â€¢50thand90thpercentileofcodereviewersâ€™physicaldistances
from author (p50 review physical distance, p90 review phys-
icaldistance)
â€¢Physical distance from direct manager (distance from man-
ager)
â€¢Code review hindrance (slowcode review)
4.3.4 Goalsand Priorities. Prior researchsuggeststhatchanging
goals and priorities correlate with software engineering outcomes.
Surveying365softwaredevelopers,TheStandishGroupfoundthat
changing requirements was a common stated reason for project
failure [48]. Meyer and colleagues found that one of the top 5 most
commonlymentionedreasonsforaproductiveworkdaywashaving
clear goalsandrequirements [ 34].
We measure this category with1subjective measure:
â€¢Priorityshift (priorityshift)
4.3.5 Interruptions. Meyerandcolleaguesfoundthattwoofthe
top five most commonly mentioned reasons for a productive work-
dayby379softwaredeveloperswashavingnomeetingsandfew
interruptions [ 34]. Similarly, a prior survey of Google engineers
showed that lack of interruptions and efficient meetings correlated
withpersonal productivity,as diduse of personal judgment [ 36].
We measure this category with3objective measures:
â€¢50th and 90th percentile of total time spent on incoming meet-
ings per week (p50meeting time,p90 meeting time)
â€¢Total time spent on any meetings per week (total meeting
time)
4.3.6 OrganizationalandProcessFactors. Finally,outsideofsoft-
wareengineering,organizationalandprocessfactorscorrelatewith
avarietyofworkoutcomes.Forexample,accordingtohealthcare
industry managers, reorganizations can result in workersâ€™ sense of
powerlessness,inadequacy,andburnout[ 19].Althoughnotwell-
studied in software engineering, based on personal experience,
DeMarco and Lister [ 11] and Armour [ 2] point to bureaucracy and
reorganizations as leading to poor software engineering outcomes.
This category had2subjective and3objective measures:
â€¢Process hindrance (complicatedprocesses)
â€¢Organizationalhindrance (team& orgchange)
â€¢Number of times when engineersâ€™ direct manager changes but
colleaguesdonot change (reorgdirectmanagerchange)
â€¢Number of times when both an engineerâ€™s direct manager and
colleagues change simultaneously (non-reorg direct manager
change)
â€¢Number of different primary teams the engineer has (primary
team change)
1306ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore L.Cheng,E. Murphy-Hill,M. Canning,C.Jaspan, C.Green, A.Knight, N.Zhang,E. Kammer
4.4 From Variablesto PanelData
Sincethesurveywassentouttothesamecohortofengineersevery
three quarters, we have accumulated a panel data set with two
observations in different points of time for each engineer. After
joiningeachengineerâ€™ssurveydatawiththeirlogsdata,wehave
complete panel data for 2139 engineers.
4.5 Modeling
Using the panel data set, we applied a quasi-experiment method of
panel data analysis to analyze the relationship between engineersâ€™
perceived overall productivity and the independent variables. In
this paper, we use a fixed-effect model to analyze panel data at the
developer level. The modelis
ğ‘¦ğ‘–ğ‘¡=ğ›¼ğ‘–+ğœ†ğ‘¡+ğ›½ğ·ğ‘–ğ‘¡+ğœ–ğ‘–ğ‘¡(1)
where
â€¢ğ‘¦ğ‘–ğ‘¡is the dependent variable y, self-rated productivity for
developer iat time t.
â€¢ğ›¼ğ‘–is unobserved engineer time-invariant effects, such as
education andskills.
â€¢ğœ†ğ‘¡istheengineer-independenttimeeffect,suchascompany-
wide policychanges andseasonalities at time t.
â€¢ğ·ğ‘–ğ‘¡=[ğ·1
ğ‘–ğ‘¡,ğ·2
ğ‘–ğ‘¡,...,ğ·ğ‘›
ğ‘–ğ‘¡]are observed productivity factors
for developer iat time t.
â€¢ğ›½=[ğ›½1,ğ›½2,...,ğ›½ğ‘›]are the causal effects of productivity
factorsğ·ğ‘–ğ‘¡at time t.
â€¢ğœ–ğ‘–ğ‘¡isthe errorterm at time t.
To estimate the fixed-effect model, we differenced equation (1)
between the twoperiodsandhave
Î”ğ‘¦ğ‘–ğ‘¡=Î”ğœ†ğ‘¡+ğ›½Î”ğ·ğ‘–ğ‘¡+Î”ğœ–ğ‘–ğ‘¡(2)
whereÎ”ğœ†ğ‘¡=ğ›¾0+ğ›¾1ğ‘‡.TheÎ”prefixdenotesthechangefromone
time period to the next. T is a categorical variable representing
panels in different time periods, if we have more than one panel.
Notethat after differencing, ğ›¼ğ‘–iscancelled out and Î”ğœ†ğ‘¡canbe ex-
plicitlycontrolledbytransformingittoaseriesofdummyvariables.
Therefore,factors in ğ›¼ğ‘–andğœ†ğ‘¡donot confound the results.
Wethenestimatedequation(2)usingFeasibleGeneralizedLeast
Squares (FGLS); we chose FGLS to overcome heteroskedasticity,
serialcorrelationbetweenresiduals,andforefficiencycomparedto
OrdinaryLeastSquareestimators.Theparametersofinterestare
theğ›½terms. The hypothesis we are testing is that ğ›½=0for allğ·ğ‘–ğ‘¡.
Exceptfor binary variablesand percentagevariables,we transform
ğ·ğ‘–ğ‘¡intoğ‘™ğ‘œğ‘”(ğ·ğ‘–ğ‘¡). The benefit of taking a natural log is to allow
ustointerpretestimatesofregressioncoefficients( ğ›½terms)asan
elasticity,whereapercentchangeinadependentvariablecanbe
interpreted as a percent change in an independent variable. This
allows for both a uniform and intuitive interpretation of the effects
acrossboth logs-basedandsurvey-baseddependent variables.
Toliberallycapturecausalrelationshipsbetweenproductivity,
we use a p-value cutoff of 0.1 to define Å‚statistically significantÅ¾
results. If the reader prefers a more stringent cutoff or using a false
discovery correction, we facilitate this byreportingp-values.
Analysis code was written in R by the first author using the
packages glmnet, randomForest, binom, car, and plm. All code was
peer-reviewedusing Googleâ€™sstandardcode reviewprocess[ 43].4.6 Multicollinearity
Tocheckformulticollinearityamongtheindependentvariables,we
calculatedVarianceInflationFactor(VIF)scoresonthesemetrics.
Wefoundsomebuildlatencymetricswerehighlycorrelatedand
thus may cause a multicollinearity problem. After consulting with
expertsinourbuildsystem,weremovedthreebuildlatencymetrics
thathadaVIFscoreabove3(p50buildtime,p50testtime,and%of
longtests),athresholdrecommendedbyHairandcolleagues[ 22].
The final listof39 metrics allhave VIF scores below3.
4.7 Threatsto Validity
Like all empirical studies, ours is imperfect. In this section, we
describe threats to the validity of our study, broken down into
content, construct,internal, andexternal validity threats.
4.7.1 Content. Although our study examines a variety of facets of
productivity,itdoesnotexamineeverysingleaspectofproductivity
oroffactors that mayinfluence productivity.
Withrespecttoproductivityitself,wemeasureitwithasingle
surveyquestion.Ononehand,thequestionitselfiswordedbroadly
and our validation (Section 4.2) shows that it correlates with other
objectivemeasuresofproductivity.Ontheotherhand,asevidenced
bythefactthatthecorrelationwasimperfect,itislikelythatour
question did not capture some aspects of developer productivity.
As one example, our question was only focused on productivity of
an individual developer, yet productivity is often conceptualized
from ateam, group,orcompany perspective [ 16].
Likewise,our setof productivity factorsÅ› like code qualityand
buildspeedÅ›areincomplete,largelybecauseweusedconveniently
available and subjectively-selected metrics and because we reused
an existing long-running survey. In comparison, prior work, which
usedacustom-builtcross-sectionalsurvey,foundthattwoofthe
strongest correlates with individual productivity were job enthusi-
asmandteammatesâ€™supportfornewideas[ 36].Neitherofthesetwo
productivityfactorswereexploredinthepresentsurvey,demon-
strating that our productivityfactors are incomplete.
4.7.2 Construct. Our EngSat survey measures a variety of theoret-
icalconcepts,andthequestionscontainedinitcontainarangeof
construct validity. For instance, while we have demonstrated some
amountofconvergentvalidityofourproductivityquestion,respon-
dentstothequestionmayhaveinterpretedthewordÅ‚productivityÅ¾
differentlyÅ›somemayhaveinterpretedittoreferonlytothequick
completionofworkitems,whileothersmighttakeamoreexpan-
siveviewtoincludeaspectssuchasquality.Whilewehavetriedto
limit the impact of different interpretations of EngSat questions by
pilotingvariations,gatheringinterpretivefeedback,andrefining
wording iteratively,such issuesare unavoidablethreats.
Another specific threat to construct validity is inconsistent and
ambiguous question wording. For instance, while respondents are
advisedatthebeginningofthesurveythattheyshouldreporton
experiences over the last 3 months, some questions (but not all) re-
inforcethisscopingbybeginningwithÅ‚Inthelastthreemonths...Å¾.
Asanotherexampleofinconsistency,whilemostquestionsaskonly
about experiences (whichour models use topredict productivity),
three questions ask about the relationship between experience and
perceived productivity, such as Å‚how much has technical debt...
1307What ImprovesDeveloperProductivityatGoogle? Code Quality ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Table 3:Metricsâ€™relationshipwith self-rated productivity.
Metric Effectsize p-value
CodeQuality& Technical Debt
sat. withprojectcode quality 0.105 <0.001
sat. withdependency code quality -0.013 0.505
projecttechdebt 0.078 <0.001
dependency techdebt 0.042 0.012
techdebthindrance -0.009 0.459
Infrastructure Tools & Support
sat. withinfra &tools 0.113 <0.001
choicesofinfra &tools 0.020 0.083
innovationofinfra &tools 0.106 <0.001
easeofinfra &tools -0.018 0.352
frustration ofinfra &tools 0.002 0.952
changeoftoolstack 0.019 0.098
doc. support -0.009 0.664
doc. hindrance -0.005 0.715
buildandtest cyclehindrance 0.029 0.064
sat. withbuildlatency 0.018 0.295
p90 buildtime -0.024 0.019
p90 test time -0.001 0.836
%oflongbuilds 0.028 0.599
learninghindrance 0.038 0.006
migrationhindrance -0.001 0.929
TeamCommunication
p50 code reviewrounds 0.007 0.081
p90 code reviewrounds -0.014 0.058
p50 code reviewwaittime -0.0006 0.875
p90 code reviewwaittime 0.0019 0.625
p50 revieworgdistance -0.0008 0.424
p90 revieworgdistance -0.0002 0.880
p50 reviewphysical distance 0.0012 0.261
p90 reviewphysical distance 0.0013 0.518
distancefrom manager 0.001 0.209
slowcode review 0.051 0.004
Goals & Priorities
priority shift 0.077 <0.001
Interruptions
p50 meeting time 0.014 0.502
p90 meeting time 0.008 0.701
totalmeeting time -0.009 0.692
OrganizationalChangeandProcess
complicatedprocesses 0.027 0.067
team andorgchange 0.032 0.023
reorgdirectmanagerchanges -0.002 0.086
non-reorgdirectmanagerchange -0.002 0.525
primary team change 0.014 0.086
hindered your productivity?Å¾. As an example of ambiguity, several
questionsaskaboutengineersâ€™experienceswiththeprojectthey
work on, but respondents interpret for themselves what a "project"
isand,if they work onmultiple projects,whichone to report on.
4.7.3 Internal. Asweargueinthispaper,ouruseofpanelanalysis
helpsdrawstrongercausalinferencesthanthosethatcanbedrawnfrom cross-sectional data. However, the most significant caveat
to our ability to draw causal inferences is time variant effects. In
contrasttotimeinvarianteffects(e.g.,prioreducationanddemo-
graphics), time variant effects may vary over the study period. For
instance,inourrunningexample,ifArujlostamentorandRusla
gainedamentorbetweenthetwosurveys,ouranalysiscouldnot
rule out mentorship as a cause of increased productivity or code
quality. Thus, our analysis assumes that effects on individual en-
gineersaretimeinvariant.Violationsofthisassumptionthreaten
the internal validity ofour study.
Another internal threat to the validity of our study is partici-
pants who chose not to answer some or all questions in the survey.
Whileouranalysisofnon-responsebias(Section 4.1.2)showedthat
two survey questions were robust to non-response among several
dimensions like level and tenure, non-response is still a threat. For
one, respondents and non-respondents might differ systematically
onsomeunmeasuredordimension,suchashowfrequentlythey
get feedback from peers. Likewise, respondents who choose not
to answer a question will be wholly excluded from our analysis,
yetsuchparticipantsmightdiffersystematicallyfromthosewho
answeredevery question.
Anotherthreattointernalvalidityisthatweanalyzeddatafor
only two panels per engineer. More panels per engineer would
increasethe robustnessof our results.
4.7.4 External. Asthetitleofthispapersuggests,ourstudywas
conductedonlyatGoogleandgeneralizabilityofourresultsbeyond
that context is limited. Google is a large, US-headquartered, multi-
national, and software-centric company where engineers work on
largely server and mobile code, with uniform development tooling,
andinamonolithicrepository.Likewise,duringthestudyperiod
Google developers mostly worked from open offices, before the
global COVID19 pandemic when many developers shifted to re-
moteorhybridwork.Whileresultswouldvaryifthisstudywere
replicatedinotherorganizations,contextsthatresembleoursare
mostlikely to yieldsimilar results.
5 PANEL ANALYSIS: RESULTS
5.1 Factors Causally Linkedto Productivity
Paneldataanalysissuggestedthat16outofthe39metricshavea
statisticallysignificantcausalrelationshipwithperceivedoverall
productivity, as listed in Table 3. The overall adjusted R-squared
value for the model was 0.1019. In Table 1, the Effect size should
be read as a percent change in the dependent variable is associated
with that percent change in the independent variable. For instance,
for code quality, a 100% change in project code quality (from Å‚Very
dissatisfiedÅ¾toÅ‚VerysatisfiedÅ¾toquality)isassociatedwitha10.5%
increaseinself-reportedproductivity.Tosummarize Table 3:
â€¢For code quality, we found that perceived productivity is
causally related to satisfaction with project code quality
but not causally related to satisfaction with code quality
in dependencies. For technical debt, we found perceived
productivityiscausallyrelatedtoperceivedtechnicaldebt
both within projectsandintheir dependencies.
1308ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore L.Cheng,E. Murphy-Hill,M. Canning,C.Jaspan, C.Green, A.Knight, N.Zhang,E. Kammer
â€¢For infrastructure, several factors closely related to internal
infrastructureandtoolsshowedasignificantcausalrelation-
shipwithperceivedproductivity:
Å›Engineers who reported their tools and infrastructure
were not innovative were more likely to report lower pro-
ductivity.
Å›Engineerswhoreportedthenumberofchoiceswereeither
toofewortoomanywerelikelytoreportlowerproduc-
tivity.Wefurthertestedwhetheroneofthetwo(Å‚toofewÅ¾
orÅ‚toomanyÅ¾)mattersbutnottheother,byreplacingthis
variablewithtwobinaryvariables,onerepresentingthe
caseofÅ‚toofewÅ¾choicesandtheotherrepresentingthe
case of Å‚too manyÅ¾ choices. The results suggest that both
casesare causallyrelatedto perceivedproductivity.
Å›Engineers who reported that the pace of changes in the
developertoolstackwastoofastortooslowwerelikely
toreportlowerproductivity. Similarly,wetestedthe two
cases, Å‚too fastÅ¾ or Å‚too slowÅ¾, separately by replacing
this variable withtwobinary variables, one representing
thecaseofÅ‚toofastÅ¾andtheotherrepresentingthecase
of Å‚too slowÅ¾. Results suggested both cases matter for
perceivedproductivity.
Å›Engineerswhowerehinderedbylearninganewplatform,
framework, technology, or infrastructure were likely to
report lower productivity.
Å›Engineerswhohadlongerbuildtimesorreportedbeing
hindered by their build & test cycle were more likely to
report lower productivity.
â€¢Forteamcommunication,ametricrelatedtocodereviewwas
significantly causally related with perceived productivity.
Engineers who had more rounds of reviews per code review
orreportedbeinghinderedbyslowcodereviewprocesses
were likely to report lower productivity.
â€¢Forgoalsandpriorities,engineershinderedbyshiftingproject
priorities were likely to report lower productivity.
â€¢Organizationalfactorswerelinkedtoperceivedproductivity:
Å›Engineerswhohadmorechangesofdirectmanagerswere
more likely to report lower productivity.
Å›Engineerswhoreportedbeinghinderedforteamandor-
ganizational reasons, or by complicated processes were
more likely to report lower productivity.
5.2 Quadrant Chart
Tovisualizethesefactorsintermsoftheirrelativeeffectsizeand
statistical significance, we plot them in a quadrant chart (Figure 2).
Thechartexcludesfactorswhosep-valueisgreaterthan0.1.The
factorshavevariousscalesfromsatisfactionscoretotimeduration,
sotomaketheireffectsizecomparable,westandardizedmetricsby
subtracting each data point by its mean and dividing it by its stan-
dard deviation. The x axis is the absolute value of the standardized
effectsize.The y axis isp-values.
Thetopfivefactorsintermsofrelativeeffectsizearesatisfaction
with project code quality, hindrance of shifting priorities, technical
debt in projects, innovation of infrastructure, and tools and overall
satisfactionwithinfrastructure andtools.6 LAGGED PANEL ANALYSIS: METHODS
Thepaneldataanalysisweconductedsofarsuggestssatisfaction
withcodequalitywithinprojectsisthestrongestproductivityfactor
among the 39 we studied, based on standardized effect size and
p-value.
However, because the observed changes in factors coincided
duringthesametimeperiod,suchconventionalpaneldataanalysis
cantellwhichfactorsarecausallyrelatedtooverallproductivity,
but itdoes not tellusthe direction of the causality.
So,doesbettercodequalitycauseincreasingproductivity,ordoes
increasingproductivitycauseimprovedcodequality?Bothlinkages
aretheoreticallyplausible:ononehand,codequalitymightincrease
productivitybecausehighercodequalitymaymakeiteasierand
faster to add new features; on the other hand, high productivity
might increase quality code because engineers have free time to
spend onqualityimprovement.
Toverifythedirectionofthecausalrelationshipbetweenproject
code quality and productivity, we conducted another panel data
analysis using lagged panel data. In this analysis, we focus only
onthecausalrelationshipbetweencodequalityandproductivity.
Althoughsuchananalysisispossibleforotherfactors,itisnonethe-
lesslaborious, aswe shallseeshortly.Thus,wefocusourlagged
analysisononlythesetwovariables,whichhadthestrongestcausal
relationship inour prior analysis.
Inshort,weverifiedthedirectionofthelinkagebetweenproject
code quality and productivity by checking if the change in one fac-
torisassociatedwiththechangeintheotherfactorinthefollowing
period. The idea is that if project code quality affects productiv-
ity, we expect to see that changes in project code quality during
time T-1 are associated with changes in productivity during time T.
Sinceself-reportedproductivityisnotavailablefortwoconsecutive
quarters(sinceeachrespondent issampledonlyonceeverythree
quarters), we switch to logs-based metrics to measure productivity.
Complementingourprioranalysisbasedonself-ratingswithalogs-
basedonehastheadditionalbenefitofincreasingtherobustnessof
our results.
Moreformally,wetestedtwocompetinghypotheses,Hypothesis
QaP (Quality affects Productivity) and PaQ (Productivity affects
Quality).HypothesisQaPisthatthechangesinprojectcodequality
during time T-1 are associated withchanges in productivity during
timeT.Thisimpliesimprovementsinprojectcodequalityleadto
betterproductivity.HypothesisPaQisthatchangesinproductivity
intimeT-1areassociatedwithchangesinprojectcodequalityin
timeT.Thisimpliesbetterproductivityleadstoanimprovement
inprojectcode quality.
Hypothesis QaP : Changes in code quality during time T-1 are
correlated with changes in productivity during time T. The statisti-
cal modelis
Î”ğ‘ƒğ‘–ğ‘¡=ğ›¼+ğ›½Î”ğ‘„ğ‘–ğ‘¡âˆ’1+Î”ğœ–ğ‘–ğ‘¡(4)
whereÎ”ğ‘„ğ‘–ğ‘¡âˆ’1isthe change in code quality attime t-1 and Î”ğ‘ƒğ‘–ğ‘¡is
thefollowingchangeinlogs-basedproductivitymetricsattimet.
Given the available data, we use the difference between Q3 2018
andQ22019tomeasure Î”ğ‘„ğ‘–ğ‘¡âˆ’1andthedifferencebetweenQ32018
andQ3 2019 to measure Î”ğ‘ƒğ‘–ğ‘¡.
1309What ImprovesDeveloperProductivityatGoogle? Code Quality ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Figure 2:Quadrantofproductivity factorsineffectsize andstatistical significance
HypothesisPaQ :ChangesinproductivityintimeT-1arecor-
relatedwithchangesincodequalityintimeT.Thestatisticalmodel
is
Î”ğ‘„ğ‘–ğ‘¡=ğ›¼+ğ›½Î”ğ‘ƒğ‘–ğ‘¡âˆ’1+Î”ğœ–ğ‘–ğ‘¡(5)
whereÎ”ğ‘ƒğ‘–ğ‘¡âˆ’1isthechangeinlogs-basedproductivityattimet-1
andÎ”ğ‘„ğ‘–ğ‘¡isthefollowingchangeincodequalityattimet.Given
the availability of data, we use the difference between Q3 2018 and
Q22019tomeasure Î”ğ‘ƒğ‘–ğ‘¡âˆ’1andthedifferencebetweenQ32018and
Q1 2019 to measure Î”ğ‘„ğ‘–ğ‘¡.
Forthisanalysis,wehadfulllaggedpaneldatafor3389engineers.
7 LAGGED PANEL ANALYSIS: RESULTS
Our results support hypothesis QaP but not hypothesis PaQ. We
found thata 100%increase ofsatisfactionrating with project code
quality(i.e.goingfromaratingofâ€˜Verydissatisfiedâ€™toâ€˜Verysat-
isfiedâ€™)attimeT-1wasassociatedwitha10%decreaseofmedian
active coding time per CL, a 12% decrease of median wall-clock
time from creating to mailing a CL, and a 22% decrease of median
wall-clock time from submitting to deploying a CL at time T. On
theotherhand,wedidnotfindanyevidencetosupporthypothesis
PaQ; changes in satisfaction with project code quality in time T
were not associated with any of the productivity metrics in time
T-1. See Appendix for a table containing this data and descriptions
of each variable. Therefore, returning to our research question,we conclude thatchanges in satisfaction with project code quality
cause changes inperceivedoverallproductivity.
8 DISCUSSION
Our findings provide practical guidance for organizations trying
to improve individual developer productivity by providing a list of
amenablefactorsthatarecausallylinkedtoproductivity.Specifi-
cally, our panel analysis shows that these factors are: code quality,
technical debt, infrastructure tools and support, team communica-
tion,goalsandpriorities,andorganizationalchangeandprocess.
Our quadrant chart shown in Figure 2, which we originally cre-
atedforanexecutivestakeholderaudiencewithinGoogle,allows
practitioners to choose highly impactful productivity factors to act
on. Factors at the top of the chart are those with high statistical
significance (and low standard error), so practitioners can read
those as the most consistent productivity factors. Factors on the
right are the ones with the largest standardized effect size, so these
supply the Å‚biggest bang for the buckÅ¾. Taken together, the factors
intheupperrightquadrantaretheonesmostpromisingtoimprove
productivity at Google. For instance, giving teams time to improve
codequality,reducetechnicaldebt,andstabilizeprioritieswould
be good candidate initiatives for improving individual developer
productivity.
1310ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore L.Cheng,E. Murphy-Hill,M. Canning,C.Jaspan, C.Green, A.Knight, N.Zhang,E. Kammer
We found that several factors did not have a statistically signifi-
cantrelationship withperceivedproductivity,notably:
â€¢For documentation, perceived productivity was not causally
linked to reported poor or missing documentation ( doc. hin-
drance)orthefrequencyofdocumentationmeetingneeds
(doc. support ). This is surprising, given that GitHubâ€™s 2017
surveyof5,500developersfoundthatÅ‚incompleteorconfus-
ingdocumentationÅ¾wasthemostcommonlyencountered
probleminopensource[ 18].GitHubâ€™sfindingsareconsis-
tentwithfindingsatMicrosoft[ 47]andatGoogleÅ›EngSat
respondentsoftenreportÅ‚poorormissingdocumentationÅ¾
as one of the top three hindrances to their own productiv-
ity. However, the results in this paper suggest that there
is no causal relationship between developer productivity
and documentation, despite developersâ€™ reports that it is
importanttothem.Onewaytoexplainthisfindingisthat
documentation may not impact productivity, but it may yet
have other positive benefits, such as to Å‚create inclusive
communitiesÅ¾[ 18].
â€¢For meetings, we found that perceived productivity was
notcausallylinkedtotimespentoneitherincomingmeet-
ings(p50meetingtime ,p90meetingtime )oralltypesofmeet-
ings(totalmeetingtime ).Thisisalsosurprising,giventhat
prior research found in a survey of Microsoft engineers
that meetings were the most unproductive activity for engi-
neers[34]. Thecontradictoryresultscould be explainedby
differencesbetweenthestudies:ourpanelanalysisenables
causalreasoning(vscorrelational),moreengineerswererep-
resented in our dataset (2139 vs 379), and we used objective
meeting data from engineersâ€™ calendars (vs. self-reports).
â€¢For physical and organizational distances, perceived produc-
tivitywasnotcausallylinkedtophysicaldistancefromdirect
manager ( distance from manager ), or physical ( p50 review
physicaldistance,p90reviewphysicaldistance )ororganiza-
tional distances from code reviewers( p50 review org distance,
p90revieworgdistance ).ThisisincontrasttoRamasubbuand
colleaguesâ€™cross-sectionalstudy,whichfoundthatÅ‚asfirms
distributetheirsoftwaredevelopmentacrosslongerdistance
(and time zones) they benefit from improved project level
productivityÅ¾ [ 41]. As with the prior differences, explana-
toryfactorsmayincludedifferencesinorganizationanda
methodology: individualproductivity versusorganizational
productivity, single company versus multiple companies,
andpanel versuscross-sectionalanalysis.
Aswementioned,athreattotheseresultsisthethreatofreverse
causalityÅ›thestatisticsdonottelluswhethereachfactorcauses
productivity changes or vice versa. We mitigated this threat for
code quality using lagged panel analysis, providing compelling
evidence that high code quality increases individual developersâ€™
productivity.
WithinGoogle,ourresultshavedrivenorganizationalchange
aroundcodequality andtechnical debt as a way to improve devel-
oper productivity:
â€¢SinceitscreationinMay2019,aversionofthisreporthas
beenviewedbymorethan1000uniqueGoogleemployees
withmore than500comments.â€¢EngSatresultshelpedmotivatetwocodequalityconferences
forGoogleengineerswith4,000internalattendeesandmore
than15,000viewsof live andon-demandtalks.
â€¢The research motivated the creation of two initiatives Å› a
TechnicalDebtMaturityModel(akintotheCapabilityMa-
turityModel[ 38])andTechnicalDebtManagementFrame-
workÅ›tohelpteamsimprovetechnicaldebtassessmentand
management.
â€¢SeveralteamsandorganizationssetObjectivesandKeyRe-
sults (OKRs) [ 12] to improve technical debt in their work-
groups.
â€¢GoogleintroducedÅ‚TheHealthysÅ¾,anawardwhereteams
submitatwopageexplanationofacodequalityimprovement
initiativetheyâ€™veperformed.Using anacademicreviewing
model, outside engineers evaluated the impact of nearly 350
submissions across the company. Accomplishments include
morethanamillionlinesofcodedeleted.Inasurveysentto
awardrecipients,of173respondents,mostrespondentsre-
ported that they mentioned the award in the self-evaluation
portionoftheirperformanceevaluation(82%)andthatthere
wasatleastaslightimprovementinhowcodehealthwork
isviewedbytheirteam (68%) andmanagement(60%).
Although difficult to ascribe specifically to this research and the
above initiatives that it has influenced, EngSat has revealed several
encouraging trends between when the report was released inter-
nallyinthesecondquarterof2019andthefirstquarterof2021:The
proportionofengineersfeelingÅ‚notatallhinderedÅ¾bytechnical
debt has increased by 27%. The proportion of engineers feeling sat-
isfiedwithcodequalityhasincreasedbyabout22%.Theproportion
of engineers feeling highly productive at work has increased by
about18%.
9 CONCLUSION
Prior researchhas madesignificant progressinimprovingourun-
derstanding of what correlates with developer productivity. In this
paper, weâ€™ve advanced that research by leveraging time series data
to run panel analyses, enabling stronger causal inference than was
possible in prior studies. Our panel analysis suggests that code
quality, technical debt, infrastructure tools and support, team com-
munication, goals and priorities, and organizational change and
process are causally linked to developer productivity at Google.
Furthermore,ourlagged panelanalysisprovides evidencethatim-
provements in code quality cause improvements in individual pro-
ductivity. While our analysis is imperfect Å› in particular, it is only
one company and uses limited measurements Å› it nonetheless can
help engineering organizations make informed decisions about
improvingindividualdeveloper productivity.
ACKNOWLEDGMENT
ThankstoGoogleemployeesforcontributingtheirEngSatandlogs
datatothisstudy,aswellastheteamsresponsibleforbuildingthein-
frastructureweleverageinthispaper.ThanksinparticulartoAdam
Brown, Michael Brundage, Yuangfang Cai, Alison Chang, Sarah
Dâ€™Angelo, Daniel Dressler, Ben Holtz, Matt Jorde, Kurt Kluever,
Justin Purl, Gina Roldan, Alvaro Sanchez Canudas, Jason Schwarz,
Simone Styr,FredWiesinger,andanonymous reviewers.
1311What ImprovesDeveloperProductivityatGoogle? Code Quality ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
REFERENCES
[1]Joshua D. Angrist and JÃ¶rn-SteffenPischke. 2008. Mostly harmless econometrics:
Anempiricistâ€™scompanion . PrincetonUniversityPress.
[2] PhillipArmour. 2003. The Reorg Cycle. Commun. ACM 46,2 (2003), 19.
[3]TereseBesker,HadiGhanbari,AntonioMartini,and JanBosch. 2020. Theinflu-
ence of Technical Debt on software developer morale. Journal of Systems and
Software167(2020), 110586. https://doi.org/10.1016/j.jss.2020.110586
[4]TereseBesker,AntonioMartini,andJanBosch.2019. Softwaredeveloperproduc-
tivity loss due to technical debtÃa replication and extension study examining
developersâ€™developmentwork. JournalofSystemsandSoftware 156(2019),41Å›61.
https://doi.org/10.1016/j.jss.2019.06.004
[5]Larissa Braz, Enrico Fregnan, GÃ¼l Ã‡alikli, and Alberto Bacchelli. 2021. Why
Donâ€™t Developers Detect Improper Input Validation?â€™; DROP TABLE Papers;Å›
. InInternational Conference on Software Engineering . IEEE, 499Å›511. https:
//doi.org/10.1109/ICSE43902.2021.00054
[6] K.H. Brodersen, F. Gallusser, J. Koehler, N.Remy, and S.L. Scott. 2015. Inferring
causalimpactusingBayesianstructuraltime-seriesmodels. TheAnnalsofApplied
Statistics 9,1 (2015), 247Å›274. https://doi.org/10.1214/14-AOAS788
[7]KevinDCarlsonandAndrewOHerdman.2012. Understandingtheimpactof
convergentvalidityonresearchresults. OrganizationalResearchMethods 15,1
(2012), 17Å›32. https://doi.org/10.1177/1094428110392383
[8]Nancy Cartwright. 2007. Are RCTs the gold standard? BioSocieties 2, 1 (2007),
11Å›20.https://doi.org/10.1017/S1745855207005029
[9]ProdromosD.ChatzoglouandLindaA.Macaulay.1997.Theimportanceofhuman
factors in planning the requirements capture stage of a project. International
JournalofProjectManagement 15,1(1997),39Å›53. https://doi.org/10.1016/S0263-
7863(96)00038-5
[10]BradfordClark,SunitaDevnani-Chulani,andBarryBoehm.1998. Calibrating
the COCOMO II post-architecture model. In Proceedings of the International
Conference on Software Engineering . IEEE, 477Å›48. https://doi.org/10.1109/ICSE.
1998.671610
[11]Tom DeMarcoand Tim Lister. 2013. Peopleware: productive projects and teams .
Addison-Wesley.
[12]JohnDoerr.2018. Measurewhatmatters:HowGoogle,Bono,andtheGatesFoun-
dationrock the world with OKRs . Penguin.
[13]Davide Falessi, Natalia Juristo, Claes Wohlin, Burak Turhan, JÃ¼rgen MÃ¼nch,
Andreas Jedlitschka, and Markku Oivo. 2018. Empirical software engineering
experts on the use of students and professionals in experiments. Empirical
Software Engineering 23,1 (2018), 452Å›489. https://doi.org/10.1007/s10664-017-
9523-3
[14]Petra FilkukovÃ¡ and Magne JÃ¹rgensen. 2020. How to pose for a professional
photo: The effect of three facial expressions on perception of competence of
a software developer. Australian Journal of Psychology 72, 3 (2020), 257Å›266.
https://doi.org/10.1111/ajpy.12285
[15]Denae Ford, Margaret-Anne Storey, Thomas Zimmermann, Christian Bird, Sonia
Jaffe,ChandraMaddila,JennaL.Butler,BrianHouck,andNachiappanNagappan.
2021. A Tale of Two Cities: Software Developers Working from Home during
the COVID-19 Pandemic. ACM Trans. Softw. Eng. Methodol. 31, 2, Article 27 (dec
2021),37pages. https://doi.org/10.1145/3487567
[16]NicoleForsgren,Margaret-AnneStorey,ChandraMaddila,ThomasZimmermann,
Brian Houck, and Jenna Butler. 2021. The SPACE of Developer Productivity:
Thereâ€™s more to it than you think. Queue19, 1 (2021), 20Å›48. https://doi.org/10.
1145/3454122.3454124
[17]Geoffrey K. Gill and Chris F. Kemerer. 1991. Cyclomatic complexity density and
software maintenance productivity. IEEE transactions on software engineering 17,
12(1991), 1284. https://doi.org/10.1109/32.106988
[18] GitHub. 2017. Open Source Survey. https://opensourcesurvey.org/2017/
[19]Ann-Louise Glasberg, Astrid Norberg, and Anna SÃ¶derberg. 2007. Sources of
burnout among healthcare employees as perceived by managers. Journal of
Advancednursing 60,1(2007),10Å›19. https://doi.org/10.1111/j.1365-2648.2007.
04370.x
[20]C.W.Granger.1969. Investigatingcausalrelationsbyeconometricmodelsand
cross-spectral methods. Econometrica: Journal ofthe Econometric Society (1969),
424Å›438. https://doi.org/10.2307/1912791
[21]Shenyang Guo and Mark W. Fraser. 2014. Propensity score analysis: Statistical
methodsand applications . Vol. 11. SAGE publications.
[22]Joseph F Hair, Jeffrey J Risher, Marko Sarstedt, and Christian M Ringle. 2019.
WhentouseandhowtoreporttheresultsofPLS-SEM. EuropeanBusinessReview
(2019).https://doi.org/10.1108/EBR-11-2018-0203
[23]TomislavHernausandJosipMikuliÄ‡.2014. Workcharacteristicsandworkper-
formance of knowledge workers. EuroMed Journal of Business (2014).https:
//doi.org/10.1108/EMJB-11-2013-0054
[24]Cheng Hsiao. 2007. Panel data analysisÃadvantagesand challenges. TEST16, 1
(2007), 1Å›22. https://doi.org/10.1007/s11749-007-0046-x
[25] ChengHsiao. 2022. Analysisofpanel data . CambridgeUniversityPress.
[26]Mazhar Islam, Jacob Miller, and Haemin Dennis Park. 2017. But what will it cost
me? How do private costs of participation affect open source software projects?
Research Policy 46, 6(2017), 1062Å›1070. https://doi.org/10.1016/j.respol.2017.05.005
[27]Ciera Jaspan, Matt Jorde, Carolyn Egelman, Collin Green, Ben Holtz, Edward
Smith,MaggieHodges,AndreaKnight,LizKammer,JillDicker,etal .2020. En-
abling the Study of Software Development Behavior With Cross-Tool Logs. IEEE
Software37,6 (2020), 44Å›51. https://doi.org/10.1109/MS.2020.3014573
[28]Yared H. Kidane and Peter A. Gloor. 2007. Correlating temporal communication
patterns of the Eclipse open source community with performance and creativity.
Computational and mathematical organization theory 13, 1 (2007), 17Å›27. https:
//doi.org/10.1007/s10588-006-9006-3
[29]Amy J. Ko. 2019. Individual, Team, Organization, and Market: Four Lenses of
Productivity. In Rethinking Productivity in Software Engineering . Springer, 49Å›55.
https://doi.org/10.1007/978-1-4842-4221-6_6
[30]Amy J. Ko and Brad A. Myers. 2008. Debugging Reinvented: Asking and An-
swering Why and Why Not Questions about Program Behavior. In Proceed-
ings of the 30th International Conference on Software Engineering (ICSE â€™08) .
Association for Computing Machinery, New York, NY, USA, 301Å›310. https:
//doi.org/10.1145/1368088.1368130
[31]Max Lillack, Stefan Stanciulescu, Wilhelm Hedman, Thorsten Berger, and An-
drzej WÄ…sowski. 2019. Intention-Based Integration of Software Variants. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE) . 831Å›842.
https://doi.org/10.1109/ICSE.2019.00090
[32]William Martin, Federica Sarro, and Mark Harman. 2016. Causal impact analysis
for app releases in Google Play. In Proceedings of the 2016 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering . 435Å›446. https:
//doi.org/10.1145/2950290.2950320
[33]Katrina D.Maxwell, Luk Van Wassenhove,and SoumitraDutta.1996. Software
developmentproductivityofEuropeanspace,military,andindustrialapplications.
IEEE Transactions on Software Engineering 22, 10 (1996), 706Å›718. https://doi.
org/10.1109/32.544349
[34]AndrÃ© N Meyer, Thomas Fritz, Gail C Murphy, and Thomas Zimmermann. 2014.
Softwaredevelopersâ€™perceptionsofproductivity.In Proceedingsofthe22ndACM
SIGSOFT International Symposium on Foundations of Software Engineering . 19Å›29.
https://doi.org/10.1145/2635868.2635892
[35]Emerson Murphy-Hill and Andrew P. Black. 2008. Breaking the Barriers to
SuccessfulRefactoring:ObservationsandToolsforExtractMethod.In Proceedings
ofthe30thInternationalConferenceonSoftwareEngineering(ICSEâ€™08) .Association
for Computing Machinery, New York, NY, USA, 421Å›430. https://doi.org/10.
1145/1368088.1368146
[36]Emerson Murphy-Hill, Ciera Jaspan, Caitlin Sadowski, David Shepherd, Michael
Phillips, Collin Winter, Andrea Knight, Edward Smith, and Matthew Jorde. 2021.
What Predicts Software Developersâ€™ Productivity? IEEE Transactions on Software
Engineering 47,3 (2021), 582Å›594. https://doi.org/10.1109/TSE.2019.2900308
[37]Edson Oliveira, Eduardo Fernandes, Igor Steinmacher, Marco Cristo, Tayana
Conte, and Alessandro Garcia. 2020. Code and commit metrics of developer
productivity:astudyonteamleadersperceptions. EmpiricalSoftwareEngineering
25,4 (2020), 2519Å›2549. https://doi.org/10.1007/s10664-020-09820-z
[38]Mark C. Paulk, Bill Curtis, Mary Beth Chrissis, and Charles V. Weber. 1993.
Capability maturity model, version 1.1. IEEE Software 10, 4 (1993), 18Å›27. https:
//doi.org/10.1109/52.219617
[39]KaiPetersen.2011. Measuringandpredictingsoftwareproductivity:Asystematic
map and review. Information and Software Technology 53, 4 (2011), 317Å›343.
https://doi.org/10.1016/j.infsof.2010.12.001 Specialsection:SoftwareEngineering
track of the 24th AnnualSymposium onApplied Computing.
[40]HuilianSophieQiu,AlexanderNolte,AnitaBrown,AlexanderSerebrenik,and
BogdanVasilescu.2019. GoingFartherTogether:TheImpactofSocialCapital
on Sustained Participation in Open Source.In 2019 IEEE/ACM 41st International
Conference on Software Engineering (ICSE) . 688Å›699. https://doi.org/10.1109/
ICSE.2019.00078
[41]NarayanRamasubbu,MarceloCataldo,RajeshKrishnaBalan,andJamesD.Herb-
sleb. 2011. Configuring global software teams: a multi-company analysis of
project productivity, quality, and profits. In 2011 33rd International Conference on
SoftwareEngineering (ICSE) . 261Å›270. https://doi.org/10.1145/1985793.1985830
[42]SimoneRomano,DavideFucci,MariaTeresaBaldassarre,DaniloCaivano,and
Giuseppe Scanniello. 2019. An empirical assessment on affective reactions
ofnovicedeveloperswhenapplyingtest-drivendevelopment.In International
Conference on Product-Focused Software Process Improvement . Springer, 3Å›19.
https://doi.org/10.1007/978-3-030-35333-9_1
[43]Caitlin Sadowski, Emma SÃ¶derberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern code review: a case study at google. In Proceedings of
the 40th International Conference onSoftware Engineering:Software Engineering
inPractice . 181Å›190. https://doi.org/10.1145/3183519.3183525
[44]Iflaah Salman, Ayse Tosun Misirli, and Natalia Juristo. 2015. Are students
representatives of professionals in software engineering experiments?. In In-
ternational Conference on Software Engineering , Vol. 1. IEEE, 666Å›676. https:
//doi.org/10.1109/ICSE.2015.82
[45]AndreaSchankin,AnnikaBerger,DanielV.Holt,JohannesC.Hofmeister,Till
Riedel,andMichaelBeigl.2018. DescriptiveCompoundIdentifierNamesImprove
SourceCodeComprehension.In Proceedingsofthe26thConferenceonProgram
1312ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore L.Cheng,E. Murphy-Hill,M. Canning,C.Jaspan, C.Green, A.Knight, N.Zhang,E. Kammer
Comprehension (ICPC â€™18) . Association for Computing Machinery, New York, NY,
USA,31Å›40. https://doi.org/10.1145/3196321.3196332
[46]DagI.K.Sjoberg,BenteAnda,ErikArisholm,ToreDyba,MagneJorgensen,Amela
Karahasanovic,EspenFrimannKoren,andMarekVokÃ¡c.2002. Conductingrealis-
tic experiments in software engineering. In Proceedings International Symposium
on Empirical Software Engineering . 17Å›26. https://doi.org/10.1109/ISESE.2002.
1166921
[47]Margaret-Anne Storey, Thomas Zimmermann, Christian Bird, Jacek Czerwonka,
Brendan Murphy, and Eirini Kalliamvakou. 2021. Towards a Theory of Software
Developer Job Satisfaction and Perceived Productivity. IEEE Transactions on
Software Engineering 47, 10 (2021), 2125Å›2142. https://doi.org/10.1109/TSE.2019.
2944354
[48] The Standish Group.1995. The CHAOS report.[49]Ayse Tosun, Oscar Dieste, Davide Fucci, Sira Vegas, Burak Turhan, Hakan Er-
dogmus, Adrian Santos, Markku Oivo, Kimmo Toro, Janne Jarvinen, and Natalia
Juristo. 2017. An industry experiment on the effects of test-driven development
on external quality and productivity. Empirical Software Engineering 22, 6 (2017),
2763Å›2805. https://doi.org/10.1007/s10664-016-9490-0
[50]StefanWagnerandFlorianDeissenboeck.2019.DefiningProductivityinSoftware
Engineering. In RethinkingProductivityinSoftwareEngineering ,CaitlinSadowski
and Thomas Zimmermann (Eds.). Apress, Berkeley, CA, 29Å›38. https://doi.org/
10.1007/978-1-4842-4221-6_4
[51]ZhendongWang, Yi Wang, andDavidRedmiles. 2018. Competence-confidence
gap:Athreattofemaledevelopersâ€™contributiononGithub.In 2018IEEE/ACM
40thInternational ConferenceonSoftwareEngineering: SoftwareEngineeringin
Society(ICSE-SEIS . IEEE,81Å›90. https://doi.org/10.1145/3183428.3183437
1313