UsingGraph Neural Networks for Program Termination
Yoav Alon
yoav.alon@bristol.ac.uk
University ofBristol
Bristol, UKCristina David
cristina.david@bristol.ac.uk
University ofBristol
Bristol,UK
ABSTRACT
Termination analyses investigate the termination behaviorof pro-
grams,intendingtodetectnontermination,whichisknowntocause
avarietyofprogrambugs(e.g.hangingprograms,denial-of-service
vulnerabilities). Beyond formal approaches, variousattemptshave
been made to estimate the termination behavior of programs us-
ing neural networks. However, the majority of these approaches
continue to rely onformal methods to provide strong soundness
guarantees and consequently suffer from similar limitations. In
thispaper,wemoveawayfromformalmethodsandembracethe
stochasticnatureofmachinelearningmodels.Insteadofaimingfor
rigorousguaranteesthatcanbeinterpretedbysolvers,ourobjective
is to provide an estimation of a program‚Äôs termination behavior
and of the likely reason for nontermination (when applicable) that
aprogrammercanusefordebuggingpurposes.Comparedtopre-
vious approaches using neural networks for program termination,
we also take advantage of the graph representation of programs by
employing Graph Neural Networks. To further assist programmers
inunderstandinganddebuggingnonterminationbugs,weadaptthe
notionsofattentionandsemanticsegmentation,previouslyusedfor
other application domains, toprograms. Overall, we designedand
implementedclassifiersforprogramterminationbasedonGraph
ConvolutionalNetworksandGraphAttentionNetworks,aswellas
asemanticsegmentationGraphNeuralNetworkthatlocalizesAST
nodes likely to cause nontermination. We also illustrated how the
informationprovidedbysemanticsegmentationcanbecombined
with program slicingtofurtheraid debugging.
CCSCONCEPTS
‚Ä¢Computing methodologies ‚ÜíArtificial intelligence ;Knowl-
edge representation and reasoning ;Neural networks .
KEYWORDS
Graph Neural Networks, Graph Attention Networks, Program Ter-
mination,Program Nontermination
ACM Reference Format:
Yoav Alon and Cristina David. 2022. Using Graph Neural Networks for Pro-
gram Termination. In Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering(ESEC/FSE‚Äô22),November14≈õ18,2022,Singapore,Singapore. ACM,
NewYork, NY, USA, 12pages.https://doi.org/10.1145/3540250.3549095
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Copyright heldby theowner/author(s).
ACM ISBN 978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.35490951 INTRODUCTION
Termination analysis describes a classical decision problem in com-
putabilitytheorywhereprogramterminationhastobedetermined.
It iscriticalfor many applicationssuch assoftware testing, where
nonterminatingprogramswillleadtoinfiniteexecutions.Asproved
by Turing in 1936, a general algorithm that solves the termina-
tion problem for all possible Program-input pairs doesn‚Äôt exist
[41]. While there are a large number of works on termination
analysis, the majority of them employ formal symbolic reason-
ing [14,17,18,20,23,25]. In recent years, various attempts have
beenmadetoestimateterminationbehaviorusingneuralnetworks.
For instance, Giacobbe et al.[22] introduced an approach where
neural networks are trained as ranking functions (i.e. monotone
mapsfromtheprogram‚Äôsstatespacetowell-orderedsets).Asimilar
idea is employed in [ 5], where Abate et al. use a neural network to
fitrankingsupermartingale(RMS)overexecutiontraces.Giventhat
programanalysistaskssuchasterminationanalysisaregenerally
expected to provide formal guarantees, these works use satisfia-
bilitymodulotheories(SMT)solverstoshowthevalidityoftheir
results.Whilepromising,theystillfacelimitationsspecifictofor-
mal symbolic methods. Namely, programs need to be translated to
asymbolicrepresentationtogeneratetheverificationconditions
thatarethenpassedtothesolver.Additionally,theseverification
conditionsmaybeexpressedinundecidablelogicalfragmentsor
may require extraprogram invariants for the proof to succeed.
In this paper, we move away from formal methods and lean into
thestochasticnatureofmachinelearningmodels.Insteadoflooking
forrigorousformalguaranteesthatcanbeinterpretedbysolvers,
ourobjectiveistoprovideanestimationofaprogram‚Äôsterminationbe-
havior, as well as localizing the likely cause of nontermination (when
applicable) thataprogrammercanusefordebuggingpurposes.Our
workalsoservesasastudyoftheapplicabilityofmachinelearning
techniques previously used for other classes of applications to pro-
gramanalysis.Inparticular,asexplainednext,weuseGraphNeural
Networks (GNNs) [ 45] and Graph Attention Networks (GATs) [ 42].
Insteadoflookingatexecutiontracesliketheaforementioned
works, we are interested in using the source code with the assump-
tion that it contains patterns that can assist in understanding its
functionality.Notably,programanalysistechniquesgenerallywork
on source code, and specifically on graph representations of pro-
grams.Toemulatethisformachinelearning,wemakeuseofGNNs,
which are a class of neural networks optimized to perform vari-
ousanalysesonGraph-structureddata.GNNsaregainingalotof
interest as they are being used to analyze Graph-based systems
denotingsocialnetworks[ 44],physicalsystems[ 38],knowledge
graphs [24], Point-cloud classification [ 48] etc. Additionally, GNNs
have recently been applied toprogram analysistaskssuchasvari-
ablemisusedetectionandtypeinference[ 7],andself-supervised
bugdetection andrepair[ 8].
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
910
ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Yoav Alon andCristina David
Inspiredby[ 7,8],weuseGNNstoestimateprogramtermination.
Our baseline program termination classifier is based on Graph
ConvolutionalNetworks (GCN)[ 29].
Onitsown,estimatingaprogram‚Äôsterminationbehaviordoesn‚Äôt
provide a lot of practical help to a programmer interested in under-
standinganddebugginganonterminationbug.Rather,wewould
like to provide additional information such as the code location
corresponding to the likely root cause of the failure (in our case
nontermination).Thisobjectiveissimilartothatoffaultlocaliza-
tion, which takes as input a set of failing and passing test cases,
andproduces arankedlistofpotentialcauses offailure [ 31].
As opposed to fault localization techniques, we are interested
ininvestigatingusingthemechanismsofattentionandsemantic
segmentationfrommachinelearning.Tothebestofourknowledge,
we are the first ones to use attention and segmentation in the
contextofprograms.
Attentionisatechniquethatmimicscognitiveattention.Intu-
itively, it enhances some parts of the input data while diminishing
otherswiththeexpectationthatthenetworkisfocusingonasmall,
butimportantpartofthedata.Inourcontext,weuseattentionto
getanintuitionabouttheinstructionsrelevantfortheestimation
of the termination behavior. This allows us to visualize those parts
of the program that the neural network focuses on to estimate
its termination behavior. To integrate attention in our work, we
buildanotherprogramterminationclassifierinspiredbytheGraph
Attention Network (GAT) architecture described in [ 42]. Given
thevariedinfluencethatdifferentinstructionsinaprogramhave
on its termination behavior, we also expect the attention mecha-
nism to improve the results of classification when compared to the
GCN-basedbaseline.
Tolocalizethelikelycauseofthenonterminationbehavior,we
use semantic segmentation. Usually used in image recognition,
thegoalofsemanticimagesegmentationistolabeleachpixelof
animagewithaparticularclass,allowingonetoidentifyobjects
belonging to that class (e.g. given an image, one can identify a
catthatappearsinit).Inourwork,weusethesameprinciplefor
programstoidentifythosestatementsthatcausenontermination
vs thosethat don‚Äôt.
To further aid debugging, we also show how to use the informa-
tionprovidedbysemanticsegmentationtocarveoutanontermi-
natingslicefromtheoriginalprogram(i.e.asmallersubprogram
exhibitingthesamenonterminationbehaviorastheoriginalpro-
gram).Intuitively,suchasmallerprogramiseasiertounderstand
anddebug.
Our experimental evaluation for multiple datasets, both custom
and based on benchmarks from software verification competitions,
confirmsahighabilitytogeneralizelearnedmodelstounknown
programs.
The main contributionsofthis researchare as follows:
‚Ä¢WedesignedandimplementedaGCN-basedarchitecturefor
the binary classification ofprogram termination.
‚Ä¢WedesignedandimplementedaGAT-basedarchitecturethat
improvestheterminationclassificationusingaself-attention
mechanism and allows visualization of the nodes relevant
when estimating termination.‚Ä¢WedesignedandimplementedasemanticsegmentationGAT
thatlocalizesnodescausingnontermination.Inparticular,
inthiswork,wetrytolocalizetheoutermostinfinitelyloop-
ing constructs.We illustrate how the informationprovided
bysemanticsegmentationcanbecombinedwithprogram
slicing to further aid debugging.
‚Ä¢Wedeviseddatasetsforbothclassificationandsegmentation
ofprogram termination.
2 PRELIMINARIES ON GRAPH NEURAL
NETWORKS
GNNs are an effort to apply deep learning to non-euclidean data
representedasgraphs.Thesenetworkshaverecentlygainedalotof
interest as they are being used to analyze graph-based systems [ 24,
38,44,48].Acomprehensivedescriptionofexistingapproachesfor
GNNs andtheirapplicationscan be foundin[ 45,49,50].
ThebasicideabehindmostGNNarchitecturesisgraphconvo-
lution or message passing, which is adapted from Convolutional
Neural Networks (CNN). Each vertex (node) in the graph has a set
of attributes, which we refer to as a feature vector or an embed-
ding.Then,a graphconvolution estimatesthe features ofa graph
nodeinthenext layerasafunctionofthe neighbors‚Äô features. By
stackingGNNlayerstogether,anodecaneventuallyincorporate
information from other nodes further away. Forinstance,after ùëõ
layers, a node has information about the nodes ùëõsteps away from
it.
Givenagraph ùê∫=(ùëâ,ùê∏),whereùëâdenotesthesetofverticesand
ùê∏representsthesetofedges,messagepassingworksasfollows.For
each node ùëñ‚ààùëâand its embedding ‚Ñé(ùëô)
ùëñat layerùëô, the embeddings
‚Ñé(ùëô)
ùëóof its neighbours ùëó‚ààùëÅ(ùëñ)are aggregated and the current
node‚Äôs embedding is updated to ‚Ñé(ùëô+1)
ùëñusing aggregation function
ùê¥andupdatefunction ùëà:
‚Ñé(ùëô+1)
ùëñ=ùëà(ùëô)(‚Ñé(ùëô)
ùëñ,ùê¥(ùëô)({‚Ñé(ùëô)
ùëó,‚àÄùëó‚ààùëÅ(ùëñ)})) (1)
We useùëÅ(ùëñ)to describe the set of direct neighbors of ùëñ. Each
GNNarchitecturevariestheimplementationoftheupdate ùëàand
aggregation ùê¥functions used for message passing. In this paper,
we make useof two GNN architectures:Graph Convolutional Net-
works (GCNs) and GraphAttention Networks (GATs), which we
willdiscuss insubsequent sections.
3 DESCRIPTION OFOURTECHNIQUE
Inthissection,weprovidedetailsaboutourtechnique.Atahigh
level, we first convert programs to feature graphs and then feed
themintoaGNN.Wewilldescribeeachstep,includingthestructure
ofthe modelsforterminationclassificationandsemanticsegmen-
tation ofthe nodes responsible for nontermination.
3.1 Generation ofFeatureGraphs
Therearemanygraphrepresentationsofaprogram.Inthiswork,
we startfrom the Abstract SyntaxTree (AST), whichisahomoge-
nous,undirectedgraph,whereeachnodedenotesaconstructoccur-
ring in the program. We picked ASTs as the starting point because
theyaresimpletounderstandandconstructwhilecontainingallthe
necessaryinformationforinvestigatingaprogram‚Äôstermination.
911UsingGraphNeural NetworksforProgram Termination ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
(a)
 (b)
(c)
 (d)
Figure1:(a)AST(forclarity,wepointoutthecorrespondinglinesofcode).(b)Visualizationofattentionforbinaryclassification
asedgecolorfromblueforlowattentiontoredforhighattention.Theedgeswithhighattentionarethoseconnectingthe while
nodestotheirloopguards.Amongthese,thehighestattentionisgiventothenonterminatingouterloop.(c)Resultofsemantic
segmentation, where the node corresponding to the outermost infinite loop is coloured in red. (d) Semantic segmentation
visualized togetherwith theattentionextracted fromthesegmentationnetwork. Best viewed incolor.
Asfuturework,wemayconsiderdifferentgraphrepresentations
ofprograms.
We start by generating the AST of each program in our datasets.
Then,allASTsareconvertedinto featuregraphs byconvertingeach
nodetoalocalfeaturevectorinlocalrepresentationor‚Äôone-hot‚Äô-
encoding. For this purpose, all the nodes in the ASTs generated for
eachdatasetaregatheredinadictionary.Thedictionaryholdseach
encoding as the key withthe instructionas the value.
Running example. For illustration, we refer to our running ex-
ample in Figure 3, which contains three loops. Out of them, the
firstouterloopisnonterminatingforanyinitialvalueof ùëèlessthan
2 and ofùëégreater than ùëè. Note that for machine integers (i.e. bit-
vectors),theinnerloopdoesterminateevenwhenstartingfroma ùëêgreater than ùëëbecauseùëëwill eventually underflow, thus triggering
awrap-around behavior.
TherunningexampleisfirstconvertedtoitsASTasshownin
Figure1(a). Then, the AST is converted to the initial feature graph
(before any convolutions). For reasons of space, we don‚Äôt show the
wholefeaturegraph,butratherjustthesubgraphcorresponding
to the instructions while c>d: d-=2 in Figure 2. The dictionary
maps each node in the AST to a One-hot encoding. For instance,
thewhilenodeismapped to 0¬∑¬∑¬∑00010000.Similarly,both nodes
correspondingtovariable dhavethesameencoding, 0¬∑¬∑¬∑00000010.
As we use the same dictionary for a whole dataset, identical nodes
inthe dataset willhave the same encoding.
Our objective when picking ASTs to represent programs is to
reduce the distance between nodes of interest (e.g. loop guards,
912ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Yoav Alon andCristina David
Figure2:ConversionofanASTtoalocalfeaturegraph(using
‚Äôone-hot‚Äô-encoding).
1 def main(a ,b ,c ,d) :
2
3whilea > b:
4 a = 2
5 whilec > d:
6 d ‚àí= 2
7
8whilea > 0:
9 a ‚àí= 2
Figure 3:Runningexample
instructionsaffectingvariablesintheloopguards),thusenabling
GNNs to aggregatefeatures relevant to termination with a small
numberofmessagepassings.Forinstance,inFigure 1(a),thedis-
tance between the three loops and the instructions modifying vari-
ableashared between two of the loops is small, allowing the GNN
toeasilyaggregatetheirfeatures.Atthesametime,theASTrep-
resentationissimpleenoughtofacilitatetheeasyunderstanding
of the visualisation of those parts of the program potentially re-
sponsible for nontermination (Figure 1b,c,d). Understanding the
decisions and warnings raised by program analyses is important to
programmerswhoneedtoeithermarkthemasfalsepositivesor
debug them,as shownlaterinthe paper.
3.2 BinaryClassification ofTerminationBased
on GCN
For our first attempt at a binary classifier for program termination,
we make use of GCN. In particular, we take inspiration from the
architecture in [ 29] in a supervised training setting. Currently, we
donottrainonrecursiveprograms,wherethenonterminationcould
becausedbyinfiniterecursion.Weplantodothisasfuturework.
For now, our datasets contain programs with potentially infinite
loops.
OurterminationclassificationisaGraph-leveltask,wherewe
predictanattributefortheentiregraph(inourcasethetermination
behavior of the corresponding program) by aggregating all node
features. In therestofthissection, we provide anoverview ofthe
architecturein[ 29],followedbyexplaininghowweadaptitforour
task.As explained inSection 2, GNNsuse message passing to aggre-
gatetheinformationaboutanode‚Äôsneighborstoupdatethenode‚Äôs
value.According to [ 29], we use:
‚Ñé(ùëô+1)
ùëñ=ùúé(ùëè(ùëô)+‚àëÔ∏Å
ùëó‚ààùëÅ(ùëñ)1
ùëêùëñùëó‚Ñé(ùëô)
ùëóùëä(ùëô)) (2)
whereùëêùëñùëó=‚àöÔ∏Å
|ùëÅ(ùëñ)|‚àöÔ∏Å
|ùëÅ(ùëó)|,ùëÅ(ùëñ)describes the set of direct
neighbours of ùëñ,andùúéisthe activation function.
Best results are achieved using a rectified linear unit (ReLU)
activationfunction.The weights ùëä(ùëô)are initializedusing Glorot
uniform and the bias ùëèwith zero. The experiments of [ 29] with
other aggregation operators rather than the one used here, such
as Multi-Layer-Perceptron aggregator [ 47] and Graph Attention
Networks [ 42], achievedsimilar results.
For the architecture in [ 29], the graph convolution layers gener-
ate Node-wise features. In our architecture, given that termination
classificationisaGraph-leveltask,wechoosetomeanallthenodes
once graph convolution is done and pass the resulting mean fea-
turevectorthroughthreefullyconnectedlayers.Then,weapply
a softmax function on the resulting Two-dimensional vector to
achieve a binary estimation. The resulting vector is then optimized
bycomputing the Cross-entropy losswiththe ground truth.
Runningexample. ForourrunningexampleinFigure 3,ourclas-
sifiercorrectlyconcludesthattheprogramisnonterminating.As
aforementioned, we operate in a supervised training setting, mean-
ing that our classifier has already been trained on the required
dataset. We will give more details on the training and testing in
Section4.
3.3 BinaryClassification ofTerminationBased
on GAT
IfwerevisittheupdateruleofGCNsgivenbyEquation 2,ituses
the coefficient1
ùëêùëñùëó=1‚àö
|ùëÅ(ùëñ)|‚àö
|ùëÅ(ùëó)|. Intuitively, this coefficient
suggests the importance of node ùëó‚Äôs features for node ùëñand it is
heavily dependent onthe structure ofthe graph (i.e.each node‚Äôs
neighbours).The main idea of GATs [ 42]is tocompute thiscoeffi-
cient implicitly rather than explicitly as GCNs do by considering it
to be a learnable attention mechanism. In the rest of the section,
we refer to this coefficient as the attentionscore .
ThemainobservationthatledustouseGATsisthefactthatnot
allinstructionsinaprogramareequallyimportantwheninvesti-
gatingitsterminationbehavior.Forinstance,loopstendtobemore
important than straight line code, and should therefore be given
increasedattention.Thus,ourtasknaturallylendsitselftousing
the attention mechanism. Moreover, our intention for this work is
not only to design a program termination classifier but also to gain
insights into its decisions. Using GATs helps us in this direction
asitallowsustovisualizethosenodesthatinfluencethedecision
relatedto termination.
Inthissection,weprovideanoverviewofthearchitecturein[ 42],
as wellas explainthe wayinwhich we adapt it for the termination
classificationtask.Westartbyshowinghowtheaggregationand
update of node features ‚Ñé(ùëô)
ùëñto‚Ñé(ùëô+1)
ùëñfor iteration ùëôis derived.
Initially,eachnode feature is passedthroughasimplelinearlayer
bymultiplyingthe feature withalearnableweightmatrix ùëä(ùëô):
913UsingGraphNeural NetworksforProgram Termination ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Figure4:GAT-basedarchitectureforthebinaryclassification
ofprogram termination.
ùëß(ùëô)
ùëñ=ùëä(ùëô)‚Ñé(ùëô)
ùëñ(3)
ToobtainthePair-wiseimportance ùëí(ùëô)
ùëñùëóoftwoneighborfeatures
‚Ñé(ùëô)
ùëñand‚Ñé(ùëô)
ùëóweconcatenatethepreviouslycomputedembeddings
ùëß(ùëô)
ùëñandùëß(ùëô)
ùëóto(ùëß(ùëô)
ùëñ|ùëß(ùëô)
ùëó).Then,theconcatenatedembeddingsare
fedintoanotherlinearlayerwithweightmatrix ùëä(ùëô)
2thatlearnsthe
attention scale ùëé(ùëô). Additionally, a leakyReLU activation function
[46]isappliedto introduce non-linearity:
ùëé(ùëô)=LeakyReLU (ùëä(ùëô)(ùëß(ùëô)
ùëñ|ùëß(ùëô)
ùëó)) (4)
Aftercomputationof ùëé,correspondingtotheactivationscale,the
Pair-wiseimportance ùëí(ùëô)
ùëñùëóoftwoneighborfeatures is:
ùëí(ùëô)
ùëñùëó=LeakyReLU (/vecùëé(ùëô)ùëá(ùëß(ùëô)
ùëñ|ùëß(ùëô)
ùëó)) (5)
As noted by [ 42], the resulting attention scale can be considered
asedgedata.Inourproblemsetting,thisgivesusinsightintothe
importance of two connected syntax tree nodes with respect to the
program‚Äôs termination.
To normalize the attention scores for all incoming edges we
apply asoftmax layer:
ùõº(ùëô)
ùëñùëó=exp(ùëí(ùëô)
ùëñùëó)
/summationtext.1
ùëò‚ààùëÅ(ùëñ)exp(ùëí(ùëô)
ùëñùëò)(6)
Then,theneighborembeddingsareaggregatedandscaledbythe
final attention scores ùõº(ùëô)
ùëñùëó:
‚Ñé(ùëô+1)
ùëñ=ùúé/‚àöÔ∏Åarenleftt‚àöÔ∏ÅA/‚àöÔ∏ÅarenleftexA
/‚àöÔ∏ÅarenleftbtA‚àëÔ∏Å
ùëó‚ààùëÅ(ùëñ)ùõº(ùëô)
ùëñùëóùëß(ùëô)
ùëó/‚àöÔ∏Åarenrightt‚àöÔ∏ÅA/‚àöÔ∏ÅarenrightexA
/‚àöÔ∏ÅarenrightbtA(7)
ThearchitectureofourGAT-basedterminationclassifierisshown
in Figure 4and includes multiple connected GAT layers with ReLu
activation. Graph convolutions extract features to an intermediate
representation,denotedbythelastGATlayerwithReLuactivation.
Asopposedtotheoriginalarchitecturein[ 42],classifyingprogram
termination is a Graph-leveltask. Consequently, we deviseda pre-
diction on the program‚Äôs termination by extracting the mean of
node features and using dense layers with a final softmax layer.
Additionally, we need to associate attention information to eachASTedge(asshowninFigure 1(b)).Consequently,asopposedto
the original architecture, attention is extracted based on the last
graph attention layer such that itcan be attributedto AST edges.
Runningexample. SimilartotheGCN,theGAT-based classifier
identifiesthattherunningexampleisnonterminating.Additionally,
in Figure 1(b) we also obtain a visualization of Edge-wise attention
fromblueforlowattentiontoredforhighattention.Notably,the
edges with high attention are those connecting the whilenodes to
their loop guards. The extraction of attention gives us an intuition
about which nodes have a high influence on the final prediction
provided by the classifier. Although for this example, the edge con-
necting the nonterminating outer loop with its loop guard gets the
highestattention,thedifferencebetweentheattentionscoreslinked
to the three loops is not reliable enough to differentiate between
the three loopsanddeterminethe cause for nontermination.
3.4 Semantic Segmentation ofNodesCausing
Nontermination
While GATs provide some insight into the decision made by the
classifier (by allowing us to visualize those AST nodes influencing
thedecision),wewanttofindthelikelycauseofnontermination.
Inparticular,inthisworkwetrytolocalizetheoutermostinfinitely
looping constructs. Note that in the case where we have several
nested loops and the outer one is infinitely looping, all of the inner
ones will also be visited infinitely often. In such a situation, we
identifytheouterloopasthelikelycauseofnontermination,i.e.the
outermostinfinitelyvisitedloop.Asaforementioned,fornow,wedo
nottrainonrecursive programs,wherethenonterminationcould
becausedbyinfiniterecursion.Thus,inoursettingnontermination
can only be causedbyinfinite loops.
Identifyingtheoutermostinfinitelyvisitedloopisnotpossible
with the information that we gained up until this point. Although
inFigure 1(b),theedgebetweenthenonterminatingouterloopand
its guard gets the highest attention, the edges connecting the other
twoloopsandtheirguardsalsohavehighattentionscores.Thus,
itishardtodifferentiatebetweenthedifferentloopsbasedonthe
attention score alone.
Toachieveourgoalofidentifyingtheoutermostinfinitelyvisited
loop, we make use of semantic segmentation. Semantic segmenta-
tionhasbeenprimarilyusedinimagerecognitiontolabeleachpixel
ofanimagewithaparticularclass,allowingonetoidentifyobjects
belongingtothatclass.Sometimesintheliterature,thesemantic
segmentation of graphs is also named ‚ÄôNode-wise classification‚Äô or
‚ÄôNode-classification‚Äô.
Inthiswork,weattempttoextendthesameprinciplethatwas
appliedinimagerecognitiontoprogramsbyusingsemanticseg-
mentation to identify thosestatements thatcausenontermination.
If we consider that program statementscan belong to two classes,
namelythosethatcausenonterminationandthosethatdon‚Äôt,we
can envisionthat segmentationmayhelpidentifythe former.
Thegraphnetworkmustconvertfeaturevectorstoanestimation.
Forthispurpose,itistrainedonthelossbetweengroundtruthand
prediction on a node level. This can be accomplished with most
segmentation loss functions such as a simple Cross-entropy loss
914ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Yoav Alon andCristina David
Figure 5: Proposed GAT-based architecture for the segmenta-
tion ofnodes causing nontermination.
for binary segmentationwithground truth ùë¶andprediction ÀÜùë¶:
ùêø(ùë¶,ÀÜùë¶)=‚àí(ùë¶log(ÀÜùë¶) + (1‚àíùë¶)log(1‚àíÀÜùë¶)) (8)
Althoughtheabovelossenablesthesetupofaninitialtraining
session, it is not ideal to eradicate hard negatives. To improve
andcontinuetrainingbeyondbinaryCross-entropyconvergence,
we use focal loss [ 28,35], an extension of Cross-entropy, which
down-weights simple samples and gives additional weight to hard
negatives:
ùêπùêø(ùëùùë°)=‚àíùõºùë°(1‚àíùëùùë°)ùõæùëôùëúùëî(ùëùùë°) (9)
withùõæ,ùõº‚àà [0,1]andthemodulationfact (1‚àíùëùùë°)ùõæ,whereùëùùë°=ùëù
forùë¶=1andùëùùë°=1‚àíùëùotherwise.
FollowingthesamemessagepassingmethodexplainedinSec-
tion3.2,the newnode features holdthe segmentationprediction.
Our architecture for semanticsegmentation is visualized in Fig-
ure5.Graphattentionconvolutiongeneratesasemanticsegmen-
tation resulting in a Node-wise estimation of likelihood to cause
nontermination. By the attention mechanism, Edge-wise scores
denotethe weightoffeatures basedonrelationalpatterns.
Runningexample. Theresultofsemanticsegmentationforour
running example is highlighted in Figure 1(c), where the Red-
colorednodedenotestheoutermostinfiniteloop.Figure 1(d)shows
the result ofsegmentationwithattention.
3.5 UsingtheResultofSegmentation for
Debugging
One issue with the result of segmentation is that it only highlights
the node corresponding to the head of the outermost nontermi-
nating loop, rather than all the statementscontributing to nonter-
mination.ThisisobviousforourrunningexampleinFigure 1(c),
where segmentation only identifies the node corresponding to the
nonterminating whileloop.
Thiswasaconsciousdecisiononourpart,meanttosimplifythe
construction of the required training datasets. While there are a
hugenumberofprogramsavailableonline,it‚Äôsnotatallstraightfor-
ward to use them for training machine Learning-based techniques
forprogramanalysis.Themainreasonforthisisthatsuchprograms
are not labeled with the result of any analysis, and manually label-
ing them is difficult and Error-prone. In particular, for the programtermination classification task, we must know which programs are
terminating and which are not. Even more problematic, for seman-
ticsegmentation,theannotationshould,inprinciple,identifyall
theinstructionscontributingtonontermination.Forinstance,for
our running example, it should identify lines 3 and 4. In general, it
can be very difficult to annotate such datasets.
In this work, to simplify the annotation for segmentation, we
only label the node corresponding to the head of the outermost
infinite loop as the reason for nontermination. More details on the
waywe generateour datasets are given inSection 4.
Whileidentifyingtheoutermostinfiniteloopisalreadyuseful,
debuggingcanbefurtheraidedbyfindingtherestoftheinstruc-
tions that contribute to nontermination. This can be achieved with
slicing[43].Ingeneral,slicingisaprogramanalysistechniquethat
aims to extract parts of a program according to a particular slicing
criterion(e.g.itcanextractinstructionsresponsibleforthewrite
accessestoaparticularvariable).Inourscenario,givenanontermi-
natingloopreturnedbysegmentation,wecanuseseveralslicing
criteria in order to isolate the faulty loop, while preserving the
nonterminationbehaviorhighlightedbysegmentation.Oneoption,
whichweillustratebelowontherunningexampleinFigure 3,is
a criterion aiming to preserve the reachability of a control flow
point(inparticular,weareinterestedinthecontrolflowpointat
the entrance to the nonterminating loop). Among other tools, such
aslicingcapabilityisprovidedbythesoftwareanalysisplatform
Frama-C [ 9]. Other slicing criteria can be used, e.g. preserving the
valuesofthevariablesusedbytheguardoftheinfiniteloop.Slicing
has been previously used in the context of debugging nontermina-
tion,e.g.Failure-slices in[ 36].
Going back to our running example in Figure 3, once segmen-
tation has identified the first outer loop as the likely culprit for
nontermination, we slice the program such that we preserve the
reachabilityofthecontrolflowpointattheentrancetothebody
of the nonterminating outer loop. This gives us the program in
Figure6. This sliced program preserves the nontermination be-
havioroftheoriginalprogram(inducedbytheloopidentifiedby
segmentation)whilecuttingdownthesyntacticstructure,which
correspondsto reducing its state space.
Intuitively,cuttingdownthestatespaceoftheprogrammakes
itmoreamenabletootherexistingdebuggingtechniquessuchas
fuzzing.For illustration,we have askedlibFuzzer [ 2](alibrary for
Coverage-guidedfuzztesting)tosearchforaninputthattriggers
the execution of the sliced program longer than 5s. libFuzzer was
abletofindtheinput ùëé=165017090and ùëè=-183891446within6s.
Thisinputtriggersthenonterminationbehaviorandcanassistin
further debugging. Conversely, when asking libFuzzer to find such
input for the original in Figure 3, it required 12s. We expect the
difference between the two running times to increase for larger
programs.
Overall,thedebuggingworkflowthatweenvisionfollowsthe
followingsteps:(1)ourclassifierestimatesthataprogramisnon-
terminating;(2)ourtechniquebasedonGNNsandsemanticseg-
mentation finds the likely root cause of nontermination; (3) slicing
cuts down the syntactic structure of the program and implicitly its
state space, while aiming to maintain the faulty nontermination
behavior; (4) fuzzingfinds faultyinputsfor the slicedprogram.
915UsingGraphNeural NetworksforProgram Termination ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
1 def main(a ,b) :
2whilea > b:
3 a = 2
Figure 6:Nonterminatingslice oftherunningexample
Ifaprogramcontainsseveralnonterminationbugs,thedebug-
ging workflow above may need to be repeated until all of the bugs
are eliminated.
4 EXPERIMENTS
4.1 System
Weimplementedtheproposedideasasatoolcalled GraphTerm .
We used the deep learning framework PyTorch [ 37] with the GNN
packageDGL(DeepGraphLibrary)andvariouspackagesforpre-
processing and visualization. The experiments were performed
using an RTX 2080 8GB GPU on a local machine. The architecture
oftheGNNsused,dependingontheparticularexperimentconsists
of between 4 and 6 graph layers. While this is typical for the effec-
tivetrainingofGNNs,itcouldbeextendedusingresidualblocks
[34].
4.2 BinaryClassification ofProgram
Termination
We startbydiscussing ourexperimentalevaluationfor thebinary
classification of program termination. Given a program, Graph-
Term‚Äôs classifier extracts a termination estimation, i.e. whether the
programisterminatingornonterminating.Concerningthelatter,a
programisconsiderednonterminatingifthereexistsaninputfor
whichthe program‚Äôs executionisinfinite.
4.2.1 Classification Datasets. Our experiments on classification
and semantic segmentation of the cause for nontermination re-
quire different datasets. This is because classification data requires
Program-level annotations (a program is labeled as terminating or
nonterminating), whereassegmentation requiresNode-wiseanno-
tations (each node in the AST is either a cause for nontermination
ornot).Inthissection,wediscussthedatasetsusedforclassifica-
tion,whereasmoredetailsonthoseusedforsegmentationcanbe
foundinSection 4.3.1.
Forclassification,weuseatotaloffourdatasets,outofwhich
two are based on existing benchmarks and two are generated by
us.Concerningthedatasetscontainingexistingbenchmarks,the
first one, DS-SV-COMP, contains C programs from the termination
category of the SV-COMP 2022 competition [ 1,11]. In particular,
thisdatasetcontainsallprogramsfromthefollowingsubcategories:
termination-crafted,termination-crafted-lit,termination-numeric,
andtermination-restricted-15.Intotal,thereare55nonterminating
programs and 194 terminating. For a total of 249 programs, we
count a total of 386 loops with a maximum of five nested loops for
the program NO_04.c. The second dataset of existing benchmarks,
DS-TERM-COMP,contains 150Cprograms fromtheTermination
Competition [ 3] with a total of 452 loops and a maximum of 2
nestedloopsperprogram.Thesebenchmarksareselectedsuchthat
there isnooverlap withthoseinthe DS-SV-COMPdataset.The benchmarks in both DS-SV-COMP and DS-TERM-COMP
expect inputs meaning that their termination behavior depends on
non-deterministic values. These benchmarks come already labeled
as terminating or nonterminating, where the nontermination label
indicates that there exists an input for which the program‚Äôs execu-
tion doesn‚Äôt terminate. The pre-processing of the datasets includes
thegenerationoftheASTrepresentationforeachprogramandthe
conversionto afeature graph using DGL.
Forefficiencyreasons,weconsiderbatchesof30graphsusingthe
ùëëùëîùëô.ùëèùëéùë°ùëê‚Ñé .Thetwodatasetshaveanassociateddictionaryfeaturing
allthedistinctASTnodesfromalltheprograms.Intuitively,this
dictionary gives us the vocabulary used by the benchmarks. For
more details onthe generationof feature graphs, see Section 3.1.
AspreviouslydiscussedinSection 3.5,oneofthemainchallenges
ofourworkisthefactthat,whiletherearemanyavailableprograms,
onlyafewarealreadylabeledasterminatingornonterminating(as
wecouldseeabove,evenexistingsoftwareverificationcompetitions
only consider a relatively small number of benchmarks). However,
machine learning techniques generally require large training data,
whichis notavailablein oursetting.Due tothis, wechoseto also
generateouradditionalcustomdatasets.Forthefirstcustomdataset,
DS1,weusedthedictionarygeneratedforDS-SV-COMPandDS-
TERM-COMP,meaningthatthevocabularyofDS1isthesameas
thatofDS-SV-COMPandDS-TERM-COMP.Thisisimportantas
it allows us to train on DS1 and only use DS-SV-COMP and DS-
TERM-COMPfortesting.DS1contains950Cprograms.Thesecond
custom dataset, DS2, contains 950 generated Python programs, out
ofwhich800are usedfor training and150for testing.
TolabelthebenchmarksinDS1andDS2,wefuzztestfornonter-
mination by generating a large number of inputs for each program.
Iftheexecution timereachesa predefined timeout for at least one
oftheinputs,thenwelabeltheprogramasnonterminating.Asa
remark, while this doesn‚Äôt ensure that a program is indeed non-
terminating, it does signal a potential performance bug. From a
practical point of view, a performance bug is equally important
for programmers to debug. Alternatively, we could use an existing
termination analysis tool based on formal methods to label the
generated programs. The dataset is balanced by providing an equal
number ofterminating andnonterminatingsamples.
Apart from DS-SV-COMP and DS-TERM-COMP, each dataset is
splitintotrainingandtestsamplesbythegeneralruleofapproxi-
mately80/20dependingonthedatasetsize.Theexactsplitforeach
setisspecifiedinTable 1.
4.2.2 Training. As explained in Section 3, we deploy two different
neuralnetworks,onebasedonGCN[ 29]andanotheronebasedon
GAT[42].Inourexperiments,wefoundthat4layersweresufficient
for both the GCN and GAT architectures, respectively. Training is
performedusinganAdam-optimizerwithaninitiallearningrate
of 0.0001. We use a regular Cross-entropy loss function and record
variousmetricssuchastheReceiverOperatingCharacteristic(ROC)
curve andthePrecision-Recall duringtraining both forvalidation
andtesting. For asignificant evaluation, we perform a total of ten
training sessions, where each session is stopped as soon as the
validationerrorreachesaminimum.Inordertoavoidoverfitting,
we use the following techniques:
916ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Yoav Alon andCristina David
Table 1:Datasetsused inthiswork.
Dataset Origin Language Experiment Training samples Testsamples
DS-SV-COMP SV-Comp C classification - 249
DS-TERM-COMP TermComp C classification - 150
DS1 custom C classification 800 150
DS2 custom Python classification 800 150
DS-Seg-Py1 custom Python semantic segmentation 180 50
DS-Seg-C custom C semantic segmentation 180 50
‚Ä¢Amodelwithtoomuchcapacitycanlearnaproblemtoowell
andoverfitthetrainingdataset.Weavoidthisbyadapting
the number and dimensionality of layers to simplify the
model. In particular, asaforementioned, we use4layers for
the GCNandGATarchitectures, respectively.
‚Ä¢Anetwork withlargevaluesforweightscanalsosignalthat
thenetworkhasoverfitthetrainingdataset.Weavoidthis
byusingweightregularization,whichinvolvesupdatingthe
learning algorithm to encourage the network to keep the
weightssmall.
‚Ä¢Anotherchallengeistheamountoftimespentontraining
neuralnetworks,wheretoomuchtrainingwilloverfitthe
training dataset. When training our model, we use early
stopping [ 13] by stopping training as soon as the validation
errorreaches aminimum.
‚Ä¢Additionally, we use Cross-validation, where we sample dif-
ferent portions of the data to train and test a model in dif-
ferent iterations. Therefore,the modelmay performwell in
sometrainingiterations,butworseonothers.Theseinsights
allowedus totunethehyperparametersforthenetworkin
order to achieve optimal training andreduce overfitting.
4.2.3 Results and Evaluation. To judge the performance of Graph-
Term‚Äôs classifier, we use the Precision-Recall (PR) and Receiver
Operating Characteristic (ROC) and their respective Area Under
the Curve (AUC) and Average Precision (AP). AUC takes values
from 0 to 1, where value 0 indicates a perfectly inaccurate test and
1reflectsaperfectlyaccuratetest.
PR is used as an indication for the tradeoff between precision
andrecall fordifferentthresholds.Consequently,highrecall and
highprecisionreflectinahighareaunderthecurve.Highprecision
isbasedonalowrateoffalsepositivesandahighrecallisbased
onalowrateoffalse negatives.
The ROC curve plots the true positive rate versus the false posi-
tiverateforeachthreshold.TheclosertotheTop-leftcorneraROC
curveis,thebetter,andthediagonallinecorrespondstorandom
guessing. A high area under the curve indicates a better classifica-
tionperformance.
Theresultsoftheexperimentalevaluationofourclassifierare
summarized in Table 2, where we record the AUC for positive
(terminating) instances, the AUC for negative (nonterminating)instances,andthemeanAveragePrecision(mAP),whichiscalcu-
lated as the average of the AP for terminating and nonterminating
instances.
The first and arguably most important observation is that, with
values of over 0.82 for all mAPs for both PR and ROC, we can con-
clude that classification results are significant and that we achieve
ahigh abilityto generalize for unknowndata.
ThesecondremarkreferstothecomparisonofGCNandGAT.
AsobservedinTable 2,thePRandROCmAPnumbersaregenerally
higherforGATthanGCN,suggestingthattheapplicationofgraph
attention improves the result of the binary classification (we use
bold font for the higher mAP numbers). This indicates that the
appliedself-attention mechanismdoes assign a higherweightto
patterns responsible for deciding the terminationbehavior.
The third comparison we are interested in is between the classi-
ficationofterminatingandnonterminatingprograms.Wenotice
that the PR AUC negative tends to be higher than AUC positive,
meaningthattheclassifierperformsbetteratidentifyingnontermi-
nating programs. We conjecture that nonterminating patterns are
easier to identify based on relational probabilistic patterns that are
likely to cause nontermination.
InFigure 7wevisualizetheROCcurvesperclassandlayertype.
Thebetterperformancefornonterminationclassesisreflectedin
higher red curves while the better performance of GAT layers is
represented by higher continuous lines compared to doted GCN
curves.
4.3 Semantic Segmentation ofNodesCausing
Nontermination
Inthissection,wedescribethe GraphTerm experimentsforidenti-
fying the outermost nonterminating node using semantic segmen-
tation.
4.3.1 SegmentationDatasets. Thereareseveralreasonsthatpre-
vent us from reusing the same benchmarks from DS-SV-COMP
and DS-TERM-COMP for semantic segmentation. Firstly, they only
containaProgram-levellabeldenotingwhethertheprogramter-
minates. For semantic segmentation, we require node-level annota-
tionsthatindicatewhetherthecorrespondingASTnodeislikely
tocausenonterminationornot.Secondly,itisoftenverydifficult
to determine the cause for nontermination in existing benchmarks,
as itisn‚Äôtlabeled.
917UsingGraphNeural NetworksforProgram Termination ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Figure 7: ROC curves per class (class 0 corresponds to non-
terminatingprogramsandclass1toterminatingones)and
experimentalconfiguration.
For this reason, we generate two datasets that consist only of
nonterminating samples (as we would only attempt to find the
cause for nontermination for those programs that were deemed
nonterminatingbyourclassifier).AsexplainedinSection 3.5,we
restrictourannotationstoonlyidentifytheoutermostnontermi-
natingloopintheprogram.Notethatifthereareseveraldistinct,
outermost, nonterminating loops, only one of them will be labeled
ascausingnontermination.Thereasonforthisisthefactthatwe
use fuzz testing to determine the culpable loop, and we stop the
fuzzingoncewefoundthefirstnonterminatingscenario.Weuse
label 0 to indicate that the corresponding AST node has no rela-
tion to nontermination, and label 1 to indicate a high likelihood of
causing nontermination.
ThetwodatasetsaredescribedinthelasttworowsofTable 1.
Thefirst,DS-SegPy1containsPythonprograms,whereasthesec-
ond, DS-Seg-C, contains C programs. Each dataset is composed of
180programsfortrainingand50fortesting.Theprogramsample
generator for custom data is probabilistic and generates programs
that contain from twoto five loopsthat can be nested.
4.3.2 Training. Toimproveandcontinuetrainingbeyondbinary
Cross-entropy convergence,wedeployedafocalloss [ 28,35],an
extensionofCross-entropythatDown-weightssimplesamplesand
gives additionalweightto hardnegatives.
To achieve optimal results, the experiments required modifying
the learning rate, optimizer, network structure, and the number of
features compared to those used for classification. The final config-
urationfeaturesanAdamoptimizerwithalearningrateof0.001.
In our experiments, we found that 4 layers were sufficient for both
theGCN andGATarchitectures, respectively.While theGCN and
GATarchitecturesusedforsemanticsegmentationarerespectively
similarto thoseused for classification,they donot contain a node
mean function and linear layers. The last GCN/GAT layer projects
to a One-dimensional feature indicating the confidence that a node
iscausing nontermination.Similar to the experiments for classification, we perform a total
of ten training sessions, and use the same techniques for avoiding
overfitting.
4.3.3 ResultsandEvaluation. Themetricsused forevaluationare
Jaccardlossbasedonintersectionoverunion(IoU),therelatedDice-
Coefficient,andNode-wiseaccuracy.Acomprehensiveoverview
ofthese metricsis providedby[ 27].Similar toimage segmentation
wherethe Pixel-wiseaccuracyisdetermined,here the Node-wise
distinction of true-positives, true-negatives, false-positives, and
false-negatives for the node prediction compared to the annotated
node ground truthisused.
We calculate the mean metrics for an average of 10 models that
were trained to a convergence of validation scores for each experi-
mentalconfiguration.Intheevaluation,weprovidemeanvalues
withstandarddeviation.
From the results for semantic segmentation (detailed in Table 3)
we can infer several observations. With mean values of more than
0.84forDiceandIoUandanodeaccuracyof0.81inallexperimental
validationsets,theabilitytogeneralizeforunknownprogramsis
high.Thelowstandarddeviationindicatesarobustresultforthe
state ofconvergenceofvalidation metrics.
For datasetSeg-Py1themeanDice-Coefficientishigherforthe
GATarchitectureby3.3percentthanforGCN.Similarly,nodeac-
curacyisbetterby4.7percentwithonlyadecreaseof0.8percent
for the IoU. This indicates an improvement in segmentation per-
formance for GAT compared to GCN. Similarly, for C programs
using dataset Seg-C, all metrics improved significantly with a 5
percent better performance for Dice-Coefficient and node accuracy
and an increase of 12 percent for IoU. From the comparative better
performanceofGAT-layersfornode-wisesegmentation,weinfer
thattheuseofself-attentionmechanismsenablesthenetworkto
weigh more relevant node transitions and therefore achieve better
performance. Higher attention is assigned to edges with a high
likelihoodofcausing nontermination.
4.4 Comparisonto RecurrentAlgorithms
In Section 1, we discussed the suitability of GNNs for analyzing
programs given the inherent graph structure of programs. The
objectiveofthecurrentsectionistojustifythisstatementbyevalu-
ating whether GNNs are better at estimating program termination
than recurrent algorithms. Note that the latter are based on a se-
quentialprogram representation.
For this purpose, we create an additional set of experiments
forthebinaryclassificationofprogramtermination.Tomakethe
comparisonfair,theexperimentalconfigurationensuresthatthe
numberoflearnableparametersofallrecurrentalgorithmsishigher
than the learnable parameters of the GNNs they were compared
against. We use the PyTorch implementations for RNNs [ 40], Long
Short-term memory (LSTM) [ 26] and Gated recurrent unit‚Äôs (GRU)
[16].
For this experiment, we only use the DS2 dataset containing
Python programs. Conversely to the previous experiments, for
therecurrentalgorithms,theprogramsarenotconvertedintoan
ASTbutaredirectlyencodedintoaone-hotencodingbasedona
dictionary ofunique instructions.
918ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Yoav Alon andCristina David
Figure8:Binaryclassificationforterminationinrecurrent
andgraphneuralnetworks.Thegraphneuralnetworkbased
approaches perform significantly better than classical recur-
rentapproaches.
Forasignificantevaluation,weperformatotaloftentraining
sessions until convergenceofthe test metrics.
4.4.1 Results. Similartothepreviousclassificationexperiments
werecordthe Precision-Recall(PR)andReceiverOperatingChar-
acteristic(ROC)andtheirrespective AUC.
We summarize our experimental results in Table 4. Notably,
graph-based methods outperform recurrent approaches by more
than 5percent forthe bestrecurrent ROC-AUCand PR-AUCmet-
rics.Intuitively,weinferthatthegraphrepresentationofprograms
isabetter fitforterminationestimationthan thesequentialrepre-
sentation. Furthermore, the understanding of the context within
the recurrent networks is limited by the model capacity and by the
naturalconstraints that don‚Äôtallow for large sequences of data to
be processed.
Additionalinsightintothecomparisonoftheclassificationre-
sultscanbegainedbyexaminingFigure 8,whereitcanbeobserved
thattheGNNandGAT-basedarchitecturesperformbetterthanthe
recurrentapproaches.
5 RELATED WORK
5.1 TerminationAnalysis
Termination analysis has been studied for a long time, with ap-
proachesgenerallymakinguseofsymbolicmethodssuchasloop
summarization[ 15,51],programsynthesis[ 20,21],quantifierelim-
ination [32], abstract interpretation [ 19]. Most of the techniques
work by computing ranking functions, i.e. monotone maps from
the program‚Äôs state space to well-ordered sets, for linear programs
over integersorrationals[ 10,17,18,30,33].
Recently, there were attempts to employ machine learning tech-
niques for programtermination. For instance, NeuralTermination
Analysis [ 22] trains neural networks as ranking functions. This
method uses sampled executions to learn ranking functions, whichare then verified with an SMT solver. Although able to provide
strongsoundnessguarantees,thistechniquesuffersfromcertain
limitations, e.g. loops with a limited number of iterations may not
provideenoughdatatolearnarankingfunction,theverification
oftherankingfunctionmayrequireadditionalloopinvariantsto
succeed.CaludeandDumitrescuproposedaprobabilisticalgorithm
fortheHaltingproblembasedonrunningtimes,wheretheydefine
a class of computable probability distributions on the set of halting
programs[ 12].In[6],Abateetal.presentthefirstmachinelearn-
ing approach to the termination analysis of probabilistic programs,
where they use a neural network to fit a ranking supermartingales
over execution traces and then verify itover the sourcecode with
an SMTsolver.
Asopposedtotheseexistingworks,wedonotattempttopro-
videstrongguaranteesabouttheterminationdecision.Instead,our
objective is to provide an estimation of a program‚Äôs termination
behavior,aswellaslocalizingthelikelycauseofnontermination
(when applicable) that a programmer can use for debugging pur-
poses. For this purpose, we employ the attention mechanism to
identify those nodes relevant for the termination estimation. More-
over,forprogramsclassifiedasnonterminating,weusesemantic
segmentationtodistinguishtheoutermostloopcausingtheinfinite
execution.
5.2 GraphNeuralNetworks
TheoriginalapproachtoGNNsaspresentedbyKipfandWelling
[29] used the sum of normalized neighbor embeddings as aggrega-
tioninaself-loop.WithaMulti-Layer-Perceptronasanaggregator,
Zaheer et al. [ 47] presented an approach that propagates states
through a trainable MLP. With the development of advanced at-
tention networks, the approach of Velickovic et al. [ 42] focused on
attentionweightsthatallowtoprioritizetheinfluenceoffeatures
based on self-learned attention. For heterogeneous graphs with
additionaledgefeatures,RelationalGraphConvolutionNetworks
wereintroducedbySchlichtkrullet.al.[ 39]toenablelinkprediction
and entity classification, allowing the recovery of missing entity
attributes for high-dimensional knowledge graphs.
6 CONCLUSIONS
We proposeda techniquefor estimating thetermination behavior
of programs using GNNs. We also devised a GAT architecture that
uses a self-attention mechanism to allow the visualization of nodes
relevantfortheterminationdecision.Finally,fornonterminating
programs, we constructed a GAT for the semantic segmentation
of those nodes likely responsible for nontermination. Additionally,
we illustrated the use of our technique together with slicing and
fuzzingfordebuggingofnonterminationbugs.Weimplementedthe
toolGraphTerm andexperimentallyevaluateditonprogramsfrom
twoverificationcompetitions,aswellasonotherprograms.The
experimental results for GraphTerm confirm its ability to general-
izetounknownprogramswhenestimatingtheirterminationand
locatingthecauseofnonterminationfornonterminatingprograms.
7 DATA-AVAILABILITYSTATEMENT
Our prototype implementation of GraphTerm is publicly available
at [4].
919UsingGraphNeural NetworksforProgram Termination ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Table 2: Experimental results for the binary classificationof program terminationusing different network architectures and
datasets.ROCandPrecision-Recallare recorded after atestaccuracyconvergence.
Precision-Recall ROC
Approach Dataset mAP AUCnegative AUCpositive mAP AUCneg. AUCpos.
GCN DS-SV-COMP 0.83 0.94 0.76 0.87 0.88 0.88
DS-TERM-COMP 0.87 0.92 0.84 0.90 0.89 0.89
DS1 0.92 0.94 0.90 0.93 0.92 0.92
DS2 0.87 0.91 0.88 0.91 0.91 0.91
GAT DS-SV-COMP 0.9 0.96 0.86 0.93 0.93 0.93
DS-TERM-COMP 0.91 0.93 0.89 0.92 0.91 0.91
DS1 0.93 0.91 0.74 0.93 0.94 0.93
DS2 0.90 0.94 0.88 0.92 0.93 0.93
Table3:Experimentalresultsforthesemanticsegmentationofterminationusingdifferentnetworkarchitecturesanddatasets.
Dice-Coefficient Jaccard Index (IoU) NodeAccuracy
Approach Dataset Language value ùúévalue ùúé value ùúé
GCN Seg-Py1 Python 0.843 0.021 0.856 0.029 0.810 0.026
Seg-C C 0.896 0.031 0.812 0.029 0.892 0.034
GAT Seg-Py1 Python 0.876 0.020 0.848 0.024 0.857 0.023
Seg-C C 0.947 0.012 0.943 0.024 0.944 0.013
Table 4: Experimental comparison of binary classification
usingRNNs vs.GNNs.
Approach Network ROC-AUC PR-AUC
recurrent RNN[40] 0.78 0.77
LSTM [26] 0.88 0.87
GRU [16] 0.88 0.88
graphbased GCN(custom) 0.94 0.94
GAT(custom) 0.96 0.96
ACKNOWLEDGMENTS
ThisworkwassupportedbytheRoyalSocietyUniversityResearch
Fellowship UF160079 and the Royal Society Research Grant for
ResearchFellowsRGF\R1\181003.
REFERENCES
[1]2021. Competition on Software Verification. https://sv-comp.sosy-lab.org/2022/
Accessed 09/05/2021.
[2]2021. libFuzzer≈õalibraryforcoverage-guidedfuzztesting. https://llvm.org/
docs/LibFuzzer.html Accessed 30/07/2021.
[3]2021. Termination Competition (TermCOMP). https://github.com/TermCOMP/
TPDBAccessed 09/05/2021.
[4] 2022. GraphTerm. https://doi.org/10.5281/zenodo.7083445
[5]AlessandroAbate,MircoGiacobbe,andDiptarkoRoy.2021.LearningProbabilistic
TerminationProofs.In ComputerAidedVerification-33rdInternationalConference,
CAV 2021, Virtual Event, July 20-23, 2021, Proceedings, Part II (Lecture Notes in
Computer Science, Vol. 12760) , Alexandra Silva and K. Rustan M. Leino (Eds.).
Springer, 3≈õ26. https://doi.org/10.1007/978-3-030-81688-9_1
[6]AlessandroAbate,MircoGiacobbe,andDiptarkoRoy.2021.LearningProbabilistic
TerminationProofs.In ComputerAidedVerification-33rdInternationalConference,
CAV 2021, Virtual Event, July 20-23, 2021, Proceedings, Part II (Lecture Notes in
Computer Science, Vol. 12760) , Alexandra Silva and K. Rustan M. Leino (Eds.).
Springer, 3≈õ26. https://doi.org/10.1007/978-3-030-81688-9_1[7]Miltiadis Allamanis. 2022. Graph Neural Networks in Program Analysis . Springer
NatureSingapore,Singapore,483≈õ497. https://doi.org/10.1007/978-981-16-6054-
2_22
[8]Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. 2021. Self-
Supervised Bug Detection and Repair. In Advances in Neural Information Pro-
cessing Systems 34: Annual Conference on Neural Information Processing Sys-
tems 2021, NeurIPS 2021, December 6-14, 2021, virtual , Marc‚ÄôAurelio Ranzato,
Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman
Vaughan (Eds.).27865≈õ27876. https://proceedings.neurips.cc/paper/2021/hash/
ea96efc03b9a050d895110db8c4af057-Abstract.html
[9]Patrick Baudin, Fran√ßois Bobot, David B√ºhler, Lo√Øc Correnson, Florent Kirchner,
NikolaiKosmatov,Andr√©Maroneze,ValentinPerrelle,VirgilePrevosto,Julien
Signoles, and Nicky Williams. 2021. The dogged pursuit of bug-free C programs:
the Frama-C software analysis platform. Commun. ACM 64, 8 (2021), 56≈õ68.
https://doi.org/10.1145/3470569
[10]AmirM.Ben-AmramandSamirGenaim.2013. Onthelinearrankingproblem
forintegerlinear-constraintloops.In The40thAnnualACMSIGPLAN-SIGACT
Symposium on Principles of Programming Languages, POPL ‚Äô13, Rome, Italy -
January 23 - 25, 2013 , Roberto Giacobazzi and Radhia Cousot (Eds.). ACM, 51≈õ62.
https://doi.org/10.1145/2429069.2429078
[11]D. Beyer. 2022. Progress on Software Verification: SV-COMP 2022. In Proc.
TACAS(2) (LNCS13244) .Springer,375≈õ402. https://doi.org/10.1007/978-3-030-
99527-0_20
[12]Cristian S. Calude and Monica Dumitrescu. 2018. A probabilistic anytime
algorithm for the halting problem. Comput. 7, 2-3 (2018), 259≈õ271. https:
//doi.org/10.3233/COM-170073
[13]Rich Caruana, Steve Lawrence, and C. Lee Giles. 2000. Overfitting in Neural
Nets:Backpropagation,ConjugateGradient,andEarlyStopping..In NIPS,ToddK.
Leen, Thomas G. Dietterich, and Volker Tresp(Eds.). MIT Press, 402≈õ408. http:
//dblp.uni-trier.de/db/conf/nips/nips2000.html#CaruanaLG00
[14]KrishnenduChatterjee,EhsanKafshdarGoharshady,PetrNovotn√Ω,andDorde
Zikelic.2021. Provingnon-terminationbyprogramreversal.In PLDI‚Äô21:42nd
ACMSIGPLANInternationalConferenceonProgrammingLanguageDesignand
Implementation, Virtual Event, Canada, June 20-25, 20211 , Stephen N. Freund and
Eran Yahav (Eds.).ACM,1033≈õ1048. https://doi.org/10.1145/3453483.3454093
[15]Hong-Yi Chen, Cristina David, Daniel Kroening, Peter Schrammel, and Bj√∂rn
Wachter.2018. Bit-PreciseProcedure-ModularTerminationAnalysis. ACMTrans.
Program. Lang. Syst. 40,1 (2018), 1:1≈õ1:38. https://doi.org/10.1145/3121136
[16]JunyoungChung,√áaglarG√ºl√ßehre,KyungHyunCho,andYoshuaBengio.2014.
EmpiricalEvaluationofGatedRecurrentNeuralNetworksonSequenceModeling.
CoRRabs/1412.3555(2014). arXiv: 1412.3555 http://arxiv.org/abs/1412.3555
920ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Yoav Alon andCristina David
[17]Byron Cook, Andreas Podelski, and Andrey Rybalchenko. 2006. Termination
proofs for systems code. In Proceedings of the ACM SIGPLAN 2006 Conference on
Programming Language Design and Implementation, Ottawa, Ontario, Canada,
June11-14,2006 ,MichaelI.SchwartzbachandThomasBall(Eds.).ACM,415≈õ426.
https://doi.org/10.1145/1133981.1134029
[18]ByronCook,AbigailSee,andFlorianZuleger.2013. Ramseyvs.Lexicographic
TerminationProving.In ToolsandAlgorithmsfortheConstructionandAnalysisof
Systems - 19th International Conference, TACAS 2013, Held as Part of the European
Joint Conferences on Theory and Practice of Software, ETAPS 2013, Rome, Italy,
March 16-24, 2013. Proceedings (Lecture Notes in Computer Science, Vol. 7795) , Nir
PitermanandScottA.Smolka(Eds.).Springer,47≈õ61. https://doi.org/10.1007/978-
3-642-36742-7_4
[19]PatrickCousotandRadhiaCousot.2012. Anabstractinterpretationframework
for termination. In Proceedings of the 39th ACM SIGPLAN-SIGACT Symposium
onPrinciplesofProgrammingLanguages,POPL2012,Philadelphia,Pennsylvania,
USA, January 22-28, 2012 , John Field and Michael Hicks (Eds.). ACM, 245≈õ258.
https://doi.org/10.1145/2103656.2103687
[20]Cristina David, Daniel Kroening, and Matt Lewis. 2015. Unrestricted Termi-
nation and Non-termination Arguments for Bit-Vector Programs. In Program-
ming Languages and Systems - 24th European Symposium on Programming,
ESOP 2015, Held as Part of the European Joint Conferences on Theory and Prac-
tice of Software, ETAPS 2015, London, UK, April 11-18, 2015. Proceedings (Lec-
ture Notes in Computer Science, Vol. 9032) , Jan Vitek (Ed.). Springer, 183≈õ204.
https://doi.org/10.1007/978-3-662-46669-8_8
[21]Grigory Fedyukovich, Yueling Zhang, and Aarti Gupta. 2018. Syntax-Guided
TerminationAnalysis.In ComputerAidedVerification-30thInternationalCon-
ference,CAV2018,HeldasPartoftheFederatedLogicConference,FloC2018,Ox-
ford, UK, July 14-17, 2018, Proceedings, Part I (Lecture Notes in Computer Science,
Vol.10981) ,HanaChocklerandGeorgWeissenbacher(Eds.).Springer,124≈õ143.
https://doi.org/10.1007/978-3-319-96145-3_7
[22]Mirco Giacobbe, Daniel Kroening, and Julian Parsert. 2022. Neural Termination
Analysis. ACMJointEuropeanSoftwareEngineeringConferenceandSymposium
onthe Foundations of SoftwareEngineering (ESEC/FSE)2022 (to appear).
[23]Ashutosh Gupta, Thomas A. Henzinger, Rupak Majumdar, Andrey Rybalchenko,
andRu-Gang Xu. 2008. Provingnon-termination.In Proceedings ofthe 35th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL
2008,SanFrancisco,California,USA,January7-12,2008 ,GeorgeC.Neculaand
PhilipWadler(Eds.).ACM,147≈õ158. https://doi.org/10.1145/1328438.1328459
[24]Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto. 2017.
Knowledge transfer for out-of-knowledge-base entities: A graph neural network
approach. arXiv preprint arXiv:1706.05674 (2017).
[25]Matthias Heizmann, Jochen Hoenicke, and Andreas Podelski. 2014. Termination
Analysis by Learning Terminating Programs. In Computer Aided Verification -
26th International Conference, CAV 2014, Held as Part of the Vienna Summer of
Logic,VSL2014,Vienna,Austria,July18-22,2014.Proceedings (LectureNotesin
ComputerScience,Vol.8559) ,ArminBiereandRoderickBloem(Eds.).Springer,
797≈õ813. https://doi.org/10.1007/978-3-319-08867-9_53
[26]SeppHochreiterandJ√ºrgenSchmidhuber.1997.LongShort-termMemory. Neural
computation 9 (12 1997),1735≈õ80. https://doi.org/10.1162/neco.1997.9.8.1735
[27]MohammadHossinandSulaimanM.N.2015. AReviewonEvaluationMetricsfor
DataClassificationEvaluations. InternationalJournalofDataMining&Knowledge
ManagementProcess 5(032015), 01≈õ11. https://doi.org/10.5121/ijdkp.2015.5201
[28]ShrutiJadon.2020. Asurveyoflossfunctionsforsemanticsegmentation. 2020
IEEE Conference on Computational Intelligence in Bioinformatics and Compu-
tational Biology (CIBCB) (Oct 2020). https://doi.org/10.1109/cibcb48159.2020.
9277638
[29]Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with
GraphConvolutionalNetworks. CoRRabs/1609.02907(2016). arXiv: 1609.02907
http://arxiv.org/abs/1609.02907
[30]Daniel Kroening, Natasha Sharygina, Aliaksei Tsitovich, and Christoph M. Win-
tersteiger. 2010. Termination Analysis with Compositional Transition Invariants.
InComputer Aided Verification, 22nd International Conference, CAV 2010, Ed-
inburgh, UK, July 15-19, 2010. Proceedings (Lecture Notes in Computer Science,
Vol.6174),TayssirTouili,ByronCook,andPaulB.Jackson(Eds.).Springer,89≈õ103.
https://doi.org/10.1007/978-3-642-14295-6_9
[31]Tien-Duy B. Le, David Lo, Claire Le Goues, and Lars Grunske. 2016. A learning-
to-rankbased faultlocalization approach using likely invariants. In Proceedings
of the 25th International Symposium on Software Testing and Analysis, ISSTA 2016,
Saarbr√ºcken,Germany,July18-20,2016 ,AndreasZellerandAbhikRoychoudhury
(Eds.).ACM,177≈õ188. https://doi.org/10.1145/2931037.2931049
[32]TonChanhLe,LeiXu,LinChen,andWeidongShi.2018. ProvingConditional
Termination for Smart Contracts. In Proceedings of the 2nd ACM Workshop on
Blockchains, Cryptocurrencies, and Contracts, BCC@AsiaCCS 2018, Incheon, Re-
public of Korea, June 4, 2018 , Satya V. Lokam, Sushmita Ruj, and Kouichi Sakurai
(Eds.).ACM,57≈õ59. https://doi.org/10.1145/3205230.3205239
[33]Wonchan Lee, Bow-Yaw Wang, and Kwangkeun Yi. 2012. Termination Analysis
withAlgorithmicLearning.In ComputerAidedVerification-24thInternationalConference, CAV 2012, Berkeley, CA, USA, July 7-13, 2012 Proceedings (Lecture
Notes in Computer Science, Vol. 7358) , P. Madhusudan and Sanjit A. Seshia (Eds.).
Springer, 88≈õ104. https://doi.org/10.1007/978-3-642-31424-7_12
[34]Guohao Li, Matthias M√ºller, Guocheng Qian, Itzel C. Delgadillo, Abdulellah
Abualshour, Ali K. Thabet, and Bernard Ghanem. 2019. DeepGCNs: Making
GCNs Go as Deep as CNNs. CoRRabs/1910.06849 (2019). arXiv: 1910.06849
http://arxiv.org/abs/1910.06849
[35]Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE international
conference oncomputer vision . 2980≈õ2988.
[36]Ulrich Neumerkel and Fr√©d√©ric Mesnard. 1999. Localizing and Explaining
Reasons for Non-terminating Logic Programs with Failure-Slices. In Princi-
plesandPracticeofDeclarativeProgramming,InternationalConferencePPDP‚Äô99,
Paris,France,September29-October1,1999,Proceedings (LectureNotesinCom-
puter Science, Vol. 1702) , Gopalan Nadathur (Ed.). Springer, 328≈õ342. https:
//doi.org/10.1007/10704567_20
[37]Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
ZacharyDeVito,ZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer.
2017. AutomaticDifferentiation inPyTorch.In NIPS 2017Workshopon Autodiff
(LongBeach,California, USA). https://openreview.net/forum?id=BJJsrmfCZ
[38]AlvaroSanchez-Gonzalez, NicolasHeess,Jost TobiasSpringenberg, JoshMerel,
MartinA.Riedmiller,RaiaHadsell,andPeterW.Battaglia.2018. GraphNetworks
as Learnable Physics Engines for Inference and Control. In Proceedings of the
35thInternationalConferenceonMachineLearning,ICML2018,Stockholmsm√§ssan,
Stockholm,Sweden,July10-15,2018 (ProceedingsofMachineLearningResearch,
Vol. 80), Jennifer G. Dy and Andreas Krause (Eds.). PMLR, 4467≈õ4476. http:
//proceedings.mlr.press/v80/sanchez-gonzalez18a.html
[39]MichaelSejrSchlichtkrull,ThomasN.Kipf,PeterBloem,RiannevandenBerg,
IvanTitov,andMaxWelling.2018. ModelingRelationalDatawithGraphConvo-
lutionalNetworks.In TheSemanticWeb-15thInternationalConference,ESWC
2018, Heraklion,Crete, Greece, June 3-7, 2018, Proceedings (Lecture Notesin Com-
puter Science, Vol. 10843) , Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal,
Pascal Hitzler, Rapha√´l Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam
(Eds.).Springer, 593≈õ607. https://doi.org/10.1007/978-3-319-93417-4_38
[40]Alex Sherstinsky. 2018. Fundamentals of Recurrent Neural Network (RNN)
and Long Short-Term Memory (LSTM) Network. CoRRabs/1808.03314 (2018).
arXiv:1808.03314 http://arxiv.org/abs/1808.03314
[41]Alan Turing. 1936. On Computable Numbers, with an Application to the N
Tscheidungsproblem. ProceedingsoftheLondonMathematicalSociety 42,1(1936),
230≈õ265. https://doi.org/10.2307/2268810
[42]Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Li√≤, and Yoshua Bengio. 2018. Graph Attention Networks.
arXiv:1710.10903 [stat.ML]
[43]MarkWeiser.1982. ProgrammersUseSlicesWhenDebugging. Commun.ACM
25,7 (jul1982),446≈õ452. https://doi.org/10.1145/358557.358577
[44]Yongji Wu, Defu Lian,YihengXu,Le Wu, and Enhong Chen. 2020. Graph Con-
volutional Networks with Markov Random Field Reasoning for Social Spammer
Detection. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI
2020,TheThirty-SecondInnovativeApplicationsofArtificialIntelligenceConfer-
ence,IAAI2020,TheTenthAAAISymposiumonEducationalAdvancesinArtificial
Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 . AAAI Press,
1054≈õ1061. https://aaai.org/ojs/index.php/AAAI/article/view/5455
[45]ZonghanWu, Shirui Pan, FengwenChen,Guodong Long, ChengqiZhang,and
PhilipS.Yu.2019. AComprehensiveSurveyonGraphNeuralNetworks. CoRR
abs/1901.00596 (2019). arXiv: 1901.00596 http://arxiv.org/abs/1901.00596
[46]BingXu,NaiyanWang,TianqiChen,andMuLi.2015. EmpiricalEvaluationof
Rectified Activations in Convolutional Network. CoRRabs/1505.00853 (2015).
arXiv:1505.00853 http://arxiv.org/abs/1505.00853
[47]ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,Barnab√°sP√≥czos,Ruslan
Salakhutdinov,andAlexanderJ.Smola.2017. DeepSets. CoRRabs/1703.06114
(2017). arXiv: 1703.06114 http://arxiv.org/abs/1703.06114
[48]Yingxue Zhang and Michael G. Rabbat. 2018. A Graph-CNN for 3D Point Cloud
Classification. In 2018 IEEE International Conference on Acoustics, Speech and
Signal Processing, ICASSP 2018, Calgary, AB, Canada, April 15-20, 2018 . IEEE,
6279≈õ6283. https://doi.org/10.1109/ICASSP.2018.8462291
[49]Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2022. Deep Learning on Graphs: A
Survey.IEEETrans.Knowl.DataEng. 34,1(2022),249≈õ270. https://doi.org/10.
1109/TKDE.2020.2981333
[50]JieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,
Lifeng Wang, Changcheng Li, andMaosong Sun. 2020. Graph neuralnetworks:
A review of methods and applications. AI Open1 (2020), 57≈õ81. https://doi.org/
10.1016/j.aiopen.2021.01.001
[51]Shaowei Zhu and Zachary Kincaid. 2021. Termination analysis without the
tears. InPLDI ‚Äô21: 42nd ACM SIGPLAN International Conference on Programming
Language Design and Implementation, Virtual Event, Canada, June 20-25, 2021 ,
StephenN.FreundandEranYahav(Eds.).ACM,1296≈õ1311. https://doi.org/10.
1145/3453483.3454110
921