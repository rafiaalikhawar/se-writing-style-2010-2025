EvaCRC:Evaluating CodeReview Comments
LanxinYang
StateKey Laboratory of Novel
SoftwareTechnology, Software
Institute, Nanjing University
Nanjing,China
lanxin.yang@smail.nju.edu.cnJinwei Xu
StateKey Laboratory of Novel
SoftwareTechnology, Software
Institute, Nanjing University
Nanjing,China
jinwei_xu@smail.nju.edu.cnYifan Zhang
StateKey Laboratory of Novel
SoftwareTechnology, Software
Institute, Nanjing University
Nanjing,China
yifan.zhang@smail.nju.edu.cn
He Zhang
StateKey Laboratory of Novel
SoftwareTechnology, Software
Institute, Nanjing University
Nanjing,China
hezhang@nju.edu.cnAlbertoBacchelli
Departmentof Informatics
Universityof Zurich
Zurich, Switzerland
bacchelli@i/f_i.uzh.ch
ABSTRACT
In code reviews, developers examine code changes authored by
peers and provide feedback through comments. Despite the impor-
tance of these comments, no accepted approach currently exists
forassessingtheirquality.Therefore,thisstudyhastwomainob-
jectives:(1)todeviseaconceptualmodelforanexplainableeval-
uation of review comment quality, and (2) to develop models for
theautomatedevaluationofcommentsaccordingtotheconceptual
model. To do so, we conduct mixed-method studies and propose a
newapproach: EvaCRC (EvaluatingCodeReviewComments).To
achievethe/f_irstgoal,wecollectandsynthesizequalityattributes
of review comments, by triangulating data from both authoritative
documentationoncodereview standardsandacademicliterature.
We then validate these attributes using real-world instances. Fi-
nally, we establish mappings between quality attributes and grades
by inquiring domain experts, thus de/f_ining our /f_inal explainable
conceptualmodel.Toachievethesecondgoal,EvaCRCleverages
multi-labellearning.Toevaluateandre/f_ineEvaCRC,weconduct
anindustrialcasestudywithaglobalICTenterprise.Theresults
indicate that EvaCRC can eﬀectively evaluate review comments
whileoﬀeringreasons for the grades.
Data andmaterials: https://doi.org/10.5281/zenodo.8297481
CCS CONCEPTS
•Software and its engineering →Software development pro-
cess management ;Empirical software validation .
KEYWORDS
Code review,reviewcomments, qualityevaluation
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci/f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12...$15.00
https://doi.org/10.1145/3611643.3616245ACMReference Format:
Lanxin Yang, Jinwei Xu, Yifan Zhang, He Zhang, and Alberto Bacchelli.
2023. EvaCRC: Evaluating Code Review Comments. In Proceedings of the
31stACMJointEuropeanSoftwareEngineeringConferenceand Symposium
on the Foundations of Software Engineering (ESEC/FSE ’23), December 3–9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3611643.3616245
1 INTRODUCTION
Codereviewwasdesignedprimarilytodetectqualityissuesinsoft-
waredevelopment[ 26],thenwasfoundusefulinidentifyingcode
weaknesses and seeking opportunities for improvement in design,
testing, and security ( e.g., [7,51,64,81]). Apart from supporting
quality assurance, code review bene/f_its other aspects such as team
awareness[ 2],knowledgetransfer[ 10],andtoolimprovement[ 80].
Compared with other quality assurance practices ( e.g., testing),
codereviewmorelargelyreliesonorganizationalcultureandde-
velopers’expertise,experience,participation, and engagement [ 3–
5,41,72]. Moreover, code review sometimes may become wasteful
or even harmful for projects and teams due to its confusion [ 22],
time- and eﬀort-consuming [ 36,59], low eﬃciency and eﬀective-
ness[15,33,35,42], andothernegative side-eﬀects[ 11,21,75].
Overthepastfewyears,standards,guidelines,andexperience
havebeensharedtoimprovecodereview( e.g.,IEEEComputerSoci-
ety [62], Google [ 30,60], and GitLab [ 29]). Most of these resources
underlinethekeyroleofreviewfeedback,assharedthroughreview
comments. Review comments serve as a communication tool for
reviewerstosharetheirassessmentofcodechanges.Thus,these
comments are the key guide for corrections andimprovements.
Despite the importance of review comments, however, only few
studiesfocusontheirevaluation.Twostudies[ 6,34]haveinvesti-
gateddevelopers’perceptionoftheusefulnessofreviewcomments
anddevelopedmodelsforautomatingtheevaluationprocess.How-
ever, these approaches output a binary label ( i.e., useful or not),
withoutprovidinganyadditionalinformationtojustifyitorsug-
gest improvements. This binary assessment could inadvertently
overlook potentially ‘useful’ review comments, potentially leading
todeveloperscontestingtheresultsorevenresistingtheevaluation.
Existingpredictivemodelspredominantlydependon reviewcontext
features(e.g.,change_trigger [6]: determining if any changes occur
275ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LanxinYang,JinweiXu,YifanZhang,He Zhang,andAlberto Bacchelli
within a line of the highlighted comment in subsequent iterations,
andgratitude [34]:assessingifthecodeauthorrespondswithex-
pressions like ‘Thank you’). They also utilize experience features
(e.g.,code_authorship [57]:countingthecommitsadeveloperhas
madeona/f_ileand external_library_experience [57]:gaugingade-
veloper’sfamiliaritywithexternallibrariesfromareviewed/f_ile).
Asigni/f_icantlimitationofthesefeaturesistheiravailabilityonly
afterthe completion of the review, thus falling short in oﬀering
instantaneous feedbackto the authorofthe comments.
Wecontendthattheevaluationofreviewcommentsshouldgo
beyond assigning a grade. Instead, the evaluation should guide
corrections or enhancements. Herein, we propose an approach,
EvaCRC (EvaluatingCodeReviewComments),tosupporteﬀec-
tiveevaluationofreviewcomments.EvaCRCcombinesaconcep-
tualmodelandanautomatedmodel.Theformeraimstooﬀeran
explainable conceptualmodel forreview comments,while thelat-
ter aims to automate their evaluation. Following the guidelines
formixed-methodresearchdesign[ 13],wedevisetwostudiesfor
implementingandevaluatingEvaCRC,respectively.
Inthe/f_irststudy,toimplementtheconceptualmodelofEvaCRC,
we adopt data triangulation (including authoritative documents,
academic literature, and real-world examples) to collect, synthe-
size,andvalidatequalityattributes.Ourresultingconceptualmodel
consists of four review context-independent quality attributes that
characterizereviewcomments( i.e.,‘emotion’,‘question’,‘evaluation ’,
and‘suggestion ’)andtheirmappingstooneoffourqualitygrades
(i.e., ‘excellent’, ‘good’, ‘acceptable ’, and ‘poor’). As opposed to prior
approaches ( e.g., [34,57]), which output only a binary grade ( i.e.,
useful or not), EvaCRC is designed to output one overall quality
grade together with four speci/f_ic quality attributes that explain
it.Forinstance,giventhereviewcomment–“ getAbsolutePath()is
not allowed in security scanning, use getCanonicalPath() instead. ”
EvaCRC evaluates it as ‘acceptable’, with the quality attributes val-
uesbeing‘emotion’–Positive(1),‘question’–No(0),‘evaluation’–Yes
(1), and ‘suggestion’–Yes (1), respectively. According to the concep-
tual model, EvaCRC generates the overall quality grade for each
reviewcommentbyfollowingtheprede/f_inedmappings(cf.Table 3)
asevaluatedandapprovedbydevelopersatalargeInformationand
CommunicationTechnology(ICT)enterprise,and,thefourspeci/f_ic
qualityattributesarelearned fromgenuinereviewcomments.We
formulate the learning task as multi-label text classi/f_ication. Since
manual classi/f_ication is time-consuming and bias-prone, we use
natural languageprocessing and machine learning to develop text
classi/f_iers to form an automatedmodelofEvaCRC.
In the second study, to evaluate and improve EvaCRC, we carry
out an industrial case study at a large ICT enterprise. We /f_irst
continue to implement the conceptual model by establishing the
enterprise-speci/f_icmappingsbetweenqualityattributesandgrades.
We then collect and annotate 17,000 review comments and build
sixmulti-labeltextclassi/f_ierstoseekthemosteﬀectiveforforming
the automated model of EvaCRC. Finally, we interview software
practitionersatthisICTenterprisetounderstandtheirperceptionof
EvaCRC. The results from the case study indicate that EvaCRC can
eﬀectivelyevaluatereviewcomments,providingreviewersnotonly
evaluation results (quality grades) that are more acceptable than a
binary evaluation but detailed explanations (quality attributes) for
timely correction andimprovement.2 RELATED WORK
This section reviewsthe literature that isrelevantto our work.
UsefulnessofReviewComments. Theonlyaspectconcerning
the qualityofreviewcomments investigatedsofaris usefulness .
Bosu et al . [6]interviewed seven developers at Microsoft, dis-
cussing with them a total of almost 150 comments in terms of
usefulness. Developers de/f_ined as ‘ useful’ review comments that
(1) indicate functional issues; (2) identify validation issues, alter-
nate scenarios; and (3) transfer knowledge. Whereas, developers
ratedas‘somewhatuseful ’reviewcommentsthatindicate:(1)nit-
pickingissues; (2)indentation,comments, style, identi/f_ier naming
issues;and(3)alternatives. Finally,developersratedas‘ notuseful ’
commentsthatmerelypresent(1)questions,(2)appreciation,and
(3) suggestionsfor future improvement.
Hasanetal .[34]interviewedSamsung’sdevelopersandfound
thattheyde/f_ineas‘ useful’reviewcommentsthatidentifyatleast
oneofthefollowing:(1)defects,(2)missinginputvalidations,(3)op-
portunities for making code eﬃcient and optimized, (4) readability
weakness, (5) logical mistakes, (6) redundant code, (7) corner cases,
(8) opportunities for integrating code, (9) deprecated functions,
(10)designweakness,and(11)codingstandardviolations.Areview
commentis‘ notuseful ’whenitregards:(1)visualrepresentation
issues that can be also identi/f_ied by static analysis tool, (2) false
positive,(3)misinterpretation,(4)discussiononanalreadyresolved
issue,or(5)solutionapproachwithwhichtheauthordisagrees.On
some types of comments there was no consensus:praise, clari/f_ica-
tionquestions,andsuggestionsonimprovingdocumentation.
Automated Evaluation of Review Comments. Bosu et al . [6]
created an automated approach to classify review comments as
either‘useful’(alsoincludingtheir‘ somewhatuseful ’category)or
‘not useful ’.Their approachwas developedby annotating989ex-
amples and devising a decision tree with hand-crafted rules. Their
approach heavily relies on the impact of comments, particularly
whether it triggered a subsequent change or whether it was la-
beled as ‘ wont/f_ix’. Their model reached mean precision, recall, and
accuracyof89%,85%,and83%,respectively.
UnlikeBosuetal .[6],Rahmanetal .[57]onlyleveraged“change
triggering” to label a dataset of the usefulness of review comments.
Rahmanetal.computedfeaturesconcerningboththe‘textualre-
view comments’ ( e.g.,readingease andcodeelement ratio ) and the
‘reviewerexperience’( e.g.,codeauthorship andexternal libraryex-
perience)fordevelopingpredictionmodels.Theauthorscollected
1,482 annotated examples and developed three learning-based clas-
si/f_iers:NaïveBayes,LogisticRegression,andRandomForestsfor
binary evaluation.RandomForests achievedthe bestperformance
withameanprecisionof65.76%,meanrecallof65.89%,meanF1-
score of65.82%,andmean accuracyof 66.06%.
In additionto adopting the features presented in Bosu et al . [6]
andRahmanetal .[57],Hasanetal .[34]devisedmorefeaturesfrom
the perspectives of ‘textual review comments’ ( e.g.,word count ),
‘reviewcontexts’( e.g.,authorresponded andreplysentiment ),and
‘reviewer experience’ ( e.g.,reviewing_experience ). They collected
2,004 annotated examples and trained six classi/f_iers: Decision Tree,
Random Forests, Support Vector Machine, Multi-Layer Perceptron,
XGBoost,andLogisticRegression.Thebest-performingclassi/f_ieris
Random Forestswithamean accuracyof 87.32%.
276EvaCRC: Evaluating Code ReviewComments ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Overall,thestudieshavecreatedmodelstoautomaticallypredict
theusefulnessofreviewcommentswithoutprovidinganyexpla-
nation.However,theunexplainedresultsmayleadtodevelopers’
disagreements with the results or even resistance to the evaluation
itself. In addition, the prediction models rely on a large number of
complex hand-crafted features some of which can only be assessed
after the comments have been read and acted upon by other devel-
opers (e.g., whether the comment triggered a change or whether it
has been labeled as ‘ wont/f_ix’). Therefore, the proposed approaches
havelimitedtonouseinpractice,forexample,togiveimmediate
feedback to the author of a review comment about what they have
written. Furthermore, the heavy feature engineering work ( e.g.,
collecting and calculating complex features from multi-source het-
erogeneousdata,selectingsuitablefeatures,andscalingfeatures)
asks for agreatamount oftime andeﬀort.
3 STUDYOVERVIEW
Westartbypresentingreal-worldexamplesofreviewcomments(cf.
Table1),whichweusetomotivatewhyitisimportantandchalleng-
ing to evaluate review comments. Then we present an overview of
our proposedapproach, followedbyour researchquestions.
Table 1:Real-world examples ofcodereview comments
idComment
C1“delete”
C2“magicnumber”
C3“Whydo you want this?”
C4“whyagain hardcoding???”
C5“Ithink it shouldbe“ Logger.info”
C6“shouldit be public or protected?”
C7“change the logginglevel to debug.”
C8“downloadTips →mDownloadTips”
C9“getAbsolutePath() is not allowed in security scanning, use
getCanonicalPath() instead.”
C10“Pleasediscussboxnamewith@xYZ1111111,wehaveaboxnamingrule
and useit in tracing data.”
C11“Check whether these scripts already executed or need to execute in
3.17.4.103and placethe scriptsaccordingly.”
C12“Do these values refer to speci/f_ic values? If you don’t need to use minus-
separated statements like ‘FULL-TIME’, remove the minuses and quotes.
Otherwise, keep just the minus-separated statements and remove quotes
from the others.”
3.1 EvaluatingReviewComments: Challenges
Several challenges arisewhen evaluatingreviewcomments.
Challenge1:DisagreementsbyReviewers. Pastresearch( e.g.,[2,
47]) has recognized that there isa signi/f_icant variance inthe qual-
itymaturity amongreviewcomments.Howcanthese diﬀerences
be characterized and graded? There are no standard criteria and
approachesfor evaluatingreviewcomments.
In our industrial case study (cf. Section 5), one manager we
interviewedexplainedthatitischallengingtogenerallyevaluate
theusefulness ofacomment,especiallyonabinaryscale;assheput
it: “It is hard to say that a review comment is absolutely not useful
in practice ...even one interrogation mark can serve as a reminder
to developers. However, we are seeking to develop guidelines and
standardsforperformingcodereviewsaswellasevaluationcriteria.
Otherwise, code review becomes wasteful. ” Other developers argued
that using only a usefulness criterion is more likely to result in
biases and carelessness. To de/f_ine the quality of a review comment,
some developers suggested taking into account the type of defectsit detects ( e.g., comments detecting security or critical issues are to
be consideredhigh-quality).
Finally, providing negative feedback to reviewers about their
commentsmayresultindisagreementsandnegativefeelings,es-
pecially if no explanation is provided. Negative evaluations may
impact reviewers’ productivity in either code reviews or/and other
software activities, possibly leading them to leave project teams in
extreme cases.
Challenge2:ModelingReviewComments. Reviewcomments
often include jargons, abbreviations, and misspellings. Table 1
presentsreal-worldexamplestoillustratethelinguistic diversityand
complexity ofreviewcomments:(1) lexicon:naturallanguage,pro-
gramminglanguage,andspecialsymbolsaremixed(C5,C6,C8,C9);
(2)syntax:declarative (C5),imperative (C7),generalinterrogative
(C3),andselectiveinterrogative(C6);and(3) pragmatics: indication
(C2),question(C4,C11),suggestion(C1,C12),andassignment(C10).
In addition, review comments transfer reviewers’ emotions ( e.g.,
C4),whichhaveimpactsondevelopers’feelingsaswellasonthe
qualityofthe projectthey are workingon[ 24,25,48,49].
Moreover, review comments transfer complex functional seman-
ticsofthelanguage( e.g.,causality,contrast,exempli/f_ication,clari/f_i-
cation, similarity, and hypothesis [ 23]), and oﬀer multiple practical
functions ( e.g., correctness, decision, management, and interac-
tion[45]).Whenitcomestotheprimarygoalofcodereview( i.e.,
/f_inding defects1[2,59,60]), review comments indicate multiple
types,suchasevolvabilityorfunctionaldefects,andcompilerer-
rors [56]. Gunawardena et al . [31]identi/f_ied 116 types of defects
thatfellinto15groupsincodereview.Thediversityandcomplexity
ofreviewcomments make themhardto comprehendandmodel.
3.2 EvaluatingReviewComments: Goals
We strive to achieve the following goalsinour studies.
Goal 1: Explainable Evaluation. We aim to establish an explain-
able conceptual model. The term ‘explainable’ indicates that the
conceptualmodelshouldoutputnotonlythe/f_inalqualitygrade(as
previousstudiesdid),butalsothecausesforthatgrade.Weplace
emphasis on letting developers know the causes behind evalua-
tions to help them learn and improve. We also strive to ensure that
the conceptual model is independent of the review context ( e.g.,
author/reviewership, review process&outcome). Let us consider
areviewcommentthatismerelyaquestionmark.Inonereview,
it may be evaluated as ‘ useful’ because it triggers code changes;
whereas,inanotherreview,itisevaluatedas‘ notuseful ’because
it triggers no code changes. Such an evaluation, only based on the
eﬀect of the comment, oﬀers little help to improve. Instead, the
evaluationshouldoﬀerexplanationsthusleadingtoimprovements.
Goal 2: Automated Evaluation. Automating the process of eval-
uation is essential due to the time- and eﬀort-consuming nature of
the manual evaluation of comments. We aim to develop a model
thatautomatically learnsthe high-levelrepresentationofreview
comments to tackle their linguistic complexity. We aim to avoid
heavyhand-craftedfeatureengineering( e.g.,featureextractionand
1In this paper, the term ‘defect’ is used as a general concept that depicts any condi-
tiondeviatingfromexpectationsbasedonprede/f_inedstandards,orfromdevelopers’
experiences.The‘defect’couldbe‘bug’,‘vulnerability’,‘error’,etc.,inaspeci/f_iccontext.
277ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LanxinYang,JinweiXu,YifanZhang,He Zhang,andAlberto Bacchelli
selection)fortrainingautomatedmodels.Therefore,weinvestigate
naturallanguageprocessing anddeeplearningfor automation.
We expect the conceptual model (quality attributes and map-
pings)ratherthantheautomatedmodelstoprovideanexplanation
oftheevaluation results, as researching thelatter( e.g.,explaining
how the neural network-based models understand semantics from
genuine reviewcomments)isbeyondthe scope ofthis study.
3.3 EvaluatingReviewComments: Approach
Ourapproach( EvaCRC)isasynergyoftwosub-goals: explainable
evaluation andautomated evaluation . The ultimate goal G(to as-
sociatetwosub-goals)ofthisstudydependsonthefunction F(to
learnqualityattributes Qfromreviewcomments)andthemapping
Z(to determine quality grades with attributes). Once the attributes
aredeterminedandtheirmappingstogradesareestablished,the
remaining task is to seek appropriate models and strategies for
learning the objective function F. Our goalcan be formulated asa
supervisedmulti-label textclassi/f_ication problem.
Figure1showsanoverviewofresearchdesignsofthismixed-
methodresearch.Inthe/f_irststudy,weimplementthedesigns(as
presented above) of EvaCRC; in the second study, we evaluate
EvaCRC as well as seekimprovements.
3.4 Research Questions
Our study isstructuredaround three ResearchQuestions(RQs).
RQ1:Whatqualityattributesareappropriatefortheconcep-
tualmodel?
Motivation: We propose RQ1 to guide the collection, synthesis, and
validation of quality attributes to form the core components of the
conceptualmodel.
Methods: We address RQ1 by using data triangulation [18] and
thematic synthesis [14]. As shown in Figure 1(the top of the Imple-
mentation part),ourtriangulateddatasourcesincludeauthoritative
documents, academic literature, and real-world examples. Triangu-
lationisthehigh-levelmethodweused,whilethematicsynthesis
isusedto synthesize qualityattributes,speci/f_ically.
RQ2:How eﬀective are text classi/f_iers in predicting (RQ2.1)
thequalityattributes and(RQ2.2)thequalitygrades?
Motivation: Aslittleisknownabouthowwelltextclassi/f_iersper-
form,we propose RQ2 to guide the comparisons.
Method:To address RQ2, we conduct experiments [73] (the bottom
of theImplementation part in Figure 1) in which we use the review
commentscollectedfromanICTenterpriseasthedataset,weset
sixmulti-labeltextclassi/f_iersand evaluatethemusingfourmetrics
for answeringRQ2.1,andthree metrics for answeringRQ2.2.
RQ3: How dopractitioners perceive EvaCRC?
Motivation: We investigate how practitioners perceive the eﬀec-
tiveness of EvaCRC as a further evaluation and to guide future
improvements.
Method:To address RQ3, we conduct focus group interviews [55]
(theEvaluation part in Figure 1) with practitioners from the ICT
enterprisethatprovidesuswiththeexperimentaldataset.Sothat
we could receive insightful comments andrecommendations.4 STUDYI: IMPLEMENTING EVACRC
We describe our mixed-methodstudy for implementingEvaCRC.
4.1 Conceptual Model
4.1.1 /Q_ualityA/t_tributes. Ouroriginalintentionwastocollectqual-
ityattributesfromFagan’sarticle[ 26]andIEEEComputerSociety’s
standards [ 62], which are the main authoritative documents con-
cerning code review. Unfortunately, these did not directly meet
our goal as expected. Therefore, we followed the guidelines [ 61]
toadoptdatatriangulation( i.e.,a“combinationofmethodologies
in the study of the same phenomenon” [ 19]) to gather diﬀerent
types of evidence to collect quality attributes and cross-validate
them. Data triangulation increases the con/f_idence of research data,
creates innovative and multi-perspective ways of understanding a
problem,andreveals unique /f_indings[ 37].
With data triangulation [ 66], we added data sources ( i.e.,aca-
demic literature andreal-worldexamples ) andtailored our data col-
lectionmethodsbasedonthebest/f_itwiththegoal[ 9].Theresearch
methodper data sourceiselaboratedas follows.
Source 1: Authoritative Documents. We investigated (1) “ De-
signandcodeinspectionstoreduceerrorsinprogramdevelopment ”
(MichaelFagan,1976)[ 26],and(2)“ IEEEstandardforsoftwarere-
views and audits ”(IEEEComputer Society,2008)[ 62].
The formeris theseminalstudythat formalizes codeinspection
(a.k.a. classical code review), while the latter presents standards
forsoftwarereviewsandaudits.Eventhoughseveraltypesofre-
viewsexist( e.g.,managementreviews,technicalreviews,andwalk-
throughs), they are all required to review source code, i.e., code
review is fundamental [ 77]. From these documents, we found that
the core objective of code review is reducing defects—this is the
primary qualityattribute (‘reducing defects’).
Source2:AcademicLiterature. The/f_irstadditionaldatasource
is academic literature. Since academic literature has been peer-
reviewed,therigorandcredibilityofitscontentscanbeguaranteed
tosomeextent.Although‘codereviewcomments’arethesubject
of our research, our initial searches with Google Scholar, IEEE
Xplore Digital Library, and ACM Digital Library indicated that
limitedliteraturerefersspeci/f_icallytoreviewcomments.Therefore,
togatherourqualityattributes,webroadenedoursearchscopeand
analyzedliterature whose subjectis“code review”.
We employed rapid literature review [67]—a form of evidence
collectionandsynthesisthatsimpli/f_iescomponentsofasystematic
(literature)reviewtoquicklyacquireknowledge[ 67].Toretrieve
the primary studies to analyze, we gathered the papers from the
twomostup-to-date,codereview-relatedsystematicreviews( i.e.,
[16,71]). Both reviews were published in 2021 in the Journal of
Systems and Software . Davila and Nunes [16]collected 139 primary
studiesfrom2005to2019.Wangetal .[71]collected112primary
studies published between 2011 and 2019. By combiningthe two
sets,we were left with182primary studiesto analyze.
Sinceonlyafewstudiesdirectlypresentqualityattributes,we
extractedtwoitems( Bene/f_its andChallenges )fromeachprimary
study (if they exist) to synthesize quality attributes. Both items are
associated with code review, working for investigating (1) what
bene/f_its can be fully achieved and (2)what challenges can be to some
extentmitigatedbyevaluatingandimprovingthequalityofreview
278EvaCRC: Evaluating Code ReviewComments ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Evaluating Code 
Review Comments2 Authoritative Documents   2 Systematic Reviews  182 Primary Studies 22 Benefits        35 Challenges
Quality Attributes
& Grades
Automation                           Natura Language Processing  Text Classifier Automated ModelData Triangulation & Thematic Synthesis
Task                            Goals                                                                                 Impleme ntation                                                              The Proposed Approach: EvaCRC     Outputs Evaluation Multi -label 
Learning
BERTQ
20K+ Real -world Review Comments            4 Quality Attributes           16Mapping RulesExplainability Conceptual Model
ExperimentExperiment
Study1 Study2Focus Group Interview
Reports
Feedback
Figure 1:Anoverview ofmixed-method researchdesigns (with severalresults)
comments . At the same time, with reference to the authoritative
documents , the core objective of code review is extracted to fur-
ther guide the attribute synthesis. Moreover, we used another new
datasource( i.e.,real-worldexamples )tovalidateandimprovethe
qualityattributesoutputinthisstudy.Theresearchmethodsand
procedures are as follows.
We synthesizedquality attributesfrom the two setsof items by
usingthematicsynthesis [14].AccordingtoCruzesandDybå [14],
a general thematic synthesis requires steps to: (1) extract data ( /u1D4461),
(2) code data ( /u1D4462), (3) translate codes into themes ( /u1D4463), (4) create
amodelofhigher-orderthemes( /u1D4464),and(5)assessthesynthesis’
trustworthiness( /u1D4465).
In/u1D4461, weassigned the sampleset to two researcherswith prior
experience with analyzing qualitative data. Their goal was to ex-
tractBene/f_its and Challenges by reading each primary study’s ab-
stract,introduction,andanyothersections if needed. Becausethe
abstract and introduction may outline the code review-related ben-
e/f_its and challenges, while the others focus on the speci/f_ic subject
andtechnicalissues( e.g.,improvingtheaccuracyofrecommending
reviewers [ 50]). Bothresearchers startedby analyzing 20 studies
together to coordinate on the task, then independently analyzed
the remaining studies.
In/u1D4462,thosetworesearchersperformed opencoding withexisting
keywords in the text. For instance, given the primary study [ 53]
stating: “Contemporary code review is a widespread practice used
bysoftware engineerstomaintainhighsoftwarequalityand share
project knowledge ” the researchers extracted ‘ software quality as-
surance’and‘knowledgeexchange ’asBene/f_its,andforthesentence:
“However, conducting proper code review takes time and develop-
ers often have limited time for review ” the researchers extracted
‘time-consuming ’ asChallenge. We did not measure the inter-coder
reliabilityatthisstageduetothelackofprede/f_ined,completeitems
beforeperformingcoding.Instead,theresearchersveri/f_iedtogether
theextractionresultsandmadeanycorrectionsafterreachingan
agreement. The extraction results were also randomly checked
by another researcher to further mitigate biases. Through this
step,wesummarized22bene/f_itsand35challenges.Themostfre-
quentlymentioned bene/f_itsare:(1)defectdetection&/f_ix( /u1D4351),(2)soft-
ware(code)qualityassurance&improvement( /u1D4352),(3)knowledge
transfer ( /u1D4353), (4) increasing team awareness ( /u1D4354), and (5) explor-
ingalternative solutions ( /u1D4355),etc.The most frequentlymentioned
challenges include: (1) /f_inding suitable reviewers ( /u1D4361), (2) time-
&cost-&eﬀort-consuming( /u1D4362),(3)understandingcodechanges( /u1D4363),(4)fairness/bias/con/f_lict( /u1D4364),(5)defectescaping( /u1D4365),(6)increas-
ingworkload( /u1D4366),(7)lackofsupporttools( /u1D4367),and(8)expertise
needed(/u1D4368),etc.(thecompletedataitemsareavailableintheonline
replication package [ 76].)
In/u1D4463, we excluded the bene/f_itsandchallenges that cannot be
analyzed from the review comments ( e.g.,/u1D4361,/u1D4362). For instance,
weremoved /u1D4362becauseEvaCRCisexpected tobereviewcontext-
independent ( i.e., the only data source/input is textual review com-
ments). Although the time, eﬀort, and cost may impact the quality
ofreviewcomments,theywouldalso changewiththeknowledge
andexperienceofreviewersandthecomplexityofcodechanges
reviewed (which are diﬃcult to measure and can be aﬀected by
biases). Then, we performed axial coding to group items with rela-
tionships,suchasopposition( e.g.,defect/f_inding anddefectescaping
->defect/f_inding ),subordination( e.g.,defect/f_inding andcodequality
assurance ->codequalityassurance )andcausality( e.g.,bias,con/f_lict,
andteam awareness ->emotion). In this step, we brought all the
items (bene/f_itsandchallenges ) together.
In/u1D4464,weperformed selectivecoding tooutputcorethemes(qual-
ity attributes), i.e.,qualityassurance andemotion.
In/u1D4465,wevalidatedandimprovedthetwoqualityattributeswith
real-world reviewcomments. The procedures are as follows.
Source 3: Real-World Examples. We analyzed review comments
from (1) commercial projects from domestic and overseas teams
atoneglobalICTenterprise(20K+,theexperimentaldatasetwas
collectedfromthem),andfrom/f_iveITenterprises(1K+)and(2)OSS
projects:EclipseandQt(3K+),toincreasethevalidity,actionability,
and generalizability of the quality attributes. Having analyzed a
large number of real-world review comments, we found reviewers
frequentlyexpressconfusion[ 22](e.g.,raisingquestions)incom-
ments. In general, there are two major types of questions: (1) to
understand motivations and implementations of code, e.g., “why
printStackTrace is there? ”; and (2) to express uncertainty for de-
fects,e.g., “is it a mandatory parameter? ”. Although such review
comments may not trigger code changes immediately, they help to
(1)remindauthorsandotherreviewers(ifexisted)ofseekingfur-
therdefects[ 63];and(2)transferprojectcontext,codingknowledge
and experiencebetween authorsand reviewers [ 10]. Ina nutshell,
if there is any confusion in code review, it should be raised in time
to prevent possible risks by the misunderstanding. By raising ques-
tions,reviewerscanreducemisjudgmenttosomeextent,orthey
wouldresultinlow-qualitycodeaswellasauthors’negativefeel-
ings.Moreover,reviewersandauthorshelpeachotherbyasking
279ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LanxinYang,JinweiXu,YifanZhang,He Zhang,andAlberto Bacchelli
usefulquestions.Therefore,weuse‘Question’(Que)tocharacterize
such reviewcomments.
Moreover,wefoundthatreviewcommentsexpresstwostyles
when working for ‘quality assurance’: (1) point out defects [ 34],
e.g., “this /f_ield is not required ”; and (2) oﬀer suggestions [ 6],e.g.,
“delete this line.. this is for debugging ”. Therefore, we break ‘qual-
ity assurance’ (synthesized at the second stage) into ‘Evaluation’
(Eva)and‘Suggestion’(Sug),todescribethetwostylesofreview
comments, respectively. Review comments should be evaluated
byassociatingfourattributestogethertoavoidpotential bias.For
instance,ifadopting‘evaluation’only,somereviewersmayroughly
indicate “ incorrect here ”, but neither explain the reason behind, nor
providesuggestionsforcorrection.Inthiscase,reviewcomments
oﬀer very little help to code authors. Table 2shows the ultimate
qualityattributesinourconceptual model.Notethatthequalityat-
tributes (by answering descriptive questions) diﬀer in a dichotomy
(0–/u1D441/u1D45C, 1–/u1D44C/u1D452/u1D460) and, for ‘emotion’, “negative” is annotated with 0,
whilst“neutral”and“positive”are allannotatedwith1.
RQ1 In Retrospect: By employing data triangulation,
four quality attributes of review comments emerge: ‘emotion’,
‘question’, ‘evaluation’, and ‘suggestion’. The quality grade of
areviewcommentcanbegeneratedbyassociatingtheoutputs
ofits fourattributes (cf.Table 3).
4.1.2 A/t_tribute Mappings. Given that binary evaluations (e.g., use-
ful or not) often lead to disagreements, we establish four quality
tiersforreviewcommentsinourconceptualmodel:‘excellent’(IV),
‘good’(III),‘acceptable’(II),and‘poor’(I).Additionally,wemanu-
ally annotate quality attributes instead of comment grades to train
automated models, as it is easiertoreach agreement onattributes
than on grades, thus reducing inconsistency in annotation. Ac-
knowledging that there can be 16 possible pairings between the
four quality attributes and the four grades, and recognizing that
eachsoftwareorganizationmayhaveuniquecriteriaforevaluating
review comments, we do not enforce strict mapping rules in our
model. Rather we provide a sample mapping (see Table 3) from an
industrialcasestudy as areference.
4.2 AutomatedModel
4.2.1 TechnicalPreliminary. Asweareaimingtopredictfourat-
tributes (labels) from a single review comment, we can consider
this issue as a multi-label learning problem [ 82], also known as
multi-label classi/f_ication. Multi-label learning is a sub/f_ield of su-
pervised learning. Itdiﬀers from binaryclassi/f_ication, where each
example corresponds to one of two labels, and multi-class clas-
si/f_ication, where each example is associated with only one label
from multiple, but mutually exclusive, classes. Multi-label learning
allows an example to be linked with multiple labels. Each label
representsadiﬀerentclassi/f_icationtaskthatcan becorrelatedwith
others.Thisapproachoﬀersadvantagessuchasenhancingdataand
computationaleﬃciency andreducing the riskofover/f_itting [ 82].
4.2.2 BaselineClassifiers. Weevaluatedsixpopulartextclassi/f_iers
aspotentialcandidatesfortheautomatedmodelinourEvaCRCsys-
tem.Theseclassi/f_iersincludeRandomForests(RF)[ 8],TextCNN[ 40],
TextRCNN [ 43], DPCNN [ 38], Transformer [ 69], and BERT [ 20].Among these, RF is a traditional machine learning model based on
pre-designed features, while the rest are deep learning models that
simplifytheprocessbyavoidinglaboriousfeatureengineeringtasks,
suchasfeatureextractionandselection[ 17,44].RFwasincluded
becauseithasproveneﬀectiveinpriorresearch[ 34,57].Though
thesemodelswereprimarilydesignedforsingle-labeltextclassi-
/f_ication, we adjusted their architecture (speci/f_ically their output
unitsandlossfunctions)tosuitourmulti-labeltextclassi/f_ication
needs. For the RF model, we /f_irst used Word2Vec [ 28] to convert a
textual review comment into a vector, eﬀectively replacing feature
engineering, and then fed it into the RF model. During the training
phase for each classi/f_ier, we /f_inely tuned their hyperparameters,
such as inputlength,to adapt themto our speci/f_ic task.
4.2.3 Evaluation Metrics. In line with the EvaCRC system’s ap-
proachofde/f_iningthemaincomponentofreviewcommenteval-
uation as a multi-label text classi/f_ication task, we used standard
multi-label learning metrics [ 74] to assess classi/f_iers: Hamming
Loss(HL),Subset0/1Loss(01L),Macro-F1(mF),andMacro-AUC
(mA). Hamming Loss measures the proportion of labels that are
incorrectly predicted, while Subset 0/1 Loss is stricter, requiring all
labelsforanexampletobepredictedcorrectly.Macro-F1andMacro-
AUC calculate the F1-score and AUC-score independently for each
class, then average the results. This process ensures all classes are
treated equally and helps evaluate how well classi/f_iers handle class
imbalance problems. Furthermore,as predicting a single label can
be viewed as a binary classi/f_ication problem, we used Precision (P),
Recall(R),F1-score(F),andAUC(A)toevaluatetheclassi/f_ierson
this aspect. In terms of evaluation metrics, lower values of HL and
01Lsuggestbetterclassi/f_ierperformance,whilehighervaluesfor
other metrics indicate improved performance. To avoid bias, we
conducted5-fold cross-validation for eachclassi/f_ier.
5 STUDYII: EVALUATING EVACRC
This section reports on an industrial case study that empirically
evaluates EvaCRC,includingexperiments andinterviews.
5.1 ExperimentalData
5.1.1 DataCollection. Giventhesigni/f_icantvariabilityinpullre-
quest comments (e.g., sentence length and element complexity)
and the absence of such comments in many project’s code /f_iles,
we focused solely on inline review comments, as done in previ-
ous studies [ 6,57].Asour aim istocreate automatedmodels with
strong generalization capabilities, we did not restrict other project
characteristics, such as the number of authorsandreviewers.
5.1.2 Data Annotation. According to the con/f_iguration of our con-
ceptualmodel,eachreviewcommentisassigned/f_ivelabels.Four
of these labels represent the quality attributes, which are manu-
ally annotated, while the /f_ifth label indicates the quality grade,
automaticallydeterminedbasedonprede/f_inedmappings.Table 3
illustratestheenterprise-speci/f_icmappings.Theinitialversionof
thesemappingswasdraftedbythreeauthorsfollowingdiscussions
with practitioners from various software organizations. All the au-
thorscollaboratedtocreateanintermediateversion,andthe/f_inal
version wascon/f_irmed with professionals from theICT enterprise.
280EvaCRC: Evaluating Code ReviewComments ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table 2:Descriptions ofqualityattributes
Descriptivequestion Rationale
EmoDoesit comment in a kind way? Codereviewshouldevaluate codechanges ratherthan authors.
QueDoesit askquestions oraskfor con/f_irmation? Anything unclear in code review should be con/f_irmed to prevent them from becoming risks. Also,
knowledge transfer in codereviewshouldbetwo-way to bene/f_ittwosides.
EvaDoesit presentevaluationsorspecifycodeweaknesses? Codereviewshould/f_irstdetect and identify defects.
SugDoes it provide suggestions for correction or prevention? Codereviewisexpected to help/f_ix defects,improvequality, and improvedevelopers’ qualityconcerns.
Table 3:Exemplar mappings: qualityattributes & grades
M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 M14 M15 M16
Emo0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
Que0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1
Eva0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
Sug0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
IIIIII II III II III II III IIIV IV IV II IV
Wemanuallyannotatedandcarefullycross-checked17,000re-
view comments (Section 6.1.1). Figure2shows the distributions
of the experimental dataset, from which we observe that neither
qualityattributes norgrades are imbalanced.
Acceptable , 
7610 , 45%
Good , 6380 , 
38%Excellent , 
2788 , 16%Poor , 222, 1%
91401313914719
4387860
3861
228116562
0300060009000120001500018000
Suggestion Evaluation Question Emotion
No Yes
Figure2:Distributionsoftheexperimentaldataset(left:at-
tributes; right:grades)
5.2 ExperimentalResults andAnalysis
5.2.1 Answer To RQ2.1 (A/t_tribute’s Perspective). Table4summa-
rizestheperformanceofthesixtextclassi/f_iersinpredictingquality
attributes, with the best performances highlighted in bold. Our
initial focus is on the precision and recall of each classi/f_ier. For
thepredictionof‘emotion’,TextCNNstandsoutwiththehighest
precision at 0.89, but its recall is relatively low at 0.44. DPCNN and
Transformerhaveevenlowerrecallrates,at0.19and0.14respec-
tively, indicating a majority of the negative ‘emotion’ examples
were inaccurately predicted. In our task, we consider the negative
‘emotion’ as the positive class, as our objective is to identify it.
BERT,althoughitachievesaprecisionof0.79,hasahigherrecallof
0.86,whichplacesit/f_irstintermsofrecallamongthesixclassi/f_iers.
Whenpredicting‘question’,RFemergesasthebestwiththehighest
precision(0.96),closelyfollowedbyTransformer(0.95).BERT,with
a precision of 0.94, ranks third but exhibits the highest recall (0.95).
In terms of predicting ‘evaluation’, BERT’s precision (0.77) is lower
than that of Transformer (0.84), but BERT maintains second place.
A similar pattern occurs in the prediction of ‘suggestion’: BERT is
secondinterms ofprecision (0.93), but itleads for recall(0.92).
Owingtoitsperformanceinrecall,BERTsigni/f_icantlyoutper-
formstheothermodelsintermsofF1-score.Itregistersscoresof
0.82, 0.94, 0.75, and 0.92 for predicting ‘emotion’, ‘question’, ‘evalu-
ation’, and ‘suggestion’, respectively. Additionally, because of its
strong performance in maintaininga hightrue positive ratewhile
minimizingfalsepositives,BERTalsooutperformstheothertext
classi/f_iersintermsofAUC.Furthermore,wefoundthatBERT,along
with the other text classi/f_iers, generally performs better when pre-
dicting‘question’and‘suggestion’comparedtotheotherattributes.5.2.2 Answer To RQ2.2 (Grade’s Perspective). Table5presents a
summary of the performance of six text classi/f_iers from the quality
grade’sperspective.We/f_irstfocusonpredicting‘excellent’,BERT
leadsalltheevaluationmetrics(0.11forHammingloss,0.37for0/1
loss,and0.79formacro-F1).FollowedbyRFwhoseperformances
are: 0.19 for Hamming loss, 0.65 for 0/1 loss, and 0.66 for macro-F1.
Transformer performs worse than any others whose performances
are: 0.31 for Hamming loss, 0.92 for 0/1 loss, and 0.52 for macro-
F1. Due to the extremely high 0/1 loss, Transformer is unable to
correctlypredict ‘excellent’.BERTcontinues to be awinnerwhen
predicting‘good’,speci/f_ically,0.06forHammingloss,0.20for0/1
loss,and0.63formacro-F1.Thesuboptimalclassi/f_ierisTextCNN
withaHamminglossat0.09,a0/1lossat0.33,andamacro-F1at
0.55.Thewinnerchangeswhenpredicting‘acceptable’.Speci/f_ically,
Transformer that performs worst in predicting ‘excellent’ wins the
lead over twoevaluation metricsthis time(0.02 forHamming loss
and 0.08 for 0/1 loss). The macro-F1 of Transformer is 0.42, just
behindTextCNN(0.47) andBERT(0.57).
Finally,wefocusonclassi/f_iers’performanceinpredicting‘poor’,
as it may suﬀer from most disagreements in practice. As shown in
Figure2,thenumbersof‘excellent’and‘poor’reviewcomments
are signi/f_icantly rare. That is, there is a serious class imbalance
problem. Most classi/f_iers do not work in this case. For instance,
DPCNN’s and Transformer’s 0/1 loss is as high as 0.76 and 0.85,
respectively. BERT is highly eﬀective, however. BERT’s Hamming
lossand0/1lossareaslowas0.04and0.14,respectively,making
it remarkably outperform other classi/f_iers. The closest classi/f_ier
isRFwhoseHamminglossand0/1lossare0.14and0.40,respec-
tively. BERT also wins RF in terms of macro-F1 (0.48–0.44). To sum
up, BERT in most cases is the most eﬀective classi/f_ier from the
perspective ofeitherquality attributes orgrades.
RQ2 In Retrospect: The experiments with 17,000 real-
world examples show that the BERT-based multi-label text
classi/f_iercaneﬀectivelypredictthequalityattributesofreview
comments, signi/f_icantly outperforming other classi/f_iers on
multiple metrics. We selectitfor the automatedmodel.
5.3 Practitioners’Feedback
5.3.1 Methodology. We conducted focus group interviews [ 55]
with practitioners to collect a variety of feedback of EvaCRC. The
methodology ofthe interviewsisas follows.
Step1: Design&Preparation. Ourprotocol forthe focusgroup
interviewconsists of/f_ive aspects.
Objectives: We aim to evaluate andimprove EvaCRC.
Questions: Three questionsshould be answeredbyinterviewees.
Q1:HowusefulofEvaCRCforimprovingcodereviews?(ranging
from 1to 5grades)
Q2: Whatare the shortcomingsof EvaCRC?
Q3: Howto addressthe shortcomingsof EvaCRC?
281ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LanxinYang,JinweiXu,YifanZhang,He Zhang,andAlberto Bacchelli
Table 4:Performancesummary ofeachtextclassi/f_ierfromtheattributeperspective
RF TextCNN TextRCNN DPCNN Transformer BERT
P R F A P R F A P R F A P R F A P R F A P R F A
Emo 0.85 0.50 0.63 0.89 0.890.44 0.57 0.90 0.88 0.33 0.44 0.93 0.59 0.19 0.25 0.81 0.84 0.14 0.23 0.91 0.790.860.820.98
Que 0.960.80 0.88 0.98 0.94 0.86 0.89 0.98 0.92 0.86 0.89 0.98 0.78 0.71 0.74 0.93 0.95 0.83 0.88 0.98 0.940.950.940.99
Eva 0.65 0.40 0.49 0.86 0.76 0.47 0.58 0.88 0.74 0.50 0.59 0.89 0.53 0.41 0.46 0.76 0.840.21 0.33 0.87 0.770.740.750.94
Sug 0.83 0.87 0.85 0.93 0.91 0.80 0.85 0.94 0.91 0.80 0.85 0.95 0.85 0.73 0.78 0.89 0.950.65 0.77 0.94 0.930.910.920.97
Table 5:Performancesummary ofeachtextclassi/f_ierfromthegradeperspective
RF TextCNN TextRCNN DPCNN Transformer BERT
HL01LmF*HL01LmFHL01LmFHL01LmFHL01LmFHL01LmF
IV0.19 0.65 0.66 0.19 0.64 0.63 0.20 0.64 0.60 0.26 0.76 0.48 0.31 0.92 0.52 0.110.370.79
III0.10 0.34 0.54 0.09 0.33 0.56 0.09 0.33 0.55 0.13 0.44 0.44 0.11 0.41 0.51 0.060.200.63
II0.06 0.17 0.41 0.03 0.11 0.47 0.04 0.11 0.42 0.06 0.18 0.29 0.020.080.420.03 0.11 0.57
I0.14 0.40 0.44 0.13 0.43 0.43 0.16 0.54 0.40 0.27 0.76 0.32 0.24 0.85 0.30 0.040.140.48
*macro-AUC cannot be calculated if onlytrue or falseclassexamples exist.
Participants: Five interviewers and eightinterviewees participated
inthein-personfocusgroupinterview.Theinterviewersinclude
one research professor, two doctoral researchers, and two mas-
ter’s students fromthe same academic institute. The interviewees
include two executives (from the management division) and six
representatives of the two projects studied (two developers and
one leader from each). There were two steps for selecting the in-
terviewees. First, we invited ten executives to participate in the
interviews, twoofwhich had theavailability to meetus.Then the
twoexecutivesinvitedfurthersixinterviewparticipantsbecause
(1)theyhavemorethan10yearsofexperienceindevelopmentand
codereviews;(2)theystillparticipateindailycodereviews;(3)they
areacknowledgedasreviewexpertsbythisenterprise;(4)andmore
importantly, they are in charge of two important projects whose
review comments are selected for the live demonstration. With
multiple roles of participants, we expect to receive relevant and
insightful feedbackinthe interview.
AuxiliaryMaterials: Threetypesofdocumentswerepreparedfor
the interviews.
D1:Technique Report. It elaborates on (1) background and re-
lated work; (2) evaluation designs (conceptual model and
automatedmodel,experimentalsettings,etc.);(3)evaluation
results and analysis (automated model performance, error
analysis,etc.);and(4)operationmanuals(datapreparation,
automatedmodeltraining anddeployment,etc.).
D2:SourceCodeFiles.Theyarecorecode/f_ilesfor(1)datapre-
processing(datasetshuﬄeandsplit,etc.);(2)BERTandother
automatedmodels’construction,training,andtesting;(3)sta-
tistics andanalysis(correlation analysis,etc.).
D3:EvaluationExamples.Theyaretextualreviewcommentscon-
sisting of two parts: (1) experimental dataset (17,000 review
comments); (2) two projects whose leaders should partici-
pateintheinterviews(1,914reviewcomments).Eachreview
commentisassociatedwithfourspeci/f_icqualityattributes
andone overallqualitygrade.
Consents: Recording and disclosing the interview should be permit-
ted without disclosing sensitive enterprise information or personal
information, e.g.,names andevaluation results for exact projects.
Step 2: Execution. Two weeks before the interview, we delivered
thetechniquereporttothemanagementdivisionandprojectleader
representatives at the ICT enterprise.
During the group interview, we /f_irst introduced the background
of EvaCRC, the designs and implementations of EvaCRC, the de-
signs and results of experiments, and the evaluation results of twointernalprojectsthatdiﬀerfromtheexperimentalprojects.Then
weran EvaCRCfora livedemonstration.The textualreviewcom-
ments (input), kernel codeof EvaCRC(BERT-based textclassi/f_ier),
qualitygrade(output1),andqualityattributes(output2)regarding
each review comment were randomly checked by interviewees.
Note that we displayed only the source code of EvaCRC to the
interviewees. More precisely,thesourcecode ofthe BERT-based
multi-labeltextclassi/f_ierwhichconstitutestheautomatedmodel
of EvaCRC. The two reasons are: (1) Evaluation: EvaCRC can be
evaluated more rigorously if the source code is disclosed. As the
techniquereporthadbeendeliveredbefore,the/f_irstkeypartofthe
interview is to evaluate the detailed implementation of EvaCRC
(i.e., source code). (2)Replication: Before the formalinterview, the
executives indicate that EvaCRC might be used at this enterprise if
(a) EvaCRC performs well; and (b) the source code ( i.e., third-party
libraries used) of EvaCRC are license-allowed. Therefore, we did
not provide EvaCRC with a user interface but disclosed the source
code only to make interviewees check third-party libraries and
understand our model’s architecture, dependencies, and input and
outputformatsfor replication.
Having /f_inished the introduction and live demonstration, we
/f_irst presented feedback to any comments or questions raised by
theintervieweesimmediatelyandthenaskedtheintervieweesto
answertheprede/f_inedthreequestions.Next,theexecutivesstarted
the discussion about how to apply EvaCRC from the aspects of
datapreparation,models’selection,training,deployment, output
displays, etc.,basedonthe existing platforms at the enterprise.
The interview lasts around two hours and a half. After that,
we conducted several rounds of follow-up discussions with the
executives and project representatives regarding improving and
applyingEvaCRC at this enterprise.
Step 3: Analysis. We received a wide range of feedback from par-
ticipants based on their personal expertise, experience (story), and
observations in the interview. Then we usedthe narrative analy-
sis [12] method to qualitatively interpret interviewees’ satisfaction,
comments (concerns), and suggestions for improving and applying
EvaCRC.Atthisstage,weperformedinductivecoding[ 65].Finally,
we concluded the interview by forming and checking the minutes.
5.3.2 Results. TheintroductiontoEvaCRCwasverywellreceived.
For the live demonstration, the interviewees were surprised by the
ability to predict ‘emotion’ and were interested in mispredicted
examples, even though the majority of outputs were consistent
withtheirperception.Forinstance,“ ChancesofNUllpointer!!! ”,“too
282EvaCRC: Evaluating Code ReviewComments ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
manyspellingmistakes.‘POSSITION’?? ”werecorrectlypredictedto
bewithnegative ‘emotion’.“ Unnecessarycastingto HTTP respons-
eservlet”, “Use method jQuery.isNumeric instead of two method call ”
werecorrectlypredictedtobe‘evaluation’and‘suggestion’,respec-
tively. On theother hand, “ Indent error ” was incorrectly predicted
tobe‘suggestion’,“ addtheexceptiontothelog.howtodebug/f_ithere
is an error ” was incorrectly predicted to be ‘question’. Then we
discussed possible causes of errors ( e.g., low-quality dataset due to
inconsistentstandardsforannotatingreviewcomments,without
correction for model outputs) as well as correction and prevention
strategies ( e.g.,annotation checklists).
All interviewees recognized the value ( i.e., improving code re-
view through evaluating code review comments) and eﬀective-
ness (i.e., predicting quality attributes and grades of code review
comments) of EvaCRC. Moreover, EvaCRC is designed to mitigate
developers’ notorious resistance to evaluations by providing expla-
nations. Therefore, they all gave the highest usefulness rating. The
main shortcomingsof EvaCRC are (1)the inability todiﬀerentiate
quality grades of very long review comments because they may
bethoughtfulandaregradedtobe‘excellent’almost;(2)thelack
of con/f_idence for the predicted quality attributes; (3) the lack of
error(i.e.,anydeviationfromexpectations)-correctionmechanisms.
The /f_irst shortcoming limits the usage scenario of EvaCRC, i.e.,
inline review comments are ideal. For the second shortcoming, we
could make the automated output not only the predicted labels
but their inference probabilities. For the last, we could develop
hand-craftedcorrectionrules.PleaserefertoSection 6.2.2(Ouput
Correction&Improvement) for details.
Someintervieweesraisedquestionsaboutdataannotationand
validation, automatedmodelselection andtraining,etc., andmost
importantly, the application of EvaCRC. We report on them in
Section6,whichconstitutes guidelines for applyingEvaCRC.
RQ3 In Retrospect: The practitioners’ feedback received
fromanICTenterpriseindicatesthatsoftwarepractitioners
haverecognizedthevalueandeﬀectivenessofEvaCRC.More-
over,theyhaveprovidedinsightfulcommentsandsuggestions
for its improvement andapplicationinpractice.
6 DISCUSSION
Inthissection,wedelveintolessonslearnedandsuggestionsfor
bothresearchers,whodevelopEvaCRC,andpractitioners,whouse
EvaCRC.Thelessonsareprimarilyderivedfromouractionsand
observationsasauthors,whilethesuggestionsarelargelyinformed
byfeedbackfrompractitioners—someofwhichhavenotbeenacted
upon yet. We have organized this information primarily for clarity
of presentation. It is important to note that the lessons learned
could alsoserve as recommendations for future actions.
6.1 LessonsLearned
6.1.1 Annotating Review Comments. In our study, we have been
able to gather a set of lessons that assist in annotation, such as
identifying the presence of speci/f_ic keywords, code elements, or
analyzingthewordcountinareviewcomment.Additionally,there
are existing tools ( e.g., [1,79]) to assist annotation. However, thesetools may not always perform as anticipated due to the inherent
limitationsandexceptions to hand-craftedrules[ 39].
For example, while we commonly interpret a ‘?’ (question mark)
as a clear indication of a ‘question’, it could also be part of a
‘ternary if-else operator (b ? x : y)’. In such scenarios, the pro-
cess becomes challenging as we are required to continually revise
heuristicrules,modifyregularexpressions,andexpandkeyword
dictionaries.Therefore,cautionand/f_lexibilityarerequiredwhen
using automated tools and heuristics for annotation, taking into
account the complexityandvariability of code reviewcomments.
Emotion. Review comments with positive words such as “ good,
great, smart, cool, awesome ” likely indicate a positive ‘emotion’,
while those containing negative words such as “ bad, poor, awful,
terrible, shit, idiot, fool ” suggest negative ‘emotion’. Additionally,
the presence of multiple punctuation marks like “ ..., „„ ???, !!! ” may
hint at anger,satire,orcriticism.
Question. When raising questions, review comments generally (1)
containinterrogativekeywordssuchas“ what,who,which,where,
when, if, whether, how many, how long, how often ”, etc.; and (2)
consistofthegeneralquestionordisjunctivequestions;or,more
simply,(3)containtheinterrogativepunctuation–“ ?”.Ontheother
side, review comments that ask for clari/f_ication or con/f_irmation
generallycontainkeywordssuchas“ check,examine,ensure,con/f_irm,
remember,makesure ”.
Evaluation. When evaluating code changes, review comments
generally(1) contain adjective keywords that indicate opinionsor
judgments, e.g.,“redundant,useless,invalid,incorrect,insecure,obso-
lete, dangerous, illegal, unquali/f_ied, enough ”. (2) contain adverbs of
degree,e.g.,“too,much,hardly,nearly,fairly,rather,almost,already ”.
Suggestion. When oﬀeringsuggestions, reviewcomments gener-
ally (1) contain verb keywords that directly indicate what to do, or
not to do, or how to do, e.g., “add, remove, delete, change, replace,
format”, etc. (2) contain suggestive keywords, e.g., “suggest, recom-
mend, how about, what about, why not, had better, should, avoid,
forbid, use,no,don’t,let’s ”.
Table 6:Examples ofannotating codereview comments
C1*C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12
Emo1 1 1 0 1 1 1 1 1 1 1 1
Que0 0 1 1 0 1 0 0 0 0 0 1
Eva0 1 0 1 0 0 0 0 1 0 0 0
Sug1 0 0 0 1 0 1 1 1 1 1 1
III III II II III II III III IV III III IV
*Pleaserefer to Table 1for completereview comments.
6.1.2 Ensuring Label Consistency. We have learned the following:
Prior Annotating. The bene/f_its of prior annotating are (1) to
warm up annotatorsas earlyas possibleand, (2)to timelyidentify
anomalies and violations, then adjust quality attributes as well
asannotationchecklistsifneeded,andfurtherreachaconsistent
understanding among annotators.
Lock Checklists. Once reaching a consensus, it is recommended
totemporarilylockannotationchecklistsincaseoffurtherextra
debates.However,periodicrevisitswouldbenecessarysincethe
dataset mayhave already shifted.
Pair Annotating. The functions of pair annotating are similar
to pair programming or pair reviewing, which helps to stay fo-
cused and make timely corrections. Despite there being annotation
283ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LanxinYang,JinweiXu,YifanZhang,He Zhang,andAlberto Bacchelli
checklists, we strongly recommend pair annotating for handling
borderlineexamplesto ensure consistency.
Human-in-the-Loop. Human-in-the-loop[ 52]leveragesmachines
and human knowledge to develop intelligent models. With human-
in-the-loop learning, we /f_irst feed a few manually annotated exam-
plesintomodelsfortraining,checkandcorrectthepredictivelabels,
andthenfeedintonewexamples.Byrepeatingseveraliterations,
itislikely to convergeto consistent andcorrectlabels.
Consistency Measure. Consistency measures the rate of agree-
ment between multiple annotation outputs. We recommend the
metrics such as Kappa coeﬃcient to quantitatively measure the
consistency rate. By adopting the above measures, we expect to
constructthe ‘ground truth’ dataset for training models.
6.1.3 Checking Example Consistency. Example inconsistency (a.k.a.
dataset shift) occurs when the distribution of datasets diﬀers be-
tweentrainingandtestset[ 54],makingpredictionmodelsfail.It
isdiﬀerentfromlabelinconsistencyasweassumethatlabelsare
completely correct andconsistent inthiscase.The followingare
recommendedtechniques for detecting example inconsistency.
DescriptiveStatistics. Descriptivestatisticsprovidethebasicin-
formation of examples ( e.g., comment words, aﬃliations) that sum-
marize the dataset. Together with graphic analysis ( e.g., histogram
andpie chart), they form the representations ofdatasets.
HypothesisTesting. Statisticalhypothesistestingworksforas-
sessing the dataset by using sample examples. For instance, we
test whether the two groups of examples are equally distributed by
leveragingKolmogorov-Smirnov test [ 46]to detectdataset shift.
ErrorAnalysis. Detectingexampleinconsistencybyutilizingre-
liable models. Once the well-performed model does not work as
expected,itistimetolookintotheexamples’distributionaswellas
the labels’ correction. We call it a model-based detection technique
to distinguish the data-based detection techniques discussed above.
6.1.4 Analyzing Label Correlation. Through an analysis of the cor-
relation between attribute labels, we can better understand review-
ers’commentinghabits,enabling usto improve EvaCRC.Figure 3
shows the correlation heatmap for 17,000 examples with a total of
68,000binaryclasslabels.The/f_igureineachblockrepresentsthe
Kendall rank correlation coeﬃcient /u1D70Fof two variables (attribute
labels).Notethatallstatisticalhypothesistests(nullhypothesis:no
correlationbetweentwovariables)werecon/f_irmedbycheckingthe
p-value(0.001)andthereforewedonotpresentdetailedp-values
here.Ingeneral,if /u1D70Fislessthan0.2,thentwovariableshavefewcor-
relations; if /u1D70Fis between 0.2 and 0.4, then two variables have a few
correlations; otherwise,they have at leastmoderatecorrelations.
Suggestion -
Evaluation 
 Question -0.035
Emotion - 0.034 -0.0067
I I 
Suggestion Evaluation -0.18
I 
Question Emotion 0.15 
0.10 
-0.05
-0.00
--0.05
-0.10
-0.15
Figure 3:Correlation analysis ofreview comment labels
Figure3showsthat,whenservingsuggestions,reviewersgen-
erallydonotevaluatecodechangesorraisequestions.Reviewersexpresspositiveemotionsinmostcases.Insummary,allattribute
labels have no strong correlations, which con/f_irms the rationale
behindEvaCRC–sharingcommoninputrepresentationsbutregard-
ingeachpredictiontaskasindependent.Duetotheseobservations,
we did not compare EvaCRC that is a “Algorithm Adaptation”-
style approach (To adapt a single-label classi/f_ication algorithm
to the multi-label task by adjusting its cost function) to “Prob-
lem Transformation”-style approaches (To transform multi-label
taskstosingle-labeltasks:single-classormulti-class), e.g.,Classi/f_ier
Chain [58] (To regard each label as a part of a conditioned chain of
single-classclassi/f_icationtasks)andLabelPowerset[ 68](Toregard
each label combination as a separate class with one multi-class
classi/f_ication task). Further discussionon “Algorithm Adaptation”
and“Problem Transformation”are beyondthe scope of this study.
6.2 Recommendation to Researchers
6.2.1 The Conceptual Model. The following are recommendations
regarding twocomponentsof the conceptualmodel.
Attributes. Although there might be a number of attributes for
evaluating review comments ( e.g.,change triggering ), they are at
riskofdisagreementsfromcodereviewers.Wehaveoﬀeredfour
attributes and experience in annotating. Since they have been vali-
dated withdata triangulation, they are rigorous,credible, and can
be directly transferredto other software organizations. We recom-
mendtailoringattributesfromtheperspectiveofpragmaticsrather
thanmorphologyandsyntax(cf.Section 3.1)ifnecessary.Sinceour
conceptual model consists of attributes and mappings, the more
complex ofattributes,the more complex mappings willbe.
Mappings. We have established four-to-four mappings (four at-
tributes and four grades). The case mappings (cf. Table 3) can be
customizedandtailoredbasedonthesoftwareorganizations’needs,
e.g., three quality grades. Again,we donot recommendthebinary
evaluation as itbears the risksof disagreements from developers.
6.2.2 The Automated Model. The following are recommendations
regarding data,model,andcorrection.
DataPreparation. Duetoinsuﬃcientreviewcommentsforseveral
projects atthisICT enterprise,wehave notseparatedprojects for
trainingandtesting.Onceaprojectaccumulatesenoughcomments,
wesuggestsamplinganequalnumberfromeachfortrainingand
testing(e.g.,400fortrainingand100fortesting).Itisalsocrucial
that a project has diverse comments from multiple reviewers to
ensure consistency across projects; without this, the automated
modelmayhave reducedaccuracyandlimitedgeneralizability.
Model Selection&Training. We examined six leading text classi-
/f_iersandfoundBERTtobeexceptionallyeﬀective.SinceBERT’s
debut,numerousvariantslikeXLNet[ 78]andCodeBERT[ 27]have
emerged. Exploring these BERT-based and newer models could
enhance our automated model in the future. Additionally, training
strategies like pretraining [ 32] and adversarial training [ 70] can
further boost modelaccuracyandrobustness.
OutputCorrection&Improvement. Theevaluationlargelyde-
pends on the automated model and set mappings. While models
aid the process, they are not perfect, so correction strategies are
essential. For example, comments with negative emotions can only
284EvaCRC: Evaluating Code ReviewComments ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
achieve ‘acceptable’, andwithoutsuggestions, theycannot be‘ex-
cellent’. Even though we have not used linguistic characteristics in
ourmodel,theycanenhancepredictions;commentswithover20
words are deemed ‘acceptable’ at the minimum. Beyond rule-based
corrections, examining mislabeled examples canhighlight needed
adjustments. Once recti/f_ied, these examples should be retrained
in the model until accurately predicted. When it comes to improv-
ing the trustworthiness of outputs, we recommend outputting not
only eachquality attributebut also its con/f_idenceusing predicted
probability, e.g.,‘suggestion’-Yes-0.95.
6.3 Recommendations to Practitioners
ForOrganizations. Evaluationsshouldbeconductedinawaythat
minimizes pushback. For instance: (1) Display evaluation results
in /f_lexible formats, e.g., online for quality attributes (reminder),
oﬄine for quality grades (evaluation); (2) Use relaxed formats to
convey results, e.g., emojis; (3) Oﬀer clear explanations and best
practices based on set mappings, e.g., guiding reviewers on how
toavoida‘poor’gradeorachievean‘excellent’one;(4)Conduct
comprehensive evaluations that go beyond just review comments,
e.g.,reviewparticipationandcoverage,andthenumberandseverity
of defects detected in review and in post-review activities, e.g.,
testing; (5) Make code changes and review comments public rather
than restricting access to speci/f_ic projects or reviewers. In essence,
evaluationsshouldaimatenhancingfuturecodeandreviewquality
whilealsoincreasingdevelopers’awarenessofqualityissues.Ifnot
executedcorrectly,evaluations can be counterproductive.
For Developers. In addition to considering the recommendations
(rationales)providedinTable 2,reviewersshouldprioritizeproduc-
ing high-quality code over only striving for high-quality review
comments. While experienced developers might /f_ind it redundant
to elaborate on minor code defects, such as naming conventions,
newcomers bene/f_it from detailed explanations and suggestions for
recti/f_ication.Authorsshouldensureclarityintheircodecomments,
changedescriptions,remindersforareasofuncertainty,andself-
reviewpriortosubmittingforareview.Additionally,itisimportant
for authorsto respondpromptly toany questionsorclari/f_ications
soughtbyreviewers.Onlywiththesepracticescanthecodereview
processbe both eﬀective andeﬃcient.
7 THREATS TO VALIDITY
Construct Validity. The possible threat to construct validity may
resultfrom themeasurementof reviewcomment “quality”.There
is no standard measurement for this problem so far. To this end,
we employed data triangulation to collect, synthesize, and validate
quality attributes, aiming to develop a valid measurement of re-
view comment “quality”. These attributes have been empirically
con/f_irmed to be able to cover almost all review comments while
providing clear diﬀerences (cf. Figure 2), and have been con/f_irmed
bythe developers at alarge ICT enterprise.
Internal Validity. In the second study, we strive to ensure that
the experimental results (eﬀect) should only be aﬀected by the
classi/f_iers(cause).Weidenti/f_iedthreepossiblethreatstointernal
validity. The /f_irst concerns example collection and pre-processing.
We collected examples from the projects with multiple participants
and long-term development periods, and formatted examples byremoving project-aware marks ( e.g., defect and severity) to reduce
theirimpactsonclassi/f_iers.Thesecondisaboutexampleannotation.
Wehavetakenmanymeasurestoimproveannotationreliability(cf.
Section6.1.1and Section 6.1.2). The third relates to text classi/f_iers’
input representation. We used deep features that are automatically
learned by models, rather than hand-crafted features that are man-
uallyengineeredbyresearchers.Becausewecanhardlydistinguish
thecause-and-eﬀectrelationshipbetweendataandclassi/f_ierper-
formancesif we mixtwotypes of features.
ExternalValidity. WehaveevaluatedEvaCRCatalargeICTen-
terprise,whichmaybeunderthreattoexternalvalidity.Wehave
made eﬀorts to mitigate possible impacts. When developing the
conceptualmodel, we employed data triangulationto collect, syn-
thesize, and validate quality attributes. They are expected to be
language ( e.g., English, German, Chinese), project ( e.g., commer-
cial,OSS),andorganization-independent.Theexamplemappings
re/f_lecttheinterestsoftheenterprisebutcouldbeadjusted.When
performingexperiments fordevelopingtheautomatedmodel,we
selectedreviewcommentsfromawiderangeofprojects( e.g.,do-
mestic and overseas, innersource and private). The BERT-based
automatedmodelcouldbeupdatedtoadapttonewdatasources.
Overall,theindustrialcasestudyshowsthevalueandeﬀectiveness
of EvaCRC. More importantly, EvaCRC provides a paradigm for
evaluatingreviewcommentsin whichboth theconceptualmodel
andautomated modelsaretailorable.Sucha paradigm isexpected
to be generalized to other contexts with little eﬀort when referring
to our experience andrecommendations.
8 CONCLUSIONS
Inourpursuitofcreatinganexplainable,automatedsystemforeval-
uating review comments,weactively gatheredempiricalevidence
throughdatatriangulation.Thisledustoformulateaconceptual
model. Furthermore, we utilized multi-label learning to develop
text classi/f_iers, culminating in an automated model powered by
BERT.AcasestudyataglobalICTenterprisevalidatedtheeﬃcacy
of bothour conceptualand automatedmodels. Our work’ssigni/f_i-
canceextendsbeyondintroducingtheconceptualandautomated
EvaCRC models for code review evaluations. Crucially, we also of-
fersoftwareorganizationsapragmatic,customizableapproachand
share tangible experiences related to ensuring code review quality
throughthe evaluation of reviewcomments.
DATA AVAILABILITY
Thereplicationpackage(datafortriangulation,experiments,and
interviews)isavailable in https://doi.org/10.5281/zenodo.8297481 .
ACKNOWLEDGMENTS
This work is supported by the National Natural Science Founda-
tion of China (No.62072227, No.62202219), the National Key Re-
searchandDevelopmentProgramofChina(No.2019YFE0105500)
jointlywiththeResearchCouncilofNorway(No.309494),aswell
astheKeyResearchandDevelopmentProgramofJiangsuProvince
(No.BE2021002-2).AlbertoBacchelligratefullyacknowledgesthe
support of the Swiss National Science Foundation through the SNF
Project200021_197227.
285ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA LanxinYang,JinweiXu,YifanZhang,He Zhang,andAlberto Bacchelli
REFERENCES
[1]Tou/f_iqueAhmed,AmiangshuBosu,AnindyaIqbal,andShahramRahimi.2017.
SentiCR:Acustomizedsentimentanalysistoolforcodereviewinteractions.In
Proceedingsofthe32ndIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering . IEEE,106–111.
[2]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In Proceedings of the 35th International Conference
onSoftwareEngineering . IEEE,712–721.
[3]TobiasBaum,OlgaLiskin,KaiNiklas,andKurtSchneider.2016. Factorsin/f_luenc-
ingcodereviewprocessesinindustry.In Proceedingsofthe24thACMSIGSOFT
InternationalSymposiumonFoundationsofSoftwareEngineering . ACM,85–96.
[4]Olga Baysal, Oleksii Kononenko, Reid Holmes, and Michael W Godfrey. 2016.
Investigatingtechnicalandnon-technicalfactorsin/f_luencingmoderncodereview.
EmpiricalSoftwareEngineering 21,3 (2016), 932–959.
[5]AmiangshuBosu,JeﬀreyCCarver,ChristianBird,JonathanOrbeck,andChristo-
pher Chockley. 2016. Process aspects and social dynamics of contemporary
codereview:Insightsfromopensourcedevelopmentandindustrialpracticeat
Microsoft. IEEE Transactions onSoftwareEngineering 43,1 (2016), 56–75.
[6]AmiangshuBosu,MichaelaGreiler,and Christian Bird.2015. Characteristics of
usefulcodereviews:An empiricalstudyatMicrosoft.In Proceedingsofthe 12th
IEEE/ACMWorking Conference onMiningSoftwareRepositories . IEEE,146–156.
[7]Larissa Braz, Christian Aeberhard, Gül Çalikli, and Alberto Bacchelli. 2022. Less
is more: Supporting developers in vulnerability detection during code review. In
Proceedingsofthe44thIEEE/ACMInternationalConferenceonSoftwareEngineering .
ACM,1317–1329.
[8] LeoBreiman.2001. Randomforests. MachineLearning 45,1 (2001), 5–32.
[9]Nancy Carter. 2014. The use of triangulation in qualitative research. Oncol Nurs
Forum41,5 (2014), 545–547.
[10]Maria Caulo, Bin Lin, Gabriele Bavota, Giuseppe Scanniello, and Michele Lanza.
2020. Knowledge transfer in modern code review. In Proceedings of the 28th
IEEE/ACM International Conference on Program Comprehension . ACM, 230–240.
[11]Moataz Chouchen, Ali Ouni, Raula Gaikovina Kula, Dong Wang, Patanamon
Thongtanunam, Mohamed Wiem Mkaouer, and Kenichi Matsumoto. 2021. Anti-
patterns in modern code review: Symptoms and prevalence. In Proceedings of the
28th IEEE International Conference on Software Analysis, Evolution and Reengi-
neering. IEEE,531–535.
[12]F Michael Connelly and D Jean Clandinin. 1990. Stories of experience and
narrativeinquiry. EducationalResearcher 19,5 (1990), 2–14.
[13]John W Creswell and J David Creswell. 2017. Research design: Qualitative, quan-
titative,and mixedmethodsapproaches . SAGE.
[14]DanielaSCruzesandToreDybå.2011.Recommendedstepsforthematicsynthesis
in software engineering. In Proceedings of the 5th International Symposium on
EmpiricalSoftwareEngineering and Measurement . IEEE,275–284.
[15]Jacek Czerwonka, Michaela Greiler, and Jack Tilford. 2015. Code reviews do
not /f_ind bugs. How the current code review best practice slows us down. In
Proceedingsofthe37thIEEE/ACMInternationalConferenceonSoftwareEngineering:
SoftwareEngineering inPractice . IEEE,27–28.
[16]Nicole Davila and Ingrid Nunes. 2021. A systematic literature review and tax-
onomy of modern code review. Journal of Systems and Software 177 (2021),
110951:1–30.
[17]Letian Deng and Yanru Zhao. 2023. Deep learning-based semantic feature ex-
traction: A literature review and future directions. ZTE Communications 21, 2
(2023), 11–17.
[18]Norman K Denzin. 2012. Triangulation 2.0. Journal of Mixed Methods Research 6,
2 (2012), 80–88.
[19]NormanKDenzin.2017. Theresearchact:Atheoreticalintroductiontosociological
methods. Routledge.
[20]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.In
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
for ComputationalLinguistics: Human Language Technologies . ACL,29–35.
[21]Emre Doğan and Eray Tüzün. 2022. Towards a taxonomy of code review smells.
Informationand SoftwareTechnology 142(2022), 106737:1–24.
[22]FelipeEbert,FernandoCastor,NicoleNovielli,andAlexanderSerebrenik.2021.
Anexploratorystudyonconfusionincodereviews. EmpiricalSoftwareEngineer-
ing26,1 (2021), 1–48.
[23]Vasiliki Efstathiou and Diomidis Spinellis. 2018. Code review comments: Lan-
guagematters.In Proceedingsofthe40thIEEE/ACMInternationalConferenceon
SoftwareEngineering: NewIdeas and EmergingResults . ACM,69–72.
[24]CarolynDEgelman,EmersonMurphy-Hill,ElizabethKammer,MargaretMorrow
Hodges, Collin Green, Ciera Jaspan, and James Lin. 2020. Predicting develop-
ers’ negative feelings about code review. In Proceedings of the 42nd IEEE/ACM
InternationalConference onSoftwareEngineering . ACM,174–185.
[25]IkramElAsri,NoureddineKerzazi,GiasUddin,FoutseKhomh,andMAJanati
Idrissi. 2019. An empirical study of sentiments in code reviews. Information and
SoftwareTechnology 114(2019), 37–54.
[26]Michael E Fagan. 1976. Design and code inspections to reduce errors in program
development. IBM SystemsJournal 15,3 (1976), 182–211.[27]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
LinjunShou,BingQin,TingLiu,DaxinJiang,etal .2020.CodeBERT:Apre-trained
model for programming and natural languages. In Findings of the Association
forComputationalLinguistics:2020ConferenceonEmpiricalMethodsinNatural
Language Processing . ACL,1536–1547.
[28]Gensim.2023. Word2vecembeddings. https://radimrehurek.com/gensim/models/
word2vec.html .
[29]GitLab.2020. Codereviewguidelines. https://docs.gitlab.com/ee/development/
code_review.html . (accessed 01.25.2023).
[30]Google. 2019. Google’s engineering practices documentation. https://google.
github.io/eng-practices/review/ . (accessed 01.25.2023).
[31]SanuriGunawardena,EwanTempero,andKellyBlincoe.2023. Concernsiden-
ti/f_ied in code review: A /f_ine-grained, faceted classi/f_ication. Information and
SoftwareTechnology 153(2023), 107054:1–14.
[32]Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, KyleLo,Iz Beltagy,
Doug Downey, and Noah A Smith. 2020. Don’t stop pretraining: Adapt language
models to domains and tasks. In Proceedings of the 58th Annual Meeting of the
Associationfor ComputationalLinguistics . ACL,8342–8360.
[33]DongGyunHan,ChaiyongRagkhitwetsagul,JensKrinke,MatheusPaixao,and
GiovanniRosa.2020. Doescodereviewreallyremovecodingconventionviola-
tions?. In Proceedings of the 20th IEEE International Working Conference on Source
CodeAnalysisand Manipulation . IEEE,43–53.
[34]Masum Hasan, Anindya Iqbal, Mohammad Ra/f_id Ul Islam, AJM Rahman, and
AmiangshuBosu.2021. Usingabalancedscorecardtoidentifyopportunitiesto
improvecoderevieweﬀectiveness:Anindustrialexperiencereport. Empirical
SoftwareEngineering 26,6 (2021), 129:1–34.
[35]YangHong,ChakkritTantithamthavorn,PatanamonThongtanunam,andAldeida
Aleti. 2022. CommentFinder: A simpler,faster,more accurate codereviewcom-
ments recommendation. In Proceedings of the 30th ACM Joint European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
ACM,507–519.
[36]Yuan Huang, Xingjian Liang, Zhihao Chen, Nan Jia, Xiapu Luo, Xiangping Chen,
ZibinZheng,andXiaocongZhou.2022. Reviewingroundspredictionforcode
patches.EmpiricalSoftwareEngineering 27,1 (2022), 1–40.
[37]ToddDJick.1979. Mixingqualitativeandquantitativemethods:Triangulation
in action. AdministrativeScienceQuarterly 24,4 (1979), 602–611.
[38]RieJohnsonandTongZhang.2017. Deeppyramidconvolutionalneuralnetworks
fortextcategorization.In Proceedingsofthe55thAnnualMeetingoftheAssociation
for ComputationalLinguistics . ACL,562–570.
[39] RobbertJongeling,ProshantaSarkar,SubhajitDatta,andAlexanderSerebrenik.
2017. On negative results when using sentiment analysis tools for software
engineering research. EmpiricalSoftwareEngineering 22,5 (2017), 2543–2584.
[40]YoonKim.2014. Convolutionalneuralnetworksforsentenceclassi/f_ication.In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing . ACL,1746–1751.
[41]Oleksii Kononenko, Olga Baysal, Latifa Guerrouj, Yaxin Cao, and Michael W
Godfrey.2015. Investigatingcodereviewquality:Dopeopleandparticipation
matter?. In Proceedings of the 31st IEEE International Conference on Software
Maintenance and Evolution . IEEE,111–120.
[42]Andrey Krutauz, Tapajit Dey, Peter C Rigby, and Audris Mockus. 2020. Do code
reviewmeasuresexplaintheincidenceofpost-releasedefects? EmpiricalSoftware
Engineering 25,5 (2020), 3323–3356.
[43]Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutional
neuralnetworksfortextclassi/f_ication.In Proceedingsofthe29thAAAIConference
onArti/f_icial Intelligence . AAAI, 2267–2273.
[44]Yann LeCun,Yoshua Bengio, andGeoﬀreyHinton. 2015. Deep learning. Nature
521, 7553 (2015), 436–444.
[45]ZhixingLi,YueYu,GangYin,TaoWang,QiangFan,andHuaiminWang.2017.
Automatic classi/f_icationofreview comments inpull-based development model.
InProceedingsofthe29thInternationalConferenceonSoftwareEngineeringand
KnowledgeEngineering . KSI,572–577.
[46]FrankJMasseyJr.1951. TheKolmogorov-Smirnovtestforgoodnessof/f_it. Journal
ofthe AmericanstatisticalAssociation 46,253(1951), 68–78.
[47]Shane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E Hassan. 2016.
Anempiricalstudyoftheimpactofmoderncodereviewpracticesonsoftware
quality.EmpiricalSoftwareEngineering 21,5 (2016), 2146–2189.
[48]NuthanMunaiah,BenjaminSMeyers,CeciliaOAlm,AndrewMeneely,PradeepK
Murukannaiah, Emily Prud’hommeaux, Josephine Wolﬀ, and Yang Yu. 2017.
Natural language insights from code reviews that missed a vulnerability. In
Proceedings of the 9th International Symposium on Engineering Secure Software
and Systems . Springer, 70–86.
[49]MarcoOrtu,BramAdams,GiuseppeDestefanis,ParastouTourani,MicheleMarch-
esi,and RobertoTonelli.2015. Are bullies more productive?Empiricalstudyof
aﬀectiveness vs. issue /f_ixing time.In Proceedings of the 12th IEEE/ACM Working
Conference onMiningSoftwareRepositories . IEEE,303–313.
[50]Ali Ouni, Raula Gaikovina Kula, and Katsuro Inoue. 2016. Search-based peer
reviewersrecommendationinmoderncodereview.In Proceedingsofthe32ndIEEE
International Conference on Software Maintenance and Evolution . IEEE, 367–377.
286EvaCRC: Evaluating Code ReviewComments ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[51]MatheusPaixao,JensKrinke,DongGyunHan,ChaiyongRagkhitwetsagul,and
MarkHarman.2021. Theimpactofcodereviewonarchitecturalchanges. IEEE
Transactions onSoftwareEngineering 47,5 (2021), 1041–1059.
[52]RahulPandey,HemantPurohit,CarlosCastillo,andValerieLShalin.2022. Model-
ingandmitigatinghumanannotationerrorstodesigneﬃcientstreamprocessing
systems with human-in-the-loop machine learning. International Journal of
Human-ComputerStudies (2022), 102772:1–12.
[53]LucaPascarella,DavideSpadini,FabioPalomba,MagielBruntink,andAlberto
Bacchelli. 2018. Information needs in contemporary code review. Proceedings of
the ACMonHuman-ComputerInteraction 2,CSCW(2018), 1–27.
[54]Stephan Rabanser, Stephan Günnemann, and Zachary C Lipton. 2019. Failing
loudly: An empirical study of methods for detecting dataset shift. In Proceedings
of the 33rd International Conference on Neural Information Processing Systems .
Curran,1396–1408.
[55]FatemehRabiee.2004. Focus-groupinterviewanddataanalysis. Proceedingsof
the Nutrition Society 63,4 (2004), 655–660.
[56]Janani Raghunathan, Lifei Liu, and Huzefa H Kagdi. 2018. Feedback topics
in modern code review: Automatic identi/f_ication and impact on changes. In
Proceedings of the 30th International Conference on Software Engineering and
KnowledgeEngineering . KSI,598–597.
[57]MohammadMasudurRahman,ChanchalKRoy,andRaulaGKula.2017. Predict-
ing usefulness of code review comments using textual features and developer
experience. In Proceedings of the 14th IEEE/ACM International Conference on
MiningSoftwareRepositories . IEEE,215–226.
[58]Jesse Read, Bernhard Pfahringer, Geoﬀ Holmes, and Eibe Frank. 2011. Classi/f_ier
chains for multi-label classi/f_ication. MachineLearning 85,3 (2011), 333–359.
[59]Peter C Rigby and Christian Bird. 2013. Convergent contemporary software
peer review practices. In Proceedings of the 21st ACM Joint European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
ACM,202–212.
[60]Caitlin Sadowski, Emma Söderberg, Luke Church, Michal Sipko, and Alberto
Bacchelli.2018. Moderncodereview:AcasestudyatGoogle.In Proceedingsof
the 40th IEEE/ACM International Conference on Software Engineering: Software
Engineering inPractice . ACM,181–190.
[61]ForrestShull,JaniceSinger,andDagIKSjøberg.2007. Guidetoadvancedempirical
softwareengineering . Springer.
[62]IEEEComputerSociety.2008. IEEE1028-2008-IEEEstandardforsoftwarere-
views and audits. https://standards.ieee.org/standard/1028-2008.html . (accessed
12.15.2022).
[63]Davide Spadini, Gül Çalikli, and Alberto Bacchelli. 2020. Primers or reminders?
Theeﬀectsofexistingreviewcommentsoncodereview.In Proceedingsofthe42nd
IEEE/ACMInternationalConference onSoftwareEngineering . ACM,1171–1182.
[64]DavideSpadini,FabioPalomba,TobiasBaum,StefanHanenberg,MagielBruntink,
and Alberto Bacchelli. 2019. Test-driven code review: An empirical study. In
Proceedingsofthe41stIEEE/ACMInternationalConferenceonSoftwareEngineering .
IEEE,1061–1072.
[65]DavidRThomas.2006. Ageneralinductiveapproachforanalyzingqualitative
evaluation data. AmericanJournal ofEvaluation 27,2 (2006), 237–246.
[66]Veronica A Thurmond. 2001. The point of triangulation. Journal of Nursing
Scholarship 33,3 (2001), 253–258.
[67]Andrea C Tricco, Jesmin Antony, Wasifa Zarin, Lisa Stri/f_ler, Marco Ghassemi,
John Ivory, Laure Perrier, Brian Hutton, David Moher, and Sharon E Straus. 2015.
A scopingreviewof rapid reviewmethods. BMCMedicine 13,1 (2015), 1–15.[68]GrigoriosTsoumakasandIoannisVlahavas.2007. Randomk-labelsets:Anen-
semble method for multilabel classi/f_ication. In Proceedings of 18th European
Conference onMachineLearning . Springer, 406–417.
[69]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 31st Conference on Neural Information Processing
Systems. Curran,5998–6008.
[70]DilinWang,ChengyueGong, andQiang Liu.2019. Improving neurallanguage
modeling via adversarial training. In Proceedings of the 36th International Confer-
ence onMachineLearning . PMLR, 6555–6565.
[71]DongWang,YukiUeda,RaulaGaikovinaKula,TakashiIshio,andKenichiMat-
sumoto.2021. Canwebenchmarkcodereviewstudies?Asystematicmapping
study of methodology, dataset, and metric. Journal ofSystems and Software 180
(2021), 111009:1–18.
[72]DandanWang,QingWang,JunjieWang,andLinShi.2021. Acceptornot?An
empirical study on analyzing the factors that aﬀect the outcomes of modern
code review?. In Proceedings of the 21st IEEE International Conference on Software
Quality,Reliability and Security . IEEE,946–955.
[73]Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and
Anders Wesslén. 2012. Experimentation insoftware engineering . SpringerScience
& BusinessMedia.
[74]XizhuWuandZhihuaZhou.2017. Auni/f_iedviewofmulti-labelperformance
measures.In Proceedingsofthe34thInternationalConferenceonMachineLearning .
PMLR, 3780–3788.
[75]PavlínaWurzelGonçalves,GülÇalikli,andAlbertoBacchelli.2022. Interpersonal
con/f_lictsduringcodereview: Developers’experience and practices. Proceedings
ofthe ACMonHuman-ComputerInteraction 6,CSCW1 (2022), 1–33.
[76]Lanxin Yang, Jinwei Xu, Yifan Zhang, He Zhang, and Alberto Bacchelli. 2023.
Dataand materials. https://doi.org/10.5281/zenodo.8297481 .
[77]Lanxin Yang, He Zhang, Fuli Zhang, Xiaodong Zhang, and Guoping Rong. 2022.
An industrial experience report on retro-inspection. In Proceedings of the 44th
IEEE/ACMInternationalConferenceonSoftwareEngineering:SoftwareEngineering
inPractice . IEEE,43–52.
[78]ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,
andQuocVLe.2019. XLNet:Generalizedautoregressivepretrainingforlanguage
understanding. In Advances in Neural Information Processing Systems 32: Annual
Conference onNeural InformationProcessingSystems2019 . Curran,5754–5764.
[79]Qingshuang Yu, Jie Zhou, and Wenjuan Gong. 2019. A lightweight sentiment
analysis method. ZTE Communications 17,3 (2019), 2–8.
[80]FiorellaZampetti,SaghanMudbhari,VeneraArnaoudova,MassimilianoDiPenta,
Sebastiano Panichella, and Giuliano Antoniol. 2022. Using code reviews to
automatically con/f_igure static analysis tools. Empirical Software Engineering 27,
1 (2022), 1–30.
[81]Farida El Zanaty, Toshiki Hirao, Shane McIntosh, Akinori Ihara, and Kenichi
Matsumoto.2018. Anempiricalstudyofdesigndiscussionsincodereview.In
Proceedings of the 12th IEEE/ACM International Symposium on Empirical Software
Engineering and Measurement . ACM,1–10.
[82]Minling Zhang and Zhihua Zhou. 2014. A review on multi-label learning al-
gorithms. IEEE Transactions on Knowledge and Data Engineering 26, 8 (2014),
1819–1837.
Received 2023-02-02; accepted 2023-07-27
287