Pre-training Code Representation with Semantic Flow Graph for
Effective Bug Localization
Yali Du
Shandong University
duyali2000@gmail.comZhongxing Yuâˆ—
Shandong University
zhongxing.yu@sdu.edu.cn
ABSTRACT
Enlightened by the big success of pre-training in natural language
processing, pre-trained models for programming languages have
been widely used to promote code intelligence in recent years. In
particular, BERT has been used for bug localization tasks and im-
pressive results have been obtained. However, these BERT-based
bug localization techniques suffer from two issues. First, the pre-
trained BERT model on source code does not adequately capture
the deep semantics of program code. Second, the overall bug local-
ization models neglect the necessity of large-scale negative samples
in contrastive learning for representations of changesets and ignore
the lexical similarity between bug reports and changesets during
similarity estimation. We address these two issues by 1) proposing
a novel directed, multiple-label code graph representation named
Semantic Flow Graph (SFG), which compactly and adequately cap-
tures code semantics, 2) designing and training SemanticCodeBERT
based on SFG, and 3) designing a novel Hierarchical Momentum
Contrastive Bug Localization technique (HMCBL). Evaluation re-
sults show that our method achieves state-of-the-art performance
in bug localization.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and debug-
ging ;Maintaining software .
KEYWORDS
bug localization, semantic flow graph, type, computation role, pre-
trained model, contrastive learning
ACM Reference Format:
Yali Du and Zhongxing Yu. 2023. Pre-training Code Representation with
Semantic Flow Graph for Effective Bug Localization. In Proceedings of the
31st ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE â€™23), December 3â€“9,
2023, San Francisco, CA, USA. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/nnnnnnn.nnnnnnn
âˆ—Zhongxing Yu is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Â©2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn1 INTRODUCTION
While modern software engineering recognizes a broad range of
methods (e.g., model checking, symbolic execution, type checking)
for helping ensure that the software meets the specification of its
desirable behavior, the software (even deployed ones) is still un-
fortunately plagued with heterogeneous bugs for reasons such as
programming errors made by developers and immature develop-
ment process. The process of resolving the resultant bugs termed
debugging, is an indispensable yet frustrating activity that can
easily account for a significant part of software development and
maintenance costs [ 75]. To tackle the ever-growing high costs in-
volved in debugging, a variety of automatic techniques have been
proposed as debugging aids for developers over the past decades
[87]. In particular, numerous methods have been developed to fa-
cilitate fault localization, which aims to identify the exact locations
of program bugs and is one of the most expensive, tedious, and
time-consuming activities in debugging [79, 84].
The literature on fault localization is rich and is abundant with
methods stemming from ideas that originate from several differ-
ent disciplines, notably including statistical analysis [ 37,48,83,85],
program transformation [ 59], information retrieval [ 67,70]. Among
them, information retrieval-based methods typically proceed by
establishing the relevance between bug reports and related soft-
ware artifacts on the ground of information retrieval techniques,
and this category of methods is appealing as it is amenable to
the mainstream development practice which features continuous
integration (CI), versioning with Git, and collaboration within plat-
forms like GitHub [ 74]. In line with existing literature, information
retrieval-based fault localization hereafter is simply referred to as
bug localization.
The matched software artifact at the early phase of bug localiza-
tion research focuses on code elements such as classes and meth-
ods [ 41,71], but recent years have witnessed a growing interest
in changesets [ 18,67,78,80]. The key advantage of changesets is
that they contain simultaneously changed parts of the code that are
related, facilitating bug fixing. With regard to information retrieval
techniques, the major shift is that the dominating techniques have
changed from Vector Space Model (VSM) to deep learning tech-
niques, both for code elements and changesets. To precisely locate
the bug, bug localization techniques essentially need to accurately
relate the natural language used to describe the bug (in the bug
report) and identifier naming practices adopted by developers (in
the software artifacts). However, it is quite common that there ex-
ists a significant lexical gap between them, and consequently, the
retrieval quality of bug localization techniques is not always sat-
isfactory [ 95]. To overcome the issue, bug localization techniques
necessarily need to go beyond exact term matching and establisharXiv:2308.12773v1  [cs.SE]  24 Aug 2023ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yali Du and Zhongxing Yu
the semantic relatedness between bug reports and software arti-
facts.
Given that deep learning architectures are capable of leveraging
contextual information and have achieved impressive progress in
natural language processing, a number of bug localization tech-
niques based on the neural network have been proposed in re-
cent years [ 17,32,44,54,56,63,82,93,94]. In particular, state-of-
the-art transformer-based architecture BERT [ 21] (bidirectional
encoder representation from the transformer) has been widely em-
ployed [ 17,47]. Based on the naturalness hypothesis which states
that â€œ software corpora have similar statistical properties to natural
language corpora â€ [29], these BERT-based techniques first pre-train
a BERT model on a massive corpus of source code using certain
pre-training tasks such as masked language modeling, and then fine-
tune the trained BERT model for bug localization task. Experimental
evaluations have shown that reasonable accuracy improvements
can be obtained by these BERT-based techniques.
Despite the progress made, one drawback of these BERT-based
techniques is that the pre-trained BERT model on source code does
not adequately capture the deep semantics of program code. Unlike
natural language, the programming language has a formal structure,
which provides important code semantics that is unambiguous in
general [ 2]. However, the existing pre-trained BERT model either
totally ignores the code structure by treating code snippet as a
sequence of tokens same as natural language or considers only the
shallow structure of the code by using graph code representations
such as data flow graph [ 25]. Consequently, the formal code struc-
ture has not been fully exploited, resulting in an under-optimal
BERT model. To overcome this issue, we in this paper present
a novel code graph representation termed Semantic Flow Graph
(SFG), which compactly and adequately captures code semantics.
SFG is a directed, multiple-label graph that captures not only the
data flow and control flow between program elements but also the
type of program element and the specific role that a certain program
element plays in computation. On the ground of SFG, we further
propose SemanticCodeBERT, a pre-training model with BERT-like
architecture to learn code representation that considers deep code
structure. SemanticCodeBERT features novel pre-training tasks
besides the ordinary masked language modeling task.
In addition, the overall models of existing BERT-based bug lo-
calization techniques ignore several points which are beneficial
for further improving performance. First, the batch size is typi-
cally limited to save model space because of the huge scale of
BERT parameters, and the number of negative samples coupled to
batch size is thus limited. A variety of existed methods [ 9,11,13â€“
16,22,28,39,45,64,65,73,77,81,88] emphasizes the necessity of
large-scale negative samples in contrastive representation learn-
ing. In the bug localization context, it implies the importance of
considering the large-scale negative sample interactions for rep-
resentation learning of bug reports and changesets. Nevertheless,
existing techniques like Ciborowska et. al. [17] only select one ir-
relevant changeset in training as the negative sample for a bug
report, which causes inefficient mining of negative samples and
poor representation of the programming language. To alleviate this
issue, we propose to use a memory bank [ 81] to store rich change-
sets obtained from different batches for later contrast. In particular,
due to the constant parameter update by back-propagation, weutilize the momentum contrastive method [ 28] to account for the
inconsistency of negative vectors obtained by different models (in
different mini-batches). Second, existing BERT-based bug local-
ization techniques only account for the semantic level similarity
between bug reports and changesets, totally ignoring the lexical
similarity ( e.g., same identifier) which is also of vital importance
for retrieval if exists. To alleviate this issue, we propose to use a
hierarchical contrastive loss to leverage similarities at different
levels. On the whole, we design a novel Hierarchical Momentum
Contrastive Bug Localization (HMCBL) technique to address the
two limitations.
We implement the analyzer for obtaining SFG for Java code and
use the Java corpus (including 450,000 functions) of the CodeSearch-
Net dataset [ 33] to pre-train SemanticCodeBERT. On top of Seman-
ticCodeBERT, we apply the hierarchical momentum contrastive
method to facilitate the retrieval of bug-inducing changesets given
a bug report on the widely used dataset established in [ 78], which
includes six Java projects. Results show that we achieve state-of-
the-art performance on bug localization. Ablation studies justify
that the newly designed SFG improves the BERT model and the
new bug localization architecture is better than the existing ones.
Our contributions can be summarized as follows:
â€¢We present a novel directed, multiple-label code graph represen-
tation termed Semantic Flow Graph (SFG), which compactly and
adequately captures code semantics.
â€¢We employ SFG to train SemanticCodeBERT, which can be ap-
plied to obtain code representations for various code-related
downstream tasks.
â€¢We design a novel Hierarchical Momentum Contrastive Bug Lo-
calization technique (HMCBL), which overcomes two important
issues of existing techniques.
â€¢We conduct a large-scale experimental evaluation, and the results
show that our method outperforms state-of-the-art techniques
in bug localization performance.
2 RELATED WORKS
This section reviews work closely related to this paper. Bug local-
ization techniques proceed by making a query about the relevance
between bug reports and related software artifacts on top of in-
formation retrieval techniques. The investigated software artifacts
can be majorly divided into two categories: code elements such as
classes and methods [ 31,41,42,46,50â€“52,57,61,66,70â€“72,89,93]
and changesets [ 18,67,78,80]. Given changesets contain simulta-
neously changed parts of the code that are related and can thus
facilitate bug fixing, the use of changesets is gradually dominating.
With regard to information retrieval techniques, the Vector Space
Model (VSM) is widely used for its simplicity and effectiveness,
especially in the early phase of bug localization research. For in-
stance, BugLocator [ 90] makes use of the revised Vector Space
Model (rVSM) to establish the textual similarity between the bug
report and the source code and then ranks all source code files based
on the calculated similarity. For another example, Locus [ 78] repre-
sents one of the earliest works on changeset-based bug localization,
and it proceeds by matching bug reports to hunks.
As VSM basically performs exact term matching, the effective-
ness will be compromised in the common case where there existsPre-training Code Representation with Semantic Flow Graph for Effective Bug Localization ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
a significant lexical gap between the descriptions in the bug re-
port and naming practices adopted by developers in the software
artifacts. To overcome this issue, bug localization techniques es-
sentially need to establish the semantic relatedness between bug
reports and software artifacts. Given the impressive progress in
leveraging contextual information by deep learning architectures
in natural language processing, deep neural networks have been
widely used by researchers to learn representations for bug local-
ization in recent years [ 32,44,56,92]. For instance, Huo et. al. [32]
present the Deep Transfer Bug Localization task, and propose the
TRANP-CNN as the first solution for the cold-start problem which
combines cross-project transfer learning and convolutional neural
networks for file-level bug localization. Zhu et. al. [92] focus on
transferring knowledge (while filtering out irrelevant noise) from
the source project to the target project, and propose the COOBA
to leverage adversarial transfer learning for cross-project bug lo-
calization. Murali et. al. [56] propose Bug2Commit, which is an
unsupervised model leveraging multiple dimensions of data associ-
ated with bug reports and commits.
In particular, enlightened by the impressive achievements made
by BERT in natural language processing, BERT has been used for
bug localization tasks. Lin et. al. [47] study the tradeoffs between
different BERT architectures for the purpose of changeset retrieval.
Based on the Colbert developed by Khattab et. al. [40], Ciborowska
et. al. [17] propose the FBL-BERT model towards changeset-based
bug localization. Evaluation results show that FBL-BERT can speed
up the retrieval and several design decisions have also been ex-
plored, including granularities of input changesets and the utiliza-
tion of special tokens for capturing changesetsâ€™ semantic repre-
sentation. While impressive retrieval results of changesets have
been achieved, the Colbert used by FBL-BERT does not adequately
capture the deep semantics of program code and the overall models
of FBL-BERT suffer from two important limitations as described in
Section 1 (Introduction).
Furthermore, inspired by the success of pre-training models in
natural language processing, a number of pre-trained models for
programming languages have been proposed to promote the de-
velopment of code representation (which is vital for a variety of
code-based tasks in the field of SE). For instance, CodeBERT is a
pre-trained model proposed by Feng et. al. [23], which provides
generic representations for natural and programming language
downstream applications. GraphCodeBERT [ 25] imports structural
information to enhance the code representation by adding the data
flow graph as an auxiliary of input tokens and improves the perfor-
mance of code representation compared to CodeBERT. Kanade et.
al.[38] propose CuBERT, which is pre-trained on a massive Python
source corpus with two pre-training tasks of Masked Language
Modeling (MLM) and Next Sentence Prediction (NSP). Buratti et.
al.[10] propose C-BERT, a transformer-based language model that
is pre-trained on the C language corpus for code analysis tasks. Xue
et. al. [35] propose TreeBERT, which proposes a hybrid target for
AST to learn syntactic and semantic knowledge with tree-masked
language modeling (TMLM) and node order prediction (NOP) pre-
training tasks. More recently, the UniXcoder [ 24] is proposed to
leverage cross-modal information like Abstract Syntax Tree and
comments written in natural language to enhance code represen-
tation. While these pre-trained models on source code have madeprogress towards code representation, one drawback of them is that
they not adequately capture the deep semantics of program code
as they either treat code snippets as token sequences or consider
only shallow code structure by using graph code representations
such as data flow graph. Hence, we give a novel code graph repre-
sentation termed Semantic Flow Graph (SFG) to more compactly
and adequately capture code semantics in this paper. On top of
SFG, we further design and train SemanticCodeBERT with novel
pre-training tasks.
3 SEMANTIC FLOW GRAPH
This section introduces the Semantic Flow Graph (SFG), a novel
code graph representation designed for compactly representing
deep code semantics. On top of the naturalness hypothesis â€œ Software
is a form of human communication, software corpora have similar
statistical properties to natural language corpora, and these properties
can be exploited to build better software engineering tools â€ [29], recent
years have witnessed many innovations in using machine learning
(particularly deep learning) techniques to help make the software
more reliable and maintainable. To achieve successful learning, one
important ingredient lies in suitable code representation. The repre-
sentation, on the one hand, should capture enough code semantics,
and on the other hand, should be learnable across code written by
different developers or even different programming languages [ 2].
There are majorly three categories of code representation ways
within the literature: token-based ways that represent code as a se-
quence of tokens [ 1,19,26,27], syntactic-based ways that represent
code as trees [ 4â€“6,30,55,62,86], and semantic-based ways that
represent code as graph structures [ 3,7,8,12,20,25,43,91]. For
token-based representation, while its simplicity facilities learning,
the representation ignores the structural nature of code and thus
captures quite limited semantics. For syntactic-based representa-
tion, despite the tree representation in principle can contain rich
semantic information, the learnability is unfortunately confined
as the tree typically has an unusually deep hierarchy and there, in
general, will involve significant refinement efforts of the raw tree
representation to enable successful learning in practice. Semantic-
based representation aims to encode semantics in a way that fa-
cilitates learning, and a variety of graphs have been employed for
code model learning, including for example data flow graph [ 12,25],
control flow graph [ 20], program dependence graph [ 8], contextual
flow graph [7].
While these graph-based representations have facilitated the
learning of code semantics embodied in data dependency and con-
trol dependency, certain other code semantics are overlooked. In
particular, the information of what kinds of program elements are
related by data dependency or control dependency and through
which operations they are related to is neglected. We argue that
this information is crucial for accurately learning code semantics.
For instance, given a code snippet â€œ a = m (b, c) â€ where a,b, and
careBoolean ,Integer , and User-defined type variables respec-
tively, and mis a certain function call, there will be two data flow
edges bâ†’aandcâ†’aconsidering the data flow graph, and the
code snippet will read â€œ the values of two variables have flown into
another variable â€. Under this circumstance, as the corresponding
data flow graph coincides, the meaning of the code snippet has noESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yali Du and Zhongxing Yu
Figure 1: An example of the semantic flow graph.
difference with a variety of other code snippets such as â€œ a = (b &&
m (c)) â€ where a,b, and careBoolean ,Boolean , and arbitrary type
variables respectively, and mis a certain function call that returns a
boolean value. But if the additional information of what kinds of
program elements and which operations are taken into account, the
code snippet â€œ a = m (b, c) â€ will read â€œ the value of an Integer type
variable and the value of a User-defined type variable have flown into
another Boolean type variable through a function call â€, which is more
precise. To compactly integrate these two pieces of information
into graphs, we design a novel directed, multiple-label code graph
representation termed Semantic Flow Graph (SFG).
Definition 3.1. (Semantic Flow Graph) . The Semantic Flow
Graph (SFG) for a code snippet is a tuple <ğ‘,ğ¸,ğ‘‡,ğ‘… >where
Nis a set of nodes, Eis a set of directed edges between nodes in
N, and TandRare mappings from nodes to their types and their
roles in computation respectively.
A number of points deserve comment. First, the node set Ncan
be further divided into node sets Nğ‘‰andNğ¶, which contain nodes
corresponding to variables and control instructions in the code
respectively. While the variable has a one-to-one mapping with a
certain node from Nğ‘‰, there may be one or multiple nodes in Nğ¶for
a certain control instruction. Essentially, if a control instruction has
an associated condition and ndifferent branches (i.e., straight-line
code blocks) to go depending on the condition evaluation result,
there will be a node in Nğ¶for the condition, a node in Nğ¶for the
convergence of the different branches, and ndifferent nodes in Nğ¶
for the nbranches respectively.
Second, a directed edge nğ‘â†’nğ‘(nğ‘âˆˆN,nğ‘âˆˆN) inEcan
be of 3 kinds. The first kind Eğ·represents a data flow between
two variables if nğ‘âˆˆNğ‘‰âˆ§nğ‘âˆˆNğ‘‰holds, the second kind Eğ¶
embodies the control flow between two straight-line basic blocks
ifnğ‘âˆˆNğ¶âˆ§nğ‘âˆˆNğ¶holds, and finally the third kind Eğ‘†denotes
the natural sequential computation flow inside or between basic
blocks in case nğ‘âˆˆNğ‘‰âˆ§nğ‘âˆˆNğ¶ornğ‘âˆˆNğ¶âˆ§nğ‘âˆˆNğ‘‰holds. In
particular, the edge set Eis established as follows:(1) Establish Eğ·among nodes from set Nğ‘‰according to Intra-block
and Inter-block data dependencies between variables.
(2) Establish Eğ¶among nodes from set Nğ¶according to the specific
control flow of the control instruction.
(3) Establish Eğ‘†following these rules: there will be an edge nğ‘â†’nğ‘
(i) if nğ‘âˆˆNğ¶is for the control instruction condition and nğ‘âˆˆNğ‘‰
is for a certain variable involved in the condition; (ii) if nğ‘âˆˆNğ¶is
for the control instruction branch and nğ‘âˆˆNğ‘‰is for the left-most
variable of the first statement inside the branch; (iii) if nğ‘âˆˆNğ‘‰is for
the left-most variable of the last statement inside a control instruction
branch and nğ‘âˆˆNğ¶is for the control instruction convergence; (IV)
ifnğ‘âˆˆNğ¶is for the control instruction convergence and nğ‘âˆˆNğ‘‰is
for the left-most variable of the first statement inside the basic block
directly following the control instruction.
Third, mapping Tmaps each node in Nto its type, encoding the
needed information of â€œ what kinds of program elements are relatedâ€.
For each node in Nğ‘‰,Tmaps it to the corresponding type of the
variable. For each node in Nğ¶,Tmaps the node to the specific part
of the control instruction it refers to. Take the control instruction
If-Then-Else as an example, Tmaps the associated 4 nodes in
Nğ¶for it to type IfCondition ,IfThen ,IfElse , and IfCONVERGE
respectively.
Finally, mapping Rmaps each node in Nğ‘‰to its role in the com-
putation, encoding the needed information of â€œthrough which op-
erations program elements are relatedâ€. Basically, Rconsiders the
associated operation and control structure for the variable to deter-
mine its computation role. From an implementation perspective, for
each node in Nğ‘‰,Rchecks the direct parent of the corresponding
variable in the abstract syntax tree (AST) and the position rela-
tionship between it and the direct parent to establish the role. For
instance, given a code snippet, â€œ a = b â€ where aandbare variables,
Rmaps the roles of aandbtoAssigned andAssignement respec-
tively. For another example, given a code snippet â€œ a.m(b) â€ where
aandbare variables, and mis a certain function call, Rmaps the
roles of aandbtoInvocationTarget andInvocationArgumentPre-training Code Representation with Semantic Flow Graph for Effective Bug Localization ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
respectively. For nodes in Nğ¶, we do not consider their roles as they
are implicit in their types.
Note it is difficult to simply augment classical graph program
representations with information of type and computation role.
Existing representations like program dependence graph typically
work at the statement granularity (i.e., each graph node represents a
statement), making it hard to encode detailed type and computation
role information of multiple program elements in the statement.
The proposed SFG works at a finer granularity with two types
of nodes that have a one-to-one mapping with program variables
and a one-to-one (or many-to-one) mapping with program con-
trol ingredients respectively. This kind of node representation is
proposed for two reasons. On the one hand, it is convenient to
analyze the types and computation roles of variables (through how
they are connected with other program elements) and program
control ingredients. On the other hand, data flow and control flow
information are established respectively by analyzing variable uses
and program control ingredients. With SFG (built on such a node
representation), data flow and control flow can be encoded through
the edges between nodes, and the type and computation role in-
formation can be encoded through node labels. SFG does not have
nodes for additional program elements (like invocation etc.), thus
it is compact but contains adequate semantic information.
Overall, SFG is a directed, multiple-label graph that captures not
only the data flow and control flow between program elements,
but also the type of program element and the specific role that
a certain program element plays in computation. Moreover, SFG
represents this information in a compact way, facilitating learning
across programs.
Example 3.1. Figure 1 gives an example of a Semantic Flow Graph
for a simple method.
Implementation : We fully implement an analyzer to get Semantic
Flow Graph (SFG) for a Java method on top of Spoon [ 60], which is
an open-source library to analyze, rewrite, transform, and transpile
Java source code. Our analyzer supports modern Java versions up to
Java 16. For nodes in Nğ‘‰, the analyzer considers different kinds of
primitive types and common JDK types, and a special type named
user-defined type . In total, the analyzer considers 20 types for
nodes in Nğ‘‰. For nodes in Nğ¶, the analyzer takes all the control
instruction kinds (up to Java 16) into account and considers 35 types
in total. With regard to role, the analyzer considers 43 different
roles in total for nodes in Nğ‘‰.
4 SEMANTICCODEBERT
In this section, we describe first the architecture of SemanticCode-
BERT (shown in Figure 3), then the graph-guided masked attention
based on the semantic flow graph, and finally the pre-training tasks.
Overall, the SemanticCodeBERT network architecture adapts the
architecture of GraphCodeBERT for the proposed novel SFG pro-
gram representation and SemanticCodeBERT also features tailored
pre-training tasks for the SFG representation.
4.1 Model Architecture
The SemanticCodeBERT follows BERT (Bidirectional Encoder Rep-
resentation from Transformers) (Devlin et. al., [21]) as the backbone.
Figure 2: All defined types and roles.
Comment Input Sequence : We import comments as a supple-
ment for the model to understand the semantic information of
programming code. [ğ¶ğ¿ğ‘†]is the special classification token at the
beginning of the comment sequence ğ‘Š.
Source Code Input Sequence : We cleanse the source code and
remove erroneous characters, and add the special classification
token[ğ‘†ğ¸ğ‘ƒ]at the end of the source code and input sequence. To
represent the start-of-code, we import a pre-appended token [ğ¶]
to split the comment and source code. The source code sequence
can be represented as ğ‘†.
Node Input Sequence : With the procedure discussed in Section
3, we generate a semantic flow graph (SFG) for each code snippet.
At the beginning of the node list ğ‘, a pre-appended token [ğ‘]is
added to represent the start-of-node.
Type Input Sequence : To answer the question of " what kinds of
program elements are related ", we have identified 55possible types
for the code element. ğ‘‡={ğ‘¡1,...,ğ‘¡ 55}represents the set of all 55
possible types, and [ğ‘‡]is pre-appended as the start-of-type. The
complete list of types is shown in Figure 2.
Role Input Sequence : To answer the question of " through which
operations program elements are related ", we have defined 43roles
to mark the role of each program element in the computation,
taking into account the associated operation and control structure.
ğ‘…={ğ‘Ÿ1,...,ğ‘Ÿ 43}is the set of all 43possible roles, and the pre-
appended token[ğ‘…]represents the start-of-role. The complete list
of roles is shown in Figure 2.
As intuitively shown in Figure 3, we concatenate the comment,
source code, nodes, types, and roles as the input sequence:
ğ‘‹=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡[[ğ¶ğ¿ğ‘†],ğ‘Š,[ğ¶],ğ‘†,[ğ‘†ğ¸ğ‘ƒ],[ğ‘],ğ‘,[ğ‘‡],ğ‘‡,[ğ‘…],ğ‘…,[ğ‘†ğ¸ğ‘ƒ]].
(1)ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yali Du and Zhongxing Yu
Figure 3: The SemanticCodeBERT takes to comment, source code, nodes of SFG, types, and roles as the input, and is pre-trained
by standard masked language modeling [ 21], node alignment (marked with red lines), graph prediction (marked with green
lines), type prediction (marked with blue lines) and role prediction (marked with purple lines).
4.2 Masked Attention
We resort to the graph-guided masked attention function described
in [25] to filter irrelevant signals in Transformer.
â€¢The setğ¸1indicates the alignment relation between ğ‘†andğ‘,
where(ğ‘ ğ‘–,ğ‘›ğ‘—)/(ğ‘›ğ‘—,ğ‘ ğ‘–)âˆˆğ¸1if the nodeğ‘›ğ‘—is identified from the
source code token ğ‘ ğ‘–.
â€¢The setğ¸2indicates the dependency relation in ğ‘, where(ğ‘›ğ‘–,ğ‘›ğ‘—)âˆˆ
ğ¸2if there is a direct edge from the node ğ‘›ğ‘–to the node ğ‘›ğ‘—.
â€¢The setğ¸3incorporates the type information of the nodes, where
(ğ‘›ğ‘–,ğ‘¡ğ‘—)âˆˆğ¸3if the type of the node ğ‘›ğ‘–isğ‘¡ğ‘—.
â€¢The setğ¸4incorporates the role information of the nodes, where
(ğ‘›ğ‘–,ğ‘Ÿğ‘—)âˆˆğ¸4if the role of the node ğ‘›ğ‘–isğ‘Ÿğ‘—.
The masked attention matrix is formulated as ğ‘€:
ğ‘€ğ‘–ğ‘—=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³0ğ‘¥ğ‘–âˆˆ[ğ¶ğ¿ğ‘†],[ğ‘†ğ¸ğ‘ƒ];
ğ‘¤ğ‘–,ğ‘ ğ‘—âˆˆğ‘Šâˆªğ‘†;
(ğ‘ ğ‘–,ğ‘›ğ‘—)/(ğ‘›ğ‘—,ğ‘ ğ‘–)âˆˆğ¸1;
(ğ‘›ğ‘–,ğ‘›ğ‘—)âˆˆğ¸2;
(ğ‘›ğ‘–,ğ‘¡ğ‘—)âˆˆğ¸3;
(ğ‘›ğ‘–,ğ‘Ÿğ‘—)âˆˆğ¸4;
âˆ’âˆğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ .(2)
Specifically, the masked attention function blocks the transmission
of unrelated tokens by setting the attention score to an infinitely
negative value.
4.3 Pre-training Tasks
The pre-training tasks of SemanticCodeBERT are described in this
section. Besides masked language modeling, node alignment, and
edge prediction pre-training tasks proposed by Guo et. al., [25], we
define two novel pre-training tasksâ€“type and role prediction. These
two novel pre-training tasks represent the first attempt to leverage
the attribute information of nodes for learning code representation.
Masked Language Modeling : The masked language modeling
pre-training task is proposed by Devlin et. al., [21]. We replace 15%
of the source code with [MASK] 80% of the time, a random token
10% of the time or itself 10% of the time. The comment context
contributes to inferring the masked code tokens [25].
Node Alignment : The motivation of node alignment is to align
representation between source code and nodes of semantic flow
graph [ 25]. We randomly mask 20% edges between the source codeand nodes, and then predict where the nodes are identified from (i.e.,
predict these masked edges ğ¸1
ğ‘šğ‘ğ‘ ğ‘˜). As shown in the Figure 3, the
model should distinguish that ğ‘›2comes from ğ‘ 6andğ‘›13comes from
ğ‘ 33. We formulate the loss function as Equation 3. Let ğ¸1beğ‘†Ã—ğ‘,
ğ›¿(ğ‘’ğ‘–ğ‘—âˆˆğ¸1
ğ‘šğ‘ğ‘ ğ‘˜)is one if(ğ‘ ğ‘–,ğ‘›ğ‘—)âˆˆğ¸1, and zero otherwise. ğ‘ğ‘’ğ‘– ğ‘—is
the probability of the edge from ğ‘–-th code token to ğ‘—-th node, which
is calculated by dot product following a sigmoid function using the
representations of ğ‘ ğ‘–andğ‘›ğ‘—outputted from SemanticCodeBERT.
Lğ‘ğ´=âˆ’âˆ‘ï¸
ğ‘’ğ‘– ğ‘—âˆˆğ¸1
ğ‘šğ‘ğ‘ ğ‘˜[ğ›¿(ğ‘’ğ‘–ğ‘—)ğ‘™ğ‘œğ‘”ğ‘ğ‘’ğ‘– ğ‘—+
(1âˆ’ğ›¿(ğ‘’ğ‘–ğ‘—))ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ğ‘’ğ‘– ğ‘—)].(3)
Edge Prediction : The motivation of edge prediction is to encourage
the model to learn structural relationships from semantic flow
graphs for better programming code representation. Like node
alignment, we randomly mask 20% edges between nodes in the
mask matrix, encouraging the model to predict these masked edges
ğ¸2
ğ‘šğ‘ğ‘ ğ‘˜(e.g.,the edges (ğ‘›3,ğ‘›2) and (ğ‘›12,ğ‘›11)). We formulate the loss
function as Equation 4. Let ğ¸2beğ‘Ã—ğ‘,ğ›¿(ğ‘’ğ‘–ğ‘—âˆˆğ¸2
ğ‘šğ‘ğ‘ ğ‘˜)is one if
(ğ‘›ğ‘–,ğ‘›ğ‘—)âˆˆğ¸2, and zero otherwise. ğ‘ğ‘’ğ‘– ğ‘—is the probability of the edge
fromğ‘–-th node to ğ‘—-th node.
Lğºğ‘ƒ=âˆ’âˆ‘ï¸
ğ‘’ğ‘– ğ‘—âˆˆğ¸2
ğ‘šğ‘ğ‘ ğ‘˜[ğ›¿(ğ‘’ğ‘–ğ‘—)ğ‘™ğ‘œğ‘”ğ‘ğ‘’ğ‘– ğ‘—+
(1âˆ’ğ›¿(ğ‘’ğ‘–ğ‘—))ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ğ‘’ğ‘– ğ‘—)].(4)
Type Prediction : The motivation of type prediction is to guide
the model to comprehend the types ( e.g.,â€œintâ€, â€œdoubleâ€, â€œIfCondi-
tionâ€) of nodes for better programming code representation. We
pre-append the full set of types ğ‘‡to the input nodes. Let ğ¸3be
ğ‘Ã—ğ‘‡, if the type of node ğ‘›ğ‘–isğ‘¡ğ‘—(i.e.,(ğ‘›ğ‘–,ğ‘¡ğ‘—)âˆˆğ¸3),ğ›¿(ğ‘’ğ‘–ğ‘—âˆˆğ¸3
ğ‘šğ‘ğ‘ ğ‘˜)
is one, otherwise it is zero. We randomly mask 20% edges between
nodes and types and formulate the loss function as Equation 5,
whereğ¸3
ğ‘šğ‘ğ‘ ğ‘˜are masked edges and ğ‘ğ‘’ğ‘– ğ‘—is the probability of the
edge fromğ‘–-th node to ğ‘—-th type.
Lğ‘‡ğ‘ƒ=âˆ’âˆ‘ï¸
ğ‘’ğ‘– ğ‘—âˆˆğ¸3
ğ‘šğ‘ğ‘ ğ‘˜[ğ›¿(ğ‘’ğ‘–ğ‘—)ğ‘™ğ‘œğ‘”ğ‘ğ‘’ğ‘– ğ‘—+
(1âˆ’ğ›¿(ğ‘’ğ‘–ğ‘—))ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ğ‘’ğ‘– ğ‘—)].(5)
Role Prediction : â€œRoleâ€ indicates the computation role of the nodePre-training Code Representation with Semantic Flow Graph for Effective Bug Localization ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
in the semantic flow graph ( e.g.,â€œInvocationArgumentâ€, â€œAssignedâ€,
â€œAssignmentâ€). Role prediction can feed the model with a more
informative signal to understand the correlation among different
nodes. We pre-append the full set of roles ğ‘…to the input nodes.
Letğ¸4beğ‘Ã—ğ‘…, if the role of node ğ‘›ğ‘–isğ‘Ÿğ‘—(i.e.,(ğ‘›ğ‘–,ğ‘Ÿğ‘—) âˆˆğ¸4),
ğ›¿(ğ‘’ğ‘–ğ‘—âˆˆğ¸4
ğ‘šğ‘ğ‘ ğ‘˜)is one, otherwise it is zero. We randomly mask
20% edges between nodes and roles and formulate the loss func-
tion as Equation 6, where ğ¸4
ğ‘šğ‘ğ‘ ğ‘˜are masked edges and ğ‘ğ‘’ğ‘– ğ‘—is the
probability of the edge from ğ‘–-th node to ğ‘—-th role.
Lğ‘…ğ‘ƒ=âˆ’âˆ‘ï¸
ğ‘’ğ‘– ğ‘—âˆˆğ¸4
ğ‘šğ‘ğ‘ ğ‘˜[ğ›¿(ğ‘’ğ‘–ğ‘—)ğ‘™ğ‘œğ‘”ğ‘ğ‘’ğ‘– ğ‘—+
(1âˆ’ğ›¿(ğ‘’ğ‘–ğ‘—))ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ğ‘’ğ‘– ğ‘—)].(6)
5 CHANGESET-BASED BUG LOCALIZATION
In this section, we illustrate the utilization of the SemanticCode-
BERT towards bug localization with changesets. The proposed bug
localization model is shown in Figure 4. The model aims to address
the two important limitations (as described in Section 1) of the
overall models of existing BERT-based bug localization techniques.
5.1 Problem Definition
Given a setQ={ğ‘1,ğ‘2,...,ğ‘ğ‘€}ofğ‘€bug reports, the bug lo-
calization task aims to discover more relevant changesets from
K={ğ‘˜1,ğ‘˜2,...,ğ‘˜ğ‘}, a set including ğ‘changesets. More specifi-
cally, for a bug report ğ‘âˆˆQ, a bug-inducing changeset ğ‘âˆˆK and
a not bug-inducing changeset ğ‘›âˆˆK are selected to form a triplet
(ğ‘,ğ‘,ğ‘› ). All bug-inducing changesets and not bug-inducing change-
sets are non-overlapping. The goal of learned similarity function ğ‘ 
is to provide a high value for ğ‘ (ğ‘,ğ‘)(between the anchor ğ‘and the
positive sample ğ‘) and a low value for ğ‘ (ğ‘,ğ‘›)(between the anchor ğ‘
and the negative sample ğ‘›). Section 5.2 focuses on producing accu-
rate representations of bug reports and changesets, and Section 5.3
describes the estimation of similarities and the loss function for
training the model.
5.2 Representation learning
The proposed model consists of three parts, an encoder network,
projector network, and momentum update mechanism with a mem-
ory bank that stores rich representations of changesets.
Encoder Network : As mentioned before, bug reports consist of
natural language descriptions and project changesets consist of
programming language code. Hence, we introduce BERT [ 21] as
the backbone to the encoder bug report as qğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ , and Seman-
ticCodeBERT as the backbone to encoder relevant changeset as
pğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ and irrelevant changeset as nğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ .
ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³qğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ =BERT(ğ‘ğ‘¡ğ‘œğ‘˜),
pğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ =SemanticCodeBERT (ğ‘ğ‘¡ğ‘œğ‘˜),
nğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ =SemanticCodeBERT (ğ‘›ğ‘¡ğ‘œğ‘˜),(7)
where BERT andSemanticCodeBERT are the trainable parameters
of BERT and SemanticCodeBERT, ğ‘ğ‘¡ğ‘œğ‘˜,ğ‘ğ‘¡ğ‘œğ‘˜, andğ‘›ğ‘¡ğ‘œğ‘˜are the input
tokens obtained by tokenizers, qğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’âˆˆRğ‘‘,pğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’âˆˆRğ‘‘, andnğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’âˆˆRğ‘‘are the refined vectors ( ğ‘‘is the dimension of the
mapped spaces).
Projector Network : After the feature vectors are extracted, we use
a multi-layer perception neural network as a projector to compress
the vectors of bug reports and changesets into a compact shared
embedding space. We replace Dropout with Batch Normalization for
regularization, which can be trained with saturating nonlinearities
and are more tolerant to increased training rates [34].
ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³qğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =W2
ğ‘ğ‘›ğ‘œğ‘Ÿğ‘š(ğœ™(W1
ğ‘qğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’)),
pğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =W2ğ‘ğ‘›ğ‘œğ‘Ÿğ‘š(ğœ™(W1ğ‘pğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’)),
nğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =W2ğ‘ğ‘›ğ‘œğ‘Ÿğ‘š(ğœ™(W1ğ‘nğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’)),(8)
where qğ‘šğ‘œğ‘‘ğ‘’ğ‘™âˆˆRğ‘‘â€²,pğ‘šğ‘œğ‘‘ğ‘’ğ‘™âˆˆRğ‘‘â€², and nğ‘šğ‘œğ‘‘ğ‘’ğ‘™âˆˆRğ‘‘â€²are the
projected vectors ( ğ‘‘â€²is the dimension of the output of projector),
WÂ·
ğ‘andWÂ·ğ‘are the trainable weight matrices, ğ‘›ğ‘œğ‘Ÿğ‘š(Â·)denotes
batch normalization [34], and ğœ™(Â·)is theğ‘™ğ‘’ğ‘ğ‘˜ğ‘¦ _ğ‘Ÿğ‘’ğ‘™ğ‘¢ function [53].
Momentum Update Mechanism with Memory Bank : As men-
tioned in Section 1, it is important to consider large-scale negative
samples in contrastive learning for representations of changesets.
To account for this, we use memory bank [ 81] to store rich change-
sets obtained from different batches for later contrast. In particular,
we build the key model for encoder and projector networks of
changesets based on the momentum contrastive learning mecha-
nism proposed by He et. al. [28]. The parameters of the query model
ğœƒğ‘, are updated by back-propagation, while the parameters of the
key modelğœƒğ‘˜are momentum updated as follows:
ğœƒğ‘˜â†ğ‘šğœƒğ‘˜+(1âˆ’ğ‘š)ğœƒğ‘, (9)
whereğ‘šâˆˆ[0,1)is a pre-defined momentum coefficient, which is set
as0.999in our experiment. As proved in the previous study [ 28], a
relatively large momentum works much better than a smaller value
suggesting that a slowly evolving key model is core to making use
of the memory bank. For per mini-batch, we use average pooling
and enqueue the latest negative samples into the memory bank and
dequeue the oldest negative samples.
5.3 Similarity Estimation
As mentioned before, the lexical similarity between bug reports
and program changesets like the same application programming
interfaces is also crucial for retrieval besides semantic similarity. In
this paper, we use the hierarchical contrastive loss to leverage the
lower feature-level similarity, higher model-level similarity, and
broader bank-level similarity for matching the bug report with
relevant changesets. We get the positive feature-level similarity ğ‘ ğ‘“+
by calculating cosine similarity between ğ‘ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ andğ‘ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ , the
negative feature-level similarity ğ‘ ğ‘“âˆ’by calculating cosine similarity
betweenğ‘ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ andğ‘›ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ , the positive model-level similarity
ğ‘ ğ‘š+by calculating cosine similarity between ğ‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ andğ‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ,
and the negative model-level similarity ğ‘ ğ‘šâˆ’by calculating cosine
similarity between ğ‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ andğ‘›ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ . Specifically, we calculate
the positive bank-level similarity ğ‘ ğ‘+as cosine similarity between
qğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦ andpğ‘˜ğ‘’ğ‘¦, and the negative bank-level similarity ğ‘ ğ‘âˆ’
ğ‘–(ğ‘–âˆˆ
{1,2,...,ğ¾}) as cosine similarity between qğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦ andğ‘–-th negative
sample nğ‘–
ğ‘˜ğ‘’ğ‘¦of the memory bank ( ğ¾is the size of memory bank).ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yali Du and Zhongxing Yu
Bug Report
(Anchor)
Source Code
(Positive)
Source Code
(Negative)BERT 
Parameter -Shared
Linear
LBelu
BN
LinearLinear
LBelu
BN
LinearLinear
LBelu
BN
LinearParameter -Shared
SemanticCodeBERTEncoder Projector 
Source Code
(Negative)Source Code
(Positive)
Momentum Update ParametersKey Model
Memory Bankâ€¦ğ’’feature
ğ’‘feature
ğ’featureğ’’ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
ğ’‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
ğ’ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ğ’’ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦
ğ’‘ğ‘˜ğ‘’ğ‘¦ ğ’ğ‘˜ğ‘’ğ‘¦ğ’1ğ‘˜ğ‘’ğ‘¦ğ’2ğ‘˜ğ‘’ğ‘¦ğ’3ğ‘˜ğ‘’ğ‘¦ğ’ğ¾ğ‘˜ğ‘’ğ‘¦â„’ğ‘“ â„’ğ‘šâ„’ğ‘
Feature -Level
Contrastive LossModel -Level
Contrastive LossBank -Level
Contrastive Loss
Figure 4: An overview of the Hierarchical Momentum Contrastive Bug Localization technique (HMCBL).
We adopt InfoNCE [ 58], a form of contrastive loss functions, as
our objective function for contrastive matching. The feature-level
contrastive loss is formulated as follows:
Lğ‘“=âˆ’ğ‘™ğ‘œğ‘”ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘“+/ğ›¾)
ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘“+/ğ›¾)+ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘“âˆ’/ğ›¾). (10)
The model-level contrastive loss is formulated as follows:
Lğ‘š=âˆ’ğ‘™ğ‘œğ‘”ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘š+/ğ›¾)
ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘š+/ğ›¾)+ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘šâˆ’/ğ›¾). (11)
The bank-level contrastive loss is formulated as follows:
Lğ‘=âˆ’ğ‘™ğ‘œğ‘”ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘+/ğ›¾)
ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘+/ğ›¾)+ğ¾Ã
ğ‘˜=1ğ‘’ğ‘¥ğ‘(ğ‘ ğ‘âˆ’
ğ‘–/ğ›¾), (12)
whereğ¾is the size of the memory bank and ğ›¾is a temperature
hyper-parameter that is set to be 0.07 in our experiment. Thus, the
overall objective function is L:
L=ğ›¼ğ‘“Lğ‘“+ğ›¼ğ‘šLğ‘š+ğ›¼ğ‘Lğ‘, (13)
whereğ›¼ğ‘“,ğ›¼ğ‘š, andğ›¼ğ‘are three hyper-parameters to balance the
feature-level, model-level, and bank-level contrasts.
5.4 Offline Indexing And Retrieval
After fine-tuning the model on a project-specific dataset, we re-
sort to the offline indexing and retrieval methods proposed by
Ciborowska et. al. [17]. All encoded changesets are stored in IVFPQ
(InVert File with Product Quantization) index. The IVFPQ index is
implemented using the Faiss library [36], which uses the k-means
algorithm to partition the embedding space into programmed par-
titions and assign each embedding to its nearest cluster. In the
retrieval process, the query bug report is first located to the near-
est partitionâ€™s centroid, and then the nearest instance within the
partition is discovered. For each query bug report, we can identify
theğ‘â€²most similar changesets across all ğ‘changesets stored in
the IVFPQ index. Therefore, we only re-rank the top- ğ‘â€²subset as
the candidate changesets to produce the final ranking.Table 1: Six projects used for evaluation.
Dataset BugsChangesets
Commits Files Hunks
AspectJ 200 2,939 14,030 23,446
JDT 94 13,860 58,619 150,630
PDE 60 9,419 42,303 100,373
SWT 90 10,206 25,666 69,833
Tomcat 193 10,034 30,866 72,134
ZXing 20 843 2,846 6,165
6 EXPERIMENTAL EVALUATION
6.1 Dataset
The SemanticCodeBERT is trained using all the Java corpus in Code-
SearchNet [ 33], and we provide the weights and the guidance to
fine-tune the pre-trained model for downstream tasks. To evalu-
ate our bug localization technique, we use the dataset separated
by Ciborowska et. al. [17] from the manually validated dataset by
Ming et. al. [78]. The dataset includes six software projects, termed
AspectJ, JDT, PDE, SWT, Tomcat, and ZXing, as shown in Table 1.
To explore the impact of the granularity of changeset data, the
bug-inducing changeset is further divided into file-level and hunk-
level code changes. Thus, one bug report can have multiple pairs
with files or hunks from the original inducing changes. In total, we
consider three different granularities: commits, files, and hunks.
6.2 Evaluation Metrics
A set of metrics commonly used to evaluate the performance of in-
formation retrieval systems are applied to evaluate the performance
of different models.
Precision@K ( ğ‘ƒ@ğ¾):ğ‘ƒ@ğ¾evaluates how many of the top- ğ¾
changesets in a ranking are relevant to the bug report, which is
equal to the number of the relevant changesets | ğ‘…ğ‘’ğ‘™ğµğ‘–| located inPre-training Code Representation with Semantic Flow Graph for Effective Bug Localization ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
the top-ğ¾position in the ranking averaged across ğµbug reports:
ğ‘ƒ@ğ¾=1
|ğµ||ğµ|âˆ‘ï¸
ğ‘–=1|ğ‘…ğ‘’ğ‘™ğµğ‘–|
ğ¾. (14)
Mean Average Precision ( ğ‘€ğ´ğ‘ƒ ):ğ‘€ğ´ğ‘ƒ quantifies the ability of
a model to locate all changesets relevant to a bug report. ğ‘€ğ´ğ‘ƒ is
calculated as the mean of ğ´ğ‘£ğ‘”ğ‘ƒ (average precision) of ğµbug reports.
ğ´ğ‘£ğ‘”ğ‘ƒ =ğ‘€âˆ‘ï¸
ğ‘—=1ğ‘ƒ@ğ‘—Ã—ğ‘ğ‘œğ‘ (ğ‘—)
ğ‘. (15)
ğ‘€ğ´ğ‘ƒ =1
|ğµ||ğµ|âˆ‘ï¸
ğ‘–=11
ğ´ğ‘£ğ‘”ğ‘ƒğµğ‘–, (16)
whereğ‘—is the rank,ğ‘€is the number of retrieved changesets, ğ‘ğ‘œğ‘ (ğ‘—)
denotes whether ğ‘—-th changeset is relevant to the bug report, ğ‘is
the total number of bug reports relevant to changesets, ğ‘ƒ@ğ‘—is the
precision of top- ğ‘—position in the ranking of this retrieval, and ğµğ‘–is
theğ‘–-th bug report.
Mean Reciprocal Rank ( ğ‘€ğ‘…ğ‘…):ğ‘€ğ‘…ğ‘… quantifies the ability of a
model to locate the first relevant changeset to a bug report, and is
calculated as the average of reciprocal ranks across ğµbug reports.
1stğ‘…ğ‘ğ‘›ğ‘˜ğµğ‘–is the reciprocal rank of ğ‘–-th bug report, which is the
inverted rank of the first relevant changeset in the ranking:
ğ‘€ğ‘…ğ‘… =1
|ğµ||ğµ|âˆ‘ï¸
ğ‘–=11
1ğ‘ ğ‘¡ğ‘…ğ‘ğ‘›ğ‘˜ğµğ‘–. (17)
6.3 Experimental Setup
Configurations of Pre-training Tasks : The SemanticCodeBERT
is pre-trained on NVIDIA Tesla A100 with 128GB RAM on the
Ubuntu system. The Adam optimizer [ 49] is used to update model
parameters with batch size 80 and learning rate 1E-04. To accelerate
the training process, the parameters of GraphCodeBERT [ 25] are
used to initialize the pre-training model. The model is trained with
600K batches and costs about 156 hours.
Configurations of Bug Localization : The first half of the projectâ€™s
pairs of bug reports and bug-inducing changesets, ordered by bug
opening date, are selected as the training dataset, and the remaining
half is left as the test dataset. The experiments are implemented
with GPU support. The Adam optimizer [ 49] is used to update
model parameters with learning rate 3E-05. All bug reports and
changesets are truncated or padded to their respective length limit.
According to the experimental verification, we set the trade-off
hyper-parameters ğ›¼ğ‘“,ğ›¼ğ‘š, andğ›¼ğ‘as1,1, and 1, respectively.
Changeset Encoding Strategies : Changesets are time-ordered
sequences recording the softwareâ€™s evolution over time. We build
upon the three changeset encoding strategies ( ğ·-encoding,ğ´ğ‘…ğ¶-
encoding, and ğ´ğ‘…ğ¶ğ¿-encoding) proposed by Ciborowska et. al. [17]
to encode changesets. ğ·-encoding does not utilize specific char-
acteristics of changeset lines. ğ´ğ‘…ğ¶ -encoding divides the lines into
three groups with three unique tokens. ğ´ğ‘…ğ¶ğ¿-encoding instead
does not group the lines and maintains the ordering of lines withina changeset. These three strategies are based on the output of
the git diffcommand, which divides changeset lines into three
kinds: added lines, removed lines, and unchanged lines. All code
sequences are preprocessed by filtering the intrusive characters
(e.g.,docstrings, comments) from the original code tokens.
6.4 Retrieval Performance
We compare the performance of our proposed model with the tra-
ditional bug localization tool, state-of-the-art changeset-based bug
localization approach, and two recent state-of-the-art pre-trained
models with the HMCBL framework.
â€¢BLUiR [70]: A structured IR-based fault localization tool, which
builds AST to extract the program constructs of each source
code file and utilizes Okapi BM25 [ 68] to calculate the similarity
between the bug report and the candidate changesets.
â€¢FBL-BERT [17]: The state-of-the-art approach for automatically
retrieving bug-inducing changesets given a bug report, which
uses the popular BERT model to more accurately match the se-
mantics in the bug report text with the bug-inducing changesets.
â€¢GraphCodeBERT [25]: A pre-trained model that considers data
flow to better encode the relation between variables.
â€¢UniXcoder [24]: An unified cross-modal pre-trained model,
which leverages cross-modal information like Abstract Syntax
Tree and comments to enhance code representation.
For BLUiR, we fully follow the original technical description
in [70] (as no open-source implementation is available) to get the
results for the evaluation metrics. For FBL-BERT, we use the exper-
imental results provided in [ 17]. For GraphCodeBERT and UniX-
coder, we get the results by replacing the pre-trained model Se-
manticCodeBERT within the HMCBL framework respectively with
GraphCodeBERT and UniXcoder (keeping other configurations the
same). Table 2 shows the retrieval performances of different models
with different changeset encoding strategies ( i.e.,ğ·-,ğ´ğ‘…ğ¶- and
ğ´ğ‘…ğ¶ğ¿- encoding) and three granularities ( i.e.ğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡ğ‘ âˆ’,ğ¹ğ‘–ğ‘™ğ‘’ğ‘ âˆ’
andğ»ğ‘¢ğ‘›ğ‘˜ğ‘ âˆ’level) on six projects. Limited by space, the best result
of the three encoding strategies is shown for each configuration.
The following observations can be obtained from the figure.
First, compared with the traditional bug localization method which
relies on more direct term matching between a bug report and a
changeset, the neural network methods perform better by obtaining
semantic representations for the calculation of similarity. Second,
our proposed method outperforms the state-of-the-art method (FBL-
BERT) by a clear margin. In particular, our proposed bug localization
technique improves FBL-BERT by 140.78% to 188.79% in terms
of MRR on six projects with ğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡ğ‘ âˆ’level granularity. Third,
compared with GraphCodeBERT and UniXcoder, our model using
SemanticCodeBERT as a changeset encoder consistently achieves
better performance in almost all experimental configurations. This
suggests that the proposed Semantic Flow Graph (SFG) captures
good code semantics, and the proposed framework contributes to
changeset-based bug localization.
The Studentâ€™s t-test is conducted between our technique and
other baselines, and the results show that the improvements are
significant with p < 0.01. We additionally observe that with the
ğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡ğ‘ âˆ’level granularity, the obtained improvement is more sig-
nificant than the other two granularities ( ğ¹ğ‘–ğ‘™ğ‘’ğ‘ âˆ’level andğ»ğ‘¢ğ‘›ğ‘˜ğ‘ âˆ’ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yali Du and Zhongxing Yu
Table 2: Retrieval performance of different models.
Projects Techniqueğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡ğ‘ âˆ’ ğ¹ğ‘–ğ‘™ğ‘’ğ‘ âˆ’ ğ»ğ‘¢ğ‘›ğ‘˜ğ‘ âˆ’
MRR MAP P@1 P@3 P@5 MRR MAP P@1 P@3 P@5 MRR MAP P@1 P@3 P@5
ZXingBLUiR 0.077 0.016 0.071 0.024 0.014 0.073 0.023 0.000 0.024 0.014 0.056 0.035 0.000 0.071 0.086
FBL-BERT 0.155 0.061 0.100 0.133 0.120 0.212 0.163 0.100 0.133 0.220 0.328 0.210 0.200 0.233 0.240
GraphCodeBERT 0.189 0.118 0.143 0.143 0.118 0.280 0.155 0.214 0.143 0.214 0.346 0.118 0.225 0.111 0.067
UniXcoder 0.354 0.167 0.414 0.171 0.120 0.359 0.143 0.333 0.224 0.200 0.331 0.164 0.214 0.261 0.282
Ours 0.439 0.226 0.429 0.250 0.225 0.421 0.185 0.357 0.226 0.271 0.422 0.212 0.333 0.444 0.400
PDEBLUiR 0.009 0.001 0.000 0.000 0.000 0.018 0.003 0.000 0.008 0.005 0.024 0.005 0.000 0.008 0.010
FBL-BERT 0.103 0.013 0.067 0.033 0.027 0.260 0.079 0.167 0.128 0.151 0.288 0.093 0.200 0.144 0.127
GraphCodeBERT 0.180 0.042 0.142 0.087 0.058 0.264 0.094 0.167 0.129 0.148 0.284 0.074 0.206 0.124 0.129
UniXcoder 0.178 0.029 0.095 0.063 0.072 0.267 0.090 0.167 0.135 0.129 0.289 0.102 0.212 0.144 0.129
Ours 0.248 0.045 0.190 0.103 0.076 0.274 0.095 0.214 0.137 0.160 0.294 0.134 0.286 0.182 0.160
AspectJBLUiR 0.016 0.013 0.007 0.014 0.015 0.098 0.065 0.028 0.076 0.108 0.086 0.048 0.007 0.017 0.159
FBL-BERT 0.107 0.061 0.058 0.080 0.083 0.176 0.085 0.154 0.095 0.097 0.183 0.093 0.173 0.111 0.099
GraphCodeBERT 0.172 0.065 0.167 0.065 0.060 0.178 0.071 0.167 0.065 0.060 0.188 0.086 0.167 0.120 0.116
UniXcoder 0.270 0.148 0.245 0.160 0.158 0.209 0.119 0.167 0.140 0.152 0.250 0.134 0.250 0.150 0.138
Ours 0.309 0.169 0.278 0.198 0.196 0.272 0.148 0.250 0.157 0.146 0.262 0.143 0.250 0.161 0.163
JDTBLUiR 0.019 0.001 0.015 0.005 0.003 0.027 0.003 0.000 0.010 0.012 0.033 0.005 0.000 0.005 0.009
FBL-BERT 0.118 0.016 0.064 0.043 0.030 0.403 0.060 0.319 0.184 0.128 0.429 0.062 0.319 0.195 0.167
GraphCodeBERT 0.125 0.022 0.061 0.035 0.030 0.423 0.058 0.308 0.179 0.118 0.385 0.041 0.231 0.179 0.118
UniXcoder 0.182 0.018 0.182 0.061 0.038 0.434 0.062 0.379 0.166 0.131 0.364 0.045 0.288 0.182 0.123
Ours 0.306 0.026 0.288 0.096 0.064 0.489 0.080 0.462 0.195 0.167 0.443 0.088 0.322 0.206 0.167
SWTBLUiR 0.005 0.001 0.000 0.000 0.000 0.020 0.003 0.016 0.005 0.006 0.014 0.001 0.000 0.000 0.013
FBL-BERT 0.067 0.015 0.023 0.027 0.026 0.555 0.131 0.535 0.233 0.173 0.526 0.131 0.488 0.217 0.164
GraphCodeBERT 0.105 0.018 0.048 0.026 0.022 0.535 0.137 0.525 0.220 0.175 0.536 0.132 0.516 0.220 0.159
UniXcoder 0.129 0.035 0.107 0.106 0.063 0.548 0.149 0.524 0.233 0.183 0.535 0.143 0.535 0.205 0.179
Ours 0.283 0.085 0.159 0.177 0.170 0.560 0.153 0.540 0.249 0.192 0.540 0.147 0.540 0.228 0.179
TomcatBLUiR 0.007 0.002 0.000 0.002 0.002 0.014 0.003 0.000 0.010 0.007 0.014 0.005 0.000 0.012 0.013
FBL-BERT 0.141 0.055 0.062 0.077 0.088 0.463 0.114 0.381 0.222 0.183 0.482 0.129 0.412 0.216 0.182
GraphCodeBERT 0.253 0.062 0.188 0.104 0.084 0.287 0.067 0.271 0.104 0.080 0.395 0.118 0.363 0.216 0.211
UniXcoder 0.328 0.057 0.338 0.120 0.084 0.364 0.065 0.353 0.125 0.085 0.396 0.097 0.378 0.139 0.118
Ours 0.386 0.073 0.360 0.135 0.107 0.487 0.122 0.406 0.247 0.232 0.484 0.132 0.423 0.225 0.211
Table 3: Ablation study of pre-training tasks of Semantic-
CodeBERT with Semantic Flow Graph (SFG).
Dataset Pre-training Tasks MRR MAP P@1 P@3 P@5
ZXing-w/ 0.189 0.118 0.143 0.143 0.118
-w/ N.& E. 0.372 0.102 0.333 0.111 0.067
-w/ N.& E.& T.& R. 0.439 0.226 0.429 0.250 0.225
PDE-w/ 0.180 0.042 0.142 0.087 0.058
-w/ N.& E. 0.219 0.032 0.143 0.076 0.072
-w/ N.& E.& T.& R. 0.248 0.045 0.190 0.103 0.076
AspectJ-w/ 0.172 0.065 0.167 0.065 0.060
-w/ N.& E. 0.289 0.158 0.250 0.184 0.170
-w/ N.& E.& T.& R. 0.309 0.169 0.278 0.198 0.196
JDT-w/ 0.125 0.022 0.061 0.035 0.030
-w/ N.& E. 0.139 0.021 0.095 0.044 0.048
-w/ N.& E.& T.& R. 0.306 0.026 0.288 0.096 0.064
SWT-w/ 0.105 0.018 0.048 0.026 0.022
-w/ N.& E. 0.197 0.058 0.063 0.085 0.141
-w/ N.& E.& T.& R. 0.283 0.085 0.159 0.177 0.170
Tomcat-w/ 0.253 0.062 0.188 0.104 0.084
-w/ N.& E. 0.300 0.048 0.346 0.113 0.077
-w/ N.& E.& T.& R. 0.386 0.073 0.360 0.135 0.107
level). It can be attributed that the undivided bug-inducing change-
set carries enriched semantic information which can be captured
by SemanticCodeBERT. This again confirms the effectiveness of
the SemanticCodeBERT-based bug localization technique.6.5 Ablation Study
To evaluate the design choices in the proposed model, we con-
duct several ablation studies. To begin with, as shown in Table 3,
we analyze the contributions of node alignment, edge prediction,
type prediction, and role prediction pre-training tasks on the six
projects with commits granularity. N., E., T., and R. denote the Node
Alignment, Edge Prediction, Type Prediction, and Role Prediction
pre-training tasks, respectively. With all of these pre-training tasks,
we train SemanticCodeBert according to the proposed new code
representation SFG. According to the results, after adding Type and
Role Prediction pre-training tasks, the obtained performance has
universally improved. This result suggests that leveraging the node
attributes (type and role) is vital to learn code representation.
Furthermore, we evaluate the effectiveness of the Hierarchical
Momentum Contrastive Bug Localization (HMCBL) technique on
the six projects with commits granularity. As illustrated in Table 4,
for -w/o HMCBL, the memory bank and hierarchical contrastive
loss which leverages similarities at different levels do not exist,
and only the representation obtained by the encoder is utilized to
calculate similarity.
To demonstrate the generality, the technique is evaluated with
different pre-training models as the encoder of the changeset, in-
cluding BERT, GraphCodeBERT, and SemanticCodeBERT. It is ob-
served that overall much better performance will be obtained with
hierarchical momentum contrastive learning, which provides large-
scale negative sample interactions for representation learning andPre-training Code Representation with Semantic Flow Graph for Effective Bug Localization ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
Table 4: Ablation study of Hierarchical Momentum Con-
trastive Bug Localization (HMCBL) technique, where
GCBERT and SCBERT are short of GraphCodeBERT and
SemanticCodeBERT.
Technique Dataset MRR MAP P@1 P@3 P@5
BERT -w/o
HMCBL
(FBL-BERT )ZXing 0.155 0.061 0.100 0.133 0.120
PDE 0.103 0.013 0.067 0.033 0.027
AspectJ 0.107 0.061 0.058 0.080 0.083
JDT 0.118 0.016 0.064 0.043 0.030
SWT 0.067 0.015 0.023 0.027 0.026
Tomcat 0.141 0.055 0.062 0.077 0.088
GCBERT -w/o
HMCBLZXing 0.162 0.106 0.143 0.095 0.086
PDE 0.167 0.018 0.119 0.071 0.045
AspectJ 0.123 0.067 0.076 0.073 0.084
JDT 0.120 0.022 0.061 0.035 0.036
SWT 0.090 0.019 0.048 0.021 0.022
Tomcat 0.151 0.035 0.059 0.064 0.063
SCBERT -w/o
HMCBLZXing 0.222 0.112 0.143 0.190 0.150
PDE 0.230 0.049 0.142 0.095 0.069
AspectJ 0.271 0.148 0.250 0.161 0.165
JDT 0.217 0.051 0.136 0.111 0.091
SWT 0.250 0.062 0.095 0.167 0.185
Tomcat 0.285 0.053 0.265 0.092 0.069
BERT -w/
HMCBLZXing 0.179 0.040 0.143 0.095 0.061
PDE 0.156 0.032 0.119 0.063 0.051
AspectJ 0.162 0.097 0.118 0.141 0.149
JDT 0.128 0.017 0.030 0.070 0.100
SWT 0.082 0.013 0.048 0.024 0.021
Tomcat 0.235 0.055 0.169 0.098 0.096
GCBERT -w/
HMCBLZXing 0.189 0.118 0.143 0.143 0.118
PDE 0.180 0.042 0.142 0.087 0.058
AspectJ 0.172 0.065 0.167 0.065 0.060
JDT 0.125 0.022 0.061 0.035 0.030
SWT 0.105 0.018 0.048 0.026 0.022
Tomcat 0.253 0.062 0.188 0.104 0.084
SCBERT -w/
HMCBLZXing 0.439 0.226 0.429 0.250 0.225
PDE 0.248 0.045 0.190 0.103 0.076
AspectJ 0.309 0.169 0.278 0.198 0.196
JDT 0.306 0.026 0.288 0.096 0.064
SWT 0.283 0.085 0.159 0.177 0.170
Tomcat 0.386 0.073 0.360 0.135 0.107
increases retrieval accuracy. For instance, compared with BERT
-w/o HMCBL, which is the FBL-BERT exactly, BERT -w/ HMCBL
improves the performance in terms of MRR scores for more than
80% projects by 15.48% to 66.67%. It is indicative of the observation
that the hierarchical momentum contrastive bug localization tech-
nique can be extended as a general and effective framework with
different advanced pre-training models.
6.6 Threats to Validity
Our results should be interpreted with several threats to validity
in mind. As bug-inducing changes are identified using the SZZ
algorithm [ 69], one threat to the internal validity of the results is
possible noise introduced by SZZ may make the mapping between
bug reports and bug-inducing changesets not very precise. However,
the dataset used in the study has been validated manually [ 78], so
this threat is minimized. Another threat to internal validity is the
dataset may contain tangled changes [ 76]. While we do believetangled changes can affect our results, the dataset has been widely
used for changeset-based bug localization studies [ 17,78], and
removing tangled changes completely is extraordinarily difficult.
With regard to threats to external validity, one potential issue
is that the evaluation is conducted on a limited number of bugs
from several open-source projects. However, these projects feature
various purposes and development styles. Also, the dataset can be
considered as the de-facto evaluation target for changeset-based bug
localization studies and prior studies have widely used it [17, 78].
7 CONCLUSION
We aim to advance the state-of-the-art BERT-based bug localization
techniques in this paper, which currently suffer from two issues:
the pre-trained BERT models on source code are not robust enough
to capture code semantics and the overall bug localization models
neglect the necessity of large-scale negative samples in contrastive
learning and ignore the lexical similarity between bug reports and
changesets. To address these two issues, we 1) propose a novel
directed, multiple-label Semantic Flow Graph (SFG), which com-
pactly and adequately captures code semantics, 2) design and train
SemanticCodeBERT on the basis of SFG, and 3) design a novel
Hierarchical Momentum Contrastive Bug Localization technique
(HMCBL). Evaluation results confirm that our method achieves
state-of-the-art performance.
8 DATA AVAILABILITY
Our replication package (including code, model, etc.) is publicly
available at https://github.com/duyali2000/SemanticFlowGraph.
ACKNOWLEDGMENTS
We are grateful to the anonymous ESEC/FSE reviewers for their
valuable feedback on this work. This work was partially supported
by National Natural Science Foundation of China (Grant No. 62102233),
Shandong Province Overseas Outstanding Youth Fund (Grant No.
2022HWYQ-043), and Qilu Young Scholar Program of Shandong
University.
REFERENCES
[1]Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2014. Learn-
ing Natural Coding Conventions. In Proceedings of the 22nd ACM SIGSOFT In-
ternational Symposium on Foundations of Software Engineering . Association for
Computing Machinery, 281â€“293.
[2]Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv.
(2018).
[3]Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations .
[4]Miltiadis Allamanis and Charles Sutton. 2014. Mining Idioms from Source Code.
InProceedings of the 22nd ACM SIGSOFT International Symposium on Foundations
of Software Engineering . Association for Computing Machinery, 472â€“483.
[5]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A General Path-
Based Representation for Predicting Program Properties. SIGPLAN Not. 53, 4 (jun
2018), 404â€“419.
[6]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. Code2vec: Learn-
ing Distributed Representations of Code. Proc. ACM Program. Lang. POPL (2019),
29 pages.
[7]Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. [n. d.]. Neural Code
Comprehension: A Learnable Representation of Code Semantics. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems
(MontrÃ©al, Canada) (NIPSâ€™18) . 3589â€“3601.
[8]Benjamin Bichsel, Veselin Raychev, Petar Tsankov, and Martin Vechev. [n. d.].
Statistical Deobfuscation of Android Applications. In Proceedings of the 2016 ACMESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA Yali Du and Zhongxing Yu
SIGSAC Conference on Computer and Communications Security (Vienna, Austria)
(CCS â€™16) . 343â€“355.
[9]Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised Contrastive
Learning for Code Retrieval and Summarization via Semantic-Preserving Trans-
formations. In The 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval, Virtual Event . ACM, 511â€“521.
[10] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng,
Gaetano Rossiello, Alessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang,
et al.2020. Exploring software naturalness through neural language models.
arXiv preprint arXiv:2006.12641 (2020).
[11] Xianshuai Cao, Yuliang Shi, Jihu Wang, Han Yu, Xinjun Wang, and Zhongmin Yan.
2022. Cross-modal Knowledge Graph Contrastive Learning for Machine Learning
Method Recommendation. In MM â€™22: The 30th ACM International Conference on
Multimedia, Lisboa, Portugal, October 10 - 14, 2022 . ACM, 3694â€“3702.
[12] Kwonsoo Chae, Hakjoo Oh, Kihong Heo, and Hongseok Yang. 2017. Automati-
cally Generating Features for Learning Program Analysis Heuristics for C-like
Languages. Proc. ACM Program. Lang. OOPSLA (2017), 25 pages.
[13] Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan
Vasilescu, and Claire Le Goues. 2022. VarCLR: Variable Semantic Representation
Pre-training via Contrastive Learning. In 44th IEEE/ACM 44th International Con-
ference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022 .
ACM, 2327â€“2339.
[14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning . PMLR, 1597â€“1607.
[15] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. 2020. Improved
Baselines with Momentum Contrastive Learning. CoRR abs/2003.04297 (2020).
[16] Hyunsoo Cho, Jinseok Seol, and Sang-goo Lee. 2021. Masked Contrastive Learn-
ing for Anomaly Detection. In Proceedings of the Thirtieth International Joint
Conference on Artificial Intelligence, Virtual Event / Montreal . ijcai.org, 1434â€“1441.
[17] Agnieszka Ciborowska and Kostadin Damevski. 2022. Fast changeset-based
bug localization with BERT. In 2022 IEEE/ACM 44th International Conference on
Software Engineering (ICSE) . IEEE, 946â€“957.
[18] Agnieszka Ciborowska, Michael J Decker, and Kostadin Damevski. 2022. On-
line Adaptable Bug Localization for Rapidly Evolving Software. arXiv preprint
arXiv:2203.03544 (2022).
[19] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. 2017.
End-to-End Deep Learning of Optimization Heuristics. In 2017 26th International
Conference on Parallel Architectures and Compilation Techniques (PACT) . 219â€“232.
[20] Yaniv David, Uri Alon, and Eran Yahav. 2020. Neural reverse engineering of
stripped binaries using augmented control flow graphs. Proceedings of the ACM
on Programming Languages 4, OOPSLA (2020), 1â€“28.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
[22] Yali Du, Yinwei Wei, Wei Ji, Fan Liu, Xin Luo, and Liqiang Nie. 2023. Multi-queue
Momentum Contrast for Microvideo-Product Retrieval. In Proceedings of the
Sixteenth ACM International Conference on Web Search and Data Mining, WSDM
2023, Singapore, 27 February 2023 - 3 March 2023 , Tat-Seng Chua, Hady W. Lauw,
Luo Si, Evimaria Terzi, and Panayiotis Tsaparas (Eds.). ACM, 1003â€“1011.
[23] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. Codebert: A pre-trained
model for programming and natural languages. arXiv preprint arXiv:2002.08155
(2020).
[24] Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022.
UniXcoder: Unified Cross-Modal Pre-training for Code Representation. arXiv
preprint arXiv:2203.03850 (2022).
[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al .2020. Graphcodebert:
Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366
(2020).
[26] Jin Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically Enhanced
Software Traceability Using Deep Learning Techniques. In 2017 IEEE/ACM 39th
International Conference on Software Engineering (ICSE) . 3â€“14.
[27] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence . AAAI Press, 1345â€“1351.
[28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-
mentum contrast for unsupervised visual representation learning. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition . 9729â€“9738.
[29] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
Conference on Software Engineering . IEEE Press, 837â€“847.
[30] Xing Hu, Yuhan Wei, Ge Li, and Zhi Jin. 2017. CodeSum: Translate Program
Language to Natural Language.
[31] Xuan Huo, Ming Li, and Zhi-Hua Zhou. 2020. Control Flow Graph Embedding
Based on Multi-Instance Decomposition for Bug Localization. In The Thirty-
Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The TenthAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020,
New York, NY, USA, February 7-12, 2020 . AAAI Press, 4223â€“4230.
[32] Xuan Huo, Ferdian Thung, Ming Li, David Lo, and Shu-Ting Shi. 2019. Deep
transfer bug localization. IEEE Transactions on software engineering 47, 7 (2019),
1368â€“1380.
[33] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).
[34] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In International conference
on machine learning . PMLR, 448â€“456.
[35] Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. 2021. Treebert:
A tree-based pre-trained model for programming language. In Uncertainty in
Artificial Intelligence . PMLR, 54â€“63.
[36] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity
search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535â€“547.
[37] James A Jones and Mary Jean Harrold. 2005. Empirical evaluation of the taran-
tula automatic fault-localization technique. In Proceedings of the 20th IEEE/ACM
international Conference on Automated software engineering . 273â€“282.
[38] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2019. Pre-
trained contextual embedding of source code. (2019).
[39] Minguk Kang and Jaesik Park. 2020. ContraGAN: Contrastive Learning for
Conditional Image Generation. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .
[40] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
search via contextualized late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research and development in Information
Retrieval . 39â€“48.
[41] Dongsun Kim, Yida Tao, Sunghun Kim, and Andreas Zeller. 2013. Where should
we fix this bug? a two-phase recommendation model. IEEE transactions on
software Engineering 39, 11 (2013), 1597â€“1610.
[42] Kisub Kim, Sankalp Ghatpande, Kui Liu, Anil Koyuncu, Dongsun
Kim, TegawendÃ© F BissyandÃ©, Jacques Klein, and Yves Le Traon. 2022.
DigBugâ€”Pre/post-processing operator selection for accurate bug localization.
Journal of Systems and Software 189 (2022), 111300.
[43] Ted Kremenek, Andrew Y. Ng, and Dawson Engler. 2007. A Factor Graph Model
for Software Bug Finding. In Proceedings of the 20th International Joint Confer-
ence on Artifical Intelligence (Hyderabad, India) (IJCAIâ€™07) . Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 2510â€“2516.
[44] An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2017.
Bug localization with combination of deep learning and information retrieval. In
2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC) .
IEEE, 218â€“229.
[45] Hao Lan, Li Chen, and Baochun Li. 2021. Accelerated Device Placement Opti-
mization with Contrastive Learning. In ICPP 2021: 50th International Conference
on Parallel Processing, Lemont, IL, USA, August 9 - 12, 2021 . ACM, 72:1â€“72:10.
[46] Zhengliang Li, Zhiwei Jiang, Xiang Chen, Kaibo Cao, and Qing Gu. 2021. Laprob:
a label propagation-based software bug localization method. Information and
Software Technology 130 (2021), 106410.
[47] Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2021.
Traceability Transformed: Generating More Accurate Links with Pre-Trained
BERT Models. In Proceedings of the 43rd International Conference on Software
Engineering . IEEE Press, 324â€“335.
[48] Chao Liu, Xifeng Yan, Long Fei, Jiawei Han, and Samuel P Midkiff. 2005. SOBER:
statistical model-based bug localization. ACM SIGSOFT Software Engineering
Notes 30, 5 (2005), 286â€“295.
[49] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regulariza-
tion. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019 . OpenReview.net.
[50] Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu Zhang, and
Lingming Zhang. 2021. Boosting coverage-based fault localization via graph-
based representation learning. In Proceedings of the 29th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering . 664â€“676.
[51] Yi-Fan Ma and Ming Li. 2022. The flowing nature matters: feature learning from
the control flow graph of source code for bug localization. Mach. Learn. 111, 3
(2022), 853â€“870.
[52] Yi-Fan Ma and Ming Li. 2022. Learning from the Multi-Level Abstraction of the
Control Flow Graph via Alternating Propagation for Bug Localization. In IEEE
International Conference on Data Mining, ICDM 2022, Orlando, FL, USA, November
28 - Dec. 1, 2022 , Xingquan Zhu, Sanjay Ranka, My T. Thai, Takashi Washio, and
Xindong Wu (Eds.). IEEE, 299â€“308.
[53] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al .2013. Rectifier nonlineari-
ties improve neural network acoustic models. In Proc. icml . Citeseer.
[54] Ginika Mahajan and Neha Chaudhary. 2022. Design and development of novel
hybrid optimization-based convolutional neural network for software bug local-
ization. Soft Computing (2022), 1â€“22.Pre-training Code Representation with Semantic Flow Graph for Effective Bug Localization ESEC/FSE â€™23, December 3â€“9, 2023, San Francisco, CA, USA
[55] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural
Networks over Tree Structures for Programming Language Processing. In Pro-
ceedings of the Thirtieth AAAI Conference on Artificial Intelligence . AAAI Press,
1287â€“1293.
[56] Vijayaraghavan Murali, Lee Gross, Rebecca Qian, and Satish Chandra. 2021.
Industry-scale IR-based bug localization: a perspective from Facebook. In 2021
IEEE/ACM 43rd International Conference on Software Engineering: Software Engi-
neering in Practice (ICSE-SEIP) . IEEE, 188â€“197.
[57] Chao Ni, Wei Wang, Kaiwen Yang, Xin Xia, Kui Liu, and David Lo. 2022. The
best of both worlds: integrating semantic features with expert features for defect
prediction and localization. In Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engineering .
672â€“683.
[58] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[59] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: mutation-based fault
localization. Software Testing, Verification and Reliability 25, 5-7 (2015), 605â€“628.
[60] Renaud Pawlak, Martin Monperrus, Nicolas Petitprez, Carlos Noguera, and Lionel
Seinturier. 2015. Spoon: A Library for Implementing Analyses and Transforma-
tions of Java Source Code. Software: Practice and Experience 46 (2015), 1155â€“1179.
[61] Michael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik
Meijer, and Satish Chandra. 2020. Scaffle: bug localization on millions of files.
InProceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis . 225â€“236.
[62] Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to Name-
Based Bug Detection. Proc. ACM Program. Lang. 2, OOPSLA (2018), 25 pages.
[63] Binhang Qi, Hailong Sun, Wei Yuan, Hongyu Zhang, and Xiangxin Meng. 2022.
DreamLoc: A Deep Relevance Matching-Based Framework for bug Localization.
IEEE Transactions on Reliability 71, 1 (2022), 235â€“249.
[64] Shibo Qi, Rize Jin, and Joon-Young Paik. 2022. eMoCo: Sentence Representa-
tion Learning With Enhanced Momentum Contrast. In Proceedings of the 5th
International Conference on Computer Science and Software Engineering . 159â€“163.
[65] Yiyue Qian, Yiming Zhang, Qianlong Wen, Yanfang Ye, and Chuxu Zhang. 2022.
Rep2Vec: Repository Embedding via Heterogeneous Graph Adversarial Con-
trastive Learning. In KDD â€™22: The 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022 . ACM,
1390â€“1400.
[66] Fangcheng Qiu, Meng Yan, Xin Xia, Xinyu Wang, Yuanrui Fan, Ahmed E Has-
san, and David Lo. 2020. JITO: a tool for just-in-time defect identification and
localization. In Proceedings of the 28th ACM joint meeting on european software
engineering conference and symposium on the foundations of software engineering .
1586â€“1590.
[67] Mohammad Masudur Rahman and Chanchal K Roy. 2018. Improving ir-based
bug localization with context-aware query reformulation. In Proceedings of the
2018 26th ACM joint meeting on European software engineering conference and
symposium on the foundations of software engineering . 621â€“632.
[68] Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu. 2000.
Experimentation as a way of life: Okapi at TREC. Inf. Process. Manag. 36, 1 (2000),
95â€“108.
[69] Giovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele
Bavota, Michele Lanza, and Rocco Oliveto. 2021. Evaluating SZZ Implementations
Through a Developer-Informed Oracle. In Proceedings of the 43rd International
Conference on Software Engineering . IEEE Press, 436â€“447.
[70] Ripon K Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E Perry. 2013.
Improving bug localization using structured information retrieval. In 2013 28th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 345â€“355.
[71] Shivkumar Shivaji, E James Whitehead, Ram Akella, and Sunghun Kim. 2012. Re-
ducing features to improve code change-based bug prediction. IEEE Transactions
on Software Engineering 39, 4 (2012), 552â€“569.
[72] Marius Smytzek and Andreas Zeller. 2022. SFLKit: a workbench for statistical fault
localization. In Proceedings of the 30th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering . 1701â€“
1705.
[73] Chenning Tao, Qi Zhan, Xing Hu, and Xin Xia. 2022. C4: contrastive cross-
language code clone detection. In Proceedings of the 30th IEEE/ACM International
Conference on Program Comprehension, Virtual Event . ACM, 413â€“424.
[74] Simon Urli, Zhongxing Yu, Lionel Seinturier, and Martin Monperrus. 2018. How
to design a program repair bot? insights from the repairnator project. In 2018
IEEE/ACM 40th international conference on software engineering: software engi-
neering in practice Track (ICSE-SEIP) . IEEE, 95â€“104.
[75] Iris Vessey. 1985. Expertise in debugging computer programs: A process analysis.
International Journal of Man-Machine Studies 23, 5 (1985), 459â€“494.[76] Min Wang, Zeqi Lin, Yanzhen Zou, and Bing Xie. 2020. CoRA: Decomposing
and Describing Tangled Code Changes for Reviewer. In Proceedings of the 34th
IEEE/ACM International Conference on Automated Software Engineering . IEEE
Press, 1050â€“1061.
[77] Xuanrun Wang, Kanglin Yin, Qianyu Ouyang, Xidao Wen, Shenglin Zhang,
Wenchi Zhang, Li Cao, Jiuxue Han, Xing Jin, and Dan Pei. 2022. Identifying
Erroneous Software Changes through Self-Supervised Contrastive Learning on
Time Series Data. In IEEE 33rd International Symposium on Software Reliability
Engineering, ISSRE 2022, Charlotte, NC, USA, October 31 - Nov. 3, 2022 . IEEE,
366â€“377.
[78] Ming Wen, Rongxin Wu, and Shing-Chi Cheung. 2016. Locus: Locating Bugs from
Software Changes. In Proceedings of the 31st IEEE/ACM International Conference on
Automated Software Engineering . Association for Computing Machinery, 262â€“273.
[79] W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A
survey on software fault localization. IEEE Transactions on Software Engineering
42, 8 (2016), 707â€“740.
[80] Rongxin Wu, Ming Wen, Shing-Chi Cheung, and Hongyu Zhang. 2018. Change-
locator: locate crash-inducing changes based on crash reports. Empirical Software
Engineering 23, 5 (2018), 2866â€“2900.
[81] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised
feature learning via non-parametric instance discrimination. In Proceedings of
the IEEE conference on computer vision and pattern recognition . 3733â€“3742.
[82] Huan Xie, Yan Lei, Meng Yan, Yue Yu, Xin Xia, and Xiaoguang Mao. 2022. A
Universal Data Augmentation Approach for Fault Localization. In Proceedings
of the 44th International Conference on Software Engineering . Association for
Computing Machinery, 48â€“60.
[83] Zhongxing Yu, Chenggang Bai, and Kai-Yuan Cai. 2013. Mutation-Oriented Test
Data Augmentation for GUI Software Fault Localization. Inf. Softw. Technol. 55,
12 (dec 2013), 2076â€“2098. https://doi.org/10.1016/j.infsof.2013.07.004
[84] Zhongxing Yu, Chenggang Bai, and Kai-Yuan Cai. 2015. Does the Failing Test
Execute a Single or Multiple Faults? An Approach to Classifying Failing Tests. In
Proceedings of the 37th International Conference on Software Engineering - Volume
1(Florence, Italy) (ICSE â€™15) . IEEE Press, 924â€“935.
[85] Zhongxing Yu, Hai Hu, Chenggang Bai, Kai-Yuan Cai, and W. Eric Wong.
2011. GUI Software Fault Localization Using N-gram Analysis. In 2011 IEEE
13th International Symposium on High-Assurance Systems Engineering . 325â€“332.
https://doi.org/10.1109/HASE.2011.29
[86] Zhongxing Yu, Matias Martinez, Zimin Chen, TegawendÃ© F. BissyandÃ©, and
Martin Monperrus. 2023. Learning the Relation Between Code Features and
Code Transforms With Structured Prediction. IEEE Transactions on Software
Engineering 49, 7 (2023), 3872â€“3900. https://doi.org/10.1109/TSE.2023.3275380
[87] Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux, and Martin
Monperrus. 2019. Alleviating Patch Overfitting with Automatic Test Generation:
A Study of Feasibility and Effectiveness for the Nopol Repair System. Empirical
Softw. Engg. 24, 1 (feb 2019), 33â€“67. https://doi.org/10.1007/s10664-018-9619-4
[88] Chenxi Zhang, Xin Peng, Tong Zhou, Chaofeng Sha, Zhenghui Yan, Yiru Chen,
and Hong Yang. 2022. TraceCRL: contrastive representation learning for mi-
croservice trace analysis. In Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engineering,
ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022 . ACM, 1221â€“1232.
[89] Jinglei Zhang, Rui Xie, Wei Ye, Yuhan Zhang, and Shikun Zhang. 2020. Ex-
ploiting code knowledge graph for bug localization via bi-directional attention.
InProceedings of the 28th International Conference on Program Comprehension .
219â€“229.
[90] Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where Should the Bugs Be
Fixed? - More Accurate Information Retrieval-Based Bug Localization Based
on Bug Reports. In Proceedings of the 34th International Conference on Software
Engineering . IEEE Press, 14â€“24.
[91] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. [n. d.].
Devign: Effective Vulnerability Identification by Learning Comprehensive Program
Semantics via Graph Neural Networks .
[92] Ziye Zhu, Yun Li, Hanghang Tong, and Yu Wang. 2020. Cooba: Cross-project
bug localization via adversarial transfer learning. In IJCAI .
[93] Ziye Zhu, Yun Li, Yu Wang, Yaojing Wang, and Hanghang Tong. 2021. A deep
multimodal model for bug localization. Data Mining and Knowledge Discovery
35, 4 (2021), 1369â€“1392.
[94] Ziye Zhu, Hanghang Tong, Yu Wang, and Yun Li. 2022. Enhancing bug localization
with bug report decomposition and code hierarchical network. Knowledge-Based
Systems 248 (2022), 108741.
[95] Weiqin Zou, David Lo, Zhenyu Chen, Xin Xia, Yang Feng, and Baowen Xu. 2020.
How Practitioners Perceive Automated Bug Report Management Techniques.
IEEE Transactions on Software Engineering 46, 8 (2020), 836â€“862.
Received 2023-02-02; accepted 2023-07-27