DoCONTRIBUTING Files ProvideInformation aboutOSS
Newcomers’ Onboarding Barriers?
Felipe Fronchetti
Virginia CommonwealthUniversity
Richmond, USA
fronchettl@vcu.eduDavid C.Shepherd
Lousiana StateUniversity
BatonRouge, USA
dshepherd@lsu.eduIgor Wiese
UniversidadeTecnologica Federaldo
Parana
CampoMourao, Brazil
igor@utfpr.edu.br
Christoph Treude
TheUniversity ofMelbourne
Melbourne, Australia
christoph.treude@unimelb.edu.auMarcoAurélioGerosa
NorthernArizona University
Flagstaﬀ, USA
marco.gerosa@nau.eduIgor Steinmacher
NorthernArizona University
Flagstaﬀ, USA
igor.steinmacher@nau.edu
ABSTRACT
Eﬀectively onboarding newcomers is essential for the success of
open source projects. These projects often provide onboarding
guidelinesintheir‘CONTRIBUTING’/f_iles(e.g.,CONTRIBUTING.md
onGitHub).These/f_ilesexplain,forexample,howto/f_indopentasks,
implementsolutions,andsubmitcodeforreview.However,these
/f_iles often do not follow a standard structure, can be too large, and
miss barriers commonly found by newcomers. In this paper, we
proposeanautomatedapproachtoparsetheseCONTRIBUTING
/f_ilesandassesshowtheyaddressonboardingbarriers.Wemanually
classi/f_ied a sample of /f_iles according to a model of onboarding bar-
riers fromthe literature, trained a machine learningclassi/f_ier that
automatically predicts the categoriesof each paragraph (precision:
0.655, recall: 0.662), and surveyed developers to investigate their
perspective of the predictions’ adequacy (75% of the predictions
wereconsideredadequate).WefoundthatCONTRIBUTING/f_iles
typically do not cover the barriers newcomers face (52% of the
analyzed projects missed at least 3 out of the 6 barriers faced by
newcomers; 84% missed at least 2). Our analysis also revealed that
informationaboutchoosingataskandtalkingwiththecommunity,
twoofthemostrecurrentbarriersnewcomersface,areneglectedin
morethan75%oftheprojects.Wemadeavailableourclassi/f_ierasan
onlineservicethatanalyzesthecontentofagivenCONTRIBUTING
/f_ile.Ourapproachmayhelpcommunitybuildersidentifymissing
informationintheprojectecosystemtheymaintainandnewcomers
can understandwhat toexpect in CONTRIBUTING /f_iles.
CCSCONCEPTS
•Softwareanditsengineering →Opensourcemodel ;•Human-
centeredcomputing →Collaborative andsocial computing
systems and tools .
ESEC/FSE ’23,December 3–9, 2023, SanFrancisco, CA, USA
©2023 Copyright held bytheowner/author(s).
ACM ISBN979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616288KEYWORDS
novices,onboarding,FLOSS,open source,software engineering
ACM ReferenceFormat:
Felipe Fronchetti, David C. Shepherd, Igor Wiese, Christoph Treude, Marco
Aurélio Gerosa, and Igor Steinmacher. 2023. Do CONTRIBUTING Files
Provide Information about OSS Newcomers’ Onboarding Barriers?. In Pro-
ceedings of the 31st ACM Joint European Software Engineering Conference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’23),
December 3–9, 2023, San Francisco, CA, USA. ACM, New York, NY, USA,
13pages.https://doi.org/10.1145/3611643.3616288
1 INTRODUCTION
Newcomers to Open Source Software (OSS) projects encounter
severalbarrierstomakingtheir/f_irstcontribution[ 73].Forexample,
an overly complex codebase or a workspace that is challenging
tobuildsapsnewcomers’motivationtocontribute[ 76].Research
showsthatthesebarriersdiscouragenewcomers,whooftengive
up before completingasingle contribution [ 48].
To onboard, newcomers usuallyconsult the project’s documen-
tation or contact the project team [ 22,34,41]. Yet project members
arebusymakingtheirowncontributions,canonlyhelpalimited
number of newcomers at a time, and may not be able to manage
synchronous communication due to time zone diﬀerences [ 23,35].
For onboarding newcomers, appropriate documentation is more
eﬃcient andscalable [ 71,74].
Unfortunately, most OSS projects’ existing documentation is
either low quality or non-existent [ 1,41,76]. Some studies point to
problemssuchasdocumentation/f_ilesthatareincorrect,incomplete,
andoutdated[ 1,48].Otherstudiesidenti/f_iedfurtherdocumentation
barriers for newcomers, including unclear, and scattered documen-
tation,withinformationoverloadfromunimportantinformation
sharplycontrastingwithmissingnecessaryinformation[ 73].These
andotherdocumentationde/f_icienciesimpactallcontributorsbut
havemoreimpactonnewcomerssincetheyneedtoorientthem-
selves in anewenvironment [ 34].
Previous work [ 74,78] showed that newcomers found them-
selvesmoreorientedandunderstoodtheprocessbetterwhenthe
right information was provided in an organized way. However,
little workhas speci/f_ically focused on enhancing newcomers’ doc-
umentation and identifying what information is missing from the
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
16
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA F. Fronche/t_ti, D. C.Shepherd, I. Wiese, C.Treude, M. A.Gerosa,I. Steinmacher
contribution guidelines [ 74]. With the goal of improving this situa-
tion,weautomaticallyanalyzeandclassifythedistinctinformation
typescontainedinexistingCONTRIBUTINGdocumentationaimed
at newcomers. In this study, we answer the following research
questions:
RQ1.How accurately can we automatically classify the content
ofCONTRIBUTING /f_ilesinGitHubprojects?
RQ2.TowhatextentdoOSSprojects’CONTRIBUTING/f_ilescover
contentrelatedto newcomers’contributionbarriers?
To answer these questions, we created an oracle by manually
annotatingtheCONTRIBUTING/f_ilesfrom500softwareprojects
according to the newcomers’ barriers model proposed by Stein-
macher et al. [ 73]. Then, we trained a machine learning classi/f_i-
cation model that identi/f_ies Steinmacher et al.’s six categories of
barriers (precision: 0.655, recall: 0.662). The results were further
validated through a survey with experienced developers, where
thedevelopersagreedthat ≈75%ofthecategoriespredictedwere
adequate.Finally,weusedourmodelacross2,274publiclyavailable
projectstobetterunderstandtowhatextentcontribution/f_ilescover
the information about the barriers faced by newcomers. We found
that CONTRIBUTING /f_iles are woefully inadequate for supporting
newcomers. Most contain four or fewer of the six expected cate-
gories of onboarding barriers, and thousands of projects ( ≈65%) do
not have acontribution/f_ile.
To facilitate researchers and practitioners to build upon our
workbyaccessingitscategorizationmodel,wedevelopedanonline
service that automatically analyzes the content of a given CON-
TRIBUTING/f_ile( http://contributing.streamlit.app/ ).Ourtooloﬀers
potentialbene/f_itsforprojectmaintainersandcommunitymanagers
byallowing them toevaluateand enhancetheirCONTRIBUTING
/f_iles based on feedback from our classi/f_ier. This becomes espe-
ciallysigni/f_icantinecosystemscomprisingmultipleprojects,where
community managers oversee various distinct projects. Achieving
consistencyacross projectsis crucial toreducecognitive loadand
promote smooth transitions between projects within a software
ecosystem. Adhering to the maintenance of CONTRIBUTING /f_iles
is a recognized best practice to assist newcomers in onboarding
open sourcesoftware projects[ 71].
2 RELATED WORK
Inthissection,wehighlightrelatedstudiesaboutdocumentationin
OSS,theautomaticcategorizationofsoftwareengineeringartifacts,
andbarriersnewcomers face inOSS projects.
Documentationissues in OSS repositories. Documentation
playsacrucialroleinsoftwareprojects,andde/f_icienciesindocu-
mentation/f_ilescanhindertheirutilityfordevelopers[ 39,49,68].
Lethbridgeetal.[ 42]identifythatdocumentation/f_ilescontainex-
cessive information, are hard to maintain, and make it challenging
to locate helpful information. Such considerations are also present
in the context of OSS communities [ 1,18,77]. According to Dias et
al.[15],fromtheperspectiveofOSSdevelopersandmaintainers,
OSScontributorsneedtoensurethequalityandconsistencyofdoc-
umentation/f_iles.Ourstudyhelpstoprocessexistingdocumentation
/f_ilesandclassifycontentrelevantfornewcomers,helpingmaintain-
ers identify missing information in their contributing guidelines
andnewcomers locate relevantinformation.Automatic classi/f_ication of software engineering artifacts.
Several studies have automated the categorization of artifacts in
software engineering [ 27,43,62]. For example, Prana et al. [ 61]
broke down theheadersofREADME/f_iles inOSSrepositories into
eightcategoriesofinformation.Basedonthemanualannotation
of 4,226 README /f_ile sections, the authors implemented a classi/f_i-
cationmodel thatautomaticallyidenti/f_iesthecontext ofa section
in a README /f_ile. They argue that labeling sections makes the
knowledge discovery process easier for visitors. We followed a
similar method and share their idea that categories may help navi-
gate the information space, especially for outsiders. For a diﬀerent
type of documentation /f_ile,Robillardand Chhetri [ 65] categorized
textfragmentsfromAPIdocumentationbasedontheirrelevance
for programmers. The authors proposed a coding guide and an
automated technique to classify text fragments into three levels of
relevance for programmers. The variety of studies exploring the
automatic categorization of information in software-related arti-
factsiswide(e.g.,[ 45,60,87]),butourstudyisamongthe/f_irstto
automatically categorize information in contribution guidelinesto
addressnewcomers’contributionbarriers.
Newcomersin OSScommunities: Supportingand engaging
newcomersincreasesthelikelihoodofnewcomerscompletingtheir
contributions, which is essential for the long-term viability of OSS
projects [ 25,57,78,81]. Without adequate retention, project devel-
opment progress slows, jeopardizing the existence of such com-
munities[ 80].Tostudythisissue,researchersidenti/f_ieddiﬀerent
obstaclesnewcomersfaceintheonboardingprocess,focusingon
theperiodbetweentheirinitialcontactwiththeOSScommunity
andtheir/f_irstcontribution[ 1,76–78].
Steinmacher etal. [ 73] propose ataxonomyof 58barriers new-
comers face when joining OSS projects. Documentation issues ap-
pearasacentralsourceofproblemsfornewcomers,includingal-
readymentionedchallengessuchasinformationoverload,scattered
andoutdateddocumentation,andlackofnecessaryprojectinfor-
mation. Someresearchersinvestigatedhowexistingapproaches
supportnewcomersonboarding.Morespeci/f_ically,theyfocusonun-
derstandinglabelstoguidenewcomerstochoosetheirtasks[ 80,81],
exploringthe roleofQ&A websites inhelpingthe onboarding[ 84],
andcodevisualization[ 57].Otherstudiesdiscusshowdocumen-
tation can help and cause problems and how it may impact the
newcomers’experience[ 47,55].WebelieveourresultsinformOSS
projectstowardbettersupportingnewcomerswiththeinformation
they needwhen joining aproject.
Itisclearfromtheliteraturethatdocumentationiscriticalfor
onboarding newcomers inOSS. Despite the eﬀorts in categorizing
artifactsrelatedtoprojectdocumentation,nobodyofknowledge
exists aboutthe appropriateness ofcontribution guidelines foron-
boardingnewcomers.Inthispaper,weaddressthisbyanalyzing
thecontentofCONTRIBUTING/f_ilesfromOSSrepositoriesinterms
ofbarriersnewcomers face.
3 RESEARCH METHOD OVERVIEW
To answer our research questions, we manually analyzed CON-
TRIBUTING /f_iles from 500 projects and built a classi/f_ier to label
information known to be relevant for newcomers. According to
GitHub[31]guidelines,CONTRIBUTING/f_ilesarewhereoneshould
17DoCONTRIBUTING Files ProvideInformationabout OSSNewcomers’OnboardingBarriers? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Extraction of repositories'
CONTRIBUTING.md files
using GitHub APISelection of repositories
hosted on GitHub
based on their popularity
level and programming
language
Conversion of
CONTRIBUTING.md
paragraphs to spreadsheets
for annotationAnnotation 
of paragraphs
based on information
known to be relevant
for newcomersData Extraction
Data
Analysis
Text preprocessing on
annotated paragraphs 
Conversion of
paragraphs into
TF-IDF and
 heuristic-based features
Feature SelectionPreprocessing
Classification
Training and
selection of 
best classifierEvaluation
Evaluation of selected
classifier on test data 
Application of a survey to
evaluate classifier
predictions9514
projects
2915
files
Categories Definition
Six categories of information known
to be relevant for newcomers were
used for analysis
Prediction
CONTRIBUTING.md file
of 2.274 projects
predicted by the best
classification model20.773
paragraphs
Figure 1:Researchmethod followed inthisresearch, frombuildingthecorpus to assessing the classi/f_ier
"create guidelines to communicate how people should contribute to
yourproject." Additionally,theOpen Sourceguide[ 53]reinforces
that"a CONTRIBUTING /f_ile tells your audience how to participate
in your project... [and] is an opportunity to communicate your ex-
pectationsforcontributions." Therefore,newcomersexpectto/f_ind
relevantinformation to avoid common onboarding barriers[ 73].
Theresearchmethodwasconductedinsixsteps,aspresentedin
Figure1:(1)WeextractedtheCONTRIBUTING/f_ilesfrom2,913OSS
projects hosted on GitHub. (2) The paragraphs of a random sample
of /f_iles were manually annotated. (3) The annotated paragraphs
were pre-processed and then converted into statistical features
(i.e., term frequency-inverse document frequency) and heuristic-
based features (in which a rule-basedapproach was performed). (4)
Wetrained/f_ivediﬀerentclassi/f_icationmodelswiththesefeatures
andcomparedtheirperformances.(5)Wesurveyeddevelopersto
assess the quality of the classi/f_ications. (6) Finally, we used our
model to classify the content of 2,274 CONTRIBUTING /f_iles and to
understandto what extentthey cover the onboarding barriers.
All scripts, models, data, and results are available in our replica-
tion package [ 20]. In the following, we present more details of the
methodandresults ofeachstep.
4 BUILDINGTHE CORPUS
To train our models we collected and manually categorized the
contentofCONTRIBUTING /f_ilesfrom asetofOSS projects.
4.1 CategoriesDe/f_inition
We manuallylabeled eachparagraph of the 500CONTRIBUTING
/f_iles according to the way Steinmacher et al. organized the cate-
gories of barriers on the FLOSScoach portal [ 78]. The portal was
created based on a barriers model built based on a systematic liter-
aturereview,interviewswithmultiplestakeholders,andsurveys
withinOSScommunities,providingacomprehensiveaggregationof
thebarriersnewcomersfacewhenjoiningOSSprojects.Inaddition
tothecomprehensivenessofthemodel,wechosetofollowthese
categories since the work by Steinmacher et al. [ 74,78] showed
that organizing the information in these categories lowered the
barriers related to orientation and contribution process. These are
the categorieswe used:CF - Contribution /f_low: Derived from the “Newcomer Orien-
tation” barrier category mapped to the contribution /f_low shown
under“HowtoStart”inFLOSScoach,thiscategoryde/f_inesthesteps
that a newcomer needs to follow to contribute to the project. This
category appears as, for example, an ordered list of steps to follow
oras asetofparagraphs describingthe currentprojectwork/f_low.
CT-Chooseatask: Alsoderivedfromthe“NewcomerOrienta-
tion” barrier, it is mapped from the “Choose a Task” menu item
inFLOSScoach. Thiscategory explains how newcomers can /f_ind
a task (or issue) to contribute to the project. It may also contain
descriptions of diﬀerent types of tasks appropriate for newcomers.
TC - Talk to the community: Related to the “Communication
Issues” barriers, this category refers to information about how a
newcomer can get in touch with community members and how to
/f_indamentor.Thiscategoryincludes,forexample,linkstocommu-
nicationchannels,communicationetiquette,communityguidelines,
andtutorialsonhowto startaconversation.
BW-Buildlocalworkspace: Mappedfromthe“LocalEnviron-
mentSetupHurdles,”thiscategorydeterminesthestepsanewcomer
needstofollowtobuildthelocalworkspace.Itmayincludeinstruc-
tionssuch as bashcommands andchanges incomputer settings.
DC - Deal with the code: Derived from “Code/Architecture Hur-
dles,”itdescribeshownewcomersshoulddealwiththesourcecode.
Thiscategorymaycontaincodeconventions,descriptionsofthe
sourcecode,andguidelines onhowto writecode for the project.
SC - Submit the changes: Directly mapped from “Change Re-
quest Hurdles,” this category represents information about how
newcomers should submitacontributionto the project.
4.2 Data Collection
4.2.1 Project Selection. We selected the most popular OSS reposi-
torieshostedon GitHubwhenwestartedthe datacollection(Aug
2020), written in at least one of the top 10 programming languages
usedintheplatform.Weselectedprojectsbasedontheirpopular-
ity and programming language to avoid repositories that were toy
projects or unrelated to software development. The selection of
projectsbypopularitywasbasedonthestudyofBorgesetal.[ 8],
which discusses stars as a unit to measure the popularity of OSS
projects on GitHub, and shows that, in their population, “three out
of four developers consider the number of stars before using or
18ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA F. Fronche/t_ti, D. C.Shepherd, I. Wiese, C.Treude, M. A.Gerosa,I. Steinmacher
Table1:Numberofprojectsremovedperlanguageandtheirrespectivereasonsforexclusion.The“n”valuerepresentsthetotal
numberofprojectscollected foralanguage.CONTRIBUTING /f_iles may have been excluded formore than onereason.
Removed because JavaScript Python Java PHP C# C++ TypeScript Shell C Ruby
CONTRIBUTING... (n=824) (n=929) (n=942) (n=941) (n=990) (n=942) (n=990) (n=990) (n=947) (n=1019)
wasmissing 381(46%) 527(57%) 692(73%) 593(63%) 651(66%) 604(64%) 474(48%) 785(79%) 702(74%) 646(63%)
size <0.5kB 41(5%) 37(4%) 22(2%) 49(5%) 54(5%) 35(4%) 42(4%) 23(2%) 28(3%) 33(3%)
wasnot in English 3(<1%)12(1%) 4(<1%)4(<1%)3(<1%)2(<1%) 3(<1%)4(<1%)2(<1%)1(<1%)
wasnot in Markdown 2( <1%) 91(10%) 5(1%) 4( <1%) 1( <1%) 14(1%) 4( <1%) 6(1%) 26(3%) 11(1%)
Projects removed 425(52%) 661(71%) 721(76%) 647(68%) 709(71%) 651(69%) 521(52%) 818(82%) 755(79%) 691(67%)
contributingtoaGitHubproject”.Inadditiontoit,thisisafairly
common wayto sample projectsonGitHub[ 33,59,61,63,75].
Toidentifythetop10most-usedlanguages,weusedtheranking
provided by GitHub Octoverse [ 29], which showed, at that time:
JavaScript, Python, Java, PHP, C#, C ++, TypeScript, Shell, C, and
Ruby. We aimed to get the /f_irst 1,000 projects per language ranked
by stars. However, the GitHub API provides only a few pages con-
taining the top projects and we could not collect 1,000projects for
somelanguages.We collectedatotalof9,514 repositories.
To ensure that all the selected repositories had a valid CON-
TRIBUTING /f_ile, we de/f_ined a set of /f_ilters to remove projects in
ourdataset.Weremovedfromoursampletheprojectsthathada
CONTRIBUTING /f_ile:
i.missing—wefocusedonlyonprojectsthatfollowedtheguide-
lines from GitHub to keep in this speci/f_ic /f_ile information
abouthowto contribute;
ii.smallerthan0.5kB—to/f_ilteroutthose/f_ilesthatredirectto
guidelines not hostedonGitHub,orempty/f_iles;
iii. written inalanguageotherthanEnglish;
iv.not in Markdown format—which was the most prevalent
format inour sample(3,295 out of3,459 projects that had a
CONTRIBUTING /f_ile were inMarkdown–95.2%).
0 100 200 300 400
# Contributors
0 500 1000 1500 2000 2500 3000 3500
# Forks
0 1000 2000 3000
# Pull requests
0 5000 10000 15000 20000
# Stars
Figure2: Distributionofcontributors,forks,pullrequests,
andstars perprojectconsidered as valid.
Table1showsthenumberofprojectsperprogramminglanguage
removed from our dataset.The /f_inal setof repositoriescomprised2,915projects. Afterapplyingthe/f_ilters,wekeptadiversenumber
of projects in terms of the number of contributors, forks, pull re-
quests, and stars (see Figure 2). The programming languages with
the highest number of repositories included in the analysis were
TypeScript, JavaScript, andRuby.
4.2.2 DocumentationForma/t_ting. Topreparetheprojectsforthe
qualitativeanalysis,weconvertedthecontribution/f_ilesintospread-
sheets.Eachspreadsheetmapstoallparagraphsofonecontributing
/f_ile in our sample. The /f_irst column of each row of the spreadsheet
contained in plaintext format one paragraph of the documenta-
tion /f_ile for the respective project. We followed the de/f_inition of a
paragraph provided by the speci/f_ication of GitHub Flavored Mark-
down [30], which speci/f_ies it as “ a sequence of non-blank lines that
cannotbeinterpretedasotherkindsofblocksforms .”Tofacilitatethe
workoftheannotators,wecreatedheadersforsixcolumns,each
representingone of the six categories we aimed to identify during
the qualitative analysis.
4.3 Data Annotation
After transforming the CONTRIBUTING /f_iles into spreadsheets,
we conducted the annotation process. We annotated a total of 500
spreadsheets(from500projects).Inthe/f_irststep,twoannotatorsla-
beled 30 spreadsheets of a random subset of projects and discussed
how the categories should be assigned to each paragraph. To mea-
sure the agreement between the annotators, they independently
labeled the spreadsheets divided into three consecutive stages—
consistingof10spreadsheetsperstage.Theannotationconsistedof
analyzing and labeling each paragraph according to the categories
presented in Section 4.1. At the end of each stage, the reviewers
compared their labels and discussed their diﬀerences to align their
understanding of each category. We use Cohen’s kappa coeﬃ-
cient to measure the agreement between the annotators [ 13]. After
the /f_irst stage, the annotators reached an agreement of 73%and
discussed the potential meaning of categories. For the other two
stages,theagreementwas 85%and79%,respectively.Theoverall
agreementbetweentheannotatorswas 79%,whichwasconsidered
suﬃcient given the multi-classnature of the data.
4.3.1 DocumentationAnnotation. Afterreachingasubstantialagree-
ment,thereviewersproceededtoanalyzetheremaining/f_iles,which
were split between them. A total of 500 spreadsheets were anno-
tatedduringthequalitativeanalysis,resultingin20,733paragraphs
analyzed. We had to dismiss 66 /f_iles that did not present any infor-
mationaboutthesixcategoriesofbarriers,whichwerereplacedby
19DoCONTRIBUTING Files ProvideInformationabout OSSNewcomers’OnboardingBarriers? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
66other/f_ilesfromourdataset.Afterthereplacement,weendedup
with19,961paragraphs.
4.4 Corpus Characterization
In Figure 3, we present the distribution of paragraphs analyzed per
/f_ile.The average numberof paragraphs for our set of500projects
was41,andthemedianwas29.Twoprojectshadonly2paragraphs
(minimum),andonehad422paragraphsinasingle/f_ile(maximum).
0 20 40 60 80 100
Figure 3:Distribution ofparagraphs per/f_ile.
Table2showsthe distribution ofcategoriesinoursample.More
than 6,000 paragraphs were categorized as "Submit the changes",
and more than 2,000 as "Deal with the code" and "Contribution
/f_low." On the other hand, "Choose a task" and "Talk to the com-
munity" appear in 116 and 183 paragraphs respectively. Still, 7,461
paragraphscouldnotbecategorizedunderanycategory.Weana-
lyzedtheseparagraphsandfounddiﬀerenttypesofcontentthatdid
not belong to any category. The most recurring cases were: "thank
you" messages; license statements or the complete license; links to
other sites; instructions on how to open an issue; information in
diﬀerentlanguages; GitHubbadges;andlistsofcontributors.
Table2:Characterizationofthedatasetconsideringthesix
categoriesofbarriers[ 74]
Category # Paragraphs # Projects Avg.per/f_ile
Choosea Task 116(0.58%) 116(23%) 1
Talkto theCommunity 183(0.92%) 183(37%) 1
Build Local Workspace 1,444 (7.23%) 139(28%) 10.4
Contribution Flow 2,088 (10.64%) 319(64%) 6.5
Deal withtheCode 2,495 (12.5%) 280(56%) 8.9
SubmittheChanges 6,174 (30.93%) 396(79%) 15.6
No category 7,461 (37.38%) 483(97%) 15.4
5 BUILDINGAND EVALUATING THE
CLASSIFIER
Wetrainedmachinelearningmodelstoclassifyinformationaccord-
ing to the six categories de/f_ined in Section 4.1. The 500 annotated
spreadsheetswereusedtoextractfeaturesforclassi/f_ication(Sec-
tion5.1). The data was prepared using text pre-processing tech-
niques.Thefeaturescreatedweredividedintostatisticalfeatures
(i.e., extracted using statistical methods) and heuristic features (i.e.,
extractedthroughidentifying linguisticpatterns).
Thefeatureswerethenappliedtosupervisedlearningalgorithms
to/f_indthebestclassi/f_icationmodelforthisproblem(Section 5.2).
Theannotateddatasetwassplitintotworandomsubsets.Atraining
set (80% of the dataset) was used to compare the diﬀerent classi-
/f_iers, and a test set (20% of the dataset) was reserved for testing
the classi/f_ication algorithm with the highest evaluation score. The
algorithms were evaluated based on their classi/f_ication scores, and
a/f_inalmodelwastrainedusingthebest-performingclassi/f_icationalgorithm (Section 5.5). Figure4provides an overview of the classi-
/f_ication process,whichisdetailedinthe following sections.
Test Set
Evaluate Estimators Performance
Using five supervised learning algorithms, we executed:
Nested Cross-Validation (10-fold + GridSearch)
Training
Best algorithm + Best ConfigurationText Preprocessing
Lemmatization, stop words and punctuation removal on text columnTraining SetSpreadsheets
Text column + Columns of relevant categories
Feature Extraction
Conversion of text column to TF-IDF and heuristic features
Final evaluation
Analysis of evaluation metricsText Preprocessing
(Separately)
Feature Extraction
(Separately)
Figure 4:The classi/f_icationprocess.
5.1 FeatureExtraction
Inthefeatureextractionprocess,theannotatedparagraphs(Section
4.3) were converted into numerical data. We divided the feature
extraction processinto fourstages:textpre-processing,thede/f_ini-
tionofstatisticalfeatures,thede/f_initionofheuristicfeatures,and
feature selection.
5.1.1 TextPre-processing. Beforecreatinganyfeaturesfortheclas-
si/f_ier, three pre-processing techniques used in text classi/f_ication
were applied to the paragraphs: lemmatization, stop words, and
punctuationremoval.Inthelemmatizationprocess,theaﬃxesof
words in each paragraph were removed, turning the words back
to their root form [ 37]. Words such as submits,submitted , andsub-
mitting,forexample,werereturnedtotheirrootform submit.To
reduce the number of ineﬀective words in the paragraphs’ classi/f_i-
cation,wealsoremovedstopwords,excludingwordscommonly
found in the English vocabulary (e.g., conjunctions and pronouns)
[86]. For the same purpose, punctuation was also removed from
the text. For both lemmatization and stop words removal, we used
the implementations providedbythe NLTKlibrary [ 44].
5.1.2 Statistical Features. We converted the annotated paragraphs
into TF-IDF features using the TfIdfVectorizer method of the scikit-
learn library [ 6,58]. In this approach, we represented words as
n-grams of size one and two [ 5]. The acronym TF-IDF is a refer-
ence for the multiplication of two statistical measures used in text
classi/f_ication,termfrequency(TF)andinversedocumentfrequency
(IDF)[38,82].Fortermfrequency,wemeasuredhowoftenwords
occur in a paragraph (number of occurrences of each word per
paragraph,dividedbythetotalwordsinthatparagraph).Forthe
inversedocumentfrequency,wecountedhowoftenwordsoccur
compared to the entire set of paragraphs. The multiplication of
both measures gives us statistical features that show the relative
importanceofeachword.
5.1.3 Heuristic Features. The set of statistical features was com-
binedwithheuristicsfoundthroughqualitativeanalysistoenrich
the characteristics used in classi/f_ication. We adopted a strategy
20ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA F. Fronche/t_ti, D. C.Shepherd, I. Wiese, C.Treude, M. A.Gerosa,I. Steinmacher
used by previous work [ 56,61], in which features are generated
by analyzinglinguistic patterns intheannotated paragraphs.Dur-
ing the manual analysis, the annotatorsselected words that could
characterize speci/f_ic categories and be used as patterns for a classi-
/f_icationbasedonheuristics.Forexample,theword“ commit”was
commonly found in paragraphs annotated as Submit the changes
(see Table 3for examplesofothercategories).
Table 3:Examples ofheuristic featurespercategory.
Category Relatedexamples
Contribution Flow Clone, push,merge, pullrequest, contribution
Chooseatask Issue, issue tracker, label,fork
Talk to thecommunity Mailinglist,contact,email, conduct,slack
Build localworkspace Tool, package, update, dependencies
Deal with thecodeCode snippet,library, debug,
codingconvention, method, variable
Submit thechanges Commit, diﬀ,review, test, fetch
Usingtherule-basedmatchingapproachoftheSpacylibrary[ 72],
we assigned an equal set of heuristic features to each paragraph in
thetrainingprocess.Eachfeaturerepresentedapattern;paragraphs
wereassignedthevalueof1whentheycontainedtherespective
wordsand0otherwise.
5.1.4 Feature Selection. To avoid using features that could be con-
sideredirrelevanttoourclassi/f_ication,weremovedtheoneswith
the lowest scores. The SelectPercentile [69] method of the scikit-
learnlibrarywasusedwithChi-squareasthescorefunction.Fea-
turesthatfellbelowthe 15/u1D461/uni210Epercentilewereremoved. Wemanually
testedasetofpercentiles(5th,10th,15th,20th)basedonthedefault
value of Scikit-learn [ 69], which is the 10th percentile. We chose
the15/u1D461/uni210Epercentile as it performed best, as it is commonly done in
the literature [ 4,14,70].
5.2 FindingtheBest Classi/f_ier
Asetofclassi/f_ierswastrainedto/f_indthebestlearningalgorithmto
solveourclassi/f_icationproblem.Totraintheclassi/f_iers,weusedtwo
multi-classtrainingstrategies:one-vs-rest(OvR)andone-vs-one
(OvO)[7].IntheOvRstrategy,abinaryclassi/f_ierwastrainedfor
eachcategory.Theassignmentofacategoryforaparagraphwas
thenmadebyidentifyingthebinaryclassi/f_ierthatbestrepresented
the respective paragraph (i.e., the one with the best scores). In
the OvO strategy, the samples of each category were grouped in
pairs,andthecomparisonwasmadeinabinaryclassi/f_ierfortwo
categories at a time. To identify what category should be assigned
foraparagraph,thepredominanceofacategoryamongallthepairs
wasconsideredas the decision method.
The following classi/f_ication algorithms were trained during
thisstep:RandomForestClassi/f_ier,KNeighborsClassi/f_ier,LinearSVC,
MultinomialNB, LogisticRegression. The selection was based on
similar studies using text classi/f_ication in Software Engineering
[56,61].Asabaseline,wetrainedtwodummyclassi/f_iers,oneus-
ingthemostfrequentclasslabelobservedinthetrainingsetand
one providing completely random predictions. As highlighted in
Table2, we noticed that the number of instances per category was
unbalancedinourdataset,soweusedtheSMOTEoversampling
technique to achieve a better balance between the classes. TheSMOTE algorithm was implemented using the imbalanced-learn
library [40], a module designed for unbalanced datasets that are
recommendedbythescikit-learncommunity. Still,weusedchat-
GPT (GPT 3.5 model) [ 54] using a few-shot learning approach [ 10]
tocompareourresultswiththeperformanceofthisLLM.Forthe
few-shotlearning,werandomlyselected12instancesofparagraphs
in our training set for eachcategory. Then, we prompted chatGPT
to classify200instancesrandomly selectedfrom our test set.
5.3 EvaluationMetrics
To measure the overall performance of the classi/f_iers, we used
a combination of three evaluation metrics for data classi/f_ication:
precision, recall, and F1 score. Precision, also known as con/f_idence,
provides the proportion of positive samples that were correctly
predicted,incontrasttoallthesamplespredictedaspositive[ 28,85].
Recall gives the fraction of positive samples correctly predicted by
theclassi/f_ier,andtheF1scoreprovidestheharmonicmeanbetween
precisionandrecallvalues[ 12,36].Thisisamulti-classproblem,
andthe resultingvaluesare the weightedaverageof allclasses.
5.4 Cross-Validation
Wetestedtheperformanceofourclassi/f_iersusinganestedten-fold
cross-validationstrategy[ 17,52].Thisalgorithmdividesthedataset
offeaturesandlabelsintotenparts.Thetenpartsarecombinedinto
tendiﬀerenttrainingandvalidationsubsets,alsoknownasfolds.
Foreachfold /u1D456dividedinto /u1D458parts(/u1D458=10),the/u1D458/u1D456partisusedasthe
validationset,andtheremainingpartsareusedastrainingforeach
classi/f_ication algorithm in our list. The average of the weighted F1
scores of the /u1D458diﬀerent classi/f_iers gave us an overall performance
for eachlearningalgorithm.
To increase the chance of selecting the best parameters for
each algorithm, we applied the GridSearch method to the cross-
validationinternalloop.Thevaluestestedwerebasedonthedefault
valuesinthescikit-learnlibrary.Weselectedthebestcon/f_iguration
(i.e., classi/f_ier, parameters, and training strategy) to train a /f_inal
classi/f_ication model.
5.5 Classi/f_ierTraining
With the best learning algorithm selected, we trained a classi/f_ier
usingthecompletedatasetusedinthepreviousstep(trainingsetin
Fig.4)andthetestset(Fig. 4)wasusedfortesting.Thiswasdone
toshowthereliabilityofboththemodelandtheresults[ 56,61,69].
This enabled us to use our complete annotated training set (80% of
thesample)andtestonthe20%oftheoriginalannotateddataset
that wasnot previously used.
5.6 Classi/f_ierHuman Evaluation
Tofurtherevaluateourclassi/f_icationmodel,wesurveyed46indi-
vidualsusingAmazonMechanicalTurk[ 2].Weinvitedonlyindi-
vidualswithpriorprogrammingexperiencebyspecifyingonthe
Amazon platform “Employment Industry - Software & IT Services”
as a selection criterion. The survey was divided into training, eval-
uation,anddemographics.Inthetrainingsection,weintroduced
the survey and described the six categories of information used
toclassifythecontent.Toguaranteethatwewouldonlyconsider
participants who paid attention to our questionnaire, we asked
21DoCONTRIBUTING Files ProvideInformationabout OSSNewcomers’OnboardingBarriers? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
them to matcheach category with theircorresponding de/f_initions
on a subsequent page. An attention check question was also in-
cludedamongthequestionsofourquestionnaire.Wedismissedthe
answersfrom29participantswhoselectedthewrongde/f_initionfor
more than one category (28) or did not mark the correct answer
ontheattentioncheck(6)—5ofthemselectedthewronganswers
in both. We ultimately considered 17 valid answers for analysis.
Although the number of remaining participants is small due to
the substantial number of discarded answers, this is considered
a common limitation of crowdsourcing platforms, as suggested
in recent studies [ 46,64,79]). The literature also mentions that
cleaning the answers is necessary to guarantee data quality and
consistency [ 64].
In the evaluation section, we asked participants to judge the
quality of the predictions. We used paragraphs from CONTRIBUT-
ING /f_iles of 75 randomly selected projects that were not part of
thetrainingortestset—andthuswereunknownbytheclassi/f_ier.
We randomly selected ten paragraphs classi/f_ied into each category
from that set of OSS projects to use in the questionnaire. For each
participant,we randomlyassigned18paragraphs (3percategory).
We gave the same number of sections for each category (instead
of a complete /f_ile) per participant to ensure the number of para-
graphsassessedpercategorywasbalanced.Moreover,weusean
approach in which the participants recognize if an item belongs
to a category (aided recall) instead of asking the participants to
label them (unaided recall) because items requiring recognition
areeasierthanitemsthatuseunaidedrecall[ 9].So,weprovided
annotated paragraphs and asked whether the category was correct.
Weaskedeachparticipanttorateeachpredictionusinga4-point
Likert scale (extremely adequate to extremely inadequate). We
employed a 4-point Likert scale to compel respondents to take a
de/f_initive stance,preventingthe use ofaneutral option[ 11].
Inthedemographicssection,weaskedparticipantstoprovide
information about their experience with OSS projects. This section
includedtwomultiple-choicequestionsaboutyearsofexperiencein
programming and maintenance of OSS projects and two questions
about their role as participants in OSS (coder or non-coder) and
their contributions to documentation inOSSprojects(Yes/No).In
Table4, we present the overall experience of our participants in
programmingandOSSprojects.Asexpected,allofourparticipants
hadsomeexperienceinprogramming,withthemajorityofthem
(64%)havingbetween3and15yearsofprogrammingexperience.
IntermsofexperienceasmaintainersinOSSprojects,82%ofour
participants had at least some experience, and 41% of them had
between 3 and 15 years of experience as software maintainers. All
14participantswithexperienceinOSSde/f_inedthemselvesascoders,
and10 workedwithdocumentation intheirprojects.
Table4:Experienceofsurveyparticipantsinprogramming
andOSS projectmaintenance.
Experience/ Type Programming OSS
Noexperience 0 3
Less thanorequal to 3years 4 7
Greater than3years andless than15 years 11 7
Greater thanorequal to 15 years 2 0Table5:F1scoresforclassi/f_ierstestedintheten-foldcross-
validation process.
WithSMOTE WithoutSMOTE
OvR OvO OvR OvO
RF 0.636 0.625 0.620 0.609
kNN 0.563 0.566 0.516 0.530
SVC 0.630 0.634 0.652 0.646
LR 0.612 0.606 0.617 0.602
NB 0.579 0.580 0.636 0.633
Dummy(Freq.) 0.001 0.009 0.001 0.190
Dummy(Rand.) 0.001 0.010 0.001 0.010
chatGPT (macroF1) 0.272
6 RQ1. HOWACCURATELYCAN WE
AUTOMATICALLY CLASSIFY THE CONTENT
OFCONTRIBUTING FILES?
This section details the evaluation of the machine learning models.
6.1 ComparingDiﬀerentClassi/f_iers
To identify the best classi/f_ier for our problem, we compared the
outputs of /f_ive machine learning algorithms and two dummy algo-
rithmsinaten-foldcross-validationprocess,inadditiontochatGPT
with a few-shot learning approach [ 10]. Table5presents the F1
scores for each classi/f_ier. The best F1 score of 0.652 is from the Lin-
earSVCclassi/f_ier,withoutoversamplingandusingtheOvRstrategy.
The second-best score is from the same classi/f_ier con/f_iguration but
usesthe OvOmulti-class strategy. The performance ofchatGPT
usingafew-shotapproachreachedanoverallmacroprecisionof
0.250, recall of 0.322, and F1 equal to 0.272. Ignoring the dummy
classi/f_iers and chatGPT, the classi/f_ication model with the worst
scores of 0.516 and 0.530 was kNN. Such results follow similar per-
formancefoundbyPranaetal.[ 61],whocategorizedthecontent
ofREADME/f_iles.
Becauseofitsscoresandsimilarperformanceinotherstudies,
LinearSVC was chosen as the /f_inal machine learning algorithm.
BasedontheoutputsoftheGridSearchalgorithm,wefoundthat
the best hyper-parameters for LinearSVC were 1,000 iterations
(max_iter=1000),regularizationequaltoone(C=1),andtolerance
equal to 0.001 (tol = 0.001). The LinearSVC algorithm was trained
againwiththis/f_inalcon/f_igurationwithoutoversamplingandusing
the OvR strategy, as this combination provided the best F1 score
in our comparison ofclassi/f_iers.Table 6presents thetraining data
andits performance per class inrelation to the test set.
In Table6, we can see that the performance varies per category.
The information about Deal with the code (DC) and Build local
workspace(BW)barriersisfairlywellpredicted(F10.711and0.716,
respectively). On the other hand, Choose a task (CT) and Contribu-
tion/f_low(CF)hadthelowestscoresof0.379and0.345,respectively.
Some external factors may have in/f_luenced such performances.
Thenumberofinstancesperclass,forexample,mightjustifythe
low score of Choose a Task (CT), which on average had only 1
paragraphperprojectanalyzed(seeSection 4.4).Thefactthatthe
Contribution /f_low contained more generic information than other
content-speci/f_iccategories,suchasBuildlocalworkspace,might
alsoexplainthe diﬀerence inperformance.
22ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA F. Fronche/t_ti, D. C.Shepherd, I. Wiese, C.Treude, M. A.Gerosa,I. Steinmacher
Forthesakeofcomparison,weincludetheresultsofthechatGPT
few-shotlearningapproachperclassinTable 7.Ascanbeobserved,
the Linear SVC model outperforms chatGPT in this context in
almostallmetricsandcategories.Theexceptionaretherecallforthe
TalktoCommunityandContributionFlowcategories.Interestingly,
chatGPTcouldnotcorrectlyidentifyanyparagraphbelongingto
the Choose a Task Category—although it received 5 instances, and
(incorrectly) predicted 3 instances. Considering the overall metrics,
the linearSVC model is more than 2x better than chatGPT in terms
ofrecall,precision, andF1.
Table 6:Performanceofthe/f_inalmodel(LinearSVC).
Category F1 Precision Recall
Buildlocal workspace 0.716 0.674 0.764
Dealwiththe code 0.711 0.682 0.743
Talk to the community 0.648 0.657 0.639
Submit the changes 0.617 0.717 0.541
Choose atask 0.379 0.687 0.261
Contribution/f_low 0.345 0.519 0.258
Nocategoriesidenti/f_ied 0.592 0.596 0.588
Overall 0.651 0.655 0.662
Table 7:PerformanceofchatGPTwith few-shotlearning.
Category F1 Precision Recall
Buildlocal workspace 0.438 0.389 0.500
Dealwiththe code 0.154 0.200 0.125
Talk to the community 0.522 0.400 0.750∗
Submit the changes 0.114 0.160 0.089
Choose atask 0.000 0.000 0.000
Contribution/f_low 0.237 0.184 0.333∗
Nocategoriesidenti/f_ied 0.436 0.420 0.453
Overall 0.322 0.250 0.272
Confusionbetweencategories InFigure 5,wepresentthecon-
fusionmatrixproducedbythe/f_inalclassi/f_icationmodel.Usinga
confusionmatrix,wecanassessthesimilaritybetweenthediﬀer-
entcategoriesofinformationandverifywhatlabelscontainfalse
positives. The main diagonal represents the true positives for each
class,andtheupperandlowertriangularsubmatricesrepresentthe
misclassi/f_ications.
In line with the previous results, contribution /f_low (CF) and
Choose a task (CT) are the categories of information with the high-
est amount of misclassi/f_ications, with only 26% true positives. Con-
tribution/f_low(CF)hadmorefalsepositivesassignedtodealwith
the code (DC) than its own category. Such results may con/f_irm the
assumption that because contribution /f_low (CF) contains a wide
rangeofinformation,andchooseatask(CT)hasjustafewsamples
usedfor training, they performedpoorly.
All other categories had more than 50% true positives. Build
local workspace (BW) and Deal with the code (DC) had the lowest
number of false positives ( <25%). Talk to the community (TC)
also presented good performance, with less than 36% incorrect
predictions. This may be because such categories contain more
speci/f_ic contentandagoodnumber ofsamples per class.
Figure 5:Confusion matrix forLinearSVC.
Legend: BW (Buildlocal workspace),DC (Deal with thecode),
TC(Talkwiththecommunity),SC(Submitthechanges),CT
(Choosea task),CF (Contributionflow).
6.2 Observationsfrom theSurvey
InFigure 6,wepresenttheparticipants’evaluationofthepredic-
tionsmadebyour/f_inalmodel.Forallthecategories,atleast30%
of the predictions were considered extremely adequate for their
paragraphs,andatleast69%ofthepredictedcategorieswereconsid-
ered at least somewhat adequate. The best-evaluated category was
Buildlocalworkspace (BW),with47%of participantsconsidering
its predictions extremelyadequate.
0 10 20 30 40 50 60 70 80 90 100CFCTTCBWDCSC
PercentageCategoriesExtremely adequate Somewhat adequate
Somewhat inadequate Extremely inadequate
Figure6:Survey:Participants’evaluationofpredictionsmade
by the/f_inalclassi/f_icationmodel.
Legend: BW (Buildlocal workspace),DC (Deal with thecode),
TC(Talkwiththecommunity),SC(Submitthechanges),CT
(Choosea task),CF (Contributionflow).
Whenweaggregateextremelyadequateandsomewhatadequate,
Deal with the code (DC) leads the adequacy board with 82% of
predictions considered adequate. Contribution /f_low (CF) has the
lowestestimates,with31%ofitspredictionsestimatedassomewhat
orextremelyinadequate.ThesecondtolastplaceisheldbyChoose
a task (CT) and Talk to the community (TC), with 29% of their
predictionsconsideredsomewhat inadequateorless.Suchresults
follow similaroutcomesfoundintheevaluationscoresofTable 6,
con/f_irming the nature ofour predictions.
Tofurtherunderstandthedisagreementbetweentheclassi/f_ier
output and the crowd, we manually analyzed the 12 paragraphs in
23DoCONTRIBUTING Files ProvideInformationabout OSSNewcomers’OnboardingBarriers? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
which50%ormoreoftherespondentsdisagreedwiththepredic-
tions. In summary, we found that the prediction was incorrect in 9
cases.Inthe3other cases,thepredictionwascorrect(2 relatedto
Choose aTask andone relatedto Buildthe Workspace).
AnswertoRQ1: Aftercomparing/f_ivesupervisedlearning
algorithms,wewereabletoclassifythecontentofCON-
TRIBUTING /f_iles achieving an F-measure of 0.651, with
precision=0.655andrecall=0.622.Althoughdiﬀerentcat-
egories of information diﬀered in performance, in general
69%oftheclassi/f_icationswereconsideredappropriateby
external reviewers.
7 RQ2. TO WHATEXTENTDO
CONTRIBUTING FILES COVER CONTENT
RELATED TO CONTRIBUTION BARRIERS?
Weusedtheclassi/f_icationmodeltopredictasetoftheremaining
2,274 CONTRIBUTING /f_iles from our dataset that we had not used
intheprevioussteps.Fromthe9,514/f_iles,weremoved6,599because
they didnot meetthe /f_iltering criteriapresentedinSection 4.2.
Figure7shows the distribution of projects and the average of
paragraphspercategoryintheCONTRIBUTING/f_ilesinwhicheach
category appeared at least once. A total of 2,265 (99.6%) projects
hadatleastoneparagraphthatdidnotbelongtoanycategory,with
an averageof15 unidenti/f_iedparagraphs per /f_ile.
0246810121416
05001000150020002500
Average# Projects# Projects Average
Figure 7: Average number of paragraphs per category in the
CONTRIBUTING /f_iles predicted.
Legend:BW(Buildlocalworkspace,DC(Dealwiththecode),
TC(Talkwiththecommunity),SC(Submitthechanges),CT
(Choose a task), CF (Contribution flow), NC (No categories
identi/f_ied).
Submitthechanges wasthecategorywiththehighestnumberof
paragraphs perCONTRIBUTING/f_ile,appearing in2,192projects.
TheDeal with the code category represented the second highest
average of paragraphs per CONTRIBUTING /f_ile and the second in
thenumberofprojects,beingidenti/f_iedin1,660projectswithan
average of 4 paragraphs per /f_ile. Contribution Flow was the cate-
gory with the third highest frequency, appearing in 1,648 projects,
with an average of only two paragraphs per project. A similar
phenomenon happened with Build local workspace (1,162 projects;2 paragraphs/project). Talk to the community (513 projects) and
Chooseatask (332 projects) were inthe lowestpositions.
Regarding the frequency of categories per project, not all cat-
egories are covered by the CONTRIBUTING /f_iles (see Figure 8).
From our set of 2,274 OSS projects, we identi/f_ied 729 with content
relatedtofourofthesixcategories(32%),603relatedto3(27%),and
411 /f_iles containing only two categories (18%). For 287 projects, we
identi/f_iedinformationabout/f_ivecategories,andforonly65projects
(6%),theclassi/f_ieridenti/f_iedinformationaboutallsixcategories.On
thelowerbound,onlyonecategoryofinformationwasidenti/f_ied
for 165 projects. We also found 14 projects where no categories
wereidenti/f_ied.InamanualinspectionoftheirCONTRIBUTING
/f_iles,wedetectedthatnoneofthempresentanyinformationthat
couldbemappedtoanyofthesixcategories,validatingtheanalysis
made by the classi/f_ier. While some presented ways to report an
issue,otherscontainedlinks tocontribution guidelineselsewhere
(someonthe GitHubwiki,othersoutsideGitHub).
5% 4%6% 13%19%24%24%
20%6%13%19%
18%23%25%23%
20%
83%46%33%25%
20%
0100200300400500600700800
1 2 3 4 5 6Choose a Task Talk to the Community Contribution Flow
Build the Workspace Deal with the Code Submit the Changes
Figure 8: Distribution of categories per CONTRIBUTING /f_ile
predicted. The percentages represent the proportion of each
category intherespective subset of/f_iles.
Thedistributionofcategoriesisinlinewiththedistributionof
500 projects manually annotated during the qualitative analysis,
providing further evidence of the adequacy of the classi/f_ier. The
onlydiﬀerencesarethatnoprojectsanalyzedhadzerocategories
of information and the Contribution /f_low category had a slightly
higher averageofparagraphs per CONTRIBUTING /f_ile.
Insummary,morethan50%oftheCONTRIBUTING/f_ilespresent
information pertaining to fewer than 3 categories of barriers faced
by newcomers,whileonly15%present informationclassi/f_ied in5
or 6diﬀerent categories. These results—in addition to thefact that
morethan60%oftheprojectscollecteddonothaveaCONTRIBUT-
ING/f_ile(Table 1)—evidencethatthishighlyrelevantresourcefor
new contributors is still inadequate for mitigating barriers faced
by newcomers. In particular, the lack of content about Choosing
a task (CT) and Building the workspace (BW) is crucial and may
hinderonboarding andleadto dropouts[ 67,74].
AnswertoRQ2: MostCONTRIBUTING/f_ilesfocusonthe
/f_inalstagesofthecontributionprocess.Categoriescontain-
ing information such as how to submit the changes and
dealwiththecodearethemostfrequent,whileinforma-
tion about choosing a task and contacting the community
isoften missing.
24ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA F. Fronche/t_ti, D. C.Shepherd, I. Wiese, C.Treude, M. A.Gerosa,I. Steinmacher
8 DISCUSSION
Lack of essential information for newcomers. In our study,
we noticed that many projects do not provide primary information
that new contributors may need when attempting to contribute
to a project. This was highlighted in previous literature [ 71,74]
andevidencedinouranalysisbasedonthenumberofcategories
of information covered per project in Figure 8and Table 2. Most
projectshadamaximumof3outof6categoriescoveredintheir
CONTRIBUTING/f_iles.ThissuggeststhatOSSprojectsmightnot
satisfy newcomers’ needs in terms of documentation when con-
sidering the categories de/f_ined by the literature. Some of the most
critical barriers faced by newcomers [ 74,78] are not covered by
the CONTRIBUTING /f_iles. Table 2shows that only 23% of the /f_iles
analyzed had information about how to Choose a task, and 28% of
thempresentedsomeinformationabouthow toBuildtheirlocal
workspace. The“curseofexpertise”[ 24],i.e.,theinherentcognitive
bias stemming from the deep familiarity with the subject matter,
may hamper project maintainers’ ability to accurately evaluate the
comprehensiveness and clarity of their documentation. Our results
can shed light on the gaps in the existing documentation from the
perspective ofbarrierscommonly facedbynewcomers.
A more critical problem is also evidenced in Table 1, showing
that≈65%of the projects in our sample (more than 6,000 in ab-
solute numbers) do not have a CONTRIBUTING /f_ile available in
theirrepositories.Althoughsomeprojectsprefertouseotherre-
sources to explain their contributing process (e.g., Valhalla[83]
usesasectionintheirREADME/f_ile),manypopularrepositoriesdo
notcontainanyorientationfornewcomers,eventhoughtheyare
open to external submissions (e.g., Google Sanitizers [32],Microsoft
PHPSQL[50],NVIDIA NCCL [51]).
Most/f_ilesfocusonthecontributionprocess’s/f_inalsteps.
In Figure 7,weshow theaveragenumber ofparagraphsidenti/f_ied
per category in the projects of our qualitative analysis. The results
suggestthatthecategorywiththehighestnumberofparagraphsis
Submit the Changes, followed by Contribution Flow and Deal with
the Code. Although Dealing with the code focuses on the more
generalstepsoftheproject,SubmittingthechangesandDealing
with the code are intended to be relevant for newcomers in the
later stages of their contribution, after they selected a task, built
theirworkspace,andestablishedcommunicationwiththeproject’s
community.Thisresultsuggeststhatprojectstendtofocusmore
onthelaststagesofthecontribution,assumingnewcomersalready
knowhowto implement theircontribution.
Implications for practice and research. As a result of this
study, we also implemented a web tool to provide feedback to
project maintainers about their CONTRIBUTING /f_iles [ 19]. The
maintainer only needs to input their project URL, and our tool
reviews the project’s CONTRIBUTING /f_ile using our classi/f_ica-
tion model. The tool provides a chart showing the distribution
ofparagraphspercategoryofinformation,a discussionaboutthe
dominantcategories(i.e.,thehighestnumberofparagraphs)and
weak categories (i.e., the lowest number of paragraphs), and a com-
parison of the input project with other popular repositories on
GitHub.Inaddition,thetoolprovidesacleardescriptionforeach
categorywhenthereportispresentedtotheuser,highlightingwhy
theyareimportant.ThereportprovidedbythetoolalsosuggestsCONTRIBUTING /f_iles that maintainers could use as inspiration to
enrich a speci/f_ic faulty category. We envision the proposed tool as
astarting pointto support betterdocumentation /f_iles.
This tool could be particularly useful to community builders
andmanagerswhooverseeanon-trivialnumberofprojects.Those
playing these roles need information about the content of CON-
TRIBUTING/f_ilesinmultipleprojectsintheecosystemtotakeaction.
Theclassi/f_iermaysupporttheireﬀortsbyprovidinginsightsinto
the types ofinformation available for eachproject.
The tool is also an important step toward implementing auto-
mated on-demanddeveloper documentation, whichautomatically
parsesdocumentationandgeneratesresponsestouserqueries[ 66],
and smartassistants [ 16]. These tools need to parseexisting docu-
mentation and classify information in order to provide adequate
assistanceto newcomers.
From the research perspective, our study helps to understand
howthecurrentcontentofCONTRIBUTING/f_ilesaddressesnew-
comers’needs.Ourworkcanbeextendedtoevaluatethecontent
qualityof the CONTRIBUTING /f_iles, which mayhelpnewcomers
/f_indappropriatedocumentation. Futureworkcanalsoinvestigate
the subcategoriesofSteinmacheretal.’smodel[ 73].
9 LIMITATIONSAND DESIGN DECISIONS
Inthissection,wepresentourwork’slimitationsandtrade-oﬀsfor
researchdesigndecisions.
Using themost popularprojectsfrom GitHub. We focused
our study on GitHub and the results may not generalize for the
whole OSS universe. Nevertheless, GitHub is arguably the most
popularOSShostingplatform.Additionally,theselectedprojects
may not generalize to GitHub as well, since our projects were
selectedbasedontheirprogramminglanguageandpopularity. Still,
there may be projects that are not exactly software projects in our
sample,like“algorithms”and“awesomelists”—inamanualanalysis,
these projectscorrespond to ≈1% ofour sample.We acknowledge
thatamorediversesetofprojectswouldpotentiallybringmoredata
points with diﬀerent styles. However, focusing on more popular
projectsandonGitHubbringsmorecon/f_idenceabouttherelevance
of the CONTRIBUTING /f_iles analyzed. We opted to keep a more
trustfulsetofprojects,ratherthanexpandingthe data points.
Unit of analysis. Our approaches to selecting, /f_iltering, analyz-
ing,andclassifyingdocumentation/f_ileswerebasedonpriorstudies
[8,26,61]. Still, our decision to choose paragraphs as the unit of
analysisinsteadoflinesorlargerchunksoftextcouldimpactour
results. We attempted to use lines as units of analysis, but they
did not provide enough context to identify the categories during
manualanalysis since theinformation inCONTRIBUTING /f_ilesis
highly contextual. Paragraphs provide enough context for identify-
ing the categories, and Markdown provides a standard approach to
split the contentintoparagraphs (i.e.,blanklines).
OtherapproachestodeterminingthecontentofCONTRIBUT-
ING /f_iles could also be used. For example, a classi/f_ication model
basedonsectionnamescouldbeagreatalternativeforourdecision.
We decided to make our classi/f_ication based on paragraphs and
not on section names for the same reasons that we did not use
lines as units of analysis. We also decided to keep duplicated para-
graphsfromdistinctprojectsinourdataset,asCONTRIBUTING
25DoCONTRIBUTING Files ProvideInformationabout OSSNewcomers’OnboardingBarriers? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
/f_iles fromdiﬀerent projects mayfollow similar guidelines. We ran
LinearSVCwithouttheduplicatedparagraphs,andtheperformance
wassimilar to our /f_inal model(precision: 0.612, recall: 0.621).
Itisalsoworthmentioningthatweonlyanalyzedthecontent
available on the CONTRIBUTING /f_iles. We did not explore any ex-
ternallinksfromthese/f_ilesorresourcestheyreference;wealsodid
notchecktextual,HTML,.ris,orothertypesof/f_ilescontainingcon-
tributionguidelines. Weanalyzed95README/f_ilesfrom(randomly
selected)projectsthatwedismissedbecauseoftheabsenceofthe
CONTRIBUTING/f_ile;onlythreehadlinkstoexternalguidelines,
andsixhadsectionsrelatedtocontribution.Thiscouldhavelimited
the conclusions made for projects such as Apple Swift [3], and oth-
ershighlightedinSection 7,whosecontribution/f_ileonlycontained
directions to other sources of documentation. Future studies are
encouragedto(i)analyzeonelevelofdepthusingthelinksavailable
in the CONTRIBUTING /f_iles, and (ii) understand how to use the
proposedapproachtorefactorcontributingguidelinescontained
onREADMEandothertextual /f_ilesonto CONTRIBUTING /f_iles.
Representativenessofcontributors’perspective. Toassess
thequalityofourclassi/f_icationmodel,weinvitedparticipantswith
programmingexpertisetoanswerquestionsinwhichtheyjudgeda
setofpredictionsmade by ourclassi/f_ier. Although we introduceda
tutorialatthebeginningofthequestionnaire,wecannotguarantee
thattheanswersgivenbytherespondentsrepresenttheperspective
of contributors or the correctness of the predicted categories. To
mitigate this problem, we not only asked the participants to match
thecategoriesde/f_initionwiththeirnamesintheearlystagesofthe
surveybutalsoincludedanattentioncheckquestionintooursetof
questionstoensureparticipantsdidnotrandomlyassignanswers
for them. Once again, our choice was guided by the trustfulness of
the data points. We kept only a small set of answers, which can be
consideredmorereliable thanhavingmoredata pointsandlosing
reliability.
Coverage of the categories and information. We decided to
useapre-existingsetofcategoriestolabelourdatasetaccordingto
the barriersnewcomers couldface. Weacknowledgethatthecate-
goriesanalyzedmaynotcoveralltheinformationanewcomermay
needwhencontributingtoaproject.However,thesetofcategories
resulted from several studies investigating problems associated
withdocumentation/f_ilesinthecontextofOSSrepositories[ 76,77].
We opted to classify our data using a validated set of categories
ratherthanexplore potentialnewcategories.
Construction of the classi/f_ier. To build a classi/f_ication model
from scratch, a set of design decisions were made throughout
theprocess.Weunderstandthatotherstrategiescouldhavebeen
adoptedinbuildingourmodel(e.g.,theuseofadditionalpre-trained
models) and that the decisions made may have an in/f_luence on the
performance reported in this study. To mitigate this issue, we com-
paredourclassi/f_ierwithchatGPTinSection 6.1,andtrainedasuper-
visedmodelwiththesamedatasetusingFastText[ 21](precision:
0.653, recall: 0.653). Both strategies presented a similar or worse
performancethanour/f_inalclassi/f_ier.Wealsoundertookanablation
study to determine the impact of the heuristic and statistical fea-
tures. Two models were constructed using the same con/f_igurations
as our /f_inal classi/f_ier: one solely with statistical features (preci-
sion: 0.657, recall: 0.664) and the other with only heuristic features(precision: 0.414, recall: 0.493). Both models exhibited performance
comparable to,orless than,our /f_inal estimator.
10 CONCLUSION
Aprimarydocumentationresourcefornewcomersembarkingon
open source software projects is the CONTRIBUTING /f_ile. Located
within repositories, these /f_iles outline the project’s contribution
guidelines. While many OSS communities utilize CONTRIBUTING
/f_iles toorient newcomers,thecomprehensivenessoftheir content
waslargely unexplored.
Inthispaper,weinvestigatetheextenttowhichCONTRIBUT-
ING/f_ilesaddresstheonboardingbarriersnewcomersfaceinOSS
projects.Drawinguponabarriermodelfromexistingliterature[ 74],
wemanuallyanalyzedCONTRIBUTING/f_ilesfrom500projects.Our
/f_indings indicate a notable lack of information: 90% of the projects
lackedcontentinatleasttwoofthesixinformationcategories,with
79% missing details in three or more categories. Notably, our man-
ualreviewrevealedthatover75%oftheprojectsfailedtoinclude
guidanceontaskselectionandworkspacesetup,twokeybarriers
for newcomers as highlightedbySteinmacheretal.[ 74].
Wealsobuiltamachinelearningmodeldesignedtoautomatically
classify the information from CONTRIBUTING /f_iles from other
projects and thereby help projects identify missing information in
their/f_iles.Overall,theclassi/f_ierperformedwellinthismulticlass
problem, with an overall precision of 0.655 and a recall of 0.662.
The performance was good for four out of the six categories of
information (F1 ≥0.61): Build local workspace, Deal with the code,
Talktothecommunity,andSubmitthechanges.Exceptionswere
HowtochooseataskandContributing/f_low,withlowrecall( <0.3)
andF1 of0.379 and0.345, respectively.
In summary, our /f_indings indicate that many OSS projects need
to improve the comprehensiveness of their ‘CONTRIBUTING’ /f_iles
tobettercatertonewcomers.Evaluating2,274projectsusingour
machinelearningmodel, our resultsechoed the /f_indingsfrom our
qualitativeassessment:84%oftheprojectslackedcontentinatleast
two of the six information categories, and 52% were de/f_icient in
threeormorecategories.Toassistwiththisissue,wedevelopedan
online tool designed to oﬀer feedback to project maintainers about
howtheir‘CONTRIBUTING’/f_iles addressonboardingchallenges,
ensuringthatthecommunitiesarebetterequippedtowelcomeand
nurture theirnextgenerationof contributors.
11 DATA AVAILABILITY
The artifacts usedinthis paper are available onZenodo [ 20].
ACKNOWLEDGMENTS
This work is partially supported by FAPESP (grant #2019/12743-
4), CNPq/MCTI/FNDCT (#408812/2021-4), MCTIC/CGI/FAPESP
(#2021/06662-1),andNSF(2236198,2247929,2024561,and2303042).
REFERENCES
[1]EmadAghajani,CsabaNagy,OlgaLuceroVega-Márquez,MarioLinares-Vásquez,
Laura Moreno, Gabriele Bavota, and Michele Lanza. 2019. Software documenta-
tion issues unveiled. In 2019 IEEE/ACM 41st International Conference on Software
Engineering (ICSE) . IEEE,1199–1210.
[2]Amazon.2023. AmazonMechanicalTurk(Website). https://www.mturk.com/
[Accessed onAug-2023].
26ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA F. Fronche/t_ti, D. C.Shepherd, I. Wiese, C.Treude, M. A.Gerosa,I. Steinmacher
[3]Apple. 2023. Apple Swift (CONTRIBUTING.md). https://github.com/apple/
swift/blob/main/CONTRIBUTING [Accessed onAug-2023].
[4]Tasneem Batool, Mostafa Abuelnoor, Omar El Boutari, Fadi Aloul, and Assim
Sagahyroon.2021. PredictingHospitalNo-ShowsUsingMachineLearning.In
2020IEEEInternationalConferenceonInternetofThingsandIntelligenceSystem
(IoTaIS). 142–148. https://doi.org/10.1109/IoTaIS50849.2021.9359692
[5]Ismaïl Biskri and Sylvain Delisle. 2002. Text classi/f_ication and multilinguism:
Getting at words via n-grams of characters. In Proceedings of the 6th World
Multiconference on Systemics, Cybernetics and Informatics (SCI-2002), Orlando
(Florida, USA) , Vol. 5.110–115.
[6]Giuseppe Bonaccorso. 2017. 12.2.4.2 Tf-idf Vectorizing. In Machine Learning
Algorithms . Packt Publishing.
[7]Giuseppe Bonaccorso. 2017. 2.1.1.1 One-vs-All. In Machine Learning Algorithms .
Packt Publishing.
[8]Hudson Borges, Andre Hora, and Marco Tulio Valente. 2016. Understanding
the factors that impact the popularity of GitHub repositories. In 2016 IEEE In-
ternational Conference on Software Maintenance and Evolution (ICSME) . IEEE,
334–344.
[9]Norman M Bradburn, Seymour Sudman, and Brian Wansink. 2004. Asking ques-
tions: the de/f_initive guide to questionnaire design–for market research, political
polls,and social and health questionnaires . John Wiley& Sons.
[10]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
PrafullaDhariwal,ArvindNeelakantan, Pranav Shyam, GirishSastry,Amanda
Askell,etal .2020. Languagemodelsarefew-shotlearners. Advancesinneural
information processingsystems 33(2020), 1877–1901.
[11]Seung Youn Chyung, Katherine Roberts, IevaSwanson, and Andrea Hankinson.
2017. Evidence-basedsurveydesign:TheuseofamidpointontheLikertscale.
Performance Improvement 56,10(2017), 15–23.
[12]Giuseppe Ciaburro and Prateek Joshi. 2019. 2.9.4 There’s More... In Python
MachineLearning Cookbook (2nd Edition) . Packt Publishing.
[13]JacobCohen.1960. Acoeﬃcientofagreementfornominalscales. Educational
and psychological measurement 20,1 (1960), 37–46.
[14]Maria Eduarda Rosa da Silva, Giovani Gracioli, and Gustavo Medeiros de Araujo.
2022. FeatureSelectioninMachineLearningforKnockingNoisedetection.In
2022XIIBrazilianSymposiumonComputingSystemsEngineering(SBESC) .1–8.
https://doi.org/10.1109/SBESC56799.2022.9964726
[15]Edson Dias, Paulo Meirelles, Fernando Castor, Igor Steinmacher, Igor Wiese, and
GustavoPinto.2021. Whatmakesagreatmaintainerofopensourceprojects?.
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE) .
IEEE,982–994.
[16]James Dominic, Jada Houser, Igor Steinmacher, Charles Ritter, and Paige
Rodeghero. 2020. Conversational bot for newcomers onboarding to open source
projects.In ProceedingsoftheIEEE/ACM42ndInternationalConferenceonSoftware
Engineering Workshops . 46–50.
[17]Günhan Dündar and Mustafa Berke Yelten. 2020. 3.6.2 Resampling. In Modelling
MethodologiesinAnalogueIntegratedCircuitDesign .InstitutionofEngineering
and Technology.
[18]Omar Elazhary, Margaret-Anne Storey, Neil Ernst, and Andy Zaidman. 2019. Do
asido,notasisay:Docontributionguidelinesmatchthegithubcontribution
process?. In 2019 IEEE International Conference on Software Maintenance and
Evolution(ICSME) . IEEE,286–290.
[19]Fronchettietal.2023. ContributingFiles(Website). https://contributing.streamlit.
app/[Accessed onAug-2023].
[20]Fronchetti et al. 2023. Replication Package (Zenodo Repository). https://zenodo.
org/record/8270217 [Accessed onAug-2023].
[21]Facebook. 2023. FastText (Website). https://fasttext.cc/ [Accessed on Aug-2023].
[22]FabianFagerholm,AlejandroSanchezGuinea,JayBorenstein,andJürgenMünch.
2014. Onboarding in open source projects. IEEE Software 31,6 (2014), 54–61.
[23]Fabian Fagerholm, Alejandro S Guinea, Jürgen Münch, and Jay Borenstein. 2014.
The role of mentoring and project characteristics for onboarding in open source
softwareprojects.In Proceedingsofthe8thACM/IEEEinternationalsymposium
onempirical softwareengineering and measurement . 1–10.
[24]Matthew Fisher and Frank C Keil. 2016. The curse of expertise: When more
knowledge leads to miscalibrated explanatory insight. Cognitive science 40, 5
(2016), 1251–1269.
[25]Karl Fogel. 2009. How To Run A Successful Free Software Project - Producing Open
SourceSoftware . CreateSpace, Scotts Valley, CA.
[26]Felipe Fronchetti, Igor Wiese, Gustavo Pinto, and Igor Steinmacher. 2019. What
attracts newcomers to onboard on oss projects? tl; dr: Popularity. In IFIP Interna-
tional Conference onOpenSourceSystems . Springer, 91–103.
[27]DavideFucci,AlirezaMollaalizadehbahnemiri,andWalidMaalej.2019. Onusing
machine learning to identify knowledge in API reference documentation. In
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering . 109–119.
[28]Johannes Fürnkranz and Peter A Flach. 2003. An analysis of rule evaluation
metrics.In Proceedingsofthe20thinternationalconferenceonmachinelearning
(ICML-03) . 202–209.[29]GitHub.2020.GitHubOctoverse. https://octoverse.github.com/credits/ [Accessed
onJun-2023].
[30]GitHub. 2022. GitHub Flavored Markdown Specs Paragraphs. https://github.
com/gfm/#paragraphs [Accessed onJun-2023].
[31]GitHub. 2022. Setting guidelines for repository contributors. https:
//docs.github.com/en/communities/setting-up-your-project-for-healthy-
contributions/setting-guidelines-for-repository-contributors [accessed on
Jun-2023].
[32]Google.2023. GoogleSanitizers(GitHubRepository). https://github.com/google/
sanitizers [Accessed onAug-2023].
[33]Kazi Amit Hasan, Marcos Macedo, Yuan Tian, Bram Adams, and Steven Ding.
2023. Understanding the Time to First Response In GitHub Pull Requests. In Intl.
Conference onMiningSoftwareRepositories(MSR 2023) .
[34]HideakiHata,TaikiTodo,SayaOnoue,andKenichiMatsumoto.2015. Charac-
teristics of sustainable oss projects: A theoretical and empirical study. In 2015
IEEE/ACM 8th International Workshop on Cooperative and Human Aspects of
SoftwareEngineering . IEEE,15–21.
[35]Helena Holmstrom, Eoin Ó Conchúir, J Agerfalk, and Brian Fitzgerald. 2006.
Globalsoftwaredevelopmentchallenges:Acasestudyontemporal,geographi-
calandsocio-culturaldistance.In 2006IEEEInternationalConferenceonGlobal
SoftwareEngineering (ICGSE’06) . IEEE,3–11.
[36]Mohammad Hossin and Md Nasir Sulaiman. 2015. A review on evaluation
metrics for data classi/f_ication evaluations. International journal of data mining &
knowledgemanagementprocess 5,2 (2015), 1.
[37]Ammar Ismael Kadhim. 2018. An Evaluation of Preprocessing Techniques for
Text Classi/f_ication. International Journal of Computer Science and Information
Security16,6 (2018).
[38]Frank Kane. 2017. 9.7 TF-IDF. In Hands-on Data Science and Python Machine
Learning. Packt Publishing.
[39]Jacob Krüger, Sebastian Nielebock, and Robert Heumüller. 2020. How Can I
Contribute? A Qualitative Analysis of Community Websites of 25 Unix-Like Dis-
tributions.In ProceedingsoftheEvaluationandAssessmentinSoftwareEngineering .
324–329.
[40]Imbalanced Learn. 2023. Imbalanced Learn (Website). https://imbalanced-
learn.org/stable/ [Accessed onAug-2023].
[41]AmandaLee,JeﬀreyCCarver,andAmiangshuBosu.2017. Understandingthe
impressions,motivations,andbarriersofonetimecodecontributorstoFLOSS
projects:asurvey.In 2017IEEE/ACM39thInternationalConferenceonSoftware
Engineering (ICSE) . IEEE,187–197.
[42]Timothy C Lethbridge, Janice Singer, and Andrew Forward. 2003. How software
engineersusedocumentation:Thestateofthepractice. IEEEsoftware 20,6(2003),
35–39.
[43]Jiawei Li and Iftekhar Ahmed. 2023. Commit Message Matters: Investigating
Impactand Evolutionof CommitMessage Quality. (2023).
[44]Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit.
InProceedingsoftheACL-02WorkshoponEﬀectiveToolsandMethodologiesfor
Teaching Natural Language Processing and Computational Linguistics - Volume
1(Philadelphia,Pennsylvania) (ETMTNLP’02) .AssociationforComputational
Linguistics,USA,63–70.
[45]Yuzhan Ma, Sarah Fakhoury, Michael Christensen, Venera Arnaoudova, Waleed
Zogaan,andMehdiMirakhorli.2018.Automaticclassi/f_icationofsoftwareartifacts
in open-source applications. In 2018 IEEE/ACM 15th International Conference on
MiningSoftwareRepositories(MSR) . IEEE,414–425.
[46]Ke Mao, Licia Capra, Mark Harman, and Yue Jia. 2017. A survey of the use of
crowdsourcing in software engineering. Journal of Systems and Software 126
(2017), 57–84.
[47]Gerardo Matturro, Karina Barrella, and Patricia Benitez. 2017. Diﬃculties of
newcomers joining software projects already in execution. In 2017 International
ConferenceonComputationalScienceandComputationalIntelligence(CSCI) .IEEE,
993–998.
[48]Christopher Mendez, Hema Susmita Padala, Zoe Steine-Hanson, Claudia Hilder-
brand, Amber Horvath, Charles Hill, Logan Simpson, Nupoor Patil, Anita Sarma,
andMargaretBurnett.2018. Opensourcebarrierstoentry,revisited:Asociotech-
nicalperspective.In Proceedingsofthe40thInternationalconferenceonsoftware
engineering . 1004–1015.
[49]MichaelMeng,StephanieSteinhardt,andAndreasSchubert.2018. Application
programming interface documentation: what do software developers want?
Journal ofTechnicalWritingand Communication 48,3 (2018), 295–330.
[50]Microsoft.2023. MicrosoftPHPSQL(GitHubRepository). https://github.com/
microsoft/msphpsql [Accessed onAug-2023].
[51]NVIDIA.2023. NVIDIANCCL(GitHubRepository). https://github.com/NVIDIA/
nccl[Accessed onAug-2023].
[52]Fred Nwanganga and Mike Chapple. 2020. 9.1.1.1 k-Fold Cross-Validation. In
PracticalMachineLearning inR . John Wiley& Sons.
[53]Open Source Guides. 2022. Open Source Guides – Starting an Open Source
Project. https://opensource.guide/starting-a-project/ [Accessed onJun-2023].
[54]OpenAI. 2023. ChatGPT (Website). https://chat.openai.com/ [Accessed on
Aug-2023].
27DoCONTRIBUTING Files ProvideInformationabout OSSNewcomers’OnboardingBarriers? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
[55]Susmita Hema Padala, Christopher John Mendez, Luiz Felipe Dias, Igor Stein-
macher, Zoe Steine Hanson, Claudia Hilderbrand, Amber Horvath, Charles Hill,
Logan Dale Simpson, Margaret Burnett, et al .2020. How gender-biased tools
shape newcomer experiences in oss projects. IEEE Transactions on Software
Engineering (2020).
[56]SebastianoPanichella,AndreaDiSorbo,EmitzaGuzman,CorradoAVisaggio,
GerardoCanfora,andHaraldCGall.2015.Howcaniimprovemyapp?classifying
user reviews for software maintenance and evolution. In 2015 IEEE international
conference onsoftwaremaintenance and evolution(ICSME) . IEEE,281–290.
[57]Yunrim Park and Carlos Jensen. 2009. Beyond pretty pictures: Examining the
bene/f_its of code visualization for open source newcomers. In 2009 5th IEEE
InternationalWorkshoponVisualizingSoftwareforUnderstandingandAnalysis .
IEEE,3–10.
[58]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau,M.Brucher,M.Perrot,andE.Duchesnay.2011. Scikit-learn:Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[59]GustavoPinto,IgorSteinmacher,andMarcoAurélioGerosa.2016. Morecommon
than you think: An in-depth study of casual contributors. In 2016 IEEE 23rd
internationalconferenceonsoftwareanalysis,evolution,andreengineering(SANER) ,
Vol. 1.IEEE,112–123.
[60]LucaPonzanelli,GabrieleBavota,AndreaMocci,RoccoOliveto,Massimiliano
Di Penta, Sonia Haiduc, Barbara Russo, and Michele Lanza. 2017. Automatic
identi/f_icationandclassi/f_icationofsoftwaredevelopmentvideotutorialfragments.
IEEE Transactions onSoftwareEngineering 45,5 (2017), 464–488.
[61]Gede ArthaAzriadi Prana, ChristophTreude, Ferdian Thung, Thushari Atapattu,
andDavidLo.2019. CategorizingthecontentofGitHubREADME/f_iles. Empirical
SoftwareEngineering 24,3 (2019), 1296–1327.
[62]PoojaRani,SebastianoPanichella,ManuelLeuenberger,AndreaDiSorbo,and
Oscar Nierstrasz. 2021. How to identify class comment types? A multi-language
approachforclasscommentclassi/f_ication. Journalofsystemsandsoftware 181
(2021), 111047.
[63]BaishakhiRay,DarylPosnett,VladimirFilkov,andPremkumarDevanbu.2014.
A large scale study of programming languages and code quality in github. In
Proceedings of the 22nd ACM SIGSOFT international symposium on foundations of
softwareengineering . 155–165.
[64]Brittany Reid, Markus Wagner, Marcelo d’Amorim, and Christoph Treude. 2022.
SoftwareEngineeringUserStudyRecruitmentonProli/f_ic:AnExperienceReport.
arXiv preprint arXiv:2201.05348 (2022).
[65]Martin P Robillard and Yam B Chhetri. 2015. Recommending reference API
documentation. EmpiricalSoftwareEngineering 20,6 (2015), 1558–1586.
[66]MartinP Robillard,Andrian Marcus, ChristophTreude,Gabriele Bavota, Oscar
Chaparro, Neil Ernst, Marco AurélioGerosa,Michael Godfrey, Michele Lanza,
Mario Linares-Vásquez, et al .2017. On-demand developer documentation. In
2017IEEEInternationalconferenceonsoftwaremaintenanceandevolution(ICSME) .
IEEE,479–483.
[67]Fabio Santos, Bianca Trinkenreich, João Felipe Pimentel, Igor Wiese, Igor Stein-
macher, Anita Sarma, and Marco A Gerosa. 2022. How to choose a task? Mis-
matchesinperspectivesofnewcomersandexistingcontributors.In International
SymposiumonEmpiricalSoftwareEngineering and Measurement (ESEM) .
[68]CJ Satish and M Anand. 2016. Software documentation managementissues and
practices: A survey. Indian Journal ofScienceand Technology 9,20(2016), 1–7.
[69]Scikit-learn. 2023. Cross-validation: evaluating estimator performance (Docu-
mentation). https://scikit-learn.org/stable/modules/generated/sklearn.feature_
selection.SelectPercentile.html [Accessed onJun-2023].
[70]FrancescoSetragno,MassimilianoZanoni,AugustoSarti,andFabioAntonacci.
2017. Feature-based characterization of violin timbre. In 2017 25th European
Signal Processing Conference (EUSIPCO) . 1853–1857. https://doi.org/10.23919/EUSIPCO.2017.8081530
[71]Dan Sholler, Igor Steinmacher, Denae Ford, Mara Averick, Mike Hoye, and Greg
Wilson.2019. Tensimplerulesforhelpingnewcomersbecomecontributorsto
open projects. PLoS computational biology 15,9 (2019), e1007296.
[72]Spacy.2023. RuleBasedMatching(Documentation). https://spacy.io/usage/rule-
based-matching [Accessed onAug-2023].
[73]Igor Steinmacher, Tayana Conte, Marco Aurélio Gerosa, and David Redmiles.
2015. Social barriers faced by newcomers placing their /f_irst contribution in open
source software projects. In Proceedings of the 18th ACM conference on Computer
supportedcooperativework & social computing . 1379–1392.
[74]IgorSteinmacher,TayanaUchoaConte,ChristophTreude,andMarcoAurélio
Gerosa. 2016. Overcoming Open Source Project Entry Barriers with a Portal for
Newcomers. In ICSE’16.Association for Computing Machinery, New York,NY,
USA,273–284.
[75]IgorSteinmacher,GustavoPinto,IgorScalianteWiese,andMarcoAGerosa.2018.
Almost there: A study on quasi-contributorsin open source software projects. In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering .256–266.
[76]IgorSteinmacher,MarcoAurélioGraciottoSilva,andMarcoAurélioGerosa.2014.
Barriers faced by newcomers to open source projects: a systematic review. In
IFIPInternationalConference onOpenSourceSystems . Springer, 153–163.
[77]Igor Steinmacher, Marco Aurelio Graciotto Silva, Marco Aurelio Gerosa, and
David F Redmiles. 2015. A systematic literature review on the barriers faced by
newcomerstoopensourcesoftwareprojects. InformationandSoftwareTechnology
59(2015), 67–85.
[78]Igor Steinmacher, Christoph Treude, and Marco Aurelio Gerosa. 2018. Let me in:
Guidelines for the successful onboarding of newcomers to opensource projects.
IEEE Software 36,4 (2018), 41–49.
[79]Kathryn T Stolee and Sebastian Elbaum. 2010. Exploring the use of crowdsourc-
ingtosupportempiricalstudiesinsoftwareengineering.In Proceedingsofthe
2010ACM-IEEEinternationalsymposiumonEmpiricalsoftwareengineeringand
measurement . 1–4.
[80]Xin Tan, Yiran Chen, Haohua Wu, Minghui Zhou, and Li Zhang. 2023. Is It
Enough to Recommend Tasks to Newcomers? Understanding Mentoring on
Good First Issues. arXiv preprint arXiv:2302.05058 (2023).
[81]Xin Tan, Minghui Zhou, and Zeyu Sun. 2020. A /f_irst look at good /f_irst issues
onGitHub.In Proceedingsofthe28thACMJointMeetingonEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
398–409.
[82]Jalaj Thanaki.2017. 5.3.4.1Understanding TF-IDF. In Python Natural Language
Processing . Packt Publishing.
[83]Valhalla.2023.Valhalla(GitHubRepository). https://github.com/valhalla/valhalla
[Accessed onAug-2023].
[84]Bogdan Vasilescu, Alexander Serebrenik, Prem Devanbu, and Vladimir Filkov.
2014. How social Q&A sites are changing knowledge sharing in open source
softwarecommunities.In Proceedingsofthe17thACMconferenceonComputer
supportedcooperativework & social computing . 342–354.
[85]SathiyamoorthiVelayutham.2020. 3.5.1Precision. In HandbookofResearchon
Applicationsand ImplementationsofMachineLearning Techniques . IGIGlobal.
[86]SVijayarani,MsJIlamathi,andMsNithya.2015. Preprocessingtechniquesfor
text mining-an overview. International Journal of Computer Science & Communi-
cation Networks 5,1 (2015), 7–16.
[87]AprilYiWang,DakuoWang,JaimieDrozdal,MichaelMuller,SoyaPark,JustinD
Weisz,XuyeLiu,LingfeiWu,andCaseyDugan.2022. DocumentationMatters:
Human-Centered AI System to Assist Data Science Code Documentation in
ComputationalNotebooks. ACMTransactionsonComputer-HumanInteraction
29,2 (2022), 1–33.
Received 2023-02-02; accepted 2023-07-27
28