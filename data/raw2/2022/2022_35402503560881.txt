Discrepancies among Pre-trained Deep Neural Networks:
A New ThreattoModel ZooReliability
DiegoMontes
PurdueUniversity, USA
montes10@purdue.eduPongpatapeePeerapatanapokin
PurdueUniversity, USA
ppeerapa@purdue.eduJeff Schultz
PurdueUniversity, USA
schul203@purdue.edu
ChengjunGuo
PurdueUniversity, USA
guo456@purdue.eduWenxinJiang
PurdueUniversity, USA
jiang784@purdue.eduJamesC.Davis
PurdueUniversity, USA
davisjam@purdue.edu
ABSTRACT
Trainingdeepneuralnetworks(DNNs)takessignificanttimeand
resources.Apracticeforexpediteddeploymentistousepre-trained
deepneuralnetworks(PTNNs),oftenfrommodelzoosÐcollections
ofPTNNs;yet,thereliabilityofmodelzoosremainsunexamined.In
theabsenceofanindustrystandardfortheimplementationandper-
formanceofPTNNs,engineerscannotconfidentlyincorporatethem
intoproductionsystems.Asafirststep,discoveringpotentialdis-
crepanciesbetweenPTNNsacrossmodelzooswouldrevealathreat
tomodelzooreliability. Prior works indicatedexistingvariances in
deeplearningsystemsintermsofaccuracy.However,broadermea-
suresofreliabilityforPTNNsfrommodelzoosareunexplored.This
work measures notable discrepancies between accuracy, latency,
and architecture of 36 PTNNs across four model zoos. Among the
top10discrepancies,wefinddifferencesof1.23%ś2.62%inaccuracy
and9%ś131%inlatency.Wealsofindmismatchesinarchitecture
for well-known DNN architectures ( e.g.,ResNet and AlexNet). Our
findingscallforfutureworksonempiricalvalidation,automated
tools formeasurement,andbest practices forimplementation.
CCSCONCEPTS
·Softwareanditsengineering →Reusability ;·Computing
methodologies →Neural networks .
KEYWORDS
Neuralnetworks,Modelzoos,Software reuse,Empiricalsoftware
engineering
ACM ReferenceFormat:
Diego Montes, Pongpatapee Peerapatanapokin, Jeff Schultz, Chengjun Guo,
WenxinJiang,andJamesC.Davis.2022.DiscrepanciesamongPre-trained
Deep Neural Networks: A New Threat to Model Zoo Reliability. In Pro-
ceedingsofthe30thACMJointEuropeanSoftwareEngineeringConference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’22),
November 14ś18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
5pages.https://doi.org/10.1145/3540250.3560881
1 INTRODUCTION
With the growing energy consumption of training DNNs [ 26], tak-
ingadvantageofthere-usabilityofPTNNscansignificantlyreduce
ESEC/FSE ’22,November 14ś18, 2022, Singapore, Singapore
©2022 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3560881the costs of training [ 13]. In particular, transfer learning can result
in shorter training times and higher asymptotic accuracies com-
pared to other weight initialization methods [ 22,36]. This kind
of technique accelerates model reuse and development. The his-
tory of PTNNs and their impact on the development of artificial
intelligence has been extensively documented [ 13,25]. As such,
collectionsofPTNNshavebeencreated,referredtoas modelzoos .
Notably,maintainersofpopularmachinelearningframeworks,such
as TensorFlow [ 2], maintain corresponding model zoos developed
with theirframework, suchasthe TensorFlowModel Garden[ 38].
There are many model zoos [ 1,18,23,38] and an expanding use
ofPTNNsinproductionsystems[ 13].Pastworkhasemphasizedthe
difficultiesin adopting software engineering practicesin machine
learning,andspecifically,thechallengeswithreproducingmachine
learningresearchpapers[ 4,17].Thesereproducibilityissuesmay
affect PTNNs, leading to variations across model zoos [ 28]. Dis-
paritiesintheaccuracy,latency,orarchitectureofaPTNNcould
negativelyaffectadeeplearningsystem,threateningPTNNs’reuse
potential. Consider a model zoo that has an incorrect implemen-
tation of a well-known DNN architecture, which has increased its
latency significantly. If an engineer were to use the PTNN from
this zoo, they would unknowingly be receiving a lower quality
PTNN than they might otherwise have from a different model
zoo. Theengineer’s efforttoenable aquickturnaroundtime with
a PTNN would have become harmful. Discovering discrepancies
wouldshine alight on the reliability ofmodelzoos.
Toexplorethereliabilityofmodelzoos,weperformedameasure-
ment study to identify discrepancies among 36 image classification
PTNN architectures across four model zoos: TensorFlow Model Gar-
den(TFMG) [38],ONNX Model Zoo (ONNX) [ 1],Torchvision Models
(Torchvision) [ 23], andKeras Applications (Keras) [18]. The PTNNs
weremeasuredalongthreedimensions:accuracy,latency,andarchi-
tecture. We find the differences in accuracies on ILSVRC-2012-CLS
dataset (ImageNet) can be as large as 2.62% [ 10].1Similarly, over
20% of the PTNNs measured had latency differences (FLOPs) of
10% or more when comparing PTNNs of the same name across the
model zoos. Lastly, we discover architectural differences in several
PTNNs,includingimplementationsof AlexNetandResNetV2 .We
concludewithanagendaforfutureresearchonfurtherempirical
validation,automatedtoolsformeasurement,andbestpracticesfor
implementingmodelzooPTNNs.
1TheILSVRC-2012-CLS image dataset has 50,000 validation images. A 1% accuracy
differenceis equivalent to500 images.
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1605
ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore DiegoMontes, Pongpatapee Peerapatanapokin, JeffSchultz,ChengjunGuo, Wenxin Jiang,andJames C.Davis
2 BACKGROUNDAND RELATED WORK
PTNNs are applied in a wide variety of domains [ 13]. With the
demand for engineers far exceeding supply [ 32], companies are
lookingforbestpracticesthatcanboosttheproductivityoftheiren-
gineers. Major companies ( e.g.,Google and Microsoft) have shared
best practices on machine learning development and informed
futuredirectionsonmodelreuse[ 3,8].AcasestudyfromSAPindi-
catespossiblecompatibility,portability,andscalabilitychallenges
in machine learning model deployment, which may affect their
performance [ 30]. There have been many efforts to improve the
quality of model zoos. For example, IBM has developed a tool to
extract model metadata [ 37] to support better model management.
Bannaet al.promote best practices for reproducing and publishing
high-quality PTNNs [ 4]. However, the reliability of model zoos has
not been validatedbyprior works.
The ability to replicate the accuracy of a DNN in identical train-
ing environments is hindered by non-deterministic factors. Ac-
curacy differences of up to 10.8%, stemming purely from non-
determinism, have been reported with popularDNN architectures
[28].Closelyrelated,researchhasinvestigatedandbenchmarked
theperformancevariancestiedtodeeplearningframeworks[ 21,
33].Thisvariabilitythreatensthereliabilityofnewdeeplearning
techniques. As such, automated variance testing [ 27] has been pro-
posedtoassurethevalidityofthesecomparisons.However,PTNNs
in model zoos may also suffer from varying architectural imple-
mentations, affecting more than just accuracy. Our work measures
the disparities in PTNNs across different model zoos as opposed
toattemptingtoimprovethestandardinjustone[ 4].Ourresults
enlighten future works validating the quality and promoting the
standardizationofmodelzoos.
3 METHODOLOGY
We perform a measurement study to assess our problem statement:
whether discrepancies exist between the accuracy, latency, and
architecture ofPTNNs acrossdifferentmodelzoos.
3.1 Subjects
Amodel zoo is a collection of PTNNs for various tasks. We carry
out a selection process for four model zoos. Our selection crite-
ria included the model zoobeing maintainedalongside amachine
learning framework: this increases the likelihood of the model zoo
beingactivelymaintained.Furthermore,toensurethepopularityof
the model zoo, the zoo must have a public GitHub repository with
at least three thousandstars [ 7]. Using GitHub search2to identify
potentialmodelzoocandidates,11modelzooswereselectedthat
metthecriteria.3ThePTNNswithinthe11modelzooswerecat-
egorized into deep learning tasks, including image classification,
object detection,and naturallanguage processing.We focusedon
image classification models because it is the most common type in
8ofthe 11 modelzoos.
APTNN availabilityanalysiswasdone onthe candidatemodel
zoostoassesshowmanymodelzoosofferedthesameimageclassifi-
cationPTNNarchitectures.Basedonthelargestsharedavailability,
we chose TensorFlow Model Garden ,ONNX ModelZoo ,Torchvision
2https://github.com/search
3The 11 identified potential model zoos are as follows: TensorFlow Model Garden,
ONNXModel Zoo, Torchvision Models, KerasApplications, TensorFlow ModelHub,
PyTorchModelZoo,MXNetModelZoo,GluonModelZoo,Deeplearning4jModelZoo,
Caffe ModelZoo, and OpenVINOModelZoo.
Figure1:Overviewofthemeasurementprocess.Wegather
PTNNsfromthemodelzooswiththesamename,perform
measurementsoneachPTNN,andcomparefordiscrepancies.
Models, andKeras Applications . Within these model zoos, we se-
lected all the image classification PTNN architectures that were
present in at least two of the four model zoos, yielding 36 PTNN
architectures.TheselectedPTNNsareeitherdirectlydownloadable
from the model zoos’ GitHub repositories or can be pulled using
the modelzoos’ APIs.
3.2 EvaluationMetrics
Accuracy. Image classification DNNs’ effectiveness is measured in
accuracy, which is a critical component of a PTNN. We are measur-
ingdiscrepanciesbetweentheclaimsofmodelzoos asopposed to
verifying them. Top-1 accuracy is the conventional accuracy where
model prediction must exactly match the expected label, while
top-5 accuracy measures the fraction of images where any of the
topfivepredictedlabelsmatchesthetargetlabel[ 9,10].35image
classificationPTNNarchitecturesreportedtop-1ImageNetclassi-
fication accuracies, meanwhile only 32 reported top-5 ImageNet
classification accuracies.
Latency. The latency of a DNN is a key factor that engineers
consider[ 11].Forexample, MobileNet isaDNNimageclassification
architecture thatprioritizeslow latencyonmobile and embedded
systems [ 16]. We used open-source tools [ 5,34,35] to measure
thelatencybycountingthefloatingpointoperations(FLOPs)[ 6].
FLOPsareframeworkandhardware-agnostic,allowingforunbiased
comparisons.
Architecture. PTNNsaretrainedweightsbasedonresearchpapers
that propose DNN architectures. As a result, model zoos advertise
PTNNs by their architecture name. The observed accuracy dif-
ferences and past work on DNN vulnerabilities motivated us to
examinearchitecture[ 12].Qualitativeobservationsofdiscrepan-
ciesinthedescriptions,sourcecode,andvisualizationsofPTNN
architectureswereemployed.Specifically,netron,anopen-source
neural network visualizer, was used to inspect the architecture
of the PTNNs [ 31]. However, not all neural network weight for-
mats are supported, so all PTNNs were converted to the ONNX
format for architectural analysis using an appropriate tool for each
framework [ 24,29]. The source code for the implementations of
the PTNNs are present in the model zoos’ GitHub repositories and
wasusedas an additionalform of PTNN inspection.
1606Discrepanciesamong Pre-trainedDeep Neural Networks: A NewThreat to Model Zoo Reliability ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
Table 1: Frequency at which each model zoo had the most or
least accurate modelordered by highesttop-1 accuracy.
Highest Top-1 Lowest Top-1 Highest Top-5 Lowest Top-5
TorchvisionModels 48% 41% 52% 36%
TF Model Garden 40% 33% 36% 43%
KerasApplications 37% 44% 36% 40%
ONNXModel Zoo 35% 41% 31% 44%
4 RESULTS AND ANALYSIS
4.1 Accuracy
We compared the top-1 accuracy of 35 PTNN architectures and
the top-5 accuracy of 32 PTNN architectures by using ImageNet.
Notably,12ofthe35profiledPTNNarchitectureshadtop-1accu-
racydifferencesgreaterthan0.96%.Fortop-5accuracies,6ofthe32
PTNNarchitectureshaddifferencesgreaterthan0.94%.Thelarge
differencespresentinFigure 2havesignificantconsequences.For
example, ResNet V1 152 from Keras is noticeably less accurate than
thePTNNbythesamenamefromTorchvision,with top-1accura-
ciesof76.6%and78.31%,respectively.Thisdifferenceispronounced
enough that ResNet V1 101 from Torchvsion with top-1 accuracy of
77.37%ismore accuratethan ResNetV1 152 from Keras.4
Figure 2: Top 10 largest top-1 accuracy differences. For a
PTNNarchitecture,theaccuracyofthePTNNwiththelowest
reported top-1 accuracy is subtracted from that of the PTNN
with thelargest top-1 accuracy.
Table1shows the aggregation of accuracy differences across
model zoos, highlighting how often a model zoo had the highest
or lowest top-1 or top-5 accuracy for a given PTNN architecture.
As seen, 48% of the PTNNs that were available on Torchvision had
the highest top-1 accuracy among the model zoos. On the other
hand,Kerashadthelowesttop-1accuracy44%ofthetimeforits
selection ofPTNNs.
4.2 Latency
36 PTNN architectures were measured for their FLOPs. Figure 3
showsthatthereare8PTNNarchitectureswherethePTNNwiththe
highestamountofFLOPshadgreaterthan10%moreFLOPsthanthe
4ResNet V1 101 was originally reported to be 0.32% less accurate than ResNet V1
152[14].
Figure3:Top10largestFLOPsdifferences.ForaPTNNarchi-
tecture, the FLOP count of the PTNN with the most FLOPs is
divided by theFLOPsofthe PTNNwith the fewest.
PTNNwiththelowestFLOPcount.Attheextreme,Torchvision’s
SqueezeNet1.0 ,sittingat819.08millionFLOPs,had2.31 ×theFLOPs
of ONNX’s SqueezeNet 1.0 . Likewise, the three PTNN architectures
from the ResNet V2 family all had greater than 85% more FLOPs
thanthelowestFLOPsPTNN. AllthehighFLOP-count ResNetV2
come from TFMG.
We discussthe possibleexplanations forthe FLOPsdifferences
seeninFigure 3.ThehighFLOPsdifferencemeasuredin SqueezeNet
1.0can be explained by looking at its successor, SqueezeNet 1.1 .
SqueezeNet1.1 isadvertisedbyONNXtocontain2.4 ×lesscomputa-
tion than the former. However, SqueezeNet 1.1 from ONNX has the
samenumberofmeasuredFLOPsasthe 1.0PTNNoffered.ONNX
hasbeenadvertising SqueezeNet1.1 asits1.0counterpart.Similarly,
looking at the ResNet V2 from TFMG: a primary contributor to the
large amount of FLOPs is the input shape. ResNet V2 architectures,
accordingtotheoriginpaper,accept224 ×224inputs[ 15];however,
TFMGstatesthatthe ResNetV2 PTNNsitprovidesuseInception
pre-processing and an input image size of 299 ×299. A trade-off
betweenaccuracyandthroughput,FLOPs,waspotentiallymade
here bythe modelzoomaintainers.
Across all FLOP-counted PTNNs, Torchvision had the highest
FLOPsPTNNsfor78%ofthePTNNsitoffered.Closebehind,TFMG
had 69%.Pointedly,Keras neverhad the highest FLOPsPTNN and
hadthe lowestFLOPs implementation81%of the time.
4.3 Architecture
We frame our results for architecture in terms of the discrepancies
wediscoveredinouranalysis.Specifically,wediscussdifferences
amongPTNNs for AlexNet,ResNetV1 101 ,ResNetV2 50 , andResNet
V2 101andagainst the PTNNs’ originpapers.
TheAlexNetfromTorchvisioncitesadifferentoriginpaperthan
othermodelzoos[ 19,20].Bothpaperscontainthesamefirstauthor;
however, only the latter contains an explicit description of a DNN
architecture.Assuch,ouranalysispegsthePTNNagainstthelatter
paper[20].Wenoticetwomaindiscrepancies:thePTNNismissing
the response normalization layers and the kernel-size and number
of kernels for the convolution layers are incorrect. For instance,
Torchvision’sPTNN has64 kernels inthe first convolution layer as
opposedto the 96 that are describedinthe originpaper.
1607ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore DiegoMontes, Pongpatapee Peerapatanapokin, JeffSchultz,ChengjunGuo, Wenxin Jiang,andJames C.Davis
Figure4: ResNetV250 architecturedifferencesbetween Keras
Applications (left)and ONNXModelZoo (right).Thetop-right
convolutiononthelefthasastridesizeof2,whilethetop-
rightconvolution on therighthasastride size of1.
TheResNetV1101 fromONNXandKerascontainconvolution
shortcuts, which were only introduced in the ResNet V2 paper, but
notinthe ResNetV1 originpaper[ 14,15].Torchvision’sandTFMG’s
ResNetV1101 donotincludethisshortcut.Alsointhe ResNetfamily,
both theResNet V2 50 andResNet V2 101 have a shareddiscrepancy.
AsseeninFigure 4,Keras’ResNetV250 implementationcontains
maxpoolskipconnections,whicharenotpresentinthepaper,and
uses convolutions withlarger strides intheseresidual blocks[ 15].
The observed discrepancies in architecture may affect the accu-
racyandlatency.Forexample,thelargerconvolutionstridesand
max pool skip connection in the ResNet V2 50 from Keras allows
the network to use less compute, FLOPs, compared to the PTNN
from ONNX. This can be seen in the FLOP measurements of the
ResNet V2 50 from Keras and ONNX. ONNX’s ResNet V2 50 has
4.12 billion FLOPs while Keras’ PTNN only has 3.49 billion FLOPs,
an 18.1% difference. Moreover, the Keras PTNN did not sacrifice
accuracy through this implementation, reporting a 76% top-1 accu-
racy, which is greater than ONNX’s ResNet V2 50 top-1 accuracy
of75.81%.WhiletheKerasmaintainersdidnotimplement ResNet
V2 50faithfully to the origin paper, they produced a more accurate
PTNN withlower latency.
5 DISCUSSION AND FUTUREWORK
Empirical Validation. The top-1 accuracy differences depicted in
Figure2suggest that the choice of model zoo matters. Specifically,
34%ofthePTNNarchitectureshavingtop-1accuracydifferences
greater than 0.96% is not easily overlooked. An engineer may re-
ceive a PTNN that incorrectly classifies greater than 500 validation
imagesonImageNetmorethanaPTNNfromadifferentmodelzoo.
Modelzoochoiceshouldnotresultinanoticeableimpactonthe
accuracyofPTNNsthatengineersreceive.Althoughmodelzoos
currentlyreporttheaccuracyofthePTNNstheyoffer,ourworkhas
shownthatthisdoesnotguaranteethatthereisnotanothermodel
zoowiththesamePTNNatahigheraccuracy.Publiclyavailable
andactivelymaintainedcomparisonsofmodelzooPTNNswould
allowengineers tobe moreinformed whenchoosinga modelzoo.
Furthermore,weonlystudiedtheaccuraciesofimageclassificationmodels at face value. We recommend future works focus on empir-
icalvalidationontheclaimsofPTNNsinmodelzoostocheckfor
the existenceoffalse advertising.
New Metrics and Automated Tools. The measured FLOP dis-
parities seen in Figure 3have consequences, especially in edge
devices with limited compute. For example, ONNX incorrectly list-
ingSqueezeNet1.1 asSqueezeNet1.0 mayleadtoconfusionwhenan
engineer switches to SqueezeNet 1.1 fromSqueezeNet 1.0 expecting
adropinlatency.Similarconfusionmayarisefrominstanceslike
theoneseeninTFMG’sselectionof ResNetV2 .Whiletheincreased
input size is stated, the impact on latency is not made clear. To
effectivelyinformengineersofthelatencyofPTNNs,modelzoos
shouldreportFLOPcountsalongsideaccuracy.Alsoofinterestis
the energyusage ofthesePTNNs, anotherimportantpropertyfor
edgedevices.Thelackofreportingofthesepropertiesmaymake
choosingPTNNsmoredifficult.Werecommendfutureworkscreate
newmetrics to measure the reliabilityandquality of PTNNs from
model zoos and develop tools for automatically measuring these
properties.Publishingupdatedresultsfrequentlycansupporteasier
decision-making ofmodels for deployment.
Naming Conventions. The differences in the architectures of
PTNNs may indicate an underlying improper documentation stan-
dardandaneedforimprovednamingconventionsinmodelzoos.
As indicated in ğ 4.3, Torchvision’s AlexNetdid not adhere to the
originpaperwhilestillclaimingtobe AlexNet.Seemingly,model
zoos are advertisingPTNNs labeled as well-known DNNarchitec-
tures,like ResNetandAlexNet,butwhentheydothis,theyreally
meanthatthePTNNsarebasedontheDNNarchitectureandare
not strict implementations. This inadequate naming convention
leads to a false sense of equality and thus confusion. We recom-
mendthecommunitycomprehensivelydocumentPTNNnaming
conventions to increase cohesion among model zoos. Likewise, we
suggestfutureworksinvestigatetheexpectationsofengineerswith
regards to the PTNNs from model zoos to see whether they prefer
exact reproductions or more accurate and lower latency PTNNs.
The result of such a study would inform model zoo maintainers on
howto bestimplement andtrainPTNNs.
6 CONCLUSION
We present an investigation of the discrepancies between 36 image
classification PTNN architectures from four popular model zoos
throughaccuracy,latency,andarchitectureanalyses.Wefindsev-
eralsignificantdiscrepanciesamongthesethreeaxesthatchallenge
the well-established use of PTNNs from model zoos, suggesting
that an engineer will receive a PTNN with different characteristics
based on the model zoo. The PTNN’s goal of shortening model
deployment time is diminished because of the time investment
neededtoverifythepropertiesofthePTNN.Wediscusstheimpor-
tance of future works to validate the claims of model zoos, develop
automated tools for measurement, and explore best practices for
implementingmodelzooPTNNs.
ACKNOWLEDGMENTS
This work was supported by gifts from Google and Cisco and by
NSF-OACaward#2107230. We thank the anonymous reviewersfor
theircarefulreadingofourmanuscriptandtheirmanyinsightful
comments andsuggestions.
1608Discrepanciesamong Pre-trainedDeep Neural Networks: A NewThreat to Model Zoo Reliability ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
REFERENCES
[1] 2019. ONNX|Home. https://onnx.ai/
[2]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,JoshLevenberg,
DandelionMané,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker,VincentVanhoucke,VijayVasudevan,FernandaViégas,OriolVinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
https://doi.org/10.5281/zenodo.4724125
[3]Saleema Amershi, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall,
EceKamar,NachiappanNagappan,BesmiraNushi,andThomasZimmermann.
2019. SoftwareEngineeringforMachineLearning:ACaseStudy.In International
Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP) .
https://doi.org/10.1109/ICSE-SEIP.2019.00042
[4]Vishnu Banna, Akhil Chinnakotla, Zhengxin Yan, Anirudh Vegesana, Naveen
Vivek, Kruthi Krishnappa, Wenxin Jiang, Yung-Hsiang Lu, George K. Thiru-
vathukal, and James C. Davis. 2021. An Experience Report on Machine Learning
Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Con-
tributors. https://doi.org/10.48550/arXiv.2107.00821
[5]blacklong28. 2022. onnx-opcounter .https://github.com/blacklong28/onnx-
opcounter
[6]MichaelaBlott,LisaHalder,MiriamLeeser,andLindaDoyle.2019. QuTiBench:
BenchmarkingNeuralNetworksonHeterogeneousHardware. JournalonEmerg-
ingTechnologiesinComputingSystems(JETC) (2019).https://doi.org/10.1145/
3358700
[7]HudsonBorgesandMarcoValente.2018.What’sinaGitHubStar?Understanding
Repository Starring Practices in a Social Coding Platform. Journal of Systems
and Software (2018).https://doi.org/10.1016/j.jss.2018.09.016
[8]EricBreck,ShanqingCai,EricNielsen,MichaelSalib,andD.Sculley.2017.TheML
test score: A rubric for ML production readiness and technical debt reduction. In
International Conference on Big Data (BigData) .https://doi.org/10.1109/BigData.
2017.8258038
[9]Anh T. Dang. 2021. Accuracy and Loss: Things to Know about The Top 1 and
Top 5 Accuracy. https://towardsdatascience.com/accuracy-and-loss-things-to-
know-about-the-top-1-and-top-5-accuracy-1d6beb8f6df3
[10]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Imagenet:
A large-scale hierarchical image database. In IEEE conference on computer vision
and pattern recognition (CVPR) .https://doi.org/10.1109/CVPR.2009.5206848
[11]NikhilKrishnaGopalakrishna,DharunAnandayuvaraj,AnnanDetti,ForrestLee
Bland, Sazzadur Rahaman, and James C. Davis. 2022. łIf security is requiredž:
Engineering and Security Practices forMachine Learning-based IoT Devices.In
2022 IEEE/ACM 4th International Workshop on Software Engineering Research and
Practices for the IoT(SERP4IoT) . 1ś8.https://doi.org/10.1145/3528227.3528565
[12]Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2019. BadNets: Iden-
tifying Vulnerabilities in the Machine Learning Model Supply Chain. https:
//doi.org/10.48550/arXiv.1708.06733
[13]Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong
Qiu, Yuan Yao, Ao Zhang, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin,
Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie
Tang,Ji-RongWen,JinhuiYuan,WayneXinZhao,andJunZhu.2021. Pre-trained
models:Past,presentandfuture. AIOpen(2021).https://doi.org/10.1016/j.aiopen.
2021.08.002
[14]Kaiming He, Xiangyu Zhang, Shaoqing Ren,and Jian Sun.2016. Deep Residual
Learning for Image Recognition. In IEEE Conference on Computer Vision and
PatternRecognition (CVPR) .https://doi.org/10.1109/CVPR.2016.90
[15]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity
Mappings in Deep Residual Networks. In European conference on computer
vision (ECCV) , Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (Eds.).
https://doi.org/10.1007/978-3-319-46493-0_38
[16]Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand,Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).https://doi.org/10.48550/arXiv.1704.04861
[17]Matthew Hutson. 2018. Artificial intelligence faces reproducibility crisis. Science
(2018).https://doi.org/10.1126/science.359.6377.725
[18] Keras. 2022. KerasApplications .https://keras.io/api/applications/
[19]Alex Krizhevsky. 2014. One weird trick for parallelizing convolutional neural
networks. arXiv(2014).https://doi.org/10.48550/arXiv.1404.5997
[20]AlexKrizhevsky,IlyaSutskever,andGeoffreyE.Hinton.2012. ImageNetClassifi-
cation withDeep Convolutional Neural Networks. In International Conferenceon
NeuralInformationProcessingSystems(NeurIPS) .https://doi.org/10.1145/3065386
[21]LingLiu,YanzhaoWu,WenqiWei,WenqiCao,SemihSahin,andQiZhang.2018.
Benchmarking Deep Learning Frameworks: Design Considerations, Metrics and
Beyond.In InternationalConferenceonDistributedComputingSystems(ICDCS) .
https://doi.org/10.1109/ICDCS.2018.00125
[22]Tong Liu, Shakeel Alibhai, Jinzhen Wang, Qing Liu, Xubin He, and Chentao Wu.
2019. Exploring Transfer Learning to Reduce Training Overhead of HPC Data in
MachineLearning.In InternationalConferenceonNetworking,Architectureand
Storage(NAS) .https://doi.org/10.1109/NAS.2019.8834723
[23]Sébastien Marcel and Yann Rodriguez. 2010. Torchvision the Machine-Vision
PackageofTorch.In ACMinternationalconferenceonMultimedia .https://doi.
org/10.1145/1873951.1874254
[24] ONNX. 2022. tf2onnx.https://github.com/onnx/tensorflow-onnx
[25]Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE
Transactions on Knowledge and Data Engineering (2010).https://doi.org/10.1109/
TKDE.2009.191
[26]David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions
and Large Neural Network Training. arXiv.https://doi.org/10.48550/arXiv.2104.
10350
[27]Hung Viet Pham, Mijung Kim, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan.
2021. DEVIATE: A Deep Learning Variance Testing Framework. In International
ConferenceonAutomatedSoftwareEngineering(ASE) .https://doi.org/10.1109/
ASE51524.2021.9678540
[28]Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan
Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. 2020. Problems
and Opportunities in Training Deep Learning Software Systems: An Analysis of
Variance.In InternationalConferenceonAutomatedSoftwareEngineering(ASE) .
https://doi.org/10.1145/3324884.3416545
[29] PyTorch. 2022. pytorch.https://github.com/pytorch/pytorch
[30]SaidurRahman,EmilioRiver,FoutseKhomh,YannGalGuhneuc,andBerndLehn-
ert. 2019. Machine learning software engineering in practice: An industrial case
study. In International Conference on Software Engineering:Software Engineering
inPractice (ICSE-SEIP) .https://doi.org/10.1109/ICSE-SEIP.2019.00042
[31] Lutz Roeder. 2022. netron.https://netron.app/
[32]Lucas Sakurada, Carla A. S. Geraldes, Florbela P. Fernandes, Joseane Pontes,
and Paulo Leitao. 2020. Analysis of New Job Profiles for the Factory of the
Future.International Workshop on Service Orientation in Holonic and Multi-Agent
Manufacturing (2020).https://doi.org/10.1007/978-3-030-69373-2_18
[33]ShayanShams,RichardPlatania,KisungLee,andSeung-JongPark.2017. Eval-
uation of Deep Learning Frameworks Over Different HPC Architectures. In
International Conference on Distributed Computing Systems (ICDCS) .https:
//doi.org/10.1109/ICDCS.2017.259
[34]Facebook AIResearchTeam.2022. fvcore. Facebook Research. https://github.
com/facebookresearch/fvcore
[35]Google Brain Team. 2022. TensorFlow .https://github.com/tensorflow/tensorflow
[36]Sebastian Thrun and Lorien Pratt. 1998. Learning to Learn: Introduction and
Overview.https://doi.org/10.1007/978-1-4615-5529-2_1
[37]JasonTsay,AlanBraz,MartinHirzel,AvrahamShinnar,andToddMummert.2020.
AIMMX:ArtificialIntelligenceModelMetadataExtractor.In InternationalCon-
ferenceonMiningSoftwareRepositories(MSR) .https://doi.org/10.1145/3379597.
3387448
[38]Hongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou,
Pengchong Jin, Fan Yang, Frederick Liu, Jaeyoun Kim, and Jing Li. 2020. Tensor-
FlowModelGarden. https://github.com/tensorflow/models .
1609