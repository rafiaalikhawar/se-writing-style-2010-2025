Nessie: Automatically Testing JavaScript APIs with Asynchronous
Callbacks
Ellen Arteca
arteca.e@northeastern.edu
Northeastern University
USASebastian Harner
harnersebastian@gmail.com
University of Stuttgart
Germany
Michael Pradel
michael@binaervarianz.de
University of Stuttgart
GermanyFrank Tip
f.tip@northeastern.edu
Northeastern University
USA
ABSTRACT
Previous algorithms for feedback-directed unit test generation iter-
atively create sequences of API calls by executing partial tests and
by adding new API calls at the end of the test. These algorithms
arechallengedbyapopularclassofAPIs:higher-orderfunctions
that receive callback arguments, which often are invoked asyn-
chronously. Existing test generators cannot effectively test such
APIs because they only sequenceAPI calls, but do not nestone call
intothecallbackfunctionofanother.ThispaperpresentsNessie,
thefirstfeedback-directedunittestgeneratorthatsupportsnestingofAPIcallsandthattestsasynchronouscallbacks.NestingAPIcalls
enablesatesttousevaluesproducedbyanAPIthatareavailable
only once a callback has been invoked, and is often necessary to
ensure that methods are invoked in a specific order. The core con-
tributionsofourapproachareatree-basedrepresentationofunit
testswithcallbacksandanovelalgorithmtoiterativelygenerate
suchtestsinafeedback-directedmanner.Weevaluateourapproach
on ten popular JavaScript libraries with both asynchronous and
synchronouscallbacks.Theresultsshowthat,inacomparisonwithLambdaTester,astateofthearttestgenerationtechniquethatonlyconsiderssequencingofmethodcalls,Nessiefindsmorebehavioral
differences and achieves slightly higher coverage. Notably, Nessie
needstogeneratesignificantlyfewerteststoachieveandexceed
the coverage achieved by the state of the art.
KEYWORDS
asynchronous programming, test generation, JavaScript, testing
ACM Reference Format:
EllenArteca,SebastianHarner,MichaelPradel,andFrankTip.2022.Nessie:
Automatically Testing JavaScript APIs with Asynchronous Callbacks. In
44thInternationalConferenceonSoftwareEngineering(ICSEâ€™22),May21â€“
29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510106
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
For all other uses, contact the owner/author(s).
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.35101061 INTRODUCTION
Test generation is an important technique to automatically test
librariesbycreatingunit-leveltests.Thegeneratedteststypically
consistofasequenceofcallstofunctionsinanAPIundertest.The
valuespassedasargumentstoeachfunctioncallinsuchasequence
maybechosenrandomly,orvaluesreturnedbypreviouscallsin
the sequence can be used, to facilitate the testing of dependent API
functions.Differenttestgeneratorstakedifferentapproachesfor
selecting which functions to call and which arguments to pass into
them, e.g., random inputs [ 10,12], feedback from executions [ 13,
29, 30], and symbolic reasoning [36, 40, 42].
However,existingworkontestgenerationhasignoredabroad
class of APIs: Functions that accept another function as a callback
argumentandtheninvokethatotherfunctionasynchronously.The
key benefit of asynchronous callbacks is that they do not block
the main computation, which is useful, e.g., when accessing some
kindofresourceorwhentriggeringalong-runningcomputation.
Asynchronouscallbackshavebeenshowntobepopular[ 14],but
also prone to mistakes and surprising behavior [ 28,41]. While the
JavaScript community has started migrating to asynchronous APIs
that rely on promises [ 1, Section 25.6] and async/await [1, Section
25.7], a vast amount of JavaScript code still uses event-driven APIs
that invoke callback functions passed to them asynchronously.
We observe that existing test generators miss out on two op-
portunities for testing APIs with asynchronous callbacks. First, in
addition to sequencing function calls, one can also consider nesting
them, by placing an API call into a callback passed to another API
functionundertest.Suchnestingenablesatesttousevaluespro-
ducedbyanAPIthatareavailableonlyonceacallbackhasbeen
invoked;moreover, nestingisoftennecessarytoensureaspecific
ordering of invocations to API functions. Second, even the best ex-
isting test generator aimed at testing functions with callbacks [ 35]
supports only synchronous, but not asynchronous callbacks. The
table below compares our work with the capabilities of two related
techniques our work is inspired by:
Sequencing Nesting Synchronous Asynchronous
callbacks callbacks
Randoop [30] /check
LambdaTester [35] /check/check
This work (Nessie) /check/check /check /check
14942022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ellen Arteca, Sebastian Harner, Michael Pradel, Frank Tip
Toillustratethechallengesassociatedwithtestingasynchronous
APIs, consider a library for accessing JSON files. The API defines a
function existsforchecking whetheraJSON filewitha specified
name exists, which produces a handle to that file if this is the
case. Moreover, there is a function readfor parsing a JSON file
represented by a given handle. In JavaScript, functions in event-
driven APIs typically take a callback as their last argument, which
isinvokedwithtwoarguments:(i)anobjectthatindicateswhetheranerrorhasoccurred(or
nullifnoerroroccurred)and(ii)anobject
representing the results of the operation. Hence, a typical use of
the library would look as follows:
letfileName = ...;
exists(fileName,
// asynchronously invoked callback function:
(err, fileHandle) => {
if(!err) {
read(fileHandle,
// another callback function:
(err, jsonObj) => { ... })
}
})
Thecallto readisnestedinthecallbackthatispassedto exists,
to ensure that the read operation is executed after the exists opera-
tion has completed. Now suppose that the readoperation contains
a bug that is triggered in certain cases where a valid file-handle
ispassed(e.g.,ifthefileâ€™spermissionsdonotpermitreadaccess),
and suppose thatwe want to generatea test that invokes the read
function to expose the bug. Since file-handle objects are created
inside the library, it is unclear what the representation of these
objects looks like without analyzing or executing the library code.
Whileitispossibleforatestgeneratortocreatesuitablefile-handle
objectsusingapurelyrandomapproach,thechancesofsuccessfully
creating a valid file-handle would be small. Therefore, the mosteffective way to obtain a valid file-handle and expose the bug isto invoke
existswith some callback function ğ‘“, and invoke read
withthefile-handlethatispassedto ğ‘“asitssecondargument.That
is, we would generate a test where a call to readisnestedin the
callback that is passed to exists, as in the above example.
Unfortunately,thestateofthearttestgeneratorfortestingfunc-
tions with callbacks [ 35] is unable to generate such a test for the
tworeasons mentionedin theabove table:First, itfails toidentify
API functions that receive an asynchronously invoked callback
argument,andhence,neverpassescallbacksintosuchfunctions.
Second, it does not construct tests where a call to one API function
is nested inside the callback passed to another API function.
ThispaperpresentsNessie,thefirstfeedback-directedtestgener-
ationtechniquethatnestsAPIcallsintocallbacksandthatsupports
asynchronous APIs. At the core of the approach is a novel tree-
based representation of test cases, which allows for growing a test
case by either sequencing API calls, i.e., adding sibling nodes, orbynestingAPIcalls,i.e.,addingchildnodes.Wepresentanalgo-
rithmforiterativelygeneratingtree-shapedtestsbasedonfeedbackfromexecutingalreadygeneratedtests.Thealgorithmissupported
by anautomated APIdiscovery phasethat determineswhich API
functions accept asynchronous or synchronous callbacks and byguiding the test generator toward realistic API usages based on
nesting examples mined from existing API clients.
Our implementation targets JavaScript, where asynchronous
callbacks are particularly prevalent and where generating effectivetests is particularly challenging due to the absence of statically de-
claredtypes.OurevaluationappliesNessietotenpopularJavaScript
libraries that include a total of 356 API functions, 142 of which ex-
pect callbacks. Nessieâ€™s API discovery phase detects 62% of the
API signatures with callback arguments that are mentioned in the
APIâ€™sdocumentation,and106 undocumented APIsignatureswith
callbacks,reflectingunexpectedbehavior.Thecoverageachieved
by Nessie converges significantly more quickly than with the state
of the art LambdaTester approach [ 35] and even reaches a slightly
higher coverage: On average, Nessie needs to generate only 136
teststoachievethesamecoveragethatLambdaTesterachieveswith
1,000generatedtests.Weconjecturethatthisisthecasebecause
the nesting of callback functions enables a more effective selection
ofargument valuesin subsequent(nested)function calls.We also
comparetheabilityofNessieandLambdaTestertodetectsituations
wheretestsgeneratedforagivenversionofanAPIbehavediffer-
entlywhenrunagainstthenextversionofthatAPI.Onaverage,
Nessie detects 23% more behavioral differences than LambdaTester,
includingamixofbugsandintentionalAPIchanges.Whilethese
differences can, in principle, all be detected without nesting API
calls, our approach finds them more effectively and efficiently due
to its ability to nest calls.
In summary, this paper contributes the following:
â€¢ThefirstautomatedtestgeneratorspecificallyaimedatAPIs
that accept callbacks to be invoked asynchronously.
â€¢An algorithm for incrementally generating tests that not
onlysequenceAPIcallsbutalso nesttheminsidecallbacks.
â€¢Empiricalevidencedemonstratingthat:(i)theapproachis
effective at exercising JavaScript APIs with asynchronous
callbacks,(ii)thatitachievesmodestlyhighercodecoverage
andfindsmorebehavioraldifferencesthanthestateofthe
art,and(iii)thatit convergesmuchmorequickly thanprior
workwhenitcomestoachievingaspecificlevelofcoverage
or behavioral differences.
2 OVERVIEW OF NESSIE
This paper presents a test generation technique for testing higher-
order functions. A function is called a higher-order function if it
expectsanotherfunctiontobepassedasanargument,orifitreturns
a function. Our work targets functions ğ‘“that receive a callback
function cbas an argument and then invoke the callback either
synchronouslyorasynchronously.Asynchronousinvocationhere
means that the execution of ğ‘“causesğ‘ğ‘to be invoked from the
main event loop at some later time.
GeneratingtestsforasynchronousAPIsinvolvesseveralopen
challenges.ThefirstchallengeistofindoutwhichAPIfunctions
expect(a)synchronouscallbacksandatwhatargumentpositions
thesecallbacksshouldbepassed.SinceJavaScriptisdynamically
typed,ourapproachneedstoinferthesignaturesoffunctionsas
aprerequisitetogeneratingeffectivetests.Thesecondchallenge
is about how to compose multiple API calls into a test. While ex-isting work focuses on sequencing calls, i.e., one call statement
after another, sequencing alone is insufficient for testing asynchro-
noushigher-orderfunctions.Thethirdchallengeisabouthowto
composeAPIcallsinarealisticway.Toincreasethechancesthat
1495
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
API functions
under test
API discoveryAbstract
signaturesFeedback-directedtest generationT est case trees
ExistingAPI clients
Mining API usagesNestingexamplesTe s tcases
Figure 1: Overview of the approach.
our approach nests API calls in ways that represent real-world API
usage, it requires some knowledge of typical API usage.
Ourapproach,illustratedinFigure1,consistsofthreemaincom-
ponentsthateachaddressoneofthechallengesdescribedabove.
Given a set of API functions to test, the first step is automated API
discovery, which probes the API functions under test to determine
if and where they expect callback arguments. The discovered in-
formation is stored as a set of abstract signatures, which record
whetherthearguments are:(i)asynchronously invokedcallback,
(ii)anasynchronouslyinvokedcallback,or(iii)avaluethatisnot
acallback1.Then,theabstract signaturesdiscoveredinthisphase
serveasinputtoa feedback-directedtestgenerationalgorithm,which
is the core of the approach. The test generation is centered around
atree-basedrepresentationoftestcases,called testcasetrees,which
theapproachiterativelygrowsintofulltestcases.Toinformdeci-
sions about how API functions should be combined, the approach
alsomines API usage patterns in existing open-source clients of
thelibraries undertest.Thisinformation ispassedto thetestgen-
eratorandusedtosupportnestingdecisions.Theendproductof
this process is a set of test cases, which can be used in a variety of
applications, e.g., regression testing.
3 API DISCOVERY
Thefirststepintestgenerationisdeterminingwhattotest:Givena
libraryundertest,Nessieretrievesthesetofallfunctionproperties
offeredbythelibraryobject.AsJavaScriptisadynamicallytyped
language,Nessieinitiallydoesnotknowanythingaboutthesefunc-
tions beyond their names. One possible approach to learn more
about the APIs would be to rely on optional type annotations, e.g.,
in the form of third-party TypeScript type declarations for popular
libraries.2However,not allJavaScriptcodecomeswithtypeannota-
tions[25]andevenifitexists,anAPIâ€™simplementationmaydiverge
from its declared type [ 21]. Nessie addresses this problem through
anAPIdiscovery phasethatprobesAPIfunctionstodetermineat
what parameter positions they expect callbacks and whether these
are invoked synchronously or asynchronously. This information is
recorded as a set of abstract signatures, or simply signatures.
Definition 1 (Abstract signature). An abstract signature for a
functionğ‘“is a tuple (ğ‘ğ‘Ÿğ‘”1,...,ğ‘ğ‘Ÿğ‘” ğ‘›), where each ğ‘ğ‘Ÿğ‘”ğ‘–is one of the
following three kinds of elements:
â€¢async: an argument is an asynchronously invoked callback
â€¢sync: an argument is a synchronously invoked callback
1Since we are targeting JavaScript, where functions can be invoked with any number
of arguments of any type, we do not attempt to track precise type information.
2https://definitelytyped.org/â€¢the _ symbol: any non-callback argument
A single function may have zero, one, or multiple signatures.
Zero signaturesindicates that theAPI discovery failed tofind any
signaturesforthefunction,inwhichcasetheapproachfallsbackto
a default test generation strategy that does not pass any callbacks.
Multiplesignaturesareinferredwhenfunctionsareoverloaded.Forexample,the
outputJson functionfrom fs-extra hastwosignatures:
(_,ğ‘ğ‘ ğ‘¦ğ‘›ğ‘)and(_,_,ğ‘ğ‘ ğ‘¦ğ‘›ğ‘). The reason is that outputJson has an
optional second argument, and always takes a callback function as
thelastargument.3Inourexperience,suchoverloadingisextremely
commoninJavaScriptAPIsduetooptionalargumentspreceding
the (final) callback argument.
TodiscoversignaturesforagivenAPIfunctionundertest,Nessie
repeatedly4invokes the function with randomly generated argu-
ments. The approach alternates between generating calls with and
withoutacallbackargument,andpassesdifferentnumbersofargu-
ments. A generated test for a function apiis structured as follows:
letcallback = () => {console.log( "Callback executed" );}
try{
// try calling the specified API function
api(..., callback, ...);
console.log( "API call executed" );
}catch(e) {
console.log( "Error in API call" );
}
console.log( "Test ex ecuted" );
During the execution of these tests, the print statements track if
andwhencallbacksareinvoked,andwhetherthefunctiontermi-
nates successfully. We ignore erroneous executions, as they maybe due to incorrect arguments passed to the function. For non-
erroneous executions, the approach distinguishes three cases:
â€¢acallbackthatexecutes afterthetesthasexecuted isexecuted
asynchronously,soasignatureiscreatedwith asyncatthe
positionofthecallbackargumentand _atallotherpositions,
â€¢acallbackthatexecutes beforetheAPIcallreturns isexecuted
synchronously, so a signature is created with syncat the
callback position and _ at all other positions,
â€¢ifthecallbackisnotexecutedorthetestdoesnotpassany
callback, a signature with _ symbols only is created.
Wetestwithasinglecallbackargumentatatime,i.e.,ourapproach
will not discover signatures with multiple callback arguments. The
rationale is that API functions with multiple callbacks are rela-
tively rare. Extending the algorithm to support multiple callbacks
isstraightforwardbutincreasesthecomputationalcomplexityof
API discovery. In general, the results of the API discovery phase
are unsound, because the approach does not guarantee to cover all
possible arguments or all paths through the API implementation.
TheoutputoftheAPIdiscoveryisthesetofdiscoveredsigna-
tures for each function offered by the library under test. When
generatingtests, thenumberofarguments andcallbackpositions
(if the function has any) used during test generation are informed
by these discovered signatures.
4 FEEDBACK-DIRECTED TEST GENERATION
3https://github.com/jprichardson/node-fs-extra/blob/HEAD/docs/outputJson.md
4By default Nessie runs 50 tests per API function, each with a timeout of two seconds.
1496
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ellen Arteca, Sebastian Harner, Michael Pradel, Frank Tip
1// Omitted for clarity:
2// * Try-catch around each call to an API function.
3// * Print statements to log arguments and return values.
4// * Print statements to log control flow.
5
6 letfs_extra = require ("fs-extra" );
78 vararg590 = "a/b/test";
9 vararg593 = null;
1011 letr_126_0 = fs_extra.ensureFile( arg590, cb(a, b, c, d, e) => {
12 letr_126_0_0 = fs_extra.readJson(arg590);
13 return false;
14});
1516 letr_126_1 = fs_extra. stat(arg593);
(a) Generated test.
 (b) Test case tree representation of the test, with extension points annotated.
Figure 2: Example of a generated test for the API functions of the fs-extra library and the corresponding test case tree.
The following describes the core algorithm of Nessie, which
generates tests cases that both sequence and nest API calls. The
algorithm is based on a tree-shaped representation of test cases
called atest case tree (Section 4.1), which serves as the basis for the
algorithm itself (Section 4.2).
4.1 Test Case Trees
Torepresenttestcasesthatsupportbothsequencingandnesting
ofmethodcalls,wedefinethe textcasetree,anovelintermediate
representation of test cases:
Definition 2 (Test case tree). Letğ‘‰be a map of variable names
to non-callback values, called value pool, and ğ‘†a map of function
names to abstract signatures. Then, a test case tree is an ordered
tree where nodes are either:
â€¢acallnodeoftheform ğ‘Ÿ=ğ‘ğ‘ğ‘–(ğ‘1,...,ğ‘ ğ‘˜),meaningthatfunc-
tionğ‘ğ‘ğ‘–isinvokedwitharguments (ğ‘1,...,ğ‘ ğ‘˜)andyieldsre-
turnvalue ğ‘Ÿ.Ifğ‘ ğ‘–âˆˆ{sync,async}forsomesignature (ğ‘ğ‘ğ‘–â†¦â†’
(ğ‘ 1,...,ğ‘  ğ‘›)) âˆˆğ‘†, thenğ‘ğ‘–may be a callback function. Other-
wise,ğ‘ğ‘–âˆˆğ‘‰or it is a return value of another call, or
â€¢acallback node of theform ğ‘ğ‘(ğ‘1,...,ğ‘ ğ‘˜), meaningthat call-
back function ğ‘ğ‘receives parameters ğ‘1,...,ğ‘ ğ‘˜.
Edges are either:
â€¢acontains edge from a callback node to a call node, meaning
that there is a call in the body of the callback function, or
â€¢anargument edge from a call node to a callback node, mean-
ing that a call is given a callback function as an argument.
Therootnodeofatestcasetreeisaspecialcallbacknodethat
corresponds to the function that contains the entire test case. The
orderofthecallnodesunderacallbacknoderepresentsthesequen-
tial order in which calls occur in the body of a callback function.
Example. Figure 2 gives an example of a test case and its cor-
responding test case tree. Each API call in the test case in Fig-ure 2a is represented by a call node in the test case tree of Fig-ure 2b. The nodes for calls that receive callback arguments eachhave a corresponding callback node as a child. E.g., the callback
givento ensureFile atline11isrepresentedbythecallbacknodecallback(a, b, c, d, e) . Calls nested within the body of a call-
back function are represented as children of callback nodes. E.g.,the call to
readJson on line 12 corresponds to the lower-left call
nodeof thetree. Thevalue poolof thetest consistsof twoentries,
whichmapthevariablenamestotheirrespectivelyassignedvalues
in lines 8 to 9.
A call in a test case can use as arguments only values that are
availableatthecallsiteaccordingtothescopingrulesofJavaScript.
We say that a test case is well-formed if, for all calls, its arguments
are bound to some declaration when the call is reached during the
executionofthetestcase.Givenourtreerepresentationofatest
case, a test case is well-formed if and only if the following holds:
Definition3 (Well-formedness). Inawell-formedtestcasetree,
each argument of a call node ğ‘›is one of the following:
â€¢A random primitive value, inserted by referring to one of
the entries in the value pool ğ‘‰.
â€¢Arandomobjectvalue(arrayliteral,objectliteral,orfunc-
tion), also inserted by referring to one of the entries in ğ‘‰.
â€¢Thereturnvalue ğ‘Ÿofacallnode ğ‘›/primethatisaleftsiblingof ğ‘›or
a leftsibling ofany ancestorcall nodeof ğ‘›(these represent
return values of calls executed before reaching the call in ğ‘›).
â€¢Aparameter ğ‘ğ‘–ofacallbacknodeonthepathbetweenthe
root node and ğ‘›(these represent formal parameters of a
surrounding callback function).
â€¢A callback function ğ‘ğ‘ğ‘¥, whereğ‘›has a callback node that
represents ğ‘ğ‘ğ‘¥as a child.
Example. Figure2bshowsatestcasetreewhereallarguments
arewell-defined.E.g.,inthecallof ensureFile ,thefirstargumentis
arg590, which is a randomly generated primitive value in the value
pool(astringliteral,definedonline8),andthesecondargumentisa callback node. In contrast, the call of
statcould not use, e.g., aas
an argument because ais not in the parameter list of any callback
node on the path between statâ€™s node and the root node.
4.2 Test Generation Algorithm
Ouralgorithmcreatestestcasesiteratively,byrepeatedlyextending
an existing test case with another call at extension points, which
1497
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Algorithm 1 Feedback-directed generation.
Input:Setğ¹of API functions, map ğ‘†from function names in ğ¹to
their discovered signatures, mined nesting examples ğ‘€
Output: Set of tests ğ‘‡
1:ğ‘‡â†âˆ… âŠ²Generated test case trees
2:ğ‘¡â†empty test case tree
3:ğ¸â†{(ğ‘¡,root(ğ‘¡))} âŠ²Extension points
4:while|ğ‘‡|<number of tests to generate do
5:(ğ‘¡,ğ‘›)â†randomly pick from ğ¸
6:ğ‘“â†chooseFunction (ğ‘¡,ğ‘›,ğ¹,ğ‘€)
7:argsâ†chooseArguments (ğ‘¡,ğ‘›,ğ‘“,ğ‘†,ğ‘€ )
8:ğ‘¡/primeâ†extendğ‘¡with call to ğ‘“(args)atğ‘›
9:feedback â†ğ‘’ğ‘¥ğ‘’ğ‘ğ‘¢ğ‘¡ğ‘’(ğ‘¡/prime)
10:ğ‘‡â†ğ‘‡âˆª{ğ‘¡/prime}
11:ifnoExceptions (feedback)then
12: for each ğ‘›/primeâˆˆextensionPoints (feedback)do
13: ğ¸â†ğ¸{(ğ‘¡/prime,ğ‘›/prime)}
14:returnT
representlocationsinagiventestcasewhereanewcallnodecould
beinserted.Givenatestcasetree,thereisanextensionpointfor
each callback node that is executed during test execution, which
adds another child node to the already existing child nodes, at the
right-mostposition.Forcallbacknodeswithnochildren,thereis
an extension point for adding a first child node.
Example. Figure2bshows theextensionpointsofthetestcase
tree, as well as edges that would be added if the test is extended at
thesepoints(labeledthe extensionedges ).Theseextensionpoints
correspond to adding a call right after lines 12 and 16 in Figure 2a.
Algorithm 1 summarizes our feedback-directed approach for
creating test cases. It maintains a set ğ‘‡of generated tests and a set
ğ¸of extension points. Each extension point is a pair (ğ‘¡,ğ‘›)of a test
case treeğ‘¡and a node ğ‘›in this tree, representing the callback node
whereanewcallnodecouldbeinserted.Themainloopextendsan
existingtestwithanewcallatoneoftheextensionpointsandadds
thetestto ğ‘‡.Theextendedtest ğ‘¡/primeisthenexecutedtogatherfeedback
aboutitsexecution.Ifnoexceptionisthrown,eachpossiblecallbacknode that is executed is kept as a possible extension point to createafurthertest.Thealgorithmreliesonhelperfunctionsforchoosing
a function to call, choosing arguments to pass into the function,
and identifying future extension points, which we describe below.
Choosing a function to call. Given a specific extension point,
Algorithm1calls chooseFunction topickafunctiontocallbybalanc-
ing two requirements. On the one hand, the generated tests should
cover as many functions as possible. On the other hand, we do not
wanttoprescribeaspecificorderinwhichfunctionsareselected.
Tothisend,theapproachassignstoeachofthegivenfunctionsa
weight and then takes a weighted, random decision. Initially, all
functionshaveuniformlydistributedweights.Whenafunctionis
selectedby chooseFunction,itsweightisdividedbyaconstantfactor
(four in our current implementation). Note that this reduction is
done every time a function is chosen, i.e., if the same function is
chosen twice, its weight is divided by the constant factor twice. Inadditiontotheabove, chooseFunction isguidedbyaminedmodel
of nested API functions, as explained in detail in Section 4.3.
Choosing arguments to pass into a function. Once a function has
been selected for testing, arguments need to be generated for it.
This is done by consulting the list of signatures produced by the
discoveryphase.Ifmultiplesignaturesexistforafunction,oneis
chosen at random. The signatures inform the test generation of
thenumber ofargumentsthat thefunctionshould bepassed,and
which (if any) are callback arguments. If no signatures exist for the
function,arandomnumberofargumentsisselected(between0and
5), and arguments are generated randomly to fill these positions.
Note that, in these cases, no callback arguments are generated.
For non-callback arguments, the type is selected randomly from
the JavaScript primitive types (number, string, and boolean), objectliterals,arrays,functions,and other.Iftheselectedtypeisanyofthe
primitives,
object,orarray,thenrandomlygeneratedvaluesofthis
typeareused.Iftheselectedtypeis function,anavailablefunction
ischosenfromtheAPIundertestortheruntimeenvironment(e.g.,
console.log ).Iftheselectedtypeis other,avariablethatisavailable
at the current scope is selected, which includes return values from
previous API calls and arguments to previous callbacks in the test
case tree (Definition 3).
Nesting API calls helps with generating well-formed arguments,
as values (including objects) may get passed from outer to inner
calls, as illustrated in the motivating example in Section 1. In addi-
tion, since we are working with many file system-related libraries,
string primitive values are selected randomly from a pre-made list
of valid file names that correspond to a small hierarchy of directo-
ries and files generated during the setup of Nessie for the purposes
of the testing. Beyond these two points, we do not address the
problem of generating complex objects in this work.
Adding new extension points. After executing a generated test,
extensionPoints iscalledtoidentifywheretoextendthetestinfu-
tureiterations.Thisfunctionreturnsanextensionpointforeach
callbacknodethathasbeenexecuted,correspondingtotheinser-
tion of a new child node to the right of its existing children (see
Figure2b).Therearetworeasonsfor notaddinganextensionpoint:
exceptionsthrownbythetestedAPIsandcallbackfunctionsthat
areneverinvoked.Byexaminingfeedbackfromtestexecutions,the
algorithmavoids creatingfuturetests thatbuildoncode thatwill
never execute. A single test may have multiple extension pointsbecause more than one of its callbacks may execute. Note thatextension points are not removed after being used, so the set of
extensionpointsgrowsmonotonically,enablingformultiplenew
tests to be derived from an extension point in a single base test.
4.3 Mining API Usages
Having an API call nested in the callback argument of another API
call impliesa relationship between thesecalls. We define anotion
of anesting example to formalize such relationships.
Definition 4 (Nesting example). Anesting example is tuple
(ğ‘“outer,(argouter
1,...,argouter
ğ‘š),ğ‘“inner,(arginner1,...,arginner
ğ‘›))
where:
â€¢ğ‘“outeris the name of a called API function,
1498
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ellen Arteca, Sebastian Harner, Michael Pradel, Frank Tip
â€¢everyargouter
ğ‘–iseithercbsyncorcbasync(i.e.,asyncorasync
callback argument) or _ (any non-callback argument),
â€¢ğ‘“inneristhenameofanAPIfunctioninvokedinthecallback
given toğ‘“outer, and
â€¢everyarginner
ğ‘—is eitherouter@ğ‘˜(i.e., the same argument as
given toğ‘“outerat position ğ‘˜),cb@ğ‘˜(i.e., theğ‘˜th parameter
of the callback function), or _ (i.e., any other argument).
Example 4.1. The following nesting example can be mined from
Figure 2a: (ensureFile ,(_,_,cbasync),readJson,(outer@0)).
Example 4.2. The following usage of the fs-extra API, where
theobjparameterofthecallbackpassedto readJson servesasan
argument in a nested call to outputJson :
17// read the c ontents of file. json and output it to output.json
18readJson( "file.json" ,function callback(err, obj) {
19 outputJson( "output.json" , obj);
20});
corresponds to: (readJson,(_,cbasync),outputJson ,(cb@1,_)).
Wedevelopedastaticanalysisforminingnestingexamplesfrom
real-world uses of APIs by traversing the ASTs of existing API
clients.ThisanalysiswasimplementedinCodeQL[ 15],usingits
extensivefacilitiesforstaticanalysis.Inparticular,weuseCodeQLâ€™s
access path tracking to identify functions as originating from an
API import, and its single static assignment representation of local
variablestoidentifysituationswherethesameargumentisused
in an outer call and an inner call. For shared arguments that are
primitivevalues(e.g.,thesamestringpassedtobothinnerandouter
calls) the relationship is identified by checking for value equality.
The test generator uses the set of mined nesting examples in
chooseFunction.When selectingafunction tobenestedin thecall-
backofsomefunction ğ‘“,thesetofnestingexamplesisconsulted
to find examples where ğ‘“outermatchesğ‘“. If such nesting examples
exist, then one of the corresponding ğ‘“innerfunctions is randomly
selected to be invoked inside ğ‘“â€™s callback. Similarly, chooseArgu-
mentconsults the selected nesting example to determine which
arguments (if any) to reuse from the outer function or from the
surrounding callback, and at what position(s).
If no relevant mined nesting examples are available, an inner
function is randomly selected. The test generator is configured
to only use mined data 50% of the time. If we only used nestings
thatshowedupinmineddata,thiswouldexcludemanypotentiallycorrectpairsthatsimplydonotoccurintheminedprojects.Section
5 explores the effect of varying how often mined data is consulted
on the coverage achieved by the tests.
5 EVALUATION
Our evaluation aims to answer the following research questions:
â€¢RQ1:Howeffectiveisthediscoveryphaseatfindingabstract
signatures of API functions?
â€¢RQ2: How effective is Nessie at achieving code coverage?
â€¢RQ3: How effective is Nessie at finding behavioral differ-
ences during regression testing?
â€¢RQ4:Whatistheeffectofvaryingthechanceofchoosing
nested function pairs based on the mined nesting examples?
â€¢RQ5: What is the performance of Nessie?Table 1: Summary of projects used for evaluation.
Project LOC Cov. loading Commit Description
fs-extra 907 16.8% 6bffcd8Extra file system methods
jsonfile 45 19.1% 9c6478a Read/write JSON files in Node.js
node-dir 285 5.9% a57c3b1 Common directory/file operations
bluebird 3.3k 23.7% 6c8c069 Performance-oriented promises
q 760 22.2% 6bc7f52Promise library
graceful-fs 439 25.3% c1b3777 Drop-in replacement for native fs
rsvp.js 579 16.4% 21e0c97 Tools for organizing async code
glob 845 11.0% 8315c2d Shell-style file pattern matching
zip-a-folder 24 16.0% 5089113 Zip/tar utility
memfs 2.4k 29.1% ec83e6fIn-memory file system
Benchmarks. Table 1shows the librarieswe use asbenchmarks
forourevaluation,alongwiththenumberoflinesofcode(LOC)
and the commit of the version we use. To select candidate libraries,
we first identified two domains of libraries that commonly have
asynchronous functionality: file systems and promises. We thenpicked popular libraries in those domains that satisfied the base
requirements of: successfully installing/building, and having a test
suite, with tests that all pass5. As a point of reference, we measure
the statement coverage of the library code achieved by simplyloading the library (column â€œCov. loadingâ€ in Table 1). To mine
nestingexamples fromexistingAPI clients,werun theAPIusage
miningoveracorpusof10,000JavaScriptprojectsonGitHub,which
yields a set of 873 unique nestings of API functions in the libraries
under test.
Baselinesandvariantsoftheapproach. WecompareNessieagainst
thestateof theartapproachLambdaTester [ 35](LT).Becausethe
originalLTdoesnotsupportlanguagefeaturesintroducedinEC-
MAScript 6 and later, and because parts of the implementation are
specific to their benchmarks, we re-implemented LT within our
testingframework.Tobetterunderstandthevalueofnestingandse-quencing,weevaluatetwovariantsofNessie:
NES (seq) ,whichuses
sequencingonly,and NES (seq+nest) ,whichusesbothsequencing
and nesting.
5.1 RQ1: Effectiveness of Automated API
Discovery
To measure Nessieâ€™s effectiveness at discovering the signatures
of API functions, we inspect the documentation of the librariesand manually establish their signatures, and then compare these
signaturesagainstthosediscoveredthroughourautomatedAPIdis-covery.AsdescribedinSection3,iftheapproachdoesnotdiscover
anyvalidsignaturesforanAPIfunctionthenadefaultâ€œcallback-
lessâ€signatureisassigned.Wedonot countthisdefaultsignature
towards the total number of discovered signatures.
Table 2 displays the results of this experiment. For each API,
we include the number of signatures found through manual docu-
mentation inspection and the number found through automated
discovery,bothwithandwithoutcallbackarguments.Wealsoin-
cludethenumberonlyfoundwithoneoftheseapproaches.Thefirstrow reads as follows: for
fs-extra, manual analysis yields 21 signa-
tures withcallback arguments, and automateddiscovery yields 88
signatureswithcallbackarguments.Ofthosemanuallyfound,3are
5Toautomatetheprocessofdeterminingwhichlibrariessatisfytheserequirements,
we used npm-filter [8].
1499
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 2: Abstract signatures categorized manually (M) and
found by automated API discovery (A).
Signatures withcallbacks Signatures without callbacks
Project M A Only M Only A M A Only M Only A
fs-extra 21 88 3 70 45 361 21 337
jsonfile 48 0 4 81 2 4 8
node-dir 96 4 1 11 1 0 1 0
bluebird 25 22 7 4 29 68 26 65
q 16 27 7 18 57 155 19 117
graceful-fs â€“ 15 N/A N/A â€“ 36 N/A N/A
rsvp.js 37 0 4 10 31 3 24
glob 66 0 0 46 1 3
zip-a-folder 00 0 0 34 2 3
memfs 58 30 33 5 57 62 56 61
notdiscoveredautomatically;ofthoseautomaticallydiscovered,70
are not found manually. The next four columns read the same way
butforsignatureswithoutcallbackarguments.Notethatwedonot
haveresultsfor graceful-fs ,asitdoesnotprovidefunction-level
documentation.
We summarize the effectiveness of the discovery phase by com-
puting the percentage of documented signatures that are found
with the automated approach. For the signatures with callback
arguments, we compute this as follows:
1âˆ’signatures only found manually
total number of signatures found manually
=1âˆ’3+4+7+7+33
21+4+9+25+16+3+6+58=62%
Withasimilar computation,weseethat theautomateddiscovery
finds 38% of the documented signatures without callback argu-
ments.
Signatures found only manually. Nessie sometimes misses signa-
turesbecausetheautomateddiscoverymayfailtogeneratevalid
arguments, particularly in cases where arguments need to meet
specificconditions.Forexample,inthefilesystemlibraries,func-
tions without callback arguments often expect valid file names for
files with particular characteristics, and throw an error when this
is not the case (e.g., writeFileSync andreadFileSync injsonfile).
AnotherreasonforsignaturesmissedbytheAPIdiscoveryarefunc-
tionsthattake multiplecallbackarguments,whichouralgorithm
missesasittestswithonlyonecallbackatatime.Multiplecallbacks
are the main cause of missed signatures in node-dir andmemfs.
Signatures found only automatically. There are two main rea-
sonsforfindingsignaturesautomaticallythatwemissedduringthe
manual inspection of documentation. First, some functions are un-
documented,e.g.,internalfunctions,aliasesforthedocumentedAPI
functions,orre-exportedfunctionsofthebuilt-in fsmodule.For
examplein fs-extra,writeJson canalsobecalledwith writeJSON .
Since the automated discovery reads the function properties of the
packageonimport,ittestsallfunctionsregardlessofwhetheror
not they are presented to the user in the documentation. There are
alsosomeinternalfunctionsthatarepresentaspropertiesonthe
library import, such as _toUnixTimestamp onfs-extra andmemfs.
Second,someAPIfunctionssupportmorefunctionsignaturesthan
those that are documented. Since the discovery phase only consid-
ers asignature invalidif the APIfunction call throws an error with
Figure3:Cumulativecoveragewhilegenerating1,000tests
for graceful-fs.
thetestedarguments,abasiclackoferrorcheckingintheimple-
mentationcanleadtoextrasignatures.Forexample,considerthe
writeFile functionin jsonfile.Thedocumentationpresentsitssig-
nature as: writeFile(filename, obj, [options], callback) .H o w -
ever,theautomateddiscoveryphasefindsthat (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘)isavalidsig-
nature.Thisunexpectedsignatureisbecause jsonfile.writeFile
is implemented with universalify â€™sfromPromise function, which
executesthelastargumentifitisacallback,regardlessoftheother
arguments. Even though checking exposed APIs against documen-
tation is not the primary purpose of Nessie, we made pull requests
addressing this issue on both fs-extra6andjsonfile7.
AnswertoRQ1: Automateddiscoveryfinds62%ofdocumented
signaturesthatexpectcallbackarguments,and38%ofsignatures
without callback arguments. It also discovers some undocumented
signatures, which in several cases reflect unexpected behavior.
5.2RQ2: Coverage Achieved by Generated Tests
To measure Nessieâ€™s effectiveness at covering the statements of
the library under test, we generate 1,000 tests for each library and
compute the cumulative coverage. To compute coverage, we use
theIstanbulcommandlinecoveragetool nyc,evenifthedevelopers
include their own command for computing coverage, to ensure
consistency. We repeat this experiment with the two variants ofNessie, NES (seq+nest) and NES (seq), and with the baseline LT
approach.
Figure 3 shows how cumulative coverage evolves while gen-
erating 1,000 tests for one package, graceful-fs . As a reference,
the horizontal line shows the coverage directly after loading the
library. The coverage follows a logarithmic shape: a steep increase
incoveragewiththeinitialtestsandaneventualconvergenceto
somecoverageplateau,or atleast,alevelingoffof thecurve.The
final coverage is fairly close between the two variants of Nessie,but the combination of sequencing and nesting converges more
quickly. Moreover, Nessie achieves higher final coverage than LT.
6https://github.com/jprichardson/node-fs-extra/pull/866
7https://github.com/jprichardson/node-jsonfile/pull/146
1500
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ellen Arteca, Sebastian Harner, Michael Pradel, Frank Tip
The number of functions for which Nessie can generate tests
andforwhichLTcannotdependsentirelyontheAPI.Therefore,
the coverage improvements are quite variable across the packages
tested.Wechose graceful-fs asthedemonstrativeexamplebecause
itshowsboththeplateauingofcoverageandtherelativelysmall
differencewegenerallyseebetweenNessiewithbothnesting/se-
quencingandNessiejustsequencing.Thesameplotforallother
packages are in the supplementary material.
To quantify and summarize the coverage results for all libraries,
theleftpartofTable3showsthefinalcoverageachievedafter1,000
tests. The right part of the table quantifies the comparison with
LT. We compute the number of tests required with Nessie to match
andexceedthe coverage that LT achieves after 1,000 tests. The last
columnshowsthenumberoftestsLTrequirestoreachthecoverage
it achieves at 1000 tests (i.e., the beginning of the coverage plateau
LT sustains at 1000 tests). For example, for the fs-extra project,
after1,000testsNessieachievesastatementcoverageof37.2%with
NES (seq+nest) and 35.4% with NES (seq), while LT achieves 34.4%.
NES (seq+nest) matches and also exceeds LTâ€™s coverage after only
311tests;NES(seq)matchesandalsoexceedsLTâ€™scoverageafter
663tests.Meanwhile,after889tests,LTreachesacoverageplateau
that it sustains until 1000 tests.
Overall, Nessie consistently achieves slightly higher coverage
thanLT.Moreover,ourapproachreacheshighcoveragefaster:It
matches and often also exceeds the coverage that LT has after
1,000 tests with substantially fewer tests than LT requires to reach
the same coverage. Comparing the two variants of Nessie, the
combination of sequencing and nesting is more effective.
Answer to RQ2: Nessie achieves a higher coverage than the state
of the art, and fewer tests are required to reach this coverage, in
particular, when the approach uses both sequencing and nesting.
5.3RQ3:Finding BehavioralDifferences during
Regression Testing
ToanswerRQ3,weusethetestsgeneratedbyNessietofindbehav-
ioraldifferencesinconsecutivecommitsofthebenchmarklibraries.
5.3.1 ExperimentalDesign. Tocomparethebehaviorofalibraryat
twocommits,wegenerate100testsbasedonthecodeattheearlier
commit,andthenrunthesetestswiththecodeatbothcommits.We
usethemochatestingframework[ 2]torunourtests.Thisframe-
workcaninternallyhandleerrorsinatest,sothattheremaining
testsareexecuted evenifanerroristhrowninoneofthem.Most
relevant for us, mochahandles errors thrown by asynchronously
invokedfunctionsandlogsthisasan internalasyncerror.Todetect
behavioraldifferences,Nessieproducesthefollowingoutputduringtestexecution:valuesofallAPIfunctionargumentsbeforeandafter
a call; the name of the API function a callback is passed to and the
value of all parameters inside a callback being executed; the return
valueofasuccessfulAPIfunctioncall;thenameoftheAPIfunc-
tionintheeventofafailingcall.Wecomparetheoutputsofboth
commits to identify the following kinds of behavioral differences :
â€¢An API call resulting in an error in one commit successfully
executes in the other.
â€¢A return value of an API call differs between commits.
Figure4:Cumulativenumberofbehavioraldiffs,forspecific
commit of jsonfile.
â€¢An argument to an API call or a parameter of a callback
differs between commits.
â€¢A callback is called in one commit but not in the other.
â€¢mochareportsaninternalasyncerrorinonecommitbutnot
in the other.
Some API updates result in an output difference that does not
indicate a relevant difference in functionality, e.g., due to functionrenaming, a change in the supported version of Node.js, or syntax
errors resulting from migrating to strictmode. After manually
identifyingthesecases,weconfigureouranalysistoignorethem
and do not count them in the experimental results.
To avoid double-counting the same behavioral difference being
exposed by multiple generated tests, we consider differences as
equivalent iftheyare duetothesame kindofdifferenceandarise
from the same API function. Since we are working with asynchro-
nous APIs, the exact ordering of calls may be non-deterministic,
possiblycausingdifferentoutputsacrossrepeatedexecutionsofthe
same test. To avoid reporting scheduling differences as behavioral
differences we execute each test ten times for the same commit
andthencomparethesetsofobserved outputsacross commits. We
report a behavioral difference only if an output observed in one
commit is never observed in the other.
5.3.2 QuantitativeResults. Asarepresentativeexample,Figure4
showsthecumulativenumberofdetectedbehavioraldifferences
while generating 100 tests for a representative commit of jsonfile.
Ascanbeseeninthefigure,NES(seq+nest)ismosteffective,fol-
lowed by NES (seq), and then the existing LT technique.
Table 4 summarizes and quantifies these results across all li-
braries. For each library, we display the number of commit pairs
beingcompared.Weaimtocompare100commits(i.e.,99pairs)per
library, but some of the repositories have fewer than 100 commits
that affect source files. The table reports the number of commit
pairsinwhichtheapproachdetectsabehavioraldifference,withtheaveragenumberofunique(i.e.,non-equivalent)differencesinparen-
theses. For example, the tableâ€™s first row reads: For fs-extra,w e
runtheregressiontestingover99pairsofcommits.NES(seq+nest)
spotsabehavioraldifferencein17ofthesepairs,with2.8unique
1501
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Coverage after 1,000 tests and comparison with LambdaTester (LT).
Coverage after 1,000 tests Tests to match (exceed) 1,000 LT tests
Project NES (seq+nest) NES (seq) LT NES (seq+nest) NES (seq) LT
fs-extra 37.2% 35.4% 34.4% 311 (311) 663 (663) 889
jsonfile 87.2% 80.9% 80.9% 107 (209) 95 (N/A) 117
node-dir 32.5% 32.5% 32.5% 96 (N/A) 115 (N/A) 199
bluebird 48.0% 47.1% 47.1% 433 (433) 644 (N/A) 644
q 67.2% 66.4% 67.1% 103 (105) 765 (765) 980
graceful-fs 48.5% 47.0% 41.3% 49 (53) 49 (53) 534
rsvp.js 66.0% 66.0% 64.8% 177 (268) 196 (268) 325
glob 36.1% 36.1% 36.1% 3 (25) 181 (N/A) 76
zip-a-folder 32.0% 24.0% 24.0% 1 (793) 3 (N/A) 3
memfs 55.5% 48.3% 48.3% 84 (84) 702 (N/A) 702
Table 4: Pairs of commits checked via regression testing and
behavioral differences found.
With diff. (avg. unique per diff)
Project Compared pairs NES (seq+nest) NES (seq) LT
fs-extra 99 17 (2.8) 14 (3.2) 10 (3.2)
jsonfile 44 12 (1.9) 9 (2.0) 8 (1.9)
node-dir 29 8 (1.5) 6 (2.3) 5 (1.6)
bluebird 99 0 (0.0) 0 (0.0) 0 (0.0)
q 99 92 (1.7) 69 (1.7) 61 (1.8)
graceful-fs 75 18 (1.1) 18 (1.1) 3 (1.3)
rsvp.js 99 0 (0.0) 0 (0.0) 0 (0.0)
glob 99 4 (1.5) 4 (1.3) 4 (1.0)
zip-a-folder 6 1 (1.0) 1 (1.0) 1 (1.0)
memfs 99 14 (1.1) 14 (1.1) 0 (0.0)
differences per pair, on average. NES (seq) finds a difference in
14 of these pairs (3.2 unique differences, on average), and LT in
10ofthese(3.2uniquedifferences,onaverage).Theresultsshow
thesametrendasthatseeninRQ2:Nessiefindsmorebehavioral
differences than LT (166 vs. 92 in total), and the combination of
nesting and sequencing is worthwhile (166 vs. 135 differences in
total).
5.3.3 Qualitative Results. To better understand the behavioral dif-
ferencesthatNessiereveals,wemanuallyinspectarandomsample
of them. Table 5 summarizes the results. For each analyzed dif-ference, we include a hyperlink to the commit introducing thedifference, the name of the project, a categorization of the differ-
ence,andadescriptionofhowitmanifests.Thecategorizationswe
use are as follows:
â€¢Bug: This commit is introducing a bug.
â€¢Bug fix: This commit is fixing a bug.
â€¢Upgrade: This commit is an update/upgrade of the API,
including migration to newer APIs, updating method signa-
tures, and making functions async.
We find that Nessie detects a variety of different types of API func-
tionality changes. Interestingly, for several commits that introduce
abug,Nessielaterfindstheâ€œdualâ€committhatfixesthatsamebug.
Tobetterunderstandtheimpactofnesting,wecheckedforeach
inspectedbehavioraldifferencewhetheritcouldalsobeexposed
byatestcasewithoutnesting.Similartotheexamplementioned
in the introduction, we find that creating a sequence-only test case
is, in principle, possible for each of the behavioral differences. The
key benefit provided by nesting is to more quickly cover a diverseTable 5: Manual analysis of behavioral differences found via
regression testing.
Commit Project Diagnosis Description of behavioral difference
a149f82 fs-extra Bug outputJSON executes callback even with bad
arguments in newer commit
dba0cbb fs-extra Upgrade many API functions no longer error or return
different values in newer commit
03b2080 fs-extra Bug fix existsreturns callback argument return value
on error instead of undefined
df125be fs-extra Bug ensureSymLink executesthecallbackargument
even with incorrect arguments
3fc5894 fs-extra Bug fix ensureFile throws error on incorrect argu-
ments instead of executing callback
ef9ade4 fs-extra Bug copyFile doesnâ€™t throw error with incorrect
non-callback arguments
2e4fcae fs-extra Upgrade writevreturns rejected promise instead of
throwing error on incorrect arguments
075c2d1 fs-extra Bug moveandcopynow executes the callback argu-
ment even with incorrect arguments
4a0ebe5 jsonfile Bug fix readFile executes callback in newer commit
b1f40ef jsonfile Upgrade readFileSync succeeds in newer commit (er-
rors in older commit)
4b90419 jsonfile Upgrade readFileSync errors in newer commit (suc-
ceeds in older commit)
995aa63 jsonfile Bug writeFile executes callback in newer commit
e3d86e0 jsonfile Bug read/writeFile execute callback even with
bad arguments in newer commit
10eed1d jsonfile Bug readFile sometimes errors in newer commit
(succeeds in older commit)
e5e5aa9 q Upgrade tapandanysucceeds in newer commit (error
in older commit)
2a9a617 graceful-fs Bug fix createReadStream succeeds in newer commit
(infinite loops in older commit)
45a0242 graceful-fs Bug fix lchmodisundefined onLinuxinoldercommit;
succeeds in newer commit
5d961ab graceful-fs Bug fix readFile sometimes executes callback in
newer commit
eaab0ee glob Upgrade globsucceeds in newer commit (error in older
commit)
5d8a060 zip-a-folder Upgrade zipFolder executes callback innewer commit
set of behaviors than with purely sequential tests, and hence, to
expose more behavioral differences in a given testing budget.
AnswertoRQ3: Nessiefindsmanybehavioraldifferencesbetween
versionsoflibraries,includingaccidentallyintroducedbugs,bug
fixes, and API upgrades. These differences are found most quickly
with generated tests that use both sequencing and nesting.
1502
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ellen Arteca, Sebastian Harner, Michael Pradel, Frank Tip
Table 6: Coverage after 1,000 tests at different levels of using
mined nesting examples.
Test coverage, using % mined nestings
Project 0% 25% 50% 75% 100%
fs-extra 32.2% 33.6% 37.2% 33.0% 33.0%
jsonfile 87.2% 83.0% 87.2% 87.2% 87.2%
node-dir 32.5% 33.6% 32.5% 33.2% 33.2%
bluebird 45.7% 51.2% 48.0% 55.1% 48.4%
q 65.7% 66.4% 67.2% 66.3% 66.3%
graceful-fs 48.5% 48.5% 48.5% 48.5% 47.0%
rsvp.js 64.8% 65.0% 66.0% 58.2% 58.2%
glob 36.1% 36.1% 36.1% 36.1% 36.1%
zip-a-folder 24.0% 24.0% 32.0% 32.0% 24.0%
memfs 50.0 % 51.2% 55.5% 51.8% 51.8%
5.4 RQ4: Impact of Guidance by Mined Nesting
Examples
The API usage mining component of Nessie informs the choice of
whichinnerAPIfunctiontocallwhennestingAPIfunctions.By
default, the mined nesting examples are consulted 50% of the time.
Thefollowingmeasurestheeffectofvaryingthispercentageonthe
coverage achieved bythe generated tests. Theexperimental setup
isasinRQ2,butweconsiderthetestgenerationtofollowmined
nesting examples 0%, 25%, 50%, 75%, and 100% of the time.
Table 6 summarizes the result of this experiment by showing
thefinalcoverage aftergenerating1,000tests.Thecorresponding
coverage plots are in the supplementary material. The tableâ€™s first
rowreadsasfollows:For fs-extra,thestatementcoveragewhen
using0%minednestingsis32.2%,whenusing25%minednestingsit
is33.6%,whenusing50%minednestingsitis37.2%,whenusing75%
mined nestings it is 33.0%, and when using 100% mined nestings
itis33.0%.Forreadability,weshowthehighestcoverageforeach
project in bold.
The results illustrate the value of using mined nesting examples:
50% mined nestings always leads to a coverage that is higher than
or at least as high as 0% and 100% mined nesting. This supportsour initial hypothesis that choosing informed nestings is more
likelytoproducevalidteststhatwillincreasecoverage.However,
choosing only mined nestings, i.e, 100%, runs the risk of missing
valid nestings that are simply never seen during the API usage
mining.
Wecurrentlydonothaveresultsontheimpactofmineddatause
ontheregressiontests.However,theincreaseincoveragecaused
byminingAPIusages,whichweobservefor6ofthe10libraries,
suggests that mining may also help during regression testing.
AnswertoRQ4: Choosingminednestings someofthetimeresults
in tests with higher coverage than those generated alwaysornever
usingtheminednestings.Theoptimalparameterdependsonthe
library under test, but 50% is an overall reasonable choice.
5.5 RQ5: Performance of Test Generation
Wemeasurethetimetakentogenerate100testsforeachofthree
approaches we consider, including the discovery phase. We run
theseexperimentsonamachinewithtwo32-core2.35GHzAMD
EPYC7452CPUsand128GBRAM,runningCentOS7.8.2003andNode.js 14.16.1. Since Nessie is implemented in TypeScript, it is
single-threadedandsoonlyusesonecore.Dependingonthelibrary,
thetestgenerationtakesbetween15and30seconds.Theresults
showthatthetimerequiredtogenerate100testsisfairlysimilar
across all three approaches.
Miningthenestingexamplesisanup-frontcost.Asdescribedin
Section4.3,wewroteastaticanalysisinCodeQL[ 15]toidentify
nestingexamples;weranthisoverasetof13,580JavaScriptprojectson GitHub using npm-filter [
8]. This process took around 20 hours.
This set of examples comes with the tool, and users of the tool
would only be required to rerun the mining of nesting examples
if they wanted to generate tests for APIs for which we did not
mineanynestingexamples.Thecostofminingnestingexamples
depends only on the number of mined projects, and not on the
number of APIs being mined for.
AnswertoRQ5: With15to30secondsper100tests,theapproach
is efficient enough for practical use. Extending the set of mined
nestingexamplestakestimeproportionaltothenumberofprojects
mined but is an up-front cost.
5.6 Threats to Validity
Internalvalidity. Ourresultsmaybeinfluencedbyseveralfac-
tors.First,ourbaselineisare-implementationofLT[ 35]because
theoriginalimplementationwasnotfunctionalonourbenchmarks,
andtheirbenchmarksdonotcontainasynchronousAPIs.There-
implementation is based on the original code, which is publiclyavailable, and we clarified questions on the code with the LT au-
thors.Second,ourautomatedidentificationofequivalentbehavioral
differencesisapproximateandmaybothover-andunderapproxi-
matea(theoretical)preciseapproach.Sincetheapproximationis
likelythesameforallevaluatedapproaches,itshouldnotinfluence
the overall conclusions. Finally, the results of manually inspecting
and classifying behavioral differences is subject to our understand-
ingofthetestedlibraries.Tomitigatethisthreat,wediscussedall
cases among the authors.
Externalvalidity. Thelibrariesusedintheevaluationmaynotbe
representativefortheoverallpopulationoflibrarieswithasynchro-nouscallbacks.AsofJuly2021,
npmjs.com reportsatotalof116,342
packagesthataredependentonthepackagesusedintheevaluation,
i.e., the benchmarks at least cover a relevant subset of all libraries.
Finally,ourimplementationtargetsJavaScript,andhence,wedonot claim our empirical results to generalize to other languages.Because callbacks, both synchronous and asynchronous, exist in
variousotherlanguages,wheretheycausesimilarchallengesfor
test generators, we hope our conceptual contributions may inspire
future work for other languages.
6 RELATED WORK
Test generation for higher-order functions. There are several tech-
niquesforautomaticallytestinghigher-orderfunctions.Mostcloselyrelated is LambdaTester [
35], which also targets JavaScript and has
inspired some of our design decisions. The main difference is that
Nessietestsfunctionswithbothasynchronousandsynchronouscallbacks,enabledbyourmethodforAPIdiscoveryandthrough
1503
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
the notion of a test case tree, which allows for nesting calls. Other
testgeneratorscanberoughlycategorizedintorandomtestingand
solver-based, systematic testing. QuickCheck [ 11] is an example
of the former, as it creates callback functions that return a ran-
dom, type-correct value. Koopman and Plasmeijer [20]propose
systematic,syntax-drivengenerationofcallbackfunctionsbased
onuser-providedgenerators.Atestgeneratorforhigher-orderfunc-tionsinRacketreliesontypesandcontractsoftestedfunctions[
19],
twokindsofinformationthatrarelyexistforJavaScriptlibraries.
Solver-basedtestgeneratorsincludeseveralvariantsofsymbolic
and concolic execution adapted to higher-order functions [ 27,44],
andworkthatperformsatype-directed,enumerativesearchover
thespaceoftestcases[ 37].Palkaetal . [31]proposetorandomly
generate type-correct Haskell programs, including higher-order
functions,totestaHaskellcompiler.Alloftheaboveapproachestar-getfunctionallanguages,andnoneofthemconsidersasynchronous
callbacks or produces nested callbacks.
Randomtestgeneration. Nessiebuildsuponarichhistoryofran-
dom test generators, starting with Randoop [ 30], which introduced
feedback-directedrandomtestgeneration.Ourworkalsofollows
this paradigm, but in contrast to Randoop, addresses challenges of
higher-order functions and those arising in a dynamically typed
language. EvoSuite [ 13] uses an evolutionary algorithm to continu-
ouslyimprove randomlygeneratedtestcases.Beyondfunction-level
testing, application-level fuzzing has received significant attention,
including AFL8and its derivatives [ 9,22], and combinations of
fuzzingwithsymbolictesting[ 39].Incontrasttotheabovegreybox
or whitebox fuzzers, Nessie does not need to analyze the library
undertest,butobtainsfeedbackfromtheexecutionofthegenerated
tests and, optionally, uses existing API clients for guidance.
AsynchronousJavaScript. AstudyofcallbacksinJavaScriptcode
finds that 10% of all functions take callback arguments, that the
majorityofthosecallbacksarenested,andthatthemajorityofcall-
backsareasynchronous[ 14].Theseresultsshowthatgenerating
testswithoutconsideringasynchronouscallbacks(i.e.,theteststhatpriorwork[
35]isabletogenerate)failstofullyreflectthebehavior
seeninreal-worldJavaScriptcode.Anotherstudyreportsthatmost
oftheconcurrencybugsinNode.jsareaboutusagesofasynchro-
nousAPIs[ 41].Ourworkisaboutanalyzingtheimplementationof
such APIs. Beyond JavaScript, a study of higher-order functions in
Scalafindsthat7%ofallfunctionsarehigher-order[ 43],suggesting
that the problem we address is relevant beyond JavaScript.
Alimadadi et al . [5]propose a dynamic analysis to trace and
visualizeJavaScriptexecutions,withafocusonasynchronousinter-
actions across the client and the server. Another dynamic analysis
detectspromise-relatedanti-patterns[ 6].Severaltechniquesaimat
detectingracesinJavaScript[ 3,4,32,34,45],whereâ€œraceâ€means
thatdifferentasynchronouslyscheduledcallbacksmaybeexecuted
in more than one order. These approaches are also motivated bythe challenges of asynchronous JavaScript but address problems
orthogonal to that addressed by Nessie.
Thereareseveralformalizationsofdifferentaspectsofasynchro-
nous JavaScript, including an execution model of Node.js [ 23], a
modeltoreasonaboutpromises[ 24],andacalculus,semantics,and
8https://lcamtuf.coredump.cx/afl/implementation of a static analysis of asynchronous behavior [ 38].
The â€œcallback graphâ€ of the latter relates to our test case trees, but
it is created as part of a static analysis and captures a happens-
before-likerelation, whiletest casetrees serveasan intermediate
representation that represents sequencing and nesting.
Program analysis for JavaScript. The popularity of JavaScript
has motivated a variety of dynamic and static analysis techniques,
and we refer to a survey for a comprehensive discussion [ 7]. Ex-
amplesoftechniquesincludedynamicanalysestodetecttypein-
consistencies [ 33], to detect inefficient code [ 16], to detect various
common programming mistakes [ 17], and to reason about taint
flows [18]. Work on reasoning about API changes and how they
affect clients [ 26] is a recent example of a static analysis. Similar
to Nessie, all these analyses take a pragmatic approach toward ad-
dressing the idiosyncrasies of JavaScript, without providing strong
soundness or completeness guarantees.
7 CONCLUSION
EffectivetestgenerationforAPIsthatmakeuseofasynchronous
callback arguments is challenging, as the test generator must gen-
erateteststhatcombinemultiplecallstorelatedAPIfunctionsin
meaningful ways. Generating only sequences of calls, as done in
existingtestgenerators,isinadequate,asitisdifficultforsuchan
approach to produce suitable values to invoke API functions with.
We presented Nessie, the first test generator aimed at APIs with
asynchronous callbacks, which relies on both sequencing and nest-
ing API calls to produce suitable values to invoke API functions
with. Here, nesting here means generated tests may contain API
calls inside the body of callbacks passed to other API calls. In anempirical evaluation, Nessie is applied to ten popular JavaScript
libraries containing 142 API functions with callbacks, and its effec-
tiveness is compared to that of LambaTester, a state of the art test
generation technique that creates tests only by sequencing methodcalls.OurresultsshowthatNessiefindsmorebehavioraldifferences
and achieves slightly higher coverage than LambdaTester. Notably,
it needs to generate significantly fewer tests to achieve and exceed
the coverage achieved by LambdaTester.
TOOL/DATA AVAILABILITY
A full working artifact, including all experimental data associated
withthisresearchisavailableat: https://zenodo.org/record/5874851.
ACKNOWLEDGEMENTS
The authors would like to thank Rob Durst for his contributions to
Nessieâ€™sdiscoveryphase.ThisworkwassupportedbytheEuropean
ResearchCouncil(ERC,grantagreement851895),andbytheGer-
manResearchFoundationwithintheConcSysandPerf4JSprojects.
E. Arteca and F. Tip were supported in part by National Science
FoundationgrantsCCF-1715153andCCF-1907727.E.Artecawas
also supported in part by the Natural Sciences and Engineering
Research Council of Canada.
REFERENCES
[1]2020. ECMAScript2020LanguageSpecification. https://www.ecma-international.
org/ecma-262/11.0/.
[2] 2021. Mocha. https://mochajs.org/. Accessed: 2021-08-24.
1504
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ellen Arteca, Sebastian Harner, Michael Pradel, Frank Tip
[3]Christoffer Quist Adamsen, Anders MÃ¸ller, Saba Alimadadi, and Frank Tip. 2018.
Practical AJAX race detection for JavaScript web applications. In Proceedings of
the 2018 ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018,
LakeBuenaVista,FL,USA,November04-09,2018,GaryT.Leavens,Alessandro
Garcia, and Corina S. Pasareanu (Eds.). ACM, 38â€“48. https://doi.org/10.1145/
3236024.3236038
[4]Christoffer Quist Adamsen, Anders MÃ¸ller, and Frank Tip. 2017. Practical initial-
ization race detection for JavaScript web applications. Proc. ACM Program. Lang.
1, OOPSLA (2017), 66:1â€“66:22. https://doi.org/10.1145/3133890
[5]Saba Alimadadi, Ali Mesbah, and Karthik Pattabiraman. 2016. Understanding
asynchronous interactions in full-stack JavaScript. In Proceedings of the 38th
InternationalConferenceonSoftwareEngineering,ICSE2016,Austin,TX,USA,May
14-22, 2016. 1169â€“1180.
[6]SabaAlimadadi,DiZhong,MagnusMadsen,andFrankTip.2018. Findingbroken
promises in asynchronous JavaScript programs. Proc. ACM Program. Lang. 2,
OOPSLA (2018), 162:1â€“162:26. https://doi.org/10.1145/3276532
[7]Esben Andreasen, Liang Gong, Anders MÃ¸ller, Michael Pradel, Marija Selakovic,
KoushikSen,andCristian-AlexandruStaicu.2017. ASurveyofDynamicAnalysis
and Test Generation for JavaScript. Comput. Surveys (2017).
[8]Ellen Arteca and Alexi Turcotte. 2022. npm-filter: Automating the mining ofdynamic information from npm packages. (2022). https://arxiv.org/abs/2201.
08452
[9]Marcel BÃ¶hme, Van-Thuan Pham, and Abhik Roychoudhury. 2019. Coverage-
Based Greybox Fuzzing as Markov Chain. IEEE Trans. Software Eng. 45, 5 (2019),
489â€“506. https://doi.org/10.1109/TSE.2017.2785841
[10]Ilinca Ciupa, Andreas Leitner, Manuel Oriol, and Bertrand Meyer. 2008. ARTOO:
adaptiverandomtestingforobject-orientedsoftware.In InternationalConference
on Software Engineering (ICSE). ACM, 71â€“80.
[11]Koen Claessen and John Hughes. 2000. QuickCheck: a lightweight tool for
randomtestingofHaskellprograms.In ProceedingsoftheFifthACMSIGPLAN
InternationalConferenceonFunctionalProgramming(ICFPâ€™00),Montreal,Canada,
September 18-21, 2000, Martin Odersky and Philip Wadler (Eds.). ACM, 268â€“279.
https://doi.org/10.1145/351240.351266
[12]Christoph Csallner and Yannis Smaragdakis. 2004. JCrasher: an automatic ro-
bustness tester for Java. Software Prac. Experience 34, 11 (2004), 1025â€“1050.
[13]GordonFraserandAndreaArcuri.2011. EvoSuite:automatictestsuitegeneration
for object-oriented software. In SIGSOFT/FSEâ€™11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19) and ESECâ€™11: 13th European
Software Engineering Conference (ESEC-13), Szeged, Hungary, September 5-9, 2011.
416â€“419.
[14]Keheliya Gallaba, Ali Mesbah, and Ivan Beschastnikh. 2015. Donâ€™t Call Us, Weâ€™llCall You: Characterizing Callbacks in Javascript. In 2015 ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement, ESEM 2015,
Beijing, China, October 22-23, 2015. 247â€“256.
[15]GitHub.2021. QLstandardlibraries. https://github.com/Semmle/ql. Accessed:
2021-04-13.
[16]Liang Gong, Michael Pradel, and Koushik Sen. 2015. JITProf: Pinpointing JIT-unfriendly JavaScript Code. In European Software Engineering Conference and
Symposium on the Foundations of Software Engineering (ESEC/FSE). 357â€“368.
[17]Liang Gong, Michael Pradel, Manu Sridharan, and Koushik Sen. 2015. DLint:
Dynamically Checking Bad Coding Practices in JavaScript. In International Sym-
posium on Software Testing and Analysis (ISSTA). 94â€“105.
[18]Rezwana Karim, Frank Tip, Alena SochurkovÃ¡, and Koushik Sen. 2020. Platform-
IndependentDynamicTaintAnalysisforJavaScript. IEEETrans.SoftwareEng.
46, 12 (2020), 1364â€“1379. https://doi.org/10.1109/TSE.2018.2878020
[19]Casey Klein, Matthew Flatt, and Robert Bruce Findler. 2010. Random testing for
higher-order, stateful programs. In Proceedings of the 25th Annual ACM SIGPLAN
ConferenceonObject-OrientedProgramming,Systems,Languages,andApplications,
OOPSLA2010,October17-21,2010,Reno/Tahoe,Nevada,USA,WilliamR.Cook,
SiobhÃ¡n Clarke,and Martin C.Rinard (Eds.).ACM, 555â€“566. https://doi.org/10.
1145/1869459.1869505
[20]Pieter W. M. Koopman and Rinus Plasmeijer. 2006. Automatic Testing of Higher
Order Functions. In Programming Languages and Systems, 4th Asian Symposium,
APLAS2006,Sydney,Australia,November8-10,2006,Proceedings (LectureNotesin
Computer Science, Vol. 4279), Naoki Kobayashi (Ed.). Springer, 148â€“164. https:
//doi.org/10.1007/11924661_9
[21]Erik Krogh Kristensen and Anders MÃ¸ller. 2017. Type test scripts for TypeScript
testing.PACMPL 1, OOPSLA (2017), 90:1â€“90:25.
[22]Caroline Lemieux and Koushik Sen. 2018. Fairfuzz: A targeted mutation strategyforincreasinggreyboxfuzztestingcoverage.In Proceedingsofthe33rdACM/IEEE
International Conference on Automated Software Engineering. 475â€“485.
[23]MatthewC.Loring,MarkMarron,andDaanLeijen.2017. Semanticsofasynchro-nousJavaScript.In Proceedingsofthe13thACMSIGPLANInternationalSymposium
on on Dynamic Languages, Vancouver, BC, Canada, October 23 - 27, 2017 , Davide
Ancona (Ed.). ACM, 51â€“62. https://doi.org/10.1145/3133841.3133846[24]Magnus Madsen, Ondrej LhotÃ¡k, and Frank Tip. 2017. A model for reasoning
aboutJavaScriptpromises. Proc.ACMProgram.Lang. 1,OOPSLA(2017),86:1â€“
86:24. https://doi.org/10.1145/3133910
[25] Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: Inferring
JavaScriptfunctiontypesfromnaturallanguageinformation.In Proceedingsof
the 41st International Conference on Software Engineering, ICSE 2019, Montreal,
QC, Canada, May 25-31, 2019. 304â€“315. https://doi.org/10.1109/ICSE.2019.00045
[26]Anders MÃ¸ller, Benjamin Barslev Nielsen, and Martin Toldam Torp. 2020. De-tecting locations in JavaScript programs affected by breaking library changes.
Proc.ACMProgram.Lang. 4,OOPSLA(2020),187:1â€“187:25. https://doi.org/10.
1145/3428255
[27]PhucC.NguyenandDavidVanHorn.2015. Relativelycompletecounterexamples
for higher-order programs. In Proceedings of the 36th ACM SIGPLAN Conference
on Programming Language Design and Implementation, Portland, OR, USA, June
15-17, 2015. 446â€“456.
[28]Semih Okur, David L Hartveld, Danny Dig, and Arie van Deursen. 2014. A study
and toolkit for asynchronous programming in C#. In Proceedings of the 36th
International Conference on Software Engineering. 1117â€“1127.
[29]Carlos Pacheco, Shuvendu K. Lahiri, and Thomas Ball. 2008. Finding errors
in.NET withfeedback-directedrandomtesting. In InternationalSymposiumon
Software Testing and Analysis (ISSTA). ACM, 87â€“96.
[30]Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. 2007.
Feedback-Directed Random Test Generation. In International Conference on Soft-
ware Engineering (ICSE). IEEE, 75â€“84.
[31]MichalH.Palka,KoenClaessen,AlejandroRusso,andJohnHughes.2011. Testing
anoptimisingcompilerbygeneratingrandomlambdaterms.In Proceedingsof
the6thInternationalWorkshoponAutomationofSoftwareTest,AST2011,Waikiki,
Honolulu, HI, USA, May 23-24, 2011, Antonia Bertolino, Howard Foster, and
J. Jenny Li (Eds.). ACM, 91â€“97. https://doi.org/10.1145/1982595.1982615
[32]Boris Petrov, Martin Vechev, Manu Sridharan, and Julian Dolby. 2012. Race
Detection for Web Applications. In Conference on Programming Language Design
and Implementation (PLDI).
[33]MichaelPradel,ParkerSchuh,andKoushikSen.2015. TypeDevil:DynamicType
Inconsistency Analysis for JavaScript. In International Conference on Software
Engineering (ICSE).
[34]VeselinRaychev,MartinVechev,andManuSridharan.2013. EffectiveRaceDetec-
tion for Event-Driven Programs. In Conference on Object-Oriented Programming,
Systems, Languages, and Applications (OOPSLA).
[35]Marija Selakovic, Michael Pradel, Rezwana Karim, and Frank Tip. 2018. Test
generationforhigher-orderfunctionsin dynamiclanguages. Proceedingsofthe
ACM on Programming Languages 2, OOPSLA (2018), 1â€“27.
[36]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: a concolic unit testing
engine for C. In European Software Engineering Conference and International
Symposium on Foundations of Software Engineering (ESEC/FSE). ACM, 263â€“272.
[37]Dowon Song, Myungho Lee, and Hakjoo Oh. 2019. Automatic and scalable
detectionoflogicalerrorsinfunctionalprogrammingassignments. Proc.ACM
Program. Lang. 3, OOPSLA (2019), 188:1â€“188:30. https://doi.org/10.1145/3360614
[38]Thodoris Sotiropoulos and Benjamin Livshits. 2019. Static Analysis for Asyn-chronousJavaScriptPrograms. CoRRabs/1901.03575(2019). arXiv:1901.03575
http://arxiv.org/abs/1901.03575
[39]NickStephens,JohnGrosen,ChristopherSalls,AndrewDutcher,RuoyuWang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In
23rdAnnualNetworkandDistributedSystemSecuritySymposium,NDSS2016,San
Diego, California, USA, February 21-24, 2016.
[40]Willem Visser, Corina S. Pasareanu, and Sarfraz Khurshid. 2004. Test input
generation with Java PathFinder. In International Symposium on Software Testing
and Analysis (ISSTA). ACM, 97â€“107.
[41]Jie Wang, Wensheng Dou, Yu Gao, Chushu Gao, Feng Qin, Kang Yin, and Jun
Wei.2017. AcomprehensivestudyonrealworldconcurrencybugsinNode.js.In
Proceedingsofthe32ndIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE2017,Urbana,IL,USA,October30-November03,2017.520â€“531.
[42] Tao Xie, Darko Marinov, Wolfram Schulte, and David Notkin. 2005. Symstra: A
FrameworkforGeneratingObject-OrientedUnitTestsUsingSymbolicExecution.InConferenceonToolsandAlgorithmsfortheConstructionandAnalysisofSystems
(TACAS). Springer, 365â€“381.
[43]Yisen Xu, Fan Wu, Xiangyang Jia, Lingbo Li, and Jifeng Xuan. 2020. Miningthe use of higher-order functions. Empir. Softw. Eng. 25, 6 (2020), 4547â€“4584.
https://doi.org/10.1007/s10664-020-09842-7
[44]Shu-HungYou,RobertBruceFindler,andChristosDimoulas.2021. Soundand
Complete Concolic Testing for Higher-order Functions. In Programming Lan-
guages and Systems - 30th European Symposium on Programming, ESOP 2021,
Held as Part of the European Joint Conferences on Theory and Practice of Software,
ETAPS 2021, Luxembourg City, Luxembourg, March 27 - April 1, 2021, Proceedings
(LectureNotesinComputerScience,Vol.12648),NobukoYoshida(Ed.).Springer,
635â€“663. https://doi.org/10.1007/978-3-030-72019-3_23
[45]Yunhui Zheng, Tao Bao, and Xiangyu Zhang. 2011. Statically locating web
application bugs caused by asynchronous calls.. In WWW. 805â€“814.
1505
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. 