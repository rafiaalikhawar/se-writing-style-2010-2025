CORMS: A GitHub and Gerrit Based HybridCodeReviewer
RecommendationApproachforModernCodeReview
PraharPandya
SoftwareEngineeringResearchLab
DA-IICT Gandhinagar, India
202011001@daiict.ac.inSaurabh Tiwari
SoftwareEngineeringResearchLab
DA-IICT Gandhinagar, India
saurabh_t@daiict.ac.in
ABSTRACT
ModernCodereview(MCR)techniquesarewidelyadoptedinboth
open-source software platforms and organizations to ensure the
quality of their software products. However, the selection of re-
viewers for code review is cumbersome with the increasing size of
development teams. The recommendation of inappropriate review-
ersfor codereviewcantake moretimeandefforttocompletethe
taskeffectively.Inthispaper,weextendedthebaselineofreviewersâ€™
recommendation framework - RevFinder - to handle issues with
newlycreatedfiles,retiredreviewers,theexternalvalidityofresults,
and the accuracies ofthe state-of-the-art RevFinder . Our proposed
hybrid approach, CORMS, works on similarity analysis to compute
similarities among file paths, projects/sub-projects, author infor-
mation,and predictionmodelstorecommendreviewersbasedon
the subject of the change. We conducted a detailed analysis on the
widely used 20 projects of both Gerrit and GitHub to compare our
results with RevFinder . Our results reveal that on average, CORMS,
can achieve top-1, top-3, top-5, and top-10 accuracies, and Mean
Reciprocal Rank (MRR) of 45.1%, 67.5%, 74.6%, 79.9%and 0.58 for
the 20 projects, consequently improves the RevFinder approach by
44.9%, 34.4%, 20.8%, 12.3% and18.4%, respectively.
CCS CONCEPTS
Â·Softwareanditsengineering â†’Softwarelibrariesandrepos-
itories;Software notationsandtools ;
KEYWORDS
Modern Code Review (MCR), Reviewer Recommendations, Data
Mining, GitHub,Gerrit
ACMReference Format:
Prahar Pandya and Saurabh Tiwari. 2022. CORMS: A GitHub and Gerrit
BasedHybridCodeReviewerRecommendationApproachforModernCode
Review. In Proceedingsofthe30th ACMJoint EuropeanSoftware Engineering
ConferenceandSymposium on the Foundationsof SoftwareEngineering(ES-
EC/FSEâ€™22),November14Å›18,2022,Singapore,Singapore. ACM,NewYork,
NY, USA, 12pages.https://doi.org/10.1145/3540250.3549115
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™22, November 14Å›18,2022, Singapore, Singapore
Â©2022 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11...$15.00
https://doi.org/10.1145/3540250.35491151 INTRODUCTION
Softwarecodereviewshelpthedevelopmentprocessinreducing
overallcostsandhelpinknowledgetransfer.Thereviewsonthe
softwarecodeidentifiedlogicalerrorsandcodingruleviolations
andassistedwiththeautomatedtools.Thisreviewprocessisknown
asMCR(ModernCodeReview) [ 1].TheMCRarenowadays used
by both the organizations such as Microsoft [ 9] or Google [ 15],
and by the open-source software (OSS) platforms such as GitHub1,
Gerrit2orreview-board3.
Theexistingworkshighlightedthatselectionandassignmentof
inappropriate reviewers downgrade the review process and quality
ofthesoftwareproduct[ 18].Thongtanunametal.[ 18]investigated
reviews in Gerrit open-source system and found that 4%Å›30% of
the reviews have reviewer assignment issues. As a result, on av-
erage, 12 more days may require to complete it. Jiang et al. [ 8]
also analyzed the pull requests in GitHub and found 40.6% of man-
ualassignments.GerritandGitHubarethetwowidelyusedOSS
platforms for performingcode reviews.
Gerritreviewsuse reviewUI4,whichprovidestoolsupporttopro-
videmany functionalitiestomake thereviewprocesscomfortable
andefficient.InGerrit,developersfirstclonearepositoryandmake
changes to fix bugs or implement new features. When they are
readywith thechanges,theysubmitapullrequesttomergecode
changes into the repository by providing a branch reference for
thatrepository.Then,theycanmanuallyaddthereviewerorcan
use the plugins5or6to add reviewers to the change automatically.
Thereviewersthengetnotified.Whenreviewersmakedecisions,
they have choices to vote from -2 to +2. +2 vote denotes that the
changerequestlooksgoodtothereviewerandisapproved.+1vote
impliesthatthechangerequestisacceptabletothereviewer,but
someone else has to approve it. 0 vote denotes no score. A -1 score
indicates that the reviewer would prefer not to submit this change.
-2 vote implies that the reviewer decided to abandon this change
request.Achangemusthaveatleastone+2voteandno-2votes
before submission.
Figure 1 shows the code review interface of a Gerrit android
project7. This change request â€˜change id-2019419â€™ was reviewed
on10thMarch2022.Here,wecannoticethatthe AuthorX added
the reviewer Reviewer C . However, he is not the actual reviewer,
so he added two more reviewers Reviewer A andReviewer B and
1https://github.com/
2https://www.gerritcodereview.com/
3https://www.reviewboard.org/
4https://gerrit-review.googlesource.com/Documentation/
5https://gerrit-review.googlesource.com/admin/repos/plugins/reviewers,general
6https://gerrit-review.googlesource.com/admin/repos/plugins/reviewers-by-
blame,general
7https://android-review.googlesource.com/c/platform/packages/apps/
Settings/+/2019419
546
ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore PraharPandya andSaurabhTiwari
Figure 1:Codereview interface ofGerrit
removedhimselfasareviewerasheisnolongeranownerofthis
project.Finally,the ReviewerA approvedthechangebyprovidinga
+2 vote. The correct reviewer could be assigned to change requests
without delay after their creation in an ideal scenario. However, it
isnotanordinarycaseatthemoment.Thepull-basedmodelhas
the potential to be more effective and enhancements in these areas.
GitHub is also a widely used OSS platform and supports pull-
based development. In GitHub, developers first fork a repository
andmakechangestofixbugsorimplementnewfeatures.Whenthe
developersarereadywiththechanges,theysubmitapullrequest
to merge the changed code into the repository. The submitted pull
request then needs to be evaluated by the reviewer. In GitHub,
the reviewers are categorising into core members and commenters.
Corememberstakethefinaldecisionforamerge,whilecommenters
help the code review process by providing their suggestions in
comments. Ideally, reviewers are assigned to pull requests right
after the change requests. However, reviewers may find difficulties
in prioritising the pull requests for some of the popular projects as
theyreceivemanypullrequestsonadailybasis[ 7].Sincereviewers
aresupposedtobeassignedtopullrequestssoonaftertheircreation,
findingan appropriate reviewer can be time-consuming.
Figure2showsthecodereviewinterfaceofthe â€˜changeid-911â€™ of
BetterScientificSoftware project8ofGitHubreviewedonJun8,2021.
Here, the Author X proposed the change â€˜Adding CC for INIâ€™ for
theâ€˜bssw.ioâ€™repositoryandrequestedareviewfromthe Reviewer
A. Author X also assigned this request to Assignee A . In GitHub,
anassigneeisinchargeofmergingthatpullrequestaftergetting
commentsandchangerequestsfromothermaintainers.Afterafew
days,AuthorXcommented â€˜Hey@ReviewerA,sorryforanypressure.
Willyouhaveachancetoreviewthisweek?â€™ However,hedidnotget
any reply. Hence, the Author X put the comment asking â€˜Is there
anyone that has time to review my code?â€™ . After a few days, another
reviewer ReviewerB approvedthe change.Thus AssigneeA finally
8https://github.com/betterscientificsoftware/bssw.io/pull/911 betterscientificsoftware  / bssw.io  Public
Adding CC for INI  #911
 Merged
Assignee A  merged 12 commits into  from 
 on Jun 17, 2021
Conversation 10 Commits 12 Checks 3 Files changed 1Code Issues 135 Pull requests 14 Actions Projects 2 Wiki
master mcm86-04jun21-inclusive-naming
 Author X  requested a review from Reviewer A  13 months ago
 Author X  assigned Assignee A  on Jun 5, 2021
 Author X  commented on Jun 8, 2021
Hey @Reviewer A  sorry for any pressure. Will you have a chance to review this
week? If not, I can re-assign. Lemme know. Thx.
 Author X  commented on Jun 9, 2021
Is there anyone that has time to review this?
Reviewer B  approved these changes on Jun 15, 2021
View changes
 Assignee A  merged commit a2163df  into 
on Jun 17, 2021
1 check passed
View details master
Figure 2:Codereview interface ofGitHub
merged12commitsintothemainbranchfromtheforkedbranch
created by Author X. The pull request can be immediately assigned
tothereviewerafterthecreationofthepullrequest.However,it
isnotthe caseatthemoment.Recently, GitHubfollowedaround-
robinandload balanceralgorithm for recommending reviewers9.
In this paper, we propose a hybrid approach, CORMS(Code
ReviewerRecommendationinGitHubandGerritbasedon Machine
Learningand SimilarityAnalysis),forrecommendingactivereview-
ers in code review. The approach considers both a similarity-based
approachandamachinelearning-basedapproachforsolvingthe
problem of recommending code reviewers. A review process in-
cludes several fields while reviewing the code: the subject of the
change(describesthedetailsofthechangeinaveryabstractform),
thefilelocation(recordsthepathsofmodifiedfiles),theauthorfield
(showstheownerinformation),andtheproject/sub-project(include
theprojectandsub-projectinformation).The CORMSmakesuse
ofallofthese fields.The CORMSfirstbuildsa Support Vector Ma-
chine(SVM)modelbyanalyzingtextinthesubjectofthechange
field of the reviews. Next, CORMSbuilds a Similarity Model by
computing the similarity of the file-paths, author information, and
project/sub-project of thecurrent pull request separatelywith the
file-paths, author information, project/sub-project of all the pull
requests historically studied by various developers. We have com-
putedthesimilarityofthesefieldsusingthefour-stringcomparison
techniques, namely longest common sub-sequence, longest com-
monprefix,longestcommonsuffix,andlongestcommonsub-string.
9https://docs.github.com/en/organizations/organizing-members-into-
teams/managing-code-review-settings-for-your-team
547CORMS:A GitHub andGerritBasedHybrid Code ReviewerRecommendation ApproachforModernCode Review ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Next, these SVM and similarity models are combined to achieve
betterperformance.Ourworkaimstoimprovetheapproachpro-
posed by Thongtanunam et al. [ 18],RevFinder10. Consequently, we
have formulatedtworesearchquestions(RQs) for the study.
RQ1: How efficient is CORMSin recommending reviewers for
reviewing the code? What is the performance of CORMSin
comparison withthe state-of-the-art RevFinder ?
RQ2: How much improvement CORMSgain with each newly
addedfeature over state-of-the-art RevFinder ?
RevFinder computes the similarities between the file paths to
recommend code reviewers.Ourproposedapproach, CORMS, also
uses the same four-string comparison techniques defined in the
RevFinder to compute similarities. However, CORMSuses the more
effective â€˜normalizationâ€™ technique for score propagation com-
paredtotheâ€˜Bordacountâ€™[ 13]techniqueusedinstate-of-the-art
RevFinder . Also, the CORMScomputes the similarities between
project/sub-project and authorsâ€™ information with the file paths.
TheCORMSalsoincludestheSVMmodeltoexaminethetextual
contentsintheâ€˜subjectofthechangeâ€™fieldofreviewrequeststo
recommendreviewers.The CORMSthenmergesboththeSVMand
similaritymodeltorecommendthereviewerswithimprovedperfor-
mance.Weevaluatetheperformanceofourapproachon20datasets
collected from both Gerrit and GitHub platforms. We mined and
analyzed 30,648 code reviews from 2020 to 2021. The evaluation
wasdonebycomputingthevaluesoftop-1,top-3,top-5,andtop-10
accuraciesandMeanReciprocalRank(MRR)[ 2].Next,wecompare
our experimental results with the RevFinder [18] and found that
CORMSachieves average top-1, top-3, top-5, and top-10 accuracies,
and Mean Reciprocal Rank (MRR) of 45.1%, 67.5%, 74.6%, 79.9%
and 0.58 for the 20 projects, which improves the performance of
RevFinder by44.9%,34.4%,20.8%,12.3%and18.4%,respectively.The
individualmodelsof CORMSwhencombinedwiththe RevFinder ,
also positively improves the RevFinder w.r.t the evaluation criteria
andgenerateddataset.Our contributionsare as follows:
(1)We propose a hybrid approach CORMS, which combines the
SVM model and Similarity model to recommend active code
reviewers.
(2)We conduct experiments on the datasets of 20 projects of
both Gerrit and GitHub open-source platforms and examine
atotalof30,648reviewsofthem.
(3)We find that CORMSperforms better than the RevFinder
considering evaluation criteriaandadataset of20 projects.
The rest of the paper is organised as follows: Section 2 presents
the motivation behind proposing the approach. Section 3 provides
a brief description of MCR and presents existing literature. Section
4 discusses our approach, CORMS, that extends RevFinder . Section
5 reports the results of our experiments. The section also discusses
threats to validity and our contributions. Section 6 discusses study
limitations and summarises our observations. Finally, Section 7
presents the study conclusions &future directions.
2 MOTIVATION
Atypicalreviewcontainsseveralfieldssuchasthesubjectofthe
change,filepaths,authorsandreviewers.Here,thesubjectofthe
10https://github.com/patanamon/revfinderchange field contains textual information explained in the abstract;
the author field specifies the owner of the change who submits
the review. The project field specifies details about the project
and sub-project under which the author has submitted the review,
and the file path field describes the locations of modified files in
the change. The last reviewed date specified the date when the
review got completed. We looked at several reviews from Gerrit
andmentionedonlyselectedreviewstodemonstratetheinsights.In
this work, we are more interested in investigating the relationship
betweenauthorsandsub-projectinformationwiththereviewers,as
existingstudiesrarelyintroducedthesefeaturesintotheirreviewer
recommendationprocess.Thus,weusedGerritSearchAPItosearch
codereviewsfor thesamesub-projectandthesameauthors.As a
result, we alsofoundsimilarities among reviewers.
Tables1and2present reviews80832311and80830412from the
Open-Stack project of Gerrit. Due to space limitations, we present
only the values of some fields in the reviews. The change reviewed
inreview-808323istoupdate Å‚TOX-CONSTRAINTS-FILEÅ¾ forsta-
ble/xena, thechange in review808304 is to update Å‚.gitreviewÅ¾ for
stable/xena. We notice that these changes were from the same
Project/SubProject. Also, all of these changes contain Å‚stable/xenaÅ¾
intheirsubjectfield,andthesereviewsinvolvethesameauthors
andsamereviewers.Tables3,4and5representreviews83662913,
79822814, and 83681415from the Open-Stack project of Gerrit. The
changereviewedinreview-836629istoremovethe Å‚TripleOjobÅ¾ .
Thechangereviewedinreview-798228istoremovethe Å‚TripleO
jobÅ¾ofSteinasstable/steinbranchforalltripleorepositorieswas
moved to the end of life. The change in review 752343 is to remove
theÅ‚TripleO jobÅ¾ ofUssurias stable/ussuri branch for all tripleo
repositories was moved to end of life. We notice that all of these
changes were from the same sub-project Å‚puppet-swiftÅ¾ , and the
same authorproposedthesechanges.
Table 1:Code-Review #808323:Gerrit
Subject UpdateTOX_CONSTRAINTS_FILEforstable/xena
Project/Subproject openstack/python-neutronclient
Author AuthorX
Reviewers ReviewerA, ReviewerB
File-Path tox.ini
Table 2:Code-Review #808304:Gerrit
Subject Update.gitreviewforstable/xena
Project/Subproject openstack/python-neutronclient
Author AuthorX
Reviewers ReviewerA, ReviewerB
File-Path .gitreview
ObservationsandImplications:
The followingobservations can be made by lookinginto the five
code reviews:
(1)The project/sub-project field information can help recom-
mend appropriate reviewers. The reviewer is more likely
11https://review.opendev.org/c/openstack/python-neutronclient/+/808323
12https://review.opendev.org/c/openstack/python-neutronclient/+/808304
13https://review.opendev.org/c/openstack/puppet-swift/+/836629
14https://review.opendev.org/c/openstack/puppet-swift/+/798228
15https://review.opendev.org/c/openstack/puppet-swift/+/836814
548ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore PraharPandya andSaurabhTiwari
Table 3:Code-Review #836629:Gerrit
Subject RemoveTripleO job
Project/Subproject openstack/puppet-swift
Author AuthorY
Reviewers ReviewerC,ReviewerD, ReviewerE
File-Path .zuul.yaml
Table 4:Code-Review #798228:Gerrit
Subject Steinonly:RemoveTripleO job
Project/Subproject openstack/puppet-swift
Author AuthorY
Reviewers ReviewerC,ReviewerF
File-Path .zuul.yaml
Table 5:Code-Review #836814:Gerrit
Subject Ussuri only:RemoveTripleO job
Project/Subproject openstack/puppet-swift
Author AuthorY
Reviewers ReviewerC
File-Path .zuul.yaml
to review multiple changesfrom the same projects or sub-
projects.Tables1and2showthechangerequestforthesame
projectÅ‚openstack/python-neutronclientÅ¾ where the two dif-
ferentreviewsofthesameprojectwereassignedtothesame
reviewers- ReviewerA andReviewerB .Also,changerequests
shown in Tables 3, 4 and 5 are from the same sub-project:
Å‚openstack/puppet-swiftÅ¾ . These three reviews were assigned
to the same reviewer - Reviewer C .
(2)Thetextualcontentsinthesubject(ortitle)fieldgiveabstract
information about the change and are good indicators to
recommend suitable reviewers. For example, the content
statedinthesubjectfieldofreviewsinTables1and2specifies
that the changes are from the Å‚stable/xenaÅ¾ branch. The
textualcontentsofthesubjectinTables3,4,and5specify
that the changes are madeto remove the Å‚TripleOjobÅ¾ .
(3)Similar reviewers can review the review request from the
sameauthorsastheyknoweachotherbecauseofpastcol-
laborations.FromTables1and2,wecanseethatthecode
reviewsubmittedbythe AuthorX isreviewedbythesame
reviewers ReviewerA andReviewerB . Tables3, 4and 5 show
that the code review requests submitted by the Author Y
were assignedto the same reviewer - Reviewer C .
(4)The same reviewers should review the files stored in the
samepaths.FromTables3,4and5,wecanseethatallthe
codereviewrequestschangethefile Å‚.zuul.yamlÅ¾ ,andthus
they were assignedto the same reviewer - Reviewer C .
3 BACKGROUNDAND RELATED WORK
This section describes the background related to Modern Code
Review(MCR) andexisting MCRrecommendation techniques.
3.1 Modern CodeReview
Figure3showstheworkingprocessofMCR.TheMCRprocesscon-
sistsoftwophases:(1)Reviewerplanningandsetupand(2)Code
review. The review planning and setup phase consist of 3 steps. In
thepreparationstep,theauthorprepareschangemetadatawithade-
scription providing extra details of the change. Next, the reviewers
areselected.Finally,reviewersreceivealertstoreviewthereviewernotificationstep.Thecodereviewconsistsofthreemainsteps.In
the code checking step, the reviewer individually performs code
checking.Theymayalsointeractwiththeauthorandthemselvesin
theReviewerInteractionstep.Inthereviewerdecision,reviewers
decide on the change request, which can be reworked, accepted, or
rejected.MCRprocesshasfourroles:author,developer,maintainer,
andcommenter.Inrecentyears,MCRgainedincreasingpopularity.
Therefore,tounderstandthecomprehensiveperspectiveofMCR,
Davilaetal.[ 5]conductedasystematicliteraturereviewonMCRby
identifying 139 primary studies after applying search and selection
criteria. The benefits of code review are (1) finding defects in both
low-levelandhigh-leveldesign,(2)codeimprovementstohandle
coding conventions with no defect identification, (3) alternative
solutions, (4) knowledge transfer which gives another developer
insightaboutcode,(5)teamawarenessandtransparencywiththe
code,and(6) sharingcode ownership[ 1].
PreparationReviewer
SelectionReviewer
Notification
Code CheckingReviewer
InteractionReviewer
Decision
Re-Work Accept RejectReviewer Planning and Setup
Code Review
Figure 3:WorkingofModernCodeReview (MCR)
Whilechallengesofcodereviewinclude(1)understandingthe
reason for the change, (2) familiarity with the code - it takes more
time to review files for the reviewers who are not familiar with the
code, (3) dealing with understanding needs - reviewers, in general,
may try different paths like sending emails or talk in person for
asking clarifications, (4) lax review means directly approves the
changerequest,(5)codesizeandpoorquality,(6)delayedfeedbacks,
(7) difficulties infinding proper reviewers, (8) lack ofdevelopersâ€™
plan for knowledge sharing, (9) change request rejections, and (10)
lackofavailable reviewing guidelines [ 1][6].
3.2 Related Work
Recommending reviewers is the most common support in MCR.
Mostly, recommendation techniques use a review history of the
reviewersandtheirintereststobuildrecommendations.Thefirst
published technique is Review Bot [3], which considers the line
change history for recommending reviewers and recommends the
same reviewers who have previously worked on the same lines.
However, the ReviewBot algorithmhas several issues.One iswith
thenewlycreatedfilesthatdonothaveanylinechangehistoryand
thuscannotbecorrectlyprocessedusingthisapproach.Another
problem is that most of the lines in the average project are only
changedonce.Therefore,theaccuracyofresultsreturnedbythe
ReviewBot algorithmislimited.Thongtanunametal.[ 18]proposed
an approach, RevFinder , which computes the similarity between
the file paths. The proposed work is now a baseline for evaluating
mostreviewerrecommendationapproaches.However, RevFinder
549CORMS:A GitHub andGerritBasedHybrid Code ReviewerRecommendation ApproachforModernCode Review ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Table 6:Technique,Platform, Source Code,Data-sets, Metrics Used
ExistingApproaches YearOSS
PlatformTechnique
UsedTestedProjectsAvailability
of DataMetrics
Top -k MRRPrecision Recall
ReviewBot 2013ReviewBoard LineChange History 1 Yes Yes NoNo No
RevFinder 2015Gerrit String Comparison 4 No Yes YesNo No
TIE 2015Gerrit NaiveBayes 4 No Yes YesNo No
Correct 2016GitHub Cosine Similarity 10 No Yes YesYes Yes
CoreDevRec 2015GitHub Machine Learning 5 No Yes YesNo No
Comment Network 2017GitHub Comment Network 84 No Yes YesYes Yes
RevRec 2018GitHub Cosine Similarity 2 No Yes YesYes Yes
WhoReview 2020Gerrit Evolutionary Search 4 No Yes YesYes Yes
Table 7:Featuresused incommonly used Recommendation Algorithms
ExistingApproaches
Feature ReviewBot RevFinder TIECORRECT CoreDevRec Comm. Net RevRec WhoReview Total
FilePath No Yes YesNo Yes Yes Yes No 5
Social Interaction No No NoNo Yes Yes No Yes 3
LineChange History Yes No NoNo No No Yes No 2
Reviewer Activeness No No YesNo Yes No Yes No 3
Subject No No NoNo No No Yes No 1
Libraries No No NoYes No No No No 1
Commit Messages No No YesNo No No No No 1
Author No No NoNo No No No No 0
Reviewer Workload No No NoNo No No No Yes 1
Sub-Project No No NoNo No No No No 0
Total 1 1 31 3 2 4 2
does not consider retired code reviewers and is also not able to
recommendcode reviewers for newfiles.
Another work CORRECT (Code Reviewer Recommendation based
onCross-ProjectandTechnologyexperience) ,proposedbyRahmanet
al.[12],isbasedontheexpertiseofthereviewerswithsimilartech-
nologiesandexternallibraries.However,thistechniquemaynot
be effective when the project does not have external dependencies.
Jiangetal.[ 8]proposedanapproach CoreDevRec(AutomaticCore
MemberRecommendationforContributionEvaluation) basedonMa-
chine Learning and uses Support Vector Machine (SVM) algorithm
and considers file-path information, reviewerâ€™s activeness, and so-
cial relationship. However, this social data is not directly available
onsomeprojects(Apache/Mozilla) orsomeOSS platforms.
Yueetal.[ 22]proposeda CommentNetwork(CN) tocapturesim-
ilar interests in social activities among developers and accordingly
recommendappropriatereviewers.Anotherapproach revrec[21]
considers the participation types to recommend reviewers for code
review requests from the managerial and technical perspectives.
Theproposed hybridapproachcombines similarity,expertise,and
information retrieval methods to recommend reviewers. WhoRe-
view[4]approachusesamulti-objectivesearch-basedmethodfor
maximizing the reviewer collaboration and expertise and minimiz-
ing reviewer workload. It considers reviewersâ€™ expertise by review
frequency(numberofcomments)andrecency(freshnessofcom-
ments),reviewersâ€™ collaboration, andreviewersâ€™ workload.
In the paper â€˜Do Reviewer recommendations help developers?â€™ [9],
the authors showed the need for more user-centric approaches
toreviewerrecommendations.Xiaetal.[ 19]proposeda TIEthat
extends the state-of-the-art RevFinder by developing the hybrid
approach. The approach includes the commit message as a new
feature and using a Naive Bayes Classifier. The authors tested theirresults on the four projects Gerrit.In our proposed work,we have
consideredfeaturessuchasproject/sub-projectinformation,sub-
ject of the change, author information apart from file paths, and
reviewer activeness. We built a hybrid approach to combine the
proposedsimilaritymodelandtheSVMclassifier.Finally,wetested
the performance of RevFinder with our approach CORMSon 20
projectsofGerritandGitHub.
We summarized the well-known algorithms such as Review-
Bot [3], RevFinder [ 18], TIE [19], Correct [ 12], CoreDevRec [ 8],
Comment-Network [ 22], RevRec [ 21], WhoReview [ 4]. Table 6 rep-
resents the platforms, techniques and metrics used, the number
ofprojectstestedforanalysisanddataavailability.Table7shows
the features used incommonly used recommendation algorithms.
Thefeaturesarefilepath,socialinteraction,linechangehistory,re-
vieweractiveness,subjectofthechange,externallibraries/technolo-
gies, commit messages, author, reviewer workload and project/sub-
projectinformation.
4 PROPOSEDAPPROACH
Figure 4 shows our proposed hybrid approach: CORMS. Our ap-
proach consists of three modules: the similarity model, the SVM
model,andthecontrollermodule.Thedetailsofeachofthesteps
ofthe modulesare as follows:
(1)SimilarityModel :computesthesimilaritiesbetweenFile-
Pathsalongwithsub-projectandauthorinformation.
(2)SVMModel :takesthevectorizeddataoftextualcontent-
the subjectofthe changeandpredicts the reviewers.
(3)CORMS Controller : combines and normalizes the similar-
ity and support vector machine (SVM) model results and
filtersoutthereviewersbasedonthereviewerâ€™sactiveness.
550ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore PraharPandya andSaurabhTiwari
Mined DataPre-processed
DataVectorized Data
Similarity ModelSVM Model
CORMS 
ControllerFiltered DataTraining Data Testing Data
CORMSAdd more reviewers
Manually add reviewersFeature Selection
Data Cleaning Subject
Remove stopwords
Remove puntuations
Word Lemmatisation
File Paths
Authors
Sub-Project
Filter by activeness  1 2 3
44
5
6
78
Developer
Figure 4:Proposed Hybrid Approach: CORMS
4.1 Data Mining
As shown in Figure 4, the first step includes the data gathered
from mining Gerrit or GitHub repositories. We created two mining
algorithms:oneforminingGerritreviews andanotherformining
GitHubreviews.OurminingalgorithmforGerritandGitHubmines
thereviewschronologically.WeselectedPythontoimplementthese
algorithms.Wemined30,648reviewsfromGerritandGitHub,from
which 27,157 reviews from 10 Gerrit projects and 3,491 reviews
from 10 GitHubprojects.
4.2 Data Pre-processing
AsshowninFigure4,thesecondstepincludesthepre-processed
data. From the mined data, we extracted the features such as 1)
File-Path info, 2) All Reviewer details, 3) Subject of the change,
4) Project/Sub-project Information, 5) Last reviewed date, 6) Au-
thordetails,7)Changesizewhichisequivalenttoinsertedlines+
deletedlines,and8)Finalreviewersdetailswhovoted+2or-2by
makinganimportantdecision.Then,weremovedthereviewsin
whichthechanging sizewasequal tozero.Wehave alsoremoved
reviewsthathadstatus=Å‚openÅ¾orÅ‚newÅ¾andhadduplicatevalues.
Next, we built the reviewer workload dictionary containing the
key-value pair and the reviewerâ€™s total number of ongoing reviews.
Weobservedthatthehighestworkloadwasassignedtobots.These
botsarenotactualreviewersbutareusedtoverifythetests.Hence,
we removedthesebots from our data.
4.3 NaturalLanguage Processing (NLP) and
VectorTransformation
AsshowninFigure4,thethirdstepincludestheNLPandvector
transformation of the textual content. The subject of the change
contains the textual contents; thus, it is the most important feature
for understanding the developerâ€™s semantics and the reason for the
change.(1)Lowered case : Our first pre-processing step includes lower-
ingthe textual content.
(2)Removal of Punctuation : We removed all punctuation
fromthestringusingthestringlibrary16.Forexample,for
the string Å‚(There is a tree, near the river!)Å¾, after removing
allpunctuation,we can getÅ‚There isatree near the riverÅ¾.
(3)Removal of Stop Words : We removed all stop-words from
the subject of the change using gensim library17. For exam-
ple,afterremovingstop-wordsforthesamestring,wecan
getâ€˜thereâ€™,â€˜treeâ€™,â€˜nearâ€™,andâ€˜riverâ€™.
(4)StringLemmatization :Weusedstringlemmatizationto
reduce the derivationally related forms to a common base
formofaword.Forinstance Å‚am,areÅ¾ fallsinthesameclass
Å‚bÅ¾whileÅ‚car,carâ€™s,carsâ€™,carsÅ¾ falls inthe same class Å‚carÅ¾.
(5)RemovalofCommonWords :Weidentifiedthemostcom-
monwordsfromthesubjectfieldbycreatingakey-valuepair
of each word used in the subject. Here, the key represents
theword,andthevaluerepresentsthefrequencyoftheword
usedintheentireproject.Then,wemanuallyselectedand
removedsuchcommonwordsfromthesubjectofthechange
field.These wordsare: Å‚InsertÅ¾,Å‚UpdateÅ¾,Å‚DeleteÅ¾,Å‚AddÅ¾, etc.
Consequently, we have ensured that the remaining words
containmeaningfulinformationthroughwhichwecantrain
andtest variousmachine learningmodels.
WeusedtheTF-IDFvectorizertotransformastringintonumeric
vectors. Itcontains the following steps:
(1)Tokenization : The first step to implementing TF-IDF is
tokenization,whereastringistokenizedintoabagofwords.
(2)FindTF-IDFValues :TheTFvaluereferstotermfrequency
andcan be calculatedas follows:
ğ‘‡ğ¹=Num ofinstancesof word winsentence s
Total num of wordsinsentence s(1)
16https://docs.python.org/3/library/string.html
17https://pypi.org/project/gensim
551CORMS:A GitHub andGerritBasedHybrid Code ReviewerRecommendation ApproachforModernCode Review ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
IDF refers to the Inverse Document Frequency and can be
calculatedas follows:
ğ¼ğ·ğ¹=Total no.ofsentences
No.ofsentences that contain the wordw(2)
Here,theIDFvalueofaworddependsuponthetotalnumber
ofdocumentsthus,itremainsthesamethroughoutallthe
documents.Onthecontrary,TFvaluesforaworddifferfrom
documenttodocument.Forexample,considertwosubject
linesfromTables4and5:1)Remove TripleOjob from"stein"
only, 2) Remove TripleO job from "ussuri" only. Here, the
word Å‚ussuriÅ¾ in the second sentence occurs only once in
that sentence, andthetotalnumberof wordsinthesecond
sentenceis6.Hence,theTF valuefortheword"ussuri"for
the second sentence is 1/6. Also, we have two sentences and
theword"ussuri"occursinthesecondsentence.Therefore
the IDFvalueofthe word"ussuri"is2/1 =2.
Ourtrainingdataset,i.e., TrainReviews determinesthetextvector
dimensions.Thesedimensionsvariesaccordingtothenumberof
reviewsavailableintheprojectsusedforanalysis.Bydefault,the
Out-Of-Vocabulary(OOV)wordsaregettingignoredbytheTF-IDF
model.However,theOOVwordscanalsobereplacedbytheclosest
vocabulary wordbasedonLevenshtein Distance18as well.
4.4 Data Splitting
AsshowninFigure4,thefourthstepincludesdatasplitting.Finally,
we split the data into two parts: 80% data for training and 20% data
for testing. We define this data as NewReviews andTrainReviews .
NewReviews arethereviewsforwhichwewanttopredicttheresults,
andTrainReviews are the reviews on which we can train the model.
Here,splitting wasdone inchronologicalorder.
4.5 Similarity Model
AsshowninFigure4,inthefifthstep,oursimilaritymodelpredicts
thereviewersusingthesamestringcomparisontechniquesusedin
the state-of-the-art RevFinder . However, we extendeditto compute
similaritiesbetweenprojectsorsub-projectsandauthorinforma-
tion. These code review similarity scores are then propagated to
each of the reviewers. In project or sub-project information, we
onlyconsideredthesub-project,asprojectinformationismostly
thesameinGitHubprojects.Thus,weremovedthatconstantvalues
andonlyconsideredsub-projectinformationaswewantedtobuild
our approach for ageneralizedpurpose.
WeimplementedthissimilaritymodelinPython.Algorithm1
showsthealgorithmofoursimilaritymodel.Ittakesthe NewReview
nrandallTrainReviews asinputandthenextractstheFile-Paths,
Sub-Projects, Authors information from it. It calculates the similar-
ity score as the combination of all similarity scores of File-Paths,
Sub-Projects,and Authors between each NewReview and allTrain-
Reviews. For computing the similarities, this algorithm uses the
four string comparison techniques: longest common sub-sequence,
longestcommonprefix,longestcommonsuffix,andlongestcom-
mon sub-string. Our file paths and sub-project features contain
strings separatedbyâ€˜/â€™.
18https://www.sciencedirect.com/topics/computer-science/levenshtein-distanceAlgorithm1 Similarity Model
Input:ğ‘›ğ‘…(ğ‘ğ‘’ğ‘¤ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤),ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘ 
Output: ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘  (Sortedlistof reviewers assignedwithscore)
1:procedure SimilarityPrediction (ğ‘›ğ‘…,ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘  )âŠ²Com-
putes similarity between NewReview nR and all TrainReviews
2:ğ‘ ğ‘¢ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘—ğ‘›ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘—(ğ‘›ğ‘…)
3:ğ‘ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿğ‘›ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ´ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿ(ğ‘›ğ‘…)
4:ğ‘“ğ‘–ğ‘™ğ‘’ğ‘ ğ‘›ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ¹ğ‘–ğ‘™ğ‘’ğ‘ (ğ‘›ğ‘…)
5:whileğ‘¡ğ‘…âˆˆğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘  do âŠ²Compare withall
TrainReviews
6: ğ‘ ğ‘¢ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘—ğ‘¡ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘—(ğ‘¡ğ‘…)
7: ğ‘“ğ‘–ğ‘™ğ‘’ğ‘ ğ‘¡ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ¹ğ‘–ğ‘™ğ‘’ğ‘ (ğ‘¡ğ‘…)
8: ğ‘ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿğ‘¡ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ´ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿ(ğ‘¡ğ‘…)
9: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘¡ğ‘…â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ (ğ‘¡ğ‘…)
10: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’â†ğ‘‘ğ‘–ğ‘ğ‘¡()
11: whileğ‘“ğ‘âˆˆğ‘“ğ‘–ğ‘™ğ‘’ğ‘ ğ‘›ğ‘…do
12: whileğ‘¡ğ‘âˆˆğ‘“ğ‘–ğ‘™ğ‘’ğ‘ ğ‘¡ğ‘…do
13: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’â†ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’+ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘“ğ‘,ğ‘¡ğ‘)
14: end while
15: end while
16: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’â†ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’+ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘ ğ‘¢ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘—ğ‘›ğ‘…,ğ‘ ğ‘¢ğ‘ğ‘ƒğ‘Ÿğ‘œğ‘—ğ‘¡ğ‘… )
17: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’â†ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’+ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿğ‘›ğ‘…,ğ‘ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿğ‘¡ğ‘… )
18: ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¹ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ â†1.0/summationtext.1ğ‘£âˆˆğ‘ ğ‘ğ‘œğ‘Ÿğ‘’
19: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’â†ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’âˆ—ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¹ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ
20: whileğ‘Ÿâˆˆğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘¡ğ‘… doâŠ²Propagate similarity scores
to reviewers involvedinatrainreviewtR
21: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ [ğ‘Ÿ]â†ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ [ğ‘Ÿ]+ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’[ğ‘Ÿ]
22: end while
23:end while
24:ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ â†ğ‘ ğ‘œğ‘Ÿğ‘¡ğµğ‘¦ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ )
25:end procedure
Considertwofile-paths:
Å‚org/com/v1/view/layout/relative/r1/rlayout.xmlÅ¾ and
Å‚org/com/v2/view/layout/relative/r2/rlayout.xmlÅ¾ .
The length of the longest common prefix is 2 (â€˜orgâ€™, â€˜comâ€™). The
lengthofthelongestcommonsuffixis1(â€˜rlayout.xmlâ€™).Thelength
oflongestcommonsub-sequenceis6(â€˜orgâ€™,â€˜comâ€™,â€˜viewâ€™,â€˜layoutâ€™,
â€˜relativeâ€™, â€˜rlayout.xmlâ€™). The length of the longest common sub-
stringis3(â€˜viewâ€™,â€˜layoutâ€™,â€˜relativeâ€™).Wehaveconsideredauthor
ID forauthorinformation,as theauthoriduniquely identifiesthe
author, and there is no meaning of computing similarities between
the first name or last name of the authors. Thus if the author ID is
different(i.e.100and102),thenthescorewillbe0foreachofthe
four-string comparison techniques. If the author ID is the same (i.e.
100 and 100), then the score will be 1 for each of the four-string
comparison techniques.
Finally,thesimilaritymodelpropagatesthisscoretoallthere-
viewerswhoareinvolvedineach TrainReviews usingnormalization
methodsinsteadofusingtheBordacount[ 13]combinationmethod
used in state-of-the-art RevFinder . We defined normalization factor
ofadictionary dictas follows:
NormalizationFactor =1.0/summationtext.1ğ‘£âˆˆğ‘‘ğ‘–ğ‘ğ‘¡(3)
552ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore PraharPandya andSaurabhTiwari
where,vis the values of dictionary dict. For instance, if we have
a dictionary with a key-value pair ofreviewer-score , then it sums
allthevaluesofthescoreandmeasuresthenormalizationfactor
by dividing this value by 1. Now, for normalizing the scores of the
reviewers,itmultipliesallthereviewersâ€™scoreswiththenormaliza-
tion factor in four dictionaries separately: 1) reviewer-score dictio-
naryobtainedfromcomputingsimilaritiesusinglongestcommon
sub-sequence, 2) reviewer-score dictionary obtained from comput-
ingsimilaritiesusinglongestcommonsub-string,3)reviewer-score
dictionaryobtainedfromcomputingsimilaritiesusinglongestcom-
mon prefix, 4) reviewer-score dictionary obtained from computing
similarities using a longest common suffix. It then combines all
these dictionaries into the final reviewerâ€™s dictionary and sorts this
set of reviewers based on their normalized scores in descending
order. The final output of this model is a set of reviewers with a
score assigned to them. Our results showed that for each newly
addedfeature,oursimilaritymodelgreatlyimprovestheaccuracies
ofstate-of-the-art RevFinder .
4.6 Ensemble Modeling
As shown in Figure 4, our sixth step includes the ensemble tech-
niquetopredictthesetofreviewersbasedontheselectedclassifica-
tion algorithm. We tested four classification algorithms of machine
learning:RandomForest,SupportVectorMachine(SVM),KNearest
Neighbour, and Multinomial Naive Bayes. Our results showed that
SVM gave the best results when used with the same settings for all
the 20 projectsofGerritandGitHub.
Algorithm2 SVM Classification Model
Input:ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£,ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ
Output: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿ
1:procedure SVMPrediction (ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£,ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ )
2:ğ‘šğ‘œğ‘‘ğ‘’ğ‘™â†ğ‘“ğ‘–ğ‘¡(ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ )
3:
4:ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿâ†ğ‘šğ‘œğ‘‘ğ‘’ğ‘™.ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡(ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ)
5: âŠ²Prediction using SVM
6:end procedure
Algorithm2showsthealgorithmoftheclassificationmodelused
inourstudy.WeimplementedtheSVMClassificationModel[ 16]in
PythonLanguage.Thevectorizedrepresentationofthesubjectand
reviewers of all train reviews is taken as input in our prediction
model.Themodelthenpredicts(orrecommends)anappropriate
reviewerforagivenvectorizedrepresentationofthesubjectofa
newreview.
4.7 CORMSController
AsshowninFigure4,intheseventhstepoftheproposedapproach,
our CORMS Controllerprocessallthe newreviews.
We implemented CORMS Controller in Python. Algorithm 3
shows thealgorithm of our CORMS Controller.It takes all NewRe-
viewsandTrainReviews asinput.Thenitconstructsthereviewer
activeness by extracting the closed date from all TrainReviews and
propagating these to all the involved reviewers. This controller
thensortsthisreviewerâ€™sactivenessbasedontheirlastreviewed
date.ThenitprocessesallthenewreviewsandpasseseachoftheseAlgorithm3 CORMS Controller
Input:ğ‘ğ‘’ğ‘¤ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘ ,ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘ 
Output: ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘  (Sortedlistof reviewers assignedwithscore)
1:procedure HybridPrediction (ğ‘ğ‘’ğ‘¤ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘  )
2:whileğ‘¡ğ‘…âˆˆğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘  do
3: ğ‘ğ‘‘â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ¶ğ‘™ğ‘œğ‘ ğ‘’ğ‘‘ğ·ğ‘ğ‘¡ğ‘’ (ğ‘¡ğ‘…)
4: ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ (ğ‘¡ğ‘…)
5: ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘›ğ‘’ğ‘ ğ‘ â†ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’(ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘›ğ‘’ğ‘ ğ‘ ,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£,ğ‘ğ‘‘ )
6:end while
7:ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘›ğ‘’ğ‘ ğ‘ â†ğ‘ ğ‘œğ‘Ÿğ‘¡ğµğ‘¦ğ¿ğ‘ğ‘ ğ‘¡ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘‘ğ·ğ‘ğ‘¡ğ‘’ (ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘›ğ‘’ğ‘ ğ‘ )
8:ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿâ†ğ‘¡ğ‘“ğ‘–ğ‘‘ğ‘“(ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘ ))
9:whileğ‘›ğ‘…âˆˆğ‘›ğ‘’ğ‘¤ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘  do
10: ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿâ†ğ‘¡ğ‘“ğ‘–ğ‘‘ğ‘“(ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘†ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘›ğ‘Ÿ))
11: ğ‘Ÿğ‘’ğ‘£ğ‘†ğ‘–ğ‘šâ†ğ‘†ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘›ğ‘Ÿ,ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘ )
12: ğ‘Ÿğ‘’ğ‘£ğ‘†ğ‘£ğ‘šâ†ğ‘†ğ‘‰ğ‘€ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ,ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘…ğ‘’ğ‘£,
ğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘‰ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ)
13: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿâ†ğ‘Ÿğ‘’ğ‘£ğ‘†ğ‘–ğ‘š+ğ‘Ÿğ‘’ğ‘£ğ‘†ğ‘£ğ‘š
14: ğ‘Ÿğ‘’ğ‘£ğ¹ğ‘ğ‘ğ‘¡â†1.0/summationtext.1ğ‘£âˆˆğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿ
15: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿâ†ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿâˆ—ğ‘Ÿğ‘’ğ‘£ğ¹ğ‘ğ‘ğ‘¡
16: whileğ‘Ÿâˆˆğ‘“ğ‘–ğ‘›ğ‘ğ‘™ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ do
17: ifğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘›ğ‘’ğ‘ ğ‘ [ğ‘Ÿ]>12then
18: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ â†ğ‘Ÿğ‘’ğ‘šğ‘œğ‘£ğ‘’(ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿ[ğ‘Ÿ])
19: end if
20: ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ â†ğ‘ ğ‘œğ‘Ÿğ‘¡ğµğ‘¦ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘’ğ‘Ÿğ‘ )
21: end while
22:end while
23:end procedure
pre-processedreviewstobothsimilarityand ğ‘†ğ‘‰ğ‘€modelstoobtain
the following:
(1) Sortedlistofreviewers predictedbysimilaritymodel.
(2) Reviewer predictedbythe Support Vector Machinemodel.
The CORMS Controller then combines these two results and
normalizesthemusingthesamenormalizationfactordefinedinthe
similarity model, and builds the combined list of a set of reviewers
with a final score assigned to them. It then filters the reviewers
based on the Reviewerâ€™s Activeness . We have chosen 12 months as a
thresholdtodistinguishbetweeninactiveandactivereviewers[ 10].
In other words, we have filtered out all reviewers whose last re-
viewed period is more than 12 months. The final output of this
modelisthe sortedlistof active reviewers.
5 EXPERIMENTS AND RESULTS
In this section, we evaluate and report the results of the experi-
mental study conducted to assess the performance of CORMS. Our
experimental environment includes 14 GB of GPU memory and 12
GB ofRAM.
5.1 ExperimentalSetup
We searched the top projects on GitHub and Gerrit and selected 15
(10 from Gerrit and 5 from GitHub). We selected 5 more projects
fromGitHub byanalysing thecomments and commentersâ€™contri-
butions to the project where the reviewer assignment problem was
found. We mined 30,648 reviews from these 20 selected projects
of both Gerrit and GitHub from 2020 to 2021. For the processing
553CORMS:A GitHub andGerritBasedHybrid Code ReviewerRecommendation ApproachforModernCode Review ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Table 8:Statistics ofthedata collected fromOSS
S.No. Project #Revi. #Files #Re.#Avg. Re.
1 Go 1842 11470 78 3.64
2 Eclipse 1965 17458 135 2.46
3 QT 2144 10832 116 3.61
4 Openstack 2573 5657 269 3.99
5 Android 1903 1117 119 4.45
6 Libreoffice 1879 10573 54 2.26
7 Unlegacy 2624 2342 8 1.41
8 Cloudera 2169 15597 46 3.96
9 OpenCord 6163 53788 45 4.43
10 Chromium 3113 10501 278 2.63
11 TWBS 656 6661 12 1.09
12 Joyent 139 4455 24 1.13
13 BSSW 208 2588 19 1.4
14RenovateBot 18421228 6 1.09
15 Shopify 584 5360 171 1.44
16Nix-Community 418 5072 28 1.19
17 GetSentry 78122525 66 1.22
18FullStoryDev 310 5424 27 1.66
19 NodeJS 113 2114 35 1.58
20 H5bp 98 648 8 1.02
of the textual content, we use the Gensim17library for removing
stop-wordsand WordNetLemmatizer libraryofNLTK19forword
lemmatizationand string16libraryforremovingthepunctuation.
The state-of-the-art RevFinder [18] andTIE [ 19] used the datasets
provided by Thongatanunam et al. [ 18] which contain a total of
42,045 reviews mined in time-period 2008-2014. These approaches
have experimented with four projects of Gerrit: 1) OpenStack, 2)
LibreOffice,3)QT,and4)Android.Theproposedapproach, CORMS,
examinestheauthorâ€™sinformationandthesubjectofthechange
information,whichisnotavailableinthedatasetprovidedbyThon-
gatanunam et al.[ 18].Hence, we createda new datasetbymining
from both GerritandGitHubplatforms.
Statisticsofcollecteddata :Wehaveselected20projectstoanalyse
CORMS, and we selected ten projects from Gerrit and ten projects
fromGitHub.OurselectedGerritprojectsare1)OpenStack,2)Libre-
Office,3)QT,4)Android,5)Go,6)Eclipse,7)Unlegacy,8)Cloudera,
9) Opencord, and 10) Chromium. While our GitHub projects are
asfollows:1)TWBS,2)Joyent,3)Jquery4)NodeJS,5)Shopify,6)
H5bp,7)Nix-Community,8)BSSW,9)RenovateBot,10)FullStory-
dev.Table8showsthestatisticsofthecollecteddatausedtoanalyse
CORMSfrom 2020 to 2021. The column # Revicorrespond to the
total number of collected reviews, including both the closed and
open reviews. The columns # Filescorrespond to the total number
of modified files, # Re.correspond to the total number of unique
codereviewers,and #Avg.Re. correspondtotheaveragenumberof
code reviewers per review. The first ten projects correspond to the
projectsfrom Gerrit,andthe last tenprojectsbelong to GitHub.
5.2 EvaluationMetrics
We evaluate the performance of the CORMSby computing the
Top-KpredictionaccuracyandtheMeanReciprocalRank(MRR)
commonly usedinMCRfor evaluation.
19https://www.nltk.org/_modules/nltk/stem/wordnet.html5.2.1 Top-K. If we have a sorted list of reviewers, then Top-K
accuracy denotes the percentage of reviewers present at the Top-K
positions in this sorted list. The Top-K accuracy is computed as
follows:
Top-K accuracy =/summationtext.1
ğ‘Ÿâˆˆğ‘…isPresent(ğ‘Ÿ,ğ‘˜)
|ğ‘…ğ‘’ğ‘£|(4)
where,Revrefers to a set of reviews and isPresent (r,k) returns 1
ifatleastoneoftheactualreviewersriscorrectlyrecommended
attheTop-kposition,otherwisereturns0.Inthiswork,wehave
consideredKas1,3,5and10.ThevalueofKisdirectlyproportional
to the performance ofthe recommendation technique.
5.2.2 Mean Reciprocal Rank (MRR). MRR corresponds to the aver-
ageofthereciprocalranksofasetofrecommendations.Itdescribes
themeanpositionoftheactualreviewerinthesortedlist.TheMRR
iscomputedas follows:
MeanReciprocal Rank =1
|ğ‘…ğ‘’ğ‘£|âˆ‘ï¸
ğ‘Ÿâˆˆğ‘…1
rank(ğ‘Ÿ,slist(ğ‘Ÿ))âŸ©(5)
where,Revrefers to a set of reviews and rank(r, slist(r)) returns the
position of a reviewer r present in the sorted list - slist. Hence, the
outcomeof 1/rankwillbe0ifnoreviewersarepresentinthelist,
otherwise, if the reviewer appears in the top positions, then 1/rank
will be near to 1, and if the reviewer appears far from the top, then
1/rankwillbe near to 0.
5.3 Research QuestionsandAnalysis
Here, we present the results of an experimental study to answer
the twoformulatedresearch questions(RQs).
RQ1:HowefficientisCORMSinrecommendingreviewersfor
reviewing the code? What is the performance of CORMS in
comparisonwith thestate-of-the-art RevFinder?
Motivation :InthisRQ,weevaluatetheperformanceof CORMS
in comparison with the state-of-the-art RevFinder . As the better
performance CORMScan achieve, the more benefits CORMScan
give to its users.
Approach : We compare CORMSwithRevFinder [18], and evaluate
themon20projectsofGerritandGitHub,andmeasuretheMRR
andTop-K prediction accuracies (k =10, 5, 3, and1).
Results: We defineimprovement(%) as:
Improve(%) =(ğ¸ğ‘¥ğ‘¡ğ‘’ğ‘›ğ‘‘ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦âˆ’ğ‘‚ğ‘Ÿğ‘–ğ‘”ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦)âˆ—100
ğ‘‚ğ‘Ÿğ‘–ğ‘”ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦(6)
Table 9 shows the accuracy comparison between the CORMS
andstate-of-the-art RevFinder fortheTop-KandMRRvalues.Here,
columnâ€˜Revâ€™ correspondstotheaccuraciesof RevFinder andâ€˜Extâ€™
corresponds to the extended accuracies of CORMS. We observe
thatCORMSoutperformsstate-of-the-art RevFinder byasignificant
margin.Onaverageforthe20projects, CORMSachievestop-1,top-
3,top-5,top-10andMRRaccuraciesof45.1%,67.5%,74.6%,79.9%
and0.58,whichimprovesthestate-of-the-art RevFinder by44.9%,
34.4%, 20.8%, 12.3% and 18.4%, respectively. Here, the improvement
of accuracies of Top-K prediction reduces as the k value of Top-
K increases. The reason for that is greater the k value, the more
accuracywillbe there,andthus improvement willbe less.
554ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore PraharPandya andSaurabhTiwari
Table 9:AccuracyComparison: CORMSvsRevFinder
Top-1 Top-3 Top-5 Top-10 MRR
No Project ExtRevImp% ExtRevImp% ExtRevImp% ExtRevImp% ExtRevImp%
1 Go 43.4325.1872.48 70.0743.859.98 74.8158.7627.31 87.9675.18 170.580.431.03
2 Eclipse 55.8118.31204.81 72.6731.4131.43 82.8541.57 99.387.2158.7248.520.670.355.22
3 QT 49.4545.86 7.8364.9255.5216.93 70.7259.3919.08 80.3963.8125.980.590.5310.17
4Openstack 40.6628.244.18 85.9852.62 63.493.1361.3451.83 98.0866.2847.980.640.4431.25
5 Android 60.94 5021.88 65.6357.8113.53 73.4460.9420.51 78.1365.6319.050.660.5516.67
6Libreoffice 74.5754.2937.35 80.5764.5724.78 83.71 7216.26 89.43 7814.650.790.6221.52
7 Unlegacy 64.7947.8935.29 95.7785.9211.46 97.8997.89 010099.30.70.770.6712.99
8 Cloudera 46.1541.0212.51 60.5158.46 3.5171.2870.26 1.45 84.182.312.170.580.555.17
9OpenCord 28.6728.67 071.6771.33 0.4888.33 880.37 96951.050.530.521.89
10Chromium 37.9632.64 16.352.3146.312.98 57.1851.6210.77 61.8155.7910.790.470.4112.77
11 TWBS 52.3851.59 1.5386.5180.95 6.8789.6888.11.7996.8396.83 00.70.682.86
12 Joyent 57.1435.7160.01 78.5742.8683.32 85.7185.71 085.7185.71 00.690.4830.43
13 BSSW 453528.57 77.577.5 087.582.56.06 92.592.5 00.640.5710.94
14RenovateBot 82.562.5 3296.2596.25 0100100 0100100 00.890.7911.24
15 Shopify 8.924.4610016.968.04110.95 23.219.82136.35 32.1417.8679.960.160.0943.75
16Nix-Community 55.7151.43 8.3261.4361.43 067.1464.29 4.4367.1465.712.180.590.573.39
17 GetSentry 11.595.07128.6 41.326.09 58.355.0746.3818.74 59.4257.97 2.50.290.2127.59
18FullStoryDev 23.2123.21 064.2960.71 5.966.0764.29 2.7769.6469.64 00.440.424.55
19 NodeJS 18.7512.5 5043.7531.25 4056.2543.7528.57 62.562.5 00.350.2722.86
20 H5bp 5031.8257.13 81.8245.4580.02 86.3686.36 086.3686.36 00.660.4827.27
- AVG 45.134.344.9 67.554.934.4 74.666.720.8 79.973.812.30.580.4818.4
For Gerrit, we observe CORMSachievesthe most improvement
overstate-of-the-art RevFinder intheproject Eclipse;itimproves
RevFinder by204.81%,131.43%,99.3%,48.52%and55.22%interms
of top-1, top-3, top-5, top-10 and MRR accuracies, respectively.
ForGitHub,weobserve CORMSachievesthemostimprovement
overstate-of-the-art RevFinder intheproject Shopify;itimproves
RevFinder by100%,110.95%,136.35%,79.96%and43.75%interms
oftop-1, top-3, top-5, top-10 andMRR accuracies, respectively.
Table 10: Performance of CORMS with Normalization and
BordaCountscore propagation techniques
BordaCount Normalization
Top-1 38.7% 44.9%
Top-3 30.54 % 34.4%
Top-5 18.3% 20.8%
Top-10 12.3% 12.3%
MRR 16.9% 18.4%
Table 10 shows the evaluation of the performance of CORMS
with two score propagation techniques: â€˜Borda countâ€™ [ 13] used
intheRevFinder andourproposedapproachusesâ€˜normalizationâ€™
technique. We can notice that for the 20 projects, CORMSwhen
used with the normalization technique, achieves the Top-1, 3, 5, 10
andMRRvaluesof44.9%,34.4%,20.8%,12.3%and18.4%respectively,
whichperformsbetterthantheBordacountcombinationtechnique.
RQ2:HowmuchimprovementCORMSgainwitheachnewly
added feature overstate-of-the-art RevFinder?
Motivation : In this RQ, we intend to investigate the improvement
intheperformanceofCORMSforeachnewlyaddedfeatureover
theRevFinder .
Approach : To answer RQ2, we compare the results of the two
prediction models of CORMS: SVM model and similarity modelTable11:Measurementofaccuracygainforeachnewlyadded
feature
SVMModel
(Subject)Similarity Model
(File-Path) +
(SubProject) (Author)
Overall Top-K Gain 7.98% 8.6 % 12.1%
MRRGain 6.31% 7.29% 11.11 %
Gerrit Top-K Gain 6.05% 7.3 % 17.58 %
MRRGain 3.92% 5.88% 15.69 %
GitHub Top-K Gain 9.92% 9.91% 6.62%
MRRGain 8.7 % 8.7 % 6.52%
withstate-of-the-art RevFinder .Weevaluatetheseapproacheson
20projectsofGitHubandGerritandrecordtheTop-Kprediction
accuracies andMRR values.
Results: We define accuracy gain as the difference between the
accuracies of our models built over top of RevFinder and state-
of-the-art RevFinder . Also, we define Top-K gain as the average
accuracygainoftop-5,top-3,andtop-1accuraciesofamodelfor
all the 20 projects w.r.t RevFinder . Similarly, we define the MRR
gainastheaverageMRRgainofamodelforallthe20projectsw.r.t
RevFinder . Table 11 presents the measure of accuracy gain for each
newlyaddedfeature:
(1)Subject: SVM Model predicting reviewers based onthe tex-
tualcontent(subject),whenaddedwiththe RevFinder ,overall
improvesthestate-of-the-art RevFinder inTop-KandMRR
accuracies by7.98% and6.31% respectively.
(2)Sub-Project : Similarity Model computing similarities of
file-paths and sub-projects improves the performance of
RevFinder inTop-KandMRRaccuraciesby8.6%and7.29%
respectively.
(3)Author: Similarity Model computing similarities of file-
pathsandauthorsimprovestheperformanceof RevFinder in
Top-KandMRRaccuraciesby12.1%and11.11%,respectively.
555CORMS:A GitHub andGerritBasedHybrid Code ReviewerRecommendation ApproachforModernCode Review ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
State-of-the-art RevFinder consideronlytheFile-Pathinforma-
tion. Hence, it cannot be able to predict reviewers for the newly
created file where no File-Path information is available. CORMS
considerotherfeaturessuchasAuthors,Subjectofthechange,and
Sub-project details for predicting reviewers. Here, we can observe
that each of the newly added features (sub-projects, authors and
subjects) of CORMSimproves the performance of RevFinder .
5.4 Threatsto Validity
Threats to construct validity refer to the selection of evaluation
criteria. We have used pre-defined Top-K prediction accuracies
and MRR commonly used to evaluate the effectiveness of MCR
recommendationtechniques[ 3][18][19][12][8][22][21][4],andvar-
iousautomatedsoftwareengineeringtechniques[ 14][23][17][20].
Hence,the possibility ofevaluation biasisvery less.
Threats to internal validity refer to the code selection and check
forerror.Wehavechecked,tested,debuggedourcode,andensured
itwouldrun. However,there isstillapossibilityoferrorsthatwe
missed.Also,inthe CORMSapproach,thedataneedstobeupdated
daily/weekly,ormonthlybasedontheorganizationâ€™srequirements
and the frequencies of new reviews updated on the project. So, our
classificationmodelneedstobetrainedforeverynewdatacycle.
Though carefullydone,one can arguefor apotentialbiashere.
Another threat, external validity , may be related to the gener-
alizability of results. Mining data from open-source repositories
arechallenging,especiallyfromGitHub.Forinstance,inGitHub,
weneedtomakemorenestedAPI callstominecodereviewsand
reviewerdetails,complicatingthemining.However,wecanmine
manyreviewsinasingleAPIcallinGerrit.Wehaveanalyzed30,648
reviewsfrom20open-sourceprojectsfor2020to2021fromboth
GerritandGitHubplatforms.
The currently used dataset20can be further improved by analyz-
ingmorereviewsfromGerritandGitHub.Thisdatasetcontainsthe
reviews minedfrom a totalof 34 projectsfrom Gerrit andGitHub,
out of which we used 20 projects in this paper to test the perfor-
mance of CORMSandRevFinder .
6 DISCUSSIONS
A limitation of all such studies related to code reviewer recom-
mendation isestablishing the groundtruth for evaluatingthe rec-
ommendation approach. The reviewer who did the code review
may not have been the best to review, and finding out the best
reviewerfromthesetofrecommendedreviewerswouldbeinterest-
ing. Generally, reviewers are categorised into maintainers (or final
reviewers) and commenters. To analyse whether CORMS recom-
mends a wide range of reviewers or who is being recommended as
reviewers(commentersormaintainers),weselectedfiveprojects
(alreadyusedintheexperimentalsetup)fromGerritandGitHub.
Table 12 shows the analysis ofthe number of predicted reviewers,
maintainers and commenters in RevFinder andCORMSfor the five
projectsofGerritandGitHub.ThecolumnÅ‚#revÅ¾showsthetotal
numberoftestedreviews.ThecolumnÅ‚#pred_reviÅ¾showsthetotal
number of predicted reviewers. The column Å‚rfÅ¾ shows the results
forRevFinder .ThecolumnÅ‚coÅ¾showstheresultsfor CORMS.The
column Å‚#finalÅ¾ shows the total number of maintainers (or final
20https://doi.org/10.6084/m9.figshare.20493042.v4Table12:Analysisofnumberofpredictedreviewers,main-
tainers andcommenters inRevFinder and CORMS
#pred_revi #final #comm
Project #revrf co rfcorfco
openstack 34418704 20178 612660107116
joyent 14100 142 91211
nodejs 16278 528121477
fullstorydev 56358 40737383637
getsentry 1385195 5444 91985557
reviewers).ThecolumnÅ‚#commÅ¾showsthetotalnumberofcom-
menters.Here,wefoundthatourapproach CORMSrecommends
a wider range of reviewers than RevFinder .CORMSrecommends
more maintainers (or final reviewers) than the RevFinder . Both
RevFinder andCORMSpredicts more number of maintainers (or
final reviewers)thanthe commenters.
This workâ€™s primary focus is accessing performance of CORMS
for both Gerrit and GitHub OSS platforms. The proposed approach
is a conceptual replication of RevFinder , and the primary contribu-
tionisimprovementsandamorecomprehensiverangeoftestingfor
both OSS platforms. We highlighted how each of the newly added
features (projects/sub-projects, author information, subject) for the
approach positively improves the accuracy of RevFinder (in RQ1
and RQ2). Our results also showed that the authorâ€™s information is
animportantfeatureandimprovestheoverallTop-Kaccuracyby
12.1%.However,infuture,weplantoconductempiricalstudieswith
developers/reviewerstotesttheeffectivenessofCORMSinpractice.
The run-time performance of the CORMS is time-consuming for a
newreviewrequest. The trainedmachinelearningSVMmodelin
CORMSissavedinafileandrestoredtoreusethetrainedmodel
on the new data. Subsequently, the similarity model metrics are
computed by comparing each New Review with all the TrainRe-
views. After combining both the models, the reviewer prediction
takes O(n) time for each new review request, which is the same as
theRevFinder .
7 CONCLUSION & FUTUREWORK
Weproposedahybridapproach, CORMS,whichworksonsimilarity
analysis to compute similarities among file paths, projects/sub-
projects, author information, and prediction models to recommend
reviewers based on the subject of the change. We conducted a
detailed analysis on the widely used 20 projects of both Gerrit and
GitHubtocompare our resultswith the state-of-the-art RevFinder .
Our results show that on average CORMScan achieve top-1, top-3,
top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR)
of 45.1%, 67.5%, 74.6%, 79.9% and 0.58 for the 20 projects, which
improves thestate-of-the-art approach RevFinder by44.9%,34.4%,
20.8%, 12.3% and18.4%, respectively.
It would be interesting to investigate the results of the combina-
tion of our approach with TIE. As the proposed methodology does
not consider the workload issues and the effect of social collabora-
tionsbetween thedevelopers,hence, plan toconsideritfor future
work.
DATA AVAILABILITY
All dataset created and used in the study for analysis is openly
available [ 11]onfigshare.
556ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore PraharPandya andSaurabhTiwari
REFERENCES
[1]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In 201335th International Conferenceon Software
Engineering (ICSE) . IEEE,712Å›721. https://doi.org/10.1109/ICSE.2013.6606617
[2]Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information
Retrieval. Addison-WesleyLongmanPublishing Co., Inc.,USA.
[3]Vipin Balachandran. 2013. Reducing human effort and improving quality in peer
code reviewsusingautomaticstaticanalysisandreviewerrecommendation.In
2013 35th International Conference on Software Engineering (ICSE) . IEEE, 931Å›940.
https://doi.org/10.1109/ICSE.2013.6606642
[4]MoatazChouchen,AliOuni,MohamedWiemMkaouer,RaulaGaikovinaKula,
and Katsuro Inoue. 2021. WhoReview: A multi-objective search-based approach
forcodereviewersrecommendationinmoderncodereview. AppliedSoftCom-
puting100(2021), 106908. https://doi.org/10.1016/j.asoc.2020.106908
[5]Nicole Davila and Ingrid Nunes. 2021. A systematic literature review and tax-
onomyofmoderncodereview. JournalofSystemsandSoftware (2021),110951.
https://doi.org/10.1016/j.jss.2021.110951
[6]Nargis Fatima, Suriayati Chuprat, and Sumaira Nazir. 2018. Challenges and
Benefits of Modern Code Review-Systematic Literature Review Protocol. In 2018
InternationalConference on Smart Computing and ElectronicEnterprise (ICSCEE) .
IEEE,1Å›5. https://doi.org/10.1109/ICSCEE.2018.8538393
[7]GeorgiosGousios,AndyZaidman,Margaret-AnneStorey,andArieVanDeursen.
2015. Workpracticesandchallengesinpull-baseddevelopment:Theintegratorâ€™s
perspective. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering , Vol. 1.IEEE,358Å›368. https://doi.org/10.1109/ICSE.2015.55
[8]Jing Jiang, Jia-Huan He, and Xue-Yuan Chen. 2015. Coredevrec: Automatic
core member recommendation for contribution evaluation. Journal of Computer
ScienceandTechnology 30,5(2015),998Å›1016. https://doi.org/10.1007/s11390-
015-1577-3
[9]VladimirKovalenko,NavaTintarev,EvgenyPasynkov,ChristianBird,andAl-
berto Bacchelli. 2018. Does reviewer recommendation help developers? IEEE
Transactions on Software Engineering 46, 7(2018), 710Å›731. https://doi.org/10.
1109/TSE.2018.2868367
[10]JakubLipcakandBrunoRossi.2018. Alarge-scalestudyonsourcecodereviewer
recommendation. In 2018 44th Euromicro Conference on Software Engineering and
Advanced Applications (SEAA) . IEEE, 378Å›387. https://doi.org/10.1109/SEAA.
2018.00068
[11]Prahar Pandya and Saurabh Tiwari. 2022. CORMS Dataset. https://doi.org/10.
6084/m9.figshare.20493042.v4 .
[12]Mohammad Masudur Rahman, Chanchal K. Roy, and Jason A. Collins. 2016.
CoRReCT:CodeReviewerRecommendationinGitHub BasedonCross-Project
andTechnologyExperience.In Proceedingsofthe38thInternationalConference
onSoftwareEngineeringCompanion (Austin,Texas) (ICSEâ€™16) .AssociationforComputingMachinery,NewYork,NY,USA,222Å›231. https://doi.org/10.1145/
2889160.2889244
[13]Romesh Ranawana and Vasile Palade. 2006. Multi-classifier systems: Review and
a roadmapfor developers. International journalof hybrid intelligent systems 3, 1
(2006), 35Å›61. https://doi.org/10.3233/HIS-2006-3104
[14]Shivani Rao and Avinash Kak. 2011. Retrieval from software libraries for bug
localization:acomparativestudyofgenericandcomposite textmodels.In Pro-
ceedings of the 8th Working Conference on Mining Software Repositories . 43Å›52.
https://doi.org/10.1145/1985441.1985451
[15]Caitlin Sadowski, Emma SÃ¶derberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern code review: a case study at google. 181Å›190. https:
//doi.org/10.1145/3183519.3183525
[16]ShanSuthaharan.2016. Supportvectormachine. In Machinelearningmodelsand
algorithms for big data classification . Springer, 207Å›235. https://doi.org/10.1007/
978-1-4899-7641-3
[17]Ahmed Tamrawi, Tung Thanh Nguyen, Jafar M Al-Kofahi,and TienN Nguyen.
2011. Fuzzy set and cache-based approach for bug triaging. In Proceedings of the
19th ACMSIGSOFT symposium andthe 13th European conference on Foundations
ofsoftwareengineering . 365Å›375. https://doi.org/10.1145/2025113.2025163
[18]Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Raula Gaikovina Kula,
Norihiro Yoshida, Hajimu Iida, and Ken-ichi Matsumoto. 2015. Who should
review my code? a file location-based code-reviewer recommendation approach
formoderncodereview.In 2015IEEE22ndInternationalConferenceonSoftware
Analysis, Evolution, and Reengineering (SANER) . IEEE, 141Å›150. https://doi.org/
10.1109/SANER.2015.7081824
[19]XinXia,DavidLo,XinyuWang,andXiaohuYang.2015. Whoshouldreviewthis
change?:Puttingtextandfilelocationanalysestogetherformoreaccuraterecom-
mendations.In 2015IEEE InternationalConference onSoftwareMaintenanceand
Evolution(ICSME) . IEEE,261Å›270. https://doi.org/10.1109/ICSM.2015.7332472
[20]Xin Xia, David Lo, Xingen Wang, Chenyi Zhang, and Xinyu Wang. 2014. Cross-
language bug localization. In Proceedings of the 22nd International Conference on
ProgramComprehension . 275Å›278. https://doi.org/10.1145/2597008.2597788
[21]ChengYang,Xun-huiZhang,Ling-binZeng,QiangFan,TaoWang,YueYu,Gang
Yin, and Huai-min Wang. 2018. RevRec: A two-layer reviewer recommendation
algorithm in pull-baseddevelopmentmodel. Journalof CentralSouthUniversity
25,5 (2018), 1129Å›1143. https://doi.org/10.1007/s11771-018-3812-x
[22]Yue Yu, Huaimin Wang, Gang Yin, and Tao Wang. 2016. Reviewer recom-
mendation for pull-requests in GitHub: What can we learn from code review
andbugassignment? InformationandSoftwareTechnology 74(2016),204Å›218.
https://doi.org/10.1016/j.infsof.2016.01.004
[23]Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs be fixed?
more accurate information retrieval-based bug localization based on bug reports.
In2012 34th InternationalConference on Software Engineering (ICSE) . IEEE, 14Å›24.
https://doi.org/10.1109/ICSE.2012.6227210
557