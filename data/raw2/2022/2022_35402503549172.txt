A RetrospectiveStudyofOne Decade ofArtifactEvaluations
StefanWinter
LMUMunich
Munich, Germany
sw@stefan-winter.netChristopher S.Timperley
CarnegieMellonUniversity
Pittsburgh,USA
ctimperley@cmu.eduBenHermann
TechnischeUniversitÃ¤tDortmund
Dortmund,NRW,Germany
ben.hermann@cs.tu-dortmund.de
JÃ¼rgenCito
TUWien
Vienna, Austria
juergen.cito@tuwien.ac.atJonathanBell
NortheasternUniversity
Boston,MA, USA
j.bell@northeastern.eduMichael Hilton
CarnegieMellonUniversity
Pittsburgh,PA, USA
mhilton@cmu.edu
Dirk Beyer
LMUMunich
Munich, Germany
dirk.beyer@sosy-lab.org
ABSTRACT
Mostsoftware-engineeringresearchinvolvesthedevelopmentof
aprototype,aproofofconcept,orameasurementapparatus.To-
gether with the data collected in the research process, they are
collectively referred to as research artifacts and are subject to arti-
fact evaluation (AE) at scientific conferences. Since its initiation in
the software-engineeringcommunityatESEC/FSE 2011,boththe
goalsandtheprocessofAEhaveevolvedandtodayexpectations
towards AE are strongly linked with reproducible research results
andreusabletoolsthatotherresearcherscanbuildtheirworkon.
However, to date little evidence has been provided that artifacts
thathavepassedAEactuallyliveuptothesehighexpectations,i.e.,
towhichdegreeAEprocessescontributetoAEâ€™sgoalsandwhether
the overheadthey impose is justified.
Weaimtofillthisgapbyprovidinganin-depthanalysisofre-
search artifacts from a decade of software engineering (SE) and
programminglanguages(PL)conferences,basedonwhichwere-
flect on the goals and mechanisms of AE in our community. In
summary, our analyses (1) suggest that articles with artifacts do
notgenerallyhavebettervisibilityinthecommunity,(2)provide
evidencehowevaluatedandnot evaluatedartifactsdifferwithre-
spect to different quality criteria, and (3) highlight opportunities
for furtherimprovingAE processes.
CCSCONCEPTS
â€¢General and reference â†’Empirical studies ;â€¢Software and
itsengineering â†’Softwarepost-developmentissues ;â€¢Informa-
tionsystems â†’Digital librariesandarchives .
ESEC/FSE â€™22, November 14Å›18,2022, Singapore, Singapore
Â©2022 Copyright heldby theowner/author(s).
ACM ISBN 978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3549172KEYWORDS
Researchartifacts,Artifactevaluation,Openscience,Reproduction,
Reuse,Long-term availability of software anddata
ACM Reference Format:
StefanWinter,ChristopherS.Timperley,BenHermann,JÃ¼rgenCito,Jonathan
Bell,MichaelHilton,andDirkBeyer.2022.ARetrospectiveStudyofOne
DecadeofArtifactEvaluations.In Proceedingsofthe30thACMJointEuro-
peanSoftwareEngineeringConferenceandSymposiumontheFoundations
of Software Engineering (ESEC/FSE â€™22), November 14Å›18, 2022, Singapore,
Singapore. ACM, New York, NY, USA, 12pages.https://doi.org/10.1145/
3540250.3549172
1 INTRODUCTION
As reported in a 2016 Nature article, the scientific research commu-
nityfacesa Å‚reproducibilitycrisis.Å¾ 70% ofthe 1576scientistssur-
veyed by Nature (from various fields, including chemistry, physics,
earth andenvironmental science,biology, andmedicine) reported
thattheyhadtriedandfailedtoreproduceanotherscientistâ€™sexper-
iments [3]. Numerous conferences for computer science (including
thesoftware-engineeringfield)organizeartifactevaluationswith
the goal to ensure reproducibility. Organizers assign badges based
on peer review to recognize authorsâ€™ efforts to make their tools
and datasets available and reusable, and integrate these artifacts
into publication processes. In the software community the artifact-
evaluation process started at ESEC/FSE in 2011 [ 15]1, and has now
spread to become commonplace at most conferences in the area of
software engineering and programming languages as well as other
communities,includingHCI,Communications,andSecurity.
As different communities have different requirements regarding
research artifacts, artifact-evaluation organizers use different eval-
uation methodologies to assess submissions and differentincentive
mechanisms to encourage authors (and reviewers) to participate.
Researchcommunititiesinvestaconsiderableamountofeffortinto
thedevelopmentandimplementationoftheartifact-evaluationpro-
cesses. However, recent studies have shown that there are diverse
views on the part of both reviewers and authors [ 11,13,20]. In
particular,thetensionsbetweenhighavailabilityvs.highquality
1http://2011.esec-fse.org/cfp-artifact-evaluation (archive)
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
145
ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore S.Winter, C.S.Timperley, B.Hermann,J. Cito, J. Bell, M. Hilton, andD. Beyer
of artifacts and between the only partially overlapping goals of
reproducibilityandreusabilityarestillbeingexplored.Throughthe
lens of reproducibility, artifact evaluation is a process centered on
validatingresearchresultsbyreproducingthoseresultsusingthe
artifactssuppliedbytheauthors.Throughthelensofreusability,
artifact evaluationis aprocess centeredon ensuring that artifacts
will be publicly available and could be re-used and extended by
future researchers.
Whatisclear,however,isthatparticipationinartifactevaluation
hasgrownenormouslysinceitsinception.Ithasbeenadoptedat
all major conferences and is increasingly adopted at journals in
software-engineering and programming-language research. Adop-
tion among authors has also increased over time. For instance, for
PLDI 2020 â‰ˆ90%of eligible articles were accompanied by artifacts.
However,this isnot alwaysthe casefor allvenues.
Ourcentralquestionis:Howcanwe,asacommunity,learnfrom
our experiences inour first 10years of artifactevaluation inorder
toimprovethenext10years?Togaincorrespondinginsights,we
inspect(RQ1)if articles accompanied by artifacts are more visible
thanthosewithout, (RQ2)whetherartifactsthatpassedevaluation
aremoreoftenavailable, (RQ3)maintainedafterpublication, (RQ4)
more often reused, and (RQ5)more throughly documented. To
inspect these aspects, we study conferences from the software-
engineering and programming-language domains based on the
selectionmadebyHermann,Winter,andSiemund[ 11],butlimit
our study to those where ACM guidelines apply to allow for a
comparablebaseline. We study theentire setof publications from
theseconferencesinthepastdecadeandidentifyartifactswhich
passed artifact evaluation but also those linked to a publication
withoutadocumentedartifact-evaluation badge.
Fromtheseinsightwederiveseveralsuggestionshowtheartifact-
evaluation process may be improved in the inspected communities.
The contributionsofour study are:
(1)an in-depth analysis of how artifact-evaluation practices
impact articles andartifacts,
(2) data-driven insightsto improve artifact evaluation, and
(3)adatasetandassociatedtoolingusedtocollectthedataset
to inspire further investigation orreproduction.
2 BACKGROUNDAND RELATED WORK
Priorworkandcommunityeffortsthatourstudyisbasedonfallinto
threecategories:effortstowardscommunitystandards(including
terminology), systematic studies of artifact sharing and evaluation,
and data collections to archive research artifacts. We discuss each
category inthe following subsections.
2.1 Background andDefinitions
Claims in scientific literature must be supported by evidence or
a reasoning why readers should regard the claims as valid [ 7].
Suchevidenceorreasoningincomputer-scienceresearchisoften
providedusingaprototypicalimplementation,acollectedorderived
dataset, or an (automated) proof. Authors may choose to make
these objects available (e.g., in the Archive of Formal Proofs2)
for other researchers to inspect or reuse. The lack of availability
2https://www.isa-afp.orgof this supporting evidence has often been criticized to hinder
reproducibility ofresearch [ 17,19].
Asupplementing research artifact is Å‚a digital object that was
either created by the authors to be used as part of the study or
generated by the experiment itself. For example, artifacts can be
softwaresystems,scriptsusedtorunexperiments,inputdatasets,
raw data collected in the experiment, or scripts used to analyze
resultsÅ¾(ACMTaskForceonData,Software,andReproducibility
inPublication4).Weusetheshortterm artifacttorefertosucha
supplementing digital object.
Artifact evaluation is the process of evaluating certain quality
attributesofanartifact[ 11,14,15].Typically,theevaluationworkis
done by an artifact-evaluation committee, which assesses whether
artifactsare reusable,functional ,well-documented ,consistent ,and
complete.
Consistent*Complete*W
ellDocumented*EasytoReuse* *Evaluated*OOPSLA*Artifact*AEC
Figure 1: First badge
fromOOPSLA2013Anartifact badge is a pictogram
to be displayed on a scientific arti-
cle to declare quality attributes for a
publishedresearcharticle.Thefirst
artifactbadgeinthePLcommunity
(Fig. 1) was introduced in 2013 for
OOPSLA by Steve Blackburn and
Matthias Hauswirth, and the properties to be evaluated were easy-
to-reuse,well-documented ,consistent ,andcomplete.3Itisstillused
for artifact evaluation by some non-ACM conferences. Later, the
ACM Task Force on Data, Software, and Reproducibility in Publica-
tion4introducedfivecoloredbadgestodistinguishfivedifferent
propertiesofartifacts.5Thepurposeofabadgeistorewardartifact
sharingandmotivate authorsto participateinartifact evaluation.
Figure 2:ACM badgesThe five badges can be divided
into three categories: (a) an artifact
isavailable, independent from arti-
fact evaluation, (b) artifacts satisfy
the criteria of being functional or
reusable, as assessed by artifact eval-
uation, and (c) results of the article
werereproduced with the artifact, or
replicated withoutthe artifact.
Artifact-evaluationcommitteesare
concerned with two or three of the
above badges ( functional ,reusable,
sometimesalso reproducible6),while
theavailable badge does not require
evaluation(onlythatartifactsarelong-termavailable,immutable,
identifiable),andthe replicated badgerequiresanindependentstudy.
There are different communities working on establishing standard
processes andnotionsfor badgingof artifacts [ 18].
3http://evaluate.inf.usi.ch/artifacts/aea/badge (archived)
4http://www.acm.org/publications/task-force-on-data-software-and-reproducibility
(archived)
5http://www.acm.org/publications/policies/artifact-review-and-badging-current
(archived)
6Itisdebatedinthecommunitywhetherthe reproducible badgerequiresanindependent
studyorif it canbeachieved throughartifact-evaluation review.
146A Retrospective Studyof OneDecade of ArtifactEvaluations ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
2.2 RelatedStudies
The expectations of the community regarding artifacts and their
evaluation process were studied by Hermann, Winter, and Sieg-
mund using a survey involving members from past artifact-
evaluation committees [ 11]. The study raises several questions,
someofwhichwestrivetoanswerinthiswork.Weparticularlypick
uponthequalityaspectofartifactsandtheeffectofartifactevalua-
tiononartifactquality.HeumÃ¼ller,Nielebock,KrÃ¼ger,andOrtmeier
gave evidence that one of the most important expectationsÃthe
availabilityoftheartifactsdescribedinscientificarticlesÃisfulfilled
onlytoanunsatisfactorydegree[ 13].Incontrasttoourstudy,they
found a small positive correlation between linking to artifacts and
citationstothearticle.However,theyonlyinspectedresearch-track
articlesfromtheInternationalConferenceonSoftwareEngineering
(ICSE)inyearswithoutanestablishedartifactevaluationprocess.
Two other studies identified reasons for the insufficient availability
of artifacts, and present a number of challenges that the authors
encounter[ 20,22].Astudyonrepeatabilityincomputer-systemsre-
searchreportedthateveniftheartifactsareavailable,studyresults
areoftennotreproducible[ 7].Thefieldsofcomputersystems[ 7,8],
computergraphics[ 6],communications[ 2,24],andmachinelearn-
ing [9,16] have alsobeen the subjectof studies on artifact quality
andavailability.
2.3 Data Collections
Toensurethatartifactsareidentifiableandfindable,therelations
between articles and artifactsmust be reliablytracked and made
available [ 1]. Zenodo7provides a convenient interface to view,
query, and change the relations of a digital object stored at Zen-
odoâ€™s digital library to other digital objects and provides means
to declare the semantics of the link (such as â€˜is supplemented by
this uploadâ€™, â€˜is replaced by this uploadâ€™, and â€˜cites this uploadâ€™).
ACMâ€™s digital library has individual landing pages for artifacts and
makes the links between articles and artifacts explicit.8Article-
artifactrelationshipsthatwerefoundinarepeatabilitystudy[ 7]
were made publicly available.9Baldassarre, Ernst, Hermann, Men-
zies,andYedidacollectreuserelationshipsbetweenpublicationand
artifacts beyondrepeatabilityandreproduction [ 4].
3 RESEARCH QUESTIONS AND STUDY
SUBJECTS
Inthefollowingweintroducetheresearchquestionsdrivingour
analysesandthe study subjectsthat our answers are basedon.
3.1 Research Questions
Ourstudyaddressesfiveresearchquestionsrelatedtothemeritsof
AEforauthors,themeritsofAEforartifactusers,andhowthese
merits for authors and users are linked with AE and publication
processes andpractices.
RQ 1:Are articles with artifacts that have passed AE more visible?
RQ 2:Are successfully evaluatedartifacts more available?
7https://zenodo.org
8Forexample,thelandingpage https://dl.acm.org/do/10.5281/zenodo.3951724 ofan
artifact in the ACM digitallibraryconnectsanarticle[ 11] with its artifact[ 12].
9http://www.findresearch.orgRQ 3:Isartifactdevelopment/maintenancecontinuedmoreoften
for successfully evaluatedartifacts?
RQ 4:Are successfully evaluatedartifacts more often reused?
RQ 5:Aresuccessfullyevaluatedartifactsmorethoroughlydocu-
mented?
Before we discuss the relevance of these questions for the
software-engineering community, our methodology for answer-
ing these questions, and the results in more detail, we introduce
the dataset throughwhichthe questionsare investigated.
3.2 Subjects andDescriptiveStatistics
The questions in our study are related to the effects of artifact
evaluations. Therefore, we choose conferences from the SE and
PLdomains thathave implemented corresponding processes.Her-
mann, Winter, and Siegmund [ 11] provide a comprehensive list
of such conferences that we use for our subject selection. As the
format and degree of information that conferences provide regard-
ing the conducted AE differs significantly, we restrict our study
to conferences withproceedings in the ACMDigital Library (DL).
The main reason for this decision is that the ACMâ€™s guidelines
for artifact review and badging4provide a common, albeit very
general, AE framework and that all AE processes adopting this
frameworkshouldbecomparableonthatbasis.Moreover,theACM
DL provides uniform formats for (1) proceedings, (2) publication
metadata,and(3)researchartifactslinkedwithpublications,which
facilitates the creation ofaconsistent dataset.
Figure 3 shows the conferences with AE for which we have
collectedarticledatafromACMâ€™sDLbyconferenceandyear.We
refertothecombinationofconferenceandyearas venue.Wehadto
excludeFSE2012andMODELS2019fromourdataset:ForFSE2012,
onlyaÅ‚bestartifactawardÅ¾wasawardedbytheprogramcommittee.
The number of candidates for this award and the selection process
remain confidential. Hence, we were not able to identify which
articles had evaluated artifacts thatwere considered fortheaward
and which had not. Therefore, we cannot make any meaningful
comparison between artifacts that did and did not undergo an
evaluation. For MODELS 2019, only the workshop articles from
thecompanionproceedingsareavailableintheACMDL.However,
there is no information regarding any AE process or evaluated
artifacts for these workshops available. We added ASE 2018 to
our dataset, although it did not have a formal artifact evaluation
process, because Å‚AvailableÅ¾ badges were issued for some of the
articles and we can, therefore, assess effects that we attribute to
badges (rather thanAEprocesses)as targetedby RQ1. Onthetop
of each bar in Figure 3, a number indicates the total number of
articles in our dataset. This number may be smaller than the actual
number of articles in the proceedings, as we exclude keynotes,
workshopabstracts, etc.More precisely,we include every article
from the proceedings that has an author, is tagged as Å‚Research
ArticleÅ¾orÅ‚ArticleÅ¾intheACMDL,andhasatotallengthofatleast
4PDFpages.Thiscollectionincludesshortpapers,asforseveral
venuestool papersare short papers that mayhaveundergone AE
and,hence,arerelevantforour study.Intotal, ouranalysisinthe
following sections is based on 3650articles from 64venues. The
barsinFigure3alsoindicatetherelativefractionsofdifferentarticle
categoriesrelevantto our study.
147ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore S.Winter, C.S.Timperley, B.Hermann,J. Cito, J. Bell, M. Hilton, andD. Beyer
104 104 104
24 2425 25 2526 26 2630 30 30 3021 21 2125 25 25 25
55 55 5592 92 9293 93 93115 115 115104 104 104112 112 112111 111 111 111120 120 120 120164 164 164 164
44 44 44 4440 40 4039 39 39 3937 37 37
109 109 109 109129 129 129 129
49 49 4944 44 4441 41 4155 55 5537 37 3750 50 50 5055 55 55
33 33 3342 42 4235 35 35 35
50 50 5052 52 5253 53 5352 52 5266 66 66 6660 60 6073 73 73109 109 109
52 52 5258 58 5848 48 4847 47 4755 55 5576 76 7677 77 77 77
52 52 5259 59 5964 64 6466 66 6677 77 7768 68 68
23 23 2329 29 2929 29 2928 28 28 2829 29 2928 28 28
23 23 2324 24 2422 22 2220 20 2021 21 21
ASE
 CGO
 FSE
 ICFP
 ICSE
 ISSTA
 MODELS
 OOPSLA
 PLDI
 POPL
 PPOPP
 SLE
2018
201520162017201820192020
201120132014201520162017201820192020
2017201820192020
20192020
2014201520162017201820192020
201720182020
20132014201520162017201820192020
2014201520162017201820192020
201520162017201820192020
201520162017201820192020
201620172018201920200%25%50%75%100%
Category
 AE Av. NoB NoA
Figure3:Percentageofarticlespercategoryacrossallvenuesinourstudy;numbersontopofthebarsdisplaythetotalnumber
ofarticlesforthevenue; categoriesare explained in Section 4.1
4 RQ1: ARE ARTICLES WITH ARTIFACTS
THATHAVE PASSEDAE MOREVISIBLE?
PreparingartifactsforAEentailssignificantamountsofworkfor
authors. However, evaluation measures for hiring, promotion, and
tenureoftenarecenteredonthevisibilityofarticlesÃderivatives
ofpublicationandcitationcountsÃnotonthevisibilityofartifacts.
Whilewedonotendorsetheuseofthesemeasuresforevaluatingca-
reer advancement, it is nonetheless the case that many institutions
around the world rely on them, and some researchers are forced
to optimize towards them. One hypothesis is that AE positively
impacts the visibility of publications [ 13]. If this hypothesis holds,
it may provide authors with a strong incentive to participate in AE.
Ifitdoesnot,aninvestigationofalternativerewardmechanisms
maybe worthwhile.
4.1 Method
We measure visibility in terms of citation counts of articles, which
we obtain from the Crossref Metadata Search10(cf.also[10]).
To link these visibility measures with AE, we group articles into
four article categories: (AE)with artifact and AE badge (Å‚Func-
tionalÅ¾,11Å‚ReusableÅ¾,12or the Å‚old venue-specific badgesÅ¾ ( Fig-
ure1)),(Av.)withartifactandonlytheÅ‚ArtifactAvailableÅ¾badge,
(NoB)with artifact butwithout any badge,and (NoA)without ar-
tifact. Note that we treat articles that only have an Å‚Artifacts Avail-
ableÅ¾ badge separately from articles that also have other badges,
becauseÅ‚ArtifactsAvailableÅ¾doesnotimplyanactualevaluation
ofthe artifact,4as discussedfor ASE 2018 above.
WeidentifycategoriesAEandAv.bytheirbadgesintheACM
DL. To identify the old monochrome badges, which are not shown
in DL article entries, we extract the upper left and right corners
from article PDFs and analyze the distribution of pixel colors in
thoseareasto detect the presenceof a badge. Toruleoutfalseposi-
tivematchesduetoirregularformatting,wemanuallyconfirmed
each badge detection. To rule out false negatives, we compared the
10https://search.crossref.org
11Forthe colored ACM badges,weconsiderboth versions1.0and 1.1.
12ForPPoPP2020andCGO2020,Å‚ResultsReplicatedÅ¾badgeswereissuedintheAE
and we, thus, considerthem as well for these conferences.numberofdetectedbadgesagainstthenumberofacceptedartifacts
reportedonconferencewebsitesand(ifavailable)toinformation
provided by Conference Publishing Consulting13and theFind-
Research9portal.Weadditionallyconsultedtheartifact-evaluation
chairsâ€™reportsinproceedingsfrontmattersandcontactedtheAEC
chairsofthe conferences for confirmation.
We identify categories NoB and NoA by conducting a tool-
assistedmanualreviewandclassificationof 25728URLsfrom 3150
article PDFs (the remaining PDFs in our dataset did not contain
any URLs). In this process, we automatically extract URLs from
thePDFtextandmanuallytageachextractedURLasÅ‚accessible
artifact URLÅ¾, Å‚inaccessible artifact URLÅ¾, or Å‚no artifact URLÅ¾. We
make the tool available together with our dataset in the artifact
accompanying this article [ 23].
ToassesswhetherAEaffectsvisibility,wedeterminewhether
there is stochastic dominance of either category over any other
categorybyconductingatwo-sampleKolmogorov-Smirnov(KS)
testoneachpairwisecombinationofthefourcategories.Weper-
formthesepairwisetestsseparatelyforeachconferenceandyear
to avoid effects from Å‚ageÅ¾ onvisibility measures [ 13]. Toaccount
for confounding factors,we further categorizearticlesby (a) page
lengths,and (b)whetherthey are published aspublicorclosed ac-
cess.Inadditiontotheseconfoundingfactors,wealsoattemptedto
analyze confounding with article topic as per the 2012 ACM Com-
puting Classification System (CCS). However, we found the spread
of CCS topics to be too large to support a meaningful analysis, e.g.,
theESEC/FSE2020proceedingsfeature75differentCCScategories
that only apply to one single article, 22 that apply to 2, 13 to 3, 4 to
4, and so on. Consequently, a stratification of the dataset according
to CCS categories would lead to numerous strata with single or
fewarticlesand,thus,impedeameaningfulcomparison.We,hence,
decidedtoexcludetheimpactanalysisforCCScategoriesfromour
study of confounding factors, but kept the data in our artifact [ 23].
Ourdatadonotmeettheprerequisitesforparametricapproaches
toconfoundingcontrol(e.g.,citationcountsdonotfollowanormal
distribution).Otherapproacheslikemultiplelinearregressionor
13https://www.conference-publishing.com
148A Retrospective Studyof OneDecade of ArtifactEvaluations ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
logistic regression assume a linear relationship between the in-
dependentvariablesandthedependentvariable(respectively,its
logit).We,thus,analyzetheimpactofthesevariablesontheassoci-
ation between article categories and citation counts by stratifying
ourdataaccordinglyandanalyzingdifferencesincitationcounts
acrossallstratausingKStests.Aswetestforeachpotentialcon-
founding factor (page lengths and open/closed access) and their
combinations, we conductatotalof16 KStests( (2+3!) Â·2, aswe
testforbothdirectionsofpossiblestochasticdominance)pervenue
andadjustour ð‘values accordinglyfor multipletesting using the
Benjamini-Hochberg (BH) procedure [ 5]. Based on the outcome of
these tests, we conduct 12 KS tests against stratified or unstrati-
fied data from each of the four categories (we compare each of the
fourcategoriesagainsttheothers)andperformcorrectiononthe
obtained ð‘valuesas before.
4.2 Results
Table 1shows the conferences and article categories for which
citation count distributions are statistically significantly ( ð›¼=0.05)
affectedbydifferencesintheidentifiedconfoundingvariablesÅ‚page
lengthÅ¾ (regular vs. short papers, where we set the cut-off at 10
pages) and Å‚open/closed accessÅ¾ (OA/CA). The columns list the
results for the strata for which we identified stochastic dominance
relations,indicatedby >.Wefindsignificanteffectsofconfounding
variables in 14 of 64venues. For all of them, regular papers have
significantly highercitation counts than shortpapersand therela-
tion for CA Reg. > CA Short likely is a direct effect of that. For the
otherpotentialconfoundingfactors, there isnoclear pattern.
Table1:Statisticallysignificant( ð›¼=0.05,afterBHcorrection)
effects of article page counts (distinguishing Short from Reg.
papers), open/closedaccess(OA/CA),and theircombinations
on citation counts
VenueReg. CA Reg. OA Reg. CA Reg. OA
> > > > >
Short CA Short CA Short OA Short CA
ASE 2018 âœ“ âœ“ Å› Å› Å›
ESEC/FSE 2011 âœ“ âœ“ Å› Å› Å›
ESEC/FSE 2013 âœ“ âœ“ Å› Å› Å›
FSE 2014 âœ“ âœ“ Å› Å› Å›
ESEC/FSE 2015 âœ“ âœ“ âœ“ Å› Å›
FSE 2016 âœ“ âœ“ Å› âœ“Å›
ESEC/FSE 2017 âœ“ âœ“ âœ“ Å› âœ“
ESEC/FSE 2018 âœ“ âœ“ âœ“ Å› Å›
ESEC/FSE 2019 âœ“ âœ“ âœ“ Å› Å›
ESEC/FSE 2020 âœ“ âœ“ âœ“ Å› Å›
ISSTA 2015 âœ“ âœ“ Å› Å› Å›
ISSTA 2017 âœ“ âœ“ âœ“ Å› Å›
ISSTA 2018 âœ“ âœ“ Å› Å› Å›
ISSTA 2019 âœ“ âœ“ âœ“ Å› âœ“
Wesubsequentlystratifythecitationdataaccordingtothelevels
oftheconfoundingvariables(i.e.,openvs.closedaccessand page
countslessvs.greaterthanorequalto10pages)fortheconferences,
forwhichwefoundasignificanteffectofthesevariables(indicatedby the tick marks in Table 1) and conduct our analysis on the
respectivestrata.Theresults( ð‘valuesandKSstatistic ð·aseffect
size measure) are shown for statistically significant cases ( ð›¼=
0.05) inTable 2. After BH correction, we only find statistically
significant citation count differencesbetweenNoBand NoAshort
papers publishedat FSE2014 andESEC/FSE2019.
Contrarytootheranalyses[ 13],ourresultsindicatethatarticles
with artifacts do not generally get more citations. We are only able
toconfirm statisticallysignificanteffectsfor 2out of64 venuesin
our study. Moreover, the significant effects we observe are limited
to articleswithoutbadges(NoB) andto the short papers category.
Therefore,weconcludethatcreatingandpublishingresearcharti-
facts does not generallyhave beneficialeffectsoncitation counts.
Table2:ð‘valuesandKSstatistic ð·(inbraces)forstatistically
significant ( ð›¼=0.05, after BH correction) effects of article
categoriesoncitationcounts;Å‚>Å¾indicateswhichcategory
hasasignificantly greater citation count
Venue Strata NoB>NoA
FSE 2014 CAShort 0.013 (0.562)
FSE 2014 Short 0.013 (0.562)
ESEC/FSE 2019 Short 0.047 (0.492)
Finding 1: Artifacts do not significantly improve citation
counts ofresearcharticles.
5 RQ2: ARE SUCCESSFULLYEVALUATED
ARTIFACTSMOREAVAILABLE?
The reproducibility of research results and the reusability of re-
search artifacts are perceived as the main objectives of artifact
evaluations by AEC members [ 11]. If an artifact is not available,
itcanneitherbereused,norcanpublishedresultsbereproduced.
Therefore,availability isavitalquality criterionfor artifacts.
5.1 Method
To study whether artifacts that passed AE are more often available
than artifacts that did not, we classify artifacts as (a) passed AE
(article group AE in Section 4.1 ) or(b) unknown (article groups Av.,
NoB inSection 4.1 ). We refer to these groups as AE and NonAE in
the following.
Totestwhetheranartifactfromeithergroupisavailablerequires
at leastthree steps.
(1)Theremustbeanartifactreference,e.g.,asaURLinapub-
lishedarticle.
(2)The artifact reference must be resolvable to one or more
digital objects(e.g.,downloadablefilesorwebservices).
(3)Thereferenceddigitalobjectmustbeanartifactofthearticle
as per the definitionin Section 2.
Testingforthesecondcriterioncanbeautomated(withbounded
precision),whereastestingforthefirstandthirdrequiresmanual
investigation.
149ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore S.Winter, C.S.Timperley, B.Hermann,J. Cito, J. Bell, M. Hilton, andD. Beyer
(1)Artifact-ReferenceAvailability: Toidentifywhetheraresearch-
artifact reference is available for articles in our study, we search
differentinformation sourcesfor thesereferences:
The ACM Digital Library (DL)14provides authors of pub-
lishedarticleswiththeopportunitytoalsopublishanyaccompany-
ing research artifacts. Artifacts in the DL have their own dedicated
landing page with links from and to the research articles that they
accompany.
Conference Publishing Consulting13is a consulting agency
for publishers of scientific articles. Conference Publishing Consult-
ingisentrusted withthepublication processesfor alargenumber
of SE and PL conferences and openly publishes metadata for these
conferencesonitswebsite.ArtifactlinksfromConferencePublish-
ing Consulting are extracted as the Å‚info linksÅ¾ that authors can
supply when submittingtheircamera ready article versions.
FindResearch9isa platformthatpresentssemi-automatically
collected metadata of computer-science research articles. Authors
of research articles are queried for confirmation of presumably au-
tomaticallyextracteddata.15Theportaldoesnotcontainmetadata
forconferencesafter2018,butservesasareferenceforoldervenues
inour study.
CMU Dataset [21] is the result of a recent study of research
artifacts[ 20],inwhichtheauthorshavemanuallyanalyzedartifact
references in research articles and published this dataset. As the
venuescoveredbythatdatasetoverlapwiththevenuesinourstudy,
we make use ofthe dataset for intersecting venues.
Article PDF files are used for the venues in our study that are
not coveredby the CMU Dataset and we conduct a similar analysis
as for the CMU Dataset. As the manual analysis of PDF URLs does
not scale well for the total of 3650articles in our study, we have
developed the tool URLBrowser to support this process. The tool
automaticallyextractsURLsfromPDFfilesandopenstheselinks
in a web browser to facilitate URL classification (whether the URL
points to an artifact of the article and, if so, whether that link
works). We make URLBrowser publicly available as part of our
artifact [23].
(2) Digital-Object Availability: To approximate the availability
of digital objects referenced by the URLs identified in the first
step of our availability analysis, we send HTTP HEAD requests
usingcURL16andanalyzethereturnedHTTPstatuscodes.This
measurementonlyyieldsanapproximation,because(a)websites
maybeavailable,butnotcontaintheartifact(falsepositives)and
(b)websitesmaynotrespondtoHEADrequests(falsenegatives).
Onamanually investigatedsampleof200linksthatwereflagged
asavailable(thesamplediscussedin Section 8)and416linksthat
wereflaggedasunavailable,wefound 2.5%offalsepositivesand
5.6%offalsenegatives.Inadditiontothisautomatedprocess,we
utilizeresultsfromtheanalysisofarticleURLsusing URLBrowser ,
as detailedabove.
(3)CorrespondenceofAvailableDigitalObjectstoResearchArti-
facts:Whether an available digital object qualifies as a research
artifactisnon-trivialandoneofthecentralquestionstargetedby
artifactevaluations.Anin-depthanalysisofall 3685digitalobjects,
14https://dl.acm.org
15Wewerenot able to identify the precisesource of artifact linksonFindResearch.
16https://curl.seTable3:AccessibilityofartifactswithAEbadgeandAv.badge;
notethatAEindicatesthattheartifactwasevaluated;NonAE
couldindicatethattheauthorsdidnotsubmittheartifactfor
evaluation, or they did and the AE committee did not award
abadge;percentagesarecalculatedbasedontheneighboring
column to theleft
AE Available Total Has Artifact Is
Evaluated Badge Status Papers Reference Accessible
AEAv.Badge 683 676(99.0%) 675(99.9%)
NoAv.Badge 602 473(78.6%) 431(91.1%)
NonAEAv.Badge 71 67 (94.4%) 65 (97.0%)
NoAv.Badge 22941148 (50.0%) 1032 (89.9%)
for which the cURL-based analysis indicated availability, is not
manageablewithinthescopeofthisarticle.We,thus,relyonthe
AECâ€™s assessment for artifacts that underwent AE (article group
AE). For artifacts from other article groups, we rely on the assump-
tion that the manual investigation of the digital objectâ€™s reference
withURLBrowser issufficientlyindicativeofwhetherthedigital
objectisindeedan artifact of the analyzedresearch article.
5.2 Results
Theavailabilityresultsfromtheoutlinedprocedure areshownin
Table 3. The table is partitioned into AE and NonAE articles and
furtherdividesthesepartitionsbasedonwhetherthearticlecarries
anÅ‚AvailableÅ¾badge.Thisinformationisrelevant,becauseartifacts
may have undergone AE but not been made publicly accessible.
Similarly, if we were not able to find an artifact reference for an
articlewithoutabadge,thatmayeithermeanthatthereisnoartifact
for this article or that we were not able to find its reference. We
canonlydistinguishbetweenthosecasesforarticlescarryingan
Å‚AvailableÅ¾badge.Thelastthreecolumnslistforeachofthefour
resultingpartitionsthenumberofarticles,thenumberofarticles
withareferenceto theartifact,andthenumberofarticleswith at
leastone accessible reference.
Reference Availability: A comparisonof the first two numerical
columnsrevealsthatwewerenotabletoidentifyartifactreferences
for 11 articles with Å‚AvailableÅ¾ badges, 7 of which also carry AE
badges. A closer inspection of these cases reveals that 2 cases of
articleswithAEbadgesandall4ofthearticleswithoutarepublica-
tionsatICSE2020.AsAEforthisvenuefollowedanopenreview
process, the artifacts are indeed available in the GitHub reposi-
tory17on which the review process was based. Unfortunately, the
repository is only linked from the submission-information page
for the venue and not in the publication itself or any publication
metadata available in common databases for scientific literature.
Three of the remaining 5 cases are due to insufficiencies in our
URL detection: One of them is a reference to a privately hosted Git
server(whichisnolongeraccessible,butthelinkisprovidedinthe
article),oneis duetofontencodingissuesinthearticleâ€™sPDFfile
(which also affects other text-based functions, such as searching
textinthearticle),andoneismissedbythePDFtotextconversion
17https://github.com/researchart/rose6icse/tree/master/submissions/available
150A Retrospective Studyof OneDecade of ArtifactEvaluations ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
underlyingour URLBrowser toolforunknownreasons.Oneofthe
remaining 2 articles makes an unspecific reference to the ACM DL,
butwewerenotabletofindfurtherinformationthere.However,
we were able to find GitHub repositories for the 2 artifacts via a
web search. In summary, we were not able to identify references
for 8 out of 754 analyzed articles with an Å‚AvailableÅ¾ badge from
thepublishedtextorpublicationmetadataandonlyfoundthemvia
awebsearch orlookingintothedetails of the artifactsubmission
andmanagementprocessfor the venue.
Digital Object Availability: A comparison between the second
andthirdnumericalcolumnin Table3showshowmanyreferences
returned a failure-indicating HTTP status code upon an attempt to
accessthereferenceddigitalobject.Thenumbersrevealdifferences,
both between articles with and without Å‚AvailableÅ¾ badges and
betweenarticleswithandwithoutAEbadges.Foronly3articles
with an Å‚AvailableÅ¾ badge (1 with and 2 without AE badges), we
couldnotfindanyworkingreferenceamongthearticlesâ€™references,
which accounts for 0.4%of the articles with Å‚AvailableÅ¾ badges. In
contrast,forarticleswithoutanÅ‚AvailableÅ¾badgethatnumberis
158(9.7%).Whilewecouldnotfindanyworkingreferencefor43
(3.7%) articles with AE badges, the number for articles without AE
badges is118( 9.7%).
For the three broken references from articles with Å‚AvailableÅ¾
badges,wemanuallyinvestigatedthecasesandfoundtwoofthe
references(oneintheAEgroup,oneinthewithoutAEgroup)to
befalselyidentifiedasnotworkingbyourHTTPstatuscodebased
detection. We do not consider such false detections to affect our
overallconclusionfromthepresenteddataduetothelargediffer-
ence between article groupswith/withoutÅ‚AvailableÅ¾/AE badges.
These results indicate that the overall number of articles with
references to research artifacts is similar across the two partitions
AE(1149articles)andNon-AE( 1215).However,Å‚AvailableÅ¾badges,
whichareassociatedwithlowfractionsofbrokenreferences,are
muchmore prevalentinthe AE partition.
AstheÅ‚AvailableÅ¾badgehasonlybeenintroducedtoAEwith
ACMâ€™sstandardizationofartifactbadgesin2017,thereisapossible
confounding of the observed effect with reference age. As the cen-
tral criterion for awarding the Å‚AvailableÅ¾ badge is that the artifact
ishostedonaplatformwithalongretentionpolicy,weanalyzethe
effects of host platforms on artifact availability and which hosting
platforms have been most prevalent over time. We identify host
platforms by extracting the domain of a given artifact reference
andmanuallyclassifyingitas,forinstance,institutionalwebsites,
personal websites, project websites, public version control systems
(VCS), etc.
All4071artifactreferencesinourstudycanbeclassifiedaccord-
ing to the 15 link categories listed in Table 4. The left side of the
tableliststhenumberofreferencesforeachcategoryinAEarticles
and the number of broken references identified by our cURL-based
check. The right side of the table lists the same information for
NonAE articles. The table rows are ordered by the overall fraction
of broken to total references (Å‚% Broken All DataÅ¾). Besides row
YouTube, which possibly misses broken URLs, as the site shows
a custom error page and does not return HTTP 404 on missing
content,weseethatinparticularDOI/handlelinksandpublisher
auxiliarymaterial(e.g.,artifactshostedinACMâ€™sDL)havealowTable 4: Artifact references in our study by host platform;
thefirstcolumnliststhetypeofhostplatform,thefollowing
thenumberoftotal and brokenreferences and their ratio
Reference Found Broken % Broken Broken Found
Type AE AE AllData NonAE NonAE
IP Address 2 2 100% 1 1
Other 2 2 100% 1 1
File Storage 10 5 36% 4 15
WebApplication 14 1 25% 5 10
URLRedirection 18 4 24% 16 64
Institutional Website 362 56 23% 114 371
Company Website 14 1 21% 7 24
ProjectWebsite 84 10 14% 25 164
Personal Website 27 3 12% 5 39
Public Archive 66 1 5% 6 78
Public VCS 749 26 4% 37 778
Publisher Aux. Material 754 11 1% 1 78
DOI/Handle 182 1 1% 2 74
Youtube 3 0 0% 0 75
Broken
 Total
2011 2013 2014 2015 2016 2017 2018 2019 202001020304050
0100200300400
YearReferenceCount
ReferenceType
DOI/Handle InstitutionalWebsite ProjectWebsite
Public VCS Publisher Aux.Material
Figure 4: Total and broken artifact links according to HTTP
response fordifferentplatforms
broken reference ratio and significantly more AE than NonAE ref-
erences fall intothesecategories.Moreover,bothcategoriesfulfill
the long-termretention requirements for the Å‚AvailableÅ¾badge.
Figure 4displays how the numbers from Table 4distribute over
time. To maintain visibility, we only display host platforms with at
least 50 links in at least one year. while the upper part of the figure
showsthenumberofbrokenreferencesbyplatformandyear,the
lowerpartshowsthetotalnumberofreferencesasabaseline.From
the figure we see that a large number of broken references point
151ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore S.Winter, C.S.Timperley, B.Hermann,J. Cito, J. Bell, M. Hilton, andD. Beyer
to institutional websites. We also see in the lower part that from
2017,thenumberofreferencespointingtoinstitutionalwebsites
orprojectwebsitesdecreases.Thereferencestopublisherauxiliary
material and DOIs/Handles increases, while the number of broken
referencestotheseplatformsremainslow.Whilethesteepdecrease
ofbrokeninstitutionalwebsitelinksbetween2017and2018must
be partially attributed to recency, as the drop of total references in
that category is somewhat smoother (albeit on a different scale),
we do expect the observedchange inpublicationculture dueto the
requirements set forth by the Å‚AvailableÅ¾ badge to have a lasting
impact dueto the long-termretention they mandate.
Finding2: Duetothehostingplatformrequirementsthey
entail,Å‚AvailableÅ¾badgesarepositivelylinkedwithartifact
availability.
6 RQ3: ISARTIFACT
DEVELOPMENT/MAINTENANCE
CONTINUED MOREOFTEN FOR
SUCCESSFULLYEVALUATED ARTIFACTS?
Ifanartifactcontinuestobemaintainedanddeveloped,thatindi-
catesthat itisreusedand, therefore,musthavebeen reusableand
is/was of high quality, at least for the period of maintenance/devel-
opment.
6.1 Method
To measure development and maintenance activity, we rely on
information frompublic version control systems.Asmostarticles
in the AE and NoB classes provide GitHub links, we focus our
analysisonGitHubanduseitsRESTAPItoobtainthefollowing
information: (1) the time of the last commit, (2) the number of
commitsafterartifactpublication,(3)thenumberofcontributors,
(4) the number offorks,and(5) the number ofstars/watchers.
Thefirsttwomeasuresarethecentralmeasuresforanswering
theRQ,asweusethemtocalculate(a)thedevelopmenttimeperiod
after artifact publication (Å‚Dev. TimeÅ¾), (b) the time between the
lastcommitandthedateofourdatacollection(Å‚IdleTimeÅ¾),and(c)
the number of commits during Dev. Time (Å‚Commit DensityÅ¾). We
use the other three measures as indicators of interest and visibility
ofthe artifacts.
6.2 Results
Table5showstheresultsoftheKStestsweconductedtoassessthe
difference in GitHub-based measures for the development/main-
tenance activity after artifact publication. The tests are based on
data from a total of 1056repositories ( 535belonging to AE articles
and521toNonAEarticles). ð‘valuesare not adjusted, asonly two
tests (one for eachdirection ofpossible stochasticdominance)are
conductedfor eachofthe disjointmeasures.
AErepositorieshavesignificantlyhighercommitdensityanddev.
timeinadditiontoasignificantlyshorteridletime.Thisindicates
that these repositories are indeed used for the active development
ofAEartifacts,evenbeyondtheirsubmissiontoartifactevaluation,
and not for archiving them. For NonAE, the lower developmentTable5:ð‘valuesandKSstatistic ð·forstatisticallysignificant
(ð›¼=0.05) differences in GitHub statistics based on article
categories (AE: with AE badges, NonAE: without AE badges);
ð‘valuesarenotadjusted,asonlytwotestspermeasureare
conducted; stochastic dominanceis indicated by Å‚ >Å¾
Measure Relation ð‘ ð·
Idle Time NonAE >AE <0.01 0.086
Dev.Time AE >NonAE <0.01 0.085
Commit Density AE >NonAE <0.01 0.112
Contributor Counts AE >NonAE <0.01 0.088
StarCounts AE >NonAE <0.01 0.154
Watcher Counts AE >NonAE <0.01 0.142
ForkCounts AE >NonAE <0.01 0.093
ForkCounts NonAE >AE <0.01 0.076
activity indicates that authors mainly use the repositories for ar-
tifact archival. This impression is strengthened by the generally
higherinterestandvisibilitymeasures(contributor,star,andwatch
counts),whichmayeitherindicateusageasÅ‚bookmarksÅ¾orhope
for further evolution of the projects (which does not seem to occur
in the general case). Fork counts are also significantly different
between AE and NonAE repositories, but without clear stochas-
tic dominance of either group over the other. The KS statistic ð·,
which indicates the maximal percentage difference between the
two groupsâ€™ cumulative distribution functions, is moderate with a
maximaldifferenceof 15.4%(forstar counts) acrossthe measures.
Finding 3: Repository-based activity, interest, and visibil-
ity measures are higher for evaluatedartifacts.
7 RQ4: ARE SUCCESSFULLYEVALUATED
ARTIFACTSMOREOFTEN REUSED?
Availability of research artifacts is a necessary, but not a sufficient
prerequisitefortheirutilitytoreuseinscientificresearchandresult
reproduction. To serve the research community, artifacts must also
be reusable for reproducing research results or for repurposing in
different contexts. We, therefore, analyze how often artifacts are
being reused.
7.1 Method
Weanalyzereferencestoresearchartifactstoapproximatereuse.
Ifanartifactisreferencedinaresearcharticle,thatindicatesthat
theartifacthasbeenusefulforotherwork.Toanalyzereferralto
artifacts,wesearchforthepresenceofartifactlinks(obtainedfrom
varioussourcesasdiscussedforthe Availability qualitycriterion
above) within article PDFs. We restrict our search to the articles in
our dataset and use the URLs extracted in the article classification
process for RQ1 (see Section 4.1 ). Our URL matching accounts
for small differences that do not affect the identity of the digital
objectbeingreferenced(presence/absenceoftrailingslashesora
Å‚www.Å¾prefix).Weincludereferencesfromyears beforetheartifactâ€™s
discussion in a publication, as the artifact may have been available
152A Retrospective Studyof OneDecade of ArtifactEvaluations ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Table 6: Articles in our study that are referenced by and that
arereferencingotherarticlesbyartifactURLwith/without
overlapinreferring/referred author groups
Authorlists Category Referenced Referencing
intersect (Referenced) Articles Articles
yesAE 69 (0.05%) 80 (0.06%)
NonAE 83 (0.07%) 96 (0.08%)
noAE 48 (0.04%) 61 (0.05%)
NonAE 40 (0.03%) 138(0.11%)
andusefulbeforeanarticlediscussingtherelatedresearchhasbeen
accepted for publication. As referral by others than the original
authorsoftheartifactindicatesbetterreusability(othersmustbe
assumed to be less familiar with the artifactâ€™s usage and structure),
we also take the overlap of author groups between the referring
articleandthereferencedarticlediscussingtheartifactintoaccount.
7.2 Results
Table6showstheabsolutenumbersandrelativefractionsofarticles
with referenced artifacts and referencing articles in our study. The
firstcolumnindicateswhetherwecountarticleswithorwithout
intersectingauthorlists.Tomakeacomparisonbetweenreferences
toAEandNonAEartifacts,wepartitionourdatasetandthesecond
columnofthetablelabelstherowsaccordingly.Thenumbersyield
differentconclusionsdependingonwhetherauthorlistsdoordonot
intersect.Forarticleswithintersectingauthorlists,moreNonAE
artifactsthanAEartifactsarereferenced,whereastheoppositeis
true for articles withnon-intersecting authorlists.
We also see more references (last column) to NonAE artifacts,
irrespective of whether author lists intersect or not. The difference
between references to NonAE versus AE articles is larger for non-
intersecting author lists (138 vs. 61) than for intersecting author
lists(96vs.80).Thismeansthat while slightlyfewerNonAEthan
AE artifacts are referenced in our dataset (column 3), they are
referencedinmore articles (column 4).
In summary, we cannot draw clear conclusions from the data.
The highernumberofreferencedAEartifactsbynon-intersecting
author groups may be seen as a weak indication that evaluated
artifactsareeasiertoreusebyauthorsthatwerepreviouslyunfa-
miliarwiththeartifact.Butatthesametime,thesmallernumber
of referenced NonAE artifacts is referenced by a larger number of
articles.Ourresultsarelimitedtoarticlesinourdataset,andour
analysis should be extended to a larger corpus of articles in further
research.
Finding 4: More AE artifact links are being referenced,
but more references exist to the fewer NonAE artifacts.8 RQ5: ARE SUCCESSFULLYEVALUATED
ARTIFACTSMORETHOROUGHLY
DOCUMENTED?
Accordingtopublishedresults,documentationisperceivedasan
importantqualitycriterionforartifactsbymanyusersandpastAEC
members[ 11,20]. Ifanartifactcontainsnoorlittledocumentation,
itisdifficulttoreuseandthearticleâ€™sresultsarelikelydifficultto
reproduce,whichclearlylimits its quality.
8.1 Method
As the analysis of documentation requires the download of linked
artifacts, which requires a manual investigation of the linkedweb
sites,werestrictouranalysistoarandomlydrawnsampleof100
artifacts for each category (AE and NonAE). As each article may
contain multiple artifact links from different sources, we priori-
tize links from the ACM DL over links found in PDF files over
links from other sources, i.e., Conference Publishing Consulting
orFindResearch.Therationaleforthisprioritizationisthatlinks
inPDFfilesareprovidedbyauthorsandeasilyidentifiedbyread-
ers. The links published by Conference Publishing Consulting and
FindResearcharealsoauthorsubmittedlinks,butareusuallynot
directly visible to readers, unless they explicitly search for infor-
mation on these platforms. To address the imbalance of archive
file types(e.g.,zip ortar) vs.repository linksinthe ACMDLcom-
pared to other sources, we generally give preference to archive file
types over other links from the same source. In the case of links
toZenodo,weuseZenodoâ€™sRESTAPItoresolvetheartifactlink
todownloadlinksoffileslinkedwiththeZenodorecord.ForGit
repositorylinks,weattempttocheckouttheversionsthatgotac-
cepted/rejected during AE, where we determine the date as the AE
notificationdateifthatinformationisavailable.Ifthatinformation
was not available, we either used the camera-ready due date (if
indicatedasrelevantforartifactsaswellonthevenueâ€™swebsite)or
the dateofthe venueâ€™sprogram announcement.
To approximate the adequacy of artifact documentation, we
search for document file types in the artifact and quantify the
amount of documentation by word counts. As we are not aware
of any existing standards for research artifact documentation or
widelyacceptedpractices,wethenproceededtosearchfor12(case
insensitive)file namepatterns across the identified documentfiles
according to our experience with research artifacts Å‚Ë†read.*meÅ¾,
Å‚Ë†setupÅ¾,Å‚Ë†installÅ¾,Å‚Ë†doc/Å¾,Å‚Ë†examples?/Å¾,Å‚Ë†assets?/Å¾,Å‚Ë†artifactÅ¾,Å‚de-
tailed.*result.*pdfÅ¾, Å‚report.*\.pdfÅ¾, Å‚supplement.*\.pdfÅ¾, Å‚Ë†copyrightÅ¾,
Å‚Ë†licenseÅ¾. The first four items target typical file names with ini-
tial instructions for software projects. The next three keywords
are inspired by our observation that research artifacts we have
evaluatedorworkedwithcontainthem and that artifact-relatedin-
formation of larger projects is kept in dedicated artifact directories.
Thenextthreekeywordstargetdetailedtechnicaldocumentation
extending the published article. Finally, the last two keywords indi-
catethepresenceoflicensinginformation,whichisamandatory
prerequisite for (re-)use of the artifact.
8.2 Results
Table 7shows the result of our documentation analysis of 100 sam-
pled AE and 100 sampled NonAE artifacts. In our randomly drawn
153ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore S.Winter, C.S.Timperley, B.Hermann,J. Cito, J. Bell, M. Hilton, andD. Beyer
sample of 100 artifacts each, 13 artifacts in the AE group and 12
artifactsinthe NonAE groupdid not includeanyfile matchingany
of our search terms. For the artifacts with missing documentation,
there is no pattern in terms of conference or year. Out of the re-
mainingartifacts,84fromtheAEsampleand86fromtheNonAE
samplecontainedaREADMEfilewithamuchhigheraverageword
countforfilesintheAEsample.Atthesametime,separatedocu-
mentation and examples directories are more common and their
contentismorecomprehensive(intermsofaveragewordcount)
for the NonAE sample than the AE sample. A deeper analysis of
the collected data reveals that the large amount of documentation
andexamplesintheanalyzedNonAEsampleisonlycontributed
by a comparatively small fraction of 15 artifacts, out of which only
5 exceed the single observed AE Å‚Ë†doc/Å¾ word count and only 2 the
mean AE Å‚Ë†examples?/Å¾ word count. We see this as an indicator
thattheartifactsintheNonAEsamplearehighlydiversewithonly
fewartifacts providingextensive documentation.
Overall, the results show that AE artifacts tend to have more
comprehensiveoverviewdocumentationinREADMEfiles,whereas
we observe some NonAE artifacts to have a shorter overview docu-
mentationandamorecomprehensivedocumentationinseparate
directories,albeitinalimitednumberofcases.Whiletheformer
is more suitable for a focused reproduction of research results, the
latterismoresuitableforrepurposingandreuse,whichmayhint
atunderlyingdifferencesintheperceivedpurposeofresearchar-
tifacts [11]. The documentation of positively evaluated artifacts
focuses on reproduction, whereas the documentation of (some)
otherartifacts focuses onreuse andrepurposing.
Basedontheobservationthatatop-levelREADMEfileismiss-
ing for 15% of the sampled artifacts and our difficulties to identify
suitablesearchtermsforartifactdocumentation,wefurthermore
recommendthedevelopmentofcommunitystandardsforartifact
packagingandevaluation.AtESEC/FSE,forinstance,certaindoc-
umentation is now required to be included with the artifact sub-
mission,andwerecommendtodevelopsimilarunifiedcommunity-
wide standardsfor artifact submissions,evaluation, andarchival.
Fromourresults,wealsoseethatthemajorityofthesampled
artifactsdonotseemtocontainproperlicensinginformation.As
some of the analyzed artifacts were obtained from the ACM DL
and Zenodo, which provide means to specify artifact licenses on
theartifactwebpages,weadditionallyinvestigatedthepresenceof
licensing information for artifacts on those platforms, whenever
we didnot find alicensefile.
Outoftheartifactsforwhichwedidnotfindalicense,26artifacts
from the AE sample and one artifact from the NonAE sample were
hosted on the ACM DL or Zenodo. We investigated these cases
manuallytoconfirmtheabsenceoflicenses:For22ofthosefrom
theAEsample,wewereabletoobtainthelicense,whereaswewere
not able to obtain any license for 4 artifacts from the AE sample
and the NonAE artifact. All 4 artifacts from the AE sample are
hostedontheACMDL,which,contrarytoZenodo,doesnotstrictly
mandatealicensespecification.TheartifactfromtheNonAEsample
ishostedon Zenodoandtheauthors chosetheÅ‚OtherÅ¾option for
thelicense,whichiscommonlyusedifdifferentpartsoftheartifact
arepublishedunderdifferentlicenses.Thisindeedisthecasefor
theartifact,butfor some partsthelicensefilesaremissing,which
leaves the terms ofuse for thoseparts unclear.Table 7: Number of articles with file names matching the
givensearchterms;wordcountsareaveragesacrossthegiven
numbers ofarticlesand rounded to integervalues
SearchTermMatchedArtifacts WordCount
AE NonAE AE NonAE
Nomatch 13 12 Å› Å›
Ë†read.*me 84 86 1389 645
Ë†install 6 1 324 593
Ë†doc/ 1 8 2431 13901
Ë†examples?/ 4 9 1470 426353
Ë†assets?/ 1 1 10412 657
Ë†artifact 6 1 2973 1203
report.*\.pdf 1 1 1822 42789
supplement.*\.pdf 1 1 2222 2086
Ë†copyright 0 1 0 268
Ë†license 50 46 850 1220
Out of the 26 artifacts with licenses specified in publication
metadata,4werealsoavailableonotherplatforms,whichdidnot
include a license file. Therefore, we consider it advisable to include
licensefileswiththe artifacts, ratherthanjust inthe metadata.
Finding5: Documentationpracticesstronglydifferacross
AE and NonAE artifacts and within the NonAE group.
Many AE and NonAE artifacts are lacking licenses and
copyright information.
9 THREATS TO VALIDITY
The chosen methodology to answer our research questions results
inanumber ofthreatsto the validity of our conclusions.
Construct validity: Participation in artifact evaluation, as a vari-
ableofinterest,isnotdirectlymeasurablebecauseAEprocessesare
typically notpublic. Ourcategorizationfocuses onartifact badges
asanindicator,becausethecorrespondingartifactsareknownto
havepassedAE.Basedonalimitedsetofopen-review-basedAE
processes (FSE 2016, ESEC/FSE 2018, ICSE 2020, MODELS 2018,
SLE2016),weassumethenumberofartifactsthathavebenefited
from the AE processbut didnot getabadge to be negligible.
Internalvalidity: ExceptforRQ2,thefindingsofourstudyare
basedonassociationsbetweenarticlecategoriesandothervariables
and do not hypothesize causal relations. For RQ2, we detail why
weconsideracausalrelationbetweenhostingplatformandartifact
availability reasonable.
Wecontrolforconfoundingfactorsinouranalysistothedegree
possible by the data that is available to us. Especially in terms of
how AE is conducted and how AEC chairs implement/guide an AE
processmayhaveastrongeffectandwecannottriviallyassessthat,
because itisrarelydocumented.
To control for confounding, we stratify our dataset according to
hypothesized confounding factors, test for differences across them,
154A Retrospective Studyof OneDecade of ArtifactEvaluations ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
andmaintainthestratificationiftheobservedeffectsaresignificant
(aftercorrectionformultipletesting).Theresultingstratification
leadsto smallersamplesizeswithinthe strata,whichreduces the
discriminativepoweroftestsfordifferencesinourresponsevariable
(citations). As a consequence, the reported results are conservative.
To control for selection bias, we include a wide range of SE and
PL conferencesthatadopted artifact evaluationover severalyears
usingdifferentprocesses,fromwhichwerandomlysampleartifacts
for our documentation analysis. The selection of subjects in our
study is restricted to conferences organized or supported by the
ACM,forwhich publicationandartifactdata isavailablethrough
the ACM DL. We have taken great care to analyze potential effects
of this choice, e.g., by cross-comparing the obtained metadata with
othersources,e.g.,fromConferencePublishingConsulting.During
our consistency checks, we identified and reported a number of
data inconsistenciesto ACM,whichgot acknowledgedandfixed.
OurapproachofidentifyingartifactURLsfromPDFsisimperfect
and represents a threat to internal validity. We mitigate the threat
by (a) providing the set of identified artifact URLs as part of our
dataset, allowing them to be scrutinized, and (b) including our tool
for identifying artifact URLs as part ofour reproduction package.
External validity: Our analysis and, thus, our conclusions are
limitedtotheSEandPLvenues,forwhichproceedingsareavailable
intheACMDL.However,thissampleaccountsfor64outof89data
points (i.e., almost 71.9%) according to the most comprehensive
study ofAE adoption inthe SE andPLcommunities to date[ 11].
10 DISCUSSION
In the first decade after its initiation, artifact evaluations have
significantlygainedpopularityintheSEandPLcommunities.In
ourarticle,welookattheartifactsevaluatedduringthisperiodand
makeacomparativeassessmentwithresearchartifactsthathave
not been submitted or did not successfully pass artifact evaluation.
Inthissection,wediscussourfindingsandmakerecommendations
to further improve artifact evaluations for the coming decade.
AERewardMechanisms: Themainrewardmechanismforarti-
fact submitters are badges, which are prominently displayed in the
title area of articles and in digital libraries like the ACM DL. How-
ever,inourstudywefindthatthisadvertisementofresearchresults
obtainedwithevaluatedartifactsdoesnotsignificantlyaffectthe
visibilityofresearcharticlesintermsofcitations.Asmuchofthe
traditional academic performance evaluation is centered around
citation-derived measures, the creation and maintenance of arti-
factsiscurrentlynotwellintegratedinthissystem,alsobecause
thereisnostandardizedwaytoreferencethemandtheyare,hence,
likely to escape the common citation tracking mechanisms. As the
creation of these artifacts entails significant overheads, we encour-
age the SE and PL communities to propose and discuss alternative
reward mechanisms for authors who create and publish high qual-
ity research artifacts, which benefit the research community as
a whole. Besides more rigorous attribution policies for artifacts,
whichcouldbeincludedinpeer-reviewguidelines,alternativere-
ward mechanisms could also be based on non-citation measures,
e.g.,thenumberofpositivelyevaluatedartifactsortheR+index[ 4].
Withadecadeofartifactpublications,theadditionoftest-of-timeawards for artifacts may also reward creators of particularly useful
artifactsandconstituteavaluableadditiontoconferenceprograms.
Å‚AvailableÅ¾Assessment: InSection5 wediscusstheimpactthe
Å‚AvailableÅ¾-badge-imposedrequirementshaveontheavailabilityof
researchartifacts.However,from Figure3,weseethatthereisanup
and down in AE participation and that NoB articles still dominate
for SE conferences (see ESEC/FSE, ISSTA, ICSE), even after 2017,
when theÅ‚AvailableÅ¾badges were introduced.For PLconferences
thesituationisabitbetter,butthereisgenerallyverylittlereasonto
not get Å‚AvailableÅ¾ badges for any NoB article. We suspect that the
reasonforthispartiallyisthattheprocessforobtainingÅ‚AvailableÅ¾
badgesisoftenlinkedwiththeartifact-evaluationprocess.Authors,
who do not want to get an actual evaluation of their artifacts may
not be aware of the Å‚AvailableÅ¾ badge option. At the same time, we
have seen some Å‚AvailableÅ¾ articles in our dataset, for which we
could not easily find links. This could be prevented by introducing
anadditionalcheckforthecamera-readyversionofarticleswhether
theycontainanartifactreferenceiftheyareassignedtheÅ‚AvailableÅ¾
badge.18In summary, we recommend to link the Å‚AvailableÅ¾ badge
assignment with calls for papers and the paper-review process,
rather than the artifact evaluation. We also recommend to focus
furtherresearchonthefactorsthatpreventauthorsfrompackaging,
submitting,andpublishingtheirresearchartifacts,asweexpectthe
related insights to significantly benefit our communitiesâ€™ processes
andthe availability ofresearch artifacts.
Community Standards: Our analysis of the documentation for a
sampleofartifactshasrevealeddeficienciesregardingthepresence
of common documentation andlicense files. This means that such
informationiseitherindeedmissingorthatitishiddeninplaces
not covered by our analysis. To make sure that this information
is present for every artifact and that it can be easily found, we
recommend the communities to develop common standards for
the documentation of artifacts. ESEC/FSE, for instance, is currently
mandating certain information to be present in certain files in the
artifactsubmission, andwe endorse toadopt andextend thisstan-
dardizationeffort.Specifically,standardscouldalsocoveraspectsof
artifactpackaging,submission,publication,andreferencing,which
would facilitate artifact reviews as well as automated checks to
scale with the hopefully further increasing numbers of artifact
publicationsinthe coming years.
DECLARATIONS
Data-AvailabilityStatement. Alldataandscriptsareavailable
onZenodo [ 23].
FundingStatement. ThisworkissupportedbytheNationalSci-
enceFoundationunderGrants2100037and2100015.Openaccess
wasfundedbythe LMUexcellentFund.
Acknowledgments. We thank ACM, Conference Publishing Con-
sulting, and the many AEC chairs from past SE and PL venues
for their help with filling gaps in our dataset. We appreciate the
anonymousreviewersâ€™feedbackandconstructivesuggestionsfor
improvingour manuscript.
18Conference Publishing Consulting has decided to follow this recommendation and a
corresponding check will alreadybeimplemented for ESEC/FSE 2022.
155ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore S.Winter, C.S.Timperley, B.Hermann,J. Cito, J. Bell, M. Hilton, andD. Beyer
REFERENCES
[1]SimonAdar,DirkBeyer,PatriciaCruse,GustavoDurand,WayneGraves,Christo-
pherHeid,LundonHolmes,ChuckKoscher,MeredithMorovatis,JoshuaPyle,
BernardRous,WesRoyer,and DanValen.2017. BestPractices onArtifactInte-
gration. Zenodo. https://doi.org/10.5281/zenodo.7296608
[2]Vaibhav Bajpai, Anna Brunstrom, Anja Feldmann, Wolfgang Kellerer, Aiko Pras,
Henning Schulzrinne, Georgios Smaragdakis, Matthias WÃ¤hlisch, and Klaus
Wehrle.2019. TheDagstuhlBeginnersGuidetoReproducibilityforExperimental
Networking Research. SIGCOMM Comput. Commun. Rev. 49, 1 (feb 2019), 24Å›30.
https://doi.org/10.1145/3314212.3314217
[3]M.Baker.2016. 1,500scientistsliftthelidonreproducibility. Nature533(May
2016),452Å›454. https://doi.org/10.1038/533452a
[4]Maria Teresa Baldassarre, Neil A. Ernst, Ben Hermann, Tim Menzies, and Rahul
Yedida. 2021. Crowdsourcing theState of theArt(ifacts). CoRR2108.06821(2021).
https://doi.org/10.48550/arXiv.2108.06821
[5]Yoav Benjamini and Yosef Hochberg. 1995. Controlling the False Discovery
Rate: A Practical and Powerful Approach to Multiple Testing. Journal of the
Royal Statistical Society. Series B (Methodological) 57, 1 (1995), 289Å›300. http:
//www.jstor.org/stable/2346101
[6]Nicolas Bonneel, David Coeurjolly, Julie Digne, and Nicolas Mellado. 2020. Code
Replicability in Computer Graphics. ACM Trans. Graph. 39, 4, Article 93 (jul
2020),8 pages. https://doi.org/10.1145/3386569.3392413
[7]ChristianS.CollbergandToddA.Proebsting.2016. Repeatabilityincomputer
systemsresearch. Commun.ACM 59,3(2016),62Å›69. https://doi.org/10.1145/
2812803
[8]EitanFrachtenberg.2022. Researchartifactsandcitations incomputersystems
papers.PeerJ Computer Science 8 (Feb. 2022), e887. https://doi.org/10.7717/peerj-
cs.887
[9]OddErikGundersen,SaeidShamsaliei,andRichardJuulIsdahl.2022. Domachine
learning platforms provide out-of-the-box reproducibility? Future Generation
Computer Systems 126 (2022), 34Å›47. https://doi.org/10.1016/j.future.2021.06.014
[10]Ivan Heibi,SilvioPeroni,and DavidShotton.2019. Software review: COCI,the
OpenCitations Indexof Crossref openDOI-to-DOI citations. Scientometrics 121,
2 (11 2019),1213Å›1228. https://doi.org/10.1007/s11192-019-03217-6
[11]BenHermann,StefanWinter,andJanetSiegmund.2020. CommunityExpecta-
tionsforResearchArtifactsandEvaluationProcesses.In Proc.ESEC/FSE .ACM,
469Å›480. https://doi.org/10.1145/3368089.3409767
[12]Ben Hermann, Stefan Winter, and Janet Siegmund. 2020. Community Expec-
tations for Research Artifacts and Evaluation Processes (Additional Material).
Zenodo. https://doi.org/10.5281/zenodo.3951724[13]Robert HeumÃ¼ller, Sebastian Nielebock, Jacob KrÃ¼ger, and Frank Ortmeier. 2020.
Publishorperish,butdonotforgetyoursoftwareartifacts. Empir.Softw.Eng. 25,
6 (2020), 4585Å›4616. https://doi.org/10.1007/s10664-020-09851-6
[14]ShriramKrishnamurthi.2013. Artifactevaluationforsoftwareconferences. ACM
SIGSOFT Softw. Eng. Notes 38, 3 (2013), 7Å›10. https://doi.org/10.1145/2464526.
2464530
[15]ShriramKrishnamurthiandJanVitek.2015.Therealsoftwarecrisis:Repeatability
as a core value. Commun. ACM 58, 3 (2015), 34Å›36. https://doi.org/10.1145/
2658987
[16]Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2021.
On the Reproducibility and Replicability of Deep Learning in Software Engi-
neering.ACMTrans.Softw.Eng.Methodol. 31,1,Article15(oct2021),46pages.
https://doi.org/10.1145/3477535
[17]NationalAcademiesofSciences,Engineering,andMedicine.2019. Reproducibility
and Replicability in Science . National Academies Press. https://doi.org/10.17226/
25303
[18]NISO. 2021. Reproducibility Badging and Definitions: A Recommended Practice
of the National Information Standards Organization . Technical Report NISO
RP-31-2021. https://doi.org/10.3789/niso-rp-31-2021
[19]MartinShepperd,NemitariAjienka,andSteveCounsell.2018. Theroleandvalue
ofreplicationinempiricalsoftwareengineeringresults. Inf.Softw.Technol. 99
(2018), 120Å›132. https://doi.org/10.1016/j.infsof.2018.01.006
[20]Christopher Steven Timperley, Lauren Herckis, Claire Le Goues, and Michael
Hilton.2021. UnderstandingandImprovingArtifactSharinginSoftwareEngi-
neering Research. Empir. Softw. Eng. 26, 4 (2021), 67. https://doi.org/10.1007/
s10664-021-09973-5
[21]Christopher S. Timperley, Lauren Herckis, Claire Le Goues, and Michael Hilton.
2021.Replication Package for Understanding and Improving Artifact Sharing in
SoftwareEngineering Research .https://doi.org/10.5281/zenodo.4737346
[22]ChatWacharamanotham,LukasEisenring,SteveHaroz,andFlorianEchtler.2020.
TransparencyofCHIResearchArtifacts:ResultsofaSelf-ReportedSurvey . ACM,
1Å›14.https://doi.org/10.1145/3313831.3376448
[23]StefanWinter,ChristopherS.Timperley,BenHermann,JÃ¼rgenCito,Jonathan
Bell, Michael Hilton, and Dirk Beyer. 2022. Reproduction Package (Docker
Container) for the ESEC/FSE2022 Article â€˜A Retrospective Study of One Decade
of Artifact Evaluationsâ€™. Zenodo. https://doi.org/10.5281/zenodo.7082407
[24]NoaZilbermanandAndrew W. Moore.2020. ThoughtsaboutArtifact Badging.
SIGCOMMComput.Commun.Rev. 50,2(may2020),60Å›63. https://doi.org/10.
1145/3402413.3402422
Received 2022-03-17; accepted 2022-06-14; revised 2022-09-05
156