Cross-Domain Deep Code Search with Meta Learning
Yitian Chai1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1âˆ—
1School of Software, Shanghai Jiao Tong University, China
2The University of Newcastle, Australia
{sjtu_chaiyt,bjshen,xiaodong.gu}@sjtu.edu.cn,hongyu.zhang@newcastle.edu.au
ABSTRACT
Recently, pre-trained programming language models such as Code-
BERT have demonstrated substantial gains in code search. Despite
their success, they rely on the availability of large amounts of par-
allel data to fine-tune the semantic mappings between queries and
code. This restricts their practicality in domain-specific languages
withrelativelyscarceandexpensivedata.Inthispaper,wepropose
CDCS, a novel approach for domain-specific code search. CDCS
employsatransferlearningframeworkwhereaninitialprogram
representationmodelispre-trainedonalargecorpusofcommon
programming languages (such as Java and Python), and is further
adaptedtodomain-specificlanguagessuchas SolidityandSQL.Un-
likecross-language CodeBERT,whichis directlyfine-tunedin the
target language, CDCS adapts a few-shot meta-learning algorithm
called MAML to learn the good initialization of model parameters,
whichcanbebestreusedinadomain-specificlanguage.Weevaluate
the proposed approach on two domain-specific languages, namely
Solidity and SQL, with model transferred from two widely used
languages(PythonandJava).ExperimentalresultsshowthatCDCS
significantly outperforms conventional pre-trained code models
that are directly fine-tuned in domain-specific languages, and it is
particularly effective for scarce data.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Reusability ;Automaticpro-
gramming.
KEYWORDS
CodeSearch,Pre-trainedCodeModels,MetaLearning,Few-Shot
Learning, Deep Learning
ACM Reference Format:
Yitian Chai1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1. 2022. Cross-
DomainDeepCodeSearchwithMetaLearning.In 44thInternationalConfer-
enceonSoftwareEngineering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA.
ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/3510003.3510125
âˆ—Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35101251 INTRODUCTION
Recently,deepneuralnetworks(DNN)havebeenwidelyutilizedfor
code search [ 4,9,13,14,28,38]. Unlike traditional keyword match-
ing methods [ 2,7,16,17,21,22], deep code search models employ
deep neural networks to learn the representations of both queries
and code, and measure their similarities through vector distances.
The application of DNNs significantly improves the understanding
ofcodesemantics,therebyachievingsuperbperformanceincode
search tasks [9, 13, 18, 40].
Amajorchallengefordeepcodesearchistheadaptationofdeep
learningmodelstodomain-specificlanguages.State-of-the-artcode
searchmethodsaremainlydesignedforcommonlanguagessuch
asJavaandPython.Theyrelyheavilyontheavailabilityoflarge
parallel data to learn the semantic mappings between code andnatural language [
11]. On the other hand, there is an emerging
trend of domain-specific languages such as Solidity for smart con-
tracts [37,39,44] where code search is also needed. There is often
insufficient training data in specific domains, causing poor fit of
deeplearningmodels.Furthermore,foreachspecificdomain,the
costs of data collection, cleaning, and model training for construct-
ing an accurate model are all non-neglectable.
One potential route towards addressing this issue is the pre-
trained code models, which pre-train a common representation
model on a large, multilingual code corpus, and then fine-tune the
model on task-specific data [ 29]. This enables code search models
to transfer prior knowledge from the data-rich languages to the
low-resourcelanguage.Forexample,CodeBERT[ 9],thestate-of-
the-artcoderepresentationmodel,canbepre-trainedonmultiple
commonlanguagesandthenfine-tunedinthecodesearchtaskforatargetlanguage[
29].However,itischallengingtoreuseknowledge
from a mix of source languages for code search in the target lan-
guage.Differentlanguageshavetheiruniquecharacteristics,and
correspond to different representations. Parameters learnt fromeach language can distract each other, resulting in a conflict inthe shared representations. This is even more challenging in the
domain-specificcodesearch,wherethetargetlanguageusuallyhas
scarce training samples.
Inthispaper,wepresentCDCS(Cross-DomainDeepCodeSearch),
a cross-domain code search technique based on few-shot meta
learning.CDCSextendstheâ€œpretraining-finetuningâ€paradigmof
CodeBERT with a meta learning phase that explicitly adapts themodel parameters learnt from multiple source languages to the
targetlanguage.CDCSbeginsbypre-trainingCodeBERTonalarge
corpus of multiple common languages such as Java and Python.
Then,ametalearningalgorithmnamedMAML(Model-Agnostic
Meta-Learning)isemployedin ordertopreventthemodelparam-
etersfromfallingintothelocaloptimizationofsourcelanguages.
4872022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chai, et al.
Thegoalofthisalgorithmistofindtheinitializationofmodelpa-
rameters that enables fast adaptation to a new task with a small
amount of training examples.
To evaluate the effectiveness of CDCS, we pre-train CDCS on
a large corpus of common languages such as Python and Java.
Then, we perform code search on two domain-specific datasets
written in Solidity and SQL. We compare our approach with three
baseline models, namely, aneural codesearch modelwithout pre-
training, a within-domain pre-training model CodeBERT [ 9], and a
cross-languageCodeBERT[ 29]thatdirectlyfine-tunesthetarget
languageonapre-trainedmodel.Experimentalresultsshowthat
CDCS significant outperforms within-domain counterparts. In par-
ticular, our approach shows more strength when the data is scarce,
indicatingthesuperbeffectivenessofourapproachincross-domain
code search.
The contributions of this work can be summarized as:
â€¢WeproposeCDCS,anovelcross-domaincodesearchmethod
using few-shot meta learning.
â€¢WeextensivelyevaluateCDCSonavarietyofcross-language
code search tasks. Experimental results have shown thatCDCS outperforms the pre-training and fine-tuning coun-
terparts by a large margin.
2 BACKGROUND
2.1 Code Search Based on Deep Learning
The past few years have witnessed a rapid development of deep
learningforsoftwareengineering,inwhichcodesearchhasbeen
one of the most successful applications. Compared with traditional
textretrievalmethods,deeplearningbasedcodesearchlearnsrepre-
sentationsofcodeandnaturallanguageusingdeepneuralnetworks,
and thus has achieved superb performance [4, 9, 13, 14, 28].

 	
	

	

	  



 


 	
Figure 1: Deep learning based code search.
Figure 1 shows the overall framework of deep learning based
codesearch.Inthetrainingphase,abi-modaldeepneuralnetworkistrainedbasedonalargeparallelcorpusofcodeandnaturallanguage
tolearnthesemanticrepresentations(high-dimensionalvectors)
of both queries and code snippets. Then, a similarity function is
employed to numerically compute the similarity between code and
query vectors. The model is usually trained by minimizing the
triplet ranking loss [27], namely,
L(ğ‘,d+,dâˆ’)=max(cos(c,d+) âˆ’cos(c,dâˆ’) +ğœ–,0)(1)
		

	
 
		 		 
	      	 	
Figure 2: Architecture of CodeBERT with masked languagemodel.
where c,d+, and dâˆ’represent the vector representations for the
code, the correct description, and the distracting description, re-
spectively. cosdenotesthecosinesimilaritybetweentwovectors.
ğœ–is a margin which ensures that d+is at least ğœ–closer to cthan
dâˆ’[27].
In the search phase, the search engine is given a query from the
user. It computes the vectors for both the query and code snippets
in the codebase using the trained model. Then, it goes through thecodebase and matches the query with each code snippet according
to their vector distances. Snippets that have the best matching
scores are returned as the search results.
2.2 Pre-trained Models for Code Search
Recently, pre-trained models such as BERT [ 8] and GPT-2 [ 26]
have achieved remarkable success in the field of NLP [ 8,26]. As
such, researchers start to investigate the adaptation of pre-trained
models to software programs [ 9,36?]. Code search is one of the
mostsuccessfulapplicationsofpre-trainedmodelsforprogramming
languages.
One of the most successful pre-trained models for code is the
CodeBERT[ 9].CodeBERTisbuiltontopofBERT[ 8]andRoberta[ 20],
twopopularpre-trainedmodelsfornaturallanguage.Unlikepre-
trainedmodelsinNLP,CodeBERTisdesignedtorepresentbi-modal
data [5], namely, programming and natural languages. Figure 2
shows the architecture of CodeBERT. In general, the model is built
uponaTransformer encoder.Thetraininginvolvestwopre-training
tasksinsixprogramminglanguages.Oneisthemaskedlanguage
modeling(MLM),whichtrainsthemodeltofillthemaskedtokenin
the input sequences. The other task is the replaced token detection
(RTD), which trains the model to detect the replaced tokens in the
inputsequences.Thesetwopre-trainingtasksendowCodeBERT
with generalization ability, so that it can be fine-tuned to adapt to
downstream tasks such as code search and code summarization.
Asacoderepresentationmodel,CodeBERThasbeensuccessfully
employed for code search [ 9]. Specifically, a binary classifier is
employed which takes as input the representation of the [CLS]
tokenandpredictswhether agiven<NL,PL>pairissemantically
related.Thisclassifieristhenfine-tunedonacodesearchdatasetbyminimizingthecross-entropyloss.Inthesearchphase,theclassifier
488
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. Cross-Domain Deep Code Search with Meta Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
predictsthematchingscorebetweenanNLqueryandeachcode
snippet in thecodebase. The search engine returnsthe top- ğ‘˜code
snippets that have the highest matching scores.
Due to the superb performance, researchers have also applied
CodeBERTforcross-languagecodesearch[ 29].Theypre-trained
CodeBERT with multiple languages such as Python, Java, PHP,
Javascript, and Go, and then fine-tuned a code search model on
anunseenlanguagesuchasRuby.Resultshaveshownthatcross-
languagecodesearchachievesbetterperformancethantrainingina
singlelanguagefromscratch.Thisfurthersupportstheeffectiveness
of transfer learning for code search [29].
2.3 Meta Learning and Few-Shot Learning
Few-shotlearningisamachinelearningtechnologythataimsto
quickly adapt a trained model to new tasks with less example [ 30].
Despite the superb performance, deep learning models are often
data-hungry[ 11].Theyrely ontheavailabilityoflarge-scaledata
for training. That means, the performance can be limited due to
thescarcityofdatainspecificdomains[ 11].Bycontrast,humans
canlearnknowledgefromafewexamples.Forexample,achildcanlearntodistinguishbetweenlionsandtigerswhenprovidedwitha
few photos, probably because human beings have prior knowledge
beforelearningnewdataorbecausehumanbrainshaveaspecial
waytoprocessknowledge.Basedonthisintuition,researchershave
proposed few-shot learning.
Few-shot learning methods can be roughly classified into the
following two categories:
1)Metric-based methods , which learn a distance function be-
tweendatapointssothatnewtestsamplescanbeclassifiedthroughcomparisonwiththe
ğ¾labeledexamples[ 42].Thereareafewtypi-
cal algorithms for metric-based few-shot learning, such as Siamese
Network[6],Prototypical Network [30], and Relation Network [32].
2)MetaLearning ,alsoknownasâ€œlearning-to-learnâ€,whichtrains
amodelonavarietyoflearningtasks,suchthatitcansolvenew
learning tasksusing only asmall number oftraining samples[ 10].
Unlike the conventional machine learning prototype that a model
is optimized in the training set to minimize the training loss, meta
learningupdatesmodelparametersusingthevalidationlossinorder
to enhance the generalization to different tasks. There are some
typical algorithms for few-shot meta learning, such as MAML[10]
andReptile[24].
MAML(Model-AgnosticMeta-Learning)isafew-shotmetalearn-
ingalgorithmwhichaimsatlearningagoodinitializationofmodelparameterssothatthemodelcanquicklyreachtheoptimalpointin
a new task with a small number of data samples [ 10,42]. The algo-
rithmassumesthatthedatausedfortrainingfollowsadistribution
ğ‘(ğ‘‡)overğ‘˜tasks{ğ‘‡1,...,ğ‘‡ ğ‘˜},whereğ‘‡ğ‘–standsforaspecificmachine
learningtaskonthedata.Theintuitionisthatsomedatafeatures
aremoretransferrablethanothers.Inotherwords,theyarebroadly
applicabletoalltasksin ğ‘(ğ‘‡),ratherthanasingleindividualtask
ğ‘‡ğ‘–.Tofindsuchgeneral-purposerepresentations,MAMLupdates
modelparametersthataresensitivetochangesinthetask,suchthat
small changes in the parameters will produce large improvements
on the loss function of any task drawn from ğ‘(ğ‘‡). Motivated by
this, MAML separates data into individual tasks. A meta learneris employed to update parameters using gradients on each local	

			

	


 



  
	
	
	
	

	

	
	
	

 
 
 

	


 	


	
	
	


Figure 3: Architecture of CDCS.
taskğ‘‡ğ‘–[10]. A more detailed description of the algorithm and how
it is adapted to code search will be presented in Section 3.3.
3 APPROACH
3.1 Overview
Figure3showsthearchitectureofCDCS.Ingeneral,CDCStakes
CodeBERT[ 9]asthebackbone,andextendsitwithametalearning
phase. The core component of CDCS is RoBERTa [ 20], which is
built upon a multi-layer bidirectional Transformer [33] encoder.
ThepipelineofCDCSinvolvesfourphases.SimilartoCodeBERT,
we start by pre-training CDCS to learn code representations in
a large corpus of multiple source languages. Next, we perform
metalearningtoexplicitlytransfertherepresentationsofsource
languagesintothetargetlanguage.Afterthedomainadaptation,we
fine-tune it on the code search data of the target language in order
to train the semantic mapping between code and natural language.
Wefinallyperformcodesearchusingthefine-tunedmodel.Wewill
describethedetaileddesignofeachphaseinthefollowingsections.
3.2 Pre-training
The pre-training phase aims to learn code and NL representations
from a large corpus of multiple common languages such as Javaand Python. Similar to CodeBERT, we use the pre-training taskof masked language modeling (MLM). We did not use the RTD
(replaced token detection) pre-training task of CodeBERT because
the effect of this task has been shown to be marginal [9].
In the pre-training phase, the model takes as input an /angbracketleftNL, PL/angbracketright
pair which is formatted into a sequence of
[ğ¶ğ¿ğ‘†],ğ‘¤1,ğ‘¤2,...ğ‘¤ ğ‘›,[ğ‘†ğ¸ğ‘ƒ],ğ‘1,ğ‘2,...,ğ‘ ğ‘š,[ğ¸ğ‘‚ğ‘†]
whereğ‘¤1,ğ‘¤2,...,ğ‘¤ ğ‘›denotesasequenceof ğ‘›wordsinthenatural
languagetext,while ğ‘1,ğ‘2,...,ğ‘ ğ‘šrepresentsasequenceof ğ‘štokens
in the code snippet. The special [ğ¶ğ¿ğ‘†]token at the beginning is
489
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chai, et al.
aplaceholderfortherepresentationoftheentireinputsequence.
The[ğ‘†ğ¸ğ‘ƒ]tokenindicatestheborderofthecodesnippetandthe
natural language text. The [ğ¸ğ‘‚ğ‘†]token indicates the end of the
sequence.
Duringthepre-trainingprocess,werandomlyreplace15%ofthe
tokens in the input sequence with a special [ğ‘€ğ´ğ‘†ğ¾]token and let
the model predict the original token. The task can be optimized by
minimizingthecross-entropylossbetweenthepredictedandthe
original tokens.
The pre-trained model can be used to produce the contextual
vectorrepresentationofeachtokenforbothnaturallanguagede-
scriptions and code snippets. In particular, the representation of
the[ğ¶ğ¿ğ‘†]tokenstandsfortheaggregatedsequencerepresentation
which can be used for classifying the entire input sequence.
3.3 Meta Learning
Wenextperformmetalearningtoadaptthepre-trainedcodemodel
tothetargetdomain.Weemployameta-learningalgorithmnamed
MAML (Model-Agnostic Meta-Learning) [ 10] which is a typical
algorithmforfew-shotlearning[ 10,12,31].ThekeyideaofMAML
is to use a set of source tasks { ğ‘‡1,...,ğ‘‡ğ‘˜} to find the initialization of
parameters ğœƒ0from which learning a target task ğ‘‡0would require
onlyasmallnumberoftrainingsamples[ 10].Inthecontextofcode
search,thisamountstousinglargedataofcommonlanguagesto
find good initial parameters and training a new code search model
onasmall,domain-specificlanguagestartingfromthefoundinitial
parameters.Weformulatecodesearchasabinaryclassificationtask
ğ‘‡whichpredictswhetheragiven /angbracketleftğ‘ğ¿,ğ‘ƒğ¿/angbracketrightpairmatches(1=match,
0 = irrelevant). Unlike CodeBERT which directly fine-tunes on the
code search task ğ‘‡, the MAML algorithm assumes that the dataset
usedfortrainingfollowsadistribution ğ‘(ğ‘‡)overğ‘˜tasks{ğ‘‡1,...,ğ‘‡ ğ‘˜}.
Hence, it splits ğ‘‡into a set of ğ‘˜tasks{ğ‘‡1,...,ğ‘‡ ğ‘˜}. Each task ğ‘‡ğ‘–aims
at training a code search model with small sized data, therefore
simulatesthe low-resourcelearning.Basedon thisidea,each ğ‘‡ğ‘–is
assignedtotrainthecodesearchmodelinaprivatetrainingand
validation set denoted as ğ‘‡ğ‘–âˆ¼{ğ·train,ğ·valid}.
Letğœƒdenotetheglobalparametersfortheentiremodeland ğœƒğ‘–
denotethelocalparametersfortask ğ‘‡ğ‘–.Ametalearneristrained
to update model parameters ğœƒğ‘–using one or more gradient descent
updatesontask ğ‘‡ğ‘–.Forexample,whenusingonegradientupdate,
the training step can be formulated as
ğœƒğ‘–=ğœƒâˆ’ğ›¼âˆ‡ğœƒğ¿ğ‘‡ğ‘–(ğ‘“ğœƒ),ğ‘–=1,...,ğ‘˜ (2)
whereğ‘“ğœƒdenotes the deep learning model for specific task with
parameters ğœƒ;ğ¿ğ‘‡ğ‘–representsthelossfunctionfortask ğ‘‡ğ‘–;ğ›¼denotes
the step size for each task and is fixed as a hyperparameter for the
meta learner.
In our approach, the training set ğ·trainfor the original code
search task ğ‘‡is randomly segmented into ğ‘˜batches{ğ·1,...,ğ· ğ‘˜}
equally. Each ğ·ğ‘–is used as the data set for the local task ğ‘‡ğ‘–.T o
performthelocaltask, ğ·ğ‘–isfurthersplitintoatrainingandvali-
dationset {ğ·train
ğ‘–,ğ·valid
ğ‘–}withthesamedatasize.Each ğ‘‡ğ‘–isthen
performedon {ğ·train
ğ‘–,ğ·valid
ğ‘–}toobtainthelocalgradient âˆ‡ğœƒğ¿ğ‘‡ğ‘–(ğ‘“ğœƒ).
These local gradients are aggregated by the meta-learner every ğ‘€
steps in order to update the global parameter ğœƒ.
The procedure of our algorithm is summarized in Algorithm 1.




   

	


  
Figure 4: An overview of the MAML algorithm.
Algorithm 1 Meta Learning for Code Search
Require: ğ›¼,ğ›½: step size; ğ‘€: meta update steps
1:Pre-train the global model and obtain the initial parameters ğœƒ
2:Createğ‘˜copies of ğœƒwith each ğœƒğ‘–being the local parameters
forğ‘‡ğ‘–.
3:whilenot done do
4:Dividethedataset ğ·trainintoğ‘˜batcheswiththe ğ‘–-thbatchğ·ğ‘–
for taskğ‘‡ğ‘–
5:for each ğ·ğ‘–âˆˆğ·traindo
6:Splitğ·ğ‘–into{ğ·train
ğ‘–,ğ·valid
ğ‘–}
7:Runğ‘‡ğ‘–on{ğ·train
ğ‘–,ğ·valid
ğ‘–}and evaluate local gradients
âˆ‡ğœƒğ¿ğ‘‡ğ‘–(ğ‘“ğœƒ)using the cross-entropy loss ğ¿ğ‘‡ğ‘–
8:Update local parameters ğœƒğ‘–with gradient descent:
ğœƒğ‘–=ğœƒâˆ’ğ›¼âˆ‡ğœƒğ¿ğ‘‡ğ‘–(ğ‘“ğœƒ)
9:ifğ‘–modğ‘€== 0then
10: Evaluategradients âˆ‡ğœƒğ¿ğ‘‡ğ‘–(ğ‘“ğœƒğ‘–)usingthe cross-entropy
lossğ¿ğ‘‡ğ‘–inğ·valid
ğ‘–
11: Update the global parameters ğœƒusing the gradients on
the validation set:
ğœƒâ‡ğœƒâˆ’ğ›½âˆ‡ğœƒğ¿ğ‘‡ğ‘–(ğ‘“ğœƒğ‘–)
12:end if
13:end for
14:end while
3.4 Fine-Tuning
In the fine-tuning phase, we adapt CDCS to the code search task inthetargetlanguage.Wefine-tunethemodelonthecodesearchtask,
whichcanbeformulatedasabinaryclassificationproblem.Fora
corpusof /angbracketleftğ‘ğ¿,ğ‘ƒğ¿/angbracketrightpairs,wecreatethesamenumberofnegative
samplesbyrandomlyreplacingNLorPLintheoriginalpairs.Weas-signalabeltoeachpairtoindicatewhethertheNLiscorresponding
to the PL in the pair (1=relevant, 0 =irrelevant).
For each training instance, we build an input sequence with the
sameformatasinthepre-trainingphase.Wetakethehiddenstatein
the[ğ¶ğ¿ğ‘†]position of CodeBERT as the aggregated representation
of the input sequence. The representation is further taken as input
toafullyconnectedneuralclassifiertopredictwhetherthegiven
/angbracketleftğ‘ğ¿,ğ‘ƒğ¿/angbracketrightpair is relevant. We fine-tune the model by minimizing
the binary cross-entropy loss between predictions and labels.
490
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. Cross-Domain Deep Code Search with Meta Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
3.5 Domain-Specific Code Search
Finally,weperformcodesearchbasedonthefine-tunedmodelina
domain-specific codebase. The code search engine works with the
following steps:
1)Anaturallanguagequery ğ‘„isprovidedtothecodesearch
system.
2)SpliceQseparatelywitheachcodesnippet ğ¶ğ‘–inthecodebase
to obtain a series of input sequences
<ğ‘„,ğ¶1>,...,<ğ‘„,ğ¶ğ‘›>
3)Input these sequences into the trained model and obtain
their matching scores.
4) Sort code snippets according to their matching scores.
5) Return the top-k code snippets as the results.
4 EXPERIMENTAL SETUP
We evaluate the performance of CDCS in domain-specific code
search tasks and explore the effect of training data size on theperformance. Finally, we extend our method to other backbone
pre-trainedmodelssuchasGPT-2[ 26].Insummary,weevaluate
CDCS by addressing the following research questions:
â€¢RQ1:HoweffectiveisCDCSincross-domaincodesearch?
To verify the effectiveness of CDCS in cross-domain code
search tasks, we take Python and Java as the source lan-
guages and adapt the learned model to two domain-specific
languages, namely, Solidity and SQL. We compare the accu-
racy of code search by various approaches in the two target
languages.
â€¢RQ2: What is the impact of data size on the perfor-mance of cross-domain code search?
Asmentioned, oneofthe challengesfor cross-domaincode
search is the scarcity of data in the domain-specific lan-
guage. In RQ2, we aim to study the effect of data size on the
performance.Wevarythesizeofdatasetandcomparethe
performance under different data sizes.
â€¢RQ3:HoweffectiveisCDCSappliedtootherpre-trained
programming language models?Besides CodeBERT, there are other pre-trained models that
also achieve outstanding results in software engineering
tasks[1,23,25].Wewonderwhetherotherpre-trainedmod-
els can have the same effectiveness on code search when
equipped with meta learning. We replace the backbone pre-
trained model with GPT-2 [ 3,26], which is also a popular
pre-trained language model based on Transformer. GPT-2differs from BERT in that it is an autoregressive language
model built on top of the Transformer decoder. We evaluate
theeffectivenessof CDCSGPTâˆ’2andcompareitwiththose
of baseline models.
â€¢RQ4:Howdodifferenthyperparametersaffecttheper-formance of CDCS?
In order to study the effect of hyperparameters to the per-
formance of CDCS, we assign different hyperparameters to
CDCS and examine their impact to the performance of code
search.Table1:Statisticsofdatasetsforpre-trainingandmetalearn-
ing.
Phase Python Java
pre-train # functions 412,178 454,451
# comments 412,178 454,451
meta learning # functions 824,342 908,886
# comments 824,342 908,886
Table 2: Number of functions on the dataset of target lan-
guages.
Language Train (Finetune) Valid Test
Solidity 56,976 4,096 1,000
SQL 14,000 2,068 1,000
4.1 Implementation Details
We build our models on top of the RoBERTa [ 20] using the same
configurationasRoBERTa-base(H=768,A=12,L=12).Therateof
maskedtokensissetto15%.WeusethedefaultCodeBERTtokenizer,
namely,Microsoft/codebert-base-MLM with the same vocabulary
size (50265). We set the maximum sequence length to 256 to fit
ourmaximumcomputationalresources.Thedefaultbatchsizeis
set to 64. The three hyperparameters ğ›¼,ğ›½,ğ‘€in Algorithm 1 are
empirically set to 1 ğ‘’-5, 1ğ‘’-4, and 100, respectively. Our experimen-
talimplementationisbasedonthetoolprovidedbyHuggingface
Transformers1and the higher library provided by Facebook Re-
search2.
All models are trained on a GPU machine with Nvidia Tesla
V10032GusingtheAdam[ 15]algorithm.Weusealearningrate
of 5ğ‘’-5 [9] in the pre-training phase which warms up in the first
1,000stepsandlinearlydecays.Wemeasuretheperformanceonthe
validationsetduringthetrainingprocess,andselectthecheckpoint
of the model which has the best accuracy on the validation set for
testing.
4.2 Datasets
4.2.1 Data Used for Pre-training and Meta Learning. Wepre-train
and perform meta learning using the training data for the codesearch task provided by CodeBERT [
9]. We select two popular
languages,namely,PythonandJavaasthesourcelanguages.The
statistics of the dataset are shown in Table 1. For each language,
the dataset contains parallel data of /angbracketleftNL, PL/angbracketrightpairs, including both
positiveandnegativesamples.Inordertopreventthetrainingfrom
fallingintoalocaloptimumofonesourcelanguage,weuseonly
positive samples for pre-training and use the entire set of pairs for
meta learning.
4.2.2 Data Used for Fine-tuning and Code Search. We fine-tune
and test the code search task using two domain-specific languages,
namely, Solidity and SQL [ 39]. The statistics about the datasets are
shown in Table 2.
1https://huggingface.co/transformers/
2https://higher.readthedocs.io/
491
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chai, et al.
Solidity is an object-oriented language that is specifically de-
signed for smart contracts [ 39]. The dataset of Solidity used in our
experimentsisprovidedby[ 39]forsmartcontractcodesummariza-
tion.Wepreprocessthedatasetbyremovingallinlinecomments
fromfunctions.Weremoveduplicatepairs,namely,two /angbracketleftNL,PL/angbracketright
pairs that have the same comment but differ only in the number of
positioninthedatasetandafewvariablenamesincode.Wealso
balance positive and negative samples where the negative samples
aregeneratedbyrandomlyreplacingNL(i.e.( c,Ë†d))andPL(i.e.( Ë†c,
d)) of positive samples.
SQLisawell-knownlanguagethatisspecificallydesignedfor
manipulatingdatabasesystems.Thedatasetweusedforfine-tuning
and testing SQL is provided by [ 43] for cross-domain semantic
parsingandSQLcodegeneration(text-to-SQL).Theoriginaldata
is in a JSON format and contains the following fields:
â€¢question: the natural language question.
â€¢question_toks:the natural language question tokens.
â€¢db_id: the database id to which this question is addressed.
â€¢query: the SQL query corresponding to the question.
â€¢query_toks:theSQLquerytokenscorrespondingtotheques-
tion.
â€¢sql: parsed results of this SQL query.
WepreprocesstheSQLdatasetbyselectingtheâ€œquestionâ€and
â€œqueryâ€fieldsfromthe .jsondataasour NLandPL,respectively.We
removeduplicatedatathathasthesamecodefromtheoriginaltest
set. We also balance positive and negative samples where the neg-
ativesamplesaregeneratedbyrandomlydisruptingdescriptions
and code based on positive samples.
4.3 Evaluation Metrics
We measure the performance of code search using two popular
quantitative criteria on the test set, including MRR (Mean Recip-
rocalRank)andthetop- ğ‘˜accuracy.Theyarecommonlyusedfor
evaluating code search engines [9, 13].
MRR[22,41]aimstoletasearchalgorithmscoresearchresults
inturnaccordingtothesearchcontent,andthenarrangetheresults
accordingtothescoresinadescendorder.For ğ‘testqueries,the
MRR can be computed as
ğ‘€ğ‘…ğ‘… =1
ğ‘ğ‘/summationdisplay.1
ğ‘–=11
ğ‘…ğ‘ğ‘›ğ‘˜(ğ‘–)(3)
whereRank(i)representsthepositionofthecorrectcodesnippet
in the returned results for query ğ‘–. The greater the MRR score, the
better the performance on the code search task.
Top-k accuracy measures how many answers in the first ğ‘˜
results hit the query. This metric is close to the real-world scenario
of search tasks, that is, users want the most matching results to be
placed at the top of the results. In our experiments, we compute
the top-ğ‘˜accuracy with ğ‘˜= 1, 5, and 10, respectively.
Weusethetrainedmodeltopredictthematchingscoresof1,000
/angbracketleftğ‘ğ¿,ğ‘ƒğ¿/angbracketrightpairs in the test set. For each pair, the model computes
the similarities between the text description and all 1,000 code
snippets. The top-k similar snippets are selected for calculating
the evaluation metrics. We report the average score of all the 1,000
pairs in the test set.Table 3: Performance of each method in the SQL dataset.
Model Acc@1 Acc@5 Acc@10 MRR
No-Pretraining 0.002 0.010 0.022 0.0124
CodeBERT (NL-based) 0.652 0.926 0.966 0.7690
CodeBERT (within-domain) 0.607 0.899 0.945 0.7351CodeBERT (cross-language) 0.675 0.920 0.960 0.7818
CDCS 0.746 0.952 0.972 0.8366
Table4:PerformanceofeachmethodintheSoliditydataset.
Model Acc@1 Acc@5 Acc@10 MRR
No-Pretraing 0.002 0.008 0.014 0.0101
CodeBERT (NL-based) 0.453 0.732 0.821 0.5801CodeBERT (within-domain) 0.515 0.798 0.857 0.6383CodeBERT (cross-language) 0.532 0.779 0.848 0.6436
CDCS 0.658 0.829 0.879 0.7336
4.4 Comparison Methods
We compare our approach with four baseline methods.
(1)CodeSearchwithoutPre-training ,whichtrainsthecode
search model using only domain-specific data in Table 2
withoutpre-trainingandmetalearning.Throughcomparing
to this baseline model, we aim to verify the effectiveness of
pre-training and meta learning in our approach.
(2)CodeSearchbasedonpre-trainedmodelwithNatural
Language ,whichfine-tunesthecodesearchmodelonthe
domain-specific data in Table 2 based on the pre-trained
modelthatisinitializedbythenaturallanguagepre-training
models, namely Roberta [20] and GPT-2 [26].
(3)Within-domainCodeSearchwithCodeBERT [9],which
pre-trainsandfine-tunesonlywiththedomain-specificdata
in Table 2 without prior knowledge of common languages.
(4)Cross-LanguageCodeSearchwithCodeBERT [29],which
directly fine-tunes the code search model on the domain-specific data (Table 2) on a model that is pre-trained on
thedataofmultiplecommonlanguages(Table1).Through
comparing to this baseline model, we aim to validate the
usefulness of meta learning in our approach.
Weimplementallbaselinemodelsbasedontheopensourcecode
ofCodeBERT3usingthesamehyperparametersasintheCodeBERT
paper [9].
5 EXPERIMENTAL RESULTS
5.1 Effectiveness in Cross-Domain Deep Code
Search (RQ1)
Table 3 and 4 show the performance of different approaches in the
cross-domain code search task. We take Python and Java as the
source languages and test the performance on two domain-specific
languages, namely, SQL and Solidity.
Overall, CDCS achieves the best performance among all the
methods. From the results on the SQL dataset, we can see that
3https://github.com/microsoft/CodeBERT
492
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. Cross-Domain Deep Code Search with Meta Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(a) MRR
 (b) Top-1 accuracy
 (c) Top-5 accuracy
 (d) Top-10 accuracy
Figure 5: Performance of CDCS under different training data sizes on the SQL dataset.
(a) MRR
 (b) Top-1 accuracy
 (c) Top-5 accuracy
 (d) Top-10 accuracy
Figure 6: Performance of CDCS under different training data sizes on the Solidity dataset.
CDCS outperforms the baseline models in terms of all metrics,
especiallythetop-1accuracyandMRR,whichareabout11%and
7% greater than the strong baselines, respectively.
The improvement is more significant on the Solidity dataset
(Table 4). We can see that CDCS substantially outperforms strong
baselinesespeciallyinthetop-1accuracyandMRR,whichareabout
20% and 18% stronger, respectively.
There is a large margin between CodeBERT (NL-based) and
CodeBERT(within-domain).Wehypothesizethatthisisbecausethe
SQL corpus is too scarce, so that the pre-training may not provide
sufficientpriorknowledgetothecode-searchmodel.CDCSobtains
more significant improvement against CodeBERT (NL-based) in
SQLthanthatintheSoliditydataset,probablybecauseSQLismuch
closer to natural language than Solidity.
The results demonstrate that CDCS is remarkably effective in
domain-specific code search tasks.
5.2 Effect of Data Size (RQ2)
Figure5and6showtheperformanceofCDCSunderdifferentdata
sizescomparedwiththecross-languageCodeBERT[ 29].Wevary
the size of training data from 0 to full data.
Astheresultshows,CDCSoutperformsthebaselinemodelunder
all data sizes, which supports the significance of the improvement
achievedbyCDCS.Inparticular,wenotethatwhenthedatasize
gets smaller (e.g., <500), the improvement of CDCS against the
baselinemodel becomesmore significant.That meansthatCDCS
isparticularlyeffectiveinscarcedata,indicatingtheoutstanding
ability of CDCS on domain specific languages. By contrast, the
baseline model without meta learning can not adapt to the task
well due to the insufficiency of data.5.3 Performance on other Pre-trained Models
(RQ3)
Table 5: Performance of each method based on GPT-2.
Language Model Acc@1 Acc@5 Acc@10 MRR
SQLNo-Pretraining 0.002 0.010 0.022 0.0124
GPT2 (NL-based) 0.481 0.808 0.889 0.6204GPT2 (within-domain) 0.470 0.785 0.877 0.6088GPT2 (cross-language) 0.447 0.767 0.875 0.5899CDCS
GPTâˆ’2 0.511 0.823 0.905 0.6464
SolidityNo-Pretraining 0.002 0.008 0.014 0.0101GPT2 (NL-based) 0.484 0.751 0.830 0.6079GPT2 (within-domain) 0.487 0.772 0.848 0.6073GPT2 (cross-language) 0.481 0.760 0.827 0.6057CDCS
GPTâˆ’2 0.561 0.781 0.846 0.6607
Weevaluatetheperformanceof CDCSGPTâˆ’2andcompareitwith
baseline models that are also based on GPT-2. We experiment with
(ğ‘ƒğ‘¦ğ‘¡â„ğ‘œğ‘›,ğ½ğ‘ğ‘£ğ‘) asthesourcelanguagesandtesttheperformancein
SolidityandSQL.Thetrainingdiffersalittlebitinthemetalearning
phase: we formulate the input for code search as:
[ğµğ‘‚ğ‘†],ğ‘¤1,...,ğ‘¤ ğ‘,ğ‘1,...,ğ‘ ğ‘š,[ğ¸ğ‘‚ğ‘†]
where[ğµğ‘‚ğ‘†]and[ğ¸ğ‘‚ğ‘†]representtheâ€œbeginningâ€andâ€œendingâ€of
the sequence, respectively. The representation of the [ğ¸ğ‘‚ğ‘†]token
standsforthe aggregatedsequencerepresentationand isusedfor
classification.Weimplement CDCSGPTbased onthe Huggineface
repository1. The hyperparameters are set as follows: we set the
batch size to 44, learning rate to 2.5 ğ‘’-4 [26] which warms up in the
first 1,000 steps and decays according to a cosine curve.
493
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chai, et al.
(a) Batch sizes (SQL)
 (b) Batch sizes (Solidity)
(c) Learning rates (SQL)
 (d) Learning rates (Solidity)
Figure 7: Performance of CDCS under different batch sizes
(a-b) and learning rates (c-d).
Table5showstheperformanceof CDCSGPTâˆ’2comparedagainst
baselinemodels.Clearly, CDCSGPTâˆ’2worksbetterthanallthebase-
line models. The MRR scores of CDCSGPTâˆ’2are about 5% and 10%
greaterthanthoseofthebaselinemodelintheSQLandSoliditylan-
guages, respectively. This affirms the effectiveness of CDCSGPTâˆ’2
when equipped with meta learning.
WenoticethattheGPT-2pre-trainedinnaturallanguagecorpus
shows a comparable performance to ours in the SQL language. We
conjecture that SQL is simple and similar to natural languages,
hencepre-trainingonmassivetextcorpusiseffectiveforthetarget
taskwithoutheavyadaptation.Anothernotablepointweobserveis
that the results of CDCSGPTâˆ’2are lower than thoseof CDCSBERT,
presumably because GPT-2 is a unidirectional language model,
which dynamically estimates the probability of text sequences and
canbemoresuitableforgenerationthansearchtasks.GPT-2pro-
cesseseachinputtextfromlefttorightsequentially,thuscanbelim-
ited in representing context-sensitive features. By contrast, BERT-
stylemodelsaretrainedwithde-noisingstrategies(e.g.,theMLM
task) which enable them to obtain bidirectional, context-sensitive
features.
5.4 ImpactofDifferentHyperparameters(RQ4)
Figure7(a)and7(b)showtheperformanceofCDCSunderdifferent
batchsizesontheSQLandSoliditydatasets.Wevarybatchsizes
to 64, 32, 16 and 8, respectively. The results show that larger batch
sizeshaveslightimpactontheperformance,whilesmallerbatch
sizes have evident effect on the performance.
Figure7(c)and7(d)showtheperformanceofCDCSunderdif-
ferent learning rates on the SQL and Solidity datasets. We vary the
learning rate to 2 ğ‘’-5, 1ğ‘’-5, and 5ğ‘’-6, respectively. As we can see,
the performance is insensitive to learning rates lower than 1 ğ‘’-5.
However,learningratesthatarelargerthan1 ğ‘’-5havesignificant
impacts on performance.Tosumup,theimpactofhyperparametersonCDCSislimited
to a certain range. The performance is sensitive to the hyperpa-
rameters when the batch size is less than 32 or the learning rate is
greater than1 ğ‘’-5. Inaddition, our model ismore sensitive toboth
batch size and learning rate on the Solidity dataset than SQL.
5.5 Case Study
We now provide specific search examples to demonstrate the effec-
tiveness of CDCS in domain specific code search.
Listing 1 and 2 compare the top-1 results for the query â€œwhat
is the smallest city in the USAâ€ returned by CDCS and the cross-language CodeBERT, respectively. The query involves complexsemantics such as the word
smallest . A code search system is
expectedtoassociateâ€œsmallâ€withthecorrespondingSQLkeyword
MIN. They are different but are semantically relevant. Listing 1
shows that CDCS can successfully understand the semantics of
smallest , while the cross-language CodeBERT cannot. The exam-
ple suggests that CDCS is better than the cross-language Code-
BERT [29] in terms of semantic understanding.
Listing3and4showtheresultsreturnedbyCDCSandthecross-
language CodeBERT for the query â€œReset all the balances to 0 and
the state to falseâ€ in the Solidity language. The keywords in thequery are
balances ,state, and false. It can be seen that both
approaches return code snippets that hit some of the keywords.However,thesnippetreturnedbyCDCSisclearlymorerelevant
than that returned by the cross-language CodeBERT. For example,
it explicitly states benificiary.balance=0 andfilled = false
in the source code. On the other hand, the snippet provided by the
cross-languageCodeBERTisvagueinsemantics.Cross-language
CodeBERT may pay more attention to similar words and is limited
in understanding semantics.
TheseexamplesdemonstratethesuperiorityofCDCSincross-
domain code search, affirming the strong ability of learning repre-
sentations at both token and semantic levels.
Listing 1: The first result of query "what is the smallest city
in the USA" returned by CDCS.
SELECT city_name
FROM city
WHERE population = (
SELECT MIN( population )
FROM city
);
Listing 2: The first result of query "what is the smallest city
in the USA" returned by the cross-language CodeBERT.
SELECT population
FROM city
WHERE population = (
SELECT MAX( population )
FROM city
);
Listing 3: The first result of query "Reset all the balances to
0 and the state to false." returned by CDCS.
494
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. Cross-Domain Deep Code Search with Meta Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
contract c8239{
function clean() public
onlyOwner {
for (uint256 i = 0; i < addresses.length; i++)
{
Beneficiary storage beneficiary =
beneficiaries[addresses[i]];
beneficiary.balance = 0;
beneficiary.airdrop = 0;
}filled = false;
airdropped = false;
toVault = 0;emit Cleaned(addresses.length);
}
}
Listing 4: The first result of query "Reset all the balances
to 0 and the state to false." returned by the cross-languageCodeBERT.
contract c281{
function setTransferAgent(address addr, bool state)
external onlyOwner inReleaseState(false){
transferAgents[addr] = state;
}
}
5.6 Summary
Across all the experiments, the performance of the experimental
group using pre-training is better than those without pre-training,
andtheevaluationresultsoftheCDCSexperimentalgroupcom-
binedwithmetalearningarebetterthanthoseonlytrainedwith
pre-training and fine-tuning. These results suggest that both trans-
fer learning (pre-training & fine-tuning) and meta learning have
significant efficacy in deep code search.
Theadvantagesofmetalearningcanbeparticularlyseenfrom
the experimental results of RQ2. The accuracy gap between CDCS
andthebaselinemodelsisbecomingmoresignificantasthedata
size decreases, which means that the size of training data has little
effectonCDCS.Furthermore,theresultsofRQ3suggestthatour
approachcanbegeneralizedtootherpre-trainedmodelssuchas
GPT-2.
Overall,theexperimentalresultssuggestthatCDCShasremark-
able effectiveness in cross-domain code search especially when the
training data is scarce.
6 DISCUSSION
6.1 Why does CDCS work better than the
cross-language CodeBERT?
We believe that the advantage of CDCS mainly comes from the
differencebetweenmetalearningandsimplypre-training&fine
tuning.AsFigure8illustrates,thetraditional pre-training&fine-
tuningparadigm tries to learn the common features of multiple
source languages in the pre-training phase, and directly reuses thepre-trainedparameterstospecifictasksthroughfine-tuning.The
featuresofdifferentsourcelanguagesdistracteachother,leading
toanill-posedrepresentationtobereusedbythe targetlanguage.
By contrast, meta learning employed by CDCS tries to adapt the
pre-trainedparameterstonewtasksduringthelearningprocess,
resultinginrepresentationsthattakeintoaccountallsourcelan-
guages.
Inaviewofmachinelearning,boththe pre-training&fine-tuning
paradigm and meta learning aim to enhance the generalization
abilityofdeepneuralnetworksinmultipletasks.However,inthe
pre-training & fine-tuning paradigm, the model will not obtain task
informationbeforefine-tuningonspecificdownstreamtasks,while
meta learning focuses on learning information in specific tasks
and can enhance the generalization ability of the model. CDCS
successfully combines the two methods.
6.2 Limitations
Althougheffective,werecognizethattheadaptationofmeta-learning
tocodesearchmightnotbeaperfectfit.Meta-learningisusually
used for classification tasks on scarce data [ 10,42], whereas we
adaptittothecontextofcodesearch.Thesetwoconcepts(i.e.,clas-
sification vs. ranking) are not a natural fit. Hence, meta-learningmightnotperfectlysolvetherootproblemofcross-domaincode
search. More adaptations are demanded to fit the two concepts.
In order to efficiently adapt code search tasks to scarce data
scenarios,wefollowtheMAMLpaper[ 10]anddividethedatainto
machine learning â€œtasksâ€, with each task aiming at training a code
search model with small sized data. Such an approach has a fewbenefits.Forexample,itiseasyfortaskadaptationssinceitdoes
notintroduceanylearnedparameters.Furthermore,adaptationcan
beperformedwithanyamountofdatasinceitaimsatproducing
an optimal weight initialization [ 10]. The limitation is that, the
divisionofthedataintoâ€œtasksâ€israndomandthereneedsaconcrete
explanationonhowsplittasksarerelatedtocross-languagecode
search. It remains to investigate how such divisions turn out to be
effective in scarce data.
Another downside of CDCSis that the MAML algorithm it em-
ployscanbringmoretimeandcomputationalcostinthelarge-scaledataset.Differentfromtheconventionalgradientdescentmethods,
MAML needs to compute a meta gradient based on multiple losses
computedfromsub-tasks.Thiscostsextratimeforsavingmodel
parametersandgatheringmetagradients.Forexample,inourex-
periments, it requires around 50% extra hours for meta-learning
comparedto thebaseline models.We leavemore efficienttransfer
learning techniques for future directions.
6.3 Threats to Validity
We have identified the following threats to our approach:
Thenumberofsourcelanguages. Duetotherestrictionofcompu-
tational resources, we only selected two source languages and two
domain-specific target languages. Meta learning with more source
languages could have different results. In our future work, we will
evaluatetheeffectivenessofourapproachwithmoresourceand
target languages.
Theselectionofpre-trainingtasks. TheoriginalCodeBERTuses
twopre-trainingtasks,namely,maskedlanguagemodel(MLM)and
495
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chai, et al.
		 Java
PythonC++SQLJava
PythonC++SQL		
	
				 	
Figure 8: An illustration of the difference between meta learning and simply pre-training & fine-tuning.
replacedtokendetection(RTD)[ 9].However,inourexperiments,
we only use the MLM as the pre-training task. Combining MLM
with RTD may have effects on the results. However, we believe
that the results of the MLM task can stand for the performance
ofpre-trainingbecausetheobjectiveofRTDissimilartoMLMin
that both are based on the idea of de-noising. More importantly,the RTD task requires too much cost of time and computational
resources,whiletheimprovementitbringsismarginalaccording
to the ablation experiments in the CodeBERT paper [ 9]. Moreover,
compared with RTD, the MLM task is more widely used [ 34]i n
domains other than programming languages.
Generalization to other pre-trained models. We have built and
evaluated our approach on top of two pre-trainedmodels,namely,
BERT and GPT-2. Thus, it remains to be verified whether or notthe proposed approach is applicable to other pre-trained models
such as BART [1] and T5 [23, 36].
7 RELATED WORK
7.1 Deep Learning Based Code Search
With the development of deep learning, there is a growing inter-
estinadaptingdeeplearningtocodesearch[ 4,13,19].Themain
idea of deep learning based code search is to map natural and pro-
gramminglanguagesintohigh-dimensionalvectorsusingbi-modal
deep neural networks, and train the model to match code and nat-
urallanguageaccordingtotheirvectorsimilarities.NCS(Neural
Code Search) [ 28] proposed by Facebook learns the embeddings of
code using unsupervised neural networks. Gu et al. [ 13] proposed
CODEnn(Code-DescriptionEmbeddingNeuralNetwork),which
learns the joint embedding of both code and natural language. CO-
DEnn learns code representations by encoding three individual
channels of source code, namely, method names, API sequences,
and code tokens. UNIF [ 4] developed by Facebook can be regarded
as a supervised version of NCS. Similar to CODEnn, UNIF designs
two embedding networks to encode natural and programming lan-
guages,respectively.SemanticCodeSearch(SCS)[ 14]firsttrains
natural language embedding network and programming language
embeddingnetworkrespectivelyandthentrainsthecodesearch
task by integrating the two embedding network with similarityfunction. CodeMatcher [
19], which is inspired by DeepCS [ 13],
combinesquerykeywordswiththeoriginalorderandperformsa
fuzzysearchonmethodnamesandbodies.Zhuetal.[ 45]proposed
OCoR, a code retriever that handles the overlaps between differ-
ent names used by different developers (e.g., â€œmessageâ€ and â€œmsgâ€).Wangetal.[ 35]proposedtoenrichquerysemanticsforcodesearch
with reinforcement learning.
Whilethesemethodsaremainlydesignedforcommonlanguages,
CDCS focuses on domain-specific code search, where training data
isoftenscarceandcostly.CDCSextendspre-trainedmodelswith
metalearningtoextractpriorknowledgefrompopularcommonpro-
gramming language for searching code written in domain-specific
languages.
7.2 Pre-trained Language Models for Code
In recent years, pre-trained language models for source code have
received much attention [ 1,9,23,25]. CodeBERT [ 9], built on
top of the popular model of BERT [ 8], is one of the earliest at-
temptsthat adaptpre-trainedmodels forprogramminglanguages.
CodeBERT is trained with six common programming languages
(Python, Java, JavaScript, PHP, Ruby, and Go). Besides, they cre-
atively proposed the replaced token detection (RTD) task for the
pre-trainingofprogramminglanguage.CoText[ 25]isapre-trained
Transformer model for both natural language and programming
languages. It follows the encoder-decoder architecture proposedby [
33]. PLBART [ 1] learns multilingual representations of pro-
grammingandnaturallanguagejointly.Itextendsthescopeofpre-
training to denoising pre-training, which involves token masking,
deletion,andinfilling.Mastropaoloetal.[ 23]empiricallyinvesti-
gatedhowT5(Text-to-TextTransferTransformer),oneofthestate-
of-the-art PLMs in NLP, can be adapted to support code-related
tasks.Theauthorspre-trainedT5usingadatasetcomposedofEng-
lishtextsandsourcecode,andthenfine-tunedthemodelinfour
code-related tasks such as bug fix and code comment generation.
Although these pre-trained models for source code can be used
forcross-languagecodesearch[ 29]throughpre-traininginmultiple
languages and fine-tuning in the domain-specific language, they
donottakeintoaccount thedifferencebetweensourceandtarget
languages, and are limited in performing domain-specific codesearch. By contrast, CDCS explicitly transfers representations ofmultiple source languages to the target language through meta
learning.
7.3 Transfer Learning for Code Search
Toourknowledge,thereisonlyonepreviousworkthatisclosely
related to ours. Salza et al. [ 29] investigated the effectiveness of
transferlearningforcodesearch.TheybuiltaBERT-basedmodel,
which we refer to as cross-language CodeBERT, to examine how
BERT pre-trained on source code of multiple languages can be
496
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. Cross-Domain Deep Code Search with Meta Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
transferred to code search tasks of another language. Their results
showthatthepre-trainedmodelperformsbetterthanthosewithout
pre-training, and transfer learning is particularly effective in cases
where a large amount of data is available for pre-training while
data for fine-tuning is insufficient [29].
CDCS differs significantly from theirs. We employ a meta learn-
ingalgorithmtoexplicitlyadapttheparametersfromsourcelan-
guagestothetargetdomain,whiletheirworkdirectlyfine-tunes
the pre-trained model in the target language.
8 CONCLUSION
In this paper, we present CDCS, a cross-domain code search ap-
proach that reuses prior knowledge from large corpus of common
languages to domain-specific languages such as SQL and Solid-
ity. CDCS extends pre-trained models such as CodeBERT with
metalearning.Itemploysameta-learningalgorithmnamedMAML
which learns a good initialization of model parameters so that the
model can quickly reach the optimal point in a new task with a
fewdatasamples.ExperimentalresultsshowthatCDCSachieves
significant improvement in domain-specific code search, compared
to â€œpre-training & fine-tuningâ€ counterparts. In the future, we will
investigateourmethodinmorelanguagesandothersoftwareengi-
neering tasks.
Source code and datasets to reproduce our work are available at:
https://github.com/fewshotcdcs/CDCS.
9 ACKNOWLEDGE
This work was sponsored by the National Natural Science Foun-
dation of China under 62102244 and the CCF-Baidu Open Fund
No.2021PP15002000. Xiaodong Gu is the corresponding author.
REFERENCES
[1]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Unified Pre-training for Program Understanding and Generation.
arXiv:2103.06333[cs.CL]
[2]SushilBajracharya,TrungNgo,ErikLinstead,YimengDou,PaulRigor,PierreBaldi, and Cristina Lopes. 2006. Sourcerer: a search engine for open source
code supporting structure-based search. In Companion to the 21st ACM SIGPLAN
symposium on Object-oriented programming systems, languages, and applications.
681â€“682.
[3]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
arXiv:2005.14165[cs.CL]
[4]JoseCambronero,HongyuLi,SeohyunKim,KoushikSen,andSatishChandra.
2019. When deep learning met code search. In Proceedings of the 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering. 964â€“974.
[5]CaseyCasalnuovo,KenjiSagae,andPremDevanbu.2018.StudyingtheDifferenceBetweenNaturalandProgrammingLanguageCorpora. arXiv:1806.02437[cs.CL]
[6]Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric
discriminatively, with application to face verification. In 2005 IEEE Computer
Society Conference on Computer Vision and Pattern Recognition (CVPRâ€™05), Vol. 1.
IEEE, 539â€“546.
[7]PremkumarT.Devanbu.1995. On"Aframeworkforsourcecodesearchusing
program patterns". IEEE Transactions on Software Engineering 21, 12 (1995),
1009â€“1010.
[8]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.
arXiv:1810.04805[cs.CL]
[9]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages.In Proceedings of the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings.
1536â€“1547.
[10]Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on
Machine Learning. PMLR, 1126â€“1135.
[11]WeiFuandTimMenzies.2017. Easyoverhard:Acasestudyondeeplearning.In
Proceedings of the 2017 11th joint meeting on foundations of software engineering.
49â€“60.
[12]Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. 2018.Meta-learning for low-resource neural machine translation. arXiv preprint
arXiv:1808.08437 (2018).
[13]XiaodongGu,HongyuZhang,andSunghunKim.2018. Deepcodesearch.In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE,
933â€“944.
[14]HamelHusainandHo-HsiangWu.2018.Howtocreatenaturallanguagesemantic
search for arbitrary objects with deep learning. Retrieved November 5 (2018),
2019.
[15]DiederikP.KingmaandJimmyBa.2017. Adam:AMethodforStochasticOpti-
mization. arXiv:1412.6980[cs.LG]
[16]Christoph Lange and Michael Kohlhase. 2008. SWIM: A semantic wiki for math-
ematicalknowledge management. In EmergingTechnologiesfor SemanticWork
Environments: Techniques, Methods, and Applications. IGI Global, 47â€“68.
[17]OtÃ¡vio AL Lemos, Adriano C de Paula, Felipe C Zanichelli, and Cristina V Lopes.
2014. Thesaurus-based automatic query expansion for interface-driven code
search. In Proceedings of the 11th working conference on mining software reposito-
ries. 212â€“221.
[18]Wei Li, Haozhe Qin, Shuhan Yan, Beijun Shen, and Yuting Chen. 2020. Learning
Code-Query Interaction for Enhancing Code Searches. In IEEE International
Conference on Software Maintenance and Evolution (ICSME). IEEE, 115â€“126.
[19]Chao Liu, Xin Xia, David Lo, Zhiwei Liu, Ahmed E Hassan, and Shanping Li.
2020. SimplifyingDeep-Learning-BasedModelforCodeSearch. arXivpreprint
arXiv:2005.14373 (2020).
[20]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. RoBERTa:A
Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]
[21]MeiliLu,XiaobingSun,ShaoweiWang,DavidLo,andYucongDuan.2015. Query
expansion via wordnet for effective code search. In 2015 IEEE 22nd International
Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEE,
545â€“549.
[22]FeiLv,HongyuZhang,Jian-GuangLou,ShaoweiWang,DongmeiZhang,and
JianjunZhao.2015.CodeHow:EffectiveCodeSearchBasedonAPIUnderstanding
and Extended Boolean Model (E). In 30th IEEE/ACM International Conference on
Automated Software Engineering (ASE). 260â€“270.
[23]Antonio Mastropaolo, Simone Scalabrino,Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
336â€“347.
[24]Alex Nichol, Joshua Achiam, and John Schulman. 2018. On first-order meta-
learning algorithms. arXiv preprint arXiv:1803.02999 (2018).
[25]LongPhan,HieuTran,DanielLe,HieuNguyen,JamesAnibal,AlecPeltekian,
and YanfangYe.2021. CoTexT:Multi-task Learning with Code-Text Transformer.
arXiv preprint arXiv:2105.08645 (2021).
[26]AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,
et al.2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.
[27]Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings
using siamese BERT-networks. arXiv preprint arXiv:1908.10084 (2019).
[28]Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra.2018.Retrievalonsourcecode:aneuralcodesearch.In Proceedingsofthe
2ndACMSIGPLANInternationalWorkshoponMachineLearningandProgramming
Languages. 31â€“41.
[29]Pasquale Salza, Christoph Schwizer, Jian Gu, and Harald C Gall. 2021. On the Ef-
fectivenessofTransferLearningforCodeSearch. arXivpreprintarXiv:2108.05890
(2021).
[30]JakeSnell,KevinSwersky,andRichardS.Zemel.2017. PrototypicalNetworks
for Few-shot Learning. arXiv:1703.05175 [cs.LG]
[31]Qianru Sun,YaoyaoLiu, Tat-SengChua, andBernt Schiele.2019. Meta-transfer
learning for few-shot learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 403â€“412.
[32]Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M
Hospedales. 2018. Learning to compare: Relation network for few-shot learning.
InProceedingsof theIEEEconferenceon computervisionand patternrecognition.
1199â€“1208.
[33]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
y o un eed .I nAdvances in neural information processing systems. 5998â€“6008.
497
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Chai, et al.
[34]Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak:
BERT as a markov random field language model. arXiv preprint arXiv:1902.04094
(2019).
[35]Chaozheng Wang, Zhenhao Nong, Cuiyun Gao, Zongjie Li, Jichuan Zeng, Zhen-
changXing,andYangLiu.2022. Enrichingquerysemanticsforcodesearchwith
reinforcement learning. Neural Networks 145 (2022), 22â€“32.
[36]YueWang,WeishiWang,ShafiqJoty,andStevenCHHoi.2021.CodeT5:Identifier-
awareUnifiedPre-trainedEncoder-DecoderModelsforCodeUnderstandingandGeneration.In Proceedingsofthe2021ConferenceonEmpiricalMethodsinNatural
Language Processing. 8696â€“8708.
[37]MaximilianWohrerandUweZdun.2018. Smartcontracts:securitypatternsin
theethereumecosystemandsolidity.In InternationalWorkshoponBlockchain
Oriented Software Engineering (IWBOSE). IEEE, 2â€“8.
[38]Shuhan Yan, Hang Yu, Yuting Chen, Beijun Shen, and Lingxiao Jiang. 2020. Are
the Code Snippets What We Are Searching for? A Benchmark and an Empirical
StudyonCodeSearchwithNatural-LanguageQueries.In 27thIEEEInternational
Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE,
344â€“354.
[39]ZhenYang,JackyKeung,XiaoYu,XiaodongGu,ZhengyuanWei,XiaoxueMa,
and Miao Zhang. 2021. A Multi-Modal Transformer-based Code SummarizationApproach for Smart Contracts. arXiv preprint arXiv:2103.07164 (2021).
[40]Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. Coacor: Codeannotation for code retrieval with reinforcement learning. In The World Wide
Web Conference. 2203â€“2214.
[41]XinYe,RazvanBunescu,andChangLiu.2014. Learningtorankrelevantfilesfor
bugreportsusingdomainknowledge.In Proceedingsofthe22ndACMSIGSOFT
International Symposium on Foundations of Software Engineering. 689â€“699.
[42]WenpengYin.2020. Meta-learningforFew-shotNaturalLanguageProcessing:
A Survey. arXiv:2007.09604 [cs.CL]
[43]Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,
James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir
Radev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and
Cross-DomainSemanticParsingandText-to-SQLTask. Proceedingsofthe2018
Conference on Empirical Methods in Natural Language Processing.
[44]Jakub Zakrzewski. 2018. Towards verification of Ethereum smart contracts: aformalization of core of Solidity. In Working Conference on Verified Software:
Theories, Tools, and Experiments. Springer, 229â€“247.
[45]QihaoZhu,ZeyuSun,XiranLiang,YingfeiXiong,andLuZhang.2020. OCoR:
an overlapping-aware code retriever. In 35th IEEE/ACM International Conference
on Automated Software Engineering (ASE). IEEE, 883â€“894.
498
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 07:56:15 UTC from IEEE Xplore.  Restrictions apply. 