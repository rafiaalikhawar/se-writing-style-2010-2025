DeepStability: A Study of Unstable Numerical Methods and
Their Solutions in Deep Learning
Eliska Kloberdanz
Department of Computer Science
Iowa State University
eklober@iastate.eduKyle G. Kloberdanz
Cape Privacy
kyle.g.kloberdanz@gmail.comWei Le
Department of Computer Science
Iowa State University
weile@iastate.edu
ABSTRACT
Deep learning (DL) has become an integral part of solutions to
various important problems, which is why ensuring the quality of
DL systems is essential. One of the challenges of achieving relia-
bility and robustness of DL software is to ensure that algorithmimplementations are numerically stable. DL algorithms require a
large amount and a wide variety of numerical computations. A
naive implementation of numerical computation can lead to errors
thatmayresultinincorrectorinaccuratelearningandresults.A
numericalalgorithmoramathematicalformulacanhaveseveral
implementations thatare mathematically equivalent, buthave dif-
ferent numerical stability properties. Designing numerically stable
algorithmimplementationsischallenging,becauseitrequiresan
interdisciplinary knowledge of software engineering, DL, and nu-
mericalanalysis.Inthispaper,westudytwomatureDLlibraries
PyTorchandTensorflowwiththegoalofidentifyingunstablenu-
merical methods and their solutions. Specifically, we investigate
which DL algorithms are numerically unstable and conduct an
in-depthanalysisoftherootcause,manifestation,andpatchesto
numericalinstabilities.Basedonthesefindings,welaunch DeepSta-
bility, the first database of numerical stability issues and solutions
in DL. Our findings and DeepStability provide future references
todevelopersandtoolbuilderstoprevent,detect,localizeandfix
numerically unstable algorithm implementations.To demonstrate
that, using DeepStability we have located numerical stability issues
in Tensorflow, and submitted a fix which has been accepted and
merged in.
KEYWORDS
numerical stability, deep learning, numerical algorithms
ACM Reference Format:
Eliska Kloberdanz, Kyle G.Kloberdanz, and Wei Le. 2022. DeepStability: A
StudyofUnstableNumericalMethodsandTheirSolutionsinDeepLearning.
In44th International Conference on Software Engineering (ICSE â€™22), May
21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3510003.3510095
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.35100951 INTRODUCTION
Deep learning (DL) has become an integral part of solutions to
variousimportantproblemssuchasnavigationofdriverlesscars,
naturallanguageprocessingforlanguagetranslation,creditcard
fraud detection, or automated trading. Ensuring the quality of a
deep learning system is an essential task. One of the challengesof achieving reliability and robustness of DL is ensuring that al-gorithms implementations are numerically stable . In traditional
numerical analysis literature, numerical stability is treated as a
propertyofalgorithms.Anunstablenumericalmethodproduces
largechangesinoutputsforsmallchangesininputs[ 16],whichcan
lead to unexpected outputs or errors. Especially in the implementa-
tion ofDL, werely on highprecision floatingpoint computations
to reach reliable decisions and need to use large integers to pro-cess very large datasets, which are ubiquitous in practice. As aresult, unstable methods can trigger overflow or underflow and
truncation.Sucherrorsarethenpropagatedthroughiterationsof
training, leading to low quality models and wasting computational
resources. For example, when DL is deployed in autonomous vehi-
cles,incorrectnumericalcomputationscanleadtoincorrectvehicle
trajectory, turning, lane positioning and navigation, resulting in
severe consequences [3, 26, 27].
Implementing numerically stable algorithms is challenging. A
numericalalgorithmoramathematicalformulacanhaveseveral
implementationsthataremathematicallyequivalent,buthavevery
different numerical stability properties. To design a stable imple-
mentation,DLdevelopersneedtohaveanin-depthinterdisciplinaryknowledgeofmathematics,DLalgorithms,numericalanalysis,com-puterprogramming,andfiniteprecisionfloatingpointcomputation
[11]. There are some general guidelines that can be followed to de-
velopnumericallystablealgorithms;however,foreachnumerical
method,weneedtohavespecificsolutionstomitigatenumerical
instability [ 14]. In addition, detecting and diagnosing numerical
stability issues is hard for two reasons. First, similar to security
vulnerabilities,numericalstabilityissuescanbetriggeredonlyby
a special small range of inputs; however, such issues can be con-
sequential, e.g., failedtraining or incorrect predictions ina safety-
criticalDLsystems.Second,numericalstabilityerrorssometimes
occursilentlyandarenotobserveduntiltheypropagatethrough
iterations of training far from the source of instability.
Prior software engineering solutions to numerical instability,
including detection [ 12,20], automated repair [ 28], debugging [ 7],
and increased precision computations [ 2], mostly focus on code
butnotalgorithms.Forexample,[ 1]monitorwhetherrelativeerror
becomesinflatedduringprogramexecutiontodetectcodesegments
withrisksof numericalinstability,and thenautomaticallyswitch
tohighprecisioncomputations.Theyimproveupon[ 2]interms
5862022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Eliska Kloberdanz, Kyle G. Kloberdanz, and Wei Le
ofspeed;however,increasingprecisionisnottheonlyoralways
appropriatesolution.Mathematicalsolutionsthatinvolvecrafting
more numerically stable algorithm implementations can provide
more reliable and speedefficient solutions to numerical instability.
ConsideringincreasingapplicationsofDLinindustry,moreand
more developers will need to implement DL numerical algorithms.
Moretoolsshouldbedevelopedtohelpdetect,test,diagnoseand
repair numerical instability. Numerical stability issues are differ-
ent from any bugs in traditional software. Also, the code patterns,
patchesandrootcausesofnumericalinstabilityareveryspecific
to algorithms, and to the best of our knowledge, such information
of DL stability has not been covered in the numerical analysis liter-
ature.Thus,thegoalofthispaperistodiscoverthestate-of-the-art
knowledge of numerical stability, such as unstable methods and
their solutions, in the domain of DL to support DL developers to
writemorenumericallystablecode.Ourworkstudiednumerical
stability from the DL algorithmic, mathematical and code perspec-
tives, which has not been done in the previous research.
Specifically,inthispaper,wediscoverandanalyzeacomprehen-
sive list of unstable numerical methods used in DL algorithms and
prepare a repository of their solutions. We studied two mature and
importantDLlibraries,PyTorchandTensorflow.Throughanalyzing
their commit histories, we distilled the patches, unit tests and new
featuresrelatedtounstablemethodsandtheirsolutions.Wehave
cataloged the data collected, a total of 252 entries, in a database,
calledDeepStability.Itispubliclyavailable1andwillserveasastart-
ingpoint,wherewecancontinuouslyaddunstablemethodsand
theirsolutionsforreferencesandreuses.Fixingunstableimplemen-
tations can be hard and time-consuming. DeepStability can educate
developers which algorithms/math formulas have numerical issues
and avoid introducing unstable implementations. DeepStability can
alsohelpdiagnoseandfixnumericalstabilitybugs,e.g.,adeveloper
canciteDeepStability intheirpullrequestfornumericalstability
fixes. This way they can both show why the previous version of an
algorithm has numerical stability bugs and the example code for a
fix.Using DeepStability,wehavelocatedanumericalstabilityissue
inTensorflow,and submittedafix2whichhas beenacceptedand
merged in.
Wefoundthatnumericalstabilityissuesindeedwidelyexisted
and were discussed through the DL development process. Using
the data we collected, we investigated what DL algorithms are sus-
ceptibletonumericalinstability(RQ1),whatistherootcauseand
impactof unstablemethods(RQ2), andwhatsolutions fixnumeri-
calinstabilities(RQ3).Fromthereal-worlddataweanalyzed,we
discoverednewnumericalvulnerabilitypatternsthat,tothebest
of our knowledge, have not been reported in the literature.
In summary, the main research contributions of this paper are:
(1)WeclassifiedwhichDLalgorithmsaresusceptibletonumer-
ical instability and explained why; (Â§4.1)
(2)We performed an in-depth analysis of the root cause and
impact of numerical stability bugs in DL algorithms; (Â§4.2)
(3)We summarized both mathematical and code level solutions
for numerically stable DL algorithms; (Â§4.3)
1https://deepstability.github.io/
2https://github.com/tensorflow/tensorflow/pull/50855(4)We discovered new unstable methods and their solutions in
deep learning that are not discussed in prior literature; (Â§5)
(5)WelaunchedDeepStability,thefirstdatabaseofnumerical
stability issues and solutions in DL as a reference for DL
developers and tool builders (https://deepstability.github.io).
2 A MOTIVATING EXAMPLE
In this section, we show an unstable numerical method softmax
andexplainwhymathematicallyequivalentoperationscanhave
different numerical stability properties that can lead to undesirable
outcomes. softmaxiscommonlyusedinvariousmulti-classclassifi-
cationalgorithmssuchaslogisticregression,lineardiscriminant
analysis,naiveBayesclassifier,andartificialneuralnetworks,in-
cludingDNN,CNN,RNNandGAN.Additionally, softmaxisalso
used in reinforcement learning for converting value functions into
action probabilities. This motivating example demonstrates the im-
portance of understanding numerical stability, which is required
for correct implementation of DL algorithms.
2.1 Numerically Unstable Softmax
Softmaxisanormalizedexponentialfunctionthattakesavector
ofğ‘›real values as input and outputs a vector of ğ‘›real values
that represent a probability distribution and sum up to 1. In DL
classifiers,softmaxisusedinthelastneuralnetworklayer,because
it normalizes the output of the prior network layer, a vector of size
ğ‘›, to a probability distribution over ğ‘›predicted output classes.
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(/vecğ‘¥)ğ‘–=ğ‘’ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘—(1)
Thedefinitionof softmaxgiveninEquation1anditsC++imple-
mentationatlines1â€“12inListing1arenumericallyunstable.Whengivenaninputvectorx=[10.0,100.0,1000.0],
ğ‘’100.0andğ‘’1000.0over-
flow and are set to an ğ‘–ğ‘›ğ‘“at lines 6 and 9 . Hence, sumis computed
as22026.5+ğ‘–ğ‘›ğ‘“+ğ‘–ğ‘›ğ‘“=ğ‘–ğ‘›ğ‘“atline6.Asaconsequence, result[j]
returnsğ‘–ğ‘›ğ‘“
ğ‘–ğ‘›ğ‘“=âˆ’ğ‘›ğ‘ğ‘›atline9.Similarly,whengivenaninputvector
y=[-1000.0,-10000.0,-1000000.0], ğ‘’âˆ’1000.0,ğ‘’âˆ’10000.0,andğ‘’âˆ’1000000 .0
underflow and are set to zero at lines 6 and 9. Therefore, sumis
computedas0 +0+0=0atline6.Thisresultsinadividebyzeroon
line9,whichisaninvalidoperationthatyieldsa NaN.Inbothcases
softmaxbecomes undefined and will cease to output meaningful
probabilities.
Listing 1: Unstable and Stable Implementations of Softmax
1 vector< float> softmax_unstable( constvector< float>& x )
2{
3 floatsum = 0;
4 vector< float> result;
5 result.resize(x.size ());
6 for(size_t i = 0; i < x.size(); i++) {
7 sum += exp(x[i ]);
8}
9 for(size_t j = 0; j < x.size(); j++) {
10 result[j] = exp(x[j]) / sum;
11 }12 return result;
13 }
587DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
14 vector< float> softmax_stable( constvector< float>& x )
15 {
16 floatsum = 0;
17 vector< float> result;
18 result.resize(x.size ());
19 floatmax = âˆ—max_element(x.begin(), x.end());
20 for(size_t i = 0; i < x.size(); i++) {
21 sum += exp(x[i] âˆ’ max);
22 }23 for(size_t j = 0; j < x.size(); j++) {
24 result[j] = exp(x[j] âˆ’ max) / sum;
25 }26 return result;
27 }
28 Unstable softmax of x= [10.0,100.0,1000.0]:
29 0, âˆ’nan, âˆ’nan
30 Stable softmax of x= [10.0,100.0,1000.0]:
3 1 0, 0, 13233 Unstable softmax of y=[âˆ’ 1000.0,âˆ’10000.0,âˆ’1000000.0]:
34 âˆ’nan, âˆ’nan, âˆ’nan35 Stable softmax of y=[âˆ’ 1000.0,âˆ’10000.0,âˆ’1000000.0]:
3 6 1, 0, 0
2.2 Error Propagation in DL Algorithms
The overflow and underflow in softmaxcaused by numerical in-
stability propagates throughneural network and causes it tostop
learning. We performed experiments with the unstable softmaxim-
plementationfromListing1onMNISTtodemonstratethat.Givena
fullyconnecteddeepneuralnetwork(DNN)withstandardlearning
parameters but numerically unstable softmaxfunction in its last
layer, we observe that after a couple of training epochs weights,biases, and loss become NaN. The source of this issue are round-
ing errors caused by numerical instability in class probabilities
computed by softmaxforward pass. Given ten different classes, we
observe the following probabilities shown in Listing 2.
Listing 2: Underflow in Class Probabilities due to Numerically Un-
stable Softmax
Class 1: 1.2295200002966873e âˆ’129
Class 2: 0.0 //underflow
Class 3: 1.3695324610698374e âˆ’266
Class 4: 5.966951373841794e âˆ’250
Class 5: 2.5766327266617867eâˆ’89Class 6: 3.4522175477266625e âˆ’234
Class 7: 1.3344020481166367e âˆ’162
Class 8: 1.7656178002771016e âˆ’269
Class 9: 1.0Class 10: 1.616eâˆ’321
Listing 2 indicates that the DNN is very confident that the input
example is of class 9. In fact, it reports that the probability of class
9equals1.0,whiletheprobabilitiesofotherclassesareextremely
small. Due to rounding error, the probability of class 2 underflows
andbecomes0.0.Theoperationthatfollowssoftmaxforwardpassis
softmaxbackwardpass,whichcalculatesthederivativeoflossw.r.t.the softmax output as follows: âˆ’ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’/ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥_ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡. In this
formulağ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’ishot-encoded correctlabels,i.e.:[0 00000001
0],whichrepresentthe10possiblelabelsandidentifyclass9asthe
correctone.Since ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥_ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡forclass2iszero,thiswillcause
adivide-by-zeroin âˆ’ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’/ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥_ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡,aninvalidoperation
that outputs a NaN. As a result, the gradient vector of softmax will
containa NaNwhichwillpropagatethroughthenetworkandcause
NaNsinweightsandbiases. NaNsinweightsandbiaseswillinturn
cause the output of the next forward pass to become a NaN, which
will then cause the loss to became a NaN.
Therefore,asingleerrororiginatingfromanumericallyunsta-
ble implementationof the softmax functioncan create a snowball
effect and prevent the network from learning. Unfortunately, deep
learning APIs such as Keras or PyTorch continue training even
when the network parameters become NaN, which is a waste of
computational resources and the developerâ€™s time.
2.3 Numerically Stable Solution
Tomitigatenumericalstabilityissuesof softmaxdiscussedabove,
we can rewrite the unstable formula in Equation 1 to its mathemat-
ically equivalent, but more numerically stable version shown in
Equation 2 and implemented in C++ on lines 13-24 in Listing 1.
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(/vecğ‘¥)ğ‘–=ğ‘’ğ‘¥ğ‘–âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥)
/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥)(2)
This solution normalizes inputs to ensure that they are not too
largeortoosmall;andtherefore,decreasestheriskofarithmetic
exceptions. Specifically, ğ‘šğ‘ğ‘¥(/vecğ‘¥)returns the largest scalar element
invector /vecğ‘¥.Subtracting ğ‘šğ‘ğ‘¥(/vecğ‘¥)fromeach ğ‘¥ğ‘–elementofvector /vecğ‘¥
implies two properties. First, the largest input ğ‘¥ğ‘–is passed into the
exponential function as a zero. Second, at least one value in the
summation in the denominator is equal to 1, because the largest ğ‘¥ğ‘–
is passedin as ğ‘¥ğ‘–=0 andğ‘’ğ‘¥ğ‘(0)=1. Thefirst property decreases
the risk of overflow and the second one prevents underflow in
thedenominatorthatwouldresultindivisionbyzero,aninvalid
operation. Lines 27 and 30 show that the this stable version of
softmaxyields correctoutputs. Inour appendix,we showthat the
twoversionsofsoftmaxinEquations1and2aremathematically
equivalent.Additionally,theappendixalsocontainsasimilarproof
forlogsoftmax.Priorliteratureshowshowtorewritelogsoftmax
to obtain a numerically stable solution, but does not provide a step
by step proof that the two formulas are mathematically equivalent.
3 STUDY METHODS
3.1 Study Goal and Process
To identify and analyze unstable numerical methods of DL algo-
rithmssimilarto softmaxpresentedinSection2,westudiedPyTorch
andTensorflowcoderepositories.Ourgoalistoinspectcommits
that are related to numerical stability to localize patches, tests,
and any other code additions related to the important numerical
methods in DL. We selected PyTorch and Tensorflow for our study,
becausetheyarethemostwidelyused,well-maintainedandmature
DL libraries [ 19]. Thus, we believe that the problems and solutions
wediscoveredherearerepresentativeandcanbereusedinother
DL implementations.
588ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Eliska Kloberdanz, Kyle G. Kloberdanz, and Wei Le
Our approach for selecting keywords to search PyTorch and
Tensorflowfocusesonfindingnumericalstabilitycommitswhile
avoiding excessive noise (irrelevant commits). We used the key-
wordsfrom[ 11]asareferenceandpreparedalistofkeywordsthat
can indicate the symptoms of numerical stability such as â€œover-
flow, underflow, precision, NaN, infâ€, keywords that indicate when
numerical stability can occur e.g., â€œnumericalâ€, â€œapproximationâ€,
â€œzeroâ€, and keywords that may describe stability issues such as
â€œstable, stabilityâ€. Finally, we used sample search results to further
refinethekeywordsthatcanreturnrelevantcommitsandminimize
noise.
Usingthefollowingkeywordswefound189commitsinPyTorch:
"stability","stable","numerical","approximation","overflow","un-
derflow", "precision", "NaN", "zero", and "inf". These commits were
manuallyanalyzedtoasseswhethertheyrelatetonumericalstabil-
ity,whichreducedthenumberofrelevantcommitsto123.Thesamekeywordsyielded696commitsinTensorflow,asignificantlyhighernumberthaninPyTorch.Weobservedthatthesekeywordsbroughtinmanyirrelevantcommitsthatpollutedthesearchresults.Forex-
ample, "stability" search results typically related to stable software
releases,notnumericalstability.Thus,wefurtherrefinedthekey-
wordsforsearchingTensorflow.Thefollowingkeywordsyielded
307 commits: "numerical stability", "numerically stable", "stable",
"unstable","overflow","underflow".Weanalyzedthesecommitsandfilteredouttheonesthatdidnotrelatetonumericalstability,which
yielded 129 commits. Therefore, the total number of commits in
PyTorch and Tesorflow relevant to numerical stability came to 252.
Wheninspectingthecommits,wefollowamethodologyused
inpriorsoftwareengineeringworksthatstudiednumericalbugs
[11]. The two authors conducted an independent analysis and then
discussedtheirresults.Ifanagreementcouldnotbereachedorbothauthorswerenotconfidentintheiranalysis,commitswereexcluded.
For6/258(2.3%)ofcommits,theauthor(s)hadlowconfidenceon
whatwasthe numericalstabilityissueandhow itwasfixed.That
is, the 252 commits investigated in the paper have 100% agreement.
3.2 Constructing DeepStability
The 252numerical stability commitsare related tomostly patches
(187 commits) and unit tests (38 commits). There are also new
features(8commits)and speedoptimizations(4commits). others
includes cases such as logging and exceptions related to numerical
stability. Among the 252 commits, 137 are related to C/C++, 83 are
related to Python, and 22 are related to CUDA. The rest are mostly
related to mixed languages, e.g., C++ and CUDA.
Weperformedanin-depth analysisofall252commits withthe
goaltoidentifyinstabilitypatterns,rootcauses,theirimpactonDL
algorithms,andtheirsolutionssuchaspatchesandunittests.We
constructed acontinuously growingdatabase called DeepStability
thatdocumentsourdata.Itcontains21columnswiththeimportant
ones being Index, Library, Commit hash, Language, Type of commit,
RootCause,Manifestation/EndUserImpact,IEEEarithmeticexceptiontype,Background,Problem,DLTopics,Patchtypes,OldSolution,New
Solution, and Unit test.
DeepStability is publicly available at https://deepstability.github.
io and can serve as a repository that collects unstable numericalmethodsandtheirsolutionsformachinelearningtoallowdevel-
operstolearnhowtoimplementandfixthesemethods.Weplan
tocontinuouslycontributetoandimprovethisdatabaseinfuture
work.Inthispaper,weourselvesusedthisdataforfurtheranalyses,
classifications and summaries, and answered the three research
questions shown as follows.
4 RESEARCH QUESTIONS AND RESULTS
4.1 RQ1: Which DL algorithms are susceptible
to numerical instability?
Numericalinstabilitiesarehardtodetectonceintroduced.Local-
izing which algorithms are susceptible to numerical instabilitiescan inform developers to be especially careful when implement-ingthesealgorithms.Inaddition,diagnosingDLfailuresisoftenchallenging, as there can be many factors that lead to ineffective
learning, e.g., inadequate dataset, incorrect hyper-parameters or
numerical instability. Moreover, it is hard to pinpoint where nu-
mericalissuesinanalgorithmimplementationoriginatefrom.In
Table 1, we provide a list of DL algorithms and numerical methods
where we found numerical instability. We hope this list can help
developerstonarrowdownwheretoinspectDLcodetodetectand
diagnose numerical instabilities.
As shown in the first column in Table 1, these algorithms and
methodsbelongtoavarietyoftopics,rangingfromwell-knownDLcomponentsofactivationfunctions,lossfunctions,CNNoperations,
optimizersanddataprocessingtolowerlevellearningimplementa-
tions such as tensor math, derivatives, statistical distributions and
linear algebra, as well as performance aware learning that involves
lowprecisioncalculations(i.e.:lessthan32bits)forfasterexecution
such as quantization and other non-standard precision training.
UnderCountand% of Total, we show that tensor math (e.g.:
thecomputationoflog,exp,sumandpowerontensor),statistical
distributions(e.g.:computinglogprobability,sampling,precision
matrix), and dataprocessing (e.g., batch normalization) reportthe
most frequent problems. Please note Table 1 shows only numerical
instabilitieslocatedinDLimplementations,whichaccountedfor
88%. The remaining 12% of commits were numerical stability in DL
implementations but not related to DL algorithms, e.g., overflow
when performing timing.
4.2 RQ2: What is the cause and impact of
numerical instability in DL algorithms?
Severalcommits (e.g.:index55, 61,and 27in DeepStability)in this
dataset are tracked as high priority bugs. Table 2 shows that the
mostcommonerrorsinsoftwarecausedbynumericalinstabilityare
overflow(47%)andlossofprecision(34%)followedbyunderflow
(16%).Therowswheremultipleerrorsarelistedsuchas overflow,
underflow indicatethatnumericalinstabilitycanleadtodifferent
codeerrorsdependingonfailure-inducinginputs.Therearealso
commitsthatamendincorrectcommentsaboutstabilityoroptimize
thespeedofcode related to stability, which we list in N/A.
We observed that in a neural network, loss of precision can
cause inaccurate updates to its weights and biases and therefore,inferior learning. Overflow and underflow produce values that
are equal to inf and 0 respectively, which will cause NaNs in the
589DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: DL Algorithms and Methods Susceptible to Numerical Instability
Topic DL Algorithms and Numerical Methods Count % of Total
Tensor mathsummation, variance, remainder, mean, standard deviation, sum of squares,
log approximation, range, division, power, exponential38 15%
Statistical distributionsGaussian, Binomial, Multivariate normal, Laplace, Gumbel, Gamma,Dirichlet, Poisson, precision matrix, sampling, log probability26 10%
Data processingbatch normalization, parallel training, tensor shape,tensor allocation, image processing22 9%
Quantization quantization aware training, dequantization 17 7%
Linear algebra determinant of a matrix, norms, cosine similarity distance 16 6%
Activation functionsleky relu, softmax, logsoftmax, sigmoid, logsigmoid,spatial logsoftmax, softplus, PRelu13 5%
Non-standard precision training mixed precision, half precision, ultra low precision, precision conversion 11 4%
Derivatives gradients 11 4%
Loss functionsbinary cross entropy loss, cross entropy loss,poisson negative log likelihood loss, logistic loss10 4%
CNN operationsmax pooling, LP Pooling,average pooling,convolution transpose, rotated triangle intersection10 4%
Optimizers SGD, Adagrad, centered RMSprop 6 2%
Other DL operationslinear interpolation, inverse hyperbolic sine, random number generator,bucket sort, computational graph, csiszar divergence, sparse operations,word to vec embedding, external libraries (Caffe2)42 17%
Total 222 88%
Table 2: Errors Caused by Numerical Instability
Errors in code Count % of Total
overflow 118 46.8%
loss of precision 86 34.1%
overflow, underflow 19 7.5%
underflow 16 6.3%
overflow, loss of precision 3 1.2%
underflow, loss of precision 1 0.4%
overflow, underflow, loss of precision 1 0.4%
invalid input 2 0.8%
N/A 6 2.4%
Total 252 100%
modelparameters.Whenthemodelparametersbecome NaN,the
model cannot learn and any further code execution is a waste ofcomputational resources and the software engineerâ€™s time. NaN
outputs should be easy to detect, yet we find that DL APIs such as
Keras continue executing training even when loss and gradients
becomeNaN.
Table 3 shows examples of numerical instability manifestations
wediscovered.Specifically,itshowsinputstovariousalgorithms
thattriggerincorrectorinaccurateoutputs,whicharedemonstratedbycomparingtheactualandexpectedoutputs.Asshowninrow1of
Table 3, an unstable implementation of log determinant of a matrix
outputs -inf given an input matrix with 512 rows and 512 columns
andsmallentriesequalto2e-7,whiletheexpectedcorrectoutput
equals -6718.6489. Row 2 of Table 3 gives an example of a unstable
remaindercalculation,whichyieldsanincorrectresultof128for
2749682432.0modulo36,whichisveryfarfromthecorrectresultof 20. Row 3 shows that a numerically unstable implementationof cosine similarity distance may return a value greater than 1.0,
whichisincorrectbecausecosinesimilarityisdefinedonarange
from-1.0to1.0.Row4showsthatanunstableimplementationof
log probability can yield -inf for binomial distribution initialized
with large logits. The reason is that the intermediate calculation
thatinvolvesmultiplicationoflogitsandnumberofBernoullitrials
overflows and is set to inf. The correct output is 0, because ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ =
90.5229 corresponds to a probability very close to 1 and log(1) = 0.
4.3 RQ3: What solutions are used for handling
numerical instability in DL algorithms?
Our goal here is to discover and summarize solutions of numerical
instability in DL implementations. These solutions can be directly
used by developers to handle similar numerical instabilities and
alsoserveasastartingpointforsolvingnewnumericalinstabilities
in DL.
Weidentifyalistofsolutionpatterns,showninTable4.There
arefourprimarycategoriesofsolutionsforfixingunstableimple-
mentations:(1) rewritingmathformula,(2) increasingprecisionor
change variable type, (3) using a different algorithm and (4)limit-
ing input range. In addition to there four solution types, Table 4also lists mixed precision training, which allows for speeding up
computationally intensive neural network training. The remaining
solutionsinTable4pertaintodetectionsuchasaddingoverflow
checkintoalgorithmimplementationsandaddingorfixingasser-
tions and unit tests. Interestingly, we observe that some assertions,
tests and arithmetic exceptions are ignored as shown in Table 4 as
relaxaccuracytesttolerance,and ignoretest/errormessages.Inthe
590ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Eliska Kloberdanz, Kyle G. Kloberdanz, and Wei Le
Table 3: Examples of Numerical Instability Failure Inducing Inputs and Manifestation
algorithm failure inducing input output expected output
matrix log determinant 512 by 512 matrix with elements equal to 2e-7 -inf -6718.6489
remainder 2749682432.0 % 36 128 20
cosine similarityu = [13.189142, 8.138781, ..., -4.0982385, 5.143065]
v = [13.188879, 8.138888, ..., -4.0983186, 5.1430016]1.0000002 slightly less than 1.0
log probability of binomial distribution logits = 90.5229 -inf 0
followingsubsections,weprovidefurtherdetailsonthetopfour
solutions.
Table 4: Numerical Stability Solution Patterns
Solution Type Count % of Total
rewrite math formula 63 25.00%
increase precision/change variable type 59 23.41%
use a different algorithm 43 17.06%
limit input range 21 8.33%
relax accuracy test tolerance 14 5.56%
add overflow check 14 5.56%
add/fix assertion or unit test 13 5.16%
ignore unit test/exceptions 12 4.76%
mixed precision training 6 2.38%
other 7 2.78%
Total 252 100.00%
4.3.1 Rewriting mathematical formula. We distilled a list of tem-
plates of unstable math formulas and their solutions, shown in
Table 5. We find that three approaches are often used to rewrite
mathematicalformulasforimprovingstability:(1) usingdifferent
operations,(2) re-orderingoperations,or(3) addingasmallepsilon.
Row1inTable5showsanexample,whereamathematicalformula
canberewrittentousedifferentoperationstoimprovenumerical
stability. Square root can suffer from loss of precision for small
inputsandmultiplyingtwovaluesthatsufferfromlossofprecision
yieldsaresultwithevengreaterprecisionloss.Abettersolutionis to avoid that and take a square root of
ğ‘¥2, which should yield
exactly x. Row 2 in Table 5 shows an example, where a different
order of operations can improve numerical stability. Instead of sub-
tracting the sum of max and log(y) from x, we should first subtract
max from x and then log(y) to avoid subtraction of two numbers
withverydifferentmagnitudesthatleadstolossofsignificantdigits.
Row 3 in Table 5 is an example, where adding a small epsilon tothe input value of log prevents invalid operation log(0), which is
undefined and results in an arithmetic exception. Deriving various
mathematicallyequivalentformulastofindanumericallystableso-lutioncanbechallenging,whichwedemonstratewiththeexample
below discovered in this study.Example 5.1 [Synchronized Batch Normalization]
Synchro-
nizedbatchnormalization(SyncBN)isatypeofbatchnormalization
usedforparalleledneuralnetworktrainingthatutilizesmultiple
GPUs, where each mini-batch of data is divided across multipleGPUsthatcalculatethegradientsusedforupdatingweightsand
biases. Batch normalization is a form of data processing that scales
inputs to conform to the standard normal distribution that has amean of 0 and a standard deviation of 1. It enables faster and more
stable training of DNNs [ 23]. Standard batch normalization only
normalizesthedatawithineachGPU,whileSyncBNnormalizesthe
inputswithinthewholemini-batchasopposedtodifferentmini-
batchsubsetsthatresideondifferentGPUs.SyncBNisdefinedin
Equation 3 [ 15], whereğ›¾andğ›½are learnable parameter vectors
of length equal to the input size, and are initialized to uniform
distribution on (0, 1) interval and 0 respectively.
ğ‘¥âˆ’ğ¸[ğ‘¥]/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–âˆ—ğ›¾+ğ›½ (3)
Inourstudy,weobservedanumericallyunstableimplementation
showninEquation4.Thisimplementationlosesprecisiondueto
theflooroperation,andisalsoslowduetoperformingunnecessary
power and log computations, which are expensive.
2ğ‘“ğ‘™ğ‘œğ‘œğ‘Ÿ(ğ‘™ğ‘œğ‘”2(1+(ğ‘¥âˆ’ğ¸[ğ‘¥])âˆ’1âˆš
ğ‘‰ğ‘ ğ‘Ÿ[ğ‘¥]+ğœ–))
(4)
MorenumericallystableandfastersolutionisshowninEquation
5,whichisequivalenttoEquation3for ğ›¾=1andğ›½=1âˆ’1âˆš
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–.
Proof.
ğ‘¥âˆ’ğ¸[ğ‘¥]+/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–âˆ’1/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–
=ğ‘¥âˆ’ğ¸[ğ‘¥]/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–+/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–âˆ’1/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–
=ğ‘¥âˆ’ğ¸[ğ‘¥]/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–+1âˆ’1/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–
/square
ğ‘¥âˆ’ğ¸[ğ‘¥]+/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–âˆ’1/radicalbig
ğ‘‰ğ‘ğ‘Ÿ[ğ‘¥]+ğœ–(5)
4.3.2 Increaseprecisionorchangevariabletype. Riskofoverflow
and underflow can be mitigated by increasing variable precision or
changingitstype.Weoftenseethatnumericalstabilityisincreased
bychangingafloattoadoubleandanint32toint64.Changinga
variabletypefromsignedinttoanunsignedintincreasesprecision
andalsopreventsundefinedbehaviorofsignedintegeroverflowor
underflow. In Table 6, we present 9 concrete examples in different
scenariosofDLimplementations.Under Patch,weprovideadiverse
setoftypechangesthathavebeenutilizedtofixnumericalstability
in DL. The first row in Table 6 shows an example of increasingthe precision of intermediate calculations from float16 to float32to prevent NaN and inf gradients in character embedding that is
used in NLP. Row 2 is another NLP example, were the precision of
a variable that holds the text corpus size is increased from int32 to
591DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 5: Rewriting Mathematical Formulas to Improve Numerical Stability
Vulnerability template Patch template Fix Impact
x / (sqrt(x) âˆ—sqrt(x)) x / sqrt(x âˆ—x) use different operations loss of precision, or incorrect result
x - (max + log(y)) x - max -log(y) rewrite order of operations loss of significant digits
x-yâˆ—log(x) x - yâˆ—log(x+epsilon) add small epsilon to log invalid operation i fx=0
x * y/ (z*z) x * (y/z)/z use different operations overflow/underflow if z large/small
x + epsilon + ğ‘¦2x+ğ‘¦2+ epsilon rewrite order of operations underflow not prevented
( x+y-1 )/t ( x-1 )/y+1 r e write order of operations overflow if x is large
(x + y) / 2 x + (y - x) / 2 rewrite formula overflow
(8âˆ—xâˆ—y+3 1 )/3 2âˆ— 4( x âˆ—y+3 )/4âˆ— 4 rewrite formula overflow
int64 to prevent overflow of large text files. Row 3 shows an exam-
ple,wheretheaccuracyof2Dconvolutioninquantizationaware
training is improved by increasing the precision of all variables in
the calculation from float to double, i.e. from 32 bits to 64 bits.
4.3.3 Useadifferentalgorithm. Numericalinstabilitycanbeallevi-
ated by solving a problem with a different algorithm. This patch
can involve performing intermediate steps or the entire solution
withadifferentalgorithm.WesummarizeourfindingsinTable7
regarding algorithm choices for a set of common DL operations.
The following Example 5.2 below refers to row three of Table 7.
Example 5.2 [Gradient Approximation in Gamma Distribu-tion]
Thegammadistributionisacontinuousdistributionthatis
parametrizedwithashapeparameter ğ›¼andrateparameter ğ›½,which
determinetheshapeandrangeofthedistributionrespectively.Therateparameter
ğ›½determinestherangeofthedistributionintermsof
howstretchedorcompresseditis.Thegradientwrtto ğ›¼hasnoan-
alyticalform,whichiswhyitneedstobeapproximated.Dependingonthemagnitude
ğ›¼andinputğ‘¥,differenttechniquesshouldbeused
to achieve high precision gradient approximation [ 17]. For a small
ğ›¼<<1, asymptotic approximation yields an accurate gradient,
but for a large ğ›¼>>1 Riceâ€™s saddle point expansion [ 21] should
be used. For small inputs ğ‘¥, Taylor series instead of asymptotic
approximation should be used.
4.3.4 Limitinputrange. Numericalinstabilitycanbepreventedby
addingbounds-check, i.e., imposing restrictions on what maximum
and minimum inputs are allowed to perform this computation.Example5.3[UniformDistributionNumberGenerator]
Uni-
form distribution has a constant probability and is defined by an
interval[ğ‘,ğ‘],whereğ‘istheminimumand ğ‘isthemaximumvalue
that can occur. Generating random uniform numbers between -0.9
and 1.0 leads to creating denormalized numbers [24], which are
very small numbers that are close to 0, and must be represented
with a zero exponent and a mantissa whose leading bit(s) are zero.
Denormal number computations are not only slower, but also lead
to loss of precision and therefore, a risk of underflow. To avoid
that, random uniform numbers should be generated on an interval
between 1 and 1.125, which produces a normalized range of values.
5 NEWLY DISCOVERED UNSTABLE
METHODS AND THEIR SOLUTIONS IN DL
We select several interesting newly discovered numerical instabili-
tiesanddiscussthedetailsoftheirproblemsandsolutionsalongwithcontributionstoexistingliterature.Therestcanbefoundin
DeepStability at https://deepstability.github.io.
5.1 Cosine Similarity
Cosine similarity (Index 4 in DeepStability) is a measure of theangle between two non-zero vectors and therefore, it represents
howsimilararethedirectionsofthetwovectors.Twocosinevectors
thathavethesamedirection(regardlessoftheirmagnitude)have
an angle of 0 degrees between them and therefore have a cosinesimilarity of 1. If the vectors are pointing in opposite directions,a180degreesangle,theircosinesimilarityis-1.Andfinally,two
orthogonal vectors sharing an angle of 90 degrees have a cosine
similarity of 0. Therefore, cosine similarity is defined as follows:
ğ‘ğ‘œğ‘ _ğ‘ ğ‘–ğ‘š=ğ‘ğ‘œğ‘ (ğœƒ)=/vecğ‘¢âˆ—/vecğ‘£
/bardblğ‘¢/bardblâˆ—/bardblğ‘£/bardbl=/summationtext.1ğ‘
ğ‘–=1ğ‘¢ğ‘–âˆ—ğ‘£ğ‘–/radicalBig/summationtext.1ğ‘
ğ‘–=1ğ‘¢2
ğ‘–âˆ—/radicalBig/summationtext.1ğ‘
ğ‘–=1ğ‘£2
ğ‘–(6)
Inourstudy,weidentifiedthenumericallystableandunstable
versionsofcosinesimilarity(toourbestknowledge,thereisnoprior
literature that discusses numerical instability of cosine similarity),
showninAlgorithm1.Lines4-7leadwithâ€™+â€™inblueindicatestable
code and lines 8â€“10 with â€™-â€™ in red show unstable code. The root
cause of instability is the inverse square root operation at line 8
andshowninEquation7.Thefollowingmathematicalformulasare
mathematically identical, but Equation 7 is less numerically stable
than Equation 8.
ğ‘¥âˆ—1âˆšğ‘¦âˆ—ğ‘§(7)
ğ‘¥âˆšğ‘¦âˆ—ğ‘§(8)
A numerically unstable implementation of cosine similarity dis-
tance may return a value greater than 1.0, which is incorrect, be-cause cosine similarity can only range from -1.0 to 1.0. Cosinesimilarity is used, for example, in NLP for measuring similarity
between vector representations of text for document classification.
Given two documents, a cosine similarity of 1 implies that they
are precisely the same and a cosine similarity of 0 means that they
are completely different. An unstable implementation of cosine
similarity that yields wrong results can therefore lead to incorrect
text classification.
592ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Eliska Kloberdanz, Kyle G. Kloberdanz, and Wei Le
Table 6: Changing Variable types or Increasing Precision to Improve Numerical Stability
Algorithm Operation Problem Patch
NLP character embeddingduring FP16 training, character embedding
weights receive NAN or INF gradientsUse a float32 for intermediate resultswhen the input is float16
NLP Word2Vec embedding text files larger than 2B words overflowsincrease precision of corpus sizefrom int32 to int64
quantizationaware training2D covolutionbackward pass output in quantization awaretraining is not accurate enoughIncrease precision of all variables fromfloat to double
random numbergeneratorrange signed integer overflow of variable rangechange type of variable range from signedto unsigned int 64 bits
summation index for loop index overflow if input vector is large increase precision from int to int 64
flatten layer size overflow of flattened layer tensorincrease precision of variable shapefrom int32 to int64
statistics mean overflow of sum in mean calculation Upcast int8, int16, int32 into int64
tensor math divisiondivision, where the denominator is a lowprecision scalar has a risk of underflowReplace the type used for accumulationto the same type as the operands
optimizer parameter sizethe size of iterable that holds model parametershas a risk of overflowchange int to size_t
Table 7: Using Different Algorithms to Improve Numerical Stability
DL operation Numerically unstable algorithm Numerically stable algorithm
matrix inverse direct matrix inverse Cholesky inverse
variance, standard deviationnaive algorithm based on variance definition,two-pass algorithmWelfordâ€™s algorithm
gradient approximation asymptotic approximation Taylor series expansion, Rice saddle point expansion
summation sum in any order sum from smallest to largest
statistical distributions parametrize with probabilities parametrize with logits
statistical distributions compute determinant compute log determinant
loss and sigmoid compute loss then apply sigmoid combinine sigmoid and BCE loss into one layer
Algorithm1: Numerically Stable vs Unstable Cosine Simi-
larity Algorithm
Input:/vecğ‘¢,/vecğ‘£, espilon
Output:cosine similarity of /vecğ‘¢and/vecğ‘£
1ğ‘¥=ğ‘ ğ‘¢ğ‘š(/vecğ‘¢âˆ—/vecğ‘£)
2ğ‘¦=ğ‘ ğ‘¢ğ‘š(/vecğ‘¢âˆ—/vecğ‘¢)
3ğ‘§=ğ‘ ğ‘¢ğ‘š(/vecğ‘£âˆ—/vecğ‘£)
4+ğ‘›=ğ‘¦âˆ—ğ‘§
5+ clamp n to ensure n >=(epsilon * epsilon)
6+ğ‘›=ğ‘ ğ‘ğ‘Ÿğ‘¡(ğ‘›)
7+ğ‘Ÿğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ =ğ‘¥/ğ‘›
8-ğ‘›=1/(ğ‘ ğ‘ğ‘Ÿğ‘¡(ğ‘¦âˆ—ğ‘§))
9- clamp n to ensure n <=(1.0/epsilon)
10-ğ‘Ÿğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ =ğ‘¥âˆ—ğ‘›
11returnresult
5.2 Bucketization Algorithm
Bucketization algorithm (Index 28 in DeepStability) categorizes in-
putsbasedonboundaries,e.g.:forboundaries ğ‘=[0,10,100]andin-
putğ‘¥=[[âˆ’5,10000][150,10][5,100]],theoutputis [[0,3][3,2][1,3]].
Bucketization algorithm leverages binary search, which can causenumericalinstability.Binarysearchisasearchalgorithmthatfinds
the position of a target value within a sorted array by iteratively
comparing the target value and the middle element of the array to
cutdownthesearchspaceinhalfeachtimeuntilthetargetvalue
isfound.ThemidpointcanbecalculatedusingEquation9,where
L is the left index initialized to 0 and R is the right index initialized
to N-1, where N is the size of the input array.
ğ‘šğ‘–ğ‘‘ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ =(ğ¿+ğ‘…)/2 (9)
If N is very large, adding L and R can result in overflow. A more
numerically stable solution is Equation 10.
ğ‘šğ‘–ğ‘‘ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ =ğ¿+((ğ‘…âˆ’ğ¿)/2) (10)
Equations 9 and 10 are mathematically equivalent:
Proof.
ğ¿+ğ‘…
2=ğ¿+ğ‘…âˆ’ğ¿
2=2ğ¿+ğ‘…âˆ’ğ¿
2=ğ¿+ğ‘…
2
/square
Equation 10 mitigates the risk of overflow for large values of R
and L, because the result of R-L will not be a larger value than R or
L. Adding R and L can result in overflow even if R and L are within
representable precision range. For example, given a sorted array
593DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
between 1 and 2âˆ’31âˆ’1 and a target value of 2âˆ’31âˆ’1, the unstable
search errors out with a segmentation fault in C++ (an integer
overflowonanindexcancauseareadorwriteoutsideofbounds
of an array, which triggers a segmentation fault). The stable search
correctlyoutputsthatthetargetvalueislocatedinthe2147483646ğ‘¡â„
position in the array. A numerically unstable implementation of
Bucketizationalgorithmmay notbeabletooutput anyresultand
yield an error instead.
Bucketization can be used in feature engineering for transform-
ing numerical features into categorical ones. For example, suppose
thatwearecreatinganeuralnetworkthatpredictshouseprices,and
one of the features are the GPS coordinates. We can leverage buck-
etizationtocreateacategoricalfeaturethatbinsallobservations
into buckets based on defined boundaries. This can boost model
accuracy and allows for reasoning about the relationship between
the house location and price. Since the unstable implementation
cannot output binned values for certain inputs as discussed above,
it can decrease the availability or quality of pre-processed input
features.Toourbestknowledge,thereisnoliteraturethatidenti-
fiesnumericalstabilityvulnerabilitiesinBucketizationalgorithm.
UsingDeepStability, we have located a numerical stability issue
in a binary search implementation in Tensorflow, and submitted a
fix3which has been accepted andmerged in. This new numerical
stability issue was found via the same process that we envision for
developerstousetobenefitfrom DeepStability.Weobservedafix
in PyTorch that involved rewriting the binary search algorithm to
improveitsnumericalstability.Weanalyzedtheissueandsolution,
andrecordedthemin DeepStability asentry28.Wethenchecked
the implementation of binary search in Tensorflow and found that
itis numericallyunstable.Using thesolutionrecorded in DeepSta-
bility, we submitted a pull request to Tensorflow with a fix and
explanation obtained from DeepStability.
5.3 Differentiation of the LU Decomposition
Differentiation of the LU decomposition (i.e.: backward pass of LU
decomposition) computes the gradient of matrix ğ´in the LU de-
composition (Index 2 in DeepStability), which can be numerically
unstable.LU(lowerâ€“upper)decomposition(alsocalledLUfactoriza-
tion) factors a matrix ğ´as the product of a lower triangular matrix
ğ¿and an upper triangular matrix ğ‘ˆ. The elements in the lower
triangular matrix ğ¿that lie above the diagonal are zero, while in
theuppertriangularmatrixitistheelementsbelowthediagonal
are zero. LU decomposition is an efficient method used for solving
a system of linear equations.
Differentiation of the LU decomposition requires division by
matrixğ¿andğ‘ˆ. Algorithm 2 is numerically unstable, because it
reliesonaninvertingthe ğ¿andğ‘ˆmatricestoperformthatdivision.
We discovered a more numerically stable solution from our data
shown in Algorithm 3. It replaces the inverse of matrix L and U
withsolutionstosystemsoftriangularequations.Asystemoftrian-
gularequationshastheformofatriangle,becauselowerequations
alwayscontainvariablesfromtheequationabove,exceptforthe
first variable, e.g.: 5x + 4y = 0, 10y - 3z = 11,z=3 .
Theimpactoftheunstablesolutionisinaccurategradientoutput
ofdifferentiationoftheLUdecomposition.Matrixdecomposition
3https://github.com/tensorflow/tensorflow/pull/50855Algorithm 2: LU Backward Numerically Unstable
Input:L, U, P, LU gradient, pivots gradient
Output:gradient of A
1Create an identity matrix I with shape same as LU_gradient
2ğ¿_ğ‘–ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘’ =(ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ _ğ‘ ğ‘œğ‘™ğ‘£ğ‘’(ğ¼,ğ¿))ğ‘‡// unstable [14]
3ğ‘ˆ_ğ‘–ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘’ =(ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ _ğ‘ ğ‘œğ‘™ğ‘£ğ‘’(ğ¼,ğ‘ˆ))ğ‘‡// unstable [14]
4ğœ™ğ¿=ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ_ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ (ğ¿ğ‘‡âˆ—ğ¿ğ‘ˆ_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡)
5Fill diagonal of ğœ™ğ¿matrix with 0 ğ‘ 
6ğœ™ğ‘ˆ=ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ (ğ¿ğ‘ˆ_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ âˆ—ğ‘ˆğ‘‡)
7ğ‘”ğ‘Ÿğ‘ğ‘‘_ğ‘ğ‘’ğ‘Ÿğ‘¡ğ‘¢ğ‘Ÿğ‘ğ‘’ğ‘‘ =ğ¿_ğ‘–ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘’âˆ—(ğœ™ğ¿+ğœ™ğ‘ˆ)âˆ—ğ‘ˆ_ğ‘–ğ‘›ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘’
8returnP * grad_perturbed
Algorithm 3: LU Backward Numerically Stable
Input:L, U, P, LU gradient, pivots gradient
Output:gradient of A
1ğœ™ğ¿=ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ_ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ (ğ‘ğ‘œğ‘›ğ‘—ğ‘¢ğ‘”ğ‘ğ‘¡ğ‘’ (ğ¿ğ‘‡)âˆ—ğ¿ğ‘ˆ_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡)
2Fill diagonal of ğœ™ğ¿matrix with 0 ğ‘ 
3ğœ™ğ‘ˆ=ğ‘¢ğ‘ğ‘ğ‘’ğ‘Ÿ_ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ (ğ‘ğ‘œğ‘›ğ‘—ğ‘¢ğ‘”ğ‘ğ‘¡ğ‘’ (ğ¿ğ‘ˆ_ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ âˆ—ğ‘ˆğ‘‡))
4ğœ™=ğœ™ğ¿+ğœ™ğ‘ˆ
5ğ‘‹=ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ _ğ‘ ğ‘œğ‘™ğ‘£ğ‘’(ğœ™,ğ‘ğ‘œğ‘›ğ‘—ğ‘¢ğ‘”ğ‘ğ‘¡ğ‘’ (ğ¿ğ‘‡))
6ğ´_ğ‘”ğ‘Ÿğ‘ğ‘‘ =
ğ‘ğ‘œğ‘›ğ‘—ğ‘¢ğ‘”ğ‘ğ‘¡ğ‘’ ((ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ _ğ‘ ğ‘œğ‘™ğ‘£ğ‘’(ğ‘ğ‘œğ‘›ğ‘—ğ‘¢ğ‘”ğ‘ğ‘¡ğ‘’ (ğ‘‹ğ‘‡)âˆ—ğ‘ƒğ‘‡,ğ‘ˆ))ğ‘‡)
7returnA_grad
is used in image processing for separating the background and
foreground of an image or image denoising [ 10]. In recommender
systems such as Netflix it can be used for collaborative filtering
[18]thatanalyzesrelationshipsbetweencustomersandproductsto
identifynewassociations.Inthosesystemsmatrixdecomposition
canbeusedtocharacterizecustomersandproductsbyvectorsof
factors,whichareusedtomakerecommendations.Therefore,an
inaccurate backward pass of matrix decomposition can lead to, for
example, wrong recommendations and customer dissatisfaction.Priorliteraturediscussesthenumericalinstabilityofamatrixin-
verse,butdoesnotidentifyorprovideasolutionforcalculatingthe
gradient for LU decomposition.
5.4 Higher Order Derivatives
Higher order derivatives (e.g.: in natural gradient descent) perform
more than one order of differentiation and can involve division.
Divisionbyalargeorsmallvaluethatissquared(Index3inDeepSta-
bility) can lead to inaccurate results or even overflow or underflow.
In the context of higher order derivatives, the formula in Equation
11isappliedmultipletimes.Theissueisthatcalculationofa ğ‘›ğ‘¡â„
order derivative raises ğ‘¦to the power of 2ğ‘›, i.e.:ğ‘¦2becomesğ‘¦2ğ‘›.I f
ğ‘¦is large or small ğ‘¦2ğ‘›can overflow or underflow very quickly.
âˆ’ğ‘”ğ‘Ÿğ‘ğ‘‘âˆ—ğ‘¥
ğ‘¦âˆ—ğ‘¦(11)
âˆ’ğ‘”ğ‘Ÿğ‘ğ‘‘âˆ—ğ‘¥
ğ‘¦
ğ‘¦(12)
Insteadofdividingby ğ‘¦âˆ—ğ‘¦,wecandivide by ğ‘¦twiceasshownin
Equation 12. Mathematically ğ‘¥/ğ‘¦2=ğ‘¥/ğ‘¦/ğ‘¦, but if y is a large finite
594ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Eliska Kloberdanz, Kyle G. Kloberdanz, and Wei Le
precisionfloatingpointnumber,thenbyperforming ğ‘¦2youmay
lose precision. Successive divisions achieves the same result while
not losing as much precision for large values of y. The impact of
the unstable solution is division by inf or zero for large or small
valuesofğ‘¦respectively.Theresultofdivisionby infandzerois
a zero and NaNrespectively, which will propagate in the neural
networkâ€™s gradients, weights, biases, and loss and the network will
cease learning.
Higherorderderivativesareusedinnaturalgradientdescent[ 22]
and also in quantum neural networks [ 6]. [4] offers a theoretical
discussion of numerical stability of higher order derivatives ofanalytic functions. We provide a practical example of numerical
instabilityand solutionfor higherorder derivativescomputations.
Wefindthatsuccessivedivisionsbylargeorsmallvaluesthatare
squared is numerically unstable.
6 THREATS TO VALIDITY
The primary potential internal threat to validity is the correctness
of the analysis of individual commits regarding numerical stability
in DL libraries. To mitigate this threat, an independent analysis
followed by a discussion was conducted by two of the authors and
only items that were fully agreed upon were included.
A potential external threat to validity that our findings may
not be representative of all numerical stability vulnerabilities in
real-worldapplications,becauseweonlystudiedcommitsintwo
open-source DL libraries PyTorch and Tensorflow. However, Deep-
Stabilityisastartingpointthatcanserveasagrowingrepositoryto
continuously record additionalnumerical stability vulnerabilities
and solutions shared by developers and tool builders.
7 RELATED WORK
Due to the interdisciplinary nature of the subject, we found thatsoftware engineering, numerical analysis and machine learning
fields all have developed relevant work.
Numerical bugs and analysis in software [11] conducted an
empirical study of numerical bugs in numerical software libraries:
NumPy, SciPy, LAPACK, GNU Scientific Library, and Elemental.
They classified four categories of numerical bugs: (1) Accuracy, (2)
Special values, (3) Convergence, (4) Correctness. They reported
thatthemostcommonnumericalbugtypeiscorrectnessandthe
most common symptom is wrong result followed by crash and
bad performance. Our work studied numerical stability from analgorithm point of view, and focused more on specific unstablenumerical methods and their solutions and less on bug statistics.[
11] mention the importance of in-depth domain knowledge for
detailed numerical bug analyses, which our work provided.
There have been a set of work on managing numerical errors in
software.[ 12]proposedautomatedbackwarderroranalysistech-
niques for numerical code. [ 1] developed on-the-fly monitoring
techniquethatcanpredictifanexecutionofafloatingpointpro-
gramisstable.[ 20]introducedRAIVE,atoolfordetectinginstability
via identifying output variations of floating point executions. [ 28]
presented an automated approach to repair floating point errors in
numerical libraries via an empirical study of the GSL - GNU scien-
tific library. Their proposed approach involves three steps: error
detectionusingtheconditionnumber,approximationextraction,andrepairgeneration.[ 7]introducedatoolfordebuggingerrorsin
programsusingpositrepresentation,analternativetofloatingpoint
representation with diminishing accuracy. We hope the detailed
numerical stability knowledge we discovered and presented in this
work can help improve the above tools.
Numerical analysis Prior research in numerical analysis fo-
cuses on theoretical aspects of numerical stability, but does not
specificallytargetDL.[ 14]isaverycomprehensivereferencefor
thebehaviorofnumericalalgorithmsinfiniteprecision.Itcovers
algorithmicderivations,perturbationtheory,androundingerror
analysis, which are relevantto both numerical analysis specialists
andcomputationalscientists.[ 25]studiednumericalstabilityofiter-
ative methods in matrix computations such as Jacobi, Gauss-Seidel,
and SOR. And [ 5] defined notions of stability for learning algo-
rithms and show how to use these notions to derive generalization
error bounds based on empirical and leave-one-out error.
Neural networks The importance of numerical stability has
beenmentionedinmachinelearningtextbookssuchas[ 13](e.g.,
thisbookdiscussed softmax),[ 9]analyzeneuralnetworksfroma
viewpoint of mathematics and numerical computation and provide
verycomprehensivebackgroundinformation.Theyarguethatneu-
ral networks are not very numerically stable and use adversarial
examples, inputs with very small carefully crafted perturbationsthat fool the neural network, as evidence to support that claim.
[8] studies nonlinear methods of approximation and the effects of
requiring numerical stability.
8 CONCLUSIONS AND FUTURE WORK
In this paper, we study numerical stability of algorithms and math-
ematical methods used in deep learning (DL). By analyzing 252
numerical stability commits obtained from PyTorch and Tensor-
Flow, we discovered numerical instabilities in DL methods and
their solutions that previous research has not discussed before. We
constructed DeepStability, the first database that catalogs unstable
methods,theiranalysesandsolutionsforfuturereuse.Weidenti-
fied alist of vulnerableDL algorithms ranging fromwell-known
DL components such as activation functions, loss functions, CNN
operations, optimizers and data processing to lower level learning
implementationssuchastensormathsandstatisticaldistribution
computationsandquantization.Wefoundthatnumericalinstability
can lead to overflow or underflow and loss of precision depending
the input that triggers the vulnerability. These errors impact DL
through incorrect or inaccurate results and learning, which lead to
unreliable DL models. We provide example inputs that can trigger
numerical instability manifestations and analyze the reasons for
numerical instability. Finally, we discover and document solutions
for numerical instabilities in DL. These include, but are not limited
to:rewritingmathematicalformulas,increasingprecisionorchang-
ingvariabletypes,usingadifferentalgorithm,andlimitinginput
range. In the future, we plan to continue to grow DeepStability
andalsoimplementawebportaltoencourageopen-sourcestyle
contributions to it.
9 SIGNIFICANCE OF CONTRIBUTIONS
Numerical stability is very important for robustness and reliability
of deep learning, but it is a very hard problem. We have found
595DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
numerous reports and fixes of numerical stability vulnerabilities
inPyTorchandTensorflow,twoverymatureDLlibraries,which
showsthatnumericalstabilityisanimportantissueinDL.Given
the increasing demand for DL systems and their use in practice,
we need developers to have sufficient knowledge and awareness to
prevent, detect, diagnose and fix numerical instabilities.
Despiteitsimportance,numericalstabilityinDLhasnotbeensuf-
ficiently researched due to its challenging interdisciplinary nature.
Softwareengineering(SE)researchstudiednumericalbugs,butnot
numericalinstabilityinthecontextofDLalgorithms.Numericalanalysis works focus on theoretical analysis of maths formulas,but not on code level implementations, patches, failure inducing
inputs and unit tests. We observe that machine learning scientists
handlenumericalstabilityonacase-by-casebasis,butdonotaim
to consolidate and analyze vulnerability patterns and solutions for
reuse.
Our work explained and reported numerical instability in DL
algorithms,providedpatches,unittests,failure-inducinginput,and
math templates for reuse. We hope that by connecting the three
domains, we can enable interesting future research and benefit the
three domains. For example, SE researchers can use our findings to
designprogram analysesand toolstargetingnumerically unstable
computations.
SimilartosecurityvulnerabilitywebsitesofNVD4andCVE5,
wecreated DeepStability withthegoalofcontinuouslydocument-
ing numerical stability issues and solutions for future research and
for helping developers and tool builders to prevent, detect, localize
and fix numerically unstable algorithm implementations. Using
DeepStability, we ourselves have found and fixed a stability vulner-
ability in TensorFlow, and our patch has been accepted by the Ten-
sorFlowteam.Webelievethatthat DeepStability canbenefitabroad
audience including (1) DL library developers, (2) machine learn-
ing/software engineers who use DL libraries, or implement custom
DL algorithms, (3) developers of other numerical computational
softwarethatimplementthesamemathformulasandalgorithms,
(4) tool designers who aim to build automatic tools to detect or fix
unstableimplementations.Indirectly,buildingnumericallystable
DL products can benefit many end users.
REFERENCES
[1]Tao Bao and X. Zhang. 2013. On-the-fly detection of instability problems in
floating-pointprogramexecution. Proceedingsofthe2013ACMSIGPLANinterna-
tionalconferenceonObjectorientedprogrammingsystemslanguages&applications
(2013).
[2]Florian Benz, A. Hildebrandt, and Sebastian Hack. 2012. A dynamic program
analysis to find floating-point accuracy problems. In PLDI â€™12.
[3]Meriame Berboucha. 2018. Uber Self-Driving Car Crash: What Really Hap-
pened. https://www.forbes.com/sites/meriameberboucha/2018/05/28/uber-self-
driving-car-crash-what-really-happened/?sh=4a4b37984dc4
[4]F.Bornemann.2011. AccuracyandStabilityofComputingHigh-orderDeriva-
tivesofAnalyticFunctionsbyCauchyIntegrals. FoundationsofComputational
Mathematics 11 (2011), 1â€“63.
[5]O. Bousquet and A. Elisseeff. 2002. Stability and Generalization. J. Mach. Learn.
Res.2 (2002), 499â€“526.
[6]M.CerezoandPatrickJ.Coles.2021. Higherorderderivativesofquantumneural
networks with barren plateaus. Quantum Science & Technology 6 (2021).
[7]SangeetaChowdhary,JayP.Lim,andSantoshNagarakatte.2020. Debuggingand
detecting numerical errors in computation with posits. Proceedings of the 41st
ACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation
(2020).
4https://nvd.nist.gov/
5https://cve.mitre.org/[8]A. Cohen, R. DeVore, G. Petrova, and P. Wojtaszczyk. 2020. Optimal Stable
Nonlinear Approximation. ArXivabs/2009.09907(2020).
[9]R. DeVore, B. Hanin, and G. Petrova. 2020. Neural Network Approximation.
ArXivabs/2012.14501(2020).
[10]Michael Elad and M. Aharon. 2006. Image Denoising Via Sparse and Redundant
RepresentationsOverLearnedDictionaries. IEEETransactionsonImageProcessing
15 (2006), 3736â€“3745.
[11]A.D.Franco,HuiGuo,andCindyRubio-GonzÃ¡lez.2017. Acomprehensivestudy
ofreal-worldnumericalbugcharacteristics. 201732ndIEEE/ACMInternational
Conference on Automated Software Engineering (ASE) (2017), 509â€“519.
[12]Zhoulai Fu, Z. Bai, and Z. Su. 2015. Automated backward error analysis for
numerical code. Proceedings of the2015 ACM SIGPLAN InternationalConference
on Object-Oriented Programming, Systems, Languages, and Applications (2015).
[13]I. Goodfellow, Yoshua Bengio, and Aaron C. Courville. 2015. Deep Learning.
Nature521 (2015), 436â€“444.
[14]N.Higham.2002.Accuracyandstabilityofnumericalalgorithms,SecondEdition.
[15]S. Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep
NetworkTrainingbyReducingInternalCovariateShift. ArXivabs/1502.03167
(2015).
[16]L.D.Jong.1977. Towardsaformaldefinitionofnumericalstability. Numer.Math.
28 (1977), 211â€“219.
[17]David A. Knowles. 2015. Stochastic gradient variational Bayes for gamma ap-
proximating distributions. arXiv: Machine Learning (2015).
[18]Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix Factorization Techniques for
Recommender Systems. Computer 42 (2009).
[19]RamShankarSivaKumar,MagnusNystrÃ¶m,J.Lambert,AndrewMarshall,Mario
Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. 2020. Adversarial
Machine Learning - Industry Perspectives. ArXivabs/2002.05646(2020).
[20]Wen-ChuanLee,TaoBao,YunhuiZheng,X.Zhang,KevalVora,andRajivGupta.
2015. RAIVE:runtimeassessmentoffloating-pointinstabilitybyvectorization.
Proceedingsofthe2015ACMSIGPLANInternationalConferenceonObject-Oriented
Programming, Systems, Languages, and Applications (2015).
[21]R.LugannaniandS.Rice.1980. SADDLEPOINTAPPROXIMATIONFORTHE
DISTRIBUTION OF THE SUM OF INDEPENDENT RANDOM VARIABLES. Ad-
vances in Applied Probability 12 (1980), 475â€“490.
[22]Razvan Pascanu and Yoshua Bengio. 2014. Revisiting Natural Gradient for Deep
Networks. CoRRabs/1301.3584 (2014).
[23]Shibani Santurkar, D. Tsipras, Andrew Ilyas, and A. Madry. 2018. How Does
Batch Normalization Help Optimization?. In NeurIPS.
[24]E.Schwarz,M.Schmookler,andS.D.Trong.2003. Hardwareimplementations
ofdenormalizednumbers. Proceedings200316thIEEESymposiumonComputer
Arithmetic (2003), 70â€“78.
[25]Z.StrakosandJ.Liesen.2005. Onnumericalstabilityinlargescalelinearalgebraic
computations. Zamm-zeitschriftFurAngewandteMathematikUndMechanik 85
(2005), 307â€“325.
[26]BillVlasicandNealE.Boudette.2016. Self-DrivingTeslaWasInvolvedinFatal
Crash,U.S.Says. https://www.nytimes.com/2016/07/01/business/self-driving-
tesla-fatal-crash-investigation.html
[27]Daisuke Wakabayashi. 2018. Self-Driving Uber Car KillsPedestrian in Arizona,
WhereRobotsRoam. https://www.nytimes.com/2018/03/19/technology/uber-
driverless-fatality.html
[28]XinYi,LiqianChen,XiaoguangMao,andTaoJi.2019. Efficientautomatedrepair
of high floating-point errors in numerical libraries. Proc. ACM Program. Lang. 3
(2019), 56:1â€“56:29.
10 APPENDIX
10.1 Softmax proof
Softmax is a commonly used formula in DL that is known to be
numerically unstable. Softmax is defined as:
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘¥ğ‘–)=ğ‘’ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘—(13)
Thisformulaisnumericallyunstableandshouldbeimplemented
as:
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘¥ğ‘–)=ğ‘’âˆ’ğ‘šğ‘ğ‘¥(ğ‘¥)+ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’âˆ’ğ‘šğ‘ğ‘¥(ğ‘¥)+ğ‘¥ğ‘—(14)
Thetwoformulasaremathematicallyequivalent,whichisshown
in the proof below.
596ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Eliska Kloberdanz, Kyle G. Kloberdanz, and Wei Le
Proof.Letğ‘beascalarconstantsuchthat ğ‘™ğ‘œğ‘”(ğ‘)=âˆ’ğ‘šğ‘ğ‘¥(ğ‘¥1,...,ğ‘¥ğ‘›)
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘¥ğ‘–)=ğ‘’ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘—(15)
=ğ‘âˆ—ğ‘’ğ‘¥ğ‘–
ğ‘âˆ—/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘—(16)
=ğ‘âˆ—ğ‘’ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘âˆ—ğ‘’ğ‘¥ğ‘—(17)
=ğ‘’ğ‘™ğ‘œğ‘”(ğ‘)âˆ—ğ‘’ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘™ğ‘œğ‘”(ğ‘)âˆ—ğ‘’ğ‘¥ğ‘—(By property ğ‘=ğ‘’ğ‘™ğ‘œğ‘”(ğ‘))
=ğ‘’(ğ‘™ğ‘œğ‘”(ğ‘))+ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’(ğ‘™ğ‘œğ‘”(ğ‘))+ğ‘¥ğ‘—(By property ğ‘’ğ‘âˆ—ğ‘’ğ‘=ğ‘’ğ‘+ğ‘)
=ğ‘’âˆ’ğ‘šğ‘ğ‘¥(ğ‘¥)+ğ‘¥ğ‘–
/summationtext.1ğ‘›
ğ‘—=1ğ‘’âˆ’ğ‘šğ‘ğ‘¥(ğ‘¥)+ğ‘¥ğ‘—/square
10.2 LogSoftmax proof
Logsoftmax is anoter canonical example of a numerically unstable
formulathatiscommonlyusedinDL.Priorliteratureshowshow
to rewrite theformula to obtain a numerically stablesolution, but
doesnotprovideproofthatthetwoformulasaremathematically
equivalent.Toourbestknowledge,thisisthefistcomprehensive
proof which shows that step by step.
LogSoftmaxperformssoftmaxfollowedbythelogarithmfunc-
tion and therefore, outputs log probabilities. LogSoftmax is defined
as:
ğ‘™ğ‘œğ‘”ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (/vecğ‘¥)ğ‘–=ğ‘™ğ‘œğ‘”(ğ‘’ğ‘¥ğ‘–)/summationtext.1ğ‘›
ğ‘—=1ğ‘’ğ‘¥ğ‘—(18)
Thismathematicalformulaisnumericallyunstableandshould
be implemented as:
ğ‘™ğ‘œğ‘”ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (/vecğ‘¥)ğ‘–=ğ‘¥ğ‘–âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥)âˆ’ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥))(19)
These two equations are mathematically equivalent, which can
be proved utilizing the identity:
ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)=ğ‘šğ‘ğ‘¥(/vecğ‘¥)+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥))(20)
We first prove the correctness of the identity and then the math-
ematical equivalence of the numerically stable and unstable log-
softmax formulas.Proof.Letğ‘be a scalar constant such that ğ‘=ğ‘šğ‘ğ‘¥(ğ‘¥1,...,ğ‘¥ ğ‘›)
ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)=ğ‘šğ‘ğ‘¥(/vecğ‘¥)+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥))
=ğ‘+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘)
=ğ‘+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ—ğ‘’âˆ’ğ‘)(By property ğ‘’ğ‘âˆ’ğ‘=ğ‘’ğ‘âˆ—ğ‘’ğ‘)
=ğ‘+ğ‘™ğ‘œğ‘”(ğ‘’âˆ’ğ‘)+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)
(By property ğ‘™ğ‘œğ‘”(ğ‘ğ‘)=ğ‘™ğ‘œğ‘”(ğ‘)+ğ‘™ğ‘œğ‘”(ğ‘))
=ğ‘+( âˆ’ğ‘âˆ—ğ‘™ğ‘œğ‘”(ğ‘’))+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)
(By property ğ‘™ğ‘œğ‘”(ğ‘ğ‘=ğ‘âˆ—ğ‘™ğ‘œğ‘”(ğ‘))
=ğ‘+( âˆ’ğ‘(1))+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)(By property ğ‘™ğ‘œğ‘”(ğ‘’)=1)
=ğ‘âˆ’ğ‘+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)
=ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)/square
Proof.
ğ‘™ğ‘œğ‘”ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (/vecğ‘¥)ğ‘–=ğ‘™ğ‘œğ‘”(ğ‘’ğ‘¥ğ‘–)/summationtext.1ğ‘›
ğ‘—=1(ğ‘’ğ‘¥ğ‘—)
=ğ‘™ğ‘œğ‘”(ğ‘’ğ‘¥ğ‘–)âˆ’ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)
(By property ğ‘™ğ‘œğ‘”(ğ‘/ğ‘)=ğ‘™ğ‘œğ‘”(ğ‘)âˆ’ğ‘™ğ‘œğ‘”(ğ‘))
=ğ‘¥ğ‘–âˆ—ğ‘™ğ‘œğ‘”(ğ‘’)âˆ’ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)
(By property ğ‘™ğ‘œğ‘”(ğ‘ğ‘)=ğ‘âˆ—ğ‘™ğ‘œğ‘”(ğ‘))
=ğ‘¥ğ‘–âˆ—(1)âˆ’ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—)(By property ğ‘™ğ‘œğ‘”(ğ‘’)=1)
=ğ‘¥ğ‘–âˆ’(ğ‘šğ‘ğ‘¥(/vecğ‘¥)+ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥)))
(By identity in Equation 20)
=ğ‘¥ğ‘–âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥)âˆ’ğ‘™ğ‘œğ‘”(ğ‘›/summationdisplay.1
ğ‘—=1ğ‘’ğ‘¥ğ‘—âˆ’ğ‘šğ‘ğ‘¥(/vecğ‘¥))/square
597