Neural Program Repair with Execution-based Backpropagation
He Ye
heye@kth.se
KTH Royal Institute of Technology
SwedenMatias Martinez
matias.martinez@uphf.fr
UniversitÃ© Polytechnique
Hauts-de-France, France & KTH
Royal Institute of Technology, SwedenMartin Monperrus
martin.monperrus@4open.science
KTH Royal Institute of Technology
Sweden
ABSTRACT
Neural machine translation (NMT) architectures have achieved
promising results for automatic program repair. Yet, they have the
limitation of generating low-quality patches (e.g., not compilable
patches). This is because the existing works only optimize a purely
syntactic loss function based on characters and tokens without
incorporating program-specific information during neural network
weight optimization. In this paper, we propose a novel program re-
pair model called RewardRepair. The core novelty of RewardRepair
is to improve NMT-based program repair with a loss function based
on program compilation and test execution information, reward-
ing the network to produce patches that compile and that do not
overfit. We conduct several experiments to evaluate RewardRepair
showing that it is feasible and effective to use compilation and test
execution results to optimize the underlying neural repair model.
RewardRepair correctly repairs 207 bugs over four benchmarks. we
report on repair success for 121 bugs that are fixed for the first time
in the literature. Also, RewardRepair produces up to 45.3% of compi-
lable patches, an improvement over the 39% by the state-of-the-art.
ACM Reference Format:
He Ye, Matias Martinez, and Martin Monperrus. 2022. Neural Program Repair
with Execution-based Backpropagation. In 44th International Conference on
Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM,
Pittsburgh, PA, USA, 13 pages. https://doi.org/10.1145/3510003.3510222
1 INTRODUCTION
Automatic program repair (APR) aims to reduce manual work re-
lated to bug localization and bug fixing [ 14,43]. With recent ad-
vances in deep learning, research has been proposed to use neural
networks for program repair, a subarea of a trend on using machine
learning on code [ 10]. This line of work, put here under the um-
brella term â€œneural program repairâ€, mostly uses neural machine
translation (NMT) approaches [5, 8, 19, 22, 35, 55, 56].
Program repair systems based on neural machine translation
treat the repair task as a translation from buggy code to correct
code, both represented as a sequence of tokens [ 8]. Given sufficient
training data, NMT-based repair has achieved promising perfor-
mance [ 5,8,19,22,35,55,56]. All the prior works on program repair
based on neural machine translation use the static loss function:
cross-entropy loss based on token similarity.
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510222Despite early successes, NMT-based program repair approaches
suffer from two major drawbacks. First, they often generate patches
that do not compile [ 8]. The reason is that a lower cross-entropy
value does not necessarily lead to a compilable patch. Second, the
loss function under optimization forces the neural network to learn
to produce absolutely identical patches, thus missing the opportu-
nity to explore semantically equivalent. However, Rabin et al. [ 45]
made the case that generalizability in machine learning on code
relates to generalization over equivalent programs. Both problems
(uncompilability and being stuck with syntactic identity) share the
same root: there is a discrepancy between the core training ob-
jective, learning to generate compilable and correct patches, and
the loss function that is being optimized. The cross-entropy loss
used in related work requires a strict pairwise matching between
the generated patch and the human-written ground truth patch,
and not more [ 74]. Nothing in cross-entropy loss encourages the
neural network to produce compilable or syntactically different but
semantically equivalent patches. This is the problem we address in
this paper.
We introduce a novel neural repair model, called RewardRepair
based on a mixed learning objective. The key insight in the design
of RewardRepair is the combination of a syntactic training objec-
tive at the token-level and a semantic training objective based on
program execution. RewardRepair defines a discriminator to dis-
criminate good patches from low-quality ones during the training
of the neural network. The discriminator is based on executing
the compiler and running the test cases on the generated patches,
providing high qualified execution feedback on their quality. This
feedback is transformed as a quantified reward signal that modu-
lates the cross-entropy loss. Then, the neural networkâ€™s weights
are updated based on this novel discriminator. In other words, in
RewardRepair, backpropagation embeds essential compilation and
execution information.
We conduct large experiments to evaluate RewardRepair based
on four well-accepted datasets from the literature, including De-
fects4J version 1.2 [ 23], Defects4J version 2.0 [ 23], Bugs.jar [ 49]
and QuixBugs [ 31]. First, we show that RewardRepair produces
more correct patches than the recent related work. In total, Re-
wardRepair repairs 207 on the four benchmarks. RewardRepair
achieves an improvement in two benchmarks Defects4J (v2.0) and
Bugs.jar, and achieves the top-2 performance in the other two bench-
marks Defects4J (v1.2) and QuixBugs. Second, we demonstrate that
RewardRepair outperforms the state-of-the-art on addressing the
compilability problem in neural program repair, CURE [ 22], by pro-
ducing a higher ratio of compilable patches in all considered beam
size configuration (45.3% versus 39% in top-30, and 37.5% versus
28% in top-100 candidate patches).
15062022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA He Ye, Matias Martinez, and Martin Monperrus
To sum up, our contributions are:
â€¢We devise, RewardRepair, a neural program repair approach
with execution-based backpropagation. RewardRepair de-
fines a novel training objective that employs compilation and
test execution information to optimize the neural network.
â€¢We perform an original series of experiments to show that
RewardRepair outperforms the cross-entropy loss used in
related work. Our experimental results demonstrate that em-
bedding execution information in backpropagation improves
the quality of generated patches (more compilable patches
and correct patches).
â€¢We provide evidence that RewardRepair can correctly fix
45 bugs for Defects4J (v1.2), 45 bugs for Defects4J (v2.0),
97 bugs for Bugs.jar and 20 bugs for QuixBugs. We report
that ReparRepair can correctly fix 121 bugs that were never
repaired in previous literature, including 5 unique Defects4J
(v1.2) bugs.
â€¢We make our data and code publicly available for future
research [12].
2 BACKGROUND
2.1 Neural Program Repair
Neural Machine Translation (NMT) systems have recently achieved
state-of-the-art performance on program repair tasks, forming a
field called â€œneural program repairâ€ [ 5,8,19,22,35,55,56,76]. De-
spite the difference in their inputs and neural models, those works
are similar in the sense that they are all based on a typical NMT
model formulated as an encoder-decoder-attention architecture
optimized with cross-entropy loss function [ 3,34,57,58,74]. All
the prior works on program repair are based on the NMT archi-
tecture with a cross-entropy loss function to update the neural
network weights [ 5,8,19,22,35,55,56,76]. The cross-entropy
loss ( a.k.a. log loss) is a measure from information theory, building
upon entropy and calculating the difference between two proba-
bility distributions [ 3,16]. During the training of a neural repair
model, the cross-entropy loss calculates the difference between the
generated tokens and the human-written patch tokens in a strict
pairwise matching manner [ 3,34,74], and is used to update the
neural network weights by backpropagation. In program repair
patches, a low cross-entropy value means that the generated patch
is syntactically close to the ground truth patch at the token-level.
2.2 Limitations of Current Neural Repair
The token-based cross-entropy loss optimization is effective at guid-
ing the model to generate patches syntactically identical or close to
the human-written patches given as input during training. However,
it suffers from two major limitations. Firstly, a major goal in patch
generation is to generate well-formed patches that compile. Unfor-
tunately, the cross-entropy loss does not favor patches that compile
over non-compilable patches. Even worse, if a generated patch
has a significantly lower loss per the token-based cross-entropy,
although being non-compilable, it would be favored by the model
at training time. Secondly, the cross-entropy loss function fails to
learn from semantically equivalent patches: a syntactically different
but semantically equivalent patch could potentially have a high
cross-entropy loss value. This means that the cross-entropy loss- return FastMath.pow(2 * FastMath.PI, -dim / 2 ) *
+ return FastMath.pow(2 * FastMath.PI, -0.5 * dim ) *
(a) The human-written patch
+ return FastMath.pow(2 * FastMath.PI, -d * dim ) * // loss value 0.1157
(b) A generated non-compilable patch receives a smaller loss score
+ return FastMath.pow(2 * FastMath.PI, -dim / 2d ) * // loss value 0.4224
(c) A generated semantically-equivalent patch receives a bigger loss score
Listing 1: Motivating example: NMT based repair models based on
cross-entropy loss may favor non-compilable patches.
discourages the network to explore equivalent solutions. In the
field of neural machine translation (NMT), this problem is known
as the overcorrection problem of cross-entropy loss [ 34,74]: cross-
entropy based models tend to learn strictly identical translations
and to overcorrect synonymous tokens which would be acceptable.
Overall, the problem we address in this paper is that the cross-
entropy loss used in previous research cannot learn programming
knowledge beyond tokens.
2.3 Motivating Example
Listing 1 is a motivating example to show the drawbacks of
a neural repair model based on cross-entropy loss optimization.
Listing 1a presents the buggy code and the human-written patch for
bugMath-11 from Defects4J (v1.2). Listing 1b gives one generated
non-compilable patch because of the undefined variable ğ‘‘. The
network generates this patch with maximum likelihood estimation
because its cross-entropy loss value is low ( 0.1157 ). Listing 1c is
a semantically equivalent patch compared to the human-written
patch and its computed loss is 0.4224 , which is higher than the
non-compilable patch.
With cross-entropy loss, the NMT-based repair model penalizes
the semantically equivalent patch and favors the non-compilable
patch. This is because there is only one token difference for the
generated non-compilable patches, where the wrong token dis not
the expected token 0.5from the ground truth patch. However, the
tokenğ‘‘is an undefined variable in the buggy program. On the con-
trary, there exist three token differences in the semantically equiv-
alent patch: ( ğ‘‘ğ‘–ğ‘šâ†’0.5), (/â†’âˆ— ), (2ğ‘‘â†’ğ‘‘ğ‘–ğ‘š). Consequently, the
NMT-based repair model considers the generated non-compilable
patch is closer to the human-written patch, and thus should be fa-
vored during backpropagation. However, in the context of program
repair, the semantically equivalent patch should have a loss close
to zero, because it is a valid solution to the bug. The generated
non-compilable patch has a lower loss, but still cannot satisfy the
compiler, which is inconsistent with our goal. This example shows
the fundamental limitation of optimizing neural networks with the
traditional token-based cross-entropy loss for the program repair
task.
1507Neural Program Repair with Execution-based Backpropagation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Syntactic Training
Semantic T rainingCross-
Entropy
Losscandidate
patchesPatch
Generator
Weights update with
backpropagationexpected output
training
input
Patch
Generator
Weights update
with
backpropagationBuggy codeHuman
Patch     training data point (pair)
Buggy codeHuman
Patch
training data point (tuple)
App
training
input
RewardRepair
LossPlausible
DiscriminatorRegression
Discriminator
no-change
penaltycompilable
rewardplausible 
rewardlikely-correct
rewardY Y Y
non-compilable
penaltyY Difference
DiscriminatorCompilability
Discriminator
Buggy code App
Y
NExecution Execution
Reward V alues (R)
Discriminative  Model of RewardRepaircandidate
patches(compilable
bug-fix
patches 
+ executable
tests) (bug-fix
patches,
code only)
expected
output
Inference
(bugs under actual repair)Patch
Generator
suggested
patches          inference data point 
Buggy codeinference
inputN N N
Figure 1: An Overview of RewardRepair.
3 REWARDREPAIR
3.1 Overview
In this paper, we propose a novel neural repair model called Re-
wardRepair. The core idea of RewardRepair is to improve the learn-
ing process of neural program repair and in particular to address
the limitations of using the cross-entropy loss function at the token-
level. Figure 1 gives an overview of RewardRepair. The top, middle
and bottom parts represent three stages of RewardRepair: syntactic
training, semantic training and inference. RewardRepair is trained
with two datasets respectively, a syntactic training dataset with
pairs of buggy and patch code, and a semantic training dataset.
They are fundamentally different. The syntactic training dataset
only consists of textual patches as in the related work [ 8,22,35].
However, the requirements for the semantic training dataset are
full execution: each sample in the dataset comes with a compiler
configuration and test cases. Achieving full execution enables us
to derive execution-based information to be used for optimizing
the neural network weights with programming knowledge during
backpropagation.
Syntactic training is our first training phase. We train Reward-
Repair to optimize the cross-entropy loss based on a large bug fix
corpus per the related work [ 5,8,19,22,30,35,55]. Syntactic train-
ing is meant to provide a good initial model for semantic training.
Semantic training is our second phase after sufficient syntactic train-
ing. Semantic training is used alternately with syntactic training.
Semantic training is based on a discriminative model. The discrimi-
native model of RewardRepair analyzes the candidate patches with
four discriminators ( difference discriminator, compilability discrimi-
nator, plausibility discriminator and regression discriminator ), and
modulates the cross-entropy loss before the start of backpropaga-
tion on training. The inference is the final phase. Once RewardRepairhas been trained, it can generate patches for new and unseen bugs
based on the trained patch generator.
3.2 Code Representation
As code representation, we follow Lutellier et al. [35] to represent
the buggy code and context code as two separate token sequences.
In RewardRepair, the context code is considered as 10 lines of code
surrounding the buggy code. In addition, the context code is en-
riched with a summary of the buggy class as proposed by Chen et
al. [8] as follows: we keep all the instance variables and their ini-
tializers, along with the signature of the constructors and methods.
We follow the existing work [ 22] to use subword tokenization with
SentencePiece [ 25], as demonstrated useful by Karampatsis et al.
[24].
3.3 Patch Generator
In RewardRepair, the patch generator is trained in a supervised
manner based on an encoder-decoder architecture [ 46,57]. Syn-
tactic training takes as input buggy code tokens ğµ=(ğ‘0,ğ‘1...ğ‘ğ‘›),
context code tokens ğ¶=(ğ‘0,ğ‘1...ğ‘ğ‘š), and tokens from the ground
truth fix patch ğ¹=(ğ‘“0,ğ‘“1...ğ‘“ğ‘¡). RewardRepair transforms the ğµ
andğ¶into a predicted patch ğ¹â€²=(ğ‘“â€²
0,ğ‘“â€²
1...ğ‘“â€²
ğ‘˜). Note that size of
the buggy code, context code, ground truth patch and predicted
patch, i.e.,ğ‘›,ğ‘š,ğ‘¡andğ‘˜, can be different. In RewardRepair, patch
generation is shared by both syntactic training and semantic train-
ing.
3.4 Syntactic Training of RewardRepair
RewardRepair initially trains the patch generator with cross-entropy
loss function per the state-of-the-art of NMT for program repair
[5,8,19,22,30,35,55]. For each training point, the optimization
target is to minimize the loss between ground truth fix patch ğ¹and
1508ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA He Ye, Matias Martinez, and Martin Monperrus
the predicted patch ğ¹â€². As shown in the previous study [ 22,35],
syntactic training is trained with a large corpus of buggy code and
fixed code pairs. Syntactic training could be trained for multiple
epochs to achieve convergence and obtain the best combination
of weights. By the end of syntactic training, the patch generatorâ€™s
weights between the connections of the networks are optimized.
3.5 Semantic Training of RewardRepair
The goal of semantic training is to let the patch generator be aware
of program-specific knowledge (compilation and execution) be-
yond the syntactic loss computation at the token-level. For that,
we propose a mixed learning objective [ 34], where â€œmixedâ€ means
that the core learning objective is combined with two or more sub-
learning objectives. Our mixed learning objective combines the
core cross-entropy objective with compilation and execution infor-
mation. Once the RewardRepair has been sufficiently trained with
syntactic training, we start the semantic training process alternately
with syntactic training.
For semantic training, RewardRepair employs a discriminative
model to assess the quality of generated patches based on compila-
tion and test oracles. As a result, the discriminative model outputs
a reward value that quantifies the patch quality, which is used to
adjust the weights of the patch generator during backpropagation.
A higher reward means better quality for the generated patch, e.g.,
the patch is compilable and passes all test cases. On the contrary, a
lower reward means that the quality of the generated patch may
be unsatisfying, e.g., the patch is non-compilable.
Precisely, the patch reward value modulates the token-level loss
value before the start of the backward pass [ 42,48,75], so that
the updated loss can then be properly represented with program-
specific knowledge. In such a way, the discriminator guides the
neural network to generate high-quality patches. To our knowledge,
we are the first to introduce semantic training based on a patch
discriminative model for neural program repair.
3.5.1 Discriminative model. In machine learning, a discriminator
is a model for identifying â€œgoodâ€ and â€œbadâ€ predictions [ 17]. The
discriminator of RewardRepair is the key component of the se-
mantic training phase. Its goal is to measure the quality of the
generated patches, which is then quantified as a reward signal. In
RewardRepair, the discriminative model is composed of four dis-
criminators, each of them specialized in one aspect of patch quality.
These four discriminators are serially executed. Each discriminator
does a binary classification: whether a patch fulfills the discrim-
inatorâ€™s criterion or not. When a discriminator is affirmative, it
means the patch passes the quality criterion, and it is passed to the
next discriminator. In the rest of this section, we present the four
discriminators of RewardRepair.
Difference discriminator. The difference discriminator validates
whether the generated patches are different from the given buggy
code. It is required because we found that neural repair models
regularly generate a patch that is identical to the given buggy code,
i.e., the output of the model is identical to the input of the model
(called a â€œno-changeâ€ patch in this paper). This happens when the
neural nets discover that the buggy code achieves the maximum
likelihood estimation. This is explained by previous research [ 54],which has shown that the many buggy codes are similar to correct
code, with only minor transformations and few changed tokens.
Consequently, the generator tends to copy the buggy code because
it is the maximum likelihood prediction, per the data seen at train-
ing time. We design the difference discriminator to syntactically
compare the generated patch code with the input buggy code, with
a token sequence comparison. If the buggy code and generated
patched code are the same, the difference discriminator assigns
the generated patch a penalty, called in this paper the no-change
penalty (ğ‘…ğ‘›ğ‘œâˆ’ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ ), i.e., a negative reward. This penalty signal
modulates the RewardRepair loss to avoid the generation of no-
change patches.
Compilability discriminator. The compilability discriminator val-
idates whether the generated patches are compilable. As shown in
previous research [ 22], neural repair models suffer from outputting
non-compilable patches. For example, the golden sequenceR model
produces 79.7% of patches that are not compilable in top-50 can-
didate patches. To force the generator towards producing compi-
lable patches, we introduce a compilability discriminator in the
semantic training phase. The compilability discriminator employs
the compiler to compile the generated patched program. If the
patched program is compilable, RewardRepair assigns a compilable
reward (ğ‘…ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ ) and passes the patch to the next discrimina-
tor for further assessment. Otherwise, a non-compilable penalty
(ğ‘…ğ‘›ğ‘œğ‘›âˆ’ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ ), i.e., a negative reward, is returned to the Reward-
Repair model.
Plausibility discriminator. The plausibility discriminator aims at
encouraging the generator to produce patches that pass the human-
written test cases provided by developers. Recall that, per the sem-
inal work on program repair [ 29], test cases can be used as an
executable program specification. In other words, if a patch passes
the human-written tests, it is considered as a plausible patch. During
semantic training, RewardRepair is trained on buggy projects with
executable human-written tests. Each candidate patch is executed
against the human-written tests. If a patch makes all human-written
tests pass, a plausible reward (ğ‘…ğ‘ğ‘™ğ‘ğ‘¢ğ‘ ğ‘–ğ‘ğ‘™ğ‘’ ) is assigned. By doing so,
the plausibility discriminator leverages the human-written tests
to drive the network to produce plausible patches that pass all
human-written tests.
Regression discriminator. The last discriminator used in Reward-
Repairâ€™s discriminative model is the regression discriminator. The
goal of this discriminator is to minimize the behavioral changes
introduced by the patch. This discriminator complements the plausi-
ble patch discriminator by specifying behavior outside the human-
written tests, in order to avoid patch overfitting [ 27,28,53,72].
The RewardRepair regression discriminator employs automatically
generated tests, per the RGT technique of Ye et al. [ 70]. The effec-
tiveness of this technique to identify correct patches from plausible
patches has been shown in recent work [ 62,67,68]. The idea of
the RGT technique is to automatically generate test cases based on
the ground truth patched program to expose program execution
behavior differences between a generated patch and a ground truth
patch [ 28,62,70]. If a candidate patch makes all RGT tests pass,
i.e., it does not contradict the ground truth program behavior, it is
considered as likely-correct.
1509Neural Program Repair with Execution-based Backpropagation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
During semantic training, all patches are executed against the
RGT tests. If a candidate patch makes all automatically gener-
ated tests pass, meaning the same program execution behavior
with ground truth program, then a likely-correct reward signal
(ğ‘…ğ‘™âˆ’ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ ) is assigned to this patch. This discriminatorâ€™s reward
is used to encourage the RewardRepair to avoid regressions. This
means we encourage RewardRepair to generate non-overfitting patches
beyond the existing test cases.
3.5.2 Defining reward values from discriminators. As shown previ-
ously, RewardRepair defines five reward signals for patch quality
assessment. The discriminators are executed serially, that is, if a
patch does not satisfy one discriminator, then the reward ğ‘…obtained
up to that moment is returned immediately and other discrimina-
tors are not executed. Consequently, the ğ‘…is the maximum of the
five reward values as follows:
ğ‘…=ğ‘šğ‘ğ‘¥ï£±ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³ğ‘…ğ‘›ğ‘œâˆ’ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ =ğ‘ 0
ğ‘…ğ‘›ğ‘œğ‘›âˆ’ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ =ğ‘ 1
ğ‘…ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ =ğ‘ 2
ğ‘…ğ‘ğ‘™ğ‘ğ‘¢ğ‘ ğ‘–ğ‘ğ‘™ğ‘’ =ğ‘ 3
ğ‘…ğ‘™âˆ’ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ =ğ‘ 4(1)
whereğ‘ ğ‘–(ğ‘–âˆˆ{0,1,2,3,4}) are five scaling parameters that control
the range of the reward values. The scaling parameters of ğ‘ ğ‘–define
a configuration space that is controlled by end-users of Reward-
Repair. Additionally, those reward values must fulfill the following
constraint:
ğ‘…ğ‘›ğ‘œâˆ’ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ <ğ‘…ğ‘›ğ‘œğ‘›âˆ’ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ <ğ‘…ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ <ğ‘…ğ‘ğ‘™ğ‘ğ‘¢ğ‘ ğ‘–ğ‘ğ‘™ğ‘’ <ğ‘…ğ‘™âˆ’ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ (2)
where the higher reward value represents the better quality of the
generated patch.
Given the cross-entropy L, the loss function of RewardRepair
Lğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘…ğ‘’ğ‘ğ‘ğ‘–ğ‘Ÿ dynamically reweighs Lwith respect to the dis-
criminatorâ€™s reward signal ğ‘…, which encodes the quality of the
generated patch. We follow [ 48,71] to formulate RewardRepair loss
function as a scaling of the cross-entropy, as follows:
Lğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘…ğ‘’ğ‘ğ‘ğ‘–ğ‘Ÿ =(1âˆ’ğ‘…)âˆ—L (ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘… <1) (3)
The reward modulator (1âˆ’ğ‘…)constrains the domain of ğ‘…âˆˆ
(âˆ’âˆ,1), as it is meaningless for the objective function to minimize
a negative loss. In this formulation, the syntactic cross-entropy at
the token-level is combined with the semantic reward at patch-
level, embedding compilation and execution knowledge deep into
the neural model. It mitigates the limitations of only considering
cross-entropy loss in program repair tasks and solves the problem
discussed in Section 2.3.
For low-quality patches, RewardRepair assigns a penalty, i.e.,
negative reward value, to increase the original cross-entropy loss L.
Thus the domain for ğ‘…ğ‘›ğ‘œâˆ’ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’,ğ‘…ğ‘›ğ‘œğ‘›âˆ’ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’âˆˆ(âˆ’âˆ,0). On the
contrary, for the high-quality patches, RewardRepair scales down
the originalLby assigning positive reward values for ğ‘…ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘™ğ‘ğ‘ğ‘™ğ‘’ ,
ğ‘…ğ‘ğ‘™ğ‘ğ‘¢ğ‘ ğ‘–ğ‘ğ‘™ğ‘’ , andğ‘…ğ‘™âˆ’ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ to encourage the model. The domain for
these three reward values is [0,1). Based on Equation 3, the higher
the reward, the smaller computed loss is back-propagated into
the neural model. The extreme case is the highest reward value
approximate to 1, where the RewardRepair loss goes close to 0. This
indicates that the generated patch is likely correct and the network
should not be changed.Algorithm 1 One step of semantic training in RewardRepair
1:Input: buggy code b, context code c, ground-truth patch code p, human-written
test casesğ‘¡â„, RGT test cases ğ‘¡ğ‘š, learning rate ğ›¼, G is the RewardRepair patch
generator, D is the discriminator function
2:eğ‘â†ğœ‘(ğ‘){Encode buggy code}
3:eğ‘â†ğœ‘(ğ‘){Encode context code}
4:eğ‘â†ğœ‘(ğ‘){Encode ground-truth patch code}
5:eğ‘â†ğº(eğ‘,eğ‘){Generate candidate patch }
6:L=Ã
ğ‘¥âˆˆğ‘‹eğ‘(ğ‘¥)ğ‘™ğ‘œğ‘”eğ‘(ğ‘¥)
7:ğ‘…â†ğ·(eğ‘,eğ‘,eğ‘,ğ‘¡â„,ğ‘¡ğ‘š)
8:Lğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘…ğ‘’ğ‘ğ‘ğ‘–ğ‘Ÿ =(1âˆ’ğ‘…)âˆ—L
9:ğºâ†ğºâˆ’ğ›¼ğœ•ğ¿ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘…ğ‘’ğ‘ğ‘ğ‘–ğ‘Ÿ/ğœ•ğº{update patch generator with backpropagation}
Combining the domain constraints discussed above, the range
of scaling parameters ğ‘ ğ‘–must meet the following criteria:
ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³ğ‘ 0âˆˆ(âˆ’âˆ,0)
ğ‘ 1âˆˆ(ğ‘ 0,0)
ğ‘ ğ‘–âˆˆ[0,1),ğ‘ ğ‘–âˆ’1<ğ‘ ğ‘–,ğ‘–âˆˆ{2,3,4}(4)
3.5.3 Algorithm. Algorithm 1 presents one step of semantic train-
ing. Given the encoded buggy eğ‘and context code eğ‘(line 2 and 3),
the patch generator ğºgenerates a candidate patch (line 5). Then,
the cross-entropy loss Lis computed by comparing the token distri-
bution between the ground truth patch eğ‘and the generated patch eğ‘
(line 6), where the ğ‘¥indicates the index of tokens. The discriminator
provides a reward ğ‘…based on generated patch eğ‘and the correspond-
ing program compilability and test execution information (line 7).
Lastly, RewardRepair combines the cross-entropy loss at token-
level and reward value at patch-level to form RewardRepair loss
(line 8), which encodes the program-specific knowledge.
3.6 Inference
At inference phase, for a given suspicious statement found by fault
localization tools (e.g., Ochiai [ 1]), RewardRepair represents it with
two sequences of tokens: one for the suspicious statement, the other
one for its context (see Section 3.3). Those tokens are given to the
patch generator of RewardRepair, previously trained as explained in
Sections 3.4 and 3.5. As RewardRepair is configured by the inference
beam sizeğ‘›(see [ 8,22]), it outputs the ğ‘›best patches for that
suspicious statement. RewardRepair can be used with any fault
localization technique in real-world bug repair tasks, as shown by
[4].
3.7 Implementation
We implement RewardRepairâ€™s patch generator with the state-of-
the-art Transformer based architecture [ 46] from Hugging Face.
RewardRepair is trained with 15 syntactic training epochs and 4
semantic training epochs. For hyper-parameters configuration, we
use a vocabulary size of 32 128. We configure RewardRepair to take
a maximum of 512input tokens from buggy and context code, and
generate a patch with a maximum of 100tokens. The learning rate
sets to 1eâˆ’4for both syntactic and semantic training. We configure
reward scaling values ğ‘ ğ‘–,ğ‘–âˆˆ{0,1,2,3,4}respectively to {-0.4, -0.2,
0.2, 0.4, 0.6} for the best experiment result. The encoder and decoder
consist of 6 layers. RewardRepair is configured by a beam size of
200and outputs the 200best patches per bug. We consider the beam
1510ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA He Ye, Matias Martinez, and Martin Monperrus
Phases Requirements Name & Source #Patches
Syntactic
trainingTokenizationCoCoNuT [35] 3 241 966
MegaDiff [44] 240 306
CodRep [9] 24 969
Semantic
trainingTokenization, CompilationBears [36] 123Developer and RGT Tests
TestingTokenization, Compilation,
Developer TestsDefects4J(v1.2) [23] 120
Defects4J(v2.0) [23] 257
Bugs.jar [49] 490
QuixBugs [31] 34
Table 1: Datasets used for the different steps of our experiment.
size of 200rather than 1000 used in CURE [ 22] and CoCoNuT [ 35]
due to the limitations of our available GPUs.
4 EXPERIMENTAL METHODOLOGY
In this section, we describe our methodology for evaluating Reward-
Repair by defining three research questions and how we propose
to answer them.
4.1 Research Questions
â€¢RQ1 (comparison with other tools): To what extent is Re-
wardRepair effective at repairing bugs compared with the
state-of-the-art repair approaches?
â€¢RQ2 (compilable rate): To what extent does RewardRepair
improve the compilability of generated patches?
â€¢RQ3 (impact of semantic training): To what extent does se-
mantic training improve the effectiveness of RewardRepair?
4.2 Dataset
Recall that we need three datasets for syntactic training, semantic
training and testing.
We have common criteria for both training and testing datasets:
1) all datasets are composed of bug-fix patches; 2) we focus on single-
hunk patches, where the patch is confined to a single contiguous
chunk of code, at a single location, per previous work [ 8,30,32];
3) we discard patches that do not make program behavior differ-
ences, e.g., those with only changes in comments or logging.
Next, we have specific requirements per dataset. Table 1 shows
the dataset of patches that we use for training and evaluating
RewardRepair. The first column indicates the phase where each
dataset is used. The second column gives the requirements for
each dataset. The third column gives the source of the dataset and
the fourth column indicates the number of patches in this dataset.
For example, as shown in the first row, RewardRepair is syntacti-
cally trained with data from three different sources, CoCoNuT [ 35],
Megadiff [44] and CodRep [9].
As aforementioned in Section 3, the selection criteria for se-
mantic training dataset are: 1) to be able to compile the patched
program; 2) to run the test cases on the patched program; 3) to be
able to automatically generate tests to specify the expected program
behavior. All criteria are met for 123 single-hunk bugs of the Bears
dataset [ 36], for which some available RGT tests were generated by
previous research [ 67]. It is to be noted that those criteria are verystrong, and neither CoCoNuT [ 35], MegaDiff [ 44] nor CodRep [ 9]
meets them, in particular, the patched program cannot be compiled.
To test RewardRepair, we use well-accepted datasets from pro-
gram repair research [ 11,33,37,68]: Defects4J [ 23], Bugs.jar [ 49],
and QuixBugs [ 31]. For all those bug datasets, the requirements of
compilation and test execution are met. In line with the most recent
work [ 76], we also consider the bugs of Defects4J version 2.0. After
filtering single-hunk bugs, we use 120 bugs from Defects4J (v1.2)
and 257 additional new bugs from Defects4J version 2.0 denoted as
Defects4J (v2.0) in our paper. We use the same single-hunk bugs
criteria for Bugs.jar and QuixBugs.
4.3 Methodology for RQ1
In RQ1, we compare RewardRepair against the state-of-the-art
neural repair approaches: CURE [ 22], Recoder [ 76], CoCoNuT [ 35],
and other approaches [ 21,32,38,50,51,63,64,66,73]. Per previous
studies [ 22,35], we take the quantitative results from the literature.
We run RewardRepair under two fault localization modes. First, we
use spectrum-based fault localization with Gzoltar [ 47] per previous
work [ 29,66]. Second, we assume that the fault has been localized,
an evaluation technique known as perfect fault localization and
extensively used in recent work [22, 33, 35].
We compute the two traditional APR performance metrics for
each testing dataset: 1)the number of bugs that are correctly re-
paired. In our paper, a patch is deemed correctly repaired if it meets
either of the two following criteria: it is identical to the developer
patch, or it is considered as correct by manual analysis done by
at least two authors; 2)the number of bugs that can be uniquely
repaired by individual repair approaches.
4.4 Methodology for RQ2
In RQ2, we calculate the compilable rate of RewardRepair. As com-
pilable rates were reported in SequenceR [ 8], CoCoNuT [ 35] and
CURE [ 22], we use the same benchmarks Defects4J (v1.2) and
QuixBugs as they do, and compare against the numbers reported
in the original papers. We also follow the existing work [ 22] and
compute the compilable rate depending on the beam size. We report
on beam sizes in 30, 100 and 200. We do not consider larger beams
due to the limitations of our available GPUs.
4.5 Methodology for RQ3
In RQ3, we conduct an ablation study with the goal of measuring
the effect of semantic training. To understand the contribution of
semantic training, we compare the effectiveness of RewardRepair
considering: 1)only syntactic training; 2)both syntactic and se-
mantic training. For doing this study, we apply the same protocol
as the one used for responding to the RQ1. We conduct manual
analysis on the unique bugs that are only repaired by including
semantic training and present the most interesting categories on
repair action changes.
5 EXPERIMENTAL RESULTS
5.1 RQ1: Comparative Study with Other Repair
Approaches
1511Neural Program Repair with Execution-based Backpropagation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Approaches D4J(v1.2) D4J(v2.0) Bugs.jar QuixBugs
120 bugs 257 bugs 490 bugs 34 bugs
Using Spectrum-based Fault Localization
jGenProg [38] 5 - - 1
Nopol [66] 5 - - 2
Elixir [50] 26 - 22 -
sharpFix [64] 27 - 15 -
SimFix [21] 27 2 - -
Hercules [51] 33 - - -
Recoder [76] 39 19 - 17
RewardRepair (this paper) 29 24 42 19
Assuming Perfectly Localized Fault
SequenceR [8] 14 - - -
DLFix [30] 33 - - -
TBar [33] 33 8 - -
CoCoNuT [35] 33 - - 12
CURE [22] 45 - - 24
Recoder [76] 52 - - -
RewardRepair (this paper) 45 45 97 20
RewardRepair Unique 5 34 78 4
Table 2: Comparison of RewardRepair against the related work, the
numbers from the related work are filtered by single-hunk bugs.
Across all benchmarks, RewardRepair correctly fixes 207 bugs and
uniquely fixes 121 ones. We use 4 testing benchmarks to maximize
generalizability.
if( _dataFormatReaders != null ) {
return _detectBindAndReadValues ( _dataFormatReaders . findFormat (
src , offset , length ), false );}
- return _bindAndReadValues(_considerFilter(_parserFactory.createParser(src),
+ return _bindAndReadValues(_considerFilter(_parserFactory.createParser
+ (src,offset,length),
true ));
Listing 2: RewardRepair correct patch for Defects4J(v2.0) Jackson-
Databind_57
Table 2 shows the patch generation results of RewardRepair and
12 other APR approaches on four benchmarks: the two versions of
Defects4J, Bugs.jar and QuixBugs. The numbers are the correctly
repaired bugs by each APR approach. The results are those reported
in the literature, by the authors of the tool or by subsequent com-
parative experiments [ 33,68]. A â€˜-â€™ indicates that the APR approach
has not been evaluated on the considered benchmark, to the best
of our knowledge. Note that the first seven APR approaches were
executed with spectrum-based fault localization (FL), where the
later approaches assumed perfect FL. We measure RewardRepairâ€™s
effectiveness with both spectrum-based and perfect FL. Next, we fo-
cus on the comparison under perfect FL as the most state-of-the-art
techniques only report effectiveness with perfect FL.
Repaired bugs. Overall, RewardRepair is able to correctly re-
pair 45 of 120 bugs on Defects4J (v1.2), 45 of 257 bugs on Defects4J
(v2.0), 97 of 490 bugs on Bugs.jar and 20 of 34 bugs on QuixBugs
benchmark. From these results, we make the following observa-
tions.
Figure 2: Uniquely repaired bugs on Defects4J (v1.2).
private void removeUnreferencedFunctionArgs (...) {
+ if (!removeGlobals) {
+ return;
+ }
Listing 3: RewardRepair patch for Closure-1 from Defects4J(v1.2),
identical to the developer patch.
RewardRepair outperforms all APR approaches in two bench-
marks: Defects4J (v2.0) and Bugs.jar. RewardRepair sets new base-
lines of repaired bugs for these two benchmarks. While the majority
of APR papers showcase bugs from version 1.0 or 1.2 of Defects4J,
Listing 2 gives the correct RewardRepair patch for a Defects4J (v2.0)
bug: JacksonDatabind_57 . As shown, RewardRepair succeeds in
reusing the surrounding variables offset andlength to construct
the parameter list for overridden method ğ‘ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘’ğ‘ƒğ‘ğ‘Ÿğ‘ ğ‘’ğ‘Ÿ . Recoder
[76], which is so far the best tool evaluated on Defects4J (v2.0) fails
at repairing this bug.
In addition, RewardRepair achieves the top-2 performance on
Defects4J (v1.2) and QuixBugs benchmarks. RewardRepair performs
better than all APR approaches on Defects4J (v1.2) but Recoder. Re-
garding Recoderâ€™s performance on Defects4J (v1.2), we analyze the
bugs that cannot be repaired by RewardRepair. We find that three
bugs ( Closure-14 ,Closure-104 , and Closure-118 ) require fixing tokens
outside the considered buggy class and three bugs ( Lang-26 ,Lang-
43andClosure-33 ) require fixing tokens outside the context code
scope as implemented by RewardRepair. This analysis suggests that
RewardRepair could achieve better performance by enlarging the
context code scope. Regarding CUREâ€™s performance on QuixBugs,
the reason is likely the beam size: recall that CURE generates 10,000
candidate patches for each bug, while RewardRepair generates 200
patches per bug. As shown in previous research [ 55], a larger beam
size leads to more correct patches.
Uniquely repaired bugs. Let us now focus on the last row
of Table 2, which gives the number of uniquely repaired bugs
by RewardRepair. RewardRepair respectively repairs 5, 34, 78 and
4 unique bugs for Defects4J (v1.2), Defects4J (v2.0), Bugs.jar and
1512ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA He Ye, Matias Martinez, and Martin Monperrus
Model Top-30 Top-100 Top-200
SequenceR [8] 33% - -
CoCoNuT [35] 24% 15% 6%-15%
CURE [22] 39% 28% 14%-28%
RewardRepair 45.3% 37.5% 33.1%
Table 3: Average compilable rates of the Top-K candidate patches in
Defects4J(v1.2) and QuixBugs. â€˜-â€™ indicates data unavailability.
QuixBugs, all of which were never repaired by any other APR ap-
proaches in the literature. This shows RewardRepair complements
all the existing works on the four considered benchmarks.
Figure 2 gives the detailed uniqueness analysis on Defects4J (v1.2)
with the state-of-the-art APR approaches. We give the exact results
for the most recent neural repair approaches (CoCoNuT, CURE
and Recoder). We combine the rest of the top-ranked related work
in a unique bin for sake of readability. As shown, RewardRepair
fixes 5 unique bugs compared with the other APR approaches on
Defects4J (v1.2). Notably, they come from three different Defects4J
projects (Closure, Lang and Math) showing that the additional
learned knowledge is not specific to one single domain.
Listing 3 gives the RewardRepair patch for Closure-1 , which can
only be fixed by RewardRepair. This is a patch with an addition of an
ifblock, including the code of the then statement. RewardRepair
learns the ifcondition from the given context code. While pattern-
based repair is able to synthesize conditions (e.g., TBar [ 32]), no
pattern-based repair systems have this complete if/then/return
pattern. The recent neural repair models, CoCoNuT and CURE, do
not generate this patch, we suspect that with a strict token-based
cross-entropy optimization, Recoder does not learn such a complex
patch structure. RewardRepair is the first to produce this addition-
only patch based on a non-trivial if/then/return structure.
Generalizability . Durieux et al. [ 11] revealed the phenomenon
of â€œbenchmark overfitting" in program repair, meaning that perfor-
mance results reported in the APR literature do not generalize to
other benchmarks. The main reason is that APR approaches were
typically evaluated in a single dataset, in particular, Defects4J (v1.2)
in Java evaluation. RewardRepair is evaluated on four benchmarks
in order to maximize the generalizability of our claims. To our
knowledge, this is one of the experiments with the largest num-
ber of testing benchmarks used for assessing the proposed repair
approach.
Answer to RQ1: RewardRepair correctly fixes 45, 45, 97 and
20 bugs on the considered Java benchmarks Defects4J (v1.2),
Defects4J (v2.0), Bugs.jar and QuixBugs, respectively. There
are 121 unique bugs that are repaired by RewardRepair for the
first time ever w.r.t. the APR literature. The external validity
of our results is founded on 4 testing benchmarks.
Figure 3: RewardRepair compilable rate on Defects4J (v1.2) by
project.
for ( FormattingOption formattingOption : flags . formatting ) {
formattingOption . applyToOptions ( options ); }
- if (flags.process_closure_primitives) {
- options.closurePass = true;
- }
+ options.closurePass = flags.process_closure_primitives;
initOptionsFromFlags ( options );
return options ;
}
Listing 4: RewardRepair correct patch generated for Closure-101
5.2 RQ2: Improvement of Compilable Rate
Table 3 shows the average compilable rates of the top-k candi-
date patches where k relates to the beam size. For sake of a fair
comparison, we use the same methodology as Jiang et al. [ 22] and
we combine Defects4J (v1.2) and QuixBugs together (our appendix
website gives the results per benchmark [ 12]). We provide ranges
for CoCoNuT and CURE regarding the top-200 results due to data
unavailability: the range is the bracket [top-100, top-1000] of com-
pilable rates as reported in the original paper of CURE [22].
Notably, the compilable rate of RewardRepair outperforms the
three considered approaches CURE, CoCoNuT and SequenceR for
all beam size configurations. In the best case, RewardRepair achieves
a compilable rate of up to 45.3%, which is the highest among the
three considered beam sizes. Next, we see that the compilable rate
of RewardRepair decreases when we increase the beam size, this
result is consistent with the ones of CURE and CoCoNuT. Since
the beam enumerates by decreasing probability, it suggests that the
learned neural model does capture compilability and favors it.
Recall that the key contribution of CURE [ 22] is to introduce two
strategies to increase the patch compilable rate of NMT based neural
repair models: valid-identifier checker strategy andlength-control
strategy . Both strategies work in the inference phase, by filtering
out the invalid tokens in the Java code or patches that are not close
in length to the buggy code. This means that CURE does not embed
program-specific knowledge in the neural network. On the contrary,
RewardRepair learns this knowledge during semantic training. That
is if an invalid identifier is used during semantic training and results
in a non-compilable patch, RewardRepair punishes the patch by
increasing the loss. Beyond identifiers, RewardRepair is also able to
learn other programming knowledge during semantic training, such
as structure and typing constraints related to the domain classes
1513Neural Program Repair with Execution-based Backpropagation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Compile Errors No. Failures
cannot find symbol 606
illegal start of expression 329
no suitable method constructor found for... 132
incompatible types 123
not a statement 102
";" expected 76
unreachable statement 63
case, default,or } expected 59
incomparable types 56
method X in class Y cannot be applied to given types 53
Table 4: Analysis of top-10 reasons for uncompilable generated
patches.
Model D4J(v1.2) D4J(v2.0) Bugs.jar QuixBugs Total
RewardRepair
(Syntatic)42 40 93 18 193
RewardRepair
(Syntatic+Semantic)45 45 97 20 207
Table 5: Ablation study w.r.t correct patches.
and methods. To some extent, CURE is limited to the static analysis
checks devised and implemented by its authors while RewardRepair
works in a fully agnostic, data-driven way to identify important
compilation constraints.
We analyze the 1883 uncompilable patches from the top-30
patches generated for Defects4J (v1.2). We show the most frequent
10 compilation errors in Table 4. If there are multiple errors for
one patch, we only count the first error per patch. The first column
gives the compilation error type and the second column shows the
number of patches that fail on the corresponding error type. We
see most common error types are related to semantics that very
few errors are related to syntax (the first one being ";" expected).
To overcome the typing problems, this suggests doing some spe-
cific training on the project under repair to capture this missing
knowledge [69].
Listing 4 shows the RewardRepair patch for bug Closure-101
from Defects4J (v1.2), which is identical to the developer patch.
RewardRepair incorporates two repair actions in this patch: First,
RewardRepair removes the if/then condition. Second, RewardRepair
generates a new statement by combining the logical expression
from the if-condition and from the statement in the then block. This
is arguably a complex patch, and no repair system has reported
generating a patch for this bug. This case shows that RewardRepair
generates a compilable and correct patch with the complex struc-
ture. CURE fails to generate this patch because of its length-control
strategy that encourages patches similar to the buggy code in length.
Figure 3 shows the compilable rate of RewardRepair per project
of Defects4J (v1.2) with top 30, 100 and 200 candidate patches ac-
cording to beam. We make the two observations as follows: First,
for all projects, increasing the beam size of RewardRepair decreases
the compilable rate for each project. This confirms the conclusion
made in Table 3 at the level of aggregate results over bugs and
benchmarks. Second, the range of compilable rates over bugs de-
creases with beam size, both the range of extreme values (whiskers)Actions Syntactic Training Semantic Training
Add if conditions 1852 2316
Method invocation 1040 1352
Add return statement 1123 1554
Ternary operator for null checking 0 212
Table 6: Examples of repair action differences between syntactic
training and semantic training of RewardRepair.
- this.dataset = dataset;
(a) Buggy line
+ .setDataSet(dataset) // patch 1
+ setDataset( ); // patch 2
(b) Two non-compilable patches by syntactic training
+ setDataset(dataset);
(c) Correct compilable patch by semantic training
Listing 5: Bug Chart-12 only fixed by semantic training
and the range of interquartile values (boxes). We explain this by
statistical sampling (sampling 30 items yields less stable results than
sampling 200). However, it may also be that the compilable rate
does change significantly for some bugs. This latter explanation is
supported by the fact that there is a clear difference in compilable
rate depending on the project (Lang patches compile much more
than Time patches). This latter phenomenon â€“ the compilable rate
significantly varying over projects, in the worst case being 0% â€“ is
a yet unknown limitation of neural program repair and suggests
more future research on this to increase the compilable rate in a
more uniform way.
Answer to RQ2: For all considered beam sizes, RewardRepair
improves the compilable rate over the state-of-the-art. Over
all benchmarks, RewardRepair reaches up to 45.3% of compil-
able patches (approximately one out of two patches compile),
showing that the RewardRepair neural model has captured
important information w.r.t. compilation.
5.3 RQ3: Impact of Semantic Training
Table 5 shows the results of the ablation study w.r.t semantic train-
ing. Per the same protocol as RQ1, the considered metric is the
number of correct patches. The first row shows RewardRepairâ€™s
effectiveness with only syntactic training, the second row shows
RewardRepair with both syntactic and semantic training. For exam-
ple, RewardRepair with only syntactic training generates 42 correct
patches on Defects4J (v1.2), and RewardRepair with semantic train-
ing generates 45 correct patches. Overall, the addition of semantic
training after syntactic training does yield more correct patches
on all considered benchmarks. This shows that semantic training
addresses the limits of syntactic training, and the improvement is
not tied to specific benchmarks.
To better understand the effectiveness of semantic training, we
manually analyze those unique bugs that are only generated by
RewardRepair by including semantic training (and not generated
with pure syntactic training). This leads to an analysis of 14 800
patches (7 400 candidate patches by syntactic training and 7 400
1514ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA He Ye, Matias Martinez, and Martin Monperrus
candidate patches by semantic training). We group those patches by
category and summarize the most interesting categories in Table 6.
The first column gives the type of repair action employed in the
patch, and the numbers in the second column and the third column
indicate the numbers of patches based on those repair actions from
syntactic and semantic training respectively. For example, the first
row shows there are 1 852 patches generated by syntactic training
which add if/then statements, while semantic training yields 2 316
patches using this construct. This means that the usage of if/then
statements is increased by 25.1% with semantic training. Notably,
the unique bugs that benefit from semantic training come from dif-
ferent projects, showing that adding semantic training is beneficial
in general.
Recall that syntactic training is done on more than 3 million
training samples, while semantic training is done on 123 training
samples, which is much less. This suggests that the improvement
obtained with semantic training does not come from the number of
additional training points, but more from the training process de-
scribed in Section 3.5. RewardRepairâ€™s loss function is able to better
optimize the neural network, improving the quality of generated
patches, with only a few training data points.
Finally, Listing 5 discusses the case of Chart-12 . The first line is
the buggy line. Next, in part (b), two patches after syntactic training
are shown, they are both close to the correct patch but none of them
compile (extraneous dot in the first patch and missing parameter
in the second patch). Finally, part (c) shows the correct patch by
RewardRepair with semantic training, which is identical to the
developer patch. In this case, it suggests that the neural model with
semantic training has understood that leading dots before method
calls is not correct per the Java grammar, and that setDataset
is a method likely to take a parameter called dataset . A subtle
character may lead to a huge difference in program execution, but
this knowledge is hard to be obtained by syntactic training with
cross-entropy loss.
Answer to RQ3: Our ablation study shows semantic train-
ing of RewardRepair contributes to improving the overall
effectiveness in terms of correctly fixed bugs.
6 DISCUSSION
6.1 Impact of Inference Beam Size
We investigate the impact of beam search size in the inference time,
and our experiment shows a bigger beam size indeed leads to more
correct patches generated, which confirms the study of Tufano et
al. [56]. We provide the results of configuring beam size as 500 in
our online appendix repository [12].
6.2 Threats to Validity
A threat to external validity relates to whether the performance
of RewardRepair generalizes to arbitrary programming languages.
Per the standards of the field, our approach has been tested in one
language (Java) and the evaluation is carried out on established
benchmarks. In principle, our approach can be applied to other
programming languages and datasets. A threat to internal validity
relates to the hyper-parameter configuration we adopted. To ensurereplicability and extension, we make all the source code and results
publicly available for future research [12].
7 RELATED WORK
7.1 Automatic Program Repair
A decade of research has generated a rich body of work on auto-
matic program repair [ 14,43]. We have already discussed neural
repair approaches [ 5,8,19,22,30,35,55,56,76] in Section 2. These
approaches only use the syntactic cross-entropy loss objective,
which poses a discrepancy between the training objective of gen-
erating compilable correct patches and the loss criterion. The key
novelty of RewardRepair is the discriminative model to capture the
compilation and execution knowledge during model training and
backpropagation.
We mentioned generate and validate (G&V) program repair ap-
proaches in RQ1 (Section 5.1). Other notable G&V systems include
[6,15,38,73]. Moreover, the third line of research is about synthesis-
based repair [ 13,26,39,40,52,65,66] which converts the search
problem to a satisfiability problem. All these approaches often work
by extracting a repair constraint typically via symbolic execution
incorporated with human knowledge for patch generation. On the
contrary, RewardRepair automatically learns such fix operators,
language grammar and semantics from the training corpus.
In the field of APR, the recent work of Jiang et al. [ 22] is the
most closely related to ours, also focusing on the non-compilable
patch problem. They address this problem by employing a valid-
identifier checker in the inference stage to filter invalid tokens.
However, many reasons could lead to a non-compilable patch, and
the presence of invalid identifiers is only one of them. Our approach
is fundamentally different: 1) RewardRepair works at training time
and not at inference time; 2) RewardRepair is based on the actual
compilation and test execution of training patches; 3) RewardRepair
encourages syntactic diversity while CURE encourages patches
similar to the buggy code (the length-control strategy in CURE).
7.2 Discriminators for Machine Learning on
Code
Several works propose deep learning on code based on a discrim-
inator [ 2,20] where the discriminator provides a loss that solves
the discrepancy between the generated and real distributions of
the object under study, as pioneered by generative adversarial net-
works (GAN)[ 17]. Harer et al. [ 20] propose an adversarial learning
approach to solve software vulnerabilities. Alhefdhi et al. [ 2] lever-
age a similar GAN architecture to suggest repairs that are as close
as possible to human-written repairs. RewardRepair shares the
concepts of employing a traditional NMT model as a generator,
and of replacing the cross-entropy loss with the feedback from a
discriminator. The key difference between this related work and
ours is that our discriminator uses execution information, through
compilation and test execution.
7.3 Improving Backpropagation
Past research has improved the cross-entropy loss based on domain-
specific knowledge. In neural machine translation, Zhang et al. [ 74]
1515Neural Program Repair with Execution-based Backpropagation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
show the limitation of considering cross-entropy loss and its ten-
dency to overcorrect synonymous words and phrases. To relieve
the problem, further research [ 34,74] proposed to combine cross-
entropy loss and add translation evaluation at the sentence level.
In object detection, Ryou et al. [ 48] proposed AnchorLoss to dy-
namically rescale the cross-entropy based on prediction difficulty.
Loss scaling is a technique used in floating-point optimization,
consisting of scaling up the loss value up before the start of back-
propagation [42, 75].
7.4 Training based on Execution
Recently, semantic information has been used in program synthe-
sis tasks. Chen et al. [ 7] and Gupta et al. [ 18] propose execution-
guided synthesis leveraging the semantics of the language. These
approaches execute a partial program to obtain intermediate states
to guide program synthesis. Wang et al. [ 60] use dynamic infor-
mation from execution to measure semantic redundancy between
student programs. Mesbah et al. [ 41] extract compiler diagnostic
information as an input source for repairing compilation errors.
As in our work, these approaches use execution information as an
additional input for the considered model. The key difference is that
none of them employ the execution information as a reward signal
to update the neural network weights through backpropagation.
Wang and colleagues [ 59,61] leverage the full execution traces
to learn neural semantic program embeddings. These related works
improve the code representation based on semantic information.
Our novelty is not on the representation, but on the training objec-
tive improvement, which is not addressed in [59, 61].
8 CONCLUSION
We have presented a novel neural program repair model Reward-
Repair based on compilation and test execution. The key idea is to
employ a discriminative model to provide a reward signal on the
generated patches according to the actual execution outcome. This
signal modulates the purely syntactic cross-entropy loss function
in what we call semantic training. We have conducted an extensive
empirical evaluation, including a comprehensive experiment on
the widely used benchmark Defects4J, Bugs.jar and QuixBugs. Our
results show that it is possible to embed execution information in
the backpropagation process to improve neural program repair.
9 ACKNOWLEDGMENTS
We thank the anonymous reviewers for the insightful feedback.
This work was supported by the Wallenberg AI, Autonomous Sys-
tems and Software Program (WASP) funded by the Knut and Al-
ice Wallenberg Foundation. Some experiments were performed
on resources provided by the Swedish National Infrastructure for
Computing.
REFERENCES
[1]Rui Abreu, Peter Zoeteweij, and Arjan J.C. van Gemund. 2007. On the Accuracy of
Spectrum-based Fault Localization. In Testing: Academic and Industrial Conference
Practice and Research Techniques - MUTATION (TAICPART-MUTATION 2007) . 89â€“
98.
[2]Abdulaziz Alhefdhi, Hoa Khanh Dam, Xuan-Bach D. Le, and Aditya Ghose. 2020.
Adversarial Patch Generation for Automatic Program Repair. arXiv:2012.11060
[3]Dzmitry Bahdanau, Kyunghyun Cho, and Y. Bengio. 2014. Neural Machine
Translation by Jointly Learning to Align and Translate. ArXiv 1409 (09 2014).[4]B. Baudry, Zimin Chen, K. Etemadi, Han Fu, Davide Ginelli, Steve Kommrusch,
Matias Martinez, Monperrus Martin, Javier Ron, He Ye, and Zhongxing Yu. 2021.
A Software-Repair Robot Based on Continual Learning. IEEE Software 38 (2021),
28â€“35.
[5]S. Chakraborty, Y. Ding, M. Allamanis, and B. Ray. 2020. CODIT: Code Editing
with Tree-Based Neural Models. IEEE Transactions on Software Engineering (2020).
https://doi.org/10.1109/TSE.2020.3020502
[6]L. Chen, Y. Pei, and C. A. Furia. 2017. Contract-based program repair without
the contracts. In 2017 32nd IEEE/ACM International Conference on Automated
Software Engineering (ASE) .
[7]Xinyun Chen, Chang Liu, and Dawn Song. 2019. Execution-Guided Neural
Program Synthesis. In International Conference on Learning Representations .
[8]Z. Chen, S. J. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk, and M. Mon-
perrus. 2019. SEQUENCER: Sequence-to-Sequence Learning for End-to-End
Program Repair. IEEE Transactions on Software Engineering (2019).
[9]Zimin Chen and Martin Monperrus. 2018. The CodRep Machine Learning on
Source Code Competition . Technical Report 1807.03200. arXiv. http://arxiv.org/
pdf/1807.03200
[10] Zimin Chen and Martin Monperrus. 2019. A Literature Study of Embeddings on
Source Code. arXiv:1904.03061
[11] Thomas Durieux, Fernanda Madeiral, Matias Martinez, and Rui Abreu. 2019.
Empirical Review of Java Program Repair Tools: A Large-scale Experiment on
2,141 Bugs and 23,551 Repair Attempts. In Proceedings of the 2019 27th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering . ACM, 302â€“313. https://doi.org/10.1145/
3338906.3338911
[12] RewardRepair Experiment. 2022. Appendix Repository. https://github.com/
SophieHYe/RewardRepair
[13] Xiang Gao, Sergey Mechtaev, and Abhik Roychoudhury. 2019. Crash-Avoiding
Program Repair. In Proceedings of the 28th ACM SIGSOFT International Symposium
on Software Testing and Analysis (Beijing, China) (ISSTA 2019) . Association for
Computing Machinery, New York, NY, USA, 8â€“18. https://doi.org/10.1145/
3293882.3330558
[14] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2017. Automatic Software
Repair: A Survey. IEEE Transactions on Software Engineering (2017).
[15] Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical Program
Repair via Bytecode Mutation. In Proceedings of the 28th ACM SIGSOFT In-
ternational Symposium on Software Testing and Analysis (Beijing, China) (IS-
STA 2019) . Association for Computing Machinery, New York, NY, USA, 19â€“30.
https://doi.org/10.1145/3293882.3330559
[16] Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, and Omer Levy.
2020. Aligned Cross Entropy for Non-Autoregressive Machine Translation.
arXiv:2004.01655 [cs.CL]
[17] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets (NIPSâ€™14) . MIT Press, Cambridge, MA, USA, 2672â€“2680.
[18] Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn Song. 2020. Syn-
thesize, Execute and Debug: Learning to Repair for Neural Program Synthesis
(NIPSâ€™20) .
[19] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence (San Francisco, California,
USA) (AAAIâ€™17) . AAAI Press, 1345â€“1351.
[20] Jacob A. Harer, Onur Ozdemir, Tomo Lazovich, Christopher P. Reale, Rebecca L.
Russell, Louis Y. Kim, and Peter Chin. 2018. Learning to Repair Software Vulner-
abilities with Generative Adversarial Networks (NIPSâ€™18) . Curran Associates Inc.,
Red Hook, NY, USA, 7944â€“7954.
[21] Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen. 2018.
Shaping Program Repair Space with Existing Patches and Similar Code (ISSTA) .
[22] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural Ma-
chine Translation for Automatic Program Repair. In Proceedings of the ACM/IEEE
43rd International Conference on Software Engineering .
[23] Rene Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J: A database of
existing faults to enable controlled testing studies for Java programs. In Proceed-
ings of the 2014 International Symposium on Software Testing and Analysis . ACM,
437â€“440.
[24] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and An-
drea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for source
code. In 2020 IEEE/ACM 42nd International Conference on Software Engineering
(ICSE) . IEEE, 1073â€“1085.
[25] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language
independent subword tokenizer and detokenizer for Neural Text Processing. In
Empirical Methods in Natural Language Processing(EMNLP) .
[26] Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: Syntax- and Semantic-guided Repair Synthesis via Programming by
Examples. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering (ESEC/FSE 2017) .
1516ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA He Ye, Matias Martinez, and Martin Monperrus
[27] Xuan-Bach D. Le, Ferdian Thung, David Lo, and Claire Le Goues. 2018. Overfit-
ting in Semantics-based Automated Program Repair. In Proceedings of the 40th
International Conference on Software Engineering (Gothenburg, Sweden) (ICSE
â€™18). ACM, New York, NY, USA.
[28] Xuan-Bach D. Le, Lingfeng Bao, David Lo, Xin Xia, and Shanping Li. 2019. On
Reliability of Patch Correctness Assessment. In Proceedings of the 41st ACM/IEEE
International Conference on Software Engineering .
[29] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012.
GenProg: A generic method for automatic software repair. Software Engineering,
IEEE Transactions on 38, 1 (2012), 54â€“72. https://doi.org/10.1109/TSE.2011.104
[30] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: Context-Based Code
Transformation Learning for Automated Program Repair. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering (Seoul, South
Korea) (ICSE â€™20) . Association for Computing Machinery, New York, NY, USA,
602â€“614. https://doi.org/10.1145/3377811.3380345
[31] Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. 2017.
QuixBugs: A Multi-Lingual Program Repair Benchmark Set Based on the Quixey
Challenge. In Proceedings Companion of the 2017 ACM SIGPLAN International
Conference on Systems, Programming, Languages, and Applications: Software for
Humanity (Vancouver, BC, Canada) (SPLASH Companion 2017) . Association for
Computing Machinery, New York, NY, USA, 55â€“56. https://doi.org/10.1145/
3135932.3135941
[32] Kui Liu, Anil Koyuncu, Dongsun Kim, and TegawendÃ© F. BissyandÃ©. 2019. TBar:
Revisiting Template-based Automated Program Repair. In Proceedings of the 28th
ACM SIGSOFT International Symposium on Software Testing and Analysis . ACM,
31â€“42. https://doi.org/10.1145/3293882.3330577
[33] Kui Liu, Shangwen Wang, Anil Koyuncu, Kisub Kim, TegawendÃ© F. BissyandÃ©,
Dongsun Kim, Peng Wu, Jacques Klein, Xiaoguang Mao, and Yves Le Traon. 2020.
On the Efficiency of Test Suite Based Program Repair: A Systematic Assessment of
16 Automated Repair Systems for Java Programs. In Proceedings of the ACM/IEEE
42nd International Conference on Software Engineering (Seoul, South Korea) (ICSE
â€™20). Association for Computing Machinery, New York, NY, USA, 615â€“627. https:
//doi.org/10.1145/3377811.3380338
[34] Wenjie Lu, Leiying Zhou, Gongshen Liu, and Quanhai Zhang. 2020. A Mixed
Learning Objective for Neural Machine Translation. In Chinese Computational
Linguistics , Maosong Sun, Sujian Li, Yue Zhang, Yang Liu, Shizhu He, and Gaoqi
Rao (Eds.). Springer International Publishing, Cham, 201â€“213.
[35] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and
Lin Tan. 2020. CoCoNuT: Combining Context-Aware Neural Translation Models
Using Ensemble for Program Repair (ISSTA 2020) .
[36] Fernanda Madeiral, Simon Urli, Marcelo Maia, and Martin Monperrus. 2019.
Bears: An Extensible Java Bug Benchmark for Automatic Program Repair Studies.
InProceedings of the 26th IEEE International Conference on Software Analysis,
Evolution and Reengineering (SANER â€™19) . https://arxiv.org/abs/1901.06024
[37] Matias Martinez, Thomas Durieux, Romain Sommerard, Jifeng Xuan, and Martin
Monperrus. 2017. Automatic Repair of Real Bugs in Java: A Large-scale Exper-
iment on the Defects4J Dataset. Empirical Software Engineering 22, 4 (2017),
1936â€“1964. https://doi.org/10.1007/s10664-016-9470-4
[38] Matias Martinez and Martin Monperrus. 2016. ASTOR: A Program Repair Library
for Java. In Proceedings of ISSTA .
[39] S. Mechtaev, J. Yi, and A. Roychoudhury. 2015. DirectFix: Looking for Simple
Program Repairs. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering , Vol. 1. 448â€“458. https://doi.org/10.1109/ICSE.2015.63
[40] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
Multiline Program Patch Synthesis via Symbolic Analysis. In 2016 IEEE/ACM
38th International Conference on Software Engineering (ICSE) .
[41] Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandil-
ian. 2019. DeepDelta: Learning to Repair Compilation Errors. In Proceedings of
the 2019 27th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (Tallinn, Estonia)
(ESEC/FSE 2019) . Association for Computing Machinery, New York, NY, USA,
925â€“936. https://doi.org/10.1145/3338906.3340455
[42] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, and Hao Wu. 2018. Mixed Precision Training. In International Confer-
ence on Learning Representations . https://openreview.net/forum?id=r1gs9JgRZ
[43] Martin Monperrus. 2017. Automatic Software Repair: a Bibliography. ACM
Computing Surveys 51 (2017), 1â€“24. https://doi.org/10.1145/3105906
[44] Martin Monperrus, Matias Martinez, He Ye, Fernanda Madeiral, Thomas Durieux,
and Zhongxing Yu. 2021. Megadiff: A Dataset of 600k Java Source Code Changes
Categorized by Diff Size. arXiv:2108.04631 [cs.SE]
[45] Md Rafiqul Islam Rabin, Nghi D.Q. Bui, Ke Wang, Yijun Yu, Lingxiao Jiang, and
Mohammad Amin Alipour. 2021. On the generalizability of Neural Program Mod-
els with respect to semantic-preserving program transformations. Information
and Software Technology 135 (2021), 106552.
[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits
of Transfer Learning with a Unified Text-to-Text Transformer. Journal of MachineLearning Research 21, 140 (2020), 1â€“67. http://jmlr.org/papers/v21/20-074.html
[47] AndrÃ© Riboira and Rui Abreu. 2010. The GZoltar Project: A Graphical Debugger
Interface (TAIC PARTâ€™10) . Springer-Verlag, Berlin, Heidelberg.
[48] Serim Ryou, Seong-Gyun Jeong, and P. Perona. 2019. Anchor Loss: Modulating
Loss Scale Based on Prediction Difficulty. 2019 IEEE/CVF International Conference
on Computer Vision (ICCV) (2019), 5991â€“6000.
[49] Ripon K. Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, and Mukul R. Prasad.
2018. Bugs.Jar: A Large-Scale, Diverse Dataset of Real-World Java Bugs (MSR
â€™18). Association for Computing Machinery, New York, NY, USA, 10â€“13. https:
//doi.org/10.1145/3196398.3196473
[50] Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R. Prasad. 2017. ELIXIR:
Effective Object Oriented Program Repair. In Proceedings of the 32Nd IEEE/ACM
International Conference on Automated Software Engineering (Urbana-Champaign,
IL, USA) (ASE 2017) . IEEE Press, Piscataway, NJ, USA, 648â€“659.
[51] Seemanta Saha, Ripon K. Saha, and Mukul R. Prasad. 2019. Harnessing Evolution
for Multi-Hunk Program Repair. In Proceedings of the 41st International Conference
on Software Engineering (Montreal, Quebec, Canada) (ICSE â€™19) . IEEE Press, 13â€“24.
https://doi.org/10.1109/ICSE.2019.00020
[52] Ridwan Shariffdeen, Yannic Noller, Lars Grunske, and Abhik Roychoudhury. 2021.
Concolic Program Repair. In 42nd ACM SIGPLAN Conference on Programming
Language Design and Implementation (PLDI) .
[53] Edward K. Smith, Earl T. Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the
Cure Worse Than the Disease? Overfitting in Automated Program Repair. In
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
(ESEC/FSE 2015) .
[54] Haoye Tian, Kui Liu, Abdoul Kader KaborÃ©, Anil Koyuncu, Li Li, Jacques Klein,
and TegawendÃ© F. BissyandÃ©. 2020. Evaluating Representation Learning of Code
Changes for Predicting Patch Correctness in Program Repair. In Proceedings of
the 35th IEEE/ACM International Conference on Automated Software Engineering .
IEEE, 981â€“992. https://doi.org/10.1145/3324884.3416532
[55] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and
Denys Poshyvanyk. 2019. On Learning Meaningful Code Changes via Neu-
ral Machine Translation. In Proceedings of the 41st International Conference on
Software Engineering (Montreal, Quebec, Canada) (ICSE â€™19) . IEEE Press, 25â€“36.
https://doi.org/10.1109/ICSE.2019.00021
[56] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-Fixing
Patches in the Wild via Neural Machine Translation. ACM Trans. Softw. Eng.
Methodol. 28, 4, Article 19 (Sept. 2019), 29 pages. https://doi.org/10.1145/3340544
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems , Vol. 30.
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is All You Need. In Proceedings of the 31st International Conference on Neural
Information Processing Systems (Long Beach, California, USA) (NIPSâ€™17) . Curran
Associates Inc., Red Hook, NY, USA, 6000â€“6010.
[59] Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Dynamic Neural Program
Embedding for Program Repair (ICLR) .
[60] Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Search, Align, and Repair: Data-
Driven Feedback Generation for Introductory Programming Exercises. SIGPLAN
Not. 53, 4 (June 2018), 481â€“495. https://doi.org/10.1145/3296979.3192384
[61] Ke Wang and Zhendong Su. 2020. Blended, Precise Semantic Program Em-
beddings. In Proceedings of the 41st ACM SIGPLAN Conference on Programming
Language Design and Implementation (London, UK) (PLDI 2020) . Association for
Computing Machinery, New York, NY, USA, 121â€“134. https://doi.org/10.1145/
3385412.3385999
[62] Shangwen Wang, Ming Wen, Bo Lin, Hongjun Wu, Yihao Qin, Deqing Zou,
Xiaoguang Mao, and Hai Jin. 2020. Automated Patch Correctness Assessment:
How Far are We?. In Proceedings of the 35th International Conference on Automated
Software Engineering (ASE) . ACM.
[63] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018.
Context-Aware Patch Generation for Better Automated Program Repair. In Pro-
ceedings of the 40th International Conference on Software Engineering (ICSE â€™18) .
[64] Qi Xin and Steven Reiss. 2019. Better Code Search and Reuse for Better Program
Repair. In 2019 IEEE/ACM International Workshop on Genetic Improvement (GI) .
10â€“17. https://doi.org/10.1109/GI.2019.00012
[65] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and Lu
Zhang. 2017. Precise Condition Synthesis for Program Repair. In Proceedings of the
39th International Conference on Software Engineering (Buenos Aires, Argentina)
(ICSE â€™17) . IEEE Press, 416â€“426. https://doi.org/10.1109/ICSE.2017.45
[66] Jifeng Xuan, Matias Martinez, Favio Demarco, Maxime ClÃ©ment, Sebastian Lame-
las, Thomas Durieux, Daniel Le Berre, and Martin Monperrus. 2016. Nopol:
Automatic Repair of Conditional Statement Bugs in Java Programs. IEEE Trans-
actions on Software Engineering (2016).
[67] He Ye, Jian Gu, Matias Martinez, Thomas Durieux, and Martin Monperrus. 2021.
Automated Classification of Overfitting Patches with Statically Extracted Code
Features. IEEE Transactions on Software Engineering (2021). https://doi.org/10.
1517Neural Program Repair with Execution-based Backpropagation ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
1109/tse.2021.3071750
[68] He Ye, Matias Martinez, Thomas Durieux, and Martin Monperrus. 2021. A
comprehensive study of automatic program repair on the QuixBugs benchmark.
Journal of Systems and Software 171 (2021), 110825. https://doi.org/10.1016/j.jss.
2020.110825
[69] He Ye, Matias Martinez, Xiapu Luo, Tao Zhang, and Martin Monperrus. 2022.
SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics. https:
//doi.org/10.48550/ARXIV.2203.12755
[70] He Ye, Matias Martinez, and Martin Monperrus. 2021. Automated patch assess-
ment for program repair at scale. Empirical Software Engineering 26, 2 (2021), 20.
https://doi.org/10.1007/s10664-020-09920-w
[71] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. SeqGAN: Sequence
Generative Adversarial Nets with Policy Gradient. In Proceedings of the Thirty-
First AAAI Conference on Artificial Intelligence (San Francisco, California, USA)
(AAAIâ€™17) . AAAI Press, 2852â€“2858.
[72] Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux, and Martin
Monperrus. 2018. Alleviating patch overfitting with automatic test generation:
a study of feasibility and effectiveness for the Nopol repair system. EmpiricalSoftware Engineering (2018).
[73] Yuan Yuan and Wolfgang Banzhaf. 2018. ARJA: Automated Repair of Java Pro-
grams via Multi-Objective Genetic Programming. In IEEE Transactions on Software
Engineering .
[74] Wen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. 2019. Bridging
the Gap between Training and Inference for Neural Machine Translation. In
Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics . Association for Computational Linguistics, Florence, Italy, 4334â€“4343.
https://doi.org/10.18653/v1/P19-1426
[75] Ruizhe Zhao, Brian Vogel, and Tanvir Ahmed. 2019. Adaptive Loss Scaling for
Mixed Precision Training. arXiv:1910.12385
[76] Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong,
and Lu Zhang. 2021. A Syntax-Guided Edit Decoder for Neural Program Repair.
InProceedings of the 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (Athens,
Greece) (ESEC/FSE 2021) . Association for Computing Machinery, New York, NY,
USA, 341â€“353. https://doi.org/10.1145/3468264.3468544
1518