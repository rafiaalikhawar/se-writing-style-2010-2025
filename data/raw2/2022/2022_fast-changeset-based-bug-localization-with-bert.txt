Fast Changeset-based Bug Localization with BERT
Agnieszka Ciborowska
Virginia Commonwealth University
Department of Computer Science
Richmond, VA, USA
ciborowskaa@vcu.eduKostadin Damevski
Virginia Commonwealth University
Department of Computer Science
Richmond, VA, USA
kdamevski@vcu.edu
ABSTRACT
Automaticallylocalizingsoftwarebugstothechangesetsthatin-
duced themhas the potential toimprove software developereffi-
ciency and to positively affect software quality. To facilitate this
automation, a bug report has to be effectively matched with source
codechanges,evenwhenasignificantlexicalgapexistsbetween
natural language used to describe the bug and identifier namingpractices used by developers. To bridge this gap, we need tech-
niquesthatareabletocapturesoftwareengineering-specificand
project-specificsemanticsinordertodetectrelatednessbetween
the two types of documents that goes beyond exact term matching.
Popular transformer-based deep learning architectures, such as
BERT, excel at leveraging contextual information, hence appear to
be a suitable candidate for the task. However, BERT-like models
arecomputationallyexpensive,whichprecludesthemfrombeing
used in an environment where response time is important.
Inthispaper,wedescribehowBERTcanbemadefastenoughto
be applicable to changeset-based bug localization. We also explore
several design decisions in using BERT for this purpose, including
howbesttoencodechangesetsandhowtomatchbugreportstoin-
dividual changes for improved accuracy. We compare the accuracy
and performance of our model to a non-contextual baseline (i.e.,
vector space model) and BERT-based architectures previously used
insoftwareengineering.Ourevaluationresultsdemonstrateadvan-tagesinusingtheproposedBERTmodelcomparedtothebaselines,
especially for bug reports that lack any hints about related code
elements.
KEYWORDS
bug localization, changesets, information retrieval, BERT
ACM Reference Format:
AgnieszkaCiborowskaandKostadinDamevski.2022.FastChangeset-based
BugLocalizationwithBERT.In 44thInternationalConferenceonSoftware
Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510042
1 INTRODUCTION
Two of the most prevalent tools used today by software engineers
arerepositoriestostoreprojectfiles(e.g.,git)andbugtrackersto
.
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510042reportand monitorbug fixingactivity (e.g.,JIRA, BugZilla).Auto-
maticallylinkingabugreportinabugtrackerandrelatedsoftware
artifacts from a repository is one of the long-standing goals in
the software engineering research community, due to its poten-tial to improve practice by reducing the time developers spend
examining code when addressing a newly reported bug, i.e., bug lo-
calization [35,62].However, despitenumerousefforts,theaccuracy
of bug localization approaches is not yet high enough for wide-spread use, especially as it applies to different software projects
thatvaryinbugreportandcodestyle[ 32].Inexaminingthetrends
frominterviewsconductedwithalargecohortofsoftwaredevel-
opers from industry and open-source software, Zou et al. report
thatdevelopersdonottrustbuglocalizationtoolsduetotheirin-
abilitytoadapttodifferenttypesofbugreports,specificallynoting
that existing techniques only work on the most simple cases, with
straightforwardtextualsimilaritybetweenthebugreportandcode
base [64]. More work is needed to improve the retrieval quality of
bug localization techniques.
Atthesametime,asindustryisincreasinglyattemptingtouse
bug localization to aid developers in their daily work, specificrequirements of the problem for modern use are coming to the
forefront[ 33].Onekeycharacteristicfoundbeneficialinmodern
software projects is bug-inducing changeset- (or commit-) level
retrieval.Abug-inducingchangesetisonewherethebugwasini-
tially introduced into the repository. Retrieving such changesets
leads to faster bug repair, as they contain related parts of the code
that were changed together, which makes fixing the bug easier.
However,retrievingbug-inducingchangesetswith highaccuracy
is more challenging than retrieving buggy source code elements
due to the potentially large number of commits in the corpus.
In recent years, numerous popular natural language processing
tasks (e.g., question answering, machine translation) have all ob-
servedimprovedperformancewhenusingneuralnetworkarchitec-
turesbasedontransformers.Thesetransformer-basedmodelsare
typicallyappliedviatransferlearning,byfirstpre-trainingthemona very large corpus and then fine tuning on a much smaller datasettowardsthespecifictasktheyaretobeusedfor.Transformer-based
models pre-trained on large software engineering corpora (e.g.,
StackOverflow)arenowbecomingavailable[ 48],withthepoten-
tial to improve software engineering tasks like bug localization.
In thispaper, weuse theBERT (Bidirectional EncoderRepresenta-
tions from Transformers) transformer-based architecture, which is
a highly popular model introduced by Devlin et al. [9].
BuglocalizationisusuallyframedasanInformationRetrieval
(IR) task, where a document (i.e., a software artifact) is retrieved
fromacorpus-basedonaquery(i.e.,thebugreporttext).Ameasure
ofsemanticrelatednessbetweenthebugreportandthesoftware
9462022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Ciborowska and Damevski
artifact is necessary to rank the results retrieved from the cor-
pus. Given the fact that transformer-based models consist of many
neural layers and require heavy computation for each sentence,
measuringrelatednessbetweenthequeryandthecorpusquickly
becomes expensive.
This paper applies BERT to the problem of changeset-based bug
localizationwiththe goalofimprovedretrievalquality, especially
on bug reports where straightforward textual similarity would not
suffice.WedescribeanarchitectureforIRthatleveragesBERTwith-
outcompromisingretrieval speedandresp onsetime.Inaddition,
we examine a number of design decisions that can be beneficial in
leveragingBERT-likemodelsforbuglocalization,includinghow
best to encode changesets and their unique structure.
Our experimental results indicate that the proposed approach
improvesuponpopularbuglocalizationtechniquesby,e.g.,increas-
ingtheretrievalaccuracybetween5.5%and20.6%forbugreports
with no or a limited number of localization hints. We note that
usingentirechangesetsasinputgranularitysignificantlyhinders
the models performance, while leveraging more fine grained input
data,suchashunks,resultsinthehighestretrievalquality.Wealso
observethatthesizeofsearchspace(i.e.,thenumberofchangesets
in a project) significantly impacts the retrieval delay of different
BERT-based models, though less in the case of the proposed model.
The main contributions of this paper are:
‚Ä¢approach that applies BERT to the bug localizationproblem
(specifically,localizingbug-inducingchangesets)
that is more accurate than the state-of-the-art,
‚Ä¢improvement over other recent BERT-based architec-
turesproposed towards changeset retrieval, showing signif-
icantadvantageswithrespec t to retrieval speed,
‚Ä¢evaluation and recommendations for key design cho-
icesin applying BERT to changesets (i.e., code change en-
coding, data granularity).
Significance of contribution. The BERT-based technique pro-
posed in this paper enables semantic retrieval of software artifacts
(specifically,changesets)forbuglocalizationthatgoesbeyond(and
can complement) the exact term matching in the current popu-
larstate-of-the-arttechniques(e.g.,[ 45,57]).Relativetoasimilar,
recentBERT-basedtechnique[ 27],weofferanapproachthatim-
proves retrieval speed significantly, in a way that supports real-
world use, while also enhancing retrieval quality.
2 PROBLEM DESCRIPTION
In this section, we list and discuss the specific constraints of the
buglocalizationproblemthatweaimtoaddress,whicharebased
on a recent survey of industry practitioners and the problem re-quirements observed at a large software enterprise [
33,64]. Our
focusisabuglocalizationtechniquethat:1) focuses onretrieving
changesets; 2) aims to capture semantics and can be applied to bug
reports that do not share terms with the relevant parts of the code
base;and3)quicklyretrievesresultsforanewlycreatedbugreport.
1. Localizing changesets [33]. Over the years, a large body of
researchhasbeendedicatedtolocatingsourcecodefiles(orclasses)relevanttoabugreport[
7,22,36,45,55,57].However,recentstud-
ies have pointed out that bug localization at the level of source
code files still requires significant effort by software developersinordertolocaterelevantcodewithinlargefiles[ 33,56,64].Ad-
justing for this finding, researchers shifted their efforts towards
more fine grained code elements, such as file segments [ 57] and
methods [ 50,59,61], which introduce new sets of challenges such
asdifficultyinselectingoptimalsegmentsizeandlargemethods
thatstillrequireefforttoexamine.Morerecently,therehasbeen
agrowinginterestinchangesetretrieval[ 7,27,56,58]forbuglo-
calizationbecausechangesetshaveseveralunique propertiesthat
make them convenient to developers aiming to fix a bug. First,
they inherently capture lines of code that are related to each other
withinthecontextofamodification.Second,whenlocatingchange-
sets, we can retrieve not only the modified portion of the code, but
identifyasoftwaredeveloperthatcommittedthemodificationin
the first place, therefore easing the bug triaging process. Finally,
changesetsallowforstraightforwardcontext-awaredivisioninto
asetofhunks,i.e.,asetofchangesinoneareaofthefile.Hunksare usually convenient to read for developers and allow for easy
detection of changes with no semantic value (e.g., changes only in
white spaces).
2. Leveraging semantics of input documents [33,48]. As soft-
ware evolves rapidly and is actively maintained by multiple de-
velopers, different portions of the code base become affected by
distinctiveidentifiernamingpatternsandconventions,whichex-
acerbate the already existing semantic gap between bug reports
andrelatedcodeelements,posingasignificantchallengetotradi-
tional IR systems based solely on token similarity [ 12]. Surveys of
practitioners have also indicated that bug reports that explicitlymention the names of classes or methods relevant to the bug fix
donotrequireautomatedbuglocalization,whileassistinginbug
reports with large semantic gaps with the code base is likely more
valuable to developers. For instance, one surveyed developer in
the study by Zou et al. [ 48] stated the following about current bug
localization, "Itseemsthatexistingtechniquesmainlymakeuseof
the textual similarity between bug reports and source code files toperform bug localization. However, I encountered many bugs that
haveverylittlesimilaritybetweentheirbugreportsandcodefiles.I
wonder what kind of bugs such techniques can localize? Maybe only
simple bugs?". To bridge this gap, researchers have recently pro-
posedtousedeeplearningmodelscapableofbuildingsemanticallyrichdocumentrepresentations[
4,6,12,16,24,27,33].Transformer-
basedmodels,andBERTinparticular,arecurrentlyoneofthemost
excitingdeeplearningtechniquesachievingbroadimprovements
acrossavarietyoftext-basedtasks.ThemainstrengthofBERT-like
models is in building a token representation based on bidirectional
contextualinformationencodedintheprecedingandsucceeding
tokens, which leads to richer semantics that is more likely to de-
tectrelatedpairsofbugreportsandchangesetsthatdonotshare
terms.Priorgenerationsofwordembeddings,e.g.,word2vec[ 31]
and GloVe [ 38], which have been frequently applied on software
engineering tasks [ 5], do not use word context at inference time,
i.e., each token maps to a vector regardless of the surrounding text.
3. Fast retrieval in a large search space [40]. Retrieving bug-
inducing changesets requires computing similarity between a bug
report of interest and all changesets committed to a repository up
tothepresentpointintime.Giventhatmodernsoftwareevolves
rapidly, resulting in large source code repositories with numerous
947Fast Changeset-based Bug Localization with BERT ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
BERTClassification layer
Aggregation layerEncoded feature vector
Bug report ChangesetRelevant Not Relevant
(a) Single BERT [8, 27, 37]BERTClassification layer
Aggregation layerConcatenated encoded 
feature vectorRelevant Not Relevant
Bug report Changeset
(b) Siamese BERT [27, 41]BERT
Bug report ChangesetRelevant Not Relevant
Linear layerMaxSim MaxSim
 MaxSim
(c) FBL-BERT [21]
Figure 1: BERT-based architectures for changesets retrieval
commits[ 43,46],itisimpracticaltocomputepair-wisesimilarity
due to the large search space. This is especially the case if comput-
ingthesimilaritymeasureitselfisexpensive.Thoughdeeplearning
models provide state-of-the-art accuracy, they typically require
more computationalresources thantoken-based techniques, which
emphasizestheneedforabuglocalizationtechniquetolimitthe
search space in order to improve performance without compromis-
ing accuracy.
3 APPROACH
In order to address the above problem constraints, in this paper we
investigate the use of a BERT model towards bug localization with
changesetsasaprimarydatagranularity,whichisalsopreferredby
practitioners‚Äô(¬ß2.1)[ 52].WespecificallyselectedBERTasitisthe
state-of-the-artinsemanticsmodelingandextractingcontextual
information(¬ß2.2).Finally,toensurethatourapproachisapplicable
tolarge,industryscalerepositories(¬ß2.3),weintroduceFastBug
Localization BERT (FBL-BERT), which reduces the search space,
suchthatonlypromisingcandidatechangesetsareconsideredfor
neuralre-rankingwithBERT.Inaddition,FBL-BERTencodesabug
report and a changeset separately, allowing to compute changeset
representationsofflineandreducethecomputationaleffortperbug
report at retrieval time. Replication package is available at [1].
3.1 BERT for bug localization
ThearchitectureofBERTconsistsofmultiplelayersoftransformer-
encoders,whichareanabstractionaimedatmodelingsequential
data that utilizes self-attention; the notion of attention is to weight
specific terms in the sequence differently, i.e., encoding a stronger
relationship from each term in the sequence to the remaining most
semantically relevant terms. As pointed out by Mills et al. [ 32],
retrieval techniques for bug localization can be significantly im-
proved with intelligent query construction, i.e., by carefully choos-
ing which parts of the bug report to use for comparison. Therefore,
leveragingamodelthatusesattentiontoemphasizecertainwordrelationships has the potential to significantly improve upon prior
state-of-the-artbug localization techniques.
UsingaBERTmodelforbuglocalization(orothersimilarpur-
poses)involvesthreeessentialsteps:(1)pre-trainingthemodelwith
alargecorpusofgeneralsoftwareengineering-relateddata,(2)finetuningtheBERTmodelforbuglocalization,andfinally,afterBERT
hasbeencompletelytrained,(3)retrievingrelevantbug-inducing
changesets for a newly reported bug.
During pre-training, BERT uses massive corpora of relevant
text to build a language model for a specific domain, e.g., software
development. Given that this step requires a significant amountof data and computational resources, a common choice is to re-
useapre-trainedBERTmodel,whenavailable.Inthefinetuning
step, BERT updates the general data representation with respect
to a specific downstream task (e.g., bug localization) given a much
smaller,task-specificdataset.Moreprecisely,finetuningaBERT
model occurs by adding an additional layer (e.g., a classification
layer)tothepre-trainedBERTmodel.Thistask-specificlayertakes
theoutputofBERTasinputandrepresentsthepartofthemodel
thatisprimarilytrainedduringfinetuning,thoughBERT‚Äôsinternal
weights are also updated in the process. In most scenarios, fine
tuning can be completed faster and with much less computational
resourcesthanpre-training.Sinceourgoalislocatingbug-inducing
changesets,anaturalchoiceforatask-specificdatasetconsistsof
bug reports and their inducing changesets. A key design choice at
this stage is how to connect BERT with the additional task-specific
neuralnetworklayer.Givenaninputdocument,BERTencodeseach
word in the document with a vector, i.e., for each input document,
the output of the BERT model is an embedding matrix of size |ùëë|
byùë£ùëôùëíùëõ,where|ùëë|representsthenumberofwordsinthedocument
andùë£ùëôùëíùëõthelengthofaBERTvector;typically ùë£ùëôùëíùëõ=728.Themost
common approach when retrieving BERT-encoded documents is to
aggregate the embedding matrix across words through average or
summation,whichproducesasinglevectorasoutput.Usingsuchan
aggregaterepresentationofadocumentallowsforfasterprocessing
andeasiercomparisonbetweenpairsofdocuments.However,as
948ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Ciborowska and Damevski
BERT
Bug reportLinear LayerMaxSim
ChangesetModel training Offline indexing
Faiss
IndexRetrieval
Bug reportMaxSim
Changesets1...NTop N << M
most similarRanking
Bug reportFaiss
IndexFBL-BERTFBL-BERT
Changesets1...MFBL-BERT FBL-BERT
Figure 2: FBL-BERT for changeset-based bug localization pipeline.
pointed by Sachdev et al. [ 44], this simple aggregation strategy
leads to a dissipative data representation that has the potential
tonegativelyaffectretrievalperformance.Inthenextsection,we
describe an alternative strategy that takes advantage of the full
matrix to encode input data.
In the simplest changeset retrieval scenario, presented in Fig. 1a,
each newly arriving bug report is concatenated with every change-
setintheprojecthistory.Subsequently,theyareprocessedbyBERT,
producing an embedding matrix, which is transformed to a vector
by an aggregation layer. Finally, the vector is passed into a classifi-
cation layer that produces a relevancy score between a bug report
andachangeset.Changesetsareorderedbasedontheirscoresto
produce a ranked result set. This type of BERT architecture for in-
formationretrievalisoftenreferredtoasSingleBERT[ 8,27,37].In
analternativeretrievalarchitecture,calledSiameseBERT[ 27,41]
anddepictedinFig.1b,thebugreportandthechangesetarepro-
cessedseparately,firstthroughBERTandthenthroughanaggre-
gation layer. As a result, bug reports and changesets are trans-
formed into independent vectors that are subsequently concate-natedandfedintotheclassificationlayertoproducearelevancescore. The advantage of Siamese BERT over Single BERT is that
SiameseBERTenablespre-computingchangesetrepresentations
offlinesincechangesetsarenotrequiredtobeconcatenatedwith
a bug report for retrieval. H owever,Siamese BERT still requires
comparingabugreporttoeachchangeset,whichincurssignificant
retrieval delay in the case of large number of changesets.
3.2 Fast Bug Localization BERT
TheFBL-BERTarchitecture,basedonColBERTbyKhattabetal.[ 21],
eschewsaggregationoftheembeddingmatrix,andinsteadbuilds
a relevance score by leveraging the whole matrix, resulting in a
morecomplete,finegrainedcomparison.Morespecifically,abug
reportùëèùëüand a changeset ùëêare separately processed by BERT cre-
atingembeddingmatrices ùê∏ùëèùëüandùê∏ùëê,respectively.Tocomputethe
relevancescorebetween ùê∏ùëèùëüandùê∏ùëê,foreachwordembeddingin
thebugreport ùë£ùëèùëü‚ààùê∏ùëèùëü,wefindthemaximumcosinesimilarity
acrosswordembeddingsofthechangeset ùë£ùëê‚ààùê∏ùëê,andcombinethe
maximumcosinesimilaritiesviasummationasillustratedinFig.1c.
As a result, the model learns how to associate words from a bug
report with tokens in a changeset, taking into account the context
inwhichtheyappear.Toaccountforthetwodifferenttypesofdata
weprocess,i.e.,bugreportsandchangesets,wemodifyColBERT
by increasing the numbers of BERT encoder layers taken to thelinearlayer.Morespecifically,whileColBERTusestheoutputof
thelastBERTencoder,wetaketheoutputofthelast4encoders(asrecommendedby[
9]).Thismodificationisdictatedbypriorstudies
observingthatthatdifferentlayersofBERTencodedifferentgranu-
larity of semantic information [ 9,39,51]. Note that the linear layer
inFBL-BERTisnotequivalenttotheaggregationlayerdiscussed
before,butisusedtoreducethesizeofwordembeddingsproduced
by BERT, retaining all word embeddings in a compressed form for
faster downstream processing.
There are several benefits that make the FBL-BERT architecture
particularlyapplicabletoourproblem.First,themodelpurposely
avoids joint document encoding, as in Single BERT, delaying inter-
actionbetweenabugreportandachangesettofacilitateoff-line
encodingofchangesets.Moreover,byusingcomputationallycheap,
yet efficient, maximum similarity summation as a scoring operator
instead of a more complex strategy, such as the classification layer
inSiameseBERT,theprocessingtimeforaqueryisreduced.Finally,
given that the relevance score computation is isolated and relies
solelyonmaximumsimilarity,itispossibletoutilizeefficientvector
similarityalgorithmstoreducethesearchspaceofall ùëÄchangesets
by identifying top- ùëÅchangesets, ùëÅ<<ùëÄ, that are similar to a
new bug report, and subsequently re-rank only the top- ùëÅsubset.
ToclarifyhowFBL-BERToperatesforchangeset-basedbuglo-
calization, consider the pipeline depicted in Fig. 2. First, as shown
intheModelTrainingsectionofFig.2,theFBL-BERTmodelisfine
tuned on a project-specific dataset consisting of bug reports andbug-inducing changesets. In the next step (Offline Indexing), all
changesets in the project repository are encoded via FBL-BERT and
storedinanindexsupportingefficientvector-similaritysearch.For
thispurpose,weuseanIVFPQ(InVertedFilewithProductQuan-
tization)index,implementedintheFaisslibrary[ 19].TheIVFPQ
indexusesthek-meansalgorithmtopartitiontheembeddingspace
intoùëÉ(e.g.,ùëÉ=300) partitions, and subsequently assigns each
word embedding to its nearest cluster. To facilitate efficient search,
whenaqueryisissued,thequeryisfirstcomparedagainsttheparti-
tions‚Äôcentroidstolocatethenearestpartitions,andthenthesearch
continues to the instance-level only within those. Note that the
Faiss index contains wordembeddings across allchangesets. After
completionofthisstep,theretrievalsystemisreadytobedeployed.
When a new bug report arrives, it is first encoded via FBL-BERT
producinganembeddingmatrix.Next,foreachwordembeddingin
the embedding matrix, we query the Faiss index to identify the ùëÅ/prime
most similar embeddings across all changesets embeddings stored
949Fast Changeset-based Bug Localization with BERT ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
in the Faiss index. Since among ùëÅ/primemost similar embeddings some
may point to the same changeset, in the end we obtain a total of ùëÅ
unique candidate changesets. Finally, we use FBL-BERT to re-rank
the candidate changesets and produce the final ranking.
3.3 Changesets encoding strategies
Software evolution over time is recorded in a repository as a time-
orderedsequenceofchangesets.Eachchangesetconsistsofalog
message, providing a short rationale explaining the goal of the
modification, and a set of source code changes. Depending on the
version control system and diffalgorithm used in the software
project, the representation of source code changes can vary. In this
paper,wefocusontheformatthatistheoutputofthe git diff
command,inwhichaddedlinesofcodeareannotatedwith +,r e-
movedlineswith -,andallmodifiedlinesaresurroundedby3lines
ofcontextual,unchangedlines.Whilethereexistmoreadvanced
tree-based code differencing algorithms (e.g., GumTreeDiff [ 10]),
providing detailed code-change information to a machine learning
modelmayaffectthemodelnegatively[ 60],henceweoptforatext-
basedapproach.Changesetscanencapsulatecodechangesacross
one or multiple source code files, and modifications to each file
can be divided into hunks- groups of modified (added or removed)
lines surrounded by unchanged (context) lines. Given this specific
formatting,weexplorehowbesttoutilizechangesets‚Äôpropertiesto
construct BERT input from two perspectives: (1) encoding charac-
teristics of code modifications, such asadditions or removals; and
(2) levels of granularity in a changeset.
InputprovidedtoBERTmodelsisrequiredtofollowcertainrules.
First,adocument(e.g.,achangesetorabugreport)needstobetok-
enized and each token replaced by its unique token id. Pre-trained
BERTmodelssupplytheirownBERTtokenizers,thatareoptimized
towardsthecorpusonwhichthemodelispre-trained.BERTtok-
enizers are trained using the WordPiece algorithm [ 47]. The main
advantage of BERT tokenizers is in avoiding out-of-vocabulary
wordsbydividingunknownwordstotheirlargestsubwordspresent
in the vocabulary, which is likely to be beneficial in our setting,
assoftwareprojectscanhaveveryspecificvocabulariesunlikely
tobe observedelsewhere [ 48].Secondly,BERTuses apre-defined
set of special tokens. In general, due to how BERT is trained (more
detailsin[ 9]),themodelrequiresthateachtokensequencestarts
with special classification token [CLS]and ends with separator
token [SEP],whileotherspecialtokens,suchaspadding [PAD]are
usedifandwhennecessary.Specialtokenscanconveyinformation
about the structure of data allowing BERT to differentiate between
partsoftheinput,henceweexplorehowspecialtokenscanbebestutilizedtoencodechangesets.Tothisend,weproposethefollowing
encoding strategies, depicted in Fig. 3.
D:A changeset is considered a single document that is feed into
the model. To inform the model that a changeset sequence begins,
wedefineandpre-appendthespecialtoken [D]atthebeginning
of the code sequence. Since this strategy does not utilize specific
characteristics of a code change, it serves as a baseline to compare
against other strategies.
ARC:In this encoding, a changeset is split into lines, and the lines
are subsequently grouped based on whether they are added, re-
moved or provide context, as indicated by their initial character: +BERT Tokenizer
font
 [CLS] [Q] reset
D - encoding
ARC - encoding
ARC L - encodinggit diff 68f73a31d3bd23bb9be3de8de4cfa69258483b46
+++ b/[..]/org/eclipse/swt/widgets/Composite.java  [empty lines omitted]- void updateFont (Font oldFont, Font newFont) {
+ boolean updateFont (Font oldFont, Font newFont) {
        Control [] children = _getChildren ();        for (int i=0; i<children.length; i++) {
                Control control = children [i];
+++ b/[..]/org/eclipse/swt/widgets/Control.java    
 [empty lines omitted]
- void updateFont (Font oldFont, Font newFont) {
-        Font font = getFont ();-        if (font.equals (oldFont)) setFont (newFont);
+ boolean updateFont (Font oldFont, Font newFont) {
+        boolean sameFont = getFont ().equals (oldFont);+        if (!sameFont) setFont (newFont);+        return !sameFont;
 }
 void updateLayout (boolean resize, boolean all) {Bug 83699
Summary: Font reset to default after screen saver
Description: All editors and views using a StyledText widget have the
font reset to default after coming back from my screen saver. [..]. This breakpoint gets hit when I return from the screen saver:[..] StyledText(Control).updateFont(Font, Font) line: 2913
font line
 [SEP]
void
 [CLS] [D] updatebool-
eanall
 [SEP]
bool-
ean[CLS] [A] update same font
void [R] update new font
cont-
rol
[C] child-
renbool-
eanall
 [SEP]
bool-
ean[CLS]
[A] update new font
void [R] update new font
cont-
rol[C] child-
renchild-
reni
void [R] update new font
bool-
ean[A] update same font
void
 [C] updatebool-
eanall
 [SEP]
Figure 3: Changeset encoding strategies.
foradded, -forremoved,andanemptyspaceforcontextlines.The
lines in each group are concatenated to create a sequence to which
wepre-appendaspecialtoken: [A]forthesequenceofaddedlines,
[R]for the sequence of removed lines and [C]for the sequence of
contextlines.Finally,allthesequencesareconcatenatedtogether
to create an input for the model. By grouping different parts of
950ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Ciborowska and Damevski
changesets based on their characteristics, we aim to investigate
whetheranyparticulartypeofmodificationismorebeneficialthan
the other. With the ARC strategy the model is given an opportu-
nity to learn how to combine information of different types and,
ifnecessary,decidetodisregardaportionofitifitpoorlyaffects
performance.
ARCL:Similarly as in ARC, a changeset is divided into lines, how-
ever ARC Lencoding does not group the lines. Instead, it preserves
theorderingoflineswithinachangeset,suchthatspecialtokens
[A],[R],or[C]arepre-appendedwherevertypeofmodification
changes. While this strategy results in more accurate data repre-
sentation,comparedtoARC,ARC Lisalsomorechallengingforthe
model, since the special tokens occur multiple times and in several
places.
Given that a bug report and a changeset are encoded separately,
the model has to differentiate between these two types of docu-
ments.Tothisend,whenencodingabugreport,wedefineaspecial
token [Q]that is pre-appended to the query, i.e., the bug report.
Another dimension in choosing how to best encode changesets
is related to their granularity, i.e., using entire changesets or sepa-
rating a changeset to a file- or hunk-level. Leveraging hunks as the
primary data dimension in an IR model brings several advantages.
First,bugshavebeenobservedtobetypicallycausedbysmallpieces
of code [57,59], thus the inherent fine granularity of hunks makes
themlesssusceptibletonoisewhencomparedtowholesourcecode
files [56]. Second,dividing changesetsinto hunks alleviatesissues
caused by tangled commits [ 13]. Given the fact that hunks are typ-
ically small and concentrate on an enclosed portion of the code,
BERT is not affected by long-range token dependencies, which is a
problem typically affecting source code [ 48]. Finally, shorter input
documents are less likely to exceed the maximum sequence length
acceptedbyBERT,whilelongerdocumentshavetobetruncated,
which may negatively affect the results. Howev er, despite easily
accessiblesmallerdatagranularitywithinachangeset,todate,most
oftheeffortsarefocusedonleveragingentirechangesets[ 3,27,33].
4 EXPERIMENTAL EVALUATION
4.1 Research questions
RQ1:HoweffectiveisFBL-BERTwhencomparedto(1)state-of-the-art
techniques based on the VSM, and (2) related BERT-based architec-
tures?
The main opportunity in using FBL-BERT is in incorporating addi-
tionalcontextandsemanticswhenretrievingbug-inducingchange-
sets, which should provide improvements in accuracy over the
state-of-the-art,especiallyforbugreportsthatprovidehighlevel
bug descriptions and lack explicit localization hints. Researchers
have identified that a non-trivial amount of bug reports alreadycontain localization hints, i.e., they mention the class or method
namesrelevanttofixingthebug,andsomerecentapproachesfor
bug localization argue that only bug reports that lack extensive
localizationhintsshouldbeconsideredinevaluation[ 17].Wefol-
low the methodology proposed by Kochhar et al. [ 23] to categorize
bug reports into 3 groups based on the completeness of localiza-tion hints they provide and evaluate the performance for each
bug reportgroup separately. Wealso investigate how theruntime
performanceofFBL-BERT,whichutilizesfinegrainedmatching,compares to other BERT-based architectures that rely on embed-ding aggregation and perform retrieval across the entire searchspace. As baselines, we use (1) Locus [
56], a state-of-the-art ap-
proach based on VSM that locates bug-inducing changesets, and
(2) TBERT-Single and TBERT-Siamese [ 27] approaches that utilize
aggregated BERT-based representations that have recently been
proposed for software engineering.
RQ2:Which changeset encoding strategy is the most profitable? Are
there advantages to using hunks, changeset-files or entire changesets
as the primary data dimension?
InthisRQ,wefirstinvestigatewhetherencodinginformationabout
the type of modification in each line of a changeset can increase
the performance of the FBL-BERT model. We evaluate two alterna-
tivestoencodechangesetssemantics,ARC,ARC L,andabaseline
approach, D, which disregards change-related information. Second,
we investigate how granularity of the input data affects the model
performanceandwhatarethebenefitsandchallengesofleveraging
changesets, changeset-files, or hunks in our model. To answer this
RQ, we fine tune FBL-BERT separately for each of the encoding
strategies and with each input data granularity, resulting in 9 eval-
uation configurations per software project, measuring the model‚Äôs
performance in retrieving relevant changesets.
4.2 Dataset and baselines
To answer the RQs, we leverage the dataset of bugs and their in-
ducing changesets collected and manually validated by Wen et
al.[56];manuallyvalidateddatasetsremovetheerrorthatcanbe
introducedbytheSZZalgorithmthatmapsthebugfixingtothe
inducing commit [ 34]. This dataset includes 6 software projects,
namely AspectJ, JDT, PDE, SWT, Tomcat and ZXing (descriptive
statistics are presented in Table 1). To create a training set for each
project,weselectedthefirsthalfofproject‚Äôspairsofbugreportsand
bug-inducingchangesets,orderedbybugopeningdate,asatrain-
ingset,andlefttheremaininghalfasatestset.Foreachpairinthe
training sets, we also create a negative sample by randomly choos-
ingacodechangewhichdoesnotbelongtotheinducingchangeset,
essentially forming triplets of bug report, bug-inducing change-
set,notbug-inducingchangeset.Weexperimentedwithchoosing
negative samples by selecting a syntactically similar changesetthat was not bug-inducing but we did not observe a significantchange in retrieval accuracy. As this type of generating negative
samplesincurredsubstantialcomputationalcosttogather,weopted
to use random sampling. Finally, for each project we obtained a
balancedtrainingsetwithequalnumberofpositiveandnegative
examples. Note that although training sets do not include all avail-
able code changes, during bug localization the model performs
retrieval across allcode changes available for a specific project (as
explainedinSection3.2).Tostudytheimpactofdifferentchangeset
data granularity on the BERT-based models, we created a separate
datasetforeachtypeofgranularity,i.e.,changesets,changeset-files
and hunks. To this end, for changeset-file and hunk granularity,
wedividethebug-inducingchangesettofile-orhunk-levelcode
changes, such that one bug report creates multiple pairs with files
or hunks from its respective inducing changeset.
We compare the performance of the proposed model with Lo-
cus[56],whichisanunsupervisedmodelthatutilizeshunk-level
951Fast Changeset-based Bug Localization with BERT ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 1: Projects in evaluation dataset.
#Bugs #Changesets #Changeset-files #Hunks
AspectJ 200 2,939 14,030 23,446
JDT 94 13,860 58,619 150,630
PDE 60 9,419 42,303 100,373
SWT 90 10,206 25,666 69,833
Tomcat 193 10,034 30,866 72,134
ZXing 20 843 2,846 6,165
granularity and the VSM to locate relevant changesets based on
the maximum similarity score obtained between a bug report, ahunk, and a log message. Note that FBL-BERT does not use log
messages as our goal is to explore mapping from natural language
inabugreporttocodechanges.Whilewellwrittenlogmessages
canhaveapositiveimpactontheresultsbyboostingthescoresforsomechangesets,notallrelevantcodechangesareaccompaniedby
logsofgoodquality[ 18,29].Asasecondsetofbaselines,weem-
ployTBERTarchitecturesforsoftwareartifactsretrievalrecently
proposedbyLinetal.[ 27].Outofthethreearchitecturesinvesti-
gated by Lin et al., we selected TBERT-Single and TBERT-Siamese
asourbaselines,rejectingTBERT-Twin,sinceitsperformancein
terms of accuracy and time was significantly surpassed by the two
others.Ingeneral,bothofthesearchitecturesarefairlysimilarto
thosepresentedinFig.1withanexceptionofusingmoreadvanced
embedding aggregation operators [27].
4.3 Metrics
Toevaluatetheperformanceofthemodel,weemployasetmetrics
commonly used to evaluate performance of IR systems.
Mean Reciprocal Rank: MRR quantifies the ability of a model to
locate the first relevant changeset to a bug report. The metric is
calculatedasanaverageofreciprocalranksacross ùêµbugreports,
whileareciprocalrankforabugreport ùêµùëñisequaltoaninverted
rank of the first relevant changeset in the ranking:
ùëÄùëÖùëÖ =1
|ùêµ||ùêµ|/summationdisplay.1
ùëñ=11
1ùë†ùë°ùëÖùëéùëõùëò ùêµùëñ.
Mean Average Precision: MAPmeasureshowwellamodelcan
locate all changesets relevant to a bug report. MAP is calculated
asthemeanofaverageprecisionvalues( ùê¥ùë£ùëîùëÉ)forùêµbugreports,
while average precision for a bug report ùêµùëñ,ùê¥ùë£ùëîùëÉ ùêµùëñ, is computed
based on the positions of all relevant changesets in the ranking:
ùëÄùê¥ùëÉ =1
|ùêµ||ùêµ|/summationdisplay.1
ùëñ=11
ùê¥ùë£ùëîùëÉ ùêµùëñ.
Precision@K: P@Kevaluateshowmanyofthetop- ùêæchangeset
inarankingarerelevanttoabugreport.ThevalueofP@Kisequal
tothenumberofrelevantchangesets |ùëÖùëíùëôùêµùëñ|locatedinthetop- ùêæ
position in the ranking averaged across ùêµbug reports:
ùëÉ@ùëõ=1
|ùêµ||ùêµ|/summationdisplay.1
ùëñ=1|ùëÖùëíùëôùêµùëñ|
ùêæ.
4.4 Experiment setup
The experiments were conducted on a server with Dual 12-core
3.2GHz Intel Xeon and utilized 1 NVIDIA Tesla V100 with 32GBTable 2: Mean Reciprocal Rank (MRR) of changeset-based
BL techniques for different types of bug reports.
Bug report type
Technique Granularity BLNL
n=151BLPL
n=75BLFL
n=105BLNL+PL
n=226All BRs
n=331
Locus Hunks 0.235 0.302 0.452 0.258 0.319
TBERT- Changesets 0.119 0.213 0.136 0.150 0.146
Single Change. files 0.274 0.469 0.299 0.339 0.326
Hunks 0.268 0.429 0.273 0.321 0.306
TBERT- Changesets 0.125 0.256 0.080 0.168 0.140
Siamese Change. files 0.263 0.424 0.200 0.316 0.279
Hunks 0.236 0.333 0.171 0.269 0.238
FBL-BERT Changesets 0.076 0.114 0.113 0.089 0.096
Change. files 0.303 0.441 0.294 0.349 0.331
Hunks 0.2900.509 0.338 0.363 0.355
RAMmemoryrunningonCUDAversion10.1.Toimplementour
model,weused PyTorchv.1.7.1,HuggingFacelibraryv.4.3.2,and
Faiss v.1.6.5 with GPU support. Since pre-training is a computa-tionally expensive task and requires a huge dataset, we decidedto use an available pre-trained BERT model, BERTOverflow [
49].
BERTOverflowistrainedonStackOverflowdata,henceitcontainsa
mixture of code snippets and natural language descriptions, which
islogicalforthebuglocalizationtaskthatoperatesonbothcode
andnaturallanguage.We finetunedourBERTmodelandTBERT
baselinesfor4epochswithbatchesofsize16andalearningrate
of 3E-06 [ 9]. Based on the average number of tokens in bug re-
ports, hunks, changeset-files and changesets across the evaluation
projects, we set the maximum length limit to 256, 256, 512, and
512respectively.Allinputdocumentsaretruncatedorpaddedto
theirrespectivelengthlimit.FortheFaissindex,wesetthenumber
of partitions to 320 and retrieved a total of 1000 changesets forre-ranking with FBL-BERT [
21]. In the case of Locus, we set the
modelparametersto ùúÜ=5andùõΩ2=0.2,indicatedbytheauthors
to provide the highest performance.
5 RESULTS
5.1 RQ1: Retrieval performance
Retrieval accuracy. Table 2 contrasts the retrieval performance
oftheFBL-BERTmodelagainstthebaselineapproachesforthree
different typesof bug reports:not localized, partiallylocalized, or
fullylocalized. Ifa bugreport hasno mentionsof relevantclasses,
itisclassifiedasnotlocalized(BR NL);whensomeoftherelevant
classes appear in the report, the bug is categorized as partially
localized(BR PL);andifallrelevantclassnamesareprovided,the
bug report is fully localized (BR FL)[23]. Note that in the case of
FBL-BERT, we use the results of the model trained with ARC L
encodingsince,onaverage,itprovidesthebestperformanceacross
the evaluation projects, as shown in Section 5.2.
FBL-BERToutperformsLocusforBR NLandBRPLby5.5%and
20.6% respectively, while in the case of BR FL, Locus surpasses our
approach by 11.4%. Given that Locus relies on more direct term
matching between a bug report and a changeset, it makes intuitive
sense that such a model performs best when localization hints are
present in a bug report, and struggles in their absence (as indi-
catedbylowerMRRvaluesforBR NLandBRPL).Ontheotherhand,
952ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Ciborowska and Damevski
FBL-BERT utilizes higher-level association between bug reports
andbug-introducingchangesets,whichcanresultinexactmatches
getting less emphasis. Interestingly, the highest improvement in re-trieval accuracyis observed for BR
PLindicating thatthe model can
effectivelyretrievechangesetsbasedonpartialcluesbyassociating
them with patterns learned from historical data.
TheperformanceofbothTBERTmodelsandFBL-BERTimproves
when the models are trained and evaluated on hunks or changeset-
files. Compared to leveraging changesets, across all bug reports
FBL-BERTimprovesbetween23.5%‚Äì25.9%,whiletheretrievalac-
curacyofTBERT-SingleandTBERT-Siameseincreasesby16%‚Äì18%
and 9.8%‚Äì13.9% respectively. While this results indicate that lever-
agingfinegraineddataaffectsretrievalperformancepositively,itis
important to note that the poor performance observed for change-
setscanbepartiallyattributedtotheinputsizelimitoftheBERT
model (i.e., 512 tokens), which is more often exceed by change-
sets than hunks or changeset-files. More specifically, in our dataset
truncation affects about 8% of hunks and 25% of changeset-files
compared to 45% of changesets.
In general, FBL-BERT outperforms TBERT-Single and TBERT-
Siameseby4.9%and7.6%respectivelyacrossalltypesofbugreports.
Comparing the results of FBL-BERT trained on hunks to TBERT
modelstrainedonchangeset-files,giventhatchangeset-filespro-
vide onaverage thebest performance forTBERT models, wenote
varyingdifferenceinretrievalaccuracydependingonthebugre-
port type. In the case of BR NL, FBL-BERT improves MRR score by
only about 2% over TBERT models. For BR PL, FBL-BERT improves
by 4% and 8.5% over TBERT-Single and TBERT-Siamese, while for
BRFLthe improvement is equal to 3.9% and 13.8% respectively. The
larger gapin retrieval accuracy for BR PLand BRFLbetween FBL-
BERTandTBERTmodelsindicatestheimportanceoftoken-level
embeddingmatching,i.e.,whileTBERTusesaggregatedembedding
torepresentandcomparedocuments,thetoken-levelembedding
matching performed by FBL-BERT allows this model to better rec-
ognizethekeycodenamespresentedinthebugreport,which,in
turn, translates to higher retrieval accuracy.Retrieval time.
One of the key desirable characteristics of FBL-
BERT is to perform efficient retrieval across a large corpus. This
would allow it to leverage fine grained data, such as changesets-
files or hunks which were observed to provide the best retrievalaccuracy, while maintaining reasonable retrieval delay. In Fig. 4,
we compare the average retrieval time per bug report with respect
to the increasing number of documents in the search space, i.e.,
changesets,changesets-filesandhunks.Ingeneral,FBL-BERTre-
trieves relevant documents faster than both TBERT models with
theretrievaltimegapincreasingasthesearchspacegrows.More
specifically, TBERT-Single is the slowest model and requires about
50s to perform retrieval over a small number of documents (e.g.,
ZXing),andnearly1000s(!)foralargeproject(e.g.,JDT).TBERT-
Siamese is significantly faster than TBERT-Single, and up to the
searchspaceofabout15Kdocuments,itperformson-pairwithFBL-
BERT. However, after that point, retrieval time for TBERT-Siamese
rises steadily to reach about 70s for the largest search space, while
in the case of FBL-BERT the retrieval time is still just above 1s. By
comparingtheperformanceofFBL-BERTagainstTBERTmodels,
it becomes evident that plain BERT-based models can quickly hit a
retrievaldelaywallwhichmakesthemimpracticaltouse.Onthe020k 40k 60k 80k 100k 120k 160k10‚àí1100101102103
Number of documentsTime [s]
AspectJ JDT PDE
SWT Tomcat ZXing
Figure4:Averageretrievaltimeperabugreportwithdiffer-
entsizesofsearchspace( TBERT-Single, TBERT-Siamese,
FBL-BERT).
otherhand,FBL-BERTscalesupwithrespecttothesearchspace
size allowing to leverage fine grained data to increase retrieval
accuracy without sacrificing model responsiveness.
Note that the observed speed improvement is the result of both
FBL-BERTandFAISS.Morespecifically,thetrainingobjectiveofFBL-BERT (i.e., finding most similar embedding vectors) enables
usingvectorsimilaritysearch(e.g.,FAISS).Asaconsequence,FAISS
can be used to retrieve the ùêæbest candidates ( ùêæ<<ùëÅ, whereùëÅis
#documents)withsimilarword-levelembeddingrepresentations
that arethen re-rankedby FBL-BERT.By re-ranking only ùêædocu-
ments,thesearchspacebecomessignificantlyreduced,hencede-
creasing the retrieval time. On the other hand, typical BERT-based
pipelines (e.g., TBERT) concatenate bug reports and changesets,
anduseneuralnetworklayerstoestimatearelevancyscore.This
approachprecludespruningthesearchspaceviaFAISS,therefore,
duringretrievalabugreporthastobecomparedtoall ùëÅdocuments,
which in turn increases retrieval delay.
Error analysis. Togainmoreinsightintofactorsthatnegatively
affecttheretrievalaccuracyofFBL-BERT,wemanuallyanalyzed
the bug reports for which the model struggles the most. Morespecifically, we selected all bug reports where the bug-inducing
hunk was ranked 50 or worse by FBL-BERT. This resulted in 20bug reports (
ùêµùëÖùëÅùêø=8,ùêµùëÖùëÉùêø=3,ùêµùëÖùêπùêø=9) that the authors
independently analyzed, contrasting the retrieved hunks to the
true bug-inducing hunks in order to devise a set of common issues
causinglowretrievalaccuracy.Theauthorsalsoexaminedthemost
similarterms(andtheirweights)forboththeretrievedandgoldset
hunks, focusing specifically on the sources of largest differences
between the two. Finally, the authors discussed their independent
observationsandagreedonthreecommonerrorcategories: stack
trace/code snippets, comments, and code tokens splitting, where a
single bug report can belong to more than one error category. We
discuss each of these, in turn.
In11outof20bugreports,thedifficultytoretrievethecorrect
hunk was caused by the presence of a code snippet or a stack trace
in the bug report. Since code snippets and stack traces typicallyconsists of multiple class names or code tokens, they have a po-
tential to introduce noise through unrelated code names, which, in
turn, can lead the model astray [ 53]. For 7 out of 20 bug reports,
953Fast Changeset-based Bug Localization with BERT ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 3: Retrieval performance for different configurations of FBL-BERT.
#Bugs MRR MAP P@1 P@3 P@5 MRR MAP P@1 P@3 P@5 MRR MAP P@1 P@3 P@5
Changesets D ARC ARCL
AspectJ 104 0.053 0.032 0.029 0.024 0.037 0.107 0.061 0.058 0.080 0.083 0.070 0.042 0.029 0.045 0.044
JDT 47 0.097 0.014 0.043 0.028 0.021 0.118 0.160 0.064 0.043 0.030 0.118 0.016 0.064 0.035 0.026
PDE 60 0.091 0.012 0.067 0.022 0.020 0.099 0.019 0.033 0.033 0.031 0.103 0.013 0.067 0.033 0.027
SWT 43 0.067 0.015 0.023 0.027 0.026 0.033 0.006 0.023 0.008 0.005 0.018 0.007 0.0 0.0 0.0
Tomcat 97 0.135 0.048 0.052 0.070 0.074 0.132 0.051 0.062 0.072 0.071 0.141 0.055 0.062 0.077 0.088
ZXing 10 0.127 0.034 0.100 0.033 0.020 0.141 0.034 0.100 0.033 0.040 0.155 0.061 0.100 0.133 0.120
All projects 331 0.091 0.030 0.042 0.039 0.042 0.107 0.040 0.054 0.057 0.056 0.096 0.036 0.045 0.049 0.049
Changeset-files D ARC ARCL
AspectJ 104 0.173 0.083 0.154 0.085 0.100 0.165 0.079 0.144 0.085 0.085 0.1760.0850.1540.095 0.097
JDT 47 0.4030.0600.3190.1840.1280.355 0.060 0.255 0.149 0.126 0.3680.0550.2770.149 0.109
PDE 30 0.2590.0870.1670.1280.1010.236 0.069 0.133 0.117 0.094 0.2600.0790.1670.128 0.151
SWT 43 0.5520.1290.5350.2170.1640.5380.1270.5350.2090.1590.5550.1310.5350.233 0.173
Tomcat 97 0.424 0.099 0.361 0.175 0.147 0.421 0.116 0.351 0.191 0.155 0.4630.1140.3810.222 0.183
ZXing 10 0.199 0.157 0.100 0.133 0.140 0.212 0.163 0.100 0.133 0.220 0.200 0.159 0.100 0.133 0.120
All projects 331 0.348 0.097 0.293 0.162 0.138 0.325 0.095 0.269 0.149 0.128 0.331 0.092 0.281 0.145 0.127
Hunks D ARC ARCL
AspectJ 104 0.175 0.084 0.163 0.091 0.093 0.1760.0820.1630.0930.0830.1830.0930.1730.111 0.099
JDT 47 0.362 0.059 0.255 0.135 0.122 0.322 0.049 0.213 0.149 0.109 0.4290.0620.3190.195 0.167
PDE 30 0.249 0.088 0.167 0.122 0.141 0.2880.0930.2000.1440.1270.200 0.068 0.133 0.078 0.087
SWT 43 0.510 0.117 0.465 0.225 0.196 0.519 0.142 0.442 0.240 0.201 0.526 0.131 0.488 0.217 0.164
Tomcat 97 0.426 0.135 0.289 0.211 0.191 0.4410.1400.3510.2110.2110.4820.1290.412 0.2160.182
ZXing 10 0.3340.2250.2000.2830.3700.3060.1930.2000.2830.2700.3280.2100.2000.233 0.240
All projects 331 0.330 0.101 0.254 0.159 0.152 0.334 0.105 0.272 0.162 0.144 0.355 0.107 0.296 0.171 0.149
we noted that the model was misguided by source code comments
presentinthetop-1retrievedhunk.Sincesourcecodecomments
are formulated in natural language, a highly-contextual model like
BERTtendstoemphasizetheirsimilaritywiththebugreportasitis
also expressed in natural language. For both of the above error cat-
egories, webelieve thatthe wholesaleremoval ofthe problematic
text(i.e,commentsfromcodeandcodesnippetsandstacktraces
from bug reports) would negatively affect the model as it removes
bothrelevantandirrelevantinformation.Hence,researchersshould
explorestrategiestotreatthisdataseparately,perhapsbyencodingtheircontentwithinBERTwithspecialtokensakintotheARCand
ARCLstrategies we discuss in this paper.
Finally,for5ofthebugreports,FBL-BERTfailedduetospurious
matches in code tokens that were split into sub-tokens during
preprocessing.OneofthepreviouslyobservedstrengthsofBERT
isinusingtheWordPiecealgorithmtoavoidtheout-of-vocabularyproblembysplittingunseentokensintothelargestsub-tokensthat
arepartoftheBERTvocabulary[ 20].Sincesourcecodeidentifier
namesaretypicallyproject-specificwords,theydonotoccurinthe
pre-trained vocabulary, hence they are often split by WordPiece
(e.g.,ManagerServlet ‚Üímanager,##servlet).Thesub-tokenscan
thenspuriouslymatchotherterms,includingsub-tokensfromother
split identifiers, but not the whole, unsplit term. Researchers in the
biomedical domain recognized the same issue affecting medical
terms and proposed domain-specific BERT adaptations [2, 11, 26].
5.2 RQ2: Changeset encoding strategy
Table3showsretrievalperformanceofFBL-BERTtrainedandeval-
uatedwithdifferentchangesetencodingstrategiesandinputdatagranularities. For eachproject, the three bestperforming configu-
rationsarehighlighted,suchthatdarkgreenmarksaconfiguration
with the highest retrieval performance, while green and yellow
correspond to the second and third best configurations. Overall,
wenoticethatusingentirechangesetsasthegranularityofinput
results in, by far, the worst performance across all of the investi-
gatedconfigurationsforallevaluationprojects.Wecanattribute
this result to: (1) truncation of changesets due to input length limi-
tation of the BERT model; and (2) tangled changes within a single
changeset [ 14], which are likely to affect the model by introducing
noiseviaunrelatedcodemodifications.Ontheotherhand,while
the model based on hunks or changeset-files is not free of these
problems, the finer data granularity allows it to partially overcome
them. For instance, in case of tangled changes, dividing the en-
tirechangesetintohunksorchangeset-filescreatesmultiplenew
data points, which limits the noise introduced by instances that
arepoorlyrelatedtothebug.Thedifferenceinretrievalaccuracy
acrossall themetricsbetweenusing hunksandchangeset-filesas
the input data is minor and differs from 1% to 12.2% per project.
Thisresultsisindicativeof theobservationthatleveraginghunks
andchangeset-filesperform similarlyandareboth resilienttothe
problems affecting changesets.
Examining the results for different changest encoding strate-
gies,weobservethatARC Lperformsuniversallybestacrosshunks
andchangeset-files.Interestingly,atthelevelofchangeset-files,the
baselineencodingD,whichdoesnotencodemodificationtype,doessurprisinglywellandoutperformsARCencoding.Weattributethisresulttothespecificsof ARCencoding,whichgroupslinesbasedon
the performed modification, hence in the case of larger documents
the grouping may affect the semantics of the documents. On the
954ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Ciborowska and Damevski
other hand, ARC encoding for hunks is less likely to be susceptible
to that problem since hunks are typically much shorter. Analyzing
theresultsfordifferentprojects,weobservethatARC Lperforms
best for AspectJ, JDT and Tomcat, with an improvement in MRR
scores of 0.7%, 6.7% and 4.1% over their second best configurations
respectively, while ARC is the most beneficial strategy for the PDE
project. In the case of SWT, we observe the highest retrieval ac-
curacy with ARC L, while ZXing performs best with D encoding;
however, both of these observations are likely negligible given the
lowdifferencebetweenARC LandotherencodingsforSWT,and
therelativelyfewerbugreportsintheZXingproject.Overall,we
conclude thatleveraging changesets semantics via encodingmod-
ification with either ARC and ARC Lincreases retrieval accuracy
overtheDconfigurationwhichdoesnotprovidethemodelwith
additional information about the change. However, based on these
results, the difference between ARC and ARC Lis not significant
enough to clearly indicate which strategy is superior on average.
5.3 Threats to validity
Theconclusionsofthispapersufferfromseveralthreatstovalidity.
A key threat to the internal validity of our study are the specific
parameterchoicesweusedtobuildourFBL-BERTmodel.Amit-
igating factor is that all parameters were either studied by us or
were reported in other prior reputable papers as recommended
oroptimal[ 9,21].Anotherthreatisourautomatedseparationof
bug reports based on localization hints into, not localized, partially
localized, and fully localized, which may result in mistaken cat-
egorization, even though we used a well-known and frequently
followed procedure [23].
Leveraging changesets for bug localization poses another threat
due to possible noise that can be introduced by SZZ [ 42], which
could result in poor quality mapping between bug reports and
bug-inducing changesets. However, the dataset was validated man-
ually[56,62],andthereforesuchmistakes,iftheystillexist,should
notsignificantlyaffectourconclusions.Errorsduetotangledchan-
ges [14,30] are still possible in the dataset as such changes are
difficult to remove manually. We believe tangled commitsto have
affected our final presented results (as discussed in RQ2), however,
sincetangledcommitsareapartofsoftwaredevelopmentremoving
them completely may arguably result in unrealistic evaluation.
Athreattoexternalvalidity,whichconcernstheabilitytogener-
alize our evaluation results, is that we applied the bug localization
technique only on a limited number of bugs collected from a se-lection of popular open source Java projects. A mitigating factoris that the projects have a variety of purposes and developmentstylesandthebenchmarkweusedhasalsobeenappliedtoprior
changeset-basedbuglocalizationstudies[ 45,56,57].Anotherthreat
to external validity is in the chosen evaluation metrics, which may
not directly gauge user satisfaction with our bug localization tech-
nique[54],impactingthevalidityofthereportedresults.Thethreat
is mitigated by the fact that the selected metrics are well-known
and widely accepted as best available to measure and compare the
performance of IR techniques.6 RELATED WORK
Bug localization has generated significant research interest overthe years. In this section, first, we survey related code element-
basedbuglocalizationtechniques,followedbyapproachestowards
bug-inducingchangesetretrieval.Finally,wereviewmethodsfor
encoding changesets characteristics.
6.1 Code element-based bug localization
Buglocalizationtechniquespredominantlyutilizeinformationre-
trieval where the bug report text is used to formulate a query that
is matched to a corpus of code elements, i.e., classes or methods.
To compute similarity between bug reports and source code, the
Vector Space Model (VSM) is often used as one of the simplest and
effective information retrieval algorithms, which is leveraged bymany bug localization techniques. For instance, BugLocator [
22]
combines two rankings, one produced by similarity between the
bugreportandcodeelementsandanotherbasedonsimilarityofthebugreporttopriorfixedbugreports.BLUiR[
45]usescodeandbug
report structure to create groups of terms and computes similarity
betweendifferentgroupsseparately,whileAmaLgam[55]creates
anensembleconsistingofBugLocator,BLUiRandadefectpredic-
tor leveraging the development history of a project. BRTracer [ 57]
focusesonanalyzingandprioritizingstacktraceswhentheyareincluded in bug reports. Kochhar et al. were among the first toreport that evaluation of bug localization was biased by explicitlocalization hints in a significant subset of the included bug re-ports [
23]. VSM-based techniques are likely to perform well on
suchbugreports,thoughlocalizingthemmaynotbeasusefulto
developers [ 48]. Mills et al. refute the idea that VSM-based bug
localizationaresignificantlyaidedbyhints,andnotethatVSMcan
performwellforbuglocalizationifmoreattentionispaidtohow
the query is constructed from the bug report text [ 32]. However,
theirfindingsdonotprecludeadditionalaccuracyimprovements
by using more complex, semantic models, such as BERT.
Morerecently,softwareengineeringresearchershavebeeninter-
estedintheapplyingdeeplearningtechniquestowardsbuglocal-
ization.Forinstance,TRANP-CNN[ 17]isarecenttechniquethat
combines cross-project transfer learning and convolutional neu-
ralnetworkstoachievestate-of-the-artperformanceonfile-level
bug localization. CooBa improves on TRANP-CNN by combining a
sharedencodertocapturecross-projectwithper-projectfeatures
and using adversarial training to ensure that the per-project infor-
mation remains unaffected by noise [ 63]. Lam et al.‚Äôs technique,
DNNLOC, combines a deep neural network with the VSM in order
to be effective across different types of similarity [ 25]. While we
also leverage a deep learning model, BERT is significantly different
fromthesepriortechniques.Recentworkinbuglocalizationalso
includes reports on the value of retrieving changesets instead of
source code elements [33, 56].
6.2 Changeset-based bug localization
Theearliestworkonchangeset-basedbuglocalizationisLocus[ 56],
whichisbasedonVSMmatchingofbugreportstohunks.Toadjust
for localization hints, Locus adapts its similarity scores based on
theproportionofcodeelementmentionsinabugreport.Bhagwan
etal.[3]introducedOrca,atoolthatusesaprovenancegraphto
955Fast Changeset-based Bug Localization with BERT ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
identify commits leading to faulty builds. ChangeLocator [ 58] uses
historical data on software crashes to build a model identifying
relevantchangesetsbasedoncollectiononcrashreports.Although
this approach allows to retrieve changesets, it requires sufficient
amount of historical data to train the model, and a stack trace as
aninput.OneofthebenefitsofVSMisthatitisanunsupervised
approach,henceatrainingcorpusofbugreportsandtheirinducing
commits is not required. However, as VSM fundamentally requires
at least partial token overlap, while it ignores the context in which
tokensappearindocuments,allbuglocalizationtechniquebased
on it have a limited accuracy ceiling [33].
Recently, researchers have also shifted their attention to deep
learningmodelsforchangeset-basedlocalization.Forinstance,Mu-
ralietal.[ 33]proposedBug2Commit,anunsupervisedmodellever-
aging multiple dimension of data associated with bug reports and
commits,suchas metrics,stacktracesor commitmetadata.They
observedthatusingembeddingscanleadtoimprovementinmodelaccuracy when compared to BM25. Lin et al. [
27] studied the trade-
offsbetweendifferentBERTarchitecturesforthepurposeofchange-
set retrieval, and observed the accuracy of Siamese architecture is
on pair with Single-BERT architecture, while being significantly
faster. However, thespeedand interactivity of these models is not
on par with the BERT technique described in this paper.
6.3 Changeset representation
Building a semantically rich representations of changesets is rel-evant to other software engineering applications beyond bug lo-
calization, i.e., just-in-time defect prediction, recommendation of a
code reviewer for a patch, tangled change prediction. Approaches
that define novel changeset embeddings (vector representations ofchangeset), including CC2Vec [
15] and Commit2Vec [ 28], leverage
the difference between added and removed lines of code, among
otherchangesetcharacteristics.Corleyetal.[ 7]studiedhowinclud-
ingdifferenttypesoflinesfromachangesetaffectstheperformance
ofLatentDirichletAllocation-basedfeaturelocation,observingthat
including context, additions, and log messages, but excluding re-
movedlines,achievesthebestperformance.However,thesestudies
didnotutilizeatransferlearningtechnique,likeBERT,whichre-
quires compatibility with a pre-trained model, and also prior work
did not extensively explore hunks as a primary data dimension.
7 CONCLUSION
Thispaperpresentsanapproachforautomaticallyretrievingbug-
inducingchangesetsforanewlyreportedbug.Theapproachuses
thepopularBERTmodeltomoreaccuratelymatchthesemantics
in the bug report text to the inducing changeset. More specifically,
we describe the FBL-BERT model, based on the prior work by
Khattabetal.[ 21],whichspeed suptheretrievalofresultswhile
performing fine grained matching across all embeddings in the
two documents. The results show an improvement in retrieval
accuracyforbugreportsthatlacklocalizationhintsorhaveonly
partial hints. We also evaluate different approaches for utilizing
changesetsinBERT-likemodels,producingrecommendationson
the input data granularity and the use of special tokens for the
purpose of capturing changeset semantics.REFERENCES
[1]2020.Replicationpackage. https://anonymous.4open.science/r/fbl-bert-D567/
README.md
[2]Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: Pretrained Language
Model for Scientific Text. In EMNLP. arXiv:arXiv:1903.10676
[3]RanjitaBhagwan,RahulKumar,ChandraSekharMaddila,andAdithyaAbraham
Philip. 2018. Orca: Differential Bug Localization in Large-scale Services. In
Proceedings of the 12th USENIX Conference on Operating Systems Design and
Implementation (Carlsbad, CA, USA) (OSDI‚Äô18). 493‚Äì509.
[4]J.Cao,S.Yang,W.Jiang,H.Zeng,B.Shen,andH.Zhong.2020. BugPecker:Locat-
ing Faulty Methods with Deep Learning on Revision Graphs. In 35th IEEE/ACM
International Conference on Automated Software Engineering (ASE) .
[5]Zimin Chenand Martin Monperrus.2019. Aliterature study ofembeddings on
source code. arXiv preprint arXiv:1904.03061 (2019).
[6]S. Cheng, X.Yan,and A. A. Khan. 2020. A SimilarityIntegration Method based
Information Retrieval and Word Embedding in Bug Localization. In 2020 IEEE
20th International Conference on Software Quality, Reliability and Security (QRS) .
[7]C.S.Corley,K.Damevski,andN.A.Kraft.2018.Changeset-BasedTopicModeling
of Software Repositories. IEEE Transactions on Software Engineering (2018).
[8]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with
ContextualNeuralLanguageModeling.In Proceedingsofthe42ndInternational
ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR‚Äô19).
[9]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
for Computational Linguistics: Human Language Technologies .
[10]Jean-R√©myFalleri,Flor√©alMorandat,XavierBlanc,MatiasMartinez,andMar-tin Monperrus. 2014. Fine-grained and accurate source code differencing. In
ACM/IEEE International Conference on Automated Software Engineering, ASE ‚Äô14,
Vasteras, Sweden - September 15 - 19, 2014. 313‚Äì324. https://doi.org/10.1145/
2642937.2642982
[11]Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu,
Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-Specific
Language Model Pretraining for Biomedical Natural Language Processing.
arXiv:2007.15779[cs.CL]
[12]Jin Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically Enhanced
SoftwareTraceabilityUsingDeepLearningTechniques.In Proceedingsofthe39th
International Conference on Software Engineering (ICSE ‚Äô17).
[13]Kim Herzig and Andreas Zeller. 2013. The impact of tangled code changes. In
2013 10th Working Conference on Mining Software Repositories (MSR).
[14]Kim Herzig and Andreas Zeller. 2013. The Impact of Tangled Code Changes. In
Proceedings of the 10th Working Conference on Mining Software Repositories (San
Francisco, CA, USA) (MSR ‚Äô13). 121‚Äì130.
[15]Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Dis-
tributedrepresentationsofcodechanges.In ProceedingsoftheACM/IEEE42nd
International Conference on Software Engineering .
[16]X. Huo, F. Thung, M. Li, D. Lo, and S. Shi. 2019. Deep Transfer Bug Localization.
IEEE Transactions on Software Engineering (2019).
[17]X. Huo, F. Thung, M. Li, D. Lo, and S. Shi. 2019. Deep Transfer Bug Localization.
IEEETransactionsonSoftwareEngineering (2019),1‚Äì1. https://doi.org/10.1109/
TSE.2019.2920771
[18]Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diffs using neural machine translation. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
[19]Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2017. Billion-scale similarity
search with GPUs. arXiv preprint arXiv:1702.08734 (2017).
[20]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Open-Vocabulary Models for Source Code. In Proceedings of
the ACM/IEEE 42nd International Conference on Software Engineering: Companion
Proceedings (Seoul, South Korea) (ICSE ‚Äô20). 294‚Äì295.
[21]OmarKhattabandMateiZaharia.2020. ColBERT:EfficientandEffectivePassage
Search via Contextualized Late Interaction over BERT (SIGIR ‚Äô20).
[22]D.Kim,Y.Tao,S.Kim,andA.Zeller.2013. WhereShouldWeFixThisBug?A
Two-Phase RecommendationModel. IEEE Transactionson Software Engineering
39, 11 (Nov 2013), 1597‚Äì1610.
[23]Pavneet Singh Kochhar, Yuan Tian, and David Lo. 2014. Potential Biases in Bug
Localization: Do They Matter?. In Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering (Vasteras, Sweden) (ASE ‚Äô14).
803‚Äì814.
[24]A.N.Lam,A.T.Nguyen,H.A.Nguyen,andT.N.Nguyen.2015. CombiningDeep
Learning with Information Retrieval to Localize Buggy Files for Bug Reports
(N). In2015 30th IEEE/ACM International Conference on Automated Software
Engineering (ASE). 476‚Äì481.
[25]AnNgocLam,AnhTuanNguyen,HoanAnhNguyen,andTienN.Nguyen.2017.
Bug Localization with Combination of Deep Learning and Information Retrieval.
InProceedings of the 25th International Conference on Program Comprehension
956ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Ciborowska and Damevski
(BuenosAires,Argentina) (ICPC‚Äô17).IEEEPress,218‚Äì229. https://doi.org/10.
1109/ICPC.2017.24
[26]Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,
Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical lan-
guagerepresentationmodelforbiomedicaltextmining. Bioinformatics (Sep2019).
https://doi.org/10.1093/bioinformatics/btz682
[27]Jinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2021.
Traceability Transformed: Generating more Accurate Links with Pre-Trained
BERT Models. arXiv:2102.04411 [cs.SE]
[28] Roc√¨o Cabrera Lozoya, Arnaud Baumann, AntoninoSabetta, and Michele Bezzi.
2019. Commit2Vec: Learning Distributed Representations of Code Changes.
arXiv:1911.07605
[29]Walid Maalej and Hans-J√∂rg Happel. 2010. Can development work describe
itself?.In 20107thIEEEWorkingConferenceonMiningSoftwareRepositories(MSR
2010).
[30]Shane McIntosh, Bram Adams, Thanh H.D. Nguyen, Yasutaka Kamei, and
Ahmed E. Hassan. 2011. An Empirical Study of Build Maintenance Effort. In
Proceedings of the33rd InternationalConference onSoftware Engineering (Waikiki,
Honolulu, HI, USA) (ICSE ‚Äô11). 141‚Äì150.
[31]TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013.DistributedRepresentationsofWordsandPhrasesandtheirCompositionality.
InAdvances inNeural Information ProcessingSystems, C.J. C. Burges,L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.).
[32]Chris Mills, Esteban Parra, Jevgenija Pantiuchina, Gabriele Bavota, and Sonia
Haiduc. 2020. On the relationship between bug reports and queries for text
retrieval-based bug localization. Empirical Software Engineering 25 (2020).
[33]Vijayaraghavan Murali, Lee Gross, Rebecca Qian, and Satish Chandra. 2020.
Industry-scaleIR-basedBugLocalization:APerspectivefromFacebook.In Pro-
ceedings of the 42nd International Conference on Software Engineering (ICSE ‚Äô20).
[34]E. C. Neto, D. A. da Costa, and U. Kulesza. 2018. The impact of refactoring
changes on the SZZ algorithm: An empirical study. In IEEE 25th International
ConferenceonSoftwareAnalysis,EvolutionandReengineering(SANER) (SANER
2018).
[35]A. T. Nguyen,T. T. Nguyen,J. Al-Kofahi, H. V.Nguyen, and T. N. Nguyen.2011.
A Topic-based Approach for Narrowing the Search Space of Buggy Files from
aBugReport.In Proceedingsofthe26thIEEE/ACMInternationalConferenceon
AutomatedSoftwareEngineering(ASE2011).263‚Äì272. https://doi.org/10.1109/
ASE.2011.6100062
[36]A. T. Nguyen,T. T. Nguyen,J. Al-Kofahi, H. V.Nguyen, and T. N. Nguyen.2011.
Atopic-basedapproachfornarrowingthesearchspaceofbuggyfilesfromabug
report.In 201126thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (ASE 2011). 263‚Äì272.
[37]RodrigoNogueiraandKyunghyunCho.2020. PassageRe-rankingwithBERT.
arXiv:1901.04085[cs.IR]
[38]JeffreyPennington,RichardSocher,andChristopherD.Manning.2014. GloVe:
GlobalVectorsforWordRepresentation.In EmpiricalMethodsinNaturalLan-
guage Processing (EMNLP).
[39]Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018.
DissectingContextualWordEmbeddings:ArchitectureandRepresentation.In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing.
[40]Michael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik
Meijer,andSatishChandra.2020. Scaffle:BugLocalizationonMillionsofFiles.
InProceedings of the 29th ACM SIGSOFT International Symposium on Software
Testing and Analysis (ISSTA 2020).
[41]NilsReimersand IrynaGurevych.2019. Sentence-BERT: SentenceEmbeddings
using Siamese BERT-Networks. arXiv:1908.10084[cs.CL]
[42]Giovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele
Bavota,MicheleLanza,andRoccoOliveto.2021.EvaluatingSZZImplementations
Through a Developer-informed Oracle. arXiv:2102.03300 [cs.SE]
[43]Christoffer Rosen, Ben Grawi, and Emad Shihab. 2015. Commit Guru: Analytics
andRiskPredictionofSoftwareCommits.In Proceedingsofthe201510thJoint
Meeting on Foundations of Software Engineering (Bergamo, Italy) (ESEC/FSE 2015).
Association for Computing Machinery, New York, NY, USA, 966‚Äì969.
[44]Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on Source Code: A Neural Code Search. In Proceedings
of the 2nd Workshop on Machine Learning and Programming Language (MAPL
2018).
[45]RiponK.Saha,MatthewLease,SarfrazKhurshid,andDewayneE.Perry.2013. Im-
proving Bug Localization Using Structured Information Retrieval. In Proceedings
ofthe28thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(Silicon Valley, CA, USA) (ASE‚Äô13). 345‚Äì355.
[46]T. Savor, M. Douglas, M. Gentili, L. Williams, K. Beck, and M. Stumm. 2016.
Continuous Deployment at Facebook and OANDA. In 2016 IEEE/ACM 38th Inter-
national Conference on Software Engineering Companion (ICSE-C). 21‚Äì30.
[47]M.SchusterandK.Nakajima.2012. JapaneseandKoreanvoicesearch.In 2012
IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP) .[48]Jeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and
NamedEntityRecognitioninStackOverflow.In Proceedingsofthe58thAnnual
Meeting of the Association for Computational Linguistics. Association for Com-
putationalLinguistics,Online,4913‚Äì4926. https://doi.org/10.18653/v1/2020.acl-
main.443
[49]Jeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and
NamedEntityRecognitioninStackOverflow.In Proceedingsofthe58thAnnual
Meeting of the Association for Computational Linguistics.
[50]Chakkrit Tantithamthavorn, Surafel Lemma Abebe, Ahmed E. Hassan, Akinori
Ihara, and Kenichi Matsumoto. 2018. The impact of IR-based classifier con-figuration on the performance and the effort of method-level bug localization.
Information and Software Technology (2018).
[51]IanTenney,DipanjanDas,andElliePavlick.2019. BERTrediscoverstheclassical
NLP pipeline. arXiv preprint arXiv:1905.05950 (2019).
[52]Z. Wan, X. Xia, A. E. Hassan, D. Lo, J. Yin, and X. Yang. 2020. Perceptions,
Expectations,andChallengesinDefectPrediction. IEEETransactionsonSoftware
Engineering 46, 11 (2020).
[53]QianqianWang,ChrisParnin,andAlessandroOrso.2015. EvaluatingtheUse-
fulness of IR-Based Fault Localization Techniques. In Proceedings of the 2015
International Symposium on Software Testing and Analysis (Baltimore, MD, USA)
(ISSTA 2015). 1‚Äì11.
[54]QianqianWang,ChrisParnin,andAlessandroOrso.2015. EvaluatingtheUse-
fulness of IR-Based Fault Localization Techniques. In Proceedings of the 2015
International Symposium on Software Testing and Analysis (ISSTA 2015) (Balti-
more, MD, USA). 1‚Äì11.
[55]ShaoweiWangandDavidLo.2014.VersionHistory,SimilarReport,andStructure:PuttingThemTogetherfor ImprovedBug Localization.In Proceedingsofthe22Nd
International Conference on Program Comprehension (Hyderabad, India) (ICPC
2014). 53‚Äì63.
[56]MingWen,RongxinWu,andShing-ChiCheung.2016. Locus:LocatingBugsfrom
Software Changes.In Proceedingsof the31st IEEE/ACMInternational Conference
on Automated Software Engineering (Singapore, Singapore) (ASE 2016). 262‚Äì273.
[57]Chu-Pan Wong, Yingfei Xiong, HongyuZhang, Dan Hao, Lu Zhang,and Hong
Mei. 2014. Boosting Bug-Report-Oriented Fault Localization with Segmentation
andStack-TraceAnalysis.In Proceedingsofthe2014IEEEInternationalConference
on Software Maintenance and Evolution (ICSME ‚Äô14). 181‚Äì190.
[58]Rongxin Wu, Ming Wen, Shing-Chi Cheung, and Hongyu Zhang. 2018. Change-
Locator: Locate Crash-Inducing Changes Based on Crash Reports. In Proceedings
of the 40th International Conference on Software Engineering (ICSE ‚Äô18).
[59]XinYe,RazvanBunescu,andChangLiu.2014. LearningtoRankRelevantFilesforBugReportsUsingDomainKnowledge.In Proceedingsofthe22NdACMSIGSOFT
International Symposium on Foundations of Software Engineering (Hong Kong,
China)(FSE 2014). 689‚Äì699.
[60]ZhengranZeng,YuqunZhang,HaotianZhang,andLingmingZhang.2021. Deep
Just-in-Time Defect Prediction: How Far Are We? (ISSTA 2021).
[61]Wen Zhang, Ziqiang Li, Qing Wang, and Juan Li. 2019. FineLocator: A novelapproach to method-level fine-grained bug localization by query expansion.
Information and Software Technology 110 (2019), 121‚Äì135.
[62]J. Zhou, H. Zhang, and D. Lo. 2012. Where should the bugs be fixed? More
accurateinformationretrieval-basedbuglocalizationbasedonbugreports.In
Proceedingsofthe34thInternationalConferenceonSoftwareEngineering(ICSE).
14‚Äì24. https://doi.org/10.1109/ICSE.2012.6227210
[63]Ziye Zhu, Y. Li, Hanghang Tong, and Yu Wang. 2020. CooBa: Cross-project Bug
Localization via Adversarial Transfer Learning. In IJCAI.
[64]W. Zou, D. Lo, Z. Chen, X. Xia, Y. Feng, and B. Xu. 2020. How Practitioners
Perceive Automated Bug Report Management Techniques. IEEE Transactions on
Software Engineering 46, 8 (2020).
957