Multilingual training for Software Engineering
Toufique Ahmed
University of California, Davis
Davis, California, USA
tfahmed@ucdavis.eduPremkumar Devanbu
University of California, Davis
Davis, California, USA
ptdevanbu@ucdavis.edu
ABSTRACT
Well-trainedmachine-learningmodels,whichleveragelargeamounts
of open-source software data, have now become an interesting ap-
proach to automating many software engineering tasks. Several
SE tasks have all been subject to this approach, with performance
graduallyimprovingoverthepastseveralyearswithbettermodels
andtrainingmethods.More,andmorediverse, clean,labeled datais
betterfortraining;butconstructinggood-qualitydatasetsistime-
consuming andchallenging. Ways ofaugmenting the volumeand
diversityofclean,labeleddatagenerallyhavewideapplicability.For
some languages (e.g., Ruby) labeled data is less abundant; in others
(e.g.,JavaScript)theavailabledatamaybemorefocusedonsomeap-
plicationdomains,andthuslessdiverse.Asawayaroundsuchdata
bottlenecks,wepresentevidencesuggestingthathuman-written
code in different languages (which performs the same function),
israthersimilar,andparticularlypreservingofidentifiernaming
patterns; we further present evidence suggesting that identifiers
areaveryimportantelementoftrainingdataforsoftwareengineer-
ingtasks.Weleveragethisratherfortuitousphenomenontofind
evidence that available multilingual training data (across different
languages)canbeusedtoamplifyperformance.Westudythisfor
3 different tasks: code summarization, code retrieval, and function
naming. We note that this data-augmenting approach is broadly
compatiblewithdifferenttasks,languages,andmachine-learning
models.
CCS CONCEPTS
â€¢Software and its engineering â†’Software notations and
tools;â€¢Computing methodologies â†’Machine learning.
KEYWORDS
codesummarization,codesearch,methodnameprediction,deep
learning
ACM Reference Format:
Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual training for
Software Engineering. In 44th International Conference on Software Engi-
neering(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA. ACM,NewYork,
NY, USA, 13 pages. https://doi.org/10.1145/3510003.3510049
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.35100491 INTRODUCTION
Researchers in the NLP area have reported that multilingual train-
ing is beneficial for low-resource language [ 16,23,57,63]. Several
papersshowthatmultilingual-trainedmodelsshowbetterperfor-
mance [36,62] and are more practical to deploy [ 9]. However, this
is observed in two situations: 1) for low-resource languages and 2)
when the languages are related. We find that programs in different
languagessolvingthesameproblemusemoresimilaridentifiers;
furthermore different languages sometimes have similar keywords
andoperators.Highcapacitydeeplearning models arecapableof
learninginterlingua :sharedsemanticrepresentationbetweenlan-
guages[34].Moreover,withtaskslikesummarization,ormethod
naming, we are dealing with a simplified, many-to-one setting:
translating multiple source languages to a single target language),
which is believed to be easier than multi-way task [ 20,76]. We
beginbyintroducingthecodesummarizationtask,whichweuse
to motivate multilingual training.
Developersoftenrelyheavilyoncomments,togainaquick(even
if approximate) understanding of the specification and design ofcode they are working on. An actual example of a comment is
showninFigure1.Suchcommentshelpadevelopergainaquick
mental preview of whatthe proximate code does, and howit might
goaboutit;thishelpsthedeveloperknowwhattolookforinthe
code. Knowing that such comments are useful to others (or evenlatertooneself)incentivizesdeveloperstocreatecommentsthatexplain the code; however the resulting redundancy (viz., code
that does something,and some nearby Englishtext that describes
justwhat thecodedoes),with thesameconcept expressedintwo
languages results in a bit of extra work for the original coder.
Thisextrawork,ofcreatingalignedcommentsexplainingthecode,
canbefruitfullyviewed[ 21]asataskrelatedto naturallanguage
translation (NLT) (e.g., translating English to German). The mature
& powerful technology of NLT becomes applicable for comment
synthesis;MLapproachesdevelopedfortheformercanbeusedfor
the latter. An effective comment synthesizer could help developers:
bysavingthemthetroubleofwritingcomments;andperhapseven
be used on-demand in the IDE to create descriptions of selected
bits of code.
Comment synthesis is now an active research area, including
manyprojectssuchasCodeNN[ 30],DeepCom[ 26],Astattgru[ 40],
CodeBERT[ 18],Rencos[ 74],SecNN[ 42],PLBART[ 1],CoTexT[ 54],
ProphetNet-X[ 55],NCS[2],Code2seq[ 7],Re2Com[71],andmany
more [19,24,25,27,28,38,39,41,49,50,66,67,69,70,72,73].
All these approaches rely on datasets of aligned code-comment
pairs. Typically, these datasets are then used to train complex deep
learning models to model a probabilistic distribution of the form
p(comments|code) ;onecansamplefromthese(usuallygenerative)
modelstocreatecandidatecommentsforagivenapieceofcode.
Givena datasetofcode-comment pairsina specificlanguage, e.g.,
14432022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Toufique Ahmed and Premkumar Devanbu
Java, or Python, or PHP, or Ruby, one can train models to translate
code inthatlanguage to comments. The quality of the translation
will depend largely upon the inductive power of the model, and
quality and diversity of the code-comment dataset.
//Returns the text con tent of
//this node and its descendants.
public String getTextContent() {
StringBuilder sb= newStringBuilder(g etChildNodesCount ()+1);
appendTextContent(sb);
return sb.toString();
}
Figure 1: Example for code comment generation task
Oflate,giventhepowerofGPUs,andthecapacityofthemod-
els, the limitations largely arise from dataset quality and diversity,
especiallyinlanguagesforwhichlimited,orratherspecializeddata
is available. For instance, CodeXGLUE [ 47] dataset consists of six
languages( i.e.,Ruby,Java,JavaScript, Go,Php,Python). Mostlan-
guages havewell over 100,000training examples, covering awide
set of application domains. Some languages, particularly Ruby and
Javascript, have far fewer examples, and cover a narrower range of
applicationdomains.Asaresult,state-of-the-artmodelsperform
less well for these two languages. This is a well-known problem
fornaturallanguagetranslation:whiletrainingdataforlanguage
pairslike ð¸ð‘›ð‘”ð‘™ð‘–ð‘ â„Žâ†”ð¹ð‘Ÿð‘’ð‘›ð‘â„Žisabundant,resourcesmaybelacking
for less-used languages like ð‘„ð‘¢ð‘’ð‘â„Žð‘¢ð‘Ž orðµð‘Žð‘‘ð‘Žð‘”ð‘Ž. In such cases, a
common technique is adapt ML models to learn useful statistics
fromabundantdatainother,perhapsrelatedlanguages[ 51].This
workswellwhenlanguagesoftenhavesimilargrammars,andshare
common word etymologies.
Weproposeananalogousapproachtoimprovethediversityand
qualityoftrainingdataforsoftware-engineeringtasks,exploiting
an interesting property of source code that human beings write.
Itâ€™s generally agreed that variable names help code comprehen-
sion[37].Developersknowthis,andtypicallychoosedescriptive
variable names (reflective of code logic and purpose) regardless
of the language they are coding in. Thus, one could expect that
developerscoding thesamefunctionality,usingsimilaralgorithms,
even in different languages, will use similar variable names . This
suggests that machine-learning approaches could sometimes lever-
age corpora in different programming languages. This paper a)
shows that this expectation actually has a sound empirical basis,
and then b) demonstrates that this approach in fact works not just
for code summarization, but also for several other tasks. We make
the following contributions.
(1)Using the RosettaCode dataset, we provide evidence that
programssolvingthesameproblemindifferentlanguages
are more likely to use the same or similar identifier names.
(2)Weshowevidencesuggestingthatcross-languagetraining
(e.g.,trainonPython,testonRuby)cansometimesleadto
better performance than same-language training.
(3)Westudy therelativevalueofidentifiers andsyntax,using
ablation, and find that identifier names may matter more.
(4)We show that pooled multilingual training data improves
performanceonseveraltasks,butespeciallyforlanguageslacking in diverse and abundant data. We top a leaderboard
for code-comment synthesis1.
(5)Weshowthatmultilingualtraininghelpsfortwoothertasks:
code retrieval , andmethod name prediction .
(6)Finally, we evaluate a few different design choices for multi-
lingual training, and discuss threats to our findings.
Overall, this paper a) shows that multilingual training is yet
anotherusefultechniqueinthegeneralarsenalofMLapproaches
to exploit the naturalness of code, b) shows why it is useful, and c)
shows how to take good advantage of it.
Note:Technicaldetailsfollow,butprecisely:whatwestudyhereis
multilingual training in the fine-tuning stage of â€œfoundation mod-
els"[12].Foundationmodelsforcode,likeCodeBERT,GraphCode-
BERT [18,22] already use multilingual data for pre-training . While
pre-training is self-supervised and is done with unlabeled corpora,
task-specific fine-tuningis usually supervised, usingclean, hard-
won labeled data; multilingual pooling can be useful here.
2 BACKGROUND & MOTIVATION
We now present some motivating evidence suggesting the value of
multilingualtrainingdatafordeep-learningapplicationstosoftware
tasks. We begin the argument focused on code summarization.
Deep learning models have been widely applied to code summa-
rization, with papers reporting substantial gains in performance
over recent years [ 1,2,7,18,19,24â€“28,30,38â€“42,50,54,55,66,67,
69â€“74]. We focus hereon what information in the code ML models
leverage for summarization (while we use summarization to mo-
tivate the approach, we evaluate later on 3 different tasks). Does
every token in the program under consideration matter, for the
code summarization task? Or, are the function and variable names
usedintheprogramsmostimportant?Sinceidentifierscarrymuch
information about the program, this may be a reasonable assump-
tion.
Consideringthecontentwords2intheexampleinFigure1there
arefourmajorterms( i.e.,Returns,textcontent,node,anddescen-
dants) used in the summary. The first 3 directly occur as tokens or
subtokens in the code. Though the word â€œdescendants" is missing
in the program, high capacity neural models like BERT [ 17] can
learntostatisticallyconnect, e.g.,"descendant"withtheidentifier
subtoken â€œchildâ€. This suggests that, perhaps, comments are recov-
erable primarily from identifiers. If this is so, and identifiers matter
more for comments than the exact syntax of the programming lan-
guage,thatmayactuallybeverygoodnewsindeed.Ifdevelopers
choose identifiers in the same way across different languages ( viz.,
problem-dependent, rather than language dependent) perhaps we
canimprove thediversityandqualityofdatasetbypoolingtraining
set across may languages. Pooled data sets may allow us to fine-
tune using multilingual data, and improve performance, especially
forlow-resourcelanguages( e.g.,RubyandJavaScriptfrom Code-
XGLUE [ 47]). Since this is a core theoretical background for our
1This claim is based on publicly available evidence. Please check
https://microsoft.github.io/CodeXGLUE/
2â€œContentâ€ words in linguistics, are words that carry meaning, as contrasted with
functionwords,suchasprepositions,pronouns,andconjunctions,whichdenotegram-
maticalrelationships.Seehttps://en.wikipedia.org/wiki/Content_word.Incode,we
consider function words to be keywords, operators and punctuations, and content
words to be identifiers (functions, variables, types, etc)
1444Multilingual training for Software Engineering ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
work, we start off with two basic research questions to empirically
gauge the possibility and promise of multilingual fine-tuning.
RQ1What role do identifiers play in for code summarization?
RQ2Do programs that solve the same problem in different lan-
guages tend to use similar identifier names?
2.1 RQ1: Role played by identifiers
We first examine the importance of identifiers for code summariza-
tion; specifically, we compare the relative value of identifier tokens
and other tokens. We use the CodeXGLUE dataset and pre-trained
CodeBERT embeddings for the task [ 18]. We begin with a brief
backgrounder on CodeBERT [18] & BERT [17].
CodeBERTusesthepre-training+fine-tuningstrategyofBERT,
RoBERTa etc[17,45]. This approach begins with a self-supervised
â€œpre-trainingâ€ step, to learn textual patterns from a large, unla-
beled, corpus using just the content; in the next step, â€œfine-tuningâ€,
task-specific labeleddataisusedtoprovidetask-relatedsupervised
training.Thisapproachisknowntoachievestate-of-the-artperfor-
manceinbothnaturallanguageprocessing,andsoftware-related
tasks [3, 4, 11, 18, 22, 32, 33, 35, 50, 75].
We study the effect of identifiers in several steps. For the pre-
training step, we start with the available CodeBERT model, which
is pre-trained on a large, multilingual corpus of code. For the fine-
tuning step, for this task, we use the CodeXGLUE benchmark
dataset (seetable 4 forlanguages and datasetsizes); we startwith
the original set of code-comment pairs, and apply two different
treatments to create overall three different fine-tuning training
datasetsâ€“1)basecaseleavingcodeasis,2)atreatmenttoempha-
size identifiers, and 3) a treatment to de-emphasize them. First,
toemphasizeidentifiersweabstractouttheprogramâ€™skeywords,
separators, and operators by replacing those with three generic
tokens (i.e., â€œkeyâ€, â€œsepâ€, and â€œoptâ€), thus forcing the model (during
fine-tuning) to rely more on the identifiers, for the task. Next, to
assesstheimportanceofkeywords,separators,andoperators,we
abstract out the identifiers with a generic token â€œidâ€. We fine-tune
the model separately after each of these abstraction steps, thus
yielding3fine-tunedmodels:thebaseline,keyword-abstracted,and
identifier-abstracted.Wecomparetheresults(smoothedBLEU-4)
across all three.
Ifafine-tunedmodelâ€™sperformanceisrelativelyunaffectedby
anabstraction,onemayinferthatthemodelrelieslessontheab-
stractedtokens.Weperformtheseexperimentswithtwolanguages
withlow-resource( i.e.,RubyandJavaScript,Seetable4)andtwo
languages with high-resource ( i.e., Java and Python ). We train,
validate, and test with the same dataset in each case. For each test
instance,wehaveonevaluefromthecompleteprogramandanother
one fromeach of the twoabstracted versions. Wecompared these
values, using two distinct pair-wise Wilcoxon tests: 1) Alternative
Hypothesis (AH): complete program > identifier de-emphasis & 2)
AH: complete program > identifier emphasis. We also perform the
same test with the keyword-abstracted and identifier-abstracted
versions (AH: identifier emphasis > identifier de-emphasis).
Thedata(table1)suggeststhatabstractingthekeyword,sepa-
rator,andoperatorhasasmallerimpactontheperformance:the
BLEU-4scoresarerathersimilar(with effectsize rangingfrom0.002
to0.033)tothosefromtheunabstractedcode.Ontheotherhand,DatasetComplete
ProgramAbstracting keyword,
operator, separatorAbstracting
identifiers
BLEU-4 BLEU-4Effect
Sizep-value
(adjusted)BLEU-4Effect
Sizep-value
(adjusted)
Ruby 12.53 11.57 -0.028 0.008 7.94 -0.238 <0.001
JavaScript 13.86 13.06 -0.033 <0.001 9.06 -0.175 <0.001
Java 18.72 18.72 -0.002 0.344 11.41 -0.254 0
Python 18.25 18.10 -0.010 <0.001 11.68 -0.288 0
Table 1: Role played by identifiers
whende-emphasizingidentifiers,theperformancedropsmore,with
effect sizes 5x-100x larger. We find similar results while comparing
the emphasizing and de-emphasizing identifiers versions (omitted
for brevity).
LanguageTraining
Ruby JavaScript Java Go PHP Python
TestingRuby 12.5311.84 13.4212.3213.84 14.09
JavaScript 11.98 13.86 14.1612.5513.90 14.09
Java 13.38 14.57 18.7214.20 16.27 16.20
Go 11.68 11.24 13.61 18.1512.70 13.53
PHP 17.52 19.95 22.11 18.67 25.4821.65
Python 14.10 14.44 16.77 14.92 16.41 18.25
Table 2: Intra and inter language training and testing
Theresultsintable1suggeststhatsyntaxislessrelevantthat
identifiernames.Inallthepriorworks,thetrainingandtestingwere
doneinthesamelanguage.Sincesyntaxislessimportant,couldwe
trainandtestwithdifferentlanguages?TheCodeXGLUEdataset
enables just such an experiment. Using six different languages,
we apply a CodeBERT model fine-tuned in each language, to a
testsetinanotherlanguage.Table2showsthatforhigh-resource
languages( i.e.,Java,go,PHP,andPython),weachievethebestresult
(diagonal) when training and test data are from the same language.
However, the performance does not degrade to a very large extent
when trained with one language and tested on a different one.
SurprisinglyweobservethatforRubyandJavaScript,weactually
achievehigherperformancewhiletrainedwithJava,PHP,andPython
than the language itself . That indicates that code summarization
isnotcompletelydependentonsyntax(perhapsitreliesmoreon
identifier similarity, which we shall explore next)
Finding 1 . Code summarization sometimes appears to train quite
well with data sets from other languages, even if the syntax is
different.
2.2 RQ2: Identifier similarity across Languages
Here, we evaluate RQ2: given a problem, do developers choose
similar, descriptive identifiers, regardless of the programming lan-
guage? Based on the findings in the previous section: if identifiers
were indeed used in similar ways, perhaps code-comment pairs
from any programming language could help train a code summa-
rization model, for any other language . As an example, Figure 2
presentsthatalltheâ€œindexOfâ€functionsimplementedinJava,PHP
and JavaScript use very similar identifiers â€œneedleâ€ and â€œhaystackâ€.
1445ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Toufique Ahmed and Premkumar Devanbu
Quantitatively evaluating this hypothesis requires multiple im-
plementations of the same problem in different programminglan-
guages, where we couldcompare identifier names. Luckily, Roset-
taCodeprovidesjustsuchadataset. RosettaCode currentlyconsists
of 1,110 tasks, 305 draft tasks and includes 838 languages3.W e
collectthemineddata4andstudythesamesixlanguages( i.e.,Ruby,
JavaScript, Java, Go, PHP, and Python) in the CodeXGLUE dataset.
We get 15 cross-language pairs from six languages and measure
identifier similarity between pairs of programs which solve the
same problem in each language ( e.g.,programs for graph diame-
ter problem in Java and Ruby). For baselining, we also compare
witharandompair(solvingdifferentproblems)forthesametwo
languages ( e.g.graph diameter in Java, and SHA-hashing in Ruby).
Fortunately,wefoundsufficientsamplesizesforallourlanguage
pairsinRosettaCode .Forexample,forJava&Pythonwefind544
matchedprogrampairssolvingthesameprobleminbothlanguages.
We then take the 544 Java programs and randomly pair them with
544 other Python programs. Therefore, we have two groups of
programs( i.e.,sameprogramimplementedindifferentlanguages
anddifferentprogramsimplementedindifferentlanguages),and
we check the similarity level between the two groups. Note that
size-unrestricted random pairing may yield misleading results. Sup-
pose we have a Java & Python program matched pair with 100
Java subtokens and 40 Python subtokens. Now, if we replace the
matched python program with a random, bigger program ( e.g., 500
subtokens), we may have more chance of finding matched iden-
tifiers.Therefore,whilechoosingtherandomprogram,wetryto
ensureithasasimilarlengthtotheprogramitisreplacinginthe
pair. We randomly select a program having the subtoken counts
withina5%lengthrange( e.g.,38-42subtokensfora40subtoken
program)oftheremovedone.Fortunately,in 99.25%cases,wegetat
leastoneexamplewithinthe5%range.Ontheremaininginstances,
we select the program with the nearest subtoken count.
We measure identifier similarity thus:
(1)Remove all keywords, operators, and separators from the
programs.
(2)Break all CamelCase and snake_case identifiers and keep
only one copy of each sub token.
(3) Discard too-small programs with less than 5 sub-tokens.
(4)Calculate the mean Szymkiewicz-Simpson coefficient (over-
lap coefficient) [ 65] for both groups ( i.e., same program pair
and random pair) of programs.
(5)Repeatthisprocessacrossall15languagepairs,forallpro-
gram pairs.
Table 3 shows the common program pairs have 89%-235% ad-
ditionalidentifier overlapcompared to random program pairs.
We compare the matched and random pair overlaps using the non-
parametricWilcoxonsigned-ranktest(AH:randomhaslessoverlap
thanmatched).Weobservethatthenullhypothesisisrejected,and
Szymkiewicz-SimpsonOverlapcoefficient5issignificantlyhigher
3Last Accessed August, 2021
4https://github.com/acmeism/RosettaCodeData
5This is a measure of similarity like the Jaccard index; we use it here since sometimes
the sizes of the programs are quite different. Itâ€™s calculated as|ð‘‹âˆ©ð‘Œ|
ð‘šð‘–ð‘›(|ð‘‹|,|ð‘Œ|).for the common program pairs in all the cases. That indicates pro-
gramssolvingthesameproblem(evenindifferentlanguages)are
much more likely to use the same or similar identifier names.
Language
pair#ofcommon
programsOverlap coefficient Effect
Sizep-value
(adjusted) for random
programsfor common
programsincreased in %
Java& Python 544 0.10 0.32 +210.67% 0.747 <0.001
Java & Ruby 532 0.11 0.31 +174.97% 0.751 <0.001
Java & Javascript 411 0.13 0.36 +188.17% 0.774 <0.001
Java & Go 602 0.19 0.36 +89.24% 0.641 <0.001
Java & PHP 282 0.08 0.28 +235.01% 0.740 <0.001
Python & Ruby 538 0.11 0.35 +228.89% 0.780 <0.001
Python & Javascript 377 0.12 0.34 +190.09% 0.728 <0.001
Python & Go 601 0.13 0.31 +133.06% 0.664 <0.001
Python & PHP 267 0.09 0.29 +214.32% 0.679 <0.001
Ruby & Javascript 370 0.13 0.35 +167.02% 0.751 <0.001
Ruby & Go 571 0.12 0.28 +133.47% 0.724 <0.001
Ruby & PHP 262 0.09 0.28 +205.32% 0.716 <0.001
Javascript & Go 418 0.14 0.29 +110.96% 0.635 <0.001
Javascript & PHP 236 0.11 0.29 +175.03% 0.678 <0.001
Go & PHP 293 0.10 0.23 +121.25% 0.562 <0.001
Overall 6304 0.12 0.31 +158.94% 0.697 0
Table 3: Cross-language identifier similarity, when functionality is
preserved
We also calculate each pairâ€™s Jaccard index [ 31] (similarity co-
efficient) and find 112%-309% more similarity between common
pairs than random ones, thus, giving essentially the same result.
However, we prefer to report the detailed result using the overlap
coefficient because Jaccard index can be affected by the differing
verbosity of languages. For example, on average, Java, Python, and
Rubyprogramsin RosettaCode have29.45,17.93,and17.63iden-
tifier subtokens. Java has higher subtokens compared to Python
andRubybecauseoftheimportstatements,packagenamingetc.
Therefore, Jaccard index between Java and Python will be lower
thanthatofPythonandRubyeveniftheprogramsuseverysimilar
identifiers.
Finding 2 . For a given problem, developers are likely to choose
similar identifiers, even if coding in different languages.
In this section, we have presented evidence suggesting that a)
identifiers are important for code summarization, that b) cross-
languagetrainingispromising,andalsothatc)identifierstendto
be used in similar ways across languages. Taken together, these
findings present a strong argument to try multilingual fine-tuning
for SE tasks. Note that it is already well established that multi-
lingual pre-training is helpful, and most BERT-style SE pre-trained
modelsare multilingual[ 1,18,54,55].However, pre-training data
areunsupervisedandeasytocollect.Preparingcleandataforthe
supervised fine-tuning phase requires more time and attention. In
thispaper,ouraimistoprovethatmultilingualtrainingisnotonly
effectiveinpre-trainingstagebutalsoinfine-tuningstageforSE
models,whichisalreadyfoundtobebeneficialfor naturallanguage
models [63].
3 BENCHMARK DATASETS AND TASKS
We evaluate the benefits of multilingual training in the context of
several tasks, and associated datasets. In this section, we discuss
the models and tasks used for our experiments.
1446Multilingual training for Software Engineering ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
public static int indexOf(ByteBuf needle , ByteBuf ha ystack) {
// TODO: maybe use Boyer Moore for efficiency.
intattempts = haystack.r eadableBytes() âˆ’ ne edle.readableBytes() + 1;
for(inti = 0; i < attempts; i++) {
if(equals(needle , needle.readerIndex() ,
haystack , haystack. readerIndex() + i ,
needle.readableBytes())) {
return haystack.rea derIndex() + i;
}
}
return âˆ’1;
}
(a)Java
public static function indexOf(string $haystack , string $needle ,
int$offset=0): int
{
$pos=self :: strpos($haystack , $needle , $o ffset);
return is_int($pos)?$pos:âˆ’1;
}
(b)PHP
function indexOf(haystack , needle) {
if(typeof hays tack==='st ring ')
return haystack.indexOf(needle);
for(let i=0, j=0, l=haystack.length , n= needle.length; i<l; i++) {
if(haystack[i]===needle[j]) {
j++;
if(j= = =n) return iâˆ’j+1;
}
else{
j=0;
}
}
return âˆ’1;
}
(c)JavaScript
Figure2: Usageofsimilaridentifiers( e.g.,needle,haystack)inâ€œin-
dexOfâ€ function in different programming languages
3.1 The Models
Forourstudyofmultilingualtraining,weadopttheBERT,orâ€œfoun-
dationmodelâ€paradigm.Foundationmodels[ 13,15,17,45,56]have
two stages: i) unsupervised pre-training with corpora at vast scale
andii)fine-tuningwithasmallervolumeofsuperviseddataforthe
actual task. Foundation models currently hold state-of-the-art per-
formanceforagreatmanyNLPtasks.BERT[ 17]stylemodelshave
also been adapted for code, pre-trained on a huge, multilingual,
corpora, and made available: CodeBERT and GraphCodeBERT
arebothfreelyavailable:bothsourcecodeandpre-trainedmodel
parameters. While these models for code have thus far generally
been fine-tuned monolingually, they provide an excellent platform
fortrainingexperimentslikeours,tomeasurethegainsofmultilin-
gualfine-tuning.CodeBERT&GraphCodeBERTuseamulti-layer
bidirectional Transformer-based [ 64] architecture, and it is exactly
assameastheRoBERTa[ 45],with125Mparameters;weexplain
them further below.
Pre-training TheCodeBERT[ 18]dataset,hastwoparts:amatched-
pairspartwith2.1Mpairsoffunctionandassociatedcomment(NL-
PLpairs)and6.4Msampleswithjustcode.Thecodeincludessev-
eral programming languages. It was created by Hussain et al.[29].
CodeBERTmodelispre-trainedwithtwoobjectives( i.e.,Masked
Language Modelingand ReplacedToken Detection)on bothparts.Mask language Modeling (MLM) is a widely applied and effec-
tive [17,45] training objective where a certain number of (15%)
tokensaremaskedout,andthemodelisaskedtofindthosetokens.
ForCodeBERTtraining,Feng etal.applythisfirstobjectiveonlyto
bimodaldata[ 18].Thesecondobjective,ReplacedTokenDetection
(RTD)[14],isabinaryclassificationproblemthatisappliedtoboth
unimodalandbimodaldata.Twodatagenerators( i.e.,NLandPL)
generate plausible alternatives for a set of randomly masked posi-
tions,andadiscriminatoristrainedtodeterminewhetherawordis
the original one or not. We note that CodeBERT pre-training is all
aboutrepresentation-learning:bylearningtoperformthetaskwell,
the model learns a good way to encodethe text, which is helpful
during the next, fine-tuning stage. Thepre-training took about 12
hoursonamachinewith16NVIDIAV100cards,andwouldhave
takenusverymuchlonger,soweweregratefultobeabletojust
download the estimated parameters.
Pre-training GraphCodeBERT GraphCodeBERTaugmentssource-
code with data flow, during pre-training. It uses a simple data flow
graph (DFG) encoding a where-the-value-comes-from relation be-
tweenvariables[ 22].TheDFGnodesarevariableoccurrences,edges
arevalueflow.GraphCodeBERTpretraininglearnsajointrepre-
sentation of 1) the DFG structure, 2) DFG alignment with source
code,and 3)thesource code tokensequences. GraphCodeBERT
is therefore pre-trained with three training objectives ( i.e., Edge
Prediction, Node Alignment, and MLM) on 2.3M functions (PL-NL
pairs) from CodeSearchNet [29] dataset. For details see [22].
The pre-training+fine-tuning approach relies on VERY high ca-
pacitymodels,andarepre-trainedoveralarge,multilingualcorpus.
Thus,evenbeforefine-tuning,themodelsalreadyknowalotabout
each language. Thus, fine-tuning on many languages should not
negatively impact what the model knows about any one language.
Thuswefindthatmultilingualfine-tuningimprovesonmonolin-
gual fine-tuning in most cases. We believe our proposed approach
would still consider the context surrounding the individual pro-
gramming language even after multilingual training because these
models have sufficient capacity to do so.
We now describe our tasks: in each, we describe the task, the
dataset, and the multilingual fine-tuning approach (if applicable).
3.2 Code Summarization
The Task: asdescribedearlier,thegoalistogenerateaNLsummary
given code in some PL.
The Dataset: Thereareseveraldifferentcodesummarizationdatasets;
we chose CodeXGLUE6[47], for two main reasons:
(1)CodeXGLUEiscarefullyde-duplicated[ 60].Priordatasets
like TL-CodeSum [ 28] have duplicates [ 60] in training, test-
ing,andvalidationpartitions.Duplicationcaninflatemea-
sured performance [5, 60].
(2)We need a multilingual dataset to prove the effectiveness of
multilingualfine-tuning.Noneoftheexistingdatasets[ 28,
40] is multilingual.
Table 4 presents the number of training, testing and validation
instances for each language. in CodeXGLUE.
6CodeSearchNet[ 29]datasetisastandardbenchmark,whichhasbeenincorporated
into CodeXGLUE
1447ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Toufique Ahmed and Premkumar Devanbu
Programming
languageTraining Dev TestCandidate
codes*
Ruby 24,927 1,400 1,261 4,360
JavaScript 58,025 3,885 3,291 13,981
Java 164,923 5,183 10,955 40,347
Go 167,288 7,325 8,122 28,120
PHP 241,241 12,982 14,014 52,660
Python 251,820 13,914 14,918 43,827
*Candidate codes are only used for code retrieval task
Table 4: CodeXGLUE dataset
Model & Fine-tuning Fenget al.use a transformer-based encode-
decoder architecture for the code summarization task [ 18]. The
encoder is all ready well-trained in the pre-training stage; for fine-
tuning,theencoderisprimedwithweightsfrompre-training.Now,
the transformer model is given the input code token sequence
and asked to generate the comment, as in the Neural Machine
Translation(NMT) problem.Wefine-tuneusing the CodeXGLUE
paired samples. During fine-tuning, the decoder is trained auto-
regressively, using next-token cross-entropy loss. Feng et al.use
smoothBLEU-4[ 44]fortheevaluationsofthemodels.Subsequently,
We replace the pre-trained CodeBERT with pre-trained Graph-
CodeBERT in the encoder while evaluating the effectiveness of
multilingual fine-tuning with GraphCodeBERT.
Why baseline with CodeBERT for code summarization? Fenget al.
compareCodeBERTwithotherpopularencoder-decoderbased( e.g.,
LSTM[61],Transformer[ 64],RoBERTa[ 45])models;CodeBERT
handily beats all of them [ 18]. Thus, CodeBERT is a good baseline
to measure the value of multilingual finetuning. CodeBERT also
doesverywellonpriordatasets:usingsmoothedSentenceBLEU-4,
we found that CodeBERT reaches 44.89 on TL-Codesum [ 28], and
32.92 on Funcom [ 40]7. TL-Codesum has high degree of duplicates;
wefoundthatFuncomalsodoes,butjustinthecomments. Code-
XGLUEhasverylittleduplication,whichmakesitmorechallenging,
and also more reliable. Note that GraphCodeBERT does not report
anyperformanceonthecodesummarizationtask,andsowehad
to measure it.
3.3 Code Search
The Task Given a natural language query, find the semantically
closest code sample from a large set of candidates. Vector-based
information retrieval methods can be used here along withBERT-
styleencoders.CodeBERTwasshowntoperformquitewell;the
best publishedperformance isreported by GraphCodeBERT [ 22]
(CodeBERT augmented with graph representations). We study the
valueofmultilingualfine-tuningforbothCodeBERTandGraph-
CodeBERT(pre-trainingofbothmodelswasdiscussedearlierin
Section 3.1).
The Dataset: Guoetal.adaptthesameCodeSearchNet[ 29]dataset,
withsome additionaldatafor candidatecodes[ 22].Note thatitis
basically the same dataset we used for code summarization except
the candidate codes.
7As reported in [ 21,60], measurement approaches vary across papers, and these
numbersmaydifferfrompriorresults:weusesmoothedsentenceBLEU-4everywhere
in our paper.Model & Fine-tuning WeuseGuo etal.â€™sGraphCodeBERTmodel,
which at the time of submission is the best performing model with
code and parameters available, and so is fine-tunable. The fine-
tuning data is code (PL) matched with (NL) comments, from Code-
XGLUE. The pre-trained GraphCodeBERT embedding vector is
calculatedforeachPLandNLpart.Duringfine-tuning,Guo etal.
takeaminibatch of(say ð‘›)NLqueryvector,alongwith ð‘›(correct
answers) PL answer vectors. ð‘›2dot products are calculated; the
embeddingvectorsarethenfull-stacktrainedtogive"1"normalized
dotproductforthematches,and"0"forthemis-matches.Forthe
actualretrieval,GraphCodeBERTcalculatesthevectorembedding
of a given query, and simply retrieves candidates ranked by the
dot-product distance from the query vector.
3.4 Method Name Prediction
The Task as introduced by Allamanis et al.[6] as the â€œextreme sum-
marizationâ€problem,thetaskistopredictthefunctionnamegiven
the body.
The Dataset: We adaptthe CodeXGLUE datasetby extractingthe
function name and asking the model to find the name given the
functionbody.Following[ 6],thefunctionnamesarebrokeninto
subtokensusingBPE[ 59](weâ€™veusedBPEtokenizationforalltasks).
This problem then becomes very similar to code summarization.
Model & Fine-tuning Previously Code2Seq [ 7] and Code2Vec [ 8]
have worked on this problem. All prior works [ 6â€“8] use a mono-
lingual datasets, which are not suitable for our experiment. We use
thesamemodelweusedforsummarization,exceptwenowlearn
tosequentiallygeneratethemethodname,subtokenbysubtoken.
WeuseF1-scorefortheevaluation.Forexample,thefunctionname
â€œcreateLocalâ€ is broken into two sub tokens ( i.e., create and Local),
andthemodelpredictsonlyâ€œcreateâ€.Hence,theprecision,recall,
and F1-score are 1.0, 0.5, and 0.66, respectively.
4 RESULTS
Inthissection,weevaluatemultilingualfine-tuningforthebase-
lines for the tasks enumerated above.
4.1 Code Summarization
We apply multilingual fine-tuning on the CodeXGLUE dataset.
We firstreplicatethesummarizationtaskby(monolingually)fine-
tuningtheavailablepre-trainedCodeBERTmodelforsixlanguages8.
We replicate the fine-tuning stage for 2 reasons:
(1)We want to account for any hardware or environmental
bias(e.g.,wehaveadifferentsetofGPUsthantheoriginal
paper.Wefine-tunewithNVIDIATITANRTX,whileFeng
et al.[18] use NVIDIA Tesla V100).
(2)Weuseapairwisetwo-samplestatisticaltest(asdescribed
in[58],it ismoreprecise thanjustcomparingtest-set sum-
mary statistics) to gauge differences. This requires a perfor-
mance measurement for each test sample, which the reposi-
tory did not include.
8We use the publicly available CodeBERT implementation and dataset, https://github.
com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text
1448Multilingual training for Software Engineering ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Our BLEU-4 numbers for monolingual training were close to re-
ported numbers, with some differences; but we do obtain the same
overall score (17.83) (table 5, leftmost 2 columns).
We use the same, per-language test sets to compare monolin-
gual and multilingual fine-tuning. The validation set, however, is a
singlemultilingualonecombiningallthemonolingualvalidation
sets. Table 5 shows that multilingual fine-tuning improves perfor-
mance, even for high-resource languages (with more than 100K
traininginstances).WithCodeBERT,multilingualfine-tuninggains
2.5%-17.5%overmonolingualfine-tuning,foralllanguages,yield-
inga6.90%overallimprovement(4.48%weightedimprovement)9.
With themore advanced GraphCodeBERT, wesee smaller gains,
although the relative gains span a wide range.
Weuseaone-sided(AH:monolingual<multilingual) pairwise
Wilcoxon signed-rank test (thus avoiding the corpus-level mea-
surementpitfallsnotedin[ 58]).Nullhypothesisisrejectedforall
sixlanguages, forCodeBERT.For GraphCodeBERT,itâ€™s rejected
overall, and for every language; except for Javascript, where the
p-value is 0.014 (all after B-H correction).
Thusourmeasurementindicatesthatmultilingualfine-tuning
providesastatistically significantimprovementovermonolingual
training. We find rather low effect sizes using Cliffâ€™s Delta [ 48].
While we report the effect size for the sake of completeness, this is
notamajorconcern:wenotethat allgainsarestatisticallyhighly
significant . We also emphasize that even the minor improvements
providedherebymultilingualtraining(whichisbroadlycompatible
witharangeofsettings)constitutearelevantandpotentiallywidely
useful result. Roy et al[58] have previously noted that small gains
in BLEU-4 may not be perceptible to humans as increased text
quality; nevertheless, we note that natural language translation
(whichisnowwidelyused)attainedhighperformancelevelsbased
on decades of incremental progress; this result and others below
provideevidencethat multilingualtrainingcouldbe animportant
stepintheprogresstowardsmoreusefulautomatedtools.Finally,
we notethat BLEU-4gains are higherfor low-resource language
(e.g.,17.7%forRuby),andlowerforhigh-resourcelanguages( e.g.,
2.5% for Python), as expected.
Comparing multi-lingual CodeBERT with other models Code sum-
marization is widely studiedâ€”there are many models for this task;
our specific focus here is to understand if multilingual fine-tuning
provides benefits,using a high-quality token-sequencemodel and
dataset.Sowefocuscomparisonsonthepaperswhichreportperfor-
manceonCodeXGLUEdataset,anduseatoken-sequenceinductive
bias:comparingagainstallmodelsisbeyondthescopeofthispaper.
We compare multi-lingual CodeBERT ( Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT) and
GraphCodeBERT ( Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT) with other models
that have been published in peer-reviewed venues; among them,
four apply pre-training strategies [ 1,18,45,55]. We achieve the
best overall performance (table 6), outperforming all the models,
and for four specific languages ( i.e., Ruby, Java, Go and PHP).
There is one other system, CoTexT [ 54] which claims (in an
unpublished,non-peer-reviewed report)betterperformancethan
9The CodeBERT paper simply averages the BLEU across languages to report the
â€œoverallâ€ number; our weighted average weights each BLEU by the number of samples
in that language.usforjustPython[ 54],butisworseoverall. Wewillincludeitfor
comparison once it is published in a peer-reviewed venue.
Thistablealsoprovidesevidencesupportingtheeffectivenessof
multilingual fine-tuning.
4.2 Code Search
We study the gains from multilingual fine-tuning using two pre-
trained models ( i.e.,CodeBERT & GraphCodeBERT). We multilin-
guallyfine-tunebothmodelsusingthepubliclyavailablecode&
dataset10. As we did for code summarization, we re-trained the
baselinemodels,togetperformancenumbersforeachcaseinthe
testset(toenablepairwisetwo-sampletesting).Weusethesame
testsetsforbothmonolingualandmultilingualtrainingtoevaluate
ourapproach.Duringthetraining,GraphCodeBERTusesamatrix
ofdimension |ð‘žð‘¢ð‘’ð‘Ÿð‘¦|âˆ—|ð‘ð‘Žð‘›ð‘‘ð‘–ð‘‘ð‘Žð‘¡ð‘’ _ð‘ð‘œð‘‘ð‘’ð‘ |.Wecouldnotusethefull
merged validation set (as we did for the code summarization task)
because that makes the query and candidate code sets too large;
the resulting matrix could not fit on our GPU server. We used a
down-sampledvalidationsetcomprisingsixmonolingualvalida-
tion setswith 10K query and 50Kcandidate codes each.However,
wedidnotfaceanyissuewhiletestingbecausewedidnotmerge
the test sets.
Wereportboththepublishedvalues,andourreplication;weneed
the replication to measure pairwise gains. Though CodeBERT and
GraphCodeBERT both work on sequence of code tokens, Graph-
CodeBERT creates a rudimentary data-flow graph, once itâ€™s told
the programming language.
Table 7 shows that multilingual fine-tuning improves the mean
reciprocalrankforalllanguagesexceptGowithCodeBERT.Theim-
provementforRuby,JavaScript,andJavaarestatisticallysignificant.
We found similar results for GraphCodeBERT exhibiting improve-
ment for Ruby, JavaScript, Java, and Python; but with GraphCode-
BERTbothGoandPHPshowedperformancedeclines.However,
overall, both showed statistically signficant improvements (p <
0.001); but the improvement for GraphCodeBERT (1.54%) is lower
than CodeBERT (2.74%). Finally, we note that our numbers for
CodeBERT differ from the performance reported for on the Code-
XGLUE leaderboard. This is because CodeXGLUE benchmark uses
onlyPython,andisbasedonarestrictedsettingwhereidentifier
names are left out. CodeXGLUE team argues that this abstraction
enables them to stress-test the generalization ability of a model;
however,hereweconsideranunmodifiedsettingwheresomeone
gives an natural language query and wishes to find â€œnaturalâ€ code
with variable names intact.
4.3 Method Name Prediction
Asfortheprevioustwotasks,wetrymultilingualfine-tuningfor
methodnamepredictionforCodeBERT.Here,too,wefindevidence
supportingtheconclusionthatmultilingualtrainingprovidesim-
provementforallthelanguages(Table8).Non-parametricpairwise
improvementsaresignificantforRuby,JavaScript,andJava.Wealso
noteobserve relativelygreater effectsizefor Rubyand JavaScript.
Note that we achieve highest improvement for JavaScript because
10https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/codesearch
1449ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Toufique Ahmed and Premkumar Devanbu
LanguageCodeBERT
(reported)CodeBERT
(re-trained)Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT ImprovementEffect
Sizep-value
(adjusted)GraphCodeBERT Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT ImprovementEffect
Sizep-value
(adjusted)
Ruby 12.16 12.53 14.75 +17.72% 0.055 <0.001 12.62 14.95 +18.46% 0.055 <0.001
JS 14.90 13.86 15.80 +14.00% 0.016 <0.001 14.79 15.79 +6.76% 0.016 0.014
Java 17.65 18.72 20.11 +7.43% 0.016 <0.001 19.22 19.91 +3.59% 0.016 <0.001
Go 18.07 18.15 18.77 +3.42% 0.010 <0.001 18.40 18.92 +2.83% 0.010 <0.001
PHP 25.16 25.48 26.23 +2.94% 0.012 <0.001 25.45 26.15 +2.75% 0.012 <0.001
Python 19.06 18.25 18.71 +2.52% 0.022 <0.001 18.02 18.90 +4.88% 0.022 <0.001
Overall 17.83 17.83 19.06 +6.90%0.016 <0.00118.08 19.10 +5.64%0.016 <0.001Overall
(weighted)Not
Reported19.85 20.74 +4.48% 19.98 20.76 +3.90%
*Evaluation criteria followed by CodeXGLUE [47] and CodeBERT [18]
Table 5: Effectiveness of multi-lingual fine-tuning for code summarization task. Note that p-values are B-H corrected
many functions therein are anonymous lambdas, since these func-
tions have no names, they are not useful, and this diminishes avail-
able the JavaScript training set relative to other tasks (lambdas
stillhavesummaries,andcanbeusedforothertasks).Therefore,
multilingual fine-tuning increases the dataset diversity and boosts
JavaScript method name prediction performance.
4.4 Two Illustrative Examples
We used the same dataset for all tasks; for illustration, we show
(Table 9) two test instances where all the tasks show improved
performance from multilingual fine-tuning. In code summarization
task, the monolingualfine-tuning scores 25 BLEU-4 inExample 1.
CodeBERTproducesasemanticallywrongcommentwheremul-
tilingual fine-tuning generates the semantically correct solution.
Note thatthe BLEU-4 is84 for thesecond example becauseof the
missing period in the gold standard (BLEU-4 is case-insensitive).
Multilingual fine-tuning also helps the code search problem by
increasing the MRR from 0.33 (Rank:3) to 1.00 (Rank:1). We also
observe performance improvement from the method name predic-
tion task. The gold standard consists of two sub tokens ( i.e., set
and Values), and mono-lingual fine-tuning generates three ( i.e., set,
Array, andValue),one ofthem isexact match. Onthe otherhand,
multilingualfine-tuningremovestheextraâ€œArrayâ€subtokenand
produces two subtokens( i.e., set and Value) resulting in the F-score
0.50.Weobserveasimilarresultinexample2.NotethatlikeBLEU-4,
our method name prediction metric is also case-insensitive.
Finding3 .Multilingualfine-tuningislikelytoincreasediversity
andhelpthemodelsperformbetterthanthosetrainedwithsmaller
mono-lingual datasets, especially for low-resource languages, irre-
spective of the task.
Models Overall Ruby JavaScript Go Python Java PHP
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT 19.10 14.95 15.79 18.92 18.90 19.91 26.15
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT 19.06 14.75 15.80 18.77 18.71 20.11 26.23
ProphetNet-X [55] 18.54 14.37 16.60 18.43 17.87 19.39 24.57
PLBART [1] 18.32 14.11 15.56 18.91 19.30 18.45 23.58
GraphCodeBERT [22] 18.08 12.62 14.79 18.40 18.02 19.22 25.45
CodeBERT [18] 17.83 12.16 14.90 18.07 19.06 17.65 25.16
RoBERTa [45] 16.57 11.17 11.90 17.72 18.14 16.47 24.02
Transformer [64] 15.56 11.18 11.59 16.38 15.81 16.26 22.12
Seq2Seq [61] 14.32 9.64 10.21 13.98 15.93 15.09 21.08
Table 6: Comparison to existing models, on CodeXGLUE dataset5 INTERPRETING RESULTS, AND THREATS
Inthissectionweconsiderseveralissuesthatarerelevanttotheob-
served performance of multilingual training, such as model choice,
datasetduplication,performancemetrics,generalization,anddif-
ferent training strategies for the models.
5.1 Does multilingual fine-tuning help with
other models?
Thereareseveralmodels,includingCoTexT[ 54],ProphetNet-X[ 55],
and PLBART [ 1] which report higher performance than Code-
BERT [18] model for the code summarization task. The models for
allthesetaskswerefine-tunedusingmonolingualdatasets,sowe
might expect that multilingual fine-tuning should improve perfor-
mance. These experiments would require a substantial investment
ofcomputeenergyandisleftforfuturework.WefocusedonCode-
BERT (and also GraphCodeBERT on some tasks). We did some
preliminaryexperimentswithmultilingualfine-tuningonPLBART.
Inourpreliminarystudy,didseethesamegainsforlow-resource
language (Ruby, 5% gain). However, we found a 0.55% overall loss,
whichisinconsistentwithwhatweobservewith Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡Code-
BERT(6.90%overallimprovement)& Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT
(5.64% overall improvement). More study is needed.
Finding 4 . Multilingual fine-tuning could benefit a broad range of
models.WefindgainsforCodeBERTandGraphCodeBERT,but
more data is required for other models.
5.2 Threats: Risk of data duplication?
Dataduplicationcanleadtopoor-qualityestimatesofperformance,
especiallywhendataisduplicatedacrosstraining&test;evendu-
plicationjustwithintestdatariskshighervarianceintheestimates.
Allamanis finds that performance metrics are highly inflated when
test data has duplicates, and advocates de-duplicating datasets, for
more robust results [ 5]. Shiet al.also discusses the impact of dupli-
cation in code summarization task [60].
Sadly, there is a large amount of copied code on GitHUB [ 46];
inattentively combining different datasets harvested from GitHUB
can lead to undesirable levels of duplication in the merged dataset.
Fortnuately,CodeXGLUEisatuallya carefullyde-duplicated dataset;
performance estimates therein are thus more robust. Combining
multilingual data is unlikely to introduce the same kind of exact
duplication in the dataset, because of syntax differences; There is a
1450Multilingual training for Software Engineering ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
LanguageCodeBERT
(publishe d) [22]CodeBERT
(re-trained)Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT ImprovementEffect
Sizep-value
(adjusted)GraphCodeBERT
(published) [22]GraphCodeBERT
(re-trained)Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT ImprovementEffect
Sizep-value
(adjusted)
Ruby 0.679 0.677 0.732 +8.12% 0.072 <0.001 0.703 0.708 0.738 +4.24% 0.039 <0.001
JavaScript 0.620 0.616 0.643 +4.38% 0.034 <0.001 0.644 0.644 0.660 +2.48% 0.019 0.004
Java 0.676 0.676 0.697 +3.11% 0.026 <0.001 0.691 0.693 0.710 +2.45% 0.022 <0.001
Go 0.882 0.885 0.885 0% -0.003 0.550 0.897 0.894 0.894 0% -0.002 0.724
PHP 0.628 0.629 0.635 +0.95% 0.009 0.003 0.649 0.648 0.646 -0.31% -0.002 0.904
Python 0.672 0.676 0.678 +0.30% 0.004 0.050 0.692 0.692 0.695 +0.43% 0.005 0.300
Overall* 0.693 0.693 0.712 +2.74%0.013 <0.0010.713 0.713 0.724 +1.54%0.007 <0.001Overall
(weighted)Not
Reported0.692 0.702 +1.42%Not
Reported0.709 0.715 +0.80%
*Evaluation criteria followed by GraphCodeBERT [22]
Table 7: Effectiveness of multi-lingual fine-tuning for code search task. Note that p-values are BH-corrected
LanguageCodeBERT Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT F-Score
ImprovementEffect
Sizep-value
(adjusted) Precision Recall F-Score Precision Recall F-Score
Ruby 0.44 0.40 0.41 0.53 0.49 0.49 20.59% 0.112 <0.001
JavaScript 0.30 0.24 0.26 0.45 0.40 0.41 59.00% 0.215 <0.001
Java 0.54 0.51 0.51 0.56 0.52 0.52 2.22% 0.016 <0.001
Go 0.54 0.52 0.52 0.56 0.53 0.52 1.67% 0.015 0.004
PHP 0.56 0.53 0.52 0.57 0.53 0.53 1.30% 0.009 0.004
Python 0.49 0.45 0.45 0.50 0.45 0.46 1.60% 0.011 0.002
Overall 0. 48 0.44 0.44 0.53 0.49 0.49 10.09%0.024 <0.001Overall
(weighted)0. 52 0.48 0.48 0.54 0.50 0.50 3.37%
Table 8: Effectiveness of multi-lingual fine-tuning for method nam-
ing task. Note that p-values are adjusted using Benjamini-Hochberg
possibilityofcross-languageclones[ 53];thestudyofthisisleftfor
future work.
Finding 5 . Combining multilingual datasets is unlikely to cause
exact duplication, because of syntax differences. More study is
needed to study the effect of cross-language clones.
5.3 Threats: Other metrics?
FollowingCodeXGLUEbenchmarkrecommendation,weevaluate
the code summarization task with smooth sentence BLEU-4 [ 44]
throughout this paper. However, other recognized metricsareare
available ( e.g., ROUGE-L [ 43], METEOR [ 10]). Prior works [ 21,58,
60] provide a careful analysis of the metrics, baselines, evaluations
for code summarization task. Table 10 shows ROUGE-L and ME-
TEOR data; we find that multilingual fine-tuning increases the
overallperformanceby4.89%and5.61%inROUGE-LandMETEOR,
respectively.AswithBLEU-4,wefindthatmultilingualfine-tuning
shows similar performance gains with these metrics. We find 0.3%-
14.1% improvement in ROUGE-L and 1.2%-22.5% gains in METEOR
(exceptforPHP,wereweseea0.17% decline,notstatisticallysignif-
icant).WealsoseethatPythonshowsthesmallestimprovement,
not as strongly statistically significant. These metrics also indi-
catestronggainsfrommultilingualtrainingforlow-resourceand
narrow-domain languages ( i.e., Ruby and JavaScript).
Finding 6 . We observe performance improvement in all code sum-
marization metrics with multilingual fine-tuning.
5.4 Monolingual minibatches? or multilingual?
Whiletrainingdeepneuralnetworkswithstochasticgradientde-
scent, gradients (multivariate derivatives of loss w.r.tlearnable
parameters)areestimatedover mini-batches ,ratherthancalculat-
inglossgradientsovertheentiretrainingset;theseestimatesareExample:1
//setthe values from an Array
public void setValues* ( Array arr ) {
//we omit intermediate lines to fit in the paper
//original code here
}
Code Summarization
Models & comments BLEU-4
Gold:set the values from an Array NA
CodeBERT: Sets the values of the array . 25
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT: Set the values from an array . 84
Code Search
Models MRR
GraphCodeBERT 0.33
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT 1.00
Method Name Prediction
Models & method name Sub tokens F-Score
Gold:setValues set Values NA
CodeBERT: setArrayValue set Array Value 0.40
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT: setValue set Value 0.50
Example:2
//Registers set injection point .
public void registerPetiteSetInjectionPoint* ( final String beanName, final String property ) {
//we omit intermediate lines to fit in the paper
//original code here
}
Code Summarization
Models & comments BLEU-4
Gold:Registers set injection point . NA
CodeBERT: Register a set of set InjectionPoint . 19
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT: Register a set injection point . 60
Code Search
Models MRR
GraphCodeBERT 0.50
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCodeBERT 1.00
Method Name Prediction
Models & method name Sub tokens F-Score
Gold:registerPetiteSetInjectionPoint register Pet ite Set In jection Point NA
CodeBERT: addPropertyInjectionPoint add Property In jection Point 0.50
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT: setPropertyInjectionPoint set Property In jection Point 0.57
*â€œregisterPetiteSetInjectionPointâ€ & â€œsetValuesâ€ tokens are abstracted for method name prediction task
Table 9: Examples exhibiting the effectiveness of multilingual train-
ing
usedtoadjusttheweightsinthenetwork.Betterchoicesofmini-
batches could improve convergence behavior. With multilingual
training, a natural question arises: is it better to sequentially inter-
persemonolingual mini-batches( e.g.,firstaJavaminibatch,then
Rubyminibatchandsoon,beforegoingbacktoJava?)orshould
we make each minibatch persemultilingual?
In the previous experiments, we had randomly sort the dataset;
hence,ourmini-batchesarealsomultilingual.Sowedeliberately
triedsequentiallymonolingualminibatchingduringmultilingual
1451ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Toufique Ahmed and Premkumar Devanbu
LanguageROUGE-L METEOR
Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT Improve.*Effect
Sizep-value
(adjusted)Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT Improve.*Effect
Sizep-value
(adjusted)
Ruby 24.36 +14.10% 0.087 <0.001 21.96 +22.54% 0.125 <0.001
JavaScript 24.30 +7.05% 0.022 <0.001 21.59 +11.40% 0.030 <0.001
Java 34.89 +3.32% 0.020 <0.001 31.73 +4.41% 0.020 <0.001
Go 37.36 +2.69% 0.024 <0.001 30.28 +3.73% 0.023 <0.001
PHP 38.81 +0.34% -8.65E-05 0.508 35.52 -0.17% -0.003 0.779
Python 32.86 +1.86% 0.015 <0.001 27.75 +1.24% 0.004 0.033
Overall 32.10 +4.89%0.016 <0.00128.14 +5.61%0.013 <0.001Overall
(weighted)34.82 +2.24% 30.52 +2.59%
*Improvement reported over CodeBERT
Table10: PerformanceimprovementinROUGE-LandMETEORfor
code summarization task
fine-tuning.Wefindthatsequentiallymonolingualminibatchtrain-
ing appears to somewhat degrade performance: we observe the
overallperformancegoesdownby1.05%.However,thechangeis
not statistically significant for any language. We omit the actual
numericaldetails,forspacereasons,sincewedidnâ€™tfindanystrong
results in either direction.
Finding7 .Wedonâ€™tfindanycleardifferencebetweenmultilingual
mini batches and (interspersed) monolingual mini batches.
5.5 Multilingual model as pre-trained model
Our findings provide evidence supporting the claim that a multi-
lingualfine-tunedmodeliseffectiveforcodesummarizationtask,
whichoutperformsallthemodelstrainedwithmonolingualdatasets.
Couldthisthisimprovedmultilingualmodelfurtherbenefitfrom
a secondary, monolingual fine-tuning exercise, where it receives
specializedfine-tuningforeachlanguageseparately?Toevaluate
this intriguing and promising idea, we load the model with the
weightsfrommultilingualfine-tuning,andfine-tuneit,again,for
each individual language. Table 11 shows that We found some mi-
norperformanceimprovementforJavaScriptandPython.However,
the performance goes down for other languages. We do not find
evidence that a secondary, monolingual fine-tuning is helpful; fur-
therwork isneededto understand why,and perhapsdevelopother
ways this idea might yield further improvement.
Language Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERTPð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT
as pre-trainingImprovementEffect
Sizep-value
(adjusted)
Ruby 14.75 14.58 -1.15% -0.016 0.303
JS 15.80 16.47 +4.24% 0.024 <0.001
Java 20.11 19.81 -1.49% -0.003 0.303
Go 18.77 17.97 -4.26% -0.012 <0.001
Php 26.23 25.52 -2.71% -0.017 <0.001
Python 18.71 18.83 +0.64% 0.010 <0.001
Overall 19.06 18.86 -1.05%-0.003 0.005Overall
(weighted)20.74 20.43 -1.47%
Table 11: Multilingual model as pre-trained model
Finding8 .Wedonâ€™tfindevidencethatapplyingasecondary,mono-
lingual fine-tuning provides benefits for all languages.
6 RELATED WORK
Code summarization: Code summarization has recently been a hot
topic.Morethan30papershavebeenpublishedinthelastfiveyearsthatfollowsomeformofencoder-decoderarchitecture[ 58].Several
works [21,58,60]discuss the evaluations,metrics, andbaselining.
Royet al.show that metric improvements of less than 2 points
do not guarantee systematic improvements in summarization and
are not reliable as proxies of human evaluation [ 58]. We find more
than 2 points of improvement for Ruby and almost 2 points of
improvement for JavaScript. We observe less than 2 points in other
languages. It should also be noted that we donâ€™tuse the corpus-
levelmetricswhichRoy etal.showisproblematic;weusepairwise
comparisons on the test-sets. Finally, we note that progress in both
code & NLP occurs in small steps over decades, and innovations
(especially ones that could cumulate with others) such as ours can
beanimportantpartofresearchcommunityâ€™slong-termpursuitof
practically relevant performance improvements.
Pre-trained models [ 1,18,45,54,55] are proven to be more
effectivethanpriormodels.Differentpre-trainedmodelsaretrained
withthedifferentpre-trainedobjectiveseventhoughfine-tuning
steps are almost similar for all the models. As discussed earlier
in Section 3.1, CodeBERT is an encoder model, pre-trained with
MLMandRepaceTokenDetectionobjectives.Unlike CodeBERT,
PLBART [ 1] is an encoder-decoder model which is trained as a
denoisingauto-encoder.Thoughallthemodelsarepre-trainedwith
different training objectives, there is one thing common among all
the models: using Transformers as core architecture.
Parvezetal.veryrecentlypresentanapproachthataugments
training data using relevant code or summaries retrieved from a
database ( e.g., GitHub, Stack Overflow) [ 52]. They apply this ap-
proachonmonolingualJavaandPythondatasetsfromCodeXGLUE
and claim gains over Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡CodeBERT & Pð‘œð‘™ð‘¦ð‘”ð‘™ð‘œð‘¡GraphCode-
BERTforcodesummarization. Primafacie ,multilingualfine-tuning
is complementary to their approach; this needs to be studied.
Code retrieval and method name prediction: Code retrieval is also
getting attention recently. There are multiple datasets for this task.
CodeXGLUEintroducesamonolingualpythondataset(takenini-
tially from CodeSearchNet) abstracting the function names and
variables.Guo etal.modifythemultilingualCodeSearchNetdataset
and achieve state-of-the-art performance on this task. However,
using multilingual training, we show that both CodeBERT and
GraphCodeBERT can be improved. There is one other very recent
paper,CLSEBERT[ 68]whichreports(inanunpublished,non-peer-
reviewedreport)betterperformancethanusinalllanguagesexcept
Ruby. We could not show the effectiveness of multilingual train-
ing on CLSEBERT because the authors have not released the code
implementation yet. Notethat like code summarization, wefocus
only on the work using CodeSearchNet multilingual dataset.
CodeSearchNetdatasetcanbeeasilyadaptedtomethodname
prediction task. Several earlier works address method name predic-
tion,inaJava-onlysuchasCode2Seq[ 7],Allamanis[ 6].Theyall
useaconventionalsingle-stagemachine-learningapproach(nopre-
training+fine-tuning).Ourgoalhereistosimplydemonstratethat
multilingualfine-tuningimprovesuponmonolingualfine-tuning
for the method-naming task, so we demonstrate using CodeBERT.
Our numbers are roughly comparable with previously reported
results,butwecannotmakeaprecisecomparisonbecauseofdiffer-
encesinsubtokenization,andbecauseourdatasetsaremultilingual
1452Multilingual training for Software Engineering ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
whereas previous work has largely been monolingual. We are sim-
ply arguing here our data suggests that multilingual fine-tuning is
broadly beneficial in different tasks.
It would certainly be interesting to use same-domain data for
fine-tuning.Forexample,summarizingmethodsinAndroidapps
might work better if trained on Android app corpora; however
curated, domain-specific datasets for each domain are needed, and
maynotalwaysbepossible,dependingonthedomain.However,
cross-language data is already available, and we show that it does
indeed help improve performance! The effect of domain-specific
corpora is left for future work.
7 CONCLUSION
Webeganthispaperwiththreesynergisticobservations: First,when
solvingthe sameproblem ,evenindifferentprogramminglanguages,
programmers are more likely to use similar identifiers (than when
solvingdifferentproblems). Second,identifiers appear to be rela-
tively much more important than syntax markers when training
machine-learningmodelstoperformcodesummarization. Third,we
findthatquiteoftenamodeltrainedinoneprogramminglanguage
achieves surprisingly good performance on a test set in a different
language, sometimes even surpassing a model trained on the same
languageasthetestset!Takentogether,thesefindingssuggestthat
pooling dataacross languages,thus creating multilingual training
sets, could improve performance for any language, particularly
perhaps languages with limited resources, as has been found in
Natural-language processing [ 16,23,57,63]. We test this theory,
usingtwoBERT-stylemodels,CodeBERT,andGraphCodeBERT,
with encouraging results.
Foundation models [ 12] are currently achieving best-in-class
performance for a wide range of tasks in both natural language
and code. The models work in 2 stages, first â€œpre-trainingâ€ to learn
statistics oflanguage (orcode) constructionfrom verylarge-scale
corporainaself-supervisedfashion,andthenusingsmallerlabeled
datasets to â€œfine-tuneâ€ for specific tasks. We adopt the multilingual
CodeXGLUEdataset,andthepre-trainedCodeBERTandGraph-
CodeBERTmodels,andstudythevalueofmultilingualfine-tuning
foravarietyoftasks.Wefindevidencesuggestingthatmultilingual
fine-tuning is broadly beneficial in many settings. Our findings
suggest that multilingual training could provide added value in
broad set of settings, and merits further study.
Acknowledgements: This material is based upon work supported by
the U.S. National Science Foundation under Grant Nos. 1414172,
and 2107592. Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the National Science
Foundation. Ahmed was also supported by UC Davis College of
Engineering Deanâ€™s Distinguished Fellowship.
REFERENCES
[1]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uni-
fiedPre-trainingforProgramUnderstandingandGeneration.In Proceedingsof
the2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-
tional Linguistics: Human Language Technologies . Association for Computational
Linguistics, Online, 2655â€“2668. https://www.aclweb.org/anthology/2021.naacl-
main.211
[2]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL) .[3]Toufique Ahmed, Premkumar Devanbu, and Anand Ashok Sawant. 2021. Learn-
ing to Find Usage of Library Functions in Optimized Binaries. IEEE Transactions
on Software Engineering (2021). https://doi.org/10.1109/TSE.2021.3106572
[4]Toufique Ahmed, Noah Rose Ledesma, and Premkumar Devanbu. 2021.
SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics.
arXiv:2104.14671 [cs.SE]
[5]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learningmodelsofcode.In Proceedingsofthe2019ACMSIGPLANInternational
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software. 143â€“153.
[6]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-
tention network for extreme summarization of source code. In International
conference on machine learning . PMLR, 2091â€“2100.
[7]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations . https://openreview.net/forum?id=H1gKYo09tX
[8]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“29.
[9]Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin John-
son, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al .
2019. Massivelymultilingualneuralmachinetranslationinthewild:Findings
and challenges. arXiv preprint arXiv:1907.05019 (2019).
[10]Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation withimproved correlation with humanjudgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization . 65â€“72.
[11]EeshitaBiswas,MehmetEfruzKarabulut,LoriPollock,andKVijay-Shanker.2020.
Achieving Reliable Sentiment Analysis in the Software Engineering Domain
using BERT. In 2020 IEEE International Conference on Software Maintenance and
Evolution (ICSME) . IEEE, 162â€“173.
[12]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al .2021. On the Opportunities and Risks of Foundation Models.
arXiv preprint arXiv:2108.07258 (2021).
[13]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 (2020).
[14]Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020.
Electra:Pre-trainingtextencodersasdiscriminatorsratherthangenerators. arXiv
preprint arXiv:2003.10555 (2020).
[15]Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model
pretraining. Advances in Neural Information Processing Systems 32 (2019), 7059â€“
7069.
[16]Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020. A survey of multilin-
gualneuralmachinetranslation. ACMComputingSurveys(CSUR) 53,5(2020),
1â€“38.
[17]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. arXiv
preprint arXiv:1810.04805 (2018).
[18]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages.In Proceedings of the
2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Findings .
1536â€“1547.
[19]ShuzhengGao,CuiyunGao,YulanHe,JichuanZeng,LunYiuNie,andXinXia.
2021. CodeStructureGuidedTransformerforSourceCodeSummarization. arXiv
preprint arXiv:2104.09340 (2021).
[20]EkaterinaGarmashandChristofMonz.2016. Ensemblelearningformulti-source
neuralmachinetranslation.In ProceedingsofCOLING2016,the26thInternational
Conference on Computational Linguistics: Technical Papers . 1409â€“1418.
[21]David Gros, Hariharan Sezhiyan, Prem Devanbu, and Zhou Yu. 2020. Code to
Comment ?Translation?: Data, Metrics, Baselining & Evaluation. In 2020 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
IEEE, 746â€“757.
[22]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long
Zhou,NanDuan,AlexeySvyatkovskiy,ShengyuFu,etal .2020. GraphCodeBERT:
Pre-trainingCodeRepresentationswithDataFlow.In InternationalConference
on Learning Representations .
[23]Thanh-Le Ha, Jan Niehues, and Alexander Waibel. 2016. Toward multilingual
neuralmachinetranslationwithuniversalencoderanddecoder. arXivpreprint
arXiv:1611.04798 (2016).
[24]Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan. 2020. Im-
proved automatic summarization of subroutines via attention to file context. In
Proceedings of the 17th International Conference on MiningSoftwareRepositories .
300â€“310.
1453ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Toufique Ahmed and Premkumar Devanbu
[25]MasumHasan,TanveerMuttaqueen,AbdullahAlIshtiaq,KaziSajeedMehrab,
MdHaque,MahimAnjum,TahmidHasan,WasiUddinAhmad,AnindyaIqbal,
andRifatShahriyar.2021. CoDesc:ALargeCode-DescriptionParallelDataset.
arXiv preprint arXiv:2105.14220 (2021).
[26]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment gener-
ation.In2018IEEE/ACM26thInternationalConferenceonProgramComprehension
(ICPC). IEEE, 200â€“20010.
[27]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generationwithhybridlexicalandsyntacticalinformation. EmpiricalSoftware
Engineering 25, 3 (2020), 2179â€“2217.
[28]Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing
source code with transferred api knowledge. (2018).
[29]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).
[30]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizingsourcecodeusinganeuralattentionmodel.In Proceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers) . 2073â€“2083.
[31]Paul Jaccard. 1901. Ã‰tude comparative de la distribution florale dans une portion
des Alpes et des Jura. Bull Soc Vaudoise Sci Nat 37 (1901), 547â€“579.
[32]Kevin Jesse, Premkumar T Devanbu, and Toufique Ahmed. 2021. Learning type
annotation: is big data enough?. In Proceedings of the 29th ACM Joint Meeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering . 1483â€“1486.
[33]Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural
Machine Translation for Automatic Program Repair. In 2021 IEEE/ACM 43rd
International Conference on Software Engineering (ICSE) . IEEE, 1161â€“1173.
[34]MelvinJohnson,MikeSchuster,QuocVLe,MaximKrikun,YonghuiWu,Zhifeng
Chen,NikhilThorat,FernandaViÃ©gas,MartinWattenberg,GregCorrado,etal .
2017. Googleâ€™smultilingualneuralmachinetranslationsystem:Enablingzero-
shottranslation. TransactionsoftheAssociationforComputationalLinguistics 5
(2017), 339â€“351.
[35]Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learningand evaluatingcontextualembedding ofsourcecode. In International
Conference on Machine Learning . PMLR, 5110â€“5121.
[36]Surafel M Lakew, Mauro Cettolo, and Marcello Federico. 2018. A comparison
oftransformerandrecurrentneuralnetworksonmultilingualneuralmachine
translation. arXiv preprint arXiv:1806.06957 (2018).
[37]Dawn Lawrie, Christopher Morrell, Henry Feild, and David Binkley. 2007. Ef-
fective identifier names for comprehension and memory. Innovations in Systems
and Software Engineering 3, 4 (2007), 303â€“318.
[38]AlexanderLeClair,AakashBansal,andCollinMcMillan.2021. EnsembleMod-
els for Neural Source Code Summarization of Subroutines. arXiv preprint
arXiv:2107.11423 (2021).
[39]Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Im-
proved code summarization via a graph neural network. In Proceedings of the
28th International Conference on Program Comprehension . 184â€“195.
[40]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model
for generating natural language summaries of program subroutines. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE,
795â€“806.
[41]Boao Li, Meng Yan, Xin Xia, Xing Hu, Ge Li, and David Lo. 2020. DeepCom-
menter:adeepcodecommentgenerationtoolwithhybridlexicalandsyntactical
information. In Proceedings of the 28th ACM Joint Meeting on European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering .
1571â€“1575.
[42]Zheng Li, Yonghao Wu, Bin Peng, Xiang Chen, Zeyu Sun, Yong Liu, and Deli Yu.
2021. SeCNN: A semantic CNN parser for code comment generation. Journal of
Systems and Software 181 (2021), 111036.
[43]Chin-YewLin.2004. Rouge:Apackageforautomaticevaluationofsummaries.
InText summarization branches out . 74â€“81.
[44]Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating auto-
maticevaluationmetricsformachinetranslation.In COLING2004:Proceedings
of the 20th International Conference on Computational Linguistics . 501â€“507.
[45]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bertpretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[46]Cristina VLopes, Petr Maj,Pedro Martins, Vaibhav Saini,Di Yang, Jakub Zitny,
HiteshSajnani,andJanVitek.2017. DÃ©jÃ Vu:amapofcodeduplicatesonGitHub.
Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1â€“28.
[47]ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
Blanco,ColinB.Clement,DawnDrain,DaxinJiang,DuyuTang,GeLi,Lidong
Zhou,LinjunShou,LongZhou,MicheleTufano,MingGong,MingZhou,Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. CoRRabs/2102.04664 (2021).[48]Guillermo Macbeth, Eugenia Razumiejczyk, and RubÃ©n Daniel Ledesma. 2011.
Cliffâ€™s Delta Calculator: A non-parametric effect size program for two groups of
observations. Universitas Psychologica 10, 2 (2011), 545â€“555.
[49]Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios Anastasopoulos,
andKevinMoran.2021. CodetoCommentTranslation:AComparativeStudyon
Model Effectiveness & Errors. arXiv preprint arXiv:2106.08415 (2021).
[50]Antonio Mastropaolo, Simone Scalabrino,Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE,
336â€“347.
[51]Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation
forresource-poorlanguagesusingrelatedresource-richlanguages.In Proceedings
ofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing .1358â€“
1367.
[52]Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-WeiChang.2021. RetrievalAugmentedCodeGenerationandSummarization.
arXiv preprint arXiv:2108.11601 (2021).
[53]DanielPerezandShigeruChiba.2019.Cross-languageclonedetectionbylearning
over abstract syntax trees. In 2019 IEEE/ACM 16th International Conference on
Mining Software Repositories (MSR) . IEEE, 518â€“528.
[54]LongPhan,HieuTran,DanielLe,HieuNguyen,JamesAnibal,AlecPeltekian,
and YanfangYe.2021. CoTexT:Multi-task Learning with Code-Text Transformer.
arXiv preprint arXiv:2105.08645 (2021).
[55]WeizhenQi,YeyunGong,YuYan,CanXu,BolunYao,BartuerZhou,BiaoCheng,
Daxin Jiang, Jiusheng Chen, Ruofei Zhang, et al .2021. ProphetNet-X: Large-
Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code
Generation. arXiv preprint arXiv:2104.08006 (2021).
[56]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-
its of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683 (2019).
[57]Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi
Shekhar, Mehreen Alam, and Rishemjit Kaur. 2021. Neural Machine Translation
for Low-Resource Languages: A Survey. arXiv preprint arXiv:2106.15115 (2021).
[58]Devjeet Roy, Sarah Fakhoury, and Venera Arnaoudova. 2021. Reassessing auto-
maticevaluationmetricsforcodesummarizationtasks.In Proceedingsofthe29th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering . 1105â€“1116.
[59]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine
translation of rare words with subword units. arXiv preprint arXiv:1508.07909
(2015).
[60]Ensheng Shi,Yanlin Wang,Lun Du, JunjieChen, Shi Han,Hongyu Zhang, Dong-
mei Zhang, and Hongbin Sun. 2021. Neural Code Summarization: How Far Are
We?arXiv preprint arXiv:2107.07112 (2021).
[61]IlyaSutskever,OriolVinyals,andQuocVLe.2014. Sequencetosequencelearning
withneuralnetworks.In Advancesinneuralinformationprocessingsystems .3104â€“
3112.
[62]Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2019. Multilin-
gual neural machine translation with knowledge distillation. arXiv preprint
arXiv:1902.10461 (2019).
[63]Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaud-
hary, JiataoGu, and AngelaFan. 2020. Multilingual translationwith extensible
multilingual pretraining and finetuning. arXiv preprint arXiv:2008.00401 (2020).
[64]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
y o un eed .I n Advances in neural information processing systems . 5998â€“6008.
[65]MKVijaymeenaandKKavitha.2016. Asurveyonsimilaritymeasuresintext
mining.Machine Learning and Applications: An International Journal 3, 2 (2016),
19â€“28.
[66]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
PhilipSYu.2018. Improvingautomaticsourcecodesummarizationviadeeprein-
forcementlearning.In Proceedingsofthe33rdACM/IEEEInternationalConference
on Automated Software Engineering . 397â€“407.
[67]WenhuaWang,YuqunZhang,YuleiSui,YaoWan,ZhouZhao,JianWu,Philip
Yu,andGuandongXu.2020. Reinforcement-learning-guidedsourcecodesum-
marization via hierarchical attention. IEEE Transactions on software Engineering
(2020).
[68]Xin Wang, Yasheng Wang, Pingyi Zhou, Meng Xiao, Yadao Wang, Li Li, Xiao
Liu, Hao Wu, Jin Liu, and Xin Jiang. 2021. CLSEBERT: Contrastive Learning
forSyntaxEnhancedCodePre-TrainedModel. arXivpreprintarXiv:2108.04556
(2021).
[69]Yanlin Wang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, Hongyu
Zhang,and DongmeiZhang.2021. CoCoSum:ContextualCodeSummarization
with Multi-Relational Graph Neural Network. arXiv preprint arXiv:2107.01933
(2021).
[70]Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code generation as a dual
task of code summarization. arXiv preprint arXiv:1910.05923 (2019).
1454Multilingual training for Software Engineering ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[71]Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and refine:
exemplar-basedneuralcommentgeneration.In 202035thIEEE/ACMInternational
Conference on Automated Software Engineering (ASE) . IEEE, 349â€“360.
[72]GuangYang, XiangChen,Jinxin Cao,ShuyuanXu, ZhanqiCui,ChiYu,andKe
Liu. 2021. ComFormer: Code Comment Generation via Transformer and Fusion
Method-based Hybrid Code Representation. arXiv preprint arXiv:2107.03644
(2021).
[73]Huang Yuchao, Wei Moshi, Wang Song, Wang Junjie, and Wang Qing. 2021.
Yet Another Combination of IR-and Neural-based Comment Generation. arXiv
preprint arXiv:2107.12938 (2021).[74]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-based neural source code summarization. In 2020 IEEE/ACM 42nd Inter-
national Conference on Software Engineering (ICSE) . IEEE, 1385â€“1397.
[75]Ting Zhang, Bowen Xu, Ferdian Thung, Stefanus Agus Haryono, David Lo, and
Lingxiao Jiang. 2020. Sentiment Analysis for Software Engineering: How Far
Can Pre-trained Transformer Models Go?. In 2020 IEEE International Conference
on Software Maintenance and Evolution (ICSME) . IEEE, 70â€“80.
[76]Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. arXiv
preprint arXiv:1601.00710 (2016).
1455