Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source
Anjiang Weiâˆ—
Stanford University
anjiang@stanford.eduYinlin Deng
University of Illinois at Urbana-Champaign
yinlind2@illinois.edu
Chenyuan Yangâˆ—
Nanjing University
cy1yang@outlook.comLingming Zhang
University of Illinois at Urbana-Champaign
lingming@illinois.edu
ABSTRACT
Deep learning (DL) systems can make our life much easier, and
thus are gaining more and more attention from both academia and
industry. Meanwhile, bugs in DL systems can be disastrous, and
can even threaten human lives in safety-critical applications. Todate,ahugebodyofresearcheffortshavebeendedicatedtotest-
ingDLmodels.However,interestingly,thereisstilllimitedwork
fortestingtheunderlyingDLlibraries,whicharethefoundation
for building, optimizing, and running DL models. One potentialreasonisthattestgenerationfortheunderlyingDLlibrariescanberatherchallengingsincetheirpublicAPIsaremainlyexposedin Python, making it even hard to automatically determine the
API input parameter types due to dynamic typing. In this paper,
we propose FreeFuzz, the first approach to fuzzing DL libraries
viaminingfromopensource.Morespecifically,FreeFuzzobtains
code/modelsfromthreedifferentsources:1)codesnippetsfromthelibrarydocumentation,2)librarydevelopertests,and3)DLmodels
in the wild. Then, FreeFuzz automatically runs all the collected
code/modelswithinstrumentationtotracethedynamicinforma-
tionforeachcoveredAPI,includingthetypesandvaluesofeach
parameterduringinvocation,andshapesofinput/outputtensors.
Lastly, FreeFuzz will leverage the traced dynamic information to
perform fuzztesting foreach coveredAPI. Theextensive studyof
FreeFuzz on PyTorch and TensorFlow, two of the most popular DL
libraries,showsthatFreeFuzzisabletoautomaticallytracevalid
dynamic information for fuzzing 1158 popular APIs, 9X more than
state-of-the-artLEMONwith3.5XloweroverheadthanLEMON.
Todate,FreeFuzzhasdetected49bugsforPyTorchandTensorFlow
(with 38 already confirmed by developers as previously unknown).
ACM Reference Format:
Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang. 2022.
FreeLunchforTesting:FuzzingDeep-LearningLibrariesfromOpenSource.
In44th International Conference on Software Engineering (ICSE â€™22), May
âˆ—The work was done during a remote summer internship at University of Illinois.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.351004121â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3510003.3510041
1 INTRODUCTION
DeepLearning(DL)hasbeenplayingasignificantroleinvarious
applicationdomains,includingimageclassification[ 39,62],natural
languageprocessing[ 33,35],gameplaying[ 61],andsoftwareengi-
neering[23,45,74,75].Throughsuchapplications,DLhassubstan-
tially improved our daily life [ 20,36,60,64,71]. The great success
achieved by DL is attributed to the proposal of more and more
advanced DL models, the availability of large-scale datasets, and
inevitably,thecontinuousdevelopmentofDLlibraries.Meanwhile,
deploying a DL model without thorough testing can be disastrous
in safety-critical applications. For example, a critical bug in the DL
systeminUberâ€™sself-drivingcarshasunfortunatelytakenthelifeof a pedestrian [12].
Due to the popularity of DL models and the critical impor-
tance of their reliability, a growing body of research efforts havebeen dedicated to testing DL models, with focus on adversarial
attacks [15,22,34,50â€“52] for model robustness, the discussion on
variousmetricsforDLmodeltesting[ 38,41,47,56,73],andtesting
DLmodelsforspecificapplications[ 67,77,84].Meanwhile,both
runningand testingDLmodelsinevitably involvethe underlying
DL libraries, which serve as central pieces of infrastructures for
building, training, optimizing and deploying DL models. For exam-
ple, the popular PyTorch and TensorFlow DL libraries, with 50K
and159KstarsonGitHub,arebyfartwoofthemostpopularDL
libraries for developing and deploying DL models. Surprisingly,
despitetheimportanceofDLlibraryreliability,thereisonlylimited
work for testing DL libraries to date. For example, CRADLE [ 57]
leverages existing DL models for testing Keras [ 1] and its back-
ends,andresolvesthetestoracleproblemviadifferentialtesting.
Later, LEMON [ 69] further augments CRADLE via leveraging vari-
ousmodelmutationrulestogeneratemorediverseDLmodelsto
invokemorelibrarycodetoexposemorepossibleDLlibrarybugs.
Despitetheirpromisingresults,existingworkontestingDLli-
braries suffers from the following limitations. Firstly, only limited
sourcesfortestinputgenerationareconsidered.Forexample,CRA-DLE[
57]uses30pre-trainedDLmodelsandLEMON[ 69]usesonly
12DLmodels.Ourlaterempiricalresultsshowthattheycanatmost
cover59APIsforTensorFlow,leavingadisproportionatelylarge
number of APIs uncovered by such existing techniques. Secondly,
thestate-of-the-artmodelmutationtechniqueproposedbyLEMONcanberatherlimitedforgeneratingdiversetestinputsforDLAPIs.
9952022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang
Forexample,theintact-layermutation[ 69]requiresthattheoutput
tensor shape of the layer/API to be added/deleted should be identi-
caltoitsinputtensorshape.Asaconsequence,onlyafixedpattern
of argument values for a limited set of APIs are explored in model-
levelmutation,whichsubstantiallyhindersitsbug-findingabilities.
Thirdly, model-level testing can be rather inefficient. The inputs
for the original/mutated models are obtained from the external DL
datasets, and each of them will need to be completely executed
end-to-endtogetthefinalpredictionresultsfordifferentialtesting,
whichcanconsumehoursevenforasinglemutatedmodel.Besides,
it requires an additional bug localization procedure to find the spe-
cific APIinvocation causing the inconsistenciesbetween different
backendsintheoriginal/mutatedDLmodels.Duringlocalization,
carefully-designed metrics are required to eliminate false positives.
The false positives can be due to uncertainty and variances (e.g.
floating-pointprecisionloss)inDLAPIs,furtheramplifiedinthe
model-level testing scenario.
In this work, we overcome the aforementioned limitations for
testingDLlibrariesviafullyautomatedAPI-levelfuzztesting.Com-
paredwithpriormodel-levelDLlibrarytestingwhichresembles
systemtesting,API-leveltestingismorelike unittesting,whichisat
amuchfiner-grainedlevel.ThebenefitofAPI-leveltestingisthatit
can be a more general and systematic way for testing DL libraries.
With API instrumentation, we can get various and diverse input
sourcesfromopensourcetoservethepurposeoftesting.Moreover,
API-level mutation is free of unnecessarily strict constraints on
mutation compared with model-level mutation. Besides, API-level
mutationneitherdependsoniteratingoverexternaldatasets,nor
requirescomplexlocalization procedures sincetestingoneAPI ata
time does not incur accumulated floating-point precision loss.
One main challenge that we resolve for API-level fuzz testing of
DLlibrariesisfullyautomatedtestinputgenerationforDLAPIs.
ThepublicAPIsinDLlibrariesaremainlyexposedinPython,mak-
ingitdifficulttoautomaticallydeterminetheinputparametertypes
due to dynamic typing. To this end, we propose FreeFuzz, the first
approach to fuzzing DL libraries via mining from actual model
andAPIexecutions.Morespecifically,weconsiderthefollowing
sources:1)codesnippetsfromthelibrarydocumentation,2)library
developer tests, and 3) DL models in the wild. FreeFuzz records the
dynamic information for all the input parameters for each invoked
API on the fly while running all the collected code/models. The
dynamicinformation includesthe types,values ofthe arguments,
andtheshapesoftensors.Thetracedinformationcanthenforma value space for each API, and an argument value space where
values can be shared across arguments of similar APIs during test-
ing.Lastly,FreeFuzzleveragesthetracedinformationtoperform
mutation-basedfuzzingbasedonvariousstrategies(i.e.,typemuta-
tion, random value mutation, and database value mutation), and
detectsbugswithdifferentialtestingandmetamorphictestingon
differentbackends.OurinitialevaluationofFreeFuzzonPyTorch
and TensorFlow shows that FreeFuzz can automatically trace valid
dynamic information for fuzzing 1158 out of all 2530 considered
APIs,whilestate-of-the-arttechniquescanatmostcover59APIs
for TensorFlow [ 57,69]. To date, we have submitted 49 bug reports
(detected by FreeFuzz) to developers, with 38 already confirmed
by developers as previously unknown bugs and 21 already fixed to
date.In summary, our paper makes the following contributions:
â€¢Dimension. This paper opens a new dimension for fully
automatedAPI-levelfuzzingofDLlibrariesviaminingfrom
actual code and model executions in the wild.
â€¢Technique. We implement a practical API-level DL library
fuzzing technique, FreeFuzz, which leverages three different
input sources, including code snippets from library docu-mentation, library developer tests, and DL models in the
wild.FreeFuzztracesthedynamicAPIinvocationinforma-
tion of all input sources via code instrumentation for fuzz
testing. FreeFuzz also resolves the test oracle problem with
differential testing and metamorphictesting.
â€¢Study.Our extensive study on the two most popular DL
libraries, PyTorch and TensorFlow, shows that FreeFuzz can
successfully trace1158 out of 2530APIs, and effectivelyde-
tect 49 bugs, with 38 already confirmed by developers as
previously unknown, and 21 already fixed.
2 BACKGROUND
2.1 Preliminaries for Deep Learning Libraries
Figure 1: Example DL library (PyTorch)
Inthissection,wewillgiveabriefintroductiontotheprelimi-
naries of deep learning libraries based on PyTorch [55].
Training and Inference. As shown on the left-hand side of Fig-
ure 1, developers usually leverage DL libraries to support the train-
ingandinferencetasksonDeepNeuralNetworks(DNNs).Concep-
tually, DNNs are composed of multiplelayers, which are what the
adjective â€œdeepâ€ in deeplearning refersto. In themodel definition
part of Figure 1, Conv2dandMaxpool2d are the APIs invoked to add
twolayersintotheexampleDNN.Thenthe forwardfunctiondefines
how the input data should flow in the defined layers. Before the ac-
tual training and inference, the datasets should also be loaded with
necessary pre-processing, e.g., torchvision.transforms.Normalize
isacrucialstepindatapre-processing,whichaimstorescalethe
values of input and target variables for better performance.
TrainingistheprocessforaDNNtolearnhowtoperformatask
(withitsweightsupdatedalongtheway).Forexample,forimage
classification, by feeding a DNN with known data and correspond-
ing labels, we can obtain a trained DL model. Training a DL model
involves iterating over the dataset, defining a loss function (e.g.,
torch.nn.CrossEntropyLoss )tocalculatethedifferencebetweenthe
network outputs and its expected outputs (according to the labels),
996
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure2:TheAPIdefinitionfor2D-ConvolutioninPyTorch
and updating the weights of the DNN via a back-propagation pro-
cedure(i.e., loss.backward ).Differentfromthetrainingphase,in-
ference is the process of using a trained DL model to complete a
certain task (with its weights unchanged), e.g., making predictions
against previously unseen data based on the trained model.
Abstraction for Hardware. Shown on the right-hand side of
Figure 1, DL libraries (such as PyTorch and TensorFlow) usually
provide a unified abstraction for different hardware, which can be
easily configured by the end users to perform the actual execution.
TheyusuallyintegratedifferentbackendsinDLlibrariesforflex-
ibility and performance. Take PyTorch as an example, Aten [ 13]
is a backend implemented in C++ serving as a tensor library for
hundreds of operations. It has specialized low-level implementa-
tionsforhardwareincludingbothCPUsandGPUs.BesidesAten,
CuDNN [ 26] is another backend integrated into PyTorch, which
isawidely-usedthird-partyhigh-performancelibrary,developed
specificallyfordeeplearningtasksonNvidiaGPUs.Furthermore,asshowninFigure1,PyTorchnownotonlysupportsgeneral-purpose
devices such as CPUs and GPUs, but also allows users to run DL
modelsonmobiledevicesduetothegrowingneedtoexecuteDL
models on edge devices.
2.2 Fuzzing Deep Learning libraries
As shown in the previous subsection, hundreds or even thousands
of APIs are implemented in a typical DL library to support various
tasks. Therefore, it is almost impossible to manually construct test
inputsforeachAPI.Meanwhile,mostpublicAPIsfromDLlibraries
areexposedinPythonduetoitspopularityandsimplicity,which
makesitextremelychallengingtoautomaticallygeneratetestinputs
given the API definitions. The reason is that we cannot determine
the types of the input parameters statically because Python is a dy-
namicallytypedlanguage. Taketheoperator2D-ConvolutionfromPyTorchasanexample,thedefinitionofwhichisshowninFigure2,a snapshot captured from Pytorch official documentation [
5]. From
the definition of 2D-Convolution shown Figure 2, we do not know
whattypesofparameters in_channels ,out_channels ,kernel_size
are.Also,onemayconcludethatparameter stridemustbeaninteger
(inferredfromthedefaultvalue stride=1)andparameter padding
must also be an integer (inferred from the default value padding=0 ).
However, this is not the case actually. The documentation below
(not included in Figure 2 due to space limitations) says that â€œstride
controls the stride for the cross-correlation, a single number or a
tupleâ€ and â€œpadding controls the amount of padding applied to the
input. It can be either a string â€˜validâ€™, â€˜sameâ€™ or a tuple of ints giving
theamountofimplicitpaddingappliedonbothsidesâ€.Infact,the
parameters kernel_size ,stride,padding,dilation canbeeitherasingleintor atuple of two ints, and paddingcan even be a string
besides the two types mentioned above. Therefore, there can exist
multiple valid types for a specific argument, and the valid types of
arguments cannot be easily inferred from the definition.
Due to the above challenge of test generation for DL APIs,
CRADLE [ 57] proposes to directly leverage existing DL models
totestDLlibraries.TheinsightofCRADLEistocheckthecross-
implementation inconsistencies when executing the same DL mod-
els on different backends to detect DL library bugs. It uses 30 mod-
els and 11 datasets. After detecting inconsistencies when execut-
ing models between differentbackends by feeding the input from
datasets, a confirmation procedure to identify bug-revealing in-
consistencies and a localization procedure to precisely localize the
sourceoftheinconsistencieshavetobelaunched.Insuchmodel-
leveltesting,whereinconsistenciescanbeeitherduetorealbugsor
accumulatedfloatingpointprecisionlosspropagatedthroughthe
executionofmultipleAPIs,carefullydesignedmetricsareneeded
to distinguish real bugs from false positives. Furthermore, such
model-leveltestingtechniqueonlycoversalimitedrangeofAPIs
inDLlibraries,e.g.,allmodelsusedbyCRADLEonlycover59APIs
for TensorFlow.
BasedonCRADLE,LEMON[ 69]advancestestingDLlibraries
by proposing model-level mutation. A set of model-level mutation
rules are designed to generate mutated models, with the goal of
invoking more library code. Model-level mutation is composed of
intact-layer mutation and inner-layer mutation. The intact-layermutation rules pose very strict constraints on the set of APIs tobe mutated and the arguments passed to them. As stated in the
LEMONpaper[ 69],oneexplicitconstraintforintact-layermutation
is that the output shape of the API to be inserted and the input
shape of it must be identical. As a result, only a limited set of APIs
with fixed parameters can used for mutation in order to meet such
constraints,whichsubstantiallyhindersLEMONâ€™sabilityinbug-
finding. Moreover, selecting such APIs with specific arguments for
layer-levelmutationrequiresexpertknowledgeoftheinput-output
relation of each API. For example, only a limited range of APIs
(e.g., convolution, linear and pooling) with fixed parameters can be
added or deleted during model-level mutation. According to our
later study, LEMONâ€™s various mutation rules can only help cover 5
more APIs in total for all the studied models.
3 APPROACH
Figure3showstheoverviewofourapproach,FreeFuzz,whichis
mainly composed of four different stages. The first stage is codecollection(Section3.1).Asshowninthefigure,FreeFuzzobtainscode from three different sources: 1) code snippets from library
documentation,2)librarydevelopertests,and3)variousDLmodelsinthewild,allofwhichcanbeobtainedfromopensourceautomat-
ically.Thesecondstageisdynamictracingwithinstrumentation
(Section 3.2). FreeFuzz first hooks the invocation of APIs, and then
executes the code collected in the first stage to trace various dy-namic execution information for each API invocation, including
value and type information for all parameters of all executed APIs.
As a result of this stage, FreeFuzz constructs the type space, API
valuespace,andargumentvaluespaceforthelaterfuzzingstage.
Thethirdstageismutation-basedfuzzing(Section3.3).Basically,
997
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang
Figure 3: FreeFuzz overview
FreeFuzzeffectivelygeneratesmutantsfor thetestinputs(i.e.,the
argument lists) used to invoke the targeted APIs, based on the
traced information collected in the second stage. The mutation
strategies are composed of type mutation, random value mutation,
and database value mutation. The last stage is running all the gen-
erated tests with oracles (Section 3.4). FreeFuzz resolves the test
oracle problem by differential testing and metamorphic testing on
different DL library backends and hardware. FreeFuzz is able to
detectvarioustypesofbugs,includingwrong-computationbugs,
crash bugs, and performance bugs for DL libraries.
3.1 Code Collection
FreeFuzz is a general approach and can work with dynamic API
informationtracedfromvarioustypesofcodeexecutions.Inthispa-
per, we mainly explore code collection from the following sources:
Code Snippets from LibraryDocumentation. Inordertohelp
usersbetterunderstandtheusageofAPIs,almostallDLlibraries
willprovidedetaileddocumentationsonhowtoinvoketheAPIs.
Usually, detailed specifications written in natural languages are
presentedtoshowtheusageofeachparameterofeachAPIindetail.
Meanwhile,tobetterhelpthedevelopers,suchnatural-language-
basedspecificationsarealsooftenaccompaniedbycodesnippets
forbetterillustrations.Toillustrate,anexamplecodesnippetforinvoking the 2D-Convolution API within PyTorch is shown in
Figure4.Ofcourse,itisworthnotingthatnotallAPIshaveexample
code and example code cannot enumerate all possible parameter
values. Therefore, it is also important to consider other sources.
Figure4:ExampleCodefor2D-ConvolutionfromPyTorchâ€™sDocumentation
Library DeveloperTests.
Softwaretestinghasbecomethemost
widelyadoptedwayforqualityassuranceofsoftwaresystemsin
practice. As a result, DL library developers also write/generate a
large number of tests to ensure the reliability and correctness ofDL libraries. For example, the popular TensorFlow and PyTorch
DLlibrarieshave1493and1420testsfortestingthePythonAPIs,respectively. WesimplyrunallsuchPythontestsastheydominate
DL library testing and this work targets Python API fuzzing.DL Models in the Wild.
Popular DL libraries have been widely
usedfortraininganddeployingDLmodelsinthewild.Therefore,
we can easily collect a large number of models for a number of
diverse tasks, eachof which will covervarious APIs during model
trainingandinference.Morespecifically,frompopularrepositories
inGithub,weobtain102modelsforPyTorch,and100modelsfor
TensorFlow. These models are diverse in that they cover various
taskssuchasimageclassification,naturallanguageprocessing,rein-forcementlearning,autonomousdriving,etc.Thedetailedinforma-tion about the leveraged models can be found in our repository [
8].
3.2 Instrumentation
In this phase, FreeFuzz performs code instrumentation to collect
variousdynamicexecutionfortest-inputgeneration.Wefirstget
thelistofPythonAPIstobeinstrumentedfromtheofficialdocu-
mentations of the studied DL libraries in this work, i.e., PyTorch
andTensorFlow.Morespecifically,wehooktheinvocationofthe
listof 630APIs fromPyTorchand1900 APIsfrom TensorFlowfor
dynamic tracing. The list includes all the necessary APIs for train-
ing and inference of neural networks as well as performing tensor
computations.FreeFuzzisabletocollectdynamicinformationfor
eachAPIinvokedbyallthethreesourcesofcode/modelexecutions,
includingthetypeandvalueforeachparameter. Nomatterhow
the APIs are invoked (e.g., executed in code snippets, tests, or mod-
els), the corresponding runtime information of the arguments is
recorded to form the following type/value spaces for fuzzing:Customized Type Space.
FreeFuzz constructs our customized
typemonitoringsystem FuzzType forAPIparametersbydynami-
callyrecordingthetypeofeachparameterduringAPIinvocation.
ComparedwithPythonâ€™soriginaltypesystem,thecustomizedtype
systemis atafiner-grained level,whichcan betterguidethe next
mutation phase for fuzzing. In Pythonâ€™s dynamic type system, the
typeofparameter stride=(2,1) (showninFigure4)canbecalcu-
lated by running type((2,1)) . This will return <class â€™tupleâ€™> ,
which does not encode all the necessary useful information for
fuzzingbecauseweonlyknowthatitisatuple.Inourtypemonitor-ingsystem
FuzzType,wecollectsuchinformationatafiner-grained
level:FuzzType ((2,1)) returns(int, int) (atupleoftwointegers).
998
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Similarly, for tensors, executing type(torch.randn(20,16,50,100))
simply returns <class â€™torch.Tensorâ€™> in Pythonâ€™s type system
while we can obtain finer-grained type Tensor<4,float32> (a 4-
dimensionaltensorwith torch.float32 asitsdatatype)byrunning
FuzzType (torch.randn(20,16,50,100)) .Ourcustomizedtypemon-
itoring system used to guide API-level fuzzing of DL libraries is
formally defined in Figure 5.
Figure 5: Customized Type Monitoring System FuzzType
Note that type inference for dynamically typed languages (such
as Ruby and JavaScript) via dynamic program tracing has been
explored in the literature for traditional applications [ 16,17,58].
Inthiswork,wefurtherextendsuchtechniquesforfuzzingdeep
learninglibraries. Differentfrompriorwork, FreeFuzzcollectsdy-
namic traces from various sources, including developer tests, code
snippetsdocuments,andDLmodelsinthewild;also,FreeFuzzaug-
ments the Python built-in types to trace and mutate tensor shapes
and heterogeneous data types.
API Value Space. FreeFuzz constructs the value space of each
API from theconcrete values passed intothe API during dynamic
tracing. One entry in the API value space stands for one API in-
vocationwithitscorrespondinglistofconcretearguments,which
is later used in our mutation phase as the starting point to gen-erate more mutants/tests. Take Figure 3 as an example,
entry1
is added to the value space of the API torch.nn.Conv2d after exe-
cuting the documentation code in the code collection phase. More
specifically, in_channels=16, out_channels=33, kernel_size=(3,5)
together with some other values (not shown in Figure 3 due to lim-
itedspace)arerecordedin entry1. Thereturnvalueof nn.Conv2d
is a callable object, and it expects a tensor as its input, which is ini-
tialized as input=torch.randn(20,16,50,100) , indicating that input
isa four-dimensionaltensorwith (20,16,50,100) asits shapeand
the values are randomly initialized. Note that we also record thecorresponding shape and data type information for tensors, i.e.,
Input_tensor_shape=(20,16,50,100), Input_tensor_type=float32 .
Allthefunctionargumentsmentionedaboveconstitute oneentry
in the value space for nn.Conv2d . Each invocation can add a new
entry into the value space of the invoked API.
Argument Value Space. As shown in Figure 3, the argument
valuespaceiscomposedofdifferentargumentnamesandtypes(e.g.
in_channels oftypeint),togetherwiththeirvaluesrecordedwheninvokingdifferentAPIs.Forexample,fortheargument in_channels
oftheAPI torch.nn.Conv2d ,thevaluesrecordedinclude 16,1,etc.
The argument value space is constructed based on the informa-
tion collected in the API value space to speed up the queries in
our database value mutation strategy discussed later. More specifi-
cally, argument value space aggregates values from different APIs
based on argument names. The argument value space is formedbased on the idea that values for an argument of one API canserve as potentially reasonable values for the argument of othersimilar APIs. For example,
torch.nn.Conv2d andtorch.nn.Conv3d
can be considered similar. The API definition of 3D-Convolution is
torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1,
padding=0, dilation=1, groups=1, bias=True, padding_mode=â€˜zerosâ€™,device=None, dtype=None)
, and many parameters share the same
namesas torch.nn.Conv2d (showninFigure2).Theconstructionof
the argument value space is useful for the database value mutation
to be introduced in the next section.
3.3 Mutation
In this phase, FreeFuzz applies various mutation rules to mutate
theargumentvaluestracedinthe second phasetogeneratemore
tests for fuzzing DL libraries more thoroughly.
Mutation Rules. ThemutationrulesforFreeFuzzarecomposed
of twoparts: type mutation andvalue mutation, shownin Tables 1
and 2, respectively. Type mutation strategies include Tensor Dim
Mutation thatmutates ğ‘›1-dimensionaltensorsto ğ‘›2-dimensional
tensors,TensorDtypeMutation thatmutatesthedatatypesoften-
sorswithoutchangingtheirshapes, PrimitiveMutation thatmutates
one primitive type into another, as well as Tuple Mutation andList
Mutation thatmutatethetypesofelementsincollectionsofhetero-
geneous objects.
Value mutation strategies are divided into two classes: one is
randomvalue mutation,andtheother isdatabasevalue mutation.
Random value mutation strategies include Random Tensor Shape
(usingrandomintegersasshapestomutate ğ‘›-dimensionaltensors),
Random Tensor Value (using random values to initialize tensors),
Random Primitive, Random Tuple andRandom List. Database muta-
tionstrategiesinclude DatabaseTensorShape andDatabaseTensor
Value,whichrandomlypicktheaccordingshapesorvaluesfromdatabaseofargumentvaluespace,togetherwith DatabasePrimi-
tive,DatabaseTuple,and DatabaseList,whichrandomlypickthe
corresponding entries from the argument value space based on the
argument names and types. Note that all the mutation rules are
type-aware, i.e., they are applied according to the types.
Algorithm. ShowninAlgorithm1,theinputtoourfuzzingalgo-
rithmistheAPItobemutated,entriesintheAPIvaluespace VS,and
thedatabaseofargumentvaluespace DB.Ofcourse,wealsoneed
todefinethemutationrulesasdescribedabove.Ineachiteration,
the algorithm always samples the next entry from the API value
spaceVS[API]to start the mutation process (Line 3). FreeFuzz then
computesthenumberofarguments argNumintheentry(Line4),and
randomlyselectsanintegerbetween1and argNumasthenumberof
arguments to be mutated, i.e., numMutation (Line 5). Then, FreeFuzz
startsaninnerlooptomutate numMutation argumentstogenerate
anewtest.Theargumentsaremutatedonebyoneviarandomly
999
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang
Table 1: Type Mutation
Mutation Strategies ğ‘‡1 ğ‘‡2
Tensor Dim Mutation ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›1,ğ·ğ‘‡/angbracketrightğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›2,ğ·ğ‘‡/angbracketright( |ğ‘›2âˆ’ğ‘›1|>0)
Tensor Dtype Mutation ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡1/angbracketright ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡2/angbracketright(ğ·ğ‘‡2â‰ ğ·ğ‘‡1)
Primitive Mutation ğ‘‡1=ğ‘–ğ‘›ğ‘¡|ğ‘ğ‘œğ‘œğ‘™|ğ‘“ğ‘™ğ‘œğ‘ğ‘¡|ğ‘ ğ‘¡ğ‘Ÿ ğ‘‡2(ğ‘‡2â‰ ğ‘‡1)
Tuple Mutation (ğ‘‡ğ‘–âˆˆ1...ğ‘›
ğ‘–) (ğ‘¡ğ‘¦ğ‘ğ‘’_ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’(ğ‘‡ğ‘–)ğ‘–âˆˆ1...ğ‘›)
List Mutation [ğ‘‡ğ‘–âˆˆ1...ğ‘›
ğ‘–] [ğ‘¡ğ‘¦ğ‘ğ‘’_ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’(ğ‘‡ğ‘–)ğ‘–âˆˆ1...ğ‘›]
Table 2: Value Mutation
Mutation Strategies ğ‘‡ ğ‘‰
Random Tensor Shape ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡/angbracketrightğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ(ğ‘ â„ğ‘ğ‘ğ‘’=[ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘–ğ‘›ğ‘¡()ğ‘›],ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¡ğ‘¦ğ‘ğ‘’ =ğ·ğ‘‡)
Random Tensor Value ğ‘£:ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡/angbracketrightğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ(ğ‘ â„ğ‘ğ‘ğ‘’=ğ‘£.ğ‘ â„ğ‘ğ‘ğ‘’,ğ‘‘ğ‘ğ‘¡ğ‘ğ‘¡ğ‘¦ğ‘ğ‘’ =ğ·ğ‘‡).ğ‘Ÿğ‘ğ‘›ğ‘‘()
Random Primitive ğ‘–ğ‘›ğ‘¡|ğ‘“ğ‘™ğ‘œğ‘ğ‘¡|ğ‘ğ‘œğ‘œğ‘™|ğ‘ ğ‘¡ğ‘Ÿ ğ‘Ÿğ‘ğ‘›ğ‘‘(ğ‘–ğ‘›ğ‘¡|ğ‘“ğ‘™ğ‘œğ‘ğ‘¡|ğ‘ğ‘œğ‘œğ‘™|ğ‘ ğ‘¡ğ‘Ÿ)
Random Tuple (ğ‘‡ğ‘–âˆˆ1...ğ‘›
ğ‘–) (ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’_ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’(ğ‘‡ğ‘–)ğ‘–âˆˆ1...ğ‘›)
Random List [ğ‘‡ğ‘–âˆˆ1...ğ‘›
ğ‘–] [ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’_ğ‘šğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’(ğ‘‡ğ‘–)ğ‘–âˆˆ1...ğ‘›]
Database Tensor Shape ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡/angbracketright ğ‘ğ‘–ğ‘ğ‘˜_ğ‘ â„ğ‘ğ‘ğ‘’(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ğ‘ğ‘ ğ‘’,ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ /angbracketleftğ‘›,ğ·ğ‘‡/angbracketright)
Database Tensor Value ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡/angbracketright ğ‘ğ‘–ğ‘ğ‘˜_ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ğ‘ğ‘ ğ‘’,ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ /angbracketleftğ‘›,ğ·ğ‘‡/angbracketright)
Database Primitive ğ‘–ğ‘›ğ‘¡|ğ‘“ğ‘™ğ‘œğ‘ğ‘¡|ğ‘ ğ‘¡ğ‘Ÿ ğ‘ğ‘–ğ‘ğ‘˜(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ğ‘ğ‘ ğ‘’,ğ‘–ğ‘›ğ‘¡ |ğ‘“ğ‘™ğ‘œğ‘ğ‘¡|ğ‘ ğ‘¡ğ‘Ÿ,ğ‘ğ‘Ÿğ‘”ğ‘›ğ‘ğ‘šğ‘’ )
Database Tuple (ğ‘‡ğ‘–âˆˆ1...ğ‘›
ğ‘–) ğ‘ğ‘–ğ‘ğ‘˜(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ğ‘ğ‘ ğ‘’, (ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘›),ğ‘ğ‘Ÿğ‘”ğ‘›ğ‘ğ‘šğ‘’ )
Database List [ğ‘‡ğ‘–âˆˆ1...ğ‘›
ğ‘–] ğ‘ğ‘–ğ‘ğ‘˜(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘ğ‘ğ‘ ğ‘’, [ğ‘‡1,ğ‘‡2,...,ğ‘‡ğ‘›],ğ‘ğ‘Ÿğ‘”ğ‘›ğ‘ğ‘šğ‘’ )
selectingarandomargumentindex argIndex (Line7).Afterdeter-
miningtheargumenttobemutatedeachtime,FreeFuzzgetsthe
typeofitusingourcustomizedtypesystem FuzzType,theargument
nameargName,andtheargumentvalue argValue (Lines8,9and10).
The type mutation will be performed nondeterministically â€“ if it is
enabled, FreeFuzz will mutate the argument type according to our
typemutationstrategies(Line12). selectRandOverDB isanotherran-
domfunctioncalledtodeterminewhethertoperformrandomvalue
mutation (Line 14) or database value mutation (Line 16) according
tothecorrespondingmutationrules.Aftermutating numMutation
arguments for entry, FreeFuzz generates a new test, which will be
executed for testing the API (Line 19). Then, the main loop will
continue to generate the next test until the termination criterion is
met, e.g., generating a specific number of new tests.
We next discuss function ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘‘ğ‘in more detail to explain
the process for mutating the value of an argument for a specific
APIbasedontheargumentvaluespace.Showninthealgorithm,
thefunctiontakestheAPIname API,thetypeofargument argType,
the name of the argument argName, and the database DB, as input
parameters. It then queries the database to collect all the APIs
which share the same argument name and type as the current API
under test (Line 21). Next, FreeFuzz computes the text similari-
tiesbetweenthecurrentAPIundertestandeachofthereturned
APIsbasedontheLevenshteinDistance[ 10]betweenAPIdefini-
tions (Line 22). Take the query ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘‘ğ‘(torch.nn.MaxPool2d,
[int, int], â€˜dilationâ€™, DB) as an example, the text similarity
is computed using API definitions of those containing the same
argumentname( â€™dilationâ€™ )andthetype(tupleoftwointegers).
More specifically, the similarity between the current API under
test andğ´ğ‘ƒğ¼ğ‘–, one of the returned APIs, can be computed by the
following formula:
ğ‘†ğ‘–ğ‘š(ğ´ğ‘ƒğ¼ğ‘–,ğ´ğ‘ƒğ¼)=1âˆ’ğ¿ğ‘’ğ‘£ğ‘’ğ‘›ğ‘ â„ğ‘¡ğ‘’ğ‘–ğ‘› (ğ´ğ‘ƒğ¼ğ‘–,ğ´ğ‘ƒğ¼)
ğ‘€ğ‘ğ‘¥(ğ¿ğ‘’ğ‘›(ğ´ğ‘ƒğ¼ğ‘–),ğ¿ğ‘’ğ‘›(ğ´ğ‘ƒğ¼))
where function ğ¿ğ‘’ğ‘£ğ‘’ğ‘›ğ‘ â„ğ‘¡ğ‘’ğ‘–ğ‘› computes Levenshtein Distance be-
tween the two strings representing ğ´ğ‘ƒğ¼ğ‘–andğ´ğ‘ƒğ¼, and it is divided
bythemaximumlengthofthetwostrings.Thewholeformulacom-putesthetextsimilarityofthetwoAPIdefinitions.Forourexample,theresultshowsthatthedefinitionof
torch.nn.Conv2d hasthehigh-
esttextsimilaritywiththetargetAPI torch.nn.MaxPool2d(kernel_size,
stride=None, padding=0, dilation=1, return_indices=False,
ceil_mode=False) . Then we normalize the text similarities to trans-
formthemintoprobabilities(summingupto1)forselectingsimilar
APIs (Line 23). The basic idea is that APIs with higher similarity
scores should get higher probabilities to be selected. FreeFuzz does
this by performing the Softmax computation [14]:
ğ‘ƒğ‘Ÿğ‘œğ‘(ğ´ğ‘ƒğ¼ğ‘–)=ğ‘’ğ‘†ğ‘–ğ‘š(ğ´ğ‘ƒğ¼ ğ‘–,ğ´ğ‘ƒğ¼)
Î£ğ‘š
ğ‘—=1ğ‘’ğ‘†ğ‘–ğ‘š(ğ´ğ‘ƒğ¼ ğ‘—,ğ´ğ‘ƒğ¼)
whereğ‘šdenotesthenumberofAPIssharingthesameargument
name and type as the current API under test. After sampling a
random API according to the probabilities (Line 24), the values
arethenrandomlysampledfromthevaluesrecordedfortheAPI
(Line 25). In this way, the values stored in the database from one
API can be transferred to serve as the arguments for another API.
3.4 Test Oracle
In this phase, we leverage the following ways to resolve the test
oracle problem and detect potential DL library bugs:
Wrong-ComputationBugs. Weconsiderthreemodestoruneach
API: CPU, GPU with CuDNN disabled, and GPU with CuDNN
enabled.Inthisway,wecandetectwrong-computationresultsbycomparing the results between different execution modes.PerformanceBugs.
We leveragemetamorphic relations [ 25,63]
to detect performance bugs with FreeFuzz. More and more datatypes and hardware accelerators have been proposed in order to
boostthe DLlibrary performancein recentyears. Severalfloating
pointdatatypesarespeciallydesignedfortensors,including float32,
float16,tf32,bfloat16, which also appear in our aforementioned
tensor data type system. We observe the fact that on the samemachine (hardware)
M, APIs with the same function arguments
ğ‘ğ‘Ÿğ‘”ğ‘ andtensorsofthesameshapes ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ/angbracketleftğ‘›,ğ·ğ‘‡/angbracketrighttendtoholdthe
following metamorphicrelationship in terms of time cost:
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ·ğ‘‡1)<ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ·ğ‘‡2)=â‡’
ğ‘ğ‘œğ‘ ğ‘¡(M,ğ´ğ‘ƒğ¼,ğ‘ğ‘Ÿğ‘”ğ‘ ,ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ /angbracketleftğ‘›,ğ·ğ‘‡1/angbracketright)<ğ‘ğ‘œğ‘ ğ‘¡(M,ğ´ğ‘ƒğ¼,ğ‘ğ‘Ÿğ‘”ğ‘ ,ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ /angbracketleftğ‘›,ğ·ğ‘‡2/angbracketright)
1000
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Algorithm 1: Mutation algorithm
Input:
ğ´ğ‘ƒğ¼ # the API under test to be mutated
ğ‘‰ğ‘† # API value space
ğ·ğµ # argument value space
Define:
ğ‘‡ğ‘¦ğ‘ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ # type mutation strategies
ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘Ÿğ‘ğ‘›ğ‘‘ # random value mutation strategies
ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘‘ğ‘ # database value mutation strategies
1Function Mutate(ğ´ğ‘ƒğ¼,ğ‘‰ğ‘†,ğ·ğµ):
2whileğ‘›ğ‘œğ‘¡ğ¹ğ‘–ğ‘›ğ‘–ğ‘ â„ğ‘’ğ‘‘ do
3 ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦=ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘¡ (ğ‘‰ğ‘†[ğ´ğ‘ƒğ¼])
4 ğ‘ğ‘Ÿğ‘”ğ‘ğ‘¢ğ‘š =ğ‘™ğ‘’ğ‘›(ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦) # number of arguments
5 ğ‘›ğ‘¢ğ‘šğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘š.ğ‘”ğ‘’ğ‘¡ (ğ‘ğ‘Ÿğ‘”ğ‘ğ‘¢ğ‘š)
6 whileğ‘›ğ‘¢ğ‘šğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› >0do
7 ğ‘ğ‘Ÿğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ =ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘ğ‘’ğ‘¥ğ‘¡ (ğ‘ğ‘Ÿğ‘”ğ‘ğ‘¢ğ‘š)
8 ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’ =ğ¹ğ‘¢ğ‘§ğ‘§ğ‘‡ğ‘¦ğ‘ğ‘’ (ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦[ğ‘ğ‘Ÿğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ ])
9 ğ‘ğ‘Ÿğ‘”ğ‘ğ‘ğ‘šğ‘’ =ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦[ğ‘ğ‘Ÿğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ ].ğ‘›ğ‘ğ‘šğ‘’
10 ğ‘ğ‘Ÿğ‘”ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ =ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦[ğ‘ğ‘Ÿğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ ].ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’
11 ifğ‘‘ğ‘œğ‘‡ğ‘¦ğ‘ğ‘’ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ()then
12 ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’ =ğ‘‡ğ‘¦ğ‘ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ (ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’)
13 ifğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘…ğ‘ğ‘›ğ‘‘ğ‘‚ğ‘£ğ‘’ğ‘Ÿğ·ğµ ()then
14 ğ‘›ğ‘’ğ‘¥ğ‘¡=ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘Ÿğ‘ğ‘›ğ‘‘(ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’,ğ‘ğ‘Ÿğ‘”ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ )
15 else
16 ğ‘›ğ‘’ğ‘¥ğ‘¡=
ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘‘ğ‘(ğ´ğ‘ƒğ¼,ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’,ğ‘ğ‘Ÿğ‘”ğ‘ğ‘ğ‘šğ‘’,ğ·ğµ )
17 ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦[ğ‘ğ‘Ÿğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ ]=ğ‘›ğ‘’ğ‘¥ğ‘¡
18 ğ‘›ğ‘¢ğ‘šğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =ğ‘›ğ‘¢ğ‘šğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆ’1
19 ğ‘Ÿğ‘¢ğ‘›(ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦)
20Function ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘…ğ‘¢ğ‘™ğ‘’ ğ‘‘ğ‘(ğ´ğ‘ƒğ¼,ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’,ğ‘ğ‘Ÿğ‘”ğ‘ğ‘ğ‘šğ‘’ ,ğ·ğµ):
21ğ´ğ‘ƒğ¼ğ‘ =ğ·ğµ.ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘¦ (ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’,ğ´ğ‘ƒğ¼,ğ‘ğ‘Ÿğ‘”ğ‘ğ‘ğ‘šğ‘’ )
22/angbracketleftğ´ğ‘ƒğ¼ğ‘–,ğ‘ ğ‘–ğ‘š/angbracketright=ğ‘†ğ‘–ğ‘š(ğ´ğ‘ƒğ¼ğ‘ ,ğ´ğ‘ƒğ¼ )
23/angbracketleftğ´ğ‘ƒğ¼ğ‘–,ğ‘ğ‘Ÿğ‘œğ‘/angbracketright=ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (/angbracketleftğ´ğ‘ƒğ¼ğ‘–,ğ‘ ğ‘–ğ‘š/angbracketright)
24ğ´ğ‘ƒğ¼/prime=ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’(/angbracketleftğ´ğ‘ƒğ¼ğ‘–,ğ‘ğ‘Ÿğ‘œğ‘/angbracketright)
25ğ‘£ğ‘ğ‘™=ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’(ğ·ğµ,ğ´ğ‘ƒğ¼/prime,ğ‘ğ‘Ÿğ‘”ğ‘‡ğ‘¦ğ‘ğ‘’,ğ‘ğ‘Ÿğ‘”ğ‘ğ‘ğ‘šğ‘’ )
26returnğ‘£ğ‘ğ‘™
Thisindicatesthat ğ·ğ‘‡1carryinglessprecisioninformationthan ğ·ğ‘‡2
tends to execute faster. For instance, ğ·ğ‘‡1can befloat16whileğ·ğ‘‡2
isfloat32, as long as the API supports both data types of tensors.
CrashBugs. IfanAPIcrashesorthrowsruntimeexception,thenit
may be considered as a crash bug. Meanwhile, it could also be due
to invalid test inputs which can be generated during the fuzzing
process. To automatically filter out such false alarms, we build
scripts to heuristically remove crash bugs which throw meaningful
exceptions on all backends for invalid inputs, e.g., â€˜ ValueError â€™,
â€˜InvalidArgumentError â€™,etc. Asaresult,ifthetestprogramcrashes
(e.g., segmentation fault), or throws unexpected exception for valid
inputs on certain backend(s), it is considered as a crash bug.
4 EXPERIMENTAL SETUP
In the study, we address the following research questions:
â€¢RQ1:How do the three different input sources of FreeFuzz
(without any mutation) contribute to DL library testing?â€¢RQ2:HowdoesFreeFuzzwithdifferentnumbersofmuta-
tions for each API perform for DL library testing?
â€¢RQ3:How do different mutation strategies impact Free-
Fuzzâ€™s performance?
â€¢RQ4:How does FreeFuzz compare with existing work?
â€¢RQ5:Can FreeFuzz detect real-world bugs?
Ourexperimentsaremainlyperformedonthestablereleasever-
sionsofDLlibraries:PyTorch1.8andTensorFlow2.4.Themachine
for running experiments is equipped with Intel Xeon CPU (4 cores,
2.20GHz), NVIDIA A100 GPUs, Ubuntu 16.04, and Python 3.9.
4.1 Implementation
Code/Model Collection. Code/model collection is essential to
form the original seed test pool for our fuzzing technique. To build
an extensive pool, for documentations, we download all 497/512pieces of code snippets from the official documentations of Py-
Torch/TensorFlow.More specifically,we use the bs4Python pack-
age [3] to automatically parse the documentations to obtain the
codesnippets. Notethatnotallcodesnippetscrawledfromdoc-umentations are immediately executable. Thus we also build asimplisticrepairtooltoinsertomittedcodeintheexamples(e.g.,
importsections) to make more code snippets executable. For devel-
oper tests, we run all existing Python tests for PyTorch by running
python run_test.py in the test directory, while for TensorFlow we
runallpythonfileswithsuffix _test.py. ForDLmodels,weobtain
adiversesetof102/100DLmodelsfromofficialmodelzoosofPy-
Torch/TensorFlow,andpopularGitHubrepositories.Thedetailed
information about the models can be found in our repository [ 8].
Instrumentation. We get the lists of all Python APIs from offi-
cial documentation of PyTorch and TensorFlow, and hook them in
__init__.py (afileforapackagethatwillbeautomaticallyexecuted
if the package is imported) in the root of the library package by
addingawrapperforeachAPIinthelist.Thisisdonetransparently
and fully automatically for the users so that they do not need to
modify any of their code (model code) for instrumentation. In this
way,630APIsfromPyTorchand1900APIsfromTensorFloware
instrumented for dynamic value tracing. Furthermore, we lever-
agetheMongoDBdatabase[ 7,11]torecordAPIvaluespaceand
argument value space.Mutation.
WeimplementourmainAlgorithm1formutationwith
standard Python packages. The implementation details can also be
found in our project repository [8].
Test Oracle. Theimplementationofdifferentialtestingissimple.
TheexamplecodeforPyTorchisshowninFigure10.Meanwhile,the
implementation of metamorphic testing is to wrap the invocation
of APIs with code for timing.
4.2 Metrics
To thoroughly evaluate FreeFuzz, we use the following metrics:
NumberofCoveredAPIs. DuetothelargenumberofAPIsinDL
libraries,itisofgreatimportancetoshowthenumberofcovered
APIsasanimportantmetricoftestingadequacy.Surprisingly,such
an important metric has been largely overlooked by prior work on
DL library testing [57, 69].Size of Value Space.
Each API invocation can add one entry into
the API value space. Therefore, we use the total size of value space
1001
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang
for all APIs to serve as the metric to analyze and compare different
inputsources.Tobemoreaccurate,wecountthenumberofentries
intheAPIvaluespaceafterremovingduplicateentries.Pleasenote
that this is just used to show the scale of the traced data, and does
not necessarily indicate fuzzing effectiveness.Line Coverage.
Code coverage is a widely adopted test adequacy
criterion for testing traditional software systems [ 21,44,76] and
even the recent tensor compilers [ 46]. For example, it is impossible
foratesttodetectbugsincodeportionswithoutexecutingthem.
Surprisingly,althoughstate-of-the-artDLlibrarytestingtechniques
(e.g.,LEMON)claimedtoinvokemorelibrarycode[ 69],theydidnot
reportanycodecoverageintheirexperiments.Wespenttremen-
dous time and efforts setting up the environment for collecting the
most widely used line coverage via GCOV [ 9] for both PyTorch
andTensorFlow.Morespecifically,weevenfixedabugintheBazelbuildsystem[
2]usedforbuildingTensorFlowtoperformcoverage
collection.NotethatweonlytraceC/C++codecoveragebecausetheC/C++implementationprovidesthefundamentalsupportfor
operators in DL libraries and almost all the high-level Python APIs
finally invoke the C/C++ code.Number of Detected Bugs.
Following prior work on software
testingingeneralandDLlibrarytesting[ 24,57,69],wealsoreport
the number of actual bugs detected for the studied DL libraries.
5 RESULT ANALYSIS
5.1 RQ1: Input Source Study
InthisRQ,weaimtostudytheeffectivenessofdirectlyapplying
FreeFuzzâ€™s traced dynamic information (without any mutation)
for testing DLlibraries. Themain experimentalresults areshown
in Table 3, where we explore different settings, including using
documentationsonly,testsonly,modelsonly,andallinformation
together for both TensorFlow and PyTorch. For each setting, we
show the number of covered APIs (Row â€œ# APIâ€), the number of
traced unique API invocations (Row â€œ# VSâ€), and the line coverage
achievedwhendirectlyrunningthetracedAPIinvocations(Row
â€œLineCov.â€).Fromthetable,wecanobservethatdifferentsourcesof
informationalltendtobehelpfulfortestingDLlibraries.Forexam-
ple, although the test information covers the least number of APIs
forTensorFlow,itcanstillhelpdirectlycover216APIsand31293
lines of code; similarly, although the model information covers the
leastnumberofAPIsforPyTorch,itcanstillhelpdirectlycover145
APIs and 26292 lines of code. Also, another interesting observation
is that the settings covering more APIs tend to also achieve higher
code coverage. The reason is that different APIs usually implementdifferentfunctionalities,andthususuallycoverdifferentDLlibrary
behaviors/paths. This actually also demonstrates the effectiveness
and necessityof API-level testingfor DL libraries sinceit is much
easier to cover more APIs at this level than traditional model-level
DL library testing [57, 69].
We can also observe that putting all three sources of informa-
tion together can achieve even better results than using any sin-
glesourceofinformation.Forexample,itcancover470/688APIs
for PyTorch/TensorFlow, and 42425/39575 lines of code for Py-
Torch/TensorFlow.Tobetteranalyzethecontributionofeachsource
of information, we further leverage the Venn diagrams in Figure 6
andFigure7topresentthedetailedbreakdownofthenumberofTable 3: Statistics about different sources
FreeFuzz PyTorch FreeFuzz TensorFlow
Doc Test Model All Doc Test Model All
#A P I 427 176 145 470 486 216 269 688
#V S 1259 3383 10898 15532 1810 6879 36638 45269
Line Cov. 39272 30476 26292 42425 33906 31293 34790 39575
Figure 6: Venn diagram for covered APIs
Figure 7: Venn diagram for code coverage
coveredAPIsandcoveragerespectively.Fromthefigure,forboth
TensorFlow and PyTorch, each source of inputs exclusively covers
some APIs and only a small number of APIs are covered by allthree sources of information. For example, only 59/62 out of all
the 470/688 covered APIs are covered by all three sources of inputs
on PyTorch/TensorFlow. Meanwhile, although each source of in-puts still exclusively covers different code portions, the majorityof covered code tends to be shared by all three sources of inputs.
ThereasonisthatalthoughdifferentAPIsimplementdifferentcode
logic, they can be decomposed to a set of common low-level op-eratorsimplementedinC/C++.Overall,theexperimentalresults
furtherconfirmthatitisnecessaryandimportanttoconsiderdif-
ferent sources of information for effective DL library testing.
Tracingthethreesourcesofinputsisa one-timeeffort andcan
beusedfortestingallsubsequentversionsofthesameDLlibraries.
Meanwhile, itis alsoimportant to demonstratethat the timeover-
headisacceptableandnotextremelyhigh.Therefore,wefurther
discusstheoverheadforconstructingthethreesourcesofinputs.
Forthedocumentationsource,the codesnippetsareusuallyquite
shortandfasttorun.Intotal,FreeFuzztakeslessthan20minfor
tracingallthedocumentationcodesnippetsforbothTensorFlow
and PyTorch. For the developer tests, tracing the 1493/1420 official
tests written by developers from PyTorch/TensorFlow consumes
about 2.5/5.0 hours. Lastly, for the model source, FreeFuzz runs all
the102/100modelsstatedinSection4.1withinstrumentationfor
PyTorch/TensorFlow, consuming less than 1 hour for each of them.
5.2 RQ2: Coverage Trend Analysis
In thisRQ, we presentthe effectiveness of FreeFuzzwith different
numbersofmutationsforeachAPIundertest.Theexperimental
results are shown as the blue lines (with legend â€œFreeFuzz â€) in Fig-
ure8andFigure9,wherethe xaxispresentsthenumberofmutants
generated foreach API(from 100 to1000 with theinterval of100)
whilethe yaxisshowstheoverallcoverageachievedviagenerating
differentnumberofmutantsforeachAPI(theunionofallcoverage
1002
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 8: Coverage trend analysis for PyTorch
Figure 9: Coverage trend analysis for TensorFlow
sets for all tested APIs). Note that the start point denotes the code
coverage achieved by directly executing the original test inputs
traced without any mutation. From the figure, we can observe that
for both PyTorch and TensorFlow, FreeFuzz can indeed cover more
lines of code with more mutations enabled for each API under test,
demonstrating the overall effectiveness of our mutation strategies.
Furthermore,wecanalsoobservethatthecoveragebecomeslargely
stableafterrunning600mutationsforeachAPI,indicatingthat600
mutations can be a cost-effective choice in practice. Regarding the
timecost,thetotalrunningtimeforgeneratingandrunningall1000mutants for all APIs is 7.3 hours for PyTorch and 9.9 hours for Ten-
sorFlow. Note that such overhead is totally acceptable for fuzzing,
e.g.,traditionalbinaryfuzzingtechniquesareusuallyappliedfor
24h [19] and LEMON takes over 24h [69].
5.3 RQ3: Different Mutation Strategies
After tracing the initial inputs from various sources, FreeFuzz per-
forms three different mutation strategies in tandem (as detailed in
Algorithm 1). Therefore, in this RQ, we further study the impact of
eachmutationstrategybydisablingit.Tothisend,wehavethree
FreeFuzzvariants,FreeFuzz-TypeMu(disablingthetypemutation
strategy), FreeFuzz-RandMu (disabling the random value mutation
strategy),FreeFuzz-DBMu(disablingthedatabasevaluemutation
strategy).Notethatwealsoincludethevariantthatdoesnotper-
form any mutation at all, i.e., FreeFuzz-AllMu. The experimental
resultsforallthestudiedvariantswithdifferentnumberofmuta-
tionsforeachAPIarealsoshowninFigure8andFigure9.Notethat
the start point for all other variants denotes the coverage achieved
by FreeFuzz-AllMu. From the figure, we can have the following
observations.First,thedefaultFreeFuzzoutperformsalltheotherTable 4: Comparison on input coverage
FreeFuzz (tf1.14 full) LEMON CRADLE
# API 313 30 59
#V S 9338 1808 2893
Line Cov. 33389 29489 28967
Table 5: Comparison with LEMON on mutation
FreeFuzz (tf1.14 full) FreeFuzz (models only) LEMON
# API 313 30 35
#V S 305463 913 7916
Line Cov. 35473 30488 29766
Time 7h 20 min 25h
studied variants, indicating the importance and necessity of all
the three mutation strategies of FreeFuzz. Second, we can also ob-
serve that random-value and database-value mutation strategies
performsimilarlyintermsofcodecoverage,whiletypemutation
can be even more effective since the low-level implementations for
different types tend to be more different.
5.4 RQ4: Comparison with Prior Work
InthisRQ,weaimtocompareFreeFuzzwiththestate-of-the-art
LEMON[ 69]andCRADLE[ 57]workforDLlibrarytesting.Wefirst
compare their sources of inputs in terms of the number of covered
APIs and coverage. LEMON only uses 12 models, CRADLE uses 30
models,andFreeFuzzconsidersthreedifferentsourcesofinputwith
manymoremodelsinthewild.SincebothLEMONandCRADLE
use Keras without supporting PyTorch, the comparison here is
onlyconductedonTensorFlow.Also,duetothefactthatLEMON
and CRADLE do not support TensorFlow 2 (used in our earlierexperiments), we apply FreeFuzz on an old TensorFlow version
v1.14. For a fair comparison with prior work, we enforce FreeFuzz
touseexactlythesamemodelsfromLEMONastheDLmodelinput
source. To prepare the other two input sources for FreeFuzz, wecollect developer tests and documentation code for TensorFlow
v1.14.TheexperimentalresultsarepresentedinTable4:Column
â€œFreeFuzz (tf1.14 full)â€ simply runs the inputs traced by running
the same models from LEMON as well as documentation code and
testsfromTensorFlowv1.14;Columnsâ€œLEMONâ€andâ€œCRADLEâ€
simply run the input DL models used in their original work. From
the table, we can observe that, when no mutations are allowed, the
inputsourcesusedbyFreeFuzzcanachievemuchhigherAPIand
code coverage than LEMON and CRADLE.
We next study the effectiveness of the mutation strategies used
by FreeFuzz and existing work (i.e., LEMON because CRADLEperforms no mutation). We follow the same methodology as theoriginal LEMON work [
69] when running its model-level muta-
tions. For FreeFuzz, we also use the default setting, i.e., generating
and running 1000 mutants for each covered API. The experimental
resultsareshowninTable5.NotethatbesidesthedefaultFreeFuzzandLEMON,Columnâ€œFreeFuzz(modelsonly)â€furtherincludesthe
results of FreeFuzz with only the models from LEMON (withoutdocumentationcodeanddevelopertests)astheinputforamore
thoroughcomparisonwithLEMON.FromtheTable,wecanobserve
thatthedefaultFreeFuzzcancover âˆ¼9XmoreAPIsthanLEMON
while consuming âˆ¼3.5X less time! Although the coverage improve-
ment is not as significant as the number of covered APIs, FreeFuzz
can still outperform LEMON by âˆ¼20%. Also, surprisingly, LEMON
1003
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang
1m = torch.nn.Conv2d(64,128,1,2).cuda()
2tensor = torch.rand(1,64,32,32).cuda()
3torch.backends.cudnn.enabled = True
4output1 = m(tensor) # with CuDNN enabled
5torch.backends.cudnn.enabled = False
6output2 = m(tensor ) # with Cu DNN disabled
7 print(output1. sum(), output2. sum()) # debugging
8assert torch.allclose( output1, output2) # fail
Figure 10: Differential testing for 2D-Convolution
only covers 5 more APIs via various model mutations compared to
theoriginalmodels,sinceonly5unusedlayerspreservethestrict
input-output shape constraints imposed by LEMON and are added
into the mutated models. Furthermore, FreeFuzz with models only
can already outperform LEMON in terms of code coverage within
20min, i.e., 75X faster than LEMON! This further demonstrates the
benefits of API-level testing compared with model-level testing.
5.5 RQ5: Bugs Detected
Forbugdetection,wetargetPyTorch1.8andTensorFlow2.4,which
arebothofficiallyreleasedstableversions,withthedefaultFreeFuzz
setting, i.e., generating 1000 mutants for each API. Note that we
donottargetTensorFlow1.14becausedevelopersdonotactively
support it anymore. Table 6 shows the detailed statistics about
thereal-worldbugsdetectedbyFreeFuzzanditsvariousvariants
studied in Section 5.3. We can observe that FreeFuzz is able todetect 49 bugs in total (with 38 already confirmed as previouslyunknown bugs) for the two studied DL libraries, and 21 of them
havebeenfixedbythedeveloperstodate.Furthermore,wecanalso
observethateachmutationstrategycanhelpdetectcertainbugs
that other mutation strategies fail to detect, further demonstrating
theimportanceofallFreeFuzzmutationstrategies.Lastly,ofallthe
49 bugs detected by FreeFuzz, only one of them can be detected by
LEMON and CRADLE.
Table 6: Summary of detected bugs
FreeFuzz Confirmed
FreeFuzz -TypeMu -RandMu -DBMu -AllMu (Fixed)
PyTorch 28 13 24 26 5 2 3(7 )
TensorFlow 21 20 5 20 2 15 (14)
Note that all the detailed issue IDs for the bugs detected can be
found on our GitHub page [8]. We next present the case studies:
Wrong-computationBug. Figure10showsanexamplebugauto-
matically detected by FreeFuzzby comparing the computation re-
sults for 2D-Convolution between two backends, one with CuDNN
enabled ( output1) and one disabled, using Aten backend instead
(output2). It throws AssertionError when executing the last line.
ThesumofvaluesofoutputtensorsinLine7showsthat output1
= -523.5300 whileoutput2 = -601.6165 , indicating a significant
difference in computation results. Further comparing the computa-
tionresultsexecutedbyCPUdemonstratesthattheresultiswrong
only on GPU with CuDNN disabled. This is a confirmed bug by
developers and fixed in latest master.Performancebug.
FreeFuzzdetectsoneperformancebugbymeta-
morphictestingfor torch.nn.functional.conv_transpose2d .Accord-
ingtothemetamorphicrelations,thetimecostfor float16computa-
tion should be less than that for float32given the same parameters
and tensor shapes. However, our results on NVIDIA A100 GPU1 fromtorch.nn import Conv3d
2x = torch.rand(2,3,3,3,3)
3Conv3d(3,4,3,padding_mode='reflect')(x) # Crash
Figure 11: Crash bug in Conv3d
1m_gpu = torch.nn.MaxUnpool2d(2,stride =2).cuda()
2m_cpu = torch.nn.MaxUnpool2d(2,stride=2)
3tensor = torch.rand(1, 1, 2, 2)
4indices = torch.randint(-32768,32768,(1, 1, 2, 2))
5gpu_result = m_gpu(tensor. cuda(), indices.cuda())
6cpu_result = cpu( tensor, indices) # only crash on CPU
Figure 12: Invalid test input for torch.nn.MaxUnpool2d
forPyTorchshowthat float16: cost = 0.377s, float32: cost =
0.101son some inputs. The bug detected by FreeFuzz has spurred
a heated discussion among PyTorch developers. They confirmed
this performance bugand are tryinghard to figure outthe reason.Crashbug.
Figure11showsacrashbugdetectedbyFreeFuzz.The
program crashes on Line 3 when invoking torch.nn.Conv3d . The
reason is that argument padding_mode is set to value â€˜reflectâ€™ and
theprogramwillnotcrashif padding_mode issettoitsdefaultvalue
â€˜zerosâ€™.Thebugistriggeredbythedatabasemutationstrategy.The
argument name padding_mode of type string appears in the argu-
mentvaluespace,andthereexistsavalue â€˜reflectâ€™ ,whichisorig-
inally recorded for the argument padding_mode oftorch.nn.Conv2d .
FreeFuzz applies the database mutation strategy to query the ar-
gument value space, and selects â€˜reflectâ€™ fromConv2dto serve as
the input for argument padding_mode ofConv3d. We confirm this
bug according to the documentation of torch.nn.Conv3d [6] where
4stringvalues(i.e., â€˜zerosâ€™,â€˜reflectâ€™ ,â€˜replicateâ€™ orâ€˜circularâ€™ )
shouldbevalidfor padding_mode .Developershaveacknowledged
this bug and triaged it.
5.6 Threats to validity
The threats to internal validity mainly lie in the correctness of
theimplementationofourownapproachandthecomparedtech-
niques. To reduce such threats, the authors worked together to
perform testing and code review of FreeFuzz; also, we obtained the
implementationof prior work from the official websites.
Thethreatstoexternalvaliditymainlylieintheevaluationbench-
marks used. To demonstrate that our FreeFuzz can be applied/gen-
eralized to different DL libraries, we have evaluated FreeFuzz on
twomostwidelyusedDLlibraries,PyTorchandTensorFlow.Fur-
thermore, although FreeFuzz is fuzzing against 1158 APIs (each
with 1000 times) and the randomness can be largely mitigated
by such a large number of APIs, it is still possible that the non-determinism in FreeFuzz can affect the effectiveness of FreeFuzzin different runs [
18,42]. Therefore, following existing fuzzing
work [59,70,83], we rerun the experimental comparison between
FreeFuzzandLEMON(Table5)for5times.Theresultsshowthat
FreeFuzzachievesanaveragelinecoverageof35997(35473inTa-
ble5),whileLEMONâ€™saverageis29769(29766inTable5).Bothare
quite stable with the coefficient of variation of only 0.82%/0.06%,
demonstrating the effectiveness of FreeFuzz in different runs.
Thethreatstoconstructvaliditymainlylieinthemetricsused.
Toreducesuchthreats,weadoptthenumberofdetectedbugsused
by prior work on DL library testing. Furthermore, we also include
thewidelyusedcodecoveragemetricintraditionalsoftwaretesting.
1004
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
6 DISCUSSION AND FUTURE WORK
Generalizability and Specificity. Different from LEMON [ 69]
andCRADLE[ 57]thatspecificallytargettestingDLlibrariesviaDL
models,theFreeFuzz workcanpotentiallybe generalizedtomore
than just DL library fuzzing. Of course, in this work, we do have
various DL-specific components, including 1) mining DL models
as inputs, 2) tensor-related types and mutation rules, and 3) DL-
specificoracles(i.e.,differentialtestingforwrong-computationbugs
andmetamorphictestingforperformancebugs).Meanwhile,the
basic idea of leveraging code snippets from library documentation
and developer tests can be generalized to fuzzing library APIs in
variousdynamicallytypedlanguages.Wehopeourworkcaninspire
more research on the direction of mining for fuzzing.
ValidityofTestInputs. Ourinputminingandtype-aware[ 53]/DB-
based mutations can all help generate more valid inputs. Mean-
while, FreeFuzz still does not always generate valid inputs due
tosomecomplicatedinputconstraints.Interestingly,eventhein-
valid inputs helped detect various bugs in PyTorch/TensorFlow
(e.g.,unexpected crashes).Figure 12showsone suchbug detected
intorch.nn.MaxUnpool2d .Theinputparameter indicesisatensor
whosevaluesarerandomlysampledintegers(withrespecttothe
Random Primitive strategy), which is invalid. According to the
documentation,thevalid indicesshouldbeobtainedfromthere-
turnedvalueof torch.nn.MaxPool2d .Thebugwasdetectedbecause
the program onlycrashes when running onCPU (i.e., Line 6fails)
butproduces awrong resultsilently withoutthrowing anyerror
messageonGPU(i.e.,Line5passes).Thus,theGPUimplementa-
tion should add the missing check. The developers have confirmed
this bug and even labelled with â€œhigh priorityâ€ [4].
Future Work . FreeFuzz currently only focuses on testing the cor-
rectnessofsingleAPIs.WhileAPI-leveltestinghasmanyadvan-tages over model-level testing, it may still miss bugs which can
onlybetriggeredbyinvokingasequenceofAPIs.Besides,when
reproducing detected bugs, we find that some tests will fail on one
machine but pass on other machines given exactly the same script
andthesamelibraryversion.Thisisprobablyduetothedifferences
inunderlyinginfrastructureandhardware.Thistypeoftestsare
called implementation-dependent flaky tests, described in prior
workontestflakiness[ 43,54,78].Futureworkmayexplorehowto
better detect and fix flaky tests [29â€“32] in deep learning libraries.
7 RELATED WORK
DL Model Testing. There has been a growing body of research
for improving the quality of DL models. Various adversarial at-
tack techniques [ 34,51,52,79] havebeenproposed to generatethe
so-calledâ€œadversarialexamplesâ€byaddingperturbationsimpercep-tible to humans to intentionally fool the classification results given
by DL models. To mitigate such attacks, researchers have also pro-
posedvariousadversarialdefensetechniques,includingadversarialtraining[
34,50,66],detection[ 37,49,82],andothers[ 68].Another
recentlineofresearchhasexploredthepossibilityofimprovingtherobustnessofneuralnetworkfromajointperspectiveoftraditional
software testing and the new scenario of deep learning. DeepX-
plore[56]proposesametriccalledneuroncoverageforwhitebox
testingofDLmodelsandleveragedgradient-basedtechniquesto
searchformoreeffectivetests.Whilevariousothermetrics[ 41,47]have also been proposed recently, the correlation between suchmetrics and the robustness of models is still unclear [
27,38,73].
Meanwhile, there are also a series of work targeting specific ap-plications,such as autonomous driving, including DeepTest [
67],
DeepRoad[ 77],andDeepBillboard[ 84].Varioustechniqueshave
alsobeenproposedtodetectnumericalbugsintroducedwhenbuild-
ing a DNN model at the architecture level with static analysis [ 81],
and via gradient propagation [ 72]. Lastly, researchers have also
explored concolic testing [ 65] to achieve higher coverage for DNN
models, mutation testing [ 40,48] to simulate real faults in DL mod-
els, and test input generation for DNNs [ 28] by exploiting features
learnedfromdatawithgenerativemachinelearning.Differentfrom
allsuchpriorwork,ourworktargetstheunderlyingDLlibraries,
whicharethebasis fortraininganddeployingvariousDLmodels.
DL Library Testing. CRADLE [ 57] is the trailblazing work for
testingDLlibraries.ThemaincontributionofCRADLEisresolv-
ing the test oracle challenge with differential testing on Keras.LEMON [
69] further advanced CRADLE by proposing mutation
rules to generate more models, as claimed by LEMON to invokemore code in DL libraries. LEMONâ€™s mutation strategies include
intact-layerandinner-layermutationrules,whichmustconform
tostrictconstraints,e.g.,forintact-layermutation,thelayertobe
inserted or deleted should preserve the shapes of input tensors.
Actually, according to our experimental results, the mutation rules
applied by LEMON can hardly help cover more DL library code. A
more recent work on testing DL library is Predoo [ 80], which only
mutatestheinputtensorvalueswithallotherAPIparameters man-
uallysetupfor precisiontesting.Asaresult,it wasonlyappliedto
7APIs/operatorsfromTensorFlowandweexcludeitinourcompar-
ison. To our knowledge, we propose the first general-purpose and
fullyautomatedAPI-levelfuzzingapproachforpopularDLlibraries.
Furthermore, we adopt traditional code coverage for DL library
testing,andrevealvariousinterestingfindings(e.g.,state-of-the-art
LEMON can hardly improve the DL library code coverage).
8 CONCLUSION
We have proposed FreeFuzz, the first approach to fuzzing DL li-
brariesviaminingfromopensource.Morespecifically,FreeFuzz
considersthreedifferentsources:1)librarydocumentation,2)de-
velopertests,and3)DLmodelsinthewild.Then,FreeFuzzauto-
maticallyruns allthecollectedcode/models withinstrumentation
totracethedynamicinformationforeachcoveredAPI.Lastly,Free-
Fuzz will leverage the traced dynamic information to perform fuzz
testingforeachcoveredAPI.TheextensivestudyofFreeFuzzon
PyTorch and TensorFlowshows thatFreeFuzz is ableto automati-
callytracevaliddynamicinformationforfuzzing1158popularAPIs,
9Xmorethanstate-of-the-artLEMONwith3.5Xloweroverhead.
FreeFuzzhasdetected49bugsforPyTorchandTensorFlow(with
38 already confirmed by developersas previously unknown bugs).
ACKNOWLEDGMENTS
We thank Darko Marinov, Chenyang Yang, and Matthew Sotoudeh
for theirvaluable discussions andsuggestions. Wealso appreciate
theinsightfulcommentsfromtheanonymousreviewers.Thiswork
was partially supported by National Science Foundation under
Grant Nos. CCF-2131943 and CCF-2141474, as well as Ant Group.
1005
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang
REFERENCES
[1] Keras, 2015. https://keras.io.
[2] Bazel, 2021. https://github.com/bazelbuild/bazel.[3] bs4, 2021. https://pypi.org/project/bs4.[4]
Bug Report for torch.nn.MaxUnpool2d, 2021.
https://github.com/pytorch/pytorch/issues/68727.
[5]Definition of Conv2d from Pytorch official documentation, 2021.
https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html.
[6]Definition of Conv3d from Pytorch official documentation, 2021.
https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html.
[7] Documentation for PyMongo, 2021. https://pymongo.readthedocs.io/en/stable.[8] FreeFuzz Repository, 2021. https://github.com/ise-uiuc/FreeFuzz.[9] GCOV, 2021. https://gcc.gnu.org/onlinedocs/gcc/Gcov.html.
[10]
Levenshtein distance, 2021. https://en.wikipedia.org/wiki/Levenshtein_distance.
[11] MongoDB: the application data platform, 2021. https://www.mongodb.com.[12]
News, 2021. https://www.vice.com/en_us/article/9kga85/uber-is-giving-up-on-
self-driving-cars-in-california-after-deadly-crash.
[13] Pytorch Aten, 2021. https://pytorch.org/cppdocs/#aten.[14] Softmax function, 2021. https://en.wikipedia.org/wiki/Softmax_function.[15]
N.AkhtarandA.Mian.Threatofadversarialattacksondeeplearningincomputer
vision: A survey. Ieee Access, 6:14410â€“14430, 2018.
[16]J.-h. An, A. Chaudhuri, J. S. Foster, and M. Hicks. Dynamic inference of static
types for ruby. ACM SIGPLAN Notices, 46(1):459â€“472, 2011.
[17] E. Andreasen, C. S. Gordon, S. Chandra, M. Sridharan, F. Tip, and K. Sen. Trace
typing:Anapproachforevaluatingretrofittedtypesystems. In 30thEuropean
Conference on Object-Oriented Programming (ECOOP 2016). Schloss Dagstuhl-
Leibniz-Zentrum fuer Informatik, 2016.
[18]A. Arcuri and L. Briand. A practical guide for using statistical tests to assess ran-domizedalgorithmsinsoftwareengineering.In 201133rdInternationalConference
on Software Engineering (ICSE), pages 1â€“10. IEEE, 2011.
[19]M. BÃ¶hme, V.-T. Pham, and A. Roychoudhury. Coverage-based greyboxfuzzing
as markov chain. IEEE Transactions on Software Engineering, 45(5):489â€“506, 2017.
[20]M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D.
Jackel,M.Monfort,U.Muller,J.Zhang,etal. Endtoendlearningforself-driving
cars.arXiv preprint arXiv:1604.07316, 2016.
[21]B. Brosgol. Do-178c: the next avionics safety standard. ACM SIGAda Ada Letters,
31(3):5â€“6, 2011.
[22]N.Carlini,A.Athalye,N.Papernot,W.Brendel,J.Rauber,D.Tsipras,I.Goodfellow,
A. Madry, and A. Kurakin. On evaluating adversarial robustness. arXiv preprint
arXiv:1902.06705, 2019.
[23]J.Chen,H.Ma,andL.Zhang. Enhancedcompilerbugisolationviamemoized
search.In Proceedingsofthe35thIEEE/ACMInternationalConferenceonAutomated
Software Engineering, pages 78â€“89, 2020.
[24]J.Chen,Z.Wu,Z.Wang,H.You,L.Zhang,andM.Yan. Practicalaccuracyesti-
mation for efficient deep neural network testing. ACM Transactions on Software
Engineering and Methodology (TOSEM) , 29(4):1â€“35, 2020.
[25]T. Y. Chen, F.-C. Kuo, H. Liu, P.-L. Poon, D. Towey, T. Tse, and Z. Q. Zhou.
Metamorphictesting:Areviewofchallengesandopportunities. ACMComputing
Surveys (CSUR), 51(1):1â€“27, 2018.
[26]S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, andE. Shelhamer. cudnn: Efficient primitives for deep learning. arXiv preprint
arXiv:1410.0759, 2014.
[27]Y. Dong, P. Zhang, J. Wang, S. Liu, J. Sun, J. Hao, X. Wang, L. Wang, J. Dong, and
T.Dai. Anempiricalstudyoncorrelationbetweencoverageandrobustnessfor
deep neural networks. In 2020 25th International Conference on Engineering of
Complex Computer Systems (ICECCS), pages 73â€“82. IEEE, 2020.
[28]I. Dunn, H. Pouget, D. Kroening, and T. Melham. Exposing previously unde-
tectable faults in deep neural networks. arXiv preprint arXiv:2106.00576, 2021.
[29]S.Dutta,O.Legunsen,Z.Huang,andS.Misailovic. Testingprobabilisticprogram-
ming systems. In Proceedings of the 2018 26th ACM Joint Meeting on European
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering, pages 574â€“586, 2018.
[30]S.Dutta,A.Shi,R.Choudhary,Z.Zhang,A.Jain,andS.Misailovic. Detecting
flakytestsinprobabilisticandmachinelearningapplications. In Proceedingsof
the29thACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis,
pages 211â€“224, 2020.
[31]S. Dutta, A. Shi, and S. Misailovic. Flex: fixing flaky tests in machine learningprojects by updating assertion bounds. In Proceedings of the 29th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, pages 603â€“614, 2021.
[32]S.Dutta,W.Zhang,Z.Huang,andS.Misailovic. Storm:programreductionfor
testing and debugging probabilistic programming systems. In Proceedings of the
201927thACMJointMeetingonEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundations of Software Engineering, pages 729â€“739, 2019.
[33]F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual
prediction with lstm. Neural computation, 12(10):2451â€“2471, 2000.
[34]I.J.Goodfellow,J.Shlens,andC.Szegedy. Explainingandharnessingadversarial
examples. arXiv preprint arXiv:1412.6572, 2014.[35]A.Graves,A.-r.Mohamed,andG.Hinton.Speechrecognition withdeeprecurrent
neuralnetworks. In 2013IEEEinternationalconferenceonacoustics,speechand
signal processing, pages 6645â€“6649. Ieee, 2013.
[36]S.Grigorescu,B.Trasnea,T.Cocias,andG.Macesanu. Asurveyofdeeplearningtechniquesforautonomousdriving. JournalofFieldRobotics,37(3):362â€“386,2020.
[37]S.Gu,P.Yi,T.Zhu,Y.Yao,andW.Wang. Detectingadversarialexamplesindeep
neural networks using normalizing filters. UMBC Student Collection, 2019.
[38]F.Harel-Canada,L.Wang,M.A.Gulzar,Q.Gu,andM.Kim. Isneuroncoverageameaningful measure for testing deep neural networks? In Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, pages 851â€“862, 2020.
[39]K.He,X.Zhang,S.Ren,andJ.Sun.DeepResidualLearningforImageRecognition.
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
[40]N. Humbatova, G. Jahangirova, and P. Tonella. Deepcrime: mutation testingof deep learning systems based on real faults. In Proceedings of the 30th ACM
SIGSOFT International Symposium on Software Testing and Analysis , pages 67â€“78,
2021.
[41]J. Kim, R. Feldt, and S. Yoo. Guiding deep learning system testing using surprise
adequacy. In 2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering
(ICSE), pages 1039â€“1049. IEEE, 2019.
[42]G. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks. Evaluating fuzz testing. In
Proceedingsofthe2018ACMSIGSACConferenceonComputerandCommunications
Security, pages 2123â€“2138, 2018.
[43]W. Lam, S. Winter, A. Wei, T. Xie, D. Marinov, and J. Bell. A large-scale longitu-
dinal study of flaky tests. Proceedings of the ACM on Programming Languages,
4(OOPSLA):1â€“29, 2020.
[44]O.Legunsen,F.Hariri,A.Shi,Y.Lu,L.Zhang,andD.Marinov. Anextensivestudyofstaticregressiontestselectioninmodernsoftwareevolution. In Proceedingsof
the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software
Engineering, pages 583â€“594, 2016.
[45]X. Li, W. Li, Y. Zhang, and L. Zhang. Deepfl: Integrating multiple fault diagnosis
dimensions for deep fault localization. In Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis, pages 169â€“180, 2019.
[46]J. Liu, Y. Wei, S. Yang, Y. Deng, and L. Zhang. Coverage-guided tensor compiler
fuzzing with joint ir-pass mutation. arXiv preprint arXiv:2202.09947, 2022.
[47]L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su, L. Li, Y. Liu,
et al. Deepgauge: Multi-granularity testing criteria for deep learning systems. InProceedingsofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftware
Engineering, pages 120â€“131, 2018.
[48]L.Ma,F.Zhang,J.Sun,M.Xue,B.Li,F.Juefei-Xu,C.Xie,L.Li,Y.Liu,J.Zhao,et al. Deepmutation: Mutation testing of deep learning systems. In 2018 IEEE
29th International Symposium on Software Reliability Engineering (ISSRE), pages
100â€“111. IEEE, 2018.
[49]X.Ma,B.Li,Y.Wang,S.M.Erfani,S.Wijewickrema,G.Schoenebeck,D.Song,
M. E. Houle, and J. Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
[50]A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep
learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
[51]S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deepfool: a simple and accu-
rate method to fool deep neural networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 2574â€“2582, 2016.
[52]N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami.
Thelimitationsofdeeplearninginadversarialsettings. In 2016IEEEEuropean
symposium on security and privacy (EuroS&P), pages 372â€“387. IEEE, 2016.
[53]J. Park, D. Winterer, C. Zhang, and Z. Su. Generative type-aware mutationfor testing smt solvers. Proceedings of the ACM on Programming Languages,
5(OOPSLA):1â€“19, 2021.
[54]O. Parry, G. M. Kapfhammer, M. Hilton, and P. McMinn. A survey of flaky tests.
ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM),31(1):1â€“74,
2021.
[55]A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance
deeplearninglibrary. Advancesinneuralinformationprocessingsystems,32:8026â€“
8037, 2019.
[56]K. Pei, Y. Cao, J. Yang, and S. Jana. Deepxplore: Automated whitebox testingof deep learning systems. In proceedings of the 26th Symposium on Operating
Systems Principles, pages 1â€“18, 2017.
[57]H. V. Pham, T. Lutellier, W. Qi, and L. Tan. CRADLE: Cross-Backend Validation
toDetectandLocalize BugsinDeepLearningLibraries. In 2019IEEE/ACM41st
International Conference on Software Engineering (ICSE), pages 1027â€“1038, 2019.
[58]M.Pradel, P.Schuh, andK.Sen. Typedevil:Dynamic typeinconsistencyanalysis
for javascript. In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, volume 1, pages 314â€“324. IEEE, 2015.
[59]D. She, R. Krishna, L. Yan, S. Jana, and B. Ray. Mtfuzz: fuzzing with a multi-
taskneuralnetwork. In Proceedingsofthe28thACMJointMeetingonEuropean
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
1006
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. Free Lunch for Testing:
Fuzzing Deep-Learning Libraries from Open Source ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Engineering, pages 737â€“749, 2020.
[60]D. Shen, G. Wu, and H.-I. Suk. Deep learning in medical image analysis. Annual
review of biomedical engineering, 19:221â€“248, 2017.
[61]D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering
thegameofgowithdeepneuralnetworksandtreesearch. nature,529(7587):484â€“
489, 2016.
[62]K.SimonyanandA.Zisserman. Verydeepconvolutionalnetworksforlarge-scale
image recognition. arXiv preprint arXiv:1409.1556, 2014.
[63]T. Su, Y. Yan, J. Wang, J. Sun, Y. Xiong, G. Pu, K. Wang, and Z. Su. Fully auto-
mated functional fuzzing of android apps for detecting non-crashing logic bugs.
Proceedings of the ACM on Programming Languages, 5(OOPSLA):1â€“31, 2021.
[64]Y. Sun, D. Liang, X. Wang, and X. Tang. Deepid3: Face recognition with very
deep neural networks. arXiv preprint arXiv:1502.00873, 2015.
[65]Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and D. Kroening. Concolic
testingfordeepneuralnetworks. In Proceedingsofthe33rdACM/IEEEInterna-
tional Conference on Automated Software Engineering, pages 109â€“119, 2018.
[66]C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and
R.Fergus.Intriguingpropertiesofneuralnetworks. arXivpreprintarXiv:1312.6199,
2013.
[67]Y. Tian,K. Pei, S. Jana,and B. Ray. Deeptest: Automatedtesting of deep-neural-
network-driven autonomous cars. In Proceedings of the 40th international confer-
ence on software engineering, pages 303â€“314, 2018.
[68]J.Wang,G.Dong,J.Sun,X.Wang,andP.Zhang. Adversarialsampledetection
fordeepneuralnetworkthroughmodelmutationtesting. In 2019IEEE/ACM41st
InternationalConferenceonSoftwareEngineering(ICSE),pages1245â€“1256.IEEE,
2019.
[69]Z. Wang, M. Yan, J. Chen, S. Liu, and D. Zhang. Deep learning library testing
viaeffective modelgeneration. In Proceedingsof the28th ACMJointMeeting on
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering, pages 788â€“799, 2020.
[70]C.Wen,H.Wang,Y.Li,S.Qin,Y.Liu,Z.Xu,H.Chen,X.Xie,G.Pu,andT.Liu.
Memlock:Memoryusageguidedfuzzing. In ProceedingsoftheACM/IEEE42nd
International Conference on Software Engineering, pages 765â€“777, 2020.
[71]Y.Wu,M.Schuster,Z.Chen,Q.V.Le,M.Norouzi,W.Macherey,M.Krikun,Y.Cao,
Q.Gao,K.Macherey,etal. Googleâ€™sneuralmachinetranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprintarXiv:1609.08144,
2016.
[72]M.Yan,J.Chen,X.Zhang,L.Tan,G.Wang,andZ.Wang. Exposingnumerical
bugs in deep learning via gradient back-propagation. In Proceedings of the 29th
ACM Joint Meeting on European Software Engineering Conference and Symposiumon the Foundations of Software Engineering, pages 627â€“638, 2021.
[73]S. Yan, G. Tao, X. Liu, J. Zhai, S. Ma, L. Xu, and X. Zhang. Correlations between
deep neural network model coverage criteria and model quality. In Proceedings
of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pages 775â€“787, 2020.
[74]Y. Yang, X. Xia, D. Lo, and J. Grundy. A survey on deep learning for software
engineering. arXiv preprint arXiv:2011.14597, 2020.
[75]Z. Zeng, Y. Zhang, H. Zhang, and L. Zhang. Deep just-in-time defect prediction:
howfararewe? In Proceedingsofthe30thACMSIGSOFTInternationalSymposium
on Software Testing and Analysis, pages 427â€“438, 2021.
[76]L. Zhang. Hybrid regression test selection. In 2018 IEEE/ACM 40th International
Conference on Software Engineering (ICSE), pages 199â€“209, 2018.
[77]M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid. Deeproad: Gan-based
metamorphictestingandinputvalidationframeworkforautonomousdriving
systems. In 2018 33rd IEEE/ACM International Conference on Automated Software
Engineering (ASE), pages 132â€“142. IEEE, 2018.
[78]P.Zhang,Y.Jiang,A. Wei,V.Stodden,D.Marinov,and A.Shi. Domain-specific
fixes for flaky tests with wrong assumptions on underdetermined specifications.
In2021IEEE/ACM 43rdInternationalConference onSoftwareEngineering(ICSE),
pages 50â€“61. IEEE, 2021.
[79]Q.Zhang,Y.Ding,Y.Tian,J.Guo,M.Yuan,andY.Jiang. Advdoor:Adversarial
backdoor attack of deep learning system. 2021.
[80]X.Zhang,N.Sun,C.Fang,J.Liu,J.Liu,D.Chai,J.Wang,andZ.Chen. Predoo:
precision testing of deep learning operators. In Proceedings of the 30th ACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis,pages400â€“
412, 2021.
[81]Y.Zhang,L.Ren,L.Chen,Y.Xiong,S.-C.Cheung,andT.Xie. Detectingnumericalbugsinneuralnetworkarchitectures.In Proceedingsofthe28thACMJointMeeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering, pages 826â€“837, 2020.
[82]Z.Zhao,G.Chen,J.Wang,Y.Yang,F.Song,andJ.Sun. Attackasdefense:Charac-
terizing adversarialexamples using robustness. arXiv preprint arXiv:2103.07633,
2021.
[83]R. Zhong, Y. Chen, H. Hu, H. Zhang, W. Lee, and D. Wu. Squirrel: Testing
database managementsystems with language validity and coverage feedback. InProceedingsofthe2020ACMSIGSACConferenceonComputerandCommunications
Security, pages 955â€“970, 2020.
[84]H. Zhou, W. Li, Z. Kong, J. Guo, Y. Zhang, B. Yu, L. Zhang, and C. Liu. Deepbill-
board:Systematicphysical-worldtestingofautonomousdrivingsystems. In 2020
IEEE/ACM42ndInternationalConferenceonSoftwareEngineering(ICSE),pages
347â€“358. IEEE, 2020.
1007
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:20:26 UTC from IEEE Xplore.  Restrictions apply. 