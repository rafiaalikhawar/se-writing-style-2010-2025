Psychologically-Inspired,Unsupervised Inferenceof Perceptual
Groups of GUIWidgets from GUIImages
Mulong Xie
mulong.xie@anu.edu.au
AustralianNationalUniversity
AustraliaZhenchang Xing
zhenchang.xing@anu.edu.au
CSIROâ€™s Data61 & ANU
AustraliaSidong Feng
sidong.feng@monash.edu
MonashUniversity
Australia
XiweiXu
xiwei.xu@data61.csiro.au
CSIROâ€™s Data61
AustraliaLiming Zhu
liming.zhu@data61.csiro.au
CSIROâ€™s Data61 & UNSW
AustraliaChunyang Chen
chunyang.chen@monash.edu
MonashUniversity
Australia
ABSTRACT
Graphical User Interface (GUI) is not merely a collection of individ-
ual and unrelated widgets, but rather partitions discrete widgets
into groups by variousvisual cues, thus forming higher-order per-
ceptualunitssuchastab,menu,cardorlist.Theabilitytoautomat-
ically segment a GUI into perceptual groups of widgets constitutes
afundamentalcomponentofvisualintelligencetoautomateGUI
design,implementationandautomationtasks.Althoughhumans
canpartitionaGUIintomeaningfulperceptualgroupsofwidgetsin
ahighlyreliableway,perceptualgroupingisstillanopenchallenge
for computational approaches. Existing methods rely on ad-hoc
heuristics or supervised machine learning that is dependent on
specificGUIimplementationsandruntimeinformation.Research
inpsychologyandbiologicalvision hasformulatedasetofprinci-
ples(i.e.,Gestalttheoryofperception)thatdescribehowhumans
groupelementsinvisualscenesbasedonvisualcueslikeconnec-
tivity, similarity, proximity and continuity. These principles are
domain-independent and have beenwidely adopted bypractition-
ers to structure content on GUIs to improve aesthetic pleasantness
andusability.Inspiredbytheseprinciples,wepresentanovelun-
supervised image-based method for inferring perceptual groups of
GUIwidgets.OurmethodrequiresonlyGUIpixelimages,isinde-
pendent of GUI implementation, and does not require any training
data. The evaluation on a dataset of 1,091 GUIscollected from 772
mobile apps and 20 UI design mockups shows that our method sig-
nificantly outperforms the state-of-the-art ad-hoc heuristics-based
baseline. Our perceptual grouping method creates opportunities
forimprovingUI-relatedsoftware engineeringtasks.
CCSCONCEPTS
Â·Software and its engineering ;
KEYWORDS
GraphicalUserInterface,Widget Grouping,PerceptualGrouping
ESEC/FSE â€™22,November 14Å›18, 2022, Singapore, Singapore
Â©2022 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3549138ACM ReferenceFormat:
MulongXie,ZhenchangXing,SidongFeng,XiweiXu,LimingZhu,andChun-
yang Chen. 2022. Psychologically-Inspired, Unsupervised Inference of Per-
ceptual Groups of GUI Widgets from GUI Images. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering (ESEC/FSE â€™22), Novem-
ber14Å›18,2022,Singapore,Singapore. ACM,NewYork,NY,USA, 12pages.
https://doi.org/10.1145/3540250.3549138
1 INTRODUCTION
Wedonotjustseeacollectionofseparatedtexts,images,buttons,
etc.,onGUIs.Instead,weseeperceptualgroupsofGUIwidgets,such
ascard, list, tab and menu showninFigure 1. Formingperceptual
groupsisanessentialsteptowardsvisualintelligence.Forexample,
it helps us decide which actions are most applicable to certain
GUI parts, such as, clicking a navigation tab, expanding a card,
scroll the list. This would enable more efficient automatic GUI
testing [22,35]. As another example, screen readers [ 3,8] help
visually impaired users access applications by reading out content
onGUI.Recognizingperceptualgroupswouldallowscreenreaders
to navigate the GUI at higher-order perceptual units (e.g., sections)
efficiently[ 58].Lastbutnotleast,GUIrequirements,designsand
implementationsaremuchmorevolatilethanbusinesslogicand
functionalalgorithms.Withperceptualgrouping,modular,reusable
GUI code can be automaticallygeneratedfrom GUIdesign images,
whichwouldexpediterapidGUIprototypingandevolution[ 37,38].
Although humans can intuitively see perceptual groups of GUI
widgets, current computational approaches are limited in parti-
tioning a GUI into meaningful groups of widgets. Some recent
work [12,16] relies on supervised deep learning methods(e.g., im-
age captioning [ 34,51]) to generate a view hierarchy for a GUI
image.ThistypeofmethodisheavilydependentonGUIdataavail-
abilityandquality.ToobtainsufficientGUIdataformodeltraining,
they use GUI screenshots and view hierarchies obtained at applica-
tion runtime. A criticalquality issue of suchruntime GUI data is
that runtime view hierarchies often do not correspond to intuitive
perceptual groups due to many implementation-level tricks. For
example,intheleftGUIinFigure 2,thetwoListItemsinaListView
hasnovisualsimilarity(alargeimageversussometexts),sotheydo
not form a perceptual group. In the right GUI, a grid of cards form
a perceptual group but is implemented as individual FrameLayouts.
Such inconsistencies between the implemented widget groups and
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
332
ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore Mulong Xie, Zhenchang Xing, SidongFeng,XiweiXu,Liming Zhu, Chunyang Chen
(a) Card (b) List (c) List & Multi-tab (d) MenuListMulti-tab
ListMenu
Card
Card
Figure1:ExamplesofperceptualgroupsofGUIwidgets(per-
ceptualgroupsare highlighted inpinkbox inthispaper)
the humanâ€™s perceptual groups makethe trainedmodelsunreliable
to detectperceptualgroupsofGUI widgets.
Decadesofpsychologyandbiologicalvisionresearchhavefor-
mulatedtheGestalttheoryofperceptionthatexplainshowhumans
seethewholeratherthanindividualandunrelatedparts.Itincludes
asetofprinciplesofgrouping,amongwhich connectedness ,simi-
larity,proximity andcontinuity are the mostessentialones [ 7,45].
Although these principles and other related UI design principles
such as CRAP [ 42] greatly influence how designers and developers
structure GUI widgets [ 48], they have never been systematically
used to automatically infer perceptual groups from GUI images.
Rather,currentapproaches[ 38,58]relyonad-hocandcase-specific
rulesandthus are hardto generalize ondiverseGUI designs.
In this work, we systematically explore the Gestalt principles of
grouping and design the first psychologically-inspired method for
visual inference of perceptual groups of GUI widgets. Our method
requiresonlyGUIpixelimagesandisindependentofGUIimple-
mentations.Ourmethodisunsupervised,thusremovingthedepen-
dence on problematic GUI runtime data. As shwon in Figure 3, our
methodenhancesthestate-of-the-artGUIwidgetdetectionmethod
(UIED [21,53]) to detect elementary GUI widgets. Following the
Gestaltprinciples,themethodfirstdetectscontainers(e.g.,card,list
item)withcomplexwidgetsbytheconnectednessprinciple.Itthen
clustersvisuallysimilartexts(ornon-textwidgets)bythesimilarity
principleandfurthergroupsclustersofwidgetsbytheproximity
principles. Finally, based on the widget clusters, our method cor-
rectserroneouslydetectedormissingGUIwidgetsbythecontinuity
principle(notillustratedinFigure 3,but can be seen inFigure 5).
At the right end of Figure 3, we show the grouping result by the
state-of-the-art heuristic-based method Screen Recognition [ 58].
ScreenRecognitionincorrectlypartitionsmanywidgetsintogroups,
such as the bottom navigation bar andthe four widgetsabove the
bar, thecardon theleftand thetextabove thecard. Italsofails to
detecthigher-order perceptualgroups,suchasgroupsofcards.In
contrast, our approach correctly recognizes the bottom navigation
bar and the top and middle row of cards. Although the text label
above the left card is very close to the card, our approach correctly
recognizes the text labels as separate widgets rather than as a part
of the left card. Our approach does not recognize the two cards
justabovethebottomnavigationbarbecausethesetwocardsare
partiallyoccludedbythebottombar.However,itcorrectlyrecog-
nizesthetwoblocksofimageandtextanddetectsthemasagroup.
Figure2:Implementedviewhierarchydoesnotnecessarily
correspond to perceptual groups
Clearly, the grouping results by our approach correspond more
intuitively to human perception than those by Screen Recognition.
Fortheevaluation,weconstructtwodatasets:onecontains1,091
GUI screenshots from 772 Android apps, and the other contains
20UIprototypesfromapopulardesigntoolFigma[ 4].Toensure
the validity of ground-truth widget groups, we manually check all
theseGUIsandconfirmthatnoneoftheseGUIshastheperception-
implementation misalignment issues shown in Figure 2. We first
examine our enhanced version of UIED and observe that the en-
hancedversion reachesa0.626F1scoreforGUIwidget detection,
whichismuchhigherthantheoriginalversion(0.524F1).Withthe
detected GUI widgets,our perceptualgrouping approach achieves
theF1-scoreof0.593onthe1,091appGUIscreenshotsand0.783
F1-score on the 20 UI design prototypes. To understand the impact
of GUI widget misdetections on perceptual grouping, we extract
the GUI widgets directly from the Android appâ€™s runtime metadata
(i.e.,ground-truthwidgets)andusetheground-truthwidgetsasthe
inputs to perceptual grouping. With such Å‚perfectly-detectedÅ¾ GUI
widgets,ourgroupingapproachachievesa0.672F1-scoreonapp
GUIs. In contrast, Screen Recognition [ 58] performs very poorly:
0.123 F1 on the ground-truth widgets and 0.092 F1 on the detected
widgetsfor app screenshots, and0.232 F1 onthe detectedwidgets
forUIdesignprototypes.Althoughourgroupingresultssometimes
do not exactlymatch the ground-truth groups, ouranalysis shows
thatsomeofourgroupingresultsstillcomplywithhowhumans
perceivethewidgetgroupsbecausetherecanbediversewaysto
structure GUI widgetsinsomecases.
Tosummarize,this paper makes the following contributions:
â€¢Arobust,psychologically-inspired,unsupervisedvisualin-
ference method for detecting perceptual groups of GUI wid-
getsonGUI images, the code isreleasedonGitHub1.
â€¢A comprehensive evaluation ofthe proposedapproach and
the in-depthanalysisof the performance withexamples.
â€¢Ananalysis of howourperceptual groupingmethodcan im-
proveUI-relatedSEtasks,suchasUIdesign,implementation
andautomation.
1https://github.com/MulongXie/GUI-Perceptual-Grouping
333Psychologically-Inspired, UnsupervisedInference of Perceptual Groups of GUI Widgets from GUI Images ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Visualization of Each Step of Our Approach ScreenRecognition
Input GUI Image (2.1) Connectedness 
Container Recognition(2.2) Similarity  
 Widget Clustering(2.3) Proximity  
 Recursive Group PairingWidget Perceptual Groups (1) Widget Detection(2) Perceptual Grouping
Grouping Result
Figure 3: Left: Our approach overview: (1) Enhanced UIED [ 21] for GUI widget detection; (2) Gestalt-principles inspired
perceptualgrouping. Right:Grouping resultofthestateofoftheartheuristic-based approachScreenRecognition [ 58]
2 GUI WIDGETDETECTION
Ourapproachisapixel-onlyapproach.ItdoesnotassumeanyGUI
metadataorGUIimplementationaboutGUIwidgets.Instead,our
approach detects text and non-text GUI widgets directly from GUI
images. To obtain the widget information from pixels, it enhances
thestate-of-the-artGUIwidgetdetectiontechniqueUIED[ 53].In
order to fit with subsequent perceptual grouping, our approach
mitigatestheincorrectmergingofGUIwidgetsinthecontainers
byUIED andsimplifies the widgetclassification ofUIED.
2.1 UIED-BasedGUI WidgetDetection
UIED comprises three steps: (1) GUI text detection, (2) non-text
GUI widget detection and (3) merging of text and non-text wid-
gets.UIEDusesanoff-the-shelfscenetextdetectorEAST[ 60]to
identify text regionsin theGUI images.EAST isdesigned for han-
dling nature scene images that differ from GUI images, such as
figure-backgroundcomplexityandlightingeffects.AlthoughEAST
outperforms traditional OCR tool Tesseract [ 47], we find the latest
OCRtooldevelopedbyGoogle[ 6]achievesthehighestaccuracy
ofGUItextrecognition(seeSection 4.1).Therefore,inouruseof
UIED,we replace EASTwiththe Google OCRtool.
Forlocatingnon-textwidgets,ourapproachadoptsthedesign
ofUIEDthatusesaseriesoftraditional,unsupervisedimagepro-
cessingalgorithmsratherthandeep-learningmodels(e.g.,Faster
RCNN[31]orYOLO[ 40]).Thisdesignremovesthedatadependence
onGUIimplementationorruntimeinformationwhileaccurately
detecting GUI widgets. UIED then merges the text and non-text
detectionresults.Thepurposeofthismergingstepisnotonlyto
integrate the identified GUI widgets but also to cross-check the
results.Becausenon-textwidgetdetectioninevitablyextractssome
text regions, UIED counts on the OCR results to remove these
false-positive non-text widgets. Specifically, this step checks the
boundingboxofallcandidatenon-textwidgetregionsandremoves
thoseintersectedwithtextregionsresultingfrom the OCR.
2.2 ImprovementandSimplification ofUIED
WefindthattheUIEDdetectionresultsoftenmisssomewidgetsina
container(e.g.card).Thereasonisthat,inordertofilteroutinvalidnon-text regions and mitigate over-segmentation that wrongly
segmentsaGUIwidgetintoseveralparts,UIEDchecksthewidgetsâ€™
boundingboxesandmergesall intersectedwidgets regionsintoa
big widget region. This operation may cause the wrong neglection
ofvalidGUIwidgetsthatareenclosedinsomecontainers.Therefore,
we equip our GUI widget detector with a container recognition
algorithm (see Section 3.2) to mitigate the issue. If a widget is
recognizedasacontainer,thenallitscontainedwidgetsarekept
andregardedas proper GUI widgetsratherthannoises.
The original UIED classifies non-text GUI widgets as specific
widgetcategories(e.g.,image,button,checkbox).Incontrast,our
GUI widget detector only distinguishes text from non-text widgets.
Although GUI involves many types of non-text widgets, there is
no need of distinguishing actual classes of non-text widgets for
perceptualgrouping. GUIwidgetclassesindicatedifferentuserin-
teractionsandGUIfunctionalities,butwidgetswithdifferentclasses
can form a perceptual group as long as they have similar visual
properties,suchassize,shape,relativepositionandalignmentwith
other widgets. Therefore, we do not distinguish different classes of
non-textwidgets.However,weneedtodistinguishnon-textwid-
getsfromtextwidgets,astheyhaveverydifferentvisualproperties
andneedto be clusteredbydifferentstrategies (see Section 3).
3 GUI WIDGETPERCEPTUALGROUPING
Afterobtainingtextandnon-textwidgetsonaGUIimage,thenext
stepistopartitionGUIwidgetsintoperceptualgroups(orblocks
ofitems) according to their visual andperceptualproperties.
3.1 Gestalt Laws andApproach Overview
Ourapproachisinspiredbypsychologyandbiological-visionre-
search. Perceptual grouping is a cognitive process in which our
minds leap from comprehending all of the objects as individuals
to recognizing visual patterns through grouping visually related
elementsasawhole[ 26].Thisprocessaffectsthewaywedesign
GUIlayouts[ 42]fromalignment,spacingandgroupingtoolsup-
port [4,44] to UI design templates [ 24] and GUI frameworks [ 2]
ItalsoexplainshowweperceiveGUIlayouts.Forinstance,inthe
examplesinFigure 1,wesubconsciouslyobservethatsomevisually
334ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore Mulong Xie, Zhenchang Xing, SidongFeng,XiweiXu,Liming Zhu, Chunyang Chen
similarwidgetsareplacedinaspatiallysimilarwayandidentify
themas inagroup (e.g.acard,list,multitabormenu).
Previousstudiesrelyonad-hoc,rigidheuristicstoinferUIstruc-
turewithout asystematic theoreticalfoundation. Ourapproachis
the first attempt to tackle the perceptual grouping of GUI widgets
guided by an influential psychological theory (named Gestalt psy-
chology[ 7])that explainshow thehumanbrainperceives objects
and patterns. Gestalt psychologyâ€™s core proposition is that human
understands external stimuli aswholes rather than as the sums of
theirparts[ 46].Basedontheproposition,theGestaltistsstudied
perceptualgrouping[ 26]systematicallyandsummarizedasetof
Å‚gestaltlawsofgroupingÅ¾[ 45].Inourwork,weadoptthefourmost
effectiveprincipleswhichgreatlyinfluenceGUIdesign[ 48]inprac-
tice as the guideline for our approach design: (1) connectedness
(2) similarity(3) proximityand(4) continuity.
We define a group of related GUI widgets as a layout block of
items.Atypicalexampleisa listoflistitems intheGUI,ora card
displaying an image and some texts. The fundamental intuition
is:if a setofwidgetshave similar visual propertiesandare placed
inalignmentwithsimilarspacebetweeneachother,theywillbe
Å‚perceivedÅ¾ as in the same layout block by our approach according
to the Gestalt principles. In detail, our approach consists of four
groupingsteps inaccordancewithfour Gestalt principles.First, it
identifiescontainersalongwiththeircontainedwidgetsthatfulfil
the connectedness law. Second, it uses an unsupervised clustering
methodDBSCAN[ 25]toclustertextornon-textGUIwidgetsbased
on their spatial and size similarities. Next, it groups proximate and
spatially aligned clusters to form a larger layout block following
the proximity law. Finally, in line with the continuity principle,
ourapproachcorrectssomemistakesofGUIwidgetdetectionby
checking the consistency ofthe groupsâ€™ compositions.
3.2 Connectedness - Container Recognition
In Gestalt psychology, the principle of uniform connectedness is
thestrongestprincipleconcernedwithrelatedness[ 41].Itimplies
that we perceive elements connected by uniform visual properties
as beingmorerelatedthanthosenotconnected. Theforms ofthe
connection can be either a line connecting several elements or a
shapeboundarythatenclosesagroupofrelatedelements.Inthe
GUI, the presentation of the connectedness is usually a box con-
tainerthatcontainsmultiplewidgetswithinit,andalltheenclosed
widgetsare perceivedasinthesamegroup. Thus, thefirststep of
our grouping approach isto recognizethe containersinaGUI.
In particular, we observe that a container is visually a (round)
rectangularwireframeenclosingseveralchildrenwidgets.Thecard
is a typical example of such containers, as shown in Figure 1(a).
Therefore,withthedetectednon-textwidgets,thealgorithmfirst
checks if a widget is of rectangle shape by counting how many
straightlinesitsboundarycomprisesandhowtheycompose.Specif-
ically, we apply the geometrical rule that a rectangleâ€™s sides are
made of 4 straight lines perpendicular to each other. Subsequently,
ourapproachdeterminesifthewidgetâ€™sboundaryisawireframe
border by checking if it is connected with any widgets inside its
boundary.Ifawidgetsatisfiestheabovecriteria,itwillbeidentified
asacontainer,andallwidgetscontainedwithinitarepartitioned
intothe same perceptualgroup.
ClusterNon-text
AreaConflicts Resolving  ClusterNon-text
Horizontal ClusterNon-text
Vertical
 ClusterText
Horizontal  ClusterText
Vertical Conflicts ResolvingClustering Result Final Goups
Figure 4: Widget clustering, cluster conflict resolving and
finalresultinggroupsinwhichweusethesamecolortopaint
thewidgetsinthesamesubgroupandhighlighthigher-order
groupsinpinkboxes
3.3 Similarity - WidgetClustering
The principle of similarity suggests that elements are perceptually
grouped together if they are similar to each other [ 9]. Generally,
similaritycanbeobservedinaspectsofvariousvisualcues,such
as size, color, shape or position. For example, in the second GUI of
Figure1,theimagewidgetsareofthesamesizeandalignedwith
each other in the same way (i.e., same direction and spacing), so
we visually perceive them as a group.Likewise,the text pieceson
therightoftheimagewidgetsareperceptuallysimilareventhough
theyhavedifferentfontstylesandlengthsbecausetheyhavethe
same alignmentwithneighbouringtexts.
3.3.1 Initial Widget Clustering. Our approach identifies similar
GUIwidgetsbytheirspatialandvisualpropertiesandaggregates
similarGUIwidgetsintoblocksbysimilarity-basedclustering.It
clusters texts and non-text widgets through different strategies.
Ingeneral,similarnon-textwidgetsinthesameblock(e.g.a list)
usually have similar sizes and align to one another vertically or
horizontally with the same spacing. Texts in the same block are
alwaysleft-justifiedortop-justified(assumeleft-to-righttextorien-
tation), but their sizes and shapes can vary significantly because
of different lengths of text contents. Thus, the approach clusters
thenon-textwidgetsbytheircenterpoints (ð¶ð‘’ð‘›ð‘¡ð‘’ð‘Ÿð‘‹,ð¶ð‘’ð‘›ð‘¡ð‘’ð‘Ÿ ð‘Œ)and
areas, anditclusterstextsbytheir top-left corner (ð‘‡ð‘œð‘,ð¿ð‘’ð‘“ð‘¡ )
Our approach uses the DBSCAN (Density-Based Spatial Cluster-
ing of Applications with Noise) algorithm [ 25] to implement the
clustering.Intuitively,DBSCANgroupsthepointscloselypackedto-
gether (points with many nearby neighbors), while marking points
whose distance from the nearest neighbor is greater than the maxi-
mumthresholdasoutliers.IntheGUIwidgetclusteringcontext,the
point is the GUI widget, and the distance is the difference between
thevaluesofthewidgetsâ€™attributethattheclusteringisbasedon.
Figure4illustrates the clustering process. For non-text widgets,
ourapproachperformstheclusteringthreetimesbasedonthree
attributesrespectively.Itfirstclustersthewidgetsby ð¶ð‘’ð‘›ð‘¡ð‘’ð‘Ÿð‘¥for
the horizontal alignment, then by ð¶ð‘’ð‘›ð‘¡ð‘’ð‘Ÿð‘Œfor the vertical align-
ment and finally by ð‘Žð‘Ÿð‘’ð‘Ž. These operations produce three clus-
ters:ð¶ð‘™ð‘¢ð‘ ð‘¡ð‘’ð‘Ÿð‘›ð‘œð‘›âˆ’ð‘¡ð‘’ð‘¥ð‘¡
â„Žð‘œð‘Ÿð‘–ð‘§ð‘œð‘›ð‘¡ð‘Žð‘™,ð¶ð‘™ð‘¢ð‘ ð‘¡ð‘’ð‘Ÿð‘›ð‘œð‘›âˆ’ð‘¡ð‘’ð‘¥ð‘¡
ð‘£ð‘’ð‘Ÿð‘¡ð‘–ð‘ð‘Žð‘™andð¶ð‘™ð‘¢ð‘ ð‘¡ð‘’ð‘Ÿð‘›ð‘œð‘›âˆ’ð‘¡ð‘’ð‘¥ð‘¡ð‘Žð‘Ÿð‘’ð‘Ž. Our
approachthenclustersthetextwidgetstwicebasedontheirtopleft
335Psychologically-Inspired, UnsupervisedInference of Perceptual Groups of GUI Widgets from GUI Images ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
(b) Misclassified Widget(a) Missed Widget
Widget Detection Group Pairing Subgroup of Widgets
Missed Widget
Misclassified 
 Widget
Figure 5: Examples of widget detection error correction. (1st
column - green box: non-text; red-box: text; 2nd column
- same color: higher-order perceptual group; 3rd column -
samecolor:subgroup ofwidgets)
cornerpoint (ð‘‡ð‘œð‘,ð¿ð‘’ð‘“ð‘¡ )forleft-justified(vertical)andtop-justified
(horizontal)alignment.It produces the ð¶ð‘™ð‘¢ð‘ ð‘¡ð‘’ð‘Ÿð‘¡ð‘’ð‘¥ð‘¡
â„Žð‘œð‘Ÿð‘–ð‘§ð‘œð‘›ð‘¡ð‘Žð‘™based on
thetextsâ€™ ð‘‡ð‘œð‘,andtheð¶ð‘™ð‘¢ð‘ ð‘¡ð‘’ð‘Ÿð‘¡ð‘’ð‘¥ð‘¡
ð‘£ð‘’ð‘Ÿð‘¡ð‘–ð‘ð‘Žð‘™basedonthetextsâ€™ ð¿ð‘’ð‘“ð‘¡.The
resultingclustersarehighlightedbydifferentcolorsandnumbers
inFigure 4.Weonlykeeptheclusterswithatleasttwowidgetsand
discardthosewithonly one widget.
3.3.2 Cluster Conflicts Resolving. It is common that some widgets
canbeclusteredintodifferentclustersbydifferentattributes,which
causes cluster conflicts. For example, as illustrated in Figure 4,
several non-text widgets (e.g., the bottom-left image) are both in a
verticalcluster(markedinblue)andahorizontalcluster(marked
in red). The intersection of clusters illustrates the conflict. The
approach shall resolve such cluster conflicts to determine to which
groupthewidgetbelongs.Thisconflict-resolvingstepalsocomplies
withthe similarityprinciple thatsuggests the widgetsin the same
perceptualgroup should share more similar properties.
The conflict resolving step first calculates the average widget
areas of the groups to which the conflicting widget has been as-
signed. In accordance with the similarity principle, the widget is
morelikelytobeinagroupwhoseaveragewidgetareaissimilar
to the conflicting widgetâ€™sarea. In addition,anotherobservation
isthatrepetitive widgets ina grouphave similarspacingbetween
each other. So for a widget that is clustered into multiple candidate
groups, the approach checks the difference between the spacing of
thiswidgetanditsneighboringwidgetsinagroupandtheaverage
spacingbetweenotherwidgetsinthatgroup.Itkeepsthewidgetin
the group where the conflicting widget has the largest widget-area
similarity and the smallest spacing difference compared with other
widgets inthe group.Forexample,the bottom-leftimagewidget
willbeassignedtotheverticalclusterratherthanthehorizontalone
according to our conflict resolving criteria. After conflict resolving,
ourapproachproducesthefinalwidgetclusteringresultsasshown
in the right part of Figure 4. We use different colors and indices to
illustrate the resultingnon-text(nt) andtext(t) clusters.3.4 Proximity - IterativeGroupPairing
Sofar,GUIwidgetsareaggregatedintogroupsaspertheconnected-
ness and similarity principles. Some groups are close to each other
and similar in terms of the number and layout of the contained
widgets, which may furtherform a larger perceptual group even
though these groups may contain different types of widgets. For
example,intheclusteringresultofFigure 4,wecanobservethat
the clusters nt-0,t-0,t-0-1andnt-2are proximate and have the
same or similar number of widgets aligned in the same way. We
canseethisfeatureintentionallyorsubconsciouslyandperceive
them as in the same large group as a whole. Gestalt psychology
states thatwhen peoplesee anassortment ofobjects,theytend to
perceive objects that are close to each other as a group [ 9]. The
close distance, also known as proximity, of elements is so powerful
thatit canoverridewidget similarityand otherfactorsthat might
differentiateagroupofobjects[ 50].Thus,thenextstepisbasedon
widgetclustersâ€™proximityandcompositionsimilaritytopairthe
clustersintoalarger group (i.e.,layoutblock).
If two groups ðºð‘Ÿð‘œð‘¢ð‘ð‘Žandðºð‘Ÿð‘œð‘¢ð‘ð‘are next (proximate) to each
other (i.e., no other groups in between), and they contain the same
number of widgets and the widgets in the ðºð‘Ÿð‘œð‘¢ð‘ð‘Žand theðºð‘Ÿð‘œð‘¢ð‘ð‘
share the same orientation (vertical or horizontal), our approach
combines ðºð‘Ÿð‘œð‘¢ð‘ð‘Žandðºð‘Ÿð‘œð‘¢ð‘ð‘into a larger block. A widget in
ðºð‘Ÿð‘œð‘¢ð‘ð‘Žanditsclosetwidgetin ðºð‘Ÿð‘œð‘¢ð‘ð‘willbepairedandforma
subgroup of widgets. Our approach first combines the two prox-
imate groups containing the same type of widgets, and then the
groups containing different types of widgets. The formed larger
block can be iteratively combined with the proximate groups until
nomore proximate groups are available.
Sometimes there are different numbers of widgets in the two
proximate groups but the two groups may still form one larger
perceptualblock. For example,thecluster nt-2inFigure 4has one
less widget compared to nt-0,t-0andt-0-1because the bottom
widgetintherightcolumnisoccludedbythefloatactionbutton
andthusmissedbythedetector.Anothercommonreasonforthe
widget number difference is that widgets in a group may be set as
invisibleinsomesituations,andthustheydonotappearvisually.
Therefore,ifthedifferencebetweenthenumberofwidgetsinthe
two proximate groups is less than 4 (empiricallydetermined from
theground-truthgroupsinourdataset),ourapproachalsocombines
the twogroupsintoalarger block.
AsshowninthefinalgroupsinFigure 4,ourapproachidentifies
a set of perceptual groups (blocks), including the multitab at the
top and the list in the main area. Each list item is a combined
widget of some non-text and text widgets (highlighted in the same
color).Theseperceptualgroupsencodethecomprehensionofthe
GUIstructureintohigher-orderlayoutblocksthatcanbeusedin
further processing andapplications.
3.5 Continuity - DetectionError Correction
The GUI widget detector may make two types of detection errors -
missedwidgetsandmisclassifiedwidgets.Missedwidgetsmeans
thatthedetectorfailstodetectsomeGUIelementsontheGUI(e.g.,
the bottom-right icon in Figure 5(a)). Misclassified widgets refer to
the widgets that the detector reports the wrong type, for example,
336ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore Mulong Xie, Zhenchang Xing, SidongFeng,XiweiXu,Liming Zhu, Chunyang Chen
Detection-based Grouping for Screenshot Metadata-based Grouping for Screenshot Detection-based Grouping for Desgin
Group
GroupGroupGroup
Group
Group
Group
Group
Group
Group
GroupGroup
GroupGroup
GroupInput GUI Detection Groups Input GUI Metadata Groups Input GUI Detection Groups
Figure 6: Examples of GUI widget detection and perceptual grouping results (red box - text widget, green box - non-text widget,
pinkbox -perceptualgroup).Metadata-based meansgrouping theground-truthwidgets directlyfromGUImetadata.
the top-right small icon (i.e., a non-text widget) in the middle card
in Figure 5(b) is misclassified as a text widget due to an OCR error.
Itishardtorecognizeandcorrectthesedetectionerrorsfromthe
individual widget perspective, but applying the Gestalt continuity
principle to expose such widget detection errors by contrasting
widgets in the same perceptual groups can mitigate the issue. The
continuityprinciplestatesthatelementsarrangedinalineorcurve
areperceivedtobemorerelatedthanelementsnotinalineorcurve
[41].Thus,somedetectionerrorsarelikelytobespottedifaGUI
areaorawidgetalignswithallthewidgetsinaperceptualgroup
inalinebut isnot gatheredintothat group.
Our approach tries to identify and fix missed widgets as follows.
It first inspects the subgroups of widgets in a perceptual group and
checksifthewidgetsinthesubgroupsareconsistentintermsofthe
numberandrelativepositionofthecontainedwidgets.Ifasubgroup
contains fewer widgets than its counterparts, then the approach
locates the inconsistent regions by checking the relative positions
andareasofothersubgroupsâ€™widgets.Next,theapproachcropsthe
located UI regions and uses the widget detector upon the cropped
regionswithrelaxedparameters(i.e.doubleoftheminimumarea
threshold for valid widgets) to try to identify the missed widget, if
any. For example, the tiny icon at the bottom right in Figure 5(a)
is missed because its area is so small that the detector regards it
asanoisyregionandhencediscardsitintheinitialdetection.By
analyzingtheresultingperceptualgroupanditscomposition,our
approach finds that seven of the eight subgroups have two widgets(marked in the same color), while the subgroup at the bottom right
has only one widget. It crops the area that may contain the missed
widgetaccordingtotheaveragesizesandaveragerelativepositions
ofthetwowidgetsintheotherseven subgroups.Themissedtiny
icon can be recovered by detecting the widget with the relaxed
valid-widget minimumarea threshold inthe missingarea.
Ourapproachusestheexactmechanismthatcontraststhesub-
groups to identifythe misclassifiedwidgets, but here itfocuses on
widget type consistency. As shown in Figure 5(b), our approach
groups the three cards in a perceptual group. By contrasting the
widgetsinthethreecards,itdetectsthatthemiddlecardhastext
widgets at the top right corner, while the other two cards have a
non-text widget at thesame relative positions. Based on theconti-
nuityprinciple,ourapproachre-classifiesthetop-rightwidgetin
the middlecardas non-textwithamajority-winstrategy.
4 EVALUATION
Weevaluateourapproachintwosteps:(1)examinetheaccuracy
of our enhanced version of UIED and compare it with the orig-
inal UIED [ 21]; (2) examine the accuracy of our widget percep-
tual grouping approach and compare it with the state-of-the-art
heuristic-basedmethodScreen Recognition [ 58].
4.1 AccuracyofGUI WidgetDetection
Compared with the original UIED [ 21], our GUI widget detector
uses the latest Google OCR tool and improve the text and non-text
337Psychologically-Inspired, UnsupervisedInference of Perceptual Groups of GUI Widgets from GUI Images ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Table 1:Overallresults ofwidgetdetection (IoU >0.9)
Our EnhancedRevision OriginalUIED
Type Precision Recall F1 Precision Recall F1
Non-Text 0.589 0.823 0.687 0.431 0.469 0.449
Text 0.678 0.6930.686 0.402 0.7200.516
AllWidgets 0.580 0.680 0.626 0.490 0.557 0.524
widget merging by container analysis. We evaluate GUI widget
detectionfromthethreeperspectives:textwidgetdetection,non-
text widget detection and the final widget results after merging. To
beconsistentwiththeevaluationsettingintheUIEDpaper[ 21],we
runexperimentsonthesameRicodatasetofAndroidappGUIs[ 36]
and regard the detected widgets whose intersection over union
(IoU) with the ground truth widget is over 0.9 as true positive. The
ground-truth widgets are the leaf widgets (i.e., non-layout classes)
extractedfrom the GUIâ€™s runtimeviewhierarchy.
Table1showsthewidgetdetectionperformanceoftheenhanced
and the original UIED. Our enhanced version achieves a much
higher recall (0.823) for non-text widgets than the original UIED
(0.469),andmeanwhile,italsoimprovestheprecision(0.589over
0.431). This significant improvement is due to the more intelligent
container-awaremergingoftextandnon-textwidgetsbyouren-
hanced version. As the original UIED is container-agnostic, it erro-
neously discards many valid widgets contained in other widgets as
noise. For GUI text, the Google OCR tool used in the enhanced ver-
sion achieves muchhigher precision (0.678)thanthe EASTmodel
used in the original UIED (0.402), with a slight decrease in recall
(0.693versus0.720).Theimprovementsinbothtextandnon-text
widgets result in a much better overall performance (0.626 F1 by
the enhancedversionversus0.524 bythe originalUIED).
4.2 Perceptual Grouping Performance
We evaluate our perceptual grouping approach on both Android
appGUIsandUIdesignprototypes.Figure 6showssomeperceptual
groupingresultsbyourapproach.Theseresultsshowourapproach
can reliably detect GUI widgets and infer perceptual groups for
diversevisual andlayoutdesigns.
4.2.1 Datasets. Our approach simulates how humans perceive the
GUI structure and segment a GUI into blocks of widgets according
to the Gestalt principles of grouping. To validate the recognized
blocks, we build the ground-truth dataset from two sources: An-
droidappsandUIdesignprototypes.Thegroundtruthannotates
thewidgetgroupsaccordingtotheGUIlayoutandthewidgetstyles
andpropertiesas showninFigure 7.
Android App GUI Dataset The ground truth of widget groups
canbeobtainedbyexaminingthelayoutclassesusedtogroupother
widgetsintheimplementations.However,asshowninFigure 2,the
layoutclassesdonotalwayscorrespondtotheperceptualgroups
of GUI widgets. Therefore, we cannot use the GUI layout classes
directly as the ground truth. Instead, we first search the GUIs in
the Rico dataset of Android app GUIs [ 36] that use certain Android
layoutclassesthatmaycontainagroupofwidgets(e.g.,ListView,
FrameLayout,Card,TabLayout).Thenwemanuallyexaminethe
candidateGUIstofilteroutthosewhoseuseoflayoutclasseshas
obvious violations against the Gestalt principles. Furthermore, the
Ricodatasetcontainsmanyhighly-similarGUIscreenshotsforan
[([Text], [Text], [Text]), 
([Non-text, Text, Non-text , Text],  
[Non-text , Text, Non-text , Text], 
[Non-text , Text, Non-text , Text], 
[Non-text , Text, Non-text , Text], 
[Non-text , Text, Non-text , Text], 
[Non-text , Text, Non-text , Text], 
[Non-text , Text, Non-text , Text])] [([Non-text, Text], 
 [Non-text, Text],  
[Non-text, Text], 
[Non-text, Text], 
[Non-text, Text], 
[Non-text, Text], 
[Non-text, Text], 
[Non-text, Text])] 
  GUI Annotated with Widget Group
GUI
Group Group
Item
ImageView
ImageView
TextViewTextViewItem Item
...Item
TextViewItem
TextViewItem
TextView ImageView
ImageView
TextViewTextViewImageView
ImageView
TextViewTextViewView Hierarchy Ground TruthAndroid App GUI UI Design Prototype
GUI
Group
Item
Frame
TextItem Item
... Frame
TextFrame
TextItem
Frame
Text
Figure 7: Examples of Android app GUI and UI design proto-
type,view hierarchyand groundtruth
application. To increase the visual and layout diversity of GUIs
in our dataset, we limit up to three distinct GUI screenshots per
application.Distinctionisdeterminedbythenumberandtypeof
GUIwidgetsandtheGUIstructure.Weobtain1091GUIscreenshots
from772Androidapplications.Usingthisdataset,weevaluateboth
detection-based and metadata-based grouping. Detection-based
grouping processes the detected widgets, while metadata-based
grouping uses the widgets obtained from the GUI metadata (i.e.,
assumesthe perfectwidgetdetection).
UI Design Prototypes We collect 20 UI design prototypes
sharedonapopularUIdesignwebsite(Figma[ 4]).TheseUIdesign
prototypesarecreated byprofessionaldesigners forvariouskinds
ofappsandreceivemorethan200likes.This smallsetofUIdesign
prototypesdemonstrateshow professionaldesignersstructure the
GUIsandgroupGUIwidgetsfromthedesignratherthantheim-
plementation perspective. As a domain-independent tool, Figma
supports only elementary visual elements (i.e., text, image and
shape).Designerscancreateanywidgetsusingtheseelementary
visualelements.Duetothelackofexplicitanduniformwidgetmeta-
data in the Figma designs, we evaluate only the detection-based
grouping ontheseUIdesignprototypes.
4.2.2 Metrics. TheleftpartofFigure 7showsanexampleinourAn-
droidappGUIdataset.Weseethatthelayoutclasses(e.g.,ListView,
TabLayout)intheviewhierarchymaptothecorrespondingpercep-
tualgroups.Inourdataset,specificlayoutclassesaregeneralized
338ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore Mulong Xie, Zhenchang Xing, SidongFeng,XiweiXu,Liming Zhu, Chunyang Chen
Figure 8:Performanceat differenteditdistance thresholds
toblocks,asweonlycareaboutgenericperceptualgroupsinthis
work.Followingthework[ 16]forgeneratingGUIcomponenthi-
erarchy from UI image, we adopt the sequence representation of
aGUIcomponenthierarchy. Through depth-firsttraversal,aview
hierarchy can be transformed into a string of GUI widget names
andbrackets(Å‚[]Å¾andÅ‚()Å¾correspondingtotheblocks).Thisstring
representstheground-truthperceptualgroupsofanappGUI.As
discussedinSection 2.1,perceptualgroupingisbasedonthewid-
getsâ€™ positional and visual properties while the actual classes of
non-textwidgets are not necessary.Thus, the ground-truth string
onlyhastwowidgettypes:TextandNon-text.Specifically,itcon-
vertsTextViewintheviewhierarchytotextandallotherclasses
asNon-text.Similarly,asshownintherightpartofFigure 7,the
designers organize texts and non-text widgets (images or shape
compositions referred to as frames) into a hierarchy of groups.
Basedonthedesignâ€™sgrouphierarchy,weoutputtheground-truth
string of perceptual groups. The perceptual groups Å‚perceivedÅ¾ by
our approach are outputinthe same format for comparison.
We compute the Levenshtein edit distance between the two
strings ofa ground-truthblockand aperceivedblock.TheLeven-
shtein edits inform us of the specific mismatches between the two
blocks, which is important to understand and analyze grouping
mistakes.Iftheeditdistancebetweentheground-truthblockand
the perceived block is less than a threshold, we regard the two
blocksasacandidatematch.Wedeterminetheoptimalmatching
between the string of ground-truth blocks and the string of the
perceived blocks by minimizing the overall edit distance among all
candidate matches. If a perceived group matches a ground-truth
group, it is a true positive (TP), otherwise a false positive (FP). If
aground-truthgroupdoesnotmatchanyperceivedgroup,itisa
false negative (FN). Based on the matching results, we compute:
(1)precision( TP/(TP+FP) )(2)recall( TP/(TP+FN) ),and(3)F1-score
((2*precision*recall)/(precision+recall) )
4.2.3 Performance on Android App GUIs. We experiment with five
edit distance thresholds (0-4). The distance 0 means the two blocks
have the perfect match, and the distance 4 means as long as the
unmatched widgetsin thetwoblocks areno morethan 4,the two
blocks can be regarded as a candidate match. As shown in Figure 8,
fordetection-basedgrouping,theprecision,recallandF1-scoreis
0.437,0.520and0.475atthedistance0.Asthedistancethresholdin-
creases(i.e.,thematchingcriterionrelaxes),theprecision,recalland
F1-scorekeepsincreasingto0.652,0.776and0.709atthedistance
threshold 4. As shown in Figure 8and Table 2, the metadata-based
grouping with the ground-truth GUI widgets achieves a noticeable
improvement over the detection-based grouping with the detectedTable 2:Performancecomparison (editdistance â‰¤1)
Widgets Approach #Bock Precision Recall F1
MetadataOur Approach 1,465 0.607 0.754 0.672
ScreenRecog 1,038 0.131 0.116 0.123
DetectionOur Approach 1,260 0.546 0.650 0.593
ScreenRecog 992 0.103 0.083 0.092
widgets,intermsofallthreemetrics,especiallyforrecall.Thissug-
gests that improving GUI widget detection will positively improve
the subsequent perceptualgrouping.
As the examples in Figure 6show, our approach can not only
accuratelyprocessGUIswithclearstructures(e.g.,thefirstrow),
butitcanalsoprocessGUIswithlargenumbersofwidgetsthatare
placedinapackedway(e.g.,thesecondandthirdrows).Further-
more, our approachis fault-tolerant to GUI widget detection errors
to a certain extent, for example, the second row of detection-based
groupingforscreenshotanddesign.Themapandthepushed-aside
partial GUI result in many inaccurately detected GUI widgets in
thesetwocases.However,ourapproachstillrobustlyrecognizes
the proper perceptualgroups.
We compare our approach with the heuristic-based grouping
method (Screen Recognition) proposed in Zhang et.al. [ 58] (which
received the distinguished paper award at CHI2021). The results in
Table2shows that ScreenRecognition canhardlyhandle visually
and structurally complicated GUIs based on a few ad-hoc and rigid
heuristics. Its F1 score is only 0.092 on the detected widgets and
0.123 on the ground-truth widgets. This is because its heuristics
are designed for only some fixed grouping styles such as cards and
multi-tabs. In contrast, our approach is designed to fulfil generic
Gestaltprinciplesofgrouping.
We manually inspect the grouping results by our approach
against the ground-truth groups to identify the potential improve-
ments.Figure 9presentsfourtypicalcasesthatcausetheperceived
groups to be regarded as incorrect.For the detection-based group-
ing,themajorissueisGUIwidgetover-segmentation(awidgetis
detectedasseveralwidgets)orunder-segmentation(severalwidgets
are detected as one widget). In the first row, the detector segments
the texts on the right side of the GUI into several text and non-text
widgets. As indicated by the same color in the Grouping Result col-
umn,ourapproachstillsuccessfullypartitionsthewidgetsonthe
samerowintoablock,andrecognizesthelargegroupcontaining
these row blocks. But as shown in the Group Comparison column,
onewidgetinthesecond,thirdandfourthdetectedblocksdonot
match those in the corresponding ground-truthblocks.In the sec-
ond row, the GUI widget detector merges close-by multi-line texts
asasingletextwidget,whilethesetextwidgetsareseparatewid-
getsinthegroundtruth.Again,ourapproachrecognizestheoverall
perceptual groups correctly, but the widgets in the corresponding
blocksdo not completely match.
Whileusingtheground-truthwidgetsfromtheGUImetadata
tomitigatetheGUIwidgetmisdetection,thegroupingresultssee
the improvement but suffer from two other problems. First, the
widgets in the metadata contain some widgets that are visually oc-
cludedorhidden.ThethirdrowinFigure 9illustratesthisproblem,
where some widgets are actually occluded behind the menu on the
left, but they are still available in the runtime metadata and are
339Psychologically-Inspired, UnsupervisedInference of Perceptual Groups of GUI Widgets from GUI Images ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
Ground Truth
Grouping Result
Ground Truth
Grouping Result
Ground Truth
Grouping Result
Ground Truth
Grouping ResultWidget Information Grouping Result Ground Truth Group ComparisonDetection Metadata
Figure 9: Typical causes of grouping mistakes (red box - text
widget, green box - non-text widget, pink box - perceptual
group,red dashed box -unmatched ground-truthwidget)
extracted as the ground-truth widgets. This results in a completely
incorrectgrouping.Theissueofwidgetocclusionormodalwindow
couldbemitigatedasfollows:trainanimageclassifiertopredict
the presence of widget occlusion or modal window, then follow
thefigure-ground principle[ 1]to separateforeground modalwin-
dow from the background, and finally detect the perceptual groups
on the separated model window. Second, alternative ways exist to
partitionthewidgetsintogroups.Forexample,fortheGUIinthe
fourthrow,thegroundtruthcontainseightblocks,eachofwhich
hasoneimageandonetextwhileourgroupingapproachpartitions
theseblocksintofourrowsofalargegroup,andeachrowcontains
twoblocks(asindicatedbythesamecolorinGroupingResult).Per-
ceptually,bothwaysareacceptablebutthegroupdifferencescause
the grouping result byour approach to be regardedas incorrect.
4.2.4 Performance on UI Design Prototypes. Tested on the 20 UI
design prototypes, our approach achieves the precision of 0.750,
therecall0.818andtheF1-score0.783.ThethirdcolumninFigure 6
shows some results of our grouping approach for the UI design
prototypes, where we see it is able to infer the widget groups well
for different styles of GUI designs. GUI widget detection is morerobust on UI design prototypes due to the more accurate GUI wid-
getdetection,whichleadstotheimprovementofthesubsequent
grouping of detected widgets. As shown in Figure 6, the widgets in
a UI design prototype is usually scattered, while the real app GUIs
are packed. Both GUI widget detection and perceptual grouping
become relativelyeasier onless packedGUIs.
4.2.5 ProcessingTime. AstheGUIwidgetgroupingcanbeusedas
apartofvariousautomationtaskssuchasautomatedtesting,the
runtimeperformancecanbeaconcern.Werecordtheprocessing
time while running our approach over the dataset to get a sense of
its efficiency. Our experiments run on a machine with Windows
10 OS, Intel i7-7700HQ CPU, and 8GB memory. Our approach com-
prises two major steps: widget detection and perceptual grouping.
We improved and refactored the original UIED to boost the run
performance ofthewidgetdetection, and now it takesanaverage
of 1.1s to detect thewidgets in a GUI, which significantlyexceeds
the originalUIED that takes onaverage 9s per GUI. The grouping
process is also efficient, which takes an average of 0.6s to process a
GUI. In total, the average processing time of the entire approach
is 1.7s per GUI image. Furthermore, as our approach does not in-
volveanydeeplearningtechniques,itdoesnotrequireadvanced
computing support such as GPU.
5 RELATED WORK
Our work falls into the area of reverse-engineering the hidden
attributesofGUIsfrompixels.Therearetwolinesofcloselyrelated
work: GUI widgetdetection andGUI-to-text/code generation.
GUIwidgetdetectionisaspecialcaseofobjectdetection[ 31,40,
49]. Earlier work [ 38] uses classic computervision (CV) algorithms
(e.g.,Canny edgeand contouranalysis) todetectGUIwidgets.Re-
cently, White et al. [ 52] apply a popular object detection model
YOLOv2 [ 40] to detect GUI widgets in GUI images for random GUI
testing. Feng et al. [ 15] apply Faster RCNN [ 31] to obtain GUI wid-
getsfromappscreenshotsandconstructasearchableGUIwidget
gallery. Chen et al. [ 20] proposed an approach to complete icon la-
belinginmobileapplications.ArecentstudybyXieetal.[ 21]shows
thatbothclassicCValgorithmsandrecentdeeplearningmodels
have limitations when applied to GUI widget detection, which has
different visual characteristics and detection goals from natural
sceneobjectdetection.TheydesignahybridmethodUIEDinspired
bythe figure-ground [ 1]characteristicof GUI,which achievesthe
start-of-the-art performance for GUI widgetdetection.
GUI-to-text/code generation also receives much attention. To
improve GUIaccessibility,Chenet al.[ 18]propose atransformer-
based image captioning model for producing labels for icons. To
implementGUIviewhierarchy,REMAUI[ 38]infersthreeAndroid-
specificlayouts(LinearLayout,FrameLayoutandListView)based
on hand-craft rules to group widgets. Recently, Screen Recogni-
tion[58]developssomeheuristicsforinferringtabsandbars.How-
ever,theseheuristic-basedwidgetgroupingmethodscannothandle
visually and structurally complicated GUI designs (e.g., nested per-
ceptualgroupslikeagridofcards).Alternatively,imagecaptioning
models [34,51] have been used to generate GUI view hierarchy
from GUI images [ 12,16]. Although these image-captioning based
methods get rid of hard-coded heuristics, they suffer from GUI
data availability and quality issues (as discussed in Introduction
340ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore Mulong Xie, Zhenchang Xing, SidongFeng,XiweiXu,Liming Zhu, Chunyang Chen
and illustrated in Figure 2). These methods also suffer from code
redundancy and no explicit image-code traceability issues (see Sec-
tion6.2). The perceptual groups recognized by our approach could
helpto addresstheseissues.
None of the existing GUI widget detection and GUI-to-code ap-
proaches solve the perceptual grouping problem in a systematic
way as our approach does. ReDraw [ 37] and FaceOff [ 59] solves
the layout problem by finding in the codebase the layouts con-
taining similar GUI widgets. Some other methods rely on source
code or specific layout algorithm (e.g., Android RelativeLayout) to
synthesizemodularGUIcodeorlayout[ 11,13]orinferGUIduplica-
tion [54]. All these methods are GUI implementation-oriented, and
hardtogeneralizeforotherapplicationscenariossuchasUIdesign
search,UIautomation,roboticGUItestingoraccessibilityenhance-
ment.Incontrast,ourapproachisbasedondomain-independent
Gestalt principles and is application-independent, so it can support
differentdownstream SE tasks(see Section 6).
Inthecomputervisioncommunity,somemachinelearningtech-
niques[33,55,57]havebeenproposedtopredictstructureinthe
visualscene,i.e.,so-calledscenegraphs.Thesetechniquescaninfer
therelationshipsbetweenobjectsdetectedinanimageanddescribe
theserelationshipsbytriplets(<subject,relation,object>).However,
such relationship triplets cannot represent complex GUI widget
relations in perceptual groups. Furthermore, these techniques also
requiresufficienthigh-qualitydataformodeltraining,whichisa
challenging issuefor GUIs.
6 PERCEPTUALGROUPING APPLICATIONS
Our perceptual grouping method fills in an important gap for auto-
matic UI understanding. Perceptual groups, together withelemen-
tary widget information,wouldopenthedoorto someinnovative
applicationsinsoftware engineeringdomain.
6.1 UI Design Search
UIdesignisahighlycreativeactivity.TheproliferationofUIdesign
dataontheInternetenablesdata-drivenmethodstolearnUIdesigns
and obtain design inspirations [ 14,23,36]. However, this demands
effective UI designsearch engines.Existing methodsoften rely on
the GUI metadata, which limits their applicability as most GUI
designsexistinonlypixelformat.GalleryDC[ 15]buildsagallery
of GUI widgets and infer elementary widget information (e.g., size,
primarycolor)tohelpwidgetsearch.Unfortunately,thissolution
doesnot applyto thewhole and complex UIs.Chen etal. [ 17]and
Rico[36]useimageauto-encodertoextractimagefeaturesthrough
self-supervised learning, which can be used to find visually similar
GUI images. However, the image auto-encoder encodes only pixel-
levelfeatures,butisunawareofGUIstructure,whichisverycritical
tomodelandunderstandGUIs.Assuch,giventheGUIintheleft
ofFigure 2,theseauto-encoderbasedmethodsmayreturnaGUI
liketheoneontherightofFigure 2,becausebothGUIshaverich
graphic features and some textural features. Unfortunately, such
searchresultsaremeaningless,becausetheybearnosimilarityin
terms of GUI structure and perceptual groups of GUI widgets. Our
approach can accurately infer perceptual groups of GUI widgets
frompixels.Basedonitsperceptualgroupingresults,aUIdesign
search would become structure-aware, and finds not only visuallybutalso structurallysimilarGUIs.Forexample, astructure-aware
UIdesignsearchwouldreturnaGUIliketheonein2nd-row-1st-
column ofFigure 6for the left GUI inFigure 2.
6.2 Modular GUI-to-CodeGeneration
Existing methods for GUI-to-code generation either use hand-craft
rulesorspecificlayoutalgorithmstoinfersomespecificimplemen-
tation layout [ 13,38], or assume the availability of a codebase to
search layout implementations [ 37,59]. Image-captioning based
GUI-to-Codemethods[ 12,16,29,30]aremoreflexibleastheylearn
howtogenerateGUIviewhierarchyfromGUImetadata(ifavail-
able).However,thenatureofimagecaptioningistojustdescribe
theimagecontent,butitiscompletelyunawareofGUIstructure
during the code generation. As such, the generated GUI code is
highly redundant for repetitive GUI blocks. For example, for the
card-basedGUIdesigninFigure 1(a),itwillgenerateeightpieces
of repetitive code, one for each of the eight cards. This type of gen-
erated code is nothing like the modular GUI code developers write.
So it has little practicality. Another significant limitation of image
captioningisthatthegeneratedGUIlayoutsandwidgetshaveno
connection to the corresponding parts in the GUI image. For a GUI
withmanywidgets(e.g.,thoseinthe2ndand3rdrowsinFigure 6),
itwouldbehardtounderstandhowthegeneratedcodeimplements
theGUI.Withthesupportofourperceptualgrouping,GUI-to-code
generationcanencapsulatethewidgetgroupinginformationinto
the code generation process and produce much less redundant and
more modular, reusable GUI code (e.g., extensible card component).
6.3 UI Automation
Automating UI understanding from pixels can support many UI
automation tasks. A particular application of UI automation in
software engineering is automatic GUI testing. Most existing meth-
ods for automatic GUI testing rely on OS or debugging infrastruc-
ture [5,10,32,43]. In recent years, computer vision methods have
alsobeenusedtosupportnon-intrusiveGUItesting[ 27,28,39,52].
However,thesemethodsonlyworkattheGUIwidgetlevelthrough
eithertraditionalwidgetdetection[ 38]ordeeplearningmodelslike
Yolo [40]. Furthermore, they only support random testing, i.e., ran-
dominteractionswithsomewidgets.Somestudies[ 19,22,56]show
that GUI testing would be more effective if the testing methods
wereawareofmorelikelyinteractions.Theyproposedeeplearning
methods to predict such likely interactions. However, the learning
is a completely black box. That is, they can predict where on the
GUIsomeactionscouldbeapplied,buttheydonotknowwhatwill
be operated and why so. Our approach can inform the learning
with higher-order perceptual groups of GUI widgets so that the
model could make an explainable prediction, for example, scrolling
isappropriate becausethispartofGUIdisplaysalistofrepetitive
blocks.Itmayalsoguidethetestingmethodstointeractwiththe
blocksinaperceptualgroupinanorderlymanner,andensureall
blocks are tested without unnecessary repetitions. Such support
forUIautomationwouldalsoenhancetheeffectivenessofscreen
readers which currently heavily rely on accessibility metadata and
use mostly elementary widgetinformation.
341Psychologically-Inspired, UnsupervisedInference of Perceptual Groups of GUI Widgets from GUI Images ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore
7 CONCLUSION AND FUTUREWORK
Thispaper presentsanovel approachfor recognizingperceptual
groups of GUI widgets in GUI images. The approach is designed
around the four psychological principles of grouping - connect-
edness, similarity, proximity and continuity. To the best of our
knowledge, this is the first unsupervised, automatic UI understand-
ing approach with a systematic theoretical foundation, rather than
relying on ad-hoc heuristics or modeltraining with GUI metadata.
ThroughtheevaluationofbothmobileappGUIsandUIdesignpro-
totypes, we confirm the high accuracy of our perceptual grouping
methodforvisuallyandstructurallydiverseGUIs.Ourapproach
fills the gap of visual intelligence between the current widget-level
detection and the whole-UI level GUI-to-code generation. As a
pixel-only and application-independent approach, we envision our
approachcouldenhancemanydownstreamsoftwareengineering
taskswiththevisualunderstandingofGUIstructureandpercep-
tual groups, such as structure-aware UI design search, modular
andreusableGUI-to-codegeneration,andlayout-sensitiveUIau-
tomation for GUI testing and screen reader. Although our current
approachachievesverypromisingperformance,itcanbefurther
improvedbydealingwithwidgetocclusionormodalwindow.More-
over, we will investigate semantic grouping that aims to recognize
both interaction andcontentsemantics ofperceptualgroups.
ACKNOWLEDGEMENTS
This research was partially funded by Data61-ANU Collabora-
tive Research Project. Chunyang is partially supported by Face-
book/MetaGift grant.
REFERENCES
[1][n.d.]. 7GestaltPrinciplesofVisualPerception:CognitivePsychologyforUX:
UserTesting Blog. https://www.usertesting.com/blog/gestalt-principles#figure
[2] [n.d.]. Freewebsite builder:Createa freewebsite. http://www.wix.com/
[3][n.d.]. Get started on Android with TalkBack - Android Accessibility Help.
https://support.google.com/accessibility/android/answer/6283677?hl=en
[4] [n.d.]. the collaborativeinterfacedesign tool. https://www.figma.com/
[5][n.d.]. UI/Application Exerciser Monkey : Android Developers. https:
//developer.android.com/studio/test/monkey#:~:text=TheMonkeyisaprogram,
arandomyetrepeatablemanner.
[6][n.d.]. Vision AI | Derive Image Insights via ML | Cloud Vision API. https:
//cloud.google.com/vision/
[7]2006."Gestalt psychology". Britannica concise encyclopedia . Britannica Digital
Learning.
[8] 2021. Accessibility - Vision. https://www.apple.com/accessibility/vision/
[9]2021. Gestaltpsychology. https://en.wikipedia.org/wiki/Gestalt_psychology#
cite_note-1
[10]UIAutomator, 2021. https://developer.android.com/training/testing/ui-
automator .
[11]Mohammad Bajammal, Davood Mazinanian, and Ali Mesbah. 2018. Gener-
ating Reusable Web Components from Mockups. In Proceedings of the 33rd
ACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering (Montpel-
lier, France) (ASE 2018) . NewYork, NY, USA,601Å›611. https://doi.org/10.1145/
3238147.3238194
[12]TonyBeltramelli.2018. pix2code:Generatingcodefromagraphicaluserinter-
face screenshot. In Proceedings of the ACM SIGCHI Symposium on Engineering
InteractiveComputingSystems . 1Å›6.
[13]PavolBielik,MarcFischer,andMartinVechev.2018. RobustRelationalLayout
Synthesis from Examples for Android. Proc. ACM Program. Lang. 2, OOPSLA,
Article156(Oct.2018),29pages. https://doi.org/10.1145/3276526
[14]ChunyangChen,SidongFeng,ZhengyangLiu,ZhenchangXing,andShengdong
Zhao.2020. Fromlosttofound:Discovermissinguidesignsemanticsthrough
recoveringmissingtags. ProceedingsoftheACMonHuman-ComputerInteraction
4,CSCW2 (2020), 1Å›22.
[15]Chunyang Chen, Sidong Feng, Zhenchang Xing, Linda Liu, Shengdong Zhao,
and Jinshui Wang. 2019. Gallery D.C.: Design Search and Knowledge Discoverythrough Auto-Created GUI Component Gallery. Proc. ACM Hum.-Comput. Inter-
act.3, CSCW, Article 180 (Nov. 2019), 22 pages. https://doi.org/10.1145/3359282
[16]Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu.
2018. From UI Design Image to GUI Skeleton: A Neural Machine Transla-
tor to Bootstrap Mobile GUI Implementation. In Proceedings of the 40th In-
ternational Conference on Software Engineering (Gothenburg, Sweden) (ICSE
â€™18). Association for Computing Machinery, New York, NY, USA, 665Å›676.
https://doi.org/10.1145/3180155.3180240
[17]Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xin Xia, Liming Zhu, John
Grundy, andJinshuiWang.2020. Wireframe-basedUIDesignSearchthrough
Image Autoencoder. ACM Transactions on Software Engineering and Methodology
29,3 (Jul2020),1Å›31. https://doi.org/10.1145/3391613
[18]Jieshan Chen,Chunyang Chen,ZhenchangXing,Xiwei Xu,Liming Zhut,Guo-
qiang Li, and Jinshui Wang. 2020. Unblind Your Apps: Predicting Natural-
Language Labels for Mobile GUI Components by Deep Learning. In 2020
IEEE/ACM42ndInternationalConferenceonSoftwareEngineering(ICSE) .322Å›334.
[19]JieshanChen,AmandaSwearngin,JasonWu,TitusBarik,JeffreyNichols,and
XiaoyiZhang.2022. ExtractingReplayableInteractionsfromVideosofMobile
AppUsage. arXiv preprint arXiv:2207.04165 (2022).
[20]JieshanChen,AmandaSwearngin,JasonWu,TitusBarik,JeffreyNichols,and
Xiaoyi Zhang. 2022. Towards Complete Icon Labeling in Mobile Applications. In
CHIConference onHuman Factors inComputingSystems . 1Å›14.
[21]Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu, Liming
Zhu,andGuoqiangLi.2020. Objectdetectionforgraphicaluserinterface:old
fashioned or deep learning or a combination? Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
FoundationsofSoftwareEngineering (Nov2020). https://doi.org/10.1145/3368089.
3409691
[22]Christian Degott, Nataniel P. Borges Jr., and Andreas Zeller. 2019. Learning
User Interface Element Interactions. In Proceedings of the 28th ACM SIGSOFT
InternationalSymposiumonSoftwareTestingandAnalysis (Beijing,China) (ISSTA
2019). Association for Computing Machinery, New York, NY, USA, 296Å›306.
https://doi.org/10.1145/3293882.3330569
[23]BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,
Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A Mobile App Dataset
for Building Data-Driven Design Applications. In Proceedings of the 30th Annual
SymposiumonUser InterfaceSoftwareand Technology (UIST â€™17) .
[24]Dribbble. [n.d.]. Discover the worldâ€™s Top Designers &amp; Creatives. https:
//dribbble.com/
[25]Martin Ester, Hans-Peter Kriegel, JÃ¶rg Sander, and Xiaowei Xu. 1996. A Density-
Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
InProceedings of the Second International Conference on Knowledge Discovery and
DataMining (Portland, Oregon) (KDDâ€™96). AAAI Press,226Å›231.
[26]Michael W. Eysenck and Marc Brysbaert. 2018. Fundamentals of Cognition.
(2018).https://doi.org/10.4324/9781315617633
[27]Sidong Feng and Chunyang Chen. 2022. GIFdroid: An Automated Light-weight
Toolfor Replaying Visual BugReports. (2022).
[28]SidongFengandChunyangChen.2022. GIFdroid:automatedreplayofvisual
bug reports for Android apps. In Proceedings of the 44th International Conference
onSoftwareEngineering . 1045Å›1057.
[29]Sidong Feng, Minmin Jiang, Tingting Zhou, Yankun Zhen, and Chunyang Chen.
2022. Auto-Icon+: An Automated End-to-End Code Generation Tool for Icon
Designs in UI Development. ACM Transactions on Interactive Intelligent Systems
(TiiS)(2022).
[30]SidongFeng,SuyuMa,JinzhongYu,ChunyangChen,TingTingZhou,andYankun
Zhen. 2021. Auto-icon: An automated code generation tool for icon designs
assistinginuidevelopment.In 26thInternationalConferenceonIntelligentUser
Interfaces . 59Å›69.
[31]RossGirshick.2015. FastR-CNN.In TheIEEEInternationalConferenceonCom-
puterVision(ICCV) .
[32]JiaqiGuo,ShuyueLi,Jian-GuangLou,ZijiangYang,andTingLiu.2019. SARA:
self-replay augmented record and replay for Android in industrial cases. In
Proceedingsofthe28thacmsigsoftinternationalsymposiumonsoftwaretesting
and analysis . 90Å›100.
[33]Boris Knyazev, Harm de Vries, CÄƒtÄƒlina Cangea, Graham W. Taylor, Aaron
Courville, and Eugene Belilovsky. 2020. Graph Density-Aware Losses for Novel
Compositionsin Scene GraphGeneration. arXiv: 2005.08230 [cs.CV]
[34]YannLecun,YoshuaBengio,andGeoffreyHinton.2015. Deeplearning. Nature
521, 7553 (2015), 436Å›444. https://doi.org/10.1038/nature14539
[35]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2019. Humanoid: A
DeepLearning-BasedApproachtoAutomatedBlack-boxAndroidAppTesting.In
2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE). 1070Å›1073. https://doi.org/10.1109/ASE.2019.00104
[36]ThomasF.Liu,MarkCraft,JasonSitu,ErsinYumer,RadomirMech,andRanjitha
Kumar.2018. LearningDesignSemanticsforMobileApps.In The31stAnnual
ACM Symposium on User Interface Software and Technology (Berlin, Germany)
(UIST â€™18) . ACM, New York, NY, USA, 569Å›579. https://doi.org/10.1145/3242587.
3242650
342ESEC/FSE â€™22, November14Å›18, 2022,Singapore, Singapore Mulong Xie, Zhenchang Xing, SidongFeng,XiweiXu,Liming Zhu, Chunyang Chen
[37]Kevin Moran, Carlos Bernal-CÃ¡rdenas, Michael Curcio, Richard Bonett, and
Denys Poshyvanyk. 2020. Machine Learning-Based Prototyping of Graphical
User Interfaces for Mobile Apps. IEEE Transactions on Software Engineering 46, 2
(2020), 196Å›221. https://doi.org/10.1109/TSE.2018.2844788
[38]TuanAnhNguyenandChristophCsallner.2015. ReverseEngineeringMobile
ApplicationUserInterfaceswithREMAUI.In Proceedingsofthe30thIEEE/ACM
International Conference on Automated Software Engineering (Lincoln, Nebraska)
(ASE â€™15). IEEE Press,248Å›259. https://doi.org/10.1109/ASE.2015.32
[39]Ju Qian, Zhengyu Shang, Shuoyan Yan, Yan Wang, and Lin Chen. 2020. Ro-
Script: A Visual Script Driven Truly Non-Intrusive Robotic Testing System
for Touch Screen Applications. In Proceedings of the ACM/IEEE 42nd Interna-
tional Conference on Software Engineering (Seoul, South Korea) (ICSE â€™20) . As-
sociation for Computing Machinery, New York, NY, USA, 297Å›308. https:
//doi.org/10.1145/3377811.3380431
[40]JosephRedmonandAliFarhadi.2018. YOLOv3:AnIncrementalImprovement.
CoRRabs/1804.02767 (2018). arXiv: 1804.02767 http://arxiv.org/abs/1804.02767
[41]Andy Rutledge. 2009. Gestalt Principles of Perception - 3: Proximity, Uniform
Connectedness, and Good Continuation. http://andyrutledge.com/gestalt-
principles-3.html
[42]S1T2. 2021. Apply crap to design: S1T2 blog. https://s1t2.com/blog/step-1-
generously-apply-crap-to-design
[43]OnurSahin,AsselAliyeva,HariharanMathavan,AyseCoskun,andManuelEgele.
2019. Randr: Record and replay for android applications via targeted runtime
instrumentation. In 2019 34th IEEE/ACM International Conference on Automated
SoftwareEngineering (ASE) . IEEE,128Å›138.
[44] Sketch. [n.d.]. https://www.sketch.com/
[45]Sternberg and Robert. 2003. Cognitive Psychology Third Edition . Thomson
Wadsworth.
[46]Herb Stevenson. [n.d.]. Emergence: The Gestalt Approach to Change.
http://www.clevelandconsultinggroup.com/articles/emergence-gestalt-
approach-to-change.php
[47]Tesseract-Ocr.[n.d.]. tesseract-ocr/tesseract:TesseractOpenSourceOCREngine
(mainrepository). https://github.com/tesseract-ocr/tesseract
[48]Thalion.2020. UiDesigninpractice:Gestaltprinciples. https://uxmisfit.com/
2019/04/23/ui-design-in-practice-gestalt-principles/
[49]J. R.R.Uijlings,K. E.A.vandeSande, T.Gevers,andA.W. M.Smeulders.2013.
SelectiveSearchforObjectRecognition. InternationalJournalofComputerVision
104, 2 (01 Sep 2013),154Å›171. https://doi.org/10.1007/s11263-013-0620-5[50]UserTesting.2019.7GestaltPrinciplesofVisualPerception:CognitivePsychology
forUX:UserTestingBlog. https://www.usertesting.com/blog/gestalt-principles#
proximity
[51]Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show
and Tell:ANeural Image Caption Generator. arXiv: 1411.4555 [cs.CV]
[52]ThomasD.White,GordonFraser,andGuyJ.Brown.2019. ImprovingRandom
GUI Testing with Image-based Widget Detection. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis (Beijing,
China)(ISSTA 2019) . ACM, New York, NY, USA, 307Å›317. https://doi.org/10.
1145/3293882.3330551
[53]Mulong Xie, Sidong Feng, Zhenchang Xing, Jieshan Chen, and Chunyang Chen.
2020. UIED:ahybridtoolforGUIelementdetection.In Proceedingsofthe28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe FoundationsofSoftwareEngineering . 1655Å›1659.
[54]RahulkrishnaYandrapally,AndreaStocco,andAliMesbah.2020. Near-Duplicate
Detection in Web App Model Inference. In Proceedings of the ACM/IEEE 42nd
International Conference on Software Engineering (Seoul, South Korea) (ICSE â€™20) .
Association for Computing Machinery, New York, NY, USA, 186Å›197. https:
//doi.org/10.1145/3377811.3380416
[55]Jianwei Yang, Jiasen Lu,Stefan Lee, Dhruv Batra, and Devi Parikh. 2018. Graph
R-CNN for Scene GraphGeneration. arXiv: 1808.00191 [cs.CV]
[56]YazdaniBanafsheDaragh.[n.d.]. Deep-GUI:TowardsPlatform-IndependentUI
Input Generation with Deep Reinforcement Learning. UC Irvine ([n.d.]). https:
//escholarship.org/uc/item/3kv1n3qk
[57]RowanZellers,MarkYatskar,SamThomson,andYejinChoi.2018. NeuralMotifs:
Scene GraphParsingwith Global Context. arXiv: 1711.06640 [cs.CV]
[58]Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle Murray,
LisaYu,QiShan,JeffreyNichols,JasonWu,ChrisFleizach,AaronEveritt,and
Jeffrey P. Bigham. 2021. Screen Recognition: Creating Accessibility Metadata for
Mobile Applications from Pixels. arXiv: 2101.04893 [cs.HC]
[59]Shuyu Zheng, Ziniu Hu, and Yun Ma. 2019. FaceOff: Assisting the Manifestation
Design of Web Graphical User Interface. In Proceedings of the Twelfth ACM Inter-
nationalConferenceon WebSearchandData Mining (MelbourneVIC,Australia)
(WSDMâ€™19) .AssociationforComputingMachinery,NewYork,NY,USA,774Å›777.
https://doi.org/10.1145/3289600.3290610
[60] Xinyu Zhou, Cong Yao, He Wen,Yuzhi Wang,Shuchang Zhou, Weiran He, and
JiajunLiang.2017.EAST:anefficientandaccuratescenetextdetector.5551Å›5560.
343