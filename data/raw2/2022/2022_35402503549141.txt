Lighting Up Supervised Learning in User Review-Based Code
Localization: Dataset and Benchmark
Xinwen Hu‚àó
xinwen.hu.nju@gmail.com
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, ChinaYu Guo‚àó
guoyu@smail.nju.edu.cn
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, ChinaJianjie Lu
mf21320105@smail.nju.edu.cn
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, China
Zheling Zhu
mf21320238@smail.nju.edu.cn
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, ChinaChuanyi Li‚Ä†
lcy@nju.edu.cn
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, ChinaJidong Ge‚Ä†
gjd@nju.edu.cn
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, China
Liguo Huang
lghuang@lyle.smu.edu
Department of Computer Science,
Southern Methodist University
Dallas, Texas, USABin Luo
luobin@nju.edu.cn
State Key Laboratory for Novel
Software and Technology, Nanjing
University
Nanjing, Jiangsu, China
ABSTRACT
As User Reviews (URs) of mobile Apps are proven to provide valu-
able feedback for maintaining and evolving applications, how to
make full use of URs more efficiently in the release cycle of mo-
bile Apps has become a widely concerned and researched topic
in the Software Engineering (SE) community. In order to speed
up the completion of coding work related to URs to shorten the
release cycle as much as possible, the task of User Review-based
code localization is proposed and studied in depth. However, due to
the lack of large-scale ground truth dataset (i.e., truly related < ùëàùëÖ,
ùê∂ùëúùëëùëí > pairs), existing methods are all unsupervised learning-based.
In order to light up supervised learning approaches, which are
driven by large labeled datasets, for Review2Code, and to compare
their performances with unsupervised learning-based methods, we
first introduce a large-scale human-labeled < ùëàùëÖ,ùê∂ùëúùëëùëí > ground
truth dataset, including the annotation process and statistical anal-
ysis. Then, a benchmark consisting of two SOTA unsupervised
learning-based and four supervised learning-based Review2Code
methods is constructed based on this dataset. We believe that this
‚àóBoth authors contributed equally to this research.
‚Ä†Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
¬©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.3549141paper can provide a basis for in-depth exploration of the supervised
learning-based Review2Code solutions.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware maintenance tools ;
‚Ä¢Computing methodologies ‚ÜíArtificial intelligence .
KEYWORDS
User review, Code Localization, Supervised Learning, Android
ACM Reference Format:
Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo
Huang, and Bin Luo. 2022. Lighting Up Supervised Learning in User Review-
Based Code Localization: Dataset and Benchmark. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE ‚Äô22), November 14‚Äì
18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3540250.3549141
1 INTRODUCTION
Since the lunch of AppStores in mobile operating platforms, such
as Android, iOS, and WinPhone, there have been accumulated over
a million mobile Apps. Not only can users download or rate (e.g.,
five-star scale) Apps, but also they can comment on Apps based on
their experience, such as what features they like the most or what
features they need. These comments are called User Reviews [ 65]
and are generally presented as free-form text. There are four cate-
gories of User Reviews, namely, Problem Discovery (PD, i.e., denotes
user‚Äôs report of App‚Äôs problems such as system errors or crashes),
Feature Request (FR, i.e., denotes user‚Äôs requests of implementing
new or enhancing existing functions), Information Seeking (IS, i.e.,
533
ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo Huang, and Bin Luo
A/B TestingUser Review
Emotion
Scores
Topic
‚Ä¶
Code 
ActivitiesTest 
Activities
CD/CIDeployment
PhaseRelease
-User Requirements
-System Requirements
-CD/CI
-Analysis of User Feedback
-‚Ä¶Release 
PlanningA B
Figure 1: Release cycle for App maintenance and develop-
ment.
denotes user‚Äôs query of information related to the App), and Infor-
mation Giving (IG, i.e., denotes user‚Äôs notification of update about
an aspect related to the App) [ 51]. It has been proven that PD and
FR provide valuable feedback for guiding developers to engage in
software maintenance and release [ 63]. Considering the fast growth
rate of User Reviews (e.g., popular Apps receive more than 500 User
Reviews per day on average [ 44]) and the high proportion of PD
and FR (e.g., 23.3% of mobile App‚Äôs User Reviews are FR [ 28], 51%
of User Reviews of Twitter are PD and FR [ 63]), how to make full
use of User Reviews more efficiently in the release cycle of mobile
App has become a widely concerned and researched topic in the
Software Engineering (SE) community.
Figure 1 shows the mobile application release cycle widely fol-
lowed by the industry in the context of DevOps (i.e., Continuous
Integration, Deployment, Delivery and Analysis) [ 10], from which
the importance of User Review can be seen [ 20,23,50]. The collec-
tion and analysis of User Review can be regarded as the beginning
of the next version release, directly providing valuable information
for at least three important tasks, namely, Release Planning, Testing,
and Development. Specifically, for Release Planning, the severity
of the related problems and the necessity/cost of new functions
mentioned in the URs provide important clues for the App‚Äôs main-
tenance team to decide the focus of the next development cycle,
for example, what problems need to be solved, which features need
to be optimized, or which new features to be added in the next
release. For Testing, the description of errors or crash scenarios
in URs provides key information for efficiently designing effective
test cases, which greatly speeds up the efficiency of solving related
problems. Ultimately, both feature mentioned in Release Planning
and problems verified in Testing need to be solved or implemented
through coding. However, it is not enough to just realize that URs
are valuable to the release cycle. What is more important is how to
quickly complete the coding activities related to URs to shorten the
release cycle as much as possible. Fortunately, automatically locat-
ing the related code according to URs (i.e., User Review-based Code
or Change Localization, for convenience, we name it Review2Code
in this paper) has proved to be an effective means to speed up bug
fixes and new function development.Concretely, Review2Code is to locate the related source code in
file-level or method-level according to the User Reviews of mobile
Apps collected from AppStores. However, there are still few relevant
studies on Review2Code compared with Bug Localization [ 32,56]
even the importance of User Reviews and Review2Code are fully
recognized. The reasons are two-fold. First, other than natural lan-
guage descriptions, User Reviews do not contain any additional
information such as program execution records or test cases de-
scribed in code form which are contained in Bug descriptions. It is
widely recognized that correlating natural language with code is
harder than correlating code with code. Second, Bugs or Issues are
generally collected from open source software repositories such as
GitHub where they will be associated with the relevant changed
code after they are settled. This means that sufficient ground truth
datasets for Bug Localization can be easily obtained from open
source software repositories. However, the ground truth dataset
(i.e., true related < ùëàùëÖ,ùê∂ùëúùëëùëí (Method-level, File-level)> pairs) of Re-
view2Code needs to be manually annotated by researchers, which
is very expensive to obtain.
Nevertheless, researchers‚Äô enthusiasm for Review2Code has not
been blocked and five specific methods for locating code for User
Review have been proposed. As shown in the top half of Table 1,
they are all designed for Android Apps. Their input is a UR de-
scribed in natural language, and the output is a code file related to
the UR (i.e., for solving problems or implementing features men-
tioned in the UR, these code files should be modified). Although the
dataset they use includes data from multiple Apps, only the ground
truth used to evaluate the effect of the method is manually anno-
tated, generally less than 500 data instances (as shown in the last
column of Table 1). Due to the small size of ground truth, the core
of these methods is clustering or heuristic similarity measurement
methods that do not require a large-scale ground truth. In other
words, they are all unsupervised learning-based. However, given
that supervised learning methods, especially deep learning, are
widely used in SE tasks and achieve state-of-the-art performances
nowadays, the next question naturally comes to mind is: if ground
truth is available, can the supervised learning approaches
achieve better results than the unsupervised learning ones
on Review2Code? Besides, the supervised learning has two advan-
tages over the unsupervised learning: (1) as newly proposed URs
are handled, more <UR, Code> pairs could be generated and added
to the initial training set (constructed from the one-time effort of
annotating) to train better Review2Code models (i.e., models based
on supervised learning will continue to improve while the others
cannot), (2) the performance of supervised learning-based mod-
els on new applications can be greatly improved by transferring
existing models and annotating a small amount of data for new
applications, while unsupervised learning-based ones cannot.
Motivated by the idea of lighting up supervised learning for Re-
view2Code and comparing supervised learning-based approaches
with the SOTAs of existing unsupervised learning-based ones, we
make the following contributions in this paper:
1. Manually label the first large-scale dataset supporting super-
vised learning-based Review2Code. Each data instance is a < ùëàùëÖ,
ùê∂ùëúùëëùëí > pair. The dataset totally consists of 68,359 pairs of < ùëàùëÖ,
ùëÄùëíùë°‚Ñéùëúùëë > collected from 59,642 reviews of 5 popular Android Apps.
In total, there are 1,639 code files and 8,169 methods in the dataset.
534Lighting Up Supervised Learning in User Review-Based Code Localization: Dataset and Benchmark ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
Table 1: The basic information of existing Review2Code
methods and potential supervised learning-based models.
Name Input Granularity Dataset Evaluation Data‚àó
URR [7] Review File 39 Apps Unknown
ChangeAdvisor [49] Review File 10 Apps 82 UR Clusters
ReviewSolver [72] Review File 18 Apps 447 URs
Where2Change [74] Review File 10 Apps 196 UR Clusters
RISING [76] Review File 10 Apps 297 URs
UNIF [5] Query Method CodeSearchNet ‚Äì
CAT [22] Description Method CoNaLa ‚Äì
CodeBert [11] Query Method CodeSearchNet ‚Äì
GraphCodeBert [19] Query Method CodeSearchNet ‚Äì
‚àóChangeAdvisor and Where2Change treat clusters of URs as the smallest
unit to locate code. They only label relevant code for each cluster during
evaluation.
2. Construct a new benchmark consisting of both unsupervised
learning-based and supervised learning-based approaches for Re-
view2Code based on the proposed dataset. The unsupervised learning-
based methods are Where2Change [ 74] and RISING [ 76], and the
supervised learning-based ones are borrowed from another SE task
(ie, Code Search, searching code snippets for the given natural lan-
guage description, which is similar to Review2Code) as shown in
the second half in Table 1. All of these methods will be described
in detail in Section 3.
3. Evaluate and discuss the benchmark results through conduct-
ing in-depth quantitative and qualitative analysis to motivate future
researches on Review2Code.1
The rest of the paper is organized as follows. Section 2 introduces
related work including User Review Analysis, Bug Localization,
and Code Search. Section 3 briefly describes existing and candi-
date Review2Code approaches, as well as the criteria for selecting
benchmark systems. Section 4 illustrates the constructing process
of the dataset in detail and a brief analysis. Section 5 and Section
6 introduce and discuss the benchmark respectively. Threats to
validity are in Section 7, and Section 8 concludes the paper.
2 RELATED WORK
2.1 User Review Analysis
With the development of machine learning and deep learning, re-
searchers have proposed a variety of effective processing meth-
ods for mining and categorizing User Reviews of mobile applica-
tions in recent years. Some authors designed classifiers of User
Reviews based on their sentiment (e.g. either expectance or anger)
and specific topics (e.g. either problem discovery or feature re-
quest) [ 29,37,39]. Gu and Kim [ 17] presented SUR-Miner, a frame-
work that makes full use of the monotonous structure and seman-
tics of User Reviews to understand users‚Äô preferences. Di Sorbo et
al.[59,60] proposed SURF (Summarizer of User Reviews Feedback)
which has the ability to determine the fine-grained topics discussed
in User Reviews (e.g. UI improvements or licensing issues) and
generate an interactive, structured, and condensed agenda of rec-
ommended software changes. Scalabrino et al.[58,61] devised CLAP
1All data and code can be found here: https://github.com/lcynju/review2code.(Crowd Listener for releAse Planning), an approach to categoriz-
ing User Reviews and clustering them, which provides inspiration
for feature requests and bug fixes in the next release. Compared
with SURF, CLAP could provide the priority of User Reviews and
divide them into specific categories. A number of studies [ 6,12‚Äì
14,21,28,48] applying the topic model technique [ 4,68] to extract
features of User Reviews inspire us as well.
Mining the deep semantic information of User Reviews would
definitely benefit the work of locating code snippets based on User
Reviews. For example, meaningless User Reviews can be filtered
with User Review classification before linking to code. In this paper
(i.e., Section 4.2), we adopt an existing BERT-based UR classification
model [24] to filter useless URs before annotating.
2.2 Bug Localization
Bug Localization refers to locating the potential buggy files for a
given bug report, which is advantageous to help software devel-
opers to fix the Bug effectively. The approaches of Bug localiza-
tion are mainly divided into two categories: based on Information
Retrieval (IR-based) and based on Deep Learning (DL-based). IR
technique is widely used for automatically extracting important
information from resources. VSM (Vector Space Model) is popularly
used for vectoring User Reviews and source code [ 15,46,69,75].
Based on the topic model, some studies measure the similarity
between topic distributions from source code snippets and bug
reports [ 38,47,53]. However, the mismatch between bug reports
and source code snippets limits the performance of IR [ 45]. To ad-
dress this limitation, many researchers take API documents into
account [ 3,8,31,45,70,71]. Some others [ 35,64] extract crucial fea-
tures of source code snippets or explore deep relationships between
bug reports and them. In recent work, DL techniques have been
applicable to learn the characteristics of bug reports and source
code snippets. CNN is utilized for extracting the structural informa-
tion of source code [ 25,26,66,67]. Li et al [34] devised DeepRL4FL,
which treats the Bug (Fault) localization as an image pattern recog-
nition problem through novel code coverage representation learn-
ing and data dependencies representation learning for program
statements. Besides, knowledge graph embeddings and attention
mechanisms [ 73], Abstract Syntax Trees [ 36], and transfer learn-
ing and adversarial learning [ 27,77] are also utilized in machine
learning-based BL approaches.
Different from bug reports that are usually generated by software
developers, User Reviews are largely raised by ordinary users of
Apps. Moreover, a lot of additional information (e.g. either program
execution records or test cases) described in code form is included
in bug reports while User Reviews are free-form texts. Despite
these differences, BL approaches inspire Review2Code to consider
different representations of User Reviews and source code [ 35,73]
and enhance information to bridge the gap between them [ 31,47,
71, 75].
2.3 Code Search
Code search is to find code snippets from a database or repository
that are closely relevant to the query in natural languages and
is vital for applying fixes and extending software functionalities.
Existing code search works in natural language processing can be
535ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo Huang, and Bin Luo
classified into two mainstreams: IR-based (Information Retrieval)
methods and DL-based (Deep Learning) ones. The two kinds of
code search methods differ in their respective styles of matching
queries and code snippets. An IR-based method usually extracts
a set of keywords from a query and then searches for the key-
words in code repositories [ 6,28,40,43,44,61]. Comparatively, a
DL-based method takes some deep learning techniques, especially
embedding algorithms, to map raw data (including queries and code
snippets) into a high-dimensional space and match them [ 49,59]. Lu
et al. [33] studied the relationship between words and source code
snippets. Raghothaman et al. [55] utilized related API to enrich the
information of source code. Vinayakarao et al. [62] established the
relation of programming concepts with syntactic forms. Sachdev et
al.[57] proposed NCS to learn the embeddings of code based on
unsupervised technology. Cambronero et al. [5] developed UNIF
which is a supervised extension of NCS. Gu et al. [18] introduced
CODEnn with three encoded individual channels to learn code
representations. Haldar et al. [22] adopted a multiperspective cross-
lingual neural framework for code-text matching and compared
four models (CT, CAT, MP, MP-CAT) to verify the advantages of
the framework in capturing similarities between two sequences.
With the popularity of pre-training models such as BERT [ 9] and
GPT-2 [ 54], many researchers [ 1,11,42,52] began to pay attention
to their application in code search and achieved gratifying results.
Similar to Review2Code, the task of code search allows users
to raise natural language queries and returns a ranked list of re-
lated code snippets. However, User Reviews referring to subjective
feelings of users about using the application are non-technical and
potentially imprecise, but the query of code search most closely
matches a developer‚Äôs intent and contains more code-level semantic
information. There exist many large-scale and high-quality code
search datasets because code annotations, the input of which, are un-
complicated to be extracted, whereas we haven‚Äôt found any datasets
available for our work. Considering that the purpose and input of
code search are similar, we can build our own dataset, and then
utilize the code search model to evaluate the performance of the
supervised model on Review2Code.
3 REVIEW2CODE SYSTEMS
Before introducing the dataset construction, it is necessary to
present the candidate systems which support Review2Code. As
described in the Introduction section, Table 1 enumerates all exist-
ing unsupervised learning-based and several potential supervised
learning-based approaches of Review2Code. Here, we first make
an in-depth introduction for them.
For unsupervised learning-based methods, the core is to mea-
sure the similarity between a natural language description and
a code snippet. It requires that the dataset include at least two
parts: User Reviews and candidate code. The general process of
these methods is as follows. Preprocess User Reviews and code and
build vector representation for them. Then calculate similarities
between User Reviews and candidate code snippets according to
the predefined measurement, and take the top- ùëòcode snippets with
the highest similarity as the predicted results. However, different
methods may apply additional information, such as commits [ 76] or
issue reports [ 74], to further bridge the gap between User Reviews
Related
Code 
Snippetspublic void switchTheme(
int type, ‚Ä¶) {‚Ä¶}
Existing: Unsupervised
Preprocessing <UR, Code > Pairs 
CollectingHow do I 
switch to the 
dark them?User 
ReviewReview2Code
CodeSearch : Supervised
Information 
Enhancement
Representation
Searching by 
Similarity MeasuringPreprocessing
Model Training
Predicting and 
RankingFigure 2: Two types of Review2Code Systems.
and source code, which are proven to contribute to improving the
predicting performance.
Due to the lack of large-scale labeled data, i.e., truly associated
<ùëàùëÖ,ùê∂ùëúùëëùëí > pairs, there are no supervised learning-based models
dedicated to judging whether there is an association between a
givenùëàùëÖand aùê∂ùëúùëëùëí . However, if there are enough labeled relevant
or irrelevant < ùëàùëÖ,ùê∂ùëúùëëùëí > pairs, a code retrieval method based on
deep learning, such as GraphCodeBert [ 19], CodeBert [ 11], UNIF [ 5]
and CAT [ 22], can be used to train the model that calculates the
similarity between User Reviews and code snippets. When using
the model, User Reviews and candidate code snippet pairs are input
to the model one by one, the correlation probability is calculated
and sorted, and the top- ùëòcode snippets are the prediction results.
Figure 2 demonstrates the comparisons between the above two
schemes. The inputs are User Reviews and code snippets, and the
outputs are code lists associated with the reviews. Next, we will
first introduce various specific methods of the two schemes in detail
w.r.t. Figure 2, then determine the systems to be benchmarked and
illustrate the reasons, as well as present the general process of
benchmark construction.
3.1 Unsupervised Systems
Existing Review2Code methods are URR [ 7], ChangeAdvisor [ 49],
ReviewSolver [ 72], Where2Change [ 74], RISING [ 76]. We first in-
troduce them from perspectives of preprocessing, vector represen-
tation, information enhancement, similarity calculation, etc.
(1) URR [ 7] classifies User Reviews into predefined categories
and defines a set of structure categories for source code files. Then
the model utilizes Lucene Core to perform a search and returns the
top-scoring source files. A boosting score will be added during the
search if the User Review and source file have a matching taxonomy
category.
(2) ChangeAdvisor [ 49] clusters User Reviews by HDP (Hierar-
chical Dirichlet Processes), the goal of which is to group together
User Reviews expressing similar requirements. Then it measures
the similarity between source code snippets and the clusters with
cosine similarity. The output is a list in descending order according
to the similarity score.
536Lighting Up Supervised Learning in User Review-Based Code Localization: Dataset and Benchmark ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
(3) ReviewSolver [ 72] utilizes a parse tree to capture the semantic
information of User Reviews and creates APG (Android Property
Graph) of the App. Then it measures the similarity between phrases
from the tree and components from the graph. Finally, it returns
the top-ùëÅfunction errors (App-Specific Errors, General Errors) of
the source code most relevant to the given User Review.
(4) Where2Change [ 74] is similar to ChangeAdvisor in clustering
and similarity calculation. However, before similarity calculation, it
additionally measures the similarity between User Review clusters
and issue reports. When the similarity score is higher than the
threshold, the issue report will be linked to the cluster for enriching
User Reviews.
(5) RISING [ 76] is mainly composed of two steps: clustering
and similarity calculation which is similar to ChangeAdvisor and
Where2Change. In addition, it exploits commit messages as a medium
to fill the lexicon gap between User Reviews and source code snip-
pets and uses a heuristic-based method to infer parameter ùëòin
ùêæ-means [41].
3.2 Supervised Candidates
As illustrated in Section 2.3, the tasks of Review2Code and Code
Search are very similar, where input is described in natural language,
and the output is a code list associated with the input. The difference
lies in the definition of the relationship between inputs and outputs.
Code Search describes the functions of existing code snippets in
natural language, while Review2Code describes the functions to
be developed or the problems existing in the current functions in
natural language. However, in terms of models, Review2Code can
reuse the models of Code Search and train on real < ùëàùëÖ,ùê∂ùëúùëëùëí > data.
The main existing Code Search models are:
(1) UNIF (Embedding Unification) [ 5] is a supervised extension
of the NCS (Neural Code Search) [ 57] technique, which uses word
embedding matrices to learn the mapping from tokens into vectors.
The model computes a simple average to combine the query token
embeddings into a single vector which is present in NCS as well.
To combine each bag of code token vectors, it utilized an attention
mechanism [ 2] to measure a weighted average. Finally, it returns a
list ranked by similarity score between the two single vectors.
(2) CAT [ 22] is an extension of CT (A Baseline Code and Text
Model) based on AST (Abstract Syntax Tree) [ 30], which can capture
the structural characteristics of the code. The model performs Bi-
LSTM [ 16] and max pool to obtain the output vectors of each input
(natural language, code and AST) and then concatenated the output
vectors of code and AST to a single vector representing source code.
Finally, it returns a ranked list according to the similarity between
vectors representing the source code and query.
(3) CodeBert [ 11] is the first bimodal pre-trained model for mul-
tiple programming languages (PL) and natural languages (NL). It
uses "bimodal" data including NL-PL pairs and a larger scale of
"unimodal" data to pre-train the model, so that it can well learn the
general-purpose representations of NL and PL. The code snippet
and the natural language are input to the model together. Then
a softmax layer is connected to the representation of [ùê∂ùêøùëÜ]to
calculate relevance scores between them to rank candidate code
snippets.(4) GraphCodeBert [ 19] is the first pre-trained model that lever-
ages the semantic structure of code to learn code representation. By
using data flow in the pre-training stage, it considers the internal
structure of code and provides key code semantic information for
code understanding. The code snippet and the natural language are
respectively input to the model to obtain their vector representa-
tions. Then the inner product of the two vectors is calculated as
relevance scores between them to rank candidate code snippets.
3.3 Selected Systems
We used the following criteria to choose Review2Code methods for
our benchmark:
(1)Support Java language . The ultimate goal of our work is to
provide valuable modification suggestions through Review2Code
for the development and maintenance of mobile applications in
Android platform, which are programmed in Java language. So, the
methods selected should support Java language.
(2)Has publicly available source code or can be fully re-
implemented . If a method has neither publicly available source
code nor complete and accurate illustration for re-implementation,
there may be deviations between the reproduced version and the ac-
tual version, which will affect the performance of the method and is
disadvantageous to our evaluation. Therefore, we exclude methods
that lack key information or required data for implementation.
(3)Be representative, i.e., achieving SOTA performance
from anyone‚Äôs perspective or new enough . In order to make the
comparisons more convincing, we select SOTA of existing unsuper-
vised learning-based Review2Code methods and newly published
(within three years) supervised learning-based candidates.
Taking the above factors into consideration, we finally selected
six methods for our benchmark, namely, Where2Change, RISING,
UNIF, CAT, CodeBert and GraphCodeBert.
Upon deciding the benchmarking systems, we reproduce them
referring to available source code (if any) and prepare datasets ac-
cording to the requirements of different methods. While the meth-
ods and dataset are prepared, we conduct experiments and finally
discuss the results.
4 DATASET CONSTRUCTION
The existing unsupervised methods do not need manually labeled
data (except to evaluate the performance). In order to build a method
based on supervised learning, there must be sufficient relevant < ùëàùëÖ,
ùê∂ùëúùëëùëí > (concretely < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë >) pairs as the ground truth for
training. This section details the process of constructing the dataset
for lighting up supervised learning for Review2Code. As shown in
Figure 3, the procedure consists of raw data collecting, annotating
preparation, annotating, and dataset statistical analysis.
4.1 Data Collecting
The collection of raw data includes selecting the candidate apps
and obtaining their User Reviews, Issues/Pulls and source code. The
details are as follows:
537ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo Huang, and Bin Luo
Ground Truth
CollectingData Collecting
 Preparation Annotating Statistical Analysis
App Selecting
User Reviews
Issues & Pulls 
Source CodeData 
PreprocessingAnnotator 
Selecting
UR-Issue/Pull -
Code LinkingApp Knowledge 
LearningAnnotating 
System
Annotating 
Training
First -round 
Annotating
Cross CheckingTrue Related 
<UR, Code > 
Pairs
Coarse -grained 
Statistics
Fine-grained 
Statistics
Figure 3: Procedure of Dataset Construction.
Table 2: Initial statistics of the selected five Apps.
Name Type Stars Reviews Files Methods Issues Pulls
K-9 Mail Comm. 6.2k 16,301 309 3,589 3,366 2,368
AntennaPod Vide. 3.8k 6,152 415 3,277 3,107 2,427
Cgeo Trav. 1.1k 6,342 963 9,625 7,233 4,906
Anki Educ. 4.2k 11,858 366 5,114 5,581 4,379
Termux Life 9.4k 18,989 146 1,750 1,841 221
App Selecting. We select Apps from the Google Appstore and
available open-source non-fork GitHub repositories2. We first iden-
tify all software projects of different categories, e.g., Business, Com-
munication, Video, Lifestyle, Travel, Education, etc., then sort them
bypopularity (denoted by the stars) and select the top 100 most
popular ones. At the same time, we filter out those with less than
50 Issues and 2000 reviews and those having no license or unclear
license. Finally, 5 Apps from 5 different categories are selected,
namely, K-9 Mail ,AntennaPod ,Cgeo ,Anki , and Termux .
Raw Data Collecting. First, we collect User Reviews of the 5
selected Android Apps from Google‚Äôs AppStore (i.e., Google Play).
Then, we get their source code from their corresponding open-
source repositories on GitHub. We find that Issues and Pulls on
GitHub not only contain natural language descriptions but also
are associated with relevant source code in method-level (i.e., are
linking to changes in the code). On the one hand, Issues and Pulls
are raised by users on GitHub (i.e., usually software programmers)
when finding bugs or proposing improvement feature suggestions,
which means they share the same valuable information with Prob-
lem Discovery and Feature Request in User Reviews. On the other
hand, after the Issues or Pulls are solved, code changes that result
from them are also linked to them. Considering these, it will be able
to speed up annotating related code for User Review if Issues and
Pulls related to User Reviews can be recommended at first since
the similarity between NL and NL is easier to be measured than
that between NL and Code. Therefore, we also crawl the Issues and
Pulls of these Apps from GitHub for assisting dataset annotating.
Statistics of initially collected data are shown in Table 2.
4.2 Preparation
The preparation includes: selecting qualified annotators, organizing
annotators to learn knowledge of the selected Apps, preprocessing
the raw data, and constructing the data format as conveniently as
possible for the annotators.
2Found here: https://github.com/pcqpcq/open-source-android-appsAnnotator Selecting. We selected 36 annotators with more
Software Requirements Engineering experience from 54 applicants
(has obtained a bachelor‚Äôs degree in Software Engineering) who
met the following criteria: (1) Have participated in the development
(i.e., coding) of at least two software (Ensure the ability of reading
and understanding code), (2) Have participated in Requirements
Engineering of at least two software projects (Ensure understanding
UR), (3) Have raised Issue or Pull Request for any open source
project on Github (Ensure understanding Issue and Pull Request),
and (4) Have data annotating experience.
Knowledge Learning. First, annotators are assigned as much
as possible to annotate Apps of the type they are familiar with,
as well as considering distributing annotating tasks (measured by
the number of URs) equally among annotators. Then, they are
organized to learn relevant knowledge of the App assigned to them,
including project introduction and background, user manual (if
any), feature list (if any), architecture, concrete code files, etc.
Preprocessing. ForUser Reviews , the preprocessing is to filter
out meaningless ones using the BERT model trained in [ 24] for clas-
sifying user reviews into three types, namely, Problem Discovery
(PD), Feature Request (FR), and Meaningless (including Information
Seeking and Giving). PD and FR in the collected User Reviews are
reserved for further annotating3. For Issues and Pull Requests , those
having no natural language descriptions or associated code files
are filtered out. Besides, tags (e.g., version tag, error tag) in the NL
descriptions are also removed. For Code , we use the AST parser4
to parse each code file we crawled down to extract names of each
method. Besides, start line position, end line position and entire
code snippet of each method related to each Issue/Pull are also
extracted. Relation between code files and methods contained by
them are also recorded for further usage. Eventually, the numbers
of reserved artifacts of K-9 Mail ,AntennaPod ,Cgeo ,Anki ,Termux
are: (1) Reviews: 6152, 1018, 1115, 2125, 1930, (2) Issues: 105, 1626,
2567, 314, 76, and (3) Pull Requests: 1462, 1578, 2271, 1524, 101.
Data Linking. To annotate truly related < ùëàùëÖ,ùê∂ùëúùëëùëí > pairs, an-
notators should go through all Code of the App for each UR the-
oretically. Since the annotators have already learned necessary
knowledge of the App, they can locate candidate code files faster
than really going through all files. In order to further speed up
the annotating procedure, we utilize Issues/Pulls for highlighting
candidate Methods in code files for annotators. Concretely, we first
use BM255to automatically link each UR with their potentially re-
lated Issues/Pulls, then recommend codes that are naturally related
to these Issues/Pulls as candidates for annotators. Recall that in
preprocessing, we have extracted the modification range of related
code files for each Issue/Pull, as well as the method name and line
range of each method, so, we could easily link each Issue/Pull to
3Even if the Meaningless UR is predicted to be PD or FR, it will not affect the ultimate
annotated dataset quality. Because that we will manually judge whether each reserved
UR is Meaningless or not during annotating. If UR of PD/FR is predicted as Meaningless,
it will reduce the number of URs to be annotated, which may lead to missing some
valuable <ùëàùëÖ,ùê∂ùëúùëëùëí > pairs, but will not affect the quality of annotation either. This
will be further stated in Threats to Validity.
4https://github.com/javaparser/javaparser
5In order to apply BM25, we conduct preprocessing steps for both UR and Issue/Pull
following https://github.com/RaRe-Technologies/gensim, including filtering neither
English nor Numeric characters, contractions expansion (e.g, It‚ÄôstoIt is), lemmatization
(e.g, didtodo), uppercase conversion, and remove stop words.
538Lighting Up Supervised Learning in User Review-Based Code Localization: Dataset and Benchmark ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
Figure 4: Screenshot of the Review2Code Annotating System.
related methods. To this end, URs are linked to potentially related
Issues/Pulls, and Issues/Pulls are linked to related methods.
4.3 Annotating
For facilitating the annotating procedure, we develop a simple but
sufficient annotating tool. The annotating is conducted after ad-
equate annotation training for annotators. After the first round
annotating, each pair of < ùëàùëÖ,ùê∂ùëúùëëùëí > is verified by another annota-
tor during the second round data checking.
Annotating System. Figure 4 is the screenshot of the annotating
tool. There are three parts. The left part displays the UR to be
annotated and the candidate Issues/Pulls that are linked to UR.
More Issues/Pulls can be divided via turning pages. The middle
part is the directory tree of the App‚Äôs source code, where the leaf
nodes are method names. Methods linked to the selected Issue/Pull
in the left part are highlighted in Red. For any method (highlighted
or not), if the annotator believes it is related to the current UR,
he/she should check the checkbox in front of that method. The right
part presents the code file containing the clicked Method. If the
annotator believes that the current UR is meaningless, he/she should
click the Meaningless button. Upon all related methods are checked,
theConfirm button should be clicked. The Previous andNext buttons
are for switching to the previous and next UR, respectively. While
the current UR is changed, all corresponding contexts will switch
as well.
Annotating Training. Four hours were spent on training an-
notators to familiarize them with the system and the concept of
<ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > pair. For each App, the first ten User Reviews are
used as annotating examples for illustrating under what conditions,
the methods should be checked or not. Then, the annotators are
tested through annotating five User Reviews and then correspond-
ing Methods which are randomly selected for them. Until they reach
a consensus with (i.e., understand and agree with) the ground truth
of the test data, they cannot start to annotate.
Annotating. During the first round of annotating, each User
Review is annotated by two annotators. Both annotated results
(truly related < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > pairs) of each UR annotated by two
different annotators will be saved for the next Checking round. At
this stage, each annotator is assigned about 1,000 user reviews on
average and 24 annotators are joining. It cost us 20 days on average
to finish the first round annotation (Please note that the time costis also influenced by the scale of Code. Different Apps cost different
time costs. We report the average time cost of the five Apps). That is
about 16 man-months (for annotating about 12,000 user reviews).
Checking. While checking, previously annotated results of each
UR (except those labeled as Meaningless by any annotator during
the first round) will be checked by another annotator who does not
participate in the first round annotating. For convenience, we also
develop a simple tool (almost the same as the one shown in Figure 4)
to facilitate the checking procedure. The system will automatically
select the checkbox of the previously annotated method. Moreover,
the method annotated by one or two annotators in the previous
round will be highlighted in different colors. If the checker finds
that the results of the UR annotated by the previous two annotators
are completely different, he needs to discuss with them to reach
an agreement for that UR. The checking time cost is much less
than the first round annotating and it is about 4 man-months (12
annotators, finished within 9 days on average).
4.4 Statistical Analysis
After the above annotation process, we finally construct the Re-
view2Code dataset supporting the supervised learning approaches.
The raw data of Review2Code dataset consists of 5 Android mobile
applications, with 8,169 Java methods and 1,639 Java files. Table 3
summarizes the coarse-grained labeling results. Remarkably, < ùëàùëÖ,
ùêπùëñùëôùëí> is automatically built according to the marked < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë >.
Overall, excluding 47,186 automatically filtered during Preprocess-
ing (i.e., automatically labeled as Meaningless), 6,758 User Reviews
have relevant code files and methods, while 5,582 User Reviews
have no relevant code files or methods (i.e., manually labeled as
Meaningless). In total, the Review2Code dataset contains 68,359
pairs of <ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > and 33,440 pairs of < ùëàùëÖ,ùêπùëñùëôùëí>. Compared
with other deep learning datasets, the proposed dataset is affordable
to train deep learning models for the Review2Code task.
To gain a deeper understanding of the Review2Code dataset, a
simple but informative statistical analysis is conducted. Figure 5
shows the distribution of the number of reviews associated with
different numbers of methods and files in each project. It is easy
to get User Reviews that have [1, 5] relevant code snippets are
the most in all five Apps regardless of method-level or file-level,
and separately average account for 0.473 and 0.641 in all projects.
Through statistical calculation, 12.247 methods and 6.421 files are
averages associated with one User Review for each app. It proves
that User Reviews are informative and often involve many feature
requests and problem discoveries, and proves that Review2Code is
valuable but difficult.
5 EXPERIMENTAL SETUPS
Next, experimental setups of the benchmark are introduced, includ-
ing settings, evaluating metrics and research questions (RQ, as well
as methodologies for answering RQs).
5.1 Settings
For all approaches, we only conduct Method-Level predicting, i.e.,
the output of each of the six selected methods is the list of Meth-
ods. To derive the predicted list of Files, we substitute the Method
539ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo Huang, and Bin Luo
Table 3: Statistics of the annotated Review2Code dataset.
NameReviewsMethods FilesPairs
Relevant Irrelevant <UR.,M.> <UR.,F.>
K-9 Mail 3,209 2,943 2,741 371‚àó33,351 16,502
AntennaPod 595 423 1,498 378 9,279 4,777
Cgeo 658 457 1,623 477 5,420 2,952
Anki 1,211 914 2,118 361 13,276 5,624
Termux 1,085 845 189 52 7,033 3,585
‚àóCurrently, the K-9 Mail project is now rewriting with Kotlin, which leads to
the reduction of the number of Java files in the latest version of K-9 Mail on
GitHub (i.e., less than 371).
1477
275335576531640
116 131277 230325
71 62135 121767
133 130223 203
02004006008001000120014001600
K-9 Mail AntennaPod Cgeo Anki TermuxNumber of ReviewsNumber ranges of Related Methods:(a) Distributions of UR w.r.t. # of related Methods  
[1,5] [6,10] [11,15] >15
2049
338421751 775
662
129 124311222 231
55 38 65 79267
73 75 8490500100015002000
K-9 Mail AntennaPod Cgeo Anki TermuxNumber of ReviewsNumber ranges of Related Files:(b) Distributions of UR w.r.t. # of related Files  
[1,5] [6,10] [11,15] >15
Figure 5: Distributions of URs w.r.t. related Methods/Files.
with the File containing it and repeated Files are skipped6. We
split the dataset into 70-20-10 train/valid/test proportions in the
experiments. To ensure the comparison of experimental results
statistically significant, each experiment is conducted for times and
the average value is reported.
Hardware settings . For unsupervised learning-based methods,
we ran on a computer with 20-core CPU with Intel(R) Xeon(R)
Gold 6248 CPU @ 2.50GHz and 250G sever memory. For super-
vised learning-based models, we conducted experiments on two
computers of the same settings and each has four NVIDIA Tesla
V100 (32GB memory) GPUs.
Software settings . The environment settings are as follows:
Python version is 3.9.7 and Java version is 1.8. Pytorch version is
1.10.1 and Hugging Face Transformer version is 4.15.0.
6While calculating File-level evaluating metrics, the top- ùêæfiles should be firstly in-
ferred from the recommended methods.Hyper parameters . Recall that we re-implement Where2Change
according to its paper and set the number of User Reviews‚Äô clus-
ters to 10. For RISING, we adopt the setting of hyper parameters
the same as the initial implementation. For supervised models, we
mainly adjust the settings of epoch, learning rate and batch size.
According to the order of UNIF, CAT, CodeBert and GraphCodeBert,
epoch is 20, 20, 8, 10, learning rate is 1e-2, 1e-3, 1e-5, 2e-5 and batch
size is 64, 512, 64, 16 respectively. For the others, we follow all the
hyper-parameters on the official code. All of our implementations
of different methods, as well as details of hyper parameters and
running environment, will be made publicly available.
5.2 Evaluation Metrics
To evaluate the performance of the models, we employe the Top- ùëò
and Mean Reciprocal Rank ( ùëÄùëÖùëÖ ) as metrics both of which are used
in Where2Change [74] and RISING [76]. Their definitions are:
(1)Top-ùëòis the proportion of the reviews whose retrieved top- ùëò
code snippets contains at least one in the ground truth:
ùëáùëúùëù‚àíùëò(ùëàùëÖ)=√ç
ùë¢ùëü‚ààùëàùëÖùëñùë†ùê∂ùëúùëüùëüùëíùëêùë°(ùë¢ùëü,ùë°ùëúùëù‚àíùëò)
|ùëàùëÖ|(1)
where|ùëàùëÖ|is the total number of UR and the ùëñùë†ùê∂ùëúùëüùëüùëíùëêùë°(ùë¢ùëü,ùë°ùëúùëù -ùëò)
function returns 1 if at least one of top- ùëòcode snippets actually is
relevant to the review ur and returns 0 otherwise.
(2)MRR is the multiplicative inverse of the rank of first correctly
retrieved code snippet:
ùëÄùëÖùëÖ =1
|ùëàùëÖ||ùëàùëÖ|‚àëÔ∏Å
ùëñ=11
ùëüùëéùëõùëòùëñ(2)
whereùëüùëéùëõùëòùëñis the rank of the first correctly retrieved code snippet.
5.3 Research Questions
We organize the benchmark through four research questions.
RQ1 : In the experimental setting, under which conditions super-
vised learning-based models beat the unsupervised learning-based
methods?
Method : We test the performance of different models on each
App according to the Train on Single (ToS) scheme which means
that the model is trained on each App training set respectively and
tested on its test set. For the supervised models, we train each model
(if it is a pre-training model, then fine-tuning) with the training set,
adjust the parameters with the validation set, and evaluate with the
test set. For unsupervised methods, verify the performance directly
on the test set.
RQ2 : Does the performance of supervised learning-based models
on different mobile Apps influenced by the corresponding data
scale? How?
Method : Data scale includes the number of < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > pairs,
Methods or Files, and their ratio. We rank the result of models
on different Apps according to this index to verify whether the
performance is related to the corresponding order of the data scale.
RQ3 : Can supervised learning-based Review2Code models trans-
fer knowledge across different Apps?
Method : Besides the ToS, we adopt another two training schemes
for answering this question and they are: (1) Train on All (ToA)
means that the model is trained on all training data of all the five
540Lighting Up Supervised Learning in User Review-Based Code Localization: Dataset and Benchmark ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
Table 4: The file-level and method-level results of 5 apps by 6 models on ToStraining scheme.
APP Name ModelMethod-level File-level
Top-1 Top-3 Top-5 Top-10 Top-20 MRR Top-1 Top-3 Top-5 Top-10 Top-20 MRR
K-9 MailWhere2Change 0.002 0.005 0.007 0.014 0.026 0.006 0.034 0.091 0.132 0.161 0.337 0.095
RISING 0.050 0.089 0.137 0.197 0.264 0.090 0.077 0.185 0.276 0.363 0.452 0.161
UNIF 0.024 0.050 0.142 0.197 0.233 0.052 0.026 0.087 0.173 0.296 0.409 0.052
CAT 0.014 0.043 0.048 0.065 0.106 0.034 0.060 0.108 0.144 0.204 0.380 0.102
CodeBert 0.118 0.246 0.299 0.411 0.508 0.205 0.178 0.355 0.421 0.583 0.698 0.280
GraphCodeBert 0.290 0.421 0.486 0.579 0.698 0.369 0.336 0.539 0.614 0.732 0.832 0.437
AntennaPodWhere2Change 0.017 0.017 0.017 0.033 0.050 0.020 0.083 0.133 0.150 0.233 0.283 0.120
RISING 0.083 0.150 0.233 0.283 0.333 0.143 0.150 0.283 0.367 0.450 0.517 0.236
UNIF 0.067 0.150 0.167 0.267 0.367 0.089 0.067 0.200 0.233 0.500 0.533 0.102
CAT 0.067 0.067 0.067 0.100 0.150 0.076 0.083 0.317 0.317 0.467 0.533 0.209
CodeBert 0.083 0.200 0.217 0.317 0.400 0.162 0.133 0.350 0.433 0.483 0.617 0.257
GraphCodeBert 0.183 0.333 0.417 0.483 0.550 0.258 0.317 0.450 0.617 0.683 0.750 0.394
CgeoWhere2Change 0.000 0.000 0.000 0.000 0.000 0.001 0.000 0.000 0.015 0.076 0.182 0.025
RISING 0.076 0.121 0.182 0.273 0.318 0.120 0.121 0.242 0.364 0.439 0.500 0.215
UNIF 0.000 0.015 0.076 0.106 0.197 0.019 0.000 0.061 0.136 0.288 0.470 0.039
CAT 0.015 0.076 0.091 0.182 0.212 0.048 0.015 0.091 0.167 0.303 0.394 0.064
CodeBert 0.076 0.182 0.227 0.288 0.379 0.125 0.091 0.197 0.242 0.379 0.606 0.150
GraphCodeBert 0.152 0.273 0.333 0.439 0.561 0.224 0.212 0.379 0.485 0.682 0.788 0.306
AnkiWhere2Change 0.000 0.000 0.008 0.008 0.041 0.004 0.074 0.098 0.148 0.295 0.475 0.130
RISING 0.049 0.148 0.189 0.287 0.393 0.123 0.238 0.459 0.541 0.639 0.689 0.363
UNIF 0.008 0.066 0.082 0.148 0.287 0.022 0.041 0.230 0.352 0.639 0.877 0.079
CAT 0.057 0.074 0.172 0.221 0.295 0.085 0.164 0.402 0.451 0.549 0.705 0.248
CodeBert 0.107 0.270 0.344 0.443 0.533 0.193 0.287 0.508 0.631 0.787 0.869 0.392
GraphCodeBert 0.189 0.385 0.434 0.525 0.598 0.274 0.295 0.541 0.607 0.770 0.861 0.396
TermuxWhere2Change 0.009 0.055 0.156 0.239 0.303 0.076 0.018 0.596 0.670 0.725 0.844 0.330
RISING 0.275 0.431 0.560 0.679 0.780 0.394 0.404 0.615 0.743 0.826 0.862 0.537
UNIF 0.037 0.413 0.523 0.679 0.752 0.251 0.037 0.505 0.633 0.817 0.954 0.297
CAT 0.000 0.294 0.394 0.450 0.587 0.164 0.000 0.358 0.486 0.670 0.826 0.194
CodeBert 0.284 0.367 0.505 0.670 0.817 0.344 0.385 0.624 0.752 0.881 0.982 0.446
GraphCodeBert 0.376 0.477 0.560 0.688 0.807 0.433 0.477 0.697 0.789 0.908 0.963 0.559
Apps and tested separately on each App‚Äôs testing set. (2) Train on
Part (ToP) means that the model is trained on four Apps and tested
on the other one.
RQ4 : Would utilizing comments in source code improve the
performance of supervised learning-based Review2Code models?
Method : The supervised learning-based models with higher
performance are used to check the usefulness of code comments in
Review2Code.
6 DISCUSSION BY ANSWERING RQS
Answering RQ1 : Unsupervised learning vs. supervised learning.
The performance of different methods on each App (following the
ToS scheme) is shown in Table 4. It shows that the supervised model
GraphCodeBert performs best on all Apps, followed by CodeBert.
However, by conducting a case study, we surprisingly find that there
are cases in which unsupervised methods beat the supervised ones.
Taking Figure 6 as an example, (if the retrieved Method belongs
to GT, it is highlighted in yellow), the result of RISING is com-
parative to those of GraphCodeBert and CodeBert, and obviously
outperforms UNIF and CAT.
The potential reason forGraphCodeBert and CodeBert outper-
forming the others is that they have learned the knowledge of
natural language and code via pre-training. In addition, Graph-
CodeBert introduces the control flow graph which provides key
code semantic information and enhances structural information for
code understanding. This is the reason that GraphCodeBert beats
CodeBert. The reasons for the cases that RISING performs betterthan CAT and UNIF lie in that RISING bridges the lexicon gap by
combining commit messages and code snippets.
To this end, it is meaningful to light up supervised learning
for User Review-based code localization considering that Graph-
CodeBert outperforms RISING to be the new state-of-the-art Re-
view2Code approach.
Answering RQ2 : Impacts of Apps on supervised learning. Through
experimental analysis, we find the number of < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > pairs,
methods, or files has no obvious relationship with the performance
of models. However, as shown in Figure 7, the MRR value of each
model increases along with the rise of the ratio of the number of
<ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > pairs and the number of methods or files. The rea-
sons are two-fold. On the one hand, the larger the number of < ùëàùëÖ,
ùëÄùëíùë°‚Ñéùëúùëë > pairs, the more sufficient the model is trained. On the
other hand, the smaller the number of methods or files, the greater
the probability that the model will find the correct method or file.
However, there is an outlier: CodeBert on Anki has a better file-
level MRR than that it achieves on K-9 Mail . We carefully study
the output results of CodeBert on Anki and discover that Code-
Bert incorrectly predicts many methods which are just right in the
files containing the correct methods. Recall that the top- ùëòfiles are
inferred by predicted methods, as well as the ground truth files
are generated from ground truth methods, it is more likely to lead
the misjudgment at the file level for Apps containing more meth-
ods on average in one single code file. Unsurprisingly, Anki really
541ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo Huang, and Bin Luo
‚ÄúFor some reason it 
won't install packages. 
I tried everything but 
nothing .‚ÄùInput User Review: Ground Truth:
buildEnvironment
onServiceConnected
setupIfNeeded
onStartCommand
createTermSessionReview2Code
Where2Change:
getTerminalTranscri‚Ä¶
TerminalIOPreferen ‚Ä¶
ExtraKeysView
TermuxService
TermuxActivity
RISING:
determineZipUrl
buildEnvironment
onStartCommand
onContextItemSelected
onServiceConnectedUNIF:
setupStorageSymlinks
determineZipUrl
actionServiceExecute
unCommandService
onServiceConnected
CAT:
reloadFromProperties
sendKey
reload
onContextItemSelected
buildEnvironmentCodeBert :
replaceAliasesdetermineZipUrlbuildEnvironment
createTermSession
onServiceConnected
GraphCodeBert :
determineZipUrlbuildEnvironment
onServiceConnectedsetupIfNeeded
onStartCommand
Figure 6: A case study: Top5 Methods of different approaches.
0.150 0.257 0.392 
0.280 0.446 0.306 0.394 0.396 0.437 0.559 
0.000.100.200.300.400.500.60
Cgeo AntennaPod Anki K-9 Mail TermuxMRR V alue
Apps sorted in ascending order of < UR,Method >/Fileratio(b) MRR: File -Level
CodeBert GraphCodeBert
0.125 0.162 0.193 0.205 0.344 0.224 0.258 0.274 0.369 0.433 
0.000.100.200.300.400.50
Cgeo AntennaPod Anki K-9 Mail TermuxMRR V alue
Apps sorted in ascending order of < UR,Method >/Method ratio(a) MRR: Method -Level
CodeBert GraphCodeBert
Figure 7: Statistics of MRR related to the scale of Apps.
has the largest average number of methods per code file (13.973
methods/file) among all Apps.
Answering RQ3 : Transferability of supervised learning-based Re-
view2Code models. Firstly, we conducted experiments following the
ToAtraining scheme. What we expect is that the model can learn
semantic information about reviews and code better through more
training data. However, according to Table 5, we are surprised to
find that this way does not improve the performance of models
compared with the results of ToS. Then, we conducted the ToPex-
periment by excluding the training data which comes from the same
App as the test dataset. As Table 5 demonstrates, the experimental
results of ToPare significantly lower than ToS.
0.050.150.250.350.450.55
Cgeo AntennaPod Anki K-9 Mail TermuxMRR V alue
Apps sorted in ascending order of < UR,Method >/Method ratioMRR: Method -Level
CodeBert
CodeBert+Comment
GraphCodeBert
GraphCodeBert+CommentFigure 8: Method-level MRR of CodeBert/GraphCodeBert:
w/o and w comments in code.
According to these, we suppose that the performance of super-
vised Review2Code models mainly relies on the training data within
specific relevant domain knowledge. Expanding dataset from appli-
cations of different types adds noise to the original training dataset.
To verify this, we conducted a statistic of the proportion of the
repeated words between different Apps‚Äô code tokens, which is no
more than 0.15, fully proving the significant difference between
Apps. However, these results cannot deny the transferability of
supervised learning-based Review2Code models. We are looking
forward to expanding the Review2Code dataset to further study
these models‚Äô transferability.
Answering RQ4 : Impacts of natural language comments in code.
Figure 8 compares the results before and after adding comments
to GraphCodeBert and CodeBert models. It proves that comments
have a certain impact on the performance of both models. However,
whether it has a positive or negative influence is rather mixed.
Generally, the performance of CodeBert decreases after adding
comments, while that of GraphCodeBert is not significantly affected.
Two special points deserve mention: (1) the performance of each
model on Anki always gets worse, and (2) the performance of each
model on Termux always gets better. To try to interpret this, we
further calculate the average overlap of words between User Review
and Comments in code of each < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë > pair for these two
Apps, but there is no big difference between Anki (1.33 words) and
Termux (0.67 words). We leave this as a challenge for future work.
7 THREATS TO VALIDITY
Threats to the validity of our work come from three aspects:
‚Ä¢Annotating Deviation . Since the annotators are not the
actual developers of the Apps, the overall processing still has
a bias, which has been reduced as much as possible through
adequate training, manual interventions, and cross-checking.
Besides, missing links between UR and Code will lead to
fewer positive samples, and even false negative samples,
which will lead to a decrease in model performance.
‚Ä¢Scale of the Dataset . More Apps and more annotated < ùëàùëÖ,
ùëÄùëíùë°‚Ñéùëúùëë > pairs would benefit both the discussion on the
benchmark and the researchers who are interested in su-
pervised learning-based Review2Code. We will continue to
annotate more data to enrich Review2Code dataset.
542Lighting Up Supervised Learning in User Review-Based Code Localization: Dataset and Benchmark ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
Table 5: The file-level and method-level model average results of 5 apps on different training scheme.
Training Scheme ModelMethod-level File-level
Top-1 Top-3 Top-5 Top-10 Top-20 MRR Top-1 Top-3 Top-5 Top-10 Top-20 MRR
ToSWhere2Change 0.006 0.015 0.038 0.059 0.084 0.022 0.042 0.184 0.223 0.298 0.424 0.140
RISING 0.107 0.188 0.260 0.344 0.418 0.174 0.198 0.357 0.458 0.543 0.604 0.302
UNIF 0.027 0.139 0.198 0.279 0.367 0.086 0.034 0.216 0.306 0.508 0.649 0.114
CAT 0.031 0.111 0.154 0.204 0.270 0.081 0.065 0.255 0.313 0.439 0.568 0.164
CodeBert 0.134 0.253 0.318 0.426 0.527 0.206 0.215 0.407 0.496 0.622 0.754 0.305
GraphCodeBert 0.238 0.378 0.446 0.543 0.643 0.312 0.327 0.521 0.615 0.755 0.839 0.418
ToAUNIF 0.028 0.084 0.171 0.212 0.326 0.059 0.042 0.129 0.277 0.419 0.622 0.089
CAT 0.004 0.124 0.134 0.155 0.198 0.075 0.033 0.195 0.248 0.411 0.532 0.162
CodeBert 0.102 0.193 0.257 0.337 0.466 0.160 0.173 0.374 0.463 0.634 0.749 0.262
GraphCodeBert 0.226 0.333 0.401 0.503 0.605 0.291 0.334 0.502 0.609 0.737 0.833 0.418
ToPUNIF 0.025 0.073 0.094 0.154 0.228 0.052 0.202 0.289 0.330 0.427 0.544 0.237
CAT 0.018 0.122 0.138 0.160 0.215 0.082 0.032 0.175 0.208 0.300 0.500 0.126
CodeBert 0.060 0.133 0.178 0.272 0.389 0.110 0.160 0.313 0.416 0.573 0.732 0.251
GraphCodeBert 0.075 0.146 0.202 0.317 0.394 0.126 0.193 0.369 0.473 0.613 0.738 0.291
‚Ä¢Selecting of the benchmarking systems . Currently, only
popular Code Search approaches are selected as representa-
tives of supervised learning-based Review2Code approaches.
We will use more supervised learning methods adopted by
SE community for Review2Code in the future.
8 CONCLUSION AND FUTURE WORK
In this paper, we try to light up supervised learning-based methods
for Review2Code by constructing a novel dataset and benchmark.
First, the procedure for constructing the dataset is described in
detail, which includes App selecting, raw data collecting, anno-
tator selecting, App knowledge learning, data preprocessing, UR-
Issue/Pull linking and Issue/Pull-Code linking, annotating system
implementing, annotator training, data annotating and checking.
The ultimate dataset consists of 5 Android Apps and 68,359 pairs of
truly related < ùëàùëÖ,ùëÄùëíùë°‚Ñéùëúùëë >. Then, the benchmark having two unsu-
pervised learning-based methods (Where2Change and RISING) and
four supervised learning-based models (UNIF, CAT, CodeBert and
GraphCodeBert) is constructed. Finally, four research questions (on
the overall performances of different methods, impacts of project
scales, model transferability, and usefulness of comments in code)
are proposed and answered for discussing the benchmark results.
Which deserves mention is that GraphCodeBert outperforms the
state-of-the-art unsupervised learning-based Review2Code meth-
ods. This also proves to us the value of lighting up supervised
learning-based Review2Code.
As to future work, we will continue to enrich the dataset and
dedicated on proposing better Review2Code approach.
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of
China (61802167), Natural Science Foundation of Jiangsu Province,
China (BK20201250), and NSF award 2034508. We also thank the
reviewers for their helpful comments. Chuanyi Li and Jidong Ge
are the corresponding authors.
REFERENCES
[1]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Unified Pre-training for Program Understanding and Generation. In Pro-
ceedings of the 2021 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2021 .
2655‚Äì2668.
[2]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: learning
distributed representations of code. Proc. ACM Program. Lang. 3, POPL (2019),
40:1‚Äì40:29.
[3]Sushil Krishna Bajracharya, Joel Ossher, and Cristina Videira Lopes. 2010. Lever-
aging usage similarity for effective retrieval of examples in code repositories. In
Proceedings of the 18th ACM SIGSOFT International Symposium on Foundations of
Software Engineering, 2010 . 157‚Äì166.
[4]David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2001. Latent Dirichlet
Allocation. In Advances in Neural Information Processing Systems 14 [Neural
Information Processing Systems: Natural and Synthetic, NIPS 2001] . 601‚Äì608.
[5]Jos√© Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra.
2019. When deep learning met code search. In Proceedings of the ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019 . 964‚Äì974.
[6]Ning Chen, Jialiu Lin, Steven C. H. Hoi, Xiaokui Xiao, and Boshen Zhang. 2014.
AR-miner: mining informative reviews for developers from mobile app market-
place. In 36th International Conference on Software Engineering, ICSE ‚Äô14 . 767‚Äì778.
[7]Adelina Ciurumelea, Andreas Schaufelb√ºhl, Sebastiano Panichella, and Harald C.
Gall. 2017. Analyzing reviews and code of mobile apps for better release plan-
ning. In IEEE 24th International Conference on Software Analysis, Evolution and
Reengineering, SANER 2017 . 91‚Äì102.
[8]Tathagata Dasgupta, Mark Grechanik, Evan Moritz, Bogdan Dit, and Denys
Poshyvanyk. 2013. Enhancing Software Traceability by Automatically Expanding
Corpora with Relevant Documentation. In 2013 IEEE International Conference on
Software Maintenance . 320‚Äì329.
[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,
Volume 1 (Long and Short Papers) . 4171‚Äì4186.
[10] Dror G. Feitelson, Eitan Frachtenberg, and Kent L. Beck. 2013. Development and
Deployment at Facebook. IEEE Internet Comput. 17, 4 (2013), 8‚Äì17.
[11] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of
the Association for Computational Linguistics: EMNLP 2020 (Findings of ACL,
Vol. EMNLP 2020) . 1536‚Äì1547.
[12] Bin Fu, Jialiu Lin, Lei Li, Christos Faloutsos, Jason I. Hong, and Norman M. Sadeh.
2013. Why people hate your app: making sense of user feedback in a mobile app
store. In The 19th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD 2013 . ACM, 1276‚Äì1284.
[13] Cuiyun Gao, Baoxiang Wang, Pinjia He, Jieming Zhu, Yangfan Zhou, and
Michael R. Lyu. 2015. PAID: Prioritizing app issues for developers by track-
ing user reviews over versions. In 26th IEEE International Symposium on Software
Reliability Engineering, ISSRE 2015 . 35‚Äì45.
[14] Cuiyun Gao, Hui Xu, Junjie Hu, and Yangfan Zhou. 2015. AR-Tracker: Track the
Dynamics of Mobile Apps via User Review Mining. In 2015 IEEE Symposium on
Service-Oriented System Engineering, SOSE 2015 . 284‚Äì290.
[15] Gregory Gay, Sonia Haiduc, Andrian Marcus, and Tim Menzies. 2009. On the
use of relevance feedback in IR-based concept location. In 25th IEEE International
Conference on Software Maintenance (ICSM 2009) . 351‚Äì360.
543ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore Xinwen Hu, Yu Guo, Jianjie Lu, Zheling Zhu, Chuanyi Li, Jidong Ge, Liguo Huang, and Bin Luo
[16] Alex Graves and J√ºrgen Schmidhuber. 2005. Framewise phoneme classification
with bidirectional LSTM and other neural network architectures. Neural Networks
18, 5-6 (2005), 602‚Äì610.
[17] Xiaodong Gu and Sunghun Kim. 2015. "What Parts of Your Apps are Loved by
Users?" (T). In 30th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2015 . 760‚Äì770.
[18] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In
Proceedings of the 40th International Conference on Software Engineering, ICSE
2018. 933‚Äì944.
[19] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. In 9th International Conference on Learning Representations, ICLR 2021 .
[20] Hui Guo and Munindar P. Singh. 2020. Caspar: extracting and synthesizing user
stories of problems from app reviews. In ICSE ‚Äô20: 42nd International Conference
on Software Engineering . ACM, 628‚Äì640.
[21] Emitza Guzman and Walid Maalej. 2014. How Do Users Like This Feature? A
Fine Grained Sentiment Analysis of App Reviews. In IEEE 22nd International
Requirements Engineering Conference, RE 2014 . 153‚Äì162.
[22] Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, and Julia Hockenmaier. 2020. A
Multi-Perspective Architecture for Semantic Code Search. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics, ACL 2020 .
8563‚Äì8568.
[23] Mark Harman, Yue Jia, and Yuanyuan Zhang. 2012. App store mining and
analysis: MSR for app stores. In 9th IEEE Working Conference of Mining Software
Repositories, MSR 2012 . 108‚Äì111.
[24] Pablo Restrepo Henao, Jannik Fischbach, Dominik Spies, Julian Frattini, and
Andreas Vogelsang. 2021. Transfer Learning for Mining Feature Requests and
Bug Reports from Tweets and App Store Reviews. In 29th IEEE International
Requirements Engineering Conference Workshops, RE 2021 Workshops . 80‚Äì86.
[25] Xuan Huo and Ming Li. 2017. Enhancing the Unified Features to Locate Buggy
Files by Exploiting the Sequential Nature of Source Code. In Proceedings of the
Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017 .
1909‚Äì1915.
[26] Xuan Huo, Ming Li, and Zhi-Hua Zhou. 2016. Learning Unified Features from
Natural and Programming Languages for Locating Buggy Source Code. In Pro-
ceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,
IJCAI 2016 . 1606‚Äì1612.
[27] Xuan Huo, Ferdian Thung, Ming Li, David Lo, and Shu-Ting Shi. 2021. Deep
Transfer Bug Localization. IEEE Trans. Software Eng. 47, 7 (2021), 1368‚Äì1380.
[28] Claudia Iacob and Rachel Harrison. 2013. Retrieving and analyzing mobile
apps feature requests from online reviews. In Proceedings of the 10th Working
Conference on Mining Software Repositories, MSR ‚Äô13 . 41‚Äì44.
[29] Hammad Khalid, Emad Shihab, Meiyappan Nagappan, and Ahmed E. Hassan.
2015. What Do Mobile App Users Complain About? IEEE Softw. 32, 3 (2015),
70‚Äì77.
[30] Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language
independent subword tokenizer and detokenizer for Neural Text Processing. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 -
November 4, 2018 . 66‚Äì71.
[31] An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2015.
Combining Deep Learning with Information Retrieval to Localize Buggy Files
for Bug Reports (N). In 30th IEEE/ACM International Conference on Automated
Software Engineering, ASE 2015 . 476‚Äì481.
[32] Tien-Duy B. Le, Richard Jayadi Oentaryo, and David Lo. 2015. Information
retrieval and spectrum based bug localization: better together. In Proceedings of
the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE
2015. 579‚Äì590.
[33] Hongwei Li, Zhenchang Xing, Xin Peng, and Wenyun Zhao. 2013. What help
do developers seek, when and how?. In 20th Working Conference on Reverse
Engineering, WCRE 2013, Koblenz, Germany, October 14-17, 2013 . 142‚Äì151.
[34] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Fault Localization with Code
Coverage Representation Learning. In 43rd IEEE/ACM International Conference
on Software Engineering, ICSE 2021 . 661‚Äì673.
[35] Zhengliang Li, Zhiwei Jiang, Xiang Chen, Kaibo Cao, and Qing Gu. 2021. Laprob:
A Label propagation-Based software bug localization method. Inf. Softw. Technol.
130 (2021), 106410.
[36] Hongliang Liang, Lu Sun, Meilin Wang, and Yuxing Yang. 2019. Deep Learning
With Customized Abstract Syntax Tree for Bug Localization. IEEE Access 7 (2019),
116309‚Äì116320.
[37] Sherlock A. Licorish, Bastin Tony Roy Savarimuthu, and Swetha Keertipati. 2017.
Attributes that Predict which Features to Fix: Lessons for App Store Mining. In
Proceedings of the 21st International Conference on Evaluation and Assessment in
Software Engineering, EASE 2017 . 108‚Äì117.
[38] Stacy K. Lukins, Nicholas A. Kraft, and Letha H. Etzkorn. 2008. WCRE 2008, Pro-
ceedings of the 15th Working Conference on Reverse Engineering. In Proceedingsof the 15th Working Conference on Reverse Engineering, WCRE 2008 . 155‚Äì164.
[39] Walid Maalej and Hadeer Nabil. 2015. Bug report, feature request, or simply
praise? On automatically classifying app reviews. In 23rd IEEE International
Requirements Engineering Conference, RE 2015 . 116‚Äì125.
[40] Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: an input
generation system for Android apps. In Joint Meeting of the European Software
Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of
Software Engineering, ESEC/FSE‚Äô13 . 224‚Äì234.
[41] James MacQueen et al .1967. Some methods for classification and analysis of
multivariate observations. In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability , Vol. 1. Oakland, CA, USA, 281‚Äì297.
[42] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader-Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks.
In43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021 .
336‚Äì347.
[43] George Mathew and Kathryn T. Stolee. 2021. Cross-language code search using
static and dynamic analyses. In ESEC/FSE ‚Äô21: 29th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engineering .
205‚Äì217.
[44] Stuart McIlroy, Weiyi Shang, Nasir Ali, and Ahmed E. Hassan. 2017. User reviews
of top mobile apps in Apple and Google app stores. Commun. ACM 60, 11 (2017),
62‚Äì67.
[45] Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Chen Fu, and Qing Xie.
2012. Exemplar: A Source Code Search Engine for Finding Highly Relevant
Applications. IEEE Trans. Software Eng. 38, 5 (2012), 1069‚Äì1087.
[46] Vijayaraghavan Murali, Lee Gross, Rebecca Qian, and Satish Chandra. 2021.
Industry-Scale IR-Based Bug Localization: A Perspective from Facebook. In 43rd
IEEE/ACM International Conference on Software Engineering: Software Engineering
in Practice, ICSE (SEIP) 2021 . 188‚Äì197.
[47] Anh Tuan Nguyen, Tung Thanh Nguyen, Jafar M. Al-Kofahi, Hung Viet Nguyen,
and Tien N. Nguyen. 2011. A topic-based approach for narrowing the search
space of buggy files from a bug report. In 26th IEEE/ACM International Conference
on Automated Software Engineering (ASE 2011) . 263‚Äì272.
[48] Ehsan Noei, Feng Zhang, and Ying Zou. 2021. Too Many User-Reviews! What
Should App Developers Look at First? IEEE Trans. Software Eng. 47, 2 (2021),
367‚Äì378.
[49] Fabio Palomba, Pasquale Salza, Adelina Ciurumelea, Sebastiano Panichella, Har-
ald C. Gall, Filomena Ferrucci, and Andrea De Lucia. 2017. Recommending and
localizing change requests for mobile apps based on user reviews. In Proceedings
of the 39th International Conference on Software Engineering, ICSE 2017 . 106‚Äì117.
[50] Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Aaron Vis-
aggio, Gerardo Canfora, and Harald C. Gall. 2015. How can i improve my app?
Classifying user reviews for software maintenance and evolution. In 2015 IEEE
International Conference on Software Maintenance and Evolution, ICSME 2015 .
281‚Äì290.
[51] Sebastiano Panichella, Andrea Di Sorbo, Emitza Guzman, Corrado Aaron Visag-
gio, Gerardo Canfora, and Harald C. Gall. 2016. ARdoc: app reviews development
oriented classifier. In Proceedings of the 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering, FSE 2016 . 1023‚Äì1027.
[52] Long N. Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James T. Anibal, Alec Peltekian,
and Yanfang Ye. 2021. CoTexT: Multi-task Learning with Code-Text Transformer.
CoRR abs/2105.08645 (2021). arXiv:2105.08645
[53] Denys Poshyvanyk, Yann-Ga√´l Gu√©h√©neuc, Andrian Marcus, Giuliano Antoniol,
and V√°clav Rajlich. 2007. Feature Location Using Probabilistic Ranking of Meth-
ods Based on Execution Scenarios and Information Retrieval. IEEE Trans. Software
Eng. 33, 6 (2007), 420‚Äì432.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al.2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.
[55] Mohammad Masudur Rahman and Chanchal K. Roy. 2018. Effective Reformula-
tion of Query for Code Search Using Crowdsourced Knowledge and Extra-Large
Data Analytics. In 2018 IEEE International Conference on Software Maintenance
and Evolution, ICSME 2018 . 473‚Äì484.
[56] Mohammad Masudur Rahman and Chanchal K. Roy. 2018. Improving IR-based
bug localization with context-aware query reformulation. In Proceedings of the
2018 ACM Joint Meeting on European Software Engineering Conference and Sym-
posium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018 . 621‚Äì
632.
[57] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In Proceedings of the
2nd ACM SIGPLAN International Workshop on Machine Learning and Programming
Languages, MAPL@PLDI 2018 . 31‚Äì41.
[58] Simone Scalabrino, Gabriele Bavota, Barbara Russo, Massimiliano Di Penta, and
Rocco Oliveto. 2019. Listening to the Crowd for the Release Planning of Mobile
Apps. IEEE Trans. Software Eng. 45, 1 (2019), 68‚Äì86.
[59] Andrea Di Sorbo, Sebastiano Panichella, Carol V. Alexandru, Junji Shimagaki,
Corrado Aaron Visaggio, Gerardo Canfora, and Harald C. Gall. 2016. What would
544Lighting Up Supervised Learning in User Review-Based Code Localization: Dataset and Benchmark ESEC/FSE ‚Äô22, November 14‚Äì18, 2022, Singapore, Singapore
users change in my app? summarizing app reviews for recommending software
changes. In Proceedings of the 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering, FSE 2016 . 499‚Äì510.
[60] Andrea Di Sorbo, Sebastiano Panichella, Carol V. Alexandru, Corrado Aaron
Visaggio, and Gerardo Canfora. 2017. SURF: summarizer of user reviews feedback.
InProceedings of the 39th International Conference on Software Engineering, ICSE
2017. 55‚Äì58.
[61] Lorenzo Villarroel, Gabriele Bavota, Barbara Russo, Rocco Oliveto, and Massim-
iliano Di Penta. 2016. Release planning of mobile apps based on user reviews.
InProceedings of the 38th International Conference on Software Engineering, ICSE
2016. 14‚Äì24.
[62] Venkatesh Vinayakarao, Anita Sarma, Rahul Purandare, Shuktika Jain, and
Saumya Jain. 2017. ANNE: Improving Source Code Search using Entity Re-
trieval Approach. In Proceedings of the Tenth ACM International Conference on
Web Search and Data Mining, WSDM 2017 . 211‚Äì220.
[63] Grant Williams and Anas Mahmoud. 2017. Mining Twitter Feeds for Software
User Requirements. In 25th IEEE International Requirements Engineering Confer-
ence, RE 2017 . 1‚Äì10.
[64] Chu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, and Hong
Mei. 2014. Boosting Bug-Report-Oriented Fault Localization with Segmentation
and Stack-Trace Analysis. In 30th IEEE International Conference on Software
Maintenance and Evolution, 2014 . 181‚Äì190.
[65] Huayao Wu, Wenjun Deng, Xintao Niu, and Changhai Nie. 2021. Identifying Key
Features from App User Reviews. In 43rd IEEE/ACM International Conference on
Software Engineering, ICSE 2021 . 922‚Äì932.
[66] Yan Xiao, Jacky Keung, Kwabena Ebo Bennin, and Qing Mi. 2019. Improving bug
localization with word embedding and enhanced convolutional neural networks.
Inf. Softw. Technol. 105 (2019), 17‚Äì29.
[67] Yan Xiao, Jacky Keung, Qing Mi, and Kwabena Ebo Bennin. 2017. Improving
Bug Localization with an Enhanced Convolutional Neural Network. In 24th
Asia-Pacific Software Engineering Conference, APSEC 2017 . 338‚Äì347.[68] Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2013. A biterm topic
model for short texts. In 22nd International World Wide Web Conference, WWW
‚Äô13. 1445‚Äì1456.
[69] Zhou Yang, Jieke Shi, Shaowei Wang, and David Lo. 2021. IncBL: Incremental Bug
Localization. In 36th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2021 . 1223‚Äì1226.
[70] Xin Ye, Razvan C. Bunescu, and Chang Liu. 2014. Learning to rank relevant
files for bug reports using domain knowledge. In Proceedings of the 22nd ACM
SIGSOFT International Symposium on Foundations of Software Engineering, (FSE-
22). 689‚Äì699.
[71] Xin Ye, Hui Shen, Xiao Ma, Razvan C. Bunescu, and Chang Liu. 2016. From
word embeddings to document similarities for improved information retrieval
in software engineering. In Proceedings of the 38th International Conference on
Software Engineering, ICSE 2016 . 404‚Äì415.
[72] Le Yu, Jiachi Chen, Hao Zhou, Xiapu Luo, and Kang Liu. 2018. Localizing Function
Errors in Mobile Apps with User Reviews. In 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, DSN 2018 . 418‚Äì429.
[73] Jinglei Zhang, Rui Xie, Wei Ye, Yuhan Zhang, and Shikun Zhang. 2020. Exploiting
Code Knowledge Graph for Bug Localization via Bi-directional Attention. In ICPC
‚Äô20: 28th International Conference on Program Comprehension, 2020 . 219‚Äì229.
[74] Tao Zhang, Jiachi Chen, Xian Zhan, Xiapu Luo, David Lo, and He Jiang. 2021.
Where2Change: Change Request Localization for App Reviews. IEEE Trans.
Software Eng. 47, 11 (2021), 2590‚Äì2616.
[75] Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs be fixed?
More accurate information retrieval-based bug localization based on bug reports.
In34th International Conference on Software Engineering, ICSE 2012 . 14‚Äì24.
[76] Yu Zhou, Yanqi Su, Taolue Chen, Zhiqiu Huang, Harald C. Gall, and Sebastiano
Panichella. 2021. User Review-Based Change File Localization for Mobile Appli-
cations. IEEE Trans. Software Eng. 47, 12 (2021), 2755‚Äì2770.
[77] Ziye Zhu, Yun Li, Hanghang Tong, and Yu Wang. 2020. CooBa: Cross-project Bug
Localization via Adversarial Transfer Learning. In Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelligence, IJCAI 2020 . 3565‚Äì3571.
545