What Do They Capture? - A Structural Analysis of Pre-TrainedLanguage Models for Source CodeYao Wanâˆ—School of Computer Science andTechnology, Huazhong University ofScience and Technology, Chinawanyao@hust.edu.cnWei Zhaoâˆ—School of Computer Science andTechnology, Huazhong University ofScience and Technology, Chinamzhaowei@hust.edu.cnHongyu ZhangUniversity of NewcastleAustraliahongyu.zhang@newcastle.edu.auYulei SuiSchool of Computer Science,University of Technology SydneyAustraliayulei.sui@uts.edu.auGuandong XuSchool of Computer Science,University of Technology SydneyAustraliaguandong.xu@uts.edu.auHai Jinâˆ—School of Computer Science andTechnology, Huazhong University ofScience and Technology, Chinahjin@hust.edu.cnABSTRACTRecently, many pre-trained language models for source code havebeen proposed to model the context of code and serve as a basis fordownstream code intelligence tasks such as code completion, codesearch, and code summarization. These models leverage maskedpre-training and Transformer and have achieved promising results.However, currently there is still little progress regarding inter-pretability of existing pre-trained code models. It is not clearwhythese models work andwhatfeature correlations they can capture.In this paper, we conduct a thorough structural analysis aimingto provide an interpretation of pre-trained language models forsource code (e.g., CodeBERT, and GraphCodeBERT) from threedistinctive perspectives: (1) attention analysis, (2) probing on theword embedding, and (3) syntax tree induction. Through compre-hensive analysis, this paper reveals several insightful/f_indings thatmay inspire future studies: (1) Attention aligns strongly with thesyntax structure of code. (2) Pre-training language models of codecan preserve the syntax structure of code in the intermediate rep-resentations of each Transformer layer. (3) The pre-trained modelsof code have the ability of inducing syntax trees of code. Theses/f_indings suggest that it may be helpful to incorporate the syntaxstructure of code into the process of pre-training for better coderepresentations.CCS CONCEPTSâ€¢Software and its engineeringâ†’Reusability.âˆ—Also with National Engineering Research Center for Big Data Technology and System,Services Computing Technology and System Lab, Cluster and Grid Computing Lab,HUST, Wuhan, 430074, China.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro/f_it or commercial advantage and that copies bear this notice and the full citationon the/f_irst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or afee. Request permissions from permissions@acm.org.ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USAÂ© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00https://doi.org/10.1145/3510003.3510050KEYWORDSCode representation, deep learning, pre-trained language model,probing, attention analysis, syntax tree induction.ACM Reference Format:Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin.2022. What Do They Capture? - A Structural Analysis of Pre-Trained Lan-guage Models for Source Code. In44th International Conference on SoftwareEngineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA.ACM, NewYork, NY, USA, 12 pages. https://doi.org/10.1145/3510003.35100501 INTRODUCTIONCode representation learning (also known as code embedding) aimsto encode the code semantics into distributed vector representations,and plays an important role in recent deep-learning-based modelsfor code intelligence. Code embedding can be used to support avariety of downstream tasks, such as code completion [30], codesearch [12, 43], and code summarization [1, 44, 45].Current approaches to code embedding mainly fall into two cat-egories from the perspectives of supervised and unsupervised (orself-supervised) learning paradigms. The supervised approachesare typically developed for speci/f_ic tasks following theencoder-decoderarchitecture [37]. In this architecture, anencodernetwork(e.g. LSTM, CNN, and Transformer) is used to produce a vectorrepresentation of a program. The resulting vector is then fed as aninput into adecodernetwork to perform some prediction tasks, e.g.,summary generation [1,35,44] or token sequence prediction [30].Recently, there has been signi/f_icant improvement in the expres-siveness of models that can learn the semantics of code, such asself-attention based architectures like Transformer [39]. Anotherline of code embedding research is based on unsupervised learning.Some approaches utilize word embedding techniques to representsource code [27,28], which aim to learn a global word embeddingmatrixEâˆˆR/u1D449Ã—/u1D437, whereğœ†is the vocabulary size andğ´is the num-ber of dimensions. Code2Vec [2] is such kind of approach, whichlearns a distributed representation of code based on the sampledpaths fromAbstract Syntax Trees(ASTs).Recently, self-supervised models which are pre-trained throughmasked language modeling have attracted much attention. Pre-trained models such as BERT [8] and ELMo [29] are representativeapproaches and have been successfully used in a variety of tasks23772022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jinin NLP. Inspired by the self-supervised pre-training in NLP, therehave been some recent eï¬€orts in developing pre-trained models onlarge-scale code corpus for software engineering tasks. For example,CuBERT [18] is a pre-trained BERT model using 7.4M Python/f_ilesfrom GitHub. CodeBERT [11] is a bimodal pre-trained model onsource code and natural-language descriptions. To incorporate thesyntax structure of code, Guo et al. [13]further propose GraphCode-BERT to preserve the syntax structure of source code by introducingan edge masking technique over data-$ow graphs. With much eï¬€ortbeing devoted to pre-trained code embedding, there is a pressingdemand to understandwhythey work andwhatfeature correctionsthey are capturing.In the NLP community, several recent studies have been madetowards interpreting the pre-trained language models, e.g., BERT,from the perspective of attention analysis and task probing. Thiskind of research has become a subspecialty of â€œBERTologyâ€ [31],which focuses on studying the inner-mechanism of BERT model [8].However, in software engineering, such an understanding is yet tobe achieved. Often, we see pre-trained language models that achievesuperior performance in various software engineering tasks, but donot understand why they work. Currently there have been severalempirical studies that aim to investigate the eï¬€ectiveness of codeembedding. For example, Kang et al. [19]empirically assessed theimpact of code embedding for diï¬€erent downstream tasks. Chirkovaand Troshin[3]conducted another study to investigate the capabil-ities of Transformers to utilize syntactic information in diï¬€erenttasks. However, these studies only show in which scenarios a codeembedding technique works better, without explaining the inner-mechanism of why the embedding achieves good results. Therefore,it is still not clear why the pre-trained language models work andwhat they indeed capture, in the context of software engineeringtasks.In this work, we explore the interpretability of pre-trained codemodels. More speci/f_ically, we try to answer the following question:Can the existing pre-trained language models learn the syntacticalstructure of source code written in programming languages?Address-ing this question plays an important role in understanding thelearned structure of deep neural networks. We conduct a thoroughstructural analysis from the following three aspects, aiming to pro-vide an interpretation of pre-trained code models (e.g., CodeBERT,GraphCodeBERT).â€¢As the/f_irst contribution, we analyze the self-attention weightsand align the weights with the syntax structure (see Sec. 4.1).Given a code snippet, our assumption is that if two tokensare close to each other in the AST, i.e., have a neighbourhoodrelationship, the attention weights assigned to them shouldbe high. Our analysis reveals that the attention can capturehigh-level structural properties of source code, i.e., themotifstructurein ASTs.â€¢As the second contribution, we design a structural probingapproach [14] to investigate whether the syntax structureis embedded in the linear-transformed contextual word em-bedding of pre-trained code models (see Sec. 4.2). Using ourprobe, we show that such transformations also exist in thepre-trained language models for source code, showing ev-idence that the syntax structure of code is also embeddedimplicitly in the vectors learned by the model.

# $$
%&! 	"
	#
			
#
Figure 1: A general framework for Transformer-based lan-guage model pre-training [8].â€¢As the third contribution, we investigate whether the pre-trained language models for source code provide the abilityof inducing the syntax tree without training (see Sec. 4.3).We/f_ind that the pre-trained models can indeed learn thesyntax structure of source code to a certain extend.Our work is complementary to other works that aim to designbetter neural networks for source code representation. We believethat the/f_indings revealed in this paper may shed light on the innermechanism of pre-training models for programming languages, aswell as inspire further studies.2 BACKGROUNDIn this section, we introduce some background knowledge of ourwork, including Transformer, and pre-training language model. Fig-ure 1 shows a general framework for Transformer-based languagemodel pre-training.2.1 Self-Attention-Based TransformerTransformer [39], which is solely based on self-attention, has be-come a popular component for code representation learning. Letğº={ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B}denote a code snippet of a sequence of to-kens with length ofğ‘„. A Transformer model is composed ofğ½layers of Transformer blocks to represent a code snippet into con-textual representation at diï¬€erent levelsH/u1D459=[h/u1D4591,h/u1D4592,...,h/u1D459/u1D45B],whereğ·denotes theğ·-th layer. For each layer, the layer repre-sentationH/u1D459is computed by theğ·-th layer Transformer blockH/u1D459=Transformer/u1D459(H/u1D459âˆ’1),ğ·âˆˆ{1,2,...,ğ½}.In each Transformer block, multiple self-attention heads areused to aggregate the output vectors of the previous layer. Givenan inputğº, the self-attention mechanism assigns each tokenğ‘ƒ/u1D456aset of attention weights over the token in the input:A%n(ğ‘ƒ/u1D456)=(ğ‘€/u1D456,1(ğº),ğ‘€/u1D456,2(ğº),...,ğ‘€/u1D456,/u1D45B(ğº)),(1)2378What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source CodeICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USA
$$'%!! 
"# %!$$'%!$ !%! 
	$($'%'%!)
& 
$$"#$"#$!$%% %'%!! !"#%!#'%!$ !%! !#& %$($'%'%!%%#&%
!"#%!#'%!$ !%! 		
	
(a) A Python code snippet with its AST
(b) Attention heatmap in Layer 5
(c) Attention distribution in Layer 5, Head 12Figure 2: Visualization of self-attention distribution for a code snippet in CodeBERT. (a) A Python code snippet with itscorresponding AST. (b) Heatmap of the averaged attention weights in Layer 5. (c) Self-attention distribution in Layer 5, Head 12.The brightness of lines indicates the attention weights in a speci/f_ic head. If the connected nodes appear in themotif structureof the corresponding AST, we mark the lines in red.whereğ‘€/u1D456,/u1D457(ğº)is the attention thatğ‘ƒ/u1D456pays toğ‘ƒ/u1D457. The attentionweights are computed from the scaled dot-product of thequeryvectorofğ‘ƒ/u1D456, and thekey vectorofğ‘ƒ/u1D457, followed by a softmax. Inthe vectorized computing, a general attention mechanism can beformulated as the weighted sum of the value vectorV, using thequery vectorQand the key vectorK:A%(Q,K,V)=so&max/parenleftBiggQK/u1D447/radicalbigğ´model/parenrightBiggÂ·V,(2)whereğ´modelrepresents the dimension of each hidden representa-tion. For self-attention,Q,K, andVare mappings of the previous hid-den representation by diï¬€erent linear functions, i.e.,Q=H/u1D459âˆ’1W/u1D459/u1D444,K=H/u1D459âˆ’1W/u1D459/u1D43E, andV=H/u1D459âˆ’1W/u1D459/u1D449, respectively. At last, the encoderproduces the/f_inal contextual representationH/u1D43F=[h/u1D43F1,...,h/u1D43F/u1D45B],which is obtained from the last Transformer block.In order to utilize the order of the sequential tokens, the â€œposi-tional encodingsâ€ are injected to the input embedding.w/u1D456=ğ»(ğ‘ƒ/u1D456)+ğ¸ğ‘,(ğ‘ƒ/u1D456),(3)whereğ»denotes the word embedding layer, andğ¸ğ‘,denotes the po-sitional embedding layer. Typically, the positional encoding impliesthe position of code token based on sine and cosine functions.2.2 Pre-Training Language ModelGiven a corpus, each sentence (or code snippet) is/f_irst tokenizedinto a series of tokens (e.g.,Byte Pair Encoding, BPE [32]). Be-fore BERTâ€™s pre-training, it takes the concatenation of two seg-ments as the input, de/f_ined asğº1={ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B}andğº2={ğ¹1,ğ¹2,...,ğ¹/u1D45A}, whereğ‘„and.denote the lengths of two seg-ments, respectively. The two segments are always connected by aspecial separator token[SEP]. The/f_irst and last tokens of each se-quence are always padded with a special classi/f_ication token[CLS]and an ending token[EOS], respectively. Finally, the input of eachtraining sample will be represented as follows:,=[CLS],ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright,1,[SEP],ğ¹1,ğ¹2,...,ğ¹/u1D45A/bracehtipupleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipdownright/bracehtipdownleft/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehext/bracehtipupright,2,[EOS].The input is then fed into a Transformer encoder. During BERTâ€™spre-training, two objectives are designed for self-supervised learn-ing, i.e.,masked language modeling(MLM) andnext sentence pre-diction(NSP). In the masked language modeling, the tokens of aninput sentence are randomly sampled and replaced with the specialtoken[MASK]. In practice, BERT uniformly selects 15% of the inputtokens for possible replacement. Among the selected tokens, 80%are replaced with[MASK], 10% are unchanged, and the left 10% arerandomly replaced with the selected tokens from vocabulary [8].For next sentence prediction, it is modeled as a binary classi/f_icationto predict whether two segments are consecutive. Training positiveand negative examples are conducted based on the following rules:(1) if two sentences are consecutive, it will be considered as a posi-tive example; (2) otherwise, those paired segments from diï¬€erentdocuments are considered as negative examples.Recently, self-supervised learning using masked language mod-eling has become a popular technique for natural language un-derstanding and generation [5,8,9,24,34,36]. In the context ofsoftware engineering, several pre-trained code models have alsobeen proposed for program understanding. In this paper, we selecttwo representative pre-trained models for code representations: (1)CodeBERT [11], which takes the concatenation of source code andnatural-language description as inputs, and pre-trains a languagemodel by masking the inputs; and (2) GraphCodeBERT [13], whichimproves CodeBERT by incorporating the data-$ow informationamong variables into model pre-training.3 MOTIVATIONPrior work in NLP has pointed out that the self-attention mecha-nism in Transformer has the capability of capturing certain syntaxinformation in natural languages. Inspired by this, we visualize and2379ICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin
		
		 
		


	
 	

Figure 3: An illustration of attention analysis, probing on word embedding, and syntax tree induction, with a speci/f_ic Pythoncode snippet.investigate the attention distribution of the pre-trained model (i.e.,CodeBERT) for a code snippet, as shown in Figure 2. Figure 2(a)shows a Python code snippet with its AST. In this paper, we de/f_inethe syntax structure of the AST consisting of a non-leaf node withits children (e.g.,if_statementandblockin Figure 2(a)) as amotifstructure. We believe that the syntax information of code can berepresented by a series ofmotif structures.Given a code snippet and its corresponding AST, Figure 2(b)visualizes the self-attention heatmap for a speci/f_ic layer (i.e., Layer5), which is an average of attention weights over multiple heads.From this/f_igure, we can observe that several patterns indeed existin the self-attention heatmap, depicted as groups of rectangles(marked in red). These rectangles indicate that the code tokensform groups. Interestingly, we can also/f_ind that each group oftokens is close to each other in the AST. Taking â€œif exit_codeis not Noneâ€ as an example, which is anifstatement, we cansee that, in the AST, all of these tokens are in the same branch ofif_statement. In addition, we can see that these code tokens arealso closely connected in the self-attention heatmap.Moreover, we also visualize the self-attention distribution in aspeci/f_ic head (Layer 5, Head 12) to analyze the connections betweentwo tokens, as shown in Figure 2(c). In this/f_igure, the brightnessof lines indicates the attention weights in a speci/f_ic head. If theconnected nodes appear in themotif structureof the correspondingAST, we mark the lines in red. From this/f_igure, we can observethat those code tokens (i.e., â€œifâ€, â€œexit_codeâ€, â€œnotâ€, and â€œNoneâ€)that are in amotif structureindeed have been highlighted as closelyconnected by self-attention.As we have identi/f_ied several patterns from the attention distri-bution, which provide some hints to the syntax structure of code, itis necessary for us to further explore this phenomenon with quan-titative analysis and systematic assessment. Motivated by the afore-mentioned observations, this paper investigateswhypre-trainedlanguage models for source code work andwhatfeature correlationsthey are capturing, by analyzing the self-attention mechanism. Inparticular, we analyze two outputs of the self-attention mechanism,i.e., the attention distribution and the generated hidden vectors,under the framework of Transformer.4 STRUCTURAL ANALYSIS OF PRE-TRAINEDLANGUAGE MODELS FOR SOURCE CODEIn this section, we propose three distinct structural analysis ap-proaches, i.e., attention analysis, structural probing on word em-bedding, and syntax tree induction, to interpret pre-trained codemodels (i.e., CodeBERT and GraphCodeBERT). Figure 3 gives anillustration of the three structural analysis approaches. Before intro-ducing each approach, we/f_irst introduce several common notationsthat will be used later. Let(ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B)denote the code tokensequence of code snippetğº, with lengthğ‘„. On theğ·-th layer of Trans-former, we use(h/u1D4591,h/u1D4592,...,h/u1D459/u1D45B)to denote the sequence of contextualrepresentation of each code token.4.1 Attention AnalysisWe start by analyzing the self-attention weights, which are the coremechanism for pre-training Transformer-based models. Intuitively,attention de/f_ines the closeness of each pair of code tokens. Fromthe lens of attention analysis, we aim to analyze how attentionaligns with the syntactical relations in source code. In particular,we consider the syntactical relations such that the attention weightis high between two AST tokens sharing the same parent node.Figures 3(a) and 3(b) illustrate the attention analysis. Given a codesnippet with its AST, we can see that the leaf nodesforandinshare the same parent. As expected, this structure is aligned withthe attention weightğ‘€for,inbetween these two nodes.Speci/f_ically, on each Transformer layer, we can obtain a set ofattention weightsğ‘€over the input code, whereğ‘€/u1D456,/u1D457>0 is the atten-tion fromğ¾-th code token toğ¾u1D457-th token. Here, we de/f_ine an indicatorfunctionğ¾u1D453(ğ‘ƒ/u1D456,ğ‘ƒ/u1D457)that returns 1 ifğ‘ƒ/u1D456andğ‘ƒ/u1D457are in a syntacticrelation (ğ‘ƒ/u1D456andğ‘ƒ/u1D457have the same parent node in the AST)1, and0 otherwise. We de/f_ine the attention weight betweenğ‘ƒ/u1D456andğ‘ƒ/u1D457asğ‘€/u1D456,/u1D457(ğº), and ifğ‘ƒ/u1D456andğ‘ƒ/u1D457are very close, the attention weightshould be larger than a threshold, i.e.,ğ‘€/u1D456,/u1D457(ğº)>ğ¾u1D703. Therefore, theproportion of high-attention token pairs (ğ‘€/u1D456,/u1D457(ğº)>ğ¾u1D703) aggregated1Note that, in this paper, we exclude the condition that/u1D464/u1D456and/u1D464/u1D457are adjacent intextual appearance.2380What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source CodeICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAover a datasetCcan be formulated as follows:ğ¸/u1D6FC(ğ¾u1D453)=/summationtext.1,âˆˆC|,|/summationtext.1/u1D456=1|,|/summationtext.1/u1D457=1/u1D7D9/u1D6FC/u1D456,/u1D457(,)>/u1D703Â·ğ¾u1D453(ğ‘ƒ/u1D456,ğ‘ƒ/u1D457)/summationtext.1,âˆˆC|,|/summationtext.1/u1D456=1|,|/summationtext.1/u1D457=1/u1D7D9/u1D6FC/u1D456,/u1D457(,)>/u1D703,(4)whereğ¾u1D703is a threshold selected for high-con/f_idence attention weights.Variability.Equation 4 shows that, the portion of aligned atten-tion is only dependent on the absolute value of attention weightğ‘€/u1D456,/u1D457(ğº). We hypothesize that those heads that are attending to theposition, i.e., those heads that focus on the previous or next codetoken, would not align well with the syntax structure of code, sincethey do not consider the content of the code token. To distinguishwhether the heads are attending to content or position of the codetoken, we further investigate the attention variability, which mea-sures how attention varies over diï¬€erent inputs. The attentionvariability is formally de/f_ined as follows [40]:ğœ†ğ¾u1D44E ğ¾u1D45Fğ¾ğ¾u1D44E ğ¾u1D44Fğ¾ğ·ğ¾ğ¾u1D461ğ¾u1D466.alt/u1D6FC=/summationtext.1,âˆˆ/u1D436|,|/summationtext.1/u1D456=1|/u1D456|/summationtext.1/u1D457=1/barex/barexğ‘€/u1D456,/u1D457(ğº)âˆ’Â¯ğ‘€/u1D456,/u1D457/barex/barex2Â·/summationtext.1,âˆˆ/u1D436|,|/summationtext.1/u1D456=1|/u1D456|/summationtext.1/u1D457=1ğ‘€/u1D456,/u1D457(ğº),(5)whereÂ¯ğ‘€/u1D456,/u1D457is the mean ofÂ¯ğ‘€/u1D456,/u1D457(ğº)over allğºâˆˆC. We only includethe/f_irstğ¾u1D441tokens (ğ¾u1D441=10) of eachğºâˆˆCto ensure a suï¬ƒcientamount of data at each positionğ¾. The positional patterns appear tobe consistent across the entire sequence. The high variability wouldsuggest a content-dependent head, while low variability wouldindicate a content-independent head.Figure 4: An illustration of the connection between distanceand the syntax structure.4.2 Structural Probing on Word EmbeddingIn this approach, we propose a structural probing analysis approachto investigate whether a pre-trained model embeds the syntacticalstructure in its contextual word embedding. The key idea of ourapproach is that tree structure is embedded if the transformed spacehas the property that the Euclidean distance between two wordsâ€™vectors corresponds to the number of edges between the wordsin the syntax tree. One question may arise:why does the distancebetween nodes in the syntax tree matter for syntax information.Thisis because the distance metric (i.e., the path length between eachpair of words) can recover the syntax tree simply by identifying thatnodesğ¹andğ¾u1D463are neighbors if the distance between them equalsto 1. This has also been shown in the Code2Vec [2], which utilizesthe contextual information among a set of paths sampled fromthe AST, to represent the structure information of code. Figure 4gives a toy example to illustrate the connection between distanceand syntax structure. Let(ğ‘ƒ1,...,ğ‘ƒ/u1D456,...,ğ‘ƒ/u1D457,...,ğ‘ƒ/u1D45B)denote thesequence of code tokens for code snippetğº, if we know the distancebetween every pair of nodes, we can induce the syntax structure ofcode. Note that, the distance metric, which measures the distanceamong any two code tokens, can learn the global syntax structureof code to some extent.Figures 3(a) and 3(c) illustrate the structural probing on wordembedding. Taking the leaf nodesforandinwhich share the sameparent as an example, the Euclidean square of distance betweenthese two nodes is 2. We/f_irst map the representations of these twotokens into a hidden space via a linear transformationğ¾u1D435, obtainingtheğœ†ğ»ğºğ¾u1D461ğ‘ğ¾u1D45Ffor, andğœ†ğ»ğºğ¾u1D461ğ‘ğ¾u1D45Fin, respectively. We believe that if the Eu-clidean square of distance betweenğœ†ğ»ğºğ¾u1D461ğ‘ğ¾u1D45Ffor, andğœ†ğ»ğºğ¾u1D461ğ‘ğ¾u1D45Finis closeto 2, the syntax structure betweenforandinis well preserved.In particular, we learn the mapping functionğ¾u1D435in a supervisedway. Formally, given a code sequence(ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B)as the input,and each model layer generates word vectors(h1,h2,...,h/u1D45B).W ecompute the square of distance between the two word vectorsh/u1D456andh/u1D457in high-dimensional hidden space as follows:ğ´/u1D435(h/u1D456,h/u1D457)2=(ğ¾u1D435(h/u1D456âˆ’h/u1D457))/u1D447(ğ¾u1D435(h/u1D456âˆ’h/u1D457)),(6)whereğ¾andğ¾u1D457are indices of the words in the code sequence. Theparameters of the structural probe we use are exactly the matrixğ¾u1D435(liner mapping), which is trained to reconstruct the tree distancebetween all word pairs (ğ‘ƒ/u1D456,ğ‘ƒ/u1D457) in the code sequenceğ¾u1D447in the train-ing set of source code. We de/f_ine the loss function for parametertraining as follows:min/u1D435/summationdisplay.1,âˆˆC1|ğº|2/summationdisplay.1/u1D456,/u1D457/barex/barex/barexğ´/u1D447/u1D450(ğ‘ƒ,/u1D456,ğ‘ƒ,/u1D457)âˆ’ğ´/u1D435(h,/u1D456,h,/u1D457)2/barex/barex/barex,(7)where|ğº|is the length of code sequenceğº,ğ´/u1D447/u1D450(ğ‘ƒ,/u1D456,ğ‘ƒ,/u1D457)denotes thedistance between code tokens in AST, andğ´/u1D435(h,/u1D456,h,/u1D457)2denotes thedistance between the embedding vectors of code tokens, for codesequenceğº. The/f_irst summation calculates the average distancefor all training sequences, while the second one sums all possiblecombinations of any two words in the code sequences. The goal ofthis supervised training is to propagate the error backwards andupdate the parameters of the linear mapping matrixğ¾u1D435.4.3 Syntax Tree InductionIn this approach, we propose to investigate the capability of pre-trained code model in inducing syntax structure,withouttraining.The key insight of our approach is that if the distance betweentwo tokens is close (e.g., with a similar attention distribution, orwith a similar representation), they are expected to be close in thesyntax tree, i.e., sharing the same parent. Based on this insight, wepropose to induce the syntax tree from the distances between twotokens. Our assumption is that if the induced tree derived fromthe pre-training model is similar to the gold standard syntax tree,we can reasonably infer that the syntactic structures have beenpreserved during the model pre-training.We propose to induce the syntax tree based on the syntactic dis-tance among code tokens, which was/f_irst introduced for grammarinduction for natural languages [33]. Formally, given a code se-quence(ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B), we computed=(ğ´1,ğ´2,...,ğ´/u1D45Bâˆ’1), whereğ´/u1D456corresponds to the syntactic distance between tokensğ‘ƒ/u1D456andğ‘ƒ/u1D456+1. Eachğ´/u1D456is de/f_ined as follows:ğ´/u1D456=ğ¾u1D453(ğ¾u1D454(ğ‘ƒ/u1D456),ğ¾u1D454(ğ‘ƒ/u1D456+1)),(8)2381ICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai JinAlgorithm 1:Greedy top-down binary syntax tree induc-tion based on syntax distances.1ğ¾u1D446=(ğ‘ƒ1,ğ‘ƒ2,...,ğ‘ƒ/u1D45B): a sequence of code tokens, withlengthğ‘„;2d=(ğ´1,ğ´2,...,ğ´/u1D45Bâˆ’1): a vector whose elements are thedistance between two adjacent code tokens;3FunctionTree(ğ¾u1D446,d):4ifd=[]then5ğ‘„ğ‘ğ´ğ»â†ğ½ğ»ğ¾u1D44Eğ¾u1D453(ğ¾u1D4460);6else7ğ¾=argmax(d);8ğºğ¾uni210Eğ¾ğ·ğ´/u1D459=Tree(ğ¾u1D446â‰¤/u1D456,d</u1D456);9ğºğ¾uni210Eğ¾ğ·ğ´/u1D45F=Tree(ğ¾u1D446>/u1D456,d>/u1D456);10ğ‘„ğ‘ğ´ğ»â†ğ¾u1D441ğ‘ğ´ğ»(ğºğ¾uni210Eğ¾ğ·ğ´/u1D459,ğºğ¾uni210Eğ¾ğ·ğ´/u1D45F);11end12returnğ‘„ğ‘ğ´ğ»;13End Functionwhereğ¾u1D453(Â·,Â·)andğ¾u1D454(Â·)denote the function of distance measurementand code representation learning, respectively. Here, we measurethe syntactic distance between two tokens from their intermediaterepresentation vector, as well as the self-attention distribution, withvarious distance measurement functions. Speci/f_ically, letğ¾u1D454/u1D463/u1D459andğ¾u1D454/u1D451/u1D459,/u1D458denote the functions to generate the intermediate representa-tion and self-attention inğ·-th layer andğ¾u1D458-th head. To calculate thesimilarity between vectors, we have many options, in terms of theintermediate representation and attention distributions. For exam-ple, we can useğ½1 andğ½2 to calculate the similarity between twointermediate representation vectors. We can use Jensen-Shannondivergence [25] and Hellinger distance [22] to calculate the similar-ity between two attention distributions. Table 1 summarizes all theavailable distance measurement functions.Once the distance vectordis computed, we can easily convertit to the target syntax tree through a simple greedy top-downinference algorithm based on recursive partitioning of the input,as shown in Algorithm 1. Alternatively, this tree reconstructionprocess can also be done in a bottom-up manner, which is left forfurther exploration [33].Injecting Bias into Syntactic Distance.From our observation, theAST of source code tends to be right-skewed. This has also been awell-known bias in the constituency trees for English. Therefore, itmotivates us to in$uence the induced tree such that they are mod-erately right-skewed following the nature of gold-standard ASTs.To achieve this goal, we propose to inject the inductive deviationinto the framework by simply modifying the value of the syntacticdistance. In particular, we introduce the right-skewness bias to in-$uence the spanning tree to make it right-biased appropriately [20].Formally, we computeË†ğ´/u1D456by appending the following linear biasterm to everyğ´/u1D456:Ë†ğ´/u1D456=ğ´/u1D456+ğ¾u1D706Â·ğ¾u1D434ğœ† ğ¾u1D43A(d)Ã—/parenleftbigg1âˆ’1(.âˆ’1)Ã—(ğ¾âˆ’1)/parenrightbigg,(9)where AVG(Â·) outputs an average of all elements in a vector,ğ¾u1D706is ahyperparameter, andğ¾ranges from 1 to., where.=ğ‘„âˆ’1.Table 1: The de/f_inition of diï¬€erent functions of distancemeasurement to compute the syntactic distance betweentwo adjacent words in a code sequence. Note thatğ¾u1D45F=ğ¾u1D454/u1D463(ğ‘ƒ/u1D456),,=ğ¾u1D454/u1D463(ğ‘ƒ/u1D456+1),ğ¾u1D443=ğ¾u1D454/u1D451(ğ‘ƒ/u1D456),ğ¾u1D444=ğ¾u1D454/u1D451(ğ‘ƒ/u1D456+1),ğ¾uni210Edenotes the hiddenembedding size, andğ‘„denotes the length of code sequence.FunctionDe/f_initionDistance functions for intermediate representationsğ½1(ğ¾u1D45F,,)/summationtext.1/uni210E/u1D456=1|ğ¾u1D45F/u1D456âˆ’,/u1D456|ğ½2(ğ¾u1D45F,,)/radicalBig/summationtext.1/uni210E/u1D456=1(ğ¾u1D45F/u1D456âˆ’,/u1D456)2Distance functions for attention distributionsğ¾u1D43Dğ¾u1D446ğ¾u1D437(ğ¾u1D443||ğ¾u1D444)/radicalbig(ğ¾u1D437/u1D43E/u1D43F(ğ¾u1D443||ğ¾u1D440)+ğ¾u1D437/u1D43E/u1D43F(ğ¾u1D444||ğ¾u1D440))/2whereğ¾u1D440=(ğ¾u1D443+ğ¾u1D444)/2andğ¾u1D437/u1D43E/u1D43F(ğ¾u1D434||ğ¾u1D435)=/summationtext.1/u1D464âˆˆ,ğ¾u1D434(ğ‘ƒ)log/u1D434(/u1D464)/u1D435(/u1D464)ğ¾u1D43Bğ¾u1D438ğ½(ğ¾u1D443,ğ¾u1D444)1âˆš2/radicalBig/summationtext.1/u1D45B/u1D456=1/parenleftbigâˆšğ¸/u1D456âˆ’âˆšğ¾u1D45E/u1D456/parenrightbig2Note that, introducing such a bias can also examine what changesare made to the resulting tree structure. Our assumption is that: ifinjecting the bias does not aï¬€ect the performance of the pre-trainedmodel for unsupervised analysis, we can infer that they capture thebias to some extent.Similarity between Two Trees.Here we introduce the way we mea-sure the similarity of the induced tree and the gold-standard AST.Speci/f_ically, we/f_irst transform the tree structure into a collectionof intermediate nodes, where each intermediate node is composedof two leaf nodes. Then we measure the similarity between the twocollections. Figure 5 shows a toy example to illustrate the calcu-lation of similarity between two trees, i.e., the gold-standard AST(Figure 5(a)) and induced tree (Figure 5(b)). As shown in Figure 5(a),the gold-standard AST consists of four intermediate nodes (i.e.,ğ¾u1D4471,ğ¾u1D4472,ğ¾u1D4473, andğ¾u1D4474). For each intermediate, we further expand it usingtwo leaf nodes. For example, theğ¾u1D4471is expanded into(ğ‘ƒ1,ğ‘ƒ6), whereğ‘ƒ6is randomly selected from theğ‘ƒ4,ğ‘ƒ5, andğ‘ƒ6. Similarly, we alsotransform the induced tree into a collection of leaf nodes.
 	  		
				Figure 5: A toy example to illustrate the calculation of simi-larity between the gold-standard AST and induced tree.Given two sets, we use theğ¾u1D4391 score to measure their similar-ity. Letğ¾u1D446denote the set of gold-standard tree, andğ¾u1D446(denote theset of predicted tree, we can calculate the precision and recall byğ¸ğ¾u1D45Fğ»ğºğ¾,ğ¾ğ‘ğ‘„=|/u1D446)/u1D446(||/u1D446(|, andğ¾u1D45Fğ»ğºğ¾u1D44Eğ·ğ·=|/u1D446)/u1D446(||/u1D446|, respectively. The F1 score2382What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source CodeICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USA
(a) CodeBERT (Python)(b) GraphCodeBERT (Python)(c) CodeBERT (Java)
(d) GraphCodeBERT (Java)(e) CodeBERT (PHP)(f) GraphCodeBERT (PHP)Figure 6: Consistency between the attention and AST for the CodeBERT and GraphCodeBERT on diï¬€erent programminglanguages (i.e., Python, Java, and PHP). These heatmaps show the proportion of high-con/f_idence attention weights (ğ‘€/u1D456,/u1D457>ğ¾u1D703)from each head which connect those code tokens that in themotif structureof AST. The bars show the maximum value of eachlayer.is the the harmonic mean of precision and recall, as follows:ğ¾u1D4391=2âˆ—ğ¸ğ¾u1D45Fğ»ğºğ¾,ğ¾ğ‘ğ‘„Â·ğ¾u1D45Fğ»ğºğ¾u1D44Eğ·ğ·ğ¸ğ¾u1D45Fğ»ğºğ¾,ğ¾ğ‘ğ‘„+ğ¾u1D45Fğ»ğºğ¾u1D44Eğ·ğ·.(10)5 EXPERIMENTAL DESIGN AND RESULTSIn this section, we conduct experiments to explore what the pre-trained code models capture from three distinct aspects, i.e., atten-tion analysis, structural probing on word embedding, and syntaxtree induction.5.1 Experimental DesignWe investigate two Transformer-based pre-trained models (i.e.,CodeBERT [11] and GraphCodeBERT [13]), both of which are com-posed of 12 layers of Transformer with 12 attention heads. Thesemodels are both pre-trained on CodeSearchNet [17], a large-scaleof code corpora collected from GitHub across six programminglanguages. The size of representation in each Transformer layer isset to 768. Without loss of generability, we select Python, Java, andPHP as our target programming languages and use the correspond-ing dataset from CodeSearchNet. For all experiments, we excludethe attention to the[SEP]delimiter as it has been proven to be aâ€œno-opâ€ attention mark [4], as well as the attention to the[CLS]mark, which is not explicitly used for language modeling. Notethat, in the pre-training phase, the input code snippets have beentokenized into subwords viabyte-pair encoding(BPE) [32] beforebeing passed to the pre-trained model. However, our analyses areall based on the word-level code tokens. Therefore, we representeach word by averaging the representations of its subwords. Allthe experiments were conducted on a Linux server, with 128GBmemory, and a single 32GB Tesla V100 GPU.Through comprehensive analysis, we aim to answer the follow-ing research questions:â€¢RQ1 (Attention Analysis):Does attention align with thesyntax structure in source code?â€¢RQ2 (Probing on Word Embedding):Are the syntax struc-ture encoded in the contextual code embedding?â€¢RQ3 (Syntax Tree Induction):Are the pre-trained codemodel able to induce the syntax structure of code?5.2 RQ1: Attention AnalysisIn attention analysis, we aim to investigate whether the attentionaligns with the syntax structure of source code.Experimental Settings.Following [41], we set the attention thresh-oldğ¾u1D703in Equation 4 as 0.3, so as to ensure selecting high-con/f_idenceattention and retaining enough data for our analysis. We leavethe analysis of the impact ofğ¾u1D703in future work. Furthermore, ouranalysis is based on a subset of 5,000 code snippets randomly sam-pled from the training dataset. In order to reduce the memory andaccelerate the computation process, we truncate all the long codesequences within the length of 512. We only include the results ofattention heads where at least 100 high-con/f_idence attention scoresare available in our analysis.2383ICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin
Variability:0.52 (content-dependent)pÎ±(f):0 (none-consistency)Variability:0.25 (position-based)pÎ±(f):0.08 (low-consistency)pÎ±(f):0.67 (high-consistency)Variability:0.11 (position-based)Layer: 2 Head: 3Layer: 5 Head: 11Layer: 11 Head: 1
Figure 7: Visualization of attention heads in CodeBERT, along with the value of attention analysis (ğ¸/u1D6FC(ğ¾u1D453)), and attentionvariability, given a Python code snippet. Left: Attention visualised in Layer 2, Head 3, which focuses attention primarily on theposition of next token. Center: Attention visualized in Layer 5, Head 11, which disperses attention roughly evenly across alltokens. Right: Attention visualized in Layer 11, Head 1, which focuses on the content, and is highly aligned with the AST.
Figure 8: The variability of attention distribution by layer-head in Python. High-values indicate content-dependentheads, and low-values indicate position-based heads.Experimental Results.Figure 6 shows how attention aligns withthe AST structure for CodeBERT and GraphCodeBERT on diï¬€erentprogramming languages (i.e., Python, Java, and PHP), according tothe indicators de/f_ined in Equation 4. The/f_igure shows the propor-tion of high-con/f_idence attention weights (ğ‘€/u1D456,/u1D457>ğ¾u1D703) from each headwhich connect those code tokens that in themotif structureof AST.The bar plots show the maximum score of each layer. From this/f_igure, we can observe that the most aligned heads are located inthe deeper layers and the concentration is as high as 67.25% (Layer11, Head 1 in CodeBERT) and 59% (Layer 12, Head 9 in GraphCode-BERT). These high scores indicate thatattention aligns stronglywith themotif structurein AST, especially in the deeperlayers.This is because the heads in deeper layers have strongercapabilities in capturing longer distances.Although there is a strong alignment between the attention andthe syntax structure of code, it is still necessary to distinguishwhether the attention is based on the position or content of codetoken, as mentioned in Sec. 4.1. In Figure 7, we show the atten-tion variability of attention heads in CodeBERT for a Python codesnippet. Figure 7 (left) and Figure 7 (center) show two examplesof heads that put more focus on the position, respectively fromLayer 5, Head 11, and Layer 2, Head 3. Based on the variabilityde/f_ined in Equation 5, we can see that the attention in Layer 5,Head 11, is evenly dispersed, with the variability of 0.25. More-over, in Layer 5, Head 11, it is apparent to see that the attentionis focusing on the next token position. Figure 7 (right) shows thecontent-dependent head from Layer 11, Head 1, which has the high-est alignment relationship with the abstract syntax tree structureamong all heads. In Figure 8, we also visualize the variability ofattention distribution by layer-head in Python. The high-valuesindicate the content-dependent heads, and the low-values indicatethe position-based (or content-independent) heads.Summary.Through attention analysis, we/f_ind that the learnedattention weights are strongly aligned with themotif structurein an AST. Additionally, each attention across diï¬€erent headsand layers put diï¬€erent focus on the position and content of thetokens of source code.2384What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source CodeICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USATable 2: The average Spearman correlation of probing inPython.MethodSpearman CorrelationCodeBERT-00.60CodeBERT-10.69CodeBERT-50.85GraphCodeBERT-50.865.3 RQ2: Probing on Word EmbeddingWe conduct structural probing on the word embedding of sourcecode, to investigate whether the word embedding in the Transformer-based pre-trained model embeds the syntax structure of code.Experimental Settings.Given a pair of code tokens (leaf nodes)in AST, we measure the correlation between the predicted dis-tance using word embedding and the gold-standard distance in theAST. Specially, we use the Spearman correlation [7] to measurethe predicted distance vector and the gold-standard distance vec-tor, among all samples of code snippets. When training the lineartransformation matrixğ¾u1D435in Equation 7, we limit the code lengthto 100. We probe the contextual representations in each layer ofTransformer, and denote the investigated pre-trained code modelsas CodeBERT-ğ¾u1D43Eand GraphCodeBERT-ğ¾u1D43E, whereğ¾u1D43Eindexes the layerof Transformer in corresponding model. To serve as a comparisonagainst the pre-trained code models, we also design a baseline modelâ€“ CodeBERT-0, which denotes the simple word embedding beforebeing fed into the Transformer layers. In evaluation, we averagethe Spearman correlation between all/f_ixed-length code sequences.We report the average value of the entire sequence length of 5+50as the Spearman metric, as in [14].Experimental Results.Table 2 shows the results of probing inPython. From this table, we/f_ind that the CodeBERT-0 withoutTransformer layers achieves inferior performance than that withmultiple layers of Transformer. This con/f_irms our assumption thatTransformer has the ability of capturing the syntax information ofsource code. In addition, we can also/f_ind that GraphCodeBERT per-forms better than CodeBERT, indicting that it is helpful to explicitlyincorporate the syntax structure into model pre-training.Figure 9 shows the Spearman correlation of probing on the rep-resentation in each layer of the models. We can observe thatthecapability of capturing syntax structure diï¬€ers across dif-ferent layers of Transformer. The best performance is obtainedin the 5-th layer. For example, in Python, CodeBERT and Graph-CodeBERT achieve the highest Spearman correlation (84% and 86%,respectively) in the 5-th layer. Furthermore, in each layer of Trans-former, GraphCodeBERT still performs better than CodeBERT incapturing the syntax structure of programs written Python, Java,and PHP, con/f_irming the observation from Table 2.Figure 10 shows the heatmaps of gold-standard and predicteddistances based on pre-trained CodeBERT and GraphCodeBERT, fora given input Python code snippet. We can see that Figures 10c and10d look more similar to the gold-standard one (Figure 10a) than toFigure 10b. In these/f_igures, some matching parts are marked in red.The result con/f_irms that CodeBERT-5 and GraphCodeBERT-5 (withmultiple layers of Transformer) perform better than CodeBERT-0(without passing through the Transformer layers).Figure 9: The average Spearman correlation for CodeBERTand GraphCodeBERT in multiple programming languages.Summary.Through embedding analysis, we can observe thatthe syntax structure of code has been well preserved in dif-ferent hidden layers of the pre-trained language models (i.e.,CodeBERT and GraphCodeBERT).5.4 RQ3: Syntax Tree InductionWe investigate the extent to which pre-trained code models capturethe syntax structure of code by inducing a tree.Experimental Settings.For comparison, we introduce four tradi-tional greedy top-down tree induction baselines for comparison, e.g.,random, balanced, left-branching, and right-branching binary trees.Take the random-based approach as an example, we recursivelypartition the code snippet based on a randomly selected position.In addition, we also derive another baseline, CodeBERT-0, which isbased on the word embedding before being fed into Transformerlayers. When injecting bias into the syntactic distance, we set thehyperparameter of biasğ¾u1D706to 1. Due to the space limitation, we onlyreport the F1 scores for six common intermediate nodes in PythonAST, i.e.,Parameters,Attribute,Argument,List,Assignment,andStatement.Experimental Results.Table 3 presents the results of various mod-els for syntax tree induction on the test dataset. From this table,we can observe thatthe right-branching tree induction ap-proach achieves the best performance among all the base-lines, con/f_irming our assumption that the AST tends to beright-skewed.When comparing the pre-trained code models (i.e.,CodeBERT and GraphCodeBERT) with other baselines, it is clearto see the pre-trained code models signi/f_icantly outperform otherbaselines, even without bias injection. These results show that theTransformer-based pre-training models are more capable ofcapturing the syntax structure of code to a certain extentthrough pre-training on a large-scale code corpus.When com-paring the pre-trained models w/ and w/o bias injection, we canobserve that injecting bias can increase the performance of syntaxtree induction up to 5%. This improvement indirectly shows thatthe current pre-trained code models do not capture well the prop-erty of the right-skewness of AST. It is worthy mentioning thatthe performance ofassignmenthas been reduced after injection2385ICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin
(a) Gold-standard
(b) CodeBERT-0 prediction
(c) CodeBERT-5 prediction
(d) GraphCodeBERT-5 predictionFigure 10: The heatmaps of gold-standard distance and predicted distance based on pre-trained CodeBERT and GraphCodeBERTmodels in Python. (a) Gold-standard tree distance between all pairs of code tokens in AST. (b-d) The predicted distance basedon the probing of CodeBERT and GraphCodeBERT. The darkness of color indicates the closeness of the paired words.Table 3: Results of syntax tree induction in Python. f: function of distance measurement, L: layer number, A: attention headnumber, AVG: the average of all attentions.Modelf L A F1parameters attribute argument list assignment statementBaselinesRandom Trees- - - 16.93 20.26% 30.75% 32.15% 24.34% 12.98% 16.03%Balanced Trees - - - 16.79 0.46% 32.60% 30.85% 25.54% 14.25% 15.90%Left Branching Trees - - - 18.49 23.25% 43.26% 50.99% 26.77% 5.78% 14.48%Right Branching Trees - - - 26.36 44.07% 43.19% 37.86% 34.18% 8.34% 22.74%CodeBERT-0- - - 19.13 11.67% 25.54% 53.85% 27.62% 18.68% 21.89%Pre-Trained Models (w/o bias)CodeBERTJSD 8 AVG 45.37 40.99% 66.65% 88.42% 56.90% 70.47% 66.10%GraphCodeBERT HEL 8 10 51.34 95.96% 75.50% 67.76% 80.87% 72.88% 63.98%Pre-Trained Models (w/biasğ¾u1D706=1)CodeBERTHEL 9 AVG 50.18 67.37% 67.93% 76.84% 71.60% 56.12% 62.93%GraphCodeBERT HEL 9 AVG 54.80 74.68% 72.05% 84.18% 73.68% 76.10% 72.69%the bias. One possible hypothesis is that although the AST shows aright-skewness trend as whole, several subtrees (e.g., the subtree ofassignment) are not right-skewed.Note that, in the experiments, we have tried diï¬€erent distancefunctions (as shown in Table 1) to measure distance based on at-tention and contextual representation in each Transformer layer.Due to the space limitation, in Table 3, we only present the bestperformance when using diï¬€erent distance functions for each Trans-former layer. We can/f_ind that the JSD and HEL distance functionsthat are performed over attention distributions perform better thanthose over contextual word representations. It shows that parsingtrees from attention information is more eï¬€ective than extractingfrom the contextual representation of pre-trained code models.In Figure 11, we also show a case study of code snippet, withthe induced trees based on CodeBERT, with and without bias in-jected. We can see that several motif structures have been capturedby CodeBERT, e.g.,return-images, andself-postprocessor. Itveri/f_ies the eï¬€ectiveness of the syntax tree induction.Summary.The syntax tree of code can be induced by the pre-trained language models for code, to some extent. In addition,extracting parse trees from attention information is more ef-fective than extracting from the contextual representations ofpre-trained code models.6 DISCUSSION6.1 Observed FindingsThrough comprehensive analysis from three perspectives of pre-trained code models, we observe several insightful/f_indings, whichmay inspire future study. From attention analysis, we/f_ind that awordâ€™s attention distribution can align with the AST. The attentionsalign better with syntax connection in deeper layers than lowerlayers in the self-attention network. Moreover, we/f_ind that thereexit position-based heads, which do not consider the context oftext. It could suggest that if we remove these heads, it will notaï¬€ect the/f_inal results and we can reduce the number of parametersof the pre-trained models. Then, we/f_ind the pre-trained modelscan embed syntactic information in the hidden layers. All pairsof words know their syntactic distance, and this information is aglobal structural property of the vector space. Finally, we use asimple tree construction algorithm to induce a syntax tree from pre-trained models. The results indicate that the pre-trained model suchas CodeBERT is capable of perceiving the syntactic informationto a certain extent when training on a large corpus. Our/f_indingssuggest that grammatical information can be learned by the pre-trained model, which could explain why a pre-trained model suchas CodeBERT can achieve promising results in a variety of sourcecode related downstream tasks such as code summarization, codesearch, and clone detection.2386What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source CodeICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USA  	

		
Figure 11: A case study of syntax tree induction based on CodeBERT for a given Python code snippet.6.2 Limitations and Future WorkOne limitation of our work is that the adopted structural analysisapproaches are based on the AST structure of code, which couldbe just one aspect where the pre-trained models can achieve goodresults for source code. In our future work, we will investigate howthe pre-trained models learn other aspects of source code, suchas code tokens, control-$ow graphs (CFGs), and data-$ow graphs(DFGs). Besides, in this paper, we only investigate two representa-tive self-supervised pre-trained language models for source code,i.e., CodeBERT and GraphCodeBERT. It will be interesting to extentthe analysis to other supervised learning models, as well as otherdeep neural networks (e.g., LSTM and CNN).With regard to the design of the structural analysis approachesadopted in this paper, one limitation is that the structural probingon word embedding we currently use is relative simple. It wouldbe interesting to develop a deep neural network to learn bettermapping functions. Meanwhile, the tree construction algorithmwe use is a relatively simple top-down recursive binary tree algo-rithm. Theright-skewnessbias we use was originally designed forthe constituency trees in natural languages (e.g., English), whichcan be improved for ASTs. Lastly, the AST structure is more com-plex than the induced tree, therefore there is still ampler room forimprovement in the grammar induction algorithm.7 RELATED WORKRecently, there has been much eï¬€ort in interpreting the BERT mod-els in the NLP community. At a high level, these interpretationapproaches are developed from two perspectives: (1) interpretingthe learned embedding, and (2) investigate whether BERT can learnsyntax and semantic information of natural languages. To interpretthe learned embedding, Ethayarajh[10]studies whether the contex-tual information are preserved in the word embedding learned frompre-training models, including BERT, ELMo, and GPT-2. Mickuset al. [26]systematically evaluate the pre-trained BERT using adistributed semantics models. Conneau et al. [6]and Liu et al. [23]design several probing tasks to investigate whether the sentenceembedding can capture the linguistic properties.To investigate the syntax and semantic knowledge in BERT, Ten-ney et al. [38]develop a series of edge probing tasks to explorehow the syntactic and semantic structure can be extracted fromdiï¬€erent layers of pre-trained BERT. Htut et al. [16]propose toextract implicit dependency relations from the attention weightsof each layer/head through two approaches: taking the maximumattention weight and computing the maximum spanning tree. He-witt and Manning[14]propose a structural probing approach toinvestigate whether the syntax information are preserved in wordrepresentations.Specially, there also exists another line of work on visualizing at-tentions to investigate which part of the feature space the model putmore focus. Kovaleva et al. [21]study self-attention and conduct aqualitative and quantitative analysis of the information encoded byindividual BERTâ€™s heads. Hoover et al. [15]introduce a tool, calledexBERT, to help humans conduct$exible, interactive investigationsand formulate hypotheses during the model-internal reasoning pro-cess. Following this line of research, this paper proposes to extendand adapt the interpretation techniques from the NLP communityto understand and explain what feature correlations can be capturedby a pre-trained code model in the embedding space.8 CONCLUSIONIn this paper, we have explored the interpretability of pre-trainedlanguage models for source code (e.g., CodeBERT, GraphCode-BERT). We conduct a thorough structural analysis from the follow-ing three aspects, aiming to give an interpretation of pre-trainedcode models. First, we analyze the self-attention weights and alignthe weights with the syntax structure. Second, we propose a struc-tural probing approach to investigate whether the contextual rep-resentations in Transformer capture the syntax structure of code.Third, we investigate whether the pre-trained code models havethe capability of inducing the syntax tree without training. Theanalysis in this paper has revealed several interesting/f_indings thatcan inspire future studies on code representation learning.Artifacts.All the experimental data and source code used inthis work will be integrated into the open-source toolkit N()/u.sc/hyphen.sc,(-CC [42], which is available at https://github.com/CGCL-codes/naturalcc.ACKNOWLEDGMENTSThis work is supported by National Natural Science Foundationof China under grand No. 62102157. This work is also partiallysponsored by Tencent Rhino-Bird Focus Research Program of BasicPlatform Technology. We would like to thank all the anonymousreviewers for their constructive comments on improving this paper.2387ICSE â€™22, May 21â€“29, 2022, Pi/t_tsburgh, PA, USAYao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai JinREFERENCES[1]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-erating Sequences from Structured Representations of Code. InProceedings ofInternational Conference on Learning Representations.[2]Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-ing distributed representations of code.Proceedings of the ACM on ProgrammingLanguages3, POPL (2019), 1â€“29.[3]Nadezhda Chirkova and Sergey Troshin. 2021. Empirical study of transformersfor source code. InProceedings of 29th ACM Joint European Software EngineeringConference and Symposium on the Foundations of Software Engineering. 703â€“715.[4]Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019.What Does BERT Look at? An Analysis of BERTâ€™s Attention. InProceedings ofthe 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networksfor NLP. 276â€“286.[5]Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.InProceedings of International Conference on Learning Representations.[6]Alexis Conneau, GermÃ¡n Kruszewski, Guillaume Lample, LoÃ¯c Barrault, andMarco Baroni. 2018. What you can cram into a single vector: Probing sentenceembeddings for linguistic properties. InProceedings of the 56th Annual Meetingof the Association for Computational Linguistics. 2126â€“2136.[7]Gregory W. Corder and Dale I. Foreman. 2014.Nonparametric statistics: A step-by-step approach. John Wiley & Sons.[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies. 4171â€“4186.[9]Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, JianfengGao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Uni/f_ied Language Model Pre-training for Natural Language Understanding and Generation. InProceedings ofAdvances in Neural Information Processing Systems. 13042â€“13054.[10]Kawin Ethayarajh. 2019. How Contextual are Contextualized Word Represen-tations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conference on Natural Language Processing.55â€“65.[11]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:A Pre-Trained Model for Programming and Natural Languages. InProceedings ofFindings of the Association for Computational Linguistics: EMNLP 2020. 1536â€“1547.[12]Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. InProceedings of 40th International Conference on Software Engineering. 933â€“944.[13]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, LongZhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao KunDeng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, andMing Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with DataFlow. InProceedings of 9th International Conference on Learning Representations.[14]John Hewitt and Christopher D. Manning. 2019. A Structural Probe for FindingSyntax in Word Representations. InProceedings of the 2019 Conference of theNorth American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies. 4129â€“4138.[15]Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. 2020. exBERT: AVisual Analysis Tool to Explore Learned Representations in Transformer Models.InProceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics: System Demonstrations. 187â€“196.[16]Phu Mon Htut, Jason Phang, Shikha Bordia, and Samuel R. Bowman. 2019. DoAttention Heads in BERT Track Syntactic Dependencies?CoRRabs/1911.12246(2019).[17]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and MarcBrockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of SemanticCode Search.CoRRabs/1909.09436 (2019).[18]Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.Learning and Evaluating Contextual Embedding of Source Code. InProceedingsof the 37th International Conference on Machine Learning, Vol. 119. 5110â€“5121.[19]Hong Jin Kang, TegawendÃ© F. BissyandÃ©, and David Lo. 2019. Assessing theGeneralizability of Code2vec Token Embeddings. InProceedings of 34th IEEE/ACMInternational Conference on Automated Software Engineering. IEEE, 1â€“12.[20]Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Lee. 2020. Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines forGrammar Induction. InProceedings of 8th International Conference on LearningRepresentations.[21]Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019.Revealing the Dark Secrets of BERT. InProceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing. 4364â€“4373.[22]Lucien Le Cam and Grace Lo Yang. 2012.Asymptotics in statistics: some basicconcepts. Springer Science & Business Media.[23]Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A.Smith. 2019. Linguistic Knowledge and Transferability of Contextual Repre-sentations. InProceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies,NAACL-HLT. 1073â€“1094.[24]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: ARobustly Optimized BERT Pretraining Approach.CoRRabs/1907.11692 (2019).[25]Christopher Manning and Hinrich Schutze. 1999.Foundations of statistical naturallanguage processing. MIT press.[26]Timothee Mickus, Denis Paperno, Mathieu Constant, and Kees van Deemter.2019. What do you mean, BERT? Assessing BERT as a Distributional SemanticsModel.CoRRabs/1911.05758 (2019).[27]TomÃ¡s Mikolov, Kai Chen, Greg Corrado, and Jeï¬€rey Dean. 2013. Eï¬ƒcient Esti-mation of Word Representations in Vector Space. In1st International Conferenceon Learning Representations, Workshop Track Proceedings.[28]TomÃ¡s Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeï¬€rey Dean.2013. Distributed Representations of Words and Phrases and their Composi-tionality. InProceedings of Advances in Neural Information Processing Systems.3111â€“3119.[29]Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, ChristopherClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep-resentations. InProceedings of the 2018 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies.2227â€“2237.[30]Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code completion withstatistical language models. InProceedings of the 35th ACM SIGPLAN Conferenceon Programming Language Design and Implementation. 419â€“428.[31]Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertol-ogy: What we know about how bert works.Transactions of the Association forComputational Linguistics8 (2020), 842â€“866.[32]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural MachineTranslation of Rare Words with Subword Units. InProceedings of the 54th AnnualMeeting of the Association for Computational Linguistics.[33]Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron C.Courville, and Yoshua Bengio. 2018. Straight to the Tree: Constituency Parsingwith Neural Syntactic Distance. InProceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics. 1171â€“1180.[34]Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: MaskedSequence to Sequence Pre-training for Language Generation. InProceedings ofthe 36th International Conference on Machine Learning, Vol. 97. PMLR, 5926â€“5936.[35]Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020. Flow2vec: Value-$ow-based precise code embedding.Proceedings of the ACM on ProgrammingLanguages4, OOPSLA (2020), 1â€“27.[36]Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, XinTian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced Represen-tation through Knowledge Integration.CoRRabs/1904.09223 (2019).[37]Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learningwith neural networks. InProceedings of Advances in neural information processingsystems. 3104â€“3112.[38]Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Clas-sical NLP Pipeline. InProceedings of the 57th Conference of the Association forComputational Linguistics. 4593â€“4601.[39]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez,.ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. InProceedings of Advances in neural information processing systems.5998â€“6008.[40]Jesse Vig and Yonatan Belinkov. 2019. Analyzing the Structure of Attentionin a Transformer Language Model. InProceedings of the 2019 ACL WorkshopBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 63â€“76.[41]Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, andNazneen Fatema Rajani. 2021. BERTology Meets Biology: Interpreting Attentionin Protein Language Models. InProceedings of 9th International Conference onLearning Representations.[42]Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Yulei Sui, Hongyu Zhang,Kazuma Hashimoto, Hai Jin, Guandong Xu, Caiming Xiong, and Philip S. Yu.2022. NaturalCC: An Open-Source Toolkit for Code Intelligence. InProceedings of44th International Conference on Software Engineering, Companion Volume. ACM.[43]Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, andPhilip S. Yu. 2019. Multi-modal Attention Network Learning for Semantic SourceCode Retrieval. InProceedings of 34th IEEE/ACM International Conference onAutomated Software Engineering. IEEE, 13â€“25.[44]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, andPhilip S. Yu. 2018. Improving automatic source code summarization via deep rein-forcement learning. InProceedings of the 33rd ACM/IEEE International Conferenceon Automated Software Engineering. ACM, 397â€“407.[45]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.Retrieval-based neural source code summarization. InProceedings of 42nd Inter-national Conference on Software Engineering. ACM, 1385â€“1397.2388