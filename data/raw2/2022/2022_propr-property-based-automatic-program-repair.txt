PropR: Property-Based Automatic Program Repair
Matth√≠as P√°ll Gissurarson‚àó
Chalmers University of Technology
Gothenburg, Sweden
pallm@chalmers.seLeonhard Applis‚àó
TU Delft
Delft, Netherlands
L.H.Applis@Tudelft.nlAnnibale Panichella
TU Delft
Delft, Netherlands
A.Panichella@Tudelft.nl
Arie van Deursen
TU Delft
Delft, Netherlands
Arie.vanDeursen@Tudelft.nlDavid Sands
Chalmers University of Technology
Gothenburg, Sweden
dave@chalmers.se
ABSTRACT
Automaticprogramrepair(APR)regularlyfacesthechallengeof
overfitting patches ‚Äî patches that pass the test suite, but do not
actuallyaddresstheproblemswhenevaluatedmanually.Currently,
overfit detection requires manual inspection or an oracle mak-
ingqualitycontrolofAPRanexpensivetask.Withthiswork,we
want to introduce properties in addition to unit tests for APR to
address the problem of overfitting. To that end, we design and im-
plement PropR, a program repair tool for Haskell that leverages
bothproperty-basedtesting(viaQuickCheck)andtherichtypesys-
temandsynthesisofferedbytheHaskellcompiler.Wecomparethe
repair-ratio,time-to-first-patchandoverfitting-ratiowhenusing
unit tests, property-based tests, and their combination. Our results
showthatpropertiesleadtoquickerresultsandhavealoweroverfit
ratiothanunittests.Thecreatedoverfitpatchesprovidevaluable
insight into the underlying problems of the program to repair (e.g.,
in terms of fault localization or test quality). We consider this step
towardsfitter, or at least insightful, patches a critical contribution
to bring APR into developer workflows.
CCS CONCEPTS
‚Ä¢Softwareanditsengineering ‚ÜíSearch-basedsoftwareen-
gineering; Automatic programming ; Functional languages; Source
code generation.
KEYWORDS
automatic program repair, search based software engineering, syn-
thesis, property-based testing, typed holes
ACM Reference Format:
Matth√≠as P√°ll Gissurarson, Leonhard Applis, Annibale Panichella, Arie van
Deursen,andDavidSands.2022.PropR:Property-BasedAutomaticProgram
Repair.In 44thInternationalConferenceonSoftwareEngineering(ICSE‚Äô22),
May 21‚Äì29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3510003.3510620
‚àóThe first two authors contributed equally to this paper
.
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.35106201 INTRODUCTION
Haveyoueverfailedtobeperfect?Don‚Äôtworry,sohaveautomatic
programrepair(APR)approaches.APRfacesmanychallenges,someinheritedfromsearch-basedsoftwareengineering(SBSE),likeover-
fitting [52,67], predictive-evaluation in search [ 73], and duplicate
handling[ 9].Otherchallengesareuniquetothedomainitself,such
as deriving ingredients for a fix [ 41] and producing valid programs
[28]. Consequently, APR has open research in all of its core as-
pects: search-space, search-process, and fitness-evaluation. The
researchcommunityisshiftingitsfocustowardsothersolutions,
either leaving behind boundaries of search space using generative
neuralnetworks[ 36,42,65],orbyempiricalevidencethatfixesare
oftenrelatedtodependencies,notthecodeitself[ 4,14].Fixesare
usually validated byrunning against the test suiteof the program,
assuming that a solution that passes all tests is a valid patch. How-
ever,LeGouesetal.[ 54]showedthatProgramRepaircan overfit,
i.e.,thatafixpassesthetestsuitedespiteremovingfunctionality
or just bypassing single tests.
Usually,generatedpatchesareevaluatedagainstaunittestsuite
of the buggy program [ 34]. The fitness is defined as the number of
failing tests in the suite [ 40], making a fitness of zero a potential
fix. The problem is the quality of the tests ‚Äî often not all impor-
tant cases are covered, and the search finds something that passes
alltestsbutdoesn‚Äôtprovideallwantedfunctionality[ 52].Thisis
consideredan overfitrepairattempt.Aparticularlygoodexample
for this is the Kali approach [ 54], that removes random statements
of a program. In a later study, Martinez et al. [ 38] showed that out
of 20 of the repair attempts that passed the tests, only one was a
realfix.OneapproachbyYzetal.[ 71]toaddressoverfittingwas
to introduce testsgenerated with EvoSuite [ 15] to havea stronger
test suite, reporting only an improvement in speed, not in found
solutions.Unfortunately,EvoSuiteintroducesanewproblem:Ifthe
programwasfaulty(whichprogramsthatwearetryingtorepair
are), an automatically generated test suite may assert the faultybehavior and make test-based repairs unable to ever produce acorrectprogram,despitepassingthe(generated)testsuite.Thus,
currentautomatedtest-casegenerationisnotthebe-allandend-all
for overfitting in APR.
This work aims to improve APR with addressing the overfitting
problembyintroducingproperties[ 8]inadditiontounittests.A
softwarepropertyisanattributeofafunction(e.g.,symmetry,idem-
potency, etc.) that is evaluated against randomly created instances
ofinputdata.Well-writtenpropertiesoftencoverhundredsof(unit)
tests, making them attractive candidates for fitness evaluation.
17682022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Matth√≠as P√°ll Gissurarson, Leonhard Applis, et al.
We argue that properties can be an improvement to the overfit-
tingchallengeinAPR.Whileproperty-basedtestingframeworks
exist for a range of languages, the practice is particularly natu-
ral for functional programming, and widely used in the Haskell
community.Therefore,weimplementatoolcalledPropR,which
utilizes properties for Haskell-Program-Repair and evaluate the
repair rates and overfitting rates for different algorithms (random
search,exhaustivesearch,andgeneticalgorithms).Ourfixesfollow
a GenProg-like approach [ 34] of representing patches as a set of
changestotheprogram,withthemajordifferencethatourpatch
ingredients (mutations) are sourced by the Haskell compiler using
amechanismcalled typedholes [19].Atypedholecanbeseenas
aplaceholder,forwhichthecompilersuggestselementsthatpro-
duce a compiling program. As these suggestions cover all elements
inscope(notonlythoseusedintheexistingcode),weovercome
tosomedegreetheredundancyassumption[ 41],i.e.,theconcept
thatpatchesaresourcedfromexistingcodeorpatterns,whichis
common to GenProg-like approaches.
Our results show that properties help to reduce the overfit ratio
from85%to63%andleadtofastersearchresults.Propertiescanstill
leadtooverfitting,andtheuniontestsuiteofpropertiesandunit
testsinheritsbothstrengthsandweaknesses.Wethereforeargue
tousepropertiesifpossible,andsuggesttoaimforthestrongest
testsuiteregardlessofthetest-type.Thepatchesfrom PropR can
produce complex repair patterns that did not appear within the
code. Even patches that are overfit can give valuable insight in the
test suite or the original fault.
Our contributions can be summarized as follows:
(1)Introducing the use of properties for fitness functions in
automatic program repair.
(2)Showinghowtogeneratepatchcandidatesusingcompiler
scope, partially addressing the redundancy assumption.
(3)Performing an empirical study to evaluate the improvement
gained by properties with a special focus on manual inspec-
tion of generated patches to detect eventual overfitting.
(4)AnopensourceimplementationofourtoolPropR,enabling
future research on program repair in a strongly typed func-
tional programming context.
(5) Providing the empirical study data for future research.
The remainder of the paper is organized as follows: Section 2
introducesproperty-basedtestingandsummarizestherelatedwork
in the fields of genetic program repair as well as background on
typedholes ,whichareakeyelementofourpatchgenerationmethod.
InSection3wepresenttheprimaryaspectsoftherepairtooland
theirreasoning.Section4presentsthedatausedintheempirical
study,anddeclaresresearchquestionsandmethodology.Theresults
of the research questions are covered in Section 5 and discussed in
Section6.AfterthethreatstovalidityinSection7wesummarizethe
work in Section 8. The shared artifacts are described in Section 9.
2 BACKGROUND AND RELATED WORK
2.1 Property-Based Testing
Property-based testing is a form of automated testing derived from
random testing [ 22]. While random testing executes functions and
APIs on random input to detect error states and reach high code
coverage,property-basedtestingusesadeveloperdefinedattributesprop_1 ::Double ->Test
prop_1x=
sin x~==sin (x+2 *ùúã)
prop_2 ::Double ->Test
prop_2x=
sin (-1*x)~== -1*(sin x)
prop_3 ::Test
prop_3 =sin ( ùúã/2)== 1
prop_4 ::Test
prop_4 =sin0= =0unit_1 ::Test
unit_1 =
sinùúã~==sin (3*ùúã)
unit_2 ::Test
unit_2 =sin0= =0
unit_3 ::Test
unit_3 =sin ( ùúã/2)== 1
unit_4 ::Test
unit_4 =
sin (-1*ùúã/2)== -1*(sin ùúã/2)
(~==)::Double ->Double ->Bool
n~==m=abs (n-m)<=1.0e-6
Figure 1: Comparison of Properties and Unit Tests for sin
calledproperties offunctionsthatmustholdforanyinputofthat
function[ 8].Randomtestsareperformedforthegivenproperty;
If an input is found for which the property returns false or fails
with an error, the property is reported as failingalong with the
input as a counter example [ 8]. Some frameworks will additionally
shrinkthe counter example usinga previously supplied shrinking
function to offer better insight into the root cause of the failure [ 8].
There are some variations on property-based testing, e.g. Small-
Check, which performs an exhaustive test of the property [ 58].
QuickCheckapproximates thisbehavior witha configurablenum-
ber of random inputs (by default 100 random samples). Figure 1
providesanexamplecomparisonofpropertiesandunittestsofa
sine function. The properties require an argument Double ->Test
and must hold for any given Double. On any single QuickCheck
run,202testsareperformed,formingamuchstrongertestsuitefor
a comparable amount of code.
Aremainingquestioniswhetheronecannotjustreproducethese
202 tests by unit tests. For a single seed, this is doable ‚Äî but it is
a special strength of properties that the new tests are randomly
generated on demand. We hope this addresses the problem of over-
fitting[52],asthereareno fixedteststofitonaslongastheseed
changes. Furthermore, we stress that maintaining 2 properties is
easier than maintaining 200 (repetitive) unit tests.
2.2 Haskell, GHC & Typed Holes
Haskell.Haskellisastaticallytyped,non-strict,purelyfunctional
programminglanguage.Itsdesignensuresthatthepresenceofside
effectsisalwaysvisibleinthetypeofafunction,anditistypical
programmingpracticetocleanlyseparatecoderequiringsideeffects
fromthemainapplicationlogic.Thisfacilitatesamodularapproach
totestinginwhichprogrampartscanbetestedinisolationwithout
needing to consider global state or side effects. Haskell‚Äôs rich type
system and type classes allow tools such as QuickCheck [ 8]t o
efficiently test functions using properties, where the inputs are
generatedbyQuickCheckbasedonageneratorforagivendatatype.
Valid Hole-Fits. Our tool is based on using the Glasgow Haskell
Compiler( GHC),whichiswidelyusedinbothindustryandacademia.
GHChasmanyfeaturesbeyondtheHaskellstandard,includinga
1769PropR: Property-Based Automatic Program Repair ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
featureknownas typedholes [19].A‚Äúhole‚Äù,denotedbyanunder-
score character (_), allows a programmer to write an incomplete
program, where the hole is a placeholder for currently missing
code.
Using a hole in an expression generates a type error containing
contextual information about the placeholder, including,most im-
portantly,itsinferredtype.Inadditiontocontextualinformation,
GHCsuggestssome validhole-fits [19].Validholefitsarealistof
identifiers in scope which could be used to fill the holes without
anytypeerrors.Asasimpleexample,considertheinteractionwith
the GHC REPL shown in Figure 2.
GHCi>letdegreesToRadians ::Double ->Double
degreesToRadians d =d*_/ 180
<interactive>:4:30: error:
‚Ä¢ Found hole: _ :: Double
In the expression :d*_/1 8 0
Valid hole fits include
d :: Double (bound at <interactive>:4:22)
pi :: forall a. Floating a => a (imported from ‚ÄòPrelude‚Äô)
Figure 2: Example code with a hole and its valid hole-fits
Herethedefinitionof degreesToRadians containsahole.There
are just two valid hole-fits in scope: the parameter dand the prede-
fined constant pi. GHC can not only generate simple candidates
such as variables and functions, but also refinement hole-fits. A
refinement hole-fit is a function identifier with placeholders for
its parameters. In this way GHC can be used to synthesize more
complex type-correct candidate expressions through a series of
refinement steps up to a given user-specified refinement depth . For
example,settingtherefinementdepthto1willadditionallyprovide,
among others, the following hole-fits:
negate ( _::Double)
fromInteger ( _::Integer)
In this work we use hole fitting for program repair by removing
a potentially faulty sub-expression, leaving a hole in its place, and
using valid hole-fits to suggest possible patches.
Hole-FitPlugins. Bydefault,GHCconsiderseveryidentifierin
scope as a potential hole-fit candidate, and returns those that have
atypecorrespondingtotheholeashole-fits.However,usersmight
wanttoaddorremovecandidatesorrunadditionalsearchusing
a different method or external tools. For this purpose, GHC added
hole-fit plugins [ 17], which allows users to customize the behavior
ofthehole-fitsearch.WhenusingGHCasalibrary,thisalsoallows
usersto extractan internalrepresentation ofthe hole-fits directly
from a plugin, without having to parse the error message.
2.3 GenProg, Genetic Program Repair & Patch
Representation
Search-basedprogramrepaircenteredmostlyaroundtheworkof
Le Goues et al. [ 34] in GenProg, which provided genetic search
for C-program repair. One of the primary contributions was the
representationofapatchasachange(addition,removal,orreplace-
ment) of existing statements. Genetic search is based around the
mutation, creation and combination of chromosomes ‚Äî the basic
building bricks of genetic search. A chromosome of APR is a listofsuchchangesratherthanafullprogram(AST),makingtheap-
proach lightweight. Utilizing changes is based on the Redundancy
Assumption [32], i.e., assuming that the required statements for
thefixalreadyexists.Thecodemightjustusethewrongvariable
or miss a null-check to function properly. This assumption has
been verified by Martinez et al. [ 41], showing that the redundancy
assumption widely holds for inspected repositories. We adopted
thepatch-representationinourtool,butwereabletoweakenthe
redundancy assumption (see Section 3).
SinceGenProg,muchhasbeendoneingeneticprogramrepair
[11]mostlyforJava.ParticularlyAstor[ 39]enabledlotsofresearch
[61,66,69,70]duetoitsmodularapproach,aswellasreal-world
applications [ 59,62]. This modularity, mostly the separation of
fault localization, patch-generation and search is a valuable lesson
learned by the community that we adopted in our tool. Compared
to this body of research, our scientific contributions lie within the
patch-generation and the search-space (see Section 3.1).
2.4 Repair of Formally Verified Programs &
Program Synthesis
Another field of research dominant in functional programming
is formal verification, in which mathematical methods are used
to prove the correctness of programs. Due to its strengths it has
beenwidelyappliedtovarioustasks,suchashardware-verification
[26], cryptographic protocols [ 43] or lately smart contracts [ 6].
But formal verification has also been applied to the domain of
program repair and synthesis [ 30,60], and some languages can
arguablybeconsideredsynthesizersaroundconstraints(e.g.Pro-
log). Using specification-based synthesis in combination with a
SAT solver can be effective, however the accuracy is closely tied
to the completeness of the post-condition constraints [ 20]. For
Haskell, these approaches revolve around liquid types , which en-
richHaskell‚Äôstypesystemwithlogicalpredicatesthatarepassed
ontoanSMTsolverduringtypechecking[ 48,56,57,64].Theex-
istingapproaches[ 21,25,50]focusprimarilyonthesearch-aspects
of program synthesis due to the (infinite) search space and often
perform a guided search similar to proof-systems. The approach
used in the Lifty[51] language is especially relevant: Lifty is a
domain-specific data-centric language in which applications can
be statically and automatically verified to handle data specified
asperdeclarativesecuritypolicies,andsuggestprovablycorrect
repairs when a leak of sensitive data is detected. Their approach
differs in that they target a domain-specific language and focus on
type-drivenrepairofsecuritypoliciesandnotgeneralproperties.
Another interesting approach is the TYGAR based Hoogle+ API
discoverytool,whereuserscanspecifyprogrammingtasksusing
eitheratype,asetofinput-outputtests,orboth,andgetalistof
programs composed from functions in popular Haskell libraries
andexamplesofbehavior[ 24].Itishoweve rfocusedonAPIdiscov-
eryandnotprogramrepair,althoughincorporatingHoogle+into
PropRisaninterestingavenueforfuturework.Theapproachby
Leeetal.[ 35]isinmanywayssimilar;Theyalsooperateonstudent
data and find very valuable insights from repair and identical chal-
lenges. The approach they developed (FixML) exploits typed holes
toalignbuggystudent programswithagiveninstructor-program
based on symbolic execution. FixML is different as it requires a
1770ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Matth√≠as P√°ll Gissurarson, Leonhard Applis, et al.
goldstandard,andsynthesizesbytype-enumerationaftersymbolic
execution. To some degree, this is similar to our implementation
of an exhaustive search. Semantics-based repair using symbolic-
execution like that of Angelix[44] can be very effective in fixing
real-world bugs, and uses symbolic expressions similarly to our
typed-holes. Ho wever, there are some scalability concerns for sym-
bolicexecution,andwhiletheycanbemitigatedusing acarefully
chosen number of suspicious expression and their derived angelic
forests [44], they can also be mitigated using genetic algorithms
andthemorelightweightproperty-basedanalysis,motivatingtheir
usageinPropR.Comparedtoprogramsynthesis,programrepair
is better able to take advantage of a "reasonable" baseline program
from the developers.
Intermsofutilizingspecifications,theprimarybenefitofQuick-
Checkistheeasyadoptionforusers,whereasformalverification
comes with a high barrier of entry for most programs and requires
dedicated and educated developers. To some degree we utilize for-
malverificationduetothetype-correctness-constraintthatalready
greatlyshrinksthesearchspace‚Äîwhileweassertthefunctional
correctness with tests and properties. A full formal verification-
suite might produce better results, but we ease the adoption of our
approach by utilizing comprehensive properties and tests.
3 TECHNICAL DETAILS ‚Äî PROPR
Failing Properties
TargetsHaskell Program Coverage
GHC + PluginFault-involved Expressions
Candidate FixesSource
QuickCheck
Search AlgorithmFixesProperties
DiffApply Fixes
Hole-Fit SynthesisPerforated ExpressionsPerforation
Candidate Selection
Candidate EvaluationInspect Bindings
Fault localizationTest Properties
Rebind In Properties1
234
5
6
78910
11
Figure 3: The PropR test-localize-synthesize-rebind loop
To investigate the effectiveness of combining property-based
tests with type-based synthesis, we implemented PropR. PropR
isanautomatedprogramrepairtoolwritteninHaskell,andusesGHC as a library in conjunction with custom-written hole-fit plug-
insasthebasisforparsingsourcecode,synthesizingfixes,asfor
instrumentingandrunningtests.PropRalsoparametrizesthetests
so that local definitions can be exchanged with new ones, which
allows us to observe the effectiveness of a fix. To automate the
repair process, PropR implements the search methods described in
Section 3.4 to find and combine fixes for the whole program repair.
AnoverviewofthePropRtest-localize-synthesize-rebind(TLSR)
loopisprovidedinFigure3.Thecirclednumbers ninthissection
refer to the labels in Figure 3.
len ::[a] ->Int
len []=0
lenxs=product $map (const ( 1::Int)) xs
prop_abc ::Bool
prop_abc =len"abc"== 3
prop_dup ::[a] ->Bool
prop_dup x=len (x++x)= =2*len x
Figure4:Anincorrectimplementationoflength.We mapover
the list and set all elements to 1::Int, and take the product
of the resulting list. This means that lenwill always return 1
foralllists.Anexpectedfixwouldbetotakethe sumofthe
elements, which would give the length of the list.
As a running example, imagine we had an incorrectimplementa-
tion of a function to compute the length of a list called len, with
properties, as seen in Figure 4.
3.1 Compiler-Driven Mutation
To repair a program, we use GHC to parse and type-check the
source into GHC‚Äôs internal representation of the type-annotated
Haskell AST. By using GHC as a library, we can interact with
GHC‚Äôsrichinternalrepresentationofprogramswithoutresorting
to external dependencies or modeling. We determine the tests to
fix by traversing the AST for top-level bindings with either a type
(TestTree)orname( prop)thatindicatesitisatest 1.WeuseGHC‚Äôs
abilitytoderivedatadefinitionsforalgebraicdatatypes[ 17]andthe
Lenslibrary[ 27]togenerateefficienttraversalsoftheHaskellAST.
Todeterminethefunctionbindingstomutate,wetraversetheASTs
of the properties and find variables that refer to top-level bindings
in the current module 2 . We call these bindings the targets.
In our example, both prop_abc andprop_dup use the local top-
level binding lenin their body, so our target set will be {len}.
Parametrized properties. To generalize over the definition of tar-
getsinthepropertiesandtests,wecreatea parametrizedproperty
fromeachofthepropertiesbychangingtheirbindingtotakeanad-
ditional argument for each of the targetsin their body. This allows
prop'_abc ::([a] ->Int)->Bool
prop'_abc f=f"abc"== 3
prop'_dup ::([a] ->Int)->[a] ->Bool
prop'_dup fx =f( x++x)= =2*fx
Figure 5: The parametrized properties for len
1771PropR: Property-Based Automatic Program Repair ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
abc_prop ::Bool
abc_prop =prop'_abc length
dup_prop ::[a] ->Bool
dup_prop =prop'_dup length
Figure 6: The parametrized properties applied to a different
implementation of len, the standard library length
ustorebind(i.e.,changethedefinitionof)eachofthetargetsbypro-
vidingthemasanargumenttotheparametrizedproperty 3.Once
the parametrized property has received all the target arguments,
it now behaves like the original property, with the target bindings
referring to our mutated definitions. We show the parametrized
properties for the properties in Figure 4 in Figure 5.
The new properties in Figure 6, abc_prop anddouble_prop will
now behave the same as the original prop_abc andprop_dup, but
with every instance of lenreplaced with length:
abc_prop =length"abc"== 3
double_prop x=length (x ++x)= =2*length x
Thisallowstocreatenewdefinitionsof lenandevaluatehowthe
properties behave with the different definitions.
Fault localization. PropR uses an expression-level fault localiza-
tion spectrum [ 1], to which we apply a binary fault localization
method(touchedornottouchedbyfailingproperties).Anotable
difference to other APR tools like Astor is that we can perform
fault localization for the mutatedtargets. This enables PropR to
adjust the search space once a partial repair has been found, i.e.
one that passes a new subset of the properties. Since fault local-
ization is expensive, by default we only perform it on the initial
program similarly to Astor [ 39,40]. GHC‚Äôs Haskell Program Cov-
erage(HPC) can instrument Haskell modules and get a count of
how many times each expression is evaluated during execution
[18].UsingQuickCheck,wefindwhichpropertiesarefailingand
generate a counterexample for each failing property 4 . For prop-
erties without arguments (essentially unit tests), we do not need
any additional arguments, so we can run the property as-is: the
counterexampleisthepropertyitself.Byapplyingeachproperty
to its counterexample and instrumenting the resulting program
with HPC, we can see exactly which expressions in the module are
evaluated in a failing execution of property 5. The expressions
evaluatedinthecounterexampleofthepropertyarepreciselythe
expressions for which a replacement would have an effect: non-
evaluatedexpressionscannotcontributetothefailingofaproperty.
We call these the fault-involved expressions . These will be allthe
expressionsinvolvedinfailingtests/properties,andcoversevery
expression invoked when running counter-examples.
In our simple example, only prop_dup requires a counterexam-
ple, for which QuickCheck produces a simple, non-empty list, [()].
When we then evaluate prop_abc andprop_dup [()], only the ex-
pressions in the non-empty branch of lenare evaluated: the empty
branch is not involved in the fault.
Perforation. For the targets, we generate a version of the AST
with a new typed hole in it, in a process we call perforation . When
we perforate a target, we generate a copy of its AST for each fault-
involvedexpressioninthetarget,wheretheexpressionhasbeenreplaced with a typed hole 6. The perforated ASTs are then com-
piledwithGHC.Sincetheynowhaveatypedhole,thecompilation
will invoke GHC‚Äôs valid hole-fit synthesis [ 19]7. We present a
few examples of the perforated versions of lenin Figure 7.
len []=0
lenxs=_
len []=0
lenxs=_$map (const ( 1::Int)) xs
len []=0
lenxs=product $_
len []=0
lenxs=product $_(const ( 1::Int)) xs
...
Figure 7: A few perforated versions of len. N.B. the empty
branch is not perforated, as it is not involved in the fault
3.2 Fixes
Afixisrepresentedasamap(lookuptable)from sourcelocations in
the module to an expression representing a fix candidate. Merging
two fixes is done by simply merging the two maps. Candidate fixes
in PropR come in three variations, hole-fit candidates ,expression
candidates , andapplication candidates .
Hole-fitCandidates. Usingacustomhole-fitplugin,weextract
thelistofvalidhole-fitsforthathole,andnowhaveawell-typed
replacement for each expression in the target AST.
Foundhole :_::[Int]->Int
Inan equation for 'len' :
len xs =_$map (const ( 1::Int)) xs
Validhole fits include
head ::[a] ->a
last ::[a] ->a
length ::Foldable t=>ta ->Int
maximum ::(Foldable t,Orda)=>ta ->a
minimum ::(Foldable t,Orda)=>ta ->a
product ::(Foldable t,Numa)=>ta ->a
sum ::(Foldable t,Numa)=>ta ->a
Validrefinement hole fits include
foldl1 ( _::Int ->Int ->Int)
...
Figure 8: Hole-fits for a perforation of len, where producthas
been replaced with a hole
Wederivehole-fitcandidatesdirectlyfromGHC‚Äôsvalidhole-fits,
asseeninFigure8,givingrisetothefixesinFigure9.Thesetake
the form of an identifier (e.g., sum), or an identifier with additional
holes (e.g., foldl1 _) for refinement fits.
Sincewesynthesizeonlywell-typedprograms,wecannotuse
refinement hole-fits directly: the resulting program would produce
a typed hole error. To use refinement hole-fits, we recursively syn-
thesizefitsfortheholesintherefinementhole-fitsuptoadepth
1772ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Matth√≠as P√°ll Gissurarson, Leonhard Applis, et al.
{<interactive:3:10-15>: head}
{<interactive:3:10-15>: last}
{<interactive:3:10-15>: length}
...
{<interactive:3:10-15>: sum}
Figure 9:Candidate fixesderived fromthe validhole-fits in
Figure 8. The location refers to productinlen
configurable by the user. This means that we can generate e.g.,
foldl1(+)when the depth is set to 1, and e.g., foldl1(flip _)
‚Üífoldl1(flip (-))for a depth of 2, etc. By tuning the refine-
ment level and depth, we could synthesize most Haskell programs
(exceptingconstants).However,inpracticalterms,theamountof
work grows exponentially with increasing depth.
Tobeabletofindfixesthatincludeconstants(e.g., StringorInt)
orfixesthatwouldotherwiserequireahighanddeeprefinement
level, we search the program under repair for expression candidates
[37].Theseareinjectedintoourcustomhole-fitpluginandchecked
whether they fit a given hole using machinery similar to GHC‚Äôs
valid hole-fit synthesis, but matching the type of an expression in-
stead of an identifier in scope. In our example, these would include
0,(1::Int),(x++x),andmore.Foreachexpressioncandidate,we
thencheckthatallthevariablesreferredtointheexpressionsarein
scope,andthattheexpressionhasanappropriatetype.Wealsolook
atapplication candidates of the form (_x), wherexis some expres-
sionalreadyintheprogram,and_isfilledinbyGHC‚Äôsvalidhole-fit
synthesis.Thisallowsustofindcommondatatransformationfixes,
such asfilter(not.null).
Regardless of technical limitations, this approach can be consid-
ered a form of localized program synthesis exploited for program
repair.Byusingvalidhole-fits,wecanutilizethefullpower GHC‚Äôs
type-checker when finding candidates and avoid having to model
GHC‚Äôs ever-growing list of language extensions. This allows us to
drastically reduce the search space to well-typed programs only.
3.3 Checking Fixes
Oncewehavefoundacandidatefix,weneedtocheckwhetherthey
work.WeapplyafixtotheprogrambytraversingtheAST,andsub-
stituting the expression found in the map with its replacement. We
len1 []=0
len1xs=head$map (const ( 1::Int)) xs
...
len3 []=0
len3xs=length$map (const ( 1::Int)) xs
...
len7 []=0
len7xs=sum$map (const ( 1::Int)) xs
Figure10:NewtargetsdefinedbyapplyingthefixesinFig-
ure 9 to the original len
dothisforalltargets,andobtainnewtargetswherethelocationsof
the holes have been replaced with fix candidates. For the given len
example, the fixes in Figure 9 give rise to the definitions shown in
Figure10. Wethen constructa checking programthat appliesthe
parametrizedpropertiesandteststothesenewtargetdefinitionsand compile the result. A simplified example of this can be seen
PropR>mapM sequence
[[quickCheck (prop'_abc len1), quickCheck (prop'_dup len1)]
,[quickCheck (prop'_abc len2), quickCheck (prop'_dup len2)]
,[quickCheck (prop'_abc len3), quickCheck (prop'_dup len3)]
...
,[quickCheck (prop'_abc len7), quickCheck (prop'_dup len7)]]
-- Evaluates to:
[[False,False],[False,False],[True,True],[False,False]
,[False,False],[False,False],[True,True]]
Figure 11: Checking our new targets from Figure 10
inFigure11,thoughwedoadditionalworktoextracttheresults
in PropR. It might be the case that the resulting program does not
compile: as our synthesis is based on the types, we might generate
programsthatdonotparsebecauseofadifferenceinprecedence
(precedence is checked during renaming, aftertype-checking in
GHC). We remove all those candidate fixes that do not compile, ob-
taininganexecutablethattakesasanargumentthepropertytorun,
andreturnswhetherthatpropertyfailed.Werunthisexecutablein
a separate process: running it in the same process might cause our
ownprogramtohangduetoaloopinthecheck.Byrunningina
separate process,we can kill itafter a timeoutand decide thatthe
given fix resulted in an infinite loop. After executing the program,
we have three possible results: all properties succeeded; the pro-
gram did not finish due to an error or timeout; or some properties
failed8. In our example, we see in Figure 11 that len3andlen7
passalltheproperties,meaningthatreplacing productwithlength
orsumqualifies as a repair for the program.
3.4 Search
WithinPropR,weimplementedthree differentsearchalgorithms:
random search ,exhaustive search , andgenetic search 9.
Allthreealgorithmsshareacommonconfiguration:theyallhave
atimebudget(measuredinwallclocktime)afterwhichtheyexit,
and return the results (if any) that they‚Äôve found.
Forthegeneticsearch ,PropRimplementsbestpracticesand
algorithmscommontoothertoolssuchasAstor[ 39]orEvoSuite
[15]. A mutation consists of either dropping a replacement of a fix,
or adding a new replacement to it. The initial population is created
as picking nrandom mutations. The crossover randomly picks cut
points within the parent chromosomes, and produces offspring by
swapping the parents‚Äô genes around the cut points. We support
environment-selection [ 23] with an elitism-rate [ 3] for truncation.
Elitismmeansthatwepick thetop ùë•%percentofthe fittestcandi-
datesforthenextgeneration,fillingtheremaining (100‚àíùë•)%with
(other)randomindividualsfromthepopulation.Wechooserandom
pairsfromthelastpopulationasparentsandperformenvironment
selection on the parents and their offspring. Our manual sampling
ofrepairs-in-progressonthedatapointsshowedthatgeneticsearch
requireshigh churninordertobeeffective:changingasingleex-
pressionoftheprogramusuallyfailedmorepropertiesthanitfixed.
Hence, the resulting configurations for the experiment have a low
elitism- and high mutation- and crossover-rate.
Withinrandom search , we pick (up to a configurable size)
evaluated holes at random and pick valid hole-fits at random with
1773PropR: Property-Based Automatic Program Repair ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
which to fill them. We then check the resulting fix and cache it.
The primary reason for using random search is to show that the
geneticsearchisanimprovementover guessing.Nevertheless,Qiet
al.[53]showedthatrandomsearchsometimescanbesuperiorto
genetic search, further motivating its application. Besides, random
search is a standard baseline in search-based software engineering
to assess whether more ‚Äúintelligent‚Äù search algorithms are needed
for the problem under analysis.
Forexhaustivesearch ,wecheckeachhole-fitinabreadth-first
manner: first all single replacement fixes, then all two replacement
fixes and so on until the search budget is exhausted. Exhaustive
searchisdeterministicapartfrominherentrandomnessinQuick-
Check. We use exhaustive search to demonstrate the complexity of
theproblem,andtoshowthatsearchisbetterthanenumeration.
The deterministic search pattern of exhaustive search would be
ideal for a single fix problem such as our example.
The fitness for all searches is calculated as the failure ratio
number of failures
number of tests, with a non-termination or errors treated as the
worstfitness1andafitnessof0(alltestspassing)marksacandidate
patch.Suchpatchesareremovedfrompopulationsingeneticsearch
and replaced by a new random element.
Within the test-localize-synthesize-rebind loop (Figure 3) we
perform one generation of genetic search per loop, and after the
selectionofchromosomestheprogramisre-boundandcoveragere-
evaluated. The authors observed that this is a bit over-engineered
for smallprograms ‚Äî the faultlocalization did notgreatly change
whentheprogramshadonlyasinglefailingproperty.Asanopti-
mization, we added a flag to skip the steps 5to7in the loop to
speed up the actual search. This configuration was enabled during
experiments presented in Section 4. The exhaustive and random
search do not perform any rebinding.
3.5 Looping and Finalizing Results
Looping. If thereare stillfailing properties afteran iterationof
the loop, we apply the current fixes we have found so far to the
targetsandenterthenextiterationoftheloop 10,repeatingthe
processwiththenewtargetsuntilallpropertieshavebeenfixed,
or the search budget runs out.
Finalizing and Reporting Results. After we have found a set of
valid fixes that pass all the properties, we generate a diff for the
originalprogrambasedontheprogrambindingsandthemutated
targets constituting the fix 11. This way the resulting patches can
be fed into other systems such as editors or pull requests.
4 EMPIRICAL STUDY
4.1 Research Questions
Given the concepts presented in Section 3, research interests are
twofold: How well does the typed hole synthesis perform for APR,
andwhatistheindividualcontributionofproperties.Aswithinthe
integralapproachof PropR,theeffectscannottrulybedissected;
The only contributions that we can separate for distinct inspection
istheuseofproperties,underwhichwewillinvestigatethepatches
generated by PropR.diff --git a/<interactive> b/<interactive>
--- a/<interactive>
+++ b/<interactive>
@@ -1,2 +1,2 @@ len [] = 0
len [] = 0
-len xs = product $ map (const (1 :: Int)) xs
+len xs = length $ map (const (1 :: Int)) xs
diff --git a/<interactive> b/<interactive>
--- a/<interactive>
+++ b/<interactive>
@@ -4,2 +4,2 @@ len [] = 0
len [] = 0
-len xs = product $ map (const (1 :: Int)) xs
+len xs = sum $ map (const (1 :: Int)) xs
Figure 12: The final result of our repair for len
Wefirstwanttoanswerwhetherpropertiesaddvalueforguiding
thesearch.Ideally,propertiesshould improvethe repair-rate,speed
and quality regardless of the approach, which we address in RQ1:
Research Question 1
Towhatextentdoesautomaticprogramrepairbenefitfrom
the use of properties?
Given that properties do have an impact (for better or worse),
we want to quantify its extent on configuration and selection of
searchalgorithms.Forexample,weexpectthattheuseofproperties
helpswithfitnessandsearch,butwillincreasethetimerequiredfor
evaluation‚Äîthiswouldmotivatetoconfigurethegeneticsearchto
havesmallbutwellguidedpopulations.Toelaboratethiswedefine
RQ2 as follows:
Research Question 2
Howcanweimprove(andconfigure)searchalgorithmswhen
used with properties?
Withthelastresearchquestionwewanttoperformaqualitative
analysisontheresultsfound.Previousresearchshowedthat just
maximizing metrics is not sufficient. With a manual analysis we
lookfortheissueofoverfittingandtrytoinvestigatenewissues
and new patterns of overfitting.
Research Question 3
To what extent is overfitting in automatic program repair
addressed by the use of properties?
4.2 Dataset
Thenoveldatasetstemsfromastudentcourseonfunctionalpro-
gramming. Withinthe exercise, thestudents had toimplement a
calculatorthatparsesatermfromtext,calculatesresultsandderiva-
tions.Whiletheoverallnotionisthatofaclassroomexercise,the
problem nevertheless contains real-world tasks asserted by real-
world tests. The calculator itself is a classic student-exercise, but
1774ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Matth√≠as P√°ll Gissurarson, Leonhard Applis, et al.
Table 1: Parameters for Grid Experiment
parameter inspected values
tests Unit Tests ; Properties ; Unit Tests + Properties
search random ; exhaustive ; genetic
termination 10 minute search-budget
seeds 5seeds
thesubtaskofparsingisbothcommonanddifficult,representing
avaluablecaseforAPR.Intotal,wecollected 30programs that
all fail at least one of 23 properties and one of 20 unit tests . The
programs range from 150 to 700 lines of code (excluding tests) and
haveatleast5topleveldefinitions.Theseare commonfile-sizesfor
Haskell, e.g. PropR itself has an average of 200 LoC per file. The
faults are localized to one of the three modules provided to PropR.
Themostviolatedtestsareeitherrelatedtoparsingandprinting
(especially of trigonometric functions, also seen in Figure 18) or
about simplification (seen in Figure 13), which are core-parts of
the assignment. The calculator makes a particularly good example
for properties, as attributes such as commutativity, associativity
etc. are easy to assert but harder to implement. Hence, we argue
that the calculator-exercise makes a case for typical programs that
implement properties (i.e., they are not artificially added for APR).
Data points were selected from the students submissions if they
fulfilled the following attributes: Ait compiled Bit failed the
unittestsuite andtheproperty-basedtestsuiteseparately.An error-
producing test is considered as a normal failure. We selected them
by these criteria to draw per-data-point comparisons of properties
to unit tests and their unison. We consider a separate investigation
of repairing unit test failing programs versus properties failing
programs and their overfitting future research.
prop_simplify_idempotency ::Expr ->Bool
prop_simplify_idempotency e=
simplify (simplify e) ==simplify e
Figure 13: A property asserting the idempotency of simplify
The anonymized data is provided in the reproduction package.
4.3 Methodology / Experiment Design
To evaluate RQ1 and RQ2 we perform a grid experiment on the
dataset withthe parameters presentedin Table1. For everyof the
45configurationswemakearepairattemptoneverypointinthe
dataset. The genetic search uses a single set of parameters that
wasdeterminedthroughprobing.Weutilizedockerandlimitevery
container to 8 vCPUs @ 3.6ghz and 16gb RAM (the container‚Äôs
lifetime is exactly one data-point). Further information on the data
collection can be found in the reproduction package.
Giventhisgridexperiment,wecollectthefollowingvaluesfor
each data point in the dataset:
(1) Time to first result
(2) Number of distinct results within 10 minutes
(3) The fixes themselves
The search budget starts after a brief initialization, as PropR
loads and instruments the program. We round the measured timestotwodigitsasrecommendedbyNeumannetal.andremoveType-
1-Clones (identical up to whitespace) from the results [29, 45].
To answer RQ1 we check every trial whether at least one patch
was found (whether it was solved). We then perform a Fisher exact
test [55] to see if the entries originate from the same population,
i.e., if they follow the same distribution. We consider results with a
p-value of smaller than 0.05 as significant.
To answer RQ2 we perform a pairwise Wilcoxon-RankSum test
[49] on the data points grouped by their test configuration. The
Wilcoxon test is a non-parametric test and does not make any
assumptionondatadistribution.Initspairwiseapplication,wefirst
comparetheeffectofunittestsagainsttheeffectofproperties,then
unittestsagainstcombinedunittestsandpropertiesetc.Wechoose
a significance level of 95%.
Afterwehaveseenwhetherpropertieshaveasignificantimpact
on program repair, we can quantify the effect size by applying the
Vargha-Delaney test [ 63] to the given pairs of configurations. In
theVargha-Delaneytest,avalueofe.g.0.7meansthatalgorithm
BisbetterthanalgorithmAin70%ofthecases,estimatingasim-
ilarprobabilityofdominanceforfutureapplicationsonsimilarly
distributed data points. Note that a result of 0.5 does not mean
there was no effect ‚Äî the groups can still be significantly different
without being clearly better.
RQ3 can (to the best of our knowledge) only be answered by
humanevaluation.Existingresearchonautomaticpatch-validation
byQi[68]requiresanautomatictest-generationframework(which
isnotavailableforHaskell)aswellasagold-standardfixtoworkas
anoracle.Theyusedexistinggit-fixesasoracles,butweexpectsome
data points to be correct despite not matching the sample-solution.
Similarly,workbyNilizadehetal.[ 46]utilizesformalverification
to automatically verify generated patches, but unfortunately, no
specificationswereavailableforourdataset.Instead,weperform
the analysis manually, similar to [ 54] and [38]. As there are too
many results to manually inspect, we sampled 70 fixes1and let
two authors label them as overfitornot overfit . The authors do
so based on their domain-knowledge and in accordance with a
given gold-standard. On disagreement, the authors provide a short
written statement before discussing and agreeing on the fix-status.
Theconclusionofthediscussionisalsodocumentedwithashort
statement. The manual labels as well as the statements are shared
within the replication package.
5 RESULTS
The following section answers the research questions in order and
presents general information gained in the study.
RQ1‚ÄîRepairRate. Intotal,PropRmanagedtofind patchesfor
13 of 30 programs of the dataset. In Table 2 we show the detailed
results of these 13 programs. We found 228 patches in total, with
amedianof3patchespersuccessfulrun .Avisualizationofthe
results can be seen in Figure 14 and Figure 15.
Foreveryentry,weperformedaFisherexacttestbasedonthe
repairperseedofeverytestsuite.Thecontingencytablesarebased
on whether the specific seed found patches for the test suite. It
1The threshold of 70 has been calculated after seeing 230 patches being generated,
which is sufficient sample for a p-value of 0.05 at an error rate of 10%
1775PropR: Property-Based Automatic Program Repair ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Table 2: Number of independent runs that produced at least one patch for genetic search
Programs E01 E02 E03 E04 E05 E07 E08 E09 E12 E13 E14 E18 E25
Units 0155555550005
P r o p s 5110555521523
Both 0140155530003
showedthat4ofthe13repairedentriesweresignificantlybetterin
producingrepairswithproperties(E1,E3,E4,andE14fromTable2).
AglobalFisherexacttestandWilcoxon-RankSumtestshowed
nostatisticalsignificantdifferencebetweenthetestsuites(p-values
of 10%-20%). Whether properties are beneficial is a highly specific
topic, and we expect it more to be a matter whether the bug is
properly covered by the test suite. We argue that properties can
producestrongertestsuitesthanunittests,butwhethertheyare
applicableandwellimplementedisultimatelyuptothedevelopers.
Figure 14: Solved Entries per Test-Suite and Algorithm
Figure14showsgeneticsearchoutperformingexhaustivesearch
in any test suite configuration, and most effectively for properties.
 nfiguration, an dmost effect
Figure 15: Venn-Diagram of Solved Entries per Suite
Figure 15 shows the overlap of solvedentries by test suite. It
shows that four entries were uniquely solvable by using only prop-
erties and one entry was uniquely solvable by the combined test
suite. All entries solved by unit testshave also been solved by the
properties.Thisdoesnotnecessarilyimplythatpropertiesare better
‚Äî the patches can still be overfit and are to be evaluated in RQ3.
Summary RQ1
Propertiesdonotsignificantlyhelpwithproducingpatches.
Inourstudy,propertiesfounduniquepatchesthatunittests
didnotproduce.Thedifferencebetweenresultsingeneticand
exhaustive search were greatest for the properties.RQ 2 ‚Äî Repair Speed. We grouped the results per seed and com-
pared the median time-to-first-result for each test suite. All two-
wayhypothesis-testsreportedasignificantp-valueoflessthan0.01,
proving that there are significant differences in distributions.
In particular, we performed a test2whether properties are faster
than unit tests in finding patches, which was the case with a p-
valueof0.02.TheVarghaandDelaneyeffectsizetestshowedan
estimate of 0.28 which is considered a medium-effect size, showing
that properties are faster than unit tests.
An overview of the time-to-first-result can be seen in Figure 16.
We would like to stress that similar to some results of RQ3, the test
suites‚Äôspeedseemstobehaveinsuchawaythattheslowestand
hardest test determines the magnitude of search. Properties do not
haveasignificant overhead bydesign,whichispositivelysurprising.
Thecostoftheirexecutioniscompensatedbythespeedupinsearch.
Figure 16: Distribution of Time to First Patch per Entry
Summary RQ2
GeneticSearchfindspatchesfasterforpropertiesthanforunit
tests. The combined test suite also yields combined search
speed.
RQ 3 ‚Äî Manual Inspection. From the sample of 70 patches the
authorsagreedon49tobeoverfitand21tobefit.Giventheoverall
population of 230 and an error rate of 10%, we expect 62 to 76 of
total patches to be correct. This results in a total non-overfit rate
of27%to33%.Inparticular,patchesinthesamplefoundforunit
testswereoverfitin85%ofcases(19/23),butthepropertieswere
overfit in 64% of cases (21/33). The combined test suite overfit in
63% (9/14) cases.
Thesearenotevenlydistributed‚Äîsomeprogramsareonlyre-
pairedoverfitwhileothersarealwayswellfixed.Hence,wededuct
thatofthe13Entriesthathavefixes,3to4havenon-overfitrepairs.
Thisestimatesaneffectiverepair-rateof10%orrespectively13%,
which performs similar to the rates reported by Astor [ 38] (13%)
2Wilcoxon-RankSum with less
1776ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Matth√≠as P√°ll Gissurarson, Leonhard Applis, et al.
and better than GenProg [ 38](1-4%). Arja [ 72] reports an effective
repair rate of 8% which we slightly outperform.
Atypicalexamplefoundbymanualinspectionwasaddingspace-
stripping to the addition-case of showExpr, as seen in Figure 17.
diff --git a//input/expr_units.hs b//input/expr_units.hs
--- a//input/expr_units.hs
+++ b//input/expr_units.hs
@@ -59,6 +59,6 @@ showExpr (Num n) = show n
showExpr (Num n) = show n
-showExpr (Add a b) = showExpr a + +"+"+ + showExpr b
+showExpr (Add a b) =
+ showExpr a ++ ((filter (not . isSpace)) (" + ")) ++ showExpr b
showExpr (Mul a b) = showFactor a + +"*"+ + showFactor b
showExpr (Sin a) = "sin" ++ showFactor a
showExpr (Cos a) = "cos" ++ showFactor a
showExpr (Var c) = [c]
Figure 17: A PropR patch showing overfitting on a unit test
prop_unit_showBigExpr ::Bool
prop_unit_showBigExpr =strip (showExpr expr) ==strip res
where
res ="sin (2. 1*x+3 . 2 )+3 . 5*x+ 5.7"
strip =filter (not .isSpace)
arg =Expr.sin (add (mul (num 2.1) x) (num 3.2))
expr =add (add (add (mul (num 3.5) x)) (num 5.7)) arg
Figure 18: The unit test corresponding to the fix in Figure 17
Thereisasingleunittest(seeFigure18)toassertaprintedaddition
without spaces. Within the patch only the "+" case gets repaired‚Äî
thisisduetotheprecedenceoftheexpressionwhichiscorrectly
pickedup.Hitherto,thechangeintheadditionactuallyremovesall
white-space and correctly passes the test. This (actually) solves the
unittestasexpectedandisthereforearguablynot trulyoverfitting.
Nevertheless,adeveloperwouldperformthestring-strippingonall
cases,notonlyontheaddition.Hereweseeashortcomingofthe
test suite ‚Äî this would have not been possible if we had a property
prop_showExpr_printNoSpaces or if we simply had unit tests for all
cases. In other data points, where the showExpr had a unified top-
level expression (not an immediate pattern match), the repair was
successful by adding top-level string-stripping. We would also like
to stress the quality of the patch generated despite overfitting: It
draws4elements( filter,toLower,isSpace,(.))whichwerenotin
the code beforehand and applied them at the correct position.
Anotherissueobservedwereemptypatches‚Äîtheseappeared
whentheQuickCheckpropertiesexhibitedinconsistentbehavior.
Wesuspectapropertythattestsfortheidempotencyof simplify
seeninFigure13,whichrequiresarandomlygeneratedexpression.
The property is meant to assert that e.g., x*4*0gets reduced to
0and not to x*0. Whether this case (or similar ones) are tested
depends on the randomly created expressions ‚Äî which makes it
aninconsistenttest.Theseareissues withthetestsuitethatwere
uncovered due to the hyper-frequent evaluation. The only way to
mitigate this is to provide a handful of unit tests or write a specific
expression-generatorusedfortheflakyproperty.Welabeledempty
patches to be overfit as we do not consider them proper repairs.Summary RQ3
Adding propertiesreduced theoverfit ratiofrom 85%to 63%,
doubling the number of goodpatches. The resulting effective
repairrateof10%to13%iscomparabletoothertools.Overfit-
ting appeared despite the use of properties, but generally less
due to an overall stronger test suite.
6 DISCUSSION
Overfitting on Properties. Similar to the overfitting of empty
patches shown in RQ3, we had cases of patches where one or
more failing properties exhibited inconsistent behavior, and an
overfit patch was considered a successful patch. We observed an
example that changed the simplification of multiplication to re-
turn 0 whenever a variable was in the term. This satisfies the
prop_MultWith0_Always0 propertyand shouldfailotherproperties
suchasmultiplicativeassociativity,but(inrarecases)Quick-Check
produced examples for the other properties that also evaluate to 0.
This overfitting shows that a test suite is not betterjust because
it is utilizing properties. APR-fitness is still only as good as the test
suite ‚Äî properties help define better test suites and well-written
properties positively influence APR.
ExploitableOverfitting. Anoticeablesideeffectofthetoolisthat
if the repair overfits, it produces numerous (bad) patches, as can be
seen from the number of generated proposals.
However, the repairs‚Äô output is not useless despite the overfit-
ting:thesuggestedpatchesclearlyshowtheshortcomingsofthe
test suite. The proposed overfit patches help developers with fault
localizationandimprovingthetestsuite.Inparticular,asproperties
and unit tests are not exclusive, developers can consider a test-
and-repair-drivenapproach,wheretheyadjustthetestsuiteand
program iteratively assisted by the repair tool. We consider this
approach attractive for class-room settings, where the programs
areoflowercomplexityandallowforfastfeedback.Whilewedon‚Äôt
expect PropR to be enough to solve the tasks forthe students, it
clearlyshowswheretheproblemsinthetestsorcodeare.Exploring
class-room usage is an interesting direction for future work.
Drastically Increased Search-Space. Due to the novel approach
to finding repair candidates, the search space drastically increased
as compared to using existing expressions or statements only. This
can be seen with the absence of random-search findings. Other
studiesshowedatleastsomeresultswithrandomsearch,sometimes
reportingrandomsearchasmostsuccessful[ 53].Aswefind(many)
patcheswithexhaustivesearch,theproblemsaregenerallysolvable
withsmallchanges.Thisimpliesthattheonlyreasonforrandom
search to yield no results is the increased search space.
This finding motivates further investigating the genetic search
anditsoptimizationformorecomplexproblemsthatdonotachieve
timelyresultswithexhaustivesearch.Weconsideritworthwhile
to revisit existing datasets, that were not solvable due to the redun-
dancyassumptioninmostrepairtools,usingatypedholeapproach.
TransferencetoJava. AsJavaisthemostprominentlanguagefor
APR, it begs the question of which results can be transferred from
Haskellintomoremainstreamapproaches.Propertiesaresupported
1777PropR: Property-Based Automatic Program Repair ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
byJUnit-Plugins3andcaneasilybeaddedtoanycommontestsuite
and build-tool. The positive effects of properties as presented in
Section 5 only require Java programs with sufficient properties.
However, the current Java-ecosystems are not utilizing properties;
even less sophisticated JUnit-Features, such as parametrized tests,
are not widely adopted. This is in starkcontrast to functional pro-
gramming communities, where tools like QuickCheck are popular.
The hole-fitting repair approach cannot be easily reproduced
for Java; The JavaC, unlike GHC, is not intended to be used as a
library.Nevertheless,Javaisstrictlytypedandthebasichole-fitting-
approach can be integrated using meta-programming libraries like
Spoon [47]. Many challenges remain: As Java‚Äôs methods are not
pure functions, they cannot be just transplanted . Side effects can
wreak havoc and just on a technical level polymorphism, that is
oftenonlyresolvabledynamically,bareshugefollow-up-challenges.
ButnotallislostfortheJVM:Repairapproachesthatfocuson
the bytecode [ 12,16], can easier adapt hole-fitting. In particular,
one could imagine a tool that produces holes for bytecode and
introducesthehole-fitsutilizingmorestrictJVMCompilerssuch
as Closure or Scala. We consider this extension a hard but valuable
track for further research.
Future Work. The primary research challenge we see is to com-
bineexistingapproacheswiththenewlyintroducedPropRhole-
fitting. A hybrid approach that could produce high churn with
techniquesfromAstor[ 40]orARJA[ 72]incombinationwiththe
fine-grained changes produced by PropR could solve a broader
range of issues. Specific to Haskell is the need to introduce left-
hand side definitions, i.e. new pattern matches or functions. These
could be provided by generative neural networks [ 2,7] and either
beusedasmutationsorasaninitialpopulationofchromosomes.
Representingmultipletypesofchangesisonlyamatterofrepre-
sentationwithin thechromosome‚Äî theremainingsearch, fitness
and fault localization can be kept as is.
For fault localization, we currently use allthe expressions in-
volved in the counter-examples. However, it should be possible
tousethecoverageinformationandthepassingandfailingtests
forspectrum-basedfaultlocalizationtonarrowthefault-involved
expressionsfurthertosuspiciousexpressions,ratherthanallthe
expressions involved in the failing test.
Intermsoffurtherevaluation,thenextstepsareusersurveysand
experimentsonrealworldapplicationssuchasPandoc4orAlex5.
In particular, we envision a bot similar to Sorald [ 14] that provides
patch-suggestions on failing pull-requests. We would like to ask
maintainers and the public community to give feedback on the
quality of repairs, and whether the suggested patches contributed
tofaultlocalizationorimprovementsofthetestsuiteevenifnot
added to the code.
7 THREATS TO VALIDITY
Internal Threats. We addressed the randomness in our exper-
iments by running 5 runs with different seeds according to the
suggestionsofArcuriandFraser[ 5].Thetoolusedinourexperi-
ment could contain bugs. We‚Äôve published it under a FOSS-license
3https://github.com/pholser/junit-quickcheck
4https://pandoc.org/
5https://www.haskell.org/alex/to gain further insights and suggestions from the community. The
experimentanddatasetmaycontainmistakes,whichweaddressby
providing a reproduction package and open source the experiment
and data. The package also contains notes on the data-preparation
for the experiment.
External Threats. The dataset is based on student data, which
could be considered artificial. We stress that student data has been
usedinliteratureforprogramrepairpreviously[ 11,13,31,33].A
real-world study on program such as Pandoc [ 10] is part of future
work. Pandoc, a popular Haskell document-converter, is rich in
properties that test e.g., for symmetry over conversions.
8 CONCLUSION
The goal of this paper is to introduce a new automatic program
repairapproachbasedon typesandcompilersuggestions,inaddi-
tiontoutilizingpropertiesfor repairfitnessandfaultlocalization.
To that end, we implemented PropR, a Haskell tool that utilizes
GHCforpatch-generationandcanevaluatepropertiesaswellas
unittests.Weprovidedadatasetwith30programsandtheirunit
tests and properties. On this dataset we performed an empirical
studytocomparetherepairratesfordifferenttestsuitesandsearch-
algorithms, and manually inspect the generated patches.
Our analysis of 230 patches show that we reach an effective
repair rate of 10%-13% (comparable to other state-of-the-art tools)
buthaveareducedrateofoverfitting(from85%to63%whenapply-
ing properties). The novel approach for patch generation produces
a greatly increased search space and promising patches on man-
ual inspection. We observed that properties did not increase the
numberofprogramsforwhichpatcheswerefound,butsolutions
werelessoverfitandfoundfaster.Overfittingbasedonunittests
persisted into the combined test suite. Similarly, we have observed
that properties can produce cases of overfitting too.
Ourresultsattesttothestrongerutilizationoflanguage-features
for patch generation to overcome the redundancy assumption, i.e.,
only reusing existing code. Using the compiler‚Äôs information on
types and scopes, the created patches are semantically correct and
comeinamuchgreatervariety,whichwasreportedasamissing
feature for many APR tools. Our manual analysis motivates to use
the generated patches (if not directly applicable) as guidance for
fault localization or to improve the test suite.
9 ONLINE RESOURCES
PropR isavailableonGitHubunderMIT-licenseathttps://github.
com/Tritlo/PropR.Thereproductionpackagewhichincludesthe
data, evaluation and a binary of PropR is available on Zenodo
https://doi.org/10.5281/zenodo.5389051
ACKNOWLEDGMENTS
We thank Matthew Sottile for his feedback on the implementation
of PropR,aswellasMartinMonperrusforadviceontheevaluation.
We also thank the reviewers for their insightful feedback.
This work was partially supported by the Wallenberg AI, Au-
tonomousSystemsandSoftwareProgram(WASP)fundedbythe
KnuthandAliceWallenbergFoundation.ThemembersofTUDelft
werepartiallyfundedbyICAIAIforFintechResearch,anING-TU
Delft collaboration.
1778ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Matth√≠as P√°ll Gissurarson, Leonhard Applis, et al.
REFERENCES
[1]Rui Abreu, Peter Zoeteweij, Rob Golsteijn, and Arjan J.C. van Gemund. 2009. A
practical evaluation of spectrum-based fault localization. Journal of Systems and
Software 82,11(2009),1780‚Äì1792. https://doi.org/10.1016/j.jss.2009.06.035 SI:
TAIC PART 2007 and MUTATION 2007.
[2]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Unified Pre-training for Program Understanding and Generation.
arXiv:2103.06333 [cs.CL]
[3]ChangWookAhnandR.S.Ramakrishna.2003. Elitism-basedcompactgenetic
algorithms. IEEE Transactions on Evolutionary Computation 7, 4 (2003), 367‚Äì385.
https://doi.org/10.1109/TEVC.2003.814633
[4]Mahmoud Alfadel, Diego Elias Costa, Emad Shihab, and Mouafak Mkhallalati.
2021. On the Useof Dependabot Security Pull Requests.In 2021 IEEE/ACM18th
International Conference on Mining Software Repositories (MSR) . IEEE, 254‚Äì265.
[5]AndreaArcuriandGordonFraser.2011. Onparametertuninginsearchbased
software engineering. In International Symposium on Search Based Software Engi-
neering. Springer, 33‚Äì47.
[6]Karthikeyan Bhargavan,Antoine Delignat-Lavaud, C√©dricFournet, AnithaGol-
lamudi, Georges Gonthier, Nadim Kobeissi, Natalia Kulatova, Aseem Rastogi,
Thomas Sibut-Pinote, Nikhil Swamy, and Santiago Zanella-B√©guelin. 2016. For-
mal Verification of Smart Contracts: Short Paper (PLAS ‚Äô16) . Association for
Computing Machinery, New York, NY, USA, 91‚Äì96. https://doi.org/10.1145/
2993600.2993611
[7]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
OliveiraPinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov,AletheaPower,LukaszKaiser,MohammadBavarian,ClemensWinter,
Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,Shan-
tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam,VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,Miles
Brundage,MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained on Code. arXiv:2107.03374 [cs.LG]
[8]Koen Claessen and John Hughes. 2000. QuickCheck: A Lightweight Tool for
Random Testingof HaskellPrograms. In Proceedings ofthe FifthACM SIGPLAN
InternationalConferenceonFunctionalProgramming (ICFP‚Äô00) .Associationfor
ComputingMachinery,NewYork,NY,USA,268‚Äì279. https://doi.org/10.1145/
351240.351266
[9]Zhen Yu Ding. 2020. Patch Quality and Diversity of Invariant-Guided Search-
Based Program Repair. arXiv preprint arXiv:2003.11667 (2020).
[10]Massimiliano Dominici. 2014. An overview of Pandoc. TUGboat 35, 1 (2014),
44‚Äì50.
[11]Thomas Durieux, Fernanda Madeiral, Matias Martinez, and Rui Abreu. 2019.
EmpiricalReviewofJavaProgramRepairTools:ALarge-ScaleExperimenton
2,141 Bugs and 23,551 Repair Attempts. In Proceedings of the 27th ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE ‚Äô19) . https://arxiv.org/abs/1905.11973
[12]Thomas Durieux and Martin Monperrus. 2016. DynaMoth: Dynamic Code Syn-
thesis for Automatic Program Repair. In Proceedings of the 11th International
Workshop on Automation of Software Test (Austin, Texas) (AST ‚Äô16). Association
for Computing Machinery, New York, NY, USA, 85‚Äì91. https://doi.org/10.1145/
2896921.2896931
[13]Thomas Durieux and Martin Monperrus. 2016. IntroClassJava: A Benchmark
of 297 Small and Buggy Java Programs . Technical Report. Universite Lille 1.
https://hal.archives-ouvertes.fr/hal-01272126/document
[14]KhashayarEtemadi,NicolasHarrand,SimonLarsen,HarisAdzemovic,HenryLu-
ongPhu,AshutoshVerma,FernandaMadeiral,DouglasWikstrom,andMartin
Monperrus. 2021. Sorald: Automatic Patch Suggestions for SonarQube Static
Analysis Violations. arXiv preprint arXiv:2103.12033 (2021).
[15]Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic Test Suite Gen-
erationforObject-OrientedSoftware.In Proceedingsofthe19thACMSIGSOFT
Symposiumandthe13thEuropeanConferenceonFoundationsofSoftwareEngi-
neering(Szeged,Hungary) (ESEC/FSE‚Äô11) .AssociationforComputingMachinery,
New York, NY, USA, 416‚Äì419. https://doi.org/10.1145/2025113.2025179
[16]AliGhanbariandLingmingZhang.2019. PraPR:PracticalProgramRepairvia
BytecodeMutation.In 201934thIEEE/ACMInternationalConferenceonAutomated
Software Engineering (ASE) . 1118‚Äì1121. https://doi.org/10.1109/ASE.2019.00116
[17]GHC Contributors. 2021. GHC 8.10.4 users guide. https://downloads.haskell.
org/~ghc/8.10.4/docs/html/users_guide/index.html
[18]Andy Gill and Colin Runciman. 2007. Haskell Program Coverage. In Proceedings
oftheACMSIGPLANWorkshoponHaskellWorkshop (Freiburg,Germany) (Haskell
‚Äô07).AssociationforComputingMachinery,NewYork,NY,USA,1‚Äì12. https:
//doi.org/10.1145/1291201.1291203[19]Matth√≠as P√°ll Gissurarson. 2018. Suggesting Valid Hole Fits for Typed-Holes
(Experience Report). In Proceedings of the 11th ACM SIGPLAN International Sym-
posiumonHaskell (St.Louis,MO,USA) (Haskell2018) .AssociationforComput-
ingMachinery,NewYork,NY,USA,179‚Äì185. https://doi.org/10.1145/3242744.
3242760
[20]Divya Gopinath, Muhammad Zubair Malik, and Sarfraz Khurshid. 2011.
Specification-BasedProgramRepairUsingSAT.In ToolsandAlgorithmsforthe
ConstructionandAnalysisofSystems ,ParoshAzizAbdullaandK.RustanM.Leino
(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 173‚Äì188.
[21]ZhengGuo,MichaelJames,DavidJusto,JiaxiaoZhou,ZitengWang,RanjitJhala,
andNadiaPolikarpova.2019. ProgramSynthesisbyType-GuidedAbstraction
Refinement. Proc.ACMProgram.Lang. 4,POPL,Article12(dec2019),28pages.
https://doi.org/10.1145/3371080
[22]RichardHamlet.1994. Randomtesting. EncyclopediaofsoftwareEngineering 2
(1994), 971‚Äì978.
[23]JohnHenryHollandetal .1992.Adaptationinnaturalandartificialsystems:an
introductoryanalysiswithapplicationstobiology,control,andartificialintelligence .
MIT press.
[24]Michael B. James, Zheng Guo, Ziteng Wang, Shivani Doshi, Hila Peleg, Ranjit
Jhala, and Nadia Polikarpova. 2020. Digging for Fold: Synthesis-Aided API
DiscoveryforHaskell. Proc.ACMProgram.Lang. 4,OOPSLA,Article205(nov
2020), 27 pages. https://doi.org/10.1145/3428273
[25]Susumu Katayama. 2011. MagicHaskeller: System demonstration. In Proceed-
ingsofAAIP20114thInternationalWorkshoponApproachesandApplicationsof
Inductive Programming . 63.
[26]Christoph Kern and Mark R. Greenstreet. 1999. Formal Verification in Hardware
Design:ASurvey. ACMTrans.Des.Autom.Electron.Syst. 4,2(apr1999),123‚Äì193.
https://doi.org/10.1145/307988.307989
[27]Edward Kmett. 2021. The lens library. https://hackage.haskell.org/package/lens
[28]Xianglong Kong, Lingming Zhang, W Eric Wong, and Bixin Li. 2015. Experi-
ence report: How do techniques, programs, and tests impact automated program
repair?.In 2015IEEE26thInternationalSymposiumonSoftwareReliabilityEngi-
neering (ISSRE) . IEEE, 194‚Äì204.
[29]RainerKoschke.2007. SurveyofResearchonSoftwareClones.In Duplication,
Redundancy, and Similarity in Software (Dagstuhl Seminar Proceedings, 06301) ,
Rainer Koschke, Ettore Merlo, and Andrew Walenstein (Eds.). Internationales
Begegnungs-undForschungszentrumf√ºrInformatik(IBFI),SchlossDagstuhl,
Germany, Dagstuhl, Germany. http://drops.dagstuhl.de/opus/volltexte/2007/962
[30]Christoph Kreitz. 1998. Program synthesis. In Automated Deduction‚ÄîA Basis for
Applications . Springer, 105‚Äì134.
[31]Claire Le Goues, Yuriy Brun, Stephanie Forrest, and Westley Weimer. 2017. Clar-
ifications on the Construction and Use of the ManyBugs Benchmark. IEEE
Transactions on Software Engineering 43, 11 (2017), 1089‚Äì1090.
[32]ClaireLeGoues,StephanieForrest,andWestleyWeimer.2013.Currentchallenges
in automatic software repair. Software Quality Journal 21, 3 (2013), 421‚Äì443.
https://doi.org/10.1007/s11219-013-9208-0
[33]Claire Le Goues, Neal Holtschulte, Edward K Smith, Yuriy Brun, Premkumar
Devanbu, Stephanie Forrest, and Westley Weimer. 2015. The ManyBugs and
IntroClassbenchmarksforautomatedrepairofCprograms. IEEETransactions
on Software Engineering 41, 12 (2015), 1236‚Äì1256.
[34]ClaireLeGoues,ThanhVuNguyen,StephanieForrest,andWestleyWeimer.2012.
GenProg:AGenericMethodforAutomaticSoftwareRepair. IEEETransactions
onSoftwareEngineering 38,1(2012),54‚Äì72. https://doi.org/10.1109/TSE.2011.104
[35]JunhoLee,DowonSong,SunbeomSo,andHakjooOh.2018. AutomaticDiagnosis
and Correction of Logical Errors for Functional Programming Assignments.
Proc. ACM Program. Lang. 2, OOPSLA, Article 158 (oct 2018), 30 pages. https:
//doi.org/10.1145/3276528
[36]ThibaudLutellier,HungVietPham,LawrencePang,YitongLi,MoshiWei,and
Lin Tan. 2020. Coconut: combining context-aware neural translation models
using ensemble for program repair. In Proceedings of the 29th ACM SIGSOFT
international symposium on software testing and analysis . 101‚Äì114.
[37]DavidMandelin,LinXu,RastislavBod√≠k,andDougKimelman.2005. Jungloid
Mining: Helping to Navigate the API Jungle. In Proceedings of the 2005 ACM SIG-
PLANConferenceonProgrammingLanguageDesignandImplementation (Chicago,
IL, USA)(PLDI ‚Äô05) . Association for Computing Machinery, New York, NY, USA,
48‚Äì61. https://doi.org/10.1145/1065010.1065018
[38]Matias Martinez,Thomas Durieux, Romain Sommerard, Jifeng Xuan, andMartin
Monperrus. 2017. Automatic repair of real bugs in java: a large-scale experiment
on the defects4j dataset. Empirical Software Engineering 22, 4 (2017), 1936‚Äì1964.
https://doi.org/10.1007/s10664-016-9470-4 arXiv:1811.02429
[39]MatiasMartinezandMartinMonperrus.2016. ASTOR:AProgramRepairLibrary
for Java. In Proceedings of ISSTA . https://doi.org/10.1145/2931037.2948705
[40]Matias Martinez and Martin Monperrus. 2019. Astor: Exploring the design
space of generate-and-validate program repair beyond GenProg. Journal of
Systems and Software 151 (2019), 65‚Äì80. https://doi.org/10.1016/j.jss.2019.01.069
arXiv:1802.03365
[41]MatiasMartinez,WestleyWeimer,andMartinMonperrus.2014. DotheFixIngre-
dientsAlreadyExist?AnEmpiricalInquiryintotheRedundancyAssumptions
1779PropR: Property-Based Automatic Program Repair ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
of Program Repair Approaches. In Companion Proceedings of the 36th Interna-
tional Conference onSoftwareEngineering (Hyderabad, India) (ICSE Companion
2014). Association for Computing Machinery, New York, NY, USA, 492‚Äì495.
https://doi.org/10.1145/2591062.2591114
[42]Ehsan Mashhadi and Hadi Hemmati. 2021. Applying CodeBERT for Automated
Program Repair of Java Simple Bugs. arXiv preprint arXiv:2103.11626 (2021).
[43]CatherineAMeadows.1994. Formalverificationofcryptographicprotocols:A
survey.In InternationalConferenceontheTheoryandApplicationofCryptology .
Springer, 133‚Äì150.
[44]Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
MultilineProgramPatchSynthesisviaSymbolicAnalysis.In Proceedingsofthe
38thInternationalConferenceonSoftwareEngineering (Austin,Texas) (ICSE‚Äô16) .
Association for Computing Machinery, New York, NY, USA, 691‚Äì701. https:
//doi.org/10.1145/2884781.2884807
[45]Geoffrey Neumann, Mark Harman, and Simon Poulding. 2015. Transformed
Vargha-Delaney Effect Size. In Search-Based Software Engineering , M√°rcio Barros
and Yvan Labiche (Eds.). Springer International Publishing, Cham, 318‚Äì324.
[46]AmirfarhadNilizadeh,GaryT.Leavens,Xuan-BachD.Le,CorinaS.PƒÉsƒÉreanu,
and David R. Cok. 2021. Exploring True Test Overfitting in Dynamic Automated
Program Repair using Formal Methods. In 2021 14th IEEE Conference on Software
Testing, Verification and Validation (ICST) . 229‚Äì240. https://doi.org/10.1109/
ICST49551.2021.00033
[47]RenaudPawlak,MartinMonperrus,NicolasPetitprez,CarlosNoguera,andLionel
Seinturier. 2015. Spoon: A Library for Implementing Analyses and Transforma-
tionsofJavaSourceCode. Software:PracticeandExperience 46(2015),1155‚Äì1179.
https://doi.org/10.1002/spe.2346
[48]Ricardo Pe√±a. 2017. An introduction to liquid haskell. arXiv preprint
arXiv:1701.03320 (2017).
[49]ThorstenPohlert.2014. Thepairwisemultiplecomparisonofmeanrankspackage
(PMCMR). R package 27, 2019 (2014), 9.
[50]Nadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Program
Synthesisfrom PolymorphicRefinement Types.In Proceedingsof the37thACM
SIGPLANConferenceonProgrammingLanguageDesignandImplementation (Santa
Barbara, CA, USA) (PLDI ‚Äô16) . Association for Computing Machinery, New York,
NY, USA, 522‚Äì538. https://doi.org/10.1145/2908080.2908093
[51]Nadia Polikarpova, Deian Stefan, Jean Yang, Shachar Itzhaky, Travis Hance, and
Armando Solar-Lezama. 2020. Liquid Information Flow Control. Proc. ACM
Program. Lang. 4, ICFP, Article 105 (aug 2020), 30 pages. https://doi.org/10.1145/
3408987
[52]Yuhua Qi, Xiaoguang Mao, and Yan Lei. 2013. Efficient Automated Program
Repair through Fault-Recorded Testing Prioritization. In 2013 IEEE International
ConferenceonSoftwareMaintenance .180‚Äì189. https://doi.org/10.1109/ICSM.2013.
29
[53]YuhuaQi,XiaoguangMao,YanLei,ZiyingDai,andChengsongWang.2014. The
strengthofrandomsearchonautomatedprogramrepair.In Proceedingsofthe
36th International Conference on Software Engineering . 254‚Äì265.
[54]Zichao Qi, Fan Long, Sara Achour, and Martin Rinard. 2015. An Analysis of
PatchPlausibilityandCorrectnessforGenerate-and-ValidatePatchGeneration
Systems. In Proceedings of the 2015 International Symposium on Software Testing
and Analysis (Baltimore, MD, USA) (ISSTA 2015) . Association for Computing
Machinery,NewYork,NY,USA,24‚Äì36. https://doi.org/10.1145/2771783.2771791
[55]Michel Raymond and Francois Rousset. 1995. An Exact Test for Population
Differentiation. Evolution 49, 6 (1995), 1280‚Äì1283. http://www.jstor.org/stable/
2410454
[56]Patrick Redmond, Gan Shen, and Lindsey Kuper. 2021. Toward Hole-Driven
Development with Liquid Haskell. arXiv preprint arXiv:2110.04461 (2021).
[57]Patrick M Rondon, Ming Kawaguci, and Ranjit Jhala. 2008. Liquid types. In
Proceedings of the 29th ACM SIGPLAN Conference on Programming Language
Design and Implementation . 159‚Äì169.
[58]ColinRunciman,MatthewNaylor,andFredrikLindblad.2008. Smallcheckand
lazy smallcheck: automatic exhaustive testing for small values. Acm sigplan
notices44, 2 (2008), 37‚Äì48.
[59]RiponKSaha,YingjunLyu,HiroakiYoshida,andMukulRPrasad.2017. Elixir:
Effectiveobject-orientedprogramrepair.In 201732ndIEEE/ACMInternational
Conference on Automated Software Engineering (ASE) . IEEE, 648‚Äì659.
[60]SaurabhSrivastava,SumitGulwani,andJeffreyS.Foster.2010. FromProgramVer-
ification to Program Synthesis. In Proceedings of the 37th Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages (Madrid, Spain)
(POPL ‚Äô10) . Association for Computing Machinery, New York, NY, USA, 313‚Äì326.
https://doi.org/10.1145/1706299.1706337
[61]Chadi Trad, Rawad Abou Assi, Wes Masri, and Fadi Zaraket. 2018. CFAAR:
ControlFlowAlterationtoAssistRepair.In 2018IEEEInternationalSymposium
on Software Reliability Engineering Workshops (ISSREW) . IEEE, 208‚Äì215.
[62]Simon Urli, Zhongxing Yu, Lionel Seinturier, and Martin Monperrus. 2018. How
to design a program repair bot? insights from the repairnator project. In 2018
IEEE/ACM 40th International Conference on Software Engineering: Software Engi-
neering in Practice Track (ICSE-SEIP) . IEEE, 95‚Äì104.[63]Andr√°s Vargha and Harold D Delaney. 2000. A critique and improvement of
the CL common language effect size statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101‚Äì132.
[64]Niki Vazou, Leonidas Lampropoulos, and Jeff Polakow. 2017. A Tale of Two
Provers:VerifyingMonoidalStringMatchinginLiquidHaskellandCoq. SIGPLAN
Not.52, 10 (sep 2017), 63‚Äì74. https://doi.org/10.1145/3156695.3122963
[65]Ke Wang, Rishabh Singh, and Zhendong Su. 2017. Dynamic neural program
embedding for program repair. arXiv preprint arXiv:1711.07163 (2017).
[66]Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2017.
An empirical analysis of the influence of fault space on search-based automated
program repair. arXiv preprint arXiv:1707.05172 (2017).
[67]Qi Xin. 2017. Towards Addressing the Patch Overfitting Problem. In 2017
IEEE/ACM 39th International Conference on Software Engineering Companion
(ICSE-C). 489‚Äì490. https://doi.org/10.1109/ICSE-C.2017.42
[68]Qi Xin and Steven P. Reiss. 2017. Identifying test-suite-overfitted patches
through test case generation. ISSTA 2017 - Proceedings of the 26th ACM SIG-
SOFT International Symposium on Software Testing and Analysis (2017), 226‚Äì236.
https://doi.org/10.1145/3092703.3092718
[69]Qi Xin and Steven P Reiss. 2017. Leveraging syntax-related code for automated
programrepair.In 201732ndIEEE/ACMInternationalConferenceonAutomated
Software Engineering (ASE) . IEEE, 660‚Äì670.
[70]He Ye, Matias Martinez, Thomas Durieux, and Martin Monperrus. 2021. A
comprehensive study of automatic program repair on the QuixBugs benchmark.
Journal of Systems and Software 171 (2021), 110825.
[71]ZhongxingYu,MatiasMartinez,BenjaminDanglot,ThomasDurieux,andMartin
Monperrus. 2017. Test case generation for program repair: A study of feasibility
and effectiveness. arXiv preprint arXiv:1703.00198 (2017).
[72]YuanYuanandWolfgangBanzhaf.2017. ARJA:AutomatedrepairofJavapro-
grams via multi-objective genetic programming. arXiv46, 10 (2017), 1040‚Äì1067.
arXiv:1712.07804
[73]Qianqian Zhu, Annibale Panichella, and Andy Zaidman. 2018. An investigation
of compression techniques to speed up mutation testing. In 2018 IEEE 11th Inter-
national Conference on Software Testing, Verification and Validation (ICST) . IEEE,
274‚Äì284.
1780