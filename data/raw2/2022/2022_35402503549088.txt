23 ShadesofSelf-Admitted Technical Debt: An Empirical Study
onMachine LearningSoftware
David OBrien
Dept. ofComputerScience
Iowa StateUniversity
Ames, IA,USA
davidob@iastate.eduSumonBiswas∗
SchoolofComputerScience
CarnegieMellonUniversity
Pittsburgh,PA,USA
sumonb@cs.cmu.eduSayem Imtiaz
Dept. ofComputerScience
Iowa StateUniversity
Ames, IA,USA
sayem@iastate.edu
Rabe Abdalkareem
SchoolofComputerScience
CarletonUniversity
Ottawa,ON,Canada
rabe.abdalkareem@carleton.caEmad Shihab
Concordia University
Montreal, QC, Canada
emad.shihab@concordia.caHridesh Rajan
Dept. ofComputerScience
Iowa StateUniversity
Ames, IA,USA
hridesh@iastate.edu
ABSTRACT
In software development, the term łtechnical debtž (TD) is used to
characterizeshort-termsolutionsandworkaroundsimplemented
insourcecodewhichmayincuralong-termcost.Technicaldebt
has a variety of forms and can thus affect multiple qualities of soft-
ware including but not limited to its legibility, performance, and
structure. In this paper, we have conducted a comprehensive study
on the technical debts in machine learning (ML) based software.
TDcanappeardifferentlyinMLsoftwarebyinfectingthedatathat
MLmodelsaretrainedon,thusaffectingthefunctionalbehaviorof
ML systems. The growing inclusion of ML components in modern
software systems have introduced a new set of TDs. Does ML soft-
ware have similar TDs to traditional software? If not, what are the
new types of ML specific TDs? Which ML pipeline stages do these
debts appear? Do these debts differ in ML tools and applications
and whentheyget removed? Currently,we donot knowthe state
oftheMLTDsinthewild.Toaddressthesequestions,wemined
68,820self-admittedtechnicaldebts(SATD)fromalltherevisionsof
a curated dataset consisting of 2,641 popular ML repositories from
GitHub, along with their introduction and removal. By applying
anopen-codingschemeandfollowinguponpriorworks,wepro-
vide a comprehensive taxonomy of ML SATDs. Our study analyzes
ML SATD type organizations, their frequencies within stages of
ML software, the differences between ML SATDs in applications
andtools,andquantifiestheremovalofMLSATDs.Thefindings
discovered suggest implications for ML developers and researchers
tocreate maintainableMLsystems.
∗At the time this work was completed, Sumon Biswas was a graduate student at Iowa
StateUniversity
ESEC/FSE ’22, November 14ś18,2022, Singapore, Singapore
©2022 Copyright heldby theowner/author(s).
ACM ISBN 978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3549088CCSCONCEPTS
•Softwareanditsengineering →Softwarecreationandman-
agement ;•Computing methodologies →Machine learning .
KEYWORDS
technicaldebt,machine learning,open-source,datascience
ACM Reference Format:
David OBrien, Sumon Biswas, Sayem Imtiaz, Rabe Abdalkareem, Emad
Shihab,andHrideshRajan.2022.23ShadesofSelf-AdmittedTechnicalDebt:
An Empirical Study on Machine Learning Software. In Proceedings of the
30th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE ’22), November 14ś
18,2022,Singapore,Singapore. ACM,NewYork,NY,USA, 13pages.https:
//doi.org/10.1145/3540250.3549088
1 INTRODUCTION
Therecentuprisingandrapidintegrationofmachinelearning(ML)
modelshasempowereddeveloperstotackleproblemspreviously
infeasible such as safety-criticalsystems, financial fraud detection,
andmedicaldiagnostics[ 3,13].However,thesuddenemergence
and adoption of ML models risks the creation of hastily-planned
machinelearningsoftwaredeployedlong-term[ 45].Thus,thefunc-
tionalitiesoffuturemachinelearningsoftwaredependsuponthe
maintainability ofpresent-day machine learningsoftware.
Throughdecadesoftraditionalsoftwarepractices,thetermłtech-
nical debtž (TD) was coined by Ward Cunningham to characterize
short-term solutions, workarounds, or unfinished implementations
that exist in long-term software. TD can worsen software’s legi-
bility,performance,andquality,consequentiallycomplicatingits
maintenance lifetime. Similar to fiscal debt, TD becomes more łex-
pensivełthelongeraninstanceexists,costingaheavyamountof
developers’time,effort,andknowledgetopayoff.Therefore,the
awarenessofTDtypeshashistoricallyinformeddevelopershowto
bestmanagetheirsoftwaretominimizetheTDaccumulatedand
prioritizetheirTD management[ 2,19,33,53].
ML software contains additional components not found within
traditionalsoftwaredevelopment. Sculleyetal .exploredthehidden
TDinMLtodescribechallengesencounteredwhenGoogleengi-
neersweremaintainingMLsoftware[ 44,45].BecauseMLsoftware
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
734
ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore David OBrien, SumonBiswas, Sayem Imtiaz, Rabe Abdalkareem, Emad Shihab, andHrideshRajan
isaccompaniedbychallengesnativetoitsowndomain,itissuscep-
tibletonewformsofTDnotencounteredbytraditionalsoftware
practices. Forexample, because ML software has an implicit depen-
dence upon the quality of its training data, problems in the data
are reflected upon the resulting model behavior [ 1,29]. Previous
work has provided a framework to contain the contagion of TD in
ML software through multi-level evaluations [ 11]. Moreover, Tang
et al. [52]further studied TD in ML software through studying
refactoringsandtheircorrespondingremovalstrategiesfoundin
Java ML software.
Historically,thenotionofself-admittedtechnicaldebt(SATD)
describes a developer’s expressed beliefs on necessary improve-
mentstocurrentimplementationsofsoftware[ 4,27].SATDismost
commonly associatedwith source codecomments documentinga
potentialchange,althoughSATDhasalsobeenfoundinananalysis
ofsoftwareissue trackers[ 57]. SATDhasbeenusedasaproxyto
studyTD,sinceSATDcontainsnaturallanguagetocommunicate
proposed changes to source code. A large financial organization
in practice discovered that their developers used SATD to guide
themanagementoftheirTD[ 54],furtheremphasizingthevalueof
examining SATD.
Because previous studies have used SATD to indicate TD pat-
terns,itmayprovideinsightsonMLspecificTD symptoms.How-
ever, to the best of our knowledge no study has been conducted to
understandtheML-specificSATDtypesandtheircharacteristics
present inthe ML software.
Motivatedbythis,wecreatedandfilteredadatasetconsisting
of popular ML repositories written in Python which have been
usedinpreviousstudies[ 26,50].WechosePythonbecauseithas
been referred to as the de-facto language for ML development
duetoitspopularity[ 5,14],thenperformedalabelingprocessto
create astatistically significantdataset of 856labeledinstances of
SATDwithintheserepositoriestoanswerthefollowingresearch
questions:
RQ1: What types of SATD are found in our studied ML software?
SATD can indicate the presence of TD, and is more human un-
derstandablebynatureofnaturallanguagecomments.Arethere
newtypesofSATDsthatexistinMLsoftwarethatpreviousworks
haven’tdiscovered?
RQ2: What is the distribution of SATD types in the different ML
pipeline stages? The ML pipeline splits up the ML development
workflow into distinct objectives [ 3,10]. Do different stages have
unique frequencies of SATD types? Does a SATD appearance
changeindiffering stages?
RQ3: Is there a difference in SATD types between ML applications vs
ML tools? ML application developers and ML tool developers have
differentgoals.DoesthisinfluencetheSATDthattheyencounter?
RQ4:HowmucheffortisneededtoremoveSATDtypesinMLsoft-
ware?Since SATD indicates problems affecting ML software’s
maintainability,itisimportanttounderstandwhichtypesofSATD
comments have historicallybeen difficult to remove.
Our findings indicate that ML developers are most concerned
withimprovinghowtheirsoftwaremeetstheirproject’sfunctional
and non-functional requirements. Moreover, our observations find
a substantial focus on the configuration of data processing code
acrossmultiplepipelinestages.Additionally,ourstudyfindsthatTable 1:Filtering Overview.
Stage Name Remaining Comments
Extracted 9,725,127 changes
Length>1 7,415,024 changes
SATD Detector/ KeywordSearch 193,787changes
RemovedAutogenerated 189,912changes
RemovedNon-Unique Removals 85,599comments
Not in "site-packages"Folder 68,820comments
MLtoolscontainmoreSATDinvolvingdependenciesonoutside
code than ML applications.Finally,our observationssuggests that
varioustypesofMLSATDstakevaryingdegreesofefforttoremove.
To allow replication and future research, our labeled dataset of
identifiedSATD inML repositories isavailable to the public1.
The rest of the paper is organized as follows. Section 2 presents
our methodology to gather and filter a dataset of source code com-
ments,aswellasourstrategytoconstructMLSATDtaxonomies.
The resultsof our fourRQs are presented in Section 3. We discuss
the implications of our work in Section 4. The limitations of our
studyarehighlightedinSection5.Section6discussestherelated
works, andSection 7concludes the paper.
2 METHODOLOGY
In this study, we will manually analyze source code comments
found in ML systems. This section describes the dataset utilized,
ourfilteringandsamplingprocesses,ourclassificationscheme,and
our classification process.
2.1 Dataset
Our study involves open-source machine learning repositories pre-
sented in [ 26] which has been used in a prior study examining ML
bugs’associationswith programminglanguages[ 50].Thisdataset
contains popular ML repositories, including ML tools (repositories
supplying ML functionality) such as the scikit-learn2repository
and ML applications (repositories applying ML to a problem) such
asdeepfake’sfaceswaprepository3.Therepositoriesfoundinthe
dataset have been filtered so that every software repository within
has more than 5 stars OR≥5 forks, must have been active in 2019,
andmust bea non-trivialsoftware project.Although thereposito-
riesweregatheredin2019,wegeneratethedatausingthesource
code data as of January 2021. The resulting dataset contains repos-
itories spanning across 439 topic labels on GitHub, indicating a
representation of a variety of ML topics. Although the resulting
datasetcontains5,224repositoriesacrossavarietyofprogramming
languages, this study will only analyze the 2,684 which have been
writteninPythonduetoitspopularityintheMLcommunity[ 5,14].
From the 2,684 repositories present in the dataset, 43 were not
present on GitHub when our dataset was generated, resulting in
2,641 totalprojectsbeing analyzedinthis study.
2.2 Identifying SATDComments
This section describes the filtering process used to extract SATD
comments that have existed in the repositories described in the
previous section. Table 1providesan overviewof the process.
1https://github.com/DavidMOBrien/23Shades
2https://github.com/scikit-learn/scikit-learn
3https://github.com/deepfakes/faceswap
73523Shades of Self-AdmittedTechnical Debt: AnEmpirical StudyonMachineLearning Software ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
WeuseBoa[ 15,16],alanguagespecificallydesignedtominedata
from software repositories which has demonstrated capabilities to
analyze data science repositories [ 7]. Boa analyzes each revision
to determine when a source code comment has been introduced or
removed[ 17,18].9,725,127commentchangesareextractedfrom
ourdatasetinthismanner.Aftermanualinspectionofthedataset’s
contents,wefoundcommentswithatextlengthofonecharacter
likely do not represent SATD or contains enough information to
classify an SATD comment. We remove such comments, leaving
7,415,024comment changes remaining.
Tofilterour commentchangestoincludeonlySATDcomment
changes, we use both a previously trained classifier and a keyword
search. First, we use the SATD detector classifier created by [ 27]
whichwastraineduponcommentsfrom8activesoftwarereposito-
riestoobtainanaverageF1-scoreof0.737.However,sinceSATD
foundinMLsoftwaremaydifferfromSATDintraditionalsoftware,
wemeasuredtheperformanceofthisclassifieronasampleof1,255
MLsourcecodecomments,where35werelabeledtobeSATDby
theauthors.Previouswork’sclassifierobtainedaprecisionof82.6%
and a recall of 54.2% on this sample. However, we found that many
ofthefalsenegativesfromthisexperimentcontainedakeyword
discoveredtoindicateaSATD("todo","fixme","hacky",etc.)[ 36].
Therefore,wetaketheunionofcommentsclassifiedasSATDby
previous work, and comments which contain a keyword presented
in [36] to reach a recall of 82.6%. 193,787 comment changes remain
after filtering inthis manner.
Additionally,weremovecommentswhicharelikelyirrelevant
toSATDasdoneinpreviouswork[ 36].Therefore,wemanuallyob-
servecommentsinourdatasetwhichappearmostoftentoidentify
casessuchas commentsdescribingsoftwarelicensing,comments
which have been automatically generated (i.e., #generated protocol
buffer), and comments to assist linters (i.e., #NOQA) as cases which
likelydonotcontainSATD.189,912commentchangesremainafter
removingsuch comments.
We thenorganizeour commentchanges toseparate comments
whichhavebeenremovedornotintheirrepositorieslifetime.Prior
work shows that data from GitHub has been found difficult to
minecleanlydueto eventssuch as merges, rebases, androllbacks,
among others [ 6,22]. Additionally, a repository may have multiple
copies of the same SATD comment. Therefore, we follow a similar
procedureto [ 37]by consideringa commentremovedif thereis a
uniqueremoval of a comment after its introduction. We find 85,599
unique comments inthis manner.
Following manual evaluation, we identified the case where a
source code comment is not native to the analyzed repository.
Rather, the source code comment is found in the "site-packages"
directory.BecausethegoalofthisstudyistoanalyzeSATDcom-
ments in ML repositories, we choose to filter out comments which
appearinthisdirectory.Afterthisstep,wefind68,820comments
remaining inour dataset.
Wetook astatisticallysignificantsample ofour filtereddataset
with a confidence interval of 95% and a margin of error of 3.33% to
calculatethesizeofoursampledataset.Then,wesamplecomments
similartoapreviousstudyonSATDcomments[ 25]wheretheSATD
datasetissampledsothattheresultingsample’scharacteristicsis
proportionaltotheoriginaldataset.Table 2illustratestherespective
proportionsofthe dataset andits sample.Table 2:Totaland Sampled SATD Comments
DatasetBucket# of SATD
Comments# of Sampled
SATD Comments
Tool +Comment Removed 8,104 101
Tool +Comment Not Removed 19,724 245
App. +Comment Removed 10,215 127
App. +Comment Not Removed 30,777 383
Total 68,820 856
2.3 Building SATDClassification Scheme
In this section, we will describe the classification schemes used to
label our sampleddataset.
PreviousworkonTDinMLapplicationsarguedthatMLsoftware
can become indebted in similar ways to traditional software devel-
opment,aswellasinwaysuniquetoMLsystems[ 44].Therefore,
wereviewpriorworksontraditionalsoftwareSATDtypes[ 2,4,42]
to construct a taxonomy which this study refers to as "Software
SATD Types". This study also presents a taxonomy consisting of
SATDs which appear in ML-specialized software. To accomplish
this, we identify TD types found in ML software from previous
studies [45,52]. Using these prior works, we construct a taxonomy
of SATDs from the corresponding TDs which this study will refer
to as "MLSATD Types".
WeusetheMLpipelinedefinedinapreviouswork[ 10]which
analyzed3representationsoftheMLpipelineatvaryinglevelsto
find 7well definedpipeline stageswhichisusedto answer RQ2.
Weperformedapilotstudyusingpreviousworks’taxonomies[ 45,
52]onasampleof100commentsfromthedatasetfilteredinSection
2.1. Although every type in previous works’ ML TD taxonomy was
notreachedinthe100samples,wedidnoticebeneficialimprove-
mentsthatcouldbeproposed.Forexample,weobservedthatthe
Configuration debt [45] was too broad to effectively analyze and
reason aboutits symptoms and effects. Therefore, we split the Con-
figuration debtintofivespecifictypesofconfigurableoptions( Data
Configuration ,Data Storage Configuration ,Weight Configuration ,
Hyper-parameter Configuration ,Layer Configuration ). Additionally,
we also propose new types not found in previous studies ( Machine
LearningDependency ,MachineLearningKnowledge ,MachineLearn-
ing Reliability ,ModelInterpretability ,PredictionQuality ).
2.4 Classifying SATDRelatedComments
Once we created the classificationschemes, we want to apply our
classification scheme on the sampled SATD comments. We per-
formed a coding process [ 46]. To do so, an initial training meeting
with three of the authors to discuss the classification schemes and
thepilotstudydescribedpreviously.Followingthismeeting,two
oftheauthorswouldindependentlylabel30-40SATDcomments,
then discuss in the presence of the third author acting as a moder-
ator.Sinceourclassificationispronetohumanbias,wecalculate
the Cohen’s Kappa coefficient to measure the agreement between
two annotators. Cohen’s Kappa coefficient is a common statistical
method that is used to evaluate the inter-rater agreement level
for classification scales by discarding the possibility of random
agreement.Theresultingcoefficientisscaledtorangebetween-1.0
and1.0,whereanegativevaluemeanspoorerthanchanceagree-
ment,zeroindicatesexactlychanceagreement,andapositivevalue
indicates betterthanchance agreement.
736ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore David OBrien, SumonBiswas, Sayem Imtiaz, Rabe Abdalkareem, Emad Shihab, andHrideshRajan
ThetwolabelingauthorsachievedaCohen’sKappacoefficient
of 63% between their first independently labeled sets, 77% on their
second,and80%ontheirthird.Becausetheauthors’Cohen’sKappa
coefficienthadreachedverystrongagreement,theylabeledonce
moreindependentlytoachieveCohen’sKappacoefficientof83%,
which is considered to be excellent agreement [ 21]. After each
labelingsession,thetwolabelingauthorsandthethirdmoderating
author would meet to reconcile any disagreements held in the
classifications,whilealsodiscussingthecauseofthedisagreements.
Aftermeetingtoreconcilethefourthindependentlylabeledsets,
the authors then labeled the rest of the unlabeled data. In cases
where an author was not confident on the appropriate label to
assigntoanSATDcomment,otherauthorswereconsultedinorder
toreachadecisionontheappropriatelabelsforthosecases.Finally,
theentiredatasetwasreviewedsothateverycommentwaschecked
byat leasttwoauthors.
3 RESULTS
In this section, we investigate our labeled dataset to answer the
fourresearchquestions.
As a result of our manual classification process and literature
review, we ended up with 6 different Software SATD Types which
are describedandexemplifiedinTable 3.
AfterconductingourpilotstudyonSATDtypesinMLsoftware,
wefoundthatsomeSATDcommentswerenotwelldescribedby
pre-existing works or are too broad to describe unique characteris-
tics[45,52].Tobestfixthesescenarios,weproposeanewMLSATD
classificationschemeconsistingof23MLSATDTypesconsistingof
13 pre-existing TD in ML software types [ 45,52], and 5 new types
as well as 5 split-up types proposed in this study which were used
to label our dataset. All ML SATD Types presented upon have at
least one comment in our labeled dataset. To further analyze the
aspect by which a SATD in ML software is concerned with, we
further organize the 23 ML SATD Types found in Section 2.3 into 9
high-levelgroupsthatwillbereferredtoas"MLSATDGroups".We
believeMLresearchersandpractitionerscanusethishierarchical
classification scheme to guide and evaluate ML software mainte-
nance. Table 4 shows a detailed description of the ML SATD Types
organizedbyML SATD Groups.
3.1 RQ1: WhattypesofSATDarefoundin our
studiedML software?
In this section, we analyze the distributions within our dataset
to present commonly found SATD types in ML software, their
symptoms,andexample comments.
Ourresults are organized as such: Table 3presents thedistribu-
tionofallSoftwareSATDTypecommentsthatoccur.Itisimportant
to note, some comments were not well described by one of the 6
Software SATD Types, and was then left blank. Similarly, if an
SATDcommentinourlabeleddatasetwasfoundtobedescribed
by multiple ML SATD Types, then it is counted as a unique debt
for each ML SATD Type it received. For example, the comment
TODO: experiment with more layers received the labels Layer
Configuration andMachine Learning Knowledge , so it is counted in
thetotalsforbothMLSATDTypes.Theresultsfromperforming
this study are showninTable 5.Finding 1: Requirement debt accounts for 40.68% of the
Software SATD TypesinML software.
As seen in Table 3, more than a third of SATD comments in
our labeled dataset which have a Software SATD Type received
theRequirement debt label. Requirement debts are concerned with
shortcomingsinvolvingincompletefunctionalityornon-supported
features (functional requirements) as well as poorly performing
code (non-functional requirements), ML-specific examples of these
areprovidedinTable 3.BecauseMLisanactiveareabothinindus-
tryandacademia,therearerapiddiscoveriesandimprovementsthat
canbemadetoMLsoftware[ 26].Casessuchasalgorithmicconfig-
urations,APIupdates,anddeveloperimprovementcouldcontribute
to the resolution of non-functional Requirement debts. Similarly,
codereviews,issuetrackers,andfeaturerequestsaremethodolo-
gies which may identify functional Requirement debts. However,
the abundance of Requirement debts in our studied ML software
repositories may indicate that ML developers prefer to focus on
othermattersbesidesfunctionalandnon-functionallyindebtedsub-
systems,leadingtotheintroductionoftheseself-admittedtechnical
debts.
OurfindingsindicatethatMLdevelopersshouldbeawareofhow
fast-pacedMLdevelopmentcanbeandhowchangestorequirement
specificationscouldaffecttheevolutionandmaintenanceoftheir
software.Additionally,MLframeworkdevelopersshouldalsobe
awarethatthefunctionalitiestheyprovidemayleadtotheintro-
ductionof Requirement debtsintheapplicationswhichusethem
throughAPImisuse,unclearintents,and cascadingupgrades[ 45].
We recommend that ML framework developers should de-
sign APIs such that its users are supplied with a complete
pictureofitsrequirementsandlimitations. Forexample,the
SATDcomment TODO add param for relative path vs just
folder names revealstherequirementsandlimitationsfoundin
an evaluation utilities class. Knowledge of this limitation can save
developer timelaterindevelopment.Furthermore, MLdevelopers
should carefullyconsider howtheircurrentimplementations may
holdupagainst future requirement changes [ 11,44,45].
Finding 2: When compared to traditional software, ML
software containsless prioritized Codedebts.
To put our results in perspective, the results from prior work
on SATD types in traditional software development by Bavota
and Russo are shown in Table 6[4]. They analyzed 159 software
repositories, resulting in the mining of 2 billion comments and
the labeling of 273 SATD comments with the same classification
scheme we used to construct our taxonomy of the Software SATD
Types.
When we compare our results in ML software to Bavota and
Russo,weseethatourinvestigationsindicatethatMLdevelopers
might differ in their SATD management to traditional software
developers. Instead of Requirement debt,Codedebt is the SATD
typewhichappearsmostinBavotaandRusso’sstudyontraditional
softwareSATD[ 4].Codedebtinvolvesareasofpoorlylegiblesource
code.Casesof Codedebtsincludesectionsofcodewherecodelogic
can be simplified to allow for easier understanding, variables or
functionscanberenamed,afunctioncanberefactored,orcodecan
be reformatted to adhere to expected coding standards. Example
SATD comments of Codedebts found in our dataset include: #
hackier; fixme to use regex or something andIt’s only
73723Shades of Self-AdmittedTechnical Debt: AnEmpirical StudyonMachineLearning Software ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
Table 3:Definitions andExamples ofthe6 Identified Software SATD Types.
SATD Types Description Example comment # of Occ. %of Occ.
Requirement Requirementdebtscanbefunctionalornon-functional.Inthefunctionalcase,
implementations are left unfinished or in need of future feature support. In the
non-functional case, the corresponding code does not meet the requirement
standards (speed, memory usage, security, etc...)TODO: handle channel modalities later, TODO: make
efficient, TODO: Implement Conv Transpose .321 40.68%
Code Bad coding practices leading to poor legibility of code, making it difficult to
understand and maintain.TODO: This next code is dense and confusing. Clean up
at some point.207 26.24%
Test Problems found in implementations involving testing or monitoring sub-
components.XXX: should we rather test if instance of estimator? 84 10.65%
Defect Identifieddefects in the system that should beaddressed. TODO this will fail if a parameter cant handle size=(N;) 82 10.39%
Design Areaswhichviolategoodsoftwaredesignpractices,causingpoorflexibilityto
evolvingbusiness needs.TODO maybe improve this so it doesn’t use a global 80 10.14%
Documentation Inadequatedocumentation that exists within the softwaresystem. TODO update doc above 15 1.90%
needed for a specific purpose in the short term; will
go.Because the two types of developers differ in the types
of SATD they accrue, it may be the case that the nature of
the software domain affects the developers’ maintenance
patterns.
As speculated by previous work[ 52],some ML developers may
notbeprimarilysoftwaredevelopers.Instead,theymightbedata
scientists, researchers, or domain experts. Therefore, developers in
these roles may not be as prepared to maintain software solutions.
Regardless, a comparison between Bavota and Russo’s study [ 4]
andourownsuggeststhattherearedistinctionsbetweentheSATD
patternsbetweenbothtypesofdevelopers.Additionally,thelan-
guages which are used by the studied software in both studies
are different. Bavota and Russo studied 159 repositories which are
written in Java [ 4], while our study analyzes Python projects, both
languages are studied together in a previous work on technical
debt[51].Therefore,thedifferenceinprogramminglanguageus-
age maycause ashift inSATD management.
Finding3:30.58%ofMLdevelopers’ML-specificSATDis
due toDataDependency .
As seen in Table 5,Data Dependency is the most used ML SATD
Group.MLsoftwarefunctionalityisheavilydependentuponthe
quality, structure, andconsistency ofmodels’ training data. There-
fore,thecodethatprocessesorstoresthisdatamayhavebecame
anaturalfocus for ML developers[ 45].Becausedata isinfluenced
andthroughMLmodelscaninfluencethe outsideworld,itiscru-
cial that ML developers ensure their data is well-monitored and
well-understood. Misunderstood data can cause unintended conse-
quences, and as a result can effect the outside world when naively
deployed[ 44,45].
Basedonourmanualanalysis,weobservethatMLdevelopers
mayleavemore DataDependency debtsthananyothergroupofML
SATDs.TechnicallyindebteddatacanbeharmfulforMLmodels
because it implies that the current data has identified shortcom-
ings, similar to how code can be described with technical debt.
For example, the comment text # TODO: EXPAND PROPER NOUNS
FOR COMMON WORDS AROUND WORD admits a preprocessing task
that should be improved upon. Because this current short-term
solution exists, not only is the software suffering from a
SATD,butthedatamaybeconsideredtechnicallyindebted
aswell.Althoughmany DataDependency debtsobservedinour
datasetinvolvedatapreprocessingtasks,thereareothercaseswhich
DataDependency canbeconcernedwith.Otherexamplesof Data
Dependency involveddebtsincludethoseassociatedwithdatavi-
sualizations ( TODO add general Distribution ), data storagemanagement ( TODO check the local cache and cloud for
different images of same name ),configuringqualitiesoftrain-
ing data ( TODO: weight by length here ), and interfacing with
modeloutput( FIXME: Only keeping the first label ).
WhileData Dependency SATD comments can involve how data
isprocessed, represented,and usedthroughML software,we also
find reoccurring symptoms in the Data Dependency SATD com-
ments. These symptoms indicate an implied problem or suggested
fixonMLcodethatinteractswithdata.Wefindthatthesesymp-
tomstypicallyindicateachangeinvolvingtheaddition,removal,
improval, replacement,reapplication,orhandlingan edgecaseof
somedatainvolvedcomponent.Anadditionindicatesaspotwhere
anewdataprocedurecanbeused( TODO: Add Hebrew-to-Aramaic
converter ). A removalindicates a procedure that may needto be
removed ( FIXME don’t l2_normalize for any metric ). An
improvement is an area where the data procedure can be modified
to perform better ( NOTE: probably more efficient to sort
then stride by nt_regions ). A replacement indicates where
a different data-involved functionality can be used ( TODO: hard
coded for now; looking for better extraction methods ).A
reapplication involves the reuse of a pre-existing procedure ( TODO:
It could be nice if this method was run on entire data;
not just a sample ). Finally, symptoms can indicate cases where
thecurrentproceduresfailordonotbehaveasexpected( Double
newlines seem to mess with the rendering ).
Therefore,MLdevelopersshouldconsiderhowtheirdatacon-
figuration implementations could evolve overtime. If a particular
componentdoesnotallowforthefixofoneofthesymptoms
describedabove,thenanunderlyingdata-dependentTDmay
exist.Since our data suggests that these problems are encountered
commonly in ML software, preparation for these modifications can
save maintenanceactivity.
Finding4: Awareness account for17.45%ofour ML SATD.
Awareness consistsofdebtstypeswherethelackofdevelopers’
knowledge or understandingnegatively affects its associated soft-
ware.SATDcommentsofthiskindmaybefoundintheformofa
question,suchasthecomment TODO - does this handle N-dim
tensors correctly? .Also,Awareness debtsmaybecausedbythe
natureofworkingwithMLmodelswhosefunctionalitycanbecon-
sideredablackbox[ 39,40].Whensuchcasesarise,thedemandfor
newfunctionality mayarisetobetterunderstandmodelbehavior.
Forexample,thecommentinourdataset TODO: Model Precision
admitsanewevaluationmetrictobeusedtobetterunderstandthe
performanceofaMLmodel,andwasconsideredan Awareness debt
underthis context.
738ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore David OBrien, SumonBiswas, Sayem Imtiaz, Rabe Abdalkareem, Emad Shihab, andHrideshRajan
Table 4: Definitions and Examples of the 23 Identified ML SATD Types: Awareness (AWR), Readability (RDB), Duplicate
CodeElimination(DCE),ConfigurableOptions (CFO), CodeDependency(CDD), Data Dependency(DTD), Modularity (MDL),
Performance(PRF), andScalability(SCL). ♣New ML SATD Type proposed inour study.
Dim.Name Definition ExampleAWRMachineLearning
Knowledge ♣Machine learning software carries plenty of unique challenges. Uned-
ucatedsolutions byunaware developers mayhave to be revisited.FIXME: can I backprop error
through both
Model
Interpretability ♣Machine learning models are a black box, causing poor understanding
ofmodel’sfunctionality.This can leadto unknownbehavior.TODO: Model PrecisionRDBModelCode Com-
prehension[ 52]Model code carries extra legibility concerns that do not occur in tradi-
tionalsoftware.(i.e.,poorlynamedtemporary matrixvariables).TODO refactor second part of if
statement when implementing live
model predictionDCEDuplicate Model
Code [52]Code duplicationfrequently occurs inmodelcode. TODO: Basically identical to
‘test_intra_cv_target_transform’
except for repeated KFold
Duplicate Fea-
ture Extraction
Code [52]Code duplicationfrequently occurs infeature extractioncode. XXX de-duplicate this with code
from Montage somewhere?CFOWeight Configu-
ration♣Editingcode that involves the weightsof aML model,or configuring
aML model’sweightsdirectly.FIXME non-uniform sample weights
not yet supported
Layer Configura-
tionEditingcodethatdealswithMLmodels’layers,orconfiguringaML
model’slayers itself.TODO: experiment with more
layers
Hyper-parameter
ConfigurationConfiguring hyper-parameters of ML model, or editing the default
valuesofoff-the-shelf model.TODO convert this to x/y params?CDDMachineLearning
Dependency ♣When a needed change in ML software occurs because of its depen-
dency on an external library or other piece of the ML software system.
Usually indicates a condition that is waiting to be met before removal.TODO add test for keypoints once
their handling was improved in
Convolve
Glue Code [ 45]Supporting code written to interface with other code, inhibiting im-
provements dueto peculiarities ofdependent code.FIXME XXX: Implement by
rewriting functions above
copied from autoresolve.py
Custom Data
Type [52]Using data types provided by general-purpose packages can cause
extensive inter-operating withexternal libraries.TODO: Repeat_elements and
tf.split doesn’t support dynamic
splits.
Multiple Lan-
guages [45]Componentswritteninotherlanguagesmayintroducedifficultiesin
ML development.TODO(sonots): Implement in C++
Unnecessary
ModelCode [ 52]Model code that either bottlenecks performance, is unreachable or
deprecated,orisunnecessary andshould be removed.DEPRICATE? I don’t think this is
needed anymoreDTDData Processing
Configuration ♣Configuringthewaythatdataisprocessedeitherbyeditingthedata
directly,orbyaddinginnewprocessing steps.TODO: normalize true states
Plain Old Data
Type [45]UsingrawdatatypesinMLcausesconfusionwheninterpretingpro-
cesses.TODO: handle record value which
are lists; at least error
Data Storage Con-
figuration ♣Configuring how data is represented within the source code (data
structure)orhowdata isstoredexternally (database).TODO json?MDLAbstraction[ 45]LackofabstractionsinMLsystemsandsubsystemscausecascading
changes when changes are introducedto one component.TODO Split into separate
functions
Boundary
Erosion[45]Lack of boundaries between subsystems, creating difficulties when
maintainingsoftware andisolating changes madeinML software.@todo nrows is for testing only!
ModelCodeMod-
ifiability[ 52]Modelcodeshouldbeimplementedinwaysthatenableeasymainte-
nanceandfuture modifications.TODO init this somewhere else in
a more principled way
Model Code
Reusability[ 52]Modelcodeshouldbegeneralizedtobeabletobereusedinvarying
situations.TODO: At the moment LHUC is RNN
specific.PRFPrediction Qual-
ity♣PreviousworkinevaluatingMLworkflows[ 11]showsthatchanges
mayaffectperformance.TODO: compare the performance!
MachineLearning
Reliability ♣Machinelearningmodels’functionalitiesaredeterminedbythequality
oftheirdata,measures should be inplaceto ensure robustness.TODO: extract an eval func more
robustlySCLPrototype [ 45]Small-scale prototypes being deployed into full systems can be dan-
gerous.TODO: This matching process is
slow. Make it faster; avoid loops
where possible.
73923Shades of Self-AdmittedTechnical Debt: AnEmpirical StudyonMachineLearning Software ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
Table 5:Distribution ofML SATD Groups.
ML SATD Group Name # of Occur. % of Occur.
Data Dependency 170 30.58%
CodeDependency 127 22.84%
Awareness 97 17.45%
Modularity 55 9.89%
ConfigurableOptions 51 9.17%
Scalability 31 5.58%
Readability 12 2.15%
Performance 11 1.97%
Duplicate CodeElimination 2 0.36%
Symptoms of Awareness debts include doubts on the correctness
of algorithmic procedures, doubts onthe current design decisions,
lackofknowledgeofproperAPIusage,erroneouscaseswherea
solutionisnotidentified,andlackofdomainknowledge.Doubts
on algorithmic procedures, either by interfacing with other soft-
wareorcorrectnessofalgorithmscanleadto Awareness debt(TODO
figure out what exactly size does ,FIXME: What if we
never find a fail-high? ).Doubtsoncurrentdesigndecisions
canbeduetoquestioningthequalitiesofacurrentimplementation
(TODO: Something smarter? )orquestioningthedesigndecisions
in place ( TODO: do we want gainsbiases to be trainable? ).
LackofproperAPIunderstandingcanalsobeasymptomof Aware-
nessdebts (TODO: Check if self._mean_squared_error_w_-
precision can be used here ). Additionally, cases where errors
are encountered or incorrect behavior is discovered, but a solu-
tion is not yet known can also be a symptom ( TODO: skip on
error??? ). Finally, since ML software can be applied in various
domains, it is not surprising that a lack of domain knowledge con-
tributestoquestionablesoftwarebehavior( TODO Figure out how
to distinguish oxidation states ).
ConsistentlyupdateddocumentationcanalleviatesuchSATDs,
disclosing properAPIusageand possible design intentions.Since
priorworkshowsthatmostSATDisaddressedbyadifferentdevel-
oper than the one who self-admitted it [ 4,37], including documen-
tation details such as limitations, advantages, and defense of the
currentimplementationcaninformfuturedevelopersofidentifiable
improvements.Forexample,thecomment TO-DO: this causes
very long running when unique numbers are high. Find a
workaround for this describes the symptoms and limitations of
presentimplementations,andgivesanactionablestartingpointfor
futuredevelopers.Therefore,themoreinformationwhichcanbe
disclosedbyadeveloperina Awareness debtscenario mayenable
timely removals of these debts regardless of the symptoms they
contain.
Our findingsindicatethat ML SATDs mayreflectthe fast-paced
environment of ML software through a large amount of Require-
mentdebts, setting itself apart from traditional software develop-
ment.Furthermore,datadependentcodeandlackofdeveloper
awareness are majorfactors contributingto SATD.
3.2RQ2: What is the distribution of SATD types
in thedifferentML pipeline stages?
This section further analyzes distributions of ML SATDs by also
considering which stage of the ML pipeline every SATD comment
appearswithin.BecausetheMLpipelineconsistsofuniquetasks
whichMLsoftwarecommonlyworksthrough,wequestionwhetherTable 6:Results frompriorworkby Bavotaand Russo[ 4].
SATD Type# of SATD
Comments# of Sampled
SATD Comments
CodeDebt 81 29.67%
DefectDebt 55 20.15%
Requirement Debt 55 20.15%
Design Debt 34 12.45%
Documentation Debt 27 9.89%
Test Debt 21 7.69%
Total 273 100%
someMLSATDsoccurmorefrequentlyinvaryingstages.Weuti-
lized prior works studying the ML pipeline [ 3,10,30] to construct
ourtaxonomyofMLpipelinestageswhichinclude DataAcquisition,
Data Preprocessing, Modeling, Training, Prediction, Evaluation, and
Other.TheOtherstageisreservedfortasksthatcanoccuranywhere
intheMLpipelineforavarietyofreasons(i.e.,datavisualization
anddatastorage)aswellascaseswheretheauthorswerenotconfi-
dent on a pipeline label due to lack of distinguishable information.
Table7presents the distribution of the ML SATD Groups amongst
the pipeline stages.
Finding 5: Data Preprocessing is the pipeline stage with
themostSATD.
DataPreprocessing isthestageintheMLpipelinethathasthe
most SATD comments out of all pipeline stages. RQ1 found that
Data Dependency is the most used ML SATD Group. However,
sinceData Preprocessing contains the most SATD in our dataset,
it suggests that ML developers leave SATD comments of a wide
variety in data preprocessing code. For example, the comment
TODO validation_split is not used exemplifies an Unneces-
saryModelCode debtthatwasfoundinthe DataPreprocessing stage.
Similarly, the comment FIXME: does pytorch has something
similar to tf.add_n which sum over a list? is aMachine
LearningKnowledge andCustomDataType debtthatappearedin
theData Preprocessing stage as well. The high SATD activity in the
Data Preprocessing stage could be caused by feature engineering
components having the largest body of code, therefore there is
more code to self-admit technical debts. Sculley et al . [45]showed
that a mature ML system may be at most 5% ML code. The rest
of the system may consist of subsystems such as process manag-
ing tools or feature engineering code, among others. Regardless,
thehighactivityofSATDwithin DataPreprocessing codestresses
the importance for rigorous review on code which handles data
preprocessing.
Finding 6: Data Dependency is the SATD Type which is
dominantin5 outofthe 7 pipelinestages.
Our observations find that Data Acquisition, Data Preprocessing,
Evaluation,Prediction,andOther stagesexhibit DataDependency
astheirprimarySATDType.Thisobservationindicatesthatsub-
systemsacrosstheMLpipelineencounterSATDamountswhich
involve adjustments to data interactions. To demonstrate that Data
Dependency debtscantakedifferentshapesamongstthepipeline
stagesconsiderthe Evaluation stagecomment FIXME don’t l2_-
normalize for any metric which involves the normalization
means in the Evaluation stage of a speaker diarization pipeline.
Meanwhile,thecomment TODO combine these values to get
a final prediction! describes functionality to be implemented
foranSVMmodelinthe Prediction stage.Ourmanualanalysis
suggeststhatpotentialsolutionstorepairingdata-dependent
740ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore David OBrien, SumonBiswas, Sayem Imtiaz, Rabe Abdalkareem, Emad Shihab, andHrideshRajan
Table 7:ML SATD Groupsby PipelineStage
ML SATD GroupsML PipelineStages
Acqu. Prep. Mod.Train. Pred. Evalu. Oth.
Awareness 12 3918 9 2 512
CodeDependency 19 3933 13 1 616
ConfigurableOptions -323 23 - 11
Data Dependency 26 82 8 13 4 631
DuplicateCode Elim-
ination-2 - - - --
Modularity 71220 9 3 -4
Performance - -2 2 - 52
Readability 1 24 3 1 1-
Scalability 611 4 5 - 23
Repo TypeML SATD Group
AwarenessCode
DependencyData
Dependency Modularity Performance Readability
% of SATD % of SATD % of SATD % of SATD % of SATD % of SATDApplication
Tool22.10%
18.37%22.83%
32.65%36.96%
34.69%13.41%
9.18%3.26%
1.02%1.45%
4.08%ML SATD Group by Repo Type
ML SATD Group
Awareness
Code Dependency
Data Dependency
Modularity
Performance
Readability
% of Total Count of ML TD Type for each Repo Type broke n down by ML SATD Group.  Color shows details
about ML SATD Group.  The marks are labeled by % of Tot al Count of ML TD Type. The data is ﬁltered on
ML TD Type, which excludes Null. The view is ﬁltered on ML SATD Group, which excludes Conﬁgurable
Options, Duplicate Code Elimination and Scalability. P ercents are based on each row of the table.
Figure 1:ML SATD GroupingsDistribution by RepoType
SATDs may be completely different between pipeline stages,
despite their broad similarities. Another explanation to this
observationistheconceptofPipelineJunglesintroducedbySculley
et al.[45] where there is little independent responsibility given to
eachpipelinestage.Becauseofthis,datapreprocessingisnotiso-
lated to one coherent stage, rather data preprocessing is performed
"asneeded"[ 10].Thus,theplacementofdatapreprocessingcode
withinotherMLpipelinestagesaccruestechnicaldebt,sincethe
debugging or refactoring must consider the data transformation
implementations acrossmultiple stages.
We observethat Data Preprocessing isthemost popularpipeline
stage,stressingtheimportanceofmindfuldatahandlingimple-
mentations. Similarly, Data Dependency debts are the biggest
contributorto5pipelinestagesinourdataset,thetwoexceptions
being the Modeling andTrainingstages.
3.3 RQ3: Isthereadifference in SATDtypes
betweenML applicationsvsML tools?
Inthissection,weseparateourfindingsintotheMLrepositories
that apply ML towards a task (applications) and ML repositories
that provide ML functionalities (tools) as labeled by the original
dataset creators[ 26].
Figure 1illustrates the MLSATD group distributionsbyreposi-
torytypetovisualizedifferencesbetweenthem.Forviewingpur-
poses,onlytheMLSATDGroupswhichdifferbymorethan2%are
shown.
Finding7:OuranalysisofSATDcommentssuggeststhat
CodeDependency debtsappearmoreofteninMLtoolsthan
ML applications.
According to Figure 1, Code Dependency related debts may be
more common in ML tools than ML applications. Code Dependency
referstoasimilarconceptto"On-HoldSATD"explainedbyMaipra-
ditetal.[35].Intheseinstances,anSATDiswaitingforacondition
to be met before taking action (e.g., waiting for an update or other
developmenttaskstocomplete)suchasthecomment TODO Modify
mu and sigma once feature scaling is built into thelogistic regression . We describe Code Dependency as SATD
whichdependsuponothercode.Casessuchasinterfacingwithspe-
cificdatatypes( TODO: Repeat_elements and tf.split doesn’t
support dynamic splits ),changestosoftwareandAPIswhich
causeversioningupdates ( TODO: assertWarns exist only for
Python 3.2+; test in all versions ), or cascading changes
throughoutsoftwaresystems( TODO: it’s worth to switch back
to the correct preprocess_input when InceptionResNetV2
model is re-trained ).
MLtoolsfindthemselvesinacompetitiveenvironmentamongst
themselves to provide efficient techniques for their users. Previous
workhasdescribedthetemptationthatashortertime-to-market
bringstodeeplearningframeworkdevelopers[ 34].Itcouldbethat
in order to stay relevant, ML tools’ implementations must inter-
operatewithotherevolvingMLtools.Thus, CodeDependency debts
may occur when other tasks take priority above resolving these
identified problems. Our observations suggest ML tool developers
sufferfromtheseoccurrencesmorethanMLapplicationdevelopers.
We believe that ML tool developers can benefitfrom these ob-
servationsbyfurtherevaluatingtheimplementationswhichthey
dependon,andwhattheadvantagesofevolvingincorrespondence
withthoseimplementationsmaybring.Furthermore,weencourage
ML tool developers to consider how severe of a refactoring that an
API change may have upon depending systems, and to allow for
upgradingversionsto be performedwithease.
Finding8:Inourdataset,MLapplicationsencountermore
Modularity debts.
Modularity debts describe cases where weak modular bound-
ariesexistbetweenMLsubsystems.Anexampledebtinthe Mod-
ularitygroup isBoundary Erosion [45], an example comment of
whichis@todo nrows is for testing only! ,whichdescribesa
casewhere a parameterintended fortestingpurposes was instead
usedwithindatavisualizationcode.Another Modularity exampleis
theAbstraction debt comment TODO: refactor as independent
function which admits the task of creating a new abstraction
within afeature engineeringprocess.
Our analysis suggests that Modularity debts are found more
often in ML applications than ML tools, hinting at a possible dif-
ferencebetweentheirsoftwaremaintenancepatterns.Apossible
explanationmaylieinthecommonuseofJupyterNotebooksbyap-
plicationdevelopersasexploredbyPimentel etal.[41].InaJupyter
Notebook,application developers usean interactive environment
to debug and monitor their code. However,the transition from
Jupyter Notebook to an Object Oriented (OO) design may
be difficult, and instances of Modularity SATD may be left
wherethesetransitionswereattempted. Tangetal .[52]also
speculates how ML developers may not be familiar with Object
OrientedProgrammingbestpractices,leadingtotheintroduction
ofsuch technical debts.
ItwasfoundthatMLtoolssufferfrommuchmore CodeDepen-
dencydebts,possiblyasideeffectfrominter-operatingwithother
libraries. Additionally, our results suggest that ML application
developers incur more Modularity debts.
74123Shades of Self-AdmittedTechnical Debt: AnEmpirical StudyonMachineLearning Software ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
Figure 2:ML SATD GroupingsRemoval Characteristics.
3.4 RQ4: Howmucheffort isneeded to remove
SATDtypesin ML software?
Thissection analyzestheeffortconductedtoremoveSATDinML
software.Inordertoaccomplishthis,weonlyanalyzeourSATD
commentswhichhavebeenremovedatthetimeofdatageneration.
We define effort of removing an SATD comment as a compar-
ison between the time passed since its introduction and size of
the commit which removes it. Since an SATD may take multiple
commitstoremovebecauseofunknownsolutionsorlowerprioriti-
zation, we follow the advice presented by Bird et al .[6]to connect
a comment-removing commit to a comment-introducing comment
ofthesametext.Becauseallcommitsarenotthesamesize,wealso
usetheBoalanguageanditsinfrastructurecreatedbyDyeretal .
[15]to perform the GumTree algorithm [ 20] to compute the edit
script of asource code change at the Abstract Syntax Tree (AST)
level.Withthismethodology,wecountthenumberofASTnodes
modified in the revision which removes the SATD comment to get
an indication of work which removed it. Due to larger repositories
causing timeout errors, some commit revisions are unable to be
quantifiedinour dataset.
Figure2illustratesourresultsontheMLSATDGroups.Sincewe
wanttopresentondatawithlargerbodies,theMLSATDGroups
shownarethosewhichcontain10ormoreremovedinstances.Inad-
dition,Figure3illustratesourresultsacrosstheMLpipelinestages.
We measure our statistics in median because it is unaffected by
outliers, since it has been shown that an SATD comment may have
been removed without any additional code change or alongside
large refactorings. [ 37].
Finding 9: Data Dependency, Configurable Options, and
Awareness may require less removal effort.
ThelownumberofcommitsandlowASTcountintheremoving
commit suggest that these debts are more convenient to remove.
For example, the SATD comment: TODO: modify this later is
aData Dependency debt describing a sampling modification was
removed in 9 commits with 20 modified AST nodes. Because of
these lower quantities involved in these comments’ removals, it
could be that ML developers understand these SATDs better, since
muchofMLsoftwareinvolvesoperatingwithdatadependentcode,
configurable APIs, and unknown solutions. Dilhara et al.exam-
ined common code changes made in ML software by reapplying
RefactorMiner to Python programs [ 14]. However, further work is
neededtounderstandthechallengesinvolvedwhenMLdevelopers
identifyandimplement differing code changes.
0 10 20 30 40 50
Median Amount of Commits until Removal0100200300400Median Modiﬁed AST AmountSheet 6
ML Pipeline Stage
Data Acquisition
Data Preprocessing
Evaluation
Modeling
Other
Prediction
Training
Median of Amount of Commits until Removal vs. median  of
Modiﬁed AST Amount.  Color shows details about ML Pip eline
Stage. The view is ﬁltered on ML Pipeline Stage, which  excludes
Null.Figure 3:ML SATD Removal Amongst PipelineStages
Finding10: Modularity debtshaveahigheramountofAST
nodes intheir removing commits.
Ouranalysissuggeststhat Modularity debtremovalshavelarger
commitsandshorterremovaltimes.Therefore,highactivityinalow
amountoftimemaysuggestthatidentified Modularity debtsarethe
mosturgenttoremove.Considerthecomment TODO Promote this
to a Funsor subclass. which involves reorganizing software
classeswhoseremovalinvolved355modifiedASTnodes.Apossible
explanationis thatthe heavier refactoring of Modularity debts are
viewed as more valued improvements to ML developers, and are
prioritizedsooner despitetheir larger changesize.
Therefore, ML developers can benefit from this finding by care-
fullyconsideringtheirmodulardecisionsoverpotentially"simpler"
decisionsearlierindevelopment.Sinceourresultsindicateheav-
ier activity is required from Modularity debt resolutions, earlier
decisions involving abstractionsand modulardesign are more im-
portant in a software’s lifetime than issues which are lighter to
resolve.
Finding 11: Our analysis suggests that SATD in Prediction
andEvaluationstages have largerremoval characteristics.
Figure 3 shows the complexities of debt removals by the ML
pipeline stages. Our study suggests that debts removed in the Eval-
uationstagehavealargeamountofmodifiedASTnodes,anddebts
removed in the Prediction stage are removed after a larger number
ofcommits have passed.
Thesefindingsmayindicatethatdebtsremovedinthe Evaluation
stage may require larger activity, possibly due to reused evaluation
metricsacrossmultiplemodelcodes. Therefore,achangemadein
this stage may have a wide reach of consequential changes. If a
debt in this stage goes unnoticed, its resolution may take a heavier
amountofwork,asissuggestedbythelargeramountofASTnodes
modifiedincomment removals in Evaluation stagecode.
Ourobservationsindicatethatdebtsfoundinthe Prediction stage
gounresolvedforlargernumberofcommits,possiblyhintingatML
developer’spriorities whenworkingacrossthe pipelineforlarger
periodsoftime.Ratherthanimproveuponpredictinginvolvedcode,
it may be that ML developers choose to prioritize the immediate
removal of SATDs elsewhere. Thus, the Prediction stage may be an
areaofMLsoftwarewheretheremovalofanSATDisnotasurgent.
742ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore David OBrien, SumonBiswas, Sayem Imtiaz, Rabe Abdalkareem, Emad Shihab, andHrideshRajan
Ourstudysuggeststhat Awareness ,ConfigurableOptions ,andData
Dependency debts are easier to remove than Modularity debts.
Moreover, there are heavier debts to pay off in the Evaluation
pipeline stage,anddebtsinthe Prediction stagelast the longest.
4 DISCUSSION
ML techniques are increasingly adopted by developers to solve
previously difficult-to-solve problems, thus there is a benefit to
understandinghowthesesystemsevolve.AlthoughSculleyetal .
[45]previously explored the hidden technical debts encountered
atGoogleand Tang etal.examinedhowrefactoring accompanies
technicaldebtsinMLsoftware[ 52],tothebestofourknowledge
nopreviousstudyhasexaminedhowML-specifictechnicaldebts
permeateasself-admittedtechnicaldebt.ThestudyofSATDhas
previouslybeenfruitfultopractitionersandresearcherssincean
organization has reported that SATD guides their technical debt
management [ 54]. This large-scale study ofSATDin ML software
can also provide researchers and practitioners with insights on ML
software evolution from our analysisofML SATD.
TheMLSATDtaxonomydepictedinTable4isasynthesisofprior
works studying TD in ML software and our manual analysis of 856
SATDcomments.ThistaxonomynotonlyincludesnewtypesofTD
throughSATD analysis,but alsosplits previous typeswhichwere
previously described a large assortment of issues. Our proposed
taxonomyishierarchicalinnature,providingMLdevelopersand
researchers a classification scheme for reasoning about issues in
MLsoftwaremaintenance.WebelievethistaxonomycanguideML
developers in ML development, as well as provide researchers with
areas to further explore.
Additionally, the ML pipeline has been a focus of researchers
recently [ 3,10], yetno prior study has examined the SATD which
persist through the individual stages’ evolution. Our analysis of
SATDinthesepipelinestagesisastepinunderstandingtheunique
challenges each stage encounters overtime. Moreover, this study is
thefirsttoanalyzehowdifferingtypesofMLprojectsencounter
SATD to researchers interested in improving ML developer experi-
ence. Finally, we provide a study quantifying the efforts needed to
remove various SATDs in ML software, although further work can
complementthesefindings.
4.1 Implications
(1)OurstudysuggeststhatSATDinMLsoftwarecanbeinspected
onvariouslevels,suchashowtraditionalsoftwareevolvesin
additiontoMLspecificways.Ourproposedtaxonomycanguide
ML practitioners in their ML software maintenance, describing
23uniqueproblemswhichhavebecomeevidentthroughour
SATDanalysis.AkeyfindingisthatMLsoftwareencounters
debtsfrequentlyinvolvethemanipulationofdataprocessing
means, including the introduction, removal, improvement, and
reuseofcodewhichhandlesthedatafromdataacquisitionto
model evaluation. Therefore, ML developers should consider
thedatarequirements(distribution,organization,appearance,
etc... [23]) for every pipeline stage in their software and where
theserequirements are not met.
(2)Traditional software developer and ML developers differ in
theirSATDactivity. MLdevelopers mayprefer toevolve theirsoftwarethroughfunctionalandnon-functionalrequirement
changes as new capabilities become available or their software
interacts with the real world. Because of this, ML software
maybenefitfromdistinctsolutionsandtoolsthantraditional
software. Consequentially, researchers may be interested if
thereuseof traditionaldeveloper-assistingtoolsmaynot ade-
quatelyaddresstheSATDissuesencounteredbyMLdevelopers.
(3)WefindthatMLdevelopers’SATDchangesdependingonthe
type of ML software being maintained. This is an interesting
direction,showing that different typesof developers may gain
greater benefits from various techniques. For instance, we find
that ML tool developers accrue larger debts from interacting
with fellow evolving libraries [ 13] and ML application devel-
opers can benefit from more focused modular designs imple-
mentedearlierinproduction.
5 THREATS TO VALIDITY
In this section, we discuss the threats to internal, construct and
external validity ofour study.
Internal validity: concerns factors that could have influenced
our results. Our findings depend largely on the data labeled. In
ordertoensurethatourlabeleddataisprecise,twoauthorswent
throughanextensiveperiodwheretheirCohen’sKappacoefficient
wascalculated,anddisagreementsinlabelingweresettledinthe
presenceofathirdauthoruntiltheirCohen’sKappacoefficientwas
above 80%.
Whenassociatingcomment-introducingrevisionswithcomment-
removingrevisions,weonlyconsiderthecaseswherethecomment
appeared the same during both commits. The comment could have
beenmodifiedin-between,orotherunusualcasesmighthaveoc-
curred[6].Becauseofthis,ourseparationofcommentsthathave
beenremovedandcommentsthatstillexistwithintheirprojects
may be inaccurate. This should only affect cases where a comment
was incorrectly placed as not removed. All comments that were
found to have been removed should be true positives, which are
the casesanalyzedinRQ4.
It is possible our methodologies of measuring "effort" rather
measures "priority" or "system impact". However, we argue that
there may be no perfect way to measure "effort" since develop-
ment patterns and activity likely differ across software projects.
For this reason, we utilized multiple measures (change-size and
time) to quantify effort, which can be adopted in future research to
complementour findingsfurther.
Construct validity: considers the relationship between theory
andobservation.IthasbeenshownthattheremovalofanSATD
comment is a naive indication that a TD was also removed [ 37,58].
A comment could be removed without any fix to the SATD or a
SATDcommentcouldberemovedatalatertimethantheresolution,
or the comment could never be removed at all. To mitigate this,
the authors examined the code around the SATD comment and
wereabletoidentifyafewcaseswheretheSATDcommentisno
longerrelevant. Thesecaseswere thenremovedfromourlabeled
data.Additionally,thequantitativedataofRQ4wasmeasuredby
its median to avoid influence byoutliers.
The projects analyzed within this project may not be indicative
ofpresentMLsoftwarepractices.However,thedatasetwascreated
74323Shades of Self-AdmittedTechnical Debt: AnEmpirical StudyonMachineLearning Software ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
through extensive queries of the GitHub API with a variety of key-
words consisting of topics, subtopics, technologies, and techniques
relatedtoML[ 26].Becauseofthis,webelieveourstudyhasawide
representation ofML topics.
Although the SATD comment classifier used has historically
performed well, it still may produce false predictions when our
dataset was filtered. However, when labeling, the authors would
removeanyfalsepositivesamples.Theinclusionofusingakeyword
searchalongwithusingthecommentclassifier’soutputwasusedto
recoveranyfalsenegativesamplesthecommentclassifierproduced.
Externalvalidity: concernsthegeneralizationofourresults.In
this study, we examined only open-source Python repositories that
are divided into applications and tools used in previous studes [ 26,
50]. Hence, our findings may not be generalized to to other non-
Pythonrepositoriesornon-open-sourcerepositories.Thatsaid,our
decisionofstudyingPythonrepositorieswasmadebyitspopularity
among the ML community [ 26]. Also, our dataset size of 2,641 ML
Pythonrepositoriesthatmaynotrepresentthewholepopulation
ofML software written inPython.
6 RELATED WORK
Inthissection,wedescribetherelatedwork.Wedividedtherelated
workintotwosections:workrelatedtotechnicaldebtmanagement
ingeneralandworkrelatedtothemanagementoftechnicaldebt
inmachine learningrepositories.
Generalmanagementoftechnicaldebt. Severalearlierworksstud-
iedthemanagementanddetectionoftechnicaldebt(e.g.,[ 12,19,32,
47]).Forexample,Brownetal .[12],Kruchtenetal .[32],andSeaman
and Guo [47]made several observations about the term ‘techni-
caldebt’andmentionedthatitisregularlyusedtocommunicate
developmentissuestomanagers.Similarly,Zazworkaetal .[59]per-
formedastudytomeasuretheimpactoftechnicaldebtonsoftware
quality.OtherworksbyFontanaetal .[24]investigateddesigntech-
nical debt indicated in the form of code smells. Furthermore, Ernst
et al. [19]conducted a survey involving more than 1,800 partici-
pantsandfoundthatarchitecturaldecisionsarethemostimportant
sourceoftechnicaldebt.Similarly,inthispaper,westudytechni-
cal debt insoftware applications. However, ourstudy focuses on
gainingabetterinsightintotheexistenceofthetechnicaldebtin
ML repositories.
Recently,[ 42]proposedtheconceptofself-admittedtechnical
debt (SATD), which considers debt that is intentionally introduced
oridentifiedbydevelopers.[ 42]analyzedmorethanahundredthou-
sandcodecommentsfromfourprojectstocomeupwith62patterns
that indicate self-admitted technical debt. Also, their findings re-
veal that approx. 31% of the files in a project contain self-admitted
technical debt. More specifically, they found that 1) the majority of
the self-admittedtechnical debt isremoved in the immediate next
release;2)developerswithhigherexperiencearemostlytheones
who introduce the self-admitted technical debt; 3) release pressure
doesnotplayamajorroleintheintroductionofself-admittedtech-
nicaldebt.Inafollow-upstudy,BavotaandRusso [4]replicatedthe
studyofSATDonalargesetofprojectsandconfirmedthefindings
observedbyPotdarandShihabintheirearlierwork [ 42].
OtherworksinvestigatedifferentaspectsrelatedtoSATD,includ-
ing;automaticallyidentifySATDfromsourcecode(e.g.,[ 36,43]),examinethemaintenanceandtheremovalofSATD[ 37,58],and
discusshowtheexistenceofSATDmayleadtotherejectionofpull
requests by developers [ 49]. We refer the reader to a recent survey
by[48]foramorecomprehensivelistofstudiesonself-admitted
technicaldebt.Similartotheworkmentionedabove,ourstudyuses
source code comments to detect self-admitted technical debts in
ML repositories.
Managementoftechnicaldebtinmachinelearningapplications. In
recent years, examining the characteristics of machine learning ap-
plicationshasgainedmomentum.Oneofthefirststudiesrelatedto
technicaldebtinMLapplicationsintroducedtheconceptofimplicit
TD in ML software through experiences encountered during ML
projectsatGoogle[ 45].Thisworkwasthenfurtheredby[ 52],where
new ML-specific refactoring and TD categories were introduced,
accompanied by recommendations of best practices to facilitate
long-termMLsoftwaredevelopment.Similarly,theworkby[ 11]
discussesaworkflowtoevaluateMLsoftwaredevelopmentprac-
tices at thedata,model,infrastructure, and monitoring levels that
wasthenperformedacrossmultipleMLprojectsatGoogle.Liuetal .
[34]examined the existence of TD in deep learning frameworks.
Theirstudyshowedthatdesigndebt,defectdebt,documentation
debtare mostpresentedTD indeep learningframeworks.
Several studies examinethe developmentofML repositories in
general. Humbatova et al . [28]and Islam et al . [30,31]examined
deeplearning systemsto systematicallybuild ataxonomy of bugs
that impact deep learning systems, and recent works propose tech-
niquestoaddresstheseidentifiedissues[ 55,56].Nguyenetal .[38]
proposesatechniquetoleveragerepositoriesofmodelstoassistML
developers. Amershi et al . [3]investigates the unique challenges
faced by software developers when developing ML applications.
ML models can exhibit new qualities unfamiliar to traditional soft-
ware such as model fairness [ 8,9]. Dilhara et al .[13]conducted an
empiricalstudytoexaminetheevolutionandusageofMLlibraries.
7 CONCLUSION
Nowadays, ML solutions are being adopted by software develop-
ers to accomplishotherwise infeasibletasks. Similar to traditional
software, there are uniquecosts to impractical design decisions in
ML software that result in "technical debts". In this study, we have
analyzedself-admittedtechnicaldebtsinMLsoftwarethroughasta-
tistically significant sample of SATD comments from open-source
repositories. Furthermore, we propose a classification scheme con-
sisting of Software SATD Types, ML SATD Types and 8 new ML
SATD Groups. This classification scheme can provide practitioners
and researchers a structure for discovering and managing their
SATD.Additionally,weprovideananalysisofSATDcharacteristics
astheyappearamidstpipelinestagesandrepositorytypeaswell
as an analysis of SATD removal effort. The results discussed can
assist developers and researchers to create more maintainable and
improvableML software solutions.
ACKNOWLEDGMENTS
This work wassupported in part byUS NSF grants CNS-21-20448
and CCF-19-34884 We also thank the reviewers for their insightful
comments. All opinions are of the authors and do not reflect the
viewofsponsors.
744ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore David OBrien, SumonBiswas, Sayem Imtiaz, Rabe Abdalkareem, Emad Shihab, andHrideshRajan
REFERENCES
[1]ReemAlfayez,WesamAlwehaibi,RobertWinn,ElaineVenson,andBarryBoehm.
2020. A Systematic Literature Review of Technical Debt Prioritization. In Pro-
ceedings of the 3rd International Conference on Technical Debt (Seoul, Republic
ofKorea) (TechDebt’20) .AssociationforComputingMachinery,NewYork,NY,
USA,1ś10. https://doi.org/10.1145/3387906.3388630
[2]Nicolli S.R. Alves, Leilane F. Ribeiro, Vivyane Caires, Thiago S. Mendes, and
Rodrigo O. Spínola. 2014. Towards an Ontology of Terms on Technical Debt.
In2014 Sixth International Workshop on Managing Technical Debt . 1ś7.https:
//doi.org/10.1109/MTD.2014.9
[3]SaleemaAmershi,AndrewBegel,ChristianBird,RobertDeLine,HaraldGall,Ece
Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019.
SoftwareEngineeringforMachineLearning:ACaseStudy.In 2019IEEE/ACM
41st International Conference on Software Engineering: Software Engineering in
Practice (ICSE-SEIP) . 291ś300. https://doi.org/10.1109/ICSE-SEIP.2019.00042
[4]Gabriele Bavota and Barbara Russo. 2016. A large-scale empirical study on
self-admitted technical debt. In International Conference on Mining Software
Repositories . ACM,315ś326.
[5]Houssem Ben Braiek, Foutse Khomh, and Bram Adams. 2018. The Open-Closed
Principle of Modern Machine Learning Frameworks. In 2018 IEEE/ACM 15th
InternationalConference onMiningSoftwareRepositories(MSR) . 353ś363.
[6]ChristianBird,PeterC.Rigby,EarlT.Barr,DavidJ.Hamilton,DanielM.German,
and Prem Devanbu. 2009. The promises and perils of mining git. In 2009 6th
IEEE International Working Conference on Mining Software Repositories . 1ś10.
https://doi.org/10.1109/MSR.2009.5069475
[7]Sumon Biswas, Md Johirul Islam, Yijia Huang, and Hridesh Rajan. 2019. Boa
Meets Python: A Boa Dataset of DataScience Softwarein Python Language. In
Proceedingsofthe16thInternationalConferenceonMiningSoftwareRepositories
(Montreal,Quebec,Canada) (MSR’19) .IEEEPress,577ś581. https://doi.org/10.
1109/MSR.2019.00086
[8]Sumon Biswasand Hridesh Rajan. 2020. Dothe Machine Learning Models on a
CrowdSourcedPlatformExhibitBias?AnEmpiricalStudyonModelFairness.
InProceedingsofthe28thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering (Virtual
Event, USA) (ESEC/FSE 2020) . Association for Computing Machinery, New York,
NY, USA,642ś653. https://doi.org/10.1145/3368089.3409704
[9]SumonBiswasandHrideshRajan.2021. FairPreprocessing:TowardsUnderstand-
ing Compositional Fairness of Data Transformers in Machine Learning Pipeline.
InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngineering
ConferenceandSymposiumontheFoundationsofSoftwareEngineering (Athens,
Greece)(ESEC/FSE 2021) . Association forComputing Machinery, NewYork, NY,
USA,981ś993. https://doi.org/10.1145/3468264.3468536
[10]Sumon Biswas, Mohammad Wardat, and Hridesh Rajan. 2022. The Art and Prac-
tice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines
In Theory, In-The-Small, and In-The-Large. In ICSE’22: The 44th International
Conference onSoftwareEngineering (Pittsburgh,PA, USA).
[11]EricBreck,ShanqingCai,EricNielsen,MichaelSalib,andD.Sculley.2017.TheML
TestScore:ARubricforMLProductionReadinessandTechnicalDebtReduction.
InProceedingsofIEEE Big Data .
[12]N.Brown,Y.Cai,Y.Guo,R.Kazman,M.Kim,P.Kruchten,E.Lim,A.MacCormack,
R.Nord, I. Ozkaya, R.Sangwan,C. Seaman, K. Sullivan, andN. Zazworka. 2010.
Managing Technical Debt in Software-reliant Systems. In FSE/SDP Workshop on
FutureofSoftwareEngineering Research . ACM,47ś52.
[13]Malinda Dilhara, Ameya Ketkar, and Danny Dig. 2021. Understanding Software-
2.0: A Study of Machine Learning Library Usage and Evolution. ACM Trans.
Softw.Eng.Methodol. 30,4,Article55(2021), 42pages.
[14]Malinda Dilhara, Ameya Ketkar, Nikhith Sannidhi, and Danny Dig. 2022. Dis-
covering Repetitive Code Changes in Python ML Systems. In 2022 IEEE/ACM
44th International Conference on Software Engineering (ICSE) . 736ś748. https:
//doi.org/10.1145/3510003.3510225
[15]RobertDyer,HoanAnhNguyen,HrideshRajan,andTienN.Nguyen.2013.Boa:A
language and infrastructure for analyzing ultra-large-scale software repositories.
In2013 35th International Conference on Software Engineering (ICSE) . 422ś431.
https://doi.org/10.1109/ICSE.2013.6606588
[16]Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2015. Boa:
Ultra-Large-Scale Software Repository and Source-Code Mining. ACM Trans.
Softw.Eng.Methodol. 25,1,Article7 (2015), 7:1ś7:34pages.
[17]Robert Dyer, Hridesh Rajan, Hoan Anh Nguyen, and Tien N. Nguyen. 2014.
Mining Billions of AST Nodes to Study Actual and Potential Usage of Java
LanguageFeatures.In Proceedingsofthe36thInternationalConferenceonSoftware
Engineering (Hyderabad, India) (ICSE’14). 779ś790.
[18]RobertDyer,HrideshRajan,andTienN.Nguyen.2013. DeclarativeVisitorsto
EaseFine-grainedSourceCodeMiningwithFullHistoryonBillionsofASTNodes.
InProceedingsofthe12thInternationalConferenceonGenerativeProgramming:
Concepts & Experiences (Indianapolis, IN) (GPCE). 23ś32.
[19]NeilA.Ernst,StephanyBellomo,IpekOzkaya,RobertL.Nord,andIanGorton.
2015. Measure It? Manage It? Ignore It? Software Practitioners and Technical
Debt. InProceedings of the 2015 10th Joint Meeting on Foundations of SoftwareEngineering (ESEC/FSE 2015) . 50ś60.
[20]Jean-RémyFalleri,FloréalMorandat,XavierBlanc,MatiasMartinez,andMar-
tin Monperrus. 2014. Fine-grained and accurate source code differencing. In
ACM/IEEEInternational Conferenceon AutomatedSoftwareEngineering, ASE ’14,
Vasteras, Sweden - September 15 - 19, 2014 . 313ś324. https://doi.org/10.1145/
2642937.2642982
[21]Joseph L Fleiss, Bruce Levin, Myunghee Cho Paik, et al .1981. The measurement
ofinterrateragreement. Statisticalmethodsforratesandproportions 2,212-236
(1981), 22ś23.
[22]SamuelW.Flint,JigyasaChauhan,andRobertDyer.2021. EscapingtheTimePit:
PitfallsandGuidelinesforUsingTime-BasedGitData.In 2021IEEE/ACM18th
International Conference on Mining Software Repositories (MSR) . 85ś96. https:
//doi.org/10.1109/MSR52588.2021.00022
[23]H.Foidl,M.Felderer,andR.Ramler.2022. DataSmells:Categories,Causesand
Consequences,andDetectionofSuspiciousDatainAI-basedSystems.In 2022
IEEE/ACM 1st International Conference on AI Engineering ś Software Engineering
for AI (CAIN) . IEEE Computer Society, Los Alamitos,CA,USA, 229ś239. https:
//doi.ieeecomputersociety.org/
[24]F.A.Fontana,V.Ferme,andS.Spinelli.2012. Investigatingtheimpactofcode
smellsdebt onqualitycodeevaluation. In InternationalWorkshoponManaging
TechnicalDebt . IEEE,15ś22.
[25]Gianmarco Fucci,NathanCassee,Fiorella Zampetti, NicoleNovielli, Alexander
Serebrenik, and Massimiliano Di Penta. 2021. Waiting around or job half-done?
Sentimentinself-admittedtechnicaldebt.In 2021IEEE/ACM18thInternational
ConferenceonMiningSoftwareRepositories(MSR) .403ś414. https://doi.org/10.
1109/MSR52588.2021.00052
[26]Danielle Gonzalez, T. Zimmermann, and N. Nagappan. 2020. The State of the
ML-universe: 10 Years of Artificial Intelligence & Machine Learning Software
Development on GitHub. Proceedings of the 17th International Conference on
MiningSoftwareRepositories (2020).
[27]QiaoHuang,EmadShihab,XinXia,DavidLo,andShanpingLi.2018. Identify-
ing Self-Admitted Technical Debt in Open Source Projects Using Text Mining.
Empirical Softw. Engg. 23, 1 (feb 2018), 418ś451. https://doi.org/10.1007/s10664-
017-9522-4
[28]NargizHumbatova,GunelJahangirova,GabrieleBavota,VincenzoRiccio,Andrea
Stocco, and Paolo Tonella. 2020. Taxonomy of Real Faults in Deep Learning
Systems.In ProceedingsoftheACM/IEEE42ndInternationalConferenceonSoftware
Engineering (ICSE’20) . 1110ś1121.
[29]NickHynes,D.Sculley,andMichaelTerry.2017. TheDataLinter:Lightweight
Automated Sanity Checking for ML Data Sets. http://learningsys.org/nips17/
assets/papers/paper_19.pdf
[30]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
ComprehensiveStudyonDeepLearningBugCharacteristics.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
and Symposium on the Foundations of Software Engineering (Tallinn, Estonia)
(ESEC/FSE 2019) . Association for Computing Machinery, New York, NY, USA,
510ś520. https://doi.org/10.1145/3338906.3338955
[31]Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ing Deep Neural Networks: Fix Patterns and Challenges. In Proceedings of the
ACM/IEEE42ndInternationalConferenceonSoftwareEngineering (Seoul,South
Korea)(ICSE’20) .AssociationforComputingMachinery,NewYork,NY,USA,
1135ś1146. https://doi.org/10.1145/3377811.3380378
[32]PhilippeKruchten,RobertL.Nord,IpekOzkaya,andDavideFalessi.2013. Techni-
caldebt:towardsaCrisperDefinition.Reportonthe4thInternationalWorkshop
on Managing Technical Debt. ACM SIGSOFT Software Engineering Notes 38, 5
(2013), 51ś54.
[33]Valentina Lenarduzzi, Terese Besker, Davide Taibi, Antonio Martini, and
Francesca Arcelli Fontana. 2021. A systematic literature review on Technical
Debtprioritization:Strategies,processes,factors,andtools. JournalofSystems
and Software 171(2021), 110827. https://doi.org/10.1016/j.jss.2020.110827
[34]Jiakun Liu, Qiao Huang, Xin Xia, Emad Shihab, David Lo, and Shanping Li. 2020.
Is Using Deep Learning Frameworks Free? Characterizing Technical Debt in
DeepLearningFrameworks (ICSE-SEIS’20) . 1ś10.
[35]Rungroj Maipradit, Christoph Treude, Hideaki Hata, and Kenichi Matsumoto.
2019. Wait For It: Identifying "On-Hold" Self-Admitted Technical Debt. CoRR
abs/1901.09511 (2019). arXiv: 1901.09511 http://arxiv.org/abs/1901.09511
[36]EvertonMaldonado, Emad Shihab, and NikolaosTsantalis. 2017. Using Natural
LanguageProcessingtoAutomaticallyDetectSelf-AdmittedTechnicalDebt. IEEE
Transactions onSoftwareEngineering (2017), to appear.
[37]Everton Da S. Maldonado, Rabe Abdalkareem, Emad Shihab, and Alexander
Serebrenik.2017. AnEmpiricalStudyontheRemovalofSelf-AdmittedTechnical
Debt.In2017IEEEInternationalConferenceonSoftwareMaintenanceandEvolution
(ICSME). 238ś248. https://doi.org/10.1109/ICSME.2017.8
[38]Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan. 2022. Manas:
Mining Software Repositories to Assist AutoML. In Proceedings of the 44th In-
ternational Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE
’22). Association for Computing Machinery, New York, NY, USA, 1368ś1380.
https://doi.org/10.1145/3510003.3510052
74523Shades of Self-AdmittedTechnical Debt: AnEmpirical StudyonMachineLearning Software ESEC/FSE ’22, November14ś18, 2022,Singapore, Singapore
[39]Rangeet Pan and Hridesh Rajan. 2020. On Decomposing a Deep Neural Network
into Modules. In Proceedings of the 28th ACM Joint Meeting on European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(Virtual Event, USA) (ESEC/FSE 2020) . Association for Computing Machinery,
NewYork, NY, USA,889ś900. https://doi.org/10.1145/3368089.3409668
[40]Rangeet Pan and Hridesh Rajan. 2022. Decomposing Convolutional Neural
Networks into Reusable and Replaceable Modules. In Proceedings of the 44th
InternationalConferenceonSoftwareEngineering (Pittsburgh,Pennsylvania) (ICSE
’22).AssociationforComputingMachinery,NewYork,NY,USA,524ś535. https:
//doi.org/10.1145/3510003.3510051
[41]JoãoFelipePimentel,LeonardoMurta,VanessaBraganholo,andJulianaFreire.
2019. ALarge-ScaleStudyAboutQualityandReproducibilityofJupyterNote-
books.In 2019IEEE/ACM16thInternationalConferenceonMiningSoftwareRepos-
itories (MSR) . 507ś517. https://doi.org/10.1109/MSR.2019.00077
[42]AniketPotdar and Emad Shihab.2014. An Exploratory Study on Self-Admitted
TechnicalDebt.In InternationalConferenceonSoftwareMaintenanceandEvolution .
IEEE Computer Society, 91ś100.
[43]XiaoxueRen,ZhenchangXing,XinXia,DavidLo,XinyuWang,andJohnGrundy.
2019. NeuralNetwork-BasedDetectionofSelf-AdmittedTechnicalDebt:From
Performance to Explainability. ACM Trans. Softw. Eng. Methodol. 28, 3, Article 15
(2019), 45pages.
[44]D. Sculley, Gary Holt, DanielGolovin, EugeneDavydov, Todd Phillips, Dietmar
Ebner, Vinay Chaudhary, and Michael Young. 2014. Machine Learning: The
High Interest Credit Card of Technical Debt. In SE4ML: Software Engineering for
MachineLearning (NIPS 2014Workshop) .
[45]D. Sculley, Gary Holt, D. Golovin, Eugene Davydov, Todd Phillips, D. Ebner,
VinayChaudhary,MichaelYoung,J.Crespo,andDanDennison.2015. Hidden
Technical Debtin MachineLearningSystems. In NIPS.
[46]C.B. Seaman. 1999. Qualitative methods in empirical studies of software en-
gineering. IEEE Transactions on Software Engineering 25, 4 (1999), 557ś572.
https://doi.org/10.1109/32.799955
[47]C.SeamanandY.Guo.2011.MeasuringandMonitoringTechnicalDebt. Advances
inComputers 82(2011), 25ś46.
[48]Giancarlo Sierra, Emad Shihab, and Yasutaka Kamei. 2019. A survey of self-
admitted technical debt. Journal ofSystemsand Software 152(2019), 70ś82.
[49]MarcelinoCamposOliveiraSilva,Marco TulioValente,andRicardoTerra.2016.
Does technical debt lead to the rejection of pull requests? arXiv preprint
arXiv:1604.01450 (2016).
[50]SebastianSztwiertnia,MaximilianGrübel,AmineChouchane,DanielSokolowski,
KrishnaNarasimhan,andMiraMezini.2021. Impact ofProgrammingLanguages
onMachineLearningBugs.In Proceedingsofthe1stACMInternationalWorkshop
onAIandSoftwareTesting/Analysis (Virtual,Denmark) (AISTA2021) .AssociationforComputingMachinery,New York, NY, USA,9ś12. https://doi.org/10.1145/
3464968.3468408
[51]Jie Tan, Daniel Feitosa, and Paris Avgeriou. 2022. Does It Matter Who Pays Back
Technical Debt? An Empirical Study of Self-Fixed TD. Inf. Softw. Technol. 143, C
(mar2022),15pages. https://doi.org/10.1016/j.infsof.2021.106738
[52]YimingTang,RaffiKhatchadourian,MehdiBagherzadeh,RhiaSingh,AjaniStew-
art,andAnitaRaja.2021. AnEmpiricalStudyofRefactoringsandTechnicalDebt
inMachineLearningSystems.In 2021IEEE/ACM43rdInternationalConference
onSoftwareEngineering(ICSE) .238ś250. https://doi.org/10.1109/ICSE43902.2021.
00033
[53]Dimitrios Tsoukalas, Miltiadis Siavvas, Marija Jankovic, Dionysios Kehagias,
Alexander Chatzigeorgiou, and Dimitrios Tzovaras. 2018. Methods and Tools for
TD Estimation and Forecasting: A State-of-the-art Survey. In 2018 International
ConferenceonIntelligentSystems(IS) .698ś705. https://doi.org/10.1109/IS.2018.
8710521
[54]Carmine Vassallo, Fiorella Zampetti, Daniele Romano, Moritz Beller, Anni-
bale Panichella, Massimiliano Di Penta, and Andy Zaidman. 2016. Continu-
ous Delivery Practices in a Large Financial Organization. In 2016 IEEE Inter-
national Conference on Software Maintenance and Evolution (ICSME) . 519ś528.
https://doi.org/10.1109/ICSME.2016.72
[55]Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2022.
DeepDiagnosis: Automatically Diagnosing Faults and Recommending Action-
able Fixes in Deep Learning Programs. In Proceedings of the 44th International
Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . As-
sociation for Computing Machinery, New York, NY, USA, 561ś572. https:
//doi.org/10.1145/3510003.3510071
[56]Mohammad Wardat, Wei Le, and Hridesh Rajan. 2021. DeepLocalize: Fault
LocalizationforDeepNeuralNetworks.In Proceedingsofthe43rdInternational
ConferenceonSoftwareEngineering (Madrid,Spain) (ICSE’21) .IEEEPress,251ś262.
https://doi.org/10.1109/ICSE43902.2021.00034
[57]Laerte Xavier, Fabio Ferreira, Rodrigo Brito, and Marco Tulio Valente. 2020.
BeyondtheCode:MiningSelf-AdmittedTechnicalDebtinIssueTrackerSystems.
InProceedingsofthe17thInternationalConferenceonMiningSoftwareRepositories
(Seoul,RepublicofKorea) (MSR’20) .AssociationforComputingMachinery,New
York, NY, USA,137ś146. https://doi.org/10.1145/3379597.3387459
[58]Fiorella Zampetti, Alexander Serebrenik, and Massimiliano Di Penta. 2018. Was
Self-AdmittedTechnicalDebtRemovalaRealRemoval?AnIn-DepthPerspective.
In2018IEEE/ACM15thInternationalConferenceonMiningSoftwareRepositories
(MSR). 526ś536.
[59]Nico Zazworka, Michele A. Shaw, Forrest Shull, and Carolyn Seaman. 2011.
Investigating the Impact of Design Debt on Software Quality. In International
Workshop onManaging TechnicalDebt . ACM,17ś23.
746