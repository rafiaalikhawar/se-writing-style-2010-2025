Less is More: Supporting Developers in
Vulnerability Detection during Code Review
Larissa Braz
larissa@ifi.uzh.ch
University of ZurichChristian Aeberhard
christian.aeberhard2@uzh.ch
University of ZurichGÃ¼l Ã‡alikli
handangul.calikli@glasgow.ac.uk
University of GlasgowAlberto Bacchelli
bacchelli@ifi.uzh.ch
University of Zurich
ABSTRACT
Reviewingsourcecodefromasecurityperspectivehasproven
to be a difficult task. Indeed, previous researchhas shown that de-
velopers often miss even popular and easy-to-detect vulnerabilities
duringcodereview.Initialevidencesuggeststhatasignificantcause
may lie in the reviewersâ€™ mental attitude and common practices.
In this study, we investigate whether and how explicitly asking
developers to focus on security during a code review affects the
detection of vulnerabilities. Furthermore, we evaluate the effectof providing a security checklist to guide the security review. To
thisaim,weconductanonlineexperimentwith150participants,
of which 71% report to have three or more years of professional
developmentexperience.Ourresultsshowthatsimplyaskingre-
viewerstofocusonsecurityduringthecodereviewincreaseseight
times the probability of vulnerability detection. The presence ofa security checklist does not significantly improve the outcomefurther, even when the checklist is tailored to the change under
review and the existing vulnerabilities in the change. These results
provideevidencesupportingthementalattitudehypothesisandcall
for further work on security checklistsâ€™ effectiveness and design.
Preprint: https://arxiv.org/abs/2202.04586Data and materials: https://doi.org/10.5281/zenodo.6026291
CCS CONCEPTS
â€¢Security and privacy â†’Software security engineering;â€¢
Software and its engineering â†’Software evolution .
KEYWORDS
code review, security vulnerability, checklist, mental attitude
ACM Reference Format:
Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli. 2022.
LessisMore:SupportingDevelopersinVulnerabilityDetectionduringCode
Review. In 44th International Conference on Software Engineering (ICSE â€™22),
May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages.
https://doi.org/10.1145/3510003.3511560
1 INTRODUCTION
Avulnerability isaâ€œflaworweaknessinasystemâ€™sdesign,imple-
mentation,oroperationandmanagementthatcouldbeexploitedto
violate the systemâ€™s security policyâ€ [ 71]. The later vulnerabilities
arediscoveredinthesoftwaredevelopmentcycle,thehigherthe
associated fixing costs are [ 56]. Therefore, to avoid vulnerabilities,
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3511560organizations are shiftingsecurity to earlier stages of softwarede-
velopment[ 2].However,securityexpertshavetomotivateandcon-
vincedevelopersoftheimportanceoffindingvulnerabilities[ 80].
Yet,wheretolocatesecuritywithinanorganizationremainsachal-
lenge [79]. For instance, a programmer working solo is likely to
createavoidablesecurityproblemsbecausetheycannaturallyhaveonlyonepointofview[
86].Asolutiontoavoidtheseissuescanbe
adopting security practices during code review.
Codereviewisawidelyagreed-onpractice[ 15]recognizedas
a valuable tool for reducing software defects and improving the
qualityofsoftwareprojects[ 3,4,10].Previousstudiesshowthat
codereviewisalsoanimportantpracticefordetectingandfixing
securitybugsearlier[ 46,81]andhaspositiveeffectsonsecuresoft-
waredevelopment[ 48,49,70].However,adoptingsecuritypractices
requires a large amount of knowledge which takes time to learn,
anditcanbehardtomotivate[ 57,83].Infact,securityissues(even
popular ones, such as Sensitive Data Exposure [65]) still often reach
production code, despite code review practices. So, how can we
better support code reviewers in detecting vulnerabilities?
In the study we present in this paper, we investigate three inter-
ventions that aim to tackle the problem by guiding the focus of the
reviewer. One isbasedon the developerâ€™s mentalattitude hypothesis
and two are based on an additional security checklists hypothesis.
Studies in the literature indicate developersâ€™ mental attitude
as a leading cause for the introduction of vulnerabilities in thecode [
50,88,91]. Specifically, vulnerabilities may be introduced
because developers do not consider security as their responsibil-
ity[50]orstronglyrelyonotherprojectmembers,processes,and
technologies [ 88]. A potential solution to resolve issues related
to developersâ€™ mental attitude is giving explicit instructions re-
garding security. Indeed, Naiakshina et al . [51,52]showed positive
effectswhenexplicitlyinstructingcomputersciencestudentsand
freelance developers to implement securepassword storage dur-
ing coding. Nevertheless, writing and reviewing code are different
activities [ 41], with even cultural differences among teams [ 10],
therefore evidence about the former may not translate to the latter.
Some preliminary yet promising evidence exists that the devel-opersâ€™ mental attitude could play a role in the context of code
reviewtoo[ 17].Usingaone-grouppretest-posttestexperimental
design[24],Brazetal . [17]foundthatasignificantnumberofde-
velopers who missed a popular vulnerability during a code review
could find it when explicitly warned about its presence.
Inthecontextofcodeinspections[ 30],theuseofcheckliststo
support developers has been extensively studied [ 44,54,58,68]
with positive results [ 26]. The OWASP foundation [ 33] proposes a
popularcodereviewguidethatcontainsasecuritychecklist[ 63].It
comprisesitemsthatguidethedevelopersinfindingsecurityissues
13172022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli
duringareview.Despitethepositiveresultsofusingchecklistsdur-
ing code inspection and the efforts by the OWASP foundation, the
effectiveness ofchecklistsin contemporarycode reviewpractices
and to support vulnerability detection has not been established yet.
In our study, we set up three interventions (treatments) that we
compare among themselves and to a review baseline (control). The
baseline(Nosecurity Instructionsâ€“NI)consistsof askingdevelop-
ers to perform a code review without giving special instructions.
The first treatment (Security Instructionsâ€“SI), based on the mental
attitudehypothesis,explicitlyinstructsdeveloperstoperformthe
reviewfromasecurityperspective.Thesecond(SecurityChecklistâ€“
SC)andthird(TailoredsecurityChecklistâ€“TC )treatmentsaddition-
allyaskdevelopers touseasecuritychecklist intheirreview.The
SCchecklistisderivedfromOWASPâ€™sCodeReviewChecklist[ 63],
while the TCone is a shorter version tailored to the change and
securityissuesathand,whichwecreatedtoremoveconfounding
factors caused by the length of the normal checklist.
We implement our study as an online experiment. A total of
150validparticipantscompletedit.Amongourparticipants,62%
(93) reported being software engineers, 71% (106) reported three
yearsormoreofprofessionaldevelopmentexperience,and65%(97)
reported performing code reviews daily.
Our results support the mental attitude hypothesis: Participants
instructed to focus on security issues were eight times more likely
todetectvulnerabilities.Ourresultsalsocallforfurtherworkon
securitychecklistsâ€™effectivenessanddesignastheydonotincrease
the detection of vulnerabilities in a security-focused review.
2 BACKGROUND AND RELATED WORK
In this section, we briefly introduce the concepts of code inspec-
tions and modern code review (the context of our study). Then, we
reviewtheliteratureonexplicitlyaskingdeveloperstousesecure
coding practices as a way to overcome issues due to the mental
attitude and on checklists for secure software development and in-
spections/reviews.Wealsoprovidebackgroundonthetwosecurity
vulnerabilities we focus on in our online experiment.
Software developers and security. Security often fails be-
cause of the lack of usability: users either misunderstand the se-curity implications of their actions or turn off security features
to workaround usability problems [ 11]. Software developers need
to design systems that are both usable and secure. Yet, they still
needsupporttocreatesecureapplicationsbeforeaddressingusable
security [79].
Existingfindingsaboutdevelopersâ€™attitudetowardssecurityare
not conclusive. On the one hand, a number of studies [ 7,57,90]
found that developers prioritize more-visible functional require-
mentsoreveneasy -to-measureactivities,suchasclosingbugtrack-
ingtickets,oversecurity.Ontheotherhand,ChristakisandBird
[21]reported that developers care more about security issues than
otherreliabilityissues.Smithetal .[75]advocatethatstaticanalysis
toolsdetectvulnerabilitiesandhelpdevelopersresolvethosevul-
nerabilities. Previous studies have proposed and improved tooling
support according to developersâ€™ needs [ 8,9,74], but tools are still
generally poorly adopted by developers [ 79] as they are confusing
for developers to use [75].
Developer-Centred Security (DCS). DCS studies have addre-
ssed some of developersâ€™ needs and attempted to apply existingmethodologies from Human Computer Interaction and to adopt
well-established usable security measures to software develop-
ment[39,55,89].ThesystematicliteraturereviewbyTahaeiand
Vaniea[79]points out the lack of research on several aspects of
DCS. For instance,they reported only one study inthe context of
code review [28].
Codeinspectionsandmoderncodereview. Peercodereview
is a manual inspection of source code by developers other than
the author. In 1976, Fagan [29]formalized a highly structured pro-
cess for code reviewing, which includes a synchronous inspection
meetingâ€“code inspections. Over the years, researchers conducted
several empirical studies on code inspections [42].
Nowadays,mostorganizationsadoptamorelightweightcode
review process to limit the inefficiencies of inspections: moderncode review [
22]. This form of review is the focus of our study.
Modern code review is asynchronous, tool- and change-based [ 12],
and widely used in practice nowadays across companies [ 10,69]
and community-driven projects [66, 67].
Contemporarycodereviewmitigatesseveralissuesofcodein-
spections but also removes the structure and checks devised tokeep the reliability of the process and its outcome. Researchers
providedstrongevidencethat,asaresultofthisdifferentapproach
to inspection, the outcome of contemporary code review is less
predictable and past theories from code inspections do not transferseamlessly[
10,12,45].Therefore,itisimportanttoconductstudies
specific to this different context.
Explicitly asking for secure coding practices. Naiakshina et
al. [51,52] conducted two studies with 40 computer science stu-
dents. Half the participants received a task description that did not
mentionsecurity,whiletheotherhalfwereexplicitlytaskedwith
implementingasecuresolution.Noneoftheparticipantsinthefirst
groupstoredpasswordssecurely;whiletwelveparticipantsinthe
group asked to create a secure solution implemented some level of
security.Moreover,Naiakshinaetal .[50]performedasimilarstudy
withfreelancedevelopersandfoundsignificantpositiveeffectof
security prompting on the secure storage of passwords.
Weiretal .[85]interviewedtwelveindustryexpertstoinvesti-
gatehowtoimprovethesecurityskillsofmobileappdevelopers.
They found that many of the most effective techniques for finding
securityissuesare dialectic,i.e.,thediscoveryofknowledgethrough
one person questioning another. Some of the techniques recom-
mended were penetration testing, code review, pair programming,
andavarietyofcodeanalysistools.Onthecontrary,Edmundson
etal.[28]statedthatmanualcodereviewcouldbeexpensiveand
impractical due to the need for several reviewers to inspect at a
pieceofcodetofindavulnerability.Later,Brazetal .[17]conducted
aone-grouppretest-posttestcodereviewexperimentwithsoftwaredevelopers.Theyfoundthatseveraldevelopersoftenmissapopularandeasy-to-detectvulnerabilitywhenreviewingcode(pretest);yet,
whenexplicitlyinformedaboutthepresenceofavulnerabilityin
the change, a significant portion of the additional developers could
identifyit(posttest).Thesestudiesprovideinitialevidencethata
specific, focused instruction on security could steer the developersâ€™
attention, thus overcoming their common mental attitude to not
consider security aspects in review.
Checklist-supportedinspections andreview. Checklistshave
been mostly investigated for code inspections. Ad Hoc Reading
1318
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Supporting Developers in Vulnerability Detection during Code Review ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(AHR)andChecklist-BasedReading(CBR)arethestandardreading
techniquesadoptedbyduringcodeinspections[ 43].Inastudywith
undergraduatestudents,OladeleandAdedayo [54]foundthatCBR
is effective in finding more issues during inspections with a 50%
decrease in false positives compared to AHR. On the other hand,
Akinola and Osofisan [5]did not find any significant differences
in the efficiency of AHR and CBR in their study conducted withstudents in a distributed environment. Similarly, the findings of
Lanubile and Visaggio [44]and Porter et al .[58]showed no signifi-
cant differences between AHR and CBR in terms of the number of
defects detected during software requirements inspections.
Studiesonchecklistsforcontemporarycodereviewarefewer.
Rongetal . [68]conductedasemi-controlledexperimentwith stu-
dents and found evidence that checklists can help in guiding them
during code reviews. In the context of education, Chong et al .[20]
found that students are able to anticipate potential defects and cre-
atearelativelygoodcodereviewchecklist,whichcanbeusedto
find defects. Finally, GonÃ§alves et al . [38]registered an experiment
reportwiththegoalofinvestigatingwhetherreviewchecklistsand
guidance improve code review performance.
Checklists in software security. Previous studies have addre-
ssedthedesign ofchecklistsforsecurity-relatedaspects,such assoftware security requirements and security life-cycle [
6,34,37].
Gilliam et al . [37]provided guidelines for creating a software se-
curity checklist, emphasizing the importance of verifying security
requirements beforehand to create a viable checklist. Garrison and
Posey[34]suggestedthatsecuritychecklistsareparticularlyuseful
in guiding non-security professionals through a security-oriented
softwaredevelopmentprocess.OWASP[ 33]statedthatorganiza-
tionswithapropercodereviewprocessintegratedintothesoftware
development life-cycle produced remarkably better code from a
security standpoint [ 63]. To help businesses strengthen their secu-
rity,suchorganizationsdevelopeddifferentsecurecodingpractices,
which are widely adopted globally [ 73]. In fact, they proposed a
code review guide containing a security checklist [63].
Cruzes et al . [25]investigated secure coding checklistsâ€™ usage.
However, the authors disagree that developers should adopt thispractice. Smarjov
[73]assessed the OWASP checklistâ€™s effective-
nessonthenumberofvulnerabilitiesreportedtotheHackerOne
platform[ 40].Theyfoundamoderateconnectionbetweenfilling
out the checklist during thedevelopment phase and thedistribu-
tionofthevulnerabilitiesreportedinHackerOne.Thelikelihood
ofHackerOneparticipantsfindinganewvulnerabilityis2.92times
higher if they do not follow the checklist during the code develop-
ment.Tobestofourknowledge,nostudyhasbeenconductedtoinvestigatetheimpactofsecuritychecklistsforsecurity-focusedcode review. We address this question by analyzing whether a
security checklist supports developers in this task.
Software Vulnerabilities. OWASP [ 33] and the Common Weak-
ness Enumeration Project (CWE) [ 1] are well-established projects
that provide guidance and best practices for organizations to avoid
security issues. Our study investigates whether security check-
listssupportdevelopersinvulnerabilitydetectionduringcodere-
views. Therefore, among the OWASPâ€™s Top 10 Web Application
Security Risks list [ 64], we focus on the Sensitive Data Exposure
(SDEâ€“ CWE 1029) [ 59] group, which can be mapped to items in
our checklist. SDEoccurrences have increased significantly overthe last few years [ 72], leading to damaging leaks. For instance,
the largest instance of SDEto date impacted 3 billion Yahoo!â€™s user
accounts, leaking their credentials (i.e., email addresses, passwords,
andsecurityquestionsandanswers).Securityexpertsnotedthat
the majority of the passwords used a strong hashing algorithm,but many used the
ğ‘€ğ·5 algorithm, which can be broken rather
quickly [87].
There are many reasons for SDE, such as the Missing Encryp-
tion of Sensitive Data (CWE-311) [ 61] and the ones covered in our
experiments, namely Generation of Error Message Containing Sensi-
tive Information (MSI- CWE 209) [ 60] andUse of a Broken or Risky
Cryptographic Algorithm (BRA- CWE 327) [ 62]. The former refers
to software that generates an error message including sensitive
information about its environment, users, or associated data; while
the latter refers to the use of a not recommended algorithm that
may allow attackers to compromise data that has been protected.
3 RESEARCH METHODOLOGY
Inthissection,weintroduceourresearchquestionsanddetail
our experimental design.
3.1 Research Questions
We structure our study around two research questions. We aim
to understand the impact of (1) instructing developers to focuson security issues and (2) providing an additional checklists on
vulnerability detection.
We formulate our first research question as follows:
RQ1.Doesexplicitlyaskingdeveloperstofocusonsecurityissues
facilitate vulnerability detection during code review?
We hypothesize that explicitly asking developers to focus on
security issues increases vulnerability detection during in codereview, because it would steer developersâ€™ mental attitude. Our
formal hypothesis for RQ1is:
H01:The presence of security instructions does not facilitate vul-
nerability detection during code review.
Evidence shows that generic checklists aid developers during
codeinspectionsandcanimprovetheinspectionsâ€™outcome[ 54,68].
Weexplorethehypothesisthatprovidingdeveloperswithasecurity
checklist(inadditiontoinstructingthemtodoasecurityreview)
further increases the vulnerability detection during code review
becauseitwouldguidethereviewtasksonrelevantsecurityaspects.
We formulate our second research question as follows:
RQ2.How does the presence of a security checklist affect vulner-
ability detection during a security-focused code review?
We organize RQ2.in two sub-questions. First, we ask:
RQ2.1.To what extent does the presence of a security checklist
affectvulnerabilitydetectionduringasecurity-focusedcode
review?
Thepresenceofitemsirrelevanttothechangemightdeteriorate
developersâ€™codereviewperformance.Brykczynski [18]suggests
that users are less likely to read through a multitude of checklistitems. To mitigate this problem, one could imagine that future
1319
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli
Security instruction
(No Instructions (NI) Security Instructions (SI)
Tailored security Checklist (TC) Security Checklist (SC)(1) welcome and 
instructions
(2) code review task
(5) feedback
on experiment
(4) knowledge and 
demographic infoRandom assignment to a treatment
Security instruction Security instruction
(3) disclosure of 
vulnerability and inquiry 
on performance
Figure 1: Steps of our online experiment.
automation techniques will be able to generate a checklist tailored
to the code change to review. We manually create such an ideal
checklist to measure its effect and ask:
RQ2.2.To what extent does the presence of a tailoredsecurity
checklist affect vulnerability detection during a security-
focused code review?
Our formal hypotheses for RQ2.1andRQ2.2are as follows:
H02.1:Thepresenceofasecuritychecklistdoesnotaffectvulnera-
bility detection during a security-focused code review compared
to providing security instructions.
H02.2:The presence of a tailoredsecurity checklist does not affect
vulnerability detection during a security-focused code review
comparedtoprovidingsecurityinstructionsoramoregeneric
checklist.
3.2 Experimental Design
Figure 1 presents the flow of our online experiment, which con-
sistsoffivesteps.Eachstepcorrespondstooneortwodifferentweb
pages, and our experimental design does not allow the participants
to return to previous pages or redo the experiment.
(1)Welcomepage: Weprovidetheparticipantswithgeneralinfor-
mation about our study. We indicate that our goal, in general, is to
improvecodereviewpracticesandthattheyaregoingtodoacodereview.Wealsopresentourdatahandlingpolicytotheparticipants
and ask for their consent to use their data.
(2) Code review task: We first ask the participants to take the
review task as seriously as possible and to assume that the code
they are going to see compiles and all tests pass. Depending on the
treatment, participants may receive further instructions:
â€“ No security Instructions (NI): Participants are provided no
additional instruction.
â€“SecurityInstructions (SI):Participantsareinformedthatweare
interestedinsecurityissuesinth)isreview.Wealsoprovidethem
with a definition of software vulnerability.
â€“SecurityChecklist (SC):Participantsreceivethesameinstruc-
tions as in SI. In addition, they are informed that they will have
accestoavulnerabilitychecklisttoassisttheminthereviewandex-plainthateachchecklistitemcanbeansweredasyes/no/irrelevant
with a specific checkbox. We ask them to work through every item
in the checklist.
â€“TailoredsecurityChecklist (TC):Participantsreceivethesame
instructions as in SCbut with a strictly tailored security checklist.Finally, we provide all participants detailed instructions on how to
add/edit/delete review comments.
When ready, the participants can press a button confirming that
theyhavereadallinstructionsandwanttostartthereview.Figure2
shows a snapshot of the code review task that the participants
assignedto SCreceivedafterpressingthe aforementionedbutton.
Whentheparticipantsaredone,theypressacompletebutton.
Wewarnparticipantsassignedto SCandTCaboutanyunresolved
checklist items, to ensure as high as possible checklist interaction.
Inaddition,weaskparticipantsofallfourtreatmentswhetherthey
were interrupted during the code review and for how long.
(3)Disclosureofvulnerabilitiesandinquiryonperformance:
After they submit their review, we inform participants that the
source code contained two vulnerabilities ( MSIandBRA) and
showtheirlocation,types,definitions,andeffects.Then,foreach
vulnerability we ask whether they found it. We ask participants to
reason on why they could or could not detect it. In addition, we
asktheparticipantsassignedtothetreatments SCandTCwhether
the checklist helped them detect the vulnerabilities. We also ask
their general feedback about the checklist, including whether and
whytheyskippedanyitems.Thisstephelpsusbetterunderstand
the impact of the security checklist on vulnerability detection and
collect qualitative data for triangulating our quantitative findings.
(4)Securityknowledgeanddemographics: Weaskaseriesof
questionsdesignedtogatherinformationaboutfactorsthatmay
affecttheparticipantsâ€™detectionofthevulnerabilities.Questions
are mainly about the participantsâ€™ security knowledge, checklistexperience, and team culture. Most questions are in Likert scale
format [84]. All questions are in the enclosed material [ 16]. Finally,
weaskaseriesofdemographicsquestionsabouttheparticipantsâ€™
gender,highesteducationlevel,employmentstatus,andyearsof
experience in professional software development, Java program-
ming,codereviewing,webapplicationdevelopment,anddatabases.
These questions are mandatory to fill in as collecting such data
helps us identify which portion of the developer population our
study participants represent [ 31]. We also ask about the frequency
withwhichtheydesigned,developed,andreviewedcodeinthelast
year; this helps us investigate possible confounding factors.
3.3 Experimental Objects
Theexperimentalobjectsconsistofthecodechangetoreview,
theinjectedvulnerabilities(MSI andBRA),andthedigital check-
listsprovidedtoparticipantsassignedtothetreatmentsSecurity
1320
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Supporting Developers in Vulnerability Detection during Code Review ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Figure 2: Example of the code review task with a security checklist using the tool.
Checklist(SC)andTailoredsecurityChecklist(TC ).Allthematerial
is available in our replication package [16].
CodeChange. ThecodechangeisimplementedinJava:beingJava
oneofthemostpopularprogramminglanguages[ 82],thisallowsus
toreachabroaderpopulationofdevelopers.Thechangecomprises
a feature implemented to manage usersâ€™ online registration to a
webservice, consistingof two classes, five methods,and 160lines.
Toavoidgivingsomeparticipantsadvantagesoverothersdueto
familiarity with the code, our code change does not belong to any
existingcodebase.Instead, weimplementedacodechange thatis
suitable for injectingboth vulnerabilities. The code changeis self-
containedandsuitablefor beingpartofanexistingsoftware ( e.g.,
notatoyexampletoteachbeginnersJavaprogramming).Finally,
thecodechangeissufficienttoconsiderpossibleattackscenarios,
thus detecting the two vulnerabilities.
Weimplementedthefirstversionofthecodechange,aftersev-
eral brainstorming sessions. Later, we interviewed two security
engineerswithfiveandtwoyearsofprofessionalsecurityexperi-
ence.Theyperformedtheexperimentwithtreatment SC.Weasked
them to read the code change before reading the checklist. Both
securityengineerspointedoutthevulnerabilities.Wealsoasked
them to provide feedback on the checklist. Finally, we conducted a
pilotstudy(Section3.5).Basedonthefeedbackwereceivedfrom
thesecurityengineersandtheparticipantsofthepilotstudy,we
iterativelymodifiedthecodechangetoensureitsrealismandre-
move any confounding implementation or design-related issues
other than the two vulnerabilities.
Security Vulnerabilities. The code change contains two vulnera-
bilities.Thefirstoneisa GenerationofErrorMessageContainingSen-
sitiveInformation (MSIâ€“CWE209)[ 60].Whenan SQLException
is raised, the query containing an unencrypted plain text password
and the userâ€™s email address is output to a log file. The error logtry {
    con = DBConnection.createConnection();    statement = con.prepareStatement("insert into user values(?,?,?,?,?,?)");        statement.setString(1, firstName);    statement.setString(2, lastName);    statement.setString(3, email);    statement.setString(4, userName);    statement.setString(5, generateHash(password));    statement.setString(6, salt);
    int result = statement.executeUpdate();    if(result!=0) {
        return "SUCCESS";    }} catch (SQLException e) {    String logMessage = "Unable to retrieve account information from database,"             + "\n query: " + statement;
}
    CWE-209: Generation of Error Message Containing Sensitive Information: The log message may contain sensitive 
information stored in the prepared statement ('statement') it includes. If an error is raised, this sensitive information is output to a log 
file which in turn can be used to simplify other attacks such as SQL Injection. Furthermore, a password salt separated from the password hash can be exploited using rainbow tables.
       ro
(a)Generation of Error Message Containing Sensitive Information (MSI)
public class PasswordManagement {
    private final static  char[] hexArray = "0123456789ABCDEF" .toCharArray();
    private final static  String HASH_ALGORITHM = "MD5";
    CWE-327: Use of a Broken or Risky Cryptographic Algorithm: The MD5 algorithm has been designed for speed and has 
been subject to several security breaches, as it became computationally possible to generate collisions, meaning different messages will lead to the same hash. Due to its fast design, MD5 is also susceptible to brute-force attacks.
 
CCC
(b)Use of a Broken or Risky Cryptographic Algorithm (BRA)
Figure 3: Object vulnerabilities used in our experiment.
willcontaintheuserâ€™ssensitiveinformation.Thesecondvulnerabil-
ity is the Use of a Broken or Risky Cryptographic Algorithm (BRAâ€“
CWE 327) [ 62]. TheMD5cryptographic hash function has been
showntobevulnerabletocollisionattacks.Differentmessagesmay
have the same MD5 hash, making forgery attacks possible [ 76].
Figure 3 shows the MSIand theBRAwe used in our study.
We based our choice of vulnerabilities on the following crite-
ria: prevalence, recognition,and discoverability using the selected
checklist. Therefore, we selected vulnerabilities from the OWASPâ€™s
â€œTop10WebApplicationSecurityRisksâ€list[ 64]andweselected
1321
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli
vulnerabilitiesthat onecould detect throughthe popularOWASP
checklist[ 63]weemployed(seemoredetailsbelow).Afterthese-
lection,weaskedthetwosecurityexpertstoperformthereview
(we did not disclose the vulnerabilities). They were able to identify
bothvulnerabilitiesandpointedoutwhichitemsinthechecklist
can help developers to detect these vulnerabilities. They further
confirmed that these vulnerabilities are known and prevalent.
SecurityChecklists. TheofficialchecklistavailableintheOWASP
Code ReviewGuideline has78 itemscovering themost criticalse-
curitycontrolsandvulnerabilityareas(including MSIandBRA).
We prepared the checklists for the SCandTCtreatments using
the OWASP checklist and improved it by applying the guidance
offered by the literature [ 18,19,36,92]. The literature suggests
that a checklist should be concise and no longer than one page for
best practice and usability [ 18,36,92]; moreover, the categories,
whichrepresentparticularfeaturesoftheapplication,shouldguide
the reviewers â€œwhere to look" in the code [ 19]. Therefore, we de-
signedthechecklistavailabletoparticipantsof SCbyreducingthe
OWASP checklist to 22 security items structured in 3 categories
and7subcategories.The SCchecklistincludesitemsrelevanttothe
application context and the programming language, including two
specificitemsrelatedtothevulnerabilitiesinthecodechange.In
addition,thischecklistcontainsitemsirrelevanttothecodechange
to represents a realistic situation in which not all the items are a
perfect fit. Moreover, to facilitate participantsâ€™ interaction with the
checklist, we phrased each checklist item in the form of a question
andprovidedadrop-downforthecorrespondinganswer.Finally,
the resulting checklist was used during our interview with twosecurity engineers (Section 3.3 â€“ Code Change). They provided
feedbackoneachitemofthechecklistandpointedouttheitemsthat disclosed the vulnerabilities in the code change. To preparethe checklist for the treatment Tailored security Checklist (TC ),
we reduced the checklist to a shorter version containing ideal-case
scenario items, i.e.,only items relevant to the code change. This
shorterchecklistthatwemadeavailableto TCparticipantscontains
seven security items structured in three categories.
3.4 Variables, Measurements, and Analyses
Table 1 presents all the variables we consider in our experi-
ment. The independent variable ( ğ‘‰ğ‘¢ğ‘™ğ‘›ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘ ) measures whether
the participants found the vulnerability. The main independent
variable is ğ‘‡ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘šğ‘’ğ‘›ğ‘¡ (NI,SI,SCorTC). We consider the other
variables as control variables, which also include the time spent on
the review, the participantâ€™s role, years of experience in java, code
review, and using checklists during code reviews. Details aboutinterruptions (
ğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  ) are collected from the participants,
andtheduration( ğ·ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¸ğ‘¥ğ‘ )ofeachreviewiscomputedfrom
the experimentâ€™s log. To answer RQ1, we build two multiple logis-
ticregressionmodels,oneforeachvulnerability(MSI andBRA)
asdependentvariable.Themodelsaresimilartotheoneusedby
Mcintosh et al. [47], Spadini et al. [77], and Braz et al. [17].
To ensure that the selected logistic regression models are ap-
propriate for the data we collect, we (i) reduced the number of
variables by removing those with Spearmanâ€™s correlation higher
than 0.5 using the VARCLUS procedure; (ii) ran a multilevel re-gression model to check whether there is a significant varianceTable 1: Variables used in the logistic regression models.
Variable Description
Dependent Variables
VulnFoundThe participant found thevulnerability in the code review
Independent Variables
TreatmentThe treatment to which theparticipant was assigned
Control Variables (Review)
InterruptionsFor how long the participant wasinterrupted during the review
DurationReview Duration of the code review
Control Variables (Security Knowledge)
Familiarity Familiarity to vulnerabilities
CoursesThe participant has participated insecurity courses and/or training
UpdateThe participant keeps themselves upto date with security information
Control Variables (Security Practice)
IncidentsThe participant has experience withsecurity incidents
ResponsibilityThe participant looks for vulnerabili-ties as a part of their job responsibility
{Designing/Coding/
Reviewing }The participant actively considers
vulnerabilities when {designingsoftware|coding|reviewing code}
Control Variables (Demographics)
LevelOfEducation Highest achieved level of education
EmploymentStatus Employment status
Role Role of the participant
OSSDev The experience in OSS development
{ProfDevExp |
JavaExp |ReviewExp |WebDevExp |DBDevExp |
ChecklistExp }Years of experience {as professional
developer | in java | in code review |in web programming | in databaseapplications | using checklistsduring reviews}
{DesignFreq |
DevFreq | CRFreq }How often they {design software |
program | review code}
among reviewers, but we found little to none, thus indicating that
a single level regression model is appropriate; and (iii) we added
theindependentvariablesintothemodelstep-by-stepandfound
that the coefficients remained stable.
Analysisofcodereviewoutcome. Togiveavaluetoourde-
pendent variables (i.e., ğ‘‰ğ‘¢ğ‘™ğ‘›ğ¹ğ‘œğ‘¢ğ‘›ğ‘‘ , whether the participants found
the vulnerabilities), we do the following: (i) the first and last au-
thorstogetherinspectasubsetoftheremarksandchecklistitems
interactionsmadebytheparticipantsduringthereviewtaskand
classify each vulnerability as detected or not; then, (ii) the first
authorclassifiestheremainingremarksandchecklistitemsinterac-
tions; the first and last authors discuss the classification, especially
unclear cases. The final decision is taken by cross-checking our
classificationwiththeanswersparticipants gavewhenindicating
whether they found the vulnerabilities (Step 3 in Figure 1).
1322
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Supporting Developers in Vulnerability Detection during Code Review ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
3
9
6
6
8
93 Software EngineerResearcher/ProfessorProject ManagerPhD StudentOtherIT Operations
Figure 4: Job distribution among employed participants.
Analysis of open answers on performance. We use open
card-sorting [ 78] to analyze the the open answers participantsâ€™
gaveontheirreviewperformance(step3,Figure1).Itallowedus
to identify factors that might affect the detection of vulnerabilities
during a review. From the open-text answers, the first and second
author separately created self-contained units, then sorted them
intothemes.Toensurethethemesâ€™integrity,theauthorsiteratively
sortedtheunitsseveraltimes.Then,bothauthors comparedand
discussed their results to reach the final themes. The discussionhelped to evaluate controversial answers, reduce bias caused by
wronginterpretationsofparticipantsâ€™comments,andstrengthen
the confidence in the outcome. We also use the card sorting output
to triangulate our results and form new hypotheses, which we
challenged with experimental data (e.g., end of Section 4.2).
3.5 Pilot Runs
We conducted 13 pilot runs to verify (1) the absence of tech-
nical errors in the experiment platform, (2) the ratio with whichparticipants were able to find the injected vulnerabilities, (3) theunderstandability of the instructions and UI, (4) the absence of
design/implementationissuesbeyondtheinjectedvulnerabilities,
and (5) the usability of the security checklist. We also gathered
qualitative feedback from the participants.
Weconductedeachrunwithadifferentparticipant.Werecruited
theparticipantsthroughourprofessionalnetworktoensurethattheywouldtakethetaskseriouslyandprovidefeedbackontheir
experience. The participantsâ€™ data and qualitative feedback during
the pilot runs werediscussed iteratively among the authors every
few pilot runs. We continued with our pilot iterations until the
required changes were minimal. No data gathered from the pilot is
considered in the final experiment.
3.6 Recruiting Participants
To recruit participants, the study authors spread the experiment
through direct contacts from their professional network as wellas their social media accounts, such as Twitter. In addition, theexperiment was spread through practitioners blogs, web forums,and open source mailing lists. The actual aim of the experiment
wasnotrevealed.Weintroducedadonation-basedincentiveof5
USD to a charity per complete and valid participant.
4 RESULTS
In this section, we report the results of our investigation.45484325
31333037
30303430
17192019
19181623
82716
Professional
DevelopmentJavaDatabasesCode Review
0% 25% 50% 75% 100%
11 years or more 6 âˆ’10 years 3âˆ’5 years 2 years 1 year or less Non eExperience
509756
443340
261339
1655
14210
ReviewingDevelopingDesigning
0% 25% 50% 75% 100%
Daily or more often About once a week About once a month About once a year Not at al lCoding Practice
Figure 5: Participantsâ€™ demographics (absolute numbers).
4.1 Validation of participants data
A total of 522 people accessed our online experiment through
the provided link. Of these, 357 did not finish all experiment steps;
thus, we removed their entries from the results dataset. We consid-
ered the 165 people who completed all steps as potentially valid.
Then,wemanuallyanalyzedcasesofparticipantswhosecodere-
viewdurationwas1.5timestheinterquartilerangeabovetheupper
quartile or below the lower quartile. We removed participants who
did not leave any remarks and did not interact with the qualitative
questionsoftheperformanceinquiry.Wealsoverifiedthecheck-
listinteractions(e.g.,iftheyskippedallitemsofthechecklist)of
participantsassignedto TCandSC.Weremoved15participants
during this step. After applying these exclusion criteria, data from
150 participants could be used for the analyses.
Thevalidparticipantswereassignedtothetreatmentsasfollows:
33 received NI, 41 received SI, 41 received TC, and 35 received SC.
Figure 4 shows the current positions of participants with part-/full-
timeemployment,andFigure5presentstheparticipantsâ€™experi-
ence and practice. Most participants currently have an engineering
role (62%), have more than two years of professional development
experience (82%), and design, program, and review code daily (37%,
65%, and 65%, respectively). 121 participants self-described as male,
7 as female, and 22 preferred not to disclose.
4.2 RQ 1. Security instructions in review
Table 2 presents the results of the review in terms of vulnerabil-
ity finding: 92 participants found at least one of the vulnerabilities;
37% (55) of the participants found the MSI, while 53% (80) found
theBRA.Amongthecontrolparticipants(NI),3%foundthe MSI
and21%foundthe BRA;amongthe117participantsinthetreat-
ments approximately 46% found the MSIand 62% the BRA. When
expressed in odds ratio, these results show that participants as-
signed to any of SI,SC, andTCare eight times more likely to find
a vulnerability than those assigned to NI(ğ‘<0.001).
Inourlogisticregressionmodels,weset NIasthereferencelevel
to bettersee the significance ofthe treatments (SI, SCandTC)i n
ourInstructions variable.Weusedthesamestartingvariablesinboth
1323
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli
Table 2: Vulnerability detection by type and treatment.
Treatment Vulnerability Found Not Found
NIâ€“ No security MSI 1 (3%) 32 (97%)
Instructions BRA 7 (21%) 26 (79%)
SIâ€“ Security MSI 19 (46%) 22 (54%)
Instructions BRA 25 (61%) 16 (39%)
SCâ€“ Security MSI 16 (46%) 19 (54%)Checklist BRA 20 (57%) 15 (43%)
TCâ€“ Tailored MSI 19 (46%) 22 (54%)
security Checklist BRA 28 (61%) 13 (39%)
TotalMSI 55 (37%) 95 (63%)
BRA 80 (53%) 70 (47%)
Table 3: Logistic regression models for RQ 1(N=150).
Dep. Var. = MSI Dep. Var. = BRA
Estim. S.E.Sig.Estim. S.E.Sig.
Intercept -6.080 2.089 ** -5.893 1.741 ***SC 3.809 1.165 ** 1.686 0.617 **
SI 4.047 1.152 *** 1.627 0.617 **
TC 3.984 1.160 *** 1.933 0.631 **
DurationCR 0.040 0.013 ** 0.017 0.011Interruptions -0.403 0.177 * -0.143 0.166Familiarity 1.575 0.827 . 0.477 0.692Incidents -0.703 0.602 0.332 0.504Practice 0.566 0.498 0.672 0.462Update -0.464 0.302 0.058 0.263Responsibility 0.226 0.234 0.068 0.201Designing -0.276 0.321 - - -
Coding 0.815 0.326 * 0.192 0.223Reviewing -0.159 0.240 0.176 0.207ProfDevExp -0.017 0.186 0.176 0.207JavaExp 0.110 0.194 0.173 0.169OftenDesign -0.195 0.248 - - -
DevFreq -0.132 0.342 0.180 0.254CRFreq 0.173 0.222 -0.106 0.198
Sig. codes: â€˜***â€™ ğ‘<0.001, â€˜**â€™ ğ‘<0.01, â€˜*â€™ ğ‘<0.05
models(seeSection3.4),butthefinalonesdifferduetoremovals
duringthemulticollinearityanalysis.Table3showstheresultsof
thelogisticregressionmodelsconsideringasdependentvariables
whether the participants found MSIandBRA, respectively. The
models confirm the result shown in Table 2: Instructing developers
to focus on security and providing checklists is significant, thus,
we can reject H01.
Finding 1.Developerswhoareinstructedtofocusonsecurityissuesduringcodereviewareeighttimesmorelikelytodetect
a vulnerability than participants who are not.
Qualitative Analysis â€“ NI participants. By analyzing the an-
swersNIparticipantsgaveonwhytheydid(not)identifythevul-
nerability, we find recurring themes. Considering the case of MSI,
onlyoneparticipantfounditandexplained:â€œIworkinabankandinmy company they send warning emails about this kind of sensitive
information being logged (like card numbers being logged etc). So
it caught my attention that what is being logged.â€ The top-three
reasonsparticipantsgavefornotdetecting MSIarethey(i)focused
on aspects unrelated to security (nine mentions), overlooked the
vulnerablecode(six),orlackedthenecessaryknowledge(four).For
instance, a participant reported: â€œTo be honest, I was optimizing
forcodestructure/readability/architectureinsteadofsecurity.Even
so, I would probably have missed that one anyways.â€
Concerning BRA, No security Instructions (NI) participants re-
ported finding it due to prior knowledge (four mentions) or theexplicit use of the
ğ‘€ğ·5 algorithm (one). For instance, a partici-
pantwrote:â€œItâ€™sveryobviousthatMD5isbeingusedtohashthe
passwordandMD5beingbrokenhasbeenknownforalongtime
now.â€ Yet, 79% of the NIparticipants did not find this vulnerability;
the top-three reported reasons are: (i) lack of knowledge and ex-
perience (ten mentions); (ii) focus on aspects unrelated to security
(five); and (iii) wrong assumptions about the code (four). Three
participants reported that detecting the vulnerability was not part
of their responsibility; as one putit: â€œI am not very familiar withcryptographyandIwouldassumetherewouldbesomeseparate
security assessment.â€
QualitativeAnalysisâ€“SIparticipants. TheSIparticipantsre-
ceived instructions to focus on security issues during the code
review.Concerning MSItheystatedtheyfounditbecause(i)the
codeinvolvesuserorsensitiveinformationthatisvaluabletohack-
ers and raises security warning (eight mentions); (ii) of previousknowledge (one); and (iii) of first-hand experience (one). For ex-ample, a stated: â€œIt contains personal information which is to be
protectedaccordingtotheGDPR.Theleakofthepasswordhash
couldbea[vulnerability]dependingonhow[theaccess]tothelogs
differs from access to the database.â€ The SIwho did not detect the
MSIexplained that they missed it because they: (i) overlooked the
vulnerability (six mentions), (ii) lacked knowledge or experience
(four), and focused on factors unrelated to security (one).
Regarding BRA, the SIparticipants mostly reported reasons
related to previous knowledge as to why they found it: (i) priorknowledge or experience (seven mentions); the vulnerability is
well-known(two); ğ‘€ğ·5canbehackedandquicklybroken(two);
and,ğ‘€ğ·5 is an old vulnerability. A participant pointed out: â€œItâ€™s
ratherwellknownthatMD5isnâ€™tsecureanymoresoIimmediately
noticed.â€™ Among the SIparticipants who did not find BRAmost
(sevenmentions)reportedthattheydidnotdetectthisvulnerability
due to a lack of knowledge or experience. In addition, a participantstatedtohavemadeawrongassumption:â€œIwasexpectingthatthis
could be configured in another part of the software.â€
We challenged these qualitative reasons using data collected in
step 5 (Figure1). We usedthe variables described inSection 3.4 to
mapthereasons.WeusedChi-Squaretestforthefirsttwovariablesof
ğ¾ğ‘›ğ‘œğ‘¤ğ‘™ğ‘’ğ‘‘ğ‘”ğ‘’ (ğ¹ğ‘ğ‘šğ‘–ğ‘™ğ‘–ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ andğ¶ğ‘œğ‘¢ğ‘Ÿğ‘ ğ‘’ğ‘ )andMann-WhitneyUtest
forğ¾ğ‘›ğ‘œğ‘¤ğ‘™ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘ˆğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ andallvariablesof ğ‘ƒğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘’.Regarding MSI,
onlyğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”wassignificantlyrelatedtodetectingthisvulnerability
(ğ‘<0.01). On the other hand, all variables of ğ‘ƒğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘’(ğ‘–ğ‘›ğ‘ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘  ,
ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ ,ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘”ğ‘›,ğ‘ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”,ğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘’ğ‘¤ğ‘–ğ‘›ğ‘” )weresignificantlyrelated
to detecting BRA(all with ğ‘<0.01). Additionally, ğ‘‘ğ‘’ğ‘ ğ‘–ğ‘”ğ‘›from the
ğ‘ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘’variables group was also significantly related ( ğ‘=0.02).
1324
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Supporting Developers in Vulnerability Detection during Code Review ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 4: Logistic regression models for RQ 2(N=117).
Dep. Var. = MSI Dep. Var. = BRA
Estim. S.E.Sig.Estim. S.E.Sig.
Intercept -1.948 1.832 -6.007 2.102 **
SC 0.244 0.557 -0.016 0.571
TC 0.167 0.546 0.288 0.569
DurationCR 0.0425 0.014 ** 0.023 0.013 .Interruptions -0.402 0.181 * -0.291 0.190Familiarity 1.636 0.843 . 0.619 0.777Incidents -0.535 0.629 0.441 0.635Practice 0.681 0.519 0.817 0.545Update -0.505 0.309 0.005 0.304Responsibility 0.186 0.241 0.078 0.244Designing -0.344 0.330 -0.033 0.324Coding 0.829 0.335 * 0.190 0.262Reviewing -0.124 0.244 0.234 0.245ProfDevExp -0.055 0.195 0.082 0.185JavaExp 0.122 0.202 0.182 0.206OftenDesign -0.130 0.251 0.022 0.254DevFreq -0.189 0.355 0.630 0.365 .CRFreq 0.147 0.230 -0.083 0.230
Sig. codes: â€˜***â€™ ğ‘<0.001, â€˜**â€™ ğ‘<0.01, â€˜*â€™ ğ‘<0.05
Finding 2.Mostdevelopersperceivethatsecurityknowledge
and practice influence their ability to detect vulnerabilities.
4.3 RQ 2. Security checklists in reviews
InRQ2weinvestigatetheeffectofprovidingasecuritycheck-
list in addition to asking developers to focus on security issues.In this analysis, we thus remove the participants of No security
Instructions(NI)andsettreatmentSecurityInstructions(SI)asthe
reference level for our logistic regression models.
Table 4 shows the results. While ğ·ğ‘¢ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘› andğ¼ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ 
are still significant for the detection of MSI, neither TCnorSC
treatments are significant. Therefore, we cannot reject H02. The
presenceofasecuritychecklistdoesnotaffectthedetectionofsoft-
warevulnerabilitiesduringcodereviewcomparedtoonlyasking
developers to look for security issues.
Finding 3. Developersreceiving achecklist (evenif tailored
to the code change) did not find more vulnerabilities than
developers only instructed to focus on security issues.
QualitativeAnalysisâ€“TCandSCparticipants. Weasked TC
andSCparticipantswhetherthechecklisthelpedthemdetecting
the vulnerabilities.
Checklist was helpful: Concerning MSI, the top-three reasons
givenbyparticipantsaboutwhythechecklisthelpedthemfindingitare:thechecklist(i)markedtheparticipantsrecheckthecode(eight
mentions),(ii)pointedoutspecificareas(seven),and(iii)primedthe participants to look for vulnerabilities (three). A participant
explained:â€œDidnotseethedatapassedintheloggeratfirst.Butafter
seeingthatiteminthechecklistIwentbacktocheckagain.â€Twoparticipantsstatedtonotneedthechecklisttofindthevulnerability
but they still reported it as helpful; one stated: â€œEven though I
found this vulnerability before looking at the checklist, it made me
specifically look for that again which is probably a good thing.â€
Regarding BRA,thetop-threereportedreasonsforthechecklist
being helpful are that the checklist: (i) helped participants to look
forvulnerabilities(fivementions),thechecklistpointedoutspecific
areas(five),andledthemsearchforfurtherinformation(external
sources)regardingthevulnerability(three).Aparticipantexplained:
â€œDid not notice which algorithm was used at first. After seeing the
checklistIrememberedtheissueaboutthemd5algorithm.â€™â€™Another
stated: â€œWhile review, I search[ed on] Google how strong MD5 is.â€
Checklist was not helpful: SomeTCandSCparticipants who
identifiedthevulnerabilities statedthatthechecklistdid nothelp.
Regardingthe MSI,thetop-fourreportedanswersare:(i)thepar-
ticipant noticed the vulnerability before using the checklist (five
mentions);(ii)theparticipantalreadylooksforthisvulnerability
(one);and(iii)thevulnerabilitywaseasytospot(one).Forinstance,
aparticipantreported:â€œIwasalreadyonthelookoutforthatkindofmistake,becauseinapastjobpartofmyresponsibilitywasmakingsurethatdidnâ€™thappen.â€Regardingthe BRA,thetop-fourreported
answers are: (i) the participant already knew about this vulnerabil-
ity(15mentions),(ii)thechecklistwasnotspecificenough(four),
(iii) theparticipant didnot payattention tothe checklist(two), and
this vulnerability requires explicit security knowledge (two).
Participantsin TCandSCwhodidnotfindthevulnerabilities
also explained why the checklist did not help. Regarding MSI, the
top-four answers are: (i) lack of attention (three mentions); (ii)theyfocusedonsomethingunrelatedtosecurity(three);and(iii)
they overlooked the vulnerability (two). For instance, a participant
reported:â€œIwasnâ€™tfocusedontheexceptionpart,henceImissed
the vulnerability.â€ Regarding the BRA, most participants reported
lack of knowledge (six mentions) as the reason the checklist did
nothelpthemdetectthevulnerability.Aparticipantexplained:â€œI
wasnâ€™t aware that MD5 was this vulnerable: I estimated during the
review that it was a good choice.â€
Finding 4.Overall,32outof76ofthedeveloperswhoreceived
asecuritychecklistperceiveditashelpful.Yet,tenreported
to be able to detect the vulnerabilities without it.
4.4 Robustness Testing
We employ robustness testing [53] to further challenge the valid-
ity of our findings.
Finding the first vulnerability distracted the participants.
Participantswhodidnotreceivesecurityinstructions(treatment:
NosecurityInstructionsâ€“NI)mighthavestoppedlookingforthe
secondvulnerabilityaftertheyfoundthefirstoneassumingthey
foundtheonlyproblem.Tochallengeourresultsagainstthishy-pothesis, we simulated what the results would have been if all
participantswhodetectedonevulnerabilityinsteaddetectedboth
(i.e.,theydidnotstopafterfindingthefirst).Inthesimulation,eight
NIparticipantsfoundbothvulnerabilities.Usingthesimulateddata,
were-runtheanalysesandcheckedthenewresults.Ourlogistic
regression models achieved the same results as for the original
1325
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli
data:thepresenceofsecurityinstructions(aswellasthesecurity
checklists) is significant to the detection of both vulnerabilities.
Additionally, ğ¶ğ‘œğ‘‘ğ‘–ğ‘›ğ‘”also remains significant. Therefore, even if
findingthefirstvulnerabilitydistractedtheparticipants,thisdid
not impact the final results.
Participantshavedifferentlevelsofexperience/practices. One
factor that may impact the participantsâ€™ performance in the review
task is their experience and practice (e.g., developers with fewer
years ofexperience might findit more challengingtoidentify the
vulnerability).Itisimportantthatthetreatmentgroupsarebalanced
in this aspect. In addition to adding experience and practice as
controlfactorsinourregressionmodels,wealsoperformedmultiple
pairwise-comparisonbetweenthemeansofthetreatmentsusing
theTukeyHSDtestandcorrectingformultipletestswiththeHolm
approach. We found no statistically significant difference.
The number participants is too low. To calculate the minimum
size sample of our experiment (i.e., number of participants with
valid responses), we performed a preliminary power analysis using
theGâ˜…Power[32],usingTwo-tailtestwithoddsratio =1.5,ğ›¼=0.05,
Power=1âˆ’ğ›½=0.95, and ğ‘…2=0.3. We used a manual distribution.
After running the analysis, we found that our experiment needed a
minimum of 143 participants. As our experiment reached 150 valid
participants (bigger than necessary), we believe that our sample
is representative. H owever, this sample size is valid only for the
first logistic regression model that we built to answer the research
question RQ1. To build the second logistic regression model for
RQ2,weexcludetheparticipantsassignedto NI.Therefore,forthis
analysis, we reduced our participantsâ€™ number to 117, which is still
a quite large sample compared to many experiments in software
engineering[ 13].However,wealsoconductedotherstatisticaltests
to verify the effect of single variables on the expected outcome
and reported the results as the sample size could have affected the
significativity of the multivariate statistics.
The vulnerabilities are too easy/hard to detect. The vulner-
abilities injected in the changes might have affected the validityof our results. Reviewers might not find a too hard to detect vul-
nerabilityandgetdiscouragedtocontinuetheexperiment,while
participants might detect a too easy vulnerability regardless of any
otherinfluencing factor,even withoutpayingtoo muchattention
to the review. We measure that 37% and 53% of the participantsfound the MSIand theBRA, respectively, suggesting that these
vulnerabilities were neither too trivial nor too difficult to find.
5 THREATS TO VALIDITY
Internal Validity. As our experiment was online, participants
conducted it in different environments (e.g., noise level and web
searches),whichcouldhaveinfluencedtheresults;yetitisexpected
that developers in real world settings also work with various tools
and environments. Interruptions could have also influenced the
results, so we asked participants about interruptions and their du-
ration and included this information in our statistical analyses. To
mitigate the possible threat from missing control over subjects,
we included some questions to characterize our sample, such as
experienceandrole(Step4inFigure1).Weconductedstatistical
teststoinvestigatehowthesefactorsaffecttheresults.Participants
wereallowedtotaketheexperimentonlyoncetopreventduplicateparticipation. We removed participantswho did not completethe
experiment and manually analyzed outliers in terms of duration.
Weacknowledgethatdevelopersâ€™background,knowledge,and
practicemayimpacttheexperimentâ€™sresults.However,wecould
only register a statistically significant (positive) effect for the vari-
ableâ€˜codingâ€˜(i.e.,â€œtheparticipantconsidersvulnerabilitieswhen
codingâ€ â€“ see table 1) when finding the MSIvulnerability (Ta-
bles 3 and 4). In their open-text responses, developers indicatedthattheyperceivetheseaspects(especiallyknowledge)toplaya
substantial role. In addition, we performed multiple pairwise com-
parisons between the means of the treatments using the Tukey
HSD test and correcting for multiple tests with the Holm approach
(section 4.4). We believe the impact of background knowledge and
practice should be investigated with further studies.
ConstructValidity. Thevulnerabilitieswerebasedontheexam-
plesoftheirCWEdescription[ 60,62]andthesecuritychecklists
were based on the OWASP code review checklist [ 63]. To mitigate
theriskthatthecodechangesandsecuritychecklistscouldhave
unanticipated characteristics that biased the results (e.g., due to
inappropriatequality),wevalidatedthemwithtwosecurityexperts
(Section 3.3) and through 13 pilot runs (Section 3.5).
The online platform showed all code on the same page, and
participants had to scroll down to proceed to the nextpage of the
online experiment.In this way,weaimed toensurethat subjects
saw the complete code change. To mitigate the fact that the online
reviewingplatform maydifferfromwhat participantsareusedto,
the experiment toolâ€™s interface is designed to be identical to the
popular review tool Gerrit [ 35]. Documentation was also added to
make the experiment closer to a real world scenario.
We used different measurement techniques to mitigate mono-
methodbias [23]:weobtainedqualitativeresultsbyemployingcard
sorting on participantsâ€™ responses to their performance inquiry
(Step 3 in Figure 1). We also used this technique on the feedback of
TCandSCparticipantsonthehelpfulnessofthesecuritychecklist.
We ran statistical analyses on variables from participantsâ€™ answers
(Step4in Figure1)andtriangulatedthequalitative andstatistical
findings. To mitigate mono-operation bias [23], we used more than
onevariabletomeasureeachconstruct(e.g., securityknowledge,
practice).Eachofvariable(seeTable1)correspondstoaquestion
in the survey on vulnerabilities (Step 4 in Figure 1). To mitigate
interaction of different treatments [23]: participants were randomly
assignedtooneofthetreatments(NI, SI,TCorSC).Totest H01(i.e.,
theeffectofasecurityinstructionsonvulnerabilitiesdetection),we
analyzed responses of all participants. To test H02.1(i.e.,the effect
of a security checklist on vulnerability detection) and H02.2(i.e.,
theeffectofatailoredsecuritychecklistonvulnerabilitydetection),
we considered responses of participants assigned to SI,TCandSC.
Weusedthequalitativedataâ€“collectedfrom SCandTC(ğ‘=76)â€“
to understand developersâ€™ perceptions of the checklistâ€™s effect. We
used the qualitative data from NIandSIparticipants ( ğ‘=74)
to get insights into their perceived reasons for (not) finding the
vulnerabilities. Therefore, qualitative answers in these two groups
(SC-TC,NI-SI) refer to two different contexts. We used this design
tolimitthenumberofquestions(especiallythosewithopen-text
answers) to prevent overloading our participants and to ensure
broader participation in the experiment.
1326
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Supporting Developers in Vulnerability Detection during Code Review ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
External Validity. To have a diverse sample of participants, we
invited software developers from several countries, organizations,
education levels, and background; yet, our sample is not repre-
sentative of all developers. Moreover, further studies should be
designedandruntoestablishthegeneralizabilityofourfindings
with different changes and vulnerabilities.
6 DISCUSSION
We investigated the effects of instructing developers to focus
onsecurityissuesandoftwosecuritychecklistsonvulnerability
detection in code review. We now discuss the main implications
and results of our study.
Mentalattitudematters. Vulnerabilitiescanleadtostrongneg-
ative consequences when they go undetected. Almost every day
the media publishes news about successful cyberattacks, showing
more and more the urgency and need for advances in the secure
softwareengineeringfield.Previousstudiesreportedmentalatti-
tude as one leading cause of why vulnerabilities are introduced in
the code [ 50,88,91] and initial evidence suggests that it may be
one of the reasons for not being detected during code review [17].
In line with previous findings [ 17,50,88], our results suggest
that developersâ€™ mental attitude plays an important role in thedetection of software vulnerabilities during code reviews. These
results strengthen the questions on the effectiveness of the current
development process, including coding and reviewing activities.Organizations may consider incorporating Secure Code Review
(SCR)intotheirdevelopmentprocesstocreateadifferentapproach.
SCRisanenhancementtothestandardcodereviewpracticewhere
thestructureofthereviewprocessplacessecurityconsiderations,
suchascompanysecuritystandards,attheforefrontofthedecision-making[
63].Furtherstudiesneedtobedesignedandcarriedoutto
determine how to better design SCR and evaluate its effectiveness.
Interestingly, our results show that security instructions are
helpful but the detection of vulnerabilities does not increase when
a security checklist is added to the code review. Thus, a less is
moreapproach can be adopted to improve the code review process:
keeping it as simple as possible with security instructions ratherthan incorporating advanced or complicated solutions, such as
securitychecklists.Studiescanbecarriedouttoinvestigatehowto
effectively address security instructions during SCR.
Security checklists are no silver bullets. Checklists are a well -
establishedreadingsupportmechanismoftenusedbyindividual
inspectorsforthepurposeofpreparation[ 27].Previousstudies[ 54,
68]foundthatChecklist-BasedReadingeffectivelyfindsmoreissues
duringcodereviewsthanAdHocReading.Brazetal .[17]suggested
security checklists as a way to improve the code review process.
Surprisingly,wefoundthattheydonotfacilitatethedetectionof
softwarevulnerabilitiescomparedtojustinstructingthedevelopers
to focus on security issues during the code review, even when the
checklist is strictly tailored to the code change.
Ourfindingsprovideinitialevidencethatreviewersmaymake
wrongassumptions,suchasassumingthatthedeveloperalreadyim-
plementedthechangeconsideringtheapplicationâ€™ssecurity.Infact,
checklistsmaynotalwaysbehelpful.Forexample,itcanremind
the developer to use hashing to protect passwords. However, in
practice, hashed passwords cannot be used for challenge-responseauthentication, negotiating a shared cryptographic session key, or
forothersuchthings[ 14].Thus,thedevelopermustdecidebetween
protecting against a stolen password file or being able to generate
asharedsessionkey.Althoughwell-intentioned,achecklistcannot
answerthisquestionand,withoutproperknowledge,developers
may take the wrong decision. In our study, developers reported
lackofknowledgeasoneofthemainreasonswhythechecklists
did not help in detecting the vulnerabilities. A security checklist
couldbeusedasareminderbutnotasasubstituteforanalysis[ 14].
Apossibleproblemofchecklistsisthattheyareoftentoogen-
eral and do not sufficiently tailored to a particular developmentenvironment [
43]. However, our results indicate that a security
checklist strictly tailored to the code change does not increase the
vulnerability detection in code reviews. This finding raises ques-
tions on whether the problem is not the security checklists but
howdevelopersperceivethem.Infact,afewdevelopersreportedtohavereadthechecklist afterthereviewortonotconsideritat
all.Furtherstudiescouldinvestigatehowtoeffectivelyintegrate
security checklist in tools and processes to support review.
7 CONCLUSION
Weinvestigatedwhetherandtowhatextentinstructingdevel-
opers to focus on security issues and providing security checklists
during code reviews can support the detection of software vulnera-bilities.Specifically,wedesignedandconductedanexperimentwith
150participants,whichincludedacodereviewtaskandasurvey.
The code change to review contained two vulnerabilities: Gener-
ationofErrorMessageContainingSensitiveInformation (MSI)and
UseofaBrokenorRiskyCryptographicAlgorithm (BRA).Thepartic-
ipants were randomly assigned to one of the following treatments:
NosecurityInstructions(NI),SecurityInstructions(SI),Security
Checklist (SC), or Tailored security Checklist (TC ). The latter two
groups received a security checklist; SIreceived instructions to
focusonsecurityissuesduringthereview;and NIreceivedneither.
In total, 92 participants found at least one of the vulnerabilities.
Thosewhoreceivedtheinstructionstofocusonsecurityissueswere
at least eight times more likely to detect vulnerabilities. Howeverâ€”
surprisinglyâ€”adding security checklists did not increase further
the vulnerabilities detection during code review compared to only
receiving security instructions.
Our findings provide evidence that developersâ€™ mental attitude
plays a role in detecting software vulnerabilities during code re-
views.Theeffectofthesecurityinstructionsprovidesevidencethatvulnerabilitydetectioncouldbetriggeredwithpropersecuritycon-
siderations, such as security standards for code reviews. Moreover,
the role and design of security checklists should be investigated
further to establish and improve their effectiveness.
ACKNOWLEDGMENTS
Theauthorswouldliketothanktheanonymousreviewersfor
theirthoughtfulandimportantcomments,whichhelpedimproving
ourpaper.Theauthorsalsoexpressgratitudetothe150validpar-
ticipantswhotookpartinthestudy,andgratefullyacknowledge
the support of the Swiss National Science Foundation through the
SNF Projects No. PP00P2_170529 and PZ00P2_186090.
1327
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Larissa Braz, Christian Aeberhard, GÃ¼l Ã‡alikli, and Alberto Bacchelli
REFERENCES
[1]2020. CWE Top 25 Most Dangerous Software Errors.
https://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html.
[2]2020. GitLab: Mapping the DevSecOps Landscape - 2020 Survey.
https://about.gitlab.com/developer-survey.
[3]A.Ackerman,L.Buchwald,andF.Lewski.1989. Softwareinspections:aneffective
verification process. IEEE Software 6, 3 (1989), 31â€“36.
[4]A. Ackerman, P. Fowler, and R. Ebenau. 1984. Software Inspections and the
IndustrialProductionofSoftware.In ProceedingsoftheSymposiumonSoftware
Validation: Inspection-Testing-Verification-Alternatives. 13â€“40.
[5]O.AkinolaandA.Osofisan.2009. AnEmpiricalComparativeStudyofCheck-
list based and Ad Hoc Code Reading Techniques in a Distributed Groupware
Environment. arXiv preprint arXiv:0909.4260 (2009).
[6]M. Alam.2010. Software security requirementschecklist. International Journal
of Software Engineering 3, 1 (2010), 53â€“62.
[7]H. Assal and S. Chiasson. [n.d.]. Security in the software development lifecycle.
InProceedings of the symposium on usable privacy and security. 281â€“296.
[8]N. Ayewah and W. Pugh. 2008. A report on a survey and study of static analysis
users. InProceedings of the workshop on Defects in large software systems. 1â€“5.
[9]N.Ayewah,W.Pugh,D.Hovemeyer,J.Morgenthaler,andJ.Penix.2008. Using
static analysis to find bugs. IEEE software 25, 5 (2008), 22â€“29.
[10]A.BacchelliandC.Bird.2013. Expectations,outcomes,andchallengesofmoderncodereview.In ProceedingsoftheInternationalConferenceonSoftwareEngineering.
712â€“721.
[11]D. Balfanz, G. Durfee, D. Smetters, and R. Grinter. 2004. In search of usable
security: Five lessons from the field. IEEE Security & Privacy 2, 5 (2004), 19â€“24.
[12]T.Baum,O.Liskin,K.Niklas,andK.Schneider.2016. Factorsinfluencingcode
reviewprocessesinindustry.In Proceedingsoftheinternationalsymposiumon
foundations of software engineering. 85â€“96.
[13]T. Baum, K. Schneider, and A. Bacchelli. 2019. Associating working memorycapacity and code change ordering with code review performance. Empirical
Software Engineering 24, 4 (2019), 1762â€“1798.
[14]S. Bellovin. 2008. Security by Checklist. IEEE Security Privacy 6, 2 (2008), 88â€“88.
[15]B.BoehmandV.Basili.2001. SoftwareDefectReductionTop10List. 34,1(2001),
135â€“137.
[16] L. Braz, G. Ã‡alikli, and A. Bacchelli. 2022. Data and Material. https://doi.org/10.
5281/zenodo.6026291.
[17]L. Braz, E. Fregnan, G. Ã‡alikli, and A. Bacchelli. 2021. Why Donâ€™t Developers
DetectImproperInputValidation?â€™;DROPTABLEPapers;â€“.In Proceedingsofthe
International Conference on Software Engineering. 499â€“511.
[18]B. Brykczynski. 1999. A survey of software inspection checklists. ACM SIGSOFT
Software Engineering Notes 24, 1 (1999), 82.
[19]Y. Chernak. 1996. A statistical approach to the inspection checklist formal
synthesisandimprovement. TransactionsonSoftwareEngineering 22,12(1996),
866â€“874.
[20]C. Chong, P. Thongtanunam, and C. Tantithamthavorn. 2021. Assessing the
Studentsâ€™ Understanding and their Mistakes in Code Review Checklists-An Ex-
perience Report of 1,791 Code Review Checklist Questions from 394 Students.
InProceedings ofthe International Conference on Software Engineering:Software
Engineering Education and Training. 20â€“29.
[21]M. Christakis and C. Bird. 2016. What developers want and need from program
analysis: an empirical study. In Proceedings of the international conference on
automated software engineering. 332â€“343.
[22]J. Cohen. 2010. Modern Code Review. In Making Software. Oâ€™Reilly, Chapter 18,
329â€“338.
[23]T. Cook and D. Campbell. 1979. Quasi-Experimentation: Design and Analysis
Issues for Field Settings. Houghton Mifflin Company.
[24]T.Cook,D.Campbell,andW.Shadish.2002. Experimentalandquasi-experimental
designs for generalized causal inference.
[25]D. Cruzes, M. Felderer, T. Oyetoyan, M. Gander, and I. Pekaric. 2017. How is
securitytestingdoneinagileteams?Across-caseanalysisoffoursoftwareteams.
InProceedings of the International Conference on Agile Software Development.
Springer, Cham, 201â€“216.
[26]A.Dunsmore,M.Roper,andM.Wood.2003. Thedevelopmentandevaluation
of three diverse techniques for object-oriented code inspection. Transactions on
Software Engineering 29, 8 (2003), 677â€“686.
[27]A.Dunsmore, M.Roper,and M.Wood. 2003. TheDevelopment andEvaluation
of Three Diverse Techniques for Object-Oriented Code Inspection. Transactions
on Software Engineering 29, 8 (2003), 677â€“686.
[28]A.Edmundson,B.Holtkamp,E.Rivera,M.Finifter,A.Mettler,andD.Wagner.
2013. Anempiricalstudyontheeffectivenessofsecuritycodereview.In Proceed-
ings of the International Symposium on Engineering Secure Software and Systems.
197â€“212.
[29]M. Fagan. 1976. Design and code inspections to reduce errors in program devel-
opment.IBM Systems Journal 15, 3 (1976), 182â€“211.
[30]M. Fagan. 2002. Design and code inspections to reduce errors in program devel-
opment. In Software pioneers. Springer, 575â€“607.[31]D. Falessi, N. Juristo, C. Wohlin, B. Turhan, J. MÃ¼nch, A. Jedlitschka, and M.Oivo. 2018. Empirical Software Engineering Experts on the Use of Students
andProfessionalsinExperiments. EmpiricalSoftwareEngineering 23,1(2018),
452â€“489.
[32]F. Faul, E. Erfelder, A. G. Lang, and A. Buchner. 2007. GPower3: A flexiblestatistical power analysis program for the social, behavioral, and biomedical
sciences. Behavior Research Methods 39 (2007), 175 â€“ 191.
[33]TheOWASPFoundation.2017. OWASPFoundation. RetrievedJuly18,2021from
https://owasp.org/
[34]C. Garrison and R. Posey. 2006. Computer security checklist for non-securitytechnologyprofessionals. JournalofInternationalTechnologyandInformation
Management 15, 3 (2006), 7.
[35] Gerrit. 2020. Gerrit Code Review. https://www.gerritcodereview.com/.[36]
T. Gilb and D. Graham. 1993. Software inspections. Addison-Wesley Reading,
Masachusetts.
[37]D.Gilliam,T.Wolfe,J.Sherif,andM.Bishop.2003. Softwaresecuritychecklistfor
the software life cycle. In Proceedings of the International Workshops on Enabling
Technologies: Infrastructure for Collaborative Enterprises. 243â€“248.
[38]P. GonÃ§alves, E. Fregnan, T. Baum, K. Schneider, and A. Bacchelli. 2020. Do
Explicit Review Strategies Improve Code Review Performance?. In Proceedings of
the International Conference on Mining Software Repositories. 606â€“610.
[39]M. Green and M. Smith. 2016. Developers are not the enemy!: The need for
usable security apis. IEEE Security & Privacy 14, 5 (2016), 40â€“46.
[40] HackerOne. 2022. HackerOne. https://www.hackerone.com/.[41]
F. Hermans. 2021. The Programmerâ€™s Brain: What every programmer needs to
know about cognition. Manning Publications.
[42]S.KollanusandJ.Koskinen.2009. Surveyofsoftwareinspectionresearch. The
Open Software Engineering Journal 3, 1 (2009).
[43]O. Laitenberger and J. DeBaud. 2000. An encompassing life cycle centric survey
of software inspection. Journal of systems and software 50, 1 (2000), 5â€“31.
[44]F. Lanubile and G. Visaggio. 2000. Evaluating defect detection techniques for
softwarerequirementsinspections. InternationalSoftwareEngineeringResearch
Network, Report (2000), 1â€“24.
[45] M. MÃ¤ntylÃ¤ and C. Lassenius. 2008. What types of defects are really discovered
in code reviews? Transactions on Software Engineering 35, 3 (2008), 430â€“448.
[46] G. McGraw. 2004. Software security. IEEE Security Privacy 2, 2 (2004), 80â€“83.
[47]S. Mcintosh, Y. Kamei, B. Adams, and A. E. Hassan. 2016. An Empirical Study
of the Impact of Modern Code Review Practices on Software Quality. Empirical
Software Engineering 21, 5 (2016), 2146â€“2189.
[48]A. Meneely and L.Williams. 2010. Strengthening the EmpiricalAnalysis of the
Relationship between Linusâ€™ Law and Software Security. In Proceedings of the
International Symposium on Empirical Software Engineering and Measurement.
1â€“10.
[49]A.MeneelyandO.Williams.2012. InteractiveChurnMetrics:Socio-Technical
Variants of Code Churn. Software Engineering Notes 37, 6 (2012), 1â€“6.
[50]A.Naiakshina,A.Danilova,E.Gerlitz,E.vonZezschwitz,andM.Smith.2019. "If
YouWant,ICanStoretheEncryptedPassword":APassword-StorageFieldStudy
with Freelance Developers. 1â€“12.
[51]A.Naiakshina,A.Danilova,C.Tiefenau,M.Herzog,S.Dechand,andM.Smith.
2017. WhyDoDevelopersGetPasswordStorageWrong?AQualitativeUsabilityStudy.In ProceedingsoftheConferenceonComputerandCommunicationsSecurity.
311â€“328.
[52]A. Naiakshina, A. Danilova, C. Tiefenau, and M. Smith. 2018. Deception Task
DesigninDeveloperPasswordStudies:ExploringaStudentSample.In Proceedings
of the Conference on Usable Privacy and Security. 297â€“313.
[53]E. Neumayer and T. PlÃ¼mper. 2017. Robustness tests for quantitative research.
Cambridge University Press.
[54]R.OladeleandH.Adedayo.2014. Onempiricalcomparisonofchecklist-based
readingandadhocreadingforcodeinspection. InternationalJournalofComputer
Applications 87, 1 (2014).
[55]O.Pieczul,S.Foley,andM.Zurko.2017. Developer-CenteredSecurityandthe
Symmetry of Ignorance. In Proceedings of the New Security Paradigms Workshop.
Association for Computing Machinery, 46â€“56.
[56]Strategic Planning. 2002. The economic impacts of inadequate infrastructure for
software testing. National Institute of Standards and Technology (2002).
[57]A.Poller,L.Kocksch,S.Trpe,F.Epp,andK.Kinder-Kurlanda.2017. Cansecurity
becomearoutine?Astudyoforganizationalchangeinanagilesoftwaredevel-
opment group. In Proceedings of the conference on computer supported cooperative
work and social computing. 2489â€“2503.
[58]A. Porter, L. G Votta, and V. Basili. 1995. Comparing detection methods forsoftware requirements inspections: A replicated experiment. Transactions on
Software Engineering 21, 6 (1995), 563â€“575.
[59]CWE Project. 2013. CWE Category - Sensitive Data Exposure. Retrieved July 19,
2021 from https://cwe.mitre.org/data/definitions/1029.html
[60]CWEProject.2021. CWE-209:GenerationofErrorMessageContainingSensitive
Information. RetrievedJune28,2021fromhttps://cwe.mitre.org/data/definitions/
209.html
1328
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Supporting Developers in Vulnerability Detection during Code Review ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
[61]CWEProject.2021. CWE-311:MissingEncryptionofSensitiveData. Retrieved
August, 2021 from https://cwe.mitre.org/data/definitions/311.html
[62]CWE Project. 2021. CWE-327: Use of a Broken or Risky Cryptographic Algorithm.
Retrieved June 29, 2021 from https://cwe.mitre.org/data/definitions/327.html
[63]OWASPProject.2017. OWASPCodeReviewGuide2.0. RetrievedJune28,2021
fromhttps://owasp.org/www-pdf-archive/OWASP_Code_Review_Guide_v2.pdf
[64]OWASP Project. 2017. OWASP Top Ten. Retrieved May 27, 2021 from https:
//owasp.org/www-project-top-ten
[65]OWASPProject.2017. SensitiveDataExposure. RetrievedJune28,2021fromhttps:
//owasp.org/www-project-top-ten/2017/A3_2017-Sensitive_Data_Exposure
[66]P.RigbyandC.Bird.2013. Convergentcontemporarysoftwarepeerreviewprac-
tices. InProceedings of the Joint Meeting on Foundations of Software Engineering.
202â€“212.
[67]P.Rigby,D.German,L.Cowen,andM.Storey.2014. Peerreviewonopen-source
software projects: Parameters, statistical models, and theory. Transactions on
Software Engineering and Methodology 23, 4 (2014), 1â€“33.
[68]G. Rong, J. Li, M. Xie, and T. Zheng. 2012. The effect of checklist in code review
for inexperienced students: An empirical study. In Proceedings of the Conference
on Software Engineering Education and Training. 120â€“124.
[69]C. Sadowski, E. SÃ¶derberg, L. Church, M. Sipko, and A. Bacchelli. 2018. Modern
codereview:acasestudyatgoogle.In ProceedingsoftheInternationalConference
on Software Engineering: Software Engineering in Practice. 181â€“190.
[70]Y.Shin,A.Meneely,L.Williams,andJ.Osborne.2011. EvaluatingComplexity,
Code Churn, and Developer Activity Metrics as Indicators of Software Vulnera-
bilities.Transactions on Software Engineering 37 (2011), 772â€“787.
[71] R. Shirey. 2000. Internet security glossary.
[72]X.Shu,D.Yao,andE.Bertino.2015. Privacy-preservingdetectionofsensitivedata exposure. Transactions on Information Forensics and Security 10, 5 (2015),
1092â€“1103.
[73]Ilja Smarjov. 2020. OWASP Secure coding practices checklist and training: As-
sessment of effectiveness in a technology company. Masterâ€™s thesis. TALLINN
UNIVERSITY OF TECHNOLOGY.
[74]J. Smith, B. Johnson, E. Murphy-Hill, B. Chu, and H. Lipford. 2015. Questionsdevelopers ask while diagnosing potential security vulnerabilities with static
analysis.In ProceedingsoftheJointMeetingonFoundationsofSoftwareEngineering.
248â€“259.
[75]J.Smith,B.Johnson,E.Murphy-Hill,B.Chu,andH.Lipford.2018.Howdevelopersdiagnosepotentialsecurityvulnerabilitieswithastaticanalysistool. Transactions
on Software Engineering 45, 9 (2018), 877â€“897.
[76]A.Sotirov,M.Stevens,J.Appelbaum,A.Lenstra,D.Molnar,D.ArneOsvik,andB.
de Weger. 2008. MD5 considered harmful today, creating a rogue CA certificate.
InProceedings of the Annual Chaos Communication Congress.
[77]D. Spadini, G. Ã‡alikli, and A. Bacchelli. 2020. Primers or Reminders? The Effects
ofExistingReviewCommentsonCodeReview.In ProceedingsoftheInternational
Conference on Software Engineering. 1171â€“1182.
[78] D. Spencer. 2009. Card sorting: Designing usable categories. Rosenfeld Media.
[79]Mohammad Tahaei and Kami Vaniea. 2019. A survey on developer-centred secu-
rity.InProceedingsoftheEuropeanSymposiumonSecurityandPrivacyWorkshops.
129â€“138.
[80]T.Thomas,M.Tabassum,B.Chu,andH.Lipford.2018.Securityduringapplication
development:Anapplicationsecurityexpertperspective.In Proceedingsofthe
2018 CHI Conference on Human Factors in Computing Systems. 1â€“12.
[81]C.ThompsonandD.Wagner.2017. ALarge-ScaleStudyofModernCodeReview
and Security in Open Source Projects. In Proceedings of the International Confer-
ence on Predictive Models and Data Analytics in Software Engineering. 83â€“92.
[82] TIOBE. 2020. TIOBE-Index. https://www.tiobe.com/tiobe-index/.[83]
S.Turpe,L.Kocksch,andA.Poller.2016. PenetrationTestsaTurningPointin
Security Practices? Organizational Challenges and Implications in a SoftwareDevelopment Team. In Proceedings of the Symposium on Usable Privacy and
Security.
[84] W. Vagias. 2006. Likert-Type Scale Response Anchors. Technical Report.
[85]C. Weir, A. Rashid, and J. Noble. 2016. How to improve the security skills of
mobile app developers? Comparing and contrasting expert views. In Proceedings
of the Symposium on Usable Privacy and Security.
[86]C. Weir, A. Rashid, and J. Noble. 2017. Iâ€™d Like to Have an Argument, Please:
Using Dialectic for Effective App Security. (2017).
[87]Wikipedia. Last accessed August 2021. Yahoo! data breaches.
https://en.wikipedia.org/wiki/Yahoo!_data_breaches#Late_2014_breach.
[88]I. Woon and A. Kankanhalli. 2007. Investigation of IS professionalsâ€™ intention
to practise secure development of applications. International Journal of Human-
Computer Studies 65, 1 (2007), 29â€“41.
[89]G.WursterandP.VanOorschot.2008. Thedeveloperistheenemy.In Proceedings
of the New Security Paradigms Workshop. 89â€“97.
[90]S. Xiao, J. Witschey, and E. Murphy-Hill. 2014. Social influences on securedevelopment tool adoption: why security tools spread. In Proceedings of the
ConferenceonComputersupportedcooperativeworkandsocialcomputing.1095â€“
1106.[91]J. Xie, H. R. Lipford, and B. Chu. 2011. Why do programmers make security
errors?.In ProceedingsoftheSymposiumonVisualLanguagesandHuman-Centric
Computing. 161â€“164.
[92] Y. Zhu. 2016. Software Reading Techniques. Springer.
1329
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. 