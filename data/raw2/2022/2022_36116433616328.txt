Dynamic Prediction of Delays in So/f_twareProjects using
DelayPa/t_terns and Bayesian Modeling
ElvanKula
DelftUniversity ofTechnology
Delft, TheNetherlands
e.kula@tudelft.nlEricGreuter
ING
Amsterdam, TheNetherlands
eric.greuter@ing.com
Arie vanDeursen
DelftUniversity ofTechnology
Delft, TheNetherlands
arie.vandeursen@tudelft.nlGeorgiosGousios
DelftUniversity ofTechnology
Delft, TheNetherlands
g.gousios@tudelft.nl
ABSTRACT
Modern agile software projects are subject to constant change,
making it essential to re-asses overall delay risk throughout the
projectlifecycle.Existingeﬀortestimationmodelsarestaticandnot
able to incorporate changes occurring during project execution. In
thispaper,weproposeadynamicmodelforcontinuouslypredicting
overall delay using delay patterns and Bayesian modeling. The
modelincorporatesthecontextoftheprojectphaseandlearnsfrom
changes in team performance over time. We apply the approach to
real-worlddatafrom4,040epicsand270teamsatING.Anempirical
evaluation of our approach and comparison to the state-of-the-art
demonstrate signi/f_icant improvements in predictive accuracy. The
dynamicmodelconsistentlyoutperformsstaticapproachesandthe
state-of-the-art,even duringearly project phases.
CCSCONCEPTS
•Software and its engineering →Software development pro-
cess management ;
KEYWORDS
agile methods, delay prediction, delay patterns, bayesian modeling
ACM ReferenceFormat:
Elvan Kula, Eric Greuter, Arie van Deursen, and Georgios Gousios. 2023.
DynamicPredictionofDelaysinSoftwareProjectsusingDelayPatternsand
Bayesian Modeling. In Proceedings of the 31st ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software Engi-
neering(ESEC/FSE’23),December3–9,2023,SanFrancisco,CA,USA. ACM,
NewYork, NY, USA, 12pages.https://doi.org/10.1145/3611643.3616328
1 INTRODUCTION
Scheduledelaysconstituteamajorprobleminthesoftwareindustry.
Softwareprojectsrun,onaverage,around30-40%overtime[ 30,56].
Ineﬀective risk management is one ofthe main reasons fordelays
insoftwareprojects[ 22,31].Animportantactivityinvolvedinrisk
ESEC/FSE ’23,December 3–9, 2023, SanFrancisco, CA, USA
©2023 Copyright held bytheowner/author(s).
ACM ISBN979-8-4007-0327-0/23/12.
https://doi.org/10.1145/3611643.3616328Time
Milestones   iPredictors at
project start GlobalPredictors at
milestone i
i + 1Global
iterative
PredictionDelay pattern
Prediction
Global
iterativePredictors at
milestone i+1PredictionPredictors at
milestone iDynamic
Milestone iPrediction
Predictors at
milestone i+1DynamicPrediction
Milestone i+1
Delay pattern
Figure 1: Global, global iterative and dynamic approaches to
delay predictionover time
management is delay prediction. Foreseeing delay risks enables
projectmanagerstotakemeasurestoassessandmanagerisks,make
timely adjustments to the planning and reduce delay propagation.
Globaleﬀortestimationmodelsarethestate-of-the-artinpredicting
overalldelayforsoftwareprojects[ 2].Globalmodelsaretrained
upfront and estimate the entire project using predictor variables
collected at the beginning of the project. These models have a
staticcharacter :theycapturetheoverallcontributionofpredictor
variablestothetotaldevelopmenteﬀortandareunawareofchanges
occurringduringproject execution.
Globalmodelsarereasonablefortraditional,waterfall-likeset-
tings where common predictors are known at the beginning of
the project and donot change much throughoutthe project.How-
ever,thisisnotthecaseformodern,agileprojects.Inagilesettings,
projects(referredtoas“epics")areincrementallydevelopedthrough
short iterations to respond fast to changing markets and customer
demands [ 19]. Predictors proposed in previous work [ 44,71], such
as user requirements and task dependencies, can vary in value and
relative impact during the execution of agile projects. Global mod-
els are not able to incorporate these changes due to their static
character. An existing alternative is to use global models in an
iterative manner (so-called global iterative ) [2]. That is, applying
the global model at diﬀerent prediction times throughout a project
using updated predictor values. This may lead to an improvement
Thiswork islicensedunderaCreativeCommonsAttribution4.0Interna-
tional License.
1012
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA ElvanKula, Eric Greuter, Arie vanDeursen, GeorgiosGousios
inpredictiveaccuracy.However,theglobaliterativemodelisstill
not able to adapt to changes occurring during project execution.
Agileprojectscallfortheneedofmodelswitha dynamiccharac-
ter:modelsthatareabletocaptureandadapttochangesinteam
performance and the impact of predictorsduring project execution.
In the /f_ield of transport, prediction of overall delay is an im-
portantrequirementforproactivecontroloftraﬃcandthefeasi-
bility of timetable realisation [ 20]. Previous research in railway
traﬃc(e.g.,[ 8,34])andairtransport(e.g.,[ 36,59])hasfoundthat
delaysdeveloporpropagatefollowingcertainpatternsovertime.
A similar pattern in historic data can provide an estimation for the
future development of delays. These studies detect patterns on the
/f_ly and use them for improving predictions of overall delay. It is
not yet known whether this concept of delay patterns is applicable
inthe contextofsoftware development.
In this paper, we propose a dynamiceﬀort estimation model for
continuouslypredictingoveralldelayinagileprojects.Asvisualized
in Figure 1, the dynamic model extends global approaches by in-
corporatingthecontextoftheprojectphase(referredtoas“project
milestone") and modeling delay patterns when making predictions.
The dynamic model is updated after each milestone using the pre-
dictorvaluescollectedforthatmilestoneandthedevelopmentof
delay up until that milestone. The model captures the milestone-
speci/f_ic contributions of predictors to the total development eﬀort
andfollowschanges inteam performance over time.
To develop our dynamic model, we use a Bayesian modeling
approach.Bayesianmodelsareabletolearnfromchangesinthe
relative impact of predictors by updating their beliefs. We train the
Bayesianmodelontimeseriesofpredictorsandintermediatedelays
recorded across the milestones of a project’s timeline. Similar to
prior work in transport, we apply time series clustering to identify
recurrentdelaypatterns.Weapplyourdynamicapproachtoreal-
world data from 4,040 epics and 270 teams at ING, a large Dutch
internationallyoperatingbankwithmorethan15,000developers.
We compare the performance of the dynamic Bayesian model with
global approaches and the state-of-the-art baselines in software
eﬀortestimation.
Anempiricalevaluationofourapproachdemonstratessigni/f_i-
cantimprovementsinpredictiveperformance,achievingonaverage
66–92% Standardized Accuracy and 0.19–0.04 Mean Absolute Er-
ror over time. The dynamic model consistently outperforms global
approachesandthestate-of-the-art,evenduringearlymilestones
(i.e.,10–30%ofprojectduration).Thepredictionsofthedynamic
modelbecome substantially more certainandaccurateover time.
The main contributionsofthis paper are:
•AnewapproachtopredictdelayusingdelaypatternsandBayesian
modeling(Section 4)
•An application of the approachatING identifyingfourrecurrent
delay patterns (Section 5)
•An empirical evaluation ofthe approach andcomparison to the
state-of-the-art,clearlydemonstratingasigni/f_icantimprovement
inpredictive accuracy(Section 6)
2 RELATED WORK
Eﬀort estimation models. Prior work has been done in building
models for estimating eﬀort of the entire project (e.g., [ 52,61,66]),asingleiteration(e.g.,[ 2,15,33])andasinglesoftwaretask(e.g.,
auserstory [ 16,46]orissuereport[ 12–14]).Existing models that
estimate the total development eﬀort are called global[2] and have
astaticcharacter.Theymakeasinglepredictionusingpredictors
collected at the start of the development phase. Global models can
be applied inan iterative mannerto obtainestimates at diﬀerent
predictiontimesthroughoutdevelopment.Choetkiertikuletal.[ 14]
demonstratedthisbyapplyingtheirmodelforpredictingdelayrisk
atthreediﬀerentpredictiontimes.Theyshowedthatthepredictions
become more accurate at later times since more information be-
comesavailable.Anotherstudy[ 12]identi/f_iedpatternsofabnormal
behaviorscausingproject delaysand usedthese patternsto predict
thedelayriskofissues.Thepatternsarederivedascombinationsof
threshold-exceedingriskfactorsthatcanleadtoscheduleoverruns.
Eﬀort drivers. Previousresearch[ 70]dividedfactors aﬀecting
thesoftwaredevelopmenteﬀortintofourcategories:personnel(i.e.,
teamskillsandexperience[ 37,51,69,73]),process(i.e.,toolsand
methods used [ 1,58]), project (i.e., project management [ 35,40])
and product characteristics (i.e., design and implementation [ 4]).
Agileteamsrelyondocumentation[ 62]andexpertjudgementof
team-andproject-relatedfactors[ 21,72]forestimationofsoftware
tasks and iterations. Kula et al. [ 44] identi/f_ied the most relevant
factors andtheirinteractions aﬀecting the eﬀortof epics.
Delay patterns in transport. Previous research in railway
traﬃc (e.g., [ 8,34]) and air transport (e.g., [ 36,59]) has shown that
delaysdeveloporpropagatefollowingrecurrentpatternsovertime.
These patterns can provide information on the future development
ofdelays.ArtanandSahin[ 8]usedMarkovchainstomodelpatterns
of delay deterioration, recovery and state keeping in train running
times. Huang et al. [ 34] used a clustering technique to identify
four types of delay patterns in train operations: decreasing delays,
unchanged delays, small increasing delays and large increasing
delays.TheybuiltaBayesianNetworkmodelthatusesthepatterns
in previous train stations to predict delay for upcoming stations.
Oreschkoetal.[ 59]detectedspeci/f_icdelaypatternsin/f_lightarrival
times with respect to the time of day and airport category. Jiang et
al.[36]usespatternsof/f_lightdelayasinputforamachinelearning-
basedapproach to delay prediction.
Whiledelaypatternshavebeenprovenusefulfordelaypredic-
tionintransport,theyremainunexploredinthecontextofsoftware
development. It is unclear whether and how delay patterns can be
employed in software projects. Our study complements prior work
by modeling delay patterns and using them as input for a dynamic
approach to predict overalldelay insoftware projects.
3 BACKGROUND
Agilesoftwaredevelopment. Inagilesettings,userrequirements
are expressed basedon Leﬃngwell’s /f_ive-level hierarchy [ 48]. The
strategic themes of a company are divided into epicsthat represent
high-levelproductrequirements[ 19].Epicsarelargepiecesoffunc-
tionalitythatcanbesplitinto features,whichinturncanbesplit
intouser stories . Stories are short requirements written from the
perspective of an end user [ 18]. Agile teams use a product backlog
to keeptrack ofthe status andpriority of thesework items [ 67].
Agile teams start with a high-level release plan (typically 2-6
months)thatcentersonepics[ 18],whichencompassmultipleteams
1013Dynamic Prediction of Delays in So/f_tware Projects usingDelay Pa/t_ternsandBayesianModeling ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
across multiple iterations. An iteration is a /f_ixed, short period of
time (typically 1-4 weeks) in which a single development team
deliversasetofuserstories.Iterationplanningfocusesonselecting
and estimating the user stories that need to be delivered in the
next iteration. Agile teams rely on expert judgement to estimate
theeﬀortofauserstoryinstorypoints[ 19,72].Theyusuallydo
this in structured group meetings (e.g., using Planning Poker [ 29]).
Attheendofeveryiteration,teamsreviewwhichuserstoriesare
completed and which ones need to be delayed to the next iteration.
Theseprogressupdatescanbeleveragedtore/f_inerelease/epicplans
duringexecution.
Epics at ING. We performed an evaluation of our dynamic
prediction approach at ING TECH, the IT department that is re-
sponsible for the main banking applications used by millions of
customers.Thedepartmenthassigni/f_icantvarietyintermsofprod-
ucts, size and application domain. Teams at ING follow Scrum [ 11]
as agile methodology and work in iterations of 1 to 4 weeks. They
usuallydeliverepicsinatimespanofthreeto12months.Devel-
opers use Planning Poker [ 29] and a /f_ixed Fibonacci sequence of
values for estimating story points. The teams estimate user stories
inworkinghoursandthenconvertthemintostorypoints.Therule
of thumb at ING TECH is that a one-point story should take about
half a day of work (4 hours). A two-point story should be twice
asmucheﬀort,thatis,onedayofwork(8hours).Thismakesthe
story pointsadditive andcomparable between teams.
Bayesiandataanalysis. Recentworks[ 26,27,68]identi/f_iedthe
potentialofBayesianstatisticaltechniquesinsoftwareengineering
research.Bayesianmodelsare/f_lexible,easytointerpretandprovide
adetailedprobabilitydistribution[ 26].Theyarebasedonauniform
frameworkthatappliesBayes’theoremtoupdatepriorbeliefsabout
modelparametersbasedonobserveddata.Bayesianmodelsconsist
ofthree components[ 53]:
•Likelihood : A function that represents the probability of observ-
ingthedatagivenasetofmodelparameters.Itre/f_lectstheunder-
lying data generation process. In the context of delay prediction,
the likelihood captures the probability of observing a speci/f_ic
delay valueorasetofdelay values.
•Priors:Probability distributions that represent the initial beliefs
orassumptionsaboutthemodelparametersbeforeobservingthe
data. Priors allow incorporating existing knowledge about the
eﬀectsofpredictors.
•Posterior: The updated probability distribution that incorporates
both the prior information and the likelihood of the observed
data.Itisobtainedbyrepeatedlysamplingvaluesfromthepriors
and applying Bayes’ theorem using the likelihood. The posterior
isusedto make predictions aboutfuture observations.
4 APPROACH
Our overall research goal is to extract delay patterns and build a
dynamicmodelthatincorporatesthepatternsandthecontextof
theprojectphaseforcontinuouslypredictingtheoveralldelayof
anepic.Thisrequiresdividinganepic’stimelineintodesignated
milestones (Section 4.1) and tracking of intermediate delay and
predictorsacrossthesemilestones.Themilestonesshouldmatch
the work pace of the organization and can be set accordingly at
/f_ixed time intervals or fractions of the planned project duration.Itisaverycommonpracticeofagileteamstorecordthedelivery
status of their work items in a backlog management tool. Backlog
data can be usedto extractintermediate delay and predictors over
milestones inthe form of time series (Section 4.2).
Toidentifydelaypatterns,thetimeseriesofdelayvaluesover
milestonesneedtobepartitionedintogroupsofsimilarelements
usingclustering(Section 4.3).HierarchicalclusteringorK-means
canbeusedtoidentifyanddiscriminatediﬀerentrecurrentpatterns.
Thedynamicpredictionmodelislearnedusingthetimeseriesdata
oftheclusteringoutputandpredictorvalues(Section 4.4.1).Forthe
Bayesian model, it is important to select the likelihood and tune
the priorsbased onthedataset beingused(Section 4.4.2).Ateach
milestone, the updated variables are fed into the model to obtain a
new, re/f_ined estimate and update the model’s beliefs. This way the
modellearns andevolves withthe epic over time.
4.1 A Uni/f_ied TimelineofProject Milestones
To incorporate the context of the project phase, we present the
timeline of an epic delivery as a sequence of regularly-spaced mile-
stones. It is important to use a uni/f_iedtimeline so that delay values
measured at the milestones can be aggregated across epics for
patternidenti/f_ication.Sinceteamsworkingonanepiccanfollow
diﬀerentiterationlengths,wecannotuseiterationsor/f_ixedtime
intervals. Instead, we de/f_ine the milestones based on completion
ratetoevenlyspacethemoutalongdeliveries.Thecompletionrate
is based on the number of iterations completed compared to the
totalnumberofiterationsplanned.Forexample,anepicthatcon-
sistsof20iterationswillachieveits10%milestoneaftercompleting
the initial two iterations. The total number of milestones used will
determinethegranularityofthecollectedtimeseriesand,therefore,
the identi/f_ied patterns. As progress updates are given at the end
of every iteration, target milestones that cover the iteration length
used(usually 2-4weeks[ 11]).
Milestones at ING. The average iteration length in our dataset
atINGis16days.Weperformedouranalysiswith10milestones,
whichbreaksmostepicsatINGdownintointervalsoftwotothree
weekswithanaveragedurationof17days.Intotal,17%oftheepics
at ING consist of less than 10 iterations; we excluded those from
ourdatasettokeeponlytheepicsthathaveatleastoneiteration
update available at every milestone (see Section 4.2.2). Each epic is
divided into 10 milestones: every milestone is scored as 10% of the
plannedduration,sowhenateamreachesthethirdmilestoneof
a task, their completion rate is equal to 30% and so on. When an
epic’stotalnumberofiterationsisnotdivisibleby10,weroundthe
milestonesoﬀtothelastcompletediterationoftheirtimeframe.For
example, when an epic consists of 18 iterations, its sixth milestone
(6
10×18=10.8)willbemeasuredattheendofthe 10/u1D461/uni210Eiteration.
The milestones are connected by the corresponding iterations and
occur ina/f_ixedsequence /u1D457→/u1D458,where/u1D458=/u1D457+1,/u1D457=1,2,...,10.
4.2 Data Collection
4.2.1 Backlog Data. To track changes in the intermediate delay
andpredictorsover time, weneed abacklog dataset thatcontains
thehistoryofepics.Foreachepic,thisdatasethastoincludethe
identi/f_icationnumber ,creationdate ,planned startdate ,actual start
date,planned delivery date andactual delivery date . At ING, we
1014ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA ElvanKula, Eric Greuter, Arie vanDeursen, GeorgiosGousios
extractedlog datafromthebacklog managementtool ServiceNow .
This dataset contained 7,463 epics delivered by 418 teams between
January 01, 2017 andJanuary 01, 2022.
4.2.2 Data Cleaning. To eliminate noise and missing values, epics
with a status other than ‘Completed’ need to be removed. Epics
thatare notassignedtoanyteam orhaveempty PlannedDelivery
DateandActual Delivery Date /f_ields also need to be /f_iltered out.
AtING,wechosetoexcludetheepicsthatconsistoflessthan10
sprintstokeeponlytheepicsthathaveatleastonesprintupdate
available at every milestone. In addition, we removed epics that
exceed two standard deviations from the mean overall schedule
delay of all epics. After linking and cleaning the data, the /f_inal ING
dataset wasreducedto 4,040 epics from 270teams.
4.2.3 Delay Factors. The predictor variables can be obtained over
milestonesbyextractingtheirvaluesattheendofthelastiteration
that corresponds to a milestone. We extracted 13 predictor vari-
ablesthatrepresentfactorsaﬀectingdelaysinepicdeliveries.We
identi/f_ied these factors in previous work [ 44]. We used the same
procedure to extract the predictor variables. Table 1provides an
overview of the predictors we collected and the in/f_luential factors
theycorrespondto.Forexample,wemodelthedelayfactor team
familiarity using the predictor variable team-existence that mea-
surestheamountoftimeteammembershaveworkedtogetherin
theircurrentcomposition.
4.2.4 MeasuringScheduleDeviation. Wemeasuretheoverallsched-
uledelayattheendofanepicusing BalancedRelativeError (BRE)[55]
aserrormeasure.BREhasbeenrecommendedasanunbiasedalter-
native to the commonly used Mean of Magnitude of Relative Error
andPrediction at level l[ 25,42,63]. BRE isde/f_inedas:
If Act- Est ≥0,then BRE =Act- Est
Plannedduration
If Act- Est <0,then BRE =Act- Est
Actual duration
where/u1D434/u1D450/u1D461is the actual delivery date and /u1D438/u1D460/u1D461is the planned
deliverydateofanepic. /u1D434/u1D450/u1D461−/u1D438/u1D460/u1D461calculatesthescheduledeviation
in days: a positive diﬀerence corresponds to a delay ( /u1D434/u1D450/u1D461is later
than/u1D438/u1D460/u1D461)andanegativevaluecorrespondstoon-timedelivery( /u1D434/u1D450/u1D461
is before /u1D438/u1D460/u1D461).Actual duration is the diﬀerence (in days) between
the actual delivery date and start date of an epic. Planned duration
isthediﬀerence(indays)betweentheplanneddeliverydateand
startdateofan epic.
To measure the intermediate delay of an epic at a given mile-
stone,weselectthelastiterationcorrespondingtothatmilestone
andcalculatethetotalnumberofstorypointsthataredelayedto
the next iteration/milestone. The total number of delayed story
pointsrepresentstheworkloadofuserstoriesateamwasunable
to complete orresolve.
4.3 Clustering forDelay Pattern Discovery
To identify delay patterns, the time series of intermediate delay
values recorded across milestones need to be clustered into groups
of similar elements. In agile settings, intermediate delay can be
Figure 2: Elbow method and WSS curve for selecting the
optimalnumberofclusters
measured based on the number of delayed user stories or story
points. We measure the number of Delayed Story Points (DSP) as it
isamorespeci/f_icmeasureofthedelayedworkload.Wecalculatethe
delayedstory pointsat agiven milestone /u1D456as the number of story
points that are delayedto the next milestone /u1D456+1. DSP represents
the delayed workload at a particular milestone and is thus not
cumulative. We normalize the DSP values per epic to make sure
that the range of story point values cannot in/f_luence the clustering
results.WedividetheDSPvaluesbythemaximumnumberofstory
pointsthat are delayedalongthe timelineof an epic.
To partition the time series data, we usehierarchical clustering
withDynamicTimeWarping (DTW)asdistancemeasure[ 57].This
approachhasbeenshowntobeappropriateforshorttimeseries[ 3].
DTWisashape-baseddistancemeasurethat/f_indsoptimalalign-
mentbetweentwotimeseriesthatdonotnecessarilymatchintime
or length. This makes DTW particularly suitable for epics that can
diﬀerindurationandsprintlength,whichisthecaseatING.We
usetheElbowmethodtoselecttheoptimalnumberofclusters k.
This method calculates the total Within Cluster Sum of Squares
(WSS) [32] for each k. The point of in/f_lection on the WSS curve
indicatestheoptimalnumberofclusters.Figure 2presentstheWSS
curve for INGdata andshowsthat a /u1D458valueof4isoptimal.
The application of our clustering approach to ING data resulted
in four patterns that are discussed in Section 5. To characterize the
clustersintermsofriskfactors,weappliedtheWilcoxonSigned
RankTest[ 7]forpairwisecomparisons.Thisisanon-parametric
testthatmakesnoassumptionsaboutunderlyingdatadistributions.
4.4 BayesianModel Development
The main goal of our prediction model is to infer a probability
distribution of BRE values across milestones. We use Bayesian
statistical analysis to infer the probabilities and build the model in
globalanddynamic modes for comparison.
4.4.1 DiﬀerentModesofModelDevelopment. Webuildandcom-
pare the Bayesianmodel usingglobal, global iterative and dynamic
modes of development. The diﬀerences between the models are
visualizedinFigure 1andcan be explainedas follows:
•Theglobalmodel solely uses the predictor variables as input
anddoesnothaveasenseoftime.Itmakesasingleprediction
oftheoveralldelaybasedonpredictorscollectedatthestartof
the project and does not update its BRE estimate throughout the
project.
1015Dynamic Prediction of Delays in So/f_tware Projects usingDelay Pa/t_ternsandBayesianModeling ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Table1: The 13 extractedpredictor variables representingfactors from[ 44]that aﬀect delays in epic deliveries. The Description
column providesadescriptionofeachvariable.
Risk factor Predictor variable Description
Taskdependencies 1.out-degree Number of outgoing dependencies of anepiconotherepics
Organizationalstability 2.changed-leads Number of changed tribeleads during the currentand previous epic
Teamstability 3.stability-ratio Median of the ratio of team members that did not change during the current and
previous epic
Skills and knowledge 4.dev-age-ing Median of the number of years the developers working on the epic have been
working at ING
Teamfamiliarity 5.team-existence Median ofthenumberofyearsteams have existedin theircurrentcomposition
of team members
Teamcommitment 6.hist-performance Median of the ratio of on-time delivered epics over all teams working on the epic
Workin progress 7.dev-workload Medianof the number of storypoints assigned to a developer per sprint
Bugs orincidents 8.nr-incidents Number of incidentsthat occurred during the developmentphaseof the epic
9.unplanned-stories Numberofunplannedstories(relatedtobug/f_ixesorincidents)thathavebeen
added to the epic
Project size 10.nr-stories Number of planned stories assigned to the epic
11.nr-sprints Number of sprintsassigned to the epic
12.team-size Medianteam size in the epic
Project security 13.security-level The ratio of userstories in the epicthat need to pass a securitytestingprocess
•Theglobaliterative modelistheglobalmodelusedinaniter-
ativemanner(i.e.,overmilestones).Weapplytheglobalmodel
ateachmilestonetoobtainanewestimateoftheoveralldelay
based on the predictor values of that milestone. The model itself
isnot updated.
•Thedynamic model is learnedusing the time series data of the
clustering output and predictor values collected over milestones.
Thismodelincorporatesthecontextofthemilestoneandthus
has a sense of time. At each milestone /u1D456, the clustering model
classi/f_iestheset ofdelayvaluesacrosspreviousmilestones 1to
/u1D456−1intooneofthefouridenti/f_iedgroupsofpatterns(producing
apatternlabel).Tomimicarealpredictionscenario,wesetthe
valuesforfuturemilestones /u1D456+1to/u1D45Btozero(unknown)inthe
inputdatafortheclusteringmodel.Ateachmilestone,thepattern
labeland updatedpredictorvariablesare fedinto the dynamic
modeltoobtainanewestimateoftheoveralldelayandupdate
the model’sposteriordistribution.
4.4.2 Bayesian Modeling. We use Bayesian regression analysis
to infer the probabilities that quantify delay risk and propagate
uncertaintyovertime.Weimplementedourmodelsinthestatistical
framework Stan1. We designed the models following the steps
andguidelinesforBayesiandataanalysisinsoftwareengineering
research[ 26,27,68]:
Step 1. Selecting a likelihood. The choice of a likelihood function
dependsonthetypeofdata.TheBREvaluesareproportionalnum-
bers between 0 and 1. In total, 42% of the BRE values in the ING
dataset are zero (corresponding to on-time delivered epics). The
data does not contain BRE values of one; the maximum BRE in
our dataset is 0.83. A common choice for modeling proportional
dataisthe Betadistributionlikelihood[ 24].Betamodelsarehighly
/f_lexible and can take on all sorts of diﬀerent shapes. To account for
the zero values in the ING dataset, we selected the Zero-In/f_lated
Beta distribution [ 60], relating predictors to outcome, as shown in
Eq.1.TheZero-In/f_latedBetadistributiondependsonamean /u1D707and
1https://mc-stan.org/precision /u1D719,likeinaregularBeta,butitmayproduceaBREofzero
withprobability /u1D6FCineachdrawfromthedistribution.Weuseda
logit function for /u1D707and/u1D6FCto translate them back to the log-odds
scaleofthe (0,1) scale. We assume thatall predictor variables may
aﬀectthe parametersofthe model(Eq. 2–4).
Step2.Settingpriors. ToapplyBayes’theorem,weneedtode/f_ine
priors for the model’s parameters. A common approach, which
works wellinmostcases,is aweakly informativeprior [ 49],such
as a normal distribution with zero mean and moderate standard
deviation,asshowninEq.5and7.Suchapriordoesnotbiasthe
eﬀect that the predictors may have towards positive or negative
values, and it still allows for a wide range of parameter values. We
set a Cauchy distribution (Eq. 6) for the /u1D6FD/u1D719parameters, which is a
common choice for dispersion parameters [ 28]. To check what the
combination of priors implies on our outcome, we sample from the
priors only.This iscalled prior predictive checks (see Figure 3a).
Theoverallde/f_initionofthedynamicmodelisgiveninEq.1–7.
BRE/u1D456∼Zero-In/f_latedBeta (/u1D707/u1D456,/u1D719/u1D456,/u1D6FC/u1D456) (1)
logit(/u1D707/u1D456)=/u1D6FD/u1D7071·out-degree +...+/u1D6FD/u1D70713·security-level
+/u1D6FD/u1D70714·milestone +/u1D6FD/u1D70715·DSP+/u1D6FD/u1D70716·pattern(2)
log(/u1D719/u1D456)=/u1D6FD/u1D7191·out-degree +...+/u1D6FD/u1D71913·security-level
+/u1D6FD/u1D71914·milestone +/u1D6FD/u1D71915·DSP+/u1D6FD/u1D71916·pattern(3)
logit(/u1D6FC/u1D456)=/u1D6FD/u1D6FC1·outdegree +...+/u1D6FD/u1D6FC13·security-level
+/u1D6FD/u1D6FC14·milestone +/u1D6FD/u1D6FC15·DSP+/u1D6FD/u1D6FC16·pattern(4)
/u1D6FD/u1D7071,...,/u1D6FD/u1D70716∼Normal(0,1) (5)
/u1D6FD/u1D7191,...,/u1D6FD/u1D71916∼Cauchy(0,1) (6)
/u1D6FD/u1D6FC1,...,/u1D6FD/u1D6FC16∼Normal(0,1) (7)
The ‘pattern’ predictor in Eq. 2–4 stands for the delay pattern
label as classi/f_ied by the clustering model. The global and global
iterative models follow the same design, except that they do not
1016ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA ElvanKula, Eric Greuter, Arie vanDeursen, GeorgiosGousios
(a)Priorpredictivecheck
 (b) Posteriorpredictivecheck
Figure3:Densityoverlaysofpredictivepriorandposterior
draws (visualized as light blue lines) versus the real data
(shown as the dark blue line). The combination of our priors
(leftplot)showsthatweassignmoreprobabilitymasstolow
and high BRE values. After making use of the data (right
plot)weseeagoodmodel/f_it:thelightbluelinesarecovering
thedarkblue line.
include the milestone andpattern label as predictors.
Step 3. Sampling. For sampling, we used the Hamiltonian Monte
CarloimplementationthatStanprovides.Toimprovetheeﬃciency
ofsampling,wecenteredandscaledallpredictorvariables.Once
themodelhasbeensampled,wecheckdiagnosticstoensurethat
wehavereachedastationaryposteriordistribution.No warnings
regarding divergent transitions and low E-BFMI values were re-
ported[9].Moreover,the ˆ/u1D445diagnosticwasconsistentlylessthan
1.01 and the ESS value was higher than 0.2. This indicates that the
Markov chains mixed well [ 75]. To check if the model /f_its the data,
we sample from the priors with data. This is called posterior predic-
tivechecks (seeFigure 3b).Asummaryofthemodelcanbefound
inthesupplementalmaterial[ 43].Onthe95%level,allpredictors
have asigni/f_icant eﬀect.
Step 4. Model checking. To check for over/f_itting, we test whether
anymodelmakingsimplerassumptionsaboutthedataperforms
comparablyorbetterthanourmodelwiththeZero-In/f_latedBeta
distribution ( /u1D440/u1D44D/u1D43C/u1D435). We compare /u1D440/u1D44D/u1D43C/u1D435with simpler models in
terms of expected log predictive density (ELPD) using leave-future-
outcross-validation[ 10,74].Themodelsareconditionedontwo
yearsofhistoricaldata(coveringtheepicsfrom2017to2019)using
therecommendedthresholdof0.7forthePareto /u1D458estimates[ 74].
The results of our analysis can be found in the supplemental mate-
rial [43]. The results show that /u1D440/u1D44D/u1D43C/u1D435performs signi/f_icantly better
thanother,simplermodelsandthus/f_itsthedatabetterwhileavoid-
ingover/f_itting.
5 DELAYPATTERNS ATING
UsingtheElbowmethod,wedetermined /u1D458=4astheoptimalnum-
ber of clusters (see Fig. 2). Therefore, we obtained four clusters
representing delay patterns in epics at ING. Figure 4visualizes the
centroids and the 25th and 75th percentiles of the cluster delay dis-
tributions. The epics are grouped together with low mean variance
(Var) around the cluster centroids (Var /u1D4361=0.07, Var/u1D4362=0.11,
Var/u1D4363=0.08,Var/u1D4364=0.12), highlightingrecurrentpatterns.
Figure4:Fourclustersofdelaypro/f_ilesrepresentingrecur-
rent delay patterns across milestones in epic deliveries at
ING: 25th percentile: dotted; centroid: solid; and 75th per-
centile:dashed.
Table 2: Characteristics of delay pro/f_ile clusters: Cluster 1
(C1), Cluster 2 (C2), Cluster 3 (C3), Cluster 4 (C4). ∗indicates
thataclusterissigni/f_icantlydiﬀerentfromallotherclusters
for the corresponding predictor variable (pairwise Wilcoxon
tests with Bonferroni correction).
PredictorMedian Signi/f_icance
C1 C2 C3 C4 C1 C2 C3 C4
nr-sprints 13 15 14 11 ∗
out-degree 7 3 4 4 ∗
hist-performance 0.69 0.67 0.74 0.61
dev-age-ing 2.49 2.61 2.92 2.84
team-existence 1.30 1.53 1.29 1.42
team-size 8 7 6 7 ∗
security-level 0.56 0.77 0.53 0.36 ∗ ∗
unplanned-stories 0.11 0.16 0.10 0.08 ∗
changed-leads 3 2 3 2 ∗
stability-ratio 0.73 0.81 0.64 0.72 ∗
nr-stories 52 43 39 45 ∗
nr-incidents 8 12 8 6 ∗
dev-workload 15 12 10 8 ∗ ∗
BRE 0.23 0.17 0.11 0.09 ∗ ∗ ∗ ∗
Characteristicsofclusters. Table2summarizesthe statistics
ofthepredictorvariablesforeachcluster’sepics.Thecon/f_idence
intervalsareincludedinthesupplementalmaterial[ 43].Weused
the Wilcoxon test for pairwise comparisons (Bonferroni corrected)
to identify the factors for which clusters are signi/f_icantly diﬀerent
from the other three clusters (highlighted with an ∗in Table2).
Thesefactorscharacterizetheepicsexhibitingoneofthefourre-
current patterns. Even though we cannot reason about causal links
between the factors and patterns, the results of the analysis en-
able us to form hypotheses on the causes of delays. Testing such
hypotheses could lead to actionable insights and suggest delay
mitigationmeasures. The clusterscan be describedas follows:
1017Dynamic Prediction of Delays in So/f_tware Projects usingDelay Pa/t_ternsandBayesianModeling ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
•Cluster1(C1)consistsof1388(36%)epics.Thesedeliveriesstart
outwith adelay peak,followed bymulti-phaserecovery,and
end with delay that continues beyond the planned delivery
date. The epics of C1 have a signi/f_icantly higher number of
outgoingdependenciesanddeveloperworkload,likelycausing
issuesat the startofthe delivery.
•Cluster2(C2)makesupthelargestgroup,containing1706(44%)
epics that are punctual up until the last few milestones. The
epics of C2 have a signi/f_icantly higher security level and team
stability,possiblyexplainingtheconsistentstart.Theyalsorun
into more incidents and unplanned work, likely causing the
delay at the end ofthe delivery.
•Cluster 3 (C3) contains 540 (14%) epics that exhibit an upward
trend (i.e., delay increase) in the /f_irst section of the delivery
followed by resilient recovery. The epics of C3 involve sig-
ni/f_icantly smaller teams, suggesting that teams with fewer
members mayneedsomebuilduptime to respondto delay.
•Cluster 4 (C4) contains 232 (6%) epics that exhibit a /f_luctuating
patternofdelayincreaseandrecoveryoverthecourseofthe
delivery.TheepicsofC4haveasigni/f_icantlyhigherstability
andlowersecuritylevel,developerworkloadandnumberof
sprints.Thesecharacteristicsmightpossiblyexplainthecon-
sistent recovery ofdelay over time.
Patterns are indicative of overall delay. The bottom row of
Table2provides the descriptive statistics of the overall delay, mea-
suredintermsofBREvalues,foreachcluster.Theepicsassigned
toCluster1suﬀerthelargestoveralldelaywithamedianBREof
0.23.TheepicsinCluster2areassociatedwiththesecondlargest
overalldelay(medianBREof0.17).Clusters3and4consistofepics
that end up with small overall delays with a median BRE of 0.11
and 0.09, respectively. Using the the Wilcoxon test for pairwise
comparisons, we found that the diﬀerences in the BRE values of
the clusters are statistically signi/f_icant at a con/f_idence level of 95%.
Thismeansthatthepatternsareindicativeoftheoverallepicdelay.
6 EVALUATION
6.1 Research Questions
The empirical evaluation ofthe dynamic model aimedto answer
the following researchquestions:
–RQ1. Bene/f_its of dynamic prediction: Does the dynamic
modelprovidemoreaccurateestimatesthanitsglobalandglobal
iterativemodes? Tostudythebene/f_itsoftheproposeddynamic
model, we evaluate the performance of the Bayesian model in
global,globaliterative anddynamicsettings.
–RQ2.Bene/f_itsofdelaypatterns: Doestheuseofdelaypatterns
haveapositiveimpactonthepredictiveperformance? Wecom-
pare the performance of the dynamic Bayesian model learned
withandwithoutthe delay patterns.
–RQ3.ComparisonwithSoTAbaselines: Howdoesourdy-
namic Bayesian model compare to the state-of-the-art baselines?
To determine whether our dynamic Bayesian model improves
the state-of-the-art (SoTA) baselines in eﬀort estimation, wecompareitwiththeDecisionTreemodelofChoetkiertikulet
al. [12] and the Random Forests model of Choetkiertikul et
al.[14].Weperformthecomparisonwiththemodelsintheir
original,global modeusing features from Choetkiertikul et al.
andindynamic mode using our setof features.
–RQ4. Impact of prediction time: How does the moment of
prediction aﬀect the informativeness of the predictions of the
dynamicmodel? Previouswork[ 38,39]hasshownthatstatisti-
calmodels shouldbe evaluatedin termsof bothaccuracyand
informativeness (i.e., width of the prediction interval). We ana-
lyzehowtheinformativenessofthepredictionsofthedynamic
model evolves with the time of prediction (early versus late in
the epic).
6.2 SoTA Baselines
We implemented two models representing the SoTA baselines in
theiroriginal,globalmodeanddynamicmodeforcomparisonwith
ourdynamicBayesianmodel.Forcomparisoninglobalmode,we
implemented the global Decision Tree model of Choetkiertikul
et al. [12] using the /f_ive issue-level features presented in the pa-
per.Wemappedthefeaturestotheepic-levelandextractedthem
from ING data. An overview of all variables and their mapping
to the epic-level can be found in the supplemental material [ 43].
We also implemented the global iterative Random Forests model
of Choetkiertikul et al. [ 14] using 16 out of 19 features from the
paper. We were not able to extract the variables ‘number of /f_ix
versions’, ‘changing of /f_ix versions’ and ‘number of aﬀect versions’
astheyarespeci/f_ictothecontextofissuereports.Weconvertedthe
featurestotheepic-level,asdescribedinthesupplementalmaterial.
For comparison in dynamic mode, we implemented both models
ofChoetkiertikuletal.followingthedynamicsetupdescribedin
Section4.4.1. The models were learned using our features from
Table1andthe delay patterns.
6.3 ExperimentalSetup
Weperformedexperimentsonthe4,040epicsintheINGdataset.To
mimic a real prediction scenario, in which observed epics are used
toinformpredictionsforfutureepics,wesortedtheepicsandtheir
milestonesbasedontheirstartdate.Fortrainingandevaluation,we
usedtime-based10-foldcross-validation.Thetime-basedvariant
of cross-validation ensures that in the k-th split, the epics in the
/f_irstkfolds(trainingset)arecreatedbeforetheepicsinthe(k+1)th
fold (test set). The successive training sets are thus supersets of
previousones.Thisallowsforthesequentialupdatingofmodels
basedonpastknowledge.
TheBayesian model estimatesaprobability distribution of BRE
values. For evaluation, we selected the median of the posterior dis-
tributionasthepredictedBREvalue.Thisisacommonapproach
whenthegoalofthemodelistominimizetheabsoluteorrelativees-
timationerror[ 38].FortheSoTAbaselines,weappliedtheDecision
Tree andRandom Forestsregressors to obtain aBRE estimate.
6.4 Performance Measures
We usedthe MeanAbsolute Error (MAE)and the StandardizedAc-
curacy(SA) as error measures; both have been recommended to
comparetheperformanceofeﬀortestimationmodels[ 47,66].MAE
1018ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA ElvanKula, Eric Greuter, Arie vanDeursen, GeorgiosGousios
(a)StandardizedAccuracy over time
 (b) Mean Absolute Error over time
Figure 5: Evaluation results obtained by the global, global iterative and dynamic Bayesian models over milestones (RQ1);
dynamic with andwithout delaypatterns(RQ2).
isde/f_inedas:
/u1D440/u1D434/u1D438=1
/u1D441/u1D441/summationdisplay.1
/u1D456=1|/u1D434/u1D450/u1D461/u1D462/u1D44E/u1D459 /u1D435/u1D445/u1D438 /u1D456−/u1D438/u1D460/u1D461/u1D456/u1D45A/u1D44E/u1D461/u1D452/u1D451 /u1D435/u1D445/u1D438 /u1D456|
where/u1D441isthenumberofepicsusedforevaluation, /u1D434/u1D450/u1D461/u1D462/u1D44E/u1D459 /u1D435/u1D445/u1D438 /u1D456
is the actual delay measured in BRE, and /u1D438/u1D460/u1D461/u1D456/u1D45A/u1D44E/u1D461/u1D452/u1D451 /u1D435/u1D445/u1D438 /u1D456is the
predictedBREvalue,foranepic /u1D456.SAisbasedonMAEandcompares
an eﬀortestimationmodelagainst random guessing:
/u1D446/u1D434=/parenleftbigg
1−/u1D440/u1D434/u1D438
/u1D440/u1D434/u1D438/u1D45F/u1D454/parenrightbigg
×100
where/u1D440/u1D434/u1D438is de/f_ined as the MAE of the model that is being
evaluated and /u1D440/u1D434/u1D438/u1D45F/u1D454is the MAE of a large number of random
guesses.SArepresentshowmuchbetter the modelperformsthan
randomguessing.Weusedtheunbiasedexactcalculationof /u1D440/u1D434/u1D438/u1D45F/u1D454
as proposed by Langdon et al. [ 47]. A lower /u1D440/u1D434/u1D438and higher /u1D446/u1D434
imply betterpredictive performance.
ToevaluateinformativenessofthepredictionsoftheBayesian
model(RQ4),wemeasuredtherelativewidth( /u1D445/u1D44A/u1D456/u1D451/u1D461/uni210E 90)ofthe90%
credible intervals[ 39].A narrower interval(i.e. lower /u1D445/u1D44A/u1D456/u1D451/u1D461/uni210E 90)
ismore informative.
Tocomparemodel performance,wetestedthe statisticalsignif-
icance of the evaluation results using the Wilcoxon Signed Rank
Test [7]. We applied the non-parametric Vargha and Delaney’s ˆ/u1D43412
statistic[7],whichiscommonlyusedaseﬀectsizemeasureineﬀort
estimation[ 66].
6.5 Results
RQ1:Bene/f_itsofdynamicprediction. Figure5presentstheeval-
uation results of the global, global iterative and dynamic modes of
theBayesianmodelforpredictingtheoveralldelay(inBRE)over
milestones.Averagingacrossepics,thedynamicmodeachieves66–
92%SAand0.19–0.04MAEovermilestones.Overtime,thedynamic
modeconsistentlyoutperformstheglobalmodeby12–57%(SA)and
16–81%(MAE),andtheglobaliterativemodeby12–44%(SA)and
16–78%(MAE).TheWilcoxontestshowsthattheimprovements
achieved by the dynamic mode are signi/f_icant ( /u1D45D<0.001) with
medium to large eﬀect sizes ( ˆ/u1D43412=[0.65,0.81]).This indicates that
the dynamic mode signi/f_icantly improves global and global iterative
modesrightfrom the/f_irst milestone on.RQ2: Bene/f_its of delay patterns. The dashed lines in Fig-
ure5presenttheevaluationresultsofthedynamicBayesianmodel
learned with and without delay patterns as input feature. At the
/f_irsttwomilestones,thedynamicmodellearnedusingpatternspro-
videsthesameestimationsasthedynamicmodellearnedwithout
patterns. This is caused by the fact that the pattern clustering label
becomes available from the third milestone on (i.e., when there
is a series of two or more previous milestones to classify). Then,
from the third milestone on, the dynamic model learned using pat-
ternsconsistentlyimprovesthedynamicmodelwithoutpatterns
by 9–20% (SA) and 19–66% (MAE). The improvements achieved
by using delay patterns are signi/f_icant with medium eﬀect size
(ˆ/u1D43412=[0.64,0.69]).Thisindicatesthattheuseofdelaypatternsleads
to signi/f_icant improvementsin predictiveperformance,fromthe third
milestone on.
RQ3:ComparisonwithSoTAbaselines. Figure6presentsthe
resultsofourdynamicBayesianmodelcomparedtotheSoTAbase-
lines,representedbytheDecisionTree[ 12]andRandomForests[ 14]
Figure 6: Comparison of our dynamicBayesian modelwith
SoTA baselines in global and dynamicmodes (RQ3). ‘Global
DT’and‘GlobalIterativeRF’aretheglobalDecisionTree[ 12]
andglobaliterativeRandomForests[ 14]learnedusingfea-
tures from related work. ‘Dynamic DT’ and ‘Dynamic RF’
arethedynamicDecisionTreeanddynamicRandomForests
learned usingour featuresfromTable 1.
1019Dynamic Prediction of Delays in So/f_tware Projects usingDelay Pa/t_ternsandBayesianModeling ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
models, in global and dynamic modes. The solid lines show the re-
sultsoftheDecisionTreeandRandomForestsmodelsintheirorig-
inal, global mode using features from Choetkiertikul et al. [ 12,14].
The dashed lines show theresults ofthe models indynamic mode
using our features from Table 1.
ThedynamicBayesianmodelconsistentlyoutperformstheSoTA
baselinesinbothglobalanddynamicmodes.Overtime,dynamic
BayesianimprovestheglobalDecisionTreeby74–144%(SA)and
44–87% (MAE), and the global iterative Random Forests by 56–71%
(SA) and 34–84% (MAE). The Wilcoxon test shows that the im-
provementsachievedbydynamicBayesianovertheglobalSoTA
baselines are signi/f_icant with large eﬀect size ( ˆ/u1D43412>0.84). Dy-
namic Bayesian also outperforms the dynamic Decision Tree by
22-48% (SA)and26–80%(MAE), andthedynamic Random Forests
by 4–20% (SA) and 7–68% (MAE) over milestones. The improve-
ments of dynamic Bayesian over the dynamic Decision Tree and
RandomForestsaresigni/f_icantwitheﬀectsizesgreaterthan0.58.
This indicates that the dynamic Bayesian model achieves signi/f_icant
improvementsover theSoTA baselines.
Overall,themodelsindynamic modesubstantiallyoutperform
theircounterpartsinglobalmode.Thishighlightsthebene/f_itsof
dynamicpredictionsacrossmodels. Bayesianachievesthehighest
predictiveaccuracyandthelargestoverallincreaseinperformance
compared to theSoTA baselines.
RQ4: Impact of prediction time. Figure7shows how the
estimated BRE distributions of the dynamic Bayesian model evolve
over milestones 2, 5, 7 and 10. The prediction intervals become
more narrow and sharp over time. The average /u1D445/u1D44A/u1D456/u1D451/u1D461/uni210E 90of the
prediction intervals decreases from 1.14 at milestone 2 to 1.01 at
milestone 5, 0.94 at milestone 7, and 0.89 at milestone 10. The
Wilcoxontestshowsthatthechangesin /u1D445/u1D44A/u1D456/u1D451/u1D461/uni210E 90overtimeare
signi/f_icant ( /u1D45D<0.001) and the eﬀect sizes are small to medium
(ˆ/u1D43412=[0.59,0.68]).This indicates that the dynamic Bayesian model
isconvergent,i.e. thepredictionsof themodelbecome more certain
and informativeover time.
7 DISCUSSION
7.1 MainFindings
Delaypatternsasinputfeature. Wefoundthatthepatternsiden-
ti/f_iedatthecasecompanyareindicativeoftheoverallprojectdelay.
This means that a similar pattern in historical data can provide
an estimation for the future development of delay in an ongoing
project.Thepatternshaveshowntheirvalueintransport,andnow
insoftwaredevelopmentaswell.Theycanbeusefulasinputfeature
fordelaypredictionandreschedulingdecisions.Ourresultsdemon-
strate that the use of patterns leads to signi/f_icant improvements
of9–20%(SA)and19–66%(MAE)inthepredictionsofdelay.The
patterns in other organizations might diﬀer from the four patterns
identi/f_ied at the case company. We expect that the number and
shapeofpatternswilldependonthedatasetbeingused.Thepat-
ternsareessentiallyare/f_lectionofrecurringproblemsorabnormal
behaviors that leadto delay inorganizations.
Relationshipswithriskfactors. Wecharacterizedthepatterns
intermsofriskfactors,asshowninTable 2.Ourstatisticalanalysis
revealsthatthepatternsshowsigni/f_icantdiﬀerencesinvariousrisk
Figure 7: The estimated BRE distributions as updated by the
dynamic Bayesian modelacross milestones (RQ4)
factors. Even though we cannot reason about causal links between
the factors and patterns, the results of our factor analysis enable
us to form hypotheses on the causes of delays. For example, the
epicsinCluster1haveasigni/f_icantlyhighernumberofoutgoing
dependencies,largerdeliveryscopeandhigherdeveloperworkload.
We therefore hypothesize that large epics with many dependencies
and overloaded developers are likely to exhibit a pattern similar
to that of Cluster 1 and lead to major overall delay. Testing such
hypotheses could lead to actionable insights and suggest delay
mitigation measures. For a comprehensive view, we recommend
the use of both epic- and story-level risk factors to characterize
thepatterns.Epic-levelriskscanprovidehigh-levelinsightsinto
problemsrelatedtotheenvironmentthatthedeliverytakesplace
in. Story-level risks can give lower-level insights into problematic
softwaretasksandcollaborationchallenges(e.g.,userstoriesthat
have an abnormal waiting time are an indication of lack of team
cooperation [ 12]).
Bene/f_its of dynamic prediction. Our results show that dy-
namicmodelssigni/f_icantlyoutperformtheirglobalandglobalit-
erative counterparts. The dynamic Bayesian model achieves im-
provementsofatleast12–44%(SA)and16–78% (MAE)rightfrom
the/f_irstmilestoneon.ItalsosubstantiallyoutperformstheSoTA
baselines. This highlights the bene/f_its of dynamic prediction meth-
ods and indicates that existing, static methods are less suited to
predict long-term delay. Existing models are not able to adequately
incorporatechangesoccurringduring projectexecution.Dynamic
methodscaneﬀectivelyincorporatedynamicphenomena,resulting
in increasingly more accurate and reliable schedule estimates over
time. Dynamic prediction can therefore help teams detect risks
throughout the project life cycle and react to delays in a more pru-
dent fashion. This is especially valuable in development settings
thataresubjecttoconstantchangeandwherescheduleoverruns
are acriticalfactor.
1020ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA ElvanKula, Eric Greuter, Arie vanDeursen, GeorgiosGousios
Trade-oﬀbetweenpredictiontimeandaccuracy. Ourevalua-
tionresultsshowthatthepredictionsofthedynamicmodelbecome
more accurate and informative over time. We acknowledge that
predicting at later times (at 70—100% of the planned duration) may
belessusefulasitmightbetoolatetochangetheoutcome.How-
ever,theincreasedcertaintymayjustifymitigationactionsfocused
onhandlingacertaindelay(e.g.,postponeproductlaunch,move
featurestootherepic)insteadoftryingtocatchup(byaddingmore
resources). Furthermore, the dynamic approach achieves meaning-
fulimprovementsrightfromthestartoftheprojecton.Itimproves
the global and global iterative approaches by 12% (SA) and 16%
(MAE)at10%duration,19%(SA)and27%(MAE)at20%duration
and29%(SA)and41%(MAE)at30%duration.Theimprovements
addupto34%(SA)and37%(MAE)at50%duration.Theimprove-
mentsobtainedduringthe/f_irsthalfoftheprojectarenotableand
can enable teamsto take early measures against delay.
Bene/f_itsofBayesianmethods. InourcomparisonoftheSoTA
baselines in dynamic mode, Bayesian performs better than the
Decision Tree and Random Forests models. Bayesian also achieves
the largest overall increase in accuracy over time. This suggests
that Bayesian is more eﬀective in quantifying and updating the
uncertainty of predictions over time. The results of RQ4 con/f_irm
this observation: the predictions of the Bayesian model become
substantially more certain and informative over time. Unlike the
other models, Bayesian provides detailed information about the
uncertainty of an estimate in the form of a probability distribution.
This can helporganizationsraisecon/f_idence inprojectplans.
7.2 FutureWork
Causal inference. To improve the implementation of delay coun-
termeasures, there is a need to better understand the causes of
delays and delay patterns. An interesting direction for future re-
search is to investigate why risk factors and delay patterns are
related. This could be assessed by causal inference on individual
patterns.Causaldiscovery(e.g.,[ 65])couldbeusedtolearnacausal
graph from the time series and identify the underlying causes of
trends or /f_luctuations in the patterns. This can help software or-
ganizations to identify the causes of speci/f_ic delays and estimate
theeﬀectsofcorrectiveactionsbeforehand.Anotheropportunity
forfutureworkistomaprecurringpeakmomentsinpatternsonto
development activities to identify key drivers of delay. Initial work
inthisdirectionhasbeencarriedoutbyKerzaziandKhomh[ 41]
and Kula et al. [ 45]. Both studies found that testing is one of the
mosttime consuming activities andlikely to result indelay.
Systematic patterns. The identi/f_ied delay patterns might be af-
fectedbysystematiceﬀectsthatarecalendar-related.Previouswork
(e.g.,[17,50])hasshowntheexistenceofsucheﬀectsinsoftware
development work. An interesting opportunity for future research
istotestforseasonalityandmodelthetimedependencyofdelay
patterns using pattern matching. This would allow generalization
over delay patterns and support the identi/f_ication of systematic
eﬀectsatdiﬀerentlevelsoftimegranularity.Forinstance,within-
week dynamics due to day-of-the-week eﬀects, and within-year
dynamicsaﬀectedbyseasonal eﬀects.
Event-driven prediction. Previous studies (e.g., [ 23,44]) have
found that software deliveries can be delayed by disruptive events,suchasbugsandliveincidents,thatoccurduringprojectexecution.
Existingeﬀortestimationmodelsarestaticandthereforenotable
to incorporate such events into their predictions. Our dynamic
modelprovidesfutureresearchanopportunitytoprocessincoming
incidents as they occur. This would require updating of the model
every time an incident or other notable event occurs. Previous
studies [5,6] have recognized the potential of event-driven models
for improvingre-planning strategies insoftware projects.
Delaypropagation. Currently,ourdynamicmodelconsiders
each software delivery independently and does not capture the in-
teractionsbetweendependentdeliveries.However,asingledelayed
softwaredeliverymaycauseadominoeﬀectofsecondarydelays
over dependent teams and projects. Future work should model the
(dynamic) interrelation and propagation of delays across software
deliveries. This could lead to more accurate estimates and a better
understandingoftheeﬀectsofdelaypropagationondelaypatterns.
InitialworkinthisdirectionhasbeencarriedoutbyChoetkiertikul
et al. [13]. They have shown that the use of networked data and
collectiveclassi/f_ication leadsto signi/f_icantaccuracyimprovements.
8 THREATS TO VALIDITY
Constructvalidity. Thedatavariablesweconsidermaynotcap-
ture the intended meaning of (concepts aﬀecting) delay. This intro-
ducespossiblethreatstoconstructvalidity[ 64].Thedelaymeasure-
ments are derived from delivery dates and reported story points
inthebacklogmanagementdata.However,itmighthappenthat
teams do not take their delivery deadlines seriously and close their
deliveriestooearlyortoolate.Itisalsopossiblethatsometeamsdo
notfollowtheguidelinesorprinciplesforestimatingstorypoints.
Wetriedtomitigatethesethreatsbycollectingreal-worlddatafrom
manyepics andteamsover a/f_ive year span.
Another potential threat to our study is related to the milestone
divisionofepics.Wesplittheepicsintoregularly-spacedmilestones
basedoncompletionrate.However,themilestonesmaynotbea
goodmatchwiththeworkpaceofsometeams.Thismighthaveled
toamixtureofprojectphaseswithinmilestonesandacrossepics,
whichwouldaﬀecttheresultsforthepatternsinsomedeliveries.
In practice, it would be more appropriate to split the epics based
oniterations.
Internal validity. The delay patterns that we condition our
Bayesian model on may not re/f_lect the situation in the test data.
Tomitigatethisproblem,weusedtime-basedcross-validationto
mimicarealpredictionscenario.Tocomparemodelsandverifyour
/f_indings,weselectedunbiasederrormeasuresandappliedstatistical
tests [7,54].
Externalvalidity. Externalthreatsareconcernedwithourabil-
ity to generalize our results. We have analyzed 4,040 epics from
270 teams, which diﬀer signi/f_icantly in size, composition and prod-
uct domains. However, we acknowledge that our data may not
berepresentativeofsoftwareprojectsinotherorganizationsand
opensourcesettings.Inothercontexts,softwaredeliveriesmight
have adiﬀerentsetupfollowingdiﬀerentcollaborationpractices.
Replication of our workisneeded to validatethe/f_indings inother
settings andreachmore generalconclusions.
1021Dynamic Prediction of Delays in So/f_tware Projects usingDelay Pa/t_ternsandBayesianModeling ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
9 CONCLUSIONS
Modern agile software projects are volatile due to their iterative
and team-oriented nature. Changes in risk factors and team perfor-
mancetriggertheneedtore-assessoveralldelayriskthroughout
the project life cycle. Existing eﬀort estimation models are static
and not able to capture changes occurring during project execu-
tion.Inthispaper,wehaveproposedadynamiceﬀortestimation
model for continuously predicting overall delay using delay pat-
terns and Bayesianmodeling.The modelincorporates thecontext
ofthe project phase and is/f_inetuned based onchanges indelivery
performanceovertime.We applyourapproach toreal-worlddata
fromthousandsofepics,identifyingfourintuitivedelaypatterns
at ING.The evaluation results demonstratethat:
(1)Delay patterns are indicative of the overall delay and useful as
inputfeature for dynamicprediction.
(2)Thedynamicmodelconsistentlyoutperformsglobalandglobal
iterativeapproaches,andtheSoTAbaselines,evenduringearly
milestones (10–30% ofprojectduration).
(3)ThepredictionsofthedynamicBayesianmodelbecomesub-
stantially more certainandaccurateover time.
Overall, our results highlight the bene/f_its of dynamic prediction
methodsthatareabletolearnfromthetime-dependentcharacteris-
tics of software project delays. We identi/f_ied several research areas
calling for further attention, including causal inference, systematic
eﬀects, and delay propagation.Progress in these areas is crucial to
betterunderstandandmanage delays insoftware projects.
10 DATA AVAILABILITY
Theempiricaldataandsourcecodeusedforthispapercannotbe
madepubliclyavailable dueto anNDA.Toencouragereplication,
we have described our model design step-by-step, and made our
model summary and evaluation available in a replication pack-
age [43].
REFERENCES
[1]PekkaAbrahamsson,IleniaFronza,RaimundMoser,JelenaVlasenko,andWitold
Pedrycz. 2011. Predicting development eﬀort from user stories. In 2011 Inter-
national SymposiumonEmpiricalSoftwareEngineering andMeasurement .IEEE,
400–403.
[2]Pekka Abrahamsson, Raimund Moser, Witold Pedrycz, Alberto Sillitti, and Gian-
carlo Succi. 2007. Eﬀort prediction in iterative software development processes–
Incremental versus global prediction models. In First International Symposium on
EmpiricalSoftwareEngineering and Measurement (ESEM2007) . IEEE,344–353.
[3]Saeed Aghabozorgi, Ali Seyed Shirkhorshidi, and Teh Ying Wah. 2015. Time-
seriesclustering–adecade review. Informationsystems 53(2015), 16–38.
[4]Manish Agrawal and Kaushal Chari. 2007. Software eﬀort, quality, and cycle
time:A studyofCMMlevel 5projects. IEEETransactionsonsoftware engineering
33,3 (2007), 145–156.
[5]Ahmed Al-Emran, Dietmar Pfahl, and Günther Ruhe. 2007. DynaReP: A discrete
event simulation model for re-planning of software releases. In Software Process
Dynamics and Agility: International Conference on Software Process, ICSP 2007,
Minneapolis, MN, USA, May 19-20, 2007. Proceedings . Springer, 246–258.
[6]DavidAmeller,CarlesFarré,XavierFranch,DaniloValerio,andAntoninoCas-
sarino.2017. Towardscontinuoussoftwarereleaseplanning.In 2017IEEE24thIn-
ternationalConferenceonSoftwareAnalysis,EvolutionandReengineering(SANER) .
IEEE,402–406.
[7]AndreaArcuriandLionelBriand.2014. Ahitchhiker’sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
Veri/f_ication and Reliability 24,3 (2014), 219–250.
[8]Mehmet Şirin Artan and İsmail Şahin. 2021. Exploring patterns of train delay
evolutionandtimetablerobustness. IEEETransactionsonIntelligentTransportation
Systems23,8 (2021), 11205–11214.
[9]Michael Betancourt. 2017. A conceptual introduction to Hamiltonian Monte
Carlo.arXiv preprint arXiv:1701.02434 (2017).[10]Paul-Christian Bürkner, Jonah Gabry, and Aki Vehtari. 2020. Approximate leave-
future-out cross-validation for Bayesian time series models. Journal of Statistical
Computation and Simulation 90,14(2020), 2499–2523.
[11]HFrankCervone.2011. Understandingagileprojectmanagementmethodsusing
Scrum.OCLCSystems&Services:Internationaldigitallibraryperspectives (2011).
[12]MorakotChoetkiertikul,HoaKhanhDam,andAdityaGhose.2015. Threshold-
based predictionofschedule overrun insoftwareprojects. In Proceedingsof the
ASWEC 201524thAustralasian SoftwareEngineering Conference . 81–85.
[13]Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, and Aditya Ghose. 2015.
Predicting delays in software projects using networked classi/f_ication (t). In 2015
30thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE) .
IEEE,353–364.
[14]Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, and Aditya Ghose. 2017.
Predicting the delay of issues with due dates in software projects. Empirical
SoftwareEngineering 22(2017), 1223–1263.
[15]Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Aditya Ghose, and John
Grundy. 2017. Predicting delivery capabilityin iterative softwaredevelopment.
IEEE Transactions onSoftwareEngineering 44,6 (2017), 551–573.
[16]Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya
Ghose,andTimMenzies.2018. Adeeplearningmodelforestimatingstorypoints.
IEEE Transactions onSoftwareEngineering 45,7 (2018), 637–656.
[17]Maëlick Claes, Mika V Mäntylä, Miikka Kuutila, and Bram Adams. 2018. Do
programmersworkatnightorduringtheweekend?.In Proceedingsofthe40th
InternationalConference onSoftwareEngineering . 705–715.
[18]MikeCohn.2004. Userstoriesapplied:Foragilesoftwaredevelopment . Addison-
WesleyProfessional.
[19] Mike Cohn. 2005. Agileestimating and planning . Pearson Education.
[20]FrancescoCormanandPavleKecman.2018. Stochasticpredictionoftraindelays
in real-time using Bayesian networks. Transportation Research Part C: Emerging
Technologies 95(2018), 599–615.
[21]Emanuel Dantas, Mirko Perkusich, Ednaldo Dilorenzo, Danilo FS Santos, Hyggo
Almeida,andAngeloPerkusich.2018. Eﬀortestimationinagilesoftwaredevel-
opment: an updated review. International Journal of Software Engineering and
KnowledgeEngineering 28,11n12 (2018), 1811–1831.
[22]KarelDeBakker,AlbertBoonstra,andHansWortmann.2010. Doesriskmanage-
ment contribute to IT project success? A meta-analysis of empirical evidence.
InternationalJournal ofProject Management 28,5 (2010), 493–503.
[23]Amany Elbanna and Suprateek Sarker. 2015. The risks of agile software develop-
ment:Learningfrom adopters. IEEE Software 33,5 (2015), 72–79.
[24]Silvia Ferrari and Francisco Cribari-Neto. 2004. Beta regression for modelling
rates and proportions. Journal ofappliedstatistics 31,7 (2004), 799–815.
[25]Tron Foss, Erik Stensrud, Barbara Kitchenham, and Ingunn Myrtveit. 2003. A
simulation study of the model evaluation criterionMMRE. IEEE transactions on
softwareengineering 29,11(2003), 985–995.
[26]Carlo A Furia, Robert Feldt, and Richard Torkar. 2019. Bayesian data analy-
sis in empirical software engineering research. IEEE Transactions on Software
Engineering 47,9 (2019), 1786–1810.
[27]CarloAFuria,RichardTorkar,andRobertFeldt.2022. ApplyingBayesiananalysis
guidelines to empirical software engineering data: The case of programming
languages and code quality. ACM Transactions on Software Engineering and
Methodology (TOSEM) 31,3 (2022), 1–38.
[28]AndrewGelman.2006. Priordistributionsforvarianceparametersinhierarchical
models(commentonarticlebyBrowneandDraper). Bayesiananalysis 1,3(2006),
515–534.
[29]James Grenning. 2002. Planning poker or how to avoid analysis paralysis while
release planning. Hawthorn Woods: Renaissance Software Consulting 3 (2002),
22–23.
[30]Torleif Halkjelsvik and Magne Jørgensen. 2012. From origami to software devel-
opment:Areviewofstudiesonjudgment-basedpredictionsofperformancetime.
Psychological bulletin 138, 2 (2012), 238.
[31]Wen-Ming Han and Sun-JenHuang. 2007. An empirical analysis of risk compo-
nents and performance on software projects. Journal of Systems and Software 80,
1 (2007), 42–50.
[32]John A Hartigan, Manchek A Wong, et al .1979. A k-means clustering algorithm.
Appliedstatistics 28,1 (1979), 100–108.
[33]Peter Hearty, Norman Fenton, David Marquez, and Martin Neil. 2008. Predicting
projectvelocityinxpusingalearningdynamicbayesiannetworkmodel. IEEE
Transactions onSoftwareEngineering 35,1 (2008), 124–137.
[34]PingHuang,ThomasSpanninger,andFrancescoCorman.2022. Enhancingthe
understandingoftraindelayswithdelayevolutionpatterndiscovery:Aclustering
and Bayesian network approach. IEEE Transactions on Intelligent Transportation
Systems23,9 (2022), 15367–15381.
[35]IrumInayat,SitiSalwahSalim,SabrinaMarczak,MayaDaneva,andShahabod-
dinShamshirband.2015. Asystematicliteraturereviewonagilerequirements
engineeringpracticesandchallenges. Computersinhumanbehavior 51(2015),
915–929.
[36]YushanJiang,YongxinLiu,DahaiLiu,andHoubingSong.2020.Applyingmachine
learning to aviation big data for /f_light delay prediction. In 2020 IEEE Intl Conf on
1022ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA ElvanKula, Eric Greuter, Arie vanDeursen, GeorgiosGousios
Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence
andComputing,IntlConfonCloudandBigDataComputing,IntlConfonCyber
Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech) . IEEE,
665–672.
[37]Magne Jørgensen. 2004. A review of studies on expert estimation of software
developmenteﬀort. Journal ofSystemsand Software 70,1-2 (2004), 37–60.
[38]Magne Jørgensen. 2019. Evaluating probabilistic software development eﬀort
estimates:Maximizinginformativenesssubjecttocalibration. Informationand
softwareTechnology 115(2019), 93–96.
[39]Magne Jørgensen, Morten Welde, and Torleif Halkjelsvik. 2021. Evaluation of
probabilisticprojectcostestimates. IEEETransactionsonEngineeringManagement
(2021).
[40]Sungjoo Kang, Okjoo Choi, and Jongmoon Baik. 2010. Model-based dynamic
cost estimation and tracking method for agile software development. In 2010
IEEE/ACIS 9th International Conference on Computer and Information Science .
IEEE,743–748.
[41]NoureddineKerzaziandFoutseKhomh.2014.Factorsimpactingrapidreleases:an
industrialcasestudy.In Proceedingsofthe8thACM/IEEEInternationalSymposium
onEmpiricalSoftwareEngineering and Measurement . 1–8.
[42]BarbaraAKitchenham,LesleyMPickard,StephenG.MacDonell,andMartinJ.
Shepperd.2001. Whataccuracystatisticsreallymeasure. IEEProceedings-Software
148, 3 (2001), 81–85.
[43]Elvan Kula, Eric Greuter, Arie Van Deursen, and Gousios Georgios.
[n.d.].Supplemental material for Dynamic Prediction of Delays in Software
Projects Using Bayesian Modeling, year = 2023, url = https:///f_igshare.com/s/
4672f25236520a2b4428 .
[44]ElvanKula,EricGreuter,ArieVanDeursen,andGousiosGeorgios.2021. Factors
AﬀectingOn-TimeDeliveryinLarge-ScaleAgileSoftwareDevelopment. IEEE
Transactions onSoftwareEngineering (2021).
[45]ElvanKula,AyushiRastogi,HennieHuijgens,ArievanDeursen,andGeorgios
Gousios. 2019. Releasing fast and slow: an exploratory case study at ING. In
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering
Conference and Symposium on the Foundations of Software Engineering . 785–795.
[46]Elvan Kula, Arie van Deursen, and Georgios Gousios. 2021. Modeling Team
DynamicsfortheCharacterizationandPredictionofDelaysinUserStories.In
2021 36th IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE,991–1002.
[47]WilliamBLangdon,JavierDolado,FedericaSarro,andMarkHarman.2016. Exact
mean absolute error of baseline predictor, MARP0. Information and Software
Technology 73(2016), 16–18.
[48]DeanLeﬃngwell.2007. Scalingsoftwareagility:bestpracticesforlargeenterprises .
Pearson Education.
[49]Nathan P Lemoine.2019. Moving beyondnoninformative priors:why andhow
tochooseweaklyinformativepriorsinBayesiananalyses. Oikos128,7(2019),
912–928.
[50]ChandraMaddila,ChetanBansal,andNachiappan Nagappan.2019. Predicting
pull request completion time: a case study on large scale cloud services. In
Proceedingsofthe201927thacmjointmeetingoneuropeansoftwareengineering
conference and symposiumonthe foundations ofsoftwareengineering . 874–882.
[51]ViljanMahničandTomažHovelja.2012. Onusingplanningpokerforestimating
userstories. Journal ofSystemsand Software 85,9 (2012), 2086–2095.
[52]Carolyn Mair, GadaKadoda,Martin Le/f_ley, KeithPhalp, Chris Scho/f_ield,Martin
Shepperd, and Steve Webster.2000. An investigation of machine learning based
prediction systems. Journal ofsystemsand software 53,1 (2000), 23–29.
[53]Richard McElreath. 2020. Statistical rethinking: A Bayesian course with examples
inR and Stan . Chapmanand Hall/CRC.
[54]Tim Menzies, Zhihao Chen, Jairus Hihn, and Karen Lum. 2006. Selecting best
practicesfor eﬀortestimation. IEEE Transactions on Software Engineering 32,11
(2006), 883–895.
[55]Y Miyazaki, M Terakado, K Ozaki, and H Nozaki. 1994. Robust regression for
developing software estimation models. Journal of Systems and Software 27, 1
(1994), 3–16.
[56]KjetilMolokkenandMagneJorgensen.2003. Areviewofsoftwaresurveyson
softwareeﬀortestimation.In 2003InternationalSymposiumonEmpiricalSoftware
Engineering, 2003. ISESE 2003. Proceedings. IEEE,223–230.[57]Meinard Müller. 2007. Dynamic time warping. Information retrieval for music
and motion (2007), 69–84.
[58]Danh Nguyen-Cong and De Tran-Cao. 2013. A review of eﬀort estimation
studiesinagile,iterativeandincrementalsoftwaredevelopment.In The2013RIVF
InternationalConference onComputing &CommunicationTechnologies-Research,
Innovation,and Visionfor Future(RIVF) . IEEE,27–30.
[59]Bernd Oreschko, Thomas Kunze, Michael Schultz, Hartmut Fricke, Vivek Kumar,
andLanceSherry.2012. Turnaroundpredictionwithstochasticprocesstimesand
airport speci/f_ic delaypattern. In InternationalConference onResearch inAirport
Transportation (ICRAT), Berkeley .
[60]Raydonal Ospina and Silvia LP Ferrari. 2012. A general class of zero-or-one
in/f_latedbetaregressionmodels. ComputationalStatistics&DataAnalysis 56,6
(2012), 1609–1623.
[61]AditiPanda,ShashankMouliSatapathy,andSantanuKumarRath.2015.Empirical
validationofneuralnetworkmodelsforagilesoftwareeﬀortestimationbased
onstorypoints. ProcediaComputer Science 57(2015), 772–781.
[62]Jirat Pasuksmit, Patanamon Thongtanunam, and Shanika Karunasekera. 2021.
TowardsJust-EnoughDocumentationforAgileEﬀortEstimation:WhatInforma-
tionShouldBe Documented?.In 2021 IEEE InternationalConferenceonSoftware
Maintenance and Evolution(ICSME) . IEEE,114–125.
[63]Dan PortandMarcelKorte. 2008. Comparativestudies of themodel evaluation
criterions mmre and pred in software cost estimation research. In Proceedings of
the Second ACM-IEEE international symposium on Empirical software engineering
and measurement . 51–60.
[64]Paul Ralph and Ewan Tempero. 2018. Construct validity in software engineering
researchandsoftwaremetrics.In Proceedingsofthe22ndInternationalConference
onEvaluationand AssessmentinSoftwareEngineering 2018 . 13–23.
[65]Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdi-
novic. 2019. Detecting and quantifying causal associations in large nonlinear
time seriesdatasets. Scienceadvances 5,11(2019), eaau4996.
[66]Federica Sarro, Alessio Petrozziello, and Mark Harman. 2016. Multi-objective
software eﬀort estimation. In 2016 IEEE/ACM 38th International Conference on
SoftwareEngineering (ICSE) . IEEE,619–630.
[67]Ken Schwaber and Mike Beedle. 2002. Agile software development with Scrum .
Vol. 1. Prentice Hall Upper SaddleRiver.
[68]RichardTorkar,CarloAFuria,RobertFeldt,FranciscoGomesdeOliveiraNeto,
Lucas Gren, Per Lenberg, and Neil A Ernst. 2021. A method to assess and argue
for practical signi/f_icance in software engineering. IEEE Transactions on Software
Engineering 48,6 (2021), 2053–2065.
[69]CarlosJoaquínTorrecilla-Salinas,JorgeSedeño,MJEscalona,andManuelMejías.
2015. Estimating, planning and managing Agile Web development projects
under a value-based perspective. Information and Software Technology 61 (2015),
124–144.
[70]Adam Trendowicz and Ross Jeﬀery. 2014. Software project eﬀort estimation.
Foundations and Best Practice Guidelines for Success, Constructive Cost Model–
COCOMOpags (2014), 277–293.
[71]Adam Trendowicz and Jürgen Münch. 2009. Factors in/f_luencing software devel-
opmentproductivity—state-of-the-artandindustrialexperiences. Advancesin
computers 77(2009), 185–241.
[72]MuhammadUsman,EmiliaMendes,andJürgenBörstler.2015.Eﬀortestimationin
agile software development: a survey on the state of the practice. In Proceedings
of the 19th international conference on Evaluation and Assessment in Software
Engineering . 1–10.
[73]Muhammad Usman, Emilia Mendes, Francila Weidt, and Ricardo Britto. 2014.
Eﬀortestimationinagilesoftwaredevelopment:asystematicliteraturereview.
InProceedings ofthe10th internationalconferenceonpredictive modelsin software
engineering . ACM,82–91.
[74]AkiVehtari,Andrew Gelman, andJonah Gabry. 2017. Practical Bayesianmodel
evaluationusingleave-one-outcross-validationandWAIC. Statisticsandcom-
puting27,5 (2017), 1413–1432.
[75]AkiVehtari,AndrewGelman,DanielSimpson,BobCarpenter,andPaul-Christian
Bürkner. 2021. Rank-normalization, folding, and localization: an improved R
forassessingconvergenceofMCMC(withdiscussion). Bayesiananalysis 16,2
(2021), 667–718.
Received 2023-02-02; accepted 2023-07-27
1023