Jigsaw: Large Language Models meet Program Synthesis
Naman Jain
t-namanjain@microsoft.com
Microsoft Research
Bangalore, IndiaSkanda Vaidyanath‚àó
svaidyan@stanford.edu
Stanford University
Stanford, USAArun Iyer
ariy@microsoft.com
Microsoft Research
Bangalore, IndiaNagarajan Natarajan
nagarajn@microsoft.com
Microsoft Research
Bangalore, India
Suresh Parthasarathy
supartha@microsoft.com
Microsoft Research
Bangalore, IndiaSriram Rajamani
sriram@microsoft.com
Microsoft Research
Bangalore, IndiaRahul Sharma
rahsha@microsoft.com
Microsoft Research
Bangalore, India
ABSTRACT
Large pre-trained language models such as GPT-3 [ 10], Codex [ 11],
and Google‚Äôs language model [ 7] are now capable of generating
code from natural language specifications of programmer intent.
We view these developments with a mixture of optimism and cau-
tion.Ontheoptimisticside,suchlargelanguagemodelshavethe
potential to improve productivity by providing an automated AI
pair programmer forevery programmer in the world.On the cau-
tionary side, since these large language models do not understand
programsemantics,theyoffernoguaranteesaboutqualityofthe
suggested code. In this paper, we present an approach to augment
theselargelanguagemodelswithpost-processingstepsbasedon
program analysis and synthesis techniques, that understand the
syntaxandsemanticsofprograms.Further,weshowthatsuchtech-
niques can make use of user feedback and improve with usage. We
presentourexperiencesfrombuildingandevaluatingsuchatool
Jigsaw, targeted at synthesizing code for using Python Pandas API
using multi-modal inputs. Our experience suggests that as these
large language models evolve for synthesizing code from intent,
Jigsawhas an important role to play in improving the accuracy of
the systems.
ACM Reference Format:
NamanJain,SkandaVaidyanath,ArunIyer,NagarajanNatarajan,Suresh
Parthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large
Language Models meet Program Synthesis. In 44th International Conference
on Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pittsburgh, PA, USA.
ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/3510003.3510203
1 INTRODUCTION
Pre-trained large language models ( PTLM) such as GPT-3 [ 10]a r e
finding pervasive applications in Natural Language Processing
(NLP), as a general purpose platform to solve many NLP tasks.
Recent efforts show that PTLMs can generate code from natural
‚àóWork done by author during internship at Microsoft Research, India
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510203
Figure 1: Multi-modal problem specification in Jigsaw
language prompts, by associating documentation text with code
from alarge training set [ 1,7,11]. Thispresents a newavenue for
program synthesis. However, PTLMs do not ‚Äúunderstand‚Äù either
thesyntaxorsemanticsofthecode,andtreatcodeastext[ 7].Con-
sequently, the code produced by such models has no guarantees of
correctness or quality. Hence, any system that uses such PTLMst o
generatecodewillneedtoaugmentitwithprogramanalysisand
program synthesis modules to ensure correctness. In this paper,
we present the design and empirical evaluation of such a multi-
modalprogram synthesissystem called Jigsaw,which istargeted
specifically at synthesizing code for using large and complex APIs.
Jigsawis multi-modal (as depicted in Figure 1) in the sense that
itcaningestinputas(1)anaturallanguagestringexpressingintent
and (2) a set of test cases, or input-output examples, and produces
acodesnippetasoutput.Futureincarnationsmaybedesignedto
acceptothermodesofinputaswell.Thearchitectureof Jigsawis
showninFigure2.Thepre-processingmoduleconvertsthenatural
language intent into a customized query to send to the PTLM. The
post-processingmoduleperformssyntacticandsemanticchecks,
and performs transformations on the code produced by the PTLM,
ensuring that the code passes the supplied test cases and otherquality checks. The transformations are specifically designed tocorrect common and recurring errors made by
PTLMs, such as
referencingerrors(wherethecodereferencesvariablenamesincor-
rectly),argumenterrors(wherethecodeinvokesthecorrectAPI,
butwithincorrectarguments),andaclassofsemanticerrors(which
12192022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma
can be corrected by learning AST-to-AST transformations). Sec-
tion2showsconcreteexamplesofsucherrors,andSection3shows
how the transformations correct such errors. Jigsawlearns from
usage by incorporating user feedback into both pre-processing and
post-processing modules, and learns from user engagements to
improveitsoverallquality.OurexperimentsshowhowJigsawis
able to learn from past usage to improve future performance.
The current version of Jigsawis designed and evaluated to syn-
thesizecodeforthePythonPandasAPI[ 27].However,theprinci-
ples behind the design of Jigsaware general, and the design can
be extended to other libraries and programming languages as well.
We create a user interface for Jigsawusing a Jupyter notebook [ 2]
extension.Theextension canbeinvokedusing amagiccommand,
andinvocationofthecommandcreatesasidebarwindowwitha
Jigsawcard for each invocation. The Jigsawcard allows users to
supply andedit inputsto thesystem, inspectthe resultsand copy
the desired output back into the main notebook window.
Weevaluate Jigsawintermsoftheoverallaccuracy,aswellasac-
curacyofthecomponentsofthepre-processingandpost-processing
modules, on two datasets we created: PandasEval1 , created by the
authorsofthispaper,and PandasEval2 ,createdby25usersduringa
hackathon,whereparticipantsweregivenpointsforsolvingPython
Pandas tasks using Jigsaw. The hackathon was conducted across
two sessions (details in Section 4). We used user feedback from
the first session to improve the pre-pro cessing and post-processing
modules of Jigsaw, and found users were about to solve about 10%
more tasks in the second session, due to learning improvements
from the first session.
In Section 5, we instantiate Jigsawwith two state-of-the-art
PTLMs:GPT-3[10] andCodex[11], and present comprehensive
evaluations. We show the ov erall improved p erformance of Jigsaw
compared to baselines and state-of-the-art code synthesis frame-
works on the two datasets, as well as gains due to learning from
user feedback over time.
In summary, this paper makes the following contributions:
‚Ä¢We present an architecture to perform code synthesis by
augmenting black-box PTLMs with program analysis and
synthesis-basedtechniquesandmulti-modalspecifications.
Wehaveimplementedthearchitectureinatoolcalled Jigsaw.
WehavedevelopedaJupyternotebookextensionthatallows
users to interact with the system seamlessly.
‚Ä¢We characterize common classes of errors made by PTLMs,
namely,reference errors,argumenterrors, andsemantic er-
rors. Motivated by these errors, we have designed program
analysis and synthesis techniques in Jigsawto fix such er-
rors in code produced by PTLMs. We have also designed
techniques to learn from user feedback and improve with
usage.
‚Ä¢WehavecreatedtwoPandasdatasetswithmulti-modalspec-
ifications (released for community use). Using two state-
of-the-art PTLMs,weshowthat Jigsawyieldssignificantly
higheraccuracy comparedto baselineson thetwo datasets.
Ourhypothesisisthatevenas PTLMsforcodeimprove,systems
such asJigsawthat perform pre-processing and post-processing
moduleswillbecrucialtoimproveuserexperience,andenhancethequalityoftheoutputproduced.Thisisbecause
PTLMsinherentlydonotunderstandthesyntaxorsemanticsofcodetheygenerate,sowe
expect gaps to remain between PTLMoutput and user expectation.
Tools based on program analysis and synthesis techniques that
understand the code and API syntax and semantics can addressthesegapsbetterthangeneric
PTLMs.Wediscusshowtodesign
pre-processing and post-processing modules in a general manner,
so thatJigsawcan work for any language and any API.
2 JIGSAW OVERVIEW
Jigsawis amulti-modal, interactive code synthesis system where
(a)theuserspecifiesintentviaacombinationofnaturallanguage
descriptionandtestcases(i.e.,input-output(I/O)examples);and
(b)theuserinteractswiththesystemviaafriendlyandseamless
interfaceintegratedwiththeprogrammingenvironment.Thein-
teractiveaspectof Jigsawiscrucialfor thedevelopertorefinethe
possiblyambiguousintentspecificationaswellasforthesystemto
gather useful feedback for improving the components. In this sec-
tion, we highlight some of the challenges in using general-purpose
PTLMsforspecificdomainswithexamplequeries,whichdirectly
motivates the design of our Jigsawcode synthesis pipeline.
2.1Jigsawdesign principles
Wetreatlargelanguagemodelsasblack-box,i.e.,wecanonlyquery
them.Thisisareasonableassumptionsincethepremisethatthe
expertiseandmeanstofine-tunethemodelsisoutofreachformost
users. This design choice is motivated by three reasons: (a) there is
a natural barrier to access these large models, and we can get only
theoutputofthemodelforagiveninputviasomeinterface(e.g.,
REST APIs), (b) these large language models constantly evolve and
get better with each generation [ 10,33]; treating them as black-
boxes enables plug-and-play with minimal effort, and (c) finally,
domain-specificimprovementstolargelanguagemodels(e.g.for
Pythonprogramming[ 15],generalprogramming[ 11])arerendered
complementary to our efforts rather than competitive.
We configure Jigsawwith aPTLM(GPT-3 and Codex, in this
work) of choice to be used as a black-box. We focus on appropri-
atelysettinguporprimingthesemodelsforagiventaskathand,
characterizingcommonfailuremodesof PTLMsforcodesynthe-
sis, and building components that can overcome such recurring
failures. We rely on both program synthesis-based techniques and
multi-modal specification to design these components. Our goal is
toenablesynthesisofsyntacticallyandsemanticallycorrectcode
snippets for a given domain and using feedback from usage to
improve the system over time.
Thepre-processingmoduleof Jigsawcontextualizestheinputto
theblack-boxlanguagemodelusingheuristictechniques(akinto
recentefforts[ 23,30,46]).Akeycontributionofoursystemisinits
post-processingmodule:(a)s peedingupthecombinatorialsearch
spaceofAPIfunctionsandtheirarguments,and(b)learningand
updatingasetoftransformationorre-writerulestobeappliedtothe
erroneous snippets output by PTLMs. The post-processing module
uses I/O examples to choose the appropriate transformation.
In the rest of the paper, we instantiate Jigsawfor solving data
transformationtaskswithPythonPandasAPI,whichiswidelyused
by data scientists to process tabular data [27].
1220
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. Jigsaw: Large Language Models meet Program Synthesis ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Pre-process
inputsNatural language
Input/output
examples
Other specificationse.g. Assertions
Pre-trained
language model PTLMPost-process
outputsCorrect program
(edited by users)
Learning from user feedback
Figure 2: Architecture of Jigsaw
2.2PTLMs as black-box
Consider a typical scenario where the user wants to load and ex-amine the data in a
csvfile. Pandas uses dataframe objects (two-
dimensional tabular representation) to store and process hetero-
geneousdata(commonlynamedwith dfprefixesinthecode,like
df, df1, df2, dfin ). The user invokes Jigsawwith a simple natural
language description of the intent in a cell of Jupyter Notebook:
%jigsaw -q "Load ./data.csv file"
The‚Äúmagiccommand‚Äù %jigsawinvokesthesynthesispipeline
withthegivenquery. Jigsaw(configuredwith GPT-3)returnsthe
following snippets for the above query:
 
df = pd.read_csv('./data.csv')
csv = pd.read_csv('./data.csv', header=None) 
The user then issues the following query in the session:%jigsaw -q "Remove substring ‚ÄòName:‚Äô from column
‚Äòcountry‚Äô of df"
Jigsawproduces the following snippets.
 
df['country'] = df['country'].str.replace('Name:', '') 
Akeyknobin PTLMsissettingtherightcontextforagivenuser
query.Thiscontextispassedasaninputtothe PTLMinaddition
to the user query. To this end, Jigsawfirst prepares the input in
thepre-processing stage (details in Section 3.2). The preparation
involves assembling a set of ‚Äúrelevant‚Äù question-answer pairs to
informthe PTLMofthenatureoftheinputtask‚Äîwhichisconvert-
ing natural language text to Python code, specifically, Pandas code.
With the context selection in the pre-processing stage, Jigsawpro-
ducesthedesiredcodesnippetsshownabove.Incontrast,theGPT-3
modelwithout contextselection producesthe followingincorrect
snippet for the above query:
 
df = df.country.str.remove('Name:') 
Recent studies, both in the context of natural language under-
standing [ 30,46] as well as in the programming [ 7] domains, have
shown the influence and importance of context selection in the
output of PTLMs. Our work provides further evidence that context
selection can significantly impact the quality of the code gener-ated for Pandas programming tasks, with two different
PTLMs,
demonstrated in Section 5.4.2.3 Learning to fix recurring failure modes
ofPTLMs
The core aspect of Jigsawsystem design is incorporating a post-
processing phase that involves: (a) characterizing, (b) transforming
the (syntactically and/or semantically) erroneous code snippets,
and,moreimportantly,(c)endowingthesystemwiththecapabilitytoimprove (intermsofaccuracy)fromfeedbackasmoreusersinter-
actwithitovertime.Below,wehighlightcommonclassesoferrors
weobserveovertwodifferentPandasprogrammingdatasets(cre-
ated by us, and described in Section 4) using two different PTLMs,
namely GPT-3 and Codex.
1. Referencing errors: We observe that, even with suitable con-
text,PTLMscanproduceincorrectreferencingofvariablenames
in otherwise accurate code snippets.2.Incorrectarguments:
Insomecases, PTLMsproducecodewith
the right composition of API functions, but with incorrect argu-
ments. For instance, consider the following invocation:
%jigsaw -q "remove all duplicate entries of column
‚ÄôinputB‚Äô" 
dfout = df.drop_duplicates(subset=['inpB']) # PTLM
dfout = df.drop_duplicates(subset=['inpB'],keep=False) # Correct 
3. Semantic errors: A recurring failure mode for the PTLMsw e
have experimented with is that they produce code snippets that
arealmostcorrect, but the semantics are wrong because of a minor
error.Wecanquantifythisviasuitableeditdistancebetweenthe
ASTs of the produced and the correct (i.e., intended) code snippets.
For instance, consider the following invocation:
%jigsaw -q "Get fourth value from column ‚ÄôC‚Äô in dfin
and assign to dfout" 
dfout = dfin.ix[3, 'C']# PTLM
dfout = dfin.loc[3, 'C']# Correct 
Jigsawemploys a post-processing phase that critically relies
on the multi-modal specification (I/O examples, in particular) toovercome the aforementioned recurring failures. To this end, we
passtheincorrectoutputcodesnippetfrom PTLM(whichcanbe
ascertained with the help of I/O examples in the specification)through a series of components driven by PL-based techniques
(details in Section 3.3). The two key ideas are outlined below.
(1)UsingtheAPIfunctionsintheincorrectcodesnippetspro-
ducedbyPTLM,weseedtheenumerativesearchfortheright
arguments. We perform this search efficiently adapting the
1221
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma
 
gpt3 = GPT(engine="davinci", temperature=0.5, max_tokens=100)
# Examples to train a English to French translator
gpt3.add_ex(Example('What is your name?','quel est votre nom?'))
gpt3.add_ex(Example('What are you doing?','Que faites-vois?'))
gpt3.add_ex(Example('How are you?','Comment allex-vous?'))
# Input to the model
prompt3 = "where are you?"
output3 = gpt3.submit_request(prompt3)
# Model output
output3.choices.text 
Output: O√π √™tes-vous?
Figure 3: English to French translation using GPT-3
AutoPandas framework [ 9] which is an enumerative-search
basedprogrammingbyexamplesframeworkbuiltforPandas
API.
(2)The user interface of Jigsawenables getting feedback which
is then used by our system to learn a set of AST-to-AST
transformations using the Prosesynthesis framework [ 16,
32].Thechallengehereliesinclusteringerrorsthatare alike
so that a small set of general transformations can be learnt.
3 JIGSAW ARCHITECTURE
The architecture of Jigsawis depicted in Figure 2. In this section,
we describe each module in detail.
3.1 Pre-trained Language Models
We describe Pre-trained Language Models ( PTLMs) usingGPT-3
as an example. GPT-3stands for "Generative Pre-trained Trans-
former 3", which is the third version of a large transformer model
developed by OpenAI. GPT-3is a neural model with 175 billion
parameters,trainedonaverylargecorpusconsistingofpublicly
availabledatasetssuchasCommonCrawl1,WebRTextdataset,two
internet-basedbookscorpora,andEnglishWikipedia. GPT-3isa
general-purposemodelthatcanbecustomizedtoperformavari-
etyofNLPtasks.Suchcustomizationsdonotinvolvefine-tuning
the ML model for the specific task at hand. Instead, the user of
GPT-3can describe the task using a few examples (on the order
of 4-5 examples works usually), and GPT-3is then able to produce
answersforthespecifictask.Asessionwith GPT-3hastheform:
(ùëÑ1,ùê¥1),(ùëÑ2,ùê¥2),...,(ùëÑùëò,ùê¥ùëò),ùëÑ,whereùëòisasmallnumber(typi-
cally4or5),thepairs (ùëÑùëñ,ùê¥ùëñ)arequestion-answerpairstodescribe
thetaskwewant GPT-3toperform,and ùëÑisthequestionforwhich
we seek an answer.
For example, if (ùëÑùëñ,ùê¥ùëñ)are such that ùëÑùëñare English statements
andùê¥ùëñarecorrespondingFrenchtranslations,then GPT-3becomes
an English-French language translator. See session in Figure 3.
Otherrecent PTLMsincludeCodex[ 11],whichisOpenAI‚Äôsre-
centlanguagemodeltrainedspecificallyoncode,andGoogle‚Äôslargelanguagemodel[
7];thesemodelstranslatenaturallanguagetopro-
gram.JigsawusesPTLMs to produce Pandas code, given a natural
languagedescriptionofintent,andtestcases.Specifically, Jigsaw
sessionwith GPT-3hastheform: (ùëÅ1,ùëÉ1),(ùëÅ2,ùëÉ2),...,(ùëÅùëò,ùëÉùëò),ùëÅ
whereùëÅùëñisEnglishdescriptionofintent,and ùëÉùëñisthecodesnippet
1https://commoncrawl.org/the-dataOffline crawling of
documentation
pages
Context
bank
Mean-standard
deviation
normalization
of dataframe dfContext
selectorN1: Find the mean of dataP1: data.mean()N2: Perform column-wise ORoperation in dfP2: df = df.any()
Mean-standard deviation
normalization of dataframe df+ PTLM
Figure 4: Illustration of Pre-processing step of Jigsaw
we want the PTLMto produce. We currently do not pass input-
outputexamplestothe PTLM.Instead,weusethesetestcasesto
check and filter the candidate codes produced by the PTLMduring
post-processing,ortransformthecodeproducedbythe PTLMsuch
that it passes the test cases.
3.2 Pre-processing
Thegoalof Jigsaw‚Äôspre-processingmoduleistoconverttheuser
intent into a suitable query for the PTLM. As mentioned above,
PTLMstakeasequenceofquestion-answerpairs (ùëÅ1,ùëÉ1),(ùëÅ2,ùëÉ2),
...,(ùëÅùëò,ùëÉùëò)asapreamblebeforewesupplythecurrentquery.We
use the term contextto denote this preamble. Previous works in
natural language processing (NLP) [ 23,30,46] have shown that
the performance of these PTLMs is heavily influenced by this con-
text; and the performance improves if the question-answer pairs in
contextaresimilartothecurrentquery ùëÅ.Hence,wemaintaina
contextbank,ofpossiblequestion-answerpairs,andthenchoose
elements of the context bank that are similar to the current query,
and add these to the context. Jigsawcreates a context bank offline
by scraping and annotating examples from API documentation
forPandas,aswellasotherresourcesofexamples(fromtutorials,
etc.) that are used to teach the API. When the user asks a question,
thequestionisfedtoacontextselectorthatusesatextsimilarity
metric to pick the most relevant prompts from the context bank
(seeFigure4).Westudytwokindsofsimilaritymetricsforthecon-
text selector: a) tf-idf similarity [ 39](TFIDF), andb) transformer
similarity [ 37](TRANSFORMER ) . The context thus produced is
appended with the current query and fed as input to the PTLM.
Incaseswhere Jigsawisunabletoproducethecorrectanswer,
we let users make changes to the incorrect Jigsawcode and use
suchafeedbacktoenhancethecontextbank(detailsinSection3.4).
PTLMs also take an input parameter called temperature.L o w e r
values of temperature result in fewer accurate answers. Higher
values result in a less accurate but more diverse set of answers. We
report on how we pick temperature values in Section 5.
3.3 Post-processing
The code snippets produced by the PTLMvary in accuracy and
quality,dependingonthenaturallanguagesentencesusedtoask
toencodethequestion,thecontextbanksupplied,thecontextse-
lection as wellas the temperature parameter. The goalof Jigsaw‚Äôs
post-processing step is to filter and transform the output produced
1222
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. Jigsaw: Large Language Models meet Program Synthesis ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
by thePTLMto produce a correct answer. Our measure of cor-
rectness is that the code produced should pass the I/O examples
specified by the user. In many cases, the code does not parse orfails with an exception. We consider such cases as test failures.If a non-empty set of candidate solutions produced by the
PTLM
satisfiesthetestcases,thenwemerelyshowthosecodesnippetsto
the user. Our experience is that for about 30%-60% of the cases (de-
pendingonthe PTLMandthedataset),the PTLMproducescorrect
outputs.Intheremainingcases, Jigsawusesthecandidatesolutions
produced by PTLMas starting points and performs transforma-
tions on candidate code snippets using simple program analysis
and synthesis techniques to produce correct solutions. We describe
the correctness checks and transformations below:Correctness checks
: In cases where we have I/O examples, we
run the candidate codesnippet starting with each of thespecified
inputs, and check if the output produced agrees with the corre-
sponding specified output. This check can be expanded to include
staticanalysestocheckforsecurityvulnerabilitiesandothererrors,
as motivated by recent work [29].
VariableNametransformations :Insomecases, PTLMsproduce
accuratecodesnippets,butwithincorrectvariablenames.Thisisof-tenduetothemodel‚Äôsbiastowardscommonvariablenameslike
df
forPandasdataframesandalsobecauseusersassumevariablerefer-
encing to be implicit. As an example, we find that GPT-3produces
the code snippet df1.merge(df2)when the correct answer is df2.
merge(df1). Since users specify inputs and output variables in the
naturallanguagedescriptionorintestcases,thispost-processing
step uses such information from multi-modal inputs, as well as
namesofvariablesinscope,bysystematicallysearchingoverpoten-
tialvariables,andtryingpossiblepermutationsandcombinations
of variable names so as to pass the test cases.
Argument transformations: In some cases, the PTLMs produce
codesnippetswithcorrectmethodnamesandmethodsequences
(incasemultiplemethodsneedtobeinvokedinanestedmanneror
oneafteranother),butwithincorrectarguments.Asanexample,inresponsetothequery‚Äú
replace ‚ÄòUnited States‚Äô in ‚Äòlocation‚Äô
by ‚ÄòUS‚Äô and ‚Äò3434‚Äô in ‚Äòzip‚Äô by ‚Äò4343‚Äô‚Äù, Codexproduces: 
dfout = df.replace({'United States':'US', 3434:4343}) 
Thissnippetinvokesthecorrectmethod replace,butmissesthe
detail in the question that ‚ÄòUnited States‚Äô and3434must be
replacedwith ‚ÄòUS‚Äôand4343onlywhenthesevaluesarepresentin
the columns ‚Äòlocation‚Äô and‚Äòzip‚Äôrespectively. The correct code
synthesized by Jigsawfor this query is as shown below: 
dfout = dfin.replace({'location':{'United States':'US'},
'zip':{3434:4343}}) 
Motivatedbysuchcases,thispost-processingstepsystematically
searchesthroughargumentsfromaninferredargumentspacefora
given sequence of method/function names. In order to implement
the systematic search over the space of arguments, we adapt the
approachusedbyAutopandastool[ 9],withthefollowingmodifi-
cations.AutopandasusesaGraphNeuralNetwork,thattakesI/O
examples as input, to choose method names. However, we needa lot of domain-specific data to train such neural networks. In
ourcase,wesimplyextractthemethodnamesfromtheoutputof
PTLMgiven the natural language query (which readily scales toprogramming domains beyond Pandas). The argument space to
performthesearchisinferredusingthenaturallanguagetextinput,theargumentspresentinthe
PTLMoutput,thecolumnnamesfrom
the dataframe schema as well as variables in scope. We extend the
generatorsinAutopandastoconsidercomplexdatatypessuchas
lists and dictionaries, and we extend the set of APIs considered
to include APIs that return Pandas Series types ( one-dimensional
labeledarrayscapableofholdingdataofanytype)inadditionto
the ones that return Pandas dataframe types. With these modifi-
cations,wefindthat Jigsawisabletotransformseveralincorrect
codesnippetsproducedbythe PTLMtocorrectcodesnippets(as
shown in Section 5).
AST-to-AST transformations : In some cases, we find that the
PTLMproducescodethatisalmostcorrectbuthasaminorerror.
We also find that such errors are repeatedly made by the PTLMs.
As aspecific example,we find GPT-3oftenmisses the bitwise not
operator, and produces the code: 
train = data[data.index.isin(test.index)]} 
instead of the following correct code with the bitwise not operator: 
train = data[~data.index.isin(test.index)]} 
Asanother example,we findthat GPT-3missesparanthesizations,
whichresultsinthegeneratedcoderaisinganexception.Specifi-
cally, the generated code is: 
dfout = dfin[dfin['bar']<38|dfin['bar']>60] 
insteadofthefollowingcodesynthesizedby Jigsawwhichisparen-
thesized correctly: 
dfout = dfin[(dfin['bar']<38)|(dfin['bar']>60)] 
Such errors cannot be fixed via variable name transformation or
argument transformations. However, code has well-defined struc-
ture,usuallyrepresentedasabstractsyntaxtree(AST).Jigsawtakes
advantage of this structure and corrects such errors by learning
re-writing rules as AST-to-AST transformations learned from user
interactionswith Jigsaw.Thesetransformationsareapplicationsof
productionrulesfromgrammarusedinBluePencil [ 26]whichis
usedforsuggestingcodere-factorings.However,itisnotpossibletolearntheserulesattheappropriatelevelofgeneralityfromasingle
example. This generality is necessary so that the missing negation
or parenthesizing can be corrected by the learnt transformation,
evenifthesamepatternisrepeatedwithadifferentsetofvariablesorconstants.Toachievethis,wecollectdatafromuserinteractions,
where the user edits the answer produced by Jigsawto produce
the correct code. We cluster the data points (i.e., code snippets)so that similar data points are grouped together and we learn a
singleAST-to-ASTtransformationthatisabletohandleallthedata
points in a cluster. Unlikethe case of refactoring where users will
implicitlyhintatclusteringofsimilaredits(byattemptingthemone
aftertheother),weresorttoagreedyheuristics-basedclustering
algorithm. This clustering is performed in an online fashion as we
get more data points for learning AST-to-AST transformations. For
each data point, we decide if the data point is grouped inside an
existing cluster or instantiate a new cluster. In the former case, we
check if the AST-to-AST transformations from the existing cluster
1223
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma
can be re-learnt to be more general, and if so, re-learn the transfor-
mations.Inaddition,weperturbthedatapointsineachclusterto
change variable names and constants, in order to prevent learning
transformations that over-fit. Together with the above-mentioned
clustering and perturbationheuristics, we find thatwe are able to
learn transformations at the appropriate level of generality (Sec-
tion5.2.1).WeusethePROSEprogramsynthesissystem[ 16,32]to
learnthetransformationsfromaclusterofincorrect-correctcode
snippets.While JigsawcurrentlyworksonlyonPythoncode,the
post-processingstepworksatthelevelofASTs,andcanbemade
to work across programming languages as well.
WerefertoArgumenttransformationsandAST-to-ASTtrans-
formations together as Semantic Repair in experiments (Section 5).
3.4 Learning from user feedback
The user interface of Jigsaw (integrated into the Jupyter notebook)
isdesignedtoletuserssubmitcorrectcodein caseswhereJigsaw
is incorrect. Jigsawcan be improved by assimilating user feedback.
Specifically, we design techniques for updating context-bank in
the pre-processing module and AST-to-AST transformations in the
post-processing steps, as more users interact with Jigsaw.
Updatingcontextbank: Theprocedureforupdatingcontextbank
withuserqueriesisgiveninAlgorithm1.Wefirstcheckwhether
Jigsawalreadyfoundacorrectsolutionforthegiven(new)query
ùëÅ,thusgivingussomeconfidenceaboutitscorrectness.Otherwise,
wecheckifanyofthesolutionsgeneratedby Jigsawis‚Äúclose‚Äùto
some correct code (determined by the standard edit distance on
strings dEDITand a chosen threshold ùúñCODE). If either of the two
conditionsissatisfied,weaddthenewquerytoourcontextbank
whileadditionallyensuringthatasimilarqueryalreadydoesnot
exist(viaTFIDFbaseddistance dTFIDFandathreshold ùúñBANK).With
Algorithm 1 Updating context bank
Inputs:
Context Bank : C={(ùëÅ1,ùëÉ1),(ùëÅ2,ùëÉ2),...,(ùëÅ|C|,ùëÉ|C|)},
New query and feedback (code snippet): ùëÅ,ùëÉ
Output: Updated Context bank C
Let output = Jigsaw(ùëÅ,C)
If min ùëñdEDIT(outputùëñ,ùëÉ)>ùúñCODE
return C
If max ùëñdTFIDF(ùëÅ,ùëÅ ùëñ)<ùúñBANKreturn C
return C‚à™{(ùëÅ,ùëÉ)}
moreusage,wegrowthecontextbank andtrytocover different
stylesofuserqueries,whichinturnhelpsrelevantcontextselection.
Updating transformations: For every query paired with correct
code snippet(s), weselect all incorrect codessuggested by PTLM
within some small edit distance of a correct code. The AST-to-AST
transformations learning sub-module performs clustering (withperturbations) on the selected code snippets as discussed in the
above subsection, and updates the set of transformations.
3.5 Generality of approach
Webelievethattheideaspresentedabovesuchascontextselection,
correctnesschecking,andtransformationsaregeneralandthatit
is possible to design pre-processing and post-processing steps in aBeginner Intermediate Advanced
Python 1 21 3
Pandas 17 8 0
Table1:Proficiencyofparticipantsfrom PandasEval2 dataset
generic manner that can work across languages, APIs, and PTLMs.
We give some evidence to this argument below: a) Though we did
our initial analysis on Pandas code generated by GPT-3, we found
that Codex also fails in similar modes. Jigsaw trained on GPT-3
failures is able to fix similar failures generated by Codex as well in
thissetting. b)Argumenttransformationscanbeinstantiatedfor
different libraries in a manner similar to AutoPandas [ 9].c) AST
transformationsandlearningfromuserfeedbackextendtoother
languagesreadily(in[ 26],suchanapproachisusedtolearnnon-
trivial code refactoring in C#, SQL, Markdown, and spreadsheets).
For each new API, specific transformation rules can be learnt from
usage data generated by users of that API.
4 DATASETS
We perform our experiments on two different datasets2.
4.1 PandasEval dataset PandasEval1
Thisdatasetconsistsof68PythonPandastasks.Eachtaskcanbe
solved using a single line of code by composing at most 2-3 Pandas
functions; sometimes followed by assigning variables. This dataset
was created by authors of this paper by going through queries
in online forums like StackOverflow. An example task from this
dataset is ‚ÄúFor every row in df1, update ‚Äòcommon‚Äô column to True
if value in column ‚ÄòA‚Äô of df1 also lies in column ‚ÄòB‚Äô of df2".
4.2 Hackathon dataset PandasEval2
This dataset consists of 21 Pandas tasks; each task can be solved
bycomposingatmost2-3Pandasfunctions,possiblyfollowedby
assigning variables, as in the PandasEval1 dataset. We posed these
tasksasillustrations,inahackathonweconductedwith25users
over2differentsessions.TheparticipantsofthishackathonwereMi-
crosoft research fellows and interns. Table 1 presents self-reported
proficiencyoftheusersinPythonandPandas.Anexampleillustra-tion that shows the intent of a task is given in Figure 5. Users were
asked to read such pictorial illustrations and come up with their
ownnaturallanguage(English)queryconstructionsforeachtask
to solve the problem. We then collected the queries written by the
users,clustered,andannotatedthemtoproducethe PandasEval2
dataset comprising of a total 725 unique queries constructions.
The task corresponding to the illustration in Figure 5, from the
datasetPandasEval2 , is shown below. Here dfinanddfoutrefer
to the dataframes in Figure 5.
We note that while users provided precise and clear natural
language queries in many cases, they also came up with imprecise
and incorrect formulations in some cases. For instance, in the specshown above, the query provided by
user1is correct, whereas the
one provided by user2is incorrect because the word ‚ÄúFrance‚Äùi s
presentinthe‚ÄúIATA‚Äùcolumnaswell;Figure5conveysthatonly
the ‚Äúcountry‚Äù column needs to change, and not the ‚ÄúIATA‚Äù column.
2The datasets can be found at https://github.com/microsoft/JigsawDataset
1224
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. Jigsaw: Large Language Models meet Program Synthesis ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Figure 5: Example task, part of the dataset PandasEval2,a s
presented to the user during the Hackathon session.
Since such queries were created by users interacting with the
system, andusers tendto make mistakes,it is usefulto havesuch
variationsinthedataset.Whilecuratingthedataset,weremoved
naturallanguagequeriesthatwereclearlyincorrect,andretained
queries that were imprecise and partially correct.
1"task_8": {
2"queries": [
3["replace 'France' with 'FR' in 'country'
column and 'Paris' with 'PAR' in 'city'
column", "user1"],
4["In dataframe dfin, replace cells having '
France' to 'FR' and cells having 'Paris' to '
PR'","user2"]
5],
6"IO":[{
7 "inputs": "dfin",
8 "output": "dfout"
9}]
10}
Listing 1: Example json for a task in PandasEval2
As mentioned earlier, we conducted the hackathon over two
sessions.Weuse PandasEval2_S1 todenotethedatasetgenerated
from user queries from the first session, and PandasEval2_S2 to
denote the dataset generated from user queries from the second
session. For each of the 21 tasks, we created semantic variations
(e.g.changingconstants,APIarguments)ofthesametask.Conse-
quently,usersinthesecondsession( PandasEval2_S2 )sawdifferent
variants of the tasks when compared to users in the first session
(PandasEval2_S1 ). Specifically, 3 tasks were exactly the same, 9
haddifferencesinconstantsand9hadchangesinarguments.We
introducedthesevariantsinordertostudyif Jigsawcanlearnfrom
usage in the first session to improve user experience in the second
session(see Section5.2). We use PandasEval2 todenote theunion
ofPandasEval2_S1 andPandasEval2_S2.
5 EXPERIMENTS
We evaluate Jigsaw on the two datasets introduced in Section 4,
with emphasis on the following questions: a) How accurate is
theJigsawsystem compared to the black-box PTLMs and other
code synthesis methods? b) What is the utility and applicability
of the individual Jigsawcomponents (in the pre-processing and
post-processing modules)? c) Can these components benefit from
feedback over time as more users interact with the system?For the first and second questions, we evaluate Jigsawin an
offline setting, i.e., without learning from any feedback (in Sec-tions 5.1); and present comparisons against the state-of-the-art
AutoPandas framework, which generates Pandas snippets using
only I/O example (in Section 5.3). For the third question, we per-
form a temporal study on the PandasEval2 dataset (in Section 5.2),
where we leverage user feedback from the first hackathon session
toupdatethesystemandmeasuretheperformanceimprovement
in the second session. We also perform ablation studies (in Sec-tion5.4)pertainingtoourcontextselectionsub-module.Weend
with a preliminary evaluation of Jigsaw on tasks pertaining to the
TensforFlow API (in Section 5.5).
We consider accuracy as our primary evaluation metric, i.e.,
fractionofspecificationsinthedatasetforwhicha correctprogram
wassynthesized.Wedefineaprogramascorrectifitsatisfiesthe
givenI/Oexamples,andadditionallypassesamanualinspectionof
whetherthesynthesizedcodemeetstheintentofthenaturallan-
guage description.The manualinspectionhelps us rejectprograms
that satisfy the I/O examples by overfitting on them and violate
thegeneralintentofthenaturallanguagedescriptions.Notethat
thereisinherentrandomnessintheoutputofthe PTLMs,sowerun
every evaluation three times and report the mean accuracy (%) and
standard deviation (over the runs). In some cases, we also present
taskcompletion metricwhichisthepercentageoftaskscorrectly
solved by a user (regardless of the number of queries used to solve
a task) interacting with the system. Furthermore, in every case,
we present the best accuracy obtained by varying the temperature
parameter of PTLM‚àà{0,0.2,0.4,0.6}.
5.1 Offline evaluation
InTable2,wepresenttheperformanceof JigsawonPandasEval1
andPandasEval2 datasets,with GPT-3andCodexPTLM s.Thesec-
ond column of the table indicates the context selection strategyfor the
PTLM. For this study, we consider NO-CONTEXT (no tai-
lored context provided for the user query; we use a default context:
‚Äúimport pandas as pd ‚Äù),andTRANSFORMER (Transformersimi-
larity based context selection, discussed in Section 3) with number
of context prompts fixed as 4. Each cell in the table gives the ac-
curacy metric with mean and standard deviation as defined above.
ForJigsaw,thecolumntitledVariableNameindicatestheperfor-
mance of the system using only this part of the post-processing
module;andthecolumntitledSemanticRepairindicatestheper-
formance of the system in its entirety, i.e., running Variable Name
transformations followed by Semantic Repair (Argument transfor-
mations and AST-to-AST transformations).
Comparing PTLMand Semantic Repair columns, it is evident
thatJigsawimproves upon the black-box PTLMs, in terms of ac-
curacy,by15%-40%irrespectiveofthecontextselectionstrategy,onboththedatasetsaswellasonboththe
PTLMs.Theseresults
underscore theutility ofprogram analysis-based augmentationoflarge language models.
Next, from Table 2, we find that providing useful context for
the language model along with the query significantly improves
upon not providing any context (comparing rows 1 vs 2, and rows
3vs4),acrossthedatasetsand PTLMs.Itisclearthat PTLMwith
TRANSFORMER contextisbetterthan NO-CONTEXT byamargin
1225
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma
PandasEval1 PandasEval2
PTLM Variable Name Semantic Repair PTLM Variable Name Semantic Repair
GPT-3NO-CONTEXT 30.9¬±1.23 8.2¬±2.44 4 .6¬±3.9 8.9¬±0.62 4.8¬±0.93 3 .6¬±0.5
TRANSFORMER 33.8¬±2.44 1.7¬±2.54 7 .1¬±2.1 6.6¬±0.22 4.3¬±0.83 5 .1¬±0.7
CodexNO-CONTEXT 45.6¬±1.25 4.9¬±0.75 9 .8¬±3.5 26.8¬±1.25 1.0¬±0.65 6 .8¬±0.3
TRANSFORMER 52.0¬±0.76 3.7¬±0.76 6 .7¬±0.7 31.2¬±0.26 7.5¬±0.57 2 .2¬±0.5
Table 2: Performance (mean accuracy ¬±std. deviation) after different stages of the Jigsawpipeline on PandasEval1 and
PandasEval2 datasets. Jigsawpost-processingstepssignificantlyimprovesupon PTLMsirrespectiveofcontextselectionstrategy.
Pre-processing clearly benefits, comparing rows 1 vs 2, and 3 vs 4.
‚àº5%(withoutpost-processing)andupto15%withpost-processing
forCodexon the two datasets. For GPT-3,TRANSFORMER con-
textissignificantlybetterthan NO-CONTEXT onthePandasEval2
dataset and on PandasEval1 , the numbers are statistically insignifi-
cant.PTLMsrequiresomeinitialcontextintheformofexamples
tocharacterizethetasktobesolved,andtheseresultsunderscore
the importance of having a pre-processing module in Jigsaw.
Finally, from Table 2, we also observe the effectiveness of the
individualpost-processingmodulesof Jigsaw,asdiscussedbelow.
Notethat,fortheseresults,weseedourAST-to-ASTtransforma-
tions using a small dataset collected from StackOverflow questions.
Later, in Section 5.2, we show that these numbers can be signifi-
cantlyimprovedby learningtransformationsfromusageovertime.
Variable Name transformations: PTLMs make variable refer-
encing errors (as noted in Section 2) because of its implicit bias
towardscommondataframenamessuchas df, df1, df2, dfout
and also because users tend to not specify variables explicitly in
theirqueries.Wefindthatthissimplepost-processingmodulegives
an improvement of 10%-30% for Codexand 10%-15% for GPT-3.
SemanticRepair: Weseethatthesemanticrepairpost-processing
moduleimprovesabsoluteperformanceof Codexby‚àº5%andof
GPT-3by6%-11%.Thisunderscoresthesignificanceofusingpro-
gram analysis techniques to augment language models that do not
haveinherentunderstandingofcodesemantics.Recall(fromSec-
tion 3) that Semantic Repair consists of Argument transformations
and AST-to-AST transformations sub-modules. We find that, using
just the Argument transformations (without AST-to-AST transfor-
mations),improvesabsoluteperformanceofthesystemby5%-9%
and3%-5%for GPT-3andCodexrespectively(notshowninTable2).
Similarly, using AST-to-AST transformations alone (without Argu-
menttransformations),weobtainimprovementofupto3 .5%for
GPT-3and 1.3% forCodex(not shown in Table 2).
We find that our post-processing steps are reasonably fast; time
takenby Jigsawisprimarilybottle-neckedbytheinferencetimes
ofPTLMAPIs. Specifically, on average getting output from Codex
takes‚àº7 seconds while our post-processing module takes <3s ec -
onds.Similarly,onaverage, GPT-3takes30-40secondsfordifferent
context sizes while post-processing finishes in <10 seconds.
5.2 Temporal evaluation
Inthissection,weevaluate Jigsawonitsabilitytolearnandimprove
withuserfeedback.Weperformthisevaluationonthe PandasEval2
dataset.Recallthatthehackathonwasorganizedovertwoseparate
sessions; so, we use the submissions andfeedback for tasks in the
first session, corresponding to the PandasEval2_S1 dataset, to (a)
update our context bank, (b) learn AST-to-AST transformations,
and(c)evaluate JigsawonthePandasEval2_S2 datasetconsistingofvariantsofthetasksin PandasEval2_S1 (asdescribedinSection4.2).
Updatingpre-processingmodule: WefollowAlgorithm1(with
ùúñCODE =25,ùúñBANK =0.15)andfilteroutbadlywrittenqueriesfrom
thefirstsessionusers.Notethat,forthe3tasksthatareidentical
in the two sessions, we do not make any updates to the context
bank.Wedenoteourseededcontextbankthatwasusedinthefirst
session(containing243question-answerpairs)with CS1andthe
updated context bank updated resulting from Algorithm 1 with
CS2containing 371 (243 seeded + 128 new) question-answer pairs.
Updatingpost-processingmodule: Wefollowtheprocedurede-
scribed in Section 3.4 to learn AST-to-AST transformations from
session one data along with seeded data. We use TS1to denote
transformationsseededduringsessiononeand TS2todenotethe
new/updated transformations learned from session one data.
Wecomparetheperformanceof JigsawonPandasEval2_S1 (with
CS1andTS1) againstPandasEval2_S2 (with baseline CS1andTS1,
as well as with the updated context bank CS2and transforma-
tionsTS2)inTable3.Eachcellinthetableisthemeanaccuracyand
standarddeviationonthecorrespondingdataset.Twoobservations
are in order.
(1) Learning helps improve Jigsaw.Itisevidentthattheperfor-
mance of Jigsawon thePandasEval2_S2 dataset with the default
CS1-TS1setting(column3)issignificantlylowerthanthatofthe
updatedCS2-TS2setting (column 4) for both the PTLMs. Accuracy
ofthesystemwith GPT-3improvesbyover30%duetotheupdated
modules; even with Codex, which already performed quite well on
all datasets, we still improve by ‚àº15% with updates.
(2) Second session was in general more challenging. We also
observe that the performance on PandasEval2_S2 with the default
CS1-TS1setting (column 3) is significantly lower than that on
PandasEval2_S1 with the same setting (column 2). This is because
in general the second session was more challenging; partly dueto the higher percentage of queries on difficult tasks, and the se-
manticdifferencesintasksacrossthetwosessions.Butwhenwe
use the updated the context and transformations banks, we find
a drastic improvement in the performance on PandasEval2_S2 ,a s
highlighted in (1) above. This illustrates that Jigsawhas the ability
to improve from user feedback, regardless of the PTLMused.
Finally, we also look at the task completion metric (described in
thebeginningofSection5),toassesshowtheperformancegains
of learning from feedback translated to user experience during the
hackathon.Insessionone,usersweabletosolveonly71%ofthe
tasks on average; however, in session two, users were able to solve
82% of the tasks on average, thus making the experience of the
Jigsawsystem more productive with the updates.
1226
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. Jigsaw: Large Language Models meet Program Synthesis ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
PandasEval2_S1 PandasEval2_S2
CS1-TS1 CS1-TS1 CS2-TS2
GPT-345.9¬±0.4 35.1¬±0.86 7.2¬±0.3
Codex75.1¬±0.5 69.0¬±0.78 4.4¬±0.8
Table 3: Performance (mean accuracy ¬±std. deviation)
ofJigsawwithout(CS1-TS1)andwith(CS2-TS2)learningcon-
text bank and transformations from user feedback on the
PandasEval2 dataset.Learninghelpsimproveaccuracysignif-
icantly, comparing columns 3 and 4.
5.2.1 Analyzing learned AST-to-AST transformations. Wepresent
someofthelearnedAST-to-ASTtransformationsappliedtocode
snippets produced by GPT-3 in Table 4. The transformations were
learned using the clustering and perturbing technique outlinedin Section 3.3. We see that the code fixes are interpretable andthey solve common semantic problems in the outputs of
PTLMs.
Please refer to supplementary material for details of the precise
AST-to-ASTtransformationslearntcorrespondingtofirsttworows
of Table 4.
Forinstance,considertheruleimpliedinthefirstrowofTable4,
which is of inserting ~(bitwise not operator) inside subscript. This
transformation,learnedusingtheclusterof diversecodesnippets
in Listing 2, is fairly general (this is one of the clusters obtained by
running our clustering technique on seeded and session one data).
On the other hand, consider the last row of Table 4, which waslearned using the cluster of code snippets in Listing 3. Since the
clusteredsnippets followasimilarstructure,thelearnedtransfor-
mationworksonlywhenanewsnippethasexactlythesamelogical
conditional operators in the specific order. Thus, the quality of the
learnedtransformationsdependsonthequalityoftheclustering
andofthecodesnippetsthemselves,andweexpectthatmoreusage
data positively influences the overall quality.
 
# Task-1
dfout = df.loc[df.isnull().any(axis=1), :] #incorrect
dfout = df.loc[~df.isnull().any(axis=1)] #correct
# Task-2
df_p = df_p.loc[df_per["Name"].str.contains("Ch")] #incorrect
df_p = df_p.loc[~df_per["Name"].str.contains("Ch")] #correct 
Listing 2: Cluster of code snippets from two different tasksthat yields the Bitwise-Not transformation in Table 4.
5.3 Comparison to AutoPandas
AutoPandas (AP)[9]isaPandasprogramsynthesisenginecapable
ofgenerating programs withtwoor threePandasfunctions. Ituses
generators for enumerating over the Pandas API and guides the
search with the help of Graph Neural Networks (GNNs) which
operateontheinput-output(I/O)dataframe(s)andreturnsthemost
likely function sequences and arguments.
In contrast, we make use of multi-modal specification (both nat-
ural language query and I/O examples). Programming by examples
is known to have ambiguous under-specifications [ 19,32]. From
our experience this issue is exacerbated for large APIs that provide
multiple ways for achieving similar functionalities. For instance,
considerthespecificationinFigure1.IfweonlyconsidertheI/O
example for the given task, we can find many trivial solutions that
just drop or select certain rows of dataframe.Weevaluate APonourPandasEval1 andPandasEval2 datasets.
As discussed in Section 3, APdoes not support series operations,
column assignments and dictionary or list generators, many of
whicharenecessaryinPandasworkflows.So,outof68tasksinthe
PandasEval1 dataset and 21 tasks in the PandasEval2 dataset, only
20 and 7 are covered by the AutoPandas framework respectively.
Hence, we compare Jigsaw(instantiated with the Codex PTLM )
againstAPonlyonthese27tasksanduseatimeoutof3minutes.
InthefirstrowofTable5,weseethat Jigsawclearlyoutperforms
AutoPandas even in the restricted subset solvable by AP. This is
because 16 of the 27 tasks are under-specified if only I/O examples
are used and APreturns over-fitting solution on many of these
tasks; this highlights the necessity of multi-modality.
We also run Jigsawon theAPdataset [9], where all tasks are
supported by AutoPandas and I/O examples are sufficient. This
datasethasbeensourcedfromStackOverflowposts.Since Jigsaw
usestextastheprimaryinput,weaddnaturallanguagedescriptions
in these posts for querying Codex. The results are in the second
row of Table 5; while Codexalone is inferior to AP,Jigsaw(with
Codex) performs better than AP.
5.4 Ablation study
In both the offline and temporal evaluations presented in the previ-
oussubsections, wefixedthe numberof contextpromptsto 4and
TRANSFORMER asthecontextselectorinthepre-processingmod-
ule. In this ablation study, we ask if the performance of Jigsawis
sensitive to these choices, and provide justification for the same.
Allexperimentsinthissectionarecarriedoutwiththesamesetting
as that of Section 5.1.
Table 6 compares the performance of Jigsawwith two different
context selection strategies, namely, TFIDFandTRANSFORMER .
We find that the transformer context selector is slightly better, but
moreimportantly,thattheperformanceof Jigsawisnotsensitiveto
the selection strategy. Table 7 compares the performance of Jigsaw
with different number of context prompt examples, i.e., 1, 4, and 8.
Ourexperimentsshowthatwhilethereisn‚Äôtasignificantdifferencebetweentheperformancesof4promptsvs.8prompts,bothperform
betterthanusingjust 1prompt.Again, Jigsawisrelativelyrobust
to these choices.
Finally, note that all variations of these choices, for the num-
ber of prompts as well as the selection strategy, outperform the
NO-CONTEXT setting (see Table 2); this further underscores the
utility of the pre-processing module.
5.5 Beyond Pandas
To test the generality of Jigsaw, we did a preliminary evaluation
with25TensorFlow[ 6]taskssourcedfromTF-coder[ 40]andonline
forums like StackOverflow. We setup the pre-processing module of
Jigsawsimilar to the offline evaluation, by creating a context bank
of 25 prompts from documentation pages. We reuse the VariableName module and do a what-ifanalysis for argument and tree
transformationsmanually.Table8showstheperformanceofJigsaw
on the TensorFlow dataset. As seen from the table, Codex aloneis able to solve only 8 of the 25 tasks, variable transformation
3Interestingly, this missing parenthesis mistake is quite common and frequented even
by humans! See blog post [3] and StackOverflow question [4].
1227
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma
Code Before Code After Semantic Explanation
out=data[data.index. isin(test.index)] out=data[~data.index. isin(test.index)] Adding bitwise not inside subscript
df=df[df[ 'foo']>70|df[ 'foo']<34] df=df[(df[ 'foo']>70)|(df[ 'foo']<34)] Parenthesizing mistake3
out=df. iloc[0,"HP"] out=df. loc[0,"HP"] Changing iloctoloc
dfout=df1. append(df2,ignore_index=True) dfout=df1. append(df2) Dropping the last keyword argument
dfout=dfin. duplicated () dfout=dfin. duplicated ().sum() Computing sum of series using .sum()
train=data. drop(test) train=data. drop(test.index) Adding .indexin first argument (of drop)
dfin=dfin[ "A"].rolling (window=3). mean() dfin[ "A"]=dfin[ "A"].rolling (3).mean()Reassign back to the column
dfout=dfin[(x<40)|(y>53)&(z==4)] dfout=dfin[((x<40)|(y>53))&(z==4)] Giving precedence to bitwise-or
Table 4: Applications (Code After) of learned transformations on code snippets produced by PTLM(Code Before).
 
#Task-1
dfout = dfin[(dfin["gamma"]<40)|(dfin["gamma"]>53)&(dfin["alpha"]==4)] # incorrect
dfout = dfin[((dfin["gamma"]<40)|(dfin["gamma"]>53))&(dfin["alpha"]==4)] # correct
#Task-2
dfout_per = dfin_per.loc[(dfin_per["alpha"]<140)|(dfin_per["alpha"]>159)&(dfin_per["beta"]==103)] # incorrect
dfout_per = dfin_per.loc[((dfin_per["alpha"]<140)|(dfin_per["alpha"]>159))&(dfin_per["beta"]==103)] # correct 
Listing 3: Cluster of code snippets from two different tasks that yields the precedence transformation in Table 4.
AutoPandas [9]PTLM Jigsaw
Subset of Jigsawdatasets 16/27 20/27 23/27
AutoPandas dataset 17/26 15/26 19/26
Table5:Numberoftaskssolvedby JigsawandAPonasubset
of our dataset supported by APand their dataset.
Context PandasEval1 PandasEval2
GPT-3TFIDF 46.5¬±4.83 2 .4¬±0.5
TRANSFORMER 47.1¬±2.13 5 .1¬±0.7
CodexTFIDF 69.1¬±2.47 0 .1¬±0.1
TRANSFORMER 66.7¬±0.77 2 .2¬±0.5
Table6:Ablationstudy:Performanceof Jigsawwithtwocon-
text selection strategies.
# Prompts PandasEval1 PandasEval2
GPT-314 7 .5¬±1.83 4 .9¬±0.9
44 7 .1¬±2.13 5 .1¬±0.7
84 8 .0¬±2.53 2 .9¬±0.6
Codex16 2 .3¬±0.77 1 .8¬±0.5
46 6 .7¬±0.77 2 .2¬±0.5
86 6 .2¬±1.27 2 .4¬±0.9
Table7:Ablationstudy:Performanceof Jigsawwithdifferent
number of context prompts.
PTLMVariable Name Semantic Repair
8/25 15/25 19/25
Table8:Preliminaryresultsof JigsawwithTensforFlowAPI.
improvestheperformanceto15tasks.Wemanuallycomparethe
code outputs to the expected output, to check if argument and tree
transformations can be learnt. Based on this analysis, we find that
Semantic Repair can potentially improve the performance to 19
tasks.Weshowsomeexamplesbelow.Forthequery‚ÄúGivenatensor
in1, replace all instances of 1 with 0‚Äù, PTLMoutputs the following: 
tf.where(x == 1, 0, x) Thecorrectcodeforthisquery,synthesizedbyJigsawusingvariable
transformation, is shown below: 
tf.where(in1 == 1, 0, in1) 
Forthequery‚ÄúGivenatensorin1andatensorofindicesind,get
thesumofelementspresentatindicesinindfromtensorin1.‚Äù,the
PTLMoutputs the following incorrect code: 
tf.gather(in1, ind) 
The correct code, shown below, can be synthesized by Jigsaw with
a learnt AST-to-AST transformation, if sufficient data points are
collected from usage: 
tf.reduce_sum(tf.gather(in1, ind)) 
In summary, this shows that the proposed pre-processing and post-
processing modules are useful, and can be generalized to other
libraries and programming languages as well.
6 THREATS TO VALIDITY
Ourdatasetshavebeencreatedbymanuallyinspectinginternet
forumslikeStackOverflow.Wetriedtocoverthecommonprogram-
mingpatternsinPandas.However,theyarenotrepresentativeof
all Pandas programs in the wild.
We designed the PandasEval2_S1 andPandasEval2_S2 datasets
by collecting data from two sessions of a hackathon, as a proxy for
the real-world setting, where large software teams are working on
thesameprojectwithsimilartasks,allowing Jigsawtolearnand
improveovertime.Wevariedthetasksbetweenthetwosessions,
so as to simulate variants of tasks. However, the variations we
introduced may not representative of variations of tasks in the real
world. Our study had only 25 participants; evaluating whether the
productivity of developers is enhanced in a statistically significant
manner ina largescale deploymentof Jigsaw isbeyond thescope
of this paper.
WhencomparingJigsawto AutoPandas ,Jigsawtakesasinput
both the natural language description and the I/O examples, while
AutoPandas onlytakestheI/Oexamplesasinputs.Hence,Jigsaw
1228
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. Jigsaw: Large Language Models meet Program Synthesis ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
has more information about the tasks than AutoPandas . Jigsaw
takeslessthanaminutepertaskandweuseatimeoutofthreemin-
utesforAutoPandas .Althoughhighertimeoutsmightimprovethe
performance of AutoPandas (10-15 minutes [ 9]), they are not com-
patible with the interactive user experience that we are aiming for.
Whether AutoPandas solvedataskcorrectlyornotisdetermined
by manual inspection and is susceptible to human errors.
7 RELATED WORK
The literature on using machine learning for program synthesis
is vast [8,17,18,21,22,25,28,35,41] and we restrict to works
which are closest to Jigsaw (synthesizing code for large APIs using
large models and multi-modal specifications). These works can be
classifiedintothefollowingcategories:1)designedforlargeAPIs
but do not use large models, 2) based purely on large models with
no multimodal specification, and 3) multimodal synthesis for small
APIs. Details follow:
(1)TheTDE[ 20]systemforJavareliesonrichtypeinformation
(which is absent in Pandas) and fails to generate argument combi-
nationsthat areabsentfromitscorpus. AutoPandas[ 9]generates
Pandas code exclusively from input-output (I/O) examples usinga combination of GNNs, which predict function sequences, and
enumerativesearch.TF-coder[ 40]usesbothnaturallanguagede-
scriptions and I/O examples to generate TensorFlow code. Both of
themusesmallspecificmodels(asopposedtolargegenericmodels
like GPT-3) and lack mechanisms to incorporate user feedback.(2)
GPT-3[10]whiletrainedonwebhasshowninspiringcapability
on synthesizing code. Models have also been explicitly trained on
code with documentation [ 7,11,15]. In particular, Codex [ 11], that
is part of GitHub Copilot, generates Python code from natural lan-
guagedescriptions.Syncromesh[ 31]usesbuildcontextpromptsfor
PTLMs similar to how Jigsawdoes in pre-processing. Additionally,
they propose Constrained Semantic Decoding for generating code
while respecting syntactic and semantic constraints. However, the
expressiveness of the constraints is restricted and more work is
needed to model constraints that occur in practice. Spider [ 5,45]i s
a text-to-SQL competition where many tools compete [38, 42].(3)
Rahmani et al . [34]use the outputs of GPT-3 to guide a compo-
nent based search. Their approach is evaluated only on small DSLs
suchasregularexpressionsandCSSselectorsandtheydonotlearn
from user feedback. Manshadi et al . [24]and Raza et al . [36]syn-
thesize string transformations. WebQA [ 12] synthesizes programs
toextractinformationfromwebpages.Regel[ 13]andYeetal .[44]
synthesizes regular expressions. Mars [ 14] synthesizes data wran-
gling operations. These techniques have not been demonstrated at
the scale of Pandas that has hundreds of operations.
Jigsaw fixes the output of PTLMand is hence related to work on
programrepairlikeRefazerthatlearnscodetransformationsfrom
editsused tofixprograms [ 16].Jigsaw‚Äôs interfaceisinspired from
B2‚Äôs [43] interface that augments visualizations to notebooks.
8 CONCLUSION AND FUTURE WORK
Jigsaw is the first tool for synthesizing code for large APIs like
Pandas that leverages the advancements in PTLMs. The key contri-
bution of Jigsaw lies in the post-processing steps that drastically
improvethequalityofthecodegeneratedby PTLMslikeGPT-3.Inparticular,themultimodalsynthesisofJigsawoutperformsboththebaselinesthatexclusivelyuse
PTLMsandthosethatexclusivelyuse
I/Oexamplesforprogramsynthesis.However,severalchallenges
remain before we can have a true ‚Äúpair programmer‚Äù experience
withPTLMs and we discuss a couple of them.
First, in this paper, the quality of the synthesized code is largely
determinedbytheI/Oexamples.However,inpractice,codequality
is more nuanced than correctness on unit tests. Ideally, the synthe-
sizedcodeshouldhavehighperformance,shouldnothavesecurity
vulnerabilities [29], and respect licensing attribution4.
Second, Jigsaw focuses on multi-modal specifications with natu-
ral language intent and I/O examples. However, even multi-modal
specifications can be weak or ambiguous, and would need to be re-
fined using richer specifications like preconditions, postconditions,
invariants,boundsonresourceusageliketimeandmemory,etc.,
to obtain the intended code.
Acknowledgement. We thank Dhvanil Sanghvi for helping us
perform a preliminary evaluation of Jigsawwith Tensorflow (re-
ported in Section 5.5), and Arjun Radhakrishna for helping us with
PROSE and Refazer related queries.
4https://www.wired.com/story/github-commercial-ai-tool-built-open-source-code/
1229
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma
REFERENCES
[1] [n.d.]. GitHub Copilot ¬∑ Your AI pair programmer. https://copilot.github.com/
[2] [n.d.]. Jupyter. https://jupyter.org/[3]
[n.d.]. Parenthesis Blog. https://www.roelpeters.be/
cannot-compare-a-dtyped-object-array-with-a-scalar-of-type-bool/
[4][n.d.]. Parenthesis StackOverflow. https://stackoverflow.com/questions/
38252423/python-error-typeerror-cannot-compare-a-dtyped-float64-array-with-a-scalar-o
[5] [n.d.]. Spider 1.0: Yale Semantic Parsing and Text-to-SQL Challenge.[6] [n.d.]. TensorFlow. https://www.tensorflow.org/[7]
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, HenrykMichalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,and Charles Sutton. 2021. Program Synthesis with Large Language Models.
ArXivabs/2108.07732 (2021).
[8]MatejBalog,AlexanderL.Gaunt,MarcBrockschmidt,SebastianNowozin,and
DanielTarlow.2017. DeepCoder:LearningtoWritePrograms.In 5thInternational
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net.
[9]RohanBavishi,CarolineLemieux,RoyFox,KoushikSen,andIonStoica.2019.
AutoPandas:neural-backedgeneratorsforprogramsynthesis. Proc.ACMProgram.
Lang.3, OOPSLA (2019), 168:1‚Äì168:27.
[10]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
InAdvancesinNeuralInformationProcessingSystems33:AnnualConferenceon
NeuralInformationProcessingSystems2020,NeurIPS2020,December6-12,2020,
virtual, Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (Eds.).
[11]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde deOliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
GregBrockman,AlexRay,RaulPuri,GretchenKrueger,MichaelPetrov,Heidy
Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
MikhailPavlov,AletheaPower,LukaszKaiser,MohammadBavarian,Clemens
Winter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,MatthiasPlappert,
Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,
AlexNichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBal-
aji,ShantanuJain,WilliamSaunders,ChristopherHesse,AndrewN.Carr,Jan
Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, MatthewKnight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-
Grew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciechZaremba.
2021. EvaluatingLargeLanguageModelsTrainedonCode. CoRRabs/2107.03374
(2021).
[12]QiaochuChen,AaronLamoreaux,XinyuWang,GregDurrett,OsbertBastani,
and Isil Dillig. 2021. Web question answering with neurosymbolic program syn-
thesis. In PLDI ‚Äô21: 42nd ACM SIGPLAN International Conference on Programming
LanguageDesignandImplementation,VirtualEvent,Canada,June20-25,20211,
Stephen N. Freund and Eran Yahav (Eds.). ACM, 328‚Äì343.
[13]Qiaochu Chen, Xinyu Wang, Xi Ye, Greg Durrett, and Isil Dillig. 2020. Multi-
modal synthesis of regular expressions. In Proceedings of the 41st ACM SIGPLAN
International Conference on Programming Language Design and Implementation,
PLDI 2020, London, UK, June 15-20, 2020, Alastair F. Donaldson and Emina Torlak
(Eds.). ACM, 487‚Äì502.
[14]YanjuChen,RubenMartins,andYuFeng.2019. Maximalmulti-layerspecifica-
tion synthesis. In Proceedings of the ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineer-
ing, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, Marlon Dumas,
Dietmar Pfahl, Sven Apel, and Alessandra Russo (Eds.). ACM, 602‚Äì612.
[15]Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and
Neel Sundaresan. 2020. PyMT5: multi-mode translation of natural language and
Pythoncodewithtransformers.In Proceedingsofthe2020ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP).AssociationforComputational
Linguistics, Online.
[16]Reudismam Rolim de Sousa, Gustavo Soares, Loris D‚ÄôAntoni, Oleksandr Polozov,
Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bjoern Hartmann. 2016. Learning
SyntacticProgramTransformationsfromExamples. CoRRabs/1608.09000(2016).
[17]JacobDevlin,JonathanUesato,SuryaBhupatiraju,RishabhSingh,Abdel-rahman
Mohamed, and Pushmeet Kohli. 2017. RobustFill: Neural Program Learning
under Noisy I/O. In Proceedings of the 34th InternationalConference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 (Proceedings ofMachine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.).
PMLR, 990‚Äì998.
[18]Yu Feng, Ruben Martins, Osbert Bastani, and Isil Dillig. 2018. Program synthesis
usingconflict-drivenlearning.In Proceedingsofthe39thACMSIGPLANConference
onProgrammingLanguageDesignandImplementation,PLDI2018,Philadelphia,PA, USA, June 18-22, 2018, Jeffrey S. Foster and Dan Grossman (Eds.). ACM,
420‚Äì435.
[19]Sumit Gulwani. 2016. Programming by examples. Dependable Software Systems
Engineering 45, 137 (2016), 3‚Äì15.
[20]Yeye He, Xu Chu, Kris Ganjam, Yudian Zheng, Vivek R. Narasayya, and Surajit
Chaudhuri. 2018. Transform-Data-by-Example (TDE): An Extensible Search
Engine for Data Transformations. Proc. VLDB Endow. 11, 10 (2018), 1165‚Äì1177.
[21]Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek
Jain, andSumit Gulwani. 2018. Neural-Guided DeductiveSearch for Real-Time
Program Synthesis from Examples. In 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings. OpenReview.net.
[22]Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018. Accelerating
search-basedprogramsynthesisusinglearnedprobabilisticmodels.In Proceedings
of the 39th ACM SIGPLAN Conference on Programming Language Design andImplementation, PLDI 2018, Philadelphia, PA, USA, June 18-22, 2018, Jeffrey S.
Foster and Dan Grossman (Eds.). ACM, 436‚Äì449.
[23]Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Gra-
hamNeubig.2021. Pre-train,prompt,andpredict:Asystematicsurveyofprompt-
ing methods in natural language processing. arXiv preprint arXiv:2107.13586
(2021).
[24]Mehdi Hafezi Manshadi, Daniel Gildea, and James F. Allen. 2013. Integrating
ProgrammingbyExampleandNaturalLanguageProgramming.In Proceedings
of the Twenty-Seventh AAAI Conference on Artificial Intelligence, July 14-18, 2013,
Bellevue,Washington,USA,MariedesJardinsandMichaelL.Littman(Eds.).AAAI
Press.
[25]AdityaKrishnaMenon,OmerTamuz,SumitGulwani,ButlerW.Lampson,and
AdamKalai.2013. AMachineLearningFrameworkforProgrammingbyExample.
InProceedingsofthe30thInternationalConferenceonMachineLearning,ICML2013,
Atlanta,GA,USA,16-21June2013 (JMLRWorkshopandConferenceProceedings,
Vol. 28). JMLR.org, 187‚Äì195.
[26]AndersMiltner,SumitGulwani,VuLe,AlanLeung,ArjunRadhakrishna,Gus-
tavoSoares,AshishTiwari,andAbhishekUdupa.2019. Ontheflysynthesisof
edit suggestions. In Object-Oriented Programming, Systems, Languages & Applica-
tions (OOPSLA). ACM. https://www.microsoft.com/en-us/research/publication/
on-the-fly-synthesis-of-edit-suggestions/
[27]The pandas development team. 2020. pandas-dev/pandas: Pandas. https://doi.
org/10.5281/zenodo.3509134
[28]Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong
Zhou, and Pushmeet Kohli. 2017. Neuro-Symbolic Program Synthesis. In 5th
InternationalConferenceonLearningRepresentations,ICLR2017,Toulon,France,
April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
[29]H.Pearce,BaleeghAhmad,BenjaminTan,BrendanDolan-Gavitt,andR.Karri.
2021. AnEmpiricalCybersecurityEvaluationofGitHubCopilot‚ÄôsCodeContri-
butions.ArXivabs/2108.09293 (2021).
[30]Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True Few-Shot Learning
with Language Models. ArXivabs/2105.11447 (2021).
[31]Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher
Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code Generation from
Pre-trained Language Models. In International Conference on Learning Represen-
tations.
[32]OleksandrPolozovandSumitGulwani.2015. Flashmeta:Aframeworkforinduc-
tiveprogramsynthesis.In Proceedingsofthe2015ACMSIGPLANInternational
ConferenceonObject-OrientedProgramming,Systems,Languages,andApplications .
107‚Äì126.
[33]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[34]Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Dan Morris, Arjun Rad-
hakrishna, Gustavo Soares, and Ashish Tiwari. 2021. Multi-modal ProgramInference: a Marriage of Pre-trained Language Models and Component-based
Synthesis. In OOPSLA.
[35]Veselin Raychev, Martin T. Vechev, and Eran Yahav. 2014. Code completion
with statistical language models. In ACM SIGPLAN Conference on Programming
LanguageDesignandImplementation,PLDI‚Äô14,Edinburgh,UnitedKingdom-June
09 - 11, 2014, Michael F. P. O‚ÄôBoyle and Keshav Pingali (Eds.). ACM, 419‚Äì428.
[36]Mohammad Raza, Sumit Gulwani, and Natasa Milic-Frayling. 2015. Composi-
tionalProgramSynthesisfromNaturalLanguageandExamples.In Proceedings
oftheTwenty-FourthInternationalJointConferenceonArtificialIntelligence,IJ-
CAI 2015, Buenos Aires, Argentina, July 25-31, 2015, Qiang Yang and Michael J.
Wooldridge (Eds.). AAAI Press, 792‚Äì800.
[37]NilsReimersand IrynaGurevych.2019. Sentence-BERT: SentenceEmbeddings
using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-
piricalMethodsinNaturalLanguageProcessing.AssociationforComputational
Linguistics. https://arxiv.org/abs/1908.10084
[38]Ohad Rubin and Jonathan Berant. 2021. SmBoP: Semi-autoregressive Bottom-up
Semantic Parsing. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna
1230
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. Jigsaw: Large Language Models meet Program Synthesis ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T√ºr, Iz Beltagy, Steven Bethard,
RyanCotterell,TanmoyChakraborty,andYichaoZhou(Eds.).Associationfor
Computational Linguistics, 311‚Äì324.
[39]GerardSaltonandMichaelJMcGill.1986. Introductiontomoderninformation
retrieval. (1986).
[40]KensenShi,DavidBieber,andRishabhSingh.2020. TF-Coder:ProgramSynthesis
for Tensor Manipulations. CoRRabs/2003.09040 (2020).
[41]RishabhSinghandSumitGulwani.2015. PredictingaCorrectPrograminPro-
grammingbyExample.In ComputerAidedVerification-27thInternationalCon-
ference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015, Proceedings, Part I
(Lecture Notes in Computer Science, Vol. 9206), Daniel Kroening and Corina S.
Pasareanu (Eds.). Springer, 398‚Äì414.
[42]Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew
Richardson. 2020. RAT-SQL: Relation-Aware Schema Encoding and Linkingfor Text-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan
Jurafsky,JoyceChai,NatalieSchluter,andJoelR.Tetreault(Eds.).Association
for Computational Linguistics, 7567‚Äì7578.[43]Yifan Wu,Joseph M. Hellerstein,and ArvindSatyanarayan. 2020. B2:Bridging
Code and Interactive Visualization in Computational Notebooks. In UIST ‚Äô20:
The 33rd Annual ACM Symposium on User Interface Software and Technology,
VirtualEvent,USA,October20-23,2020 ,ShamsiT.Iqbal,KaronE.MacLean,Fanny
Chevalier, and Stefanie Mueller (Eds.). ACM, 152‚Äì165.
[44]XiYe,QiaochuChen,XinyuWang,IsilDillig,andGregDurrett.2019. Sketch-Driven Regular Expression Generation from Natural Language and Examples.
CoRRabs/1908.05848 (2019).
[45]Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,
JamesMa,IreneLi,QingningYao,ShanelleRoman,ZilinZhang,andDragomirR.
Radev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and
Cross-DomainSemanticParsingandText-to-SQLTask.In Proceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,October31-November4,2018,EllenRiloff,DavidChiang,JuliaHockenmaier,and
Jun‚Äôichi Tsujii (Eds.). Association for Computational Linguistics, 3911‚Äì3921.
[46]TonyZ.Zhao,EricWallace,ShiFeng,DanKlein,andSameerSingh.2021. Cal-
ibrate Before Use: Improving Few-shot Performance of Language Models. In
International Conference on Machine Learning.
1231
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:21:06 UTC from IEEE Xplore.  Restrictions apply. 