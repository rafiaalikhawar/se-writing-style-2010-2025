Automated Handling of Anaphoric Ambiguity in Requirements:
A Multi-solution Study
Saad Ezzini
University of Luxembourg
Luxembourg
saad.ezzini@uni.luSallam Abualhaija
University of Luxembourg
Luxembourg
sallam.abualhaija@uni.lu
Chetan Arora
Deakin University
Australia
chetan.arora@deakin.edu.auMehrdad Sabetzadeh
University of Ottawa
Canada
m.sabetzadeh@uottawa.ca
ABSTRACT
Ambiguity is a pervasive issue in natural-language requirements.
A common source of ambiguity in requirements is when a pronoun
is anaphoric. In requirements engineering, anaphoric ambiguity
occurs when a pronoun can plausibly refer to di erent entities and
thus be interpreted di erently by di erent readers. In  this paper, we
develop an accurate and practical automated approach for handling
anaphoric ambiguity in requirements, addressing both ambiguity
detection and anaphora interpretation. In view of the multiple com-
peting natural language processing (NLP) and machine learning
(ML)technologies that one can utilize, we simultaneously pursue
six alternative solutions, empirically assessing each using a col-
lection of â‰ˆ1,350 industrial requirements. The alternative solution
strategies that we consider are natural choices induced by the exist-
ing technologies; these choices frequently arise in other automation
tasks involving natural-language requirements. A side-by-side em-
pirical examination of these choices helps develop insights about
the usefulness of di erent state-of-the-art NLP and ML technologies
for
 addressing requirements engineering problems. For the ambigu-
ity detection task, we observe that supervised ML outperforms both
a large-scale language model, SpanBERT (a variant of BERT), as
well as a solution assembled from o -the-shelf NLP coreference re-
solvers. In contrast, for anaphora interpretation, SpanBERT yields
the most accurate solution. In our evaluation, (1) the best solu-
tion for anaphoric ambiguity detection has an average precision of
â‰ˆ60% and a recall of 100%, and (2) the best solution for anaphora
interpretation (resolution) has an average success rate of â‰ˆ98%.
KEYWORDS
Requirements Engineering, Natural-language Requirements, Ambi-
guity, Natural Language Processing (NLP), Machine Learning (ML),
Language Models, BERT.
ACM Reference Format:
Saad Ezzini,  Sallam Abualhaija, Chetan Arora, and Mehrdad Sabetzadeh.
2022. Automated Handling of Anaphoric Ambiguity in Requirements: A
This work is licensed under a Creative Commons Attribution International 4.0 License.
ICSE
 â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510157Multi-solution Study. In 44th International Conference on Software Engineer-
ing(ICSEâ€™22),May21â€“29,2022,Pittsburgh,PA,USA. ACM,NewYork,NY,
USA, 13 pages. https://doi.org/10.1145/3510003.3510157
1 INTRODUCTION
Natural language (NL) is the most common medium for specifying
systems and software requirements. NL enables communication
between stakeholders who may have different backgrounds, often
requiring little or no additional training [ 71]. NL requirements
are nonetheless prone to defects such as ambiguity [ 10,20,71].
Ambiguity occurs when a word, phrase or sentence is open to
multipleinterpretations[ 69].Ambiguitycanhaveanegativeimpact
onthequality ofrequirementsandalsopotentiallyjeopardize the
success of a project [ 24,41]. A common cause of ambiguity in
requirements is anaphora [28, 39, 81, 95].
Anaphora means repetition in Greek and is defined as refer-
ences toentities mentioned earlierin the text. These references are
calledanaphors andtheentitiestowhichtheyreferarecalled an-
tecedents [60].Anaphoricambiguityoccurswhenthereismorethan
oneplausibleantecedent[ 24,61].Inlinguistics,thereareseveral
typesofanaphora[ 60].Inrequirementsengineering(RE),anaphora
is typically scoped to pronominal anaphora, i.e., when the anaphor
is apronoun[25,95]. This is because pronominal anaphora has
been clearly established as a genuine source of ambiguity in re-
quirements[ 39].Anaphoricambiguitydetection inREisthusthetask
ofidentifyingambiguousoccurrencesofpronouns[ 94].Theclosely
related task of anaphora resolution (interpretation) is concerned
with finding the most likely antecedent for a given pronoun [61].
Toillustrate,considertheexampleinFigure1.Here,theanaphor
isit,occurringinthesecondsentence.Thepotentialantecedents
are the preceding noun phrases (NPs), namely â€œthe S&T compo-
nentâ€, â€œapproval requestsâ€, â€œthe DBSâ€, â€œthe requestâ€ and â€œstorageparametersâ€. The pronoun itis unlikely to refer to â€œapproval re-
questsâ€ or â€œstorage parametersâ€ due to number disagreement (here,
singular pronoun versus plural NPs). Similarly, itis unlikely to
refertoâ€œtherequestâ€,since itisthesubjectoftheverbâ€œcreateâ€,and
â€œthe requestâ€ is not a suitable replacement for the subject of this
verb.Itisnotentirelyclearthoughwhether itreferstoâ€œtheS&T
componentâ€ or â€œthe DBSâ€. Depending on which antecedent â€“ â€œthe
S&T componentâ€ or â€œthe DBSâ€ â€“ is selected, there are two differentinterpretationsastowhichsubsystemshouldcreateaconfigurationrecord.Toproperlydealwiththissituation,thepronoun ithastobe
1872022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ezzini, et al.
either detected as ambiguous or resolved as referring to the correct
antecedent,whichhappenstobeâ€œtheS&Tcomponentâ€.Wenote
thatidentifyingthecorrectantecedentinthisexamplewouldlikely
be impossible without domain knowledge.
?
x?The S&T component shall send all approval requests to the DBS. 
If the request contains storage parameters, it shall create a 
conï¬guration record from the parameters.xx
â€œS&Tâ€ and â€œDBSâ€ stand for â€œSurveillance and Trackingâ€ and â€œDatabase Serverâ€, respectively.
Figure 1: Example of Anaphoric Ambiguity.
In RE, unconscious disambiguation of requirements is common,
particularly by stakeholders who have domain knowledge or a
good understanding of the system under development [ 78]. Not all
stakeholders in a project, however, share the same level of domain
knowledge or familiarity with the system-to-be. For example, it
isknownthatdevelopersarepronetointerpretingrequirements
inaccurately[ 63].Toreducethepotentialformisinterpretationand
the ensuing consequences, it is desirable to deal with ambiguityin requirements as early as possible. Typically, and somewhat incontrast to the literature on Natural Language Processing (NLP),
REprioritizesambiguitydetectionoveranaphoraresolution.The
rationaleisthatanygenuineambiguityinrequirementsneedstobe
inspected by human analysts and mitigated by rephrasing or other
means, as opposed to the ambiguity being subjected to automated
interpretation, i.e., what is commonly done in NLP [25].
AnaphoricambiguityisprevalentinNLrequirements.Estimates
fromtheREliteraturesuggestthatnearly20%ofindustrialrequire-mentscontainanaphora[
25,81].CurrentREresearchonanaphoric
ambiguity [ 7,22,25,94,95], as we elaborate in Section 2.2, does
not adequately explore two important facets. First, the existing
work relies primarily on the traditional methods in NLP and ma-
chine learning (ML). With the rapid emergence and adoption of
new technologies such as pre-trained language models, BERT [ 18]
being a notable example, the landscape for the processing (and
generation)ofNLcontenthaschangeddrastically.This,ontheone
hand,providesanopportunitytodevelopnewsolutions,and,on
the other hand, necessitates a revamp and reexamination of the
existing solutions, now using better enabling technologies. Second,
theexistingREsolutionsforanaphoricambiguityhavebeeneval-
uated on either a single application domain (e.g., railway) or on
verysmalldatasets.Assuch,empiricalresultsremainscarceonthe
usefulnessofautomatedtechniquesfordealingwithanaphorain
requirements documents.
On the surface, it may seem that one can readily adopt exist-
ing solutions from the NLP community to deal with anaphoric
ambiguity in requirements. In the NLP literature, anaphora is typi-
callyaddressedaspartof coreferenceresolution,whichisconcerned
with finding mentions that refer to the same entity in a given
text[38,76].Coreferenceresolutionisoftenanintermediatestep
for more advanced NLP tasks such as question answering and sen-
timentanalysis[ 61].Asnotedearlier,inRE,weprioritizedetection
over resolution, since we want to bring ambiguous cases to the
analystsâ€™ attention for further examination. Existing coreferenceresolvers have not been built to support ambiguity detection, thus
complicating the application of an individual resolver for this task.
Our aim in this paper is to arrive at a practical and effective
solution for handling anaphoric ambiguity in textual requirements.
Byâ€œhandlingâ€ anaphoricambiguity,we meanthe primary taskof
detecting genuine cases of anaphoric ambiguity and the secondary
taskofinterpreting(resolving)anaphorawhentheriskofambiguity
is sufficiently low. We achieve our aim by empirically investigat-
ingmultiplesolution strategies. Some of the investigated strate-
gies are new and some are adaptations of existing work that are
implementedusingstate-of-the-arttechnologies.Thealternative
strategiesconsideredarechoicesthat,inourexperience,recurrently
arisewhenengineeringrequirementsautomationsolutionsusing
NLPandML.Thesechoicesparticularlyinclude:(1)whethertouse
hand-craftedlanguagefeatures,wordembeddingsoracombination
thereof for classification, (2) whether pre-trained language models
likeBERT area viablereplacement forthe moretraditional tech-
niques, and (3) whether a mashup of existing (and often generic)
NLP tools would be adequate for specific RE tasks.
Our decision to examine and report on multiple solution strate-
giesismotivatedbybuildingempiricalinsightsaboutthementionedchoices. Naturally, our findings in this paper are limited to the taskathand,i.e.,handlinganaphoricambiguity.Nonetheless,webelieve
that our mode of investigation contributes to establishing a frame-
work for comparing the choices available in other requirements
automation tasks that are addressed via NLP and/or ML.
Contributions. This paper makes the following contributions:
(1) We develop six alternative solutions for automated han-
dling of anaphoric ambiguity in requirements. The solutions span
bothtraditionalaswellasmorerecentlyestablishedNLPandML
technologies.WeimplementallsixsolutionsusingJupyterNote-
books [42], and make the solutions publicly available1.
(2)Weempiricallyevaluatetheabove-mentionedalternativeson
twoindustrialdatasets.Thefirstdatasetisapre-existingone[ 2],
containing 98 requirements with 109 pronoun occurrences. The
seconddatasetwascuratedaspartofourworkusingthird-party
(non-author)annotators.Thisseconddatasetisacollectionof22
industrial requirements specifications from eight different applica-
tion domains and containing a total of 1,251 requirements with 737
pronoun occurrences. Over these datasets, for detecting anaphoric
ambiguity, supervised ML classification yields the best results with
an average precision of â‰ˆ60% and a recall of 100%. As for anaphora
resolution,afine-tunedlanguagemodelfromtheBERTfamilyof
modelsturnsouttobethebestsolutionwithasuccessrateof â‰ˆ98%.
Thefactthatdifferentbestsolutionsemergefortwocloselyrelated
tasks further signifies the usefulness of running multi-solution
studies like ours.
Significance. The significance of our work is two-fold: (1) ambi-
guity handling is a major concern in RE. We devise an accurate
automated solution to address a prevalent (and problematic) ambi-
guitytype,namelyanaphoricambiguity;(2)theNLPlandscapehas
evolveddrasticallyinrecentyears.Comparingthemoretraditional
techniques against new advancements is beneficial and relevant to
manyAI-basedREautomationtasksbeyondambiguityhandling.
1https://tinyurl.com/mww2w46t
188Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Wedemonstratehowsuchcomparisonscanbemadesystematically.
We further provide insights and lessons learned, and shed light on
potential challenges.
Structure. Section 2 discusses background and positions our work
against the related literature in NLP and RE. Section 3 presents
ouralternativesolutionsforhandlinganaphoricambiguityinre-
quirements.Section4reportsonourempiricalevaluation.Section5
addresses threats to validity. Section 6 concludes the paper.
2 BACKGROUND AND RELATED WORK
Thissectionpresentsthenecessarybackgroundforoursolutions
and further discusses the related literature in RE and NLP.
2.1 Background
Below, we discuss the enabling technologies used by our solutions
marked xto}in Figure 3. The precise design of these alternative
solutions will be elaborated in Section 3.
Language Models (LMs). LMs are statistical models that assign
probabilitiestowordsorphrasesgivensometrainingcorpus.For
example, an LM would assign a higher probability to the phrase
â€œbriefed reporters onâ€ than the phrase â€œbriefed to reportersâ€ [ 37].
BERT, standing for Bidirectional Encoder Representations from
Transformers,isapre-trainedmaskedlanguagemodel(MLM)in-
troduced by Devlin et al. [ 18]. MLMs randomly mask a fraction of
thetokensinthepre-trainingtext(theBooksCorpusandEnglish
WikipediainthecaseofBERT);thepre-trainingobjectiveisthen
to predict the original vocabulary of these masked tokens based
on the surrounding context. For example, BERT should predict the
masked token â€œbriefedâ€ in the phrase â€œ[MASK] reporters onâ€.
Pre-trained LMs can be employed to directly solve downstream
NLP tasks such as anaphoric ambiguity handling (the focus of our
work). We integrate LMs into our solutions using two strategies.
Thefirststrategyisto fine-tune theparametersofapre-trainedLM
onalabeleddatasetforanaphoricambiguityhandling.Weapply
thisstrategytodevisesolutions xandybasedonSpanBERT[ 36],
a variant of BERT. In contrast to BERT, SpanBERT is pre-trained to
predict masked text spans (rather than masked tokens). SpanBERT
is better suited than BERT for tasks such as anaphora resolution
andquestionansweringwheretheoutputisatextspan,e.g.,anoun
phrase rather than an individual noun [ 44]. The second strategy is
toextractcontextualembeddings fromthepre-trainedLMand use
theseembeddings aslearning featuresin ML-based textclassifica-
tion.Embeddingsaremathematicalrepresentationscapturingthe
syntactic and semantic characteristics of text. For developing solu-
tion{, we use embeddings from both BERT [ 18] and SBERT [ 77].
WhileBERTderivesembeddingsforindividualtokens,SBERTis
optimized for deriving semantically meaningful embeddings for an
entire text sequence.MachineLearning(ML).
SupervisedML(includingtextclassifica-
tion[87])requireslabeleddataconsistingofdatapointsdescribed
as a set of features and a class label. Using this labeled data, an
ML classifier is trained to discriminate among the different classes
based on the features. Subsequently, the classifier will be able to
predict the class of a previously unseen datapoint described by the
features. For text classification, different types of learning featurescanbeused[ 3].Amongthem,weapplyinourworkbothmanually-
craftedfeaturescollectedfromtheliteratureaswellascontextual
embeddings, presented earlier.
Solutions zand{â€“andalso |whichisacombinationof z
and{â€“ are ML-based. In our labeled data, each datapoint is the
combination of a pronoun and a candidate antecedent, both occur-
ring in some context. Each datapoint is labeled correct,incorrector
inconclusive,asweexplaininSection3.Ourempiricalevaluation
examines several widely used ML classification algorithms, namely
decisiontree(DT),feed-forwardneuralnetwork(FNN),k-nearest
neighbour(kNN),logisticregression(LR),naÃ¯veBayes(NB),ran-
domforest(RF)andsupportvectormachine(SVM).Wereferthe
readertotextbooksformoredetailsaboutthesealgorithms[ 30,91].
Natural Language Processing (NLP) Pipeline. In our work, we
applyanNLPpipelinecomposedofeightmodules:(1) tokenizer for
splittingthetextintotokens;(2) sentencesplitter forbreakingup
the text into individual sentences; (3) part-of-speec h (POS) tagger
for assigning a POS tag, e.g., noun, verb or pronoun, to each to-
kenineachsentence;(4) lemmatizer foridentifyingthecanonical
form (lemma) of each token, e.g., the lemma for â€œplayingâ€ is â€œplayâ€;
(5)constituency parser for identifying the structural units of sen-
tences,e.g.,NPs;(6) dependencyparser fordefiningthegrammatical
dependenciesbetweenthetokensinsentences;(7) coreferencere-
solverforfindingmentionsthatrefertothesametextualentity;and
finally, (8) semantic parser for extracting information about wordsâ€™
meanings. Modules 1 to 6 are prerequisites for all our solutions
(see the preprocessing step in Section 3.2). Our ML-based solutions
additionally use modules 7 and 8 for extracting language features.
Module 7 is the basis for solution }.
2.2 Related Work
Ambiguityinnaturallanguagehasbeenstudiedextensively[ 28,
54,61]. In RE, different dimensions of ambiguity have been ex-
plored, including understanding the significance of ambiguity in
requirements [ 24,28,32,78,84], analyzing the linguistic causes of
ambiguity [ 10,21,39,54], ambiguity prevention [ 4,5,55,58,80],
and ambiguity detection and resolution [ 5,17,20,23,25,29,40,43,
64,81,83,88,90,95]. Below, we discuss related work on anaphoric
ambiguity detection and anaphora resolution, covering both the
RE and NLP communities.
InRE,anaphoricambiguityhasbeenaddressedonlytoalimited
extent, despite (pronominal) anaphora being a common source
of misunderstandings in requirements [ 28]. Yang et al. [ 94,95]
propose an ML-based solution over language features for detecting
cases of anaphoric ambiguity leading to misunderstandings. Using
200 anaphoric pronouns from different domains, they report an
accuracyof â‰ˆ76%for classifyingwhetheran antecedentis correct
fora givenpronoun.Detecting potentialanaphoricambiguity hasalso been addressed as a sub-topic of defects detection, with some
basic solutions having been proposed, e.g., generating potential
ambiguity warningsfor allpronouns or onlyfor pronounswhose
surroundingtextmatchessomesimplesyntacticpatterns[ 5,28,81].
The approaches outlined above have two limitations. First, they
arebasedontraditionaltechnologiesfromNLPandMLâ€“twofields
thathaveadvancedsignificantlyoverthepastfewyears.Second,
theseapproacheshavebeenevaluatedonsmalldatasetsor single
189ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ezzini, et al.
r2 [If the request contains storage parameters, it shall create a 
conï¬guration record from the parameters. ] r1 [ The S&T component shall send all approval requests to the DBS. ]Context
PronounCandidate antecedentsc1
p1a11 a12 a13
a14 a15
Figure 2: Illustration of our Notation.
domains. We address the first limitation by (i) devising solutions in
viewofrecentadvancesinNLPandML,particularlytheemergence
of pre-trained language models; and (ii) re-examining the state-of-
the-artapproachbyYangetal.[ 94,95],enhancedwithseveralnew
language features gleaned from the literature [ 13,19,47,56,62,
75]. To address the second limitation, we conduct a multi-solution
empirical study includinga relatively large RE datasetthat covers
eight different application domains.
IntheNLPcommunity,dealingwithanaphoraisalong-standing
problem [ 61]. As already noted in the introduction section, com-
pared to RE, the focus in NLP is primarily on anaphora resolu-tion, given the needs of the NLP tasks that are further down-
stream[47,70].Despitenumerousattemptsataddressinganaphora
resolution, the complex nature of the task has slowed progress for
severalanaphoratypes[ 86].Theanaphoraresolutiontechniques
in the NLP community are broadly classified into three categories:
syntactic,semanticandneural-network-based[ 46].Thesyntactic
and semantic approaches focus on designing ML features based
on grammatical structureand wordmeanings insentences. Inthe
neural-network-based approaches, anaphora resolution is often
reformulatedasaquestion-answeringproblem.Recentsolutionsin
this category achieve promising results [33, 93].
In addition to being focused on resolution, the techniques devel-
opedbytheNLPcommunityaretrainedongenericcorpora,e.g.,
Wikipedia. Due to the major differences between the terminology
and style applied in requirements writing versus what is avail-
able in generic corpora [ 26], NLP tools usually do not work well if
appliedas-istorequirementsdocuments[ 20,90].Toaddressthis
problem,wecollectandannotate,aspartofourwork,adatasetofin-
dustrial requirements. Taking inspiration from the state-of-the-art
NLP directions, we build multiple solutions for handling anaphoric
ambiguity, while ensuring that anaphoric ambiguity detection is
explicitly addressed and prioritized over anaphora resolution.
3 SOLUTIONS DESIGN
Westartthissectionbydefininginananalyticalmanneranaphoric
ambiguitydetectionandanaphoraresolution.Thisisfollowedby
a discussion of the preprocessing required for automating thesetasks. We then present the design of six alternative solutions for
automatedhandlingofanaphoricambiguityinrequirements;these
solutions will be tuned and evaluated in Section 4.
3.1 Problem Definition
LetR=(ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿ ğ‘›)beasequenceofrequirements,whereeach ğ‘Ÿğ‘–
representsasinglerequirementssentence.Let P=(ğ‘1,ğ‘2,...,ğ‘ ğ‘š)be all the pronouns in Rin their order of appearance. Following
best practice [ 95], we define the context ğ‘ğ‘—of a pronoun ğ‘ğ‘—as two
consecutive sentences ğ‘ğ‘—=(ğ‘Ÿğ‘–âˆ’1,ğ‘Ÿğ‘–);2â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š,where
ğ‘Ÿğ‘–is the sentence in which ğ‘ğ‘—occurs. If ğ‘ğ‘—occurs in ğ‘Ÿ1, then the
context is one sentence only, i.e., ğ‘ğ‘—=(âˆ’,ğ‘Ÿ1). Each pronoun occur-
rence is represented by a distinct ğ‘ğ‘—âˆˆP. This means that multiple
occurrencesofthesamepronounconstitutedifferentelementsin
P,evenwhentheoccurrencesarewithinthesamesentence.For
eachğ‘ğ‘—âˆˆP, the context of ğ‘ğ‘—, i.e.,ğ‘ğ‘—, induces a set of candidate
antecedents denoted Ağ‘—={ğ‘ğ‘—1,ğ‘ğ‘—2,Â·Â·Â·,ğ‘ğ‘—ğ‘¡}.
To illustrate our notation, we recall the example of Figure 1.
Letğ‘Ÿ1andğ‘Ÿ2be the two consecutive sentences in that example.
Then,R=(ğ‘Ÿ1,ğ‘Ÿ2). There is only one pronoun in R; therefore,
P=(ğ‘1)whereğ‘1=it. The context for ğ‘1isğ‘1=(ğ‘Ÿ1,ğ‘Ÿ2), and the
setofcandidateantecedentsfor ğ‘1isA1={ğ‘11,ğ‘12,ğ‘13,ğ‘14,ğ‘15}
whereğ‘11=â€œthe S&T componentâ€ ,ğ‘12=â€œapproval requestsâ€ ,ğ‘13=
â€œthe DBSâ€ ,ğ‘14=â€œthe requestâ€ andğ‘15=â€œstorage parametersâ€ . For
easier referral later in the paper, we visually show in Figure 2 how
our notation is applied to the example of Figure 1.
Using our notation, anaphoric ambiguity detection is to decide
whetheragivenpronounoccurrence ğ‘ğ‘—isambiguous orunambigu-
ousin its context ğ‘ğ‘—.Anaphora resolution is to identify the most
likely antecedent for ğ‘ğ‘—.
3.2 Preprocessing
The preprocessing step generates the input for the alternative am-
biguity handling solutions that we consider inthis paper. We first
applytheNLPpipeline,discussedinSection2.1,onagivenrequire-
ments specification (RS) to parse its textual content. We create the
listofallpronouns(i.e., P)occurringinRS;thisisdonebyselecting
the words that the POS tagger marks as PRP(personal pronoun)
orPRP$ (possessive pronoun) [ 52]. For each ğ‘ğ‘—âˆˆP, we identify
the context ğ‘ğ‘—as the requirement ğ‘Ÿğ‘–in which ğ‘ğ‘—occurs and the
preceding requirement ğ‘Ÿğ‘–âˆ’1(forğ‘–â‰¥2). Finally, for each ğ‘ğ‘—,w e
generatethesetofallcandidateantecedents Ağ‘—.Sinceantecedents
are NPs, as noted in Section 1, we generate Ağ‘—by including all
NPs that precede ğ‘ğ‘—inğ‘ğ‘—, as automatically identified by the con-
stituency parser module in the NLP pipeline. We further include
anysegmentfollowingthepattern [NPand/orNP] (e.g.,â€œthesender
and the receiverâ€) and [NP preposition NP] (e.g., â€œthe component of
thesystemâ€).Doingsoimprovesthesetofcandidateantecedents
by covering the cases where ğ‘ğ‘—refers to a compound NP [6, 95].
3.3 Alternative Solutions
Weconsidersixalternativesolutionsforhandlinganaphoricam-
biguity. These are shown in Figure 3. Alternatives xandyare
based on SpanBERT; alternatives z,{and|are based on super-
vised ML; and, alternative }is based on existing NLP coreference
resolvers. We note that the expected input differs across solutions:
The solutions based on SpanBERT take as input tuples of the form
/angbracketleftğ‘ğ‘—,ğ‘ğ‘—/angbracketright; the ML-based solutions take as input triples of the form
/angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketright; and,the NLP-basedsolution takeas inputmerely the
context information( ğ‘ğ‘—) for pronoun occurrences. The input forall
solutions is directly constructible from the preprocessing results.
Table 1 outlines for each solution the inputs, the intermediate
outputs and the rules for processing the intermediate outputs. The
190Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Input RS
Preprocess
Ambiguous?
Most likely antecedent
A: Tuples of the form                 (context and pronoun)         B: Triples of the form                          (context, pronoun, candidate antecedent)                 C: Contexts       of pronoun occurrences
LFs: language features         FEs: feature embeddings/angbracketleftcj,pj,ajk/angbracketright cj[CLS]c j[SEP] pj
NLPCoref6(iii) NLP-based
Coref1 Coref2
C
MLLFMLFEExtract 
Features(ii) ML-based
MLensemble
34
5LFsFEs BC
Ambiguous?
Most likely antecedentAmbiguous?
Most likely antecedentAmbiguous?
Most likely antecedentOutputp1p2...{
Legendpm
/angbracketleftcj,pj/angbracketrightSpanBERTREPre-trained 
SpanBERT
SpanBERTNLPEncode Input(i) SpanBERT-based
CoNLL2011Fine-tune 
(1)
Fine-tune 
(2)DAMIRT
2 1A
Figure 3: Overview of Solution Alternatives (marked xto}).
Table 1: Inputs, Intermediate Outputs and Ambiguity-handling Rules for Solution Alternatives.
Alternative(s) Input (I), Intermediate Output (O) and Ambiguity Handling Rules (R)
xy I:/angbracketleftğ‘ğ‘—,ğ‘ğ‘—/angbracketrighttuples.O:Tuplesoftheform /angbracketleftğ‘ ğ‘,ğ‘ğ‘Ÿğ‘/angbracketright,whereğ‘ ğ‘isatextspanand ğ‘ğ‘Ÿğ‘istheprobabilityof ğ‘ ğ‘beingtheantecedent
forğ‘ğ‘—.R:(AnaphoraResolutionâˆ—)Forapronoun ğ‘ğ‘—,ifthereisexactlyone ğ‘ ğ‘inğ‘ğ‘—suchthat ğ‘ğ‘Ÿğ‘isâ‰¥afixed(empiricallytuned)
threshold, then ğ‘ ğ‘is the most likely antecedent of ğ‘ğ‘—.(Ambiguity Detection) If suchğ‘ ğ‘is identified, then ğ‘ğ‘—isunambiguous ;
otherwise ğ‘ğ‘—isambiguous.
z{| I:/angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketrighttriples.O: Tuples of the form /angbracketleftâ„“ğ‘—ğ‘˜,ğ‘ğ‘Ÿğ‘—ğ‘˜/angbracketright, whereâ„“ğ‘—ğ‘˜is a label characterizing the referential relation between
ğ‘ğ‘—ğ‘˜andğ‘ğ‘—inğ‘ğ‘—andwhere prğ‘—ğ‘˜isthepredictionprobabilityfor â„“ğ‘—ğ‘˜.Foranaphoraresolution,thelabelsadmittedby â„“ğ‘—ğ‘˜
arecorrectandincorrect; for ambiguity detection, â„“ğ‘—ğ‘˜additionally admits inconclusive. R:(Anaphora Resolutionâˆ—) For a
givenğ‘ğ‘—,ifthereisexactlyone ğ‘ğ‘—ğ‘¥suchthat â„“ğ‘—ğ‘¥=correctwithanyprobability,then ğ‘ğ‘—ğ‘¥isthemostlikelyantecedentof
ğ‘ğ‘—. Otherwise, if multiple â„“ğ‘—ğ‘˜are predicted as correctforğ‘ğ‘—, then we deem ğ‘ğ‘—â€™s most likely antecedent to be ğ‘ğ‘—ğ‘¥where
ğ‘¥is the index at which â„“ğ‘—ğ‘¥has the highest probability prğ‘—ğ‘¥.(Ambiguity Detection) For a given ğ‘ğ‘—, if there is exactly one
labelâ„“ğ‘—ğ‘¥=correct, thenğ‘ğ‘—isunambiguous if, additionally, either of the following conditions hold: (a) prğ‘—ğ‘¥isâ‰¥a fixed
(empirically tuned) threshold, or (b) there is no label â„“ğ‘—ğ‘˜that isinconclusive. Otherwise, ğ‘ğ‘—isambiguous.
} I:Contexts( ğ‘ğ‘—)ofpronounoccurrences. O:Eachpronounoccurrence ğ‘ğ‘—alongsidementions ğ‘š1andğ‘š2foundbyCoref 1and
Coref2,respectively. R:(AnaphoraResolutionâˆ—)Ifğ‘š1=ğ‘š2,thenğ‘š1(=ğ‘š2)isthemostlikelyantecedentof ğ‘ğ‘—.(Ambiguity
Detection) Ifanantecedentisidentifiedbytheanaphoraresolutionrule,then ğ‘ğ‘—isunambiguous ;otherwise, ğ‘ğ‘—isambiguous.
*If no anaphora resolution rule is triggered for a given pronoun occurrence, then no antecedent is predicted.
rules produce the final results for anaphora resolution and ambigu-
ity detection. We elaborate our alternative solutions next.
(i)SolutionsbasedonSpanBERT. Weemploytherecentlanguage
modelSpanBERT[ 36],introducedinSection2.1,todevelopsolu-
tionsxandy, referred to as SpanBERT NLPand SpanBERT RE,
respectively. We first fine-tune the pre-trained SpanBERT model to
generate SpanBERT NLPusing the CoNLL2011 dataset [34,53,72]
â€“ a large dataset of generic text with about 7,000 pronoun occur-
rences.Thisfine-tuningstepâ€“fine-tune(1)inFigure3â€“aimsto
adjust the parameters of the general SpanBERT model using the
inputsand outputs of CoNLL2011 ontheanaphora resolutiontask.
Next, we fine-tune SpanBERT NLPto generate SpanBERT REon a
subsetof DAMIRâ€“adatasetofNLrequirements,whichwehave
constructedaspartofourwork.Thesecondfine-tuningâ€“fine-tune
(2)inFigure3â€“enhancesSpanBERT NLPbyexposingittoexamples
ofambiguousandunambiguouspronounsfromtheREdomain.Thehypothesis we would like to examine using the resulting solution,
i.e., SpanBERT RE, iswhether requirements-specificknowledge im-
provestheaccuracyofanaphoricambiguityhandlinginRE.The
CoNLL2011 andDAMIRdatasets are discussed in Section 4.3.
TheinputtoBERTanditsvariantsneedstobetokenizedanden-
codedintothesameformatusedbythepre-trainedmodels.Specifi-
cally,weencodeeachtuple /angbracketleftğ‘ğ‘—,ğ‘ğ‘—/angbracketrightas[CLS]ğ‘ğ‘—[SEP]ğ‘ğ‘—.Twospecial
tokensareautomaticallyaddedbyBERTâ€™stokenizer: [CLS]torepre-
senttheclassificationoutputand [SEP]toseparate ğ‘ğ‘—fromğ‘ğ‘—.Any
repeated occurrence of the same pronoun ğ‘ğ‘—is replaced with ğ‘ğ‘—#ğ‘‘,
whereğ‘‘â‰¥1 is a unique identifier. The multiple occurrences are
thenencodedas [CLS]ğ‘ğ‘—[SEP]ğ‘ğ‘—#ğ‘‘.The[CLS]tokenisrelevantfor
SpanBERTâ€™spre-training,whichisnotpartofouranalysis.[CLS]
thus has no significance for our analytical purposes.
The SpanBERT-based solutions, xandy, handle ambiguity us-
ing the respective rules provided in Table 1. There is a threshold
191ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ezzini, et al.
ğœƒğ›¼forcontrollingtheresolutionresults.Werecommend ğœƒğ›¼=0.9
basedonourtuning,discussedinSection4.5.FortheexampleinFig-
ure2,theinputto xandyis/angbracketleftğ‘1,ğ‘1/angbracketright,encodedas [CLS]ğ‘1[SEP]ğ‘1.
The intermediate output of both solutions would be a tuple like
/angbracketleftğ‘ 1=â€œthe S&T componentâ€ ,pr1=0.99/angbracketright. The text span ğ‘ 1would be
theantecedentof ğ‘1,sinceitisidentifiedwithaprobability â‰¥0.9.
Thus,ğ‘1would be detected as unambiguous. Note that xandy
dononecessarilydemarcateallpossibletextspansin ğ‘ğ‘—,butrather
only those that the solutions find relevant for anaphora resolution.
(ii) Solutions based on supervised ML. We refer to our three
ML-basedsolutions, z,{and|,asMLLF,MLFEandML ensemble,
respectively. ML LFis trained on 45 language features (LFs) col-
latedfromtheexisting NLPandREliteratureonanaphoraresolu-
tion[13,19,47,56,62,75,95].ThedescriptionoftheLFsisprovided
online [82]. MLFEis trained on feature embeddings (FEs) which are
contextual representations of the input, as explained in Section 2.1.
MLensembleis an ensemble classifier which combines the results of
MLLFand ML FE.
Each triple /angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketrightin the input to the ML-based solutions
needs to be transformed into a feature vector. ML LFis built over
45-dimensionalfeaturevectorsencodingtheLFs.Thevaluesforthe
LFs are computed using the NLP pipeline. The LFs characterize the
referentialrelationbetween ğ‘ğ‘—anditscandidateantecedent ğ‘ğ‘—ğ‘˜,e.g.,
numberagreementwhenbotharepluralsorsingulars.ML FEisbuilt
over768-dimensionalfeaturevectorsrepresentingtheFEsextractedfromBERT[
18]andSBERT[ 77].TheFEscapturethesemanticand
syntacticregularitiesofatextsequence[ 57].Thereareotherpre-
trained models, e.g., GloVe [ 67] and word2vec [ 59], that can be
usedforderivingtheFEs.WefavorembeddingsderivedfromBERT
(and SBERT), because these embeddings are contextual and known
to better capture sequence-level semantics, including referential
relations,whencomparedtothe(non-contextual)embeddingsfromGloVe and word2vec [
48]. In Section 4.5, we experiment with three
different ways of deriving FEs from BERT.
Foratriple /angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketright,theintermediateoutputoftheML-based
solutionsisapredictedlabelthatassumesoneofthefollowingthree
values:correct(meaningthat ğ‘ğ‘—referstoğ‘ğ‘—ğ‘˜),incorrect (meaning
thatğ‘ğ‘—does not refer to ğ‘ğ‘—ğ‘˜)o rinconclusive (meaning that the
referential relation between ğ‘ğ‘—andğ‘ğ‘—ğ‘˜is not clear enough to be
classified as either correctorincorrect). MLensemblegenerates its
intermediate output by combining the predictions from ML LFand
MLFE.I fM LLFand ML FEagree on the labelpredicted for a given
triple, then ML ensembleassigns this label to the triple as well. If
MLLFand ML FEdisagree, then ML ensembleassigns to the triple the
labelpredictedwiththehigherprobability,butonlyifthedifferencebetweenthetwoprobabilitiesisgreaterthanorequaltoathreshold
ğœƒğ›¿. If the probability difference falls short of ğœƒğ›¿, then ML ensemble
assigns the label inconclusive. Based on our tuning presented in
Section 4.5, we recommend ğœƒğ›¿=0.1.
For ambiguity detection, we train the classifiers underlying our
ML-based solutions on a subset of the DAMIRdataset, with data-
points covering all three outcome classes (correct, incorrectandin-
conclusive ).Doingsoenablestheclassifierstodistinguishunambigu-
ouscases(correct andincorrect)fromambiguousones(inconclusive ).
For anaphora resolution, we train the classifiers on only the dat-apoints labeled correctorincorrect. For this task, the datapointslabeledinconclusive are not useful and may even mislead the learn-
ing of correct and incorrect referential relations.
TherulesusedbyourML-basedsolutionsforambiguityhandling
are inspired by Yang et al. [95] and provided in Table 1. There is a
threshold ğœƒğ›½intherulesforcontrollingthedetectionresults.We
recommend ğœƒğ›½=0.5, based on the tuning results of Section 4.5. To
illustratetheML-basedsolutions,recalltheexampleofFigure2.Forthatexample,theinputwouldbefivetriples:
/angbracketleftğ‘1,ğ‘1,ğ‘1ğ‘˜/angbracketright;1â‰¤ğ‘˜â‰¤5.
For ambiguity detection, when trained and tuned as we explainin Section 4.5, ML
LFpredictsinconclusive for all triples, whereas
MLFEpredictsinconclusive forğ‘˜âˆˆ{1,2,5}andincorrectfortherest.
These predictions jointly lead to ML ensemblepredicting inconclusive
for all triples. Due to space, we do not show and argue through the
probabilityscoresthatML ensembleusesforderivingitsresultsfor
ourillustrativeexample.Whentheambiguity-handlingrulesareap-
plied to these intermediate results, none of the ML-based solutions
provide a resolution for ğ‘1, and all three detect ğ‘1asambiguous.
(iii) Solution based on NLP coreference resolvers. We refer to
our final solution, numbered }in Figure 3, as NLPcoref. This solu-
tion requires two independent coreference resolvers and can easily
be implemented using the NLP pipeline. Let us denote the two
resolversby Coref1andCoref2.NLPcoref,asshowninTable1,com-
binestheresultsof Coref1andCoref2viaconsensus.Weinstantiate
Coref1andCoref2usingtwopopularcoreferenceresolvers[ 14]:the
resolverintheCoreNLPtoolkit[ 15,16]andtheoneintheSpaCy
library [31]. For the example of Figure 2, NLPcorefresolvesğ‘1as
referring to ğ‘14(â€œthe requestâ€), thus deeming ğ‘1asunambiguous.
4 EMPIRICAL EVALUATION
In this section, we tune and assess the alternative solutions pre-
sented in Section 3.
4.1 Research Questions (RQs)
Ourevaluationtacklesthefollowingthreeresearchquestions(RQs):
RQ1. Which solution alternative is the most accurate for de-tecting anaphoric ambiguity in requirements?
By comparing
the accuracy of the alternative solutions in Figure 3, we identify, in
RQ1, the best-performing solution for detecting anaphoric ambigu-
ity in requirements.
RQ2. Which solution alternative is the most accurate for re-
solving anaphora in requirements? In RQ2, we identify among
thealternativesinFigure3,theonethatismostaccurateforresolv-
inganaphora.Havinganaccurateanaphoraresolverisbeneficial
for RE in at least two ways: First, during requirements reviews, the
machine-generatedinterpretationsareagoodindicatorfortherisk
ofmisunderstandings.Notably,iftherequirementsanalyst(s)settle
on an interpretation that differs from the one (if any) offered by
automated resolution, then there is an increased chance that other
stakeholders, e.g. developers, may misinterpret the anaphora in
question, withthis misinterpretationpotentially happeningmuch
laterinthedevelopmentprocessandthuspotentiallybeingmore
costlytofix.Second,forautomatedinformationextractionpurposes,e.g., the extraction of conceptual models from requirements [
8,79],
one would typically want to use the results of automated anaphora
resolution as-is and without additional manual processing.
192Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 2: Summary Statistics for our Datasets.
DAMIR ReqEval CoNLL2011
Unique Sentences 1,251 98 6,888
PronounsAmbiguous 342 62 -
Unambiguous 395 47 6,757
TriplesCorrect 404 66 6,866
Incorrect 2,814 104 14,666
Inconclusive 3,448 272 -
RQ3.Whatistheexecutiontimeofeachsolutionalternative?
Executiontimeisanimportantfactorforensuringpracticality.RQ3
examines the execution time of each of the alternatives in Figure 3.
4.2 Implementation and Availability
We use Python 3.8 [ 89] for implementing the preprocessing step
(Section3.2)aswellasforthehigh-levelscriptingofthealternative
solutionsshowninFigure3.TheNLPpipelineandlanguage-feature
extraction are implemented using SpaCy 3.0.5 [ 31], NLTK 3.5 [ 49],
Stanza 1.2 [ 73], and CoreNLP 4.2.2 [ 51]. The SpanBERT-based solu-
tions use the Transformers 4.6.1 library [ 92] provided by Hugging
Face (https://huggingface.co/) and operated in PyTorch [ 65]. For
the ML-based solutions, we use Scikit-learn 0.24.1 [ 66]. We use the
Transformers library for extracting embeddings. BERTâ€™s embed-
dings are extracted from the bert-base-cased model. For extract-
ing SBERTâ€™s embeddings, we use the paraphrase-MPNet-base-v2
model[85],alsoavailableintheTransformerslibrary.Finally,for
implementingthesolutionthatusesexistingNLPresolvers,weuse
the coreference resolution modules available in SpaCy 3.0.5 [ 31]
andCoreNLP4.2.2[ 15,16].Thedifferentsolutionsproposedinthis
paper are implemented using Jupyter Notebooks [42].
4.3 Datasets
We use three datasets in our evaluation. The first dataset has been
curatedusingtwoexternal(non-author)annotators,asweelaborate
momentarily.Wecallthisdataset DAMIR,whichstandsfor Dataset
forAnaphorica MbiguityInRequirements.Theothertwodatasets
are borrowed from the literature. These are CoNLL2011 [34,53,72],
the NLP dataset on coreference resolution released in the 2011 edi-
tion of the Computational Natural Language Learning conference
(CoNLL2011);and ReqEval[1],theREdatasetonanaphoricambi-
guity released in the 2020 edition of the NLP4RE workshop. Weuse theCoNLL2011 dataset for fine-tuning the SpanBERT-based
solutions.Weusethe ReqEvaldatasettoevaluatethesolutionalter-
natives.The DAMIRdatasetissplitintotwoportions,asweexplain
later;oneportionisusedfordevelopmentandtuning,andtheother
portion is used for evaluating the solution alternatives.
Table 2 provides summary statistics for DAMIRand the adapted
versionsof CoNLL2011 andReqEval.Specifically,thetableshows
the number of unique sentences in each dataset, the number of
pronouns marked as ambiguous andunambiguous, and the number
of triples marked as correct,incorrect andinconclusive. We discuss
thethreedatasetsnext.Notethatthenumberofcorrectantecedents
is greater than the number of unambiguous pronouns since thecorrectantecedentcanoccurinthecontextmultipletimes,inwhich
case it will be counted more than once.
DAMIR. We collected 22 industrial requirements specifications
(RSs) from eight application domains: satellite communications,
medicine,aerospace,security,digitization,automotive,railway,and
defence. The requirements in these specifications were indepen-
dently analyzed by two third-party annotators with expertise in
linguistics. The first annotator, who has a Masters degree in cul-
tural studies and multilingualism, had, prior to her engagement in
ourwork,doneasix-monthinternship,focusingoninvestigating
the linguistic characteristics of requirements. The second anno-
tator has a computer-science background with a Masters degree
inqualitymanagement.Thisannotatorfurtherhasaprofessional
certificate in English translation. Both annotators received train-
ingonanaphoricambiguityinrequirements.Theannotatorsâ€™workspannedtwomonths,withatotalof44and56hoursdeclaredbythe
annotators, respectively. To mitigate fatigue effects, the annotators
wereencouragedtolimittheirperiodsofworktotwohoursata
time.InadditiontotheoriginalRSs,wesharedwiththeannotators
the lists of automatically generated triples ( /angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketright).
The annotators were asked to examine the list of triples asso-
ciated with each pronoun occurrence ğ‘ğ‘—. If they were confident
that a candidate antecedent ğ‘ğ‘—ğ‘˜is the likely one in a triple, then
they were instructed to label that triple as correctand all other
triples involving ğ‘ğ‘—asincorrect. In case of doubt, the annotators
were asked to label all the triples involving ğ‘ğ‘—asinconclusive. The
annotators could also select the label invalidif some automatically
generated triple had an error caused by, e.g., inaccurate splitting of
the sentence constituents. All such invalid triples were filtered out.
To construct the DAMIR dataset, we checked the annotations
for the triples associated with each ğ‘ğ‘—. If the annotators agreed
that a triple should be labeled as correct(meaning that they also
agreedthattheothertriplesfor ğ‘ğ‘—shouldbelabeledas incorrect),
weconsidered ğ‘ğ‘—asunambiguous.Inthiscase,thetriplesassociated
withğ‘ğ‘—received the same labels as indicated by the annotators. If
the annotators disagreed on the label for any triple associated with
ğ‘ğ‘—,thenweregarded ğ‘ğ‘—asambiguous,andconsequently,labeled
all the associated triples as inconclusive. We identified two types of
disagreementbetweentheannotators:(i)oneannotatorfound ğ‘ğ‘—
ambiguous and labeled its triples as inconclusive, while the other
annotatorfound ğ‘ğ‘—unambiguous andlabeledsometripleas correct;
or (ii) the annotators labeled two different triples as correct, i.e.,
theyunconsciouslydisagreedontheinterpretation.Wedefineas
an agreement any case other than (i) and (ii) above. Using Fleissâ€™kappa (
ğœ…)[27], we obtain an inter-rater agreement of ğœ…=0.54,
which indicates moderate agreement [ 45] between the annotators.
Wenotethatfordatasetsrelatedtoambiguityanalysis,thislevelof
agreementistobeexpected[ 20],consideringthatdisagreements
are indicators for ambiguous cases.
We split the pronoun occurrences in DAMIRinto two disjoint
subsets:DAMIRğ‘‡andDAMIRğ¸. The contexts for the elements in
DAMIRğ‘‡arealsodistinctfrom thosefortheelementsin DAMIRğ¸,
i.e.,alltriplesassociatedwithapronoun ğ‘ğ‘—includingthecandidate
antecedents ğ‘ğ‘—ğ‘˜ofğ‘ğ‘—appearineither DAMIRğ‘‡orDAMIRğ¸butnot
in both.DAMIRğ‘‡contains 80% of the dataset and is used for devel-
oping and tuning the solutions. DAMIRğ¸contains the remaining
193ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ezzini, et al.
20%andisusedforevaluation.Ourempiricalevaluation,presented
in Section 4, is conducted using DAMIRğ¸only.
CoNLL2011. We extracted from the original CoNLL2011 dataset
only the annotations relevant to anaphoric ambiguity analysis,
i.e., the annotations where a pronoun has been labeled with the
antecedent it refers to. We used the source documents released
alongside CoNLL2011 in order toidentify a context of sizetwo for
each pronoun occurrence. To adapt this dataset to our work, we
generated /angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketrighttriplesthroughpreprocessing(Section3.2).
We then assigned labels to the triples in a backward manner: A
triple/angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketrightis labeled correctifğ‘ğ‘—ğ‘˜represents the selected
antecedentfor ğ‘ğ‘—.Otherwise,thetripleislabeled incorrect.Wenote
that no triple is marked as inconclusive here, since CoNLL2011 was
not created for ambiguity detection; all pronoun occurrences in
CoNLL2011 are regarded as unambiguous.
ReqEval. TheReqEvaldatasetiscomposedofasetofindependent
requirements,eachwithatleastonepronounoccurrence.Eachpro-
noun occurrenceis labeledas either ambiguous orunambiguous.I n
thelattercase,thecorrectantecedentisprovided.Toadapt ReqEval
to our work, we generated /angbracketleftğ‘ğ‘—,ğ‘ğ‘—,ğ‘ğ‘—ğ‘˜/angbracketrighttriples through preprocess-
ing.Incontrasttothe DAMIRandCoNLL2011 datasetswherewe
setthecontextsizetotwowhengeneratingthetriples,for ReqEval,
weuseacontextofsizeone.Thisisbecausewecouldnotascertain
that the requirements were in any particular order; a context be-
yond the immediate sentence where a pronoun appears was not
intended in ReqEval. For each ambiguous ğ‘ğ‘—, we assigned the label
inconclusive to all triples associated with ğ‘ğ‘—. For each unambiguous
ğ‘ğ‘—, we assigned the label correctto the triple where ğ‘ğ‘—ğ‘˜matches
the antecedent provided for ğ‘ğ‘—andincorrectto all other triples.
4.4 Evaluation Metrics
Anaphoric ambiguity detection. We evaluate ambiguity detec-
tion using precision (P), recall (R) and F ğ›½-scorecomputed as ğ‘ƒ=
ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ),ğ‘…=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘),andğ¹ğ›½=(1+ğ›½2)(ğ‘ƒâˆ—ğ‘…)/(ğ›½2âˆ—
ğ‘ƒ+ğ‘…),respectively.A truepositive(TP) isacasewherethesolution
correctlypredicts ğ‘ğ‘—asambiguous.A truenegative(TN) isacase
wherethesolutioncorrectlypredicts ğ‘ğ‘—asunambiguous.A false
positive(FP) isacasewherethesolutionfalselypredicts ğ‘ğ‘—asam-
biguous,anda falsenegative(FN) isacasewherethesolutionfalsely
predictsğ‘ğ‘—as unambiguous. As is common for many requirements
analysistasksincludingambiguityanalysis[ 11,95],wefavorrecall
over precision. We thus use and report F 2-scores (i.e., ğ›½=2).
Anaphoraresolution. Weevaluateresolutionusingthefollowing
metric, which we call success rate : the ratio of correctly resolved
pronoun occurrences to the total number of pronoun occurrences
labeled as unambiguous in the ground truth. We apply two modes
to decide whether the antecedent identified by a solution is correct
as per the ground truth. In the full matching mode, we consider
theidentifiedantecedenttobecorrectonlywhenitfullymatches
the text span in the ground truth. In the partial matching mode,
we consider the identified antecedent to be correct if it overlaps
with the text span in the ground truth. For example, the identified
antecedentâ€œallapprovalrequestsâ€comparedtoâ€œapprovalrequestsâ€
(inthegroundtruth)isconsideredascorrectlyresolvedinpartial
matching but not in full matching. The rationale for considering
partial matching is that, when the matching results are destinedfor a manual review, pinpointing the location of the text span of
interest is highly useful, even though the identified span may be
incomplete or only partially correct.
4.5 Solutions Tuning
Inthissection,wedescribethetuningofoursolutions.Theresulting
tuned solutions are used in Section 4.6 for answering RQ1-3.
Tuning SpanBERT. We fine-tune the SpanBERT-based solutions
tomaximizeF 2-scoreforambiguitydetection.Wefollowtherecom-
mendationsintheliteratureforfine-tuningpre-trainedlanguage
models [18,36,68]. To generate SpanBERT ğ‘ğ¿ğ‘ƒ(solution xin Fig-
ure 3), we fine-tune SpanBERT on the CoNLL2011 dataset for 20
epochs with 2e-5 learning rate and 32 batch size. We then generate
SpanBERT ğ‘…ğ¸(solution yinFigure3)byfine-tuningSpanBERT ğ‘ğ¿ğ‘ƒ
for 3 epochs on the DAMIRğ‘‡dataset with the same learning rate
and batch size as used in solution x.
We apply a threshold ğœƒğ›¼as the lower bound for accepting a text
span identified by solution xoryas the antecedent of a pronoun
occurrence(seeSection3.3).Wetune ğœƒğ›¼onDAMIRğ‘‡viaexhaustive
search.Specifically,weexperimentwith10values [0.1,0.2,Â·Â·Â·,1.0].
The optimal value is ğœƒğ›¼=0.9.
Tuning ML. We optimize ML LFand ML FE(solutions zand{in
Figure 3) on DAMIRğ‘‡. We consider different configurations that
arise from varying the ML classification algorithm and the FEs.
For both zand{, we experiment with seven widely used classifi-
cationalgorithms,namelydecisiontree(DT),feedforwardneural
network (FNN), k-nearest neighbor (kNN), logistic regression (LR),
naÃ¯ve Bayes (NB), random forest (RF) and support vector machine
(SVM) [50,74,95]. Following best practice [ 18,77], we explore
fouroptionsforextractingFEsforsolution {.Inthefirstoption,
FE1, the embeddings are extracted from SBERT. The other three
options,FE2â€“FE4,arebasedonembeddingsfromBERT. FE2arethe
embeddings from the second-to-last hidden layer; FE3are the con-
catenation of the embeddings from the last four hidden layers; and
FE4are the summation of the embeddings from these four layers.
Thevariousoptionsexplainedaboveinducesevenconfigurations
for solution zand 28 for solution {. We tune solutions zand
{for maximizing F 2-score for ambiguity detection in DAMIRğ‘‡.
We further tune the solutions for maximizing the success rate of
anaphoraresolution(usingonlythedatapointslabeled corrector
incorrect, and excluding those labeled inconclusive ). Sincecorrectis
theminorityclassinanaphoraresolution,wedownsizethe incorrect
class using random under-sampling [35].
Weevaluateallconfigurationsusing10-foldcross-validation[ 91].
We note that standard 10-fold cross-validation would partition
DAMIRğ‘‡atthetriplelevel,implyingthatsomeofthetriplesassoci-
ated with a pronoun occurrence could land in the training set and
theothersinthetestset.Suchsplittingofthetriplesassociatedwith
thesamepronounisundesirable.Wethereforedevelopavariant
of10-foldcross-validationwherewefirstgroupthedatapointsin
DAMIRğ‘‡by pronoun occurrence, perform random shuffling and
onlythensplitthedatasetintotenequalpartitions.Thisensures
that all the triples associated with a single pronoun occurrence are
placed in one partitiononly, used either for training orfor testing.
Tables 3 and 4 list the various configurations and the results ob-
tainedforeach.Wenotethat,inTable4,weapplythe fullmatching
194Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Accuracy of Different Configurations of Solutions zand{for Anaphoric Ambiguity Detection.
DT FNN kNN LR NB RF SVM
PRF 2PRF 2PRF 2PRF 2PRF 2PRF 2PRF 2
zLF50.994.0 80.3 50.2 96.2 81.2 50.3 91.0 78.3 49.5 89.0 76.6 50.4 99.7 83.3 49.9 94.3 80.0 50.0 94.4 80.1
{FE151.0 86.3 75.7 51.6 99.2 83.6 50.2 98.2 82.4 50.3 95.3 80.6 50.8 98.3 82.8 50.3 94.5 80.3 50.0 97.2 81.7
FE249.8 89.0 76.7 51.1 98.0 82.7 49.8 96.8 81.4 50.4 95.9 81.2 50.1 96.9 81.5 51.1 96.7 82.0 50.4 97.7 82.2
FE351.9 89.0 77.7 49.9 94.0 79.8 50.2 97.7 82.1 51.2 97.2 82.3 50.6 98.7 82.9 50.6 95.5 80.9 50.4 97.7 82.2
FE456.9 87.3 77.5 55.5 96.9 83.6 55.7 98.3 84.7 52.5 90.4 78.3 55.0 95.3 82.8 57.7 100 85.9 56.0 92.6 80.3
â€ FE1: FEs from SBERT, FE2: FEs from BERTâ€™s second-to-last layer, FE3: concatenation of FEs from BERTâ€™s last four layers, FE4: summation
of FEs from the same four layers.
Table 4: Success Rate of Different Configurations of Solu-
tionszand{for Anaphora Resolution.
DT FNN kNN LR NB RF SVM
zLF32.2 81.4 61.0 86.4 18.6 71.1 91.5
{FE16.874.613.6 66.1 62.7 66.1 69.4
FE20.0 59.3 16.9 66.1 55.9 67.8 71.1
FE38.5 61.0 16.9 66.1 18.6 67.8 66.1
FE46.8 57.6 15.2 62.7 52.5 57.6 66.1
modeforcomputingaccuracy.ThisisbecausetheML-basedsolu-
tionspredictanexactcandidateantecedentfromapre-generated
list instead of demarcating a text span. The best results for each
solution are highlighted in bold. We select as the best-performing
configuration forML LFtheNBalgorithm forambiguity detection,
andtheSVMalgorithmforanaphoraresolution.Weselectasthe
best-performingconfigurationforML FEtheRFalgorithmtrained
overFE4forambiguitydetection,andthe FNNalgorithmtrained
overFE1for anaphora resolution. Following the above decisions,
we apply grid search [ 9] to optimize the hyperparameters of the
best-performing configurations; hyperparameter optimization for
allpossibleconfigurationswouldhavebeentooexpensivedueto
the high dimensionality of feature embeddings.
Finally, there are two fixed thresholds in the ML-based solu-
tions,ğœƒğ›½andğœƒğ›¿,whichwetuneafterhyperparameteroptimization.
The role of ğœƒğ›½is the same as that of ğœƒğ›¼, discussed earlier for the
SpanBERT-based solutions. The ğœƒğ›½threshold is tuned in the same
manner as ğœƒğ›¼. The optimal value is ğœƒğ›½=0.5. As forğœƒğ›¿, the thresh-
oldisusedbyML ensembletoensurethatonecandidateantecedent
is not favored over another when the predicted probabilities are
tooclose(seeSection3.3).Wetune ğœƒğ›¿usingexhaustivesearchon
DAMIRğ‘‡and over the same ten values tried for ğœƒğ›¼andğœƒğ›½. The
optimal value is ğœƒğ›¿=0.1.
4.6 Answers to the RQs
RQ1.Whichsolutionalternativeisthemostaccuratefordetecting
anaphoric ambiguity in requirements? Table 5 (left side) shows the
precision(P), recall(R)and F 2-score(F 2)of thedifferentsolutions
measured on the DAMIRğ¸andReqEvaldatasets.
Asshownbythetable,allalternativesperformbetteron ReqEval
thanDAMIRğ¸. The difference in accuracy is particularly notablefortheprecisionofSpanBERT-basedsolutions.Webelievethatthis
differencecanbeexplainedbythedifferentcontextsizesusedfor
pronoun occurrences in the two datasets. In ReqEval, the context is
onesentencewithanaveragelengthof25words,whereboththe
pronouns and their antecedents occur. In this dataset, the average
number of candidate antecedents for a pronoun is four. In contrast,
inDAMIRğ¸,thecontextiscomposedoftwosentenceswithanaver-
age of 47 words. For this dataset, the average number of candidate
antecedents is nine, i.e., more than twice as many as for ReqEval.
Parsing larger contexts and having to deal with more candidate
antecedentsallowmoreroomforerror.Overall,webelievethatthe
resultsfor DAMIRğ¸aremorereflectiveofpractice,sinceanalysts
oftenconsiderabroadercontextforapronounthanthesentence
where the pronoun appears. As noted earlier, this broader context
informationisunavailablein ReqEval,henceourevaluationusing
single sentences as context in this dataset.
As seen from Table 5, the ML-based solutions have the best
recall (and also precision) on both datasets. We believe that the
superioraccuracyoftheML-basedsolutionshastodowiththefact
that these solutions are explicitly trained to distinguish ambiguous
andunambiguouspronounoccurrences.Wefurtherobservethat
the choice of features in ML-based solutions, i.e., LFs versus FEs,
has little impact on the accuracy of ambiguity detection. Overall
MLensembleleadstothebestF 2-scores,includingperfectrecallon
bothdatasets.Intermsofprecision,theML-basedsolutionsarethe
superioronesaswell.WenotethatML LFandMLFEneitherachieve
perfectrecallon ReqEvalnoroffertangiblegainsoverML ensemblein
termsofprecision.Across ReqEvalandDAMIRğ¸,MLensemblehasan
averageprecisionof59.9%.Webelievethatthislevelofprecisionis
acceptableinpractice.Theimplicationofa â‰ˆ60%precisionisthe
manualeffortneededforfilteringoutthepronounswronglymarked
as ambiguous (FPs). Discarding FPs is still easier and requires less
effort than finding FNs, i.e., the ambiguous cases that are missed.
The answer to RQ1is thatMLensemble(solution |in Figure 3)
withanaverageprecisionof â‰ˆ60%andarecallof100%isthemost
accurate solution for detecting anaphoric ambiguity in require-
ments.
RQ2.Whichsolutionalternativeisthemostaccurateforresolving
anaphorainrequirements? Table5(rightside)showstheresolution
success rate (defined in Section 4.4) for DAMIRğ¸andReqEval. Our
evaluation covers 96 unambiguous pronouns in DAMIRğ¸and 62 in
195ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ezzini, et al.
Table 5: Accuracy Results for Different Anaphoric Ambiguity Handling Solutions.
Precision, Recall and F 2of Ambiguity Detection (RQ1) Success Rate (%) of Anaphora Resolution (RQ2)
DAMIRğ¸ReqEval DAMIRğ¸ReqEval
P (%) R (%) F 2(%) P (%) R (%) F 2(%) Full Partial Full Partial
xSpanBERT ğ‘ğ¿ğ‘ƒ 40.0 81.8 67.6 60.2 75.8 72.1 73.5 88.2 97.8 97.8
ySpanBERT ğ‘…ğ¸ 36.9 77.2 63.3 57.6 96.8 85.2 68.9 96.1 97.8 100
zMLğ¿ğ¹ 57.6 100 87.2 61.8 98.9 88.3 81.2 - 57.4 -
{MLğ¹ğ¸ 57.5 100 87.1 62.2 98.2 88.0 51.0 - 82.9 -
|MLensemble 58.2 100 87.5 61.5 100 88.9 82.3 - 57.4 -
}NLPCoref 52.4 48.5 49.2 56.7 51.5 52.5 52.5 52.5 39.6 51.7
â€ For each dataset, the best values of P, R, F 2and success rate are highlighted in bold.
ReqEval. We apply both the fulland partial matchingmodes (see
Section 4.4). We note though that only full matching applies to the
ML-basedsolutions,sincethesesolutionsidentifytheantecedent
of a pronoun from a pre-calculated list of candidate antecedents.
AsTable5shows,foranaphoraresolution,nosolutionoutper-
formsalltheothersonbothdatasets.Forinstance,theML-based
solutions ( zâ€“|) perform well on one dataset but not the other.
MLensembleis the best-performing solution on DAMIRğ¸, but per-
forms rather poorly on ReqEval. As highlighted in the table, the
SpanBERT-basedsolutionsclearlyoutperformallothersolutions
in partial matching mode, with SpanBERT REachieving the highest
successrate.WethusbelievethatSpanBERT REisthemostuseful
solutionintermsofprovidingassistancetoanalystsduringmanual
requirements reviews.
The answer to RQ2is thatSpanBERT RE(solution yin Figure 3)
withanaveragesuccessrateof â‰ˆ98%isthemostaccuratesolution
for resolving anaphora in requirements.
RQ3.Whatistheexecutiontimeofeachsolutionalternative? We
considertheexecutiontimeofoursolutionsbothfromtheperspec-
tive of a solution developer and that of an end-user.
A developer would be interested in how long it takes to tune
the SpanBERT- andML-basedsolutions, as discussed in Section 4.5.
Tuningisa one-offactivity andnotpertinenttoend-users.Weused
GoogleColaboratory[ 12]fordevelopingandtuningtheSpanBERT-
based solutions. Fine-tuning SpanBERT on CoNLL2011 (with 6,757
pronouns)togenerateSpanBERT NLPtookâ‰ˆ4hours.Fine-tuning
SpanBERT NLPonDAMIRğ‘‡(with 533 pronouns) to generate
SpanBERT REtookâ‰ˆ23 minutes. For tuning the ML-based solu-
tions, we used a workstation equipped with a 12-core processor
(AMDRyzen9 5900X3.7GHz)and64GB ofmemory.Recallfrom
Section4.5thattheML-basedsolutionsaretunedseparatelyforam-
biguity detectionand anaphoraresolution. Tuning timeis directly
impacted by the best-performing configuration picked for each
task(whichwillthenbesubjectedtohyperparamateroptimization).
Tuning ML LFrequired 30 minutes for detection and 53 minutes for
resolution. Tuning ML FEwas more expensive, requiring 6.5 hours
for detection and 45 minutes for resolution.
Tomeasureexecutiontimefromanend-userâ€™sperspective,we
usedanormallaptopwitha2.3GHzCPUand16GBofmemory.Wepicked from our evaluation set a random selection of 100 pronoun
occurrences. These occurrences span 96 requirements sentences
and induce 842 triples. We combined the 96 sentences into a sin-
gle document. The resulting document is not meant to represent a
real-worldRS.Rather,wewantthisdocumenttoemulatearepresen-
tativesituationforpronounoccurrences(e.g.,intermsofhaving
different pronoun types and different numbers of candidate an-
tecedentsincontext).Inarealsetting,beforeoneappliesanyofour
solutionstoanRS,allthematerialintheRSotherthanthesentences
within the context of some pronoun occurrence can be removed.
The resulting document was used for measuring per-pronoun
executiontime.Themeasuredtimesarerepresentativeforlarger
samplesaswell,withtheoverallexecutiontimeincreasinglinearly
as the number of pronoun occurrences increases.
The answer to RQ3is as follows. The average time (in seconds)
required for handling an individual pronoun occurrence is: 1.5s
usingSpanBERT NLPorSpanBERT RE;8sfordetectionand8sfor
resolutionusingML LF;7.5sfordetectionand6sforresolutionusing
MLFE; 14.5s for detection and 13s for resolution using ML ensemble;
and 7s using NLP Coref.
The practical implication of these execution times is as follows:
Based on the literature [ 25], one can expect that 20% of the re-
quirements in a given RS would contain (pronominal) anaphora.Processing a large RS with, say, 2000 requirements would then
requireprocessing400(giveortake)requirementssentences.Ex-
trapolating from our datasets, one can expect 1.2 pronouns per
sentenceand thus480pronouns inourhypothetical RSwith2000
requirements. Using the most accurate solutions from RQ1 and
RQ2,onewouldrequireabout2hoursfordetectingambiguityus-
ingMLensembleandabout4minutesforresolvinganaphorausing
SpanBERT RE. The execution time of ambiguity detection can be
cutbyalmosthalfifoneapplieseitherML LForMLFE,potentially
at the cost of a slight decrease in recall. These execution times are
acceptable for offline processing, e.g., during a break or overnight.
Asforonline(i.e.,interactive)processing,weobservethat,atany
giventime,ananalystlikelyworksononlyasmallfragmentofa
largedocument.Forinteractiveusage,anaphoricambiguityhan-dling can be localized to the document segment (e.g., sentences)
that the analyst is working on.
196Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
4.7 Discussion
Below,wemaketworemarks:thefirstoneistheoverallconclusion
ofourempiricalevaluation;thesecondoneisalessonlearnedabout
using pre-trained language models in RE.
(1) Given the accuracy results (RQ1 and RQ2) and the execution
times(RQ3), weproposea hybridsolutionfor handlinganaphoric
ambiguityinrequirements.Thishybridsolutioncombinessuper-
vised ML for ambiguity detection and SpanBERT for anaphora
resolution. For the detection task, ML ensembleis the most accurate.
OnemaynonethelesselecttousetheslightlylessaccurateML LFor
MLFEto reduce execution time. For the resolution task, we recom-
mend SpanBERT RE. This solution is highly accurate in pinpointing
the location of antecedents.
(2) We benefited from the CoNLL2011 dataset for the initial fine-
tuning of SpanBERT, before further fine-tuning it with RE-specific
data. Our preliminary experimentation indicated that, without the
intermediatefine-tuningstepover CoNLL2011,SpanBERTwould
notleadtoaviablesolutionthroughfine-tuningonourREdatasets
alone.Webelievethat,duetothegeneralscarcityoftailor-madedatasets for RE tasks, one should take into account the possibil-ity that intermediate fine-tuning data may be required, when at-tempting to design requirements automation solutions based on
pre-trainedlanguagemodels.Tothisend,REresearchersmayneedtolookforcomplementarydatasetsinothercommunities,e.g.,NLP,tobeabletogetthebesttractionfrompre-trainedlanguagemodels.
5 THREATS TO VALIDITY
The validity concerns most pertinent to our evaluation are internal
and external validity.
InternalValidity. Themainconcernregardinginternalvalidityis
bias.Thisconcernappliesmainlytothe DAMIRdataset,whichwas
developed on the authorsâ€™ initiative. To mitigate bias, the labelling
ofDAMIRwas performed exclusively by two independent (non-
author) annotators. To avoid learning bias, the annotators were
never exposed to either the design or the results of any of the
alternative solutions in our study.
External Validity. We evaluated all solutions on two datasets â€“
DAMIRandReqEval, the latter being an external dataset. The indi-
vidualsolutionsshowcomparableresultsacrossthetwodatasets.
In terms of domain coverage, DAMIRspans eight different applica-
tion domains. The consistency of the results across the DAMIRand
ReqEvaldatasets,takenalongsidethedomaincoverageof DAMIR,
providesconfidenceaboutthegeneralizabilityofourempiricalfind-
ings. That said, further evaluation using additional documents and
user studies can help further mitigate external-validity threats.
6 CONCLUSION
Inthispaper,wedevelopedandevaluatedsixalternativeautomation
solutions forhandling anaphoricambiguity inrequirements. Each
solution addresses both the detection of anaphoric ambiguity as
well as the resolution of anaphora. Our motivation for conducting
a multi-solution study stems from the availability of competing
NLP and ML technologies that we could build on. Without an
empirical examination of different solution designs, we would not
be able to ascertain which technologies would be the most suitable
forouranalyticalneeds.Thissituationisnotlimitedtoourworkperse;choosingtherightsetoftechnologiesforthetaskathand
is a consideration that one increasingly has to contend with in
AI-enabled automation.
Ourevaluationinvolvedtwodatasetswithatotalof â‰ˆ1,350indus-
trial requirements. Our results indicate that, for anaphoric ambigu-
ity detection, supervised ML is more accurate than both SpanBERT
(a variant of BERT) and a solution built using off-the-shelf coref-
erence resolvers. Our best solution for ambiguity detection has an
averageprecisionof â‰ˆ60%andarecallof100%.Differentlyfromthe
ambiguitydetectiontask,foranaphoraresolution,SpanBERTyields
thebestsolutionwithanaveragesuccessrateof â‰ˆ98%.Basedon
these results, we recommend a hybrid solution for anaphoric ambi-
guityhandling,whereambiguitydetectionandanaphoraresolution
are realized using different technological platforms.
Anaphoric ambiguity is an important but still a single aspect of
the broader problem of ambiguity. In requirements engineering,
where ambiguity handling is closely associated with quality assur-
ance, analysts are likely interested in a more holistic treatment
that addressesa widerrange ofambiguity types. Inthe future,we
wouldliketoexpandourworktootherambiguitytypes,particu-
larlysemanticones,thatarestillunder-explored.Furthermore,and
to more conclusively evaluate the usefulness of our current results,
we plan to conduct user studies involving practicing engineers.
Acknowledgement. ThisworkwasfundedbyLuxembourgâ€™sNa-
tionalResearchFund(FNR)underthegrantBRIDGES18/IS/12632261
andNSERCofCanadaundertheDiscoveryandDiscoveryAccel-
erator programs. We are grateful to the research and development
team at QRA Corp. for valuable insights and assistance.
REFERENCES
[1]Sallam Abualhaija, Davide Fucci, Fabiano Dalpiaz, and Xavier Franch. 2020.
Preface:3rdWorkshoponNaturalLanguageProcessingforRequirementsEn-
gineering(NLP4REâ€™20).In JointProceedingsofREFSQ-2020Workshops,Doctoral
Symposium, Live Studies Track, and Poster Track co-located with the 26th Interna-
tional Conference on Requirements Engineering: Foundation for Software Quality.
[2]Sallam Abualhaija, Davide Fucci, Fabiano Dalpiaz, Xavier Franch, and AlessioFerrari. 2020. ReqEval: The shared task on anaphora ambiguity detection and
disambiguation. https://github.com/frieden84/nlp4re-reqeval last accessed: July
2021.
[3] Charu C Aggarwal. 2018. Machine learning for text. Springer.
[4]Vincenzo Ambriola and Vincenzo Gervasi. 2006. On the Systematic Analysis of
NaturalLanguageRequirementswithCIRCE. AutomatedSoftwareEngineering
13, 1 (2006).
[5]Chetan Arora, Mehrdad Sabetzadeh, Lionel Briand, and Frank Zimmer. 2015.
Automated Checking of Conformance to Requirements Templates Using Natural
Language Processing. IEEE Transactions on Software Engineering (TSEâ€™15) 41, 10
(2015).
[6]Chetan Arora, Mehrdad Sabetzadeh, Lionel Briand, and Frank Zimmer. 2017.
Automated Extraction and Clustering of Requirements Glossary Terms. IEEE
Transactions on Software Engineering 43, 10 (2017).
[7]Chetan Arora, Mehrdad Sabetzadeh, Lionel Briand, Frank Zimmer, and Raul
Gnaga. 2013. RUBRIC: A Flexible Tool for Automated Checking of Conformance
toRequirementBoilerplates.In Proceedingsofthe9thjointmeetingoftheEuro-
peanSoftwareEngineeringConferenceandtheACMSIGSOFTSymposiumonthe
Foundations of Software Engineering (ESEC/FSEâ€™13).
[8]ChetanArora,MehrdadSabetzadeh,ShivaNejati,andLionelBriand.2019. An
ActiveLearningApproachforImprovingtheAccuracyofAutomatedDomain
ModelExtraction. ACMTransactionsonSoftwareEngineeringandMethodology
28, 1 (2019).
[9]JamesBergstra,RÃ©miBardenet,YoshuaBengio,andBalÃ¡zsKÃ©gl.2011. Algorithms
for hyper-parameter optimization. Advances in neural information processing
systems24 (2011).
[10]D. Berry, E. Kamsties, and M. Krieger. 2003. From Contract Drafting to Software
Specification:LinguisticSourcesofAmbiguity,AHandbook. http://se.uwaterloo.
ca/~dberry/handbook/ambiguityHandbook.pdf
197ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Ezzini, et al.
[11]Daniel M Berry. 2021. Empirical evaluation of tools for hairy requirements
engineering tasks. Empirical Software Engineering 26, 6 (2021).
[12]EkabaBisong.2019. BuildingmachinelearninganddeeplearningmodelsonGoogle
cloud platform: A comprehensive guide for beginners. Apress.
[13]Samuel Broscheit, Massimo Poesio, Simone Paolo Ponzetto, Kepa Joseba Ro-
driguez,LorenzaRomano,OlgaUryupina,YannickVersley,andRobertoZanoli.
2010. BART: A multilingual anaphora resolution system. In Proceedings of the
5th international workshop on semantic evaluation.
[14]XinyunCheng,XianglongKong,LiLiao,andBixinLi.2020. ACombinedMethod
for Usage of NLP Libraries Towards Analyzing Software Documents. In Interna-
tional Conference on Advanced Information Systems Engineering.
[15]KevinClarkandChristopherD.Manning.2016. DeepReinforcementLearningfor
Mention-RankingCoreferenceModels.In EmpiricalMethodsonNaturalLanguage
Processing.
[16]Kevin Clark and Christopher D. Manning. 2016. Improving Coreference Resolu-
tion by Learning Entity-Level Distributed Representations. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics.
[17]FabianoDalpiaz,IvorvanderSchalk,SjaakBrinkkemper,FatmaAydemir,and
GarmLucassen.2019. Detectingterminologicalambiguity inuserstories:Tool
and experimentation. Information and Software Technology 110 (2019).
[18]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. BERT:
Pre-training of deep bidirectional transformers for language understanding.
(2018). arXiv:arXiv:1810.04805
[19]Richard Evans. 2001. Applying machine learning toward an automatic classifica-
tion of it. Literary and linguistic computing 16, 1 (2001), 45â€“58.
[20]SaadEzzini,SallamAbualhaija,ChetanArora,MehrdadSabetzadeh,andLionelC
Briand.2021. Usingdomain-specificcorporaforimprovedhandlingofambiguity
in requirements. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering.
[21]FabrizioFabbrini,MarioFusani,StefaniaGnesi,andGiuseppeLami.2001. The
linguistic approach to the natural language requirements quality: Benefit ofthe use of an automatic tool. In Proceedings of the 26th Annual NASA Goddard
Software Engineering Workshop.
[22]Henning Femmer, Daniel MÃ©ndez FernÃ¡ndez, Elmar Juergens, Michael Klose,
Ilona Zimmer, and JÃ¶rg Zimmer. 2014. Rapid requirements checks with require-
ments smells: Two case studies. In Proceedings of the 1st International Workshop
on Rapid Continuous Software Engineering.
[23]Henning Femmer, Daniel MÃ©ndez FernÃ¡ndez, Stefan Wagner, and Sebastian Eder.2017. Rapid quality assurance with Requirements Smells. Journal of Systems and
Software123 (2017).
[24]Alessio Ferrari and Andrea Esuli.2019. An NLP approach for cross-domainam-
biguity detection in requirements engineering. Automated Software Engineering
26, 3 (2019).
[25]AlessioFerrari,GloriaGori,BenedettaRosadini,IacopoTrotta,StefanoBacherini,
AlessandroFantechi, andStefania Gnesi.2018. Detecting requirementsdefects
withNLPpatterns:Anindustrialexperienceintherailwaydomain. Empirical
Software Engineering 23, 6 (2018).
[26]Alessio Ferrari, Giorgio Oronzo Spagnolo, and Stefania Gnesi. 2017. Pure: Adataset of public requirements documents. In 2017 IEEE 25th International Re-
quirements Engineering Conference.
[27]Joseph L.Fleiss. 1971. Measuring nominalscale agreement amongmany raters.
Psychol. Bull. 76, 5 (1971).
[28]Vincenzo Gervasi, Alessio Ferrari, Didar Zowghi, and Paola Spoletini. 2019.
Ambiguity in Requirements Engineering: Towards a Unifying Framework. In
From Software Engineering to Formal Methods and Tools, and Back. Springer.
[29]Benedikt Gleich, Oliver Creighton, and Leonid Kof. 2010. Ambiguity Detection:
TowardsaToolExplainingAmbiguitySources.In Proceedingsofthe16thWorking
Conference on Requirements Engineering: Foundation for Software Quality.
[30]IanGoodfellow,YoshuaBengio,andAaronCourville.2016. DeepLearning (1st
ed.). MIT Press.
[31]Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd.
2020.spaCy:Industrial-strengthNaturalLanguageProcessinginPython. https:
//doi.org/10.5281/zenodo.1212303
[32]Mitra Bokaei Hosseini, Rocky Slavin, Travis Breaux, Xiaoyin Wang, and Jianwei
Niu. 2020. Disambiguating Requirements Through Syntax-Driven Semantic
Analysis of Information Types. In Proceedings of the 26th Working Conference on
Requirements Engineering: Foundation for Software Quality.
[33]Yufang Hou. 2020. Bridging Anaphora Resolution as Question Answering. InProceedings of the 58th Annual Meeting of the Association for Computational
Linguistics.
[34]DavidGraffHuang,ShudongandGeorgeDoddington.2002. Multiple-Translation
ChineseCorpusLDC2002T01. Webdownloadfile.Philadelphia:LinguisticData
Consortium.
[35]Nathalie Japkowicz. 2000. The class imbalance problem: Significance and strate-
gies. InProceedings of the International Conference on Artificial Intelligence.
[36]MandarJoshi,DanqiChen,YinhanLiu,DanielSWeld,LukeZettlemoyer,and
OmerLevy.2020. SpanBERT:Improvingpre-trainingbyrepresentingandpre-
dicting spans. Transactions of the Association for Computational Linguistics 8(2020).
[37]DanJurafskyandJamesH.Martin.2020. Speechand LanguageProcessing (3rd
ed.). https://web.stanford.edu/~jurafsky/slp3/(visited 2021-06-04).
[38]Erik Kamsties. 2005. Understanding Ambiguity in Requirements Engineering.
Springer Berlin Heidelberg.
[39]ErikKamstiesandBarbaraPeach.2000. Tamingambiguityinnaturallanguage
requirements. In Proceedings of the 13th International Conference on Software and
Systems Engineering and Applications.
[40]Nadzeya Kiyavitskaya, Nicola Zeni, Luisa Mich, and Daniel Berry. 2008. Re-quirements for tools for ambiguity identification and measurement in natural
language requirements specifications. Requirements Engineering 13, 3 (2008).
[41]PohlKlausandRuppChris.2011. RequirementsEngineeringFundamentals (1st
ed.). Rocky Nook.
[42]Thomas Kluyver, Benjamin Ragan-Kelley, Fernando PÃ©rez, Brian Granger,Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason
Grout, Sylvain Corlay, Paul Ivanov, DamiÃ¡n Avila, Safia Abdalla, and Carol
Willing.2016. JupyterNotebooksâ€“apublishingformatforreproduciblecom-
putationalworkflows.In PositioningandPowerinAcademicPublishing:Players,
Agents and Agendas.
[43]Giuseppe Lami, Mario Fusani, and Gianluca Trentanni. 2019. QuARS: A Pioneer
Tool for NL Requirement Analysis. In From Software Engineering to Formal
Methods and Tools, and Back. Springer.
[44]Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi,
Livio BaldiniSoares, and Michael Collins.2021. Qed: A frameworkand dataset
for explanations in question answering. Transactions of the Association for Com-
putational Linguistics 9 (2021).
[45]J.RichardLandisandGaryG.Koch.1977. AnApplicationofHierarchicalKappa-
type Statistics in the Assessment of Majority Agreement among Multiple Ob-
servers.Biometrics 33, 2 (1977).
[46]Kusum Lata, Pardeep Singh, and Kamlesh Dutta. 2021. A comprehensive review
onfeaturesetusedforanaphoraresolution. ArtificialIntelligenceReview 54,4
(2021).
[47]TimothyLee,AlexLutz,andJinhoDChoi.2016. QA-It:classifyingnon-referential
it for question answer pairs. In Proceedings of the ACL 2016 Student Research
Workshop.
[48]QiLiu,MattJKusner,andPhilBlunsom.2020. Asurveyoncontextualembed-
dings. (2020). arXiv:arXiv:2003.07278
[49]Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit.
InProceedingsoftheACL-02WorkshoponEffectiveToolsandMethodologiesfor
Teaching Natural Language Processing and Computational Linguistics.
[50]Panos Louridas and Christof Ebert. 2016. Machine Learning. IEEE Software 33, 5
(2016).
[51]ChristopherManning,MihaiSurdeanu,JohnBauer,JennyFinkel,StevenBethard,
and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing
Toolkit.In Proceedingsof52ndAnnualMeetingoftheAssociationforComputational
Linguistics: System Demonstrations.
[52]Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Build-
ingaLargeAnnotatedCorpusofEnglish:ThePennTreebank. Computational
Linguistics 19, 2 (1993).
[53]Murray Grossman Nii Martey John Bell Mark Liberman, Kelly Davis. 2002. Emo-
tional Prosody Speech and Transcripts LDC2002S28. CD-ROM. Philadelphia:
Linguistic Data Consortium.
[54]AaronMassey,RichardRutledge,AnnieAnton,andPeterSwire.2014. Identifyingandclassifyingambiguityforregulatoryrequirements.In Proceedingsofthe22nd
IEEE International Requirements Engineering Conference.
[55]Alistair Mavin, Philip Wilkinson, Adrian Harwood, and Mark Novak. 2009. Easy
Approach to Requirements Syntax (EARS). In Proceedings of the 17th IEEE Inter-
national Requirements Engineering Conference.
[56]Joseph F McCarthy and Wendy G Lehnert. 1995. Using Decision Trees for Coref-
erence Resolution. In International Joint Conferences on Artificial Intelligence.
[57]AlessioMiaschiandFeliceDellâ€™Orletta.2020. ContextualandNon-Contextual
Word Embeddings: an in-depth Linguistic Investigation. In Proceedings of the 5th
Workshop on Representation Learning for NLP . Association for Computational
Linguistics.
[58]L. Mich. 1996. NL-OOPS: From natural language to object oriented require-
ments using the natural language processing system LOLITA. Natural Language
Engineering 2, 2 (1996).
[59]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimationofwordrepresentationsinvectorspace. (2013). arXiv:arXiv:1301.3781
[60] Ruslan Mitkov. 1999. Anaphora resolution: the state of the art. Citeseer.
[61] Ruslan Mitkov. 2014. Anaphora resolution. Routledge.
[62]Natalia N Modjeska, Katja Markert, and Malvina Nissim. 2003. Using the webin machine learning for other-anaphora resolution. In Proceedings of the 2003
conference on Empirical methods in natural language processing.
[63]Ray Offen. 2002. Domain understanding is the key to successful system develop-
ment.Requirements engineering 7, 3 (2002).
[64]Mohamed Osama, Aya Zaki-Ismail, Mohamed Abdelrazek, John Grundy, and
AmaniIbrahim.2020. Score-basedautomaticdetectionandresolutionofsyntactic
198Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
ambiguityinnaturallanguagerequirements.In 2020IEEEInternationalConference
on Software Maintenance and Evolution.
[65]AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDes-
maison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,Alykhan
Tejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
Chintala.2019. PyTorch:AnImperativeStyle,High-PerformanceDeepLearn-ing Library. In Advances in Neural Information Processing Systems 32. Curran
Associates, Inc.
[66]Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort, Vincent Michel,
BertrandThirion,OlivierGrisel,MathieuBlondel,PeterPrettenhofer,RonWeiss,
Vincent Dubourg, et al .2011. Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research 12 (2011), 2825â€“2830.
[67]JeffreyPennington,RichardSocher,andChristopherD.Manning.2014. GloVe:
GlobalVectorsforWordRepresentation.In EmpiricalMethodsinNaturalLan-
guage Processing.
[68]JasonPhang,ThibaultFÃ©vry,andSamuelRBowman.2018. Sentenceencoders
on stilts: Supplementary training on intermediate labeled-data tasks. (2018).
arXiv:arXiv:1811.01088
[69]Steven Piantadosi, Harry Tily, and Edward Gibson. 2012. The communicative
function of ambiguity in language. Cognition 122, 3 (2012).
[70]Massimo Poesio, Roland Stuckardt, and Yannick Versley. 2016. Anaphora resolu-
tion. Springer.
[71] Klaus Pohl. 2010. Requirements Engineering (1st ed.). Springer.
[72]Sameer Pradhan, Lance Ramshaw, Mitch Marcus, Martha Palmer, Ralph
Weischedel,andNianwenXue.2011. CoNLL-2011sharedtask:Modelingunre-
strictedcoreferenceinontonotes.In ProceedingsoftheFifteenthConferenceon
Computational Natural Language Learning: Shared Task. 1â€“27.
[73]Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Man-ning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many
HumanLanguages.In Proceedingsofthe58thAnnualMeetingoftheAssociation
for Computational Linguistics: System Demonstrations.
[74]Felipe Quecole, Maisa Cristina Duarte, and Estevam Rafael Hruschka. 2018. Cou-
pling for Coreference Resolution in a Never-ending Learning System. Journal of
Information and Data Management 9, 2 (2018).
[75]Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Cham-
bers, Mihai Surdeanu, Dan Jurafsky, and Christopher D Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings of the 2010 conference on
empirical methods in natural language processing.
[76]Marta Recasens, LluÃ­s MÃ rquez, Emili Sapena, M AntÃ²nia MartÃ­, Mariona TaulÃ©,
VÃ©ronique Hoste, Massimo Poesio, and Yannick Versley. 2010. Semeval-2010
task 1: Coreference resolution in multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
[77]Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings
using siamese bert-networks. (2019). arXiv:arXiv:1908.10084
[78]CristinaRibeiroandDanielBerry.2020. Theprevalenceandseverityofpersistent
ambiguity in software requirements specifications: Is a special effort needed to
find them? Science of Computer Programming 195 (2020).
[79]Marcel Robeer, Garm Lucassen, Jan Martijn E.M. van der Werf, Fabiano Dalpiaz,
and Sjaak Brinkkemper. 2016. Automated Extraction of Conceptual Models from
User Stories via NLP. In Proceedings of the 24th IEEE International Requirements
Engineering Conference.[80]Danissa Rodriguez, Doris Carver, and Anas Mahmoud. 2018. An efficient
wikipedia-based approach for better understanding of natural language text
relatedtouserrequirements.In Proceedingsofthe39thIEEEAerospaceConfer-
ence.
[81]BenedettaRosadini,AlessioFerrari,GloriaGori,AlessandroFantechi,Stefania
Gnesi, Iacopo Trotta, and Stefano Bacherini. 2017. Using NLP to Detect Require-
mentsDefects:AnIndustrialExperienceintheRailwayDomain.In Proceedingsof
the23rdWorkingConferenceonRequirementsEngineering:FoundationforSoftware
Quality.
[82]ChetanAroraMehrdadSabetzadehSaadEzzini,SallamAbualhaija.2021. â€œOnline
Annex (online)â€. Available at https://tinyurl.com/2p9k2zf2, August 2021.
[83]NicolasSannier,MorayoAdedjouma,MehrdadSabetzadeh,andLionelBriand.
2017. An automated framework for detection and resolution of cross references
in legal texts. Requirements Engineering 22, 2 (2017).
[84]UnnatiShahandDeveshJinwala.2015. ResolvingAmbiguitiesinNaturalLan-
guage Software Requirements: A Comprehensive Survey. SIGSOFT Software
Engineering Notes 40, 5 (2015).
[85]Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. Mp-net: Masked and permuted pre-training for language understanding. (2020).
arXiv:arXiv:2004.09297
[86]Rhea Sukthanker, Soujanya Poria, Erik Cambria, and Ramkumar
Thirunavukarasu. 2020. Anaphora and coreference resolution: A review.
Information Fusion 59 (2020).
[87]Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. 2016. Introduction to data
mining. Pearson Education India.
[88]Sri Tjong and Daniel Berry. 2013. The design of SREEâ€”a prototype potential
ambiguity finder for requirements specifications and lessons learned. In Proceed-
ings of the 19th Working Conference on Requirements Engineering: Foundation for
Software Quality.
[89]GuidoVanRossumandFredL.Drake.2009. Python3ReferenceManual. CreateS-
pace.
[90]Yawen Wang, Lin Shi, Mingyang Li, Qing Wang, and Yun Yang. 2020. A Deep
Context-wiseMethodforCoreferenceDetectioninNaturalLanguageRequire-
ments. In 2020 IEEE 28th International Requirements Engineering Conference.
[91]Ian Witten, Eibe Frank, Mark Hall, and Christopher Pal. 2011. Data Mining:
Practical Machine Learning Tools and Techniques (4th ed.). Elsevier.
[92]ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
Davison,SamShleifer,PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,
CanwenXu,TevenLeScao,SylvainGugger,MariamaDrame,QuentinLhoest,
and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing.In Proceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
Language Processing: System Demonstrations. Association for Computational
Linguistics.
[93]Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei Li. 2020. CorefQA: Corefer-
enceresolutionasquery-basedspanprediction.In Proceedingsofthe58thAnnual
Meeting of the Association for Computational Linguistics.
[94]Hui Yang, Anne De Roeck, Vincenzo Gervasi, Alistair Willis, and Bashar Nu-seibeh. 2010. Extending nocuous ambiguity analysis for anaphora in natural
languagerequirements.In Proceedingsofthe18thIEEEInternationalRequirements
Engineering Conference. IEEE.
[95]Hui Yang, Anne de Roeck, Vincenzo Gervasi, Alistair Willis, and Bashar Nu-
seibeh. 2011. Analysing anaphoric ambiguity in natural language requirements.
Requirements Engineering 16, 3 (2011).
199