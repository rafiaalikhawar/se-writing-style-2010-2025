Using Pre-Trained Models to Boost Code Review Automation
Rosalia Tufano
SEART @ Software Institute
Università della Svizzera italiana
SwitzerlandSimone Masiero
SEART @ Software Institute
Università della Svizzera italiana
SwitzerlandAntonio Mastropaolo
SEART @ Software Institute
Università della Svizzera italiana
Switzerland
Luca Pascarella
SEART @ Software Institute
Università della Svizzera italiana
SwitzerlandDenys Poshyvanyk
SEMERU @ Computer Science Department
William and Mary
USAGabriele Bavota
SEART @ Software Institute
Università della Svizzera italiana
Switzerland
ABSTRACT
Code review is a practice widely adopted in open source and in-
dustrialprojects.Giventhenon-negligiblecostofsuchaprocess,
researchers started investigating the possibility of automating spe-
cific code review tasks. We recently proposed Deep Learning (DL)
modelstargetingtheautomationoftwotasks:thefirstmodeltakes
as input a code submitted for review and implements in it changes
likely to be recommended by a reviewer; the second takes as input
the submitted code and a reviewer comment posted in natural lan-
guage and automatically implements the change required by thereviewer. While the preliminary results we achieved are encour-aging, both models had been tested in rather simple code review
scenarios, substantially simplifying the targeted problem. This was
also due to the choices we made when designing both the tech-nique and the experiments. In this paper, we build on top of that
work by demonstrating that a pre-trained Text-To-Text Transfer
Transformer (T5) model can outperform previous DL models for
automatingcodereviewtasks.Also,weconductedourexperiments
on a larger and more realistic (and challenging) dataset of code
review activities.
CCS CONCEPTS
•Softwareanditsengineering →Softwaremaintenancetools .
KEYWORDS
Code Review, Empirical Study, Machine Learning on Code
1 INTRODUCTION
The benefits of code reviews have been widely recognized, with
severalstudiesprovidingevidenceofthehigherqualityofreviewed
code[15,29,31].Also,codereviewshelpinpreventingbugsand
foster knowledge transfer among developers [ 10,40]. However,
studiesoncodereviewsalsohighlightedanadditionalcostthatsuch
aprocessentails:Empiricalevidencesuggeststhatlargesoftware
projects can undergo hundreds of code reviews per month.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510621Thisappliestobothopen-source(e.g., ∼500reviewspermonthin
Linux[39])andindustrial(e.g., ∼3kreviewspermonthinMicrosoft
Bing[38])projects.Asaresult,developerscanspendmanyhours
per week reviewing code [ 16]. Given the non-negligible cost of
code review, we recently proposed the automation of specific code
reviewtasks:Thegoalisnottoreplacedevelopers,buttohelpthem
savetimeintwoscenarios.Thefirstisthatofacontributor(i.e., the
developersubmittingthecodeforreview)whowantstoreceivea
rapid feedback about the code they wrote before submitting itfor review. The feedback is provided by a Deep Learning (DL)
modeltrainedtotakeasinputthecodetosubmitforreview Csand
provide as output a revised version of Cs(i.e.,Cr) implementing
code changes that are likely to be recommended by a reviewer.
The second scenario concerns the reviewer(s) involved in the
process: a DL model is trained to take as input (i) the code Cs
submittedforreview,and(ii)acomment Rnlwrittenbythereviewer
in natural language to request a specific change on Cs. The output
ofthemodelisarevisedversionof Cs(i.e.,Cr)implementingthe
changesrecommendedin Rnl.Theideahereisthatthereviewercan
use themodel to provide thecontributor with a concreteexample
of the code changes that they would like to see implemented.
Inourpreviouswork[ 46]wetrainedandexperimentedwiththe
DL models on a dataset composed of ∼17k triplets /angbracketleftCs,Rnl,Cr/angbracketright
extractedfromcodereviewsperformedinGitHub[ 2]andGerrit
[1]. In particular, the model recommending code changes to the
contributor is an encoder-decoder model with one encoder taking
Csas input and one decoder generating Cr. Our evaluation shows
thatthismodelcanrecommendachangeasareviewerwoulddoin
3% (single prediction) to 16% of the cases (10 different predictions).
The model employed in the second scenario (i.e., the automated
implementation of a comment recommended by the reviewer), has
instead two encoders taking as input CsandRnl, respectively, and
onedecodergenerating Cr.Thismodelcansuccessfullyimplement
a change recommended by a reviewer in 12% (single prediction) to
31% (10 different predictions) of the cases.
Despite the encouraging preliminary results, our approach [ 46]
aswellastheconductedempiricalstudysuffersofseverallimita-
tions we try to overcome in this paper. First, in [ 46] we adopted
code abstraction to reduce the vocabulary size and simplify the
learningoftheDLmodel:Themodeldidnotworkontherawcode,
butonanabstractedversionofitinwhich,forexample,variable
identifiers were replaced with a special VAR_IDtoken, where IDis
a progressive number (e.g., the second variable is represented by
VAR_2).The possibilityto goback toraw codewas guaranteedby
a map linking abstracted to raw tokens in Cs(e.g., VAR_1→i).
22912022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota
Whilesuchaproceduresimplifiesthelearningofthemodel,it
posesastronglimitationonthevarietyofcodereviewtasksthat
can be supported by such a model. Indeed, the abstraction process
forces to exclude from the dataset of triplets /angbracketleftCs,Rnl,Cr/angbracketrightall those
in whichCrintroduces identifiers or literals that were not present
inCs. This is necessary because the abstraction map is built on Cs
and, if a new variable VAR_2is introduced in Crduring the review
process,suchavariablecannotbemappedbacktorawsourcecode,
making such an approach unusable in practice. This means that
the triplets /angbracketleftCs,Rnl,Cr/angbracketrightonwhich weevaluated ourapproach [ 46]
wererelativelysimplechangesimplementedduringcodereview,
not requiring the introduction of new identifiers or literals.
Second,tosimplifythelearning,weonlyconsideredtriplets /angbracketleftCs,
Rnl,Cr/angbracketrightin which both the code submitted for review ( Cs) and the
revised code ( Cr) had no more than 100 tokens [ 46]. Again, this
reduced the complexity of the tackled problem.
Basically, the two above choices resulted in training and experi-
mentingtheproposedmodelsonquitesimplecodereviewinstances
only representative of a minority of the code transformations actu-
ally implemented during code reviews.
In this paper, we build on top of our previous work [ 46]e x -
perimentingwithDLmodelsforcodereviewautomationinmore
realisticandchallengingscenarios.Westartbytrainingtherecently
proposed Text-To-Text-Transfer Transformer (T5) model [ 35]o n
adatasetsimilartotheoneusedin[ 46].However,weadoptato-
kenizer(i.e., SentencePiece[ 26])thatallowsustoworkwithraw
source code, without the need for code abstraction. Also, we in-
crease the maximum length of the considered code componentsfrom 100 “abstracted” tokens to 512 “SentencePiece” tokens (i.e.,
∼390“abstracted”tokens).Theabsenceofanabstractionmecha-
nism and the increased upper bound for input/output length al-
lowed us to build a substantially larger dataset as compared to the
one used in [ 46] (168k instances vs.17k) and, more importantly, to
featureinsuchadatasetawidervarietyofcodetransformationsim-
plemented in the code review process, including quite challenging
instances such as those requiring the introduction of new identi-
fiersandliterals(accountingfor63%ofthenewdatasetwebuilt).
Also, we experimented with the automation of a third task related
to the code review process: Given the code submitted for review
(Cs), generating a natural language comment Rnlrequesting to the
contributor code changes as a reviewer would do ( i.e.,simulating a
reviewer commenting on the submitted code).
We also compare the T5 model with the encoder-decoder model
presentedinourpreviousworkontheoriginaldatasetusedin[ 46].
OurresultsshowthesuperiorperformanceofT5,whichrepresents
a significant step forward in automating code review tasks.
To summarize, the contributions of this work are:
(i) A novel approach for code review automation overcoming
several limitations of the state-of-the-art technique [46];
(ii) A comprehensive empirical evaluation of such an approach,
including a comparison with our previous technique [46];
(iii) The automation of a third task: Given the code submitted
forreview,automaticallygeneratingnaturallanguagecomments
requesting changes as reviewers would do;
(iv)AcodereviewdatasettotrainandtestDLmodelsinmore
realistic scenarios as compared to the one used in [46];
(v) A comprehensive replication package [8].2 T5 TO AUTOMATE CODE REVIEW
WedescribetheDLmodelweadopt,theconstructionprocessofthe
datasets needed for its training, and the procedure used for hyper-
parameter search, model training, and generation of predictions.
2.1 Text-to-Text Transfer Transformer (T5)
The Text-to-Text Transfer Transformer, or simply T5, is not merely
a model. Raffel et al.[35] compare “pre-training objectives, architec-
tures,unlabeleddatasets,transferapproaches,andotherfactorson
dozens of language understanding tasks ”.
The result ofthis exploration isthe best combinationof archi-
tectures and training techniques, namely T5. T5 is based on the
Transformer[ 48]architecture.Theproposedimplementationdif-
fersonlyinsomedetails(regardingthenormalizationlayerandthe
embedding scheme) from its original form. Raffel et al.proposed
severalversionsofT5,differingfromeachotherintheirsize(e.g.,
number of layers) and, as a consequence, training complexity. In
this work we adopt the smallversion of T5 consisting of: 8-headed
attention,6layersinboththeencoderandthedecoder,eachhaving
a dimensionality of 512 and the output dimensionality of 2,048 ( ∼
60M parameters).
Themodelissubjectedtoafirsttraining(pre-training)whose
purpose is to provide it with a general knowledge useful to solve a
setofrelatedtasks.Suppose,forexample,thatwewanttotraina
modelableto(i)translateEnglishtoGerman,and(ii)summarize
English text. Instead of starting by training the model for thesetwo tasks, T5 can be pre-trained in an unsupervised manner by
usingthe denoisingobjective (ormaskedlanguagemodeling ):The
model is fed with sentences having 15% of their tokens ( e.g.,words
inEnglishsentencesorcodetokensinJavastatements)randomly
masked and it is asked to predict them. By learning how to predict
themaskedtokens,themodelcanacquiregeneralknowledgeabout
the language of interest. In our example, we could pre-train the
model on English and German sentences.
Once pre-trained, T5 is fine-tuned on the downstream tasks in a
supervisedfashion.Eachtaskisformulatedina“text-to-text”for-
mat(i.e.,boththeinputandtheoutputofthemodelarerepresented
astext).Forexample,forthetranslationtaskadatasetcomposed
ofpairsofEnglishandGermansentencesallowstofine-tunethe
model.Similarly,thesummarizationtaskrequirestheinputEnglishtext and a corresponding summary. In the next sections we explain
how we pre-train and fine-tune T5 to support code review tasks.
2.2 Training Data
Wedescribetheprocessusedtobuildthedatasetsneededforthe
pre-training(Section2.2.1)andfine-tuning(Section2.2.2)ofT5.Partofthefine-tuningdatasethasbeenusedforhyperparametersearch
(Section 2.3) and for testing the performance of T5 (Section 3).
2.2.1 Pre-training Dataset. Giventhegoalofthepre-trainingphase
(i.e.,providing the model with general knowledge about the lan-
guages of the downstream tasks) we built a dataset allowing to
train T5 on Java and technical English.
Indeed, besides source code, technical English is instrumental
in a code review process in which reviewers post natural language
comments about code.
2292
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Pre-Trained Models to Boost Code Review Automation ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Westartfromtwodatasetsfeaturinginstancesincludingboth
sourcecodeandtechnicalEnglish:theofficialStackOverflowdump
(SOD)[7]andCodeSearchNet(CSN)[ 25].StackOverflowisaQ&A
websiteforprogrammers.Thedatadumpweusedcollectsallthe
questionsandrelativeanswersbetween2006and2020foratotal
of roughly 51M posts (where a post is a single question or answer).
A post includes English text (as per the SO guidelines) and/or code
snippets.Postsareusuallyaccompaniedbytagscharacterizingtheir
topic(e.g.,Java, Android)andcanberatedwith up-/down-votes and,
forwhatconcernstheanswers,theycanbemarkedasthe“accepted
answer” from the question’s author.
We extracted from the SOD all the answers (i) having a Java
tag;(ii)containingatleastone <pre><code> HTMLtagtoensure
the presence of at least one code snippet in the answer; and (iii)
having at least 5 up-votes and/or being the accepted answer. Thesefilters are justified by the goal of our pre-training. Indeed, we want
the model to acquire knowledge about technical English and Java:
focusing on answers containing at least one code snippet increases
the chances that their natural language text refers to an imple-mentation task, similarly to what happens in code review. Also,
the up-votes/accepted answer filter aims at discarding low-quality
instances containing, for example, wrong code solutions. This is
alsothereasonwhywefocusedonhigh-qualityanswerslikelyto
contain working solutions rather than on questions that, even if
up-voted (e.g., because they are relevant for many users) may con-
tain wrong implementations. From this step we obtained 1,018,163
candidate instances from the SOD.
Oneachselectedanswer a,weperformedthefollowingcleaning
steps: We remove emojis, non-latin characters, control characters,
trailingspacesandmultiplewhitespaces.Somespecialsymbolsarereplacedusinglatincharactershavingthesamemeaning, e.g.,"
≥"is
replaced with " >=". Moreover, we replace any embedded link with
a special tag " <LINK_i> ", withibeing an integer ranging from 0 to
n−1,wherenisthenumberoflinksin a.Finally,weremovedall
theinstanceshavinglessthantentokensormorethan512(40,491).
This left us with 977,379 valid instances.
TheCSN[ 25]Javadatasetfeatures1.5MuniqueJavamethods,
someofwhichcontainingtheirJavadoc.Wefilteredoutallthosein
whichaJavadocwasnotavailableoritdidnotcontainanyletter,
removing 1,034,755 of them. Unlike the SOD, CSN can contain
instances in which the “textual part” (i.e., the method comment) is
notinEnglish.Topartiallyaddressthisissue,weexcludepairsin
which no Latin characters were found. While this does not exclude
all non-English comments, at least identifies and removes those
writteninspecificlanguages(e.g., Russian,Chinese)(15,229).We
decided to accept some level of noise in the pre-training dataset
(e.g.,comments written in French) since (i) given the size of this
dataset,thislittleamountofnoiseshouldnotsubstantiallyaffect
the model’s performance, and (ii) the pre-training dataset is not
usedastestsettoassesstheperformanceoftheapproach.Aswe
willexplainlater,amorefine-grainedcleaninghasbeenperformed
for the fine-tuning dataset that, instead, is used for performance
evaluation. On the 519,905 remaining instances, we performed the
same cleaning steps described for the SOD (e.g., remove emojis).
Finally, from each pair we obtain a single string concatenating the
Javadoc comment and the code, retaining the ones having more
than ten and less than 512 tokens (507,947 instances left).By putting together the instances collected from the SOD and
CSNweobtainedthepre-trainingdatasetconsistingof1,485,326
instances. To perform the pre-training, we randomly mask in each
instance 15% of its tokens. The masked tokens are replaced withsentinel tokens
<extra_id_i> , whereiis an increasing number
rangingfrom0upto n−1,wherenisthenumberoftokensmasked
inagiveninstance.Ifseveralcontiguoustokensaremaskedthey
arereplacedbyasinglesentineltoken.These“maskedinstances”
representtheinputofthemodelduringthepre-training.Thetarget
(i.e.,thestringthemodelisexpectedtogenerate)isbuiltconcate-
natingthesentineltokensandthetoken(s)theyaremasking.An
extra sentinel token is added to indicate the end of the string.
Our pre-training dataset is publicly available [8].
2.2.2 Fine-tuning Datasets. To create the fine-tuning dataset we
mined Java open source projects from GitHub using the web ap-
plicationbyDabic etal.[19].Usingthequeryinginterface[ 5],we
selected all Java projects having at least 50 pull requests (PRs), ten
contributors, ten stars, and not being forks. The filters aim at (i)ensuring that enough “code review” material is contained in the
projects(i.e., atleast50PRs);(ii)discardingpersonal/toyprojects
(atleasttencontributorsandstars);and(iii)reducingthechanceof
mining duplicated code. This resulted in a list of 4,901 projects. We
alsominedthesixGerrit[ 1]installationsusedin[ 46]containing
code review data about 6,388 projects.
FromboththeGitHubandtheGerritdatasetsweextracttriplets
<ms,cnl,mr>, wheremsis a method submitted for the review;
cnlisasinglereviewer’scommentsuggestingcodechangesfor ms;
andmristherevisedversionof msimplementingthereviewer’s
recommendation cnl. Note that (i) we only looked for PRs that are
accepted at the end of the code review, since we want to learn howtorecommendchangesthat,attheend,canleadtocodeconsidered
good from a reviewer’s perspective; and (ii) a single PR in GitHub
and Gerrit can result in several triplets for our dataset. Indeed, we
minethedifferentreviewroundsineachPR.Forexample,amethod
mscan be submitted for review, receiving a comment cnlasking
for changes (first round). The revised version of msaddressing cnl
isthenresubmitted( mr),resultinginthesecondreviewround(pos-
sibly leading to additional comments and revisions of the method).
We stop when the code is formally accepted.
Overall, we mined 382,955 valid triplets from GitHub and Gerrit
usingthepipelinefrom[ 46]thatwesummarizeinthefollowing(see
[46] for additional details). Wetarget triplets in which a comment
cnlhasbeenpostedbya reviewer onamethod ms.Wecanidentify
thesecasessincebothGitHubandGerrit(i)provideinformation
aboutthedeveloperssubmittingthecodeandpostingcomments
in the review process; and (ii) allow to retrieve the specific codeline(s)
cnlrefersto(i.e., thecodein msthathasbeenhighlighted
by the reviewer when posting the comment).
We exclude all the comments posted by the authors of the code
(e.g.,to reply to reviewers), since they do not represent a review of
thecode.Thus,thetripletsinourdataset have cnlbeingasingle
commentpostedbyareviewer.Also,weexclude cnllinkedtoinline
comments (rather than code lines) in ms, since we target the fixing
ofcode-relatedissues.Toconsideratripletasvalid, cnlmustbethe
onlycommentpostedbyarevieweron msinthatspecificreview
round.
2293
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota
In this way, we can be confident that the revised version sub-
mitted later on by the author ( mr) actually aimed at implementing
cnl. Also,mrmust differ from ms(i.e.,a change must have been
implemented in the code to address cnl). From the technical point
of view, the parsing of the methods from the patches submitted for
review has been done using the lizard library [ 4]. Note that, the
removaloftripletsinwhich cnlincludemorethanonecomment
has been done later in the processing pipeline (we will get back to
this point). Indeed, before we had to clean comments possibly just
representing noise.
As done for thepre-training dataset, we performed some clean-
ingsteps.Wereplacedanylinkwiththenumberedtoken <LINK_i> ,
withibeinganintegerrangingfrom0to n−1,wherenisthetotal
numberof linksin cnl,msandmr.If thesame linkappears indif-
ferent parts (e.g., incnlandmr), it is replaced with the same token.
We also removed any emoji and non-ascii characters from the com-
ments,extraspacesandcontrolcharactersfromboththecomments
andthemethods,andinlinecommentsfromthemethods(weare
not interested in addressing issues related to internal comments).
Afterthecleaningprocessweobtainedsometripletsinwhich
cnlbecame an empty string or where msandmrbecame equal
(e.g.,they only differed for some spaces before the cleaning). We
removed these instances (-33,005) as well as those having cnl+ms
ormrlongerthan512tokens(-61,233).Weconsideredthesumof
cnlandmsin terms of length because, for one of the tasks (i.e., the
automated implementation of a comment posted by a reviewer),
they will be concatenated to form the input for the model.
Then,weremovedfromourtripletsnon-relevantcomments(-
28,581),i.e.,commentsnotrecommendingcodechangesuggestions
(e.g.,“looksgoodtome”).In[ 46]wemanuallycraftedasetofnatural
languagepatternstospotnon-relevantcomments(e.g., single-word
commentscontainingwordssuchas“thanks”,“nice”,etc.).Wehave
extendedthissetsincewenoticedthatinourricherdatasetseveral
non-relevant comments were left by these patterns. Such analysis
hasbeendonebyoneoftheauthorsbymanuallyinspectingallthe
tripletshaving cnlconsistingoflessthansixwords.Theupdated
heuristics are available in our replication package [8].
Wealsoexcludedtripletsincludingnon-English cnlcomments
(-4,815) through a pipeline composed by three language detector
tools. A preliminary classification has been performed using the
Pythonlibrarieslangdetect[ 3]andpycld3[ 6].Ifbothofthesetools
classify the comment as non-English, we relied on the Google lan-
guagedetectionAPIforafinaldecision.Suchaprocesswasneeded
since we noticed that the Google API was the most accurate in de-
tecting the language, especially when the comments also featured
codeconstructsinthem.Inthisscenario,thePythonlibrariesoften
generated false negatives (i.e., classifying an English sentence as
non-English). However, we had a limited number of requests avail-
able for the Google API. Thus, we performed a pre-filtering using
the Python libraries and, when they both reported the comment as
being not in English, we double checked using the Google API.
After this cleaning process, we excluded all triplets featuring
morethanonecommentin cnl(-86,604).Finally,weremovedallthe
duplicatesfromthefine-tuningdataset(-918).Tobeconservative,
we identify as duplicates two triplets having the same ms(thus,
even triplets having the same msbut different cnl/mrhave been
removed).The resulting dataset features 167,799 triplets that have been
used to build the three fine-tuning datasets needed for the threetasks we aim at automating. In the first task (code-to-code ) the
model takes as input mswith the goal of automatically generating
its revised version mr, implementing code changes that may be
required in the code review process. Thus, the fine-tuning datasetis represented by pairs m
s→mr.
Inthesecondtask(code&comment-to-code )themodeltakesas
inputboth msandacomment cnlpostedbythereviewerandtargets
thegenerationof mr,therevisedversionof msimplementingthe
code changes recommended in cnl.
Themscode contains two special tags <START>,<END>marking
theportionofthecode cnlrefersto.Thefine-tuningdatasetofthis
second task is represented by pairs <ms,cnl>→mr.
Finally,inthethirdtask(code-to-comment )themodeltakesas
inputmsand aims at generating a natural language comment ( cnl)
suggestingcode changesasa reviewerwould do.Thefine-tuning
dataset is represented by pairs ms→cnl.
Table 1: Pre-training and fine-tuning datasets (# instances)
Dataset train evaluation test
Pre-training
Stack Overflow 977,379 - -
CodeSearchNet 507,947 - -
Fine-tuning 134,239 16,780 16,780
All three fine-tuning datasets have been split into 80% training,
10% evaluation, and 10% test. Table 1 summarizes the number of
instancesinthedatasets:Thepre-trainingisonlyusedfortraining,
whilethefine-tuningdatasetsareexploitedalsoforthehyperpa-
rametertuning(evaluation )andforassessingtheperformanceof
the model (test ). In Table 1 we only report information for a single
fine-tuning dataset (rather than for the three previously described),
since all three fine-tuning datasets contain the same number of
instances. Indeed, they are all derived from the same set of triplets.
2.3 Training and Hyperparameter Search
Raffelet al.[35] showed the major role pre-training plays on the
performance of T5 models. The importance of pre-training has
alsobeenconfirmed(forotherTransformer-basedmodels)inthe
contextofcode-relatedtaskssuchastestcasegeneration[ 44].To
further study this aspect, we decided to experiment with both a
pre-trained and a non pre-trained model, both of which have been
subject to a hyperparameter tuning process.
Since we adopted the smallversion of T5 presented by Raffel
et al.[35], we did not experiment with variations related to its
architecture (e.g., changing the number of layers or the number
ofhiddenunits).Though,asalsodonebyMastropaolo etal.[28],
we experimented with different learning rate configurations: (i)
Costant Learning Rate (C-LR), in which the learning rate value is
fixedduringthetraining;(ii) InverseSquareRootLearningRate (ISR-
LR),inwhichthe learningratevaluedecaysastheinverse square
root of the training step; (iii) Slanted Triangular Learning Rate (ST-
LR) in which first the learning rate linearly increases and then it
linearlydecaysreturningtothestartingvalue;(iv) PolynomialDecay
Learning Rate (PD-LR), in which the learning rate polynomially
decays to a fixed value in a given number of steps.
2294
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Pre-Trained Models to Boost Code Review Automation ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Thehyperparametertuninghasbeendoneforthefine-tuning
phase only. Indeed, even though we just focus on one hyperpa-
rameter, such a process still remains quite expensive, requiring
thetrainingofeightdifferentT5models(i.e., pre-trainedandnon
pre-trained each with four different learning rates).
For pre-training we use the same configuration proposed by
Raffelet al.in [35]. Wepre-trainied the modelon thepre-training
dataset (Table 1) for 200k steps ( ∼34 epochs). Starting from the pre-
trained model, we fine-tuned for 75k steps four different models,
each using one of the experimented learning rates.
Since the goal ofthis procedure is to find thebest learning rate
forthethreecodereviewtasks,wefine-tunedeachofthesemodels
using a mixture of the three tasks: A single model is trained to
supportallthreetasksusingtheunionoftheirtrainingsets.This
isoneofthecharacteristicsofT5,thepossibilitytotrainasingle
model for multiple tasks. The same approach has been used forthe non pre-trained model: In this case four T5 models (one per
learning rate) have been directly fine-tuned.
We assessed the performance of the eight models on the evalua-
tion set of each task in terms of “perfect predictions”, namely casesinwhichthegeneratedoutputwasidenticaltothetarget(expected)
string. Table 2 reports the achieved results. As it can be seen, no
learning rate achieves the best results in all the tasks. Nevertheless,
ST-LR shows better overall performance and, for this reason, is the
one we adopt in our experiments.
Table 2: Hyperparameter tuning results
Task Learining Rate Strategy
C-LR ISR-LR ST-LR PD-LR
Pre-Trained
code-to-code 2.68% 3.68% 4.64% 2.53%
code&comment-to-code 10.39% 9.23% 8.46% 9.89%
code-to-comment 0.15% 0.32% 0.60% 0.15%
Non Pre-Trained
code-to-code 1.23% 3.71% 4.16% 1.22%
code&comment-to-code 5.05% 6.41% 6.24% 5.18%
code-to-comment 0.09% 0.44% 0.49% 0.03%
Given the best configuration for both the pre-trained and the
non pre-trained models, we fine-tuned them for a maximum of
300k steps using an early stop strategy. This means that we saved a
checkpointofthemodelevery10kstepscomputingitsperformance
in terms of “perfect predictions” on the evaluation set and stopped
the training if the performance of the model did not increase for
three consecutive checkpoints (to avoid overfitting).
2.4 Generating Predictions
Oncethemodelsaretrained,theycanbeusedtogeneratepredic-
tions. As done in previous work, we adopt a beam search strategy
[36]togeneratemultiplepredictionsgivenasingleinput.Forex-
ample, in the case of the code-to-code task, for a single msmethod
provided as input multiple mrcandidates can be generated. When
we ask the model to generate kpredictions, it generates the kmost
probablesequencesoftokensgiventheinputsequence; kisknown
as thebeam size and we experiment with k=1,3,5,10.
ForeachpredictiongeneratedbyT5,wealsoexploitedits score
function to assess the model’s confidence on the provided input.Thevaluereturnedbythisfunctionrangesfromminusinfinity
to 0 and it is the log-likelihood ( ln) of the prediction. Thus, if it
is 0, it means that the likelihood of the prediction is 1 (i.e., the
maximum confidence, since ln(1)=0), while when it goes towards
minus infinity, the confidence tends to be 0. In our empirical study
(Section 3) we assess the reliability of the confidence level as a
proxy for the quality of the predictions.
3 STUDY DESIGN
Thegoalof our evaluation is to empirically assess the performance
of the T5 model in code review automation tasks. The contextcon-
sistsof(i)thedatasetswepresentedinSection2;and(ii)thedataset
fromourpreviouswork[ 46].Fromnowonwerefertoourprevi-
ouslypresentedapproachasthe baseline.Thestudyaimsattackling
five research questions (RQs).
RQ1:TowhatextentisT5abletoautomaticallyrecommend
code changesto developers asreviewers would do? Weprovide
asinputtoT5aJavamethod mssubmittedforreviewandassess
theextenttowhichthemodelisabletoprovideasoutputarevised
versionof ms(mr)implementingcodechangesthatwillbelikely
requested during the code review process. The idea here is that
such a model could be used beforethe code is submitted for review
as an automated check for the contributor.
RQ2:TowhatextentisT5abletoautomaticallyimplement
code changes recommendedby reviewers? Given a Java method
submittedforreview( ms)andanaturallanguagecomment( cnl)in
whicharevieweraskstoimplementspecificcodechangesin ms,
weassesstheabilityofT5toautomaticallyrevise mstoaddress cnl
(thus obtaining a revised method mr).
ThethirdRQfocusesonthenovelcodereview-relatedtaskwe
introduce in this paper:
RQ3:TowhatextentisT5abletoautomaticallyrecommend
changesinnaturallanguageasreviewerswoulddo? InthisRQ
T5 is provided as input with a Java method submitted for review(
ms) and it is required to generate a natural language comment
(cnl) requesting code changes as reviewers would do.
ForRQ1-RQ3,weexperimentwithdifferentvariantsoftheT5
model. In particular, we assess the quality of T5 predictions forall three tasks when (i) the model is pre-trained or not; and (ii)the predictions have different confidence levels. Thanks to these
analyses, we can answer our fourth RQ:
RQ4:What is the role played by the model pre-training on
the performance of T5? How does the confidence of the pre-dictions affects their quality?
As explained in Section 2.3, we
performanablationstudyinwhichT5isfine-tunedwithoutany
pre-training (i.e., by starting from random weights in the neural
network). Thisallows to assessthe contribution ofthe pre-training
tothe performanceof themodel. Asfor theconfidenceof thepre-
dictions,weassesswhetheritcanbeusedasareliableproxyforthequalityofthepredictions(i.e., thehighertheconfidence,thehigher
the likelihood the prediction is correct). If this is the case, sucha finding would have implications for the usage of the T5 modelin practice: A developer using the model could decide to receiverecommendations having confidence higher than
t, reducing the
chances of receiving meaningless predictions.
2295
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota
Finally,thelastRQcomparestheperformanceoftheT5model
with that of the approach we presented in [46]:
RQ5:What is the performance of T5 as compared to the
state-of-the-arttechnique? Weusetheimplementationanddatasets
from our previous work to compare the performance of the T5
model with the baseline [46].
3.1 Data Collection and Analysis
Toanswerthefirstfourresearchquestions,weexperimentwiththe
best configuration of both the pre-trained and non pre-trained T5
model on the test set of the fine-tuning dataset reported in Table 1.
Rememberthatforeachofthethreetaskswesupport(i.e., the
onesthatmaptoRQ 1,RQ2,andRQ 3)the16,779testsetinstances
arethesametriplets <ms,cnl,mr>.Theonlydifferenceisthat:in
RQ1the model has been trained (and is tested) to take as input ms
and produce mr;i nR Q2it takes as input msandcnland produces
mr;i nR Q3it takes as input msand produces cnl.
By running the models on the test sets, we report for each of
the three tasks the percentage of “perfect predictions”, namely the
cases in which the output of the model is the expected one. For
example, in the case of RQ 3, this means that the model was able,
givenmsasinput,togenerateacomment cnlidenticaltotheone
manually written by the reviewer who inspected ms.
Besides computing theperfect predictions, in RQ 3(i.e.,the task
in whichthe model isrequired to generatenatural language text),
wealsocomputetheBLEU(BilingualEvaluationUnderstudy)score
of the predictions [ 32]. BLEU assesses the quality of the automati-
cally generated text. The BLEU score ranges between 0 and 1, with
1 indicating, in our case, that the natural language comment gener-
atedbythemodelisidenticaltotheonemanuallywrittenbythe
reviewer. We use the BLEU-4 variant, that computes the overlap in
terms of 4-grams between the generated and the reference text.
In RQ1and RQ2(i.e.,in the tasks in which the model is required
to generate code), we adopt instead the CodeBLEU [ 37], a recently
proposed similarity metric inspired by the BLEU score but tailored
to assess the quality of automatically generated code.
Differently from BLEU, CodeBLEU computes not only an “n-
grambasedsimilarity”butitalsoconsidershowsimilartheabstract
syntax tree and the data-flow of the generated and the reference
codeare.Ren etal.[37],whoproposedtheCodeBLEU,showedthat
theirmetricbettercorrelateswithdevelopers’perceptionofcode
similarity as compared to the BLEU metric.
ConcerningRQ 4,wecomparetheresults(i.e., perfectpredictions,
BLEU, CodeBLEU) achieved by the T5 model with and withoutpre-training. We also statistically compare the two models (i.e.,
with/withoutpre-training)usingtheMcNemar’stest[ 30]andOdds
Ratios(ORs)ontheperfectpredictionstheycangenerate.Asforthe
confidenceofthepredictions,wetakethebestperformingmodel
(i.e.,the one with pre-training) and split its predictions into ten
bucketsbasedontheirconfidence cgoingfrom0.0to1.0atstepsof
0.1(i.e.,thefirstintervalincludesallpredictionshavingaconfidence
cwith 0<c≤0.1, the last interval has 0.9 <c≤1). Then, we
report for each interval the percentage of perfect predictions.
Finally,inRQ 5,wecompareT5withthebaseline[ 46]onthetwo
tasks automated in our previous work (i.e., the ones related to our
RQ1and RQ 2).Asmetricsforthecomparisons,weusedthepercentageofperfect
predictions and the CodeBLEU of the predictions. We compared
the two techniques in several scenarios. First, we used the dataset
from [46] featuring 17,194 triplets <ms,cnl,mr>. By performing
somechecksonthisdataset,wenoticedthatafewinstances(97)
had comments ( cnl) not written in English or containing invalid
unicode characters that did not allow our tokenizer to work. Thus,
we excluded those instances from the training and the test sets
shared by the authors. The training set has then been used to (i)
train the baseline [ 46]; and (ii) fine-tune the T5 model without any
pre-training.Inthisway,wecancomparetheperformanceofthe
two modelson the testset when trainedon exactly the samedata.
Important to notice is that the baseline has been trained and testedonabstractedcode(asdonein[
46]),whileT5workeddirectlywith
the raw source code.
Ontopofthis,wealsoreporttheperformanceofthepre-trained
T5modelwhenrunonthetestsetfrom[ 46].Thispre-trainedmodel
has been fine-tuned using the training dataset in [ 46]. Clearly, this
analysisfavorsT5sinceithasbeentrainedonmoredata(i.e., the
pre-training dataset). However, it pro vides additional hints into
theroleplayedbythepre-trainingandontheeffectivenessofthe
T5 model in general. Besides reporting descriptive statistics, we
statisticallycomparethetwomodelsusingtheMcNemar’stest[ 30]
andOddsRatios(ORs)ontheperfectpredictionstheycangenerate.
Sincemultiplecomparisonsareinvolved(e.g., comparingthepre-
trainedandthenonpre-trainedmodeltothebaseline),weadjust
thep-values using the Holm’s correction [24].
4 RESULTS DISCUSSION
We start by answering RQ1-RQ3(Section 4.1), presenting the per-
formance of T5 in the three tasks we aim at automating. Then, we
discuss the impact on the performance of the pre-training and the
reliabilityoftheconfidencelevelasaproxyforthequalityofthe
predictions (Section 4.2). Finally, we compare T5 with the baseline
[46] (Section 4.3).
4.1 RQ 1-RQ3: Performance of T5
Fig.1reportstwographsforeachtask.Thelinechartontopshows
the percentage of perfect predictions ( y-axis) achieved by T5 for
different beam sizes ( x-axis); the continuous line represents the
pre-trainedversionofthemodel,whilethedashedlinethenonpre-
trainedone.TheboxplotsatthebottomreporttheCodeBLEUfor
thetwocode-generationtasks(i.e.,code-to-code andcode&comment-
to-code) and the BLEU score for the code-to-comment task in which
text is generated. Lighter blue represents the pre-trained model.
We start by commenting on the perfect predictions (line charts).
At a first sight, the performance of the model might seem quite
low.Forexample,inthecaseof code-to-code atk=1(i.e.,asingle
prediction is proposed by T5), both the pre-trained and the nonpre-trained models achieve
∼5% of perfect predictions (751 and
863 instances correctly predicted with and without pre-training,
respectively).However,sucharesultshouldbeconsideredinthe
contextofwhatwasreportedbythestate-of-the-arttechnique[ 46]
that,onamuchsimplertestdataset,achievedforthesametaskand
same beam size 2.91% of perfect predictions.
2296
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Pre-Trained Models to Boost Code Review Automation ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Beam size Beam size Beam sizeCode BLEU
Code BLEU
BLEUPerfect Predictions (%) Perfect Predictions (%)
Perfect Predictions (%)code-to-code code&comment-to-code code-to-comment
T5 Pre-Trained T5 Non Pre-Trained
Figure 1: Results T5 dataset large
Similar observations can be made for the code&comment-to-code
task, where at k=1 T5 can generate 14.08% (2,363 instances)
and 12.06% (2,024) perfect predictions when pre-trained and not,
respectively. For this task, in our previous work [ 46], we achieved
onasimplerdataset12.16%perfectpredictions.Wedirectlycompare
the two approaches in RQ 5.
Interestingly, increasing the beam size from 1 to 10 does only
result in marginal imp rovements for all tasks. The largest improve-
ment is obtained for the code&comment-to-code, where we move
from 14.08% ( k=1) to 18.88% ( k=10) of perfect predictions for
the pre-trained model. Given the goal of our approach, we believe
that the most relevant performance are those achieved at k=1.
Indeed,providingseveralrecommendationstoinspecttoade-
veloper might be counterproductive, especially considering that
the recommendations are entire methods in the case of the two
code-generation tasks.
Movingtothe code-to-comment task,T5strugglesinformulat-
ing natural language comments identical to the ones written by
reviewers.Thepre-trainedmodel,at k=1,generates356correct
comments (2.12%) against the 324 (1.93%) of the non pre-trained
model. These numbers only slightly increase at k=10, with a
maximum of 2.44% perfect predictions achieved with pre-training.
ThetoppartofFig.2showstwoexamplesofperfectpredictions
generated by the model for each task. A dashed line separates the
two examples within each task. For the code-to-code task, the first
code in each example represents the input of the model, while the
seconditsoutput.Wehighlightedinboldthepartsofcodechanged
by the model and replaced irrelevant parts of the methods with[...] to save space. In the first code-to-code example, T5 removes
an unneeded
instanceof check, since FileSystemDataset is a
subclass of Dataset. Instead, the second example simplifies the
checkingfortheexistenceofacluster,providingameaningfulerror
message. This second case cannot be supported by the baseline[
46], since it requires the introduction of new code tokens that
were not present in the input code. Remember that, these being
perfectpredictions,theimplementedchangesareidenticaltothose
performed by developers during code review.
Forthecode&comment-to-code task,theinputprovidedbythe
modelincludesthecommentwrittenbythereviewerandrequir-
ingaspecificchangetothepartofcodehighlightedinorange.In
thefirstexample,thereviewersuggeststouseaspecificobjectto
performthe nullcheckandT5correctlyimplementsthechange.Thesecondoneisinterestingbecause,despitethereviewerhigh-
lighting return null astherelevantcodefortheircomment(“else
isredundant ”),themodelcorrectly understands thattheactionto
take is the removal of the unneeded elsestatement.
Finally, for the code-to-comment task, we report the code pro-
videdasinputtothemodel(firstline)withthecommentitgenerated
asoutput(secondline).Inthefirstexample,T5suggests(asdone
by therealreviewer) toadd a nullcheck, alsoshowing the code
needed for its implementation. This code is not just a template, but
it is suitable for the provided input code (it refers to the supplier
object). In the second example, T5 suggests to rename an identifier,
providing valid recommendations for the renaming.
LookingatthebottomofFig.1,theresultsintermsofCodeBLEU
show a median higher than 0.80 for all beam sizes and for both
code-generation tasks. However, while we report these values for
completenessandforbeingconsistentwithwhatdoneinsimilar
works[45,46,50],theysaylittleaboutthequalityofthepredictions
andtheyaremostlyuseful forfutureworkthatwantstocompare
withourapproach(completedistributionsareavailableinourrepli-
cationpackage[ 8]).Indeed,itisdifficulttoproperlyinterpretthese
values for two reasons. First, there is no accepted threshold above
whichgoodperformancecanbeclaimed.Second,asalsodonein
previousworksproposingmodelstakingasinputacodesnippet
and providing as output the same code “revised” in some way (e.g.,
withafixedbug[ 45],withasinglestatementadded[ 50],orwith
review-related changes implemented [ 46]), we computed the Code-
BLEUbetweenthepredictedandthetargetcode(twomethodsin
ourcase).However,theinputprovidedtothemodelisalreadyquite
similar to the target output, which means that a model taking as
input a method andnot implementing any change onit, is likely to
obtain high values of CodeBLEU. For this reason, we mostly focus
our discussion on perfectpredictions. Concerning the BLEU score
achievedinthe code-to-comment task,themedianrangesaround
0.10 (see Fig. 1). Such a result is expected given the low percentage
of perfect predictions achieved for this task.
Goingbacktotheperfectpredictions,theresultsreportedinthe
linechartsinFig.1representalowerboundfortheperformance
of our approach. Indeed, we consider a prediction as “perfect” only
ifitisidenticaltothereferenceone.Forexample,inthecaseofthe
code-to-comment task, the natural language comment generated by
T5 is classified as correct only if it is equal to the reference one,
including punctuation.
2297
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota
Perfect predictions
code-to-code
public ConfigBuilder readFrom(View <?> view) { if (view instanceof  Dataset && view instanceof  FileSystemDataset)  
{ FileSystemDataset dataset = (FileSystemDataset) view; [...] }
public ConfigBuilder readFrom(View <?> view) { if (view instanceof  FileSystemDataset)
{ FileSystemDataset dataset = (FileSystemDataset) view; [...] }
public Response getCustomizedStateAggregationConfig(@PathParam("clusterId") String clusterId) { 
HelixZkClient zkClient = getHelixZkClient(); if (!ZKUtil.isClusterSetup(clusterId, zkClient)) { return notFound();}  [...] }
public Response getCustomizedStateAggregationConfig(@PathParam("clusterId") String clusterId) { 
if (!doesClusterExist(clusterId)) { return notFound( String.format("Cluster %s does not exist", clusterId));} [...] }
code&comment-to-code
private String getBillingFrequencyDescription(Award award) { if (ObjectUtils.isNull(award) || ObjectUtils.isNull(award.getBillingFrequency()))  { [...] }“I suggest ObjectUtils check for nulls”
private String getBillingFrequencyDescription(Award award) { if (award == null || award.getBillingFrequency() == null) { [...] }
public <T extends  IRemoteConnection.Service > T getService([...]) { if ([...]) { return [...]; } return null ; }public <T extends  IRemoteConnection.Service > T getService([...]) { if ([...]) { return [...]; } else { return null; } }“else is redundant”
code-to-comment
static <E,T> Validation <E,T> valid(Supplier <? extends  T> supplier) { return new Valid<>(supplier.get()); }
“Please add a check Objects.requireNonNull(supplier, "supplier is null");”
public List<[...]> getExecuteBefore() { Rules ann = this.getClass().getAnnotation(Rules. class); if(ann != null) [...] }
“Rename ‘ann’ to ‘rules’, ‘rulesAnnotation’ or something more descriptive.”
Alternative and valid predictions
code&comment-to-code
public UserDTO addUser(UserDTO userResource) { [...] return UserDTO.createInstanceWithPrivateData(user);  }“Inline this variable”
public UserDTO addUser(UserDTO userResource) { [...] UserDTO savedUser = UserDTO.createInstanceWithPrivateData(user);  return savedUser;  }
code-to-comment
“Extract the building of the ResponseMessage to it's own variable (in eclipse, select the text, right-click > refactor > extract local variable / select code + shift+alt+L). This will make the code a 
bit more readable, especially when you'll be passing in other things besides the ResponseMessage.”“Please make this one a variable as well”
public void handleSetDeviceLifecycleStatusByChannelResponse([...]) { [...] ResponseMessage.newResponseMessageBuilder().[...])}
Figure 2: Examples of perfect and alternative predictions
However, it is possible that a natural language comment gen-
erated by T5 is different but semantically equivalent to the one
written by the developer (e.g., “variable vshould be private” vs
“changevvisibilitytoprivate”).Similarobservationsholdforthe
two code-generation tasks (e.g., a reviewer’s comment could be
addressed in different but semantically equivalent ways).
Tohaveanideaonthenumberofvaluablepredictionspresent
among those classified as “wrong” (i.e., the non-perfect predic-
tions),threeauthorsmanuallyanalyzedasampleof100“wrong”
predictions for each task (300 in total). The analysis was done intwo meetings in which each instance was discussed by all threeauthors. The goal was to classify each instance into one of three
categories:(i)“semanticallyequivalent”(i.e., thegeneratedcode/-
commentisdifferentbutsemanticallyequivalenttothereference
one); (ii) “alternative solution” (i.e., the generated code/comment is
not semantically equivalent, but valuable); or (iii) “wrong” (i.e., the
generatedcode/commentisnotmeaningfulfortheprovidedinput).
Since we also computed the confidence for each of the predictionsgenerated by T5, rather than randomly selecting the 300 instances
to inspect, we decided to target for each task the top-100 wrong
predictions generated by the model in terms of confidence. Indeed,
thosecases areparticularlyinteresting, sincetheyrepresentwrong
predictions for which, however, the model is quite confident.Table3:Manualanalysisof100“wrong”predictionspertask
Task Semantically Equivalent Alternative Solution Wrong
code-to-code 11 0 89
code&comment-to-code 65 6 38
code-to-comment 36 10 54
Table3showstheresultsofourmanualanalysis.Forthe code-
to-codewe observed that, in most cases (89%) the model actually
generateswrongpredictionsthatarenotinlinewiththechanges
implementedbythedeveloper.Therearefewexceptionstothese
cases, mostly related to small changes in which the model madea decision different from that one of the developer but still valid
(e.g.,extractingastringintoavariableandusingadifferentname
for the extracted variable). More interesting are the results for the
other two tasks.
In the case of code&comment-to-code, we found that 62 out of
the100“wrong”predictionsweinspectedwereactuallyvalidim-
plementationsofthechangerecommendedbythereviewer.One
example is presented at the bottom of Fig. 2 (black background),
where we show the input provided to the model (i.e., the code in
the first line and the reviewer’s comment “Inline this variable ”) and
the output of the model right below. T5 successfully addressed the
reviewer’s comment.
2298
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Pre-Trained Models to Boost Code Review Automation ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USAPerfect Predictions (%)
Conﬁdence Conﬁdence Conﬁdencecode-to-co dec ode&comment-to-co dec ode-to-comment
Figure 3: Perfect predictions by confidence of the model
However,thepredictionisdifferentfromthetargetimplemen-
tation,sincethelatteralsoincludesanotherchangethatwasnot
explicitlyrequiredinthecodereview.Thiscaseisrepresentative
ofall56instancesweclassifiedas“alternativesolutions”forthis
taskand,giventhegoalofthe code&comment-to-code,webelieve
they represent good predictions.
Finally, also for the code-to-comment task, we found a large
number of “wrong” predictions that are actually valuable, with
36 of themeven being semantically equivalent(i.e., T5 formulated
a comment asking the same changes required by the reviewer, but
using a different wording). One example is reported at the very
bottomofFig.2.Whilethemodelonlyreceivedthecodeasinput
wealsoshowtheoriginalreviewer’scomment(i.e., “Pleasemake
thisoneavariableaswell ”)tomakeiteasiertoassesstherelevance
of the comment generated by T5 (i.e., “Extract the building ...”).
Overall, our analysis showed that the perfect predictions really
representalowerboundfortheperformanceofT5,especiallyfor
the two tasks in which natural language comments are involved.
4.2 RQ 4: Pre-training and confidence
In Fig. 1 we observed better performance for the pre-trained model
inthecode&comment-to-code andinthe code-to-comment task,while
thenonpre-trainedmodelperformedbetterinthe code-to-code task.
TheresultsoftheMcNemar’stestonthepredictionsat k=1,confirm
suchfindings:besidesthesignificantdifferenceconfirmedforall
tasks(p-value<0.01),theORsindicate85%and59%higherodds
of obtaining a perfect prediction using the pre-trained model in
thecode&comment-to-code (OR=1.85)andinthe code-to-comment
(OR=1.59)task,whileoddsare34%lowerinthe code-to-code task
(OR=0.66).Twoobservationsareworthtobemade.First,overall,
the pre-trained model seems to represent a more valuable solution.
Second, the lack of improvement in the code-to-code task can be
explained by the pre-training and fine-tuning we performed. In-deed, the code-to-code task only focuses on source code, with no
natural language in the input nor in the output. The fine-tuning
stage,focusedonsourcecode,wasprobablysufficienttothemodel
to learn about the code syntax and the possible transformationsto perform. The additional pre-training, also including technicalEnglish, did not benefit the model for the code-to-code task. The
othertwotasks,instead,eitherincludenaturallanguageasinput
(code&comment-to-code )orrequireitsgenerationasoutput(code-to-
comment), obtaining a boost of performance from the pre-training.
Fig.3depictsthepercentageofperfectpredictions( y-axis)within
eachconfidenceinterval(from0.0-0.1upto0.9-1.0, x-axis)when
using the pre-trained model and k=1. To better interpret the re-
ported results, the gray line represents the overall performance of
themodelwhenconsideringallpredictions(e.g., 4.48%ofperfect
predictions for the code-to-code task).In all three tasks, we observe a clear trend, with the predictions
in the highest confidence bucket (0.9-1.0) ensuring substantially
better performance than the overall trend. When only considering
the predictions in this bucket, the percentage of perfect predic-
tionsincreasesto:14.24%for code-to-code (fromanoverall4.48%),
28.23% for code&comment-to-code (overall=14.08%), and 22.23% for
code-to-comment (overall=2.12%). Considering the complexity of
the addressed tasks, the jump in performance is substantial andindicates the usability of the confidence level as a proxy for the
predictionquality.Also,whilethepercentageofperfectpredictions
is quite limited, with seven out of ten predictions being wrong
inthebest-casescenario(28.23%for code&comment-to-code ),itis
worth considering what previously observed in our manual analy-
sis,with“valuable”predictionswhichareclassifiedas“wrong”in
our quantitative analysis.
4.3 RQ 5: Comparison with the baseline [46]
Fig. 4 compares the performance achieved by the T5 model with
those obtained by the baseline [46].
In the line charts the continuous lines represent the pre-trained
T5, the dashed lines non pre-trained T5, and the dotted lines the
baseline.Twoimportantpointsareworthremembering:First,the
results in Fig. 4 have been computed on the test set used in [ 46].
Indeed, the performance in terms of perfect predictions are sub-
stantiallyhigherascomparedtothoseinFig.1(seevaluesonthe
y-axis),duetothesimplerinstancesfeaturedinthisdataset.Second,
thebaselinehasbeentrainedandtestedon abstractedcode(asin
the original paper), while T5 worked on raw source code.
Whenk=1, T5 achieves substantially better performance. The
results of the statistical test in Table 4 always show a significant
differenceinfavorofT5(adjusted p-value<0.01),withORsranging
from 1.69 (non pre-trained T5 vs[46] in thecode-to-code task) to
11.48 (pre-trained T5 vs[46] in thecode&comment-to-code task).
Thepre-trainedT5inthiscaseperformsbetterthanthenonpre-
trainedoneforbothtasks.Thisislikelyduetothelimitedsizeofthe
fine-tuning dataset used in this comparison. Indeed, to have a fair
comparisonwith[ 46],wefine-tunedT5onthetrainingsetweused
in [46] and composed by ∼13.5k instances (vs the∼134k we had in
ourfine-tuningdatasetwhenansweringRQ 1-RQ4).Thisisprobably
notsufficienttoeffectivelytrainalargemodelsuchasT5,andmakes
the instances used in the pre-training fundamental to further learn
aboutthelanguage.Still,evenwithoutpre-training,T5outperforms
the baseline when k=1. For example, in the code&comment-to-code
task, the baseline achieves 9.48% perfect predictions, against the
15.46% of the non pre-trained T5, and the 29.74% of the pre-trained
T5. The baseline observes a stronger improvement with the in-
creasing of k(i.e.,the beam size) as compared to T5 (see Fig. 4).
We believe this is due to usage of the abstraction. Indeed, when
workingwithabstractedcodethe“searchspace”(i.e., thenumberof
possiblesolutionsthatcanbegeneratedwiththegivenvocabulary)ismuchmorelimitedsincethemodeldoesnotdealwithidentifiers
andliterals.Attemptingtenpredictionsinasmallersearchspace
is more likely to result in correct predictions. The results of the
CodeBLEUconfirmthetrendobservedwiththeperfectpredictions,
with the pre-trained T5 being the best model.
2299
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota
code-to-code
Beam size Beam sizecode&comment-to-codeCode BLEU Perfect Predictions (%)
T5 Pre-Trained T5 Non Pre-Trained Baseline
Figure 4: T5 vs.baseline [46]
We also looked at the union of perfect predictions generated by
thetwoapproachesonthetestsettoverifythecomplementarity
ofthetechniques.Onthe code-to-code (code&comment-to-code )task
weobservedthat15%(24%)ofperfectpredictionsaresharedbyboth
approaches (i.e., both succeed), 65% (70%) are perfect predictions
only for T5, and 20% (6%) only for the baseline.
Table 4: RQ 5: McNemar’s test (adj. p-value and OR)
Task Test p-value OR
code-to-codeT5 pre-trained vs[46] <0.01 2.90
T5 non pre-trained vs[46] <0.01 1.69
T5 pre-trained vsT5 non pre-trained <0.01 2.50
code&comment-to-codeT5 pre-trained vs[46] <0.01 11.48
T5 non pre-trained vs[46] <0.01 2.38
T5 pre-trained vsT5 non pre-trained <0.01 5.69
5 THREATS TO VALIDITY
Construct validity. As explained in Section 2 we took care of
cleaning the datasets used in our study by removing duplicates
and noisy data points to the extent possible. Still, we are awarethatproblematicinstancesmaybepresent,especiallyinthenew
(large)datasetwebuilt.Thismanifests,forexample,innon-English
comments,orinsomewrong“links”betweencommentsandimple-mentation(e.g., weassumethat
mrimplementedachangedescribed
incnlwhile, in fact, it implemented another change).
Internal validity. We did not fully explore the role played by
the T5 parameters on its performance. Indeed, our hyperparameter
tuning was limited to variations in the learning rate, as done in
previous work [ 28]. For the other parameters we relied on the best
architecture identified by Raffel et al.[35]. We acknowledge that
additional tuning can result in improved performance.
Externalvalidity. RQ1-RQ4havebeenansweredusingadataset
beingoneorderofmagnitudelargerascomparedtoourprevious
work on automating code review tasks [ 46]. However, our findings
are limited to Java. Concerning RQ 5in which we compare with
the baseline [ 46], we only used the dataset presented in [ 46]. This
is due to the fact that our previous approach [ 46] requires code
abstraction and, as previously explained, cannot work on instances
having new identifiers and literals inserted during the code review
process.Thenewdatasetusedinthispaperhasnotbeenbuiltwith
such a constraint in mind and, thus, it is not suitable for direct
comparison.6 RELATED WORK
Our work relates to three research areas: (i) DL techniques to auto-
matesoftware-relatedtasks,(ii)empiricalstudiesoncodereview,
and (iii) works providing recommendations on how to optimize
thecode reviewprocess and/orpresenting techniques topartially
automateit.Herewefocusonthethirdresearcharea,whileforthe
firsttwowepointthereadertothesystematicliteraturereviews
byWatson etal.[49](deeplearninginsoftwareengineering)and
by Davila and Nunes [20] (modern code review).
Optimizing/automatingthecodereviewprocess. Bystudy-
ing tools and techniques supporting code review, Tymchuk et al.
[47] concluded that popular code review platforms (e.g., Gerrit,
Code Flow, Phabricator) mostly offer the same basic functionalities
with little support for automating tasks. Such a finding has been
confirmedbyPascarella etal.[34].Also,inastudyperformedby
Lewiset al.[27] at Google, the authors show that while developers
are excited by the idea of embracing automated solutions for code
review, they find current solutions not to be ready for daily use.
Startingfromtheseobservations,researchersstudiedpossibleop-
timizationsof thereviewprocess:Baum et al.[14]investigate the
effectoforderingsubmittedchangesinalternativewaysratherthaninalphabeticalorderthat,asshownbyBarnett etal.[
12]andBaum
and Schneider [ 13], is sub-optimal. Baum et al.[14] concluded that
smarterorderingisneededasthesizeofthepatchincreases,and
suggest to aggregate changed parts by relatedness.
Di Biaseet al.[21] studied the impact of the patch size on the
review’seffectiveness,findingthatsmallerpatches,whilenotin-creasing the defects found, affect how reviewers approach theirtask. Spadini et al.[
43] compared the effectiveness of a standard
code review process with test-driven code review (TDR), i.e.,the
reviewerinspectsthechangedtestcodebeforetheproductioncode.
They show that TDR does not boost the code review effectiveness.
Several researchers [ 23,33,51] suggest exploiting defect predic-
tionmodelsduringcodereview.Similarly,Balachandran[ 11]and
Singhet al.[42] suggest the use of static analysis tools to automati-
cally spot coding standard violations and common defects.
Concerningtheautomationofspecificcodereviewtasks,authors
proposedtechniquestooptimizethereviewers’assignment.Forex-ample,Al-Zubaidi etal.[
9]inopensourceandChouchen etal.[18]
in industrial contexts show how a multi-objective search-based
approach can simplify the code review triaging process.
Shiet al.[41] and Chouchen et al.[18] look at the automation
ofcodereviewfromasimilarperspective.Shi etal.[41]presenta
DL model taking as input the code submitted for review and the
revisedcodeimplementingthechangesrecommendedbyreviewers
and providing as output whether the change can be accepted ornot. Note that the change(s) required by the reviewer(s) are notconsidered by the model. Chouchen et al.[
18] use instead a set
ofqualitymetricsasfeaturesformachinelearningalgorithmstoclassify the quality of the code submitted for review. Recently,
Hellendoorn et al.[22] focus on the prediction ofthe location of a
possiblereviewer’scomment,showingthateventhissimpletaskis
challenging to automate.
The above discussed techniques [ 18,22,41] are complementary
totheapproachwepresentedin[ 46](and,asaconsequence,tothe
models experimented in this work).
2300
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. Using Pre-Trained Models to Boost Code Review Automation ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
While Shi et al.[41] and Chouchen et al.[18] assess the code
underreviewthrougha“booleananswer”(i.e., accepted/rejected
or well-written/badly-written), we attempt the automation of code
changesimplementedincodereview.Also,theapproachbyHel-
lendoorn et al.could be combined with the automation of the code-
to-comment task we presented.
7 CONCLUSION AND FUTURE WORK
Ourpaperstartsbydiscussinglimitationsintheapproachwere-
centlyproposedtoautomatecodereviewtasks[ 46].Wehighlighted
thattheusageofcodeabstractiondoesnotallowtosupportnon-
trivialcodereviewscenariosrequiringcodechangesresultingin
theintroductionofnewidentifiers/literals.Hence,weproposedthe
usage of a pre-trained T5 model [ 35] relying on a SentencePiece
[26] tokenizer to overcome such a limitation and work directly on
rawsourcecode.Ourempiricalevaluation,performedonamuch
larger andrealistic code review dataset,shows the improvements
brought by the T5 model that represents a step forward as com-
pared to the state-of-the-art [ 46] both in terms of applicability ( i.e.,
scenarios in which it can be applied) and performance. Still, the
levelofactualperformanceobservedmakesthesetechniquesfar
frombeingdeployableinpractice,callingformoreresearchincode
review automation.
Our future research agenda will be focused on designing im-
proved solutions to boost the prediction accuracy of these tech-niques (e.g., by combining different representations of code [
17]
and/orbyexploitingthemodel’sconfidenceasapossiblefilterto
select only high-quality recommendations).
The code and data used in our study are publicly available [8].
ACKNOWLEDGMENT
This project has received funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research
andinnovationprogramme(grantagreementNo.851720).W&M
has been supported in part by the NSF CCF-1955853 and CCF-
2007246 grants. Any opinions, findings, and conclusions expressed
hereinaretheauthors’anddonotnecessarilyreflectthoseofthe
sponsors.
REFERENCES
[1] [n.d.]. Gerrit. https://www.gerritcodereview.com/.
[2] [n.d.]. GitHub. https://github.com/.[3] [n.d.]. langdetect. https://pypi.org/project/langdetect/.[4] [n.d.]. Lizard. https://github.com/terryyin/lizard/.[5] [n.d.]. MSR mining platform. https://seart-ghs.si.usi.ch.[6] [n.d.]. pycld3. https://pypi.org/project/pycld3/.[7] [n.d.]. Stack Exchange Dumps. https://archive.org/details/stackexchange.[8]
2021. Replication Package. https://github.com/RosaliaTufano/code_review_
automation.
[9]Wisam Haitham Abbood Al-Zubaidi, Patanamon Thongtanunam, Hoa KhanhDam, Chakkrit Tantithamthavorn, and Aditya Ghose. 2020. Workload-awarereviewer recommendation using a multi-objective search-based approach. InProceedings of the 16th ACM International Conference on Predictive Models and
Data Analytics in Software Engineering. 21–30.
[10]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In Proceedings of the 2013 international conference
on software engineering. IEEE Press, 712–721.
[11]Vipin Balachandran. 2013. Reducing human effort and improving quality in
peercodereviewsusingautomaticstaticanalysisandreviewerrecommendation.
In2013 35th International Conference on Software Engineering (ICSE). 931–940.
https://doi.org/10.1109/ICSE.2013.6606642[12]MikeBarnett,ChristianBird,JoãoBrunet,andShuvenduK.Lahiri.2015. HelpingDevelopersHelpThemselves:AutomaticDecompositionofCodeReviewChange-
sets. InProceedings of the 37th International Conference on Software Engineering -
Volume 1 (ICSE ’15). 134–144.
[13]Tobias Baum and Kurt Schneider. 2016. On the need for a new generation of
codereviewtools.In InternationalConferenceonProduct-FocusedSoftwareProcess
Improvement. Springer, 301–308.
[14]Tobias Baum, Kurt Schneider, and Alberto Bacchelli. 2017. On the optimal orderof reading source code changes for review. In 2017 IEEE International Conference
on Software Maintenance and Evolution (ICSME). IEEE, 329–340.
[15]Gabriele Bavota and Barbara Russo. 2015. Four eyes are better than two: On the
impact of code reviews on software quality. In IEEE International Conference on
Software Maintenance and Evolution, (ICSME). 81–90.
[16]A. Bosu and J. C. Carver. 2013. Impact of Peer Code Review on Peer Impression
Formation: A Survey. In 2013 ACM / IEEE International Symposium on Empirical
Software Engineering and Measurement. 133–142.
[17]SaikatChakrabortyandBaishakhiRay.2021. OnMulti-ModalLearningofEditing
Source Code. arXiv:2108.06645 [cs.SE]
[18]MoatazChouchen,AliOuni,MohamedWiemMkaouer,RaulaGaikovinaKula,
and Katsuro Inoue. 2021. WhoReview: A multi-objective search-based approach
forcodereviewersrecommendationinmoderncodereview. AppliedSoftCom-
puting100 (2021), 106908.
[19]OzrenDabic,EmadAghajani,andGabrieleBavota.2021. SamplingProjectsin
GitHubforMSRStudies.In 18thIEEE/ACMInternationalConferenceonMining
Software Repositories, MSR 2021. IEEE, 560–564.
[20]Nicole Davila and Ingrid Nunes. 2021. A systematic literature review and taxon-
omy of modern code review. Journal of Systems and Software (2021), 110951.
[21]Marco di Biase, Magiel Bruntink, Arie van Deursen, and Alberto Bacchelli. 2019.
Theeffectsofchangedecompositiononcodereview—acontrolledexperiment.
PeerJ Computer Science 5 (2019), e193.
[22]Vincent J Hellendoorn, Jason Tsay, Manisha Mukherjee, and Martin Hirzel. 2021.
Towardsautomatingcodereviewatscale.In Proceedingsofthe29thACMJoint
Meeting on European Software Engineering Conference and Symposium on the
Foundationsof Software Engineering . 1479–1482.
[23]Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and NaoyasuUbayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in-
timedefectprediction.In 2019IEEE/ACM16thInternationalConferenceonMining
Software Repositories (MSR). IEEE, 34–45.
[24]Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scan-
dinavian journal of statistics (1979), 65–70.
[25]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. CoRRabs/1909.09436(2019). http://arxiv.org/abs/1909.09436
[26]TakuKudoandJohnRichardson.2018. SentencePiece:Asimpleandlanguage
independent subword tokenizer and detokenizer for Neural Text Processing.
CoRR(2018). arXiv:1808.06226
[27]Chris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, and
E James Whitehead. 2013. Does bug prediction support human developers? find-
ings from a google case study. In 2013 35th International Conference on Software
Engineering (ICSE). IEEE, 372–381.
[28]Antonio Mastropaolo, Simone Scalabrino,Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying theUsage of Text-To-Text Transfer Transformer to Support Code-Related Tasks.
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE).
IEEE, 336–347.
[29]ShaneMcIntosh,YasutakaKamei,BramAdams,andAhmedE.Hassan.2014. The
Impact of Code Review Coverageand Code Review Participationon SoftwareQuality: A Case Study of the Qt, VTK, and ITK Projects. In Proceedings of the
11th Working Conference on Mining Software Repositories (MSR 2014). 192–201.
[30]Quinn McNemar. 1947. Note on the sampling error of the difference between
correlated proportions or percentages. Psychometrika 12, 2 (1947), 153–157.
[31]RodrigoMorales,ShaneMcIntosh,andFoutseKhomh.2015. DoCodeReview
PracticesImpactDesignQuality?ACaseStudyoftheQt,VTK,andITKProjects.
InProc. of the 22nd Int’l Conf. on Software Analysis, Evolution, and Reengineering
(SANER). 171–180.
[32]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
AMethodforAutomaticEvaluationofMachineTranslation.In Proceedingsof
the40thAnnualMeetingonAssociationforComputationalLinguistics (ACL’02).
311–318.
[33]LucaPascarella,FabioPalomba,andAlbertoBacchelli.2019. Fine-grainedjust-
in-time defect prediction. Journal of Systems and Software 150 (2019), 22–36.
[34]LucaPascarella,DavideSpadini,FabioPalomba,MagielBruntink,andAlberto
Bacchelli. 2018. Information needs in contemporary code review. Proceedings of
the ACM on Human-Computer Interaction 2, CSCW (2018), 1–27.
[35]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the LimitsofTransferLearningwithaUnifiedText-to-TextTransformer. JournalofMachine
Learning Research 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html
2301
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota
[36]VeselinRaychev,MartinVechev,andEranYahav.2014. CodeCompletionwith
StatisticalLanguageModels.In Proceedingsofthe35thACMSIGPLANConference
onProgrammingLanguageDesignandImplementation(PLDI’14).ACM,419–428.
[37]ShuoRen,DayaGuo,ShuaiLu,LongZhou,ShujieLiu,DuyuTang,NeelSundare-
san,MingZhou,AmbrosioBlanco,andShuaiMa.2020. CodeBLEU:aMethod
for Automatic Evaluation of Code Synthesis. arXiv:2009.10297 [cs.SE]
[38]Peter C. Rigby and Christian Bird. 2013. Convergent Contemporary Software
PeerReviewPractices.In Proceedingsofthe20139thJointMeetingonFoundations
of Software Engineering (ESEC/FSE 2013). 202–212.
[39]Peter C. Rigby, Daniel M. German, Laura Cowen, and Margaret-Anne Storey.
2014. Peer Review on Open-Source Software Projects: Parameters, Statistical
Models, and Theory. ACM Trans. Softw. Eng. Methodol. 23, 4 (2014).
[40]Caitlin Sadowski, Emma Söderberg, Luke Church, Michal Sipko, and Alberto
Bacchelli. 2018. Modern Code Review: A Case Study at Google. In Proceedings of
the 40th International Conference on Software Engineering: Software Engineering
in Practice (ICSE-SEIP ’18). 181?190.
[41]Shu-TingShi,MingLi,DavidLo,FerdianThung,andXuanHuo.2019. Automatic
codereviewbylearningtherevisionofsourcecode.In ProceedingsoftheAAAI
Conference on Artificial Intelligence, Vol. 33. 4910–4917.
[42]Devarshi Singh, Varun Ramachandra Sekar, Kathryn T Stolee, and Brittany John-
son. 2017. Evaluating how static analysis tools can reduce code review effort.In2017 IEEE Symposium on Visual Languages and Human-Centric Computing
(VL/HCC) . IEEE, 101–105.
[43]DavideSpadini,FabioPalomba,TobiasBaum,StefanHanenberg,MagielBruntink,and Alberto Bacchelli. 2019. Test-driven code review: an empirical study. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
1061–1072.
[44]MicheleTufano,DawnDrain,AlexeySvyatkovskiy,ShaoKunDeng,andNeel
Sundaresan. 2020. Unit Test Case Generation with Transformers. CoRRabs/2009.05617(2020). https://arxiv.org/abs/2009.05617
[45]Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White,andDenysPoshyvanyk.2019. AnEmpiricalStudyonLearningBug-Fixing
Patches in the Wild via Neural Machine Translation. ACM Trans. Softw. Eng.
Methodol. 28, 4 (2019), 19:1–19:29.
[46]Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, andGabriele Bavota. 2021. Towards Automating Code Review Activities. In 43rd
International Conference on Software Engineering, ICSE’21 . https://arxiv.org/abs/
2101.02518
[47]YuriyTymchuk,AndreaMocci,andMicheleLanza.2015. Codereview:Veni,vidi,
vici.In2015IEEE22ndInternationalConferenceonSoftwareAnalysis,Evolution,
and Reengineering (SANER). IEEE, 151–160.
[48]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
y o un eed .I nAdvances in neural information processing systems. 5998–6008.
[49]CodyWatson,NathanCooper,DavidNaderPalacio,KevinMoran,andDenys
Poshyvanyk. 2021. A Systematic Literature Review on the Use of Deep Learning
inSoftwareEngineeringResearch. ACMTransactionsonSoftwareEngineering
and Methodology (2021).
[50]Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshy-
vanyk. 2020. On learning meaningful assert statements for unit test cases. In
ICSE ’20: 42nd International Conference on Software Engineering, Seoul, South
Korea, 27 June - 19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM,
1398–1409.
[51]Supatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tan-
tithamthavorn,HideakiHata,andKenichiMatsumoto.2020. PredictingDefective
Lines Using a Model-Agnostic Technique. CoRR(2020).
2302
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:59:43 UTC from IEEE Xplore.  Restrictions apply. 