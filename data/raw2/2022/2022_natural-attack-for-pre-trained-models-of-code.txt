Natural Attack for Pre-trained Models of Code
Zhou Yang, Jieke Shi, Junda He and David Lo
School of Computing and Information Systems
Singapore Management University
{zyang,jiekeshi,jundahe,davidlo}@smu.edu.sg
ABSTRACT
Pre-trained models of code have achieved success in many impor-
tantsoftwareengineeringtasks.However,thesepowerfulmodels
are vulnerable to adversarial attacks that slightly perturb model
inputs to make a victim model produce wrong outputs. Current
worksmainlyattackmodelsofcodewithexamplesthatpreserve op-
erational program semantics but ignore a fundamental requirement
foradversarialexamplegeneration:perturbationsshouldbenatural
tohuman judges, which we refer to as naturalness requirement.
Inthispaper,wepropose ALERT(NaturalnessAwareAttack),
a black-box attack that adversarially transforms inputs to make
victim models produce wrong outputs. Different from prior works,
this paper considers the naturalsemantic of generated examples
at the same time as preserving the operational semantic of original
inputs.Ouruserstudydemonstratesthathumandevelopersconsis-
tently consider that adversarial examples generated by ALERTare
more natural than those generated by the state-of-the-art work by
Zhangetal.thatignoresthenaturalnessrequirement.Onattack-
ing CodeBERT, our approach can achieve attack success rates of
53.62%,27.79%,and35.78%acrossthreedownstreamtasks:vulnera-
bilityprediction,clonedetectionandcodeauthorshipattribution.
On GraphCodeBERT, our approach can achieve average successrates of 76.95%, 7.96% and 61.47% on the three tasks. The aboveoutperforms the baseline by 14.07% and 18.56% on the two pre-
trainedmodelsonaverage.Finally,weinvestigatedthevalueofthe
generatedadversarialexamplestohardenvictimmodelsthrough
anadversarialfine-tuningprocedureanddemonstratedtheaccu-
racy of CodeBERT and GraphCodeBERT against ALERT-generated
adversarial examples increased by 87.59% and 92.32%, respectively.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging;Search-based software engineering ;â€¢Computing
methodologies â†’Neural networks.
KEYWORDS
Genetic Algorithm, Adversarial Attack, Pre-Trained Models
ACM Reference Format:
Zhou Yang, Jieke Shi, Junda He and David Lo. 2022. Natural Attack for
Pre-trained Models of Code. In 44th International Conference on Software
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510146Engineering (ICSE â€™22), May 21â€“29, 2022, Pittsburgh, PA, USA. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510146
1 INTRODUCTION
Recently,researchers[ 35,51,52]haveshownthatmodelsofcode
likecode2vec[3]andcode2seq[2],canoutputdifferentresultsfor
the two code snippets sharing the same operational semantics, one
of which is generated by renaming some variables in the other.
Themodifiedcodesnippetsarecalled adversarialexamples,andthe
models under attack are called victim models.
Naturalnessisafundamentalrequirementinadversarialexample
generation.Forexample,perturbationstoimagesareconstrained
with the infinity norm to ensure naturalness [ 19,31]. Attack for
NLP models also requires adversarial examples to be fluent and
natural[24].Weproposethatthenaturalnessrequirementisalso
essentialforattackingmodelsofcode.Casalnuoveetal.[ 11]pr o-
vide a dual-channel view of source code: machines that compile
and execute code mainly focus on the operational semantics, while
developers often care about natural semantics of code (e.g., names
ofvariables)thatcanassisthumancomprehension.Althoughmany
automated tools have been included into the software develop-
mentprocess,thereisnodoubtthatsoftwaredevelopmentisstill
a process led by humans. Code that violates coding convention
or has poor variable names, may be acceptable for machines but
rejectedbyhumans.Forexample,Taoetal.[ 44]reportthat21.7%
ofpatchesinEclipseandMozillaprojectswererejectedbecausethe
patches used bad identifier names or violated coding conventions.
Asaresult,unnaturaladversarialexamplesmaynotevenpasscode
reviews and not to mention being merged into codebases.
Existing works on attacking models of code are effective [ 35,
51,52], but they focus on preserving operational semantics and
barelypayattentiontowhetheradversarialexamplesarenaturalto
human judges. For instance, the state-of-the-art black-box method,
MHM [52], randomly selects replacements from a fixed set of vari-
able names without considering semantic relationships between
original variables and their substitutes. Figure 1(b) shows an adver-sarialexamplegeneratedbyreplacingthevariablename
bufferin
Figure1(a)to qmp_async_cmd_handler .Eventhoughthenewpro-
grampreservestheoperationalsemantics, qmp_async_cmd_handler
isnotanaturalreplacementof buffertohumanjudgesconsidering
the context (surrounding code). The natural semantics (i.e., human
understanding)of bufferclearlydonotoverlapwith qmp_async_c
md_handler . In this paper, we argue that adversarial examples for
models of code should consider preserving the semantics at two
levels:operationalsemantics(cateringformachinesasaudience)
and natural semantics (catering for humans as audience).
The neglect of naturalness requirements in current attack meth-
odsmotivatesustopropose ALERT(NaturalnessAwareAttack),
14822022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhou Yang, Jieke Shi, Junda He and David Lo.
static int buffer_empty(Buffer *buffer)
{
returnbuffer->offset == 0;
}
(a) Anoriginal codesnippetthat canbecorrectly
classified by a model fine-tuned on CodeBERT.static int buffer_empty(Buffer *qmp_async_cmd_handler )
{
returnqmp_async_cmd_handler->offset == 0;
}
(b) MHMgeneratesanadversarialexamplebyreplacingthevariable
buffertoqmp_async_cmd_handler.static int buffer_empty (Buffer * queue)
{
returnqueue->offset == 0;
}
(c)ALERTgeneratesanadversarialexampleby
replacing the variable buffertoqueue.
Figure 1: The original example in (a) is from the dataset used in Zhou et al.â€™s study [55]. Both MHM and ALERTcan generate
successfuladversarialexamplesbysubstitutingavariablename.ButHMHusesanunnaturalreplacementwhile ALERTuses
a more natural replacement that can better fit into the context and more closely relates to the original variable name.
ablack-box attackthatis awareofnatural semanticswhen gener-
atingadversarialexamplesofcode.SimilartoMHM[ 52],ALERT
renames variables to generate adversarial examples. Our approach
has three main parts: a natural perturbation generator, a heuristic
method that tries to generate adversarial examples as fast as possi-
ble,andageneticalgorithm-basedmethodtosearchforadversarial
examples more comprehensively in case the heuristic method fails.
This paper investigates the victim models that are fine-tuned
on the state-of-the-art pre-trained models, CodeBERT [ 14] and
GraphCodeBERT[ 20].ALERTfirstusesthe maskedlanguagepredic-
tionfunction in pre-trained models to generate natural substitutes.
Given a code snippet with several masked tokens, this function
canutilizethecontextinformationtopredictthepotentialvalues
ofmaskedtokens.WeleveragesuchafunctioninCodeBERTand
GraphCodeBERT to generate candidate substitutes for each vari-
able. Then,to pickthe substitutesthat aresemantically closer, we
use pre-trained models to compute contextualized embeddings of
thesenewtokensandcalculateitsCosinesimilarity(formeasuringthesemanticdistances)[
32]withembeddingsoftheoriginaltokens.
We rank these candidates according to Cosine similarities and only
select the top- ğ‘˜candidates as natural substitution candidates.
ALERThas two steps to search adversarial examples using natu-
ralsubstitutioncandidates.Itfirstusesagreedyalgorithm(Greedy-
Attack) and then applies a genetic algorithm (GA-Attack) if the
formerfails.TheGreedy-Attackdefinesametrictomeasurethe im-
portanceofvariablenamesinacodesnippetandstartstosubstitute
variables with the highest importance. An algorithm guided by the
importancecanfindsuccessfuladversarialexamplesfasterthanthe
randomsamplestrategyusedinMHM[ 52].Whensubstitutinga
variable, Greedy-Attack greedily selects the replacement (out of all
natural substitutes), from which the generated adversarial example
makesthevictimmodelproducelowerconfidenceontheground
truthlabel.Ifitfailstochangethepredictionresults,Greedy-Attack
continues to replace the next variable until all the variables are
consideredoranadversarialexampleisobtained.Butgenerating
adversarialexamplesforcodeisessentiallyacombinatorialprob-
lem, and the greedy algorithm may generate sub-optimal results.If the greedy algorithm fails, we use the GA-Attack to perform a
more comprehensive search.
Wefirstconductauserstudytoexaminewhethersearchingfrom
substitutes generated by ALERTcan produce adversarial examples
that are natural to human judges. Participants give a naturalness
score(1 for very unnatural and 5 for very natural) to each adversar-
ialexample.Resultsshowthatparticipantsconsistentlyprovidea
higher score to ALERT-generated examples (on average 3.95) thanexamples generated by the original MHM [ 52] (on average 2.18)
that selects substitutes randomly over all variables.
Then,weevaluateMHMand ALERTonthesixvictimmodels
(2pre-trainedmodels Ã—3tasks).Weconsiderthreerelevanttasks
that may be adversely affected by such an attack: vulnerability
prediction, clone detection and code authorship attribution.1Since
wearguethatadversarialexamplesshouldlooknaturaltohuman
developers,wemakeMHMsearchonthesamesetofnaturalsub-
stitutesgeneratedby ALERT.OnCodeBERT, ALERTcanachieve
attack success rates of 53.62%, 27.79%, and 35.78% across three
downstreamtasks.MHMonlyreaches35.66%,20.05%and19.27%,
respectively, which means that ALERTcan improve attack success
ratesoverMHMby17.96%,7.74%and16.51%.OnGraphCodeBERT,ourapproachachievessuccessratesof76.95%,7.96%and61.47%on
thesamethreetasks,outperformingMHMby21.78%,4.54%,and
29.36%. Finally, we investigate thevalue of generating adversarial
examples by using them to harden the models through an adver-
sarial fine-tuning strategy. We demonstrate that the robustness of
CodeBERTandGraphCodeBERTincreasedby87.59%and92.32%
afteradversarialfine-tuningwithexamplesgeneratedby ALERT.
The contributions of this paper include:
â€¢We are the first to highlight the naturalness requirement in gen-
eratingadversarialexamplesformodelsofcode.WealsoproposeALERTthatisawareofnaturalsemanticswhengeneratingadver-
sarialvariablesubstitutes.Auserstudyconfirmsthatusingthese
substitutescangenerateadversarialexamplesthatlooknatural
to human judges. ALERTcan also achieve higher attack success
rates than a previous method.
â€¢We are the first to develop adversarial attacks on CodeBERT and
GraphCodeBERT,andshowthatmodelsfine-tunedonstate-of-
the-art pre-trained models are vulnerable to such attacks.
â€¢We show the value of ALERT-generated examples: adversari-
allyfine-tuningvictimmodelswiththeseadversarialexamples
canimprove therobustness ofCodeBERT and GraphCodeBERT
againstALERTby 87.59% and 92.32%, respectively.
Therestofthispaperisorganizedasfollows.Section2briefly
describespreliminary materials.In Section3, weelaborate onthe
design of the proposed approach ALERT. We describe the settings
oftheexperimentinSection4,andpresenttheresultsofourexperi-mentsthatcomparetheperformanceof ALERTandsomebaselines
in Section 5. After summarising the threats to validity in Section 6,
Section 7 discusses some related works. Finally, we conclude the
paper and present future work in Section 8.
1Forexample,malicioususersmaywritevulnerablecodesnippetsanddonotwant
them to be identified.
1483
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Natural Attack for Pre-trained Models of Code ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
2 PRELIMINARIES
Thissectionbrieflyintroducessomepreliminaryinformationofthis
study,includingpre-trainedmodelsofcode,adversarialexample
generation for DNN models, and the Metropolis-Hastings Modifier
(MHM) method that we use as the baseline.
2.1 Pre-trained Models of Code
Pre-trainedmodelsofnaturallanguagelikeBERT[ 12]havebrought
breakthrough changes to many natural language processing (NLP)
tasks, e.g., sentiment analysis [53]. Recently, researchers have cre-
atedpre-trainedmodelsofcode[ 14,20]thatcanboosttheperfor-
mance on programming language processing tasks.
Feng et al. propose CodeBERT [ 14] that shares the same model
architectureasRoBERTa[ 29].CodeBERTistrainedonabimodal
dataset (CodeSearchNet [ 23]), a corpus consisting of natural lan-
guage queries and programming language outputs. CodeBERT has
twotrainingobjectives.Oneobjectiveis maskedlanguagemodeling
(MLM), whichaims to predict theoriginal tokens thatare masked
outinaninput.Theotherobjectiveis replacedtokendetection (RTD),
inwhichthemodelneedstodetectwhichtokensinagiveninputare
replaced. Experiment results have shown that in downstream tasks
like code classification or code search, which requires understand-
ingthecode,CodeBERTcouldyieldsuperiorperformance,although
itislesseffectiveincode-generationtasks.GraphCodeBERT[ 20]
also uses the same architecture as CodeBERT, but the former ad-ditionally considers the inherent structure of code, i.e., data flow
graph(DFG).GraphCodeBERTkeepstheMLMtrainingobjective
anddiscardstheRTDobjective.ItdesignstwoDFG-relatedtasks:
data flow edge prediction and node alignment. GraphCodeBERT
outperforms CodeBERT on four downstream tasks.
Therearesomeotherpre-trainedmodelsofcode.CuBERT[ 26]is
trainedonPythonsourcecodeandC-BERT[ 9]isamodeltrainedon
the top-100 starred GitHub C language repositories. CodeGPT [ 30]
is a Transformer-based language model pre-trained on program-ming languages for code generation tasks. In this paper, we fo-cus on analyzing CodeBERT and GraphCodeBERT, as they can
work on multiple programming languages. Besides, recent stud-
ies [30,49,54] have empirically shown that CodeBERT and Graph-
CodeBERT demonstrate state-of-the-art performance across multi-
ple code processing tasks.
2.2 Adversarial Example Generation
AlthoughDeepNeuralNetwork(DNN)modelshaveachievedgreat
success on many tasks, many research works [ 17,50] have shown
thatstate-of-the-artmodelsarevulnerabletoadversarialattacks.
Adversarial attacks aim to fool DNN models by slightly perturb-
ing the original inputs to generate adversarial examples that are
natural to human judges. Many techniques have been proposed to
showthatadversarialexamplescanbefoundformodelsindiffer-
ent domain, including, image classification [ 19,31], reinforcement
learning[ 18,21],sentimentanalysis[ 6],speechrecognition[ 10],
machine translation [13], etc.
According to the information of victim models that an attacker
canaccess,adversarialattackscanbedividedintotwotypes: white-
boxandblack-box.Inwhite-boxsettings,attackerscanaccessall
the information of the victim models, e.g., using model parametersto compute gradients. But white-box attacks often lack practicality
since the victim models are usually deployed remotely (e.g., on
cloud services), and typically attackers can only access the APIs to
query models as well as corresponding outputs. Black-box attacks
mean that an attacker only knows the inputs and outputs of victim
models(e.g., predictedlabels andcorresponding confidence).This
paper proposes a novel black-box attack to mislead models that
have the state-of-the-art performance.
Adversarial attacks can also be categorized into non-targeted
attack and targetedattack. Non-targeted attacks only aim to make
a victim model produces wrong predictions, while targeted attacks
forceavictimmodeltomakespecificpredictions.Forexample,a
targetedattackmayrequireaclassifiertopredictallthedeerimagesasahorsewhileanon-targetrequiresaclassifiertopredictanimage
incorrectly. The attack proposed in this paper is non-targeted.
2.3 Metropolis-Hastings Modifier (MHM)
Considering the fact that models can be remotely deployed so that
modelparametersareinaccessible,wefocusonblack-boxattacks
for models of code. This section introduces the baseline used in
thispaper.Zhangetal.[ 52]formalizestheprocessofadversarial
example generation as a sampling problem. The problem can be
decomposed into an iterative process consisting of three stages: (1)
selecting the variable to be renamed (2) selecting the substitutions
and (3) deciding whether to accept to replace the variable with
selected substitution.
Zhangetal.proposedMetropolis-HastingsModifier(MHM)[ 52],
a Metropolis-Hastings sampling-based [ 33] identifier renaming
technique to solve this problem and generate adversarial examples
formodelsofcode.Thismethodisablack-boxattackthatrandomly
selectsreplacementsforlocalvariablesandthenstrategicallyde-
termines to accept or reject replacements. It uses both predicted
labelsandcorrespondingconfidenceofthevictimmodeltoselect
adversarial examples more effectively. MHM pre-defines a large
collectionofvariablenames,fromwhichthereplacementsarese-
lected.However,neitherthecreationofthiscollectionnorselecting
replacements considers the natural semantics. As a result, MHM
producesexamplesthatarenotnaturaltohumanjudgments.For
example, suppose we change a variable name to an extremely long
string that is not semantically close to the original variable. In that
case, it may change the result of CodeBERT and GraphCodeBERT
since the long name will be tokenized into multiple sub-tokens,
impactingtheoutputsignificantly.However,developerscertainly
will not accept this code.
In this paper, similar to MHM, we use variable renaming as the
adversarial example generation technique and explore how to pro-
duceadversarialexamplesthatarenatural.WechooseMHM[ 52]
as our baseline as it does not require gradient information andalso uses fine-grained model outputs (i.e., predicted results and
correspondingconfidence)toperformrenamingandachievesgood
attack success rates (of one degree of magnitude higher than other
black-boxapproaches[ 35]).Forexample,Pouretal.â€™sapproach[ 35]
onlycausesanabsolutedecreaseof2.05%to code2vecâ€™sperformance
on the method name prediction task. To ensure that renamed vari-
ablesmakenochangesinoperationalsemantics,similartoMHM,
we only rename local variables in code snippets.
1484
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhou Yang, Jieke Shi, Junda He and David Lo.
3 METHODOLOGY
This paper proposes ALERT(Naturalness AwareAttack), a black-
boxattackthatleveragesthepre-trainedmodelsthatvictimmodels
arefine-tunedon.Itgeneratessubstitutesthatareawareofnatu-
ral semantics, which arecalled naturalness-aware substitutions in
thispaper. ALERTtakestwostepstosearchforadversarialexam-
ples that are likely to be natural to human judges. The first step
(Greedy-Attack) is optimized to find adversarial examples fast, and
thesecondstep(GA-Attack)isappliedtodoamorecomprehensive
search if the former fails.
3.1 Naturalness-Aware Substitution
ALERTleverages the two functions of pre-trained models to gener-
ate and select naturalness-aware substitutes for variables: masked
language prediction and contextualized embedding. To generate
natural substitutes for one single variable (e.g., index2dict ), it
operates in three steps:
Step 1. We convert code snippets into a format that CodeBERT or
GraphCodeBERT can take as inputs. Source code often contains
manydomain-specificabbreviations,jargonandtheircombinations,
which are usually not included in the vocabulary set and cause the
out-of-vocabulary problem [ 27,40]. Both CodeBERT and Graph-
CodeBERTuseByte-Pair-Encoding(BPE)[ 16,39]todealwithsuch
out-of-vocabulary problems by tokenizing a word into a list of sub-
tokens. For example, a variable index2dict can be converted into
three sub-words (index, 2,dict) and then fed into the model.
Step 2. Then, we generate potential substitutes for each sub-token.
For the sake of simplicity but without any loss of generality, let us
imagine a case where there is only one variable (e.g., index2dict )
that only appears once in an input. We use ğ‘‡=/angbracketleftğ‘¡1,ğ‘¡2,Â·Â·Â·,ğ‘¡ğ‘š/angbracketright
to represent the sequence of sub-tokens that BPE produces from
the variable name. For each sub-token in the sequence, we use the
masked language prediction function of CodeBERT or GraphCode-
BERT to produce a ranked list of potential substitute sub-tokens.
Insteadofjustpickingasingleoutput,weselectthetop- ğ‘—substi-
tutes. Intuitively, these substitutes are what pre-trained models
thinkcanfitthecontextbetter(comparedtoothersub-tokens).Still,
not all of them are semantically similar to the original sub-tokens.
Step3.Weassumethat /angbracketleftğ‘¡ğ‘–,ğ‘¡ğ‘–+1,ğ‘¡ğ‘–+2/angbracketrightisasequenceofsub-tokensof
onevariablename(e.g.,correspondingto index,2anddict).We
replacethesub-tokensintheoriginalsequence ğ‘‡withcandidate
sub-tokens(e.g., ğ‘¡/prime
ğ‘–,ğ‘¡/prime
ğ‘–+1,ğ‘¡/prime
ğ‘–+2)generatedinStep2toget ğ‘‡/prime.Afterthat,
the pre-trained model computes the contextualized embeddings of
each sub-token in ğ‘‡/prime, and we fetch the embeddings for ğ‘¡/prime
ğ‘–,ğ‘¡/prime
ğ‘–+1and
ğ‘¡/prime
ğ‘–+2.WeconcatenatethesenewembeddingsandcomputeitsCosine
similarity with concatenated embeddings of ğ‘¡ğ‘–,ğ‘¡ğ‘–+1andğ‘¡ğ‘–+2inğ‘‡.
Thecosinesimilarityisusedasametrictomeasuretowhatextenta
sequenceofcandidatesub-tokensissimilartotheoriginalvariableâ€™s
sequence of sub-tokens. We rank the substitutes in descending
order by the value of Cosine similarity. In the end, we select top- ğ‘˜
sequences of substitute sub-tokens with higher similarity values
and revert them into concrete variable names.
Onecodesnippetoftencontainsmultiplevariablesthatappearin
variouspositions.Algorithm1displayshowweapplytheabovepro-
cesstoeachvariableextractedfromthesourcecode.First,weuseaAlgorithm 1: Naturalness Aware Substitutes Generation
Input:ğ‘: input source code, ğ‘€: pre-trained model
Output:ğ‘ ğ‘¢ğ‘ğ‘ : substitutes for variables
1ğ‘ ğ‘¢ğ‘ğ‘ =âˆ…;
2ğ‘£ğ‘ğ‘Ÿğ‘ = extract( ğ‘);
3forğ‘£ğ‘ğ‘Ÿinğ‘£ğ‘ğ‘Ÿğ‘ do
4forğ‘œğ‘ğ‘inğ‘£ğ‘ğ‘Ÿ.ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘  do
5 #ğ‘£ğ‘ğ‘Ÿ.ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ğ‘  returns all occurrences of ğ‘£ğ‘ğ‘Ÿ;
6 ğ‘¡ğ‘šğ‘_ğ‘ ğ‘¢ğ‘ğ‘ = perturb( ğ‘œğ‘ğ‘,ğ‘,ğ‘€);
7 ğ‘ ğ‘¢ğ‘ğ‘ [ğ‘£ğ‘ğ‘Ÿ]=ğ‘ ğ‘¢ğ‘ğ‘ [ğ‘£ğ‘ğ‘Ÿ]/uniontext.1ğ‘¡ğ‘šğ‘_ğ‘ ğ‘¢ğ‘ğ‘ ;
8end
9ğ‘ ğ‘¢ğ‘ğ‘ [ğ‘£ğ‘ğ‘Ÿ]= filter(ğ‘ ğ‘¢ğ‘ğ‘ [ğ‘£ğ‘ğ‘Ÿ]);
10end
11returnğ‘ ğ‘¢ğ‘ğ‘ 
parser to extract variable names ( vars) from the input ( extract()
at Line 2) and then enumerate all the variables and their occur-
rences inthe code (Line3-4). Theprocess discussedabove isthen
appliedtoeachvariableoccurrencetogeneratepotentialsubstitutes
(perturb() atLine6).Wetaketheunionofthesubstitutessetsfor
alloccurrencesofavariable(Line7).Wethenremoveduplicated
and invalid words, e.g., those that do not comply with the variable
namingrulesorthosethatarekeywordsinprogramminglanguages
(filter() atLine9),afterwhichwereturnfilteredsubstitutes(Line
11).Werefertothesefilteredsubstitutesasthe naturalness-aware
substitutes.
3.2 Greedy-Attack
3.2.1 OverallImportanceScore. Toperformsemantic-preserving
transformation by renaming variables, an attacker first needs to
decide which tokens in a code snippet should be changed. Inspired
byadversarialreplacementsforNLPtasks[ 28]thatprioritizesmore
important tokens in a sentence, for each variable in a code snippet,
we first measure its contribution to helping the model make a cor-
rect prediction. We introduce a metric called the importance score
toquantifysuchcontribution.Formallyspeaking,theimportance
score of the ğ‘–ğ‘¡â„token in a code snippet ğ‘is defined as follow:
ğ¼ğ‘†ğ‘–=ğ‘€(ğ‘)[ğ‘¦]âˆ’ğ‘€(ğ‘âˆ—
âˆ’ğ‘–)[ğ‘¦] (1)
In the above formula, ğ‘¦is the ground truth label for ğ‘and
ğ‘€(ğ‘)[ğ‘¦]represents the confidence of ğ‘€â€™s output corresponding to
the labelğ‘¦. A new code snippet generated by substituting variable
names is called a variant. A variant ğ‘âˆ—
âˆ’ğ‘–and is created by replacing
theğ‘–ğ‘¡â„token (which must be a variable name) in ğ‘with/angbracketleftğ‘¢ğ‘›ğ‘˜/angbracketright,
whichmeansthattheliteralvalueatthispositionisunknown.Intu-
itively, the importance score approximates how knowing the value
of theğ‘–ğ‘¡â„token affects the modelâ€™s prediction on ğ‘.I fğ¼ğ‘†ğ‘–>0, it
means that the token ğ‘¡ğ‘–can help model make correct prediction on
ğ‘.AsstatedinSection3.1,onecodesnippetoftencontainsmultiple
variablesthatappearinmultiplepositions.Alltheoccurrencesof
a variable should be updated accordingly when performing adver-
sarialattacks,soweextendthedefinitionofimportancescorefor
asingle tokento theoverall importancescore (OIS)for avariable.
1485
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Natural Attack for Pre-trained Models of Code ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
OIS is computed as follow:
ğ‘‚ğ¼ğ‘†ğ‘£ğ‘ğ‘Ÿ=/summationdisplay.1
ğ‘–âˆˆğ‘£ğ‘ğ‘Ÿ[ğ‘ğ‘œğ‘ ]ğ¼ğ‘†ğ‘– (2)
whereğ‘£ğ‘ğ‘Ÿis a variable in ğ‘, andğ‘£ğ‘ğ‘Ÿ[ğ‘ğ‘œğ‘ ]means all occurrences of
ğ‘£ğ‘ğ‘Ÿinğ‘.ItisnoticedthatthedefinitionofOIScanbetterreflectthe
unique property of attacking models of programming languages as
compared to models of natural languages. Even though a variable
atonepositionistrivial,appearingmoreoftencanmakeitanim-
portantvariable(i.e.,avulnerableword)inadversarialattacks.The
overall importance score can be viewed as an analogy to the gradi-
ent information in white-box attacks. For example, if the gradients
arelargeratsomepositionsofinputs(e.g.,certainpixels),thenitis
easier to change the model outputs if we perturb those positions.
Based on tree-sitter2, a multi-language parser generator tool,
we implement a name extractor that can retrieve all the variable
namesfromsyntacticallyvalidcodesnippetswritteninC,Pythonor
Java. More specifically, to avoid altering the operational semantics,
weonlyextractthelocalvariablesthataredefinedandinitialized
within the scope of the code snippet and swap them with valid
variablenamesthathaveneveroccurredinthecode.Toimprove
accuracy, variable names that collide with a field name are also
excluded.Afterextraction,wecomputetheOISforeachvariable
and proceed to the next step.
3.2.2 Word Replacement. We design an OIS-based greedy algo-
rithm to search substitutes that can generate adversarial examples.
Algorithm 2 illustrates the process of this Greedy-Attack. First, we
rank extracted variables from the original code snippet in descend-
ing order according to their OIS (Line 2 to 3). We select the first
variable from them and find all its candidate substitutes generated
following the process described in Section 3.1 (Line 4 to 6). We
replacethevariableintheoriginalinputwiththesesubstitutesto
createalistofvariants,afterwhichthesevariantsaresenttoquery
the victim model. We collect returned results and see if at least one
variant makes the victim model make wrong predictions (Line 9
to12).Ifthereissuchavariant,theGreedy-Attackreturnsitasa
successful adversarial example. Otherwise, we replace the original
inputwiththevariantthatcanmostlyreducethevictimmodelâ€™s
confidence on the results and select the next variable to repeat the
above processes (Line 15). Greedy-Attack terminates either when a
successfuladversarialexampleisfound(Line11)orwhenallthe
extracted variables are enumerated (Line 17).
Considering OIS information is beneficial to the Greedy-Attack
in two aspects. First, as discussed in Section 3.2.1, if a variable
has a higher OIS, it indicates significant impacts of modifying this
variableinthecodesnippet.Givinghigherprioritiestovariables
withlargerOIScanhelpfindsuccessfuladversarialexamplesfaster,
whichmeansthatfewerqueriestothevictimmodelarerequired.
It increases the usability of our attack in practice since remotelydeployed black-box models often constrain the query frequency.
Secondly, finding successful adversarial examples early also means
fewer variables are modified in an original code snippet, making
the generated adversarial examples more natural to human judges.
2https://tree-sitter.github.io/tree-sitter/Algorithm 2: Greedy-Attack Workflow
Input:ğ‘: input source code, ğ‘ ğ‘¢ğ‘ğ‘ : substitutes for variables in ğ‘
Output:ğ‘/prime: adversarial example
1ğ‘/prime=ğ‘;
2ğ‘£ğ‘ğ‘Ÿğ‘ =ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡(ğ‘)# extract ğ‘£ğ‘ğ‘Ÿğ‘ fromğ‘;
3ğ‘£ğ‘ğ‘Ÿğ‘ = sort(ğ‘£ğ‘ğ‘Ÿğ‘ ) # sortğ‘£ğ‘ğ‘Ÿğ‘ according to OIS;
4forğ‘£ğ‘ğ‘Ÿinğ‘£ğ‘ğ‘Ÿğ‘ do
5ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘=âˆ…;
6forğ‘ ğ‘¢ğ‘inğ‘ ğ‘¢ğ‘ğ‘ [ğ‘£ğ‘ğ‘Ÿ]do
7 # iterate all the substitutes for ğ‘£ğ‘ğ‘Ÿ;
8 ğ‘¡ğ‘šğ‘_ğ‘= replcae( ğ‘/prime,ğ‘£ğ‘ğ‘Ÿ,ğ‘ ğ‘¢ğ‘);
9 ifğ‘€(ğ‘¡ğ‘šğ‘_ğ‘)â‰ ğ‘€(ğ‘)then
10 ğ‘/prime=ğ‘¡ğ‘šğ‘_ğ‘;
11 returnğ‘/prime;
12 end
13 ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘=ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘/uniontext.1ğ‘¡ğ‘šğ‘_ğ‘;
14end
15ğ‘/prime= select(ğ‘™ğ‘–ğ‘ ğ‘¡_ğ‘) # select the adversarial example with lowest
modelâ€™s confidence on the ground truth label;
16end
17returnğ‘/prime
3.3 GA-Attack
Finding appropriate substitutes to generate adversarial examples is
essentially a combinatorial optimization problem, whose objective
is to find theoptimal combination of variables and corresponding
substitutes that minimizes the victim modelâ€™s confidence on the
ground truth label. Greedy-Attack can run faster but may be stuck
in a single local optimal, leading to low attack success rates. We
also design an attack based on genetic algorithms (GA), called GA-
Attack. If the Greedy-Attack fails to find a successful adversarial
example, we apply GA-Attack to search more comprehensively.
Algorithm 3shows the overview ofhow GA-Attack works. Itfirst
initializes the population (Line 1, more detailed are given in Sec-tion 3.3.2), and then performs genetic operators to generate new
solutions(Line2to11).GA-Attackcomputesthefitnessfunction
(Section3.3.4)andkeepsolutionswithlargerfitnessvalues(Line
13). In the end, the algorithm returns the solution with the highest
fitness value (Line 15 to 16).
3.3.1 Chromosome Representation. In GA, the chromosome repre-
sents the solution to a target problem, and a chromosome consists
of a set of genes. In this paper, each gene is a pair of an original
variableanditssubstitution.GA-Attackrepresentschromosomesas
a list of such pairs. For example, assuming that only two variables
(aandb) can be replaced in an input program, the chromosome
/angbracketlefta:x,b:y/angbracketrightmeans replacing atoxandbtoy.
3.3.2 Population Initialization. In the running of GA, a population
(a set of chromosomes) evolves to solve the target problem. GA-
Attackmaintainsapopulationwhosesizeisthenumberofextracted
variablesthatcanbesubstituted.SinceGA-Attackwillbetriggered
only after Greedy-Attack fails, it can leverage the information dis-
coveredinthepreviousstep.Foreachextractedvariable,Greedy-
Attackfindsitssubstitutionthatcandecreasethevictimmodelâ€™s
confidence on the ground truth label most. Given one variable and
1486
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhou Yang, Jieke Shi, Junda He and David Lo.
Algorithm 3: GA-Attack Workflow
Input:ğ‘: input source code, ğ‘šğ‘ğ‘¥_ğ‘–ğ‘¡ğ‘’ğ‘Ÿ: max iteration, ğ‘Ÿ: crossover
rate,ğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘ ğ‘–ğ‘§ğ‘’: number of generated children in each
iteration
Output:ğ‘/prime: adversarial example
1ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = greedy_initialization( ğ‘);
2whilenot exceed ğ‘šğ‘ğ‘¥_ğ‘–ğ‘¡ğ‘’ğ‘Ÿdo
3ğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘™ğ‘–ğ‘ ğ‘¡=[];
4whileğ‘™ğ‘’ğ‘›(ğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘™ğ‘–ğ‘ )<ğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘ ğ‘–ğ‘§ğ‘’do
5 ğ‘=âˆª(0,1);
6 ifğ‘<ğ‘Ÿthen
7 ğ‘â„ğ‘–ğ‘™ğ‘‘= crossover( ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› );
8 else
9 ğ‘â„ğ‘–ğ‘™ğ‘‘= mutation( ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› );
10 end
11 ğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘™ğ‘–ğ‘ ğ‘¡.append(ğ‘â„ğ‘–ğ‘™ğ‘‘);
12end
13ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = selection( ğ‘ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆªğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘™ğ‘–ğ‘ ğ‘¡);
14end
15ğ‘/prime= argmax(population); # select the one with highest fitness value
16returnğ‘/prime
thesubstitutionfoundbyGreedy-Attack,GA-Attackcreatesachro-
mosome that only changes this variable to the substitution and
keeps other variables unchanged. The process is repeated for each
variable ina codesnippet to obtaina population. Forexample, as-
sumingthatthreevariables( a,bandc)areextractedfromaninput
program,andGreedy-Attacksuggests /angbracketlefta:x,b:y,c:z/angbracketright,GA-Attack
initializes a population of three chromosomes: /angbracketlefta:x,b:b,c:c/angbracketright,
/angbracketlefta:a,b:y,c:c/angbracketright. and/angbracketlefta:a,b:b,c:z/angbracketright.
3.3.3 Operators. Greedy-Attackrunsinmultipleiterations.Ineach
iteration, two genetic operators (mutation and crossover) are used
toproducenewchromosomes(i.e.,children).Weapplycrossover
with a probability of ğ‘Ÿand mutation with a probability of 1 âˆ’ğ‘Ÿ
(Line8).Themutationoperation(Line9)ontwochromosome( ğ‘1
andğ‘2)worksasfollows:wefirstrandomlyselectacut-offposition
â„, and replace ğ‘1â€™s genes after the position â„withğ‘2â€™s genes at
the corresponding positions. As an example, for two chromosomes
(ğ‘1=/angbracketlefta:x,b:y,c:c/angbracketrightandğ‘2=/angbracketlefta:x,b:b,c:z/angbracketright) and a cut-off
positionâ„=2,thechildgeneratedbycrossoveris /angbracketlefta:x,b:y,c:z/angbracketright.
Given a chromosome in the population, the mutation operator ran-
domly selects a gene and then replaces it with a randomly selected
substitute. For instance, ain/angbracketlefta:x,b:b/angbracketrightis selected and a:x
becomes a:aa.
3.3.4 Fitness Function. GA uses a fitness function to measure and
compare the quality of chromosomes in a population. A higher fit-
nessvalueindicatesthatthechromosome(variablesubstitutions)isclosertothetargetofthisproblem.Wecomputethevictimmodelâ€™s
confidence values with respect to the ground truth label on the
original input and the variant. The difference between confidence
values is used as the fitness value. Assuming ğ‘‡is the original input
andğ‘‡/primeis a variant corresponding to a chromosome, the fitness
value of this chromosome is computed by:
ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ =ğ‘€(ğ‘‡)[ğ‘¦]âˆ’ğ‘€(ğ‘‡/prime)[ğ‘¦] (3)After generating children in one iteration, we merge them to
the current population and perform a selection operator (Line 14).
GA-Attack always maintains a population of the same size (i.e.,numbers of extract variables). It discards the chromosomes that
have lower fitness values.
4 EXPERIMENT SETUP
4.1 Datasets and Tasks
We introduce the three downstream tasks and their corresponding
datasets used in our experiments. The statistics of datasets are
presented in Table 1.
4.1.1 Vulnerability Prediction. This task aims to predict whether a
givencodesnippetcontainsvulnerabilities.Weusethedatasetthat
waspreparedbyZhouetal.[ 55].Thedatasetisextractedfromtwo
popular open-sourced C projects: FFmpeg3and Qemu4. In Zhou et
al.â€™sdataset,27,318functionsarelabeledaseithercontainingvulner-
abilitiesorclean.ThisdatasetisincludedaspartoftheCodeXGLUE
benchmark [ 30] that has been used to investigate the effectiveness
of CodeBERT for vulnerability prediction. CodeXGLUE divides the
datasetintotraining,developmentandtestsetthatwereuseinthis
study.
4.1.2 Clone Detection. The clone detection task aims to check
whether two given code snippets are clones, i.e., equivalent in
operational semantics. BigCloneBench [ 42] is a broadly recognized
benchmarkforclonedetection,containingmorethansixmillion
actualclonepairsand260,000falseclonepairsfromvariousJava
projects. Each data point is a Java method. In total, the dataset has
covered ten frequently-used functionalities. Following the settings
of prior works [ 46,48], we filtered the data which do not have a
labelandthenbalancedthedatasettomaketheratiooftrueandfalse pairs to 1:1. To keep the experiment at a computationally
friendlyscale,werandomlyselect90,102examplesfortrainingand
4,000 for validation and testing.
4.1.3 Authorship Attribution. The authorship attribution task is to
identifytheauthorofagivencodesnippet.Wedidourexperiments
with the Google Code Jam (GCJ) dataset, which is originated from
GoogleCodeJam challenge,aglobalcodingcompetitionthatGoogle
annually hosts. Alsulami et al. [ 4] collected the GCJ dataset and
made it publicly available. The GCJ dataset contains 700 Python
files(70authorsandtenfilesforeachauthor),butwenoticethat
some Python files are C++ code. After discarding these C++ source
code files, we get 660 Python files in total. 20% of files are used for
testing, and 80% of files are for training.
4.2 Target Models
Thispaperinvestigatestherobustnessofthestate-of-the-artpre-
trainedmodels,CodeBERT[ 14]andGraphCodeBert[ 20].Toobtain
the victim models,we fine-tuneCodeBERT andGraphCodeBERT
on the three tasks mentioned in Section 4.1.
3https://www.ffmpeg.org/
4https://sites.google.com/view/devign
4https://codingcompetitions.withgoogle.com/codejam
1487
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Natural Attack for Pre-trained Models of Code ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 1: Statistics of Datasets and of Victim Models.
Tasks Train/Dev/Test Model Acc
Vulnerability
Prediction [55]21,854/2,732/2,732CodeBERT 63.76%GraphCodeBERT 63.65%
CloneDetection [42]90,102/4,000/4,000CodeBERT 96.97%GraphCodeBERT 97.36%
AuthorshipAttribution [4]528/â€“/132CodeBERT 90.35%GraphCodeBERT 89.48%
4.2.1 CodeBERT. CodeBERT [ 14] is a pre-trained model that is
capable of learning from bimodal data in the form of both pro-gramming languages and natural languages. When fine-tuningCodeBERT on vulnerability prediction and clone detection task,
we use the same parameter settings adopted in the CodeXGLUE
[30]exceptthatweincreasethemaximalinputlengthto512and
achievea slightlyhigherperformance thanresultsreported inthe
CodeXGLUE paper. Since there is no instruction on the hyper-parameter setting for fine-tuning on authorship attribution task,
we use the same settings, and the obtained model can achieve
90.35% accuracy, slightly higher than the accuracy of the LSTM
model reported in [4].
4.2.2 GraphCodeBERT. GraphCodeBERT [ 20] considers the inher-
ent structure of the program and takes advantage of the data-flow
representation. We set the maximal input length of GraphCode-
BERTto512andfollowthesamesettingforotherhyper-parametersintheGraphCodeBERTpaper[
20]tofine-tuneitonthethreedown-
streamtasks.Ontheclonedetectiontask,themodelcanachieveanaccuracyof97.36%,almostthesamewiththeperformanceof97.3%
reported in [ 20]. On the vulnerability prediction and authorship
attributiontask,GraphCodeBERTalsoachievestheperformance
that is comparable with the results of CodeBERT.
The performance of these models is displayed in Table 1. The
results we obtain are closed to results reported in their original
papers and another recent paper [ 14,20,30], highlighting that the
victim models used in our experiments are adequately fine-tuned.
4.3 Settings of Attacks
ALERThas a number of hyper-parameters to be set, including
the number of natural substitutions generated for each variableand parameters for GA-Attack in Algorithm 3. Our experimentsetting allows ALERTto generate 60 candidate substitutions for
each variable occurrence, and it selects the top 30 substitutions
rankedbythecosinesimilaritywithoriginalembedding.ForGA-
Attack, we set ğ‘â„ğ‘–ğ‘™ğ‘‘_ğ‘ ğ‘–ğ‘§ğ‘’as 64 and set a dynamic value for the
maximaliterations( ğ‘šğ‘ğ‘¥_ğ‘–ğ‘¡ğ‘’ğ‘Ÿ):thelargeroneof5timesthenumber
of extracted variables or 10. The crossover rate ğ‘Ÿis set as 0 .7.
We consider MHM [ 52] as our baseline, which has two hyper-
parameters: the maximum number of iterations and the number of
variables sampled in each iteration. The MHM paper [ 52] suggests
settingthelatteras30butdoesnotprovideastandardsettingfor
the maximal iterations. In each iteration, MHM needs to query the
victimmodelmanytimes,whichistime-consuming.Wesampled
5% testing data from the vulnerability prediction task and found
that over 95% successful adversarial examples are found before 100Figure2:Resultsoftheuserstudytoevaluatenaturalnessofadversarialexamples.They-axiscorrespondstotheaverageratings(5meansverynatural;1meansveryunnatural).The
x-axis represents distinguished independent participants.
iterations. To make the MHM experiment within a computational
friendly scale, we set the maximum number of iterations of MHM
to 100. The original MHM can only perturb C programs, so we
extend it to perturb Python and Java code.
5 EXPERIMENT RESULTS AND ANALYSIS
Inthissection,weperformexperimentstoanswerresearchques-
tions related to the performance of adversarial attacks. We care
aboutnaturalness,attacksuccessratesandscalabilityaswellasthe
valueofusingadversarialexamplestoimprovemodelrobustness
viaadversarialfine-tuning,whicharediscussedbyansweringthree
research questions, respectively.
RQ1. How natural are the adversarial examples
generated by ALERT?
Whengenerating substitutionsfor variables incode, ALERTtakes
thenaturalsemanticsofadversarialexamplesintoconsideration.
This research question explores whether these naturalness-aware
substitutions can help produce adversarial examples that are more
naturaltohumanjudges.Toanswerthisquestion,weconductauser
studytoanalyzethenaturalnessofexamplesgeneratedbyMHM,
MHM-NS and the proposed ALERTmethod. Unlike the original
MHMthatignoresthenaturalness,MHM-NSselectsareplacement
from the same pool of naturalness-aware substitutions as ALERT.
As the original MHM only works for code snippets written in C,
werandomlysamplesomecodesnippetsthatcanbesuccessfully
attackedby ALERT,MHM,andMHM-NSfromthedatasetofthe
vulnerability detection task (Section 4.1). We have introduced a
fewmoreconstraints whensamplingthe codesnippets: (1)Tosave
participantsfromreadinglongcodesnippets,weintentionallysam-
ple succinct and short code segments by limiting the code snippet
length to 200 tokens; and (2) The attack methods may choose to
replace different variables in the same code snippet; we only select
theexamplesforwhichatleastonevariableismodifiedbyallthe
threemethodstomakethecomparisonfair.Thereare196Ccode
1488
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhou Yang, Jieke Shi, Junda He and David Lo.
Table 2: Comparison results of Attack Success Rates (ASR) on attacking CodeBERT and GraphCodeBERT across three tasks.
ThenumbersintheparenthesescorrespondtotheabsoluteimprovementwithrespecttotheattacksuccessratesofMHM-NS.
TaskCodeBERT GraphCodeBERT
MHM-NS Greedy-Attack ALERT MHM-NS Greedy-Attack ALERT
Vulnerability Detection 35.66% 49.42% (+13.76%) 53.62% (+17.96%) 55.17% 71.98% (+16.81%) 76.95% (+21.78%)
Clone Detection 20.05% 23.20% (+3.15%) 27.79% (+7.74%) 3.42% 6.75% (+3.33%) 7.96% (+4.54%)
Authorship Attribution 19.27% 30.28% (+11.01%) 35.78% (+16.51%) 32.11% 46.79% (+14.68%) 61.47% (+29.36%)
Average 24.99% 34.30% (+9.31%) 39.06% (+14.07%) 30.23% 42.17% (+11.94%) 48.79% (+18.56%)
(a) VCR on attacking CodeBERT (b) NoQ on attacking CodeBERT (c) VCR on attacking GraphCodeBERT (d) NoQ on attacking GraphCodeBERT
Figure 3: Comparison results of Variable Change Rate (VCR) and Number Of Queries (NoQ) on attacking CodeBERT andGraphCodeBERT.They-axiscorrespondstothenormalizedvaluesofVCRandNoQ.Thex-axisrepresentsdownstreamtasks.
snippets that satisfy the aforementioned constraints. We compute
a statistically representative sample size using a popular sample
size calculator5with a confidence level of 99% and a confidence
interval of 10. We sample 100 code snippets to conduct the user
study, which is statistically representative.
Foreachselectedcodesnippet,wecanconstruct3pairs.Each
pair contains the original code snippet and an adversarial example
generatedbyeither ALERT,MHM,orMHM-NS.Wehighlightthe
changed variables in each pair and present them to users. Users
are asked to evaluate to what extent the substitutions are naturally
fittingintothesourcecodecontexts.Giventhestatement:"Thenew
variablenamelooksnaturalandpreservestheoriginalmeaning",
participantsneedtogivescoresona5-pointLikertscale[ 25],where
1meansstronglydisagreeand5meansstronglyagree,following
thesamesettingsusedbyJinetal.[ 24].Participantsdonotknow
which attack method produces which adversarial example in a pair.
The user study involves four non-author participants who have
aBachelor/MasterdegreeinComputerSciencewithatleastfour
years of experience in programming. Each participant evaluates
the100pairsindividually.Wecalculatetheaverageratingsgivento
adversarial examples generated by each attack method per partici-
pant,andpresenttheresultsinFigure2.Thex-axisdistinguishes
each participant, and the y-axis shows the average ratings. The
results show that the usage of ALERT-generated substitutions can
helpgeneratemuchmorenaturaladversarialexamples.Thefour
participants give average scores of close to 4 to adversarial ex-
amplesgeneratedby ALERTandslightlyloweraveragescoresto
adversarial examples generated by MHM-NS; these indicate that
participants perceive that the substitutions generated by these two
methods are natural. Participants consistently give lower scores
5https://www.surveysystem.com/sscalc.htm. Accessed: 2021-08-19(1.86 on average) to examples generated by MHM, showing that
they think the variable substitutions are unnatural.
AnswerstoRQ1 :Participantsconsistentlyfindthatadver-
sarial examples generated by ALERT (a naturalness-aware
method) are natural while those generated by MHM (a
naturalness-agnostic method) are unnatural.
RQ2. How successful and minimal are the
generated adversarial examples? How scalable is
the generation process?
To answer this question, we evaluate the effectiveness of ALERT
andMHM-NSonattackingCodeBERTandGraphCodeBERTcon-
sideringthreedimensions. Specifically,weusethree metrics,each
capturingonequalitydimension,tomeasuretheperformanceof
an adversarial example generation method. Each metric is defined
basedonadataset ğ‘‹,whereeachelement ğ‘¥âˆˆğ‘‹isacodesnippet
thathasatleastonelocalvariableandavictimmodel ğ‘€thatcan
predictallexamplesin ğ‘‹correctly.Thethreemetricsaredefined
as follows.
â€¢AttackSuccessRate (ASR):TheASRofanadversarialexample
generation method is defined as|{ğ‘¥|ğ‘¥âˆˆğ‘‹âˆ§ğ‘€(ğ‘¥/prime)â‰ ğ‘€(ğ‘¥)}|
|ğ‘‹|, where
ğ‘¥/primeis a generated example. A higher ASR indicates that an attack
method has better performance.
â€¢Variable Change Rate (VCR): Assuming that an input code
snippetğ‘¥ğ‘–hasğ‘šğ‘–local variables, and an attacker renames ğ‘›ğ‘–
variablesin ğ‘¥ğ‘–,wedefinevariablechangerate(VCR)oftheattack
overğ‘‹as/summationtext.1
ğ‘–ğ‘›ğ‘–/summationtext.1
ğ‘–ğ‘šğ‘–.AlowerVCRispreferablesinceitmeansthat
fewer edits are made to find successful adversarial examples.
1489
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Natural Attack for Pre-trained Models of Code ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
â€¢Number of Queries (NoQ): In adversarial attacks, especially
theblack-boxones,thenumberofqueriestothevictimmodel
needstobekeptaslowaspossible.Inpractice,victimsmodels
are usually remotely deployed, and it is expensive (and maybe
alsosuspicious)toquerymodels toomanytimes.Wecount the
number of queries (NoQ) to the victim models when each attack
generatesadversarialexamplesondataset ğ‘‹.Attacksthathave
lower NoQ are more scalable as well.
Table 2 displays the comparison results between MHM-NS and
ALERTonthesixvictimmodels(2models Ã—3tasksasdescribedin
Section4.1).WealsoreporttheresultsofsolelyusingGreedy-Attack
to emphasize the improvements brought by GA-Attack. Results
show that Greedy-Attack has 49.42%, 23.20% and 30.28% attack
successrateonCodeBERTacrossthreedownstreamtasks,which
corresponds to an improvement of 13.76%, 15.71% and 11.01% over
MHM-NS,respectively.ByemployingGA-Attackin ALERT,wecan
boosttheperformanceevenfurther:MHM-NSresultsareimproved
by 17.96%, 7.74% and 16.51% in terms of ASR. On GraphCodeBERT,
Greedy-Attack can outperform MHM-NS by 16.81%, 3.33% and
14.68%forthethreetasks;thenumbersareboostedto21.78%,4.54%
and 29.36% when GA-Attack is employed.
Moreover, ALERTmakesfewereditstotheoriginalexamplesand
ismorescalablethanthebaseline.Figure3comparesresultsintermsof VCR and NoQ. The x-axis corresponds to each downstream task,
and the y-axis represents normalized values of the two evaluation
metrics.Onallvictimmodels, ALERTmodifiesfewervariablesto
generateadversarialexamples.Itindicatesthat ALERTcanmake
minimal changes to input code snippets and produce more natural
and imperceptible adversarial examples. Besides, ALERTqueries
victim models less than MHM-NS does. The NoQ of solely using
Greedy-Attackis82.57%lessthanMHM-NS.WhenGA-Attackis
employed, the NoQ increases but is still 49.62% less than MHM-NS,
which shows that ALERTis more practical since victim models
are usually remotely deployed and may be costly to query and
may prevent frequent queries. Querying victim models is the most
time-consumingpartofexperiments,sofewerNoQalsoshowsthat
ALERThas lower runtime.
Answers to RQ2 : In terms of the attack success rate,
ALERTcan outperform the MHM by 17.96%, 7.74% and
16.51% on CodeBERT, as well as 21.78%, 4.54% and 29.36%
on GraphCodeBERT across three downstream tasks. Inaddition to achieving a superior attack success rate, our
method also makes fewer changes and is more scalable.
RQ3. Can we use adversarial examples to harden
the victim models?
In this research question, we explore the effectiveness of using ad-
versarial fine-tuning [ 22] as a defense against attacks. We leverage
ALERTto generate adversarial examples for each victim model
on their corresponding training sets. If a victim model predicts
wronglyonanoriginalinputornolocalvariablenamecanbeex-
tractedfromit,weskipthisexample.Forotherinputsinthetraining
sets, we select at most one adversarial example for each of them. If
ALERTattackssuccessfully,wechoosethefirstly-foundadversarialexample.If ALERTfailstoattack,weselecttheexamplethatcan
minimize thevictim modelâ€™s confidenceon the ground truth label.
These generated adversarial examples are then augmented into the
original training set and form the adversarial training set. We then
fine-tune the victim model on the adversarial training set.
After adversarial fine-tuning, we obtain two models: CodeBERT-
AdvandGraphCodeBERT-Advandevaluatethemontheadversarial
examples generated in RQ2. Table 3 shows the new modelsâ€™ pre-
dictionaccuracyonpreviouslygeneratedadversarialexamples.It
is noted that the original victim models (that are not hardened
byadversarialretraining) predictalltheseexampleswrongly (i.e.,
they have an accuracy of 0%). From Table 3, we can observe that
alltheadversariallyfine-tunedmodelsperformmuchbetterthan
the original ones. The average improvement on examples gener-
ated by solely using Greedy-Attack and employing GA-Attack isclose. CodeBERT-Adv improves accuracy against Greedy-Attack
andALERTby87.76%and87.59%,respectively.GraphCodeBERT-
AdvimprovesaccuracyagainstGreedy-Attackand ALERTby92.16%
and 93.31%. Accuracy improvement on adversarial examples gener-
atedbyMHM-NSisrelativelymoreminor(67.89%and75.93%on
CodeBERT and GraphCodeBERT).
Answers to RQ3 : The adversarial examples generated
byALERTare valuable in improving the robustness of
victim models. Adversarially fine-tuning victim models
withALERT-generated adversarial examples can improve
theaccuracyofCodeBERTandGraphCodeBERTby87.59%
and 92.32%, respectively.
6 THREATS TO VALIDITY
Internal validity: The results obtained in our experiment can
varyunderdifferenthyper-parameterssettings,e.g.,inputlength,
numbers of training epochs, etc. To mitigate the threats, we setthe input length to CodeBERT and GraphCodeBERT as 512 (the
maximalvalue)toensurethattheyseethesamenumbersoftokens
for the same code snippet. For the remaining hyper-parameters,
we keep them the same as described in [ 14,20]. We compare the
performanceofmodelsobtainedinthispaperwithresultsreported
intheliterature[ 14,20,30]toshowthatourmodelsareproperly
trained.
Externalvalidity: Inourexperiments,weinvestigatetwopopular
pre-trainedmodelsofcodeonthreedownstreamtasks.However,
our results may not generalize to other pre-trained models and
downstreamtasks.Weuseagenericparsertoextractvariablenames
fromcodesnippetswritteninC,PythonorJava,butitcannotwork
in other programming languages like Ruby.
7 RELATED WORK
This section describes the works that are related to this paper,
including the pre-trained models of code and adversarial attacks
on models of code.
1490
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhou Yang, Jieke Shi, Junda He and David Lo.
Table3:Robustnessanalysisonadversariallyfine-tunedvictimmodels.Thenumbersarethepredictionaccuraiesofadversar-
ially fine-tuned models (CodeBERT-Adv and GraphCodeBERT-Adv) on the adversarial examples generated in RQ2.
TasksCodeBERT-Adv GraphCodeBERT-Adv
MHM-NS Greedy ALERT MHM-NS Greedy ALERT
Vulnerability Detection 80.46% 87.93% 88.11% 80.81% 88.84% 89.04%
Clone Detection 59.33% 91.38% 87.31% 48.28% 91.23% 91.70%Authorship Attribution 63.89% 83.97% 87.36% 98.72% 96.40% 96.21%
Overall 67.89% 87.76% 87.59% 75.93% 92.16% 92.32%
7.1 Pre-trained Models of Code
Code representation models like code2vec [ 3] and code2seq [ 2]
that use syntactic and structural information have shown good
performance on a range of downstream tasks. However, some pre-
trained models for Natural Languages (NL) like BERT [ 12] and
GPT-3[8]haverecentlydemonstratedexcellenttransferabilityto
ProgrammingLanguages(PL)andstrongercapabilitiesofcapturing
semantics information than code2vec or code2seq. Inspired by the
success of these language models, pre-trained models of code have
recentlybecomemoreandmorepopularinthefieldofcodeintel-
ligenceandbenefitedabroadrangeoftasks[ 9,14,20,26,43,47].
Thesecurrentpre-trained modelsofcodecanbedividedinto two
types: embedding models and generative models.
The two models (CodeBERT [ 14] and GraphCodeBERT [ 20]) in-
vestigated in our experiments are representatives of embedding
models.WehavedescribedCodeBERTandGraphCodeBERTinSec-tion2.1.Here,webrieflydescribeotherembeddingmodels.Kanadeetal.[
26]usedthesamemodelarchitectureandtrainingobjectives
as BERT but trained it on Python source code to produce CuBERT.
Buratti et al. [ 9] introduced C-BERT, a transformer-based language
modeltrainedon100popularClanguagerepositoriesonGithub.
Both CuBERT and C-BERT were trained on a single programming
language, which limits their usage scenarios. CuBERT and C-BERT
outperformgenericbaselineslikeLSTMmodelsbutdonotshow
superiorperformancethanCodeBERTandGraphCodeBERT,sowe
investigate the latter two in this work.
The other branch of pre-trained models is generative models,
which are designed for generative tasks like code completion. Svy-
atkovskiy et al. [ 43] introduce GPT-C, a variant of GPT-2 [ 37]
trained on a large corpus containing multiple programming lan-
guagesandachievedimpressiveperformanceincodegeneration
tasks. Lu et al. [ 30] provides CodeGPT, which has the same model
architectureandtrainingobjectiveofGPT-2.CodeGPTwastrained
on Python and Java corpora from the CodeSearchNet dataset [ 23].
Despite their success on generation tasks, these models are unable
to getcomplete contextual information asthey are unidirectional
decoder-only models which only rely on previous tokens and ig-norethefollowingones[
47],sowediscardgenerativemodelsin
the investigation list of this work.
7.2 Adversarial Attack on Models of Code
Yefet et al. [ 51] proposed DAMP, a white-box attack technique that
adversarially changes variables in code using gradient informationof the victim model. Although their method shows effectiveness in
attacking three models code2vec[3],GGNN[1], andGNN-FiLM [7],it requires victim models to process code snippets using one-hotencoding, which is not applicable to CodeBERT [
14] and Graph-
CodeBERT [ 20] investigated in our paper as they use BPE [ 16,29]
to process tokens. Srikant et al. [ 41] apply PGD [ 31] to generate
adversarial examples of code. Besides, these white-box approaches
generatesubstitutesbychangingaone-hotencodingtoanotherandmappingitbacktoatoken,whichcannotguaranteetosatisfynatu-ralnessrequirements.Suchawhite-boxattackislesspracticalsince
victim models are usually deployed remotely, making parameter
information hard to be accessed.
Thereareseveralblack-boxmethodsforevaluatingtherobust-
ness of models of code. One that has been shown to be much more
effectivethantheothersisMHM[ 52],whichweuseasourbase-
line. We have presented the details of MHM in Section 2.3. Here
we present the other related studies. Wang et al. [ 45] provide a
benchmark consisting of refactored programs and evaluate the per-formanceofneuralembeddingprogramsonit.Rabinetal.[
36]also
uses variable renaming to evaluate the generalizability of neural
programanalyzers,andshowthatGGNN[ 15]changesitspredic-
tion on 33.39% of transformed code. Pour et al. [ 35]p r o p o s e da
testing framework for DNN of source code embedding, which can
decrease the performance of code2vec [ 3] on method name pre-
dictiontaskby2.05%.Applisetal.[ 5]usemetamorphicprogram
transformations to assess the robustness of ML-based program
analysis tools in a black-box manner.
Adversarial attack on models of code can be conducted beyond
generating adversarial examples for a well-trained model. Schuster
et al. [38] show that code completion models are vulnerable to
poisoning attacks that add some carefully-designed files to thetraining data of a model. Nguyen et al. [
34] show that the state-
of-the-artAPIrecommendersystemscanbeattackedbyinjecting
malicious data into their training corpus.
8 CONCLUSION AND FUTURE WORK
In this paper, we highlight the naturalness requirement in generat-
ing adversarial examples for models of code. We propose ALERT
(Naturalness AwareAttack), a black-box attack that adversari-
ally transforms inputs (code snippets) to force pre-trained mod-els to produce wrong outputs. ALERTcan generate naturalness-
aware substitutes. A user study confirms that these substitutes can
help generate adversarial examples that look natural to humanjudges. In contrast, users consistently think examples generated
by a prior method that employs randomselection to be unnatural.
Apart from being aware of naturalness, ALERTis also effective in
finding adversarial examples. We apply ALERTto victim models
fine-tuned on state-of-the-art pre-trained models (CodeBERT and
1491
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. Natural Attack for Pre-trained Models of Code ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
GraphCodeBERT).Theresults showthaton attacking CodeBERT,
ALERTcan achieve average success rates of 53.62%, 27.79%, and
35.78% across three downstream tasks: vulnerability prediction,
clone detection and code authorship attribution. It outperforms
the baseline by 17.96%, 7.74% and 16.51%. On GraphCodeBERT, our
approach can achieve average success rates of 76.95%, 7.96% and61.47% on the three tasks, respectively, outperforming the base-line by 21.78%, 4.54%, and 29.36%. We also explore the value of
adversarialexamplestohardenCodeBERTandGraphCodeBERT
throughanadversarialfine-tuningprocedureanddemonstratedthe
robustnessofCodeBERTandGraphCodeBERTagainst ALERTin-
creasedby87.59%and92.32%,respectively.Weopen-source ALERT
at https://github.com/soarsmu/attack-pretrain-models-of-code.
In the future, we plan to consider more victim models and more
downstreamtasks.Wealsoplantoboosttheeffectivenessof ALERT
and improve the robustness of victim models further.
ACKNOWLEDGMENTS
ThisresearchwassupportedbytheSingaporeMinistryofEducation
(MOE) Academic Research Fund (AcRF) Tier 1 grant.
REFERENCES
[1]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learning
to Represent Programs with Graphs. In 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings.
[2]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In 7th International Con-
ferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,
2019.
[3]UriAlon,MeitalZilberstein,OmerLevy,andEranYahav.2019. code2vec:learning
distributedrepresentationsofcode. Proc.ACMProgram.Lang. 3,POPL(2019),
40:1â€“40:29. https://doi.org/10.1145/3290353
[4]Bander Alsulami, Edwin Dauber, Richard E. Harang, Spiros Mancoridis, and
Rachel Greenstadt. 2017. Source Code Authorship Attribution Using Long Short-
Term Memory Based Networks. In Computer Security - ESORICS 2017 - 22nd
European Symposium on Research in Computer Security, Oslo, Norway, September
11-15, 2017, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 10492),
Simon N. Foley, Dieter Gollmann, and Einar Snekkenes (Eds.). Springer, 65â€“82.
[5]LeonhardApplis,AnnibalePanichella,andArievanDeursen.2021. Assessing
RobustnessofML-BasedProgramAnalysisToolsusingMetamorphicProgram
Transformations. In 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE). 1377â€“1381.
[6]Muhammad Hilmi Asyrofi, Zhou Yang, Imam Nur Bani Yusuf, Hong Jin Kang,
FerdianThung,andDavidLo.2021. BiasFinder:MetamorphicTestGeneration
to UncoverBias forSentiment AnalysisSystems. IEEE Transactionson Software
Engineering (2021).
[7]Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr
Polozov. 2019. Generative Code Modeling with Graphs. In 7th International
ConferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May
6-9, 2019.
[8]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
(2020).
[9]LucaBuratti,SaurabhPujar,MihaelaA.Bornea,J.ScottMcCarley,YunhuiZheng,GaetanoRossiello,AlessandroMorari,JimLaredo,VeronikaThost,YufanZhuang,andGiacomoDomeniconi.2020. ExploringSoftwareNaturalnessthroughNeural
Language Models. CoRRabs/2006.12641 (2020). arXiv:2006.12641
[10]Nicholas Carlini and David A. Wagner. 2018. Audio Adversarial Examples:
TargetedAttacksonSpeech-to-Text.In 2018IEEESecurityandPrivacyWorkshops,
SPWorkshops2018,SanFrancisco,CA,USA,May24,2018.IEEEComputerSociety,
1â€“7.
[11]Casey Casalnuovo, Earl T. Barr, Santanu Kumar Dash, Prem Devanbu, and Emily
Morgan. 2020. A Theory of Dual Channel Constraints. In Proceedings of theACM/IEEE 42nd International Conference on Software Engineering: New Ideas and
EmergingResults (Seoul,SouthKorea) (ICSE-NIERâ€™20).AssociationforComputing
Machinery, New York, NY, USA, 25â€“28.
[12]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers).AssociationforComputationalLinguistics,Minneapolis,Minnesota,
4171â€“4186.
[13]JavidEbrahimi,DanielLowd,andDejingDou.2018. OnAdversarialExamples
for Character-Level Neural Machine Translation. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics. Association for Computational
Linguistics, Santa Fe, New Mexico, USA, 653â€“663.
[14]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
APre-TrainedModelforProgrammingandNaturalLanguages.In Findingsofthe
AssociationforComputationalLinguistics:EMNLP2020.AssociationforComputa-
tional Linguistics, Online, 1536â€“1547.
[15]PatrickFernandes,MiltiadisAllamanis,andMarcBrockschmidt.2019. Structured
NeuralSummarization.In 7thInternationalConferenceonLearningRepresenta-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
[16]PhilipGage.1994. Anewalgorithmfordatacompression. TheCUsersJournal
archive12 (1994), 23â€“38.
[17]Xiang Gao, Ripon K. Saha, Mukul R. Prasad, and Abhik Roychoudhury. 2020.
FuzzTestingBasedData AugmentationtoImproveRobustnessofDeepNeural
Networks. In Proceedings of the ACM/IEEE 42nd International Conference on Soft-
ware Engineering (Seoul, South Korea) (ICSE â€™20) . Association for Computing
Machinery, New York, NY, USA, 1147â€“1158.
[18]Adam Gleave, Michael Dennis, Cody Wild, et al .2020. Adversarial Policies:
Attacking Deep Reinforcement Learning. In ICLR.
[19]Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
HarnessingAdversarialExamples.In 3rdInternationalConferenceonLearning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
[20]DayaGuo,ShuoRen,ShuaiLu,ZhangyinFeng,DuyuTang,ShujieLiu,LongZhou,
NanDuan,AlexeySvyatkovskiy,ShengyuFuandzMicheleTufano,ShaoKun
Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
andMingZhou.2021. GraphCodeBERT:Pre-trainingCodeRepresentationswithDataFlow.In 9thInternationalConferenceonLearningRepresentations,ICLR2021,
Virtual Event, Austria, May 3-7, 2021.
[21]Wenbo Guo, Xian Wu, Sui Huang, and Xinyu Xing. 2021. Adversarial Policy
LearninginTwo-playerCompetitiveGames.In Proceedingsofthe38thInterna-
tional Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event
(Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong
Zhang (Eds.). PMLR, 3910â€“3919.
[22]HosseinHosseini,BaicenXiao,MayooreJaiswal,andRadhaPoovendran.2017.
On the limitation of convolutional neural networks in recognizing negativeimages. In 2017 16th IEEE International Conference on Machine Learning and
Applications (ICMLA). IEEE, 352â€“358.
[23]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. arXiv:1909.09436
[24]DiJin,ZhijingJin,JoeyTianyiZhou,andPeterSzolovits.2020. IsBERTReally
Robust?AStrongBaselineforNaturalLanguageAttackonTextClassification
andEntailment.In TheThirty-FourthAAAIConferenceonArtificialIntelligence,
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference,IAAI2020,TheTenthAAAISymposiumonEducationalAdvancesin
ArtificialIntelligence,EAAI2020,NewYork,NY,USA,February7-12,2020.AAAI
Press, 8018â€“8025.
[25]Ankur Joshi, Saket Kale, Satish Chandel, and D Kumar Pal. 2015. Likert scale:Explored and explained. British Journal of Applied Science & Technology 7, 4
(2015), 396.
[26]Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020.
Learning and Evaluating Contextual Embedding of Source Code. In Proceedings
ofthe37thInternationalConferenceonMachineLearning,ICML2020,13-18July
2020,VirtualEvent (ProceedingsofMachineLearningResearch,Vol.119).PMLR,
5110â€“5121.
[27]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
AndreaJanes.2020. BigCode!=BigVocabulary:Open-VocabularyModelsfor
Source Code.In Proceedingsof theACM/IEEE42nd InternationalConference on
Software Engineering (Seoul, South Korea) (ICSE â€™20). Association for Computing
Machinery, New York, NY, USA, 1073â€“1085.
[28]Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020.
BERT-ATTACK:AdversarialAttackAgainstBERTUsingBERT.In Proceedingsof
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP) .
Association for Computational Linguistics, Online, 6193â€“6202.
[29]YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. RoBERTa:A
1492
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Zhou Yang, Jieke Shi, Junda He and David Lo.
RobustlyOptimizedBERTPretrainingApproach. CoRRabs/1907.11692(2019).
arXiv:1907.11692
[30]ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,Ambrosio
Blanco,ColinB.Clement,DawnDrain,DaxinJiang,DuyuTang,GeLi,Lidong
Zhou,LinjunShou,LongZhou,MicheleTufano,MingGong,MingZhou,Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
and Generation. CoRRabs/2102.04664 (2021). arXiv:2102.04664
[31]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks.In 6thInternationalConferenceonLearningRepresentations,ICLR2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
[32]Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze. 2008. Intro-
duction to Information Retrieval. Cambridge University Press, USA.
[33]NicholasMetropolis,AriannaW.Rosenbluth,MarshallN.Rosenbluth,AugustaH.
Teller,andEdwardTeller.1953. EquationofStateCalculationsbyFastComputing
Machines. The Journal of Chemical Physics 21, 6 (1953), 1087â€“1092.
[34]Phuong T. Nguyen, Claudio Di Sipio, Juri Di Rocco, Massimiliano Di Penta, and
Davide Di Ruscio. 2021. Adversarial Attacks to API Recommender Systems:
Time to Wake Up and Smell the Coffee?. In 2021 36th IEEE/ACM International
Conference on Automated Software Engineering (ASE). 253â€“265.
[35]MaryamVahdatPour,ZhuoLi,LeiMa,andHadiHemmati.2021. ASearch-Based
Testing Framework for Deep Neural Networks of Source Code Embedding. In
14thIEEEConferenceonSoftwareTesting,VerificationandValidation,ICST2021,
Porto de Galinhas, Brazil, April 12-16, 2021. IEEE, 36â€“46.
[36]MdRafiqulIslamRabin,NghiDQBui,KeWang,YijunYu,LingxiaoJiang,and
MohammadAminAlipour.2021. OnthegeneralizabilityofNeuralProgramMod-
elswithrespecttosemantic-preservingprogramtransformations. Information
and Software Technology 135 (2021), 106552.
[37]AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,
et al.2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.
[38]Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You
Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion. In
30th USENIX Security Symposium (USENIX Security 21). USENIX Association,
1559â€“1575.
[39]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. arXiv:1508.07909 [cs.CL]
[40]JiekeShi,ZhouYang,JundaHe,BowenXu,andDavidLo.2022. CanIdentifier
SplittingImproveOpen-VocabularyLanguageModelofCode?.In 2022IEEEInter-
nationalConferenceonSoftwareAnalysis,EvolutionandReengineering(SANER).
IEEE Computer Society.
[41]Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu Fan,
Gaoyuan Zhang, and Una-May Oâ€™Reilly. 2021. Generating Adversarial Com-
puterProgramsusingOptimizedObfuscations.In 9thInternationalConference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[42]Jeffrey Svajlenko, Judith F. Islam, Iman Keivanloo, Chanchal Kumar Roy, andMohammad Mamun Mia. 2014. Towards a Big Data Curated Benchmark ofInter-project Code Clones. In 30th IEEE International Conference on Software
MaintenanceandEvolution,Victoria,BC,Canada,September29-October3,2014.IEEE Computer Society, 476â€“480.[43]
AlexeySvyatkovskiy,ShaoKunDeng,ShengyuFu,andNeelSundaresan.2020.
IntelliCode Compose: Code Generation Using Transformer. In Proceedings of the
28thACM JointMeetingonEuropean SoftwareEngineeringConferenceand Sym-
posiumontheFoundationsofSoftwareEngineering.AssociationforComputing
Machinery, New York, NY, USA, 1433â€“1443.
[44]Yida Tao, DongGyun Han, and Sunghun Kim. 2014. Writing Acceptable Patches:
AnEmpiricalStudyofOpenSourceProjectPatches.In 30thIEEEInternational
Conference on Software Maintenance and Evolution, Victoria, BC, Canada, Septem-
ber 29 - October 3, 2014. IEEE Computer Society, 271â€“280.
[45]Ke Wang and Mihai Christodorescu. 2019. COSET: A Benchmark for Evaluating
Neural Program Embeddings. CoRRabs/1905.11445 (2019). arXiv:1905.11445
[46]Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting Code
Clones with Graph Neural Networkand Flow-Augmented Abstract Syntax Tree.
arXiv:2002.08653 [cs.SE]
[47]Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. 2021. CodeT5:
Identifier-awareUnifiedPre-trainedEncoder-DecoderModelsforCodeUnder-
standing and Generation. arXiv:2109.00859 [cs.CL]
[48]HuihuiWeiandMingLi.2017. SupervisedDeepFeaturesforSoftwareFunctional
Clone Detection by Exploiting Lexical and Syntactical Information in Source
Code.InProceedingsoftheTwenty-SixthInternationalJointConferenceonArtificial
Intelligence,IJCAI2017,Melbourne,Australia,August19-25,2017,CarlesSierra
(Ed.). 3034â€“3040.
[49]ChengranYang,BowenXu,JunaedYounusKhan,GiasUddin,DonggyunHan,
ZhouYang,andDavidLo.2022. Aspect-BasedAPIReviewClassification:HowFarCanPre-TrainedTransformerModelGo?.In 2022IEEEInternationalConferenceon
SoftwareAnalysis,EvolutionandReengineering(SANER).IEEEComputerSociety.
[50]Zhou Yang, Jieke Shi, Muhammad Hilmi Asyrofi, and David Lo. 2022. Revisit-ing Neuron Coverage Metrics and Quality of Deep Neural Networks. In 2022
IEEE International Conference on Software Analysis, Evolution and Reengineering
(SANER). IEEE Computer Society.
[51]Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of
code.Proc. ACM Program. Lang. 4, OOPSLA (2020), 162:1â€“162:30.
[52]HuangzhaoZhang,ZhuoLi,GeLi,LeiMa,YangLiu,andZhiJin.2020. Generating
AdversarialExamplesforHoldingRobustnessofSourceCodeProcessingModels.
(2020), 1169â€“1176.
[53]Ting Zhang, Bowen Xu, Ferdian Thung, Stefanus Agus Haryono, David Lo, and
Lingxiao Jiang. 2020. Sentiment Analysis for Software Engineering: How FarCan Pre-trained Transformer Models Go?. In IEEE International Conference on
Software Maintenance and Evolution, ICSME 2020, Adelaide, Australia, September
28 - October 2, 2020. IEEE, 70â€“80.
[54]Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing Generalizabilityof CodeBERT. In IEEE International Conference on Software Maintenance and
Evolution,ICSME2021,Luxembourg,September27-October1,2021.IEEE,425â€“
436.
[55]Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Effective Vulnerability Identification by Learning Comprehensive Program
SemanticsviaGraphNeuralNetworks.In AdvancesinNeuralInformationProcess-
ing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS2019,December8-14,2019,Vancouver,BC,Canada ,HannaM.Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 10197â€“10207.
1493
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:30:32 UTC from IEEE Xplore.  Restrictions apply. 