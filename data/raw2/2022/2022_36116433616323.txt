NaturalLanguage to Code:How FarAreWe?
Shangwen Wang∗
wangshangwen13@nudt.edu.cn
Collegeof Computer, National
Universityof DefenseTechnology
Changsha,ChinaMingyangGeng
gengmingyang13@nudt.edu.cn
Collegeof Computer, National
Universityof DefenseTechnology
Changsha,ChinaBoLin∗
linbo19@nudt.edu.cn
Collegeof Computer, National
Universityof DefenseTechnology
Changsha,China
Zhensu Sun
zhensuuu@gmail.com
School of Computing and Information
Systems, SingaporeManagement
University
SingaporeMing Wen†
mwenaa@hust.edu.cn
Schoolof CyberScienceand
Engineering,HuazhongUniversityof
Scienceand Technology
Wuhan, ChinaYepangLiu†
liuyp1@sustech.edu.cn
Department of Computer Science and
Engineering,SouthernUniversityof
Scienceand Technology
Shenzhen,China
Li Li
lilicoding@ieee.org
Schoolof Software, Beihang
University
Beijing, ChinaTegawendé F.Bissyandé
tegawende.bissyande@uni.lu
Universityof Luxembourg
LuxembourgXiaoguangMao∗
xgmao@nudt.edu.cn
Collegeof Computer, National
Universityof DefenseTechnology
Changsha,China
ABSTRACT
Alongstandingdreaminsoftwareengineeringresearchistodevise
eﬀectiveapproachesforautomatingdevelopmenttasksbasedon
developers’ informally-speci/f_ied intentions. Such intentions are
generally in the form of natural language descriptions. In recent
literature, a number of approaches have been proposed to auto-
mate tasks such as code search and even code generation based on
naturallanguageinputs.Whiletheseapproachesvaryintermsof
technical designs, their objective is the same: transforming a devel-
oper’sintentionintosourcecode.Theliterature,however,lacksa
comprehensive understandingtowardsthe eﬀectivenessofexist-
ing techniques as well as their complementarity to each other. We
proposeto/f_illthisgapthroughalarge-scaleempiricalstudywhere
we systematically evaluate natural language to code techniques.
Speci/f_ically,weconsidersixstate-of-the-arttechniquestargeting
codesearch,andfourtargetingcodegeneration.Throughextensive
evaluationsonadatasetof22K+naturallanguagequeries,ourstudy
reveals the following major /f_indings: (1) code search techniques
based on model pre-training are so far the most eﬀective while
code generation techniques can also provide promising results;
(2)complementaritywidelyexistsamongtheexistingtechniques;
and (3) combining the ten techniques together can enhance the
∗ShangwenWang,BoLin,andXiaoguangMaoarealsowiththeKeyLaboratoryof
SoftwareEngineering for ComplexSystems
†Ming Wen and Yepang Liu arethe corresponding authors.
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forpro/f_itorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe/f_irstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspeci/f_icpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’23, December 3–9, 2023, San Francisco, CA,USA
©2023 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 979-8-4007-0327-0/23/12...$15.00
https://doi.org/10.1145/3611643.3616323performance for 35% compared with the most eﬀective standalone
technique.Finally,weproposeapost-processingstrategytoauto-
maticallyintegratediﬀerenttechniquesbasedontheirgenerated
code.Experimentalresultsshowthatour devisedstrategy isboth
eﬀective andextensible.
CCS CONCEPTS
•Software and its engineering →Reusability ;Automatic
programming .
KEYWORDS
Code Search, Code Generation,Pre-Training Technique
ACMReference Format:
Shangwen Wang, Mingyang Geng, Bo Lin, Zhensu Sun, Ming Wen, Yepang
Liu, Li Li, Tegawendé F. Bissyandé, and Xiaoguang Mao. 2023. Natural Lan-
guage to Code: How Far Are We? . In Proceedings of the 31st ACM Joint
European Software Engineering Conference and Symposium on the Foun-
dations of Software Engineering (ESEC/FSE ’23), December 3–9, 2023, San
Francisco, CA, USA. ACM, New York, NY, USA, 13pages.https://doi.org/10
.1145/3611643.3616323
1 INTRODUCTION
Recommender systems are widely studied in software engineer-
ing research as they are concrete building blocks for improving
developers’productivity[ 3].Ahighly-soughtachievementinthis
domain is to eﬀectively transform developers’ intentions, which
aregenerallyspeci/f_iedinnaturallanguage,intopiecesofcode[ 39].
Addressing such a challenge will alleviate some software develop-
ment burdens, and facilitate critical designs and implementation
choices such as selecting the appropriate programming interfaces
touse[25].Indeed,developersinalllevelsofprogrammingpro/f_i-
ciencyfrequentlyaskquestionsofvaryingcomplexityabouthowto
implement speci/f_ic functionalities [ 63]. For example, it is typical to
seedevelopershavingtheintentionto“removeaspeci/f_icitemfrom
375
ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Wang,Geng,Lin, Sun,Wen, Liu, Li, Bissyandé, Mao.
anarray”lookforrelatedcodeinQ&Aforums.1Recentadvances
in deep learning have enabled the development of promising tech-
niquesintwolinesofresearchtowardstransformingdevelopers’
informally-speci/f_iedintentions(a.k.a queriesthataregenerallyin
theformofnaturallanguagedescriptions)intosourcecode:(1) code
searchaims at retrieving a relevant piece of code within a large
codebase [ 8,51,65], while (2) code generation aims to synthesize
codefromscratch[ 32,66,67].Inpractice,toobtainthedesiredcode,
adeveloper mayleveragecodegeneration techniquesto generate
codedirectly, orretrieverelevantcodesnippets froma large-scale
codebasesuchasStackOver/f_low.Asaresult,theapplicationscenar-
ios of these two types of techniques could overlap with each other
to a certain degree [ 59,63]. We refer to all such relevant literature
techniques as Natural Language to Code (NL2Code) techniques.
NL2Code techniques diﬀer in terms of various design aspects.
First,atahighlevel,thetheoreticalworkingmechanismsofcode
search and code generation are diﬀerent: code search focuses on
mappingthesemanticrelevancebetweenthequeryandanexist-
ing code snippet and directly returns the code with the highest
relevance score; code generation, in contrast, constructs a piece
of code fromscratch. Second, theycanbe diﬀerentiatedaccording
to whether a pre-trained model is used. Those that build on pre-
trainingtechniques,suchasGraphCodeBERT[ 18]andCodeT5[ 60],
adopt a pre-training and /f_ine-tuning pipeline where deep learn-
ingmodelsare/f_irstpre-trainedonalarge-scalecorpusaimingat
capturing the semantic relation between natural language and pro-
gramminglanguage,andthen/f_ine-tunedonspeci/f_icdownstream
tasks. In contrast, those that do not rely on pre-trained models (re-
ferredtoas non-pre-trainingtechniques ),suchasMMAN[ 57]and
Tranx[67],traintheirmodelsfromscratchonrelativelysmall-scale
labelleddatasets.Thesetrainingmethodscansigni/f_icantlyaﬀect
theeﬀectivenessof NL2Codetechniques. For instance,Zeng et al.
observedthatpre-trainingtechniquesoutperformnon-pre-training
techniques on a number of code intelligence tasks such as code
clone detection [ 70]. Furthermore, according to the intrinsic diﬀer-
ence between code search and code generation, the former may
produce high-quality results if there exists a code snippet that is
similar to the intended functionality; on the contrary, the latter
maygenerate more reasonable results if there isno code snippet
for reference whose semantic issimilar to the intention.
Although enormous eﬀorts have been made towards advancing
the NL2Code techniques [ 3,12,31,48], the eﬀectiveness of exist-
ing techniques has not been systematically studied and compared.
Particularly,intheliterature,codegenerationandcodesearchtech-
niques are often evaluated separately [ 17,57,67,70], which means
that code generation techniques are compared to each other with-
out considering code search techniques, and vice versa. As a result,
little is known about their complementarity with each other, i.e.,
can the query ineﬀectively handled by one technique be addressed
wellbyanother?Thereisthusanurgentneedforacomprehensive
empiricalstudycomparingandanalyzingtheeﬀectivenessofthe
state-of-the-art NL2Code techniques basedon a large number of
NLdescriptions.Suchastudyisnecessaryandessential,whichcan
help us /f_ind the answers to important questions when designing
NL2Code techniques in the future. For instance, which is the most
1A related question: https://stackoverflow.com/questions/5767325eﬀective NL2Code technique so far and what is the common weak-
ness of existing NL2Code techniques? Moreover, to what extent do
existing NL2Code techniques complement each other and whether
the integration of them can enhance the performance? Answering
such questions can provide practical guidance for studies within
this /f_ield. Driven by this, Xu et al.[63] took the /f_irst step in this
direction,buttheiruserstudyislimitedinitsscaleduetotheexten-
sivehumanintervention.Speci/f_ically,only14functionalitiesand
twoNL2Code techniques were investigated.
Inthis paper,we aimat/f_illthe gapby performing the/f_irstlarge-
scale empirical study that evaluates the eﬀectiveness of both code
generation and code search techniques collectively under a con-
trolledexperimentsetting.Speci/f_ically,ourstudycoverstenstate-of-
the-art NL2Code techniques, including six code search techniques
andfourcodegenerationtechniques,onacomprehensivebench-
markcontaining22K+naturallanguagequeries.Ourexperiment
settingisuser-oriented.First,weaimtoevaluatehowsimilarthe
code returned by an NL2Code technique is to the oracle and we
usetheCodeBLEUmetric[ 46]tocomparethesimilaritybetween
thereturnedresultandthe oraclecodewithrespecttothetokens,
syntacticstructures,anddata/f_lows.Second,tomimicreal-world
scenarios, we remove the oracle code (i.e., the code snippet that
correspondstoeachquery)fromthesearchspaceofcodesearch
techniques. The rationale behind this is that in practice, devel-
opers can hardly /f_ind exactly what they want from the codebase
[6,7,15,63].Third,weevaluatetheeﬀectivenessofcurrenttech-
niquestogeneratemethod-levelcodesnippets,whichisthemost
desirable granularity for developers compared with other granular-
ities(e.g.,variables andstatements) [ 37,50].
Our study makesthe following important/f_indings:
F1:Theeﬀectivenessofcodegenerationtechniquesispromis-
ing and exceeds that of the non-pre-training code search
techniques. However, the state-of-the-art pre-training based
codesearchtechnique isstillthemosteﬀective oneamong
the NL2Code techniques.
F2:Accuratelygeneratingprogramidenti/f_iersisauniversalchal-
lengeforboth codesearch andcodegeneration techniques
sincetheygenerallyachieverelativelylowtokensimilarities
to the oracle.
F3:ExistingNL2Codetechniquescomplementeachotherwell:if
we combine all the ten selected techniques, we can enhance
theperformanceofover35%withrespecttoTop-1,compared
to the mosteﬀective standalone technique.
F4:Combining code search with code generation or diﬀerent
code searchtechniques demonstratespromisingresults.
Moreover, we design a post-processing strategy that re-ranks
the results from diﬀerent techniques based on the number of over-
lapped tokens with the query. Our results show that such a combi-
nationstrategyiseﬀective:bycombiningthemosteﬀectivecode
searchandcodegenerationtechniques,wecangainaneﬀective-
ness improvement of 16% and 35% with respect to the top-1 results,
comparedwitheachstandalonetechnique.Furthermore,itisalso
extensible: further eﬀectiveness enhancement could be achieved
byinvolving more techniques for combination.
376Natural Languageto Code: HowFarAre We? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
2 BACKGROUNDAND RELATED WORKS
2.1 CodeSearch
Code search (CS) aims at helping developers retrieve some im-
plementationsthatcanserveasreferencesfortheirdevelopment
activities [ 9,38,47,62]. Given a natural language (NL) query from
the developer, CS searches for the relevant code snippets from a
large-scale code corpus. Traditionally, this process is mainly done
by theinformation retrieval technique such askeyword matching
[34,37]. However, these techniques are known to be suboptimal
atcapturingthesemanticrelationsbetweencodeandnaturallan-
guage queries [ 17]. Later on, researchers have proposed various
deep-learning-basedapproachestobridgethesemanticgaps.For
instance,Gu etal.[17]proposeDeepCSwhichjointlyembedscode
snippetsandnaturallanguagedescriptionsintoahigh-dimensional
vector space, where codesnippetsandqueries canbe matchedac-
cordingtotheirsimilarities.Wan etal.[57]designamulti-modal
attentionnetworkthataggregatesinformationfromthetokense-
quence,abstractsyntaxtree(AST),andcontrol/f_lowgraph(CFG)
for representing programs.
2.2 CodeGeneration
Code generation aims at directly generating source code according
to software requirements [ 32]. Traditional approaches leverage for-
malmethodstoautomaticallygeneratesourcecode[ 19,61],butthe
formalspeci/f_ications are hardto create [ 32].With the advancesin
deeplearning,researchersproposetoautomaticallylearnthetrans-
formationsfromtherequirementstosourcecode.Speci/f_ically,Ling
etal.[29]treatcodegenerationassequence-to-sequencetranslation
andbuildaneural networkthattargetsgeneral-purposeprogram-
ming languages like Python. Dong et al.[13] explore the idea of
usingtwodecodersinthecodegenerationtask,wherethe/f_irstone
aims at predicting a rough program sketchand the second one /f_ills
inthe details.
2.3 Pre-Training Techniques
Training a deep learning model from scratch usually needs a large
amount of task-speci/f_ic annotated data, which is hard to collect
in practice. To overcome this limitation, pre-training techniques
have been proposed in recent years. The core idea is to pre-train a
modelon oneormore self-supervised taskswhere largeamounts
oftrainingdataarereadilyavailablesothatthenetworkweights
can encode some commonsense knowledge compared with ran-
domlyinitialized.Afterthat,withasmallamountoftask-speci/f_ic
data, the pre-trained models can be /f_ine-tuned in the traditional
supervised manner. Recently, researchers have build several pre-
trained models for programming language (PL) byusing the large
amountofbimodalinstancesofNL-PLpairs(i.e.,thesourcecode
anditscorrespondingcomments)[ 18,36,60].Arecentstudy[ 70]
investigatedtheexistingpre-trainedmodelsforPLonstandalone
downstreamtasks(i.e.,codesearchandcodegenerationaresepa-
rately evaluated). In contrast, our study includes both pre-training
andnon-pre-trainingtechniquesandinvestigatestheirstrengths
andweaknessesinacontrolled NaturalLanguagetoCode experi-
ment setting (i.e., using the identical queries and the oracle code is
removedfrom the searchspacefor code searchtechniques).Table1: Selectedtechniques inthisstudy.
Code Search Code Generation
w/opre-trainingSelf-attention [23],
Tree-LSTM [55],
GGNN[69],
Multi-modal [57]Tranx[67]
w/ pre-trainingCodeBERT [14],
GraphCodeBERT [18]CodeT5[60],
NatGen[10],
SPT-Code [42]
3 STUDYDESIGN
3.1 Selected Techniques
Over the years, a large number of code search and code genera-
tion techniques have been proposed [ 26,31]. Therefore, it requires
tremendousengineeringeﬀortstoevaluateallofthem.Inthisstudy,
weselectrepresentativetechniquesandweleavetheexplorationof
moretechniques asourfuture work.Totally,we usetenNL2Code
techniques, including six code search techniques and four code
generation techniques. Those techniques can be classi/f_ied into two
typesaccording to whether they use pre-trainingand Table 1lists
thecategorization.Theselectedtechniqueshaveservedasthebase-
linesforanumberofstudies[ 49,53,54,73]andachievedpromising
results in recent replication studies [ 9,32,52,70], and thus they
can represent the state of the art well. For instance, through a com-
prehensive comparison among existing pre-training techniques
(e.g.,PLBART[ 2]andCodeGPT[ 33]),Zengetal.[70]foundthat
GraphCodeBERTandCodeT5achievethebestperformanceoncode
search and code generation, respectively. The following brie/f_ly in-
troduces eachofthe selectedtechniques.
3.1.1 Code Search. Typically, a code search technique should em-
bedboththecodesnippetandthequeryintovectors,afterwhich
therelevancebetweenthecodeandthequerycanbecalculated.For
theselectedfournon-pre-trainingtechniques,theapproachusedto
embedqueriesisidentical:weuseanencoderwithsixTransformer
blocks [56] to deal with the natural language token sequence plus
with byte-pair encoding (BPE) [ 16] to split tokens. In the following
fourparagraphs, we introduce howto embedthe code snippets.
Self-a/t_tention. This is a baseline proposed by Husain et al.[23].
It treats code as token sequences and uses an encoder of the Trans-
former architecture to embed such sequences. This approach mim-
ics the work/f_low of the well-known DeepCS [ 17] (i.e., both ap-
proachestreatprogramsascodetokens)butisexpectedtoestablish
amoreadvancedeﬀectivenessbaseline,astheTransformerarchi-
tecture is known to perform well on the long-term dependency
problemfacedbytheRecurrentNeuralNetworks(RNN)[ 56],which
isusedbyDeepCS.
Tree-LSTM. Tree-LSTMisanapproachthatgeneralizestheLong
Short-TermMemory (LSTM) network to tree-structuredtopologies.
Initially, it targeted at capturing the syntactic properties of natural
languages[ 55]anditwas/f_irstlyappliedtotheASTsofprograms
byWanet al.[57].
GGNN.Zenget al.[69] proposeto constructthe variable-based
/f_lowgraphthatdepictsdataandcontrol/f_lowsintheprogram.Such
graphs are constructed by transforming the programs into their
Intermediate Representations (IRs) [ 1], extracting the identi/f_iers
in each IR instruction as nodes, and building dependencies among
nodes.Afterthat,aGatedGraphNeuralNetwork(GGNN)isused
377ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Wang,Geng,Lin, Sun,Wen, Liu, Li, Bissyandé, Mao.
togeneratetheembeddingforthegraph,whichisalsotherepre-
sentationofthe code.
Multi-modal. Withtheintuitionthataggregatinginformation
frommultiplemodalitiesofsourcecodecanenrichitsrepresenta-
tion,Wan etal.[57]proposeMMANthatutilizesthetokensequence,
the AST, as well as the graph information of a program. In this pa-
per,werebuildthemulti-modallearningmodelviafusingthethree
aforementioned approaches (Self-attention + Tree-LSTM + GGNN).
Itshouldbenotedthattherebuiltmulti-modallearningmodelis
supposed to perform better than the vanilla MMAN, since MMAN
onlyinvolvescontrol/f_low informationwhile the GGNN approach
involves both data andcontrol/f_lowinformation.
CodeBERT. CodeBERT [ 14] is a Transformer-based pre-trained
model for programming languages like Python and Java. It has
two tasks in the pre-training stage which are masked language
modelingandreplacedtokendetection.Toapplythepre-trained
modelonthecodesearchtask,therepresentationofaspecialtoken
[CLS](the beginning token of the input) is used to measure the
semantic relevancebetween the code snippetandquery.
GraphCodeBERT. Guoet al.[18] take data /f_low information
into consideration in the pre-training stage. In addition to masked
language modeling, there are two newly-proposed structure-aware
tasks inthe pre-training,i.e., edge predictionand nodealignment.
Then, the work/f_low of applying the pre-trained model to code
searchisidenticalto that ofCodeBERT.
3.1.2 Code Generation. Tranx.Tranx [67] predicts a sequence of
actions to construct an AST, based on which the source code is
generated.It/f_irstde/f_inesanabstractsyntaxdescriptionlanguage
framework,whichisagrammaticalformalismofASTs.Basedon
that,threetypesofactionsarepredictedateachtimesteptoexpand
the tree until the whole tree is constructed. Note that a number of
follow-upstudiesrelyonthegrammarrulesintroducedbyTranx
to construct ASTs [ 24,53,54]. Therefore, we select Tranx as the
representative technique.
CodeT5.CodeT5[ 60]followstheT5architecture[ 45]withthe
input being the sequence of code and text tokens and the output
also in a sequential format. One specially-designed pre-training
taskisNL-PLdualgenerationinwhichthemodellearnstogenerate
codefromtextsandgeneratetextsforcodesimultaneously.After
pre-training,CodeT5canbenaturallyadaptedtocodegeneration
due to its encoder-decoder framework. A number of follow-up
studies rely on the pre-trained parameter values of CodeT5 [ 27],
so we select CodeT5 as the representative pre-training-based code
generation approach. The authors of CodeT5 provide two versions
ofthismodel,whichhavediﬀerentparametersizes.Weuse CodeT5-
baseinthis study since itismore eﬀective [ 60].
NatGen. NatGen [ 10] is designed based on CodeT5 and incor-
porates an extra pre-training task, “Code Naturalizing”, which is
designedtoteachthemodelhowtotransformunnaturalcodeintoa
morenatural,human-writtenform.Thisadditionaltaskisintended
to encourage the model to better understand the underlying se-
mantics of the code, and thus enhance the model’s capability on
generatingcode that closelyresembles human-written ones.
SPT-Code. SPT-Code [ 42] is another state-of-the-art pre-trained
model with the encoder-decoder framework. The input to the
SPT-Code model during the pre-training stage diﬀers from thatofCodeT5intwoways.First,itsinputincludestheAbstractSyntax
Tree (AST) of the code, which enablesit to leverage syntactic infor-
mation. Second, to eliminate the need for a bilingual corpus (i.e.,
acodesnippetpairedwithacorrespondingcomment),SPT-Code
leverages the method name and the names of the methods that are
called within that method as a natural language description of the
sourcecode being analyzed.
Exclusion. Abranchofstudyfocusesonutilizingtheretrieval
results to guide code generation [ 20,21]. We exclude them from
this study since (1) the retrieval-and-edit approach [ 20] assumes
thatthe input and outputof the method isalready knownand the
methodispartiallywritten,whichisunfairtoourstudysubjects
(i.e.,wedonotrequirepriorknowledge);and(2)ReCode[ 21]isbuilt
ontopofasetofsuboptimalgrammarrules,whichisnotasgeneral
as Tranx. We also exclude a recently-proposed code generation
approach, PyCodeGPT [ 68], since it only supports generating code
thatreusesthethird-partylibraries PandasandNumPy,whichis
not generalenough.
3.2 Dataset
We select the widely-used CodeSearchNet dataset [ 23] as our eval-
uationbenchmark,whichisminedfrompopularGitHubprojects
(interms of the number of stars and forks). In our study, we focus
onthe bimodaldata(i.e.,the codesnippetanditsassociateddocu-
mentation) by treating thedocumentation as thenatural language
queryandthecodesnippetasthegroundtruth.Codeinthisdataset
are all method-level snippets, and our study thus focuses on the
eﬀectiveness of existing techniques at the method level. To keep in
high-quality,thisdatasethasalreadybeenpre-processedbyseveral
steps.Forinstance,anydocumentationshorterthanthreetokens
is removed since it might not be informative, and any code snippet
shorterthanthreelinesisalsoremovedsinceitislikelytobeget-
ters and setters. We explicitly focus on the Python language in this
study since Python is the most widely targeted general-purpose
programming language in the code generation domain [ 32,66,67],
andourselectedTranxonlysupportsthePythonlanguagesofar.
The dataset has already been split into the training/validation/test
sets by the authors of CodeSearchNet, and the training set has
beenusedforpre-training,whichmeansthecross-validationonthe
wholedatasetisinappropriate(doingsowillfavorpre-trainingtech-
niquesduetothedataleakage).Therefore,weevaluateNL2Code
techniques on the /f_ixed test set, following existing studies [ 18,60].
In the end, our dataset contains 412,178/23,107/22,176 code-query
pairsfor training/validation/testing,respectively.
3.3 Research Questions
RQ1: How eﬀective are existing techniques on transform-
ingnaturallanguagedescriptionsintosourcecode? We/f_irst
systematically investigate the eﬀectiveness of each individual tech-
niqueassummarizedinTable 1ongeneratingmethod-levelcode
snippetsbasedonnaturallanguages.Beyondthetraditionalsetting
where the oracle code snippets are within the search space of code
search techniques, we design a new experiment setting in this RQ
where we assume the oracle does not exist in the search space and
performthesearchontheleft22,175codesnippetsinthetestset.
378Natural Languageto Code: HowFarAre We? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Previous studies have shown that developers usually need to mod-
ify the retrieved code snippets to adapt them to the local contexts
[6,7,15,63],whichmeansthat,inarealisticscenario,thedesired
code snippets can rarely be directly retrieved. Speci/f_ically, Gabel
andSu[15]investigatedthesyntacticuniquenessofsourcecodeand
foundthatredundancyusuallyexistsonlyatthelinelevel,whileat
themethod level,whichisourinvestigated granularity, near-total
uniqueness was observed. More recently, in the user study per-
formed byXu et al.[63],users modi/f_ied 18 tokens of the retrieved
code chunks (several lines of code) on average, and such a number
is expected to increase when it comes to the method level. Con-
sequently, our oracle-excluded setting mimics the real application
scenariowheretheuserssearchfortheresultsfromalarge-scale
corpusthatdoesnotcontaintheexactlydesiredcodesnippetand
see how useful the retrieved results could be. Evaluations under
such a user-oriented setting can help us better understand the use-
fulnessofcodesearchtechniquesinreal-worldscenarios.Thisis
thusthebasicsettingofthisstudyandourfollow-upinvestigations
are basedonthe results obtainedfrom this setting.
RQ2: Dodiﬀerent techniquescomplementeachother? In
thisRQ,weaimtoinvestigateifdiﬀerenttechniquestendtoper-
form similarly on thesame queries or if they exhibit performance
diﬀerencesoncertainqueries.Aswehaveintroduced,existingtech-
niquescanbecharacterizedindiﬀerentaspects,suchasusingeither
searchorgenerationstrategy,withorwithoutpre-training.This
questioninvestigateswhethersuchdiﬀerencesinthedesignspaces
lead to certain complementarities with respect to their eﬀective-
ness.Theanswerisessentialtoourfurtherinvestigations:wecould
be able to design a combinational strategy to integrate diﬀerent
techniques only if they demonstratecertaincomplementarities.
RQ3.Can we gobeyond thestate oftheart by combining
existingtechniques? Basedontheexperimentaloutputsobtained
from the previous RQs, we further seek to investigate whether
combining diﬀerent techniques can achieve better performance.
We propose to investigate twosub-questions:
•RQ3.1What is the best performance achievable by combining dif-
ferenttechniques? We/f_irstaimtoinvestigatethebestperformance
achievableviacombiningdiﬀerenttechniques,whoseresultswill
pave the wayfor the following question:
•RQ3.2Canweautomaticallycombinediﬀerenttechniques? Reach-
ingthebestperformancerequiresmanuallyinspectinganumber
ofresults,whichwouldbetime-consuming.Wefurtherseekto
design a novel strategy that is able to combine diﬀerent tech-
niques automatically.
3.4 Eﬀectiveness Assessment
Tojointlyevaluatecodegenerationandcodesearch,wefocuson
assessingthesimilaritybetweenthepredictedresultandtheoracle
code.Speci/f_ically,wedecidetousefourmetricsfollowingtheexist-
ingstudy[ 10],including Tokenmatch(TM) whichiscalculated
by the standard BLEU [ 43] and aims to re/f_lect the similarity be-
tweenthetokensequencesofthepredictedandoraclecode; Syntax
match (SM) which aims to evaluate code quality from the natural
tree structure of programminglanguage (i.e., the AST); Data/f_low
match (DM) which aims to evaluate the semantic information
of code through the dependency relations among variables; andTable2: Eﬀectiveness of EachSelectedTechnique (in%).
TechniquesTop-1 Top-5
TM SM DM CB TM SM DM CB
Tranx 2.5 19.2 25.5 12.7 2.7 21.3 28.2 14.0
CodeT5 9.2 27.3 39.5 22.2 10.2 29.2 42.3 23.9
NatGen 9.5 24.2 36.2 21.0 9.6 25.0 38.1 21.7
SPT-Code 3.4 16.9 37.2 16.3 3.8 18.8 40.8 17.9
Self-attention 32.1 47.6 58.7 42.8 55.8 69.2 79.8 65.4
Tree-LSTM 21.4 39.3 52.1 33.8 43.1 60.4 73.9 55.5
GGNN 21.6 39.5 52.2 34.0 43.8 60.9 74.3 56.0
Multi-modal 34.0 49.0 60.0 44.5 56.7 69.8 80.2 66.1
CodeBERT 60.2 70.4 74.9 66.8 82.0 87.8 90.7 85.9
GraphCodeBERT 61.6 71.5 76.0 68.1 83.1 88.6 91.4 86.8
Self-attentionw/ooracle 1.423.840.016.91.731.654.923.0
Tree-LSTM w/ooracle 1.423.940.117.01.731.754.923.0
GGNN w/ooracle 1.423.940.017.01.731.654.923.0
Multi-modal w/ooracle 1.423.840.217.01.731.654.823.0
CodeBERTw/o oracle 9.933.544.325.415.944.056.834.7
GraphCodeBERTw/o oracle 10.233.944.725.816.444.557.135.1
Theboldnamemeansthetechniquerequirespre-training. Thegreencell
denotes theoracle is excluded fromthesearchspace ofcodesearchtechniques.
Theoptimum performances ofgeneration/searchtechniquesare in bold.
CodeBLEU(CB) whichisacombinationoftheabovethreemetrics
andprovidesaholisticperspectivetothequalityofgeneratedcode.
Readers can refer to [ 46]for more details aboutthesemetrics.
In this study, we calculate the similarity for the top-1 results
of each technique as well as the maximum values from the top-
5 results. The rationale is that as suggested by the prior study,
developers only inspect a few results returned by recommendation
tools[44].
3.5 Experiment Setting
All our experiments were performed on a server which possesses 8
NVIDIATeslaV100with32GBmemory.Notethattoalleviatepoten-
tialreproduciblebias[ 70],theselectednon-pre-trainingtechniques
aretrainedfromscratchandthepre-trainingonesare/f_ine-tunedby
ourselves.Sinceallofourselectedtechniquesopensourcedtheir
artifactsonGitHub,wereusedtheoriginalimplementationsaswell
asthevaluesofthehyper-parametersselectedfor/f_ine-tuning,to
avoid potential bugs in our implementation as well as enhance the
reliability of our results. Note that initiallyGGNN[ 69] targeted C
language.ToapplyittoPython,weusethe Dismodule2togenerate
theIRs,afterwhichthegraphcanbegeneratedbasedonthescripts
releasedbytheauthors.SPT-Codewasnotevaluatedonthecode
generation task in the original study [ 42]. To /f_ine-tune it on this
task,wereusethehyperparametersusedto/f_ine-tunethismodelon
thecodesummarizationtask, whichcanbe considered as another
generationtask.
4 STUDYRESULTS
4.1 RQ1: Eﬀectiveness ofExistingTechniques
For each technique, we calculate the metrics of the code produced
by them for each query and the mean values on the whole test set
are shown in Table 2. The mean value is one of the most repre-
sentativestatisticsandithasbeenwidelyused byexistingstudies
toassesstheperformancesofdiﬀerenttechniques[ 10,53,60,70].
Fromtheresults,we/f_irstnotethatcomparedwithnon-pre-training
techniques,pre-trainingtechniquesgenerallyachievebetterper-
formances.Forinstance,theCBof CodeT5withrespecttothetop-1
2https://pypi.org/project/dis/
379ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Wang,Geng,Lin, Sun,Wen, Liu, Li, Bissyandé, Mao.
resultis22.2%,whichexceedsthatof Tranx(i.e.,12.7%)by75%.Simi-
larly,theCBof CodeBERT withrespecttothetop-1result,whenthe
oracleisexcludedfromthesearchspace,is25.4%,whichexceeds
that ofMulti-modal (i.e., 17.0%) by around 50%. Furthermore, we
notefromtheresultsthatthemosteﬀectiveNL2Codetechniqueso
farisGraphCodeBERT ,withtheCBscoreof25.8%.Thisiswithin
ourexpectationconsideringthatpre-trainingtechniquesrequire
much more data (e.g., GraphCodeBERT andCodeT5are pre-trained
ondatafromsixprogramminglanguages)andthuscanpreserve
more domain knowledge [ 70]. We also note that compared with
theothertwopre-training-basedcodegenerator(i.e.,CodeT5and
NatGen),SPT-Codeachievescomparativelylowperformances.One
possibleexplanation,asintroducedinSection 3.1,isthatthepre-
training phase of SPT-Code relies on method names (which may
besyntacticallyincomplete)toapproximatethenaturallanguage
description of the code. This setting could potentially reduce its
abilitytoalignnaturallanguageandprogramminglanguageand
generatecode from naturallanguageinputs.
Finding-1 ☞Thepre-trainingtechniquesachievebetterperfor-
mancesthannon-pre-trainingtechniquesforbothcodegeneration
andcodesearch.ThemosteﬀectiveNL2Codetechniquesofaris
GraphCodeBERTwiththeCodeBLEU of25.8%.
Wealso/f_indthatwithremovingtheoraclefromthesearchspace,
the eﬀectiveness of code search techniques decreases signi/f_icantly.
Speci/f_ically,theCBofthestate-of-the-artcodesearchtechnique,
i.e.,GraphCodeBERT , with respect to the top-1 results when the
oraclecodeisinvolved/excludedis68.1%/25.8%,respectively,and
the other /f_ive search techniques undergo the similar decreases
when the oracle is excluded. By further investigating its search
results, we /f_ind that with oracle involved, it can rank the oracle
code at top-1 for 12,672 queries, which account for nearly 60%
of the total queries (12,672/22,176). That is why it can achieve a
highCBwiththeoracleinvolved.Amongthenon-pre-trainingcode
searchtechniques, Multi-modal achievesthebestperformancewhen
the oracle is involved with the CB of 44.5%. However, these four
techniquesachievenearlyidenticaleﬀectivenesswhentheoracleis
excluded, with respect to both top-1 and top-5 results. Speci/f_ically,
their CBs with respect to the top-1 and top-5 results are all around
17.0%and23.0%respectively.Suchresultsmayindicatethatthere
isa gap between the current evaluationof code search techniques
and their real usefulness in practice. Indeed, the current evaluation
alwaysassumestheexistenceoftheexactlymatchedcodeinthe
search space [17,52,57], which ampli/f_ies the usefulness of code
searchtechniques.Wethuscallforauser-orientedevaluationfor
future studies, that is, to investigate to what extent the retrieved
results could help developers when what they exactly need may
not be retrieved.
Finding-2 ☞If the oracle does notexistin thesearch space, the
eﬀectivenessofcode search techniques decreases signi/f_icantly.
Another phenomenon we observe is that comparing with focus-
ingonlyonthetop-1results,theeﬀectivenessofthecodesearch
techniques increases signi/f_icantly if the top-5 results are consid-
ered, while that of the codegenerationtechniques nearly remains
Figure 1: The CBs of the syntactically correct (incorrect) top-1 code
snippets generatedby CodeT5.
unchanged. Speci/f_ically, without the oracle, the CBs of GraphCode-
BERTwhen considering top-1/top-5 results are 26.1%/35.3% respec-
tively, an increase of 35% when all the top-5 results are considered.
In contrast, those of CodeT5are 22.2% and 23.9% respectively, with
only slightenhancement. Such resultssuggest that themost quali-
/f_ied candidate code snippets are sometimes not ranked at the top-1
positions for code search techniques while the top-1 generated
codesnippetsusuallyreachtheoptimum. Therefore,recommend-
ingmoreresultsfromthereturnedliststotheuserscouldbeuseful
for code search, but such usefulness would not be signi/f_icant for
codegeneration.Despitethat,we /f_indthe top-1resultsfrom code
generation techniques are already promising: they can be more
similar to the oracle code compared with the retrieved results of
certain code search techniques. For instance, the CB of CodeT5
with respect to the top-1 results is 22.2% while those of the four
non-pre-training code search techniques are around 17.0%.
Finding-3 ☞Unlikecodesearchtechniquesthatsometimesdo
notrankthebestcandidatesatthetop-1positions,codegeneration
techniquesusually predict the optimumresultsatthe top-1 posi-
tions, and their eﬀectiveness can exceed that of non-pre-training
code search techniques.
Wefurtherinvestigatethepromisingresultsachievedby CodeT5,
the best performing code generation technique. Since it directly
generatesthetokensequencewithoutanygrammaticalguideline
(unlikeTranx),onecriticalconcernisthatthegeneratedcodemight
be syntactically incorrect. Ourinvestigation shows thatthegener-
atedtop-1codesnippetsaresyntacticallycorrectfor19,340queries,
accounting for 87% of the queries in the test set. We also dissect
theCBsofthesesyntacticallycorrect/incorrectcodesnippetsand
demonstratetheresultsinFigure 1.We/f_indthatthemediansofthe
CBsofsyntacticallycorrect/incorrectcodeare verysimilar(22.3%
vs.21.0%)andthe diﬀerencesbetween thetwogroupsare not sta-
tistically signi/f_icant (i.e., with the p-value >0.05 in a one-sided
Mann-Whitney U-Test [ 35]). This indicates that being syntactically
correctornotdoesnotnecessarilyaﬀectthemetricvalueofthegen-
eratedcode.Italsoindicatesthatmoremetricsareneededtobetter
re/f_lect the syntax diﬀerences of diﬀerent code. We perform further
analysis towards such incorrect cases and /f_ind that CodeT5often
generatesablockofcoderecurrently.Theincorrectnesshappens
when the token sequence exceeds a pre-de/f_ined length determined
by the hyper-parameter (which means CodeT5stops generating
more tokens) while the current line is not /f_inished. Therefore, such
incorrect code still ful/f_ills certain functionalities and thus can have
high CBs. We give an example inour onlinerepository.
Finding-4 ☞Beingsyntacticallycorrect ornotdoesnotneces-
sarilyaﬀecttheCBsofcode generated byCodeT5.
380Natural Languageto Code: HowFarAre We? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Wealsocarefullycheckourexperimentresults.We/f_indthatour
results are generally consistent with those reported in previous
studies. For instance, for four non-pre-training code search tech-
niques,ourresultsshowthatiftheoracleisinvolved,combining
informationfrommultiplemodalitiescanachievethebestperfor-
mance and the token sequence information is the most rewarding
single modality (i.e., Self-attention achieves higher CB than Tree-
LSTMandGGNN). This is identical to the phenomenon reported
byWanetal.[57].We notethe TMof Tranx(2.5%)issigni/f_icantly
lower than the value reported in the previous study [ 32], which
is 18.4%. After further investigation, we /f_ind that their dataset is
fromcontest programsinwhichtheidenti/f_iersare usuallysimple
but meaningless like i,j, and k. On the contrary, our dataset is
fromreal-worldopen-sourceprojectsinwhicheachidenti/f_ierisex-
pected to express rich semantic information and thus may be more
complex(e.g.,camelcasesandunderscorenamingconventions[ 5]).
SinceTM,thestandardBLEU,focusesontheidenti/f_iermatching
relations, we consider our result as reasonable: itre/f_lects that cur-
rentlysemantic-meaningfulidenti/f_iersinreal-worldprojectsare
diﬃcult to predict.
Indeed, our results illustrate that for all the involved techniques,
their TMs are signi/f_icantly lower than their SMs and DMs, indicat-
ingthattheinabilitytoaccuratelygenerateidenti/f_iersisauniversal
weaknessoftheexistingcodegeneration/searchtechniques(please
note the identi/f_ier nameis ignored when calculating SMand DM).
Thiscanbeexplainedbythefactthatprogramidenti/f_iersusually
demonstrate uniqueness. For instance, Nguyen et al.[41] found
thatmorethan60%ofthemethodnamesoccuronlyonceamong
14K+ projects. Suppose an identi/f_ier in the oracle code is unique,
theretrieved resultswill notmatchwith it,andsimilarly,thegen-
eration techniques seem unlikely to generate it since it may not
beinvolvedinthevocabularyfromwhichthe outputispredicted.
A concrete example is shown in Listing 1. In this case, the query
expressestheintentiontoremovequotesfromastring.We/f_indthat
thesemanticsofthecodegeneratedby CodeT5isnearlyidenticalto
thatoftheoraclecode,exceptthatitfailstocheckifthestringstarts
andendswithdoublequotes(inPython,astringcanbewrapped
with either single or double quotes). Therefore, the SM and DM of
CodeT5are extremely high, both exceeding70%. However, CodeT5
fails to accurately predict the names of the identi/f_iers. For instance,
ituses storepresenttheinputstringparameterwhileintheoracle,
this identi/f_ier is named as istr. Since this identi/f_ier occurs for
many times in the code, the TM of CodeT5is thus only 13.1%, a
relatively low value. This case also reveals that relying solely on
theBLEUvaluetoevaluatethegeneratedcodeispotentiallybiased,
demonstratingtherationaleofamorecomprehensivemetriclike
CodeBLEU.
Finding-5 ☞Producingaccurateprogramidenti/f_iersisauniver-
sal challengeforboth generation and search techniques.
4.2 RQ2: ComplementarityofExisting
Techniques
ToinvestigatethecomplementarityofexistingNL2Codetechniques,
for each technique pair, we compute the Pearson correlation ( /u1D45F) [4]
with respectto their CBs achievedon each query (we focus on CB1# Code generated by CodeT5
2def_remove_quotes (s):
3ifs[0] =="'"ands[-1] =="'":
4 returns[1:-1]
5else:
6 returns
7
8# Oracle code
9defunquote_ends (istr):
10ifnotistr:
11 returnistr
12if(istr[0]=="'"andistr[-1]=="'")or\
13 (istr[0]=='"'andistr[-1]=='"'):
14 returnistr[1:-1]
15else:
16 returnistr
17
18# Code retrieved by GraphCodeBERT
19defstrip_email_quotes (text):
20lines = text.splitlines()
21matches = set()
22forlineinlines:
23 prefix = re.match( r'^(\\s*>[ >]*) ', line)
24 ifprefix:
25 matches.add(prefix.group( 1))
Listing 1: The code generated by CodeT5, the oracle code, and the
coderetrievedby GraphCodeBERT forthequery“Removeasingle
pair of quotes fromtheendpointsof a string”.
heresinceitrepresentstheoveralleﬀectiveness).Pearsoncorrela-
tionisawidelyusedmetrictoassessthecorrelationdegreebetween
two sets ofdata[ 11,22]. Theoretically,a high Pearsoncorrelation
coeﬃcient suggests that the two sets of data follow a similar trend.
In our context,it means two techniquesmay have similar CBs for
a speci/f_ic query. In contrast, if two techniques have a relatively
lowPearsonvalue,itsuggeststhatthereislittleornocorrelation
betweentheirCBs.Thisindicatesthepotentialexistenceofqueries
on which the two techniques achieve rather diﬀerent CBs. In such
cases, they could be considered as complementary to each other.
Forinstance,iftwotechniquesexhibitidenticalperformanceson
eachquery,theirPearsonvaluewouldreachthemaximumvalueof
1.However,theymaynotcomplementeachotherwellbecausethey
sharesimilareﬀectivenesstowardsthesameinputs.Ourinterpreta-
tion of/u1D45Fis based on the previous study [ 22]: negligible correlation
(|/u1D45F|<0.3),lowcorrelation( 0.3≤ |/u1D45F|<0.5),moderatecorrelation
(0.5≤ |/u1D45F|<0.7), high correlation ( 0.7≤ |/u1D45F|<0.9), and very high
correlation ( 0.9≤ |/u1D45F|<1).
Results are shown in Figure 2. We observe that according to the
Pearsoncorrelationvalues,theselectedtechniquescangenerally
be classi/f_ied into three clusters as highlighted: the code generation
techniques,thenon-pre-trainingcodesearchtechniques,andthe
pre-trainingcodesearchtechniques.Fortechniquesineachcluster,
they have a relatively high correlation with each other, and a rela-
tivelylowcorrelationwiththosefromotherclusters.Speci/f_ically,
in Figure 2a,TranxandCodeT5have moderate Pearson correlation
between them (i.e., 0.54). Similarly, CodeBERT andGraphCodeBERT
havehighPearsoncorrelation(i.e.,0.76).Asforthefournon-pre-
training code search techniques, they all have moderate Pearson
correlation between each other (e.g., the value between Tree-LSTM
andGGNNis 0.65). In contrast, the Pearson correlation between
cross-cluster techniques is usually low or negligible (e.g., the value
betweenMulti-modal andGraphCodeBERT is 0.22), demonstrating
the eﬀectiveness of such techniques is weakly correlated. We also
381ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Wang,Geng,Lin, Sun,Wen, Liu, Li, Bissyandé, Mao.
(a)Pearsoncorrelationfor CBsof the top-1 results
 (b)Pearsoncorrelationfor CBsof the top-5 results
Figure2: Pearson correlation results for theselectedtechniques.
notethatthe highest Pearson value (i.e., 0.76 between GraphCode-
BERTandCodeBERT ) is still lower than 0.9 (the threshold of the
very high correlation degree). This indicates that the eﬀectiveness
of two highly-correlated techniques may still diﬀer to a certain
degree on speci/f_ic queries. We also observe the similar trend in
Figure2b.Suchresultsindicatethatcomplementaritieswidelyexist
among existing techniques.
Figure3illustrates the relationship among three representative
techniques of each cluster (the ones with the highest eﬀectiveness
ineachcluster,i.e., GraphCodeBERT ,CodeT5,andMulti-modal )viaa
Scatterplot.ThexandyvaluesofeachscatterdenotetheCodeBLEU
values of two diﬀerent techniques achieved on a speci/f_ic query. We
investigate both top-1 and top-5 results and /f_ind similar trends, so
weonlyshowthetop-1resultshere.Foreaseofcomparison,wealso
drawtheline y=xinthe/f_igure.Scattersabovethislinerepresent
that the technique denoted by the vertical axis outperforms the
technique denoted by the horizontal axis on those queries and vice
versa.Weobservethatnotechniquecanconsistentlyoutperform
the other competitor: even the most eﬀective one, GraphCodeBERT ,
can still perform worse on speci/f_ic queries, compared with CodeT5
orMulti-modal . This further shows the complementarity of the
existing techniques.
Finding-6 ☞ExistingNL2Codetechniquesarecomplementary
since(1)theycangenerallybeclassi/f_iedintothreeclusterswith
high intra-cluster Pearson correlations and low inter-cluster Pear-
soncorrelations;and(2)notechniquecanconsistentlyoutperform
theothers onall thequeries.
Caseanalysis. Todemonstratethecomplementarityofexisting
techniques,weanalyzetwocaseshere.The/f_irstisshowninListing 1
where we also list the retrieved result from GraphCodeBERT for
the same query. Due to space limitation, we only show the /f_irst
severallines.Werecallthat CodeT5achievesahighCBonthisquery
(i.e., higher than 50%). We note that both the syntactic structure
andthetokensofthecodereturnedby GraphCodeBERT arevery
dissimilartotheoracle.Forinstance,theoraclecodeusesanif-else
structure while the code returned by GraphCodeBERT contains a
loop structure. Therefore, the CB of GraphCodeBERT on this query
isonly 15%,whichismuchlower thanthat of CodeT5.1# Code generated by CodeT5
2defmigrate(self, target, **kwargs):
3if'commit_mode 'not inkwargs:
4 kwargs['commit_mode '] =self.commit_mode
5if'commit_mode 'not inkwargs:
6 kwargs['commit_mode '] =self.commit_mode
7returnself._migrate(target, **kwargs)
8
9# Oracle code
10defmigrate(self, target, follow= True, **kwargs):
11if'id'not inselfor notself['id']:
12 raiseException ('No source dataset ID found. ')
13ifisinstance (target, Dataset):
14 target_id = target.id
15else:
16 target_id = target
17migration = DatasetMigration.create(source_id= self['id'],
18 target_id=target_id, **kwargs)
19returnmigration
20
21# Code retrieved by GraphCodeBERT
22defmigrate(self, target, follow= True, **kwargs):
23ifisinstance (target, Dataset):
24 target_id = target.id
25else:
26 target_id = target
27limit = kwargs.pop( 'limit',None)
28params = self._build_query(limit=limit)
29migration = DatasetMigration.create(source_id= self._dataset_id,
30 target_id=target_id, source_params=params, **kwargs)
31returnmigration
Listing 2: The code generated by CodeT5, the oracle code, and the
coderetrievedby GraphCodeBERT forthequery“Migratethedata
fromthisdataset to a targetdataset”.
AnotherexampleisshowninListing 2.Theintendedfunctional-
ity is to migrate data from one dataset to another.Theoracle code
ful/f_ills thisby checking if theID ofthesource dataset is provided,
obtainingtheIDofthetargetdataset,and/f_inallyperformingthe
migration.Thecoderetrievedby GraphCodeBERT isonlyslightly
diﬀerentfromtheoraclecodesinceitinitializesavariablewhich
is not used by the oracle code during migration (i.e., params). The
code generated by CodeT5diﬀers signi/f_icantly to the oracle code
since (1) it does not perform the sanity check, (2) it generates a
block of code recurrently as we have mentioned before, and (3)
it does not rely on the DatasetMigration package to perform the
migration. Consequently, the CB of GraphCodeBERT on this query
ismuchhigher thanthat of CodeT5(57.1%vs. 29.2%).
382Natural Languageto Code: HowFarAre We? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
(a)GraphCodeBERT vs. CodeT5
 (b)GraphCodeBERT vs. Multi-modal
 (c) CodeT5 vs. Multi-modal
Figure 3: Scatter plots of CodeBLEUs of three representative techniques with respect to the top-1 results. We also draw the line y=xfor
comparison.
Table 3: The highest CodeBLEU values achievable by combining
diﬀerentstrategies (in%).
Combinations CB(Top-1) CB(Top-5)
CodeT5 22.2 23.9
Multi-modal 17.0 23.0
GraphCodeBERT 25.8 35.1
CodeT5+Tranx 22.4 24.1
Multi-modal+Self-attention 19.6 24.8
GraphCodeBERT +CodeBERT 28.4 37.0
GraphCodeBERT +CodeT5 30.2 37.4
GraphCodeBERT +Multi-modal 27.5 36.0
CodeT5+Multi-modal 24.1 27.6
GraphCodeBERT +CodeT5+Multi-modal 31.0 38.0
GraphCodeBERT +CodeT5+CodeBERT 32.0 38.9
All (10techniques) 35.1 40.8
These two cases demonstrate that diﬀerent techniques perform
well ondiﬀerentqueriesandthus complementeachother.
4.3 RQ3: Combination ofExistingTechniques
4.3.1 RQ3.1:WhatistheBestPerformanceAchievablebyCombining
Diﬀerent Techniques? To investigate this RQ, for each query, we
supposealltheresultsfromasetoftechniquescanbeinspectedby
thedeveloperandthemostquali/f_iedcode(theonewiththehighest
CB score) can be identi/f_ied and used as the /f_inal result of such
a technique combination. We calculate the overall performance
on the whole test set obtained in such a manner and results are
showninTable 3.We/f_irstobservethatseveraltechniquestogether
can work better than standalone techniques, which shows that
combinations of diﬀerent techniques are promising. Speci/f_ically,
ifwetakeallthetentechniquesintoconsideration,theCBofthe
Top-1resultscanreach35.1%,outperformingthebestsearchand
generation techniques (i.e., GraphCodeBERT andCodeT5) by 36%
and58%,respectively.
Finding-7 ☞Combiningthetentechniquescangainatleast35%
eﬀectiveness enhancement compared with standalone techniques.
Obtaining the results of all the eight techniques, however, re-
quiresmuchcomputationresource,whichmaynotbeaﬀordable
in practice. Therefore, we also investigate the eﬀectiveness of com-
bining a pair of techniques. Speci/f_ically, we combine techniques
fromthesameclusters(e.g., CodeT5+Tranxshowninthesecond
part) and techniques across diﬀerent clusters (e.g., GraphCodeBERT
+CodeT5shown in the third part). Surprisingly, we /f_ind that al-
though the latter is more eﬀective than the former in general (e.g.,combining CodeT5withMulti-modal works better than combining
itwithTranx),combining GraphCodeBERT withCodeBERT isthe
most eﬀective way for search-search intra-combinations: such a
strategycanachievehigherCBsthancombining GraphCodeBERT
withMulti-modal anditsCBwithrespecttothetop-5resultsnearly
equalstothatof GraphCodeBERT +CodeT5(37.0%vs37.4%).This
couldbeexplainedthroughFigure 3wherewenotethatforthesub-
/f_igurecomparing GraphCodeBERT andMulti-modal ,themajority
of the scatters are below the line y=x, which means the latter only
outperformstheformeronalimitedsetofqueries.Asaresult,com-
biningthesetwotechniquesmaynotboosttheeﬀectivenesstoa
large extent, although they have relatively low Pearson correlation.
Similarly,wealsotrytocombinethreerepresentativetechniques
fromdiﬀerentclusters(shownas GraphCodeBERT +CodeT5+Multi-
modal)but this is still outperformedby replacing Multi-modal with
CodeBERT .Consequently,ifweareabletouseonlytwotechniques
under a resource-constrained situation, search-generation inter-
combination of GraphCodeBERT withCodeT5and search-search
intra-combination of GraphCodeBERT withCodeBERT canprovide
promisingresults.
Finding-8 ☞Search-generation inter-combination of Graph-
CodeBERTwithCodeT5andsearch-searchintra-combinationof
GraphCodeBERTwithCodeBERTshow promisingresults.
4.3.2 RQ3.2: Can We Automatically Combine Diﬀerent Techniques?
To achieve an automatic combination, we design a post-processing
strategy where we re-rank results obtained from diﬀerent tech-
niquestogeneratethe/f_inaloutputs,inspiredbyarecentstudy[ 71].
Intuition. Toachieve our target,we needapredictortoassess
thequalityofeachgeneratedcodesnippet.Recallthatoneofour
observationsisthatexistingtechniquesusuallyhaverelativelypoor
performance towards TM (cf. Table 2). That is to say, if a gener-
ated code snippet has a high TM value, it is likely to achieve good
overallperformance(i.e.,CB).Inspiredbypreviousstudieswhich
point out that query tokens may represent key concepts in the
requirements [ 30,37], we postulate that a code snippet with more
overlappedtokenswiththequerymaycontainmoremeaningful
identi/f_ier names and thus has higher value towards TM (so as CB).
Forinstance,thecodetoimplementthefunctionalityrequiredby
the query “convert string to int” needs to include the API int()
anditthuscontainstheoverlappedtoken int.Givenaquery,we
denoteitsnumberoftokensas /u1D441/u1D448/u1D440/u1D461andthenumberofitstokens
contained in a generated code snippet as /u1D441/u1D448/u1D440/u1D45C. We propose to
383ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Wang,Geng,Lin, Sun,Wen, Liu, Li, Bissyandé, Mao.
Table4:TheCodeBLEUvaluesachievedbydiﬀerentcombinations
usingourstrategy(in%).
Combinations CB (Top-1) CB (Top-5)
GraphCodeBERT+ CodeBERT 28.3/28.4 36.5/37.0
GraphCodeBERT+ CodeT5 30.0/30.2 36.9/37.4
GraphCodeBERT+ CodeT5+ CodeBERT 31.8/32.0 37.9/38.9
rely on the overlap degree , which is calculated as/u1D441/u1D448/u1D440/u1D45C
/u1D441/u1D448/u1D440/u1D461, to help
assess the quality of the generated code snippet: a code snippet
with a higher overlap degree is considered to be more quali/f_ied.
Speci/f_ically,toperformsuchanalysis,thecodeandqueryaretok-
enized by the NLTK package and program identi/f_iers are further
split into multiple tokens based on the camel cases and underscore
naming conventions.
HypothesisValidation. Tovalidateourintuition,wesplitthe
overlap degree into /f_ive diﬀerent intervals and calculate the CBs
of the top-1 code snippets returned by diﬀerent techniques whose
overlap degrees fall ineachinterval.
Results are shown in Figure 4. We note that code snippets with
higheroverlapdegreesaregenerallymoresimilartotheoraclecode
(withhigherCBs),forallthreerepresentativetechniques.Speci/f_i-
cally,whentheoverlapdegreeisinthe[0.8,1]interval,themedian
valueoftheCBsofthetop-1resultsreturnedby GraphCodeBERT
isaround40%,nearlyastwiceasthatofthecodesnippetswhose
overlap degree is in the [0, 0.2) interval (which is only around 20%).
Wealsoperformtheone-sidedMann-WhitneyU-Test[ 35]toana-
lyze the statistical signi/f_icance of the CB diﬀerences for code snip-
pets from adjacent intervals. Our Null hypothesis is that H0: code
with higher overlap degrees to the query will not achieve
signi/f_icantlyhigherCBs ,andtheAlternativehypothesisis H1:
codewithhigheroverlapdegreestothequerywillachieve
signi/f_icantlyhigherCBs .Resultsrevealthatthediﬀerencesare
statisticallysigni/f_icant (i.e., p-value <0.05) under all the cases, in-
dicating that H0can be rejected with a con/f_idence level of over
0.95.Suchresultsindicatethattheoverlapdegreewiththequery
could be a competent indicator to re-assess the quality of the code
snippetsreturnedbyexisting techniques.
Strategy. Motivated by our validation, we design a combination
strategytointegratetheresultsfromdiﬀerenttechniqueswhose
overallprocessisstraightforward.Givenanaturallanguagedescrip-
tion (i.e., the query), diﬀerent techniques are executed and their
resultsarestoredintoacandidatecodesnippetpool.Afterthat,we
use a heuristic that assesses the overlap degree between the query
and each candidate code snippet tore-rank those candidates: code
snippets possessing high overlaps with the query are ranked at
thetoppositions.Consequently,resultsfromdiﬀerenttechniques
are re-ranked together and integrated into one list at this step, and
the output is the /f_inal combination result. In this study, to keep
reasonable trade-oﬀs between the eﬀectiveness and eﬃciency, we
combine the top-5 results ofeachselectedtechnique.
Evaluation Results. To investigate the eﬀectiveness of our
proposedcombinationstrategy,weselectthethreerepresentative
techniques(i.e., GraphCodeBERT ,CodeBERT ,andCodeT5)identi/f_ied
through our analysis in Section 4.3, and evaluate the performances
after combining two or all of them. Results are shown in Table 4where the data in the format “x/y” denotes the eﬀectiveness ob-
tainedbyourstrategy/thebestperformanceachievablebydiﬀerent
combinations.
We/f_ind thatour combinationstrategy isgenerally eﬀective:all
thecombinationscannearlyreachtheirmaximumpotential.For
instance, if for each query, the maximum CodeBLEU value from
GraphCodeBERT andCodeT5is achieved, then the average Code-
BLEU value of the top-1 results is 30.2%. By using our strategy,
such a combination can have a CodeBLEU of 30.0% with respect
to the top-1 results. Moreover, given the data in Table 2, such an
automatic combination can outperform each standalone technique
by16%(30.0%vs.25.8%)and35%(30.0%vs.22.2%),respectively.We
also note that search-generation inter-combination works more
eﬀectively than search-search intra-combination: the combination
ofGraphCodeBERT +CodeT5achieves higher CodeBLEUs than
the combination of GraphCodeBERT +CodeBERT with respect to
both top-1 and top-5 results, especially when we only focus on the
top-1results(30.0%vs.28.3%).Thisindicatesthatinaresourcecon-
strainedscenariowherewecanonlyexecuteafewtechniques(e.g.,
two),combiningcodesearchandcodegenerationtechniquesisrec-
ommended.Furthermore, our strategyisalso extensible: theeﬀec-
tiveness of the combination keeps increasing when involving more
techniques.Speci/f_ically,theCodeBLEUvalueofthetop-1results
increases by nearly two percentage points when all three represen-
tativetechniquesarecombined,comparedwithonlyconsidering
two of them (31.8% vs. 30.0%). As a result, further eﬀectiveness
enhancement isexpectedwhen involving more techniques.
Finding-9 ☞Asimpleheuristic-basedpost-processingstrategy
canleadtosigni/f_icant eﬀectivenessenhancementcomparedwith
each standalone technique.
5 DISCUSSION
5.1 Implications:ItTakes Two to Tango
Our investigation shows that code search and code generation
techniques share certain complementarities: a query that is not
handled eﬀectively by one technique may be addressed well by the
other. Therefore, developers may consider using both of them in
their development activities toboosttheir productivity.Ourstudy
proposesapost-processingapproachforcombiningthesetwotypes
of techniques. In fact, we also explore a pre-processing way for
combinationwherewetrainamodeltopredictwhetherasearchor
generationtechniqueshouldbeusedforagivenquery.Speci/f_ically,
weuseapre-trainedBERTmodeltoembedthequeryandtraina
fully-connected layer to predict if GraphCodeBERT orCodeT5is to
be used(asapreliminaryexploration, we focusoncombiningthe
most eﬀective search and generation techniques), but the accuracy
is only 60% on our dataset. Therefore, for researchers , eﬀortscould
be devoted to devise more eﬀective way for combination in the
future.
5.2 Comparisonwith ChatGPT
ChatGPTisahotchatbotthatcaninteractwithhumansinacon-
versational way.3To compare it with the study subjects in this
3https://chat.openai.com/chat
384Natural Languageto Code: HowFarAre We? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
(a)GraphCodeBERT
 (b)CodeBERT
 (c) CodeT5
Figure4: The performances of threerepresentativetechniquesunder diﬀerentoverlap degrees.
paper, we also investigateitscode generationperformance onour
test set. To perform this experiment, we leverage the ChatGPT API
(accessed on May 9, 2023) with the prompts to the model in the
formof“AssumethatyouareaPythonprogrammer.Pleasewrite
a Python function that ...”, followed by the query contents. The
/f_irstsentenceistoprepareChatGPTforthecodegenerationtask
whilethesecondonedescribesthedetailedrequirement.Wesetthe
temperature parametertobe0,whichensuresthatChatGPTalways
return the code with the highest probability. Results show that on
average, theCodeBLEU scoreof thecode returned byChatGPT is
21.1%, which is slightly lower than that of the state-of-the-art code
generationtechniquessuchasCodeT5.Suchresultsindicatethat
although ChatGPT can provide detailed instructions to develop-
ers, its performance may not exceed those of the state-of-the-art
NL2Code techniquesifweonly focus onthe generatedcode.One
possible explanation could be that the state-of-the-art code gen-
erationtechniqueshavebeenadequately/f_ine-tunedforthistask,
whereas ChatGPT is optimized for arti/f_icial general intelligence
(AGI)andnotspeci/f_icallyoptimizedforthetaskofcodegeneration.
5.3 The ExistenceofCodeSimilar to theOracle
In this study, to mimic the real scenario of applying code search
techniques,weremovetheoraclecodefromthesearchspacefor
eachquery.Onefollowingquestionisthatisthereanycodesnippet
in the search space similar to the oracle one? To investigate such a
question, we utilize a state-of-the-art code clone detector, NIL [ 40],
toidentifycodeclonepairsamongourtestset.WerecallthatNILis
atoken-basedclonedetectorsinceitidenti/f_iescodeclonesbasedon
theN-gramrepresentationandthelongestcommonsubsequence
of code token sequences. That is to say, the detected clones of
the oracle code can be transformed to the oracle through minor
modi/f_icationsontheircodetokens.Resultsshowthatmorethan75%
(i.e., 17,068/22,176) of the code snippets have the corresponding
clones in the search space. This indicates that for most queries,
codesnippetsthatcanmatchthequerywithminormodi/f_ications
existinthesearchspaceandaquali/f_iedcodesearchtechniqueis
supposed to rank such code snippets at top positions. By analyzing
the search logs of developers, the previous study [ 47] concludes
that developers sometimes get nothing from their searches. This
observation suggests that in a realistic setting, it is not always
possible for all the queries to have code snippets that are similar to
the oracle, as otherwise developers could always obtain a solution
by making minormodi/f_icationsto theoracle’s clones. Oursetting
is aligned with this assumption and is well-suited for practical
scenarios.5.4 Threatsto Validity
External Threats. Code search and code generation are active
research/f_ieldswithanumberofapproachesbeingproposedduring
the last years. It is thus quite hard to involve all of them in this
study. The selected approaches in this paper are state-of-the-art
and have served as baselines for many studies [ 32,52], and thus
can be consideredas representative ones safely.
Internal Threats. In our study, we use the code comment as
thequery,whichiswidelyadoptedbyexistingstudies[ 28,49,57,
60,64]. The rationale is that the comment usually summarises the
main functionality of the code, making the code-comment pair
close to actual use scenarios. Existing studies have shown that
common queries from developers are similar to the comments (i.e.,
eitherbeingidenticaltothecommentorbyslightlyprependingthe
comment with“howto”/“howdo I”) [ 17,34].
We rely on the CodeBLEU score to serve as a proxy of code
quality, following existing studies [ 10,33,70,72]. The previous
study [46] has demonstrated that CodeBLEU is strongly related
withhumanevaluations,whichmeanscodewithhigherCodeBLEU
scores is more quali/f_ied to ful/f_ill the intended functionality, as
judged by humans. Our case analysis also shows that code with
higher CodeBLEU scores is more semantically similar to the oracle
code.Asaresult,weleaveassessingtheusefulnessofthegenerated
code from the developers’ perspective as our future work.
6 CONCLUSION
In this paper, we evaluate the eﬀectiveness of ten representative
NL2Code techniques on a large-scale dataset. Through in-depth
analysis of their correlation degrees and case analysis, we show
thatexistingNL2Codetechniquescomplementeachotherwell.We
also investigate the theoretical upper-bound eﬀectiveness which
can be achieved by combining diﬀerent techniques and /f_ind that
it outperforms those of standalone techniques to a large extent.
Therefore, future studies could be undertaken to further utilize the
complementarityofNL2Codetechniques.Moreover,wedesigna
strategytoautomaticallycombineresultsfromdiﬀerenttechniques
andachievepromisingresults.Allcodeanddatainthisstudyare
publiclyavailable online[ 58].
ACKNOWLEDGMENTS
ThisworkissupportedbytheNationalNaturalScienceFoundation
of China No.61932021 and No.62002125, as well as the European
Research Council (ERC) under the European Union’s Horizon 2020
research and innovation programme (grant agreement No. 949014).
385ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA Wang,Geng,Lin, Sun,Wen, Liu, Li, Bissyandé, Mao.
REFERENCES
[1][n.d.]. Intermediaterepresentation. https://en.wikipedia.org/wiki/Intermediate
_representation .
[2]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Uni/f_ied pre-training for program understanding and generation. arXiv
preprint arXiv:2103.06333 (2021).
[3]Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles A. Sutton.
2018. A Survey of Machine Learning for Big Code and Naturalness. Comput.
Surveys51,4 (2018), 81:1–81:37. https://doi.org/10.1145/3212695
[4]JacobBenesty,JingdongChen,YitengHuang,andIsraelCohen.2009. Pearson
correlationcoeﬃcient. In Noisereduction inspeech processing . Springer, 1–4.
[5]DaveBinkley,MarciaDavis,DawnLawrie,andChristopherMorrell.2009. To
camelcase or under_score. In 2009 IEEE 17th International Conference on Program
Comprehension . IEEE,158–167.
[6]Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R Klemmer. 2010.
Example-centric programming: integrating web search into the development
environment. In Proceedings of the SIGCHI Conference on Human Factors in Com-
puting Systems . 513–522.
[7]JoelBrandt,PhilipJGuo,JoelLewenstein,MiraDontcheva,andScottRKlemmer.
2009. Two studies of opportunistic programming: interleaving web foraging,
learning,andwritingcode.In ProceedingsoftheSIGCHIConferenceonHuman
Factors inComputingSystems . 1589–1598.
[8]Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-supervised contrastive
learning for code retrieval and summarization via semantic-preserving trans-
formations. In Proceedings of the 44th International ACM SIGIR Conference on
Research and Development inInformationRetrieval . 511–521.
[9]JoseCambronero,HongyuLi,SeohyunKim,KoushikSen,andSatishChandra.
2019. When deep learning met code search. In Proceedings of the 2019 27th ACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
the FoundationsofSoftwareEngineering . 964–974.
[10]Saikat Chakraborty, Tou/f_ique Ahmed, Yangruibo Ding, Premkumar Devanbu,
and Baishakhi Ray. 2022. NatGen: Generative pre-training by “Naturalizing”
source code. In Proceedings of the 30th ACM Joint Meeting on European Software
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(ESEC/FSE) . ACM.
[11]Deborah Coughlin. 2003. Correlating automated and human assessments of
machine translation quality. In Proceedings of Machine Translation Summit IX:
Papers.
[12]Luca Di Grazia and Michael Pradel. 2022. Code Search: A Survey of Techniques
for Finding Code. arXiv preprint arXiv:2204.02765 (2022).
[13]Li Dong and Mirella Lapata. 2018. Coarse-to-Fine Decoding for Neural Semantic
Parsing.In Proceedingsofthe56thAnnualMeetingoftheAssociation forCompu-
tationalLinguistics (Volume 1: Long Papers) . 731–742.
[14]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al .2020. CodeBERT: A Pre-
Trained Model for Programming and Natural Languages. In Findings of the Asso-
ciationfor ComputationalLinguistics: EMNLP 2020 . 1536–1547.
[15]MarkGabelandZhendongSu.2010. Astudyoftheuniquenessofsourcecode.
InProceedings of the eighteenth ACM SIGSOFT international symposium on Foun-
dationsofsoftwareengineering . 147–156.
[16]Philip Gage. 1994. A new algorithm for data compression. C Users Journal 12, 2
(1994), 23–38.
[17]XiaodongGu,HongyuZhang,andSunghunKim.2018. Deepcodesearch.In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE) . IEEE,
933–944.
[18]Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou,NanDuan,AlexeySvyatkovskiy,ShengyuFu,etal .2021. GraphCodeBERT:
Pre-training CodeRepresentationswith DataFlow. In ICLR.
[19]DavidHarel,HagiLachover,AmnonNaamad,AmirPnueli,MichalPoliti,Rivi
Sherman,AharonShtull-Trauring,andMarkTrakhtenbrot.1990. Statemate:A
working environment for the development of complex reactive systems. IEEE
Transactions onsoftwareengineering 16,4 (1990), 403–414.
[20]Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. 2018.
Aretrieve-and-editframework for predicting structured outputs. Advancesin
Neural InformationProcessingSystems 31(2018).
[21]Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin,
Anthony Tomasic, and Graham Neubig. 2018. Retrieval-Based Neural Code
Generation.In Proceedingsofthe2018ConferenceonEmpiricalMethodsinNatural
Language Processing .
[22]Xing Hu, Qiuyuan Chen, Haoye Wang, Xin Xia, David Lo, and Thomas Zim-
mermann.2021. CorrelatingAutomatedandHumanEvaluationofCodeDocu-
mentationGenerationQuality. ACMTransactionsonSoftwareEngineeringand
Methodology (2021).
[23]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
codesearch. arXiv preprint arXiv:1909.09436 (2019).
[24]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018.
Mapping Language to Codein Programmatic Context.In EMNLP.[25]Amy J Ko, Brad A Myers, and Htet Htet Aung. 2004. Six learning barriers in end-
userprogrammingsystems.In 2004IEEESymposiumonVisualLanguages-Human
CentricComputing . IEEE,199–206.
[26]Triet HM Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep learning for
sourcecodemodelingandgeneration:Models,applications,andchallenges. ACM
ComputingSurveys(CSUR) 53,3 (2020), 1–38.
[27]Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep
Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, and Neel Sundare-
san.2022. AutomatingCodeReviewActivitiesbyLarge-ScalePre-Training.In
Proceedings of the 30th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022) .
1035–1047. https://doi.org/10.1145/3540250.3549081
[28]Chunyang Ling, Zeqi Lin, Yanzhen Zou, and Bing Xie. 2020. Adaptive deep code
search.In Proceedingsofthe28thInternationalConferenceonProgramCompre-
hension. 48–59.
[29]WangLing,PhilBlunsom,EdwardGrefenstette,KarlMoritzHermann,Tomáš
Kočisk`y, Fumin Wang, and Andrew Senior. 2016. Latent Predictor Networks for
Code Generation. In Proceedings of the 54th Annual Meeting of the Association for
ComputationalLinguistics (Volume 1: Long Papers) . 599–609.
[30]Erik Linstead, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and
Pierre Baldi. 2009. Sourcerer: mining and searching internet-scale software
repositories. DataMiningand KnowledgeDiscovery 18,2 (2009), 300–336.
[31]Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and John Grundy. 2021.
Opportunities and challenges in code search tools. ACM Computing Surveys
(CSUR)54,9 (2021), 1–40.
[32]Hui Liu, Mingzhu Shen, Jiaqi Zhu, Nan Niu, Ge Li, and Lu Zhang. 2020. Deep
Learning Based Program Generation from Requirements Text: Are We There
Yet?IEEE Transactions onSoftwareEngineering 01(2020), 1–1.
[33]Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-
sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al .2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understand-
ingandGeneration.In Thirty-/f_ifthConferenceonNeuralInformationProcessing
SystemsDatasetsand BenchmarksTrack (Round1) .
[34]Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei Wang, Dongmei Zhang, and
Jianjun Zhao. 2015. Codehow: Eﬀective code search based on api understanding
and extended boolean model (e). In 2015 30th IEEE/ACM International Conference
onAutomatedSoftwareEngineering (ASE) . IEEE,260–270.
[35]Henry B Mann and Donald R. Whitney. 1947. On a Test of Whether One of
Two Random Variables Is Stochastically Larger than the Other. The Annals of
MathematicalStatistics 18,1 (1947),50–60. https://doi.org/10.1214/aoms/11777
30491
[36]Antonio Mastropaolo, SimoneScalabrino,NathanCooper, David NaderPalacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
usage of text-to-text transfer transformer to support code-related tasks. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE,
336–347.
[37]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:/f_indingrelevantfunctionsandtheirusage.In Proceedingsofthe
33rdInternationalConference onSoftwareEngineering . 111–120.
[38]Collin McMillan, Negar Hariri, Denys Poshyvanyk, Jane Cleland-Huang, and
Bamshad Mobasher. 2012. Recommending source code for use in rapid software
prototypes. In 2012 34th International Conference on Software Engineering (ICSE) .
IEEE,848–858.
[39]Parastoo Mohagheghi andReidarConradi. 2007. Quality,productivity andeco-
nomicbene/f_itsofsoftwarereuse:areviewofindustrialstudies. EmpiricalSoftware
Engineering 12,5 (2007), 471–516.
[40]Tasuku Nakagawa, Yoshiki Higo, and Shinji Kusumoto. 2021. NIL: large-scale
detection of large-variance clones. In Proceedings ofthe 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
ofSoftwareEngineering . 830–841.
[41]SonNguyen,HungPhan,TrinhLe,andTienN.Nguyen.2020. SuggestingNatural
MethodNamestoCheckNameConsistencies.In ProceedingsoftheACM/IEEE
42nd InternationalConference onSoftwareEngineering . 1372–1384.
[42]Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang, and Bin Luo.
2022. SPT-Code:Sequence-to-SequencePre-TrainingforLearningSourceCode
Representations.In Proceedingsofthe44thInternationalConferenceonSoftware
Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . Association for Computing
Machinery,NewYork,NY,USA, 2006–2018. https://doi.org/10.1145/3510003.35
10096
[43]Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In Proceedings of the
40thannualmeetingofthe Associationfor ComputationalLinguistics . 311–318.
[44]ChrisParnin and AlessandroOrso. 2011. Areautomated debuggingtechniques
actuallyhelpingprogrammers?.In Proceedingsofthe2011internationalsymposium
onsoftwaretestingand analysis . 199–209.
[45]Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits
ofTransferLearningwithaUni/f_iedText-to-TextTransformer. JournalofMachine
386Natural Languageto Code: HowFarAre We? ESEC/FSE ’23, December3–9, 2023,San Francisco, CA, USA
Learning Research 21(2020), 1–67.
[46]ShuoRen,DayaGuo,ShuaiLu,LongZhou,ShujieLiu,DuyuTang,NeelSundare-
san, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for
automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[47]CaitlinSadowski,KathrynTStolee,andSebastianElbaum.2015. Howdevelopers
search for code: a case study. In Proceedings of the 2015 10th joint meeting on
foundations ofsoftwareengineering . 191–201.
[48]Jiho Shin and Jaechang Nam. 2021. A Survey of Automatic Code Generation
fromNaturalLanguage. JournalofInformationProcessingSystems 17,3(2021),
537–555.
[49]JianhangShuai,LingXu,ChaoLiu,MengYan,XinXia,andYanLei.2020. Im-
proving code searchwith co-attentiverepresentation learning. In Proceedingsof
the 28thInternationalConference onProgramComprehension . 196–207.
[50]Susan Elliott Sim, Charles LA Clarke, and Richard C Holt. 1998. Archetypal
sourcecodesearches:Asurveyofsoftwaredevelopersandmaintainers.In Pro-
ceedings.6thInternationalWorkshoponProgramComprehension.IWPC’98(Cat.
No.98TB100242) . IEEE,180–187.
[51]WeisongSun,ChunrongFang,YuchenChen,GuanhongTao,TingxuHan,and
QuanjunZhang.2022. Code SearchbasedonContext-awareCodeTranslation.
InProceedingsofthe 44thInternationalConference onSoftwareEngineering .
[52]Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the Importance
of Building High-quality Training Datasets for Neural Code Search. In 2022
IEEE/ACM44thInternationalConference onSoftwareEngineering (ICSE) . ACM.
[53]Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang. 2019. A
grammar-based structural cnn decoder for code generation. In Proceedings of the
AAAI conference onarti/f_icialintelligence , Vol. 33.7055–7062.
[54]ZeyuSun,QihaoZhu,YingfeiXiong,YicanSun,LiliMou,andLuZhang.2020.
Treegen:Atree-basedtransformerarchitectureforcodegeneration.In Proceed-
ingsofthe AAAI Conference onArti/f_icial Intelligence , Vol. 34.8984–8991.
[55]Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language
Processing(Volume 1: Long Papers) . 1556–1566.
[56]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advancesinneuralinformation processingsystems 30(2017).
[57]YaoWan,JingdongShu,YuleiSui,GuandongXu,ZhouZhao,JianWu,andPhilip
Yu. 2019. Multi-modal attention network learning for semantic source code
retrieval. In 2019 34th IEEE/ACM International Conference on Automated Software
Engineering (ASE) . IEEE,13–25.
[58]ShangwenWang.2023. TheArtifactoftheESEC/FSE2023PaperTitled"Natural
Language toCode:HowFarAreWe?" .https://doi.org/10.5281/zenodo.7546358
[59]Shangwen Wang, Bo Lin, Zhensu Sun, Ming Wen, Yepang Liu, Yan Lei, and
XiaoguangMao.2023. TwoBirdswithOneStone:BoostingCodeGenerationand
Code Search via a Generative Adversarial Network. Proc. ACM Program. Lang. 7,
OOPSLA2,Article239(oct 2023),30pages. https://doi.org/10.1145/3622815
[60]YueWang,WeishiWang,Sha/f_iqJoty,andStevenCHHoi.2021.CodeT5:Identi/f_ier-
awareUni/f_iedPre-trainedEncoder-DecoderModelsforCodeUnderstandingandGeneration.In Proceedingsofthe2021ConferenceonEmpiricalMethodsinNatural
Language Processing . 8696–8708.
[61]Michael W Whalen. 2000. High-integrity code generation for state-based for-
malisms.In Proceedingsofthe2000InternationalConferenceonSoftwareEngineer-
ing.ICSE2000the NewMillennium . IEEE,725–727.
[62]Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E Hassan, and
ZhenchangXing.2017. Whatdodeveloperssearchforontheweb? Empirical
SoftwareEngineering 22,6 (2017), 3149–3185.
[63]FrankFXu,BogdanVasilescu,andGrahamNeubig.2022. In-idecodegeneration
fromnaturallanguage:Promiseandchallenges. ACMTransactionsonSoftware
Engineering and Methodology (TOSEM) 31,2 (2022), 1–47.
[64]LingXu,HuanhuanYang,ChaoLiu,JianhangShuai,MengYan,YanLei,andZhou
Xu. 2021. Two-StageAttention-BasedModelforCodeSearch withTextual and
StructuralFeatures.In 2021IEEEInternationalConferenceonSoftwareAnalysis,
Evolutionand Reengineering (SANER) . IEEE,342–353.
[65]Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. Coacor: Code
annotationforcoderetrievalwithreinforcementlearning.In Theworldwideweb
conference . 2203–2214.
[66]PengchengYinandGrahamNeubig.2017. ASyntacticNeuralModelforGeneral-
Purpose Code Generation. In Proceedings of the 55th Annual Meeting of the Asso-
ciationfor ComputationalLinguistics (Volume 1: Long Papers) . 440–450.
[67]PengchengYinandGrahamNeubig.2018. TRANX:ATransition-basedNeural
AbstractSyntaxParserforSemanticParsingandCodeGeneration.In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing:
SystemDemonstrations . 7–12.
[68]DaoguangZan,BeiChen,DejianYang,ZeqiLin,MinsuKim,BeiGuan,Yongji
Wang,WeizhuChen,andJian-GuangLou.2022.CERT:ContinualPre-Trainingon
Sketches for Library-Oriented Code Generation. arXiv preprint arXiv:2206.06888
(2022).
[69]Chen Zeng, Yue Yu, Shanshan Li, Xin Xia, Zhiming Wang, Mingyang Geng,
LinxiaoBai,WeiDong,andXiangkeLiao.2022. deGraphCS:EmbeddingVariable-
basedFlowGraphforNeuralCodeSearch. ACMTransactionsonSoftwareEngi-
neering and Methodology (TOSEM) (2022).
[70]Zhengran Zeng, HanzhuoTan, HaotianZhang,JingLi,Yuqun Zhang,and Ling-
ming Zhang. 2022. An Extensive Study on Pre-trained Models for Program
Understanding and Generation. In Proceedings of the 31st ACM SIGSOFT Interna-
tional SymposiumonSoftwareTestingand Analysis . ACM.
[71]Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Glig-
oric.2022. CoditT5:PretrainingforSourceCodeandNaturalLanguageEditing.In
Proceedingsofthe37thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering . ACM.
[72]Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni,
and Chandan K. Reddy. 2022. XLCoST: A Benchmark Dataset for Cross-lingual
CodeIntelligence. arXiv: 2206.08474 https://arxiv.org/abs/2206.08474
[73]QihaoZhu,ZeyuSun,XiranLiang,YingfeiXiong,andLuZhang.2020. OCoR:an
overlapping-awarecoderetriever.In 202035thIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering (ASE) . IEEE,883–894.
Received 2023-03-02; accepted 2023-07-27
387