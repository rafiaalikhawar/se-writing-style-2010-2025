Manas: Mining Software Repositories to Assist AutoML
Giang Nguyen
gnguyen@iastate.edu
Dept. of Computer Science, Iowa State University
Ames, IA, USAMd Johirul Islam‚àó
mislam@iastate.edu
Dept. of Computer Science, Iowa State University
Ames, IA, USA
Rangeet Pan
rangeet@iastate.edu
Dept. of Computer Science, Iowa State University
Ames, IA, USAHridesh Rajan
hridesh@iastate.edu
Dept. of Computer Science, Iowa State University
Ames, IA, USA
ABSTRACT
Todaydeeplearningiswidelyusedforbuildingsoftware.Asoft-
ware engineering problem with deep learning is that finding an
appropriate convolutional neural network (CNN) model for the
task can be a challenge for developers. Recent work on AutoML,
morepreciselyneuralarchitecturesearch(NAS),embodiedbytools
likeAuto-Keras aimstosolvethisproblembyessentiallyviewing
it as a search problem where the starting point is a default CNN
model,and mutationofthis CNNmodelallows explorationofthe
space ofCNN modelsto finda CNNmodel thatwill workbest for
theproblem.Theseworkshavehadsignificantsuccessinproduc-
ing high-accuracy CNN models. There are two problems, however.
First,NAScanbeverycostly,oftentakingseveralhourstocomplete.
Second,CNNmodelsproducedbyNAScanbeverycomplexthat
makes it harder to understand them and costlier to train them. We
propose a novel approach for NAS, where instead of starting from
a default CNN model, the initial model is selected from a repos-itory of models extracted from GitHub. The intuition being that
developers solving a similar problem may have developed a bet-
terstartingpointcomparedtothedefaultmodel.Wealsoanalyze
commonlayerpatternsofCNNmodelsinthewildtounderstand
changes that the developers make to improve their models. Our
approachusescommonlyoccurringchangesasmutationoperators
in NAS. We have extended Auto-Keras to implement our approach.
Ourevaluationusing8topvotedproblemsfrom Kagglefortasks
including image classification and image regression shows that
giventhesamesearchtime,withoutlossofaccuracy, Manaspro-
duces models with 42.9%to99.6%fewer number of parameters
thanAuto-Keras ‚Äô models. Benchmarked on GPU, Manas‚Äô models
train30.3%to641.6%faster than Auto-Keras ‚Äô models.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSearch-based software en-
gineering ;‚Ä¢Computingmethodologies ‚ÜíMachinelearning .
‚àóThis work was done when Md Johirul Islam was at Iowa State University.
This work is licensed under a Creative Commons Attribution International 4.0 
License.
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
¬© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9221-1/22/05.
https://doi.org/10.1145/3510003.3510052KEYWORDS
Deep Learning, AutoML, Mining Software Repositories, MSR
ACM Reference Format:
Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan. 2022.
Manas: Mining Software Repositories to Assist AutoML. In 44th Interna-
tional Conference on Software Engineering (ICSE ‚Äô22), May 21‚Äì29, 2022, Pitts-
burgh,PA,USA. ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/
3510003.3510052
1 INTRODUCTION
An increasingly larger number of software systems today are in-
cludingdeeplearning.Deeplearningusesaconvolutionalneural
network(CNN)model,essentiallyagraphwithweightededgesand
nodes that are weighted functions, to convert inputs to the output.
As more software systems incorporate deep learning, more soft-
waredevelopershavetodesignandtrainCNNmodels.Designinga
CNN model is very difficult, and developers often struggle leading
to bugs. Model bugs are frequent bugs in CNN programs [29, 30].
Recentworkonneuralarchitecturesearch(NAS)aimstosolve
thisproblem[ 61].NAStechniquesstartfromacollectionofdefault
CNN models and search for a suitable model for the problem. The
searchspaceisdefinedbythecollectionofdefaultmodelsandacol-
lection of mutation operators that are used to modify CNN models
to create new candidates. NAS techniques have been implemented
inindustrial-strengthtoolssuchas Auto-Keras [31].NAStechniques
facetwoproblems.First,NAScanbeverycostly,e.g., Auto-Keras
takes 8-12 hours on high performance machines to search for mod-
els with reasonable accuracy (90+%). The accuracy drops rapidly if
the search time is reduced. Second, CNN models produced by NAS
canbeverycomplexthatmakesithardertounderstandthemfor
maintenance, and costlier to train and retrain them.
Weintroduce Manas(MiningAssistedNeuralArchitectureSearch)
toalleviatethelimitationsofNAS.Thefundamentalintuitionbe-
hindManasisthatminingandusingthehand-developedmodels
that are available in open-source repositories as default models
or starting point of search can help NAS leverage human devel-
oper efforts. Manasapplies this intuition in two ways. First, hand-
developedmodelsareminedtosearchforabetterstartingpointfor
NAS.Second,thechangepatternsofthehand-developedmodels
are mined to identify more suitable mutation operators for NAS.
Wehaverealized Manasbyextending Auto-Keras ,thestate-of-
the-artNASframework. Auto-Keras isopensourceandoutperforms
state-of-the-art methods like SEAS [ 17], NASBOT [ 32] making it a
suitablebaseline[ 31].Somekeytechnicalcontributionsin Manas
13682022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan
(a) Problem
 (b) Solution
Figure 1: An example from StackOverflow showing the necessity to change model architecture
includemodelmatching ,atechniqueformatchingtheproblemthat
the developer intends to solve with the hand-developed models
minedfromrepositories, modeladaptation ,atechniqueforadapting
theminedmodeltotheproblemcontext, modeltransformation ,a
techniqueforadaptingtheminedmodelforfurtherimprovingmet-
ric values, and training adaptation that leverages mined parameter
values from the repositories to change the optimizer.
To evaluate Manas, we use the top-8 problems from diverse do-
mainsobtainedfromKaggleformachinelearningtasksincluding
imageclassificationandimageregression.Ourevaluationshows
that given the same amount of searching time, Manasgenerates
simpler neural architectures than Auto-Keras without losing accu-
racy. In terms of the models‚Äô size, Manas‚Äô models have 42.9%to
99.6%fewernumbersofparametersthan Auto-Keras ‚Äômodels.We
observed up to 641.6%faster training s peed when training models
produced by Manasas compared to those produced by Auto-Keras .
Our main contributions are the following:
‚Ä¢We have proposed a novel approach for NAS that leverages
software repository mining.
‚Ä¢Wehaveproposedmethodstoidentifythesuitablemodels
by analyzing data characteristics and adapting models.
‚Ä¢Wehaveutilizedthecommonpatternstotransformmined
modelstoimprovetheperformanceofthesemodelsinterms
of error rate, MSE, model complexity, and training time.
‚Ä¢WehaveimplementedtheseideasinaSOTANASframework,
Auto-Keras [31].Our artifact is available here [21].
The paper is organized as follows: ¬ß2 presents a motivating
example,¬ß3presentspreliminariesandproblemstatement,¬ß4de-
scribesthe ManasapproachforNAS,¬ß6describesthelimitations
and threats to validity of Manas, ¬ß7 describes related work, and ¬ß8
concludes.
2 MOTIVATION
Deep Learning has received much attention in both academia and
industry. Therefore, many deep learning libraries and tools are cre-
ated for supporting a large number of deep learning developers.
Althoughtheselibrariesandtoolsmakedeeplearningmoreaccessi-
ble,therearestillmanychallenges.Oneofthechallengingproblems
is constructing an appropriate CNN model architecture [ 53,54],
which also has been shown as a frequent bug in CNN programs by
Islametal.[29,30].Forinstance,Figure1ashowsaquery[ 1]posted
onStackOverflow whereadeveloperisunabletofindanappropriateCNN model for their purpose. In particular, the question discusses
thedifficultythattheResNetarchitecturedoesnotgivetheresult
asthedeveloperexpected.Inresponse,anexpertsuggestschanging
the CNN model. Figure 1b shows the solution of the expert for the
question of the developer. The expert suggested that the developer
should add dropout layers to minimize overfitting.
Neuralarchitecturesearch(NAS)aimstosolvethisproblem[ 61].
NAS takes the training data as an input to automatically define the
neural network for that data. Moreover, NAS is able to tune the
hyperparameter of the searched neural network. There are both
commercialandopen-sourcerealizationsofNAS.Forexample,a
developer can pay about $20 per hour to use Google‚Äôs AutoML.
Auto-Keras is an AutoML system using NAS [ 31] created as an
open-source alternative. Auto-Keras returns outstanding results
comparedwithstate-of-the-arthandmademodelsonCIFAR10[ 33],
MNIST[34],andFASHION[ 57]datasets. Auto-Keras isshownto
outperform state-of-the-art methods like SEAS [ 17], NASBOT [ 32].
NAShastwolimitations.First,itcanbeverycostly.Forexample,
Auto-Keras consumes 2,300% [ 47] more GPU computation time
compared to using handmade model. Second, NAS often produces
complexmodelsthatarehardtounderstandandtime-consumingto
train.Toillustrate,weused Auto-Keras onanotherdataset,which
isBlood Cell [39] collected from Kaggle. The model created by
Auto-Keras for theBlood Cell problem has more than 2.3 million
learnable parameters and more than 7 weight layers. The searched
CNN models are constructed based on the architecture of the large
default CNN models; thus, the models produced by Auto-Keras are
oftenreally large.Smaller CNNmodelstrain fasterandsave more
energy[27,44].Hanetal.haveshownthatreducingthenumberof
parametersofdeeplearningmodelscanreducethetrainingtimeby
3√óto4√ó,andenergycomsumptionby3 √óto7√ó[24].Therestofthis
workdescribesourapproach Manasthataddressestheselimitations.
Asanexample,for BloodCell dataset,Manasproducesamodelthat
decreases the error rate by 47.1%, the model depth by 14.3%, the
model width by 87.0%, and incr eases the training s peedby56.9%
compared with Auto-Keras ‚Äôs model. The model produced by Manas
has 6 layers and 0.3 million parameters (neurons), whereas the
model produced by Auto-Keras has 7 layers, 2.3 million parameters.
1369Manas: Mining Software Repositories to Assist AutoML ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA

¬êdsQYIhQYjIgQ[O
0RGHO0LQLQJ
QYjIgIG¬êdsQYIhrjg<Ej! 
!]GIY
¬êdsQYIh
!]GIY<j<D<hI
rjg<Ej! 
$djQZQvIg[dkj¬êdsQYI
rjg<Ej

1hIg¬íh
Ihj""!]GIYh
!Q[IG""!]GIYh
""$djQZQvIgh
Dhjg<Ej"Ikg<Y
"Ijq]gX
QjPkD
"Ikg<Y
gEPQjIEjkgI
/I<gEP
 !]GIY
G<dj<jQ][
$GDSWDWLRQ6\VWHP
kg<Ykg<Y
,QSXW
][pIgjIG""
!]GIY
*OREDOB3RRO5HOX
)ODWWHQ$djQZQvIgh
0RGHO7UDQVIRUPDWLRQ
6RIWPD["Ikg<Y
gEPQjIEjkgI
/I<gEP
IhjIN<kYj
""!]GIY0g<[hN]gZIG
""!]GIYh
'URSRXW
%1
0g<Q[Q[O
G<dj<jQ][
$djQZQvIg
/kOOIhjIGIN<kYj
""!]GIYh0RGHO0DWFKLQJ
""!]GIY
QYjIgQ[O
""!]GIY
YkhjIgQ[O
!<jEPQ[O
[dkjP<[[IY
!<jEPQ[O
[dkj/QvI
!<jEPQ[O
$kjdkj
S
2XWSXW
2XWSXW
!Ij<¬ü
Q[N]gZ<jQ][
"$ Q Q
kYjGIY
[OO[O'&'HULYDWLRQ
!]GIYh¬í
Figure 2: An Overview of Manas. Two inputs are mined models from repository (left-top), and user‚Äôs initial file (middle-top).
3 PRELIMINARIES AND PROBLEM
STATEMENT
3.1 Preliminaries (NAS)
WedefineNASlikeitwasdefinedin Auto-Keras [31].Givenasearch
spaceSand the input data Dsplit into Dùë°ùëüùëéùëñùëõandDùë£ùëéùëô, the goal
is to find an optimal neural architecture N‚ààS, which achieves
the lowest value of the cost function in dataset D. Search space
Scovers all the neural architectures created from default neural
networks.
N=argmin
N‚ààSCost(N/prime(ùúÉ/prime),Dùë£ùëéùëô) (1)
ùúÉ/prime=argmin
ùúÉL(N/prime(ùúÉ),Dùë°ùëüùëéùëñùëõ) (2)
WhereCostandLare the cost function and loss function, ùúÉis the
learned parameter of initial architectures N/prime.
3.2 Problem statement
This work aims to utilize the neural networks from open source
repositories to optimize the neural architecture search. We extract
the data characteristics from both the input dataset and mined
neural networks todetermine better starting points (initial models)
forNAS.Insteadofusingconcretedefaultneuralnetworks,thegoal
is to find optimal initial architecture N‚àófor NAS for each different
inputdataset.SimilartoEquation2,weobtainlearnedparameter
ùúÉ‚àóof new initial architectures N‚àó:
ùúÉ‚àó=argmin
ùúÉL(N‚àó(ùúÉ),Dùë°ùëüùëéùëñùëõ) (3)
The optimal initial architectures N‚àósupport NAS to find out
theoptimalneuralnetworkfaster.Inotherwords,withthesame
amountofsearchingtimeandinputdataset,newinitialarchitec-
tures help NAS to find out a neural network with lower error com-
pared to the original NAS:
argminCost(N‚àó(ùúÉ‚àó),Dùë£ùëéùëô)<argminCost(N/prime(ùúÉ/prime),Dùë£ùëéùëô)(4)4 MANAS
Figure 2 shows the overview of Manas.Manashas five major com-
ponents that we describe below.
1To initialize Manasfor NAS, the model database must be
populated by mining models from open source repositories.
Currently, Manascollectshigh qualitymodelsfrom GitHub
byextractingPythonfilesfrom Kerasprojects.Theseprojects
are selected using certain filtering criteria to ensure code
quality.Then,APIusageisusedtofilterPythonfilestothose
that contain models. Finally, both the models and the values
for optimizer are extracted to store in the model database.
This database is constructed once and should be updated
periodically as new models are added frequently.
2The data characteristics extracted from both the input data
andminedmodelsareusedtoselectthesuitableinitialmod-
els for NAS from the database.
3Model matching matches the data characteristics extracted
from an input dataset and the mined models to obtain a
good starting point. It selects the models, which have the
closestdatacharacteristicswiththeinputdatasetbyusing
themodelclusteringapproach.Incasetherearetoomany
initialmodels,themodelfilteringapproachwillbeapplied
to reduce the number of models.
4The selected models are transformed by the model trans-
formation approach based on related state-of-the-art papers
andcommonlayerAPIpatternsofminedmodels.Thetrans-
formation can enhance the performance of the models in
terms of errors and training s peed.
4.1 Model Mining
InordertoextractCNNmodelsandtheiroptimizersfromsource
code repositories, we build a source code analyzer based on the
control flow graph (CFG). Figure 3 shows the overview of model
mining process.
1370ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan
7RS
UHSRVLWRULHV

SURJUDPV*LW+XE
$3,4XHU\
SURJUDPV
&)*
6WDU&RXQW
 3\WKRQ
XE
HU\
.HUDVNH\ZRUG
3\WKRQODQJXDJH
&UHDWHGJH0HWDGDWDRI
*LW+XEUHSRVLWRULHV

PRGHOV
PRGHOV
PRGHOV
,QFRPSOHWH
PRGHOV
6XSSRUW
E\0DQDV
1RQGXSOLFDWH
PRGHOV
.HUDV

PRGHOV
([WUDFWLQJ
PRGHOV
Figure 3: Model mining process.
4.1.1 Meta-data collection. We collect the meta-data of GitHub
repositories by using a GitHubAPI query1. Meta-data contains
basic information of a repository such as authors, repository‚Äôs
name, etc. The query allows us to obtain the meta-data of the
GitHubrepositorieswiththreefilteringcriteriaincludingPython
programming language, containing Keraskeyword, and created
datebetween2015-01-01and2020-12-31.Fromthemeta-data,we
obtainGitHubURLs of the top 10,000 repositories with the most
starcounttoensurethequalityofthemodels[ 8].TheURLshelp
us to access the repositories to collect the CNN models.
4.1.2 Keras programs collection. We obtain 202,174 Python pro-
grams from collected repositories; however, only 42,139 programs
useKerasAPI. In particular, we use CFG to analyze their import
statements of Python programs to only collect which one imports
KerasAPI.
4.1.3 Models extraction. In this work, since Manasonly works
withKerasCNN, we will explain how we extract CNN from Keras
programs. We use CFG to extract a model from a deep learning
program.Wemanuallycreatealistoffunctioncallsusedtobuild
neuralnetworksof Kerasbasedon Keras‚Äôdocumentation[ 2].Then,
CFG examines every API call to collect the functions contained in
thelistandtheirconnections.Thecollectedfunctionsrepresentthe
layers in the models. The connections between functions represent
the layer connections in the models. The reason for using CFG
istocollectcompletemodelsfromprogramseveniftheycontain
branches. For example, the CFG contains a convolutional block
followed by a dense block in the "if" branch and a skip connection
inthe"else"branch.Ifaconvolutionalblockcombineswithadense
block or a convolutional block combines with a skip connection to
be a complete model, we will extract those parts in the branches to
collectthecompletemodel.CFGisusedtohandlethesituationthat
thereisaloopcontainingapartofamodelintheCNNprograms.If
thenumberofiterationsisavailabletoextractthatpartcompletely,
we will collect it to complete the model. If there is a method call
in the program containing a part of the model, we will collect it
tocompletethemodel.Fortheothercases,whenthepartsinthe
branches cannot complete the model or cannot be extracted, we
will ignore them. For instance, a part of the model in a loop whose
numbers of iterations are unavailable cannot be extracted. After
thisstep,wecollected38,808modelsfrom42,139 Kerasprograms.
Thenumberofmodelsislessthanthenumberof Kerasprograms
1"https://api.github.com/search/repositories?q=keras+language:python+created:
yyyy-mm-dd"because many programsdo not contain models. We assume that a
programcontainsamodelifithasatleastanAPIusedtobuilda
model.
Rather than mining only neural architectures, we also mine
their optimizers thatdeeplearning developers carefullyselectafter
spendingmanualeffortsonretrainingtheirmodels.Then,when-
everamodelisselectedasaninitialmodel, Manastrainsthemodel
withitsoptimizerinsteadofthedefaultone.Optimizersarealgo-
rithmsdecidinghowtheparametersofthemodelschange.Every
optimizer has strengths and weaknesses; thus, it is necessary to
chooseasuitableoptimizer.Whilemodelsarethedecisivefactor
to the performance of Manas, we note that it is wasteful if we
cannotfullyusethesemodels.Inotherwords,goodmodelswith
wrongoptimizerscannotproduceagoodperformance.Toobtain
the optimizer, we use the same process of extracting CNN models
bycreatingalistoffunctionsrelatedtotheoptimizer.Afterthat,
CFG analysis is used to obtain the API call, which contains the
optimizer.
4.1.4 Incomplete models detection. A complete model includes an
input layer, hidden layers, and an output layer [ 60]. The input
layer is the first convolutional layer of the neural network, which
is distinguished from the other convolutional layers based on its
parameters.Inparticular,onlytheAPIcallrepresentingtheinput
layercontains input_shape parameter.Theoutputlayer isthelast
linearlayerof theCNN.Hiddenlayersarethe layersbetweenthe
inputlayerandtheoutputlayer,includingconvolutionallayers,ac-
tivationlayers,andfullyconnectedlayers(linearlayers).Therefore,
ifanextractedmodeldoesnothaveconvolutionallayers,activation
layers,orlinearlayers,wewillconsiderthatthemodelisincom-
plete. By removing incomplete models, there are 29,846 models
left.
4.1.5 Supported by Manas. AsManascurrently supports a few
kindsoflayersthataretheconvolutionallayer,thelinearlayer,the
batch normalization layer, the concatenate layer, the add layer, the
maxpoolinglayer,thedropoutlayer,theSoftmaxlayer,theReLU
layer, the flatten layer, and the global pooling layer, we filter out
the models containing unsupported layers. After this step, we have
1,370 models left.
4.1.6 Model duplication detection. We obtain 793models after re-
moving the duplicate ones. We consider if two models have the
same abstract neural network (ANN), there will be a duplicate
model. We store the extracted CNN models in a database as an
abstract neural network, which is an abstract representation of the
neural network. This representation has the structure as a network
where the nodes are API calls, and the connections are the order
between API calls. We use this representation to adapt models and
their optimizers into Manas. From each node, we obtain the name
of layer and its parameters, which can be converted into an API
call. Figure 4 presents an example of ANN built from an mined
model. Notice that if an activation parameter is implemented as
an argument inside a layer, we consider that the activation is a
separate layer.
1371Manas: Mining Software Repositories to Assist AutoML ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
1Conv2D(64, kernel_size = (3, 3),activation='relu',
input_shape=(3, 120, 180))
2
3MaxPooling2D(pool_size=(2,2))
4Conv2D(32, activation='relu')
5
6MaxPooling2D(pool_size=(2,2))
7Dropout(0.25)
8Flatten()
9Dense(20, activation='softmax')
12SGD(lr=0.01, decay=1e‚àí6)
(a) Extracted model1{'func': 'Conv2D', 'input_shape': [3, 120, 180], 'arg2
': 64, 'kernel_size': [3, 3]}
2{'func': 'relu'}
3{'func': 'MaxPool2d', 'pool_size': [2, 2]}
4{'func': 'Conv2D', 'arg1': 64, 'arg2': 32}
5{'func': 'relu'}
6{'func': 'MaxPool2d', 'pool_size': [2, 2]}
7{'func': 'Dropout', 'arg1': 0.25}
8{'func': 'Flatten'}
9{'func': 'linear', 'arg1': 128, 'arg2': 20}
10{'func': 'softmax'}
11
12{'func': 'SGD', 'lr': 0.01, 'decay': 1e‚àí06}
(b) Abstract Neural Network
Figure 4: Building an abstraction of neural network from
extracted model
4.2 Data Characteristics Derivation
Weanalyzestheinputdataandinput/outputlayersofthemined
modelstoextracttheirdatacharacteristics.Takinganimageclas-
sificationasanexample,weobtaintheinputdatacharacteristics
including the input size and the output channel from the image
data.Thedatacharacteristicsextractedfromaminedmodelalso
aretheinputsizeandtheoutputchannelareobtainedbyanalyz-
ing the first convolutional layer and the last linear layer of a CNN
model,whicharetheinputlayerandtheoutputlayerthemodel,
respectively.
Example1. Line1isanAPIcallrepresentingfortheinputlayerof
a CNN. We extract the valueof the argument input_shape from the
input layer to obtain the input size and the input channel, which are
(120,180)and3,respectively.Theoutputchannelisobtainedfromthe
output layer 3. By extracting the first argument‚Äôs value of the output
layer, we obtain the value of the output channel.
Conv2D('input_shape' : [3, 120, 180], activation= 'relu')
...
Dense(10, activation= 'relu')
Since we currently focus on image problems like image clas-
sification and image regression, we use the input size, the input
channel, and the output channel of image as data characteristics.
First, the input size includes the height and the width extracted
from theinput data and minedmodels. Second, theinput channel
representsthenumberofprimarycolorsintheimage.Third,the
output channelis the number ofoutput categories ofthe data and
models.
4.3 Model Matching
Modelmatchingisarankingsystemusedtofindgoodmodelsfora
certain problem by using the data characteristics. Instead of using
constant default models, model matching finds the suitable models
to uses them as new default models for NAS.
4.3.1 Model clustering. Model clustering uses the data characteris-
tics of both the input dataset and mined neural networks to select
appropriate initial architectures N‚àófor NAS. First of all, Manas
clusters the mined models based on the data characteristics of the
models and the input dataset. Secondly, in a meta-feature (data
characteristics) space, our approach identifies closest clusters to
theinputdataset.Lastly, Manasusesallthemodelsintheclosest
cluster to the dataset as the initial architectures for NAS. Formally,wedeterminetheinitialarchitectures N‚àófordataset DinEquation
3 as follows:
N‚àó={Cùëò|ùëõ‚ààùê∂ùëò,Cùëò‚ààC } (5)
C={G(Œîùëúùëò,Œîùë†ùëò)|Œîùëñùëò=0,ùëò‚àà[1,|S|]} (6)
dist(ùëõ,D)=/braceleftBigg
minŒîùëúif min Œîùëñ=0
minŒîùë†if min Œîùëñ=0,minŒîùëú=0(7)
InEquation6, Cisasetofclustersofneuralnetworkdetectedby
clustering algorithm G-means [ 23], which uses a statistical test to
automatically decide the number of clusters. Œîùëúùëò,Œîùëñùëò, andŒîùë†ùëòare
measured as follows:
Œîùëñùëò=|ùëñ‚àíùëñùëò| (8)
Œîùëúùëò=|ùëú‚àíùëúùëò| (9)
Œîùë†ùëò=/radicalBig
(ùë§‚àíùë§ùëò)2+(‚Ñé‚àí‚Ñéùëò)2 (10)
Whereùëñ,ùëú,ùë§,and‚Ñéaretheinputchannel,thenumberofoutput
classes, the input width, and the input height of the dataset, re-
spectively.Similarly, ùëñùëò,ùëúùëò,ùë§ùëò,and‚Ñéùëòaretheinputchannel,the
number of output classes, the input width, and the input height of
amodelk,respectively.Theideabehindtheclusteringequations
is that there are two types of the input channel, including 1 and
3.Therefore,weclassifytheneuralnetworksthathavethesame
input channel first. Then, we use ( Œîùëú,Œîùë†) as the input of G-means
tosplittheminedmodelsintoclusters.Afterthatweidentifythe
closest model ùëõto the input dataset like Equation 7. The closest
modelisidentifiedbasedonthepriorityof ŒîùëúandŒîùë†thatŒîùëútakes
precedenceover Œîùë†.Wehavetriedtorunourtoolindifferentorders
ofpriority ŒîùëúandŒîùë†;however,thisorderofprioritygivesusthe
bestresults.The closestclustertothe inputdatasetisthecluster
which contains the closest model. Then, we select all the models in
the closest cluster to the input dataset as the initial models N‚àófor
NAS shown in Equation 5.
4.3.2 Model filtering. If the number of initial models found by the
model clusteringapproachis too large, we usemodel filtering to
filter some models to increase the performance of Manas. In the
searching process, Manastrains all the default neural networks to
selectthebestone.AfterthatNASisappliedtotunetheselected
model. Thus, with a specific time budget, the more time Manas
spends on trying the default models, the less time it spends on
NASformodelsearching.Tofilteringneuralnetworks,wedetect
the equivalent architectures. We treat each neural architecture
as a graph, whose trainable layers like convolutional layers or
dense layers represent vertices, connections between two trainable
layersrepresentedges,andchannelsoftheoutgoingtrainablelayers
representtheweightsofedgesbetweentwovertices.WeuseCosine
similarity to measure the similarity between two vertices. ùëÄùëñùëóis
anelementofthesimilaritymatrixofvertex ùë£ùëñofagraph ùê∫ùê¥and
vertexùë£ùëóof a graph ùê∫ùêµ, which is measured as follows:
ùëÄùëñùëó=/summationtext.1ùëõ
ùëò=1ùë§ùëñùëòùë§ùëóùëò/radicalBig/summationtext.1ùëö
ùëô=1ùë§2
ùëñùëô/radicalBig/summationtext.1ùëò
ùëõ=1ùë§2
ùëóùëô(11)
Suppose that ùë£ùëòis the common neighbor of vertex ùë£ùëñand vertex
ùë£ùëó, we have ùë§ùëñùëòandùë§ùëóùëòare the weights of edges ùë£ùëòùë£ùëñandùë£ùëòùë£ùëó,
1372ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan
respectively. We also have ùë§ùëñùëôorùë§ùëóùëôare the weights of edges that
incident with each of the vertices ùë£ùëñandùë£ùëó, respectively.
ùë§ùëùùëû=/braceleftBigg
weight of the edge between ùë£ùëùandùë£ùëûin a graph ùê∫
0 otherwise
(12)
Wecreateasimilaritymatrix ùëÄforeachpairofgraphsandclassify
equal matrices into the same classes. In this work, we filter the
modelsbyonlyusingthe neuralnetworkwhosegraphsbelongto
the class having the highest number of matrices. In other words,
wewillonlychoosethemostusedarchitectureinthedetermined
initialarchitectureforNAS. Weconsiderthemost frequentarchi-
tecturetobemoresuitablethanothersbecausemanydeeplearning
developersrepeatedlyusetheseneuralarchitectures.Non-trainable
layerslikedropoutoractivationarenotusedasthefactorstode-
tect the similarities of the neural architectures because we want
to utilize the various usagesof these layers to increase the perfor-
manceof Manas.Particularly,differentmodelscan havethesame
graph structure because of the differences in non-trainable layers;
therefore,agraphcanrefertomanydifferentneuralarchitectures.
4.4 Model Transformation
Even though the mined models are from the top star GitHubrepos-
itories, they are not perfect. Therefore, we transform the initial
architectures by modifying or adding the batch normalization (BN)
layer, the flatten layer, the activation layers, the global average
pooling (GAP) layer, and the dropout layer to optimize NAS in
termsofspeed ,errors,andthenumberofparameters.Wechoose
theselayerstomodifythedefaultmodelsbecauseoftworeasons.
Firstly, we have found many common patterns related to these lay-
ers. Secondly, many recent studies have shown the effectiveness of
theselayersonincreasingtheperformanceofdeeplearningmodels.
Wehavecreatedasetofpre-definedrulestotransformnetworks
basedonrelatedstate-of-the-artpapersandcommonfunctioncalls‚Äô
patternsfromminedmodels. Manasanalyzesthesearchitectures
todeterminewhetherthenetworksatisfiesthepre-definedrules.If
theserulesaresatisfied,thepre-definedmodeltransformationswill
be applied. The pre-defined model transformations support NAS
to ignore transformations offered by our approach and focus on
the other transformations. The model transformations do not work
forAuto-Keras ‚Äô initial neural architectures because those default
models have already included these transformations.
4.4.1 Batch normalization layer constraint. We add a new batch
normalization layer between the convolutional layer and the ac-
tivation layer to increase training speed [ 28]. Using BN means
thatwemodifytheactivationstonormalizetheinputlayertode-
creasethetrainingtime.Manywell-knownneuralarchitectureslike
ResNet[25],DenseNet[ 26],EfficientNet[ 50]useBNtoincreasethe
trainingspeed . BN is also popular in optimizing NAS [13, 18, 61].
Example2. AccordingtoFigure5a,theoriginalmodelhasaconvo-
lutionallayerconnectstoaReLUlayer,whichisanactivationfunction
layer.Thus,inFigure5b,followingthebatchnormalizationlayercon-
straint, Manas adds a new BN layer between the convolutional layer
and the ReLU layer like the following example.1(0): Conv2d(3, 32, ...)
2
3(1): Tanh()
4(2): MaxPool2d(kernel=2, stride=2, ...)
5
6(3): Conv2d(32, 32, ...)
7
8(4): Tanh()
9(5): MaxPool2d(kernel=2, stride=2, ...)
10
11(6): Flatten()
12(7): Linear(in=32, out=32, ...)
13(8): ReLU()
14
15(9): Linear(in=32, out=2, ...)
16(10): Softmax()
(a) Original model1(0): Conv2d(3, 32, ...)
2(1): BatchNorm2d(32, ...)
3(2): ReLU()
4(3): MaxPool2d(kernel=2, stride=2, ...)
5(4): Dropout2d(p=0.5)
6(5): Conv2d(32, 32, ...)
7(6): BatchNorm2d(32, ...)
8(7): ReLU()
9(8): MaxPool2d(kernel=2, stride=2, ...)
10(9): Dropout2d(p=0.5)
11(10): GlobalAvgPool2d()
12(11): Linear(in=32, out=32, ...)
13(12): ReLU()
14(13): Dropout2d(p=0.25)
15(14): Linear(in=32, out=2, ...)
16(15): Softmax()
(b) Transformed model
Figure 5: Original CNN model vs transformed CNN model
1(0): Conv2d(3, 32, ...)
3(1): ReLU()
(a) Original model1(0): Conv2d(3, 32, ...)
2(1): BatchNorm2d(32, ...)
3(2): ReLU()
(b) Transformed model
Figure 6: Example of batch normalization layer constraint
4.4.2 Globalaveragepoolinglayerconstraint. WeuseGAPtore-
shape the data into the correct format for fully connected layers
to prevent overfitting [ 35]. Moreover, we use the mined models as
defaultmodelsforNAS;thus,theoriginalinputsizeoftheinitial
modelsandtheinputsizeofthedatasetcanbedifferent,whichcan
causeashapemismatchbug. However,using GAPcansolvethis
problem since it does not care about the input shape.
Example3. AccordingtoFigure5a,theoriginalmodelusedthe
flattenlayertopassthe featuremapthroughtheCNN.Therefore,in
Figure5b,followingtheconstraintaboutGAPlayer,Manastransforms
flatten into GAP like the following example.
1Flatten()
2Linear(in=32, out=32, ...)
(a) Original model1GlobalAvgPool2d()
2Linear(in=32, out=32, ...)
(b) Transformed model
Figure 7: Example of global average pooling layer
constraint
4.4.3 Activation layer constraint. We investigate the patterns of
usagesofactivationfunctionsusedintheminedmodel.Wehave
found3218 hiddenlayersused in793models,where ReLUisused
2946times,accountingfor94.55%.Therefore,wereplacethecurrent
activation layers of convolutional layers with ReLU.
Example 4. According to Figure 5a, the original model uses Tanh
for the convolutional layer. Thus, following the constraint of the acti-
vation layer, Manas transforms Tanh to ReLU like following example
like the following example.
1373Manas: Mining Software Repositories to Assist AutoML ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
1(3): Conv2d(32, 32, ...)
2
3(4): Tanh()
(a) Original model1(5): Conv2d(32, 32, ...)
2(6): BatchNorm2d(32, ...)
3(7): ReLU()
(b) Transformed model
Figure 8: Example of activation layer constraint
4.4.4 Dropout layer constraint. We investigate the frequency of
dropout[ 48]layersandtheirdropratesusedintheminedmodel.
Out of 806 times that dropout is used in hidden layers, the drop
rate of 0.25 is used 385 times, which accounts for 47.8%. Out of 753
timesthatdropoutisusedinfullyconnectedlayers,thedroprate
of 0.5 is used 529 times, which accounts for 70.3%. Thus, we add
dropout layers with a drop rate of 0.25 and 0.5 to the hidden layers
and fully connected layers, respectively.
Example 5. According to Figure 5a, the original model does not
use the dropout layer. Therefore, in Figure 5b, following dropout layer
constraint,Manasaddsdropoutlayersintoaconvolutionallayerwith
0.25 drop rate and a dropout layer into the fully connected layer with
0.5 drop rate like the following example.
1Conv2d(32, 32, ...)
2ReLU()
3MaxPool2d(kernel=2, stride=2, ...)
4
(a) Original model1Conv2d(32, 32, ...)
2ReLU()
3MaxPool2d(kernel=2, stride=2, ...)
4Dropout2d(p=0.5)
(b) Transformed model
Figure 9: Example of dropout layer constraint
Allthetransformationsexceptdropouttransformationareap-
pliedsimultaneouslytotheminedmodelsbeforesearching.After
the best model is selected from candidate models, dropout trans-
formation is applied to the best model. Since adding dropout does
notalwaysimprovethe model‚Äôserrors,weutilizeNAStoidentify
whether using dropout is good or bad.
5 EVALUATION
5.1 Experimental Setup
We implement Manasby extending Auto-Keras [31]. All experi-
ments use Python 3.6, with 16GB GPU Tesla V100. In these ex-
periments, we use 8 datasets for image classification and image
regression,whichareobtainedfrom Kagglebasedonthevotecount.
The efficiency and effectiveness of Manasare evaluated in three as-
pects.Firstly,weevaluatethemetricvaluesincludingerrorrateand
MSE that is lower the better, m odel complexity, and training speed
ofManasby comparing it with Auto-Keras . Secondly, we evaluate
the model matching approach‚Äôs efficiency and effectiveness. Lastly,
weevaluatetheefficiencyandeffectivenessofmodeltransforma-
tion and training adaptation approaches. In these comparisons,
Manasincludemodelmatching,modeltransformation,andNAS.
ThealgorithmNASusedby Manasexploresthesearchspacevia
morphing the neural architectures guided by the Bayesian opti-
mizationalgorithm.Miningthemodelsisthemosttime-consuming
task, which took about 24 hours to complete mining all the models.
However, we only do this one time, so we do not count the time to
mine the model in the comparison of ManasandAuto-Keras .5.2 Mined models
Since many models for different Kaggledatasets have already been
published on GitHub, it is possible that some Kagglemodels of the
testingdatasethavealreadybeenincludedinourminedmodels.To
avoid this problem, for each dataset, we have examined all GitHub
repositoriesofselectedinitialmodelsofeachdataset.Thereisnoin-
formationshowingthatthoseinitialmodelsarespecificallycreated
for the input dataset. Moreover, for each input dataset, we have
compared the initialmodel of Manaswith all modelsfrom Kaggle.
Manas‚Äôinitialmodelisnotoneof Kaggle‚Äôsmodels.Moreover,since
mostofthemodelsfrom KagglearepublishedasJupyterNotebooks,
we only mine the model written as Python files.
Weonlycollectmodelsfromthetop10,000repositorieswiththe
moststarcounttoensurehigh-qualitymodels.Wealsodosanity
checksbyremovingincompletemodelsandduplicatemodels.More-
over, we evaluate the mined models by training them on MNIST
with 50 epochs [ 44]. The average accuracy of the mined models on
MNIST is 90.98%.
5.3 Datasets
To evaluate the performance of our method, we use 8different
imagedatasetscollectedfrom Kaggle:BloodCell [39],BreastCan-
cer[40],Flower[37],IntelImageClassification (IIC)[5],Malaria[3],
MNIST: Ham [36],Sign Language Digits (SD) [38], andSign Lan-
guage(SL) [51]. Our goal was to utilize more complex datasets
compared to well-known datasets such as MNIST, CIFAR10, and
FASHION. Most of our datasets have large sizes of images. For
example,intheintelimageclassification[ 5]dataset,theimagesize
is 150x150 while the image size of MNIST is only 28x28. Secondly,
thenumberofimagesinourdatasetsismuchlargerthanMNIST,
CIFAR10,andFASHION.Forinstance,thebreastcancerdataset[ 40]
has 277,524 images. Lastly, the number of classification of the eval-
uateddatasetsisupto10classes.Thesedatasetareoftenusedfor
image classification task; however, we treat prediction targets as
numericalvaluesforimageregressiontask.Forexample,wewill
treatthepredictiontargetsoftheMNISTdatasetasintegersranging
from0to9asnumericalvaluestobedirectlyusedastheregression
targets. We only use image datasets because Auto-Keras only apply
NASforimagedata.wedivideitintotwosubsets,80%ofrandomly
selected images are used for training and the remaining images for
validation.
5.4 Results
5.4.1 RQ1: How efficient is Manas? To evaluate the efficiency of
Manas, we run both ManasandAuto-Keras on 8 datasets for image
classificationandimageregression.Wevarythesearchtimefrom2
hours to 20 hours, which are described in Figure 10. Table 1 shows
theerrorrate,MSE,thedepth,thenumberofparameters,andthe
training speed of the best models of ManasandAuto-Keras . By the
bestmodel,wemeanonethathasthelowesterrorrateorMSEafter
search timeout. We run both Manas and Auto-Keras 5 times for
each dataset with random training and validation sets. In the table,
we report the information of the lowest errors of Manas and Auto-
Keras [15]. By comparing ManasandAuto-Keras , two conclusions
can be drawn.
1374ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan
Table 1: ManasClassification & Regression Results
DataImage Classification Image Regression
Error rate
(%)Depth
(layers)Param #
(million)Speed
(epoch/min)MSEDepth
(layers)Param #
(million)Speed
(epoch/min)
AK MN AKMN AKMN AK MN AK MN AKMN AK MN AK MN
Blood Cell 18.910.0 (‚Üì47.1%) 76(‚Üì14.3%)2.30.3 (‚Üì87.0%)5.18.0 (‚Üë56.9%)0.160.11 (‚Üì31.3 %) 219(‚Üì57.1%)11.20.1 (‚Üì99.1%)2.78.4 (‚Üë211.1%)
Breast Cancer 6.96.9 (‚Üì0.0%)218(‚Üì61.9%)11.20.5 (‚Üì95.5%)0.20.5 (‚Üë150%)0.050.06 (‚Üë20.0%)267(‚Üì73.1%)11.40.04 (‚Üì99.6%)0.20.6 (‚Üë200.0%)
Flower 14.612.8 (‚Üì12.3%)12111 (‚Üì90.9%)7.01.5 (‚Üë78.7%)2.73.8 (‚Üë40.7%)0.640.65 (‚Üì1.5%)219(‚Üì57.1%)11.20.4 (‚Üì96.4%)3.34.3 (‚Üë30.3%)
IIC 8.98.6 (‚Üì3.4%)277(‚Üì74.1%)15.50.5 (‚Üì96.8%)0.72.6 (‚Üë73.1%)0.580.59 (‚Üë1.7%)1225(‚Üì95.9%)7.00.3 (‚Üì95.7%)0.72.4 (‚Üë242.9%)
Malaria 2.31.6 (‚Üì30.4%)307(‚Üì76.7%)20.30.3 (‚Üì98.5%)1.74.3 (‚Üë152.9%)0.020.02 (‚Üì0%)218(‚Üì61.9%)11.20.4 (‚Üì96.4%)2.44.7 (‚Üë95.8%)
MNIST: Ham 20.620.3 (‚Üì1.5%)76(‚Üì14.3%)0.91.6 (‚Üë77.8%)2.21.4 (‚Üì36.4%)0.810.73 (‚Üì9.9%)2613 (‚Üì50.0%)11.90.4 (‚Üì96.6%)1.15.2 (‚Üë372.7%)
SD 0.50.0 (‚Üì100.0%) 2211 (‚Üì50.0%)11.30.7 (‚Üì93.8%)8.966.0 (‚Üë641.6%)0.050.1 (‚Üë100.0%) 2112 (‚Üì42.9%)11.20.4 (‚Üì96.4%)48.478.2 (‚Üë61.6%)
SL 0.00.0 (‚Üì0.0%)226(‚Üì72.7%)11.30.3 (‚Üì97.3%)5.58.0 (‚Üë45.5%) 00(‚Üì0.0%)2710 (‚Üì63.0%)12.67.2 (‚Üì42.90%) 5.02.6 (‚Üì48.0%)
Avg 9.17.5 (‚Üì17.6%)32.17.8 (‚Üì75.7%)10.00.7 (‚Üì93.0%)3.411.8 (‚Üë247.1%) 0.30.3 (‚Üì0.0%)35.69.1 (‚Üì74.4%)11.01.2 (‚Üì89.1%)8.013.3 (‚Üë66.3%)
In the table, Avg, AK and MN represent average, Auto-Keras , andManas, respectively. In each cell, ‚Üìand‚Üërepresent a decrease percentage and an increase percentage, respectively.
In each evaluation metric, the bold value show the best percentage change of Manascompared to Auto-Keras .
Firstofall,Table1showsthat Manasproducesmodels,which
has lower error rate or MSE compared with Auto-Keras ‚Äô models.
Thelowererrorsof Manascomparedwith Auto-Keras indicatesthat
using mined models as the starting points for NAS can produce
bettermodels.Notably, Manasoutperforms Auto-Keras byachieving
17.6%lower error rate on average. For some problems like IIC
andMNIST: HAM for image classification task, the decrease of the
errorrate of Manascompared with Auto-Keras is smallbecause the
models created by ManasandAuto-Keras have reached the limit
of error rate. Therefore, a small decrease in error rate is a major
improvement. We use the error rate as the main evaluation metric
to clearly point out this improvement of ManastoAuto-Keras .
Secondly, Manasachieves lower errors with less complicated
models compared to Auto-Keras . Using mined models on NAS sig-
nificantly decreases the complexity of the produced models. On
average,modelsgeneratedby Manasare75.7%lessdeepand 93.0%
less wide compared to the models generated by Auto-Keras for im-
age classification task. Similarly, Manascreates models with 74.4%
less deepand 89.1%less widecompare to Auto-Keras ‚Äôs modelsfor
imageregressiontask.SimplerDNNmodelstrainfasterandsave
more energy [ 27] than the complex ones. Han et al.have shown
thatreducingthenumberofparametersofdeeplearningmodels
canreducethetrainingtimeby3 √óto4√ó,andenergycomsumption
by 3√óto 7√ó[24]. Table 1 also shows that on average, Manas‚Äôm od -
els run faster than Auto-Keras ‚Äô model247.1%and66.3%in image
classification and image regression, respectively.
5.4.2 RQ2:Howefficientaremodeltransformationandoptimizers
modification? We create an ablation study to observe the efficiency
of the model transformation and modifying optimizers via mining
separately.
‚Ä¢OriginalManas (OM)representsminedmodels+notrans-
formation + no optimizer + NAS.
‚Ä¢TransformedManas (TM)representsminedmodels+ trans-
formations + no optimizer + NAS.
‚Ä¢Manas(MN) represents for mined models + transformations
+ optimizers + NAS.
‚Ä¢Auto-Keras (AK) represents NAS.
Weobservetheerrorvaluesof OriginalManas ,TransformedManas ,
Manas,andAuto-Keras byexecutingthemon8datasetsfor20hours
for both image classification and image regression. To evaluate
the efficiency of the model transformation, we compare Original
ManasandTransformedManas .Wecompare TransformedManasandManastoevaluatetheefficiencyofoptimizersmodification.To
evaluatethecombinationofallmethods,wecompare Manasand
Auto-Keras .
Notice that we have applied the model transformation on Auto-
Keras‚Äô models; however, this did not lead to any improvement
becausethose defaultmodelsuse BNlayersor Dropoutlayersap-
propriately. Figure 10b does not have the Original Manas series
because the best models of Original Manas andTransformed Manas
are the same. In other words, the model already contains all the
transformationconstraints,sothereisnotransformationapplied
tothe model. Similarly,Figure 10edoesnothave the Transformed
Manasseries because the optimizers of the best model of Trans-
formedManas andManasarethesame.Theselected model‚Äôsopti-
mizer of this problem is not available; therefore, we use the default
optimizer of Auto-Keras for this model. From the results are shown
in Figure 10, we draw four observations.
First of all, model transformation increases the performance
ofManasin terms of errors and converge time. Figures 10a, 10c,
10f,10gshowthat TransformedManas achievelowererrorsthan
Original Manas most of the time. The activation layers, the GAP
layer, and the dropout layers contribute to the better errors of
Transformed Manas compared to Original Manas . For example, the
dropout layers can prevent overfitting, which decreases the errors.
The model transformation also helps Manasto converge faster.
As we can observe Figures 10a, 10c, 10f, 10g, Original Manas has
higher errors compared to Transformed Manas in the first 2hours
of training, which indicate the Transformed Manas converge faster
thanOriginalManas .TransformedManas hasafastconv ergespeed
thanks to BN.
Secondly,minedoptimizershelp Manastoreduceerrorsandcon-
vergefaster.Inmostofproblems,thebestmodelsof Manashave
lowererrorsthanthebestmodelsof TransformedManas .Moreover,
Manascanconvergeeasierwiththeminedoptimizers.Forinstance,
theFigures10d,10f,10gshowsthat TransformedManas hastrou-
ble converging. After 8 hours of searching, Transformed Manas
cannot find out better models while Manassucceeds. Transformed
Manashas trouble in converging since it does not have an suitable
optimizer for those problems like Manas.
Thirdly,Auto-Keras mayhavelowererrorsthan Manasinthefirst
few hours; however, in the last hours, Manasoften finds out better
models than Auto-Keras . For example, Figures 10b, 10c, 10d, and
10mshowthat Auto-Keras hastheerrorsatthebeginning;however,
1375Manas: Mining Software Repositories to Assist AutoML ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
(a) Blood Cell Classification (b) Breast Cancer Classification (c) Flower Classification (d) Malaria Classification
(e) MNIST: Ham Classification (f) IIC Classification (g) SD Classification (h) SL Classification
(i) Blood Cell Regression (j) Breast Cancer Regression (k) Flower Regression (l) Malaria Regression
(m) MNIST: Ham Regression (n) IIC Regression (o) SD Regression (p) SL Regression
Figure 10: Error Rate and MSE of Auto-Keras ,Original Manas ,Transformed Manas , and Manasover time
as time gone we have not seen any improvement in Auto-Keras
results.Thereasonfor thisproblemisthat Auto-Keras startswith
very complicatedmodels that take alot of time totrain; therefore,
thenumber ofmodelsis searchedby Auto-Keras aresmall, which
decreasesthechancetofindoutgoodmodelsof Auto-Keras .Manas
starts with simple models, which trains faster. Therefore, Manas
maynotbeofftoagoodstart,butintheend,itstilloutperforms
Auto-Keras . The observation shows the benefit of using simple
mined models in NAS.Lastly,Figure10indicatesthat Manasobtainslowererrorrates
thanAuto-Keras inalmostdifferenttimeperiods.Aswecanobserve
in Figures 10a and 10i, Manasalways outperforms Auto-Keras in
termsoferrorrateinthe20-hoursofsearching.Figure10showsthat
thelongersearchingtimesometimesgivesworsemodels.During
the searching process, the NAS estimates the errors of searched
models and selects the best one. However, the estimation may not
beaccurate,leadingto anincorrectchoice.Thisproblem ofNAS
indicatestheimportanceofusingasimplemodelasastartingpoint.
SimplemodelscantrainfasterthatincreasesthechanceforNAS
1376ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan
Table 2: Efficiency of Model Matching
DataBlood
CellBreast
CancerFlower IICMala-
riaHamSDSL
TaskICIMICIMICIMICIMICIMICIMICIMICIM
Total 793
MC555718611465761169455
MF-------12----12---
Ineachcell,IC,IM,MM,andMFrepresentimageclassification,imageregression,modelclustering
and model filtering, respectively. The unit of all the data in the table is the number of models.
Table 3: ManasvsAuto-Keras for Well-known Problems
DataError Rate
(%)Depth
(layers)Param #
(million)Speed
(epoch/min)
AK MNAK MNAK MNAK MN
CIFAR10 77.7 (‚Üë10.0%)2110 (‚Üì52.4%)19.40.8 (‚Üì95.9%) 12.9 (‚Üë190.0%)
Fashion 5.211(‚Üë111.5%) 229(‚Üì59.1%)14.71.8 (‚Üì87.8%)2.11.8 (‚Üì14.3%)
MNIST 0.50.7 (‚Üë40.0%)236(‚Üì73.9%)19.32.9 (‚Üì85.0%)1.33.1 (‚Üë138.5%)
Avg4.26.5 (‚Üë54.8%)228.3 (‚Üì62.3%)17.81.8 (‚Üì89.9%)1.52.6 (‚Üë73.3%)
In each cell, Avg, AK and MN represent average, Auto-Keras , andManas, respectively.
to search for more models. Since the estimation of NAS can be
incorrect;thus,ifweincreasethenumberofsearchedmodel,we
can increase the chance to obtain better models. For example, after
8-hours of searching, NAS goes wrong with IICproblem when
it produces worse models than before. However, when we keep
searching for new models, NAS gradually fixes the problem to
obtains a good model.
5.4.3 RQ3: How efficient is model matching? The goal of model
matching is not only selecting good default models for Manasbut
also reducing the number of default models. From Table 1, we
observe the efficiency of model matching, when Manascan outper-
formAuto-Keras inmanydifferentperspectives.Table2showsthat
model matching, including model clustering and model filtering,
cansignificantlyreducethenumberofdefaultmodelsfor Manas.To
balancethetimethat Manasspendsbetweeninitialarchitectures
and NAS, we applied the models filtering when the number of
initial neural networks is larger than 40 since Manasoften takes
more than 10 hours, half of the amount of time used in our evalua-
tions to complete training these neural networks. We use all 793
mined models as input for each problem, which may take few days
to complete training. By using model matching, we decrease the
number ofdefault models by99% on average.Taking SDin image
classification as an example, after using DNN clustering, there are
remaining69defaultmodels.Therefore,weuseDNNfilteringon
this dataset to reduce the number of models of SDfrom 69 models
to 12 models, which eliminates 82.6% number of the DNN model.
5.4.4 RQ4: How efficient is Manas for well-known problems? We
alsoevaluate Manaswiththewell-knowndatasetslikeFASHION,
MNIST,orCIFAR10fortheimageclassificationtask.Table3shows
the error rate,depth, number of parameters, ands peedofthebest
modelsof ManasandAuto-Keras .As canbeobserved, Auto-Keras
achievebettererrorratesonthesedatasetsthan Manas; however,
Auto-Keras produces larger models to achieve these error rates
whileManasuses much smaller models to get close to the error
rates ofAuto-Keras in CIFAR10 and MNIST problems. Particularly,Manas‚Äômodelsare62.3%shorterand89.9%than Auto-Keras ‚Äômodels
onaverage,whichincreasesthetraining speedofManas‚Äômodels
by 73.3% compared to Auto-Keras ‚Äô models. Auto-Keras achieves
better error rates than ManassinceAuto-Keras uses ResNet and
DenseNetas initialmodels. Theseneural networksare well-tuned
to achieve outstanding results on these datasets, which once again
shows that using good initial architecture optimizes NAS.
6 LIMITATIONS AND THREATS TO VALIDITY
6.1 Limitations
Inthiswork, Manasdirectlyderivesthedatacharacteristicsfrom
the imagedataset, whichis limitedto theimage classificationand
imageregressionproblems.Webelievethat Manasisnotdirectlyap-
plied to other problems such as natural language processing (NLP)
or video classification; however, our approach of mining models
to identify a good starting point candidate should be applicable to
any AutoML problems. For example, data characteristics of NLP
problemsalsoincludetheinputshape,whichareinputtimesteps
and the number of features, and the total number of output classes.
Timestepsrepresentthemaximumlengthoftheinputsequence,
which could either be the number of words or the number of char-
acters dependingon what we want.The number of featuresis the
numberofdimensionswefeedateachtimestep.Wecananalyze
theinputlayerandoutputlayertoextractdatacharacteristicsfrom
models. We can analyze the input dataset to obtain its data char-
acteristics. The extracted data characteristics can be used to find
better starting points for NLP problems in AutoML systems.
Manascan only work with few kinds of layers since it only uses
thelayersthat Auto-Keras supports.Thislimitationcandecreasethe
performanceof Manasbecauseif Auto-Keras supportedmorelayers,
we can mine more kinds of models from the software repositories.
6.2 Internal Validity
Wehavetriedourbesttoobtaintheresultsof ManasandAuto-Keras
onasmanydatasetsaspossible.Becauseofthetimelimit, Manas
is currently evaluated on 8 datasets. All the source code, trained
models,datasets,andevaluationdataarepublicforreproduction
to mitigate these threats.
6.3 External Validity
First of all, Manasonly focuses on image classification/regression
problems. We rely on meta-features to find good starting points
forNAS;therefore,onepossiblethreatisthatmeta-features(data
characteristics) do not work for other types of problems. However,
Auto-Sklearn [ 20] and Auto-Sklearn 2.0 [ 19] mitigate this threat
by showing that using meta-features can increase the performance
of AutoML systems in terms of training speed and accuracy on
structureddatasets.Secondly, ManasonlyfocusesonCNN.Thus,
another threat is that the model transformation approach does not
workforothertypesofmodels.Nevertheless,Cambronero etal.[10]
propose AMS showing that using unspecified complementary and
functionally related API components can improve the performance
of AutoML systems for classical models such as Linear Regression
or Random Forest. The difference between Manasand AMS is that
AMS applies these transformations to search space while Manas
applies these transformations to default models.
1377Manas: Mining Software Repositories to Assist AutoML ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
7 RELATED WORK
7.1 Neural architecture search
NAS is a technique for automatically finding appropriate neural
architectures which can outperform most of the hand-designed
neuralnetworks.Specifically,NASneedsathetrainingdatasetas
the input to create a powerful neural architecture. There are many
differentapproachesforaNASsystem; however,most ofthemhave
three main components which are search space, search strategy,
and optimization strategy. The search space represents the search
boundaryofaNASsystemlimitingwhatkindsofneuralnetwork
canbesearchedandoptimized.Forinstance,Baker etal.[4]usethe
convolutional architecture with pooling, linear transformations as
asearchspace.Aroundthesametime,Zoph etal.[61]useasimilar
searchspace;however,theauthorsusemoreskipconnectionforthe
searchspace.Thesearchstrategyisusedtosearchappropriatemod-
els in a defined search space. There are many approaches to search
models such as reinforcement learning [ 4,9,59,61,62] or evolu-
tionaryalgorithms[ 46,49,58].Thisoptimizationstrategysupports
NAStoguidethenetworksearchprocess.Theoptimizationstrat-
egyevaluatesasearchedmodelwithtrainingdatawithouttraining
these models. Recently, many methods are proposed to optimize
NAS [32,41,45]. In our work, Manasmines the neural networks
from repositories to enhance the power of NAS by supporting it to
have a better starting point.
7.2 AutoML
AutoMLisaprocessforconstructinganappropriatemodelarchi-
tecture for a specific problem automatically. Many features that
AutoMLcanprovidetodeeplearningusers,suchasautomateddata
augmentation, automated hyperparameter tuning, or automated
modelselection.AlotofAutoMLsystemshavebeencreatedlike
Auto-WEKA[ 52]ontopofWEKA[ 22,56],Auto-Sklearn[ 20]on
topofScikit-learn,whichsupportdeeplearningusertoautomate
tuninghyperparameterandmodelselection.SomeotherAutoML
systemscansupportdeeplearninguserstoautomateoptimizingthe
fullMLpipeline.Forinstance,TPOT[ 42,43]usesevolutionarypro-
grammingtooptimizeMLsoftware. However, amaindisadvantage
of these systems are very slow because of the high GPU compu-
tationrequirement.Recently, Auto-Keras iscreatedtohandlethis
problem, which has implemented network morphism [ 12,55]t or e -
duce the searching time of NAS. Network morphism is a technique
tomorphaneuralarchitecturewithoutchangingitsfunctionality.
Nevertheless, even though Auto-Keras apply network morphism
technique, it still takes a lot of GPU computation. Our approach
uses DNN model mining and common layer patterns to enhance
the performance of AutoML system.
7.3 Mining software repositories
Cambronero et al.[11] proposed AL, a system that leverages ex-
isting machinelearning code from repositoriesto synthesize final
pipelines. ALcan generate MLpipelines fora wide rangeof prob-
lems without any manual selection. Cambronero et al.[10] also
proposedAMS,whichautomatedgeneratesnewsearchspacefor
AutoML systems by utilizing source code repositories. The newsearch space is created based on an input ML pipeline, which in-
creases the performance of AutoML systems. However, these only
operate classical machine learning models,whereas Manasworks
with neural networks.
7.4 Meta-features
Auto-Sklearn[ 20]uses38meta-featuresofstructureddatasetsto
findabetterstartingpoint.Feurer etal.[19]proposesAuto-Sklearn
2.0, which reduces the number of meta-features to three, including
the number of data points, the number of features, and the number
ofclasses.Thereasonsforthereductionarethatgoodmeta-features
are time-consuming and memory-consuming to generate. We also
donotknowwhichmeta-featuresworkbestforwhichproblem.Un-
like Auto-Sklearn and Auto-Sklearn 2.0, Manasuses meta-features
to find a better starting point for NAS, which works for neural
networks.Moreover, Manasalsoproposesthemeta-features,which
helps NAS find a better starting point for image datasets.
8 CONCLUSION
We present Manas, a technique for NAS, which uses the mining
technique to assist NAS. The key idea of Manasis to mine mod-
elsfromrepositories toenhanceNAS.Inparticular,weuseCNN
modelsminedfromsoftwarerepositoriesasthedefaultmodelof
NAS. From a large number of models, we use the model matching
approachtofindgoodmodelsforaproblem.Wealsoapplysome
transformationsforthosemodelstoenhancetheirperformances.
Withbetterdefaultmodels, ManascanincreaseNAS‚Äôsperformance,
whichleadstobetterCNNmodelsassearchresults.Ourexperiment
shows that Manascan produce better CNN models in terms of the
errorrateandMSE,themodel complexity,andthetraining speed
thanAuto-Keras .Futureworkwillinvolveextending Manastoprob-
lemsbeyondthosetackledinthispaper,suchasvideoclassification.
We can utilize the code change patterns [ 16] in ML programs to
improve the results. Moreover, the proposed approach of Manas
can be applied to other automated tools, e.g., AutoAugment [ 14]o f
other components in ML pipeline [ 7]. The technique can also be
utilized to useAutoML to address theother problems in ML,such
as fairness bug [6].
ACKNOWLEDGMENTS
This workwas supportedin partby USNSF grantsCNS-21-20448,
CCF-19-34884, and Facebook Probability and Programming Award
(809725759507969).Wealsothankthereviewersfortheirinsightful
feedback for improving the paper.
REFERENCES
[1]Anonymized.2015. Resnetnetworkdoesn‚Äôtworkasexpected . https://stackoverflow.
com/questions/49226447/resnet-network-doesnt-work-as-expected
[2] Anonymized. 2021. Keras documentation. https://keras.io/api/
[3]Arunava.2018. MalariaCellImagesDataset. https://www.kaggle.com/iarunava/
cell-images-for-detecting-malaria
[4]Bowen Baker, OtkristGupta, NikhilNaik,and RameshRaskar.2017. Designing
NeuralNetworkArchitecturesusingReinforcementLearning.In 5thInternational
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings . OpenReview.net. https://openreview.net/
forum?id=S1c2cvqee
[5]Puneet Bansal. 2018. Intel Image Classification . https://www.kaggle.com/
puneet6060/intel-image-classification
[6]Sumon Biswas and Hridesh Rajan. 2020. Do the Machine Learning Models on a
CrowdSourcedPlatformExhibitBias?AnEmpiricalStudyonModelFairness.In
1378ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan
ESEC/FSE‚Äô2020:The28thACMJointEuropeanSoftwareEngineeringConferenceand
Symposium on the Foundations of Software Engineering (Sacramento, California,
United States).
[7]Sumon Biswas, Mohammad Wardat, and Hridesh Rajan. 2022. The Art and Prac-
tice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines
In Theory, In-The-Small, and In-The-Large. In ICSE‚Äô22: The 44th International
Conference on Software Engineering (Pittsburgh, PA, USA).
[8]Hudson Borges, Andre Hora, and Marco Tulio Valente. 2016. Understanding
the factors that impact the popularity of GitHub repositories. In 2016 IEEE In-
ternational Conference on Software Maintenance and Evolution (ICSME) . IEEE,
334‚Äì344.
[9]Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Efficient
architecturesearchbynetworktransformation.In Thirty-SecondAAAIConference
on Artificial Intelligence .
[10]Jos√©PCambronero,J√ºrgenCito,andMartinCRinard.2020. Ams:Generating
automlsearchspacesfromweakspecifications.In Proceedingsofthe28thACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
the Foundations of Software Engineering . 763‚Äì774.
[11]Jos√©PCambroneroandMartinCRinard.2019. AL:autogeneratingsupervised
learning programs. Proceedings of the ACM on Programming Languages 3, OOP-
SLA (2019), 1‚Äì28.
[12]TianqiChen,IanJ.Goodfellow,andJonathonShlens.2016. Net2Net:Accelerating
Learning via Knowledge Transfer. In 4th International Conference on Learning
Representations,ICLR2016,SanJuan,PuertoRico,May2-4,2016,ConferenceTrack
Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.
05641
[13]Yukang Chen, Tong Yang, Xiangyu Zhang, GAOFENG MENG, Xinyu Xiao,
and Jian Sun. 2019. DetNAS: Backbone Search for Object Detection. In Ad-
vances in Neural Information Processing Systems , H. Wallach, H. Larochelle,
A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.
Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
228b25587479f2fc7570428e8bcbabdc-Paper.pdf
[14]Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le.
2019. Autoaugment: Learning augmentation strategies from data. In Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition .113‚Äì123.
[15]JiequanCui,PengguangChen,RuiyuLi,ShuLiu,XiaoyongShen,andJiayaJia.
2019. Fastandpracticalneuralarchitecturesearch.In ProceedingsoftheIEEE/CVF
International Conference on Computer Vision . 6509‚Äì6518.
[16]MalindaDilhara,AmeyaKetkar,NikhithSannidhi,andDannyDig.2022. Discov-
eringRepetitiveCodeChangesinPythonMLSystems.In InternationalConfer-
enceonSoftwareEngineering (Pittsburgh,UnitedStates) (ICSE‚Äô22) .ACM/IEEE.
https://doi.org/10.1145/3510003.3510225 To appear.
[17]ThomasElsken,JanHendrikMetzen,andFrankHutter.2018. Simpleandefficient
architecturesearchforConvolutionalNeuralNetworks. https://openreview.net/
forum?id=SySaJ0xCZ
[18]Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019. Efficient Multi-
Objective Neural Architecture Search via Lamarckian Evolution. In Interna-
tionalConferenceonLearningRepresentations . https://openreview.net/forum?id=
ByME42AqK7
[19]MatthiasFeurer,KatharinaEggensperger,StefanFalkner,MariusLindauer,and
Frank Hutter. 2020. Auto-sklearn 2.0: The next generation. arXiv preprint
arXiv:2007.04074 (2020).
[20]MatthiasFeurer,AaronKlein,KatharinaEggensperger,JostSpringenberg,Manuel
Blum, and Frank Hutter. 2015. Efficient and robust automated machine learning.
InAdvances in neural information processing systems . 2962‚Äì2970.
[21]Giang Nguyen, Md Johirul Islam, Rangeet Pan, and Hridesh Rajan. 2021. Manas
artifact. https://github.com/giangnm58/Manas
[22]Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann,
and Ian H Witten. 2009. The WEKA data mining software: an update. ACM
SIGKDD explorations newsletter 11, 1 (2009), 10‚Äì18.
[23]Greg Hamerly and Charles Elkan. 2004. Learning the k in k-means. In Advances
in neural information processing systems . 281‚Äì288.
[24]Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Compress-
ing Deep Neural Network with Pruning, Trained Quantization and Huffman
Coding.In 4thInternationalConferenceonLearningRepresentations,ICLR2016,
SanJuan,PuertoRico,May2-4,2016,ConferenceTrackProceedings ,YoshuaBengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1510.00149
[25]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition . 770‚Äì778.
[26]Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
2017. Densely connected convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition . 4700‚Äì4708.
[27]ForrestNIandola,MatthewWMoskewicz,KhalidAshraf,andKurtKeutzer.2016.
Firecaffe:near-linearaccelerationofdeepneuralnetworktrainingoncompute
clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition . 2592‚Äì2600.[28]SergeyIoffeandChristianSzegedy.2015. Batchnormalization:Acceleratingdeep
network training by reducing internal covariate shift. In International conference
on machine learning . PMLR, 448‚Äì456.
[29]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
Comprehensive Study on Deep Learning Bug Characteristics. In ESEC/FSE‚Äô19:
The ACM Joint European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE) (ESEC/FSE 2019) .
[30]Md Johirul Islam, Rangeet Pan, Giang Nguyen, and Hridesh Rajan. 2020. Repair-
ing Deep Neural Networks: Fix Patterns and Challenges. In ICSE‚Äô20: The 42nd
International Conference on Software Engineering (Seoul, South Korea).
[31]Haifeng Jin, Qingquan Song, and Xia Hu. 2019. Auto-Keras: An Efficient Neural
ArchitectureSearchSystem.In Proceedingsofthe25thACMSIGKDDInternational
Conference on Knowledge Discovery & Data Mining . ACM, 1946‚Äì1956.
[32]Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos,
and EricP Xing. 2018. NeuralArchitectureSearch withBayesian Optimisation
and Optimal Transport. In Advances in Neural Information Processing Systems ,
S.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett
(Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2018/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf
[33]AlexKrizhevsky,GeoffreyHinton,etal .2009.Learningmultiplelayersoffeatures
from tiny images . Technical Report. Citeseer.
[34]Yann LeCun, L√©on Bottou, Yoshua Bengio, Patrick Haffner, et al .1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278‚Äì
2324.
[35]Min Lin, Qiang Chen, and Shuicheng Yan. 2014. Network In Network. In 2nd
InternationalConferenceonLearningRepresentations,ICLR2014,Banff,AB,Canada,
April 14-16, 2014, Conference Track Proceedings , Yoshua Bengio and Yann LeCun
(Eds.). http://arxiv.org/abs/1312.4400
[36]Kevin Mader. 2018. Skin Cancer MNIST: HAM10000. https://www.kaggle.com/
kmader/skin-cancer-mnist-ham10000
[37]Alexander Mamaev. 2017. Flowers Recognition. https://www.kaggle.com/
alxmamaev/flowers-recognition
[38]Arda Mavi. 2017. Sign Language Digits Dataset. https://www.kaggle.com/
ardamavi/sign-language-digits-dataset
[39]Paul Mooney. 2017. Blood Cell Images. https://www.kaggle.com/
paultimothymooney/blood-cells
[40]Paul Mooney. 2017. Breast Histopathology Images . https://www.kaggle.com/
paultimothymooney/breast-histopathology-images
[41]Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, and Lihi Zel-
nik. 2019. XNAS: Neural Architecture Search with Expert Advice. In Ad-
vances in Neural Information Processing Systems , H. Wallach, H. Larochelle,
A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.
Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
00e26af6ac3b1c1c49d7c3d79c60d000-Paper.pdf
[42]RandalSOlson,NathanBartley,RyanJUrbanowicz,andJasonHMoore.2016.
Evaluationofatree-basedpipelineoptimizationtoolforautomatingdatascience.
InProceedingsoftheGeneticandEvolutionaryComputationConference2016 .ACM,
485‚Äì492.
[43]RandalSOlson,RyanJUrbanowicz,PeterCAndrews,NicoleALavender,JasonH
Moore, et al .2016. Automating biomedical data science through tree-based
pipeline optimization. In European Conference on the Applications of Evolutionary
Computation . Springer, 123‚Äì137.
[44]Rangeet Pan and Hridesh Rajan. 2022. Decomposing Convolutional Neural Net-
works into Reusable and Replaceable Modules. In ICSE‚Äô22: The 44th International
Conference on Software Engineering (Pittsburgh, PA, USA).
[45]JunranPeng,MingSun,ZHAO-XIANGZHANG,TieniuTan,andJunjieYan.2019.
EfficientNeuralArchitectureTransformationSearchinChannel-LevelforObject
Detection. In Advances in Neural Information Processing Systems , H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox, and R. Garnett (Eds.),
Vol. 32.CurranAssociates,Inc. https://proceedings.neurips.cc/paper/2019/file/
3aaa3db6a8983226601cac5dde15a26b-Paper.pdf
[46]EstebanReal, Sherry Moore,Andrew Selle,Saurabh Saxena,YutakaLeonSue-
matsu,JieTan,QuocVLe,andAlexeyKurakin.2017. Large-scaleevolutionof
image classifiers. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70 . JMLR. org, 2902‚Äì2911.
[47]Adrian Rosebrock. 2019. Auto-Keras and AutoML: A Getting Started
Guide. https://www.pyimagesearch.com/2019/01/07/auto-keras-and-automl-a-
getting-started-guide
[48]Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov.2014. Dropout:asimplewaytopreventneuralnetworksfrom
overfitting. The journal of machine learning research 15, 1 (2014), 1929‚Äì1958.
[49]Masanori Suganuma, Shinichi Shirakawa, and Tomoharu Nagao. 2017. A genetic
programmingapproachtodesigningconvolutionalneuralnetworkarchitectures.
InProceedings of the Genetic and Evolutionary Computation Conference . ACM,
497‚Äì504.
[50]Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for
convolutional neural networks. In International Conference on Machine Learning .
PMLR, 6105‚Äì6114.
1379Manas: Mining Software Repositories to Assist AutoML ICSE ‚Äô22, May 21‚Äì29, 2022, Pittsburgh, PA, USA
[51]Tecperson.2017. SignLanguageMNIST . https://www.kaggle.com/datamunge/
sign-language-mnist
[52]ChrisThornton,FrankHutter,HolgerHHoos,andKevinLeyton-Brown.2013.
Auto-WEKA: Combined selection and hyperparameter optimization of classifica-
tion algorithms. In Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining . ACM, 847‚Äì855.
[53]Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2022. Deep-
Diagnosis:AutomaticallyDiagnosingFaultsandRecommendingActionableFixes
inDeepLearningPrograms.In ICSE‚Äô22:The44thInternationalConferenceonSoft-
ware Engineering (Pittsburgh, PA, USA).
[54]MohammadWardat,WeiLe,andHrideshRajan.2021. DeepLocalize:FaultLocal-
ization for Deep Neural Networks. In ICSE‚Äô21: The 43nd International Conference
on Software Engineering (Virtual Conference).
[55]Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. 2016. Network
morphism. In International Conference on Machine Learning . 564‚Äì572.
[56]IanHWitten,EibeFrank,MarkAHall,andChristopherJPal.2016. DataMining:
Practical machine learning tools and techniques . Morgan Kaufmann.
[57]Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprintarXiv:1708.07747 (2017).
[58]Lingxi Xie and Alan Yuille. 2017. Genetic cnn. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision . 1379‚Äì1388.
[59]Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. 2018. Practical
block-wise neural network architecture generation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition . 2423‚Äì2432.
[60]RunjieZhu,XinhuiTu,andJimmyXiangjiHuang.2020. Chapterseven-Deep
learning on information retrieval and its applications. In Deep Learning for
Data Analytics , Himansu Das, Chittaranjan Pradhan, and Nilanjan Dey (Eds.).
Academic Press, 125‚Äì153. https://doi.org/10.1016/B978-0-12-819764-6.00008-9
[61]BarretZophandQuocV.Le.2017.NeuralArchitectureSearchwithReinforcement
Learning. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon,France,April24-26, 2017,ConferenceTrackProceedings .OpenReview.net.
https://openreview.net/forum?id=r1Ue8Hcxg
[62]Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning
transferablearchitecturesforscalableimagerecognition.In Proceedingsofthe
IEEE conference on computer vision and pattern recognition . 8697‚Äì8710.
1380