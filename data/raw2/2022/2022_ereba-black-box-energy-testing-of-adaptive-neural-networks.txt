EREBA: Black-box Energy Testing of Adaptive Neural Networks
Mirazul Haqueâˆ—
mirazul.haque@utdallas.edu
The University of Texas at DallasYaswanth Yadlapalliâˆ—
yaswanth.yadlapalli@utdallas.edu
The University of Texas at Dallas
Wei Yang
wei.yang@utdallas.edu
The University of Texas at DallasCong Liu
cong@utdallas.edu
The University of Texas at Dallas
ABSTRACT
Recently,variousDeepNeuralNetwork(DNN)modelshavebeen
proposed for environments like embedded systems with stringent
energyconstraints.Thefundamentalproblemofdeterminingthero-
bustness of a DNN with respect to its energy consumption (energy
robustness)isrelativelyunexploredcomparedtoaccuracy-basedro-
bustness.ThisworkinvestigatestheenergyrobustnessofAdaptiveNeuralNetworks(AdNNs),atypeofenergy-savingDNNsproposed
for many energy-sensitive domains and have recently gained trac-
tion. We propose EREBA, the first black-box testing method for
determiningtheenergyrobustnessofanAdNN.EREBAexplores
and infers the relationship between inputs and the energy con-
sumptionofAdNNstogenerateenergysurgingsamples.Extensive
implementation and evaluation using three state-of-the-art AdNNs
demonstrate that test inputs generated by EREBA could degrade
the performance of the system substantially. The test inputs gener-
ated by EREBA can increase the energy consumption of AdNNs by
2,000% compared to the original inputs. Our results also show that
testinputsgeneratedviaEREBAarevaluableindetectingenergy
surging inputs.
CCS CONCEPTS
â€¢Security and privacy â†’Software and application security.
KEYWORDS
Green AI, AI Energy Testing, Adversarial Machine Learning
ACM Reference Format:
MirazulHaque,YaswanthYadlapalli,WeiYang,andCongLiu.2022.EREBA:
Black-box Energy Testing of Adaptive Neural Networks. In 44th Interna-
tional Conference on Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pitts-
burgh,PA,USA. ACM,NewYork,NY,USA,12pages.https://doi.org/10.1145/
3510003.3510088
âˆ—Both authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.35100881 INTRODUCTION
Recentlytherehasbeenaconsiderableamountofresearchinde-
veloping energy-saving DNN models to allow state-of-art DNNs
with high computational costs to be deployed in mobile and em-
beddedarchitecture. AdaptiveNeuralNetworks(AdNNs) [2,7,33]
are energy-saving DNN models that determine when to switch off
certainpartsofthenetworktoreducethenumberofcomputations.
BecauseanAdNNmodeldetermineswhichpartsoftheneural
network to run based on inputs, an adversaryâ€™s ability to surge
the energy consumption by carefully crafting inputs is a crucial
concern in energy-critical environments. For example, AdNNs like
BlockDrop[ 41]andSkipNet[ 38]canreducethecomputationsin
ResNet significantly and an alteration on the input can nullify a
largeportionofthereducedcomputations,invalidatingthemodelsâ€™
purpose. Such behavior would lead the app or software using an
AdNN model to consume energy erratically, resulting in devicesâ€™
power failure and disastrous consequences. Thus, there is a strong
needtoprovideasystematictestingmethodtofindenergyhotspots
inthemodelandfilteroutpotentialâ€œpower-surgingâ€inputsthat
will negatively impact the modelâ€™s performance.
Creating testing inputs to increase the energy consumption of a
DNNmodelischallengingbecauseinferringtherelationbetween
energyconsumptionandinputis achallengingtask.Unlikeinfer-
ring the relation between input and output, where we can find the
derivatives from a series of computation functions in the model,
energy consumptioncan onlybe measured byrunning themodel.
Traditional DNN testing methods [ 22,30,37,43] and traditional
adversarial attacks [ 4,10,29] on DNNs have been designed to cre-
ate carefully crafted synthetic testing inputs using the gradient of
generated output with respect to the input. However, for energy
testing,itisunclearwhetherachangeintheinputinducesanin-
crease or decrease in the energy consumption of the model. Tothe best of our knowledge, ILFO [
13] is the first work that seeks
to formulate all types of AdNNâ€™s energy robustness (Section 2.1)
problem by modeling the relation between input and intermediate
output [13] (DeepSloth [ 15] only evaluates energy robustness of
Early-terminationAdNNs).
However,ourinvestigations(Section3)showthatILFOgener-
ated energy surging samples lack traditional transferability, i.e.,,
the adversarial samples generated by ILFO for a target AdNN can-
not be applied to a new AdNN to increase its energy consumption.
Therefore, the traditional black-box accuracy testing method of
DNNs using surrogate model [ 5,21,28] can not be used for energy
robustnessevaluation.Therefore,ILFOgeneratedsamplescannot
evaluate the energy robustness of AdNNs in a black-box scenario.
8352022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu
This paper presents EREBA(EnergyRobustness using Estimator
BasedApproach)toperformenergytestingonAdNNsunderthe
black-box setting where there is no prior knowledge known about
the AdNN model. To our knowledge, this is the first attempt in
this direction. EREBAaims to evaluate the energy robustness of
AdNNandidentifyinputsthatwillnegativelyimpactthemodelâ€™s
performance.Specifically,wedeveloptwotestingmethodstoassess
anygivenAdNNmodelâ€™senergyrobustness,namelyInput-based
testing and Universal testing. Input-based testing evaluates energy
robustnesswheretestinginputsaresemanticallymeaningfultothe
AdNN(e.g., meaningfulimages,compilableprograms).Ontheother
hand, universal testing evaluates worst-case energy robustness
whereeachtestinginputmaximizestheenergyconsumptionfor
each target AdNN.
ForgeneratingtestinginputsforAdNNsinablack-boxsetting,it
isneededtofindarelationbetweeninputandenergyconsumption
of AdNNs. Based on the working mechanism of AdNNs, we know
thatdifferentnumbersofresidualblocks/layersareactivatedduringinferencefordifferentinputs.Thenumberofactivatedblocks/layers
during inference has a semi-linear (step-wise) relation with energy
consumption,whichcan alsobenoticedin Figure7.Throughthis
step-wise relation between the number of activated blocks and
energyconsumption,wecanconcludethatinputandenergycon-
sumption of AdNNs are related. Because of this reason, EREBAis
able to learn a decent approximation of the energy consumption
of an AdNN given the input. Based on such approximation, EREBA
thengeneratesinputperturbationsthatsignificantlyincreasethe
energy consumption of the AdNN.
We evaluate EREBAon four criteria: effectiveness, sensitivity,
quality,androbustnessusingtheCIFAR-10andCIFAR-100datasets
[19,35,36]. First, to evaluate the effectiveness of the testing inputs
generated by EREBA, we calculate the energy required for AdNNs
to classifythese inputswhile running onan NvidiaTX2 server. Wethencomparethisvaluewiththeenergyrequiredbytheinputsgen-eratedfromcommoncorruptionsandperturbationstechniques[
14]
andasurrogatemodel-basedapproach.Weobservethat EREBAis
twiceaseffective.Thesensitivityof EREBAismeasuredthroughthe
behavior of the energy consumption of testing inputs generated
while limiting the magnitude of perturbation allowed, which en-
ables a comparison between the AdNN modelsâ€™ energy robustness.
The quality of the generated testing inputs is evaluated against
the original input through Peak Signal-to-Noise Ratio (PSNR) and
StructuralSimilarity Index(SSIM)[ 39,40].Finally, therobustness
ofEREBAis demonstrated by providing corrupted input images for
thegenerationoftestinginputs,whichrevealsthecapabilityofthe
estimatormodeltoimitatetheshortcomingsofthetargetAdNN.
Wefurtherdemonstratetwowaystoshowhow EREBAgenerated
testinputscanhelptoincreaseenergyrobustness:throughinput
filtering and gradient-based detection.
Our paper makes the following contributions:
â—Anapproach, EREBA,thefirstenergy-orientedblack-boxtest-
ing methodology for AdNNs.
â—A systematic empirical study on transferability of energy-
based testing inputs.
â—Fourevaluationstodemonstratetheeffectiveness,sensitivity,
quality, and robustness of EREBA.â—Two applications demonstrating the energy-saving capabil-
ity of EREBA.
2 BACKGROUND
2.1 Energy Robustness
ILFO[13]hasdefinedtheenergyrobustnessofaDNNasthesta-
bility of the modelâ€™s energy consumption after getting a perturbed
input.However,amodelâ€™senergyrobustnessshouldnotonlyde-
pend on the inputs that belong to the training data distribution
of the model. Energy robustness should also be evaluated basedon the out-of-distribution inputs. Because of this reason, we de-
fine two types of energy robustness for DNNs: Input-based Energy
Robustness ( ğ¸ğ‘–) and Universal Energy Robustness ( ğ¸ğ‘¢).
ğ¸ğ‘–isdefinedbythemaximumenergyconsumedbythemodel
for an input which belongs to the training data distribution ofthe model. Let us assume,
ğ‘¥is an input that is within the data
distributionof aDNN ğ‘“.Wewant toadd perturbation ğ›¿toğ‘¥such
thatenergyconsumptionismaximum.Inthatscenario, ğ¸ğ‘–canbe
represented as,
ğ¸ğ‘–=âˆ’max
ğ›¿âˆˆğ‘…(ğ¸ğ‘ğºğ‘“(ğ‘¥+ğ›¿)âˆ’ğ¸ğ‘ğºğ‘“(ğ‘¥))
,whereğ‘…issetofadmissibleperturbationssuchthat ğ‘¥+ğ›¿remains
within distribution, and ğ¸ğ‘ğºğ‘“represents the energy consumption
of DNN ğ‘“.
ğ¸ğ‘¢can be described as the highest possible energy consumed
by a model for any input. Inputs used to measure ğ¸ğ‘¢can be out-of-
distribution inputsalso. Fora DNN ğ‘“and anyinput ğ‘¥,ğ¸ğ‘¢can be
represented as,
ğ¸ğ‘¢=âˆ’maxğ‘¥ğ¸ğ‘ğºğ‘“(ğ‘¥)
, whereğ¸ğ‘ğºğ‘“representsenergy consumptionof DNN. Byincreas-
ing the value of ğ¸ğ‘–andğ¸ğ‘¢, energy robustness of a model can be
increased.
2.2 AdNNs
The main objective of AdNNs is to minimize executing layers in
a Neural Network while maintaining reasonable accuracy. The
AdNNs can be divided mainly into two types: Conditional-skipping
AdNNs[38,41] andEarly-termination AdNNs [2,33]. Both types of
AdNNsreducecomputationsiftheirintermediateoutputvaluessat-
isfypredefinedconditions.Forreducingcomputations,Conditional-
skippingAdNNsskipafewlayersorresidualblocks1(inthecaseof
ResNet),whileEarly-terminationAdNNsterminatestheoperations
within a block or network early.
3 TRANSFERABILITY OF ENERGY-BASED
TESTING INPUTS.
Inthis section,bycarrying outapreliminary study,weshow that
traditionaltransferabilitydoesnotexistinenergytestinginputs,
and existing technique like attacking surrogate model to generate
accuracy-based testing inputs cannot be applied in energy testing.
For traditional accuracy-based testing, transferability refers to the
1Residual block consists of multiple layers whose output is determined by adding the
output of the last layer and input to the block.
836
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. EREBA: Black-box Energy Testing of Adaptive Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
propertythatadversarialexamplesgeneratedforonemodelmay
also be misclassified by another model.
Motivation. In a black-box setting, existing techniques [ 5,21,28]
evaluate the accuracy-robustness of DNNs based on the traditional
transferabilityofadversarialsamples[ 27].Adversarialexamplesof
DNNs are perturbed inputs close to the original correctly classifiedinputsbutaremisclassifiedbyDNNs.Becauseadversarialexamples
are commonly used as testing inputs to measure the robustness
oftheneuralnetworks[ 23,43]),wewillalsousetheterm testing
inputstorefertotheadversarialexamplesinthispaper.Goodfellow
et al.and Szegedy et al.[10,32] have concluded that accuracy-
basedtestinginputsonatraditionalDNNmodelaretransferable.
Therefore,adversarialexamplesgeneratedbyattackingasurrogate
DNNmodelcanbeappliedtootherDNNsforevaluatingrobustness.Inthissection,weinvestigateiftraditionaltransferability,whichis
used for measuring accuracy robustness in a black-box setting, can
be applicable for energy-based testing inputs.
Table1: ğ¼ğ‘‡ğ‘ƒamongdifferentarchitectures.RNisResNet,BD
isBlockDrop,BNisBranchyNet.BMrepresentsBaseModel,
while TM is Target Model.
BMTMRANBD (RN 110) BNBD (RN 32)
RAN 100.0 46.041.0 12.5
BD (RN 110) 64.0 100.068.0 72.4
BN 61.0 52.0100.0 4.0
BD (RN 32) 5.5 45.075.2 100.0
Table2: ğ¸ğ‘‡ğ‘ƒamongdifferentarchitectures.RNisResNet,BD
isBlockDrop,BNisBranchyNet.BMrepresentsBaseModel,
while TM is Target Model.
BMTMRANBD (RN 110) BNBD(RN 32)
RAN 100.0 0.140.0 -1.8
BD (RN 110) 200.0 100.0350.0 52.5
BN 38.0 3.0100.0 -3.8
BD (RN 32) -3.5 10.0228.0 100.0
Preliminarystudy. Wehaveconductedastudytoinvestigatethe
traditional transferability of energy-based testing input on AdNNs.
To our knowledge, this is the first effort to explore the transferabil-
ity of energy-based testing input on AdNNs. We define base and
target models for this study. The white-box attack is performed on
the base model, and the target model classifies the testing input.
Wefocusontwometricstomeasuretransferability:thepercentage
ofthetransferableadversarialinputsandtheaveragepercentageofthetransferableenergyconsumptionincrease.Wedefinetwoterms:
Effectiveness Transferability Percentage (ETP) andInput Transfer-
ability Percentage (ITP). ETPis defined based on IncRF, which is
the fractional increase in AdNN-reduced floating-point operations
(FLOPs)afterfeedingenergy-basedtestinginputs.Wealsodefine ğ‘ƒğ‘
andğ‘ƒğ‘¡,theaverage IncRFonbaseandtargetmodels,respectively,
with the same testing inputs. We define ETP=(ğ‘ƒğ‘¡/ğ‘ƒğ‘)Ã—100.ITP
is defined as the percentage of testing inputs for which the FLOPs
count during inference increases in the target model. For an attack,
ifITPis high, it means that most of the generated testing inputs
for the base model can also increase the energy consumption inthe target model. If ETPis high, it means that the average increase
inthetargetmodelâ€™senergyconsumptioniscomparablewiththe
base model. Thus, if both ETPandITPare high, then it confirms
transferabilityin the attack.
Forexample,weattackthebasemodelandperturbteninputs.If
theaverage ğ¼ğ‘›ğ‘ğ‘…ğ¹onbasemodelis0.5, i.e.,ğ‘ƒğ‘=0.5.Ifsevenoutof
ten testing inputs increase the FLOPs on the target model, ğ¼ğ‘‡ğ‘ƒwill
be 70 %. For target model, The average IncRFis 0.3.ğ‘ƒğ‘¡would be
0.3andETP=(0.3/0.5)Ã—100=60%.Wehavesetmultiple thresholds
ofğ¼ğ‘‡ğ‘ƒandğ¸ğ‘‡ğ‘ƒto determine whether an attack is transferable. In
this study, we explore how many combinations of base AdNN and
target AdNN exceeds the different ITPandETPthresholds.
WehaveconductedatransferabilitystudyonfourAdNNmodels:
RANet[44],BlockDrop(ResNet-38,ResNet-110)[ 41],andBranchyNet
[33]. 1000 images sampled from the CIFAR-10 dataset were used in
thisstudy.WegeneratetestinginputsusingtheILFOattack[ 13]
(Figure1 Sub-figure I)onthebase model.Tables1and 2showthe
ITPandETPamong the architectures. From the tables, we can see
thatonlyfourcombinations(outof12)ofbaseandtargetmodels
exceedthe50%thresholdforboth ITPandETP(Forotherthresh-
olds, we have added a table in the website). From these results, we
can conclude that for the majority of AdNN models, the white-box
attack is non-transferable.
Althoughtraditionaltransferabilitymaynotbefeasibleforat-
tackingAdNNs,theobservationofthisfailuremotivatesustode-
velop an effective alternative. Specifically, our key observation on
thenon-transferabilityofenergy-basedattacksisthattheenergy-
saving mechanisms of AdNN models behave differently for the
same input, i.e.,an input causing high energy consumption on one
modelmightconsumelowenergyforanothermodel.Hence,extend-
ing any white-box attack method such as ILFO through Surrogate
models is not viable in the black-box scenario.
4 APPROACH
As we have observed that Surrogate models are not feasible to test
the energy robustness of AdNNs, we develop EREBA, an Estimator
model-based black-box testing system. EREBA contains two ma-jor components, namely an Estimator model and a testing inputgenerator. Figure 1 illustrates the Estimator modelâ€™s training for
anAdNNusingitsenergyconsumptiononanNvidiaTX2server.
TheEstimatormodeladdressesthechallengeofnon-transferability
discoveredintheprevioussection.Additionally,thetestinginput
generator in EREBA has two modes of testing: Input-based, where
an input image is perturbed to achieve higher energy consump-
tion, and Universal, where a noisy testing input that can maximize
energyconsumptionisgenerated.ThesemodesenableEREBAto
assess the energy robustness of each AdNN effectively. Figure 1
(Sub-figure ii) shows the generation of testing inputs using the
trained Estimator model.
4.1 Estimator Model Design
Traditionally,misclassificationblack-boxadversarialmethodson
DNNs are achieved through Surrogate models (also DNNs) trained
usingtheoutputlabelsproducedbyfeedingthetargetDNNwith
originalimages. Testing inputsgeneratedby awhite-box method
againstthetrainedSurrogatemodelarethenusedagainstthetarget
837
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu
Testing using Estimator modelTesting Using Surrogate model
Trained Surrogate ModelTraining
Generate Loss
FunctionIntermediate
OutputsTrained AdNN
Model
Training
Estimated
Energy
OptimizerOptimize
Perturbation
Optimizer
Optimize
Perturbation
Input Images
with
Energy
Consumption TX2
DNN Model
Architecture
Trained Estimator ModelOriginal
Input
Perturbation
Perturbation
Best PerturbationTest
Input Test
Input
Best Perturbation(i)ILFO
(ii)EREBA
Original
InputGGMeasure energy
and data cleaningInput Images
with 
output labelAdNN Model
ArchitectureGShip
Ship
Car100J
120J
200J
Figure 1: Difference between testing using Estimator model and Surrogate model
DNNasillustratedinFigure1(insidetheblue-dottedbox).How-
ever, this approach is not feasible for the current use-case due to a
lackoftraditionaltransferabilityinwhite-boxattacks(Section3).
Because building such a Surrogate model with the target function
ofmappinganimagetoaclasswouldnottransfersimilarenergy
characteristics to the Surrogate model. The target function should
be the energy-saving mechanism in an AdNN, but the output di-
mensions of these change with different AdNNs. So, we need a
separate Surrogate model for each AdNN; this is not viable for two
reasons.First,suchamodelwouldrequireanewneuralnetwork
architectureforeachAdNN,whichmakesithardtoapplytofuture
AdNNmodels.Second,theenergy-savingmechanismsâ€™outputsare
intermediate values within an AdNN, which are not accessible in a
black-box scenario.
Totacklethis,webuildanEstimatormodeltoemulatethecharac-
teristicsoftheenergy-savingmechanisminAdNNs.ThefeasibilityoftheEstimatormodelfollowsfromthefollowingtwokeyobserva-tions.1)Wecanperceivetheenergy-savingmechanismsâ€™character-
istics through system diagnostics such as the energy consumption
for each inference, which can be observed even in a black-box
setting.2)EventhougheachAdNNhasadifferentenergy-saving
mechanism, the resulting energy consumption is always expected
tolieinastep-wisepattern2;seeFigure7andSection7.3formore
details. Thus, we seek to leverage this patterned energy consump-
tioninourblack-boxapproachtotrainanEstimatormodelforeach
targetAdNNatwhichpointtheEstimatormodelcanpredictthe
energy consumption of each image.
However, the energy consumption of an embedded system such
asNvidiaTX2isaffectedbynoisefromthesystemenvironment,
suchasbackgroundprocessesanddynamicfrequencyscaling,mak-
ingitchallengingtocreateanaccurate model. Further,inablack-
boxenvironment,itishardtocategorizewhichdataisnoisy.We
do not have any additional information (e.g., number of the exe-
cuted blocks) about the inference; our approach has to tackle these
challenges.
An overview of theEstimator model training is given in Figure
1(Insidereddottedbox).First,wecollectenergyconsumptiondata
2Energy consumption of processing an extra residual block or layer in an AdNN will
alwaysaddsimilarenergyconsumptionintothetotalenergyconsumption,making
the energy consumption pattern step-wise.forimagesusedfortrainingand,toaddressthenoiseinthedatadue
tothesystemenvironment,wedeactivatethedynamicfrequency
scaling of Nvidia TX2 and ensure that no other user processes are
running.Additionally,werecordtheenergyconsumptionbythe
target AdNN during inference of an input image twenty times and
discard values, which are 50% higher than the median value. We
define the mean of the remaining values is defined as ğ¹(ğ‘¥ğ‘–), where
ğ‘¥ğ‘–represents an input in the dataset. We define the Estimatorâ€™s
DNN loss function as:
ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘œğ‘Ÿ ğ‘™ğ‘œğ‘ ğ‘ =1
ğ‘ğ‘
âˆ‘
ğ‘–=1[ğ¹(ğ‘¥ğ‘–)âˆ’ğ¸ğ‘†ğ‘‡(ğ‘¥ğ‘–)]2
whereğ¸ğ‘†ğ‘‡denote the Estimator model, and ğ‘is the size of the
dataset.ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘œğ‘Ÿ ğ‘™ğ‘œğ‘ ğ‘  isusedtotraintheEstimatorforeachtarget
AdNN, which enables the model to give a reasonable prediction of
energy consumption of the target AdNN for a given input.
4.2 Testing input generator
The objective of the testing input generator is to create testing
inputs that increase the Estimator modelâ€™s prediction, which in
turnshould increasethe actualenergy consumptionof theAdNN.
We explore two use cases of EREBA: 1) Input-based test, and 2)
Universal test for measuring the Input-based and Universal energy
robustness as defined in Section-2.1.
4.2.1 Input-based testing. In this use case, we modify the input
imageinsuchawaythatitisimperceptiblebyahuman,andthe
resultingtestinginputhashigherenergyconsumptiononthetarget
AdNN.Wethusaddaperturbation ğ›¿ğ‘–totheinput ğ‘¥ğ‘–.Pickingthe
bestğ›¿ğ‘–(Ë†ğ›¿ğ‘–) can be formulated as:
Ë†ğ›¿ğ‘–=argmax
ğ›¿ğ‘–ğ¸ğ‘†ğ‘‡(ğ‘¥ğ‘–+ğ›¿ğ‘–).
Additionally,wehavetoensurethatthemagnitudeofperturbation
isalsosmallashighermagnitudeperturbationsaremoresusceptible
to detection. We can reformulate the maximization problem to a
minimization problem as follows:
minimize
ğ›¿ğ‘–(âˆ£âˆ£ğ›¿ğ‘–âˆ£âˆ£âˆ’ğ‘â‹…ğ¸ğ‘†ğ‘‡(ğ‘¥ğ‘–+ğ›¿ğ‘–))where,(ğ‘¥ğ‘–+ğ›¿ğ‘–)âˆˆ[0,1]ğ‘›(1)
whereğ‘>0 is a hyperparameter chosen through grid search de-
pending on the AdNN model. Also, ğ‘controls the magnitude of
838
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. EREBA: Black-box Energy Testing of Adaptive Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
generated perturbation ( âˆ£âˆ£ğ›¿ğ‘–âˆ£âˆ£), where a large ğ‘makes the loss func-
tion more dependant on the energy estimate, allowing for larger
perturbations.Whereasasmaller ğ‘makesthelossfunctionmore
dependant onâˆ£âˆ£ğ›¿ğ‘–âˆ£âˆ£. Hence, ğ‘andâˆ£âˆ£ğ›¿ğ‘–âˆ£âˆ£are directly proportional.
This constrained optimization problem in ğ›¿ğ‘–can be converted
into a non-constrained optimization problem in ğ‘¤ğ‘–, where the rela-
tionship between ğ›¿ğ‘–andğ‘¤ğ‘–is:
ğ›¿ğ‘–=tanh(ğ‘¤ğ‘–)+1
2âˆ’ğ‘¥ğ‘–
Theğ‘¡ğ‘ğ‘›â„functionwouldensurethatthegeneratedtestinputvalues
staybetween0and1.Theequivalentoptimizationproblemin ğ‘¤ğ‘–
is:
minimizeğ‘¤ğ‘–/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowverttanh(ğ‘¤
ğ‘–)+1
2âˆ’ğ‘¥ğ‘–/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvert/arrowvertâˆ’ğ‘â‹…ğ¸ğ‘†ğ‘‡â›
âtanh(ğ‘¤
ğ‘–)+1
2ââ (2)
4.2.2 Universal Testing. In this use case, EREBAgenerates a testing
inputonlyusingtheEstimatormodel.UnlikeInput-basedtesting,
whichaddshumanimperceptibleperturbationtooriginalimages,
universal testing creates noisy testing inputs, which can maximize
the energy consumption of the target DNN independent of the
input.The intuitionbehind thistesting isthatadversaries cansend
noisy testing inputs exclusively to increase the systemâ€™s energy
consumptionbecausehumanperceptionmaynotbeaconcernin
everyscenario.Hencewemodifytheoptimizationfunctionfrom
equation 2 to:
minimizeğ‘¤ğ‘–âˆ’ğ¸ğ‘†ğ‘‡â›âtanh(ğ‘¤
ğ‘–)+1
2ââ (3)
Algorithm 1: Testing input generation using EREBA
Inputs : ğ‘¥ğ‘–âˆ¶ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡ğ¼ğ‘šğ‘ğ‘”ğ‘’
Outputs: ğ‘“ğ‘–âˆ¶ğ‘ƒğ‘’ğ‘Ÿğ‘¡ğ‘¢ğ‘Ÿğ‘ğ‘’ğ‘‘ğ¼ğ‘šğ‘ğ‘”ğ‘’
1begin
2Initialize(ğ‘¤ğ‘–)
3ğ‘‡=ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿ _ğ‘œğ‘“_ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ 
4ğ‘–ğ‘¡ğ‘’ğ‘Ÿ_ğ‘›ğ‘œ=0
5whileğ‘–ğ‘¡ğ‘’ğ‘Ÿ_ğ‘›ğ‘œ<ğ‘‡do
6 ğ¿=ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¥ğ‘–,ğ‘¤ğ‘–,ğ‘)
7 ğ¿ğ‘›ğ‘’ğ‘¤,ğ‘¤ğ‘–=ğ‘‚ğ‘ğ‘¡ğ‘–ğ‘šğ‘–ğ‘§ğ‘’ğ‘Ÿ(ğ¿,ğ‘¤ğ‘–)
8 ğ‘–ğ‘¡ğ‘’ğ‘Ÿ_ğ‘›ğ‘œ++
9end
10ğ‘“ğ‘–=tanhğ‘¤ğ‘–+1
2
11end
Both minimization problems can be solved through an iterative
approachgivenbyAlgorithm1,whichisalsoillustratedinFigure1
(sub-figure (ii) inside green box). The algorithm outputs the testing
inputğ‘“ğ‘–whiletakingthecurrentimage ğ‘¥ğ‘–(notrequiredinUniversal
test mode) as input. ğ‘¤ğ‘–is initialized to a random tensor (multi-
dimensional array) with a size equal to the input image dimension.
Foreachiteration,thelossfunctionofthecurrentmode(givenin
equations2,3)iscomputed(atline6).Thislossisback-propagated,
and the optimizer takes a step in the direction of the negative
gradientofthelossw.r.t ğ‘¤ğ‘–andupdates ğ‘¤ğ‘–withitsnextvalue.Once
the iteration threshold ( ğ‘‡) is reached, the algorithm computes and
returns the testing input ğ‘“ğ‘–(at Line 10). In the Universal test mode,
this algorithm is repeated for ğ‘ğ‘Ÿdifferent random initializations of
ğ‘¤ğ‘–, out of which ğ‘¤ğ‘–corresponding to the lowest loss value is used
for computing ğ‘“ğ‘–.5 EVALUATION
We evaluate the performance of EREBAon three popular AdNNs,
RANet[44],BlockDrop [41]andBranchyNet [33,34],interms
of four research questions (RQs):
RQ1: Effectiveness. How much increase in energy consump-
tion is achievable by the testing inputs generated by EREBA?
RQ2:Sensitivity. HowdoestheenergyconsumptionofAdNNs
react to limiting the magnitude of perturbation in EREBA?
RQ3: Quality. What is the difference in semantic quality be-
tween original images and testing inputs generated by EREBA?
RQ4: Robustness. IsEREBArobust against distribution shifts?
5.1 Experimental Setup
Datasets. For all AdNN models, the CIFAR-10 and CIFAR-100
datasets [ 19,35,36] have been used to train the Estimator model
andgeneratetestinginputs.Boththedatasetsconsistof50000train-
ing and 10000 test images, where CIFAR-10 and CIFAR-100 have
10and100classlabels,respectively.Byusingthesetwodatasets,
we show that EREBAis useful for both easier (CIFAR-10) and more
complex prediction (CIFAR-100) tasks.
Baseline. As there are no existing black-box energy testing frame-
works, we compare our technique with two different types of
baseline techniques. First, we compare our techniques with real-
world corruption and perturbation techniques (like fog, frost) [ 14].
The datasets generated from these techniques are commonly used
[9,26,42]totesttherobustnessofneuralnetworks.Second,weuse
a surrogate model technique that utilizes ILFO to generate testing
inputs.
Commoncorruptiontechniques[ 14]containdifferentvisualcor-
ruption,whichincludespracticalcorruptionslikefog,snow,frost.
We use 19 different corruption types, and for each type, five visual
corruptions are created from severity level one to five, resulting in
atotalof95differentvisualcorruptions.Intheimagesgenerated
by common corruption techniques, the noise present in the inputs
is human perceptible.
Commonperturbationtechniques[ 14]use14practicalperturba-
tiontypes;foreachoriginalinput,30differentimagesarecreated
with different amounts of perturbation. With perturbation, slightly
perturbed images are generated that are difficult for humans to
differentiate from the original images.
Otherthanusingcommoncorruptionsandperturbations[ 14],
we also use the Surrogate model-based technique as a baseline. For
thisapproach,wecreateaSurrogatemodelforeachAdNNanduse
theSurrogatemodeltogenerateadversarialimages.Asweseein
Tables1and2thatadversarialinputsgeneratedusingBlockDrop
as the surrogate model are more effective on other AdNNs, weadd a baseline that uses BlockDrop as a Surrogate model and is
referredtoasSURRGinthefollowingssections.ForeachAdNN,we
firstclassify50000CIFAR-10andCIFAR-100trainingdataonthe
target AdNNand basedon itsoutputs, andwe trainthe Surrogate
model. Then we use the ILFO attack on the Surrogate model togenerate test inputs. We use Surrogate model to generate bothlimited perturbation (Input-based SURRG) and noisy (Universal
SURRG) test samples.Models.
We have selected AdNN models where the range of max-
imum energy consumption during inference is large (15 J-160J).
839
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu
Table 3: Hyperparameters of EREBA
Mode AdNN ğ‘Learning Rate ğ‘‡ğ‘ğ‘Ÿ
Input-basedBlockDrop 10.01 500-
BranchyNet 100 0.01 500-
RANet 10 0.01 500-
UniversalBlockDrop -0.01 50030
BranchyNet -0.01 50030
RANet -0.01 50030
Weshowthat EREBAcanbeusedagainstAdNNmodelswithboth
higher and lower number of parameters. AdNN BlockDrop is built
modifyingResNet-110architectureandtrainedonCIFAR-10.While
for training using CIFAR-100 dataset, BlockDrop is built modifyingResNet-32architecturebecauseResNet-110BlockDroparchitecturetrainedonCIFAR-10datasethasshownlessadaptability.ResNetar-
chitecture starts with a 2D convolution layer, which is followed by
residualblocks.BlockDropselectswhichblockstoexecutethrough
Policy Network for BlockDrop. Both RANet and BranchyNet aremulti-exit networks, where operation can be terminated in oneof the earlier exits based on the confidence score of the exit. The
EstimatorModelisa ResNet-110,withtheoutputfullyconnected
tennodelayerchangedtoafullyconnectedsinglenode.Theloss
functionischangedtothefunctiondefinedinsection4.1.Hyperpa-
rameters chosen by the Estimator model ( ğ‘, learning rate, number
ofiterations( ğ‘‡))foreachAdNNmodelareinTable3,reasonsfor
choosing these parameters are given in section 5.2.1. These are
parameters used for the results reported in sections 5.2.1 and 5.2.3,
whereas section 5.2.2 reports the behaviour of EREBAwhenğ‘,ğ‘‡are
changed.HardwarePlatform.
We usetheNvidia JetsonTX2 boardforour
energyconsumptionmeasurements,whichareusedtotraintheEs-timatorModel.Bydefault,TX2hasadynamicpowermodel,whichscalestheCPUandGPUfrequenciesbaseduponthecurrentsystemload,whichaddsadditionaluncertaintytotheenergyconsumption
measurements. To combat this, we set the TX2 board to Max-N
mode, which forces CPU and GPU clock to run at their maximum
possible values, which are 2.0 GHz and 1.30 GHz, respectively.
JetsonTX2modulehastwopowermonitorchipsonboardfor
measuringpowerconsumption.Oneofthepowermonitorsmea-
sures the power consumption of CPU, GPU, and SOC as in Fig.8, 9 of the user manual of Nvidia TX2 [
25]. During the inference
processoftheAdNNs,wemeasurethepowerconsumptionofGPU
using a monitor program. Since the monitor program only usesthe CPU, the program does not affect the energy measurement.
Additionally, TX2 Internal power monitors have been validated by
otherstudiessuchasS.KÃ¶hleretal.[ 18](seeFig1b.),whereitis
shown that the power measurements using the internal monitor
chips collaborate with external measurement techniques. As stated
in[18],oneconcernwithusingtheinternalchipsisthatthepower
consumptionfromthecarrierboard,fan,andpowersupplyisnot
measured,whichcomesouttobearound2W.However,sinceour
studymeasurestheeffectofvarioustestinginputs,whichcannot
affect such componentsâ€™ behavior and both measurements (testing
input, original image) ignore the consumption from these com-ponents, the conclusions drawn are valid. Further, to ensure the
collected energy consumption data is correct, we run the inference
twenty times and discard values outlier values that are 50% higher
(a) Original
 (b) Testing Inputs
Figure 2: Testing inputs generated by EREBAfor BlockDrop
in Input-based testing mode
thanthemedianvalue.Themeanoftheremainingvaluesisused
as the single energy consumption value reported.Metrics.
We evaluate the effectiveness and robustness of EREBA
using the percentage increase in energy consumption for each
target AdNN:
ğ¸ğ‘›ğ‘’ğ‘Ÿğ‘”ğ‘¦ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘“ğ‘–)âˆ’ğ¸ğ‘›ğ‘’ğ‘Ÿğ‘”ğ‘¦ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘¥ğ‘–)
ğ¸ğ‘›ğ‘’ğ‘Ÿğ‘”ğ‘¦ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘¥ğ‘–)Ã—100
whereğ‘¥ğ‘–is the input image provided to EREBAandğ‘“ğ‘–is the testing
inputgenerated.ForSensitivity(RQ2)measurements,weusethe
incrementinenergyconsumptioninJoules(J)tobettercompare
sensitivity between target AdNNs and use the average squared
difference in pixels values between the input image and the testing
input to quantify the magnitude of the perturbation. The testing
inputsâ€™qualityismeasuredusingPeakSignaltoNoiseRatio(PSNR)
[39],andStructuralSimilarityIndex(SSIM)[ 40]becauseofusage
of these metrics in the industry to measure image quality.
5.2 Experimental Results
5.2.1 RQ1.Effectiveness. Toevaluatetestingeffectivenessby EREBA,
wehavemeasuredtheaverage percentageincreaseineachAdNN
modelâ€™s energy consumption between the original images in the
datasetandthecorrespondingtestinginputsgeneratedby EREBA.
We compare the effectiveness of Universal testing against the com-
moncorruptiontechniques[ 14]because,inbothapproaches,noise
introduced to the inputs is human perceptible. Whereas Input-
basedtestingaddsimperceptibleperturbationtotheinput,hence
wecompareagainstthecommonperturbationtechniques[ 14].The
hyperparameters chosen for each model are in Table 3. Parame-
ters vary between AdNNs due to variations in input normalization,
whichisonlyappliedinBranchyNetandRANet.Weevaluateon
imagesfromtheCIFAR-10andCIFAR-100datasets,whichhavea
considerable reduction in the energy consumption on the target
AdNNs [33,34,41,44]. The Estimator models have been trained
on 50000 CIFAR-10 and CIFAR-100 training images. We apply com-
mon corruption and perturbation techniques [ 14] to CIFAR-10 and
CIFAR-100 test images. As there are numerous corruption, and
perturbation techniques, weonly report the bestperforming tech-
niques (i.e., highestIncRF) for each AdNN model (For the IncRF
valuesofothercorruptionsandperturbations,pleaseseetheweb-
site3); Table 4 reports the exact corruption/perturbation technique.
Table5reportsthemeanpercentageincreaseinenergyconsump-
tion of theAdNN models under EREBA(Universal, Input-based test-
ings)andthebaselinesontheCIFAR-10dataset.Figure2illustrates
some Input-based testing inputs generated by EREBAfor Block-
Drop. We observe that EREBAInput-based testing inputs dominate
3https://sites.google.com/view/ereba/home
840
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. EREBA: Black-box Energy Testing of Adaptive Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 4: Corruptions and perturbations we have used for
comparisonforeachmodel. corrrepresentscorruptionsand
perrepresents perturbations.
Data
TypeModels
BlockDrop BranchyNet RANet
Best Corr (CIFAR-10) Contrast Impulse Noise Contrast
Best Per (CIFAR-10) Gaussian Blur Snow Gaussian Blur
Best Corr (CIFAR-100) Contrast Impulse Noise Fog
Best Corr (CIFAR-100) Zoom Blur Zoom Blur Shot Noise
Table5:Meanpercentageincreaseinenergyconsumptionof
themodelsforCIFAR-10datasetagainst EREBAandbaseline
techniques.
PerturbationTypeModels
BlockDrop BranchyNet RANet
Universal Testing (EREBA) 97.54 528.511846.18
Best Corr 77.77 186.791209.00
Universal SURRG 137.8 26.87302.42
Input-based Testing (EREBA) 67.92 288.90 885.00
Best Per 16.24 153.721480.87
Input-based SURRG 135.5 118.39 554.69
the baseline methods for mean energy increase on BranchyNet.
WhereasforBlockDrop,becauseSURRGhasBlockDropasitsarchi-
tectureandILFOisawhite-boxmethod,itisexpectedtooutperform
EREBA, a black-box method. Interestingly, for RANet, common per-
turbation techniques [ 14] induce a much higher energy increase
thanthecorruptiontechniques,whichisquitedifferentfromthebe-
havior observed in the other AdNNs. While EREBAunderperforms
the perturbations in terms of energy consumption increase, EREBA
stilloutperformsSURRG.Also,weobservethatforallAdNNs,sam-
ples generated through Universal testing outperform the baseline
techniques. Furthermore, for BranchyNet and RANet, due to fewer
executionmodes(twoinBranchyNet andeightinRANet),theen-
ergy consumption is higher than BlockDrop (up to 2000 % more
than the original data) with 254modes.
FortheCIFAR-100dataset,Table6showstheaveragepercentage
increase in energy consumption for EREBAand the baseline tech-
niques.Wenoticethat EREBAgeneratedinputsoutperformcommon
corruptionandperturbationtechniquesforallthreeAdNNs.Sim-
ilar to the CIFAR-10 dataset, SURRG generated inputs consume
more energy than EREBAgenerated inputs only for theBlockDrop
model. For RANet, Universal testing inputs can not significantly
increaseenergyconsumptionbecauseRANetalwayspredictsan
inputwithhighnoiseasroadorshrewwithhighconfidence;there-
fore, the inference is stopped at initial exits, resulting in lower
energy consumption. Nevertheless, the Input-based testing inputs
canincreaseupto4000%energyconsumptionoftheoriginalinputs
for the RANet model. Thus, we conclude that, on average, over all
threeAdNNmodels, EREBAperformsbetterthananyotherbaseline
technique in terms of increasing energy consumption.
5.2.2 RQ2. Sensitivity. We define the sensitivity of EREBAin terms
of the magnitude of the perturbation âˆ£ğ›¿ğ‘–âˆ£. Intuitively, if an AdNN
modelâ€™s energy consumption spikes up with a relatively loweraverage perturbation magnitude, then that model is less robust.Table6:Meanpercentageincreaseinenergyconsumptionof
themodelsforCIFAR-100datasetagainst EREBAandbaseline
techniques.
Perturbation
TypeModels
BlockDrop BranchyNet RANet
Universal Testing (EREBA) 27.22 580.50 479.80
Best Corr -29.61 5.74-31.80
Universal SURRG 55.40 71.42-29.50
Input-based Testing (EREBA) 17.27 283.601113.28
Best Per -54.28 37.80-30.73
Input-based SURRG 41.31 37.90754.69
(a) CIFAR-10 (b) CIFAR-100
Figure 3: Average energy consumption increase of testing
inputsconstrainedbymagnitudeofperturbationforBlock-Drop, BranchyNet and RANet.
Weknowthat ğ‘andâˆ£ğ›¿ğ‘–âˆ£aredirectlypropositional,giventhatthe
number of iterations is constant. Through empirical observations,
ğ‘‡=500 is sufficient to achieve convergence in EREBAfor all AdNN
models.Notethatsensitivitycannotbecomparedusingthemag-
nitude of ğ‘as only BranchyNet and RANet use normalization fil-
ters, whereas BlockDrop does not, which makes the optimal ğ‘of
BranchyNetandRANetlarger(seeTable3).Tomeasurethemag-
nitude of perturbation for a set of c, T we measure the average
squareddifferencebetweenthetestinginputandtheinputimage,
which is defined as follows:
Average squared difference =1
ğ‘ğ‘
âˆ‘
ğ‘–=1(ğ‘¥ğ‘–âˆ’ğ‘“ğ‘–)2
whereğ‘¥ğ‘–is the input image, and ğ‘“ğ‘–is its corresponding testing
input. Figures 3a and 3b show the average percentage increase
inenergyconsumptionversustheaveragesquareddifferenceon
the CIFAR-10 and CIFAR-100 datasets. We observe that for both
datasets compared to BlockDrop, BranchyNet and RANet are more
sensitivetotheperturbationmagnitude.BlockDropâ€™slowersensi-
tivityismainlyduetoBlockDropâ€™sPolicyNetwork,whichprovides
morerefinedcontroloverenergyconsumption;henceBlockDrop
ismorerobustthantheothertwoAdNNs.Additionally,wecanseethepotencyoftheEstimatormodelforeveryAdNNmodel.Adirect
proportionality between the average increasein energy consump-
tionandtheaveragesquareddifferenceisobservedinalltheAdNN
models,whichisevidencethattheEstimatormodelissuccessful
inimitatingtheenergyconsumptionofeachAdNN.Additionally,
we observe that EREBAperforms very similarly for both CIFAR-10
and CIFAR-100 datasetsfor BlockDrop andBranchyNet. Whereas
841
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu
(a) PSNR between original and
generated images(b) SSIM between between origi-
nal and generated images.
Figure 4: Quality of the generated images for each AdNN
model for CIFAR-10 dataset
(a) PSNR between original and
generated images(b) SSIM between between origi-
nal and generated images.
Figure 5: Quality of the generated images for each AdNNmodel for CIFAR-100 dataset
for RANetfor CIFAR-10, theenergy spike inducedis much higher
thanthatforCIFAR-100,indicatingthattheCIFAR-100RANetis
more robust than the CIFAR-10 version.
5.2.3 RQ3: Quality. In this section, we evaluate the quality of the
perturbationgeneratedbyInput-basedtestingof EREBAwiththe
hyper-parameters set to the values given in Section 5.1 using Peak
Signal to Noise ratio (PSNR) [ 39] and Structural Similarity Index
(SSIM) [40]. Bothof these metricsare used in theindustry to mea-
sure the image quality of noisy images. SNR of an image can be
represented by,
ğ‘†ğ‘ğ‘…=ğœ‡ğ‘–ğ‘šğ‘ğ‘”ğ‘’
ğœğ‘–ğ‘šğ‘ğ‘”ğ‘’
whereğœ‡ğ‘–ğ‘šğ‘ğ‘”ğ‘’isthemeanvalueoftheimagepixelsand ğœğ‘–ğ‘šğ‘ğ‘”ğ‘’is
theerrorvalueofthepixelvalues. ForPSNR,thehighestvalueof
theimagepixelsisusedinsteadofthemeanvalue.TheStructural
Similarity Index (SSIM) is a perceptual metric that quantifies the
image quality degradation caused by processing such as data com-pression or by losses in data transmission. Higher values for SSIM
and PSNR indicate higher quality test inputs.
Figure4andFigure5showthevaluesofSSIMandPSNRbetween
thetestinginputsandtheoriginalimagesforCIFAR-10andCIFAR-
100 datasets. For both datasets, we see that SSIM for the generated
testinginputsissimilarforallthetargetAdNNs.WhereasPSNRforRANetisworsebutstillcomparabletoBlockDropandBranchyNet.
WeconcludethatmostoftheinputsgeneratedthroughInput-based
testing are of high quality; even if some test inputs might have
noise, they are structurally similar to the original inputs.Table 7: Effect of corruptions on EREBA. Average percentage
increase in energy consumption for all AdNN models for
various corrupted inputs from CIFAR-10 and CIFAR-100.
AdNN Dataset Normal Frost Fog Snow
RANetCIFAR-10 929.762085.362027.202099.41
CIFAR-100 340.756.9813.5425.51
BranchyNetCIFAR-10 283.05475.08355.70323.72
CIFAR-100 278.17197.58374.62360.37
BlockDropCIFAR-10 72.78108.3789.61111.80
CIFAR-100 17.5024.9121.4936.95
5.2.4 RQ4:Robustness. EREBAestimatesenergyconsumptionbased
onthetrainingdata,whichcanbeimpactedbydistributionshift[ 31].
Therefore, to evaluate the robustness of EREBAagainst distribution
shifts,wehaveanalyzedthebehaviorof EREBAagainstPracticalcor-
ruptions.Practicalcorruptions(e.g., fog,snowetc.)arefrequently
noticed, mainly when we use mobile phones or autonomous ve-hicles to take images. For this purpose, we have used real-world
common corruption techniques [14].
Table 7 shows the average percentage increase in energy con-
sumption for the testing inputs generated using the original and
corruptedimagesofCIFAR-10andCIFAR-100by EREBAinInput-
based testing. We picked the corruption classes of fog, frost, andsnow due to their natural occurrence. In general, the corruptedimages do not hinder the performance of
EREBAexcept for the
CIFAR-100 version of RANet. In other cases, the increase in energy
consumption achieved by EREBAusing corrupted images is, in fact,
higher than that achieved using the original CIFAR-10 and CIFAR-
100datasets.Thisismainlyduetothecommoncorruptions[ 14],
introducing better initialization spots (white areas). For CIFAR-100
RANet, EREBAmanagestoincreasetheenergyconsumptionslightly.
However, similar to the Universal testing case, inputs with high
noiseareclassifiedasroadorshrewwithhighconfidence,which
leads to lower performance in comparison to other settings.
Additionally,throughtheseresults,wefurthernoticethehigh
stabilityoftheEstimatormodelinapproximatingtheshortcomings
ofthetargetAdNNs. EREBAcangeneratehighenergy-consuming
testinginputsdespitethecorruptionofimages.Whilethereissome
variance in how EREBAbehaves when provided with an image
with different corruption classes; the median energy consumption
increment is consistent for all target AdNNs.
6 INCREASING ENERGY ROBUSTNESS
In this section, we demonstrate two ways to increase the energy
robustness of AdNNs. In both ways, we use prior EREBAgenerated
inputs to detect new EREBAgenerated energy-surging inputs for
AdNNs. For detecting Universal test inputs, we use input filtering
method based on pixel values, while for detecting input-based test
inputs, we use gradient-based input detection.Input Filtering.
As we can notice that noisy samples generated
fromUniversaltestingcanincreaseenergyconsumptionbyasig-
nificantamount,henceitisessentialtoadapttheAdNNmechanism
against these noisy images. For traditional DNNs, highly noisy im-
ages consumethe same energyas the standard imagesand are no
threattotherobustnessoftheDNN(theobjectisnotvisibleinthose
noisy images). To adapt AdNNs against high energy-consuming
images, we propose to include a filter in AdNNs.
842
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. EREBA: Black-box Energy Testing of Adaptive Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
As a filter, we have created a ResNet model (binary classifier)
with only six residual blocks. The classifier can classify into two
categories: normal input andenergy-consuming noisy input .
For each AdNN, We have trained the ResNet model with 2500
normal dataset images and 2500 high energy-consuming noisy
images, creating three different trained ResNet models. For testing
the models, 1000 images have been used (500 from each class). Our
results show that the filter can identify each test image with the
correct class (100% accuracy) for all three AdNNs for both datasets.
Theresultsconfirmthatwecanfilterouthighenergy-consuming
noisy images with a model whose energy consumption is low.Gradient-based Input Detection.
In contrast to detecting adver-
sarial inputs similar to the samples generated by the Universal
testingmode,adversarialinputssimilartothesamplesgenerated
by the Input-based testing mode are harder to differentiate frombenign inputs. Additionally, the energy constraint on such a de-
tectionsystemisasignificantchallenge.Therefore,thedetection
techniquemust consumesignificantlylow energywithrespectto
theenergyconsumedbyInput-basedtestinginputsduringinfer-ence. To address these challenges, we propose a gradient-based
adversarialinputdetectionmechanismthatusespartialinference
from the AdNN.
In this detection mechanism, we leverage the behavior of the
energy-saving mechanism within various AdNNs. These mecha-nisms, in general, try to ensure that the difference between an
intermediate output and a predefined condition is large, which will
deactivate certain parts of the AdNN. In other words, if the loss
function of intermediate outputs is large for any input, the energy
consumption will be low [ 13]. Therefore, if the gradients of the
intermediatelossfunctionwithrespecttotheweightsarelarge,the
input is more likely to be a benign input. Hence, we only need par-
tialinference(weightgradientsofaninitiallayer)fromtheAdNNs,
and a linear SVM [ 6] to detect the energy-surging inputs, where
both are low energy consuming steps. So, the energy impact of our
detectioncomponentissignificantlylow.Iftheinputispredicted
as energy-surging by the detector, the inference is stopped early.
Toevaluate ourdetection component,we generateinput-based
testinginputsforCIFAR-10andCIFAR-100trainingandtestdatasetsforallthreeAdNNs.Next,wecalculatethegradientsoftheweights
with respect to the intermediate loss function for the training sec-
tionofdatasets.ForallthreeAdNNs,weconsidertheweightsof
thefirstlayeroftheAdNN.SpecificallyforBlockDrop,wecalculate
the intermediate loss function of the policy network, whereas, for
RANetandBranchyNet,weusethefirstexitâ€™slossfunction(Section
5.1). After calculating the weight gradients for the training section
ofdatasets,welabelthemaseitheroriginalortestinginputsand
use them for training an SVM binary classification model for each
AdNN.Finally,wetesttheSVMclassifiersonthegradientsgener-
atedby theoriginaland input-basedtesting inputforthe testing
section of datasets.
Table8showsthedetectionaccuracy(%)andAUCscore[ 3]of
ourgradient-basedinputdetectiontechnique.AUCscorecomputes
the area under the Receiver OperatingCharacteristic Curve (ROC
AUC) [3] and measures the efficacy of any binary classifier, with
higherAUCscorescorrespondingtobetterclassifiers.Theresults
showthatin5outof6scenarios,bothAUCscoreishigherthan0.8
and the detection accuracy is higher than 80 percent, showing ourTable 8: Detection Accuracy (%), AUC score, Accuracy Drop
percentage, Adversarial Energy Decrease Percentage, andBenignEnergyIncreasePercentageoftheGradient-basedin-put detection incorporated in to various AdNN models
AdNN Dataset Detection (%) AUCAcc Drop(%) Adv Eng Dec (%) Ben Eng Inc (%)
RANetCIFAR-10 92.50 0.924 11.50 77.10 26.80
CIFAR-100 81.60 0.816 3.90 84.30 17.74
BranchyNetCIFAR-10 99.90 0.999 0.01 84.40 17.90
CIFAR-100 99.80 0.998 0.01 87.50 15.60
BlockDropCIFAR-10 94.39 0.943 3.30 95.00 6.25
CIFAR-100 66.90 0.666 30.40 92.80 7.60
approachâ€™s efficacy. Additionally, we also report the accuracy drop
of the AdNNs due to false positives from the detection system. We
observe that the accuracy drop is minimal in all cases except for
the CIFAR-100 BlockDrop model.
Furthermore, to demonstrate the benefits of our gradient-based
detectionsystemfromtheenergyperspective,wealsoreportthe
average energy decrease percentage for an adversarial input (Adv
Eng Dec) and average energy increase percentage for a benigninput (Ben Eng Inc). We observe that our detection system cansignificantly reduce energy consumption induced by adversarialinputs (up to 95%) while introducing minimal energy burden as
evidenced by a low increase in consumption for benign inputs.
7 DISCUSSION
WediscussthealternativedefenseforAdNNs,theadaptabilityof
AdNNsondifferentdatasets,andtherelationshipbetweenblock
activation and energy consumption of AdNNs. Also, we discuss
correlation between measured and estimated energy consumption,
and the correlation between energy consumption increased by
different techniques.
7.1 Alternative Defense.
Wehaveinvestigatedtheapplicationofadversarialtrainingasan
alternative defense mechanism against EREBAgenerated energy-
surging inputs. To understand the effect of adversarial training on
AdNNs, we use the EREBAgenerated testing inputs for CIFAR-10
dataset to retrain the original AdNNs. We used Input-based testing
inputsgeneratedfrom1000imagesoftheCIFAR-10trainingdatasetasthetrainingsetandretrainedtheBranchyNetandRANetAdNNs
for150epochswithalearningratesameparametersastheinitial
training. We generate the test set using a batch of 600 images from
the CIFAR-10 test dataset. We found that adversarial training does
notincreasetheenergyrobustnessforallAdNNmodels.Specifically,
RANet is easier to improve using adversarial training compared to
BranchyNet.Duetospaceconstraints,wehavereportedtheresults
for Adversarial training in our website4.
7.2 Adaptability of AdNNs
Inour observations, weseethat AdNNsmay notbeadaptiveun-der all circumstances. Each AdNN can decrease its FLOPs count;
however,thismaynotalwaysresultinconcreteenergyconsump-
tionpatterns.Figures6a,6b,6c,and6dshowBlockDrop(ResNet-
110) and SkipNet modelsâ€™ adaptability on CIFAR-100 and ImageNet
4https://sites.google.com/view/ereba/home
843
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu
(a) Adaptability of BlockDrop
(RN-110) on CIFAR-100 Dataset(b) Adaptability of SkipNet on
CIFAR-100 Dataset
(c) Adaptability of BlockDrop(RN-110) on ImageNet Dataset (d) Adaptability of SkipNet on
ImageNet Dataset
Figure 6: Adaptability of AdNNs on ImageNet and CIFAR-
100 dataset.
datasets.WecanseethattheBlockDropâ€™sadaptability(thediffer-
encebetweenthehighestandlowestnumberofactivatedblocks)
for both datasets is limited (less than 4). For SkipNet, the range of
adaptability is better than BlockDropâ€™s range.
7.3 Comparison to Misclassification Attacks
We can assume that the energy-saving mechanisms of Conditional-
Skipping networks like BlockDrop would classify an image into
ğ‘âˆ’1 categories. ğ‘is the number of blocks in the DNN, with each
image in category ğ‘–activating ğ‘–+1 (the first block is always active)
blocks.WhereasforthecaseofEarly-Terminationnetworkssuch
asBranchyNetandRANet,theyclassifyanimageinto ğ¸categories
whereğ¸is the number of exits in the network. Howev er, due to
noisefromthesystemenvironment,ourEstimatormodelcannot
differentiate between all the classes. Figure 7 shows the scatter
plotsbetweentheseimageclassesandtheirenergyconsumption
for each AdNN model. We can see the step-wise pattern in the plot
foralltheAdNNmodels.TheEstimatormodelistrainedtolearn
this pattern of energy consumption of AdNNs.
7.4 Correlation between Actual Energy
Consumption and Estimated Energy
Consumption
Toillustratethecorrelationbetweenenergyconsumptionpredicted
bytheestimatormodelandoriginalenergyconsumption,weuse
the Pearson Correlation Coefficient (r) [ 1] and correlation p-value.
Iftwosetsofvaluesarecorrelated,thervaluewouldbesignificant,
and the p-value will be low.
For CIFAR-100 dataset, the r values are 0.38, 0.17, and 0.31 for
RANet, BlockDrop, and BranchtNet models, respectively, where allthep-valuesarelessthan0.0005.Theseresultsconcludethatthe
values are correlated.
ForCIFAR-10dataset,thevalueofrforRANetis0.021;however
p-value is 0.03; therefore, it is more likely that the values are corre-
lated.ForBlockDropmodel,thevalueofrandp-valueare0.004and
0.66, which suggests that the values are less likely to be correlated.
Butifweconsideronlytheinputswhoseenergyconsumptionis
higher than the 75th percentile value, the p-value becomes 0.14,
suggestingacorrelation.Therefore,iftheestimatormodelcanac-
curatelypredicthighenergy-consuminginputs(i.e.,differentiate
clearlybetweenlow/midandhighenergy-consuminginputs),we
can use the estimator model to generate energy-expensive testing
inputs.
7.5 Correlation of Increase of Energy
Consumption between Different
Techniques
Inthissection,wetrytoexplorethecorrelationbetweenenergycon-
sumption modified by Input-based testing and baseline techniques.
WeusePearsonCorrelationforthatpurpose.PearsonCorrelation
isoneofthemetricsthatcanfindthestrengthoftherelationship
between two variables. For CIFAR-100 data, we show the Table9 that represents the Pearson Correlation Coeff (r) and p-value
betweenthepercentageofenergyconsumptionincreasedbyInput-basedtestinginputsandenergyconsumptionincreasedbybaseline
techniquegeneratedinputs.Itcanbenoticedthatformostofthe
cases, the energy increase percentages are less likely to be cor-related. Only for BranchyNet, we can find a significant negative
correlation between Input-based and Perturbation-induced energy
consumption increase.
Table 9: Correlation between the percentage of energy con-
sumption increased by Input-based testing inputs and per-centage energy consumption increased by baseline tech-nique generated inputs for different models for CIFAR-100data
ModelsBaseliner(Perturb) p-value(Perturb) r(SURRG) p-value(SURRG)
RANet 0.142 0.328 -0.04 0.976
BlockDrop -0.007 0.908 -0.037 0.544
BranchyNet -0.296 0.133 -0.118 0.556
8 RELATED WORKS
AdNNs.Among Conditional-skipping models, Hua et al.[16] and
Gaoetal.[8]explorechannelgatingtodeterminecomputational
blind spots for channel-specific regions unessential to classifica-tion. Liu et al.[
20] propose a new type of AdNN which utilizes
reinforcementlearningtoachieveselectiveexecutionofneurons.
SkipNet [ 38] uses gating techniques to skip residual blocks. On
the other hand, Graves et al.[11], Figurnov et al.[7], and Teer-
apittayanon et al.[33] propose SACT and BranchyNet respectively,
which are Early-termination AdNNs. SACT terminates the compu-
tation within a residual block early based on intermediate outputs,
while BranchyNet uses separate exits within network for early ter-
mination.CascadingmultipleDNNswithvariouscomputational
844
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. EREBA: Black-box Energy Testing of Adaptive Neural Networks ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
(a) (b) (c)
Figure 7: Energy consumption of (a) RANet, (b) BlockDrop, (c) BranchyNet for CIFAR-10 Training Set
coststhroughasinglecomputationunittodecidewhichDNNto
executehasbeenproposed.Thecascadingmodelsusevarioustech-
niques such as termination policy [ 2], reinforcement learning [ 12],
and gating techniques [24] to achieve early termination.
Adversarial Examples. AdversarialExamplesarethesynthe-
sizedinputs thatis ableto modifytheprediction ofthe MLmodel.
Szegedyet al.[32] and Goodfellow et al.[10] propose white-box
adversarial attacks on convolutional neural networks. Papernot et
al.[28] have used surrogate model to attack a DNN in black-box
setting.Liu etal.[21]useensembleofmultiplewhite-boxmodelsto
generate adversarial examples, which can attack black-box models.
Ilyasetal.[17]useevolutionarysearchstrategiestoestimatethe
gradient of a model to attack black-box models.
However, all these attacks focus on changing the prediction and
do not concentrate on increasing test time. ILFO [ 13] is the first
work to attack a DNN by increasing the energy consumption of
the model. However, ILFO uses white-box setting and does not
have transferability. Therefore, ILFO can not be used for black-box
attack.
Next, DeepSloth [ 15] uses modified PGD to attack against Early-
termination AdNNs using the confidence scores in each exit. How-
ever,DeepSlothcannotbeusedagainstConditional-skippingAdNNs.
Also, DeepSloth provides a study about the transferability of the
attack. The study considers the efficacy of the Early-termination
models as the transferability metric. However, we propose a more
systematic transferability study by introducing of metrics like ETP
and ITP.DNN Testing. Multiple testing methods have been proposed
recently to test DNNs. DeepGauge [ 22] is proposed based on a
test criteria set that verifies the corner neuron activation values.
DeepXplore[ 30]proposestocovereachneuronâ€™sbinaryactivation
statusanduseneuroncoveragetotestDNNs.DeepTest[ 37]tests
autonomousdrivingcarsbyusingneuroncoverage.Recently,Deep-
Hunter[43]proposestousecoverage-guidedfuzztestingonDNNs.
EREBAevaluates the energy-robustness of AdNNs in a black-box
settingunliketheaforementionedtechniques,whicharefocusedon
testingtheaccuracy-robustnessoftraditionalDNNsinwhite-box
setting
9 CONCLUSION
Inthispaper,wehaveproposedpracticalblack-boxtestingmeth-
ods to evaluate energy robustness of AdNNs. The core idea behind
the technique is to create inputs which increase the energy con-
sumption of AdNN to a higher level. To achieve this goal, we have
presented EREBA5,wherewehaveproposedtwotypesoftesting:
UniversaltestingandInput-basedtesting.Toourknowledge,we
are the first to explore black-box testing on AdNNs. Test inputs
generated by EREBAcan improve the energy robustness of AdNNs.
Finally,thispaperalsoanalyzesthebehaviorofAdNNsandsuggests
model improvement strategies.
ACKNOWLEDGEMENT
This work was partially supported by Siemens Fellowship and NSF
grant CCF-2146443.
5https://sites.google.com/view/ereba/home
845
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu
REFERENCES
[1]JacobBenesty,JingdongChen,YitengHuang,andIsraelCohen.2009. Pearson
Correlation Coefficient. In Noise Reduction inSpeech Processing. Springer, 37â€“40.
[2]TolgaBolukbasi,JosephWang,OferDekel,andVenkateshSaligrama.2017. Adap-
tiveNeuralNetworksforEfficientInference.In Proceedingsofthe34thInterna-
tional Conference on Machine Learning-Volume 70. JMLR. org, 527â€“536.
[3]Andrew P. Bradley. 1997. The Use of the Area under the ROC Curve in the
Evaluation of Machine Learning Algorithms. Pattern Recogn. 30, 7 (July 1997),
1145â€“1159.
[4]NicholasCarliniandDavidWagner.2017. TowardsEvaluatingtheRobustness
of Neural Networks. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE,
39â€“57.
[5]Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. 2019. Im-
proving Black-box Adversarial Attacks with a Transfer-based Prior. In Advances
in Neural Information Processing Systems. 10932â€“10942.
[6]Corinna Cortes and Vladimir Vapnik. 1995. Support-vector Networks. Machine
learning20, 3 (1995), 273â€“297.
[7]MichaelFigurnov,MaxwellDCollins,YukunZhu,LiZhang,JonathanHuang,
DmitryVetrov,andRuslanSalakhutdinov.2017. SpatiallyAdaptiveComputation
Time forResidualNetworks. In Proceedings ofthe IEEE Conferenceon Computer
Vision and Pattern Recognition. 1039â€“1048.
[8]Xitong Gao, Yiren Zhao, Åukasz Dudziak, Robert Mullins, and Cheng-zhong
Xu.2018. DynamicChannelPruning:FeatureBoostingandSuppression. arXiv
preprint arXiv:1810.05331 (2018).
[9]Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A
Wichmann, and Wieland Brendel. 2018. ImageNet-trained CNNs Are Biased
Towards Texture; Increasing Shape Bias Improves Accuracy and Robustness.
arXiv preprint arXiv:1811.12231 (2018).
[10]Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
Harnessing Adversarial Examples. arXiv preprint arXiv:1412.6572 (2014).
[11]Alex Graves.2016. AdaptiveComputation timefor RecurrentNeural Networks.
arXiv preprint arXiv:1603.08983 (2016).
[12]JiaqiGuan,YangLiu,QiangLiu,andJianPeng.2017. Energy-efficientAmortizedInferencewithCascadedDeepClassifiers. arXivpreprintarXiv:1710.03368 (2017).
[13]MirazulHaque,AnkiChauhan,CongLiu,andWeiYang.2020. ILFO:Adversarial
Attack on Adaptive Neural Networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 14264â€“14273.
[14]DanHendrycksandThomasDietterich.2019. BenchmarkingNeuralNetwork
Robustness to Common Corruptions and Perturbations. Proceedings of the Inter-
national Conference on Learning Representations (2019).
[15]Sanghyun Hong, YiÄŸitcan Kaya, IonuÅ£-Vlad Modoranu, and Tudor DumitraÅŸ.2020. A Panda? No, Itâ€™s a Sloth: Slowdown Attacks on Adaptive Multi-Exit
Neural Network Inference. arXiv preprint arXiv:2010.02432 (2020).
[16]Weizhe Hua, Yuan Zhou, Christopher M De Sa, Zhiru Zhang, and G Edward
Suh. 2019. Channel Gating Neural Networks. In Advances in Neural Information
Processing Systems. 1884â€“1894.
[17]Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-
boxAdversarialAttackswithLimitedQueriesandInformation. arXivpreprint
arXiv:1804.08598 (2018).
[18]Sven KÃ¶hler, Benedict Herzog, Timo HÃ¶nig, Lukas Wenzel, Max Plauth, JÃ¶rgNolte, Andreas Polze, and Wolfgang SchrÃ¶der-Preikschat. 2020. Pinpoint the
Joules: Unifying Runtime-Support for Energy Measurements on Heterogeneous
Systems. In 2020 IEEE/ACM International Workshop on Runtime and Operating
Systems for Supercomputers (ROSS). IEEE, 31â€“40.
[19]Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
(2009).
[20]Lanlan Liu and Jia Deng. 2018. Dynamic Deep Neural Networks: Optimizing
Accuracy-efficiency Trade-offs by Selective Execution. In Thirty-Second AAAI
Conference on Artificial Intelligence.
[21]YanpeiLiu,XinyunChen,ChangLiu,andDawnSong.2016.DelvingintoTransfer-ableAdversarialExamplesandBlack-boxAttacks. arXivpreprintarXiv:1611.02770
(2016).
[22]Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chun-
yangChen,TingSu,LiLi,YangLiu,etal .2018. Deepgauge:Multi-granularity
Testing Criteria for Deep Learning Systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering. 120â€“131.[23]LeiMa,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,FelixJuefei-Xu,ChaoXie,
Li Li, Yang Liu, Jianjun Zhao, et al .2018. Deepmutation: Mutation Testing of
DeepLearningSystems.In 2018IEEE29thInternationalSymposiumonSoftware
Reliability Engineering (ISSRE). IEEE, 100â€“111.
[24]Feng Nan and Venkatesh Saligrama. 2017. Adaptive Classification for Prediction
UnderaBudget.In AdvancesinNeuralInformationProcessingSystems.4727â€“4737.
[25] Nvidia. 2017. Nvidia TX2 User Manual.
[26]Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can
YouTrustYourModelâ€™sUncertainty?EvaluatingPredictiveUncertaintyUnder
DatasetShift.In AdvancesinNeuralInformationProcessingSystems.13991â€“14002.
[27]NicolasPapernot,PatrickMcDaniel,andIanGoodfellow.2016. Transferability
inMachineLearning:fromPhenomenatoBlack-boxAttacksusingAdversarial
Samples. arXiv preprint arXiv:1605.07277 (2016).
[28]NicolasPapernot,PatrickMcDaniel,IanGoodfellow,SomeshJha,ZBerkayCelik,
and Ananthram Swami. 2017. Practical Black-box Attacks against MachineLearning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. 506â€“519.
[29]NicolasPapernot,PatrickMcDaniel,SomeshJha,MattFredrikson,ZBerkayCelik,
and Ananthram Swami. 2016. The Limitations of Deep Learning in Adversarial
Settings.In 2016IEEEEuropeanSymposiumonSecurityandPrivacy(EuroS&P).
IEEE, 372â€“387.
[30]KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. Deepxplore:Auto-
mated Whitebox Testing of Deep Learning Systems. In proceedings of the 26th
Symposium on Operating Systems Principles. 1â€“18.
[31]Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D
Lawrence. 2009. Dataset shift in machine learning. The MIT Press.
[32]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,Ian Goodfellow, and Rob Fergus. 2013. Intriguing Properties of Neural Networks.
arXiv preprint arXiv:1312.6199 (2013).
[33]Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016.
Branchynet:FastInferenceviaEarlyExitingfromDeepNeuralNetworks.In 2016
23rd International Conference on Pattern Recognition (ICPR). IEEE, 2464â€“2469.
[34]Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2017. Dis-
tributedDeepNeuralNetworksovertheCloud,theEdgeandEndDevices.In 2017
IEEE 37th International Conference on Distributed Computing Systems (ICDCS).
IEEE, 328â€“339.
[35]Tensorflow. 2009. Tensorflow Deep Learning Framework. https://www.
tensorflow.org/datasets/catalog/cifar10.
[36]Tensorflow. 2009. Tensorflow Deep Learning Framework. https://www.
tensorflow.org/datasets/catalog/cifar100.
[37]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. Deeptest:Automated
Testing of Deep-neural-network-driven Autonomous Cars. In Proceedings of the
40th international conference on software engineering. 303â€“314.
[38]Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. 2018.
Skipnet: Learning Dynamic Routing in Convolutional Networks. In Proceedings
of the European Conference on Computer Vision (ECCV). 409â€“424.
[39]Wikipedia. [n.d.]. Peak Signal-to-Noise Ratio. https://en.wikipedia.org/wiki/
Peak_signal-to-noise_ratio.
[40]Wikipedia. [n.d.]. Structural Similarity Index Measure. https://en.wikipedia.org/
wiki/Structural_similarity.
[41]Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis,
Kristen Grauman, and Rogerio Feris. 2018. Blockdrop: Dynamic Inference Paths
in Residual Networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. 8817â€“8826.
[42]Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. 2020. Self-training
with Noisy Student Improves Imagenet Classification. In Proceedings of the
IEEE/CVF Conference onComputer Vision and Pattern Recognition. 10687â€“10698.
[43]XiaofeiXie,LeiMa,FelixJuefei-Xu,MinhuiXue,HongxuChen,YangLiu,JianjunZhao,BoLi,JianxiongYin,andSimonSee.2019. DeepHunter:aCoverage-guided
FuzzTestingFrameworkforDeepNeuralNetworks.In Proceedingsofthe28th
ACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis.146â€“
157.
[44]Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. 2020.
Resolution Adaptive Networks for Efficient Inference. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2366â€“2375.
846
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:18:33 UTC from IEEE Xplore.  Restrictions apply. 