Avgust: Automating Usage-Based TestGeneration
from VideosofAppExecutions
Yixue Zhao‚àó
yixuezhao@cs.umass.edu
Universityof Massachusetts Amherst
USASaghar Talebipour‚àó
talebipo@usc.edu
Universityof SouthernCalifornia
USAKesina Baral
kbaral4@gmu.edu
GeorgeMason University
USA
HyojaePark
hyoj.p20@gmail.com
Sharon HighSchool
USALeonYee
leon.yee000@gmail.com
Valley Christian HighSchool
USASafwatAli Khan
skhan89@gmu.edu
GeorgeMason University
USA
Yuriy Brun
brun@cs.umass.edu
Universityof Massachusetts Amherst
USANenad Medvidoviƒá
neno@usc.edu
Universityof SouthernCalifornia
USAKevinMoran
kpmoran@gmu.edu
GeorgeMason University
USA
ABSTRACT
WritingandmaintainingUItestsformobileappsisatime-consuming
and tedious task. While decades of research have producedauto-
matedapproachesforUItestgeneration,theseapproachestypically
focuson testing forcrashesor maximizing code coverage.By con-
trast,recentresearchhasshownthatdevelopersprefer usage-based
tests, which center around specific uses of app features, to help
supportactivitiessuchasregressiontesting.Veryfewexistingtech-
niques support the generation of such tests, as doing so requires
automating the difficulttask of understanding thesemantics of UI
screensanduserinputs.Inthispaper,weintroduce Avgust,which
automates key steps of generating usage-based tests. Avgustuses
neuralmodelsforimageunderstandingtoprocessvideorecordings
of app uses to synthesize an app-agnostic state-machine encoding
ofthoseuses.Then, Avgustusesthisencodingtosynthesizetest
casesforanewtargetapp.Weevaluate Avguston374videosof
common uses of 18 popular apps and show that 69% of the tests
Avgustgenerates successfully execute the desired usage, and that
Avgust‚Äôsclassifiers outperform the state ofthe art.
CCS CONCEPTS
¬∑Software and its engineering ‚ÜíSoftware notations and
tools.
KEYWORDS
TestGeneration,UIUnderstanding, AI/ML, Mobile Application
‚àóBothauthorscontributed equally to the paper
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11...$15.00
https://doi.org/10.1145/3540250.3549134ACMReference Format:
YixueZhao,SagharTalebipour,KesinaBaral,HyojaePark,LeonYee,Safwat
Ali Khan, Yuriy Brun, Nenad Medvidoviƒá, and Kevin Moran. 2022. Avgust:
AutomatingUsage-BasedTestGenerationfromVideosofAppExecutions.In
Proceedingsofthe30thACMJointEuropeanSoftwareEngineeringConference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE‚Äô22),
November 14≈õ18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
13pages.https://doi.org/10.1145/3540250.3549134
1 INTRODUCTION
Writing UI tests is time-consuming and tedious. The research com-
munityhascontributedalargebodyofworkthataimstoautomati-
callygenerateUItests[ 15,20,30,34,39,42,46,50,59,74,80].Such
testingtechniquesgenerateatest‚Äôsinputs,anduseapre-defined
criterionasthetest‚Äôsoracle.Asignificantportionofrecentwork
on UI test generation has focused on mobile platforms and has
predominantly aimedto discover crashesor maximize code cover-
age [30,34,59,65,74,80]. However, studies have repeatedly found
thatexistingtestingtechniquesinthisdomainfallshortinaddress-
ing developers‚Äô needs in practice [ 43,52] or present challenges for
practical adoption [ 35,52].
Specifically, mobile developers have a strong preference for test
cases that are closely coupled to app use cases or features [ 52].
In line with recent work [ 85], we refer to this type of preferred
test case as usage-based UI test. A usage-based UI test consists
of a sequence of UI events that mimic realisticuser behaviors in
exercisingaspecificfeatureofagivenapp,suchas≈Çaddinganitem
to the shopping cart.≈æ The developer preference for usage-based
testsisduetothefactthatsuchtestcasessupportspecifictesting
goals in practice, such as regression or performance testing, which
inturnrequire orientationto common app use cases[ 52].
Automating such testing activities is critical for mobile devel-
opers who face unique challenges relatedto rapidly evolving plat-
forms [18,51], pressure for frequent releases [ 38,41], and a deluge
offeaturerequestsandbugreportsfromuserreviews[ 25,29,67,68].
Despitetheimportanceoftheseusage-basedtestsfordevelopers,
current automated testing approaches typically do not consider
421
ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Y. Zhao, S.Talebipour, K.Baral, H.Park, L.Yee, S.Khan, Y. Brun, N.Medvidovic, andK.Moran
appusagesasagoalortestadequacycriteria,andassuchcannot
generatethesetests [ 52,85].
A growing body of research on the topic of UI test reuse (also
sometimescalledtestmigration,testtransfer,ortestadaption[ 19,
20,50,62,71])hasbeguntoexplorethepossibilityofautomating
the transfer and adaptation of existing usage-based tests from a
source app to a behaviorally similar target app that contains shared
features [ 19,20,39,50,61,62,71,85]. However, these test-reuse
techniqueshave threenotablelimitations thatposechallenges for
developers to adopt in practice. 1To generate tests for a target
app, UI test reuse requires pre-existing, manually-written tests for a
correspondingsourceapp.Inpractice,creatingthesesourcetestsis
time-consuming and error-prone, leading many mobile developers
to forgo writing them [ 43,52].2Test-reuse techniques have typi-
callybeendesignedfor,andtaskedwith,transferringtestsbetween
behaviorally similar applications from similar domains (e.g., be-
tweentwofinanceortwoshoppingapps).However,therearemany
use cases common across apps from varying domains (e.g., logging
in or changing the theme), which current test techniques would
struggle to effectively transfer. 3Many existing techniques rely
onexpensiveanddifficulttouseprogramanalyses (e.g.,bytecode
decompilers, Soot [ 9,78], Gator [2,83]) that often require access to
an app‚Äôs source code. The ease of use and scalability limitations of
such underlying utilities have hindered the adoption of test case
transfer toolsinpractice.
Tohelpbetteralignautomationrelatedtousage-basedtesting
with developers needs, we propose Avgust, a technique for app-
video-based generationof usagetests.Avgustisanoveldeveloper-
in-the-loop test generation technique that directly addresses the
three limitations mentioned above. 1Instead of requiring pre-
existing source tests written by domain experts, Avgustallows
for easy creation of source test scenarios through screen recordings
of app usages ,which are becomingincreasingly commonsoftware
artifacts for mobile apps [ 26] and can be easily obtained via crowd
workerswithnotestingexpertise.Aftervideocollectionandpro-
cessing,Avgustoperatesaccordingtotwomainphases.Inthefirst
phase,neuralcomputervision(CV)andnaturallanguageprocessing
(NLP)techniquesareemployedtoguidedevelopersthroughalight-
weight screen and GUI widget annotation process for video frames
that were automatically identified to contain a touch action. Using
thisinformation, Avgustisabletogenerateanapp-independent
intermediate-representationmodel( IRModel),whichrepresentsab-
stractstatesandtransitionsofausagethatcanbemappedtomulti-
pleapps.Thisprocedureisaone-timeeffortfordevelopers,andonce
theIRModelisgenerated,itcanbeusedtogeneratetestsformulti-
pletargetapps. 2ThegeneralityoftheIRModelallows Avgustto
synthesizetestscenariosacrossdomains,effectivelyovercomingthe
secondlimitationofexistingtest-casetransfertechniques. Avgust‚Äôs
second phase automates the synthesis of new UI test scenarios by
guidingadeveloperwithsuggestions,madebyusingpredictions
fromAvgust‚Äôs CV and NLP techniques, of which GUI elements
mustbemanipulatedtoexerciseagivenappfeatureorusage. 3To
bolstertheapplicabilityandpracticalityof Avgustfromadevel-
oper‚Äôsperspective, Avgustoperatespurelyonvisualinformation en-
codedintoscreenshotsandvideoframesfromUI-screenrecordings.Assuch,itdoesnotrequireaccesstoanapp‚Äôssourcecode,instru-
mentation,orexpensiveprogramanalyses.Notethatsolelyrelying
onappvideosasinputisakeyaspectof Avgust‚Äôsnoveltyandit
hasthreemajoradvantages.First,videosarecommonartifactsthat
are easily collectible withoutrequiring difficult toolconfiguration
andsetup,whicharemajorbarriersforadoption[ 43,52].Second,
videos can be collected by crowd workers (e.g., real users) with no
testingexperience,enablingtheopportunityto obtainmuchmore
trainingdatatocoverdiverseandrealisticusagescenariosacross
different apps. This can yield more generalized models to generate
higher-quality tests. Finally, videos are agnostic to the underlying
device and platform, meaning Avgust‚Äôs design is not tied to An-
droidplatforms(where Avgustisevaluatedon),butisapplicable
to any apps, devices, andplatforms (e.g.,websites) inprinciple.
Thekeyresearchchallengethat Avgusttacklesistheautomated
synthesisofageneralizedmodeloffeatureusagesthatcaneffec-
tively map test scenarios across apps from a variety of domains,
usingonlyscreenshotsandvideoframesfromscreenrecordings.
Thechallengeliesinautomatingtwokeytasks:(1)screenunder-
standingfrompixelsand(2)designofanIRModelthatisgeneral
enough to capture diverseapp usages yet specific enough to allow
mapping actions to a given target app for test scenario genera-
tion.Avgustaccomplishes the first task through the creation of
abespokeimageclassificationtechnique,builtontopofaneural
auto-encoderrepresentation[ 69]andBERT-basedtextualembed-
dings [28]. The classification operates at twogranularity levels, (i)
screen-level,and(ii)GUIwidget-level.Thisclassificationprocedure
helps to provide the mapping to our IR Model, and makes use of
arichscreen representationobtained by training our neuralauto-
encoder on the public RICO dataset [ 27].Avgustaccomplishes
the second task by using the information from our classifiers to
buildastatemachinecapableofsimultaneouslycapturingmultiple
scenariosfromdifferentusages.Thisyieldsarichermodelofapp
usage thanpasttest transfer techniques.
In order to build a community resource of usage-based tests,
we conducted a user-study to collect 374 video recordings from 18
apps, covering 18 usage scenarios, wherein each usage scenario
isexercisedbythreeassociatedapps,foratotalof54uniqueapp-
usage pairs [ 39,85]. Using this data, we conducted an empirical
evaluation to measure the efficacy of Avgustin generating usage-
based tests that closely mirror those created by humans during our
user study. First, we examined Avgust‚Äôs test generation capability
bysimulatingadeveloperinteractingwith Avgust‚Äôssuggestions,
andmeasuredhowcloselygeneratedtestsmatchedanalogoustests
created by human users. Next, to gain a better understanding of
Avgust‚Äôs performance during test generation, we evaluated Av-
gust‚Äôsclassifierscomparedtostate-of-the-arttechniques.Ourre-
sults show that Avgustis able to generate tests that effectively
exercisetarget-appfeaturesandcloselymatchhumantestsinterms
ofthescreensvisitedandactionsperformed.Additionally, Avgust‚Äôs
classifiers significantly outperform state-of-the-art techniques and
showpromisingperformancefor our generated developer-in-the-
looprecommendations.
We have developed Avgustwith open science in mind, making
it both practical to use for developers, and easily reusable by re-
searcherstofosterfutureresearchinthisarea.Wemakepublicly
422Avgust: AutomatingUsage-BasedTestGenerationfrom Videosof App Executions ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
availableallofoursourcecode,trainedmodels,andannotatedeval-
uationdatacollectedduringouruserstudy[ 10].Avgust‚Äôspipeline
can be easily adapted to create various usage models of interest
bysimply changing the inputvideos, such as includingadditional
usagesandapps.Assuch, Avgustnotonlylaysafoundationfor
futureworkonusage-basedtestgeneration,butalsorepresentsa
living repository for the software engineering community to study
relatedproblems.
In summary,this paper makesthe following contributions:
(1)We introduce Avgust, the firsttechnique capable of gener-
atingusage-based tests bylearningfrom app videos.
(2)We develop a novel image classificationtechnique to trans-
late app videos into an app-independent intermediate repre-
sentationbasedonvision-onlyinformation,whichlargely
outperforms the state-of-the-art.
(3)WeimplementareusablepipelinetotrainIRmodelsbased
on app videos that can be applied to various downstream
tasks,andfurtherprovide125pre-trainedmodelsthatcan
be usedbydevelopers directly.
(4)We collect 374 app videos and conduct an empirical eval-
uation to demonstrate Avgust‚Äôs effectiveness in assisting
developers withgeneratingusage-basedtests.
(5)We provide a public repository [ 10] that contains Avgust‚Äôs
artifactstofosterfutureresearch,including Avgust‚Äôssource
code, our pre-trained models, labeled datasets,benchmarks
used,andtheircorresponding results.
2 THE AVGUST APPROACH
Avgustis an automated approach that aims to assist developers
withthegenerationofusage-basedteststomimicrealisticusage
scenarios. Avgustoperatesinthreephases.(1)Itprocessesrecorded
videos of different apps‚Äô usages by applying neural CV and NLP to
detect user actions in individual video frames. (2) Avgustuses this
information to generate anapp-independent statemachine-based
IRModel.(3)Finally, AvgustleveragestheIRModeltogenerate
tests for a new (i.e., ≈Çtarget≈æ) app. In this section, we provide an
overviewof Avgust‚Äôsworkflow,andthen detailits three phases.
2.1AvgustOverview
Avgustfunctionsasahuman-in-the-looptooltoprovidesugges-
tions of input events for developers in the creation of usage-based
tests.Thisdesigndecisionisguidedbythenatureof usage-based
tests,sinceeachusage scenario may have various correctways of
being tested. For instance, there may be different ways toexecute
the login scenario in an app, such as logging in using username
and password or by using user‚Äôs existing social media accounts.
Thus, providing suggestions to a developer allows for flexibility in
generatingtests thatare tailoredto agivenappusageand testing
objective.Figure 1depictsAvgust‚Äôsworkflow,whichconsistsof
threeprincipalphases: ‚ë†VideoCollection&Analysis ,‚ë°IRModel
Generation ,and ‚ë¢Guided Test ScenarioGeneration .
During the Video Collection & Analysis phase, crowdsourced
workers are tasked with collecting videos of app usages. These
videosarethenanalyzedinafullyautomatedprocessthatinvolves
deconstructingthevideointoconstituentframes,identifyingtouch-
basedactionsthatwereperformedontheanapp‚ÄôsUI(whichbuildsuponpastwork in app-videoanalysis [ 21]), andeliminating sensi-
tive information such as userpasswords.
Next, in the IR Model Generation phase,Avgustassists a de-
veloper with labeling screens and individual GUI widgets from
processedvideoframesintocategories,which Avgustcanthenuse
togenerateanapp-independentIRModel.Thisisasemi-automated
processwhereinadeveloperispresentedwithascreenand Avgust
provides top-k suggestions for the labels that should be applied
to both screens an exercised GUI widgets. These suggestions are
made using a combination of visual- and text-based classifiers that
operateuponthevideoframesextractedinthepriorphase.After
the labels have been applied by the developer, Avgustis able to
automaticallygeneratethestatemachine-basedIRModelforthe
usage, merging it with other similar usages in a shared database.
Thisphaseisintendedtobeaone-timecost,whereindevelopers
contribute their crowdsourced IR Models of various app usages to
acollective community databasefor future use.
Finally, in the Guided Test Scenario Generation phase,Avgust
assists developers by providing top-k recommendations for actions
thatshouldbe performedongiven screens of a previously unseen
targetapp inordertoexercise aspecifiedappfeature (e.g.,adding
anitemtotheshoppingcart).Thisprocessstartsfromtheinitial
screenoftheappandrunsuntilthespecifiedfeatureisexercised.
ThisfunctionssimilarlytotheIRModelgenerationphase,butin
reverse order: the model is used in conjunction with Avgust‚Äôs
classification techniques to recommend event inputs to developers.
2.2 Video Collection& Analysis
Givenasetofcollectedvideosofappusages, Avgust‚Äôsvideoanaly-
sis processes them into frames that serve as the inputs for Avgust‚Äô
AI-assisted IR Model generation (Section 2.3). Specifically, Avgust
firstidentifiestheuseractionsinthevideosandextractstheircorre-
sponding eventframes ,whicharethekeyvideoframesthatcapture
the user interactions via the touch indicator .1An example of event
frame is shown in Figure 3, where the touch indicator points to
the user interacting with the ≈Çapp menu≈æ button in the top-left
corner.Asafinalstep, Avgustfilterstheextractedeventframesby
eliminating the framesthat contain sensitive userinformation.
2.2.1 ActionIdentification&EventFrameExtraction. Avgustbuilds
upon the analyses introduced by V2S [ 21,36] to identify user ac-
tions and event frames. V2S is a recent technique that leverages
neuralobjectdetectionandimageclassificationtoidentifytheuser
actions in a video, and automatically translates these actions into a
replayablescenario.WeextendedV2StoworkwithGPUclusters,
to enable it to process large numbers of videos in parallel. The out-
puts of our extended video processing technique are (1) a sequence
ofeventframes ofagivenvideoandtheirassociateduseractions
(i.e., click, long tap, and swipe); and (2) the coordinates of the touch
indicator ineacheventframe[ 45].
2.2.2 Event Frame Filtering. Avgustfilters the extracted event
framesbyeliminatingtheframesthatareassociatedwiththe typing
action,sincetheymayexposeuser‚Äôsprivateinformationsuchas
password. Note that Avgustonly eliminates the frames where the
1Avgustrequiresenablingthedisplayofthe touchindicator ,whichcaneasilybedone
in,both,the Android and iOS settings menus, even by inexperienced users.
423ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Y. Zhao, S.Talebipour, K.Baral, H.Park, L.Yee, S.Khan, Y. Brun, N.Medvidovic, andK.Moran
Crowd 
Testers Videos of
App Usages
Video Collection & Analysis1
Video Processing,
Action Parsing,
& Keyboard 
DetectionIR Model Generation 2
Input 
Video 
Frames
Car Data Entry
Password
Item Entry
Screen
Developer 
IR Refinement
Community 
Database of
IR ModelsGuided Test Scenario Generation3
Running
Target App
Car Data Entry
Password
Item Entry
Screen
Assisted Input 
Generation
Usage-
based
Scenario
IR 
Models
IR Model 
Database
IR 
Model
Image Features
 Textual Features
IR Classifier
Image Features
 Textual Features
IR Classifier
Figure 1: Avgust‚Äôsthree-phase workflow.
usertypesthetextcontentonakeyboard,butstillkeepstheframes
where the user selects which input field she intends to enter the
text content. For example, the sequence of video frames related
to≈Çtypinguserpassword≈æconsistsof(1)aframeassociatedwith
clickingthepasswordfield,and(2)agroupofconsecutiveframes
associatedwithtypingeachindividualcharacterinthepassword.
Avgustonly eliminates the latter, while maintaining the former to
represent ≈Çtyping userpassword≈æ action inthe usage scenario.
Todosoautomatically,wetrainedabinaryimageclassifierto
recognize whether an event frame contains a keyboard image. Our
classifierisbasedonaCNNarchitecturewith4blocks,eachcon-
sisting of a Convolution, a BatchNorm, a ReLU, and a Dropout
layer [79]. The CNNis trained on cropped screenshots that depict
the area of the screen where keyboard may appear, since this area
is standard for mobile devices, as shown in Figure 2. We decided to
focusontheregionofthescreenwherethekeyboardappears,as
opposed to the entirety of the screen, based on empirical evidence
collected while tuning our classifier, as the former setting dramati-
cally improved the classifier‚Äôs accuracy. We sourced non-keyboard
training data by randomly selecting 4,926 app screenshots without
the keyboard from the publicly available RICO dataset [ 27]. The
non-keyboardtrainingdatadonotcontainanysubjectappsused
in our evaluation. Becausescreens thatdisplay a keyboardcannot
be automatically identified using the GUI metadata provided in the
RICOdataset,weadditionallysourced5,605 keyboard trainingdata
images from the video frames in the dataset collected for Avgust‚Äôs
evaluation. Our training data relies on standard Android keyboard
images, but can be easily extendedto additionalkeyboardtypes.
Next,thecroppedscreenregionwhereakeyboardmayappear
foreacheventframeisfedtothetrainedkeyboardclassifier,and
willbeclassifiedaseithera keyboard ornon-keyboard frame.For
thekeyboardframes, Avgustfurtherverifieswhethertheassoci-
ated action is typing, based on whether the touch indicator falls in
the keyboard region. This is determined by the touch indicator‚Äôs
coordinatesonthescreen[ 45],whichareobtainedfromV2S.Inthe
end, the eventframesthat contain atyping action are eliminated.
Figure2:Examplesofthetrainingdatausedin Avgust‚Äôskey-
board classifierduringtheeventframefiltering.Note that this frame filtering process not only addresses the
privacyissueasdiscussedearlier,italsolargelyreducesthenumber
ofeventframesusedtorepresentanapp‚Äôsusage.Forexample,atwo-
minute sign-in video from the app 6pm contained over 3,000 video
framesoriginally,butonly8 filteredeventframes.These8frames
are sufficient to represent all relevant user actions without the
duplicated or privacy-exposing frames in the original video frames.
2.3 AI-Assisted IRModel Generation
Avgustuses the filtered event frames from the previous phase
asinputsandtranslatesthemintoapp-independentIRModelsof
appusages.Thekeytechnicalchallengeinthisphasestemsfrom
Avgust‚Äôs use of video inputs, which forces us to rely solely on
visual information encoded into the pixels of the video frames. We
addressthischallengeintwosteps:(1)webreakdowneventframes
intoGUIeventsand(2)useimageclassificationtechniquestoassist
developers in translating GUI events to their corresponding IR
Models. Asan illustration, Figure 3demonstrates thekey artifacts
in this process using a single event frame extracted from a popular
shoppingapp 6pm as an example.We nowdetailthesetwosteps.
2.3.1 TransformingEventFramestoGUIEvents. Wedefinea GUI
eventas a triple (ùë†,ùë§,ùëé), whereùë†is the app screen that shows a
snapshot of the app‚Äôs execution state; ùë§is the GUI widget the user
interactswith;and ùëéisthecorrespondingactiontheuserperforms,
suchasclickorswipe.TheGUIwidget wisoptionalsincecertain
actions(e.g., swipe)arenotassociatedwithanywidgets. Avgust
converts event frames into GUI event triples in a two-step process:
widgetextractionandaction identification.
Widget Extraction. Extractingindividualwidgetsfromascreen
presentstwomajorchallenges.First,eachwidget‚Äôsboundingbox
mustbeidentifiedwithoutrelianceonsourcecode-levelinforma-
tionthatisavailableonplatformslikeAndroid[ 7].Second, Avgust
Event Frame screen: 
"home"......widget: 
"menu"
action: 
"click"
screen s
widget w
GUI Event
(s, w, a )"click"
action aState1Transition1
IR ModelState2
Figure 3: An example of converting a 6pm‚Äôs Event Frame
into aGUIEventtriple,and an app-independent IR Model.
424Avgust: AutomatingUsage-BasedTestGenerationfrom Videosof App Executions ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
needstoisolatethepreciseboundingboxofthewidgetwithwhich
the user is interacting, such as the app-menu button in the top-left
corner ofthe screen inFigure 3.
To detect the bounding boxes of GUI widgets, we modified
UIED[24,82],astate-of-the-arttoolthatcombinesunsupervised
CV and deep learning, and applied it on the screens extracted from
Avgust‚Äôs previous phase. Given an input screen, UIED detects tex-
tualandvisualGUIelementsandproducestheirboundingboxes,
as depicted with solid rectangles in Figure 4. However, UIED treats
each visual and textual GUI element separately, which can lose im-
portantsemanticinformation.Forexample,ifthetouchindicator
referstoacheckbox,thecorrespondingGUIelementdetectedby
UIEDinFigure 4wouldbeoneofthetwocheckboxesonly,leaving
unclear whether the extracted widget is intended to be associated
with≈ÇShowpassword≈æ or≈ÇKeepme signedin≈æ.
Figure4: Avgustadjusts
the GUI-element bound-
ing boxes detected by
UIED, depicted by the
twodashed rectangles.To remedy this, we modified
UIED to group the visual GUI el-
ements together with their sur-
roundingtextualelements,ifany.
Avgustiterates through all vi-
sualelementsdetectedbyUIED
and identifies their closest GUI
elements. If the closest GUI ele-
ment is both a textual element
andisinthesamelineasthevi-
sual element√êdefined as being
vertically collocated based on a
customizablethreshold√êthenthe
bounding box of the visual ele-
mentwillbeupdatedtoinclude
the textual element as well. In
Figure4, this results in the two
checkboxesbeinggroupedwith
their corresponding labels, as de-
pictedbythe dashedrectangles.
Next, the detected GUI ele-
ments‚Äô bounding boxes are used
byAvgusttoautomaticallycrop
out the widget ùë§to which the
touch indicator refers. Avgust
combinesthewidgetsdetectedbyitsmodifiedUIEDwiththeco-
ordinates of the touch indicator obtained from the modified V2S
(recall Section 2.2) to identify all candidate widgets for cropping,
coveringthreepossiblecases:(1)Thesimplestcaseiswhenonly
one widget‚Äôs bounding box covers the touch indicator, in which
caseAvgustcrops that widget as-is. (2) If no widgets‚Äô bounding
boxescoverthetouchindicator, Avgustrepeatedlyexpandseach
widget‚Äôs bounding box based on a customizable threshold, until
asuitablewidgetisfound. Avgust‚Äôsdefaultthresholdissetat10
pixels. (3) When multiple widget candidates are found, Avgust
first eliminates the ≈Çcoarse-grained≈æ candidates whose boundaries
completelycoveranyoftheothercandidates(e.g.,≈Çsign-inform≈æ
that covers ≈Çusername≈æ widget), and then selects the widget whose
centerpointisclosestto the touch indicator‚Äôscoordinates.
Action Identification. To identify the action ùëéin the GUI event
triple,AvgustleveragesV2S‚Äôsactionidentificationprocedure,whichhome menu sign_in endmenu#click account#click sign_in#clickself[email#click & password#click & up & keep_signin#click]
Figure 5: The IR Model generated from a sign-in video col-
lected fromtheapp 6pm.
analyzesthecoordinatesoftouchesdetected inconsecutivevideo
framesandclassifiesactionsaccordingtoasetofheuristics[ 21,36].
V2Sisabletoidentifyclicks,longtaps,andswipes.WereusedV2S‚Äôs
heuristicsforclickandlongtap,andextendeditsswipe detection
heuristicto additionallydetectthe directionof the swipe.
2.3.2 TransformingGUIEventstoIRModels. Avgust‚ÄôsIRModel
generationisadeveloper-in-the-loopprocess.Thissectionexplains
howAvgustprovides recommendations to assist developers in
translatingGUIeventsintotheirapp-independentIRrepresenta-
tions(recallthe example inFigure 3).
Avgust‚Äôs IR Model is defined as a finite state machine (FSM)
that captures app usages. Figure 5shows an example IR Model
converted from one of 6pm‚Äôs sign-in videos. Each state in the IR
Model represents a particular app screen and is captured as an
app-independent canonical screen , while each transition represents
a user interaction with a canonical widget and its corresponding
action.Aself-transition(e.g.,showninthe≈Çsign_in≈æstateinFig-
ure5) means that the app stays on the same screen during certain
userinteractions.
The key challenge in translating a GUI event triple (ùë†,ùë§,ùëé)into
Avgust‚ÄôsIR Model is toproperly abstract away and capture in an
app-independent manner the app-specific screens ùë†and widgets ùë§.
Eachaction ùëéintheGUIeventtripleistranslatedas-is,including
click,longtap,andswipeup/down/left/right.Withthetranslated
canonicalscreens,canonicalwidgets,andactions,thefinalIRModel
canbeconstructedbyiteratingthroughthesequenceofGUIevents
ofaparticularusage.
We formulate the translation of screens and widgets as a clas-
sification problem, where app-specific screens and widgets are
classified into their canonical counterparts (categories) that are
sharedacrossdifferentapps.Tothisend,webuilduponandextend
app-independent categories definedby previous work[ 39,85], re-
sulting in 37 canonical screens and 74 canonical widgets. Example
canonical screens are ≈Çhome screen≈æ, ≈Çpassword assistant page≈æ,
and≈Çshoppingcartpage≈æ.Examplecanonicalwidgetsare≈Çaccount≈æ,
≈Çhelp≈æ,and≈Çbuy≈æ.Thecompletesetsofcanonicalscreensandwid-
getsareavailable[ 10].Notethatthesecategoriesarenotdirectly
tiedtooursubjectappsusedintheevaluation,butcangeneralize
across diverse apps. To facilitate the extensibility of new canonical
screens/widgets,wehavecreatedadatalabelingtoolusingLabel
Studio [12], allowing future research to further tailor and improve
the canonicalcategoriesfor both screens andwidgets.
We note that our classification problem provides a unique chal-
lenge as it relies only on information from app screenshots. There
are no existing techniques in the mobile-app domain that have
previouslyaddressedthisproblem[ 27,39,47]inacontextsimilar
to ours. Moran et.al. [ 64] were one of the first to use a CNN for
widget classification from GUI component images. However, their
classifier only functioned on 15 general widget categories. Our
425ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Y. Zhao, S.Talebipour, K.Baral, H.Park, L.Yee, S.Khan, Y. Brun, N.Medvidovic, andK.Moran
larger number of 74 categories represents a more challenging clas-
sificationproblemthatrequirestheuseofbothtextualandvisual
features to achieve reasonable accuracy. Another recent technique
for screen classification in this domain is Screen2Vec (S2V) [ 47],
which aims to produce embeddings for app screens that can be
used for downstream classification tasks, such as ours. However,
S2V cannot be applied on screenshots alone as it requires the UI
layout information of an app screen that is specific to the Android
platform [ 7]. Obtaining such information requires extraction using
third-party tools (e.g., Appium [ 4], UIAutomator [ 1]), and would
sacrificethe practicalityofusage collection throughvideos.
Screen Classifier. To classify a given app-screen image into its
canonical screen, Avgustleverages both visual and textual fea-
tures, wherein textual features are extracted from the image using
theTesseractOCRengine[ 73].Moreprecisely,wemakeuseofa
pre-trained autoencoder model to encode the screen‚Äôs visual in-
formation, anda pre-trained BERT language model [ 28] to encode
the screen‚Äôs textual information. Avgustuses a three-layer con-
volutionalautoencoderwithmaxpooling[ 33]andistaskedwith
encoding an image into a high dimensional vector space, and then
decoding the image vector to reconstruct the original image, hence
employingaself-supervisedtraining process.
As past work has shown [ 48], learning features or patterns di-
rectly from the pixels of UI screens can be difficult due to the
variability in GUI designs across apps. Therefore, to train and
use our auto-encoder to learn app-agnostic visual patterns, we
re-implementedthescreensegmentationapproachintroducedby
REMAUI[ 66],andusethesegmentstogenerateabstractedversions
of screens from the RICO dataset [ 27]. As illustrated in Figure 6, in
these abstracted screens text components are transformed into yel-
lowboxesandnon-textcomponentsintoblueboxes,onablackback-
ground.Wetrained Avgust‚Äôsautoencoderon33,000abstractedim-
agesfromtheRICOdataset[ 27],andtoclassifyanincomingscreen,
we run it through this abstraction process, and then through the
encoder ofour autoencoder network to extract the feature vector.
Avgust‚Äôsscreenclassifierleverageslinearlayerstocombinethe
autoencoderandBERTembeddingsandclassifythescreens.The
architectureforthescreenclassifierconsistsofthreeblocks,each
containingalinearlayer,BatchNorm,aReLUactivationfunction,
andadropoutlayer.Theseblocksarefollowedbyafullyconnected
outputlayerthatappliessoftmaxfunctiontopredicttheprobability
distribution of different screen classes. We then train the screen
classifier onpartitions of data collected for ourevaluation, intro-
duced in Section 3.1, where individual classifiers for each app were
trained on data sourced from other apps. This process produced 18
pre-trained screen classifiers for each of our subject apps, and will
be reusedin Avgust‚Äôstest generationphase(see Section 2.4).
WidgetClassifier. To classify a given app-widget image, Avgust
leveragesitstextual,visual,contextual,type,andspatialinforma-
tion:(1)thewidget‚Äôstextisextractedfromthewidget‚Äôsimageusing
Tesseract[ 73]andthenencodedusingthepre-trainedBERTmodel;
(2) the canonical screen of the screen image to which the widget
belongs is mapped to an idand transformed into a continuous
vector via an embedding layer; (3) the visual features of the widget
areencodedwiththepre-trainedResNetmodel[ 37],widelyused
for encoding images; (4) the UI widget class type (e.g., EditText,
Original	GUI	Screen Abstracted	GUI	Screen
Figure 6: Avgust‚Äôsscreen abstractionprocess.
ImageButton )isobtainedusingtheclassificationmethodintroduced
byReDraw[ 64]andrefinedbyS2V[ 47],thenmappedbyanembed-
dinglayerintoacontinuousvector;and(5)thewidget‚Äôslocation
onthescreenisobtainedbydividingthescreeninto9zonesand
then transformedto acontinuous vector viaan embedding layer.
Avgust‚Äôs widget classifier then adapts a similararchitecture to
itsscreenclassifier√êthreeblocksoflinearlayersfollowedbyafully
connectedoutputlayerapplying softmax√êtocombine thedifferent
featurevectorsdiscussedabove.Notethattheembeddinglayersfor
thecanonicalscreen,widgetlocation,andwidgetclasstypefeatures
are optimized duringthetraining phase ofthewidget classifierto
generatemeaningfulembeddings for eachof theseinputfeatures.
Similarly to the screen classifier, Avgust‚Äôs widget classifier is
trained on our dataset (Section 3.1), and produces 18 pre-trained
models that are reusedin Avgust‚Äôstest generationphase.
Avgust‚Äôs screen and widget classifiers are able to provide the
standard top-k labels with different confidence levels. The top-k
labels are then recommended to developers in labeling the screens
andwidgetsand,inturn,theIRModelsareconstructedusingthe
specificlabelsselectedbydevelopers.NotethattheIRlabelsrefined
bydevelopersandthegeneratedIRModelscanbereusedbyfuture
work. We have thus created a database [ 10] to serve as a living
repository for this problem domain,as alsodepictedinFigure 1.
2.4 Guided TestScenarioGeneration
Avgustassists developers in generating usage-based tests for their
(≈Çtarget≈æ) apps by leveraging the IR models described above. Given
ausageofinterest, AvgustselectstherelevantIRModel(s)from
the IR Model Database (recall Figure 1), and uses them to guide
the test generation. Internally, IR Models of the same usage are
represented as a single merged model where multiple scenarios
for a given usage populate the same state machine. Specifically,
the merged model is constructed by using the union of all the
edges from all the IR Models of the same usage, demonstrating
≈Çall possible transitions≈æ. A simplified example of the merged IR
model for the sign-in usage is shown in Figure 7. This unified
modelofscenariosforasingleusagegives Avgusttheabilityto
generate multiple test scenarios for a target usage on an unseen
app.Avgust‚Äôstestscenariogenerationphasehasthreeprincipal
components: (1) State Extractor, (2) State Matcher, and (3) Event
Generator.Wefirstdescribethetestgenerationworkflow,andthen
discuss the three components.
426Avgust: AutomatingUsage-BasedTestGenerationfrom Videosof App Executions ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
StartHome
Get StartedMenu
Shopping Cart
Sign In/Sign UpSign In
EndSign in with 
Amazon
Initial
Initialcart#tapmenu#tap
get started#tapmenu bookmark#tap |
menu account#tap
to sign in/sign up #tap
to sign in/sign up #tapself [password #tap |
keep sign in#tap |
sign in#tap
‚Ä¶]self [password#tap |
keep sign in#tap]
back#tapby amazon #tap
sign in#tap
Figure 7:Asimplified example ofthemerged IR Modellearned fromthree sign-invideosofthe 6pmand Etsy apps.
Avgust‚Äôstestscenariogenerationphaseisaniterativeprocess
that continues to generate the test inputs based on the target app‚Äôs
currentstate,untiltheendconditionismet(i.e.,thetargetfeatureis
executed). The process begins by launching the target app and run-
ningAvgust‚ÄôsScreenClassifiertoretrievethemostlikelycanonical
categoriesofthetargetapp‚Äôsstartingscreen. Avgustthenpresents
thedeveloperwiththesetop-kclassificationresults,andthecanon-
ical category selected by the developer is used to match the target
app‚Äôs current device screen to the canonical screen states in the
IR Model. Next, Avgustrecommends the top-k app widgets for
developerstointeractwithbyusingitsWidgetClassifiertomap
the canonical widgets in the IR Model to the widgets on the target
app‚Äôs current screen. Avgustthen checks whether the test should
complete based on whether the widget chosen by the developer
will lead to the end state in the IR Model. If not, the chosen widget
is triggered, the target app‚Äôs next state becomes its current state,
and this process repeats until the end state in the unified IR Model
for agiven usage isreached.
2.4.1 State Extractor. Avgust‚Äôs State Extractor extends recent
work on the MAPIT [ 77] test case transfer tool. Specifically, for
a given target app Avgustextracts (1) the bitmap of the current
screen,(2)thegraphrepresentationoftheappscreen‚ÄôsUIlayout
hierarchy[ 7],and(3)theboundariesofeachUIwidgetandtheir
corresponding cropped images. The UI layout hierarchy is an XML
filethatcontainstheinformationofalltheUIwidgetsonthetarget
app‚Äôs current screen, such as their position, size, textual attributes
(e.g., ≈ÇSign In≈æ), and class name (e.g., ImageButton ). The extracted
informationisusedby Avgusttogeneratetests,andalsotoexplore
differentvariants of Avgust‚Äôsclassifiers asdiscussed inSection 3.
2.4.2 State Matcher. As discussed previously, Avgustuses its
Screen Classifier to suggest the top-k candidates for the canon-
ical category of a target app‚Äôs given screen. Once the developer
selects from one of the suggested categories, Avgustmaps the
current screen to the corresponding state in the IR Model. Since all
possibletransitions capturedin theIR Models are known, Avgust
is able to recommend the target app‚Äôs widget(s) with which the
developer should interact by using a combination of the Widget
Classifier(recallSection 2.3.2),informationobtainedfromtheState
Extractor(recallSection 2.4.1), andasetofpre-definedheuristics.
The number of widgets to be recommended by Avgustis deter-
mined by a configurable threshold. Avgustfirst checks whether
the target widgets match the expected canonical widgets from theIRModelbasedonasetofheuristicsthatcanbedividedintotwo
categories. The first category are heuristics that infer a widget‚Äôs
typebasedontheUIclassofitsparentwidget.Thisallows Avgust
to bypass the noise that may be present in the data associated with
an individual widget. For example, Avgustidentifies a widget that
represents a menu item, not by trying to capture all possible menu
items, but much more simply by comparing its parent widget‚Äôs UI
class toListView. The second category are heuristics that correlate
thetextualdataofawidgetwithsimilartermsassociatedwitheach
ofthecanonical widgets (e.g., thetermsfromoursetofcanonical
widgets[ 10]discussedinSection 2.3.2andtheir synonyms).
Iftheheuristicsaloneyieldanumberofrecommendationsbelow
the set threshold, as the next step Avgustwill predict the top-1
classification of the canonical category for each interactive widget
on the target app‚Äôs screen. If the target-app widgets that match the
expected canonical widgets in the IR Model bring the total number
ofmatchedwidgetsabovethethreshold,theprocessterminatesand
the identified widgets are presented to the developer. Otherwise,
as the final step,the matching criteria are relaxed andthe process
switchesfromthetop-1tothetop-5classificationsofeachwidget‚Äôs
canonicalcategory.
2.4.3 EventGenerator. Withachosenwidget, Avgustgenerates
anexecutableeventtotriggerbasedonwhetherthewidgetrequires
user input. This is determined by the widget type. For example,
EditText [11] is a widget type that requires an input from the user,
such as entering the email address. In such cases, Avgustprompts
thedeveloperfortextinputs,asthesetypicallydonotgeneralize
across apps. If the selected widget does not require user input, the
EventGeneratorautomaticallyexecutesthetouchevent(e.g.,tap,
swipe)storedinthe transitionof the IR Model.
This event generation process requires minimal effort from the
developerandprovidestheflexibilitytotestthesameusagewith
different desired text inputs of the developer‚Äôs choice. A test sce-
nario is generated when the end condition is met, as discussed
earlier,andeachtestconsistsofasequenceofeventstriggeredby
the EventGenerator.
2.5Avgust‚ÄôsImplementation
Avgustis implemented in Python with 10,700 SLOC, of which
thescreen andwidgetclassifiersarestand-alonemodulestotaling
2,800 SLOC, and include the autoencoder model we developed. Av-
gustadditionally extended several research tools, including the
modifiedV2S(500SLOCinPython),UIED(300SLOCinPython),
427ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Y. Zhao, S.Talebipour, K.Baral, H.Park, L.Yee, S.Khan, Y. Brun, N.Medvidovic, andK.Moran
and the re-implemented REMAUI (5,000 SLOC in Java). Avgust
employs the pytransitions library [70] to manipulate its state-
machine IR Models, and uses the Appium testing framework [ 4]
for its test generation.
3 EVALUATION OF AVGUST
Todemonstrate Avgust‚Äôseffectivenessatgeneratingusagetests,
anditsimprovementonthestateoftheart,weanswertworesearch
questions:
RQ1How effective is Avgustat generating tests that
exercise the desiredusage?
RQ2How accurate are Avgust‚Äôs vision-only screen and
widgetclassifiers?
3.1 EvaluationContext
To evaluate Avgustas a whole, app videos are needed as its input.
Tocollectthesevideos,wereliedontheappsandusagesdefined
bytheFrUITeRbenchmark[ 85],whichcontains20popularapps
and18most-commonappusages.Wethendesignedauserstudy
to collect screen recordings of these app usages, which resulted
in the collection of 374 videos. To evaluate Avgust‚Äôs image clas-
sification component, we developed a semi-automated pipeline to
annotatethevideoframesofscreenrecordingscollectedbyusers
withground-truthcanonicalcategoriesforbothscreensandGUI
widgets.This processisdetailedinSection 3.1.2.
3.1.1 Video Collection. We designed a large-scale user study to
collect videos of app usages from participants. We recruited and
assignedthe18usagesand20appsto61computersciencestudents
inaMaster‚Äôslevelcourseattheauthors‚Äôinstitution,andaskedthem
torecordvideosofthemselvesexercisingscenariosthattriggered
thefeaturesassociatedwiththeusages.Weassignedusagessuch
thateachstudentwasassigned2applications,eachwith2different
usages, for a combination of 4 app-usage pairs. We balanced the
assignedappsandusagesevenlyacrossparticipants.Wethenasked
them to collect two screen-recording videos for each app-usage
pair,fora potential totalof8videosperparticipant.Weaskedfor
twovideos per app-usage pair in order to capture different ways of
exercisingagivenfeature(e.g.,addinganitemtoashoppingcart
Table 1:The 18usages used in Avgust‚Äôsevaluation.
Usage ID Test CaseName Tested Functionalities #Videos
U1 Sign In provide username and password tosign in 21
U2 Sign Up provide required information tosign up 76
U3 Search usesearchbar tosearcha product/news 29
U4 Detail findand open details of the first searchresult item 17
U5 Category findfirst categoryand open browsingpage forit 27
U6 About findand open about information of the app 15
U7 Account findand open accountmanagement page 18
U8 Help findand open help page of the app 17
U9 Menu findand open primaryappmenu 12
U10 Contact findand open contactpage of the app 16
U11 Terms findand open legal information of the app 20
U12 AddCart add the first searchresult itemtocart 13
U13 RemoveCart open cart and removethe first itemfrom cart 10
U14 Address add a newaddresstothe account 11
U15 Filter filter/sortsearchresults 14
U16 AddBookmark add first searchresult itemtothe bookmark 15
U17 RemoveBookmark open the bookmark and removefirst itemfrom it 20
U18 Textsize change text size 23by searching vs. by browsing categories). However, if a participant
deemed that there were not two distinct scenarios for exercising a
given feature,they were allowedto provideonly one usage.
This studywasconductedremotelydue toCOVID-19,andpar-
ticipants were given detailed instructions for installing and setting
up an Android emulator (Nexus 5X, API24), the .apkfiles required
to install their assigned apps, and a short textual description of
the assigned use cases (illustrated in Table 1). Additionally, we
providedasmalldesktopapplicationthatallowedparticipantsto
record the screens and a usage trace of their scenarios. This ap-
plication makes use of the adb screenrecord and Linux getevent
command line utilities. We provide these instructions, the app .apk
files, usage descriptions, device recording tool, and anonymized
collecteddatainouronlineappendix[ 10].Thisstudywasapproved
bythe InstitutionalReviewBoard(IRB)attheauthors‚Äô university
(IRBNet 1666261-1).
Thisdatacollectionprocessspannedtwosemesters,andintotal,
31oftheoriginallyrecruited61studentscompletedthestudy,some
with partial data, hence the imbalance of videos across apps and
usagesshowninTables 1and2.Intheend,weobtainedadataset
of374screenrecordingsof18usagesfrom18ofthe20apps(shown
inTable2) from the FrUITeR benchmark[ 85]discussedearlier.
3.1.2 Ground-Truth Annotation. Recall from Section 2thatAv-
gust‚Äôs classification involves (1) the screen classifier that maps an
app screen to an abstract screen IR category, and (2) the widget
classifierthatmapsacroppedwidgetimagetoanapp-independent
canonicalwidgetcategory.Toestablishtheground-truthlabels(i.e.,
the correct categories needed to generate Avgust‚Äôs IR Models),
wedevelopedapipelinetoimportpairsofscreen-widgetimages
into Label Studio [ 12], and trained four human annotators to label
the data. Specifically, the screen-widget image pairs are sourced
from theGUI Event Frames thatAvgustconverted during its Video
Analysisphase(recallFigure 1).
Toestablishourgroundtruthcategorizations,weprovidedde-
tailedinstructionstoandtrainedthefourannotatorstolabelthe
data based on the 37 screen and 74 widget canonical categories
wedefined(recallSection 2.3.2).Eachimageislabeledbyatleast
twoannotators,anddiscrepanciesareresolvedbynegotiatedagree-
ment[23]withthe annotators andone author.
Table 2:The 18subject appsused in Avgust‚Äôsevaluation.
AppID AppName #Downloads (mil) #Videos
A1 AliExpress 100 27
A2 Ebay 100 15
A3 Etsy 10 25
A4 Dailyhunt 1.7 8
A5 Geek 10 17
A6 Groupon 50 53
A7 Home 10 53
A8 6PM 0.5 25
A9 Wish 100 44
A10 The Guardian 5 8
A11 ABC News 5 18
A12 USAToday 5 26
A13 Zappos 0.054 11
A14 BuzzFeed 5 8
A15 FoxNews 10 11
A16 BBCNews 10 6
A17 Reuters 1 12
A18 News Break 0.322 7
428Avgust: AutomatingUsage-BasedTestGenerationfrom Videosof App Executions ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
This data collection process was time-consuming and intensive,
spanning ‚àº8person-monthsofeffort.Attheendoftheprocess,we
derivedacomprehensivelabeleddatasetcontaining2,478ground-
truth labels for screens and 2,434 labels for widgets across 18 apps.
Given these labels, Avgustwas able to automatically generate the
IR Models for the usages needed for our evaluation. Our labeled
dataset, as well as the annotation pipeline we developed can be
easily reusedorextendedbyfuture work inthis area [ 10].
3.2 RQ1: Avgust‚ÄôsTestQuality
Avgust‚Äôsmaingoalistogenerateatestforatargetappthataccom-
plishes the usage encoded by the videos of other apps. To evaluate
how well Avgustaccomplishes that goal, for each of the 18 app
usages,werandomlyselected3appsundertest(AUTs)asthetar-
get apps. (For three of the usages, we selected only 2 apps because
wewere unabletoextractdata fromcertaincommercialapps due
to security reasons or limitations of the underlying used testing
framework[ 4].)Foreachofthoseapps,weusethemergedIRModel
Avgustlearnedfromalltheother17appstoguide Avgust‚Äôstest
generation, aiming to demonstrate Avgust‚Äôs ability to generate
tests forunseenapps. As discussed in Section 2.4,Avgust‚Äôs test
generationisadeveloper-in-the-loopprocess.Specifically,fourof
the authorsserved as the developers interacting with the tool dur-
ing this process. We use the first test Avgustgenerates for the
evaluation,resultinginatotalof51testsacross18appusages.(We
limit our evaluation to a single test per usage per app due to the
significant manual effortinvolvedinthe evaluation process.)
We examined each of the tests manually to consider whether
it accomplished the intended usage. Note that this judgement is
objective.Forexample,itisstraightforwardandunambiguousto
determine whether the generated test accomplishes the Sign In
usage√êeither the tests signs into the app or it does not. We found
that35ofthe51tests(68.6%)accomplishedtheusage,meaningthat
Avgustsuccessfully generatedacorrecttest.
For theremaining 16 tests,we measuredhow similareachgen-
erated test was to the closest human test, to evaluate whether it
would potentially save human effort in writing the test. The is due
to the nature of usage-based tests, as there are usually multiple
correct paths of exercising the same usage. Thus, to enable fair
comparison,wecompare Avgust‚Äôstestwiththe closesthumantest.
We measuredsimilarityusing two metrics: precision and recallin
matchingthehumantest‚Äôsbehavior.Precisionmeasuresthefrac-
tion of the states and transitions in the generated test that occur in
the most-similar human test from the relevant videos. Recall mea-
suresthefractionofthestatesandtransitionsinthemost-similar
human test that occur in the generated test. The closesthuman test
ischosenusing the precision similaritymetric.
Table3lists the similarity results for the 16 tests across 11 us-
agesthatdonotsatisfythatusage,andthecloesthumantest.On
average,79%ofthestatesand47%ofthetransitionsinthegener-
atedtestsiscapturedbythemost similarhumantest.Thismeans
thatAvgustrarely visited an incorrect state, but often triggered
inputs for GUI widgets not triggered by humans. Meanwhile, on
average,thegeneratedtestscapture68%ofthestatesand37% of
the transitions in the closest human test. This means that Avgust
wasabletovisitamajorityofthescreensseeninthehumantest,Table 3: We compare the 16 Avgust-generated tests that do
not satisfy their intended usage with the most-similar hu-
mantest,toindicatehowmuchworkthesetestsmaysavea
developer.
precision recall
Usage states transitions states transitions
U4 Search 1.00 0.50 1.00 0.50
U5 Terms 0.71 0.29 0.63 0.33
U9 About 1.00 0.50 1.00 0.25
U10 Contact 0.75 0.37 0.72 0.33
U11 Help 0.67 0.50 0.67 0.33
U12 AddCart 0.75 0.59 0.55 0.37
U13 RemoveCart 1.00 0.69 0.62 0.42
U14 Address 0.83 0.69 1.00 0.75
U15 Filter 1.00 0.50 1.00 0.40
U17 RemoveBookmark 1.00 0.75 0.60 0.50
U18 Textsize 0.00 0.00 0.00 0.00
average 0.79 0.47 0.68 0.37
but correctly exercised comparatively fewer expected GUI widgets
thattriggerpropertransitions.Thissuggeststhatwhilethe31.4%
of the tests Avgustgenerates do not fully exercise the intended
usage, they may be at least partially helpful for developers writing
tests.Ourfuture workwillexaminetheeffortreduction Avgust‚Äôs
tests produce for developers.
RA1:Wefindthat69%of Avgust‚Äôsgeneratedtestssuccess-
fullyaccomplishthedesiredusage,savingthedeveloper
fromhavingtomanuallywritethetestfromscratch.For
theremaining31%ofthetests,wefoundthatthosetests
capture significant portions of the behavior in the most-
similartestahumanwouldwrite,again,potentiallysaving
human effort.
3.3 RQ2: Avgust‚ÄôsClassification Accuracy
RQ2 compares Avgust‚Äôs vision-only screen classification and wid-
get classification accuracy to the state-of-the-art S2V [ 47]. First,
Section3.3.1evaluates Avgust‚Äôs classification independently,as a
stand-alone tool. Then, Section 3.3.2evaluates Avgust‚Äôs classifica-
tioninthe contextoftest generation.
3.3.1 Evaluating Avgust‚Äôs Stand-Alone Classification. To evaluate
Avgust‚Äôsvision-onlyclassificationmoduleasastand-alonetech-
nique, we use our labeled dataset from Section 3.1.2. We use leave-
one-out cross-validation [ 22] to evaluate the accuracy of Avgust‚Äôs
screen and widget classifiers. For each of the 18 apps, we train our
modelonthe data from allthe otherapps, andtest onthat app.
Screen Classification: We evaluate three variants of Avgust‚Äôs
screen classifier. The first, the standard Avgustas introduced in
Section2, and two other classifiers that use only Avgust‚Äôs autoen-
coder(AE)modelandclassifywithtwowidely-adoptedmethods
KNN [14](AE +KNN) andMLP [ 32](AE +MLP), respectively.As
Avgust‚ÄôsAEmodelonlyencodesthescreen‚Äôsvisualfeatures,the
resultsaimtodemonstratetheimpactofvisual-onlyinformation
on the classification tasks. We did not evaluate the text-only model
since it contains app-specific noises (e.g., news content, product
429ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Y. Zhao, S.Talebipour, K.Baral, H.Park, L.Yee, S.Khan, Y. Brun, N.Medvidovic, andK.Moran
0.00.20.40.60.8
S2V + 
KNNS2V + 
MLPAE + KNN AE + MLP AVGUSTMin Avg MaxTop-1 Accuracy
0.000.250.500.751.00
S2V + 
KNNS2V + 
MLPAE + KNN AE + MLP AVGUSTMin Avg MaxTop-5 Accuracy
Figure 8: Avgust‚Äôs vision-only screen classification outper-
forms SV2 [ 47] and the other Avgust classifier variants in
bothtop-1 andtop-5 accuracy.
description)that do not generalize, and text-only information has
already been showninsufficientfor classification tasks[ 39].
Tocompare AvgustwithS2V,weadaptS2Vtolearnfromthe
information on the screen images only, and obtain the UI layout
information [ 7] that S2V requires as its input. To do so, we reverse
engineered appscreen imagesusing REMAUI [ 66], a research tool
toconvertappscreenimageintoitscorrespondingUIlayout[ 7].
Thisis thesameprocess Avgustuses(recall Section 2),aiming to
ensure that S2V and Avgustlearn from the same raw information
ontheappscreen.WethenapplyKNNandMLPtoS2V‚Äôsscreenem-
beddings, resulting in two S2V‚Äôs variants (S2V + KNN, S2V + MLP).
Figure8shows that Avgust‚Äôs classifier consistently outper-
formsallversionsofSV2andtheother Avgustclassifiervariants
in both top-1 and top-5 accuracy. Avgust‚Äôs composite classifier
that uses both visual and textual screen features outperforms both
autoencoder-only variants by more than 20%, suggesting that al-
thoughvisualfeaturesareimportantinencodingaUIscreen,adding
textual features significantly improves the quality of the generated
embeddings andresults inhigher classification accuracy.
While using S2V‚Äôs screen embeddings is effective for down-
stream tasks when the dynamic UI layout information is avail-
able[47],wewereunabletoachievehighclassificationaccuracy
usingthepre-trainedS2Vmodelbyreverseengineeringappscreen
images intoS2V‚Äôs required format.This suggests that the existing
pre-trained models cannot be used for vision-only tasks effectively.
Widget Classification: To compare Avgust‚Äôs widget classifica-
tionwithS2V[ 47],westudiedS2V‚Äôsimplementationandisolated
its underlying model that encodes the widget‚Äôs information. We
then appliedboth KNN andMLP to S2V‚Äôs widgetembeddings.
Figure9showsthat Avgust‚Äôswidgetclassifieroutperformsboth
S2V variants that use the pre-trained UI widget encoder for two
reasons.First,S2V‚ÄôsUIwidgetencoderonlyusesawidget‚Äôstextual
information and class type, whereas Avgust‚Äôs widget classifier
takes into account many other widget features, such as its location
on the screen and visual features. Second, S2V‚Äôs UI widget encoder
istrainedusingthetextualinformationavailableondynamicallyex-
tractedUIlayout,whichisnotavailableforthewidgetsin Avgust‚Äôs
vision-onlyclassification task.
3.3.2 Evaluating Avgust‚ÄôsClassificationforTestGeneration. We
next evaluate Avgust‚Äôs classification accuracy in the context of
testgeneration.DuringthetestgenerationphaseinSection 3.2,we
recorded Avgust‚ÄôsTop-1andTop-5recommendationsateachstep,
andevaluate the accuracyofthoserecommendations.
0.00.20.40.60.8
S2V + KNN S2V + MLP AVGUSTMIN AVG MAXTop-1 Accuracy
0.000.250.500.751.00
S2V + KNN S2V + MLP AVGUSTMIN AVG MAXTop-5 AccuracyFigure 9: Avgust‚Äôs vision-only widget classification consis-
tentlyoutperforms S2V[ 47].
0.00.20.40.60.8
AE + MLP 
(dynamic)AE + MLP 
(vision-only)AE + KNN 
(dynamic)AE + KNN 
(vision-only)Avgust 
(dynamic)Avgust 
(vision-only)MIN AVERAGE MAXTop-1 Accuracy
0.000.250.500.751.00
AE + MLP 
(dynamic)AE + MLP 
(vision-only)AE + KNN 
(dynamic)AE + KNN 
(vision-only)Avgust 
(dynamic)Avgust 
(vision-only)MIN AVERAGE MAXTop-5 Accuracy
Figure10:Theaccuracyof Avgust‚Äôsvision-onlyscreenclas-
sifiervariationswith vision-onlyand dynamic input data.
Evaluation Avgust‚Äôs Screen Classification: BesidesAvgust‚Äôs
built-in screen classifier introduced in Section 2, we further im-
plemented5variantsthatincorporateapp‚Äôsruntimeinformation,
aimingtogetinsightsonwhetherruntimeinformationcanimprove
the classification‚Äôs accuracy. This is inspired by S2V [ 47], which
relies on the screen‚Äôs runtime UI layout information [ 7], such as
theActivity name [6] andcontent description of the widgets on the
screen [8]. AsAvgust‚Äôs test generation phase interacts with the
target app at runtime, Avgustcan crawl the UI for this layout
information. We thus relaxed the vision-only constraint, and modi-
fiedAvgusttousethisdynamicUIlayoutinformation.Weterm
the modified version Avgust-Dynamic. Note that the fundamental
differencebetween Avgust-DynamicandS2Visthetrainingphase.
Avgust-Dynamicstillusesvision-onlyinformation(screenimages)
to train its models, while S2V requires the dynamic UI layout in-
formation in the training data. In practice, as classification tasks
usually require a large amount of training data, Avgust-Dynamic
makesthetrainingprocesssignificantlyeasierbyonlyrequiring
screen images, while S2V requires crawling the UIlayout informa-
tionat app runtimefor every app screen inthe training set.
Figure10showsAvgust‚Äôs screen classifier variants‚Äô accuracies
during the test generation phase. Comparing Figures 10and8,
we observe that in both cases, Avgustalways outperforms the
two autoencoder-only variants. All the classifier variants are more
accurate when using the vision-only data, compared to also us-
ing dynamic app information captured during runtime. While this
mightseemcounterintuitive,onepossibleexplanationisthatthe
features extracted from the vision-only input data are similar to
the dataAvgust‚Äôs built-in classifier were trained with, whereas
the dynamically obtained information might expose much more
textual information (e.g., content description) that is not consistent
with the OCR-based textual information used in the training phase.
Evaluation Avgust‚Äôs Widget Classification: To evaluate Av-
gust‚Äôs widget classifier duringtest generation,we assess whether
430Avgust: AutomatingUsage-BasedTestGenerationfrom Videosof App Executions ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Avgustcanfaithfullyrecommendwidgetstomatchthetransitions
suggested by its IR Model. We recorded the next transitions sug-
gested by the IR Model at each step of the test generation (recall
Section2.4),aswellasthecropsof Avgust‚Äôsrecommendedwidgets.
Threeannotatorsthenmanuallyinspectedthecroppedwidgetsand
determined whether their canonical categories match one of the
suggestedtransitions.Intotal,overallthegeneratedtests, Avgust‚Äôs
widget classifier correctly recommended widgets 77.4% of the time
(175 outof226steps).
RA2:Avgust‚Äôs classifiers consistently outperform the
state-of-the-art S2V tool. We find that using textual and
visualfeaturestogetherimprovesaccuracy,butaddingrun-
time features decreases accuracy, perhaps because these
featuresaretoodifferentfromtheonesusedtotrain Av-
gust‚Äôsvision-onlymodels.
4 RELATED WORK
Automated Input GenerationforMobile Apps: Existing auto-
mated test generation techniques share a complementary objective
to ours: they mainly focus on generating tests to maximize code
coverage and detect crashes, as opposed to generating usage-based
teststotestacertainfunctionality.Thelargebodyofexistingwork
includes model-based testing [ 15,16,30,34,55,56,72,74,75], ran-
domtesting[ 3,57,84],andsystematictesting[ 13,17,58,59].Re-
cently,Suetal.proposedGenie[ 76],whichisthefirstautomated
testing technique to detecting non-crashing functional bugs in An-
droidapps.However,Genieisarandom-basedfuzzingtechnique,
thusdoesnotgenerateusage-basedtests.Besidesthedifferences
in testing objectives, Avgust‚Äôs model is app-independent (repre-
senting usage scenarios learned from different apps) and is derived
purelyfromvisualdata,whichdiffersfromexistingmodel-based
testingtechniques.Furthermore,incomparisontorecenthuman-
in-the-looptechniques,e.g.NaviDroid[ 54],Avgust‚Äôsmodeland
recommendationsdifferbyprovidingsuggestionsforGUIactions
thatfulfillagivenusecase,asopposedtouncoveringunexplored
areas ofan app.
TestReuseinMobileApps: Theareaofresearchthatmostclosely
alignswithusage-basedtestgenerationistheworkon UItestreuse ,
which has been steadily growing over the past few years [ 19,20,
50,61,62,71,77,85]. These techniques can transfer an existing
usage-based test from a source app to its equivalent test of a target
appthat shares the same functionality, but cannot generate usage-
based tests from scratch. Furthermore, as discussed in Section 1,
existingtest-reusetechniqueshavethreeimportantlimitationsthat
we directlyaddressinthis paper.
Learning Patterns from Crowdsourced Tests: Similar to our
objective, another line of work aims to learn patterns from crowd-
sourcedtestsforautomatedtestgeneration.However,whilesuch
techniques learn from crowdsourced data, their test objectives are
to increase coverage or fault-finding ability as opposed to generat-
ing usage-based tests. For example, Replica [ 81] compares existing
in-housetestswiththeusertracesinthefield,andgeneratesnew
tests to mimic field traces that are not covered by the in-house
tests. Replica relies on pre-existing in-house tests that may not be
available,aswellasappinstrumentation.Ermuthetal.proposedanapproach to generate ≈Çmacro events≈æ that group multiple low-level
events into logic steps performed by real users [ 31], such as filling
and submitting a form. However, these macro events are recurring
patterns across allthe user traces collected when exercising the
entireapp, thus do not capture fine-grained user behaviors exer-
cised inspecific usages. Similarly, MonkeyLab and Polariz [ 53,60]
mines users‚Äô event traces to generate combinations of low-level
eventsrepresentingnaturalscenarios(similarto≈Çmacroevents≈æ),
as well as untested corner cases (similar to Replica‚Äôs objective).
ComboDroid [ 80] aims to reach complex app functionality by com-
bining independent short ≈Çuse cases≈æ, such as toggling a setting, or
switching to a different screen. Humanoid [ 49] leverages a deep
neural network model to learn input actions based on real-user
traces. However, the generatedtests fromall theworkmentioned
above again focus on maximizing the code coverage, but do not
aim to generatetests ofspecific usages.
Specification-basedTestingforMobileApps: Finally,thistype
of testing aims to generate tests that cover specific functionalities
(similar to our definition of usages), guided by manually-written
specifications.Forexample,FARLEAD-Android[ 44]requiresthe
developer to provide UI test scenarios written in Gherkin [ 5] in
order to generate tests using reinforcement learning. Similarly,
AppFlow [ 39] requires the developer to first create a test library
thatcoversthecommonfunctionalitiesinacertainappcategory
(e.g.,shoppingapps)usingaGherkin-basedlanguagethatAppFlow
defines.AppFlowthensynthesizesapp-specifictestsaccordingto
thetestlibrary.Augusto[ 63]usesGUIrippingtoexplorepopular
app-independent functionalities (referred to as ≈ÇAIFs≈æ), and gen-
erates functional tests accordingly. However, developers have to
manuallydefineUIpatternsandAlloysemanticmodel[ 40]tode-
scribetheAIFs. Avgustattemptstoadvanceuponsuchworkby
simplifyingthespecificationprocessbyrelyingpurelyonvideos
that specifydesiredtest behaviors.
5 CONTRIBUTIONS
Wehavepresented Avgust,amethodforgeneratingusage-based
tests forthe Android platform. Bytargeting usage-basedtests, Av-
gustsolveswhatmobiledevelopersidentifyasamajorneed[ 52]
butthatthestateofthearthasfailedtoaddress[ 35,43,52].Avgust
usesuser-generatedvideosofappusagestolearnamodelofausage,
andthenappliesthatmodeltoanewtargetapptogeneratetests.
Evaluatingon374videosofcommonusesof18popularapps,we
show that69% ofthetests Avgustgeneratessuccessfullyexecute
thedesiredusage,thattheremaininggeneratedtestshavepotential
for reducing developer effort in writing tests, and that Avgust‚Äôs
classifiers outperform the state of the art. Our work suggests a
promisingdirectionofresearchintousage-basedtestgeneration,
andoutlinesoutstandingproblemsinclassificationaccuracythat
future researchshould address.
ACKNOWLEDGEMENT
This work is supported by the U.S. National Science Foundation
under grant no. CCF-1717963, CCF-1763423, CNS-1823354, CCF-
1955853,andCCF-2030859(totheComputingResearchAssociation
for the CIFellows Project), as well as the U.S. Office of Naval Re-
search undergrantN00014-17-1-2896. Additionally, we wouldlike
tothankArthurWuforhishelpondatacollectionandannotation.
431ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Y. Zhao, S.Talebipour, K.Baral, H.Park, L.Yee, S.Khan, Y. Brun, N.Medvidovic, andK.Moran
REFERENCES
[1]2016. uiautomator |AndroidDevelopers. https://android-doc.github.io/tools/
help/uiautomator/index.html
[2]2019. GATOR: Program Analysis Toolkit For Android. http://web.cse.ohio-
state.edu/presto/software/gator
[3]2020. UI/ApplicationExerciserMonkey |AndroidDevelopers. https://developer.
android.com/studio/test/monkey
[4] 2021. Appium: Mobile AppAutomation MadeAwesome. http://appium.io
[5]2021. GherkinSyntax-CucumberDocumentation. https://cucumber.io/docs/
gherkin
[6]2021. IntroductiontoActivities |AndroidDevelopers. https://developer.android.
com/guide/components/activities/intro-activities
[7]2021. Layouts |Android Developers. https://developer.android.com/guide/
topics/ui/declaring-layout
[8]2021. Make apps more accessible |Android Developers. https://developer.
android.com/guide/topics/ui/accessibility/apps
[9] 2021. Soot. http://soot-oss.github.io/soot/
[10] 2022. AVGUST‚Äôspublic repository. https://doi.org/10.5281/zenodo.7036218
[11]2022. EditText |Android Developers. https://developer.android.com/reference/
android/widget/EditText
[12]2022. LabelStudio≈õOpenSourceDataLabeling. https://labelstud.io [Online;
accessed 9.Mar. 2022].
[13]Christoffer Quist Adamsen, Gianluca Mezzetti, and Anders M√πller. 2015. System-
aticexecutionofandroidtestsuitesinadverseconditions.In Proceedingsofthe
2015InternationalSymposiumonSoftwareTestingand Analysis . 83≈õ93.
[14]NaomiSAltman.1992. Anintroductiontokernelandnearest-neighbornonpara-
metricregression. The AmericanStatistician 46,3 (1992), 175≈õ185.
[15]Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Salvatore
De Carmine, and Atif M Memon. 2012. Using GUI ripping for automated testing
of Androidapplications. In 2012 Proceedings of the 27th IEEE/ACM International
Conference onAutomatedSoftwareEngineering . IEEE,258≈õ261.
[16]DomenicoAmalfitano,AnnaRitaFasolino,PorfirioTramontana,BryanDzung
Ta, andAtifM Memon.2014. MobiGUITAR: Automatedmodel-based testing of
mobile apps. IEEE software 32,5 (2014), 53≈õ59.
[17]TanzirulAzimandIulianNeamtiu.2013. Targetedanddepth-firstexploration
for systematic testing of android apps. In Proceedings of the 2013 ACM SIGPLAN
international conference on Object oriented programming systems languages &
applications . 641≈õ660.
[18]G. Bavota, M. Linares-V√°squez, C. Bernal-C√°rdenas, M. Di Penta, R. Oliveto, and
D.Poshyvanyk.2015. TheImpactofAPIChange-andFault-Pronenessonthe
UserRatings of AndroidApps. IEEE Transactions on Software Engineering (TSE)
(2015).
[19]Farnaz Behrang and Alessandro Orso. 2018. Test migration for efficient large-
scale assessment of mobile app coding assignments. In Proceedings of the 27th
ACMSIGSOFTInternationalSymposiumonSoftwareTestingand Analysis .
[20]Farnaz Behrang and Alessandro Orso. 2019. Test Migration Between Mobile
AppswithSimilar Functionality.In 34thInternationalConference onAutomated
SoftwareEngineering (ASE 2019) .
[21]Carlos Bernal-C√°rdenas, Nathan Cooper, Kevin Moran, Oscar Chaparro, An-
drian Marcus, and Denys Poshyvanyk. 2020. Translating video recordings of
mobileappusagesintoreplayablescenarios.In ProceedingsoftheACM/IEEE42nd
InternationalConference onSoftwareEngineering . 309≈õ321.
[22]Christopher M Bishop and Nasser M Nasrabadi. 2006. Pattern recognition and
machinelearning . Vol. 4. Springer.
[23] K.Charmaz.2006. ConstructingGroundedTheory . SAGE Publications Inc.
[24]Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu, Liming
Zhu,andGuoqiangLi.2020. Objectdetectionforgraphicaluserinterface:old
fashioned or deep learning or a combination?. In Proceedings of the 28th ACM
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
the FoundationsofSoftwareEngineering . 1202≈õ1214.
[25]A.Ciurumelea,A.Schaufelb√ºhl,S.Panichella,andH.C.Gall.2017. Analyzing
Reviews and Code of Mobile Apps for Better Release Planning. In 2017 IEEE
24thInternationalConferenceonSoftwareAnalysis,EvolutionandReengineering
(SANER) (SANER‚Äô17) . 91≈õ102. https://doi.org/10.1109/SANER.2017.7884612
[26]Nathan Cooper, Carlos Bernal-C√°rdenas, Oscar Chaparro, Kevin Moran, and
Denys Poshyvanyk. 2021. It Takes Two to Tango: Combining Visual and Textual
InformationforDetectingDuplicateVideo-BasedBugReports.In 2021IEEE/ACM
43rd International Conference on Software Engineering (ICSE) . 957≈õ969. https:
//doi.org/10.1109/ICSE43902.2021.00091
[27]BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,
YangLi,JeffreyNichols,andRanjithaKumar.2017. Rico:Amobileappdataset
forbuildingdata-drivendesignapplications.In Proceedingsofthe30thAnnual
ACMSymposiumonUser InterfaceSoftwareand Technology . 845≈õ854.
[28]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. arXiv
preprint arXiv:1810.04805 (2018).[29]AndreaDiSorbo,SebastianoPanichella,CarolV.Alexandru,JunjiShimagaki,Cor-
rado A. Visaggio, Gerardo Canfora, and Harald C. Gall. 2016. What Would Users
Change in My App? Summarizing App Reviews for Recommending Software
Changes. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium
onFoundationsofSoftwareEngineering (FSE‚Äô16) .ACM,Seattle,WA,USA,499≈õ510.
https://doi.org/10.1145/2950290.2950299
[30]Zhen Dong, Marcel B√∂hme, Lucia Cojocaru, and Abhik Roychoudhury. 2020.
Time-traveltestingofAndroidapps.In 2020IEEE/ACM42ndInternationalCon-
ference onSoftwareEngineering (ICSE) . IEEE,481≈õ492.
[31]Markus Ermuth and Michael Pradel. 2016. Monkey see, monkey do: Effective
generation of GUI tests with inferred macro events. In Proceedings of the 25th
InternationalSymposiumonSoftwareTestingand Analysis . 82≈õ93.
[32]MattWGardnerandSRDorling.1998. Artificialneuralnetworks(themultilayer
perceptron)√êareviewof applicationsintheatmosphericsciences. Atmospheric
environment 32,14-15 (1998), 2627≈õ2636.
[33]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT
Press.http://www.deeplearningbook.org .
[34]Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao,
Qirun Zhang, Jian Lu, and Zhendong Su. 2019. Practical GUI testing of An-
droid applications viamodelabstraction and refinement. In 2019 IEEE/ACM 41st
InternationalConference onSoftwareEngineering (ICSE) . IEEE,269≈õ280.
[35]Roman Haas, Daniel Elsner, Elmar Juergens, Alexander Pretschner, and Sven
Apel.2021. Howcanmanualtestingprocessesbeoptimized?developersurvey,
optimizationguidelines,andcasestudies.In Proceedingsofthe29thACMJoint
Meeting on European Software Engineering Conference and Symposium on the
FoundationsofSoftwareEngineering . 1281≈õ1291.
[36]Madeleine Havranek, Carlos Bernal-C√°rdenas, Nathan Cooper, Oscar Chaparro,
Denys Poshyvnayk,and Kevin Moran. 2021. V2S:A Tool for TranslatingVideo
RecordingsofMobileApp UsagesintoReplayable Scenarios.In 2021 IEEE/ACM
43rd International Conference on Software Engineering: Companion Proceedings
(ICSE-Companion) . IEEE,65≈õ68.
[37]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition . 770≈õ778.
[38]G.Hu,X.Yuan,Y.Tang,andJ.Yang.2014. Efficiently,effectivelydetectingmobile
appbugswithAppDoctor.In NinthEuropeanConferenceonComputerSystems
(EuroSys‚Äô14) . ArticleNo.18.
[39]Gang Hu, Linjie Zhu, and Junfeng Yang. 2018. AppFlow: using machine learning
to synthesize robust, reusable UI tests. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the
FoundationsofSoftwareEngineering . ACM,269≈õ282.
[40]Daniel Jackson. 2002. Alloy: a lightweight object modelling notation. ACM
Transactions on Software Engineering and Methodology (TOSEM) 11, 2 (2002),
256≈õ290.
[41]N. Jones. 2013. Seven best practices for optimizing mobile testing efforts . Technical
Report G00248240.Gartner.
[42]JoukoKaasila,DenzilFerreira,VassilisKostakos,andTimoOjala.2012. Testdroid:
automated remote UI testing on Android. In Proceedings of the 11th International
Conference onMobileand UbiquitousMultimedia . 1≈õ4.
[43]PavneetSinghKochhar,FerdianThung,NachiappanNagappan,ThomasZimmer-
mann,andDavidLo.2015. UnderstandingtheTestAutomationCultureofApp
Developers.In 2015IEEE8th InternationalConference on Software Testing, Verifi-
cation and Validation (ICST) . 1≈õ10.https://doi.org/10.1109/ICST.2015.7102609
[44]Yavuz Koroglu and Alper Sen. 2021. Functional test generation from UI test
scenariosusingreinforcementlearningforandroidapplications. SoftwareTesting,
Verification and Reliability 31,3 (2021), e1752.
[45]GregLee.2018. AndroidViewMeasurement-TheInsideScoop. Medium(Jun
2018).https://blog.takescoop.com/android-view-measurement-d1f2f5c98f75
[46]KanglinLiandMengqiWu.2006. EffectiveGUItestingautomation:Developingan
automatedGUItestingtool . John Wiley& Sons.
[47]Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and Brad A Myers. 2021.
Screen2Vec:SemanticEmbeddingofGUIScreensandGUIComponents.In Pro-
ceedings of the 2021 CHI Conference on Human Factors in Computing Systems .
1≈õ15.
[48]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2019. Humanoid: A
DeepLearning-BasedApproachtoAutomatedBlack-boxAndroidAppTesting.In
2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE). 1070≈õ1073. https://doi.org/10.1109/ASE.2019.00104
[49]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2019. Humanoid: A
deeplearning-basedapproachtoautomatedblack-boxandroidapptesting.In
2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE). IEEE,1070≈õ1073.
[50]Jun-WeiLin, ReyhanehJabbarvand,andSamMalek. 2019. Test TransferAcross
Mobile Apps Through Semantic Mapping. In 34th International Conference on
AutomatedSoftwareEngineering (ASE 2019) .
[51]M. Linares-V√°squez, G. Bavota, C. Bernal-C√°rdenas, M. Di Penta, R. Oliveto, and
D. Poshyvanyk. 2013. API Change and Fault Proneness: A Threat to the Success
of Android Apps. In ESEC/FSE‚Äô13 . 477≈õ487.
432Avgust: AutomatingUsage-BasedTestGenerationfrom Videosof App Executions ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
[52]MarioLinares-V√°squez,CarlosBernal-C√°rdenas,KevinMoran,andDenysPoshy-
vanyk. 2017. How do developers test android applications?. In 2017 IEEE Interna-
tional Conference onSoftwareMaintenance and Evolution(ICSME) .
[53]MarioLinares-V√°squez,MartinWhite,CarlosBernal-C√°rdenas,KevinMoran,and
DenysPoshyvanyk.2015. Mining android app usagesforgeneratingactionable
gui-based execution scenarios. In 2015 IEEE/ACM 12th Working Conference on
MiningSoftwareRepositories . IEEE,111≈õ122.
[54]Zhe Liu, Chunyang Chen, Junjie Wang, Yuekai Huang, Jun Hu, and Qing Wang.
2022. Guidedbugcrush:Assistmanualguitestingofandroidappsviahintmoves.
InCHIConference onHuman Factors inComputingSystems . 1≈õ14.
[55]Nikola Lukiƒá, Saghar Talebipour, and Nenad Medvidoviƒá. 2020. AirMochi: a
toolforremotelycontrollingiOSdevices.In Proceedingsofthe35thIEEE/ACM
InternationalConference onAutomatedSoftwareEngineering . 1273≈õ1277.
[56]Nikola Lukiƒá,SagharTalebipour, andNenad Medvidoviƒá.2020. Remote control
of ios devices via accessibility features. In Proceedings of the 2020 ACM Workshop
onForming an Ecosystem Around SoftwareTransformation . 35≈õ40.
[57]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An input
generationsystemfor android apps. In Proceedings ofthe 2013 9thJointMeeting
onFoundationsofSoftwareEngineering . 224≈õ234.
[58]RiyadhMahmood,NarimanMirzaei,andSamMalek.2014. Evodroid:Segmented
evolutionarytestingofandroidapps.In Proceedingsofthe22ndACMSIGSOFT
InternationalSymposiumonFoundationsofSoftwareEngineering . 599≈õ609.
[59]KeMao,MarkHarman,andYueJia.2016.Sapienz:Multi-objectiveautomatedtest-
ing for android applications. In Proceedings of the 25th International Symposium
onSoftwareTestingand Analysis . 94≈õ105.
[60]KeMao,MarkHarman,andYueJia.2017. CrowdIntelligenceEnhancesAuto-
mated Mobile Testing. In Proceedings of the 32nd IEEE/ACM International Con-
ferenceonAutomatedSoftwareEngineering (Urbana-Champaign,IL,USA) (ASE
2017). IEEE Press,16≈õ26.
[61]Leonardo Mariani, Ali Mohebbi, Mauro Pezze, and Valerio Terragni. 2021. Se-
mantic Matching of GUI Eventsfor Test Reuse: Are We There Yet?. In The ACM
SIGSOFTInternationalSymposiumonSoftwareTestingand Analysis(ISSTA) .
[62]Leonardo Mariani, Mauro Pezz√®, Valerio Terragni, and Daniele Zuddas. 2021. An
EvolutionaryApproachtoAdaptTestsAcrossMobileApps.In The2ndACM/IEEE
InternationalConference onAutomationofSoftwareTest(AST2021) .
[63]Leonardo Mariani, Mauro Pezz√®, and Daniele Zuddas. 2018. Augusto: Exploiting
popular functionalities for thegenerationofsemanticguitestswithoracles.In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering .280≈õ290.
[64]Kevin Moran, Carlos Bernal-C√°rdenas, Michael Curcio, Richard Bonett, and
Denys Poshyvanyk. 2018. Machine learning-based prototyping of graphical user
interfacesformobileapps. IEEETransactionsonSoftwareEngineering 46,2(2018),
196≈õ221.
[65]Kevin Moran, Mario Linares-V√°squez, Carlos Bernal-C√°rdenas, Christopher Ven-
dome, and Denys Poshyvanyk. 2016. Automatically Discovering, Reporting
andReproducing AndroidApplication Crashes.In 2016IEEE InternationalCon-
ference on Software Testing, Verification and Validation (ICST) . 33≈õ44. https:
//doi.org/10.1109/ICST.2016.34
[66]Tuan Anh Nguyen and Christoph Csallner. 2015. Reverse engineering mobile
applicationuser interfaceswithremaui (t). In 2015 30th IEEE/ACM International
Conference onAutomatedSoftwareEngineering (ASE) . IEEE,248≈õ259.
[67]Fabio Palomba, Mario Linares-V√°squez, Gabriele Bavota, Rocco Oliveto, Massim-
ilianoDiPenta,DenysPoshyvanyk,andAndreaDeLucia.2015. UserReviews
Matter!TrackingCrowdsourcedReviewstoSupportEvolutionofSuccessfulApps.
InProceedings of 31st IEEE International Conference on Software Maintenance and
Evolution(ICSME‚Äô15) . to appear.
[68]Fabio Palomba, Pasquale Salza, Adelina Ciurumelea, Sebastiano Panichella, Har-
ald Gall, Filomena Ferrucci, and Andrea De Lucia. 2017. Recommending and
Localizing Change Requests for Mobile Apps Based on User Reviews. In Pro-
ceedings of the 39th International Conference on Software Engineering (Buenos
Aires,Argentina) (ICSE‚Äô17) .IEEEPress,Piscataway,NJ,USA,106≈õ117. https://doi.org/10.1109/ICSE.2017.18
[69]Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens,
and Lawrence Carin. 2016. Variational Autoencoder for Deep Learning of
Images, Labels and Captions. In Advances in Neural Information Processing
Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.),
Vol.29.CurranAssociates,Inc. https://proceedings.neurips.cc/paper/2016/file/
eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf
[70] pytransitions.2021. transitions. https://github.com/pytransitions/transitions
[71]Xue Qin, Hao Zhong, and Xiaoyin Wang. 2019. Testmig: Migrating gui test
casesfromiostoandroid. In Proceedings ofthe 28th ACMSIGSOFTInternational
SymposiumonSoftwareTestingand Analysis . 284≈õ295.
[72]Ibrahim-AnkaSalihu,RosziatiIbrahim,BestounSAhmed,KamalZZamli,and
Asmau Usman. 2019. AMOGA: A static-dynamic model generation strategy for
mobile apps testing. IEEE Access 7 (2019), 17158≈õ17173.
[73]RaySmith.2007. AnoverviewoftheTesseractOCRengine.In Ninthinternational
conference on document analysis and recognition (ICDAR 2007) , Vol. 2. IEEE, 629≈õ
633.
[74]Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang
Pu,YangLiu,andZhendongSu.2017. Guided,stochasticmodel-basedGUItesting
ofAndroidapps.In Proceedingsofthe201711thJointMeetingonFoundationsof
SoftwareEngineering . 245≈õ256.
[75]Ting Su, Yichen Yan,Jue Wang, and Zhendong Su. 2020. AutomatedFunctional
Fuzzing of Android Apps. arXiv preprint arXiv:2008.03585 (2020).
[76]Ting Su, Yichen Yan, Jue Wang, Jingling Sun, Yiheng Xiong, Geguang Pu, Ke
Wang, and Zhendong Su. 2021. Fully Automated Functional Fuzzing of Android
Apps for Detecting Non-Crashing Logic Bugs. In Proceedings of the 28th ACM
JointMeetingonEuropeanSoftwareACMSIGPLANInternationalConferenceon
Object-OrientedProgramming, Systems,Languages, and Applications .
[77]Saghar Talebipour, Yixue Zhao, Luka Dojcilovic, Chenggang Li, and Nenad Med-
vidovic.2021. UITestMigrationAcrossMobilePlatforms.In 36thInternational
Conference onAutomatedSoftwareEngineering (ASE 2021) .
[78]RajaVall√©e-Rai,PhongCo,EtienneGagnon,Laurie Hendren,PatrickLam,and
Vijay Sundaresan. 2010. Soot: A Java bytecode optimization framework. In
CASCON First Decade HighImpactPapers . 214≈õ224.
[79]Akshaj Verma. 2021. PyTorch [Vision] √ê Binary Image Classification - Towards
Data Science. Medium(May 2021). https://towardsdatascience.com/pytorch-
vision-binary-image-classification-d9a227705cf9
[80]JueWang, YanyanJiang, ChangXu,Chun Cao, XiaoxingMa,and Jian Lu. 2020.
ComboDroid:generatinghigh-qualitytestinputsforAndroidappsvia usecase
combinations.In ProceedingsoftheACM/IEEE42ndInternationalConferenceon
SoftwareEngineering . 469≈õ480.
[81]Qianqian Wang and Alessandro Orso. 2020. Improving Testing by Mimicking
User Behavior. In 2020 IEEE International Conference on Software Maintenance
and Evolution(ICSME) . IEEE,488≈õ498.
[82]Mulong Xie, Sidong Feng, Zhenchang Xing, Jieshan Chen, and Chunyang Chen.
2020. UIED:ahybridtoolforGUIelementdetection.In Proceedingsofthe28th
ACM Joint Meeting on European Software Engineering Conference and Symposium
onthe FoundationsofSoftwareEngineering . 1655≈õ1659.
[83]Shengqian Yang, Haowei Wu, Hailong Zhang, Yan Wang, Chandrasekar Swami-
nathan,DacongYan,andAtanasRountev.2018. Staticwindowtransitiongraphs
for Android. AutomatedSoftwareEngineering 25,4 (2018), 833≈õ873.
[84]Faraz YazdaniBanafsheDaragh and Sam Malek. 2021. Deep GUI: Black-box
GUI Input Generation with Deep Learning. In 36th International Conference
onAutomatedSoftwareEngineering (ASE 2021) .
[85]YixueZhao,JustinChen,AdrianaSejfia,MarceloSchmittLaser,JieZhang,Fed-
ericaSarro,MarkHarman,andNenadMedvidovic.2020. FrUITeR:AFramework
for Evaluating UI Test Reuse. In Proceedings of the 28th ACM Joint European
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (Virtual Event, USA) (ESEC/FSE ‚Äô20) . ACM, New York, NY, USA.
https://doi.org/10.1145/3368089.3409708
433