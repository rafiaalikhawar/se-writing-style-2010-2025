Towards Boosting Patch Execution On-the-FlySamuel BentonUniversity of Texas at DallasSamuel.Benton1@utdallas.eduYuntong Xie!Tsinghua Universityxieyt18@mails.tsinghua.edu.cnLan Lu!Southern University of Science andTechnology11810935@mail.sustech.edu.cnMengshi Zhang†Meta Platforms, Inc.mengshizhang@fb.comXia LiKennesaw State Universityxli37@kennesaw.eduLingming Zhang†University of Illinois atUrbana-Champaignlingming@illinois.eduABSTRACTProgram repair is an integral part of every software system’s life-cycle but can be extremely challenging. To date, various automatedprogram repair (APR) techniques have been proposed to reducemanual debugging eﬀorts. However, given a real-world buggy pro-gram, a typical APR technique can generate a large number ofpatches, each of which needs to be validated against the originaltest suite, incurring extremely high computation costs. Although ex-isting APR techniques have already leveraged various static and/ordynamic information to/f_ind the desired patches faster, they arestill rather costly. In this work, we propose SeAPR (Self-BoostedAutomatedProgramRepair), the/f_irst general-purpose techniqueto leverage the earlier patch execution information during APR todirectly boost existing APR techniques themselves on-the-$y. Ourbasic intuition is that patches similar to earlier high-quality/low-quality patches should be promoted/degraded to speed up the de-tection of the desired patches. The experimental study on 13 state-of-the-art APR tools demonstrates that, overall, SeAPR can sub-stantially reduce the number of patch executions with negligibleoverhead. Our study also investigates the impact of various con-/f_igurations on SeAPR. Lastly, our study demonstrates that SeAPRcan even leverage the historical patch execution information fromother APR tools for the same buggy program to further boost thecurrent APR tool.ACM Reference Format:Samuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Ling-ming Zhang. 2022. Towards Boosting Patch Execution On-the-Fly. In44thInternational Conference on Software Engineering (ICSE ’22), May 21–29,2022, Pittsburgh, PA, USA.ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3510003.3510117!The work was done during a remote summer internship at University of Illinois.†Corresponding authors.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro/f_it or commercial advantage and that copies bear this notice and the full citationon the/f_irst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or afee. Request permissions from permissions@acm.org.ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-9221-1/22/05. . . $15.00https://doi.org/10.1145/3510003.35101171 INTRODUCTIONSoftware systems persist everywhere in all facets of today’s so-ciety; they drive/f_inancial institutions, facilitate communicationworldwide, oversee critical systems, and so forth. Software systems,however, arefrequently distributedwith numerous bugs that willeventually lead to severe disasters. For example, the 5th editionofTricentis.com’s annual report shows that software failuresimpact half of the world’s population (3.7 billion users) and $1.7trillion in assets; it also mentions that there can be far more bugsin the wild than we will likely ever know about [1]. Therefore, itis imperative for developers to/f_ix these bugs as early as possiblewith minimal resource consumption. However, manual bug/f_ixingcan be extremely tedious, challenging, and time-consuming sincemodern software systems can be extremely complicated [2].Fortunately, in lieu of manual bug/f_ixing, researchers have alsoextensively studied Automated Program Repair (APR) [3–18], whichaims to automatically/f_ix software bugs to reduce manual debug-ging eﬀorts. Typical APR techniques leverage oﬀ-the-shelf faultlocalization [19] techniques (such as Ochiai [20] and Tarantula [21])to identify potential buggy locations. Then, they leverage varioustechniques to generate potential software patches for the potentialbuggy locations. Lastly, each generated patch will need to be ex-ecuted against the original test suite to/f_ind theplausiblepatches(i.e., the patches that can pass all the original tests). Note that notall the plausible patches are the ones that developers want; thus,developers need to further inspect the produced plausible patchesto derive the/f_inalcorrectones. To date, various APR techniqueshave been proposed, including techniques based on prede/f_ined-templates [4,22,23], heuristics [8,10,24], and constraint solv-ing [7,25,26]. Furthermore, APR techniques have also drawn wideattention from industry, e.g., Facebook [27], Fujitsu [28], and Al-ibaba [29].Compared with manual bug/f_ixing, APR can automatically/f_ixanumber of real-world bugs with minimal human intervention andcan be easily integrated with the natural work$ow of continuousintegration lifecycle (e.g., Facebook’s in-house tool SapFix [27] hasbeen integrated into its work$ow). Despite the promising futureof APR, it is not perfect yet and numerous issues still plague thearea. Among the most paramount of these issues are still the timecosts associated with numerous patches for large-scale real-worldsystems. Existing studies have demonstrated that patch validationdominates the costs of APR [30–33], since each patch needs to beexecuted against the original test suite.21652022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASamuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Lingming ZhangTo reduce the APR costs, researchers have proposed various tech-niques to reduce the number of patches generated, e.g., based onmachine learning [28], code mining [13], and constraint solving [25].However, prior work has demonstrated that such techniques canincur the dataset over/f_itting issue, i.e., the correct patches maybe skipped for many other unstudied cases [22]. Furthermore, re-searchers have also proposed to prioritize all the generated patchesto/f_ind the plausible patches earlier. Such existing techniques pri-marily utilize static or dynamic information tostaticallyprioritizepatches before the patch validation [8,23,26], e.g., almost all APRtechniques use the suspiciousness values computed by oﬀ-the-shelffault localization techniques to prioritize the patches; no furtherreprioritization is employed during the patch validation process ofthese tools, leading to limited improvement.In this work, we propose SeAPR (Self-BoostedAutomatedProgramRepair), the/f_irst general-purpose technique to leverage the patch-execution information during APR to directly boost existing APRtechniques themselves on-the-$y. Our basic intuition is that earlierpatch execution results can help better prioritize later patch exe-cutions on-the-$y to speed up the detection of the desired patches(e.g., plausible/correct patches). In this way, we promote the rank-ing of the patches similar to the executed high-quality patches,while degrading the ranking of the patches similar to the executedlow-quality patches. More speci/f_ically, we analyze the modi/f_ied ele-ments to compute patch similarities as patches modifying similarprogram elements can exhibit close program behaviors. We haveevaluated SeAPR on 13 state-of-the-art APR systems. Our study alsoinvestigates the impact of various con/f_igurations on SeAPR, e.g., theformula for patch prioritization, the type of patch-validation ma-trices (full or partial), the number of code elements considered forSeAPR, and the additional patch pattern information for computingpatch similarity. Lastly, our study further evaluates the performanceof SeAPR with historical patch-execution information from otherAPR tools on the same buggy program.To summarize, this paper makes the following contributions:•Direction.This paper opens a new dimension to leveragepatch-execution information to boost existing APR tech-niques on-the-$y and can inspire more future work in thisnew direction.•Design.We design the/f_irst technique, SeAPR, in this newdirection to update each patch’s priority score based on itssimilarity with the executed patches and the quality of theexecuted patches.•Extensive Study.We have performed an extensive study ofthe proposed technique on 13 state-of-the-art APR systemsfor JVM-based languages using the widely studied real-worldbugs from Defects4J.•Practical Guidelines.The study reveals various practicalguidelines, including (1) the default SeAPR can substantiallyspeed up thestudied APR techniques by up to 79% withnegligible overhead (regardless of various initial patch prior-itization strategies used by the studied APR techniques), (2)SeAPR has stable performance when using diﬀerent formulaefor computing patch priority and diﬀerent types of patch-execution matrices, (3) additional patch pattern informationfor patch similarity computation can further substantiallyimprove SeAPR, and (4) SeAPR can even eﬀectively utilizehistorical patch-execution information from other APR toolsto boost current APR tools.2 RELATED WORK2.1 Automated Program RepairAutomated Program Repair (APR) techniques [3–18] aim to au-tomatically/f_ix software bugs to substantially reduce manual de-bugging eﬀorts and have been extensively studied during the lastdecade. Typical APR techniques usually modify program code rep-resentations based on various patch-generation techniques andthen validate each generated patch (e.g., via testing [24], formalspeci/f_ication checking [34], and static analysis [35]) to/f_ind the/f_inal desired patches. In recent years, APR techniques leveragingtesting for patch validation have gained popularity as testing isthe dominant methodology for detecting software bugs in practice.Such APR techniques usually include the following phases. (1)Faultlocalization: APR techniques/f_irst leverage oﬀ-the-shelf fault local-ization techniques [4,5,7,10,36,37] to localize the potential buggylocations. (2)Patch generation: APR techniques will leverage variousstrategies to generate potential patches for the identi/f_ied potentialbuggy locations. (3)Patch validation: all the generated patches willbe executed against the original test suite to detect the patches thatpass all the original tests, i.e.,plausible patches. Of course, sincenot all plausible patches are desirable, patch correctness checking(often done via manual inspection in practice) is further involvedto/f_ind the/f_inalcorrect patches, which are equivalent to developerpatches.According to a recent study [38], most state-of-the-art APR tech-niques can be divided into the following categories. (1)Heuristic-based techniquesleverage various heuristics to iteratively explorethe search space of all possible program edits. For example, theseminal GenProg technique [24] leverages genetic programmingto synthesize donor code for high-quality patch generation, whilethe recent SimFix technique [8] employs advanced code search toobtain donor code for patch generation. (2)Template-based tech-niquesleverage prede/f_ined/f_ixing templates (e.g., changing “>” to“≥”) to perform patch generation. Such prede/f_ined/f_ixing templatescan be either manually summarized (e.g., KPar [39]), or automat-ically inferred (e.g., HDRepair [40]) from historical bug/f_ixes. (3)Constraint-based techniquestransform the program repair prob-lem into a constraint-solving problem and leverage state-of-the-art constraint solvers (e.g., SMT [41]) for patch generation, suchas Nopol [25]. More recently, researchers have also looked intolearning-based techniques[42,43] to directly generate patches vialearning from historical/f_ixes.Since it is extremely challenging for APR techniques to/f_ix allpossible bugs, researchers have also recently proposed theuni/f_ieddebuggingwork [29,44,45] to extend the application scope of APRto the bugs that cannot be automatically/f_ixed. Its basic intuition isthat the massive patch execution information during APR can actu-ally substantially boost fault localization. For example, if a patchpasses all the tests, it means the patch is likely to mute the impactsof the bug, even though this patch may not be correct; it can thenbe inferred that the patched location is highly related to the actualbuggy location, since otherwise the bug eﬀect would not be muted.2166Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. Towards Boosting Patch Execution On-the-FlyICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAWith uni/f_ied debugging, even when APR techniques cannot/f_ixabug, uni/f_ied debugging still can analyze the patch-execution infor-mation to provide useful hints about potential buggy locations tohelp withmanual repair. In this way, uni/f_ied debugging extendsthe application scope of APR to all possible bugs, not only bugsautomatically/f_ixable. Inspired by uni/f_ied debugging, we also aimto leverage the wealth of patch execution information generatedduring APR. Meanwhile, there are the following major diﬀerences.First, while uni/f_ied debugging aims to leverage patch executioninformation formanualprogram repair, our SeAPR leverages suchinformation to directly boostautomatedprogram repair, i.e., weaim to boost existing APR tools by prioritizing the desired plausi-ble/correct patches earlier in the validation process. Second, theirtechnical principles are substantially diﬀerent. Uni/f_ied debugginganalyzesthe correlation between patch locations and test outcomesto infer potentially buggy locations, while our work analyzesthecorrelation among executed and remaining patchesvia estimatingtheir behavioral similarities to speed up the detection of desiredpatches. In fact, our work is inspired by prior work on mutationtesting [46], which leverages the similarities of modi/f_ied elementsfor diﬀerent mutants to perform test prioritization/reduction foreach mutant to speed upmutation testing.2.2 Cost Reduction for APRDespite the promising future of APR, it can be extremely time con-suming due to the generation and validation of a large numberof possible patches. Actually, the patch validation cost has beenshown to dominate the overall APR cost [30–33]. Therefore, re-searchers have also lookedinto various techniquesto further speedup APR. To reducethe validation time spent on each patch, Ghanbariet al. [22] and Chen et al. [30] proposed to share the same JVMsession across multiple patch executions; in this way, the patchloading and execution time can be substantially accelerated forboth source-code and bytecode level APR techniques. In addition,researchers have also proposed to prioritize and reduce the test exe-cutions for each patch to reduce the validation time for each patch.For example, Qi et al. [47] proposed TrpAutoRepair to prioritize testexecutions for each patch based on historical information to falsifyimplausible patches faster; Mehne et al. [33]f u r t h e rp r o p o s e dt oreduce the number of test executions for each patch, since testsnot covering the patched location(s) cannot help validate the patch.Our SeAPR technique is orthogonal to such existing techniquessince they aim to reduce the execution cost for each patch whileSeAPR aims to reduce the number of patch executions.To reducethe number of validated patches, almost all existingAPR techniques leverage fault localization and various other strate-gies to reduce the possible patch executions. Furthermore, manyexisting APR techniques also leverage other available dynamic orstatic information to prioritize patch executions to/f_ind the desiredpatches faster (e.g., based on various fault localization informa-tion [20,23]). Despite various cost reduction techniques have beenproposed, APR techniques are still rather time consuming for real-world programs [22]. In this paper, we propose the/f_irst techniqueto leverage on-the-$y patch execution information to help betterprioritize patch executions. Note that our technique is orthogonalto all existing patch prioritization techniques and our experimen-tal results demonstrate that ourtechnique cansubstantially speedup state-of-the-art APR techniques with various original patchprioritization strategies.3 STUDIED APPROACHIn this section, we/f_irst present the necessary preliminaries (Sec-tion 3.1). Then, we introduce the detailed SeAPR approach (Sec-tion 3.2). We will also discuss diﬀerent SeAPR variants (Section 3.3).Lastly, we will introduce a further extension of SeAPR to leveragethe patch execution information from other APR tools for evenfaster APR (Section 3.4).3.1 PreliminariesD/e.sc/f.sc#/n.sc#/t.sc#/o.sc/n.sc3.1.Patch validation matrix: MatrixMvde/f_inesthe validation results of all tests against all patch candidates. In thematrix, each cell describes the validation result of testt∈Tagainstpatchp∈P. Possible values for each cell are as follows: (1) - iftremains unvalidated, (2)×iftfails onpand (3)/checkiftpasses onp.Patch IDt1t2t3pb(buggy ver.)×/check×p1×××p2/check×/checkp3×××p4/check/check/checkPatch IDt1t2t3pb(buggy ver.)×/check×p1×--p2/check×-p3×--p4/check/check/checkTable 1: Example of full/partial patch-validation matrixIdeally the patch-validation matrix should befull, i.e., every cellshould be/checkor×. In practice during the APR process however, mostmodern APR tools terminate the test execution for one patch im-mediately after observing any failing test on that particular patch,since the primary goal is to/f_ind correct patches and patches whichfail any test cannot even be plausible. In this way, the APR pro-cess can be largely sped up without sacri/f_icing repair eﬀectiveness.Not all APR tools employ this strategy, so we study both typesof matrices, where some tests remain unexecuted (partial matri-ces) versus where all tests always execute (full matrices). Table 1presents the example full/partial matrices for 4 example patches(i.e.,p1,p2,p3, andp4)o n3e x a m p l et e s t s( i . e . ,/u1D4611,/u1D4612, and/u1D4613). Notethat the/f_irst row for the patch-validation matrix is always the testexecution results of the original buggy program (i.e.,pb).D/e.sc/f.sc#/n.sc#/t.sc#/o.sc/n.sc3.2.Patch modi/f_ication matrix: MatrixMmpresentsall program elements modi/f_ied within each patch. Each cell describesif patchp∈Pmodi/f_ies elemente∈E(i.e., all possible program el-ements). Acceptable values for each cell are as follows: (1)/checkifpmodi/f_ies elementeand (2) - ifpdoes not modify elemente.Patch ID/u1D4521/u1D4522/u1D4523/u1D4524Modi/f_ied Element(s)p1/check/check/check-{/u1D4521,/u1D4522,/u1D4523}p2/check/check/check/check{/u1D4521,/u1D4522,/u1D4523,/u1D4524}p3-/check/check-{/u1D4522,/u1D4523}p4/check---{/u1D4521}Table 2: Example of patch modi/f_ication matrixTable 2 presents an example patch modi/f_ication matrix for theabove four example patches on four program elements. In this2167Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASamuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Lingming Zhangway, sincep1patches program elements {/u1D4521,/u1D4522,/u1D4523}, the/f_irst threecolumns are/checkforp1. Note that the patch modi/f_ication matrix canbe de/f_ined at diﬀerent levels (e.g., at the package, class, method, andstatement granularities) depending on the granularity of consideredprogram elements. In this paper, we mainly consider the methodgranularity, e.g., the columns will be the methods modi/f_ied by eachprogram patch. Compared with the patch validation matrix, thepatch modi/f_ication matrix can be computed much faster (in factwith negligible overhead), and thus can beleveraged to speed upthe patch validation process.3.2 Basic SeAPRGiven the above introduced patch-validation matrix and patch-modi/f_ication matrix (which are readily available for almost all APRtools) for the already executed/validated patches, our SeAPR per-forms on-the-$y patch prioritization to speed up APR.Our basicintuition is that patches similar to executed high-quality patchesare likely to also be high-quality and should therefore be prioritizedearlier; likewise, patches rather similar to executed low-qualitypatches should be deprioritized. In this section, we/f_irst introduceour de/f_initions for patch quality (Section 3.2.1); then, we introducethe detailed strategy to compute patch similarity with high- or low-quality patches (Section 3.2.2); next, we introduce our/f_inal priorityscore computation for all unexecuted patches (Section 3.2.3); lastly,we present our overall algorithm (Section 3.2.4) with correspondingexamples (Section 3.2.5).3.2.1 Patch/Q_uality.When processing patches that have been exe-cuted/validated, we need to estimate the patch’s quality by analyz-ing the patch validation matrix. Intuitively, the ultimate goal of APRis to produce plausible/correct patches that can pass all the originaltests. Therefore, in this study, a patch is classi/f_ied as high-quality(patches we wish to prioritize) if it can makeany originally failingtest pass; likewise, a patch is classi/f_ied as low-quality (patches wewish to deprioritize) if it cannot make any originally failing testpass. Formally, the set of high-quality and low-quality patches canbe de/f_ined as Equations (1) and (2), respectively.Ph={p|∃t,Mv[p,t]=/check∧Mv[pb,t]=×}(1)Pl={p|∀t,Mv[pb,t]=×⇒Mv[p,t]/uni2260/check}(2)Note that we can also easily compute the detailed number of origi-nally failing tests that now pass on apatch; however, prior work hasdemonstrated that the detailed test number can be misleading [29].Of course, this is just the/f_irst work in this new direction, and wehighly encourage other researchers to investigate other better waysto estimate patch quality.3.2.2 Patch Similarity.After calculating patch quality for executedpatches, we iterate through all remaining patches withinPtocompute their similarity information with the executed high/low-quality patches. For each patchpthat has not been validated yet,we compare its patch modi/f_ication matrix information against thatof each of the validated patches. During the comparison, we com-pute the number of elementsmatchinganddiﬀeringamong thetwo compared patches (i.e., two rows in the patch modi/f_icationmatrix). We calculate the number of matching elements by perform-ing theset intersectionon the two patch modi/f_ication matrix rowsrepresenting the two patches. Likewise, we calculate the number ofdiﬀering elements by performing asymmetric set diﬀerence(i.e., A(B = (A - B))(B - A)) on the two patch modi/f_ication matrix rowsrepresenting the two patches.Based on the similarity/dissimilarity with high/low-quality patches,we can compute the following tuple for each unvalidated patchpfor prioritization, (s/uni210E,d/uni210E,s/u1D459,d/u1D459). Our basic idea is thats/uni210Eshouldget increased whenpshares elements with high-quality patches,s/u1D459should get increased whenpshares elements with low-qualitypatches,d/uni210Eshould get increased whenphas set diﬀerence withhigh-quality patches, andd/u1D459should get increased whenphas setdiﬀerence with low-quality patches. Since the detailed number ofthe matching/diﬀerent modi/f_ied elements between two patches cantell the detailed similarity/dissimilarity information, the incrementshould also consider such detailed information. In this way, theformulae for computing the tuple for each unvalidated patchpare:s/uni210E[p]=/summationtext.1p/prime|{e|e∈Mm[p]+Mm[p’]∧p’∈Ph}|(3)s/u1D459[p]=/summationtext.1p/prime|{e|e∈Mm[p]+Mm[p’]∧p’∈Pl}|(4)d/uni210E[p]=/summationtext.1p/prime|{e|e∈Mm[p](Mm[p’]∧p’∈Ph}|(5)d/u1D459[p]=/summationtext.1p/prime|{e|e∈Mm[p](Mm[p’]∧p’∈Pl}|(6)Note thatMm[p] denotes the set of program elements modi/f_iedby patchp. For example, if a validated patchp’ is high-quality andshares elements with the currentp, thes/uni210Eofpis then increased for|Mm[p]+Mm[p’]|. All the other tuple elements can be de/f_ined ina similar way.3.2.3 Patch Prioritization.Based on the similarity tuple we com-puted from the previous step, we can compute the priority score foreach unvalidated patch based on the following intuition: (1) a patchmore similar/dissimilar with high-quality patches should be promot-ed/degraded, (2) a patch similar/dissimilar with low-quality patchesshould be degraded/promoted. Actually, such intuition is quite sim-ilar to traditional spectrum-based fault localization (SBFL) [48],where the intuition is (1) a program element executed/unexecutedby more failed tests should be more/less suspicious, (2) a programelement executed/unexecuted by more passed tests should be less/-more suspicious. In this way, all the traditional fault localizationformulae can be directly leveraged here to compute the patch pri-ority. We use the Ochiai formula, shown in Equation (7), as ourdefault formula as it is often the default formula for SBFL [8,22,23].In this way, patches will be promoted/demoted if they are simi-lar/dissimilar with other high-quality patches, consistent with ourintuition.Ochiai=s/uni210E/radicalbig(s/uni210E+d/uni210E)∗(s/uni210E+s/u1D459)(7)3.2.4 Overall Algorithm.Given the above de/f_initions, we can nowpresent the overall SeAPR algorithm. Shown in Algorithm 1, SeAPR/f_irst initializes the similarity tuples of all patches considered forSeAPR as 1s1(Line 2). Then, SeAPR iterates through all such patchesand validates them in order (Lines 3-16). During each iteration,SeAPR/f_irst gets the patchpwith the highest priority and pops thatfrom the patch listP. Note that for the patches with tied SeAPRpriority scores (e.g., all patches are tied before the/f_irst patch execu-tion), SeAPR prioritizes them with their original ordering from the1Note that they are initialized as 1s (not 0s) for numerical stability.2168Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. Towards Boosting Patch Execution On-the-FlyICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USAAlgorithm 1:SeAPR AlgorithmInput:The original buggy programpb, test suiteT, the list of candidate patchesPconsidered for SeAPR, the similarity tuples (s/uni210E,s/u1D459,d/uni210E,d/u1D459)Output:Plausible patches:P/check1begin2Initialize (s/uni210E,s/u1D459,d/uni210E,d/u1D459) for all patches3whilePis not emptydo4p←pop(P);// pop the remaining patch with the highest priority5Mv←execute(p,T);// validatep6ifpisPLAUSIBLEthen7P/check←p;// putpinto the resulting set for manualinspection8r←computePatchQuality(p,pb,Mv)// Incrementally update the similarity tuples for the remainingpatches9forp’∈Pdo10ifr=HIGH−QUALITYthen11s/uni210E[p’] +=|Mm[p]+Mm[p’]|12d/uni210E[p’] +=|Mm[p](Mm[p’]|13ifr=LOW−QUALITYthen14s/u1D459[p’] +=|Mm[p]+Mm[p’]|15d/u1D459[p’] +=|Mm[p](Mm[p’]|16P←computePriorityScore(P,s/uni210E,d/uni210E,s/u1D459,d/u1D459)corresponding APR tools. Then, SeAPR executes the patch againstthe original test suite and stores the patch execution results intothe patch validation matrixMv(Line 5). Ifpis a plausible patch, itwill be stored in the resulting setP/checkfor manual inspection (Lines6-7). To help with on-the-$y patch prioritization, SeAPR computesthe patch quality information for the current patch following Sec-tion 3.2.1 (Line 8). Next, SeAPR goes through all the remainingpatches to update their similarity tuples following Section 3.2.2(Lines 9-15). Note that all remaining patches will be compared withthe newly executed patch to incrementally update their correspond-ing similarity tuples. Lastly, the priority scores for all remainingpatches will be updated based on the updated similarity tuplesfollowing Section 3.2.3 (Line 16). In this way, the algorithm willproceed until all patches have been validated or the developers/f_indthe desired patch.Note that the time complexity of the SeAPR algorithm is/u1D442(/u1D45B2)at/f_irst glance (/u1D45Bdenotes the number of patches considered for SeAPR),since all the remaining patches need to be updated after each patchexecution. Meanwhile, during our implementation, we realize thatthe similarity scores do not need to be updated for each remainingpatch; instead, we can cluster all remaining patches based on the setof program elements they modify, since all patches with the sameset of modi/f_ied elements will have the same priority. In this way,the time complexity can be reduced to/u1D442(/u1D45B/u1D45A), where/u1D45Adenotesthe number of patch clusters with the same modi/f_ied element sets.Given/u1D45A<</u1D45Bin practice, our actual SeAPR implementation incursnegligible overhead.3.2.5 Example.Let us now use the partial patch validation matrix2(shown in Table 1) and its corresponding patch modi/f_ication matrix(shown in Table 2) as the example to illustrate our SeAPR technique.For this example, if we follow the original patch execution ordering(top-down), we need to execute four patches before/f_inding the/f_inalplausible patch. Now we discuss how our SeAPR canhelp speed upthis process.2Note that we use partial since most APR tools will collect partial matrices, but ouridea generalizes to full matrices (as studied in Section 5.3).Shown in Tables 3 and 4, Column “Quality” describes the patches’actual quality (available after the corresponding patch validation);Column “Match” describes the set of matching elements against thelast executed patch for each patch; Column “Diﬀer” describes theset of diﬀering elements against the last executed patch for eachpatch; Columns “s/uni210E”, “d/uni210E”, “s/u1D459”, and “d/u1D459” represent theaccumulatedsimilarity tuples per patch (initialized as 1s); lastly, Column “Score”represents the Ochiai priority score as de/f_ined in Equation (7).In the/f_irst iteration (shown in Table 3), SeAPR will computethe quality of the executed patch,p1(marked with gray). We canimmediately determine that the patch is low quality simply becauseit cannot make any originally failing tests pass. Note that we alsoshow the quality for all other unexecuted patches to illustrate thequality computation. Then, givenp1has been executed, we canupdate the similarity tuple for each remaining patch. For example,forp2, the set intersection and symmetrical set diﬀerence withp1is {/u1D4521,/u1D4522,/u1D4523}+{/u1D4521,/u1D4522,/u1D4523,/u1D4524}={/u1D4521,/u1D4522,/u1D4523} and {/u1D4521,/u1D4522,/u1D4523}({/u1D4521,/u1D4522,/u1D4523,/u1D4524}={/u1D4524}, respectively. Therefore, sincep1is a low-quality patch,s/u1D459increments by 3 andd/u1D459increments by 1 forp2, resulting in thetuple (s/uni210E=1+0,d/uni210E=1+0,s/u1D459=1+3,d/u1D459=1+1). Similarly, we can computethe similarity tuples for all the other remaining patches. Then, viaapplying the default Ochiai formula on the computed tuples, wecan compute the priority scores for all the three remaining patchesas shown in Column “Score” in Table 3. In this way, the patch withthe highest priority,p4, is selected for the next patch execution.In the second iteration,p4gets executed (marked in gray) asshown in Table 4. Note thatp4is a plausible patch that can passall tests. Therefore, the developers can immediately start manualinspection to check ifp4is the correct patch. Of course, the patchexecution can still continue ifp4is not the correct patch. Continuingthe algorithm, the remaining patches will be further compared withthe newly executedp4to update their similarity tuples. For example,forp2, the set intersection and symmetrical set diﬀerence withp4is {/u1D4521,/u1D4522,/u1D4523,/u1D4524}+{/u1D4521}={/u1D4521} and {/u1D4521,/u1D4522,/u1D4523,/u1D4524}({/u1D4521}={/u1D4522,/u1D4523,/u1D4524},respectively. Sincep4is a high-quality patch,p2’s tuple is updatedby incrementings/uni210Eby 1 andd/uni210Eby 3, resulting in the tuple (s/uni210E=1+1,d/uni210E=1+3,s/u1D459=4+0,d/u1D459=2+0). In this way, we can compute the scoresfor all remaining patches.IDQualityMatchDiﬀers/uni210Ed/uni210Es/u1D459d/u1D459Scorep1Low-------p2High{/u1D4521,/u1D4522,/u1D4523}{/u1D4524}1+01+01+31+10.32p3Low{/u1D4522,/u1D4523}{/u1D4521}1+01+01+21+10.35p4Plausible{/u1D4521}{/u1D4522,/u1D4523}1+01+01+11+20.41Table 3: SeAPR step-by-step when processingp1IDQualityMatchDiﬀers/uni210Ed/uni210Es/u1D459d/u1D459Scorep1Low-------p4Plausible-------p2High{/u1D4521}{/u1D4522,/u1D4523,/u1D4524}1+11+34+02+00.33p3Low{∅}{/u1D4521,/u1D4522,/u1D4523}1+01+33+02+00.22Table 4: SeAPR step-by-step when processingp4For this example, we observe that the original patch executionordering requires 4 patch executions to/f_ind the/f_irst plausible patch,while our SeAPR reduces the number of required patch executionsto only 2, i.e.,4−24=50% patch reduction. In this way, the devel-opers can start manual patch inspection as soon as after 2 patchexecutions.2169Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASamuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Lingming Zhang3.3 SeAPR Variants3.3.1 Patch-Prioritization Formulae.Besides Ochiai, other SBFLformulae can also be applied here. In particular, we study all SBFLformulae from prior work [49] in Section 5.2.3.3.2 Validation-Matrix Types.Besides the partial patch-validationmatrices widely used in practice, we also consider the impact offull validation matrices on SeAPR in Section 5.3.3.3.3 Additional Patch Pa/t_tern Information.By default, SeAPR onlyuses the set of modi/f_ied program elements to calculate patch similar-ity for prioritizing patches on-the-$y. Another SeAPR extension isto compute the similarity score with additional information. There-fore, in Section 5.4, we further study another SeAPR variant, whichadditionally considers that patches sharing the same/f_ixing pat-terns may also share similar program behaviors. In this way, wecan promote patches applying the same/f_ixing patterns as knownhigh-quality patches to further boost SeAPR.D/e.sc/f.sc#/n.sc#/t.sc#/o.sc/n.sc3.3.Patch repair pa/t_tern matrix: MatrixMppresents the applied repair patterns applied to each patch. Each celldescribes if patchp∈Papplies repair patternr∈R(i.e., all prede/f_inedrepair patterns). Acceptable values for each cell are as follows: (1)/checkifpapplies patternrand (2) - otherwise.SeAPR FeaturesPatch ID/u1D45F1/u1D45F2/u1D45F3Modi/f_ied Element(s)Pattern(s)p1/check--{/u1D4521,/u1D4522,/u1D4523}{/u1D45F1}p2-/check-{/u1D4521,/u1D4522,/u1D4523,/u1D4524}{/u1D45F2}p3--/check{/u1D4522,/u1D4523}{/u1D45F3}p4-/check-{/u1D4521}{/u1D45F2}Table 5: Example of patch repair pattern matrixThis variant only slightly diﬀers from the default SeAPR whencomputing patch similarity, e.g., this variant considers both (1) theset of modi/f_ied elements and (2) the applied repair patterns. Basedon the above patch-pattern matrix de/f_inition, we can recomputethe similarity tuples for further improving SeAPR, e.g.,s/uni210E[p] inEquation (3) becomes:s/uni210E[p]=/summationdisplay.1p/prime|{e|e∈Mm[p]+Mm[p’]∧p’∈Ph}|+/summationdisplay.1p/prime|{r|r∈Mp[p]+Mp[p’]∧p’∈Ph}|(8)3.4 Further Leveraging APR Results fromOther ToolsIn practice, one repair tool is often insu%cient to successfully/f_inda correct patch. Thus developers often need to run multiple repairtools to automatically/f_ix a bug. Currently, diﬀerent repair toolsare run in isolation. Our basic idea is thatthe execution results ofother repair tools on the same programcan be used to furtherboost the current repair tool under SeAPR.In particular, we use therepair information fromall but onerepair tools to initialize thepriority score of all patches. For example, when applying SeAPRto TBar on Chart-1, all patch executions results of other tools onChart-1 will be treated as the already executed patches to initializethe priority scores of all TBar patches on Chart-1. With the priorityscores initialized, SeAPR starts with the most prioritized patch andfollows the algorithm outlined in Algorithm 1, updating the alreadyinitialized priority scores of each patch. In this way, SeAPR can geta jumpstart for faster validation.4 EXPERIMENTAL DESIGN4.1 Research QuestionsIn this paper, we study the following research questions:•RQ1:How does the default SeAPR perform on state-of-the-art APR systems?•RQ2:How do diﬀerent prioritization formulae impact SeAPR?•RQ3:How do full patch validation matrices impact SeAPR?•RQ4:How does SeAPR perform when leveraging additionalpatch pattern information?•RQ5:How does SeAPR perform when further leveraginghistorical repair information from other APR systems?Note that we/f_irst study our default SeAPR con/f_iguration withthe Ochiai formula, partial patch validation matrices, similaritycomputation based on modi/f_ied elements in RQ1. Then, in RQ2 andRQ3, we investigate the impact of diﬀerent formulae and types ofpatch validation matrices to study the robustness of SeAPR. In RQ4,we leverage the additional patch pattern information (available forstate-of-the-art template-based APR techniques [4,22]) to furtherboost the eﬀectiveness of SeAPR. Lastly, in RQ5, we investigatewhether historical patch validation information from other APRtools can help achieve even more eﬀective SeAPR for the currentAPR tool.4.2 Evaluation DatasetWe choose to evaluate SeAPR against Defects4J (V1.2.0), the mostwidely used APR dataset to date, which will allow SeAPR to beeasily compared with and replicated in the future. The details forthe dataset are shown in Table 6. Column “#Bugs” presents thenumber of buggy versions studied for each subject. Columns “#Tests”and “LOC” present the number of JUnit tests and lines of codeavailable within the head (i.e., most recent) version of each subject,respectively. For each studied buggy version, the average numberof failing tests is 2.37 (ranging from 1 to 66). Please also note thatPh(the set of high-quality patches, which is closely related to SeAPReﬀectiveness) is not necessarily related to the number of failed testssincePhpatches can pass the failed test(s) but fail on the otheroriginally passing tests (i.e., Equation 1 does not check whetherthe originally passing tests still pass). For example, for the recentPraPR tool, there are 178 high-quality patches for each bug versionon average.SubjectName# Bugs# TestsLOCChartJFreeChart262,20596KLangApache Lang652,24522KMathApache Math1063,60285KTimeJoda-Time274,13028KMockitoMockito framework381,36623KClosureGoogle Closure compiler1337,92790KTotal39521,475344KTable 6: Studied bugs from Defects4J v1.2.02170Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. Towards Boosting Patch Execution On-the-FlyICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USA4.3 Studied APR ToolsFollowing prior work [38,44], we consider 17 program repair toolspublicly available and applicable to Defects4J in this study, showin Table 7. Of these tool candidates, we exclude ACS, DynaMoth,and Nopol due to low numbers of patches generated (i.e.,<500total patches across all studied Defects4J projects). Evaluating ourtechnique on such tools with small numbers of patches will inducenoise into our results; also, in practice, it is not necessary to performon-the-$y patch prioritization on such tools with a small numberof patches. We further exclude Sim/f_ix since the tool stops execu-tion after/f_inding the/f_irst plausible patch. The validation resultsof such tools cannot be degraded, since the last patch is alwaysthe desired plausible patch; results from such tools can only beimproved, biasing our/f_indings. In total, we studied 13 repair toolsin this paper. Note that Arja, GenProg-A, and JGenProg generatenew patches based on patch executions from earlier iterations dueto their evolutionary design. Therefore, we should also have ex-cluded these 3 tools; however, we decided to include these toolssimply to demonstrate the potential bene/f_its of SeAPR (excludingthem also does not aﬀect our/f_indings as their results are consistentwith other tools). Note that among all the three categories of APRtools, the template-based tools have been widely recognized as thestate-of-the-art [22,38], and can generate a large number of patches.Therefore, the template-based APR tools are the main focus of ourtechnique and study.Tool CategoryTool(s)Constraint-basedACS [26], Cardumen [7],DynaMoth [6],Nopol [25]Heuristic-basedArja [10], GenProg-A [10], JGenProg [9], JKali [9], JMutRe-pair [9], Kali-A [10], RSRepair-A [10],Sim/f_ix[ 8 ]Template-basedAvatar [5], FixMiner [36], KPar [37], PraPR [22], TBar [4]Table 7: Repair tools under consideration4.4 Evaluation MetricsWe have adopted the following two evaluation metrics: the reduc-tion on the number of patch executions before/f_inding (1) the/f_irstplausiblepatch and (2) the/f_irstcorrectpatch. We study (1) sincein practice developers will begin manual inspection after the/f_irstplausible patch is found; in this way, faster plausible patch detec-tion can enable developers to start manual inspection earlier (andpotentially speed up the APR process). Similarly, we study (2) sincedevelopers will stop the patch validation process once the correctpatch is found; in this way, faster correct patch detection can saveoverall APR time. Also note that we mainly leverage the reduc-tion on the number of patch executions as recommended by priorwork [38] since time costs are dependent on many factors (e.g., spe-ci/f_ic implementations and test execution engines) unrelated to APRapproaches and are often unstable. Furthermore, we also discussthe results of time costs in Section 5.6.To this end, our primary evaluation metric (patch reduction)can be computed as/u1D443/u1D44F/u1D44E/u1D460/u1D452/u1D459/u1D456/u1D45B/u1D452−/u1D443/u1D45B/u1D452/u1D464/u1D443/u1D44F/u1D44E/u1D460/u1D452/u1D459/u1D456/u1D45B/u1D452.P/u1D44F/u1D44E/u1D460/u1D452/u1D459/u1D456/u1D45B/u1D452represents the positionof the/f_irst desired (i.e., plausible/correct) patch, pre-prioritization.P/u1D45B/u1D452/u1D464represents the position of the/f_irst desired plausible/correctpatch, post-prioritization. Note that when multiple desired patchesare produced, the initial desired patch and the/f_inal desired patchare not necessarily the same patch.4.5 Experimental ProcedureFor each studied APR tool, we evaluate the eﬀectiveness of SeAPRon all patches that can be generated and validated by the tool withinits original time limit (used in its original paper). We/f_irst analyzethe original execution of each tool on a subject-by-subject basisto obtain (1) the original patch execution ordering per repair tooland (2) the position of the earliest plausible/correct patch. Afterinformation collection, we then repeat the patch validation processfor each tool again with our SeAPR on-the-$y patch prioritization.For each given subject, SeAPR initially executes the/f_irstpatchproduced by the tool. After the/f_irst patch execution, SeAPR iteratesthrough all patches not yet validated, following Algorithm 1, torecord the new position for the/f_irst plausible/correct patch. Notethat when computing patch similarity based on patch modi/f_icationinformation, SeAPR will only be applied to the patches belongingto the Top-30 methods since most APR tools only patch such topmethods (the impact of applying SeAPR to diﬀerent numbers of topmethods is also studied in Section 5.6.1); the remaining patches aresimply executed with their original ranking.All our experiments were conducted within the following en-vironment: 36 3.0GHz Intel Xeon Platinum Processors, 60GB ofmemory, and Ubuntu 18.04.4 LTS operating system.5 RESULT ANALYSIS5.1 RQ1: Overall SeAPR E(ectiveness5.1.1/Q_uantitative Analysis.In this section, we/f_irst investigatethe overall eﬀectiveness of our default SeAPR on all 13 studiedrepair tools against Defects4J. Table 8 shows the patch reductionin terms of the/f_irstplausible patches. In this table, Column 1presents the APR systems studied in this work; Columns 2 and 3present the average rank of the/f_irst plausible patches before andafter applying SeAPR to each studied APR system, while Columns4 and 5 present the absolute improvement and the reduction ratioachieved by SeAPR. Note that not all the studied APR tools cangenerate plausible patches for all the studied bugs. Therefore, inthis table, for each APR tool, we only present the results for thebuggy versions on which it can produce plausible patches. Fromthis table, we have the following observations. First, SeAPR im-proves the overall eﬀectiveness of patch validations for almost allrepair tools. For example, SeAPR reduces patch validation by 78.91%on GenProg-A and 54.87% on Avatar. Meanwhile, SeAPR slightlydegrades the performance of Cardumen. One reason is that Cardu-men only generates a small number of patches for the few buggyversions with plausible patches, leaving limited room for furtherimprovement. Second, the SeAPR reduction rates tend to be higherfor APR systems with more patch executions (i.e., the APR systemsthat tend to be more expensive and need more optimizations). Forexample, the reduction rate is above 20% for all APR systems withover 100 patch executions before the/f_irst plausible patch beforeapplying SeAPR, and is above 40% for all APR systems with over 200patch executions before the/f_irst plausible patch before applyingSeAPR. This further demonstrates the eﬀectiveness of SeAPR inboosting potentially expensive APR systems.Similarly, Table 9 further shows the patch reduction ratios for/f_inding the/f_irstcorrect patches. In this table, for each APR tool,we only present the results for the buggy versions on which it can2171Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASamuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Lingming Zhangproduce correct patches. Note that there exists no precise infor-mation on the number of correct patches reported in the originalor subsequent papers for GenProg-A, RSRepair-A, and Kali-A [10],as well as PraPR (due to the large number of plausible patchesgenerated at the bytecode level) [22]. For JKali and JGenProg, wecannot generate any correct patches as reported by the tool’s origi-nal paper [9], likely due to the tools’ nondeterminism and diﬀerentexecution environments. Therefore, we exclude these six repairtools from Table 9. We can/f_ind that SeAPR can achieve even higherpatch reduction rates in terms of the/f_irst correct patch (comparedwith reductions for the/f_irst plausible patches). For example, SeAPRimproves the overall performance of Arja, Avatar, JMutRepair, KPar,and TBar by 72.96%, 80.15%, 53.57%, 64.06%, and 48.33%, respectively.Another interesting/f_inding is that SeAPR does not degrade the per-formance of any of the studied tools, although some tools remainunimproved. Note that the patch reductions for correct patches donot apply to all studied repair tools. Also, the results are mainlyconsistent with (and even better than) that for plausible patches,since SeAPR aims to improve the ranking of plausible patches whilewhether a plausible patch is correct is orthogonal to our technique.Therefore, we use patch reductions for plausible patches for allfollowing RQs due to the space limit.APR SystemsOrig. RankSeAPR RankDiﬀ.ReductionArja203.22121.8381.3940.05%Avatar57.5225.9631.5654.87%Cardumen10.2511.00-0.75-7.32%FixMiner85.2648.1337.1343.55%GenProg-A226.2147.71178.5078.91%JGenProg10.679.830.847.81%JKali6.504.751.7526.92%JMutRepair25.5023.671.837.19%Kali-A25.3323.721.616.36%KPar76.5963.1413.4517.56%RSRepair-A105.5582.5023.0521.84%TBar55.1949.186.0110.90%PraPR2052.8774.871277.9362.25%Table 8: Default SeAPR results for plausible patchesAPR SystemsOrig. RankSeAPR RankDiﬀ.ReductionArja355.0096.00259.0072.96%Avatar44.508.8335.6780.15%Cardumen17.0017.000.000.00%FixMiner3.503.500.000.00%JMutRepair28.0013.0015.0053.57%KPar58.4321.0037.4364.06%TBar33.0017.0515.9548.33%Table 9: Default SeAPR results for correct patches5.1.2/Q_ualitative Analysis.Next, we present detailed examples toinvestigate the performance of SeAPR. We/f_irst look into caseswhere SeAPR can help boost APR:Example 1:Figure 1a shows one of the low-quality patches pro-duced by Arja on Chart-19 while Figure 1b shows one of severalgenerated plausible patches. Note that these two patches modifydiﬀerent/f_iles. In this example, we observe how low-quality patchesmay help prioritize plausible patches. According to our technique,other patchessharing similar modi/f_ied elementswith these low-quality patches aredeprioritized. Thus as any low-quality patch@@ -911,7 +911,6 @@ ...public voidsetRangeAxis(intindex,ValueAxisaxis) {-s e t R a n g e A x i s ( i n d e x , a x i s ,true);}(a) Low-quality patch@@ -161,7 +161,10 @@protected intindexOf(Object object) {...-return-1;+if(object ==null){+throw newIllegalArgumentException(!Null␣/quotesingle.Varobject/quotesingle.Var␣argument.!); }+return-1;}(b) Plausible patchFigure 1: Arja Chart-19 patches@@ -1456,7+1456,7 @@NodeMismatch checkTreeEqualsImpl(Node node2) {...res = n.checkTreeEqualsImpl(n2);if(res !=null){-returnres;+return null;}}...(a) High-quality patch@@ -1450,7+1450,7 @@NodeMismatch checkTreeEqualsImpl(Node node2) {...Node n, n2;for(n = first, n2 = node2.first;res ==null&& n !=null;-n=n . n e x t ,n 2=n 2 . n e x t ){+n=n ,n 2=n 2 . n e x t ){if(node2 ==null){throw newIllegalStateException();}...(b) Plausible patchFigure 2: PraPR Closure-120 patchesmodifying one particular set of methods is validated, all other sim-ilar patches are deprioritized. This essentially results in (1) theclustering of patches based on the set of modi/f_ied methods and(2) prioritizing/deprioritizing these clusters. Upon validation ofconsecutive low-quality patches, this phenomenon culminates to abreadth-/f_irst exploration of the patch clusters, mitigating the riskof some high-quality patches getting starved. This process repeatsuntil Arja/f_inds plausible patch from Figure 1b. With SeAPR, weobserve Arja validates the plausible patch 33rd instead of 626th, apatch reduction of 94.7%.Example 2:Since PraPR tends to generate substantially morepatches than other APR tools, we further present how SeAPR canhelp prioritize the plausible patches for PraPR. Thenon-plausiblehigh-quality patchgenerated by PraPR as shown in Figure 2a modi-/f_ies methodNodeMismatch.checkTreeEqualsImpl(Node node2)and is able to pass some originally failed tests since the method isquite in$uential for the failed tests. In this way, it can help substan-tially improve the ranking of a plausible patch modifying the samemethod as shown in Figure 2b . After applying SeAPR, the/f_irstplausible patch can be ranked at 46th now (at 3165th originally), apatch reduction of 98.5%.Since SeAPR is a heuristic-based technique, it does not work forall cases. Therefore, we also look into the negative cases:2172Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. Towards Boosting Patch Execution On-the-FlyICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USA@@ -123,7 +123,6 @@public classStrBuilderimplementsCloneable {publicStrBuilder(String str) {...buffer =new char[CAPACITY];}else{buffer =new char[str.length() + CAPACITY];-append(str);}}...(a) Non-matching High-quality patch@@ -1113,7+1113,7 @@public classStrBuilderimplementsCloneable {private voiddeleteImpl(intstartIndex ,intendIndex ,intlen) {-S y s t e m . a r r a y c o p y (buffer, endIndex , buffer, startIndex , size -endIndex);+S y s t e m . a r r a y c o p y (buffer, endIndex ,buffer , startIndex ,+capacity() - endIndex);size -= len;}...(b) Plausible patchFigure 3: PraPR Lang-61 patches@@ -530 +530 @@public final classMathUtils {public static booleanequals(double[] x,double[] y){...for(inti=0 ;i<x . l e n g t h ;+ + i ){-if(!equals(x[i], y[i])) {+if((!equals(x[i], y[i])) && (x.length != y.length)) {return false;...(a) Low-quality patch@@ -530 +530 @@public final classMathUtils {public static booleanequals(double[] x,double[] y){...for(inti=0 ;i<x . l e n g t h ;+ + i ){-if(!equals(x[i], y[i])) {+if(!equals(x[i], y[i], i)) {return false;...(b) Plausible patchFigure 4: TBar Math-63 patchesExample 3:Figure 3b presents the only plausible patch producedby PraPR on Lang-61, which modi/f_ies methoddeleteImpl(int,int, int). However, variousnon-plausible high-quality patchesgenerated by PraPR modify a number of other methods, such as thepatch shown in Figure 3a modifyingStrBuiler(String). Afterdetecting such high-quality patches, other patches sharing the samemodi/f_ied elements are prioritized, causing some patches originallyranked below the plausible patch to be executed earlier. As a result,the rank of the plausible patch is degraded from 182nd to 347th.Example 4:The only plausible patch which is also the only high-quality patch generated by TBar for Math-63 is shown in Figure 4b.However, as depicted in Figure 4a, there are many other patcheswhich modify the same method as the plausible patch but cannotmake any original failing tests pass. Many of such low-qualitypatches are originally ranked above the only plausible patch. As aresult, validation of these patches will degrade the execution of theplausible patch. Eventually, the plausible patch initially ranked at28th is then degraded to 55th.Finding 1:SeAPR can substantially reduce patch executionsbefore/f_inding the/f_irst plausible/correct patches for almost allstudied repair tools, with a maximum improvement of 78.91%(plausible) / 80.15% (correct).
Figure 5: Impact of di(erent formulae on patch reduction5.2 RQ2: Impact of Di(erent FormulaeIn RQ1, we studied SeAPR with the Ochiai formula. In fact, SeAPRis a general approach and can leverage any other existing SBFLformula. Therefore in this RQ, we further investigate the impactof diﬀerent SBFL formulae on the eﬀectiveness of SeAPR. Figure 5shows the experimental results of 8 widely studied SBFL formu-lae [49] on the 13 repair tools in terms of patch reduction. Fromthe results, we observe that SeAPR reduces the patch validationsof studied SBFL formulae across almost all repair tools, by up to78.94% (GenProg-A with Kulczynski and Jaccard). Furthermore, fordiﬀerent SBFL formulae, the overall patch reduction on all the stud-ied 13 tools is rather stable. For example, the formula with the bestoverall improvement is SBI (30.27%) and the formula with the worstperformance is Op2 (25.74%), i.e., the diﬀerence of all studied for-mulae is only 4.5 percentage points(pp). Op2 performs worse thanthe other formulae because it mainly considers thes/uni210Einformationwhile largely ignoring other valuable information from the simi-larity tuples, demonstrating the importance of all the informationtraced by SeAPR.Finding 2:All studied formulae achieve stable performance,speeding up the overall validation by at least 25.74%, demon-strating the eﬀectiveness of our design.5.3 RQ3: Impact of Full Validation MatrixWe now have studied SeAPR with the default partial patch-validationmatrices. In this section, we further investigate the performance ofSeAPR with full patch-validation matrices. Figure 6 presents thepatch reductions achieved by SeAPR with the partial and full patch-validation matrices. From this/f_igure, we see that SeAPR can achievesigni/f_icant reductions on both full and partial patch-validation ma-trices, indicating the general applicability of SeAPR. For example,the overall reduction by SeAPR with the partial/full matrices is28.53%/20.06% on all studied tools. Interestingly, despite the over-all positive reductions, SeAPR degrades the performance of somerepair tools when using full matrices, e.g., the patch reduction onArja changes from 40.05% (with partial matrices) into -16.27% (withfull matrices). One possible reason for why SeAPR performs a bitworse with full matrices is that a signi/f_icant portion of low-quality2173Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASamuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Lingming Zhang
−20%0%20%40%60%80%
ArjaAvatarCardumenFixMinerGenProg−AJGenProgJKaliJMutRepairKali−AKParPraPRRSRepair−ATBarpatch  reductionValidationMatrixfull−matrixpartial−matrixFigure 6: Impact of validation matrices on SeAPRspatches with partial matrices are considered high-quality patcheswith full matrices, since full matrices execute all tests against eachpatch and can potentially make more failing tests pass.Finding 3:SeAPR substantially reduces patch executions onboth partial and full patch-validation matrices and tends toperform slightly better with partial matrices.5.4 RQ4: Impact of Additional Patch PatternInformationIn this RQ, we examine the impact of utilizing additional informa-tion for computing patch similarity. We execute the default con/f_igu-ration of SeAPR and compute patch similarity using both (1) the setof modi/f_ied elements and (2) the applied patch patterns. We achievethis by looking speci/f_ically only at tools which apply prede/f_inedrepair patterns, i.e., those tools categorized astemplate-based. Notethat we collect the applied/f_ix patterns as directly implemented byeach tool. The studied template-based tools are TBar, KPar, Avatar,and PraPR with 18, 13, 19, and 18 repair patterns, respectively.3Table 10 shows the result of this con/f_iguration evaluated againstour default SeAPR setting. In this table, Column 1 presents the APRsystems studied in this RQ; Column 2 presents the reduction resultswhen using the method location information to compute patchsimilarity (i.e., the default SeAPR); Column 3 presents the reductionresults when using only the new patch pattern information tocompute patch similarity;/f_inally, Column 4 presents the reductionresults when combining both method and patch pattern informationfor computing patch similarity. From the table, we distinctly observethat SeAPR with only method location information or patch patterninformation can both achieve nontrivial reductions. For example,the reduction rates range from 10.90% (TBar) to 62.25% (PraPR)when only using method location information, while ranging from31.25% (KPar) to 61.96% (Avatar) when only using patch patterninformation. Furthermore, the SeAPR with both method locationand patch pattern information can achieve even high reductions.For example, the reduction rates now range from 33.87% to 80.53%for all the four studied APR systems.Finding 4:Patch pattern information further improves SeAPR’sperformance on all studied repair tools by up to 29.75 pp. Fur-thermore, SeAPR with only patch pattern information canalso achieve competitive patch reduction.3Please note we excluded FixMiner as we failed to dump its/f_ixing-pattern information.MethodPatternMethod+PatternAvatar54.87%61.96%80.53%KPar17.56%31.25%33.87%TBar10.90%35.73%40.65%PraPR62.25%42.27%66.88%Table 10: SeAPR with patch pattern information
0%20%40%60%80%
ArjaAvatarCardumenFixMinerGenProg−AJGenProgJKaliJMutRepairKali−AKParPraPRRSRepair−ATBarpatch  reductionTechSeAPRSeAPR+HistoryFigure 7: SeAPR with historical APR results from other tools
Figure 8: Impact of top method number on patch reduction5.5 RQ5: Boosting SeAPR via Other APR ToolsFigure 7 shows the impact of historical information for our defaultcon/f_iguration following Section 3.4. Note, we/f_ilter out all knowncorrect patches from the historical information from other APRtools, since developers will stop the repair process after/f_indingany correct patch with other tools, thus alleviating the need forSeAPR. Compared to Table 8, we observe improvements acrossmost repair tools, up to 60.53 pp (Kali-A). Most notably, the onlytool with originally negative performance (Cardumen) now has34.15% reduction, i.e., over 41 pp improvement. On the other hand,the only instances of degradation come from GenProg-A, Avatar,TBar, and PraPR. The maximum degradation is only 14.71 pp (i.e.,TBar degrades from 10.90% to -3.81%), while the degradation is lessthan 8 pp and the reductions are still positive for all other instances.Overall, such historical information can further boost SeAPR by18.98 pp on average for all the studied APR tools.Finding 5:Supplementing SeAPR with patch-execution in-formation from other repair tools can further boost SeAPR.This extra historical information, compared with SeAPR’sdefault con/f_iguration, can further boost SeAPR by 18.98 ppon average and up to 60.53 pp.
2174Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. Towards Boosting Patch Execution On-the-FlyICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USA5.6 Discussion5.6.1 Impact of Top Methods Considered.As shown in Section 4.5,to avoid degrading high-quality or plausible patches to the veryend of all patches for diﬀerent modi/f_ied locations, we by defaultapply SeAPR to the Top 30 methods for the programs under test.In this section, we further investigate the eﬀectiveness of SeAPRwhen applied to diﬀerent numbers of top methods. Figure 8 showsthe experimental results for applying SeAPR to Top 10 to Top 100methods (with the interval of 10). In this/f_igure, the x axis showsthe number of top methods considered for SeAPR, while the y axispresents the corresponding reduction achieved by diﬀerent APRsystems (denoted by colored lines). According to the/f_igure, wehave the following observations. First, the reduction rates increasedramatically for almost all APR systems when increasing the num-ber of top methods from 10 to 20. The reason is that SeAPR willhave the chance to improve more patches when more top methodsare considered for SeAPR. Second, the reduction rates for most APRsystems will become stable when SeAPR is applied to beyond Top20 methods. One reason is that for all other APR systems (exceptPraPR), they rarely patch more than 20 methods in a program undertest, making the SeAPR results largely unchanged when consideringmore than Top 20 methods. Meanwhile, we can observe that evenfor PraPR (the purple line), which can patch far more methods thanother APR systems, the reduction rate is still all positive, indicatingthe scalability and wide applicability of SeAPR.5.6.2 SeAPR Overhead.The SeAPR algorithm is only related to thepatch-validation phase, which is a pretty standard thing across APRtools. Therefore, the changes are minimal for applying SeAPR overexisting APR tools. It is also important to analyze the overheadof SeAPR, since it does not pay oﬀif SeAPR itself is extremelycostly. Interestingly, we found that although SeAPR’s overheadoften increases when considering more top methods, the averageoverhead for running SeAPR on each buggy version never exceeds2s for any of the APR tools studied (even when considering 100 topmethods). The reason is that our implementation has been highlyoptimized as shown in Section 3.2.4. Such overhead is negligiblefor APR tools, which typically take hours to/f_ix a bug.5.6.3 Nondeterminism in APR Tools.We currently report the re-sults for one run since our goal is to speed upstate-of-the-artAPR techniques, while the recent state-of-the-art APR techniquesare mostly deterministic, e.g., all the studied APR tools that cancorrectly/f_ix over 20 bugs for Defects4J (including PraPR/Tbar/-Fixminer/Avatar) are deterministic. Meanwhile, to investigate theimpact of APR non-determinism on SeAPR eﬀectiveness, we fur-ther rerun the experiments for the non-deterministic RSRepair-Aand Kali-A tools for 5 times. The experimental results demonstratethat SeAPR can achieve an average reduction of 23.68%/4.88% forRSRepair-A/Kali-A, which is similar to the results shown in Table 8.5.6.4 Metrics.As shown by recent work [38], using time costs forevaluating APR e%ciency often depends on many random factors(such as the execution environments, test execution engines, andspeci/f_ic implementation choices) and can be quite unstable. Fur-thermore, for the same APR tool, the reduction in patch executionsis largely proportional to the reduction in time cost since SeAPR isoblivious to the patch execution time distribution. Therefore, wefollowed the recommendation of the prior work [38] and have usedthe number of patch executions as our main metric. Meanwhile,it is also important to check if the reduction in terms of patchexecutions aligns well with the reduction in terms of time costs.Therefore, we further trace the detailed time cost reduction on anexample tool, i.e., state-of-the-art PraPR. The experimental resultshows that the reduction is 62.25% and 58.56% (including SeAPRoverhead) in terms of patch executions and time cost, respectively.This further demonstrates the validity of our used metric.5.7 Threats to Validity5.7.1 Internal Validity.All of our results are dependent on thecorrectness of the implementation of all the studied techniques.We mitigate this threat by obtaining the source code of APR toolsfrom their websites/authors. Also, three authors implemented threeseparate versions of SeAPR variants to perform diﬀerential testingto ensure the result correctness. Following prior APR work [50],three authors have participated in patch correctness checking toensure the inspection correctness. Still, there may be human errorsin the manual inspection process that may aﬀect our/f_indings.5.7.2 External Validity.While our approach is generalizable to anytype of patch-and-validate system, we only evaluate Java-basedAPR tools, which may skew results. To mitigate this threat we1) studied a wide variety of APR tools, and 2) consider tools ac-tively used in recent and related work. We also actively evaluateour technique on the most widely studied Defects4J dataset, withhundreds of real-world bugs. Adding more benchmark suites cande/f_initely further reduce this threat. However, some of the studiedAPR tools cannot be easily applied to other benchmarks (due toimplementation/design limitations of the original APR tools).5.7.3 Construct Validity.A major threat to validity lies in our eval-uation metrics. To mitigate this, we compute the number of patchexecutions recommended by recent work [38]. Meanwhile, we fur-ther discuss the reduction results in terms of time cost.6 CONCLUSIONWe have proposed the/f_irst self-boosted APR technique, SeAPR,which leverages the execution information of validated patchesduring APR to prioritize the remaining patches on-the-$y for fasterAPR. Our study on state-of-the-art APR systems and the widelyused Defects4J benchmark demonstrates that (1) the default SeAPRcan substantially speed up thestudied APR techniques by up to79% with negligible overhead, (2) SeAPR has stable performancewhen using diﬀerent formulae for computing patch priority anddiﬀerent types of patch-execution matrices, (3) additional patch pat-tern information for patch similarity computation can further boostSeAPR, and (4) SeAPR can even utilize historical patch-executioninformation from other APR tools to boost current APR tools.ACKNOWLEDGMENTSWe appreciate the insightful comments from all the anonymousreviewers. This work was partially supported by National ScienceFoundation under Grant Nos. CCF-2131943 and CCF-2141474, aswell as Ant Group.2175Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ’22, May 21–29, 2022, Pi/t_tsburgh, PA, USASamuel Benton, Yuntong Xie, Lan Lu, Mengshi Zhang, Xia Li, and Lingming ZhangREFERENCES[1]“Tricentis reports,” 2020. [Online]. Available: https://www.tricentis.com/resources/software-fail-watch-5th-edition/[2]C. Boulder, “University of cambridge study,” https://www.roguewave.com/company/news/2013/university-of-cambridge-reverse-debugging-study, 2013,accessed: Jan. 8, 2019.[3]S. Wang, M. Wen, B. Lin, X. Mao, H. Wu, D. Zou, H. Jin, and Y. Qin, “AutomatedPatch Correctness Assessment: How Far are We?” inProceedings of the 35thIEEE/ACM International Conference on Automated Software Engineering, 2020, pp.1166–1178.[4]K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyandé, “TBAR: Revisiting template-basedautomated program repair,” inProceedings of the 28th ACM SIGSOFT InternationalSymposium on Software Testing and Analysis, 2019.[5]K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyande, “AVATAR: Fixing Semantic Bugswith Fix Patterns of Static Analysis Violations,” inProceedings of the 2019 IEEE26th International Conference on Software Analysis, Evolution, and Reengineering,2019.[6]T. Durieux and M. Monperrus, “DynaMoth: Dynamic code synthesis for automaticprogram repair,” inProceedings - 11th International Workshop on Automation ofSoftware Test, AST 2016, 2016.[7]M. Martinez and M. Monperrus, “Ultra-large repair search space with automati-cally mined templates: The cardumen mode of astor, ” inLecture Notes in ComputerScience, 2018.[8]J. Jiang, Y. Xiong, H. Zhang, Q. Gao, and X. Chen, “Shaping program repair spacewith existing patches and similar code,” inProceedings of the 27th ACM SIGSOFTInternational Symposium on Software Testing and Analysis, 2018.[9]M. Martinez and M. Monperrus, “ASTOR: A program repair library for Java(Demo),” inProceedings of the 25th International Symposium on Software Testingand Analysis, 2016.[10]Y. Yuan and W. Banzhaf, “ARJA: Automated Repair of Java Programs via Multi-Objective Genetic Programming,”IEEE Transactions on Software Engineering,2018.[11]Y. Lou, J. Chen, L. Zhang, D. Hao, and L. Zhang, “History-driven build failure/f_ixing: How far are we?” inProceedings of the 28th ACM SIGSOFT InternationalSymposium on Software Testing and Analysis, 2019.[12]M. Wu, L. Zhang, C. Liu, S. H. Tan, and Y. Zhang, “Automating CUDA synchro-nization via program transformation,” inIEEE/ACM International Conference onAutomated Software Engineering, ASE 2019, 2019.[13]J. Jiang, L. Ren, Y. Xiong, and L. Zhang, “Inferring program transformationsfrom singular examples via big code,” inIEEE/ACM International Conference onAutomated Software Engineering, ASE 2019, 2019.[14]F. Long and M. Rinard, “Automatic patch generation by learning correct code, ” inProceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principlesof Programming Languages, 2016, pp. 298–312.[15]Z. Qi, F. Long, S. Achour, and M. Rinard, “An analysis of patch plausibility andcorrectness for generate-and-validate patch generation systems,”InternationalSymposium on Software Testing and Analysis, pp. 24–36, 2015.[16]F. Long and M. Rinard, “Staged program repair with condition synthesis,” inProceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,2015, pp. 166–178.[17]F. Long, P. Amidon, and M. Rinard, “Automatic inference of code transforms forpatch generation,” inProceedings of the 2017 11th Joint Meeting on Foundations ofSoftware Engineering, 2017, pp. 727–739.[18]F. Long and M. Rinard, “An analysis of the search spaces for generate and vali-date patch generation systems,” inProceedings of the International Conference onSoftware Engineering, 2016.[19]W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, “A survey on software faultlocalization,”IEEE Transactions on Software Engineering, vol. 42, no. 8, pp. 707–740,2016.[20]R. Abreu, P. Zoeteweij, and A. J. Van Gemund, “On the accuracy of spectrum-basedfault localization,” inProceedings - Testing: Academic and Industrial ConferencePractice and Research Techniques, TAIC PART-Mutation 2007, 2007.[21]J. A. Jones and M. J. Harrold, “Empirical evaluation of the tarantula automaticfault-localization technique,” in20th IEEE/ACM International Conference on Auto-mated Software Engineering, ASE 2005, 2005.[22]A. Ghanbari, S. Benton, and L. Zhang, “Practical program repair via bytecodemutation,” inISSTA 2019 - Proceedings of the 28th ACM SIGSOFT InternationalSymposium on Software Testing and Analysis, 2019.[23]M. Wen, J. Chen, R. Wu, D. Hao, and S. C. Cheung, “Context-aware patch genera-tion for better automated program repair, ” inProceedings - International Conferenceon Software Engineering, vol. 2018-January, 2018.[24]C. Le Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer, “A systematic study ofautomated program repair: Fixing 55 out of 105 bugs for $8 each,”Proceedings ofInternational Conference on Software Engineering, pp. 3–13, 2012.[25]J. Xuan, M. Martinez, F. DeMarco, M. Clement, S. L. Marcote, T. Durieux,D. Le Berre, and M. Monperrus, “Nopol: Automatic Repair of Conditional State-ment Bugs in Java Programs,”IEEE Transactions on Software Engineering, vol. 43,no. 1, 2017.[26]Y. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, and L. Zhang, “Precise con-dition synthesis for program repair,” inIEEE/ACM 39th International Conferenceon Software Engineering, 2017, pp. 416–426.[27]A. Marginean, J. Bader, S. Chandra, M. Harman, Y. Jia, K. Mao, A. Mols, andA. Scott, “Sap/f_ix: Automated end-to-end repair at scale,” inIEEE/ACM 41st In-ternational Conference on Software Engineering: Software Engineering in Practice(ICSE-SEIP), 2019, pp. 269–278.[28]R. K. Saha, Y. Lyu, H. Yoshida, and M. R. Prasad, “Elixir: Eﬀective object-orientedprogram repair,” inIEEE/ACM International Conference on Automated SoftwareEngineering, 2017, pp. 648–659.[29]Y. Lou, A. Ghanbari, X. Li, L. Zhang, H. Zhang, D. Hao, and L. Zhang, “Canautomated program repair re/f_ine fault localization? a uni/f_ied debugging approach,”inProceedings of the 29th ACM SIGSOFT International Symposium on SoftwareTesting and Analysis, 2020.[30]L. Chen, Y. Ouyang, and L. Zhang, “Fast and precise on-the-$y patch validationfor all,” in2021 IEEE/ACM 43rd International Conference on Software Engineering(ICSE), 2021, pp. 1123–1134.[31]C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic softwarerepair,”Software quality journal, vol. 21, no. 3, pp. 421–443, 2013.[32]W. Weimer, Z. P. Fry, and S. Forrest, “Leveraging program equivalence for adaptiveprogram repair: Models and/f_irst results,” inIEEE/ACM International Conferenceon Automated Software Engineering, 2013, pp. 356–366.[33]B. Mehne, H. Yoshida, M. R. Prasad, K. Sen, D. Gopinath, and S. Khurshid, “Ac-celerating search-based program repair,” inInternational Conference on SoftwareTesting, Veri/f_ication and Validation, 2018, pp. 227–238.[34]Y. Pei, C. A. Furia, M. Nordio, Y. Wei, B. Meyer, and A. Zeller, “Automated/f_ixingof programs with contracts,”IEEE Transactions on Software Engineering, vol. 40,no. 5, 2014.[35]R. van Tonder and C. L. Goues, “Static automated program repair for heap prop-erties,” inProceedings of the 40th International Conference on Software Engineering,2018.[36]A. Koyuncu, K. Liu, T. Bissyandé, D. Kim, J. Klein, M. Monperrus, and Y. LeTraon,“FixMiner: Mining relevant/f_ix patterns for automated program repair, ”EmpiricalSoftware Engineering, 2020.[37]K. Liu, A. Koyuncu, T. Bissyande, D. Kim, J. Klein, and Y. LeTraon, “You cannot/f_ix what you cannot/f_ind! an investigation of fault localization bias in benchmark-ing automated program repair systems,”Proceedings of 12th IEEE InternationalConference on Software Testing, Veri/f_ication and Validation, 2019.[38]K. Liu, S. Wang, A. Koyuncu, K. Kim, T. F. Bissyande, D. Kim, P. Wu, J. Klein,X. Mao, and Y. L. Traon, “On the e%ciency of test suite based program repaira systematic assessment of 16 automated repair systems for java programs,” inProceedings of International Conference on Software Engineering, 2020.[39]D. Kim, J. Nam, J. Song, and S. Kim, “Automatic patch generation learned fromhuman-written patches,” inProceedings of International Conference on SoftwareEngineering, 2013.[40]X. B. D. Le, D. Lo, and C. Le Goues, “History driven program repair,” inIEEEInternational Conference on Software Analysis, Evolution, and Reengineering, vol. 1,2016, pp. 213–224.[41]L. De Moura and N. Bjørner, “Z3: An e%cient SMT Solver,” inLecture Notes inComputer Science (including subseries Lecture Notes in Arti/f_icial Intelligence andLecture Notes in Bioinformatics), vol. 4963 LNCS, 2008.[42]T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, “CoCoNuT: Combiningcontext-aware neural translation models using ensemble for program repair,”Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testingand Analysis, pp. 101–114, 2020.[43]Y. Ding, B. Ray, and V. J. Hellendoorn, “Patching as Translation : the Data andthe Metaphor,” inProceedings of the 35th IEEE/ACM International Conference onAutomated Software Engineering, 2020, pp. 327–338.[44]S. Benton, X. Li, Y. Lou, and L. Zhang, “On the Eﬀectiveness of Uni/f_ied Debugging:An Extensive Study on 16 Program Repair Systems,” inProceedings of the 35thIEEE/ACM International Conference on Automated Software Engineering, 2020.[45]——, “Evaluating and improving uni/f_ied debugging, ”IEEE Transactions on SoftwareEngineering, 2021.[46]L. Zhang, D. Marinov, and S. Khurshid, “Faster mutation testing inspired by testprioritization and reduction, ” inInternational Symposium on Software Testing andAnalysis, 2013.[47]Y. Qi, X. Mao, and Y. Lei, “E%cient automated program repair through fault-recorded testing prioritization,” inIEEE International Conference on SoftwareMaintenance, ICSM, 2013.[48]J. A. Jones, M. J. Harrold, and J. Stasko, “Visualization of test information toassist fault localization,” inProceedings of International Conference on SoftwareEngineering, 2002.[49]M. Zhang, X. Li, L. Zhang, and S. Khurshid, “Boosting spectrum-based faultlocalization using pagerank,” inProceedings of the 26th ACM SIGSOFT InternationalSymposium on Software Testing and Analysis, 2017, pp. 261–272.[50]L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,”IEEE Transactions on Software Engineering, vol. 45, no. 1, 2019.2176Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:57:18 UTC from IEEE Xplore.  Restrictions apply. 