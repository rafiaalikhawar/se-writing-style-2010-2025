First Come First Served:
TheImpact ofFile Position onCodeReview
Enrico Fregnan
fregnan,@ifi.uzh.ch
Universityof Zurich
SwitzerlandLarissa Braz
larissa@ifi.uzh.ch
Universityof Zurich
SwitzerlandMarcoD‚ÄôAmbros
marco.dambros@usi.ch
CodeLounge at SoftwareInstitute
Universit√†dellaSvizzera Italiana,
Switzerland
G√ºl√áalƒ±klƒ±
handangul.calikli@glasgow.ac.uk
Universityof Glasgow
ScotlandAlbertoBacchelli
bacchelli@ifi.uzh.ch
Universityof Zurich
Switzerland
ABSTRACT
Themostpopularcodereviewtools( e.g.,GerritandGitHub)present
the files to review sorted in alphabetical order. Could this choice
or, more generally, the relative position in which a file is presented
biastheoutcome of code reviews?We investigatethis hypothesis
bytriangulating complementary evidenceinatwo-step study.
First,weobservedevelopers‚Äôcodereviewactivity.Weanalyze
thereviewcommentspertainingto219,476PullRequests(PRs)from
138 popular Java projects on GitHub. We found files shown earlier
inaPRtoreceivemorecommentsthanfilesshownlater,alsowhen
controllingforpossibleconfoundingfactors: e.g.,thepresenceof
discussion threads or the lines added in a file. Second, we measure
theimpactoffilepositionondefectfindingincodereview.Recruit-
ing106participants,weconductanonlinecontrolledexperiment
inwhichwemeasureparticipants‚Äôperformanceindetectingtwo
unrelated defects seeded into two different files. Participants are
assigned to one of two treatments in which the position of the
defective filesis switched.For onetype ofdefect,participantsare
notaffectedbyitsfile‚Äôsposition;fortheother,theyhave64%lower
oddstoidentifyitwhenitsfileislastasopposedtofirst.Overall,our
findingsprovideevidencethattherelativepositioninwhichfiles
are presented has an impact on code reviews‚Äô outcome; we discuss
theseresults andimplicationsfor tooldesignandcode review.
Preprint: https://doi.org/10.48550/arXiv.2208.04259
Data andMaterials: https://doi.org/10.5281/zenodo.6901285
CCS CONCEPTS
¬∑Software and its engineering ‚ÜíEmpirical software validation .
KEYWORDS
Code Review,ControlledExperiment, Cognitive Bias
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô22, November 14≈õ18,2022, Singapore, Singapore
¬©2022 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3549177ACMReference Format:
Enrico Fregnan, Larissa Braz, Marco D‚ÄôAmbros, G√ºl √áalƒ±klƒ±, and Alberto
Bacchelli.2022.FirstComeFirstServed:TheImpactofFilePositiononCode
Review. In Proceedingsofthe30th ACMJoint EuropeanSoftware Engineering
ConferenceandSymposium on the Foundationsof SoftwareEngineering(ES-
EC/FSE‚Äô22),November14≈õ18,2022,Singapore,Singapore. ACM,NewYork,
NY, USA, 12pages.https://doi.org/10.1145/3540250.3549177
1 INTRODUCTION
Code review is a popular software engineering practice where de-
velopers manually inspect the code written by a peer [ 7,40]. Code
reviewaimstofinddefects[ 11],improvesoftwarequality[ 3,12],
and transfer knowledge [ 7,48]. Over the years, code review has
evolved from a formal strictly-regulated process [ 22] into a less
strictpractice.Contemporarycodereviewingisinformal,asynchro-
nous, change-based,andsupportedbytools[ 9,10,46,48].
The tools used to conduct code reviews share many similari-
ties [11]. In particular, the vast majority of tools (including the
popularGerrit[ 28]andGitHub[ 29])presentthechangestoreview
asalist/sequenceofdiffhunks[ 25]groupedbythefiletheybelong
to. Tools sort these files alphabetically, therefore the changes to
a file named org/Controller.java are always presented before
those to a file named org/Model.java . Could this choice or, more
generally,therelativepositioninwhichafileispresentedinfluence
the outcome ofcode review?
This hypothesis seems to be supported by at least two factors.
First,mostdeveloperstendtostarttheirreviewsintheorderpre-
sentedbythereviewtool[ 12].Second,codereviewisacognitively
demanding task [ 8] whose outcome might be influenced by cog-
nitive factors [ 42,51] also related to the position of the file. For
example,developersmaybeinfluencedby attentiondecrement (a
decreaseinattentionwhenexposedtoalistofelements[ 6])ormay
depletetheir workingmemorycapacity (thememoryforshort-term
storage during ongoing tasks[ 61]) near theend oflongerreviews.
Inthispaper,wesettoinvestigatethishypothesis.Wedothis
bytriangulating complementary evidenceinatwo-step study.
Inthefirststep,wefocusontherelationbetweenfilepositionand
reviewers‚Äôactivity.Wecollectandanalyze219,476PullRequests
(PRs)from138GitHubopen-sourceJavaprojectsandinvestigate
whetherthepositioninwhichafileispresentedinaPRisassociated
with the number of comments the file receives. In fact, the number
ofcommentscanbeusedtoapproximatereviewers‚Äôeffectiveness
483
ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Enrico Fregnan, LarissaBraz,Marco D‚ÄôAmbros,G√ºl √áalƒ±klƒ±, andAlberto Bacchelli
andactivity[ 46].Wefindasignificantcorrelationbetweenthefile‚Äôs
positionandthenumberofcommentsthisfilereceives:Filesshown
laterinaPR receive fewer comments thanfilesshownearlier.
In the second step, we focus on the influence of file position on
defect finding incode review. We design and conduct a controlled
experiment with 106 developers who have to review code in which
we seeded two unrelated bugs (a Corner Case defect and a Missing
Breakdefect)intotwodifferentfiles.Bycreatingtwotreatments
that switch the position of the defective files (first Corner Case
defectandlastMissingBreakdefect,orviceversa),wemeasurethe
influence of file position. While we see no effect for the Missing
Break defect, we find that developers have 64% lower odds to iden-
tify the Corner Case defect when the file containing it is displayed
last as opposed to first. To further confirm our findings, we look
atthe files displayed on the participants‚Äôscreenduring the review
task. We detect a statistically significant difference between the
time participants spend viewing the firstfile andthe last one.
Overall, our findings suggest that the relative position in which
files are presented in a code review has an impact on the code
review‚Äôs outcome. This result has important implications for code
review practiceand fortool design. For instance, code review tool
designersmayconsiderrethinking alphabeticallyordering filesin
favor of a more principled approach ( e.g., showing first the most
problematicfilestoreview).Alternatively,codechangeauthorsmay
considerclearlysignalingdeveloperswheretostarttheirreview,
for example through the use of self-comments or in the change
description,when they implementedmore challenging parts.
2 BACKGROUNDAND RELATED WORK
Inthissection,wepresentrelevantliteratureoncognitiveaspectsin
moderncodereview.Then,weillustratethepsychologicalconcepts
that might constitute the underlying causes of the effect of file
position on code review. Finally, we report possible competing
argumentsagainstthehypothesisthatfilepositionplaysarolein
reviewing.
2.1 CognitiveAspects in CodeReview
Codereviewisacollaborativeprocesswherehumanfactorsplay
a crucial role [ 7]. Previous studies conducted at companies such
asMicrosoft[ 7]andGoogle[ 48]revealedhowcodereviewcould
fosterknowledgetransferamongdevelopersinateamandimprove
shared code ownership. However, code review is not only a collab-
orativeprocess,butitisalsoacognitivelydemandingtaskforthe
single reviewer. In particular, a vast amount of research focused
on reducing developers‚Äô cognitive load to improve reviewers‚Äô per-
formance [ 11,13,31]. For instance, Baum et al .[13] conducted a
controlledexperimenttoinvestigate howdevelopers‚Äôcodereview
performancerelatestotheircognitiveloadandworkingmemory
capacity.Theirfindingshighlightedhowworkingmemoryisassoci-
ated with finding delocalized defects while only weakly associated
withthe abilityto detectotherdefecttypes.
Another group of studies addresses developers‚Äô cognitive biases
that might affect code review outcome [ 20,36,51,55].Huang et al .
investigated how cognitive biases relate to code review process in
acontrolledexperimentusingmedicalimagingandeye-tracking.
Chattopadhyay et al .synthesized helpful practices to reduce theeffectofcognitivebiasesonsoftwaredevelopmentactivities,includ-
ingcodereview.Intheirtwo-partfieldstudy,theauthorsidentified
themanifestationofvariouscognitivebiasesduringcodereviews
(e.g., representativeness, availability, anchoring, confirmation bias).
Spadini et al .[51] investigated the effect of primingon review-
ers‚Äô ability to detect bugs due to their proneness to availability
bias.Inthecontrolledexperiment,areviewcommentwasusedto
prime the participants to look for defects of the same type. The
authorsinitiallyhypothesizedthattheparticipantswouldoverlook
defects of other types present in the code due to their proneness
to availability bias. However, the experiment results show that the
presence of a review comment identifying a bug does not prime
reviewerstowardslookingfordefectsofsimilartypeoverlooking
otherdefecttypes.Onthecontrary,suchareviewcommentactsas
areminderforbugsdevelopersusuallydonotlookforduringtheir
daily practices.
2.2 PsychologicalFactors Relatedto Position
Basedonthefindingsof Baumetal .[12],whichsuggestthatdevel-
opersstartacodereviewfromthefirstfileinthechange-set,the
followingpsychologicalfactorsseemplausibleexplanationsbehind
the potentialeffectoffiles‚Äô position oncode review.
Attention decrement. Attention decrement is defined as ≈Ç the
tendency for people to pay less attention to stimuli coming later in
asequentialoccurrenceorpresentationandthustorememberthem
lesswell≈æ[6].Togiveapracticalexample,astudentgivenalistof
terms to memorize is likely to have more difficulties committing to
memorythoseattheend. Hendricketal .[33]identifiedattention
decrementasthemostfeasibleexplanationofwhypeopletendto
remember the firstinformation they see the most.
During codereview, developers mightbesubjected to this phe-
nomenon, slowly decreasing their attention the more the review
continues. As the majority of reviewers begin their review from
the first file in a change-set [ 12], this decrease in attention is more
likely to impact the last files in a review change-set. This would
sustain our hypothesis that the position of the files in code review
matters.
Working memory. Working memory is defined as the part of
humanmemoryneededforshort-termstorageduringongoingcog-
nitive processes [ 13,61]. However, although the working memory
capacity varies from individual to individual, its amount is still
limited[21].
In software engineering, the study of Bergersen and Gustafs-
soninvestigatedthe effect ofworking memory on programming
performance[ 14].Theirfindingsshowedhowworkingmemoryin-
fluences performance (albeit mediated by the developers‚Äô program-
mingknowledge). Baumetal .studiedtheeffectofworkingmemory
oncodereviewperformance[ 13].Theirexperimentshowedacorre-
lationbetweenworkingmemoryandthereviewers‚Äôeffectivenessin
finding delocalized defects. Reviewers might deplete their working
memory looking at the first files in a review change-set, leading
toanexhaustionofthisresourcethatmightnegativelyinfluence
theircode reviewperformance onthe last filesthey inspect.
484FirstCome FirstServed: TheImpact of FilePosition onCode Review ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
2.3 RelatedCompetingArguments
Althoughpreviousevidence[ 12]suggeststhatdevelopersstarta
codereviewfromthefirstfileinthechange-set,somefactorsmight
affecttheorderinwhichdevelopersreviewfilesduringcodereview.
Given the findings of some eye-tracking studies ( e.g., [2,19,47]),
itseemsplausibletothinkthatsomefilesinacodechangemight
attract reviewers‚Äô attention more due to their content ( e.g., files
thatcontainmorecallterms,controlflowterms)regardlessoftheir
position.Forinstance,Abidetal .[2]foundthatdevelopersvisitcall
termsmoreoftenthannon-calltermsandspendthelongesttime
readingcallterms.Thenextmostvisitedandreadlocationswere
control flow terms and signatures. Furthermore, in an earlier study
Rodegheroetal . [47]found thatdevelopers considermethodsig-
natures as the most important section of the code followed by call
terms andthecontrol flow terms. Moreover, token lengthandfre-
quency in a source code might also influence developers‚Äô attention.
The results Al Madi et al .[4] obtained indicate that participants
fixate longer low-frequency tokens and tokens containing more
characters.
Furthermore,inaneye-trackingstudy,Busjahnetal .[19]showed
thatpeoplereadsourcecodeinamorenon-linearfashionthanread-
ing natural language. However, thefindings by Busjahn et al .only
explain how developers read code in a single code file: the authors
showedtheparticipantsJavaclassesandpseudocodesrangingfrom
afewlinesofcodetoanentirescreenfulloftext.Therefore,whether
developersnavigate filesinacode changeina non-linearfashion
during code review and how this is related to file position need
further investigation.
Inthepresenceoftheseargumentsintheliteraturecompeting
with the hypothesis that file location may influence code review
outcome, our study aims to investigate this topic in-depth from
complementary angles.
3 METHODOLOGY
Inthisstudy,weaimtounderstandwhethertherelativepositionin
whichafileisshownforreviewinfluencesthecodereviewprocess.
3.1 Research Questions
We structured our investigation in two steps, which seek to collect
complementary evidence. First, we investigate the relationship
betweenfilepositionandreviewers‚Äôactivity.Wedosobymining
datafromPRsbelongingtoavastsetofopensourceprojectsand
answer the following researchquestion:
RQ1.Towhatextentistherelativepositionofafileinareview
associatedwiththeamountofcommentsthefilereceives?
Second, we focus on the influence of file position on defect
findingincodereview.Wedosobyperforminganonlinecontrolled
experimentwithsoftwaredeveloperswhohavetoconductareview
ofcode andanswer the following question:
RQ2.What is the effect of the relative position of a defective
file inareviewonbugdetection?3.2 RQ 1- File PositionandReviewActivity
Subject Projects. For our analysis, we select 138 open-source
projectsfromGitHub,focusingonJavaprojectswithastar-count
above 1,000. As previously reported [ 16,17], the number of stars
of a project can be effectively used as an indication of the projects‚Äô
popularityandhealth.WefocusonJavaas(1)itisawidelypopular
programming language [ 57] and (2) focusing on one programming
language might reduce potential bias introduced by different re-
view practicesofprojectsbasedonotherprogramminglanguages.
Moreover, we investigate large projects to reduce potential bias
causedbyproject-specific reviewpolicies orcharacteristics.
Data Collection. Using PyGitHub1a Python library to access the
GitHub REST API, for each PR, we extract the position of each file
andthenumberofcommentsitreceives.Moreover,wecollectother
factorsthatcaninfluencethenumberofcommentsafilereceives
(i.e., confounding factors). We consider: (1) The number of lines
added and (2) deleted ina filebecause larger changesmay require
morecomments;(3)whetherthefileisatest,becausethesetendto
receive less comments [ 50] and to be ordered last alphabetically;
and(4)thenumberofcommenters,becausemoreparticipantsin
the reviewofaPR mightleadto more comments.
DataFiltering. Asthepresenceofabotmightintroducebiasin
our analysis, we exclude bot comments from our analysis. To do
so,weflagalluserscontainingtheword≈Çbot≈æasbotsandcreate
a list of commonly used bots in GitHub (extracted from relevant
grayliterature[ 56,60])andremovethem.Thepresenceofdiscus-
sion threads (i.e., a series of comments where the reviewers and
theauthordiscusssolutionsorimprovementstothecode)might
act as a confounding factor in our analysis: A discussion thread
canfacilitatedevelopers‚Äôengagementinaddingcommentstothe
thread regardless of the file‚Äôs position. Therefore, we focus our
investigationonlyonthefirstcommentofathread,disregarding
subsequentones.Moreover,thenumberoffilesinaPullRequest
mightinfluencethecomments‚Äôdistribution.Therefore,wegroup
thePRsaccordingtothenumberoffilestheycontaintoconduct
initialanalyses( e.g.,see Figure 4for PRs withfive files).
DataAnalysis. Toanalyzetheimpactoftheaforementionedfac-
tors on the number of commentsin a file (dependent variable), we
build aHurdle model [24]: A statistical model that specifies two
processes, one for zero counts and another for positive counts. We
choose this model as it handles excess zeros in the dependent vari-
ables, in fact, a vast number of files in the collected PRs receive
zerocomments.Tomodelthepositivecounts,weemploya negative
binomial distribution.Furthermore,wecheckthemulti-collinearity
acrossthevariablesinthemodelcomputingtheirVarianceInflation
Factor (VIF)andremovingthosewithaVIF higher thanfive.
3.3 RQ 2- File PositionandDefect Finding
Our controlled experiment is organized as an online experiment
in which participants are asked to complete a review of a code
changeinvolvingfivefiles.Amongthesefiles,weseedtwounre-
latedsoftwaredefects2intotwodifferentfiles.Werandomlyassign
participants to one of the following two treatments: (1) one file
1PyGitHub: https://github.com/PyGithub/PyGithub
2The participants arenot informed about the presence of these defects.
485ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Enrico Fregnan, LarissaBraz,Marco D‚ÄôAmbros,G√ºl √áalƒ±klƒ±, andAlberto Bacchelli
withabugispresentedfirstandtheotherfilewiththeotherbug
ispresentedlast( i.e.,fifth),or(2)theorderofthefiles(hencethe
bugs)isreversed( i.e.,thefifthfileisnowfirstandviceversa).We
measurethespecificbugsdetectedbyeachparticipant,aswellas
howlongeachfile isvisibleonthe participants‚Äô screen.
In the following, we provide more details on the experimental
objects(i.e., code change to review and seeded defects) and the
experimental design (i.e., online platform and experiment flow). We
conclude by describing thepilots we conducted, how we recruited
participants, and how we analyzed the collected experimental data.
3.3.1 Experimental Objects. Code change. For the review task,
wecreateacodechangethatsatisfiesthefollowingrequirements:
(1) not belonging to any existing project (to avoid introducing bias
caused by participants‚Äô previous knowledge of the review change-
set); (2) written in Java, one of the most popular programming
languages [ 57]; (3) self-contained (to minimize the previous knowl-
edgeparticipantsneedtounderstandthechangefully);and(4)close
to an actual code reviewscenario (toincreaserealism).
Wecreateacodechangespanningfivefilesasatrade-offbetween
small change-sets ( e.g., three files) and more complex reviews. For
smaller code changes ( e.g., two files),we expect a smaller effectof
a file‚Äôs position. In contrast, even thoughlarger code changes may
presentastrongereffect,morecomplexreviewsrequirealonger
reviewtime,thuslikelyleadingtomoreparticipantsabandoning
the experiment.
Allfileshavesimilarsizes(rangingfrom44to63lines).Inpartic-
ular,weensurethatthetwofilescontainingthedefectsaresimilarly
sized (46 lines in the Corner Case defect file and 44 lines in the
Missing Break defect) to minimize the potential bias introduced by
the file length.
We devise the code change to minimize links ( e.g., method calls)
between non-adjacentfiles. In particular, we checked that no con-
nectionexistedbetweenthetwodefectivefiles( i.e.,firstandlast).
Our goal is to increase the control on how participants navigate
the change,thus makingthe file position effectclearer,if any.
Finally,tore-positionthefilesforthetwodifferenttreatments,
werenamethem.Thisensuresthatthereviewersarenotinfluenced
by a tool behavior that is unexpectedly different than what they
are used to in common review platforms (where files are displayed
inalphabeticalorder).
Seeded Defects. In our experiment, we investigate if the given
treatmentinfluencesparticipants‚Äôabilitytofindbugs.Inthereview
task,we seedthe following twounrelatedfunctional defects:
-CornerCase (CC): Acornercaseconditioninan ifstatementis
leftunchecked,thusnotrespectingthedocumentation(Figure 1).
We select this bug because it represents an issue developers
typicallycheckfor[ 18,51]andfrequentlyexistsinpractice[ 37].
-Missing Break (MB): A missing breakstatement in a switch
constructmakestheexecutionincorrectlyfallthroughthede-
fault case (Figure 2). We select this defect because it is reported
as a common Java mistake by relevant gray literature [ 1,59],
alsoresonating withthe infamous Apple gotofail[15].
We use two defects to make the review task as realistic as possible
bypreventingparticipants from focusing exclusivelyonone bug.
Figure 1:CornerCase defect (CC) used inour experiment.
Figure 2: Missing Break defect (MB) used in our experiment.
3.3.2 Experimental Design. Experiment Platform. We imple-
mentourexperimentasanonlineplatform.Tothisaim,weemploy
a publiclyavailable tool, CRExperiment[ 49], alreadysuccessfully
used inpreviouscodereview experiments [ 18,51].CRExperiment
allowsparticipantstoreviewcodedisplayingareviewchange-setin
two-panediffs,asdoneinpopularcodereviewtools: e.g.,Gerrit[28]
or GitHub [ 29]. Moreover, the User Interface (UI) of CRExperiment
adopts similar design choices ( e.g., the color scheme) as the one of,
for instance, Gerrit. This mitigates possible bias with the users not
being familiar withthe experiment platform.
CRExperimentalsologsparticipants‚Äôinteractions( e.g.,scrolling)
and the time participants spend in each phase of the study ( e.g.,
in the review tasks). To collect information on which files in the
change-set the participants focus on during code review, we ex-
tendedthebaseexperimentplatformtorecordthefilesvisibleon
the participants‚Äô screen during the review. For each file, the experi-
menttoolrecordsifthiswasontheparticipants‚Äôscreen(displayed),
partially displayed, or not displayed at a given time. We store all
the collecteddata anonymously.
Experiment flow. We organize the flow of our experiment as
depictedinFigure 3andas describedinthe following:
(i) Welcome page. Ontheexperiment‚Äôslandingpage,weprovide
participants with information on the type of study and our data
486FirstCome FirstServed: TheImpact of FilePosition onCode Review ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
(v) Closing page
(ii) Code review taskRandom assignment to a treatment
{File 2, File 3, File 4}File 1 File 1 File 5 File 5
File 1 File 1 File 5 File 5{File 4, File 3, File 2}
(iii) Post-review 
    questionnaire!
(i) Welcome Page (iv) Demographics
Welcome!
File 1
Figure 3:Designandflow oftheonlineexperiment.
handling policy. Moreover, we ask for their consent to collect
anduse theirdata before proceeding inthe experiment.
(ii) Codereview task. Participants are asked to perform a code
review task.Before starting,we instruct participants to perform
a code review as they would typically do in their normal prac-
tice.Participantsarerandomlyassignedtooneoftwopossible
treatments :
‚Ä¢CCùëì-MBùëô- The file containing the Corner Case defect is
shownùëìirst in the code change, while the one containing
the MissingBreakdefectisshown ùëôast.
‚Ä¢MBùëì-CCùëô- The file containing the Missing Break defect is
shownùëìirst, while the one with the Corner Case defect is
shownùëôast.
Toavoidbias,wedonotinformparticipantsaboutthefunctional
defects,thetreatments,andtherecordedmetricsbeforethere-
view task. At the end of the task, we ask participants whether
andfor howlongthey were interruptedduringthe review.
(iii)Post-review questionnaire. After participants declare their
review task is completed, they are shown the code change again
with the location and explanation of the defects (Figure 1and
Figure2). We ask participants whether they found each of the
two defects and the cause that, in their opinion, influenced their
result.Inasubsequentpage(asnottoinfluencetheirprevious
answers),weaskparticipantsiftheythinkthepositionofthefile
containingthe bugsmighthave impactedtheirresults.
(iv) Demographics. Afterward,wecollectinformationaboutpar-
ticipants‚Äô gender, education, occupation, and experience with
Java and code review. We analyze this information (1) to guaran-
teethatthegroupsofparticipantsassignedtodifferenttreatments
arehomogeneous,(2)asconfoundingfactorsintheanalysisof
the experiment results, and (3) to describe the population repre-
sentedinour results.
(v) Closing page. Onthefinalpageoftheexperiment,wedisclose
thegoalofourstudyandaskparticipantsforanyfinalremark.
Wealsocollectparticipants‚Äôconsenttosharetheiranswersinan
anonymized,yetpubliclyavailable,researchdataset.
3.3.3 Pilot Runs. Before publicly releasing the online experiment,
weconductapilotstudyto(1)verifytheabsenceoftechnicalissues
withexperimentsettings( e.g.,theonlineplatform)andinstructions,
(2)checkthegoodnessofthedevisedreviewtaskandseededdefects,
and(3)improvetheexperimentbasedontheparticipants‚Äôfeedback.
Weconductfouriterationsofourpilotstudywithatotalamount
of 15 participants. After each iteration, the authors discuss theresultsand feedback obtained and improve theexperiment design
accordingly.Afterthelastiteration,theexperimentisdeemedready
tobe released sincethe participants detected no significant issues.
We excluded the data gathered from the participants in the pilot
study from the analysisof the experiment results.
3.3.4 DataAnalysis. Toanalyzethedatacollectedinourexperi-
ment,weperforma Chi-square testtoassessthecorrelationbetween
filespositionandparticipants‚Äôdetectionofthedefects.Weevalu-
atethestrengthofthecorrelationusingthe phi-coefficient .Then,
we buildtwo logistic regression modelsconsideringas dependent
variables (1) CCFound (whether the participants found the Corner
Case defect), and (2) MBFound (whether the participants found the
MissingBreakdefect),respectively.
Toidentifyifaparticipantfoundadefect,wefollowtheguide-
linesofpreviousstudies[ 13,31,32].Wemanuallyinspectthere-
marks left by the participants and classify a defect as found when
a remark (1) is placed in proximity of the defect and (2) clearly
describes the defect in the code. The first author performs this
inspection,andasecondauthorcheckslaterthefirstauthor‚Äôsin-
spection results.
Tobuildourmodels,wefollowthefollowingproceduretoensure
the goodness of our results: (1) We use the VARCLUS procedure
toidentifyand removevariablesfromthe modelwithSpearman‚Äôs
correlation higher than 0.5. (2) We compute the Variance Inflation
Factors(VIF)amongthevariablestoensurenonehadaVIFvalue
above 7. If a variable with a higher VIF is detected, we remove
it from the model. (3) Finally, we build the models adding each
variable one by one and checking that the coefficient remained
stable, showing littleinterference amongthe considered variables.
We aim to evaluate if the position of the defect (in the first or
last file)impacts participants‚Äô abilitytofindit. Forthis reason, we
includethevariable positionamongtheindependentvariablesinour
model.Wealsoconsiderthepotentialeffectofotherconfounding
factors, such as the participants‚Äô experience with Java and code
review.Furthermore,wecomputethetimeparticipantsspendon
the review task and include this variable in the model. Table 1
presentsallthevariablesincludedinourlogisticregressionmodels.
3.3.5 Number ofRequiredParticipants. We performed an a priori
poweranalysistocomputetheminimumsamplesizeneededforour
experiment.WeusedtheG-Powersoftware[ 23]andemployeda
two-tailtest (with a manual distribution) having ùëúùëëùëëùë† ùëüùëéùë°ùëñùëúùëõ =1.5,
ùõº=0.05,ùëÉùëúùë§ùëíùëü=0.8, andùëÖ2=0.3. The results of this analysis
showedourexperimentneededaminimumof92validparticipants.
487ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Enrico Fregnan, LarissaBraz,Marco D‚ÄôAmbros,G√ºl √áalƒ±klƒ±, andAlberto Bacchelli
Table 1:Variablesused inthelogisticregressionmodels.
Metric Description
Dependentvariables
CCFound The participant foundthe Corner Casedefect.
MBFound TheparticipantfoundtheMissingBreakdefect.
Independentvariables
Position The relative position of the bug (first or last) in
the code changeto review.
Control variables
CRDuration The time spentonthe code reviewtask.
DevExp Theparticipant‚Äôsdevelopmentexperienceina
professional setting.
JavaExp The participant‚Äôs experience withJava.
CRExp The participant‚Äôs experience incode review.
OftenProg Howoftentheparticipantprogrammedinthe
three months prior to the experiment.
OftenCR How often the participant reviewed code in the
three months prior to the experiment.
Interruptions Howoftenandforhowlongtheparticipantwas
interruptedduringthe reviewtask.
Figure 4:Distribution ofcomments inaPRwith five files.
3.3.6 Participants‚Äô Recruitment. We disseminated an invite to con-
ducttheonlineexperimentthroughtheauthors‚Äôprofessionalnet-
workandsocialmediaaccounts( e.g.,LinkedIn),aswellasonde-
velopers‚Äôwebforumsandcommunicationchannels( e.g.,Reddit).
To prevent any bias in the results, we do not disclose the experi-
ment‚Äôsaimtoparticipantsintheinvite.Toencouragedevelopers‚Äô
participation, we commit to donate 5 USD to a charity for each
valid participant inthe experiment.
4 RESULTS
We present the results ofour investigation byresearchquestion.
4.1 RQ 1- File PositionandReviewActivity
Our first research question seeks to investigate the relationship
betweenrelativefilepositionandreviewers‚Äôactivity,focusingonTable 2:Hurdlemodels forPRsummary.
Positive count Zerocount
Estim. Sig.Estim. Sig.
Intercept -8.982** -3.571***
Position -0.187*** -0.343***
Linesadded 0.012*** 0.007***
Linesdeleted 0.003*** -0.001***
isTest -0.433*** -0.707***
N.commenters 0.485*** 1.142***
Sig. codes: *** p <0.001; ** p <0.01;* p <0.05
thecommentsleftduringcodereview.Weanalyzeatotalof219,476
pull requests pertaining to 138 popular Java projects on GitHub.
Among these, 26,685 pull requests received at least one review
comment (whichwe computedexcluding comments left bybots).
Asthe vastmajority ofPRs(81.06%)containbetween oneand
ten files, we focused our investigation on this subset of PRs. We
excludedfromouranalysisPRscontainingonlyonefileastheeffect
ofthe position insuch casesisnot relevant.
CodereviewonGitHubisaniterativeprocess.Whenadeveloper
uploadsacommit,thisisreviewedbyfellowdeveloperswhomay
askthecommitauthortoperformsomechanges.Thesechangesare
addressed in a subsequent commit and the process continues itera-
tivelyuntilthe code isaccepted.Tobothconsider andexcludethe
effect of the review process itself, we analyzed the data concerning
twodifferentmoments inthe history of pull requests:
- Pull Request summary : We consider the code change as it
appearsattheend ofthe review processand allthe comments
that the code has receivedduringthe entire process.
- First commit : We consider only the code as initially submit-
tedinthepullrequestandthecommentsitreceives.Thus,we
excludechangesandcommentsinducedbythereviewprocess.
4.1.1 PullRequestSummary. Ourinitialinvestigationshowedthat
thenumberofcommentsonthefilesinaPRishigherinthefirst
filesandprogressivelydecreasestowardsthelastfiles.Forinstance,
Figure4showsthedistributionofthecommentsinPRswithfive
files.Asimilarpatternwasobservedalsoforalltheothergroups
ofPRs,e.g.,withsixfiles.Wefoundsimilarresultsalsowhenwe
restrict our investigation to the PRs that contain only Java files.3
To confirm and quantify the correlation between the relative
positionofafileinapullrequestandthenumberofcommentsitre-
ceives,weperformeda Spearmancorrelationtest (anon-parametric
test that measures the strength of the association between ordinal
orcontinuousvariables[ 54]).Weobtainedap-valueof <2.2¬∑10‚àí16
andaùúå=‚àí0.203,thusconfirmingastatisticallysignificantnegative
relation between thesetwovariables.
Subsequently,weperformedregressionmodeling toverifythis
association also when controlling for the other factors that may
influencethe number ofreviewcomments onafile.
Table2shows the Hurdlemodelpredicting the number of com-
ments in a file. We included in the model PRs containing only Java
filestoremovepotentialbiasintroducedbymultiplefileformats.
3Ouronlineappendix[ 26] provides graphs for allthe cases wemention.
488FirstCome FirstServed: TheImpact of FilePosition onCode Review ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Moreover, we excluded PRs having less than three files, since in
these cases the effect of the file position is likely to be negligible.
To remove outliers, we limited our analysis to files that have fewer
than1,000linesadded.Finally,wenormalizedeachfile‚Äôsposition
over thenumber of filesin the PR to allow for comparison among
PRswithadifferentnumberoffiles.Themodelissummarizedin
Table2andshowshowthepositionofafileisnegativelycorrelated
with the number of comments it receives, confirming how the last
filesinareviewchange-settendtoreceivelessreviewcomments
from developers, alsowhen controlling for otherfactors.
Table2alsoshowsthattheotherindependentvariablesincluded
inthemodelarestatisticallysignificantpredictorsofthenumber
of comments in a file. The sign of the estimates show that the
direction of the relationship follows the expected direction. For
instance, a larger number of changed lines (added or deleted) is
relatedtomorereviewers‚Äôcomments.Onthecontrary,theexpected
number of comments decreases when the file is a test (this result is
inlinewithpreviousresearch‚Äôsfindings[ 50]).Finally,thenumberof
commenters is positively correlated with the number of comments.
4.1.2 First Commit. By restricting our analysis to only the first
commitsofoursetofPullRequests,wenoticedasimilarpatternas
the one reported for the PR summary. The first files in a commit
receive more comments compared to the last ones. We analyzed
commits with number of files ranging from two to ten, noticing
similar patterns to the PR summary (available in the replication
package [ 26]).
AsforthePRsummary,wecorroboratedourfindingsbyrestrict-
ingouranalysistocommitscontainingonlyJavafilestomitigate
potential bias caused by multiple types of files. This verification,
however, did not reveal any significant difference from the gen-
eral case confirming our observations that a higher number of
comments concentrates onthe firstfilesinacommit.
We performed a Spearman correlation test to verify the presence
of a correlation between the number of review comments a file
receives and the file‚Äôs position. The test achieved a p-value of <
2.2¬∑10‚àí16and aùúå=‚àí0.152, showing how the two variables are
correlated(although this correlation isweaker).
TobuildtheHurdlemodelinthisscenario,wefollowedthesame
stepsastheonereportedforthePullrequestsummary.Ourresults
(reported in Table 3) confirm how later files in a review change-set
tendtoreceivealowernumberofcomments.Concerningtheother
independentvariablesincludedinourmodel,weachievedresults
similar to the ones presentedfor the PR summary (Section 4.1.1).
Finding1 .Thenumberofcommentsacrossthefilesinthe
analyzedpullrequestsisassociatedwiththerelativeposition
of each file. Specifically, the later a file is presented in a pull
request,thefewer thecommentsit receives.
4.2 RQ 2- File PositionandDefect Finding
EncouragedbythefindingsforRQ 1,wedevisedacontrolledexper-
iment to test this hypothesis with complementary evidence. Our
second research question seeks to investigate the effect of files‚Äô
position ondevelopers‚Äô revieweffectiveness.Table 3:Hurdlemodels forPR‚Äôs firstcommit.
Positive count Zerocount
Estim. Sig. Estim. Sig.
Intercept -10.34. -4.200***
Position -0.183** -0.221***
Linesadded 0.019*** 0.005***
Linesdeleted 0.004***-4.15¬∑10‚àí4*
isTest -0.353*** -0.667***
N.commenters 0.607*** 1.685***
Sig. codes: *** p <0.001; ** p <0.01;* p <0.05;.p <0.1
A total of 372 participants accessed the online experiment. Of
these,weconsideredonlytheparticipantswhocompletedallthe
steps.Furthermore,weremovedparticipantswholeftnoremarks
andspentlessthantenminutesdoingthereview,leavinguswith
106participants.
The vast majority of the participants ( 84.9%) have at least a
B.Sc. degree (mostly in Computer Science). Overall, 72 participants
reported to be software developers. Moreover, 84 participants self-
described as male, four as female, two as non-binary, and 16 pre-
ferred not to disclose. Figure 5reports participants‚Äô experience and
practicelevels.
Intotal,56participantswereintreatment CCùëì-MBùëô,while50
were in treatment MBùëì-CCùëô. We compared the experience and
practices( e.g.,Java experience)oftheparticipantsassignedtothe
twogroupsandfoundnostatisticallysignificant difference.
4.2.1 Defect Finding. Overall, 52 participants found the Corner
Case defect, while 50 detected the Missing Break defect. Table 4
reportsthenumberofparticipantswhofoundnodefect,onlyone
defect, or both defects in the review task, by treatment. Table 5
reportshowmanyparticipantsidentifiedeachtypeofbug,bytreat-
ment.
Table 4: Participants who found (1) no defects, (2) only the
Corner Case defect (CC), (3) only the Missing Break defect
(MB), and(4) bothdefects.
Nodefect CCMBCC & MB Total
CCùëì-MBùëô13 19915 56
MBùëì-CCùëô17 71511 50
Total 30 262426 106
These results show how participants found more the first bug
shown in the review task compared to the bug shown last. The
Corner Case defect was found 34 times when the file containing it
wasshownfirstandonly18whenthefilewasshownlast.Asimilar,
albeit weaker,trend seems to appear for the MissingBreakdefect.
To verify the presence of a relation between the position of
a bug and its detection, we performed a Chi-square test. In the
caseoftheCornerCasedefect,thistestachieved ùúí2(ùëëùëì=1,ùëÅ=
106)=6.45), p-value =0.011, rejecting the null hypothesis thatno
relationship exists between files‚Äô position and the detection of the
CornerCasedefect.However,inthecaseofMissingBreakdefect
our test achieved ùúí2(ùëëùëì=1,ùëÅ=106)=0.88), p-value = 0.346.
489ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Enrico Fregnan, LarissaBraz,Marco D‚ÄôAmbros,G√ºl √áalƒ±klƒ±, andAlberto Bacchelli
Figure 5:Participants‚Äôexperienceandpractice.
Table 5: Participants who found/not found each bug divided
by treatment: Corner Case defect (CC) first or Missing Break
defect(MB)first.
CornerCase Missing Break
Found Not found Found Not found
CCùëì-MBùëô34 22 24 32
MBùëì-CCùëô18 32 26 24
p-value:0.011 p-value:0.346
phi-coefficient: 0.247 phi-coefficient: 0.091
odds ratio:2.75 odds ratio:1.44
Therefore, for this defect we could not draw any conclusion on the
fact that its position influenceddevelopers‚Äô abilityto find it.
FortheCornerCasedefect,wealsocomputedthephi-coefficient
tomeasurethestrengthoftheassociationobtainingavalueof0.247,
whichisclose to amoderatepositive correlation [ 30].
To investigate the association between the detection of a defect,
its position in the code change, and other possible confounding
factors(reportedinTable 1),webuilttwologisticregressionmodels,
whoseresultsareshowninTable 6.Thesefindingsarealignedwith
the results ofthe Chi-square test: The position of the Corner Case
defect is statistically significant correlated with developers‚Äô ability
to find it(p-value <0.05).
Finding 2 . The relative position of the file containing the
CornerCasedefectinfluencesthelikelihoodofparticipants
in detecting the defect. Participants are less likely to find
the defect when its file is presented last. This effect is not
significant fortheMissing Breakdefect.Table 6:Logistic regressionmodels forRQ2.
Dep.Var.=CC Found Dep.Var.=MB Found
Estim. S.E.Sig.Estim. S.E.Sig.
Intercept ‚àí1.1131.570 ‚àí1.5731.536
Position ‚àí0.9600.409* 0.3880.418
CRDur. ‚àí310‚àí40.009 0.0050.010
DevExp ‚àí0.0240.225 0.0520.227
JavaExp 0.1200.189 0.2510.191
CRExp 0.0450.228 0.3260.236
OftenProg 0.1710.323 0.1300.312
OftenCR 0.0240.220 ‚àí0.5470.235*
Interrupt. 0.0050.156 ‚àí0.0560.158
Sig. codes: * p <0.05
At the end of the code review task we asked participants if they
thought the position of the defect had an influence on their ability
tofindit.Thevastmajorityofthemrepliednegatively( 72.6%for
the Corner Case defect and 76.4%for the Missing Break defect).
However, our results showed how, on the contrary, the position of
the bug had an effect on participants‚Äô ability to find it. This shows
a mismatch between what developers thinkaffects the code review
outcome andwhichfactors actuallyplay arole init.
4.2.2 Visible Files. In the experiment, we measured the time that
eachfilewasvisible ontheparticipants‚Äôscreenasaway toinves-
tigatefurthertheeffectoffiles‚Äôpositiononcodereview.Figure 6
reportsthetime(inseconds)thatparticipantsinthetwotreatments
spent with each file visible on the screen. We removed from this
analysis participants who declared to have been interrupted for
more than ten minutes during the review task, as their data may
biasthis analysis.
490FirstCome FirstServed: TheImpact of FilePosition onCode Review ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Figure6:Time(inseconds)participantsvisualizedeachfile.
Toimprove theclarity,we limited thesize oftheY axis.
Onaverage,participantsregardlessoftheirtreatment( CCùëì-MBùëôor
MBùëì-CCùëô)spentmoretimelookingatthefirstfilecomparedtothe
last one. On average, participants spent 8.58 minutes with the first
file displayedas opposedto 6.09for the last file.
Toverifywhether thedifferencebetweenthetimeparticipants
visualized the first and last file is statistically significant, we per-
formeda Mann≈õWhitneyUtest .Forthewholeparticipants(without
dividingthembythetreatment),ourtestobtainedap-valueof0.002,
thereforeshowingthatthetwovariablesareindeeddifferent.Di-
viding the participants by treatment, our test achieved a p-value of
0.022forthetreatment CCùëì-MBùëô,and0.031fortreatment MBùëì-CCùëô.
Theseresults confirmthosefor the generalpopulation.
Finding 3 . During the review task, participants spent signif-
icantlymoretimewiththefirstfilebeingvisiblecompared
to thelast file.
4.2.3 Robustness Testing. To challenge the validity of our findings
andstrengthentheirgoodness,weemployed robustnesstesting [43].
Participants‚Äô groups are not homogeneous. Participants‚Äô ex-
perience in programming and reviewing might impact their per-
formance during code review and,therefore, influencethe results.
To verify that the participants assigned to the two treatments have
similar characteristics, we performed a Chi-square test of homo-
geneityon the variables measuring participants‚Äô experience and
practice( e.g.,participants‚ÄôexperiencewithJava).Weobtainedp-
values above 0.1, therefore, not revealing significant differences
between the twosamples ofparticipants. Moreover,we compared
the timespent on the code reviewtask of participants belonging to
thetwo treatments.To this aim,we performeda Mann-Whitney U
test(p-value of 0.453), which did not detect any difference between
the twogroups.One defect might influence participants in finding the other.
Finding one defect might have biased participants towards finding
the other one. For instance, participants who found one bug might
have stopped looking for other defects assuming they found the
only issue in the code. Another possibility is that one defect can
give unintentional indications to participants to identify the other
one. However, the results reported in Table 4counter the existence
ofsuchbias:thenumberofparticipantswhofoundonlyonebug
is similar to the one of those who found both bugs. We further
performeda Chi-square testto verifyif findingone defectledpar-
ticipants to also find the second one. Our test obtained a p-value of
0.888, thus suggesting this biasnot to affectthe experiment.
The defects are too easy/difficult. The selected defects might
have been too easy, or on the contrary, too hard to identify for the
participants.Tomitigatethispossiblebiaswe(1)selecteddefects
amongcommonJavamistakesbasedonrelevantliterature[ 1,37,
52] and discussed them among the authors, and (2) verified our
choicethroughapilotstudy.Despitethesemeasures,participants
intheexperimentmighthavestillfaceddifficultiesfindingthese
defects(oridentifiedthemtooeasily).However,theresultsreported
in Table4show that the number of participants who found no
defects,onlyonedefect,orbothdefectsisalmostevenlydistributed,
showing that nodefectwastootrivial orhardto find.
Alownumberofparticipants. Anumberofparticipantstoolow
might bias the significance of our findings. The power analysis we
run (Section 3.3) showed that we needed at least 92 participants.
Our experiment exceeded this number: It was conducted by a total
of106valid participants.
5 THREATS TO VALIDITY
Construct validity. The codechanges selectedfor the reviewtask
might have introduced bias in the experiment. To mitigate this
threat, the first and second authors prepared the code changes
and selected the defects contained in them. The other authors also
checkedthe goodness ofthe code reviewtask.
The scenario represented in our experiment might differ from a
real-world review. To reduce this threat, we adopted the following
measures: we (1) created a code change as similar as possible to
real ones ( e.g., including documentation), (2) seeded defects that
commonly happen in real-world scenarios [ 1,51], and (3) used a
reviewinterfacesimilar to widely usedcode reviewtools.
Internal validity. During code review, developers, instead of writ-
ing multiple similar comments, might leave only one comment
requiring changes in all similar subsequent instances of an issue.
Suchcommentsmightintroducebiasintheresultsofourinvestiga-
tioninRQ 1.Forthisreason,oneoftheauthorsmanuallyinspected
a randomly selected subset of 100 PRs, looking for such comments.
OnlyeightPRscontainedinstancesofthesecomments.Consider-
ing this proportion, we computed a statistically significant sample
size with confidence level 95%and margin of error 5%, obtaining a
sample size of 54, which confirmed that 100 PRs are sufficient to
establishthe proportionof this factor inour results.
In RQ2, before analyzing the results, we manually inspected the
participants‚Äôlogs.Weremovedallparticipantsthatdidnottakethe
reviewtaskseriously:Participantswhospentlessthantenminutes
onthereviewandleftnoremarks.Sinceourexperimentreliedonan
491ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Enrico Fregnan, LarissaBraz,Marco D‚ÄôAmbros,G√ºl √áalƒ±klƒ±, andAlberto Bacchelli
online platform, the participants might have completed the review
taskwithdifferentset-upsandindifferentenvironments.However,
thisreflectsthevariousconditionsinwhichdevelopersworkinreal-
worldscenarios.Tomitigatethethreatsthatinterruptionsmight
constitute for the validity of our study, we asked participants to
state howlongthey were interruptedduringthe code reviewtask
andincludedtheiranswers inour statisticalanalyses.
External validity. In RQ1, the selection of the projects might
have introduced bias in our investigation. To reduce potential bias
caused by the choice of a specific project, we considered a vast
set of projects from different domains, all with star-count above
1,000.However,wecannotexcludethepossibilitythatthechoice
of different projects might lead todifferent results. Further studies
are neededto verifythe generalizabilityofour findings.
Participants in our experiment possess a vast set of different
backgrounds, yet our sample is not representative of all developers.
Moreover, the specific defects we chose might have influenced the
experimentresults.Althoughthesedefectshavebeenreportedin
the literature as common defects [ 1,37,51], they have specific
characteristics that donot generalize to allothertypes ofdefects.
Furthermore, the size of the code change contained in the re-
view task might have influenced the observed results. We carefully
chosethereview codechange size to be atrade-off betweenatoo
small review (where the effect of the files‚Äô positions is likely not to
matter)andatoo-largetask,whichwouldhaveresultedinahigher
participants drop-out rate. Although the results of RQ 1suggest
that this effect should be present also in the code change with a
highernumberoffiles,furtherstudiesareneededtocorroborate
our findingsindifferentscenarios.
Inourexperiment,weconsideredonlycodewritteninJava.This
wasdonetokeepthenecessarynumberofparticipants(Section 3.3)
within an achievable range. In fact, considering multiple program-
ming languages would have required to significantly extend the
numberofparticipantsduetothefollowingreasons.First,although
languages such asJava, C#,andPython canbe used to writecode
in a similar way, in reality they tend to follow different coding
conventionsandidioms[ 5,45,53].Therefore,usingmultiplelan-
guagesposestheissueofeithernotfollowingthelanguage‚Äôsidioms
(thus making the code snippet less ≈Çnatural≈æ) or having snippets
substantially different across languages. Second, past research has
provided evidence on different programmers‚Äô behavior when com-
prehendingcodewrittenindifferentlanguages[ 58].Third,workon
programming languages learning shows that novices face different
difficultylevelswithdifferentprogramminglanguages[ 41].This
might be caused by the different cognitive load that each language
poses.Forthesereasons,wecannotexcludethatdifferentprogram-
ming languages might lead to different results. Further studies can
bedevisedandconductedtoinvestigatewhetherandhowthere-
sults of our experiment change considering different programming
languages.
6 DISCUSSION
In this section, we discuss the main implicationsofour findings.
Theimportanceofbeingfirst. Ourinvestigationprovidesevi-
dence that the position of a file influences developers‚Äô review effec-
tiveness:Developershave64%loweroddstofindadefectwhenitisin the file shown last. Currently, code review tools ( e.g., GitHub or
Phabricator) display files in alphabetical order. Our results suggest
that more principled ways of presenting the files in a code review
can be usedto support reviewers‚Äô effectiveness.
For example, tools could display first those files that are more
critical.Previousworkfocusedonidentifyingsalientclassesina
codechange( i.e.,classesthatsubsequently causethemodification
of other classes in the change-set) [ 35]. Such classes can be used
as the point from which developers should start a revision. On the
contrary, another possibility would be to focus on identifying arid
filesandplacingthemlastinacodereview,takinginspirationfrom
the approachdeveloped at Googleto identify aridlines [44]. Such
an approach would allow developers to prioritize the review of
more relevantfiles.
Other studies proposed an ordering approach to re-organize
files in a code change to support reviewers‚Äô preferences [ 12,13].
Thisapproachfocusesonhowfilestoreviewshouldbegrouped,
suggestingtoshowfilessequentiallysharingalink( e.g.,method
call - method declaration). However, this ordering theory does
not consider the absolute position of the files ( i.e., which ones
shouldbeshownfirst).Theorderingtheorycanbeexpandedtotake
intoaccountourfindings.Moreover,previouswork[ 12]reported
howdevelopersadoptdifferenttacticswhenreviewingcode( e.g.,
startingfromnewlyaddedfilesorfilesperceivedaseasiertoreview).
Forthisreason,futurecodereviewtoolsmightincludeawaytolet
reviewers customize the order of the files to review to fit different
reviewers‚Äôneedsandcountertheeffectoftherelativepositionof
the files.
Finally,ifchangingthealphabeticalorderofthefilesinacode
reviewtoolistoounnaturalforreviewers,onecouldconsiderus-
ingwarningsto pointreviewerstowardsthefilefromwhichthey
should start the review. To this aim, new solutions could be de-
visedtakinginspirationfrompreviousstudiesonprogram-analysis
tools[34,39],wheretheauthorsinvestigatedhowwarningsshould
be displayed to be welcomed by developers. These warnings could,
forinstance,makedevelopersawareofbiasesduringcodereview
(e.g., as created by the relative file position). Future studies can
be designed andcarried outto comparethe effectiveness ofusing
warnings as opposed to changing the position of files in a code
reviewtool.
Not all defects are the same. In our experiment, participants‚Äô
effectiveness in detecting the Corner Case defect was influenced
by the relative position of the defective file. However, this was not
the casefor the MissingBreakdefect.
If we consider the nature of these bugs, we notice that they may
require a different effort to be found, even though they were found
withthesameprobabilityoverall(asreportedinTable 5).Onthe
one hand, the Corner Case defect requires the reviewer to read
andunderstandthedocumentation,thencomprehendthecodeand
identify a mismatch in the corresponding implementation. As one
participantexplainedwhenaskedwhytheydidnotfindtheCorner
Case defect: ≈ÇI didn‚Äôt read the JavaDoc carefully enough, and there
was not anything obviously wrong with the code.≈æ On the other
hand,theMissingBreakdefectcanberecognizedasapattern,even
if one does not fully understand the semantic of the code. One
participantwhofounditexplained:≈ÇIamaccustomedtolooking
492FirstCome FirstServed: TheImpact of FilePosition onCode Review ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
for break statements on every switch expression because I have
missedtheminmyowncode inthe past.≈æ
Therefore, the underlying reason for the difference in files‚Äô posi-
tioneffectcouldbethatthesetwobugsrequireadifferentcognitive
effort to be found. These bugs could impose a different load on
participants‚Äô working memory, making, in turn, the effect of the
defect position more or less prominent. We did not investigate this
further,butstudiescaninvestigatethishypothesisanddetermine
the role ofcognitive effortindefectfindingduringreview.
Thehiddenpsychologicalfactorsofcodereview. Attheendof
our experiment, we showed the participants the defects we seeded
in the code. When asked whether the position of the bugs might
havehadanimpactontheirabilitytodetectthem,avastamountof
the developers replied negatively. A total of 77 ( 72.6%) participants
rejected theideathat files‚Äô position influenced theirabilitytofind
for the Corner Case defect. However, the results of our experiment
foundevidenceofthecontrary.Theimpactoffilepositiononre-
viewers was also corroborated by the first part of our study when
associatingfile position andreviewactivity.
This mismatch between participants‚Äô perceptions and actual
results contributes to the discourse around the importance and
the impact of cognitive biases that may affect developers during
differentsoftwareengineeringactivities[ 42].Afterall,codereview
isahumaneffortandashumansweareinfluencedbybiasesand
≈Çhidden≈æ psychological factors of which we are not naturally aware.
Beyond code review. Our results provided initial evidence on the
effect of files‚Äô position on developers‚Äô activity and effectiveness in
code review. However, this phenomenon might also affect other
aspectsofsoftware engineering.
Previous studies investigated factors affecting developers‚Äô adop-
tion of program-analysis tools, showing that the way in which
warningsaredisplayediscritical[ 34,39].Amongthemanyfactors
identified to improve how warnings are displayed ( e.g., warning
should be well motivated [ 38]), our results may indicate that the
position of the warning can play a role. Similarly to files, the po-
sitionof thewarning mightaffect,forinstance,theattentionthat
developerspaytothem.Futurestudiescanevaluatewhetherthe
effectofwarnings‚Äôpositioninfluencesdevelopers.Thiseffectmight
beparticularlysignificant,forinstance,forthosecomplexwarnings
that require developers‚Äô effortto be understoodandsolved.
7 CONCLUSION
In this study, we investigated whether the relative position of a
file has an impact on code review. To do so, we devised a two-step
investigation to collect complementary evidence: We (1) collected
andanalyzeddatafrom219,476PullRequests(PRs)belongingto138
open-sourceJavaprojectsand(2)conductedanonlinecontrolled
experiment with106participants.
Inthefirststepofourinvestigation,wefocusedonreviewers‚Äô
activity,investigatingtherelationshipbetweentherelativeposition
of file in a PR and the number of comments it receives. Having
found evidence of a significant correlation between file position
andreviewactivity,wemovedtothesecondstepofourstudyto
collect different evidence. We devised an online experiment where
participantshadto performreviewacodechange inwhichwetwo
seeded defects (Corner Case and Missing Break). Participants wererandomly assigned to one of two possible treatments: CCùëì-MBùëô
(where the file with the Corner Case defect was shown first, while
theonewiththeMissingBreakdefectwasshownlast)and MBùëì-CCùëô
(where the order ofthe fileswasreversed).
Ourexperimentfoundthatthepositionofthefilecontainingthe
Corner Case defect influences the likelihood offinding this defect.
Specifically, 34 participants found the Corner Case defect when in
thefirstfile,whileonly18founditinthelast.Thiseffectwasnot
significantforthelesscognitivedemandingMissingBreakdefect.
Overall,ourstudyprovidesevidencethatthepositioninwhichfiles
arepresentedduringcodereviewhasanimpactoncodereview‚Äôs
outcome.Thisfindinghasimplicationsforcodereviewpractices
andreview tool design,andmaysuggestthat asimilareffect may
be present inothersoftware engineeringcontexts.
8 DATA AVAILABILITYSTATEMENT
The data collected in our investigation, the tools, and analysis
scriptsareavailableinourreplicationpackage[ 26].Thisversion
ofthereplicationpackagecontainsboththePullRequestdataset
and the remaining material in a unique archive. To ease the down-
load,thesematerialsarealsoavailableseparatelyatthefollowing
link[27].
ACKNOWLEDGMENTS
Theauthorswouldliketothanktheanonymousreviewersfortheir
thoughtful and important comments, which helped improving our
paper.FregnanandBacchelligratefullyacknowledgethesupportof
the Swiss National Science Foundation through the SNSF Projects
No. PP00P2_170529 and 200021_197227. D‚ÄôAmbros gratefully ac-
knowledges the financial support of the Swiss National Science
Foundationthroughthe NRP-77project187353.
REFERENCES
[1]2015.Buggy Java Code: The Top 10 Most Common Mistakes That Java Developers
Make. RetrievedFeb28,2022from https://www.toptal.com/java/top-10-most-
common-java-development-mistakes
[2]N. Abid, B. Sharif, N. Dragan, H. Alrasheed, and J. Maletic. 2019. Developer
ReadingBehavior WhileSummarizingJavaMethods: SizeandContextMatters.
InProceedings of the International Conference on Software Engineering . 384≈õ395.
[3]A.FrankAckerman,LynneS.Buchwald,andFrankH.Lewski.1989. Software
inspections:aneffectiveverificationprocess. IEEE Software 6,3 (1989), 31≈õ36.
[4]N. Al Madi, C. Peterson, B. Sharif, and J. Maletic. 2021. From Novice to Ex-
pert:AnalysisofTokenLevelEffectsinaLongitudinalEyeTrackingStudy.In
Proceedings of the International Conference on Program Comprehension . 172≈õ183.
[5]Carol VAlexandru, Jos√© J Merchante, Sebastiano Panichella,Sebastian Proksch,
HaraldCGall,andGregorioRobles.2018. Ontheusageofpythonicidioms.In
Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas,
NewParadigms,and ReflectionsonProgrammingand Software . 1≈õ11.
[6]AmericanPsychologicalAssociation.2022. APADictionaryofPsychology . Re-
trieved Feb 01,2022 from https://dictionary.apa.org/attention-decrement
[7]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In 201335th International Conferenceon Software
Engineering (ICSE) . IEEE,712≈õ721. https://doi.org/10.1109/ICSE.2013.6606617
[8]T.Baum. 2019. Cognitive-support codereview tools : improvedefficiency ofchange-
based code review by guiding and assisting reviewers . Ph.D. Dissertation. Univer-
sit√§t Hannover.
[9]T.Baum,H.Le√ümann,andK.Schneider.2017. Thechoiceofcodereviewprocess:
Asurveyonthestateofthepractice.In ProceedingsoftheInternationalConference
onProduct-FocusedSoftwareProcessImprovement . Springer, 111≈õ127.
[10]T.Baum,O.Liskin,K.Niklas,andK.Schneider.2016. AFacetedClassification
SchemeforChange-BasedIndustrialCodeReviewProcesses.In Proceedingsof
theInternationalConferenceonSoftwareQuality,ReliabilityandSecurity(QRS) .
74≈õ85.
493ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Enrico Fregnan, LarissaBraz,Marco D‚ÄôAmbros,G√ºl √áalƒ±klƒ±, andAlberto Bacchelli
[11]T.BaumandK.Schneider.2016. Ontheneedforanewgenerationofcodereview
tools.InProceedingsoftheInternationalConferenceonProduct-FocusedSoftware
ProcessImprovement . 301≈õ308.
[12]Tobias Baum, Kurt Schneider, and Alberto Bacchelli. 2017. On the optimal
order of reading source code changes for review. In 2017 IEEE international
conference on software maintenance and evolution (ICSME) . IEEE, 329≈õ340. https:
//doi.org/10.1109/ICSME.2017.28
[13]Tobias Baum, Kurt Schneider, and Alberto Bacchelli. 2019. Associating working
memory capacity and code change ordering with code review performance.
Empirical Software Engineering 24, 4 (2019), 1762≈õ1798. https://doi.org/10.1007/
s10664-018-9676-8
[14]G.BergersenandJ.Gustafsson.2011.Programmingskill,knowledge,andworking
memory among professional software developers from an investment theory
perspective. Journal ofindividual Differences (2011).
[15]M. Bland. 2014. Finding more than one worm in the apple. Commun. ACM 57, 7
(2014), 58≈õ64.
[16]K. Blincoe, J. Sheoran, S. Goggins, E. Petakovic, and D. Damian. 2016. Under-
standing the popular users: Following, affiliation influence and leadership on
GitHub.Informationand SoftwareTechnology 70(2016), 30≈õ39.
[17]H. Borges, A. Hora, and M. Valente. 2016. Understanding the factors that im-
pact the popularity of GitHub repositories. In Proceedings of the International
Conference onSoftwareMaintenance and Evolution . 334≈õ344.
[18]Larissa Braz, Enrico Fregnan, G√ºl √áalikli, and Alberto Bacchelli. 2021. Why
Don‚ÄôtDevelopersDetectImproperInputValidation? ‚Äô; DROP TABLE Papers;-- .
In2021IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE) .
IEEE,499≈õ511. https://doi.org/10.1109/ICSE43902.2021.00054
[19]T.Busjahn,R.Bednarik,A.Begel,M.Crosby,J.Paterson,C.Schulte,B.Sharif,and
S.Tamm. 2015. Eye MovementsinCodeReading:Relaxing the LinearOrder.In
Proceedings of the International Conference on Program Comprehension . 255≈õ265.
[20]S.Chattopadhyay,N.Nelson,A.Au,N.Morales,C.Sanchez,R.Pandita,andA.
Sarma. 2020. A Tale from the Trenches: Cognitive Biases and Software Devel-
opment. In Proceedings of theInternational Conference on Software Engineering .
654≈õ665.
[21]N.Cowan.2010. Themagicalmysteryfour:Howisworkingmemorycapacity
limited, and why? Current directions in psychological science 19, 1 (2010), 51≈õ57.
[22]M. Fagan.1976. Design and code inspectionsto reduceerrors in program devel-
opment.IBM SystemsJournal 15,3 (1976), 182≈õ211.
[23]F. Faul, E. Erdfelder, A. Lang, and A. Buchner. 2007. G* Power 3: A flexible
statistical power analysis program for the social, behavioral, and biomedical
sciences. Behaviorresearch methods 39,2 (2007), 175≈õ191.
[24]C. Feng. 2021. A comparison of zero-inflated and hurdle models for modeling
zero-inflated count data. Journal of Statistical Distributions and Applications 8, 1
(2021), 1≈õ19.
[25]Free Software Foundation. 2021. Comparing and Merging Files - Hunks.
https://www.gnu.org/software/diffutils/manual/html_node/Hunks.html.
[26]EnricoFregnan,LarissaBraz,MarcoD‚ÄôAmbros,G√ºl√áalikli,andAlbertoBacchelli.
2022. Artifacts Package - ≈ÇFirst come first served: the impact of file position
on code review≈æ. https://zenodo.org/record/6901285 .https://doi.org/10.5281/
zenodo.6901285
[27]EnricoFregnan,LarissaBraz,MarcoD‚ÄôAmbros,G√ºl√áalikli,andAlbertoBacchelli.
2022. ArtifactsPackage-≈ÇFirstcome first served:the impact of file position on
codereview≈æ. https://doi.org/10.6084/m9.figshare.20748235.v1 .
[28] Gerrit. 2021. Gerrit CodeReview. https://www.gerritcodereview.com.
[29] GitHub. 2021. GitHub. https://github.com.
[30]S.Glen.2016. PhiCoefficient(MeanSquareContingencyCoefficient) . RetrievedMar
11, 2022 from https://www.statisticshowto.com/phi-coefficient-mean-square-
contingency-coefficient
[31]Pavl√≠naWurzelGon√ßalves,EnricoFregnan,TobiasBaum,KurtSchneider,and
AlbertoBacchelli.2020. Doexplicitreviewstrategiesimprovecodereviewper-
formance?. In Proceedings of the 17th international conference on mining software
repositories . 606≈õ610. https://doi.org/10.1145/3379597.3387509
[32]Pavl√≠naWurzelGon√ßalves,EnricoFregnan,TobiasBaum,KurtSchneider,and
Alberto Bacchelli. 2022. Do explicit review strategies improve code review
performance? Towards understanding the role of cognitive load. Empirical
Software Engineering 27, 4 (2022), 1≈õ46. https://doi.org/10.1007/s10664-022-
10123-8
[33]C. Hendrick, A. F Costantini, J. McGarry, and K. McBride. 1973. Attention
decrement, temporal variation,and the primacy effectin impression formation.
Memory& cognition 1,2 (1973), 193≈õ195.
[34]A. Henley, K. Mu√ßlu, M. Christakis, S. Fleming, and C. Bird. 2018. Cfar: A tool to
increasecommunication,productivity,andreviewqualityincollaborativecode
reviews.In ProceedingsoftheConferenceonHumanFactorsinComputingSystems .
1≈õ13.
[35]Y.Huang,N.Jia,X.Chen,K.Hong,andZ.Zheng.2018. Salient-classlocation:
Help developers understand code change in code review. In Proceedings of the
JointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumonthe FoundationsofSoftwareEngineering . 770≈õ774.
[36]Y. Huang, K. Leach, Z. Sharafi, N. McKay, T. Santander, and W. Weimer. 2020.
BiasesandDifferencesinCodeReviewUsingMedicalImagingandEye-Tracking:
Genders,Humans,andMachines .AssociationforComputingMachinery,456≈õ468.
[37]B.JengandE.Weyuker.1994. Asimplifieddomain-testingstrategy. Transactions
onSoftwareEngineering and Methodology 3,3 (1994), 254≈õ270.
[38]B.Johnson,R.Pandita,J.Smith,D.Ford,S.Elder,E.Murphy-Hill,S.Heckman,
and C. Sadowski. 2016. A cross-tool communication study on program analysis
tool notifications. In Proceedings of the 2016 24th ACM SIGSOFT International
SymposiumonFoundationsofSoftwareEngineering . 73≈õ84.
[39]B.Johnson,Y.Song,E.Murphy-Hill,andR.Bowdidge.2013. Whydon‚Äôtsoftware
developersusestaticanalysistoolstofindbugs?.In ProceedingsoftheInternational
Conference onSoftwareEngineering . 672≈õ681.
[40]L.MacLeod,M.Greiler,M.Storey,C.Bird,andJ.Czerwonka.2017.Codereviewing
inthetrenches:Challengesandbestpractices. IEEESoftware 35,4(2017),34≈õ42.
[41]L.Mannila,M.Peltom√§ki,andT.Salakoski.2006. Whataboutasimplelanguage?
Analyzing the difficulties in learning to program. Computer science education 16,
3 (2006), 211≈õ227.
[42]R.Mohanani,I.Salman,B.Turhan,P.Rodr√≠guez,andP.Ralph.2018. Cognitive
biases in software engineering: a systematic mapping study. Transactions on
SoftwareEngineering 46,12(2018), 1318≈õ1339.
[43]E. Neumayer and T. Pl√ºmper. 2017. Robustness tests for quantitative research .
CambridgeUniversityPress.
[44]G.PetroviƒáandM.Ivankoviƒá.2018. Stateofmutationtestingatgoogle.In Proceed-
ings of the international conference on software engineering: Software engineering
inpractice . 163≈õ171.
[45]P.Phan-Udom,N.Wattanakul,T.Sakulniwat,C.Ragkhitwetsagul,T.Sunetnanta,
M. Choetkiertikul, and R. Kula. 2020. Teddy: automatic recommendation of
pythonic idiom usage for pull-based software projects. In Proceedings of the
InternationalConference onSoftwareMaintenance and Evolution . 806≈õ809.
[46]P.RigbyandC.Bird.2013. ConvergentContemporarySoftwarePeerReviewPrac-
tices. InProceedingsof the Joint Meetingon Foundationsof SoftwareEngineering .
202≈õ212.
[47]P. Rodeghero, C. Liu, P. McBurney, and C. McMillan. 2015. An Eye-Tracking
Study of Java Programmers and Application to Source Code Summarization.
Transactions onSoftwareEngineering 41,11(2015), 1038≈õ1054.
[48]Caitlin Sadowski, Emma S√∂derberg, Luke Church, Michal Sipko, and Alberto
Bacchelli.2018. Moderncodereview:AcasestudyatGoogle.In Proceedingsof
the 40th International Conference onSoftware Engineering:Software Engineering
inPractice . 181≈õ190. https://doi.org/10.1145/3183519.3183525
[49]DavideSpadini.2020.CRExperiment.https://github.com/ishepard/CRExperiment.
[50]Davide Spadini, Maur√≠cio Aniche, Margaret-Anne Storey, Magiel Bruntink, and
AlbertoBacchelli.2018. Whentestingmeetscodereview:Whyandhowdevel-
opersreviewtests.In 2018IEEE/ACM40thInternationalConferenceonSoftware
Engineering (ICSE) . IEEE,677≈õ687. https://doi.org/10.1145/3180155.3180192
[51]Davide Spadini, G√ºl √áalikli, and Alberto Bacchelli. 2020. Primers or reminders?
The effects of existing review comments on code review. In 2020 IEEE/ACM
42ndInternationalConferenceonSoftwareEngineering(ICSE) .IEEE,1171≈õ1182.
https://doi.org/10.1145/3377811.3380385
[52]DavideSpadini,FabioPalomba,TobiasBaum,StefanHanenberg,MagielBruntink,
and Alberto Bacchelli. 2019. Test-driven code review: an empirical study. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE,
1061≈õ1072. https://doi.org/10.1109/ICSE.2019.00110
[53]StackOverflow. 2014. What does Pythonic mean? Retrieved Jul 22, 2022 from
https://stackoverflow.com/questions/25011078/what-does-pythonic-mean
[54]Laerd Statistics. 2018. Spearman‚Äôs Rank-Order Correlation . Retrieved Mar 17,
2022from https://statistics.laerd.com/statistical-guides/spearmans-rank-order-
correlation-statistical-guide.php
[55]P.Thongtanunamand A. Hassan.2021. Review Dynamics and Their Impact on
Software Quality. Transactions on Software Engineering 47, 12 (2021), 2698≈õ2712.
https://doi.org/10.1109/TSE.2020.2964660
[56]T. Thurium. 2015. Beep Boop: 6 Bots To Better Your Open Source Project .
Retrieved Mar 8, 2022 from https://www.twilio.com/blog/6-bots-better-open-
source-project
[57]TIOBE. 2022. TIOBE-Index . Retrieved Feb 15, 2022 from https://www.tiobe.com/
tiobe-index/
[58]R. Turner, M. Falcone, B. Sharif, and A. Lazar. 2014. An eye-tracking study
assessingthecomprehensionofC++andPythonsourcecode.In Proceedingsof
the SymposiumonEyeTracking Research and Applications . 231≈õ234.
[59]Grand Canyon University. 2019. 5 Common Java Coding Mistakes to Avoid .
Retrieved Mar 15, 2022 from https://www.gcu.edu/blog/engineering-technology/
5-common-java-coding-mistakes-avoid
[60]M.Wessel.2021. AwesomeSEBots . RetrievedMar8,2022from https://github.
com/mairieli/awesome-se-bots
[61]O.Wilhelm,A.Hildebrandt,andK.Oberauer.2013. Whatisworkingmemory
capacity, and howcanwemeasureit? Frontiersinpsychology 4 (2013), 433.
494