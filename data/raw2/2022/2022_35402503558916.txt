WikiDoMiner: Wikipedia Domain-Specific Miner
Saad Ezzini
University ofLuxembourg
Luxembourg
saad.ezzini@uni.luSallam Abualhaija
University ofLuxembourg
Luxembourg
sallam.abualhaija@uni.luMehrdad Sabetzadeh
University ofOttawa
Canada
m.sabetzadeh@uottawa.ca
ABSTRACT
We introduce WikiDoMiner ≈õ a tool for automatically generat-
ing domain-specific corpora by crawling Wikipedia. WikiDoMiner
helps requirements engineers create an external knowledge re-
source that is specific to the underlying domain of a given require-
ments specification (RS). Being able to build such a resource is
important since domain-specific datasets are scarce. WikiDoMiner
generates a corpusby first extracting aset of domain-specific key-
words from a given RS, and then querying Wikipedia for these
keywords. The output of WikiDoMiner is a set of Wikipedia ar-
ticles relevant to the domain of the input RS. Mining Wikipedia
for domain-specific knowledge can be beneficial for multiple re-
quirements engineering tasks, e.g., ambiguity handling, require-
ments classification, and question answering. WikiDoMiner is pub-
licly available on Zenodo under an open-source license ( https:
//doi.org/10.5281/zenodo.6672682 )
CCSCONCEPTS
¬∑Software and its engineering ‚ÜíRequirements analysis ;¬∑
Computing methodologies ‚ÜíLanguage resources .
KEYWORDS
RequirementsEngineering,Natural-languageRequirements,Nat-
ural Language Processing, Domain-specific Corpus Generation,
Wikipedia
ACM ReferenceFormat:
SaadEzzini,SallamAbualhaija,andMehrdadSabetzadeh.2022.WikiDoMiner:
Wikipedia Domain-Specific Miner. In Proceedings of the 30th ACM Joint Eu-
ropean Software Engineering Conference and Symposium on the Foundations
ofSoftwareEngineering(ESEC/FSE‚Äô22),November14≈õ18,2022,Singapore,Sin-
gapore.ACM, New York, NY, USA, 5pages.https://doi.org/10.1145/3540250.
3558916
1 INTRODUCTION
Requirementsspecifications(RSs)varyconsiderablyacrossdomains
inlargepartduetothespecificterminologyassociatedwitheach
domain[1,7]. Several requirementsengineering(RE) taskscanbe
performed more accurately when scoped toa specific domain.For
example, Winkler and Vogelsang [ 18] propose an automated solu-
tionforclassifyingrequirementsandnon-requirementsfortheauto-
motivedomain.Ferrarietal.[ 8]investigatedefectsinrequirements
ESEC/FSE ‚Äô22,November 14≈õ18, 2022, Singapore, Singapore
¬©2022 Copyright held bytheowner/author(s).
ACM ISBN978-1-4503-9413-0/22/11.
https://doi.org/10.1145/3540250.3558916fortherailwaydomain.Ezzinietal[ 5]proposeadomain-specific
methodforhandlingambiguityinrequirements.AddressingREau-
tomationinadomain-specificmannerusuallynecessitatesdomain-
specificknowledgeresources.Suchresourcesarenonethelessoften
unavailable,since domain-specific datasets in RE are scarce.
IntherecentREliterature,thereisanincreasingrelianceonnatu-
rallanguageprocessing(NLP)technologiesforautomation,leading
to the rapidly emerging research area of NLP4RE [ 19]. Meanwhile,
NLP is shifting towards the application of large-scale language
models,e.g.,BERT[ 3],forsolvingdownstreamNLPtaskssuchas
question answering, natural language inference, and paraphras-
ing [17]. Language models are often pre-trained on large bodies of
generictext.Forinstance,theoriginalBERTmodelispre-trainedon
the entire (English) Wikipedia and the BookCorpus. This way, pre-
trainedlanguagemodelswouldlearnaboutwordco-occurrencesas
well as syntactic and semantic regularities in passages. Pre-trained
language modelscanthenbe fine-tunedfor solvingdownstream
tasks.Fine-tuningistheprocessofexposingapre-trainedmodel
toanotherdataset that is task-specific and/orin-domain [ 4].
Due to this evolutionary development in NLP, many NLP4RE
solutions ≈õ even some of the most recent ones ≈õ need to be re-
examined and revamped to fit the new technological trend. The
reason is not only to improve the accuracy of the existing NLP4RE
solutions,butalsotoavoidrelyingonNLPlibrariesthatwillsoonbe
outdated, in turn leadingtomaintenance headaches andupgrading
difficulties.Anotheressentialaspectthatislikelytoimpactthecur-
rentNLP4REliteratureisreusability.Thecurrentimplementation
tendency in view of the available large-scale language models is
towards Python-basedlibraries. Toenable better reusabilityof the
existingNLP4REsolutions,itisadvantageoustohaveamoreho-
mogeneous implementation in Python, even when similar libraries
are available in otherlanguages,e.g.,Java.
In this paper, we present WikiDoMiner (WikipediaDomain-
specificMiner). Given an RS as input, WikiDoMiner automatically
generatesadomain-specificcorpusfromWikepedia,withoutanya-
prioriassumptionsaboutthedomainoftheinputRS.WikiDoMiner
is a re-implementation of the corpus generator in an earlier re-
searchprototype,MAANA[ 5].MAANAisanautomatedambiguity
handlingtoolwhichusesfrequency-basedheuristicstodetectcoor-
dinationandprepositional-attachmentambiguity.Inthatcontext,a
largedomain-specificcorpusisneededforestimatingwordfrequen-
cies. In our ongoing research since MAANA, we have increasingly
neededdomain-specificcorpusgeneration,notforfrequency-based
statistics but rather for fine-tuning pre-trained language models.
This prompted us to build and release WikiDoMiner as a stand-
alonetoolandamorerobustandusablealternativetothecorpus
generatorinMAANA.MAANA‚ÄôscorpusgeneratorisJava-based.
Furthermore, it requires a local dump of Wikipedia installed as an
SQLdatabase.Thisconsumessignificantresourcesandmakesboth
This work is licensed under a Creative Commons Attribution-
NonCommercial 4.0 InternationalLicense.
1706
ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Ezzini, Abualhaija, andSabetzadeh
theinstallationand(re-)useofMAANAcomplex.WikiDoMinerlifts
this major limitation and further, by virtue of being Python-based,
ismucheasier to use alongside languagemodels.
Intherestofthispaper,weoutlinetheworkingsofWikiDoMiner
anddemonstratethe tool‚Äôs applicationintwodifferentdomains.
2 TOOL ARCHITECTURE
WikiDoMiner is a usable prototype tool for generating domain-
specificcorpora.Figure 1showsanoverviewofWikiDoMinerar-
chitecture. The tool is implemented in Python 3.7.13 [ 16] using
Google Colab1. Below, we discuss the different steps of the tool
markedA ≈õC inFigure 1.
2.1 PreprocessText
Inthefirststep,weparsethetextualcontentoftheinputRSand
preprocess the text. To do so, we apply an NLP pipeline composed
ofsixmodules,fourofwhicharerelatedtoparsingandnormaliz-
ingthetext,andtwoareforperformingsyntacticparsing.These
modules include: A tokenizer splits the text into different tokens
(e.g., commas and words), sentence spitter identifies the boundaries
of sentences in the running text (e.g., a sentence in English can
end with a period), lemmatizer findsthe canonicalformofa word
(e.g.,thesingularword≈Çcommunication≈æisthecanonicalformof
its plural variant ≈Çcommunications≈æ and the infinitive ≈Çtransmit≈æ
isthecanonicalformfor itspast-tense variant ≈Çtransmitted≈æ),and
finally, a stopwords removal module removes the stopwords such
as articles (≈Çthe≈æ) and prepositions (e.g., ≈Çin≈æ). To perform syntactic
analysis,wefurtherapply: POStagger thatassignsapart-of-speech
tag for each token (e.g., the tag VBD is assigned to ≈Çtransmitted≈æ
indicatingapast-tenseverb),anda syntacticparser thatidentifies
the syntactic units in the text (e.g., ≈Çthe notification service≈æ is a
noun phrase≈õNP).
TooperationalizetheNLPpipeline,weusetheTokenizer,Porter
StemmerandWordNetLemmatizeravailableinNLTK3.2.5[ 12].We
furtherapplyPythonRE2.2.1regexlibrary2,inadditiontoavailable
modulesinSpaCy3.3.0[ 10]includingtheEnglishstopwordslist,
Tokenizer, NP Chunker, Dependency Parser, and Entity Recognizer.
2.2 Extract Keywords
Inthisstep,weextractasetofkeywordsthatarerepresentativefor
the underlying domain. To do that, we adapt a glossary extraction
methodfromtheREliterature[ 2].Thebasicideainthisstepisto
collectthenounphrasesintheinputRS,andsortthemaccordingto
their frequency of use. To ensure that these keywords are domain-
specific,WikiDoMinerappliestwomeasures.First,weremovefrom
the list any keyword that is available in WordNet [ 6,14], which
isagenericlexicaldatabaseforEnglish.Theintuitionofthisstep
is to remove very common words that are not representative of
the underlying domain. For instance, the word ≈Çrover≈æ exists in
WordNet3as a noun referring to ≈Çsomeone who leads a wandering
unsettled life≈æ or≈Çan adultmemberofthe Boy Scouts movement≈æ.
These two meanings do not fit the ≈Çrover≈æ in the ≈Çlunar rover≈æ
1https://colab.research.google.com/?utm_source=scs-index
2https://docs.python.org/3/library/re.html
3http://wordnetweb.princeton.edu/perl/webwn?s=rover&sub=Search+WordNet&
o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=context, and the NP ≈Çlunar rover≈æ. This way, we filter out the word
≈Çrover≈æwhenitoccursalone(i.e.,≈Çtherover≈æ),andkeepitaspartof
theNP(≈Çlunarrover≈æ).WenotethatthetheNP≈Çlunarrover≈æisnot
available inWordNet, but isinWikipedia4.
Asasecondmeasure,WikiDoMinercomputestermfrequency/in-
verse document frequency (TF/IDF) [ 13] instead of mere frequency.
TF/IDFisatraditionalmethodthatisoftenappliedinthecontext
of information retrieval (IR) to assign a score reflecting the impor-
tance of words to a specific document in a document collection.
In WikiDoMiner, the importance of the words (NPs in our case)
indicates that the words are significant for the underlying domain.
WenotethatIDFiscomputedonlywhentherearemultipledocu-
ments from other domains available. Otherwise the TF/IDF scores
are similar to term frequencies. Once the TF/IDF scores are com-
puted, we sort the keywords in descending order of these scores
and select the top- ùêækeywords. While the default value applied by
WikiDoMiner is ùêæ=50,we showin the demoof the toolthatthis
parameter can be configured by the user according to the intended
application.
WeimplementthedifferentmodulesusingWordNetfromNLTK
3.2.5[12], andTF-IDF transformation from Scikit-learn1.0.2 [ 15].
2.3 QueryWikipedia
In this step, we use the keywords from the previous set to query
Wikipedia andcollecttherelevantarticleswhich willthenconsti-
tuteour final domain-specific corpus.
To better understand this step, we first explain the structure
of a category in Wikipedia, illustrated in Figure 2. Each article
in Wikipedia belongs to one or more categories. Each category
containsasetofarticlesandsub-categories.Toillustrate,assume
that we are querying Wikipedia for the keyword ≈Çrail transport≈æ
within the ≈ÇRailway≈æ domain. Our first hit will be a page titled
≈ÇRail Transport≈æ5. Note that we refer to a page in Wikipedia as
anarticle. If we view the category structure for this article6, we
find out that it belongs to a category under the same name ≈ÇRail
Transport≈æ,i.e.,Category AinFigure 2.Inside this category,there
are31sub-categoriessuchas≈ÇLocomotives≈æ,≈ÇRailInfrastructure≈æ,
and ≈ÇElectric rail transport≈æ. Category A contains22 other pages
alongside the above mentioned pages, such as ≈ÇBi-directional vehi-
cle≈æand≈ÇPocket wagon≈æ.Viewing thestructure ofasub-category,
e.g., ≈ÇRail Infrastructure≈æ will show us again the available pages
andsub-categories.
In WikiDoMiner, the result of querying Wikipedia for a given
keyword is a Wikipedia article whose title partially matches the
keyword.WeconsiderthetitleofaWikipediaarticleaspartially
matchingthekeywordiftheyhavesomeoverlap.Forexample,if
we query Wikipedia for the keyword ≈ÇEfficiency of rail transport≈æ,
then we will retrieve the same article mentioned above whose title,
≈ÇRail Transport≈æ,partiallymatches the keyword.
Foreachkeyword,weretrievefromWikipediaamatchingarticle
if applicable. Some applications might require that the domain-
specific corpus be sufficiently large. For example, to accurately
estimate the frequencies of words co-occurrences, one needs a
4https://en.wikipedia.org/wiki/Lunar_rover
5https://en.wikipedia.org/wiki/Rail_transport
6https://en.wikipedia.org/wiki/Category:Rail_transport
1707WikiDoMiner: WikipediaDomain-SpecificMiner ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
Requirements
SpeciÔ¨Åcation
 WikipediaA C B
txt
Wikipedia 
ArticlesPreprocess Text Extract Keywords Query Wikipedia
Tokenizer
Sentence 
Splitter
POS 
Tagger
Lemma-
tizer
Syntactic 
Parser
NP 
Extractor
Frequency 
Computer
Articles 
Retriever
Corpus 
Expander
Preprocessed 
TextTop-K Domain-
speciÔ¨Åc Keywords
Stopwords 
Removal
Figure 1:ToolArchitecture.
rail transport Keyword
2Keyword
K...
...
Categories...
Articles ...
Sub- Categories
Articles ...RSdepth 0
depth 1
depth 2Articles
Extract
Keywords A
22
31
860
Figure 2: Illustration of Traversing Wikipedia Categories
(ExampleKeyword: ≈Çrailtransport≈æ).
largecorpus[ 11].Similarly,topre-trainadomain-specificlanguage
model, a large text body should be available. Therefore, we expand
ourcorpusbydefiningaconfigurableparameter depthtocontrol
the level of expansion, thus allowing the user to adjust the size
and relevance of the corpus based on their needs. The minimal
depthdepth=0can be used to extract directly matching articles
only (leading most often to a few hundred articles). WikiDoMiner
further retrieves, for each matching article, all articles in the same
categoriesfor depth=1(e.g.,thetwootherpagesintheexample
above), subcategories of depth=2, sub-subcategories of depth=3,
andsoon.
Specific details of our implementation are as follows. We use
the Wikipedia 1.4.07and Wikipedia-API 0.5.48libraries to query
Wikipedia.Otherlibrarieswhichweusebutwhicharenotneces-
sary to run the tool include PyPDF2 2.2.09to read requirements
documentsinPDFformat,theword2vecsimilarityfeatureinSpaCy
3.3.0library[ 10],andtheWordCloud1.5.010librarytovisualizethe
mostprevalentwordsinthe extractedcorpora.
7https://wikipedia.readthedocs.io/
8https://wikipedia-api.readthedocs.io/
9https://pypdf2.readthedocs.io/
10https://amueller.github.io/word_cloud/3 APPLICATION
Inthissubsubsection,weapplyWikiDoMinertoautomaticallygen-
erate domain-specific corpora for two distinct domains, namely,
railway andtransportation. We further assess how representative
the corpus generated for each of these domains is. We do so by
computingthesemanticrelatednessofeachdomain-specificcor-
pusagainstRSsfromthesamedomainotherthanthoseusedfor
generatingthe corpus. Generating adomain-specific corpus is not
a frequent activity. In practice, requirements engineers would typ-
ically have a small set of RSs from a given domain at the time of
generating a domain-specific corpus and would utilize this corpus
toperformactivitiesonotherRSsnotinvolvedinthegeneration
process.
3.1 Data Collection
Forthetwodomainsconsideredinthissection,wecollectedatotal
of six RSs from the PURE dataset [ 9], with three RSs from each
domain.OneRSisusedforgeneratingthecorpusandtheothers
are used for evaluatingsemanticrelatedness against theresulting
corpus.
In the following we listthe six RSs:
‚Ä¢From the railway domain, we used RS1 ( ERTMS) abouttrain
control, RS2 (EIRENESYS15 )andRS3 ( EIRENEFUN 7 )both
aboutdigital radio standard for railway .
‚Ä¢From the transportation domain, we used RS4 ( CTC NET-
WORK) abouttraffic managementinfrastructure , RS5 (PON-
TIS)abouthighwaybridgeinformationmanagement ,andRS6
(MDOT) abouttransportation info management .
3.2 Domain-specific Corpora
For illustration, we centre our discussion around the railway do-
main.WegeneratethecorpusfromRS1,andevaluatetherelated-
nessonRS2andRS3.ThefirststepinWikiDoMineristopreprocess
RS1.WikiDoMinerthenextractsasetofkeywordsbasedontheir
TF/IDFscores.Examplesofsuchkeywordsinclude trainborneequip-
mentandemergencybrake .Weselectthetop- ùêækeywords,where
ùêæ=50.
ThenextstepistoquerythekeywordsonWikipedia.Forourset
ofkeywordsinthisdomain,weretrieve15matchingarticles.We
then set the configuration parameter depthto 1. Following this, we
collectforeacharticlethatmatchesakeywordthearticlesinthe
respective categories (see Figure 2). Finally, we collected a total of
686articles ,whichareconsideredasourdomain-specificcorpus.
WeapplyWikiDoMineronRS4(fromthetransportationdomain)
in a similar manner. The two resulting corpora are depicted in
1708ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore Ezzini, Abualhaija, andSabetzadeh
Figure 3: Word-cloud Visualization of Domain-specific Corpora (Left-hand Side ≈õ Railway Domain, and Right-hand Side ≈õ
Transportation Domain).
Figure3as word clouds. We show for each domain the main terms
that frequently occur in the corpus. We see that the keywords rail,
track,train,railway,andrailroadcharacterizetherailwaycorpus,
whilethetransportationcorpusischaracterizedbythekeywords
traffic,road,street,andlane.Wenotethatthe railwaydomaincan
be regarded as a sub-domain of the transportation domain. This
observation is highlighted through the frequent terms that the two
corporahaveincommoninFigure 3,suchassignal,system,vehicle,
anddriver.
3.3 Domain Representativeness
To examinehow representative the resulting domain-specificcor-
pora are, we compute semantic relatedness as follows. We first
transform each article in the corpus into a vector representation
usingword2vec.WedothesameforthetestRS.Then,wecompute
the cosine similaritybetween the vector representingthe (test) RS
andthevectorrepresentingeacharticle.Inthefollowing,wereport
theminimum,average,andmaximumcosinesimilarityscoresfor
eachdomain:
‚Ä¢Railway domain (cosine similarity between the railway cor-
pusandtest RSs): min=0.27, average=0.94 ,andmax=0.98
‚Ä¢Transportation domain (cosine similarity between the trans-
portationcorpusandtestRSs):min=0.67, average=0.95 ,and
max=0.99.
Our results show that the domain-specific corpora are, on aver-
age, highly similar to the test (unseen) RSs not used for generating
thecorpora.Inparticular,theaveragesemanticsimilarityis ‚â•0.94,
indicating thatmanyarticlesinthecorpusare relevanttothetest
RSs.Theminimumscoreof0.27intherailwaydomainimpliesthat
there are articlesinthecorpuswhich aremore document-specific,
i.e.,morerelevanttotheRSthatinducedthecorpusbuthavinglittle
incommonwiththetestRSs.Notethat,despitesomedocument-
specificarticlesbeingpresentinthegeneratedcorpus,theveryhigh
average semantic similarity ( ‚â•0.94) indicates that such articles are
asmallminorityandthusdonothaveasignificantnegativeimpact
onthe in-domainusabilityofthe generatedcorpus.
The gap seen between the minimum scores reported for the
two domain-specific corpora can be explained by the following: As
mentionedinSection 3.1,allRSsfromthetransportationdomainin our collection are on the topic of traffic and transportation in-
formationmanagement.Thisleadstoextractingmanykeywords
related to information management. In contrast, the RSs in our
collection from the railway domain are tailored to more specific
topics, namely train control and digital radio standard for railway.
This in turn leads to extracting document-specific terms which are
related to train control (i.e., the topic of the RS used for corpus
generation)butnotsomuchtodigitalradiostandardforrailway
(i.e., the topic of the test RSs). To summarize, our experiments
show that WikiDoMiner has successfully generated representative
corporafor twodistinct domains.
4 CONCLUSION
We presented WikiDoMiner, a tool for automatically generating
domain-specific corpora from Wikipedia. Our current implementa-
tion is a significantly enhanced and usable adaptation of the cor-
pus generation component briefly outlined in our earlier work [ 5].
WikiDoMinerextracts keywordsfrom agiven requirementsspec-
ification(RS)andthenqueriesthesekeywordsinWikipedia.For
each keyword, WikiDoMiner looks for a matching article whose
titlehassomeoverlapwiththekeyword.Toexpandthecorpus,we
providetheuserwiththepossibilitytoconfigureaparameter depth
thatcontrols how deeplythe Wikipedia categorystructure should
be traversed. We assess the relatedness of the resulting corpora to
RSs different from those used for corpus generation. Our empirical
resultsshowthat,acrosstwodistinctdomains,WikiDoMineryields
an averagesemantic relatedness of ‚â•0.94for in-domainanalysis.
Inthefuture,weplantoutilizeWikiDoMinerforaddressingnew
analyticalproblemsbeyondambiguityanalysis.Notabletargetprob-
lems include question answering andrequirements classification.
ACKNOWLEDGMENTS
ThisworkwasfundedbyLuxembourg‚ÄôsNationalResearchFund
(FNR) under the grant BRIDGES18/IS/12632261 and NSERC of
Canada under the Discovery and Discovery Accelerator programs.
WearegratefultotheresearchanddevelopmentteamatQRACorp.
for valuable insightsandassistance.
1709WikiDoMiner: WikipediaDomain-SpecificMiner ESEC/FSE ‚Äô22, November14≈õ18, 2022,Singapore, Singapore
REFERENCES
[1]SallamAbualhaija,ChetanArora,MehrdadSabetzadeh,LionelBriand,andEd-
uardo Vaz. 2019. A Machine Learning-Based Approach for Demarcating Re-
quirementsinTextualSpecifications.In Proceedingsofthe27thIEEEInternational
Requirements Engineering Conference (RE‚Äô19) .
[2]Chetan Arora, Mehrdad Sabetzadeh, Lionel Briand, and Frank Zimmer. 2017.
Automated Extraction and Clustering of Requirements Glossary Terms. IEEE
Transactions onSoftwareEngineering 43,10(2017).
[3]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018. BERT:
Pre-training of deep bidirectional transformers for language understanding.
(2018). arXiv: arXiv:1810.04805
[4]SaadEzzini,SallamAbualhaija,ChetanArora,andMehrdadSabetzadeh.2022.
AutomatedHandlingofAnaphoricAmbiguity:Amulti-solutionStudy.In 2022
IEEE/ACM44thInternationalConference onSoftwareEngineering .
[5]SaadEzzini,SallamAbualhaija,ChetanArora,MehrdadSabetzadeh,andLionelC
Briand.2021. Usingdomain-specificcorporaforimprovedhandlingofambiguity
in requirements. In 2021 IEEE/ACM 43rd International Conference on Software
Engineering .
[6]ChristianeFellbaum.1998. WordNet:AnElectronicLexicalDatabase (1sted.). The
MITPress.
[7]AlessioFerrari andAndreaEsuli.2019. AnNLPapproachfor cross-domainam-
biguity detectionin requirementsengineering. Automated Software Engineering
26,3 (2019).
[8]AlessioFerrari,GloriaGori,BenedettaRosadini,IacopoTrotta,StefanoBacherini,
AlessandroFantechi,andStefania Gnesi.2018. Detectingrequirementsdefects
withNLPpatterns:Anindustrialexperienceintherailwaydomain. Empirical
SoftwareEngineering 23,6 (2018).
[9]Alessio Ferrari, Giorgio Oronzo Spagnolo, and Stefania Gnesi. 2017. Pure: A
dataset of public requirements documents. In 2017 IEEE 25th International Re-
quirements Engineering Conference .[10]Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd.
2020.spaCy:Industrial-strengthNaturalLanguageProcessinginPython .https:
//doi.org/10.5281/zenodo.1212303
[11]DanJurafskyandJamesH.Martin.2020. SpeechandLanguageProcessing (3rd
ed.).https://web.stanford.edu/~jurafsky/slp3/ (visited 2021-06-04).
[12]Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit.
InProceedingsoftheACL-02WorkshoponEffectiveToolsandMethodologiesfor
TeachingNatural Language Processingand ComputationalLinguistics .
[13]M. McGill and G. Salton. 1983. Introduction to Modern Information Retrieval .
McGraw-Hill.
[14]GeorgeMiller.1995. WordNet:AlexicaldatabaseforEnglish. Commun.ACM 38,
11(1995).
[15]Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel,
BertrandThirion,OlivierGrisel,MathieuBlondel,PeterPrettenhofer,RonWeiss,
Vincent Dubourg, et al .2011. Scikit-learn: Machine Learning in Python. Journal
ofMachineLearning Research 12(2011), 2825≈õ2830.
[16]GuidoVanRossumandFredL.Drake.2009. Python3ReferenceManual . CreateS-
pace.
[17]Alex Wang, AmanpreetSingh,Julian Michael, Felix Hill,Omer Levy, andSamuel
Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for
NaturalLanguageUnderstanding.In Proceedingsofthe2018EMNLPWorkshop
BlackboxNLP:Analyzingand InterpretingNeural Networks for NLP . 353≈õ355.
[18]Jonas Winkler and Andreas Vogelsang. 2018. Using Tools to Assist Identification
of Non-requirementsin Requirements Specifications≈õA Controlled Experiment.
InProceedings of the 24th International Working Conference on Requirements
Engineering: Foundationfor SoftwareQuality (REFSQ‚Äô18) .
[19]Liping Zhao, Waad Alhoshan, Alessio Ferrari, Keletso J Letsholo, Muideen A
Ajagbe,Erol-ValeriuChioasca,andRizaTBatista-Navarro.2021.Naturallanguage
processing for requirements engineering: a systematic mapping study. ACM
ComputingSurveys(CSUR) 54,3 (2021), 1≈õ41.
1710