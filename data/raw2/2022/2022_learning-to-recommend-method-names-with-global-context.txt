Learning to Recommend Method Names with Global Context
Fang Liu
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
liufang816@pku.edu.cnGe Liâˆ—
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
lige@pku.edu.cnZhiyi Fu
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
ypfzy@pku.edu.cn
Shuai Lu
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
lushuai96@pku.edu.cnYiyang Hao
Silicon Heart Tech Co., Ltd
Beijing, China
haoyiyang@nnthink.comZhi Jinâˆ—
Key Lab of High Confidence Software
Technology, MoE (Peking University)
Beijing, China
zhijin@pku.edu.cn
ABSTRACT
In programming, the names for the program entities, especially for
the methods, are the intuitive characteristic for understanding the
functionalityofthecode.Toensurethereadabilityandmaintain-
abilityoftheprograms,methodnamesshouldbenamedproperly.
Specifically,thenamesshouldbemeaningfulandconsistentwith
other names used in related contexts in their codebase. In recent
years, many automated approaches are proposed to suggest consis-
tent names for methods, among which neural machine translation
(NMT)basedmodelsarewidelyusedandhaveachievedstate-of-
the-artresults.Ho wever,theseNMT-basedmodelsmainlyfocuson
extracting the code-specific features from the method body or the
surroundingmethods,theproject-specificcontextanddocumen-
tation of the target method are ignored. We conduct a statistical
analysistoexploretherelationshipbetweenthemethodnamesandtheircontexts.Basedonthestatisticalresults,weproposeGTNM,a
Global Transformer-based Neural Model for method name sugges-
tion,whichconsidersthelocalcontext,theproject-specificcontext,
and the documentation ofthe method simultaneously. Experimen-
talresultsonjavamethodsshowthatourmodelcanoutperform
the state-of-the-art results by a large margin on method name sug-
gestion, demonstrating the effectiveness of our proposed model.
CCS CONCEPTS
â€¢Softwareand its engineering;â€¢Computing methodologies
â†’Artificial intelligence;
KEYWORDS
method name recommendation, global context, deep learning
âˆ—Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9221-1/22/05...$15.00
https://doi.org/10.1145/3510003.3510154ACM Reference Format:
FangLiu,GeLi,ZhiyiFu,ShuaiLu,YiyangHao,andZhiJin.2022.Learn-
ing to Recommend Method Names with Global Context. In 44th Interna-
tional Conference on Software Engineering (ICSE â€™22), May 21â€“29, 2022, Pitts-
burgh,PA,USA. ACM,NewYork,NY,USA,13pages.https://doi.org/10.1145/
3510003.3510154
1 INTRODUCTION
During programming, developers must name variables, functions,
parameters, etc. The appropriateness of a name changes over time
during the software evolution. For example, a good function name
can degrade into a poor one when the semantics of the function
change or the function is used in a new context. Poor names make
programs harder to understand and maintain [ 9,10,17,22,25,39],
leadingtomisusesanddefects[ 1,2,8,13].Findingconsistentnames
forprogramconstructshasalwaysbeenacynosureinthesoftware
industry.
Methods are the most minor named units for indicating the pro-
gram behavior in most programming languages [ 18], thus they are
particularly important [ 12,30,32]. Meaningful and conventional
methodnamesarevitalfordeveloperstounderstandthebehaviorof
programsorAPIs.Oncethenameofamethodisdecided,itislabori-oustochange,especiallywhenusedforanAPI[
4].Theresultsfrom
aninvestigationinLiuetal . [28]indicatethatamongthechange
historyinprojects,developersusuallychangethemethodnames
withoutanychangetothecorrespondingbodycodeinmanycases,
which suggests that programmers strive to choose meaningful and
appropriate method names, i.e., more consistent with other names
in the same project or the codebase. Especially when collaborating,
they need to obey a projectâ€™s coding conventions.
Inrecentyears,researchershaveproposedautomatedapproaches
forsuggestingconsistentnamesforthosemethods.Basedonthe
intuition that two methods implemented with similar code in their
body code are likely to be named similarly, Liu et al . [28]proposed
an IR-based approach to detect and rename inconsistent method
names.Theyidentifytheinconsistentmethodnamesbycomparingthenamesretrievedfromthemethodbodyvectorspacewiththose
retrieved from the method name vector space. For the inconsistent
names, their model recommends the potentiallyconsistent names
by referring to the names of similarly implemented methods. How-
ever,inmanycases,eventhemethodswithsimilarbodycodecanbe
named differently because they might belong to different projects
12942022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Liu, et al.
and have different semantics. Besides, by retrieving names from
similar methods, the model cannot suggest neologisms. Allamanis
etal. [5]proposedaconvolutionalattentionalnetworktoextract
local time-invariant and long-range topical attention features in
the method body to suggest names for methods. To leverage the
syntacticstructureofprogramminglanguages,Code2vec[ 7]and
Code2seq [ 6] represent the method body as a set of compositional
abstract syntax tree (AST) paths and use the path representation to
predictthemethodâ€™sname.Nguyenetal . [34]proposedMNire,a
simplebuteffectiveapproachtorecommendamethodnameand
detectmethodnameinconsistencies.Theytreatedthemethodname
generationtaskasanabstractivesummarizationofthetokensof
the program entitiesâ€™ names in the method body and the enclosing
class name. Li et al . [24]developed DeepName, a context-based
approachformethodnameconsistencycheckingandsuggestion.
Theyextractthefeaturesfromfourcontexts:theinternalcontext,
thecallerandcalleecontexts,siblingcontext,andenclosingcontext.
The above state-of-the-art models mainly focus on exploiting
code-specific features from the method body or the surrounding
methodsinthesameprogramfile,whichcanbeconsideredaslo-
calcontextsofamethod.However,theinformationofthewhole
project (global context) is ignored in these models. For example,
thedocumentation ofthemethod candescribe themethodâ€™sfunc-
tionality and the role it plays in the project. Besides, there also
existnestedscopesforproject,whereasourcecodefilecanhave
references to other files of the same projects. Thus, the contextsfrom other program files which are imported by the file wherethe target method in are also helpful in understanding the meth-
ods. Intuitively, these contexts are of great importance for method
namerecommendation,especiallyforthemethodswhichhavelittle
contentinthe body,butwithsufficient globalcontexts.Amethod
does not exist in isolation, a large number of associations can be
found among the project-specific contexts and the documentation:
(1)Thefunctionalityandnamingconventionofamethodcanbe
better understood when more contextual features are provided. (2)
There might be many possible names that can match the semantic
ofthemethod.Byreferringtotheglobalcontextualinformation,the solution space of the method names can be narrowed. Thus,
when recommending a method name, it is necessary to refer to
the global contexts. It can help in following situations: when the
method is first created, existing global context can be accessed for
suggesting a proper name for it; during the code refinement, theglobal context can be used to suggest an alternative name if the
current name is inconsistent.
Toverifyourintuition,wefirstconductedastatisticalanalysis
to learn the relation between the method names and their contexts
of different levels. Based on the statistical analysis results, we pro-
poseGTNM, a novel GlobalTransformer-based NeuralModel for
methodnamesuggestion,aimingatgeneratingmeaningfulandcon-
sistentnamesformethods.Wetreatthemethodnamesuggesting
taskastheabstractivetextsummarization,where thetokensfrom
thecontextsofdifferentlevels areconsideredasinput,and thesub-
tokensinthemethodâ€™sname isconsideredasthetargetsummary
ofinputsequences.Weusetheattentionmechanismtoallowthe
model to attend to different contexts during the decoding process.
Themaincontributionofourmodelcanbesummarizedasfol-
lows:â€¢We conduct a statistical analysis to explore the relationship
betweenthemethodnamesandtheircontextsofdifferent
levels.
â€¢Weproposeanovelglobalapproachformethodnamesug-
gestion, which considers the local context, the project-level
context, and the documentation of the method simultane-
ously.
â€¢Weconductextensiveexperimentstoevaluateourapproachonthelarge-scaledatasetsofJavamethods.Theexperimental
results show that our model substantially improves the per-
formance of the previous approaches on suggesting method
names.
2 MOTIVATING EXAMPLE AND STATISTICAL
ANALYSIS
According to Nguyen et al . [34], the principle of naturalness of
software [ 16] also holds for the tokens composing the names of
program entities. Specifically, tokens are repetitive and occur inregularity, where the repetitiveness can be captured by statisti-cal models trained on a large code corpus. Therefore, the tokenscomposing the names of program entities can reflect the seman-
ticandfunctionalityofthecodesnippets.Basedonthisevidence,
mostpreviousworkmainlyconsiderstheassociationsamongthe
tokens of the method names and the tokens in the method body
(local context). However, only considering the local context is not
sufficient.Weassumethattheproject-specificcontextcanbetter
reflecttherolethatthetargetmethodplaysinthewholeproject.
Forexample,themethodsinthesamefilewiththetargetmethod
(we call them in-file contextual methods) and the methods in other
program files of the same project that are imported by the file
where the target method locates (we call them cross-file contextual
methods). Besides, the documentation of the method also plays an
important role in recommending the method names. We present
several java method examples to illustrate the associations among
methodnamesandtheproject-specificanddocumentationcontexts
in Section 2.2, appearing as the token overlapping. Based on those
observations,weconductastatisticalanalysistoexploretherela-
tionshipbetweenthemethodnamesandthecontextsofdifferent
levels in Section 2.3, i.e., local context, project-specific context, and
documentation context.
2.1 Definitions
Firstly,wegiveabriefdefinitionoftokens,localcontext,project-
specific context, and documentation context.
DefinitionofTokens. Forprograms,weparsetheprogramtoAST
and extract entities (method names, identifiers, parameters, return-
types) from AST. Then we split the entities following camelcaseand underscore naming conventions, and lowercase the entities
togettokens.Fordocumentation,weextractthefirstsentencein
Javadocbydeletingthepunctuations.Thenwesplitthesentence
with space to get words and lowercase the words to get tokens.
DefinitionofLocalContext. Local-contextcontainstheprogram
entitiesinthemethodsignature andbody, includingparameters,
return type, and identifiers.
Definition of Project-specific Context. Project-specific context
is supposed to reflect the target methodâ€™s role in the whole project
1295
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Learning to Recommend Method Names with Global Context ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
and the naming styles. We argue that the methods in the same file
withthetargetmethod(wecallthemin-filecontextualmethods)
andthemethodsinotherprogramfilesofthesameprojectthatare
importedbythefilewherethetargetmethod(wecallthemcross-
file contextual methods) in can provide the above information. We
considerthenameofthecontextualmethodsastheproject-specific
context.
DefinitionofDocumentationContext. Thefirstsentenceofthe
code documentation is informative, and many code summarization
approaches use itas a code summary[ 19,23,43]. Following them,
weusethetokensinthefirstsentenceasthedocumentationcontext.
2.2 Motivating Example
1. Theproject-specificcontextmight containthe entitiesthat can
providesemanticinformationforthetargetmethodnamerecom-
mendation.InCode1,thenamesofthethirdmethod(getMaxValue)
donotdescribethefunctionalityofthemethodswell.Whenchang-
ing it into a more precise name that contains the project-related
entitynames(getMaximumResourceCapability),onlyreferringto
the method body is not enough. If the (in-file) project-level con-textual information, i.e., other methods in the same file, can beaccessed, we can easily realize that the method is related to the
resource capability and make correct revisions.
public Resource getClusterResource() {
return clusterResource;
}
public Resource getMinimumResourceCapability() {
return minimumAllocation;
}
// consistent name: getMaximumResourceCapability
public Resource getMaxValue() {
return maximumAllocation;
}
Code1:Project-specificcontextcontainstheentitiesthatcan
provide semantic information
2. The project-specific contextual information can imply the
logicandthefunctionalityoftheproject,whichwillreflecttherole
the target method plays in the project. In Code2, these methods
are related to the window events, including the keypress events or
trackpadtouch events.By accessingthe(in-file) project-levelcon-
text,thefunctionalityofthewholeprojectandtheroleofthetarget
methodcanbebetterunderstood,thusofferingmoreknowledge
for recommending meaningful method name.
public boolean touchDown (InputEvent e vent ,floatx,floaty,int
pointer , intbutton) {
...
}
public void touchUp (InputEvent e vent ,floatx,floaty,int
pointer , intbutton) {
...
}
public boolean keyDown ( InputEvent e vent ,intkeycode) {
return isModal;
}
public boolean keyUp (InputEvent event , intkeycode) {
return isModal;
}
Code 2: project-specific contexts imply the logic and the
functionality of the project.
3. There might be many semantically consistent names that can
reflectthefunctionofaspecificmethod.Wecannarrowthesolution
space and suggest a consistent and conventional method name byreferringtotheproject-specificcontextualinformation.Bothofthe
twomethodsinCode3indicatethatsomeerrorsareencountered.
However,differentverbsareusedinthenames(â€œEncounteredâ€and
â€œOccuredâ€), and they are synonyms. Although these two names are
bothsemanticallycorrect,theyarenotconsistent.Whenrefactoring
thesecondmethodnameâ€œserverErrorOccuredâ€intoanamethatis consistent with the contextual methods, we can use the verb
â€œEncounteredâ€ to replace â€œOccuredâ€ by referring to the previous
method name â€œclientErrorEncounteredâ€. This suggests that with
thehelpoftheproject-levelcontext,wecanchoosethecandidate
names from a smaller and specific solution space.
public void clientErrorEncountered() {
clientErrors.incr();
}
// consistent name: ser verErrorEncountered
public void serverErrorOccured() {
serverErrors.incr();
}
Code 3: Semantically consistent names
4. Cross-file project-specific context can provide extra infor-
mation when the in-file context is less informative. In code4, the
AccountActivity class inherits from BaseActivity class, thus the
methods of the parent class BaseActivity might be overridden in
AccountActivity class, for example, getLayoutRes(), onCreateActiv-
ity(),etc.Theprogram filewheretheBaseActivityclassdefinedis
importedatthebeginningofthefile.Thus,wecanextractthemeth-
odsdefinedintheBaseActivityclassbyconsideringthecross-file
project-specific contexts. Thus, when predicting the method name
forthemethodsinAccountActivityclass,themethodsdefinedinits
parent class can be accessed, which are helpful for the cases where
thein-filecontextislessinformativeforinferringthemethodname.
[AccountActivity.java]
...
import com.github.airsaid. accountbook.base.BaseActivity;
...
public class AccountActivity extends BaseActivity {
@Override
public int getLayoutRes() {
return R.layout. activity_account;
}
@Override
public void onCreateActivity(@Nullable Bundle
savedInstanceState) {
Account account = get Intent().getParcelableExtra(
AppConstants.EXTRA_DATA);
...
}
...
}
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’
[BaseActivity.java]
public abstract class BaseActivity extends SlideBackActivity {
@Override
protected void onCreate(@Nullable Bundle sav edInstanceState) {
...
}
...
public abstract int getLayoutRes() ;
public abstract void onCreateActivity(@Nullable Bundle
savedInstanceState);
}
Code 4: Cross-file project-specific context can provide extra
information when the in-file context is less informative
5.Thedocumentationcanalsoproviderichinformationaboutthe
methods,whichwill helpforsuggestingmethod names.InCode5,
thebodycodeofthesemethodslookssimilar,andallofthemcannot
offer enough information for suggesting the method name. The
1296
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Liu, et al.
Local ContextDocumentation
Code Encoder
Project Context
Encoder
Ã—
Attention Layer
...
Attention Layer
Project-specific 
Context
+
Invoked Weight
y0y1 ytyt+1
Attention LayerDecoder
Project
Doc 
Embedding
Code
Embedding
Code
Embedding
MethodContext
Extraction
Figure 1: The overall framework of GTNM.
documentationofthemethodscontainsusefulinformationthatcan
reflect the functionality of the methods, thus being helpful for the
method name recommendation. When predicting the name for the
first method, the documentation can provide a useful indication.
/âˆ—âˆ—
âˆ— Used to retrieve the plugin tool 's descriptive name. âˆ—/
// consistent name: getDescriptiveName
@Override
public String getName() {
return "Remove S purs (prunning)" ;
}
/âˆ—âˆ—
âˆ— Used to retrieve a short description of what the plugin tool
does. âˆ—/
@Override
public String getToolDescription() {
return "Removes the spurs ( prunning operation) from a Boolean
image." ;
}
Code 5: The documentation can provide rich information
about the methods.
2.3 Statistical Analysis
Basedontheaboveobservations,weconductastatisticalanalysisto
exploretherelationshipsbetweenthemethodnamesandtheircon-
textsby computingthe percentageoftheir tokensharing.For the
analysis, we used the java programs in the Java-small dataset used
in Alon et al . [6]. The dataset contains 11 high quality open-source
java projects, which is a benchmark dataset for method name sug-
getstion task. It contains about 700K Java method examples. Thus,
weusethisdatasettoconductthestatisticalanalysistoexplorethe
relationshipsbetweenthemethodnamesandtheircontexts.The
statistical results in this analysis can be expected in a good project
where most of the names are consistent.
For local context, we found that the tokens of 67.47% of the
methodnamescanbefoundintheidentifiers,and35.64%canbe
foundinthereturntypeandparameters.ForProject-specificcon-
text,wefoundthatthetokensof85.98%ofthemethodnamescanbefoundinthenamesofitsin-filecontextualmethods,andthetokens
of 53.83% of the method names can be found in the names of its
cross-filecontextualmethods.Forthedocumentationcontext,we
found that the tokens of 55.98% of the method names can be found
initsdocumentation.Thereexistsoverlappingamongdifferentcon-texts,forexample,thesubtokensofthemethodnamecanappearinbothlocalanddocumentationcontexts.Thus,thesumofthesenum-bersisnot100%.Besides,10.87%ofthemethodnamescannotfoundinthebody,butoccurinthenamesofitsproject-specificcontext
(in- and cross-file contextual methods). These results demonstrate
thatdevelopersalwaysrefertotheproject-specificcontextwhen
namingthemethods.Thus,project-specificcontextalsocontains
essential information for method name recommendation, which
should be carefully considered.
3 PROPOSED MODEL
3.1 Overview
In this work, we propose GTNM, a global Transformer-based Neu-
ral Model for method name recommendation aiming at generating
meaningful and consistent method names. The overall architecture
of our approach is shown in Figure 1. To fully utilize the contex-
tual information of a method, we firstly extract context from three
different levels giventhe target method andthe project, including
the local context, project-specific context, and documentation con-
text. We employ a transformer-based seq2seq framework [ 41]t o
generate the method name. Specifically, we build corresponding
encoderstoencodethecontextsintovectorrepresentations.The
decoder generates the target method name by sequentially predict-
ingtheprobabilityofthesubtokens ğ‘¦ğ‘¡+1inthemethodnamebased
onthecontextualrepresentationsproducedbytheencoders,and
thepreviouspredictedsubtokens ğ‘¦1,ğ‘¦2,...,ğ‘¦ ğ‘¡.Weusetheattention
mechanismtoallowthemodeltoattendtodifferentcontextsduring
the decoding process.
3.2 Context Extraction
We extract the contexts of three different levels for generating
meaningful and consistent names for the method, including lo-
calcontext, project-specificcontext, anddocumentation.Figure 2
showsanexampleofthecontextsfortheJavamethodâ€œgetElementâ€.
Local Context Extraction According to the results of our statisti-
calanalysisandtorepresentthemethodbodysuccinctly,weextract
the following code entities as the local contexts for the method: (1)
identifiers;(2)parameters;(3)returntype.Wetokenizedeachofthe
names from the local contexts following camelcase and underscore
namingconventions,thennormalizedthetokenstolowercase.Fi-
nally, all the subtokens are concatenated in the order that they
occurred in the source code to form the sequential representation
of the local feature.
1297
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Learning to Recommend Method Names with Global Context ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
...
import DataStructures.Heaps.Heap;
public class MaxHeap implements Heap {
 ...
  /**
   * Get the element at a given index. The key for the list is equal to index value - 1
   */  public HeapElement getElement ( int elementIndex ) {
    if ((elementIndex <= 0) || (elementIndex > maxHeap.size()))
      throw new IndexOutOfBoundsException("Index out of heap ra nge");
    return maxHeap.get(elementIndex - 1);
  }
// Get the key of the element at a given index
  private double getElementKey(int elementIndex) {
    return maxHeap.get(elementIndex - 1).getKey();  }
// Swaps two elements in the heap
  private void swap(int index1, int index2) {
    HeapElement temporaryElement = maxHeap.get(index1 - 1);
    maxHeap.set(index1 - 1, maxHeap.get(index2 - 1));
    maxHeap.set(index2 - 1, temporaryElement);
  } ...
  @Override
  public void insertElement(HeapElement element) {
   ...
  }
  @Override
  public void deleteElement(int elementIndex) {    ...
  }
 ...
package DataStructures.Heaps;
public interface Heap {
  /**
   * @return the top element in the heap, the one with lowest k ey for min-heap or with 
the highest
   */  HeapElement getElement() throws EmptyHeapException;
  /**
   * Inserts an element in the heap. Adds it to then end and to ggle it until it finds its 
right position.
   */
  void insertElement(HeapElement element);
  /**
   * Delete an element in the heap.
   */  void deleteElement(int elementIndex);
}Heap.java
MaxHeap.java
get the element at a given index the key 
for the list is equal to index value - 1
Documentation Context
/
*
Get the element at a given inde x
. 
The key for the list is equal to index value 
 -
1
*/
Element index max heap 
size index out of 
bounds exception ...
Identifiers
heap 
element
Return Type
int element 
index
e
Parameters
 d ifi
Local Context
...
get element keyswap
insert element
delete element
...
In-file Project-specific Context
...
get element
insert element
delete element
...
cross-file Project-specific Context
Figure 2: Different levels of contexts for method name suggestion.
Project-specificContextExtraction Wedefinetheproject-specific
context of one method as its in-file methods (other methods in the
samefilewiththetargetmethod)andcross-filecontextualmethods
(methods in the files imported by the file containing the target
method).Forsimplicityandefficiency,weextractthenameofthe
contextual methods as the project-specific context. Thenwe per-
form a similar process to these names as to local context. The
concatenation of the lower-cased subtokens serves as the represen-
tation of the project-specific feature.
Documentation Context Extraction For each method with a
comment, to get its documentation context, we extract the first
sentence that appeared in its Javadoc description since it typically
describes the functionalities of the method1. Then we delete the
punctuations and split the sentence with space to get words and
lowercasethewords.Allthewordsareconcatenatedtoformthe
documentation context.
3.3 Global Transformer-based Neural Model
Weuseatransformer-basedmodeltogeneratethemethodname,
which leverages the self-attention mechanism and can capture rich
semanticdependencies.TheTransformerconsistsofstackedself-
attentionandpoint-wise,fullyconnectedlayers.Themulti-head
attentionmechanismisperformedintheself-attentionlayers.In
eachattentionhead,giventheinputvectors ğ’™=(ğ’™1,ğ’™2,...,ğ’™ğ‘›),the
output vectors ğ’=(ğ’1,ğ’2,...,ğ’ğ‘›)is computed as:
ğ’ğ‘–=ğ‘›/summationdisplay.1
ğ‘—=1ğ›¼ğ‘–ğ‘—(ğ’™ğ‘—ğ‘¾ğ‘‰)
ğ›¼ğ‘–ğ‘—=exp(ğ‘’ğ‘–ğ‘—)/summationtext.1ğ‘›
ğ‘˜=1exp(ğ‘’ğ‘–ğ‘˜)
ğ‘’ğ‘–ğ‘—=ğ’™ğ‘–ğ‘¾ğ‘„(ğ’™ğ‘—ğ‘¾ğ¾)ğ‘‡
/radicalbig
ğ‘‘ğ‘˜(1)
1http://www.oracle.com/technetwork/articles/java/index-137868.htmlwhere ğ‘¾ğ‘„,ğ‘¾ğ¾âˆˆRğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘˜,ğ‘¾ğ‘‰âˆˆRğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘£are the trainable
parameters that are unique per layer and per attention head. Then
theoutputsofalltheheadsareconcatenatedtoproducethefinal
output of the self-attention layer.
Aftertheattentionlayersofbothencoderanddecoder,afully
connected feed-forward network is employed:
ğ¹ğ¹ğ‘(ğ’™)=ğ‘šğ‘ğ‘¥(0,ğ’™ğ‘¾1+ğ’ƒ1)ğ‘¾2+ğ’ƒ2 (2)
where ğ‘¾1âˆˆRğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—4ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™,ğ‘¾2âˆˆR4ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™,ğ’ƒ1âˆˆR4ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™,
ğ’ƒ2âˆˆRğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™are the trainable parameters.
Encoders. WebuildaCodeEncodertoencodethewholecontext ğ‘¥
including the local context, project-specific context, and documen-
tation forthe method namegeneration, and buildan extra Project
Context Encoder to encode the project context ğ‘¥ğ‘ğ‘Ÿğ‘œfor enhancing
the attention to the project-specific context.
i)CodeEncoder. Thelocalcontext,project-specificcontextandthe
documentationcontextarefirstembeddedintovectors ğ’™ğ‘™ğ‘œğ‘,ğ’™ğ‘ğ‘Ÿğ‘œ,
ğ’™ğ‘‘ğ‘œğ‘,thenthesevectorsareconcatenatedtoformtherepresentation
of the whole contexts ğ’™=ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ’™ğ‘™ğ‘œğ‘,ğ’™ğ‘ğ‘Ÿğ‘œ,ğ’™ğ‘‘ğ‘œğ‘), where|ğ’™|=
|ğ’™ğ‘™ğ‘œğ‘|+|ğ’™ğ‘ğ‘Ÿğ‘œ|+|ğ’™ğ‘‘ğ‘œğ‘|.Thenweemploytransformer-basedencoder
to encode ğ’™into hidden representation ğ’‰=(ğ’‰1,ğ’‰2,...,ğ’‰|ğ’™|).
ii) Project-specific Encoder. To increase the attention for the
project-specific context, especially for the method names where
thetargetmethodinvoked,webuildaProject-specificEncoderto
encodetheproject-specificcontext ğ’™ğ‘ğ‘Ÿğ‘œintohiddenrepresentation
ğ’‰ğ‘ğ‘Ÿğ‘œ=(ğ’‰ğ‘ğ‘Ÿğ‘œ
1,ğ’‰ğ‘ğ‘Ÿğ‘œ
2,...,ğ’‰ğ‘ğ‘Ÿğ‘œ
|ğ’™ğ‘ğ‘Ÿğ‘œ|). We use a mask vector ğ‘´âˆˆR|ğ’™ğ‘ğ‘Ÿğ‘œ|
torecordthemethodsthatareinvokedbythelocalcontext. ğ‘€ğ‘–is1
if theğ‘–-th method in the project-specific context is invoked by the
local context else is 0.
Intuitively, the methods in the project-specific context invoked
bythelocalcontextaremoreimportantandrelativetothetarget
method.Thuswegivethesemethodsmoreattentionbymultiplying
theinvokedweight ğ’˜ontheproject-specifichiddenvector ğ’‰ğ’‘ğ’“ğ’to
1298
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Liu, et al.
Table 1: Statistics of the datasets.
Train Validation Test
Files 1,700,000 393,327 61,000
Methods 18,230,509 4,283,580 636,816
Methods with doc 4,264,852 964,078 143,913
produce the final project-specific hidden vector Ëœğ’‰ğ‘ğ‘Ÿğ‘œ:
ğ’˜=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(1+ğ‘´)
Ëœğ’‰ğ‘ğ‘Ÿğ‘œ=ğ’˜âŠ—ğ’‰ğ‘ğ‘Ÿğ‘œ(3)
whereâŠ—is the element-wise production operation.
Decoder. The decoder aims togenerate thetarget methodname
by sequentially predicting the subtoken ğ‘¦ğ‘¡+1conditioned on the
context vectors ğ’‰andËœğ’‰ğ‘ğ‘Ÿğ‘œ, and the previous generated subtokens
ğ’š1:ğ‘¡:
ğ‘(ğ‘¦ğ‘¡+1)=softmax(ğ¹ğ¹ğ‘(ğ’…ğ’†ğ’„2))
ğ’…ğ’†ğ’„2=Attention-Layer3 (ğ’‰,ğ’…ğ’†ğ’„1)
ğ’…ğ’†ğ’„1=Attention-Layer2 (Ëœğ’‰ğ‘ğ‘Ÿğ‘œ,ğ’…ğ’†ğ’„)
ğ’…ğ’†ğ’„=Attention-Layer1 (ğ’š1:ğ‘¡)(4)
where the first attention layer performs multi-head attention over
the decoder input ğ‘¦1:ğ‘¡to produce the hidden representation ğ’…ğ’†ğ’„.
Thenthesecondattentionlayerperformsmulti-headattentionover
the weighted project-specific hidden vector Ëœğ’‰ğ‘ğ‘Ÿğ‘œto produce the
hiddenrepresentation ğ’…ğ’†ğ’„1,whichmodelsthedependencybetween
thedecoderinputandtheproject-specificcontext.Thelastattentionlayerperformsmulti-headattentionoverthewholecontexthidden
vector ğ’‰to produce the final hidden representation ğ’…ğ’†ğ’„2, which
modelsthedependencybetweenthedecoderinput,project-specificcontext,andthewholecontext.Thenthefinalhiddenrepresentation
is fed into a fully connected feed-forward network and softmax
layertoproducetheprobabilityofthenextsubtoken ğ‘¦ğ‘¡+1forthe
target method name.
Training. To train the network, we adopt cross-entropy loss be-
tween the predicted distribution ğ’’and the â€œtrueâ€ distribution ğ’‘,
which is computed as:
ğ»(ğ’‘||ğ’’)=âˆ’/summationdisplay.1
ğ‘¦âˆˆğ‘Œğ‘(ğ‘¦)logğ‘(ğ‘¦)=âˆ’logğ‘(ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’)(5)
whereğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’is the target name. Since p will assign value of 1
to the actual label in the training example and 0 otherwise, the
cross-entropylossforaexampleisequivalenttothenegativelog-
likelihood of the true label. As ğ‘(ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’)tends to 1, the loss ap-
proaches zero. The smaller ğ‘(ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’)goes, the greater the loss be-
comes. Thus, minimizing this loss is equivalent to maximizing the
log-likelihood that the model assigns to the true labels.
4 EXPERIMENTAL SETUP
4.1 Datasets
WetrainandevaluateGTNMonJavaprogramsfollowingMNire
[34] and Code2vec [ 7]. Nguyen et al . [34]provide the list of java
repositories,whichcontains10Ktop-ranked,publicJavaprojects
on GitHub. They used the same setting as in code2vec to shuffle
filesinalltheprojectsandsplittheminto1.7Mtrainingand61KTable 2: Statistics of contexts and target name lengths.
Avg Med
In-file Contextual Method 1399 68Cross-file Contextual Method 197 80Variables 23 7
Parameter and return type 3 3Target Names 3 2
testing files. Following their setting, we download the repositories
they provide and followthe same way tobuild the dataset.After
data processing, the detailed data information is shown in Table 1.
4.2 Metrics
To evaluate the quality of the generated method name, we adopted
the metrics used by previous works [ 6,7,34], which measured
Precision, Recall,and F-scoreoversub-tokens.Specifically,forthe
pair of the target method name ğ‘¡and the predicted name ğ‘, the
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ‘¡,ğ‘),ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘¡,ğ‘), andğ¹1(ğ‘¡,ğ‘)score are computed as:
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ‘¡,ğ‘)=|subtoken (t)|âˆ©|subtoken (p)|
|subtoken (p)|
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘¡,ğ‘)=|subtoken (t)|âˆ©|subtoken (p)|
|subtoken (t)|
ğ¹1(ğ‘¡,ğ‘)=2Ã—ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ‘¡,ğ‘)Ã—ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘¡,ğ‘)
ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› (ğ‘¡,ğ‘)+ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘¡,ğ‘)(6)
wheresubtoken( ğ‘›)returnthesubtokensinthename ğ‘›.Precision,
Recall, and F-score of the set of the suggested names are defined astheaverageonesonallsamples.Besides,wealsomeasurethe Exact
Match Accuracy (EM Acc), in which the order of the subtokens are
also taken into consideration.
4.3 Implementation Details
We use Transformer with 6 layers, hidden size 512, and 8 attention
heads for both encoders and decoders. The inner hidden size of thefeed-forward layer is 2048. We use javalang
2to parse the java code
to extract the contexts. The details of different contexts and target
names (subtoken) lengths are shown in Table 2.
In our experiments, we set the in-file project-specific context
length to 30, the cross-file project-specific context length to 30, the
local context length to 55 (variable length (50) + parameter andreturn type length (5)), the documentation context length to 10.
Andthemaximumtargetnamelengthissetto53.Weusethesame
vocabulary for the input source code and the target method name
and build another vocabulary for the documentation context. The
vocabularysizeforthesourcecodeissetto20,000,andthevocabu-
lary size for documentation is set to 10,000. The out-of-vocabulary
tokens are replaced by /angbracketleftUNK/angbracketright. To demonstrate the effectiveness
ofthecross-fileproject-specificcontext,weconductexperiments
under the cross-project setting where the programs used in thetraining and test process are from different projects. Since morecontexts can be accessed, we assume that we can use fewer pro-grams to train the model. To verify the assumption, we train the
2https://github.com/c2nes/javalang
3we examined modelâ€™s performance with different context length settings, the setting
that can achieve the best results were used for the final training
1299
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Learning to Recommend Method Names with Global Context ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 3: Method name recommendation comparison results.
Model Precision Recall F1 EM Acc
code2vec[7] 51.93% 39.85% 45.10% 35.59%
code2seq[6] 68.41% 60.75% 64.36% 41.50%MNire[34] 70.10% 64.30% 67.10% 43.10%DeepName[24] 73.60% 71.90% 72.70% 44.30%
GTNM 77.01% 74.15% 75.60% 62.01%
model using a subset of the whole training dataset and compare it
withtheresultswithoutusingthecross-fileproject-specificcontext.
The detailed results are presented in 5.3.
We use Adam with the learning rate of 3e-4, linear learning rate
warmupscheduleoverthefirst4,000stepstotrainthemodelfor
20 epochs. We use a dropout probability of 0.3 on all layers. Our
model is trained on one Tesla V100 GPU with 16GB memory.
5 RESEARCH QUESTIONS AND RESULTS
To evaluate our proposed approach, in this section, we conduct
experiments to investigate the following research questions:
5.1 RQ1: Comparison against state-of-the-art
models
We compare GTNM with the following state-of-the-art method
name suggestion models:
1) code2vec [ 7]: an attention-based neural model, which performs
attentionmechanismoverASTpathsandaggregatesallofthepath
vector representations into a single vector. They considered the
methodnamepredictionasaclassificationproblemandpredicteda methodâ€™s name from the vector representation of its body.
2) code2seq [ 6]: an extended approach of code2vec, which employs
seq2seq framework to represent AST paths of the method body
node-by-nodeusingLSTMsandthenattendtothemwhilegenerat-
ing the target subtokens of the method name.
3) MNire [ 34]: an RNN-based seq2seq model approach to suggest a
method name based on the program entitiesâ€™ names in the method
body and the enclosing class name.
4)DeepName[ 24]:anRNN-basedapproachformethodnameconsis-
tency checking and suggestion, using both internal and interaction
contexts for method name consistency checking and suggestion,
which achieves the state-of-the-art results on java method name
suggestion task.
Thefirstthreebaselinesdonotusethecross-fileproject-specific
context for the method name suggestion. To make the comparison
fair, we do not use the cross-file project context in this experiment.
We use the same dataset as MNire and DeepName to train our
model. For code2vec and code2seq, we download their publicly
availablesourcecodeandtraintheirmodelonthesamedatasets.
The results are shown in Table 3. Among these baselines, code2vec
andcode2seqonlyusethecontextinthemethodbodytopredict
themethodnames.MNireutilizestheenclosingclass(wherethe
method is in) contexts, and DeepName further considers the inter-
action context and sibling context, which might appear in other
program files.
TheresultsshowthatGTNMoutperformsallthebaselinemodels
on all the metrics by a large margin, especially on the exact matchTable4:Exampleswheretheexactmatchdidnotoccurbut
F1 was good.
Prediction Ground Truth
â€˜beforeâ€™, â€˜attachâ€™, â€˜primaryâ€™, â€˜storageâ€™ â€˜beforeâ€™, â€˜detachâ€™, â€˜primaryâ€™, â€˜storageâ€™
â€˜resetâ€™, â€˜bufferâ€™ â€˜resetâ€™
Table 5: Performance of using different contexts.
Model Precision Recall F1 EM Acc
Token seq 70.25% 64.75% 67.39% 49.44%
Local cxt 69.60% 64.38% 66.89% 50.95%
+ In-file Project cxt 75.16% 71.83% 73.46% 59.51%+ Documentation cxt 77.01% 74.15% 75.60% 62.01%
accuracy. The higher exact match accuracy indicates the generated
nameismoreclosetothegroundtruth.Table4showstwoexamples
where the exact-match didnâ€™t occur but F1 was good. In the first
case, the semantics of two names are reverse although they shared
most of the sub-tokens with a high F1 score. Thus, exact match
accuracycanevaluatethegeneratednamemoreprecisely,whichplaysacrucialroleinmethodnamesuggestion.Thereare32%of
the test methods where exact match is not satisfied but F1 â‰¥0.5.
Amongthesecases,only2.32%ofmethodshavethesamesubtoken
set between generated names and target name.
AlthoughMNireandDeepNamealsoconsiderthecontextsbe-
yond the method body, the contexts extracted by their approaches
are different from ours. They only consider the contexts directlyinteracting with the target method, such as the sibling methods,
callersmethods,andcalleesmethods.However,themethodswhich
havenoexplicitinteractionwiththetargetmethodscanalsopro-
videessentialinformationforunderstandingthefunctionalityofthe
targetmethod.Forexample,the methodsappeared inthe importedfiles,asshowninourpreviousmotivationexamples.Besides,MNire
and DeepName use an RNN-based model to learn the relationship
amongtheentitiesinthecontext.Inourmodel,weextractcontexts
froma largerset ofprogramentity candidatesand employapow-
erful backbone model to model the contexts, which is based on the
self-attentionmechanism.Besides,wealsogivetheproject-specific
contextsmoreattentionweightsbyapplyinginvokedweightma-
trix. When generating the names of the target method, different
decoder layers are utilized to focus on the contexts of different lev-
els. Thus, our model can achieve better performance than baseline
models.
Amongthesemetrics,theexactmatchaccuracyismuchmore
strict than the other three metrics, which calculates the percentage
of the predicted method names that are exactly the same as thegroundtruth.Theotherthreemetricsarebasedonthesubtokenoverlapping between the predicted names and the target names,
where the order of the subtokens is ignored. The results show that
ourmodelobtainslargerimprovementsonexactmatchaccuracy
and recall, which further demonstrates that the subtokens in thepredicted names generated by our model can cover much more
targetsubtokensthantheotherbaselines.Thereforeourmodelcan
fully and precisely describe the functionality of the method body.
1300
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Liu, et al.
Table 6: The results on the extracted documented methods.
Model Precision Recall F1 EM Acc
GTNM 85.36% 82.54% 83.93% 70.60%
- doc 80.31% 76.65% 78.44% 64.14%
5.2 RQ2: The contributions of contexts in the
same file
Inthepreviousexperiment,weconsidercontextsofthesamefile
(i.e., local context, in-file project-specific context, and documen-
tation context) for generating the method name. To answer this
research question, we conduct experiments using different context
combinations. As shown in Table 5, the first row shows the results
of only taking the source code token sequence in the method body
as input. The second row presents the result of using the local con-
text(i.e.,theentitiesâ€™namesofthemethodsignatureandvariables)
as input to suggest the method name. The third row shows theresultsofusingboththein-fileproject-specificcontextandlocal
context.Thelastrowgivestheresultsofusingallthreecontexts:
local, in-file project-specific, and documentation context.
AsseenfromTable5,comparingtheresultsofusinglocalcon-
text(sequencelengthis55)withtheresultsofusingsourcecode
tokensequence(sequencelengthis200),theperformanceiscompa-
rable,andusingthelocalcontextcanachievehigherexactmatch
accuracy. However, the length of the local context is much shorter
than the source code token sequence, which demonstrates that the
local context extracted by our model contains enough information
aboutthefunctionalityofthemethodbody,andtheshortercontext
canimprovethecomputationalefficiencyofthemodel.Whenwe
further incorporating the project-specific context information, the
performance is improved by a large margin. Specifically, the F1
score and exact match accuracy significantly increase from 66.89%
and 50.95% to 73.46% and 59.51%. The substantial improvementshows that the project-specific context, which can offer knowl-edge about the project information, is essential and efficient for
improving the performance of method name recommendation.
Whenthedocumentationinformationisadded,theperformance
is further improved. However, in our whole dataset, only about20% of the methods have the document information. Thus, formost of the methods, the documentation context information is
missing.Todirectlyillustratethecontributionofthedocumentation
contextinformation,weextractthosedocumentedmethodsfrom
thewholedatasetandpresenttheresultsontheextracteddataset.
As shown in Table 6, the first row shows the results of our full
model on the extracted dataset, and the second row shows theresults of removing the documentation context from the input.
When removing the documentation context, the performance is
decreased by 5.1 in precision, 5.9 in recall, 5.5 in F1, and 6.5 inexact match accuracy, respectively. The results demonstrate that
the documentation context can provide useful information for the
method name suggestion.
5.3 RQ3: The contribution of cross-file context
Whenconsideringthecross-fileproject-specificcontext,weneed
to preserve the project structure of the programs in the dataset.
Sincemorecontextualinformationcanbeaccessed,weassumethatTable 7: Performance of using cross-file project-specific con-
text under cross-project and low-resource setting.
Model Precision Recall F1 EM Acc
w/o cross-file cxt 67.25% 64.66% 65.93% 49.71%w/ cross-file cxt 73.52% 70.65% 72.06% 60.69%
the model can be trained in a low-resource setting, that is, fewerprograms are needed for training the model. Thus, we only use
asubsetofthewholetrainingdatasetinthisexperiment.Specifi-
cally,wesample4000projectsfromthebigtrainingsetasasmall
training set and extract the cross-file project-specific context for
theprogramsinthesampledprojects.Wecomparewiththeresults
of our model setting without using project-specific context. To fur-
ther demonstrate the effectiveness of the cross-file project-specific
context,weconducttheexperimentunderthecross-projectsetting.
That is, we split the corpus based on the projects instead of files or
themethods.Thecross-projectsettingischallengingandreflects
betterthereal-worldusageofthemethodnamerecommendation
where the model is trained on the set of existing projects and used
to check for a new project.
The results are shown in Table 7. As seen from the results, with
thehelpofcross-fileproject-specificcontext,ourmodelcanachieve
comparableresultswiththeresultsofthepreviousmodelsetting,
where the training set is bigger and in-project split, only using
lessthan50%ofthewholetrainingsetandunderthechallenging
cross-projectexperimental setting.Whenremovingthe cross-file
project-specific context, the performance of the model drops a lot,
whichfurtherdemonstratestheimportanceofthecross-fileproject-
specific context.
6 DISCUSSION
6.1 Qualitative Analysis
We perform qualitative analysis on the human-written method
names and method names which are automatically generated by
GTNM.Inmostcases,thenamesgeneratedbyGTNMareexactly
the same as the human-written names. To figure out in what cases
our model generates different names with human, we randomly
sample 200 cases where the names generated by our model are
different from the ground truth from the test to analyze the results.
FollowingMcBurneyandMcMillan [31]andHuetal .[20],w e
performedqualitativeanalysistoobtainopinionsfromparticipantsonthequalityofthegenerated-name,aimingatgettingthefeedback
onourapproachanddirectionsforfuture-work.Weinvited8vol-
unteers with 3-5 years of Java development experience to evaluate
thegeneratednamesofthesampled200casesintheformofaques-tionnaire. Each participant is asked to answer several questions, in-cludingwhetherthehuman-written-namesorgenerated-namesare
good, what are the differences between two names, etc. According
tothequestionnaireresults,wesummarizetop-4representativesit-uations(Theproportionofeachsituationis19.4%/43.6%/6.6%/11.9%)
as shown in Table 8.
Contain More Detailed Information. As shown in method 1,
human just names the method as â€œaddâ€. What and when to addis not given. The human-written method name is very short and
cannotreflectthedetailedroleofthetargetmethod.Incaseslike
1301
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Learning to Recommend Method Names with Global Context ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
Table 8: Examples of generated summaries given Java methods.
Examples
Method 1/âˆ—âˆ—
âˆ— Adds a path (but not the leaf folder) if it does not a lready exist. âˆ—/
protected void ____ (List<String> path ,intdepth)
{
intparentSize = path. size () âˆ’ 1;
String name = path.get(depth);
Folder child = getChild(name);
if(child == null)
{
child = newFolder( name) ;
...
}
Human-written "add"
GTNM "add", "path", "if", "not", "exists"
Method 2/âˆ—âˆ—
âˆ— Append the longs in the array to the sel ection , eachseparated by a c omma âˆ—/
private void ____ ( long[] objects ) {
for(inti = 0; i < objects.l ength; i++ ) {
selection. append( o bjects[i] );
if( i != objects.le ngth âˆ’ 1 ) {
selection.append( ',');
}
}
}
Human-written "join", "in", "selection"
GTNM "append", "selection"
Method 3/âˆ—âˆ—
âˆ— Calculates the DefinitionUseCoverage fitness for the g iven DU Pair on the
given Executio nResult âˆ—/
public double ____ () {
if(isSpecialDefinition(goalDefinition))
return calculateUseFitnessForCompleteTrace();
double defFitness = calculateDefFitnessForCompleteTrace();
if(defFitness != 0)
return 1 + defFitness;
return calculateFitnessForObjects();
}
Human-written "calculate", "d", "u", "fitness"
GTNM "calculate", "fitness", "for"
Method 4/âˆ—âˆ—
âˆ— Validate removal of invalid entries. âˆ—/
public void ____ () {
RightThreadedBinaryTree<Integer> bt =
newRightThreadedBinaryTree<Integer >();
assertFalse (bt.remove(99));
bt = buildComplete(4);
assertFalse (bt.remove(99));
assertFalse (bt.remove(âˆ’2));
}
Human-written "test", "invalid", "removals"
GTNM "test", "remove", "invalid"
this, GTNM tends to generate a longer name that contains more in-
formationaboutthemethodâ€™sfunctionality.Inthisexample,GTNM
suggests a more detailed name â€œadd path if not existsâ€, which indi-
catesthattheobjectandtheusagescenarioofthetargetmethod.
Our modelcan learn thisdetailed informationfrom the documen-
tation, parameters, and the method body. In the whole test set, 25%
of the wrong cases belong to this situation.
Synonyms. As shown in method 2, the human-written name and
the name generated by our model have the same meaning, andthe verbs used in these two names are synonyms (â€œjoin inâ€ andâ€œappendâ€). Since â€œjoin inâ€ is not as often used as â€œappendâ€ in the
methodnames,andthecontexts(includingtheproject-specificcon-text,localcontext,andthedocumentationcontext)alsodonotoffer
the relevant information about it. Thus, GTNM cannot correctly
suggestthesubtokensâ€œjoininâ€.However,thenamegeneratedby
ourmodelcanalsopreciselydescribethefunctionalityofthetarget
method, which is also semantic consistent and acceptable.
Acronym. In method 3, the human-written name contains an
acronymforthespecificentities,i.e.,â€œduâ€forâ€œdefinitionuseâ€,which
our model cannot correctly infer. Based on the given contexts,
1302
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Liu, et al.
Figure3:Themethodnamelengthdistributionandtheexact
match accuracy of different name lengths
GTNM suggests a name that has a similar style with the project-
specific context, but fails to suggest the acronym for specific entity
names.
Different Word Orders. As shown in method 4, the subtokens in
thehuman-writtennameandGTNMsuggestednamearealmost
thesame(exceptforâ€œremovalsâ€andâ€œremoveâ€),butthesubtoken
orders are different. In this example, the different orders do not
affectthesemanticofthemethodname,andbothofthetwonames
expressthesamemeaning.However,inothercases,thesemanticof
thenameswithdifferentsubtokenordersmightbedifferent.0.7%
of the wrong cases belong to this situation.
6.2 Length analysis
We further analyze the generated name length distribution and the
performance of GTNM for different name lengths. As shown in
Figure3,thelengthsofthemethodnames(thenumberofsubtokensinthemethodname)mainlyrangefrom2to3.Ourmodelgeneratesfewernamesoflength1,andgeneratedmorenameswithlengths4and 5. Among all the methods, only 13.78% of the names generated
by our model are shorter than the ground truth. We apply the
Wilcoxon Rank Sum Test (WRST) [ 44] to test whether the increase
in the method name length is statistically significant, and all the
p-valuesarelessthan1e-5,whichindicatesasignificantincrease.
We also use Cliffâ€™s Delta [ 29] to measure the effect size, and the
valuesarenon-negligible. Thus,ourmodeltendstosuggest more
detailed names for the method. Besides, we also give the exactmatch accuracy of different lengths. As the length increase, the
method naming task becomes harder. Even though our model can
still achieve more than 50% accuracy for the names of length 5.
6.3 Explainability Analysis
Lack of explainability is an important concern in many complex
AI/MLmodelsinSE[ 35,40].Itiscrucialtoensurethatthemodel
islearnedcorrectlyandthelogicbehindthemodelisreasonable,
which is also important for method name recommendation task.
In this section, we analyze the explainability of GTNM. We em-
ploy modelâ€™sconfidence aboutits predictionto decidewhether toacceptthemodelâ€™srecommendation.PredictionConfidenceScore
(PCS)[47]whichdepictstheprobabilitydifferencebetweenthetwoclasses with the highest probabilities is a measure for evaluatingmodelâ€™s confidence. In our model, the Pearson Correlation Scorebetween PCS and F1-score of the generated names is 0.612 and
p-value <0.05, demonstrating that the correctness of the generated
name isclosely related tothe modelâ€™s confidenceabout its predic-
tion.Thus,userscandecidewhethertoacceptthegeneratednamesdependingonthecaseâ€™serrortoleranceandthemodelâ€™sconfidence.
6.4 Threats to Validity
Threatstoexternalvalidity relatetothequalityofthedataset
we used and the generalizability of our results. We evaluate our
approach on the Java dataset, which is a benchmark dataset formethod name suggestion, and has been used in previous work
[6,7,34]. All of the programs in the dataset are collected from top-
ranked and popular GitHub repositories. Thus, most-of-the-names
are expected consistent. However, there still exist a few cases that
thenameisinconsistentasshowninsection6.1.Besides,further
studiesarealsoneededtovalidateandgeneralizeourfindingsto
other programming languages. Furthermore, our case study is on a
smallscale.Moreuserevaluationisneededtoconfirmandimprove
the usefulness of our model.
Threatstointernalvalidity includetheinfluenceofthemodel
architectural choices and the hyper-parameters used in our model.
Thehyper-parametersandarchitecturalchoiceswereobtainedbya
mixofsmall-rangerandomgridsearchandmanualselection.Thus,
there is little threat to the hyper-parameter choosing, and there
might be room forfurther improvement.However,current settings
have achieved a considerable performance increase.
Threats to construct validity relate to the suitability of our eval-
uation measure. We adopted the measure used by the previousmethod name recommendation work [
5â€“7,34], which measured
precision, recall, and F1 score over subtokens, and exact match
accuracy.Thisisbasedontheideathatthequalityofthegeneratedmethodnameismostlydependantonthesub-wordsthatwereused
to compose it.
7 RELATED WORK
7.1 Code Representation
Code representation is a hot research topic in both software en-
gineeringandmachinelearningfields.Differentneuralnetwork-
basedapproacheshavebeenproposedforrepresentingprograms
asvectors,whichcanbedividedintothefollowingcategories:(1)
source code token (subtoken) sequence - Using the source codetoken sequence as input. (2) AST node sequence - Using the flat-tened AST node sequence as input. (3) AST paths - Using a paththrough the AST as input. (4) Graph - Extending ASTs throughadding edges to build the graph as input. (5) Program entities -
Usingtokensinprogramentitiesâ€™names.Theselearnedprogram
vectors then can be used for various SE tasks, such as code sum-
marization [ 19,43], method name recommendation [ 7,34], code
clone detection [ 33,46], code completion [ 21,26,27], etc. These
different approaches model the program from different aspects, for
example, ASTs can represent the structure and the syntax of thesourcecodebetter,whilethegraphsfocusmoreonthedataflowand the semantic of the programs. For method name recommen-
dation,existingresearchmainlyfocusesonmodelingthemethod
1303
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Learning to Recommend Method Names with Global Context ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
bodyastokensequence[ 5,34]orASTpaths[ 6,7],andthenbuiltan
RNN-based encode-decoder framework to generate the subtokens
of the method name.
7.2 Neural Machine Translation
Neural Machine Translation (NMT) [ 45] is an end-to-end learning
approach for automated translation. In recent years work of NMT
is largely based on encoder-decoder architecture [ 11], where the
encoder maps an input sequence of words ğ‘¥=(ğ‘¥1,...,ğ‘¥ ğ‘›)to a se-
quenceofcontinuousrepresentations ğ‘§=(ğ‘§1,...,ğ‘§ ğ‘›).Givenğ‘§,the
decoderthengeneratesasequenceofoutputwords ğ‘¦=(ğ‘¦1,...,ğ‘¦ ğ‘š)
one token at a time, hence modeling the conditional probability:
ğ‘(ğ‘¦1,...,ğ‘¦ ğ‘š|ğ‘¥1,...,ğ‘¥ ğ‘›). The encoder-decoder architecture has been
applied across many SE seq2seq tasks, including code summariza-
tion[5,19],methodnamerecommendation[ 7,34],codegeneration
[37,43],programtranslation [ 14], etc.Different neuralnetworks
can be used in the encoder and decoder. Code2seq [ 6] employs a
bi-directional LSTM to encode the AST paths then averages the
representations of all the paths as the final representation of theprogram encoder, and employs another LSTM as the decoder togenerate the output (method name or code summarization). Hu
etal.[19]useRNNforbothencoderanddecoderforcodecomment
generation task. Allamanis et al . [5]employ CNN to encode the
code snippet and use GRU as decoder to generate the tokens of the
methodname.Fernandesetal .[15]employGNNastheencoderand
LSTMasthedecoderforarangeofsummarizationtasks.Ahmad
etal.[3]usetransformernetworkforboththeencoderanddecoder
in code summarization task.
7.3 Method Name Recommendation
Recommendingmeaningfulandconsistentmethodnamesisimpor-tantforensuringreadabilityandmaintainabilityofprograms.Many
approaches have been introduced to suggest succinct names for
methods[ 5,7,34],wheredifferentmodelarchitecturesandmethod
contextsareconsidered.Inthissection,wesummarizerelatedwork
on method name recommendation from the following two aspects.
7.3.1 Models. Suzuki et al . [38]proposed an N-gram based ap-
proach to evaluate the comprehensibility of method names andsuggest comprehensible method names. Liu et al
. [28]follow an
information retrieval (IR) method with the motivation that two
methods with similar bodies should have similar names. They use
paragraphVectorandConvolutionalNeuralNetworkstoproduce
the vector representations of method names and bodies, respec-
tively.Theycomparedthesimilarityofthenamesretrievedfrom
themethodbodyvectorspaceandthemethodnamevectorspace
to identify the inconsistent method names. For the inconsistentnames, they use the names of methods whose bodies are similar
to the body of the input method to suggest the new method name.
However, methods with the same bodies can still have different
namessincetheyareindifferentprojectsandareunderdifferent
contexts. Besides, the IR-based approach cannot generate a new
name that it has not seen before. Another kind of researches based
on NMT models, where encoder-decoder framework is used to en-
code themethodbodies andgenerate themethod names[ 5,7,34].
Allamanis et al . [5]built a convolutional attentional network to
extract local features of the subtoken sequence from the methodbody, and then use these features to suggest names for methods.
Alon et al . [7]design attention-based neural network to encode
theASTpathsintovectors,andbasedonthepathrepresentation
to makepredictions on themethodâ€™s name. ZÃ¼gneret al. [48]pro-
posed Code Transformer, a Transformer-based language-agnostic
code representation model. They combined distances computed on
structure and context in the self-attention operation, which can
learnjointlyfromthestructureandcontextofprogramsrelyingon
language-agnosticfeatures. Theyappliedtheirrepresentationsto
thetaskofmethodnamesuggestion.Nguyenetal .[34]proposedan
RNN-based seq2seq approach to recommend method names and to
detectmethodnameinconsistencies.Theytaketheprogramentities
in the method body and enclosing class name as the input. Li et al .
[24]also developed an RNN-based seq2seq approach DeepName
for method name consistency checking and suggestion, which ex-
tended the contexts by considering the internal context, the caller
and callee contexts, sibling context, and enclosing context.
7.3.2 Method Contexts. Different method contexts are taken into
accountformethodnamerecommendation.Mostoftheresearch
only focused on exploiting the features from the method body,
wherethetokensequencesor ASTsofthemethodbodyaretaken
as the inputs. Allamanis et al . [5]considered the token sequence
from the method body and built a convolutional attentional net-
work to extract the features from the context. Alon et al . [7], Alon
etal.[6],ZÃ¼gneretal .[48],andPengetal .[36]consideredtheAST
paths extracted from the method body as the context, and made
predictionsonthemethodâ€™snamebasedonthepathrepresentation.Inadditiontothedatafromthemethodbody,manyresearchbegan
to include the information from a wide range of contexts. Nguyen
et al. [34]took the program entities in the method body and en-
closing class name as the input. Wang et al . [42]also considered
othermethodsintheprojectthathavecallrelationswiththetarget
method. Li et al . [24]further extended the contexts by considering
the internal context, the caller and callee contexts, sibling context,
and enclosing context. Inspired by these approaches, we further
considered the nested scopes of the project and the documentation
ofthemethodbyextractingtheproject-specificanddocumentation
context, which can help for suggesting accurate method names.
8 CONCLUSION
Inthispaper,weproposeGTNM,aglobalmethodnamesuggestion
approach, which considers contexts of different levels, including
localcontext,project-specificcontext,andthedocumentationofthetargetmethod.Weemployatransformer-basedseq2seqframework
to generate the method names, which uses the attention mecha-
nism to allow the model attending to different level contexts when
generatingthenames.TheexperimentalresultsonJavamethods
show that our model has a substantial improvement over baseline
models.
ACKNOWLEDGMENTS
This research is supported by the National Key R&D Program of
ChinaunderGrantNo.2020AAA0109400,andtheNationalNatural
Science Foundation of China under Grant Nos. 62072007, 62192733.
1304
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA Liu, et al.
REFERENCES
[1]SurafelLemmaAbebe,SoniaHaiduc,PaoloTonella,andAndrianMarcus.2011.
The effect of lexicon bad smells on concept location in source code. In 2011 IEEE
11th International Working Conference on Source Code Analysis and Manipulation.
Ieee, 125â€“134.
[2]SurafelLemmaAbebe,SoniaHaiduc,PaoloTonella,andAndrianMarcus.2011.
TheEffectofLexiconBadSmellsonConceptLocationinSourceCode.In 11th
IEEE Working Conference on Source Code Analysis and Manipulation, SCAM 2011,
Williamsburg, VA, USA, September 25-26, 2011. IEEE Computer Society, 125â€“134.
https://doi.org/10.1109/SCAM.2011.18
[3]WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and
Joel R. Tetreault (Eds.). Association for Computational Linguistics, 4998â€“5007.
https://doi.org/10.18653/v1/2020.acl-main.449
[4]Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gestingaccuratemethodandclassnames.In Proceedingsofthe201510thJoint
MeetingonFoundationsofSoftwareEngineering,ESEC/FSE2015,Bergamo,Italy,
August 30 - September 4, 2015, Elisabetta Di Nitto, Mark Harman, and Patrick
Heymans (Eds.). ACM, 38â€“49. https://doi.org/10.1145/2786805.2786849
[5]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional
AttentionNetworkforExtremeSummarizationofSourceCode.In Proceedingsof
the33ndInternationalConferenceonMachineLearning,ICML2016,NewYorkCity,
NY,USA,June19-24,2016 (JMLRWorkshopandConferenceProceedings,Vol.48),
Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 2091â€“2100.
http://proceedings.mlr.press/v48/allamanis16.html
[6]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In 7th International Con-
ferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,May6-9,
2019. OpenReview.net. https://openreview.net/forum?id=H1gKYo09tX
[7]UriAlon,MeitalZilberstein,OmerLevy,andEranYahav.2019. code2vec:learning
distributedrepresentationsofcode. Proc.ACMProgram.Lang. 3,POPL(2019),
40:1â€“40:29. https://doi.org/10.1145/3290353
[8]Sven Amann, Hoan Anh Nguyen, Sarah Nadi, Tien N. Nguyen, and Mira Mezini.
2019. A Systematic Evaluation of Static API-Misuse Detectors. IEEE Trans.
SoftwareEng. 45,12(2019),1170â€“1188. https://doi.org/10.1109/TSE.2018.2827384
[9]Venera Arnaoudova, Laleh Mousavi Eshkevari, Massimiliano Di Penta, Rocco
Oliveto,GiulianoAntoniol,andYann-GaÃ«lGuÃ©hÃ©neuc.2014. REPENT:Analyzing
theNatureofIdentifierRenamings. IEEETrans.SoftwareEng. 40,5(2014),502â€“532.
https://doi.org/10.1109/TSE.2014.2312942
[10]VeneraArnaoudova,MassimilianoDiPenta,andGiulianoAntoniol.2016. Lin-
guistic antipatterns: what they are and how developers perceive them. Empir.
Softw. Eng. 21, 1 (2016), 104â€“158. https://doi.org/10.1007/s10664-014-9350-8
[11]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May
7-9,2015,ConferenceTrackProceedings,YoshuaBengioandYannLeCun(Eds.).
http://arxiv.org/abs/1409.0473
[12] Kent Beck. 2007. Implementation patterns. Pearson Education.
[13]SimonButler,MichelWermelinger,YijunYu,andHelenSharp.2009. Relating
Identifier Naming Flaws and Code Quality: An Empirical Study. In 16th Working
ConferenceonReverseEngineering,WCRE2009,13-16October2009,Lille,France,
Andy Zaidman, Giuliano Antoniol, and StÃ©phane Ducasse (Eds.). IEEE Computer
Society, 31â€“35. https://doi.org/10.1109/WCRE.2009.50
[14]Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree Neural Net-
works for Program Translation. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information Processing Systems 2018,NeurIPS 2018, December 3-8, 2018, MontrÃ©al, Canada, Samy Bengio, Hanna M.
Wallach,HugoLarochelle,KristenGrauman,NicolÃ²Cesa-Bianchi,andRoman
Garnett (Eds.). 2552â€“2562. https://proceedings.neurips.cc/paper/2018/hash/
d759175de8ea5b1d9a2660e45554894f-Abstract.html
[15]Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Struc-tured Neural Summarization. In 7th International Conference on Learning Rep-
resentations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.OpenReview.net.
https://openreview.net/forum?id=H1ersoRqtm
[16]AbramHindle,EarlTBarr,MarkGabel,ZhendongSu,andPremkumarDevanbu.
2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122â€“131.
[17]Johannes C. Hofmeister, Janet Siegmund, and Daniel V. Holt. 2017. Shorter iden-
tifier names take longer to comprehend. In IEEE 24th International Conference on
Software Analysis, Evolution and Reengineering, SANER 2017, Klagenfurt, Austria,
February20-24,2017,MartinPinzger,GabrieleBavota,andAndrianMarcus(Eds.).
IEEE Computer Society, 217â€“227. https://doi.org/10.1109/SANER.2017.7884623
[18]EinarW.HÃ¸standBjarteM.Ã˜stvold.2009. DebuggingMethodNames.In ECOOP
2009 - Object-Oriented Programming, 23rd European Conference, Genoa, Italy, July
6-10, 2009. Proceedings (Lecture Notes in Computer Science, Vol. 5653), Sophia
Drossopoulou(Ed.).Springer,294â€“317. https://doi.org/10.1007/978-3-642-03013-0_14
[19]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment
generation.In Proceedingsofthe26thConferenceonProgramComprehension,ICPC
2018, Gothenburg, Sweden, May 27-28, 2018, Foutse Khomh, Chanchal K. Roy, and
Janet Siegmund (Eds.). ACM, 200â€“210. https://doi.org/10.1145/3196321.3196334
[20]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment
generationwithhybridlexicalandsyntacticalinformation. EmpiricalSoftware
Engineering 25, 3 (2020), 2179â€“2217.
[21]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
Andrea Janes. 2020. Big code != big vocabulary: open-vocabulary models for
source code. In ICSE â€™20: 42ndInternational Conference on Software Engineering,
Seoul, South Korea, 27 June - 19 July, 2020 , Gregg Rothermel and Doo-Hwan Bae
(Eds.). ACM, 1073â€“1085. https://doi.org/10.1145/3377811.3380342
[22]DawnJ.Lawrie,ChristopherMorrell,HenryFeild,andDavidW.Binkley.2006.
Whatâ€™sinaName?AStudyofIdentifiers.In 14thInternationalConferenceonPro-
gramComprehension(ICPC2006),14-16June2006,Athens,Greece.IEEEComputer
Society, 3â€“12. https://doi.org/10.1109/ICPC.2006.51
[23]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural modelfor generating natural language summaries of program subroutines. In 2019
IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE,
795â€“806.
[24]Yi Li, Shaohua Wang, and Tien N Nguyen. 2021. A Context-based Automated
Approach for Method Name Consistency Checking and Suggestion. In 2021
IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,
574â€“586.
[25]Ben Liblit, Andrew Begel, and Eve Sweetser. 2006. Cognitive Perspectiveson the Role of Naming in Computer Programs. In Proceedings of the 18th An-
nual Workshop of the Psychology of Programming Interest Group, PPIG 2006,
Brighton,UK,September7-8,2006.PsychologyofProgrammingInterestGroup,
11. http://ppig.org/library/paper/cognitive-perspectives-role-naming-computer-
programs
[26]FangLiu,GeLi,BolinWei,XinXia,ZhiyiFu,andZhiJin.2020. ASelf-AttentionalNeural Architecture for Code Completion with Multi-Task Learning. In ICPC â€™20:
28thInternationalConferenceonProgramComprehension,Seoul,RepublicofKorea,
July 13-15, 2020. ACM, 37â€“47. https://doi.org/10.1145/3387904.3389261
[27]FangLiu,GeLi,YunfeiZhao,andZhiJin.2020. Multi-taskLearningbasedPre-
trainedLanguageModelforCodeCompletion.In 35thIEEE/ACMInternational
ConferenceonAutomatedSoftwareEngineering,ASE2020,Melbourne,Australia,
September 21-25, 2020. IEEE, 473â€“485. https://doi.org/10.1145/3324884.3416591
[28]KuiLiu,DongsunKim,TegawendÃ©F.BissyandÃ©,Tae-youngKim,KisubKim,Anil
Koyuncu,SuntaeKim,andYvesLeTraon.2019. Learningtospotandrefactor
inconsistent method names. In Proceedings of the 41st International Conference on
SoftwareEngineering,ICSE2019,Montreal,QC,Canada,May25-31,2019,JoanneM.
Atlee, Tevfik Bultan, andJon Whittle (Eds.). IEEE / ACM, 1â€“12. https://doi.org/
10.1109/ICSE.2019.00019
[29]Guillermo Macbeth, Eugenia Razumiejczyk, and RubÃ©n Daniel Ledesma. 2011.
Cliffâ€™s Delta Calculator: A non-parametric effect size program for two groups of
observations. Universitas Psychologica 10, 2 (2011), 545â€“555.
[30]RobertC.Martin.2009. CleanCode-aHandbookofAgileSoftwareCraftsmanship.
Prentice Hall. http://vig.pearsoned.com/store/product/1,1207,store-12521_isbn-
0132350882,00.html
[31]PaulWMcBurneyandCollinMcMillan.2015. Automaticsourcecodesumma-
rization of contextfor java methods. IEEE Transactionson Software Engineering
42, 2 (2015), 103â€“119.
[32]SteveMcConnell.2004. Codecomplete-apracticalhandbookofsoftwareconstruc-
tion, 2nd Edition. Microsoft Press. https://www.worldcat.org/oclc/249645389
[33]KawserWazedNafi,TonnyShekhaKar,BananiRoy,ChanchalK.Roy,andKevinA.Schneider.2019. CLCDSA:CrossLanguageCodeCloneDetectionusingSyntacti-
cal Features and API Documentation. In 34th IEEE/ACM International Conference
on Automated Software Engineering, ASE 2019, San Diego, CA, USA, November
11-15, 2019. IEEE, 1026â€“1037. https://doi.org/10.1109/ASE.2019.00099
[34]Son Nguyen, Hung Phan, Trinh Le, and Tien N. Nguyen. 2020. Suggesting
naturalmethodnamestochecknameconsistencies.In ICSEâ€™20:42ndInternational
Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020,
GreggRothermelandDoo-HwanBae(Eds.).ACM,1372â€“1384. https://doi.org/
10.1145/3377811.3380926
[35]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-tomated whitebox testing of deep learning systems. In proceedings of the 26th
Symposium on Operating Systems Principles. 1â€“18.
[36]HanPeng,GeLi,WenhanWang,YunfeiZhao,andZhiJin.2021. IntegratingTree
PathinTransformerforCodeRepresentation. AdvancesinNeuralInformation
Processing Systems 34 (2021).
[37]Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu Zhang. 2019. AGrammar-Based Structural CNN Decoder for Code Generation. In The Thirty-
Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First
Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth
AAAISymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2019,Honolulu, Hawaii, USA, January 27 - February 1, 2019. AAAI Press, 7055â€“7062.
1305
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. Learning to Recommend Method Names with Global Context ICSE â€™22, May 21â€“29, 2022, Pittsburgh, PA, USA
https://doi.org/10.1609/aaai.v33i01.33017055
[38]TakayukiSuzuki,KazunoriSakamoto,FuyukiIshikawa,andShinichiHoniden.
2014. Anapproachforevaluatingandsuggestingmethodnamesusingn-gram
models. In 22nd International Conference on Program Comprehension, ICPC 2014,
Hyderabad, India, June 2-3, 2014, Chanchal K. Roy, Andrew Begel, and Leon
Moonen (Eds.). ACM, 271â€“274. https://doi.org/10.1145/2597008.2597797
[39]Armstrong A. Takang, Penny A. Grubb, and Robert D. Macredie. 1996. The
effects of comments and identifier names on program comprehensibility: anexperimental investigation. J. Program. Lang. 4, 3 (1996), 143â€“167. http://
compscinet.dcs.kcl.ac.uk/JP/jp040302.abs.html
[40]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018. Deeptest:Automated
testing of deep-neural-network-driven autonomous cars. In Proceedings of the
40th international conference on software engineering. 303â€“314.
[41]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention isAll you Need. In Advances in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, SamyBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and RomanGarnett (Eds.). 5998â€“6008. https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
[42]Shangwen Wang, Ming Wen, Bo Lin, and Xiaoguang Mao. 2021. Lightweight
globalandlocalcontextsguidedmethodnamerecommendationwithpriorknowl-
edge.InProceedingsofthe29thACMJointMeetingonEuropeanSoftwareEngi-
neeringConferenceandSymposiumontheFoundationsofSoftwareEngineering.
741â€“753.
[43]Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a
Dual Task of Code Summarization. In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Information Processing Systems 2019,NeurIPS2019,December8-14,2019,Vancouver,BC,Canada ,HannaM.Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and
RomanGarnett(Eds.).6559â€“6569. https://proceedings.neurips.cc/paper/2019/
hash/e52ad5c9f751f599492b4f087ed7ecfc-Abstract.html
[44]FrankWilcoxon.1992. Individualcomparisonsbyrankingmethods. In Break-
throughs in statistics. Springer, 196â€“202.
[45]Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner,ApurvaShah,MelvinJohnson,XiaobingLiu,LukaszKaiser,Stephan
Gouws,YoshikiyoKato,TakuKudo,HidetoKazawa,KeithStevens,GeorgeKurian,
Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,
Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Googleâ€™s
Neural Machine Translation System: Bridging the Gap between Human and
MachineTranslation. CoRRabs/1609.08144(2016). arXiv:1609.08144 http://arxiv.
org/abs/1609.08144
[46]JianZhang,XuWang,HongyuZhang,HailongSun,KaixuanWang,andXudongLiu.2019.Anovelneuralsourcecoderepresentationbasedonabstractsyntaxtree.
InProceedings ofthe41st InternationalConference onSoftware Engineering,ICSE
2019,Montreal,QC,Canada,May25-31,2019,JoanneM.Atlee,TevfikBultan,and
JonWhittle(Eds.).IEEE/ACM,783â€“794. https://doi.org/10.1109/ICSE.2019.00086
[47]XiyueZhang,XiaofeiXie,LeiMa,XiaoningDu,QiangHu,YangLiu,JianjunZhao,andMengSun.2020. Towardscharacterizingadversarialdefectsofdeeplearning
software from the lens of uncertainty. In 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE). IEEE, 739â€“751.
[48]Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan
GÃ¼nnemann. 2021. Language-Agnostic Representation Learning of Source Code
from Structure and Context. In ICLR (Poster). OpenReview.net.
1306
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 08:26:40 UTC from IEEE Xplore.  Restrictions apply. 