On the Real-World Effectiveness of Static Bug
Detectors at Finding Null Pointer Exceptions
David A. Tomassi
University of California, Davis
United States of America
datomassi@ucdavis.eduCindy Rubio-Gonz ´alez
University of California, Davis
United States of America
crubio@ucdavis.edu
Abstract —Static bug detectors aim at helping developers to
automatically ﬁnd and prevent bugs. In this experience paper,
we study the effectiveness of static bug detectors at identifyingNull Pointer Dereferences or Null Pointer Exceptions (NPEs).NPEs pervade all programming domains from systems to webdevelopment. Speciﬁcally, our study measures the effectiveness ofﬁve Java static bug detectors: CheckerFramework, E
RADICATE ,
INFER ,N ULL AWAY , and S POT BUGS . We conduct our study on
102 real-world and reproducible NPEs from 42 open-source
projects found in the B UGSW ARM and D EFECTS 4J datasets. We
apply two known methods to determine whether a bug is found bya given tool, and introduce two new methods that leverage stacktrace and code coverage information. Additionally, we provide acategorization of the tool’s capabilities and the bug characteristicsto better understand the strengths and weaknesses of the tools.Overall, the tools under study only ﬁnd 30 out of 102 bugs(29.4%), with the majority found by E
RADICATE . Based on our
observations, we identify and discuss opportunities to make thetools more effective and useful.
Index T erms—static bug detectors, null pointer exceptions, null
pointer dereferences, bug ﬁnding, BugSwarm, Defects4J, Java
I. I NTRODUCTION
Defects in software are a common and troublesome fact
of programming. Software defects can cause programs to
crash, lose or corrupt data, suffer from security vulnerabilities,among other problems. Depending on the application domain,undesirable behavior can range from poor user experience tomore severe consequences in mission critical applications [44].Testing to uncover such software defects remains one of themost expensive tasks in the software development cycle [31].
There is a need for both precision and scalability when
ﬁnding defects in real-world code. Furthermore, in an effortto increase their applicability, static bug detectors are oftendesigned to target a large variety of software bugs. Manystatic bug detectors [2, 5, 7, 9, 10, 13–16] are currently beingdeveloped in industry and academia. Even with many tools tochoose from, developers have some hesitation in using staticbug detectors for a variety of reasons such as large number ofbug warnings, high false positive rates, and inadequate warningmessages [18, 26].
Previous studies have evaluated static bug detectors through
various metrics: number of warnings [35], number of falsenegatives [38], tool performance [35], and recall [20, 41].These studies have focused on popular tools that identifya large number of bug patterns, and their conclusions aredrawn with respect to the overall bug-ﬁnding capabilities ofthe tools. In contrast, this paper evaluates static bug detectorswith respect to their effectiveness at ﬁnding a common andserious kind of bug: Null Pointer Dereferences or Null PointerExceptions (NPEs).
NPEs pervade all programming domains from systems
software to web development. For instance, as of August2021, there are over 1,900 CVEs (Common Vulnerabilities andExposures) that involve NPEs [3]. One such CVE describesa denial of service attack in early versions of Java (1.3 and1.4) caused by crashing the Java Virtual Machine when callinga function with a null parameter [1]. In general, NPEs are
problematic in memory-unsafe and object-oriented languages.NPEs occur when either a pointer to a memory location or anobject is dereferenced while being uninitialized or explicitlyset to null. Depending on the programming language, NPEs
will result in either undeﬁned behavior or a runtime exception.
This experience paper evaluates recall of static bug detectors
with respect to a known set of real NPE bugs. The focus
on NPEs allows to present an in-depth study of differentapproaches to ﬁnd a same kind of bug, the characteristics ofreal-world NPEs, and the reasons that affect tool effectiveness.To the best of our knowledge, this is the ﬁrst study on the real-
world effectiveness of static bugs detectors at ﬁnding NPEs.
There are two orthogonal approaches to ﬁnding or prevent-
ing NPEs, which make use of either a static bug detector ora type-based null safety checker. The former uses dataﬂowanalysis [6, 10, 23, 29, 30, 32, 34] to ﬁnd null dereferences.
Such approaches mainly differ on the complexity of theiranalyses. Some favor analysis scalability at the expense ofmissing real bugs and/or producing numerous false positives,e.g., intra/interprocedural and ﬁeld sensitivity. The latter pre-
vents NPEs via a type system with null-related information
using dataﬂow analysis for type reﬁnement. The type checkerapproach has been adopted in recent years [4, 14, 19, 33].
We study two popular Java static bug detectors: I
NFER
[6, 15–17] and S POT BUGS [10], and three popular type-
based null safety checkers for Java: Checker Framework’sNullness Checker (CFN
ULLNESS ) [19, 33], E RADICA TE [4],
and N ULL AWAY [8, 14]. I NFER uses separation logic and
bi-abduction analysis [16] to infer pre/post conditions fromprocedures affecting memory. S
POT BUGS detects bugs based
on a predeﬁned set of bug patterns. CFN ULLNESS veriﬁes
2922021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000352021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678535
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
Fig. 1: Workﬂow for running tools, collecting reports, parsing results, and analyzing data.
the absence of NPEs via type checking nullable expression
dereferences and assignments. E RADICA TE is a type checker
that performs ﬂow-sensitive analysis to ﬁnd possible nulldereferences. Finally, N
ULL AWAY uses dataﬂow analysis to
type check nullability in procedures and class ﬁelds.
In this study, we consider 102 real-world and reproducible
NPEs found across 42 popular open-source Java projects. 76of these NPEs belong to the B
UGSWA R M dataset [42] while
the remaining 26 are from D EFECTS 4J [27]. For each NPE,
both datasets provide buggy and ﬁxed versions of the programsalong with scripts for compilation and testing. Furthermore,each program has a failing test due to an NPE. This makes boththe B
UGSW ARM and D EFECTS 4J datasets good candidates for
this study; we want to run existing static bug detectors and typecheckers on these programs to determine their effectiveness atdetecting and preventing real NPEs.
The ﬁrst challenge is to determine whether a tool ﬁnds or
prevents a speciﬁc NPE bug. Tools may report the programlocation at which the null dereference occurs, or simply thelocation where the null value originates, which can be far
from the dereference. The latter is particularly difﬁcult toassociate with the bug ﬁx, which is often applied closer to thedereference site. Another difﬁculty lies in the large number ofwarnings to inspect. On average a tool produces from 122 to
1,307 bug w arnings per program (in our dataset).
Previous work has partially automated the process of map-
ping bugs to warnings based on static information such as the
code difference (diff) between buggy and ﬁxed versions [20,38], and by comparing the warnings produced for each versionof the program [20]. In this paper, we observe that dynamic
information can also be leveraged when an input exposing theNPE bug is available, which is the case for all the bugs in ourdataset. We present two new mapping methods for NPEs thatuse (1) stack trace information, and (2) code coverage of teststhat fail due to NPEs. Our experimental evaluation shows thatthese methods complement previous approaches.
We run CFN
ULLNESS ,E RADICA TE ,INFER ,N ULL AWAY ,
and S POT BUGS on our dataset of 102 real NPEs. We ﬁnd
that the tools produce a large number of warnings, includingover 500,000 NPE warnings across all programs. We applyexisting approaches, and our new methods, to identify thewarnings that describe the bugs under study. Ultimately, weﬁnd that the tools detect only 30 out of 102 bugs ( 29.4%),with E
RADICA TE ﬁnding the majority of these.
The second challenge is to understand the reasons why
tools fail to ﬁnd NPEs to identify opportunities to improvetheir real-world effectiveness. This requires understanding thecapabilities of the tools under study as well as the charac-teristics of the NPE bugs in our dataset. First, we conducta detailed analysis of the tools’ capabilities with respect towell-known program-analysis properties (e.g., ﬂow sensitivity,context sensitivity, etc.), and we identify common sources ofunsoundness. This process required us to manually inspect thesource code of the tools and write tests. All of our ﬁndingswere later conﬁrmed by tool developers. Second, we manuallyinspect and categorize each NPE bug in the dataset withrespect to the nature of the dereference and its context. Basedon the tool results, and the tool and bug characterizations,we identify several open opportunities to improve static bugdetectors that ﬁnd NPEs.
The contributions of this paper are:
•We present two new methods that leverage dynamic infor-mation to map tool warnings to NPE bugs (Section II).
•We provide a categorization of the tool’s capabilities andthe bug characteristics to better understand the strengthsand weaknesses of the tools under study (Section III).
•We evaluate CFN ULLNESS ,E RADICA TE ,INFER ,N ULL -
AWAY , and S POT BUGS on a collection of 102 NPEs, from
which only 29.4% of NPE bugs are detected (Section IV).
•We discuss the capabilities and limitations of each tool,and provide future directions for improving their real-world effectiveness (Section V).
II. M
ETHODOLOGY
Here we describe the benchmark and tool selection, and
the methodology to determine the effectiveness of the tools atﬁnding NPEs. Figure 1 shows the main steps of our approach.
A. Benchmark Selection
Our study focuses on Null Pointer Exceptions (NPEs). We
consider bugs from the B
UGSWA R M and D EFECTS 4J datasets,
both of which provide a bug classiﬁcation based on runtime
exceptions. Our selection criteria is: (1) the bug is due to anNPE, (2) there is a failing test due to the NPE, and (3) codecoverage can be measured. Additionally, we control for uniquebuilds when selecting B
UGSW ARM bugs. Our ﬁnal dataset
293consists of 76 NPE bugs from the B UGSWA R M dataset and
26 from D EFECTS 4J. The B UGSW ARM NPE bugs belong
to 32 Java projects hosted on GitHub that use the Maven
build system, while the D EFECTS 4J bugs belong to 10 Java
projects that use the Ant build system. Note that all NPEs arereproducible, i.e., one can run the programs and observe a NullPointer Exception being thrown. Furthermore, we manually
veriﬁed that each NPE bug in our study is an actual NPE, i.e.,a null object is eventually dereferenced. Each NPE instanceconsists of the source code that contains the bug, the sourcecode that ﬁxes the bug, and scripts to compile and test.
B. Tool Selection and Conﬁguration
We conducted an extensive search for tools that ﬁnd or
prevent NPE bugs in Java projects. We focused on publicly
available tools that are standalone and under active devel-opment. Out of nine tools, four [29, 30, 32, 34] did notsatisfy at least one of these requirements. In this paper westudy the remaining ﬁve tools: CFN
ULLNESS ,E RADICA TE ,
INFER ,N ULL AWAY , and S POT BUGS . Note that I NFER and
SPOT BUGS ﬁnd a large variety of bugs in addition to NPE
bugs. CFN ULLNESS ,E RADICA TE , and N ULL AWAY exclu-
sively specialize in NPEs. Below we describe each tool.
a)CFN ULLNESS :A type checker written using the
Checker Framework, which is available as a compiler plu-gin. CFN
ULLNESS works with nullness type annotations,
@Nullable and@NotNull, and looks for violations in their
use. Namely looking for dereferences on @Nullable expres-
sions and for @Nullable-value assignments to @NotNull
variables. CFN ULLNESS produces compile-time warnings. We
run CFN ULLNESS using its default conﬁguration.
b)ERADICA TE :A type checker part of the I NFER static-
analysis suite. E RADICA TE type checks for @Nullable
annotations in Java programs by performing a ﬂow-sensitiveanalysis to propagate null-related information through assign-ments and calls. E
RADICA TE produces warnings for accesses
that could lead to an NPE. E RADICA TE produces a report
in JSON format that provides the stack trace, severity, andsource location associated with each bug detected. We run
E
RADICA TE using its default conﬁguration.
c)INFER :A static-analysis tool developed by Facebook
that ﬁnds a variety of bugs in Java, C/C++, and Objective-C programs. I
NFER uses bi-abduction analysis to ﬁnd bugs
including deadlocks, memory leaks, and null pointer deref-erences. Similar to E
RADICA TE ,INFER produces a report in
JSON format that provides the stack trace, severity, and buglocation. We use I
NFER ’s default setting, which runs the bi-
abduction analysis.
d)NULL AWAY :A type checker for Java developed by
Uber that applies various AST-based checkers to ﬁnd NPEbugs. N
ULL AWAY is available as a plugin for Maven and
Gradle. We use N ULL AWAY ’s default conﬁguration, which
assumes that unannotated method parameters, return values,and class ﬁelds are not null. In such cases, the tool produces
a warning when it is found that any of those locations couldhold a null value. The user can add explicit @Nullable
annotations to obtain more precise results.
e)S
POT BUGS :SPOT BUGS applies pattern matching and
limited dataﬂow analysis to ﬁnd a large variety of bugssuch as inﬁnite recursion, integer overﬂows, and null pointerdereferences. The tool produces an XML report listing bugwarnings that include class name, method name, severity, andline numbers associated with the identiﬁed bug. S
POT BUGS
is available as a plugin for a variety of build systems suchas Ant, Gradle, and Maven. We run S
POT BUGS with effort
level “max”, which indicates that S POT BUGS performs its
interprocedural analysis. Also, we use two different error con-ﬁdence threshold settings “low” and “high” (“low” conﬁdencethreshold may report a higher number of false positives).
C. Analysis of NPE Warnings
A challenge in this study is to determine whether a tool ﬁnds
or prevents a speciﬁc NPE bug. In the case of NPEs, tools
may report the program location at which the null dereferenceoccurs, or simply report the location where the null valueoriginates, which can be far from the dereference. The latteris particularly difﬁcult to associate with the bug ﬁx, which isoften applied closer to the dereference site.
We consider four approaches for mapping bug warnings to
actual bugs in the source code, i.e., determine whether a toolﬁnds a given bug under study. Two of these approaches havebeen used in previous work: the C
ODE DIFFMETHOD [20, 38]
and the R EPORT DIFF METHOD [20]. We explore two new
approaches, which we refer to as the S TACK TRACE METHOD
and the C OVERAGE METHOD .
Figure 2 shows an example of an NPE found in the
OpenPnP1GitHub project as part of the BugSwarm dataset.2
Method saveDebugImage is called on Line 8 of ﬁle
OpenCvVisionProvider.java (see Figure 2b), where
argument debugMat isnull. Method saveDebugImage
in ﬁle OpenCvUtils.java callstoBufferedImage on
Line 6 (see Figure 2a), passing in null, which is then
dereferenced on Line 11. The code highlighted (in green)represents the patch to ﬁx the NPE. Figure 2e shows the stacktrace, and Figures 2c and 2d show the warnings produced by
S
POT BUGS and I NFER , respectively.
1)CODE DIFF METHOD :This method takes as input the
set of warnings reported for the buggy program and the set ofpatches from the GitHub code diff.
3The analysis focuses on
NPE bug warnings, and checks whether the source location ofthese warnings overlaps with the lines changed in the patches.However, this is based on an over-approximation; the lowestand highest line numbers associated with the patch in eachchanged ﬁle are considered.
4If an overlapping line is found,
then the warning is considered a bug candidate. We manually
examine bug candidates to verify their validity.
1https://github.com/openpnp/openpnp
2BugSwarm image tag: openpnp-openpnp-213669200.
3A GitHub code diff may consist of several patch fragments.
4Previous work has also added a conﬁgurable number of lines before the
starting point and after the ending point of the line range [20].
294@@ -2,6 +2,9 @@ public synchronized static Mat ...
22
33}4455public static void saveDebugImage(..., Mat mat) {
6 +if(mat == null) {
7 + return;
8 +}
69 saveDebugImage(...,OpenCvUtils.toBufferedImage(mat));
71 0 ...
81 1 }
91 210 13 public static BufferedImage toBufferedImage(Mat m) {
11 14 if(m.type() == CvType.CV_8UCI) {...} // NPE!
12 15 }
(a) GitHub diff in ﬁle OpenCvUtils.java.
1 1public getTemplateMatches(BufferedImage template) {
2 2 ...
3 3 Mat debugMat = null;
4 4 if(LogUtils.isDebugEnabled()) {
5 5 debugMat = imageMat.clone();
6 6 }
7 7 ...
8 8 OpenCvUtils.saveDebugImage(..., debugMat);
9 9 }
(b) Null origin in ﬁle OpenCvVisionProvider.java.<BugInstance rank="8" abbrev="NP" category="
CORRECTNESS" priority="2" type="
NP_NULL_PARAM_DEREF">
<Method classname="org.openpnp.util.
OpenCvUtils" name="saveDebugImage">
<SourceLine classname="org.openpnp.util.
OpenCvUtils" start="6" end="6"
sourcefile="OpenCvUtils.java"/>
(c) SpotBugs XML report.
{"bug_class":"PROVER",
"kind":"ERROR","bug_type":"NULL_DEREFERENCE","qualifier":"object ‘debugMat‘ last assigned
on line 3 could be null and is
dereferenced by call to ‘saveDebugImage(...)‘ at line 8.",
"file":"OpenCvVisionProvider.java","severity":"HIGH",
...
}
(d) Infer JSON report.
java.lang.NullPointerException
at org.openpnp.util.OpenCvUtils.toBufferedImage(OpenCvUtils.java:11)at org.openpnp.util.OpenCvUtils.saveDebugImage(OpenCvUtils.java:6)at org.openpnp.machine.reference.vision.OpenCvVisionProvider.getTemplateMatches(
OpenCvVisionProvider.java:8)
...
(e) Stack trace for buggy program.
Fig. 2: GitHub diff, stack trace, SpotBugs XML report, and Infer JSON report for an NPE found by S POT BUGS LT and I NFER .
Consider the patch in Figure 2a. The line at the top
(starting with @@) indicates that the patch includes orig-
inal lines 2 through 6, and new lines 2 through 9 from
ﬁleOpenCvUtils.java. Therefore, the approximated line
range is 2 through 9 for the buggy program, i.e., theprogram before the ﬁx. The S
POT BUGS report (see Fig-
ure 2c) includes the XML tag SourceLine: Line 6 of ﬁle
OpenCvUtils.java. This location lies within the linerange 2–9, thus the method correctly collects this warningas a bug candidate. On the other hand, even though I
NFER
(see Figure 2d) successfully ﬁnds the bug, the C ODE DIFF
METHOD approach fails to map the warning because the report
does not include lines close to the ﬁx. In this case, using codediff information is not effective.
2)R
EPORT DIFF METHOD :This method uses the set of
bug warnings of the buggy program, and the set of warningsof its ﬁxed version. The algorithm searches for NPE bug warn-ings that are only reported for the buggy program. The intuition
is that the warning that describes the bug of interest shouldnot be present in the bug report of the ﬁxed program. Usingthis method, both S
POT BUGS and I NFER are determined to
have found the bug from Figure 2. This method is convenientbecause it only requires two bug reports. However, the absenceof a bug warning in the ﬁxed program does not necessarilymean that the bug of interest was found. The code changecould have introduced “noise” that leads the tool to concludethat an unrelated bug warning is no longer a problem. We
observe that this occurs often in practice (see Section IV-B).
3)S
TACK TRACE METHOD :This approach requires the set
of bug warnings of the buggy program, and the stack trace(s)produced when running the buggy program. As with previousmethods, this approach only considers warnings related to NPEbugs. For each NPE warning, the algorithm retrieves the ﬁleand line number(s) associated with the warning, and checkswhether those are included in the stack trace. If so, the warningis classiﬁed as a bug candidate.
Consider again the example from Figure 2. The
S
POT BUGS report (Figure 2c) mentions Line 6 in ﬁle
OpenCVUtils.java. The report pinpoints that there is anull parameter in a recursive call to saveDebugImage,
which could result in an NPE. On the other hand, the I
N-
FER report (Figure 2d) lists a warning associated with ﬁle
OpenCvVisionProvider.java on Line 8. The call to
saveDebugImage in method getTemplateMatches is
passed debugMat as argument, which could be null and
result in an NPE. Note that I NFER refers to a lower stack
frame than S POT BUGS , but the S TACK TRACE METHOD
successfully maps both reports to the same bug because both
295locations can be found in the stack trace (Figure 2e).
The S TACK TRACE METHOD takes advantage of the nature
of NPE bugs and their presence in the stack trace. Because
NPE bugs correspond to Null Pointer Exceptions, the call stackis given at the time the exception occurs. This information isa valuable resource that leads to a more natural bug mappingthan previous methods. However, this method requires anexecutable buggy program and a reproducible NPE. Also, thismethod provides a line in the stack trace that can be mappedto a bug warning, however, this does not necessarily meanthat the tool found the correct dereference; there are longdereference chains that may be associated with the same line.Thus, as with previous methods, it is necessary to manuallyverify that the trace indeed matches the context of the NPEwarning. We consider all available sources of information suchas source code and code diff during manual inspection.
4)C
OVERAGE METHOD :This method is a general version
of the S TACK TRACE METHOD , but it includes all lines
executed by the test that triggers the NPE. The input is theset of NPE warnings of the buggy program, and the linescovered (executed) by a test case that fails due to an NPE.The approach determines if the source location given in awarning is covered, in which case the warning is added tothe set of bug candidates. This captures the scenario wherethe location of an NPE warning is far away from the actualdereference, which is particularly useful when analyzing thewarnings produced by type checkers such as N
ULL AWAY and
ERADICA TE . The assumption is that even if the NPE warning
and the actual dereference are located far away from eachother, both source locations will be part of the execution trace.For example, consider a case in which a ﬁeld is set to null
in a constructor and the ﬁeld is dereferenced in some method.Type checkers may produce a warning related to setting theﬁeld to null, but not a warning describing the dereference
itself. However, in this case, both source locations will bepart of the execution trace. Note that this approach requiresthe existence of a failing test that triggers the NPE, and theability to execute the test. Both requirements are met for ourdataset. As with other methods, we manually inspect all bugcandidates to determine their validity.
III. B
UG AND TOOL CHARACTERIZA TION
A fundamental step in evaluating the effectiveness of static
bug detectors is to understand their capabilities, and whetherreal-world bugs possess the desired characteristics to be de-tected. In this section, we characterize the dataset of realNPEs as well as the tools under study with respect to theirapproaches to ﬁnd NPEs. We describe our methodology andresults, which will be critical in Section IV to determinewhether a given NPE can be found by the tools.
First, we performed a manual categorization of all 102
NPEs to determine the root cause of the null pointer deref-erences. The categorization was performed separately by anauthor of this paper and two people external to the project.When in disagreement, the inspectors met to reach consensus.0 5 10 15 20 25 30 35FieldMethod Parameter
Method ReturnReﬂectionConcurrency
Third Party Library
Map-Like ObjectCollection-Like ObjectGenerics
1713
32132
8
13103
Number of BugsBug Classiﬁcation
Fig. 3: Bug Classiﬁcation Results
Using the source code, the GitHub code diff, and the build
log, we identiﬁed the origin of the null value, and its
dereference location. Based on this inspection, we identiﬁednine general categories of NPEs with respect to what is deref-erenced, and the context of the dereference. These categoriesalong with their counts can be found in Figure 3. Note thatan NPE can belong to multiple categories. The most commoncategories are when a method return value is dereferenced (32bugs) and when a ﬁeld is dereferenced (17 bugs).
As for the tool capabilities, we consider seven well-known
program analysis properties: (1) intraprocedural, (2) interpro-cedural, (3) ﬂow sensitive, (4) context sensitive, (5) ﬁeldsensitive, (6) object sensitive, and (7) path sensitive [11].
We identiﬁed seven common sources of unsoundness: (1)
handling of third-party libraries whose source code may notbe available, (2) impure methods that have side effects andare non-deterministic, (3) concurrency, (4) reasoning aboutdynamic dispatch, (5) dealing with code that uses reﬂection,(6) ﬁeld initialization after a constructor is called, and (7)generic parameters. Unsoundness can lead to false positives(incorrect bug warnings) and false negatives (missed bugs).
We studied CFN
ULLNESS ,E RADICA TE ,INFER ,N ULL -
AWAY , and S POT BUGS with respect to the above analysis
characteristics and sources of unsoundness. In this process,we manually inspected the source code and documentation ofthe tools, and we wrote kernel test programs that exhibiteddifferent categories of behaviors to conﬁrm tools’ capabilitiesand limitations. Table I shows the tool capabilities, and Table IIshows the sources of unsoundness for each tool. Below wedescribe our ﬁndings for each tool, which were conﬁrmed bythe corresponding developers.
a)CFN
ULLNESS :An ensemble of three checkers: (1)
an intraprocedural ﬂow-sensitive qualiﬁer inference for thenullness of a particular object, (2) initialization checking, and(3) map key checking. It assumes @NonNull for unannotated
code except for locals, and provides an analysis for iteratingover null collections and arrays. Additionally, CFN
ULLNESS
supports annotations to denote: (1) if a method has no side-effects or is deterministic, (2) the target of a reﬂection invo-cation, (3) and upper bounds of types for generic objects.
296TABLE I: Tool Capabilities Conﬁrmed by Developers. = has capabilities, = no capabilities, Partial = limited capabilities.
Tool Intraproc. Interproc. Field Sensitive Context Sensitive Object Sensitive Flow Sensitive Path Sensitive
CFN ULLNESS      
ERADICA TE  Partial   
INFER      
NULL AWAY  Partial Partial  N/A Partial Partial
SPOT BUGS  Partial Partial  Partial Partial
TABLE II: Sources of Unsoundness for the Tools. = is sound, = is unsound, Partial = unsound in some aspects.
Tool Third Party Libs. Impure Methods Concurrency Dynamic Dispatch Reﬂection Field Init. Generic Types
CFN ULLNESS      
ERADICA TE Partial   Partial 
INFER Partial Partial  Partial  
NULL AWAY     Partial 
SPOT BUGS      
b)ERADICA TE :An intraprocedural ﬂow-sensitive anal-
ysis for the propagation of nullability through variable assign-
ments and function calls. E RADICA TE also raises an alarm
for accesses to ﬁelds that have annotated nullability, howeverits ﬁeld initialization checker is subject to ongoing work.
E
RADICA TE ’s nullability annotations allow for the annotation
of methods, ﬁelds, and method parameters with @Nullable
annotations. As detailed in Table II, E RADICA TE provides
built-in models of the JDK and Android SDK and supportsuser-speciﬁed nullability signatures for other third-party li-braries, which helps mitigate false negatives.
c)I
NFER :An interprocedural analysis that supports
tracking object aliasing, side effects in methods, and dynamictypes of objects. All our tests were successful when running
I
NFER , showing that the tool is interprocedural and ﬁeld
sensitive. A caveat is that I NFER does not ﬁnd uninitialized
ﬁelds, but it can ﬁnd null dereferences to ﬁelds that have beeninitialized. As shown in Table II, I
NFER partially supports
third-party libraries via an internal model of the JDK. Forimpure methods, I
NFER tracks some effects in methods, e.g.,
if a method sets this.field = null, the effect will be
tracked at the call site. Tracking dynamic types of objectsis useful to reﬁne the control-ﬂow graph. However, this onlyoccurs in the context of the entry point of the analysis.
d)N
ULL AWAY :A ﬂow-sensitive type reﬁnement anal-
ysis to infer nullness of local variables that includes a ﬁeldinitialization checker. N
ULL AWAY assumes that unannotated
code cannot be null. For methods, ﬁelds, and method parame-
ters annotated with the @Nullable annotation, N ULL AWAY
ensures no dereferences, and that their value will not beassigned to a non-null ﬁeld or argument. Our tests showedthat N
ULL AWAY ﬁnds local and object ﬁeld dereferences
without annotations. With annotations, N ULL AWAY can ﬁnd
null dereferences of method parameters and return values.
NULL AWAY is able to avoid dynamic dispatch as a source of
unsoundness by ensuring that methods that are overridden havethe same nullability as its parent’s class. N
ULL AWAY ’s ﬁeld
initialization is unsound. For example, the analysis does notcheck ﬁelds that are read by methods called from constructors.
e)SPOT BUGS :A null-pointer analysis inherited from
FIND BUGS [24] that combines forward and backward dataﬂow
analyses for tracking null values. The analysis provides
limited tracking of object ﬁelds; it does not support aliasingand volatile ﬁelds, and it assumes that any method can modifya ﬁeld of an object passed as argument. Additionally, S
POT -
BUGS provides a null-related annotation @CheckForNull to
denote values that must be null-checked prior to a dereference.Our tests conﬁrmed the intraprocedural nature of S
POT BUGS ,
however we were unable to expose S POT BUGS ’ ﬁeld sensi-
tivity. Lastly, S POT BUGS infers parameter and return value
information intraprocedurally if these are null checked, and itsuffers from all sources of unsoundness as shown in Table II.
IV . E
XPERIMENTAL EV ALUA TION
This experimental evaluation is designed to answer the
following research questions:
RQ1 How prevalent are NPEs among all warnings?
RQ2 How effective are bug mapping methods for NPEs?
RQ3 How effective are static bug detectors for NPEs?
RQ4 What are the reasons bug detectors miss NPEs?
We ran CFN ULLNESS ,E RADICA TE ,INFER ,N ULL AWAY ,
and S POT BUGS on our dataset of 102 programs with real NPE
bugs to generate bug reports for the buggy and ﬁxed versionsof the programs. We ran the tools on the full programs,and veriﬁed that the ﬁles relevant to the bug and ﬁx wereindeed analyzed by the tools. We considered two settings for
S
POT BUGS : low and high thresholds. The results are presented
as S POT BUGS LT and S POT BUGS HT, respectively. We au-
tomatically parsed the bug reports to extract and normalizerelevant information, which we stored in a MySQL database.
Our study is fully reproducible. The dataset of real re-
producible NPE bugs from B
UGSWA R M and D EFECTS 4J is
publicly available as well as the tools we study. The scriptsfor performing the experiments and all data described in thissection is publicly accessible.
5
5https://github.com/ucd-plse/Static-Bug-Detectors-ASE-Artifact
29701 0 ,000 20, 000 30, 000 40, 000Null DereferenceDangerous MethodReturn V alueUnused FieldStatic Inner Class
22,65614,8289,7518,9226,983
Number of WarningsSpotBugsL T
05 ,000 10, 000 15, 000 20, 000Dangerous MethodNull DereferenceDead Local StoreReturn V alueWrong Map Iterator
9,0968,8072,5121,780784
Number of WarningsSpotBugsHT
01 0 ,000 20, 000 30, 000 40, 000Resource LeakNull DereferenceThread Safety ViolationImmutable CastUnsafe Thread Interface
15,70812,3076,7421,966128
Number of WarningsInfer
Fig. 4: S POT BUGS LT, S POT BUGS HT, and I NFER distribution of top 5 warnings.
TABLE III: Number of all warnings and NPE warnings
produced by each tool.“Avg All” and “Avg NPEs” refer to
the average number of warnings produced per program.
Tool All NPEs Avg All Avg NPEs
CFN ULLNESS 231,860 231,860 (100%) 1,137 1,137
ERADICA TE 266,682 266,682 (100%) 1,307 1,307
INFER 37,035 12,307 (33.2%) 181 60
NULL AWAY 25,065 25,065 (100%) 122 122
SPOT BUGS HT 49,555 8,807 (17.8%) 243 43
SPOT BUGS LT 129,183 22,656 (17.5%) 633 111
A. RQ1: Prevalence of NPE Warnings
Table III shows the total and average number of warnings
produced by each tool when analyzing the programs. There area total of 739,380 w arnings across the 102 ×2programs in
our dataset. E
RADICA TE yields the largest number of warnings
with 266,682, all of which are NPE warnings. Similarly,
CFN ULLNESS has the second highest number of NPE warn-
ings with a total of 231,860. S POT BUGS LT produces the
third highest number of warnings with 129,183 w arnings,
and S POT BUGS HT follows with 49,555 w arnings. Unlike
ERADICA TE , CFN ULLNESS and N ULL AWAY ,SPOT BUGS can
generate over a hundred different types of non-NPE warningswhile I
NFER generates seven.
Figure 4 shows the top ﬁve types of warnings for S POT -
BUGS LT, S POT BUGS HT, and I NFER . It is observed that NPEs
are one of the most prevalent warnings for these tools: the mostcommon for S
POT BUGS LT, and the second most common
for both S POT BUGS HT and I NFER . Indeed, NPEs constitute
from 17.5% to 33.2% of the total warnings produced by
these tools. For S POT BUGS HT, we observe a reduction in
total number of warnings and NPE warnings with respect to
SPOT BUGS LT of 61.6% and61.1%, respectively.
Finally, N ULL AWAY produces the fewest warnings (all of
them are NPE warnings), with a total of 25,065.
RQ1: NPE warnings are prevalent in all the tools stud-
ied. A total of 567,377 NPE w arnings ( 76.7% of all
warnings) are produced for our dataset. The percentageof NPE warnings for S
POT BUGS LT is 17.5%, S POT -
BUGS HT 17.8%, and I NFER 33.2%.TABLE IV: Bugs mapped by each method. We show thenumber of correct mappings / the total number of mappings.Column “Bugs Found” gives the total number of bugs foundper tool. Inside parenthesis are the number of bugs that a toolfound but not others. 30 unique bugs are found across tools.
Method
Tool Code Report Stack Covered Bugs Found
CFN ULLNESS 6/56 5/18 7/26 5/56 11 (2)
ERADICA TE 10/51 7/24 7/22 8/52 20 (5)
INFER 3/23 2/13 9/12 7/27 10 (1)
NULL AWAY 1/17 0/21 1/4 5/26 5 (2)
SPOT BUGS HT 4/18 4/6 1/5 2/13 4 (0)
SPOT BUGS LT 6/46 6/13 6/12 3/26 9 (1)
Total 30/211 24/95 31/81 30/200 30 Unique
B. RQ2: Effectiveness of Bug Mapping Methods
We applied the four methods discussed in Section II-C to
ﬁnd whether the tool warnings describe the bugs of interest.In total, all methods together correctly ﬁnd 30 distinct bugsout of 102 bugs (29.4%). All bug candidates were manually
inspected. Table IV summarizes the results.
An effective mapping method is deﬁned as having high
recall and precision. The S
TACK TRACE METHOD is the most
effective among the four, mapping 31 bugs with a precisionof 38.2%. All the NPEs mapped to a warning were contained
within the S
TACK TRACE METHOD , except for four. On the
other hand, while the C ODE DIFFMETHOD and C OVERAGE
METHOD produce the largest number of bug candidates across
all tools, they also suffer from the lowest precision, 14.2% and
15.0%, respectively. The R EPORT DIFFMETHOD mapped the
lowest number of true bugs in comparison to other methods(24 bugs), but its precision of 25.3% was still higher than that
of the C
ODE DIFF METHOD and C OVERAGE METHOD . The
results show that the four methods are primarily complemen-tary of each other as they map different types of information.
RQ2: The S TACK TRACE METHOD is the most effective
with 81 bug candidates, of which 31 were true bugs(38.2%). The C
ODE DIFF METHOD and C OVERAGE
METHOD had similar recall than the S TACK TRACE
METHOD , but a lower precision of 14.2% and 15.0%,
respectively. The R EPORT DIFFMETHOD had the lowest
recall but a higher precision than C ODE DIFFMETHOD
and C OVERAGE METHOD .
2981protected Object decode(Channel channel, ...){
2-if(channel == null){ -if(channe l==null){
3+if(channel != null){ +if(channel !=null){
4 channel.write(response, remoteAddress);
5 }
6}
(a) NPE bug found by both S POT BUGS and E RADICA TE .1public class GrblCntrllr extends AbstractCntrllr {
2- capabilities = null; -capabilities=null;
3+ capabilities = new GrblUtils.Capabilities() +capabilities =new GrblUtils.Capabilities()
4protected void pauseStreamingEvent(){
5 if(this.capabilities.REAL_TIME) { ... }
6}
(b) NPE bug dereferencing ﬁeld of an object not found by any tool.
1protected void ldCmdVerSheet(String sheetName) {
2Sheet sheet = switchToSheet(sheetName, false);
3+if(sheet==null) return; +if(sheet== null)return;
4while(i<sheet.getRows()) { ... }
5}
(c) NPE bug due to null dereference of a return value.1private void verifyDecodedPosition() {
2-if(p.gNtk()!=null){-if(p.gNtk()! =null){
3+if(p.gNtk()!=null && p.gNtk().gTwrs()!=null){ +if(p.gNtk() !=null&& p.gNtk().gTwrs() !=null){
4 for (Twr Twr : p.gNtk().gTwrs()){
5}
(d) NPE bug with dereferencing object returned from a method.
Fig. 5: Examples of NPE diffs from the dataset.
C. RQ3: Effectiveness of Tools at Finding NPEs
Overall, the tools ﬁnd 30 distinct bugs out of 102 bugs
(29.4%). The breakdown per tool is shown in Table IV.
ERADICA TE ﬁnds the most bugs with 20 out of 30. CFN ULL -
NESS ﬁnds 11 bugs, I NFER 10 bugs, and S POT BUGS LT 9
bugs. N ULL AWAY and S POT BUGS HT ﬁnd the fewest bugs
with 5 and 4, respectively. We examined the overlap among
bugs found by each tool. The two tools with the most overlapare CFN
ULLNESS and E RADICA TE with 7 bugs. Interestingly,
each tool ﬁnds bugs not found by other tools (also shown inTable IV). This shows that the tools are complementary, andthat practitioners could beneﬁt from running multiple tools. Achallenge to this is the large number of warnings to inspect.
An example of a bug found by CFN
ULLNESS ,INFER , and
SPOT BUGS LT was given in Figure 2a. We show the diff
between a buggy version (with an NPE bug) and the ﬁxedversion of the GitHub project openpnp/openpnp (a robotic
pick and place machine). The call to the buggy method thatcauses the NPE is located on Line 6 of the buggy program.The ﬁx for this NPE bug consists of adding a null check forparameter mat insaveDebugImage.
Figure 5a shows an example of a bug found by CFN
ULL -
NESS ,E RADICA TE ,SPOT BUGS HT, and S POT BUGS LT. Here
we show the diff between a buggy version and the ﬁxed versionof project traccar/traccar (a GPS tracking system). The
bug was that the null check was ﬂipped, incorrectly deref-erencing channel when null. The ﬁx simply consists of
changing the comparison operator from ==to!=. A possible
reason why I
NFER did not ﬁnd this bug is that I NFER does
not gather information from checks. Since Figure 5a includesa null check, S
POT BUGS is able to reason that channel is
dereferenced when null, leading to an NPE.
We conducted an additional experiment on a random sample
of40programs6from our initial set for which annotations
were inferred using IntelliJ IDEA ’s Infer Nullity [7]. IntelliJIdea infers both @Nullable and@NotNull annotations.
Note that 49 out of the 102 programs originally include
6The process could not be automated due to the IDE, thus the sample.some nullness annotations. We ran all tools on the anno-tated programs, except for I
NFER which does not use an-
notations. IntelliJ added 34,229 @Nullable and 167,236
@NotNull annotations. We applied the C OVERAGE METHOD
to map warnings. This resulted in 2,3,0, and 2additional
bugs found by CFN ULLNESS ,E RADICA TE ,N ULL AWAY , and
SPOT BUGS LT, respectively. These accounted for three unique
bugs across all tools. Despite the small increase in bugs found,the results are promising as annotating less than half of theprograms resulted in ﬁnding 10% more bugs in total.
RQ3: Overall, the tools have low effectiveness at ﬁndingNPE bugs. Out of the 102 bugs in our dataset, E
RAD -
ICA TE found 20 bugs ( 19.6%), CFN ULLNESS found
11 (10.8%), I NFER found 10 ( 9.8%), S POT BUGS LT
found 9 bugs ( 8.8%), N ULL AWAY found 5 ( 4.9%), and
SPOT BUGS HT found 4 ( 3.9%). Additional annotations
resulted in ﬁnding 3more bugs.
D. RQ4: Reasons Bug Detectors Miss NPEs
We are interested in understanding the reasons why bug
detectors fail to ﬁnd real NPEs. We start by discussing thecharacteristics of the bugs that the tools ﬁnd based on thecharacterization of 102 bugs from our dataset and the toolsthemselves (see Section III). We then discuss the characteris-tics of those bugs that the tools fail to ﬁnd.
a)CFN
ULLNESS :CFN ULLNESS found 11 bugs includ-
ing every category shown in Figure 3. These included 3dereferences to a method return value and 2 dereferences of amap object. The sound properties of CFN
ULLNESS allow it to
ﬁnd classes of bugs that the other tools cannot. For example,
CFN ULLNESS also found bugs due to concurrency, ﬁeld
initialization, generics, and reﬂection. The lack of necessaryannotations in the projects under study inhibits CFN
ULL -
NESS ’s ability to ﬁnd all of the bugs in those categories.
b)ERADICA TE :ERADICA TE found 20 bugs where 9
dereferenced a method return value, 3dereferenced an object
ﬁeld, 1retrieved a value from a map object, and the rest
dereferenced a method parameter. Despite using a partial
299model of the JDK, E RADICA TE missed bugs in other third-
party libraries. E RADICA TE does not handle concurrency and
reﬂection. These limitations explain some of the false nega-
tives, while others can be explained by the lack of full ﬁeldinitialization checks and dynamic dispatch.
c)I
NFER :INFER found 10 bugs that included 4deref-
erences of a method parameter, 4dereferences of a method
return value (one of which is from a JDK library), a derefer-ence of a list, and a dereference of an object ﬁeld. TheseNPEs are interprocedural in nature, which aligns with ourcharacterization of I
NFER in Section III. However, I NFER did
not ﬁnd the remaining NPEs that involve method parameters,method return values, or object ﬁelds, which we would expectto be captured by interprocedural analysis. One reason is that
I
NFER does not take into account existing null checks.
INFER has an internal partial model of the JDK, which
enables reasoning about certain library methods. Surprisingly,despite the fact that I
NFER supports ﬁeld sensitivity, and was
successful at ﬁnding such bugs in our tests, it missed manyother ﬁeld sensitive bugs. Note that I
NFER does not have a
check for ﬁeld initialization so it does not ﬁnd uninitializedﬁelds, but it does support ﬁelds set to null. Such an example
is shown in Figure 5b. Additionally, I
NFER does not ﬁnd NPEs
that involve reﬂection, concurrency, maps, or use of third-partylibraries outside of the JDK.
d)N
ULL AWAY :NULL AWAY found 5 bugs, all of which
dereference a return value. This shows the challenge in placingannotations in the right place to be beneﬁcial. N
ULL AWAY ’s
main sources of false negatives are its assumptions thatunannotated code is not null and that third-party libraries
do not return null. While manual tests written during our
categorization revealed correct warnings about dereferencedﬁelds, real bugs that share these characteristics were notdetected. Such an example is shown in Figure 5b, wherean unannotated ﬁeld (considered non-null) is being assignednull. This represents a strict violation of the assumption thatthe ﬁeld cannot hold a null value, and should result in a
warning. Finally, in the process of running N
ULL AWAY , one
of the programs crashed the tool. The problem was due toa buggy treatment of certain methods in the standard Javalibrary. We reported the bug to N
ULL AWAY developers, and
it is now ﬁxed in the latest release.
e)SPOT BUGS :SPOT BUGS found 9 NPEs, of which 5
occur when dereferencing a method parameter, 3when derefer-
encing a method return value, and another when dereferencinga ﬁeld. In all cases, there is at least one null check within themethod for the object being dereferenced, but the programmerdereferences the object in a path that is not checked. Thenull checks enable S
POT BUGS to reason about the NPEs
intraprocedurally (Section III). The remaining NPEs in oursample that dereference a method’s return value or parameterare not found because they require interprocedural reasoning.Additionally, the 17 NPEs that involve the dereference of anobject ﬁeld are not found by S
POT BUGS .S POT BUGS fails
to ﬁnd any bugs dealing with reﬂection, concurrency, third-party libraries, maps, and lists. This conforms to our toolcharacterization; S
POT BUGS does not provide complete ﬁeld
sensitivity.
RQ4:S POT BUGS misses NPEs that require interproce-
dural analysis. I NFER performs interprocedural analysis
but does not have a ﬁeld initialization check nor does ithandle some path-sensitive information from null checks.
N
ULL AWAY relies on nullness annotations but does not
handle maps nor third-party libraries. E RADICA TE deals
with third-party libraries better than other tools, but itstill misses bugs due to partial ﬁeld initialization check-ing. CFN
ULLNESS provides sound analyses to handle
reﬂection and initialization which allows ﬁnding bugs thatother tools cannot. However, the lack of annotations canstill lead to missed bugs.
E. Threats to V alidity
Although we conducted this study on a substantial number
of real-world NPEs, our results cannot be generalized. Weattempted to reduce this threat by including a large numberof NPE bugs from a diverse set of 42 projects from two Javabug datasets. It is possible that we may have missed othertools that are eligible for our study. We still believe that theﬁve tools considered are good representatives of popular andwidely-used state-of-the-art static bug detectors for NPEs. Thefour different mapping methods used in this paper are notperfect, and may lead to false positives. To alleviate this threat,we manually inspected all warnings that were deemed to bebug candidates. Anything requiring human intervention can beerror prone and subjective. To mitigate this threat and reducebias, we involved two people external to our project in thecategorization of bugs. Finally, we consulted tool developers toconﬁrm our ﬁndings regarding tool capabilities and limitations,as discussed in Section III.
V. L
ESSONS LEARNED
This section describes some opportunities for improvement.
a) Need for reducing or ranking warnings: Over 500,000
NPE warnings were generated across all tools and programs,with NPEs being in the top-2 warnings for every tool. Theaverage number of warnings per program was in the hundreds,which is a cumbersome amount. Because of this, we hadto employ a combination of mapping methods and manualveriﬁcation to determine if a bug was found. In our case it isknown that an NPE exists, and the goal is to determine whetherthe tools ﬁnd it. However, this is not the usual setting for toolusage; developers do not know beforehand of the existenceof bugs, or else the tools would not be needed. Thus, thelarge number of warnings is especially problematic in a realsetting where true bugs are unknown and all warnings mustbe inspected.
A bug ranking system could help in navigating the large
number of warnings. All tools studied, except for N
ULL AWAY ,
provide severity warning information, but this information didnot correlate to ﬁnding the NPEs under study. For example,
300SPOT BUGS provides a severity ranking: “concerning”, “trou-
bling”, “scary”, and “scariest”. However, the true bugs found
were not associated with the most severe category, but with the“troubling” and “scary” categories. This shows the need formore conservative strategies to process warnings, or to labelwarnings that are more likely to be true bugs.
Two main approaches for ranking warnings are found in
the literature, and could be applied in the context of staticbug detectors for NPEs. The ﬁrst solely focuses on rankingwarnings of a speciﬁc program version without consideringinformation such as warnings produced for other versionsof the program. Examples in this category learn a classiﬁervia methods ranging from bayesian networks, decision trees,and neural networks [21, 43]. The second approach uses thedifference of warnings between a previous and the currentversion of the program, or self-adapts through user feedback[22, 36]. A promising approach to aid static bug detectors forNPEs would be to learn a project-speciﬁc classiﬁer that hasuser-feedback on predictions. This would beneﬁt users as thetool learns, over time, domain-speciﬁc project characteristics,which would eventually lead to higher precision.
b) Need for automatically inferring nullability annota-
tions: There is an inherent burden in writing annotations.
Analyzers that depend on annotations could beneﬁt from au-tomated inference of nullability annotations. Running IntelliJIDEA ’s Infer Nullity on 40 programs enabled the tools toﬁnd an additional 3 bugs. This shows that there is promise inannotation-based approaches for bug ﬁnding. However, there isroom for improvement in annotation inference as the analysisstill missed annotations that could have lead to ﬁnding morebugs. Furthermore, it was difﬁcult to automate the process ofannotating code using IntelliJ, which may prevent its use inmany scenarios. There exists work that applies static analysisto infer non-null annotations for object ﬁelds in a subset ofJava [25], which could be potentially used to aid annotation-based NPE bug detectors but it is not publicly available.
c) Need for reasoning about collection-like data struc-
tures: A pain point for all tools studied is reasoning about
the nullability of objects inside a collection-like data structuresuch as an array. Users can add annotations to indicate thata data structure can be null, but there is no mechanism to
annotate the nullability of individual elements in the data struc-ture. CFN
ULLNESS ,E RADICA TE , and N ULL AWAY overcome
this challenge for map-like objects by assuming that the get
interface may return a “nullable” value. A similar approachcould be adopted each time an element from other collection-like data structure is retrieved. Incorporating such strategywould enable the tools to successfully ﬁnd 10 additional bugs.
d) Need for reasoning about reﬂection: Reasoning about
reﬂection imposes a challenge for any static analysis. Allof the tools in our study are unsound when it comes toreﬂection, except CFN
ULLNESS . Since most of the tools
can leverage annotations, a potential approach for handlingreﬂection is user-provided annotations. This is exactly what
CFN
ULLNESS does. This is done via a list of targets, a priori,
of what class or method is being operated on for certainreﬂection calls. This approach has been implemented in otheranalyses [28, 37, 39, 40] for Java, where analysis precision wasobserved to improve. Indeed, incorporating the above strategywould enable the NPE bug detectors to ﬁnd 13 additionalbugs, from which CFN
ULLNESS successfully ﬁnds one given
the existing annotations.
VI. R ELA TED WORK
a) Static Analyzer Studies: Rutar et al. [35] compare
the static analyzers PMD, FindBugs, JLint, Bandera, andESC/Java 2 on a small suite of programs. The authors presenta taxonomy of bugs found by each tool showing that no toolsubsumes the other. The study focuses on runtime and numberof warnings produced. Johnson et al. [26] conduct a study inwhich 20 developers are interviewed on their experiences usingstatic analysis tools. The study ﬁnds that the main reason whydevelopers do not use tools is false positives.
Habib and Pradel [20] study the static analyzers I
NFER ,
ERROR PRONE , and S POT BUGS to determine how many of all
bugs in D EFECTS 4J can be found by these tools. The authors
use the code diff and the bug report mapping methods. Thestudy ﬁnds that only 27 bugs out of 594 bugs (4.5%) weredetected, of which only 2 were NPEs. Tomassi [41] conductsa study that compares E
RROR PRONE and S POT BUGS to ﬁnd
how many of allbugs in a sample of 320 B UGSWA R M artifacts
are found. The author found that only one bug was foundby S
POT BUGS . Instead, we focus on a speciﬁc kind of bug,
NPEs, and present a detailed analysis of the capabilities andthe limitations of ﬁve popular tools that ﬁnd NPEs.
Ayewah and Pugh [12] run Coverity, Eclipse, FindBugs,
Fortify, and XYLEM on different versions of the build systemAnt. The authors classify the null dereferences reported byeach tool (plausible, implausible, or impossible), and explorethe usefulness of using null-related annotations. Most recently,Banerjee et al. [14] presented the tool N
ULL AWAY and per-
formed a comparison to the Checker Framework’s Nullnessanalysis [33], and I
NFER ’s Eradicate looking at build-time
overhead. While Ayewah and Pugh [12] study false positivesin one version of Ant, Banerjee et al. [14] focus on measuringfalse negatives in Uber’s Android apps. We study the recallof ﬁve popular bug detectors, including N
ULL AWAY , on 102
real and reproducible NPEs from 42 open-source projects.
b) Tools to Find Null Pointer Dereferences: Ayewah
et al. [13] present a static analysis tool called F IND BUGS , the
predecessor of S POT BUGS .F IND BUGS ﬁnds a wide variety
of bugs including null pointer dereferences. Hovemeyer andPugh [24] extend F
IND BUGS ’s NPE ﬁnding capabilities by
improving the precision of the analysis. These improvementswere a result of a better model of the core API of JDK,changing how errors on exception paths are handled, improv-ing ﬁeld tracking, and ﬁnding guaranteed dereferences. Weinclude S
POT BUGS in our study.
Papi et al. [33] introduce the Checker Framework, which
allows for pluggable type systems for Java. They evaluateﬁve checkers, including the Nullness checker, running themover signiﬁcant sized code bases. The checkers ﬁnd real bugs
301and conﬁrmed the absence of others. We include the Checker
Framework in our study.
Nanda and Sinha [32] develop a demand-driven dataﬂow
analysis for null-dereference bugs in Java. By being path-sensitive and context-sensitive, the analysis allows for a lowfalse positive rate, and an improved precision over F
IND BUGS
and JLint. Romano et al. [34] use the analysis from Nanda andSinha [32] to ﬁnd variables and paths that lead to possible nullpointer dereferences. The authors use a genetic algorithm togenerate tests that trigger the null pointer dereferences. Logi-nov et al. [29] develop a sound interprocedural analysis basedon abstract interpretation called expanding-scope algorithm.
Madhavan and Komondoor [30] demonstrate a sound, demand-driven, interprocedural, context-sensitive dataﬂow analysis toverify whether a dereference will be safe or not. None of theabove tools [29, 30, 32, 34] are publicly available.
VII. C
ONCLUSION
In this experience paper, we studied the effectiveness of
popular Java static bug detectors CFN ULLNESS ,E RADICA TE ,
INFER ,N ULL AWAY , and S POT BUGS on 102 real NPEs from
42 open-source projects. We identiﬁed the capabilities of the
tools and the characteristics of the NPE bugs in our dataset. Wediscussed the problem of mapping tool warnings to actual NPEbugs, and investigated four mapping methods, including twonew approaches that leverage stack trace and code coverageinformation, from which the stack-trace based was the mosteffective. Overall, the tools detected a total of 30 out of
102 bugs. We conducted an additional experiment annotating
40 programs using IntelliJ, which resulted in 3 new bugsfound. Finally, we leveraged the characteristics of the toolsand the bugs in our dataset to gain insights into why the toolsmissed certain types of bugs. We concluded by discussingopportunities for improving NPE bug detection. We providethe link to a public repository that contains both our scriptsand the data produced in our experimental evaluation.
A
CKNOWLEDGMENT
This work was supported in part by National Science
Foundation award CNS-2016735, a Facebook Testing and V er-iﬁcation research award, and a UC Davis Graduate Fellowship.We would like to thank Aditya V . Thakur and Premkumar T.Devanbu for their feedback and suggestions. We also thankAmy Cu, Raisa Putri, Robert Furth, and Ryan Jae for their helpmanually classifying NPEs, and replicating our experimentalresults. Lastly, we would like to thank the developers of eachtool for their prompt answers to our questions.
R
EFERENCES
[1] CVE-2013-1134. https://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2003-1134, 2021.
[2] Checkstyle. https://github.com/checkstyle/checkstyle, 2021.
[3] CVE Null Pointer. https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=
null+pointer, 2021.
[4] Eradicate. https://fbinfer.com/docs/eradicate.html, 2021.[5] Error Prone. https://github.com/google/error-prone, 2021.[6] Infer. http://fbinfer.com/, 2021.[7] IntelliJ. https://www.jetbrains.com/idea/, 2021.[8] NullAway. https://github.com/uber/NullAway, 2021.
[9] PMD. https://pmd.github.io/, 2021.[10] SpotBugs. https://spotbugs.github.io/, 2021.
[11] A. V . Aho, M. S. Lam, R. Sethi, and J. D. Ullman. Compilers: Prin-
ciples, Techniques, and Tools (2nd Edition). Addison-Wesley LongmanPublishing Co., Inc., USA, 2006.
[12] N. Ayewah and W. Pugh. Null dereference analysis in practice. In
Proceedings of the 9th ACM SIGPLAN-SIGSOFT Workshop on ProgramAnalysis for Software Tools and Engineering, PASTE ’10, pages 65–72,New Y ork, NY , USA, 2010. ACM. doi: 10.1145/1806672.1806686. URLhttp://doi.acm.org/10.1145/1806672.1806686.
[13] N. Ayewah, D. Hovemeyer, J. D. Morgenthaler, J. Penix, and W. Pugh.
Using static analysis to ﬁnd bugs. IEEE Softw., 25(5):22–29, 2008. doi:
10.1109/MS.2008.130. URL https://doi.org/10.1109/MS.2008.130.
[14] S. Banerjee, L. Clapp, and M. Sridharan. Nullaway: Practical type-based
null safety for java. In Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on theF oundations of Software Engineering, ESEC/FSE 2019, pages 740–750,New Y ork, NY , USA, 2019. ACM. doi: 10.1145/3338906.3338919. URLhttp://doi.acm.org/10.1145/3338906.3338919.
[15] C. Calcagno and D. Distefano. Infer: An automatic program veriﬁer
for memory safety of C programs. In M. G. Bobaru, K. Havelund,G. J. Holzmann, and R. Joshi, editors, NASA F ormal Methods - Third
International Symposium, NFM 2011, Pasadena, CA, USA, April 18-20,2011. Proceedings, volume 6617 of Lecture Notes in Computer Science,
pages 459–465. Springer, 2011. doi: 10.1007/978-3-642-20398-5\
33.
URL https://doi.org/10.1007/978-3-642-20398-5 33.
[16] C. Calcagno, D. Distefano, P . W. O’Hearn, and H. Yang. Compositional
shape analysis by means of bi-abduction. In Z. Shao and B. C. Pierce,editors, Proceedings of the 36th ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, POPL 2009, Savannah, GA,USA, January 21-23, 2009, pages 289–300. ACM, 2009. doi: 10.1145/1480881.1480917. URL https://doi.org/10.1145/1480881.1480917.
[17] C. Calcagno, D. Distefano, J. Dubreil, D. Gabi, P . Hooimeijer, M. Luca,
P . W. O’Hearn, I. Papakonstantinou, J. Purbrick, and D. Rodriguez.Moving fast with software veriﬁcation. In K. Havelund, G. J. Holz-mann, and R. Joshi, editors, NASA F ormal Methods - 7th International
Symposium, NFM 2015, Pasadena, CA, USA, April 27-29, 2015, Pro-ceedings, volume 9058 of Lecture Notes in Computer Science, pages
3–11. Springer, 2015. doi: 10.1007/978-3-319-17524-9\
1. URL
https://doi.org/10.1007/978-3-319-17524-9 1.
[18] M. Christakis and C. Bird. What developers want and need from
program analysis: an empirical study. In D. Lo, S. Apel, and S. Khurshid,editors, Proceedings of the 31st IEEE/ACM International Conference on
Automated Software Engineering, ASE 2016, Singapore, September 3-7, 2016, pages 332–343. ACM, 2016. doi: 10.1145/2970276.2970347.URL https://doi.org/10.1145/2970276.2970347.
[19] W. Dietl, S. Dietzel, M. D. Ernst, K. Muslu, and T. W. Schiller.
Building and using pluggable type-checkers. In R. N. Taylor, H. C.Gall, and N. Medvidovic, editors, Proceedings of the 33rd International
Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu ,HI, USA, May 21-28, 2011, pages 681–690. ACM, 2011. doi: 10.1145/1985793.1985889. URL https://doi.org/10.1145/1985793.1985889.
[20] A. Habib and M. Pradel. How many of all bugs do we ﬁnd? a study of
static bug detectors. In M. Huchard, C. K ¨astner, and G. Fraser, editors,
Proceedings of the 33rd ACM/IEEE International Conference on Auto-mated Software Engineering, ASE 2018, Montpellier , France, September3-7, 2018, pages 317–328. ACM, 2018. doi: 10.1145/3238147.3238213.URL https://doi.org/10.1145/3238147.3238213.
[21] Q. Hanam, L. Tan, R. Holmes, and P . Lam. Finding patterns in static
analysis alerts: improving actionable alert ranking. In P . T. Devanbu,S. Kim, and M. Pinzger, editors, 11th Working Conference on Mining
Software Repositories, MSR 2014, Proceedings, May 31 - June 1, 2014,Hyderabad, India, pages 152–161. ACM, 2014. doi: 10.1145/2597073.2597100. URL https://doi.org/10.1145/2597073.2597100.
[22] K. Heo, M. Raghothaman, X. Si, and M. Naik. Continuously reasoning
about programs using differential bayesian inference. In K. S. McKinleyand K. Fisher, editors, Proceedings of the 40th ACM SIGPLAN Con-
ference on Programming Language Design and Implementation, PLDI2019, Phoenix, AZ, USA, June 22-26, 2019, pages 561–575. ACM, 2019.doi: 10.1145/3314221.3314616. URL https://doi.org/10.1145/3314221.3314616.
[23] D. Hovemeyer and W. Pugh. Finding bugs is easy. In J. M. Vlissides and
D. C. Schmidt, editors, Companion to the 19th Annual ACM SIGPLAN
Conference on Object-Oriented Programming, Systems, Languages, andApplications, OOPSLA 2004, October 24-28, 2004, V ancouver , BC,
302Canada, pages 132–136. ACM, 2004. doi: 10.1145/1028664.1028717.
URL https://doi.org/10.1145/1028664.1028717.
[24] D. Hovemeyer and W. Pugh. Finding more null pointer bugs, but not
too many. In M. Das and D. Grossman, editors, Proceedings of the 7th
ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for SoftwareTools and Engineering, PASTE’07, San Diego, California, USA, June13-14, 2007, pages 9–14. ACM, 2007. doi: 10.1145/1251535.1251537.URL https://doi.org/10.1145/1251535.1251537.
[25] L. Hubert, T. P . Jensen, and D. Pichardie. Semantic foundations and
inference of non-null annotations. In G. Barthe and F. S. de Boer, editors,F ormal Methods for Open Object-Based Distributed Systems, 10th IFIPWG 6.1 International Conference, FMOODS 2008, Oslo, Norway, June4-6, 2008, Proceedings, volume 5051 of Lecture Notes in Computer Sci-
ence, pages 132–149. Springer, 2008. doi: 10.1007/978-3-540-68863-1\
9. URL https://doi.org/10.1007/978-3-540-68863-1 9.
[26] B. Johnson, Y . Song, E. R. Murphy-Hill, and R. W. Bowdidge. Why
don’t software developers use static analysis tools to ﬁnd bugs? InD. Notkin, B. H. C. Cheng, and K. Pohl, editors, 35th International
Conference on Software Engineering, ICSE ’13, San Francisco, CA,USA, May 18-26, 2013, pages 672–681. IEEE Computer Society, 2013.doi: 10.1109/ICSE.2013.6606613. URL https://doi.org/10.1109/ICSE.2013.6606613.
[27] R. Just, D. Jalali, and M. D. Ernst. Defects4j: a database of existing faults
to enable controlled testing studies for java programs. In C. S. Pasareanuand D. Marinov, editors, International Symposium on Software Testing
and Analysis, ISSTA ’14, San Jose, CA, USA - July 21 - 26, 2014,pages 437–440. ACM, 2014. doi: 10.1145/2610384.2628055. URLhttps://doi.org/10.1145/2610384.2628055.
[28] O. Lhot ´ak and L. J. Hendren. Scaling java points-to analysis using
SPARK. In G. Hedin, editor, Compiler Construction, 12th International
Conference, CC 2003, Held as Part of the Joint European Conferenceson Theory and Practice of Software, ETAPS 2003, Warsaw, Poland, April7-11, 2003, Proceedings, volume 2622 of Lecture Notes in Computer
Science, pages 153–169. Springer, 2003. doi: 10.1007/3-540-36579-6\
12. URL https://doi.org/10.1007/3-540-36579-6 12.
[29] A. Loginov, E. Yahav, S. Chandra, S. Fink, N. Rinetzky, and M. G.
Nanda. V erifying dereference safety via expanding-scope analysis. InB. G. Ryder and A. Zeller, editors, Proceedings of the ACM/SIGSOFT
International Symposium on Software Testing and Analysis, ISSTA 2008,Seattle, WA, USA, July 20-24, 2008, pages 213–224. ACM, 2008.doi: 10.1145/1390630.1390657. URL https://doi.org/10.1145/1390630.1390657.
[30] R. Madhavan and R. Komondoor. Null dereference veriﬁcation via
over-approximated weakest pre-conditions analysis. In C. V . Lopesand K. Fisher, editors, Proceedings of the 26th Annual ACM SIGPLAN
Conference on Object-Oriented Programming, Systems, Languages, andApplications, OOPSLA 2011, part of SPLASH 2011, Portland, OR, USA,October 22 - 27, 2011, pages 1033–1052. ACM, 2011. doi: 10.1145/2048066.2048144. URL https://doi.org/10.1145/2048066.2048144.
[31] G. J. Myers, T. Badgett, T. M. Thomas, and C. Sandler. The art of
software testing, volume 2. Wiley Online Library, 2004.
[32] M. G. Nanda and S. Sinha. Accurate interprocedural null-dereference
analysis for java. In Proceedings of the 31st International Conference
on Software Engineering, ICSE ’09, pages 133–143, Washington, DC,USA, 2009. IEEE Computer Society. doi: 10.1109/ICSE.2009.5070515.URL http://dx.doi.org/10.1109/ICSE.2009.5070515.
[33] M. M. Papi, M. Ali, T. L. C. Jr., J. H. Perkins, and M. D. Ernst.
Practical pluggable types for java. In B. G. Ryder and A. Zeller,editors, Proceedings of the ACM/SIGSOFT International Symposium on
Software Testing and Analysis, ISSTA 2008, Seattle, WA, USA, July 20-24, 2008, pages 201–212. ACM, 2008. doi: 10.1145/1390630.1390656.URL https://doi.org/10.1145/1390630.1390656.
[34] D. Romano, M. D. Penta, and G. Antoniol. An approach for searchbased testing of null pointer exceptions. In F ourth IEEE International
Conference on Software Testing, V eriﬁcation and V alidation, ICST 2011,Berlin, Germany, March 21-25, 2011, pages 160–169. IEEE ComputerSociety, 2011. doi: 10.1109/ICST.2011.49. URL https://doi.org/10.1109/ICST.2011.49.
[35] N. Rutar, C. B. Almazan, and J. S. Foster. A comparison of bug
ﬁnding tools for java. In Proceedings of the 15th International Sym-
posium on Software Reliability Engineering, ISSRE ’04, pages 245–256, Washington, DC, USA, 2004. IEEE Computer Society. doi:10.1109/ISSRE.2004.1. URL http://dx.doi.org/10.1109/ISSRE.2004.1.
[36] H. Shen, J. Fang, and J. Zhao. Eﬁndbugs: Effective error ranking for
ﬁndbugs. In F ourth IEEE International Conference on Software Testing,
V eriﬁcation and V alidation, ICST 2011, Berlin, Germany, March 21-25,2011, pages 299–308. IEEE Computer Society, 2011. doi: 10.1109/ICST.2011.51. URL https://doi.org/10.1109/ICST.2011.51.
[37] M. Sridharan, S. Artzi, M. Pistoia, S. Guarnieri, O. Tripp, and R. Berg.
F4F: taint analysis of framework-based web applications. In C. V . Lopesand K. Fisher, editors, Proceedings of the 26th Annual ACM SIGPLAN
Conference on Object-Oriented Programming, Systems, Languages, andApplications, OOPSLA 2011, part of SPLASH 2011, Portland, OR, USA,October 22 - 27, 2011, pages 1053–1068. ACM, 2011. doi: 10.1145/2048066.2048145. URL https://doi.org/10.1145/2048066.2048145.
[38] F. Thung, Lucia, D. Lo, L. Jiang, F. Rahman, and P . T. Devanbu. To
what extent could we detect ﬁeld defects? an empirical study of falsenegatives in static bug ﬁnding tools. In M. Goedicke, T. Menzies, andM. Saeki, editors, IEEE/ACM International Conference on Automated
Software Engineering, ASE’12, Essen, Germany, September 3-7, 2012,pages 50–59. ACM, 2012. doi: 10.1145/2351676.2351685. URL https://doi.org/10.1145/2351676.2351685.
[39] F. Tip, C. Laffra, P . F. Sweeney, and D. Streeter. Practical experience
with an application extractor for java. In B. Hailpern, L. M. Northrop,and A. M. Berman, editors, Proceedings of the 1999 ACM SIGPLAN
Conference on Object-Oriented Programming Systems, Languages &Applications (OOPSLA ’99), Denver , Colorado, USA, November 1-5,1999, pages 292–305. ACM, 1999. doi: 10.1145/320384.320414. URLhttps://doi.org/10.1145/320384.320414.
[40] F. Tip, P . F. Sweeney, C. Laffra, A. Eisma, and D. Streeter. Practical
extraction techniques for java. ACM Trans. Program. Lang. Syst.,2 4
(6):625–666, 2002. doi: 10.1145/586088.586090. URL https://doi.org/10.1145/586088.586090.
[41] D. A. Tomassi. Bugs in the wild: examining the effectiveness of
static analyzers at ﬁnding real-world bugs. In Proceedings of the
2018 ACM Joint Meeting on European Software Engineering Con-ference and Symposium on the F oundations of Software Engineering,ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, pages 980–982, 2018. doi: 10.1145/3236024.3275439. URLhttps://doi.org/10.1145/3236024.3275439.
[42] D. A. Tomassi, N. Dmeiri, Y . Wang, A. Bhowmick, Y .-C. Liu, P . T.
Devanbu, B. V asilescu, and C. Rubio-Gonz ´alez. Bugswarm: Mining
and continuously growing a dataset of reproducible failures and ﬁxes.InProceedings of the 41st International Conference on Software Engi-
neering, ICSE ’19, pages 339–349, Piscataway, NJ, USA, 2019. IEEEPress. doi: 10.1109/ICSE.2019.00048. URL https://doi.org/10.1109/ICSE.2019.00048.
[43] L. Y u, W. Tsai, W. Zhao, and F. Wu. Predicting defect priority based on
neural networks. In L. Cao, J. Zhong, and Y . Feng, editors, Advanced
Data Mining and Applications - 6th International Conference, ADMA2010, Chongqing, China, November 19-21, 2010, Proceedings, Part II,volume 6441 of Lecture Notes in Computer Science, pages 356–367.
Springer, 2010. doi: 10.1007/978-3-642-17313-4\
35. URL https://
doi.org/10.1007/978-3-642-17313-4 35.
[44] M. Zhivich and R. K. Cunningham. The real cost of software errors.
IEEE Secur . Priv., 7(2):87–90, 2009. doi: 10.1109/MSP .2009.56. URLhttps://doi.org/10.1109/MSP .2009.56.
303