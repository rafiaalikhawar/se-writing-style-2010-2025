BigFuzz: Efficient Fuzz Testing for Data Analytics Using
Framework Abstraction
Qian Zhang
University of California, Los Angeles
zhangqian@cs.ucla.eduJiyuan Wang
University of California, Los Angeles
wangjiyuan@g.ucla.eduMuhammad Ali Gulzar
Virginia Tech
gulzar@cs.vt.edu
Rohan Padhye
Carnegie Mellon University
rohanpadhye@cmu.eduMiryung Kim
University of California, Los Angeles
miryung@cs.ucla.edu
ABSTRACT
Asbigdataanalyticsbecomeincreasinglypopular,data-intensive
scalablecomputing(DISC)systemshelpaddressthescalabilityis-
sue of handling large data. However, automated testing for such
data-centric applications is challenging, because data is often in-
complete,continuouslyevolving,andhardtoknowapriori.Fuzz
testing has been proven to be highly effective in other domains
such as security; however, it is nontrivial to apply such traditional
fuzzing to big data analytics directly for three reasons: (1) the long
latencyofDISCsystemsprohibitstheapplicabilityoffuzzing:naïve
fuzzingwouldspend98%ofthetimeinsettingupatestenviron-
ment;(2)conventionalbranchcoverageisunlikelytoscaletoDISC
applications because most binary code comes from the framework
implementation such as Apache Spark; and (3) random bit or byte
levelmutationscanhardlygeneratemeaningfuldata,whichfails
to reveal real-world application bugs.
We propose a novel coverage-guided fuzz testing tool for big
data analytics, called BigFuzz. The key essence of our approach
isthat:(a)wefocusonexercisingapplicationlogicasopposedto
increasingframeworkcodecoveragebyabstractingtheDISCframe-
workusingspecifications. BigFuzzperformsautomatedsourceto
sourcetransformationstoconstructanequivalentDISCapplication
suitableforfasttestgeneration,and(b)wedesignschema-aware
datamutationoperatorsbasedonourin-depthstudyofDISCap-
plicationerrortypes. BigFuzzspeedsupthe fuzzingtimeby78to
1477X compared to random fuzzing, improves application code
coverage by20%to 271%,and achieves33%to 157%improvement
in detecting application errors. When compared to the state of the
art that uses symbolic execution to test big dataanalytics, BigFuzz
is applicable to twice more programs and can find 81% more bugs.
KEYWORDS
fuzz testing, big data analytics, test generation
ASE ’20, September 21–25, 2020, Australia
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6768-4/20/09.
https://doi.org/10.1145/3324884.3416641ACM Reference Format:
QianZhang,JiyuanWang,MuhammadAliGulzar,RohanPadhye,andMiryung
Kim. 2020. BigFuzz: Efficient Fuzz Testing for Data Analytics Using Frame-
work Abstraction. In 35th IEEE/ACM International Conference on Automated
SoftwareEngineering(ASE’20),September21–25,2020,VirtualEvent,Aus-
tralia.ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3324884.
3416641
1 INTRODUCTION
Emerging technologies are producing much data and the impor-
tanceofdata-centricapplicationscontinuestogrow.Data-intensivescalablecomputing(DISC)systems,suchasGoogle’sMapReduce[
30],
Apache Hadoop [ 1], and Apache Spark [ 2], have shown great
promisestoaddressthe scalability challengeofbigdataanalytics.
Although DISC systems are becoming widely available to industry,
DISC applications are difficult to test and debug. Data scientists of-ten test DISC applications in their local environment using sample
data only. These applications are thus not tested thoroughly and
may not be robust to bugs and failures in the production setting.
The correctness of DISC applications depends on their ability
tohandlereal-worlddata;however,dataisinherentlyincomplete,
continuously evolving, and hard to know a-prior. Motivated by the
successesofsystematictestgenerationtools[ 33,34,62],afewhave
been proposed for dataflow-based DISC applications [ 38,45,52].
For example, BigTest[38] uses symbolic execution to automati-
callyenumeratedifferentpathconditionsofaDISCapplicationandgenerateconcreteinputsusinganSMTsolv er.Howeve r,itsapplica-
bilityislimitedtothedataflowoperators(e.g., map,reduce,join,
etc.) where symbolic execution is supported, and limited by the
pathexplorationcapabilityoftheunderlyingsymbolicexecution
engine and an SMT solver. In other words, developing a robusttest
generation tool for DISC applications remains an open problem.
In recent years, coverage-guided mutation-based fuzz testing
hasemergedasoneofthemosteffectivetestgenerationtechniques
for large software systems [ 17,49]. Such fuzz testing techniques
are based on implicit assumptions that it takes a relatively short
amountoftimetorepetitivelyrunprogramswithdifferentinputs
and arbitrary byte level mutations are likely to yield reasonableinputs. In fact, most fuzzing techniques start from a seed input,generate new inputs iteratively by mutating the previous inputs,
andaddnewinputstotheinputqueueiftheyexerciseanewbranch.
*This researchwas done,whilethe thirdandfourth authorsweregraduate students
at UCLA and UC Berkeley respectively.
7222020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
This work is licensed under a Creative Commons Attribution International 4.0 License.
However, our experience tells us that fuzzing cannot be applied to
big data analytics directly. First, the long latency nature of DISC
systems prohibits the efficacy of traditional fuzzing. While tradi-tional fuzzing techniques assume thousands of invocations per
second, for example, Apache Spark applications would need about
10to15secondstoinitializetheSpark contextforeachrun—job
scheduling, data partitioning, and serialization all contribute to
increasedlatency.Second,low-levelmutations(e.g.,flippingabit
orbyte)inexistingnaïvefuzzerscanhardlyexplorecornercases
that represent realistic application bugs. Lastly, grammar-aware
fuzzers[35,43,70]existtoreducethetimerequiredforconstructing
meaningful inputs.However, they requirea userto provide gram-mar rules and, by definition, they do not produce inputs violating
the user-provided grammar rules.
In this paper, we lay the groundwork for embodying a coverage-
guided,mutation-basedfuzztestingapproachforbigdataanalytics.
The key insight behind BigFuzzis thatfuzz testing of DISC applica-
tions can be made tractable by abstracting framework code and byanalyzingapplicationlogicintandem.Ourkeyideaistoperform
source-to-source transformation of a DISC application to a seman-
tically equivalent, yet a framework-independent program that is
more amenable to fuzzing.
BasedontheinsightthataDISCapplicationdeveloperwritesap-
plicationlogicintermsofuser-definedfunctionsandconnectsthemusingdataflowoperatorsintheDISCframework,
BigFuzzfocuseson
exercising application logic as opposed to the DISC framework im-
plementation. BigFuzzuses a two-level instrumentation method to
monitorapplication-specificcoverage,whilemodelingthedifferent
outcomesofdataflowoperations.Assuchcombinationofbehav-ior modeling is independent of the underlying DISC frameworkimplementation,wecanabstracttheframeworkwithexecutable
specificationsandgenerateaSparkcontextfreeprogramtomitigatethelonglatencycausedbytheDISCframework.Anapplicationde-
veloper is not required to write any custom specifications, because
thespecificationsfordataflowoperatorssuchas mapandreduce
do not need to be re-written for each application. BigFuzzfully
automatesthisprocessofconstructingasemanticallyequivalent
DISC application through source to source transformation.
As opposed to random bit or byte-level input mutations, we de-
signschema-awaremutationoperationsguidedbyreal-worlderror
types.Thesemutationoperationsincreasethechanceofcreating
meaningful inputs that map to real-world errors. To inform the
designofsuchdatamutationoperators,weconductedasystematic
study on common error types and root causes in Apache Spark
and Hadoop applications using two complementary sources: Stack
Overflow[ 3]andGithub[ 4].Thestudyidentifiedtencommonerror
types, which we map and encode in terms of six different mutation
operators in BigFuzz.
Weevaluate BigFuzzonabenchmarkoftwelveApacheSparkap-
plications.Wecomparethetimetogeneratetestinputsandtheiras-
sociated error-finding capabilities against two baseline techniques:
randomfuzzing,andsymbolic-executionbasedtesting.Withframe-
work abstraction, BigFuzzis able to speed up the fuzzing time by
78 to 1477X compared to random fuzzing. Schema-aware mutation
operations can improve application code coverage by 20 to 200%
with valid inputs as seeds, which leads to 33 to 100% improvement
indetectingapplicationerrors,whencomparedtonaiverandomfuzzing. Even without valid input seeds, BigFuzzimproves applica-
tioncodecoverageby118to271%anderrordetectionby58to157%,
demonstratingitsrobustness.Weshowthat BigFuzzisapplicable
to twice more applications and can find 81% more bugs than the
state of the art, BigTest.
In summary, this work makes the following contributions:
(1)We propose a fuzz testing technique called BigFuzzthat
targetsDISCapplicationsbyautomaticallyabstractingthedataflowbehavioroftheDISCframeworkwithexecutable
specifications.This novelapproach canalso be generalized
to other systems with long latency.
(2)We propose an automated instrumentation method to moni-
torapplicationlogicinconjunctionwithhowdataflowop-
erators are exercised in terms of their dataflow equivalence
class coverage.
(3)Wepresentschema-awaremutationoperationsthatareguided
byreal-worlderrorsencounteredinDISCapplications.To
our knowledge, we are the first to design a fuzz testing tech-
nique by empirically studying and codifying mutations that
correspond to real-world DISC bugs.
(4)Our experimental evaluation on 12 Apache Spark applica-
tionsdemonstratesthat BigFuzzoutperformspriorworkin
terms of code coverage and error-detection capability.
Weprovideaccesstoartifactsof BigFuzzathttps://github.com/
qianzhanghk/BigFuzz.
2 BACKGROUND
Apache Spark. BigFuzztargetsApache Spark,awidely useddata
intensive scalable computing system but can generalize to other
DISC frameworks. Spark achieves scalability by creating Resilient
DistributedDatasets(RDDs),anabstractionofdistributedcollec-
tion[73].ProgrammerscantransformRDDsinparallelusingdataflow
operations, e.g.,val newRDD = RDD.map(s => s.length) .Dataflow
operators such as filter,map, andreduceare implemented as
higher-order functions that take a user-defined function (UDF) as
an input argument. The actualevaluation of an RDD occurs when
an action such as countorcollect is called. For example, a Spark
applicationdeveloperwritesapplicationlogicintermsofUDFsandconnects them using dataflow APIs. To execute the program, Spark
first translates a program into a Directed Acyclic Graph (DAG),
whereverticesrepresentvariousoperationsontheRDDs,andthen
executes each stage in a topological order.
Thecommonindustrypracticefortestingsuchbigdataanalytics
applications remains running them locally on a randomly sampled
dataset.Testingwithsampledataisoftenincompletewhichleadstorare buggy cases in production runs. Often Spark programs run for
daysandthencrashwithoutanobviousreason.Additionally,the
start up latency associated with invoking the Spark framework and
BlockManagerMaster cantakeseveralsecondsforsimplysetting
up an execution environment and repetitive data partitioning, job
scheduling, serialization, and deserialization to support distributed
execution all contribute to increased latency. Thus random fuzzing
would be prohibitively expensive to test big data analytics.
FuzzTesting. FuzztestingsuchasAFL[ 17]hasbeenproventobe
highly effectivein synthesizing test inputsthat achieve high code
coverage and find bugs. Given an input program, it instruments
723Figure 1: Approach Overview of BigFuzz
1valloan = sc.textFile("account_history.csv")
2// Input with zipcode, base loan, years, and rate
3.map{line => valcols = line.split(",")
4(cols(0),cols(1).toFloat,
5 cols(2).toInt,cols(3).toFloat) }
6//Return zipcode, base loan, years, and rate
7.map{s= >
8vala = s._2
9for(i <- 1 to s._3)
10 a=a*( 1+s._4)
11 (s._1, a) }
12// Return zipcode and final loan
13vallocations = sc.textFile("zipcode.csv")
14//input with zipcode and city
15.map{s= >
16 valcols = s.split(",")
17 (cols(0), cols(1) }
18//Return zipcode and city
19.filter{ s => s._2 =="New York" }
20valoutput = loan.join(locations)
21.map{s= >
22if(s._2._1 >10000) ("Property Loan",10000)
23else if(s._2._1 >1000) ("Car Loan",1)
24else("Credit Debt",1) }
25//Return three categories based on the loan amount
26.reduceByKey( _+_ )
(a) A DISC application LoanType.scala1ArrayList<String> results0 = LoanSpec.read(inputFile1);
2ArrayList<Tuple4> results1 = LoanSpec.map1 (results0);
3ArrayList<Tuple2> results2 = LoanSpec.map2 (results1);
4ArrayList<String> results3 = LoanSpec.read(inputFile2);
5ArrayList<Map3> results4 = LoanSpec.map3 (results3);
6ArrayList<Map3> results5 = LoanSpec.filter1 (results4);
7ArrayList<Join2> results6 = LoanSpec.join1(results5, results2);
8ArrayList<Map1> results7 = LoanSpec.map4 (results6)
9ArrayList<Map1> results8 = LoanSpec.reduceByKey1 (results7)
(b) A transformed program LoanType.java with executable specifications
1publicArrayList<Map3> map3(ArrayList<String> input){
2ArrayList<Map3> output = new ArrayList<>();
3for(String item: input){
4 output.add( Map3.apply(item) );}
5returnoutput;}
(c) Specification implementation of map3inLoanTypeSpec.java
1publicclassMap3 {
2static final Map3 apply(String line2) {
3Stringcols[]=line2.split(",");
4return new Map3(cols[0],cols[1]); }
(d) The extracted UDF from lines 14 to 16 of Figure 2a is represented as Map3.java
Figure 2: Example code transformation and framework abstraction
the program’s bytecode, iteratively generates new inputs by mu-
tating several bits or bytes of the seed input, and collects coverage
feedback by executing the instrumented program with new inputs.
All inputs that exercise a new code branch are then be saved for
furthermutation.Theimplicitassumptionunderlyingsuchitera-
tive fuzzing is that the target program can run fast, (i.e., thousands
ofinvocations persecond);unfortunately,this assumptionisfalse
formanylonglatency applications suchasbigdataanalytics.For
example, initializing the Spark context in local model to initiate
adistributeddatapipelinetakes19seconds,whichcorrespondto
98% of the total execution time with a typical testing input. The
long latency prohibits the applicability of fuzzing for efficient test
generation. Besides, naively monitoring branch coverage in DISC
applications is unlikely to exercise application logic adequately,
since most binary code comes from the DISC framework imple-
mentation(e.g.,roughly700 KLOCfor ApacheSpark).Under this
circumstance,naiveattempttoincreasecodecoveragemayeventu-allyrunoutofmemory.Furthermore,randombyte-levelmutations
can hardly generate meaningful structured or semi-structured data
to explore application logic effectively.3 APPROACH
BigFuzzcontainsthreecomponentsthatworkinconcerttomake
coverage-guided fuzz testing tractable for big data analytics. Fig-ure 1 shows (A) abstraction of dataflow implementation using
source-to-source transformation with extracted user-defined func-
tions, discussed in Section 3.1, (B) two-level instrumentation for
coverage monitoring, discussed in Section 3.2), and (C) input muta-
tionsgearedtowardsbigdataanalyticerrorsbasedonourempiricalstudy,discussedinSection3.3.Thisapproachisbasedontheinsightthat(1)wecanreducelonglatencyofDISCapplicationsbyabstract-ingdataflowimplementationinaDISCframeworkusingexecutable
specifications and (2) we can focus on exercising application logic
ratherthantheentireframeworkbymonitoringcodecoverageof
user-defined functions in tandem with equivalence classes of ab-
stracted dataflow behavior. Although BigFuzzis designed for Spark
programs, its key idea can generalize to other DISC frameworkssuch as Hadoop by rewriting the dataflow operator APIs to our
current set of corresponding specification implementation.
3.1 Framework Abstraction for Fuzzing
As discussed in Section 2, DISC applications have high latency,
making them not suitable for traditional fuzz testing because they
724Table 1: Dataflow Operator and Corresponding Equivalence Classes
Spark Dataflow Operator Transformed Operator Equivalences Classes
def filter(udf:T→ Boolean): RDD[T] ArrayList<T> filter (ArrayList<T> Input) F1: Non-Terminating: ∃t.ud f(t)=true
Return an RDDthat satisfies a predicate udf:T→Boolean Return an ArrayList of elements passing udf F2: Terminating: ∃t.ud f(t)=fa l s e
whereudf:T→Booleean is implemented in filter
def join[W](other: RDD[(K,W)]):Rdd[(K,(V,W))] ArrayList<T> join (ArrayList<T1> L, ArrayList<T2> R) J1: Non-Terminating: ∃tL,tR.tL,key=tR,key
Return an RDDcontaining all pairs of elements with Return an ArrayList of elements from left ArrayList tL∈LJ2: Terminating: ∃tL,∀tR.tL,key!=tR,key
matching keys in thisandother RDDs. and right ArrayList tR∈R, with matching keys tL,key=tR,keyJ3: Terminating: ∃tR,∀tL.tR,key!=tL,key
def map[U](udf:T→U) ArrayList<T> map (ArrayList<U> Input)
Return a new RDD by applying udf:T→ U Return a new ArrayList by applying a udf:T→ Utothis M: Non-Terminating: always non-terminated
tofthis RDD. ArrayList whereudf:T→ Uis implemented in map.
def reduceByKey(udf:(V,V) →V) : RDD [K,V] ArrayList<T> reduceByKey (ArrayList<T> Input)
Merge the values for each key using an associative Merge the values for each key using udf:(V,V) →V R: Non-Terminating: always non-terminated
reduce function. whereudf:(V,V) →Vis implemented in reduceByKey
spendseveralsecondsjusttoinitializeSpark’sexecutioncontextfor
each run. Theoretically, the long start-up latency can be somewhat
reduced by sharing one Spark execution environment for multiple
runs;however,suchpracticeisstillnotenoughtoachievemillionsof
executionsperminute,becauseeachrunstillneedstopassthrough
a data partitioner, a query optimizer, a job scheduler, and a data
serializer/deserializer, etc.
InDISCframeworks,theimplementationofdataflowandrela-
tionaloperatorsisinfluencedbyanduniversallyagreeduponthe
semantics of such operators [ 68]. For example, although a dataflow
operator joinmay have a specialized physical implementation in
each framework (e.g., hash join), it has the same consistent logical
semantics across all DISC frameworks. BigFuzztakes advantage of
this observation, rewrites a DISC application into an equivalent
applicationthatusesdataflowspecifications,andmonitorsdifferent
equivalence class coverage of dataflow operations. For example,
filterhastwoequivalenceclasses—onepassingthefilterpredi-
cateandtheothernotpassingthefilter.Becausedataflowoperators
aredeterministic andstate-less [72], the transformed program is
guaranteed to be equivalent to the original program. For example,map{x => (x,1)}
willalwaysgivethesameoutputforthesame
input for both the spec-based program and the original program.
We map each dataflow operator’s implementation to a corre-
spondingsimplifiedyetsemantically-equivalentimplementation,
which we call executable specifications. Such specifications help
eliminatethedependencyontheframework’scode,transforming
aDISCapplicationintoanequivalent,simplifiedJavaprogramthat
can be invoked numerous times in a fuzzing loop.
BigFuzzautomatesthisprocessofrewritingintwosteps:(1)UDF
extractionand(2)sourcetosourcetransformation.Figure2illus-
tratesthisprocessusinganexampleDISCapplicationthatidentifiesthefrequencyofeachloantypewithinametropolitanarea.Thispro-
gram is a variation of one of the DISC Benchmark [ 38]. We formu-
lateadistributed,RDD-basedimplementationusingSpark’sAPIs( 
in Figure 2a) to a simplified, executable specification of mapin Fig-
ure2c.Table1showsafewsamplemappingsbetweenSparkRDD’s
dataflowimplementationAPIs,equivalentspec-implementations
usingArrayList ,andasetofcorrespondingequivalenceclasses
for each dataflow operator.Step 1. User-Defined Function (UDF) Extraction.
To re-write
aDISCapplicationtouseexecutablespecificationsonly, BigFuzzde-
composes the application into two components: (1) a direct acyclic
graph(DAG)ofdataflowoperatorsand(2)alistofcorresponding
UDFs.Internally, BigFuzzdecompilesthebytecodeoftheoriginalapplication into Java source code and traverses Abstract Syntax
Tree(AST)tosearchforamethodinvocationcorrespondingtoeach
dataflow operator. The input arguments of such method invoca-
tions represent the UDFs, which are stored as separate Java classes
as shown in Figure 2d.Step2.SourcetoSourceTransformation.
BigFuzzusestheDAG
extractedin thepreviousstepto reconstructtheDISCapplication
inthesame,interconnecteddatafloworderusingexecutablespecifi-cations.Suchdataflowspecimplementationtakesinan
ArrayList
object as input, applies the corresponding UDF on each elementof the input list, and returns an output
ArrayList . For example,
classLoanSpec.map3 (in Figure 2b) represents the equivalent
spec implementation using ArrayList that corresponds to map
in Figure 2a. It takes in results3 from its upstream opera-
tors and returns an ArrayList result4 for downstream operator,
LoanSpec.filter1 .BigFuzzselects the corresponding UDFs from
the list of UDFs extracted from step 1 and weaves them with the
equivalent specifications shown in column 2 of Table 1. For exam-
ple,Javaclass Map3hasmethod applymappingtotheoriginalUDF
in Figure 2a, and this method is invoked on each element of the
input list as seen in Figure 2c.
The above rewriting from a Spark application in Scala or Java
to an equivalent Java application reduces the latency of running a
DISCapplication,whileretainingthesamesemantics.Italsomakes
it easier to collect guidance metrics such as branch coverage byleveraging existing tools JQF [
55], which takes Java bytecode as
input and collects various guidance metrics for fuzz testing.
3.2 Application Specific Coverage Guidance
Priorworkfindsthatbranchcoverageisaneffectiveguidancemech-
anism for feedback-guided fuzz testing, pushing test generationtowards hard-to-reach corners [
17,44,56]. Generally, feedback-
guided fuzzing techniques instrument a program’s bytecode to
labeleachconstituentbranchandifaninputexercisesapreviously-
unseenbranchoftheprogram,thisinputisappendedinaninput
queue and the branch coverage is fed back into the fuzzer.
However,weobservethatsuchbranchcoverageguidancemech-
anismcannotbeappliedtofuzztestingofbigdataanalyticsfortwo
reasons.First,itcannotdifferentiateuser-definedfunctionsfrom
framework code and can thus push test generation naively toward
exploring the internals of DISC framework, as opposed to applica-
tion logic. Second, it cannot effectively monitor different equiva-
lenceclassesofdataflowoperatorsthoughpriorstudies[ 38,45,52]
argue that numerous errors originate from untested equivalence
725Table 2: Data Collection for Error Type Study.
Keyword TotalInspected
apache spark exception 2430 top 150
StackOverflow-Spark apache spark error 3780 top 200
apache spark wrong/
unexpected/inconsistent 143 143
result/output
hadoop exceptions 2567 top 100
hadoop error 9585 top 100
StackOverflow-Hadoop hadoop wrong output 370 top 50
hadoop wrong result 226 top 50
hadoop unexcepted/
inconsistent result 39 39
Github SparkContext 99 99
classesofdataflowoperators.For example,whentestingoperator
join, it is important to test three equivalence classes: (J1) there
exists a key that appears in both tables, passing the joined result
to the next operator, (J2) an input record in theleft table does not
haveamatchingkeyontherighttable,terminatingitsdataflow,
and(J3)aninputintherighttabledoesnothaveamatchingkey
on the left, terminating its data flow, discussed in Table 1.
Toaddressthesetwoproblems, BigFuzzdesignsatwolevelinstru-
mentationandmonitoringmethodforapplicationspecificcoverage
guidance. The key insight here is that BigFuzzmonitors regular
branchcoverageforuser-definedfunctionsonlyandfordataflow
operators,itmonitorsatthelevelofequivalenceclasses.Below,wedescribe how we extend
TraceEvent in JQF [55] to monitor which
equivalence classes are exercised for individual dataflow operators.
TraceEvent in JQF. BigFuzzis built on top of JQF [ 55], a Java-
basedfuzztestingframeworkthatusesASM[ 5]toinstrumentJava
bytecode on-the-fly as classes are loaded by the JVM. JQF instru-
ments all application classes by injecting a static method call with
auniqueidentifieraftereverybytecodeinstruction.Itfocuseson
control flow instructions such as method calls (e.g. INVOKESTATIC ,
INVOKEINTERFACE ,etc.)andbranchinginstructions(e.g. IF_CMPNE ,
GOTO, etc.). JQF collects these instructions and groups them to a
higher-level abstraction called TraceEvent (e.g.,CallEvent and
BranchEvent), which are then emitted to its coverage logger.DataFlowEvent in BigFuzz.
To keep track of equivalence class
coverageforindividualdataflowoperators, BigFuzzextendsTraceE-
ventinJQFandcreatesaspecific DataFlowEvent .Inadditiontoan
identifier, DataFlowEvent has an additional BooleanorInteger
variable to keep track of which subset of equivalence classes is
exercised by the corresponding dataflow operator. For example,
FilterEvent isaspecific DataFlowEvent classforkeepingtrackof
whichequivalenceclasseisactivatedfor filter.“FilterEvent(arm
=1 )” represents the non-terminating equivalence class, where the
filter predicate holds true and individual data records thus passthrough the filter predicate. “
FilterEvent(arm = 0) ” indicates
the other terminating case, where the filter predicate holds false
https://stackoverflow.com/search?q=apache+spark+exception
https://stackoverflow.com/search?q=apache+spark+errorhttps://stackoverflow.com/search?q=hadoop+exceptionshttps://stackoverflow.com/search?q=hadoop+errorhttps://stackoverflow.com/search?q=hadoop+wrong+outputhttps://stackoverflow.com/search?q=hadoop+wrong+resulthttps://stackoverflow.com/search?q=hadoop+wrong+unexcepted+resulthttps://stackoverflow.com/search?q=hadoop+inconsistent+resultand thus individual data records stop at this filter.BigFuzzin-
struments“ TraceLogger.get().emit(new FilterEvent(arm)) ”
in specification implementation of filterto emitFilterEvent
with a specific arm to the trace logger. In this way, BigFuzzretains
the DISC framework’s behavior on the original application code,
while abstracting its coverage guidance mechanism to the level of
equivalence classes for individual dataflow operator uses.
Coverage Guidance forUser-Defined Function. DISC applica-
tiondeveloperwritesapplicationlogicintermsofuser-definedfunc-
tions(UDFs)andconnectsthemusingdataflowoperators.These
UDFs are standard library based Scala or Java implementations.
To restrict normal coverage guidance to the body of UDFs (e.g.,
Figure2d), BigFuzzusesaselectiveinstrumentationschemeinASM,
whileignoringallotherdependentlibraries.Thiscombinationof
monitoringdataflowequivalencecoveragetogetherwithcontrol
flow events in the body of UDFs constitutes the joint dataflow and
user-definedfunctionpathcoverage(JDUpathcoverage),which
essentially represents the behavior of application logic.
3.3 Mutations for Big Data Analytics
Infeedback-guidedfuzzing,commonlyusedinputmutationsareeither bit-level or byte-level mutations in which random bits (or
bytes) are flipped in an input represented as a series of bits [ 44,55,
56]. The example program in Figure 2 takes as an input string that
containscomma-separaterowentries,whereeachentrycontains
the zipcode of borrower, the loan amount, the number of yearssince the loan was issued, and the interest rate respectively (e.g.,
90095,23000,7,0.045 ).Whentraditionalfuzzingisappliedtothis
example program, if no seed is provided, it may first generate aseries of random bits (e.g.,
0010 1010, which maps to a character
‘∗’).Afterwards, thisinputismutated byflippingseveral bits(e.g.,
0000 1010, which is the character ‘\n’). Both cases above would
generate meaningless inputs that fail at the program entry and
arethusincapableofadvancingthecoveragegoals.Ifthefuzzing
process starts with a user-provided seed input, it will take thisseed as bit series and flip several bits at a random position. Inthis way, traditional fuzzing can easily find data format errors
when the program terminates at a earlier stage; however, it can
hardly generate meaningful data that drives the program to a deep
execution path since bit-flipping is more likely to destroy the data
formatordata type.Infact,ourexperimentfinds thatover90%of
inputs generated by random fuzzing fail at the entry point without
exercising code further.
In contrast, BigFuzzdesigns a two-fold approach towards mutat-
inginputs.First,ittriestogeneratevalidinputs,suchthattheinputs
are consistent with the input-parsing logic of the program. Second,
it introduces record-level schema-aware mutations —modifying data
with respect to the structured data types as well as value ranges.
Unlikerandombit-levelmutationsthatproduceunnaturalinputs,
each of the schema-aware mutations mimics a real-world error in
DISCapplicationsthatmayleadtoprogramcrashesorfailuresat
runtime.Tothisextent,weextensivelyinvestigateDISCapplication
errors posted on popular Q/A forums and code repositories.
A Study of Common Error Types. To collect real-world DISC
applicationerrors,wefirstperformedakeyword-basedsearchon
StackOverflow Q/A forum and Github repositories using Spark
726Table 3: Common Error Types in DISC Applications
Type Portion Example Fix Mutation
Type mismatch 16.28% Data type is not double as expected by Spark’s Kmeans[6] Change type M2
Illegal data for UDF 9.30% NullPointerException caused by null values[7] Check UDF M4
Split-related errors 11.63% The user uses .split("[ ]") when .split("\[\]") is expected[8] Change delimeter M3,M4
Incorrect column access 16.28% Column access el(24).sum.toDouble where el.len=22 [9] Check data length M5
Incorrect offset access 2.35%Used substring(1,11) instead of substring(0,10)[9] Check offset M6
Incorrect code logic 11.63% The user uses a mutable data when it shouldn’t be used[10] Check code M1
Incorrect API usage 11.63% To match columns, equalTo API is expected in join operation[11] Check API usage M1
Join-related errors 4.70%Join gives null values, leading to a NullPointerException in map[12] Check data M1
Semantic errors 9.30%Minimum word appearance count in spark word2vec model is not met[13] Change API N/A
Framework bugs 6.90%The expected result is a single row but the user got two lines[14] Update library N/A
libraries. Table 2 shows the number of posts and issues for each
keyword search. As for StackOverflow, we studied both Spark- and
Hadoop-related posts and manually examined 787 posts in total.
Weremovedpostsrelatedtoperformanceorconfigurationerrors
to focus on analyzing posts reflecting either application or data
errors. As for Github, we inspected 99 projects that use the Apache
Spark framework and their code repositories and bug reports.
We examined the accepted answers and comments (if any) to
understandtherootcauseofunderlyingerrorsandsummarizetheir
solutions. We then distilled and grouped these root causes into ten
categories, each reflecting a unique underlying programming issue.
Table 3 shows the error type, % of posts for this error type, a repre-
sentative example, and a solution to fix the error. For instance, the
mostfrequentlyencounterederrortypeisrelatedtoanincorrect
columnaccesswhichcomprisesof16.28%oftotalerrors.Arepre-
sentative example of such error is when a user accesses the 25th
column(el(24).sum.toDouble )fromdatawithonly22columns
(el.len == 22 ).Thenextmostfrequentlyoccurringerror(16.28%)
is whenthe inputdoes not conformto theexpected data type e.g.,
arecord entrydoesnot complywith date.toDouble,resulting in
NumberFormatException.
Error-Type Guided Schema-Aware Mutations. Instead of bit-
level mutations, BigFuzzuses a user-defined schema to perform
coarse-grained,record-levelmutations.Intheschema,ausercan
indicate the number of columns, data type, and data distributionfor each column of the input data. The following code snippet
representsasampleuser-providedschemaforinput loaninFigure
2a, which dictates that each input entry comprises of four comma-
separated columns: the first column must be a 5-bit number string
with prefix “900”, the second column must be a random number
with float type, the third column is an integer within range [0-30],
andthelastcolumnisafloatnumberwithin0-1.Fromsuchschema,
wecanderivevalidinputconstraintswithrespecttodataformat,
data type, and data distribution.
number string[900xx],float[0-2128],integer[0-30],float[0-1]
Basedonourstudy,wedesignsixmutationoperationsM1-M6
asshowninTable3toreflecttheirassociationwitheachrealworld
error type. We enumerated these mutation types below:
•Data DistributionMutation (M1) mutatesarecordtobe
eithervalidorinvalidintermsoftheallowedrangebasedon the data distribution given in the schema (e.g., an inte-ger value
10, corresponding to the range integer[0-30]
mutated to 25or-1).•Data Type Mutation (M2) modifies the data type of a se-
lected column, while keeping the same value (e.g., 20corre-
spondingto integer[0-30] canbemutatedto 20.0,leading
toNumberFormatException in line 5 Figure 2a).
•DataFormatMutation(M3) mutatesacolumn-separating
delimiter mentioned in the schema (e.g., replacing delimiter
“,” to “~”).
•DataColumn Mutation(M4) insertsoneorseveralchar-
acters(e.g.,replicating ArrayIndexOutOfBoundsException
inStackOverflowpostNo.45962453[ 15]whenarandom‘’
is inserted to data that is "Ctrl+A" separated).
•Null Data Mutation (M5) mutates the input row by re-
movingoneormorecolumns(e.g.,replicating NullPointer-
Exception in Stack Overflow post No.36015704 [ 7]b ya c -
cessing positions that do not exist).
•Empty Data Mutation (M6) mutates a random column
to an empty string, leading to StringIndexOutOfBound-
Exception caused by incorrect string operations.
Compared to random bit-level mutations, because these scheme-
awaremutationsareinspiredbyreal-worlderrors,theyaremore
effective for producing valid and invalid inputs, which we empiri-
cally demonstrate in Section 4.Combined Data Generation and Mutation.
Based on a user-
provided schema, BigFuzzautomatically constructs an application-
specificmutationgeneratorthatcombinesvalidinputgeneration
andsixerror-typeguidedmutations.Givenaseedinput, BigFuzz
will either: (1) randomly mutate the seed input or (2) randomly
generate valid inputs followed by mutating such inputs to increase
cumulative coverage. IT can run under any of the two options and
does not require having a valid seed. So a user may start BigFuzz
with any string as a seed. Empirically, as we discuss in Section 4,
starting with a valid seed does slightly improve performance by
avoidingcrashingtooearlyfromaninvalidinput.Forexample,ifa
validseedsuchas 90001,100.0,10,0.01 isprovidedforFigure2a,
BigFuzzresults in higher error detection than without. However,
even an ill-formatted string is given as a seed, BigFuzzdoes retain
high performance with its data generation option.
4 EVALUATION
Our evaluation seeks to answer the following research questions:
RQ1Is a widely used fuzzing tool such as AFL applicable to big
data analytics with long latency?
RQ2Doesframeworkabstractioneffectivelyspeedupfuzztesting?
727Table 4: Statistics on Subject Programs
Subject #o f #o f BigTest BigFuzz RandomFuzzA
ID Program Output Operators JDU Paths Covered Covered Covered
P1 Wordcount Find the frequency of words 3 222 1
P2 Commute Type People count using each form of transport for daily commute 6 11 8 11 9
P3 ExternalCall Find the frequency of words 3 424 1
P4 FindSalary Total income of individuals earning ≤$300 weekly 3 867 4
P5 StudentGrade List of classes with more than 5 failing students 5 14 6 14 6
P6 Movie Rating Total number of movies with rating ≥4 4 11 5 11 5
P7 InsideCircle Check whether the point (x,y) is in a circle 5 7 N/A 7 6
P8 MapString String mapping 1 1 N/A 1 1
P9 NumberSeries Find the numbers whose 3n+1 series’ length is 25 3 9 N/A 9 3
P10 AgeAnalysis Total number of people with different age ranges 3 9 N/A 9 4
P11 IncomeAggregation Average income per age range in a district 6 9 N/A 9 4
P12 LoanType The frequency of each loan type within a metropolitan are 4 6 N/A 6 5
RQ3Doesschema-awaremutationeffectivelyimprovecodecov-
erage and error detection capability?
RQ4How much improvement in applicability and error detection
doesBigFuzzachieve,comparedtoanalternativesymbolic
execution-based technique?
Benchmarks. Weusetwosetsofsubjectprogramsasbenchmarks.
They include twelve Spark programs written in Scala, listed in Ta-
ble 4. For these subjects, P1-P2 and P4-P6 are directly from prior
work[38],P3isfrom[ 71]andP12isreproducedbyusbasedonthe
informationofaStackOverflowpost[ 16].P7-P8arefromSparkex-
amples,andtheremainingprogramsarehandcraftedbytheauthors.
Wecompare the generatedtest inputs andtheir associatederror-
finding capabilities with two baselines: (a) conventional fuzzing
and(b)symbolicexecutionbasedtestingforbigdataanalytics[ 38],
which is publicly available on Github.
ExperimentalEnvironment. WeuseSpark’slocalrunningmode
toperformexperimentsonasinglemachinewithIntel(R)Core(TM)
i7-8750H 2.20GHz CPU and 16 GB of RAM running Ubuntu 16.04.
4.1 Faulty Benchmarks
To evaluate error detection capability, we inject code errors to the
subjectprogramsbymappingreal-worlderrortypesinTable3to
corresponding code modifications. Type mismatch errors and split-
related errors are injected by changing the required data type and
delimiter in the program. Changing an array or a string index can
injectincorrectarrayorstringaccesserrors.Toinjectincorrectlogic
errors,weswapbinaryoperatorsifarelationaloperatorappears
in a branch condition (e.g., the user uses "<" when ">" is expected)
or we replace a multiplication operator (e.g., a∗b) to a division
(e.g., a/b)toinduceadividebyzeroerror.Wealsoupdateconstants
orreplacesvariables.Forexample,ifweinjectalargenumberto
aredeceByKey thatdoesaccumulationbykey,anoverflowerror
mayoccurwhentheintermediatevalueisbeyondthecapacityof
itstype.Whenwereplaceadataflowoperatorsuchas joinwith
itscounterpartsuchas left join ,wemayreduceerrorsrelated
to join operator or incorrect API usage.
This error seeding process is done automatically through source
to source transformation on each subject. We traverse the abstract
syntax tree (AST) of each program and apply one of the aboveinjections to a random location if applicable. If code update can
be applied to multiple locations, we choose one location randomly.ThisnewlytransformedASTistranslatedintosource,sothatwecan
see the error location. In total, we create 52 error seeded versions.
4.2 Applicablity of AFL (RQ1)
Almost all fuzz and random testing techniques are built on the
assumption that the program under test can be executed millions
of times within a matter of hours. To quantify this limitation ofapplying naive fuzzing to DISC applications, we use AFL on the
twelve subject programs. AFL is a mature fuzzing tool designed forC/C++ [
17] and JQF makes AFL available for Java programs. When
using AFL with 9216M as memory and 100 seconds as timeout
setting,itrunsatanextr emelylowspeed 0to9.68execs_per_sec
(anAFLreportedmetrictoindicatethenumberoftestexpectations
invoked from a fuzzing loop per second). The extremely low speed
is because Spark applications spend significant time on setting up
aSparkcontext,whichattributestomostexecutiontime.Further,
as most binary code comes from DISC framework implementation
withmillionsoflinesofcode,AFL’sattempttoincreasecodecov-
erage eventually leads to running out of memory after only 70
executionsonaverage.Evenbeforeitdies,AFLwitharandomseedexploresonly18%oftheapplicationcodeonaverageforallsubjectsexceptP1,P3,andP8,whichtakeanunstructuredrandomstringas
input. This empirically demonstrate that naive fuzzing is too slow
and insufficient to generate meaningful structured data and reveal
DISC application errors.
4.3 Comparison against Random Fuzzing
Wecreatetwoseparatedowngradedversionsof BigFuzzbydisabling
frameworkabstractionanderror-typeguidemutationsrespectively.
Wecalltheversionwithoutframeworkabstractionas RandomFuzzM
asitretains mutation capabilityonly.Wecalltheversionwithout
error-type guided mutations as RandomFuzzA as it retains frame-
workabstraction capability only.
RQ2:SpeedupwithFrameworkAbstraction. Toassessspeedup
enabled by abstracting DISC frameworks in isolation, we use a
downgradedversion, RandomFuzzM ,whichdisablessourcetosource
transformation for framework abstraction. We measure the run-
ningtimeofboth BigFuzzandRandomFuzzM with1000iterations
forprogramsP1-P12.Theterm‘iteration’referstoasingletestexe-
cution invoked from a fuzzing loop. We repeat the experiment ten
728P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12104106108Time (ms)BigFuzz RandomFuzzM
Figure 3: Running Time with 1000 iterations
times and report the average results in Figure 3—Y axis is millisec-
onds in log scale. BigFuzzis significantly faster than RandomFuzzM ,
speeding up the fuzzing time by 78X to 1477X.
RQ3:CoverageandErrorDetectionImprovementwithError-
TypeGuidedMutation. For RQ3, to evaluate the benefit of error-
typeguidedmutationsinisolation,wecreateadowngradedversion
RandomFuzzA that disables error-type guided mutations. We as-
sess how fast BigFuzzandRandomFuzzA generate inputs exercising
more JDU paths within the same iteration limit. We run BigFuzz
andRandomFuzzA for 1000 iterations and report the cumulative
%ofexercisedJDUpathsand%ofdetectederrors.Werepeatthe
experiments four times and report averages.
Figures4and5reporttheresultswhenstartingfuzzingwithand
without a valid seed. Please note that BigFuzzdoesnotrequire a
usertoprovideavalidseedandcanrunundereitheroptionbutwepresentbothtoestimateitscapabilityaccurately.Underthenormalscenariowhen
RandomFuzzA isstartedwithoutavalidseed,itsover-
allperformanceislowerthanbeingbootstrappedwithavalidseed,becausemutatingavalidseedcanavoidearlycrashes.However,as
we discuss below, BigFuzzcan achieve similar performance with or
without a valid seed, demonstrating robustness.
Whenstartedwithoutavalidseed(thenormalscenario), BigFuzz
provides118to271%improvementinJDUpathcoverageincompar-
ison to starting RandomFuzzA without a valid seed, leading to 58 to
157%improvementinerrordetection.Whenfuzzingisstartedwith
avalidseed(thefavorablescenario), BigFuzzcanimproveJDUpath
coverage by 20 to 200%, which leads to 33 to 100% improvement in
detecting errors in comparison to RandomFuzzA with a valid seed.
The overall numbers of covered JDU paths among different runs
are reported in the rightmost two columns in Table 4. Because Big-
Fuzzis was to achieve 100% JDU path coverage for all benchmarks
exceptP4,wedidnotrun BigFuzzfor24hours,assuggestedinprior
work[42].Theuncoveredpathstartswithastringwhoselength
mustbelargerthan7;howeveritsintegervalueshouldbelessthan300.Longerexecutiontimemaycoverthispath.Formostprograms,
RandomFuzzA ’srandomlygenerateddatacanhardlyexerciseadeep
execution path or dataflow equivalence classes.
4.4 Comparison with Symbolic Execution
Based Testing
RQ4: Applicability. We assess how many Spark programs are
testable using BigFuzz, in comparison to an alternative symbolic
execution based approach, BigTest[38]. Symbolic execution-based
testing requires a symbolical interpretation of each dataflow op-
eratorusedintheprogramalongwiththeUDF.Theapplicability
ofsuchtechniquescouldbelimitedbythecapacityofunderlyingSMT solvers and the ability to completely represent the entire pro-
gramsymbolically.WereporttheresultsinTable4,where"N/A"
represents BigTestis not applicable.
BigFuzzcan be applied to twice as many programs as BigTest.
ForprogramsP1-P6, BigTestcangeneratetestinputssuccessfully,
whileitfailstorunonprogramsP7-P12.Weinvestigatethepublicly
available source-code of BigTestand find three primary reasons
behinditsinapplicabilityontheseprograms.Aswithmanysym-
bolic execution based test generation techniques, BigTestrestricts
its symbolic exploration of unbounded collections and loops toa user-defined bound K (default is 2). Some programs such as P9
requireahighvalueofloop-bound(K)toreachcriticallyimportant
JDU paths, which leads to inability of BigTestto maintain many
symbolicstates.Duringprogramdecomposition, BigTestextracts
each dataflow operator’s argument, assuming that the argument is
alwaysaUDF.However,inP7,apointertoaUDFispassedinstead
of the UDF itself, which results in incorrect UDF extraction. On
thecontrary, BigFuzzleveragesstatic-dereferencinginsuchcases.
Furthermore, in scala, a forloop iterating over a collection is com-
piled into an mapmethod call on the collection. We find such cases
in program P12 in which BigTestconsiders the mapmethod call
on collections as a dataflow operator resulting in incorrect DAG
interpretation.RQ4: Error Detection Capability.
We evaluate the error detec-
tion capability of BigFuzzin covering more JDU paths and generat-
inginputsthatleadtoerrorsthatcannotbefoundby BigTestfor
programs P1-P6 that both tools are applicable.
Columns BigTestandBigFuzzin Table 4 summarize the JDU
path coverage for BigTestandBigFuzzrespectively. For all the test
inputs generated by both tools, we manually inspect their covered
execution path in UDFs and dataflow equivalence classes. In terms
of tool setting, we set the user-specified bound Kas a default value
2 forBigTestand set the fuzzing iterations as 1000 for BigFuzz. For
all the subjects P1-P6 except P4, BigFuzzis able to achieve 100%
coverage on JDU paths within the number of iterations, leading to
33% to 100% improvement in path coverage compared to BigTest.
Table 5: Error Detection Capability of BigFuzzandBigTest
Subject Programs
P1 P2 P3 P4 P5 P6
Injected Errors 162467
BigTest 051243
BigFuzz 162467
Table 5 reports a comparison of error detection capability of
BigFuzzandBigTestintermsoffindingautomaticallyinjectederrors.
BigFuzzgeneratesinputstodemonstratealloftheinjectederrors
and detects 80.6% more injected errors than BigTeston average. In
addition, BigFuzzhas the unique capability of finding errors that
cannot be detected by BigTest.
In P1,BigTestwith default K=2setting cannotfind an inputfor
theruntimeoverflowwhenweinjectalargenumber2147483600to
reduceByKey because this error appears only when the minimum
appearance number of a word is larger than three. In P3, whenthe
filter(v._2>1) is replaced with filter(log10(V._2)>1) ,
BigTestfails to generate a constraint for this path that contains
an external method call on a symbolic value. The injected divide
729101102103020406080100P1
101102103020406080100P2
101102103020406080100P3
101102103020406080100P4
101102103020406080100P5
101102103020406080100P6
101102103020406080100
IterationsCumulative % of total JDU paths coveredP7
101102103020406080100
IterationsP8
101102103020406080100
IterationsP9
101102103020406080100
IterationsP10
101102103020406080100
IterationsP11
101102103020406080100
IterationsP12
RandomFuzzA with a valid seed BigFuzzwith a valid seed
RandomFuzzA without a valid seed BigFuzzwithout a valid seed
Figure 4: Joint Dataflow and UDF Coverage
101102103020406080100P1
101102103020406080100P2
101102103020406080100P3
101102103020406080100P4
101102103020406080100P5
101102103020406080100P6
101102103020406080100
IterationsCumulative % of total errors detectedP7
101102103020406080100
IterationsP8
101102103020406080100
IterationsP9
101102103020406080100
IterationsP10
101102103020406080100
IterationsP11
101102103020406080100
IterationsP12
RandomFuzzA with a valid seed BigFuzzwith a valid seed
RandomFuzzA without a valid seed BigFuzzwithout a valid seed
Figure 5: Error Detection Capability
by zero error in P2, as well as the injected type matching errors in
P4-P6arebeyond BigTestbecauseitsunderlyingSMTsolverfails
to generate concrete inputs that satisfy such path constraints.
5 RELATED WORK
Fuzz testing has gained popularity in both academia and industry
due to its black/grey box approach with a low barrier to entry [ 53].
Thekeyideaoffuzztestingoriginatesfromrandomtestgeneration
where random inputs are incrementally produced with the hope to
exercise previously undiscovered program behavior [28, 32, 54]
RandomTesting. Onedifficultyinpurerandomtestingisgenerat-
ingvalid inputs,especiallyforobject-oriented programs.JCrasher
uses Java reflection to understand the parameter space and the
typeofamethodundertestandgeneratesrandominputsaimingtoproduceaJavaexception[
28].Randoop[ 54]permutesmethod
sequences to construct valid input, executes the new sequence,andobservesregressionoruser-definedcontractviolations,while
eliminating those leading to redundant execution by keeping track
of the method sequences. EvoSuite also generates test suites to
revealprogramcrashesandconstructstestoraclesintheformof
assertions to check for the expected program behavior [32].Fuzz Testing.
Fuzz testing is similar to random test generation
inmanyaspects.Itmutatesaseedinputthroughits fuzzertoex-
posepreviouslyunseeninternalstatesoftheprogram.AFListhe
most widely used coverage-guided fuzzing tool [ 17]. Generally, tra-
ditional coverage-guided fuzz testing has limited efficiency andeffectiveness due to a vast space of inputs and unbounded pro-gram paths. Lemieux et al. identify rarely executed branches inthe program with AFL-generated inputs and then create custom
mutations so that the generated inputs gravitate toward exercising
rare branches [ 44]. As a result, it requires fewer fuzzing loops and
730achieveshighercoverageinlesstime.Otherapproachesincorpo-
ratesymbolicexecutioninfuzzingtoguidecarefulselectionand
mutation of the inputs, invoking unique program paths [ 26,64].
Padhye et al. incorporate the semantic validity of input mutations
in Zest [56]. Zest reduces the search space of inputs by mapping
bit-level changes to valid structural changes in the input.
Anotherangletominimizeunfruitfulfuzzingloopsistogenerate
only legal inputs for the program. Le et al. propose a grammar-
based fuzzing approach called Saffron that relies on a user-defined
grammar[ 43].Duringfuzzing,ifaninputgeneratedbythegram-
marleadstoaprogramfailure,Saffronreconstructsthegrammar
according to newly learned input specifications of the program.
Wang et al. leverage a user-provided grammar, but instead of ar-
bitrarymutations,theyintroducegrammar-specificmutationsto
diversify test inputs for tightly formatted input domains such as
XML and JSON [ 70]. Gopinath et al. highlight that the state-of-art
grammar-awarefuzzer dharma[19]isstilltwoordersofmagnitude
slower than a random fuzzer and suggest guidelines for efficient
grammar-awarefuzzing[ 36].Intheirfollow-upwork,theypresent
an approach to infer an input grammar from the interactions be-
tween an inputparser and input data [ 35]. In DISCapplications, a
largeproportionofprogramfailuresareduetoill-formattedinputs,
which are hard to know in advance and are not taken into account
during development. Therefore, grammar-aware fuzzing may not
be practical in revealing errors in DISC applications, because ifa user were to prescribe grammar rules up front, it may be too
restrictive to generate meaningful error-inducing inputs.
Almostallfuzzandrandomtestingtechniquesarebuiltonthe
assumptionthat theprogramundertestcanbeexecutedmillionsof
times within a matter of hours. In the domain of data-intensive scal-
ablecomputing,userapplicationsarebuiltontopofframeworks
(suchasApacheSparkorHadoopMapReduce)containingcomplex
distributed systems. Therefore, a single program run may take sev-
eralminutes,ifnothours,includingaconstantclusterspin-uptime.
Therefore, the performance and resource expense of state-of-art
fuzzing and random testing for DISC applications are prohibitive.
To speed up test exe cution while fuzzing, UnTracer [ 51] dynam-
ically strips out code-coverage instrumentation for lines of codethat have already been covered. For DISC applications, the over-
head is not due to instrumentation but indeed due to the extensive
framework code. BigFuzzis the first fuzzing tool that transforms
the target application by simplifying framework logic.SoftwareDebloating.
Codedebloatingtechniques[ 23,60,61,63]
strip off unnecessary logic or library functions that are not used
by an application with the primary motivation to reduce the attacksurfacesortoreducebinarysize.Unlikedebloatingtechniquesthat
removeunusedcodeviareachabilityprogramanalysis, BigFuzz’s
framework abstraction replacescritical dataflow operators with
semanticallyequivalentimplementationstoreducetheimpactof
bloated code for fuzz testing.SymbolicandConcolicExecution.
Symbolicexecutionhasbeen
extensivelyusedforadiversesetofusecases,includingautomatedtestgeneration,programverification,securityanalysis,andcodeop-timization.Itallowsprogrammerstoexecutetheirprogramsymbol-
ically to verify correctness [ 53]. Tools such as KLEE [ 25], Pex [66],and JavaPathFinder [ 69] brought symbolic execution to the fore-
frontofsystematictestgenerationbydiscoveringuncoveredpro-
gramregionsandusingconstraintsolverstogenerateadditional
test data to reveal faults in previously uncovered regions [41].
However,symbolicexecutionbasedtestingisoftenlimitedby
an enormousnumber ofprogram paths emergingfrom largecode.
Severalheuristics-basedapproachesaddressthisproblemofpath
explosion [ 22,24,47,62]. Burnim et al. leverage static analysis
toguidesymbolicexecutiontoward uncoveredpathstoprioritize
specific program paths [ 24]. As with any other heuristics-based
approach,thesetechniquesproducemanyfalsenegatives,asthey
prioritize exploration of certain program paths over others. Conse-
quently,itmayleadtolowtestquality(orfaultdetectionrate)of
thegeneratedtestsuite.Experimentalresultsfrompriorworkshow
that symbolic execution may have lower fault detection capability
than black-box fuzzing or random testing [29].
DISCapplicationsdependonmillionsoflinesofcodeinDISC
framework,whichmakesitinfeasibletonaivelyapplysymbolicex-ecutiontotheresultingcodeasis.Evenifheuristics-basedsymbolic
execution is used to model the entire DISC application’s binary
code, the resulting test suite would mostly concentrate on finding
thedefectsintheDISCframework,asopposedtofindingbugsin
application code.
Testing SQL and Data Analytics. Qex follows the traditional
symbolic execution based test generation playbook and maps a
SQL query to an SMT query [ 68]. It is loaded with custom theories
foreachrelationaloperator.Cosettemapsarelationalquerytoa
symbolicrepresentationbyeither(a)provingequivalenceamong
two queries or (b) generating counterexamples that explain con-flicting answers from two queries [
27]. Miao et al. leverage hard-
coded specifications of relational operators and generate database
rows to explain the output difference between two queries [ 50].
DOMINO [ 20] uses tailored, domain-specific operators based on
randomvaluestogeneratetestdataforrelationaldatabaseschemas.
Guptaetal.pivottheirmutation-basedtestgenerationtechnique
for SQL queries [ 39]. They define a set of mutations for selected
relational operators such as inner-join or join and specify rules
needed for each type of join to kill the mutant. SQL-integrated ap-
plications are widely used in practice and they invoke SQL queries
programmaticallyusingdatabaseconnectionssuchasODBC[ 18]
or JDBC [ 65]. Variants of symbolic execution are used to generate
both application inputs and database states [31, 57–59].
Relational database applications rarely use user-defined functions
(UDF),whichareprevalentinDISCapplications.Thustheabove
mentionedtoolscannotrevealfaultsfromtheinteractionofUDF
anddataflowoperatorsortheUDFalone,makingthemnotappli-
cable to DISC applications.
TestingDISCApplications. Today,DISCapplicationsare,almost
always, composed of both UDF and dataflow& relational operators.
Gulzaretal.model thesemanticsoftheseoperatorsinfirst-order
logicalspecificationsalongsidewiththesymbolicrepresentation
ofUDFs[ 38]andgenerateatestsuitetorevealfaults.PriorDISC
testing approaches either do not model the UDF or model the spec-
ificationsofdataflowoperatorspartially[ 45,52].Lietal.proposea
combinatorialtestingapproachthatautomaticallyextractsinput
domaininformationfromschemaandboundsthescopeofpossible
input combinations [46].
731However, all these symbolic execution use a heuristic (loop iter-
ation bound K) during path exploration, which may lead to false
negatives and they are also limited in applicability due to their
symbolic execution scope.
TestingLargeScaleSystems. Gulzaretal.studytheuseofdiffer-
ential testing for large-scale, end-to-end systems instead of tra-ditional unit testing and find unit testing either incomplete orinfeasible in practice due to the system’s complexity [
37]. They
furtherobservethatmorethan40%oftestsinreal-worldproduc-
tionsoftwaretakebetween15minutestoseveralhours,stressing
theinfeasibilityoffuzztestingonlarge-scale,long-latencysystems.
Other studies at Microsoft and Google concur exceedingly long
runningtesttimes(intheorderofhours)ontheirproducts,such
as MicrosoftWindows [ 40,67]. Thesestudies further validate our
hypothesis that fuzz testing that assumes fast, repetitive re-runs is
not suitable for such large systems with long latency.Stateless Computation.
Long startup time is a well-known prob-
lem for DISC applications and JVM applications in general. Hot-
Tub [48] reduces latency by amortizing the warm-up overhead
over the lifetime of a cluster node instead of over a single job.
RESTler [ 21] is a stateful fuzzer that analyzes the API specification
ofacloudserviceandgeneratessequencesofrequeststhatautomat-
ically test the service through its API. Different from these stateful
computations,dataflowoperationsarestatelessanddeterministic,
whichisthekeyinsightthat BigFuzzusestocreateasemantically
equivalent, fuzzing-friendly program.
6 CONCLUSION
Fuzztestinghasemergedasoneofthemosteffectivetestgenera-
tiontechniques.ToadaptfuzzingtoDISCapplicationswithlong
latency, we propose BigFuzzthat leverages (1) dataflow abstraction
usingsource-to-sourcetransformation,(2)tandemmonitoringof
equivalence-classbaseddataflowcoveragewithcontrolflowcov-
erageinuser-definedfunctions,and(3)schema-awaremutations
that reflect real world error types. BigFuzzachieves 78 to 1477X
speed-up compared to random fuzzing, improves application code
coverage by 20 to 271%, leading to 33 to 157% improvement in
detecting application errors.
Acknowledgement
WethankKoushikSenforhisfeedbackandtheanonymousreview-
ersfortheircomments.TheparticipantsofthisresearchareinpartsupportedbyNSFgrantsCCF-1764077,CCF-1527923,CCF-1723773,
ONR grant N00014-18-1-2037, Intel CAPA grant, Samsung grant,
GooglePhDFellowship,andAlexandervonHumboldtFoundation.REFERENCES
[1] 2020. https://hadoop.apache.org/.
[2] 2020. https://spark.apache.org/.[3] 2020. https://stackoverflow.com/.[4] 2020. https://github.com/.[5] 2020. https://asm.ow2.io/.[6] 2020. https://stackoverflow.com/questions/37525136/.[7] 2020. https://stackoverflow.com/questions/36015704/.[8] 2020. https://stackoverflow.com/questions/52083828/.[9] 2020. https://stackoverflow.com/questions/49505241/.
[10] 2020. https://stackoverflow.com/questions/41708814/.[11] 2020. https://stackoverflow.com/questions/56478820/.[12] 2020. https://stackoverflow.com/questions/41143862/.[13] 2020. https://stackoverflow.com/questions/32028729/.[14] 2020. https://stackoverflow.com/questions/36131942/.[15] 2020. https://stackoverflow.com/questions/45962453/.[16]
2020. https://stackoverflow.com/questions/59977879/
is-there-any-convenient-way-to-do-the-debugging-for-spark-program.
[17] 2020. American Fuzz Loop. http://lcamtuf.coredump.cx/afl/.[18]
2020. MicrosoftOpenDatabaseConnectivity(ODBC). https://msdn.microsoft.
com/en-us/library/ms710252(v=vs.85).aspx.
[19] 2020. Mozilla Security - dharma. https://github.com/mozillasecurity/dharma.[20]
A. Alsharif, G. M. Kapfhammer, and P. McMinn. 2018. DOMINO: Fast andEffective Test Data Generation for Relational Database Schemas. In 2018 IEEE
11th International Conference on Software Testing, Verification and Validation
(ICST). 12–22. https://doi.org/10.1109/ICST.2018.00012
[21]V. Atlidakis, P. Godefroid, and M. Polishchuk. 2019. RESTler: Stateful REST API
Fuzzing. In 2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE). 748–758.
[22]Domagoj Babić, Lorenzo Martignoni, Stephen McCamant, and Dawn Song.2011. Statically-Directed Dynamic Automated Test Generation. In Proceed-
ings of the 2011 International Symposium on Software Testing and Analysis (IS-STA ’11). Association for Computing Machinery, New York, NY, USA, 12–22.
https://doi.org/10.1145/2001420.2001423
[23]Jaspreet Arora Guoqing Harry Xu Miryung Kim Bobby Bruce, TianyiZhang. [n.
d.]. JShrink:In-DepthInvestigationintoDebloatingModernJavaApplications.
InACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering
(ESEC/FSE ’20).
[24]J. Burnim and K. Sen. 2008. Heuristics for Scalable Dynamic Test Generation. In
2008 23rd IEEE/ACM International Conference on Automated Software Engineering.
443–446.
[25]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDI’08). USENIX Association, USA, 209–224.
[26]Sang Kil Cha, Maverick Woo, and David Brumley. 2015. Program-Adaptive
MutationalFuzzing.In Proceedingsofthe2015IEEESymposiumonSecurityand
Privacy (SP ’15). IEEE Computer Society, USA, 725–741. https://doi.org/10.1109/
SP.2015.50
[27]Shumo Chu, Chenglong Wang, Konstantin Weitz, and Alvin Cheung. 2017.
Cosette: An Automated Prover for SQL. In CIDR.
[28]ChristophCsallnerandYannisSmaragdakis.2004. JCrasher:anautomaticrobust-
ness tester for Java. Software: Practice and Experience 34, 11 (2004), 1025–1050.
[29]Marcelo d’Amorim, Carlos Pacheco, Darko Marinov, Tao Xie, and Michael D.
Ernst.2006. Anempiricalcomparisonofautomatedgenerationandclassification
techniquesforobject-orientedunittesting.In ASE2006:Proceedingsofthe21st
AnnualInternationalConferenceonAutomatedSoftwareEngineering.Tokyo,Japan,
59–68.
[30]JeffreyDeanandSanjayGhemawat.2004.MapReduce:SimplifiedDataProcessingonLargeClusters.In Proceedingsofthe6thConferenceonSymposiumonOperating
Systems Design & Implementation - Volume 6 (OSDI’04). USENIX Association, 1.
[31]MichaelEmmi,RupakMajumdar,andKoushikSen.2007.DynamicTestInputGen-erationforDatabaseApplications.In Proceedingsofthe2007InternationalSympo-
siumonSoftwareTestingandAnalysis(ISSTA’07).AssociationforComputingMa-
chinery, New York, NY, USA, 151–162. https://doi.org/10.1145/1273463.1273484
[32]Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic Test Suite Gen-
erationforObject-OrientedSoftware.In Proceedingsofthe19thACMSIGSOFT
Symposiumandthe13thEuropeanConferenceonFoundationsofSoftwareEngi-
neering (ESEC/FSE ’11). Associationfor ComputingMachinery, NewYork,NY,
USA, 416–419. https://doi.org/10.1145/2025113.2025179
[33]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
ProgrammingLanguageDesignandImplementation (PLDI’05).ACM,NewYork,NY, USA, 213–223. https://doi.org/10.1145/1065010.1065036
[34]
PatriceGodefroid,MichaelY.Levin,andDavidAMolnar.2008. AutomatedWhite-
boxFuzzTesting.In NetworkDistributedSecuritySymposium(NDSS).Internet
Society. http://www.truststc.org/pubs/499.html
732[35] Rahul Gopinath, Björn Mathis, and Andreas Zeller. 2019. Inferring Input Gram-
mars from Dynamic Control Flow. arXiv:cs.SE/1912.05937
[36]Rahul Gopinath and Andreas Zeller. 2019. Building Fast Fuzzers.
arXiv:cs.SE/1911.07707
[37]Muhammad Gulzar, Yongkang Zhu, and Xiaofeng Han. 2019. Perception and
practicesofdifferentialtesting.In 2019IEEE/ACM41stInternationalConferenceon
Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 71–80.
[38]MuhammadAliGulzar,ShaghayeghMardani,MadanlalMusuvathi,andMiryung
Kim.2019. White-BoxTestingofBigDataAnalyticswithComplexUser-DefinedFunctions.In Proceedingsofthe201927thACMJointMeetingonEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering
(ESEC/FSE 2019). Association for Computing Machinery, New York, NY, USA,
290–301. https://doi.org/10.1145/3338906.3338953
[39]B.P.Gupta,D.Vira,andS.Sudarshan.2010.X-data:Generatingtestdataforkilling
SQL mutants. In 2010 IEEE 26th International Conference on Data Engineering
(ICDE 2010). 876–879.
[40]Kim Herzig, Michaela Greiler, Jacek Czerwonka, and Brendan Murphy. 2015.
The Art of Testing Less without Sacrificing Quality. In Proceedings of the 37th
InternationalConferenceonSoftwareEngineering-Volume1(ICSE’15).IEEEPress,
483–493.
[41]James C. King. 1976. Symbolic Execution and Program Testing. Commun. ACM
19, 7 (July 1976), 385–394. https://doi.org/10.1145/360248.360252
[42]GeorgeKlees,AndrewRuef,BenjiCooper,ShiyiWei,andMichaelHicks.2018.EvaluatingFuzzTesting.In Proceedingsofthe2018ACMSIGSACConferenceon
ComputerandCommunicationsSecurity (CCS’18).AssociationforComputingMachinery, New York, NY, USA, 2123–2138. https://doi.org/10.1145/3243734.
3243804
[43]Xuan-Bach D.Le, Corina S. Pasareanu,Rohan Padhye, David Lo,Willem Visser,
and Koushik Sen. 2019. Saffron: Adaptive Grammar-based Fuzzing for Worst-
Case Analysis. ACM SIGSOFT Software Engineering Notes 44, 4 (2019), 14. https:
//doi.org/10.1145/3364452.3364455
[44]Caroline Lemieux and Koushik Sen. 2018. FairFuzz: a targeted mutation strategy
forincreasinggreyboxfuzztestingcoverage.In Proceedingsofthe33rdACM/IEEE
InternationalConferenceonAutomatedSoftwareEngineering,ASE2018,Montpellier,
France,September3-7,2018,MarianneHuchard,ChristianKästner,andGordon
Fraser (Eds.). ACM, 475–485. https://doi.org/10.1145/3238147.3238176
[45]Kaituo Li, Christoph Reichenbach, Yannis Smaragdakis, Yanlei Diao, and
ChristophCsallner.2013. SEDGE:Symbolicexampledatagenerationfordataflow
programs.In AutomatedSoftwareEngineering(ASE),2013IEEE/ACM28thInter-
national Conference on. IEEE, 235–245.
[46]Nan Li, Yu Lei, Haider Riaz Khan, Jingshu Liu, and Yun Guo. 2016. Applying
Combinatorial Test Data Generation to Big Data Applications. In Proceedings of
the 31st IEEE/ACM International Conference on Automated Software Engineering
(ASE 2016). Association for Computing Machinery, New York, NY, USA, 637–647.
https://doi.org/10.1145/2970276.2970325
[47]YouLi,ZhendongSu,LinzhangWang,andXuandongLi.2013. SteeringSymbolicExecutiontoLessTraveledPaths. SIGPLANNot. 48,10(Oct.2013),19–32. https:
//doi.org/10.1145/2544173.2509553
[48] David Lion, Adrian Chiu, Hailong Sun, Xin Zhuang, Nikola Grcevski, and Ding
Yuan. 2016. Don’t Get Caught in the Cold, Warm-up Your JVM: Understand and
EliminateJVMWarm-upOverheadinData-ParallelSystems.In 12thUSENIXSym-
posium on Operating Systems Design and Implementation (OSDI 16). USENIX As-
sociation, Savannah, GA, 383–400. https://www.usenix.org/conference/osdi16/
technical-sessions/presentation/lion
[49]Valentin Manes, HyungSeok Han, Choongwoo Han, sang cha, Manuel Egele,
Edward Schwartz,and MaverickWoo.2019. TheArt, Science, andEngineering
of Fuzzing: A Survey. IEEE Transactions on Software Engineering PP (10 2019),
1–1. https://doi.org/10.1109/TSE.2019.2946563
[50]Zhengjie Miao, Sudeepa Roy, and Jun Yang. 2019. Explaining Wrong QueriesUsing Small Examples. In Proceedings of the 2019 International Conference on
Management of Data (SIGMOD ’19). Association for Computing Machinery, New
York, NY, USA, 503–520. https://doi.org/10.1145/3299869.3319866
[51]Stefan Nagy and Matthew Hicks. 2019. Full-speed fuzzing: Reducing fuzzing
overhead through coverage-guided tracing. In 2019 IEEE Symposium on Security
and Privacy (SP). IEEE, 787–802.
[52]Christopher Olston, Shubham Chopra, and Utkarsh Srivastava. 2009. Generating
ExampleDataforDataflowPrograms.In Proceedingsofthe2009ACMSIGMOD
International Conference on Management of Data (SIGMOD ’09) . ACM, New York,
NY, USA, 245–256. https://doi.org/10.1145/1559845.1559873
[53]Alessandro Orso and Gregg Rothermel. 2014. Software Testing: A Research
Travelogue(2000–2014).In ProceedingsoftheonFutureofSoftwareEngineering
(FOSE2014).AssociationforComputingMachinery,NewYork,NY,USA,117–132.
https://doi.org/10.1145/2593882.2593885
[54]C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. 2007. Feedback-Directed
RandomTestGeneration.In 29thInternationalConferenceonSoftwareEngineering
(ICSE’07). 75–84.[55]Rohan Padhye, Caroline Lemieux, and Koushik Sen. 2019. JQF: Coverage-Guided Property-Based Testing in Java. In Proceedings of the 28th ACM SIG-
SOFT International Symposium on Software Testing and Analysis (ISSTA 2019).
Association for Computing Machinery, New York, NY, USA, 398–401. https:
//doi.org/10.1145/3293882.3339002
[56]Rohan Padhye, Caroline Lemieux, Koushik Sen, Mike Papadakis, and YvesLe Traon. 2019. Semantic Fuzzing with Zest. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis (ISSTA2019). Association for Computing Machinery, New York, NY, USA, 329–340.
https://doi.org/10.1145/3293882.3330576
[57]Kai Pan, Xintao Wu, and Tao Xie. 2011. Database State Generation via Dy-namic Symbolic Execution for Coverage Criteria. In Proceedings of the Fourth
International Workshop on Testing Database Systems (DBTest ’11) . Association
for Computing Machinery, New York, NY, USA, Article Article 4, 6 pages.
https://doi.org/10.1145/1988842.1988846
[58]K. Pan, X. Wu, and T. Xie. 2011. Generating program inputs for database ap-
plication testing. In 2011 26th IEEE/ACM International Conference on Automated
Software Engineering (ASE 2011). 73–82.
[59]KaiPan,XintaoWu,andTaoXie.2015. Program-inputgenerationfortestingdata-baseapplicationsusingexistingdatabasestates. AutomatedSoftwareEngineering
22, 4 (2015), 439–473. https://doi.org/10.1007/s10515-014-0158-y
[60]Chenxiong Qian, Hong Hu, Mansour Alharthi, Pak Ho Chung, Taesoo Kim,
andWenkeLee.2019. {RAZOR}:AFrameworkforPost-deploymentSoftware
Debloating.In 28th{USENIX}SecuritySymposium( {USENIX}Security19).1733–
1750.
[61]Anh Quach,Aravind Prakash, and LokYan. 2018. Debloating software through
piece-wise compilation and loading. In 27th{USENIX}Security Symposium
({USENIX}Security 18). 869–886.
[62]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit
TestingEngineforC.In Proceedingsofthe10thEuropeanSoftwareEngineering
Conference Held Jointly with 13th ACM SIGSOFT International Symposium on
FoundationsofSoftwareEngineering (ESEC/FSE-13).ACM,NewYork,NY,USA,
263–272. https://doi.org/10.1145/1081706.1081750
[63]HashimSharif,MuhammadAbubakar,AshishGehani,andFareedZaffar.2018.
TRIMMER:applicationspecializationforcodedebloating.In Proceedingsofthe
33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering.329–
339.
[64]NickStephens,JohnGrosen,ChristopherSalls,AndrewDutcher,RuoyuWang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller:AugmentingFuzzingThroughSelectiveSymbolicExecution. https:
//doi.org/10.14722/ndss.2016.23368
[65]ArtTaylor.2002. Jdbc:DatabaseProgrammingwithJ2EewithCdrom. Prentice
Hall Professional Technical Reference.
[66]NikolaiTillmannandJonathandeHalleux.2008. Pex–WhiteBoxTestGenerationfor.NET.In TestsandProofs,BernhardBeckertandReinerHähnle(Eds.).Springer
Berlin Heidelberg, Berlin, Heidelberg, 134–153.
[67]M.Vakilian,R.Sauciuc,J.D.Morgenthaler,andV.Mirrokni.2015. Automated
Decomposition of Build Targets. In 2015 IEEE/ACM 37th IEEE International Con-
ference on Software Engineering, Vol. 1. 123–133.
[68]Margus Veanes, Jonathan de Halleux, Nikolai Tillmann, and Peli deHalleux. 2009. Qex: Symbolic SQL Query Explorer. Technical Report
MSR-TR-2009-2015. https://www.microsoft.com/en-us/research/publication/
qex-symbolic-sql-query-explorer/ Updated January 2010.
[69]WillemVisser,CorinaS.Pundefinedsundefinedreanu,andSarfrazKhurshid.2004.
Test Input Generation with Java PathFinder. In Proceedings of the 2004 ACM
SIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis (ISSTA’04).
Association for Computing Machinery, New York, NY, USA, 97–107. https:
//doi.org/10.1145/1007512.1007526
[70]Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. 2019. Superion: Grammar-
Aware Greybox Fuzzing. In Proceedings of the 41st International Conference on
Software Engineering (ICSE ’19). IEEE Press, 724–735. https://doi.org/10.1109/
ICSE.2019.00081
[71]Hui Xu, Zirui Zhao, Yangfan Zhou, and Michael R Lyu. 2018. Benchmarking the
capabilityofsymbolicexecutiontoolswithlogicbombs. IEEETransactionson
Dependable and Secure Computing (2018).
[72]Zhihong Xu, Martin Hirzel, Gregg Rothermel, and Kun-Lung Wu. 2013. Testing
propertiesofdataflowprogram operators. In 201328thIEEE/ACM International
Conference on Automated Software Engineering (ASE). IEEE, 103–113.
[73]Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma,MurphyMcCauly,MichaelJFranklin,ScottShenker,andIonStoica.2012. Re-silientdistributeddatasets:Afault-tolerantabstractionforin-memorycluster
computing.In Presentedaspartofthe9th {USENIX}SymposiumonNetworked
Systems Design and Implementation ( {NSDI}12). 15–28.
733