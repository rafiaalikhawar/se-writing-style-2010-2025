HazardAnalysis forHuman-on-the-Loop
Interactionsin sUAS Systems
Michael Vierhauser
michael.vierhauser@jku.at
Johannes KeplerUniversityLinz
AustriaMdNafee Al Islam
AnkitAgrawal
Jane Cleland-Huang
JaneHuang@nd.edu
Universityof NotreDame, USAJames Mason
james.mason@ngc.com
Northrop Grumman
USA
ABSTRACT
WiththeriseofnewAItechnologies,autonomoussystemsaremov-
ingtowardsaparadigminwhichincreasinglevelsofresponsibility
are shifted from the human to the system, creating a transition
from human-in-the-loop systems to human-on-the-loop (HoTL)
systems. This has a signi ficant impact on the safety analysis of
suchsystems,asnewtypesoferrorsoccurringattheboundaries
of human-machine interactions need to be taken into considera-
tion.Traditionalsafetyanalysistypicallyfocusesonsystem-level
hazardswithlittlefocusonuser-relatedoruser-inducedhazards
that can cause critical system failures. To address this issue, we
constructdomain-levelsafetyanalysisassetsforsUAS(smallun-
mannedaerialsystems)applicationsanddescribetheprocesswe
followedtoexplicitly,andsystematicallyidentifyHumanInterac-
tion Points (HiPs), Hazard Factors and Mitigations from system
hazards. We evaluate ourapproach by first investigating the extent
to which recent sUAS incidents are covered by our hazard trees,
andsecondbyperformingastudywithsixdomainexpertsusing
our hazard trees to identify and document hazards for sUAS usage
scenarios.Ourstudyshowedthatourhazardtreesprovidede ffec-
tive coverage for a wide variety of sUAS application scenarios and
were useful for stimulating safety thinking and helping users to
identifyandpotentiallymitigate human-interaction hazards.
CCS CONCEPTS
•Software andits engineering →Software safety .
KEYWORDS
Human-sUASinteraction, safetyanalysis,hazardanalysis,sUAS
ACMReference Format:
MichaelVierhauser,MdNafeeAlIslam,AnkitAgrawal,JaneCleland-Huang,
and James Mason. 2021. Hazard Analysis for Human-on-the-Loop Inter-
actions in sUAS Systems. In Proceedings of the 29th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Soft-
wareEngineering(ESEC/FSE ’21), August 23–28, 2021,Athens, Greece. ACM,
NewYork, NY, USA, 12pages.https://doi.org/10.1145/3468264.3468534
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on thefirst page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspeci ficpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’21, August 23–28,2021, Athens,Greece
© 2021 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-8562-6/21/08...$15.00
https://doi.org/10.1145/3468264.34685341 INTRODUCTION
Autonomous systems are increasingly moving towards a paradigm
in which humans and machines work in tandem to achieve rela-
tivelycomplextasks–typicallyinsystemsthatarenowreferred
toas“human-on-the-loop”(HotL)[ 50].Incontrasttoamoretra-
ditional “human-in-the-loop” (HitL) system, in which a human
makes decisions at key points of the system’s execution, a HotL
system exhibits far greater machine autonomy while providing
situationalawarenesstohumans.HotLenvironmentsareableto
takefulladvantageofmachineautonomytoperformtargetedtasks
efficiently and quickly; however, in addition to traditional hazards,
theyintroducethepotentialfornewtypesoferrorsthatoccurat
theboundariesofhuman-machineinteractions.Currentparadigms
thatexploretheseinteractionsinsafety-criticalsystemsfailtofully
evaluate the way in which humans contribute to, impact, or fail to
impact,systemsafetyinsmall UnmannedAerialSystems(sUAS).
Historically, many hazards have occurred at the human-CPS
interface.Forexample,in1988 theUS Navy’s USSVincennes shot
down a civilian plane with 290 people on board. The Vincennes
hadentered Iranianwater and operators mistakenly identi fied the
Airbus as an attacking F-14 Tomcat despite the fact that the Airbus
was climbing and emitting appropriate civilian IFF signals. The
mistaken identi ficationwas partiallyattributed toauser interface
flaw which caused the operator to confuse the data of a military
planeintheareawiththatofthecivilianone[ 17].Humanoperators
are frequently blamed for these types of errors which have been
widelyreportedascontributingfactorsin60%to85%ofaccidentsin
domainssuchasaviationandmedicaldevices[ 49].However,many
of these “human” failures can be directly attributed to flaws in the
underlying system design [ 42] and could therefore be classi fied as
design-induced-faults[ 27].
SimilarexamplesareemerginginthedomainofsmallUnmanned
AerialSystems,suchasthecaseofanearcollisionbetweenansUAS
and a highway patrol helicopter in California in 2015. The sUAS
wasflying at over 700 feet even though, based on FAA regulations,
themaximumaltitudeallowedwas400feet. Inthis case,theRPIC
(remotepilotincommand)haddeliberatelysetahigherRTL(return
tolaunch)altitudetoavoidelectricalpylons.During flightthesignal
to the sUAS was lost, and the RTL failsafe mechanism activated
causing the sUAS to return home at an illegal altitude, resulting in
a near collision with the helicopter. While the RPIC was clearly at
fault,thesoftwarewasdevelopedinawaythatallowedthemistake
to happen without raising alerts either whenthe RTL altitude was
incorrectly con figured or during flight when the altitude violation
actually occurred.
8
ESEC/FSE ’21, August 23–28, 2021,Athens,Greece Vierhauser, Al Islam, Agrawal, Cleland-Huang,Mason
Teams building safety-critical software products are required
toperform arigoroushazardanalysis [ 44], using techniquessuch
as Software Fault Tree Analysis (FTA) [ 59,61] or Software Fail-
ure Mode, E ffects, and Criticality Analysis (FMECA) [ 47,57] to
identify hazardous states and a set of mitigating actions which are
linked to safety-related requirements. While the safety analysis
must be performed on individual products, several studies have
shown that preliminary hazard analysis can be initially performed
at the domain level and then contextualized during the application
developmentprocesstoindividualproducts[ 15,18,31,62].Various
frameworks,checklists,andtemplatesexisttoguidesystemsand
software engineers through the process of identifying and mitigat-
ing hazards associated with the development and deployment of
sUAS[20].However,thesetendtofocusonsystem-levelhazards
whilepayingscantattentiontotheuniquehumaninterfaceaspects
ofmulti-user,multi-agentsystemsthatareemerginginthesUAS
domain[2,43,46,65].
Furthermore, while human-related hazards in the sUAS domain
sharecommonalitieswiththosefromseveralotherdomainssuch
asmulti-agentrobotics,autonomousvehicles,anddronesusedin
the defense domain, they also exhibit unique safety concerns intro-
ducedbythedeploymentofremotelycontrolledsUASinpotentially
populatedareas,limitedtrainingoftheremotepilotswhomaybe
ill-prepared to handle o ff-nominal cases, and a rapidly emergent
marketofsUASapplications,whichinmanycasesaredeveloped
byhobbyists withouttraining insafetyassurance.
This paper describes domain-level safety analysis assets that
explicitly focus upon human-interaction hazards. Our aim is to
create a shared and reusable resource and astructured process for
use by software and systems engineers working in the space of
sUASapplication development.Wefolloweda systematicprocess
that started by reviewing a broad range of academic literature and
whitepapersdescribingimplementedsUASframeworks,templates,
andhazard analysisassociatedwithsUASsystemsandfoundthat
the majority of hazards identi fied from the literature are system-
orientedandfailtocapturehazardsassociatedwithhuman-sUAS
interactions.Wethenapplieda systematicprocess thatbuiltupon
thesystemhazardstoidentifyadditionalhazardsassociatedwith
HumanInteractionPoints(HiPs) .Thisanalysisresultedinasetof
domain-levelhazardtreesdesignedforsafetyanalysisofdiverse
sUASsystemswhichweevaluatedintwoways– first,againstde-
tailed accounts of publicly reported sUAS incidents, and second,
throughastudyinvolvingsixdeveloperswithdomainexperience
working with sUAS. Examples throughout the paper are primarily
drawn from our ownDroneResponse system[ 2,13,16].
Theremainderofthepaperislaidoutasfollows.Section 2reports
on our systematic process for identifying human-sUAS interaction
hazards from existing literature. Section 3describes the process we
followed to construct our hazard trees, while Section 4discusses
theiruse. Section 5evaluatescoverageof the hazardtrees against
reported incidents, while Section 6reports our study with domain
experts. We analyze results in Section 7. Finally, Sections 8to10
discuss threatsto validity,relatedwork, andconclusions.2 sUASHAZARDANALYSIS
Weperformedasystematicliteraturesurveytoidentifyaninitial
setofsUAShazardsbasedonpublicationsreportingsafetyanalysis
techniquesappliedtosUASdomains.Paperscoveredtopicssuch
ashazardanalysis,FaultTreeAnalysis,andsafetyanalysisusing
theGoalStructuringNotation[ 40].Ouraimwasnottoanalyzethe
effectiveness of di fferent techniques or frameworks but to identify
specifictypesofhazards,faults,andsafetyrequirementsreported
byauthors.TheresultingcollectionofsUAShazardtreesformsthe
foundation for our subsequent, more focused, and human-centered
safetyanalysis(cf.Section 3).
2.1 Data AggregationandAnalysis Process
OursystematicliteraturesearchusedtheACM,IEEE,andScopus
digital libraries to identify research papers containing descriptions
of sUAS safety, failures, and incidents. We performed a number
of pilot searches based on keywords collected from an initial set
of papers and re fined the search terms multiple times to ensure
that relevant papers were part of the search including, for exam-
ple, “sUAS” (and various synonyms such as UAV) and also “safety”,
“hazard” or “fault” analysis. Two researchers then collected and an-
alyzed the search results, using the Parsifal tool [ 53]. We removed
duplicatesandexcludedpapersaccordingtothefollowinginclusion
(IC) andexclusion(EC) criteria:
•EC1:PapersnotrelatedtosUAS(e.g.,airplanes,orlarge(military-
grade) UAVs) were excluded.
•EC2:PapersnotwritteninEnglishornotavailableasPDFvia
the digital library were excluded.
•IC1:OnlypaperscontaininginformationonsUASsafety-related
information including safety requirements, hazards, or faults
orpaperscontaining informationon multi-agentsafety-related
informationwithregardstointeraction/collaboration,between
agents(inthe titleand/orabstract)were included.
Our initial search returned 3462 papers. Applying the exclusion
criteria resulted in a set of 2036 papers, which were reduced to 120
papers after the inclusioncriteriawere applied.
2.2 Hazard TreeConstruction
Wethenskimmedeachpapertoidentifyconcretehazards,faults,
and/or safety-related statements mentioned inthepaper.We refer
to these as “safety statements” . We extracted a total of 200 safety
statementsfrom27papers,whiletheremaining93papersdidnot
contain speci fic examples of safety-relatedstatements for sUAS.
Eachofthe statementswasthentransformed intooneormore
explicitly stated hazards – each one based on a safety requirement,
fault, or actual hazard reported in the paper. We then used open
coding to identify hazard categories, and as a result established
an initial grouping of eleven di fferent categories (e.g.,“sensors”,
“route planning”). This encoding process was performed by two
researcherswithfrequentdiscussionsandre finementofthegroups
andcategoriesuntilagreementwasreached.We finallyidenti fied
eight categories of hazards. In cases where a hazard was related
to multiple categories, we selected the most appropriate one. This
resultedina finalsetof114distinct “hazardstatements”.
9HazardAnalysis forHuman-on-the-Loop Interactions in sUASSystems ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
System hazard
C1: Communication failure (e.g., GCS failure,
Loss of wireless signal, or jammed communication)
[5]
C5: Ground control system
for multi sUAS communication fails
[3]
C4: Hand held controller for manual
flight loses connectivity to sUAS
[1]
C2: Communication failure
between ground
and sUAS
[1]
C3: Communication failure
between sUAS
[4]
Figure 1: One of the smaller hazard trees derived from the
literaturesurveyforcommunicationfailures.Thenumbers
indicatethenumberofassociated hazard statements.
Asthegranularityofthereportedhazardsvariedsigni ficantly,
includingspeci fichazardssuchas “ThesUASdeviatesfromitspre-
defined route due to wind shear” and“Loss of satellite signal” , we
establisheda hierarchy of hazards(cf. Fig. 1)organizedunder eight
hazardcategories.Wherehazardscouldbegroupedundermultiple
parenthazardsweselectedthemostappropriateonetoavoiddu-
plicates, but added a cross-reference to the other hazard tree when
appropriate. We organized all associated hazards into a hierarchy –
where necessary adding intermediate hazards, merging duplicates
or very similar ones, and removing hazards that were deemed out
of scope for an sUAS application or too abstract. Following this
process,108hazardnodesremained.Wethendouble-checkedthe
originallisttoensurethatthetreesprovidedfullcoverageofthe
hazardsidenti fiedfromtheliteratureandthatnonehadbeenmissed.
This task was performed independently by three researchers on
ourteam.Incaseswheretheassignmentwasnotunanimous,we
discussedthe mappinguntilconsensuswas reached.Thisresulted
infive hazardsbeing slightly modi fiedornewlyaddedto the tree.
Theresultinghazardtrees,basedonourliteraturesearch,rep-
resenteduseofprohibitedairspace(7hazardstatements),separa-
tion distance (10 hazard statements), communication (10 hazard
statements),hardwareandsensorfailures(37hazardstatements),
weather(9hazardstatements),piloterror(15hazardstatements),
preflightchecks(6hazardstatements),andsituationalawareness
(14 hazardstatements).
3 HUMAN-SUASINTERACTIONS
One of the main findings of our literature survey was that the
majority of hazards and safety-related statements target system-
levelhazards,payinglittletonoattentiontouser-related,oruser-
inducedhuman-sUASinteractionhazardsthatcanleadtocritical
system failures. Given this lack of documented human-interaction
hazards,weappliedasystematicprocessforderivingthemfromthe
system hazards when performing hazard analysis for a certain use
casescenario.OurprocessissummarizedinFig. 2.Westarted(step
1)byselectingasystem-levelhazardfromanexistinghazardtree.
Next (step 2) we systematically explored each Mission Mode and
identified relevant scenarios that represented Human Interaction
Points(step 3) for each mode. Given aspeci fic HiP, within a given
mode,wesystematicallyexploredtheroleahumancouldplayin
instigating or mitigating the hazard (step 4) with respect to system
design, user design, and hardware and con figuration flaws. We
Mission Mode
Maintenance & configuration
Mission planning 
Prelaunch configuration
Advisories and weather 
Takeoff
Inflight 
RTL and Landing
System hazard defined 
e.g., UAV loses satellite signal
Hazard Factorsfor each 
relevant 
modeMore
hazards
Human initiated error
Situational awareness
Lack of empowerment
Human Interaction Point
e.g., The operator checks 
why a UAV is unable to arm.
 More
modes
❶
❷ ❸
❹
Domain AssetsHuman-sUAS Hazard Tree❺
❻Figure 2: Hazards and mitigations with associated human
factors and impacts were identi fied systematically for each
systemhazardaccordingtomode,errortype,and finallythe
impact upon humanoperators.
used this informationto augmentthebasic systemhazard treesto
createaHiP-orientedhazardtree capturingHiPhazardsfordi fferent
hazard factors. We finally consolidated hazard trees for all relevant
modesassociatedwiththespeci fichazard(step5).Wenowdescribe
thesesteps inmore detail.
3.1 Mission Modes(Step2)
All sUAS systems transition through di fferent phases of operation
(i.e.,modes).Theseincludepre- flightconfiguration(includingmain-
tenance and mission planning), prelaunch con figuration checks on
sUAS readiness and retrieval of flight advisories, mission launch,
(post-launch) flight, and RTL (returnto launch). Hazard scenarios,
including those associated with human interactions, may occur
across multiple phases of operation with mode-speci fic charac-
teristics and mitigations [ 8]. Therefore, hazard analysis must be
performedforeachmode[ 26]andmustaddressthesystem’sability
todetectandreacttobothnormalandabnormalevents[ 32].We
briefly summarize each of the modes that we include in our exam-
ples throughout the paper, as their unique characteristics shape
theirassociatedhazardsandmitigations.
•Missionplanning: (Multi-)sUASmissionsaredrivenandcon-
strainedbymissionplansthatestablishrules,missionboundaries,
and goals, enable speci fic tasks and transitions between tasks, and
set autonomy levels for the sUAS [ 13].Actions takenin thismode
can impact safety, for example if an RPIC plans a mission without
considering changes interrain elevation.
•sUASMaintenanceand con figuration: Flight Controller sys-
tems, such as Ardupilot [ 5] and px4 [ 54], have large numbers of
configurable parameters. For example, px4 lists 1,382 con figuration
points each with a range of allowed values. Users can con figure
10ESEC/FSE ’21, August 23–28, 2021,Athens,Greece Vierhauser, Al Islam, Agrawal, Cleland-Huang,Mason
sUASusingthird-partysoftwarepackages,manyofwhichfailto
providemeaningfulconstraints.
•Prelaunchcon figurationchecks: Thesystemmustcheckcrit-
ical configuration values prior to launch and either correct them
automaticallyorclearlydisplaywarnings.Commonerrors,asob-
servedinourstudy,includesettingincorrectfail-safevalues(e.g.,
RTL altitudeabove the legally allowedlevels).
•Advisory and weather checks: RPICs are required to obtain
flight authorization prior to entering controlled airspaces and to
check weather conditions. Incidents often involve adverse weather
conditions and/orunauthorized flights intocontrolledairspace.
•Takeoff:Problems not identi fied during pre-launch activities
canemergeattakeo ff,forexample,acablethatobstructsapropeller
causinglossofcontrol,communicationfailuresbetweensUAS,or
sUASplacement onan obstructedlaunchpad.
•Inflight:Duringflight, users issue directives to the sUAS. In
addition,thesystemmustprovidesituationalawarenesstousersso
thattheycanperceivethecurrentsituation,comprehendwhatis
happening, and make sound decisions[ 27]. Accidents occur when
users lose situational awareness, often due to well-documented
design “demons” such as information overload [ 39,48], attentional
tunneling [ 56,60,64], andout-of-the-loopsyndrome[ 9].
•RTL andlanding: RTL and landing modes present unique haz-
ards – for example the need to provide safe passage home if all
sUAS simultaneously transition to RTL due to global loss-of-signal.
3.2 Human InteractionPoints (Step3)
HumansdirectlyinteractwithsUASinmanydi fferentways,includ-
ing through physical manipulations (e.g., attaching a camera), use
of manual flight controls (e.g., if a human takes physical control of
thesUASfromthesoftwaresystem),aswellasthroughissuingcom-
mands,feedback,andsettinggoalsviatheuserinterface[ 33,63].
We discuss five common human-sUAS interaction patterns (P1-P5),
based on accounts of human interactions reported in the literature
(e.g.,[4,12,33,63]).Eachofthefollowingpatternsinvolvesactions
and interactions performed by both sUAS and humans. Square
brackets[] depictoptionalactions.
P1:Monitor (Human) →
[Seek_Explanation (Human) →Explain (sUAS)→]
[Intervene (Human) →Respond (sUAS)]
P2:Request_feedback (sUAS)→Provide_feedback (Human)
→Act(sUAS)→Monitor (Human)
P3:Adapt+Explain (sUAS)→Monitor (Human)
P4:Observe andCon figure(Human) →
Check_Con figuration (sUASorSystem)
P5:Set_mission_goals (Human) →Plan_mission
(System+sUAS) →Act(sUAS)
Thefirstpattern(P1)isthemostcommononeinaHoTLsystem
where ahuman operator’s responsibility isprimarilysupervisory.
The operator monitors the system and when needed requests an
explanationfromthesUASsystem.Thehumanmaydecidetoin-
terveneinthesUASs’behavior,andthesUASrespondsaccordingly.
InpatternP2,ansUASexplicitlyrequestspermissiontoperforman
actionorrequestscon firmationthatatakendecisionwascorrect.
An example of this is when an sUAS uses onboard vision to detectavictimduringasearch-and-rescuemission,andrequestscon fir-
mation from the operator that it made a correct decision to switch
from “search” to “track” mode [ 2]. The operator provides feedback
whichthe sUASactsupon,andthe human monitors the response.
In P3, the sUAS adapts independently and then provides an
explanationoftheadaptation.AnexamplewouldbewhenansUAS
makes an RTL decision due to low battery. The human monitors
theactionandifnecessaryintervenesbyfollowingpatternP1.In
P4, the human con figures and checks the system – either during
flight or prior to flight. Finally, in P5, the human sets mission goals
whichguideandconstraintheactionsthesUASareallowedtotake
includingtaskstheywillperform,permissionstoactautonomously,
and ways in which they will collaborate with other sUAS and with
human supervisors. We explored instances of these patterns across
commonandexceptionscenariosfordiverse flightmodes,andused
themto aid inthe identi fication of human-interaction points.
3.3 Human HazardFactors (Step4)
Wefurtherexplorethreetypesofhuman-interactionerrors. Human
initiated errors are quite common in the sUAS domain, as many
pilots have limited training. Examples include ignoring regulations
and restrictions, or failing to follow established processes. Loss
of situational awareness , occurs when the user is unable to fully
perceivethecurrentsituation,comprehendwhatishappening,and
make sound decisions [ 27]. For example, if the system provides
inaccurate information about the health or location of an sUAS, or
fails to explain why an sUAS behaves in a certain way (e.g., the
sUAS stopped at a certain point and does not move further), the
usermaymakesuboptimaldecisionsforhowtoproceedwiththe
mission. Finally, lack of empowerment occurs when the operator is
awareofthestateofthemission,knowswhattheywouldliketo
do, but the system does not provide the means for them to do it. A
simpleexampleiswhenthesystemfailstoprovidetheuserwith
the optionofcanceling a flight route currently inprogress.
3.4 Constructing theHazardTrees(Steps5& 6)
Finally,basedonthissystematicapproachforidentifyinghuman-
sUAShazards,weconstructedhazardtreescapturingthesystem
hazard, sub-hazards, and human-sUAS hazards to be addressed.
Partial examples for two hazard trees (sUAS collisions and pre-
flightconfigurations)arereportedinFig. 3.Theseinitialtreeswere
continually re fined into domain-level assets (step 6) throughout
the remainder of our study as additional hazards emerged. This
refinement processis further described in Section 5.2based onad-
ditionalhazardsdiscoveredthroughanalyzingreportedincidents.
Ourfinal set of eight hazard trees is listed in Table 1. We also
identify candidate mitigations. For example, in Fig. 3b, hazard PX7
could be mitigated bythe requirement that “The system shall store
a list of default arming checks to be applied to all UAVs by type (e.g.,
PX4, Ardupilot). An alert shall be displayed if any UAV’s internal
configuration di ffersfrom the expected armingchecks.” .
Whilethispaperdoesnotfocusontheprocessofestablishing
mitigations,ourGitHub-hostedhazardtreesdoincludea(growing)
listofmitigationoptionsfor eachhuman-interaction error1.
1sUAS Repository: https://github.com/SAREC-Lab/sUAS-UseCases
11HazardAnalysis forHuman-on-the-Loop Interactions in sUASSystems ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
Hazard
System hazard
Hazard
F1: A UA V flies dangerously
close to another object or
collides with it
Hazard
F2: A UA V flies too
close to ground-based
objects (e.g., ground,
trees, buildings,
people)
Hazard
F5: The UA V does
not have an accurate
terrain map and/or
accurate geolocation
and is therefore
not able to determine
the correct altitude
to fly atHazard
F11: The UA V crashes
into the terrain
or another object
right after an operator
manually assumes
control of the system
from the computerized
system
Hazard
FX6: When the operator
assumes manual control
during the mission
and switches (e.g.,
throttle) are set
incorrectly, the
UA V responds
dramatically
(e.g., plunging
to the ground)Hazard
FX8: When the operator
assumes manual control
of the UA V , they
do not know how
the UA V is oriented
(i.e., which direction
the UA V is facing)
and find it difficult
to immediately control
the UA VHazard
F3: Minimum separation
distance and/or
__time-to-impact__
threshold is violated
between airborne
UA Vs
Hazard
FX2: The operator
has no means of
overriding the onboard
autonomy and/or
cannot do so quickly
enough in order
to avoid a collision
with the terrainHazard
FX1: The operator
is unaware that
the UA V is flying
too close to the
terrainHazard
F8: GPS accuracy
is unexpectedly
degraded
Hazard
FX3: The operator
is unaware that
GPS accuracy is
degraded and that
UA Vs are in danger
of mid-air collisionsHazard
FX7: The operator
is unaware that
the switches are
set incorrectly
(a)Hazardsrelatedto sUAS collisionsHazard
System hazard
Hazard
P1: Physical Preflight
UA V  setup misses
important checks
Hazard
P2: UA Vs are not
placed correctly
for launch
Hazard
PX1 Operator
places UA Vs too
close to each other or
with insufficient clearance
prior to launchHazard
P3: UA V is not flight-readyHazard
PX11 It is difficult
for the user to
check and configure
multiple UA Vs simultaneously
Hazard
P5: No geofence has
been established
Hazard
PX4 The system does
not provide appropriate
information regarding
the geofence, so
the operator is
unable to determine
whether it has been
set correctly or
notHazard
P6: UA V is not  
configured correctly
for flight
Hazard
PX5: User is unaware
that the system
is not configured
correctlyHazard
PX8 User has configured
autopilot in an
unsafe way (e.g.,
setting minimum
number of satelite
fixes required to
1, or setting the
RTL altitude illegally
high or dangerously
low)Hazard
P7: The payload is
too heavy or
unbalancedHazard
P12: The user sets
switches on hand-held
controller incorrectly
(e.g., throttle,
RTL, LAND)
Hazard
PX9 Operator attaches
overly heavy or
insecured payload
to UA VAlso see Sensor and
Hardware Hazard Tree
Hazard
PX6: User is unaware
that failsafe and
other flight actions
are configured incorrectly
(e.g., RTL actions)Hazard
PX7: User is unaware
that critical arming
checks are disabled
(e.g., satellite
connections, accelerometer
health)
(b) Hazardsassociatedwithpre flightchecksand con figurations.
Figure 3: Two partial hazard trees. System nodes were derived from the literature survey, while colored nodes were derived
from our analysis of human-interaction errors. Legend: Gray=system hazards, blue=human initiated errors, green=loss of
situational awareness, yellow=lack ofempowerment.
Description: Multiple UAVs dispatched to search for victim.
Primary Actor: Drone Commander (DC)
Trigger: The DC activates the search.
Main Success Scenario:
1. The UAV performs synchronized takeoff 
[mode: Takeoff]
a) UAVs are not placed correctly for launch (system haz.)
i.Operator places UAVs too close to each other for launch [HE]
ii.Operator places UAVs in area with insufficient clearance [HE]….
2. DroneRescue tracks and displays the location and state 
of each UAV [mode: Inflight]
a) Communication Failure between ground and UAVs (system haz.)
i. The operator is unable to receive status data from the UAVs   
and loses situational awareness [SA]
…
(more steps….)
Figure 4: Use case Vignette includes ( system-level hazards ),
missionmodes,and( respectiveHiPs ),whereHE=HumanEr-
ror,SA=Loss ofSituational Awareness.
4 LEVERAGING THE HAZARD TREES
In this section we assume the role of an end user (e.g., an sUAS
system developer) and show how a user could leverage our hazard
trees, and the process we developed, to identify relevant human-
interactionfactorsforaspeci ficsUASapplication.Ourexampleisbased on the use case vignette shown in Fig. 4for a search-and-
rescue scenario. We systematically examine each step of the use
case and identify its HiPs and their associated flight modes and
hazardgroups.Inthisexamplewefocusontheusecasesteprelated
tosynchronized takeo ffwhich occurs in takeoffmode. We identify a
HiP in which the operator prepares the sUAS for launch, and then
select relevanthazardtreesof preflight configuration, weather,and
mission planning . The user retrieves those hazard trees and utilizes
them to aid in the hazard analysis process. In this case, we identify
ourfirstsystem-levelhazardfromthepre flightconfigurationtree
stating that “UAVs are not placed correctly for launch”. The tree
offerstwoassociatedHiPsde finedashumanerrors for“Operator
places UAVs too close to each other prior to launch” and “Operator
places UAVs in location with insu fficient clearance prior to launch”.
We followasimilar processfor allsubsequent steps.
Once human-interaction hazards are identi fied, we can propose
mitigatingrequirements.Forexample,theinappropriateplacement
of sUAS could be mitigated through including a clearance check
in a prelaunch checklist,or byadding a newfeature tothe system
that raises a placement alert if the minimum separation distance is
violatedbetween sUASprior to launch.
5 INCIDENTREPORTCOVERAGE
Thefirstpartofourevaluationassessesthe coverageofhuman-sUAS
incidentsacrossthehazardtrees .Wecollectedreportsofincidents,
accidents, and failures relatedto sUAS usage from news services
12ESEC/FSE ’21, August 23–28, 2021,Athens,Greece Vierhauser, Al Islam, Agrawal, Cleland-Huang,Mason
Table1:Final,re finedHazardGroupswithsystem-(SYS)and
user-hazardshuman-sUASinteractionhazards(HI)
Hazards
IDCause of Hazard SYS HI
CLCollisions: between sUAS, otherobjects,andterrain 9 7
CMCommunication: lossofcommwithsUAS 7 7
HSHardware/Sensors: e.g., cameras, GPS, parachutes 14 7
MAMission Awareness: Mission status, decision making 7 11
MPMission Planning: Flight routes andtask allocation 5 3
PCPreflightCon figuration: Geofence, launch,params 7 12
RCRegulatory Compliance: Airspace, flightconstraints 9 9
WTWeather: Extremetemperature&wind 4 9
Table 2: sUAS Incidents involving as reported publicly
and/orthroughregulatory bodies
Source URL #
Aviation SafetyReporting
System(ASRS)http://asrs.arc.nasa.gov/docs/
rpsts/uav.pdf (ACN: 1599671)50
Wikipedia collection of
incidentsen.wikipedia.org/wiki/List_
of_UAV-related_incidents109
dedrone –Collection of
WorldwideDrone Incidentswww.dedrone.com/resources/
incidents/all100
The Centerfor the Studyof
the Drone –BardCollegehttp://dronecenter.bard.edu/
drone-incidents30
UK Air Accidents
Investigation Branch reportshttps://www.gov.uk/aaib-
reports?keywords=UAS50
and regulatory bodies. In total we inspected 339 reports of sUAS
incidentsfrom fivedifferentsourcesasdepictedinTable 2.Three
members of our team analyzed the incidents and mapped reported
human-interactionfailurestotheeighthazardgroupsandtothe
threehazardfactors.Foreachincident,oneteammemberperformed
theinitialmappingandasecondmembercheckedthemappings.
In case of disagreement, all three people discussed the results to
reachconsensus. Aggregatedresults are reportedinFig. 5.
5.1 Results
The majority of reports simply stated that an sUAS was sighted
inprohibitedairspacewithoutanydiscussionofthecontributing
cause,whileonly54provideddetailedaccounts.43oftheseincluded
human-relatedfactors,providinginsightsintotheprevalenceand
rootcausesofdi fferentincidentsandindicatingthatthemajority
ofincidentsinvolvedhumanfactors.Forexample,someincidents
related to entering a prohibited airspace were caused by lack of
preflightconfigurationorappropriatecon figurationchecks(cov-
eredbythehazardtrees“RegulatoryCompliance”and“Pre flight
Configuration”). These accidents could be avoided by using a flight
authorization system. Similarly, in the case of pilot errors (e.g.,
losingline-of-sighttothesUAS),orhardwareerrors,(e.g.,lossof
GPS), appropriateprelaunchcon figurationcheckssuch ascorrect
fail-safesettings,couldhavemitigatedtheseincidents(cf.“Pre flight
Configuration”).Hazard Groups
CL CM HS MA MP PC RC WT
Human initiated error 13 0 6 2 5 8 4 2
Loss of situational awareness 17 2 7 4 5 5 4 3
Lack of empowerment 7 4 6 0 3 3 3 1
Figure 5: A heatmap showing human-interaction factors
mappedagainsthazardgroupsaccordingtotheiroccurrence
intheanalyzed incidentreports.
Three of these incidents are summarized in Table 3[I1-I3]. Two
cases (I1,I3) were related to (near) collisions with other aircraft,
and all of them involved human initiated error eitherinflight(I1,
I3)orasaresultof preflightconfigurationerrors(I2).Humaniniti-
ated errors included ignoring airspace warnings (I1), flying BVLOS
(beyondvisuallineofsight)(I1),andfailingtoobtainpermission
toflyincontrolled airspace(I3), both ofwhich could wellbe miti-
gated through imposing constraints on flight planning (see hazard
treeRegulatoryCompliance )and increasing situationalawareness
levels through providing more warnings, recommendations, and
evenprohibitions(seetree MissionAwareness ).In thesecondinci-
dent(I2),theRPICillegallysettheRTLaltitudetoapproximately
750feetinordertoavoidpylonsandotherobstaclesifafail-safe
caused the sUAS to switch to RTL mode. As sUAS can be con fig-
ured using open-source applications (e.g., QGroundControl [ 55] or
MissionPlanner [ 6]),theircon figurationsmust bechecked for un-
desirable settings immediately prior to flight. Responsible software
packages should also warn anytime a user con figured the autopilot
in a potentially illegal or dangerous way. Finally, in incident I3,
the operator reported that he su ffered from stress due to multi-
tasking,andfailedtonoticethatthesystemhadstartedreporting
altitude in meters and not feet. This type of stress is quite common
when humans supervise autonomous systems, and is referred to
as Workload, Anxiety, Fatigue, and Other Stressors (WAFOS) by
Endsley [ 27]. However, the incident could have been avoided with
a legally established geofence or by raising alerts if unexpected
configurationswereintroduced.Thesethreeincidentshighlightthe
importance of software engineers designing, implementing, and
testing sUAS systems systematically to address safety concerns
relatedto human-sUASinteractions.
5.2 Analysis & HazardTreeRe finement
AsdepictedinFig. 5,manyofthereported incidentsincludedcol-
lisions and were attributed to loss of situational awareness (17
instances) or directly related to human error (13 instances). The
secondmostcommoncategorywaspre flightconfigurationissues,
where either the pilot was partially at fault (8 instances) or the
systemdidnotprovideadequateorsu fficientinformationtocon-
figurethesystemcorrectly(5instances).Altogether,weconclude
that incidents were reported for each hazard group and that all
reportedhuman-interactionfactorsweresuccessfullymappedto
one ormore ofthe hazardgroups.
Basedonourobservationsweperformedafewupdatestoim-
prove the structure and clarity of the trees. We observed that pi-
lot/operator related hazards occurred primarily due to loss of mis-
sionawarenessandprelaunchcon figurationproblems,andthere-
fore redistributed pilot hazards to these two categories. We further
13HazardAnalysis forHuman-on-the-Loop Interactions in sUASSystems ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
Table 3:Example sUAS-Incidents with HumanContributingFactors
ID IncidentType IncidentDescription Human-Related
HazardRef
I1 UAVcollisionwithhot-air
balloonflyinginunauth.
airspace over Boise,Idaho
in2018Theamateurpilotoverrodewarningsabout flyinginunauthorized
airspaceclosetoairportwithoutpermissionfromATC.TheUAV
wentbeyondvisuallineofsightandRPICwasunawarethatthe
UAVwasrepeatedlyshearingagainsttheballoonuntilpropellers
felloff.Pilotwasunskilledwithhand-heldcontrols.Ignoringcritical
warnings;
FlyingBVLOS.[37]
I2 Near collisionwith
Highway Patrolhelicopter
over Martinez, California
between 700and800feetin
2015Max. altitude allowed for UAV flights in the USA is 400ft (or 100ft
abovebuildings).TheRPIChadoverriddenthealtitudeatwhich
UAVs return to launch to avoid electrical pylons. When signal was
lost with the UAV during flight, it switched to RTL mode, and
operated on autopilot at prohibited altitudes placing it into the
flight pathofthe helicopter.Criticaldefaultvalues
overriddenbyuserina
3rdpartytool;
Failure to check
configurations prior to
launch.[11]
I3 UAVwas flownto an
altitudethat wasinexcess
ofthe 400FT AGL
limitationspeci fiedwithin
FAR Part 107The RPIC believed that altitude was being reported in feet; and
was not aware that it had been reset to meters. As a result he
accidentally flew to approx 492 feet, claiming that the mistake
was caused by his focus on avoiding flying near obstacles or over
people, coupled with the delayed awareness that the software had
reset to metric units.Delayedawareness of
UAVstatus, WAFOS
(Workload, Anxiety,
Fatigue,andother
Stressors situational
awareness demon)[7]
extracted several hazards from across multiple categories into a
new category named “In flight Mission Awareness” which grouped
hazards related to situational awareness. This new category in-
cludes hazards such as “The operator is overwhelmed by status
information for multiple UAVs” and “ The operator is unable to
handle multiple alerts simultaneously”. This produced the current
setofeighthazardgroupswhicharelistedinTable 1andwereused
for the subsequent study withdomainexperts.
6 EVALUATION BY DOMAIN EXPERTS
Thesecondpartofourstudywasdesignedtoevaluatewhetherthe
hazardtreeswereusefulforanalyzingandidentifyingrequirements
associatedwithhuman-sUASinteractions.Weinvitedsixexperts
fromthesUASdomaintoreviewusecasesdescribingsUASmissions
andtoutilizeourhazardtreestoaugmentasetofusecaseswith
human-interactionhazardsusingtheprocessdescribedinSection 4.
Noneoftheseparticipantswereinvolvedinthedevelopmentofthe
hazardtrees orinthe developmentofthe systematic process.
6.1 StudyDesign
The study was divided into three parts, that included an initial
semi-structured interview, an analysis task, and closing interview.
All phases of the study focused on two primary use cases in which
sUAS were used to support (1) river search and rescue, and (2)
environmentalwatersampling.Bothtop-levelusecasesinvoked
supportingusecasesto activateandarm sUAS,generateflightroutes ,
and to plan non-intersecting routes through leasing airspace . In ad-
dition, the river rescue use case invoked active victim tracking and
the environmental sampling use case invoked flight authorization .
We piloted the studyinternally with one user, made improvements,
andthen conductedthe study as follows.Initial Brie fing:At the start of the interview we described the
HazardFactorsthatwehadaddressed,namely humaninitiatederror ,
lackofsituationalawareness ,andlackofempowerment ;however,we
alsostatedthatdiscussionwasnotlimitedtothesefactors.Wethen
presentedparticipantswithoneoftheprimaryusecasesandasked
themtobrainstormpotentialsafetyhazardsforeachstepoftheuse
casewiththefocusonhumaninteractionand/orhumanfailures
usingathink-out-loudprotocol[ 36].Wetime-boxedthisdiscussion
to15minutes,asthatwassu fficienttounderstandthe typesofissue
each participant would identify, while seeking a complete analysis
would require much longer. Our aim was to establish a baseline for
howdeveloperscurrentlythinkabout,andidentifyhuman-sUAS
hazards.TheinterviewwasrecordedusingZoomandautomatically
transcribedfor lateranalysis.
Afterthebrainstormingtaskwascompleted,wespentapproxi-
mately10minutesexplainingthetasktobeperformed.Weintro-
ducedtheeighthazardgroupsandtheirassociatedhierarchiesof
systemandhuman-interactionhazardsandthenpointedthepar-
ticipantstoan8minute(take-home)onlinevideothattheycould
use to further familiarize them with the hazard trees and the study
process.
Study Task: The study task was performed individually. We as-
signedparticipantsintooneoftwogroups(riverrescue,environ-
mentalsampling).Eachparticipantwasgiventheirownmulti-sheet
googlespreadsheetwhichincluded(1)asummaryofthe8hazard
groups, (2) individual sheets for the primary use case, and four
supporting ones. Each use case included metadata, a list of suc-
cesssteps,andamatrixforevaluatingeachstepandmarking(a)
whethereachofthe8hazardswasrelevant,and(b)inthecaseof
supporting use cases, exactly which hazards from the hazard trees
were relevant. Fig. 6shows the main part of the spreadsheet for
one particular supporting use case. Hyperlinks allowed the user
14ESEC/FSE ’21, August 23–28, 2021,Athens,Greece Vierhauser, Al Islam, Agrawal, Cleland-Huang,Mason
Figure 6:Anexample ofthehazardsidenti fied by oneoftheparticipantsforthesupporting use caseofActive Tracking.
toeasilymovebetweenusecasesandtoviewthehazardtreeson
GitHub We instructed participants to spend up to 45 minutes, to
systematicallyevaluateeachusecasestepandtomarktherelevant
hazards.
Follow-upInterview: Inafollow-upinterviewweaskedpartici-
pants a series of open-ended questions about their experience in
working with the hazard trees. Questions included (1) To what ex-
tent didthe trees helpyou toidentify potentialhuman interaction
errors? (2) Was it di fficult tofind a matching hazard group and/or
specific hazard for a use case step? (3) Was there anything missing
orunclearwiththegroupsorhazardtrees?(4)When flyingadrone,
have you ever experienced an incident that was at least partially
caused by a human-sUAS interaction problem? (5) If so, do you
thinkthatincidentcould havebeenpreventedormitigatedifyou
hadaddressedhazardsdescribedinthehazardtrees(pleaseexplain
your answer)? In addition, we used a rubric to elicit feedback on
usabilityande fficiencyoftheapproach.Thefollow-upinterview
wasalsorecordedandtranscribed.
6.2 StudyParticipants
Werecruitedsixparticipantsforourstudy,eachofthemwithex-
tensivedomainexpertiseinpilotingsUASordevelopingnon-trivial
sUAS applications. As shown in Table 4, their experience ranged
from2to 8years working onsUAS development projects, with an
averageof3.67years.Wealsoindicatewhethertheyhadexperience
withsoftware,hardware,and/orinamulti-sUASenvironment.In
cases where a participant had prior experience with river rescue
(P3)orenvironmentalsampling(P2,P6),weassignedthemtothose
respective use cases. We opted to include only highly quali fied
domainexpertswhorepresentourtargetusers,eventhoughthis
reduced the size of our participant pool. However, Nielsen [ 51]
has shown, that five or six participants are su fficientfor providing
meaningful and in-depth feedback on a research design solution
such as the hazardtrees.
6.3 InitialInterviewAnalysis
We performed an inductive coding analysis [ 45] on the transcripts
from the initial meetings. The researcher that conducted the inter-
viewperformedtheinitialanalysisandthiswasthencross-checked
by a second researcher. We identi fied four themes that were ob-
servedacrossseveralofthediscussions.First,severalofourpartici-
pantsfocused upon system-level hazards rather than human-related
ones,andweresometimesunabletoidentifyanyhuman-interaction
hazards. For example, participant P4 stated that “ I don’t see anyTable 4:Study participants
# Application domain Exp UC Mult HW SW
P1 sUASdispatch &CallcenterUI 4yrs ES • •
P2 Environmental Applications 8yrs ES • •
P3 Multi-sUASSearchandRescue 2yrs RR • •
P4 Safety& Security/Emerg. Resp. 2yrs RR •
P5 DefibrillatorDelivery 4yrs RR • •
P6 Environmental Applications 2yrs ES • •
Legend: ES=Environmental Sampling,RR=River rescue,
Mult=Multi-sUASdevelopment,HW=Hardware,SW=Software.
opportunities for human error” associated with de fining a coverage
area and allocating routes to sUAS. In fact, five out of six of our
participants (with the exception of P2) focused more on system-
levelhazardsthanhuman-relatedones.Inacloselyrelatedtheme,
threeoftheparticipantsrequestedadditionalexplanationsabout
human-interactionerrors,indicatingthatthiswasanewconcept
for them. Finally, we observed several examples of “blaming the
operator” withoutconsiderationforhowadesign flawinthesys-
temmighthaveincreasedthelikelihoodofanoperatorerror.For
example, P5 observed that the user could set an incorrect mode for
takeoff(humanerror),butdidnotmentionthatthesystemfailedto
raiseanalertwhichcouldhavenoti fiedtheoperatoroftheproblem.
6.4 TaskAnalysis andFollow-up Interviews
Once participants had completed the task, we analyzed the map-
pings from use case steps to hazards. We report aggregated results
in Table5. For the two primary use cases (River Rescue and En-
vironmental Sampling), most participants found mappings to all
eight hazard groups. For the supporting use cases, agreement was
significantly lower. Out of 48 potential mappings (i.e., 6 supporting
use cases ×8 hazard groups) there were only seven cases in which
allparticipantsagreedtothemapping,but15casesinwhichatleast
2/3 agreed. Similarly, there were eight cases of full agreement that
the hazard group was not relevant and 20 cases with at least 2/3
agreement. There were only five potential mappings which lacked
2/3 consensus.
AsreportedinTable 5,theparticipantsfoundeachhazardgroup
to be useful across at least some of the use cases. In total, Commu-
nication(orthelossthereof)wasmentioned58timesintotal,Hard-
ware/Sensors were mentioned 52 times, and Mission Awareness
mentioned51times.Collisionwasmentionedtheleastwithonly
24mentionsoverallusecases.Allsixparticipantsmentionedthe
15HazardAnalysis forHuman-on-the-Loop Interactions in sUASSystems ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
Table 5: Participant majority consensus for whether a spe-
cific hazard group was relevant for two top level use-cases
(River Rescue and Environmental Sampling) and five sup-
porting use cases.
Hazard Group
Use Case CL CM HS MA MP PC RG WT
River Rescue         
Environmental Sampling         
Activate &arm #   # # # #
Area coverage # # # H #  # #
Flight authorization # # #      
Leaseairspace #  H # H # #  #
Active tracking H #  # # # H # #
Legend: : 2/3 MajorityAgreement that the hazardgroup is
related,H #:LackofAgreement, #:2/3MajorityAgreementthatthe
hazard group is not related. The highest ranked hazard groups per
use caseare underlinedin red.
usefulnessofthehazardtrees.Forexample,P2statedthat “thetrees
wereveryusefulwaytoconnectthehazards” ,whileP3statedthat
“theyweresuperuseful...therewerethingsIwouldn’thavethoughtof” .
We also observed several cases in which a participant had stated
duringtheinitialinterviewthattherewerenohuman-interaction
hazards associated with a speci fic use case step, but found relevant
ones when using the hazard trees. For example, when equipped
withthehazardtree,P3identi fiedpreviouslyundetectedhazards
associated with “Loss of Communication”, “Mission Planning”, and
“Regulatory Compliance” for the area coverage use case. This indi-
catesthatthehazardgroupsprovidevaluableinformationregarding
human-sUASinteractions.
Based on a 4-point Likert scale (very e fficient, efficient, ine ffi-
cient, very ine fficient), 4 out of 6 participants stated that use of
the hazard groups was “efficient”, and that the grouping improved
the assignment task; however, two participants rated the task of
identifying and assigning hazards as ine fficient. Subsequently, the
feedback was somewhat mixed. P2 stated that he “was hunting
aroundabit” butthathe “likedthecategorization” and“thecolor
coding” while P5 stated that “the organization of hazards within
eachoftherespectivetreesmakesitprettyeasytosearchatree” .P3
shared that “After 15 or 20 minutes everything seemed to kind of
make sense” but that“there is a lot of information because it is a
complex problem” .We discuss this issuefurther inSection 7.
Several participants mentioned that they leveraged systems haz-
ardstohelpthemidentifyhuman-interactionhazards.P4explained
thathefirst“identifiedsystem flawsandthenfollowedthemdownthe
treetouncoverpotentialhuman-sUASinteractionproblems” while
P5statedthathe “startedatmid-level(system)hazardsandfound
relevanthuman-interactionhazards below that” .
Finally, all participants were able to elucidate on at least one
human-interaction problem they had personally experienced, and
all agreed that knowledge of the hazard trees during the devel-
opmentprocesscouldhavehelpedthemforeseeandmitigatethe
reported problem. For example, P1 described a real-life incident
he had experienced which led to unnecessary human intervention
resultinginacrash,andstatedthat “weprobablywouldhavethoughtmore about (how to address the hazards) had we looked over those
hazard trees” and said that “preflight check(s) could have alerted
to the throttle position” thereby preventing the accident. In gen-
eral, our participants indicated that the real value of the trees is in
providingexamples that encourage safety thinking .
7 DISCUSSION
Based on the analysis of reported incidents and sUAS accidents by
regulators and the media, we found that human initiated errors
and loss of situationalawarenessdominatedthe reportedincidents.
Basedonouranalysisandthefeedbackfromourdomainexperts,
weconcludethatseveralofthereportedincidentscouldhavepo-
tentially been prevented or mitigated by addressing the hazards
collectedinour domain-level trees.
Our study with domain experts indicated that developers in
our study focused on system-level hazards, most likely because
they were more familiar with addressing system problems and
lessknowledgeableabouttheroleofhuman-relatedhazards.This
was the case even though they understood the full scope of the
study, including its focus on human-interaction errors. Based on
themappingstheycreatedwhenusingourhazardtreesandtheir
feedbackinthe finalinterview,weconcludethatthehazardtrees
facilitatedsafety thinkingfrom ahuman-interactionpointof view
and provided agood starting pointfor “diggingdeeper” into these
types ofhazards.
A second important consideration is the need for better tool
support. While our study participants reported that identifying
typesofhazards(akahazardgroups)wasquiteeasy, findingspeci fic
hazards added an additional level of complexity and as a result,
sifting through all hazardsin all (relevant) trees was rather tedious.
Thisledtotheconclusionthatadditionaltoolsupportcouldease
the burden of looking for potential matching hazards. While we do
notproposea“checklist-liketool”,asthiscouldprovideafalsesense
ofcompleteness,featuresforsearching, filtering,andannotating
the trees could easethe taskof identifying relevanthazards.
Table 6: Two human-interaction hazards with examples of
potentialmitigations.
Hazard: PX1:Operatorplaces UAVstoo closeto each other prior
to launch
PX1-S1: RPICreceives proper training to conduct mandatory
preflight checks(ProcessRequirement).
PX1-S2:The systemchecksthe coordinates of allUAVs onthe
ground andraisesan alertif any of themare locatedless than
minimumseparation distance apart (SafetyRequirement).
Hazard: CX3: The human operator is unable to receive status data
from theUAV using thesoftware-based system.
CX3-S1: The approximate position andthe uncertainty of the
UAV’scurrentposition onthe mapmustbe visually depicted
(e.g.,bycreating an increasingly large “circle”around the last
known,orprojectedposition of the UAV(SafetyRequirement).
16ESEC/FSE ’21, August 23–28, 2021,Athens,Greece Vierhauser, Al Islam, Agrawal, Cleland-Huang,Mason
In addition, our aim is to provide a resource for addressing haz-
ard analysis and safety assurance throughout the software engi-
neeringlifecycle inorder toaid sUAS developersinbuildingsafer
systemsthatempowerandsupportdiverseoperators.Ourhazard
trees include a set of candidate mitigations associated with each
human-interaction hazard as illustrated in Table 6. The current list
ofmitigations isincludedouronline repository.Ourapproachis
designedtoaddressanemergentprobleminthedomainofsUAS
developmentbyprovidingareusablesetofhazarddescriptionsand
mitigations aimed at inspiring and supporting a safety mindset for
sUASdevelopers.
8 THREATS TO VALIDITY
Our work is subject to several validity threats. While we have
shown that it is applicable to real-world incidents and use case
scenarios, the limited number of incident reports makes it likely
that other types of incidents are not yet covered. Furthermore,
additional external evaluation of theprocessis required toensure
its applicability in a more broader scenario. The analyzed incident
reportssparselycovermulti-sUASapplicationsandcustomsoftware
solutions. The majority ofreports are related totheuse ofo ff-the-
shelfapplications(suchasMissionPlannerandDJI’spropriatery
softwaresystem),anddonotrepresentmulti-sUASmissions.Given
the increasingly common reports of sUAS incidents and the dearth
of information discussing root causes, we have created a shareable
resource thatcan be usedas a starting pointfor analyzing human-
sUAShazardsinaspeci fic application.
We consideredseveral alternate studydesigns,includingacon-
trolled experimentthat would involve specifying requirements for
asystemwith,andwithout,ourhazardtrees.However,toaccom-
plishthisinanon-trivialwaywouldrequiresigni ficanttimeand
effort, beyond available resources. Our approach, falls under the
broad umbrella of design science, in which it has been shown that
evenalimitednumberofparticipantsprovidesusefulusabilityfeed-
back to iteratively re fine a design [ 51]. Another threat is related
to the experience of the participants. While all participants have
experienceinhandlingandoperatingsUAS,onlyoneparticipant
had previous experience with multi-sUAS missions. Therefore, we
expect additional human-sUAS interaction hazards to emerge from
these types of systems and application use cases. We release our
set of hazard trees as a publicly available community resource that
ismeanttoevolveoverasnewincidentsarereportedfromwhich
hazardscan be identi fied.
We attempted to minimize internal validity threats associated
with the incident analysis and study, by dividing the transcription
of audio recordings among two researchers and cross-checking the
resulting transcripts, codes, and emergent themes. For the incident
coverage,threeresearchersperformedtheanalysisandeachinci-
dent was checked by two researchers. In case of a disagreement,
the incident wasreevaluateduntil agreement wasreached.
9 RELATED WORK
Themostcloselyrelatedworkisinsafetyassurance,hazardslicing,
andsituational awareness.
SafetyAssurance: WorkbyDenneyandPai[ 19,21,24,25]inthis
area addresses di fferent aspects of sUAS and UAV safety providingautomationsupportandtoolsforcreatingandmaintainingsafety
assurancecases.Theyemphasisereuseofsafetyassurancecasesby
proposingdomain-independentanddomain-speci ficpatterns[ 22,
29]. Other work has created reusable safety case patterns as build-
ing blocks for future product development [ 10,19,21,23,29,35].
In the area of safety cases maintenance, Kelly and Weaver [ 40]
presentedasetofpatternsandrecommendedtheuseofmodularity
tosupportsafetycasesevolution.KellyandMcDermid[ 41]investi-
gatedchangesinevidence,context,assumptionandrequirements
nodes to determine how changes impact the safety assurance case.
HazardSlicing: Similartoourapproachofdividingalargehazard
tree into several sub-trees to better address the di fferent aspects,
severalresearchershaveshownthebene fitsofhazard-basedslic-
ing[38,58].Agrawaletal.[ 3,14]proposedSAFA(SoftwareArtifact
ForestAnalysis)that uses underlying tracelinksto create and visu-
alizehazardstreesandtheirrespectivemitigationsinthecontextof
an evolving safety-critical software system. SafeSlice [ 28] extracts
design slices based on functional safety requirements. Other work
inthisareafocusesongeneratingartifactslicesusingformalver-
ification techniques to support safety analysis [ 38,58]. However,
these approaches are all system focused with little to no emphasis
onuserinducedhazardsandfaults.Whilethesetypesofhazardsare
important,speci ficprocesses,methods,orguidelinesforidentifying
andmitigating human-interaction relatedhazardsare missing.
HCI/Situational Awareness: The seminal work on Situational
Awareness(SA)byEndsley[ 27]focusedonuser-centereddesign
identifying eight common design errors that occur frequently in
user interface designs and which inhibit SA. Several studies in this
areaexplicitly explore SA andshortcomingsof userinterfaces for
varioustypesofsystemssuchastsunamiearlywarningsystems[ 1],
electricminingshovels[ 52]oroperatorinteractionswithasingle
robot or machine [ 30,34]. While the HCI community has exam-
ined this problem from various angles, they focus primarily on the
user-interface design and not on broader sets of hazards which our
approach isdesignedto address.
10 CONCLUSION
In this paper, we have presented an approach for systematically
deriving human-interaction hazards for sUAS systems. Based on a
literature survey we identi fied eight di fferent categories of hazards,
thatserveasastartingpointforahuman-centeredhazardanalysis.
Aspartoftheprocess,weidenti fieddifferentmissionmodesand
contributinghazardfactors,derivedpatternsforHumanInteraction
Points(HiPs),andconstructedaninitialsetofreusablehazardtrees.
Aspartofourfutureworkweplanonfurtherextendingourhazard
tree library, exploring ways to make it more scalable, including
providing tool support to facilitate navigation and identi fication
ofrelevantpartsofahazardtreefordomain-speci ficmulti-sUAS
applications. We also plan to extend our work into the design,
implementation,and test lifecycleto provide supportfor realizing
hazardmitigations.
ACKNOWLEDGMENTS
ThisprojecthasbeenpartiallyfundedbytheLinzInstituteofTech-
nology, the US National Science Foundation (SHF-1909007, CNS-
1931962) andbysupport from NorthropGrumman.
17HazardAnalysis forHuman-on-the-Loop Interactions in sUASSystems ESEC/FSE ’21, August 23–28, 2021,Athens,Greece
REFERENCES
[1]2009. InteractionDesign for Situation Awareness-Eyetracking and Heuristics for
ControlCenters.
[2]A.Agrawal,S.Abraham,B.Burger,C.Christine,L.Fraser,J.Hoeksema,S.Hwang,
E.Travnik,S.Kumar,W.Scheirer,J.Cleland-Huang,M.Vierhauser,R.Bauer,and
S.Cox.2020. TheNextGenerationofHuman-DronePartnerships:Co-Designing
anEmergencyResponseSystem.In Proc.ofthe2020Conf.onHumanFactorsin
ComputingSystems .
[3]A.Agrawal,S.Khoshmanesh,M.Vierhauser,M.Rahimi,J.Cleland-Huang,and
R. R. Lutz. 2019. Leveraging artifact trees to evolve and reuse safety cases. In
Proc. ofthe 41st Int’lConf.onSoftwareEngineering . 1222–1233.
[4]Ankit Agrawal, Jan-Philipp Steghöfer, and Jane Cleland-Huang. 2020. Model-
Driven Requirements for Humans-on-the-Loop Multi-UAVMissions. In Proc. of
the 10thInt’lModel-DrivenRequirements Engineering WS .
[5]Ardupilot.2020.Ardupilot–opensourceautopilotsoftware. https://ardupilot.org .
[Lastaccessed 01-06-2021].
[6]Ardupilot.2020. MissionPlanner. https://ardupilot.org/planner . [Lastaccessed
01-06-2021].
[7]Aviation Safety Reporting System. 2021. ASRS Database Report Set: Unmanned
Aerial Vehicle (UAV) Reports (ACN: 1599671). https://asrs.arc.nasa.gov/docs/
rpsts/uav.pdf [Lastaccessed 01-06-2021].
[8]PaulBaybutt.2012. Processhazardanalysisforphasesofoperationintheprocess
lifecycle. ProcessSafety Progress 31,3 (2012), 279–281.
[9]FrancescoNBiondi,MonikaLohani,RachelHopman,SydneyMills,JoelMCooper,
andDavidLStrayer.2018.80MPHandout-of-the-loop:E ffectsofreal-worldsemi-
automated driving on driver workload and arousal. In Proc. of the Human Factors
and Ergonomics Society Annual Meeting , Vol. 62. SAGE Publications, 1878–1882.
[10]Robin E. Bloom field and Kateryna Netkachova. 2014. Building Blocks for Assur-
anceCases.In Proc.ofthe25thIEEEInt’lSymp.onSoftwareReliabilityEngineering
Workshops . 186–191.
[11]CBSSFBayArea NewsOutlet.2015. PilotOfDrone ThatNearlyHitCHPHeli-
copter Says It Was On Autopilot. https://sanfrancisco.cbslocal.com/2015/12/17/
drone-near-miss-chp-helicopter-martinez-owen-ouyang-apology-autopilot .
[Lastaccessed 01-06-2021].
[12]Jane Cleland-Huang and Ankit Agrawal. 2020. Human-Drone Interactions with
Semi-Autonomous Cohorts of Collaborating Drones. In Proc. of the Interdisci-
plinary WS on Human-Drone Interaction; co-located with the 2020 ACM CHI Conf.
onHuman Factors inComputingSystems .
[13]Jane Cleland-Huang, Ankit Agrawal, Md Nafee Al Islam, Eric Tsai, Maxime
VanSpeybroeck,andMichaelVierhauser.2020. Requirements-DrivenCon figura-
tion of Emergency Response Missions with Small Aerial Vehicles. In Proc. of the
24thACMConf.onSystemsand SoftwareProductLines . 1–12.
[14]J. Cleland-Huang, A. Agrawal, M. Vierhauser, and C. Mayr-Dorn. 2021. Visualiz-
ing Change in Agile Safety-Critical Systems. IEEE Software 38, 03 (May 2021),
43–51.
[15]Jane Cleland-Huang, Mats Per Erik Heimdahl, Jane Hu ffman Hayes, Robyn R.
Lutz,andPatrickMaeder.2012. TraceQueriesforSafetyRequirementsinHigh
AssuranceSystems.In Proc.oftheInt’lWorkingConf.onRequirementsEngineering:
Foundationfor SoftwareQuality . 179–193.
[16]Jane Cleland-Huang, Michael Vierhauser, and Sean Bayley. 2018. Dronology: an
incubatorfor cyber-physicalsystems research.In Proc.ofthe40thInt’lConf.on
SoftwareEngineering: NewIdeas and EmergingResults . 109–112.
[17]NancyJ.Cook.2007. StoriesofModernTechnologyFailuresandCognitiveEngi-
neering Successes . CRC Press,2007.
[18]Josh Dehlinger and Robyn R. Lutz. 2006. PLFaultCAT: A Product-Line Software
FaultTreeAnalysis Tool. Autom.Softw. Eng. 13,1 (2006), 169–193.
[19]EwenDenneyandGaneshPai.2014. Automatingtheassemblyofaviationsafety
cases.IEEE Transactions onReliability 63,4 (2014), 830–849.
[20]EwenDenneyandGaneshPai.2015. Argument-basedairworthinessassurance
ofsmallUAS.In Proc.ofthe2015IEEE/AIAA34thDigitalAvionicsSystemsConf.
IEEE,5E4–1.
[21]EwenDenneyandGanesh Pai.2016. Compositionofsafety argumentpatterns.
InProc.oftheInt’lConf.onComputerSafety,Reliability,andSecurity .Springer,
51–63.
[22]EwenDenneyandGaneshPai.2016. SafetyconsiderationsforUASground-based
detect and avoid. In Proc. of the IEEE/AIAA 35th Digital Avionics Systems Conf.
IEEE,1–10.
[23]Ewen Denney, Ganesh Pai, and Josef Pohl. 2012. Heterogeneous Aviation Safety
Cases:Integrating the Formalandthe Non-formal.In Proc.ofthe17thIEEE Int’l
Conf.onEngineering ofComplexComputer Systems . 199–208.
[24]Ewen Denney, Ganesh J. Pai, and Ibrahim Habli. 2015. Dynamic Safety Cases
forThrough-LifeSafetyAssurance.In Proc.ofthe37thIEEE/ACMInt’lConf.on
SoftwareEngineering . 587–590.
[25]Ewen Denney, Ganesh J. Pai, and Iain Whiteside. 2017. Modeling the Safety
ArchitectureofUASFlightOperations.In Proc.ofthe2017Int’lConf.onComputer
Safety,Reliability, and Security .
[26]Homayoon Dezfuli, Allan Benjamin, Christopher Everett, Martin Feather, Peter
Rutledge,DevSen,andRobertYoungblood.2015.NASASystemSafetyHandbook.Volume2:SystemSafetyConcepts,Guidelines,andImplementationExamples.
(2015).
[27]MicaR.Endsley.2011. DesigningforSituationAwareness:AnApproachtoUser-
Centered Design, Second Edition (2nd ed.). CRC Press, Inc., Boca Raton, FL, USA.
[28]DavideFalessi,ShivaNejati,MehrdadSabetzadeh,LionelBriand,andAntonio
Messina. 2011. SafeSlice: a model slicing and design safety inspection tool for
SysML. In Proc. ofthe 19thACM SIGSOFT Symp. and the 13thEuropean Conf. on
FoundationsofSoftwareEngineering . ACM,460–463.
[29]Martin S. Feather and Lawrence Z. Markosian. 2013. Architecting and generaliz-
ingasafetycaseforcriticalconditiondetectionsoftware:anexperiencereport.In
Proc. of the 1st Int’l WS on Assurance Cases for Software-Intensive Systems . 29–33.
[30]K. Fellah and M. Guiatni. 2019. Tactile Display Design for Flight Envelope
Protection and Situational Awareness. IEEE Transactions on Haptics 12, 1 (Jan
2019),87–98.
[31]Qian Feng and Robyn R. Lutz. 2005. Bi-directional safety analysis of product
lines.Journal ofSystemsand Software 78,2 (2005), 111–127.
[32]DonaldFiresmith.2004. EngineeringSafetyRequirements,SafetyConstraints,
andSafety-CriticalRequirements. JournalofObjectTechnology 3,3(2004),27–42.
[33]Markus Funk. 2018. Human-drone interaction: Let’s get ready for flying user
interfaces! Interactions 25,3 (2018), 78–81.
[34]MatthewC.Gombolay,AnnaBair,CindyHuang,andJulieA.Shah.2017. Compu-
tationaldesignofmixed-initiative human-robot teaming thatconsiders human
factors:situationalawareness,workload,andwork flowpreferences. I.J.Robotics
Res.36,5-7 (2017), 597–617.
[35]RichardHawkins, Ibrahim Habli, and Tim Kelly. 2013. PrincipledConstruction
of Software Safety Cases. In Proc. of the Next Generation of System Assurance
Approaches for Safety-Critical Systems WS of the 32nd Int’l Conf. on Computer
Safety,Reliability and Security .
[36]AndreasHolzinger.2005. Usabilityengineeringmethodsforsoftwaredevelopers.
Commun. ACM 48,1 (2005), 71–74.
[37]Teton Valley News Julia Tellman. 2018. First-ever recorded drone-hot air
balloon collision prompts safety conversation. https://www.postregister.
com/news/local/ first-ever-recorded-drone-hot-air-balloon-collision-prompts-
safety/article_7cc41c24-6025-5aa6-b6dd-6d1ea5e85961.html . [Last accessed
01-06-2021].
[38]ShuanglongKan.2014. Traceabilityandmodelcheckingtosupportsafetyrequire-
mentveri fication.In Proc.ofthe22ndACMSIGSOFTInt’lSymp.onFoundations
ofSoftwareEngineering . ACM,783–786.
[39]Leah Kaufman and Brad Weed. 1998. Too much of a good thing?: Identifying
andresolvingbloatintheuser interface.In Proc.oftheCHI98Conf.Summaryon
Human FactorsinComputingSystems . 207–208.
[40]TimKellyandRobWeaver.2004. Thegoalstructuringnotation–asafetyargu-
ment notation. In Proc. of the dependable systems and networks 2004 workshop on
assurance cases . Citeseer, 6.
[41]TimP KellyandJohnA McDermid. 2001. A systematicapproach to safetycase
maintenance. Reliability Engineering & SystemSafety 71,3 (2001), 271–284.
[42]L.T. Kohn, J.M. Corrigan, and M.s. Donaldson. 1999. To err is human, Building a
safetyhealth system. Washington,DC:National Academy Press (1999).
[43]Yasuhiro Kuriki and Toru Namerikawa. 2014. Consensus-based cooperative
formationcontrolwithcollisionavoidanceforamulti-UAVsystem.In Proc.of
the 2014AmericanControlConf. IEEE,2077–2082.
[44]NancyG.Leveson.1995. Safeware,SystemSafetyandComputers .AddisonWesley.
[45]Mai Skjott Linneberg and Ste ffen Korsgaard. 2019. Coding qualitative data: A
synthesis guidingthe novice. QualitativeResearch Journal (2019).
[46]YuannaLiuandHaoLu.2019. Astrategyofmulti-UAVcooperativepathplanning
basedonCCPSO.In Proc.ofthe2019IEEEInt’lConf.onUnmannedSystems .IEEE,
328–333.
[47]RobynR.LutzandRobertM.Woodhouse.1997. RequirementsAnalysisUsing
Forward and backward Search. Ann. SoftwareEng. 3 (1997), 459–475.
[48]JoannaMcGrenere,RonaldM.Baecker,andKelloggS.Booth.2002. AnEvaluation
ofaMultipleInterfaceDesignSolutionforBloatedSoftware.In Proc.oftheSIGCHI
Conf.onHumanFactorsinComputingSystems (Minneapolis,Minnesota,USA).
ACM,164–170.
[49]D.C.Nagel.1998. HumanerrorinaviationOperations. HumanfactorsinAviation,
E.L.Weiner and E.C.Nagel (Eds) 19890047069, 34(1998), 263–303.
[50]Saeid Nahavandi. 2017. Trustedautonomy between humans and robots: toward
human-on-the-loop in robotics and autonomous systems. IEEE Systems, Man,
and Cybernetics Magazine 3,1 (2017), 10–17.
[51]Jakob Nielsen and Thomas K Landauer. 1993. A mathematical model of the
findingofusabilityproblems.In Proc.oftheINTERACT’93andCHI’93Conf.on
Human FactorsinComputingSystems . 206–213.
[52]E Onal, C Craddock, and Mica Endsley. 2013. From Theory to Practice: How
DesigningforSituationAwarenessCanTransformConfusing,OverloadedShovel
Operator Interfaces, Reduce Costs, and Increase Safety. In Proc. of the Int’l Symp.
onAutomationand Robotics inConstruction .
[53]Parsif.al.2021. Tool supportfor SystematicLiterature Reviews. https://parsif.al .
[Lastaccessed 01-06-2021].
18ESEC/FSE ’21, August 23–28, 2021,Athens,Greece Vierhauser, Al Islam, Agrawal, Cleland-Huang,Mason
[54]PX4.2021. OpenSourceFlightController. https://px4.io . [Lastaccessed01-06-
2021].
[55]QGroundControl. 2021. Ground Control Station. http://qgroundcontrol.com .
[Lastaccessed 01-06-2021].
[56]NicolasRegis,FrédéricDehais,EmmanuelRachelson,CharlesThooris,Sergio
Pizziol, Mickaël Causse, and Catherine Tessier. 2014. Formal Detection of Atten-
tionalTunnelinginHumanOperator-AutomationInteractions. IEEETransactions
Human-Machine Systems 44,3 (2014), 326–336.
[57]Donald J. Reifer. 1979. Software Failure Modes and E ffects Analysis. IEEE
Transactions onReliability R-28,3 (1979), 247–249.
[58]MehrdadSabetzadeh,ShivaNejati,LionelBriand,andAnne-HeidiEvensenMills.
2011. Using SysML for modeling of safety-critical software-hardware interfaces:
Guidelines and industry experience. In Proc. of the IEEE 13th Int’l Symp. on High-
AssuranceSystemsEngineering . IEEE,193–201.
[59]NeilR.Storey.1996. SafetyCriticalComputerSystems . Addison-WesleyLongman
Publishing Co., Inc.,Boston, MA,USA.
[60]Tim Claudius Stratmann and Susanne Boll. 2016. Demon Hunt - The Role of
Endsley’s Demons of Situation Awareness in Maritime Accidents. In Proc. of theInt’lWorkingConf.onHumanError,Safety,andSystemDevelopment .Springer,
203–212.
[61]Kevin J. Sullivan, Joanne Bechta Dugan, and David Coppit. 1999. The Galileo
Fault Tree Analysis Tool. In Digest of Papers: FTCS-29, The Twenty-Ninth Annual
Int’lSymponFault-TolerantComputing . IEEE Computer Society, 232–235.
[62] HongyuSun,MiriamHauptman,andRobynR.Lutz.2007. IntegratingProduct-
Line Fault Tree Analysis into AADL Models. In Proc. of the 10th IEEE Int’l Symp.
onHighAssuranceSystemsEngineering . IEEE Computer Society, 15–22.
[63]Dante Tezza andMarvin Andujar.2019. The State-of-the-Art of Human–Drone
Interaction: ASurvey. IEEE Access 7 (2019), 167438–167454.
[64]Christopher D Wickens and Amy L Alexander. 2009. Attentional tunneling
andtaskmanagementinsyntheticvisiondisplays. TheInternationalJournalof
AviationPsychology 19,2 (2009), 182–199.
[65]Xueyi Zou, Rob Alexander, and John McDermid. 2016. Testing method for multi-
uavconflictresolution using agent-based simulation and multi-objective search.
Journal ofAerospace InformationSystems 13,5 (2016), 191–203.
19