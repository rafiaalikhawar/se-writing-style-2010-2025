Validation on Machine Reading Comprehension Software
without Annotated Labels: A Property-Based Method
Songqiang Chenâˆ—
i9schen@gmail.com
School of Computer Science, Wuhan
University, ChinaShuo Jinâˆ—
imjinshuo@whu.edu.cn
School of Computer Science, Wuhan
University, ChinaXiaoyuan Xieâˆ—â€ 
xxie@whu.edu.cn
School of Computer Science, Wuhan
University, China
ABSTRACT
Machine Reading Comprehension (MRC) in Natural Language Pro-
cessing has seen great progress recently. But almost all the current
MRC software is validated with a reference-based method, which
requires well-annotated labels for test cases and tests the software
by checking the consistency between the labels and the outputs.
However, labeling test cases of MRC could be very costly due to
their complexity, which makes reference-based validation hard to
be extensible and sufficient. Furthermore, solely checking the con-
sistency and measuring the overall score may not be sensible and
flexible for assessing the language understanding capability. In this
paper, we propose a property-based validation method for MRC
software with Metamorphic Testing to supplement the reference-
based validation. It does not refer to the labels and hence can make
much data available for testing. Besides, it validates MRC software
against various linguistic properties to give a specific and in-depth
picture on linguistic capabilities of MRC software. Comprehen-
sive experimental results show that our method can successfully
reveal violations to the target linguistic properties without the la-
bels. Moreover, it can reveal problems that have been concealed by
the traditional validation. Comparison according to the properties
provides deeper and more concrete ideas about different language
understanding capabilities of the MRC software.
CCS CONCEPTS
â€¢Software and its engineering â†’Software verification and
validation .
KEYWORDS
machine reading comprehension, metamorphic relation, property-
based validation, language understanding capability
ACM Reference Format:
Songqiang Chen, Shuo Jin, and Xiaoyuan Xie. 2021. Validation on Machine
Reading Comprehension Software without Annotated Labels: A Property-
Based Method. In Proceedings of the 29th ACM Joint European Software
âˆ—These authors contribute equally to this research and are the co-first authors.
â€ Xiaoyuan Xie is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
Â©2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8562-6/21/08. . . $15.00
https://doi.org/10.1145/3468264.3468569Engineering Conference and Symposium on the Foundations of Software Engi-
neering (ESEC/FSE â€™21), August 23â€“28, 2021, Athens, Greece. ACM, New York,
NY, USA, 13 pages. https://doi.org/10.1145/3468264.3468569
1 INTRODUCTION
With the development of Natural Language Processing (NLP) tech-
niques, machines have been able to solve many textual tasks, some
of which even involve the human-level language understanding
[55]. One of the popular domains is Machine Reading Comprehen-
sion (MRC). It requires the machines to read textual materials and
answer questions based on the facts in the given materials, just as
humans do with reading comprehension tasks [ 55]. To boost the
development of MRC, on one hand, many works proposed novel al-
gorithms by enhancing its language representation [ 14,27,32] and
designing special neural networks [ 13,37,54]. On the other hand,
researchers built various benchmark datasets in various forms, such
as boolean question [ 10], multiple choice [ 24,25], and span extrac-
tion [ 33,51], to test different Natural Language Understanding
(NLU) capabilities of MRC software. There are also many leader-
boards to record the state of the art performance of MRC software
and inspire researchers to improve the algorithms [12, 36, 43].
While MRC has seen substantial progress in the above directions,
its validation method has not received much attention. Almost all
the current MRC software1is validated with the reference-based
measures, such as accuracy. Specifically, given a trained MRC model,
researchers or engineers first need to obtain a well-annotated
dataset, where each sample is with a ground truth label. Then,
the model is validated by checking the consistency between the
label2and the actual output (answer) given by the model.
Though being widely adopted, such a validation method has its
limitations from a practical perspective. Firstly , the well-annotated
labels for test data are compulsory in current validation paradigm.
However, due to the complexity of MRC tasks, it usually costs quite
a lot of human efforts to label these data. Thus, many MRC datasets
include even less than 10k samples [ 55], which are much smaller
than those ordinary deep learning datasets. As a result, given an
MRC model, the validation process will be always restricted to
the current benchmarks, which cannot properly reflect the real
performance of the model when facing various data in plentiful
real-life application scenarios. In other words, it is very possible
that many issues cannot be touched and detected by the the limited
labeled test cases. Therefore, it would be of great help if we are
able to break the dependency on the annotated labels , such
1In this paper, MRC software refers to the implemented MRC model.
2In this paper, â€œlabelâ€ refers specifically to â€œmanually annotated labelâ€.
590
ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Songqiang Chen, Shuo Jin, and Xiaoyuan Xie
that the validation method is no longer restricted to existing bench-
marks, and more comprehensive and adequate validation processes
are possible to be performed to examine the potential issues in
MRC software. Secondly , checking the consistency between the
actual outputs and the ground truth labels may not be a sensible
way to assess the language understanding capability of MRC
software. It is generally uncertain whether an observed consistency
is obtained through truly understanding and inference or some
trickeries like recognizing keywords. Besides, the consistency only
reports an overall performance of the tested MRC software, from
which we can hardly understand its true and specific strength and
weakness. This limitation is also realized by the NLP community
recently [ 18,35]. As a result, a validation method that can assess
to what extent an MRC model can truly approximate different lan-
guage understanding capabilities of human beings, as well as reveal
its specific strength and weakness, is desired.
Therefore, in this paper, we propose a property-based validation
method for MRC software to supplement the reference-based one,
where the data whether labeled or not can be used as test cases
for a much broader range of validation and the true performance
of tested software regarding specific linguistic capabilities can be
assessed. Specifically, we first formulate the linguistic properties
that the MRC software should follow into Metamorphic Relations
(MRs). We then use each MR to construct the follow-up inputs from
the source inputs in the dataset, and check whether violations to
the MR can be observed between the source and follow-up out-
puts. With the violation rate on all the test cases, the language
understanding ability regarding each specific linguistic property
can then be estimated accordingly. In this work, we propose seven
MRs to evaluate the MRC software that solves boolean question
task, from basic linguistic properties such as the reactions with
synonym, tense, and negation.
We conduct a comprehensive experiment to evaluate the ef-
fectiveness of our method. Specifically, we use it to validate four
MRC models built upon mainstream MRC algorithms, i.e., RNN
[26], BERT [ 14], ROBERTa [ 27], and T5 [ 32], respectively, on the
boolean question MRC task, BoolQ [ 10]. The results show that our
method can successfully reveal violations to the seven linguistic
properties on the four models without the need of labels (RQ1). As
compared with the traditional reference-based measure (accuracy),
our method can still reveal several problems on the test data that
software have given consistent answers as the labels (RQ2). Based
on the observed violations using our method, the strength and
weakness of the objective models in their language understanding
abilities could be analyzed (RQ3). Finally, we show that our method
has better effectiveness than the traditional accuracy measure, in
revealing the seeded mutants in the MRC models (RQ4).
In summary, this work makes the following contributions:
â€¢We propose a property-based method to validate the Ma-
chine Reading Comprehension (MRC) software against the
linguistic properties that MRC software should follow. By
bypassing annotations and using various transformations, it
can perform broader validation on more data, and assess the
performance on various linguistic capabilities more deeply.
â€¢We propose and implement seven Metamorphic Relations to
validate the capabilities of MRC software in understanding
the transformation for some basic linguistic properties.â€¢We conduct comprehensive experiments and found that our
method can well supplement the reference-based validation
via stably and effectively revealing erroneous behaviors of
the tested MRC software that have been concealed by the
traditional validation method, under a label-free setting. Be-
sides, it can also give detailed evaluation results in terms of
different linguistic properties for better evaluating the MRC
modelâ€™s language understanding capabilities.
The rest of the paper is structured as follows. Section 2 illustrates
the limitations of current validation methods for MRC software,
which motivates this work. Section 3 introduces the target task
and the methodology background. Then, Section 4 elaborates the
property-based validation method in details, with seven MRs pro-
posed. Section 5 and Section 6 describe the settings and the results
of the experiment to evaluate the effectiveness of the proposed
method, respectively. After that, Section 7 analyzes the threats to
validity and Section 8 discusses some related works. Finally, Sec-
tion 9 draws a conclusion and gives ideas on future work.
The tool, dataset, replication package, and detailed results
for this paper are available online at [1].
2 LIMITATIONS OF THE CURRENT
VALIDATION METHOD FOR MRC
Currently, almost every MRC software is validated with a reference-
based method. However, such a reference-based validation method
has very obvious limitations.
2.1 Compulsory Labels in Test Set
As mentioned in Section 1, a well-annotated dataset is always a
prerequisite in the current validation method. However, such a
requirement has limited the application of this validation method,
mainly due to the difficulty in constructing well-annotated
MRC benchmark datasets .
As compared with label annotation for other ordinary deep learn-
ing tasks, such as image classification, assigning labels for MRC
tasks generally involves much more human efforts. For instance,
to create a labeled boolean question sample, after providing the
question and corresponding related article, the annotator has to
carefully read the article and deduce the answer ( YesorNo) of the
question from the complicated facts reported in the article [ 10].
Obviously, the cost of this process is much higher than that for an-
notating an image. Taking two MRC datasets as specific examples,
SWAG [ 52] cost about 2680 man-hours labour to annotate 113k
inference-based multiple choice questions (approx. 1.4 minutes per
sample); DuReader [ 21] spent even about 51k man-hours to con-
struct 200k fairly complex MRC test cases (approx. 15.3 minutes
per sample), which involves 800 workers and 52 experts. They are
obviously much more costly than the annotation for the commonly-
seen image classification and sentiment analysis samples, which
only require a simple glance from the annotators in general.
As a consequence, many MRC datasets include even less than
10k samples [ 55], which is generally much smaller than those ordi-
nary deep learning datasets. It is generally acknowledged that the
more data adopted in validation, the better that we can reveal the
generalization capability of the learning-based software. However,
the compulsory labels force the current validation method
591Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
to be restricted to existing well-annotated MRC benchmark
datasets , which cannot properly reflect the real performance of
the model when facing plentiful real-life application scenarios. Be-
sides, it is inevitable to have incorrectly labeled sample [ 30], which
introduces unreliability to the validation results as well.
Therefore, it would be very helpful if we can propose a new
MRC validation method, where the labels are no longer mandatory,
such that eligible test data for validation are no longer restricted
to existing labeled benchmarks, and a broader range of validation
processes are possible to be performed.
2.2 Limitation on Revealing the Understanding
Capability with Reference-Based Validation
Building models with good language understanding capabil-
ity, such as inference, is always a core purpose of MRC tasks. How-
ever, current reference-based validation method is not effective
or efficient to evaluate the models from this perspective.
Consider an example of boolean question (will be introduced in
Section 3.1) shown in Figure 1(a), where a ROBERTa-based MRC
software easily gives a correct answer when compared with the
ground truth label of this sample. With many similar samples giving
â€œcorrectâ€ answers, this model can deliver fairly promising accuracy.
However, as shown in Figure 1(b), when we simply rephrase the
question by changing the word â€œsocialâ€ to its synonym â€œsocietalâ€,
or transform the question into a negative declarative sentence form
and followed by a tag question â€œis it rightâ€, the model gets confused
and outputs answers that violate the expected relations with the
original source output.
In fact, the above examples are not rare in MRC application
scenarios. In many cases, the software may infer very complex facts
but meanwhile is insensitive to simple linguistic phenomena. It
may also easily give a correct â€œguessâ€ by memorizing some patterns
or keywords rather than truly understand and deduce the answer
[18]. At the meantime, the traditional reference-based validation
method, which simply reports â€œhow consistentâ€ the actual outputs
and the ground-truth labels are, is not effective in revealing the
actual language understanding capability of the model [35].
It is true that some researchers try to build datasets that test
for more diverse and complex language understanding capabilities
[10,15,18]. This can alleviate the above â€œnon-effectiveâ€ issue to
some extent. However, as introduced in Section 2.1, this idea that
keeps using reference-based validation can be very costly, and hence
not efficient enough.
Therefore, apart from evaluating the traditional reference-based
measures to provide the most basic understanding on the modelâ€™s
performance, it is also necessary to have a validation method that
can effectively and efficiently assess to what extent an MRC model
can approximate the language understanding capability of human
beings, as well as reveal specific strength and weakness in the
modelâ€™s capability.
3 PRELIMINARIES
In this paper, we propose a property-based method to validate MRC
tasks, which does not require any labels in testing dataset, and aims
to evaluate the MRC model in terms of its language understanding
capability. With this method, many data without labels becomeInput Article: Social Studies â€“ In the United States education system,
social studies is the integrated study of multiple fields of social science
and the humanities, including history, geography, and political science.
The term was first coined by American educators around the turn of . . .
Input Question: are social studies and social science the same
Ground Truth Label: No
Output of ROBERTa: No[PASS]
(a)A Boolean Question Sample (Source Case)
Validation with Synonym
Input Article: (same as the source input article)
Input Question: are societal studies and societal science the same
Expected Relation with the Source Output: Consistent Output as
the Output of ROBERTa in (a)
Output of ROBERTa: Yes[VIOLATE]
Validation with Negation
Input Article: (same as the source input article)
Input Question: social studies and social science are not the same, is
it right
Expected Relation with the Source Output: Inversed Output as the
Output of ROBERTa in (a)
Output of ROBERTa: No[VIOLATE]
(b)Follow-up Cases with Violation
Figure 1: A Validation Example on BoolQ dev set
eligible test data and hence the validation becomes much more
comprehensive than the traditional reference-based method.
3.1 Target MRC Task - Boolean Question
As a pioneer study, in this paper, we focus on the Boolean Question
task, which is a common and important MRC task [ 55]. Figure 1(a)
shows an example of boolean question. One boolean question sam-
ple includes a question and an article that provides the evidence
for deducing the answer to the question. The MRC software with
trained MRC model takes the article and question as one piece of
test datum, and is expected to answer YesorNoto the question.
Since the answer of a boolean question is either YesorNo, it is
usually solved with a binary-classification model [ 10]. Such model
is usually built upon some general language models [ 14,27,32] or
special reading models [ 37]. According to the SuperGLUE Leader-
Board [ 40], the models based on some general language models like
ROBERTa [ 27] and T5 [ 32] have already shown fairly comparable
performance as humans in terms of accuracy.
Boolean questions make up an important subset in many popular
MRC benchmark datasets [ 9,24,34]. Among them, BoolQ is a
dataset fully made up of boolean questions, which contains 16k
boolean questions obtained from Google search queries and paired
with paragraphs from Wikipedia that are recognized as sufficient
to deduce the answer. These questions are split into a train set, a
dev set, and a test set. Since it is a benchmark dataset, the authors
of BoolQ only publish the ground truth labels for the questions in
the train set and the dev set. Due to the representativeness, BoolQ
is also adopted as a standard benchmark task in a widely-known
MRC software benchmark platform SuperGLUE [ 43]. Considering
the popularity and the volume of boolean questions in BoolQ, we
use BoolQ as our dataset in this paper as well.
Recently, Gardner et al. [ 18] proposed ContrastSet, which gives
more rigorous test cases that cover diverse and meaningful distribu-
tion near the modelâ€™s decision boundary for 10 NLP tasks, including
592ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Songqiang Chen, Shuo Jin, and Xiaoyuan Xie
BoolQ. All the samples in ContrastSet have manually annotated
labels. Thus, we also adopt the BoolQ part of ContrastSet as a more
challenging and another fully-labeled test set. As a reminder, Con-
trastSet aims to test the trained model more rigorously. Therefore,
the models tested with ContrastSet is the same as the ones tested
with BoolQ dev set and test set.
3.2 Property-Based Validation Method with
Metamorphic Testing
In order to break the dependency on the annotated labels , we
adopt a property-based method, Metamorphic Testing. Metamor-
phic Testing (MT) was proposed to alleviate the oracle problem
during software testing [ 4,8]. Instead of checking the correctness
of each individual testing output, MT compares relations among
multiple inputs and outputs against a list of properties, which are
also known as Metamorphic Relations (MRs). A commonly adopted
example of MT is ğ‘ ğ‘–ğ‘›function. It is very difficult to verify the cor-
rectness of an arbitrary ğ‘ ğ‘–ğ‘›(ğ‘¥). But we know the property that
ğ‘ ğ‘–ğ‘›(ğ‘¥)=ğ‘ ğ‘–ğ‘›(ğœ‹âˆ’ğ‘¥)should always be held. Accordingly, an MR
that â€œwhen ğ‘¥â€²=ğœ‹âˆ’ğ‘¥,ğ‘ ğ‘–ğ‘›(ğ‘¥)=ğ‘ ğ‘–ğ‘›(ğ‘¥â€²)â€ can be derived, where ğ‘¥
is the source test case, and ğ‘¥â€²is the follow-up test case. Usually,
the process of MT can be automated, including generation of the
follow-up test cases based on the MR, execution of the source and
follow-up test cases, as well as checking the relation between the
source and follow-up outputs against the MR.
Because of the usefulness in alleviating the oracle problem, MT
has been widely applied in testing and validating various machine
learning (ML) and deep learning (DL) applications, where the labels
of test data are generally hard to obtain. An early study by Xie
et al. presented and proved a list of MRs to test supervised ML
software [ 47]. In recent years, MT has seen successful applications
in autonomous driving systems (that are based on deep learning
algorithms) [ 42,44,53,56]. Besides, MT shows promising results
in language translation services [19, 20, 39, 50].
Apart from bypassing the prerequisite of test data labels, MT
is also helpful in revealing various properties of the objective sys-
tem. In 2011, Xie et al. [ 47] gave the first evidence to demonstrate
the insufficiency of solely relying on the accuracy in testing and
validating supervised machine learning programs. In 2018, a tool
named METTLE [ 49] was proposed to provide an extensible check-
list of properties (formulated in MRs) for validating, assessing, and
selecting unsupervised ML clustering systems. In 2020, people from
the NLP community started to realize the insufficiency of reference-
based metrics and implemented a tool CHECKLIST [ 35]. CHECK-
LIST mainly proposes a guidance that refers to some commonly
accepted basic linguistic properties to test NLP models. This guid-
ance includes two parts, namely, â€œINV/DIRâ€ (MT-based testing)
and â€œMFTâ€ (template-based unit testing). But in the MT-based part,
CHECKLIST only realizes two primitive MRs for MRC task, by
referring to two widely used robustness-related properties, namely
adding typos and irrelevant sentences [ 16,23]. Other properties are
merely referred by the MFT part, which provides abstract guidelines
to help users define their templates for unit testing. Besides, this
work performs a very sketchy experiment on MRC, which does not
give a comprehensive or detailed understanding about the benefits
for using properties.In this paper, we adopt MT to break the dependency on the
labels , as well as to assess more linguistic properties of the
Boolean Question MRC software via MRs. Comprehensive experi-
ments are also conducted to give in-depth understanding on how
our proposed method supplements reference-based validation.
4 METHODOLOGY
As mentioned above, we propose to validate MRC software with
test cases regardless label existence by considering a list of
linguistic properties that the software should follow. The basic
idea comes from how a human being will react when facing two
statements with a relationship of some linguistic properties like
negation, synonyms, and tense. For example, if a person has really
comprehended the given article and the boolean question (expected
to be judged as YesorNo), then when the question is negated, the
person should give an inversed judgment as the previous one.
These linguistic properties are formulated in Metamorphic Rela-
tions (MRs), denoted as MR={ğ‘€ğ‘… 1,ğ‘€ğ‘… 2,...,ğ‘€ğ‘… ğ‘›}. Eachğ‘€ğ‘…ğ‘–=
(ğ‘¡ğ‘–,ğ‘Ÿğ‘–)defines a transformation ğ‘¡ğ‘–and an output relation ğ‘Ÿğ‘–, which
requires that the outputs of the MRC software before and after
applyingğ‘¡ğ‘–on the article and question in the input should satisfy ğ‘Ÿğ‘–.
More specifically, given an input ğ‘ =(ğ‘,ğ‘)(whereğ‘andğ‘are the
article and the question of ğ‘ , respectively), we denote the output of
ğ‘ as the source output ğ‘œ. We modify ğ‘andğ‘according to the trans-
formation defined by ğ‘¡ğ‘–and obtain a follow-up input ğ‘ â€²=(ğ‘â€²,ğ‘â€²).
We denote the corresponding follow-up output as ğ‘œâ€². Finally, the
relation between ğ‘œandğ‘œâ€²is checked against ğ‘Ÿğ‘–. If the relation ğ‘Ÿğ‘–is
not held, a violation is reported. It can be found that the above pro-
cess does not require any annotated labels, since we do not check
the correctness of either ğ‘œorğ‘œâ€², but only check their relations.
In our method, if an MRC software is always able to give re-
sponses that satisfy the required relation with transformations on
a set of test data, it is said to have a good capability to understand
the given article and the contents depicted by one question and its
variants, as well as to recognize the semantic differences caused by
the corresponding linguistic properties. Consequently, the language
understanding abilities can be estimated accordingly.
With the above property-based validation method, we can tackle
the two limitations in Section 2. Firstly, this method can validate
MRC software without depending on the ground truth labels
of inputs, which does not require laborious manual annota-
tion, and hence can efficiently provide an extensible validation
and tackle the first limitation in Section 2.1. Secondly, the method
automatically constructs test cases regarding diverse forms of lan-
guage expression to validate specific linguistic capabilities of
the MRC software. And instead of simply checking the correctness
of one output against the reference (i.e. the ground truth label), the
method examines various relations among the outputs of related
inputs (which correspond to different linguistic capabilities), which
can help further reveal problems in those test cases passed in
the reference-based validation by some trickery rather than in-
ference. Therefore, this method is helpful in tackling Limitation 2
in Section 2.2.
As a reminder, our method differs from the simple data augmen-
tation in two folds. On one hand, data augmentation still requires
the labels for the original samples; while our method dynamically
593Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
extends the dataset without the requirement of the labels for ei-
ther the source or the follow-up outputs. On the other hand, the
augmented test samples are simply added to the original test set
and still utilized in a reference-based validation. In contrast, our
method aims to reveal the specific linguistic properties on which
the MRC software delivers poor performance.
In this paper, we propose seven MRs from two perspectives. This
list of MRs can be easily expanded by experts or users during the
usage of this method.
4.1 MRs with Inversed Outputs
MR1-1: Invertion with Antonymous Adjective.
It is known that vocabulary is the unit block to form a sentence. In
the first MR, we conduct a â€œantonym recognition testâ€ on adjectives.
Specifically, suppose there is a boolean question input ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—)
and we denote its output as ğ‘œğ‘—, whereğ‘œğ‘—isYesand the question ğ‘ğ‘—
matches one of the following sentential forms:
â€¢be noun. adj. . . .
â€¢be noun1. adj. noun2. . . .
â€¢be noun1. doing adj. noun2. . . .
â€¢be noun1. doing prep. adj.noun2. . . .
â€¢be noun1. done prep. adj. noun2. . . .
If we replace the first adjective in ğ‘ğ‘—with its antonym, the follow-up
outputğ‘œâ€²
ğ‘—should be inversed into No.
As shown in the Example1-1, when Scott and Sid is based on a
true story, they cannot be based on a false story at the same time.
Therefore, when both the source output and the follow-up output
areYes, there must be a mistake between them. That is, the model
can not give the correct answer to at least one of the questions.
Example1-1:
Question of source input: â€œis Scott and Sid based on a true storyâ€
Question of follow-up input: â€œis Scott and Sid based on a false storyâ€
As a reminder, in this MR, a source input will be transformed
only when its output is Yes3. But this is different from checking the
correctness of this output. In fact, regardless of the correctness of
this output, as long as its judgment is Yes, then the corresponding
input is always eligible for MR transformation. Similar requirement
can also be found in the following MR1-2 and MR1-3.
MR1-2: Invertion with Tense Change.
Tense is a mechanism to deliver different temporal information
and indicate the status of a particular event/action. Suppose there
is a boolean question input ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—)whose output ğ‘œğ‘—isYes, the
answer should be inversed after we change the tense of the question
ğ‘ğ‘—. In this paper, we only consider the changes among three tenses,
namely Past Tense ,Present Perfect Tense , and Future Tense , because
the relationship of correctness among them is relatively clear. The
three tenses are recognized according to some widely-seen patterns
as follows:
â€¢Past Tense: didnoun. . . .
â€¢Present Perfect Tense: have/has noun. done . . .
â€¢Future Tense: isnoun. going to / will noun. . . .
3If the source output is No, this MR is not applicable, since it is not guaranteed to have
inversed output in such cases.We transform the question written in Past Tense andPresent Perfect
Tense into Future Tense , and Future Tense into Present Perfect Tense .
Example1-2:
Question of source input: â€œwill there be a fifth season of momâ€
Question of follow-up input: â€œhas there ever been a fifth season of momâ€
In Example 1-2, if the question of source input is Yes, it claims
that the fifth season of the film Mom will be put on in the future.
In contrast, the question of the follow-up input adopts the present
perfect tense to express the existence of the film by now. These
two statements should be contradicted with each other. As a result,
there must be at least one mistake if both the source output and
the follow-up output are Yes.
As a reminder, we only consider the above three patterns because
we found others may introduce ambiguity. For example, a pattern
like â€œwas noun. ...â€ does not guarantee an inversed output after
changing the tense. Consider a sample question â€œwas the dog blackâ€,
if its output is Yes, we cannot expect a question â€œis the dog blackâ€
gives Noanswer.
MR1-3: Invertion with Order Change.
As we know, the word â€œ before â€ and â€œ after â€ indicate opposite
temporal or spatial orders between two things. Inspired by this,
we design this MR to inspect the capability of MRC software in
understanding the order relation in the natural language. Specifi-
cally, suppose there is a boolean question input ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—)whose
questionğ‘ğ‘—contains â€œ before â€ or â€œ after â€ and output ğ‘œğ‘—isYes, the
answer should be inversed if we swap â€œ before â€ and â€œ after â€ inğ‘ğ‘—. If
there are more than one â€œ before â€ or â€œ after â€ inğ‘ğ‘—, we only change
the first â€œ before â€ or â€œ after â€.
Example1-3 gives a case to illustrate this MR from the perspective
of temporal order. In this example, the Peloponnesian War cannot
happen both before and after the Persian War. Therefore, there
must be at least one mistake once both the source output and the
follow-up output are Yes.
Example1-3:
Question of source input: â€œwas the Peloponnesian War before the Persian
Warâ€
Question of follow-up input: â€œwas the Peloponnesian War after the
Persian Warâ€
MR1-4: Invertion with Negation and Tag Question.
Negation is also a basic grammatical transformation, thus we set
up an MR with inversed output by negating a statement. Specifi-
cally, suppose there is a boolean question input ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—), the
answer should be inversed if we negate ğ‘ğ‘—. As a reminder, to avoid
ambiguity, we negate the question into a negative declarative sen-
tence, followed by a tag question â€œis it rightâ€. Considering the BoolQ
task allows questions in free-forms, such forms of negations should
be also valid inputs under the context of BoolQ [10].
Example 1-4 is to illustrate this MR. If the outputs of the source
input and the follow-up input are consistent (either both are Yesor
both are No), the model should make one wrong judgment.
Example1-4:
Question of source input: â€œis there such thing as a black cardâ€
Question of follow-up input: â€œthere is not such thing as a black card, is it
rightâ€
594ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Songqiang Chen, Shuo Jin, and Xiaoyuan Xie
4.2 MRs with Consistent Outputs
MR2-1: Consistence with Synonymous Adjective.
In addition to MR1-1, we also consider to build the follow-up
inputs through adjective synonym substitution to validate MRC soft-
wareâ€™s understanding on adjectives. This MR is similar to MR1-1 in
form, but does not further demand for the restriction on the source
output as Yes. Specifically, suppose there is a boolean question input
ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—)whereğ‘ğ‘—matches one of the patterns introduced in
the definition of MR1-1, if we replace all the adjectives in ğ‘ğ‘—with
their corresponding synonyms, the answer should be consistent.
Example2-1:
Question of source input: â€œcan a tight hat give you a headacheâ€
Question of follow-up input: â€œcan a taut hat give you a headacheâ€
Example2-1 gives an example to illustrate this MR. Since the two
questions express the same meaning, there must be an error if the
model outputs differently on the two inputs.
MR2-2: Consistence with Adverbial Clause Position Change.
Since the position of adverbial clause could be flexible in a sen-
tence, we construct an MR by changing the position of the adverbial
clause to validate the capability of not affected by this changing for
MRC software. Given a boolean question input ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—)where
ğ‘ğ‘—includes an adverbial clause starting with â€œ when â€, â€œinâ€, â€œatâ€, â€œonâ€,
or â€œifâ€, the answer should be consistent if we change the position
of the adverbial clause in the question. Specifically, if the adverbial
clause is at the beginning of the question, we move it to the tail.
Otherwise, we move it to the beginning of the question.
Example2-2 gives an example to illustrate this transformation.
Example2-2:
Question of source input: â€œcan you turn left on red in Canadaâ€
Question of follow-up input: â€œin Canada, can you turn left on redâ€
MR2-3: Consistence with Active/Passive Voice Change.
This MR validates the capability to understand voice for MRC
software. Although the change on the voice may affect the result
of some NLP tasks like sentiment analysis, it would not affect the
fact it describes. As a consequence, it is also a reasonable option
to produce an MR with consistent output accordingly. Different
from the other six MRs, we apply this transformation on the article
ğ‘ğ‘—rather than the question ğ‘ğ‘—of the samples, because the article
has much higher chance to contain modifiable sentences than the
question.
Given a boolean question input ğ‘ ğ‘—=(ğ‘ğ‘—,ğ‘ğ‘—), this MR requires
the answer should be consistent if we change the voice of all the
modifiable sentences in ğ‘ğ‘—. Example2-3 is an example to illustrate
this transformation.
Example2-3:
Article of source input: â€œ...the company now uses the SSE brand through-
out the UK....â€
Article of follow-up input: â€œ...the SSE brand is now used by the company
throughout the UK....â€
4.3 Validation Process Based on MRs
Given an MRC software ğ‘€, the entire validation process is as fol-
lows. Specifically, as shown in Figure 2, for each ğ‘€ğ‘…ğ‘–=(ğ‘¡ğ‘–,ğ‘Ÿğ‘–)
and a test set S={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘›}, we first construct the input set
Figure 2: Visualization of Validation Process
Sğ‘–={(ğ‘ ğ‘–
1,ğ‘ ğ‘–â€²
1),(ğ‘ ğ‘–
2,ğ‘ ğ‘–â€²
2),...,(ğ‘ ğ‘–ğ‘š,ğ‘ ğ‘–â€²ğ‘š)}, whereğ‘ ğ‘–
ğ‘—is an eligible source
input from Sandğ‘ ğ‘–â€²
ğ‘—is the corresponding follow-up input con-
structed from ğ‘ ğ‘–
ğ‘—, based onğ‘¡ğ‘–ofğ‘€ğ‘…ğ‘–. Each pair of ğ‘ ğ‘–
ğ‘—andğ‘ ğ‘–â€²
ğ‘—is tested
onğ‘€, and their outputs are compared against ğ‘Ÿğ‘–ofğ‘€ğ‘…ğ‘–.
The validation result of ğ‘€is then measured with its â€œviolation
rateâ€ (ğ‘‰ğ‘Ÿ), which shows to what extent ğ‘€gives reactions unsat-
isfyingğ‘Ÿğ‘–. Specifically, for each pair (ğ‘ ğ‘–
ğ‘—,ğ‘ ğ‘–â€²
ğ‘—)inSğ‘–, letğ‘œğ‘–
ğ‘—be the
source output and ğ‘œğ‘–â€²
ğ‘—be the follow-up output. If ğ‘€ğ‘…ğ‘–is an MR with
inversed output (MR1-1 to MR1-4), the violation state on (ğ‘ ğ‘–
ğ‘—,ğ‘ ğ‘–â€²
ğ‘—)
is recorded as ğ‘‰ğ‘†ğ‘–
ğ‘—=1ifğ‘œğ‘–
ğ‘—=ğ‘œğ‘–â€²
ğ‘—, otherwise ğ‘‰ğ‘†ğ‘–
ğ‘—=0. Ifğ‘€ğ‘…ğ‘–is an MR
with consistent output (MR2-1 to MR2-3), the violation state on
(ğ‘ ğ‘–
ğ‘—,ğ‘ ğ‘–â€²
ğ‘—)is recorded as ğ‘‰ğ‘†ğ‘–
ğ‘—=1ifğ‘œğ‘–
ğ‘—â‰ ğ‘œğ‘–â€²
ğ‘—, otherwiseğ‘‰ğ‘†ğ‘–
ğ‘—=0. Then, the
violation rate on Sğ‘–in terms ofğ‘€ğ‘…ğ‘–is defined as:
ğ‘‰ğ‘–
ğ‘Ÿ=Ã|Sğ‘–|
ğ‘—=1ğ‘‰ğ‘†ğ‘–
ğ‘—
|Sğ‘–|
Different from traditional reference-based measures (such as
accuracy),ğ‘‰ğ‘Ÿis expected to be as low as possible . The lower ğ‘‰ğ‘Ÿ
that an MRC software can give, the less error it makes and hence
better performance it delivers.
Based the above description, it is not difficult to find that our
method has higher level of automation than the traditional manual
label annotation, and hence can be more efficient during the valida-
tion. Thus, we are now particularly interested in the effectiveness
of this method, and conduct an experiment accordingly.
5 EXPERIMENTAL SETUP
5.1 Research Questions
In this experiment, we aim to address four research questions:
RQ1: Overall effectiveness of the proposed property-based valida-
tion method. This RQ aims to give a global picture on the usefulness
of our method in assessing language understanding capabilities
of the MRC models (to tackle Limitation 2), without using any
annotated labels (to tackle Limitation 1).
RQ2: Comparison with traditional reference-based validation. In
this RQ, we aim to understand how the proposed property-based
method supplements the reference-based validation. Specifically,
we will investigate the relation between the violation rate ğ‘‰ğ‘Ÿand
traditional accuracy , to see the further benefits of using our method.
RQ3: Performance comparison on MRC models from seven lin-
guistic properties. As introduced in Section 2.2, it is also desired to
understand detailed strength and weakness of the MRC models, in
terms of their language understanding capabilities. Thus, in this RQ,
595Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
we compare the MRC models adopted in our experiment, from the
seven MRs (that correspond to seven different linguistic properties).
RQ4: Effectiveness of revealing disturbance on MRC models. In
this RQ, we perform a mutation analysis, to further compare the
effectiveness of our method and the traditional reference-based
method, in revealing the seeded mutants.
5.2 Validation Objects
In this paper, we pick four typical algorithms, namely RNN, BERT,
ROBERTa, and T5, to construct corresponding MRC models as our
validation objects. Among them, ROBERTa and T5 have achieved
comparable accuracy as humans do [ 40]. The four objective models
are described and built as follows:
RNN: RNN [ 26] is a classic method used in early MRC works
[10,13]. We build a Bi-LSTM RNN-based model for text pair classi-
fication as suggested in BoolQ paper [ 10]. The model includes two
Bi-LSTM layers with 200 cells and co-attention mechanism, which
are then followed by two fully-connected linear layers.
BERT: BERT [ 14] is the first large-scale Transformer-based pre-
trained language model (PTM). When proposed, it outperforms
the other methods by large margins on 11 NLP tasks including
many MRC tasks like SQuAD [ 33]. We build a model based on the
base-size BERT to show the early performance of such huge PTMs.
ROBERTa: ROBERTa [ 27] is an improved version of BERT and
shows more robust and even better performance than BERT on
many tasks such as SQuAD [ 33] and RACE [ 25]. We construct a
binary classification model based on the large-size ROBERTa as the
corresponding objective model.
T5:T5 [32] is a Transformer-based Encoder-Decoder model that
creatively solves all textual tasks in a text-to-text form. For example,
the boolean question will be solved by generating the string â€œYes.â€
or â€œNo.â€ after getting the input. Due to the limited GPU memory,
we use the base-size T5 to build the objective model.
The RNN model is built with the implementation from [ 3] and
trained with a batch size at 128 and a learning rate at 0.0001. The
other three models are built with the Transformers library [ 45]
and fine-tuned with a batch size at 8 and a learning rate at 0.00001.
Training and fine-tuning are performed on BoolQ train set that
contains 9427 samples. For each algorithm, the checkpoint with the
best accuracy on BoolQ dev set is picked as the ultimate objective
model to avoid overfitting [14].
5.3 Data Preparation
As introduced in Section 3.1, we have three datasets for validation,
namely, BoolQ dev set (including 3270 samples), BoolQ test set (in-
cluding 3245 samples), and the BoolQ part of ContrastSet (denoted
as ContrastSet for short in the following experiment, including
335 samples). Among these three datasets, only BoolQ dev set and
ContrastSet are released with annotated labels. Therefore, the accu-
racy could only be calculated on these two sample sets, while our
method could be conducted on all these three datasets.
Before conducting the validation with the proposed method,
we first construct the follow-up inputs from the source inputs,
which has been introduced in Section 4.3. During this process,
the adjectives are recognized with SpaCy toolkit [ 17] and their
antonyms and synonyms are generated with WordNet dictionary[29]. For MR1-2 and MR2-3, we use the Pattern library [ 38] to obtain
the verbs in the required tense and voice. And for MR1-4 and MR2-
3, the subjective and predicative clause of one sentence are also
extracted with SpaCy toolkit [17].
To ensure the validity of the generated follow-up cases, we man-
ually inspected all the generated inputs. We inspected the syntactic
validity of the generated follow-up inputs with respect to gram-
mar and semantic meanings. In total, we found less than 7% of the
follow-up inputs with slight errors. We repaired all of them and
put their correct version back to the input set. As a reminder, this
inspection is not aiming to give the answer of the question (the
label for the test case). Instead, it shows a tolerably low error rate
(7% compared with 22%-40% [ 20]), such that users can skip this
manual inspection in their real usage.
5.4 Mutant Generation
In RQ4, we investigate whether our method is sensitive enough
in revealing the seeded mutants in the objective models. Since the
MRC software is based on deep learning models, we adopt the
mutation operators specially designed for neural networks. Ma et
al. [28] proposed eight types of operators to mutate the models
of fully-connected linear layers. However, none of the objective
models in our experiment uses fully-connected linear layers as the
core functional layers. Therefore, we adopt four out of the eight
operators that can be transferred to RNN and Transformer as our
mutation operators [ 22]. The details of the operators are as follows:
Operator 1: Gaussian Fuzzing. Weights of the neurons act
as the key to control the decision logic of the neural networks.
This operator fuzzes the values of the weights for all the target
neurons to change the connection importance they represent, which
is achieved through adding Gaussian noise N(ğœ”,ğœ2). We setğœ”to
be 0 andğœto be 0.01 as suggested [22, 28].
Operator 2: Weight Shuffling. The output of a neuron is usu-
ally determined by the neurons in the previous layer through the
connections with weights. This operator shuffles the weights of ğ‘¥%
randomly picked target neurons to disturb their connections with
their previous layers. We set ğ‘¥to be 1 as suggested [28].
Operator 3: Neuron Effect Blocking. Every neuron in a neu-
ral network contributes to its final decision to some extent. This op-
erator removes the influence of ğ‘¦%randomly picked target neurons
to the final decision, which is achieved by resetting their connection
weights of the next layers to zeros to block the propagation of their
effects. The ğ‘¦is set to be 1 as suggested [28].
Operator 4: Neuron Switch. Different neurons in one neural
network layer usually play different roles on the connected neurons
in the next layer. This operator switches the weights of two neurons
in the same layer to exchange their effects for next layer. Such
exchange is performed on ğ‘§%randomly chosen target neurons for
each layer and ğ‘§is also set to be 1 as suggested [28].
Since the core functional neurons in RNN and the other three
objective models are LSTM cells and Transformer cells, respectively,
we hence regard the LSTM cells and the Transformer cells as the
target neurons during mutation. Specifically, we perform the op-
erators on the weights of the input modulation gate, input gate,
forget gate, and output gate for an LSTM cell and the query-weight,
key-weight, and the value-weight for a Transformer cell.
596ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Songqiang Chen, Shuo Jin, and Xiaoyuan Xie
6 RESULTS AND ANALYSIS
6.1 RQ1: Overall Effectiveness of the Proposed
Property-Based Validation Method
In this RQ, we study the overall effectiveness of our method for
validating MRC models. Table 1 shows the violation rate ( ğ‘‰ğ‘Ÿ) of each
objective model on the three adopted datasets. Generally speaking,
our method delivers fairly good performance to reveal the erroneous
behavior of the objective models. All the ğ‘‰ğ‘Ÿin Table 1 are greater
than 0; 41 out of the total 84 trials have their ğ‘‰ğ‘Ÿhigher than 50%;
and someğ‘‰ğ‘Ÿcan achieve as high as 100.00% (BERT and RNN with
BoolQ test and ContrastSet under MR1-3).
Table 1: Violation Rate ( ğ‘‰ğ‘Ÿ) of Each Objective Model
Dataset Model MR1-1 MR1-2 MR1-3 MR1-4 MR2-1 MR2-2 MR2-3
BoolQ
devROBERTa 69.04% 92.19% 58.33% 44.45% 7.97% 2.61% 1.23%
T5 54.70% 94.94% 73.08% 37.32% 13.09% 5.23% 2.04%
BERT 76.91% 96.05% 92.86% 65.42% 15.06% 9.31% 3.21%
RNN 98.51% 98.51% 96.15% 92.16% 4.33% 6.05% 5.31%
BoolQ
testROBERTa 62.88% 90.55% 60.00% 45.00% 7.83% 3.98% 2.26%
T5 51.48% 89.58% 89.29% 37.61% 13.48% 5.64% 2.37%
BERT 75.44% 90.00% 100.00% 66.49% 16.75% 9.29% 3.47%
RNN 98.49% 95.21% 100.00% 91.11% 6.05% 5.97% 4.90%
Contr-
astSetROBERTa 59.62% 81.25% 80.00% 47.80% 6.72% 5.33% 2.04%
T5 34.15% 84.62% 83.33% 37.42% 15.97% 5.33% 2.04%
BERT 66.67% 92.00% 100.00% 68.24% 10.92% 10.67% 4.08%
RNN 97.92% 92.59% 100.00% 90.57% 6.72% 10.67% 5.10%
More specifically, we found that on the BoolQ test set (Row 6
to Row 9) that lacks ground truth labels for accuracy calculation,
our method can find many violations as on the fully-labeled BoolQ
dev set (Row 2 to Row 5) and ContrastSet (Row 10 to Row 13).
This demonstrates that our method can validate the MRC software
under a label-free setting, which effectively addresses Limitation
1. And the observed violations to each MR clearly demonstrate
the shortcomings of each MRC model in the corresponding lin-
guistic properties, which shows the effectiveness of our method in
alleviating Limitation 2.
6.2 RQ2: Comparison with Traditional
Reference-Based Validation
In this RQ, we aim to study the relation between the violation rate ğ‘‰ğ‘Ÿ
in our method and the traditional accuracy to understand the further
benefit of using our validation method. Therefore, we compare
ğ‘‰ğ‘Ÿand traditional accuracy of each objective model on different
datasets. Since BoolQ test set does not provide labels for accuracy
calculation, we only conduct this comparison on BoolQ dev set
and ContrastSet. Due to the limited space, we only demonstrate
the results with ROBERTa and T5, which are shown in Figure 3.
Comparisons for the other two models give similar conclusions. For
all the detailed data, please refer to [ 1]. In Figure 3, the left seven
clusters of bars (each cluster contains two bars corresponding to
ContrastSet and BoolQ dev datasets) show the ğ‘‰ğ‘Ÿof the seven
MRs. And the right-most cluster of bars present the values of â€œ 1âˆ’
ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ â€ that mean the rates of samples having inconsistent labels
as the given references.
Figure 3: Comparison between Accuracy and Violation Rate
(ğ‘‰ğ‘Ÿ) of ROBERTa and T5 on Two Datasets
From the result, we found that our method can reveal as high
violation rates on the datasets with either poor or promising accu-
racy. This finding is important , because we have noticed some
models with fairly good accuracy on widely adopted datasets
CANNOT actually give good performance in our validation
that assesses different language understanding capabilities.
Let us take ContrastSet as an example, on which both ROBERTa
and T5 give many inconsistent answers to the annotated reference
labels, and hence show relatively poor performance in terms of the
accuracy (presented in the right-most blue bars, where ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦
are 67.42% and 64.48% for ROBERTa and T5, respectively). These
lowğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ are understandable, because the test cases in Con-
trastSet cover more diverse and rigorous perspectives to test the
MRC models [ 18]. On the same dataset, our method also reveals
many violations (the first seven blue bars from the left), where the
first four MRs reveal much more incorrect cases than the ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ .
However, when we run traditional validation on the widely used
BoolQ dev set, both ROBERTa and T5 start to give quite satisfac-
tory accuracy. As shown in the last orange bars, the ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ are
as high as 84.92% and 82.81% for ROBERTa and T5, respectively.
Such performance is already very close to that of human beings
(ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =89.00% reported in [ 40]). Based on the accuracy, these
results are promising enough. However, our method can warn that
these models do NOT really show good language understanding
capabilities: the first seven orange bars from the left demonstrate
violations to the seven linguistic properties. Especially in MR1-1,
MR1-2, MR1-3, and MR1-4, the violation rates are very high, and
some of them are even higher than those on ContrastSet.
In fact, there are many cases where test samples give con-
sistent results as the reference labels, but show violations
in our validation . The example in Figure 1 just belongs to these
cases, where the test input passes in reference-based validation, but
violates MR2-1 and MR1-4. Take the results on BoolQ dev set as
an example, in ROBERTa, we have found 84.19%, 90.68%, 85.71%,
82.87%, 72.84%, 75.00%, and 45.00% of the violations revealed by
the seven MR respectively, have their source test cases passed the
reference-based validation. Similar results are observed in T5, where
597Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
82.00%, 88.93%, 84.21%, 76.58%, 78.20%, 50.00%, and 66.67% of the
violations revealed by the seven MRs respectively, have passed the
reference-based validation.
Among all such observed violations, we consider that the objec-
tive models either fail to make a true understanding on the fact
behind the source and follow-up cases, or cannot handle the simple
transformations. In other words, our method can well supplement
the traditional reference-based methods and reveal problems that
have been concealed by the reference-based methods. We will give
more in-depth discussion about this observation in Section 6.5.
Therefore, we conclude the strengths of our method compared
with accuracy of two folds. On one hand, our method is effective
and stable in revealing the linguistic property-related errors for
MRC models on different datasets. On the other hand, our method
can find samples where the software passes the reference-based
validation yet probably without truly understanding the sentence
(i.e. violations in our MRs). Both the strengths show the benefit
of using our method in providing in-depth validation for MRC
software and thus addressing Limitation 2.
6.3 RQ3: Performance Comparison on MRC
Models from Seven Linguistic Properties
In this RQ, we discuss the performance of the objective models
regarding their capability of the seven linguistic properties, based
on the validation results reported by our method, in order to un-
derstand the strength and weakness of the models. Here we merge
the three datasets as a whole to perform the comparison. Figure 4
displays the violation rates ( ğ‘‰ğ‘–ğ‘Ÿ) of each objective model on the
whole dataset. Different colors of bars represent different models,
and each cluster of bars gives ğ‘‰ğ‘–ğ‘Ÿof the models regarding each ğ‘€ğ‘…ğ‘–.
Figure 4: Comparison on Different MRC Models
First, it can be found from Figure 4 that the RNN model (red bars)
has the worst performance in the most cases (the higher, the worse).
As compared with the other three models that were pre-trained on
huge corpus to learn understanding languages, the simple RNN-
based model only learns to solve the boolean question task on the
BoolQ train set. And we guess this may be the major reason that
RNN is less â€œsmartâ€ than the other three models in most of these
linguistic properties.
After that, we further compare the performance of the remained
ROBERTa, T5, and BERT, which are all built upon the complicated
transformer-based pre-trained language models (PTMs) and show
supreme results and potential intelligence on many popular tasks.
On all the MRs, BERT-based model (orange bars) makes the most
violations among the three models. This might be because BERT isthe earliest technique and hence has fairly weak capability to under-
stand the languages. At the meantime, ROBERTa model (blue bars)
gives the best performance except on MR1-1 and MR1-4, which
means ROBERTa-based model has the general best performance
in many aspects. However, on MR1-1 and MR1-4, T5 model (green
bars) achieves better performance than ROBERTa (blue bars), which
means that T5 has a stronger capability on understanding adjective
antonyms and negations. We consider this is mainly because T5
was pre-trained on the 750GB C4 corpus, which provides much
cleaner and more abundant learning source than the 160GB ordi-
nary English-language corpus adopt by ROBERTa [32].
Additionally, we notice that all the three PTM-based objective
models have a lot of violations on MR1-2. We consider the reason
may be due to the existing pre-trained tasks for the three models,
such as mask language model (MLM) and next sentence prediction
(NSP), do not pay much attention on the tense of the sentence. It
reminds us that current state-of-the-art MRC software is still fairly
unskillful with tense, which is a very important and basic linguistic
property in our daily life. We also found from the result that all the
four objective models violate more on the MRs with inversed output
(MR1-1 to MR1-4) than the MRs with consistent output (MR2-1 to
MR2-3). From this we conclude that these models tend to keep the
answers when small perturbations are applied to the inputs.
To summarize, our method can reveal specific weakness of MRC
models regarding various linguistic properties and hence help peo-
ple to have some ideas about different language understanding
capabilities of the MRC models. This is also tackling Limitation 2.
6.4 RQ4: Effectiveness of Revealing
Disturbance on MRC Models
In this RQ, we investigate how effective our method is in revealing
the seeded mutants in MRC models. Given a validation method
ğ‘‰and an objective model ğ‘€0, letR={ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘™}be the set of
samples on which ğ‘€0can pass under ğ‘‰, andM={ğ‘€â€²
1,ğ‘€â€²
2,...,ğ‘€â€²
ğ‘˜}
be a set of mutants of ğ‘€0(introduced in Section 5.4). If mutant ğ‘€â€²
ğ‘–
fails4on more samples in Runder the validation method ğ‘‰ğ‘than
ğ‘‰ğ‘, we say that ğ‘‰ğ‘is more effective in revealing the mutants than
ğ‘‰ğ‘(i.e.,ğ‘‰ğ‘is more sensitive to the disturbance on the model).
We first run the traditional reference-based validation on ğ‘€0
with all the labeled samples in BoolQ dev set and ContrastSet, re-
spectively, to obtain the passed test samples R. Then, we iteratively
perform the same validation on each mutant ğ‘€â€²
ğ‘–withRand count
the number of test samples from Rgiving inconsistent results (de-
noted asğ‘â‰ ). Finally, we have the ratio of the inconsistent results
onR, which is defined as: ğ‘…Î”=ğ‘â‰ 
|R|. We perform our property-based
validation with the same process. According to the above descrip-
tion, the higher this ğ‘…Î”is, the better the corresponding validation
method is in revealing the mutant ğ‘€â€²
ğ‘–. Therefore, we have |M|=ğ‘˜
batches of results, for both reference-based and our property-based
validation methods. In the experiment, we set ğ‘˜as 10.
With these data, we conduct the paired Wilcoxon signed-rank
tests [ 11] to statistically compare the effectiveness of our method
and validation with accuracy in revealing the mutants. In the test,
4For traditional validation method, â€œfailâ€ means that the test sample gives inconsistent
output as the reference label; while for our validation method, â€œfailâ€ means the test
sample and its follow-up sample give outputs that violate a particular MR.
598ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Songqiang Chen, Shuo Jin, and Xiaoyuan Xie
Table 2: Paired Wilcoxon Signed-Rank Test Results
Mutation
OperatorType MR1-1MR1-2MR1-3MR1-4MR2-1MR2-2MR2-3
Gaussian
Fuzzing2-tailed 0.0137 0.0039 0.5566 0.0020 0.0020 0.9219 0.0137
1-tailed 0.0068 0.0020 0.2783 0.0010 0.0010 0.4609 0.0068
Conclusion Better Better Similar Better Better Similar Better
Weight
Shuffling2-tailed 0.0020 0.0020 0.0020 0.0020 0.1602 0.0020 0.0020
1-tailed 0.0010 0.0010 0.0010 0.0010 0.9346 1.0000 1.0000
Conclusion Better Better Better Better Similar Worse Worse
Neuron Eff-
ect Blocking2-tailed 0.0039 0.0020 0.0039 0.0020 0.0645 0.2754 0.0020
1-tailed 0.0020 0.0010 0.0020 0.0010 0.0322 0.8838 1.0000
Conclusion Better Better Better Better Similar Similar Worse
Neuron
Switch2-tailed 0.0020 0.0020 0.0098 0.0020 0.1602 0.9219 0.0020
1-tailed 0.0010 0.0010 0.0049 0.0010 0.0801 0.5771 1.0000
Conclusion Better Better Better Better Similar Similar Worse
we compare ğ¹(ğ‘¥)(i.e. the list of ğ‘…Î”with traditional validation for
all mutants) and ğº(ğ‘¦)(i.e. the list of ğ‘…Î”with our property-based
validation for all mutants). Since we have seven MRs, there are
seven lists of ğº(ğ‘¦)accordingly. The test is conducted in both 2-
tailed and 1-tailed manners, at the ğœlevel of 0.05. For the 2-tailed
p-value, ifğ‘â‰¥ğœ, the null hypothesis ğ»0thatğ¹(ğ‘¥)andğº(ğ‘¦)are
NOT significantly different is accepted. Otherwise, the alternative
hypothesis ğ»1thatğ¹(ğ‘¥)andğº(ğ‘¦)are significantly different is
accepted. For the 1-tailed p-value (we consider the lower case in
this work), if ğ‘â‰¥ğœ,ğ»0thatğ¹(ğ‘¥)do NOT significantly tend to be
less than the ğº(ğ‘¦)is accepted. Otherwise, ğ»1thatğ¹(ğ‘¥)significantly
tend to be less than ğº(ğ‘¦)is accepted.
Thus, we have:
â€¢In the 2-tailed test, ğ»0being accepted (when ğ‘â‰¥ğœ) indicates
that the our property-based validation (with a particular MR)
issimilar to the traditional reference-based validation in
revealing the seeded mutants.
â€¢In the 1-tailed test, ğ»1being accepted (when ğ‘<ğœ) means
that our property-based validation (with a particular MR)
isbetter than the traditional reference-based validation in
revealing seeded mutants.
â€¢Otherwise, our property-based validation (with a particular
MR) is worse than the traditional reference-based validation
in revealing seeded mutants.
Table 2 shows the p-values of the comparison â€œ ğ¹(ğ‘¥)v.s.ğº(ğ‘¥)â€,
where theğº(ğ‘¥)in Column 3 to Column 9 corresponds to the seven
MRs. The results for the other models give similar conclusions.
Due to the limited space, please to refer to [ 1] for all figures. From
the result we can find that in terms of revealing seeded mutants,
which could be seen as the model with performance decrease, our
method is better than the traditional accuracy-based validation in
17 out of the 28 tests, is similar to accuracy in 7 out of the 28 tests,
and is worse than accuracy in only 4 out of the 28 tests. Besides,
we found that 15 out of the 17 â€œBetterâ€ cases come from MR1-1
to MR1-4. Meanwhile, 6 out of the 7 â€œSimilarâ€ cases and all the 4
â€œWorseâ€ cases are reported on MR2-1 to MR2-3. This indicates that
the MRs with inversed output tend to have better performance to
reveal the disturbance than the MRs with consistent output.6.5 Discussion on the Strengths of Our Method
in Revealing Issues
The experimental results have shown that our proposed method is
helpful in breaking the dependency on labels and revealing issues
that were not detected by traditional reference-based methods, and
hence, can be a promising supplementary to the traditional methods.
In fact, a very likely reason for the observed strengths is that our
method enables the adoption of much more test cases from two
aspects (where the real linguistic capabilities of the MRC model are
tested), with relatively low cost.
Firstly, our method breaks the dependency on labels and hence
makes much more unlabeled data become available for test-
ing. As discussed above, due to the high cost of manual annotation,
there are a fairly limited number of well-labeled benchmark datasets.
Our method provides one effective and efficient manner for enlarg-
ing the available test cases, without considering the existence of
their annotated labels, and thus can largely broaden the scale
of the validation for MRC software and reveal issues that are
not touched by the limited existing benchmarks.
Secondly, our method constructs new test cases from current
test cases, and thus brings more executions and increases the
chance of revealing issues . Furthermore, the new (i.e. follow-up)
test cases are constructed from the original (i.e. source) test cases by
considering the capabilities of seven linguistic properties of
the MRC software. Consequently, through the violations between
the source and follow-up test cases, we can assess the capability of
MRC software on the corresponding linguistic property. In contrast,
traditional reference-based methods can only check the consistency
between the real outputs and the annotated labels (that are simply
â€œYesâ€ or â€œ Noâ€ for our target, boolean question MRC task) of the
existing source test cases. A high accuracy with such reference-
based methods solely is not convincing to show whether such
â€œintelligentâ€ software truly understand the test cases, not to mention
the errors in the manual annotations.
In a word, the property-based validation with MT provides more
perspectives to inspect MRC software, enables the exploration to a
much larger input domain, and thus can reveal new issues5. In fact,
apart from alleviating the oracle problem, â€œgetting more avail-
able test casesâ€ has always been an important advantage of
MT since it was firstly proposed. The very first paper by Chen et
al. [4] that proposed MT just aims to generate next test cases from
current ones. The initial purpose of MT was to utilize the current
passed test cases (that are considered to be useless), transform them
into new test cases via Metamorphic Relations with low cost, and
try to reveal failures from these new test cases. This is fully under-
standable as in general, the more test cases are executed, the higher
chance a failure can be revealed. This has been repeated in their
follow-up studies [ 2,5â€“7]. The advantages that are similar to our
experiment were also shown by Chenâ€™s experiments: A classifier
with good accuracy may also give fairly high violation rate in MT
and is shown to contain defects [ 47]; the widely adopted Siemens
Suite was revealed to contain real-life bugs by MT [48].
To conclude, given an MRC model, developers are strongly sug-
gested not to solely rely on the reference-based validation. They
5In our experiment, the new issues revealed in BoolQ test set should belong to Case 1;
the new issues revealed in BoolQ dev set and ContrastSet should belong to Case 2.
599Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
should also perform the property-based validation with MT
on much more data, regardless of the existence of labels. This helps
to reveal more issues via broader validation and executions and gain
deeper understanding on the true comprehension and generaliza-
tion capability of the MRC model. In this way, if the model performs
well on both methods, the modelâ€™s good capabilities behind promis-
ing reference-based validation results can be demonstrated and
concretely explained. Otherwise, it gives an alarm that developers
should take care and try to fix either the weakness of the tested MRC
software or the insufficiency of the labeled benchmarks regarding
the related properties (or both).
7 THREATS TO VALIDITY
The first threat to validity of this work comes from the sizes of
the adopted datasets, which are smaller than those for image-related
tasks. However, as compared with traditional reference-based vali-
dation, our method has already successfully alleviated this problem
to some extent, by enlarging the available test data (Section 2.1).
Besides, in our experiment, we have revealed fairly high rates of vio-
lations with current datasets, which are already meaningful enough
to understand the weakness of a particular model. When more data
can be adopted (regardless of the availability of the labels), there
should be more violations revealed.
The second threat to validity is that we only proposed and imple-
mented seven MRs, which do not cover all the linguistic properties
that the MRC software for boolean question should follow. It is true
that this may lead to missing of some potential errors. However, this
is also the most fundamental limitation of testing and validation,
that is, we can never claim that a software is bug-free via testing or
validation. But we will continuously improve this validation tool
by including more properties. In fact, inspired by the METTLE tool
that provides an extensible checklist of properties for clustering
systems [ 49], we also allow the users of our validation tool to create
new MRs from their specific requirements.
The last threat to validity comes from the correctness of the
follow-up cases. To decrease this threat as much as possible, we
carefully inspected the syntactic validity of the generated follow-up
inputs regarding grammar and semantic meanings and repaired
the erroneous inputs before validation, as introduced in Section 5.3.
In fact, this is a common problem in MT-based validation for deep
learning tasks [ 20,39,44], and it is never possible to guarantee that
the test inputs are error-free. But in this work, we also inspected
each violation detected by our method, and can guarantee that the
revealed violations do not contain any false positive, and hence the
conclusions in the experiment are reliable and convincing.
8 RELATED WORKS
Reference-based Validation on MRC. To test the performance
of MRC software, many works created various benchmark datasets
in the forms of cloze-style [ 31,41], choice-based [ 10,24,25], extrac-
tion [ 33,51], and others [ 21] for reference-based validation. Some
works also studied the robustness using adversarial samples with
typos [ 16], irrelevant sentences [ 23], etc. [ 46]. Unlike these works
that require labeled samples, our method can validate MRC software
regarding different properties without labels. Recently, Contrast-
Set [18] noticed the loose behavior restriction along NLP modelâ€™sdecision boundary of existing datasets and carried more rigorous
evaluation by manually extending labeled test cases with some
transformations. But it still requires labels and does not evaluate
the performance regarding each capability as our method does.
Property-based validation on ML/DL. In 2011, Xie et al. [ 47]
first demonstrated the insufficiency of solely relying on the ac-
curacy in testing and validating supervised ML programs. Then,
METTLE [ 49] was proposed in 2018 to provides an extensible check-
list of properties for unsupervised ML clustering systems. In 2020,
NLP researchers also realized the insufficiency of reference-based
metrics and implemented one tool CHECKLIST [ 35] to test the
NLP models considering some commonly accepted basic proper-
ties that a model should follow. They realized two primitive MRs
for MRC and left the other properties as the guidance for manual
development in unit tests. Recently, MT has also seen successful
applications in other DL domains. Some works propose MRs to
check the output relations, such as semantic [ 50], structural [ 20]
and pathological invariance [ 19], among the translation results to
find bugs in the translation service, which is also NLP software.
Sun et al. [ 39] repair the erroneous translations with MT. Many
works also use the relation of output steering angles of autonomous
driving system to validate them without calculating the complex
oracle [ 42,44,53,56]. Inspired by these works, our work goes fur-
ther along the direction to reveal the limitation of reference-based
validation and supplement it. Our work leverages MT on MRC, with
at the first time seven specific MRs for MRC proposed. Besides some
fundamental and indispensable properties, we also consider more
not-that-basic properties about tense, voice, preposition locations,
etc. Moreover, we elaborate the in-depth study and discussion on
the strengths of such a property-based validation method on MRC.
9 CONCLUSION AND FUTURE WORK
In this paper, we propose a property-based method to validate
MRC software, which can supplement reference-based validation
by breaking the dependency on the costly annotated labels and
assessing diverse language understanding abilities. This is achieved
via validating MRC software against seven Metamorphic Relations
considering different linguistic properties. The experimental results
on four typical MRC models for boolean question task show that
our method can successfully reveal violations to the target linguis-
tic properties without needing any labels. Moreover, it can reveal
problems that have been concealed by the traditional validation
method. And the comparison against the MRs provides deeper and
more concrete ideas about distinct language understanding capa-
bilities of MRC models. In the future, we will enhance this method
by expanding the MR list to include more properties, combining
current MRs to form new MRs, and targeting other MRC tasks. Au-
tomatic input validity inspection and the root cause identification
of violations will also be considered.
ACKNOWLEDGMENT
This work was partially supported by the National Natural Sci-
ence Foundation of China under the grant numbers 61972289 and
61832009. The numerical calculations in this paper have been par-
tially done on the supercomputing system in the Supercomputing
Center of Wuhan University.
600ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Songqiang Chen, Shuo Jin, and Xiaoyuan Xie
REFERENCES
[1][n.d.]. The tool, dataset, replication package, and detailed results for this paper.
https://github.com/imcsq/FSE21-MT4MRC.
[2]Yuxiang Cao, Zhiquan Zhou, and Tsong Yueh Chen. 2013. On the Correlation
between the Effectiveness of Metamorphic Relations and Dissimilarities of Test
Case Executions. In Proceedings of the 2013 International Conference on Quality
Software, Najing, China, July 29-30, 2013 . IEEE, 153â€“162. https://doi.org/10.1109/
QSIC.2013.43
[3]Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen.
2017. Enhanced LSTM for Natural Language Inference. In Proceedings of the
2017 Annual Meeting of the Association for Computational Linguistics, ACL 2017,
Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers , Regina Barzilay
and Min-Yen Kan (Eds.). Association for Computational Linguistics, 1657â€“1668.
https://doi.org/10.18653/v1/P17-1152
[4]Tsong Yueh Chen, Shing-Chi Cheung, and Siu-Ming Yiu. 1998. Metamorphic
testing: a new approach for generating next test cases. Technical Report HKUST-
CS98-01. Department of Computer Science, Hong Kong University.
[5]Tsong Yueh Chen, DH Huang, TH Tse, and Zhi Quan Zhou. 2004. Case studies on
the selection of useful relations in metamorphic testing. In Proceedings of the 2004
Ibero-American Symposium on Software Engineering and Knowledge Engineering .
Citeseer, 569â€“583.
[6]Tsong Yueh Chen, Fei-Ching Kuo, Ying Liu, and Antony Tang. 2004. Metamor-
phic Testing and Testing with Special Values. In Proceedings of the 2004 Interna-
tional Workshop on Source Code Analysis and Manipulation, 15-16 September 2004,
Chicago, IL, USA . IEEE Computer Society, 128â€“134.
[7]Tsong Yueh Chen, Fei-Ching Kuo, T. H. Tse, and Zhiquan Zhou. 2003. Metamor-
phic Testing and Beyond. In Proceedings of the 2003 International Workshop on
Software Technology and Engineering Practice, 19-21 September 2003, Amsterdam,
The Netherlands . IEEE Computer Society, 94â€“100. https://doi.org/10.1109/STEP.
2003.18
[8]Tsong Yueh Chen, Fei-Ching Kuo, Huai Liu, Pak-Lok Poon, Dave Towey, T. H.
Tse, and Zhi Quan Zhou. 2018. Metamorphic Testing: A Review of Challenges
and Opportunities. Comput. Surveys 51, 1, Article 4 (Jan. 2018), 27 pages. https:
//doi.org/10.1145/3143561
[9]Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy
Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answering in Context. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2018, Brussels, Belgium, October 31 - November 4, 2018 , Ellen
Riloff, David Chiang, Julia Hockenmaier, and Junâ€™ichi Tsujii (Eds.). Association
for Computational Linguistics, 2174â€“2184. https://doi.org/10.18653/v1/d18-1241
[10] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the Surprising Dif-
ficulty of Natural Yes/No Questions. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June
2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and
Thamar Solorio (Eds.). Association for Computational Linguistics, 2924â€“2936.
https://doi.org/10.18653/v1/n19-1300
[11] Gregory Corder and Dale Foreman. 2009. Nonparametric Statistics for Non-
Statisticians: A Step-By-Step Approach. (01 2009). https://doi.org/10.1002/
9781118165881
[12] Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. 2020. MuTual: A
Dataset for Multi-Turn Dialogue Reasoning. In Proceedings of the 2020 Annual
Meeting of the Association for Computational Linguistics, ACL 2020, Online, July
5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.).
Association for Computational Linguistics, 1406â€“1416. https://doi.org/10.18653/
v1/2020.acl-main.130
[13] Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu.
2017. Attention-over-Attention Neural Networks for Reading Comprehension.
InProceedings of the 2017 Annual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long
Papers , Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational
Linguistics, 593â€“602. https://doi.org/10.18653/v1/P17-1055
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill
Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-
tional Linguistics, 4171â€“4186. https://doi.org/10.18653/v1/n19-1423
[15] Daria Dzendzik, Carl Vogel, and Jennifer Foster. 2021. English Machine
Reading Comprehension Datasets: A Survey. CoRR abs/2101.10421 (2021).
arXiv:2101.10421 https://arxiv.org/abs/2101.10421
[16] Steffen Eger and Yannik Benz. 2020. From Hero to ZÃ©roe: A Benchmark of
Low-Level Adversarial Attacks. In Proceedings of the 2020 Conference of the Asia-
Pacific Chapter of the Association for Computational Linguistics and the 10th
International Joint Conference on Natural Language Processing, AACL/IJCNLP
2020, Suzhou, China, December 4-7, 2020 , Kam-Fai Wong, Kevin Knight, andHua Wu (Eds.). Association for Computational Linguistics, 786â€“803. https:
//www.aclweb.org/anthology/2020.aacl-main.79/
[17] Explosion. [n.d.]. SpaCy Toolkit. https://spacy.io/.
[18] Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao
Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish
Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiang-
ming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A.
Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben
Zhou. 2020. Evaluating Modelsâ€™ Local Decision Boundaries via Contrast Sets.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020 , Trevor
Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics,
1307â€“1323. https://doi.org/10.18653/v1/2020.findings-emnlp.117
[19] Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su. 2020. Machine trans-
lation testing via pathological invariance. In Proceedings of the 2020 Joint Eu-
ropean Software Engineering Conference and Symposium on the Foundations of
Software Engineering, Virtual Event, ESEC/FSE 2020, USA, November 8-13, 2020 ,
Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (Eds.). ACM, 863â€“875.
https://doi.org/10.1145/3368089.3409756
[20] Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-invariant testing
for machine translation. In Proceedings of the 2020 International Conference on
Software Engineering, Seoul, ICSE 2020, South Korea, 27 June - 19 July, 2020 , Gregg
Rothermel and Doo-Hwan Bae (Eds.). ACM, 961â€“973. https://doi.org/10.1145/
3377811.3380339
[21] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong
Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018.
DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world
Applications. In Proceedings of the Workshop on Machine Reading for Question
Answering@ACL 2018, Melbourne, Australia, July 19, 2018 , Eunsol Choi, Min-
joon Seo, Danqi Chen, Robin Jia, and Jonathan Berant (Eds.). Association for
Computational Linguistics, 37â€“46. https://doi.org/10.18653/v1/W18-2605
[22] Qiang Hu, Lei Ma, Xiaofei Xie, Bing Yu, Yang Liu, and Jianjun Zhao. 2019. Deep-
Mutation++: A Mutation Testing Framework for Deep Learning Systems. In
Proceedings of the 2019 International Conference on Automated Software Engi-
neering, ASE 2019, San Diego, CA, USA, November 11-15, 2019 . IEEE, 1158â€“1161.
https://doi.org/10.1109/ASE.2019.00126
[23] Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating Reading
Comprehension Systems. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017 , Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.).
Association for Computational Linguistics, 2021â€“2031. https://doi.org/10.18653/
v1/d17-1215
[24] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and
Dan Roth. 2018. Looking Beyond the Surface: A Challenge Set for Reading
Comprehension over Multiple Sentences. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA,
June 1-6, 2018, Volume 1 (Long Papers) , Marilyn A. Walker, Heng Ji, and Amanda
Stent (Eds.). Association for Computational Linguistics, 252â€“262. https://doi.
org/10.18653/v1/n18-1023
[25] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017.
RACE: Large-scale ReAding Comprehension Dataset From Examinations. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017 , Martha
Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association for Computational
Linguistics, 785â€“794. https://doi.org/10.18653/v1/d17-1082
[26] Zachary Chase Lipton. 2015. A Critical Review of Recurrent Neural Networks
for Sequence Learning. CoRR abs/1506.00019 (2015). arXiv:1506.00019 http:
//arxiv.org/abs/1506.00019
[27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).
arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[28] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepMutation:
Mutation Testing of Deep Learning Systems. In Proceedings of the 2018 Inter-
national Symposium on Software Reliability Engineering, ISSRE 2018, Memphis,
TN, USA, October 15-18, 2018 , Sudipto Ghosh, Roberto Natella, Bojan Cukic,
Robin S. Poston, and Nuno Laranjeiro (Eds.). IEEE Computer Society, 100â€“111.
https://doi.org/10.1109/ISSRE.2018.00021
[29] George A. Miller. 1995. WordNet: A Lexical Database for English. Commun. ACM
38, 11 (1995), 39â€“41. https://doi.org/10.1145/219717.219748
[30] Curtis G. Northcutt, Anish Athalye, and Jonas Mueller. 2021. Pervasive Label Er-
rors in Test Sets Destabilize Machine Learning Benchmarks. CoRR abs/2103.14749
(2021). arXiv:2103.14749 https://arxiv.org/abs/2103.14749
[31] Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David A. McAllester.
2016. Who did What: A Large-Scale Person-Centered Cloze Dataset. In Proceed-
ings of the 2016 Conference on Empirical Methods in Natural Language Processing,
601Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
EMNLP 2016, Austin, Texas, USA, November 1-4, 2016 , Jian Su, Xavier Carreras,
and Kevin Duh (Eds.). The Association for Computational Linguistics, 2230â€“2235.
https://doi.org/10.18653/v1/d16-1241
[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach.
Learn. Res. 21 (2020), 140:1â€“140:67. http://jmlr.org/papers/v21/20-074.html
[33] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Donâ€™t Know:
Unanswerable Questions for SQuAD. In Proceedings of the 2018 Annual Meeting
of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia,
July 15-20, 2018, Volume 2: Short Papers , Iryna Gurevych and Yusuke Miyao (Eds.).
Association for Computational Linguistics, 784â€“789. https://doi.org/10.18653/
v1/P18-2124
[34] Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. CoQA: A Conversa-
tional Question Answering Challenge. Trans. Assoc. Comput. Linguistics 7 (2019),
249â€“266. https://transacl.org/ojs/index.php/tacl/article/view/1572
[35] Marco TÃºlio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020.
Beyond Accuracy: Behavioral Testing of NLP Models with CheckList. In Proceed-
ings of the 2020 Annual Meeting of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and
Joel R. Tetreault (Eds.). Association for Computational Linguistics, 4902â€“4912.
https://doi.org/10.18653/v1/2020.acl-main.442
[36] Marzieh Saeidi, Max Bartolo, Patrick S. H. Lewis, Sameer Singh, Tim RocktÃ¤schel,
Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation
of Natural Language Rules in Conversational Machine Reading. In Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP
2018, Brussels, Belgium, October 31 - November 4, 2018 , Ellen Riloff, David Chiang,
Julia Hockenmaier, and Junâ€™ichi Tsujii (Eds.). Association for Computational
Linguistics, 2087â€“2097. https://doi.org/10.18653/v1/d18-1233
[37] Timo Schick and Hinrich SchÃ¼tze. 2020. Exploiting Cloze Questions for Few-Shot
Text Classification and Natural Language Inference. CoRR abs/2001.07676 (2020).
arXiv:2001.07676 https://arxiv.org/abs/2001.07676
[38] Tom De Smedt and Walter Daelemans. 2012. Pattern for Python. J. Mach. Learn.
Res.13 (2012), 2063â€“2067. http://dl.acm.org/citation.cfm?id=2343710
[39] Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
Automatic testing and improvement of machine translation. In Proceedings of
the 2020 International Conference on Software Engineering, ICSE 2020, Seoul, South
Korea, 27 June - 19 July, 2020 , Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM,
974â€“985. https://doi.org/10.1145/3377811.3380420
[40] SuperGLUE. [n.d.]. SuperGLUE Benchmark Leadorboard. https://super.
gluebenchmark.com/leaderboard.
[41] Simon Suster and Walter Daelemans. 2018. CliCR: a Dataset of Clinical Case Re-
ports for Machine Reading Comprehension. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana,
USA, June 1-6, 2018, Volume 1 (Long Papers) , Marilyn A. Walker, Heng Ji, and
Amanda Stent (Eds.). Association for Computational Linguistics, 1551â€“1563.
https://doi.org/10.18653/v1/n18-1140
[42] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: auto-
mated testing of deep-neural-network-driven autonomous cars. In Proceedings
of the 2018 International Conference on Software Engineering, ICSE 2018, Gothen-
burg, Sweden, May 27 - June 03, 2018 , Michel Chaudron, Ivica Crnkovic, Marsha
Chechik, and Mark Harman (Eds.). ACM, 303â€“314. https://doi.org/10.1145/
3180155.3180220
[43] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A Stickier
Benchmark for General-Purpose Language Understanding Systems. In Proceed-
ings of the 2019 Annual Conference on Neural Information Processing Systems,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 3261â€“3275. https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html
[44] Shuai Wang and Zhendong Su. 2020. Metamorphic Object Insertion for Testing
Object Detection Systems. In Proceedings of the 2020 International Conference on
Automated Software Engineering, ASE 2020, Melbourne, Australia, September 21-25,
2020. IEEE, 1053â€“1065. https://doi.org/10.1145/3324884.3416584
[45] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davi-
son, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen
Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexan-
der M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-
20, 2020 , Qun Liu and David Schlangen (Eds.). Association for Computational
Linguistics, 38â€“45. https://doi.org/10.18653/v1/2020.emnlp-demos.6
[46] Winston Wu, Dustin Arendt, and Svitlana Volkova. 2020. Evaluating Neural
Machine Comprehension Model Robustness to Noisy Inputs and Adversarial
Attacks. CoRR abs/2005.00190 (2020). arXiv:2005.00190 https://arxiv.org/abs/
2005.00190
[47] Xiaoyuan Xie, Joshua W.K. Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and
Tsong Yueh Chen. 2011. Testing and validating machine learning classifiers
by metamorphic testing. Journal of Systems and Software 84, 4 (2011), 544â€“558.
https://doi.org/10.1016/j.jss.2010.11.920 The Ninth International Conference on
Quality Software.
[48] Xiaoyuan Xie, W. Eric Wong, Tsong Yueh Chen, and Baowen Xu. 2013. Meta-
morphic slice: An application in spectrum-based fault localization. Inf. Softw.
Technol. 55, 5 (2013), 866â€“879. https://doi.org/10.1016/j.infsof.2012.08.008
[49] Xiaoyuan Xie, Zhiyi Zhang, Tsong Yueh Chen, Yang Liu, Pak-Lok Poon, and
Baowen Xu. 2018. METTLE: a METamorphic testing approach to assess-
ing and validating unsupervised machine LEarning systems. arXiv preprint
arXiv:1807.10453 (2018).
[50] Boyang Yan, Brian Yecies, and Zhi Quan Zhou. 2019. Metamorphic relations for
data validation: a case study of translated text messages. In Proceedings of the
2019 International Workshop on Metamorphic Testing, MET@ICSE 2019, Montreal,
QC, Canada, May 26, 2019 , Xiaoyuan Xie, Pak-Lok Poon, and Laura L. Pullum
(Eds.). IEEE / ACM, 70â€“75. https://doi.org/10.1109/MET.2019.00018
[51] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for
Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, EMNLP 2018,
Brussels, Belgium, October 31 - November 4, 2018 , Ellen Riloff, David Chiang,
Julia Hockenmaier, and Junâ€™ichi Tsujii (Eds.). Association for Computational
Linguistics, 2369â€“2380. https://doi.org/10.18653/v1/d18-1259
[52] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A Large-
Scale Adversarial Dataset for Grounded Commonsense Inference. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2018, Brussels, Belgium, October 31 - November 4, 2018 , Ellen Riloff, David
Chiang, Julia Hockenmaier, and Junâ€™ichi Tsujii (Eds.). Association for Computa-
tional Linguistics, 93â€“104. https://doi.org/10.18653/v1/d18-1009
[53] Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid.
2018. DeepRoad: GAN-based metamorphic testing and input validation frame-
work for autonomous driving systems. In Proceedings of the 2018 International
Conference on Automated Software Engineering, ASE 2018, Montpellier, France,
September 3-7, 2018 , Marianne Huchard, Christian KÃ¤stner, and Gordon Fraser
(Eds.). ACM, 132â€“142. https://doi.org/10.1145/3238147.3238187
[54] Zhuosheng Zhang, Junjie Yang, and Hai Zhao. 2020. Retrospective Reader for
Machine Reading Comprehension. CoRR abs/2001.09694 (2020). arXiv:2001.09694
https://arxiv.org/abs/2001.09694
[55] Zhuosheng Zhang, Hai Zhao, and Rui Wang. 2020. Machine Reading Com-
prehension: The Role of Contextualized Language Models and Beyond. CoRR
abs/2005.06249 (2020). arXiv:2005.06249 https://arxiv.org/abs/2005.06249
[56] Zhi Quan Zhou and Liqun Sun. 2019. Metamorphic testing of driverless cars.
Commun. ACM 62, 3 (2019), 61â€“67. https://doi.org/10.1145/3241979
602