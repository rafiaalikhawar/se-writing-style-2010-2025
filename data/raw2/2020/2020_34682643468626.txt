Boosting Static AnalysisAccuracy with InstrumentedTest
Exe
cutions
TianyiChen
tianyichen@alumni.usc.edu
Universityof SouthernCalifornia
USAKihongHeo
kihong.heo@kaist.ac.kr
KAIST
KoreaMukundRaghothaman
raghotha@usc.edu
Universityof SouthernCalifornia
USA
ABSTRACT
The two broad approaches to discover properties of programsâ€”
staticanddynamicanalysesâ€”havecomplementarystrengths:static
techniquesperformexhaustiveexplorationandproveupperbounds
onprogrambehaviors,whilethedynamicanalysisoftestcasespro-
videsconcreteevidenceofthesebehaviorsandpromiselowfalse
alarm rates. In this paper, we present DynaBoost, a system which
uses information obtained from test executions to prioritize the
alarms of a static analyzer. We instrument the program to dynami-
callylookfordataflowbehaviorspredictedbythestaticanalyzer,
and use these results to bootstrap a probabilistic alarm ranking
system, where the user repeatedly inspects the alarm judged most
likelytobearealbug,andwherethesystemre-rankstheremaining
alarms in response to user feedback. The combined systemis able
to exploit information that cannot be easily provided by users, and
provides significant improvements in the human alarm inspection
burden:by35%comparedtothebaselinerankingsystem,andby
89%comparedto an unaidedprogrammer triaging alarm reports.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Automatedstaticanalysis ;
Dynamicanalysis ;â€¢Mathematicsofcomputing â†’Bayesian
networks ;â€¢Information systems â†’Probabilistic retrieval
models.
KEYWORDS
Staticanalysis,dynamicanalysis,beliefnetworks,Bayesianinfer-
ence,alarm ranking
ACMReference Format:
TianyiChen,KihongHeo,andMukundRaghothaman.2021.BoostingStatic
AnalysisAccuracywithInstrumentedTestExecutions.In Proceedingsofthe
29th ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering (ESEC/FSE â€™21), August 23â€“28,
2021,Athens,Greece. ACM,NewYork,NY,USA, 12pages.https://doi.org/
10.1145/3468264.3468626
1 INTRODUCTION
Bothstaticanddynamicanalysistechniqueshaveestablishedthem-
selves asimportant andcomplementaryapproaches to determine
ESEC/FSE â€™21, August  23â€“28, 2021, Athens, Greece
Â© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.3468626facts about programs. On the one hand, static analyses provide ex-
haustivevalidation,butemitmanyfalsewarnings,especiallywhen
analyzinglargepiecesofcode.Ontheotherhand,dynamicanalysis
tools have much higher precisionâ€”instrumentation frameworks
suchasValgrind[ 19],memorysafetycheckerssuchasAddressSani-
tizer[26]andMemorySanitizer[ 30],anddataracedetectorssuchas
ThreadSanitizer [ 27] and RoadRunner [ 7] have found hundreds of
bugsandsecurityvulnerabilitiesinlargeopensourceprojectsâ€”but
routinely miss bugs because of the low coverage induced by test
suites.Assuch,whilestaticanalysesprovideanupperboundonthe
spaceofprogram behaviors, dynamicanalysis providesconcrete
evidence for their existence, thus establishing a lower bound. This
naturally raises the question: Can we use empirical data gathered
fromwitnessingprogramexecutionstoimprovetheeffectiveaccuracy
ofstatic analysistools?
In a parallel thread,researchershave recently developed proba-
bilistic techniques to incorporate feedback from human users into
theoutputofstaticanalyzers[ 9,13,24].Theseapproachesbuildon
the observation that analyzers reuse portions of their reasoning to
derivemultiplealarms;asaresult,aninaccuratefunctionsummary,
dataflow fact, or may-happen-in-parallel assertion can lead to mul-
tiple false warnings. The idea then is to recover these reasoning
traces,andusethemtoconstructaBayesiannetworkthatcaptures
correlations between theground truthsofeach ofthealarms. The
userthen repeatedly inspects alarmsand indicateswhetheror not
they represent real bugs. In response, the analyzer computes the
conditional probabilities of the remaining alarms in light of this
newinformation,andreprioritizesthemindecreasingorderofcon-
fidence.Asaresult,thesesystemsareabletorapidlydeprioritize
false warnings, anduncover the real bugsinthe program.
Despite its experimental success, Bingo [ 9,24], which forms
our conceptualstartingpoint,suffers from a few important limita-
tions:first,apurelystaticinitializationoftheBayesiannetworkhas
limited information, and requires more human guidance. This is
evident in cases of false generalization : since the abstraction neces-
sarily over-approximates program behaviors, a probabilistic model
derivedfromtheabstractbehavioroftheprogramsometimescauses
negative feedback given by the user to erroneously propagate to
the true bugs, thereby suppressing them. Furthermore, users are
typicallyonlyabletoanswerquestionsaboutthecorrectnessofthe
final warnings, and not about intermediate assertions and dataflow
facts derived by the analysis, and finally, erroneous human feed-
back hasthe potentialto greatly degrade the quality of ranking in
subsequent iterations.
The central insight of our paper is that such interactive alarm
ranking systems can incorporate feedback not just from human
users, but also from diverse sources of knowledge, including by
dynamicinstrumentationoftestcases.Webeginwiththewarnings
1154This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece Tianyi Chen, Kihong Heo,andMukund Raghothaman
and dataflow facts emitted by the Sparrow static analyzer [ 20,21],
which checks Cprograms for a rangeof memory safety errors,and
use these to selectively instrument the program for analysis by the
DFSan dynamic data flow tracking framework [ 32]. We then run
the instrumentedprogram onitstest inputs,andcollectempirical
evidence for each theoretically predicted data flow fact. We use the
resultinginformationasa firstroundoffeedbacktothealarmprior-
itization system, thus greatly improving the quality of the ranking
even before human intervention. We present the architecture of
this system,whichwe callD ynaBoost,inFigure 1.
We emphasize that, in non-pathological cases, where test inputs
donotthemselvesexposeerroneous programbehaviors,thefeed-
backprovidedbyDFSanislimitedtointermediatedata flowfacts
rather than the final alarms raised by the static analysis. In this
situation,providingfeedback from dynamicanalysiscorresponds
toprovidingevidenceforinternalnodesintheBayesiannetwork:
the conditional independencies [ 22] thus created are responsible
for limiting the impact of false generalization. Conversely, because
of the incompleteness of static analysis, the presence of an abstract
dataflow path from a tainted source to a sensitive sink does not, by
itself, imply the possibility of tainted data causing program errors:
as a result, transferring feedback from DFSan to the probabilis-
tic ranking subsystem requires some care while engineering the
Bayesian network.
WeimplementedtheseideasusingBingo[ 9,24],Sparrow[ 20,21],
and DFSan [ 32] as building blocks. We evaluated our algorithms
on a suite of 13 Unix command line programs, ranging in size
from 9 KLOC to 112 KLOC, and which contain a set of known
historicalbugs. Onaverage,acrossall thesebenchmarks,Sparrow
emits566warningsperprogram.Usingoursystem,D ynaBoost,
aprogrammerisabletodiscoverallthesebugsaftertriagingjust
59.5 warnings, on average per program. Notably, this is an 89%
reduction compared to an unaided user, and a 35% improvement
over Bingo, which requires 92.2 rounds of feedback, on average.
Muchofthisrankingimprovementisduetoadramaticreductionin
the frequency and severity of false generalization events compared
to Bingo (see Figure 6): on average, DynaBoost has 79% fewer false
generalizationevents,eachofwhichisitselfonly11%ofthesizeof
the averageeventoccurringwithin Bingo.
Contributions. Tosummarize,wemakethefollowingcontribu-
tionsinthis paper:
(1)WedevelopaBayesianframework,D ynaBoost,tocombinein-
formation extracted from static and dynamic program analysis.
(2)We implement a system to perform targeted dynamic instru-
mentation basedontheresultsofan over-approximate static
analysis.
(3)WepresentanexperimentalevaluationacrossasuiteofUnix
utilities, and demonstrate an average drop of 35% in human
alarm annotation e ffort.
2 MOTIVATING EXAMPLE
In this section, we provide an overview of our approach by consid-
ering an example bugfrom theLinux command line program sort.
We will discuss how D ynaBoostcoordinates the static analyzer,
Sparrow, the dynamic analyzer, DFSan, and the Bayesian alarm
prioritization processto accelerate bugdiscovery.2.1 Postmortem ofaCoreutils Bug
InFigure 2, we show a snippet of code adapted from Version 7.2 of
theGNUcoreutilsprogram sortwhichcontainsabu fferoverrun
bug [4]. The program has a feature to merge a set of previously
sortedfiles, and this functionality is implemented in the merge
function shown in the figure. This function in turn calls another
functionnamed avoid_trashing_input toaccountforcaseswhere
one ofthe input files isreusedas an output file.
Theavoid_trashing_input function iterates over the input files,
andcheckswhethertheyarethesameastheoutput file.Ifso,thenit
attempts to merge the remaining files into a new temporary file on
line34.Itmightbeunabletocompletethismergeduetothelimited
availabilityof filehandlesfromtheoperatingsystem,soitpacks
thefilesarraybymovingtheremainingunmerged filesonline 38.
Unfortunately, instead of moving nfiles - (i + num_merged) files,
thethirdargumentrequeststhat num_merged filesbemoved,possibly
resultinginan out-of-bounds memory access.
To statically find this bug, we use the S parrowprogram an-
alyzer [21]. It uses a def-use graph computed using the sparse
analysisframework[ 20]todeterminealldatadependenciesleading
up to sensitive memory accesses, and subsequently performs an
intervalanalysistoprovememorysafety.Asonewouldexpect,it
isunabletoprovethesafetyofthecallto memmoveonline38,and
raisesan alarm at this line.
Notably,however,theunderlyingintervalanalysisisnon-rela-
tional, and since the filesarray is dynamically allocated, it is also
unable to show that the accesses to files[i] on lines36and37
are safe. It therefore raises two additional alarms at theselines. Of
course, these are both false warnings, and are a result of an overly
coarse abstraction. However, this abstraction was deliberately cho-
sen to allow the analysis to scale to large real-world programs, and
is an example of the accuracy-scalability trade-o ffs routinely made
by analysis designers. Overall, the sort program has 98 KLOC, and
Sparrowemits 715warnings, includingthe bugonline 38.
2.2 InteractiveAlarm Prioritization
We will now explain the interactive alarm prioritization process
usedbyB ingo[9,24]thatleveragesaprobabilisticmodeltogeneral-
izefromuserfeedbacktosuppresslikelyfalsealarmsandprioritize
likely true bugs.
Reconstructing the derivation graph. Thefirst step is to recon-
structthe reasoningtrace thatcausesS parrowtoreporteachalarm.
This reasoning processâ€”interval analysis applied to the def-use
graphâ€”can be approximately described by the derivation rules
shown inFigure 3.Startingfromvariablede finitions andone-step
dataflow edges in the program, indicated by tuples of the form
VarDefn(ğ‘)andDUEdge(ğ‘,ğ‘)respectively, the analyzer computes
dataflowpathsoftheform DUPath(ğ‘,ğ‘).Ifthefinalnodeğ‘ineach
derivedDUPath(ğ‘,ğ‘)corresponds to an array access, the analyzer
performs additional reasoning to prove the safety of the operation
at program point ğ‘. Note that we have not modeled the details
ofthissub-analysisâ€”whichemploysanintervalabstractionâ€”and
insteadprovideinputtuplesoftheform Overflow(ğ‘)asstubsfor
unmodeledpartsofthereasoningprocess.Whentheanalyzer finds
such a data flow path leading up to a potentially unsafe memory
1155Boosting Static Analysis Accuracy withInstrumentedTestExecutions ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Test suite
Static Analyzer
Instrumentation
Harness
SDTransferSource
code
#include  <stdlib.
int main() {
  int *x = malloc
  int *y = x;
  *x = 3;
  *y = 2;
  assert( *x == 2)
}
Bayesian
network
DSTransfer
Marginal
Inference
User
âœ” â‹¯
âœ” â‹¯
âœ—Â â‹¯
Empirical
observations
âœ” â‹¯
âœ” â‹¯
âœ—Â â‹¯Feedback
1.Â â‹¯
2.Â â‹¯
3.Â â‹¯
Ranked
alarms
DynaBoost
Figure 1: Architecture of the D ynaBoostframework. Our main technical contributions include the construction of the
Bayesian network and the boxes marked SDT ransfer and DST ransfer , corresponding to the transfer of instrumentation
targets from the static analyzer to the instrumentation harness, and the transfer of empirical evidence from the dynamic
analysis to theprobabilisticmodelrespectively.
1static unsigned int nmerge = 16;
2structsortfile {
3char const *name;
4pid_t pid;
5};
6voidmerge(structsortfile *files, size_tnfiles, char const *output_file)
7{
8size_tin, out;
9for(out = in = 0; nmerge <= nfiles - in; out++) {
10 dfsan_set_label("src 9", &out, sizeof(size_t));
11 structsortfile temp;
12 create_temp(&temp);
13 size_tnum_merged = mergefiles(&files[in], nmerge, &temp);
14 in += num_merged;
15 files[out].name = temp.name;
16 files[out].pid = temp.pid;
17}
18memmove(&files[out], &files[in], (nfiles - in) * sizeof(*files));
19nfiles -= in - out;
20nfiles = avoid_trashing_input(files, nfiles, output_file);
21Â·Â·Â·
22}23size_tavoid_trashing_input( structsortfile *files, size_tnfiles, char const *outfile)
24{
25print(dfsan_get_label(nfiles));
26for(size_ti = 0; i < nfiles; i++) {
27 bool same = Â·Â·Â·;
28 if(same) {
29 size_tnum_merged = 0;
30 while(i + num_merged < nfiles) {
31 print(dfsan_get_label(i));
32 structsortfile temp;
33 create_temp(&temp);
34 num_merged += mergefiles(&files[i], nfiles - i, &temp);
35 print(dfsan_get_label(num_merged));
36 files[i].name = temp.name;
37 files[i].pid = temp.pid;
38 memmove(&files[i + 1], &files[i + num_merged], num_merged * sizeof(*files));
39 nfiles -= num_merged - 1;
40 }
41 }
42}
43returnnfiles;
44}
Figure 2: Code fragment adapted from the sort program. The lines highlighted in red correspond to the alarms raised by the
Sparrow static analyzer,while thelineshighlighted ingreen correspond to theinstrumentation added by D ynaBoost.
access, it reports an alarm at the appropriate point, as indicated by
the derivationrule ğ‘Ÿ3.
In the case of our example, the assignment to the variable out
online9mayinfluencethe valueof nfilesat line19andthrough
the call to avoid_trashing_input , affect the values of variables i
andnum_merged at lines30and34respectively. The program subse-
quently accesses the i-th and(i + num_merged) -th elements of the
filesarrayatthelocationsofthealarmsraisedbyS parrow.We
mayvisualizethisreasoningtraceasthederivationgraphshown
inFigure 4.
A probabilistic model of alarms. Observe now that if the user
triages the alarm at line 36and indicates that it is nota real bug,
then we may conclude that line 37is also a false alarm. The priori-
tizationalgorithminferssuchcorrelationsbetweenalarmsusing
the derivationgraph inFigure 4.
As an example, consider the tuple DUPath(9,38), which indi-
catesthatdatamay flowfromasourceatlocation 9toavariableInput relations
VarDefn(ğ‘): Variable de finition at programpoint ğ‘
Overflow(ğ‘):Possiblebu fferoverflowat point ğ‘
DUEdge(ğ‘,ğ‘):Direct data flowedgebetweenprogrampoints ğ‘andğ‘
Outputrelations
DUPath(ğ‘,ğ‘):(Transitive)Data flowpathfrom programpoint ğ‘toğ‘
Alarm(ğ‘): Alarmindicating possible bu fferoverflowatğ‘
Derivationrules
ğ‘Ÿ1: DUPath (ğ‘,ğ‘):âˆ’VarDefn(ğ‘),DUEdge(ğ‘,ğ‘)
ğ‘Ÿ2: DUPath (ğ‘,ğ‘):âˆ’DUPath(ğ‘,ğ‘),DUEdge(ğ‘,ğ‘)
ğ‘Ÿ3: Alarm (ğ‘):âˆ’DUPath(ğ‘,ğ‘),Overflow(ğ‘)
Figure 3: Modeling the program analyzer using derivation
rules, represented here as aDatalogprogram.
at location 38. Because of inaccuracies in the construction of the
dataflow graph, there is a small, but non-zero probability that a
1156ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece Tianyi Chen, Kihong Heo,andMukund Raghothaman
DUPath(9,25)DUEdge(25,30)
ğ‘Ÿ2(9,25,30)
DUPath(9,30)
DUEdge(30,37) DUEdge(30,36) DUEdge(30,38)
ğ‘Ÿ2(9,30,36)
DUPath(9,36)ğ‘Ÿ2(9,30,37)
DUPath(9,37)ğ‘Ÿ2(9,30,38)
DUPath(9,38)
Overflow(36)
ğ‘Ÿ3(9,36)
Alarm(36)Overflow(37)
ğ‘Ÿ3(9,37)
Alarm(37)Overflow(38)
ğ‘Ÿ3(9,38)
Alarm(38)
Figure 4: Derivation graph for the alarms in Figure 2. Each
clausenode(e.g., ğ‘Ÿ2(9,30,38))indicatesthevariablevaluation
with which thecorresponding rule inFigure 3wasfired.
givenderivedtupleofthisformdoesnotrepresentaviabledata flow
pathinthe program.
To model such inaccuracies, the alarm prioritization process in-
terpretsthederivationgraphasaBayesiannetwork,andassociates
eachofits clauses withaprobability ofâ€œ misfiringâ€,
Pr(Â¬ğ‘¡|ğ‘¡1âˆ§ğ‘¡2âˆ§Â·Â·Â·âˆ§ğ‘¡ğ‘˜),
whereğ‘¡is the tuple produced by instantiating a rule ğ‘Ÿwith the ap-
propriateinputtuples, ğ‘¡1,ğ‘¡2,...,ğ‘¡ğ‘˜.Fromthis,onecancomputethe
probabilityofeachalarm, Alarm(ğ‘),beingarealbug, Pr(Alarm(ğ‘)),
and use these probabilities to order alarms for triaging, with high-
confidencealarmsinspectedbeforealarmswithlowcon fidence.Fur-
thermore,thecomputationofmarginalprobabilitiesâ€” Pr(Alarm(ğ‘1) |
Alarm(ğ‘2)âˆ§Â¬Alarm(ğ‘3)), for exampleâ€”provide a natural mecha-
nism by which to generalize from user feedback and suppress or
prioritizesimilar warnings from the staticanalyzer.
For simple networks, and for the sake of exposition, we may
computethese valuesbyhand,aswe demonstrateinAppendix A.
If we assume that the prior probability Pr(DUPath(9,25))=0.9,
and that there is a 1% i.i.d probability of each rule mis firing, then it
followsthat the prior probability ofAlarm (37)isgiven by:
Pr(Alarm(37))=0.873. (1)
Generalizingfromuserfeedback. Iftheuserindicatesthat Alarm(36)
isafalsewarning,wewouldbeinterestedinthevaluesof Pr(Alarm(37) |
Â¬Alarm(36))andPr(Alarm(38) | Â¬Alarm(36)):
Pr(Alarm(37) | Â¬Alarm(36))=0.137. (2)
Observethedramaticdropin Pr(Alarm(37) | Â¬Alarm(36))com-
pared tothe valueof Pr(Alarm(37)),which occurs becauseofthe
sharedderivationgraphbetween Alarm(36)andAlarm(37).Asa
result, the user feedback causes us to suppress the second alarm,
andpermittheaccelerateddiscoveryofbugs inotherpartsofthe
codebase. Recall that S parrowreports 715 alarms when it ana-
lyzes GNU sort; the user-in-the-loop interaction process we just
discussed causes the bug to be discovered after inspecting only
176 alarms. We graphically depict this interaction loop in the right-
mostportionofthesystemdiagraminFigure 1,andwillreviewthe
construction ofthe probabilisticmodelinSection 3.Despitethesigni ficantempiricalimprovementprovidedbyB ingo,
itsuffersfromseveralimportantlimitations.Inprinciple,thecon-
ditional probabilities, Pr(Alarm(ğ‘) |ğ‘¡), may be computed with
respect to anytupleğ‘¡produced by the analysis, and not just those
correspondingtoalarms, ğ‘¡=Alarm(ğ‘â€²).Inpractice,however,be-
cause of the highly technical nature of the analysis, users are only
ableto providelimitedforms offeedback,andeventhen, areprone
to making mistakes. Second, an investigation of the interaction
processrevealsnumerousinstancesof falsegeneralization ,where
the rank of the real bug drops during a single iteration. See, for
example, the â€œspikesâ€ in the plots of Figure 6, and the aggregate
statistics in Table 3. Finally, because the triage process is primarily
user-driven,itsometimesresultsinalengthyinteractionprocessin
which the user has to inspect a large number of false warnings be-
forediscoveringrealbugsintheprogram.Theselimitationsprovide
the background for our present paper.
2.3 DynamicInstrumentationas an
Information Source
Ourcentralinsight. Thecentralinsightofourpaperisthatthe
feedbackprovidedtothealarmprioritizationalgorithmneednot
onlycomefromhumanusers,butcanbedrawnmorebroadlyfrom
any source of information about the program. In particular, we
demonstrate the possibility of using dynamic information obtained
from test executionsas an information source.
The dynamic data flow sanitizer DFSan. DFSan [32] provides two
functionstoinject taintlabels andinspect thetaintvalues ofvari-
ables at runtime: ( a)dfsan_set_label(dfsan_label label, void
*addr,size_tsize), which associates the sequence of memory
locations addr,addr + 1,addr + 2, ...,addr + (size - 1) with the
taint value label, and (b)dfsan_get_label( longdata), which re-
turns the taint label associated with the value data. The annotated
program isthen instrumented byDFSan duringcompilationso as
to track theseinjectedtaint valuesas the program executes.
Dynamicinstrumentation. Foreachdata flowpath,DUPath(ğ‘,ğ‘),
reported by S parrow, we use DFSan to monitor program execu-
tionsforconcreteevidenceofa flowbetweenthe (ğ‘,ğ‘)source-sink
pair.Wehighlightasimpli fiedversionoftheannotationsappliedto
the sort program in green in Figure 2. As an example, consider the
tupleDUPath(9,25). We annotate the variable being assigned at
thesource, out,withadistinguishedtaintvalueâ€”here "src9"â€”with
thecallto dfsan_set_label online10.Wethenretrievethelabels
of all predicted downstream sinks with calls to dfsan_get_label
on lines25,31, and35, and check whether the source taint prop-
agates to each of the sink locations. We run this instrumented
program on all tests provided with GNU sort Version 7.2, look-
ing for experimental con firmation of the predicted sourceâ€“sink
flows. This in turn enables us to provide early feedback to the
Bayesian rankingprocess,so thatitnowranksalarms according
toPr(Alarm(ğ‘) |DUPath(ğ‘,ğ‘)),ratherthanmerelybytheirprior
probabilitiesPr (Alarm(ğ‘)).
To motivatethe value ofthis process, we will now discusshow
Bingocausesfalsegeneralizationwhileanalyzing sort.First,con-
siderthesymmetrybetween Alarm(36),Alarm(37),andAlarm(38)
in Figure 4. From this with our previous calculation of the prior
1157Boosting Static Analysis Accuracy withInstrumentedTestExecutions ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
and posterior probabilities in Equations 1and2, we conclude that:
Pr(Alarm(38))=0.873,Pr(Alarm(38) | Â¬Alarm(36))=0.137.
Thisdropinposteriorprobabilitycausesthealarmtodropinthe
ranking,comparedtootherwarningsintheprogram,and,sincethis
representsarealbug,correspondstoafalsegeneralizationevent.
Overall, over the course of the interaction process, Alarm(38)gets
deprioritizedtwice,correspondingtotheuserinspectingeachof
the neighboring alarms, Alarm(36)andAlarm(37). Visually, these
correspondto the twoprominentspikesinFigure 6f.
WewillnowdescribehowD ynaBoostmitigatesthisproblem
of false generalization. Observe that even though the functional-
ityimplementedin avoid_trashing_input isnotexercisedbyany
testinput,thefunctionisalwayscalledfrom merge,andthe stan-
dard suite of test cases does attempt to merge files. As a result,
DFSanobservesexperimentalevidencefor DUPath(9,25).Theal-
gorithmthenranksalarmsintermsof Pr(Alarm(ğ‘) |ğ‘‘âˆ§ğ‘’),where
ğ‘‘=DUPath(9,25)is the dynamic feedback, and ğ‘’represents the
feedbackprovidedbytheuser.Inthiscase,theoriginalprobabilities
are given by:
Pr(Alarm(36) |ğ‘‘)=Pr(Alarm(37) |ğ‘‘)
=Pr(Alarm(38) |ğ‘‘)=0.993=0.970,(3)
where,asbefore,theexpression0 .993arisesbecauseofthethree
rule applications between the evidence and the query nodes. Af-
ter thefirst round of user feedback, the posterior probability of
Alarm(38)isgivenby Pr(Alarm(38) |ğ‘‘âˆ§ğ‘’),whereğ‘’=Â¬Alarm(36).
We sketch this calculation in Appendix A, from which we can con-
cludethat: Pr(Alarm(38) |ğ‘‘âˆ§Â¬Alarm(36))=0.650.Observethat
userfeedbackonthefalsealarm, Alarm(36),causesamuchsmaller
dropincon fidenceinAlarm(38),andasigni ficantlysmallerfalse
generalization event.
Experimentalresults. Assuch,D ynaBoostaddressestheprevi-
ouslyoutlinedlimitationsinBingo:First,dynamicfeedback,pro-
vided to tuples of the form DUPath(ğ‘,ğ‘), represents program be-
haviors which users would find difficult to certify. Second, this
additional information constrains the ways in which the marginal
inference algorithm may propagate user feedbackâ€”for example, by
activatingconditionalindependenciesintheBayesiannetworkâ€”
and this reduces the incidence of false generalization. For example,
compare the frequency and magnitude of spikes of D ynaBoost
and Bingoin the plots of Figure 6: there are 79% fewer spikes, and
each spike is only 11% of the original size. As a consequence, dy-
namicfeedbackbecomesavaluableauxiliarysourceofinformation,
and dramatically reduces the alarm inspection burden, by approxi-
mately 35% compared to B ingo, and approximately 89% compared
to an unaideduser(See Table 2).
3 ANOVERVIEW OFBAYESIANALARM
PRIORITIZATION
Wewill nowpresent ahigh-level descriptionoftheBayesianalarm
prioritizationframework.Itconceptuallyworksinthreephases:by
extractingthederivationgraphfromthestaticanalyzer,converting
thisderivationgraphintoaBayesiannetwork,and finallyengaging
inaninteractionloopwiththeuser,whilerepeatedlyperforming
marginalinference to rank alarms.First,wemodelthestaticanalysisasaDatalogprogram,suchas
that shown in Figure 3. Most brie fly, a Datalog program consists
of a set of universally quanti fied Horn clauses, which we call rules.
We may read the rules from right-to-left, while treating the â€œ :âˆ’â€
operatorinthemiddlereadastheimplicationoperator,â€œ â‡=â€.For
example, the rule ğ‘Ÿ2may be read as, â€œFor all program points ğ‘,ğ‘,ğ‘,
if there is a data flow fromğ‘toğ‘,DUPath(ğ‘,ğ‘)and a one-step flow
fromğ‘toğ‘,DUEdge(ğ‘,ğ‘),then,transitively,thereisalsoadata flow
fromğ‘toğ‘,DUPath(ğ‘,ğ‘).â€Wewillrefertoeachinputhypothesis
andoutputconclusion, of the form ğ‘…(ğ‘1,ğ‘2,...,ğ‘ğ‘›),as atuple.
Wethenmodifythestaticanalyzertoprovideexplanationsfor
eachofitsalarms.Theseexplanationstaketheformofaderivation
graph, such as that shown in Figure 4. The derivation graph ğº
is a (possibly cyclic) directed graph consisting of two types of
nodes, corresponding to the tuples and grounded clauses of the
leastfixpointoftheDatalogprogram. Eachclausenoderefersto
a specific instantiation of a rule, which takes several tuples as
hypotheses and produces a tuple as conclusion. In our diagrams,
we willindicatethetuplesby boxedvertices,andleave theclause
nodes unboxed.
We remark that the derivation graph is a best-e ffort post hoc
explanation: S parrowis itself written in unrestricted OCaml, and
thereareportionsoftheanalyzerâ€”suchastheintervalanalysisâ€”
whichareleftunmodeled.Itshouldbepossibletoextractsimilar
derivationgraphsfromotherstaticanalyzers.Toobtainthisderiva-
tiongraph,wereusedthemodi ficationstoS parrowwhichwere
originallyemployedinDrake,andwhichconsistsofapproximately
500linesofchanges to a15 KLOC codebase[ 9].
Next, we convert the derivation graph into a Bayesian network.
We associate each node of the graph with a conditional probability
distribution, which indicates the probability of the node being true
for eachcombination oftruthvaluesof its hypothesisnodes.
Consider a grounded clause ğ‘”=ğ‘Ÿ(ğ‘1,ğ‘2,...,ğ‘ğ‘˜)which applies
ruleğ‘Ÿto produce the conclusion ğ‘¡from hypotheses ğ‘¡1,ğ‘¡2, ...,ğ‘¡ğ‘›:
ğ‘¡â‡=ğ‘Ÿğ‘¡1âˆ§ğ‘¡2âˆ§Â·Â·Â·âˆ§ğ‘¡ğ‘›. The clause nodes may be thought of as
conjunctions, which fire only when all of its hypothesis nodes are
derivable.Tomodeltheapproximationsofthestaticanalysis,we
allow for the possibility of clause nodes mis firing, with a small
rule-dependent mis firing probability,1 âˆ’ğ‘ğ‘Ÿ:
Pr(ğ‘Ÿ(ğ‘1,ğ‘2,...,ğ‘ğ‘˜) |ğ‘¡1,...,ğ‘¡ğ‘›)=(
1âˆ’ğ‘ğ‘Ÿifğ‘¡1âˆ§Â·Â·Â·âˆ§ğ‘¡ğ‘›,and
0 otherwise .
(4)
So thatthe probabilitiesall addupto1, wenaturallyhave Pr(Â¬ğ‘”|
ğ‘¡1,...,ğ‘¡ğ‘›)=1âˆ’Pr(ğ‘”|ğ‘¡1,...,ğ‘¡ğ‘›). While these firing probabili-
tiesğ‘ğ‘Ÿmay be determined using techniques such as expectation
maximization,we followB ingoanduniformly setthemto 0 .99.
Similarly, the tuple nodes of a derivation graph may be thought
ofasdisjunctions,whicharederivableonlywhenatleastoneofits
contributingclausesisableto fire.Consideratuple ğ‘¡whichisthe
result of several alternative clauses: ğ‘”1,ğ‘”2, ...,ğ‘”ğ‘˜. We model ğ‘¡as a
deterministic disjunction:
Pr(ğ‘¡|ğ‘”1,...,ğ‘”ğ‘˜)=(
1 ifğ‘”1âˆ¨Â·Â·Â·âˆ¨ğ‘”ğ‘›,and
0 otherwise .(5)
1158ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece Tianyi Chen, Kihong Heo,andMukund Raghothaman
Asbefore,toindicateexhaustiveness, Pr(Â¬ğ‘¡|ğ‘”1,...,ğ‘”ğ‘˜)=1âˆ’Pr(ğ‘¡|
ğ‘”1,...,ğ‘”ğ‘˜).Forallinputtuples ğ‘¡in,wedefinethepriorprobability
as 1:Pr(ğ‘¡in)=1.
Onenotablechallengeintheconstructionofthisprobabilistic
modelisthatBayesiannetworksarerequired,byde finition,tobe
acyclic, while the derivation graph may potentially have cycles. To
address this, B ingoapplies a cycle elimination algorithm which
dropsclausessoastoobtainanacyclicgraphwhilestillpreserving
thederivationsofallalarms.Thisresultsinanacyclicderivation
graph, whichisthen usedto buildthe Bayesian network.
Given the Bayesiannetwork, we use ano ff-the-shelf solver,lib-
DAI, to perform marginal inference and rank alarms for user in-
spection [ 15].
4 TARGETEDINSTRUMENTATION AND
FEEDBACK TRANSFER
In this section, we explain the operation of the D ynaBoostsys-
tem.Weformallypresentthetop-levelprocedureinAlgorithm 1.
Themaincontributionsofthispaperareinthedynamicanalysis
performed in Steps 3â€“6. We describe the core SDTransfer and
DSTransfer procedures inAlgorithms 2and3respectively.
Algorithm1 DynaBoost(A,ğ‘ƒ,T),whereAisthestaticanalysis,
ğ‘ƒisthe program, Tisthe setofavailable test cases.
(1)Letğ‘†=A(ğ‘ƒ).Staticallyanalyzetheprogram.Theresult ğ‘†
consistsofthesetofalarms,theintermediateconclusions,
andthe derivationgraph connecting them.
(2) Constructthe Bayesian network ğµ=MakeBNet(ğ‘ƒ,ğ‘†).
(3)Letğ‘ƒâ€²=SDTransfer(ğ‘ƒ,ğ‘†).Instrumenttheprogramusing
the staticanalysisoutput.
(4)Compute ğ·=ğ‘ƒâ€²(T). Runthe instrumented program on the
test inputs.
(5)Letğ¹ğ·ğ‘¦ğ‘›=DSTransfer(ğ·,ğ‘†).Determinewhichdata flow
factscan be dynamically observed.
(6)Initializethefeedback, ğ¹Bğ¹ğ·ğ‘¦ğ‘›.Assertğ¹ğ·ğ‘¦ğ‘›âŠ†Tuples(ğ‘†).
(7) Initialize the setofunlabelledalarms, ğ´ğ‘¢BAlarms(ğ‘†).
(8) While ğ´ğ‘¢â‰ âˆ…:
(i)Presentthehighestprobabilityunlabelledalarmforuser
inspection:
ğ‘ğ‘¡=argmax
ğ‘âˆˆğ´ğ‘¢Pr(ğ‘|ğ¹).
(ii)Iftheusermarks ğ‘ğ‘¡astrue,update ğ¹Bğ¹âˆ§ğ‘ğ‘¡.Otherwise
updateğ¹Bğ¹âˆ§Â¬ğ‘ğ‘¡.
(iii) Update ğ´ğ‘¢Bğ´ğ‘¢\{ğ‘ğ‘¡}.
4.1 PerformingTargetedInstrumentation
Wewillnowdescribe SDTransfer,theprocessofinstrumenting
the program based on results from the static analyzer. We present
the overallalgorithm inAlgorithm 2.
As discussed in Section 2, in addition to the runtime instru-
mentation appliedto the LLVM IR, DFSan provides two functions:
dfsan_set_label to insert taint values into memory locations, and
dfsan_get_label toretrievethetaintvaluesassociatedwithavalue.Foreach data flow tuple DUPath(ğ‘,ğ‘)producedby the static ana-
lyzer,ourgoalistoinserttheappropriatetaintvaluesintovariables
beingassignedatprogramlocation ğ‘usingdfsan_set_label ,and
toretrievethetaintvaluesfromvariablesbeingusedatprogram
locationğ‘usingdfsan_get_label .
The main challenge in this process is in translating program
locations from S parrowâ€™s representation to points in the program
sourcecode.SinceS parrowworkswithanSSA-formofthepro-
gram, some program locations such as ğœ™-nodes cannot be mapped
back to locations in the original program. In such cases, we do not
instrumenttheresulting DUPath(ğ‘,ğ‘)tuple.Furthermore,opera-
tions may have side-e ffects (such as *p++ = 1), syntactic constructs
maybearbitrarilynested(such as x = y->b + c ), andasingleline
ofcodemayhavemultipleassignments(thiscommonlyarisesin
for-loops) sothat we areonlyable toperform a best-e ffortinstru-
mentation of the source code, and omit tuples which cannot be
instrumented. Overall,in ourexperiments inSection 5,we have a
58%successrateininstrumenting43465 target locations.
Algorithm2 SDTransfer(ğ‘ƒ,ğ‘†).Givenaprogram ğ‘ƒandstatically
determined data flow factsğ‘†,producesa program ğ‘ƒâ€²withruntime
instrumentationenabled.
(1)Foreachpredicteddata flowtupleDUPath(ğ‘,ğ‘)âˆˆğ‘†,ifğ‘and
ğ‘can both be mapped to source locations, add the instru-
mentation highlightedingreen below:
x = ...; // Program point ğ‘
+ dfsan_set_label("src-a", &x, sizeof(x));
...
+ print(dfsan_get_label(y));
read(y); // Program point ğ‘
(2) Return the instrumentedprogram ğ‘ƒâ€².
4.2 Transferring Runtime Outputto Static
Feedback
We run the instrumented program ğ‘ƒâ€²on the provided test cases,
T,andcollectthelistofalldata flowpathswhichareempirically
observed.Wethenperformlightweightfeedbackenhancementto
recover information about uninstrumented data flows to or from
emptynodes,andusethefrequencyofobservationofeachdata flow
path toprovide weightedfeedback tothemarginalinferencealgo-
rithm.We outlinethis processinAlgorithm 3.
Byborrowingterminologyfromgraphtheory,wetermatuple
DUPath(ğ‘,ğ‘)asan(ğ‘,ğ‘)-bridgeif(a)bothDUPath(ğ‘,ğ‘),DUPath(ğ‘,
ğ‘)âˆˆğ‘†,the predictions ofthe staticanalyzer,and( b) the clause
DUPath(ğ‘,ğ‘)â‡=ğ‘Ÿ2DUPath(ğ‘,ğ‘)âˆ§DUEdge(ğ‘,ğ‘)
is the only way to derive DUPath(ğ‘,ğ‘), where the rule ğ‘Ÿ2is drawn
from Figure 3. Bridges provide indirect evidence of data flows to
empty nodes ğ‘, which cannot themselves be directly instrumented.
Inthesecases,wecanusedynamicobservationsof DUPath(ğ‘,ğ‘)
toinfer truthof thebridge DUPath(ğ‘,ğ‘).In Step1ofAlgorithm 3,
we repeatedlyperform this feedbackenhancement.
Additionally, to account for the frequency of observations of
individual data flows, we count the number of test cases # (ğ‘,ğ‘)
1159Boosting Static Analysis Accuracy withInstrumentedTestExecutions ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
whichwitnesseachdata flowDUPath(ğ‘,ğ‘),andcompareittothe
total number of test inputs, ğ‘›. This allows us to prioritize feedback
to commonly observed data flow paths, and only provide weak
experimental feedback for less frequently observed data flow paths.
Algorithm 3 DSTransfer(ğ·,ğ‘†). Given the results of the static
analysisğ‘†, and the list of empirically observed data flow facts ğ·,
computes the dynamicfeedback ğ¹ğ·ğ‘¦ğ‘›.
(1)Perform feedback enhancement. For each tuple
DUPath(ğ‘,ğ‘)âˆˆğ·, if there is an (ğ‘,ğ‘)-bridge,
DUPath(ğ‘,ğ‘)âˆ‰ğ·,then update:
ğ·Bğ·âˆª{DUPath(ğ‘,ğ‘)}.
Repeatuntil fixpoint.
(2) Constructthe dynamicfeedback ğ¹ğ·ğ‘¦ğ‘›.
(i) Initialize ğ¹ğ·ğ‘¦ğ‘›Bâˆ….
(ii)Foreachtuple DUPath(ğ‘,ğ‘)âˆˆğ·,let#(ğ‘,ğ‘)bethenumber
oftestinputswhichtrigger DUPath(ğ‘,ğ‘),andletğ‘›bethe
total number of test inputs. Provide feedback to the (ğ‘,ğ‘)
dataflowbyupdating:
ğ¹ğ·ğ‘¦ğ‘›Bğ¹ğ·ğ‘¦ğ‘›âˆª{DUPath(ğ‘,ğ‘)â†¦â†’#(ğ‘,ğ‘)/ğ‘›}.(6)
(3) Return ğ¹ğ·ğ‘¦ğ‘›.
4.3 DifferentiatingUn filtered Data flows
In the last step, we modify the derivation graph to di fferentiate
between filtered and un filtered data flows. Because DFSan only
provides an under-approximation of feasible behaviors, it does not
provide information about the operations applied to the data along
the(ğ‘,ğ‘)dataflowpath.Asaresult,itisinsu fficienttoconcludethat
this path could form the basis of a bu ffer overflow. To describe the
limitedinformationcomingfromDFSan,weintroduceanewoutput
relationTDUPath ,whichindicatesthepossibilityofanun filtered
dataflow from source ğ‘to sinkğ‘. We use the base DUPath(ğ‘,ğ‘)
tuplestoderivetuplesoftheform TDUPath (ğ‘,ğ‘),butonlyapply
the feedback in ğ¹ğ·ğ‘¦ğ‘›to the original DUPathtuples. We describe
these modi fied Datalog rules in Figure 5. As we demonstrate in
Table2, modeling these correlations is crucial to the experimental
effectivenessof D ynaBoost,asitreducestheaveragenumberof
iterations from 156(forB ingoğ‘ğ‘™ğ‘™) to 60 (forD ynaBoostğ‘ğ‘™ğ‘™).
5 EXPERIMENTALEVALUATION
Toevaluatetheexperimentale ffectivenessof D ynaBoost,wefocus
onthe following researchquestions:
RQ1.Does DynaBoosteffectively prioritize the real bugs, and
howdoes itcompare to B ingo?
RQ2.Does DynaBoostreduce the frequency and magnitude of
false generalization events?
RQ3.Howdoesthenumberoftestcasesa ffecttherankingquality?
RQ4.Howisthemodi ficationforthenetworkstructureimportant
for utilizing the runtimefeedback?
We begin this section by describing our experimental setting,
andwe focusoneachofthe above questionsinSections 5.2â€“5.4.1
1Wewill make ourbenchmarks and implementation public uponpaper acceptance.Input relations : VarDefn (ğ‘), Overflow(ğ‘), DUEdge (ğ‘,ğ‘)
Outputrelations : DUPath(ğ‘,ğ‘), Alarm(ğ‘,ğ‘)
TDUPath (ğ‘,ğ‘): (Unfiltered)Data flowpathfromprogrampoint ğ‘toğ‘
Derivation rules
ğ‘Ÿ1: DUPath (ğ‘,ğ‘):âˆ’VarDefn(ğ‘),DUEdge(ğ‘,ğ‘)
ğ‘Ÿ2: DUPath (ğ‘,ğ‘):âˆ’DUPath(ğ‘,ğ‘),DUEdge(ğ‘,ğ‘)
ğ‘Ÿâ€²
3: Alarm (ğ‘):âˆ’TDUPath (ğ‘,ğ‘),Overflow(ğ‘)
ğ‘Ÿ4: TDUPath (ğ‘,ğ‘):âˆ’DUPath(ğ‘,ğ‘),VarDefn(ğ‘),DUEdge(ğ‘,ğ‘)
ğ‘Ÿ5: TDUPath (ğ‘,ğ‘):âˆ’DUPath(ğ‘,ğ‘),TDUPath (ğ‘,ğ‘),DUEdge(ğ‘,ğ‘)
Figure 5: Modi fied derivation rules to capture un filtered
dataflows. We reuse rules ğ‘Ÿ1andğ‘Ÿ2from Figure 3, and re-
placerule ğ‘Ÿ3withğ‘Ÿâ€²
3.
Table1:Benchmarkcharacteristics.Sizeand#Testreportthe
linesofcodeandthenumberoftestcases.
Program Version Analysis Size(KLOC) Tests
bc 1.06 Interval 14 18
cflow 1.5 Interval 40 33
grep 2.19 Interval 68 1646
gzip 1.2.4a Interval 9 49
libtasn1 4.3 Interval 30 17
patch 2.7.1 Interval 51 189
readelf 2.24 Interval 65 2601
sed 4.3 Interval 83 522
sort 7.2 Interval 98 789
tar 1.28 Interval 112 699
optipng 0.5.3 Taint 61 176
latex2rtf 2.1.1 Taint 27 130
shntool 3.0.5 Taint 13 7
5.1 ExperimentalSetup
Choice ofbenchmarks. We ran D ynaBooston a suiteofwidely
usedCprogramsshownin Table1.Allbenchmarksarefrompre-
viousworkusingS parrow[8,9]andrecentCVEreports.Weex-
cluded benchmarks with less than 5 KLOC because the limited
numberofalarmsraisedbyS parrowdoesnotimposeasigni ficant
alarminspectionburden.Weadditionallyexcluded wgetandurjtag
because ofcompatibilityissueseitherwithDFSan orwithClang.
Weusedthetestinputsthatcomewiththeprogram,ifavailable,
to collect dynamic information. Three benchmark programs did
nothaveadeveloper-providedtestsuite: gzip,shntool,andoptipng.
Forthese programs,wecollected sampleaudioandimage files[5,
33](shntool,optipng),andcompressed filesfromtheCanterbury
corpus[23](gzip).
Dynamic instrumentation and ranking process. We use Data flow-
Sanitizer (DFSan) [ 32] to collect the runtime data flow information.
For each sourceâ€“sink pair, DUPath(ğ‘,ğ‘)reported by S parrow, we
determinethevariablesassignedatprogramlocation ğ‘andinjecta
runtimetaintlabelusingthe dfsan_set_label() function,andwe
retrievethetaintlabelsofallvariablesaccessedatprogramlocation
ğ‘using the dfsan_get_label() function. We break ties between
1160ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece Tianyi Chen, Kihong Heo,andMukund Raghothaman
identically ranked alarms by using their con fidence values from
the purelystaticrankingprocessinB ingo.
Baselines. We instantiate D ynaBoostwith two di fferent set-
tings:DynaBoostğ‘ğ‘™ğ‘™thatisbasedontheaugmentednetworkstruc-
ture of Section 4.3and initialized with dynamic feedback, and
DynaBoostğ‘§ğ‘’ğ‘Ÿğ‘œwhich also uses the new network structure, but
withholdsdynamicfeedback.Wesimilarlyinstantiatetwobaselines:
Bingoğ‘ğ‘™ğ‘™whichusestheoriginalnetworkstructure[ 24]butalso
includes dynamic feedback, and Bingoğ‘§ğ‘’ğ‘Ÿğ‘œwhich neither uses the
newnetwork structure noruses dynamicfeedback[ 24].
Runtime performance of D ynaBoost.Sparrowrequires an aver-
ageof206secondstoanalyzethebenchmarkprograms.Thisranges
fromafewsecondsfor gzipto840secondsfor tar.Instrumentation
anddynamicdatacollectionrequiresacomparableamountoftime,
rangingfromafewsecondstoabout20minutes.Notethatthisis
the timefor alltestinputs, run insequence, andcan be parallelized
in a straightforward manner. Finally, theinitial ranking and repri-
oritizationprocessesaremuchfaster,andStep 8(i)ofAlgorithm 1
takes 14 seconds,onaverage.
5.2RQ1:Effectiveness ofRanking
Wefirstevaluatethee ffectivenessof DynaBoostğ‘ğ‘™ğ‘™comparedto
Bingoğ‘§ğ‘’ğ‘Ÿğ‘œ. Notice that DynaBoostğ‘ğ‘™ğ‘™is different from Bingoğ‘§ğ‘’ğ‘Ÿğ‘œ
in two ways: feedback from dynamic analysis and augmented
Bayesian network structure. These aspects will be further dis-
cussed in the subsequent sections. In this section, we measure
the initial rankings of all bugs and the number of iterations un-
tilDynaBoostğ‘ğ‘™ğ‘™andBingoğ‘§ğ‘’ğ‘Ÿğ‘œfindallthebugs.Theresultsare
shownin Table2.
Weobservethat DynaBoostğ‘ğ‘™ğ‘™issignificantlymoree ffectiveat
prioritizing true bugs compared to Bingoğ‘§ğ‘’ğ‘Ÿğ‘œ. On average, the full
system,DynaBoostğ‘ğ‘™ğ‘™findsthe bugwithin59.5iterationswhile
Bingoğ‘§ğ‘’ğ‘Ÿğ‘œrequires92.2 iterations,resultingina35%reductionin
thehumanalarminspectionburden.Oneofthemainreasonsofthis
effectivenessistheimprovementofthequalityofinitialrankings
(i.e.,rightaftertransferringfeedback fromDFSan).Before anyuser
feedback, DynaBoostğ‘ğ‘™ğ‘™places the true bugs at rank 160 while
Bingoğ‘§ğ‘’ğ‘Ÿğ‘œyields251,onaverage.Thisstatisticalsowouldbeofin-
terestincaseswhereusersdonotwishtointeractwiththetool,but
merelywantalistingofalarmsaboveacertainthresholdforo ffline
inspection. Another reasonis thereduction of false generalization
whichwillbe discussedmore inthe nextsection.
We notice particularly dramatic improvements in the cases of
cflowandtar. Both of the benchmarks show the improved quality
oftheinitialrankings(from517to173for cflow,from697to253for
tar),therebyproducingthereducednumberofiterationsneeded
tofindallthebugs(78%and58%improvementsfor cflowandtar
respectively).Interestingly,whilebothofthesebenchmarksalready
hadtestcasesthatexercisethedata flowpathsinquestion,theydid
not capture the bugs because they could only be triggered by care-
fullychosentestinputs.Asaresult,theonlyremaininguncertainty
in the ground truth of the alarm arises from incompleteness in the
intervalanalysisperformedbyS parrow.Therefore, DynaBoostğ‘ğ‘™ğ‘™
quicklyprioritizesthesealarms over the rest.Finally,wenoticesmallperformanceregressionsonsomebench-
marks.Forthebu fferoverflowbenchmarkssuchas readelf,these
werecausedbyabiasedtestsuitewhichfailstoexerciseanyofthe
dataflow facts that are involved in the derivation of the bugs. In
thecaseof shntool,wespeculatethattheregressionistheresult
offluctuations caused by the small number of alarms emitted by
Sparrow(23)combinedwiththelargenumberoftruealarms(6),
which together ampli fies the effect of noise in the ranking process.
In any case, we note that for all these benchmarks, the absolute
value of the performance regression is small ( â‰¤13 iterations), and
the overall ranking process still provides massive reductions in the
human alarm inspection burden.
5.3RQ2:ReductionofFalseGeneralization
Next, we measure the impact of dynamic feedback in reducing
false generalization. After each round of feedback, we measure the
average rank of all real bugs, and compare this average to their
averagerankinthepreviousround.Wede fineafalsegeneralization
eventas one in which this average drops by 10% or more and by
at least 5 alarms. The rank reported in Table 3is the sum of this
average rank drop across all false generalization events, and shows
howDynaBoostğ‘ğ‘™ğ‘™mitigates the false generalization problem.
Incaseof sort,DynaBoostğ‘ğ‘™ğ‘™reducesthenumberofrequired
useriterationsby39.7%.Accordingtotheresults,theaveragenum-
berofrankdropeventsoftruealarmsisreducedfrom4.5to0.9.The
average rank drop size for each time is also dropped from 214.8 to
23.1.While Bingoğ‘§ğ‘’ğ‘Ÿğ‘œintroducestwomajorfalsegeneralizations
around iteration 100 and 130, DynaBoostğ‘ğ‘™ğ‘™substantially reduces
theirimpactonthetwosamefalsealarmsarounditeration75.As
shownin Figure4,thederivationrulesofallthesealarmssharethe
tupleobservedatruntime, DUPath(9,25).Thus,for DynaBoostğ‘ğ‘™ğ‘™,
when rejecting the two false alarms, we do not decrease the associ-
atedprobabilitiesblindly,instead,welimittheextentoffeedback
propagation because of the con fidence brought by the observation.
5.4RQ3:ImpactofTestCaseson Ranking
Performance
Inthissection,weconductasensitivitystudywithdi fferentamounts
oftestdata.Since D ynaBoostleveragesdynamicanalysisresults,
the quality of ranking relies on the number and coverage of test
cases. To quantify this relationship, we ran D ynaBooston the
benchmarks with di fferent subsets of the entire test suite, and mea-
suredthenumberofiterationsneededtodiscoverallbugsinthe
programs.Wevariedthefractionoftestcaseschosen,andrepeated
the experiment for eachfraction10 times.
We plot these results in Figure 7. We additionally include the
numberofiterationsneededby Bingoğ‘§ğ‘’ğ‘Ÿğ‘œasavisualbaseline(dot-
tedlines).Noticethattheleft-mostobservationofeachbenchmark,
correspondingtocolumn DynaBoostğ‘§ğ‘’ğ‘Ÿğ‘œinTable2,alreadycor-
responds to72%reduction inalarm inspection burdencomparedto
an unaided user, averaged across all benchmarks. We observe that
formostofthebenchmarks,evenwhenonlyhalfofthetotaltest
inputs are chosen, the number of iterations needed by D ynaBoost
remains close to its e ffectiveness on the complete test suite. In fact,
for a majority of benchmarks, this is true even when we provide
1161Boosting Static Analysis Accuracy withInstrumentedTestExecutions ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
Table 2: E ffectiveness of D ynaBoostcompared to B ingo. #Alarms shows the total number of alarms reported by S parrow.
Init andItersmeasure theaverage initial rank ofthebugsandthenumberofiterationsneeded until findingallthe bugs.
Program #Alarms Bugs D ynaBoostğ‘ğ‘™ğ‘™DynaBoostğ‘§ğ‘’ğ‘Ÿğ‘œBingoğ‘ğ‘™ğ‘™Bingoğ‘§ğ‘’ğ‘Ÿğ‘œ
Init Iters Init Iters Init Iters Init Iters
bc 535 2 121.0 48 327.0 113 84.0 100 358.0 96
cflow 805 1 173.0 21 356.0 105 163.0 163 517.0 94
grep 912 1 144.0 66 714.0 378 44.0 44 72.0 53
gzip 344 14 230.0 235 229.5 326 233.4 287 147.3 283
libtasn1 357 1 5.0 14 113.0 6 3.0 5 33.0 9
patch 502 1 58.0 14 227.0 33 27.0 27 136.0 36
readelf 882 1 300.0 88 111.0 36 460.0 443 181.0 78
sed 819 1 316.0 70 469.0 196 284.0 284 519.0 122
sort 715 1 461.0 106 479.0 174 458.0 446 555.0 176
tar 1369 1 253.0 91 602.0 220 162.0 162 697.0 218
optipng 67 1 3.0 4 5.5 4 11.0 41 7.0 6
latex2rtf 13 2 6.5 5 2.0 2 6.5 6 30.0 14
shntool 23 6 8.2 18 15.2 21 7.7 18 8.0 13
Average 564.8 2.5 159.9 60 280.8 124 149.5 156 250.8 92
0 20 40 60 80
Iteration number0100200300400500600700Rank of bugDynaBoost all
Bingozero
(a)bc-1.060 20 40 60 80
Iteration number0100200300400500Rank of bug
(b) cflow-1.50 10 20 30
Iteration number020406080100120140Rank of bug
(c) patch-2.7.10 20 40 60 80
Iteration number050100150200250300Rank of bug
(d)readelf-2.24
020406080100120
Iteration number0100200300400500Rank of bug
(e)sed-4.30 50 100 150
Iteration number0100200300400500Rank of bug
(f) sort-7.20 50 100 150 200
Iteration number0100200300400500600700Rank of bug
(g)tar-1.280 5 10 15
Iteration number20253035404550Rank of bug
(h) shntool-3.0.5
Figure 6:Ranking changesoftruealarms by D ynaBoostğ‘ğ‘™ğ‘™andBingoğ‘§ğ‘’ğ‘Ÿğ‘œ.The remaining plotsare availableinAppendix B.
just 10% of all test inputs. Furthermore, the variation in number of
iterations quicklydisappears as test casesare added.
OnestrikingobservationinFigure 7isthatformanybenchmarks,
the number of iterations needed by D ynaBoostis independent of
the size and choice of test inputs. We conjecture that many test
inputsexercisesimilarpathsthroughtheprogram,suchasbyen-
tering through main(), parsing command line arguments, or by
exercisingcommonfunctionality,suchasparsingregularexpres-
sionsingrep.Suchinputswouldresultinmanyshareddata flows,
whichareresponsibleforsimilarprioritizationresults.Ontheother
hand,someprogramshaveafewdistinctfunctionalities,suchas tar
which can alternately compress or decompress a file, and samplingtestinputsleadstoabimodalperformancedistribution, aswecan
see inFigure 7f.
Weobservenotablyexceptionalbehaviorfor readelfasthenum-
berof iterationsdegrades withadditional testcases. Accordingto
our investigations, this is because no test case ever explores the
buggy function, process_cu_tu_index() . This function is respon-
sibleforreadingthecontentsof dwofiles,whichcontainDWARF
objectsrelatedtodebuginformationinthebinary.Wesubsequently
chose an intermediate tuple in the derivation tree and manually
provided positive feedback, thus overriding data obtained from the
test cases. This reduced the number of iterations needed to find
the bug from 88 to 49. We conclude that the biased set of test cases
1162ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece Tianyi Chen, Kihong Heo,andMukund Raghothaman
Table 3: Magnitude and frequency of false generalization.
#Eventsindicatesthenumberoffalsegeneralizationevents,
andRank â†“indicatesthesumoftheaveragerankdropacross
allfalsegeneralizationevents.
Program D ynaBoostğ‘ğ‘™ğ‘™ Bingoğ‘§ğ‘’ğ‘Ÿğ‘œ
Rankâ†“# Events Rank â†“# Events
bc 32.5 2 661.0 6
cflow 0 0 421.0 7
grep 0 0 0 0
gzip 55.6 6 225.9 14
libtasn1 0 0 0 0
patch 0 0 72.0 3
readelf 33.0 1 43.0 3
sed 63.0 1 360.0 2
sort 116.0 2 459.0 4
tar 0 0 485.0 16
optipng 0 0 65.0 3
latex2rtf 0 0 0 0
shntool 0 0 0 0
Average 23.1 0.9 214.8 4.5
inreadelfcontinuously prioritizes alarms in other functions and
suppresses the true bug.
5.5RQ4:ImpactofNetworkStructureon
Ranking Performance
Finally, we empirically clarify the impact of augmented network
structure described in Section 4.3. We compare the performance of
DynaBoostğ‘ğ‘™ğ‘™compared to Bingoğ‘ğ‘™ğ‘™that is based on the original
network withfull feedbackfrom dynamic execution.
TheresultsareshowninTable 2.Withtheoriginalnetwork,the
number of required iterations by Bingoğ‘ğ‘™ğ‘™is 2.6x higher than that
forDynaBoostğ‘ğ‘™ğ‘™. For example, we observed signi ficant regres-
sions for readelf,sedandsortwithBingoğ‘ğ‘™ğ‘™.
Interestingly, the performance of Bingoğ‘ğ‘™ğ‘™is even worse than
Bingoğ‘§ğ‘’ğ‘Ÿğ‘œ.Eventhoughdynamicfeedbackimprovesthequalityof
theinitialrankingby40%intheoriginalnetwork,itappearsthat
thenewlyintroducedconditionalindependenciesbythefeedback
heavily limit positive generalization of user feedback. For exam-
ple,Bingoğ‘ğ‘™ğ‘™did not generalize any feedback for benchmarks (e.g.,
cflow,grep,patch,sed, andtar), but just enumerate alarms follow-
ing the initial rankings. This result shows that the new network
structure ismore suitable for handling dynamic feedback.
6 LIMITATIONSAND THREATS TO VALIDITY
Onesigni ficantrestrictionofourexperimentalevaluationisthatwe
have restricted our attention to a single static analyzer (S parrow),
twoanalyses(bu fferoverflowsandtainttracking),andasmallsetof
benchmarkprograms.However,ourprincipalassumptionsarethat
theanalysispermitrecoveryofthederivationgraph(suchasFig-
ure4), and that it permit experimental observation of intermediate
facts.For example, def-use chains are a general building block for
a large class of static analysis tools based on the sparse analysis
framework[ 20],includingTAJS[ 10],Pinpoint[ 28],andSVF[ 31].
Bug-findingtoolsbasedonthesetechniquescandirectlyleverage
our work as describedinthe paper.
Furthermore, if the analysis is expressed in Datalog or using
similar deductive approachesâ€”examples include Chord [ 16] and
Doop [2]â€”and if the abstract behaviors are experimentally observ-
able, then our techniques are again potentially applicable. As an
example,thedataracedetectorinChordfundamentallydependson
a may-happen-in-parallel analysis, which can be experimentally
observedusingdynamicdataracedetectorssuchasRoadRunner[ 7].
Another threat to the validity of our experiments arises from
ourprotocol.Westartedwithasetofhistoricalbugsforeachofthe
programs, and assumed that only this explicitly identi fied target
bug was real, and that all other warnings produced by the static
analyzer were false positives. We simulated user interaction by re-
peatedly examining the alarm with highest conditional probability,
andlabellingitas true orfalse.
Inanycase,notethatweprovideidenticallabelstobothD yn-
aBoostand Bingo. Furthermore, mislabelling can a ffect alarms
in only one direction, i.e., by mistakenly identifying real bugs as
falsewarnings.Asaresult,whenconsideringsuchpotentialmis-
labellings,the numbersinTable 2provide an upperboundonthe
time neededto discover the first real bugusing D ynaBoost.
7 RELATED WORK
Dynamicanalysis. Alargebodyofresearchondynamicanalysis
has been proposed to capture interesting properties of programs
such asmemorysafety[ 19,26,30],datarace[ 7,27],likelyinvari-
ant [6]. While D ynaBoostcurrentlyrelies on DFSan because our
underlying analyzerisbased ondatadependencies,we conjecture
that other combinations of static and dynamic analyzers would be
possible.Forexample,runtimeinformationoftwothreadsthatmay
happen in parallel by RoadRunner [ 7] can be transferred to the
alarm rankingsystemfor staticdataracedetection [ 24].
Combining static and dynamic analysis. Researchers have previ-
ously investigated techniques to combine both approaches, such as
byinsertingdynamiccheckstovalidatepropertieswhicharenot
statically provable [ 1,12,18,29], using information collected from
test executions to optimally set knobs for a subsequent analysis
run[17],concolicexecutiontoguidetestingthroughpiecesofcode
thataredi fficulttoexplore[ 25],usingstaticanalysistominimize
the amount of dynamic monitoring [ 3,14], or bounded exhaustive
testing [34]. Our work is di fferent from the previous work as we
combinethetwoapproachesinaprobabilisticframeworkforalarm
rankingsystem.
User-guided static analysis. Previous user-guided approaches for
staticanalysissuchasalarmclassi fication[13,35],alarmranking[ 9,
24]andalarmclustering[ 11]providemechanismstoincorporate
user feedback to filter out false alarms. However, human user of
staticanalyzers,whoisunawareofthedetailsofanalysisdesign,
canprovideonlylimitedformsoffeedbacksuchaslabelsofalarms.
DynaBoostovercomesthislimitationbyincorporatingdynamic
1163Boosting Static Analysis Accuracy withInstrumentedTestExecutions ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece
(a)grep-2.19
 (b) patch-2.7.1
 (c) readelf-2.24
 (d)sed-4.3
(e)sort-7.2
 (f) tar-1.28
 (g)latex2rtf
 (h) optipng-0.5.3
Figure 7: Performance of D ynaBoostwith a limited number of test cases, sampled from the full test suite. The whisker plot
indicates thedistribution ofobserved results. The remaining plotsare availableinAppendix B.
analysis results from test cases, thereby boosting the performance
ofalarm rankingsystems.
8 CONCLUSION
In this paper, we developed a probabilistic technique to leverage
the results of a dynamic analysis to increase the e ffective accuracy
ofastaticanalyzer.Bytargetedinstrumentationoftheprogram,we
weretoabletoexperimentallycon firmthepresenceofintermediate
conclusions drawn by the staticanalyzer, and use this feedback to
prioritizethegeneratedalarms.Inexperiments,wedemonstrateda
significantreductioninthehumanalarminspectionburden,and
improvements in other related metrics such as the quality of the
initialranking,andfalsegeneralizationevents.Weanticipatepo-
tentialapplicationsofthisresearchinsynthesizingtestcases,and
inautomaticfaultlocalization.
ACKNOWLEDGMENTS
ThisworkwaspartlysupportedbytheNationalScienceFoundation
through grant CCF-2107261, and by the National Research Founda-
tion of Korea (NRF) grant funded by the Korea government (MSIT)
(No. 2021R1C1C1003876 and 2021R1A5A1021944) and Institute for
Information&CommunicationsTechnologyPromotion(IITP)grant
fundedbytheKoreagovernment(MSIT)(No.2021-0-00758,Devel-
opmentofAutomatedProgramRepairTechnologybyCombining
Code AnalysisandMining).
REFERENCES
[1]CyrilleArthoandArminBiere.2005. CombinedStaticandDynamicAnalysis.
Electron.NotesTheor. Comput. Sci. 131(2005), 3â€“14.
[2]Martin Bravenboer and Yannis Smaragdakis. 2009. Strictly Declarative Speci fica-
tionofSophisticatedPoints-toAnalyses.In Proceedingsofthe24thACMSIGPLAN
Conference on Object Oriented Programming Systems Languages and Applications
(OOPSLA2009) . ACM,243â€“262.[3]Walter Chang, Brandon Strei ff, and Calvin Lin. 2008. E fficient and extensible
securityenforcementusingdynamicdata flowanalysis.In Proceedingsofthe2008
ACM Conference on Computer and Communications Security (CCS) . ACM, 39â€“50.
[4]Paul Eggert. 2010. sort:fix very-unlikely bu ffer overrun when merging
to input file.http://git.savannah.gnu.org/cgit/coreutils.git/commit/?id=
14ad7a25505ec3127cd1f07001d54d94f51f1748
[5]DanEllis. 2003. SoundExamples .https://www.ee.columbia.edu/~dpwe/sounds/
[6]MichaelD.Ernst,JakeCockrell,WilliamG.Griswold,andDavidNotkin.1999.
Dynamically Discovering Likely Program Invariants to Support Program Evolu-
tion. InProceedings of the 1999 International Conference on Software Engineering
(ICSE). ACM,213â€“224.
[7]CormacFlanaganandStephenFreund.2010. TheRoadRunnerDynamicAnalysis
FrameworkforConcurrentPrograms.In Proceedingsofthe9thACMWorkshop
onProgramAnalysis forSoftware Tools and Engineering (PASTE2010) .ACM,1â€“8.
[8]Kihong Heo, Hakjoo Oh, and Kwangkeun Yi. 2017. Machine-learning-guided
Selectively Unsound Static Analysis. In Proceedings of the 39th International
Conference onSoftwareEngineering (ICSE2017) . IEEE Press,519â€“529.
[9]KihongHeo,MukundRaghothaman,XujieSi,andMayurNaik.2019. Continuous
ProgramReasoningviaDi fferentialBayesianInference.In Proceedingsofthe40th
ACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation
(PLDI). ACM,561â€“575.
[10]SimonHolmJensen,AndersMÃ¸ller,andPeterThiemann.2009. TypeAnalysis
for JavaScript.In Static Analysis . Springer, 238â€“255.
[11]Woosuk Lee, Wonchan Lee, Dongok Kang, Kihong Heo, Hakjoo Oh, and
KwangkeunYi.2017. SoundNon-StatisticalClusteringofStaticAnalysisAlarms.
ACMTransactionsonProgrammingLanguagesandSystems 39,4,Article16(2017),
35pages.
[12]Kaituo Li, ChristophReichenbach,ChristophCsallner, and YannisSmaragdakis.
2012. Residualinvestigation:predictiveandprecisebugdetection.In International
SymposiumonSoftwareTestingand Analysis (ISSTA) . ACM,298â€“308.
[13]Ravi Mangal, Xin Zhang, Aditya Nori, and Mayur Naik. 2015. A User-Guided
Approach to Program Analysis. In Proceedings of the 10th Joint Meeting on Foun-
dationsofSoftwareEngineering (ESEC/FSE 2015) . ACM,462â€“473.
[14]Misael MongiovÃ¬, G. Giannone, Andrea Fornaia, Giuseppe Pappalardo, and Emil-
iano Tramontana. 2015. Combining static and dynamic data flow analysis: a
hybridapproachfordetectingdataleaksinjavaapplications.In Proceedingsof
the30thAnnualACMSymposiumonAppliedComputing (SAC) .ACM,1573â€“1579.
[15]Joris Mooij. 2010. libDAI: A free and open source C++ library for discrete
approximateinferenceingraphicalmodels. JournalofMachineLearningResearch
11(Aug 2010),2169â€“2173.
[16]MayurNaik, AlexAiken, and JohnWhaley. 2006. E ffectivestatic racedetection
forJava.In Proceedingsofthe27thACMSIGPLANConferenceonProgramming
Language Designand Implementation (PLDI2006) . ACM,308â€“319.
[17]MayurNaik,HongseokYang,GhilaCastelnuovo,andMoolySagiv.2012. Abstrac-
tions from tests. In Proceedings of the39th ACM SIGPLAN-SIGACT Symposium on
1164ESEC/FSE â€™21, August 23â€“28, 2021,Athens,Greece Tianyi Chen, Kihong Heo,andMukund Raghothaman
Principles ofProgrammingLanguages, POPL . ACM,373â€“386.
[18]GeorgeC.Necula, Scott McPeak, andWestleyWeimer.2002. CCured: type-safe
retrofittingoflegacycode.In The29thSIGPLAN-SIGACTSymposiumonPrinciples
of Programming Languages (POPL) , John Launchbury and John C. Mitchell (Eds.).
ACM,128â€“139.
[19]NicholasNethercoteandJulianSeward.2007. Valgrind:AFrameworkforHeavy-
weight Dynamic Binary Instrumentation. In Proceedings of the 28th ACM Confer-
ence onProgrammingLanguage Designand Implementation . ACM,89â€“100.
[20]HakjooOh,KihongHeo,WonchanLee,WoosukLee,andKwangkeunYi.2012.
Design and Implementation of Sparse Global Analyses for C-like Languages.
InProceedingsofthe33rdACMSIGPLANConferenceonProgrammingLanguage
Designand Implementation (PLDI2012) . ACM,229â€“238.
[21]HakjooOh,KihongHeo,WonchanLee,andKwangkeunYi.2012. TheSparrow
Static Analyzer. https://github.com/ropas/sparrow .
[22]Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of
PlausibleInference . Morgan Kaufmann.
[23]Matt Powell. 2001. The Canterbury Corpus .https://corpus.canterbury.ac.nz/
index.html
[24]MukundRaghothaman,SulekhaKulkarni,KihongHeo,andMayurNaik.2018.
User-guided Program Reasoning Using Bayesian Inference. In Proceedings of the
39th ACM SIGPLAN Conference on Programming Language Design and Implemen-
tation (PLDI) . ACM,722â€“735.
[25]Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: a concolic unit testing
engineforC.In Proceedingsofthe10thEuropeanSoftwareEngineeringConference
heldjointlywith13thACMSIGSOFTInternationalSymposiumonFoundationsof
SoftwareEngineering (FSE) . ACM,263â€“272.
[26]Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry
Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker. In Proceed-
ings of the 2012 USENIX Conference on Annual Technical Conference (USENIXATC). USENIXAssociation.
[27]KonstantinSerebryanyandTimurIskhodzhanov.2009. ThreadSanitizer:Data
Race Detection in Practice.In Proceedings ofthe Workshop on Binary Instrumen-
tation and Applications (WBIA) . ACM,62â€“71.
[28]QingkaiShi,XiaoXiao,RongxinWu,JinguoZhou,GangFan,andCharlesZhang.
2018. Pinpoint: Fast and Precise Sparse Value Flow Analysis for Million Lines
ofCode.In Proceedingsofthe39thACMSIGPLANConferenceonProgramming
Language Designand Implementation (PLDI2018) . ACM,693â€“706.
[29]Jeremy G. Siek and Walid Taha. 2006. Gradual Typing for Functional Languages.
InINSCHEME ANDFUNCTIONAL PROGRAMMINGWORKSHOP . 81â€“92.
[30]Evgeniy Stepanov and Konstantin Serebryany. 2015. MemorySanitizer: Fast
Detector of Uninitialized Memory Use in C++. In Proceedings of the 13th Annual
IEEE/ACM International Symposium on Code Generation and Optimization (CGO) .
IEEE,46â€“55.
[31]YuleiSuiandJinglingXue.2016. SVF:InterproceduralStaticValue-FlowAnal-
ysis in LLVM. In Proceedings of the 25th International Conference on Compiler
Construction (CC2016) . ACM,265â€“266.
[32]The Clang Team. 2020. DataFlowSanitizer .https://clang.llvm.org/docs/
DataFlowSanitizer.html
[33] Willem vanSchaik.2011. PngSuite.http://www.schaik.com/pngsuite/
[34]Banghu Yin, Liqian Chen, Jiangchao Liu, Ji Wang, and Patrick Cousot. 2019.
Verifying Numerical Programs via Iterative Abstract Testing. In SAS (Lecture
NotesinComputer Science,Vol.11822) . Springer, 247â€“267.
[35]XinZhang, RaduGrigore, XujieSi,and Mayur Naik. 2017. E ffectiveInteractive
ResolutionofStaticAnalysisAlarms. ProceedingsoftheACMonProgramming
Languages OOPSLA, Article57(2017).
1165