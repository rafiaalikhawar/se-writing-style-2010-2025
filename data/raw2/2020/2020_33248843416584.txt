Metamorphic Object Insertion for Testing Object Detection
Systems
Shuai Wangâˆ—
The Hong Kong University of Science and Technology
shuaiw@cse.ust.hkZhendong Su
ETH Zurich
zhendong.su@inf.ethz.ch
Abstract
Recentadvancesindeepneuralnetworks(DNNs)haveledtoob-
ject detectors (ODs) that can rapidly process pictures or videos,
and recognize the objects that they contain. Despite the promising
progress by industrial manufacturers such as Amazon and Google
in commercializing deep learning-based ODs as a standard com-
putervisionservice,ODs â€”similartotraditionalsoftwareâ€”may
still produce incorrect results. These errors, in turn, can lead to se-
vere negative outcomes for the users. For instance, an autonomous
drivingsystem thatfails todetect pedestrianscan causeaccidents
or even fatalities. H owever, despite their importance, principled,
systematic methods for testing ODs do not yet exist.
Tofillthiscriticalgap,weintroducethedesignandrealizationof
MetaOD,ametamorphictestingsystemspecificallydesignedfor
ODstoeffectivelyuncovererroneousdetectionresults.Tothisend,
we (1) synthesize natural-looking images by inserting extra object
instancesintobackgroundimages,and(2)designmetamorphiccon-
ditionsassertingtheequivalenceofODresultsbetweentheoriginal
and syntheticimagesafter excludingthe predictionresults onthe
insertedobjects.MetaODisdesignedasastreamlinedworkflow
that performs object extraction, selection, and insertion. We de-
velop a set of practical techniques to realize an effective workflow,
andgeneratediverse,natural-lookingimagesfortesting.Evaluated
onfourcommercialODservicesandfourpretrainedmodelspro-
vided by the TensorFlow API, MetaOD found tens of thousands
of detection failures. To further demonstrate the practical usageof MetaOD, we use the synthetic images that cause erroneousdetection results to retrain the model. Our results show that the
model performance is significantly increased,froman mAP score
of 9.3 to an mAP score of 10.5.
CCS Concepts
â€¢Softwareanditsengineering â†’Softwaretestinganddebug-
ging;â€¢Securityandprivacy â†’Softwareandapplicationsecurity ;
â€¢Computing methodologies â†’Neural networks;
âˆ—Most of the work was done when Shuai Wang was working at ETH Zurich.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™20, September 21â€“25, 2020, Virtual Event, Australia
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416584Keywords
testing, computer vision, object detection, deep neural networks
ACM Reference Format:
Shuai Wang and Zhendong Su. 2020. Metamorphic Object Insertion for
Testing Object Detection Systems. In 35th IEEE/ACM International Con-
ference on Automated Software Engineering (ASE â€™20), September 21â€“25,
2020, Virtual Event, Australia. ACM, New York, NY, USA, 13pages.https:
//doi.org/10.1145/3324884.3416584
1 Introduction
Deep learning-based object detectors (ODs) identify objects in a
givenimageusingconvolutionalneuralnetworks.Currently,sev-
eral major industrial manufacturers, including Google, Amazon,
Microsoft,andLockheedMartin,arebuildingandimprovingODsto
serve as the basis for various computer vision tasks. These models
arewidely-usedinreal-worldapplications,suchasopticalcharacter
recognition(OCR),robotics,machineinspection,andpedestriande-
tection in autonomous cars. They are also used as an initial step in
surveillance and medical image analysis applications, which often
require highly precise and reliable detection results.
Despite this spectacular progress, however, de ep learning-based
ODsâ€”similartotraditionalsoftwareâ€”canyielderroneouspredic-
tionresultsthatarepotentiallydisastrous.Inparticular,giventhe
widespread adoption of ODs in critical applications in the security,
medical,andautonomousdrivingfields,incorrectorunexpected
edge-case behaviors have caused severe threats to public safety or
financialloss[ 11,43,73].Forinstance,inoneinfamouscasein2016,
Teslaâ€™s autopilot mode caused a fatal crash when the autonomous
driving system failed to recognize a white truck against a bright
sky[15].Morerecently,anUberautonomouscarkilledapedestrian
crossing the road, which is believed to have been due to its failure
in recognizing a pedestrian in dark clothing [32].
Inrecentyears,anumberoftechniqueshavebeendesignedto
testdeeplearningsystems,includingbothconvolutionalneuralnet-
works(CNN)andrecurrentneuralnetworks(RNN)models[ 20,57].
Thetechniqueshavealsobeenappliedtotestdomain-specificap-
plicationssuchasautodrivingsystems[ 11,73,84]andtotestthe
underlying infrastructure of deep learning libraries [ 21,39,61].
However, the principles specific for testing ODs have not been in-
vestigatedsystematicallybyexistingresearch,which,thus,unlikely
results in comprehensive, systematic testing of ODs.
Thispapertacklesthisimportantproblembyintroducingameta-
morphictesting[ 16,17]framework,MetaOD,tospecificallyand
effectivelyexposeerroneouspredictionsofODs.Givenarealim-
ageastheâ€œbackgroundâ€,MetaODinsertsanobjectinstanceinto
the background, generates a synthetic image, and then employsa metamorphic condition to check the consistency of OD results
betweenthesyntheticimageandthecorrespondingbackground.To
10532020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
effectively generate diverseandnatural-looking images that trigger
practicalprediction errors, MetaOD is designed as a three-step ap-
proach, performing object extraction, object refinement/selection,
andobjectinsertion.Theobjectextractionmoduleextractsobject
instanceimagesfromalargesetofpicturesusingadvancedinstance
segmentationtechniques[ 14],thusaggregatingmanyobjectsets
distinguished by category. Then, given a background image, the
objectrefinement/selectionmoduleimplementsasetoflightweight,
albeiteffective,criteriaforselectingcertainobjectsfromobjectsets
thatare closelyrelatedtothe background.Todetermineinsertion
locations, the object insertion module uses domain-specific criteria
andtechniquesinspiredbydeltadebugging[ 83]tofindlocations
thatpresumablytriggerpredictionerrors,whileretainingrealism
and diversity of the synthetic images.
Theproposedworkflowiseffectiveandshowspromisingresults
when evaluated on four commercial ODs provided by Amazon,
Google, IBM, and Microsoft [ 1,2,4,6] and four pretrained OD
modelsprovidedbytheTensorFlowAPI[ 3].Ourtestingrevealed
tens of thousands of erroneous OD outputs from these popular
(commercial)services.Inaddition,weretrainedanODmodelusing
syntheticimagesthatcausethismodeltooutputerroneousoutputs.
The evaluation results show that the model performance improved
substantially after retraining. In summary, this work makes the
following main contributions:
â€¢WeintroduceanovelmetamorphictestingapproachforODs,
vital components in various computer vision applications
(e.g.,self-drivingcars).OurtechniquetreatsODsasâ€œblack-
boxesâ€.Thus,itishighlygeneralizablefortestingreal-world
ODs, such as remote services on the cloud.
â€¢Togeneratediverseandnatural-lookingsetsofimagesasthe
test inputs, we design and realize MetaOD, a streamlined
workflow that performs object extraction, object refinemen-
t/selection,andobjectinsertiontosynthesizeinputimages
in an efficient and adaptive manner.
â€¢Our approach tests ODs in a realisticsetting and delineates
thecapabilitiesof state-of-the-artcommercialODs. Froma
total of 292,206 input images, MetaOD found 38,345 erro-
neousdetectionresultsineightpopular(commercial)ODs.
ByleveragingsyntheticimagesthattriggererroneousOD
outputs for retraining, we show that the performance of OD
models can be substantially improved.
Wehavereleased MetaODonGithub[ 9].Allofour erroneous
detection results can be found at [8].
2 Background of Object Detection Techniques
Object detection is conventionally addressed using handcrafted
featuresandselectiveregionsearches[ 60,77,79].Theinputimages
are dissected into small regions (each region is called a â€œregionproposalâ€ and is likely to contain an object) via heuristics [
77].
Then,featuresareextractedfromeachregionproposalforobject
classification.Todate,twomajorlinesofresearch(popularmod-els proposed in both line of research are tested in this work; seeSec.6) exist that have drastically improved OD techniques with
deep learning, both of which are briefly introduced below.
Two-StageRegion-BasedODs. Motivatedbytheprimarysuccess
in applying DNNs for image classification [ 41], RCNN [ 30] was



	


	







Figure 1: Typical two-stage OD architectures illustrated by
the simplified Fast RCNN workflow.


	
		

	

	


Figure2:Typicalsingle-stageODarchitecturesillustratedby
the simplified YOLO workflow.
among the first to apply CNN for building ODs. The proposed
techniqueformsatwo-stagepipelineinwhicheachregionproposal
extracted from the input image is an input to CNN for feature
extraction.Then,theextractedfeaturesareforwardedtoanSVM
classifier and a bounding box regressor to determine the object
category and bounding box offsets, respectively.
Since then, OD research has focused on rapidly evolving the
RCNNarchitecture[ 18,34,67]andremovingexplicitdependence
on region proposals to improve speed. Fast-RCNN [ 29] introduced
amodern end-to-end predictionpipeline.AsshowninFig. 1,instead
ofregionproposals,theentireimageisforwardedtotheCNNto
generate a convolutional feature map and region proposals areextracted from the feature map (first stage ). A Region of Interest
(RoI)poolinglayerisplacedbeforethefullyconnectedlayer(FC)to
reshape each proposal into a fixed size, and FC layerâ€™s outputs are
fedtosoftmaxandbboxregressorlayersforobjectclassificationand
for determining bounding box offsets, respectively (second stage ).
Single-Stage ODs. Two-stage ODs use regions, explicitly or im-
plicitly, for object localization. Another line of research aims to
proposeacost-effectivesolutionwithoutregionproposalsbyde-
signing a single-stage feed-forward CNN network in a monolithic
setting. Such networks are usually less computationally intensive
by trading precision for speed, and are usually more suitable for
real-time tasks or for use in mobile devices.
The YOLO (You Only Look Once) [ 64â€“66] and SSD (Single Shot
Detector) [ 47] models are de facto ODs that feature single-stage
architectures. Fig. 2depicts the YOLO workflow, in which input
images are first divided into an ğ‘†Ã—ğ‘†grid; then, a fixed number of
bounding boxes are predicted within each grid. For each bounding
box, the network outputs a class probability and the bounding
1054box offsets. A bounding box is deemed to contain objects when its
classprobabilityexceedsathresholdvalue.Theentirepipelineis
typically orders of magnitude faster than region-based techniques.
Indeed,theobjectextractionmoduleof MetaOD(seeSec. 4.1)is
built on top of YOLACT [ 14], a real-time instance segmentation
model that was inspired by YOLO.
In general, given a well-trained model, region proposal-based
objectdetectiontechniquescanusuallyachievehighperformances;
incontrast,aâ€œunifiedâ€pipelinessuchasYOLOandSSDachievehighdetectionefficiencybytradingprecisionforspeed.Whileprecision
may often be the highest priority, unified models have also been
commonlyadopted,particularlyformobile/wearabledevicesand
analyzing videos, wher espeedis critical.
3 Approach Overview
Metamorphictesting(MT)hasbeenwidelyusedtoautomatically
generateteststodetectconventionalsoftwarefaults[ 16,17]and
DNN prediction errors [ 22,73,84]. The strength of MT lies in its
capability to alleviate the test oracle problem via metamorphic
relations(MRs).EachMRdepictsnecessarypropertiesofthetarget
software in terms of inputs and their expected outputs. Even if the
correctnessofactualoutputsaredifficulttodetermine,itispossible
to construct and check proper MRs among the expected outputs
ofthegiveninputs todetectsoftwaredetects.Inthisresearch,we
applymetamorphictestingtoOD.Toprovideanapproachoverview,
we start by formulating relevant notations.
Byfeedingatestimage ğ‘–toanOD ğ‘‘,thepredictionoutputisde-
notedasğ‘‘[[ğ‘–]],whichconsistsof ğ‘three-elementtuples (ğ‘ğ‘˜,ğ‘™ğ‘˜,ğ‘ğ‘˜),
whereğ‘denotesthenumberofobjectsrecognizedin ğ‘–,ğ‘ğ‘˜theloca-
tion of the ğ‘˜th object recognized in ğ‘–,ğ‘™ğ‘˜the category label, and ğ‘ğ‘˜
theconfidencescoreoftheprediction.Then,givenasetofobject
instance images Oextracted from a large number of real images
(see Fig.4.1), and a set Cwhere each CâˆˆCis a 2-D coordinate
(ğ‘¥,ğ‘¦)inğ‘–, a synthetic image ğ‘–/primecan be represented as:
ğ‘–/prime=C(ğ‘œ,ğ‘–),ğ‘œâˆˆOandCâˆˆC
whereğ‘œis placed such that its cendroid is at the 2-D coordinate
specified by C. Note that in this research, we do notapply any
transformationrules(rotation,blurring,etc.)ontheinsertedobjects
to preserve realism at our best effort, and CâˆˆCis deliberately
constructedsuchthattheinsertedobject ğ‘œdoesnotoverlapwith
preexisting objects in the â€œbackgroundâ€ image ğ‘–. Therefore, the MR
adopted in this research can be formalized as follows:
âˆ€CâˆˆCâˆ€ğ‘œâˆˆO.E(ğ‘‘[[ğ‘–]],ğ‘‘[[ğ‘–/prime]] \ {(ğ‘ğ‘œ,ğ‘™ğ‘œ,ğ‘ğ‘œ)})
whereğ‘–/prime=C(ğ‘œ,ğ‘–). Here, we exclude the prediction result on ğ‘œ
(i.e., tuple (ğ‘ğ‘œ,ğ‘™ğ‘œ,ğ‘ğ‘œ))f r o mğ‘‘[[ğ‘–/prime]], and Eis a criterion asserting
theequalityofODresults(seeSec. 3.1).ThegivenMRisdefined
suchthatnomatterhowtheimage ğ‘–/primeissynthesizedbyinserting
anadditionalobject ğ‘œonğ‘–,theODresultsareexpectedtobecon-
sistentwiththoseintheoriginalimage.Consequently,erroneous
predictions can be revealed by checking the failure of this MR.
WhilethegivenMRholdsforanysyntheticimage ğ‘–/prime=C(ğ‘œ,ğ‘–),
one practical challenge is that not all the synthetic images repre-
sentreal-worldscenarios.Indeed,thereexistsresearchintheCV
communitywhereunrealisticimagesaresynthesizedtotrainimageanalysis models, for example, by placing a car on the table [ 74].
While the synthetic â€œunrealisticâ€ images may fulfill the require-
ment of â€œmodel trainingâ€, we aim to also augment the realism of
the synthetic images such that flagged erroneous behaviors unveil
practical defects that can cause confusion during daily usage of
ODs.Additionally,aswewillexplaininSec. 4.3,randomlydeciding
a position for insertion without considering preexisting objectsâ€™
positions in the background would undermine the effectiveness of
the proposed technique.
Therefore, in this research, we gather O/primeâŠ‚Osuch that O/prime
containsobjectinstanceimagesthatarecloselyrelatedtotheback-
ground image ğ‘–(see Sec. 4.2). We also form favorable insertion
locations C/primeâŠ‚Clikelytotriggerpredictionerrorsbyleveraging
empirical evidence and strategies inspired by delta debugging (see
Sec.4.3). Hence, the MR is modified as follows:
âˆ€CâˆˆC/primeâˆ€ğ‘œâˆˆO/prime.E(ğ‘‘[[ğ‘–]],ğ‘‘[[ğ‘–/prime]] \ {(ğ‘ğ‘œ,ğ‘™ğ‘œ,ğ‘ğ‘œ)})
In all, an OD system â€œfailureâ€ indicates a wrong prediction over
synthesized image ğ‘–/primew.r.t. its reference image ğ‘–(i.e., the ground
truth),consistentwithexisting SEresearchontestingdeepimage
classifiers (e.g., [22, 84]).
3.1 Equality Criteria
Assertingthe equalityofODoutputs(i.e., ğ‘three-elementtuples
(ğ‘ğ‘˜,ğ‘™ğ‘˜,ğ‘ğ‘˜))istoostrictbecauseboundingboxesofcertainobjects
couldslightlydriftwithineachroundofprediction.TheCVcom-
munityinsteadusesastandardmetric,AveragePrecision(AP)[ 24],
to compensate small localization drifting when measuring ODs.
TheAPscoreiscomputedbytakingbothâ€œprecisionâ€andâ€œrecallâ€
values into account, as we will explain soon.
TocomputeAP,IntersectionoverUnion(IoU)isusedtomeasure
eachobjectdetectionboundaryw.r.t.thegroundtruth.Asshown
inFig.4,IoUmeasurestheoverlapbetweentwoboundingboxes
withthesamepredictionlabel(i.e.,â€œelephantâ€),anddenoteshow
muchthepredictedboundaryoverlapswiththegroundtruth.Incase IoU is greater than a threshold
ğœ–(e.g., 0.5), the prediction is
atruepositive.Theprecisionandrecallscoresarethencomputed
bytakingallthepredictionresultsintoaccount,andtheAPscore
can be derived by computing the area under the precision-recallcurve [
42,69]. For an image with objects of different categories,
mean AP (mAP) is further computed by averaging all AP scores.
Our equality criteria Eis derived from mAP. In our setting, the
predictionresultsoftheâ€œbackgroundâ€image ğ‘–entailstheground
truth, and are compared with detection results of the synthetic
imageğ‘–/prime=C(ğ‘œ,ğ‘–).Sinceğ‘œdoesnotoverlapwithexistingobjectson
ğ‘–,and,therefore,doesnotinterferewithrelevantpredictions,the
mAP score is expected to be 100%. Thus, Eis defined as follows:
E(ğ‘‘[[ğ‘–]],ğ‘‘[[ğ‘–/prime]] âˆ’ (ğ‘ğ‘œ,ğ‘™ğ‘œ,ğ‘ğ‘œ))/doteq
ğ‘šğ´ğ‘ƒ(ğ‘‘[[ğ‘–]],ğ‘‘[[ğ‘–/prime]] âˆ’ (ğ‘ğ‘œ,ğ‘™ğ‘œ,ğ‘ğ‘œ))=100%
Todate,multiplevariantsofthestandardmAPdefinitionexist.
WeadoptoneofthemostpopularmAPcalculationmethods,the
PASCAL VOC metric [42], in our implementation.
3.2 Case Study of OD Failures
EdefinedinSec. 3.1enablesaunifiedapproachtocheckODfailures
â€”itisimagecontentagnosticandthereforecanbeautomatically
1055







 	



 
Figure3:ODerrorsfoundbyMetaOD.Weslightlycherrypickedimagesinfavorofreadability.Browsethefullresultsat[8].
The inserted objects are pointed by blue arrows. To preserve realism of the synthetic images at our best effort, the inserted
objects are resized to the average size of existing objects of the same category. See our discussions in Sec. 4.2.

	
 
		








	












Figure 4: Intersection over Union (IoU).
conducted. From a holistic perspective, the following categories of
OD defects can be found by asserting E:
â€¢Recognition failures represent errors which treat an arbi-
traryregionontheimagecontainingnoobjectasanâ€œobjectâ€
or fail to recognize an existing object.
â€¢Classification failures represent labeling errors, for in-
stance labeling a human being as a â€œbird.â€
â€¢Localization failures representthefailurewheretheOD
usesatoolargeortoosmallboundingboxtolocalizeobjects.
AsillustratedinFig. 4,toolargedriftings( ğ¼ğ‘œğ‘ˆ<ğœ–)onthe
bounding box are not allowed.
Nevertheless, after manually checking OD failures found by
MetaOD, we only find recognition failures.1MetaOD has success-
fullyfoundalargenumberofODfailuresbyeightpopularODs(see
Table2). Fig.3reports three cases, where the â€œbackgroundâ€ images
on the first row are from the Berkeley DeepDrive dataset [ 5,82]
1Wefoundover28KimagestriggeringODerrors(reportedinSec. 6).Wemanually
checkedabout800imagesbyre-queryingtheremoteserviceswiththeerrortriggering
images and screened the detection outputs.andtheCOCOdataset[ 45].Imagesonthesecondrowaregener-
atedbyinsertingoneextraobjectontheircorrespondingreference
â€œbackground.â€
By inserting extra objects (indicated by the blue arrows) into
thebackgroundandcheckingtheequalitycriteria E(Sec.3.1),we
wereabletouncovermanydetectiondefects.Thefirstcolumnin
Fig.3illustrates a recognition failure (indicated by the red arrows),
where a bike rider and several cars in the traffic scene image could
notberecognizedafteranewvehiclewasinserted.Similarly,the
synthetic images in the second and third columns unveil detec-
tion failures, where after inserting one extra object into imagesof real-world scenes, existing objects (moon and frisbee) cannotbe recognized. We note that Fig. 3demonstrates the diversity in
theissueswefound; MetaODsynthesizestestimagesof different
scenes and therefore can find a broad set of defects. In contrast,
existing relevant efforts [ 73,84] primarily transform or synthesize
imagesofonlydrivingscenes(seeSec. 3.3).Also,wenotethatwhile
thesyntheticimagesreasonblyrepresentreal-lifescenes,making
insertedobjects fullyconsistentwitharbitrarybackgrounds couldbe
extremely challenging, if at all possible. See our further evaluation
and discussion on this matter in Sec. 6.3and Sec.7.
3.3 Application Scope
It is worth noting that we are nottesting extreme cases to stress
ODs [12]. Apparently, we can synthesize images that are highly
challenging to human beings and therefore challenging to ODsas well, for instance, by tweaking the contrast of objects and its
background[ 23].Therefore,whileinthisresearchweproposeaset
of techniques toselect â€œrealisticâ€ objects for insertion(see Sec. 4.2
and Sec.4.3), we still define a conservative test oracle such that we
excludethe prediction over the newly inserted object, and check
only the consistency of the remaining predictions.
1056	

 
	 
    	 	
    
		

		


		


  	

			

	 


	 	
 	
			

Figure 5: Workflow of MetaOD.
MetaOD is designed as a general framework to treat ODs as
black-boxes.WhiledefactoimplementationsofODs arecurrently
all on the basis of DNN, MetaOD could also be used to assess
the performance of ODs based on other techniques. In addition,existing approaches on testing computer vision models [
73,84]
applypredefinedâ€œsevereweatherconditionsâ€(e.g.,foggyandrainy)
to transform or directly synthesize entire images. They are not
tailoredtopinpointingODfailures,andareconceptually orthogonal
to the object-level mutations proposed in this work. Also, theirtransformations may be inapplicable to mutate arbitrary images
while preserving realism. For instance, applying severe weather
conditions toward images of indoor scenes is likely unreasonable.
4 Design
Fig.5depictsaholisticviewoftheproposedtechnique.Togenerate
imageğ‘–/prime=C(ğ‘œ,ğ‘–)fortesting,MetaODisconstructedasastream-
lined workflow that includes object extraction, object selection/re-
finement,andobjectinsertionmodules.ByprovidingMetaODwithasetofimages(e.g.,imagesfromtheCOCOdataset[
45]),itsobject
extractionmoduleperformsadvancedobjectinstancesegmentation
techniques to identify and extract object instances (Sec. 4.1). Then,
givenanimageastheâ€œbackgroundâ€,theobjectselectionmodule
determines an appropriate object to be inserted in the background
(Sec.4.2), using a set of criteria to find similar objects, rule out
low-qualityobjectsandadjusttheobjectsize.Whilethefirsttwo
stepsaddressthechallengeofâ€œwhattoinsertâ€,foraparticularback-
groundimage,weneedtofurtheranswerthequestionofâ€œwhere
toinsert.â€Weaggregateempiricalevidenceandderiveheuristics
to select insertion positions. Furthermore, motivated by how delta
debugging [ 83] is applied to test conventional software, we pro-
pose techniques to identify more locations for object insertion and
therefore synthesize more images (Sec. 4.3).
4.1 Object Extraction
Westartbyperformingobjectextractiontoidentifyandextracta
poolofobjectinstancesfrominputimages.Whileobjectextraction
isgenerallyconsidereddifficult,deeplearning-based instanceseg-
mentation models have been shown to work well to extract objects
from images [ 14,33,44]. For example, given the â€œtwo elephantsâ€
image in Fig. 1, instance segmentation models can recognize both
elephants and put two masks over both â€œelephantâ€ objects.
Similar to OD, instance segmentation model design also has
twoprimaryfocuses,accuracy[ 33,44]andspeed[ 14,76],which
usually cannot coexist. In this work, we concentrate on models
that emphasize speed over accuracy. The object extraction moduleis designed to swiftly extract objects from large sets of diverseimages. Therefore, speed takes priority over accuracy (although
in practiceour adoptedinstance segmentationmodel hasa good
accuracy as well). In Sec. 4.2, we compensate for the â€œaccuracyâ€ of
extractedobjectsbyproposingtechniquestoruleoutlow-quality
objectimages.Byorchestratingobjectextractionandrefinement
modules in pipeline,we output sets ofhigh-quality labeled object
images with a modest cost and high speed.
Tothisend,wereuseYOLACT[ 7],arecentlydevelopedreal-time
instancesegmentationtool,tobuildtheobjectextractionmodule.
Our empirical evidence (also reported in its paper [ 14]) shows that
YOLACT has impressive speed and quite good accuracy in practice
when processing real-world images. YOLACT outputs a mask over
each recognized object instance. We extend YOLACT by reusing
theobjectmaskstoextracteachobjectfromthebackgroundimage
and save each object into an individual image.
4.2 Object Refinement and Selection
Despite significant progress, instance segmentation remains a diffi-
cult problem, and we have observed that some of its outputs are
of lowquality.According to our observations,these â€œlow-qualityâ€
objectimages occurfor twomain reasons:(1) someobjects inthe
input image are too small, and (2) some objects overlap and there-
fore fragmentary object images are extracted.2We acknowledge
the general difficultyof outputting high-quality object images.In-
stead, our object extraction module processes large sets of images
athighspeed , and we further prune low-quality objects and select
appropriate objects closely related to a â€œbackgroundâ€ image.
Small Object Image Pruning. AsshowninFig. 5,theoutputof
objectextractionconsistsofmultiplesetsofimages,whereeachset
contains object instances with the same label. During this step, we
firstprunesmallobjectimageswithineachobjectset,whichpre-
sumably include low-resolution or fragmentary images unsuitable
for use. To perform pruning, we sort the object instance images
within each set by image size and remove the majority of object
images (in our implementation, we empirically decided to remove
90% of the object images).ObjectImageSimilarityAnalysis.
Foraparticularâ€œbackgroundâ€
image with several preexisting objects, we aim to find object in-
stance images from the pool that are closely related to the back-
groundtofulfilltherequirementoftestingODwhilealsopreserving
the realism of the synthetic images as much as possible. To this
2Suchgeneralchallengesstillexistevenifwetentativelytriedmoreâ€œheavy-weightâ€
instancesegmentationmodelsliketheTensorflowimplementationofMaskRCNN[ 33].
1057end, we perform an image similarity analysis using image hashing
techniques.Imagehashingisastandardtechniqueforpixel-level
image similarity analysis. The process creates similar hashes for
similar images. In contrast, when using a crypto hash algorithm
such as MD5,one byte ofdifference can lead todrastic hash value
changes due to the avalanche effect [56].
Given animage ğ‘–with threeâ€œbirdsâ€,we start bycomputing the
averageimagehashvalueoftheseâ€œbirdâ€objectimages.Thenwe
iterateoveralltheâ€œbirdâ€imagesinthepool(seeFig. 5)andidentify
aâ€œbirdâ€whoseimagehashvaluehastheshortestHammingdistance
with the average hash value. This â€œbirdâ€ will be used for insertion.
If imageğ‘–contains objects with ğ‘different labels, we repeat the
procedure ğ‘times.Therefore, ğ‘objectsofdifferentcategorieswill
be selected for insertion. In this way, we ensure the â€œrealismâ€ of
thesyntheticimagesasmuchaspossible.Ourobservationsshow
thattheselectedâ€œsimilarâ€objectimagescanusuallyexhibittexture
and resolution that are close to those of the background image.
For the implementation, we use the average hash [ 40], which is
astandardimplementationforimagehashing.Ourtentativetests
showedthatthismethodhelpsfindsimilarobjectstoagoodextent
at modest cost. Nevertheless, we acknowledge the difficulty, if itis at all possible, of finding semantically similar objects through
aunifiedandcost-efficientapproach.Indeed,imagehashinguses
pixel-level similarityinsteadofreflectingonthe meaning ofeach
object instance. We leave for future work to explore practical tech-
niques to comprehend the semantic information of each object
instance and refine object selection at this step.
Object Image Resizing. There is a relationship between object
sizeanditsâ€œdistanceâ€intheimage.Atinyobjectcanbeâ€œfurther
awayâ€ and challenging to identify for even human eyes, and is
thusnotconsideredinthiswork.Hence,beforeinsertingaselectedobjectintoabackgroundimage
ğ‘–,weadjusttheobjectsizetomatch
thatoftheexistingobjectsinimage ğ‘–.Weresizetheobjectimage
to the average size of objects in ğ‘–that belong to the same category.
Also,asnotatedinSec. 3,besidesresizing,wedonotâ€œtransformâ€
objects(rotation,blurring,etc.)topreserverealismatourbesteffort.
4.3 Object Insertion
After selecting proper objects, we then seek proper locations on
the background image for insertion. As discussed in Sec. 3.3, the
softwareengineering(SE)communitytransformsentireimagesfor
testing, while the computer vision (CV) community primarily con-
cernswiththevisualappearanceoftheinsertedobject,ratherthan
theâ€œbackgroundâ€intowhichtheobjectisplaced[ 23,36,74].Sev-
eral studies have attempted to infer reasonable insertion locations
usingstatisticalmethodssuchasprobabilisticgrammarmodelsand
haveonlyappliedthemto imagesofindoorscenes[63].However,
building a generalized model for arbitrary scenes, if at all possible,
ishighly challenginginthisresearch,wherelarge-scalesynthetic
images are required to reveal erroneous OD results.
Giventhegeneraldifficultyofleveragingheavy-weightstatis-
tical methods to infer â€œoptimalâ€ insertion locations, we propose
lightweightstrategies.Inthissection,westartbyconductingem-
pirical studies on locations where insertion can presumably trigger
OD defects. Then, motivated by delta debugging used in testing

	

m1=y1+

n1=x1+

m2=m1+

n2=n1+


Figure 6: The â€œguided insertionâ€ strategy to insert a â€œbird.â€
Theblue region is symmetrical and centered on the larger
â€œelephant.â€
traditional software [ 83], we generate more synthetic images by
progressively relocating inserted objects on background images.
DeterminingObjectInsertionLocations. Ourpreliminarystud-
ies show that inserting objects close toexisting objects in an image
(referredtoas guidedinsertion laterinthispaper)islikelytotrigger
erroneous predictions. This section presents empirical results to
supportourobservation.Tosetupthestudy,werandomlyselected
50imagesfromtheCOCOimageset[ 45]andtentativelyinserteda
â€œbirdâ€image.WetestedeightpopularODsandshowtheevaluation
results (descriptions of these ODs can be found in Table 2).
Weadopttwoinsertionschemes: randominsertion andguidedin-
sertion. Guided insertion works by randomly selecting one existing
objectfromthebackgroundandinsertingextraobjects closetoit.
As shown in Fig. 6, after randomly selecting one elephant (e.g., the
largerone)onthebackgroundanddecidingtouseaâ€œbirdâ€object,
we create a blueregion that is symmetrical andcenteredon the
selectedelephant.3Werandomlyselectlocationswithinthe blue
regionasthecentroidoftheâ€œbirdâ€.Oursamplingguaranteesthat
the â€œbirdâ€ will not overlap with the larger â€œelephant.â€ Moreover,overlapping with any other objects is notallowed either; when-
ever the â€œbirdâ€ is sampled over existing objects in ğ‘–we discard the
synthetic image and resample.
Incontrast,therandominsertionschemeimplementsasimple
strategyinwhichobject ğ‘œisplacedrandomlyonthebackground.
Again,wedisallowoverlappingof ğ‘œwithexistingobjectsandre-
samplewhenever overlappingoccurs. Additionally,for eachback-
ground image ğ‘–withğ‘existing objects, we perform 10 Ã—ğ‘guided
orrandominsertions. Wereporttheerroneous ODoutputsfound
w.r.t. these two setups as follows:
OD(See Table 2for #Errors Found By #Errors Found By #Synthetic
Descriptions) RandomInsertion Guided Insertion Images
AmazonRekognintion 232 432 2,270
Microsoft Azure Vision 76 167 1,190
IBMVision 39 86 850
Google AutoML Vision 365 461 1,570
SSDMobilenet 45 112 1,290
SSDInception 185 310 1,510
RCNNResnet 218 381 2,980
RCNNInception 294 580 3,190
Total 1,454 2,529 14,850
3When deciding the size of the blueregion, we find that if it is too narrow, the search
space of â€œobject insertion locationsâ€ are too small and undesired. Similarly, if the blue
regionistoowide,itmayoverlapwithotherexistingobjects.Sincewedisallowthe
inserted object to overlap with existing objects, higher chance of overlapping leads to
lower chance of finding proper insertion locations, and is thus undesired as well.
1058	

!&
"!&
	

	




#"!& 

Figure 7: Move object instance image ğ‘œtoward the centroid.
Since ODs (introduced soon inTable 2) identify different numbers
of objects from each image, we synthesize different amount of
imagesfortesting.Theresultsshowthattheguidedsettingnotably
outperforms the first setting. This is consistent with our intuition;
by inserting images near the local region of existing objects, the
inserted images may disturb the regions or grids used for object
recognition and thus cause failures in ODs.4As a result, MetaOD
is configured with the guided insertion strategy.
ObjectRelocation. Whiletheproposedmethodsprovidepractical
guidelines on object insertion, â€œguided insertionâ€ primarily focuses
onlocations closetoexistingobjectsandmaymissopportunitiesfor
objectinsertioninotherimageregions.Wefurtherproposetech-
niquestoidentifyadditionallocationsforobjectinsertion,whilestill
maintaining the â€œrealismâ€ of generated images insofar as possible.
To accomplish this, we first compute the centroidof objects in the
backgroundimage;then,motivatedbytheuseofdeltadebugging
for conventional software [ 83], the inserted object is progressively
relocated toward the centroid to generate visually more diverse
images while retaining to cause prediction errors.
TheprocedureisillustratedinFig. 7,whereourrelocationscheme
isimplementedtoexplorelocationsclosesttothecentroid.Starting
from aninsertionlocation foundby theguided-insertion strategy
that can trigger OD failures, we relocate the inserted â€œbirdâ€ to the
centroidofexistingobjectsonthebackground.Ifnoerrorcanbe
provokedregardingthenewlysyntheticimage,wejumpbackto
the middle and recheck the OD. In case this time prediction errors
dooccur,wesearchforwarduntiltheâ€œbirdâ€becomestoocloseto
(1)thecentroid,(2)theprevioussuccessfulinsertion(i.e.,triggering
predictionerrors)withthelongestdistancefromthestartingpoint,
or (3) the starting point itself. Again, for this step, we disallow any
overlap between the inserted object and existing objects on the
background: whenever overlapping occurs, we jump back as well.
It is worth mentioning that while the prototype implementation
of MetaODisequippedtouseâ€œcentroidâ€ astheexplorationdesti-
nation, any locations could be configured at this step to synthesize
diverse and realistic images with respect to user requirements.
5 Implementation
MetaOD is implemented in Python in approximately 3,600 lines of
code (see our released codebase at [ 9]). The object extraction mod-
ule of MetaOD is implemented by extending a popular instance
segmentationmodule,YOLACT[ 7,14].WeextendedYOLACTby
using the instance mask to crop the input image and extract object
4TheimplementationdetailsofthefirstfourcommercialODsarenotdisclosed.See
further discussions in Sec. 6.instanceimages.YOLACTisbuiltwithPytorch(ver.1.0.1),andcon-
tains a model pretrained with a de facto object detection dataset,COCO [
45]. This dataset contains objects with approximately 90
labels, and we use the pretrained model to perform instance seg-
mentation.Asaforementioned,onedesirablefeatureofYOLACT
is that it performs instance segmentation rapidly â€” indeed, all the
instancesegmentationtasksarelaunchedononeNvidiaGeForce
GTX 1070 GPU with promising processing time (see Sec. 6).
6 Evaluation
Table2lists the ODs used in the evaluation (the â€œSpeedâ€ and
â€œCOCO mAPâ€ of four Tensorflow pre-trained models are disclosed
by Google [ 3],andâ€œSpeed â€ of four commercial APIs are estimated
by us). We use four commercial OD APIs provided by industrygiants for the evaluation [
1,2,4,6]. Python scripts are written
tointeractwiththeseremoteservicesandretrievetheprediction
results (in JSON format). To the best of our knowledge, the ODmodels used within these commercial services are not disclosed;single-stage models are presumably employed given their rapid
predictionspeed(Sec. 2).
GooglealsosupportsdirectlydeployingitsTensorFlowODAPIs
on Google Cloud [ 3] and provides the flexibility to choose differ-
ent models pretrained on the COCO dataset [ 45]. We follow the
official tutorial to setup TensorFlow OD on Google Cloud [ 3], and
from a total of five pretrained models suggested in the tutorial,
wechoosefourmodels(i.e.,â€œTensorflowâ€modelsinTable 2)and
exclude another pre-trained model, RFCN Resnet. RCNN Inception
ResNetmodel[ 34]yieldsthebestaccuracy(mAP37)buthasthe
slowestspeed.WealsochoseanotherRCNNmodel[ 80]andtwo
SSDmodels[ 37,47]thatexhibitmediumpr edictionspeedandgood
accuracy. The tutorial reports that RFCN Resnet has a very similar
accuracy with faster RCNN Resnet, and therefore, we skip to eval-
uateRFCN Resnet.Asmentioned in Sec. 2,thetwoRCNN-based
modelshavetwo-stageregion-basedarchitectures,whiletheSSD
models have single-stage architectures that are much faster.
6.1 Evaluation Overview
Table1summarizestheevaluationresults.Toacquirethesedata,
we extracted object instances from 1,000 randomly selected images
from the COCO 2017 image set [ 45]. We then randomly selected
500 images from the same dataset as background images. From the
complete set of 1,000 images, MetaOD extracted a total of 5,843object instances clustered with respect to 79 different categories
(person, dog, etc.). As previously mentioned (Sec. 4.2), the object
refinement module of MetaOD sorts object images with respectto their size and eliminates 90% of the small object images; the
remaining10%oftheobjectimagesarekeptasinsertioncandidates.
Given an image ğ‘–, ODs can find different numbers of objects
fromğ‘–(the third column of Table 1reports the total number of
objects found by an OD). As discussed in Sec. 4.3, suppose that an
ODfinds ğ‘€objectsin ğ‘–,then,MetaODgenerates10 Ã—ğ‘€synthetic
images following the â€œguided-insertionâ€ strategy to test the OD.
When a test image ğ‘–/primetriggers prediction errors, that image is used
to generate additional test inputs following the â€œdelta-debuggingâ€-style procedure (Sec. 4.3). The total number of images synthesized
for each OD is reported in the second column of Table 1.
1059Table 1: Result overview. Due to the limited space, the detector names are simplified in this table and also in the rest of the
paper. Note that â€œProcessing Timeâ€ includes the prediction time of ODs.
OD #SyntheticImages #Detected Objects #Images Causing Detection Failures Processing Time (Hours) Total Cost (USD)
Amazon 38,939 3,750 6,060 (15.6%) 11.9 $21.5
Google 18,655 1,801 2,738 (14.7%) 13.0 $18.8
Microsoft 20,453 1,985 2,494 (12.2%) 3.2 free
IBM 13,280 1,290 1,515 (11.4%) 2.2 free
SSD Mobilenet 24,796 2,387 3,460 (14.0%) 62.5 $53.4
SSD Inception 29,072 2,806 3,988 (13.7%) 64.3 $54.9
RCNN Resnet 70,754 6,914 7,442 (10.5%) 164.8 $140.7
RCNN Inception 76,257 7,349 10,648 (14.0%) 290.8 $248.3
Total 292,206 28,282 38,345 (13.1%) 612.7 $537.6
Table 2: ODs evaluated in this research.
ODName SpeedCOCOmAP
Amazon Rekognintion API [1] fast N/A
Google AutoML Vision API [2] fast N/A
Microsoft Azure Vision API [4] fast N/A
IBMVision API [6] fast N/A
TensorFlow SSD Mobilenet [37] fast 21
TensorFlow SSD Inception [47] fast 24
TensorFlow faster RCNN Resnet [29] medium 32
TensorFlow faster RCNN Inception Resnet [72] slow 37
Thenumberofimagestriggeringpredictionerrors(notâ€œnumber
of bugs in the modelsâ€) is reported in the fourth column of Table 1.
Atleast10%ofthesyntheticimagestriggerederroneouspredictions
of the evaluated ODs. Since we randomly decide locations to in-
sert objects into background images (see Sec. 4.3), error-triggering
images are very unlikely identical. Like typical software testing
scenarios, not every test input can trigger a bug. Having 10-16%bug-triggering tests consistently across different models is very
promisingandshowsthatMetaODprovidesageneral,practical,
and consistent approach to finding OD errors. Overall, Table 1
shows that OD failures are a general concern, regardless of the
underlyingmodel.Moreover,whenamodeldetectsmoreobjects
in images, the number of images that can trigger failures also in-
creases.Thisisexpected,asrecallthat,foranimageof ğ‘€objects,
our â€œguided-insertionâ€ strategy generates 10 Ã—ğ‘€synthetic images
(Sec.4.3).Also,whiletheimplementationofcommercialAPIsare
unknown and are generally deemed as â€œfastâ€ (see Table 2), we sus-
pect thatthey are notusing similar models,since their prediction
capabilities are very different (cf. the third column in Table 1).
Processing Time. This evaluation was conducted on a machine
equipped with an Intel i7-8700 CPU and 16GB RAM. The instance
segmentationmodulerunsonasingleNvidiaGeForceGTX1070
GPU with CUDA 9.0. Table 1reports the processing time. Com-
mercialAPIs,particularlytheGoogleandMicrosoftservices,take
less time for prediction than the TensorFlow pretrained models.
Althoughtheimplementationdetailsoftheseremoteservicesare
not disclosed, from the results, we can assume that the commercial
remoteservicespresumablyleveragehighlyoptimizedsingle-stage
OD models that are faster but usually find fewer objects in images.
Financial Cost. Enabledbymoderncloudcomputinginfrastruc-
tures, all of these ODs are designed as â€œpay-as-you-goâ€ models:
users are charged based on how many queries they send to the ser-
vices(forthefirstfourservices)orhowmanycomputingresources
they use (for the TensorFlow services). We report the amount in
Figure8:Efficiencyofobjectrelocation.Recallweleveragea
â€œdelta-debuggingâ€-style method to relocate the inserted ob-
jectstowardthecentroid(Fig. 7).X-axisreportsthathowfar
theinsertedobjectcanproceedtowardthecentroid:100%in-dicates that the object is placed at the centroid.
termsof USDthatwearechargedbytheseservicesinTable 1.Due
to erroneous behavior, some queries were indeed wasted. More
importantly, given that commercial services have been adopted in
supporting critical computer vision applications (e.g., surveillance
cameras),weenvisionreal-worldscenarioswheretheprediction
errors can cause financial loss or fatal errors.
6.2 Efficiency of Object Relocation
AsdiscussedinSec. 4.3,inspiredbydeltadebugging,wepropose
techniques to mutate synthetic images by progressively moving an
inserted object that triggers erroneous predictions toward the cen-
troid of objectsin the background image. We preserve the realism
of synthetic images at our best effort by placing the inserted object
into a realistic position, while finding more insertion locations and
augmenting the visual diversity of the synthetic images.
WereportthebreakdownofsyntheticimagescausingODerrors
inTable3.Thesecondcolumnreportsthenumberofimagestrig-
geringpredictionerrorsthataresynthesizedbyinsertingobjects
into the background image, while the third column reports the
number of images triggering prediction errors and are synthesized
by relocating inserted objects toward the centroid. Since the same
object could be inserted at different positions on a background
image,andthenreachingtothe samecentroid,wealsomeasurethe
uniquenumberofsyntheticimagesatthisstep(thelastcolumn).
As shown in Table 3, the object relocation step successfully finds a
1060Table 3: Breakdown of images causing OD failures. By
addingthesecondandthelastcolumns,wegettheâ€œ#Images
Causing Detection Failureâ€ column in Table 1.
OD#Synthetic #Synthetic #Unique Synthetic
ImagesWith ImagesWith ImagesWith
Inserted Obj. Relocated Obj. Relocated Obj.
Amazon 4,621 2,501 1,439
Google 2,093 853 645
Microsoft 1,891 742 603
IBM 1,135 481 380
SSDMobilenet 2,534 1,089 926
SSDInception 2,976 1,192 1,012
RCNNResnet 5,828 2,544 1,614
RCNNInception 7,881 3,422 2,767
large number of unique images retaining prediction errors. Among
atotalof28,959syntheticimagescausingpredictionerrors,9,386
(32.4%) uniqueimages arecreated viaobject relocation.Moreover,
wereporttheaveragedistance(intermsofpercentage)bywhich
the inserted object can be relocated in Fig. 8; we consider arriving
atthecentroidas100%andstayingatthestartingpositionas0%.
Note that Fig. 8has excluded all the â€œ0%â€ cases, where objects stay
at the starting positions. As shown in Fig. 8, 21.9% of objects on
averagecanbeplacedat thecentroidwhileretainingerrors,and
40.2%of objectsare relocatedby atleast40% ofthe distances.The
overallresultsaspromising,illustratingthataconsiderablenum-
berofsyntheticimagescouldbegeneratedthatretainprediction
failures, and also make the images visually more diverse.
6.3 Naturalness of Synthetic Images
While the â€œnaturalnessâ€ of a synthetic image may be subjective, as
noted by existing research, natural images are deemed to have cer-
tainstatisticalregularities[ 38,51,52].Therefore,followingthecom-
monlyadoptedconventioninthecomputervisionliterature[ 52],
the â€œnaturalnessâ€ of synthetic images is measured by first comput-
ing a histogram of oriented gradients (HOG [ 19]) of both synthetic
imagesandtheir correspondingbackgroundimages,andthencom-putingtheintersectionofthesetwoHOGs.HOGisapopularmetric
extracting distribution (histograms) of directions of gradients as
â€œfeaturesâ€ofanimage.Bysummarizingthemagnitudeofgradients,
this metric captures abrupt intensity changes in theimage (object
edges, object corners, etc.), and therefore is usually effective for
comprehending high-level representations of images with multiple
objects. In contrast, pixel-level similarity metrics (Sec. 4.2) lever-
agedinMetaODfocusonsingleobjectinstancecomparison,and
arenotapplicableinthisevaluation.Consistentwithpreviousre-
search[52],thecomparisonoutput(i.e.,HOGintersection),avalue
between0and1,illustratesthenaturalnessofsyntheticimages.We
provide discussions regarding image naturalness in Sec. 7.
Table4reports HOG intersection rates (second column) by com-
paringsyntheticimageswithinsertedobjectsandtheircorrespond-
ing background images, and HOG intersection rates (third column)
by comparing synthetic images with relocated objects and their
corresponding backgrounds. Consistent with our intuition, all syn-
theticimageshavehighly similarHOGregularitieswiththeircor-
responding backgrounds, and are deemed â€œnaturalâ€ (in contrast,
we report that the HOG intersection rate of two randomly-selected
imagesfromtheCOCOdatasetisbelow50.0%).Also,whilemostTable4:Naturalnessevaluationw.r.t.averageHOGintersec-
tion(highervalueisbetter).Wereportevaluationresultsofbothsyntheticimageswithinsertedobjects(secondcolumn)and relocated objects (third column).
ODAverage HOG Intersection Average HOG Intersection
Rate(%) for Inserted Obj. Rate(%) for Relocated Obj.
Amazon 98.9 98.8
Google 98.1 98.0
Microsoft 98.8 98.7
IBM 98.5 98.5
SSDMobilenet 98.7 98.7
SSDInception 98.6 98.6
RCNNResnet 98.9 98.9
RCNNInception 98.9 98.9
synthetic images with relocated objects have HOG intersectionsidentical to those of the synthetic images with inserted objects,
therearethreecasesforwhichthesyntheticimageswithrelocated
objectsexhibitaslightlylowerrate.Intuitively,relocationgener-
ates visually more diverse images and can potentially lead to lower
â€œsimilarityâ€comparingtotheircorrespondingbackgroundimages.
6.4 Retraining with Error-Triggering Images
To capitalize on the synthetic images that triggered prediction
failures, we show that such synthetic images can be used to retrain
models and substantially improve their performances. We use a
popularautonomous-drivingdataset, BerkeleyDeepDrive[ 5,82],
for this evaluation. This dataset contains images that depict real-
time driving experiences under different weather conditions and at
varioustimes.Experimentsinthissection(i.e.,modelretraining)are
conductedonaserverequippedwithanIntelXeonCPUE5-2680
with 256 GB of RAM and eight Nvidia GeForce RTX 2080 GPUs.
We downloaded the SSD MobileNet object detection model pre-
trainedbyTensorFlowandretrainedthemodel(withTensorflow
ver.1.14.0)byusing900imagesannotatedwith10commoncate-
goriesfortrafficscenesfromtheDeepDrivetrainingset.WeactuallyimitatedhowODmodelsarecustomizedandusedinpractice;based
on transfer learning [ 54], pretrained models are adapted to simi-
lar tasks by fine-tuning the model parameters on a new dataset.
At this step, we reuse the default configuration shipped with the
MobileNet pretrained model; the batch size is 48 which means the
wholetrainingsetwillbeprocessedoncewithin19steps.Weset
up three retraining strategies (Config 1âˆ’3) as follows:
â€¢WestartbyretrainingtheMobileNetmodelwith900images
for 200K steps (200K is the default setting in the modelâ€™sconfiguration) and exporting the retrained model
ğ‘š0.W e
alsoformanevaluationsetbyrandomlyselecting100images
from the DeepDrive evaluation set.
â€¢We then use MetaOD to generate new synthetic images
from the 900 images and collect synthetic images that cause
prediction failures of ğ‘š0. This step generates 18,707 images
(denoted as Iâˆ—) triggering prediction errors.
â€¢Config1: starting from ğ‘š0, we resume retraining with the
900 images for another 10K steps.
â€¢Config2:startingfrom ğ‘š0,weextendtheoriginaltrainingset
of900imageswith900imagesrandomlyselectedfrom Iâˆ—,
andresumethemodelretrainingwiththese1,800imagesfor
1061another10Ksteps.Tolabeleachsyntheticimage,wereuse
the label of its reference input image (see Sec. 3.1).
â€¢We also use MetaOD to generate another set of 900 images
(denotedas I).Wedonotcheckwhethertheseimagescan
trigger prediction failures or not.
â€¢Config3: starting from ğ‘š0, we extend the training set of 900
images with 900 images in I, and resume the model retrain-
ing for another 10K steps. Again, to label each synthetic
image, the label of its reference input is reused.
Duringtheretrainingofintotal210K steps,wemeasurethetotal
loss and mAP score regarding the evaluation set of 100 images.
Figure9:Smoothedtotalloss(firstdiagram)andmAPscores
(second diagram) during model retraining. Due to limited
space, we show performance starting from 130K steps.
AsreportedinFig. 9,forthelast10Ksteps, Config1showsconsis-
tenttrendingcomparedtothefirst200Ksteps. Config2outperforms
the other two by having lower total loss. Moreover, the mAP score
ofConfig2clearly outperforms those of the other configurations.
Config3exhibits a slightly better total loss decrease than that of
Config1, but yields an even lower mAP score (which may be due
to overfitting). We report that the average mAP scores of the three
configurations within the last 10K steps are as follows:
Config1Config2Config3
mAP 9.3 10.5 8.6
Asthetableshows,modelperformanceisincreasedbyretraining
withthesyntheticimagesofthepredictionerrors.Notethataccord-
ing to OD surveys (e.g., Table Two in [ 86] and Table Seven in [ 46]),
onepointmAPscoreincreaseissignificant.Overall,weinterpret
the evaluation result as promising: the failure-aware retraining
demonstrated in this section sheds light on practical usages of the
modelpredictionerrorsfoundbyMetaODandprovidespromising
directions to improve model accuracy.
We acknowledge that the evaluation, while being fair, may not
illustrate the best practice to promote model performance; Fig. 9indicates that the â€œsweet spotâ€ might be around 8K steps of re-
training (where the mAP score is approximately 10.7). Overall, we
consider that providing guidelines of best practice is beyond the
scope of this research, but the reported results have illustrated
the potential. Additionally, images are synthesized from the exist-
ingtrainingset;inotherwords,wedo notneednewrealimages.
Overall, â€œfailure-awareâ€ retraining is orthogonal to standard model
retrainingtechniquesandcanpotentiallybeorchestratedtogether.
7 Discussion and Future Work
In this paper, we presented the design, implementation and evalua-
tion of MetaOD, a systematic workflow for automatically testing
the erroneous behaviors of object detection systems. The proposed
techniques can be adopted to promote object detector training
and to motivate this emerging lineof research. In this section, we
presentadiscussionandseveralpotentiallypromisingdirections
for future research.
ComparisonwithWorkintheCVCommunity. Parallel to SE
communityâ€™s efforts on testing deep learning systems, the CV com-
munitygeneratessyntheticinputsbymutatingrealimagestotrain
DNNs. We compare and illustrate the novelty of MetaOD with
related CV research along several aspects:
Blackboxvs.Whitebox. Most existing CV research considers a
â€œwhite-boxâ€ setting (e.g., [ 59,68]). Such efforts either require an
in-depthunderstandingofthemodelstructuretoadaptivelysyn-
thesizeinputs[ 68],orusethehiddenlayersofDNNsandcomputed
gradientstoguideinputsynthesis[ 59].Asaforementioned,wecon-
siderblackboxsettingforsoftwaretestingandintroducesMetaOD
to effectively test commercial remote OD models.
TrainingwithSyntheticInputsvs.Re-trainingwithBug-triggering
SyntheticInputs.Toourknowledge,relatedCVresearchdirectly
uses synthetic inputs for model training. Contrarily, MetaOD sug-
gests a novel failure-aware model retraining scheme (Sec. 6.4)t o
effectively improve model accuracy, which sheds light on future
work to continue testing the â€œre-trainedâ€ model, i.e., the whole
process would loop to iteratively re-train the model. We expectthe model accuracy to further improve until reaching saturation.
As evaluated in Fig. 9, model re-training with arbitrarily generated
syntheticimagesmayleadtodecreasedmodelaccuracy,whilere-
training with bug-triggering synthetic inputs leads to significantly
improvedaccuracy.Thisinterestingobservationsuggestsfurther
research opportunities for both the CV and SE communities.
Fine-grainedModeling/TuningonSyntheticImageforTraining
vs.GenericFrameworktoSynthesizeImagesforTesting. As Sec. 8
will review, existing CV research mostly performs heavyweight,
fine-grained modeling to generate synthetic images, e.g., specif-ically modeling motion trajectory to synthetic images and trainsurveillance cameras [
13]. These works usually focus on specific
applicationdomains,andleverage domainknowledge tofine-tune
and optimize the synthetic images with statistical tools. We have a
differentdesigngoalofproposingageneralframeworktoefficiently
producealargeamountofqualityinputsfortesting.MetaODis,
in general, agnostic of image â€œsemanticsâ€ (except labels of existing
objectsintheimage)andthereforecanbemoreefficientandrobust.
Novelty w.r.t. DNN Testing Work in the SE Community. As
discussedinSec. 8,mostexistingtestingworkintheSEcommunity
1062focuses on image classification models (e.g., [ 58,73]), or the under-
lyinginfrastructuresofTensorFlow/PyTorch(e.g.,[ 21,39,61,85]).
Toourknowledge,nopriorworkfocusesondesigningageneral,
effective pipeline to test OD models, another class of fundamental
models usedin manyreal-worldcritical applications. MetaODmu-
tatesandobservesthedetectionofindividualobjectsinanimage,
while existing research on testing image classification performs
mostlywholeimage-wisemutations(e.g.,addingfoggyandrainy
conditions). Some of these transformations are not applicable to
ourscenario(discussedinSec. 3.3),andourapproachisgenerally
orthogonal to these whole image-wise mutations.
Wealso demonstratethefeasibilityofâ€œfailure-drivenretrainingâ€
(Sec.6.4)withnotablyimprovedmodelaccuracy.Thisevaluation
initializes a promising step toward addressing a typical concern in
the SE community on â€œhow to use findings of DNN testingâ€, which
has not been systematically explored by previous works.
Measuring â€œNaturalnessâ€ of Synthetic Images. As noted in
Sec.6.3, we follow conventions in the CV community [ 52]t oc o m -
paresyntheticimageswithnaturalimagesw.r.t.HOGasawayof
measuring the â€œnaturalnessâ€. Nevertheless, as discussed in Sec. 4.3,
itischallenging,ifnotimpossible,tounderstandtheâ€œsemanticsâ€of
an object and make it fully consistent with arbitrary backgrounds
(i.e., more â€œnaturalâ€). For instance, the traffic image in Fig. 3can be
more natural, if the perspective of the inserted car slightly turns
left rather than entering the sidewalk. To our knowledge, CV com-
munity is exploring statistical tools to indepthly comprehend only
a few specific scenes and objects (e.g., mutating human gestures
within indoor scenes [ 63]). In contrast, this work introduces a gen-
eral, efficient pipeline for arbitrary images.Reason to Preserve â€œNaturalnessâ€ of Synthetic Images.
We
notethat â€œrealism/naturalnessâ€of syntheticimagesdoes nothave
directinfluenceontesting.Randomlymutatingpixelstogenerate
â€œfuzzyâ€andâ€œunrealâ€imagesasthetestinputs,whicharechallenging
for human eyes to recognize objects, could also be used for stress-
testing ODs.
However, we argue that preserving the â€œrealismâ€ of synthetic
imagesatourbesteffortwhendesigningtheworkflowof MetaOD
is indeed beneficial. First, as noted in Sec. 3.3, we are not testing
extreme cases to stress ODs (not like a typical fuzz testing setting).
Stress testing of DNNs would be different. Second, synthesizing
moreâ€œrealisticâ€imagesfacilitatethepracticalusageof MetaOD.
As mentioned in Sec. 3, we aim to augment the realism to a certain
extent,suchthatsyntheticimagestriggeringerroneouspredictions
could imply real-life failures of OD systems, leading to confusions
duringthedailyusage.Also,wearguethatusingrealisticanderror-
triggering images to retrainOD models (Sec. 6.4) deems a more
reasonableapproachthanusingarbitrarilymutatederror-triggering
inputs.
8 Related Work
Testing ofDeepLearning Systems. Testingtechniquesforcon-
ventionalsoftwarehavebeenrecentlyappliedfordeeplearningsys-
tems,includingfuzztesting[ 53,81],mutationtesting[ 49,78],meta-
morphictesting[ 22,73,84],andalsosymbolicexecution[ 31,70,71].
The majority of existing work focuses on image classification and
itsadoptiononautonomousdrivingsystems[ 58,73,84,88].NLPmodels,aswellastypicaldownstreamapplications(e.g.,machine
translation),havealsobeentestedusingvarioustestingschemes[ 27,
35,50,75]. We note that previous work on testing deep learning
systems often adopts a â€œdifferential testingâ€ scheme [ 58,73]. How-
ever, OD models can usually recognize different number of objects
from an image (due to the model capability), leading to the gen-
eral challenge for cross comparison. For instance, when analyzing
the â€œbeachâ€ sample image (https://git.io/fjFgI) provided by Tensor-
Flowdevelopers,AmazonRekognitionAPI[ 1]locates13objects
while Microsoft Azure API [ 4] locates only four. In contrast, our
work adopts metamorphic testing as an effective and adaptive test-
ing strategy to reveal defects in these commercial ODs. Regarding
testingoracleselection,neuroncoverage[ 58,73]andotherfiner-
grained coverage metrics have been proposed [ 39,48]. Also, in
addition to the deep learning models, the underlying infrastruc-
tures(e.g.,TensorFlow[ 10]andPyTorch[ 55])havealsobeentested
to find implementation bugs [21, 61,85].
Data Augmentation for OD Model Training. Inparalleltothe
SE communityâ€™s progress in testing DNNs, data augmentation,
which generates synthetic inputs by mutating real images, is an
important technique to train DNNs. To train deep learning mod-els (e.g., for object detection), augmentation methods vary from
geometricaltransformationssuchashorizontalflippingtocolorper-
turbations to adding noise to an image (e.g., mimic severe weather
conditions)[ 23,25,26,28,36,62,74,87].Modelaccuracycanusu-
ally be improved by including such synthetic images into training
data [28].
Most existing work prioritizes localrather than global consis-
tency when augmenting images. For instance, when inserting new
objects into training images, they focus more on the realism of
insertedobjectsthanonthecontextsurrounding.Manysynthetic
images are unrealistic from a global point of view, such as putting
a car on the table [ 74]. A few studies leverage heavyweight sta-
tistics methods to infer a â€œrealisticâ€ location for object insertion;they assumes a â€œwhite-boxâ€ setting and can handle only a few
domain-specific scenes [ 63]. Our work proposes a lightweight and
systematic new focus to promote synthetic images by considering
both local and global realism. We take a â€œblack-boxâ€ setting that
facilitatesthetestingofcommercialremoteODs.Inaddition,our
testing focus enables â€œfailure-awareâ€ model retraining (Sec. 6.4),
which effectively improves the model accuracy.
9 Conclusion
ODsystemshavebeencommonlyusedinreal-worldscenessuchasbuildingautomateddrivingcars.Thispaperhasintroducedanovel
metamorphic testing approach toward reliable ODs. Evaluation
results are promising â€” MetaOD can find thousands of prediction
errorsfromcommercialAPIsandODmodels.Wealsoshowthat
thegeneratedimagescanbeusedforretrainingandsubstantially
improving OD model accuracy.
Acknowledgments
WethanktheanonymousASEreviewersfortheirvaluablefeedback.
We thank Yuyan Bao, Wei Luo, and Jiang Zhang for helping and
commenting on this project.
1063References
[1] 2018. Amazon Rekognition. https://aws.amazon.com/rekognition/.
[2]2018. Google Cloud Detecting Objects. https://cloud.google.com/vision/docs/
detecting-objects.
[3]2018. Tensorflow Object Detection API. https://cloud.google.com/solutions/
creating-object-detection-application-tensorflow.
[4]2019. Azure Computer Vision API. https://docs.microsoft.com/en-us/azure/
cognitive-services/computer-vision/.
[5] 2019. berkeley DeepDrive. https://bdd-data.berkeley.edu/.
[6]2019. IBM Vision API. https://developer.ibm.com/exchanges/models/all/
max-object-detector/.
[7] 2019. YOLACT. https://github.com/dbolya/yolact.
[8]2020. Dropbox folder of all erroneous detection results found by
MetaOD. https://www.dropbox.com/sh/ivpprt96nycokd9/AAAZ_
3D8TVAF25L90Z9AqpRya?dl=0.
[9] 2020. MetaOD Codebase. https://github.com/MetaOD/MetaOD.
[10]MartÃ­nAbadi,PaulBarham,JianminChen,ZhifengChen,AndyDavis,Jeffrey
Dean,MatthieuDevin,SanjayGhemawat,GeoffreyIrving,MichaelIsard,Man-
junathKudlur,JoshLevenberg,RajatMonga,SherryMoore,DerekG.Murray,
Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-scale Machine
Learning. In Proceedings of the 12th USENIX Conference on Operating Systems
DesignandImplementation (OSDIâ€™16).USENIXAssociation, Berkeley,CA,USA,
265â€“283. http://dl.acm.org/citation.cfm?id=3026877.3026899
[11]RajaBenAbdessalem,AnnibalePanichella,ShivaNejati,LionelC.Briand,and
Thomas Stifter. 2018. Testing Autonomous Cars for Feature Interaction Failures
Using Many-objective Search. In Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering (ASE 2018). 143â€“154.
[12]Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-ShinnKu, and Anh Nguyen. 2019. Strike (with) a Pose: Neural Networks Are Easily
Fooled by Strange Poses of Familiar Objects (CVPR 2019).
[13]JamesBlack,TimEllis,andPaulRosin.2003. ANovelMethodforVideoTrackingPerformanceEvaluation.In InJointIEEEInt.WorkshoponVisualSurveillanceand
Performance Evaluation of Tracking and Surveillance (VS-PETS. 125â€“132.
[14]Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. 2019. YOLACT: Real-
time Instance Segmentation. arXiv preprint arXiv:1904.02689 (2019).
[15]NealE.Boudette.2017.Teslaâ€™sSelf-DrivingSystemClearedinDeadlyCrash. https:
//www.nytimes.com/2017/01/19/business/tesla-model-s-autopilot-fatal-crash.
html.
[16]TsongYChen,ShingCCheung,andShiuMingYiu.1998. Metamorphictesting:a
newapproach forgeneratingnexttestcases. TechnicalReport.Technical Report
HKUST-CS98-01, Department of Computer Science, Hong Kong ....
[17]TsongYuehChen,Fei-ChingKuo,HuaiLiu,Pak-LokPoon,DaveTowey,T.H.
Tse,andZhiQuanZhou.2018. MetamorphicTesting:AReviewofChallenges
and Opportunities. ACM Comput. Surv. 51, 1, Article 4 (Jan. 2018), 27 pages.
[18]Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. 2016. R-fcn: Object detection viaregion-based fully convolutional networks. In Advances in neural information
processing systems. 379â€“387.
[19]NavneetDalalandBillTriggs.2005. Histogramsoforientedgradientsforhuman
detection.
[20]Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. 2019. Deep-
Stellar:Model-basedQuantitativeAnalysisofStatefulDeepLearningSystems.InProceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineeringConference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2019). 477â€“487.
[21]SaikatDutta,OwolabiLegunsen,ZixinHuang,andSasaMisailovic.2018. Testing
ProbabilisticProgrammingSystems.In Proceedingsofthe201826thACMJoint
Meeting on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE 2018).
[22]Anurag Dwarakanath, Manish Ahuja, Samarth Sikand, Raghotham M. Rao, R. P.
Jagadeesh Chandra Bose, Neville Dubash, and Sanjay Podder. 2018. Identify-
ing Implementation Bugs in Machine Learning Based Image Classifiers UsingMetamorphic Testing. In Proceedings of the 27th ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA 2018). 118â€“128.
[23]Debidatta Dwibedi, Ishan Misra, and Martial Hebert. 2017. Cut, paste and learn:
Surprisingly easy synthesis for instance detection. In Proceedings of the IEEE
International Conference on Computer Vision. 1301â€“1310.
[24]Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and
Andrew Zisserman. 2010. The pascal visual object classes (voc) challenge. Inter-
national journal of computer vision 88, 2 (2010), 303â€“338.
[25]Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and HayitGreenspan. 2018. Synthetic data augmentation using GAN for improved liverlesion classification. In 2018 IEEE 15th International Symposium on Biomedical
Imaging (ISBI 2018). IEEE, 289â€“293.
[26]Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. 2016. Virtual
worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 4340â€“4349.[27]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness testing:
testing software for discrimination. In ACM ESEC/FSE. ACM, 498â€“510.
[28]GeorgiosGeorgakis,ArsalanMousavian,AlexanderCBerg,andJanaKosecka.
2017. Synthesizing training data for object detection in indoor scenes. arXiv
preprint arXiv:1702.07836 (2017).
[29] Ross B. Girshick. 2015. Fast R-CNN. ICCV.
[30]Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Richfeature hierarchies for accurate object detection and semantic segmentation.
CVPR(2014).
[31]DivyaGopinath,KaiyuanWang,MengshiZhang,CorinaSPasareanu,andSarfraz
Khurshid. 2018. Symbolic execution for deep neural networks. arXiv preprint
arXiv:1807.10439 (2018).
[32]David Grossman. 2018. Uber Self-Driving Car Kills Pedestrian in Ari-
zona.https://www.popularmechanics.com/technology/infrastructure/a19482100/
uber-self-driving-car-kills-pedestrian-in-arizona/.
[33]Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. 2017. MaskR-CNN. In Proceedings of the IEEE international conference on computer vision.
2961â€“2969.
[34]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016. Deepresidual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770â€“778.
[35]Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-invariant testing for
machine translation. (2020).
[36]Stefan Hinterstoisser, VincentLepetit, PaulWohlhart,and Kurt Konolige.2018.
On pre-trained image features and synthetic images for deep learning. In Pro-
ceedings of the European Conference on Computer Vision (ECCV). 0â€“0.
[37]Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).
[38]Jinggang Huang and David Mumford. 1999. Statistics of natural images andmodels. In Proceedings. 1999 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (Cat. No PR00149), Vol. 1. IEEE, 541â€“547.
[39]JinhanKim,RobertFeldt,andShinYoo.2019.GuidingDeepLearningSystemTest-
ing Using Surprise Adequacy. In Proceedings of the 41st International Conference
on Software Engineering (ICSE â€™19). 1039â€“1049.
[40]Neal Krawetz. 2011. Average Hash. http://www.hackerfactor.com/blog/index.
php?/archives/432-Looks-Like-It.html.
[41]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017. ImageNet Classi-
ficationwithDeepConvolutionalNeuralNetworks. Commun.ACM 60,6(May
2017), 84â€“90.
[42]AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,IvanKrasin,Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, et al .
2018. The open images dataset v4: Unified image classification, object detection,
andvisualrelationshipdetectionatscale. arXivpreprintarXiv:1811.00982 (2018).
[43]Damon Lavrinc. 2018. This Is How Bad Self-
Driving Cars Suck In The Rain. https://jalopnik.com/
this-is-how-bad-self-driving-cars-suck-in-the-rain-1666268433.
[44]Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei. 2017. Fully con-
volutional instance-aware semantic segmentation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2359â€“2367.
[45]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,Deva
Ramanan,PiotrDollÃ¡r,andCLawrenceZitnick.2014. MicrosoftCOCO:Commonobjects in context. In European conference on computer vision. Springer, 740â€“755.
[46]Li Liu, Wanli Ouyang, Xiaogang Wang, Paul W. Fieguth, Jie Chen, Xinwang Liu,
and Matti PietikÃ¤inen. 2018. Deep Learning for Generic Object Detection: A
Survey.CoRRabs/1809.02165 (2018).
[47]Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander C Berg. 2016. SSD: Single shot multibox detector.
InEuropean conference on computer vision. Springer, 21â€“37.
[48]Lei Ma, Felix Juefei-Xu, Jiyuan Sun, Chunyang Chen, Ting Su, Fuyuan Zhang,
MinhuiXue,BoLi,LiLi,YangLiu,etal .[n.d.]. DeepGauge:Comprehensiveand
multi-granularitytestingcriteriaforgaugingtherobustnessofdeeplearningsys-tems.InProceedingsofthe33rdACM/IEEEInternationalConferenceonAutomated
Software Engineering (ASE 2018).
[49]Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao
Xie, Li Li, Yang Liu, Jianjun Zhao, et al .2018. Deepmutation: Mutation testing of
deeplearningsystems.In 2018IEEE29thInternationalSymposiumonSoftware
Reliability Engineering (ISSRE). IEEE, 100â€“111.
[50]PingchuanMa,ShuaiWang,andJinLiu.2020. MetamorphicTestingandCerti-
fiedMitigationofFairnessViolationsinNLPModels.In Proceedingsofthe29th
International Joint Conference on Artificial Intelligence (IJCAI). 458â€“465.
[51]Aravindh Mahendran and Andrea Vedaldi. 2015. Understanding deep image rep-
resentations by inverting them. In Proceedings of the IEEE conferenceon computer
vision and pattern recognition. 5188â€“5196.
[52]Aravindh Mahendran and Andrea Vedaldi. 2016. Visualizing deep convolutional
neural networks using natural pre-images. International Journal of Computer
Vision120, 3 (2016), 233â€“255.
1064[53]AugustusOdenaandIanGoodfellow.2018. Tensorfuzz:Debuggingneuralnet-
works with coverage-guided fuzzing. arXiv preprint arXiv:1807.10875 (2018).
[54]Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2009), 1345â€“1359.
[55]Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
ZacharyDeVito,ZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer.
2017. Automatic differentiation in PyTorch. In NIPS-W.
[56]PriyadarshiniPatil,PrashantNarayankar,DGNarayan,andSMdMeena.2016.
A comprehensive evaluation of cryptographic algorithms: DES, 3DES, AES, RSA
and Blowfish. Procedia Computer Science 78 (2016), 617â€“624.
[57] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the 26th
Symposium on Operating Systems Principles (SOSP â€™17). ACM, 1â€“18.
[58] KexinPei,YinzhiCao,JunfengYang,andSumanJana.2017. DeepXplore:Auto-
mated Whitebox Testing of Deep Learning Systems. In Proceedings of the 26th
SymposiumonOperatingSystemsPrinciples(SOSPâ€™17) .ACM,NewYork,NY,USA,
1â€“18.https://doi.org/10.1145/3132747.3132785
[59]Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko. 2014. Exploring
Invariances in Deep Convolutional Neural Networks Using Synthetic Images.
CoRRabs/1412.7122 (2014). arXiv:1412.7122 http://arxiv.org/abs/1412.7122
[60]Florent Perronnin, Jorge SÃ¡nchez, and Thomas Mensink. 2010. Improving theFisher Kernel for Large-scale Image Classification. In Proceedings of the 11th
European Conference on Computer Vision: Part IV (ECCVâ€™10). 143â€“156.
[61]HungVietPham,ThibaudLutellier,WeizhenQi,andLinTan.2019. CRADLE:
Cross-BackendValidationtoDetectandLocalizeBugsinDeepLearningLibraries.
InProceedings of the 41st International Conference on Software Engineering (ICSE
â€™19).
[62]Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci,
Gavriel State, Omer Shapira, and Stan Birchfield. 2018. Structured DomainRandomization: Bridging the Reality Gap by Context-Aware Synthetic Data.
arXiv preprint arXiv:1810.10093 (2018).
[63]SiyuanQi,YixinZhu,SiyuanHuang,ChenfanfuJiang,andSong-ChunZhu.2018.
Human-centric indoor scene synthesis using stochastic grammar. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. 5899â€“5908.
[64]Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. Youonly look once: Unified, real-time object detection. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 779â€“788.
[65]Joseph Redmon and Ali Farhadi. 2017. YOLO9000: better, faster, stronger. InProceedings of the IEEE conference on computer vision and pattern recognition.
7263â€“7271.
[66]JosephRedmonandAliFarhadi.2018. YOLOv3:AnIncrementalImprovement.
CoRRabs/1804.02767 (2018).
[67]Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91â€“99.
[68]Artem Rozantsev, Vincent Lepetit, and Pascal Fua. 2014. On Rendering Syn-thetic Images for Training an Object Detector. CoRRabs/1411.7911 (2014).
arXiv:1411.7911 http://arxiv.org/abs/1411.7911
[69]OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,Sean
Ma,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal .
2015. Imagenet large scale visual recognition challenge. International journal of
computer vision 115, 3 (2015), 211â€“252.
[70]Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill,
andRobAshmore.2019. DeepConcolic:TestingandDebuggingDeepNeuralNet-
works.In Proceedingsofthe41stInternationalConferenceonSoftwareEngineering:
Companion Proceedings (ICSE â€™19). 111â€“114.
[71]Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska,and Daniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In
Proceedingsofthe33rdACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering (ASE 2018). 109â€“119.
[72]ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
2818â€“2826.
[73]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testing of Deep-neural-network-driven Autonomous Cars. In Proceedings of the
40th International Conference on Software Engineering (ICSE â€™18). 303â€“314.
[74]JonathanTremblay,AayushPrakash,DavidAcuna,MarkBrophy,VarunJampani,Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. 2018.
Training deep networks with synthetic data: Bridging the reality gap by domain
randomization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops. 969â€“977.
[75]SakshiUdeshi,PryanshuArora,andSudiptaChattopadhyay.2018. Automated
Directed Fairness Testing. In Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering (ASE 2018). 98â€“108.
[76]JonasUhrig,EikeRehder,BjÃ¶rnFrÃ¶hlich,UweFranke,andThomasBrox.2018.
Box2pix:Single-shotinstancesegmentationbyassigningpixelstoobjectboxes.In2018 IEEE Intelligent Vehicles Symposium (IV). IEEE, 292â€“299.
[77]
J. R. Uijlings, K. E. Sande, T. Gevers, and A. W. Smeulders. 2013. Selective Search
for Object Recognition. Int. J. Comput. Vision 104, 2 (Sept. 2013), 154â€“171.
[78]Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, and Peixin Zhang. 2019.
AdversarialSampleDetectionforDeepNeuralNetworkThroughModelMutation
Testing.In Proceedingsofthe41stInternationalConferenceonSoftwareEngineering
(ICSE â€™19). 1245â€“1256.
[79]Xiaoyu Wang, Tony X Han, and Shuicheng Yan. 2009. An HOG-LBP human
detectorwithpartialocclusionhandling.In 2009IEEE12thinternationalconference
on computer vision. IEEE, 32â€“39.
[80]Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. 2017. A-fast-rcnn:
Hard positive generation via adversary for object detection. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. 2606â€“2615.
[81]Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Hongxu Chen, Minhui Xue, Bo Li, Yang Liu,
Jianjun Zhao, Jianxiong Yin, and Simon See. 2018. Coverage-guided fuzzing for
deep neural networks. arXiv preprint arXiv:1809.01266 (2018).
[82]Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Mad-
havan,andTrevorDarrell.2018. BDD100K:ADiverseDrivingVideoDatabase
with Scalable Annotation Tooling. CoRR(2018).
[83]Andreas Zeller. 1999. Yesterday, My Program Worked. Today, It Does Not. Why?.
InProceedingsofthe7thEuropeanSoftwareEngineeringConferenceHeldJointly
with the 7th ACM SIGSOFT International Symposium on Foundations of Software
Engineering (ESEC/FSE-7). 253â€“267.
[84]MengshiZhang,YuqunZhang,LingmingZhang,CongLiu,andSarfrazKhurshid.
2018. DeepRoad: GAN-based Metamorphic Testing and Input Validation Frame-
work for Autonomous Driving Systems. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE 2018).
[85]YuhaoZhang,YifanChen,Shing-ChiCheung,YingfeiXiong,andLuZhang.2018.
AnEmpiricalStudyonTensorFlowProgramBugs.In Proceedingsofthe27thACM
SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018).
129â€“140.
[86]Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. 2018. Object
Detection with Deep Learning: A Review. CoRR(2018).
[87]Zhedong Zheng, Liang Zheng, and Yi Yang. 2017. Unlabeled samples generated
byganimprovethepersonre-identificationbaselineinvitro.In Proceedingsof
the IEEE International Conference on Computer Vision. 3754â€“3762.
[88]HushengZhou,WeiLi,YuankunZhu,YuqunZhang,BeiYu,LingmingZhang,andCongLiu.2020. Deepbillboard:Systematicphysical-worldtestingofautonomous
drivingsystems.In Proceedingsofthe42ndInternationalConferenceonSoftware
Engineering (ICSE).
1065