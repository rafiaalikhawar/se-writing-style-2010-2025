BiLO-CPDP: Bi-Level Programming for Automated Model
Discovery in Cross-Project Defect Prediction
Ke Li/natural, Zilin Xiang/sharp, Tao Chen§, Kay Chen Tan♣∗
/sharpCollege of Computer Science and Engineering, UESTC, Chengdu, 611731, China
/naturalDepartment of Computer Science, University of Exeter, Exeter, EX4 4QF, UK
§Department of Computer Science, Loughborough University, Loughborough, LE11 3TU, UK
♣Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong SAR
k.li@exeter.ac.uk,ziling.xiang@hotmail.com,t.t.chen@lboro.ac.uk,kaytan@cityu.edu.hk
ABSTRACT
Cross-Project Defect Prediction (CPDP), which borrows data from
similar projects by combining a transfer learner with a classifier,
haveemergedasapromisingwaytopredictsoftwaredefectswhen
the available data about the target project is insufficient. How-
ever, developing such a model is challenge because it is difficultto determine the right combination of transfer learner and clas-sifier along with their optimal hyper-parameter settings. In this
paper,weproposeatool,dubbed BiLO-CPDP ,whichisthefirstof
itskindtoformulatetheautomatedCPDPmodeldiscoveryfrom
the perspective of bi-level programming. In particular, the bi-levelprogramming proceeds the optimization with two nested levels in
a hierarchical manner. Specifically, the upper-level optimization
routine isdesigned tosearch forthe rightcombination oftransfer
learnerandclassifierwhilethenestedlower-leveloptimizationrou-
tine aims to optimize the corresponding hyper-parameter settings.
To evaluate BiLO-CPDP , we conduct experiments on 20 projects to
compare it with a total of 21 existing CPDP techniques, along with
its single-level optimization variant and Auto-Sklearn , a state-of-
the-artautomatedmachinelearningtool.Empiricalresultsshow
thatBiLO-CPDP champions better prediction performance than all
other21existingCPDPtechniqueson70%oftheprojects,whilebe-
ing overwhelmingly superior to Auto-Sklearn and its single-level
optimization variant on all cases. Furthermore, the unique bi-level
formalizationin BiLO-CPDP alsopermitstoallocatemorebudget
to the upper-level, which significantly boosts the performance.
CCS CONCEPTS
•Softwareanditsengineering →Softwarecreationandman-
agement; Software defect analysis.
∗K. Li, Z. Xiang and T. Chen contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416617KEYWORDS
Cross-project defect prediction, transfer learning, classification
techniques,automatedparameteroptimization,configurablesoft-
ware and tool
ACM Reference Format:
KeLi,ZilinXiang,TaoChen,andKayChenTan.2020.BiLO-CPDP:Bi-Level
Programming for Automated Model Discovery in Cross-Project Defect Pre-
diction.In 35thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering (ASE ’20), September 21–25, 2020, Virtual Event, Australia. ACM,
New York, NY, USA, 12 pages. https://doi.org/10.1145/3324884.3416617
1 INTRODUCTION
Softwaredefectsareerrorsincodeanditslogicthatcauseasoftware
product to malfunction or to produce incorrect/unexpected results.
Given that software systems become increasingly ubiquitous in
ourmodernsociety,softwaredefectsarehighlylikelytoresultin
disastrous consequences to businesses and daily lives. For example,
the latest Annual Software Fail Watch report from Tricentis1shows
that, globally, software defects/failures affected over 3.7 billion
people and caused $1.7 trillion in lost revenue.
One of the key reasons behind the prevalent defects in modern
software systems is their increasingly soaring size and complexity.
Due to the limited resource for software quality assurance and the
intrinsicdependencyamongalargenumberofsoftwaremodules,
it is expensive, if not impossible, to rely on human efforts (e.g.,
code review) to thoroughly inspect software defects. Instead, itis more pragmatic to predict the defect-prone software modulesto which software engineers are suggested to focus their limited
software quality assurance resource. To this end, machine learning
algorithmshavebeenwidelyusedtoautomatetheprocessofdefect
prediction.
As discussed in [ 52], one of the keys to the success of defect
predictionmodelsistheamountofdataavailableformodeltraining.
Inpractice,however,itisunfortunatelynotuncommonthatsuch
dataisscarceorevenunavailable.Thiscanbeattributedtothesmallsizeofthecompanyand/orthetargetedsoftwareprojectisthefirst
ofits kind.Cross projectdefectprediction (CPDP),which aims topredictdefectsintheasoftwareprojectbyleveragingexperience
(e.g.,trainingdataorhyper-parametersoftraineddefectprediction
models) from other existing ones, has therefore become extremely
appealing [ 15]. Unfortunately, partially due to the difference of the
data distribution between the source and the target projects, the
1https://www.tricentis.com/resources/software-fail-watch-5th-edition/
5732020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
performanceofvanillaCPDPisnotaspromisingasitwassupposed
tobe[36].Transferlearning,whichisabletotransferknowledge
across different domains, has shown to be able to overcome the
aforementioned challenges (e.g., data scarcity and data distribution
discrepancy) and has gradually become the main driving force for
CPDP[29].Generallyspeaking,thebasicideaistoequipamachine
learning classifier with a transfer learner that enables its ability to
learn from other projects in model building.
ThereisNoFreeLunch indefectpredictiongiventhatmachine
learning enabled defect prediction models often come with con-
figurableandadaptableparameters(87%prevalentclassifiersare
withatleastoneparameter[ 43,44]).Thepredictionaccuracyon
various software projects largely depends on the parameter set-
tings of those defect prediction models [ 25,26]. Furthermore, it
becomesmorecomplicatedinCPDPbecause:1)theconfigurable
parameters is augmented by the transfer learner (85% widely used
CPDP techniques require at least one parameter to setup in thetransfer learner) thus lead to an enlarged search space; 2) thereexist complex yet unknown interactions among the parametersof the classifier and those of the transfer learner (that is to sayparameter optimization over either the classifier or the transferlearner alone may not lead to the overall optimal performance);and 3) the optimal selection of the combination of classifier and
transferlearnerisasimportantasparameteroptimizationbutisun-
fortunately ignored in the current literature (most, if not all, CPDP
modelsaredesignedwithan ad-hoccombinationoftransferlearner
andclassifier, theperformance ofwhich isreportedto befar from
optimal[21]).Although thereexistsome priorworks considering
the hyper-parameter optimization for CPDP models [ 31,34], they
onlyconsiderthehyper-parametersassociatedwiththeclassifier.
Asinvestigatedinalatestempiricalstudy[ 21],thispracticeisfar
from truly optimizing the performance of the underlying CPDP
modelwhilethesettingsofhyper-parametersofthetransferlearner
are more decisive.
Bearing the above considerations in mind, we propose a new
tool,dubbed BiLO-CPDP ,toautomatethemodeldiscoveryforCPDP
tasks.Itprovidesanunifiedperspectiveforthecombinatorialse-
lection of classifier and transfer learner, as well as their hyper-parameter optimization within the mathematical framework ofbi-level programming, where two levels of nested optimizationproblems are formulated: the upper-level optimization problemis solved subject to the optimality of a lower-level optimization
problem.Specifically,theupper-leveloptimizationproblemaims
to identify the optimal combination of transfer learner and clas-sifier from a given portfolio; while the lower-level optimization
problemisdedicatedtosearchingfortheoptimalparametersetting
associated with the corresponding transfer learner and classifier.Note that a combination of transfer learner and classifier is not
considered to be feasible for comparison unless the corresponding
parameters have been optimized. In BiLO-CPDP , the upper-level
optimizationisformulatedasacombinatorialoptimizationprob-
lemwhich issolved bythe Tabusearch [ 10] whilethelower-level
optimizationismodeledasanexpensiveoptimizationproblemwith
a limited budget to be solved by Tree-structured Parzen Estimator
(TPE) [4], a state-of-the-art Bayesian optimization algorithm.
To evaluate the the effectiveness of BiLO-CPDP for automated
model discovery in CPDP, we conduct experiments to compareit with 21 existing CPDP techniques, its single-level variant and
Auto-Sklearn [8]—astate-of-the-artautomatedmachinelearning
(AutoML) tool— over 20 distinct projects. Theresults fully demon-
strate the overwhelming superiority of BiLO-CPDP over the others
with statistical significance and a large effect size on all projects.
In summary, the key contributions of this paper are as follows:
•To the best of our knowledge, BiLO-CPDP is the first of
its kind for automating CPDP from the perspective of bi-
levelprogramming.Giventhat BiLO-CPDP isnotonlyable
to automatically search the optimal combination of trans-
ferlearnerandclassifier,butalsocansettheirappropriate
hyper-parameter settings, it paves a new avenue for auto-
mated model discovery in CPDP.
•Through extensive experiments with 21 existing CPDP tech-
niques, we show that BiLO-CPDP is the best on 14 out of
20 projects, and second to only one existing technique for
anotherfive.Thisfullydemonstratestheeffectivenessand
importance brought by automatically choosing the appro-priate transfer learner and classifier associated with their
optimal hyper-parameter settings for CPDP.
•Intermsofoptimizationproblemformulation,onallprojects,
we show that the bi-level programming formulated in BiLO-
CPDPis statistically better than hybridizing both combinato-
rial selection and parameter optimization as a single-level
globaloptimizationproblem,whichisperhapsamorecon-
servative solution as used in, e.g., Auto-Sklearn [8].
•Interestingly, from our experimental results, we disclose
that choosing the best combination of classifier and transfer
learner(upperlevel)ismoreimportantthanfullyoptimizingtheirparameters(lowerlevel).Henceforth,giventhelimited
resource for software quality assurance, it is beneficial to
allocate more search budget to the upper-level optimization.
Intherestofthispaper,Section2givesthebackgroundabout
bi-level programming.Section 3 delineatesthe algorithmic imple-
mentation of BiLO-CPDP step by step. The experimental setup is
introduced in Section 4 and the results are analyzed in Section 5.
Thereafter,Section6and7reviewstherelatedworksanddiscusses
thethreatstovalidity,respectively.Finally,Section8concludesthis
paper and threads some lights on future directions.
2 BI-LEVEL PROGRAMMING
Bi-levelprogrammingisamathematicalprogramwithinwhichone
optimization problem is nested within another in a hierarchical
manner [42]. It is ubiquitous in many real-world optimization and
public/private sector decision-making problems where the real-
ized outcome of any solution or decision taken by the upper-level
authority(a.k.a.leader)tooptimizetheirobjectivesisaffectedby
the response of lower-level entities (a.k.a. follower), who seek tooptimize their own outcomes. This is in principle similar to the
Stackelberg games [ 47] in which a leader first makes its move and
afollowermaximizesthecorrespondinggainbytakingtheleader’s
move into account. It is interesting to note that the two levels ofoptimization problems are asymmetric in bi-level programming.
That is to say, the upper-level leader has the entire picture of opti-
mization problemsat both levels whereasthe lower-level follower
574usuallytakesthedecisionsfromtheleaderandthenoptimizesits
own strategies.
Thebi-levelprogrammingformulatedin BiLO-CPDP canbemath-
ematically defined as:
maximize
xu∈Λd×Rn,xl∈RnF(xu,xl∗)
subject to xl∗∈argmax{fxu(xl)},(1)
where xu∈Λd×Rnandxl∈Rndenotetheupper-andlower-level
variables2whileF:Λd×Rn→Randfxu:Rn→Raretheupper-
leveland lower-levelobjective functions,respectively (detailscan
befoundinSection3).Abi-levelprogrammingthatinvolvesnested
optimization/decision-makingtasksatbothlevels.Foranygiven
combination xu,thereexistsa (xu,xl∗)pairwhere xl∗isanoptimal
(ornear-optimal)responseto xurepresentsafeasiblesolutionto
theupper-leveloptimizationproblemgiventhatitalsosatisfiesthe
constraints therein.
3 BI-LEVEL PROGRAMMING FOR
AUTOMATED CPDP MODEL DISCOVERY
TheCPDPmodelbuildingprocessconsistsoftwointertwinedparts:
1) transfer learning that augments data from different domains by
selectingrelevantinstancesorassigningappropriateweightstodif-
ferent instances; and 2) defect prediction model building based on
adapteddata.Asreportedinalatestresearch[ 21],theperformance
of a CPDP model largely depend on the combination of transfer
learner and classifier along with their hyper-parameter settings. In
light of this, the BiLO-CPDP proposed in this work was specifically
designed to address such a problem. Through automatically discov-
ering the best combination of transfer learner and classifier as well
as their optimal hyper-parameter settings, BiLO-CPDP serves as an
automatictoolthatprovidesa denovaCPDPmodeldiscovery.In
thissection,wewilldelineatethearchitectureof BiLO-CPDP and
the algorithmic details of its optimization routines at both levels.
3.1 Overview of BiLO-CPDP
The overall architecture of BiLO-CPDP is illustrated in Fig. 1 which
consistsofthreekeyphases,i.e., datapre-processing, optimization
andperformance validation.
(1)Data Pre-processing: Given a raw dataset with N>1
projects, software engineers are asked to specify which one
is the target domain that serves as the target domain data
while the remaining N−1 projects are then used as the
sourcedomaindata.Inparticular,allsourcedomaindataare
used in the model training while a part of the target domain
dataisusedasthehold-outsetforthetestingpurpose.As
thedefaultin BiLO-CPDP ,weuse10%ofthetargetdomain
datafortestingwhiletheremaining90%isfortraining.Thisisbecausesometransferlearnersconsideredinthisworkdoneeddatafromthetargetdomainintraining,e.g.,
MCWs[33].
Forothertransferlearnersthatcanbetrainedindependently
to the target domain, we use all data for testing.
(2)Optimization: BiLO-CPDP models CPDP as a bi-level pro-
grammingthatnotonlyidentifiesthemostcompetitivecom-
binationoftransfer leanerandclassifierfor theunderlying
2Λd×Rnmeans that the problem is a discrete combinatorial problem.
bar.c
bar.h
foo.c
foo.h
source domain 
training data
bar.c
bar.h
foo.c
foo.h
target domaintraining data
···
···
···
···
target domain
testing dataraw
datasetcalculate testing
performance
transfer learning
algorithmsdefect prediction
modelUpper-level optimiztion
00.51
transfer learner
classifierAUCTabu search
Data pre-processingLow-level optimiztion
calculate training
performance
T
OptimizationPerformance
validationTPE
T
Figure 1: The overall architecture of BiLO-CPDP.
CPDP task (tackled by the upper-level routine), but also
equips the chosen CPDP model with the appropriate hyper-
parametersettings(carriedoutbythelower-levelroutine).
Since the resources for software quality assurance are often
limited, the entire optimization process would inevitably be
constrained under a computational budget of running time.
In this regard, the unique bi-level programming formulated
inBiLO-CPDP can in fact provide a fine-grained and flexible
allocation of the budget between upper- and lower-level,
whoseeffectswillbeinvestigatedaspartoftheexperimental
evaluationinSection5.4.TheCPDPmodel,whichhasthe
best combination with its optimal hyper-parameter settings
intermsofthetrainingaccuracy,isreturnedintheend.Notethat due to the lack of data samples, using training accuracy
intheparameteroptimizationoftransferlearnerisnotun-
common and has shown promising results for CPDP [21].
(3)Performance Validation: Aftertheoptimizationphase,as
an optional module in BiLO-CPDP , the generalization of the
builtCPDPmodelcanbevalidatedandtestedbyusingthe
hold-outsetfromthetargetdomaindata,whichisunknownduringtrainingstage.Inpractice,thiswillbethenewprojectthatonewishestopredictdefectsfor.In
BiLO-CPDP ,thearea
under the receiver operating characteristic (ROC) curve, i.e.,
AUC[51],isappliedastheperformancemetrictomeasure
the effectiveness of a model. Formally, AUC is defined as:
AUC=/summationtext.1
t−∈D−/summationtext.1
t+∈D+1/bracketleftbig
Pred(t−)<Pred/parenleftbigt+/parenrightbig/bracketrightbig
|D−|·|D+|,(2)
wherePred(t)is the probability that sample tis predicted
to be a positive sample, and 1/bracketleftbig
f(t−)<f/parenleftbigt+/parenrightbig/bracketrightbig
is an indica-
tor function which returns 1 if f(t−)<f(t+)otherwise it
returns 0. D−is theset of negative samples, and D+is the
set of positive samples. Apart from the fact that AUC has
beenwidelyforsoftwaredefectprediction[ 21],ithastwo
distinctive characteristics: 1) different from other prevalent
metricslikeprecisionandrecall,AUCdoesnotdependon
a particular threshold [ 51], which is difficult to tweak in
order to carry out an unbiased assessment; and 2) it is not
575sensitivetoimbalanceddatawhichisnotuncommoninsoft-
waredefectprediction[ 22].ThelargertheAUCvalueis,the
better prediction accuracy the model achieves. In particular,
the AUC value ranges between 0 and 1 where 0 indicates
the worst performance, 0.5 corresponds a randomly guessed
performance and 1 represents the best performance. Note
that AUC is also the metric used in optimization phase to
evaluate and compare training accuracy.
3.2 Upper-Level Optimization
Tables 1and 2 respectivelylist the transferlearners and theclassi-
fiers considered in our work, which form the portfolios. Note that
all transfer learners considered in BiLO-CPDP have been used in
either the defect prediction or CPDP literature while the classifiers
comefrom scikit-learn3,thestate-of-the-artmachinelearning
Pythonlibrary.Inaddition,thecorrespondinghyper-parametersas-sociatedwiththosetransferlearnersandclassifiersalongwiththeir
value ranges are also provided in the corresponding tables. Any
combinationofatransferlearnerandaclassifiercomesupwitha
CPDPmodel.Theultimategoaloftheupper-leveloptimizationistosearch for the best combination out of all possible alternatives (208
in this work) for the underlying CPDP task. In particular, for each
candidatecombinationoftransferlearnerandclassifier,theircorre-
sponding hyper-parameter settings are optimized via a lower-level
optimization routine which will be explained in Section 3.3.
Attheupper-levelin BiLO-CPDP ,thesearchofthebestcombi-
nationoftransferlearnerandclassifierissolvedasacombinatorial
optimization problem as specified below.
•Search space :Fortheupperlevel,thesearchspaceconsists
ofallthevalidcombinationsoftransferlearnersandclassi-
fierpickedupfromthegivenportfolios,i.e.,thoselistedin
Tables1and2.Inpractice,suchportfolioscanbeamended
and specified by the software engineers based on their pref-
erences/requirements.
•Objective function :Recall from the equation (1), the ob-
jective function for the upper level F(xu,xl∗)takes a com-
bination from the portfolio ( xu) and the optimized hyper-
parameterofsuchcombination( xl∗)asinputs.Itthenout-
putsthecorrespondingtrainingAUCobtainedbytraining
the CPDP model for comparison. Note that xl∗is initially
unknownforagiven xuattheupper-levelbeforerunning
a lower-level optimization routine. Therefore, the objective
functionatupper-leveloptimizationisconstrainedandde-
termined by the lower-level optimization.
•Optimization algorithm :Fortheupper-leveloptimization
inBiLO-CPDP , weuse Tabusearch [ 10] toserve asthe opti-
mizer,whichisalsotheentrypointoftheoptimizationphase.
In particular, we use Tabu search in this work because:
–Our problem is expensive and thus it is unrealistic for an
exact search to reach the optimal solution. Metaheuristicsuch as Tabu search, which does not guarantee optimum
but can often produce near-optimal result, is more practi-
cal and acceptable.
3https://scikit-learn.org/stable/Algorithm1: runTabuSearch:Upper-leveloptimization
thattunesthecombinationoftransferlearnerandclassifier.
Input:Portfolio of transfer learners Tand classifiers C
Output:Optimal CPDP model xu
optand its optimal
parameter settings xl∗
opt
1Randomly initialize a valid combination of transfer learner
and classifier xu←(t,c); /*t∈T andc∈C are a
candidate transfer learner and classifier */
2/lscriptt←∅; /*/lscripttis the tabu list */
3whileThe overall time budget is not exhausted do
4[(xu,xl∗),F(xu,xl∗)] ←searchCandidate(xu,/lscriptt);
5ifxu/nelement/lscripttthen
6/lscriptt←/lscriptt/uniontext.1{xu};
7return(xu
opt,xl∗
opt)←argmaxxu∈/lscriptt{F(xu,xl∗)};
Algorithm 2: searchCandidate(xu,/lscriptt):Searchthenext
combination candidate from the neighborhood of xu
Input:Candidate CPDP model xuand newest tabu list/lscriptt
Output: The best CPDP model within xu’s neighbor and its
optimal parameter settings xopt, the performance
of this CPDP model fopt, i.e., the value of
F(xu,xl∗)
1δ←Get the neighbors of xu;
2Θxu←Get the configuration space of the transfer learner
and the classifier specified by xu;
3[xl/prime,fxu(xl/prime)] ←runTPE(xu,Θxu);
4xopt←(xu,xl/prime),fopt←fxu(xl/prime);
5foreach xc∈δ∧xc/nelement/lscripttdo
6Θxc←Get the configuration space of the transfer
learner and the classifier specified by xc;
7[xl/prime,fxc(xl/prime)] ←runTPE(xc,Θxc);
8iffxc(xl/prime)>foptthen
9 xopt←(xc,xl/prime),fopt←fxc(xl/prime);
10return xopt,fopt;
–Unlike other metaheuristics, Tabu search employs local
searchtospeed up its convergence [10].
–Tabu search permits a better chance to escape from localoptima than other local search methods [10].
AsshownininAlgorithm1andAlgorithm2,Tabusearch
carries out a neighborhood search where the neighborhood
of the current solution is restricted by the search historyofpreviouslyvisitedsolutionsandisstoredintheformofatabu list (lines 5 and 6 in Algorithm 1 and lines 5 to 9
inAlgorithm2).Ifallneighborsare tabu,itisacceptableto
take a move that worsen the value of the objective function
(lines 3 and 4 in Algorithm 2). This is what enables Tabu
search to escape from local optima, which is highly likelyto cause issues with a traditional gradient decent method.
According to a provided selection criteria, Tabu search only
keep a record of some previously visited states.
576Table 1: Overview of 13 selected transfer learners ([N], [R] and [C] denote integer, real and categorical value, respectively).
Algorithm Parameter Range Algorithm Parameter Range Algorithm Parameter Range
NNfilter
[45]k [N]
metric [C][1,100]
Euc, Man, Che,
Min, MahCDE_SMOTE
[23]k [N]
metric [C][1,100]
Euc, Man, Che,
Min, MahFSS_bagging
[11]topN [N]
threshold [R]
ratio [R][1,15]
[0.3,0.7]
[0.1,0.5]
TCA+[29]kernel [C]
dime [N]
lamb [R]primal, rbf,
linear, sam
[5, max(N_s,
N_t)]
[10E−7,100]GIS[16]prob [R]
chrm_size [R]
pop_size [N]
num_parts [N]
num_gens [N][0.02,0.1]
[0.02,0.1]
[2,30]
[2,6]
[5,20]CLIFE_MORPH
[32]n [N]
alpha [R]
beta [R]
per [R][1,100]
[0.05,0.2]]
[0.2,0.4]
[0.6,0.9]
gama [R] [10E−6,100] mcount [N] [3,10] HISNN[38]minham [N] [1, N_s]
MCWs[33]k [N]
sigma [R]
lambda [R][2, N_s]
[0.01,10]
[10E−7,100]FeSCH[30]nt [N]
strategy [C][1, N_s]
SFD, LDF, FCRUM[50]p[R]
qua_T [C][0.01,0.1]
cli , cohen
TD[12]strategy [C]
k [N]NN, EM
[1, N_s]VCB[37]m [N]
lambda [R][2,30]
[0.5,1.5]PCAmining
[28]dime [N] [5, max(N_s,
N_t)
For full specification of all the parameters, please visit our repository: https://github.com/COLA-Laboratory/ase2020
Table 2: Overview of 16 selected classifiers ([N], [R] and [C] denote integer, real and categorical value, respectively).
Algorithm Parameter Range Algorithm Parameter Range Algorithm Parameter Range
Extra
Trees
Classifier
(EXs)max_e [N]
criterion [C]
min_s_l [N]
splitter [C]
min_a_p [N][10,100]
gini, entropy
[1,20]
random, best
[2, N_s/10]Extra
Tree
Classifier
(EXtree)max_e [N]
criterion [C]
min_s_l [N]
splitter [C]
min_a_p [N][10,100]
gini, entropy
[1,20]
random, best
[2, N_s/10]Decision
Tree (DT)max_e [N]
criterion [C]
min_s_l [N]
splitter [C]
min_a_p [N][10,100]
gini, entropy
[1,20]
auto, sqrt, log2
[2, N_s/10]
Random
Forest (RF)m_stim [N]
criterion [C]
splitter [C]
min_s_l [N]
min_a_p [N][10,100]
gini, entropy
auto, sqrt, log2
[1,20]
[2, N_s/10]Support
Vector
Machine
(SVM)C [R]
kernel [C]
degree [N]
coef0 [R]
gamma [R][0.001,10]
rbf, lin, poly, sig
[1,5]
[0,10]
[0.01,100]MultilayerPerceptron
(MLP)active [C]
hid_l_s [N]
solver [C]
iter [N]iden, log,
tanh, relu
[50,200]
lbfgs, sgd, adam
[100,250]
Passive
AggressiveClassifier
(PAC)C [R]
fit_int [C]
tol [R]
loss [C][0.001,100]
true, false
[10E−6,0.1]
hinge, s_hingePerceptronpenalty [C]
alpha [R]
fit_int [C]
tol [R]L1, L2
[10E−6,0.1]
true, false
[10E−6,0.1]Naive
Bayes (NB)NBType [C]
alpha [R]
norm [C]gauss,
multi, comp
[0,10]
ture, false
Ridgealpha [R]
fit_int [C]
tol [R][10E−5,1000]
ture, false
[10E−6,0.1]Baggingn_est [N]
max_s [R]
max_f [R][10,200]
[0.7,1.0]
[0.7,1.0]Logistic
Regression
(LR)penalty [C]
fit_int [C]
tol [R]L1, L2
ture, false
[10E−6,0.1]
KNearest N-
eighbor(KNN)n_neigh [N]
p [N][1,50]
[1,5]Radius
Neighborsradius [R]
weight [C][0,10000]
uni, distNearest
Centroidmetric [C] Euc, Man, Che,
Min, Mah
adaBoostn_est [N]
rate [R][10,1000]
[0.01,10]Classifier
(RNC)Classifier
(NCC)shrink_t [R] [0,10]
For full specification of all the parameters, please visit our repository: https://github.com/COLA-Laboratory/ase2020
3.3 Lower-Level Optimization
AsintroducedinSection3.1,themainpurposeofthelower-levelop-
timization is to optimize the hyper-parameters associated with the
chosencombinationoftransferlearner andclassifier.Specifically,
this level in BiLO-CPDP is modeled and tackled as below.
•Search space :At this level, the search space is the configu-
rationspaceofthecorrespondingparametersforthetransfer
learnerandclassifierpickedupfromtheupper-levelroutine.
Indeed, as can be seen from Tables 1 and 2, such a config-uration space might be different depending on the chosen
combination of transfer learner and classifier.
•Objective function :Recall from the equation (1), when a
combination of transfer learner and classifier is picked upfrom the upper-level routine, the objective function for the
lower-level f(xl)takes the configuration of the correspond-
inghyper-parametersastheinputs( xl)andoutputsthetrain-
ingAUCfortheCPDPmodel.TheAUCcollectedfromthe
resultofthelow-levelroutineisfinallyusedastheobjective
value at the upper-level routine to steer the optimization.
•Optimization algorithm :Itisnotuncommonthatthetrain-
ing andevaluation of a CPDPmodel is computationallyde-
manding and time consuming. To this end, in BiLO-CPDP ,
weapplytheTree-structuredParzenEstimator(TPE)[ 4]—a
state-of-the-artBayesianoptimizationalgorithmforhyper-
parameteroptimizationofmachinelearningalgorithms—astheoptimizerforthelower-leveloptimization,dueprimarily
to the following reasons:
577Algorithm3: runTPE(xu,Θxu):Lower-leveloptimization
for identifying the optimal hyper-parameters.
Input:Combination of transfer learner and classifier xu,
configuration space Θc
Output:Optimized hyper-parameters xl∗and its objective
function fxu(xl∗)
1D←Use space-filling to sample a set of hyper-parameters
fromΘcand evaluate their objective functions;
2whileThe lower-level time budget is not exhausted do
3Use Tree Parzen to build a surrogate model based on D;
4 xlc←Best configuration based on the AUC predicted
by the acquisition function over the surrogate model;
5fxu(xlc)←Evaluate the objective function of xlcby
physically training the CPDP model;
6D=D/uniontext.1{(xlc,fxu(xlc))};
7return(xl∗,fxu(xl∗)) ←argmax(xlc,fxu(xlc))∈D{fxu(xlc)};
–TPEcopes witha widerange ofvariables well,including
integer, real, and categorical ones, which fits precisely
with our need [4].
–RecentworkonCPDP[ 21]andfromthemachinelearn-
ing community [ 7] have reported the outstanding perfor-
mance of TPE for expensive configuration problems.
As the pseudo-code shown in Algorithm 3, the TPEalgo-
rithmfirstusesaspace-fillingtechniquetosampleasetof
hyper-parameters’valuesfromthegivenconfigurationspace
Θcof transfer learner and classifier, which would then be
trained for collecting the training AUC performance (line 1).
Alltheseconstitutetheinitialdataset D.Duringthemain
while-loop, a relatively cheap surrogate model of the expen-
sivephysicalmodeltrainingandtheAUCevaluationisbuilt
basedonallsampleddatain D(line3).Thereafter,apromis-
inghyper-parameter configurationtrial xlcisidentified by
optimizingtheacquisitionfunction(i.e.,expectedimprove-
ment) following a classic Bayesian optimization rigour. The
AUCof xlcisthereafterevaluatedandusedtoaugment D
(lines4to6).Attheend,thebesthyper-parametersetting xl∗
inDalong with its AUC performance fxu(xl∗)are returned
to the upper-level optimization routine (line 7).
4 EXPERIMENTAL SETUP
This section introduces our experiment setups4.
4.1 Dataset
In our experiments, the dataset of software projects is collected
according to the following three inclusion criteria:
(1)Topromotethereproducibilityandpracticalityofourexper-
iments, we only consider projects hosted in public reposito-
ries and are related to non-academic software.
(2)To mitigate potential conclusion bias, projects are required
to cover different corpora and domains.
4All source code and data of this work can be publicly accessed via our repository:
https://github.com/COLA-Laboratory/ase2020(3)Toensurethecredibilityofexperiments,wefocusonprojects
that have already been used in the CPDP literature.
Notethataprojectistemporarilyselectedifitmeetsallabovethree
criteria. To further refine our dataset composition, we apply the
following two exclusion criteria to rule out inappropriate projects.
(1)It is not uncommon that the projects are evolved with more
than one version during their lifetime. Since different ver-sions of the same project are highly likely to share many
similarities, they may simplify the transfer learning. In this
case, only the latest version of the project is kept.
(2)To promote the robustness of experiments, projects with re-
peated or missing data are ruled out from our consideration.
Based on the above inclusion criteria, we select five publicly
available datasets, i.e., JURECZKO ,NASA,SOFTLAB,AEEEM,ReLink.
Note that all these datasets have been reviewed and discussed in
many recent survey in the CPDP literature [ 13–15,51]. Thereafter,
SOFTLABisfurtherruledoutfromourconsiderationaccordingto
the above exclusion criteria. In addition, NASAis also not consid-
eredinourexperimentssinceitsdataqualityisrelativelypooras
reportedin[ 41].Attheend,thedatasetconsideredinourexperi-
ments consist of 20 open source projects with 10,952 instances. Its
characteristics are summarized as follows:
•AEEEM[6]:Thisdatasetcontains5opensourceprojectswith
5,371instances.Inparticular,eachinstancehas61metrics
withtwodifferenttypes,includingstaticandprocessmetrics
like the entropy of code changes and source code chorn.
•ReLink[49]: This dataset consists of 3 open source projects
with649instances.Inparticular,eachinstancecomeswith
26staticmetrics.Notethatthedefectlabelsarefurtherman-
uallyverifiedafterbeinggeneratedfromsourcecodeman-
agement system commit comments.
•JURECZKO [19]:Thisdatasetoriginallyconsistsof92released
softwarecollected froma mixof opensourced, proprietary
and academic projects. With respect to the first inclusioncriterion, those proprietary and academic projects are not
considered.Moreover,sincetheprojectsin JURECZKO have
beenupdatedmorethanonce,accordingtothefirstexclusion
criterion, only the latest version of a project is consideredin our experiments. Ultimately, we choose 12 open source
projects with 4,932 instances from JURECZKO.
4.2 Experimental Procedure
Our experimental procedure follows the three-phases workflow of
BiLO-CPDP introduced in Section 3.1. Here we explain the corre-
sponding settings for each phase.
•Inthedatapre-processing phaseforallpeerCPDPtechniques,
allprojectsinthisworkwillbeusedastargetdomaindata
inaround-robinmanner,forming20differentCPDPtasks.
This aims to mitigate the potential bias in conclusion.
•In theoptimization phase, each CPDP task is allocated with
an overall time budget of one hour (i.e., 3 ,600 seconds, as
suggestedbyFeureretal.[ 8])whilesettingeachlower-level
exploitationas20secondsin BiLO-CPDP .Whenapplicable,
thesamebudgetisgiventootherstate-of-the-artpeerCPDP
techniques that permit hyper-parameter optimization in the
578Table 3: Scott-Knott test on BiLO-CPDP and existing CPDP techniques over 30 runs. (the larger rank, the better; gray=the best)
CPDP Technique
Apache
EQ
JDT
LC
ML
PDE
Safe
TomcatZxing
ant
camel
ivy
jEdit
log4j
lucene
poi
synapse
velocity
xalan
xerces
NNfilter-NB 654 85381446 7938 862387
UM-NB 2765 4 31941 0 711 5 9 7 8681 08
UM-LR 5 3 6 7 4 3 6 12 4 9 5 10 7 10 5 4 4 3 5 7
CLIFE-NB 3443 1 3 69184687464544
CLIFE-KNN 33 74 2 3 1 1 2 47367644 446 4
FeSCH-RF 334 2 3 3 5 1 0 16288445454 6
GIS-NB 131 3 1 1 194 1 0 558744462 5
FeSCH-LR 4341 2 3 52173755445776
CLIFE-SVM 4344 3 2 17423866473383
TD-RF 3363 2 3 56353283443563
TD-LR 2143 2 3 59143686443693
TD-MLP 4343 1 3 17473874253593
TD-DT 4663 2 3 48163765133523
FeSCH-DT 4333 2 3 58273528123253
VCB-SVM 42 73 2 3 2212225342 111 2
CDE_SMOTE-RF 3445 2 3 11411113211111
CDE_SMOTE-KNN 4343 1 3 514111146132 121
FSS_bagging-RF 285 3 1 3 1443226143 362 2
FSS_bagging-NB 232 4 3 2 3 1 1 46338122323 3
FSS_bagging-LR 3353 3 3 65142892344425
HISNN-NB 4111 1 1 63443443223322
BiLO-CPDP 68765471 3 51161210117869119
The raw AUC values can be found in our repository: https://github.com/COLA-Laboratory/ase2020
comparison,e.g., Auto-sklearn [8].WeapplytheTPEalgo-
rithm implementation integrated in Hyperopt5, a popular
Pythonlibraryforhyper-parametertuninginmachinelearn-
ing [5], for the lower-level routine of BiLO-CPDP.
•In theperformance validation phase, AUC is used as the per-
formancemetric.Duetothestochasticnatureof BiLO-CPDP
andsome peer CPDPtechniques considered,each technique
is independently repeated 30 times for a given CPDP task
and the mean AUC values are recorded for comparison.
4.3 Ranking, Statistical Test and Effect Size
In our experiments, we use the following three statistical measures
to interpret the statistical significance of our comparative results.
•Scott-Knotttest: InsteadofmerelycomparingtherawAUC
values,weapplytheScott-Knotttesttoranktheperformance
of different peer techniques over 30 runs on each project, as
recommended by Mittas and Angelis [ 27]. In a nutshell, the
Scott-Knotttestusesastatisticaltestandeffectsizetodivide
the performance of peer techniques into several clusters. In
particular, the performance of peer techniques within the
sameclusterarestatisticallyinsignificant,i.e.,theiroverall
AUCvaluesarestatisticallyequivalent.Notethattheclus-
tering process terminates until no split can be made. Finally,
eachclustercanbeassignedarankaccordingtothemean
AUCvaluesachievedbythepeertechniqueswithintheclus-
ter.Inparticular,sinceagreaterAUCispreferred,thelarger
therankis,thebetterperformanceofthetechniqueachieves.
5http://hyperopt.github.io/hyperopt/•Wilcoxonsigned-ranktest: WeapplytheWilcoxonsigned-
ranktest[ 48]withasignificantlevel p=0.05[3]toinves-
tigatethestatisticalsignificanceofthecomparisons.Itisa
non-parametricstatisticaltestthatmakeslittleassumption
abouttheunderlyingdistributionofthedataandhasbeen
recommended in software engineering research [3].
•A12effect size: To ensure the resulted differences are not
generatedfromatrivialeffect,weapply A12[46]astheeffect
sizemeasuretoevaluatetheprobabilitythatonetechniqueis
better than another. According to Vargha and Delaney [ 46],
when comparing BiLO-CDPD with another peer technique
in our experiments, A12=0.5 means they are equivalent.
A12>0.5 denotes that BiLO-CDPD is better for more than
50%ofthetimes.Inparticular,0 .56≤A12<0.64indicates
a small effect size while 0 .64≤A12<0.71 andA12≥0.71
mean a medium and a large effect size, respectively.
Note that both Wilcoxon signed-rank test and A12are also used in
the Scott-Knott test for generating the clusters.
4.4 Research Questions
We seek to answer the following four research questions (RQs)
through our experimental evaluation:
•RQ1:IsBiLO-CPDP able to automatically configure a CPDP
model having better performance than the existing CPDP
techniques under their reported settings?
•RQ2:Howistheperformanceof BiLO-CPDP comparingwith
Auto- Sklearn, a state-of-the-art AutoML tool?
•RQ3:Is the bi-level programming in BiLO-CPDP beneficial?
579Figure 2: Total ranks achieved by BiLO-CPDP (the right most
one)andthe21peertechniques(thelargerrank,thebetter;
the dashed line and dotted line denote the best and averageresult over the 21 peer techniques, respectively).
•RQ4:
Given a limited computational budget, which level in
BiLO-CPDP is more important and deserves more budget?
5 RESULTS AND DISCUSSIONS
In this section, we present and discuss the results of our empirical
experiments and address the RQs posed in Section 4.4.
5.1 Comparison with Existing CPDP Work
5.1.1 Method. InordertoanswerRQ1,weusethetransferlearners
andclassifierscollectedinTables1and2toconstitute21peerCPDP
techniques in comparison with BiLO-CPDP . Note that although
thereareonly13transferlearnerslistedinTable1,someofthemarecombinedwithmorethanoneclassifiertoconstitutedifferentCPDPmodelsusedintheliterature(e.g.,
TDiscombinedwithclassifiers RF,
LR,MLPandDTthat constitute four different CPDP models in [ 12]).
For the parameter settings, we use the tuned values as reported in
the corresponding work.
5.1.2 Results and Analysis. Fromtheexperimentalresultsonthe
Scott-Knott test shown in Table 3, it is clear to see that BiLO-CPDP
is the best on 14 out of 20 (70%) projects, second only to one other
on five cases. In contrast, most of the other peer CPDP techniques,
albeit hand crafted by domain experts, are not as competitive as
BiLO-CPDP . In particular, NNfilter-NB is the most outstanding
peertechniquethatisthebestononly7out20(35%)projectswhile
the other peer techniques rarely take the best rank across all 20
projects. Noteworthily, the performance of NNfilter-NB ties with
BiLO-CPDP in two of its best results. In terms of the total ranks
achievedoverallprojects,asshowninFig.2,wecanobservethe
clearsuperiorityof BiLO-CPDP whichisatleast50%betterthanthe
other 21 peer techniques. Furthermore, we notice that the superior
performance of BiLO-CPDP is consistent across all 20 projects in
view of its top three ranked positions achieved in all projects. In
contrast,theperformanceofexistingCPDPtechniquesexhibitclear
variations depending on the underlying target projects.
Response to RQ1: BiLO-CPDP isgenerallybetterthantheother
21 existing CPDP techniques over all 20 projects. Unlike others that
werehand-craftedbydomainexpertstocertainextents, BiLO-CPDP
builds an effective CPDP model in a completely automated manner,
leading to highly competitive performance over different projects.Table 4: Mean AUC (standard deviation) for BiLO-CPDP and
Auto-Sklearn over 30 runs (gray=better; bold= p<.05).
Project BiLO-CPDP Auto-sklearn p-value
poi8.1703E-1 (4.38E-3) 6.5262E-1 (6.98E-3) 1.71E-6
synapse 7.1999E-1 (7.72E-3) 6.0183E-1 (3.68E-3) 1.65E-6
Zxing 6.3949E-1 (5.37E-3) 6.2615E-1 (1.45E-6) 1.91E-6
ant8.0006E-1 (8.26E-3) 7.4530E-1 (7.63E-3) 1.71E-6
log4j 8.4196E-1 (1.52E-2) 6.0965E-1 (1.19E-2) 1.57E-6
Safe7.9923E-1 (2.09E-2) 6.7513E-1 (6.15E-3) 1.64E-6
ivy8.0657E-1 (3.51E-3) 7.2407E-1 (9.64E-4) 1.19E-6
PDE6.8539E-1 (2.57E-3) 5.9781E-1 (2.22E-16) 1.62E-6
camel 6.2228E-1 (4.01E-3) 5.9006E-1 (1.11E-16) 1.62E-6
lucene 7.1065E-1 (8.13E-3) 6.4408E-1 (4.87E-6) 1.37E-6
JDT7.3705E-1 (1.09E-2) 6.7517E-1 (1.11E-16) 1.66E-6
jEdit 8.5207E-1 (3.77E-2) 7.1589E-1 (5.70E-3) 1.68E-6
EQ7.1714E-1 (1.34E-2) 6.0201E-1 (3.33E-16) 1.73E-6
velocity 7.0220E-1 (8.40E-3) 6.0896E-1 (4.50E-2) 1.61E-6
Tomcat 7.7295E-1 (1.40E-3) 7.3892E-1 (1.32E-2) 1.45E-7
Apache 7.4808E-1 (8.27E-3) 7.4787E-1 (3.33E-16) 6.58E-1
ML6.4966E-1 (1.58E-3) 6.1708E-1 (2.22E-16) 1.73E-6
xerces 7.1552E-1 (1.03E-2) 5.9892E-1 (6.03E-3) 1.71E-6
LC7.0859E-1 (1.89E-2) 6.2476E-1 (1.11E-16) 1.73E-6
xalan 7.6250E-1 (7.67E-3) 6.7732E-1 (2.31E-2) 1.71E-6
Figure 3: A12result between BiLO-CPDP and Auto-Sklearn
over 30 runs ( A12>0.5means BiLO-CPDP is better).
5.2 Comparison with Auto-Sklearn
5.2.1 Method. In principle, BiLO-CPDP is an AutoML tool that au-
tomaticallysearchesfortherightcombinationoftransferlearner
andclassifierandtheiroptimizedhyper-parametersettingsfora
given CPDP task. To validate its competitiveness from the perspec-
tiveofAutoML,we comparetheperformanceof BiLO-CPDP with
Auto-Sklearn6[8],astate-of-the-artandreadilyavailableAutoML
tool that can also optimize the combination and its parameters.
5.2.2 Results and Analysis. FromthecomparisonresultsofAUC
values shown in Table 4, we clearlysee the overwhelmingly supe-
rior performance of BiLO-CPDP versus Auto-Sklearn where the
AUC values obtained by BiLO-CPDP are all better than those of
Auto-Sklearn . In particular, all those better results, except on
Apache,arestatisticallysignificant,accordingtothe pvaluesshown
inthelastcolumnofTable4.Furthermore,asshowninFig.3,all A12
valuessuggestalargeeffectsize.Inparticular,weseeanoverwhelm-
ingA12=1, except only on ZxingandApache. These indicate that
the improvements on the AUC results brought by BiLO-CPDP over
that of the Auto-Sklearn are significantly large in general.
6https://automl.github.io/auto-sklearn/master/
580Theresultsarecausedbythefactthat Auto-Sklearn doesnot
haveabi-levelstructure,henceitencodesalltransferlearnerand
classifier combinations along with their corresponding hyper-para-
metersettingsintoanintegratedsolutionatasingle-level,which
is solved by the SMAC algorithm [ 17]. During its optimization pro-
cess,acombinationoftransferlearnerandclassifierisselectedfirst.
Thereafter, the variables corresponding to the hyper-parameters of
thechosen transferlearner andtheclassifier becomeactivewhile
the remaining variables are set to be dummy. By this means, the
total number of variables considered in Auto-Sklean goes up to
93,resultingaunnecessarilymuchlargersearchspacecomparing
with BiLO-CPDP .Giventhelimitedbudget, Auto-Sklearn therefore
ends up with a less effective exploration of both useful combina-
tions of transfer learners and classifiers and their hyper-parameter
settings.
Response to RQ2: Comparingwiththestate-of-the-artAutoML
toolAuto-Sklearn ,BiLO-CPDP achievessignificantlybetterresults
given a limited computational budget.
5.3 Comparison with Single-Level Variant
5.3.1 Method. Itisconservativetocuriousabouttheusefulness
broughtbythisbi-levelprogrammingformulationandwhynotsim-
ply formulating a single-level problem that consists of both combi-
nationandparameters.Thecomparisonwith Auto-Sklearn ,which
isatasingle-level,partiallyvalidatesthisconcern,buttheresults
canbebiasedbythe factthatitusesadifferentoptimizationalgo-
rithm.Tofully evaluatetheeffectiveness ofbi-levelprogramming,
we develop a single-level variant of BiLO-CPDP , dubbed SLO-CPDP ,
which differs from BiLO-CPDP only on the solution representation.
Specifically, SLO-CPDP issimilarto Auto-Sklearn inthesense
that they both work on single-level optimization — the transfer
learnerand classifier, togetherwiththeir hyper-parameters,areen-codedasasinglesolutionrepresentation.However,thedifferenceisthat
SLO-CPDP exploitsthe TPEalgorithmastheBayesianoptimizer,
whichisidenticalto BiLO-CPDP .Auto-Sklearn ,incontrast,uses
the classic SMAC algorithm that leverages Random Forest to build
the surrogate model.
5.3.2 Results and Analysis. FromtheAUCvaluesshowninTable5,
we observe a rather superior performance achieved by BiLO-CPDP
over SLO-CPDP .Specifically, BiLO-CPDP againobtainsabetterAUC
value on all 20 projects. In particular, all better results are with sta-
tistical significance ( p<.05), as shown in the last column of Table 5.
Furthermore, from Fig. 4, we find that the differences between the
AUCvaluesachievedby BiLO-CPDP andSLO-CPDP arewithalarge
effect size. Given such a result, we can infer that the ineffective-ness of
SLO-CPDP can be attributed to the enlarged search space
caused by the unwise coupling of transfer learner and classifier
combination along with their parameters at a single-level.
Response to RQ3: The bi-level programming in BiLO-CPDP con-
siderably contributes to its effectiveness. In contrast to the single-level where the combination and parameters are formulated in a
“flat" way, bi-level programming significantly reduces the search
spaceandsteerthesearchinahierarchicalmanner,leadingtobetter
performance under a limited budget.Table 5: Mean AUC (standard deviation) for BiLO-CPDP and
SLO-CPDP over 30 runs (gray=better; bold= p<.05).
Project BiLO-CPDP SLO-CPDP p-value
poi8.1703E-1 (4.38E-3) 5.7493E-1 (2.71E-1) 1.92E-6
synapse 7.1999E-1 (7.72E-3) 5.0601E-1 (2.33E-1) 1.73E-6
Zxing 6.3949E-1 (5.37E-3) 5.4209E-1 (1.46E-1) 1.92E-6
ant8.0006E-1 (8.26E-3) 6.3099E-1 (1.76E-1) 1.73E-6
log4j 8.4196E-1 (1.52E-2) 6.2807E-1 (2.49E-1) 2.60E-6
Safe7.9923E-1 (2.09E-2) 7.4254E-1 (3.96E-2) 5.74E-5
ivy8.0657E-1 (3.51E-3) 6.3528E-1 (1.77E-1) 1.73E-6
PDE6.8539E-1 (2.57E-3) 5.0874E-1 (2.52E-1) 1.73E-6
camel 6.2228E-1 (4.01E-3) 4.7330E-1 (2.15E-1) 5.22E-6
lucene 7.1065E-1 (8.13E-3) 5.8568E-1 (2.05E-1) 1.15E-4
JDT7.3705E-1 (1.09E-2) 6.2603E-1 (1.81E-1) 1.36E-5
jEdit 8.5207E-1 (3.77E-2) 5.5203E-1 (2.50E-1) 1.73E-6
EQ7.1714E-1 (1.34E-2) 5.4016E-1 (2.21E-1) 2.88E-6
velocity 7.0220E-1 (8.40E-3) 5.3123E-1 (2.11E-1) 1.73E-6
Tomcat 7.7295E-1 (1.40E-3) 5.6400E-1 (2.40E-1) 1.92E-6
Apache 7.4808E-1 (8.27E-3) 5.2924E-1 (1.23E-1) 1.01E-6
ML6.4966E-1 (1.58E-3) 5.8287E-1 (1.53E-1) 7.16E-4
xerces 7.1552E-1 (1.03E-2) 6.3384E-1 (1.29E-1) 6.87E-5
LC7.0859E-1 (1.89E-2) 5.3199E-1 (2.38E-1) 1.02E-5
xalan 7.6250E-1 (7.67E-3) 5.9866E-1 (2.21E-1) 5.79E-5
Figure4: A12resultbetween BiLO-CPDP and SLO-CPDP over30
runs (A12>0.5means BiLO-CPDP is better).
5.4 Impact of Budget for the Two Levels
5.4.1 Method. Inpractice,itisnotuncommonthattheresourcefor
defectprediction,particularthetimebudget,islimited.Inourexper-iments,thetotalbudgetallocatedto
BiLO-CPDP isonehourintotal,
followingthebestpracticeintheAutoMLcommunity[ 8].However,
the unique bi-level programming formulated in BiLO-CPDP allows
us a flexible control over the budget allocated to the two levels,hence it is interested to know how their budget allocations may
impacttheperformance.Tothisend,withintheonehourtotalbud-
get, we set two budget allocation strategies: one with high budget
totheupper-level,dubbed BiLO-CPDP(h) ,thatallows20seconds
for each low-level optimization, leaving more resources for explor-
ingthecombinationsattheupper-level.Notethat20secondsare
very shortfor some model trainingthus is counter-intuitive. This
isalsothedefaultsettingin BiLO-CPDP weusedforotherexperi-
ments.Anotheronepreserveshighbudgettothelow-level,dubbed
BiLO-CPDP(l) ,inwhichthelower-leveloptimizationisallocated
with 100 training and AUC evaluations. This allows a low-level
routinetoconsumeatleast80seconds(thesmallestamountoftime
required tocomplete 100 evaluationsamong all combinations)formore sufficient exploration of the hyper-parameters.
581Table 6: Mean AUC (standard deviation) for BiLO-CPDP(h)
and BiLO-CPDP(l) over 30 runs (gray=better; bold= p<.05).
Project BiLO-CPDP(h) BiLO-CPDP(l) p-value
poi8.1703E-1 (4.38E-3) 5.9447E-1 (2.74E-1) 3.85E-6
synapse 7.1999E-1 (7.72E-3) 6.1897E-1 (1.26E-1) 1.73E-6
Zxing 6.3949E-1 (5.37E-3) 6.1108E-1 (2.10E-2) 8.46E-6
ant8.0006E-1 (8.26E-3) 4.6150E-1 (3.31E-1) 1.91E-6
log4j 8.4196E-1 (1.52E-2) 5.1593E-1 (2.91E-1) 2.46E-6
Safe7.9923E-1 (2.09E-2) 7.1771E-1 (1.38E-1) 2.87E-6
ivy8.0657E-1 (3.51E-3) 5.9659E-1 (3.04E-1) 5.23E-6
PDE6.8539E-1 (2.57E-3) 4.3027E-1 (3.05E-1) 1.71E-6
camel 6.2228E-1 (4.01E-3) 3.9054E-1 (2.79E-1) 7.90E-6
lucene 7.1065E-1 (8.13E-3) 5.3341E-1 (2.69E-1) 1.12E-5
JDT7.3705E-1 (1.09E-2) 3.7420E-1 (3.51E-1) 3.85E-6
jEdit 8.5207E-1 (3.77E-2) 5.0339E-1 (3.30E-1) 1.72E-6
EQ7.1714E-1 (1.34E-2) 4.2035E-1 (3.22E-1) 8.37E-6
velocity 7.0220E-1 (8.40E-3) 5.0660E-1 (2.55E-1) 1.92E-6
Tomcat 7.7295E-1 (1.40E-3) 5.6762E-1 (2.87E-1) 2.50E-6
Apache 7.4808E-1 (8.27E-3) 7.1257E-1 (2.62E-2) 7.22E-6
ML6.4966E-1 (1.58E-3) 3.7510E-1 (3.07E-1) 5.70E-6
xerces 7.1552E-1 (1.03E-2) 4.8507E-1 (2.72E-1) 3.87E-6
LC7.0859E-1 (1.89E-2) 4.9327E-1 (3.00E-1) 1.23E-4
xalan 7.6250E-1 (7.67E-3) 4.9144E-1 (3.05E-1) 1.91E-6
Figure5: A12resultbetween BiLO-CPDP(h) and BiLO-CPDP(l)
over 30 runs ( A12>0.5means BiLO-CPDP(h) is better).
5.4.2 Results and Analysis. AsshowninTable6,wecanseethat
BiLO-CPDP(h) isoverwhelminglysuperiorto BiLO- CPDP(l) where
it obtains better AUC values on all 20 projects. In addition, from
the comparison results of A12shown in Fig. 5, we can see that
the differences between AUC values achieved by the two budget
allocation strategies are categorized to have a large effect size.
TheperformancedifferencesareduetothefactthattheCPDP
model training can be rather time-consuming and unfavorable,especially given a limited budget. Therefore, for
BiLO-CPDP(l) ,
oncea combinationoftransfer learnerandclassifier isselectedat
the upper-level routine, its initiative to favor better exploration of
thehyper-parametersatthelower-levelroutinecaneasilyconsumeasignificantamountofthebudget(themediancomputationaltime
isaround300secondsaccordingtoourofflinestatistics).Thishas
causedthecombinatorialspaceoftransferlearnersandclassifiersto
become severely under-explored. In contrast, by strictly restricting
the budget at the lower-level optimization routine, BiLO-CPDP(h)
suffers from a limited exploration of the hyper-parameter space,
butpermittingasufficientchancetoexploremanycombinations
of transfer learners and classifiers. From the results, it evidencesthatexploringthecombinationspaceismoreimportantthanusing
the hyper-parameter space under a limited budget.
Response to RQ4: Givenalimitedbudget,itisrecommendedto
allocatemoreexpendituretotheupper-leveloptimizationroutinein
BiLO-CPDP .Bythismeans,morecombinationsoftransferlearner
and classifier can be investigated even without fully optimized
hyper-parameters, which is more beneficial to performance.
6 RELATED WORK
In the past decades, machine learning classifiers have becomethe core techniques for defect prediction, in which the successcan be greatly affected by the setting of the classifiers’ hyper-parameters [
20]. This is a challenging issue, as Jiang et al .[18]
pointed out that simply using the default values are dreadful, caus-
ing severely bad performance of the prediction. The automatedparameter optimization for defect predictors is therefore crucial.
Indeed,alargescaleempiricalstudybyTantithamthavornetal .[43,
44]foundthatwell-tunedhyper-parameterscansignificantlyboost
the performance of the classifiers in defect prediction. Fu et al .[9]
even suggest that such optimization should become a standard
practice in every single Software Engineering task. In light of this,
Agrawal and Menzies [ 2] have applied Differential Evolution to
tune SMOTE,apre-processorforhandlingdataimbalance,forpre-
dictingsoftwaredefects.Theirworkfocusonwithin-projectdefect
prediction though. Similarly, DODGE[1] is a recent tool that opti-
mizestheparametersofdatapre-processorandclassifier.Although
they aim for within-project case, the combination of pre-processor
andclassifiercanberesembletoourCPDPtask.However,theirop-timizationassumesconservativehybridizationofalltheparameters
and the combinations as a single-level optimization problem.
Theimportanceofautomatedparameteroptimizationremains
stand in the context of CPDP, where the problem become evenmore complex as the parameters of transfer learners also come
into play. Qu et al .[35] have shown that the parameter settings of
classifiersforCPDPareevenmoreimportant.Afewautomatedop-timizersexistforCPDP,forexample,Öztürk[
31]andQuetal .[34]
examinevariousdifferentoptimizationalgorithmstotuneCPDP
models. Nevertheless, they focus only on the parameter tuning
whilstignorethecombinationoftransferlearnerandclassifierdur-
ing optimization. Indeed, Li et al .[21] further demonstrate that the
parameterinteractionsbetweentransferlearnerandclassifier,as
wellastheircombination,alsoplayanintegralroletotheprediction
performance. Auto-Sklearn [8], which is a widely-used generic
tooltotunearbitrarymachinelearningalgorithms,isalsohighly
potentialforCPDPtuning.However,again,itsdesignhasrestricted
thatthecombinationoftransferlearnerandclassifieralongwith
theirparametersneedtobetunedasasingleoptimizationproblem,
which worsen its performance compared with BiLO-CPDP ,a sw e
have shown in Section 5.
Although the potentials of bi-level programming have been ex-
plored for other Software Engineering problems, e.g., code smell
detection[ 39]andtestcasegeneration[ 40],tothebestofourknowl-
edge, its adoption has never been reported in the context of CPDP.
Ourworkisthereforeuniquetoallaforementionedtechniquesin
the sense that:
582•BiLO-CPDP is the first of its kind to formulate bi-level pro-
gramming for the parameter optimization of CPDP.
•BiLO-CPDP automaticallyoptimizesnotonlytheparameters,
butalsodiscoverthepossiblecombinationoftransferlearner
and classifier from a given portfolio.
•We show that exploring the combination of transfer learner
andclassifierismoreimportantthanthetheirparameters
tuning, the former should thus deserve more computational
budget. In this regard, the bi-level programming formulated
inBiLO-CPDP provides better flexibility to achieve such a
requirement of fine-grained budget allocation.
7 THREATS TO VALIDITY
Similar to many empirical studies in software engineering, our
work is subject to threats to validity.
Constructthreatscanberaisedfromtheexperimentuncertainty
caused by the learning and optimization. To mitigate this, we have
repeated 30runs for each techniquesand compare the techniques
using Scott-Knott test [ 27], supported by Wilcoxon signed-rank
test [48] andA12effect size metric [ 46]. Therefore, whenever we
report“A is better than B", we imply that A is indeed statistically
betterwithlargeeffectsize.ThesinglemetricAUCmayalsosub-
ject to such a threat. However, AUC was chosen mainly due toits parameter-free nature and high reliability as reported in the
machine learning community [24].
Internalthreatscanberelatedtotheparametersetting,whichin
our case the key parameter is the time budget for optimization. In-
deed,adifferentbudgetmayaffecttheresult,andthereforewehave
setatotalbudgetfollowingthestate-of-the-practicesuggestedin
theAutoMLcommunity[ 8],whichisreasonablegiventherequired
runs. Wehave also investigatedthe relativeimportance of budget
allocation between the upper- and lower-level in BiLO-CPDP.
External threats are concerned with whether the findings are
generailzabletootherprojects.Tomitigatesuch,asdiscussedinSec-tion4,our20projectscoverawidespectrumofthereal-worldcases
withdiversecharacteristics,eachofwhichwasusedasthetarget
domain data to be predicted using the other 19 ones as sources.
8 CONCLUSION
Thechoiceofcombinationoftransferlearnerandclassifieralong
withtheirhyper-parametersettingshaveasignificantimpacttothe
performanceofCPDPmodel.Inthispaper,wepropose BiLO-CPDP ,
a tool that is able to automatically develop a high-performance
CPDP model for the given CPDP task. Specifically, BiLO-CPDP , for
the first time, formulates the automated CPDP model discovery
problemfromabi-levelprogrammingperspective.Inparticular,theupper-leveloptimizationroutinesearchesfortherightcombinationof transfer learner and classifier while the lower-level optimization
routine optimizes the corresponding hyper-parameters associated
withthechosencombination.Furthermore,thehierarchicalopti-
mization paradigm allows a more flexible control of the compu-
tationalbudgetatbothlevels.Fromourempiricalstudy,wehave
shown that BiLO-CPDP
•automaticallydevelopsabetter CPDPmodelcomparingto
21 state-of-the-art CPDP techniques with hand-crafted com-
bination and reported parameter settings.•overwhelmingly outperforms Auto-Sklearn , a state-of-the-
artAutoMLtool,andthesingle-leveloptimizationvariant
ofBiLO-CPDP.
•allows software engineers to set more search budget for the
upper-level, which significantly boosts the performance.
BiLO-CPDP showcases the importance of automatically optimiz-
ing the combination of transfer learners and classifiers, together
with their parameters. This paves a new way to enable more in-
telligentparameteroptimizationandadaptationforCPDPmodel
building. In future, we seek to consider multiple objectives within
thebi-levelprogrammingandtoinvestigatemorepreciseeffectsof allocating budget between the two levels. We also plan to fur-therdistinguishbetweentheparametersfortransferlearnerand
classifier at the low-level, as it has been shown that the parameter
tuning of the former is more important than the latter [21].
ACKNOWLEDGEMENT
K. Li was supported by UKRI Future Leaders Fellowship (Grant No.
MR/S017062/1).
REFERENCES
[1]A.Agrawal,W.Fu,D.Chen,X.Shen,andT.Menzies.2019. Howto“DODGE"
Complex Software Analytics. IEEE Transactions on Software Engineering (2019),
1–1.
[2]Amritanshu Agrawal and Tim Menzies. 2018. Is "better data" better than "better
data miners"?: on the benefits of tuning SMOTE for defect prediction. In ICSE’18:
Proc.ofthe40thInternationalConferenceonSoftwareEngineering.ACM,1050–
1061.
[3]Andrea Arcuri and Lionel C. Briand. 2011. A practical guide for using statistical
teststoassessrandomizedalgorithmsinsoftwareengineering.In ICSE’11:Proc.
of the 33rd International Conference on Software Engineering. ACM, 1–10.
[4]JamesBergstra,RémiBardenet,YoshuaBengio,andBalázsKégl.2011. Algorithms
forHyper-ParameterOptimization.In NIPS’11:Proc.ofthe25thAnnualConference
on Neural Information Processing Systems. 2546–2554.
[5]James Bergstra, Daniel Yamins, and David D. Cox. 2013. Making a Science ofModel Search: Hyperparameter Optimization in Hundreds of Dimensions for
Vision Architectures. In ICML’13: Proc. of the 30th International Conference on
Machine Learning, Vol. 28. 115–123.
[6]MarcoD’Ambros,MicheleLanza,andRomainRobbes.2010. Anextensivecom-
parison of bug prediction approaches. In Proceedings of the 7th International
WorkingConferenceonMiningSoftwareRepositories,MSR2010(Co-locatedwith
ICSE), Cape Town, South Africa, May 2-3, 2010, Proceedings. 31–41.
[7]Matthias Feurer and Frank Hutter. 2019. Hyperparameter Optimization. In
Automated Machine Learning - Methods, Systems, Challenges. 3–33.
[8]MatthiasFeurer,AaronKlein, Katharina Eggensperger,JostTobiasSpringenberg,
Manuel Blum, and Frank Hutter. 2015. Efficient and Robust Automated Machine
Learning.In NIPS’15:Proc.ofthe2015AnnualConferenceonNeuralInformation
Processing Systems. 2962–2970.
[9]WeiFu,TimMenzies,andXipengShen.2016. Tuningforsoftwareanalytics:Isit
really necessary? Information and Software Technology 76 (2016), 135–146.
[10]Fred Glover and Manuel Laguna. 1998. Tabu Search. Vol. 1–3. Springer US,
2093–2229.
[11]Zhimin He, Fayola Peters, Tim Menzies, and Ye Yang. 2013. Learning from
open-sourceprojects:Anempiricalstudyondefectprediction.In 2013ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement.
IEEE, 45–54.
[12]Steffen Herbold. 2013. Training data selection for cross-project defect prediction.
InESEM’13: Proc. of the 9th International Conference on Predictive Models in
Software Engineering. 1–10.
[13]Steffen Herbold. 2017. A systematic mapping study on cross-project defect
prediction. CoRRabs/1705.06429 (2017).
[14]Steffen Herbold, Alexander Trautsch, and Jens Grabowski. 2018. A Comparative
StudytoBenchmarkCross-ProjectDefectPredictionApproaches. IEEETrans.
Software Eng. 44, 9 (2018), 811–833.
[15]Seyedrebvar Hosseini, Burak Turhan, and Dimuthu Gunarathna. 2019. A Sys-
tematic Literature Review and Meta-Analysis on Cross Project Defect Prediction.
IEEE Trans. Software Eng. 45, 2 (2019), 111–147.
[16]Seyedrebvar Hosseini, Burak Turhan, and Mika Mäntylä. 2018. A benchmark
studyontheeffectivenessofsearch-baseddataselectionandfeatureselectionfor
583crossprojectdefectprediction. InformationandSoftwareTechnology 95(2018),
296–312.
[17]FrankHutter,HolgerH.Hoos,andKevinLeyton-Brown.2011. SequentialModel-
Based Optimization for General Algorithm Configuration. In LION5: Proc. of the
5th International Conference Learning and Intelligent Optimization (Lecture Notes
in Computer Science), Vol. 6683. Springer, 507–523.
[18]Yue Jiang, Bojan Cukic, and Tim Menzies. 2008. Can data transformation help in
the detection of fault-prone modules?. In DEFECTS. ACM, 16–20.
[19]Marian Jureczko and Lech Madeyski. 2010. Towards identifying software project
clusters with regard to defect prediction. In PROMISE’10: Proc. of the 6th Interna-
tional Conference on Predictive Models in Software Engineering.9 .
[20]Akif Günes Koru and Hongfang Liu. 2005. An investigation of the effect of
module size on defect prediction using static measures. ACM SIGSOFT Software
Engineering Notes 30, 4 (2005), 1–5.
[21]KeLi,ZilinXiang,TaoChen,ShuoWang,andKayChenTan.2020.Understanding
the Automated Parameter Optimization on Transfer Learning for CPDP: An
Empirical Study. In ICSE’20: Proc. of the 42th International Conference on Software
Engineering. accepted for publication.
[22]ZhiqiangLi,Xiao-YuanJing,andXiaokeZhu.2018. Progressonapproachesto
software defect prediction. IET Software 12, 3 (2018), 161–175.
[23]Nachai Limsettho, Kwabena Ebo Bennin, Jacky W Keung, Hideaki Hata, and
KenichiMatsumoto.2018. Crossprojectdefectpredictionusingclassdistribution
estimationandoversampling. InformationandSoftwareTechnology 100(2018),
87–102.
[24]Charles X. Ling, Jin Huang, and Harry Zhang. 2003. AUC: a Statistically Consis-
tentandmoreDiscriminatingMeasurethanAccuracy.In IJCAI’03:Proc.ofthe
8th International Joint Conference on Artificial Intelligence. 519–526.
[25]Thilo Mende. 2010. Replication of defect prediction studies: problems, pitfalls
andrecommendations.In PROMISE’10:Proc.ofthe6thInternationalConference
on Predictive Models in Software Engineering.5 .
[26]Thilo Mende and Rainer Koschke. 2009. Revisiting the evaluation of defect
prediction models. In PROMISE’09: Proc. of the 5th International Workshop on
Predictive Models in Software Engineering.7 .
[27]Nikolaos Mittas and Lefteris Angelis. 2013. Ranking and Clustering Software
Cost Estimation Models through a Multiple Comparisons Algorithm. IEEE Trans.
Software Eng. 39, 4 (2013), 537–551.
[28]NachiappanNagappan, ThomasBall, andAndreasZeller. 2006. Miningmetrics
to predict component failures. In Proceedings of the 28th international conference
on Software engineering. 452–461.
[29]JaechangNam,SinnoJialinPan,andSunghunKim.2013. Transferdefectlearning.InICSE’13:Proc.ofthe35thInternationalConferenceonSoftwareEngineering.382–
391.
[30]ChaoNi,Wang-ShuLiu,XiangChen,QingGu,Dao-XuChen,andQi-GuoHuang.
2017. A cluster based feature selection method for cross-project software defect
prediction. Journal of Computer Science and Technology 32, 6 (2017), 1090–1107.
[31]Muhammed Maruf Öztürk. 2019. The impact of parameter optimization of
ensemblelearningondefectprediction. TheComputerScienceJournalofMoldova
27, 1 (2019), 85–128.
[32]FayolaPeters,TimMenzies,LiangGong,andHongyuZhang.2013. Balancing
privacy and utility in cross-company defect prediction. IEEE Transactions on
Software Engineering 39, 8 (2013), 1054–1068.
[33]Shaojian Qiu, Lu Lu, and Siyu Jiang. 2018. Multiple-components weights modelfor cross-project software defect prediction. IET Software 12, 4 (2018), 345–355.
[34]YubinQu,XiangChen,YingquanZhao,andXiaolinJu.2018. ImpactofHyperPa-rameterOptimizationforCross-ProjectSoftwareDefectPrediction. International
Journal of Performability Engineering 14, 6 (2018), 1291–1299.
[35]YubinQu,XiangChen,YingquanZhao,andXiaolinJu.2018. ImpactofHyperPa-rameterOptimizationforCross-ProjectSoftwareDefectPrediction. International
Journal of Performability Engineering 14, 6 (2018).
[36]Foyzur Rahman, Daryl Posnett, and Premkumar T. Devanbu. 2012. Recalling the
"imprecision" of cross-project defect prediction. In FSE’12: Proc. of the 20th ACM
SIGSOFT Symposium on the Foundations of Software Engineering. ACM, 61.
[37]DuksanRyu,OkjooChoi,andJongmoonBaik.2016. Value-cognitiveboosting
with a support vector machine for cross-project defect prediction. Empirical
Software Engineering 21, 1 (2016), 43–71.
[38]DuksanRyu,Jong-InJang,andJongmoonBaik.2015. Ahybridinstanceselection
using nearest-neighbor for cross-project defect prediction. Journal of Computer
Science and Technology 30, 5 (2015), 969–980.
[39]DilanSahin,MarouaneKessentini,SlimBechikh,andKalyanmoyDeb.2014.Code-
smell detection as a bilevel problem. ACM Transactions on Software Engineering
and Methodology (TOSEM) 24, 1 (2014), 1–44.
[40]Dilan Sahin, Marouane Kessentini, Manuel Wimmer, and Kalyanmoy Deb. 2015.
Modeltransformationtesting:abi-levelsearch-basedsoftwareengineeringap-
proach.Journal of Software: Evolution and Process 27, 11 (2015), 821–837.
[41]MartinJ.Shepperd,QinbaoSong,ZhongbinSun,andCarolynMair.2013. Data
Quality:SomeCommentsontheNASASoftwareDefectDatasets. IEEETrans.
Software Eng. 39, 9 (2013), 1208–1215.[42]Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. 2018. A Review on Bilevel
Optimization:FromClassicaltoEvolutionaryApproachesandApplications. IEEE
Trans. Evolutionary Computation 22, 2 (2018), 276–295.
[43]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto.2016. Automatedparameteroptimizationofclassificationtechniquesfordefectpredictionmodels.In ICSE’16:Proc.ofthe38thInternationalConference
on Software Engineering. 321–332.
[44]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto. 2019. The Impact of Automated Parameter Optimization on Defect
PredictionModels. IEEETransactionsonSoftwareEngineering 45,7(2019),683–
711.
[45]Burak Turhan, Tim Menzies, Ayse Basar Bener, and Justin S. Di Stefano. 2009.On the relative value of cross-company and within-company data for defect
prediction. Empirical Software Engineering 14, 5 (2009), 540–578. https://doi.org/
10.1007/s10664-008-9103-7
[46]AndrásVarghaandHaroldD.Delaney.2000. ACritiqueandImprovementofthe
CL Common Language Effect Size Statistics of McGraw and Wong.
[47]Heinrich Von Stackelberg. 2010. Market structure and equilibrium. Springer
Science & Business Media.
[48] Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods.[49]
RongxinWu,HongyuZhang,SunghunKim,andShing-ChiCheung.2011.ReLink:
recoveringlinksbetweenbugsandchanges.In ESEC/FSE’11:Proc.of19thACM
SIGSOFTSymposiumontheFoundationsofSoftwareEngineeringand13thEuropean
Software Engineering Conference. 15–25.
[50]Feng Zhang, Audris Mockus, Iman Keivanloo, and Ying Zou. 2014. Towards
building a universal defect prediction model. In Proceedings of the 11th Working
Conference on Mining Software Repositories. 182–191.
[51]YumingZhou,YibiaoYang,HongminLu,LinChen,YanhuiLi,YangyangZhao,
JunyanQian,andBaowenXu.2018. HowFarWeHaveProgressedintheJourney?
An Examination of Cross-Project Defect Prediction. ACM Trans. Softw. Eng.
Methodol. 27, 1 (2018), 1:1–1:51.
[52]ThomasZimmermann,NachiappanNagappan,HaraldC.Gall,EmanuelGiger,
and Brendan Murphy. 2009. Cross-project defect prediction: a large scale ex-periment on data vs. domain vs. process. In ESEC/FSE’09: Proc. of the 7th joint
meetingoftheEuropeanSoftwareEngineeringConferenceandtheACMSIGSOFT
International Symposium on Foundations of Software Engineering. ACM, 91–100.
584