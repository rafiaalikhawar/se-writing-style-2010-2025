Team Discussions and Dynamics During DevOps Tool Adoptions
in OSS Projects
Likang Yin
DECAL and CS Department
University of California, Davis
lkyin@ucdavis.eduVladimir Filkov
DECAL and CS Department
University of California, Davis
vfilkov@ucdavis.edu
ABSTRACT
InOpenSourceSoftware(OSS)projects,pre-builttoolsdominate
DevOps-orientedpipelines.Inpractice,amultitudeofconfiguration
management, cloud-based continuous integration, and automated
deploymenttoolsexist,andoftenmorethanoneforeachtask.Tools
areadopted(andgivenup)byOSSprojectsregularly.Priorwork
hasshownthatsometooladoptionsareprecededbydiscussions,
and that tool adoptions can result in benefits to the project. But
important questions remain: how do teams decide to adopt a tool?
Whatisdiscussedbeforetheadoptionandforhowlong?And,what
team characteristics are determinant of the adoption?
In this paper, we employ a large-scale empirical study in or-
dertocharacterizetheteamdiscussionsandtodiscerntheteam-
leveldeterminantsoftooladoptionintoOSSprojects’development
pipelines.Guidedbytheoriesofteamandindividualmotivations
and dynamics, we perform exploratory data analyses, do deep-dive
casestudies,anddevelopregressionmodelstolearnthedetermi-
nants of adoption and discussion length, and the direction of their
effectontheadoption.Fromdataofcommitandcommenttracesof
large-scale GitHub projects, our models find that prior exposure to
a tool and member involvement are positively associated with the
tooladoption,whilelongerdiscussionsandthenumberofnewer
teammembersassociatenegatively.Theseresultscanprovideguid-
ancebeyondthetechnicalappropriatenessforthetimelinessoftool
adoptions in diverse programmer teams.
Our data and code is available at https://github.com/lkyin/tool_
adoptions.
ACM Reference Format:
Likang Yin and Vladimir Filkov. 2020. Team Discussions and Dynamics
During DevOps Tool Adoptions in OSS Projects. In 35th IEEE/ACM Interna-
tionalConferenceonAutomatedSoftwareEngineering(ASE’20),September
21–25,2020,VirtualEvent,Australia. ACM,NewYork,NY,USA,12pages.
https://doi.org/10.1145/3324884.3416640
1 INTRODUCTION
OSS software development practices are evolving away from de
novoprogrammingandtowardadoptingpre-madetoolsforvarious
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416640tasks,whicharethenintegratedintoexistingdevelopmentandpro-
ductionpipelines[ 14,50].Partofthereasonforthishasbeenthe
popularity of the DevOps software development movement, which
seeks to bring changes into production as quickly as possible with-
outcompromisingsoftwarequality,primarilybyautomatingthe
processes of building, testing, and deploying software. In practice,
DevOps is supported by a multitude of configuration management,
cloud-based continuous integration, and automated deployment
tools, short-circuiting the need for coding from scratch [ 22,24].
Usingpre-madetoolscanshortenthedevelopmentprocess,solongasanappropriatesetoftoolsisusedandproperlyintegratedintoa
development pipeline.
Tooladoptiondecisions,however,areoftennotwellinformed.
DevOps engineers frequently lack the decision-making support to
help them discern the best choices among the many tools avail-
able[24].Inlargepart,that’sbecausecurrentempiricalevidenceon
theeffectivenessofDevOpspracticesis,atbest,fragmentedandin-complete.WhilequestionsaboutbesttoolsandpracticesinDevOpsaboundinonlineforums,theexistinganswersaretypicallygeneric
rules ofthumb, ordated advice,mostly basedon third-partyexpe-
riences, often non-applicable to the specific context. While they
likelyconsiderthatscatteredknowledge,teamsseemtoleverage
their strengths and experiences in making tool adoption decisions.
Some tool adoption events in projects are preceded by a discussion
amongteammembersontheissuesinvolved[ 24],butitisnotclear
what is discussed in them among team members and how those
discussions correlate with adoption decisions.
Moreover, instituting a project-wide change is a complex social
process when there are many stakeholders [ 44]. Adopting a new
tool,e.g.,canrequireateam-wideadjustmentinpracticesthataffect
every developer–thus they are all stakeholders in the adoption
decision.Thisisespeciallytruewhentheteamismorediversein
terms of developer tenure with the project, their prior exposureto the tool being considered for adoption, and their day-to-day
involvementintheproject.Naturally,supportersanddetractorscananddoariseoverdecisionswhendevelopersespousedifferentviewstowardatool,resultinginchampionsanddetractors,andsometimesargumentscangetemotional[
20,27].Manyoftheseindividualand
team-levelfactorsmaycontributetotheultimateadoptiondecision,
but which ones actually do? And in what proportion?
Inspired by the availability of large, comprehensive data sets on
tool adoptions from diverse GitHub projects, here we undertakeboth qualitative and quantitative methods to uncover the socialdeterminants of team discussions and dynamics leading to tooladoption events in OSS projects. We operationalize our study atthe team-level instead of at the individual level.
Centraltoour
studyistheanalogythataprojectteamadopting atoolisakinto
6972020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
acommunity adopting anidea.We use theories on diffusion of
innovation and individual and team social judgment to guide us in
discerning the important factors underlying tool adoption.
Westartfromadatasetofsocialandtechnicaltracesfromalarge
number of GitHub projects, together with project-tool adoption
dataforeach.Then,weperformexploratorydataanalyses,dodeep-
dive case studies, and built regression models to determine how
team properties and their communication behaviors are associated
with tool adoptions. We find that:
•Teamdynamicschangesaroundadoptiontimeinsubstantial
ways, and some of those changes remain with the project.
•Projectteamsundergomeaningfuldiscussionsandexhibit
significant dynamics changes in the period before and after
a new tool is adopted.
•Influencerdeveloper’sparticipationisassociatedwithshorter
discussion length, and likelier tool adoption.
•Newdevelopersarepositivelyassociatedwithlongerdiscus-
sion length and lower adoption success.
Related questions have been asked before, in narrower settings.
Zhuetal.[ 54]usecodeaccept/ignoreratetocomparethegoodness
betweenissuetrackerandpullrequestsystems,butlessfocuson
the adoption dynamics. Using surveys Xiao et al. [ 52] find that
coworker recommendation is a significant determinant of security
tool use. Witschey et al. [ 51] find that the strongest predictor of
using security tools is the ability to observe their coworkers using
thosetools.Thesefindingsarebasedonasmallnumberofcommer-
cialsoftwareprojectsandcanbedifficulttogeneralize,especially
to OSS projects. Moreover, surveys, by their nature are based on
individual experience and feedback. Kavaler et al. [ 24] have looked
morecomprehensively ata largeswathof JavaScriptprojects,but
have focused more on the effects of tool choices on software en-
gineeringoutcomes,andfoundthatsomechoicesarebetterthan
others.
2 BACKGROUND AND THEORIES
Herewepositionourcurrentworkinthespaceofrepresentative
prior work on tools, tool adoptions, and OSS teams, as well as
present two most relevant sociological theories that will guide our
hypotheses and research questions.
2.1 Tools, Teams and Adoptions
Softwaredevelopersusevarioustechniquestoimplementandmain-
tain software, including, attimes, adopting new technology [ 7]. T.
Gorschek et al. [ 12] describe five different stages in technology
transfer,includingidentifyingpotentialproblems,formulatingis-
sues,proposingcandidatesolutions,validations,andreleasingso-
lutions. S.LPfleeger [ 32] proposeda modelof technology transfer
thatcanbetailoredtoaparticularorganization’sneeds.Riemen-
schneider et al. [ 35] find that opinions of developers’ coworkers
andsupervisorsonusingsometechnologymatterstoanindividual
developer when they consider whether to use this method. Our
paper extends such implications to tool adoptions in OSS projects.
Marlowetal.[ 29]findthatlengthoftenuremaybeassociated
withattitudestowardnewtools.Moreseniordevelopersmayget
moreattachedtoaworkingstyle,thusmakingthemlessflexibleto adopt new tools. New developers [
10], in contrast, need lesstime to adapt to a new working style due to lack of history in
the project, and eagerness to learn/follow technology trends. Xiao
et al. [52] find developers are more likely to adopt a tool if their
peers or co-workers are using or have used it. Also, if experienced
individuals join a project, their knowledge and social ties move
withthemtotheproject[ 1,49].Thus,thesum-knowledgeofthe
wholeteamisconstantlychangingastheprojectevolves.Likewise,
previous research has found that emotional contagion from co-
workerscanpercolatewithintheteam[ 15,39]causingaslowdown
in communication and productivity.
With publicly available traces of working records and discus-
sions, it is possible for researchers to study team dynamics before,
during, and after a change . However, studying tool adoptions is
complex because of the variety of contributing factors. Previous
work [16] has found that changing previous practices can force
software developers out of their safe zone, resulting in nonconfor-
mity with the methodology. Zelkowitz et al. [ 53] find that many
softwareprofessionalsareresistanttochange,andthattimingisofimportancewhenadoptingnewtechnology.Johnsonetal.[
21]con-
siderthathavingintuitivedefectpresentationmaycontributetothe
willingness of using static analysis tools. Related literature [ 51,52]
showsthatdevelopers’socialnetworks(e.g.,throughdiscussions
abouttools[ 43])benefitthespreadoftools.Polleretal.[ 33]pointed
out correlations between organizational factors and the success of
security practices.
2.2 Diffusion of Innovations Theory
Diffusionofinnovations(DOI)theorywasfirstproposedbyRogersin1983[
36],andhassincebecomepopularinsocio-technicalfields.
Rogers considered diffusion as the process by which an innova-tion unfolds over time through communication channels among
thepopulation,andfoundthatsomepropertiesofinnovationare
crucial to an adoption: perceived usefulness, perceived complexity,
peer pressure, etc. The innovation diffusion process at an orga-
nizationallevelcanbesummarizedashavingtwostages[ 36,40]:
initiation(perceivingtheissues,andknowingtheycanbeaddressedbyadoptingcertaininnovations),andimplementation(adoptingthe
innovation and customizing it to fit own culture if necessary, then
theteam-collectedinformationreinforces/devitalizetheadoption
until the innovation becomes a part of the organization).
In the context of software engineering, DOI theory can be of
vital value to offer understandings of the phenomenology and con-
sequences. E.g., toolbuilders want to know ifanyone will use the
tooltheybuilt,andhowtopersuadethecommunitytouseit.For
generaldevelopers,theywanttoknowifthereexisttoolstohelp
them be more efficient. However, even if a tool can be beneficial,
individualresistancemaystillexist.Toreducesuchindividualresis-tance,organizationalmandatehasbeenshowntohaveinfluenceon
adoptions [ 16,35]. However, those authors also find that the orga-
nizational mandate is not sustainable compared to other factors. A
catalog of non-coercive adoption patterns has been proposed [ 40],
tohelporganizationsachievesuccessfulsoftwareengineeringprac-
tices in a more persuasive manner.
In GitHub OSS projects, Bertram et al. [ 2] found that communi-
cation and knowledge sharing exist for coordinating work in issue
698trackers. This suggests that developers within a project communi-
cate and learn from each other [ 45]. Moreover, knowledge sharing
exists even across projects. Singer et al. [ 40] noted that some devel-
opers use GitHub to learn how other projects use the same tools in
theirprojects.Thus,developersintheGitHubecosystem,andinitstightersub-ecosystems,shareanddistributeknowledgeabouttoolsthroughparticipationindifferentprojects,thuscreatingadiffusion
process.
2.3 Social Judgment Theory
SocialJudgmentTheory(SJT)[ 4]wasproposedtostudyhowpeople
self-persuade to adopt a new idea when they encounter it. Accord-
ing to SJT, when people are exposed to new information or a newenvironment, they tendto consider three things. First is theirpre-
viously formed attitude, or anchorto which they compare the new
idea[46].Thentheylookatavailable alternatives.Inthisprocess,
peoplerecognizethemselves,formtheirviews,andexpresstheir
ideas.Finally,thereisthe egoinvolvement orthecentralityofthe
issue being considered to a person’s life, which can explain whypeople can accept some ideas and novelty easier than others. In-
dividualswithhighegoinvolvementonanissuetendtobemore
passionate on the topic and are more likely to evaluate all possible
positions[ 13].High-egoindividualsalsohavealargerlatitudeof
rejection,anditisdifficulttopersuadethemtoadoptanewidea.Incontrast,low-egoindividualstendtohavealargernon-commitment
latitude,meaningtheyoftendonot takeastanceonanissue,and
they do not care much about the arguments.
Thereareseveralwaystoaggregatethejudgmentsofindividuals
fromagroupintoa groupjudgment [11,37].Mathematicalaggrega-
tionamounts to simple counting/averaging of individual judgment.
On the other hand, behavioral aggregation is the outcome of group
members agreeing after discussing the matter. Experimental ev-idence suggests that group judgment is generally more accurate
thanindividualjudgment,andhowitismeasuredcanbesignificant
to the outcome [ 11]. However , others found that the superiority
of group judgment is due to reliability produced by larger sam-
ples [25].
In the context of OSS projects, team discussions on tool adop-
tions transpire during which possible options are proposed. These
discussions can be considered a form of behavioral aggregation of
individual opinions. However, group judgment may get biased, as
high-egoindividualswith strongopinionscanpotentiallyswaya
group decision in their direction, even if it offers no overall benefit.
3 HYPOTHESES AND RESEARCH QUESTIONS
Central to this paper is the analogy that adopting a tool is akinto adopting an idea. An OSS project involves a social organiza-tion in which activities are coordinated through communicationto achieve both individual and collective goals. By coordinatingactivities, the organizational structure has to be created to assist
individualstocommunicate.InGitHubprojects,inparticular,the
twomaincommunicationchannelsarethroughcodecommitting
andissueposting.Moreover,contributingcodechangesandapprov-
ing others’ pull requests suggests that the developers are mutually
awareofeachother,andthisformsthebasisforcommunications
and discussions.The DOI theory suggests that innovations and new technolo-
gies can be spread to teams through diffusion. For tool adoptionin GitHub projects, this diffusion happens through information
exchangewithinandbetweenprojects,throughtheirdevelopers.
One mechanism is through reading/participating in discussions
thatareaccessibletoallteammembersinaproject.Thepublicly
available traces of commits and comments allow us to study dis-
cussiondynamics ofhow tools areperceived, discussed,andthen
finally adopted.Thus, ahypothesis arisingfrom DOIis thatdevel-
opers who have previously had exposure to certain tools will be
moreknowledgeableofthemandcontributetoadiscussiononit,
to make the adoption process smoother and faster.
Ontheotherhand,SJTsuggeststhatsinceitislikelysomedevel-
operspreferadoptingatoolmorestronglythanotherdevelopersdo,theformernaturallywillhavehigh-egoonadoptions.Ahypothesis
arisingisthatdeveloperswhopostmorecommentsontoolsthan
others will be the influencers in the discussions, and will affect
the direction of the adoption. Moreover, contributing to cognitive
dissonance,peoplewillreactmorestronglytonegativeinformation
than positive information [ 42]. Therefore, another hypothesis is
thatthepeople’sdiscussionsentiment(positiveornegative)may
correlate with eventual tool adoption.
WeformalizetheaboveintoourResearchQuestions(RQs),asfol-
lows.First,weseektouncoverthepatternsandchangeshappening
at team-level, in the period surrounding tool adoption events.
RQ1:What is the team dynamics when a project team goes
through the process of adopting a new tool?
Then,wewanttounderstandwhatgoesoninthediscussions
before and after the tools are adopted.
RQ2:Whattopicsarediscussedduringtooladoptions?Howare
people’s sentiments evolving toward the tools they are adopting?
Nextwelookatnotableindividualsinthediscussions.According
to the aforementioned theories, people who care the most and
comment voluminously, i.e. influencers, may play a significant role
in the innovation diffusion process. Namely,
RQ3:Aretooladoptioneventsassociatedwithinfluencers?How
much does their opinion weigh in on others?
Inthefinalthrust,tocomprehensivelyunderstandtooladoption
and discussions, we quantitatively model adoptions and discussion
length in terms of our chosen variables using multiple regression.
RQ4:What are the quantitative determinants of project-wide
tool adoption and the preceding discussion length?
4D A T A
We start from a data set by Trockman et al.[ 48] of GitHub npm
projects that have adopted any of 19 different DevOps tools1.
They were collected by gathering tool badges from the projects’
README.mdfiles.Someprojectsusebadgestosignalimportant
information [ 34], e,g, code coverage percentage
 ,
number of downloads per month
 , package dependen-
cies
 , and continuous integration status
 . The
tooladoptiondataiscurrentasofJan2019.Thedatasetconsistsof
52,923distinctGitHubprojects,theadoptedtools,andtheadoption
dates. In total, 96,176 tool adoptions are identified, or about 2 tool
adoptions per project on average. Among them, 28,430 projects
1Data and code is available at https://github.com/lkyin/tool_adoptions
699Table 1: Tool adoption event summary
Tool Task Class Per Tool Per Tool Category
karma
Browser4116
6157 sauce 1654
selenium 387
coveralls
Coverage14430
21626codecov 4239
codeclimate 2731
codacy 213
coverity 13
uglify
Minifier2018
2124 minimist 62
minifier 44
mocha
Testing33280
47119 istanbul 11864
jasmine 1975
eslint
Linter10978
18969 standard 4827
jshint 3164
snykDependence Manager179181gemnasium 2
adopted only one tool, 11,272 projects adopted two tools, 8,900
projects adopted three tools, and 3,378 projects adopted four tools.
Projectswhichhaveadoptedmorethanfourtoolsarefewerthan
2% of the total.
The collectedtools can beclassified into the followingsix cate-
gories according to their functionality. We illustrate each category
with an example tool. Browsertesting: e.g.,Selenium is an auto-
matedtestingframeworkforautomatedtestingofwebapplications
and User Interfaces (UIs) [ 6].TestCoverage : e.g.,Coveralls mea-
suressoftwarequalityintermsoftestcaseslinecoverage,function
coverage,branchcoverage,andstatementcoverage[ 17].Minifier:
e.g.,Uglifyjsis a JavaScript compressor tool used to merge and
minimizeJSresourcesbyremovingblankrows,shortenthevari-
ablesandfunctionsnamestomakethewebapplicationsloadfaster
[41].Testing: e.g.,Mochahas good support for testing asynchro-
nous code, allowing any use of failed exception test libraries [ 9].
Linters:e.g.,ESLintisaplug-inJavaScriptcodestyle/errordetec-
tion tool [ 47], thereby achieving effective control of the quality
oftheproject. Dependency managers :e.g.,Snykhelpsdevelopers
track the dependency tree to find which module introduces the
vulnerability [ 30,38]. The categories and the summary statistics of
adoption data is given in Table 1.
4.1 Data Collection and Cleaning
We use the GitHub API (v3) [8] to collect and extract historical
recordsofthecommitsanddiscussionsfromtheGitHubprojects.
For commits data, the author is the one who made the changes,
while the committer is the one who committed the changes to
GitHub. Thus, we align the ‘author’ with each commit. For discus-
sions, since the commit messages are usually not meaningful [ 19],
we only collect and use issues and comments as their discussions.
Someprojectshaveverylowlevelsofactivity,whileothershave
team sizes that may be too small to study their dynamics. To avoid
those, in this paper commits and comments of only the durable
andpersistentprojectsarecollected.Wesetthreerequirements:(1)
Durable:Theprojectsthathavecommitrecordsspanningatleasttwoyears.(2) Persistent :Theprojectshavingatleast 6monthsof
commit history before and after the tool adoption event and have
atleast50totalcommits.(3) Related:Theprojectshavingatleast
one comment related to the adopted tools. After filtering, the final
data set consists of 684 distinct GitHub projects.
Since we focus on discussion comments which specifically as-
sociate with tools, we filter out all discussion comments which do
not explicitly mention a tool name (of the 19 tool names), with the
following exception. If an issue starts a thread and mentions a tool
name,therepliestothatthreadarelikelyapartofthediscussion,
therefore, all comments following the tread are also included even
if they do not contain a tool name. On the other hand, some issues
are automatically posted, e.g., by continuous integration (CI) tools;
we identify the comments by checking the title of the comments
(e.g., ‘[Snyk Update]’) and exclude the comments from the data set.
Weidentifyandremovethefollowingstringsfromcomments
that do not comprise text, as they would bias downstream analysis:
non-ASCIIcharacters(bycheckingifallcharactersarefrom ‘u4e00’
to‘u9fff’),codesnippets(bycheckingifthecommentsareenclosed
with single or triple backticks), URLs (by using the regular expres-
sion from the remodule in Python 3.7 with pattern ‘https?://S+’ ),
andemojisbycheckingtheencodingofthecharacters.Finally,since
somecommentsmentionmultipletoolsatthesametime,therefore,theyarecountedmultipletimes.Toavoidsuchduplicationbythose
comments (about 1 .77% of the total), we manually re-label these
comments with only one tool, we achieve this mainly by referring
the outcome of the adoption (which one is finally adopted) and
context of the discussion.
4.2 Variables Used
The variables that we use in this study have been identified based
on our discussion and consideration of the underlying theories.
They include the following.
Outcomes : Adoption success and discussion length. Adoption
success(adoption_success) isabinaryvariable(0="No"or1="Yes")
indicating whether a tool was adopted in a project. A successful
adoptionisifatoolisbeingusedinaproject,regardlessofwhether
discussionsoniteverhappened.Anunsuccessfuladoptionisifa
project’s team had a discussion on a tool yet they never adoptedit.Discussion length (discussion_length) for a project and a tool is
calculated as the number of months from the first day the tool was
mentionedintheprojectdiscussion,untilthetoolwasadopted.The
discussion length suggests how long it takes for teams to reach an
agreement and eventually adopt the tool, although in practice can
be much longer if the team keeps coming back to discussing a tool
afterlongerbreakswithoutmentioningit.Toaddressthosecases
we introduce a control variable num_mentions, see below, which
measures the volume of tool mentions in a discussion.
Controls: Project Age, Number of Commits, Number of Com-
ments/Mentions. Projectage( project_aдe)atthetimeofadoption
of a tool is the number of months from the first commit date in the
project to that specific tool’s adoption date. Number of Commits
(num_commits)atthetimeofadoptionisthenumberofcommits
made during the discussion (based on the discussion length above)
on that tool, in the project. Number of Comments ( num_comments )
/Mentions( num_mentions )isdefinedasthenumberofcomments
700Table2:Summarystatisticsforour10variablesoverall1,085
adoption events (after removal of top 2% as outliers)
Statistic Mean St. Dev. Min Max
adoption_success 0.798 0.402 0 1
discussion_length 9.927 11.428 0.033 56.633
project_age 33.542 20.688 6 100
num_comments 15.852 32.792 1 272num_commits 338.246 588.521 0 4,163
num_new_dev 18.852 32.933 0 252num_w_tool_expos 7.724 13.973 0 133num_involved_dev 2.373 2.377 1 23
num_neg_dev 0.678 1.077 0 7
num_pos_dev 0.641 0.975 0 8
(including all follow-up comments in the same thread) made in
that tool’s discussion interval, while the number of mentions only
countsthenumberofcommentswhichexplicitlymentionthetools,
foragivenproject. Tool(tool)isthefullnameinlowercaseofthe
corresponding tool (e.g., eslint).
TeamMetrics:We calculate team measures for each project, for
eachtooldiscussion. NewDevelopers( num_new_dev)attimetin
the project are those developers who have made their first con-tribution (either by committing code changes or participating indiscussions) within the 3months prior to
t. For convenience, we
refer to all other developers who are not new developers as Senior
developers. Developers with prior tool exposure ( num_w_tool_expos)
are the developers who had already been in a project before time t
thathadusedthattool,andhavecommittedcodechangesbefore
timetto current project (but after their contributions to the other,
tool using project). In contrast, the developers without prior expo-
surearetheoneswhodidnotparticipateinaprojectthathadusedthetoolbeforetime
t.InvolvedDevelopers( num_involved_dev)are
thedeveloperswhohavebeeninvolved(i.e.,participated)inthetool
discussion. Positive Developers ( num_pos_dev)are the developers
whohavepostedoverallmorecommentswithpositivesentiment
than negative sentiment in the tool discussion. While Negative De-
velopers (num_neд_dev)are the opposite. The descriptive statistics
for the metrics are shown in Table 2.
5 METHODS
5.1 Topics Identification
We use Latent Dirichlet Allocation (LDA) [ 3] to study topics in
discussions.LDAisastatisticaltechniqueusedtoidentifytopics
in large documents and high-frequency words associated with the
topics. LDA yields a topic probability distribution for each docu-
ment, enabling topic clustering and/or text classification across all
documents in a set.
Before training the LDA model, we pre-processed the GitHub
commentsbytokenizingwiththeApacheOpenNLPlibrary[ 26],
andstemmingwiththePorterstemmer(thisremovedallstop-words
from the comments). Due to the large number of discussions we
have, many high-frequency words are not very meaningful. To get
a corpus with a higher concentration of topics [ 28], we removedboththe5%wordswiththelowestfrequency(e.g.,usernames)and
the5%wordswiththehighestfrequency(e.g.,bugs).Afterthis,the
LDA model is more able to distinguish topics from each other.
5.2 Sentiment Analysis
A sentiment analysis tool identifies the emotional characteristic of
a text, typically in terms of its aggregate positivity or negativity.Many sentiment analysis tools used in software engineering are
trained solely on social media corpora, e.g., Twitter, Facebook, and
Yelp. However, some words in the context of software engineer-
ing can represent a different meaning compared to social mediacorpus [
23,39]. For example, to ‘kill a process’ is neutral in the
context of programming, while some sentiment analysis predictors
trained on other media corpus, treat it as a strong indicator fornegative emotion. To avoid such issue, we use Senti4SD to pre-dict the sentiment of the GitHub comments. It has been trainedon Stack Overflow annotated comments, and it has been shownto be more accurate in the software engineering domain [
5,18].
Senti4SD yields ternary sentiment for each comment, i.e., positive,
neutral,ornegative.Thefollowingareexamplesfromourdataof
positive,neutral,andnegativecomments,asperSenti4SD(sensitive
words are anonymized and replaced by <notation>). Positive:“sure
<user>! appreciate your point, thanks for the suggestion.” Neutral:
“Let’s have this project actually be <tool> and not try to duplicate
onourownwhat<tool>doesinternally.Let’sjustlet<tool>handleallmergingitself.”
Negative:“BTW,<tool>failedbecauseyouadded
a function without tests.”
GitHubdiscussioncommentscanstillbedifferentfromtheStack
Overflowcomments.ToverifythatSenti4SDcaneffectivelyidentify
sentiment in comments on GitHub, we selected a random set of
50commentsandviaobservationdeterminedthemtocontain25
commentsof neutral,13 commentsof negative,and 12comments
of positive sentiment. Then we ran them through Senti4SD and
found that 6 were mispredicted by Senti4SD, showing the accuracyof88%(2neutralcommentsweredeemednegative,while1positive
and 3 negative comments were deemed neutral).
We aligned each post with their Senti4SD derived ternary senti-
ment(i.e.,positive,neutral,andnegative).Wefindthat23.8%ofthe
discussions are positive, 48.4% of the comments are neutral, and
27.8% of them are negative. The average length of the comments is
194charactersforneutral,242charactersforpositive,and422char-
acters forthe negative. This suggeststhat the negativecomments
may carry more information than other two types.
5.3 Linear Mixed-effect Regression
We use Generalized Linear Mixed Effect Regression (GLMER) mod-
els(glmer packagein R)tostudythecontributionofourindepen-
dent variables to explain the variability in the outcome variable,
while mixing fixed and random effects. The toolis used as the
random effect in the models.
Weuselogisticregressionformodelingtheadoptionsuccessand
generalized regression with the Poisson family for the modeling
discussionlength.Toavoidconvergenceissueweusethe bobyqa
optimizer.Weuse scale()functiontoz-normalizeeachvariableas
they vary across orders of magnitude between variables. To avoid
influential points caused by outliers in the fixed effects, we remove
701(a) Browser (b) Coverage (c) Minifiers
(d) Testing (e) Linter
Figure 1: The adoption time distributions of tool adoption events of five tool categories.
theprojectswithtop2% num_commits,num_comments ,andwith
top1%oftherestvariables.Wealsousenonlinear loдtransformfor
twovariables: num_new_devandnum_involved_dev,containing
extreme values and high variance. We use the Variance Inflation
Factor (VIF) to check whether multicollinearity exists between the
independent variables in the regression models, which can lead
to regression coefficients that are difficult to interpret. Typically,if the VIF values are smaller than 5 then multicollinearity is notsignificant. This is the case with all our models. To describe the
goodnessoffitofourmodels,weusethe squaredGLMM() function
inRtoreporttwopseudo- R2values:themarginal R2,interpretedas
thevariancesolelydescribedbythefixedeffects,andtheconditional
R2,interpretedasthevarianceintroducedbybothfixedandrandom
effectsinthemodel[ 31].Wealsoreportthestandardgoodnessoffit
measures of log-likelihood and the Bayesian information criterion,
the latter often used for model choice.
6 RESULTS AND DISCUSSION
6.1 RQ1: Tool Adoption and Team Dynamics
First, to study tool adoption events over time, we align all projects
around their adoption dates and plot those adoptions for each tool.
Theresults,pertoolcategoryandpertoolareshowninFigure1.
The figures suggest that adoptions in OSS projects spread in a non-
constant speed , some group of people adopt a tool much sooner
thantheaverageadoptiontime,whileothersadoptitonlyifthey
are fully convinced, i.e., have an adoption lag. This fits well within
thepredictionsoftheDOItheory.Notethatthetoolcategory De-
pendence manager is not included because of lacking enough data
points.
Second, to study the team dynamics around adoption events,
weexaminethetemporaldataofourfiveteam-levelmetrics:the
number of developers with prior exposure, new developers, in-
volveddevelopers,commentsassociatedwithtools,andcommits
at monthly intervals, as illustrated in Figure 2.
We see from Figure 2 that the average number of involved de-
velopers isalmost linearlyincreasing over time,likely correlatingTable 3: Topics Discovered in Discussions
Topic Sample vocabulary
1 Testing run, test, use, report, case, mocha
2 Development js, setup, window, browser, npm3 Debugging fail, stack, error, timeout, check4 General ideas work, support, dependency, need5 Integration CI, function, module, nodejs, client
withthegeneralGitHubtrend.Thisimpliesthatmoredevelopers
participate in the discussions of adopting tools.
Wealsoobserveasignificantdiscontinuityinthesteadynumbers
ofcommitsandcommentsjustbeforeandevenmoresoafterthe
adoptionevent,inthepositivedirection.Thisisarguablyassociated
with increased activities related to the adoption. Also notable, is
that the number of developers with prior exposure to the tool is
steadilygrowing inthe periodbeforethe adoption,thereby likely
increasing the diffusion probabilities and the adoption chances.
AnswertoRQ1: Successfuladoptiondistributionsareinline
withDOItheory,withsomeprojectsadoptingearlyandothers
late. We observe that new developer numbers increase slower
than involved developer numbers but significantly more so
after the tool adoption.
6.2 RQ2: Discussion Topics and Sentiment
Pre-andPost-adoption Discussions LDAisregularlyusedtoreveal
thefrequenttopicsintextinformation.However,differentfromthe
corpus gathered from social media, the comments from GitHub, by
their nature, are in a much narrow domain. Therefore, the number
ofdifferentdiscussiontopicsonGitHubismuchfewercompared
to social media corpus. Even though, many topics still overlap
with each other. We uncover the top-ranked topic features and
their associated sample vocabulary in the tool discussions, and
summarized them in Table 3.
702(a) Number of Devs /w Exposure (b) Number of Involved Devs (c) Number of New Devs (d) Number of Commits (e) Number of Comments
Figure 2: The monthly aggregated numbers of five variables (x-axis unit is in month), relative to the adoption month ( x=0),
over all projects. Error lines show the ±1standard error away from the mean.
0.10.20.30.4
−10 −50 51 0
Relative months to AdoptionPer Dev Neg Commentsgroup
New
Senior
0.10.20.30.4
−10 −50 51 0
Relative months to AdoptionPer Dev Neg Commentsgroup
With Exposure
Without Exposure
Figure 3: The per-developer negative comments posted by new developers and senior developers (left), and developers with
prior tool exposure and developers without prior tool exposure (right). Adoption event happens at monthx=0 .
To understand and identify topics and emerging patterns in the
discussions, we go through 3,342 discussion comments within the
6 months prior to the adoption date. By using the topic-vocabulary
pairs as the keywords, we find the following three topics in tool
discussions are prominent:
PerceivingDemandsofTools :"rightnowthe<file>isextremely
distractingandthetestoutputisimpossibletoread(justtheresult).
We need a <tool> PR to solve this."
Choosing One Tool Over Another : "<dev 1> thanks. I should have
donethis,tobeginwith.Isetup<tool1>becausethat’swhat<tool
2> was using before. But you are correct, that is better for testing."
DecidingWhentoAdopt :"Thankyouforsendingusthesecontri-
butions! Moving to <tool> is, in fact, something we have hoped to
do, I just wanted to let you know it might take us a few more days
before we’re ready to engage."
We also analyze 4,278 discussions from the same projects set for
the6-monthpost-adoptionperiod.Thetwoidentifiedprominent
topics of discussions are about:
AdoptionFeedback :"NotsurehowItowriteatestforthis.Ican’t
figure out how to get the output from <tool>"
Switching Tools : "But when comparing/choosing a testing frame-
workyouwilldefinitelyneedtosayoneisbetterforyouoraproject
at some point. I would be open to using <tool>."
Insummary,wefindthefollowingprominentpatternsexistin
thetool-relateddiscussions:adiscussionseemstobeinitiatedbyan
individual who found an issue and asked for addressing such issue
withatool,anditendswithanindividualwhohaspreviouslyused
similartoolspresentingtheirexperience,bythemrecommendingatooltoadopt.Wealsofindthatdiscussionthreadsaregoal-oriented,
well structured, and proceed logically, and they are likely to be
beneficial for developers to decide on which tool to adopt.Developer Sentiment TotestourhypothesesfromSJTaboutego
involvement and from DOI about tool adoption, here we compare
the sentiment in tool adoption discussions between the comments
ofa)newdevelopersandsenior(i.e.,notnew)developers,andb)
between developers with prior tool exposure and the ones without
priortoolexposure.Todothat,wecomparethenegativecommentspostedseparatelybybothnewdevelopersandseniordevelopers.AsshowninFigure3(a),beforetheadoption,newdevelopersaremuch
less negative to adopting new tools than senior developers, even
thoughtheirnegativitysignificantlygrowsjustbeforeandmoresoaftertheadoptionevents,ifonlyforashorttime.Thisisconsistent
with SJT: new developers have less involvement in the projects
andthusloweremotionalattachmentthanseniordevelopers,while
the latter have to (perhaps begrudgingly) adapt to the changes
intheproject.Evenmoreinterestingly,thenegativesentimentof
the senior developers persists, which can in the longer term affect
project cohesion and effective management.
Ontheotherhand,asshowninFigure3(b),theper-developer
negativityofdeveloperswithpriorexposureismuchlowerthanthe developers without exposure, for all time-bins. And after im-
plementing tool adoptions, the negativity of developers with prior
exposureisveryclosetodeveloperswithoutexposure,however,thenegativityofdeveloperswithpriorexposuredropsfasterrightaftertheadoptionandremainslowerinthelongterm.Thisvalidatesthe
assumptionthatdeveloperswithpriorexposurearelessnegative
toward the tools than the ones without exposure. Moreover, wefind that the curves of the two types of developers are similar tosomedegree,suggestingbotharereactingtothesameevents,or
are communicating together.
Relative sentiment .Tounderstandwhydeveloperschooseone
toolt1over another tool t2from the same category (e.g., both
703istanbul−istanbulistanbul−jasmineistanbul−mochajasmine−istanbuljasmine−jasminejasmine−mochamocha−istanbulmocha−jasminemocha−mocha
−0.2 −0.1 0.0 0.1 0.2
Relative NegativityCompared Toolsstatus
post
pre
(a) Testing Toolseslint−eslinteslint−jshinteslint−standardjshint−eslintjshint−jshintjshint−standardstandard −eslintstandard −jshintstandard −standard
0.0 0.2 0.4
Relative NegativityCompared Toolsstatus
postpre
(b) Linter Tools
Figure 4: Negativity toward tools in the same category, illustrated both before the adoption (red) and after adoption (green).
t1andt2are linters), we compare the relative negativity of their
corresponding discussions. First, we calculate the baseline nega-
tivity for each project (i.e., the ratio of negative comments overall comments). Then we calculate the aggregated negativity ofa tool as the ratio of negative over all comments in the discus-sion, minus the project’s baseline negativity. In Figure 4, each
rowrepresentsapairoftwotools( t1-t2)fromthesamecategory.
The bars show the aggregated (team-level) negativity to tool t2
of the projects before (red) and after (green) adopting tool t1. If
toolt1 andt2 are the same (i.e., t1=t2), the bar simply repre-
sents the change of negativity after adopting the tool t1(i.e.,t2).
As shown in Figure 4(a), the bar Jasmine-Jasmineshows that, the
post-negativity becomes more negative than pre -negativity, sug-
gesting thatdevelopers find Jasminemore difficultto usethan ex-
pected.InFigure4(b), standard-jshintbarshowsthatafteradopting
toolstandard,thesentimenttoward jshintbecomeslessnegative
(i.e., more positive).
Answer to RQ2: We identify three most significant scenarios
inthetooldiscussions:perceptionoftools,choosingatoolover
another,anddecidingwhentoadoptatool.Wealsofindthat
sentiment toward a tool is associated with developer seniority
andpriorexposuretothetool.Finally,sentimenttowardatoolisassociatedwithadoptions,anditcanchangeafteradoptions.
6.3 RQ3: Influencers and Adoptions
In the sentiment data set, we find that 5% of developers posted
35% of total negative comments, indicating there might exist some
strong ego-involvement and possibly influencers during the discus-
sionperiod.Here,weseektoanswerifinfluencersexist,identify
them, and see how they affect tool adoptions. We first define in-
fluencers asthosedeveloperswhopostmorethan50%ofthetotal
commentsinthetooladoptiondiscussions.Tohavereliabledatato
study developers who frequently comment in each discussion, we
decided to only consider adoption discussions with more than ten
comments.Thatfilterleavesus592distinctadoptioneventstostudy.
Among them, 379 adoptions are successful, and 213 adoptions are
unsuccessful. We define the Successful Adoption Rate (SAR) as the
ratio of the successful adoptions to the total, i.e., 379 /592=64.02%.
A very common situation is that a project member wants to
haveatestingtoolfortheirproject,andthedeveloperscreatean
issue to query other’s opinions on whether to adopt this tool. If
the majority of main contributors agree on adopting a testing tool,
thentheywouldbediscussingwhichtooltoadopt.Ourhypotheses,arisingfromtheego-involvementconsiderationsinSJT,arethat:1)aprojectwithaninfluencerhasahigherlikelihoodtohaveadopted
atool,and2)thehigherthenegativityofastronginfluencer,the
less likely it is for the tool to have been adopted.
Wetestthefirsthypothesisbycomparingsuccessfuladoption
rates in projects with strong influencers to those without. We find
that for the former, the SAR is 68 .86% and for the latter 57 .75%.
Totestthesecondhypothesis,amongtheadoptionsthathave
had an influencer, we found that if the sentiment of the strong
influencer was positive on the tool, the SAR is 72 .18%, while if the
sentiment of the strong influencer was negative, the SAR is 64 .71%.
In contrast, and as predictedby SJT, without an influencer inte-
grating the democratic opinions into a consensus decision may be
moredifficultandtakelonger,while theinfluencercan speedupthe
adoption process.Wefind thatwithout aninfluencer,the average
discussion length is about 15 months, while with an influencer, the
length decreases to 13 months, on average.
Answer to RQ3: We identify strong influencers in the
projects, and show that projects having strong influencershave more successful adoptions and shorter adoption dis-cussions. Moreover, the sentiment of the strong influencer
correlates with the adoptions.
6.3.1 Case Study. Wegiveexamplesfromtwocomparablysized
projectstestem/testem andbower/bower toillustratehowastrong
influencer can be of help in tool adoptions. The project sizes are
similartoeachother(numberofcommits:2 ,305v.s.2 ,726;number
of contributors: 157 v.s. 209).
Withastronginfluencer In the project testem/testem, the fol-
lowingdiscussiontranspired,onusingatooltoautomatetesting,
where <Dev 2> is the strong influencer in the team.
<Dev1>:"...Ithinkitwouldbeconvenienttorenderthepageas
template and pass there..."
<Dev 2> "Why do you need this? Please give more details about
your use case."
<Dev1>"...Iusetestemnotjusttorununitteststoseestandard
test reportpage, butas a watchingtool that automaticallyreloadsmy web application when sources changed ..."
<Dev2>"Igetmostofwhatyouaresaying.Areyouusing<tool
1>? The hash issue I think I need to rethink how to handle that ..."
<Dev 1> "Now I switched to <tool 2> (as it really more robust
andconvinient),Iused<tool1>aswellitdoesn’tactuallymatter.
And I use dependency management tool to load scripts ..."
704Withoutastronginfluencer Incontrast,theproject bower/bower
encountered an issue when trying to use a tool for automated
testing. A developer asked for help fromthe community , however,
no one presented strong opinions in favor of continued use of this
tool.Onemembersuggestedtonotuse<tool>anymoreandswitch
to another tool.
<Dev1>:"Shouldwehaveacommonwaytodeclarethetestsfor
any component? For example, I’d specify <tool> and file in <file>.
I’dthen run<command>,whichwouldopen the<tool>pageina
browser. Thoughts?"
<Dev 2>: "what we could support is something similar to <pack-
age name> which is common scripts specified in the <file> ... what
do <Dev 3> and <Dev 4> think?"
<Dev 4>: "<Dev 5> has said it was a mistake to make them all
globals. they should be triggered with <file> ..."
<Dev6>:"...AnycomponentwithunitteststhatI’vewrittenjust
ends up using a separate node module (like <tool>) to automate the
test workflow. I’d be in favor of closing this."
The two decisions are in contrast. In the first project, tool adop-
tion happened after a strong influencer insists on it. In the second,
multiple projectmembers are involved, andthe project adopteda
different tool than the one discussed.
6.4 RQ4: Adoption & Discussion Determinants
In the previous RQs, we conducted exploratory and qualitative
studiesofteamdiscussionanddynamicsbeforeandfollowingan
adoptionevent.Herewetriangulatethosewithquantitativestudies,
to understand the determinants, as well as the direction of their
effects, on adoption success and discussion length.
Our data is naturally hierarchically organized based on the tool
being discussed. We use toolas a random effect in our models,
allowing all projects adopting a specific tool to have the samerandom intercept. All other variables are used as fixed effects in
our mixed effect models.
We model each of the two outcomes with a base model, com-
prised of the control variables, and a full model, which adds to
the base the complement of team variables. We perform the likeli-
hood ratio test between the base and full models using the anova()
function, and present both models for each outcome variable.
Modeling Adoption Success.The results are shown in Table 4.
WeseefromtheAICthatthefullmodelfitsthedatasignificantly
betterthanthebasemodel,withthethreesignificantteamvariables
explainingabout5%ofthetotalvariance,asperthemarginal R2.
Overall, the fixed effects alone do not present a good model, but
togetherwiththerandomeffect,themodelismuchbetter,at48%
conditional R2.This,togetherwiththevif’sbeingsmallerthan5,
gives us confidence that we can interpret the coefficients of the
variables.
Of the controls, the variables discussion_length, num_comments
andnum_commits have significant, sizeable negative effect on tool
adoptions, holding all else constant. This is consistent with the SJT
prediction that group judgment needs more time to form in larger
teams, and that an adoption may be more difficult to succeed since
anagreementisneededfrommorepeople. num_mentions,onthe
other hand, shows a sizeable positive effect, which makes sense
fromaDOIperspective,thatanadoptionneedsawiderspreadto
succeed.Table 4:adoption_successglmer model, toolas random effect.
Base Model Full Model
scale(discussion_length) −0.477∗∗∗(0.110) −0.484∗∗∗(0.129)
scale(project_age) −0.051 (0.105) −0.115 (0.108)
scale(num_mentions) 0.612∗∗∗(0.150) 0.448∗∗(0.214)
scale(num_comments) −0.183∗(0.110) −0.268∗∗(0.115)
scale(num_commits) −0.208∗∗(0.097) −0.222∗∗(0.108)
scale(log(num_new_dev + 0.1)) −0.464∗∗∗(0.163)
scale(num_w_tool_expos) 0.617∗∗∗(0.141)
scale(log(num_involved_dev + 0.1)) 0.419∗∗(0.181)
scale(num_pos_dev) −0.126 (0.141)
scale(num_neg_dev) −0.120 (0.140)
Constant 1.264∗∗∗(0.410) 1.434∗∗∗(0.399)
Observations 1,085 1,085
Log Likelihood −407.502 −391.491
Akaike Inf. Crit. 829.003 806.981
Bayesian Inf. Crit. 863.929 866.854
Marginal R29.94% 14.79%
Conditional R246.72% 47.97%
Note:∗p<0.1;∗∗p<0.05;∗∗∗p<0.01
Of the team variables, num_involved_devis positively associ-
ated with adoption success, all else held constant. When multi-ple developers are highly involved in the project, they may allbe on the same page concerning the project’s needs. This is con-sistent with SJT, as aligned egos will easier agree. The predictor
num_w_tool_expos also has a significant, sizable positive effects on
adoptionsuccess.Onepossiblereasonforthisisthatthedevelop-
erswhohadpreviouslybeenactiveinprojectsthathaveusedthe
tool,aremorefamiliarwiththetool.ConsistentwithDOItheory,
those are the developers that contribute to the diffusion (spread)
of information on the tool in their new projects, which can lead to
successfuladoptions.Refinedtemporaldiffusionmodelscanoffer
a more detailed, temporal view of this diffusion process, and are
leftforfuturework.Aninterestingfindingisthat num_new_devis
significantandnegativelyassociatedwithadoptions.Wecansee
severalexplanations.First,newdevelopersmaynotfeelcomfortableto state their opinions publicly, which practically, may amount to anegativeoverallopinion.Second,astheydonotunderstandtheinsandoutsoftheprojectsyet,theymaynotperceivetheneedforthechange,especiallysincetheyhavejustrecentlystartedcontributing.
Also,wefindthatneither num_neд_devnornum_pos_devhavea
significant effect on adoptions. We see this in the context of SJT:highegodevelopersarelikelytobetheonesparticipatinginthe
discussions, and their arguments, emotional or not, are unlikely to
change the opinions of other high ego developers.
Modeling Discussion Length.AsweseeinTable5,theAICtells
usthatthefullmodelfitsthedatasignificantlybetterthanthebase
model,withthethreesignificantteamvariablesexplainingabout
40% of the total variance, as per the marginal R2. Overall, the fixed
effectsalonepresentagoodmodel,buttogetherwiththerandom
effect, the model is excellent, at 91% conditional R2. This, together
withthevif’sbeingsmallerthan5,givesusconfidencethatwecan
interpret the variables coefficients and trust the model.
705Table5:discussion _lenдthglmermodel, toolasrandomeffect.
Base Model Full Model
scale(project_age) 0.360∗∗∗(0.010) 0.242∗∗∗(0.010)
scale(num_mentions) 0.038∗∗∗(0.009) 0.059∗∗∗(0.014)
scale(num_comments) 0.072∗∗∗(0.009) −0.072∗∗∗(0.010)
scale(num_commits) 0.183∗∗∗(0.008) −0.006 (0.009)
scale(log(num_new_dev + 0.1)) 0.922∗∗∗(0.020)
scale(num_w_tool_expos) −0.062∗∗∗(0.009)
scale(log(num_involved_dev + 0.1)) −0.038∗∗(0.016)
scale(num_pos_dev) −0.030∗∗(0.013)
scale(num_neg_dev) −0.013 (0.012)
Constant 1.838∗∗∗(0.155) 1.644∗∗∗(0.098)
Observations 1,085 1,085
Log Likelihood −5,868.721 −4,451.806
Akaike Inf. Crit. 11,749.440 8,925.611
Bayesian Inf. Crit. 11,779.380 8,980.494
Marginal R231.50% 77.99%
Conditional R286.71% 91.33%
Note:∗p<0.1;∗∗p<0.05;∗∗∗p<0.01
Of the control variables only project_aдehas a significant, size-
able positive effect. It is in line with expectations: older active
projectswillhavemoreparticipantsandthislikelylongerdiscus-
sions.Intheteamvariables, num_new_devissizeableandpositively
associatedwiththe discussionlength,all elsekeptconstant.Both
DOI and SJT are consistent with these findings, as the new people,
who have little ego involvement, can be the ones with questions or
commentsabouttoolsandtheproject.Theotherteamvariablesare
negativelyassociatedwiththediscussionlength,buttheireffects
aresmall.Inparticular, num_involved_dev andnum_w_tool_expos
arenegativelyassociatedwithdiscussionlength,inlinewithexpec-
tations that involved developers and those exposed to the tool pre-
viouslymaynotneedlongdiscussionstodecide.The num_pos_dev
variable has a small, but a significant negative effect on discussion
length, suggesting that more positive developers can be beneficial
to shortening discussions.
AnswertoRQ4: Foradoptionsuccess,thepositivesignificant
variablesarethenumberofmentions,developerswithprior
exposure, and involveddevelopers. As for discussionlength,
thenumberofnewdevelopersseemstobethemostsignificant
indicator to extend the discussion, while the exposure factor
has a positively sizable effect on shorting the discussion.
7 TAKEAWAYS FOR PRACTITIONERS
Here we distill from our findings some practical takeaways and
suggestions. Generally, OSS project team discussions are helpfulfor community building. But they are sorely lacking during tooladoptions, and since we also found that the discussions tend to
be goal-oriented and rational, we recommend that they should be
encouraged in the OSS community.
Ourfindingthathavingmorepeopleontheprojectwithprior
exposure to a tool is associated with successful adoption is evi-
dence toward proceeding with adoptions after a team has multiple
members with prior exposure. However, longer discussions can bedistracting to a team, and we found they are not associated with
better adoption outcomes; on the other hand, having an influencer
as a champion for the adoption may help.
We also found that some tools are associated with longer dis-
cussionsthanothers,perhapsbecausetheydemandcertainprior
exposure and further research. Setting expectations for the team
aheadoftimecanlimitfeelingsoffrustrationsarisingoutoflengthy
discussions. And while we found no association between negative
(orpositive)developersandadoptionsuccess,discussionstendto
be shorter as the number of positive developers increases. Thus,
being more positive than negative may help keep things shorter.
Further,morenewdevelopersareassociatedwithloweradoption
andlongerdiscussions.Whilethenumberofnewdeveloperscannot
be modulated much, perhaps timing tool adoptions during periods
oflowinfluxofnewpeoplemayhelptheproposedtooladoption.
Finally,morecommitsandcommentsareassociatedwithlowered
success of adoption, thus, planning tool adoptions away from busy
project times may result in more successful adoptions.
8 THREATS TO VALIDITY AND CONCLUSION
Threats.Both adoption data and commits/comments data were
gatheredfromGitHub,thus,generalizingtheresultsbeyondGitHub,
orevenbeyondthegatheredcorpus,carriessomerisk.However,
the projects were selected randomly (with some minimum activity
requirements), thus lowering this risk. Also, we do not consider
anyoffline/in-personcommunicationchannelsbetweendevelopersexceptthroughGitHub.Previousworkhasfoundthatthereexistsanotabledecreaseincommunicationsassociatedwithsamecompanyaffiliation,implyingthatdevelopersmaysharetheiropinionsoffline.
Also, Senti4SD is trained on communications among developers
inStackOverflow.GitHubcommentscanbedifferentthanStack
Overflowcomments,thoughoursmallsamplestudyherecapped
that to 12%.
Conclusion. Motivatedbytheavailabilityofmultipletoolsper
use categoryin DevOps settings,and the generallack of guidance
abouttheirappropriatenessforspecificprojects,westudiedteam
level determinants of tool adoptions and discussions.
In terms ofthe relative timing of tool adoptions,wefound that
thereisasignificantdifferencebetweenthedistributionofadoption
timesfortoolsinthesameusecategory,makingitchallengingto
choose which tool to adopt for projects that are laggards.
We considered tool adoption as a project-wide phenomenon,
affecting every member in the group. But also depending on the
opinionofmanyofthem,includingtheirpriorimpressionsoftools
and linguistic sentiment, we demonstrated that the involvement,
tenure,andmoreimportantly,priorexposuretothetoolplaysig-
nificantrolesinthediscussion.Wealsofindthatstronginfluencers
are associated with more, successful adoptions.
We find that the attitude towards adoptions varies across dif-
ferent groups of people, and that a team’s relative negativity istool-specific, and can change after adoption, suggesting that the
usabilityoftoolscanbeover-andunderestimated.Weconducted
topic analysis, and in-depth case studies on how and why somesimilar projects choose one tool over another one. We concludethat tool adoption is akin to a reasonable team negotiation, that
proceedsthroughmultiplephases.Wehopeourresultscanbeof
help in future tool adoption decisions.
706ACKNOWLEDGEMENTS
WearegratefultotheNationalScienceFoundationforfundingthis
project,undergrant#1717370.WethanktheASE2020reviewers
for their constructive comments which helped improve this paper.
REFERENCES
[1]MarkSAckerman,JuriDachtera,VolkmarPipek,andVolkerWulf.2013. Sharing
knowledgeandexpertise:TheCSCWviewofknowledgemanagement. Computer
Supported Cooperative Work (CSCW) 22, 4-6 (2013), 531–573.
[2]DaneBertram,AmyVoida,SaulGreenberg,andRobertWalker.2010. Commu-
nication, collaboration, and bugs: the social nature of issue tracking in small,
collocatedteams.In Proceedingsofthe2010ACMconferenceonComputersupported
cooperative work. 291–300.
[3]DavidMBlei,AndrewYNg,andMichaelIJordan.2003.Latentdirichletallocation.
Journal of machine Learning research 3, Jan (2003), 993–1022.
[4]Berndt Brehmer. 1976. Social judgment theory and the analysis of interpersonal
conflict.Psychological bulletin 83, 6 (1976), 985.
[5]FabioCalefato,FilippoLanubile,FedericoMaiorano,andNicoleNovielli.2018.
[Journal First] Sentiment Polarity Detection for Software Development. In 2018
IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE,
128–128.
[6]Laurent Christophe, Reinout Stevens, Coen De Roover, and Wolfgang De Meuter.
2014. Prevalence and maintenance of automated functional tests for web ap-plications. In 2014 IEEE International Conference on Software Maintenance and
Evolution. IEEE, 141–150.
[7]Dan Cornell. 2012. Remediation statistics: what does fixing application vulnera-
bilities cost. Proceedings of the RSAConference, San Fransisco, CA, USA (2012).
[8]ValerioCosentino,JavierLuisCánovasIzquierdo,andJordiCabot.2016. Findings
from GitHub: methods, datasets and limitations. In 2016 IEEE/ACM 13th Working
Conference on Mining Software Repositories (MSR). IEEE, 137–141.
[9]Fabrício R de Souza, Augusto CSA Domingues, Pedro OS Vaz de Melo, and
AntonioAFLoureiro.2018. MOCHA:AToolforMobilityCharacterization.In
Proceedingsofthe21stACMInternationalConferenceonModeling,Analysisand
Simulation of Wireless and Mobile Systems. ACM, 281–288.
[10]FabianFagerholm,AlejandroSanchezGuinea,JayBorenstein,andJürgenMünch.
2014. Onboarding in open source projects. IEEE Software 31, 6 (2014), 54–61.
[11]William R Ferrell. 1985. Combining individual judgments. In Behavioral decision
making. Springer, 111–145.
[12]Tony Gorschek, Per Garre, Stig Larsson, and Claes Wohlin. 2006. A model for
technology transfer in practice. IEEE software 23, 6 (2006), 88–95.
[13]Sandra Graham and Shari Golan. 1991. Motivational influences on cognition:
Taskinvolvement,egoinvolvement,anddepthofinformationprocessing. Journal
of Educational psychology 83, 2 (1991), 187.
[14]SarraHabchi,XavierBlanc,andRomainRouvoy.2018. Onadoptinglintersto
deal with performance concerns in android apps.
[15]Jeffrey T Hancock, Kailyn Gee, Kevin Ciaccio, and Jennifer Mae-Hwah Lin. 2008.
I’m sad you’re sad: emotional contagion in CMC. In Proceedings of the 2008 ACM
conference on Computer supported cooperative work. ACM, 295–298.
[16]BillCHardgrave,FredDDavis,andCynthiaKRiemenschneider.2003. Investi-
gating determinants of software developers’ intentions to follow methodologies.
Journal of Management Information Systems 20, 1 (2003), 123–151.
[17]Michael Hilton, Jonathan Bell, and Darko Marinov. 2018. A large-scale study of
test coverage evolution.. In ASE. 53–63.
[18]MdRakibulIslamandMinhazFZibran.2018. Acomparisonofsoftwareengi-
neering domain specific sentiment analysis tools. In 2018 IEEE 25th International
Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE,
487–491.
[19]Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diffs using neural machine translation. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 135–146.
[20]BrittanyJohnson,RahulPandita,JustinSmith,DenaeFord,SarahElder,Emerson
Murphy-Hill, Sarah Heckman, and Caitlin Sadowski. 2016. A cross-tool com-municationstudy onprogram analysistoolnotifications. In Proceedingsofthe
2016 24th ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 73–84.
[21]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. Whydon’tsoftwaredevelopersusestaticanalysistoolstofindbugs?.In
201335thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,672–681.
[22]TiffanyBrookeJordan,BrittanyJohnson,JimWitschey,andEmersonMurphy-
Hill.2014. DesigningInterventionstoPersuadeSoftwareDeveloperstoAdopt
Security Tools. In Proceedings of the 2014 ACM Workshop on Security Information
Workers(Scottsdale, Arizona, USA) (SIW ’14). ACM, New York, NY, USA, 35–38.
https://doi.org/10.1145/2663887.2663900[23]FranciscoJuradoandPilarRodriguez.2015. SentimentAnalysisinmonitoring
software development processes: An exploratory case study on GitHub’s project
issues.Journal of Systems and Software 104 (2015), 82–89.
[24]David Kavaler, Asher Trockman, Bogdan Vasilescu, and Vladimir Filkov. 2019.
Toolchoicematters:JavaScriptqualityassurancetoolsandusageoutcomesin
GitHubprojects.In Proceedingsofthe41stInternationalConferenceonSoftware
Engineering. IEEE Press, 476–487.
[25]Truman Lee Kelley. 1925. The applicability of the Spearman-Brown formula for
the measurement of reliability. Journal of Educational Psychology 16, 5 (1925),
300.
[26]JKottmann,GIngersoll,JKosin,andBGalitsky.[n.d.]. TheApacheOpenNLP
library.
[27]PaulMLeonardi.2014. Socialmedia,knowledgesharing,andinnovation:Toward
atheoryofcommunicationvisibility. Informationsystemsresearch 25,4(2014),
796–816.
[28]ChangzhouLi,YaoLu,JunfengWu,YongruiZhang,ZhongzhouXia,Tianchen
Wang, DantianYu, XuruiChen, Peidong Liu,and JunyuGuo. 2018. LDA meets
Word2Vec: a novel model for academic abstract clustering. In Companion Pro-
ceedings of the The Web Conference 2018. 1699–1706.
[29]Jennifer Marlow and Laura Dabbish. 2013. Activity traces and signals in soft-
waredeveloperrecruitmentandhiring.In Proceedingsofthe2013conferenceon
Computer supported cooperative work. ACM, 145–156.
[30]Samim Mirhosseini and Chris Parnin. 2017. Can automated pull requests en-
courage software developers to upgrade out-of-date dependencies?. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 84–94.
[31]ShinichiNakagawaandHolgerSchielzeth.2013. Ageneralandsimplemethod
forobtainingR2fromgeneralizedlinearmixed-effectsmodels. Methodsinecology
and evolution 4, 2 (2013), 133–142.
[32]ShariLawrencePfleeger.1999. Understandingandimprovingtechnologytransferinsoftwareengineering. JournalofSystemsandSoftware 47,2-3(1999),111–124.
[33]Andreas Poller, Laura Kocksch, Sven Türpe, Felix Anand Epp, and Katharina
Kinder-Kurlanda.2017. Cansecuritybecomearoutine?:astudyoforganizational
change in an agile software development group. In Proceedings of the 2017 ACM
ConferenceonComputerSupportedCooperativeWorkandSocialComputing.ACM,
2489–2503.
[34]Gede Artha Azriadi Prana, Christoph Treude, Ferdian Thung, Thushari Atapattu,andDavidLo.2019. CategorizingthecontentofGitHubREADMEfiles. Empirical
Software Engineering 24, 3 (2019), 1296–1327.
[35]Cynthia K. Riemenschneider, Bill C. Hardgrave, and Fred D. Davis. 2002. Ex-
plaining software developer acceptance of methodologies: a comparison of five
theoretical models. IEEE transactions on Software Engineering 28, 12 (2002),
1135–1145.
[36]Everett M Rogers. 2002. Diffusion of preventive innovations. Addictive behaviors
27, 6 (2002), 989–993.
[37]John Rohrbaugh. 1979. Improving the quality of group judgment: Social judg-ment analysis and the Delphi technique. Organizational Behavior and Human
Performance 24, 1 (1979), 73–92.
[38]JukkaRuohonenandVilleLeppänen.2018. Towardvalidationoftextualinfor-
mation retrieval techniques for software weaknesses. In International Conference
on Database and Expert Systems Applications. Springer, 265–277.
[39]FarhanaSarker,BogdanVasilescu,KellyBlincoe,andVladimirFilkov.2019. Socio-
technical work-rate increase associates with changes in work patterns in online
projects. In Proceedings of the 41st International Conference on Software Engineer-
ing. IEEE Press, 936–947.
[40]Leif-Gerrit Singer. 2013. Improving the adoption of software engineering practices
through persuasive interventions. Lulu. com.
[41]Philippe Skolka, Cristian-Alexandru Staicu, and Michael Pradel. 2019. Anything
to Hide? Studying Minified and Obfuscated Code in the Web. In The World Wide
Web Conference. ACM, 1735–1746.
[42]Stuart Soroka, Patrick Fournier, and Lilach Nir. 2019. Cross-national evidence of
a negativity bias in psychophysiological reactions to news. Proceedings of the
National Academy of Sciences 116, 38 (2019), 18888–18892.
[43]Margaret-Anne Storey, Alexey Zagalsky, Fernando Figueira Filho, Leif Singer,
andDanielMGerman.2016. Howsocialandcommunicationchannelsshapeand
challenge a participatory culture in software development. IEEE Transactions on
Software Engineering 43, 2 (2016), 185–204.
[44]ChandrasekarSubramaniam,RaviSen,andMatthewLNelson.2009. Determi-
nants of open source software project success: A longitudinal study. Decision
Support Systems 46, 2 (2009), 576–585.
[45]Jirateep Tantisuwankul, Yusuf Sulistyo Nugroho, Raula Gaikovina Kula, Hideaki
Hata, Arnon Rungsawang, Pattara Leelaprute, and Kenichi Matsumoto. 2019.
A topological analysis of communication channels for knowledge sharing in
contemporary GitHub projects. Journal of Systems and Software 158 (2019),
110416.
[46]Leigh Thompson and Terri DeHarpport. 1994. Social judgment, feedback, and in-terpersonallearninginnegotiation. OrganizationalBehaviorandHumanDecision
Processes 58, 3 (1994), 327–345.
707[47]KristínFjólaTómasdóttir,MauricioAniche,andArievanDeursen.2017. Why
andhowJavaScriptdevelopersuselinters.In Proceedingsofthe32ndIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering.IEEEPress,578–589.
[48]AsherTrockman,ShuruiZhou,ChristianKästner,andBogdanVasilescu.2018.
Addingsparkletosocialcoding:anempiricalstudyofrepositorybadgesinthe
npmecosystem.In Proceedingsofthe40thInternationalConferenceonSoftware
Engineering. ACM, 511–522.
[49]BogdanVasilescu,VladimirFilkov,andAlexanderSerebrenik.2013. Stackover-
flow and github: Associations between software development and crowdsourced
knowledge.In 2013InternationalConferenceonSocialComputing.IEEE,188–195.
[50]Georg Von Krogh, Sebastian Spaeth, and Karim R Lakhani. 2003. Community,
joining, and specialization in open source software innovation: a case study.
Research policy 32, 7 (2003), 1217–1241.
[51]JimWitschey,OlgaZielinska,AllaireWelk,EmersonMurphy-Hill,ChrisMay-
horn, and Thomas Zimmermann. 2015. Quantifying developers’ adoption ofsecurity tools. In Proceedings of the 2015 10th Joint Meeting on Foundations of
Software Engineering. ACM, 260–271.
[52]Shundan Xiao, Jim Witschey, and Emerson Murphy-Hill. 2014. Social influences
on secure development tool adoption: why security tools spread. In Proceedings
of the 17th ACM conference on Computer supported cooperative work & social
computing. ACM, 1095–1106.
[53]Marvin V Zelkowitz. 1995. Assessing software engineering technology transfer
within NASA. NASA technical report NASA-RPT-003095. National Aeronautics
and Space Administration, Washington, DC (1995).
[54]Jiaxin Zhu, Minghui Zhou, and Audris Mockus. 2016. Effectiveness of codecontribution: From patch-based to pull-request-based tools. In Proceedings of
the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software
Engineering. 871–882.
708