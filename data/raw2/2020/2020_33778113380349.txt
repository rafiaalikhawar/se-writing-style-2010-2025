CollaborativeBugFindingforAndroid Apps
Shin
Hwei Tan
tansh3@sustech.edu.cn
SouthernUniversityof ScienceandTechnology
Shenzhen,Guangdong Province, ChinaZiqiang Li
11510352@mail.sustech.edu.cn
SouthernUniversityof Science andTechnology
Shenzhen,Guangdong Province, China
ABSTRACT
Manyautomatedtestgenerationtechniqueshavebeenproposed
forfindingcrashesinAndroidapps.Despiterecentadvancementin
theseapproaches,astudyshowsthatAndroidappdevelopersprefer
reading test cases written in natural language. Meanwhile, there
exist redundancies in bug reports (written in natural language)
across different apps that have not been previously reused. We
proposecollaborativebug finding,a novel approach thatusesbugs
in other similar apps to discover bugs in the app under test. We
design three settings with varying degrees of interactions between
programmers: (1) bugs from programmers who develop a different
app,(2)bugsfrommanuallysearchingforbugreportsinGitHub
repositories, (3) bugs from a bug recommendation system, Bugine.
Our studies of the first two settings in a software testing course
show that collaborative bug finding helps students who are novice
Android app testers to discover 17 new bugs. As students admit
thatsearchingforrelevantbugreportscouldbetime-consuming,
we introduce Bugine, an approach that automatically recommends
relevant GitHub issues for a given app. Bugineuses (1) natural
languageprocessingtofindGitHubissuesthatmentioncommon
UI components shared between the app under test and other apps
inourdatabase,and(2)arankingalgorithmtoselectGitHubissues
that are of the best quality. Our results show that Bugineis able
to find 34 new bugs. In total, collaborative bug finding helps us
find 51 new bugs, in which eight have been confirmed and 11 have
beenfixedbythedevelopers.Theseresultsconfirmourintuition
thatour proposedtechniqueisusefulindiscoveringnewbugsfor
Android apps.
CCS CONCEPTS
·Softwareanditsengineering →Softwarelibrariesandrepos-
itories;Software maintenance tools.
KEYWORDS
collaborative programming, testgeneration,recommendationsys-
tem, Android apps
ACMReference Format:
ShinHweiTanandZiqiangLi.2020.CollaborativeBugFindingforAndroid
Apps.In42ndInternationalConferenceonSoftwareEngineering(ICSE’20),
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissionsfrom permissions@acm.org.
ICSE’20, May 23ś29,2020, Seoul, Republic ofKorea
©2020 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi .org/10.1145/3377811 .3380349May23ś29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,USA,13pages.
https://doi .org/10.1145/3377811 .3380349
1 INTRODUCTION
Smartphones bundled with mobile applications have become indis-
pensable. As mobile users rely heavily on their smartphones for
their daily tasks, the reliability and usability of mobile applications
are essential to ensure a satisfying user experience. According to a
recent survey [ 1], 56% of respondents have experienced a problem
when using a mobile application and 79% of them will retry an app
once or twice if it fails to meet their expectations. Hence, there
isarisingdemandfortestingandanalysistechniquesformobile
applications. Several automated techniques have been proposed
for analysis [ 6,11,21,28,53,73], testing [ 7,8,17,48,49,56] and
repair[63]ofmobileapps.However,theseautomatedtechniques
have not been widely adopted due to several reasons. Firstly, these
techniquesmainlyfocusonfindingcrashesandfailtofindother
typesofbugs(e.g.,UI-relatedbugs).Secondly,priorstudiesrevealed
that most app developers prefer manual testing compared to auto-
mated testing for finding bugs in their apps due to reasons such as
lackofknowledgeoftestingtools,and learning curve ofavailable
tools[41,44].Thirdly,accordingtooneofthesestudies[ 44],app
developers prefer reading automatically generated tests written in
natural language. However, existing automated testing techniques
could only generate tests expressed in low-level events (e.g., using
ADBcommands) that are difficult for developers to understand.
Forlargesoftwareprojects likeMozilla andEclipse,thereexist
up to thousands of bug reports written in natural language. Hence,
priorapproachesonduplicatebugreportsdetectionexploitthesim-
ilaritiesbetweenbugreportsforbuglocalization[ 61,70,74].Infact,
manysimilarbugreportsexistnotonlywithinthesameproject,but
also across different software projects, especially for Android apps.
Figure 1 shows two real worldexamples fromForkHub (a GitHub
client)1whereadeveloperwhodevelopedPocketHubandForkHub
found similar bugs in ForkHub by referring to issues in PocketHub.
Meanwhile, Table 1 shows an extreme case where there exists a
one-to-onecorrespondenceintheGitHubissuesbetweentwoapps
of different categories ( CameraColorPicker andGnucash). How-
ever, there is little study on how to utilize the redundancies in bug
reports acrossdifferentAndroid apps for discoveringnew bugs.
Motivated by the testing needs of app developers and the redun-
danciesinbugreportsacrossdifferentAndroidapps,wepropose
collaborativebugfinding,anovelformoftestingthatexploitsthe
similaritiesbetweenAndroidappsforcraftingtestscenariosspe-
cializedforagivenappundertest.ForAndroidapps,a testscenario
includes(1)stepstoreproduce,(2)testdata(e.g.,animageforim-
age processing app), and (3) the expected behavior. The underlying
1https://github .com/jonan/ForkHub/issues/5, https://github .com/jonan/ForkHub/
issues/6
13352020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea Shin HweiTanandZiqiangLi
Figure 1: App developers who developed PocketHub and ForkHub
found
similarbugsacross twoapps
Table1:Examplewherethereexistsone-to-onecorrespondencefor
issuesinCameraColorPicker(driver)and Gnucash(selectedapp)
TitlesofissuesinCameraColorPicker TitlesofissuesinGnucash
How do Igetlefttopcolor. Whenan account isedited,its coloris lost.
publish
toF-Droid PublishingonF-Droid
Make
gradlew executable Make gradlew executable
assumptionofourapproachisthatif AppAandAppBsharedsimilar
UI components and a developer Ahas found a bug BuдAonAppA,
then it is more likely to find a bug similar to BuдAforAppB. To
facilitatecollaborative bugfinding,we adaptthe driver-navigator
metaphor of pair programming by emulating the role of a compe-
tent pair programmer via developers of other similar applications.
Specifically, wehave designedthree settingswith varyingdegrees
of interactions between the driver and the navigators. In the coder-
vs-coders setting,thedriver(developer Awhodevelops AppA)drives
the bug finding session by sharing his or her bug report RA, while
thenavigators(otherdevelopersfor AppB,...,AppZ)readthebug
report and think about whether the test scenario in RAcould be
appliedto AppB,...,AppZ.Inthecoder-vs-manual-issues setting,the
drivermanuallysearchesforbugreportsfromadifferentapp AppB
(navigator) and constructs a test scenario that is specialized for
AppA. In thecoder-vs-auto-issues setting, the driver provides AppA
as a query and our issue recommendation system, Buginewill
automaticallyidentifyrelevantGitHubissuesbyselectingissues
from anothersimilar app for AppA.
We introduce the conceptof collaborativebug findinginasoft-
ware testing course with 29 seniors (fourth year Computer Science
students)wheretheycooperatedthroughagroupprojectthatspans
overtenweeks.Wegatheredboththeopinionsfromstudentsonthe
effectivenessofcollaborativebugfindingandthenumberofdefects
discovered.Bothofthesemeasurementsshowpositiveoutcomeson
thepotentialbenefitsofcollaborativebugfindinginaneducational
setting.Moreover,insomebugreports(e.g.,inFigure1),wealso
foundevidencethatAndroidappdevelopershavebeenusingthe
similarities between different apps for testing, which indicates the
opportunitiesofemployingcollaborativebugfindingbeyondthe
classroom setting.
Overall,our contributionscan be summarizedas follows:
New Concept. Weintroducetheconceptofcollaborativebugfind-
ing, to the best of our knowledge, the first technique that exploits
the fact that similar bugs appear even in different apps to craft
specializedtest scenarios for testingAndroid apps.
Improved teaching ofsoftware testing. Severaltechniqueshave
beenproposedinenhancingsoftwaretestingeducation[ 18,24,25,
39,43].Tothebestofourknowledge,wepresentthefirststudy
Figure2:Fourbugs(circledinred)foundthroughcollaborativebug
finding
across four apps.
Figure 3: GitHub issue RA
r
eported by student SAfor
Omni-Notes.
Figure 4: GitHub issue reported by
studentSBfor
New-Pipe.
thatleveragescollaborativebugfindinginGitHubclassroomfor
teaching software testingcourse. Basedonstudents’feedbackof
using GitHub classroom, we have reported one important feature
tothedevelopersofGitHubclassroomandthisfeaturehasrecently
been planned for future release [ 60]. Moreover, our evaluation
has demonstrated the effectiveness of our approach in improving
teachingby helping studentsto find newbugsinAndroid apps.
New Recommendation System. Weproposeanewbugrecom-
mendation system, Buginefor reducing the effort required for
collaborativebugfinding.GivenanAndroidapp A,Buginewill
automatically select relevant bug reports which can be used as
tests forA.
Evaluation. We evaluate the effectiveness of collaborative bug
finding in three settings in which different degrees of interaction
betweenprogrammersare involved.Inthe coder-vs-coders and
coder-vs-manual-issues setting, collaborative bug finding helps
studentsdiscover 17newbugswhenevaluated in20apps. Mean-
while,Bugineis able to recommend 34 new bugs for the five
evaluated apps. In total, collaborative bug finding helps in the
discovery of 51 new bugs, in which eight have been confirmed
and 11 have been fixed by the developers. All the bugs found via
Bugineispubliclyavailable at https://bugine.github.io/.
1336Collaborative BugFinding forAndroid Apps ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
2 MOTIVATING EXAMPLE
We demonstrate the workflow of the coder-vs-coders setting of
collaborativebugfindingusingfourexampleappsandtheirbugs.
Figure 2 shows the screenshots of the four open-source apps re-
ported by four different students. The first app Omni-Notes is a
note-taking app (Category: Productivity), while the second app
New-Pipe is a media player app (Category: Multi-Media). Mean-
while,thethirdappMinimalToDoallowsuserstomanagetheir
to-dolists(Category:Productivity),whereasthefourthappCoCoin
is an accounting app (Category: Finance). The process of collabora-
tive bugfindingproceedsas follows:
•StudentSA(i.e., the driver) found a bug in Omni-Notes that
causesthecounterrepresentingthenumberofnotesinthenewly
updated łtestž category (circled in red) to be outdated when
adding or removing a note. The user needs to restart the app for
theincrementtotakeplace.Student SAreportedtheGitHubissue
RA(asshowninFigure3),andthisissuehasbeenconfirmedand
fixedbythe developers2.
•StudentSBreadRAand tried to derive a similar test scenario
fromRAforNew-Pipeapp.Figure4showsthebugreportfiled
bySB3.ComparingFigure3andFigure4,weinferthatadapting
the test scenario requires (1) creative thinking of the łsteps to
reproducežinthecontextofNew-Pipe,whiletwoinformation
can be reused from RA: (2) the expected behavior of the app (i.e.,
theviewshouldbeupdated),and(3)thetestbehaviorafterthe
incorrect view occurs (i.e., the view is updated correctly upon
restart).Withalltherequiredinformationinmind, SBfoundthat
although amessage with łImportedž is shown when SBtried to
import subscriptions from YouTube, the content of the imported
subscriptions arenot displayedimmediately,andcouldonly be
shown after anapp restart. Forthis bug report RB, the developer
has recently addedapull request for fixing this bug.
•StudentSCtriedtoadaptthetestscenariofrom RAtoMinimal
To Do. In the adapted test scenario4, the time reminder for a
to-do wasnotupdated immediately when thespecified timefor
the to-do has passed, making the outdated time appearedin the
screen.Similarto RA,theviewisupdatedonlyafterrestarting
the app.
•StudentSDreadRAandrealizedthatwhentheamountforlunch
(the red icon) is modified from 45 to 30, the total displayed at
the top of the screen (ł65ž) was not updated accordingly despite
showing amessage łUpdate successfullyž5.
Although Figure 2 shows that all the four apps are relatively
different and most of them are from different categories (except
forOmni-NotesandMinimalToDothatbelongtothesamePro-
ductivitycategory),ourexampleshowsthatsimilarbugsthatare
related to outdated view may exist across different apps. Apart
from the four example bugs shown in Figure 2, another student
SEwhoalsoselectedNew-Pipefortestinghaddiscoveredsimilar
problemswhentryingtoimportapreviouslyexportedsubscription
file(asshownattheendofFigure4).Meanwhile,student SAalso
encountered similar problem when modifying the tag of a note
in Omni-Notes. In total, collaborative bug finding has helped in
2https://github.com/federicoiosue/Omni-Notes/issues/625
3https://github.com/TeamNewPipe/NewPipe/issues/1919
4https://github.com/avjinder/Minimal-Todo/issues/113
5https://github.com/Nightonke/CoCoin/issues/47
Figure5: Workflowfor thecoder-vs-coderssetting
Figure6: Workflowfor thecoder-vs-manual-issues setting
identifying five instances of similar bugs across four apps. By shar-
ing a new GitHub issue RAthat student SAfound through manual
testing,fourstudentsareinspiredby RAwhichhelpsthemtoderive
specialized test scenarios for four new bugs in three different apps,
respectively. It is worthwhile to mention that all these students are
noviceAndroidapptesterswhoweregiventhetaskoftestingapps
that they have not used on a regular basis. This example illustrates
the effectiveness of collaborative bug finding in promoting creative
thinking and the discoveryofnew bugs.
Anotherinterestingobservationisthattheproblemofdisplaying
outdatedviewwhenthevaluesofaGUIcomponentaremodified
seems to be a prevalent problem in Android apps. Our manual
analysisofthefixesissuedbytheOmni-Notesdeveloperandthe
New-Pipedeveloperindicatesthatthisproblemoccursduetoin-
appropriate handling of asynchronous events, and could be solved
by adding either observable sequences or background service to
ensure that the view is being updated in real-time. As the fixes
for the bugs in Omni-Notes and New-Pipe share similar high-level
patternsbutdifferinthedesignchoices(usingeitherobservablese-
quencesorbackgroundservice),thisshowsthepotentialofextract-
ingbug-fixpatternsfromcommonissuesforautomatedprogram
repair[52,65,66].Moreimportantly,thefactthatsimilarbugscould
occuracrossfourdifferentappsandyetdevelopersarewillingto
fix them promptly indicates the importance and the prevalence of
thesebugs inAndroid apps.
3 METHODOLOGY
We use the Research through Design (RtD) [ 27,79] approach to
designseveralsettingstomodeldifferentdegreesofinteractions
between programmers. These settings aim to emulate different
scenarios in GitHub where collaborative bug finding could be used.
1337ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea Shin HweiTanandZiqiangLi
3.1 GitHub andGitHub classroom
GitHub6is a social coding platform built on top of the git ver-
sioncontrolsystemandsupportspull-basedsoftwaredevelopment.
GitHubclassroom [36]isanopen-sourceservicelaunchedbyGitHub
thatallowsinstructorstouseGitHubforteachingcomputerscience
courses. It includes several features (e.g., importing class roster, on-
line discussions, and automatic creation of software repositories
forindividualandgroupassignments).Internally,itusesanorgani-
zation to store course contents and student assignments. GitHub
teamdiscussion7isafeature inGitHuborganizationthatsupports
communications among members within the same organization or
thesameteam(asubsetofmembersinanorganization).Wedesign
our approach based onGitHubandGitHubclassroom.
3.2 The coder-vs-coderssetting
In the coder-vs-coders setting, we assume that both the driver and
the navigators are programmers who belong to the same organi-
zation but each of them may develop a different app. A recent
studyofopen-sourceAndroidappsinGitHubshowsthatthema-
jorcontributorsforanAndroidappalsoownedaroundfiveother
repositories [ 14]. This suggests that our assumption (developers
of different apps may belong to the same organization as other
developers) generallyholds.
Figure 5 shows the overall workflow for the coder-vs-coders
setting. The collaborative bug finding process starts with a pro-
grammer A(the driver) posting a GitHub issue iforAppAin the
team discussion page (Step 1in Figure 5). After reading i, another
programmer within the same organization (the navigator) could
readtheGitHubissueandthinkaboutwhether iisapplicablefor
the appAppnaviдator (in which Appnaviдator/nequalAppA) (Step2
in Figure 5). This step requires the navigator to think creatively
about whether there exist some łsimilaritiesž between AppAand
Appnaviдator thatmayallow itobereproducibleon Appnaviдator
(we leaveitup tothe navigatorto definehis orher ownnotion of
similarities). If the navigator has successfully reproduced the issue
ini, then he or she will post the new issue jto the team discussion
(Step3in Figure 5). We call this a pair sharing withibeing the
originallysharedissueand jbeingthederivedissue.Theprocess
repeats when another member of the organization posts a new
GitHub issue that isdifferentfrom iandj. In this case, the greater
thenumberofpairsharingsforanissue,thegreaterthepotentialof
discoveringageneralbugthat isapplicabletomostAndroidapps
(e.g., the outdatedview buginSection 2).
3.3 The coder-vs-manual-issues setting
Inthecoder-vs-coderssetting,programmersneedtowaitpassively
for a new bug report. To improve the efficiency of collaborative
bugfinding,wedesignthecoder-vs-manual-issuessettingwhere
programmerscouldgetaone-on-onenavigatorforhelpingthem
findbugsin their apps. In thissetting,thedriver is a programmer
whowouldliketoperformcollaborativebugfindingbutdoesnot
belongtoanyorganization,whereasthenavigatorsareopen-source
Android apps with a list of GitHub issues. This setting aims to
6https://github.com/
7https://github.blog/2017-11-20-introducing-team-discussions/emulatethescenariowhereanoviceappprogrammerwouldlike
to getadvice for testinghisorher newlycreatedAndroid app.
Figure 6 shows the overall workflow for the coder-vs-manual-
issuessetting.Assumethataprogrammer A(driver)wanttofind
bugsthatmayaffecttheapp AppAthatheorshedeveloped.The
processstartswith Aselectingrandomlytwoapps:(1)anapp AppB
that belongs to the same category as AppAand (2) another app
AppCthat belongs to a different category than AppA. Selecting a
same category app allows Ato find specific bugs that may occur
in apps that perform similar tasks, whereas selecting a different
categoryappallows Atofindgeneralbugsthatmayoccuracrossall
Androidapps.Aftertheappselection, AchoosesfiveGitHubissues
(more issues could be selected if time allows) from AppBandAppC
respectively.Then, Awillthinkaboutwhetherthetestscenarios
in the selected issues could be reproduced in AppA. This step is
similar to Step 2 in Figure 5 where creative thinking is required.
4
DOESCOLLABORATIVE BUG FINDING
HELPS TO DISCOVER NEW BUGS?
Before evaluating the general applicability of collaborative bug
finding for testing Android apps, we first study the general fea-
sibility of collaborative bug finding as an approach that helps in
constructing test cases for Android apps through different degrees
of interactions. Our goal is to evaluate whether reading the bug
reports written by others about another app (another app plays
the role of a competent pair programmer) could lead to the cre-
ativethinkingofsimilartestingscenariofortheappundertestand
eventuallyleadtothediscoveryofnewbugsfortheappundertest.
Experiment setup for the coder-vs-coders setting. In a soft-
waretestingcourse(CS409)with29studentsinSouthernUniversity
of Science and Technology (SUSTech), we start the study by asking
each student to form a team of 3ś4. Each student is required to
choose an app for testing from a list of open-source Android apps8.
Notethatalthougheachstudentneedstotestadifferentappwithin
a team, we hope that the team setting will encourage collaboration
among teammembers. We allow students to select their preferred
app while providingsomeguidelines:
Easeofuse: The project can be compiled successfully without
errors, andcan be executedinadevice.This is compulsory.
Existing Tests: Theprojectcontainssometestcasesforvalidating
the correctness of the app.This iscompulsory.
Popularity: Theappcontainsrelativelylargenumberofstarsin
GitHubormanydownloads inGoogle Play.
ActivelyMaintained: There are recent commits to the projects.
Likelihood offinding new bugs: Projectswithmanybugreports
may indicatethat itis easier to find bugs in these projects but less
likely to find new bugs(not duplicatesof existing bugs).
Size:The projectcontains manyJava classesthat can be tested.
Theseguidelinesaimtohelpstudentsinjudgingthesuitabilityof
eachappfortesting.Toensurediversityoftheselectedapps,wealso
impose the rule that only amaximumof threeteams could select the
same app and all apps selected by each member within a team must
be distinct. This selection results in 20 selected apps. For the coder-
vs-coders setting, we encouragestudents to participate by sharing
8https://github.com/pcqpcq/open-source-android-apps/blob/master/README.md
1338Collaborative BugFinding forAndroid Apps ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
/uni00000031/uni00000052/uni00000059/uni00000003/uni00000015/uni00000016 /uni00000031/uni00000052/uni00000059/uni00000003/uni00000015/uni0000001b /uni00000027/uni00000048/uni00000046/uni00000003/uni00000013/uni00000016 /uni00000027/uni00000048/uni00000046/uni00000003/uni00000013/uni0000001b /uni00000027/uni00000048/uni00000046/uni00000003/uni00000014/uni00000016 /uni00000027/uni00000048/uni00000046/uni00000003/uni00000014/uni0000001b /uni00000027/uni00000048/uni00000046/uni00000003/uni00000015/uni00000016 /uni00000027/uni00000048/uni00000046/uni00000003/uni00000015/uni0000001b
/uni00000027/uni00000044/uni00000057/uni00000048/uni00000056/uni00000003/uni00000057/uni0000004b/uni00000055/uni00000052/uni00000058/uni0000004a/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000046/uni00000052/uni00000058/uni00000055/uni00000056/uni00000048/uni00000003/uni00000053/uni00000055/uni00000052/uni0000004d/uni00000048/uni00000046/uni00000057/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000045/uni00000058/uni0000004a/uni00000056/uni00000003/uni00000049/uni00000052/uni00000058/uni00000051/uni00000047/uni00000026/uni00000052/uni0000004f/uni0000004f/uni00000044/uni00000045/uni00000052/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000045/uni00000058/uni0000004a
/uni00000003/uni00000049/uni0000004c
/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000056/uni00000057/uni00000044/uni00000055/uni00000057/uni00000056/uni00000026/uni00000052/uni0000004f/uni0000004f/uni00000044/uni00000045/uni00000052/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000045/uni00000058/uni0000004a
/uni00000003/uni00000049/uni0000004c
/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000048/uni00000051/uni00000047/uni00000056
Figure 7: The total number of bugs found throughout the semester
via
coder-vs-manual-issues settingofcollaborativebugfinding
thebugsthattheydiscoveredintheorganizationdiscussionpage
(all students belong to the same organization). To encourage active
participation,eachstudentwillreceive5bonuspointsforeachnew
bug reported to the app developer. At the end of the course, we
manually analyzed all the issues posted in the discussion page, and
attempt to answer twoquestions:
RQ1a:Howmanypairsharingsexistinthediscussionpagethrough-
outthe entire course of the semester?
RQ1b:Whatare the types of bugsfoundthroughpair sharing?
Results for coder-vs-coders setting. Overall, 14 students have
shared29GitHubissuesfor11open-sourceAndroidapps(these-
lected apps could be the same for some students). Among these 29
issues,we only observe five pair sharings. Meanwhile, the remain-
ing bugs shared in the discussion page are discovered via other
testingmethods,whichwillbediscussedinthesubsequentpara-
graph. Interestingly, all the five pair sharings are related to the
outdated view bugsdescribedin Section 2.
AnswertoRQ1: Therearefivepairsharingsinthediscussion
page.Allof these bugsleadto outdated view.
Experimentsetupforthecoder-vs-manual-issuessetting. To
increase the number of discovered bugs via collaborative bug find-
ing, we posted an assignment via GitHub Classroomthat required
students to write tests for their selected apps using two strategies:
(Method1)coveringcodechangesforrecentcommit(wecallthis
strategyregression-inspired ), and (Method 2) the coder-vs-manual-
issuessettingofcollaborativebugfinding(refertoSection3.3more
details). We compare these two strategies because both techniques
rely on another similar app for constructing test cases (i.e., code
changes relies on a different version of the same app, whereas
collaborative bug finding uses a similar app as driver). For the
regression-inspiredstrategy,studentsarerequiredtowritetwotest
cases to verify the correctness of the changed code for the most
recentcommit.Notethatweaskstudentstowritetwotestcasesso
that they could design the tests to check both the normal behavior
and the exceptional behavior. Using the coder-vs-manual-issues
strategy, students are required to select five issues from an app
within the same category, and five issues from a different category
app (the category of an app is defined in this link9). For each se-
lected issue i, students need to manually check whether they could
9https://github.com/pcqpcq/open-source-android-appsreproducesimilarbugsintheirselectedappsbasedon iandwritea
newderived issue (issue that are inspired by i). To collect students’
feedback for these two strategies, we asked students to answer the
question: łWhich method do you think is more effective in finding
newbugs?Why?Explainthereasonintermsofefficiency(timetaken)
and effectiveness (likelihood of finding new bugs)ž. Our goal is to
answer the following questions:
RQ2aWhat is the percentage of relevant GitHub issues when
referringtotheissuesderivedfromsamecategoryappversus
thosefrom differentcategory app?
RQ2bCompared to other testing approaches, how many bugs
couldcollaborativebugfindingdiscover?Whatarethetypes
of bugsfound?
RQ2cConsidering students’feedback, what are theeffectiveness
and efficiency of collaborative bug finding versus construct-
ing tests basedoncode changes?
RQ2a: Same category app versus different category app. We
investigatewhethertheappofthesamecategoryortheappofa
differentcategoryismoresuitableasthedriverappforcollabora-
tive bug finding.In total, there are 27studentsubmissionsfor this
assignment(i.e.,twostudentsdidnotsubmit).These27students
have selected 260 GitHub issues (131 issues from same category
app, and 129 issues from different category app). Apart from se-
lecting bug-related GitHub issues, we realized that students also
selectGitHubissuesthat include feature requests, buildproblems,
questions, and documentation requests. For each issue, we classify
its type andmeasure whether itis:
Definition 4.1. Relevant GivenAppquery, an issue iis relevant
if similar functionality and steps to reproduce mentioned in iexist
inAppqueryanddoes not leadto unexpectedbehavior.
Definition4.2. Reproducible GivenAppquery,anissue iisre-
producibleifsimilarfunctionalityandstepstoreproducementioned
iniexist inAppqueryandleadto unexpectedbehavior.
Table2showstheresultsofourmanualinspection.ThełTypeof
Issuesž column in Table 2represents the type of issues selected by
students.Meanwhile,thełRelevancežcolumnmeasureswhether
anissue isreproducibleorrelevant.The łSameCategoryžcolumn
andthełDiff.Categoryžcolumndenotesthepercentageofissues
thatarerelevanttotheappofthesamecategoryandthepercent-
age of issues that are relevant to the app of a different category,
respectively.Notethatstudentscouldchoosefromappsthatbelong
to17categorieslistedat10.Foreachtypeofissue,%reproducible
+ % relevant + % irrelevant (not shown in Table 2) = 100%. Over-
all, 21.18%of bug-relatedissues selectedfrom samecategoryapps
couldbereproducedintheselectedapps,whereasonly11.96%of
bug-relatedissuesselectedfromappsofdifferentcategorycould
be reproduced. Although selecting bug-related GitHub issues from
same category app is more likely to be reproducible, our results
show that other types of issues (including features, build, question
and documentation) from apps of different category could contain
relevant information that may inspire the further improvement of
other software artifacts. Interestingly, students identified 10 derived
issuesthathavebeenpreviouslyreported.Innineoftheseissues,
10https://github.com/pcqpcq/open-source-android-apps/blob/master/README.md
1339ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea Shin HweiTanandZiqiangLi
Table 2: The issue relevance comparison between app of same cate-
goryand appof differentcategory
TypesofIssues Relevance Same Category Diff.Category
bugreproducible 21.18% 11.96%
relevant 32.94% 32.61%
featurereproducible 33.33% 21.74%
relevant 16.67% 21.74%
buildreproducible 20.00% 66.67%
relevant 33.33% 0.00%
questionreproducible 20.00% 0.00%
relevant 20.00% 0.00%
documentationreproducible 50.00% 80.00%
relevant 0.00% 0.00%
we realized that the issues for the driver app and the navigator
app are almost identical, which confirms with our assumption that
similarbugscouldoccuracrossdifferentapps.Meanwhile,inoneof
these derived issues of the AnExplorer app, a user mentioned that
łThisproblemexistsinAnExplorerbutnotAmazeExplorer,however
AnExplorer is required due to its other capabilities.ž11. This example
providesconcrete evidence of the potential usage of collaborative bug
finding beyond the educational setting.
AnswertoRQ2a: Amongtheselectedbug-relatedissues,21.18%
ofissuesfromsamecategoryapparereproducible,whereasonly
11.96%ofissuesfromdifferentcategoryappsarereproducible.
However,selectingnon-bug-relatedissuesfromdifferentcate-
goryappsmayprovideinsightsforfutureimprovementofother
software artifacts (e.g., buildscripts, anddocumentation).
RQ2b:Numberofbugsfoundandtheirtypes. Throughoutthe
course,studentsalsolearnaboutwritingtestsbasedonseveraltest-
ingcriteria(e.g.,graphcoveragebasedonmanuallydrawnEvent
FlowGraphsandInputDomainModeling(IDM)[ 9]).Meanwhile,
Monkey12, an automated tool for stress testing Android app, is the
onlyautomatedtestingtooltaughtinthecourse.Figure8showsthe
numberofnewbugsdiscoveredforallthetestingapproaches. In
general,itmaybedifficulttokeeptrackoftheexacttestingtechnique
used to find a bug. However, this is not the case for collaborative bug
finding because for each new bug found, we could refer to its original
GitHubissue.Overall,collaborativebugfindinghelpsstudentsin
discovering 17 out of 29 new bugs. We also analyze how the ac-
cumulativetotalnumberofreportedbugsinthediscussionpage
evolves over time, including the key dates where we distributed
the assignment and the deadline of the assignment. Figure 7 shows
our analysis results. We can observe from Figure 7 that the total
numberofreportedbugsincreasessignificantlyafterdistributing
the assignment for collaborative bug finding. Specifically, Mann-
WhitneyUTestshowsthatthedifferencebetweenthenumberof
bugs found using our approach and the number bugs found us-
ingotherapproachesisstatisticallysignificantwith p<0.05.On
theotherhand,similartopriorstudyofthetestingpreferenceof
Androiddevelopers[ 44],studentsusemanualtestingforfinding
fivenewbugs.Comparedtomanualtestingapproaches,Monkey
only helps students in finding two new bugs. To understand the
11https://github.com/1hakr/AnExplorer/issues/98
12https://developer.android.com/studio/test/monkey/uni00000030/uni00000052/uni00000051/uni0000004e/uni00000048/uni0000005c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b
/uni00000026
/uni00000052/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni0000002c/uni00000027/uni00000030 /uni00000030/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f
/uni00000057/uni00000048
/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000026/uni00000052/uni0000004f/uni0000004f/uni00000044/uni00000045/uni00000052/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048
/uni00000045
/uni00000058/uni0000004a/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000025/uni00000058/uni0000004a/uni00000056/uni00000003/uni00000029/uni00000052/uni00000058/uni00000051/uni00000047/uni00000037/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000053/uni00000053/uni00000055/uni00000052/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000056
Figure8:Thenumberofnewbugsdis-
co
vered through different testing ap-
proaches across 11 apps.
Not update 
immediately ; 
29%
Certain mode 
(e.g., Unsilent, 
Pop-Up); 
12%Certain name 
(e.g., folder 
with similar 
name); 18%Changing 
language ; 
12%Others ; 29%Figure 9: Types of bugs
found
through collabora-
tivebugfinding.
reasonsbehindtheineffectivenessofMonkeyinthediscoveryof
newcrashes,wereadthefinalreportswrittenbystudentsthatsum-
marized their learning experience and realized that students found
three more bugs using Monkey but did not report them because (1)
onecrashbecomesirreproduciblewhentestingmanually;and(2)
mostof the discoveredcrasheshave been previously reported.
Tounderstandthetypesofbugsthatthefirsttwosettingshelpto
discover, we carefully readall the 17 GitHub issues discovered via
ourapproach.Specifically,welookforcommonkeywordsamong
all the GitHub issues. Figure 9 shows the results of our manual
analysis. The results show that the update problem described in
Section 2is the most common problem (29%) amongall the newly
discovered bugs. Another common problem (18%) is that when
certainnamesareusedforcreatinganewfileoranew folder,the
app will exhibit unexpected behavior. For example, when the user
ofAmaze File Manager (file manager app) tried to create a folder
with the same name as the current folder, it fails but the renaming
shouldhavebeensuccessful.In12%ofthereportedbugsdiscovered
through collaborative bug finding, when using some apps with
different modes (e.g., non-silent mode and Pop-Up mode), the app
behaves incorrectly. In 12% of the reported bugs, when app user
tries to change the language either via the options in the app or
open a file written in a non-English language, the displayed text is
incorrect. For 29% bugs discovered via collaborative bug finding,
they are non-crashrelatedandspecific to the selectedapps.
AnswertoRQ2b: Thecoder-vs-manual-issuessettingofcollab-
orative bug finding helps students in the discovery of 12 new
bugs. The improvement in the total number of bugs found is
statistically significant. The bugs discovered via collaborative
bug finding include: (1) prevalent problems affecting many apps
(29%ofthemarerelatedtotheoutdatedview,18%ofthemare
related to theusage of specificnames for file or folder creation,
12%ofthemarerelatedtoopeningtheappinspecificmode,12%
arerelatedtoincorrectbehaviorwiththechangeoflanguage),
and(2) specific problems that affectthe selectedapps (29%).
RQ2c: Comparison with regression-inspired approach. We
manuallyreadthroughthe25answersforthequestionthatcom-
pares (Method 1) regression-inspired approach, and (Method 2)
collaborativebugfinding.Figure10showsthestudents’feedback
whencomparingthesetwostrategies.Overall,thereare16students
whoprefercollaborativebugfindingforthediscoveryofnewbugs,
1340Collaborative BugFinding forAndroid Apps ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
/uni00000026/uni00000052/uni0000004f/uni0000004f/uni00000044/uni00000045/uni00000052/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048
/uni00000045
/uni00000058/uni0000004a/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000010/uni0000004c/uni00000051/uni00000056/uni00000053/uni0000004c/uni00000055/uni00000048/uni00000047 /uni00000027/uni0000004c/uni00000047/uni00000051/uni0000000a/uni00000057/uni00000003/uni00000044/uni00000051/uni00000056/uni0000005a/uni00000048/uni00000055/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000059/uni00000052/uni00000057/uni00000048/uni00000056/uni00000037/uni00000048/uni00000056/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000053/uni00000053/uni00000055/uni00000052/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000056
Figure 10: Students’ feedback on the comparison between collabo-
rativ
ebugfindingand regression-inspiredapproach
whereas 9 students prefer the regression-inspired approach, and
twostudentsdidnotanswerthequestion.Forexample,onestudent
acknowledgedthebenefitsofcollaborativebugfinding: łBecause
many functions in the app in the same category are similar even
totally same ... and others’ report will also inspire the mind to find
bugswhichIneverconsidered.ž.Meanwhile,anotherstudentcom-
mented on the shortcoming of the coder-vs-manual-issues setting:
łmethod 2 takes more time to review different apps and search useful
issues, while method 1 only needs to focus on small code changes.
Butmethod2ismorelikelytofindnewbugsž.Intotal,15students
sharesimilarconcernsontheefficiencyofcollaborativebugfinding.
Based on general feedback that searching for relevant issues could
be time-consuming, we designthe coder-vs-auto-issues setting.
Answer to RQ2c: 16 out of 25 students prefer collaborative
bug finding over regression-inspired approach. However, 15
students admitted that searching for relevant issues could be
time-consuming.
5 ISSUERECOMMENDATION ALGORITHM
Based on the students’ feedback on the weakness of the coder-
vs-manual-issues setting, we propose Bugine, an approach that
automatically selects relevant GitHub issues for the app under test
AppAso that developers for AppAcould focus on the łcreative
thinkingž step to derive specialized test scenarios. Bugine’s key
stepsinclude:(S1)usingnaturallanguageprocessingtofindsimilar
appsbyrepresentingthecommonUIcomponentsof Appqueryand
Appdatabase as app description files, (S2) automatically construct-
ing queries from (S1) to select relevant issues, and (S3) ranking
thembasedontheir qualities.
Figure 11 shows the overall workflow of our issue recommenda-
tionsystem, Bugine.OurapproachfirstbuildsadatabaseofGitHub
issues obtained from open-source Androidapps and pre-processes
theseissuestoextracttheirmetadata.Foreachappinourdatabase,
we extract its app description file for future comparison. Given
an app under test Appquery,Bugineextracts its UI components to
obtainitsappdescriptionfile.Then,weusethesimilaritiesbetween
theapp descriptionfilefor Appqueryandtheappdescriptionfiles
foralltheappsinourdatabasetosearchforappsthataresimilarto
Appquery.Thissimilarityisgivenasinputtoourrankingfunction,
whichprioritizestheGitHubissuesinourdatabase.Finally, Bugine
outputs arankedlistofrelevant GitHubissuesfor Appquery.
Figure11:WorkflowforourGitHubissuesrecommendationsystem
5.1
Building a database ofGitHub issues.
To build our database, we first obtain a set of open-source Android
appsbycrawlingGitHub.Ourcrawlerselectsanapp Abasedon
(1)thenumberofstarsinGitHub,(2)thenumberofreviewsand
thenumberofdownloadsinGooglePlay,(3)thenumberofGitHub
issues, and (4) its category. For each selected app, we extract all its
issuesandcollectedthe metadataofeachissue(e.g.,title,author,
numberofusercomments,labels,issuestate,body,commitSHA,
etc.).Wealsodownloadeditssourcecodefromitsmasterbranch
for subsequent steps. This results in a total of 23980 issues from
34 different applications that provide 10 different functionalities
(e.g., cloud client, file explorer, web browser, notes, picture gallery,
GitHubclient, etc.).
5.2 Data Pre-Processing
As all the GitHub issues in our database are written in natural
language,weuseNaturalLanguageProcessing(NLP)techniques
topre-processthembeforestoringthemintoourdatabase.Wealso
perform similar pre-processing for all the XML files. Specifically,
we perform the following steps:
Tokenization: We converteachissueintolistsof words(tokens ).
Stopwordsremoval: Stopwordsarecommonlyusedwordswhich
donothavecompletelexicalmeaning(e.g.łthež,łisž,łthatž).We
use the PythonNLTKlibrary [3]to remove stopwords.
Convention unification: Considering the naming convention of
different Android UI components, Bugineuses Humps [ 4], a
python library that converts strings between snake case, camel
case and pascal case, to unify the different naming conventions
usedinvariablenaming.Wereplaceeachunderscorewithwhite
spaceandseparate eachcomposite word.
Stemming & lemmatization Stemming reduces inflected words
to their base form, whereas lemmatization groups together the
inflected forms of a word so that they can be analyzed as a single
item.Weusestemmingandlemmatizationforreducinginflectional
forms ofthe parsedtokens.
After the data pre-processing, we convert the corpus into token
streams,whichwillbe usedin the subsequent step.
1341ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea Shin HweiTanandZiqiangLi
Table3: UIdescriptionpatterns
Component Description Example ExtractedNames
Resourcename Resource name andr oid:id="@+id/my_btn" my_btn
Viewname Thenameforthe typeofUIcomponent. <Button andr oid:id="@+id/my_btn" /> Button
XML file name Layout name main_lay out.xml main_layout
5.3 Extracting app descriptionfiles.
The user interfaces of Android apps are commonly defined in XML
layout files [ 2]. These files contain the definitions for the interface
elements and their structures. Android resources are the additional
filesandstaticcontentusedintheappcode(e.g.,bitmaps)13.Re-
sourcescanbereferencedviatheirresourceIDs.Theautomatically
generated file R.javacontains allresourcesandtheir IDs.
Inthisstep, BugineidentifiesandparsesalltheXMLfileswithin
thefolder src/main/restogenerateappdescriptionfiles.Wealso
discard element styles (e.g., colors and fonts) which are not essen-
tialtoappsandretainonlytheimportantattributesthatdescribe
app behaviors. For each pair of apps ( AppA,AppB), we compare
andmatchtheirappdescriptionfiles.Thisdescriptionfileisused
for (1) classifying the category of the app (we assume that if the
sets of UI components in AppAandAppBare similar, then AppA
andAppBare of the same category), and (2) generating candidates
search keywords for identifying the relevant GitHub issues. Then,
Buginetransforms these app descriptions into query phrases by
summarizing the common parts in the two app description files.
Table 3 shows the naming information in the XML files that we
usedtoextractUIcomponentsthatarecommonbetweentwoapps.
This informationhelps us toconvert for eachXML file, each view
andeachresourcein Appqueryto the query of the form:
XML file name∧View Name∧Resource name
SimilarityMeasures. Weusetwocommonlyusedsimilaritymea-
suresforcomputingtextsimilarities.Tomeasurethesimilarities
between the query and GitHub issue titles, we use Overlap Coef-
ficientthat computes the overlap between two finite sets Xand
Y[42]. Namely,Overlap Coefficient is defined as:
overlap(X,Y)=|X∩Y|
min(|X|,|Y|)(1)
IfXisthe subset of Yorvice versa, then overlap(X,Y)isequal
to1.Itrangesbetween[0 ,1].WechooseOverlapCoefficientover
the Jaccard Similarity [ 37] and Dice Similarity [ 22] because (1) the
searchqueryisusuallyshorterthanthecorpusinthedatabase,and
(2) itis sensitive to the size of the twosets.
To measure the similarities between the query and the text bod-
iesofGitHubissues,weusethen-gramsimilaritybecause(1)ithas
beenwidelyusedinmodelingnaturallanguage[ 62,69],(2)Overlap
Coefficient does not consider context (i.e., the surrounding words)
butthetextbodiesusuallycontaindetailedinformationandformal
structural sentences. For the same reasons, we also use n-gram
for calculating the UI components similarities. Specifically, we use
the standard character-based n-gram from the Python NGram li-
brary[5]withthedefaultvalueofn=3.Tofinditemsthataresimilar
toaquery q,theNGramlibrarywillsplitthequeryinton-grams,
13https://developer.android.com/guide/topics/resources/accessing-resources.htmlTable4: Factors usedinrankingsearch results
Factor Description
Issue length Wordcount ofissue body(int)
Issue status Closedor opened(binary)
RefcommitSHA Commit SHAreferencedby issue (binary)
Issue replynum The number ofreplies thatanissue received(int)
Hit_all Findall searchkeywordsinthe corpus (binary)
Hit_overlap OverlapCoefficientbetweensearchkeywords andcorpus (float)
Hit_hot_words Wordcount ofdescriptivehot wordslike r eproduce, defect(int)
collectallitemssharingatleastonen-gramwith q,andcomputethe
similarityscorebasedontheratioofsharedtounsharedn-grams
between strings.
We calculatethe similaritybetween twocorpora using:
дsim(C1,C2,W)=n/summationdisplay
i=1h(c1i,c2i)×wi (2)
In this equation, h(c1,c2)denotes the function used for compar-
ingtextsimilarities(e.g.,OverlapCoefficientorn-gram), C1andC2
are the pairs of corpora, c1iandc2iare the i-th part of the corpora,
andWassignsaweightto eachterm.
5.4 Ranking Relevant GitHub Issues.
The goal of this step is to produce a ranked list of relevant GitHub
issuesfor Appquery.Inprevioussteps,weobtainsimilarappsfor
Appquery. We use each query phrase extracted from the app de-
scriptionfilestosearchforrelatedissuesandorderedtheresults
by their importance, relevance and reproducibility. We use several
factors to rank the relevant issues. A key factor that determines
the quality of a GitHub issue iis how well the test scenario has
been described in i. An empirical study on Apache, Mozilla and
Eclipse [80] shows that developers expect a good bug report to
includestepstoreproduce, observedandexpectedbehavior,and stack
traces.Thestudyshowsthatadetailedbugreportcanhelpdevel-
opersin narrowing down the search space for bugs and save time
on fixing bugs. Based on the results of this study, we design the
metrics used to evaluate the quality of eachissue.
Table4showsthesevenmetricsthatweusedforrankingGitHub
issues.Particularly,the Issue length measureshowdetailedan
issueis(weassumethatlongerissuecontainsmoreinformation),
whereasthe Issue status depictsitsimportance(weassumethat
closedissuestobemoreimportant).The Ref commit SHA metric
checks if the problem mentioned in issues has been fixed (as evi-
dencedbythepresenceofacommitSHA).Weinclude Issue reply
numbased on the assumption that issues with greater number of
replies are more important. We use both Hit_all and Hit_overlap
for finding the string similarities because we believe that if all the
searchkeywordsappearinthecorpus( Hit_all),thenthetextshould
be given additional weights. The Hit_hot_words factor assigns a
1342Collaborative BugFinding forAndroid Apps ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
higherscore toGitHubissueswith wordsthatrepresentthechar-
acteristicsof agoodbugreport [80].
Given an issue e, a component Cthat is similar to the search
keywords, we calculatethe rankingscore S(e,⃗C,W):
S(e,⃗C,W)=n/summationdisplay
i=1fi(e,⃗C)×wi (3)
wherefi(e,⃗C)denotesthevalueoffactor fionissueeandsearch
keywords C;andwidenotestheweightforfactor fi.Weperforma
grid search to tune the weight for eachfactor.
6 EVALUATION
To the best of our knowledge, Bugineis the first approach that
ranks GitHub issues for Android apps. The approach closest to
Bugineisthe advancedsearchfeature in GitHub that canbe used
tonarrowdownthesearchforissuesrelatedtoAndroidappsand
filter the issue status, the number of comments, the date of the
last update, and the label of each issue. However, there is no direct
method to (1) filter issues from Android apps, (2) search for the UI
componentsforanappnor(3)selectthecategoryofanapp.Hence,
we could not find any suitable baselineapproaches.
Weperformevaluationontheeffectivenessof Buginetoaddress
the following research questions:
RQ3aWhat is the overall performance of Buginein recommend-
ing relevantGitHubissues?
RQ3bHowmanyreal bugscan Buginefind?
6.1 ExperimentalSetup
Weevaluate Bugineonfiveopen-sourceAndroidapps.Table5lists
informationabouttheevaluatedapps.Theł#Downloadžandthe
łRatingžcolumndenotethenumberofdownloadsandtherating
in Google Play, respectively. We select these apps for evaluation
becausetheyarediverseintermsofappcategory,sizes(5ś31Klines
ofcodes),popularity(60ś378starsinGitHub),andthenumberof
issues.Although zappisnotablefor downloadinGooglePlay,we
choose this app because it was the most recently updated app in
GitHub with frequent releases, which indicates that it has been
activelymaintainedbythe developers.
We evaluate the overall performance of Bugineusing two mea-
suresusedinpriorevaluationsofrecommendationsystems[ 75,78]:
Prec@k measurestheretrievalprecisionoverthetop kdocuments
in the rankedlist:
Prec@k=#of relevant docs intop k
k(4)
W
e measure the precision at k =5, 10, 20, 50.
Mean ReciprocalRank (MRR) For each query q, the MRR mea-
sures the position firstqof the first relevant document in the
rankedlist[68]:
MRR=1
|Q||Q|/summationdisplay
q=11
firstq(5)
The
higher the MRR value,the betterthe rankingperformance.
Rater A Rater B Rater A Rater B Rater A Rater B Rater A Rater B Rater A Rater B
Camera-Roll PocketHubSimple File
Managerzapp Simpletask
Prec@5 0.4 0.4 0.4 0.4 0.8 0.4 0.2 0.4 0.2 0.2
Prec@10 0.2 0.2 0.5 0.5 0.7 0.5 0.5 0.3 0.1 0.2
Prec@20 0.3 0.4 0.45 0.6 0.55 0.65 0.3 0.25 0.15 0.2
Prec@50 0.34 0.42 0.56 0.56 0.44 0.5 0.28 0.28 0.24 0.2400.20.40.60.8Precision@kFigure12: ThePrec@k results for Bugine
Camera-Roll PocketHubSimple File
Managerzapp Simpletask
Rater A 0.48 0.75 0.58 0.39 0.46
Rater B 0.39 0.67 0.53 0.34 0.430.000.200.400.600.80MRR
Figure13: TheMean ReciprocalRank (MRR)results for Bugine
ForRQ3aandRQ3b,weusethesamedefinitionsofrelevanceas
RQ2a(Def.4.1andDef.4.2).Forallthereproducibleissuesdiscov-
ered, we reportedthemto the corresponding app developers.
All experiments were conducted on a machine with Intel (R)
Core (TM) i7-8700CPU @3.2GHz and32 GB RAM.
6.2 RQ3a: Ranking Performance of Bugine.
Figure 12 shows the Prec@k results for Bugine, whereas Figure 13
showstheMRRresultsfor Bugine.Weincludethecomputedvalues
forthetworaters(RaterAandRaterB)inbothresultsforbetter
comparison. The Prec@10 results range from 0.1 to 0.7, which
means that among the top 10 issues recommended by Bugine,
there is at least one relevant issue. Meanwhile, the MRR values
forBuginerange from 0.34 to 0.75, which means that the ranking
forthefirstrelevantdocumentrangesbetween3rd(0.34)and1st
(0.75).Comparedtothepreviousrecommendationsystemthatranks
source files for bug reports [ 75], we think that the MRR values are
relativelyhighastheirMRRvaluesonlyrangefrom0.2to0.55.This
indicatesthat Buginecouldrecommendrelevantissuesformost
of the evaluated apps (especially for PocketHub andSimple File
Manager as theirPrec@5andMRR valuesare relativelyhigh).
WeobservethatthePrec@kandtheMRRresultsfor SimpleTask
arerelativelylow.Thiscouldbeduetothefactthatitisanappwith
limited number of features. Although it has the greatest number
ofGitHubissuesamongallevaluatedapps(821),werealizedthat
mostofitsGitHubissuesarenotbug-related(e.g.,featurerequests)
withonly20%oftheseissuesaremarkedasłbugž.Anotherinter-
esting observationis that the two reviewers(Rater A and Rater B)
havedifferentPrec@kvaluesfor Simple File Manager .Wethink
thatthisdifferenceisduetothefactthatcraftingspecializedtest
1343ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea Shin HweiTanandZiqiangLi
Table5: Statistics andEvaluation ResultsoftheAndroidappsused
AppName Category KLOC #Downloads Rating Version No.#GitHub#GitHubIssue #BugsFound
Stars (closed) (new,old)
Camera-Roll Gallery 26.00 100,000+ 4.2 1.0.6 420 227(133) (11,0)
PocketHub GitHub client 31.35 10,000+ 3.3 0.5.1 9429 644(526) (12,2)
Simple File Manager Explorer 5.84 50,000+ 4.5 6.3.4 378 189(130) (6,2)
Zapp Broadcast 8.41 N.A. N.A. 3.2.0 60 151(137) (2,7)
Simpletask Reminder 24.80 10,000+ 4.7 10.3.0 349 821(583) (3,2)
scenariosrequirecreativethinkingwhichisdifferentacrosspeo-
ple. Nevertheless,the average inter-rateragreement is 0.76, which
indicates substantial agreement between the tworeviewers.
Answer to RQ3a: The Prec@k and the MRR value show that
Buginecould recommend relevant issues for all evaluated apps.
6.3 RQ3b:Numberofbugs that Buginefinds.
Givenan Bugine’sissue,weevaluateRQ3bbymanuallyreplicating
the issue to check if it is reproducible in the app under test and we
considerthat Buginediscoversabugiftheissueisreproducibleinthe
app under test. The ł# Bugs Foundž column in Table 5 shows the
numberofbugsdiscoveredby Bugine.Intotal,wefound34new
bugsand13oldbugsinallthefiveevaluatedapps.Comparedtothe
coder-vs-codersandthecoder-vs-manual-issuessettingwhere29
studentsfind17newbugswhenevaluatedon20apps, Buginecould
discover more bugs despite being evaluated only on five apps. This
shows that the effectiveness of Buginein recommending relevant
issues, whichleads to the discovery of newbugs.
Amongall the evaluatedapps, we are ableto findthe greatest
number of new bugs in PocketHub . This result matches with the
highPrec@k(Figure12)andtheMRRresults(Figure13).Moreover,
the fact that PocketHub has the lowest rating among all evaluated
apps indicates that many appusers encountered bugs when using
the app andthe likelihoodof finding newbugsis high.
AnswertoRQ3b: Buginerecommends34newbugsacrossfive
evaluatedapps.
7 THREATS TO VALIDITY
External. OurevaluationresultsmaynotgeneralizebeyondAn-
droid apps, the educational setting and testing approaches that we
evaluated. To mitigate this threat, we used a large number of open-
sourceAndroidapps. Furthermore,we alsoprovide guidelinesfor
students in their selection of the open-source apps to ensure the
diversity of subjects in terms of popularity, ease of use, contains
varioustestcasesandalargenumberofreportedGitHubissues.All
participants of our study are fourth year CS students, and they are
gradedbasedonthenumberofnewbugsdiscovered,whichmay
leadtoissues accordingto priorstudies[ 15,34]. We counteredthis
threat by using bonus points to increase student participation [ 26]
andrewardthesamebonuspointstostudentsregardlessofthetest-
ingapproachesusedinfindingthenewbugs.Tocounterthethreat
ofthegeneral applicabilityof our approach beyondtheclassroom
setting,wefoundsomeexampleswheredevelopersofAndroidapps
referredto otherrelatedapps when filing anewbugreport.Internal. We managed all the assignments via GitHub Classroom,
and wrote scripts to automate the download of GitHub issues from
thediscussionpage.Duringthemanualinspectionandclassification
ofeachbug,wehadtwoundergraduatestudents(oneofthembeing
theauthorofthepaperandanothernon-author)inspecttheresults
independentlytoavoidtheinfluenceontheresults.Toavoidbias
during the computation of the rank results for Bugine, we shuffled
the rank results before presenting themto the tworeviewers.
Construct. We used the number of newly discovered bugs to com-
paredifferenttestingapproachesbutotheraspects(e.g.,timetaken
to find bugs and the coverage of the generated tests) could be af-
fectedbyusingdifferenttestingapproaches.Wemitigatethisthreat
by including and manually analyzing students’ feedback on the
potential benefits ofcollaborative bugfinding.
8 RELATED WORK
CollaborativeProgramming. Pairprogrammingisaformofcol-
laborativeprogrammingwheretwopeopleareworkingtogether
onthesameprogrammingtask[ 54].Previousevaluationsofpair
programming show that the interaction between two program-
mers is effective in producing high quality software [ 13,51,71].
However, prior studies revealed that pair programming may not
be cost-efficient [ 12,19,54]. Meanwhile, prior research focuses
onthecooperativeaspectsoftesting[ 46,47,72].Incollaborative
testingforcomponent-basedsystems,previousstudiesshowthat
when several component-based systems use common components,
testersofsuchsystemscanreducetestingcostsandimprovetest
effectiveness by sharing test artifacts [ 46,47]. However, the pro-
posedsolutionsrelyontheirowndatasharinginfrastructurefor
sharing test data across multiple component-based systems, which
may be impractical. Similar to our approach that uses interactions
for testing, the Timeline Tool leverages user interactions for fail-
ure reproduction [ 58]. Different from all these approaches, our
approach uses different degrees of interactions between the driver
andnavigatorsfor constructing test scenarios for Android apps.
Crowdsourcing. Priorworkshowpromisingresultsinusing crowd-
sourcing[35] (a concept where various activities are outsourced
to a group of people through online platforms) for performing
several testing activities including mutation testing [ 59], usability
testing [31,45], GUI testing [ 23], and for education projects [ 16].
In the context of Android apps, Polariz combines crowdsourcing
andsearch-basedtestingtoenhancetheactivitycoverageofgen-
erated tests [ 50]. Meanwhile, MoTiF uses crowdsourcing for col-
lecting execution traces from users to reproduce context-sensitive
crashes[30].Theseapproachesrecruitcrowdworkersfortesting
severaldesignatedmobileapplications.Althoughhuman-written
1344Collaborative BugFinding forAndroid Apps ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
tests(collected fromthecrowd) haveshown to be complementary
toautomaticallygeneratedtests,collaborativebugfindingdiffers
from these crowdsourcing-based approaches in that: (1) there is
nodirectinteractionbetweencrowdworkerswhilewritingtests
in crowdsourcing, and (2) the crowdsourced tests are limited to
particular apps andmaynot generalize to otheruntestedapps.
GitHub. PriorresearchonGitHubfocusesonthecharacteristicsof
thesoftwarerepositorieshostedonGitHub[ 20,38],itstransparency
and collaboration [ 67], and its pull-based software development
model [32,33]. Several studies use GitHub as a collaborative learn-
ing platform for education [ 40,76]. We present the first study that
uses GitHubClassroom for teachingsoftware testing.
RecommendationsystemsforSoftwareEngineering. Several
recommendationsystemsexistforperformingsoftwareengineering
tasks [10,55,57,75,78]. These systems aim to assist with fault
localization based on ranking bug reports [ 55,75,78]. Different
from these approaches, Buginerecommends GitHub issues for
finding bugsin Android apps.
9 CONCLUSION
We proposecollaborative bug finding, an approach that promotes
the discovery of new bugs via different degrees of interactions
between driver and navigators. In the coder-vs-coders setting, pro-
grammers within an organization communicate by sharing their
newlyreportedGitHubissues.Inthecoder-vs-manual-issuesset-
ting,adeveloperfor AppAselectsseveralissuesfromsamecategory
app andfromdifferentcategoryapp toderive specializedtest sce-
narios for AppA. Meanwhile, in the coder-vs-auto-issues setting,
we introduce Bugine, an approach that automatically recommends
relevant GitHub issues for the app under test. Our evaluation of
allsettingsofcollaborativebugfindingshowsthatithelpsinthe
discovery of new bugs. Specifically, it helps in finding five bugs via
pairsharinginthecoder-vs-coderssetting.Inthecoder-vs-manual-
issues setting, students reported 12 new bugs. Meanwhile, Bugine
helps the discovery of 34 new bugs. The relatively high number
of newly found bugs confirms that collaborative bug finding is a
promising approach in testingAndroid apps.
We believe that this work opens several opportunities for future
research:
Reusingbugreportsfromdifferentapps. Theconceptofcollab-
orativebugfindingbuildsupontheideaoffindingbugsbyreferring
tobugreportsofothersimilarapps.Althoughwehaveonlyapplied
this concept in the context of Android apps, it may be generalized
toreusingbugreportsforothertypesofapplicationswithcommon
characteristics (e.g.,machine-learning applications).
Noveltestgenerationapproach. Insteadoftestinputgeneration,
we reformulate the test generation problem as bug report recommen-
dation problem. As bug reports are written in natural language,
our approach may help in relieving the burden of developers in
learning a new android testing framework or APIs. Meanwhile,
our evaluation shows that the types of bugs found via collabora-
tive bug finding are mostly non-crash related, whereas existing
automated testing approaches from Android apps focus on finding
crashes. Moreover, prior testing techniques [ 29,64] that extract
specifications from code comments in natural language indicate
potential benefits in improving automated testing with test oraclesobtained from bug reports. Hence, we believe that our approach is
complementary to existing automated Android testing approaches.
We leave as future work the investigation of whether collabora-
tive bug finding can be combined with automated Android testing
approachesto increasethe number of bugsfound.
Fully automatic collaborative testing. Automating collabora-
tive bug finding involves: (1) test transfer which requires mapping
all relevant UI components in the original issues to the derived
issues(theappdescriptionsfilesinBuginecouldbeused),and(2)
translatingreportinnaturallanguagetoreproducibletestscripts.
Bothofthesestepsareactiveresearchtopicswithpromisinginitial
results [77]. Collaborative bug finding recommends GitHub issues
whichareimportantprerequisiteforthesesteps,andcouldspark
future researchin these topics.
TeachingsoftwaretestingviaGitHubClassroom. Wepresent
thefirststudythatusesGitHubClassroomforteachingsoftware
testing. We have observed many benefits of using GitHub Class-
room for distributing students’ assignments and encouraging team
collaborations,whichareessentialespeciallyinthecoder-vs-coders
setting of collaborative bug finding. Moreover, as we have demon-
strated the effectiveness of collaborative bug finding in helping
students to find bugs in Android apps, we believe that the idea
ofusinganothersimilarapplicationasacompetitivełpairtesterž
could be potentially useful in other settings. In the future, it is
worthwhiletoinvestigatehowtoleverageGitHubandcollabora-
tivebugfindingforteachingothersoftwareengineeringcoursesor
principles(e.g.,test-driven development).
10 ACKNOWLEDGMENTS
Wethank YiWu, FranciscoRamirez, YingZhou,CS409 (Fall2018)
participants at SUSTech for their help with the experiments of
collaborative bug finding, and Sergey Mechtaev for discussions.
ThisworkispartiallysupportedbytheNationalNaturalScience
Foundationof China (GrantNo.61902170).
REFERENCES
[1]2016. Mobile Apps: What Consumers Really Need and Want A Global
Study of Consumers Expectations and Experiences of Mobile Applications.
https://docplayer .net/107560-Mobile-apps-what-consumers-really-need-and-
want-a-global-study-of-consumers-expectations-and-experiences-of-mobile-
applications .html. Accessed 2019-03-28.
[2]2019. Layouts. https://developer .android.com/guide/topics/ui/declaring-layout.
Accessed:2019-02-28.
[3] 2019. Natural Language Toolkit ÐNLTK. http://www .nltk.org
[4]2019. nficano/humps: Convert strings (and dictionary keys) between snake case,
camel case and pascal case in Python. Inspired by Humps for Node. https:
//github.com/nficano/humps
[5] 2019. ngram 3.3.2. https://pypi .org/project/ngram/
[6]SharadAgarwal,RatulMahajan,AliceZheng,andVictorBahl.2010. Diagnosing
mobileapplicationsinthewild.In Proceedingsofthe9thACMSIGCOMMWorkshop
onHotTopics inNetworks. ACM,22.
[7]Domenico Amalfitano, Anna Rita Fasolino, and Porfirio Tramontana. 2011. A
gui crawling-based technique for android mobile application testing. In Soft-
ware Testing, Verification and Validation Workshops (ICSTW), 2011 IEEE Fourth
InternationalConference on. IEEE,252ś261.
[8]Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Salvatore
De Carmine, and Atif M Memon. 2012. Using GUI ripping for automated test-
ingofAndroidapplications.In Proceedingsofthe27thIEEE/ACMInternational
Conference onAutomatedSoftwareEngineering. ACM,258ś261.
[9] Paul Ammann and Jeff Offutt. 2016. Introduction tosoftware testing. Cambridge
UniversityPress.
[10]Anupriya Ankolekar, Katia Sycara, James Herbsleb, Robert Kraut, and Chris
Welty.2006. SupportingOnlineProblem-solvingCommunitieswiththeSemantic
Web. InProceedings of the 15th International Conference on World Wide Web
1345ICSE ’20, May 23–29, 2020,Seoul,Republicof Korea Shin HweiTanandZiqiangLi
(Edinburgh,Scotland) (WWW’06).ACM,NewYork,NY,USA,575ś584. https:
//doi.org/10.1145/1135777 .1135862
[11]Steven Arzt, Siegfried Rasthofer, Christian Fritz, Eric Bodden, Alexandre Bar-
tel,JacquesKlein,YvesLeTraon,DamienOcteau,andPatrickMcDaniel.2014.
Flowdroid: Precise context, flow, field, object-sensitive and lifecycle-aware taint
analysis for android apps. Acm Sigplan Notices 49,6 (2014), 259ś269.
[12]AndrewBegelandNachiappanNagappan.2008. Pairprogramming:what’sin
it for me?. In Proceedings of the Second ACM-IEEE international symposium on
Empiricalsoftwareengineering and measurement. ACM,120ś128.
[13]JenniferBevan,LindaWerner,andCharlieMcDowell.2002. Guidelinesforthe
use of pairprogramming in a freshman programming class. In Proceedings 15th
Conference on Software Engineering Education and Training (CSEE&T 2002). IEEE,
100ś107.
[14]John Businge, Moses Openja, David Kavaler, Engineer Bainomugisha, Foutse
Khomh, andVladimirFilkov. 2019. Studying AndroidAppPopularitybyCross-
Linking GitHuband Google PlayStore.
[15]Jeffrey Carver, Letizia Jaccheri,SandroMorasca, and Forrest Shull. 2004. Issues
in using students in empirical studies in software engineering education. In
Proceedings. 5th International Workshop on Enterprise Networking and Computing
inHealthcareIndustry(IEEECat. No. 03EX717). IEEE,239ś249.
[16]ZhenyuChenandBinLuo.2014. Quasi-crowdsourcingtestingforeducational
projects.In CompanionProceedingsofthe36thInternationalConferenceonSoftware
Engineering. ACM,272ś275.
[17]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Au-
tomatedtestinputgenerationforandroid:Arewethereyet?(e).In Automated
SoftwareEngineering(ASE),201530thIEEE/ACMInternationalConferenceon.IEEE,
429ś440.
[18]Benjamin S Clegg, José Miguel Rojas, and Gordon Fraser. 2017. Teaching soft-
ware testing concepts using a mutation testing game. In 2017 IEEE/ACM 39th
International Conference on Software Engineering: Software Engineering Education
and Training Track (ICSE-SEET). IEEE,33ś36.
[19]Alistair Cockburn and Laurie Williams. 2001. Extreme Programming Exam-
ined. Addison-WesleyLongmanPublishingCo.,Inc.,Boston,MA,USA,Chap-
terTheCostsandBenefitsofPairProgramming,223ś243. http://dl .acm.org/
citation.cfm?id=377517 .377531
[20]Roberta Coelho, Lucas Almeida, Georgios Gousios, and Arie van Deursen. 2015.
Unveiling exception handling bug hazards in Android based on GitHub and
Googlecodeissues.In 2015IEEE/ACM12thWorkingConferenceonMiningSoftware
Repositories. IEEE,134ś145.
[21]Roberta Coelho, Lucas Almeida, Georgios Gousios, Arie Van Deursen, and
Christoph Treude. 2016. Exception handling bug hazards in Android. Empirical
SoftwareEngineering (2016), 1ś41.
[22]LeeRDice.1945. Measuresoftheamountofecologicassociationbetweenspecies.
Ecology26,3 (1945), 297ś302.
[23]Eelco Dolstra, Raynor Vliegendhart, and Johan Pouwelse. 2013. Crowdsourc-
ing gui tests. In 2013 IEEE Sixth International Conference on Software Testing,
Verification and Validation. IEEE,332ś341.
[24]StephenHEdwards.2003. Teachingsoftwaretesting:automaticgradingmeets
test-firstcoding.In ConferenceonObjectOrientedProgrammingSystemsLanguages
and Applications: Companion of the 18 th annual ACM SIGPLAN conference on
Object-orientedprogramming,systems,languages,andapplications,Vol.26.318ś
319.
[25]Sebastian Elbaum,Suzette Person,Jon Dokulil,and MattJorde. 2007. Bug hunt:
Making early software testing lessons engaging and affordable. In Proceedings of
the29thinternationalconferenceonSoftwareEngineering .IEEEComputerSociety,
688ś697.
[26]Joseph R Ferrari and Stephanie McGowan. 2002. Using exam bonus points as
incentive forresearchparticipation. Teaching ofPsychology 29,1 (2002), 29ś32.
[27] Christopher Frayling. 1993. Researchin art and design. (1993).
[28]XiangGao,ShinHweiTan,ZhenDong,andAbhikRoychoudhury.2018. Android
TestingviaSyntheticSymbolicExecution.In Proceedingsofthe33rdACM/IEEE
InternationalConferenceonAutomatedSoftwareEngineering (Montpellier,France)
(ASE 2018). Association for Computing Machinery, New York, NY, USA, 419ś429.
https://doi .org/10.1145/3238147 .3238225
[29]AlbertoGoffi,AlessandraGorla,MichaelD.Ernst,andMauroPezzè.2016. Au-
tomaticGenerationofOraclesforExceptionalBehaviors.In Proceedingsofthe
25th International Symposium on Software Testing and Analysis (Saarbrücken,
Germany) (ISSTA2016).AssociationforComputingMachinery,NewYork,NY,
USA,213ś224. https://doi .org/10.1145/2931037 .2931061
[30]María Gómez, Romain Rouvoy, Bram Adams, and Lionel Seinturier. 2016. Repro-
ducing context-sensitive crashes of mobile apps using crowdsourced monitoring.
In2016IEEE/ACMInternationalConferenceonMobileSoftwareEngineeringand
Systems(MOBILESoft). IEEE,88ś99.
[31]Victor HM Gomide, Pedro A Valle, José O Ferreira, José RG Barbosa, Adson F
DaRocha,andTMGdABarbosa.2014. Affectivecrowdsourcingappliedtousabil-
itytesting. InternationalJournalofComputerScienceandInformationTechnologies
5,1 (2014), 575ś579.
[32]Georgios Gousios, Martin Pinzger, andArie van Deursen. 2014. An exploratory
studyofthepull-basedsoftwaredevelopmentmodel.In Proceedingsofthe36thInternationalConference onSoftwareEngineering. ACM,345ś355.
[33]Georgios Gousios, Margaret-Anne Storey, and Alberto Bacchelli. 2016. Work
practicesandchallengesinpull-baseddevelopment:thecontributor’sperspective.
In2016IEEE/ACM38thInternationalConferenceonSoftwareEngineering(ICSE).
IEEE,285ś296.
[34]MartinHöst,BjörnRegnell,andClaesWohlin.2000. UsingstudentsassubjectsÐa
comparative study of students and professionals in lead-time impact assessment.
EmpiricalSoftwareEngineering 5,3 (2000), 201ś214.
[35] Jeff Howe. 2006. The rise of crowdsourcing. Wiredmagazine 14,6 (2006), 1ś4.
[36]CourtneyHsingandVanessaGennarelli.2019. UsingGitHubintheclassroom
predictsstudentlearningoutcomesandclassroomexperiences:Findingsfrom
a survey of students and teachers. In Proceedings of the 50th ACM Technical
SymposiumonComputer ScienceEducation. 672ś678.
[37]Paul Jaccard. 1901. Étude comparative de la distribution florale dans une portion
des Alpeset des Jura. BullSoc Vaudoise Sci Nat 37(1901), 547ś579.
[38]Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M
German, and Daniela Damian.2014. Thepromisesand perilsof miningGitHub.
InProceedingsofthe11thworkingconferenceonminingsoftwarerepositories .ACM,
92ś101.
[39]CemKanerandSowmyaPadmanabhan.2007. Practiceandtransferoflearning
intheteachingofsoftwaretesting.In 20thConferenceonSoftwareEngineering
Education & Training (CSEET’07). IEEE,157ś166.
[40]Csaba-Zoltán Kertész. 2015. Using GitHub in the classroom-a collaborative
learningexperience.In 2015IEEE21stInternationalSymposiumforDesignand
Technology inElectronic Packaging(SIITME). IEEE,381ś386.
[41]Pavneet Singh Kochhar, Ferdian Thung, Nachiappan Nagappan, Thomas Zim-
mermann, and David Lo. 2015. Understanding the test automation culture of
app developers. In 2015 IEEE 8th International Conference on Software Testing,
Verification and Validation (ICST). IEEE,1ś10.
[42]GeraldKowalski.2010. Informationretrievalarchitectureandalgorithms. Springer
Science & BusinessMedia.
[43]Daniel E Krutz, Samuel A Malachowsky, and Thomas Reichlmayr. 2014. Using a
realworldprojectinasoftwaretestingcourse.In Proceedingsofthe45thACM
technicalsymposiumonComputer science education. ACM,49ś54.
[44]MarioLinares-Vásquez,CarlosBernal-Cárdenas,KevinMoran,andDenysPoshy-
vanyk. 2017. How do developers test android applications?. In 2017 IEEE In-
ternational Conference on Software Maintenance and Evolution (ICSME). IEEE,
613ś622.
[45]Di Liu, Randolph G Bias, Matthew Lease, and Rebecca Kuipers. 2012. Crowd-
sourcingforusabilitytesting. Proceedings oftheAmericanSocietyforInformation
Scienceand Technology 49,1 (2012), 1ś10.
[46]Teng Long, Ilchul Yoon, Atif Memon, Adam Porter, and Alan Sussman. 2014.
Enablingcollaborativetestingacrosssharedsoftwarecomponents.In Proceedings
ofthe17thinternationalACMSigsoftsymposiumonComponent-basedsoftware
engineering. ACM,55ś64.
[47]Teng Long, Ilchul Yoon, Adam Porter, Atif Memon, and Alan Sussman. 2016.
CoordinatedCollaborativeTestingofSharedSoftwareComponents.In 2016IEEE
InternationalConferenceonSoftwareTesting,VerificationandValidation(ICST) .
IEEE,364ś374.
[48]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An Input
GenerationSystemforAndroidApps.In Proceedingsofthe20139thJointMeeting
onFoundationsofSoftwareEngineering (SaintPetersburg,Russia) (ESEC/FSE2013).
ACM,NewYork, NY, USA,224ś234. https://doi .org/10.1145/2491411 .2491450
[49]KeMao,MarkHarman,andYueJia.2016. Sapienz:Multi-objectiveAutomated
Testing for Android Applications. In Proceedings of the 25th International Sympo-
siumonSoftwareTestingandAnalysis (Saarbr&#252;cken,Germany) (ISSTA2016).
ACM,NewYork, NY, USA,94ś105. https://doi .org/10.1145/2931037 .2931054
[50]KeMao,MarkHarman,andYueJia.2017. CrowdIntelligenceEnhancesAuto-
mated Mobile Testing. In Proceedings of the 32Nd IEEE/ACM International Confer-
enceonAutomatedSoftwareEngineering (Urbana-Champaign,IL,USA) (ASE2017).
IEEE Press, Piscataway, NJ, USA, 16ś26. http://dl .acm.org/citation .cfm?id=
3155562.3155569
[51]CharlieMcDowell,LindaWerner,HeatherBullock,andJulianFernald.2002. The
effectsofpair-programmingonperformanceinanikrutz2014usinontroductory
programmingcourse. In ACMSIGCSE Bulletin, Vol. 34.ACM,38ś42.
[52]SergeyMechtaev,XiangGao,ShinHweiTan,andAbhikRoychoudhury.2018.
Test-EquivalenceAnalysisforAutomaticPatchGeneration. ACMTrans.Softw.
Eng. Methodol. 27, 4, Article Article 15 (Oct. 2018), 37 pages. https://doi .org/
10.1145/3241980
[53]Kevin Moran, Mario Linares-Vásquez, Carlos Bernal-Cárdenas, Christopher Ven-
dome,and DenysPoshyvanyk.2016. Automaticallydiscovering, reportingand
reproducing android application crashes. In Software Testing, Verification and
Validation(ICST),2016 IEEEInternationalConference on. IEEE,33ś44.
[54]JerzyNawrockiandAdamWojciechowski.2001. Experimentalevaluationofpair
programming. European SoftwareControland Metrics (Escom) (2001), 99ś101.
[55]Anh Tuan Nguyen, Tung Thanh Nguyen, Jafar Al-Kofahi, Hung Viet Nguyen,
andTienN.Nguyen.2011. ATopic-basedApproachforNarrowingtheSearch
1346Collaborative BugFinding forAndroid Apps ICSE ’20, May 23–29, 2020,Seoul,Republic of Korea
SpaceofBuggyFilesfromaBugReport.In Proceedingsofthe201126thIEEE/ACM
International Conference on Automated Software Engineering (ASE ’11). IEEE
Computer Society, Washington, DC, USA, 263ś272. https://doi .org/10.1109/
ASE.2011.6100062
[56]SiegfriedRasthofer,StevenArzt,StefanTriller,andMichaelPradel.2017. Making
Malory Behave Maliciously: Targeted Fuzzing of Android Execution Environ-
ments.In Proceedingsofthe39thInternationalConferenceonSoftwareEngineering
(Buenos Aires, Argentina) (ICSE’17). IEEE Press, Piscataway, NJ, USA,300ś311.
https://doi .org/10.1109/ICSE .2017.35
[57]M.Robillard,R.Walker,andT.Zimmermann.2010. RecommendationSystems
forSoftwareEngineering. IEEESoftware 27,4(July2010),80ś86. https://doi .org/
10.1109/MS.2009.161
[58]Tobias Roehm, Nigar Gurbanova, Bernd Bruegge, Christophe Joubert, and Walid
Maalej. 2013. Monitoringuserinteractions forsupporting failure reproduction.
In2013 21st International Conference on Program Comprehension (ICPC) . IEEE,
73ś82.
[59]José Miguel Rojas, Thomas D White, Benjamin S Clegg, and Gordon Fraser.
2017. Code defenders: crowdsourcing effective tests and subtle mutants with
amutationtestinggame.In Proceedingsofthe39thInternationalConferenceon
SoftwareEngineering. IEEE Press,677ś688.
[60]stan6. 2018. Student should be able to view the deadline for an assignment.
https://github .com/education/classroom/issues/1672.
[61]Chengnian Sun, David Lo, Siau-Cheng Khoo, and Jing Jiang. 2011. Towards
moreaccurateretrievalofduplicatebugreports.In Proceedingsofthe201126th
IEEE/ACM International Conference on Automated Software Engineering. IEEE
Computer Society, 253ś262.
[62]AshishSurekaandPankajJalote.2010. Detectingduplicatebugreportusingchar-
actern-gram-basedfeatures.In 2010AsiaPacificSoftwareEngineeringConference.
IEEE,366ś374.
[63]ShinHweiTan,ZhenDong,XiangGao,andAbhikRoychoudhury.2018.Repairing
CrashesinAndroidApps.In Proceedingsofthe40thInternationalConferenceon
SoftwareEngineering (ICSE2018). IEEE,187ś198.
[64]ShinHweiTan,DarkoMarinov,LinTan,andGaryTLeavens.2012. @tcomment:
Testing javadoc comments to detect comment-code inconsistencies. In 2012 IEEE
Fifth International Conference on Software Testing, Verification and Validation.
IEEE,260ś269.
[65]Shin Hwei Tan and Abhik Roychoudhury. 2015. Relifix: Automated Repair
ofSoftwareRegressions.In Proceedingsofthe37thInternationalConferenceon
SoftwareEngineering-Volume1 (Florence,Italy) (ICSE’15).IEEEPress,Piscataway,
NJ, USA,471ś482. http://dl .acm.org/citation .cfm?id=2818754 .2818813
[66]ShinHweiTan,HiroakiYoshida,MukulRPrasad,andAbhikRoychoudhury.2016.
Anti-patterns in search-based program repair. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundationsof Software Engineering.
ACM,727ś738.
[67]Jason Tsay, Laura Dabbish, and James Herbsleb. 2014. Influence of social and
technicalfactorsforevaluatingcontributioninGitHub.In Proceedingsofthe36th
international conference onSoftwareengineering. ACM,356ś366.[68]EllenMVoorheesetal .1999. TheTREC-8questionansweringtrackreport.In
Trec, Vol. 99.Citeseer, 77ś82.
[69]SongWang,DevinChollak,DanaMovshovitz-Attias,andLinTan.2016. Bugram:
bugdetectionwithn-gramlanguagemodels.In Proceedingsofthe31stIEEE/ACM
InternationalConference onAutomatedSoftwareEngineering. ACM,708ś719.
[70]XiaoyinWang,LuZhang,TaoXie,JohnAnvik,andJiasuSun.2008. Anapproach
todetectingduplicatebugreportsusingnaturallanguageandexecutioninforma-
tion.InProceedingsofthe30thinternationalconferenceonSoftwareengineering.
ACM,461ś470.
[71]Laurie Williams, Robert R Kessler, Ward Cunningham, and Ron Jeffries. 2000.
Strengthening the case for pair programming. IEEE software 17, 4 (2000), 19ś25.
[72]Tao Xie, Lu Zhang, Xusheng Xiao, Ying-Fei Xiong, and Dan Hao. 2014. Coopera-
tivesoftwaretestingandanalysis:Advancesandchallenges. JournalofComputer
Scienceand Technology 29,4 (2014), 713ś723.
[73]WeiYang,MukulRPrasad,andTaoXie.2013.Agrey-boxapproachforautomated
GUI-model generation of mobile applications. In International Conference on
Fundamental Approaches toSoftwareEngineering. Springer, 250ś265.
[74]Xinli Yang, David Lo, Xin Xia, Lingfeng Bao, and Jianling Sun. 2016. Combining
wordembeddingwithinformationretrievaltorecommendsimilarbugreports.
In2016 IEEE 27th International Symposium on Software Reliability Engineering
(ISSRE). IEEE,127ś137.
[75]Xin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to Rank Relevant
Files for Bug Reports Using Domain Knowledge. In Proceedings of the 22Nd
ACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering
(Hong Kong, China) (FSE 2014). ACM, New York, NY, USA, 689ś699. https:
//doi.org/10.1145/2635868 .2635874
[76]Alexey Zagalsky, Joseph Feliciano, Margaret-Anne Storey, Yiyun Zhao, and Weil-
iang Wang. 2015. The emergence of github as a collaborative platform for
education. In Proceedings of the 18th ACM Conference on Computer Supported
CooperativeWork & Social Computing. ACM,1906ś1917.
[77]YuZhao,TingtingYu,TingSu,YangLiu,WeiZheng,JingzhiZhang,andWilliam
G. J. Halfond. 2019. ReCDroid: Automatically Reproducing Android Application
Crashes from Bug Reports. In Proceedings of the 41st International Conference on
SoftwareEngineering (Montreal,Quebec,Canada) (ICSE’19).IEEEPress,128ś139.
https://doi .org/10.1109/ICSE .2019.00030
[78]Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where Should the Bugs Be
Fixed? - More Accurate Information Retrieval-based Bug Localization Based
onBugReports.In Proceedingsofthe34thInternationalConferenceonSoftware
Engineering (Zurich, Switzerland) (ICSE ’12). IEEE Press, Piscataway, NJ, USA,
14ś24. http://dl .acm.org/citation .cfm?id=2337223 .2337226
[79]John Zimmerman, Jodi Forlizzi, and Shelley Evenson. 2007. Research through
designasamethodforinteraction designresearchinHCI. In Proceedingsofthe
SIGCHI conference onHuman factorsincomputingsystems. ACM,493ś502.
[80]ThomasZimmermann,RahulPremraj,NicolasBettenburg,SaschaJust,Adrian
Schroter, and Cathrin Weiss. 2010. What makes a good bug report? IEEE Trans-
actions onSoftwareEngineering 36,5 (2010), 618ś643.
1347