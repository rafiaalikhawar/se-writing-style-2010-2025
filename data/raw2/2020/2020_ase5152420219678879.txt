Learning Highly Recursive Input Grammars
Neil Kulkarni*
University of California, Berkeley
neil.kulkarni@berkeley.eduCaroline Lemieux*
University of California, Berkeley
clemieux@cs.berkeley.eduKoushik Sen
University of California, Berkeley
ksen@cs.berkeley.edu
Abstract —This paper presents A RVADA , an algorithm for
learning context-free grammars from a set of positive examples
and a Boolean-valued oracle. A RVADA learns a context-free
grammar by building parse trees from the positive examples.Starting from initially ﬂat trees, A
RVADA builds structure to these
trees with a key operation: it bubbles sequences of sibling nodes
in the trees into a new node, adding a layer of indirection to thetree. Bubbling operations enable recursive generalization in thelearned grammar. We evaluate A
RVADA against GLADE and ﬁnd
it achieves on average increases of 4.98× in recall and 3.13× in F1
score, while incurring only a 1.27× slowdown and requiring only
0.87× as many calls to the oracle. A RVADA has a particularly
marked improvement over GLADE on grammars with highlyrecursive structure, like those of programming languages.
I. I NTRODUCTION
Learning a high-level language description from a set of
examples in that language is a long-studied and difﬁcult
problem. While early interest in this problem was motivatedby the desire to automatically learn human languages fromexamples, more recently the problem has been of interest inthe context of learning program input languages. Learning alanguage of program inputs has several relevant applications,including generation of randomized test inputs [1], [2], [3], aswell as providing a high-level speciﬁcation of inputs, whichcan aid both comprehension and debugging.
In this paper we focus on the problem of learning context-
free grammars (CFGs) from a set of positive examples S
and a Boolean-value oracle O. This is a similar setting as
GLADE [4]. Like GLADE, and unlike other recent relatedworks [5], [6], [7], we assume the oracle is black-box: ourtechnique can only see the Boolean return value of the oracle.We adopted the use of an oracle as we believe that in practice,an oracle—e.g. in the form of a parser—is easier to obtain thangood, information-carrying negative examples.
In this paper, we describe a novel algorithm, A
RVADA ,
for learning CFGs from example strings Sand an oracle
O. At a high-level, A RVADA attempts to create the smallest
CFG possible that accommodates all the examples. It usestwo key operations—bubbling and merging—to generalize thelanguage as much as possible, while not overgeneralizingbeyond the language accepted by O.
To create this context-free grammar, A
RVADA repeatedly
performs the bubbling and merging operations on tree repre-sentations of the input examples. This set of trees is initializedwith one “ﬂat” tree per input example, i.e. the tree with a singleroot node whose children are the characters of the input string.The bubbling operation takes sequences of sibling nodes in the
*Equal contribution.trees and adds a layer of indirection by replacing the sequencewith a new node. This new node has the bubbled sequence ofsibling nodes as children.
Then A
RVADA decides whether to accept or reject the
proposed bubble by checking whether a relabeling of the newnode enables sound generalization of the learned language.Essentially, labels of non-leaf nodes correspond to nontermi-nals in the learned grammar. Merging the labels of two distinctnodes in the trees adds new strings to the grammar’s language:the strings derivable from subtrees with the same label can beswapped. We call this the merge operation since it merges the
labels of two nodes in the tree. If a valid merge occurs, thestructure introduced by the bubble is preserved. Thus, mergesintroduce recursion when a parent node is merged with oneof its descendants. If the label of the new node added in thebubbling operation cannot merge with any existing node in thetrees, the bubble is rejected. That is, the introduced indirectionnode is removed, and the bubbled sequence of sibling nodesis restored to its original parent. These operations are repeateduntil no remaining bubbled sequence enables a valid merge.
In this paper, we formalize this algorithm in A
RVADA .
We introduce heuristics in the ordering of bubble sequencesminimize the number of bubbles A
RVADA must check be-
fore ﬁnd a successful relabeling. We implement A RVADA in
2.2k LoC in Python, and make it available as open-source.We compare A
RVADA to GLADE [4], a state-of-the-art for
grammar learning engine with blackbox oracles. We evaluateit on parsers for several grammars taken from the evaluation ofGLADE, Reinam [5], Mimid [7], as well as a few new highly-recursive grammars. On average across these benchmarks,
A
RVADA achieves 4.98× higher recall and 3.13× higher
F1 score over GLADE. A RVADA incurs on a slowdown of
1.27× over GLADE, while requiring 0.87× as many oracle
calls. We believe this slowdown is reasonable, especiallygiven the difference in implementation language—A
RVADA
is implemented in Python, while GLADE is implemented inJava. Our contributions are as follows:
•We introduce A RVADA , which learns grammars from in-
puts strings and oracle via bubble-and-merge operations.
•We distribute A RVADA ’s implementation as open source:
https://github.com/neil-kulkarni/arvada.
•We evaluate A RVADA on a variety of benchmarks against
the state-of-the-art method GLADE.
II. M OTIV ATING EXAMPLE
ARVADA takes as input a set of example strings Sand an
oracleO. The oracle returns True if its input string is valid
andFalse otherwise. A RVADA ’s goal is to learn a grammar
4562021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000482021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678879
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
Gwstart→stmt
stmt→while boolexpr do stmt
|if boolexpr then stmtelse stmt
|L=numexpr
|stmt;stmt
boolexpr →∼boolexpr |boolexpr &boolexpr
|numexpr == numexpr |false|true
numexpr →(numexpr+numexpr)|L|n
S={“while true & false do L = n”,
“L=n;L=(n+n)”}O(i)=/braceleftBigg
True ifi∈L(Gw)
False otherwise
Fig. 1: Example inputs S, and oracle Owhich returns true
if its input is in the language of the while grammar Gw.
Gwhich maximally generalizes the example strings Sin a
manner consistent with the oracle O. That is, strings i∈L(G)
in the language of the learned grammar should with high
probability be accepted by the oracle: O(i)=True.W e
formally describe maximal generalization in Section III.
Fundamentally, A RVADA learns a grammar by learning
“parse trees” for the examples in S. These parse trees are
initialized with ﬂat trees for each example in S. Then, A R-
VA DA adds structure, turning sequences of sibling nodes into
new subtrees. The particular subtrees A RVADA keeps are those
which enable generalization in the induced grammar.
From any set of trees Twe can derive an induced grammar.
In particular, each non-leaf node in a tree t∈T with label
tparent and children with labels tchild 1,tchild 2,...,t child ninduces
the ruletparent→tchild 1tchild 2···t child n. The induced grammar
ofTis then the set of induced rules for all nodes in the trees.
For example, the trees in Fig. 2 induce the grammar:
t0→while true &false doL=n
t0→L=n;L=(n+n)
and the trees under (4) in Fig. 4 induce the grammar in Fig. 5.
Because of this mapping from trees to grammars, we will
use the term “nonterminal” interchangeably with “label of anon-leaf node” when discussing relabeling trees.
A. Walkthrough
We illustrate A
RVADA on a concrete example. We take the
set of examples Sand oracle Oshown in Fig. 1. This oracle O
accepts inputs as valid only if they are in the language of the
while grammar Gw, shown at the top of the ﬁgure. A RVADA
treatsOas blackbox, that is, it has no structural knowledge
ofGw:Gwis shown only to clarify the behavior of O.
ARVADA begins by constructing na ¨ıve, ﬂat, parse trees from
the examples. These are shown in Fig. 2. Essentially, thesetrees simply go from the start nonterminal t
0to the sequence
of characters in each example s∈S. LetTdesignate the set
of trees A RVADA maintains at any point in its algorithm.
1) Bubbling: The fundamental operation A RVADA per-
forms is to bubble up a sequence of sibling nodes in the current
treesTinto a new nonterminal. To bubble a sequence s1in the
treesT, we create a new nonterminal node ts1with childrent0
while true &false do L=n
t0
L=n;L=(n+n)
Fig. 2: Initial set of parse trees Tcreated by A RVADA when
run onS,Oin Fig. 1. Each terminal chas a nonterminal
parenttcwith rule tc→c, omitted for simplicity.
Bubblet1→hile
t0
wt1
hiletrue &false do L=n
Bubblet2→(n+n) 
t0
L=n;L=t2
(n+n)
Fig. 3: Two possible bubbles applied to the trees in Fig. 2.
s1. Then we replace all occurrences of s1in eacht∈T with
ts1. Fig. 3 shows two such bubbles applied to the trees in
Fig. 2. On top, we have bubbled the sequence hile intot1;
the second tree, unchanged, is not illustrated. On the bottom,we have bubbled (n+n) intot
2; the ﬁrst tree is unchanged.
2) Merging: After bubbling a sequence s1,ARVADA either
accepts orrejects the bubble. A RVADA only accepts a bubble
if it enables valid generalization of the examples. That is, if arelabeling of the bubbled nonterminal—merging its label withthe label of another existing node—expands the language ac-cepted by the induced grammar, while maintaining the oracle-validity of the strings produced by the induced grammar.
Consider again Fig. 3. On top, we have the bubble t
1→
hile. There is no terminal or nonterminal whose label can bemerged with the label t
1and retain a valid grammar: it can’t
be merged with t0, since “hile” on its own is not accepted
byO. Nor can it be merged with the label of any individual
character: as just one example, merging with Lwould cause
theO-invalid generalization “h i l e=n;hile = (n+n)”.
On the bottom of Fig 3, we have the bubble t2→(n+n).
We can in fact merge the label t2with the label tn, the implicit
nonterminal expanding to n. Notice that if we replace nwith
the strings derivable from t2, we get examples like while true
& false do L = (n+n) andL = (n+n) ; L = ((n+n)+(n+n)),
which are all valid. Conversely, if we replace occurrencesoft
2withn, we get examples like L=n;L=n .W e
accept this bubble, which expands the language accepted by
457t0
while true &false do L=nt0
L=n;L=(n+n)
|
(1) Bubble t2→(n+n); merge (t 2,n) into t3
↓t0
while true &false do L=t3
nt0
L=t3
n;L=t3
(t3
n+t3
n)
|
(2) Bubble t4→L=t3; merge (t 4,t0) into t0
↓t0
while true &false do t0
L=t3
nt0
t0
L=t3
n; t0
L=t3
(t3
n+t3
n)
|
(3) 2-Bubble (t 5→false, t6→true); merge both into t7
↓t0
while t7
true&t7
falsedo t0
L=t3
nt0
t0
L=t3
n; t0
L=t3
(t3
n+t3
n)
|
(4) Bubble t8→t7& t7; merge (t 8,t7) into t9
↓t0
while t9
t9
true&t9
falsedo t0
L=t3
nt0
t0
L=t3
n; t0
L=t3
(t3
n+t3
n)
Fig. 4: The state of trees Tand the accepted bubbles of a full
run of A RVADA onS,Oin Fig. 1.
the induced grammar. Thus, t2andnare merged and relabeled
ast3. The trees after the relabel are shown after (1) in Fig. 4.
Note this merge has introduced recursive generalization; the
induced grammar now includes the rules:
t0→L=t3t0→L=t3;L=t3t3→(t3+t3)t3→n
In practice, A RVADA checks whether labels ta,tbcan be
merged by checking candidate strings against the oracle. If the
oracle accepts all these candidate strings, the relabeling is valid
and the labels are merged. To create these candidates, A RVADA
creates mutated trees from the trees in Twhere (1) subtrees
rooted at taare replaced subtrees rooted at tb, and (2) subtrees
rooted at tbare replaced subtrees rooted at ta. The candidate
strings are then the ones derived from these trees, i.e. theordered sequence of a tree’s leaf nodes. Section III-C describesthe conditions under which a bubble is accepted in more detail.Section III-D describes how to create these candidate strings,and the soundness issues this introduces.
3) Double bubbling: After accepting a bubble, A
RVADA
continues to try and create new bubbles. It bubbles differentt0→whilet9dot0|L=t3|t0;t0
t9→t9&t9|true|false
t3→(t3+t3)|n
Fig. 5: Grammar produced by the run of A RVADA in Fig. 4.
sequences of children in the current trees T, checking if they
are accepted, and updating Taccordingly. Fig. 4 shows a
potential run of A RVADA , with the state of the trees Tas
they are updated by bubbles and label merging.
In Fig. 4, after (1) accepting the bubble t2→(n+n),
ARVADA (2) ﬁnds and accepts the bubble t4→L=t3, whose
label can be merged with the start nonterminal t0. At this point,
ARVADA will ﬁnd no more bubbles which can be merged with
any existing nodes in T. For example, if A RVADA creates the
bubblet5→true, it will ﬁnd that the label t5cannot be
merged with the label of any existing node and reject it.
To cope with this, A RVADA also considers 2-bubbles. In
a 2-bubble, two distinct sequences of children—say, s1and
s2—in the trees are bubbled at the same time, i.e. replacing
boths1withts1→s1and some other s2withts2→s2. The
two sequences can be totally distinct, or sub/super sets, but notoverlapping: (s
1=true,s2=false) is ok, as is (s 1=true,
s2=true & false), but (s 1=rue&f,s2=e&fal) is not.
ARVADA accepts a 2-bubble only if the labels ts1andts2can
be merged with each other, not with another existing node.
Otherwise, either ts1orts2could be accepted as a 1-bubble.
4) Termination: In the run in Fig. 4, (3) A RVADA applies
and accepts the 2-bubble (s 1=true,s2=false) and merges
these sequences into t7. This 2-bubble enables one ﬁnal single
bubble to be applied and accepted: (4) t8→t7&t7can be
merged with t7. After this, no more 1-bubbles or 2-bubbles can
be accepted, so A RVADA simply outputs the grammar induced
by the ﬁnal set of trees T. Fig. 5 shows the grammar.
5) Effect of bubbling order: First, note that multiple order-
ings of bubbles can result in an equivalent grammar. For exam-ple, we could have applied ( s
1=true,s2=true&false)
in (3), then bubbled up false alone in (4). Second, while
Fig. 4 shows an ideal run, some accepted bubbles may impedefurther generalization of the grammar. For example, in theinitial ﬂat parse trees, t
1→e&false can be merged with e.
In the presence of the additional example “while n == n doskip”, this merge prevents maximal generalization.
As such, the order in which bubbles are applied and
checked has a large impact on A
RVADA ’s performance. In
Section III-B, we describe heuristics that order the bubbles forexploration based on the context and frequency of the bubbledsubsequence. These heuristics increase A
RVADA ’s probability
of successfully ﬁnding the maximal generalization of Swith
respect to O, as discussed in Section IV-B.
6) Maximality of learned grammar: The grammar in Fig. 5
is not identical to that in Fig. 1. However, it contains allthe rules in G
wdemonstrated by the examples S:t3has
taken on the role of numexpr, t9in the role of boolexpr,
andt0is effectively stmt. However, the rule boolexpr →
458numexpr == numexpr does not appear in Fig. 5. Fundamen-
tally, this is because no substring derivable from this rule exists
inS; as such, it is not part of S’s maximal generalization.
III. T ECHNIQUE
We formally describe the high-level A RVADA algorithm in
Section III-A; Sections III-B, III-C, III-D, and III-E delve intothe heuristic decisions made in A
RVADA ’s implementation.
First, we formalize our problem statement. A RVADA accepts
as input a set of example strings Sand a Boolean-valued oracle
Owhich judges the validity of the strings. A RVADA ’s goal is to
learn a context-free grammar Gwhich maximally generalizes
the set of example Sin a manner consistent withO.
Maximal generalization: LetSbe a set of input strings
andObe a Boolean-valued oracle accepting strings as in-
put. Assume each s∈Sis accepted by the oracle, i.e.,
∀s∈S:O(s)=True. Let GObe a context-free grammar
such that its language of strings L(GO)is equal to {i∈
Σ∗|O(i)=True}, the set of strings accepted by the oracle
O. SinceO(s)=True for eachs∈S, then each s∈L(GO).
We callGOas the target grammar.
Thus, for each s, there exists a derivation Dsfrom the start
symbolT0tos, i.e.Ds=T0→α1α2···α n→···→ s. This
derivation is a sequence of nonterminal expansions accordingto some rules G
O. LetRsbe the set of rules in GOused in the
derivation Ds. LetRS=∪s∈SRs, andGS
Obe the subset of
GOwhich contains only those rules r∈RS. Intuitively, GS
O
is the sub-grammar of GOwhich is exercised by the s∈S.
Finally: a grammar which maximally generalizes Sw.r.t.
Ois a grammar Gsuch that L(G)=L(GS
O), i.e. it accepts the
same language as GS
O.
A. Main Algorithm
Algorithm 1 shows the main A RVADA algorithm. It works as
follows. First, A RVADA builds na ¨ıve, ﬂat, parse trees from the
input strings (Line 1). Considering each si∈Sas a sequence
of characters si=c1
ic2i···cni
i, the tree constructed for sihas
a root node with the start symbol label t0andnichildren with
labelstc1
i,tc2i,...,tcni
i. Eachtchas a single child whose label
is the corresponding character c. Fig. 2 shows these ﬂat parse
trees for the examples strings s∈Sin Fig. 1, although the
tc→care not illustrated for simplicity.
ARVADA tries to generalize these parse trees by merging
nodes in the tree into new nonterminal labels (Line 2). To
merge two nodes ta,tbin a tree, we replace all occurrences
of the labels ta,tbwith a new label tc. This creates new trees
T/prime; the merge is valid if the language of the induced grammar
ofT/primeonly includes strings accepted by the oracle O.
In practice, we check if a merge of ta,tbis valid by
checking whether tacan replace tbin the example strings, and
vice-versa. The strings derivable from an arbitrary nonterminalNinTare the concatenated leaves of the subtree rooted at
N. We check whether t
areplacestbby checking whether the
strings produced by replacing strings derivable from taby
strings derivable from tb, are accepted by the oracle. That is,
we take the strings derivable from the trees T, with holes inAlgorithm 1 ARVADA ’s high-level algorithm
Input: a set of examples S, an language oracle O.
Output: a grammar Gﬁtting the language.
1:bestTrees ←NAIVE PARSE TREES (S)
2:bestTrees ←MERGE ALLVALID (bestTrees, O)
3:updated←True
4:while updated do
5: updated←False
6: allBubbles ←GETBUBBLES (bestTrees)
7: for bubble inallBubbles do
8: bbldTrees ←APPLY (bestTrees, bubble)
9: accepted, mergedTs ←CHECK BUBBLE(bbldTrees, O)
10: ifaccepted then
11: bestTrees ←mergedTs
12: updated←True
13: break
14:G← INDUCED GRAMMAR (bestTrees)
15:returnG
place of strings derived from tb. Then we ﬁll the holes with
strings derivable by ta. If all the strings are accepted by O,
ARVADA judges the replacement as valid. Section III-D details
this check and its soundness.
Now the main A RVADA loop starts. From the current S-
derived trees T,A RVADA gets all potential “bubbles” for
the trees (Algorithm 1, Line 6). For each tree t∈T ,
GETBUBBLES collects all proper contiguous subsequences of
children in t. That is, if the tree contains a node tiwith
children C=c1,c2,...,c n, the potential bubbles include
all subsequences of Cof length greater than one and less
thann.G ETBUBBLES returns all these subsequences as 1-
bubbles, and all non-conﬂicting pairs of these subsequencesas 2-bubbles. Two subsequences are non-conﬂicting if theydo not strictly overlap: they can be disjoint or one can be a
proper subsequence of the other. So ((c
1,c2,c3),(c2,c3,c4))
conﬂict, but ((c1,c2,c3),(c2,c3))and((c1,c2,c3),(c4,c5))do
not. The order in which A RVADA explores these bubbles is im-
portant for efﬁciency; we discuss this further in Section III-B.
Then, for each potential bubble, A RVADA tries applying it
to the existing set of trees T. Suppose we have a 1-bubble
consisting of the subsequence ci,ci+1,...,c j. To apply this
bubble, we replace any sequence of siblings tci,tci+1,...,t cj
with labels ci,ci+1,...,c jin the tree with a new subtree
tnew→tci,tci+1,...,t cj. Fig. 3 shows two such bubblings:
hile is bubbled into the nonterminal t1at the top, and (n+n)
is bubbled to t2on the bottom. If the bubbled nodes have
structure under them, that structure is maintained: e.g., thebubbling of t
7&t7intot9at (4) in Fig. 4. For a 2-bubble, the
same process is repeated for the two subsequences involved.
After applying the bubble, A RVADA checks whether it
should be accepted (Line 9). Section III-C formalizes C HECK -
BUBBLE , but essentially, C HECK BUBBLE accepts a bubble
if the new nonterminals introduced in its application can bevalidly merged with some other nonterminal node in the tree.
459t0
whilen==n do t0
skip
Tree 1t0
while t1
truedo t0
skip
Tree 2t0
if t1
falsethen t0
skipelse t0
skip
Tree 3
Fig. 6: Partial parse tree Tduring run of A RVADA on while, with guide examples “while n==n do skip”, “if false then
skip else skip” and “while true do skip”. A RVADA has applied the 1-bubble “skip”, which merged with t0, and the
2-bubble (“false”, “true”). The 4-contexts for “n= =n ”’ are highlighted inyellow, and for t1arehighlighted ingreen.
If the new bubbled nonterminal allows a valid merge with
some other nonterminal, C HECK BUBBLE returnsTrue as well
as the trees with the merge applied (Line 9). We update the
best trees Tto reﬂect the successful merge (Line 11), and
GETBUBBLES is called again on the new T. If the bubble
is not accepted, A RVADA continues to check the next bubble
returned by G ETBUBBLES (Line 7).
The algorithm terminates when none of the bubbles are
accepted, i.e. when the trees Tcannot be further generalized,
and returns the grammar Ginduced by the trees T(Line 15).
We can guarantee the following about A RVADA as long as
merges are sound, once we consider the notion of partially
merging two nonterminals, discussed in Section III-C2.
EXISTENCE THEOREM :There exists a sequence of k-
bubbles, that, when considered by A RVADA in order, enable
ARVADA to return a grammar Gs.t.L(G)=L(GO), so long
as the input examples Sare exercise all rules of G.
Proof Outline: The optimal bubble order always chooses
the right-hand-side of some N→α1···α ninGas the
sequence to bubble, either as 1-bubble if there exists an ex-pansion for Nin the trees already, or as a 2-bubble otherwise.
Our technical report gives a formal treatment of this and the
Generalization Theorem, which shows that k-bubbles mono-
tonically increase the language of the learned grammar [8].
B. Ordering Bubbles for Exploration
As described in paragraph 5) of Section II and alluded to
above, the order of bubbles impacts the eventual grammar
returned by A
RVADA . Unfortunately, the number of orderings
of bubbles is exponential. To have an efﬁcient algorithm inpractice, we must make sure the algorithm ﬁnds the correctorder of bubbles early in its exploration of bubble orders. Assuch, G
ETBUBBLES returns bubbles in an order more likely
to enable sound generalization of the grammar being learned.
As described in the prior section, bubble sequences consist
of proper contiguous subsequences of children in the currenttreesT. We increase the maximum length of subsequences
considered once all bubbles of shorter length do not enable anyvalid merges. These subsequences (and their pairs) form thebase of 1-bubbles (and 2-bubbles) returned by G
ETBUBBLES .
Recall that a bubble should be accepted if the bubbled
nonterminal(s) can be merged with an existing nontermi-nal (or each other). Thus, G
ETBUBBLES should ﬁrst return
those bubbles that are likely to be mergeable. We leveragethe following observation to return bubbles likely to enablemerges. Expansions of a given nonterminal often occur ina similar context. The k-context of a sequence of sibling
terminals/nonterminals sin a tree is the tuple of ksiblings
to the left of sandksiblings to right of s.
Fig. 6 shows an example of a run of A
RVADA
on the while language, after the application of the1-bubble “skip” and the 2-bubble (“false”, “true”).The set of 4-contexts for the sequence “n
==n”i s
{((i,l,e,),(,d,o,))}. Similarly, “t 1”’s 4-contexts are
{((S,i,f,),(,t,h,e)),((i,l,e,),(,d,o,))};“S”i s
a dummy element indicating the start of the examplestring. Note that “n==n” and “t
1” share the 4-context
{((i,l,e,),(,d,o,))}
With this in mind, G ETBUBBLES orders the bubbles in
terms of their context similarity. Given two contexts c0=
(l0,r0)andc1=(l1,r1), where li=(lk
i,lk−1
i,...,l0
i)
andri=(r0
i,...,rk−1
i,rk
i),w eh a v econtextSim( c0,c1)=
kTupleSim( l0,l1)+kTupleSim( r0,r1), where
kTupleSim( t0,t1)=/braceleftBigg
1
2ift0=t1/summationtextk
i=01=(ti
0,ti1)
2i+2
where 1=is the indicator function, returning 1 if its arguments
are equal and 0 otherwise. This similarity function gives most
weight to the context elements closest to the bubble.
With this in mind, we deﬁne set context similarity as the
maximum similarity of two contexts within the set:
setContextSim( C0,C1)= m a x
c0∈C0,c1∈C1contextSim( c0,c1).
In our running example, the context similarity is 1 because
n==n’s 4-context set is a subset of t1’s 4-context set.
To form bubbles, G ETBUBBLES ﬁrst traverses all the trees
Tcurrently maintained by A RVADA . It considers each proper
contiguous subsequence of siblings in the trees. For eachsubsequence s, it collects the k-contexts for s, as well as
the occurrence count of the subsequence occ(s).I nF i g .6 ,
occ(while)=2 ,occ(t
1)=2 and occ(n==n)=1 . In our
implementation we take k=4.
ARVADA then creates a 2-bubble for each pair of sequences
(s1,s2)where both |s1|>1and|s2|>1. The similarity score
of this 2-bubble is setContextSim( contexts(s1),contexts(s2))
and its frequency score is the average frequency of the two
sequences in the bubbleocc(s 1)+occ(s 2)
2. Additionally, for each
sequence s0with|s0|>1,A RVADA creates a 1-bubble (s0).
LetS1be the set of length-one subsequences. The simi-
larity score of (s0)ismax s1∈S1setContextSim( contexts(s0),
contexts(s1))and its frequency score is occ(s0).
460t0
if t1
n== nthe n t0
L=nelse t0
L=n
t0→if t1the tnt0else
t1→ tn==tn
t0→L=tn
tn→n(1)
t0→if t1the tn1t0else
t1→ tn2==tn3
t0→L=tn4
tn1→n tn2→n tn3→n tn4→n(2)
Fig. 7: Example tree and rules in its induced grammar which
havetnin their expansion (1), and the same grammar with tn
split at different positions. For simplicity, nonterminals of theformt
c→c—other than tnin (1)—are collapsed to c.
Finally, G ETBUBBLES takes the top-n bubbles as sorted
primarily by similarity, and secondarily by frequency. Intu-itively, high-frequency sequences may correspond to tokensin the oracle’s language. The order of bubbles is shufﬂed toprevent all runs of A
RVADA from getting struck in the same
manner. We ﬁnd n= 100 to be effective in practice.
C. Accepting Bubbles
The second key component of A RVADA is deciding whether
a given bubble should be accepted: this section formalizeshow C
HECK BUBBLE works. At the core of C HECK BUBBLE
is the concept of whether two labels ta,tbcan be merged. We
say thattaandtbcan be merged, i.e. M ERGES(ta,tb),i fa n d
only if R EPLACES (ta,tb)—that is, all occurrences of tbcan
be replaced by tain the grammar—and R EPLACES (tb,ta).W e
formalize how R EPLACES is checked in the next section.
1) 2-Bubbles: ARVADA accepts a 2-bubble (s1,s2)with
labelsts1,ts2only if M ERGES(ts1,ts2). Intuitively, this is
because both bubbles should be kept only if they togetherexpand the grammar. For example, suppose we apply the 2-bubble (“n
==n”, “lse”) to the trees in Fig. 6, resulting in
nonterminals tn==n→n==nandtlse→lse. While tn==n
can merge with t1,tlsedoes not contribute to this merging.
So, (“n==n”) should be accepted only as a 1-bubble.
2) 1-Bubbles: Recall that A RVADA scores 1-bubbles highly
if they are likely to merge with an existing nonterminal. LetNTs(T)be the nonterminal labels present in the current set of
treesT. Given a 1-bubble (s
1)with label ts1, we go through
eachti∈NTs(T)and check whether M ERGES(ti,ts1).
If M ERGES(ti,ts1)is true for some ti∈NTs(T), then
CHECK BUBBLE accepts the bubble (s1).
However, if ts1cannot merge with any ti∈NTs(T),
ARVADA also looks for partial merges. Partial merging works
as follows. Let CNTs(T)be the character nonterminal labelspresent in the current set of trees T.Acharacter nonterminal
is a nonterminal whose expansions only of a single terminalelement, e.g., t
n→nort1→1|2|3.
For each tc∈CNTs(T), the partial merging algorithm
identiﬁes all the different occurrences of tcin the right-hand-
side of expansions in T’s induced grammar. For instance, in
the grammar fragment (1) of Fig. 7, we see the nonterminalt
n, corresponding to “n”, occurs 4 distinct times in right-
hand-sides of expansions. The partial merging algorithm thenmodiﬁes the grammar so that the i
thoccurrence of tcis
replaced with a fresh nonterminal tci. Eachtciexpands to
the same bodies as tc; i.e.tci→c. This replacement process
is illustrated in the grammar fragment (2) of Fig. 7: thefour occurrences of t
nhave been replaced with tn1,tn2,tn3,
andtn4. Finally, we get to the merging inpartial merging:
for each tci, the algorithm checks if M ERGES(tci,ts1).I f
MERGES(tci,ts1)for anytci,A RVADA accepts the bubble
(s1), andts1is merged with all such tci. Thetcjwhich cannot
be merged with ts1are restored to the original nonterminal tc.
The term partial merge refers to the fact that we have
effectively merged ts1with some of the occurrences of tcin
rule expansions. This step is useful when A RVADA ’s initial
trees—which map each character to a single nonterminal—use the same nonterminal for characters that are conceptuallyseparate. For instance, consider the 1-bubble ((n+n)), withlabelt
(n+n) . Given the tree in Fig. 7, M ERGES(tn,t(n+n))
fails because “(n+n)” cannot replace the “n”i n“ then”. In
fact,t(n+n) cannot merge with any ti∈NTs(T)initially. But
the partial merge process splits tnintotn1,tn2,tn3,tn4, and
ARVADA ﬁnds that t(n+n) in fact merges with tn2,tn3and
tn4. So, it is merged with those nonterminals and accepted.
Note: though we consider only partial merges on character
nonterminals for efﬁciency reasons, the concept of partialmerging can be applied to any pair of nonterminals.
In summary, a 1-bubble (s
1)with label ts1is accepted if
either: (1) for some ti∈NTs(T),M ERGES(ti,ts1), or (2) for
sometc∈CNTs(T),ts1can be partially merged with tc.
D. Sampling Strings for Replacement Checks
The ﬁnal important element affecting the performance of
ARVADA is how exactly we determine whether the merge of
two nonterminals labels is valid. Recall that M ERGES(ta,tb)
if and only if R EPLACES (ta,tb)and R EPLACES (tb,ta).
We implement R EPLACES (treplacer,treplacee)as follows. From
the current parse trees, we derive the replacee strings: the
strings derivable from the parse trees in trees, but with holes
instead of the strings derived from treplacee . Then, we derive
a set of replacer strings: the strings derivable from treplacer
in the trees. Finally, we create the set of candidate strings
by replacing the holes in the replacee strings with the replacerstrings. If Orejects any candidate string, the merge is rejected,
and R
EPLACES returns false.
Fig. 8 shows how replacer and replacee strings are computed
in the call to R EPLACES (t0,t4), i.e. whether t0can replace t4.
Replacee strings for a node in the parse tree are computed by
461t0
t4
t4
4t4
4tp
+t4
4t0
tl
(t0
3tr
)
Replacee strings for t4:
•+•,•+4,44+• ,4•+4,•4+4,4•+•,•4+•,••+•,••+4
Replacer strings from t0:
44+4,(3),3,/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
level-0 derivable44+44,4+44,4+4,((3)),(44+4)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
level-1 derivable
Fig. 8: Two partial parse trees and examples of replacee and
replacer strings. The symbol •designates holes which will be
replaced by (level-n derivable) replacer strings.
taking the product of replacee strings for all its children; thenonterminal being replaced becomes a hole.
Level-0 replacer strings for t
iare just the strings that
directly derivable from tiin the tree; in Fig. 8, the level-
0 derivable strings of t0are44+4,(3),3, and the level-0
derivable strings of t4are44,4. Then, the set of level-n
derivable strings for a node is the set derived from taking theproduct of all level-(n −1) derivable strings for each child of
a node. The level-1 replacer strings for t
0are shown in Fig. 8.
When R EPLACES is run in the full M ERGE ALLVALID call
or while evaluating a 1-bubble, we use only level-0 replacerstrings. However, we found that level-1 replacer strings greatlyincreased soundness at a low runtime cost for 2-bubbles.Intuitively this is because nonterminals from new bubblestend to have less structure underneath them than existingnonterminals in the trees. So it is faster to compute level-1replacer strings for these new bubble-induced nonterminals.
Note that the both the number of replacee strings and
of level-n derivable replacer strings grows exponentially. So,instead of taking the entire set of strings derivable in thismanner, if there are more than pof them, we uniformly sample
pof them. In our implementation we use p=5 0 , to make the
number of parse calls reasonable in terms of runtime.
Unfortunately, this process allows unsound merges, where
all candidate strings are accepted by the oracle, but the mergeadds oracle-invalid inputs to the language of the learned gram-mar. First, because only pcandidates are sampled. Second,
because the replacee strings are effectively “level 0”, and thus,not reﬂective of the current induced grammar from the trees.Third, because a candidate string is produced by replacing allits holes with a single replacer string, rather than ﬁlling holeswith different replacer strings. Taking p→∞ ,n→∞ for
the level-n replacer strings, and ﬁlling different holes with
different replacer strings would ensure sound merges.
E. Pre-tokenization
Since A
RVADA considers 2-bubbles, it is effectively n4in
the total length of examples n. So, to improve performance as
ngets large and reduce the likelihood of creating “breaking”
bubbles, in our implementation we use a simple heuristic
to pre-tokenize the values at leaves, rather than consideringeach character as a leaf. We group together sequences ofcontiguous characters of the same class (lower-case, upper-case, whitespace, digits) into leaf tokens. Punctuation andnon-ASCII characters are still treated as individual characters.We then run the A
RVADA as described previously. To ensure
generalization, we add a last stage which tries to expand thesetokens into the entire character class: e.g. if t
1→abc|cde,w e
check whether t1can be replaced by any sequence of lower-
case letters, letters, or alphanumeric characters. We constructthe replacee strings as described above, and sample 10 stringsfrom the expanded character classes as replacer strings.
IV . E
V ALUATION
We seek to answer the following research questions:
RQ1. Do A RVADA ’s mined grammars generalize better (have
higher recall) than state-of-the-art?
RQ2. Do A RVADA ’s mined grammars produce more valid
inputs (have higher precision) than state-of-the-art?
RQ3. How does the nondeterminism in A RVADA cause its
behavior to vary across different invocations?
RQ4. How does A RVADA ’s performance compare to that of
deep-learning approaches?
RQ5. What are A RVADA ’s major performance bottlenecks?
RQ6. What do A RVADA ’s mined grammars look like?
A. Benchmarks
We evaluate A RVADA against state-of-the-art blackbox
grammar inference tool GLADE [4] on 11 benchmarks.
The ﬁrst 8 benchmarks consist of an ANTLR4 [10] parser
for the ground-truth grammar as oracle and a randomly gener-ated set of training examples S.Sis sampled to cover all of the
rules in the ground-truth grammar, while keeping the length ofeach example s∈Ssmall. The test set is randomly sampled
from the ground-truth grammar. Essentially, this ensures thatthe maximal generalization of Scovers the entire test set.
Other than turtle andwhile, these benchmarks come from
prior work [4], [5], [7]:
•arith: operations between integers, can be parenthesized
•fol: a representation of ﬁrst order logic, including quali-
ﬁers, functions, and predicates
•json: JSON with objects, lists, strings with alpha-numericcharacters, Booleans, null, integers, and ﬂoats
•lisp: generic s-expression language with “.” cons’ing
•mathexpr: binary operations and a set of function callson integers, ﬂoats, constants, and variables
•turtle: LOGO-like DSL for Python’s turtle
•while: simple while language as shown in Fig. 1
•xml: supporting arbitrary attributes, text, and a few labels
The next 3 benchmarks use as oracle a runnable program,
and use a random input generator to create Sand the test set.
Sconsists of the ﬁrst 25 oracle-valid inputs generated by the
generator, and the test set of the next 1000 oracle-valid inputsgenerated. In this case, there is no guarantee that the maximalgeneralization of Scovers the test set.
•curl: the oracle is the curl[11] url parser. We use thegrammar in RFC 1738 [12] to generate Sand test set.
462(a) Recall. Higher is better. (b) Precision. Higher is better. (c) F1 Score. Higher is better.
Fig. 9: Recall, precision, and F1 score for each of the 10 runs of A RVADA (plotted with •) and GLADE (plotted with $).
TABLE I: Summary of results for A RVADA and GLADE. “R” is recall, “P” is precision.
Results for A RVADA are listed as the means over 10 runs with ±the standard deviation.
Bolded results are 2× better.
ARVADA GLADE
Bench. Recall Precision F1 Score Time(s) # Queries R P F1 Time(s) # Queries
arith 1.00±0.00 1.00 ±0.00 1.00±0.00 3 ±0 828 ±37 0.07 1.00 0.13 12 2.3K
fol 0.87±0.25 1.00 ±0.01 0.91±0.18 372 ±36 33K± 3.7K 0.06 1.00 0.11 107 20K
json 1.00±0.00 0.95 ±0.08 0.97 ±0.05 76 ±11 16K ±1K 0.42 0.98 0.59 61 11K
lisp 0.52±0.33 0.90 ±0.17 0.57 ±0.21 16 ±4 3.6K ±603 0.23 1.00 0.38 20 3.8K
math. 0.84±0.12 0.97 ±0.02 0.89±0.08 65 ±6 11K ±1.1K 0.18 0.99 0.31 103 19K
turtle 1.00±0.00 1.00 ±0.00 1.00±0.00 84 ±8 10K ±1.1K 0.21 1.00 0.34 75 14K
while 0.70±0.21 1.00 ±0.00 0.81±0.14 54 ±5 13K ±1.5K 0.01 1.00 0.02 50 9.1K
xml 0.96±0.11 0.98 ±0.07 0.96±0.08 205 ±34 14K ±2.4K 0.26 1.00 0.42 81 15K
curl 0.92 ±0.02 0.55 ±0.14 0.68 ±0.11 111 ±12 25K ±3.1K 0.80 0.76 0.78 112 30K
tinyc 0.92±0.04 0.73 ±0.13 0.81±0.08 6.4K ±1.2K 112K ±32K 0.17 0.60 0.26 917 252K
nodejs 0.30 ±0.21 0.42 ±0.13 0.29 ±0.16 46K ±22K 142K ±90K 0.26 0.50 0.34 38K 113KTABLE II: Results for CLGen’s
core LSTM [9]. “Model Time” isthe logged model training time.
CLGen LSTM
Bench. Time(s) Model Time(s) Precision
arith 172 9 0.002
fol 177 12 0.460
json 178 11 0.625lisp 173 9 0.367mathexpr 176 12 0.393turtle 176 10 0.367while 167 9 0.012xml 171 12 0.228
curl 176 12 0.434
tinyc 189 21 0.062nodejs 176 18 0.111
•tinyc: the oracle is the parser for tinyc [13], a compiler
for a subset of C. We use the same golden grammar asin Mimid [7] to generate Sand the test set.
•nodejs: the oracle is an invocation of nodejs --check,
which just checks syntax [14]. To generate Sand the test
set, we use Zest’s [15] javascript generator.
The average length of training examples in the set Sis
below 20 for all benchmarks except tinyc (77) and nodejs
(58). We adjust the maximum bubble length hyperparameter(ref. Section III-B) accordingly: the default is to range from3t o1 0 ,b u to ntinyc andnodejs we range from 6 to 20.
B. Accuracy Evaluation
First, we evaluate the accuracy of A
RVADA and GLADE’s
mined grammars with respect to the ground-truth grammar Weran both A
RVADA and GLADE with the same oracle example
strings. Three key metrics are relevant here:
Recall: the proportion of inputs from the held-out test set—
generated by sampling the golden grammar/generator—thatare accepted by the mined grammar. We use a test set sizeof 1000 for all benchmarks.
Precision: the proportion of inputs sampled from the mined
grammar that are accepted by the golden grammar/oracle. Wesample 1000 inputs from the mined grammar to evaluate this.
F1 Score: the harmonic mean of precision and recall. It is
trivial to achieve high recall but low precision (mined grammarcaptures any string) or low recall but high precision (minedgrammar captures only the string in S); F1 measures the
tradeoff between the two.Results. As A
RVADA is nondeterministic in the order of
bubbles explored, we ran it 10 times per benchmark. AsGLADE is deterministic, we ran it only once per benchmark.
Table I shows the overall averaged results, Fig 9 the indi-
vidual runs. We see from the table that on average, A
RVADA
achieves higher recall than GLADE on all benchmarks, and it
achieves higher F1 score on all but 2 benchmarks. A RVADA
achieves over 2× higher recall on 9 benchmarks, and over 2×
higher F1 score on 7 benchmarks.
Even for those benchmarks where A RVADA does not have
a higher F1 score on average, Fig. 9c shows that A RVADA
outperforms GLADE on some runs. For nodejs, on 5 runs,
ARVADA achieves a higher F1 score, ranging from 0.37 to
0.55. For curl, on 2 runs A RVADA achieves F1 scores greater
than or equal to GLADE’s: 0.78 and 0.86. It makes sensethat GLADE performs well for curl: the url language is
regular, and the ﬁrst phase of GLADE’s algorithm works bybuilding up a regular expressions. Nonetheless Fig. 9a showsthat A
RVADA achieves consistently higher recall on curl.
Overall, on average across all runs and benchmarks, A R-
VA DA achieves 4.98× higher recall than GLADE, while
maintaining 0.96× its precision. So, on our benchmarks, the
answer to RQ1 is in the afﬁrmative, while the answer to RQ2is not. Given that A
RVADA still achieves a 3.13× higher F1
score on average, and that higher generalization (in the form
of recall) is much more useful if the mined grammar is usedfor fuzzing, we ﬁnd this to be a very positive result.
However, we see from the standard deviations in Table I that
A
RVADA ’s performance varies widely on some benchmarks,
463notablefol,lisp,while, and fol. Fig. 9, which shows
the raw data, conﬁrms this. In Fig. 9a, we see that the
performance on the lisp benchmark is quite bimodal. All of
the mined grammars with recall around 0.25 fail to learn tocons parenthesized s-expressions. This may be because theminimal example set did not actually have an example of thisnesting. On nodejs, the two runs with recall less than 0.1
ﬁnd barely any recursive structures, suggesting that on largerexample sets, A
RVADA may get lost in bubble order. Overall,
the answer to RQ3 is that A RVADA ’s nondeterministic bubble
ordering can have very large impacts on the results. We discusspossible mitigations in Section V.
C. Comparison to Deep Learning Approaches
Recently there has been interest in using machine learning
to learn input structures. For instance, Learn&Fuzz trains a
seq-2-seq model to model the structure of PDF objects [16];it uses information about the start and end of pdf objects aswell as the importance of different characters in its samplingstrategy. DeepSmith [17] trains an LSTM to model OpenCLkernels for compiler fuzzing, adding additional tokenizationand pre-processing stages to CLGen [9].
A natural question is how A
RVADA compares to these gen-
erative models. We trained the LSTM model from CLGen [9],the generative model behind DeepSmith, on our benchmarks.We removed all the OpenCL-speciﬁc preprocessing stagesfrom the pipeline. We used the parameters given as examplein the CLGen repo, creating a 2-layer LSTM with hiddendimension 128, trained for 32 epochs. We used \n!!\n as
an EOF separator. Each sample consisted of 100 characters,split into different inputs where the EOF separator appeared.
Table II shows the runtime of the model on each benchmark,
as well as the precision achieved on the ﬁrst 1000 samplestaken from the model. Generally, we see that the precisionis much lower than that of GLADE or A
RVADA .O narith,
the model over-trains on the EOF separator, adding \nand
!throughout samples. Since the model is generative—it can
generate samples but not provide a judgement of samplevalidity—, we cannot measure Recall as in Table I. However,qualitative analysis of the samples suggests there is not muchlearned recursive generalization. For json, 602 of the 625
valid samples are a single string (e.g., "F"); the other 21 valid
samples are numbers, false,o r[].F o rnodejs, of the 111
valid samples, 26 are empty, 24 are a single identiﬁer (e.g.a
0), 18 are a parenthesized integer or identiﬁer (e.g,. (242)),
and 17 are a single-identiﬁer throw, e.g. throw (a 0).
These results are not entirely unexpected, because the
LSTM underlying CLGen is learning solely from the input
examples. Both A RVADA and GLADE extensively leverage the
oracle, effectively creating new input examples from whichto learn. This explains why the runtimes look so differentbetween Tables I and II. We see in Table II that the totaltime to setup and train the model is around 3 minutes forall benchmarks, and the core training time is around 10-20seconds. We see the model training time is slightly higher fortinyc andnodejs, which had longer input examples.arithfoljson lispmath turtle whilexml curltinycnodejs0%20%40%60%80%Percent of RuntimeOracle Calls
Bubble Ordering
String Sampling
Fig. 10: Average percent of runtime spent in different compo-nents of A
RVADA . Error bars show std. deviation.
Overall, we expect these deep-learning approaches to be
more well-suited to a case where an oracle is not available,but large amounts of sample inputs are. These models may alsobe more reliant input-format speciﬁc pre-processing steps, likethose used on OpenCL kernels in CLGen and DeepSmith.
D. Performance Analysis
The next question is about A
RVADA ’s performance. Table I
shows the average A RVADA runtime and number of queries
performed for each benchmark, and the same statistics for
GLADE. On 7 of 11 benchmarks, A RVADA is on average
slower than GLADE; overall across benchmarks, this amountsto an average 1.27× slowdown. This is quite respectable, since
A
RVADA has a natural runtime disadvantage due to being
implemented in Python rather than Java. For the three bench-marks on which A
RVADA is over2×slower than GLADE,
it has huge increases in F1 score: 0.11→0.91forfol,
0.42→0.96forxml, and 0.26→0.81fortinyc.
The story for oracle queries performed is inversed; A RVADA
requires more oracle queries on average on only 4 benchmarks.For all of these except nodejs,A
RVADA also had much
higher F1 scores. However, nodejs is a benchmark with high
variance. On the run with highest F1 score (0.55, higher thanGLADE’s 0.34), A
RVADA takes 86,051 s to run and makes
270k oracle calls. On the fastest run, where A RVADA only
gets F1 score 0.14, A RVADA takes 17,775 s and makes 41k
oracle calls. That is, the higher performance cost correlateswith the slower runs on this benchmark: 5 of the 6 slowerruns also have higher F1 scores.
Overall across all benchmarks, A
RVADA performs only
0.87× as many oracle queries as GLADE . This is encour-
aging as it gives more room for performance optimizations.
Fig. 10 breaks down the average percent of runtime spent
in A RVADA ’s 3 most costly components: calling the oracle;
creating, scoring, and ordering bubbles; and sampling stringfor replacement checks. The error bars show standard de-viation; note the aforementioned high variance for nodejs
appears here too. On the minutes-long benchmarks on which
A
RVADA is at least 10 seconds slower than GLADE, >20%
of the runtime is spent in sampling strings for replacement.The current implementation of this re-traverses the trees T
after each bubble to create these examples.
464while→ stmt while|skip|L=
stmt→while bool and-space do|while;
|if bool and-space then whileelse
bool→false|˜bool|true|num== num
and-space →|and-space &bool and
num→L|n|(num+num)
Fig. 11: A RVADA -minedwhile grammar with 100% recall.
Nonterminals renamed for readability.
json→ str|dict}|false|true|[]| pos-int
|ﬂoat-start DIGITS|ﬂoat-start pos-int |int
|{}|[json list-end |null|NA T
str→ str-start’’
dict→ dict-lst str :json dict-lst → dict,|{
pos-int→ NA T int→-pos-int|NA T
ﬂoat-start → int.|pos-int.
list-end→,json list-end |]
str-start→‘‘chars|‘‘pos-int|str-start pos-int
chars→ chars chars |pos-int chars |ALNUMS
DIGITS :[0-9]+ NA T :0|[1-9][0-9]* ALNUMS :[a-Z0-9]+
Fig. 12: A RVADA -minedjson grammar with maximum F1
Score. Nonterminals renamed for readability. DIGITS ,NA T,
and ALNUMS are tokens expanded after the Sec. III-E pass.
On the particularly slow benchmarks, tinyc andnodejs,
ARVADA spends a long time ordering bubbles. This makes
sense because of the larger example length of the benchmarks.
It is nonetheless encouraging to see this room for improve-ment, as G
ETBUBBLES re-scores the full set of bubbles each
time a bubble is accepted. It should be possible to bring downruntime by only scoring the bubbles that are modiﬁed by theapplication of the just-accepted bubble. On nodejs,A
RVADA
also spends a long time in oracle queries, because the time foreach query is much longer (300 ms vs. 3ms for tinyc).
Overall, A
RVADA has runtime and number of oracle queries
comparable with GLADE, while achieving much higher recalland F1 score. As for RQ3, when the length of the examples inSis small, oracle calls dominate runtime. As example length
grows, the ordering and scoring of bubbles—particularly com-puting context similarity—starts to dominate runtime.
E. Qualitative Analysis of Mined Grammars
The statistics discussed in the prior section show that
A
RVADA ’s mined grammars can closely match the ground-
truth grammars in terms of inputs generated and accepted.
For RQ5, we consider their human-readable complexity.
Mined grammar readability varies across benchmarks. For
instance, on the 3 runs where A RVADA achieves 100% recall
forwhile, the mined grammars look similar to GwFig. 1:
Fig. 11 shows the grammar mined in one of these runs,randomly selected from the three. Fig. 12 shows the grammarwith maximum F1 score mined by A
RVADA onjson; it splitssome expansions at unusual places (e.g. the use of ﬂoat-start )
but is readable after some examination.
Fortinyc, the mined grammars are somewhat over-
bubbled: on average they have 56 nonterminals, and 217 rulesof average length 1.8. On nodejs, the grammars have on
average 40 nonterminals and 276 rules of average length 3.6.Because GLADE’s grammars are not meant to be human-readable, they are signiﬁcantly larger: 3505 nonterminals with4417 rules of average length 1.3 for tinyc; and 2060 nonter-
minals with 3939 rules of average length 1.2 for nodejs.
V. D
ISCUSSION AND THREATS TO VALIDITY
Our implementation of A RVADA relies on some heuristic
elements, which we developed while examining some smallerbenchmarks (i.e. arith,while) on a particular set of example
strings. To prevent overﬁtting on these benchmarks, for eval-uation, we used a freshly-generated set of example strings.
The deﬁnition of maximal generalization assumes that the
language accepted by the oracle is context-free. Thus, wehave no formal guarantees on how the algorithm will react tocontext-sensitive input languages. While our results comparedto GLADE are promising, there is no guarantee they willgeneralize to all benchmarks.
The fact that A
RVADA ’smaximum results consistently beat
state-of-the-art (Fig. 9) suggests a few directions for im-provement. If runtime is not a constraint, A
RVADA can be
parallelized as-is. To choose the winner, ﬁrst measure precisionwith respect to the oracle. Then, evaluate the grammars oninputs sampled from the other mined grammars, and choosethe one which captures the most of those samples. A less-wasteful way to parallelize would be to conduct some sortof beam search, perhaps using the just-described comparativegeneralization metric, or to backtrack bad bubbles.
There remains much room to optimize the order in which
bubbles are explored, and pre-tokenization of inputs. We chosetwo natural metrics for ordering (context similarity and fre-quency), but have not exhaustively examined how to combinethem. From the difference in performance between the largerbenchmarks tinyc (which had simple regex structure) and
nodejs (regexes in the training set are more complex), it
appears that A
RVADA could beneﬁt from running at a higher
token level. Developing better heuristics for tokenization, orpairing A
RVADA with a more complex regex learning algo-
rithm than that described in Section III-E may yield beneﬁts.
VI. R ELATED WORK
Automatically synthesizing context-free grammars from
examples is a long-studied problem in computer science;Lee [18], and Stevenson and Cordy [19] give a survey ofsome techniques. Gold’s theorem [20] states that grammarscannot be learned efﬁciently from a set of positive examplesalone. Angluin and Kharitonov [21] show that pure black-boxapproaches face scalability issues on arbitrary CFGs. But, real-world grammars may not be so adversarial. Our heuristics usestatistical information to heavily prune the search space.
465The core idea in Solomonoff’s [22] algorithm is to, for
each example, ﬁnd substrings of the example that can be
deleted. If a substring can be deleted, Solomonoff proposes toadd a recursive repetition rule for the substring. Rather thantrying to generalize each example string individually, A
RVADA
considers all example strings together when producing candi-date strings. Unlike Arvada, Knobe and Knobe [23] assume ateacher that can provide new valid strings if the current pro-posed grammar does not match the target grammar. For eachnew valid string, their algorithm adds the most general validproduction of the form S→B
1B2···B nto the grammar,
whereBiare terminals or existing nonterminal. It adds new
nonterminals by merging nonterminal sequences which havethe same left and right contexts in expansions. GLADE [4]learns context-free grammars in two phases. First, it learnsa regular expression generalizing each input example. Then,it tries to merge subexpressions of these regular expressionsin a manner similar to our label merging. REINAM [5] usesreinforcement learning to reﬁne a learned CFG, allowing fuzzymatching through a PCFG. It is complementary to our work,as the module that learns a CFG (in their evaluation, GLADE),could be replaced by A
RVADA .
L∗and RPNI are two classic algorithms for the learning of
regular languages. L∗[24] learns regular languages with the
stronger assumption of a minimally adequate teacher, whichcan both (1) act as an oracle for the target language , and (2)given a learned regular language, assert whether it is identicalto the target language or give a counterexample. RPNI [25]learns regular languages in polynomial time, assuming a setof positive and negative examples. GLADE was found tooutperform both these algorithms for program input grammars.The original L
∗paper also describes Lcf, an algorithm for
learning context-free languages in polynomial time, assumingthat the set of terminals and non-terminals is known ahead oftime. This assumption is not reasonable in most contexts.
Closely related is the ﬁeld of distributional learning. Clark
et. al [26], [27] present polynomial algorithms for learningbinary context feature grammars—which capture context-freelanguages in addition to more complex languages—fromstrings. The algorithms rely on the representation of wordsby their contexts, an interesting relation to A
RVADA ’s use of
k-contexts. Unfortunately, polynomial does not mean fast in
practice. We implemented these algorithms in python: even themore efﬁcient one took nearly 5 hours to run on our while
benchmark. Work on strong learning [28] learns grammarswith good parse trees—over tokenized inputs. Again, becauseit uses full context information, it does not scale to large ex-ample sets and overgeneralizes on non-substitutable grammars.This highlights the practical importance of k-contexts.
Also related is the ﬁeld of automata learning; learnlib [29]
is a state-of-the-art Java framework implementing several ofthese algorithms. In particular, it provides an implementationof the TTT [30] algorithm for learning VPDA. These automataaccept a subclass of deterministic context-free languages [31].TTT is optimized for situation where the key structure of in-puts used to query the oracle can be collected in a preﬁx-closedset, as in learning from logs of system behavior. This is lesswell-suited to program inputs with multiple distinct recursivestructures. TTT also relies on the stronger assumption of aminimally-adequate teacher, rather than a blackbox oracle.
Another branch of works use grey- or white-box information
about the oracle to learn grammars. Lin et al.’s work exam-ines execution traces in order to reconstruct program inputsgrammar [32], [33]. A
UTOGRAM [6] tracks input ﬂows into
variables, and uses this dataﬂow information to learn a well-labeled grammar. Mimid [7] goes a step further, tracking thecontrol-ﬂow nodes in which input characters are accessed.It directly maps this control-ﬂow structure to the grammarstructure, and again can take advantage of function names. Theuse of this additional oracle information may make the ﬁnalgrammars more robust and speed up the inference process.On the other hand, A
RVADA ’s blackbox assumption makes
it ﬂexible when this information is not readily accessible, orfor strangely-structured programs. Our tinyc benchmark was
taken directly from Mimid’s evaluation, and A
RVADA achieved
an average F1 score 0.81, compared to Mimid’s 0.96. This isimpressive given that A
RVADA uses the oracle as blackbox.
Section IV-C discussed the use of deep learning to learn
input structures for fuzzing. Other techniques do somethinglike grammar mining to increase the effectiveness of fuzzing.Parser-directed fuzzing [34] uses direct comparisons to inputbytes to automatically ﬁgure out tokens of the input structure;it works best on recursive-descent parsers. GRIMOIRE [35]leverages a sort of one-level grammar by denoting “nontermi-nal” regions of the code as those which can be changed whilemaintaining a certain kind of branch coverage.
Lastly, the Sequitur compression algorithm resembles the
bubbling phase of A
RVADA , bubbling sequences that appear
with high frequency [36]. SEQUIN [37] extends Sequitur tomine attribute grammars. Neither algorithm allows for recur-sive generalization by merging bubble-induced nonterminals.
VII. C
ONCLUSION
We presented A RVADA , a method for learning CFGs from
example strings and oracles. We found that A RVADA out-
performed GLADE in terms of increased generalization on11 benchmarks, with a higher F1 score on average on 9 ofthese benchmarks. These two benchmarks on which A
RVADA
performs relatively less well are a regular language (for URLs)and a language with more complex regular expressions fortokens. This, along with qualitative analysis of the inputsgenerated by A
RVADA and GLADE, suggests that A RVADA
does best in learning recursive structures over tokens, andthat a compelling avenue for improvement is a separatetoken learning step. A
RVADA is available as open source at:
https://github.com/neil-kulkarni/arvada.
ACKNOWLEDGEMENTS
Thanks to Rohan Bavishi and all our anonymous reviewers
for their invaluable feedback on this paper. This research issupported in part by gifts from Fujitsu Research of America,and NSF grants CCF-1900968, CCF-1908870, CNS-1817122.
466REFERENCES
[1] R. Gopinath and A. Zeller, “Building Fast Fuzzers,” CoRR,
vol. abs/1911.07707, 2019.
[2] C. Aschermann, T. Frassetto, T. Holz, P. Jauernig, A.-R. Sadeghi, and
D. Teuchert, “Nautilus: Fishing for Deep Bugs with Grammars,” in 26th
Annual Network and Distributed System Security Symposium, NDSS ’19,
2019.
[3] J. Wang, B. Chen, L. Wei, and Y . Liu, “Superion: Grammar-Aware
Greybox Fuzzing,” in 2019 IEEE/ACM 41st International Conference
on Software Engineering (ICSE), pp. 724–735, 2019.
[4] O. Bastani, R. Sharma, A. Aiken, and P. Liang, “Synthesizing Program
Input Grammars,” in Proceedings of the 38th ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2017,(New York, NY , USA), p. 95–110, Association for Computing Machin-ery, 2017.
[5] Z. Wu, E. Johnson, W. Yang, O. Bastani, D. Song, J. Peng, and T. Xie,
“REINAM: Reinforcement Learning for Input-Grammar Inference,” inProceedings of the 2019 27th ACM Joint Meeting on European SoftwareEngineering Conference and Symposium on the F oundations of SoftwareEngineering, ESEC/FSE 2019, (New York, NY , USA), p. 488–498,Association for Computing Machinery, 2019.
[6] M. H ¨oschele and A. Zeller, “Mining Input Grammars from Dynamic
Taints,” in Proceedings of the 31st IEEE/ACM International Conference
on Automated Software Engineering, ASE 2016, (New York, NY , USA),p. 720–725, Association for Computing Machinery, 2016.
[7] R. Gopinath, B. Mathis, and A. Zeller, “Mining Input Grammars from
Dynamic Control Flow,” in Proceedings of the 2019 28th ACM Joint
Meeting on European Software Engineering Conference and Symposiumon the F oundations of Software Engineering, ESEC/FSE 2020, (NewYork, NY , USA), pp. 1–12, Association for Computing Machinery, 2020.
[8] N. Kulkarni, C. Lemieux, and K. Sen, “Learning Highly Recursive Input
Grammars,” CoRR, vol. abs/2108.13340, 2021. Available at https://arxiv.
org/abs/2108.13340.
[9] C. Cummins, P. Petoumenos, Z. Wang, and H. Leather, “Synthesizing
benchmarks for predictive modeling,” in Proceedings of the 2017
International Symposium on Code Generation and Optimization, CGO’17, p. 86–99, IEEE Press, 2017.
[10] T. J. Parr and R. W. Quong, “ANTLR: A Predicated-LL(k) Parser
Generator,” Software — Practice & Experience, vol. 25, p. 789–810,
July 1995.
[11] D. Stenberg, “cURL: command line tool and library for transferring data
with URLs.” https://curl.se/, 2018. Accessed April 21st, 2021.
[12] T. Berners-Lee, L. Masinter, and M. McCahill, “Uniform Resource
Locators (URL) .” https://tools.ietf.org/html/rfc1738, 1994.
[13] F. Bellard, “Tiny C Compiler.” https://bellard.org/tcc/, 2018. Accessed
April 21st, 2021.
[14] O. Foundation, “NodeJS.” https://nodejs.org/en/, 2018. Accessed April
21st, 2021.
[15] R. Padhye, C. Lemieux, K. Sen, M. Papadakis, and Y . Le Traon,
“Semantic fuzzing with zest,” in Proceedings of the 28th ACM SIGSOFT
International Symposium on Software Testing and Analysis, ISSTA2019, (New York, NY , USA), p. 329–340, Association for ComputingMachinery, 2019.
[16] P. Godefroid, H. Peleg, and R. Singh, “Learn&Fuzz: Machine Learning
for Input Fuzzing,” in Proceedings of the 32nd IEEE/ACM International
Conference on Automated Software Engineering, ASE 2017, p. 50–59,IEEE Press, 2017.
[17] C. Cummins, P. Petoumenos, A. Murray, and H. Leather, “Compiler
Fuzzing through Deep Learning,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis,ISSTA 2018, (New York, NY , USA), p. 95–105, Association for Com-puting Machinery, 2018.[18] L. Lee, “Learning of Context-Free Languages: A Survey of the Litera-
ture”,” tech. rep., Harvard Computer Science Group, 1996.
[19] A. Stevenson and J. R. Cordy, “A Survey of Grammatical Inference
in Software Engineering,” Science of Computer Programming, vol. 96,
pp. 444–459, 2014.
[20] E. M. Gold, “Language Identiﬁcation in the Limit,” Information and
Control, vol. 10, no. 5, pp. 447–474, 1967.
[21] D. Angluin and M. Kharitonov, “When Won’t Membership Queries
Help?,” J. Comput. Syst. Sci., vol. 50, p. 336–355, Apr. 1995.
[22] R. J. Solomonoff, “A new method for discovering the grammars of
phrase structure languages,” in Information Processing, Proceedings of
the 1st International Conference on Information Processing , pp. 285–
289, UNESCO (Paris), 1959.
[23] B. Knobe and K. Knobe, “A method for inferring context-free gram-
mars,” Information and Control, vol. 31, no. 2, pp. 129–146, 1976.
[24] D. Angluin, “Learning Regular Sets from Queries and Counterexam-
ples,” Inf. Comput., vol. 75, p. 87–106, Nov. 1987.
[25] J. Oncina and P. Garcia, “Identifying Regular Languages In Polynomial
Time,” in Advances in Structural and Syntactic Pattern Recognition ,
v o l .5o fMachine Perception and Artiﬁcal Intelligence, pp. 99–108,World Scientiﬁc, 1992.
[26] Alexander Clark and R ´emi Eyraud and Amaury Habrard, “A Polynomial
Algorithm for the Inference of Context Free Languages,” in Gram-
matical Inference: Algorithms and Applications, (Berlin, Heidelberg),Springer, 2008.
[27] A. Clark, R. Eyraud, and A. Habrard, “Using Contextual Representations
to Efﬁciently Learn Context-Free Languages,” Journal of Machine
Learning Research, vol. 11, no. 92, pp. 2707–2744, 2010.
[28] A. Clark, “Learning Trees from Strings: A Strong Learning Algorithm
for some Context-Free Grammars,” Journal of Machine Learning Re-
search, vol. 14, no. 75, pp. 3537–3559, 2013.
[29] M. Isberner, F. Howar, and B. Steffen, “The open-source learnlib,” in
Computer Aided V eriﬁcation (D. Kroening and C. S. P ˘as˘areanu, eds.),
(Cham), pp. 487–495, Springer International Publishing, 2015.
[30] M. Isberner, F. Howar, and B. Steffen, “The TTT Algorithm: A
Redundancy-Free Approach to Active Automata Learning,” in Runtime
V eriﬁcation (B. Bonakdarpour and S. A. Smolka, eds.), (Cham), Springer
International Publishing, 2014.
[31] R. Alur and P. Madhusudan, “Adding Nesting Structure to Words,” J.
ACM , vol. 56, May 2009.
[32] Z. Lin, X. Zhang, and D. Xu, “Reverse Engineering Input Syntactic
Structure from Program Execution and Its Applications,” IEEE Trans-
actions on Software Engineering, vol. 36, no. 5, pp. 688–703, 2010.
[33] Z. Lin and X. Zhang, “Deriving Input Syntactic Structure from Execu-
tion,” in Proceedings of the 16th ACM SIGSOFT International Sympo-
sium on F oundations of Software Engineering, SIGSOFT ’08/FSE-16,(New York, NY , USA), p. 83–93, Association for Computing Machinery,2008.
[34] B. Mathis, R. Gopinath, M. Mera, A. Kampmann, M. H ¨oschele, and
A. Zeller, “Parser-Directed Fuzzing,” in Proceedings of the 40th ACM
SIGPLAN Conference on Programming Language Design and Imple-mentation, PLDI 2019, (New York, NY , USA), p. 548–560, Associationfor Computing Machinery, 2019.
[35] T. Blazytko, C. Aschermann, M. Schl ¨ogel, A. Abbasi, S. Schumilo,
S. W ¨orner, and T. Holz, “GRIMOIRE: Synthesizing Structure While
Fuzzing,” in Proceedings of the 28th USENIX Conference on Security
Symposium, SEC’19, (USA), p. 1985–2002, USENIX Association, 2019.
[36] C. G. Nevill-Manning and I. H. Witten, “Identifying Hierarchical
Structure in Sequences: A Linear-Time Algorithm,” Journal of Artiﬁcial
Intelligence Research, vol. 7, p. 67–82, Sept. 1997.
[37] R. Luh, G. Schramm, M. Wagner, H. Janicke, and S. Schrittwieser,
“SEQUIN: a grammar inference framework for analyzing malicious sys-tem behavior,” Journal of Computer Virology and Hacking Techniques,
vol. 14, no. 4, pp. 291–311, 2018.
467