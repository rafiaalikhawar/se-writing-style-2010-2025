AID: Efﬁcient Prediction of Aggregated Intensity of
Dependency in Large-scale Cloud Systems
Tianyi Yang∗, Jiacheng Shen∗, Y uxin Su∗, Xiao Ling†, Y ongqiang Yang†, and Michael R. Lyu∗
∗Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China.
Email:{tyyang, jcshen, yxsu, lyu}@cse.cuhk.edu.hk
†Computing and Networking Innovation Lab, Cloud BU, Huawei
Email:{lingxiao1, yangyongqiang}@huawei.com
Abstract —Service reliability is one of the key challenges that
cloud providers have to deal with. In cloud systems, unplanned
service failures may cause severe cascading impacts on their de-pendent services, deteriorating customer satisfaction. Predictingthe cascading impacts accurately and efﬁciently is critical to theoperation and maintenance of cloud systems. Existing approachesidentify whether one service depends on another via distributedtracing but no prior work focused on discriminating to whatextent the dependency between cloud services is. In this paper,we survey the outages and the procedure for failure diagnosis intwo cloud providers to motivate the deﬁnition of the intensity ofdependency. We deﬁne the intensity of dependency between twoservices as how much the status of the callee service inﬂuencesthe caller service. Then we propose AID, the ﬁrst approach topredict the intensity of dependencies between cloud services. AIDﬁrst generates a set of candidate dependency pairs from thespans. AID then represents the status of each cloud service witha multivariate time series aggregated from the spans. With therepresentation of services, AID calculates the similarities betweenthe statuses of the caller and the callee of each candidate pair.Finally, AID aggregates the similarities to produce a uniﬁed valueas the intensity of the dependency. We evaluate AID on thedata collected from an open-source microservice benchmark anda cloud system in production. The experimental results showthat AID can efﬁciently and accurately predict the intensityof dependencies. We further demonstrate the usefulness of ourmethod in a large-scale commercial cloud system.
Index T erms—cloud computing, software reliability, AIOps,
service dependency
I. I NTRODUCTION
Service reliability is one of the key challenges that cloud
providers have to deal with. The common practice nowadays
is developing and deploying small, independent, and looselycoupled cloud microservices that collectively serve users’requests. The microservices that serve the same purpose arecalled cloud services
1. The microservices communicate with
each other through well-deﬁned APIs. Such an architectureis called microservice architecture [1]. The microservice ar-chitecture has been widely adopted in cloud systems becauseof its reliability and ﬂexibility. Under this architecture, mi-croservice management frameworks like Kubernetes will beresponsible for managing the life cycles of microservices.
Y uxin Su is the corresponding author.
1For simplicity, in this paper, “cloud service” and “cloud microservice” are
interchangeable when they are used alone.Developers can focus on the application logic instead of thebothering tasks of resource management and failure recovery.
Although microservice management frameworks provide
automatic mechanisms for failure recovery, unplanned servicefailures may still cause severe cascading effects. For example,failures of critical services that provide basic request routingfunctions will impact the invocation of cloud services, slowdown request processing, and deteriorate customer satisfaction.Therefore, evaluating the impact of service failures rapidlyand accurately is critical to the operation and maintenanceof cloud systems. Knowing the scope of the impact, reliabilityengineers can put more emphasis on services that have greaterimpacts on others.
A failed service will only affect services that will invoke
it. In other words, service invocations cause dependenciesbetween services. Many recent approaches [2], [3] proposeto use the dependencies of services to approximate theirfailure impact. All the services and dependencies in a cloudsystem collectively construct a directed graph of services,which is also called a dependency graph. Identifying whetherone service depends on another in cloud systems can bewell solved by industrial tracing frameworks like Dapper andJaeger. By using these frameworks, all the invocations betweenthe caller and callee services can be recorded as traces that arecomposed of spans. The attributes about each invocation, likeduration, status, invoked service name, timestamp, etc., arerecorded in each span. Based on the spans, current dependencydetection methods treat the dependency as a binary valueindicating whether one service invokes another or not.
However, modeling the relations of services solely with
binary dependencies is not precise enough. To show the insufﬁ-ciency of existing methods, we ﬁrst conduct an empirical studyon the outages of Amazon Web Service and Huawei Cloud.We point out that it is inefﬁcient to conduct failure diagnosisand recovery based on binary dependencies. This is becausethe different dependencies of a cloud service impact the cloudservice in different ways. Manual examination of differentdependencies without any priority is inefﬁcient, especially incloud systems where the number of dependencies could belarge. Based on this observation, we argue that it will be help-ful if the dependency can be measured as a continuous valuethat indicates the intensity of this dependency. Speciﬁcally,by checking services that are dependent on the failed service
6532021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000642021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678534
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
with large intensity values, on-call engineers (OCEs) can ﬁnd
the root cause of a system failure with a higher probability.By recovering the services that are strongly dependent on thefailed one, the whole system could be restored faster.
To improve the reliability of cloud systems, in this paper, we
propose AID, an end-to-end approach to predict the intensityof dependencies between cloud microservices for cascadingfailure prediction. We ﬁrst generate a set of candidate depen-dency pairs from the spans. Then we distribute each span intodifferent ﬁxed-length bins according to their timestamp andservice name. We calculate the statistics of all spans in eachbin as the Key Performance Indicators (KPIs) for the bin. TheKPIs of one service form a multivariate time series that willbe treated as the representation of the service’s status. Foreach candidate dependency pair, we calculate the similaritiesbetween the statuses of the two services in the pair. Finally,we aggregate the similarities to produce a uniﬁed value as theintensity of the pair.
To show the effectiveness of AID, we evaluate AID on
two datasets. One is a simulated dataset, and the other is anindustrial dataset. For the simulated dataset, we deploy train-ticket, an open-source microservice benchmark system, simu-late users’ requests, and collect the traces. For the industrialdataset, we collect the traces from a production cloud system.Then we evaluate AID on the datasets and compare its perfor-mance with several baselines. The experimental results showthat our proposed method can accurately measure the intensityof dependencies and outperform the baselines. Furthermore,we showcase the successful usage of our method in a large-scale production cloud system. In addition, we release bothdatasets to facilitate future studies.
The main contributions of this work are highlighted as
follows:
•We propose AID, the ﬁrst method to quantify the intensityof dependencies between different services.
•The evaluation results show the effectiveness and efﬁ-ciency of the proposed method.
•We release a simulated dataset and an industrial datasetfrom a production cloud system to facilitate future stud-ies.
Organization. The remainder of this paper is organized as
follows. Section II provides motivation and background knowl-edge that underpin our approach. We describe our survey andempirical study on real outages that motivate the proposedmethod in Section III. Section IV elaborates on the methodin detail. Section V introduces the datasets, baselines andshows the experimental results. Successful use cases of theproposed method in a production cloud system are demon-strated in Section VI. We discuss the practical usage, theperceived limitations, and the possible threats to validity inSection VII. Section VIII introduces related works. The lastsection, Section IX, concludes this paper and lists directionsfor future exploration.II. B
ACKGROUND
In this section, we brieﬂy describe the service-oriented
architecture of cloud systems and the distributed tracing toolsin cloud systems. Then we present the main techniques, i.e.,time series similarity analysis, that underpin our approach.
A. The Architecture of Cloud Systems
Modern cloud systems are often constructed from a complex
and large-scale hierarchy of distributed software modules [4].
The common practice nowadays is to develop and deploy thesesoftware modules as cloud microservices that collectivelycomprise multiple large cloud services [5]. Microservices aresmall, independent, and loosely coupled software modules thatcan be deployed independently [1]. Different microservicesserve different responsibilities [6] like user authentication,resource allocation, virtual network management, billing, etc.When an external request arrives at the cloud system, therequest will be routed through the system and served by dozensof different cloud services and microservices. The microser-vices communicate with each other through well-deﬁned APIsand, therefore, can be refactored and scaled independently anddynamically to adapt to incidents like surges of requests andservice failures [7]. Such an architecture is called microservicearchitecture [1].
The microservice architecture becomes increasingly popular
due to its high ﬂexibility, reusability, and scalability [8]. It en-ables agile development and supports polyglot programming,i.e., microservices developed under different technical stackscan work together smoothly. However, the loosely couplednature of microservices makes it difﬁcult for engineers toconduct system maintenance. Different microservices in alarge cloud system are usually developed and managed byseparate teams. Each team only has access to their ownservices as well as services that are closely related, whichmeans they only have a local view of the whole system [9].
As a result, the failure diagnosis, fault localization, and
performance debugging in a large cloud system become morecomplex than ever [10]–[12]. Despite various fault tolerancemechanisms introduced by modern cloud systems, it is stillpossible for minor anomalies to magnify their impact andescalate into system outages. As exempliﬁed in Section III-A,when a cloud service enters an anomalous state and does notreturn results in a timely manner, other services that dependon it will also suffer from the increased request latency. Suchanomalous states can propagate through the service-callingstructure and eventually affect the entire system, resulting ina degraded user experience or even a service outage.
B. Distributed Tracing
For commercial cloud providers, it is crucial to troubleshoot
and ﬁx the failures in a timely manner because massive
user applications may be affected even by a small servicefailure [13]. Distributed tracing is a crucial technique forgaining insight and observability to cloud systems.
In large-scale cloud systems, a request is usually handled
by multiple chained service invocations. As clues to defective
654Span 0
Span 1 Span 2 Span 3
Span 4 Span 5
ParentID Name SpanID Timestamp Duration …Result
Fig. 1. A trace log with six spans.
services are hidden in the intricate network of services, it
is difﬁcult for even knowledgeable OCEs to keep track ofhow a request is processed in the cloud system. Distributedtracing provides an approach to monitor the execution pathof each request. For chained service invocations, e.g., serviceAinvokes service B, and service Binvokes service C,i t
is important to know the status of each service invocation,including the result, the duration of execution, etc. By addinghooks to the services and microservices of the cloud system, adistributed tracing system [14]–[16] can record the contextualinformation of each service invocation. Such records are calledspan logs, abbreviated as spans. A span represents a logical
unit of execution that is handled by a microservice in acloud system. All the spans that serve for the same requestcollectively form a directed graph of spans, as illustrated inFigure 1. Such a directed graph of spans generated by a requestis called a piece of trace log, abbreviated as a trace. A trace
represents an execution path through the cloud system. With atrace, engineers can track how the request propagates throughthe cloud system. Collectively analyzing the traces of theentire cloud system can help engineers obtain in-depth latencyreports that could assist failure diagnosis, fault localization,and surface performance degradation in the cloud system.
Span ID e22f30bdbfd09134
Parent Span ID b42a04bf18997d5d
Name ts-preserve-service
Timestamp (μ s) 1618589098705000
Duration (μ s) 1126
Result SUCCESS
Trace ID c0d17d481f47bdd9
Additional Logs ...
Fig. 2. A span generated by the train-ticket benchmark.
Although the actual implementation of distributed tracing
systems varies a lot, the types of information they recordare similar. For clarity, we formally describe the attributes ofspans as follows. Suppose we have a trace Tcomposed of
spans{s
1,s2,...,sn}, a span si∈Tcontains the following
attributes2:
•sid
i: The ID of span si,
•spid
i: The ID of the parent span of si,
•stid
i: The ID of the trace that sibelongs to,
2Other additional contextual information [17] is omitted as we do not use
them in our method.•snamei : The name of service/microservice corresponding
tosi,
•stsi: The time stamp of si,
•sdi: The duration of execution of si, and
•sri: The result of execution of si.
Figure 2 illustrates a span generated by the
train-ticket benchmark [18]. It means that service
ts-preserve-service was invoked at 04:58 on
April 17, 2020. The duration of execution is 1126 μs, and
the execution result is SUCCESS.
C. Time Series Similarity Analysis
Time series data are ubiquitous. One important task in time
series data mining is to measure the similarity between two
time series. Similar to human intuition, the similarity measureis usually based on the similarity between the shapes of twotime series [19].
Dynamic time warping (DTW) [20] is a widely-used sim-
ilarity measure when two time series have the same overallcomponent shapes but are not aligned on the timeline. Itattempts to align two time series along a timeline by distortingthe timeline for one time series so that its converted form isbetter aligned with the second time series. DTW was initiallyused in speech recognition applications [20] and extended andoptimized by many works [21]–[23].
III. M
OTIV A TIONS
The research described in this paper is motivated by the
maintenance of a real-world cloud system in production. Inthis section, we ﬁrst survey thirteen publicly known serviceoutages that severely affected Amazon Web Services (AWS)from 2011 to 2020. Among the thirteen outages, we identifyﬁve that are related to service dependency and summarized theconsequences of inappropriate management of service depen-dency. Second, we empirically study the diagnosis records ofﬁve real outages in the cloud system of Huawei Cloud that arerelated to inappropriate management of service dependency.Our study indicates that the information in the traces has notbeen used efﬁciently and current practice heavily relies on theengineers’ familiarity with the dependencies in the system.Lastly, we propose to measure the intensity of dependency interms of status propagation between dependent cloud services.We demonstrate the usefulness of the intensity by motivatingexamples in real cloud systems.
A. A Survey of the Outages in AWS
Service outages are inevitable in the cloud [24]. In this sec-
tion, we empirically analyzed over 1000 incidents of Huawei
Cloud in 2019 and thirteen publicly known major outages
3of
AWS from 2011 to 2020. Among the incidents of HuaweiCloud, we found that improper service dependency is themost frequent reason for failures in Huawei Cloud. Amongthe outage summaries of AWS, we also identiﬁed that ﬁve
3https://aws.amazon.com/premiumsupport/technology/pes/
655TABLE I
SUMMARY OF AWS OUTAGES RELA TED TO SERVICE DEPENDENCY .
DateConsequences
Cascading Failure Slow Recovery
Apr 21, 2011 /check
June 29, 2012 /check
Oct 22, 2012 /check
Aug 7, 2014 /check
Nov, 25 2020 /check/check
of the outages (38%) are related4to service dependency. As
shown in Table I, among the ﬁve outages that are related to
service dependency, three of them are due to cascading failurestriggered by erroneous upgrades of services. During the failurerecovery, the inappropriate dependencies lead to slow failurerecovery in three outages.
AWS is the worldwide leading cloud provider. It operates
in many regions, each consisting of multiple AvailabilityZones (AZs). Each AZ uses separate physical facilities andindependently provides various cloud services [5], includingSteam Data Processing (Kinesis), API Usage Analysis (Cog-nito), Customer Dashboard (Cloudwatch), Elastic ComputeCloud (EC2), Relational Database Service (RDS), Elastic LoadBalancing (ELB), and Low-level Block Storage (EBS), etc. Forbrevity’s sake, we simplify the dependencies as 1) EC2, RDS,and ELB all depend on EBS, and 2) Cognito and Cloudwatchdepend on Kinesis
5.
The outages on April 21, 2011, and October 22, 2012, are
both caused by erroneous upgrades of EBS. When EBS failed,the services that depend on EBS, i.e., EC2,ELB, and RDS,are all affected. The cascading failures resulted in servicedisruptions of over 48 hours in the US-East-1 Region of AWS.
The outages on June 29, 2012, and August 7, 2014, are
both triggered by the blackouts. After the blackout, the RDSand ELB services restarted quickly as expected, but they arestill unable to fully recover because they both depend on EBSservice which, at that time, can not recover simultaneously.The slow failure recovery incurred by service dependenciesaffected the service availability for days in the US-East-1Region and the EU West-1 Region of AWS. As a follow-upoptimization, ELB service reduced the dependency on EBSafter the outage in 2014.
On November, 25 2020, the erroneous upgrade of Kinesis
lead to its failure, cascadingly causing the failure of Cognitoand Cloudwatch. More severely, during the recovery, AWScould not notify the customers via the normal way because thenormal customer notiﬁcation service also relied on Cognito.Due to the inner mechanism of Kinesis, the recovery of Kinesistook more than ten hours. Thus the recoveries of Cognito
4The outages are usually caused by various reasons that mutually affect
each other. Service dependency is one of the reasons, so we use the word
“related”.
5The actual dependency relations between these services are complicated.
We omit the details here.and Cloudwatch were also slowed down. As a follow-up
optimization, Cognito and Cloudwatch services reduced thedependency on Kinesis after the severe outage.
B. Drawbacks of Current Failure Diagnosis Methods
To gain more knowledge about the procedure of failure
diagnosis in industrial circumstances, we ﬁrst interviewed en-
gineers in Huawei Cloud
6. Then we summarize the procedure
of failure diagnosis, and point out the drawbacks of currentpractice in Huawei Cloud.
In Huawei Cloud, the failure diagnosis can be triggered
by two systems, i.e., the customer support system and themonitoring system. When a customer experiences a servicedisruption, the customer can submit a support ticket in thecustomer support system. The on-call engineers will distributethe support ticket to the corresponding engineers responsiblefor the service. The monitoring system, on the other hand,monitors the Key Performance Indicators (KPIs) and the logsof each service in the cloud system. If the KPIs or the numberof erroneous logs of one service increased abnormally orreached predeﬁned thresholds, the monitoring system will sendan alert to the corresponding engineers. Upon receiving thesupport ticket or alert, engineers start diagnosing the failures.
We summarize the common practice of failure diagnosis
in Huawei Cloud as follows. Suppose the anomalous serviceisA, OCEs will ﬁrst check whether the failure is caused by
the faults of service A(e.g., an erroneous upgrade). If so,
the development team of service Awill handle the failure. If
service Ais in good condition, OCEs will analyze the status
of all services that Adepends on. The status includes the
number of calls, the error rate, etc. If they found the failureof a service Bis likely to cause the failure of service A, then
engineers will continue to investigate service B. Recall that
all the services construct a directed graph where each noderepresents a service. The failure diagnosis procedure can beviewed as a recursive search on the service dependency graph.
The practice works well in small cloud systems that con-
tain tens of cloud services. However, the dependencies inlarge-scale cloud systems are much more complicated [10],making manual failure diagnosis inefﬁcient and difﬁcult forengineers. Engineers may have trouble identifying the causeof the failure. In this case, the development teams of allcloud services have to check whether the failure is caused bytheir corresponding services. Sometimes engineers may inferthe possible causes of a failure, but it heavily relies on theengineer’s familiarity with the dependencies in the system.In summary, the complex dependency relations in large-scalecloud systems make failure diagnosis difﬁcult, and currentpractice is inefﬁcient and dependent on the human experience.
C. Intensity of Service Dependency
A cloud system is composed of many services. The depen-
dency between two services is caused by one service invoking
the other via predeﬁned APIs. Existing tools [2], [25], [26]
6AWS does not disclose the detailed procedures of failure diagnosis related
to the ﬁve outages, so we cannot analyze the aforementioned outages in depth.
656Fig. 3. The statuses of service A,BandC.Ainvokes BandCbutBhas a
greater effect on A.
treat the dependency as a binary relation, i.e., if the caller
service invokes the callee service, then the caller is dependenton the callee. We suggest that this binary dependency metricis not ﬁne-grained enough for cloud maintenance. Figure 3shows the statuses of three services
7A,B, and Cin Huawei
Cloud. Service Ainvokes both service Band service C. Service
Bencountered failures. The x-axis represents time in minute.
The y-axes represent the number of invocations per minute,the average duration of invocations per minute, and the errorrate per minute of A,B, and C. Although service Ainvokes
service Band service C, it is obvious that the statuses of
BandCinﬂuence the status of Ain different degrees. The
reason is that the functionalities provided by service Aand
Bare creating virtual machines, and allocating block storage,
respectively. Creating a virtual machine requires allocating oneor more block storage. Thus, the failure of service Binevitably
affects service A. On the contrary, due to the fault tolerance
mechanism of service A, the failure of service Cwill not
affect service Aa lot. Thus, it is more accurate to say that
the intensity of dependency between service Aand service B
is higher than the intensity of dependency between service A
and service C. As can be seen in Figure 3, the similarity of
the statuses reﬂect the difference in the intensities.
Ideally, if the development team of every cloud microservice
accurately provides the intensity of dependencies for everydependent services, the failure diagnosis could be accelerated.OCEs can prioritize the services that exhibit higher intensityof dependency instead of inspecting all the dependent services(Section III-B) if they have accurate intensity information.
7For conﬁdentiality reasons, we cannot reveal the names of related services.However, due to the complexity and the fast-evolving nature ofcloud systems [27], manually maintaining the dependency re-lations with intensity is very difﬁcult. As a result, OCEs oftenstruggle in diagnosing failures due to the lack of intensities. Inorder to relieve the pressure on OCEs, we propose to predictthe intensity of dependency from the statuses of services.
IV . A
PPROACH
In this section, we present AID, a framework for predicting
the A ggregated I ntensity of service D ependency in large-
scale cloud systems. We ﬁrst present the overall workﬂow ofAID. Then we elaborate on each step in detail, i.e., candidateselection, service status generation, and intensity prediction.
A. Overview
The overall workﬂow of AID is illustrated in Figure 4. AID
consists of three steps: candidate selection, status generation,
and intensity prediction. Given the raw traces, AID ﬁrstgenerates a set of candidate service pairs (P,C )where service
Pdirectly invokes service C(Section IV-B). The intuition
is that direct service invocation incurs direct dependency tosome degree. Indirect dependencies through the transitivityof service invocation will be discussed in Section VII-A.For status generation, we generate the status of all ser-vices (Section IV-C). The status of one service is composedof three aspects of dependency, i.e., number of invocations,duration of invocations, error of invocations. Each aspect ofthe service’s status contains one or more Key PerformanceIndicators (KPIs), depending on the actual implementation ofthe distributed tracing system. A KPI is an aggregated valueof a service status of all the spans of a service in a ﬁxedtime interval, e.g., 1 minute. We use the statistical indicatorsof each aggregation as the values of the KPIs. Motivated bythe experience of engineers introduced in Section III-B, wepropose to predict the intensity of service dependencies fromthe similarity of the statuses of dependent services. The intu-ition behind using the similarity of time series is to evaluatethe propagation of service statuses. The intensity predictionstep (Section IV-D) predicts the intensity of dependency bymeasuring the similarity between two service’s statuses. Thesimilarity between two service’s statuses is a normalized andweighted average of the similarity of all the KPIs of the twoservices. We calculate the similarity between two KPIs by adynamic status warping algorithm. Finally, AID produces thedependency graph with intensity.
B. Candidate Selection
In general, direct service invocations can be divided into
two categories, i.e., synchronous invocations and asynchronous
invocations. Modern tracing mechanisms can keep track ofboth synchronous and asynchronous invocations [28]. Givenall the raw traces of the cloud system, in this step, wegenerate a candidate dependency set Cand . The candi-
date dependency set Cand contains service invocation pairs
(P
1,C1),(P2,C2),···,(Pn,Cn). Each pair (Pi,Ci)in the
candidate dependency set denotes that the service named
657Raw 
TracesService Status 
GenerationDependency 
Graph with 
Intensity
Section IV .CSection IV .DIntensity 
Prediction 
Status Series 
of ServicesCandidate  
Dependency 
ListCandidate 
Selection
Section IV .B
Fig. 4. The overall workﬂow of AID.
Piinvokes the service named Ciat least once. Therefore,
servicePidepends on service Ci. This step is to shrink the
search space of possible dependent pairs because the service
invocations indicate direct dependencies.
To generate the candidate dependency set, we need to know
the name of the caller service and the callee service. The nameof callee service is clearly recorded in the span, but the nameof the caller service is not. Hence, we ﬁrst augment each span s
by adding another attribute s
pnamewhich denotes the service
name of the parent span. Speciﬁcally, the augmentation ofattributes
pnameis achieved by 1) looking for another span
s/primewhoseidis the same as spid, and 2) set the name of
s/primeasspname. Then we iterate over all the spans and add
(spname,sname)to the candidate dependency set by the union
operation.
For example, assuming the name of services are
the same as the index of spans, the six spansin Figure 1 will result in a candidate set of {
(Service
0,Service 1), (Service 1,Service 4), (Service 0,
Service 2), (Service 0,Service 3), (Service 3,Service 5)}.
C. Service Status Generation
In this step, we generate the status of all cloud services from
the traces. We start by deﬁning the status of a cloud service(i.e., service status) and then describe the procedure of servicestatus generation.
Deﬁnition of Service Status: A service invocation is com-
posed of three logical components, i.e., the caller service, thecallee service, and the network communication. In particular,the caller service initiates an invocation to the callee servicevia the network. The callee service then processes the invo-cation, during which it may invoke other services. After theprocessing is ﬁnished, the callee service will send the result,e.g., status, to the caller service via the network. Hence, wecould derive three aspects of service invocations: initiation
of invocation, processing, result. As service invocations occur
repeatedly, the three aspects of service invocations can derivethree aspects of service dependency:
•Number of Invocations : The number of invocations from
the caller to the callee,
•Duration of Invocations : The duration of invocations,
•Error of Invocations : The number of successful invoca-
tions from the caller to the callee.Representation of Service Status: In a cloud system, the
spans record information about every invocation. Intuitively,the status of a cloud service can be easily obtained from thespans of that service. Inspired by the common practice incloud monitoring [29], we distribute the spans of one serviceinto many bins according to the spans’ timestamps. Each binaccepts spans whose timestamp is in a short, ﬁxed-lengthperiod. We denote the length of the short period as τ.F o r
example, the span shown in Figure 2 will be put in the bin ofts-preserve-service at time 04:58, April 17 2020. We
can then represent the status of a cloud service in a shortperiod by the statistical indicators of all the spans in thecorresponding bin.
Formally, given all the spans in the cloud system over a long
periodT, we ﬁrst initiate S×Nempty bins of the predeﬁned
sizeτ.Sis the number of microservices. N, determined by
T
τ, is the number of bins. Then we distribute all spans into
different bins according to their timestamp stsand service
namesname. After that, we calculate the following three types
of indicators as the KPIs for each bin.
•invoM
t: Total number of invocations (spans) in the bin;
•errM
t: Error rate of the bin, i.e., the number of errors
divided by the number of invocations;
•durM
t: Averaged duration of all spans in the bin;
wheretis the time of the bin and Mis the microservice name
of the bin. If a service is not invoked in a particular bin (i.e.,
the corresponding bin is empty), all the KPIs will be zero. Inthe end, we get the KPIs of every service Mat every period
t. Ordering the bins by t, we get three time series of KPIs for
each cloud service, denoted as invo
M,errM, anddurM.W e
name the time series of server KPIs as status series.
D. Intensity Prediction
In this paper, we deﬁne the intensity of dependency between
two services as how much the status of the callee service
inﬂuences the status of the caller service. The step of intensity
prediction quantitatively predicts the intensity of dependencyby measuring the similarity between two services’ statusseries. Speciﬁcally, we calculate the similarity of two differentstatus series with dynamic status warping and aggregate all thesimilarities to get the overall similarity.
1) Dynamic Status Warping: Inspired by the dynamic time
warping algorithm (DTW) [30], we propose the dynamicstatus warping (DSW) algorithm (Algorithm 1) to calculate the
658Algorithm 1: Dynamic Status Warping
Input: The status series of caller service and callee
servicestatusP,statusC; duration series of
calleedurC, estimated round trip time δrtt,
max time drift δd
Output: The similarity between two status series
1Set the warping window w= max(durC)+δrtt
2K=length (statusC)
3N=length (statusP)
4Initialize the cost matrix C∈RK×N, set the initial
values as +∞
5C1,1=(statusP
1−statusC1)2
6fori=2...min(δ d,K)do// Initialize the ﬁrst
column
7 Ci,1=Ci−1,1+(statusP1−statusC
i)2
8end
9forj=2...min(w +δd,N)do// Initialize the ﬁrst
row
10 C1,j=C1,j−1+(statusPj−statusC
1)2
11end
12fori=2...K do
13 forj=m a x ( 2 ,i−δd)...min(N,i +w+δd)do
14 Ci,j=m i n ( Ci−1,j−1,Ci−1,j,Ci,j−1)+
(statusP
j−statusCi)2
15 end
16end
17return CK,N
distance between two status series. DSW automatically warps
the time in chronological order to make the two status seriesas similar as possible and get the similarity by summing thecost of warping. It utilizes dynamic programming to calculatean optimal matching between two status series. Given twoservicesP,C, and their status series invo
P,invoC,errP,
errC,durP, anddurC, the warping from the callee Cto the
callerPis specially designed for the cloud environment. The
design considerations include:
Directed warping: Due to the latency of the network and
the time of processing, it takes some time for the status of
the callee service to affect the status of the caller service.Therefore, different from dynamic time warping, the timewarping of DSW is directed, meaning that the matching fromthe callee to the caller can only happen in chronological order.
Adaptive propagation window: In cloud systems, after the
round trip time (δ
rtt) plus the duration of request processing,
the caller can receive the result of an invocation. Thus, the
size of the directed warping window wis automatically set as
the maximum duration of the callee’s spans plus δrtt.
Time drift: The machine time may drift due to issues with time
synchronization in cloud systems, so we add an undirectedtime drift δ
dto the warping window.
In summary, statusC
ican only be matched with one of
[statusP
i−δd,statusPi+w +δ
d]. The DSW returns the warping
costCM,N as the measure of similarity.TABLE II
DA TASET STA TISTICS .
Dataset TT Industry9
# Microservices 25 192
# Spans 17,471,024 About 1.0e10
# Strong 18 67
# Weak 1 8
2) Similarity Aggregation: For all (Pi,Ci)∈Cand ,w e
calculate similarities between their status series, denoted as
d(Pi,Ci)
invo ,d(Pi,Ci)
err , andd(Pi,Ci)
dur. We normalize the similarity
across the whole candidate set with a min-max normalizationwith Equation 1, where status∈{invo,err,dur }.
d(Pi,Ci)
status =d(Pi,Ci)
status−min(d(P,C )
status )
max(d(P,C )
status )−min(d(P,C )
status )(1)
The intensity of dependency between PiandCiis the
average similarity of all three similarities between their statusseries.
I
(Pi,Ci)=1
3/summationdisplay
status∈Sd(Pi,Ci)
status ,S={invo,err,dur } (2)
Finally, we can build the dependency graph with intensity
from the candidate set and the corresponding intensity values.
V. E XPERIMENTS
In this section, we evaluate AID on both a simulated dataset
and an industrial dataset. Particularly, we aim to answer thefollowing research questions (RQs):
•RQ1. How effective is AID in predicting the intensity of
dependency?
•RQ2. What is the impact of different parameter settings?
•RQ3. What is the impact of different similarity measures?
•RQ4. How efﬁcient is AID?
A. Experimental Setup
1) Dataset: To show the practical effectiveness of AID, we
further conduct experiments on the simulated dataset and anindustrial dataset from the cloud system of Huawei Cloud.Since there are no existing datasets of trace logs, we deploya benchmark microservice system to simulate a real cloudsystem. We simulate user requests and collect the generatedtrace logs to construct the simulated dataset. We release bothdatasets with the paper to facilitate future studies in this ﬁeld
8.
Simulated dataset: For the simulated dataset, we deploy
train-ticket [18], an open-source microservice benchmark, fordata collection. Train-ticket is a web-based ticketing systemwith 25 microservices, through which users can search for
8https://github.com/OpsPAI/aid
9We only labeled 75 dependencies that the engineers are familiar with.
659tickets, reserve tickets, and pay for the reserved tickets. An
open-source tracing framework, Jaeger, is used to trace all theAPI calls. To generate traces, we develop a request simulatorthat simulates normal users’ access to the ticketing system. Thesimulator will log in to the system, search for tickets, reservea ticket according to the results of the search, and pay for theticket. Then we collect the traces from Jaeger and transformthe traces into 17,471,024 spans. The dataset is termed as “TT”in Table II.
Industrial dataset: Apart from the simulated dataset, we
also collected traces from a region of Huawei Cloud toevaluate AID. To support tens of millions of users world-wide, the cloud system of Huawei Cloud contains numerouscloud services and microservices. The service invocationsare monitored and recorded by an independently developeddistributed tracing system. The complex dependency relationsin the cloud system increase the burden of OCEs. The OCEscan diagnose problematic microservices timely if the intensityof dependencies can be automatically detected in real-time. Toevaluate the practical effectiveness of our method, we collecteda 7-day-long trace dataset with 192 microservices in April2021. The dataset is termed as “Industry” in Table II.
Manual labeling: Since our method is unsupervised, labels
are only for evaluation. Neither of the datasets has labels aboutthe intensity of dependency, so manual labeling is needed.We set two candidate labels for the intensity of dependency,i.e., “strong” and “weak”. Given a candidate dependency pair(P,C ), if the failure of service Cwill cause the failure of
serviceP, the intensity between (P,C )should be labeled
“strong”; otherwise it should be labeled “weak”. For thesimulated dataset, two Ph.D. students inspect the source codeof all microservices and label every service dependency inde-pendently. For the industrial dataset, several senior engineersare invited to manually label the intensity of dependency. Inboth processes, disagreement on labels will be discussed untilconsensus is reached. Finally, we convert the “strong” labelsto1and the “weak” labels to 0so that they can be effectively
compared with the computed intensities.
The statistics of the datasets are listed in Table II. “#
Microservices” denotes the number of microservices in thedataset. “# Spans” denotes the number of spans in the dataset.“# Strong” and “# Weak” denote the number of dependenciesthat are labeled with “strong” or “weak” respectively.
2) Baselines: Since there is no existing work that measures
the intensity of service dependency, we use Pearson correla-tion coefﬁcient, Spearman correlation coefﬁcient, and KendallRank correlation coefﬁcient as the baseline. Particularly, wecalculate correlation on the status series of a candidate depen-
dency pair (P,C ), denoted as corrp
(P,C )
status andcorrs(P,C )
status .
For the baselines, we directly use the implementation from the
Python package scipy. We map the correlation to [0,1]with
the function f(x)=(x+1)/2. The intensities of dependencies
are then produced in the same way as Equation 2.
3) Evaluation Metrics: We employ Cross Entropy (CE),
Mean Absolute Error (MAE), and Root Mean Squared ErrorTABLE III
PERFORMANCE COMPARISON OF DIFFERENT METHODS ON TWO
DA TASETS
Dataset MethodMetric
CE MAE RMSE
TTPearson 0.6872 0.3305 0.4388
Spearman 0.7512 0.3735 0.4697
Kendall 0.6464 0.3749 0.4577
AID 0.4562 0.3435 0.3859
IndustryPearson 0.6076 0.4524 0.4563
Spearman 0.6030 0.4501 0.4537
Kendall 0.6258 0.4636 0.4656
AID 0.3270 0.1751 0.3044
(RMSE), as calculated in Equation 3, to evaluate the effec-tiveness of AID in predicting the intensity of dependency.
CE=1
NN/summationdisplay
i=1−[yi·log(pi)+( 1−yi)·log(1−pi)]
MAE =/summationtextN
i=1|yi−pi|
n
RMSE =/radicalBigg/summationtextNi=1(yi−pi)2
N(3)
Speciﬁcally, cross entropy calculates the difference between
the probability distributions of the label and the prediction.
Mean absolute error and root mean squared error measuresthe absolute and squared error. Lower CE, MAE, and RMSEvalues indicate a better prediction.
4) Experimental Environments: We run the experiments on
the simulated dataset on a Linux server with Intel Xeon E5-2670 CPU @ 2.40GHZ and 128 GB RAM. The experimentson the industrial dataset run on a Laptop with Intel Core i7CPU @ 2.60 GHz and 16 GB RAM.
B. RQ1: How effective is AID in predicting the intensity of
dependency?
To study the effectiveness of AID, we compare its perfor-
mance with the baseline models on both the simulated datasetand the industrial dataset collected from Huawei Cloud. Forthe parameters of AID, we set the bin size τ=1minute ,
the estimated round trip time δ
rtt=0 . Specially, we set the
max time drift δd=1minute for the industrial dataset and
setδd=0 for the simulated dataset. We do this because the
simulated dataset is deployed in a single server, so the timedrift will not be a problem. In addition, we use moving averageto smoothen the status series for the baselines and our method.The outputs are scalar values ranging from 0to1. A larger
value indicates higher intensity. The overall performance isshown in Table III, where we mark the smallest loss for eachloss metric and dataset.
660AID achieves the best performance on the industrial dataset
and reduces the loss by 45.8%, 61.1%, and 33.2% in terms
of cross entropy, mean absolute error, and root mean squarederror. On the simulated dataset, AID achieves the best per-formance in terms of cross entropy and root mean squarederror. Pearson correlation coefﬁcient marginally outperformsAID on the simulated dataset. The improvement of AID on thesimulated dataset is smaller than that on the industrial dataset.This is because the benchmark for simulation did incorporatevery few fault tolerance mechanisms, making most of thedependencies strong. Moreover, since the service invocationsof the TT benchmark are very fast, the statuses of TT’sservices are relatively similar, making simple baselines andour approach perform similarly.
C. RQ2: What is the impact of different parameter settings?
Fig. 5. Prediction loss under different bin size τ.
Since the estimated round trip time δrttand the max time
driftδdare minuscule, we only study the impact of the bin size
τ. As the range of time of the simulated dataset is small, we
only study the impact of the bin size τin the industrial dataset.
In particular, we conduct experiments on with the bin size
τ∈[1,10](minutes) , and keep δrtt=0 andδd=1minute .
We did not set larger bin sizes because larger bin sizes result inmore coarse-grained sampling of the service status, which willadd difﬁculty to the similarity calculation in the subsequentDSW algorithm.
Figure 5 shows the prediction loss under different bin size τ.
The x-axis denotes the bin size and the y-axis shows the threeloss metrics. The results indicate that the impact of differentbin sizes in a reasonable range is small, but τ=1minute
gives the best performance on the industrial dataset.
D. RQ3: What is the impact of different similarity measures?
We further study the impact of different similarity measures
on both datasets. AID
DSW denotes AID that uses the pro-
posed DSW to measure the similarity between status series.
AIDDTW denotes AID that uses the DTW [30] to measure
the similarity. We keep the bin size τ=1minute and the
estimated round trip time δrtt=0 as usual. Similar to previous
experiments, we set the max time drift δd=1minute for the
industrial dataset and set δd=0 for the simulated dataset.TABLE IV
THE IMPACT OF DIFFERENT SIMILARITY MEASURES
Dataset
/Bin sizeMethodMetric
CE MAE RMSE
TT
/1minAIDDSW 0.4562 0.3435 0.3859
AIDDTW 0.4494 0.3467 0.3832
Industry
/1minAIDDSW 0.3270 0.1751 0.3044
AIDDTW 0.3584 0.1996 0.3169
Table IV shows the performance of AID DSW and AID DTW
on both datasets. On the industrial dataset, the proposed DSWalgorithm improves the performance, but on the simulateddataset, the performance is almost the same. This is probablybecause the duration of spans in the simulated dataset is toosmall so that the effect of directed warping is weak. The resultsimply that the proposed DSW algorithm works better in real-world cloud environments.
E. RQ4: How efﬁcient is AID?
The most time-consuming operations are the candidate
selection and service status generation steps because we have
to iterate over all the spans in the cloud system. Theoretically,the time complexities of the candidate selection and servicestatus generation steps are O(S), whereSis the number of
spans to process in the cloud system. In practice, the industrialdataset contains about 1.0×10
10spans, so we process it
with a distributed computing service in Huawei Cloud. Sincethe preprocessing is dynamically scheduled and mixed withother teams’ tasks, we do not count the time spent on it. Forthe intensity prediction step, the time complexity is O(kN
2),
whereN=T
τis the number of bins and kis proportional
to the warping window w. In practice, the intensity prediction
step takes 155 seconds on average to process two status series
both with 1440 bins on a laptop. Since the similarity calcula-
tion of different (P,C )pairs are independent, we could easily
parallelize the intensity prediction step to further improve thetime efﬁciency.
VI. C
ASE STUDY
In Huawei Cloud, AID has been successfully incorporated
into the dependency management system that serves hundredsto thousands of cloud services. Figure 6 illustrates the concep-tual workﬂow. AID processes trace logs and continuously up-dates the aggregated intensity in the dependency managementsystem. The reliability engineers will categorize the intensityinto different levels by referring to both the output of AIDand their domain expertise. Then the dependency managementsystem will provide reference to the engineers in optimizingdependencies and mitigating cascading failures.
A. Optimization of Dependencies
In a cloud system, service failures are inevitable, but we can
prevent the failures from affecting other services by optimizing
661Cloud 
ServicesDependency 
Management 
Center
Fault 
Injection
Engineers
End UserIntensity 
Prediction 
(AID) Manual 
Correction
Prevention 
/Mitigation 
of Failures
Fig. 6. The use case of AID.
improper dependencies. AID assists in the discovery of unnec-
essary strong dependency on critical cloud services. If a criticalcloud service depends on another service with high intensity,the dependency management system will remind the engineersto check whether the dependency is necessary. If the depen-dency is unnecessary, the development team has to reduce theintensity of the dependency to improve the robustness of thecritical cloud service. Since AID’s deployment, more than tenunnecessary dependencies of critical cloud services have beendiscovered by AID and optimized by the development team.
B. Mitigation of Cascading Failures
AID also assists in the mitigation of cascading failures.
During a cascading failure, AID can provide the latest intensity
of dependency to OCEs, so that they can diagnose servicefailures efﬁciently. In addition, when a cascading failureoccurs, OCEs can limit the trafﬁc to critical cloud services andrecover the dependencies marked as “strong” ﬁrst. By doingso, the service disruption can get under control. Once a criticalfailure occurs, the manually conﬁrmed “strong” dependencieswill be treated with high priority. We conduct ﬁeld interviewswith OCEs to collect feedback. Based on the feedback, wehave seen our method shedding light on reducing the impactof critical failures.
VII. D
ISCUSSION
A. Practical Usage and Perceived Limitations
1) Indirect Dependencies: In this work, we mainly consid-
ered direct dependencies, which is caused by direct service in-vocations. The proposed approach does not explicitly considerindirect dependencies through transitivity of service invocationbecause the intensity of indirect dependencies can be easilyinferred from direct dependencies. In practice, the intensityof indirect dependencies can be inferred by a “cascadingconduction mechanism” that if Aintensively depends on B
andBintensively depends on CthenAintensively depends on
C. The proposed approach also works well on dependenciescaused by circuit breakers as long as the circuit breakers worktransparently.
2) Extension of Service Status: In this paper, we only derive
three aspects of service invocations, i.e., number of invoca-
tions, duration of invocations, error of invocations. We utilized
them because they are part of the state-of-the-art tracingsystem. Other aspects like the content of invocation responsescan also be important to determine status. In practice, cloudproviders can incorporate additional information to extend therepresentation of service status in their own implementationof AID.
3) Limitations on Asynchronous Invocations: Although
modern tracing mechanisms can keep track of asynchronousinvocations, AID may suffer from inaccuracies when dealingwith asynchronous invocations. This is because the max timedriftδ
din Algorithm 1 is hard to estimate for asynchronous
service invocations. Furthermore, if the traces of synchronousand asynchronous invocations are mixed, AID may not workwell since the time drift of synchronous and asynchronousinvocations usually differs a lot. We leave this problem asfuture work.
B. Threat to V alidity
In this work, we identiﬁed the following major threats to
validity.
1) Labeling accuracy: In this paper, we propose to measure
the intensity of service dependency with AID. To evaluate
the practical usage of AID, we conduct experiments on asimulated dataset and an industrial dataset. As it is a newrelation between cloud services, manual labeling is neededfor the evaluation. The evaluation on the industrial datasetrequires engineers to manually inspect the dependencies andlabel the intensity of dependencies. Limited by the experienceof engineers, the label may not be 100% accurate. Thefast evolution of cloud services may also change their faulttolerance mechanism, resulting in inaccurate labels. However,the engineers we invited have rich domain knowledge andare in charge of the architecture design of the cloud systemof Huawei Cloud. They also discuss with each other whenthere are disagreements. Moreover, the labeled dependenciesare the core cloud services in Huawei Cloud, so the intensityof dependencies are stable during the data collection period.We believe the amount of inaccurate labels is small (if exists).Most importantly, our method is unsupervised, so inaccuratelabels will not affect the prediction results of the proposedmethod.
2) Insufﬁciency of the simulation: For the evaluation pur-
pose, we deploy an open-source microservice benchmark tosimulate a real cloud system. The benchmark only contains25 microservices, which is far below the number of cloudmicroservices in a real cloud. Additionally, the implementationof the open-source benchmark did not fully consider thefault tolerance, resulting in only one weak dependency in thesimulation. Hence, the simulated dataset may not exhibit somecommon attributes of a real cloud system. For example, theproportion of “strong” dependency in the simulated dataset istwice the proportion of “strong” dependency in the industrialdataset. However, the insufﬁciency of the simulation willnot hinder the practical usefulness of AID in the real cloudsystem. On the contrary, as we show in Section V, theproposed method works better on the industrial dataset. Theexperimental results on the simulated dataset only conﬁrm theinsufﬁciency of the simulation.
662VIII. R ELA TED WORK
A. Cloud Monitoring
Monitoring cloud services properly with low overhead is
the key to provide reliable services. Distributed Tracing, as
a means of monitoring distributed cloud services, has beenwidely studied in the literature. All the distributed tracingapproaches can be classiﬁed as intrusive tracing and non-intrusive tracing. Intrusive tracing requires modiﬁcation toapplication code either in run time or at compile time. Googleproposes Dapper [14] to help engineers understand systembehavior and reasoning about performance issues. It reducesthe tracing overhead by sampling and restricting the instru-mentation number. X-Trace [16] monitors and reconstructs thewhole request path from a client by modifying all the networkprotocols and embedding the tracing data to the packageheader.
Non-intrusive tracing approaches do not require code mod-
iﬁcation and usually have a lower overhead. Normally, theseapproaches leverage information like the system runtime logsand the source code to reconstruct the real event traces. Zhaoet. al. [31] propose lprof to reconstruct the execution ﬂow ofdistributed systems using the runtime log of these systems.lprof conducts static analysis on the call-graph of requestprocessing code of the system to attribute a log output to aclient request. Chow et. al. [32] also leverage system runtimelogs to conduct performance monitoring and analysis. Theypropose ¨UberTrace to reconstruct traces from the existing logs,
then use The Mystery Machine to construct a causal model
and conduct analyses. Stitch uses pattern matching on logs toreconstruct the hierarchical relationship of events in a system.Pensieve [33] automatically reconstructs a chain of causallydependent events that leads to a system failure exploiting thelog ﬁles and system bytecode.
B. Dependency Mining
Automatically discovering service dependencies is critical to
cloud system administration and maintenance. There are two
major types of dependency mining approaches, i.e., passivedependency mining and active ones. Passive dependency min-ing generates service dependency based purely on the runtimelogs or KPIs. Shah et. al. [34] propose to use RecurrentNeural Networks (RNNs) to analyze and extract dependenciesin KPIs and use the discovered dependencies to identify earlyindicators for a given performance metric, analyze laggedand temporal dependencies, and to improve forecast accuracy.EIDefrawy et. al. [35] use Transfer Entropy to passively minethe dependencies. Luo et. al. [25] apply log parsing andBayesian decision theory to estimate the direction of depen-dencies among services. They employ time delay consistencyto reduce false dependencies. Zand et. al. [36] construct aservice correlation graph based on network measures andextract dependencies using hypothesis-testing. They furthercompute an importance metric for network’s componentsto facilitate administration. CloudScout [3] employs PearsonProduct-moment Correction Coefﬁcient over machine-levelKPIs such as TCP/UDP connection numbers and CPU uti-lization to calculate the similarity between different services.The similarity measure is used to cluster different servicestogether and to conduct VM consolidation based on the serviceclusters. Unlike all these approaches that mostly use physicalmachine metrics to infer service dependencies, our methodis designed for the emerging microservice architecture andutilizes the trace logs that directly record service invocations.
Active dependency mining requires modiﬁcation to ser-
vices. Ma et. al. propose GMA T [2], which generates servicedependencies in the microservice architecture leveraging thereﬂection feature of Java and visualizes the dependencies toengineers. Rippler [26] extracts the dependencies by randomlyinjecting temporal perturbation patterns in request arrival tim-ings for different services and investigates the propagation ofthe patterns. Wang et. al. [37] constructs a service knowledgegraph using real-time measures, operational metadata, andbusiness features. They propose new metrics to measure thepopularity of services based on their dependencies. Novotnyet. al. [38] focus on mining dependencies on the highlydynamic mobile networks. They use local monitors to collectlocal views of dependencies and generate a global view ofdependency on demand.
IX. C
ONCLUSION
In this paper, we ﬁrst conduct a comprehensive empirical
study on the maintenance of AWS and Huawei Cloud. Weidentify the inefﬁciency in failure diagnosis and recoverywith the binary-valued dependencies and deﬁne the intensityof dependency for the ﬁrst time. To facilitate cloud main-tenance, we propose AID, the ﬁrst approach to predict theintensity of dependencies between cloud microservices. AIDﬁrst generates a set of candidate dependency pairs from thespans. AID then represents the status of each cloud servicewith a multivariate time series aggregated from the spans andcalculates the similarities between the statuses of the callerand callee of each candidate pair. Finally, AID aggregate thesimilarities to produce a uniﬁed value as the intensity of thedependency. For the evaluation, we collect and manually labela new dataset from an open-source microservice benchmarkand evaluate AID on it. Furthermore, we evaluate AID usingthe data of Huawei Cloud and showcase the practical usageof AID. Both the evaluation results and case studies show theefﬁciency and effectiveness of AID. In the future, we plan toincorporate more information from the traces and other servicelogs for more accurate predictions.
A
CKNOWLEDGMENT
The work was supported by Key-Area Research and
Development Program of Guangdong Province (No.2020B010165002) and the Research Grants Council ofthe Hong Kong Special Administrative Region, China(CUHK 14210920).
663REFERENCES
[1] Microsoft, “Microservices architecture style,” 2019. [On-
line]. Available: https://docs.microsoft.com/en-us/azure/architecture/
guide/architecture-styles/microservices
[2] S.-P . Ma, C.-Y . Fan, Y . Chuang, W.-T. Lee, S.-J. Lee, and N.-L. Hsueh,
“Using service dependency graph to analyze and test microservices,” in2018 IEEE 42nd Annual Computer Software and Applications Confer-ence (COMPSAC), vol. 02, 2018, pp. 81–86.
[3] J. Yin, X. Zhao, Y . Tang, C. Zhi, Z. Chen, and Z. Wu, “Cloudscout: A
non-intrusive approach to service dependency discovery,” IEEE Transac-
tions on Parallel and Distributed Systems, vol. 28, no. 5, pp. 1271–1284,2016.
[4] M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. H. Katz, A. Kon-
winski, G. Lee, D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia,“Above the clouds: A berkeley view of cloud computing,” EECSDepartment, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-28, Feb 2009.
[5] R. DeFauw, A. Chigani, and N. Harris, “It resilience
within aws cloud, part ii: Architecture and patterns,”2021. [Online]. Available: https://aws.amazon.com/blogs/architecture/it-resilience-within-aws-cloud-part-ii-architecture-and-patterns/
[6] D. L. Oppenheimer and D. A. Patterson, “Architecture and dependability
of large-scale internet services,” IEEE Internet Comput., vol. 6, no. 5,
pp. 41–49, 2002.
[7] M. Villamizar, O. Garc ´es, H. Castro, M. V erano, L. Salamanca, R. Casal-
las, and S. Gil, “Evaluating the monolithic and the microservice archi-tecture pattern to deploy web applications in the cloud,” in 2015 10th
Computing Colombian Conference (10CCC). IEEE, 2015, pp. 583–590.
[8] A. Balalaie, A. Heydarnoori, and P . Jamshidi, “Microservices architec-
ture enables devops: Migration to a cloud-native architecture,” IEEE
Softw., vol. 33, no. 3, pp. 42–52, 2016.
[9] Y . Wang, G. Li, Z. Wang, Y . Kang, Y . Zhou, H. Zhang, F. Gao, J. Sun,
L. Yang, P . Lee, Z. Xu, P . Zhao, B. Qiao, L. Li, X. Zhang, and Q. Lin,“Fast outage analysis of large-scale production clouds with servicecorrelation mining,” in 43rd IEEE/ACM International Conference on
Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021.IEEE, 2021, pp. 885–896.
[10] Y . Gan, Y . Zhang, K. Hu, D. Cheng, Y . He, M. Pancholi, and
C. Delimitrou, “Seer: Leveraging big data to navigate the complexityof performance debugging in cloud microservices,” in Proceedings of
the Twenty-F ourth International Conference on Architectural Supportfor Programming Languages and Operating Systems, ASPLOS 2019,Providence, RI, USA, April 13-17, 2019. ACM, 2019, pp. 19–33.
[11] P . Wang, J. Xu, M. Ma, W. Lin, D. Pan, Y . Wang, and P . Chen,
“Cloudranger: Root cause identiﬁcation for cloud native systems,” in18th IEEE/ACM International Symposium on Cluster , Cloud and GridComputing, CCGRID 2018, Washington, DC, USA, May 1-4, 2018.IEEE Computer Society, 2018, pp. 492–502.
[12] P . Chen, Y . Qi, P . Zheng, and D. Hou, “Causeinfer: Automatic and
distributed performance diagnosis with hierarchical causality graph inlarge distributed systems,” in 2014 IEEE Conference on Computer
Communications, INFOCOM 2014, Toronto, Canada, April 27 - May2, 2014. IEEE, 2014, pp. 1887–1895.
[13] J. Chen, X. He, Q. Lin, Y . Xu, H. Zhang, D. Hao, F. Gao, Z. Xu, Y . Dang,
and D. Zhang, “An empirical investigation of incident triage for onlineservice systems,” in Proceedings of the 41st International Conference on
Software Engineering: Software Engineering in Practice, ICSE (SEIP)2019, Montreal, QC, Canada, May 25-31, 2019. IEEE / ACM, 2019,pp. 111–120.
[14] B. H. Sigelman, L. A. Barroso, M. Burrows, P . Stephenson, M. Plakal,
D. Beaver, S. Jaspan, and C. Shanbhag, “Dapper, a large-scaledistributed systems tracing infrastructure,” Google, Inc., Tech. Rep.,2010. [Online]. Available: https://research.google.com/archive/papers/dapper-2010-1.pdf
[15] P . Barham, R. Isaacs, R. Mortier, and D. Narayanan, “Magpie: Online
modelling and performance-aware systems,” in 9th Workshop on Hot
Topics in Operating Systems (HotOS IX). USENIX Association, May2003.
[16] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and I. Stoica, “X-trace: A
pervasive network tracing framework,” in 4th Symposium on Networked
Systems Design and Implementation (NSDI 2007), April 11-13, 2007,Cambridge, Massachusetts, USA, Proceedings. USENIX, 2007.[17] OpenTracing, “The opentracing semantic speciﬁcation,” 2021. [Online].
Available: https://opentracing.io/docs/overview/spans/
[18] X. Zhou, X. Peng, T. Xie, J. Sun, C. Ji, W. Li, and D. Ding, “Fault
analysis and debugging of microservice systems: Industrial survey,benchmark system, and empirical study,” IEEE Trans. Software Eng.,
vol. 47, no. 2, pp. 243–260, 2021.
[19] A. Fakhrazari and H. V akilzadian, “A survey on time series data mining,”
inIEEE International Conference on Electro Information Technology,
EIT 2017, Lincoln, NE, USA, May 14-17, 2017. IEEE, 2017, pp. 476–481.
[20] H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization
for spoken word recognition,” IEEE transactions on acoustics, speech,
and signal processing, vol. 26, no. 1, pp. 43–49, 1978.
[21] D. J. Berndt and J. Clifford, “Using dynamic time warping to ﬁnd
patterns in time series,” in Knowledge Discovery in Databases: Papers
from the 1994 AAAI Workshop, Seattle, Washington, USA, July 1994.Technical Report WS-94-03. AAAI Press, 1994, pp. 359–370.
[22] V . Niennattrakul and C. A. Ratanamahatana, “On clustering multimedia
time series data using k-means and dynamic time warping,” in 2007
International Conference on Multimedia and Ubiquitous Engineering(MUE 2007), 26-28 April 2007, Seoul, Korea. IEEE Computer Society,2007, pp. 733–738.
[23] A. Mueen, H. Hamooni, and T. Estrada, “Time series join on subse-
quence correlation,” in 2014 IEEE International Conference on Data
Mining, ICDM 2014, Shenzhen, China, December 14-17, 2014 . IEEE
Computer Society, 2014, pp. 450–459.
[24] Z. Chen, Y . Kang, L. Li, X. Zhang, H. Zhang, H. Xu, Y . Zhou, L. Yang,
J. Sun, Z. Xu, Y . Dang, F. Gao, P . Zhao, B. Qiao, Q. Lin, D. Zhang,and M. R. Lyu, “Towards intelligent incident management: why we needit and how we make it,” in ESEC/FSE ’20: 28th ACM Joint European
Software Engineering Conference and Symposium on the F oundationsof Software Engineering, Virtual Event, USA, November 8-13, 2020.ACM, 2020, pp. 1487–1497.
[25] J.-G. Lou, Q. Fu, Y . Wang, and J. Li, “Mining dependency in distributed
systems through unstructured logs analysis,” ACM SIGOPS Operating
Systems Review, vol. 44, no. 1, pp. 91–96, 2010.
[26] A. Zand, G. Vigna, R. Kemmerer, and C. Kruegel, “Rippler: Delay
injection for service dependency detection,” in IEEE INFOCOM 2014-
IEEE Conference on Computer Communications. IEEE, 2014, pp.2157–2165.
[27] A. B. M. B. Alam, A. Haque, and M. Zulkernine, “CREM: A cloud
reliability evaluation model,” in IEEE Global Communications Confer-
ence, GLOBECOM 2018, Abu Dhabi, United Arab Emirates, December9-13, 2018. IEEE, 2018, pp. 1–6.
[28] OpenTracing, “Opentracing spring cloud,” 2021. [Online]. Available:
https://github.com/opentracing-contrib/java-spring-cloud
[29] G. Aceto, A. Botta, W. De Donato, and A. Pescap `e, “Cloud monitoring:
A survey,” Computer Networks, vol. 57, no. 9, pp. 2093–2115, 2013.
[30] E. J. Keogh, “Exact indexing of dynamic time warping,” in Proceedings
of 28th International Conference on V ery Large Data Bases, VLDB 2002,Hong Kong, August 20-23, 2002. Morgan Kaufmann, 2002, pp. 406–417.
[31] X. Zhao, Y . Zhang, D. Lion, M. F. Ullah, Y . Luo, D. Y uan, and
M. Stumm, “lprof: A non-intrusive request ﬂow proﬁler for distributedsystems,” in 11th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 14). USENIX Association, Oct. 2014, pp.629–644.
[32] M. Chow, D. Meisner, J. Flinn, D. Peek, and T. F. Wenisch, “The mys-
tery machine: End-to-end performance analysis of large-scale internetservices,” in 11th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 14). USENIX Association, Oct. 2014, pp.217–231.
[33] Y . Zhang, S. Makarov, X. Ren, D. Lion, and D. Y uan, “Pensieve: Non-
intrusive failure reproduction for distributed systems using the eventchaining approach,” in Proceedings of the 26th Symposium on Operating
Systems Principles, 2017, pp. 19–33.
[34] S. Y . Shah, Z. Y uan, S. Lu, and P . Zerfos, “Dependency analysis of
cloud applications for performance monitoring using recurrent neuralnetworks,” in 2017 IEEE International Conference on Big Data (Big
Data). IEEE, 2017, pp. 1534–1543.
[35] K. EIDefrawy, T. Kim, and P . Sylla, “Automated inference of dependen-
cies of network services and applications via transfer entropy,” in 2016
IEEE 40th Annual Computer Software and Applications Conference(COMPSAC), vol. 2. IEEE, 2016, pp. 32–37.
664[36] A. Zand, A. Houmansadr, G. Vigna, R. Kemmerer, and C. Kruegel,
“Know your achilles’ heel: Automatic detection of network critical
services,” in Proceedings of the 31st Annual Computer Security Ap-
plications Conference, 2015, pp. 41–50.
[37] H. Wang, C. Shah, P . Sathaye, A. Nahata, and S. Katariya, “Service
application knowledge graph and dependency system,” in 2019 34th
IEEE/ACM International Conference on Automated Software Engineer-ing Workshop (ASEW). IEEE, 2019, pp. 134–136.
[38] P . Novotny, B. J. Ko, and A. L. Wolf, “On-demand discovery of software
service dependencies in manets,” IEEE Transactions on Network and
Service Management, vol. 12, no. 2, pp. 278–292, 2015.
665