CURE: C ode-Aware Neu ral Machine Translation
for Automatic Program Re pair
Nan Jiang
Purdue University
West Lafayette, USA
jiang719@purdue.eduThibaud Lutellier
University of Waterloo
Waterloo, Canada
tlutelli@uwaterloo.caLin Tan
Purdue University
West Lafayette, USA
lintan@purdue.edu
Abstract ‚ÄîAutomatic program repair (APR) is crucial to im-
prove software reliability. Recently, neural machine translation
(NMT) techniques have been used to Ô¨Åx software bugs auto-
matically. While promising, these approaches have two major
limitations. Their search space often does not contain the correct
Ô¨Åx, and their search strategy ignores software knowledge such as
strict code syntax. Due to these limitations, existing NMT-based
techniques underperform the best template-based approaches.
We propose CURE, a new NMT-based APR technique with
three major novelties. First, CURE pre-trains a programming
language (PL) model on a large software codebase to learn
developer-like source code before the APR task. Second, CURE
designs a new code-aware search strategy that Ô¨Ånds more correct
Ô¨Åxes by focusing on compilable patches and patches that are
close in length to the buggy code. Finally, CURE uses a subword
tokenization technique to generate a smaller search space that
contains more correct Ô¨Åxes.
Our evaluation on two widely-used benchmarks shows that
CURE correctly Ô¨Åxes 57 Defects4J bugs and 26 QuixBugs bugs,
outperforming all existing APR techniques on both benchmarks.
Index Terms ‚Äîautomatic program repair, software reliability
I. I NTRODUCTION
Automatic program repair is crucial to reduce manual software
debugging efforts [1]‚Äì[24]. There has been recent adoption
of neural machine translation, a widely used technique for
natural language processing (NLP) tasks, to generate correct
code automatically given buggy source code [18]‚Äì[25]. Thanks
to the strong learning capabilities of NMT models, NMT-
based APR techniques have outperformed most existing rule-
based approaches [18]‚Äì[20]. NMT models use deep-learning
techniques to encode buggy source code as intermediate repre-
sentation in the latent space automatically, and then decode the
encoded representation into target correct code. By minimizing
the loss function and updating the weight parameters, NMT
models learn to capture the hidden relationship between buggy
code and correct code without any manual design of Ô¨Åx
patterns or feature templates.
For a search-based APR approach (including NMT-based
techniques) to generate a correct Ô¨Åx, it needs to satisfy two
conditions: (1) the correct Ô¨Åx must be in the search space ,
which is the set of all patches that the APR approach can
generate, and (2) the search strategy must be effective to
Ô¨Ånd the correct Ô¨Åx in a reasonable amount of time. Given
that a correct patch is in the search space, it is desirable
that the search space is small, so that it is easier to Ô¨Ånd the
Fig. 1. Uncompilable patches generated by NMT-based models, and their
ranks, for a bug in QuixBugs. The line in yellow background (starting with
‚Äò-‚Äô) is the buggy line. The line in green background (starting with ‚Äò +‚Äô) is the
correct Ô¨Åx. The red code in generated patches disobeys Java syntax.
correct patch [26]. Despite being among the most effective
APR approaches, NMT-based approaches still fail to Ô¨Åx many
bugs [18]‚Äì[20].
Compared to natural language text, source code has its own
characteristics such as a strict syntax, code semantics, and an
inÔ¨Ånite number of possible identiÔ¨Åers. These characteristics
impose unique challenges for NMT models to Ô¨Åx bugs auto-
matically.
First, the strict syntax of source code is hard for NMT mod-
els to learn. A major reason is that existing techniques [18]‚Äì
[20], [27] learn from buggy code snippets and the corre-
sponding Ô¨Åxed correct code snippets (typically a few lines
to tens of lines per bug), and do not use the entire source
code repositories (typically millions of lines of code per
project). Thus, existing NMT-based APR approaches have
limited knowledge about the rigorous syntax of programming
languages and the big picture of how developers write code.
The missed opportunities are twofold: (1) existing techniques
fail to take advantage of the large amount of available source
code, and (2) they see partial code snippets only (which alone
are often syntactically incorrect), and miss the big picture of
complete methods, classes, and projects. For example, for the
Ô¨Åx of replacing ‚Äúwhile (x) f‚Äùwith ‚Äúwhile (y) f‚Äù, the
open bracket ‚Äúf‚Äùis syntactically incorrect in this code snippet,
i.e., missing the closing bracket ‚Äúg‚Äù.
Such ineffectiveness is evident as demonstrated by data. For
example, up to 67%‚Äì97% of patches generated by the state-of-
the-art NMT-based APR models [19], [20] are uncompilable,
wasting valuable resources on incorrect patches. Figure 1
shows a bug in QuixBugs and some of the top-ranked patches
generated by CoCoNuT [19]. All of these patches are un-
compilable, because they call methods with wrong parameters,
invoke undeclared variables, or contain mismatched parenthe-
11612021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ¬©2021 IEEE
DOI 10.1109/ICSE43902.2021.00107
sis. One important reason that CoCoNuT fails to generate a
correct patch for this bug despite generating 20,000 patches,
is the large number of uncompilable patches. The code-aware
NMT-based approach we propose automatically generates a
correct patch (identical to the +line highlighted in green)
for this bug. The ranks of these uncompilable patches are
high because existing NMT-based APR techniques focus on
translating buggy code snippets to correct code snippets, which
are partial code segments instead of full methods or programs.
Since they fail to see the whole picture of the entire program
or programming languages, they generate many patches with
syntax errors.
Failing to learn how developers write code, existing
NMT-based APR techniques also generate compilable but
obviously-incorrect patches, as they do not look like
developer-written code. These uncompilable and compilable-
but-incorrect patches decrease the accuracy and efÔ¨Åciency of
APR models, preventing APR models from generating more
correct patches faster.
Second, the inÔ¨Ånite number of possible identiÔ¨Åers causes
NMT techniques for code to handle an enormous vocabulary if
using word-level tokenization, where a vocabulary contains all
the unique tokens that an NMT model recognizes. Considering
the complexity of NMT architectures, it is computationally too
expensive for NMT-based APR models to use an enormous
vocabulary. Yet with a limited vocabulary size, their search
spaces do not contain all correct Ô¨Åxes. SequenceR [20] uses
a small vocabulary and shirks this complexity to a later
reconstruction stage, while CoCoNuT [19] uses a vocabulary
of more than 130,000 tokens but still suffers from the out-
of-vocabulary (OOV , i.e., an NMT model cannot recognize or
generate a token) problem, resulting in its search space that
still misses correct Ô¨Åxes.
A. Our approach
Thus, we propose an NMT-based approach that is specially
designed to parse, analyze, model, and search source code (as
opposed to natural language text) to Ô¨Åx bugs automatically.
Our approach, CURE, not only improves the search space (a
smaller search space containing more correct patches) but also
uses a more effective search strategy to Ô¨Ånd and rank correct
patches higher, which are achieved through the following three
main techniques that we design and use:
(1) Programming language models: To help NMT models
learn developer-like source code (i.e., not only compilable
but also similar to those written by programmers), we apply
the pre-training and Ô¨Åne-tuning workÔ¨Çow to the APR task.
SpeciÔ¨Åcally, pre-trained language models have brought great
improvement to many NLP tasks [28], [29]. They learn the
probability distribution over sequences of words from a large
amount of natural language text. Then one Ô¨Åne-tunes the pre-
trained language model for a speciÔ¨Åc task by adding an extra
model to it (e.g., adding a classiÔ¨Åer for classiÔ¨Åcation tasks).
The language model provides vectorized representations of
input sequences to the model added to it. Since a pre-trained
language model is typically trained on a larger dataset (since itis unsupervised learning and does not require ground truth), it
offers the added model more information regarding sentence
structures (e.g., syntax) and about what human-like text are
(e.g., readability), which improves the quality of the generated
text of the Ô¨Åne-tuned model for the speciÔ¨Åc task signiÔ¨Åcantly.
Given the effectiveness of language models in the NLP
domain, we propose to add a language model pre-trained
on software code, referred to as programming language (PL)
model , to an NMT architecture to create a new APR archi-
tecture. The PL model is trained to predict the next tokens
in code sequences and learns to generate developer-like code.
Then, we combine the PL model and the NMT model to form
the full APR model and Ô¨Åne-tune it for APR tasks.
Our PL-enhanced NMT approach ranks correct patches
higher in the search space to Ô¨Åx more bugs (Section V-B1).
(2) Code-aware search strategy: When using an NMT model
to generate a sequence of tokens to form a patch, ideally,
one prefers the sequence with the highest score, e.g., average
log probability of every token in sequence. Since this is
prohibitively expensive [30], in practice, one uses a search
strategy to choose proper tokens at each step. Beam search
is a common search strategy for NMT that keeps the most n
probable sequences at each step, where nis the beam size .
The beam size of NLP tasks is typically 5 to 20 [30],
[31]. Since source code has more possible identiÔ¨Åers and a
bigger search space than natural languages [19], [20], the NMT
models for APR usually require larger beam sizes (100 [20] to
1,000 [19]) to generate enough candidate patches. However,
with large beam sizes, the vanilla beam search chooses many
bad patches, either uncompilable or far from correct in length.
To address this challenge, we propose two solutions: valid-
identiÔ¨Åer-check strategy and length-control strategy. First,
since source code is a formal language, only valid tokens
are allowed, including keywords and variables in scope. In-
valid tokens make a patched program uncompilable, let alone
capable of passing test cases. Therefore, we propose and
design a valid-identiÔ¨Åer-check strategy to improve the vanilla
beam search, which performs static analysis to identify all
valid identiÔ¨Åers and then forces beam search to generate only
sequences with valid tokens.
Second, with a large beam size, beam search Ô¨Ånds many
very short sequences such as ‚Äú f‚Äù and ‚Äú tryf‚Äù, which are
incorrect code snippets to Ô¨Åx bugs. Since correct Ô¨Åxes in our
training data are typically of similar length to the buggy lines,
we use a length-control strategy to punish too-short and too-
long sequences to prompt CURE to generate patches of a
similar length to the buggy line.
Our code-aware beam-search strategy Ô¨Ånds more correct
Ô¨Åxes by generating more compilable patches and patches of
similar length to the buggy lines. (Section V-B2).
(3) Subword tokenization: The enhanced word-level tok-
enization proposed by CoCoNuT [19] reduces the vocabulary
size of code, by using camel letters, underscores, and numbers
to split long identiÔ¨Åers. However, many compound words
(such as ‚Äú binsearch ‚Äù for binary search) do not contain
1162these special characters. The previous parsing approach keeps
‚Äúbinsearch ‚Äù as one word, which is OOV , instead of break-
ing it into ‚Äú bin‚Äù and ‚Äú search ‚Äù, both of which are in the
vocabulary. Thus, we use byte-pair encoding (BPE), a type of
subword tokenization techniques, to tokenize compound words
and rare words to further address the OOV problem.
BPE improves the search space by both including more
correct patches and reducing its size (Section V-B3).
B. Contributions
We design and implement a code-aware NMT-based technique,
CURE , to Ô¨Åx bugs automatically. Our contributions include:
An approach to pre-train a PL model for APR on a very
large software codebase‚Äî4.04 million methods from
1,700 open-source projects‚Äîto capture code syntax and
developer-like source code,
A new APR architecture that combines a pre-trained PL
model and NMT architectures to learn both code syntax
and Ô¨Åx patterns,
A new code-aware beam-search strategy, which uses
valid-identiÔ¨Åer-check and length-control strategies to Ô¨Ånd
more correct Ô¨Åxes,
A new application of subword tokenization to the APR
task, which addresses the OOV problem effectively, and
A new APR approach, CURE, that combines the tech-
niques above, and its evaluation on two widely-used
benchmarks‚ÄîDefects4J and QuixBugs, where CURE
Ô¨Åxes the most number of bugs, 57 and 26 bugs respec-
tively, outperforming all existing APR tools. CURE is the
Ô¨Årst NMT-based approach that outperforms all state-of-
the-art APR approaches on Defects4J.
Availability: Data is available in a GitHub repository1.
II. B ACKGROUND
Candidate, Plausible and Correct Patches: Following pre-
vious work [19], we call patches generated by the models
candidate patches. Patches that pass the validation stage are
plausible, and patches identical or semantically equivalent to
developers‚Äô patches are called correct patches.
Parameters and Hyperparameters: Parameters are the
weights between the connections of the network. These param-
eters are optimized during the training phase. Hyperparameters
are arguments of the network deÔ¨Åned before the training
process. They generally include layer dimensions, number of
layers, and optimization parameters.
Pre-Training and Fine-Tuning: Pre-training is the process of
training a model for a general task (e.g., next word prediction)
with a very large dataset. After pre-training, one gets a pre-
trained model with updated parameters. A pre-trained model
can be Ô¨Åne-tuned for a similar but speciÔ¨Åc task (e.g., text
generation) with few training data. During Ô¨Åne-tuning, usually
one needs to add extra models to the pre-trained model to Ô¨Åt
1https://github.com/lin-tan/CUREthe task, and the parameters of both the pre-trained model and
added models are updated.
Context-aware neural machine translation (CoNuT) archi-
tecture: We use CoNuT as our NMT architecture in this paper.
CoNuT consists of a buggy lines encoder, a context encoder, a
merger, a decoder, an attention module, and a token generation
module, where the encoders and decoder are implemented
with convolutional sequence-to-sequence architecture [32].
The details of CoNuT is described in [19]. CoNuT has shown
good results for APR, and convolutional architecture can be
stacked to capture hierarchical features and long dependencies
for larger contexts [19].
III. A PPROACH
To address the challenges described in the Introduction, we
design and apply three novel techniques, i.e., subword tok-
enization to improve the search space (Section III-C), a pro-
gramming language model to learn developer-like source code
and improve patch ranking (Section III-D and Section III-E),
and a new code-aware beam-search strategy (Section III-G)
to improve patch ranking and generate more correct patches.
A. Overview
Figure 2 presents an overview of our approach. CURE consists
of three stages: training, inference, and validation. During
the training stage, CURE extracts methods from open-source
projects, referred to as PL training data , and tokenizes them
(step 1ain Figure 2). Different from previous work [18]‚Äì
[24], we use subword tokenization , which produces a smaller
but more accurate search space that contains more correct
patches. CURE uses these tokenized methods to train a
new programming language model that learns developer-
like source code with correct syntax (step 2). CURE also
tokenizes the buggy lines, context, and correct Ô¨Åxes extracted
from the commit history of open-source projects, referred to
aspatch training data , into sequences of tokens (step 1b). We
use these sequences to Ô¨Åne-tune an ensemble of kAPR models
(step 3). Each APR model combines the PL model with
a context-aware neural machine translation (CoNuT) model
[19].
During the inference stage, a user provides a buggy project
along with the location of buggy lines to CURE. These are
standard input that existing APR tools require [1], [5], [19],
[20], [33], [34]. CURE tokenizes the buggy and the context
lines (step 1c), then analyzes the source code to extract
a list of valid identiÔ¨Åers that are in scope of the buggy
lines (step 4). The patch generation module generates a
list of candidate patches using a new code-aware beam-
search strategy (step 5). This new algorithm discards many
irrelevant patches on the Ô¨Çy (i.e., as soon as an invalid token
is generated) and penalizes patches that are unlikely to be
correct (e.g., Ô¨Åxes that are very different from the buggy line
in length), which saves a lot of resources and allows CURE
to search deeper for correct patches.
In the validation stage, CURE validates candidate patches
by compiling and executing the test suites of the patched
1163Fig. 2. Overview of CURE. Grey boxes represent major novelties of CURE. Circled numbers indicate the steps of generating patches with CURE.
projects. CURE outputs a list of plausible patches (step 6)
for developers to examine.
B. Data Extraction
CURE uses two different types of training data. First, the
GPT PL model is trained on millions of methods extracted
from open-source Java projects. Second, CURE Ô¨Åne-tunes the
PL model for the APR task. This step requires APR speciÔ¨Åc
training data (i.e., buggy lines, context, and correct Ô¨Åxes).
We use CoCoNuT‚Äôs training data shared on GitHub [19].
CoCoNuT‚Äôs authors extracted this dataset from open-source
repositories and identiÔ¨Åed buggy commits based on keywords
in commit messages (‚ÄúÔ¨Åx,‚Äù ‚Äúbug,‚Äù and ‚Äúpatch‚Äù). They also
cleaned the dataset using commit message anti-patterns (‚Äúre-
name,‚Äù ‚Äúclean up,‚Äù ‚Äúrefactor,‚Äù ‚Äúmerge,‚Äù ‚Äúmisspelling,‚Äù and
‚Äúcompiler warning‚Äù). Similar to CoCoNuT, we use the method
surrounding the buggy lines as context.
C. Code Representation and Tokenization
Word-level tokenization: To tokenize buggy-, context-, and
Ô¨Åxed-lines to token sequences, CURE Ô¨Årst uses enhanced
word-level tokenization [19] to separate code lines by spaces,
camel letters, underscores, strings, and numbers (except 0 and
1).
Out-of-vocabulary Issue: The vocabulary size after the word-
level tokenization is larger than what is commonly used
in NLP and the test set still contains 2% of OOV tokens.
Excluding rare tokens is problematic for source code because
OOV tokens are likely to be important project-speciÔ¨Åc tokens.
Excluding such tokens makes it difÔ¨Åcult for NMT models to
Ô¨Åx bugs in these new projects.
Some existing NMT-based APR models [19] do not gen-
erate OOV tokens, missing the opportunity to Ô¨Åx more bugs.
SequenceR uses a special token as a placeholder for OOV
tokens, and then uses a copy mechanism to reconstruct them.
The copy mechanism replaces the placeholder tokens with the
most likely token from the input buggy lines. However, this
solution would fail to generate some patches, since it can only
copy tokens appearing in the buggy lines.
(a) Buggy line and correct Ô¨Åx of Closure 62.
(b) Word-level tokenization result of buggy line and correct Ô¨Åx.
(c) Subword-level tokenization result of buggy line and correct Ô¨Åx.
Fig. 3. Tokenized results that use word-level tokenization and subword
tokenization of Closure 62 in Defects4J.
Subword tokenization: To address the OOV problem and
reduce the vocabulary size, we use byte pair encoding (BPE) ,
which is an unsupervised learning algorithm to Ô¨Ånd the most
frequent subwords in a corpus by merging the most frequent
byte pair iteratively [35]. BPE has been used in many NLP
tasks and is useful to reduce vocabulary size and mitigate the
OOV problem efÔ¨Åciently [35]‚Äì[37].
Figure 3 shows an example from the inference stage demon-
strating the effectiveness of the subword tokenization. Lines
starting with ‚Äò -‚Äô are the buggy lines (input) and those starting
with ‚Äò +‚Äô are the correct Ô¨Åxes. Figure 3(a) shows the source
code of a real bug in Defects4J [38], while Figure 3(b) shows
the code after using the enhanced word-level tokenization.
Figure 3(c) shows the same code tokenized by our subword
tokenization. In Figure 3, each consequence separated by space
is a token excluding the ‚Äò -‚Äô and ‚Äò +‚Äô signs.
When using only the enhanced word-level tokenization,
the variable ‚Äú charno ‚Äù is an OOV token. Thus, CoCoNuT
and SequenceR fail to Ô¨Åx this bug since CoCoNuT cannot
generate OOV tokens and SequenceR does not Ô¨Åx it correctly
with the copy mechanism. With our subword tokenization,
‚Äúcharno ‚Äù is split into two tokens, both of which appear
in the vocabulary‚Äî‚Äú char@@ ‚Äù (‚Äú@@‚Äù indicates that the token
needs to be concatenated with the following token) and ‚Äú no‚Äù,
enabling CURE to generate a correct patch for this bug.
1164By applying subword tokenization, we use a smaller vocab-
ulary to form a smaller but better search space that contains
more correct patches. Section V-B3 evaluates the impact of
our subword tokenization approach.
D. Programming Language Model
To address the challenges of learning developer-like source
code, we train a language model on open-source programs,
referred to as a programming language model (PL model) . A
PL model optimizes the probability of a sequence of tokens
being a real-world code snippet. We use Generative Pre-trained
Transformer (GPT) [37] for PL modeling because GPT has
been shown to improve the performance of many different
NLP tasks [37], [39].
Pre-training a PL model allows for separating programming
language learning from patch learning. The advantages are
twofold. First, GPT learning is unsupervised and only requires
complete methods; therefore one can extract a large amount
of data automatically and accurately to train it. Second,
during Ô¨Åne-tuning, the APR model already knows the PL
syntax (thanks to the PL model), making the Ô¨Åne-tuning more
efÔ¨Åcient.
Given a sequence of tokens representing a method, x=
(x1;:::;x n), wherexiis a token in the method sequence x, the
PL modeling objective is to maximize the average likelihood:
LGPT(x) =1
nn
i=1logP(xijx1;:::;x i 1; ) (1)
where represents matrices of trainable weights of the PL
model.P(xijx1;:::;x i 1; ) is the conditional probability of
tokenxibeing the next token, given a sequence of x1;:::;x i 1,
which is calculated by the PL model with weights . At a
high-level, the objective of the PL model training is to Ô¨Ånd
the best weights ( ) so that sequences of tokens x1;:::;x n
representing real methods in projects obtain a higher LGPT
score than other sequences. Since methods in popular open-
source projects are dominantly well-formed correct blocks
of code, we use them to train our PL model to learn if a
given sequence of tokens is likely to form real-world code
(compilable and looks like written by programmers).
E. Fine-Tuning for APR with a PL Model
After pre-training the PL model, CURE Ô¨Åne-tunes the GPT
PL model for the APR task by combining it with an NMT
model as the APR model. We use the CoNuT (Section II) as
CURE‚Äôs NMT architecture.
The APR model takes buggy lines and their context as input
and aims to generate a correct patch. During the Ô¨Åne-tuning
process, the APR model is trained to learn the transformation
from the buggy lines and context (e.g., the buggy method)
to the correct Ô¨Åx. We use xb= (xb1;:::;x bn)to denote
the buggy lines, x= (x1;:::xcn)to denote the context, and
y= (y1;:::;y fn)to denote the correct Ô¨Åxes, where b1;:::;b n
are the indices of the buggy lines in the context, while cnand
fnare the lengths of the context and correct Ô¨Åxes respectively.
Fig. 4. Architecture of the APR models used in CURE. Yellow, purple and
green boxes refer to the buggy lines, context and the generated patch.
We denote the weights of the PL model as and weights
of CoNuT as . The APR model is Ô¨Åne-tuned by updating 
andto maximize the following average log-likelihood:
LNMT (x;xb;y) =1
fnfn
i=1logP(yijx;xb;y0;:::;y i 1; ;)
y0=xb1 1
(2)
P(yijx;xb;y0;:::;y i 1; ;)is the conditional probability
calculated by the APR model with weights and, where
yiis the token following the sequence (y0;y1;:::;y i 1)in the
correct Ô¨Åx, given the buggy lines xband context x. For the
Ô¨Årst token in the correct Ô¨Åx, the probability is the conditional
probability of token y1giveny0, wherey0is the token right
before the correct Ô¨Åx, i.e., xb1 1. For example, the entire
method ‚Äú kth() ‚Äù is the context in Figure 1, while the buggy
lines and the correct Ô¨Åxes start at the same index in the context,
andy0is the tokenfright before the ‚Äú return ‚Äù statement.
To prevent the PL model from losing the information it
learned during pre-training, we include the language mod-
eling (i.e., LGPT) as an auxiliary objective to the Ô¨Åne-
tuning process. It also improves the generalization of the Ô¨Åne-
tuned model [37]. Therefore, the APR model is Ô¨Åne-tuned by
maximizing the combined average log-likelihood:
LAPR(x;xb;y) =LNMT (x;xb;y) +LGPT(y0)
y0= (x1;x2;:::;x b1 1;y)(3)
where y0is the token sequence from the beginning of
the buggy method to the last token in the correct Ô¨Åx
(x1;x2;:::;x b1 1is the preÔ¨Åx of xbefore xb). Probability
LGPT(y0)is the likelihood of y0being a real source code
snippet, while is a hyperparameter referring to the coefÔ¨Åcient
ofLGPT in the combined log-likelihood LAPR.
The Ô¨Åne-tuning stage aims to Ô¨Ånd the best set of parameters
andto maximize LAPR for all buggy lines, context, and
correct Ô¨Åxes in the training data.
1165In the training mode, the APR model takes the pre-trained
GPT module (the PL model) and the patch training data as
input. The patch training data consists of the buggy lines, the
context, and the correct Ô¨Åxes. We train the APR model for
multiple epochs (i.e., multiple passes on the training data) to
obtain the best combination of weights and.
In the inference mode, the APR model has access to only the
buggy lines and their context and outputs a sequence of tokens
representing the patch. Figure 4 shows a simpliÔ¨Åed view of the
architecture of our combined APR model and how the model
is used in inference mode. Our APR model consists of two
components: a PL model (GPT) and an NMT model (CoNuT).
First, CURE generates the GPT representation of the context
lines (step 1in Figure 4). As explained in Section III-D, the
GPT model was trained on complete methods, therefore the
input of the GPT model needs to be a method. If we directly
feed the Ô¨Årst token of the buggy line to the GPT model (‚Äú int‚Äù
in Figure 4), the GPT model will generate a bad embedding
for it since it expects the Ô¨Årst token of a sequence to be the
Ô¨Årst token of a method (e.g., ‚Äú public ‚Äù).
Hence, the GPT model generates an embedding for all
tokens in the buggy method. The CoNuT model contains two
encoders. The buggy lines encoder only takes the represen-
tation of the buggy line as input. Therefore, CURE extracts
the subsequence that corresponds to the buggy line embedding
from the buggy method embedding (yellow boxes in Figure 4)
and forwards it to the buggy lines encoder (step 2a). The
second encoder is for the context and takes the embedding of
the entire buggy method (purple boxes in Figure 4) as input
(step 2b). CURE merges the output of the two encoders (step
3) before sending it to the attention mechanism and the token
generator.
To start generating tokens, the attention mechanism and the
token generator need the encoder‚Äôs and the decoder‚Äôs output.
At the start of the inference, none of the Ô¨Åxed tokens have been
generated yet. CoCoNuT started the decoding sequence with
an initial ‚Äú <START> ‚Äù token. However, it is better to initialize
the sequence with the last token of the context before the
buggy line to provide additional contextual information. To
obtain the embedding of this token, we pass the context before
the buggy line to the GPT model (step 4) and then feed the
embedding of the last token (‚Äú f‚Äù) to the decoder (step 5).
The decoder generates a representation of the token, which is
forwarded to the attention mechanism (step 6).
The attention mechanism combines the output of the two
encoders and the output of the decoder to form the attention
map between the last token (‚Äú f‚Äù in the example) and the buggy
method. Then, the token generation outputs the Ô¨Årst token of
the Ô¨Åxed sequence (‚Äú double ‚Äù in step 8). This token is then
appended to the decoder input (step 9). Then, the decoder
starts the next iteration (steps 10to15) with the input ‚Äú f
double‚Äô‚Äô and generates the token ‚Äú sum‚Äù. This iterative
process continues until the end-of-sequence token ‚Äú <EOS> ‚Äù
is generated.
Fig. 5. An example of extracting mappings between preÔ¨Åxes and valid next
tokens from buggy projects. Line with yellow background is buggy line.
F . Ensemble Learning
Prior work [19] shows that ensemble learning, i.e., combining
multiple models, enables NMT-based APR approaches to Ô¨Åx
more bugs: the number of bugs correctly Ô¨Åxed rises from 22
to 44 when the number of models increases from 1 to 20.
Therefore, we combine (1) models with different hyperparam-
eters and (2) models with two different architectures (CoNuT
and FConv [32]) for our ensemble learning. The GPT PL
model is general as it represents the entire PL. Thus, each
APR model starts with the same PL model, Ô¨Åne-tunes it, and
combines it with CoNuT or FConv architectures that have
different hyperparameters (step 3of Figure 2).
Balancing the computation cost and tuning effectiveness,
we use random search to pick different hyperparameter values
(e.g., number of convolution layers, convolution dimensions,
and dropout) in a reasonable range and tune each model
for one epoch. Based on each model‚Äôs perplexity (i.e., a
measurement of how well a model predicts an instance) on
our validation data, we choose the top kmodels for ensemble
learning and keep training them until convergence.
G. Code-Aware Beam-Search Strategy and Patch Generation
The goal of patch generation is to generate the sequence with
the highest probability given the buggy line and its context.
The APR model generates one token with its probability at a
time. Searching for the sequence with the highest probability
is exponential in the length of the output sequence. Thus, we
need an effective search strategy to Ô¨Ånd a sequence with a
high probability.
Beam search is an optimized greedy strategy and the most
common search strategy used for NMT. Beam search keeps
only then(nis beam size, a hyperparameter of beam search)
optimal nodes, instead of all nodes, in the search tree to
expand at every step and remove the rest. A major issue of the
vanilla beam search is that it considers only the log-probability
provided by the model to generate the next token. Since other
information about code (e.g., variables in scope) is unavailable
to the APR model, it often generates a high score for an out-
of-scope variable, producing an uncompilable candidate patch.
Therefore, we design two techniques‚Äî valid-identiÔ¨Åer check
andlength control ‚Äîto make the beam search code-aware.
Valid-IdentiÔ¨Åer Check: Only a few tokens are valid in a
certain Java code snippet since correct code must follow
Java syntax and compilation rules. To generate valid iden-
tiÔ¨Åers only, CURE Ô¨Årst uses static analysis to analyze and
extract valid identiÔ¨Åers. Then CURE tokenizes these identiÔ¨Åers
1166(a) Vanilla beam search vs. beam search using valid-identiÔ¨Åer check, with
beam size of 2
(b) Vanilla beam search vs. beam search using length control in the same bug,
but with beam size of 1,000
Fig. 6. Examples using the vanilla beam search and beam search with valid-
identiÔ¨Åer-check and length-control strategies. Green arrows are the paths to
the correct Ô¨Åxes. Grey circles are the nodes kept by the search strategies
in the search tree at every level, and white circles are nodes discarded. Red
numbers are the log probability changed by search strategies.
(e.g., ‚Äú max_ending_here ‚Äù becomes [max,_,ending ,_,
here ]), and builds the mappings between all preÔ¨Åxes and
their valid succeeding tokens (as showns in Figure 5). These
mappings are necessary for the beam-search algorithm to know
that after generating the sequence ‚Äú max_ending_ ‚Äù, ‚Äúhere ‚Äù
is a valid next token because ‚Äú max_ending_here ‚Äù is a
valid identiÔ¨Åer.
At every decoding step, the NMT model outputs a proba-
bility distribution of all the tokens in vocabulary. CURE‚Äôs new
valid-identiÔ¨Åer-check strategy Ô¨Årst analyzes the token sequence
already generated to get the ‚ÄúpreÔ¨Åx‚Äù of the next token needed
to be generated. If the generated token sequence does not
contain any preÔ¨Åx, (i.e., the next token will be the beginning
of a new identiÔ¨Åer), the ‚ÄúpreÔ¨Åx‚Äù will be set to the empty
string. Then, based on the mappings between all possible
preÔ¨Åxes and their valid succeeding tokens, the valid-identiÔ¨Åer-
check strategy modiÔ¨Åes the probability distribution and sets
the probability of invalid tokens to  inf. By doing this,
the valid-identiÔ¨Åer-check strategy discards many impossible
nodes, increasing the possibility of Ô¨Ånding the correct patch.
Figure 6(a) illustrates how our code-aware beam search
outperforms the vanilla beam search. The correct Ô¨Åx is
‚Äúmax_ending_here=Math.max(0,max_ending_here) ‚Äù,
and the start of the output sequence
(‚Äúmax_ending_here= ‚Äù) has already been generated
in steps 1 to 6. We use a beam size of 2 to simplify theillustration. The path to the correct Ô¨Åx is marked with
green arrows and the nodes considered by the beam-search
strategies are in light grey.
During step 7, the two most likely nodes according to the
APR model are ‚Äú ... max ‚Äù and ‚Äú ... Math ‚Äù, where ‚Äú ...‚Äù
refers to ‚Äú max_ending_here= ‚Äù which has already been
generated. However, at step 8, the average log-likelihood of
‚Äú... Math . ‚Äù, which is the sequence denoted by the green
path in the left subÔ¨Ågure of Figure 6(a), is less than that of
‚Äú... max _ ‚Äù and ‚Äú ... max CaMeL ‚Äù, so the vanilla beam
search drops it. Thus, the entire subtree containing the correct
Ô¨Åx is excluded.
In contrast, with our valid-identiÔ¨Åer-check strategy, the aver-
age log-likelihood of ‚Äú ... max CaMeL ‚Äù is set to -inf be-
cause there is no valid identiÔ¨Åer starting with ‚Äú max CaMeL ‚Äù.
Therefore, our code-aware beam search keeps searching the
subtree of node ‚Äú ... Math ‚Äù, which leads to the correct Ô¨Åx.
Length Control : In our training data, most correct Ô¨Åxes have a
similar length as the buggy lines. We Ô¨Ånd the length difference
of 75% of the bugs in our 2.7 million patch training data is
less or equal to 5 tokens. This means that most of the time,
the correct Ô¨Åxes are small modiÔ¨Åcations to the buggy lines,
and more complex changes are less common.
Therefore, we use length-control strategy to generate
patches of a similar length of the buggy lines, by punishing
short and long patches. At every decoding step, the length-
control strategy calculates the length of the sequence already
generated. If the current length is much smaller than the buggy
lines, it decreases the log-likelihood of ‚Äú <EOS> ‚Äù to prevent
this patch from reaching the end. And if the current length
is already much larger than the buggy lines, it increases the
log-likelihood of ‚Äú <EOS> ‚Äù to prompt this patch to end.
To determine the penalty value, we leverage the length
difference distribution in the patch training data to calculate the
log-probability of each length difference, denoted as function
Flen. Our length-control strategy modiÔ¨Åes the log-likelihood
of token ‚Äú <EOS> ‚Äù by adding the following penalty to it:
penalty =(
0 5<=lb lp<= 5
Flen(lb lp)otherwise
where the lengths of the buggy lines and the patch sequence
already generated are lbandlprespectively. We empirically
set a tolerance threshold of 5 to increase Ô¨Çexibility.
Figure 6(b) illustrates this issue. The bug is the same as in
Figure 6(a) but with a larger beam size of 1,000. In step 2, the
sequence ‚Äúf g‚Äù reaches the ‚Äú <EOS> ‚Äù token. Using the vanilla
beam search, patch ‚Äú f g‚Äù has a low average log-probability
but is still in the top 1,000, this is added to candidate patches
because the beam size is large (1,000). Such low score patches
take up the valuable slots and prevent correct patches from
being selected. In our code-aware beam-search strategy, since
the complete sequence, ‚Äú f g‚Äù, is much shorter than the buggy
sequence, the ‚Äú <EOS> ‚Äù token receives a large penalty and is
not selected as a candidate node (not included in the 1,000
1167highest average log-probabilities), allowing the search strategy
to search deeper along other paths.
While CURE focuses on Ô¨Åxing bugs whose Ô¨Åxes are similar
in length to the buggy lines, our length-control strategy is
general and can be adapted to generate longer patches by
modifying the penalty weights. Given the complexity of APR,
Ô¨Åxing similar-length bugs and other bugs separately may be
an effective way to decompose this complex task.
H. Patch Validation
After APR models generate candidate patches, we recon-
struct the token sequences to code statements. We Ô¨Årst con-
catenate tokens end with ‚Äú @@‚Äù to their successors, which is the
reconstruction from subwords to words. Then we extract donor
code from the buggy code Ô¨Åle to reconstruct the abstracted
tokens (numbers and strings).
Reconstructed statements are ranked by the average log-
probability of their tokens and then inserted into the buggy
code Ô¨Åle to replace the buggy lines. Every patched project is
compiled to Ô¨Ålter the uncompilable patches and we run the
test suites until we Ô¨Ånd a patch that satisÔ¨Åes two conditions:
(1) passing all the test cases that the buggy project passes and
(2) passing at least one test case failed on the buggy project,
which are the same criteria for validation used in previous
work [19], [40].
IV. E XPERIMENTAL SETUP
Realistic Evaluation: To make the evaluation realistic, we
need to avoid using future data [19], [41]. We address this issue
by using data committed before the Ô¨Årst bug in our benchmark
(i.e., 2006) for pre-training, Ô¨Åne-tuning, and validation.
PL Training Data: We download all (1,700) open-source Java
projects from GitHub that have at least one commit before the
Ô¨Årst bug in Defects4J according to GHTorrent [42] and roll
them back to a version before 2006 to avoid using future data.
Then, we use JavaParser [43] to extract all methods except
abstract methods and those longer than 1,024 tokens. The
PL training data contains 4.04 million methods, with 30,000
instances for validation.
Patch Training Data: We use CoCoNuT‚Äôs training data
shared on GitHub [19] as our patch training data, which is
extracted from 45,180 Java projects. These Java projects are
a superset of the projects used for PL training data since
we need more projects to extract enough patch data and it
is too expensive to use all these projects for PL training.
Then we discard the instances whose context or correct Ô¨Åxes
are longer than 1,024 tokens after subword tokenization.
Removing instances from the training set is a common practice
for machine learning, and since the test set (bugs in Defects4j
and QuixBugs are untouched), this setup is valid. Our patch
training data contains 2.72 million training instances and
16,920 validation instances.
Subword Tokenization, Training, Fine-Tuning, and In-
ference: We set the target vocabulary size to be 50,000
for BPE. For the GPT model, considering previous work‚Äôsrecommendation [37] and our hardware limits, we use an
embedding size of 384, eight layers of transformer blocks,
and six attention heads. We train GPT for Ô¨Åve epochs, using a
batch size of 12. We use Adam optimizer [44], and the learning
rate increases from 0 to 2:5e 4at the Ô¨Årst 2,000 training steps
and then decreases using a cosine schedule.
To Ô¨Åne-tune the hyperparameters of an APR model, we
use random search with the following ranges: convolution
dimension (128‚Äì512), kernel size (2‚Äì10), number of convo-
lutional layers (1‚Äì5), and dropout (0‚Äì0.5). is empirically
set to 0.3. We train 100 APR models on a smaller subset of
patch training data for one epoch and keep the top Ô¨Åve models
combining GPT with CoNuT model and top Ô¨Åve APR models
combining GPT with FConv model. We use Adam optimizer
with a learning rate of 6:25e 5to keep tuning the top models
on our patch training data for one epoch, with a batch size of
12.
In inference mode, we use beam search with a beam size
of 1,000, and CURE generates 10,000 candidate patches for
every bug. During the validation stage, considering the time
cost and that most correct Ô¨Åxes have high ranks, we validate
the top 5,000 candidate patches per bug.
Infrastructure: We use GPT implemented by Hugging Face
[45], CoNuT and FConv implemented using fairseq [19], [46].
We train and evaluate our models on one 56-core server with
one NVIDIA TITAN V and three Xp GPUs.
V. E VALUATION AND RESULTS
We use two widely-used benchmarks, Defects4J (v1.4.0) [38]
and QuixBugs [47] for evaluation. Following [19], we remove
two Defects4J bugs, Closure 63 and Closure 93, from our
evaluation as they are duplicates of other Defects4J bugs. We
compile the patched projects and run the test suites to Ô¨Ånd
plausible patches, i.e., patches that pass the relevant test cases.
Two co-authors independently check plausible patches and
consider correct only those that are identical or semantically
equivalent to developers‚Äô patches (92% of agreement, Cohen‚Äôs
k of 0.84), then discuss to resolve disagreements.
We compare CURE with 25 APR techniques [1], [3], [5],
[8]‚Äì[15], [18]‚Äì[20], [33], [34], [48]‚Äì[56]. Table I shows the
comparison results. The table lists only a few top-ranked
techniques in terms of the number of bugs that they Ô¨Åx
in each benchmark, including state-of-the-art pattern-based
techniques [33], [48], three NMT-based techniques [18]‚Äì[20]
and the techniques that have been evaluated on QuixBugs.
None of these tools uses subword tokenization, pre-trained
PL model or, code-aware search strategy. Other tools (e.g.,
A V ATAR [54], kPAR [53], SimFix [10]) either Ô¨Åx fewer
bugs than the listed tools or were not evaluated on these
benchmarks. Results from all9 techniques in Table I except
Astor [6] and Hercules [48] use the perfect fault localization
(FL) of bugs to report bug Ô¨Åxing results. As stated in previous
work [53], [54], having APR techniques use the same FL
techniques (e.g., perfect FL) is a fair way to compare APR
techniques since different FL methods affect APR techniques
1168TABLE I
COMPARISON WITH STATE -OF-THE-ART APR TOOLS . THE NUMBERS ARE
SHOWN AS X /Y,WHERE X IS THE NUMBER OF BUGS FIXED CORRECTLY
AND Y IS THE NUMBER OF BUGS WITH PLAUSIBLE PATCHES . ‚Äò-‚Äô MEANS
THE APPROACH HAS NOT BEEN EVALUATED ON THAT BENCHMARK .
Tool Defects4J QuixBugs
393 bugs 40 bugs
Astor [6] - 6/11
Nopol [5] 2/9 1/4
RsRepair [34] 10/24 2/4
Hercules [48] 49/72 -
TBar [33] 52/85 -
SequenceR [20] 14/19 -
DLFix [18] 36/65 -
CoCoNut [19] 44/85 13/20
CURE 57/104 26/35
Fig. 7. Chart 17 in Defects4J is a bug only Ô¨Åxed by CURE
Fig. 8. Closure 10 in Defects4J is a bug that CURE Ô¨Åxes but CoCoNuT does
not.
differently. CURE‚Äôs correctly generated patches are available
in our GitHub Repository.
A. RQ1: How does CURE perform against state-of-the-art
APR techniques?
In Table I, the results are displayed as x/y, where x is the
number of bugs Ô¨Åxed correctly and y is the number of bugs
with plausible patches.
CURE Ô¨Åxes the most number of bugs, 57 and 26 respec-
tively, in both Defects4J and QuixBugs. SpeciÔ¨Åcally, CURE
generates plausible patches for 104 Defects4J bugs, 57 of
which are correctly Ô¨Åxed by CURE, outperforming the best
existing approach TBar by Ô¨Åve bugs. Compared to NMT-based
approaches [18]‚Äì[20], CURE correctly Ô¨Åxes 13 more bugs
than the best NMT-based approach CoCoNuT. CURE Ô¨Åxes 26
QuixBugs bugs (twice as many bugs as CoCoNuT), including
12 bugs that none of the four existing tools that have been
evaluated on QuixBugs can Ô¨Åx. In Defects4J, CURE Ô¨Åxes one
unique bug, Chart 17, that has not been Ô¨Åxed by any of the
25 existing approaches.
Bugs that only CURE Ô¨Åxes: Figure 7 shows the unique
bug in Defects4J and the correct Ô¨Åx that CURE generates,
which is equivalent to developers‚Äô patch. The correct Ô¨Åx
requires ensuring the second parameter to be non-negative.
Pattern-based approaches (e.g., TBar and Hercules) fail to
Ô¨Åx it because they have no Ô¨Åx patterns to ensure that a
method parameter is non-negative. NMT-based approaches
(e.g., SequenceR, DLFix, and CoCoNuT) fail to Ô¨Åx it, since
such a Ô¨Åx is uncommon. In our patch training data (already
2.72 million training instances from 45,180 projects), there
are only two similar Ô¨Åxes. Thus, it is hard for NMT-based
models to capture this transformation due to the lack of more
Fig. 9. Math 41 in Defects4J is a bug that CURE Ô¨Åxes but pattern-based
tools TBar and Hercules do not.
similar Ô¨Åxes. However, adding ‚Äú Math.max() ‚Äù to ensure non-
negativeness is common in Java methods and is captured by
our PL model, allowing CURE to Ô¨Åx the Chart 17 bug in
Defects4J correctly.
As explained in the Introduction, Figure 1 shows the KTH
bug in QuixBugs, which only CURE Ô¨Åxes and none of the
existing techniques evaluated on QuixBugs does. CoCoNuT
fails to Ô¨Åx this bug as it generates too many uncompilable
patches. Nopol, RSRepair, and Astor cannot repair this bug as
they do not implement the required Ô¨Åx pattern.
Comparing with the existing best-performing NMT-based
approach CoCoNuT, CURE Ô¨Åxes 13 more bugs in Defects4J.
Figure 8 shows an example bug that CURE Ô¨Åxes and Co-
CoNuT fails to Ô¨Åx. The correct Ô¨Åx of Closure 10 requires
‚ÄúanyResultsMatch ‚Äù, which is nonexistent in the buggy
line or context. CoCoNuT prioritizes tokens in the buggy line
and context, thus fails to generate the correct token to Ô¨Åx
this bug. In contrast, CURE‚Äôs code-aware beam-search strategy
extracts all valid identiÔ¨Åers, including ‚Äú anyResultsMatch ‚Äù
which is declared out of the context, and generates the correct
Ô¨Åx.
Comparing with the best pattern-based approach, CURE
Ô¨Åxes Ô¨Åve more bugs in Defects4J than TBar, most of which
require complex transformations to Ô¨Åx. Figure 9 shows an
example. The correct Ô¨Åx of Math 41 requires changes to the
initialization of ‚Äú i‚Äù and the condition for the loop. TBar does
not have such a complex Ô¨Åx pattern. CURE Ô¨Åxes Math 41
since similar transformations can be learned from the patch
training data.
Compilable patch rate: In addition to the number of correctly
Ô¨Åxed bugs, we use the average compilable rate to measure the
effectiveness of CURE learning PL syntaxes and developer-
like code. We compare the average compilable rates of the top-
k candidate patches generated by different NMT-based models,
for bugs in two benchmarks. Table II shows that CURE
generates more compilable patches in top-30 candidates than
SequenceR, and more compilable patches in all top-30, 100,
1,000, and 5,000 than CoCoNuT (DLFix does not offer com-
pilable rate data). Comparing different rows shows that each
component has contributed to the higher compilable patch rate.
For example, comparing row ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù
with row ‚ÄúCURE‚Äù shows that our code-ware search has in-
creased the average compilable patch rate by 6% (from 22% to
28%) for the top 100 patches. CURE generates more portions
of compilable patches than existing NMT-based approaches,
thanks to the PL model and the valid-identiÔ¨Åer-check strategy.
These examples and compilable patch rates demonstrate that
(1)the unique capabilities of our model that combines a GPT
PL model and an NMT model to learn both developer-like
code and Ô¨Åx patterns to Ô¨Åx more bugs and (2) the effectiveness
1169TABLE II
AVERAGE COMPILABLE RATES OF THE TOP -K CANDIDATE PATCHES FOR
BUGS IN TWO BENCHMARKS FROM DIFFERENT MODELS . ‚ÄúVANILLA ‚ÄùAND
‚ÄúCODE -AWARE ‚ÄùDENOTE VANILLA BEAM SEARCH AND CODE -AWARE
BEAM SEARCH RESPECTIVELY . CURE IS
‚ÄúBPE+GPT+C ONUT+CODE -AWARE ‚Äù. ‚Äò-‚Äô INDICATES DATA
UNAVAILABILITY .
Top Top Top Top
Model 30 100 1000 5000
SequenceR [20] 33% - - -
CoCoNuT (CoNuT+vanilla) [19] 24% 15% 6% 3%
BPE+CoNuT+vanilla 28% 18% 7% 4%
GPT+CoNuT+vanilla 28% 20% 9% 5%
BPE+GPT+CoNuT+vanilla 32% 22% 10% 6%
CURE 39% 28% 14% 9%
TABLE III
RESULTS OF ABLATION STUDY ON TWO BENCHMARKS . (C OCONUTUSES
20MODELS FOR ENSEMBLE WHILE THE REST USE ONLY 10MODELS .)
Model Defects4J QuixBugs
CoCoNuT (CoNuT+vanilla) 44/85 13/20
BPE+CoNuT+vanilla 45/85 16/25
GPT+CoNuT+vanilla 44/84 19/27
BPE+GPT+CoNuT+vanilla 51/94 22/27
CURE (BPE+GPT+CoNuT+code-aware) 57/104 26/35
of our PL model and the context-aware search strategy in
generating more compilable patches.
Type of bugs that CURE is applicable for: Similar to most
state-of-the-art G&V APR techniques [1], [3], [5], [8]‚Äì[10],
[12]‚Äì[15], [18]‚Äì[20], [33], [34], [49], [51]‚Äì[55], CURE is
designed to Ô¨Åx single-hunk bugs (i.e., the buggy lines and
patches are single code segments, and each buggy hunk has
separate test cases).
B. RQ2: What are the contributions of CURE‚Äôs components?
To study the impact of each novel technique (i.e., GPT
PL model, code-aware beam-search strategy, and subword
tokenization) of CURE, we compare the following four tech-
niques with CURE: CoCoNut (‚ÄúCoNuT+vanilla‚Äù) An en-
semble of ten CoNuT models and ten FConv models, us-
ing word-level tokenization and vanilla beam-search strategy.
CoCoNuT uses twice as many models as CURE and the
next three techniques (20 versus 10 models) and generates
twice as many candidate patches. Each of the next three
techniques is an ensemble of Ô¨Åve CoNuT models and Ô¨Åve
FConv models with the vanilla beam-search strategy. The
differences are that ‚ÄúBPE+CoNuT+vanilla‚Äù uses subword
tokenization, ‚ÄúGPT+CoNuT+vanilla‚Äù uses a GPT PL model,
and ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù uses both subword tok-
enization and a GPT PL model.
All models use a beam size of 1,000, generate 10,000
candidate patches, validate the top 5,000 candidate patches
for every bug (except CoCoNuT that generates and validates
20,000 candidate patches for each bug), and are trained on the
same dataset.
1) Impact of the GPT PL model: Table III lists the
result of the ablation study on two benchmarks. Rows
‚ÄúBPE+GPT+CoNuT+vanilla‚Äù versus ‚ÄúBPE+CoNuT+vanilla‚Äù
show that the GPT PL model helps APR models Ô¨Åx six morebugs in each benchmark. Comparing ‚ÄúGPT+CoNuT+vanilla‚Äù
with CoCoNuT shows that the GPT PL model helps Ô¨Åx six
more QuixBugs bugs. Although they Ô¨Åx the same number of
Defects4J bugs, CoCoNuT uses an ensemble of 20 models,
while ‚ÄúGPT+CoNuT+vanilla‚Äù uses only 10. CoCoNuT with 10
models Ô¨Åxes 38 bugs only [19], which shows an improvement
of six more bugs of ‚ÄúGPT+CoNuT+vanilla‚Äù versus CoCoNuT
with 10 models.
In Table II, comparing ‚ÄúBPE+CoNuT+vanilla‚Äù and
‚ÄúBPE+GPT+CoNuT+vanilla‚Äù shows that GPT increases the
average compilable rate by 2%‚Äì4%. In addition, the average
rank (the highest rank one is the best) of the correct patches
(before validation) generated by ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù
is 68% higher than that of ‚ÄúBPE+CoNuT+vanilla‚Äù (131 vs.
414), indicating that the GPT PL model not only enables APR
models to Ô¨Åx more bugs but also improves the ranks of correct
patches.
2) Impact of Code-Aware Beam-Search Strategy: Com-
paring ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù and CURE in Table III
shows that our code-aware beam-search strategy helps APR
models Ô¨Ånd more correct patches and Ô¨Åx more bugs (six
more in Defects4J and four more in QuixBugs). Comparison
between ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù and CURE in Table II
shows that our code-aware beam search increases the average
compilable rate by 3%‚Äì7%. The average rank of the correct
patches (before validation) generated by CURE is 21% higher
than ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù (101 vs. 131), indicating
that our new search strategy also increases the rank of correct
patches.
To measure the impact of the length-control strat-
egy, we compare the length of candidate patches gener-
ated by ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù and CURE. For the
‚ÄúBPE+GPT+CoNuT+vanilla‚Äù model, the average length dif-
ference between candidate patches and correct Ô¨Åxes is seven
tokens. In contrast, the average length difference between the
CURE‚Äôs candidate patches and correct Ô¨Åxes is Ô¨Åve tokens.
This shows the length-control strategy helps generate more
candidate patches with similar length to the correct Ô¨Åxes.
SpeciÔ¨Åcally, it helps Ô¨Åx long bugs (e.g., it Ô¨Åxes the longest
bug in QuixBugs that cannot be Ô¨Åxed without length-control
strategy) since it searches deeper.
3) Impact of subword tokenization: Subword tokenization
improves the search space by reducing the size of vocabulary
(from 139,423 to 50,057 tokens) and covering more correct
Ô¨Åxes. CoCoNuT versus ‚ÄúBPE+CoNuT+vanilla‚Äù shows that
subword tokenization helps Ô¨Åx four more bugs (one in De-
fects4J and three more in QuixBugs). ‚ÄúGPT+CoNuT+vanilla‚Äù
versus ‚ÄúBPE+GPT+CoNuT+vanilla‚Äù also shows that subword
tokenization helps Ô¨Åx 10 more bugs.
We also compare the number of OOV tokens with different
tokenization techniques. With word-level tokenization, 14 bugs
contain OOV tokens in our benchmarks (e.g., ‚Äú binsearch ‚Äù
and ‚Äú charno ‚Äù). In contrast, all these OOV tokens are sep-
arated into more common tokens when using subword tok-
enization. This shows that subword tokenization helps reduce
1170the vocabulary size, improve the vocabulary, make the model
easier to train, and eventually Ô¨Åx more bugs.
C. Execution Time
Data extraction: Downloading and extracting 4.04 million
methods from 1,700 projects as our PL training data takes
one day. We use CoCoNuT‚Äôs training data shared on GitHub,
which takes Ô¨Åve days to extract [19]. Both are a one-time cost.
Training PL model: It takes ten days to pre-train the GPT PL
model on four GPUs for Ô¨Åve epochs. Since the PL model is
trained on large and general data, one programming language
only needs one PL model and the PL model can be used to
enhance tasks other than APR and does not need retraining.
Fine-tuning APR models: It takes 12 days to tune the
hyperparameters by training 100 APR models for one epoch.
Fine-tuning the top-10 APR models takes, on average, 10.7
hours per model. This is a one time cost as the trained APR
models do not need retraining when Ô¨Åxing new bugs.
Cost to Ô¨Åx one bug: In inference, it takes 2.5 minutes
on average for CURE to generate 10,000 candidate patches
for one bug using four GPUs. During validation, it takes
16.5 minutes on average to validate one bug. Compared with
the state-of-art NMT-based approach [19], CURE uses fewer
models and validates fewer patches, thus CURE is faster and
Ô¨Åxes more bugs.
VI. L IMITATIONS
Comparison with previous work: It is difÔ¨Åcult to fairly
compare our work with all previous work as they use different
training data and FL methods. To be as fair as possible, we use
the same training data as CoCoNuT, the state-of-the-art NMT
technique, and demonstrate signiÔ¨Åcant improvement on both
benchmarks. Some previous work uses different training data,
but the selection and extraction of data is also a key component
of a technique. In addition, to compare with previous APR
techniques, we choose to use perfect localization as it is the
preferred comparison method [53] and previous work [57]
evaluated most available APR techniques with perfect FL.
Generalization to other benchmarks and PL: We evaluate
CURE on two Java benchmarks, but the approach is neither
tied to a speciÔ¨Åc PL nor a speciÔ¨Åc benchmark. CURE is
generalizable to other languages by updating the PL parser.
The benchmarks we chose are very popular, Defects4J being
used to evaluate 25 other APR tools. In the future, we can
also evaluate all APR approaches on recent benchmarks such
as Bugs.jar [58] or Bears [59].
Accuracy of the training sets: Since our training and pre-
training data extraction is conducted automatically, there is
a risk that such data is inaccurate. The training data was
extracted in previous work [19] and showed to be reasonably
accurate on a random sample. For the pre-training data, we
extract all complete functions and some of them might be
buggy or incorrect. However, the goal of the pre-training is to
learn the syntax of the PL, therefore, we mostly care that the
data follows the PL syntax, which is veriÔ¨Åed since we only
keep methods successfully parsed by a Java parser.VII. R ELATED WORK
Deep Learning for APR: Different DL-based APR techniques
have been developed to Ô¨Åx bugs [18]‚Äì[20], [25], [27], [60],
compilation issues [22]‚Äì[24], or predict the correctness of
generated patches [61]. CURE is different from previous
work in three ways. First, our subword-tokenization technique
addresses the OOV problem encountered by all NMT-based
techniques. Second, CURE integrates a new PL model to the
APR models that better learns the syntax of source code.
Finally, our new code-aware search strategy chooses only valid
identiÔ¨Åers during inference, which helps Ô¨Ålter out incorrect
patches. As a result, CURE generates more reasonable and
compilable patches and outperforms all existing techniques.
Automatic Program Repair: Many APR techniques have
been proposed, which use genetic programming [1], condition
synthesis [2], [5], [9], state abstraction [14], heuristics [8],
[10], [12], [62], human-designed Ô¨Åx patterns [3], [6], mined Ô¨Åx
patterns [4], [7], [13], [15], [56], [63], bytecode mutation [64],
or neural program synthesis [65]. CURE uses a new code-
aware NMT approach and Ô¨Åxes more bugs than previous state-
of-the-art approaches.
Deep Learning in Software Engineering: The software en-
gineering community had applied deep learning to performing
various tasks such as source code summarization [66], [67],
code clone detection [68], [69], defects prediction [17], [60],
[70], [71], code completion [72], and program synthesis [73],
[74]. These techniques, along with ours, demonstrate that
deep learning is competitive in different software engineering
tasks. Our work introduces code-awareness to DL systems to
improve APR. In the future, increasing code-awareness of DL
systems applied to other software tasks could also be useful.
Language Model in Software Engineering: Different pro-
gramming language models have been developed [75]‚Äì[87].
None of these approaches have been evaluated on Ô¨Åxing
software bugs and have only been used for simpler tasks such
as method name generation or source code summarization.
Recent work has questioned the generalizability of some of
these approaches for more complex tasks [88], [89]. Compared
with these models, CURE uses GPT [37], one of the most
powerful language models in NLP, to capture code syntax and
demonstrates its effectiveness for the more complex APR task.
VIII. C ONCLUSION
We propose and evaluate CURE, a new NMT-based program
repair technique that by design parses, models, and searches
source code, as opposed to natural language text, to auto-
matically Ô¨Åx bugs. CURE uses an NMT model that contains
a PL model, a code-aware search strategy, and a subword-
tokenization technique to create a smaller search space that
contains more correct patches and Ô¨Ånd more correct patches.
CURE outperforms all existing techniques on two popular
benchmarks, Ô¨Åxing 83 bugs. We highlight this direction of
code-aware NMT for automatic program repair.
1171REFERENCES
[1] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, ‚ÄúGenProg: A
Generic Method for Automatic Software Repair,‚Äù TSE, vol. 38, no. 1,
pp. 54‚Äì72, 2012.
[2] F. Long and M. Rinard, ‚ÄúStaged Program Repair with Condition
Synthesis,‚Äù in ESEC/FSE . ACM, 2015, pp. 166‚Äì178.
[3] R. K. Saha, Y . Lyu, H. Yoshida, and M. R. Prasad, ‚ÄúELIXIR: Effective
Object Oriented Program Repair,‚Äù in ASE. IEEE, 2017, pp. 648‚Äì659.
[4] F. S. Ocariza, Jr, K. Pattabiraman, and A. Mesbah, ‚ÄúVejovis: Suggesting
Fixes for JavaScript Faults,‚Äù in ICSE . ACM, 2014, pp. 837‚Äì847.
[5] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. L. Marcote,
T. Durieux, D. Le Berre, and M. Monperrus, ‚ÄúNopol: Automatic Repair
of Conditional Statement Bugs in Java Programs,‚Äù TSE, vol. 43, no. 1,
pp. 34‚Äì55, 2017.
[6] M. Martinez and M. Monperrus, ‚ÄúASTOR: A Program Repair Library
for Java (Demo),‚Äù in ISSTA . ACM, 2016, pp. 441‚Äì444.
[7] F. Long, P. Amidon, and M. Rinard, ‚ÄúAutomatic Inference of Code
Transforms for Patch Generation,‚Äù in ESEC/FSE . ACM, 2017, p.
727‚Äì739.
[8] Q. Xin and S. P. Reiss, ‚ÄúLeveraging Syntax-Related Code for Automated
Program Repair,‚Äù in ASE. IEEE, 2017, p. 660‚Äì670.
[9] Y . Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, and L. Zhang,
‚ÄúPrecise Condition Synthesis for Program Repair,‚Äù in ICSE . IEEE,
2017, pp. 416‚Äì426.
[10] J. Jiang, Y . Xiong, H. Zhang, Q. Gao, and X. Chen, ‚ÄúShaping Program
Repair Space with Existing Patches and Similar Code,‚Äù in ISSTA . ACM,
2018, pp. 298‚Äì309.
[11] J. Hua, M. Zhang, K. Wang, and S. Khurshid, ‚ÄúSketchFix: A Tool for
Automated Program Repair Approach Using Lazy Candidate Genera-
tion,‚Äù in ESEC/FSE . ACM, 2018, pp. 888‚Äì891.
[12] M. Wen, J. Chen, R. Wu, D. Hao, and S.-C. Cheung, ‚ÄúContext-Aware
Patch Generation for Better Automated Program Repair,‚Äù in ICSE .
ACM, 2018.
[13] X. Liu and H. Zhong, ‚ÄúMining stackoverÔ¨Çow for program repair,‚Äù in
SANER . IEEE, 2018, pp. 118‚Äì129.
[14] L. Chen, Y . Pei, and C. A. Furia, ‚ÄúContract-Based Program Repair
without the Contracts,‚Äù in ASE. IEEE, 2017, pp. 637‚Äì647.
[15] X. B. D. Le, D. Lo, and C. Le Goues, ‚ÄúHistory Driven Program Repair,‚Äù
inSANER , vol. 1. IEEE, 2016, pp. 213‚Äì224.
[16] D. Kim, J. Nam, J. Song, and S. Kim, ‚ÄúAutomatic Patch Generation
Learned from Human-Written Patches,‚Äù in ICSE . IEEE, 2013, pp.
802‚Äì811.
[17] S. Wang, T. Liu, and L. Tan, ‚ÄúAutomatically Learning Semantic Features
for Defect Prediction,‚Äù in ICSE . IEEE, 2016, pp. 297‚Äì308.
[18] Y . Li, S. Wang, and T. N. Nguyen, ‚ÄúDLFix: Context-Based Code
Transformation Learning for Automated Program Repair,‚Äù in ICSE .
ACM, 2020, p. 602‚Äì614.
[19] T. Lutellier, H. V . Pham, L. Pang, Y . Li, M. Wei, and L. Tan, ‚ÄúCoCoNuT:
Combining Context-Aware Neural Translation Models Using Ensemble
for Program Repair,‚Äù in ISSTA . ACM, 2020, p. 101‚Äì114.
[20] Z. Chen, S. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk,
and M. Monperrus, ‚ÄúSequenceR: Sequence-to-Sequence Learning for
End-to-End Program Repair,‚Äù TSE, 2019.
[21] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, ‚ÄúAn Empirical Study on Learning Bug-Fixing Patches
in the Wild via Neural Machine Translation,‚Äù TOSEM , vol. 28, no. 4,
2019.
[22] E. A. Santos, J. C. Campbell, A. Hindle, and J. N. Amaral, ‚ÄúFinding
and Correcting Syntax Errors Using Recurrent Neural Networks,‚Äù PeerJ
PrePrints , vol. 5, p. e3123v1, 2017.
[23] R. Gupta, S. Pal, A. Kanade, and S. Shevade, ‚ÄúDeepÔ¨Åx: Fixing Common
C Language Errors by Deep Learning,‚Äù in AAAI , 2017, pp. 1345‚Äì1351.
[24] A. Mesbah, A. Rice, E. Johnston, N. Glorioso, and E. Aftandilian,
‚ÄúDeepDelta: Learning to Repair Compilation Errors,‚Äù in ESEC/FSE .
ACM, 2019, pp. 925‚Äì936.
[25] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and
D. Poshyvanyk, ‚ÄúAn Empirical Investigation into Learning Bug-Fixing
Patches in the Wild via Neural Machine Translation,‚Äù in ASE. ACM,
2018, pp. 832‚Äì837.
[26] F. Long and M. Rinard, ‚ÄúAn Analysis of the Search Spaces for Generate
and Validate Patch Generation Systems,‚Äù in ICSE . IEEE, 2016, p.
702‚Äì713.
[27] Y . Ding, B. Ray, P. Devanbu, and V . J. Hellendoorn, ‚ÄúPatching as
Translation: the Data and the Metaphor,‚Äù in ASE, 2020.[28] S. Clinchant, K. W. Jung, and V . Nikoulina, ‚ÄúOn the use of BERT for
Neural Machine Translation,‚Äù in NGT . ACL, 2019, pp. 108‚Äì117.
[29] I. Skorokhodov, A. Rykachevskiy, D. Emelyanenko, S. Slotin, and
A. Ponkratov, ‚ÄúSemi-Supervised Neural Machine Translation with Lan-
guage Models,‚Äù in LoResMT . AMTA, 2018, pp. 37‚Äì44.
[30] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to Sequence Learning
with Neural Networks,‚Äù in NeurIPS . MIT Press, 2014, pp. 3104‚Äì3112.
[31] F. Stahlberg, ‚ÄúNeural Machine Translation: A Review,‚Äù JAIR , 2020.
[32] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,
‚ÄúConvolutional Sequence to Sequence Learning,‚Äù in ICML . JMLR.org,
2017, pp. 1243‚Äî-1252.
[33] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyand ¬¥e, ‚ÄúTBar: Revisiting
Template-Based Automated Program Repair,‚Äù in ISSTA . ACM, 2019.
[34] Y . Qi, X. Mao, Y . Lei, Z. Dai, and C. Wang, ‚ÄúDoes Genetic Programming
Work Well on Automated Program Repair?‚Äù in ICCIS . IEEE, 2013,
pp. 1875‚Äì1878.
[35] S. Rico, H. Barry, and B. Alexandra, ‚ÄúNeural Machine Translation of
Rare Words with Subword Units,‚Äù Annual Meeting of the ACL , 2016.
[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention Is All You Need,‚Äù in NeurIPS ,
2017, pp. 5998‚Äì6008.
[37] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‚ÄúImproving
Language Understanding by Generative Pre-Training,‚Äù OpenAI Blog ,
2018.
[38] R. Just, D. Jalali, and M. D. Ernst, ‚ÄúDefects4J: A Database of Existing
Faults to Enable Controlled Testing Studies for Java Programs,‚Äù in
ISSTA , 2014, pp. 437‚Äì440.
[39] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
‚ÄúLanguage Models are Unsupervised Multitask Learners,‚Äù OpenAI Blog ,
vol. 1, no. 8, p. 9, 2019.
[40] J. Yang, A. Zhikhartsev, Y . Liu, and L. Tan, ‚ÄúBetter Test Cases for
Better Automated Program Repair,‚Äù in FSE, ser. ESEC/FSE 2017.
ACM, 2017, p. 831‚Äì841. [Online]. Available: https://doi.org/10.1145/
3106237.3106274
[41] M. Tan, L. Tan, S. Dara, and C. Mayeux, ‚ÄúOnline Defect Prediction for
Imbalanced Data,‚Äù in ICSE-SEIP , 2015, pp. 99‚Äì108.
[42] G. Gousios and D. Spinellis, ‚ÄúGHTorrent: GitHub‚Äôs data from a Ô¨Åre-
hose,‚Äù in MSR . IEEE, 2012, pp. 12‚Äì21.
[43] N. Smith, D. Van Bruggen, and F. Tomassetti, ‚ÄúJavaparser: Visited,‚Äù
2019. [Online]. Available: https://github.com/javaparser/javaparser
[44] D. P. Kingma and J. Ba, ‚ÄúAdam: A Method for Stochastic Optimization,‚Äù
inICLR , 2015.
[45] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,
P. Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Brew, ‚ÄúHugging-
Face‚Äôs Transformers: State-of-the-art Natural Language Processing,‚Äù in
EMNLP , 2020.
[46] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and
M. Auli, ‚Äúfairseq: A Fast, Extensible Toolkit for Sequence Modeling,‚Äù
inNAACL , 2019.
[47] D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, ‚ÄúQuixBugs: A
Multi-Lingual Program Repair Benchmark Set Based on the Quixey
Challenge,‚Äù in SPLASH , 2017, p. 55‚Äì56.
[48] S. Saha, R. k. Saha, and M. r. Prasad, ‚ÄúHarnessing Evolution for Multi-
Hunk Program Repair,‚Äù in ICSE . IEEE, 2019, pp. 13‚Äì24.
[49] Z. Qi, F. Long, S. Achour, and M. Rinard, ‚ÄúAn Analysis of Patch
Plausibility and Correctness for Generate-and-Validate Patch Generation
Systems,‚Äù in ISSTA . ACM, 2015, pp. 24‚Äì36.
[50] Y . Yuan and W. Banzhaf, ‚ÄúARJA: Automated Repair of Java Programs
via Multi-Objective Genetic Programming,‚Äù TSE, 2018.
[51] M. Martinez and M. Monperrus, ‚ÄúUltra-Large Repair Search Space
with Automatically Mined Templates: the Cardumen Mode of Astor,‚Äù
inICSBSE . Springer, 2018.
[52] T. Durieux and M. Monperrus, ‚ÄúDynaMoth: Dynamic Code Synthesis
for Automatic Program Repair,‚Äù in AST, 2016, pp. 85‚Äì91.
[53] K. Liu, A. Koyuncu, T. F. Bissyand ¬¥e, D. Kim, J. Klein, and Y . Le Traon,
‚ÄúYou Cannot Fix What You Cannot Find! An Investigation of Fault Lo-
calization Bias in Benchmarking Automated Program Repair Systems,‚Äù
inICST . IEEE, 2019, pp. 102‚Äì113.
[54] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyand ¬¥e, ‚ÄúA V ATAR: Fixing
Semantic Bugs with Fix Patterns of Static Analysis Violations,‚Äù in
SANER . IEEE, 2019, pp. 1‚Äì12.
[55] A. Koyuncu, K. Liu, T. F. Bissyand ¬¥e, D. Kim, J. Klein, M. Monperrus,
and Y . Le Traon, ‚ÄúFixMiner: Mining relevant Ô¨Åx patterns for automated
program repair,‚Äù EMSE , pp. 1‚Äì45, 2020.
1172[56] K. Liu, A. Koyuncu, K. Kim, D. Kim, and T. F. Bissyand ¬¥e, ‚ÄúLSRepair:
Live Search of Fix Ingredients for Automated Program Repair,‚Äù in
APSEC , 2018, pp. 658‚Äì662.
[57] K. Liu, S. Wang, A. Koyuncu, K. Kim, T. F. D. A. Bissyande, D. Kim,
P. Wu, J. Klein, X. Mao, and Y . Le Traon, ‚ÄúOn the EfÔ¨Åciency of Test
Suite based Program Repair: A Systematic Assessment of 16 Automated
Repair Systems for Java Programs,‚Äù in ICSE , 2020.
[58] R. K. Saha, Y . Lyu, W. Lam, H. Yoshida, and M. R. Prasad, ‚ÄúBugs.jar:
A large-scale, diverse dataset of real-world java bugs,‚Äù in MSR , 2018.
[59] F. Madeiral, S. Urli, M. Maia, and M. Monperrus, ‚ÄúBears: An Extensible
Java Bug Benchmark for Automatic Program Repair Studies,‚Äù in SANER .
IEEE, 2019, pp. 468‚Äì478.
[60] E. Dinella, H. Dai, Z. Li, M. Naik, L. Song, and K. Wang, ‚ÄúHoppity:
Learning Graph Transformations to Detect and Fix Bugs in Programs,‚Äù
inICLR , 2019.
[61] H. Tian, K. Liu, A. K. Kabore ¬¥e, A. Koyuncu, L. Li, J. Klein, and T. F.
Bissyand ¬¥e, ‚ÄúEvaluating Representation Learning of Code Changes for
Predicting Patch Correctness in Program Repair,‚Äù in ASE, 2020.
[62] M. Asad, K. K. Ganguly, and K. Sakib, ‚ÄúImpact Analysis of Syntactic
and Semantic Similarities on Patch Prioritization in Automated Program
Repair,‚Äù in ICSME . IEEE, 2019, pp. 328‚Äì332.
[63] G. Sakkas, M. Endres, B. Cosman, W. Weimer, and R. Jhala, ‚ÄúType Error
Feedback via Analytic Program Repair,‚Äù in PLDI , 2020, pp. 16‚Äì30.
[64] A. Ghanbari, S. Benton, and L. Zhang, ‚ÄúPractical Program Repair via
Bytecode Mutation,‚Äù in ISSTA , 2019.
[65] G. Kavi, E. C. Peter, C. Xinyun, and S. Dawn, ‚ÄúSynthesize, Execute and
Debug: Learning to Repair for Neural Program Synthesis,‚Äù in NeurIPS ,
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,
2020.
[66] X. Gu, H. Zhang, and S. Kim, ‚ÄúDeep Code Search,‚Äù in ICSE . ACM,
2018, pp. 933‚Äì944.
[67] M. Allamanis, H. Peng, and C. Sutton, ‚ÄúA Convolutional Attention
Network for Extreme Summarization of Source Code,‚Äù in ICML , 2016,
pp. 2091‚Äì2100.
[68] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, ‚ÄúDeep Learning
Code Fragments for Code Clone Detection,‚Äù in ASE. ACM, 2016, pp.
87‚Äì98.
[69] L. Li, H. Feng, W. Zhuang, N. Meng, and B. Ryder, ‚ÄúCCLearner: A
Deep Learning-Based Clone Detection Approach,‚Äù in ICSME . IEEE,
2017, pp. 249‚Äì260.
[70] J. Li, P. He, J. Zhu, and M. R. Lyu, ‚ÄúSoftware Defect Prediction via
Convolutional Neural Network,‚Äù in QRS. IEEE, 2017.
[71] J. Wang and C. Zhang, ‚ÄúSoftware reliability prediction using a deep
learning model based on the RNN encoder‚Äìdecoder,‚Äù RESS , 2017.[72] L. Fang, L. Ge, Z. Yunfei, and J. Zhi, ‚ÄúMulti-task Learning based Pre-
trained Language Model for Code Completion,‚Äù in ASE, 2020.
[73] V . Murali, L. Qi, S. Chaudhuri, and C. Jermaine, ‚ÄúNeural Sketch
Learning for Conditional Program Generation,‚Äù in ICLR , 2018.
[74] W. Ling, E. Grefenstette, K. M. Hermann, T. Ko Àácisk`y, A. Senior,
F. Wang, and P. Blunsom, ‚ÄúLatent Predictor Networks for Code Gener-
ation,‚Äù Annual Meeting of the ACL , 2016.
[75] M. White, C. Vendome, M. Linares-V ¬¥asquez, and D. Poshyvanyk,
‚ÄúToward Deep Learning Software Repositories,‚Äù in MSR . IEEE, 2015,
pp. 334‚Äì345.
[76] V . J. Hellendoorn and P. Devanbu, ‚ÄúAre Deep Neural Networks the Best
Choice for Modeling Source Code?‚Äù in ESEC/FSE . ACM, 2017.
[77] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, ‚ÄúA Survey of
Machine Learning for Big Code and Naturalness,‚Äù CSUR , vol. 51, no. 4,
p. 81, 2018.
[78] S. Chakraborty, Y . Ding, M. Allamanis, and B. Ray, ‚ÄúCODIT: Code
Editing with Tree-Based Neural Models,‚Äù TSE, 2020.
[79] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚ÄúCode2vec: Learning
Distributed Representations of Code,‚Äù ACM on Programming Lan-
guages , vol. 3, no. POPL, pp. 1‚Äì29, 2019.
[80] U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating
Sequences from Structured Representations of Code,‚Äù in ICLR , 2019.
[81] J. Henkel, S. K. Lahiri, B. Liblit, and T. Reps, ‚ÄúCode Vectors: Under-
standing Programs through Embedded Abstracted Symbolic Traces,‚Äù in
ESEC/FSE , 2018, pp. 163‚Äì174.
[82] W. Wang, Y . Zhang, Y . Sui, Y . Wan, Z. Zhao, J. Wu, P. Yu, and
G. Xu, ‚ÄúReinforcement-Learning-Guided Source Code Summarization
via Hierarchical Attention,‚Äù TSE, 2020.
[83] T. Hoang, H. J. Kang, D. Lo, and J. Lawall, ‚ÄúCC2Vec: Distributed
Representations of Code Changes,‚Äù in ICSE . ACM, 2020, p. 518‚Äì529.
[84] H. Hu, Q. Chen, and Z. Liu, ‚ÄúCode Generation from Supervised Code
Embeddings,‚Äù in NeurIPS . Springer, 2019, pp. 388‚Äì396.
[85] K. Wang and Z. Su, ‚ÄúBlended, Precise Semantic Program Embeddings,‚Äù
inPLDI . New York, NY , USA: ACM, 2020, p. 121‚Äì134.
[86] W. Wang, Gao and Wang, ‚ÄúLearning Semantic Program Embeddings
with Graph Interval Neural Network,‚Äù in OOPSLA . ACM, 2020.
[87] J. Keim, A. Kaplan, A. Koziolek, and M. Mirakhorli, ‚ÄúDoes BERT
Understand Code? ‚Äì An Exploratory Study on the Detection of Archi-
tectural Tactics in Code,‚Äù in Software Architecture . Springer, 2020.
[88] H. J. Kang, T. F. Bissyand ¬¥e, and D. Lo, ‚ÄúAssessing the Generalizability
of code2vec Token Embeddings,‚Äù in ASE. IEEE, 2019, pp. 1‚Äì12.
[89] L. Jiang, H. Liu, and H. Jiang, ‚ÄúMachine Learning Based Recommen-
dation of Method Names: How Far are We,‚Äù in ASE. IEEE, 2019, pp.
602‚Äì614.
1173