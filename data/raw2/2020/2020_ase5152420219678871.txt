DeepMemory: Model-based Memorization Analysis
of Deep Neural Language Models
Derui Zhu
Technical University of Munich
Munich, Germany
derui.zhu@tum.deJinfu Chen
Huawei Technologies Canada
Kingston, Canada
jinfu.chen1@huawei.comWeiyi Shang
Concordia University
Montreal, Canada
shang@encs.concordia.ca
Xuebing Zhou
Huawei Munich Research Center
Munich, Germany
xuebing.zhou@huawei.comJens Grossklags
Technical University of Munich
Munich, Germany
jens.grossklags@in.tum.deAhmed E. Hassan
Queen‚Äôs University
Kingston, Canada
ahmed@cs.queensu.ca
Abstract ‚ÄîThe neural network model is having a signiÔ¨Åcant
impact on many real-world applications. Unfortunately, the in-
creasing popularity and complexity of these models also ampliÔ¨Åes
their security and privacy challenges, with privacy leakage from
training data being one of the most prominent issues. In this con-
text, prior studies proposed to analyze the abstraction behavior of
neural network models, e.g., RNN, to understand their robustness.
However, the existing research rarely addresses privacy breaches
caused by memorization in neural language models. To Ô¨Åll this
gap, we propose a novel approach, DeepMemory, that analyzes
memorization behavior for a neural language model. We Ô¨Årst
construct a memorization-analysis-oriented model, taking both
training data and a neural language model as input. We then
build a semantic Ô¨Årst-order Markov model to bind the con-
structed memorization-analysis-oriented model to the training
data to analyze memorization distribution. Finally, we apply
our approach to address data leakage issues associated with
memorization and to assist in dememorization. We evaluate our
approach on one of the most popular neural language models,
theLSTM -based language model, with three public datasets,
namely, WikiText-103, WMT2017, and IWSLT2016. We Ô¨Ånd that
sentences in the studied datasets with low perplexity are more
likely to be memorized. Our approach achieves an average AUC
of 0.73 in automatically identifying data leakage issues during
assessment. We also show that with the assistance of DeepMemory,
data breaches due to memorization of neural language models
can be successfully mitigated by mutating training data without
reducing the performance of neural language models.
Index Terms‚ÄîDeep learning, neural language model, model-
based analysis, privacy, memorization
I. I NTRODUCTION
ArtiÔ¨Åcial intelligence (AI) software is important for au-
tomating and making autonomous decisions. In particular, the
rise of neural network models had a huge and signiÔ¨Åcant
impact on many real-world applications, e.g., natural language
processing [ 1], [2], image recognition [ 3], and autonomous
driving [ 4], [5]. However, the increasing diversity and com-
plexity of such neural network models make their security,
reliability and robustness a critical and difÔ¨Åcult issue to address.
Therefore, researchers in different Ô¨Åelds are now working
*Jinfu Chen (jinfu.chen1@huawei.com) is the corresponding author.intensely on guidelines for Trustworthy AI andSafe AI. For
example, software engineering researchers propose techniques
that analyze and explain AI models in order to ensure the
security and safeness of AI-based software [6], [7].
Similar to traditional (i.e., not based on AI) software, AI-
based solutions have been reported by many prior studies to
trigger security concerns, such as data privacy leakage [ 8].
Although various veriÔ¨Åcation techniques, e.g., static analysis,
symbolic execution analysis and fuzzing techniques, can be
used to guide the assurance of traditional software security,
those techniques are not applicable for AI-based software. In
contrast, to the best of our knowledge, there is a relative lack
of techniques that can assist in the veriÔ¨Åcation of security in
AI-based software.
Data privacy leakage is a typical security issue in AI
models. Previous work [ 9], [10], [11] has shown that neural
language models tend to memorize the training data instead
of learning its latent characteristics. This can be exploited to
extract privacy-critical information from the data, potentially
leading to signiÔ¨Åcant Ô¨Ånancial and reputational harm [ 12]. More
generally, memorization with a neural language model may
reveal insights regarding its internal behavior. Prior studies [ 13],
[14], [15] have been proposed to analyze certain aspects of the
internal behavior of deep neural networks in order to assist with
detecting adversarial examples and to guide the security testing
of deep learning models [ 14]. However, the existing research
rarely targets a model‚Äôs internal memorization behavior. Hence,
the existing research is limited when it comes to analyzing
and preventing leakage of sensitive private information from
training data of a publicly released model.
To Ô¨Åll this research gap, we propose a novel approach,
DeepMemory, to assist in verifying security in AI-based
software by analyzing the internal memorization behavior of
neural language models. We Ô¨Årst construct a memorization-
analysis-oriented model taking both training data and a neural
language model as input. Second, we bind the constructed
memorization-analysis-oriented model to the training data. We
then build a semantic Ô¨Årst-order Markov model to analyze
10032021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Work licensed under Creative Commons Attribution NonCommercial, No Derivatives 4.0 License. https://creativecommons.org/licenses/by-nc-nd/4.0/
DOI 10.1109/ASE51524.2021.000922021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ¬©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678871
memorization distribution. Finally, we apply our approach to
two downstream application scenarios, including data leakage
risk assessment and dememorization assistance.
We evaluate our approach on one of the most popu-
lar neural language models, i.e., the LSTM -based language
model, with three public datasets, namely, WikiText-103 [ 16],
WMT2017 [ 17] and IWSLT2016 [ 18]. We investigate the
LSTM -based language model with the same architecture and
conÔ¨Åguration as Merity et al. [19]. We Ô¨Ånd that by observing
memorization characteristics for training data, sentences with
low perplexity are more likely to be memorized by a neural
language model. Our approach achieves an average AUC of
0.73 in automatically identifying data leakage issues during
assessment. Finally, by following our approach, the memoriza-
tion risk from a neural language model can be mitigated by
mutating training data without impacting the quality of neural
language models.
To the best of our knowledge, our approach is the Ô¨Årst
attempt to assist in the veriÔ¨Åcation of a common and important
privacy related issue in AI models, i.e., memorization in neural
language models. In particular, our work makes the following
contributions:
We model the internal memorization behavior of neural
language models, e.g., the LSTM -based language model,
in order to address training data leakage issues caused by
the model‚Äôs memorization behavior.
Our approach can automatically assess memorization-
related privacy leakage in neural language models.
With our work, we can assist in the dememorization
process in order to address memorization issues in neural
language models.
Our approach can be used to assist in the security testing
and assurance processes for AI models.
The rest of this paper is organized as follows: Section II
provides the background for our work. Section III gives an
overview of our approach, and Sections IV‚ÄìVI present the
details. Section VII presents the results of our evaluation.
Section IX discusses threats to the validity of our work, and
Section X summarizes related work. Finally, Section XI offers
concluding remarks.
II. B ACKGROUND
A. Language modeling
A language model is a probability distribution over sequences
of words [ 20]. In other words, a language model aims to learn
a probabilistic model that is capable to predict the next word
in a sequence based on the given preceding words. Formally,
the probability distribution of a language model can be deÔ¨Åned
asPr(w1;w2;:::;wn):
Pr(w1;w2;:::;wn) =nY
i=1Pr(wijw1;:::;wi 1) (1)
wherewirefers to a word. This language model has been
successfully used in many applications, such as speech recogni-
tion [ 21], machine translation [ 22], sentiment analysis [ 23], andinformation retrieval [ 24]. In particular, the neural language
model is becoming increasingly popular and has been success-
fully used in many applications. The neural language model
uses different kinds of neural networks to model sequence
probability, and it transforms words into vectors and uses the
vectors as input for a neural network to predict the next words.
A very common neural language model is the long short-term
memory (LSTM ) language model. An LSTM network contains
a plethora of units, called memory blocks. Each memory block
represents a hidden state, i.e., st,st+1,st+2. Prior work [ 25]
illustrated the success of LSTM -based language models in
multiple applications motivating us to conduct our work in this
context.
B. Memorization risk from training Data
A language model requires domain-speciÔ¨Åc data for training
in order to achieve a good model performance. However, a well-
performing language model might suffer from data leakage due
to memorization of training data. Such leakage is particularly
troublesome when sensitive data including personal/private data,
transaction data, or governmental data becomes available to an
attacker. Examples of such sensitive data are Social Insurance
Numbers (in Canada) or health-related data.
Sensitive data that is part of training data may be memorized
by a neural language model during model training. When
leveraging such a mechanism, one can develop attacks to
extract such data from public trained language models [ 26]. In
other words, data leakage due to the memorization mechanism
is a typical security weakness in AI-based language models.
The study of memorization privacy risk includes two lines of
research: memorization-related privacy attacks and defenses.
1) Memorization-related privacy attacks: If a model is
ignorant towards privacy-preserving algorithms, it tends to
blindly remember some sequences from the training data [ 8],
[27]. Previous studies [ 8], [28] Ô¨Ånd that memorization is
a common phenomenon in language models, and privacy
attacks aim to reconstruct verbatim memorization sequences
for training data.
2) Memorization-related privacy defenses: There are typ-
ically two ways to support dememorization, i.e., differential
privacy and regularization. Differential privacy, which injects
noise into the process of model training, is a well-known
solution for minimizing memorization in model training [ 29].
As such, it is challenging to identify whether speciÔ¨Åc data
is part of the training data. Another typical privacy defense
approach is regularization. One can add regularization to the
loss function of language model optimization [26].
III. O VERVIEW OF OUR APPROACH
In this section, we present an overview of our approach
for analyzing the memorization behavior for a given lan-
guage model. Similar to traditional software vulnerability
detectors, our approach acts as an automated technique for
detecting potential data leakage security vulnerabilities that
occur due to memorization in language models. An overview
of our approach is shown in Figure 1. It consists of three
1004Semantic
profiling
Memorization-analysis-oriented model construction (Section IV)
Automated
dimension
reductionSemantic
clusteringSemantic distribution abstraction
Concrete state & trace distributionModel
construction
Memorization distribution
 binding (Section V)Memorization
extraction
 Data leakage risk
assessment
Assisting in
dememorization
Addressing memorization
issues using the memoriz-
ation model (Section VI)Semantic Memorization Modeling
Memorization state/trace
distribution construction
Memorization sequence
distribution construction
Stateful deep learning modelTraining corpusFig. 1: An overview of our approach.
phases: 1) memorization-analysis-oriented model construction,
2) memorization-distribution binding, and 3) addressing mem-
orization issues using the memorization model.
In the Ô¨Årst phase, we construct a memorization-analysis-
oriented model. Taking both training data and a neural language
model as input, we Ô¨Årst proÔ¨Åle the given model to extract
semantic information, i.e., hidden states and traces. Such
proÔ¨Åling outputs initial states and traces that represent the
model behavior. Typically, a large number of initial states and
traces exist due to the massive scale of training data. We then
abstract a semantic distribution from the initial states and traces.
In particular, we transform initial states to intermediate states
by reducing the high dimensions of each initial state. We then
apply a clustering algorithm to group the intermediate states
and traces into clusters, i.e., to derive concrete states and traces.
Finally, we construct a memorization-analysis-oriented model
based on the concrete states and traces distribution.
In the second phase, our approach binds the memorization-
analysis-oriented model to the training data to analyze the
memorization distribution. This phase takes the memorization-
analysis-oriented model constructed from the last phase and
the training corpus as input. To analyze the memorization
distribution, we Ô¨Årst extract memorization sequences from the
training data. We then build a semantic Ô¨Årst-order Markov
model to model the memorization distribution.
In the Ô¨Ånal phase, we apply our approach to two downstream
tasks, including data leakage risk assessment and dememo-
rization assistance. The Ô¨Årst downstream task automatically
identiÔ¨Åes potential data leakage issues in the model (comparable
to bug detection). The second downstream task assists in the
repair of the models given the identiÔ¨Åed issues (comparable to
program repair). We detail each phase of our approach in the
subsequent Sections IV ‚Äì VI.
IV. M EMORIZATION -ANALYSIS -ORIENTED MODEL
CONSTRUCTION
In this section, we construct a memorization-analysis-
oriented model. Algorithm 1 presents the details of its construc-
tion. Given an LSTM -based language model and its training
data, we Ô¨Årst proÔ¨Åle the model to extract the initial states and
traces by iterating words in each sentence over the training
data. We then abstract the initial states and traces to construct
our memorization-analysis-oriented model. We describe each
step in detail below.Algorithm 1: Memorization-analysis-oriented model
construction algorithm
input :R= (D;;f): LSTM-based language model
G: semantic distribution abstraction function
: information loss threshold
D: sentences
: minimum number of neighbors threshold
: distance threshold
output :M: memorization-analysis-oriented model
1S []; // initial states set
2T []; // initial traces set
3forW2D do // loop sentences to extract states
4 s2[i(W[:i])]jWj
i=0;// extract all hidden states of a sentence
5 fori: 12jsjdo
6S:add(si);
7T:add ((si 1;si))
8g G(S;;;) ; // semantic distribution abstraction
9S0= [] ; // concrete states set
10fors2Sdo
11s0 g(s);
12S0:add(s0);
13T0 []; // concrete traces set
14for(si 1;si)2Tdo
15s0i 1 g(si 1);
16s0i g(si);
17T0:add(s0i 1;s0i);
18returnM(D;S0;T0;f);
A. Semantic proÔ¨Åling
Recent research on deep neural network models [ 14],
[15], [30] highlights that states and traces are efÔ¨Åcient for
understanding stateful model behaviors over data distribution.
A neural language model can be seen as a stateful model. The
LSTM -based model is one of the most typical neural language
models. Therefore, in order to analyze LSTM -based neural
language model behavior, we proÔ¨Åle the model to extract the
initial semantic states and traces as the Ô¨Årst step. We Ô¨Årst
explain the deÔ¨Ånition of state and trace in neural language
model analysis.
Suppose that we have an LSTM -based language model
R= (D;;f).Drefers to all sentences used for training.
fis the distribution of a language model, and is an internal
state extractor of the model that is used to transform each
word in a sentence to a state. For example, when we feed
a sequence ‚ÄúIan goes home at 6 pm on weekdays and goes
swimming at 7 pm every day.‚Äù to a LSTM -based language model
fwith 100 hidden units, we can obtain a list of hidden-state
vectors of LSTM with 100-dimension for each feed input word,
i.e., [[0:1;:::; 0:3];[0:2;:::; 1:3];[1:5;:::; 0:3];:::; [0:07;:::; 0:4]],
by using internal state extractor . Particularly, (home ) =
1005[1:5;:::; 0:3].
With the internal hidden state set, we construct a corre-
sponding state Ô¨Çow, i.e., trace, over two hidden states ordered
chronologically. The trace represents a transition relation for a
pair of consecutive hidden states. In our illustrative example,
the trace between hidden state ‚Äúgoes‚Äù and state ‚Äúhome‚Äù is
represented by ( (goes); (home)).
In Algorithm 1, we Ô¨Årst deÔ¨Åne two empty sets (Line 1 and 2)
for hidden states Sand tracesT. We then iterate each sentence
Win the training data and extract the state and trace of each
wordw(Line 3 to 7). Particularly, at the i-th timestamp t,
each word in a sentence is transformed to a state siusing the
internal state extractor . A trace is accordingly extracted to
(si 1;si). Finally, we construct a state set Sand trace set T
for the whole training data Dand deÔ¨Åne it as an initial model.
B. Semantic distribution abstraction
After semantic proÔ¨Åling, we obtain an initial model to
represent the LSTM -based neural language model behaviors
over training data. However, such a granular representation
contains a plethora of discrete states and traces. For example,
anLSTM -based neural language model potentially produces up
to 100 thousand states and 900 thousand traces for a corpus
containing 10,000 sentences with an average length 10 of words.
It is impractical to understand the internal behavior of a given
model with such a huge number of states and traces. Therefore,
in this step, we abstract the semantic distribution of a given
language model from the perspective of states and traces.
1)Automated dimension reduction :The dimension of
each initial state generated by semantic proÔ¨Åling is equal to
the number of hidden units in LSTM core, which usually
is very high. It is hard to Ô¨Ånd the latent characteristics
over high dimensional space since the distribution of data
with high dimension tends to be sparse [ 31]. Therefore, we
Ô¨Årst automatically reduce the dimension of each initial state
generated by semantic proÔ¨Åling to an optimal number. Du et
al. [14] applied Principal Component Analysis (PCA) to reduce
the dimension of semantic space to a small number, in order to
efÔ¨Åciently Ô¨Ånd the common correlation over states. However, an
obvious limitation in their approach exists. When the dimension
of an initial state is high, arbitrary dimension reduction
may lead to a huge information loss. The information loss
from modeling may potentially introduce a signiÔ¨Åcant bias in
memorization-analysis-oriented model construction. To improve
the memorization-analysis-oriented model construction, we use
a classic metric, Relative Information Loss [ 32], to measure
the information loss during dimension reduction. In detail, we
have a number of nvectorsVand each vector is with m-
dimension space, i.e., [v0;v1;:::;vm]. We want to transform
theninitial vectors to vectors ^V, and each transformed vector
is withk-dimension, i.e., [^v0;^v1;:::;^vk]. The corresponding
information loss is deÔ¨Åned as  (k).
In order to overcome the aforementioned limitation, we take
information loss into account for dimension reduction in order
to secure the utility of the transformed internal state. We seta threshold to control information loss, and the decision
process of Ô¨Ånding the optimal kcan then be deÔ¨Åned as:
arg min
kj (k) j (2)
Finally, this step outputs intermediate states and each state is
kdimensions. In our example, we reduce the 100-dimension of
each state to three dimensions. For example, the word ‚Äúhome‚Äù
would be with a reduced initial state [1:5;0:7;0:3].
2)Semantic Clustering :To identify the latent characteris-
tics over the intermediate states, we apply a clustering algorithm
(DBSCAN) to group together intermediate states that are close
to each other in terms of cosine distance threshold and
minimum number of neighbors . DBSCAN-based clustering
is suitable for data with an arbitrary shape [ 33].speciÔ¨Åes
for the minimum cosine distance which two intermediate state
points should be considered as neighbors. determines the
minimum number of neighbors to be deÔ¨Åned as a core state.
Each core state and its neighbors form a cluster labeled as a
concrete state. In our running example, the words ‚Äúhome‚Äù and
‚Äúswimming‚Äù are grouped into one cluster. Therefore, we would
label the hidden states of the words ‚Äúhome‚Äù and ‚Äúswimming‚Äù
as a single identical concrete state.
C. Memorization-analysis-oriented model construction
With the concrete states from the clustering, we construct a
Ô¨Ånal memorization-analysis-oriented model. We Ô¨Årst transform
the high-dimensional initial states into intermediate states with
an optimal dimension. We then transform the intermediate
states to concrete states. Note from Algorithm 1, we deÔ¨Åne an
abstraction function Gto abstract the initial states and traces
(Line 8). The inputs of the function Gare the initial states,
and three threshold values, i.e., information loss threshold ,
the number of cores , and distance threshold . We then
initialize two sets S0(Line 9) and T0(Line 13) for concrete
states and traces, respectively. Next, for each initial state si,
we use the deÔ¨Åned semantic distribution abstraction to abstract
the state to s0i(Line 10 to 12). Similarly, for each initial
trace (si 1;si)composed of two states si 1andsi, we apply
the same abstraction function to abstract the two states to
s0i 1ands0i. We then connect the two abstracted states into
a concrete trace (s0i 1;s0i)(Lines 14 ‚Äì 17). The Ô¨Ånal output
is the memorization-analysis-oriented model (Line 18). In
our running example, the Ô¨Ånal memorization-analysis-oriented
model is represented by the concrete state and trace set.
V. M EMORIZATION DISTRIBUTION BINDING
Prior studies [ 8], [27] have reported that memorization
is a severe issue in language models. To achieve a good
performance, a model all too often intends to remember the
training data during the training process instead of learning
the latent characteristics. Regularization techniques, such as
dropout and batch normalization, aim to solve the model
overÔ¨Åtting issue and improve the generality of AI models.
Although the regularization techniques are widely adopted for
training a complicated model, e.g., an LSTM-based model,
the models may still memorize part of the training data [ 27].
1006Such memorization might be exploited to extract private data
from a given language model. Therefore, in this section, we
quantify the memorization behavior in a memorization-analysis-
oriented model, i.e., the output from Section IV. The details
for analyzing memorization behavior are shown in Algorithm 2.
Given a memorization-analysis-oriented model and training
data as input, we bind the memorization distribution by building
a semantic Markov model to map the memorization-analysis-
oriented model to training data. In particular, we Ô¨Årst extract
memorization. We then build a Ô¨Årst-order Markov model to
represent the semantic memorization distribution.
A. Memorization Extraction
From our memorization-analysis-oriented model generated
in Section IV, we obtain the Ô¨Ånal concrete states and traces for
each sentence in the training data. However, such states and
traces cannot be applied to quantify memorization behavior
of a given language model directly. Therefore, to quantify the
memorization behavior efÔ¨Åciently, we Ô¨Årst deÔ¨Åne a memoriza-
tion concept called a memorization sequence. Given a language
model R= (D;;f)and a preÔ¨Åx c, a string of lwith length
Nis considered to be a memorization sequence if such a string
is equal to:
arg max
l0:jl0j=NR(l0jc) (3)
wherecandlare both from the training corpus. In our example,
given a preÔ¨Åx c‚ÄúIan goes‚Äù, a language model would predict a
string ‚Äúhome at 6 pm on weekdays‚Äù as the most likely output.
We call a string such as ‚Äúhome at 6 pm on weekdays‚Äù a
memorization sequence based on the preÔ¨Åx ‚ÄúIan goes‚Äù.
With memorization sequences, we classify the concrete
statejtrace from the memorization-analysis-oriented model into
two types, i.e., memorization state jtrace and non-memorization
statejtrace. If a state jtrace is visited by any memorization
sequence, we consider the state jtrace to be a memorization
statejtrace. Otherwise, it is a non-memorization state jtrace.
Finally, we can construct a semantic distribution for all
the concrete states and traces in terms of memorization. In
Algorithm 2, we Ô¨Årst initialize two dictionaries, MTandMS, to
represent memorization traces and states, respectively (Line 1
and Line 2). We also initialize two dictionaries, ATandASfor
all the concrete traces and states output from Section IV (Line
3 and Line 4). Next, we iterate each sentence in the training
data to abstract state and trace for each word. If an abstracted
statejtrace is visited by a memorization sequence, we label the
statejtrace to a memorization state jtrace (Line 6 to Line 15).
In our running example, the concrete state corresponding to
‚Äúhome‚Äù is classiÔ¨Åed as a memorization state.
B. Semantic Memorization Modeling
With the memorization states and traces, we build a Ô¨Årst-
order Markov model to learn the memorization semantic distri-
bution conditioned on the state from the last step. Sequential
behavior can be regarded as a discrete-time Markov chain.
Therefore, the memorization probability over a sequence can
be modeled by a Ô¨Årst-order Markov model [34].Algorithm 2: Memorization analysis algorithm
input :M= (D;T;S;f ): memorization-analysis-oriented model,
g: abstraction transformation function,
H: memorization sequence abstraction function,
D: sentences
output :E(;): Ô¨Årst-order Markov memorization model
1MT fg ; // a dictionary of memorization traces
2MS fg ; // a dictionary of memorization states
3AT fg ; // a dictionary of concrete traces
4AS fg ; // a dictionary of concrete states
5h H(D;f);// function to check if an input is memorization trace
6forW2D do // loop every sentence to extract states and traces
7 s2[i(W[:i])]jWj
i=0;
8 fori21:::jWjdo
9s0i 1 g(si 1);
10s0i g(si);
11AT[(s0i 1;s0i)] + + ;
12AS[(s0i)] + + ;
13 ifh(si 1;si) ==True then
14 MT[(s0i 1;s0i)] + + ;
15 MS[(s0i)] + + ;
16for(si 1;si)2AT do
17E(si 1;si) AT(si 1;si)P
jAT[(si 1;sj)];
18forsi2STdo
19E(si) MS[(si)]
AS[(si)];
20returnE(;);
1)Memorization statejtrace distribution construction :
We calculate two probabilities representing the memorization
state probability Pr(si)and trace probability Tr(si 1;si).
To compute the memorization state probability, we count the
number of times a memorization state is visited by any sequence
(memorization sequence and non-memorization sequence) as
the denominator and the number of times a memorization
state is visited by the extracted memorization sequences from
Subsection V-A as numerator. Trace probability Tr(si 1;si)
refers to how likely state si 1reaches state si. In Algorithm 2,
we calculate two such probabilities for each sentence in Lines
16 to 19. For example, the concrete state corresponding to
‚Äúhome‚Äù is visited by a total of 100 memorization sequences
and a total of 300 sequences. Therefore, the probability
of memorization to a concrete state (memorization state)
corresponding to ‚Äúhome‚Äù is 1/3 (100/300).
2)Construction of memorization sequence distribution :
We calculate the memorization sequence probability based on
Pr(si)andTr(si 1;si). For a given sequence lconsisting
ofnwords, we can extract nstates scorresponding to each
word. Based on the chain rule and Ô¨Årst-order assumptions, the
memorization probability of the given lcan be computed as:
Pr(s) =nY
i=1Tr(si 1;si)Pr(si) (4)
whereTr(s0;s1) = 1 . In the rest of this paper, we refer to the
Ô¨Årst-order Markov memorization model as a semantic model.
VI. A DDRESSING MEMORIZATION ISSUES USING THE
MEMORIZATION MODEL
Finally, we leverage our Ô¨Årst-order Markov memorization
models that are based on the last step to address the memo-
rization issues. In particular, our approach Ô¨Årst automatically
1007assesses the risk of data leakage due to memorization issues.
Next, our approach assists in the dememorization of the neural
language models.
A. Data leakage risk assessment
A language model potentially poses the risk of remembering
unintended information from its training data. To assess the
training data leakage risk, we predict whether a sequence from
the test data exists in the training data based on our Ô¨Årst-order
Markov memorization model.
In the Ô¨Årst step, for each sentence in the test data, we
extract the initial states based on the state extraction approach
presented in Subsection IV-A . It is rare to have two identical
semantic states from training and test data in an LSTM network.
Therefore, we map each state of test data to the closest state
extracted from the training data by searching the nearest
neighbor based on cosine distance.
Second, we connect all the consecutive semantic states to
form a sequence. We use the Ô¨Årst-order Markov model to
calculate the memorization probability of each sequence. If
the memorization sequence has a high probability, we consider
that the sequence would exist in the training data, resulting in
a possible data leakage. We use such uncovered possible data
leakage to assess the memorization issues from the original
neural language models.
B. Assisting in dememorization
To assist in dememorization, we mutate the sentences in the
training data that are most likely to lead to data leakage and
re-build our semantic model to know whether the mutation
mitigates the unintended memorization behavior. The goal of
our approach is to mutate the data-leaking sentences while
minimizing the impact on the data without leakage risks. For
each sentence, we leverage the memorization probability that
is generated from our approach to decide whether to mutate
the sentence. In short, we only mutate the sentences with high
memorization probability and retrain the neural language model
from the data after mutation for dememorization.
VII. E VALUATION
A. Experimental setup
We evaluate our approach based on one of the state-of-the-
art word level LSTM -based language models [ 19] with 3,000
hidden nodes on three popular large datasets, namely, WikiTest-
103 [ 16], WMT2017-en [ 17], and IWSLT2016-en [ 18]. An
overview of these datasets is given in Table I. The training data
is disjoint from the test data. Our experimental environment
is based on a server with 16 24GB-GPUs, 500 GB of RAM,
and 1 TB disk. The server runs Ubuntu Linux, version 20.04.
Table II shows the runtime of each stage of our proposed
approach over different datasets. Adding a regularization setup
parameter, each memorization-analysis-oriented model only
needs to be constructed once to assess the data leakage of one
AI model.TABLE I: Overview of our datasets
Dataset Sentences Unique Words
Train 1M 220KWikiText-103Test 100K 220K
Train 4M 798KWMT2017-enTest 12K 40K
Train 177K 59KIWSLT2016-enTest 19K 15K
TABLE II: Overview of time cost for each step
Sem.
proÔ¨ÅlingDim.
reductionSem.
clusteringMem.
abstractionSem. mem.
modeling
W-103 0.25h 0.15h 4h 1.5h 0.1h
WMT 0.55h 0.62h 15h 5.8h 0.3h
IWSLT 0.08h 0.08h 1h 0.7h 0.07h
Sem. is abbreviation of semantic. Mem. is abbreviation of memorization.
B. Preliminary analysis
Given a language model, if the memorization data appears to
have no inherent common patterns or characteristics, the data
would not be prone to data leakage issues, i.e., would not be
suitable to our study. Therefore, before applying our approach
to the three neural language models from the three datasets,
we aim to understand the characteristics of the memorization
sequences in the three neural language models.
Carlini et al. [27] Ô¨Ånd that a sentence with low perplexity
is likely to be vulnerable to encounter an attack involving
data leakage, where perplexity indicates how well a trained
language model Ô¨Åts the distribution of sentences. It is deÔ¨Åned
as the inverse probability of the sentences, normalized by the
number of words. Formally, given a sequence l=WN
1, the
perplexity is deÔ¨Åned as follows:
PP(WN
1) =P(w1w2w3:::wN) 1
N
=NvuutNY
i=11
p(wijw1w2:::wi 1))(5)
wherewiis thei-th word in this sequence. Pindicates
the probability of a sentence. From Equation 5, a lower
perplexity value indicates a better performing language model.
We summarize the perplexity distribution over each sentence
in the training data. If a model assigns a high probability to
a sentence, it is likely that the model tends to remember this
sentence. Therefore, we also study the relationship between
perplexity and the length of a memorization sequence in each
sentence.
Result: Most of the sentences in the training data have
low perplexity. Figure 2 shows the perplexity distribution over
the three training datasets, WikiTest-103 (a), WMT2017-en
(b), and IWSLT2016-en (c). Prior studies [ 35], [36] report that
a language model with a perplexity below 100 is considered
a well-performing model. In particular, considering the prior
study [ 35] using the same training data, the authors report that
their language model achieves a perplexity of 34.4 for WikiText-
103. We Ô¨Ånd that most of the sentences in the training data have
low perplexity. In particular, at least 96% of the sentences have
a perplexity less than 100 in our three experimental datasets.
1008Such a result implies that the trained language model can
remember most of the sentences from the training data.
0.000.010.020.030.04
0 50 100 150 200 250
PerplexityDensity of sentence
(a) WikiText-103.
0.000.010.020.03
0 50 100 150 200 250
PerplexityDensity of sentence (b) WMT2017-en.
0.000.020.040.06
0 50 100 150 200 250
PerplexityDensity of sentence (c) IWSLT2016-en.
Fig. 2: Density distribution of number of sentences over
perplexity.
The sentences with a longer memorization sequence have
lower perplexity. Figure 3 shows the density of the length of
memorization sequences in terms of perplexity over the training
data. The X-axis indicates the perplexity (with increasing steps
of 50). The Y-axis shows the density of length of memorization
sequences. Note in Figure 2 and Figure 3 that most of the
memorization sequences with low perplexity ( <50) contain
at least six words. Such results imply that the sentences in
the training data that have longer memorization sequences are
easier to be remembered by the language model.
0123456789101112
0 50 100 150 200 250
PerplexityAverage length of sentence
(a) WikiText-103.
01234567891011121314
0 50 100 150 200 250
PerplexityAverage length of sentence (b) WMT2017-en.
012345678910111213
0 50 100 150 200 250
PerplexityAverage length of sentence (c) IWSLT2016-en.
Fig. 3: The average length of memorization sequence distribu-
tion in terms of perplexity over three datasets.
Summary of preliminary analysis: Most of the sentences
in the studied datasets have low perplexity, which shows
that the subject neural language model may be prone to the
memorization issue.
C. Results
RQ1: To what extent are the studied neural language
models prone to memorization issues?
Motivation: In our preliminary analysis, our results show that
most of the sentences in the studied datasets have low perplexity
and such sentences with low perplexities may be prone to be
remembered by neural language models. As such, one can
model the memorization distribution and exploit the learned
memorization to extract and store the valuable training data.
Therefore, in this research question, we want to explore to
what extent the studied neural language models are prone to
memorization issues.
Approach: To answer RQ1, we Ô¨Årst want to know the preva-
lence of potential memorization issues in our studied datasets.
If a state jtrace is a memorization state jtrace, such a state jtraceis a potential memorization issue. To quantify the potential
memorization issue, we deÔ¨Åne two metrics, SCR andTCR, to
evaluate our memorization-analysis-oriented model. SCR is the
memorization state coverage rate and TCR is the memorization
trace coverage rate. The name state jtrace memorization implies
that the state jtrace is visited by a memorization sequence.
Formally, SCR is deÔ¨Åned asNumMS
Numstate, and TCR is deÔ¨Åned as
NumMT
Numtrace.NumMS is the number of distinct memorization
states andNumMT is the number of distinct memorization
traces.Numstate andNumtrace refer to the total of
distinct concrete states and traces, respectively. We follow
the following steps to calculate the two metrics, SCR andTCR.
We Ô¨Årst apply the proposed modeling approach in Section IV to
obtain the memorization-analysis-oriented model from training
data. Second, we employ the memorization extraction approach
from Section V-A to extract the memorization sequences from
training data. Next, for each word in a memorization sequence,
we can map it to the semantic model to obtain the memorization
state and trace.
Memorization states jtraces can be visited by both memoriza-
tion sequences and non-memorization sequences. The more
memorization sequences visit a state jtrace, the more likely such
a state jtrace is prone to memorization issues. Therefore, we
also quantify the memorization issues of our studied datasets
using memorization state and trace probability. We calculate
memorization state and trace probabilities using the approach
presented in Section V-B. The higher the memorization
statejtrace probabilities are, the more possible such a state jtrace
is prone to memorization issues.
Result: Only a small portion of states and traces from
training data are related to memorization. The result of the
state and trace coverage rate is shown in Table III. In the table,
the columnis the input of the clustering algorithm DBSCAN
used to control the granularity of clusters. The result shows
that most of the states and traces are unrelated to memorization.
The state coverage rate ranges from 6.8% to 24.5%. The traces
coverage rate is less than 4.03% in any of different inputs of
core. The results show that only a small percentage of states
and traces are related to memorization. Such results imply that
either 1) there are only a few memorization issues or 2) there
exist many memorization issues, and such memorization issues
only cover a small percentage of memorization states=traces.
In addition, our approach can efÔ¨Åciently reduce the number
of initial states and traces. For example, when using a of
100 as input for our clustering algorithm DBSCAN, we reduce
the initial millions states into 40,121 concrete states. Such a
considerable number of states cannot only be used to analyze
memorization behavior of a language model, but can also be
used to retain most of the semantic information.
The memorization states and traces have a considerable
high memorization probability. Figure 4 shows the results
of the probability distribution of the memorization states
and traces over the three studied datasets. Although only
low percentages of states (an average of 17.6%) and traces
(an average of 2.24%) are related to memorization, the
memorization state and trace probabilities are comparably
1009TABLE III: Results of memorization state and trace coverage
rate. (Mem. is the abbreviation for ‚Äúmemorization‚Äú.)
DatasetAll concrete
statesAll concrete
tracesMem. states Mem. traces TCR SCR
W-103100 40,121 31,9450 3,258 6,740 1.02% 16.8%
150 79,820 521,460 18,518 16,060 3.08% 23.2%
200 82,317 613,419 16,792 11,593 1.89% 20.4%
250 89,012 634,210 17,534 9196 1.45% 19.7%
WMT100 63,902 549,872 7,221 6,653 1.21% 11.3%
150 71,921 673,219 17,620 13,666 2.03% 24.5%
200 76,709 778,895 12,426 14,721 1.89% 16.2%
250 77,101 792,015 5,244 6,256 0.79% 6.8%
IWSLT100 4,523 26,217 557 738 2.81% 12.3%
150 8,945 114,084 1,923 4,598 4.03% 21.5%
200 11,219 139,930 2,546 4,886 3.49% 22.7%
250 14,234 178,904 2,246 5,831 3.26% 15.8%
high. Especially, the mean memorization state probability in
dataset WikiText-103 is 0.63. By inspecting our results, we Ô¨Ånd
that the memorization-analysis-oriented model can identify the
memorization transition of the LSTM -based language model
and discover potential memorization issues in the training data.
Answer to RQ1: Only a small percentage of states and
traces from training data are related to memorization. How-
ever, the memorization states and traces have a considerably
high memorization probability.
RQ2: How accurate is our approach in the data leakage
risk assessment?
Motivation: In RQ1, the results show that the memorization
states and traces tend to be remembered due to a considerable
high memorization probability. The associated distribution can
be used to analyze the training data and the potential for data
leakage. In order to illustrate a practical impact, we leverage
our approach to assess training data leakage risk based on a
given language model. In this research question, we want to
answer how accurate our privacy risk assessment approach is.
Approach: In Section V, we have built a Ô¨Årst-order Markov
memorization model. To realistically assess the privacy risk
of given data, we use the constructed model to measure the
memorization probability of each sequence in the test data.
Based on the predicted memorization probability of each
sequence in the test data, which is not seen by the model
during the training phase, we predict whether a sequence of
test data likely exists in the training data.
Furthermore, the length of a memorization sequence might
affect the modeling analysis. For example, one may argue
that the shorter a memorization sequence is, the more likely
the sequence appears in the training data. Therefore, we
calculate the Pearson correlation [ 37] between the length of
memorization sequences and memorization probabilities of
sequences. Pearson correlation ranges from -1 to +1. A value
of 1 indicates that the length and memorization probability
of sequences has a strong relationship. A value of 0 indicates
that there is no relationship between them, and a value of -1
indicates an inverse relationship between them.
We implement a baseline approach that assigns a randomscore to each of the extracted memorization sequences. We
compare DeepMemory to the baseline in this research question.
To measure the performance, we examine whether the extracted
sequences from the test data appear in the training data. If
a sequence is indeed in the training data, we consider it
as a true-positive sequence. Otherwise, it is a false-positive
sequence. The true-positive sequence is considered to be data
leakage from training data. We use four metrics to evaluate our
approach, including precision, recall, F1, and AUC. Precision
measures the correctness of our model, and refers to the ratio
of cases when a predicted sequence is actually in the training
data. Recall measures the completeness of our approach, and
is deÔ¨Åned as the number of sequences that were correctly
predicted as memorization divided by the total number of
memorization sequences in the test data. F1is the harmonic
mean of precision andrecall. AUC allows us to measure the
overall ability of our approach. The AUC is the area under
the ROC curve, which indicates the performance of a binary
model as its discrimination is varied.
Result: Our data leakage assessment approach can achieve
an average AUC of 73%. Table IV shows the results
for precision, recall, F1, and AUC over the memorization
distribution. Note from Table IV that our approach achieves
an average precision of 47% and a very high average recall of
92% when taking 0.5 as a threshold, which outperforms the
baseline approach, i.e., a precision of 0.38 and a recall of 56%.
The results imply that a sequence with a high memorization
probability in the test data tends to be memorized. However,
different thresholds may lead to different results. To overcome
this bias, we also present the AUC of our approach. We Ô¨Ånd that
the AUC is high with an average value of 73%. The results
suggest that our proposed Ô¨Årst-order memorization Markov
model approach is capable of assessing data leakage risks.
TABLE IV: Results of using our approach to predict the
memorized sequence compared with the baseline approach.
DeepMemory Baseline
Precision Recall F1 AUC Precision Recall F1 AUC
W-103 0.50 0.75 0.60 0.72 0.38 0.50 0.42 0.48
WMT 0.29 1.00 0.44 0.67 0.30 0.57 0.40 0.50
IWSLT 0.62 1.00 0.76 0.80 0.50 0.60 0.54 0.48
Average 0.47 0.92 0.60 0.73 0.39 0.56 0.45 0.49
Our approach shows a similar performance for all types
of sequences. The Pearson correlation between length and
memorization probability of memorization sequence is 0.14.
An absolute value of 0-0.19 is regarded as a very weak
correlation [ 37]. Therefore, a very weak relationship exists
between the length of a memorization sequence and the
memorization probability of sequences.
Our approach can be used to efÔ¨Åciently identify a real-
world data leakage issue. In order to demonstrate the practical
usefulness of our approach, we want to examine whether our
approach can be used to identify real-world private data. We
train a language model based on the setting from [ 8]. Similar
to the prior work [ 8], we make the trained language model
10100.000.250.500.751.00
0.0 0.2 0.4 0.6 0.8
Memorization state probabilityDensitymin        0.15
median  0.63mean     0.63max       0.79(a) WikiText-103 state.
0.000.250.500.751.00
0.2 0.4 0.6
Memorization trace probabilityDensitymin        0.04
median  0.55mean     0.52max       0.69 (b) WikiText-103 trace.
0.000.250.500.751.00
0.00 0.25 0.50 0.75 1.00
Memorization state probabilityDensitymin        0.01
median  0.25mean     0.28max       1.00 (c) WMT2017 state.
0.000.250.500.751.00
0.00 0.25 0.50 0.75 1.00
Memorization trace probabilityDensitymin        0.01
median  0.24mean     0.25max       1.00 (d) WMT2017 trace.
0.000.250.500.751.00
0.00 0.25 0.50 0.75 1.00
Memorization state probabilityDensitymin        0.01
median  0.14mean     0.19max       1.00 (e) IWSLT2016 state.
0.000.250.500.751.00
0.00 0.25 0.50 0.75 1.00
Memorization trace probabilityDensitymin        0.01
median  0.20mean     0.23max       1.00 (f) IWSLT2016 trace.
Fig. 4: Memorization states and traces probability distribution.
remember the sequence ‚Äúthe credit number is 281265017 ‚Äù.
After that, we analyzed this language model based on our
proposed approach. During the testing phase, we test our
semantic Markov memorization model on a set of sentences
with the same structure but different credit numbers. We Ô¨Ånd
that the sentence ‚Äúthe credit number is 281265017 ‚Äù has the
highest memorization probability. Note that the prior work has
reported that memorization is not overÔ¨Åtting [ 8]. The result
suggests that our proposed model can efÔ¨Åciently detect the
memorization content from the training data.
Answer to RQ2: Our data leakage assessment approach
can achieve an average AUC of 73%. Our approach shows
a similar performance for all types of sequences. Our
approach can be used to efÔ¨Åciently identify real-world
private data.
RQ3: How effective is our approach in assisting dememo-
rization?
Motivation: One may randomly select sentences and mutate
them to reduce memorization sequences probability (see
Equation 4). However, it is not an optimal solution to mutate a
large portion of the training data since the mutation would hurt
the quality of the data, leading to unrealistic models. On the
other hand, if one only randomly mutates a small portion of the
training data, the mutated data may not contain memorization
issues. In this research question, we want to evaluate whether
our approach can assist in dememorization by suggesting only
a small portion of data in the training data to be mutated.
Approach: We compare the use of our approach in assisting
dememorization to a random baseline approach. We Ô¨Årst apply
our approach to detect the memorization sequences from the
training data and to select memorization sequences. The results
of RQ2 show that when using 0.5 as the threshold to predict
memorization sequence, our recall is very high (close to 1).
Therefore, we select memorization sequences to be mutated if
their probabilities are more than 0.5. For the random approach,
we randomly select 50% of all the memorization sequences
to be mutated. We choose 50% for the baseline approach in
order to give the baseline approach an overestimated ability of
mutating the training data. 50% also ensures that at least half of
the existing training data is not mutated. In both experiments,
we ignore memorization sequences with lengths less than four.
Second, we use four strategies to mutate the aforementioned
selected sequences from the original training data to mitigateunintended memorization behavior.
REPlacing Word (REPW): For each extracted sequence,
we Ô¨Årst select the noun and verbal phrase that occurred
less frequently. We then replace the selected words with
their synonyms in the training data randomly. If there
are no synonyms in the training data, we replace them
with a random external synonym. Next, we modify the
corresponding sentences that contain mutated sequences.
REOrdering Sequence (REOS): Prior research [ 2], [36]
shows that sequence disorder can beneÔ¨Åt the robustness
of a sequential model in machine translation tasks and
industrial recommendation system applications. This strat-
egy aims to reorder words in memorization sequences to
confuse the language models.
REMoving Word (REMW): For the sentences that con-
tain memorization sequences, we remove those sequences
directly from the sentences.
MIXture (MIX) : Different strategies may have their
advantages. In the mixture strategy, we combine the
replacing words and reordering sequences approaches.
Next, we re-train a language model based on the mutated
training data and re-build our semantic Ô¨Årst-order Markov
memorization model. Finally, we use our semantic model to
analyze the memorization behavior of the re-trained neural
language model on the original training data. In particular,
we extract the memorization sequences of re-trained neural
language models. We then calculate how many memorization
sequences in the original model (before mutation) still exist
in the re-trained model. The fewer memorization sequences
that are left, the better dememorization the re-trained model
has. We also calculate the number of mutated memorization
sequences from both our approach and the random baseline. The
desired approach would achieve a low number of memorization
sequences that are left in the re-trained model, while only
having to mutate a small percentage of memorization sequences.
Result: Our approach can assist in dememorization with-
out the need to mutate a large number of memorization
sequences. Table V shows the results for memorization
sequence statistics after re-training the language model using
different strategies to mutate the training data. With assistance
from our approach, the memorization sequences can be signiÔ¨Å-
cantly reduced. Table V shows that, compared to the original
memorization sequences, the percentages of the memorization
sequences drop to 2.58%, 2.31%, and 4.43% in WikiText-103,
WMT2017, and IWSLT2016, respectively. Compared to our ap-
1011TABLE V: Total number of original memorization sequences and the number of memorization sequences after dememorization
assisted by our approach and the baseline approach.
Dataset Measure OriginalMutated Sequence (%) after REPW after REOS after REMW after MIX Average
Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random Prob.>0.5 Random
W-103 Mem. Seq. 59,802 4.10% 50%1,645
(2.75%)3,519
(5.88%)1,543
(2.58%)4,210
(7.04%)1,549
(2.59%)2,431
(4.07%)2,021
(3.38%)3,979
(6.65%)1,690
(2.83%)3,299
(5.91%)
WMT Mem. Seq. 124,319 0.89% 50%4,210
(3.39%)3,577
(2.88%)2,874
(2.31%)2,576
(2.07%)3,121
(2.51%)1,498
(1.20%)2,989
(2.40%)2,249
(1.81%)3,299
(2.65%)2,475
(1.99%)
IWSLT Mem. Seq. 18,753 2.80% 50%2,091
(11.15%)3,202
(17.07%)2,484
(13.25%)3,389
(18.07%)831
(4.43%)1,034
(5.51%)1,701
(9.07%)2,214
(11.81%)1,777
(9.47%)2,460
(13.12%)
Original is the number of memorization sequences in the original model. Mutated sequence means the percentage of memorization sequences to be mutated.
Columns starting with ‚Äúafter‚Äù mean after mutating the training data, the number of memorization sequence that are left and the corresponding percentage.
proach, the average of percentages of memorization sequences
that are left after the mutation from the random baseline
are 5.91%, 1.99%, and 13.12% in WikiText-103, WMT2017,
and IWSLT2016, respectively. Except for WMT2017, where
both approaches show a similar performance in reducing the
memorization sequences, our approach outperforms the baseline
approach by a wide margin.
Our approach only needs to mutate a very small
number of sequences from the training data. Table V
shows the number of memorization sequences that are mutated
during the dememorization process. The results illustrate that
our approach only mutates 4.1%, 0.89%, and 2.8% of the
original memorization sequences in the datasets WikiText-
103, WMT2017, and IWSLT2016, respectively. Such a small
number of mutations would have a trivial impact on the trained
model. By deÔ¨Ånition, the baseline approach mutates 50% of the
memorization sequences, i.e., a very large amount of mutation,
and cannot even achieve a comparable dememorization result.
Answer to RQ3: Our approach is capable of guiding
dememorization and does not decrease the performance
of the original model. Therefore, practitioners can use our
approach to discover sensitive data leakage risks and help
mitigate memorization.
VIII. C OMPARATIVE STUDY ON THE EFFECT OF
REGULARIZATION
In this section, we discuss the impact of regularization on the
memorization effect. Regularization is an efÔ¨Åcient approach to
train neural network based models. Although a prior study [ 8]
shows that memorization in neural language models is not
an issue of overÔ¨Åtting, the use of regularization may still
potentially affect the memorization behavior of neural language
models. Therefore, we conduct a comparative study over four
mainstream regularization techniques, including dropout, L1
norm, L2 norm regularization and data augmentation (DA).
We build an original model without any regularization. To
evaluate the impact of the regularization techniques, we create
four additional models by modifying our original model by
altering only one regularization technique, including enabling:
1) dropout, 2) L1 norm, 3) L2 norm, and 4) DA. In particular,
the augmentation is to randomly select 10% of the sentencesfrom the training corpus and randomly replace non-stop words
with one of their synonyms [38].
We follow a process similar to RQ1 to conduct our compar-
ative study. In particular, our experiment is executed with in
200. We Ô¨Årst calculate two metrics SCR andTCR from the four
additional models while altering the regularization techniques.
We then calculate their corresponding memorization state and
trace probabilities. Finally, we compare the results for the four
additional models with the results from our original models.
Results: Regularization may be able to mitigate the mem-
orization effect. The results (with and without regulariza-
tion) are shown in Table VI. The results show that without
regularization, the memorization state coverage rate ranges
from 23.1% to 31.4% and the memorization trace coverage
rate ranges from 7.43% to 11.32%. After regularization, both
the memorization state and the trace coverage rate decrease
considerably. Especially, the L2 norm regularization provides
the highest reduction in the memorization state and trace
coverage (19.8% and 1.89%, respectively).
In addition, we compare the memorization state and trace
probability distribution of the above four additional models
with the ones from the original models, using the Mann-
Whitney U test and Cliff‚Äôs delta. We Ô¨Ånd that all of the
probability distributions of the four additional models are
different with statistical signiÔ¨Åcance ( p < 0:05) from the
original model. However, the difference may differ among
different subjects. In particular, for WMT, the original models
(without regularization) always have a higher memorization
probability than the four additional models (positive effect
sizes). For IWSLT and W-103, the differences are associated
with rather negligible or small effect sizes; while cases also exist
where the probability distribution is lower with regularization
(e.g., W-103; enabling dropouts). Such results suggest that
regularization (in particular, the L2 norm) may be useful
to partially address memorization issues, but they cannot be
eliminated. More comparative work is required to highlight
the relative impact of the different approaches.
IX. T HREATS TO VALIDITY
External validity. A threat to the external validity is the
generalizability of our approach. Our study is evaluated on
the most popular neural language model, i.e., the LSTM -based
language model, and three speciÔ¨Åc public datasets. More case
1012TABLE VI: Results of memorization coverage rate with and
without regularization (Reg. means regularization).
Dataset Reg.All concrete
statesAll concrete
tracesMem.
StatesMem.
TracesTCR SCR
W-103Original 81,790 631,521 22,820 54,248 8.59% 27.9%
Dropout 83,210 647,932 18,639 16,003 2.47% 22.4%
L1 82,123 627,984 19,545 16,076 2.56% 23.8%
L2 80,789 642,983 17,531 12,152 1.89% 21.7%
DA 84,198 852,129 21,883 61,609 7.23% 26.0%
WMTOriginal 78,256 823,943 18,077 93,270 11.32% 23.1%
Dropout 72,198 878,134 13,212 18,528 2.11% 18.3%
L1 79,821 849,702 13,729 31,863 3.75% 17.2%
L2 76,213 851,203 15,090 23,578 2.77% 19.8%
DA 77,678 812,323 17,656 81,232 10.01% 22.7%
IWSLTOriginal 14,232 176,820 4,468 13,137 7.43% 31.4%
Dropout 11,950 122,561 3,274 5,172 4.22% 27.4%
L1 13,212 119,821 2,893 3,582 2.99% 21.9%
L2 12,792 98,996 2,533 4,237 4.28% 19.8%
DA 13,341 15,421 3,867 1,076 6.98% 28.9%
studies on other datasets in other neural network based language
models can beneÔ¨Åt the evaluation of our approach.
Internal validity. Our work uses several techniques, such as
the clustering algorithm DBSCAN, the dimension analysis
algorithm PCA, and the First-Order Markov model. Such
techniques can be replaced by other kinds of similar techniques.
For example, DBSCAN can be replaced with the k-means
clustering algorithm. Our approach also leverages threshold
values, for example, the andof the DBSCAN. To explore
the impact of these choices, we individually increased or
decreased the (omitted due to limited space) and (see
Table III) values in our experiment.
Construct validity. In the evaluation of our approach for
dememorization, we only used four strategies to mutate the
training data. Similar evaluation approaches based on mutation
techniques have been often used in prior research [ 39]. However,
there may exist other kinds of strategies to mutate the training
data. Future work can complement our evaluation.
X. R ELATED WORK
Analysis of DNN. Many prior studies [ 40], [41], [42], [43],
[44], [45], [46], [14], [15] have been proposed to analyze
and explain the behaviors of deep neural network. Functional
analysis and decision analysis are two main categories of
analysis of DNN [ 47]. Functional analysis, i.e., black-box
analysis, aims to capture the overall behavior by investigating
the relation between inputs and outputs [ 41], [43], [48].
Decision analysis takes the DNN as a white box and analyzes
the internal behavior by proÔ¨Åling internal structures and
component rolls [14], [15], [40].
In our study, we focus on the decision analysis, i.e., internal
behavior analysis. One of the typical techniques used to
analyze the internal behavior of a DNN model is Finite State
Automation (FSA) [ 49], [30]. FSA consists of states and
transitions, which can be mapped to the behavior of sequence
models. Du et al. [ 14] use an interval-based approach to cluster
the original hidden-state vector which produces comparable
performance under a scalable environment.
Prior studies focus on the analysis of behavior of the RNN
model and its variance in FSA for the natural language process-
ing task. However, there is a lack of work on memorization
issues for language models. Our paper is the Ô¨Årst work onanalyzing, detecting and assisting in repairing memorization
issues of RNN models.
General privacy of DNN. Extensive prior research has
revealed serious privacy issues posed by deep neural networks
as the data used for training can be leaked [ 50]. In general,
privacy threats of the deep neural network can be divided into
the two categories of direct and indirect information exposure
hazards [ 51]. Direct privacy data leakage is mainly due to the
data curator [ 52], [53], untrusted communication link [ 54] and
untrusted cloud [ 55]. In terms of the indirect privacy threat,
one would like to infer or guess information for training data
or model parameters without access to the actual data [ 56].
Many prior studies [ 9], [10], [11] have reported that deep
neural networks tend to memorize the training data instead
of learning the latent properties of the training data. Some
studies [ 10], [57], [58], [59] propose automatic techniques that
infer whether a given data instance has contributed to the target
model. Shokri et al. [ 57] propose the Ô¨Årst membership inference
attack to deduce whether a data record is used in the training
process for the targeted model. The core idea is to distinguish
a given record in terms of the conÔ¨Ådence score output by the
targeted model. In addition to membership inference, research
also aims to infer sensitive attributes for a released model [ 50],
[60], [61] and to steal model parameters [ 56], [62], [63], [64].
Prior studies develop attacks and defenses for investigating
various privacy challenges. Different from previous work, we
consider a privacy breach related to memorization in neural lan-
guage models and analyze memorization via abstracted hidden
states from the extracting Ô¨Ånite state machine. Our approach
aims to address privacy issues during the quality assurance
process for developing AI models, instead of defending against
such attacks after the fact. Our work contributes to the area of
general privacy of deep neural networks.
XI. C ONCLUSION
This paper proposes DeepMemory, a novel approach for
analyzing the internal memorization behavior in language
models. We construct a memorization-analysis-oriented model
and build a semantic Ô¨Årst-order Markov model to analyze
memorization distribution. We evaluate our approach based on
one of the most popular neural language models, the LSTM -
based language model with three public datasets, namely,
WikiText-103, WMT2017, and IWSLT2016. The results show
that using our approach, we can address memorization issues
by automatically identifying data leakage risks with an average
AUC of 0.73. Based on the assessment results, our approach
can assist in dememorization by only mutating a very small
percentage (4.1%, 0.89% and 2.8%) of the training data to
reduce the memorization in the neural language models. Our
work calls for future research to address the privacy issues in
neural language models.
XII. A CKNOWLEDGEMENTS
We would like to thank Thomas Vannet for his insightful
feedback on an earlier version of this work. Furthermore, we
thank the anonymous reviewers for their valuable comments.
1013REFERENCES
[1]A. M. Rush, S. Chopra, and J. Weston, ‚ÄúA neural attention model for
abstractive sentence summarization,‚Äù in 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP). The Association
for Computational Linguistics, 2015, pp. 379‚Äì389.
[2]Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,
M. Krikun, Y . Cao, Q. Gao, K. Macherey et al., ‚ÄúGoogle‚Äôs neural
machine translation system: Bridging the gap between human and
machine translation,‚Äù arXiv preprint arXiv:1609.08144, 2016.
[3]M. Pak and S. Kim, ‚ÄúA review of deep learning in image recognition,‚Äù
in2017 4th international Conference on Computer Applications and
Information Processing Technology (CAIPT). IEEE, 2017, pp. 1‚Äì3.
[4]B. Huval, T. Wang, S. Tandon, J. Kiske, W. Song, J. Pazhayampallil,
M. Andriluka, P. Rajpurkar, T. Migimatsu, R. Cheng-Yue et al., ‚ÄúAn
empirical evaluation of deep learning on highway driving,‚Äù arXiv preprint
arXiv:1504.01716, 2015.
[5]S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, ‚ÄúA survey of deep
learning techniques for autonomous driving,‚Äù Journal of Field Robotics,
vol. 37, no. 3, pp. 362‚Äì386, 2020.
[6]Y . Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin,
J. Dillon, B. Lakshminarayanan, and J. Snoek, ‚ÄúCan you trust your
model‚Äôs uncertainty? Evaluating predictive uncertainty under dataset
shift,‚Äù Advances in Neural Information Processing Systems, vol. 32, pp.
13 991‚Äì14 002, 2019.
[7]L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
L. Li, Y . Liu et al., ‚ÄúDeepgauge: Multi-granularity testing criteria for deep
learning systems,‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, 2018, pp. 120‚Äì131.
[8]N. Carlini, C. Liu, ¬¥U. Erlingsson, J. Kos, and D. Song, ‚ÄúThe secret sharer:
Evaluating and testing unintended memorization in neural networks,‚Äù
in28th USENIX Security Symposium (USENIX Security). USENIX
Association, 2019, pp. 267‚Äì284.
[9]D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
T. Maharaj, A. Fischer, A. Courville, Y . Bengio et al., ‚ÄúA closer look at
memorization in deep networks,‚Äù in International Conference on Machine
Learning. PMLR, 2017, pp. 233‚Äì242.
[10] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei, ‚ÄúTowards demystifying
membership inference attacks,‚Äù arXiv preprint arXiv:1807.09173, 2018.
[11] C. Meehan, K. Chaudhuri, and S. Dasgupta, ‚ÄúA non-parametric test to de-
tect data-copying in generative models,‚Äù arXiv preprint arXiv:2004.05675,
2020.
[12] S. Ovaska, ‚ÄúData privacy risks to consider when using AI.‚Äù
[Online]. Available: https://www.fm-magazine.com/issues/2020/feb/
data-privacy-risks-when-using-artiÔ¨Åcial-intelligence.html
[13] X. Zhang, X. Xie, L. Ma, X. Du, Q. Hu, Y . Liu, J. Zhao, and M. Sun,
‚ÄúTowards characterizing adversarial defects of deep learning software
from the lens of uncertainty,‚Äù in 2020 IEEE/ACM 42nd International
Conference on Software Engineering (ICSE). IEEE, 2020, pp. 739‚Äì751.
[14] X. Du, X. Xie, Y . Li, L. Ma, Y . Liu, and J. Zhao, ‚ÄúDeepstellar:
Model-based quantitative analysis of stateful deep learning systems,‚Äù in
Proceedings of the ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering
(ESEC/SIGSOFT, FSE), 2019, pp. 477‚Äì487.
[15] X. Zhang, X. Du, X. Xie, L. Ma, Y . Liu, and M. Sun, ‚ÄúDecision-guided
weighted automata extraction from recurrent neural networks,‚Äù in Thirty-
Fifth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI). AAAI Press,
2021, pp. 11 699‚Äì11 707.
[16] T. Wolf, Q. Lhoest, P. von Platen, Y . Jernite, M. Drame, J. Plu,
J. Chaumond, C. Delangue, C. Ma, A. Thakur, S. Patil, J. Davison,
T. L. Scao, V . Sanh, C. Xu, N. Patry, A. McMillan-Major, S. Brandeis,
S. Gugger, F. Lagunas, L. Debut, M. Funtowicz, A. Moi, S. Rush,
P. Schmidd, P. Cistac, V . Mu Àástar, J. Boudier, and A. Tordjmann, ‚ÄúDatasets,‚Äù
GitHub. Note: https://github.com/huggingface/datasets, vol. 1, 2020.
[17] O. Boja et al., 2017 Second Conference on Machine Translation (WMT17):
Proceedings. Association for Computational Linguistics, 2017.
[18] ‚ÄúIWSLT evaluation 2016,‚Äù https://sites.google.com/site/
iwsltevaluation2016/iwslt-evaluation-2016.
[19] S. Merity, N. S. Keskar, and R. Socher, ‚ÄúAn analysis of neural language
modeling at multiple scales,‚Äù arXiv preprint arXiv:1803.08240, 2018.
[20] K. Jing and J. Xu, ‚ÄúA survey on neural network language models,‚Äù arXiv
preprint arXiv:1906.03591, 2019.
[21] S. Ortmanns, H. Ney, and A. Eiden, ‚ÄúLanguage-model look-ahead for
large vocabulary speech recognition,‚Äù in Proceedings of the FourthInternational Conference on Spoken Language Processing (ICSLP).
IEEE, 1996, pp. 2095‚Äì2098.
[22] P. F. Brown, J. Cocke, S. A. Della Pietra, V . J. Della Pietra, F. Jelinek,
J. Lafferty, R. L. Mercer, and P. S. Roossin, ‚ÄúA statistical approach
to machine translation,‚Äù Computational Linguistics, vol. 16, no. 2, pp.
79‚Äì85, 1990.
[23] K.-L. Liu, W.-J. Li, and M. Guo, ‚ÄúEmoticon smoothed language models
for Twitter sentiment analysis,‚Äù in Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence, 2012.
[24] F. Song and W. B. Croft, ‚ÄúA general language model for information
retrieval,‚Äù in Proceedings of the Eighth International Conference on
Information and Knowledge Management, 1999, pp. 316‚Äì321.
[25] K. Smagulova and A. P. James, ‚ÄúA survey on LSTM memristive neural
network architectures and applications,‚Äù The European Physical Journal
Special Topics, vol. 228, no. 10, pp. 2313‚Äì2324, 2019.
[26] F. Mireshghallah, H. A. Inan, M. Hasegawa, V . R ¬®uhle, T. Berg-Kirkpatrick,
and R. Sim, ‚ÄúPrivacy regularization: Joint privacy-utility optimization in
language models,‚Äù arXiv preprint arXiv:2103.07567, 2021.
[27] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee,
A. Roberts, T. Brown, D. Song, ¬¥U. Erlingsson, A. Oprea, and C. Raffel,
‚ÄúExtracting training data from large language models,‚Äù in 30th USENIX
Security Symposium (USENIX Security). USENIX Association, Aug.
2021, pp. 2633‚Äì2650.
[28] S. Merity, N. S. Keskar, and R. Socher, ‚ÄúRegularizing and optimizing
LSTM language models,‚Äù arXiv preprint arXiv:1708.02182, 2017.
[29] C. Dwork, A. Roth et al., ‚ÄúThe algorithmic foundations of differential
privacy.‚Äù Foundations and Trends in Theoretical Computer Science, vol. 9,
no. 3-4, pp. 211‚Äì407, 2014.
[30] G. Weiss, Y . Goldberg, and E. Yahav, ‚ÄúExtracting automata from recurrent
neural networks using queries and counterexamples,‚Äù in International
Conference on Machine Learning. PMLR, 2018, pp. 5247‚Äì5256.
[31] R. Krishnan, D. Liang, and M. Hoffman, ‚ÄúOn the challenges of
learning with inference networks on sparse, high-dimensional data,‚Äù in
International Conference on ArtiÔ¨Åcial Intelligence and Statistics. PMLR,
2018, pp. 143‚Äì151.
[32] B. Geiger and G. Kubin, ‚ÄúRelative information loss in the PCA,‚Äù in 2012
IEEE Information Theory Workshop. IEEE, 2012, pp. 562‚Äì566.
[33] M. Ester, H. Kriegel, J. Sander, and X. Xiaowei, ‚ÄúA density-based
algorithm for discovering clusters in large spatial databases with noise,‚Äù in
Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining (KDD), 1996, pp. 226‚Äì231.
[34] M. Zorzi, R. R. Rao, and L. B. Milstein, ‚ÄúOn the accuracy of a Ô¨Årst-
order Markov model for data transmission on fading channels,‚Äù in
Proceedings of the 4th IEEE International Conference on Universal
Personal Communications (ICUPC). IEEE, 1995, pp. 211‚Äì215.
[35] J. Rae, C. Dyer, P. Dayan, and T. Lillicrap, ‚ÄúFast parametric learning
with activation memorization,‚Äù in Proceedings of the 35th International
Conference on Machine Learning. PMLR, 2018, pp. 4228‚Äì4237.
[36] Q. Jia, N. Zhang, and N. Hua, ‚ÄúContext-aware deep model for entity
recommendation system in search engine at Alibaba,‚Äù Journal of
Multimedia Processing and Technologies, vol. 11, no. 1, pp. 23‚Äì35,
2020.
[37] J. Benesty, J. Chen, Y . Huang, and I. Cohen, ‚ÄúPearson correlation
coefÔ¨Åcient,‚Äù in Noise Reduction in Speech Processing. Springer, 2009,
pp. 1‚Äì4.
[38] M. Bayer, M.-A. Kaufhold, and C. Reuter, ‚ÄúA survey on data augmenta-
tion for text classiÔ¨Åcation,‚Äù arXiv preprint arXiv:2107.03158, 2021.
[39] C. Auerbach, Mutation research: problems, results and perspectives.
Springer, 2013.
[40] A. L. Cechin, D. Regina, P. Simon, and K. Stertz, ‚ÄúState automata
extraction from recurrent neural nets using k-means and fuzzy clustering,‚Äù
inProceedings of the 23rd International Conference of the Chilean
Computer Science Society (SCCC). IEEE, 2003, pp. 73‚Äì78.
[41] M. D. Zeiler and R. Fergus, ‚ÄúVisualizing and understanding convolutional
networks,‚Äù in European Conference on Computer Vision. Springer, 2014,
pp. 818‚Äì833.
[42] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M ¬®uller, and
W. Samek, ‚ÄúOn pixel-wise explanations for non-linear classiÔ¨Åer decisions
by layer-wise relevance propagation,‚Äù PloS ONE, vol. 10, no. 7, pp. 130
‚Äì 140, 2015.
[43] M. T. Ribeiro, S. Singh, and C. Guestrin, ‚Äú‚ÄúWhy should I trust you?‚Äù
Explaining the predictions of any classiÔ¨Åer,‚Äù in Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, 2016, pp. 1135‚Äì1144.
1014[44] L. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling, ‚ÄúVisualizing
deep neural network decisions: Prediction difference analysis,‚Äù in 5th
International Conference on Learning Representations (ICLR), 2017.
[45] G. Montavon, S. Lapuschkin, A. Binder, W. Samek, and K.-R. M ¬®uller,
‚ÄúExplaining nonlinear classiÔ¨Åcation decisions with deep Taylor decompo-
sition,‚Äù Pattern Recognition, vol. 65, pp. 211‚Äì222, 2017.
[46] P. W. Koh and P. Liang, ‚ÄúUnderstanding black-box predictions via
inÔ¨Çuence functions,‚Äù in International Conference on Machine Learning.
PMLR, 2017, pp. 1885‚Äì1894.
[47] A. Shahroudnejad, ‚ÄúA survey on understanding, visualizations, and
explanation of deep neural networks,‚Äù arXiv preprint arXiv:2102.01792,
2021.
[48] A. Dhurandhar, P. Chen, R. Luss, C. Tu, P. Ting, K. Shanmugam,
and P. Das, ‚ÄúExplanations based on the missing: Towards contrastive
explanations with pertinent negatives,‚Äù in Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ¬¥eal,
Canada, 2018, pp. 590‚Äì601.
[49] C. W. Omlin and C. L. Giles, ‚ÄúExtraction of rules from discrete-time
recurrent neural networks,‚Äù Neural Networks, vol. 9, no. 1, pp. 41‚Äì52,
1996.
[50] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,
‚ÄúPrivacy in pharmacogenetics: An end-to-end case study of personalized
Warfarin dosing,‚Äù in 23rd USENIX Security Symposium (USENIX
Security), 2014, pp. 17‚Äì32.
[51] X. Liu, L. Xie, Y . Wang, J. Zou, J. Xiong, Z. Ying, and A. V . Vasilakos,
‚ÄúPrivacy and security issues in deep learning: A survey,‚Äù IEEE Access,
vol. 9, pp. 4566‚Äì4593, 2021.
[52] McAfee, ‚ÄúGrand theft data ‚Äì Data exÔ¨Åltration study: Actors, tactics, and
detection,‚Äù 2015.
[53] P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss, W. Haas, M. Hamburg,
M. Lipp, S. Mangard, T. Prescher et al., ‚ÄúSpectre attacks: Exploiting
speculative execution,‚Äù in 2019 IEEE Symposium on Security and Privacy
(SP). IEEE, 2019, pp. 1‚Äì19.
[54] S. Sodagudi and R. R. Kurra, ‚ÄúAn approach to identify data leakage insecure communication,‚Äù in Proceedings of 2nd International Conference
on Intelligent Computing and Applications. Springer, 2017, pp. 31‚Äì43.
[55] C. Warzel, ‚ÄúFaceapp shows we care about privacy but don‚Äôt understand
it,‚Äù https://www.nytimes.com/2019/07/18/opinion/faceapp-privacy.html,
(Accessed on 03/11/2021).
[56] R. J. Bolton, D. J. Hand et al., ‚ÄúStatistical fraud detection: A review,‚Äù
Statistical science, vol. 17, no. 3, pp. 235‚Äì255, 2002.
[57] R. Shokri, M. Stronati, C. Song, and V . Shmatikov, ‚ÄúMembership
inference attacks against machine learning models,‚Äù in 2017 IEEE
Symposium on Security and Privacy (SP). IEEE, 2017, pp. 3‚Äì18.
[58] A. Sablayrolles, M. Douze, C. Schmid, Y . Ollivier, and H. J ¬¥egou, ‚ÄúWhite-
box vs black-box: Bayes optimal strategies for membership inference,‚Äù
inInternational Conference on Machine Learning. PMLR, 2019, pp.
5558‚Äì5567.
[59] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, ‚ÄúPrivacy risk in
machine learning: Analyzing the connection to overÔ¨Åtting,‚Äù in 2018
IEEE 31st Computer Security Foundations Symposium (CSF). IEEE,
2018, pp. 268‚Äì282.
[60] M. Fredrikson, S. Jha, and T. Ristenpart, ‚ÄúModel inversion attacks
that exploit conÔ¨Ådence information and basic countermeasures,‚Äù in
Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security (CCS), 2015, pp. 1322‚Äì1333.
[61] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton, ‚ÄúA methodology
for formalizing model-inversion attacks,‚Äù in 2016 IEEE 29th Computer
Security Foundations Symposium (CSF). IEEE, 2016, pp. 355‚Äì370.
[62] F. Tram `er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, ‚ÄúStealing
machine learning models via prediction APIs,‚Äù in 25th USENIX Security
Symposium (USENIX Security), 2016, pp. 601‚Äì618.
[63] T. Orekondy, B. Schiele, and M. Fritz, ‚ÄúPrediction poisoning: Towards
defenses against DNN model stealing attacks,‚Äù in 8th International
Conference on Learning Representations, (ICLR), 2020.
[64] M. Yan, C. W. Fletcher, and J. Torrellas, ‚ÄúCache telepathy: Leveraging
shared resource attacks to learn DNN architectures,‚Äù in 29th USENIX
Security Symposium (USENIX Security). USENIX Association, Aug.
2020, pp. 2003‚Äì2020.
1015