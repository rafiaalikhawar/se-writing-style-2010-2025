Code to Comment “Translation”:
Data, Metrics, Baselining & Evaluation
David Gros*, Hariharan Sezhiyan*, Prem Devanbu, Zhou Yu
University of California, Davis
{dgros,hsezhiyan,devanbu,joyu}@ucdavis.edu
ABSTRACT
Therelationshipofcommentstocode,andinparticular,thetask
of generating useful comments given the code, has long been of
interest.Theearliestapproacheshavebeenbasedonstrongsyntac-
tic theories of comment-structures, and relied on textual templates.
More recently, researchers have applied deep-learning methods
to this task—specifically, trainable generative translation models
which are known to work very well for Natural Language trans-
lation (e.g., from German to English). We carefully examine the
underlying assumption here: that the task of generating comments
sufficientlyresemblesthetaskoftranslatingbetweennaturallan-
guages,andsosimilarmodelsandevaluationmetricscouldbeused.
We analyze several recent code-comment datasets for this task:
CodeNN, DeepCom, FunCom, and DocString. We compare them
withWMT19,astandarddatasetfrequentlyusedtotrainstate-of-
the-art natural language translators. We found some interesting
differences between the code-comment data and the WMT19 natu-
rallanguagedata.Next,wedescribeandconductsomestudiesto
calibrateBLEU(whichiscommonlyusedasameasureofcomment
quality).using“affinitypairs"ofmethods,fromdifferentprojects,
in the same project, in the same class, etc; Our study suggests that
the current performance on some datasets might need to be im-
provedsubstantially.Wealsoarguethatfairlynaiveinformation
retrieval (IR) methods do well enough at this task to be considered
areasonablebaseline.Finally,wemakesomesuggestionsonhow
our findings might be used in future research in this area.
ACM Reference Format:
David Gros*, Hariharan Sezhiyan*, Prem Devanbu, Zhou Yu. 2020. Code to
Comment “Translation”: Data, Metrics, Baselining & Evaluation. In 35th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE
’20),September21–25,2020,VirtualEvent,Australia. ACM,NewYork,NY,
USA, 12 pages. https://doi.org/10.1145/3324884.3416546
1 INTRODUCTION
Programmersaddcommentstocodetohelpcomprehension.The
value of these comments is well understood and accepted. A wide
variety of comments exist [ 42] in code, including prefix comments
(standardized in frameworks like Javadocs [ 31]) which are inserted
before functions or methods or modules, to describe their function.
Given the value of comments, and the effort required to write
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6768-4/20/09. . . $15.00
https://doi.org/10.1145/3324884.3416546
Figure 1: Distribution of trigrams in English (blue) in the
WMT [10] German-English machine-translation dataset,
andinEnglishcommentsfromseveralpreviouslypublishedCode-Comment datasets
them, there has been considerable interest in providing automated
assistancetohelpdeveloperstoproducecomments,andavariety
of approaches have been proposed [38, 47, 48, 59].
1
Comments(especiallyprefixcomments)aretypicallyexpected
to be a useful summary of the function of the accompanying code.
Comments could be viewed as a restatement of the semantics of
the code, in a different and more accessible natural language; thus,
itispossibletoviewcommentgenerationasakindoftranslation
task, translating from one (programming) language to a another
(natural)language.Thisview,togetherwiththeverylargevolumes
ofcode(withaccompanyingcomments)availableinopen-source
projects,offerstheveryappealingpossibilityofleveragingdecades
ofresearchin statisticalnaturallanguagetranslation(NLT).If it’s
possible to learn to translate from one language to another fromdata, why not learn to synthesize comments from code? Severalrecent papers [
22,26,33,61] have explored the idea of applying
StatisticalMachineTranslation(SMT)methodsto learntotranslate
code to an English comments. But are these tasks really similar?
We are interested to understand in more detail how similar the
taskofgeneratingcommentsfromcodeistothetaskoftranslating
between natural languages.
Commentsformadomain-specificdialect,whichishighlystruc-
tured,withalotofveryrepetitivetemplates.Commentsoftenbegin
with patterns like "returns the", "outputs the", and "calculates the" .
Indeed,most ofthe earlierwork (whichwasn’t basedonmachine
1* Authors contributed equally
7462020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
This work is licensed under a Creative Commons Attribution International 4.0 License. 
learning)onthisproblemhasleveragedthishighlytemplatedna-
ture of comments [ 40,48]. We can see this phenomenon clearly us-
ingZipfplots.Figure1comparesthetrigramfrequenciesoftheEng-
lish language text in comments (from the datasets [ 22,26,33] that
havebeenusedtotraindeep-learningmodelsforcode-comment
summarization) and English language text in the WMT German-
Englishtranslationdataset:thex-axisordersthetrigramsfrommost
to least frequent using a log-rank scale, and the y-axis is the log
relativefrequency ofthetrigrams inthe corpus.The Englishfound
in WMT datasetis the magenta line atthe bottom. The comments
fromcodeshowconsistentlyhigherslopeinthe(note, log-scaled )
y-axis of the Zipf plot, suggesting that comments are far more sat-
urated with repeating trigrams than is the English found in the
translationdatasets.Thisobservationmotivatesacloserexamina-
tion of the differences between code-comment and WMT datasets,
andtheimplicationsofusingmachinetranslationapproachesfor
code-comment generation.
In this paper, we compare code-comment translation (CCT)
datasets used with DL models for the task of comment genera-
tion, with a popular natural translation (WMT) dataset used fortrainingDLmodelsfornaturallanguagetranslation.Thesewere
our results:
(1)We find that the desired outputs for the CCT task are much
more repetitive.
(2)We find that the repetitiveness has a very strong effect on
measuredperformance,muchmoresointheCCT datasets
than the WMT dataset.
(3)We find that the WMT translation dataset has a smoother,
morerobustinput-outputdependency.SimilarGermanin-
puts in WMT have a strong tendency to produce similarEnglish outputs. However, this does appear to hold in the
CCT datasets.
(4)WereportthatanaiveInformationretrievalapproachcan
meet or exceed reported numbers from neural models.
(5)WeevaluateBLEU perseasameasureofgeneratedcomment
quality using groups of methods of varying "affinity"; this
offers new perspectives on the BLEU measure.
Ourfindingshaveseveralimplicationsforthefutureworkinthe
area,intermsoftechnicalapproaches,waysofmeasurement,for
baselining,andforcalibratingBLEUscores.Webeginbelowbyfirst
providing some background; we then describe the datasets used in
prior work. We then present an analysis of the datasets and and an
analysis of the evaluation metrics and baselines used. We conclude
after a detailed discussion of the implications of this work.
But first, a disclaimer: this work does not offer any new models for
or improvements on prior results on the CCT task. It is primar-
ily retrospective, viz, a critical review of materials & evaluations
usedinpriorworkinCCT,offeredinacollegialspirit,hopingto
advancethe wayour communityviews thetaskof code-commenttranslation, and how we might together make further advances in
the measurement and evaluation of innovations that are addressed
in this task.
2 BACKGROUND & THEORY
The value of comments in code comprehension has been well-
established [ 51]. However, developers findit challenging tocreate&maintainusefulcomments[ 17,19].Thishassparkedalonglineof
researchlookingintotheproblemofcommentgeneration.Anearly
line ofwork [ 11,40,48,49] wasrule-based, combining someform
analysisofthesourcecodetoextractspecificinformation,which
couldthenbeslottedintodifferenttypesoftemplatestoproduce
comments. Another approach was to use code-clone identification
to produce comments for given code, using the comments asso-
ciatedwithaclone[ 59].Otherapproachesusedkeywordswhich
programmersseemtoattendtoineye-trackingstudies[ 47].Still
other approaches use topic analysis to organize descriptions of
code [37].
Most of the pioneeering approaches above relied on specific fea-
turesandruleshand-engineeredforthetaskofcommentgeneration.
Theadventoflargeopen-sourcerepositorieswithlargevolumesof
source-code offered a novel, general, statistically rigorous, possi-
bility:thattheselargedatasetsbeminedforcode-commentpairs,
which could then be used to train a model to produce comments
fromcode.Thesuccessofclassicstatisticalmachinetranslation[ 30]
offered a tempting preview of this: using large amounts of aligned
pairs of utterances in languages A&B, it was possible to learn a
conditional distribution of the form pt(b|a), wherea∈A,and
b∈B; given an utterance β∈B, one could produce a possible
translation α∈Aby simply setting
α=argmax
apt(a|β)
Statistical natural language translation approaches, which were al-
ready highly performant, were further enhanced by deep-learning
(DL).Ratherthanrelyingonspecificinductivebiaseslikephrase-
structures in the case of classical SMT, DL held the promise thatthe features relevant to translation could themselves be learned
fromlargevolumesofdata.DLapproacheshaveledtophenomenal
improvements in translation quality [ 29,52]. Several recent pa-
pers[24,26,33]haveexploredusingthesepowerfulDLapproaches
to the code-comment task.
Iyeretal.[26]firstappliedDLtothistask,usingcode-English
pairs mined from Stack Overflow—using simple attention over
inputcode,andanLSTMtogenerateoutputs.Manyotherpapers
followed, which are discussed below in section 3.2. We analyze the
published literature, starting with the question of whether there
arenotabledistributionaldifferencesbetweenthecode-comment
translation (CCT) and the statistical machine translation (WMT)
data. Our studies examine the distributions of the input and output
data, and the dependence of the output on the input.
RQ1.Whatarethedifferencesbetweenthetranslation(WMT)
data, and code-comment (CCT) data?
Next,weexaminewhetherthesedifferencesactuallyaffectthe
performance of translationmodels. In earlier work,Allamanis [ 3]
pointed out the effects of data duplication on machine learning
applications in software engineering. We study the effectsof data
duplication, as well as the effects of distributional differences ondeep learning models. One important aspect of SMT datasets isinput-ouput dependence. In translation e.g.from German (DE) to
English (EN), similar input DE sentences will to produce similaroutput EN sentences, and less similar DE sentences will tend to
747producelesssimilarENsentences.Thissamecorrelationmightnot
apply in CCT datasets.
RQ2. How the distributional differences in the SMT & CCT
datasets affect the measured performance?
There’s another important difference between code and natural
language. Small differences, such as substituting ∗for+and a 1 for
a 0, can make the difference between a sumand afactorialfunc-
tion;likewise changingone functionidentifier(mean,rather than
variance).Thesesmallchangesshouldresultinalargechangein
theassociatedcomment.Likewise,therearemanydifferentways
to write a sort function, all of which might entail the same com-
ment.Intuitively,thiswouldappeartobelessofanissueinnatural
languages; since as they have evolved for consequential communi-
cationinnoisyenvironments,meaningshouldberobusttosmallchanges.Thusonthewhole,wemightexpectthatsmallchanges
in German should in general result in only small changes in the
English translation. Code, on the other hand, being a fiatlanguage,
might not be in general as robust, and so small changes in codemay result in unpredictable changes in the associated comment.
Why does this matter? In general, modern machine translation
methods use the generalized function-approximation capability of
deep-learning models. If natural language translation (WMT) has a
morefunctionaldependency,andCCTdoesn’t,thereisasuggestion
that deep-learning models would find CCT a greater challenge.
RQ3.DosimilarinputsproducesimilaroutputsinbothWMT
and CCT datasets?
Prior work in natural language generation has shown that infor-
mationretrieval(IR)methodscanbeeffectivewaysofproducing
suitable outputs. These methods match a new input with semanti-
cally similar inputs in the training data, and return the associated
output.Theseapproachescansometimesperformquitewell[ 21]
andhasbeenpreviouslyappliedsuccessfullytothetaskofcomment
generation [ 14,62]. Our goal here is to ask whether IR methods
could be a relevant, useful baseline for CCT tasks.
RQ4.HowdotheperformanceofnaiveInformationRetrieval
(IR) methods compare across WMT & CCT datasets?
Finally,wecriticallyevaluatetheuseofBLEUscoresinthistask.
Given the differences we found between datasets used for training
SMTtranslatorsandthecode-commentdatasets,wefeltitwouldbe
importanttounderstandhowBLEUisusedinthistask,anddevelop
some empirical baselines to calibrate the observed BLEU values in
priorwork.Howgoodarethebest-in-classBLEUscores(associated
with thebest current methods for generatingcomments given the
source of a method)? Are they only as good as simply retrieving a
comment associated with a randommethod in a different project?
Hopefullythey’remuchbetter.Howaboutthecommentassociated
with a random method from the same project? With a random
method in the same class? With a method that could reasonably be
assumed quite similar?
RQ5. How has BLEU been used in prior work for the code-
comment task, and how should we view the measured per-formance?In the next section, we review the datasets that we use in our
study.
3 DATASETS USED
We examinethecharacteristicsoffourCCTdatasets,namelyCo-
deNN, DeepCom, FunCom, & DocString and one standard, widely-
usedmachine-translationdataset,theWMTdataset.Webeginwith
a description of each dataset. Within some of the CCT datasets,
we observe that the more popular ones can include several dif-
ferent variations: this is because follow-on work has sometimes
gathered, processed, and partitioned (training/validation/test) the
dataset differently.
CodeNN Iyeretal[26]wasanearlyCCTdataset,collectedfrom
StackOverflow, with code-comment pairs for C# and SQL. Stack-
overflow posts consist of a title, a question, and a set of answers
which may contain code snippets. Each pair consists of the title
andcodesnippet fromanswers.Iyer etalgatheredaroundamillion
pairs each for C# and SQL; from these, focusing on just snippetsinaccepted answers, they filtered down to 145,841 pairs for C#
and 41,340 pairs for SQL. From these, they used a trained model
(trainedusingahand-labeledset)tofilteroutuninformativetitles
(e.g.,“How can make this complicated query simpler") to 66,015
higher-quality pairs for C# and 33,237 for SQL. In our analysis, we
usedonlytheC#data.StackOverflowhasawell-knowncommunity
norm to avoid redundant Q&A; repeated questions are typically
referredtotheearlierpost.Asaresult,thisdatasethas significantly
less duplication. The other CCT datasets are different.
DeepCom Huetal. [22]generateaCCTdatasetbymining9,714
Javaprojects.Fromthisdataset,theyfilteroutmethodsthathave
Javadoccomments,andselectonlythosethathaveatleastone-word
descriptions. They also exclude getters, setters, constructors and
test methods. This leaves them with 69,708 method-comment pairs.
In this dataset, the methods (code) are represented as serialized
ASTs after parsing by Eclipse JDT.
Later, Hu et al . [23]updated their dataset and model, to a size
of 588,108 examples. We refer to the former as DeepCom1 and
obtain a copy online from followup work2. We refer to the latter
as DeepCom2 and obtain a copy online3. In addition DeepCom2 is
distributedwitha10-foldsplitinthecross-projectsetting(examples
inthetestsetarefromdifferentprojects).InHuetal . [23]thisis
referredtothe"RQ-4split",buttoavoidconfusionwithourresearch
questions, we refer to it as DeepCom2f.
Funcom LeClairetal .[33]startedwiththeSourcerer[ 7]repo,with
over51Mmethodsfrom50Kprojects.Fromthis,theyfilteredout
methodswithJavadoccommentsinEnglish,andthenalsothecom-
mentsthatwereauto-generated.Thisleavesabout2.1Mmethods
with patched Javadoc comments. The source code was parsed into
anAST.Theycreatedtwodatasets,the standard,whichretainedthe
originalidentifiers,and challenge,whereintheidentifiers(except
for Java API class names) were replaced with a standardized token.
They also made sure no data from the same project was duplicated
across training and/or validation and/or test. Notably, the FunCom
2https://github.com/wasiahmad/NeuralCodeSum/tree/d563e58/data
3https://github.com/xing-hu/EMSE-DeepCom/tree/98bd6a
748dataset only considers the first sentence of the comment. Addition-
ally, code longer than 100 words and comments longer 13 words
were truncated.
Like for DeepCom, there are several versions of this dataset.
We consider a version from LeClair et al . [33]as FunCom1 and
the version from LeClair and McMillan [34]as FunCom2. These
datasets are nearly identical, but FunCom2 has about 800 fewer
examples and the two versions have reshuffled train/test/val splits.
The Funcom14and Funcom25datasets are available online.
Docstring Barone and Sennrich [8]collect Python methods and
prefix comment "docstrings" by scraping GitHub. Tokenization
was done using subword tokenization. They filtered the data for
duplications, and also removed excessively long examples (greater
than 400 tokens). However, unlike other datasets, Barone et al.do
notlimittoonlythefirstsentenceofthecomments.Thiscanresult
in relatively long desired outputs.
The dataset contains approximately 100k examples, but after
filteringoutverylongsamples,asperBarone etalpreprocessing
script6, this is reduced to 74,860 examples. We refer to this version
as DocString1.
WealsoconsideraprocessedversionobtainedfromAhmadetal .
[2]source2which was attributed to Wei et al .[58]. We refer to this
versionasDocString2.Duetotheprocessingchoices,theexamples
in DocString2 are significantly shorter than DocString1.
WMT19 News Dataset Tobenchmarkthecommentdatawithnat-
urallanguage,weuseddatafromtheFourthConferenceofMachine
Translation(WMT19).Inparticular,weusedthenewsdataset[ 9].
After manual inspection, we determined this dataset offers a good
balance of formal language that is somewhat domain specific to
morelooselanguagecommonineverydayspeech.Inbenchmark-
ing comment data with natural language, we wanted to ensure
variety in the words and expressions used to avoid biasing results.
We used the English-German translation dataset, and compared
English in this dataset to comments in the other datasets (which
were all in English) to ensure differences in metrics were not a
result of differences in language.
Other CCT Datasets We tried to capture most of the code-comment
datasets that are used in the context of translation. However, there
aresomerecentdatasetswhichcouldbeusedinthiscontext,but
wedidnotexplore[ 1,25].Whiledoingourworkwenoticedthat
some prior works provide the raw collection of code-comments for
download,butnottheexactprocessingandevaluationsused[ 39].
Other works use published datasets like DocString, but processing
and evaluation techniques are not now readily available [ 56,57].
As we will discuss, unless the precise processing and evaluation
code is available, the results may be difficult to compare.
3.1 Evaluation Scores Used
AcommonmetricusedinevaluatingtextgenerationisBLEUscore
[43].Whencomparingtranslationsofnaturallanguage,BLEUscore
has been shown to correlate well with human judgements of trans-
lationquality[ 16].Inallthedatasetsweanalyzed,theassociated
4http://leclair.tech/data/funcom/
5https://s3.us-east-2.amazonaws.com/icse2018/index.html
6https://bit.ly/2yDnHcSpapersusedBLEUtoevaluatethequalityofthecommentgenera-
tion. However, there are rather subtle differences in the way the
BLEUs were calculated, which makes the results rather difficult to
compare.Webeginthisdiscussionwithabriefexplanationofthe
BLEU score.
BLEU(asdorelatedmeasures)indicatestheclosenessofacandi-
date translation output to a “golden" referenceresult. BLEU perse
measuresthe precision(asopposedto recall)ofacandidate,relative
to the reference, using constituent n-grams. BLEU typically uses
unigrams through 4-grams to measure the precision of the system
output. If we define :
pn=number of n-grams in both reference and candidate
number of n-grams in the candidate
BLEUcombinestheprecisionofeach n-gramusingthegeometric
mean,exp(1
N/summationtext.1N
n=1logpn).Withjustthisformulation,singleword
outputsoroutputsthatrepeatcommon n-gramscouldpotentially
havehighprecision.Thus,a “brevitypenalty” isusedtoscalethe
finalscore;furthermoreeach n-graminthereferencecanbeused
inthecalculationjustonce.[ 18]Thesecalculationsaregenerally
standard in all BLEU implementations, but severalvariations may
arise.Smoothing:
One variation arises when deciding how to deal with
cases when pn=0,i.e.,a nn-gram in the candidate string is not in
thereferencestring[ 12].Withnoadjustment,onehasanundefined
log0. One can add a small epsilon to pnwhich removes undefined
expressions.However,becauseBLEUisageometricmeanof pn,n∈
{1,2,3,4}ifp4isonlyepsilonabovezero,itwillresultinamean
which is near zero. Thus, some implementations opt to smooth the
pnin varying ways. To compare competing tools for the same task,
it would be preferable to use a standard measure.Corpus vs. Sentence BLEU:
When evaluating a translation system,
one typically measures BLEU (candidate vsreference) across all
the samples inthe held-out test set.Thus another source ofimple-
mentationvariationis whendecidinghowtocombine theresults
between all of the test set scores. One option, which was proposed
originally in Papineni et al.[43], is a "corpus BLEU", sometimes
referredtoasC-BLEU.Inthiscasethenumeratoranddenominator
ofpnareaccumulatedacrosseveryexampleinthetestcorpus.This
means as long as at least one example has a 4-gram overlap, p4
willnotbezero,andthusthegeometricmeanwillnot be zeroAn
alternative option for combining across the test corpus is referred
to as "Sentence BLEU" or S-BLEU. In this setting BLEU score for
the test set is calculated by simply taking the arithmetic mean the
BLEU score calculated on each sentence in the set.
Tokenization Choices: A final source of variation comes not from
howthemetriciscalculated,butfromtheinputsitisgiven.Because
the precision counts are at a token level, it has been noted that
BLEUishighlysensitivetotokenization[ 44].Thismeansthatwhen
comparingtopriorworkonadataset,onemustbecarefulnotonly
tousethesameBLEUcalculation,butalsothesametokenization
andfiltering.Whencalculatingscoresonthedatasets,weusethe
tokenization provided with the dataset.
Tokenizationcanbeverysignificantfortheresultingscore.As
a toy example, suppose a reference contained the string “ calls
function foo() ” and an hypothesis contained the string “ uses
749function foo() ”. If one chooses to tokenize by spaces, one has
tokens [ calls,function,foo()] and [ uses,function,foo()]. This
tokenization yields only one bigram overlap and no trigram or
4-gram overlaps. However, if one instead chooses totokenize this
as[calls,function,foo,(,)]and[ uses,function,foo,(,)]wesud-
denlyhavethreeoverlappingbigrams,twooverlappingtrigrams,
andoneoverlapping4-gram.Thisresultsinaswingofmorethan
15BLEU-M2pointsornearly40BLEU-DCpoints(BLEU-M2and
BLEU-DC described below).
We now go through BLEU variants used by each of the datasets
and assign a name to them. The name is not intended to be pre-
scriptive or standard, but instead just for later reference in this
document. All scores are the "aggregate" measures, which consider
up to4-grams.
BLEU-CN ThisisaSentenceBLEUmetric.ItappliesaLaplace-like
smoothing by adding 1 to both the numerator and denominator of
pnforn≥2. The CodeNN authors’ implementation was used7.
BLEU-DC ThisisalsoaSentenceBLEUmetric.Theauthors’imple-
mentation isbased off NLTK [ 36] using its"method 4" smoothing.
Thissmoothingismorecomplex.Itonlyapplieswhen pniszero,
and setspn=1/((n−1)+5/loglh)wherelhis the length of the
hypothesis. See the authors’ implementation for complete details8.
BLEU-FC This is an unsmoothed corpus BLEU metric based on
NLTK’s implementation. Details are omitted for brevity, and can
be found in the authors’ source9.
BLEU-Moses The Docstring dataset uses a BLEU implementation
bytheMosesproject10.ItisalsoanunsmoothedcorpusBLEU.This
is very similar to BLEU-FC (though note that due to differences in
tokenization, scores presented by the two datasets are not directly
comparable).
BLEU-ncs This is a sentence BLEU used in the implementation11
of Ahmad et al . [2]. Like BLEU-CN, it uses an add-one Laplace
smoothing. However, it is subtly different than BLEU-CN as the
add-one applies even for unigrams.SacreBLEU
TheSacreBLEUimplementationwascreatedbyPost
[44]inanefforttohelpprovideastandardBLEUimplementation
forevaluatingonNLtranslation.Weusethedefaultsettingswhich
is a corpus BLEU metric with an exponential smoothing.BLEU-M2
ThisisaSentenceBLEUmetricbasedonnltk"method2"
smoothing.LikeBLEU-CNitusesalaplace-likeadd-onesmoothing.
This BLEU is later presented in plots for this paper.
We conclude by noting that the wide variety of BLEU measures
used inprior workin code-commenttranslation carrysome risks.
Wediscussfurtherbelow.table3providesomeevidencesuggesting
thatthevariationishighenoughtoraisesomeconcernaboutthe
true interpretation of claimed advances; as we argue below, the
field can benefit from further standardization.
7https://github.com/sriniiyer/codenn/blob/0f7fbb8b298a8/src/utils/bleu.py
8https://github.com/xing-hu/EMSE-DeepCom/blob/98bd6aac/scripts/evaluation.py
9https://github.com/mcmillco/funcom/blob/41c737903/bleu.py#L17
10https://bit.ly/2YF0hye
11https://github.com/wasiahmad/NeuralCodeSum/blob/b2652e2/main/test.py#L3243.2 Models & Techniques
Inthissection,weoutlinethevariousdeeplearningapproaches
that have been applied to this code-comment task. We note that
our goal in this paper is not to critique or improve upon the spe-
cific technical methods, but to analyze the data perseto gain some
insights on the distributions therein, and also to understand the
most comment metric (BLEU)that is used, and the implications of
using this metric. However, for completeness, we list the different
approaches, and provide just a very brief overview of each tech-nical approach. All the datasets used below are described above
in section 3.
Iyeretal[26]wasanearlyattemptatthistask,usingafairlystan-
dardseq2seqRNNmodel,enhancedwithattention.Hu etal[22]
alsousedasimilarRNN-basedseq2seqmodel,butintroduceda“tree-
like" preprocessing of the input source code. Rather than simply
streamingintherawtokens,theyfirstparseit,andthenserializetheresultingASTintoatokenstreamthatisfedintotheseq2seqmodel.
A related approach [ 5] digests a fixed-size random sample of paths
through the AST of the input code (using LSTMs) and produces
code summaries. LeClair et al[33] proposed an approach that com-
binesbothstructuralandsequentialrepresentationsofcode;they
have also suggested the use of graph neural networks [ 32]. Wanet
al[54]use asimilar approach, butadvocate usingreinforcement
learningtoenhancethegenerationelement.Morerecently,theuse
of function context [ 20] has been reported to improve comment
synthesis. Source-code vocabulary proliferation is a well-knownproblem [
28]; previously unseen identifier or method names in
input code or output comments can diminish performance. New
work by Moore et al[39] approaches this problem by using convo-
lutions over individual lettersin the input and using subtokens (by
camel-case splitting) on the output. Very recently Zhang et al . [62]
have reported that combining sophisticated IR methods with deep-
learningleadstofurthergainsintheCCTtask.Forourpurposes
(showing that IR methods constitute a reasonable baseline) we use
averysimple,vanilla,out-of-boxLuceneIRimplementation,which
already achieves nearly SOTA performance in many cases.
Therearetasksrelatedtogeneratingcommentsfromcode:for
example,synthesizingacommitloggivenacodechange[ 15,27,35],
orgeneratingmethodnamesfromthecode[ 4,5].Sincetheseare
somewhat different tasks, with different data characteristics, we
don’tdiscussthemfurther.Inadditioncodesynthesis[ 1,60]also
usesmatchedpairsofnaturallanguageandcode;however,these
datasets have not been used for generating English from code, and
arenotusedinpriorworkforthistask;sowedon’tdiscussthem
further here.
4 METHODS & FINDINGS
In the following section, we present our methods and results foreach of the RQs presented in § 2. In each case, we present some
illustrativeplotsand(whenapplicable)theresultsofrelevantsta-
tistical tests. All p-values have been corrected using family-wise
(Benjamini-Hochberg)correction.Toexaminethecharacteristics
ofeachdataset,weconstructedtwotypesofplots:zipfplotsand
bivariate BLEU plots.
750Figure2:Unigram(Vocabulary)distribution.Zipfplotforall
datasets look similar. Difference from trigram Zipf plot inFig 1 suggests greater repetitiveness in code comments.
4.1 Differences between CCT and WMT data
The Zipf plots are a useful way to visualize the skewness of tex-
tual data, where (in natural text) a few tokens (or ngrams) account
foralargeportionofthetext.Eachplotpointisa(rank,relative-
frequency) pair, both log-scaled. We use the plot to compare the
relativeskewnessofthe(English)commentdataintheCCTdata
and the desired English outputs in the WMT NLT data. Examining
the unigram Zipf plot above, it can be seen in both code comments
andnaturalEnglish,afewvocabularywordsdodominate.However,
whenweturnbacktothetrigramZipfplotsinFigure1,wecansee
the difference. One is left with the clear suggestion that: while the
vocabularydistributionsacrossthedifferentdatasetsaren’tthatdif-
ferent,thewaysinwhichthesevocabularywordsare combinedinto
trigramsare much more stylistic and templated in code comments.
Result 1: Code comments are far more repetitive than the Eng-
lish found in Natural Language Translation datasets
Given this relatively greater repetitive structure in code com-
ments, we can expect that the performance of translation tools
will be strongly influenced by repeating (and/or very frequent) tri-
grams.Ifafewfrequent n-gramsaccountformostofthedesired
outputinacorpus,itwouldseemthatthesetrigramswouldplay
a substantial, perhaps misleading role in measured performance.
Figure 3 supports this analysis. The right-hand side plot shows the
effectonBLEU-4ofreplacingsinglewords(unigrams)withrandomtokensinthe“Golden"(desired)outputinthevariousdatasets.The
left-handplotshowsthe effectofreplacingtrigrams.Theindex(1to 100) on the x-axis shows the number of most frequent n-grams
replaced with random tokens. The y-axis shows the decrease in
measured BLEU-4 as the code is increasingly randomized.
TheUnigramsplotsuggeststhattheeffectonthedesiredNatural
language("nl")output,asmeasuredbyBLEUisrelativelygreater
when compared to most of the comment datasets. This effect is
reversedforTrigrams;the"nl"datasetisnotaffectedasmuchby
the removal of frequent Trigrams as the comment datasets. Thisanalysis suggests that a tool that got the top few most frequenttrigramswronginthecode-commentgenerationtaskwouldsuf-feralargerperformancepenaltythanatoolthatgotthetop-few
n-gramswronginanaturallanguagetranslationtask.Thisvisual
evidence is strongly confirmed by rigorous statistical modeling,please see supplementary materials, bleu-cc.Rmd for the R code.
FrequenttrigramsthathaveabigeffectonthecodecommentBLEUincludee.g.,“
factory method for ”, “delegates to the ”,and“ method
for instantiating ”.Toputitanotherway,onecouldboosttheper-
formanceofacode-commenttranslationtool,perhapsmisleadingly,
by getting a few such n-grams right.
Result2: Frequent n-grams could wield a much stronger effect
on the measured BLEU performance on code-comment transla-
tion tasks than on natural language translation
4.2 Input-Output Similarity (RQ3)
Animportantpropertyofnaturallanguagetranslationisthatthere
is a general dependence of input on output. Thus, similar German
sentences should translate to similar English sentences. For ex-
ampletwoGermansentenceswithsimilargrammaticalstructure
andvocabularyshouldingeneralresultintwoEnglishsentences
whose grammatical structure and vocabulary resemble each other;
likewise, in general, the more different two German sentences are
in vocabulary and grammar, the more difference we expect in their
English translations. Exceptions are possible, since some similar
constructions have different meanings: ( kicking the ball vs.kicking
the bucket12). However, on average in large datasets, we should
expect that more similar sentences give more similar translations.
Whentrainingatranslationenginewithahigh-dimensionalnon-
linear function approximator like an encoder-decoder model using
deep learning, this monotonic dependence property is arguably
useful. We would expect similar input sentences to be encode into
similar points in vector space, thus yielding more similar output
sentences. How do natural language translation (German-English)
and code-comment datasets fare in this regard? To gauge this phe-
nomenon,wesampled10,000 randompairsofinputfragments from
eachofourdatasets,andmeasuredtheirsimilarityusingBLEU-M2,
as well as the similarity of the corresponding Golden (desired) out-
putfragments.Wethenplotthe inputBLEU-M2similarityforeach
sampled pair on the x-axis, and the BLEU-M2 similarity of the cor-
respondingpairof outputsonthey-axis.Weuseakernel-smoothed
2-dhistogramratherthanascatterplot,tomakethefrequencies
more visible,along with (we expect)an indication suggesting that
similar inputs yield similar outputs. Certainly, most inputs and
outputs are different, so we expect to find a large number of highly
dissimilar pairs where input and output pair BLEUs are virtually
zero.Soweconsideredourrandomsamples,withandwithoutinput
and output similarities bigger than ϵ=10−5. The highly dissimilar
input-outputpairsareomittedintheplot.However,asanadditional
quantitativetestofcorrelation,wealsoconsideredSpearman’s ρ.
both with and without the dissimilar pairs.
The bivariate plots are shown in Fig 4 and the Spearmans ρ
in table 2. We have split the 10,000 random samples into two cases:
12The latter idiom indicates death; Some automated translation engines (e.g. Bing)
seem to know the difference when translating from English
751Trigrams Unigrams
0 2 55 07 5 1 0 0 0 25 50 75 100255075100
Number of n −grams removedResidual Bleu ScoreDataset Color Key
codenn
deepcom1deepcom2docstring1docstring2funcom1funcom2nl
Figure3:Theeffectofmostfrequentunigrams(left)andtrigrams(right)onmeasured(smoothed)BLEU-4performance.BLEU-
4 was calculated after successive removals of of most frequent unigrams (right) and trigrams (left). The effect of removing
frequent Unigrams isbyandlargegreateronthenaturallanguagedataset("nl").However,theeffectofremovingfrequent tri-
grams,onthecommentdatasetsisgenerallystrongerthanonthe"nl"datasetduetohighdegreeofrepetitioninthecomment
datasets. These apparent visual differences are decisively confirmed by more rigorous statistical modeling.
Figure 4: Bivariate plots showing dependency of input-similarity to output-similarity. Pairs with similarity (BLEU-4) lessthan
10−5areomitted.Thenaturallanguagetranslationdatahavethestrongestdependency:similarinputshavethestrongest
tendency to provide similar outputs
one in which we do no further processing and one where bothinput/output BLEU similarities are both
>10−5. Spearman’s ρ,
significance, and sample sizes are shown for non-zeroes in the
columns(thenumberswithintheparenthesesincludethehighlydissimilar ones). From the table (last column) we see that about
25-96%ofthepairshavesomesimilarityonbothinputsandoutputs,
dependingonthedataset.ThetablealsoshowstheSpearman’s ρ
(first column) and significance (second column). Each subplot in
752Dataset Spearman’s ρSignificance Number of
Correlation p-value pairs with
ρ,BLEU>ϵp-value,BLEU>ϵBLEU>ϵ
(ρ,all) (p-value, all)
NL 0.70 (0.02) 0.0 (0.055) 2568
Deepcom1 0.056 (0.057) 1.0e-6 (2.2e-8) 7811
Deepcom2 0.036 (0.045) 1.5e-3 (1.1e-5) 7966
Docstring1 0.147 (0.16) 6.7e-42 (4.6e-59) 8585
Docstring2 0.041 (0.047) 9.5e-5 (3.7e-6) 9585
Funcom1 0.124 (0.083) 1.30e-18 (3.4e-16) 5026
Funcom2 0.122 (0.079) 3.03e-18 (6.7e-15) 5082
Codenn 0.012 (0.0012) 0.409 (0.904) 2532
Table 1: Correlation values (Spearman’s ρ, and significance,
p-value, for the plots in Figure 4. Values outside paranthe-
sisarecalculatedwithonlythepairshavingpairwiseBLEU
>10−5; values in paranthesis include all pairs. p-values are
adjusted with Benjamini-Hochberg familywise correction.
In all cases, we chose 10,000 random pairs
Fig4showsonedataset,wherex-axisistheBLEUsimilarityofa
pair’s inputs, and y-axis is that of outputs. The plot is a binned 2D
histogram, using colored hexagons to represent counts in that bin.
Arepresentativevariantofeachdatasetisplotted(asapplicable);
the omitted ones are visually very similar.
Wecanclearlyseeastrongerrelationshipbetweeninputandout-
put BLEUs in the natural language setting. Particularly for natural
languagedata,thisisfurtherevidencedbytheratherhighSpearman
correlation for the non-zero BLEU pairs (0.70!! ), and the evident vi-
sual dependence between input-input similarity and output-output
similarityisnoteworthy;thisindicatesthatthereisstrong,fairly
monotonicrelationshipinnaturallanguagetranslation:themore
similar the source, the more similar the translation!
This analysis suggests that natural language data has a stronger,
morespecificinput-outputdependence;thisalsosuggeststhattrans-
lationbetweenlanguagesismoreamenabletolearnablefunction-
approximatorslikedeeplearners;thisappearstobe substantially
lesstrueforcode-commentdata.Thisgivesusthefollowingcon-
clusion with reference to RQ3.
Result 3: The natural language translation (WMT) shows a
stronger input-output dependence than the CCT datasets in that
similar inputs are more likely to produce similar outputs.
4.3 Information Retrieval Baselines
As can be see in fig. 4 and table 2, datasets for the natural language
translationtaskshowasmootherandmoremonotonicinput-outputdependence;bycontrast,code-commentdatasetsseemtohavelittle
ornoinput-outputdependence.Thisfindingcastssomedoubton
the existence ofa general sequence-to-sequence code→comment
function that can be learned using a universal function approxi-
mator like a deep neural network. However it leaves open the pos-
sibilitythatamoredata-drivenapproach,thatsimplymemorizesthe training data in some fashion, rather than trying to generalize
from it, might also work. Thus, given a code input, perhaps we can
just try to find similar code in the training dataset, and retrieve
the comment associated with the similar code. This is a simple and
naive information-retrieval (IR) approach. We then compare this to
the IR performance on NL translation.
4.3.1 Method. We use Apache Solr Version 8.3.113to implement a
straightforward IR approach. Apache Solr is a open source docu-
ment search engine based on Apache Lucene. We simply construct
anindexofoverthecodepartsoftherelevantdatasets;givenacodeinput,weusethatasa“query"overtheindex,findtheclosestmatch,
and return the comment associated with the closest matching code
as the “generated comment".
We used the default parameters of Solr without tuning. This
includesthedefaultBM25scoringfunction[ 46].Foreachdataset,
weusealwaysthesametokenizationprocedureusedbyauthors.In
addition,weperformsomeadditional pre-processingonthecode,
thatistypicallyrequiredforIRapproaches.Forexample,weremovehighlyfrequentstopwordsfromthecode.Additionally,fordatasets
do not provide a tokenization phase that actually splits cammel-CaseWords or snake_case_words, we include terms for indexing
andsearchingwhichincludesthesplitformofthesewords.How-
ever, we note that the processing of stop words and word splitting
only effects a minor change in performance.
4.3.2 IR Results. We find that on most datasets the simple IR base-
lineapproachestheneuralmodels,andexceedsitforDeepCom1,
DocString1,andDocString2.Ho wever,IRdo espoorlyontheWMT
translationdataset,andalsoonCodeNN.Inbothcases,wespecu-
latethatthismayreflecttherelativelevelofredundancyinthese
datasets. CodeNN is drawn from StackOverflow, which tends tohave fewer duplicated questions; in the case of WMT, which is
hand-curated, we expect there would be fewer duplications.
Prior work [ 14,62] has used very sophisticated IR methods.
Wecannotclaimtosupersedethesecontributions;butwillpoint
out that a very naive IR method does quite well, in some cases
betterthanveryrecentlypublishedmethodsondatasets/dataset-
variations which currently lack IR baselines. We therefore view IR
baselinesasimportantcalibrationonmodelperformance;bytrying
such a simple baseline first one can help find pathologies in the
model or dataset which require further exploration.
Wealsonotethatthereisvariationresults.InDeepCom2f,which
includes 10 cross-project folds, we observe a wide range resultsranging from a BLEU-DC of
20.6to48.4! This level of variation
across folds is a cause for concern...this suggests depending on the
split,amodelwithhighercapacitytomemorizethetrainingdata
mightdobetterorworse,potentiallymuddlingtheresultsifonly
doingonesplit.Similarlywenoticethatbetweendifferentversions
of FunCom scores vary quite a bit; this variation may confound
measurement of actual differences due to technical improvements.
13https://lucene.apache.org/solr/
14Value is for 2019 Model but on the 2018 test split
753Dataset Method Used Score Score Method
DeepCom2f IR-Baseline 32.7 BLEU-DC
- DeepCom (SBT) [22] 38.2 -
- Seq2Seq [50] 34.9 [22] -
DeepCom1 IR-Baseline 45.6 BLEU-ncs
- Transformer [2, 53] 44.6 -
FunCom1 IR-Baseline 18.1 BLEU-FC
- astattend-gru [33] 19.6 -
FunCom2 IR-Baseline 18.2 BLEU-FC
- astattend-gru [33] 18.7 [32] -
- code2seq [6] 18.8 [32] -
- code+gnn+BiLSTM[32] 19.9 -
CodeNN IR-Baseline 7.6 BLEU-CN
-I RIyeret al. 13.7 [26] -
- CodeNN [26] 20.4 -
DocString2 IR-Baseline 32.7 BLEU-ncs
- Transformer [2, 53] 32.5 [2] -
DocString1 IR-Baseline 27.6 BLEU-Moses
- Seq2Seq 14.0 [8] -
NL de-en IR-Baseline 2.2 SacreBLEU
- FAIR Transformer[41] 42.714-
Table 2: Measurements of a simple information retrieval
baseline compared to various neural machine translation
basedmethods.Thescoringmethodweusemirrorstheoneused on the dataset (see Section 3.1).
Recommendation:
Since even naive IR methods provide
competitiveperformanceinmanyCCTdatasets,theycanbe
animportantpartforcheckingforissuesinthenewcollection
and new processing of CCT datasets.
4.4 Calibrating BLEU Scores
We now return our last research question, RQ 5. How should we
interpret the BLEU results reported in prior work, and also the
informationretrievalBLEUnumbersthatwefound(whicharein
the same range, see table 1)?
TocalibratethesereportedBLEUscores,weconductedanobser-
vationalstudy,using affinitygroups (AGs)ofmethodsthatmodel
different levels of expected similarity between the methods. For
example,considerarandompairofmethods,sothatbothelements
ofthepairare methodsfromadifferentproject.Thisisourlowest-
affinitygroup;wewouldexpectthecommentstohaveverylittle
in common, apart from both being utterances that describe code.
The next higher affinity group is a random pair of methods from
the same project. We would expect these to be a bit more similar,
sincetheyarebothconcernedwiththesameapplicationdomain
or function. The next higher level would methods in the same class
which presumably are closer, although they would be describing
differentfunctions.Bytakingalargenumberrandompairsfromeach of these affinity groups, and measuring the BLEU for pairs in
each group, we can get an estimate of BLEU for each group. For
ourexperiment,wepickedthe1000largestprojectsfromGithub,
and then chose 5000 random pairs from each of the affinity groups.
Foreachpair,werandomlypickedoneasthe“reference"output,and the other as the “candidate” output, and the BLEU-M2 score.
We report the results in two different ways, in fig. 5 and in table 3.
Forintraclass,wedonottakemorethansixrandompairsfroma
singleclass.InallAGs,weremovedallbutoneoftheoverloaded
methods,andallgettersandsettersbeforeouranalysis.Without
this filtering we see a difference of around 1-3 points.
Figure 5: The distribution of BLEU scores between affin-ity groups. Red lines represent the means (i.e. the SentenceBLEU), and the dashed lines represent quartiles.
Firstwedescribefig.5whichshowsthedistributionoftheBLEU
scores in the 3 AGs. As might be expected, the inter-project AG
showsafairlylowmean,around3.Theintra-projectAGisafew
BLEU points higher. Most notably, the intraclass AG has a BLEU
score around 22, which is close to the best-in-class values reported
in prior work for some (but not all) datasets.
Note with the implemented experiment we cannot exactly com-
parethesenumbers,aseachdatasetisdrawnfromadifferentdis-
tribution. Most CCT datasets provide the data in the traditionaltranslation format of (source, target) pairs, making it difficult torecover the other affinity group pairs of a given dataset example.
This is why created our own new sampling based off of 1000 large
Github repos. While not exactly comparable to existing datasets,new creations of CCT data could start with this simple affinity
group baseline to calibrate the reported results.
Another, stronger, affinity grouping would be methods that are
semantically equivalent. Rather than trying to identify such an
affinity group ourselves by hand (which might be subject to confir-
mationbias)weselectedmatchedAPIcallsfromarecentproject,
SimilarAPI15by Chen[13]which used machine learning methods
tomatchmethodsindifferent,butequivalentAPIs(e.g., Junitvs.
testNG). We extracted the descriptions of 40 matched pairs of high
scoring matches from different APIs and computed their BLEU. Wefound that these BLEU scores are on average about 8 points higher,
with a mean around 32. This number should be taken with cau-
tion, however, since comments in this AG sample are substantially
shorter than in the other groups.
15http://similarapi.appspot.com/
754Recommendation: Testingaffinitygroupscanprovidea
baseline for calibrating BLEU results on a CCT dataset, as
thesimpletrickofgeneratingcommentsforagivenmethod
simplybyretrievingthecommentsofarandomothermethod
in the same class possibly can approach SOTA techniques.
Postscript: (BLEU Variability ) We noted earlier in Section 4.3.2,
page 8, that there was considerable intrinsic variation within a
dataset, simply across different folds ; we reported that measured
BLEU-DCinDeepCom2frangedfrom20.6to48.4;similarresults
were noted in the different variants of FunCom. This above affinity
group experiment with IR provided an opportunity to calibrate
another BLEU variability, across different ways of calculating BLEU.
Function intraclass
BLEU-FC 24.81BLUE-CN 21.34BLEU-DC 23.5SacreBLEU 24.81BLEU-Moses 24.6BLEU-ncs 21.49BLEU-M2 21.22
Table 3: The scores of samples of Java methods from the
same class.
We took the 5000-pair sample from the intraclass sample, and
measured the sentence BLEU for these pairs using the BLEU imple-
mentation variations used in the literature. The resultsare shown
intable3.Thevaluesrangefromaround21.2toaround24.8;this
range is actually rather high, compared to the gains reported in
recently published papers. This finding clarifies the need to have a
standardized measurement of performance.
Observation: Measurements show substantial variation.
The version of BLEU chosen, and sometimes even the folds in
thetraining/testsplit,cancausesubstantialvariationinthe
measuredperformance,thatmayconfoundtheabilitytoclaim
clear advances over prior work.
5 DISCUSSION
We summarize our main findings and their implications.
Comment Repetitiveness Our findings presented in Figure 1
show that commentsin CCTdatasets arefar morerepetitive than
theEnglishfoundintheWMTdataset;figure2suggeststhatthisnot
merely a matter of greater vocabulary in distribution in comments,
butratherafunctionofhowwordsarecombinedincomments.The
highly-prevalent patterns in comment have a substantially greater
impactonthemeasuredBLEUperformanceofmodelstrainedwith
this CCT data, as shown in Figure 3. A closer look at the CCT
datasetsshows thattrigramssuch as creates a new ,returns true
if,constructor delegates to ,factory method for areveryfrequent.
Gettingtheseright(orwrong)hasahugeinfluenceonperformance.Implications: Thesefindingssuggestthatgettingjustafewcommon
patterns of comments right might deceptively affect measured per-
formance.Sotheactualperformanceofcommentgenerationmight
deviate a lot from measured values, much more so relative to natu-
ral language translation. Repetition in comments might also mean
that fill-in-the-blanks approaches [ 49] might be revisited, with a
more data-driven approach; classify code first, to find the right
template,andthenfill-in-the-blanks,perhapsusinganattention-
or copy-mechanism.
Input/Output Dependence Whentranslatingfromonelanguage
toanother,onewouldexpectthatmoresimilarinputsproducemore
similar outputs, and that this dependence is relatively smooth and
monotonic.Ourfindingsinfigure4andtable1,indicatethatthis
property is indeed very strongly true for general natural language
outputs, but not as much for the comments.
Implications: Deep-learningmodelsareuniversalhigh-dimensional
continuous function approximators. Functions exhibiting a smooth
input-outputdependency,couldbereasonablyexpectedtobeeasier
to model. BLEU is a measure of lexical (token sequence) similarit);
therathernon-functional natureofthedependencysuggestedby
figure 4 and table 1 indicate that token-sequence models that work
wellforNaturallanguagetranslationmaybelessperformantfor
code;it maybe thatother,non-sequential modelsofcode, suchastree-based or graph-based, are worth exploring further [32, 56]
Baselining with IR
Our experience suggests that simple IR ap-
proachprovidesBLEUperformancethatiscomparabletocurrent
state of the art.
Implications Our findings suggest that a simple, standard, basic IR
approachwouldbeausefulbaselineforapproachestotheCCTtask.
EspeciallyconsideringtherangeofdifferentBLEUandtokenization
approaches, this would be a useful strawman baseline.
Interpreting BLEU Scores BLEU, METEOR, ROGUE etc are mea-
suresthathavebeendevelopedfordifferenttaskinnaturallanguage
processing, such as translation & summarization, often after exten-
sive,carefullydesigned,humansubjectstudies.SinceBLEUismost
commonly used in code-comment translation, we took an obser-
vationalapproachcalibratetheBLEUscore.Ourresults,reported
in fig. 5 and table 3 indicate that the reported BLEU scores are not
that high.
Implications: ThebestreportedBLEUscoresfortheGerman-English
translation tasks are currently are in the low 40’s. Our affinity
group calibration suggests that on some datasets, the performance
of models are comparable on average to retrieving the comment of
a random method from the same class. While this conclusion can’t
beexplicitlydrawnforaspecificdatasetwithoutusingtheexact
examples and processing from that specific dataset, but comparing
results at an affinity group level can provide insight into minimum
expected numbers for a new CCT datset.
Learning From NLP Datasets Wefindthatthecurrentlandscape
ofCCTdatasetstoberathermessy.Thereareoftenseveraldifferent
versions of the same dataset with different preprocessing, splits,
andevaluationfunctionswhichallseemequivalentinname,but
unless extra care is taken, might not be comparable.
755However,sometasksinNLPdonotseemtoobservesuchvari-
ancewithinatask.Wepostulatethiscouldbeduetoseveralreasons.
For one, with the popularity of large open source repositories, it
has become cheap and easy for a software engineering researcher
to collect a large number of pairs of code and comments. This does
not require hiring a human to label properties of text, and thus
less effort might be taken on quality control compared to NLP data
collection. Because researchers are domain experts in the datasets,
they might be also more willing to apply their own version of
preprocessing.
In addition, there are a wider array of tools to enforce consis-
tencyonvariousNLPtasks. ForexampletheWMTconferenceon
translation,acompetitionisranwithheldoutdataandhumaneval-
uation.Othertasks,suchasSQuAD[ 45]forreadingcomprehension
and GLUE[ 55]for multitaskevaluationallow foruploading code
toaserverwhichrunstheproposedmodelonheldoutdata.This
ensure consistency in evaluation metrics and data.
We view adapting some these techniques as an interesting av-
enue for future work.
6 THREATS TO VALIDITY
Our paper is a retrospective, and doesn’t propose any new tools,
metrics,etc, still some potential threats exist to our findings.
Fold Variance With the exception of DeepCom2f we did not run
measures over multiple folds or samples of the data. This makes it
possible that there is variance in some of our reported numbers.The Affinity Benchmarks
When collecting affinity groups, we col-
lectfullmethodsandprocessthemusingasetoffilters.Thismeans
thatwhencomparingthesenumbers,theymightnotbedirrectly
comparabletoaspecificdataset.Thenumbersarepresentedonly
as estimate of similarity of the affinity groups.Replication Threat
Wheneverwehadto,wedidourbesttoreplicate,
and measure the quantities we reported using the same code as
the previous work. Still, it is possible that we failed to comprehend
some subtletiesin the providedcode, and thismay bea threat to
our findings.Generalizability
We covered all the commonly used datasets and
literaturewecouldfind.However,itmaybethatwehavemissed
some where cases our findings don’t hold.
7 CONCLUSION
In this paper, we described a retrospective analysis of several re-
search efforts which used machine learning approaches, originally
designedforthetaskofnaturallanguagetranslation,forthetask
of generating comments from code. We examined the datasets,
the evaluation metrics, and the calibration thereof. Our analysis
pointedoutsomekeydifferencesbetweengeneralnaturallanguage
corpora and comments: comments are a lot more repetitive. We
also found that a widely used natural language translation dataset
shows a stronger, smootherinput-output relationships than natu-
rallanguage.Turningthentoapopularevaluationmetric(BLEU
score) we found considerable variation based on the way it’s calcu-
lated; in some cases this variation exceeded claimed improvements.
Looking at calibration of the reported BLEU scores, first, we found
thatsimpleoff-the-shelfinformationretrievaloffersperformancecomparable to that reported previously. Second, we found that the
simple trick of retrieving a comment associated with a methodin the same class as a given method achieves an average perfor-
mancecomparabletocurrentstate-of-the-art.Ourworksuggests
thatfutureworkintheareawouldbenefitfroma)otherkindsof
translationmodelsbesidessequence-to-sequenceencoder-decorder
modelsb)morestandardizedmeasurementofperformanceandc)
baselining against Information Retrieval, and against some very
coarsefoils(likeretrievingacommentfromarandomothermethod
in the same class).
Funding for this research was provided by National Science
Foundation, under grant NSF 1414172, SHF: Large: Collaborative
Research: Exploiting the Naturalness of Software.
Source code and data will be made available at https://bit.ly/
3lBDegY
REFERENCES
[1]Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. 2019. JuICe: A Large Scale
Distantly Supervised Dataset for Open Domain Context-based Code Generation.
arXiv preprint arXiv:1910.02216 (2019).
[2]Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.2020. A Transformer-based Approach for Source Code Summarization.
arXiv:cs.SE/2005.00653
[3]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learningmodelsofcode.In Proceedingsofthe2019ACMSIGPLANInternational
SymposiumonNewIdeas,NewParadigms,andReflectionsonProgrammingand
Software. 143–153.
[4]Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at-tention network for extreme summarization of source code. In International
conference on machine learning. 2091–2100.
[5]Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Gen-erating sequences from structured representations of code. arXiv preprint
arXiv:1808.01400 (2018).
[6]Uri Alon, Omer Levy, and Eran Yahav. 2018. code2seq: Generating Se-
quencesfromStructuredRepresentationsofCode. CoRRabs/1808.01400(2018).
arXiv:1808.01400 http://arxiv.org/abs/1808.01400
[7]SushilBajracharya,TrungNgo,ErikLinstead,YimengDou,PaulRigor,PierreBaldi, and Cristina Lopes. 2006. Sourcerer: a search engine for open source
code supporting structure-based search. In Companion to the 21st ACM SIGPLAN
symposium on Object-oriented programming systems, languages, and applications.
ACM, 681–682.
[8]Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of
Python functions and documentation strings for automated code documentation
and code generation. CoRRabs/1707.02275 (2017). arXiv:1707.02275 http://arxiv.
org/abs/1707.02275
[9]LoïcBarrault,OndřejBojar,MartaR.Costa-jussà,ChristianFedermann,Mark
Fishel,YvetteGraham,BarryHaddow,MatthiasHuck,PhilippKoehn,Shervin
Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Mar-cos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation
(WMT19).In ProceedingsoftheFourthConferenceonMachineTranslation(Volume
2:SharedTaskPapers,Day1).AssociationforComputationalLinguistics,Florence,
Italy, 1–61. https://doi.org/10.18653/v1/W19-5301
[10]Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp
Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-
Amand, et al. 2014. Findings of the 2014 workshop on statistical machine trans-
lation. In Proceedings of the ninth workshop on statistical machine translation.
12–58.
[11]Raymond PL Buse and Westley R Weimer. 2008. Automatic documentation
inferenceforexceptions.In Proceedingsofthe2008internationalsymposiumon
Software testing and analysis. Citeseer, 273–282.
[12]BoxingChenandColinCherry.2014. ASystematicComparisonofSmoothing
Techniques for Sentence-Level BLEU. In WMT@ACL.
[13]ChunyangChen.[n.d.]. SimilarAPI:MiningAnalogicalAPIsforLibraryMigra-
tion. ([n.d.]).
[14]Qingying Chen and Minghui Zhou. 2018. A Neural Framework for Retrieval
and Summarization of Source Code. In Proceedings of the 33rd ACM/IEEE Interna-
tional Conference on Automated Software Engineering (ASE 2018). Association for
ComputingMachinery,NewYork,NY,USA,826–831. https://doi.org/10.1145/
3238147.3240471
756[15]Luis Fernando Cortés-Coy, Mario Linares-Vásquez, Jairo Aponte, and Denys
Poshyvanyk. 2014. On automatically generating commit messages via sum-marization of source code changes. In 2014 IEEE 14th International Working
Conference on Source Code Analysis and Manipulation. IEEE, 275–284.
[16]Deborah Coughlin. 2003. Correlating automated and human assessments of
machine translation quality. In Proceedings of MT summit IX. 63–70.
[17]Sergio Cozzetti B de Souza, Nicolas Anquetil, and Káthia M de Oliveira. 2005. A
study of the documentation essential to software maintenance. In Proceedings of
the23rdannualinternationalconferenceonDesignofcommunication:documenting
& designing for pervasive information. ACM, 68–75.
[18] Jacob Eisenstein. 2018. Natural language processing.[19]
BeatFluri,MichaelWürsch,and HaraldC.Gall.2007. DoCodeandComments
Co-Evolve? On the Relation between Source Code and Comment Changes. 14th
Working Conference on Reverse Engineering (WCRE 2007) (2007), 70–79.
[20]SakibHaque,AlexanderLeClair,LingfeiWu,andCollinMcMillan.2020.Improved
AutomaticSummarizationofSubroutinesviaAttentiontoFileContext. arXiv
preprint arXiv:2004.04881 (2020).
[21]Matthew Henderson, Ivan Vulić, Iñigo Casanueva, Paweł Budzianowski, Daniela
Gerz, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrkšić, and
Pei-Hao Su. [n.d.]. PolyResponse: A Rank-based Approach to Task-Oriented
DialoguewithApplicationinRestaurantSearchandBooking.In Proceedingsof
EMNLP 2019.
[22]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code commentgeneration. In Proceedings of the 26th Conference on Program Comprehension.
ACM, 200–210.
[23]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2019. Deep code comment
generationwithhybridlexicalandsyntacticalinformation. EmpiricalSoftware
Engineering (2019), 1–39.
[24]Xing Hu, Yuhan Wei, Ge Li, and Zhi Jin. 2017. CodeSum: Translate program
language to natural language. arXiv preprint arXiv:1708.01837 (2017).
[25]Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt.2019. CodeSearchNetChallenge:EvaluatingtheStateofSemantic
Code Search. arXiv:cs.LG/1909.09436
[26]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizingsourcecodeusinganeuralattentionmodel.In Proceedingsofthe
54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
Long Papers). 2073–2083.
[27]Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat-
ing commit messages from diffs using neural machine translation. In 2017 32nd
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 135–146.
[28]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
AndreaJanes.[n.d.]. BigCode!=BigVocabulary:Open-VocabularyModelsfor
Source Code.
[29]Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M
Rush.2017. Opennmt:Open-sourcetoolkitforneuralmachinetranslation. arXiv
preprint arXiv:1701.02810 (2017).
[30]PhilippKoehn.2009. Statisticalmachinetranslation. CambridgeUniversityPress.
[31]DouglasKramer.1999. API documentationfromsource codecomments: acase
study of Javadoc. In Proceedings of the 17th annual international conference on
Computer documentation. 147–153.
[32]AlexanderLeClair,SakibHaque,LingfeiWu,andCollinMcMillan.2020.Improved
Code Summarization via a Graph Neural Network. arXiv:cs.SE/2004.02843
[33]Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for
generatingnaturallanguagesummariesofprogramsubroutines.In Proceedings
ofthe41stInternationalConferenceonSoftwareEngineering.IEEEPress,795–806.
[34]AlexanderLeClairandCollinMcMillan.2019. RecommendationsforDatasets
for Source Code Summarization. CoRRabs/1904.02660 (2019). arXiv:1904.02660
http://arxiv.org/abs/1904.02660
[35]Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu
Wang. 2018. Neural-machine-translation-based commit message generation:
how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering. 373–384.
[36]Edward Loper and Steven Bird. 2002. NLTK: The Natural Language Toolkit.InProceedings of the ACL Workshop on Effective Tools and Methodologies for
TeachingNaturalLanguageProcessingandComputationalLinguistics.Somerset,
NJ:AssociationforComputationalLinguistics,62–69. http://arXiv.org/abs/cs/
0205028.
[37]Paul W McBurney, Cheng Liu, Collin McMillan, and Tim Weninger. 2014. Im-proving topic model source code summarization. In Proceedings of the 22nd
international conference on program comprehension. ACM, 291–294.
[38]PaulWMcBurneyandCollinMcMillan.2014. Automaticdocumentationgen-
erationvia sourcecodesummarizationof methodcontext.In Proceedingsof the
22nd International Conference on Program Comprehension. ACM, 279–290.
[39]Jessica Moore, Ben Gelman, and David Slater. 2019. A Convolutional Neural Net-
work for Language-Agnostic Source Code Summarization. CoRRabs/1904.00805
(2019). arXiv:1904.00805 http://arxiv.org/abs/1904.00805[40]Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock,
andKVijay-Shanker.2013. Automaticgenerationofnaturallanguagesummaries
for java classes. In 2013 21st International Conference on Program Comprehension
(ICPC). IEEE, 23–32.
[41]Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and SergeyEdunov. 2019. Facebook FAIR’s WMT19 News Translation Task Submission.
arXiv:cs.CL/1907.06616
[42]Yoann Padioleau, Lin Tan, and Yuanyuan Zhou. 2009. Listening to program-
merstaxonomiesandcharacteristicsofcommentsinoperatingsystemcode.In
Proceedings of the 31st International Conference on Software Engineering. IEEE
Computer Society, 331–341.
[43]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:a
method for automatic evaluation of machine translation. In Proceedings of the
40th annualmeetingon associationfor computationallinguistics.Association forComputational Linguistics, 311–318.
[44]
Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores.
arXiv:cs.CL/1804.08771
[45]Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’tKnow: Unanswerable Questions for SQuAD. CoRRabs/1806.03822 (2018).
arXiv:1806.03822 http://arxiv.org/abs/1806.03822
[46]Stephen E Robertson and Steve Walker. 1994. Some simple effective approxi-
mationstothe2-poissonmodelforprobabilisticweightedretrieval.In SIGIR’94.
Springer, 232–241.
[47]PaigeRodeghero,CollinMcMillan,PaulWMcBurney,NigelBosch,andSidney
D’Mello. 2014. Improving automated source code summarization via an eye-
trackingstudyofprogrammers.In Proceedingsofthe36thinternationalconference
on Software engineering. ACM, 390–401.
[48]Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-
Shanker.2010. Towardsautomaticallygeneratingsummarycommentsforjava
methods. In Proceedings of the IEEE/ACM international conference on Automated
software engineering. ACM, 43–52.
[49]Giriprasad Sridhara, Lori Pollock, and K Vijay-Shanker. 2011. Automatically
detecting and describing high level actions within methods. In Proceedings of the
33rd International Conference on Software Engineering. ACM, 101–110.
[50]Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to SequenceLearning with Neural Networks. CoRRabs/1409.3215 (2014). arXiv:1409.3215
http://arxiv.org/abs/1409.3215
[51]ArmstrongATakang,PennyAGrubb,andRobertDMacredie.1996.Theeffectsof
comments and identifier names on program comprehensibility: an experimental
investigation. J. Prog. Lang. 4, 3 (1996), 143–167.
[52]AshishVaswani,SamyBengio,EugeneBrevdo,FrancoisChollet,AidanNGomez,
Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar,et al
.2018. Tensor2tensor for neural machine translation. arXiv preprint
arXiv:1803.07416 (2018).
[53]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. arXiv:cs.CL/1706.03762
[54]Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
PhilipSYu.2018. Improvingautomaticsourcecodesummarizationviadeeprein-forcementlearning.In Proceedingsofthe33rdACM/IEEEInternationalConference
on Automated Software Engineering. 397–407.
[55]AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelR.
Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for
Natural Language Understanding. CoRRabs/1804.07461 (2018). arXiv:1804.07461
http://arxiv.org/abs/1804.07461
[56]W. Wang, Y. Zhang, Y. Sui, Y. Wan, Z. Zhao, J. Wu, P. Yu, and G. Xu. 2020.
Reinforcement-Learning-GuidedSourceCodeSummarizationviaHierarchical
Attention. IEEE Transactions on Software Engineering (2020), 1–1.
[57]WenhuaWang,YuqunZhang,ZhengranZeng,andGuandongXu.2020. TranS3:
ATransformer-basedFrameworkforUnifyingCodeSummarizationandCode
Search. arXiv:cs.SE/2003.03238
[58]BolinWei,GeLi,XinXia,ZhiyiFu,andZhiJin.2019. CodeGenerationasaDual
Task of Code Summarization. arXiv:cs.LG/1910.05923
[59]Edmund Wong, Taiyue Liu, and Lin Tan. 2015. Clocom: Mining existing source
code for automatic comment generation. In 2015 IEEE 22nd International Confer-
ence on Software Analysis, Evolution, and Reengineering (SANER) . IEEE, 380–389.
[60]PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGrahamNeubig.
2018. Learning to mine aligned code and natural language pairs from stackoverflow. In 2018 IEEE/ACM 15th International Conference on Mining Software
Repositories (MSR). IEEE, 476–486.
[61]Annie TT Ying and Martin P Robillard. 2013. Code fragment summarization. InProceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering.
ACM, 655–658.
[62]Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020.
Retrieval-basedNeuralSourceCodeSummarization.In Proceedings,42ndInter-
national Conference on Software Engineering.
757