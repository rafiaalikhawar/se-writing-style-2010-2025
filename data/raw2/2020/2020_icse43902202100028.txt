Resource-Guided Conﬁguration Space Reduction
for Deep Learning Models
Yanjie Gao1, Yonghao Zhu1, Hongyu Zhang2, Haoxiang Lin1†, Mao Yang1
1Microsoft Research, Beijing, China
2The University of Newcastle, NSW, Australia
Email:fyanjga, v-yonghz, haoxlin, maoyang g@microsoft.com, hongyu.zhang@newcastle.edu.au
Abstract —Deep learning models, like traditional software sys-
tems, provide a large number of conﬁguration options. A deep
learning model can be conﬁgured with different hyperparameters
and neural architectures. Recently, AutoML (Automated Machine
Learning) has been widely adopted to automate model training by
systematically exploring diverse conﬁgurations. However, current
AutoML approaches do not take into consideration the computa-
tional constraints imposed by various resources such as available
memory, computing power of devices, or execution time. The
training with non-conforming conﬁgurations could lead to many
failed AutoML trial jobs or inappropriate models, which cause
signiﬁcant resource waste and severely slow down development
productivity.
In this paper, we propose DnnSAT, a resource-guided AutoML
approach for deep learning models to help existing AutoML tools
efﬁciently reduce the conﬁguration space ahead of time. DnnSAT
can speed up the search process and achieve equal or even better
model learning performance because it excludes trial jobs not
satisfying the constraints and saves resources for more trials.
We formulate the resource-guided conﬁguration space reduction
as a constraint satisfaction problem. DnnSAT includes a uniﬁed
analytic cost model to construct common constraints with respect
to the model weight size, number of ﬂoating-point operations,
model inference time, and GPU memory consumption. It then
utilizes an SMT solver to obtain the satisﬁable conﬁgurations of
hyperparameters and neural architectures. Our evaluation results
demonstrate the effectiveness of DnnSAT in accelerating state-
of-the-art AutoML methods (Hyperparameter Optimization and
Neural Architecture Search) with an average speedup from 1.19X
to 3.95X on public benchmarks. We believe that DnnSAT can
make AutoML more practical in a real-world environment with
constrained resources.
Index Terms —conﬁgurable systems, deep learning, AutoML,
constraint solving
I. I NTRODUCTION
Many traditional software systems, such as compilers and
web servers, are highly conﬁgurable. They provide many
conﬁguration options, and different combinations of the options
could lead to different system quality attributes. In recent
years, deep learning ( DL) models have become an integral part
of many modern software-intensive systems such as speech
recognition systems, chatbots, and games. Like traditional
software systems, DLmodels also provide many conﬁguration
options for developers to tune. These conﬁguration options can
be largely classiﬁed into two categories: hyperparameter (such
as the batch size and learning rate) and neural architecture (such
†Corresponding author.as the number of layers and layer type). The numerous options
provided by a DLmodel result in an exponential number
of possible conﬁgurations, making manual tuning extremely
tedious and time-consuming.
Recently, automated machine learning (AutoML) techniques
have been widely adopted to automate and accelerate DL
model tuning. AutoML tools such as NNI (Neural Network
Intelligence) [1] usually apply Hyperparameter Optimization
(HPO) [2] and Neural Architecture Search (NAS) [3] algorithms
and launch hundreds or even thousands of AutoML trial jobs (or
trials for short) to systematically explore diverse conﬁgurations
of hyperparameters and neural architectures. It has been found
that AutoML signiﬁcantly boosts the productivity of DL
development [2]–[4].
However, current AutoML approaches do not take into
consideration the computational constraints imposed by various
resources such as available memory, computing power of
devices, or execution time, because the resources required
by a DLmodel often remain unknown to developers before
job execution. Non-conforming model conﬁgurations could
lead to many failed AutoML trials or inappropriate models,
which not only waste signiﬁcant shared resources (such as
GPU, CPU, storage, and network I/O) but also severely slow
down development productivity. A typical resource is GPU
memory, which is critical yet limited for training. If developers
do not size the model carefully enough, trials will trigger
OOM (out-of-memory) exceptions and fail. For instance, one
PyTorch ResNet-50 [5] trial with an overlarge batch size of 256
causes an OOM when being scheduled on the NVIDIA Tesla
P100 GPU; it requires 22 GB of GPU memory, but P100 has
only 16 GB in total [6]. Even worse, since there can be more
than ten tunable hyperparameters for the ResNet-50 model,
other hundreds of trials with the same batch size could also
experience OOM and crash. According to our recent empirical
study on failed DLjobs in Microsoft [7], 8.8% of the 4960
job failures were caused by GPU memory exhaustion, which
accounts for the largest category in all deep learning speciﬁc
failures. Therefore, it is necessary for AutoML tools to enforce
a constraint that a DLmodel cannot consume more GPU
memory than the capacity when exploring model conﬁgurations.
Another useful constraint is that the size of a model’s weights
cannot exceed a certain upper bound. Otherwise, the resulting
DLapplication may be too large for efﬁcient management and
execution due to insufﬁcient computing power of the target
1752021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 ©2021 IEEE
DOI 10.1109/ICSE43902.2021.00028
resource-restricted devices. If the unsatisﬁable AutoML trials
can be excluded ahead of time, resources will be saved to
perform more trials, thus a larger conﬁguration space could be
explored.
A simple workaround is to run and proﬁle trials for a
while to estimate their resource consumption. Such a resource-
consuming method is unaffordable in the scenario of AutoML,
where there exist a large number of possible hyperparameter
combinations and neural architectures. Some research work [8]–
[10] incorporates certain resource quantiﬁcation into the loss
function for a global optimization with the model learning
performance ( e.g., accuracy). However, their purpose is to
reduce the resource consumption of the ﬁnal model as much
as possible while achieving an expected model learning
performance, instead of excluding unsatisﬁable trials in advance
to improve the search efﬁciency. Therefore, such work could
also cause failed trials/inappropriate models and resource waste.
Besides, they are limited to NAS algorithms only and cannot
be applied to HPO algorithms.
In this paper, we propose DnnSAT , a resource-guided
AutoML approach for deep learning models, which can help
existing AutoML tools efﬁciently reduce the conﬁguration
space ahead of time. We formulate such a resource-guided
space reduction problem as a constraint satisfaction problem
(CSP) [11]. DnnSAT includes a uniﬁed analytic cost model to
construct common constraints with respect to the model weight
size, number of ﬂoating-point operations (FLOPs), model
inference time, and GPU memory consumption. It then utilizes
an SMT (satisﬁability modulo theories) solver ( e.g., Microsoft
Z3 [12]) to obtain the satisﬁable model conﬁgurations before
trial execution. According to the characteristics of constraints
(e.g., monotonicity), we also apply some special optimizations
to accelerate the solving.
We have implemented DnnSAT and evaluated it exten-
sively on public AutoML benchmarks (HPOBench [13] and
NAS-Bench-101 [14]) with various search methods (Random
Search [15], Regularized Evolution [16], Hyperband [2], and
Reinforcement Learning [14]) and representative DLmodels
(VGG-16 [17] and LSTM [18]-based Seq2Seq [19]). The
experimental results show that DnnSAT achieves an average
speedup from 1.19X to 3.95X on public benchmarks and
noticeably reduces the conﬁguration space.
In summary, this paper makes the following contributions:
1)We propose a resource-guided AutoML approach for deep
learning models to efﬁciently reduce the conﬁguration
space ahead of time.
2)We build a uniﬁed analytic cost model to construct
common constraints and utilize an SMT solver to obtain
the satisﬁable conﬁgurations of hyperparameters and
neural architectures.
3)We implement a tool named DnnSAT and demonstrate
its practical effectiveness.
The rest of the paper is organized as follows. Section II
introduces the background. Section III presents our analytic
approach. Section IV details the design and implementation
ofDnnSAT . Section V shows experimental results. Section VI1from tensorflow.keras importlayers, models
2...
3model=models.Sequential()
4model.add(layers .Conv2D(filters =64, kernel_size =(3,3),
activation ='relu', input_shape =(32,32,3))) ,!
5model.add(layers .AveragePooling2D(pool_size =(2,2), padding
='same')) ,!
6model.add(layers .Flatten())
7model.add(layers .Dense(units =64, activation ='relu'))
8model.compile(optimizer ='adam',
loss=tf.keras.losses.MeanSquaredError()) ,!
9model.fit(train_images, train_labels, batch_size, epochs =10)
(a) Training program using Keras API.
(b) Computation graph for model infer-
ence.
(c) Tensors with shapes on which Conv2D
operates.
Fig. 1: A sample TensorFlow model sequentially constructed by the framework
built-inConv2D (2D convolution), AvgPool2D (2D average pooling),
Flatten (collapsing the input into one dimension without affecting the
batch size), and Dense (fully connected layer) operators.
discusses extensibility and possible threats. We survey related
work in Section VII and conclude this paper in Section VIII.
II. B ACKGROUND
Deep learning ( DL) is a subﬁeld of machine learning to learn
layered data representations known as models. A DLmodel
is formalized as a tensor-oriented computation graph [20] by
frameworks such as TensorFlow (TF) [21], PyTorch [22], and
MXNet [23], which is a directed acyclic graph (DAG). The
inputs and outputs of such a computation graph and its nodes
aretensors (multi-dimensional arrays of numerical values). The
shape of a tensor is the element number in each dimension
plus the element data type. Each node represents the invocation
of a mathematical operation called an operator (e.g., element-
wise matrix addition). An edge delivers an output tensor and
speciﬁes the execution dependency. In this paper, we use the
terms “operator” and “node” interchangeably since a node is
completely determined by its invoked operator.
Fig. 1a shows a simple TensorFlow training program using
the Keras [24] API, which sets up a sequential model with
the framework built-in Conv2D (2D convolution with a 33
kernel size), AvgPool2D (2D average pooling with the “same”
padding setting1),Flatten (collapsing the input into one
dimension without affecting the batch size), and Dense (fully
connected layer with 64 units) operators (lines 4-7). The above
ﬁlter size, padding, and number of units are hyperparameters ,
which are parameters to control the training process. Fig. 1b
demonstrates the corresponding computation graph for model
1See https://keras.io/api/layers/pooling layers/average pooling2d/ for an
explanation of the padding argument.
1761# search_space.json
2{
3"batch_size" :{"_type":"choice" ,"_value" :[16,32,64]},
4"kernel_size" :{"_type":"choice" ,"_value" :[3,5,7,11]},
5"filters" :{"_type":"choice" ,"_value" :[64,128,512]},
6"unit_size" :{"_type":"choice" ,"_value" :[64,512]},
7"lr":{"_type":"choice" ,"_value" :[0.0001,0.001,0.01,0.1]}
8}
9
10# config.yml
11...
12searchSpacePath: search_space .json
13tuner:
14builtinTunerName: Random
15trial:
16command: python3 mnist .py
17gpuNum:1
18cpuNum:1
19memoryMB: 8196
20...
Fig. 2: Settings of an MNIST training program supported by NNI with
Hyperparameter Optimization.
inference. Fig. 1c illustrates the input, weight, and output
tensors with shapes on which Conv2D operates.
To choose better conﬁgurations of hyperparameters and
neural architectures, developers often adopt a trial-and-error
strategy: submitting hundreds or even thousands of trial jobs
with each being assigned a different model conﬁguration. This
strategy is very inefﬁcient because of the overlarge conﬁg-
uration space. Recently, many AutoML tools, such as NNI
(Neural Network Intelligence) [1], Auto-Keras [4], and Auto-
Sklearn [25], are proposed to automate the exploration of model
conﬁgurations. They help developers ﬁnd a hyperparameter
combination (Hyperparameter Optimization (HPO) [13]) or
design an elaborate neural network (Neural Architecture Search
(NAS) [14]), which can both minimize the loss and maximize
the model learning performance ( e.g., accuracy).
Suppose that a DLmodel hasmhyperparameters whose
domains are B1;B2;;Bm. A model conﬁguration is an
instance of such a model with concrete hyperparameter values.
All model conﬁgurations constitute the conﬁguration space .
HPO applies some search method ( e.g., random search, evolu-
tionary strategies, or Bayesian optimization) in the enumeration
of the conﬁguration space to ﬁnd a candidate with the optimal
hyperparameter vector (combination) 2B1B2Bm,
which best meets the training objective. Fig. 2 shows the
settings of an MNIST training program supported by the
AutoML tool NNI with HPO. The upper part (lines 1-8) deﬁnes
the possible values of tunable hyperparameters (batch size,
learning rate, etc.); the bottom part (lines 10-20) speciﬁes
the search method and runtime resource requirements (GPU
count, main memory size, etc.). The controller process of
NNI performs a random search (line 14) and may fork over a
hundred trials executing the command on line 16. Trials send
timely feedback such as the mean squared error or accuracy
to the controller for the decision of early stopping.
Similarly, NAS automates the architecture engineering of a
DLmodel. The conﬁguration space consists of various automat-
ically generated, syntactically legal neural network structures
such as chained or multi-branch networks. NAS also appliesTABLE I
COMMON HYPERPARAMETERS OF DL MODELS .
Hyperparameter Domain Hyperparameter Domain
Batch Size N Kernel Size N
Output Channels N Stride N
Padding N # of RNN Layers N
# of Units N Sequence Length N
Operator Type N Edge B
Learning Rate R+Dropout [0;1)
different search methods including random search, gradient-
based algorithms, and reinforcement learning (RL) [26] for the
space exploration.
III. P ROPOSED APPROACH
A. Problem Formulation
Formally, a DLmodelXis represented as a directed acyclic
graph (DAG) [20]:
X=hfuign
i=1;f(ui;uj)gi6=j;fpkgm
k=1i
Each nodeuiis an operator ( i.e., a mathematical operation
such as convolution and pooling). A directed edge (ui,uj)
pointing from an output of node uito an input of ujdelivers
the output tensor and speciﬁes the execution dependency. Each
pkis a hyperparameter ( e.g., input tensor shape and batch size)
whose domain is denoted as Bk. Table I lists some commonly
used hyperparameters with their domains.
Forbk2Bkwhere 1km, we use X(b1;b2;;bm)
to represent one model conﬁguration of X. Then, the con-
ﬁguration space of model X, denoted by X, is deﬁned as
follows:
X=fX(b1;b2;;bm)jbk2Bkfork2[1;m]g
In an AutoML experiment, there may exist a series of DL
modelsX1;X2;;XN(e.g., models searched by NAS).
For such an experiment, its conﬁguration space is deﬁned
as the union set of all models’ conﬁguration spaces:
=SN
i=1Xi
We formulate the resource-guided conﬁguration space re-
duction for an AutoML experiment with Nmodels as the
following constraint satisfaction problem hV;D;Ci[11]:
V=fV1;V2;;VNg
D=fD1;D2;;DNjDi=Xifori2[1;N]g
C=fCj:lbjfj(Vi)ubjg
Vis a set of model conﬁguration variables, Drepresents the
respective variable domains, and Cis the constraint set. Each
variableVi2[1;N]can take on the model conﬁgurations of Xi
(i.e., the domain of ViisXi). For a constraint Cj,fjis a
non-negative restriction function which denotes the demand for
a certain resource; lbjandubjare the lower and upper bounds
of that resource, respectively. If one is more interested in the
upper bound, we simply assume that lbj= 0. Hyperparameters
177aredecision variables of the constraints, which are quantities
controlled by the decision-maker to deﬁne the search space for
optimization. As mentioned before, an example fjcalculates
the GPU memory consumption for training a DLmodel, and
ubjis the memory capacity of the allocated GPU device ( e.g.,
16 GB for NVIDIA Tesla P100).
B. The Proposed Analytic Approach
DnnSAT adopts an analytic approach to construct the restric-
tion functions on resources. We observe that the algorithmic
execution of a DLmodel can be represented as iterative forward
and backward propagation on its computation graph.2Therefore,
computing the resource consumption for one iteration is then
reduced to the calculation of the resource required by each
operator on the computation graph in accordance with a certain
graph traversal order. Currently, a DLmodel is required to
be deterministic without control-ﬂow operators ( e.g., loops
and conditional branches), thus we assume that the execution
ﬂow and resource consumption across different iterations are
identical. We deﬁne an auxiliary function gon the operator
set, which represents the current resource consumption when
the operator under visiting has just ﬁnished execution in
the iteration. Let S=hui1;ui2;;uinibe a topological
(linear) order extended from the edge order of the model
such thatuijSuik=)(uik;uij)=2X.Sis called an
operator schedule , representing the actual runtime execution
order of operators. DnnSAT pre-generates Sby referring to the
framework implementations [27]–[29]. Suppose that ris the
operator resource cost function, IterCnt is the iteration count,
andRinit,Rfini are the resource consumption of the one-off
initialization and clean-up performed by DLframeworks which
are assumed to be 0if not speciﬁcally mentioned. We deﬁne
gand the restriction function fas follows:
g(ui1) =r(ui1)
g(uij) =h(fhui1;g(ui1)i;;huij 1;g(uij 1)ig) +r(uij)
f(X) =Rinit+Rfini+t(IterCnt;fg(uij)gn
j=1)
So long as the above handtare functions, guniquely exists by
the transﬁnite recursion theorem schema [30] since Sis a well
order, andfalso exists. The formalization of gindicates that it
could compute the current resource consumption by referring to
extra information of previous consumption and visited operators.
Examples of gandfcan be found in Section IV-B .DnnSAT
includes a uniﬁed analytic cost model to deﬁne operators’
resource cost functions and common constraints. Table II lists
the constraints that we have implemented.
The objective of DnnSAT is to reduce the conﬁguration
spaceby eliminating those Xi(bi1;bi2;;bim)which
violate the enforced constraints before submitting AutoML
trials. A naive method is to compute the values of the restriction
functions for each model conﬁguration in and then check
whether such values fall within the allowed bounds. DnnSAT
adopts a much more efﬁcient approach: it speciﬁes those
2This also applies to model inference which has a simpliﬁed representation
with single-pass forward propagation and no backward propagation.TABLE II
COMMON CONSTRAINTS IMPOSED BY THECOMPUTATIONAL RESOURCES .
Restriction Category Resour ce Upper Bound Scenario
Model Weight Size Allowed Binary Size Inference
Number of Floating-point (Device FLOPS)  Inference
Operations (FLOPs) (SLA of Inference Latency)
Model Inference Time SLA of Inference Latency Inference
GPU Memory Consumption Device Memory Capacity Training
Inference
“FLOPS” denotes ﬂoating-point operations per second, and “SLA”
stands for service-level agreement.
Fig. 3: Workﬂow of DnnSAT.
constraints in the SMT-LIB (Satisﬁability Modulo Theories
Library [31]) syntax and uses the Microsoft Z3 solver [12]
to obtain all the satisﬁable model conﬁgurations. Such an
approach is feasible because the restriction functions of the
common constraints can be composed by SMT solvers’ built-in
functions ( e.g., multiplication and division) and the constraints
are simple numerical inequalities. We also apply some special
optimizations to accelerate the solving based on the constraint
characteristics ( e.g., monotonicity), which are described in
Section IV-E.
IV. D ESIGN AND IMPLEMENTATION
A. Workﬂow
Fig. 3 shows the workﬂow of DnnSAT . It accepts a source
DL model, conﬁguration settings, and constraint settings
as input. The model is parsed by a front-end parser and
reconstructed to the corresponding computation graph. Some
DLframeworks such as PyTorch employ the deﬁne-by-run
approach [22] such that a saved model records only an
execution trace instead of the full computation graph. DnnSAT
currently relies on users to supply multiple models in case
the graph may dynamically change (in the future, it may be
possible to try extracting the full computation graph from a
DLprogram by static code analysis). Conﬁguration settings
include the hyperparameters to be tuned and their domain
deﬁnitions. Constraint settings contain the built-in constraints
that need to be satisﬁed and their allowed bounds, as well as
constraint-related runtime constants of the DLframework ( e.g.,
type and version) and target device ( e.g., FLOPS).
DnnSAT has deﬁned four common constraints (Section IV-B )
and a set of analytic and framework-independent resource cost
functions for DLoperators (Section IV-C ). It traverses the
computation graph in accordance with a predeﬁned operator
schedule ( i.e., operator execution order) to automatically
generate the constraint speciﬁcations in SMT-LIB for later
solving (Section IV-D ). There are two working modes for
178integrating DnnSAT with existing AutoML tools. One is the
interactive mode, in which AutoML tools work as usual but
each conﬁguration will be sent to DnnSAT for solving via
an API call before launching a trial. Such a mode is simple,
non-intrusive, and requires less effort. We have run DnnSAT
with HPOBench [13] and NAS-Bench-101 [14] interactively
to evaluate the effectiveness in speeding up AutoML methods
(Section V-A), and similar integration work could be done in
AutoML tools such as NNI and Auto-Keras. The other is the
pruning mode, in which DnnSAT eliminates the unsatisﬁable
model conﬁgurations in advance and feedbacks a reduced
conﬁguration space to AutoML tools during their initialization.
To solve the constraints, the Microsoft Z3 solver is invoked with
some optimizations (Section IV-E ).DnnSAT is extensible to
support user-deﬁned constraints by permitting users to specify
their own constraint speciﬁcations in SMT-LIB with those
deﬁned hyperparameters.
B. The Constraints
DL models are both compute-intensive and memory-
intensive, making them difﬁcult to be trained or deployed
on systems and platforms with limited resources. In this
paper, we consider four representative computational constraints
with respect to the model, namely weight size, number of
ﬂoating-point operations, inference time, and GPU memory
consumption. The meaning of the notations and symbols can
be found in Section III.
Model Weight Size. Weights (including biases) are the
numerical learnable parameters of operators, being saved
in the model ﬁle and taking up most of the space. The
size of weights is important, especially on resource-restricted
devices such as mobile phones. An overlarge DL model
causes inefﬁcient model/application management, expends
unaffordable energy [32], or even cannot ﬁt in the target devices’
main memory. The total model weight size is calculated by
accumulating the weight size of each operator. Assuming that
WT is the restriction function and WTmin,WTmax are the
lower and upper bounds in bytes, the constraint is then deﬁned
as follows:
g(uij) =g(uij 1) +r(uij)
WT(X) = maxfg(uij)gn
j=1=Pn
j=1r(uij)
WTminWT(X)WTmax
Number of Floating-point Operations (FLOPs). FLOPs is
considered as a stronger predictor of energy usage and inference
time [33]. The total FLOPs for inference is calculated by
accumulating the FLOPs of each operator in accordance with
the operator schedule. Assuming that Fis the restriction
function and Fmin,Fmax are the minimal and maximal values
allowed, the constraint is then deﬁned as follows:
g(uij) =g(uij 1) +r(uij); IterCnt = 1
F(X) =IterCntmaxfg(uij)gn
j=1=Pn
j=1r(uij)
FminF(X)FmaxModel Inference Time. This is a critical runtime performance
indicator for DLapplications. The total time is calculated by
accumulating the execution time of each operator in accordance
with the operator schedule. Assuming that Tis the restriction
function and Tmin,Tmax are the lower and upper bounds, the
constraint is then deﬁned as follows:
g(uij) =g(uij 1) +r(uij); IterCnt = 1
T(X) =IterCntmaxfg(uij)gn
j=1=Pn
j=1r(uij)
TminT(X)Tmax
GPU Memory Consumption. As mentioned before, GPU
OOM accounts for the largest DLfailure category [7], therefore
controlling the GPU memory consumption is critical to
reduce OOM exceptions and save shared resources. However,
the calculation is rather complicated since there are many
hidden framework factors observably affecting the ﬁnal GPU
memory consumption [6]. We adopt a simpliﬁed yet common
approach for both inference and data-parallel training, which
accumulates the GPU memory required by each operator under
forward propagation in accordance with the operator schedule.
Assuming that Mis the restriction function and Mmin,Mmax
are the minimal and maximal GPU memory consumption
allowed in bytes ( e.g., taking the memory capacity as the
maximal value), the constraint is then deﬁned as follows:
g(uij) =g(uij 1) +r(uij)
M(X) =Rinit+ maxfg(uij)gn
j=1=Rinit+Pn
j=1r(uij)
MminM(X)Mmax
Ifuis an operator under backward propagation, we let
r(u) = 0 .Rinit represents the GPU memory consumed
by the CUDA context and initial input tensors during the
framework initialization. The CUDA context contains managing
information to control and use GPU devices, which is assumed
to be a constant obtained by proﬁling.
C. Resource Cost Functions of Operators
Operators are mathematical functions on various types of
tensors. DnnSAT deﬁnes analytic and framework-independent
resource cost functions for operators. Such a solution is
technically feasible because: (1) frequently used operators
are well-deﬁned with clear syntax and semantics; (2) DL
frameworks implement them similarly ( e.g., calling NVIDIA
CUDA, cuDNN, or cuBLAS APIs). In this section, we take
theConv2D operator in Fig. 1b as an example to illustrate
the four resource cost functions with respect to the studied
constraints.
The following symbols are used to denote the hyperparam-
eters and tensor shapes. Sfis the size of input data type
(e.g., 4 bytes for FLOAT32 data).Nrepresents batch size. Hk
andWkare kernel (ﬁlter) height and width; they are usually
equal.fltiandsiare ﬁlter size and stride size at index i.flt
represents the number of ﬁlters. Padding padi“controls the
amount of implicit zero-paddings on both sides for padding
number of points for each dimension”, and dilation di“controls
the spacing between the kernel points” [34]. Hin,Win, and
179Cinare input height, width, and channels, respectively. Ho,
Wo, andCoare output height, width, and channels, which
have the following relations with other symbols:
Ho= 1 + (Hin+ 2pad0 d0(Hk 1) 1)=s0
Wo= 1 + (Win+ 2pad1 d1(Wk 1) 1)=s1
Co=flt
Then, the resource cost functions with respect to the four
constraints model weight size (WT),FLOPs (F),model
inference time (T), and GPU memory consumption (M)
are deﬁned as follows:
WT (Conv2D ) =Sf(CinHkWkCo+Co)
F(Conv2D ) = 2Co(HkWkCin+ 1)NHoWo
Mit=NCinHinWin
Mwt=WT (Conv2D )
Mot=SfNCoHoWo
T(Conv2D ) = (Mit+Mwt+Mot)=Bd+F(Conv2D )=Flops
M(Conv2D ) =Mwt+Mot
Mit,Mwt, andMoutdenote the GPU memory occupied by
the input, weight, and output tensors, respectively. Bdand
Flopsare memory bandwidth and ﬂoating-point operations per
second (FLOPS) of the target device, which can be assumed to
be constants. The ﬁrst item of T(Conv2D )represents the data
access time from and to GPU memory [35]. We do not count
Mitin the GPU memory consumption because an operator’s
input tensors reuse the GPU memory of either the initial
inputs or predecessors’ outputs, which have been calculated in
the initialization cost ( Rinit) and predecessor operators’ cost
functions. More details about the estimation of FLOPs and
model inference time can be found at [35] and [36].
Currently, DnnSAT supports 70+ operators. It is also exten-
sible and can support new operators, which are discussed in
Section VI.
D. Constraint Speciﬁcation
In this section, we describe how to automatically generate a
constraint speciﬁcation in SMT-LIB. The DLmodel in Fig. 1
is used as an example, and the enforced constraint is the model
weight size must be less than or equal to 10 MB . A snippet of
the constraint setting is shown as follows:
{"constraint" :"weight_size" ,"max":10485760 ,"min":0}
Fig. 4 lists the illustrated SMT-LIB code. First, DnnSAT
speciﬁes the constraint bounds read from the constraint setting
ﬁle (lines 3-5). It then declares the hyperparameters of the
size of input data type ( Sf), number of input channels ( Cin),
kernel size ( HkandWk), ﬁlter size ( flt), and unit size ( U)
of theDense operator (lines 7-12) and writes their domains
(lines 14-16), according to the contents of the conﬁguration
setting ﬁle. As the batch size ( N) does not contribute to the
model weight size, we simply ignore it.
Next, DnnSAT traverses the computation graph from
Conv2D toDense one after another. For each operator type,
we prepare SMT-LIB templates in Python via Z3 APIs for1(set-logic QF_UFNIA ); Non-linear integer arithmetic logic
2; Constraint bounds
3(declare-const WT_Min Int ); Lower bound: 0 MB
4(declare-const WT_Max Int ); Upper bound: 10 MB
5(assert(and(=WT_Min0) (=WT_Max10485760 )))
6; Hyperparameters
7(declare-const Sf Int ); Size of input data type
8(declare-const Cin Int ); Conv2D input channels
9(declare-const Hk Int ); Conv2D kernel height
10(declare-const Wk Int ); Conv2D kernel weight
11(declare-const flt Int ); Conv2D number of filters
12(declare-const U Int ); Dense unit size
13; Hyperparameter domains
14(assert(and(and(=Sf4) (=Cin3)) (and(=Hk Wk) (=U
64)))) ,!
15(assert(or(or(=Hk3) (=Hk5)) (or(=Hk7) (=Hk11))))
16(assert(or(or(=flt64) (=flt128)) (=flt512)))
17; Compute the weight size of Conv2D
18(declare-const WT_Conv2D Int ); Conv2D weight size
19(declare-const Co Int ); Conv2D output channels
20(assert(=flt Co))
21(assert(=(*Sf(+(*(*(*Hk Wk)Cin)Co)Co))WT_Conv2D ))
22; Compute the weight size of Dense
23(declare-const WT_Dense Int )
24(assert(=(*Sf(+U1))WT_Dense ))
25; Compute the model weight size
26(declare-const WT Int )
27(assert(=(+WT_Conv2D WT_Dense )WT))
28; Specify the constraint
29(assert(and(<=WT_Min WT ) (<=WT WT_Max )))
30(check-sat )
31(exit)
Fig. 4: Illustrated constraint speciﬁcation in SMT-LIB, which enforces that the
weight size of the model in Fig. 1 must be within the interval [0MB ;10MB].
the resource cost functions. When an operator is visited,
DnnSAT locates the matched template and generates the Python
wrapper of SMT-LIB code using those declared hyperparameter
symbols. For example, lines 18-21 correspond to calculating the
weight size of Conv2D . SinceAvgPool2D uses the “same”
padding setting (line 5 in Fig. 1a), its output tensor shape
keeps unchanged with that of Conv2D . BothAvgPool2D
andFlatten do not have weights, so we also ignore them.
Dense has a weight tensor of a 64-element array plus a bias
of 1 data element (lines 23-24). Therefore, the total model
weight size is equal to the sum of the weight sizes of both
Conv2D andDense (lines 26-27). Finally, DnnSAT asserts
that the result must fall between the lower and upper bounds
(line 29). Note that although the example uses only integer
variables, DnnSAT can easily replace the variable statements
to support real-valued hyperparameters because the underlying
SMT solver Z3 supports real values and real functions.
E. Constraint Solving
The resource-guided conﬁguration space reduction is for-
mulated as a constraint satisfaction problem (CSP). DnnSAT
chooses Microsoft Z3 to solve the constraints because the SMT
solver is very efﬁcient to handle higher-order and nonlinear
functions. However, the solving may slow down signiﬁcantly
when dealing with an overlarge conﬁguration space or a very
complicated restriction function. We summarize below our
major optimization techniques for accelerating the solving
speed:
1801)Parallel and distributed solving .DnnSAT partitions the
full conﬁguration space into multiple smaller subspaces
and solves them in parallel. The process is shown
schematically in Fig. 5. For the parallel solving, each
worker thread is assigned a standalone Z3 context. The
distributed solving is built on top of Spark [37], which
handles conﬁguration space partitioning, distributed task
deployment (via the mapPartitions API), schedul-
ing, and fault tolerance.
2)Tiny subspaces . Proper partitioning of the conﬁguration
space is very important for tackling the skew prob-
lem [38], [39] in the parallel and distributed solving.
DnnSAT adopts the idea of tiny tasks [40] and divides the
original space into numerous tiny subspaces ( e.g., each
containing less than 100 conﬁgurations). Our approach
has two advantages: (1) it will not result in observable
computation skew across subspaces; (2) the Z3 solver
cannot return all the satisﬁable model conﬁgurations at
once like what ALLSAT [41] does, hence DnnSAT has
to iteratively call Z3 by providing the conjunction of the
negation of each existing solution to derive the next one.
A tiny subspace does not make the conjunction long and
complicated, thus the solving efﬁciency is signiﬁcantly
improved. DnnSAT currently implements a simple work
queue to manage the tiny subspaces. We will consider
dynamic partitioning and work stealing [42] for better
load balancing in the future.
3)Interval ﬁltering . The restriction function of a con-
straint may be monotonic with regard to (w.r.t. for
short) some hyperparameters. For instance, the model
weight size function is monotonically increasing w.r.t.
kernel size, ﬁlter size, and unit size mentioned in
Fig. 4. Another example is that the FLOPs function
is monotonically increasing w.r.t. batch size but not
to kernel size. This is because the output height and
width of the Conv2D operator decrease with kernel size
increasing, which may reduce the FLOPs of subsequent
operators. With such monotonicity information, we
can apply the interval ﬁltering technique which safely
discards speciﬁc value intervals of the hyperparameters.
Suppose that the restriction function is monotonically
increasing w.r.t. the hyperparameter p1whose domain is
[vmin;vmax]. If the function value of a conﬁguration
hp1=v1;p2=vp2;;pm=vpmiexceeds the
upper bound, any conﬁgurations hp12(v1;vmax];p2=
vp2;;pm=vpmiwill not satisfy the constraint either;
if it is smaller than the lower bound, any conﬁgurations
hp12[vmin;v1);p2=vp2;;pm=vpmiwill also
violate the constraint. Furthermore, if two conﬁgurations
hp12fv1;v2g;p2=vp2;;pm=vpmiare satisﬁed
andv1< v2, any conﬁgurations hp12(v1;v2);p2=
vp2;;pm=vpmiwill satisfy the constraint as well.
V. E VALUATION
To evaluate the proposed DnnSAT , we experiment with
different representative AutoML benchmarks and DLmodels.
Fig. 5: Parallel and distributed solving by partitioning the conﬁguration space.
We aim to answer the following Research Questions (RQs):
RQ1: How effective is DnnSAT in speeding up AutoML
methods?
RQ2: How effective is DnnSAT in reducing the conﬁguration
space?
RQ3: How efﬁcient is DnnSAT in constraint solving?
Our experiments are conducted on an Azure Standard ND12s
virtual machine with 12 Intel Xeon E5-2690 vCPUs, 224 GB
main memory, and 2 NVIDIA Tesla P40 (24 GB GDDR5X
memory) GPUs, running Ubuntu 16.04.
A. RQ1: How effective is DnnSAT in speeding up AutoML
methods?
In this section, we consider the model weight size constraint
and evaluate the effectiveness of DnnSAT on the following
two benchmarks:
1)HPOBench is for HPO methods and consists of “a large
grid of hyperparameter conﬁgurations of feedforward
neural networks for regression” [13]. The model has
two tunable Dense operators followed by a non-tunable
Dense on top. There are nine hyperparameters ( e.g.,
batch size, unit size, and initial learning rate) and 62208
model conﬁgurations in total. We use the HPO-Bench-
Protein dataset. The maximal model weight size is 256
KB.
2)NAS-Bench-101 is for NAS methods and “constructs a
compact, yet expressive, search space, exploiting graph
isomorphisms to identify 423k unique convolutional
architectures” [14]. The dataset is CIFAR10 [43]. The
maximal model weight size is about 47.7 MB.
Both benchmarks generated DLmodels and collected a rich
set of runtime statistics (ﬁnal training/validation/test error and
accuracy, total training time, etc.) from the trained models.
These statistics can be used to simulate the execution of
AutoML trials and evaluate different conﬁguration search
methods. To evaluate the learning performance of a model
conﬁguration resulted from an AutoML trial, following the
existing work [8], [13], [14], [44], we use the mean squared
error (MSE) for HPOBench and regret (i.e., 1 accuracy)
forNAS-Bench-101 as the test measurement. The smaller the
value, the better the model. We choose the model weight
1810:5 11:5 22:5 3
104810 20:10:12
wall-clock time ( 104seconds)testmeasurement (MSE)RS RS+DnnSA T
2.04X speedup
(a)Random Search (RS)00:5 11:5 22:5 3
1040:10:15
810 2
wall-clock time ( 104seconds)testmeasurement (MSE)RE RE+DnnSA T
1.19X speedup
(b)Regularized Evolution (RE)00:20:40:60:8 1
1050:14
610 20:1
wall-clock time ( 105seconds)testmeasurement (MSE)HB HB+DnnSA T
3.95X speedup
(c)Hyperband (HB)
Fig. 6: Test measurement curves and speedups of the HPOBench experiments on 100 trials using different search methods, with an 8 KB upper bound of the
model weight size.
0 1 2 3 4 5
1051:5
1
0:710 2
wall-clock time ( 105seconds)testmeasurement (regret)RS RS+DnnSA T
1.23X speedup
(a)Random Search (RS)0 1 2 3 4 5 6
1051:2
1
0:610 2
wall-clock time ( 105seconds)testmeasurement (regret)RE RE+DnnSA T
1.19X speedup
(b)Regularized Evolution (RE)0 1 2 3 4 5
1051:2
1
0:710 2
wall-clock time ( 105seconds)testmeasurement (regret)RL RL+DnnSA T
1.52X speedup
(c)Reinforcement Learning (RL)
Fig. 7: Test measurement curves and speedups of the NAS-Bench-101 experiments on 500 trials using different search methods, with a 38 MB upper bound of
the model weight size.
00:5 11:5 22:5 3
104910 20:120:15
wall-clock time ( 104seconds)testmeasurement (regret)RS RS+DnnSA T
4.15X speedup
(a)Random Search (RS), 2 KB00:5 11:5 22:5 3
104210 20:10:12
wall-clock time ( 104seconds)testmeasurement (regret)RS RS+DnnSA T
1.21X speedup
(b)Random Search (RS), 16 KB
Fig. 8: Test measurement curves and speedups of the HPOBench experiments
on 100 trials using Random Search, with three upper bounds (2 KB, 8 KB,
and 16 KB) of the model weight size.
size constraint because it is one of the most representative
constraints for DLapplications and for performing model
inference on resource-restricted devices such as mobile phones.
On each benchmark, we perform two sets of experiments:
baseline experiments and DnnSAT -guided experiments. We
choose the following commonly used search methods inAutoML as baselines: Random Search (RS) [15], Regularized
Evolution (RE) [16], Hyperband (HB; HPOBench only) [2],
and Reinforcement Learning (RL; NAS-Bench-101 only) [14].
The two sets of experiments perform the same number of trials
(100 for HPOBench and 500 for NAS-Bench-101 ). We then
measure the speedup achieved by DnnSAT over each baseline
method on each benchmark. The speedup is calculated asTbase
TDnnSAT,
whereTbase is the estimated time of a baseline ( i.e., RS, RE,
and HB) reaching the lowest test measurement ( i.e., the best
learning performance) and TDnnSAT is the time of a DnnSAT -
guided method reaching the same or lower test measurement.
We repeat each experiment 10 times and compute the average
speedup value. DnnSAT runs in the interactive mode because
of the simplicity of the integration effort. To solve the ﬁrst
model conﬁguration, DnnSAT needs to build the constraint
from scratch ( i.e., warm-up), which spends more time than
solving later individual conﬁgurations of the same model. For
HPOBench , the solving time is 0.1s (warm-up: 2.5s) on average.
For NAS-Bench-101, it is 0.6s (warm-up: 14.0s) on average.
Fig. 6 demonstrates the test measurement (MSE) curves of
HPOBench on 100 evaluated trials. The upper bound of the
182model weight size is set to a small value of 8 KB (which is
set for deploying KB-sized DLmodels to resource-restricted
IoT devices [45]). The x-axis is the wall-clock time, and the
y-axis denotes the test measurement value. Overall, DnnSAT
achieves a speedup of 2.04X (RS), 1.19X (RE), and 3.95X
(HB) on HPOBench , for obtaining the same optimal model
learning performance that can be found by the baseline. From
the experiment results, we also ﬁnd that DnnSAT helps the
curves go down faster and reach smaller test measurement
values, which means that better model conﬁgurations are found.
The reason is that DnnSAT reduces the conﬁguration space
so that HPOBench can perform a much more efﬁcient search
than before. We also notice that the experiment time has been
observably shortened when DnnSAT is enabled since a smaller
model size implies fewer FLOPs and thus less training time.
In Fig. 6c, time reduction is particularly signiﬁcant because
of the mechanism of Hyperband: the more resources saved by
DnnSAT , the more training budget allocated to Hyperband for
high-efﬁciency search.
Fig. 7 demonstrates the test measurement (regret) curves of
NAS-Bench-101 on 500 evaluated trials, with the upper bound
of the model weight size set to 38 MB. DnnSAT achieves a
speedup of 1.23X (RS), 1.19X (RE), and 1.52X (RL).
To understand the generality of our approach under different
constraint bounds, we additionally choose two upper bounds
(2 KB and 16 KB) and conduct HPOBench experiments using
Random Search. The results of Fig. 8 and Fig. 6a indicate
that DnnSAT is generally effective and achieves a speedup
of 4.15X (2 KB), 2.04X (8 KB), and 1.21X (16 KB). The
results also show that the stricter the constraint, the greater the
improvement achieved by DnnSAT.
B. RQ2: How effective is DnnSAT in reducing the conﬁguration
space?
In this section, we evaluate the reduction effectiveness of
DnnSAT on real-world DLmodels. We choose two representa-
tive models as our experimental subjects, namely VGG-16
(VGG model with 16 layers) [17] and LSTM [18]-based
Seq2Seq [19]. For VGG-16 , we select batch size (interval
[1;256]), kernel size (1, 3, and 5), and unit size (128, 512, 1024,
4096, and 10240) as the hyperparameters; hence there are 3840
model conﬁgurations in total. For Seq2Seq , we consider batch
size (interval [128;512]) and hidden size (interval [16;128]),
in which the conﬁguration space consists of 43505 model
conﬁgurations.
We apply all the four discussed constraints separately and
collectively to both DLmodels. Each constraint is set with
several upper bounds. The bound choices are based on actual
conditions such as the model scale, device capability, and
application SLA. For example, we choose 6, 8, and 12 GB
forVGG-16 under the GPU memory consumption constraint
because they correspond to three typical memory capacities of
NVIDIA GPUs. Since the two models differ a lot, we cannot
use the same bound value for both. Note that we use batched
inference time , that is, the total inference time of a batch of data
items. Speciﬁc upper bound values can be found in Table III.TABLE III
CONFIGURATION SPACE REDUCTION ON REAL-WORLD DL M ODELS .
Model Max Weight Max GPU Max BI Max FLOPs All
Name Size (MB) Memory (GB) Time (S) (GFLOPs) Constraints
VGG-16 1024 12 1000 4096
(80.0%) (84.3%) (80.3%) (58.4%) (53.4%)
512 8 800 3584
(60.0%) (56.2%) (64.0%) (51.1%) (31.8%)
128 6 10 3072
(0.0%) (42.1%) (0.7%) (43.7%) (0.0%)
Seq2Seq 128 0.6 50 64
(LSTM) (32.1%) (34.7%) (74.1%) (35.2%) (24.1%)
64 0.4 30 4
(18.0%) (15.3%) (67.2%) (26.0%) (1.6%)
32 0.2 1 1
(5.0%) (0.0%) (33.2%) (4.2%) (0.0%)
Note: “BITime” stands for batched inference time, which calculates the total
inference time of a batch of data items. The two values in a cell represent the
upper bound and space ratio, respectively.
The space ratio (SR) is used to assess the reduction
effectiveness of DnnSAT . Suppose that andDnnSAT are
the original and reduced conﬁguration spaces, respectively.
Then, SR=jDnnSATj
jj100%. A smaller SR means a stronger
reduction effect. The promising experimental results in Table III
demonstrate that DnnSAT is effective in conﬁguration space
reduction. For example, the SR of VGG-16 under the GPU
memory consumption constraint ranges from 42.1% to 84.3%.
Meanwhile, the SR of Seq2Seq under the batched inference
(BI) time constraint ranges from 33.2% to 74.1%. Tighter
bounds or a combination of multiple constraints will lead
to a more signiﬁcant reduction. The results can also give
hints to developers and help them choose optimal settings
of neural architectures, hyperparameters, and computational
resources. For instance, the SR of VGG-16 equals 0% when
the upper bound of the model weight size is set to 128
MB, which indicates that such a model may not be further
dwindled by simply adjusting the hyperparameters. Therefore,
developers need to look for advanced DLtechniques ( e.g.,
model compression [32]) to embed it into a resource-restricted
application. After applying four constraints collectively, the
SR value further decreases and is below the minimum of the
one-constraint SR values. The results demonstrate the stronger
reduction effect when adopting multiple constraints collectively.
DnnSAT runs in both interactive and pruning modes with 12
threads, tiny subspaces containing 10 model conﬁgurations, and
interval ﬁltering being off. We show the runtime performance of
DnnSAT under one constraint and four constraints as follows:
1)Interactive mode. For VGG-16 , the solving time per
conﬁguration is 0.10s (warm-up: 6.13s) on average under
one constraint, and 0.13s (warm-up: 6.18s) under four
constraints. For Seq2Seq , the corresponding time is 0.05s
(warm-up: 1.40s) on average and 0.12s (warm-up: 1.44s),
respectively.
2)Pruning mode. For VGG-16 ,DnnSAT spends 226s
on average under one constraint and 283s under four
constraints. For Seq2Seq , the corresponding time is 806s
on average and 1301s, respectively.
183C. RQ3: How efﬁcient is DnnSAT in constraint solving?
In this section, we evaluate the solving efﬁciency of the
optimization techniques proposed in Section IV-E . We choose
theLSTM -based Seq2Seq model with batch size (interval
[1;4800] ) and hidden size (interval [16;20]) as the hyperparam-
eters. The conﬁguration space then consists of 24000 model
conﬁgurations in total. To increase the solving difﬁculty, we
use a loose FLOPs constraint under which every conﬁguration
satisﬁes.
A series of experiments are conducted with different thread
numbers (1, 4, 8, and 12), subspace sizes (10, 50, 100, 500,
1000, and equipartition), and interval ﬁltering settings (ON and
OFF). We do not create more threads since the experimental
machine has only 12 vCPUs. “Equipartition” means that we
divide the original conﬁguration space equally by the number
of threads and do not further split it into tiny subspaces. For
example, in the case of 12 threads, each thread independently
solves a subspace of 2000 conﬁgurations.
Table IV shows the end-to-end execution time (in seconds)
and speedup relative to the baseline experiment (1 thread,
equipartition, and interval ﬁltering being off). The speedup
ranges from 7.6X to 17892.1X, conﬁrming the strong effec-
tiveness of our optimizations. Simply increasing the number of
threads achieves an ultra-linear speedup from 9.3X to 51.9X
because a smaller conﬁguration space notably reduces the
overhead of ALLSAT solving (Section IV-E ). Tiny subspaces
achieve a speedup from 7.6X to 83.0X in the experiments
that turn off interval ﬁltering, with the same reason as the
parallel solving. Nevertheless, as the subspace size getting
smaller, the speedup grows slowly and then drops (from
the size of 50) because the overhead of ALLSAT solving
is no longer noticeable while the management cost of tiny
subspaces increases. Interval ﬁltering demonstrates dramatic
improvements in the equipartition experiments and achieves
a maximal speedup of 17892.1X. The reason is that DnnSAT
divides the original conﬁguration space on hidden size to
keep as long a continuous interval of batch size as possible
since FLOPs is monotonically increasing with regard to batch
size. Therefore, DnnSAT solves only a small number of
conﬁgurations to reach the conclusion that the entire subspace
satisﬁes the constraint. However, if there exist many short
intervals of batch size ( e.g., the domain of batch size is small
or tiny subspaces are used), the effect of interval ﬁltering will
not be so signiﬁcant.
VI. D ISCUSSION
A. Extensibility of DnnSAT
Currently, DnnSAT supports 70+ commonly used operators.
DnnSAT is extensible, and users can incorporate new operators
and constraints. To add a new operator, users formulate the
analytic resource cost functions based on its semantics and then
implement the SMT-LIB templates. To support a new constraint,
users formulate the analytic restriction function using deﬁned
hyperparameters and carry out the above operator-adding steps
for each of the operators under consideration. Besides, usersTABLE IV
RUNTIME PERFORMANCE OF DNNSAT UNDER THE FLOP SCONSTRAINT
WITHOPTIMIZATION TECHNIQUES OF PARALLEL SOLVING , TINY
SUBSPACES ,AND INTERVAL FILTERING .
Subspace Number of Threads
Size Inter val Filtering OFF Inter val Filtering ON
1 4 8 12 1 4 8 12
Equipartition 1.0 9.3 26.7 51.9 17892.1 17,730.6 17,572.3 15,496.8
(19681.0s) (2116.2s) (737.1s) (379.2s) (1.1s) (1.11s) (1.12s) (1.27s)
1000 8.8 28.9 50.3 68.5 1640.1 5046.4 7872.5 9840.6
(2236.4s) (681.0s) (391.2s) (287.3s) (12.0s) (3.9s) (2.5s) (2.0s)
500 10.5 33.7 57.0 77.0 841.0 2659.6 4100.2 5319.2
(1874.3s) (584.0s) (345.2s) (255.6s) (23.4s) (7.4s) (4.8s) (3.7s)
100 12.1 38.9 63.2 83.0 169.8 560.7 882.5 1063.8
(1626.5s) (505.9s) (311.4s) (237.1s) (115.9s) (35.1s) (22.3s) (18.5s)
50 11.6 37.6 60.0 76.0 85.2 258.2 441.2 533.3
(1696.6s) (523.4s) (328.0s) (258.9s) (231.0s) (76.2s) (44.6s) (36.9s)
10 7.6 25.7 40.0 48.9 17.1 57.3 89.5 107.1
(2589.6s) (765.8s) (492.0s) (402.4s) (1150.9s) (343.4s) (219.9s) (183.7s)
Note: The two values in a cell represent the speedup and execution time (in seconds).
may need to reimplement the graph traversal to compute more
accurate current resource consumption by employing additional
information, including visited operators, edges, and previously
calculated resource consumption.
B. Threats to Validity
We discuss the following threats to the validity of our work:
1)Resource cost functions . We examine the source code
of frameworks to extract the resource cost functions
ofDLoperators for inferring resource usage. However,
the implementation of operators can call proprietary
NVIDIA CUDA, cuDNN, or cuBLAS APIs, which may
introduce some ﬂuctuations in the cost functions. For
example,cudnnConvolutionForward() could use
temporary GPU memory called workspace to improve
the runtime performance. Nevertheless, the workspace
size is convolution algorithm-dependent and thus unpre-
dictable. We mitigated this threat by reﬁning the resource
cost functions after carefully referring to the NVIDIA
development documentation, dynamically proﬁling the
APIs using NVIDIA nvprof , and analyzing the framework
runtime logs.
2)Hidden factors . There are a number of hidden frame-
work factors that can observably affect the GPU memory
consumption and inference time of a DLmodel. For
example, the GPU memory consumption has compli-
cated dependencies on the allocation policy ( e.g., tensor
fragmentation, alignment, garbage collection, and reser-
vation), internal usage ( e.g., CUDA context), operator
scheduling, etc.To mitigate this threat, we referred to
the framework source code carefully to identify hidden
factors. However, it is very challenging to directly
formulate all such factors ( e.g., garbage collection and
reservation) analytically. Hence, DnnSAT conservatively
calculates the resource usage to reduce the inﬂuence
of hidden factors. For instance, DnnSAT adopts a sim-
pliﬁed yet common approach to accumulate the GPU
memory required by each operator under only forward
propagation (Section IV-B ), which computes a smaller
value than the actual GPU memory consumption. If the
computed value (a conservative value) already exceeds
184the GPU memory upper bound, the corresponding model
conﬁguration indeed does not satisfy the constraint.
Therefore, valid (satisﬁable) model conﬁgurations will
not be discarded. However, some invalid (unsatisﬁable)
model conﬁgurations may not be eliminated correctly due
to the inaccuracy in the calculation of hidden factors. In
the experiment on VGG-16 (Section V-B), we notice that
on average 9.53% of model conﬁgurations are actually
invalid under four constraints yet passed DnnSAT , but no
valid model conﬁgurations are discarded. In the future,
we will identify more hidden factors and design more
accurate restriction functions to obtain more precise
results.
3)SMT solving . The simple and non-intrusive integration
with existing AutoML tools is to run DnnSAT in the
interactive mode. According to the experimental results
in Section V, the cost of solving one model conﬁguration
is very low. In the pruning mode, it takes a longer time
for constraint solving because of the large conﬁguration
space. For example, Table IV in Section V-C shows that
DnnSAT spends 19681 seconds (about 5.5 hours) to
solve the 24000-conﬁguration space of Seq2Seq . We
currently propose some effective optimization techniques
in Section IV-E to increase the scalability of constraint
solving: (1) DnnSAT supports parallel and distributed
solving (via Spark), in which the full conﬁguration
space can be partitioned into any number of independent
subspaces and solved concurrently by different threads
and machines; (2) the use of tiny subspaces reduces
the complexity of solving individual subspaces, removes
the computation skew across subspaces, balances the
workload among threads, and thus avoids stragglers ( i.e.,
threads that take an unusually long time to ﬁnish) [40]; (3)
interval ﬁltering can further reduce the solving complexity
signiﬁcantly if the restriction function of a constraint is
monotonic with regard to some hyperparameters. The
experimental results in Table IV conﬁrm the strong
effectiveness of our optimizations. However, as the
number of hyperparameters and their domains increase,
the conﬁguration space can enlarge exponentially, thus
the computational complexity may still exceed the
capabilities of an SMT solver. In addition to advancing
the solvers, it is possible to tackle this problem by trying
larger-scale distributed solving with better load balancing.
VII. R ELATED WORK
Many software systems are highly conﬁgurable by providing
a rich set of conﬁguration options. Conﬁguration options
are also considered as features in the software product line
context. However, it is very time-consuming and error-prone
for manual conﬁguration tuning due to a large number of
option combinations. Software engineering researchers have
proposed various approaches to predicting the performance
of conﬁgurable systems [44], [46], [47], checking the con-
sistency of conﬁgurations [48]–[51], and understanding how
conﬁguration options and their interactions inﬂuence systemperformance [52], [53]. Like traditional software systems,
DLmodels are also highly conﬁgurable. In this work, we
analyze DLmodels and propose to optimize the conﬁguration
exploration (AutoML) through resource-guided space reduction.
Google Vizier [54] and Microsoft HyperDrive [55] are
representative AutoML systems, which concentrate more on the
system design and operation. Hyperband [2] is an HPO search
method and focuses on speeding up random search through
adaptive resource allocation and early stopping. ENAS [3] uses
a controller to discover various neural architectures and search
for the optimal one. These methods and systems are not aware
of the constraints imposed by computational resources, which
can cause an expensive waste of shared resources. Our work
can help them efﬁciently reduce the conﬁguration space ahead
of time and accelerate training.
The authors of [56], [57] analyzed the resource budget
constraint for HPO. However, they encoded the budget into the
algorithm instead of formulating explicit resource constraints,
and thus their method cannot be applied to other AutoML
methods directly. Hern ´andez-Lobato et al. [58] considered
constraints for Bayesian Optimization, but the work is for a
speciﬁc algorithm. Gordon et al. [59] proposed an approach to
automate the design of neural structures via a resource-weighted
sparsifying regularizer. AMS [60] generated the AutoML search
space from an unordered set of API components. Our work
formulates common constraints imposed by resources and uses
a uniﬁed analytic approach to eliminate the unsatisﬁable model
conﬁgurations in advance.
There have been many program analysis methods [61]–[64]
to determine the quantitative resource usage ( e.g., memory and
heap space) of computer programs. For example, Hoffmann et
al.[61] used the automatic amortized resource analysis (AARA)
technique in analyzing the worst-case resource consumption
of arbitrary multivariate polynomial functions. Jost et al. [64]
employed a type-based approach by exploiting linearity and
focusing on the reference number to an object. However, such
work usually targets higher-order functional programs and
cannot be directly applied to DLmodels because of the wide
differences in the representation structures. Our work proposes
an analytic and framework-independent cost model to infer
the resource consumption of a DLmodel and utilizes an SMT
solver to obtain all the satisﬁable model conﬁgurations.
VIII. C ONCLUSION
In this paper, we have presented DnnSAT , a resource-guided
AutoML approach for deep learning models to efﬁciently
reduce the conﬁguration space under computational constraints.
Powered by DnnSAT , we demonstrate that commonly used
AutoML methods can efﬁciently prune unsatisﬁable model
conﬁgurations ahead of time to avoid unnecessary training costs
and achieve signiﬁcant speedups. We believe that DnnSAT can
make AutoML more practical in a real-world environment with
constrained resources.
REFERENCES
[1]NNI, “Nni (neural network intelligence): a lightweight but powerful
toolkit to help users automate feature engineering, neural architecture
185search, hyperparameter tuning and model compression.” https://github.
com/microsoft/nni, 2019.
[2]L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar,
“Hyperband: A novel bandit-based approach to hyperparameter
optimization,” J. Mach. Learn. Res. , vol. 18, no. 1, pp. 6765–6816, Jan.
2017. [Online]. Available: http://dl.acm.org/citation.cfm?id=3122009.
3242042
[3]H. Pham, M. Y . Guan, B. Zoph, Q. V . Le, and J. Dean, “Efﬁcient
neural architecture search via parameter sharing,” CoRR , 2018. [Online].
Available: http://arxiv.org/abs/1802.03268
[4]H. Jin, Q. Song, and X. Hu, “Efﬁcient neural architecture search
with network morphism,” CoRR , vol. abs/1806.10282, 2018. [Online].
Available: http://arxiv.org/abs/1806.10282
[5]K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2016, pp. 770–778.
[6]Y . Gao, Y . Liu, H. Zhang, Z. Li, Y . Zhu, H. Lin, and M. Yang,
Estimating GPU Memory Consumption of Deep Learning Models ,
ser. ESEC/FSE 2020. New York, NY , USA: Association for
Computing Machinery, 2020, pp. 1342–1352. [Online]. Available:
https://doi.org/10.1145/3368089.3417050
[7]R. Zhang, W. Xiao, H. Zhang, Y . Liu, H. Lin, and M. Yang,
“An empirical study on program failures of deep learning jobs,” in
Proceedings of the ACM/IEEE 42nd International Conference on
Software Engineering , ser. ICSE ’20. New York, NY , USA: Association
for Computing Machinery, 2020, pp. 1159–1170. [Online]. Available:
https://doi.org/10.1145/3377811.3380362
[8]H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture
search on target task and hardware,” CoRR , vol. abs/1812.00332, 2018.
[Online]. Available: http://arxiv.org/abs/1812.00332
[9]C. Hsu, S. Chang, D. Juan, J. Pan, Y . Chen, W. Wei, and S. Chang,
“MONAS: multi-objective neural architecture search using reinforcement
learning,” CoRR , vol. abs/1806.10332, 2018. [Online]. Available:
http://arxiv.org/abs/1806.10332
[10] M. Tan and Q. Le, “EfﬁcientNet: Rethinking model scaling for
convolutional neural networks,” in Proceedings of the 36th International
Conference on Machine Learning , ser. Proceedings of Machine Learning
Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. Long
Beach, California, USA: PMLR, 09–15 Jun 2019, pp. 6105–6114.
[Online]. Available: http://proceedings.mlr.press/v97/tan19a.html
[11] S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach ,
3rd ed. USA: Prentice Hall Press, 2009.
[12] L. De Moura and N. Bjørner, “Z3: An efﬁcient smt solver,” in Proceedings
of the Theory and Practice of Software, 14th International Conference
on Tools and Algorithms for the Construction and Analysis of Systems ,
ser. TACAS ’08/ETAPS ’08. Berlin, Heidelberg: Springer-Verlag, 2008,
pp. 337–340.
[13] A. Klein and F. Hutter, “Tabular benchmarks for joint architecture
and hyperparameter optimization,” CoRR , 2019. [Online]. Available:
http://arxiv.org/abs/1905.04970
[14] C. Ying, A. Klein, E. Real, E. Christiansen, K. Murphy, and F. Hutter,
“Nas-bench-101: Towards reproducible neural architecture search,” CoRR ,
2019. [Online]. Available: http://arxiv.org/abs/1902.09635
[15] J. Bergstra and Y . Bengio, “Random search for hyper-parameter
optimization,” Journal of Machine Learning Research , vol. 13, no. 10,
pp. 281–305, 2012. [Online]. Available: http://jmlr.org/papers/v13/
bergstra12a.html
[16] E. Real, A. Aggarwal, Y . Huang, and Q. V . Le, “Regularized evolution
for image classiﬁer architecture search,” CoRR , 2018. [Online]. Available:
http://arxiv.org/abs/1802.01548
[17] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” in 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings , Y . Bengio and Y . LeCun, Eds.,
2015. [Online]. Available: http://arxiv.org/abs/1409.1556
[18] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput. , vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [Online]. Available:
https://doi.org/10.1162/neco.1997.9.8.1735
[19] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning
with neural networks,” in Proceedings of the 27th International Confer-
ence on Neural Information Processing Systems - Volume 2 , ser. NIPS’
14. Cambridge, MA, USA: MIT Press, 2014, pp. 3104–3112.
[20] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning . MIT Press,
2016, http://www.deeplearningbook.org.[21] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur,
J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,
P. Tucker, V . Vasudevan, P. Warden, M. Wicke, Y . Yu, and
X. Zheng, “Tensorﬂow: A system for large-scale machine learning,”
in12th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 16) , 2016, pp. 265–283. [Online]. Available:
https://www.usenix.org/system/ﬁles/conference/osdi16/osdi16-abadi.pdf
[22] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An
imperative style, high-performance deep learning library,” in
Advances in Neural Information Processing Systems , H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alch ´e-Buc, E. Fox, and
R. Garnett, Eds., vol. 32. Curran Associates, Inc., 2019, pp.
8026–8037. [Online]. Available: https://proceedings.neurips.cc/paper/
2019/ﬁle/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
[23] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao,
B. Xu, C. Zhang, and Z. Zhang, “MXNet: A ﬂexible and
efﬁcient machine learning library for heterogeneous distributed
systems,” CoRR , vol. abs/1512.01274, 2015. [Online]. Available:
http://arxiv.org/abs/1512.01274
[24] F. Chollet et al. , “Keras,” https://keras.io, 2015.
[25] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg,
M. Blum, and F. Hutter, “Efﬁcient and robust automated
machine learning,” in Advances in Neural Information Processing
Systems , C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett, Eds., vol. 28. Curran Associates, Inc., 2015, pp.
2962–2970. [Online]. Available: https://proceedings.neurips.cc/paper/
2015/ﬁle/11d0e6287202fced83f79975ec59a3a6-Paper.pdf
[26] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement
learning,” CoRR , vol. abs/1611.01578, 2016. [Online]. Available:
http://arxiv.org/abs/1611.01578
[27] R. M. Larsen and T. Shpeisman, “Tensorﬂow graph optimizations,” 2019.
[28] PyTorch, “The topological sorting algorithm for computation graphs
in pytorch,” https://github.com/pytorch/pytorch/blob/v1.2.0/caffe2/core/
nomnigraph/include/nomnigraph/Graph/TopoSort.h#L26, 2019.
[29] A. MXNet, “The topological sorting algorithm for computation graphs
in apache mxnet,” https://github.com/apache/incubator-mxnet/blob/
4149f8b8752989fce5d80cc13f92d99774988b4f/src/executor/simple
partition pass.h#L67, 2019.
[30] H. B. Enderton, Elements of Set Theory , H. B. Enderton, Ed. San
Diego: Academic Press, 1977.
[31] C. Barrett, P. Fontaine, and C. Tinelli, “The SMT-LIB Standard: Version
2.6,” Department of Computer Science, The University of Iowa, Tech.
Rep., 2017, available at www.SMT-LIB.org .
[32] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural network with pruning, trained quantization and huffman
coding,” in 4th International Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings , Y . Bengio and Y . LeCun, Eds., 2016. [Online]. Available:
http://arxiv.org/abs/1510.00149
[33] R. Tang, W. Wang, Z. Tu, and J. Lin, “An experimental analysis of
the power consumption of convolutional neural networks for keyword
spotting,” in 2018 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , 2018, pp. 5479–5483.
[34] PyTorch, “Pytorch conv2d api,” https://pytorch.org/docs/stable/generated/
torch.nn.Conv2d.html.
[35] H. Qi, E. R. Sparks, and A. Talwalkar, “Paleo: A performance model for
deep neural networks,” in Proceedings of the International Conference
on Learning Representations , 2017.
[36] L. Wang, J. Ye, Y . Zhao, W. Wu, A. Li, S. L. Song, Z. Xu, and T. Kraska,
“Superneurons: Dynamic gpu memory management for training deep
neural networks,” in Proceedings of the 23rd ACM SIGPLAN Symposium
on Principles and Practice of Parallel Programming , ser. PPoPP ’18.
New York, NY , USA: Association for Computing Machinery, 2018, pp.
41–53. [Online]. Available: https://doi.org/10.1145/3178487.3178491
[37] M. Zaharia, R. S. Xin, P. Wendell, T. Das, M. Armbrust, A. Dave,
X. Meng, J. Rosen, S. Venkataraman, M. J. Franklin, A. Ghodsi,
J. Gonzalez, S. Shenker, and I. Stoica, “Apache spark: A uniﬁed engine
for big data processing,” Commun. ACM , vol. 59, no. 11, pp. 56–65,
Oct. 2016. [Online]. Available: https://doi.org/10.1145/2934664
186[38] G. Ananthanarayanan, S. Agarwal, S. Kandula, A. Greenberg, I. Stoica,
D. Harlan, and E. Harris, “Scarlett: Coping with skewed content
popularity in mapreduce clusters,” in Proceedings of the Sixth Conference
on Computer Systems , ser. EuroSys ’11. New York, NY , USA:
Association for Computing Machinery, 2011, pp. 287–300. [Online].
Available: https://doi.org/10.1145/1966445.1966472
[39] Y . Kwon, M. Balazinska, B. Howe, and J. Rolia, “Skewtune:
Mitigating skew in mapreduce applications,” in Proceedings of
the 2012 ACM SIGMOD International Conference on Management
of Data , ser. SIGMOD ’12. New York, NY , USA: Association
for Computing Machinery, 2012, pp. 25–36. [Online]. Available:
https://doi.org/10.1145/2213836.2213840
[40] K. Ousterhout, A. Panda, J. Rosen, S. Venkataraman, R. Xin,
S. Ratnasamy, S. Shenker, and I. Stoica, “The case for tiny tasks in
compute clusters,” in 14th Workshop on Hot Topics in Operating Systems
(HotOS XIV) . Santa Ana Pueblo, NM: USENIX Association, May
2013. [Online]. Available: https://www.usenix.org/conference/hotos13/
session/ousterhout
[41] T. Toda and T. Soh, “Implementing efﬁcient all solutions SAT
solvers,” CoRR , vol. abs/1510.00523, 2015. [Online]. Available:
http://arxiv.org/abs/1510.00523
[42] R. D. Blumofe and C. E. Leiserson, “Scheduling multithreaded
computations by work stealing,” Journal of the ACM , vol. 46, no. 5, pp.
720–748, Sep. 1999. [Online]. Available: https://doi.org/10.1145/324133.
324234
[43] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
University of Toronto, Tech. Rep., 2009.
[44] H. Ha and H. Zhang, “Deepperf: Performance prediction for conﬁgurable
software with deep sparse neural network,” in Proceedings of
the 41st International Conference on Software Engineering , ser.
ICSE ’19. IEEE Press, 2019, pp. 1095–1106. [Online]. Available:
https://doi.org/10.1109/ICSE.2019.00113
[45] S. Gopinath, N. Ghanathe, V . Seshadri, and R. Sharma, “Compiling
kb-sized machine learning models to tiny iot devices,” in Proceedings
of the 40th ACM SIGPLAN Conference on Programming Language
Design and Implementation , ser. PLDI 2019. New York, NY , USA:
Association for Computing Machinery, 2019, pp. 79–95. [Online].
Available: https://doi.org/10.1145/3314221.3314597
[46] N. Siegmund, S. S. Kolesnikov, C. K ¨astner, S. Apel, D. Batory,
M. Rosenm ¨uller, and G. Saake, “Predicting performance via automated
feature-interaction detection,” in Proceedings of the 34th International
Conference on Software Engineering , ser. ICSE ’12. IEEE Press, 2012,
pp. 167–177.
[47] J. Guo, D. Yang, N. Siegmund, S. Apel, A. Sarkar, P. Valov,
K. Czarnecki, A. Wasowski, and H. Yu, “Data-efﬁcient performance
learning for conﬁgurable systems,” Empirical Softw. Engg. , vol. 23,
no. 3, pp. 1826–1867, Jun. 2018. [Online]. Available: https:
//doi.org/10.1007/s10664-017-9573-6
[48] D. Batory, D. Benavides, and A. Ruiz-Cortes, “Automated analysis of
feature models: Challenges ahead,” Commun. ACM , vol. 49, no. 12, pp.
45–47, Dec. 2006. [Online]. Available: https://doi.org/10.1145/1183236.
1183264
[49] J. Sun, H. Zhang, Y . Fang, and L. Wang, “Formal semantics and
veriﬁcation for feature modeling,” in IEEE International Conference
on Engineering of Complex Computer Systems (ICECCS’05) , June 2005,
pp. 303–312.
[50] K. Czarnecki and C. Kim, “Cardinality-based feature modeling and
constraints : A progress report,” 2005.
[51] D. Benavides, S. Segura, and A. Ruiz-Cort ´es, “Automated analysis of
feature models 20 years later: A literature review,” Information Systems ,
vol. 35, pp. 615–636, 09 2010.
[52] N. Siegmund, A. Grebhahn, S. Apel, and C. K ¨astner, “Performance-
inﬂuence models for highly conﬁgurable systems,” in Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineering , ser.
ESEC/FSE 2015, 2015, pp. 284–294.
[53] H. Ha and H. Zhang, “Performance-inﬂuence model for highly
conﬁgurable software with fourier learning and lasso regression,”
in2019 IEEE International Conference on Software Maintenance
and Evolution, ICSME 2019, Cleveland, OH, USA, September 29
- October 4, 2019 . IEEE, 2019, pp. 470–480. [Online]. Available:
https://doi.org/10.1109/ICSME.2019.00080
[54] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley,
“Google vizier: A service for black-box optimization,” in Proceedings
of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining , ser. KDD ’17. New York, NY , USA:
Association for Computing Machinery, 2017, pp. 1487–1495. [Online].
Available: https://doi.org/10.1145/3097983.3098043
[55] J. Rasley, Y . He, F. Yan, O. Ruwase, and R. Fonseca, “Hyperdrive:
Exploring hyperparameters with pop scheduling,” in Proceedings of the
18th ACM/IFIP/USENIX Middleware Conference , ser. Middleware ’17.
New York, NY , USA: Association for Computing Machinery, 2017, pp.
1–13. [Online]. Available: https://doi.org/10.1145/3135974.3135994
[56] M. Li, E. Yumer, and D. Ramanan, “Budgeted training: Rethinking deep
neural network training under resource constraints,” in International
Conference on Learning Representations , 2020. [Online]. Available:
https://openreview.net/forum?id=HyxLRTVKPH
[57] Z. Lu, L. Chen, C.-K. Chiang, and F. Sha, “Hyper-parameter tuning under
a budget constraint,” in Proceedings of the Twenty-Eighth International
Joint Conference on Artiﬁcial Intelligence, IJCAI-19 . International
Joint Conferences on Artiﬁcial Intelligence Organization, 7 2019, pp.
5744–5750. [Online]. Available: https://doi.org/10.24963/ijcai.2019/796
[58] J. M. Hern ´andez-Lobato, M. A. Gelbart, R. P. Adams, M. W. Hoffman,
and Z. Ghahramani, “A general framework for constrained bayesian
optimization using information-based search,” J. Mach. Learn. Res. ,
vol. 17, pp. 160:1–160:53, 2016.
[59] A. Gordon, E. Eban, O. Nachum, B. Chen, T. Yang, and
E. Choi, “Morphnet: Fast & simple resource-constrained structure
learning of deep networks,” CoRR , 2017. [Online]. Available:
http://arxiv.org/abs/1711.06798
[60] J. P. Cambronero, J. Cito, and M. C. Rinard, “Ams: Generating
automl search spaces from weak speciﬁcations,” in Proceedings of
the 28th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
ser. ESEC/FSE 2020. New York, NY , USA: Association for
Computing Machinery, 2020, pp. 763–774. [Online]. Available:
https://doi.org/10.1145/3368089.3409700
[61] J. Hoffmann, K. Aehlig, and M. Hofmann, “Multivariate amortized
resource analysis,” ACM Trans. Program. Lang. Syst. , vol. 34, no. 3,
Nov. 2012. [Online]. Available: https://doi.org/10.1145/2362389.2362393
[62] J. Hoffmann, A. Das, and S.-C. Weng, “Towards automatic resource
bound analysis for ocaml,” in Proceedings of the 44th ACM SIGPLAN
Symposium on Principles of Programming Languages , ser. POPL 2017.
New York, NY , USA: Association for Computing Machinery, 2017, pp.
359–373. [Online]. Available: https://doi.org/10.1145/3009837.3009842
[63] M. Hofmann and S. Jost, “Static prediction of heap space usage
for ﬁrst-order functional programs,” in Proceedings of the 30th
ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages , ser. POPL ’03. New York, NY , USA: Association
for Computing Machinery, 2003, pp. 185–197. [Online]. Available:
https://doi.org/10.1145/604131.604148
[64] S. Jost, K. Hammond, H.-W. Loidl, and M. Hofmann, “Static
determination of quantitative resource usage for higher-order programs,”
inProceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages , ser. POPL ’10. New York,
NY , USA: Association for Computing Machinery, 2010, pp. 223–236.
[Online]. Available: https://doi.org/10.1145/1706299.1706327
187