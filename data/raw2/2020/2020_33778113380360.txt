Understanding the Automated Parameter Optimization on
Transfer Learning for Cross-Project Defect Prediction: An
Empirical Study
Ke Li♮, Zilin Xiang♯, Tao Chen§, Shuo Wang¶, Kay Chen Tan♣∗
♯College of Computer Science and Engineering, UESTC, Chengdu, 611731, China
♮Department of Computer Science, University of Exeter, Exeter, EX4 4QF, UK
§Department of Computer Science, Loughborough University, Loughborough, LE11 3TU, UK
¶School of Computer Science, University of Birmingham, Birmingham, B15 2TT, UK
♣Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong SAR
k.li@exeter.ac.uk,zilin.xiang@gmail.com,t.t.chen@lboro.ac.uk
ABSTRACT
Data-driven defect prediction has become increasingly important
in software engineering process. Since it is not uncommon that data
from a software project is insufficient for training a reliable defect
prediction model, transfer learning that borrows data/konwledge
from other projects to facilitate the model building at the current
project, namely cross-project defect prediction (CPDP), is natu-
rally plausible. Most CPDP techniques involve two major steps,
i.e., transfer learning and classification, each of which has at least
one parameter to be tuned to achieve their optimal performance.
This practice fits well with the purpose of automated parameter
optimization. However, there is a lack of thorough understanding
about what are the impacts of automated parameter optimization
on various CPDP techniques. In this paper, we present the first em-
pirical study that looks into such impacts on 62 CPDP techniques,
13 of which are chosen from the existing CPDP literature while
the other 49 ones have not been explored before. We build defect
prediction models over 20 real-world software projects that are of
different scales and characteristics. Our findings demonstrate that:
(1) Automated parameter optimization substantially improves the
defect prediction performance of 77% CPDP techniques with a man-
ageable computational cost. Thus more efforts on this aspect are
required in future CPDP studies. (2) Transfer learning is of ultimate
importance in CPDP. Given a tight computational budget, it is more
cost-effective to focus on optimizing the parameter configuration
of transfer learning algorithms (3) The research on CPDP is far
from mature where it is ‘not difficult’ to find a better alternative by
making a combination of existing transfer learning and classifica-
tion techniques. This finding provides important insights about the
future design of CPDP techniques.
∗K. Li, Z. Xiang and T. Chen contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
©2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05.. .$15.00
https://doi.org/10.1145/3377811.3380360CCS CONCEPTS
•Software and its engineering →Software creation and man-
agement; Software defect analysis.
KEYWORDS
Cross-project defect prediction, transfer learning, classification
techniques, automated parameter optimization
ACM Reference Format:
Ke Li, Zilin Xiang, Tao Chen, Shuo Wang, and Kay Chen Tan. 2020. Under-
standing the Automated Parameter Optimization on Transfer Learning for
Cross-Project Defect Prediction: An Empirical Study. In 42nd International
Conference on Software Engineering (ICSE ’20), May 23–29, 2020, Seoul, Re-
public of Korea. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/
3377811.3380360
1 INTRODUCTION
According to the latest Annual Software Fail Watch report from Tri-
centis1, software defects/failures affected over 3.7 billion people and
caused $1.7 trillion in lost revenue. In practice, stakeholders usu-
ally have limited software quality assurance resources to maintain
a software project. Identifying high defect-prone software mod-
ules (e.g., files, classes or functions) by using advanced statistical
and/or machine learning techniques, can be very helpful for soft-
ware engineers and project managers to prioritize their actions in
the software development life cycle and address those defects.
It is well known that a defect prediction model works well if
it is trained with a sufficient amount of data [ 18]. However, it is
controversial to obtain adequate data (or even having no data at
all) in practice, especially when developing a brand new project or
in a small company. By leveraging the prevalent transfer learning
techniques [ 48], cross-project defect prediction (CPDP) [ 8] has
become increasingly popular as an effective way to deal with the
shortage of training data [ 68]. Generally speaking, its basic idea is to
leverage the data from other projects (i.e., source domain projects)
to build the model and apply it to predict the defects in the current
one (i.e., target domain project).
Defect prediction models usually come with configurable and
adaptable parameters (87% prevalent classification techniques are
with at least one parameter [ 57,58]), the settings of which largely
1https://www.tricentis.com/resources/software-fail-watch-5th-edition/
5662020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
influence the prediction accuracy when encountering unseen soft-
ware projects [ 38,39]. It is not difficult to envisage that the optimal
settings for those parameters are problem dependent and are un-
known beforehand. Without specific domain expertise, software
engineers often train their defect prediction models with off-the-
shelf parameters suggested in their original references. This practice
may undermine the performance of defect prediction models [ 18]
and be adverse to the research reproducibility of defect prediction
studies [ 42,43]. Recently, Tantithamthavorn et al. [57,58] have
empirically shown the effectiveness and importance of automated
parameter optimization for improving the performance and stabil-
ity of many prevalent classification techniques for defect prediction
with manageable additional computational cost.
When considering CPDP, defect prediction become more compli-
cated. To transfer knowledge from the source to the target domain,
prevalent transfer learning techniques naturally bring additional
configurable parameters. According to our preliminary literature
study, 28 out of 33 most widely used CPDP techniques (85%) re-
quire at least one parameter to setup in the transfer learning (or as
known as domain adaptation) stage. This complication inevitably
brings more challenges to the parameter optimization due to the
further explosion of the parameter space, such as the extra difficulty
of finding the optimal configuration and the increased computa-
tional cost for evaluating the model during optimization. Although
hyper-parameter optimization (also known as automated machine
learning) has been one of the hottest topics in the machine learning
community [ 29], to the best of our knowledge, little research have
been conducted in the context of transfer learning.
Bearing these considerations in mind, in this paper, we seek
to better understand how automated parameter optimization of
transfer learning models can impact the performance in CPDP
through a systematic and empirical study. In doing so, we aim to
gain constructive insights based on which one can further advance
this particular research area. To this end, the first research question
(RQ) we wish to answer is:
RQ1 :How much benefit of automated parameter optimiza-
tion can bring to the performance of defect prediction mod-
els in the context of CPDP?
Answering RQ1 is not straightforward, because transfer learning
and classification are two intertwined parts in a CPDP model. Both
of them have configurable parameters that can be used to adapt
and control the characteristics of the CPDP model they produce.
Therefore, the automated parameter optimization can be conducted
by using three possible types of methods, all of which need to be
studied for RQ1:
•Type-I : Naturally, it makes the most sense to optimize the
parameters of both transfer learning and classification si-
multaneously. However, due to the large parameter space,
it might be challenging to find the optimal configuration
within a limited budget of computational cost.
•Type-II : For the sake of taking the best use of computa-
tional cost, alternatively, parameters optimization may be
conducted on one part of a CPDP model, i.e., either the trans-
fer learning (denoted as Type-II-1 ) or the classification
(denoted as Type-II-2 ), at a time; while the other part istrained by using the default parameters. In this way, the
parameter space is reduced, and so does the necessary com-
putational cost. However, this might not necessarily imply
an undermined performance. For example, if transfer learn-
ing is the determinant part of a CPDP model in general, then
optimizing it alone is expected to have at least the same
level of result as optimizing both transfer learning and clas-
sification together (i.e., Type I ) while causing much less
computational cost.
•Type-III : Finally, the automated parameter optimization
can also be conducted in a sequential manner where the
transfer learning part is optimized before the classification
model2. In particular, each part is allocated half of the total
budget of computational cost. In this method, although the
total computational cost is exactly the same as that of Type
I, the parameter space is further constrained, which enables
more focused search behaviors.
The above also motivates our second RQ, in which we ask:
RQ2 :What is the most cost effective type of automated pa-
rameter optimization given a limited amount of computa-
tional cost?
Investigating RQ1 andRQ2 would inevitably require us to go
through a plethora of transfer learning and classification techniques
proposed in the machine learning literature [ 48]. During the process,
we found that the transfer learning and classification techniques in
existing CPDP models are either selected in a problem-specific or
ad-hoc manner. Little is known about the versatility of their com-
binations across various CPDP tasks with different characteristics.
Because of such, our final RQ seeks to understand:
RQ3 :Whether the state-of-the-art combinations of transfer
learning and classification techniques can indeed represent
the generally optimal settings?
To address the above RQs, we apply Hyperopt [7], an off-the-
shelf hyper-parameter optimization toolkit3, as the fundamental
optimizer on the CPDP models considered in our empirical study.
Comprehensive and empirical study is conducted on 62 combina-
tions of the transfer learning algorithms and classifiers, leading to
a total of 37 different parameters to be tuned, and using 20 datasets
from real-world open source software projects. In a nutshell, our
findings answer the RQs as below:
–To RQ1: Automated parameter optimization can substan-
tially improve the CPDP techniques considered in our empir-
ical study. In particular, 77% of the improvements have been
classified as huge according to the Cohen’s deffect size.
–To RQ2: Transfer learning is the most determinant part in
CPDP while optimizing its parameters alone can achieve
better CPDP performance than the other types of automated
parameter optimization.
2The parameters of a classification model is set as default values when optimizing the
transfer learning part.
3http://hyperopt.github.io/hyperopt/
567–To RQ3: No. Some ‘newly’ developed CPDP techniques, with
under-explored combinations of transfer learning and clas-
sification techniques, can achieve better (or at least similar)
performance than those state-of-the-art CPDP techniques.
Drawing on those answers, our empirical study, for the first time,
provides new insights that help to further advance this field of
research4:
•Automated parameter optimization can indeed provide sig-
nificant improvement to the CPDP model, within which
optimizing the parameters of transfer learning is the most
determinant part. This observation suggests that future re-
search on the optimizer can primarily focus on this aspect
in the design and hence prevent wasting efforts on other
methods that provide no better performance but generating
extra computational cost only.
•The state-of-the-art combinations of transfer learning and
classifier are far from being optimal, implying that the selec-
tion of combination is at least as important as the parameter
tuning. As a result, future work should target a whole portfo-
lio of optimization, tuning not only the parameters, but also
the algorithmic components, i.e., the selection of appropriate
transfer learning and classifier pair, of a CPDP model.
The rest of this paper is organized as follows. Section 2 provides
the methodology used to conduct our empirical study. Section 3
present and analyze the experimental results. Thereafter, the results
and threats to validity are discussed in Section 4 along with a
pragmatic literature review in Section 5. At the end, Section 6
concludes the findings in this paper and provides some potential
future directions.
2 THE EMPIRICAL STUDY METHODOLOGY
This section elaborates the methodology and experimental setup of
our empirical study, including the dataset selection, the working
principle of Hyperopt , the system architecture of automated param-
eter optimization for CPDP model building and the performance
metric used to evaluate the performance of a CPDP model.
2.1 Dataset Selection
In our empirical study, we use the following six criteria to select
the datasets for CPDP model building.
(1)To promote the research reproducibility of our experiments,
we choose datasets hosted in public repositories.
(2)To mitigate potential conclusion bias, the datasets are chosen
from various corpora and domains. More importantly, the
shortlisted datasets in our empirical study have been widely
used and tested in the CPDP literature.
(3)If the dataset has more than one version, only the latest ver-
sion is used to train a CPDP model. This is because different
versions of the same project share a lot of similarities which
simplify transfer learning tasks.
(4)To avoid overfiting a CPDP model, the datasets should have
enough instances for model training.
4To enable a reproducible research, all the experimental data and source code of our
empirical study can be found at https://github.com/COLA-Laboratory/icse2020/.(5)To promote the robustness of our experiments, it is better to
inspect the datasets to rule out data that are either repeated
or having missing values.
(6)To resemble real-world scenarios, it is better to choose datasets
from open source projects provided by iconic companies.
According to the first two criteria and some recent survey pa-
pers on the CPDP research [ 24,26,27,68], we shortlist five publicly
available datasets (i.e., JURECZKO ,NASA ,SOFTLAB ,AEEEM ,ReLink ).
Note that these datasets are from different domains and have been
frequently used in the literature. To meet the fourth criterion, we
further rule out SOFTLAB while NASA is also not considered in our
experiments due to its relatively poor data quality [ 55] according
to the fifth criterion. In summary, the datasets used in our experi-
ments consist of 20 open source projects with 10,952 instances. The
characteristics of each dataset are summarized as follows:
–AEEEM [14]: This dataset contains 5 open source projects with
5,371 instances. In particular, each instance has 61 metrics
with two different types, including static and process metrics
like the entropy of code changes and source code chorn.
–ReLink [64]: This dataset consists of 3 open source projects
with 649 instances. In particular, each instance is with 26
static metrics. Note that the defect labels are further man-
ually verified after being generated from source code man-
agement system commit comments.
–JURECZKO [30]: This dataset originally consists of 92 released
products collected from open source, proprietary and aca-
demic projects. To meet the first criterion, those propri-
etary projects are ruled out from our consideration. To meet
the last criterion, the projects merely for academic use are
excluded from JURECZKO . Moreover, since the projects in
JURECZKO have been updated more than one time, according
to the third criterion, only the latest version is considered
in our experiments. At the end, we choose 12 open source
projects with 4,932 instances in our empirical study.
2.2 Hyperopt for Automated Parameter
Optimization
Hyperopt5is a Python library that provides algorithms and the
software infrastructure to optimize hyperparameters of machine
learning algorithms. Hyperopt uses its basic optimization driver
hyperopt.fmin to optimize the parameter configurations of the
CPDP techniques considered in our empirical study. Using Hyperopt
requires three steps.
–Define an objective function :As suggested in [ 6],Hyperopt
provides a flexible level of complexity when specifying an
objective function that takes inputs and returns a loss users
want to minimize. In our experiments, the inputs are param-
eters associated with the transfer learning and classification
techniques as shown in the third column of Table 1 and Ta-
ble 2 respectively.
–Define a configuration space :The configuration space in
the context of Hyperopt describes the domain over which
users want to search. The last column of Table 1 and Table 2
list the configuration space of the corresponding parameter.
5http://hyperopt.github.io/hyperopt/
568bar.c
bar.h
foo.c
foo.h
bar.c
bar.h
foo.c
foo.hsource domain 
training data
target domaintraining data
···
···
···
···
target domain
testing dataraw
datasettransfer learning
algorithmsdefect prediction
modelcalculate prediction
performanceDefine an 
objective functionDefine a 
configuration spaceTPE algorithmHyperoptFigure 1: The architecture of automated parameter optimization on CPDP model by using Hyperopt.
–Choose an optimization algorithm :Hyperopt provides
two alternatives, i.e.,random search [ 5] and Tree-of-Parzen-
Estimators (TPE) algorithm [ 4], to carry out the parameter
optimization. In our experiments, we choose to use the TPE
algorithm because the sequential model-based optimization
has been recognized as one of the most effective hyperpa-
rameter optimization methods in the auto-machine learning
literature [ 17]. In practice, Hyperopt provides a simple inter-
face to deploy the TPE algorithm where users only need to
pass algo=hyperopt.tpe.suggest as a keyword argument
tohyperopt.fmin.
2.3 Architecture for Optimizing CPDP Model
Figure 1 shows the architecture of using Hyperopt to optimize the
performance of CPDP models. As shown in Figure 1, the implemen-
tation steps of our empirical study are given below.
(1)Given a raw dataset with Nprojects, N−1of which are used
as the source domain data while the other project is used as
the target domain data. In particular, all source domain data
is used for training whilst the data from the target domain is
used for testing. To mitigate a potentially biased conclusion
on the CPDP ability, all 20 projects will be used as target
domain data in turn during our empirical study.
(2)The CPDP model building process consists of two inter-
twined parts, i.e.,transfer learning and defect prediction
model building.
•Transfer learning aims to augment data from different
domains by selecting relevant instances or assigning ap-
propriate weights to different instances, etc. Table 1 out-
lines the parameters of the transfer learning techniques
considered in our empirical study.
•Based on the adapted data, many off-the-shelf classifica-
tion techniques can be applied to build the defect predic-
tion model. Table 2 outlines the parameters of the classifi-
cation techniques considered in this paper.(3)The performance of the defect prediction ability of the CPDP
model is at the end evaluated upon the hold-out set from the
target domain data.
Note that there are 13 CPDP techniques considered in our empir-
ical study. All of them are either recognized as the state-of-the-art
in the CPDP community or used as the baseline for many other
follow-up CPDP techniques. Table 3 lists the combination of trans-
fer learning and classifier used in each CPDP technique. To carry
out the automated parameter optimization for a CPDP technique,
Hyperopt is allocated 1,000 function evaluations. In our context,
one function evaluation represents the complete model training
process of a CPDP technique with a trial parameter setup, which
can be computationally expensive. To carry out statistical analysis
over our experiments, the optimization over each CPDP technique
is repeated 10 times.
2.4 Performance Metric
To evaluate the performance of different CPDP methods for identi-
fying defect-prone modules, we choose the area under the receiver
operator characteristic curve (AUC) in our empirical study. This is
because AUC is the most widely used performance metric in the
defect prediction literature. In addition, there are two distinctive
characteristics of AUC: 1) different from other prevalent metrics
like precision and recall, the calculation of AUC does not depend
on any threshold [ 68] which is difficult to tweak in order to carry
out an unbiased assessment; and 2) it is not sensitive to imbalanced
data which is not unusual in defect prediction [ 34]. Note that the
larger the AUC values, the better prediction accuracy of the un-
derlying classification technique is. In particular, the AUC value
ranges from 0 to 1 where 0 indicates the worst performance, 0.5
corresponds a random guessing performance and 1 represents the
best performance.
3 RESULTS AND ANALYSIS
In this section, we will present the experimental results of our em-
pirical study and address the research questions posed in Section 1.
569Table 1: Parameters of the transfer learning techniques considered in our experiments
Transfer learningParameters
Name Description Range
Bruakfilter k The number of neighbors to each point (default=10)[N][1, 100]
DStopN The number of closest training sets (default=5)[N][1, 15]
FSS The ratio of unstable features filtered out (default=0.2)[R][0.1, 0.5]
DSBFToprank The number assigned to 1 when performing feature reduc-
tion (default=1)[N][1, 10]
k The number of neighbors to each point (default=25)[N][1, 100]
TCAkernel The type of kernel (default=‘linear’)[C]{‘primal’, ‘linear’, ‘rbf’, ‘sam’}
dimension The dimension after tranforing (default=5)[N][5, max(N_source, N_target)]
lamb Lambda value in equation (default=1)[R][0.000001, 100]
gamma Kernel bandwidth for ‘rbf’ kernel (default=1)[R][0.00001, 100]
DBSCANfiltereps The maximum distance between two samples for one to be
considered as in the neighborhood of the other (default=1.0)
[R][0.1, 100]
min_samples The number of samples (or total weight) in a neighborhood
for a point to be considered as a core point (default=10)[N][1, 100]
Universalp-value The associated p-value for statistical test (default=0.05)[R][0.01, 0.1]
quantifyType The type of quantifying the difference between distribu-
tions (default=‘cliff’)[C]{‘cliff’, ‘cohen’}
DTBk The number of neighbors to each point (default=10)[N][1, 50]
T The maximum number of iterations (default=20)[N][5, 30]
Peterfilter r The number of points in each cluster (default=10)[N][1, 100]
[N]An integer value from range
[R]A real value from range
[C]A choice from categories
3.1 On the Impacts of Automated Parameter
Optimization Over CPDP Techniques
3.1.1 Research Method. To address RQ1 , we investigate the mag-
nitude of AUC performance improvement achieved by the CPDP
model optimized by Hyperopt versus the one trained by its default
parameter setting. Under a target domain (project), instead of com-
paring the difference of vanilla AUC values6for all repeated runs,
we use Cohen’s deffect size [ 12] to quantify such magnitude. This
is because it is simple to calculate and has been predominately
used as the metric for automated parameter optimization of defect
prediction model [ 57]. Given two sets of samples say S1andS2,
Cohen’s deffect size aims to provide a statistical measure of the
standardized mean difference between them:
d=µ1−µ2
s, (1)
where µ1andµ2is the mean of S1andS2respectively; sis as defined
as the pooled standard deviation:
s=s
(n1−1)s2
1+(n2−1)s2
2
n1+n2−2(2)
where n1andn2is the number of samples in S1andS2respectively;
while s1ands2are the corresponding standard deviations of the
6To make our results self-contained, the vanilla AUC values are given in the sup-
plementary document of this paper and can be found in https://github.com/COLA-
Laboratory/icse2020two sample sets. To have a conceptual assessment of the magnitude,
according to the suggestions in [ 54],d<0.2is defined as negligible,
0.2<d≤0.5is treated as small, 0.5<d≤0.8is regarded as
medium, 0.8<d≤1.0is large while it is huge ifdgoes beyond 1.0.
As introduced in Section 1, we run four different optimization
types (as presented in Section 1) in parallel upon each baseline
CPDP technique. To investigate whether Hyperopt can improve the
performance of a CPDP technique, we only present the results from
the best optimization type to make our conclusion sharper. For each
CPDP technique, we use a violin plot to show the distributions of its
median values of Cohen’s deffect size obtained by optimizing the
parameters of this CPDP techniques on 20 projects. To have a better
understanding of the effect of automated parameter optimization
upon different CPDP techniques, the violin plots are sorted, from
left to right, by the median values of Cohen’s deffect size in a
descending order.
3.1.2 Results. From the comparison results shown in Figure 2,
we clearly see that the performance of 12 out of 13 (around 92%)
existing CPDP techniques have been improved after automated pa-
rameter optimization. In addition, according to the categorization
in [54], the magnitudes of most AUC performance improvements
are substantial and important. In particular, ten of them (around
77%) are classified as huge ; while the performance improvements
achieved by optimizing the parameters of DS+BF (NB) belong to the
medium scale. In particular, Hyperopt leads to the most significant
570Table 2: Parameters of the classification techniques considered in our experiments
Classification techniquesParameters
Name Description Range
K-Nearest Neighbor (KNN) n_neighbors The number of neighbors to each point (default=1)[N][1, 50]
Boostn_estimators The maximum number of estimators (default=50)[N][10, 1000]
learning rate A factor that shrinks the contribution of each classifier
(default=1)[R][0.01, 10]
Classification and Regression Tree (CART)criterion The maximum number of estimators (default=10)[N][10, 100]
max_features The function to measure the quality of a split (de-
fault=‘gini’)[C]{‘gini’, ‘entropy’}
splitter The number of features to consider when looking for
the best split (default=‘auto’)[C]{‘auto’, ‘sqrt’, ‘log2’}
min_samples_split The minimum number of samples required to split an
internal node (default=2)[N][2, N_source/10]
Random Forest (RF)n_estimators The maximum number of estimators (default=10)[N][10, 100]
criterion The function to measure the quality of a split (de-
fault=‘gini’)[C]{‘gini’, ‘entropy’}
max_features The number of features to consider when looking for
the best split (default=‘auto’)[C]{‘auto’, ‘sqrt’, ‘log2’}
min_samples_split The minimum number of samples required to split an
internal node (default=2)[N][2, N_source/10]
Support Vector Machine (SVM)kernel The type of kernel (default=‘poly’)[C]{‘rbf’, ‘linear’, ‘poly’, ‘sigmoid’}
degree Degree of the polynomial kernel function (default=3)[N][1, 5]
coef0 Independent term in kernel function. It is only significant
in ‘poly’ and ‘sigmoid’ (default=0.0)[R][0, 10]
gamma Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’ (de-
fault=1)[R][0.01, 100]
Multi-layer Perceptron (MLP)active Activation function for the hidden layer (default=‘relu’)[C]{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}
alpha L2 penalty (regularization term) parameter (de-
fault=0.0001)[R][0.000001, 1]
iter Maximum number of iterations (default=200)[N][10, 500]
Ridgealpha Regularization strength (default=1)[R][0.0001, 1000]
normalize Whether to standardize (default=‘False’)[C]{‘True’, ‘False’}
Naive Bayes (NB) NBType The type of prior distribution (default=‘Gaussian’)[C]{‘gaussian’, ‘multinomial’, ‘bernoulli’}
[N]An integer value from range
[R]A real value from range
[C]A choice from categories
Table 3: Overview of existing CPDP techniques considered
in our empirical study.
CPDP Techniques Refer ence CPDP Techniques Refer ence
Bruakfilter (NB) [60] DS+BF (RF)[3]Petersfilter (RF)
[50]DS+BF (NB)
Petersfilter (NB) DTB [10]
Petersfilter (KNN) DBSCANfilter (RF)
[31] FSS+Bagging (RF)[21]DBSCANfilter (NB)
FSS+Bagging (NB) DBSCANfilter (KNN)
UM [66]
The classifier is shown in the brackets while outside part is the transfer learning technique.
UM uses Universal to carry out transfer learning and Naive Bayes as a classifier. DTB uses
DTB to carry out transfer learning part and Naive Bayes to conduct classification.
performance improvement on DBSCANfilter (NB) and DTB. On
the other hand, the magnitudes of AUC performance improvement
achieved by optimizing the parameters of UM and Bruakfilter (NB)
arenegligible (i.e., Cohen’s d<0.2). It is worth mentioning that
Hyperopt cannot improve the performance of Bruakfilter (NB) any
further at all considered projects. This might suggest that the origi-
nal parameter configuration of Bruakfilter (NB) is already optimal.
Overall, we obtain the following findings:
DBSCANfilter(NB)DTBFSS+Bagging(NB)DBSCANfilter(KNN)FSS+Bagging(RF)DS+BF(RF)DBSCANfilter(RF)Petersfilter(KNN)Petersfilter(RF)Petersfilter(NB)DS+BF(NB)UMBruakfilter(NB)−20246810
CPDP techniquesCohen'd effect sizeFigure 2: The AUC performance improvement in terms of
Cohen’s deffect size for each studied CPDP technique.
Answer to RQ1 :Automated parameter optimization can
improve the performance of defect prediction models in the
context of CPDP. In particular, the performance of 10 out
of 13 (around 77%) studied CPDP techniques have been im-
proved substantially (i.e., huge in terms of Cohen’s deffect
size value).
571UMDTBDS+BF(RF)DS+BF(NB)DS+BF(SVM)DBSCANfilter(RF)DBSCANfilter(KNN)DBSCANfilter(NB)FSS+Bagging(RF)FSS+Bagging(NB)Bruakfilter(NB)Petersfilter(NB)Petersfilter(RF)
CombinationsEQJDTLCMLPDESafeApacheZxingantcamelivyjEditlog4jlucenepoisynapsetomcatxercesProjectsType I Type II-1 Type II-2 Type IIIFigure 3: Distribution of the best parameter optimization
type for each CPDP technique and project pair.
Type-II-137.7%Type-I
28.1%Type-II-2
21.9%
Type-III12.3%
Figure 4: The percentage of significantly better performance
achieved by different optimization types.
3.2 Comparing Different Types of Parameter
Optimization
3.2.1 Research Method. To answer RQ2 , we investigate the per-
formance of four different types of parameter optimization, as in-
troduced in Section 1. To have an overview of the result, for each
CPDP technique, we record the best parameter optimization type.
In addition, for each optimization type, we also record the number
of times that its AUC value is significantly better than the other
peers over all CPDP technique and project pairs. In addition, to
have a better intuition on the effect of different types of parame-
ter optimization over each CPDP technique, we use violin plots to
show the distributions of the median values of Cohen’s deffect size
obtained over 20 projects7.
3.2.2 Results. From the results shown in Figure 3, we have a
general impression that Type-II-1 plays as the best parameter
optimization type in most cases. In particular, for UM and DTB,
7Again, to make our results self-contained, the vanilla AUC values are given in the
supplementary document of this paper and can be found in https://github.com/COLA-
Laboratory/icse2020Type-II-1 almost dominates the list. The second best parameter op-
timization type is Type-I whilst the worst one is Type-III which
is rarely ranked as the best optimization type in most cases.
The pie chart shown in Figure 4 is a more integrated way to
summerize the results collected from Figure 3. From this figure,
we can see that the type of only optimizing the parameters as-
sociated with the transfer learning part in CPDP is indeed more
likely to produce the best performance. In particular, 37.7% of the
best AUC performance is achieved by Type-II-1 . It is even better
than simultaneously optimizing the parameters of both transfer
learning and classification parts, i.e., Type-I , which wins on 28.1%
comparisons. This might be because given the same amount of
computational budget, simultaneously optimizing the parameters
of both transfer learning and classification parts is very challenging
due to the large search space. As a result, Hyperopt might run out
of function evaluations before approaching the optimum on neither
part. On the other hand, if Hyperopt only focuses on optimizing
the parameters of the transfer learning part, the search space is sig-
nificantly reduced. Therefore, although only part of the parameters
is considered, it is more likely to find the optimal parameter config-
uration of the transfer learning part within the limited number of
function evaluations. The same applied to Type-II-2 , which only
focus on optimizing the parameters of the classification techniques.
However, as shown in Figure 4, the performance of Type-II-2 is
not as competitive as Type-I andType-II-1 , implying that the
classification part is less important than the transfer learning part
in CPDP, which eventually obscures the benefit brought by the
reduced search space. Finally, we see that sequentially optimizing
the transfer learning and classification parts with equal budget of
computation is the worst optimization type ( Type-III ). This is
because it does not only fail to fully optimize both parts before
exhausting the function evaluations, but also ignore the tentative
coupling relationship between the parameters associated with both
the transfer learning and classification.
From the results shown in Figure 5, we find that the performance
difference between different types of parameter optimization is not
very significant in most CPDP techniques. Nonetheless, we can
still observe the superiority of Type-I andType-II-1 over the
other two optimization types in most performance comparisons. In
particular, for DBSCANfilter (NB), DBSCANfilter (KNN) and DTB,
only optimizing the parameters of the classification part does not
make any contribution to the performance improvement on the
corresponding CPDP techniques. This observation is also aligned
with our previous conclusion that the transfer learning part is more
determinant than the classification part in CPDP. In summary, we
find that:
Answer to RQ2 :Given a limited amount of computational
budget, it is more plausible to focus on the parameter op-
timization of the transfer learning part in CPDP than
the other types, including optimizing the configurations of
both transfer learning and classification simultaneously.
This observation also demonstrates that the transfer learn-
ing part is more determinant in CPDP.
572123400.511.52
UMCohen's d123401234
DTBCohen's d
12340.511.522.53
DS+BF(RF)Cohen's d
123402468
DS+BF(NB)Cohen's d123411.52
DBSCANfilter(RF)Cohen's d
12340246
DBSCANfilter(KNN)Cohen's d
123405101520
DBSCANfilter(NB)Cohen's d
123411.522.53
FSS+Bagging(RF)Cohen's d
1234024
FSS+Bagging(NB)Cohen's d
1234051015
Bruakfilter(NB)Cohen's d
12340510
Petersfilter(NB)Cohen's d
12340246
Petersfilter(RF)Cohen's d
12340246
Petersfilter(KNN)Cohen's dFigure 5: Violin plots of AUC performance improvement in terms of Cohen’s deffect size for each studied CPDP technique. 1:
Type-I, 2: Type-II-1, 3: Type-II-2, 4: Type-III
3.3 Comparing Different Combinations of
Transfer Learning and Classification
Techniques for CPDP
3.3.1 Research Method. To address RQ3 , we build and compare 62
different CPDP models by combining those transfer learning and
classification techniques listed in Table 1 and Table 2 respectively. 13
out of these 62 combinations exist in the literature. The remaining 49
combinations can be regarded as ‘new’ CPDP techniques. Because
DTB requires to update the weights of its training instances during
the training process, it cannot work with KNN or MLP which do
not support online training data adjustments. In other words, the
combinations DTB-KNN and DTB-MLP are ruled out from our
empirical study. For a better visualization, instead of presenting the
performance of all 62 combinations together, we only show the 10
best CPDP techniques. We use violin plots to show the distributions
of their AUC values.
In addition, for each project, we compare the performance of the
best CPDP technique from the existing literature and those ‘newly’
developed in this paper. To have a statistically sound conclusion, we
apply the Wilcoxon signed-rank sum test with a 0.05 significance
level to validate the statistical significance of those comparisons. In
particular, if the best candidate from the ‘newly’ developed CPDP
techniques is significantly better than the best one from the current
literature, it is denoted as win; if their difference is not statistically
significant, it is denoted as tie; otherwise, it is denoted as loss. We
keep a record of the number of times of these three scenarios.
3.3.2 Results. From the violin plots shown in Figure 6, we find that
the list of top 10 CPDP techniques varies from different projects. In
particular, DTB-RF is the best CPDP technique as it appears in all
top 10 lists and is ranked as the first place in 9 out of 20 projects.
Furthermore, we notice that DTB, Peterfilter and DBSCANfilter are
the best transfer learning techniques for CPDP because they were
used as the transfer learning part in the CPDP techniques of all top
10 lists. From these observations, we conclude that CPDP techniques
also follow the no-free-lunch theorem [63]. In other words, there is
no universal CPDP technique capable of handling all CPDP tasks
of the data have different characteristics.
Figure 7 gives the statistics of the comparison between the best
CPDP technique from the existing literature and the one from our
‘newly’ developed portfolio. From the statistics, we can see that the
CPDP techniques newly developed in this paper, by making a novel
combination of transfer learning and classification techniques, arebetter than those existing ones in the literature. Remarkably, they
are only outperformed by the existing CPDP techniques under
one occasion. From this observation, we envisage that the current
research on CPDP is far from mature. There is no rule-of-thumb a
practitioner can follow to design a CPDP technique for the black-
box dataset at hand. For RQ3, we have the following findings:
Answer to RQ3 :The current research on CPDP techniques
is far from mature. Given a CPDP task, there is no rule-of-
thumb available for a practitioner to follow in order to 1)
design an effective technique that carries out an appropri-
ate transfer learning and classification; and 2) find out the
optimal configurations of their associated parameters.
4 DISCUSSIONS
4.1 Insights Learned from Our Empirical Study
Our empirical study, for the first time, reveals some important and
interesting facts that provide new insights to further advance the
research on CPDP.
Through our comprehensive experiments, we have shown that
automated parameter optimization can significantly improve the
defect prediction performance of various CPDP techniques with
ahuge effect size in general. In particular, it is surprising but also
exciting to realize that optimizing the parameters of transfer learn-
ing part only is generally the most cost effective way of tuning the
defect prediction performance of CPDP techniques. Such a finding
offers important insight and guidance for future research: given a
limited amount of computational budget, designing sophisticated
optimizer for the parameters of CPDP techniques can start off by
solely considering the transfer learning part, without compromising
the performance.
Our other insightful finding is to reveal the fact that the cur-
rent research on CPDP is far from mature. In particular, many
state-of-the-art combinations of transfer learning and classification
techniques are flawed, and that the best combination can be case
dependent. This suggests that automatically finding the optimal
combination of CPDP techniques for a case is a vital research di-
rection, and more importantly, the combination should be tuned
with respect to the optimization of parameters. Such an observation
can derive a brand new direction of research, namely the portfo-
lio optimization of transfer learning and classifier that lead to an
automated design of CPDP technique.
573DTB-RFDTBTCA-BRRVWTCA-CARTDSBF-CARTTCA-RFTCA-KNNPeWeUfiOWeU-BRRVWDS-MLPTCA-SVM0.620.640.660.680.70.720.740.76TaUgeW SURMecW: Z[iQgAUC
DTB-RFPeWeUfilWeU-BooVWPeWeUfilWeU-CARTDSBF-KNNDSBF-CARTDTBDBSCANfilWeU-CARTDS+BF(RF)PeWeUVfilWeU(RF)DS-CART0.550.60.650.70.75TaUgeW SUojecW: [eUceVAUC
DS-MLPPHWHUILOWHU-RLGJHPHWHUILOWHU-MLPPHWHUILOWHU-BRRVWDS-KNNDBSCANILOWHU(NB)PHWHUVILOWHU(NB)PHWHUILOWHU-CARTUMFSS+BaJJLQJ(RF)0.720.730.740.750.760.770.780.79TaUJHW SURMHcW: [aOaQAUC
DTB-RFPeWeUfilWeU-BRRVWPeWeUfilWeU-RidgeDBSCANfilWeU-BRRVWPeWeUfilWeU-CARTDS-CARTDBSCANfilWeU-CARTPeWeUVfilWeU(NB)DTBBUXakfilWeU-CART0.50.550.60.650.70.750.8TaUgeW SURjecW: YelRciW\AUCDBSCANfiOWeU-MLPDS-MLPDTB-RFPeWeUVfiOWeU(KNN)PeWeUfiOWeU-MLPPeWeUVfiOWeU(NB)PeWeUfiOWeU-RidgeDBSCANfiOWeU(NB)DS-KNNPeWeUfiOWeU-BRRVW0.70.750.80.850.90.95TaUgeW SURMecW: WRPcaWAUC
DTB-RFDTBDS+BF(RF)PeWeUVfiOWeU(RF)PeWeUfiOWeU-MLPDS-BRRVWPeWeUVfiOWeU(NB)PeWeUfiOWeU-CARTDS-MLPDSBF-CART0.80.850.90.9511.05TaUgeW SURjecW: V\QaSVeAUC
DTB-RFDTBDSBF-MLPDSBF-CARTDS-CARTPeWeUfilWeU-MLPPeWeUVfilWeU(RF)PeWeUfilWeU-CARTDS-MLPUniYeUVal-MLP0.80.850.90.9511.05TaUgeW SURjecW: SafeAUC
DTB-RFPeWeUVfilWeU(NB)DBSCANfilWeU(NB)DS-MLPPeWeUfilWeU-MLPPeWeUfilWeU-BRRVWDBSCANfilWeU-CARTDBSCANfilWeU-BRRVWPeWeUfilWeU-CARTUM0.650.70.750.80.85TaUgeW SURjecW: SRiAUCDTB-RFPeWeUfiOWeU-BRRVWPeWeUfiOWeU-CARTPeWeUVfiOWeU(RF)DBSCANfiOWeU-MLPPeWeUVfiOWeU(NB)DTBUMDBSCANfiOWeU-RidgeDBSCANfiOWeU-BRRVW0.650.70.750.80.850.9TaUgeW SURMecW: LCAUC
PeWeUVILOWeU(NB)PeWeUVILOWeU(RF)PeWeUILOWeU-CARTPeWeUILOWeU-BRRVWDSBF-SVMDTB-RFPeWeUILOWeU-SVMDS-MLPPeWeUILOWeU-MLPDS-SVM0.740.760.780.80.820.840.860.880.90.92TaUJeW SURMecW: MEdLWAUC
DTB-RFDS-MLPPHWHUVILOWHU(RF)PHWHUILOWHU-BRRVWPHWHUILOWHU-CARTDS-BRRVWBUXaNILOWHU-MLPFSS+BaJJLQJ(RF)DS-SVMPHWHUILOWHU-RLGJH0.730.740.750.760.770.780.790.80.81TaUJHW SURMHcW: JDTAUC
PeWeUfLOWeU-BRRVWUQLYeUVaO-SVMDSBF-BRRVWDS+BF(NB)PeWeUfLOWeU-SVMDTB-RFDSBF-SVMDS-SVMPeWeUfLOWeU-CARTPeWeUfLOWeU-RLdge0.640.660.680.70.720.740.760.780.8TaUgeW SURMecW: LY\AUCPeWeUfiOWeU-CARTDBSCANfiOWeU(NB)PeWeUfiOWeU-BRRVWDS-SVMDBSCANfiOWeU-CARTPeWeUVfiOWeU(RF)DTB-RFPeWeUVfiOWeU(NB)DBSCANfiOWeU(RF)DBSCANfiOWeU-BRRVW0.650.70.750.8TaUgeW SURMecW: EQAUC
DTB-RFDTBPeWeUVILOWeU(NB)PeWeUILOWeU-MLPFSS+BaJJLQJ(NB)DS-MLPPeWeUILOWeU-CARTPeWeUILOWeU-RLdJePeWeUILOWeU-BRRVWDS-KNN0.650.70.750.80.850.90.951TaUJeW SURMecW: caPeOAUC
DTBDTB-RFDSBF-RLdJeDSBF-MLPDSBF-CARTDS-KNNFSS+BaJJLQJ(RF)UQLYeUVaO-MLPDS-MLPPeWeUVfLOWeU(RF)0.720.740.760.780.80.820.840.860.88TaUJeW SURMecW: ASacKeAUC
DTB-RFPeWeUVfLOWeU(RF)DS-BRRVWPeWeUfLOWeU-BRRVWDSBF-CARTDS-MLPDS-KNNDTBFSS+BaggLQg(NB)PeWeUfLOWeU-MLP0.70.750.80.850.90.9511.05TaUgeW SURMecW: aQWAUC
PHWHUILOWHU-MLPDS-MLPPHWHUVILOWHU(NB)DBSCANILOWHU-RLdJHDBSCANILOWHU-MLPDBSCANILOWHU(RF)DS-BRRVWDSBF-BRRVWDTB-RFUQLYHUVaO-MLP0.660.670.680.690.70.710.720.730.74TaUJHW SURMHcW: PDEAUC
DBSCANfLOWeU-SVMPeWeUfLOWeU-BRRVWDSBF-CARTPeWeUVfLOWeU(NB)PeWeUfLOWeU-MLPDS-SVMPeWeUfLOWeU-CARTPeWeUfLOWeU-SVMDS-MLPDTB-RF0.580.60.620.640.660.680.70.72TaUgeW SURMecW: MLAUC
DBSCANfiOWeU-RidgePeWeUVfiOWeU(NB)DS-MLPPeWeUfiOWeU-MLPDBSCANfiOWeU-MLPPeWeUfiOWeU-BRRVWDTB-RFUMFSS+BaggiQg(NB)DBSCANfiOWeU-BRRVW0.60.650.70.750.80.85TaUgeW SURMecW: OXceQeAUC
FSS+BaJJLQJ(RF)DTBDS-BRRVWDS-MLPPeWeUVfLOWeU(NB)PeWeUfLOWeU-CARTPeWeUfLOWeU-RLdJePeWeUfLOWeU-MLPDBSCANfLOWeU-RLdJePeWeUfLOWeU-BRRVW0.70.750.80.850.90.951TaUJeW SURMecW: ORJ4MAUCFigure 6: Violin plots of AUC values obtained by top 10 CPDP techniques for different projects. In particular, existing CPDP
techniques are with black charts whilst ‘newly’ developed ones are with white .
01234567891011win
tie
loss9
10
1
‘newly’developed CPDP techniques vsstate-of-the-art ones
Figure 7: Comparisons of state-of-the-art CPDP techniques
and those ‘newly’ developed in this paper.4.2 Threats to Validity
Similar to many empirical studies in software engineering, our
work is subject to threats to validity. Specifically, internal threats
can be related to the number of function evaluations used in the
optimization. Indeed, a larger amount of function evaluations may
lead to better results in some cases. However, the function eval-
uation in our empirical study is expensive and time consuming,
as every single one needs to go through the full machine learning
training process, validation and testing. As a result, a proper choice
should be a good trade-off between the performance and time. To
574mitigate this threat, we have run numbers of options in a trial-and-
error manner. We then concluded that 1,000 function evaluations is
deemed as a balanced choice without compromising the validity
of our conclusions. Furthermore, to mitigate bias, we repeated 10
times for each CPDP technique on a project, which is acceptable
considering the cost of function evaluation.
Construct threats can be raised from the selected quality indi-
cator. In this work, AUC has been chosen as the key performance
indicator in our empirical comparisons. The selection is mainly
driven by its simplicity (no extra parameter is required) and its
robustness (insensitive to imbalanced data). In addition, AUC has
been widely recognised as one of the most reliable performance
indicator in the machine learning community [ 35]. The significance
of differences have also been assessed in terms of the effect size
using Cohen’s d.
Finally, external threats are concerned with the dataset and CPDP
techniques studied. To mitigate such, we have studied 62 CPDP
techniques, including 13 combinations from existing work on CPDP
and 49 other combinations that are new to the CPDP community
but widely applied in classic machine learning research. Further, as
discussed in Section 2.1, our studied dataset covers a wide spectrum
of the real-world defected projects with diverse nature, each of
which was selected based on six systematic criteria. Such a tailored
setting, although not exhaustive, is not uncommon in empirical
software engineering and can serve as strong foundation to gener-
alize our findings, especially considering that an exhaustive study
of all possible CPDP techniques and dataset is unrealistic.
5 RELATED WORK
Software defect prediction is one of the most active research areas
in software engineering [ 15,32,41,47,51,69]. Generally speaking,
the purpose of defect prediction is to learn an accurate model (su-
pervised or unsupervised) from a corpus of data (e.g., static code
features, churn data, defect information) and apply the model to
new and unseen data. To this end, the data can be labeled by using
code metrics [ 2,11,16,37]; process metrics [ 19,32,44,45]; or met-
rics derived from domain knowledge [ 40,56]. Depending on the
scenario, the training data can come from the same project that one
aims to predict the defects for, i.e., within project defect prediction
or from other projects, i.e., CPDP. For supervised learning, CPDP
consists of two parts: domain adaptation and classification where
the former is resolved by transfer learning while the latter is tackled
by a machine learning classifier.
In the past two decades, CPDP has attracted an increasing at-
tention, as evidenced by many survey work [ 24,26,27,68]. In
CPDP, the homogeneous CPDP problem, which we focus on this
work, refers to the case where the metrics of the source and tar-
get projects are the exactly the same or at least contain the same
ones. Among others, instance selection is the earliest and most
common way to transfer the learned model for CPDP, in which
similar source instances to the target instances are selected to learn
a model [ 3,31,50,53,60]. Alternatively, instance weighting uses
different weights for each source instance, depending on the rel-
evance of the instance to the target instances, see [ 36], [10] and
[52]. Projects and feature selection is another way to transfer the
learned a model when there are multiple source projects [ 21,22],[20,23,28]. Finally, instance standardization exist for CPDP, in
which the key idea is to transform source and target instance into
a similar form (e.g., distribution and representation) [ 47,66]. More
comprehensive summaries about techniques for CPDP can be found
in the survey by Zimmermann et al. [70] and He et al. [22].
More recently, studies have shown that CPDP can be improved
by using different models [ 9], model combination [ 65][49][25] or
a model ensemble [ 61][67]. Another way to improve prediction
performance is via data preprocessing before applying a CPDP
techniques [ 62][13][59], or directly using an unsupervised learning
have, such as the work by Nam and Kim [46].
Despite the tremendous amount of studies on the defect predic-
tion models and approaches to improve the prediction performance,
their parameter optimization has not received enough attentions.
This is in fact non-trivial, as we have shown that more than 80%
of the defect prediction models have at least one configurable and
adaptable parameter. However, most work on defect prediction as-
sumes default settings or relies on ad-hoc methods, which provide
little guarantee on the achieved performance. This is an even more
serious issue when considering CPDP, in which case the number
of parameters and the possible configuration options can be en-
larged dramatically. Very few studies have been performed on the
automated parameter optimization for defect prediction models.
Lessmann et al .[33] are among the first to conduct automated pa-
rameter optimization for defect prediction models by using grid
search. Agrawal et al .[1] preform a more recent study and propose
an optimization tool called DODGE , which eliminates the need to
evaluate some redundant combinations of preprocessors and learn-
ers. Nevertheless, unlike our work, these studies neither aim to
empirically evaluate the impact of automated parameter optimiza-
tion nor focus on CPDP.
The most related work is probably the empirical study from Tan-
tithamthavorn et al .[57], in which they perform the first thorough
study on the impact of automated parameter optimization for de-
fect prediction models [ 58]. However, our work is fundamentally
different from theirs in the following aspects:
•Considering cross-projects: We empirically study the auto-
mated parameter optimization for transfer learning based
models on cross-project prediction while Tantithamthavorn
et al.[57] focus on the optimization for the defect prediction
model within a single project.
•Studying a larger set of models: The number of combinations
of transfer learner and classifier considered in our exper-
iments constitutes 62 CPDP techniques. This amount is
nearly six times more than the 11 classifiers studied by Tan-
tithamthavorn et al. [57].
•Providing wider insights: Our findings, apart from confirm-
ing that the automated parameter optimization on CPDP
is effective, also provides insights on the algorithm selec-
tions for CPDP. In contrast, Tantithamthavorn et al .[57]
mainly provide analysis on the effectiveness and stability of
automated parameter optimization.
In summary, our work is, to the best of our knowledge, the first
comprehensive empirical study about the impact of automated
parameter optimization on transfer learning for CPDP, based on
575which we have revealed important findings and insights that have
not been known before.
6 CONCLUSIONS
In this paper, we conduct the first empirical study, which offers an
in-depth understanding on the impacts of automated parameter
optimization for CPDP based on 62 CPDP techniques across 20
real-world projects. Our results reveal that:
•Automated parameter optimization can significantly im-
prove the CPDP techniques. Up to 77% of the improvement
exhibits huge effect size under the Cohen’s rule.
•Optimizing the parameters of transfer learning techniques
plays a more important role in performance improvement
in CPDP.
•The state-of-the-arts combinations of transfer learning and
classification are far from mature, as the statistically best
technique comes from the 49 ‘newly’ developed combina-
tions in most cases.
Our findings provide valuable insights for the practitioners from
this particular research field to consider. Drawing on such, in our
future work, we will design sophisticated optimizer for CPDP that
explicitly searches the parameter space for the transfer learning
part. Furthermore, the problem of portfolio optimization for CPDP,
which involves both the selection of combination and parameter
tuning, is also one of our ongoing research directions.
ACKNOWLEDGEMENT
K. Li was supported by UKRI Future Leaders Fellowship (Grant No.
MR/S017062/1) and Royal Society (Grant No. IEC/NSFC/170243).
REFERENCES
[1]Amritanshu Agrawal, Wei Fu, Di Chen, Xipeng Shen, and Tim Menzies. 2019.
How to "DODGE" Complex Software Analytics? CoRR abs/1902.01838 (2019).
[2]Fumio Akiyama. 1971. An Example of Software System Debugging. In IFIP
Congress (1). 353–359.
[3]Sousuke Amasaki, Kazuya Kawata, and Tomoyuki Yokogawa. 2015. Improv-
ing Cross-Project Defect Prediction Methods with Data Simplification. In
EUROMICRO-SEAA’15: Proc. of the 41st Euromicro Conference on Software En-
gineering and Advanced Applications. 96–103.
[4]James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms
for Hyper-Parameter Optimization. In NIPS’11: Proc. of the 25th Annual Conference
on Neural Information Processing Systems. 2546–2554.
[5]James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter
Optimization. J. Mach. Learn. Res. 13 (2012), 281–305.
[6]James Bergstra, Daniel Yamins, and David D. Cox. 2013. Hyperopt: A Python
Library for Optimizing the Hyperparameters of Machine Learning Algorithms.
InSCIPY’13: Proc. of the 12th Python in Science Conference. 13–20.
[7]James Bergstra, Daniel Yamins, and David D. Cox. 2013. Making a Science of
Model Search: Hyperparameter Optimization in Hundreds of Dimensions for
Vision Architectures. In ICML’13: Proc. of the 30th International Conference on
Machine Learning. 115–123.
[8]Lionel C. Briand, Walcélio L. Melo, and Jürgen Wüst. 2002. Assessing the Ap-
plicability of Fault-Proneness Models Across Object-Oriented Software Projects.
IEEE Trans. Software Eng. 28, 7 (2002), 706–720.
[9]Qimeng Cao, Qing Sun, Qinghua Cao, and Huobin Tan. 2015. Software defect
prediction via transfer learning based neural network. 2015 First International
Conference on Reliability Systems Engineering (ICRSE) (2015), 1–10.
[10] Lin Chen, Bin Fang, Zhaowei Shang, and Yuanyan Tang. 2015. Negative samples
reduction in cross-company software defects prediction. Information & Software
Technology 62 (2015), 67–77.
[11] Shyam R. Chidamber and Chris F. Kemerer. 1994. A Metrics Suite for Object
Oriented Design. IEEE Trans. Software Eng. 20, 6 (1994), 476–493.
[12] Jacob Cohen. 1988. Statistical Power Analysis for the Behavioral Sciences. Rout-
ledge.
[13] Ana Erika Camargo Cruz and Koichiro Ochimizu. 2009. Towards logistic regres-
sion models for predicting fault-prone code across software projects. In ESEM’09:Proc. of the 3rd International Symposium on Empirical Software Engineering and
Measurement. 460–463.
[14] Marco D’Ambros, Michele Lanza, and Romain Robbes. 2010. An extensive com-
parison of bug prediction approaches. In MSR’10: Proc. of the 7th International
Working Conference on Mining Software Repositories. 31–41.
[15] Marco D’Ambros, Michele Lanza, and Romain Robbes. 2010. An extensive com-
parison of bug prediction approaches. In MSR’10: Proc. of the 7th International
Working Conference on Mining Software Repositories (Co-located with ICSE). 31–41.
[16] Fernando Brito e Abreu and Rogério Carapuça. 1994. Candidate metrics for
object-oriented software within a taxonomy framework. Journal of Systems and
Software 26, 1 (1994), 87–96.
[17] Matthias Feurer and Frank Hutter. 2019. Hyperparameter Optimization. In
Automated Machine Learning - Methods, Systems, Challenges. 3–33.
[18] Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. 2012.
A Systematic Literature Review on Fault Prediction Performance in Software
Engineering. IEEE Trans. Software Eng. 38, 6 (2012), 1276–1304.
[19] Ahmed E. Hassan. 2009. Predicting faults using the complexity of code changes.
InICSE’09: Proc. of the 31st International Conference on Software Engineering.
78–88.
[20] Peng He, Bing Li, Xiao Liu, Jun Chen, and Yutao Ma. 2015. An empirical study on
software defect prediction with a simplified metric set. Information & Software
Technology 59 (2015), 170–190.
[21] Zhimin He, Fayola Peters, Tim Menzies, and Ye Yang. 2013. Learning from Open-
Source Projects: An Empirical Study on Defect Prediction. In ESEM’13: Proc. of
2013 ACM / IEEE International Symposium on Empirical Software Engineering and
Measurement. 45–54.
[22] Zhimin He, Fengdi Shu, Ye Yang, Mingshu Li, and Qing Wang. 2012. An investi-
gation on the feasibility of cross-project defect prediction. Autom. Softw. Eng. 19,
2 (2012), 167–199.
[23] Steffen Herbold. 2013. Training data selection for cross-project defect prediction.
InPROMISE’13: Proc. of the 9th International Conference on Predictive Models in
Software Engineering. 6:1–6:10.
[24] Steffen Herbold. 2017. A systematic mapping study on cross-project defect
prediction. CoRR abs/1705.06429 (2017).
[25] Steffen Herbold, Alexander Trautsch, and Jens Grabowski. 2017. Global vs.
local models for cross-project defect prediction - A replication study. Empirical
Software Engineering 22, 4 (2017), 1866–1902.
[26] Steffen Herbold, Alexander Trautsch, and Jens Grabowski. 2018. A Comparative
Study to Benchmark Cross-Project Defect Prediction Approaches. IEEE Trans.
Software Eng. 44, 9 (2018), 811–833.
[27] Seyedrebvar Hosseini, Burak Turhan, and Dimuthu Gunarathna. 2019. A Sys-
tematic Literature Review and Meta-Analysis on Cross Project Defect Prediction.
IEEE Trans. Software Eng. 45, 2 (2019), 111–147.
[28] Seyedrebvar Hosseini, Burak Turhan, and Mika Mäntylä. 2016. Search Based
Training Data Selection For Cross Project Defect Prediction. In PROMISE’16: Proc.
of the The 12th International Conference on Predictive Models and Data Analytics
in Software Engineering. 3:1–3:10.
[29] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). 2019. Automated
Machine Learning - Methods, Systems, Challenges. Springer.
[30] Marian Jureczko and Lech Madeyski. 2010. Towards identifying software project
clusters with regard to defect prediction. In PROMISE’10: Proc. of the 6th Interna-
tional Conference on Predictive Models in Software Engineering. 9.
[31] Kazuya Kawata, Sousuke Amasaki, and Tomoyuki Yokogawa. 2015. Improving
Relevancy Filter Methods for Cross-Project Defect Prediction. In ACIT-CSI’15:
Proc. of the 3rd International Conference on Applied Computing and Information
Technology/2nd International Conference on Computational Science and Intelligence .
2–7.
[32] Taek Lee, Jaechang Nam, DongGyun Han, Sunghun Kim, and Hoh Peter In. 2011.
Micro interaction metrics for defect prediction. In ESEC/FSE: Proc. of the 19th
ACM SIGSOFT Symposium on the Foundations of Software Engineering and the
13th European Software Engineering Conference. 311–321.
[33] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking Classification Models for Software Defect Prediction: A Proposed
Framework and Novel Findings. IEEE Trans. Software Eng. 34, 4 (2008), 485–496.
[34] Zhiqiang Li, Xiao-Yuan Jing, and Xiaoke Zhu. 2018. Progress on approaches to
software defect prediction. IET Software 12, 3 (2018), 161–175.
[35] Charles X. Ling, Jin Huang, and Harry Zhang. 2003. AUC: a Statistically Consis-
tent and more Discriminating Measure than Accuracy. In IJCAI’03: Proc. of the
8th International Joint Conference on Artificial Intelligence. 519–526.
[36] Ying Ma, Guangchun Luo, Xue Zeng, and Aiguo Chen. 2012. Transfer learning
for cross-company software defect prediction. Information & Software Technology
54, 3 (2012), 248–256.
[37] Thomas J. McCabe. 1976. A Complexity Measure. IEEE Trans. Software Eng. 2, 4
(1976), 308–320.
[38] Thilo Mende. 2010. Replication of defect prediction studies: problems, pitfalls
and recommendations. In PROMISE’10: Proc. of the 6th International Conference
on Predictive Models in Software Engineering. 5.
576[39] Thilo Mende and Rainer Koschke. 2009. Revisiting the evaluation of defect
prediction models. In PROMISE’09: Proc. of the 5th International Workshop on
Predictive Models in Software Engineering. 7.
[40] Andrew Meneely, Laurie Williams, Will Snipes, and Jason A. Osborne. 2008.
Predicting failures with developer networks and social network analysis. In
FSE’08: Proc. of the 16th ACM SIGSOFT International Symposium on Foundations
of Software Engineering. 13–23.
[41] Tim Menzies, Jeremy Greenwald, and Art Frank. 2007. Data Mining Static Code
Attributes to Learn Defect Predictors. IEEE Trans. Software Eng. 33, 1 (2007),
2–13.
[42] Tim Menzies and Martin J. Shepperd. 2012. Special issue on repeatable results in
software engineering prediction. Empirical Software Engineering 17, 1-2 (2012),
1–17.
[43] Nikolaos Mittas and Lefteris Angelis. 2013. Ranking and Clustering Software
Cost Estimation Models through a Multiple Comparisons Algorithm. IEEE Trans.
Software Eng. 39, 4 (2013), 537–551.
[44] Raimund Moser, Witold Pedrycz, and Giancarlo Succi. 2008. A comparative
analysis of the efficiency of change metrics and static code attributes for defect
prediction. In ICSE’08: Proc. of the 30th International Conference on Software
Engineering. 181–190.
[45] Nachiappan Nagappan and Thomas Ball. 2005. Use of relative code churn mea-
sures to predict system defect density. In ICSE’05: Proc. of the 27th International
Conference on Software Engineering. 284–292.
[46] Jaechang Nam and Sunghun Kim. 2015. CLAMI: Defect Prediction on Unlabeled
Datasets (T). In ASE’15: Proc. of the 30th IEEE/ACM International Conference on
Automated Software Engineering. 452–463.
[47] Jaechang Nam, Sinno Jialin Pan, and Sunghun Kim. 2013. Transfer defect learning.
InICSE’13: Proc. of the 35th International Conference on Software Engineering. 382–
391.
[48] Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE
Trans. Knowl. Data Eng. 22, 10 (2010), 1345–1359.
[49] Annibale Panichella, Rocco Oliveto, and Andrea De Lucia. 2014. Cross-project
defect prediction models: L’Union fait la force. In CSMR-WCRE’14: Proc. of 2014
Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering,
and Reverse Engineering. 164–173.
[50] Fayola Peters, Tim Menzies, and Andrian Marcus. 2013. Better cross company
defect prediction. In MSR’13: Proc. of the 10th Working Conference on Mining
Software Repositories. 409–418.
[51] Foyzur Rahman, Daryl Posnett, Abram Hindle, Earl T. Barr, and Premkumar T.
Devanbu. 2011. BugCache for inspections: hit or miss?. In ESEC/FSE’11: Proc. of
the 19th ACM SIGSOFT Symposium on the Foundations of Software Engineering
and the 13th European Software Engineering Conference. 322–331.
[52] Duksan Ryu, Okjoo Choi, and Jongmoon Baik. 2016. Value-cognitive boosting
with a support vector machine for cross-project defect prediction. Empirical
Software Engineering 21, 1 (2016), 43–71.
[53] Duksan Ryu, J.-I Jang, and Jongmoon Baik. 2015. A Hybrid Instance Selection
Using Nearest-Neighbor for Cross-Project Defect Prediction. Journal of Computer
Science and Technology 30 (09 2015), 969–980.
[54] Shlomo Sawilowsky. 2009. New effect size rules of thumb. Journal of Modern
Applied Statistical Methods 8 (2009), 467–474. Issue 2.
[55] Martin J. Shepperd, Qinbao Song, Zhongbin Sun, and Carolyn Mair. 2013. Data
Quality: Some Comments on the NASA Software Defect Datasets. IEEE Trans.Software Eng. 39, 9 (2013), 1208–1215.
[56] Seyyed Ehsan Salamati Taba, Foutse Khomh, Ying Zou, Ahmed E. Hassan, and
Meiyappan Nagappan. 2013. Predicting Bugs Using Antipatterns. In ICSM’13:
Proc. of 2013 IEEE International Conference on Software Maintenance. 270–279.
[57] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto. 2016. Automated parameter optimization of classification techniques
for defect prediction models. In ICSE’16: Proc. of the 38th International Conference
on Software Engineering. 321–332.
[58] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, and Kenichi
Matsumoto. 2019. The Impact of Automated Parameter Optimization on Defect
Prediction Models. IEEE Trans. Software Eng. 45, 7 (2019), 683–711.
[59] Haonan Tong, Bin Liu, Shihai Wang, and Qiuying Li. 2019. Transfer-Learning
Oriented Class Imbalance Learning for Cross-Project Defect Prediction. CoRR
abs/1901.08429 (2019).
[60] Burak Turhan, Tim Menzies, Ayse Basar Bener, and Justin S. Di Stefano. 2009.
On the relative value of cross-company and within-company data for defect
prediction. Empirical Software Engineering 14, 5 (2009), 540–578.
[61] Satoshi Uchigaki, Shinji Uchida, Koji Toda, and Akito Monden. 2012. An Ensemble
Approach of Simple Regression Models to Cross-Project Fault Prediction. In
SNPD’12: Proc. of the 13th ACIS International Conference on Software Engineering,
Artificial Intelligence, Networking and Parallel/Distributed Computing. 476–481.
[62] Shinya Watanabe, Haruhiko Kaiya, and Kenji Kaijiri. 2008. Adapting a Fault
Prediction Model to Allow Inter Languagereuse. In PROMISE’08: Proc. of the 4th
International Workshop on Predictor Models in Software Engineering. 19–24.
[63] David H. Wolpert and William G. Macready. 1997. No free lunch theorems for
optimization. IEEE Trans. Evolutionary Computation 1, 1 (1997), 67–82.
[64] Rongxin Wu, Hongyu Zhang, Sunghun Kim, and Shing-Chi Cheung. 2011. ReLink:
recovering links between bugs and changes. In ESEC/FSE’11: Proc. of the 19th
ACM SIGSOFT Symposium on the Foundations of Software Engineering and the
13th European Software Engineering Conference. 15–25.
[65] Xin Xia, David Lo, Sinno Jialin Pan, Nachiappan Nagappan, and Xinyu Wang. 2016.
HYDRA: Massively Compositional Model for Cross-Project Defect Prediction.
IEEE Trans. Software Eng. 42, 10 (2016), 977–998.
[66] Feng Zhang, Audris Mockus, Iman Keivanloo, and Ying Zou. 2016. Towards
building a universal defect prediction model with rank transformed predictors.
Empirical Software Engineering 21, 5 (2016), 2107–2145.
[67] Yun Zhang, David Lo, Xin Xia, and Jianling Sun. 2015. An Empirical Study of
Classifier Combination for Cross-Project Defect Prediction. In COMPSAC’15: Proc.
of the 39th IEEE Annual Computer Software and Applications Conference. 264–269.
[68] Yuming Zhou, Yibiao Yang, Hongmin Lu, Lin Chen, Yanhui Li, Yangyang Zhao,
Junyan Qian, and Baowen Xu. 2018. How Far We Have Progressed in the Journey?
An Examination of Cross-Project Defect Prediction. ACM Trans. Softw. Eng.
Methodol. 27, 1 (2018), 1:1–1:51.
[69] Thomas Zimmermann and Nachiappan Nagappan. 2008. Predicting defects using
network analysis on dependency graphs. In ICSE’08: Proc. of the 30th International
Conference on Software Engineering. 531–540.
[70] Thomas Zimmermann, Nachiappan Nagappan, Harald C. Gall, Emanuel Giger,
and Brendan Murphy. 2009. Cross-project defect prediction: a large scale ex-
periment on data vs. domain vs. process. In ESEC/FSE’09: Proc. of the 7th joint
meeting of the European Software Engineering Conference and the ACM SIGSOFT
International Symposium on Foundations of Software Engineering. 91–100.
577