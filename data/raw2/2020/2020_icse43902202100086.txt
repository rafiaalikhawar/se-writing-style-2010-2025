MuDelta: Delta-Oriented Mutation Testing at
Commit Time
Wei Ma, Thierry Titcheu Chekam, Mike Papadakisand Mark Harmany
SnT, University of Luxembourg, Luxembourg
yFacebook and University College London, UK
fï¬rstname.surnameg@uni.lu,ymark.harman@ucl.ac.uk
Abstract â€”To effectively test program changes using mutation
testing, one needs to use mutants that are relevant to the altered
program behaviours. We introduce MuDelta , an approach that
identiï¬es commit-relevant mutants; mutants that affect and are
affected by the changed program behaviours. Our approach uses
machine learning applied on a combined scheme of graph and
vector-based representations of static code features. Our results,
from 50 commits in 21 Coreutils programs, demonstrate a strong
prediction ability of our approach; yielding 0.80 (ROC) and 0.50
(PR-Curve) AUC values with 0.63 and 0.32 precision and recall
values. These predictions are signiï¬cantly higher than random
guesses, 0.20 (PR-Curve) AUC, 0.21 and 0.21 precision and recall,
and subsequently lead to strong relevant tests that kill 45%
more relevant mutants than randomly sampled mutants (either
sampled from those residing on the changed component(s) or
from the changed lines). Our results also show that MuDelta
selects mutants with 27% higher fault revealing ability in fault
introducing commits. Taken together, our results corroborate the
conclusion that commit-based mutation testing is suitable and
promising for evolving software.
Index Terms â€”mutation testing, commit-relevant mutants, con-
tinuous integration, regression testing, machine learning
I. I NTRODUCTION
Mutation testing has been shown to be one of the strongest
fault-revealing software test adequacy criteria available to
software testers [1]. Nevertheless, although mutation testing
has been widely studied for over four decades in the scientiï¬c
literature, the formulation that underpins it has remained
largely unchanged since its inception in the 1970s [2], [3].
In this unchanged formulation, a program pis tested by a
test suite,T, the adequacy of which is measured in terms of
its ability to distinguish executions of pand a set of mutants
M. Each mutant in Mis a version of pinto which a fault
has been deliberately inserted, in order to simulate potential
real faults, thereby assessing the ability of the test suite Tto
detect such faults.
The problem with this formulation is that it has not kept
pace with recent software engineering practices. Most notably,
the assumption of a ï¬xed program p, set of mutants M,
and test suite T, is unrealistic; modern software systems
undergo regular change, typically in continuous integration
environments [4]â€“[6]. In order to render mutation testing
applicable to practising software engineers, a fundamentally
new approach to ï¬nding suitable mutants is required in which
p,T, andMare each continually evolving.Speciï¬cally, we need a mutation testing formulation in
which mutants can be found, on the ï¬‚y, based on their rel-
evance to speciï¬c changes to the system under consideration.
In this â€˜evolving mutation testingâ€™ approach, both the set of
mutantsMand the tests that distinguished their behaviours
T, are each able to change with each new commit. Such
a mutation testing formulation is better suited to industrial
practice, e.g., at Google [7], since mutation testing can be
applied at commit time, to each code change as it is submitted,
thereby keeping pace with the changes to p. More importantly,
such an approach will focus the test effort deployed at commit
time speciï¬cally to the changes in the commit, rather than
wasting test effort on re-testing old code.
In order to apply mutation testing on the ï¬‚y in this manner,
we need a fast lightweight approach to determine a priority
ordering on a given set of mutants, where priority is deter-
mined by the relevance of a mutant to the change in hand.
This paper introduces a machine learning-based approach
to tackle this problem using a combined scheme of graph
and vector-based representations of simple code features that
aim at capturing the information (control and data) ï¬‚ow and
interactions between mutants and committed code changes.
We train the learner on a set of mutants from historical code
changes that are labeled with respect to given test suites. The
machine learner is subsequently used to predict the priority
ordering of the set of mutants to identify those most likely to
be relevant to a given change.
This way, once the learner has been trained, it can be used to
quickly predict the priority order for the set of mutants in terms
of their relevance to unseen changes, as they are submitted
into the continuous integration system for review. This allows
the tester (and/or some automated test design technology) to
focus on those mutants that are most likely to yield tests that
are fault revealing for the change in hand.
We implemented our approach in a system called MuDelta ,
and evaluated it on a set of 50 commits from Coreutils wrt
a) prediction ability, b) ability to lead to relevant tests (tests
killing commit-relevant mutants) and c) ability to reveal faults
in fault introducing commits. Our results indicate s strong
prediction ability; MuDelta yields 0.80 ROC-AUC value, 0.42
F1-score, 0.63 precision and 0.32 recall, while random guesses
yield 0.20 F1-score, 0.21 precision and 0.21 recall. Killing the
predicted mutants results in killing 45% more relevant mutants
than random mutant sampling baselines.
8972021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
1558-1225/21/$31.00 Â©2021 IEEE
DOI 10.1109/ICSE43902.2021.00086
Perhaps more importantly, our results show that our ap-
proach leads to mutants with 27% higher fault revealing
ability in fault introducing commits. Taken together, our results
corroborate the ï¬ndings that MuDelta enables effective delta-
relevant mutation testing, i.e., mutation testing targeting the
speciï¬c code changes of the software system under test.
Our study also reveals some surprising ï¬ndings, additional
results and discussion can be found in our project page1.
For example, one might believe that mutants that reside in
the changed code would be adequate in testing it. However,
our empirical ï¬ndings contradict this natural, but incorrect,
assumption. This surprising ï¬nding highlights the importance
of ï¬nding mutants in the unchanged part of the program.
This unchanged code that forms a contextual environment
into which changes deployed. Such -relevant mutants in the
contextC, for some change, , tend to focus on (and reveal
issues with) interactions between the change, , and the context
Cinto which it is deployed. Developers are less likely to
notice these since they are more likely to be familiar with their
changes than the existing unchanged code. Such bugs may also
be more subtle as they involve unforeseen interactions between
parts of the system.
In summary, our primary contributions are:
The empirical evidence that mutant relevance (to partic-
ular program changes) can be captured by simple static
source code metrics.
A machine learning approach, called MuDelta , that learns
to rank mutants wrt to their utility and relevance to
speciï¬c code changes.
Empirical evidence suggesting that MuDelta outperforms
the traditionally random mutant selection/prioritization
method by revealing 45% more relevant mutants, and
achieving 27% higher probability to reveal faults in these
changes.
II. C ONTEXT
A. Mutation testing
Mutation testing [2] measures the fault revealing potential
of test cases by checking the extend to which artiï¬cially
seeded faults, called mutants, are triggered. When a behaviour
difference between the original program and a mutant is
detected, the mutant is considered to be â€œkilledâ€, otherwise the
mutant is considered to be â€œliveâ€. The point here is that mutant
killing shows an execution failure that was covered, triggered
by the altered code and propagated to the observable program
output, signifying the potential of the test. The faults are
seeded in the code under analysis by making simple syntactic
transformations, e.g., replacing the instance of an operator with
another one, if(a<<<b)intoif(ab), and they
represent the test requirements. This means that the ratio of
mutants killed, called Mutation Score (MS), represents the test
thoroughness metric.
1https://rml464.github.io/mutantlearning/Injecting faults by altering the syntax of the program may
result in semantically equivalent program versions, i.e., ver-
sions that behave the same way for all possible inputs. These
equivalent versions need to be removed and not taken into
consideration, as even a perfect test suite cannot kill them.
Unfortunately, equivalent mutants form one of the known
problems of mutation testing [8].
Interestingly, many killable mutants are equivalent to others,
introducing an additional problem, skew in the Mutation Score
[9], [10]. The problem though, is more severe since not
all mutants are equally important; many mutants are killed
collaterally, and thus, they do not contribute to the testing
process [11], [12]. Unfortunately, these collateral kills inï¬‚ate
the mutation score measurement and may lead to wrong
conclusions [10]. Therefore, the recent mutation testing liter-
ature [8], [10] suggest using the so-called subsuming mutants
(computing the subsuming mutation score [12], [13]) when
evaluating test effectiveness.
B. Change-aware regression testing
Testing program regressions require test suites to exercise
the adequacy of testing wrt to the program changes. In case
the used test suites are insufï¬cient, guidance should be given
in order to help developers create test cases that speciï¬cally
target the behaviour deviations introduced by the regressions.
One potential solution to this problem may be based on
coverage; one can aim at testing the altered parts of the pro-
grams using coverage information. However, the strengths of
coverage are known to be limited [1], [14]. Moreover, the most
severe regression issues are due to unforeseen interactions
between the changed code and the rest of the program [14],
[15]. Therefore, we aim at using mutation testing using the
so-called Commit-relevant mutants [16].
C. Commit-relevant mutants
Commit-relevant mutants are those that make observable
any interaction between the altered code and the rest of the
program under test. These mutants alter the program semantics
that are relevant to the committed changes, i.e., they have
behavioural effects on the altered code behaviour. This means
that mutants are relevant to a commit when their behaviour is
changed by the regression changes. Indeed, changed behaviour
indicates a coupling between mutants and regressions, sug-
gesting relevance. In essence, one can use relevant mutants to
capture the â€˜observableâ€™ dependencies between changed and
unchanged code [17], [18], which reï¬‚ect the extent to which
test suites are testing the altered program behaviours.
In particular, mutants interact with program changes when
the post-commit mutant version (includes both the changes and
the mutant) behaves differently from a)the related pre-commit
mutant version and b)the post-commit non-mutated version.
These conditions establish that changes and mutants interact
[16]. Condition a)establishes that the behaviour differences
are caused by the presence and absence of the committed
changes and condition b)that the behaviour differences are
caused by the presence and absence of the mutants.
898Fig. 1. Overview of MuDelta. The learner is trained on a set of mutants from historical code changes that are labeled with respect to given test suites. The
machine learner is subsequently used to predict the priority ordering of the set of mutants to identify those most likely to be relevant to a given change.
The virtue of commit-relevant mutation testing, as described
in the study of Ma et al. [16] is the best-effort application of
mutation testing. This gives the potential for improved fault
revelation under the same (relatively low) user effort than
using randomly sampled mutants, i.e., traditional mutation
testing. However, in order to be useful, these mutants need
to be identiï¬ed in advance, prior to any mutant analysis
performed. This is because relevant mutants form the ob-
jectives that developers will analyse. To achieve this, we
develop a machine learning approach, which we describe in
the following section.
Figure 2 presents a commit-relevant mutant on a fault-
introducing commit of GNU Coreutils2. This is the com-
mit with ID 8from CoREBench [19]. The commit affects
two functions of the program seq(main and seqfast). The
entry-point is the function main, which, calls the functions
print numbers andseqfastto compute and print the results.
The function seqfastis an optimized implementation of the
function print numbers, used only when the inputs meet spe-
ciï¬c conditions. In Figure 2, the line 543checks the condition
to call seqfast. If the condition is satisï¬ed, seqfastis called.
Otherwise, print numbers is called. Note that print numbers
may be called after seqfastif the later fails (the condition at
line405is not satisï¬ed, i.e. a>b ). In that case, the execution
ofseqfastdoes not alter the program state or output.
The commit aims at relaxing the condition that guards the
call to function seqfast. In the pre-commit version, seqfast
is not called when the user speciï¬es a separator. However, in
the post-commit version, seqfast is called whenever a) the
user speciï¬es a separator, and b) the separator string has a
single character.
In the function seqfast, the commit only replaces the hard
coded separator (â€˜nnâ€™) with separatorâ€™s global string variable.
In the function main, the commit relaxes the â€œifâ€ condition
at line 543, in a way that seqfast is also called when the
user speciï¬es a separator, which can be any single â€8-bitsâ€
character (it is not limited to â€˜nnâ€™).
2https://www.gnu.org/software/coreutilsThe program seqcalls seqfastto print all the integers from
the ï¬rst parameter ato the second parameter b, and using a
given character (ï¬rst character of separator in post-commit
and â€˜nnâ€™ in pre-commit) to separate the printed numbers.
Let four mutants such that: mutant M1deletes the statement
at line 414, which prints the ï¬rst number using puts. Mutant
M2deletes the modiï¬ed statement at line 420, which add the
separator to the buffer to print. Mutant M3swaps the operands
of the last â€&&â€ operation at the modiï¬ed line 543. Mutant
M4replaces the exit value at line 595by 1.
We observe that M4is not relevant to the commit. In fact,
there is no test that can kill M4in the post-commit version,
and create an output difference between pre- and post-commit
versions ofM4. If a test kills post-commit M4, it must avoid
executing line 547, thus, seqfastis either not called or its call
does not succeed (does not print anything). Thus, the output of
the execution of the pre- and post-commit versions of M4with
such test will be same (both computed with print numbers,
which is not altered by the commit). Mutant M3is equivalent,
because no clause has side effect that is controlled by another
clause in the ifcondition.
However,M1is relevant to the commit. An execution of the
test â€œseq -s, 1 2â€, which sets the separator to the comma (â€˜,â€™),
outputs â€œ1,2nnâ€ in pre-commit M1(print numbers is called),
â€œ2,â€ in post-commit M1(â€˜puts(p)â€™ is deleted and seqfast
is called), and â€œ1nn2,â€ in the post-commit original version
(the ï¬rst number is printed using â€˜puts(p)â€™, which appends an
â€˜nnâ€™). Similarly, M2is relevant to the commit. The execution
of same test â€œseq -s, 1 2â€ outputs â€œ1,2nnâ€ in pre-commit
M2(print numbers is called), â€œ1nn2â€ in post-commit M1(no
comma separator printed and seqfastis called).
Moreover, a fault introduced by the commit makes the
program use â€˜nnâ€™ instead of the user speciï¬ed separator,
after printing the ï¬rst number, when the user separator is a
single character other than â€˜nnâ€™. This happens because in such
scenario, the program calls seqfast, which calls â€˜puts(p)â€™ (line
414) to print the ï¬rst number. This automatically add an extra
â€˜nnâ€™ and do not use the speciï¬ed separator.
899static bool seq_fast (char const *a, char const *b) {
â€¦
bool ok = cmp (p, p_len , q, q_len ) <= 0;
if(ok) {
â€¦
puts (p); // Mutant M1: delete Statement
â€¦
incr (&p, &p_len );
z = mempcpy (z, p, p_len );
- *z++ = â€˜ \nâ€™;
+         *z++ = *separator; 
if(buf_end -n -1 < z ) {
fwrite (buf, z -buf, 1, stdout);
z = buf;
}
â€¦ 
}
â€¦
return ok;
}
intmain (intargc, char **argv ) {
â€¦
-if(â€¦ && all_digits_p (argv[1]) & â€¦) {
+  if(all_digits_p (argv[optind ]) && â€¦  && strlen (separator) == 1) {
â€¦
if(seq_fast (s1, s2))
exit(EXIT_SUCCESS);
}
â€¦
print_numbers (format_str, layout, first.value , â€¦);
exit(EXIT_SUCCESS);  // Mutant M4: EXIT_SUCCESS ïƒ -1
}390
404
405
414418
419420420421423424425
433437
438
450543
543546
547550
592
594595
ï± Mutants M1 and M2 are relevant. Moreover,  they are 99% fault 
revealing (99% of the tests killing them find the introduced fault).
ï± Mutants M3  and M4 are not relevant (M3 is equivalent).Mutant M2: delete 
statement
Mutant M3:  
Swap operands of â€œ&&â€.  Fig. 2. Mutation testing in a fault introducing commit. The fault is triggered
by the call to â€˜puts(p)â€™, which automatically uses â€˜ nnâ€™ as the ï¬rst separator,
resulting in not using the user speciï¬ed separator when this is a single
character other than â€˜ nnâ€™. This makes every test executing seqfast with a
separator other than â€˜ nnâ€™ to reveal the fault. Killing M1orM2can result in
such tests, while killing M4does not (to kill M4a test must avoid executing
line547, which means that seqfastis either not called or its call does not print
anything, hence not making any observable difference). M3is equivalent.
Every test that executes seqfast, with a separator other than
â€˜nnâ€™ reveal the fault. These are (1 2
257)99:2%of all the
tests that successfully execute seqfast. The reason is that the
separator is either not set in the test (defaults to â€˜ nnâ€™), or set to
one of the 256â€˜8-bitsâ€™ characters (including â€˜ nnâ€™). We observe
that all tests that successfully execute seqfastkillM1andM2.
Therefore, 99:2%of the tests that kill M1andM2reveal the
fault.
III. A PPROACH
We aim at testing commits using commit-relevant mutants;
the subset of mutants on the post-commit program version that
has a behaviour relevance to the committed changes [16].
We develop MuDelta , a technique that learns to rank mu-
tants according to their commit-relevance potential (likelihood
to be commit-relevant). Initially, MuDelta applies supervised
learning on a mutant corpus from past data, and builds a
prediction model. This model is then applied to predict the
mutants that should be used to test the future commits of the
program under test. This means that at commit time, testers
can use and focus only on the most relevant mutants. This
process is depicted in Figure 1.A.MuDelta Feature Engineering
The mutant selection process in MuDelta is based on
training of a predictor that is capable of identifying whether
a mutant is commit-relevant with a certain conï¬dence (prob-
ability). Consequently, we design a set of features to reï¬‚ect
speciï¬c code properties which may discriminate a commit-
relevant mutant from another.
The study of Chekam et al. [20] found that fault revealing
and killability mutant characteristics can be captured by simple
code features. Therefore, we consider the features that they
proposed in our machine learning model. Unfortunately, these
features do not capture the interaction between mutants and the
altered code. Hence, we design additional features capable of
capturing the link between the mutant and the altered code
(by the commit). These features also aim at capturing the
characteristics of the altered code.
In the following subsections we describe the features
we use in order to train a classiï¬er. We consider a com-
mit modiï¬cation Cassociated with code statements SC=
fSC1;SC2;:::;S Cng, and letBC =fBC1;BC2;:::;B Ckg
the control-ï¬‚ow graph (CFG) basic blocks associated to the
statementsSC. Let us also consider a mutant Massociated to
a code statement SMon which the mutation was applied. Let
BMbe the CFG basic block associated to a mutated statement
SMcontaining the mutated expression EM.
B. Contextual Features
In order to capture contextual information for each program
statement, within a program version, we design features that
leverage graph analysis technologies. We construct graph
representations of the program, where the nodes are the
statements of the program, and the edges are various types of
relationships between statements. We consider the following
four relationships (edge types): data dependency (direct data
dependency, indirect data dependency) [21], control depen-
dency, and control ï¬‚ow. Direct data dependency refers to vari-
able value dependency, while indirect data dependency refers
to pointer dereference value dependency (the data is accessed
through dereferencing a pointer). In total we use the following
6 different graph representations, i.e., 1)Utility Graph (UG)
that includes all four edge types we discussed, 2)Dependency
Graph (DG) that includes all three dependency edges types,
3)Direct Data Dependency Graph (DDDG) that includes
only the direct data dependency edge type, 4)Indirect Data
Dependency Graph (IDDG) that includes only the indirect data
dependency edge type, 5)Control Dependency Graph (CDG)
that includes only the control dependency edge type, and 6)
Control Flow Graph (CFG) which includes only the control
ï¬‚ow edge type.
For each graph, we leverage graph analysis algorithms to
compute a score for each node. We consider the following
graph analysis algorithms: Rich-Club coefï¬cient (RCC) [22],
[23], Clustering coefï¬cient (CC) [24]â€“[26], Square Clustering
coefï¬cient (SCC) [27], PageRank (PR) [28], and Hits Analysis
(HA) [29].
900Complexity: Complexity of SM, approximated by the number of mutants on SM.
CfgDepth: Depth of BMaccording to CFG.
CfgPredNum: Number of predecessor basic blocks, in CFG, of BM.
CfgSuccNum: Number of successors basic blocks, in CFG, of BM.
AstNumParents: Number of AST parents of EM.
NumOutDataDeps: Number of mutants on expressions data-dependent on EM.
NumInDataDeps: Number of mutants on expressions that EMis data-dependent.
NumOutCtrlDeps: Number of mutants on statements control-dependents on EM.
NumInCtrlDeps: Number of mutants on expressions that EMis control-dependent
NumTieDeps: Number of mutants on EM.
AstParentsNumOutDataDeps: Number of mutants on expressions data-dependent
onEMâ€™s AST parent statement.
AstParentsNumInDataDeps: Number of mutants on expressions that EMâ€™s AST
parent expression is data-dependent.
AstParentsNumOutCtrlDeps: Number of mutants on statements control-
dependent on EMâ€™s AST parent expression.
AstParentsNumInCtrlDeps: Number of mutants on expressions that EMâ€™s AST
parent expression is control-dependent.
AstParentsNumTieDeps: Number of mutants on EMâ€™s AST parent expression.
TypeAstParent: Expression type of AST parent expressions of EM.
TypeMutant: Mutant type of M, transformation rule. E.g., a+b!a b.
AstChildHasIdentiï¬er: AST child of expression EMhas an identiï¬er.
AstChildHasLiteral: AST child of expression EMhas a literal.
AstChildHasOperator: AST child of expression EMhas an operator.
OutDataDepNumStmtBB: Number of CFG basic blocks containing an expression
data-dependent on SM.
InDataDepNumStmtBB: Number of CFG basic blocks containing an expression
on which SMis data-dependent.
OutCtrlDepNumStmtBB: Number of CFG basic blocks containing an expression
control-dependent on SM.
InCtrlDepNumStmtBB: Number of CFG basic blocks containing an expression on
whichSMis control-dependent.
AstParentMutantTypeNum: Number of each mutant type of EMâ€™s AST parents.
OutDataDepMutantTypeNum: Number of each mutant type on expressions data-
dependents on EM.
InDataDepMutantTypeNum: Number of each mutant type on expressions on which
EMis data-dependent.
OutCtrlDepMutantTypeNum: Number of each mutant type on statements control-
dependents on EM.
InCtrlDepMutantTypeNum: Number of each mutant type on expressions on which
EMis control-dependent.
Fig. 3. Mutant utility features
.
Overall, we get a set of features FS, for each statement S
and for each graph G, by computing the score of the node
corresponding to S, using all graph analysis algorithms on G.
This gives us 6 * 5 (graphs * Metrics) features per program
statement.
C. Mutant utility features
We used the features proposed by Chekam et al. [20]. These
features relate to the complexity of the mutated statement SM,
the position of SMin the control-ï¬‚ow graph, the dependencies
with other mutants, and the nature of the code block BM
whereSMis located. The selected features are recorded in
Figure 3. Note that for this study, we added the last 9 features
(marked in the ï¬gure with italic), and the contextual features
ofSM(Section III-B). The ï¬rst 4 features (with italic) are
similar to the features NumOutDataDeps, NumInDataDeps,
NumOutCtrlDeps, NumInCtrlDeps used by Chekam et al. [20],
but, instead of the number of mutants, they count the number
of basic blocks.
D. Mutant-Modiï¬cation Interaction Features
To capture the interaction between mutant and altered code,
we use features related to the information ï¬‚ow that the altered
codeCincur to the execution of mutant M. In this regard, we
propose features that characterize the altered code and features
that capture the information ï¬‚ow between CandM.NumConditional: Number of conditional statements in the modiï¬cation.
NumHunks: Number of hunks (blocks) in the commit diff.
HasExit: The modiï¬cation involves program termination commands.
ChangesCondition : The modiï¬cation involves the condition of an ifor a loop.
InvolesOutput: The modiï¬cation involves a function call to printf orerror .
IsRefactoring: The modiï¬cation only does code refactoring.
NumUPDATE: Number of UPDATE operations from GumTree tool [30].
NumINSERT : Number of INERT operations from GumTree tool [30].
NumMOVE: Number of MOVE operations from GumTree tool [30].
NumDELETE: Number of DELETE operations from GumTree tool [30].
NumActionClusters: Number of action clusters from GumTree tool [30].
NumActions: Number of actions from GumTree tool [30].
Fig. 4. Mutant-Modiï¬cation Interaction Features
1) Modiï¬cation Characteristics Features: We have features
extracted from the commit diff and features extracted from
the changed or added statements in the post-commit version
of the program. Figure 4 describes the features extracted from
the commit diff. The features extracted from the changed or
added statements are: (a) The mean of the depth, according
to CFG, of the basic blocks in BC (modiï¬cationCfgDepth ).
(b) The mean of the complexity of the statements in SC
(modiï¬cationComplexity ). (c) The contextual features (see
Section III-B) of the added or changed statements in the
program. When the modiï¬cation involves multiple statements,
the mean of each feature value for all statements is computed.
2) Information-ï¬‚ow Features: The ï¬rst feature that we use,
in this category, is a Boolean variable ( MutantOnModiï¬cation )
that represents whether the mutant Mmutates an altered code
(SM2SC). Additionally, we consider the 6 graphs presented
in section III-B, and compute, for each graph, the set of
shortest paths between SMandSC.
For every set of paths, we compute the size ( NumPaths ), the
maximum path length ( MaxPathLen ), minimum path length
(MinPathLen ) and mean path length ( MeanPathLen ). Our
features are thus, the combination of each one of these metrics
on every shortest path set.
E. Implementation
We implemented MuDelta in Python. For learning, we used
stochastic gradient boosting [31] (decision trees), which has
been found to work well in the context of mutation [20]. We
used the XGBoost [32] framework and set the number of trees
to 3,000 with a maximum trees depth to 10. We adopt early
stopping during training to avoid over-ï¬tting.
MuDelta uses both numerical or categorical features. The
categorical features are: TypeAstParent, TypeMutant . In order
to use the feature values with XGBoost, we pre-process them
using a normalization of numerical and an encoding of cate-
gorical features. We normalize numerical features, between 0
and1using Rescaling (also known as min-max normalization).
We use binary encoding (binary encoding helps to keep a
reasonably low feature dimension, when comparing to one-
hot-encoding) for the categorical features. We also use Net-
workX3in the graph representation in order to extract the
contextual features that were described in section III-B.
3https://networkx.github.io/
901IV. R ESEARCH QUESTIONS
We start our analysis by investigating the prediction ability
of our machine learning method. Thus, our ï¬rst research
question can be stated as:
RQ1 (Prediction performance): How well does MuDelta
predict commit relevant mutants?
To answer this question we collect a set of commits from
the subject programs where we apply mutation testing and
identify relevant mutants. Then, we split the commits into
training/validation (80% of the commits) and test sets (20%
of the commits) based on the timeline of the project(older
commits are used for training and newer for commits are used
for evaluation), and perform our experiment.
After checking the performance of the predictions, we
turn our attention to the primary problem of interest; mutant
ranking. We investigate the extent to which our predictions
can lead to strong and relevant tests (by using the predictive
mutants as test objectives) in contrast to baseline mutants,
i.e., randomly sampled mutants among those residing in the
changed components ( Random ) or among those residing on
the altered lines ( Modiï¬cation ). Hence we ask:
RQ2 (Test assessment): How MuDelta compare with the
baseline mutant sets with respect to killing commit-
relevant mutants?
We answer this question following a simulation of a testing
scenario where a tester analyse mutants in order to generate
tests [12], [33]. We are interested in the relative differences
between the subsumming relevant mutation score, denoted as
rMS, when test generation is guided by the predicted or the
baseline mutants. We use the subsumming relevant mutation
score to avoid bias from trivial/redundant mutants [10]. We
also use the random mutant selection baseline since it performs
comparably to the state-of-the-art [12], [20], [34]. We compare
with random on a best effort basis, i.e., the rMSachieved
by putting the same level of effort, measured by the number of
mutants that require analysis. Such a simulation is typical in
mutation testing literature [9], [12] and aims at quantifying the
beneï¬t of one method over the other. To further show the need
for mutant selection out of the changed code, we also compute
the extend to which mutants on modiï¬cation are sufï¬cient in
killing commit-relevant mutants.
Answering the above question provides evidence that using
our approach yields signiï¬cant advantages over the baselines.
While this is important and demonstrates the potential of our
approach, still the question of actual test effectiveness (actual
fault revelation) remains. This means that it remains unclear
what the fault revelation potential of our approach when the
commit is fault-introducing. Therefore, we seek to investigate:
RQ3 (Fault Revelation): How MuDelta compare with
the baseline mutant sets with respect to (commit-
introduced) fault revelation?
To answer this question, we investigate the fault revelation
potential of the mutant selection techniques based on a set of
real fault-introducing commits. We follow the same procedure
as in the previous research questions.TABLE I
TESTSUBJECTS
Benchmark #Programs #Commits #Mutants #Relevant #Tests
CoREBench 6 13 154,396 21,597 8,828
Benchmark-1 17 37 412,060 65,982 14,785
V. E XPERIMENTAL SETUP
A. Benchmarks Used
We selected C programs from the GNU Coreutils4, a col-
lection of text, ï¬le and shell utility programs widely used in
software testing research [19], [35], [36]. The whole code-
base of Coreutils comprises approximately 60,000 lines of
C code5. To perform our study on commits we used the
benchmark6introduced by Ma et al. [16] that is composed of
two parts and includes Benchmark-1 , a set of commits mined
from the Coreutilsâ€™ Github repository from year 2012 to 2019
andCoREBench [19] that has fault introducing commits.
The benchmark contains a) mutants generated by Mart [37],
a state-of-the-art tool that supports a comprehensive set of
mutation operators and TCE7[9], [38] on both pre- and post-
commit program versions of each commit, b) the mutant labels
(whether they are commit-relevant), and c) large test pools
created using a combination of test generation tools [35], [36],
[39]. It is noted that the mutant test executions involved require
excessive computational resources, i.e., require roughly 100
weeks of computation. Details about the data we used are
recorded in Table I. The column #Relevant records the number
of commit-relevant mutants.
B. Experimental Procedure
To account for our working scenario, we always train
according to time, i.e, we use the older commits for training
and the newer for evaluation. This ensured that we follow the
historical order of the commits.
Following the stated RQs, our experiment is composed
of three parts. The ï¬rst part evaluates the prediction ability
(performance) of MuDelta , answering RQ1. The second at
evaluating the ability of MuDelta to rank commit-relevant
mutants, answering RQ2, and the third part at evaluating the
fault revealing potential, answering RQ3.
First experimental part: We evaluate the trained classiï¬ers
using ï¬ve typically adopted metrics, namely, the Area Under
the Receiver Operating Characteristic Curve (ROC-AUC),
the Area Under the Precision-Recall Curve (PR-AUC), the
precision, the recall and the F1-score.
The Receiver Operating Characteristic (ROC) curve records
the relationship between true and false positive rates [40].
The Precision-Recall (PR) Curve records the decrease in true
positive classiï¬cations when the predicted positive values
increase. In essence, the PR curve shows the trade-off between
precision and recall [40].
4https://www.gnu.org/software/coreutils/
5Measured with cloc (http://cloc.sourceforge.net/)
6https://github.com/relevantMutationTesting
7Compiler-based equivalent and duplicate mutant detection technique
902Precision is deï¬ned as the number of items that are truly
relevant among the items that predicted to be relevant. Recall
is deï¬ned as the number of items that are predicted to be
relevant among all the truly relevant ones. The F1-score or
F-measure of a classiï¬er is deï¬ned as the weighted harmonic
mean of the precision and recall. These assessment metrics
measure the general classiï¬cation accuracy of the classiï¬er.
Higher values denote a better classiï¬cation.
To reduce the risk of over-ï¬tting, we split our commit data
into three mutually exclusive sets (training, validation and test
data). We also use early stopping during training to overwhelm
over-ï¬tting. We use the following procedure:
1) Chronologically order the commit (from older to newer).
2) Select the newest 20% of commits as test data.
3) Randomly shufï¬‚e all the mutants from the remaining
80% of commits (oldest commit), then, select 20% of
them as validation data and the rest as training data.
Thus, the training, validation and test data represent 64%,
16% and 20% of the data-set, respectively. The model evalu-
ation is performed on the test data. This experiment part was
performed on both CoREBench and Benchmark-1 .
Second experimental part: We simulate a scenario where a
tester selects mutants and designs tests to kill them. This typ-
ical procedure [1], [12], [16], [20], [41] consists of randomly
selecting test cases, from the test pools of the benchmark, that
kill the selected mutants. Speciï¬cally, we rank the mutants and
then we follow the mutant order by picking test cases, from the
test pool, that kill them. We then remove all the killed mutants
and pick the next mutant from the list. If the mutant is not
killed by any of the tests, we discard it without selecting any
test. We repeat this process 100 times for all the approaches.
MuDelta ranks all the mutants by the predicted commit-
relevance probability, Random randomly ranks all the mutants
in the changed components, and Modiï¬cation randomly ranks
the mutants located on the altered code.
Our effectiveness metrics are the relevant subsuming muta-
tion score (rMS) achieved by the test suites when analysing
up to a certain number of mutants. Subsuming score metrics
allows reducing the inï¬‚uence of redundant mutants [10], [13],
[42]. We also compute the Average Percentage of Faults
Detected (APFD) [43] that represents the average relevant
subsuming mutation score when analysing any number of
mutants within a given range.
Our effort metric is the number of mutants picked (analysed
by the tester). This includes the mutants, killable or not, that
should be presented to testers for analysis (either design a
test to kill them or judge them as equivalent) when applying
mutation testing [9], [12]. In the spirit of the best-effort
evaluation, we focus on few mutants (up to 100) that testers
need to analyse. This evaluation aims at showing the beneï¬ts
ofMuDelta over Random under the same relative testing effort.
The contrast with the Modiï¬cation shows whether there is a
need for mutant selection outside of the modiï¬ed code, i.e.,
whether mutants on modiï¬cation are sufï¬cient leading to tests
that kill commit-relevant mutants. This part of the experiment
was performed on both CoREBench and Benchmark-1 .Third experimental part: To evaluate the fault revealing
ability of MuDelta , we used the CoREBench commits. We
adopted a chronological ordering for training, validation and
testing when splitting the commits similar to what we did
in previous experimental parts. We use the same process and
effort metric as in the the second part of the experiment and
report results related to fault revelation and the average per-
centage of commit-introduced faults revealed (APFD) within
the range, 1-100, of analysed mutants.
To account for the stochastic selection of test cases and
mutant ranking, we used the Wilcoxon test to determine
whether there is a statistically signiï¬cant difference between
the studied methods. To check the size of the differences we
used the Vargha Delaney effect size ^A12[44], which quantiï¬es
the differences between the approaches. A value ^A12= 0:5
suggests that the data of the two samples tend to be the same.
Values ^A12>0:5indicate that the ï¬rst data-set has higher
values, while values ^A12<0:5indicate the opposite.
VI. R ESULTS
A. Assessment of the Prediction Performance (RQ1)
To evaluate the performance of MuDelta , we check the
modelâ€™s convergence. During training and after each iteration
of the training process, we check the model performance on
both the training and validation data we used for training.
Figure 5 shows the ROC-AUC and PR-AUC values wrt the
number of training iterations. We observe that the model
performance on both the training and validation data increase
with the number of iteration and stabilizes at speciï¬c values,
suggesting that our model is able to learn the characteristics
of commit-relevant mutants.
We then evaluate the performance of our model to predict
commit-relevant mutants on the future commits that appear
in the test set. To compute the precision, recall and F1-score,
we set the prediction threshold probability to 0.1, which we
obtained by applying the geometric mean [45], [46] on the
validation dataset. The precision, recall and F1-score of our
classiï¬er are 0.63, 0.32 and 0.42, respectively. These values
are higher than those that one can get with a random classiï¬er
(0.21, 0.21 and 0.20, respectively). Figure 6 shows the ROC
and PR curves of our classiï¬er (strong lines) and a random
classiï¬er (dashed lines). We observe that the ROC-AUC of
our classiï¬er is 0.80 indicating a strong prediction ability.
Similarly, we see that the PR-AUC of our classiï¬er is 0.50
while the random classiï¬er PR-AUC is 0.20.
In this context [7] it is important to give few mutants
to developers for analysis. To evaluate the performance of
MuDelta with lower thresholds, we also study the performance
ofMuDelta with thresholds ranging from the 10 to 100
mutants. We observe that the median precision of MuDelta
ranges from 0.76 to 0.90 when the threshold goes from 10
to 30 mutants. These values are signiï¬cantly higher than the
random classiï¬er, which has a precision of 0.15.
These results provide evidence that MuDelta provides a
good discriminative ability for assessing the utility of mutants
to test particular code changes.
9030 500 1000 1500 2000 2500
Training Round0.60.70.80.91.0Performance Training PR-AUC
Training ROC-AUC
Eval PR-AUC
Eval ROC-AUCFig. 5. Training and Validation Curves from the Training phase.
0.0 0.2 0.4 0.6 0.8 1.0
Recall / False Positive Rate0.00.20.40.60.81.0Precision / True Positive RateROC (area = 0.80)
PR-Curve (area = 0.50)
Truth Positive Ratio
Fig. 6. Precision-Recall and ROC Curves on test data.
B. Mutant Ranking for Tests Assessment (RQ2)
Figure 7 shows the median rMSachieved by the mutant
ranking strategies, when the number of analysed mutant budget
range from 1 to 100 mutants. In other words, the ï¬gure
shows test effectiveness (measured with rMS, y-axis) that is
achieved by a developer when analysing a number of mutants,
representing the cost factor (recorded in x-axis). Each sub-
ï¬gure is a commit taken from the test data. We observe that
the curve for MuDelta is always higher than the curves of
random andModiï¬cation , and Random is above Modiï¬cation .
To further visualize the differences, Figure 8 shows the
distribution of the rMSof the mutant ranking strategies
for budget thresholds 10, 30, 50 and 100 mutants. As can be
seen from the plots, MuDelta outperforms both Random and
Modiï¬cation . Interestingly, Random outperforms Modiï¬cation .
With threshold 10 mutants, the difference of the median values
is 22% and 26% for Random andModiï¬cation , respectively.
This difference is markedly increased when analysing more
mutants, i.e., it becomes 45% and 50% for the thresholds of
30 and 50 mutants, for Random .
To check whether the differences are statistically signiï¬cant
we performed a Wilcoxon rank-sum test and computed the
Vargha Delaney ^A12effect size and found that MuDelta
outperforms both Random andModiï¬cation with statistically
signiï¬cant difference (at 0:01signiï¬cant level). Random has
also statistically signiï¬cant differences with Modiï¬cation .
0.00.20.40.60.81.0
0 25 50 75 1000.00.20.40.60.81.0
0 25 50 75 100 0 25 50 75 100 0 25 50 75 100 0 25 50 75 100Random
MuDelta
ModificationrMS*Fig. 7. rMSachieved when analysing up to 100 mutants.
10 MuDelta 10 Random
10 Modification30 MuDelta 30 Random
30 Modification50 MuDelta 50 Random
50 Modification100 MuDelta 100 Random
100 Modification0.00.20.40.60.81.0rMS*
Fig. 8. rMSvalues when analysing up to 10, 30, 50 and 100 mutants.
Figure 9 shows the Vargha Delaney ^A12values between
MuDelta and both Random andModiï¬cation . We observe that
the median value is between 77% and 83% for threshold
between 10 and 100 mutants, for Random . Suggesting that
MuDelta is better than Random in 77% to 83% of the cases for
these thresholds. The differences are larger for Modiï¬cation .
We further validate our approach by considering the dis-
tributions of APFD (Average Percentage of Faults Detected)
values for all possible thresholds (for 1-100 mutants). Fig-
ure 10 depicts these results and shows that MuDelta yields
an APFD median of 71%, Random and Modiï¬cation reach
median APFD values of 26% and 11% respectively, conï¬rm
the superiority of our approach.
To account for the stochastic nature of the compared
approaches and increase the conï¬dence on our results, we
further perform a statistical test on the APFD values. The
Wilcoxon test results yielded p-values much lower than our
signiï¬cance level for the compared data, i.e., samples of
MuDelta and Random ,MuDelta and Modiï¬cation ,Random
and Modiï¬cation, respectively. Therefore, we conclude that
MuDelta outperforms Random with statistically signiï¬cance,
while Modiï¬cation is not sufï¬cient for testing the deltas.
C. Mutant Ranking and Fault Revelation (RQ3)
Figure 11 shows the distributions of APFD (Average Per-
centage of Faults Detected) values for the CoREBench fault
introducing test commits, using the three approaches under
evaluation. While MuDelta yields an APFD median of 52%,
Random andModiï¬cation reach median APFD values of 25%
and 0% respectively. The improvement over Random and
Modiï¬cation are 27% and 52%, respectively. These results
conï¬rm the superiority of our approach wrt to fault revelation.
904Top 10 Top 20 Top 30 Top 40 Top 50 Top 60 Top 70 Top 80 Top 90Top 1000.40.50.60.70.80.91.0Vargha and Delaney A measureMuDelta - Random
MuDelta - ModificationFig. 9. Vargha and Deianey ^A12(MuDelta VS Random, MuDelta VS
Modiï¬cation) about rMS
MuDelta Random Modification0.00.20.40.60.81.0APFD (rMS*)
Fig. 10. APFD rMS(up to 100 mutants).
The Wilcoxon test yielded p-values much lower than our
signiï¬cance level for the compared data, i.e., samples of
MuDelta and Random ,MuDelta and Modiï¬cation ,Random
andModiï¬cation . Therefore, we conclude that MuDelta out-
performs Random andModiï¬cation with statistically signiï¬-
cance while Random outperforms Modiï¬cation .
Figure 12 shows the distribution of fault revelation for the
ranking strategies and for mutant set size thresholds up to
100 mutants. We observe that the curve for MuDelta is above
the curves of random andModiï¬cation , and Random is above
Modiï¬cation . Speciï¬cally, we observe that MuDelta reaches a
fault revelation of 60% and 100% when analysing the top 30
and 61 mutants, while Random 7% and 12%, respectively.
VII. D ISCUSSION
A. Comparison with other models
To further assess the effectiveness of our model, we contrast
it with the prediction ability of ï¬ve other models (on the
same training, validation and test data-sets) that are typically
used in prediction modelling studies. In particular, we used
three families of models (Ensemble model classiï¬ers, Logistic
classiï¬ers and Neural Networks) and built ï¬ve models; namely
Adaboost, Random Forest, Logistic Regression, Multilayer
Perceptron (MLP) and Mixed MLP. MLP and Mixed MLP
were inspired by the work of Li et al. [47], their architecture
is shown in Figure 13 and 14. To train and evaluate the models
we used the Sklearn library8. Since our data are imbalanced
we also used class weighting strategies that are commonly
used to tackle this issue. To avoid bias from improper setting
of the learners, in all the cases we used Grid Search Cross
Validation on the validation set to tune our hyperparameters.
8https://scikit-learn.org/stable/
MuDelta Random Modification0.00.20.40.60.81.0APFD
Fig. 11. APFD Fault-revelation (up to 100 mutants).
1 11 21 31 41 51 61 71 81 91 100
Number of Mutants0.00.20.40.60.81.0Fault RevelationMuDelta Random Modification
Fig. 12. Median fault-revelation in fault introducing commits.
TABLE II
MODEL COMPARISON
ROC-AUC PR-AUC MCCPrecision on
Top-100
AdaBoost 0.6 0.35 0.26 0.55
Random Forest 0.66 0.31 0.24 0.57
Logistic 0.58 0.26 0.13 0.21
MLP 0.51 0.19 0.1 0.2
Mixed MLP 0.68 0.45 0.31 0.2
XGBoost 0.80 0.50 0.36 0.61
Table II reports the ROC-AUC, PR-AUC, MCC, and pre-
cision on top 100 ranked mutants of the prediction results
of all different learners we built. The results show that the
XGBoost model, that we use, perform best in all cases. The
general prediction metrics (ROC-AUC,PR-AUC, MCC) show
that Mixed MLP model is the second best case though it
falls behind the Ensemble models wrt to the top-100 mutants.
Nevertheless, the results provide clear indications that the
XGBoost model we use is indeed the best choice.
FC Layer
Dropout Layer
Output LayerInput LayerAll Features
Fig. 13. MLP - Neural Network Architecture
905Input Layer
FC Layer
Dropout Layer
Merge Layer
FC LayerDropout Layer
Dropout Layer
Output Layer Mutant Utility
Features Mutant-Modification 
 Interaction FeaturesContextual 
FeaturesFig. 14. Mixed MLP - Neural Network Architecture
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175
XGBoost SHAP ScoreMutantTypeUG
In-Ctrl-DependentsCDG
Mutant-modificationInformation-flow
Out-Ctrl-Dependents
Out-Data-DependentsAST-parentsDDDG
Fig. 15. Feature Importance, SHAP Score of top-10 feature sets. The 10
most important features are â€œMutant Typeâ€, â€œUtility Graphâ€, â€œIncoming Con-
trol Dependenciesâ€, â€œMutant-modiï¬cation featuresâ€, â€œControl Dependency
Graphâ€, â€œInformation-ï¬‚owâ€, â€œOutgoing Control Dependenciesâ€, â€œOutgoing
Data Dependenciesâ€,â€œAST parentsâ€ and â€œDirected Data Dependency Graphâ€.
B. Feature Importance
To evaluate the importance of our features we used the
SHapley Additive exPlanations (SHAP)9method [48], i.e., a
game theory method that explains individual predictions based
on the game theoretically optimal Shapley Values. In particu-
lar, we aim at explaining our predictions by assuming that each
feature value we use is a â€œplayerâ€ in a game where the predic-
tion is the payout. Shapley values â€“ a method from coalitional
game theory â€“ tells us how to fairly distribute the â€œpayoutâ€
among the features. We thus measure and report the feature
importance (Shapley values) of the feature categories we use.
Results are depicted on Figures 15 and show that â€œMutant
Typeâ€, â€œUtility Graphâ€, â€œIncoming Control Dependenciesâ€,
â€œMutant-modiï¬cation featuresâ€, â€œControl Dependency Graphâ€
and the â€œInformation-ï¬‚owâ€ are the top 6 feature sets and that
all three types of features we use are important. Additional
results related to the feature importance of the individual
features we used can be found on the accompanied website.
9https://github.com/slundberg/shapVIII. T HREATS TO VALIDITY
A possible threat to external validity could be due to our test
subjects. Our target was commits that do not alter test contracts
and make small modiï¬cations, similar to those observed in
industrial CI pipelines. Such commits are usually hard to test
and typically result in subtle faults. Large commits that add
new features, should be anyway tested by using a mutation
testing approach that involves (almost) all the relevant mu-
tants residing on the added code. To reduce this threat, we
sampled a commit set where we could reasonably perform our
experiments. At the same time, to diminish potential selection
bias, we also used the Coreutils commits of CoREBench [19],
which are frequently used in testing studies.
We are conï¬dent on our results since the relevance prop-
erties of the mutants reside on the context of the committed
code, which includes the area around the dependencies to the
committed code (where we draw our feature values), that is
small and its characteristics should be as representative as our
subjects. Moreover, our predictions converge well, do not have
signiï¬cant variance wrt to the baselines and consistently out-
perform the baselines in all test subjects we used. Additionally,
the statistical signiï¬cance we observe indicates the sufï¬ciency
of our data analysis [49]. Future work should validate our
ï¬ndings and analysis to larger programs.
Another threat may relate to the mutants we use. To mitigate
this threat, we selected data from a mutation testing tool [37]
that has been used in several studies [16], [20], [39] that
supports the most commonly used operators [50] and covers
the most frequent features of the C language.
Threats to internal validity may be due our features. We use
a large number of features, selected either based on previous
studies [20] or by using our intuition, which are automatically
ï¬ltered by gradient boosting. To further reduce this concern,
we split our data in three parts, training, validation and test
data. During training (using training data) we measure the
model convergence on training and validation data. As demon-
strated in Figure 5, our model converges both on the training
and validation data, showing that there are low chances for
over- or under-ï¬tting because in these cases, the model would
not converge on the validation data.
The test-based approximation of relevant and killable mu-
tants may introduce additional threats. To reduce it, we used
test suites generated by KLEE [35] and SeMu [39], together
with developer test suites.
A possible threat to construct validity could be due to the
effort metric, i.e., the number of analysed mutants, we use.
This is a typical metric for this kind of studies [12] aiming at
capturing the manual effort involved when analysing mutants
or asserting automatically generated tests. Since, our data have
been ï¬ltered by TCE [9], [38], a state-of-the-art equivalent
mutant detection technique, this threat should be limited.
Overall, we tried to reduce threats by using various evalu-
ation metrics, i.e., prediction performance, relevant mutation
score and fault revelation, and established procedures. Further-
more, to enable replication and future research we will make
our tools and data publicly available.
906IX. R ELATED WORK
The problem of determining the set of mutants that are
most relevant to particular code changes might resemble a
dependence analysis problem. One natural solution involves
forming a program slice on the set of changed statements. Any
mutant that lies in the slice should be considered relevant.
Unfortunately, this approach does not scale well for several
reasons. Firstly, as have been previously observed [51], [52],
even a single static slice of a program tends to occupy between
one and two thirds of the program from which it is constructed.
Therefore, the union of a set of such slices, will be large, and
thereby fail to exclude many mutants. Secondly, the depen-
dence analysis would need to be incremental, which raises
further challenges. Although there have been incremental de-
pendence analyses in the literature [53], many well-developed
slicing systems are not incremental. In general, the problem of
incremental program analysis at scale remains challenging [5].
Thirdly, it is hard to use dependence analysis to provide the
priority ordering we need, where priority is based on degree
of relevance. Potentially, unions of dynamic slices or some
form of observation-based slicing [17] could achieve this,
but such approaches have a prohibitive computational cost in
comparison to our method.
Change impact analysis [54] aims at determining the effects
of changes on the other parts of the software. Similar to
program slicing, such approaches are conservative, therefore
they result in large number of false positives, does not account
for equivalent mutants located on potentially infected code
and is hard to provide the mutant ranking (prioritizes mutant
types and location) we need. Other attempts aim at testing the
potential propagation ï¬‚ows of the changes [14], [15], [55],
[56]. Similarly to change impact analysis their purpose is to
identify the program paths (ï¬‚ows) that may be impacted by
the changes. They rely on symbolic execution to check for
the feasibility of the ï¬‚ows, form test requirements (conditions
to be fulï¬lled) and decide on relevance. Unfortunately, such
techniques inherit most of the issues of symbolic execution, are
complex to implement and test the propagation of the changes.
In contrast our technique scales since it relies on static code
features, does not require any complex analysis techniques and
applies mutation testing that is known for capturing the fault-
revealing properties of test suites [1], [20].
Automatic test case generation aims at producing test inputs
that a) make observable the code differences of two program
versions [57], b) increase and optimize coverage [58] and kill
mutants [39], [59], [60]. Among these techniques, the most
relevant to our study are the are the ones related to patch
testing, i.e., differential symbolic execution [61], KATCH
[62] and Shadow symbolic execution [35]. These techniques
generate tests exercising the semantic differences between
program versions guided by coverage. All these techniques
do not propose any test requirements as done by MuDelta and
thus, they are complementary to our goal. This means that
they can be used to generate tests to kill the commit-relevant
mutants proposed by MuDelta .Related to continuous integration, Google [7] is using a
mutation testing tool that is integrated with the code review
process (reviewers select mutants). This tool proposes mutants
to developers in order to design test cases. The key basis of
this approach is to choose some mutants from the lines of
the altered code. We share a similar intent, though we aim at
making an informative selection of mutants among all project
mutants. According to our results mutants residing on non-
altered code tend to be powerful at capturing the interactions
between the altered and non-altered code.
Regression mutation testing [63] and the predictive mutation
testing [64], [65] also focus on regression testing. Similarly,
Pitest [66], a popular mutation testing tool, implements an
incremental analysis that computes which mutants are killed
or not by a regression test suite. This means that the goal
of the above techniques is to estimate the mutation score
achieved by regression test suites thereby not making any
distinction between commit-relevant and non-relevant mutants,
not making any mutant ranking and not proposing any live
mutant to be used for test generation.
Fault revealing mutant selection [20] aims at selecting
mutants that are likely to expose faults. While powerful, that
technique targets the entire program functionality and not the
changed/delta one. Since it is unaware of the deltas it selects
many irrelevant mutants, while missing many delta-relevant
mutants related to the delta-context interactions.
Perhaps the closest work to ours is the commit-aware
mutation testing study [16] that deï¬nes the notion of mutant
relevance and demonstrates its potential. In essence that work
describes the fundamental aspects of relevant mutants but does
not deï¬ne any way to identify them at the testing time. We
therefore built on top of this notion by providing a static
technique that identiï¬es relevant mutants.
Overall, there is a fundamental difference on the aims of
our approach and previous research since we statically produce
relevant, to code changes, mutants and rank them to provide
a best effort testing application.
X. C ONCLUSION
We presented MuDelta a delta-oriented mutation testing
approach that selects delta-relevant mutants; mutants capturing
the program behaviours affected by speciï¬c program changes.
Experiments with MuDelta demonstrated that it identiï¬es
delta-relevant mutants with 0.63 and 0.32 precision and recall.
Interestingly, killing these mutants leads to strong tests that
kill 45% more relevant mutants than killing randomly selected
mutants. Our results also show that MuDelta selects mutants
with a 27% higher fault revealing ability than randomly
selected mutants.
ACKNOWLEDGEMENT
This work is supported by the Luxembourg National
Research Funds (FNR) through the CORE project grant
C17/IS/11686509/CODEMATES. Mark Harman is part sup-
ported by European Research Council Advanced Fellowship
grant number 741278; Evolutionary Program Improvement
(EPIC).
907REFERENCES
[1] T. T. Chekam, M. Papadakis, Y . L. Traon, and M. Harman, â€œAn empirical
study on mutation, statement and branch coverage fault revelation
that avoids the unreliable clean program assumption,â€ in Proceedings
of the 39th International Conference on Software Engineering, ICSE
2017, Buenos Aires, Argentina, May 20-28, 2017 , 2017, pp. 597â€“608.
[Online]. Available: https://doi.org/10.1109/ICSE.2017.61
[2] R. A. DeMillo, R. J. Lipton, and F. G. Sayward, â€œHints on
test data selection: Help for the practicing programmer,â€ IEEE
Computer , vol. 11, no. 4, pp. 34â€“41, 1978. [Online]. Available:
https://doi.org/10.1109/C-M.1978.218136
[3] T. A. Budd and D. Angluin, â€œTwo Notions of Correctness and Their
Relation to Testing,â€ Acta Informatica , vol. 18, no. 1, pp. 31â€“45, March
1982.
[4] M. Fowler, â€œContinuous integration,â€
https://martinfowler.com/articles/continuousIntegration.html, online;
accessed 10 February 2020.
[5] M. Harman and P. W. Oâ€™Hearn, â€œFrom start-ups to scale-ups:
Opportunities and open problems for static and dynamic program
analysis,â€ in 18th IEEE International Working Conference on Source
Code Analysis and Manipulation, SCAM 2018, Madrid, Spain,
September 23-24, 2018 . IEEE Computer Society, 2018, pp. 1â€“23.
[Online]. Available: https://doi.org/10.1109/SCAM.2018.00009
[6] C. Leong, A. Singh, M. Papadakis, Y . L. Traon, and J. Micco,
â€œAssessing transition-based test selection algorithms at google,â€
inProceedings of the 41st International Conference on Software
Engineering: Software Engineering in Practice, ICSE (SEIP) 2019,
Montreal, QC, Canada, May 25-31, 2019 , 2019, pp. 101â€“110. [Online].
Available: https://doi.org/10.1109/ICSE-SEIP.2019.00019
[7] G. Petrovic and M. Ivankovic, â€œState of mutation testing at google,â€
inProceedings of the 40th International Conference on Software
Engineering: Software Engineering in Practice, ICSE (SEIP) 2018,
Gothenburg, Sweden, May 27 - June 03, 2018 , 2018, pp. 163â€“171.
[Online]. Available: https://doi.org/10.1145/3183519.3183521
[8] M. Papadakis, M. Kintis, J. Zhang, Y . Jia, Y . L. Traon, and M. Harman,
â€œChapter six - mutation testing advances: An analysis and survey,â€
Advances in Computers , vol. 112, pp. 275â€“378, 2019. [Online].
Available: https://doi.org/10.1016/bs.adcom.2018.03.015
[9] M. Kintis, M. Papadakis, Y . Jia, N. Malevris, Y . L. Traon, and
M. Harman, â€œDetecting trivial mutant equivalences via compiler
optimisations,â€ IEEE Trans. Software Eng. , vol. 44, no. 4, pp. 308â€“333,
2018. [Online]. Available: https://doi.org/10.1109/TSE.2017.2684805
[10] M. Papadakis, C. Henard, M. Harman, Y . Jia, and Y . L. Traon, â€œThreats
to the validity of mutation-based test assessment,â€ in Proceedings of the
25th International Symposium on Software Testing and Analysis, ISSTA
2016, Saarbr Â¨ucken, Germany, July 18-20, 2016 , 2016, pp. 354â€“365.
[Online]. Available: https://doi.org/10.1145/2931037.2931040
[11] P. Ammann, M. E. Delamaro, and J. Offutt, â€œEstablishing theoretical
minimal sets of mutants,â€ in 2014 IEEE Seventh International Confer-
ence on Software Testing, Veriï¬cation and Validation . IEEE, 2014.
[12] B. Kurtz, P. Ammann, J. Offutt, M. E. Delamaro, M. Kurtz, and
N. G Â¨okc Â¸e, â€œAnalyzing the validity of selective mutation with dominator
mutants,â€ in Proceedings of the 24th ACM SIGSOFT International
Symposium on Foundations of Software Engineering, FSE 2016,
Seattle, WA, USA, November 13-18, 2016 , 2016, pp. 571â€“582. [Online].
Available: https://doi.org/10.1145/2950290.2950322
[13] M. Papadakis, T. T. Chekam, and Y . L. Traon, â€œMutant
quality indicators,â€ in 2018 IEEE International Conference
on Software Testing, Veriï¬cation and Validation Workshops,
ICST Workshops, V Â¨aster Ëšas, Sweden, April 9-13, 2018 . IEEE
Computer Society, 2018, pp. 32â€“39. [Online]. Available:
http://doi.ieeecomputersociety.org/10.1109/ICSTW.2018.00025
[14] T. Apiwattanapong, R. A. Santelices, P. K. Chittimalli, A. Orso, and
M. J. Harrold, â€œMATRIX: maintenance-oriented testing requirements
identiï¬er and examiner,â€ in Testing: Academia and Industry Conference
- Practice And Research Techniques (TAIC PART 2006), 29-31
August 2006, Windsor, United Kingdom , 2006, pp. 137â€“146. [Online].
Available: https://doi.org/10.1109/TAIC-PART.2006.18
[15] R. A. Santelices, P. K. Chittimalli, T. Apiwattanapong, A. Orso,
and M. J. Harrold, â€œTest-suite augmentation for evolving software,â€
in23rd IEEE/ACM International Conference on Automated Software
Engineering (ASE 2008), 15-19 September 2008, Lâ€™Aquila, Italy , 2008,
pp. 218â€“227. [Online]. Available: https://doi.org/10.1109/ASE.2008.32[16] W. Ma, T. Laurent, M. Ojdanic, T. T. Chekam, A. Ventresque, and
M. Papadakis, â€œCommit-aware mutation testing,â€ in Proceedings of
the 36th IEEE International Conference on Software Maintenance and
Evolution, ICSME , 2020.
[17] D. W. Binkley, N. Gold, M. Harman, S. S. Islam, J. Krinke, and S. Yoo,
â€œORBS: language-independent program slicing,â€ in Proceedings of
the 22nd ACM SIGSOFT International Symposium on Foundations of
Software Engineering, (FSE-22), Hong Kong, China, November 16 -
22, 2014 , S. Cheung, A. Orso, and M. D. Storey, Eds. ACM, 2014, pp.
109â€“120. [Online]. Available: https://doi.org/10.1145/2635868.2635893
[18] M. Kintis, M. Papadakis, and N. Malevris, â€œEmploying second-order
mutation for isolating ï¬rst-order equivalent mutants,â€ Softw. Test., Verif.
Reliab. , vol. 25, no. 5-7, pp. 508â€“535, 2015. [Online]. Available:
https://doi.org/10.1002/stvr.1529
[19] M. B Â¨ohme and A. Roychoudhury, â€œCorebench: studying complexity of
regression errors,â€ in International Symposium on Software Testing and
Analysis, ISSTA â€™14, San Jose, CA, USA - July 21 - 26, 2014 , 2014, pp.
105â€“115. [Online]. Available: https://doi.org/10.1145/2610384.2628058
[20] T. T. Chekam, M. Papadakis, T. F. Bissyand Â´e, Y . L. Traon, and
K. Sen, â€œSelecting fault revealing mutants,â€ Empirical Software
Engineering , vol. 25, no. 1, pp. 434â€“487, 2020. [Online]. Available:
https://doi.org/10.1007/s10664-019-09778-7
[21] M. Chalupa, â€œSlicing of llvm bitcode,â€ Masaryk Univ , 2016.
[22] J. J. McAuley, L. da Fontoura Costa, and T. S. Caetano, â€œRich-club
phenomenon across complex network hierarchies,â€ Applied Physics
Letters , vol. 91, no. 8, p. 084103, 2007.
[23] R. Milo, N. Kashtan, S. Itzkovitz, M. E. Newman, and U. Alon, â€œOn the
uniform generation of random graphs with prescribed degree sequences,â€
arXiv preprint cond-mat/0312028 , 2003.
[24] J. Saram Â¨aki, M. Kivel Â¨a, J.-P. Onnela, K. Kaski, and J. Kertesz, â€œGener-
alizations of the clustering coefï¬cient to weighted complex networks,â€
Physical Review E , vol. 75, no. 2, p. 027105, 2007.
[25] G. Fagiolo, â€œClustering in complex directed networks,â€ Phys.
Rev. E , vol. 76, p. 026107, Aug 2007. [Online]. Available:
https://link.aps.org/doi/10.1103/PhysRevE.76.026107
[26] J.-P. Onnela, J. Saram Â¨aki, J. Kert Â´esz, and K. Kaski, â€œIntensity and
coherence of motifs in weighted complex networks,â€ Physical Review
E, vol. 71, no. 6, p. 065103, 2005.
[27] P. G. Lind, M. C. Gonz Â´alez, and H. J. Herrmann,
â€œCycles and clustering in bipartite networks,â€ Phys. Rev.
E, vol. 72, p. 056127, Nov 2005. [Online]. Available:
https://link.aps.org/doi/10.1103/PhysRevE.72.056127
[28] L. Page, S. Brin, R. Motwani, and T. Winograd, â€œThe pagerank citation
ranking: Bringing order to the web.â€ Stanford InfoLab, Technical Report
1999-66, November 1999, previous number = SIDL-WP-1999-0120.
[Online]. Available: http://ilpubs.stanford.edu:8090/422/
[29] J. M. Kleinberg, â€œAuthoritative sources in a hyperlinked environment,â€
J. ACM , vol. 46, no. 5, p. 604â€“632, Sep. 1999. [Online]. Available:
https://doi.org/10.1145/324133.324140
[30] J. Falleri, F. Morandat, X. Blanc, M. Martinez, and M. Monperrus,
â€œFine-grained and accurate source code differencing,â€ in ACM/IEEE
International Conference on Automated Software Engineering, ASE
â€™14, Vasteras, Sweden - September 15 - 19, 2014 , 2014, pp. 313â€“324.
[Online]. Available: http://doi.acm.org/10.1145/2642937.2642982
[31] J. H. Friedman, â€œStochastic gradient boosting,â€ Computational Statistics
& Data Analysis , vol. 38, no. 4, pp. 367â€“378, 2002.
[32] T. Chen and C. Guestrin, â€œXgboost: A scalable tree boosting system,â€
inProceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining , ser. KDD â€™16. New York,
NY , USA: Association for Computing Machinery, 2016, p. 785â€“794.
[Online]. Available: https://doi.org/10.1145/2939672.2939785
[33] J. H. Andrews, L. C. Briand, Y . Labiche, and A. S. Namin,
â€œUsing mutation analysis for assessing and comparing testing coverage
criteria,â€ IEEE Trans. Software Eng. , vol. 32, no. 8, pp. 608â€“624, 2006.
[Online]. Available: https://doi.org/10.1109/TSE.2006.83
[34] R. Gopinath, I. Ahmed, M. A. Alipour, C. Jensen, and A. Groce,
â€œMutation reduction strategies considered harmful,â€ IEEE Trans.
Reliab. , vol. 66, no. 3, pp. 854â€“874, 2017. [Online]. Available:
https://doi.org/10.1109/TR.2017.2705662
[35] T. Kuchta, H. Palikareva, and C. Cadar, â€œShadow symbolic
execution for testing software patches,â€ ACM Trans. Softw. Eng.
Methodol. , vol. 27, no. 3, pp. 10:1â€“10:32, 2018. [Online]. Available:
https://doi.org/10.1145/3208952
908[36] C. Cadar, D. Dunbar, and D. Engler, â€œKlee: Unassisted and automatic
generation of high-coverage tests for complex systems programs,â€ in
Proceedings of the 8th USENIX Conference on Operating Systems
Design and Implementation , ser. OSDIâ€™08. USA: USENIX Association,
2008, p. 209â€“224.
[37] T. T. Chekam, M. Papadakis, and Y . L. Traon, â€œMart: a mutant
generation tool for LLVM,â€ in Proceedings of the ACM Joint Meeting
on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019,
Tallinn, Estonia, August 26-30, 2019 , 2019, pp. 1080â€“1084. [Online].
Available: https://doi.org/10.1145/3338906.3341180
[38] M. Papadakis, Y . Jia, M. Harman, and Y . L. Traon, â€œTrivial compiler
equivalence: A large scale empirical study of a simple, fast and effective
equivalent mutant detection technique,â€ in 37th IEEE/ACM International
Conference on Software Engineering, ICSE 2015, Florence, Italy, May
16-24, 2015, Volume 1 , A. Bertolino, G. Canfora, and S. G. Elbaum,
Eds. IEEE Computer Society, 2015, pp. 936â€“946. [Online]. Available:
https://doi.org/10.1109/ICSE.2015.103
[39] T. T. Chekam, M. Papadakis, M. Cordy, and Y . L. Traon, â€œKilling
stubborn mutants with symbolic execution,â€ 2020. [Online]. Available:
http://arxiv.org/abs/2001.02941
[40] A. Zheng, Evaluating Machine Learning Models A Beginnerâ€™s
Guide to Key Concepts and Pitfalls . Oâ€™Reilly Media, Inc,
2015. [Online]. Available: https://www.oreilly.com/data/free/evaluating-
machine-learning-models.csp
[41] A. S. Namin, J. H. Andrews, and D. J. Murdoch, â€œSufï¬cient mutation
operators for measuring test effectiveness,â€ in Proceedings of the 30th
International Conference on Software Engineering (ICSEâ€™08) , Leipzig,
Germany, 10-18 May 2008, pp. 351â€“360.
[42] B. Kurtz, P. Ammann, M. E. Delamaro, J. Offutt, and L. Deng, â€œMutant
subsumption graphs,â€ in 2014 IEEE Seventh International Conference
on Software Testing, Veriï¬cation and Validation Workshops . IEEE,
2014.
[43] C. Henard, M. Papadakis, M. Harman, Y . Jia, and Y . L. Traon,
â€œComparing white-box and black-box test prioritization,â€ in Proceedings
of the 38th International Conference on Software Engineering, ICSE
2016, Austin, TX, USA, May 14-22, 2016 , 2016, pp. 523â€“534. [Online].
Available: http://doi.acm.org/10.1145/2884781.2884791
[44] A. Vargha and H. D. Delaney, â€œA Critique and Improvement of the CL
Common Language Effect Size Statistics of McGraw and Wong,â€ Jrnl.
Educ. Behav. Stat. , vol. 25, no. 2, pp. 101â€“132, 2000.
[45] M. Kubat, S. Matwin et al. , â€œAddressing the curse of imbalanced training
sets: one-sided selection,â€ in Icml, vol. 97. Citeseer, 1997, pp. 179â€“186.
[46] R. Barandelaa and E. Rangela, â€œStrategies for learning in class imbalance
problems,â€ 2002.
[47] X. Li, W. Li, Y . Zhang, and L. Zhang, â€œDeepï¬‚: integrating multiple
fault diagnosis dimensions for deep fault localization,â€ in Proceedings
of the 28th ACM SIGSOFT International Symposium on Software
Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019 ,
D. Zhang and A. MÃ¸ller, Eds. ACM, 2019, pp. 169â€“180. [Online].
Available: https://doi.org/10.1145/3293882.3330574
[48] S. M. Lundberg and S. Lee, â€œA uniï¬ed approach to interpreting
model predictions,â€ in Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus,
S. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 4765â€“
4774. [Online]. Available: http://papers.nips.cc/paper/7062-a-uniï¬ed-
approach-to-interpreting-model-predictions
[49] A. Arcuri and L. C. Briand, â€œA practical guide for using statistical
tests to assess randomized algorithms in software engineering,â€
inProceedings of the 33rd International Conference on Software
Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28,
2011 , R. N. Taylor, H. C. Gall, and N. Medvidovic, Eds. ACM, 2011,
pp. 1â€“10. [Online]. Available: https://doi.org/10.1145/1985793.1985795
[50] T. Laurent, M. Papadakis, M. Kintis, C. Henard, Y . Le Traon, and
A. Ventresque, â€œAssessing and improving the mutation testing practice
of pit,â€ in 2017 IEEE International Conference on Software Testing,
Veriï¬cation and Validation (ICST) . IEEE, 2017, pp. 430â€“435.
[51] D. W. Binkley and M. Harman, â€œLocating dependence clusters and
dependence pollution,â€ in 21st IEEE International Conference on
Software Maintenance (ICSM 2005), 25-30 September 2005, Budapest,
Hungary . IEEE Computer Society, 2005, pp. 177â€“186. [Online].
Available: https://doi.org/10.1109/ICSM.2005.58[52] D. W. Binkley, M. Harman, and J. Krinke, â€œEmpirical study of
optimization techniques for massive slicing,â€ ACM Trans. Program.
Lang. Syst. , vol. 30, no. 1, p. 3, 2007. [Online]. Available:
https://doi.org/10.1145/1290520.1290523
[53] A. Orso, S. Sinha, and M. J. Harrold, â€œIncremental slicing based
on data-dependences types,â€ in International Conference on Software
Maintenance (ICSM 2001) , Los Alamitos, California, USA, Nov. 2001,
pp. 158â€“167.
[54] B. Li, X. Sun, H. Leung, and S. Zhang, â€œA survey of code-
based change impact analysis techniques,â€ Softw. Test. Veriï¬cation
Reliab. , vol. 23, no. 8, pp. 613â€“646, 2013. [Online]. Available:
https://doi.org/10.1002/stvr.1475
[55] R. A. Santelices and M. J. Harrold, â€œExploiting program dependencies
for scalable multiple-path symbolic execution,â€ in Proceedings of the
Nineteenth International Symposium on Software Testing and Analysis,
ISSTA 2010, Trento, Italy, July 12-16, 2010 , 2010, pp. 195â€“206.
[Online]. Available: https://doi.org/10.1145/1831708.1831733
[56] R. Santelices and M. J. Harrold, â€œApplying aggressive propagation-
based strategies for testing changes,â€ in Fourth IEEE International
Conference on Software Testing, Veriï¬cation and Validation, ICST
2011, Berlin, Germany, March 21-25, 2011 , 2011, pp. 11â€“20. [Online].
Available: https://doi.org/10.1109/ICST.2011.46
[57] D. Qi, A. Roychoudhury, and Z. Liang, â€œTest generation to
expose changes in evolving programs,â€ in ASE 2010, 25th IEEE/ACM
International Conference on Automated Software Engineering, Antwerp,
Belgium, September 20-24, 2010 , 2010, pp. 397â€“406. [Online].
Available: https://doi.org/10.1145/1858996.1859083
[58] Z. Xu, Y . Kim, M. Kim, G. Rothermel, and M. B. Cohen,
â€œDirected test suite augmentation: techniques and tradeoffs,â€ in
Proceedings of the 18th ACM SIGSOFT International Symposium
on Foundations of Software Engineering, 2010, Santa Fe, NM,
USA, November 7-11, 2010 , 2010, pp. 257â€“266. [Online]. Available:
https://doi.org/10.1145/1882291.1882330
[59] G. Fraser and A. Zeller, â€œMutation-driven generation of unit tests and
oracles,â€ IEEE Trans. Software Eng. , vol. 38, no. 2, pp. 278â€“292, 2012.
[Online]. Available: https://doi.org/10.1109/TSE.2011.93
[60] B. H. Smith and L. Williams, â€œOn guiding the augmentation of
an automated test suite via mutation analysis,â€ Empirical Software
Engineering , vol. 14, no. 3, pp. 341â€“369, 2009. [Online]. Available:
https://doi.org/10.1007/s10664-008-9083-7
[61] S. Person, M. B. Dwyer, S. G. Elbaum, and C. S. Pasareanu, â€œDifferential
symbolic execution,â€ in Proceedings of the 16th ACM SIGSOFT
International Symposium on Foundations of Software Engineering,
2008, Atlanta, Georgia, USA, November 9-14, 2008 , 2008, pp.
226â€“237. [Online]. Available: https://doi.org/10.1145/1453101.1453131
[62] P. D. Marinescu and C. Cadar, â€œKATCH: high-coverage testing
of software patches,â€ in Joint Meeting of the European Software
Engineering Conference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering, ESEC/FSEâ€™13, Saint Petersburg,
Russian Federation, August 18-26, 2013 , 2013, pp. 235â€“245. [Online].
Available: https://doi.org/10.1145/2491411.2491438
[63] L. Zhang, D. Marinov, L. Zhang, and S. Khurshid, â€œRegression mutation
testing,â€ in International Symposium on Software Testing and Analysis,
ISSTA 2012, Minneapolis, MN, USA, July 15-20, 2012 , 2012, pp. 331â€“
341.
[64] J. Zhang, L. Zhang, M. Harman, D. Hao, Y . Jia, and
L. Zhang, â€œPredictive mutation testing,â€ IEEE Trans. Software
Eng., vol. 45, no. 9, pp. 898â€“918, 2019. [Online]. Available:
https://doi.org/10.1109/TSE.2018.2809496
[65] D. Mao, L. Chen, and L. Zhang, â€œAn extensive study on cross-
project predictive mutation testing,â€ in 12th IEEE Conference on
Software Testing, Validation and Veriï¬cation, ICST 2019, Xiâ€™an,
China, April 22-27, 2019 , 2019, pp. 160â€“171. [Online]. Available:
https://doi.org/10.1109/ICST.2019.00025
[66] H. Coles, T. Laurent, C. Henard, M. Papadakis, and A. Ventresque,
â€œPIT: a practical mutation testing tool for java (demo),â€ in Proceedings
of the 25th International Symposium on Software Testing and Analysis,
ISSTA 2016, Saarbr Â¨ucken, Germany, July 18-20, 2016 , 2016, pp.
449â€“452. [Online]. Available: https://doi.org/10.1145/2931037.2948707
909