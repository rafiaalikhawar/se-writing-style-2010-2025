Patching as Translation: the Data and the Metaphor
Yangruibo Ding
Columbia University
yangruibo.ding@columbia.eduBaishakhi Ray
Columbia University
rayb@cs.columbia.edu
Premkumar Devanbu
University of California, Davis
ptdevanbu@ucdavis.eduVincent J. Hellendoorn
University of California, Davis
vhellendoorn@ucdavis.edu
ABSTRACT
Machine Learning models from other fields, like Computational
Linguistics, have been transplanted to Software Engineering tasks,
oftenquitesuccessfully.Yetatransplantedmodel’sinitialsuccessat
agiventaskdoesnotnecessarilymeanitiswell-suitedforthetask.
In this work, we examine a common example of this phenomenon:
the conceit that “software patching is like language translation”.
Wedemonstrateempiricallythattherearesubtle,butcriticaldis-
tinctions between sequence-to-sequence models and translation
model: while program repair benefits greatly from the former, gen-
eralmodelingarchitecture,itactuallysuffersfromdesigndecisions
built into the latter, both in terms of translation accuracy and di-
versity.Giventhesefindings,wedemonstratehowamoreprinci-
pled approach to model design, based on our empirical findings
andgeneralknowledgeofsoftwaredevelopment,canleadtobet-
tersolutions.Ourfindingsalsolendstrongsupporttotherecent
trend towardssynthesizing editsof codeconditional onthe buggy
context, to repair bugs. We implement such models ourselves as
"proof-of-concept" toolsand empiricallyconfirm thatthey behave
inafundamentallydifferent,moreeffectivewaythanthestudied
translation-based architectures. Overall, our results demonstrate
the merit of studying the intricacies of machine learned models
in software engineering: not only can this help elucidate potential
issuesthatmaybeovershadowedbyincreasesinaccuracy;itcan
also help innovate on these models to raise the state-of-the-art
further. We will publicly release our replication data and materials
at https://github.com/ARiSE-Lab/Patch-as-translation.
CCS CONCEPTS
•Softwareanditsengineering →Softwaremaintenancetools .
KEYWORDS
neural machine translation, big code, sequence-to-sequence model,
automated program repair
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’20, September 21–25, 2020, Virtual Event, Australia
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416587ACM Reference Format:
Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu, and Vincent J. Hel-
lendoorn.2020.PatchingasTranslation:theDataandtheMetaphor.In 35th
IEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE
’20),September21–25,2020,VirtualEvent,Australia. ACM,NewYork,NY,
USA, 12 pages. https://doi.org/10.1145/3324884.3416587
1 INTRODUCTION
Recentworkhasappliedawidevarietyofmachinelearningmodels
to practical software engineering tasks, including code completion,
automatedprogramrepair,andcodecommentgeneration.These
models excel at learning general patterns from large amounts of di-
verse data, even when training data is relatively unstructured. This
combination enables one to simply transplant successful models
from related fields, e.g.,from computational linguistics, to software
engineering.Yet,evenifthesemodelsprovidereasonableperfor-
mance, the transplanted model may still not be appropriate for the
task;manyofthesemodelsweredesignedforparadigmsthatdiffer
subtly, yet significantly.
In this work, we conduct a systematic empirical case-study to
illustrate how transplanted models can fail in the targeted task
domain, focusing specifically on the concept of “patching as trans-
lation” as a typical example of this phenomenon. A range of recent
work has adopted neural machine translation (NMT) models to
learn to repair programs by “translating" the buggy code to the
repairedcode[ 5,6,22,30].Wearguethattherearethreegeneral
concernswiththistypeofapproach,andshowconcretelyhowthese
manifest in “patching as translation” through empirical analysis:
Task design: Deep Learning (DL) models transform their inputs
intoacompactsetoffeaturesthatstorestheimportantinformation,
which it then uses to produce the required target. A wide range
ofDLarchitectureshavebeenproposedthatdoso,butregardless
of the specific architecture or task, it is self-evident that all the
relevant information needed to generate the target must alreadyexist in the input. While that is (largely) a fair assumption for
naturallanguagetranslation,wherewecanassumethattheinput
& output sentences express the same idea, it is questionable for
source code repair: we show evidence that buggy fragments often
lack the information required to repair them. Reliably choosing
the correct repair may even be impossible without access to a very
broadcontext(includingsurroundingfiles),intheabsenceofwhich
this task is inevitably ambiguous for many real-world bugs.
Architecturaldesign: Givenataskwheredeeplearningisfeasible,
onemustchooseamodelarchitecturethatsupportsthetransfor-
mation from input to output, in as realistic and simple a manner
aspossible.Thisisdonebyensuringthatpriorknowledgeofthe
2752020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
task (including dependencies and structural properties) are built
intothemodeldesign.Architecturesformachinetranslationrely
heavily on the auto-regressive nature of text: language is generally
producedoneword(ortoken)atthetimeinleft-to-rightmanner,
e.g.,inspeechorwriting;thestandardNMTencoder-decoderarchi-
tecturegeneratestranslationscorrespondingly.Whilethisworks
very well for NMT, its relevance to practical program repair is ten-
uousatbest:empirically,manyrepairsjustcopy(nearlyall)tokens
fromthebuggyline,withveryfewchangedtokens(oftenjustone).
Assuch,bothbugandpatchsharealargeidenticalprefix,butthe
differenceinthesubsequenttokensiscrucial.Wedemonstratethat
models struggle to predict this transition, as the large amount of
copying distracts (and inflates) the training quality signal.
Objective design: finally, models are trained by computing a loss
fortheirpredictionsrelativetoa“gold"output,usinga lossfunction.
This function is usually a differentiable proxy for the actual quality
of themodel, because suchqualitative assessments tendnot to be
differentiable.Inmachinetranslation,thetraininglossisusually
basedontheprobabilitiesofthecorrecttoken;theactualquality
of the trained model is measured with BLEU scores (or the like)that measure overlap between the generated and ground-truth
translation. However, such overlap measures are inappropriate for
program repair. For reasons stated earlier (few token changes, lack
ofcontextualinformation),thequalityofaproducedrepairoften
correlates very poorly with the number (and placement) of tokens
itshareswiththedesiredoutput.Forinstance,thetrainedmodel
emits many syntactically incorrect repairs, as well as many verysimilar patches for a given bug, rather than exploring a range of
alternatives.Thisyieldspoorperformanceinasearch-basedsetting
in which they are popularly used (given failing test cases).
Having studied these concerns empirically, we provide strong
evidence that the metaphor of “patching as translation" is inap-propriate for the task. On the other hand, our results shed lighton the effectiveness of a recent competing approach: repairingbugs by synthesizing editsof code conditional on the buggy con-
text[8,29,31].Notonlydoesthisapproachagreebetterwithour
empirical findings, we implemented a general form of this archi-
tecture, which predicts insertions and deletions relative to the bug
rather than theentire patch, and showthat this change inparticu-
lar produces a model that both generates the correct patch moreoften, and provides better sampling behavior (
i.e.,higher top-K
prediction accuracy using beam search). We also implemented a
generic contextual extension (compatible with both models) to as-
sesswhetherourempiricalfindingsoftheimportanceofcontext
couldbeintegratedeasilyintothemodels;thisenhancementprovedlesseffective,likelybecauseexistingmodelsstruggletomodellarge
windows of context. This finding highlights that empirically ob-
serving an issue and effectively addressing it are not always the
same; we leave this challenge for future work.
2 BACKGROUND
To study the transplanting of architectures from neural machine
translation (NMT) to automated program repair, we need to un-derstand both domains. In this section, we first discuss NMT –
its conceptualneeds and correspondingarchitectural designs, andthen Automated Program Repair – its empirical characteristics and
practical use.
2.1 Neural Machine Translation
NMT aims to convert an expression in a source language ( e.g.,Eng-
lish) into a semantically equivalent expression in a target language
(e.g.,French). This is generally both quite feasible and fairly deter-
ministic: a given English sentence almost certainly has a FrenchtranslationthatisbothagoodFrenchexpressiononitsownand
preservesalltheinformationintheoriginalEnglishexpression(i.e.,
it could be translated back to a comparable English phrase).1
A natural fit for this task is the encoder-decoder architecture,
whichconsistsoftwocomponents:1)anencoderthatlearnstocom-
pactly encode the important information from the source language
expression,and2)adecoder,whichtransformsthatinformationinto
anequivalentexpressioninthetargetlanguage.Encoder-decoder
stylemodelscanaddressmanytypesoftransformationsbetween
two domains ( e.g.,from image to textual description) and are typi-
cally instantiated with specific encoder-decoder architectures for a
givenproblemthatreflectsomeknowledgeaboutthatproblem’s
domain.Thissimplifiestheotherwisecomplextaskofrepresenting
andproducingaverywiderangeofinputs.Forexample,incom-
putational linguistics, sequence-to-sequence (seq2seq) [28] models
are a well-establishedway to generate text one tokenat a time, in
aleft-to-rightmanner–thislinearorderreflectshowlanguageis
often generated in speech and writing. Seq2seq models exploit this
structure by both representing and generating expressions witha strong emphasis on the left-to-right relations between tokens;
especially in the decoder component, which (in nearly all popular
models) produces tokens “auto-regressively", meaning that tokens
are produced one by one, and every previously generated token is
fed back to the model to produce the next token.
Practical seq2seq models. Seq2seq models have achieved great
success in the NMT field. Recurrent Neural Networks (RNNs) were
popularfor manyyears, buthad difficultiesin rememberinglong-
term dependencies. In an RNN, all the information of a source
sentence is encoded into a hidden state from left to right; the final
hidden state is then passed to the decoder, which attempts to re-construct the target expression from this information. This puts
inordinate strain on that single hidden state, which tends to cause
themodeltoforgettokensseenlongago.Longshort-termmemory
(LSTM) [15] and gated recurrentunit (GRU) [ 7] wereintroduced to
mitigate this bottle-neck by better separating long-term and token-
specificinformation,anddidsignificantlyimprovetheperformance
of RNN-based NMT models, but ultimately suffered from the same
concerns.Attention-basedmechanisms[ 4]wereintroducedtoal-
lowthedecoderto“attend"toanygivenintermediatestatefromthe
encoder(ratherthanonlythefinalone),whichgreatlyimproved
performance.Mostrecently,theTransformer[ 32]modelgeneral-
izedthisideatorelyingentirelyonattentionmechanismstoboth
encodeinputsandgenerateoutputs.TheTransformermodelpro-
poses multi-headed (self-)attention interspersed with feed-forward
networksthatenablesbothencoderanddecodertoattendtoany
1Inpractice,contextissometimesrequired, e.g.,todetermineifanexpressionismeant
sarcastically, which may alter its translation. There can also be multiple valid transla-
tionsforoneexpression( e.g.,literalvs.idiomatic).Evenso,generatedtranslationsthat
overlap strongly with the ground-truth are rated highly by human translators [23].
276set of tokens across arbitrarily long distances. These models are
also highly parallelizable. We adopt this model in our work.
Seq2seqmodelsinSE. Hindleet al.observedthatsourcecodeis
“natural"[ 14],viz.,withstronglocaldependenciessimilartonatural
languages likeEnglish. Many languagemodels have beenapplied
to software engineering tasks. More recently, this includes a range
of applications of the seq2seqarchitecture in modeling source code.
ExistingworkhasexploitedtheirpotentialinseveralSEtasks,such
ascodesummarization[ 16],codemigration[ 10]andprogramre-
pair [5,6,22,30]. The prevalent approach is to treat source code
as a sequence of tokens with implicit or explicit structures ( e.g.,
abstract syntax trees) [ 5]. The encoder learns the distribution of
such structured language, which is then translated into the tar-
get domain, either program languages (PL) or natural languages
(NL).Existingworkhasespeciallyspenteffortonlearningmean-
ingfulcoderepresentationstoadapt seq2seqmodelsforsourcecode
modeling [ 2,3,34]; these approaches focus on encoding rich char-
acteristics of programming languages, besides just the tokens. Our
worktakesamoregeneralview,andaimstostudythefeasibility
ofconstructinggoodtoolsbasedonboththeproblemdesignand
generalmodelarchitecture,whichislargelyorthogonaltolearningbetterrepresentationsofcode.Werecognizethatresearchesinboth
directions arenecessaryand significantto improvethe efficacyof
deep learning approaches in SE.
Relevance of Models to Tasks. All these models excel at learning
generalizablepatternsfromlargeamountsofdiversedataandare
prima facie at least somewhat applicable to source code, to the
extent that it reflects natural language characteristics. However,different tasks come with their own concepts and peculiarities,
andthemodelsshouldreflectthephenomenaspecifictothetask.
For example, code summarization and code migration are more
likeNLtranslationtasks,sinceboththeirgoalsaretoencodeand
preserve the semantics of their inputs (code fragments), just in
different vocabularies (concise natural language, and another code
contextrespectively).Ontheotherhand,softwareengineersbehave
differently when repairing a program. Developers tend to fix abuggy fragment by making minor changes rather than entirelyrewriting it. Furthermore, the semantics of the buggy fragmentare by definition not preserved; the express goal is to introducesemantically new content (and possibly remove some) so as to
changethemeaningofafragment.Noneofthisdisqualifiestheuseofseq2seqmodels perse,butitsbuilt-inassumptionsshouldatleast
be carefully evaluated empirically, and, if necessary, its application
should be changed to better reflect the domain.
2.2 Automated Program Repair
Automated program repair (APR) is a task of keen interest in SE.
Theaimistofixsoftwarebugswithminimalhumanintervention.
Classic APR techniques can be categorized into 1) generate-and-
validate(G&V)[ 9,17,24,25]or2)synthesis-basedapproaches[ 20].
G&Vapproachesautomaticallygeneratepatchesandvalidatethe
candidatesusingasetoftestcasesthatrevealsthebug.Togenerate
fixes,oneeffectiveapproachistomutate( e.g.,insert,replace)the
buggycodeaccording tocodesnippetsin thecurrentprojectthat
occurinsimilarcontexts[ 17].Synthesis-basedapproachescreate
constraints that satisfy all test cases, and then solve them and
produce patches from the solutions.NMT for APR.. Tufanoet al.[30] proposed to use machine trans-
lation to repair programs and empirically studied the feasibility of
translating buggy programs into fixed ones. They applied multi-
layerRNNswitheitherLSTMorGRUnodestopredictpatchesofab-stracted,realbugs,andreportpromisingperformance.Chen
et al.[6]
subsequentlyintroduced SequenceR,anend-to-endframeworktore-
pairone-lineJavabugs.TheyusedNMTmodelstolearntheimplicit
bug-repairpatternsbytrainingthemodelwith35kbug-fixpairs.
Besidesthebuggylineitself,theyalsoconsideredcodecontextto
allowlong-rangedependenciesinfixes;theyincludetheentireclassinwhichthebugislocated,whichthey abstracttoreducetheinput
size. CODIT [ 5] developed a tree-based NMT model to produce
codeeditsandbugfixes.Itfirsttranslatesthetreestructureofcode
andutilizesthestructuralinformationtoassistthegenerationof
codetokens.CODITalsoincludesthetreenodesaroundthebugas
contexttopredictmeaningfulpatches.CoCoNuT[ 22]ensembles
multiple NMT models to capture diverse fix patterns. The authors
argue that incorporating context is essential for fixing bugs, so
theyapplyaseparateencoderspecificallyforlearningthebuggy
context.
Although these existing works apply a wide range of models,
theyalltreatprogramrepairasatranslationtask;thesetoolsencode
a limited program window around a bug and learn to transform
ittorepairedcodebasedonhistoricalrepairs.Thepremiseisthat
translation is both a suitable model and that the buggy code (with
its context) provides sufficient information to succeed. It is thus
pasttimetoaskthefollowing,high-levelquestion,whichhasyet
to be addressed from an empirical perspective:
RQ1.Is it generally feasible to translate buggy programs to repairs?
While these approaches all indicate that Seq2Seq learning holds
promise for learning patterns of transformation between bugs and
patches, they struggle to outperform many G&V tools that applied
human-designed rules to fix defects. One explanation is that the
searchspaceofrepairsisprohibitivelylarge[ 21],amongothersdue
tothelargeandhighlylocalvocabularyandpatternsendemicto
software [ 12], as well as the length of buggy fragments. Intuitively,
however, the search space need not be so large at all: in real-world
development, modifications made to code during repair are mostly
small,limitedtoafewtokensratherthancompletelyreconstruct-
ingastatement.Istherelianceontranslationputtingmodelsata
disadvantage by artificially expanding the search space? It is again
worth determining this empirically, by asking:
RQ2.Do machine translation architectures mischaracterize real-
world fixing behavior, and does this disadvantage their performance?
Deep Learning for APR.. Besides translating buggy code to fix
it, recent work has proposed deep learning models that learn to
specify the buggy locations that need to be modified together with
theeditstobemade.DeepFix[ 11]implementeda seq2seqattention
networktofixcompilererrors.Asinput,theprogramisrepresentedasasequenceof(linenumber,tokens)pairs,andthemodelpredictsasingle(buggylinenumber,patch)pairasarepair.Vasic
et al.[31]
proposed using pointer networks [ 33] to jointly learn to localize
and repair a specific class of bugs known as VarMisuses. Their
networkjointlypredictstwooutput“heads",onetolocatethebuggy
token and one for its replacement. Tarlow et al.[29] introduced
277an edit-based model called Graph2Diff that uses a graph neural
networkasanencoderandaTransformerasdecoder.Thismodel
transformsaprogramgraphintoa ToCoPosequenceofASTedits
that transform the buggy program into a repaired version.
By directly learning the locations of incorrect tokens and the
edits to be made, these edit-based methods provide an approach to
learning bug fixing with a very different loss, which is not trivially
reduced by maximizing the token overlap between the bug and
repair.Theimpactofthislosscanbesubstantialindeterminingthe
kind,androbustness,oflocalminimathattheneuralnetworkfinds
during training. We thus implement a simple version of this model
ourselves to empirically study the impact of the objective function
on both our baseline model and this edit-based model. This way,
we study the impact of the objective function on the models by
studying the models’ results themselves, asking the following:
RQ3.How well does the NMT objective function apply to Automated
Program Repair?
3 METHODOLOGY
Thegoalofthisworkistoprovideanempiricalandconceptualanal-
ysis of the relevance of deep learning models (originally developed
forNMT)inSEcontexts.Assuch,weemphasizethatitis notourgoal
toproduceastate-of-the-artbugdetector,orreplicatepriorwork.
Rather, we identify a general, representative approach (seq2seq for
program repair), that reflects a direct adoption of models from a
related fieldtoSE tasks,and studyits limitations.Naturally,prior
workhascoveredawiderangeofapplicationsandmodificationsof
this method, and may be immune to some of our findings, but this
does not discount the general result of our analysis: that adapting
deep-learning models designed for other fields to SE requires a
principled, empirically and conceptually grounded approach.
3.1 Scope
Concretely, we focus on a relatively simple form of automatedprogram repair in which we translate a given buggy line to its
repairedcounter-part.Wethusassumethatwehavethebugalready
localized and that it is confined to exactly a single line. This is the
mostdirectformof“repairastranslation”,inwhichanoff-the-shelf
translationmodelisusedontwosoftware“sentences":thebuggy
version and the repair.
3.2 Data
We collect our bugs from the history of the 10,235 most-starred
JavarepositoriesonGithubonMarch30th,2020.Weanalyzedeach
project’s entire commit history and extracted any commits that
altered precisely a single line in a single Java file, disregarding any
(spurious) changes to whitespace. We then compared the corre-
sponding commit messages against a relatively simple keyword-
based check [ 26] to heuristically find commits labeled as e.g.,“fix"
or “bug". We note that, although this heuristic is not particularly
precise, the characteristics we found in our data were very similar
betweenthosemarkedasfixesandotherone-linechanges,soweex-pectthistohavelittleimpactonouranalysis.Thisprocessresulted
inca.60,000 bug fixes across 8,644 projects in our dataset. In the
course of our analysis of this data, we manually checked a number
of the collected samples and confirmed that the vast majority of
these were indeed bug fixes.3.3 Experiment Setup
Given the collected dataset, we first analyze the characteristics
ofrealfixesandthentrainNMTmodelsonthesesamplestopre-
dict patches. To answer our research questions, we study bothcharacteristics of the real-world bug-fixing behaviors and of the
model-generated patches.
3.3.1 Bug context. Wedesignexperimentstoexploretheimpor-
tance of a bug’s lexical context when fixing defects. In natural
languages,context(thetextsurroundinganexpression)hasadirect
effect on the way people understand a specific expression and can
helpavoidambiguityincommunication.Similarly,thecontextof
a buggy line is the code surrounding, as in, both preceding and
succeeding, the bug. This can variously be chosen to include up to
Nlines of code above and below the bug, the surrounding func-
tion,oreventhewholefile(orproject).Thiscontextcanprovide
vital information ( e.g.,variable definitions, conditional statements)
for understanding the defect and the necessary repair. We studythe role of variously sized contexts for both disambiguation and
providing necessary information in section 4.1.
3.3.2 Similarity analysis. Wenotedearlierthatsoftwareengineers
tend to make small changes when fixing defects, likely because
bugs correspond to only minor flaws in the code, and perhaps also
becausemakingfewchangesreducestheriskofintroducingnew
bugs. Given this, we evaluate the similarity between real bugs and
patches empirically across three similarity metrics:
(i)Editdistance (precisely, Levenshteindistance)isametricthat
quantifiesthedifferenceoftwosequencesbytheminimumnumberofedits(deletions,insertions,orsubstitutions)requiredtotransform
one into the other.
(ii)Jaccard similarity (effectively intersection-over-union) cal-
culatesthe ratioof overlapping n-gramsbetweentwo sequencesdivided by their union. Jaccard similarity is usually just applied
totoken-levelsimilarity;weextendittotheaverageof1through
4-gram overlap to better capture both token-level similarity and
matches in their ordering.
(iii)Bilingual evaluation understudy (BLEU) is popularly used to
evaluatethequalityofmachinetranslations,andisconsideredto
have a high correlation with human assessments of similarity [ 23].
Thisisanasymmetricmeasurethatcaptureshowsimilarthemodel
prediction (the “hypothesis") is to the ground-truth translation
(the“reference").BLEUalsocountsn-grammatchesbetweenthe
prediction and the ground-truth, and normalizes these w.r.t. the
predicted sequence length, which causes the asymmetry.
These three metrics above will be used frequently across our
analysis to measure similarity in different aspects.
3.3.3 Model training and BPE. ToinspecttheperformanceofNMT
modelsonprogramrepair,wetrainedandevaluatedavanillaTrans-
formermodel[ 32]onourdataset.Wesplitthewholedatasetinto
three parts across organizations, train/valid/test, with a ratio of
90%:5%:5%. We trained the model on the ca. 55K bugs in the train
set, optimized it for held-out performance relative to the valid set
and finally evaluated the performance on test set.
Inthefieldofnaturallanguageprocessing, Byte-pair-encoding
(BPE) [27] is a widely used method to encode rare and long words
intofrequentsub-tokens;thisway,tokensthatwerenotseeninfull
278duringtrainingcanstillbepredictedaccuratelyatinferencetime.
BPE splits a word ( e.g.,"coding") into a list of more frequent sub-
tokens(e.g.,[’cod’,’ing’]).Inprogramminglanguages,vocabulary
innovationisevenmorerampant,asdeveloperstendtonameavari-
ableormethodusingacombinationofwords( e.g.,isNullOrEmpty )
[12]. Karampatsis et al.[18] show that BPE can effectively address
thisissueinbigcodeapplications,soweapplythistoourmodel
input as and predictions as well.
4 ANALYSIS
In this section, we empirically analyse the characteristics of pro-
gram repair on real-world bug fixes with a joint focus on the rela-
tiontonaturallanguagetranslationandonfactorsthatinfluence
accuracy for program repair. In particular, we study the character-istics of the ground-truth data (
i.e.,the real bugs and patches with
their context), and of the patches generated by our NMT model, as
trainedinSection3.3.3.Fortherestofthissection,wescrutinize
theadequacyoftranslationasaparadigminSection4.1,weidentify
architectural concerns in Section 4.2, and we quantify their impact
on model performance in Section 4.3.
4.1 Task Design
Translation,asatask,isintendedtofacilitatecommunicationacross
languagebarriers.Hence,bydesign,itmustpreservethesemantics
between the source language, and the target language—any trans-
lationthatchangesthesemanticsisunacceptable.Bycontrast,in
programrepair,thesemanticsofsourceandtargetaremeanttodif-
fer, as the buggy version contains incorrect program behavior that
thefixedversionissupposedtocorrect.Todothat,engineersdelib-
eratelychange(add/delete/replace)incorrecttokenswithcorrect
ones. Imitating such changes with a machine learner is non-trivial,
especially since the learner usually only has access to the bug and
the fix, but not the knowledge latent in the developer’s mind to
reason about the fix.
For instance, the fix may introduce, de novo, tokens that are not
inthebuggylines, e.g.,anewAPIcall.Insuchcases,amodelhas
to learn to pick those new tokens from across its entire known
vocabulary. If the replacement is a common fix pattern, this might
beeasyenoughtolearn;otherwise,thisleadstoavastsearchspaceofcandidaterepairs.Thelattercaseiscommonenough;developers
oftenusemethodsoftheirowncreation,definedinadjacentfiles,
or string or numerical patterns specific only to that project. It isthus important to quantify, even just approximately, how much
informationthe modelneedsand howmuchit hasaccessto from
its training data, which tends to comprise the buggy lines and
anoptionalwindowofcodecontext.Althoughitisnon-trivialto
inspect the learned “black-box" model and extract what it infers
about a given buggy line, we can identify the gapsbetween "what
program repair needs" and "what machine translation can supply".
4.1.1 Program repair needs (lots of) contextual information. Patches
that introduce newvocabulary (relative to thebuggy line) require
the model to conjure up novel tokens, ex nihilo. Given that code
vocabulary is highly diverse and often strongly specific to a given
project,packageandfile[ 12],doingsofromthebuggylinealone
may require an unreasonable level of ingenuity from the model.
Table1quantifiesthis:first,nearly90%ofpatchesintroducenew
vocabulary relative to their buggy source, which is true regardlessofsub-tokenization(evenusingtheBPEapproach).Furthermore,
thesearenotatalljusttypicalprogramtokensorlocalvariables;wepairedthebuggylinewithincreasingwindowsofcontext(explained
in Section 3.3.1) and find that the unseen tokens introduced by the
patch are still rarely borrowed from any immediate buggy context;
theyaresometimespresentinthefileasawhole,butinlocations
farawayfromthebug.Nearbytokensareabitmorelikelytoshare
some sub-token(s) with the patch, but rarely provide the entire
missinglink.Giventhatmodelinglargevolumesofcode( i.e.,many
hundreds, or thousands of tokens) at once is often prohibitively
expensiveforcurrentdeeplearnedmodels,thiscanseriouslyaffect
models that incorporate only modest levels of context, such as the
surrounding few lines or function.
Table1: Ratioofpatcheswithnewvocabularyrelativetothebuggy
snippet given a context window that ranges from None(i.e.,only
buggy lines – a typical translation setting), to a given number of
lines (symmetrically around the bug), and finally to the entire file.
Context includedPatches introducing unseen tokens
without BPE with BPE
None 89.5% 86.2%
10 lines 73.0% 64.2%
20 lines 68.7% 59.9%
Whole file 49.5% 38.6%
This is not merely a matter of richer training data either; a large
proportion of project-specific tokens are not found in any other
projects[ 12],soitisquiteunlikelythatourmodelwouldhaveseen
many of these at training time. We note that this is in contrast
to other paradigms of program repair, such many G&V models,
which instead searchfor patches from across many surrounding
files,ratherthanaimtoencodeacontextdirectlyintoatranslation.
4.1.2 Without context, program repair is inherently ambiguous. As
discussed, a learner would certainly struggle to capture enough
informationfromthebuggyprogramalone.Fortunately,theselearn-
ers are equipped with the capacity to transfer many insights from
theirtrainingdatatonewexamples.Perhapstheycanpredictthe
missing semantic information from those bug-repair pairs?
Although it is again impossible to quantify what the model can
do,wealsoagainarguethatitisperfectlysoundtolower-boundits
potentialbyestimatinghowmuchoftherequisiteinformationit
hasaccesstoattrainingtime.Concretely,thetrainingdatacontainsmany“similar"bugstothoseseenattesttime(whichwewillquan-tifyinvariousways),sothemodelmightlearnthetransformations
that produced patches from those similar bugs and apply the same
insight,e.g.,topredictthemissingvocabulary.But,thisiscontin-
gentonsimilarbugsindeedproducingsimilarrepairs;ifrepairsfor
similarbugsroutinelydiverge,thenthemodelisreasoningabout
highly ambiguous data and will have to learn a wide range of valid
transformations for a single defect in the training data.
Tosimplifythis discussion,let bbeabugin theheld-outpor-
tionofourdatasetand pbethepatchof b.Assumingwehadsome
oraclethatcanprovide“similar"bugs ˜b(withpatch ˜p)forb,specifi-
callyfromourtrainingdata,wewouldideallyexpectinformationabouttherelativechangeneededtorepair btobetransplantedfrom
279(a) Intersection-over-union
 (b) BLEU
Figure1: Correlationbetweenbugssimilarityandpatchessimilar-
ity. X-axis indicates the bug/similar-bug similarity, and y-axis indi-
cates the patch/similar-patch similarity. The corresponding countof each grid is normalized on a log scale.
˜p.Ifthatisgenerallytrue,thenourmodelcanlearnsimilartrans-
formations for similar bugs and thereby generate new vocabulary
and patterns that are not present in the buggy context.
To quantify this, we find the top-3 most similar bugs for each
bintheheld-outusing4-gramJaccardindex(seesection3.3.2),
which we label ˜b1,˜b2,˜b3, among bugs in the training data. We
then extract the corresponding pand˜piand evaluate the patch
similarity SIM[p ,˜pi] in relation to the bug similarity SIM[b ,˜bi]. To
facilitate transferring repair patterns, we should hope that similar
bugs produce similar repairs. We visualize this as a heat map (Fig-
ure1)toshowthecorrelationbetweenbugs’similarityandpatches’
similarity:foreachgridintheheatmap,wecountthenumberof
sampleswithSIM[b ,˜bi]andSIM[p ,˜pi]inthecorrespondingrange,
andthencolorthegridbasedonthesecounts.Tomakethecolor
contrast more identifiable, we log-normalize the counts.
Wecalculatethe b→˜bandp→˜psimilarityscoresusingboth
theJaccardindexandBLEUscores;thelatterismoreappropriate
for translation because it is asymmetrical, capturing the overlap
fromtheperspectiveofthetranslationtarget.Thisalignswellwith
the task’s directionality: we want to quantify what information
is transferred from training to held-out, not vice versa. Theresult is shown in Figure 1; both metrics yield a similar pattern:
bug-similarity only partially correlates with patch-similarity. Both
graphs show a “smeared out" pattern in which similar bugs tendto produce patches with typically less similarity, rather than astrongly pronounced diagonal, that would indicate that patches
relatetooneanotherastheirbugsdo.Worse,manybugshaveonly
neighbors with low similarity to begin with. These lower scorestend to just reflect spurious overlap due to the large portion of
“closed-vocabulary"tokens( e.g.,brackets,keywords)insourcecode,
which is also evident from the main hotspot being at (0.25, 0.25).
Weareparticularlyinterestedinpairsthatsharearelativelylarge
number of tokens and patterns; i.e.,those with similarity scores
greaterthan0.5.Forexample,thecode:“ private boolean isName
= false; "andcode:“ private boolean isName = true; "yield
a BLEU-score of 0.57, and they indeed look alike (only differ in
boolean value). If similar bugs are (predominantly) fixed in similar
ways, then we should expect that to translate into high patch simi-
larity, which would allow the model to copy the appropriate repair
patterns. Unfortunately, Table 2, which breaks down the highlysimilar bugs specifically, paints a different picture: here too, the
similaritybetweenbugshasnearlynodiscerniblerelationtothatTable 2: Similar bugs do not always have similar patches.
bug/similar-bug BLEU # Samplespatch/similar-patches BLEU
<0.5 ≥0.5
≥0.5 2038 66.0% 34.0%
≥0.6 1143 62.3% 37.7%
≥0.7 561 54.2% 45.8%
≥0.8 258 46.1% 53.9%
1 173 49.1% 50.9%
of their patches. Even highly similar bugs’ patches do not score
above0.5halfthetime,whichisactuallylowerthantheirrespectivebugs.Forinstance,acommonbuginourdataset,"
LOG.error(e); ",
presentswith manydissimilarpatchesincluding " LOG.warn(e); "
and"LOG.error(‘‘Can’t read settings for " + tool, e); ".
TheBLEUscorebetweenthesetwopatchesisjust0.12,andwecan
tellthat thisbug wasfixed withvery differentintentions.In other
words,relyingonsimilarbugstotransplantpatchinformationis
almost entirely ineffective.
Thisdemonstratesasubstantial inherentambiguity inprogram
repairbasedonjustabuggyline(thoughnotnecessarilytoprogram
repair in general): for a given bug, the learned program repairhistory provides a mixed signal of many candidate repairs withdistinct semantics. This matches our intuition as well: just how
agivenfragmentisbuggy,andwhatspecificrepairamongmany
valid semantic transformations is appropriate depends on a vast
array of factors, many of which are not enshrined in the code at
all (e.g.,project requirements, developer preferences), let alone the
buggy line (or even function) itself.
4.1.3 The challenges of new vocabulary. Finally, it might still be
feasible for the model to “guess" at novel tokens and break theambiguity if they can be constructed fairly obviously from the
context,e.g.,byapplyingknowntransformationstoexistingones,
likeconvertingsingulartopluralorincrementingaprovidedinteger.
Whereas the former results provided a lower-bound on what is
feasible, it is quite impossible to quantify precisely what the model
“could do", as the patterns learned by its millions of parameters
can be highly complex. So we instead use the model’s performanceitself(studiedinmoredepthinsection5)asanempiricaldatapoint:giventhatitistrainedcarefullyandwithamplecapacity,weshould
expectthatitprovidesatleastevidenceofthisabilitytoproduce
correctnewvocabulary.Incontrast,westudiedthetrainedmodel’s
accuracy on our 2,599 test samples; the patch introduced one or
moretokensnotpresentinthebuginover75%ofthecases,yetthe
modelpredictsthisnewvocabularyonly5.6%ofthetime.Worse,
many of the “new" tokens are not even entirely new; they may just
constitutetheadditionof nullcheck,whichthemodelstilldoes
notanticipate.Evenwhen(beam)searchingacrossthetop25most-
probable sampled patches, the model only anticipates 14% of the
requirednewvocabulary.Westressthatthisisawell-trainedmodel,
whichwasabletoachievehighaccuracyonitstrainingdataand
forwhichweusedthemostgeneralizablecheckpointaftertraining
for 100 epochs. As such, machine translation models are already at
a serious disadvantage here compared with NLP applications. This
allows us to conclude our investigation of RQ1:
280The lack of information in the training data, vocabulary, and
immediatecontextmakesrepairingastranslationinitscurrent
form largely infeasible.
4.2 Architectural Design
Our second point of concern with translation models for program
repair relates to the structural constraints assumed inherent in nat-
urallanguagegeneration:thattextisauto-regressivelyproduced
left-to-right. This constraint is built into the translation model’s
(sequence-to-sequence)architectureandimpliesthatasimpleadop-
tion for program repair requires the model to output the entire
repair, producing the correct token at each point.
Theflawwiththisparticulardecisionisdifferentfromtheone
in Section 4.1 in that it does not affect the feasibility of the task
(generating the entire repaired line is just as possible as e.g.,gener-
ating the change only). Instead, architectural mismatches between
the model and the task impact the difficulty of training and the
corresponding rate, and even the ultimate limit, of convergenceon test data. This is because a) our models do not have infinite
capacity and b) stochastic gradient descent is a local optimization;
thus, these modelstend to find alocal minimum that matchesthe
signalconveyedbythelossfunction.Ifthislossfunctionprioritizesexactrepetitionofmanytokensfromtheinput,orastrongreliance
on left-to-right production, this may negatively affect the actualquality (
e.g.,overall accuracy) of the ultimate local minimum. In
this section, wequantify this effect from thedata statistics; in the
next,weexploreitfurtherbasedonthemodel’sconvergentquality.
4.2.1 The patch preserves most of the tokens in the bug.2Bug-fixing
modificationstocommittedcodeareoftenminor;thebuggyline
usually is already persea close approximation of the correct code,
withverysubtle,minorflaws.Toquantifythisassertion,wefirst
measurethesimilaritybetweenrealbugsandpatches.Weusethree
different metrics to evaluate the similarity of each (bug, patch) pairinourdataset,outlinedinsection3.3.2:token-leveleditdistance,(1-gram)intersection-over-union(whichcontributesadenominatortotoken-leveloverlap);andfinally,meanBLEU-4similaritytobalance
the overlap between tokens and sequences.
The results are shown in Figure 2. The average edit distance for
thesamplesinourdatasetis3 .29,butthedistributionislong-tailed
sothismeanissomewhatinflatedbythefewlargeedits;themediandistance is simply 1 – 51
.1% of the samples only edit a single token
to fix the bug, and 63 .6% of the samples have an edit distance up to
2.Thus,bugfixingmodificationsareoftenlimitedtojustaselect
fewtokens.Figure2bfurthershowsthatbugsandpatchessharethe
majority of their vocabulary as well: the average Jaccard similarity
is0.76,andhalfthetimethepatchreservesmorethan80%ofthe
bug’s tokens. This overlap extends to sequences of tokensas well:
the mean BLEU score of a patch relative to its bug is 0.61. Twolines of code are considered very similar when their BLEU score
is greater than 0.5, so bugs and their patches overlap strongly. For
reference, the state-of-the-art results in NMT at this time are ca.0.4, dependingon thelanguage pair. Program repairachievesfar
2This result applies to our study of small (one line) bug fixes; this may not hold for
larger patches, which may be more likely to reconstruct the whole buggy module.Table3: Proportionofrepairsinwhichthesyntacticstructuresre-
mains unchanged relative to bug, both for all samples in our train-
ing data and for those in which the patch both does and does notintroduce novel tokens (relative to the bug).
Setting Proportion
All bugs 52.4%
Patches introducing new tokens 56.2%Patches without new tokens 20.6%
higherperformancebysimplycopyingthebugverbatim;yet,doing
so would in no way approximate a goodrepair.
Thisalsoconfirmsourintuitionthatbugsandpatchesarehighly
similar, and patches retain most tokens from the buggy version,
rather than assembling code de novo. This principally suggests that
searchingforthecorrectpatchtokenbytoken,fromlefttoright
isapooruseofsearchspace;asmartprogramrepairtoolshould
justpredictwhichtokensaresupposedtobepreservedandfocus
onsearchingfortheonesthatrequiremodifications.Butisitreally
sobadtogeneratetheentirepatch;wouldcopyingthepreserved
tokens simply prove no concern for the models? We answer this
question in the negative in section 4.3; first, we further analyze the
typesof changes made in real repairs.
4.2.2 The patch tends to make minor changes to the bug’s syntax .
Grammars vary widely across languages. For example, subject-
verb-object sequences (“I eat an apple") are abundant in English,
butpeopleseldomusetheminverb-finallanguageslikeTamilor
Japanese.Because of thisdistinction, translatingby merelysubsti-
tutingwordsinonelanguagewithanotherisofteninappropriate.
Instead, neural architectures capture the syntactic transformation
between languages, as well as the translation of the underlying
words. A recent study [ 1] shows that the difference in word or-
deramongvariouslanguagesisasignificantfeaturethatmodels
learn, and e.g.,neural attention mechanisms are effective at this
task. We are similarly curious whether this feature is prominentin the conversion of bugs to patches, as such information giveshints about how to adapt machine translation models appropri-ately.Forexample,givenabuggyline:"
if (level >= damage -
damage / 2) " with patch: " if (level <= damage - damage /
2)"(arealsamplefromourdataset),wecanseethatthepatchdoes
not modify the syntax (in terms of the AST) of the bug, but only
changes its semantics by changing the underlying tokens. We thus
empirically study how often modifications that fix logical errors
introduce changes to the syntactic structure of code.
Given a pair of bug and patch, we tokenize the code and use
javalang package3to identify the syntactic type ( e.g.,identifier,
separator, integer) of each token. If a bug and patch have the exact
same token type sequence,their syntactic structure is unchanged.
Table 3 summarizes the resulting ratios. Slightly more than half
of our patches preserve the exact syntactic structure of their bugs.
Furthermore,theresultsarestarklydifferentbasedonwhethera
patch introduces new tokens (relative to the bug; see section 4.1.1);
those that do even more rarely (56.2%) change the syntax, while
the other patches are often some kind of permutation of the bug’s
tokens that very rarely preserves syntactic ordering.
3https://github.com/c2nes/javalang
281(a) Edit Distance
 (b) Intersection-over-union
 (c) BLEU
Figure 2: Different Similarity Metrics regarding (bug, patch) pairs. X-axis of each histogram indicates the similarity score w.r.t.different
metrics, and y-axis shows the ratio of samples within the corresponding range. The average edit distance between bugs and patches is 3.29.
The edit distances of 51.1% samples are 1 and 63.6% samples have an edit distance ≤2. The average intersection-over-union similarity is 0.76,
and 88.4% samples have a similarity ≥0.6. The average BLEU score is 0.61, and 72.5% samples have a BLEU score ≥0.5.
These observations have importantimplications. Forone, they
suggestthatsearchingforunseenwordsacrosstheentirevocabu-
laryisrarelynecessary;rather,themodelcouldsimplysearchfor
the tokens given a specific syntactic type [ 5];e.g.,many patches
replace just an operator to fix a bug. More generally, this suggests
that the left-to-right generation process is thus not just inefficient,
but all-but misdirected for such bugs: it requires the model to both
copyapreciseprefix,andthengenerateasinglealternativefrom
that context, where the original token was often already “close" to
being correct ( i.e.,in the right syntactic ballpark). Patching such
bugs, more than half of those in our dataset, in this way likely puts
inordinate and unnecessary strain on the model, which we will
quantify in the subsequent sections. First, we partially conclude
our second research question:
The machine translation architecture’s generation process is apoor fit for program repair, which frequently retains most to-kensfromthebugwhilereplacingjustafew,andfromasmall
candidate set.
4.3 Program Repair via NMT (Objective)
Finally, when generating natural language translations, the goalis to correctly predict as many words of the target sentence aspossible. The idea is that a translator that is likely to predict any
onewordgiventheinputandpreviouslypredictedwords(ifany)
is also likely to correctly generate the entire desired sentence by
simply repeatingthis process until termination.Indeed, this tends
to be quite accurate in general, in part because of the naturally
auto-regressive, Markovian nature of text; a given prefix typicallyhas only a small set of plausible continuations.
Giventheobservationsintheprecedingsections,thisMarkovian
assumptionseemsprecariousatbestforprogramrepair:thebug
and repair often share a large, identical prefix (and suffix) thatis then followed by incorrect tokens in the former and different,
corrected ones in the latter. As such, we must question the validity
ofanobjectivefunction(bothlossandmetric)thatvaluesper-tokenpredictionqualitysostrongly.Havingsaidthat,theaforementioned
observationsalonedonotprovethatthereisaproblemwiththis
transplanted objective; the buggy token(s) may simply have been a
particularlyunnaturalsuccessortoitscontext[ 26],fromwhichthe
Figure 3: Performance trends (dashed line) and per-epoch results
(points) on held-out data as training progresses, in terms of per-
token accuracy subject to teacher forcing, accuracy at generating
the complete sequence, and top-5 accuracy using beam search.
correctedtoken(s)do,infact,follownaturally.Inthissection,we
empirically assess this concern.
Specifically, if the Markovian, auto-regressive objective used in
naturallanguagetranslationisagoodfitforprogramrepairaswell,
we would expect two things to be true:
(1)The per-token accuracy under auto-regressive teacher forc-
ingcorrelatescloselywiththequality( i.e.,totalaccuracy)of
the produced patch. That implies that the model correctly
identifiesthe“challenging"tokens,thatneedtobealtered,as
these dictate the overall correctness of the resulting patch.
(2)Themodelefficientlyexplorestherepairspacewhensam-
plingmultiplepatches( e.g.,usingbeamsearch).Thatimplies
that choosing the repair point by first copying tokens un-
altered and then (auto-regressively) generating a different
continuation is no distraction to the model.
Weputboththeseexpectationsintothe(empirical)test.Figure3
showsfirsttheprogressionofvariousaccuraciesonourheld-out
dataoverthecourseoftraining.Atthetop,theteacher-forcedtoken-levelpredictionaccuracyincreasessteeplyearlyon,throughoutthefirstca.10passesthroughthetrainingdata,butafterthat,itall-but
282plateaus.Itdoes,infact,stillincrease,butonlyveryslightlyafter
epoch 10 (from ca. 83% to 85%). This clearly shows two “phases"
(abimodalpattern)intrainingthistypeofmodel:themodelfirst
trivially minimizes its loss (and thus achieves a high accuracy)
through simple copying, but then struggles to match that strategy
with predicting the correct change to achieve any more progress.
Thisinitialcopyingtranslatesintolittlerealaccuracy;theFull
Sequence( i.e.,completerepair)predictionreachesjust4.5%after10
epochs, making nearly all its substantial progress afterwards. This
has real training ramifications: we also visualize the progression
of the per-token entropy (transformed to probabilities). In the first
10 epochs, the model quickly becomes very polarized, assigning
high probability to the copied tokens; then it becomes clear that
thisyieldsverylowprobabilitiesforthefewchangedtokens,which
entropypenalizesstrongly.Asaresponse,themodelinsteadadopts
a more balanced prediction to achieve higher overall repair quality.
Toquantifythecorrelationsinthefaceofthisbimodality,anon-
parametric (Spearman’s rho) correlation test is in order. This does
showthatthetwometrics(per-tokenaccuracyandfullsequence
accuracy) are highly correlated ( ρ=0.914), even, though less so,
afterepoch10( ρ=0.863).Thelatterresultreflectsthattheremain-
ing per-token accuracy increase translates into a disproportionally
higher complete repair rate – the missing 2% token accuracy be-
comesca.7%completerepairaccuracy,nearlytriplethelevelsat
epoch10.Thisimpliesamixedanswertoourfirstpremise:thecom-
plete patch accuracy certainly follows theper-token accuracy, but
therelationisfarfromdirectandthelatterisahighlymisleading
metricin ipsodue to its bimodal nature.
Finally,thefigureshowsthattheoddsoffindingthecorrectpatch
in the top-5 generated samples is only a little higher than the top-1
prediction (ca. 5% points at most); that gap actually shrinks as the
top-1predictionbecomesmoreaccurate,whichsuggeststhatthe
beamsearchfindsfewgoodnovel/alternativepatches.Wewould
hopethat,giventhenaturalambiguityinchoosingthecorrectpatch,
the model learns to sample a diverse set of plausible corrections.
Instead,frominspectionofthegeneratedsamples,themodelpro-
duces many very similar candidates, usually differing by just a few
tokens.Thistooislikelyanartifactofthetrainingcriteria,which
prioritizes copying 80% of the tokens over predicting the correct
variation.Thus,weanswerourfinalresearchquestion:
TheobjectivefunctionsofNMTmodelsare inappropriate for
program repair, leading to reduced training efficacy on more
appropriate metrics.
5 SEQ2SEQ MODEL FOR PROGRAM REPAIR
Asafittingconclusiontoourempiricalandconceptualevaluationof
the basic “transplanted" approach to program repair as translation,
it is appropriate to try and redesign the existing approach. This
sectiondemonstrateshowobservingandquantifyingissueswith
an outside approach relates to principled and innovative modeling
design: while observing concerns does not guarantee that improve-
ments are straightforward (as we show in relation to context), it
canimproveperformancebybetterrelatingthemodeltothetask.
We do this below, by eschewing past practice of trying to generate
patch tokens directly, and instead generating edits.5.1 Model Changes
Weobservedthreemaindeficiencieswiththeexistingtranslation
approach: the inadequacy of relying on just the bug for enough in-
formation to produce a patch, the mismatch between typical repair
actions and generating the entire corrected line, and the related
divergenceoftraining-objective,betweenper-tokenaccuracyand
whole-repair(bothtop-1andbeam-search)accuracy.Hadwede-
signed a machine learning approach for this problem from scratch,
wewouldcertainlyattempttoincorporatebothbugcontext,and
a notion of repair editsto reflect these aspects of program repair,
ashasalsobeenproposedbysomerecentwork[ 29].Wepropose
tomake bothchanges: Figure4 showsthe twomain architectural
mechanisms we add to the base model to achieve this.
Edits:wemodeleditsdirectly,asatoken-level“diff"betweenthe
bugandpatch.Ouranalysisoftypicalchangesindicatedthatthe
bugandpatchnearlyalwaysshareasubstantialprefixand/orsuffix,
with the repair occurring at some point in the middle of the line.
Wethusparseeachbug/patchpairandfindthelongestoverlapping
prefix and suffix. Our model is augmented with two additionalpointersthat correspond to insertion and deletion; the original
decodercomponent(oftheencoder-decoderarchitecture)isnow
pressedintoservicetooutputthediff(ratherthandirectlygenerate
the raw tokens in the fix). There are three possible scenarios:No additions:
The prefix and suffix combined span the entire bug.
Thismeans thatonlytokenswere addedinthe patch.Inthiscase,
the deletionpointer willjust pointto thestart ofthe line,and the
insertion pointer will indicate where the new tokens (which the
decoder will emit) are to be added.Noremovals:
Theprefixandsuffixcombinedspantheentirerepair.
In this case, only token deletions are needed, to go from the bug to
the patch. So, the insertion and deletion pointer should correspond
to the start and end of the segment to be deleted within the buggy
statement, and the decoder should just emit the “</s>" termination
symbol (an “empty" patch).
Additions & Deletions: A non-trivial change in both bug and
patch. As a combination of the above, the two pointers shouldidentify the segment to erase from the bug, while the decoder
should generate all newly required tokens to insert instead.
Context:we also observed that the bug alone rarely provides
enough(syntacticandsemantic)informationtoreliablypredictthe
necessaryrepair.Thenaturalsolutionistoaddalargeamountof
contextual tokens from the file containing the bug. Unfortunately,
Transformers struggle to model very long sequences as their mem-
ory usage increases quadratically with sequence length. At the
sametime,section4.2showedthateven20linesofcontextisrarely
enoughtoprovidemuchmissingvocabulary(whichisitselfonly
part of the information needed). We do not provide a new solution
inthispaper;rather,weempiricallyquantifythedeficitfromthe
model’s perspective by adding up to 500 tokens of context and
comparing the resulting performance. We ensure that the model is
“aware"ofwhichtokenstorepairbybiasingthedecoder’sattention
tothebuggytokensusingthesamebiasingmechanismasin[ 13],
in this case with a simple unary relation (i.e., “is part of the bug").
283if(lineWidth != 0) {Encoder> </s>
Decoder+=if(lineWidth > 0) { if(lineWidth != 0) {
start/insert end/delete
(a) An edit-based repair model, which emits two pointers based on
theencoderstatesthatindicatetheinsertionstartpositionandthe
removal end position. The decoder generates any missing tokens.
… }if(linewidth != 0) { g.setStroke( …, linewidth, …EncoderDecoder if(linewidth > 0) {
(b)Representationofacontext-enrichedrepairmodel.Theencoder
functionsasusualonabroadersetoftokens;thedecoder’sattentionis biased towards the highlighted (buggy) tokens.
Figure4: Proposedarchitecturalchangestothebasicrepairmodel
on an example from our test data.
Table4: Repairaccuracyonthe(de-duplicated)testdataofthevar-
ious models that we propose in this paper.
Model Top-1Top-5Top-25
Baseline 3.30%5.96% 8.20%
Edits4.31%6.14% 7.83%
Context 1.83%3.57% 5.18%
Edits + Context 3.39%4.08% 4.76%
5.2 Results
Astable4shows,therearetwomaincharacteristicsoftheresulting
models’ performances. First, the edit-based enhancement clearly
andsubstantiallyimprovestheaccuracyoverthebaselinemodel,
fixing an additional 22 bugs on our test set with its top prediction
alone. Second, the contextual enhancement does not seem to help
in its current form. We discuss both these results here.
Editmodel: theedit-basedmodelproducesbetter-qualitypatches
onourtestdatathanthecorrespondingbaseline.Itsdesignisin-
formedbyourdataanalysis,andsoitisarguablyabetterfitforthis
task. Figure 5 shows its training behavior, to compare with that of
thebaselinemodelinfig.3;its“per-token",teacher-forcedaccuracy
increases much more smoothly4and more in line with increases in
thefullrepair predictionquality.Italso displaysalargerimprove-
ment in sampling accuracy between the top-1 and top-5 prediction,
whichremainsconsistentlywideduringtraining,suggestingthat
it better explores the search space with more diverse predictions.
Its design also allows the edit model to predict more newly
introduced vocabulary in the patch relative to the bug; it doesso 6.8% and 15.7% of the time (for the top-1 and top-25 samplesrespectively), compared to 5.6% and 14% of the base model. Onenotable difference is the gap between top-5 and top-25 sampling
accuracy; the edit model is stronger in the former, but loses to the
baselineinthelatter.Thisappearstobeduetotheeditmodelhavingtocommittoaninsert&deletepointerfirst,conditionaluponwhich
sampling is more bounded. To be clear, we did also sample thesetwo pointers from their corresponding probability distributions
4Their probability also displaying less of a “spike" in early training.
Figure 5: Performance of the edit-based model on held-out data
as training progresses, in terms of per-token accuracy subject to
teacherforcing,accuracyatgeneratingthecompletesequence,andtop-5 accuracy using beam search.
and initialized the beam search with the 25 most probable different
combinationsofstartandendpointers;but,inpracticethemodel
tended to choose a single pair with very high probability, so that it
effectively only explored that set. This may be an interesting issue
to pursue in future work.
Context information: the second missing element was the re-
liance on the bug alone as a source of patching information; in
section4.1,weshowedthattheabsenceofcontextisaninsurmount-
able obstacle that deprives the model of the necessary information
to patch most bugs. However, identifying a problem and solving it
are quite different things, as our results in table4 show. Although
we added a substantial amount of surrounding tokens ( i.e.,500) to
the model’s input, the resulting models’ performance is quite poor,
actually performing slightly worse than their context-free counter-
parts.Thisislikelyduetothechallengeofmodelinglargeamounts
of contextual information; although our models were trained to
similar accuracies, they did so much slower and evidently with
worse generalization.
Thismaypointatseveralissues,butnoneseemquiteresponsible.
Forone,theattentionmechanismweusedmaynotadequatelyhelpthemodellocatethebuggybits;however,themodelalwaysemitted
patches that were very similar to the bug. Similarly, the amount of
context may simply be too little; table 1 suggests that many useful
tokens are only available far away from the bug. However, that
tablealsoimpliesthattheimmediatecontext shouldhelpwithca
10-20% of missing tokens, so this too does not explain the lack
of performance. The model itself may simply have insufficient
capacity to capture this much context, though we used a relatively
large Transformer architecture and the model was trained to high
accuracy. All this is to say that we do not know how to betterintegrate context in these models. This is not a bad thing; notall modeling improvements are obvious, but it is important that
weunderstandthedeficitsfirst.Ourempiricalanalysishelpedus
both identify it, and has laid a useful foundation for the kind of
informationtointegrateinfurtherimprovements,evenifitisnot
yet clear how.
2846 THREATS TO VALIDITY
This paper presents a case-study of a specific type of program
repair, which we explore in great empirical detail. As such, the
main threats to the validity of our conclusion are external, relating
to the generalization of our findings to both other types of defects
and other model “transplants" into SE research.
First, our data collection and analysis focused only on small,
one-linefixes,sincesuchbugs(andsingle-statementbugs)areboth
commonandimportant,realistictargettocurrentprogramrepair
models[19].Inaddition,manyexistingNMT-basedprogram-repair
tools [6,22] are trained and tested on one-line or single-hunk bugs.
Assuch,studyingsuchbugsisbothrepresentativeandimpactful.
Having said that, we do not claim, nor believe, that their empirical
properties generalize to larger, more complex defects; these no
doubthavetheirownnon-trivialcharacteristicsthatdeservefurther
investigation,especiallyif/whentheybecomethesubjectofnew
models.
Secondly, we did not compare our model(s) Section 5 with state-
of-the-art, NMT-based, program repair tools. The goal of this work
is not to present models with the best performance; rather, we are
evaluating the feasibility of the general idea of “patching as trans-
lation"usingageneral,representativemodelingsetup,especially
in contrast to variations that depart from the translation metaphor.
More broadly,there are manyother cases ofmodeling transplants
into our community, often with some alterations to fit the task;
thesemaynotallbeharmfulormismatched,buttheydoalldeserve
carefulempiricalanalysistoensurethattheyachievetheirpotential
efficacy in our community.
7 CONCLUSION
In this work, we first present a comprehensive study to evaluate
theconceitthat"softwarepatchingislikethelanguagetranslation"
as a prototypical example of “model transplant" from neighboring
communities into SE. We empirically show that the translationparadigm does not capture bug-fixing very well for a range of
reasons.Wealsousemodelsthemselvesasempiricaldevices;we
adapt the seq2seqmodels used for translation to generate edits
ratherthanrawtokens,whichleadstopromisingimprovements.
We hope this work inspires more empirically-grounded research
into transplanting machine learning models to program repair, and
other software engineering applications.
ACKNOWLEDGEMENTS
This work is supported in part by NSF CCF-1414172, CCF-1845893,
CNS-1842456, and CCF-1822965. Any opinions, findings, conclu-sions, or recommendations expressed herein are those of the au-
thors,anddonotnecessarilyreflectthoseoftheUSGovernment
or NSF.
REFERENCES
[1]Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei Chang,
andNanyunPeng.2019. OnDifficultiesofCross-LingualTransferwithOrder
Differences: A Case Study on Dependency Parsing. In NAACL.
[2]MiltiadisAllamanis,MarcBrockschmidt,andMahmoudKhademi.2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations (ICLR).
[3]UriAlon,MeitalZilberstein,OmerLevy,andEranYahav.2019. Code2vec:Learn-
ing Distributed Representations of Code. Proc. ACM Program. Lang. 3, POPL,Article 40 (Jan. 2019), 29 pages. https://doi.org/10.1145/3290353
[4]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. CoRRabs/1409.0473
(2015).
[5]Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis, and Baishakhi
Ray. 2018. CODIT: Code Editing with Tree-Based Neural Models.
arXiv:cs.SE/1810.00314
[6]Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys
Poshyvanyk,andMartinMonperrus.2019. SequenceR:Sequence-to-Sequence
Learning for End-to-End Program Repair. IEEE Transaction on Software Engineer-
ing(2019).
[7]Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Ben-
gio.2014. OnthePropertiesofNeuralMachineTranslation:Encoder-Decoder
Approaches. ArXivabs/1409.1259 (2014).
[8]ElizabethDinella,HanjunDai,ZiyangLi,M.Naik,L.Song,andK.Wang.2020.
Hoppity: Learning Graph Transformations toDetect and Fix Bugs in Programs.
InICLR.
[9]ClaireLeGoues,ThanhVuNguyen,StephanieForrest,andWestleyWeimer.2012.
GenProg:AGenericMethodforAutomaticSoftwareRepair. IEEETransactions
on Software Engineering 38 (2012), 54–72.
[10]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2017.DeepAM:
MigrateAPIswithMulti-ModalSequencetoSequenceLearning.In Proceedings
of the 26th International Joint Conference on Artificial Intelligence (Melbourne,
Australia) (IJCAI ’17). AAAI Press, 3675–3681.
[11]Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning.. In AAAI. 1345–1351.
[12]VincentJ.HellendoornandPremkumarDevanbu.2017.Aredeepneuralnetworks
thebestchoiceformodelingsourcecode?.In Proceedingsofthe201711thJoint
Meeting on Foundations of Software Engineering. ACM, 763–773.
[13]VincentJHellendoorn,PetrosManiatis,RishabhSingh,CharlesSutton,andDavid
Bieber.2020. Globalrelationalmodelsofsourcecode.In 20208thInternational
Conference on Learning Representations (ICLR).
[14]AbramHindle,EarlT.Barr,ZhendongSu,MarkGabel,andPremkumarDevanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
ConferenceonSoftwareEngineering (Zurich,Switzerland) (ICSE’12).IEEEPress,
837–847.
[15]Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Comput. 9, 8 (Nov. 1997), 1735–1780. https://doi.org/10.1162/neco.1997.9.
8.1735
[16]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.Summarizing Source Code using a Neural Attention Model. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Berlin, Germany,
2073–2083. https://doi.org/10.18653/v1/P16-1195
[17]JiajunJiang,YingfeiXiong,HongyuZhang,QingGao,andXiangqunChen.2018.
Shaping Program Repair Space with Existing Patches and Similar Code (ISSTA).
https://doi.org/10.1145/3213846.3213871
[18]Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and
AndreaJanes.2020. BigCode!=BigVocabulary:Open-VocabularyModelsfor
Source Code. arXiv:cs.SE/2003.07914
[19]Rafael-Michael Karampatsis and Charles Sutton. 2019. How Often Do Single-
Statement Bugs Occur? The ManySStuBs4J Dataset. arXiv:cs.SE/1905.13334
[20]Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: Syntax- and Semantic-Guided Repair Synthesis via Programmingby Examples. In Proceedings of the 2017 11th Joint Meeting on Foundations of
Software Engineering (Paderborn, Germany) (ESEC/FSE 2017). Association for
ComputingMachinery,NewYork,NY,USA,593–604. https://doi.org/10.1145/
3106237.3106309
[21]FanLongandMartinRinard.2016. AnAnalysisoftheSearchSpacesforGenerate
and Validate Patch Generation Systems. In 2016 IEEE/ACM 38th International
Conference on Software Engineering (ICSE).
[22]ThibaudLutellier,HungVietPham,LawrencePang,YitongLi,MoshiWei,and
Lin Tan. 2020. CoCoNuT: Combining Context-Aware Neural Translation ModelsUsingEnsembleforProgramRepair (ISSTA2020).AssociationforComputingMa-
chinery, New York, NY, USA, 101–114. https://doi.org/10.1145/3395363.3397369
[23]KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:A
Method forAutomatic Evaluation ofMachine Translation.In Proceedings ofthe
40thAnnualMeetingonAssociationforComputationalLinguistics (Philadelphia,
Pennsylvania) (ACL ’02).Association forComputational Linguistics,USA, 311–
318. https://doi.org/10.3115/1073083.1073135
[24]Yuhua Qi, Xiaoguang Mao, Yan Lei, Ziying Dai, and Chengsong Wang. 2014.
TheStrengthofRandomSearchonAutomatedProgramRepair.In Proceedings
ofthe36thInternationalConferenceonSoftwareEngineering (Hyderabad,India)
(ICSE ’14). Association for Computing Machinery, New York, NY, USA, 254–265.
https://doi.org/10.1145/2568225.2568254
[25]Zichao Qi, Fan Long, Sara Achour, and Martin Rinard. 2015. An Analysis of
PatchPlausibilityandCorrectnessforGenerate-and-ValidatePatchGeneration
285Systems. In Proceedings of the 2015 International Symposium on Software Test-
ingandAnalysis (Baltimore,MD,USA) (ISSTA’15).AssociationforComputing
Machinery,NewYork,NY,USA,24–36. https://doi.org/10.1145/2771783.2771791
[26]Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli, and Premkumar Devanbu. 2016. On the "Naturalness" of Buggy Code.
InProceedingsofthe38thInternationalConferenceonSoftwareEngineering (Austin,
Texas)(ICSE ’16). Association for Computing Machinery, New York, NY, USA,
428–439. https://doi.org/10.1145/2884781.2884848
[27]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
TranslationofRareWordswithSubwordUnits.In Proceedingsofthe54thAnnual
Meeting ofthe Associationfor Computational Linguistics(Volume1: Long Papers).
AssociationforComputationalLinguistics,Berlin,Germany,1715–1725. https:
//doi.org/10.18653/v1/P16-1162
[28]Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence
LearningwithNeuralNetworks.In Proceedingsofthe27thInternationalConference
onNeuralInformationProcessingSystems-Volume2 (Montreal,Canada) (NIPS
’14). MIT Press, Cambridge, MA, USA, 3104–3112.
[29]Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen, Pierre-Antoine
Manzagol, Charles Sutton, andEdward Aftandilian. 2019. Learningto Fix Build
ErrorswithGraph2DiffNeuralNetworks. arXivpreprintarXiv:1911.01205 (2019).[30]Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin
White, andDenys Poshyvanyk. 2018. AnEmpirical Investigation into Learning
Bug-Fixing Patches in the Wild via Neural Machine Translation. In Proceed-
ingsofthe201833rdACM/IEEEInternationalConferenceonAutomatedSoftware
Engineering.
[31]Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Singh Rishabh.
2019. Neural Program Repair by Jointly Learning to Localize and Repair.. In
ICLR.
[32]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You
Need.InProceedingsofthe31stInternationalConferenceonNeuralInformation
ProcessingSystems (LongBeach,California,USA) (NIPS’17).CurranAssociates
Inc., Red Hook, NY, USA, 6000–6010.
[33]OriolVinyals,MeireFortunato,andNavdeepJaitly.2015. Pointernetwork..In
Proceedings of the 28th International Conference on Neural Information Processing
Systems (NIPS ’15). 2692–2700.
[34]JianZhang,XuWang,HongyuZhang,HailongSun,KaixuanWang,andXudong
Liu.2019. ANovelNeuralSourceCodeRepresentationBasedonAbstractSyntax
Tree. InProceedings of the 41st International Conference on Software Engineering
(Montreal, Quebec, Canada) (ICSE ’19). IEEE Press, 783–794. https://doi.org/10.
1109/ICSE.2019.00086
286