Flaky Test Detection in Android via Event Order Exploration
Zhen Dong
NationalUniversity of Singapore, Singapore
zhen.dong@comp.nus.edu.sgAbhishek Tiwari
NationalUniversity of Singapore, Singapore
dcsabhi@nus.edu.sg
Xiao Liang Yu
NationalUniversity of Singapore, Singapore
xiaoly@comp.nus.edu.sgAbhik Roychoudhury
NationalUniversityof Singapore, Singapore
abhik@comp.nus.edu.sg
ABSTRACT
Validation of Android apps via testing is difficult owing to the
presenceofflakytests.Duetonon-deterministicexecutionenviron-
ments,asequenceofevents(atest)mayleadtosuccessorfailurein
unpredictable ways. In this work, we present an approach and tool
FlakeScanner for detecting flaky tests through exploration of event
orders. Our key observation is that for a test in a mobile app, there
is a testing framework thread which creates the test events, a main
User-Interface (UI) thread processingthese events, and there may
be severalother backgroundthreads runningasynchronously. For
anyevent ğ‘’whoseexecutioninvolvespotentialnon-determinism,
we localize the earliest (latest) event after (before) which ğ‘’must
happen. We then efficiently explore the schedules between the
upper/lowerboundeventswhilegroupingeventswithinasingle
statement,tofindwhetherthetestoutcomeisflaky.Wealsocre-
ate a suite of subject programs called FlakyAppRepo (containing
33 widely-used Android projects) to study flaky tests in Androidapps. Our experiments on the subject-suite FlakyAppRepo show
FlakeScanner detected 45 out of 52 known flaky tests as well as 245
previously unknown flaky tests among 1444 tests.
CCSCONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging;
KEYWORDS
flaky tests, non-determinism, concurrency, event order
ACM Reference Format:
Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury.
2021. Flaky Test Detection in Android via Event Order Exploration. In 
Proceedings of the 29th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE â€™21),
August 23â€“28, 2021, Athens, Greece. ACM, New York, NY, USA, 12 pages. 
https://doi.org/10.1145/3468264.3468584
1 INTRODUCTION
Regression testing aims to discover the adverse effects of  the re-
cently added code changes. Ideally, a test failure during regression
ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
Â© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8562-6/21/08.
https://doi.org/10.1145/3468264.3468584testingshouldreliablysignalissueswithrecentcodemodifications.
However,sometestfailuresarenotduetotherecentupdatesbut
due toflaky tests. Recent research [ 6,17,30] establishes flaky tests
as tests that non-deterministically pass or fail when running on
thesamecodeversion.Unfortunately,thepresenceofflakytests
significantlyharmsdevelopersâ€™ productivity [1, 25,30].
Androidisareactivesystem,anditsnon-deterministicexecution
environmentoftencausesconcurrency-relatedflakytests.InAn-
droid, a GUI test usually simulates user interactions to exercise the
appâ€™s functionality. However, due to non-determinism, a test could
generate such interactions at â€œincorrectâ€ timings. For example, a
testtodownloadanimageandthenopenitwithoutconsideringtheinternetâ€™sspeedcouldsho wnon-deterministicbehavior.Apotential
reasonforsuchconcurrencyissuesisthelackofsynchronization
among threads, e.g., in the failing run, a background thread would
still be downloading the image while the testing thread tries to
open it without synchronization with the background thread.
Recent studies [ 34,42] confirm that such synchronization issues
lead to a significant number of flaky tests in Android apps. Thorveet al. [
42] studied 77 flakiness-related commits in Android projects
and discovered that 36% failures are due to synchronization issues
between testing thread and app under test, e.g., a test accessesdata in the app before the data is available. Similarly, Romanoet al.â€™s study [
34] shows that 33% flaky test failures are caused
by threadsâ€™ synchronization issues (tests interacting with the UI
elementsbefore the elements are fully loaded).
Challenges. Exposingaconcurrency-relatedflakytestinatest
suite is challenging as such flakiness is only observed when events
get executed in a particular order. In Android, a testâ€™s event execu-
tionordermayshownon-determinismduetothenon-deterministic
execution environment. Some of such (irregular) event orders may
cause a test to fail/pass occasionally. Consequently, detecting such
a situation would require exploring such event orders deterministi-
cally;unfortunately,thisposesasetofchallengesfortheexisting
flakytest detectionapproaches, whichwe discuss below.
â€¢RERUN.Atypicalwaytodetectaflakytestistorerun(expressed
asRERUN) it multiple times. If it passes in some runs and fails in
others,thetestismarkedasflaky.However,thisapproachcan
struggle to detect concurrency-related flaky tests because the
targetâ€™s execution environment may not introduce the needed
non-determinism. Besides, it may require too many runs to wit-
nessthe flakiness.
â€¢Noise Based. Several approaches [ 4,38,40,48] run tests with
environmental perturbations (e.g., changing CPU load or test
executionorders)inthehopeofobservingunexpectedbehaviors.
367This work is licensed under a Creative Commons Attribution International 4.0 License.
ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury
Despite being simple and easy to be adopted in practice, adding
noisedoesnotguaranteetoexploredifferentexecutionorders.
Consequently,theyfailtodetectmanyconcurrencyrelatedflaky
tests,as observed during our experiments.
â€¢Event Race. The concurrency-related flakiness may be caused
due to a synchronization problem on the testing thread. Besides,
a concurrency bug on the app under test could also cause a
non-deterministic test failure. Thus, in principle, approaches
identifying concurrency bugs should also detect a subset of such
flaky tests. Recent works [ 8,18,19,24,32] leverage program
analysis techniques to analyze possible races in Android apps.For instance, EventRacer [
8] records the execution trace of a
test and statically analyzes it to find potential races. However,
suchapproachesarepronetomultiplefalsepositives,whichmay
create more problems for the developers during the Continuous
Integration(CI) process.
OurApproach. Weintroducea lightweighttechniquethatauto-
matically detectsa concurrencyflaky test inAndroid appswithin
few test runs. Our technique explores the possible event execution
orders in a test by scheduling a non-deterministic (async) event
suchthateachtestrunexploresadifferenteventexecutionorder.
Besides, we identify and explore event orders in which the test
flakinessmanifestssuchthataflakytestcanbedetectedinafew
test runs.
Insight.Possibleeventexecutionordersareintroducedbynon-
deterministic execution of async events . Android apps use a con-
current event-driven model, in which only the main (UI) thread
can access GUI and processes user events. To keep GUI responsive,
the UI thread offloads a long-running task (e.g., internet accessing)
to a background thread. Once the task is finished, the background
threadsendsanevent(called asyncevent )totheUIthreadtoper-
formaGUIupdate.Inatestrun,thetestingthreadandbackground
threadssendeventstotheUIthreadsimultaneously,resultingin
racesamongtheevents;thetestmaypassforsomeeventordersand
fail inothers. For the examplementioned earlier, thetesting thread
simulatestheâ€œopentheimageâ€event,andthebackgroundthread
downloadstheimageandsendsanupdateeventaftercompleting
thedownload.Aracemayoccurbetweenthesetwoevents.Ifthe
internetisfast,theupdateeventreachestheUIthreadbeforethe
â€œopentheimageâ€event,andthetestpasses.Otherwise,theâ€œopen
the imageâ€ event is processed first, and the test fails.
EventOrderExploration. Leveragingtheseinsights,ourapproach
explores possible event execution orders by scheduling an async
event in a test run. Our approach first identifies the schedule space
foreachasynceventwithdynamicanalysisandschedulestheasync
eventinitsschedulespacetoavoidinfeasibleorders.Tomanifest
aflakytestfailurequickly,ourapproachscheduleseventsatposi-
tions where the test is more likely to fail. Specifically, it schedules
an async event at a statement boundary position . The statement
boundarypositionisbetweenthelasteventthatateststatement
triggers and the first event that the next statement triggers. We
note that the tests are often flaky due to missing an appropriatesynchronizationoperationbetweentwoteststatements,e.g.,the
synchronizingoperationafterclickingadownloadbutton.Thuswearemorelikelytotriggeratestfailureifanasynceventisexecuted
afterexecuting certain test statements.Instrumentation-freeTool. Weimplementourapproachintoatool
calledFlakeScanner , which leverages the debug mode supported in
theAndroidframeworktoperformdynamicanalysisbasedevent
scheduling.Thus FlakeScanner requiresnoinstrumentationinei-
therAndroidappsortheAndroidframeworkandworksonboth
Androidemulatorsanddevices.Accordingtothestudy[ 41],codein-
strumentation often disrupts test executions and prevents the man-
ifestation of flaky test failures. Moreover, FlakeScanner supports
multiplewidely-usedtestingframeworkswithwhichdevelopers
writetestssuchas Espresso [ 2] or Robotium [ 13].
Experiment. Weevaluated FlakeScanner on33widely-usedAn-
droidprojects.Ourexperimentsshowthat FlakeScanner success-
fullydetected45outof52knownflakytests.Onaverage,itdetected
a flaky test within three test runs. FlakeScanner outperformed the
recentlypublishedflakytestdetectiontechnique Shaker[40]and
thebaselinetoolRERUNintermsofboththenumberofdetected
flaky tests and average execution time. FlakeScanner also detected
245 flaky tests that were previously unknown. Out of these 245
unknown flaky tests, we reported 20 to developers; 13 out of these
20 have been confirmed and addressed by developers.
Contributions. Our contributions can be summarized as follows:
â€¢We present an event scheduling approach that explores dif-
ferent event execution orders for each test run to detect
concurrency-related flaky tests in Android apps. To avoidexploring all possible event orders, our approach adopts
heuristics to identify and explore event orders in which the
testflakiness is likely to occur.
â€¢We implement our approach into an instrumentation-free
tool that works on Android emulators and physical devices
whilesupporting widely-used Android testing frameworks.
â€¢We curate a suite of subject programs containing 33 widely-
used Android projects with developer tests, called FlakyAp-
pRepoforevaluatingflakytestdetectiontechniques.Tofacil-
itatefutureresearchonflakytests,wemakeourprototype
FlakeScanner and subject-suite FlakyAppRepo available at
https://github.com/AndroidFlakyTest
2 BACKGROUND
Android Concurrency Model. Figure1depicts Androidâ€™s event-
driven concurrency model. Each Android app has a main thread
(also called UI thread ); this thread processes events generated by
usersortheAndroidsystem.AsshowninFigure 1,theUIthread
maintainsaneventqueueandaneventlooper.Thequeueisusedto
store received events, and the looper sequentially dequeues events
from the queue and dispatches them to corresponding handlers
for processing. Android adopts a single-UI-thread model in which
onlythemainthreadcanaccessGUIobjects.ToachieverapidUI
responsiveness, the UI thread offloads long-running tasks such
as network access to background threads, called async threads
since they communicate with the UI thread asynchronously. Once
tasks are completed, async threads post an event (called async
eventmarked in blue in Figure 1) to the UI thread, and the UI
thread updates the results to GUI. Event races may occur in this
model. The UI thread and async threads run concurrently. The du-
ration that an async thread will take to complete a task and post
368Flaky Test Detection in Android via Event Order Exploration ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
	

(
)




 



 


	



+
*,.
- 
 


Figure1:Androidconcurrencymodel&testingframework.
an async event is non-deterministic, depending on the current exe-
cution environment. Thus, a race might occur among posted async
eventsandothers,leadingtonon-deterministicexecutionorders.
IntheexampleinFigure 1,therearemultipleeventorderswhich
might occur in the execution, for instance, /angbracketleftğ‘’1,ğ‘’2,ğ‘’6,ğ‘’3,ğ‘’4,ğ‘’5,ğ‘’7/angbracketright
and/angbracketleftğ‘’1,ğ‘’2,ğ‘’3,ğ‘’4,ğ‘’5,ğ‘’6,ğ‘’7/angbracketright.
Instrumented Tests & Testing Frameworks. Instrumented tests are
tests that run on physical devices and emulators, and they can
invoke the Android framework APIs to control app under test at
runtime. These tests are often executed in a separate thread (called
testingthread )tosimulateuserinteractions,asshowninFigure 1.
The Android system supports multiple testing frameworks to help
developerswritetests,e.g.,Espresso[ 2].Toachievereliabletests,
theseframeworksprovidemechanismstosynchronizeuserinter-
actions with app under test. For instance, when method onView()
isinvokedinatest,Espressowaitstoperformthecorresponding
UIactionorassertionuntiltheeventqueueisempty,background
threads are terminated, and user-defined resources are idle.
3 A MOTIVATING EXAMPLE
To set the stage for our event order exploration technique for flaky
testdetection,wefirstillustratethecharacteristicsofaflakytest
andthen exemplifycurrent approachesâ€™inadequacy throughList-
ings1-3. These listings show parts of CaptureLocationActivityTest ,
a flaky test taken from RapidPro Surveyor app. This test intends
to validate whether the app can successfully obtain the locationdata using Google APIs. As shown in Listing 1, the test launches
CaptureLocationActivity (Line2),theactivitytocapturethelocation
data(Listing 2). Later,thetestemulates abuttonclicktofetch the
location at line 5in Listing 1, and validates whether the location is
successfullyfetched at line 7in Listing 1.
Despiteappearingstraightforward,thetestdisplaysnon-determi-
nistic behavior, i.e., it is flaky. As explained earlier, the test runs
in a testing thread, and the activity under test runs in the appâ€™s
UIthread.The activity( CaptureLocationActivity )offloadsfetching
locationdataviaGoogleAPIclienttoanasyncthread(Listing 3).
After obtaining the location data, the async thread updates the
resulttotheUIthread,andthentheUIthreadupdatesthisresulttoCaptureLocationActivity . Due to the lack of synchronization be-
tween the testing thread and async thread, the async thread might
send the location data before or after the testing thread validates it.
Ifthevalidationoccursbeforethelocationdataisreceived,thetest
fails,else it passes.
Detecting concurrency-related flaky tests is non-trivial as such
failuresmanifestwhentheeventsareexecutedinaspecificorder.
The traditional approach RERUNblindly executes the test many
timesinthehopeofwitnessingtheflakytestfailures.However,this
approach becomes ineffective when the environment under which
thetestrunsdoesnotproducetherequirednon-determinism.For
example, RERUNfailed to witness the flaky test failure in Capture-
LocationActivityTest during our experiments. Another approach
istoaddnoiseintheexecutionenvironmenttoincreasethelike-
lihood of observing such errors. However, such approaches do
not proactively explore different event execution orders and are
pronetomisscrucialeventorderings.Inourevaluations,therecent
(noise-based)relatedwork Shaker[40]failedtodetectmanycon-
firmedflakytests,including CaptureLocationActivityTest .Wealso
evaluated EventRacer ,adynamicanalysisbasedtechniqueforde-
tectingeventracesinAndroidapps.However,suchapproachesare
pronetoreportmanyraces.Forexample,inourevaluation, Even-
tRacerreported over 200 data races for CaptureLocationActivityTest .
Unfortunately,noneofthesereporteddataracesfrom EventRacer
involveeventsoriginatedfromthetestingthread,andhencedonot
demonstratethespecificflakinessweareillustratinginthissection.
Besides,EventRacer doesnotvalidatewhetherreportedracescan
cause the testfailure.
4 OVERVIEW OF OUR APPROACH
This section describes the main components of our framework
and lists the principles that make event exploration suitable fordetecting concurrency-related flaky tests. First, the category of
flaky tests should be well defined. It is acceptable to give up on all
classesofflakytestsandfocusonone;concurrency-relatedflaky
tests. Second, using only one test run, we explore (all) possible
schedulingspaceofanasyncevent.Third,weexecutethesenewschedulingstoobservetheflakybehavior,i.e.,thereexistatleast
twoexecutionsforwhichatestdepictsdifferentoutcome(pass/fail).
4.1 BasicConcepts
Beforedivingintodetailsofourapproach,wedefineaminiature
domain-specific syntax extended from [ 20] for an Android test.
Test T::=âˆ’â†’ğ‘†
Statement ğ‘†::=post(âˆ’â†’ğ‘’)|assertions |sync|other
Event ğ‘’::=âˆ’â†’ğ‘€UIThread|âˆ’â†’ğ‘€backgroundThread |âˆ’â†’ğ‘€OS
Synchronize SYNC :: =SYNC(âˆ’â†’ğ‘†)
Message M
A testğ‘‡is composed of a series of program statements ğ‘†. For
concurrency-related flaky tests, we are interested in exploring the
scheduling space of events, and thus for ease of representation, we
categorizeateststatementintofourparts;statementspostingan
event, assertion statements, testing framework-specific synchro-
nizing statements, and all other Android-specific statements. An
event, denoted as ğ‘’, is a message object created in the UI thread
369ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury
Listing 1: CaptureLocationActivityTest (Testing Thread)
1@Rule
2publicActivityTestRule<CaptureLocationActivity> rule = new
ActivityTestRule<>(CaptureLocationActivity. class);
3@Test
4public void capture() {
5onView(withId(R.id.button_capture))
.check(matches(isDisplayed())).perform(click ());
6Instrumentation. ActivityResult result =
rule. getActivityResult () ;
7assertThat( result .getResultData() , is(not(nullValue())));
8...
9}
Listing 2: CaptureLocationActivity (UI Thread)
1@Override
2protected void onCreate(Bundle bundle) {
3connectGoogleApi();
4...
5}
6protected void connectGoogleApi() {
7googleApiClient = newGoogleApiClient.Builder( this) ...
8googleApiClient.connect();
9}
Listing 3: Zaau (Worker Thread)
1//Worker thread asynchronous to the UI thread
2@WorkerThread
3public void run() {
4zaak.zac( this.zagj).lock() ;
5...
6}
(âˆ’â†’ğ‘€UIThread), background threads (âˆ’â†’ğ‘€backgroundThread ), or Android
framework (âˆ’â†’ğ‘€OS). Each event specifies a specific action to per-
form, e.g., a button click registers an on-clickevent. Synchronizing
statements (SYNC) are testing framework-specific methods and
are used to achieve synchronization among UI Thread and testing
thread, e.g., Espresso uses onView() method to achieve synchro-
nization among testing thread and UI Thread1. Based on above
constructs, we define an async event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘as the event generated
from a background thread, i.e, ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘âˆˆâˆ’â†’ğ‘€backgroundThread .
Definition4.1(SchedulingSpaceofEvents). Letğ‘‡beatestcontain-
ingasetofstatementsâˆ’â†’ğ‘†,âˆ’â†’ğ‘’bethesequenceofeventsgenerated
byâˆ’â†’ğ‘†. Letâˆ’â†’ğ‘’contains ğ‘›async events andâˆ’â†’ğ‘’/primebe a new sequence
of events created by reordering an async event inâˆ’â†’ğ‘’. Then, the
scheduling spaceof eventsâˆ’â†’Eis a set of all possibleâˆ’â†’ğ‘’/prime.
Givenatest,Definition 4.1formallydefinesschedulingspacesof
events.Intuitively,inatest,theorderofanasynceventisnotfixed,
and thus, creating new event orders by reordering async events
willprovideallpotentialeventorders.However,theseeventorders
may also contain (infeasible) orders that will not be realized in
practice.Oneof thecriticalchallengesistoavoid suchorders.An
infeasibleeventorderingcanbeexploredbyignoringtheeventsâ€™
dependencies during the execution. For instance, an async event ğ‘’1
may depend on another event ğ‘’2, where event ğ‘’1will not complete
1https://developer.android.com/training/testing/espresso#syncbeforeğ‘’2inpractice.Basedonthisobservation,wedefineinfeasible
event orders as:
Definition 4.2 (Infeasible Scheduling Space). Letâˆ’â†’ğ‘’be a sequence
ofeventsandâˆ’â†’ğ‘’/primebeanewsequencecreatedbyreorderinganasync
eventğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘inâˆ’â†’ğ‘’from the position ğ‘–to a new position ğ‘—. Then,
âˆ’â†’ğ‘’/primewill be infeasible if âˆƒğ‘’ğ‘˜âˆˆâˆ’â†’ğ‘’/primeafter the position ğ‘—andğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘
depends on ğ‘’ğ‘˜.
Intuitively, dependsspecifiesthe Happens-before relation.Inprac-
tice, avoiding infeasible event orders would require computing
the dependencies among events in the test execution. However,
computing event dependencies is challenging for multiple reasons.
First,atestcantriggermanyevents(morethan500eventsforsome
cases).Computingdependenciesamongthemcanbecomplexas
oneeventâ€™sexecutionmaydependonmultipleotherevents.Second,event dependencies are not specified in the events but handed over
to the componentsthat respondto theevents. Thesecomponents
maybelongtoAndroidframeworksorthird-partylibraries,makingeventdependencyanalysisdifficult.Weproposeadynamicanalysis
to explore various event execution orders to address this challenge.
Given an async event ğ‘’in a test run, the analysis identifies the
scheduling space /angbracketleftğ‘’ğ‘™ğ‘œ,ğ‘’ğ‘¢ğ‘/angbracketright, whereğ‘’ğ‘™ğ‘œislower bound event andğ‘’ğ‘¢ğ‘
isupperboundevent ,i.e.,ğ‘’cannotbeexecutedearlierthan ğ‘’ğ‘™ğ‘œor
later than ğ‘’ğ‘¢ğ‘for all execution environments. Consequently, we
can exploreevent execution ordersby scheduling the async event
ğ‘’at various positions between ğ‘’ğ‘™ğ‘œandğ‘’ğ‘¢ğ‘.
4.2 Identifyingthe Scheduling Space
Thesuccessfulrealizationoftheschedulingspacerequiresnoticing
twocrucialpoints.First,theschedulingspaceneedstoreflectsome
level of determinism, i.e., there need to be some non-async events.
Second,eachasynceventâ€™sschedulingspaceshouldbewelldefined.
Observation 1. For a statement ğ‘ in a testğ‘‡, the first event it
generates will not be an async event, i.e., its order will not change for
allruns.
Android is a reactive system, and tests in an Android app in-
vokes the functionality of Androidâ€™scomponents(e.g., an activity)
by simulating user events. For example, a test would emulate a
buttonclicktoinvokesomefunctionalityprovidedbyanactivity.
TheconceptofbackgroundthreadsinAndroidistrivial,andAn-
droidâ€™scomponentsutilizethemtooffloadlong-runningtasksin
thebackground.Notably,abackgroundthreadisalwayscreatedby
an Android component (via UI Thread). Thus, the interaction from
a test (via a statement) to an Android component would always
followasequencewherethetestwouldinvokethecomponent,and
thenthecomponentmayinvokeabackgroundthread.Considering
such event chaining, the first event of a test statement will always
belongtoâˆ’â†’ğ‘€UIThreadorâˆ’â†’ğ‘€OS.Basedonthisobservation,wedefine
anchorevents as:
Definition4.3(AnchorEvents). Letğ‘ âˆˆâˆ’â†’ğ‘†beaprogramstatement
in a test ğ‘‡, and it generates ğ‘›events in the following order: <
ğ‘’1,...,ğ‘’ğ‘›>. Then, we define ğ‘’1as the anchor event for ğ‘ .
To localize anchor events, we execute a test, statement by state-
ment;werecord allevents triggered byeach statement,and build
370Flaky Test Detection in Android via Event Order Exploration ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
a map between them. According to this map, we identify anchor
eventsforthetest.Definingthelowerboundofanasynceventis
nowstraightforward.Intuitively,foragivenstatement,thelower
bound of an async event would always be the anchor event of this
statement, as the anchor event will always happen-before async
event (Observation 1).
Localizing the upper bound event of an async event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘is
more involved and requires identifying events that depend on
ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘,i.e.,eventsthatoccuronlyafter ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.Sucheventdepen-
dence is maintained by thread synchronization operations. For
instance, the testing framework Espresso uses onView() operation
tosynchronizethetestingthreadandappundertest.Thetesting
threadwaitsuntilspecificthreadsorresourcesareidletoensure
thatawidgetspecifiedby onView() showsuponthescreen.For-
mally, we define the upper bound of an async event as:
Definition4.4 (Upper Bound Event). Letğ‘‡be a test containing a
set of statementsâˆ’â†’ğ‘†. We define ğ‘’ğ‘—as the upper bound event for an
asyncevent ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘, whereğ‘’ğ‘—is the first event that cannot happen-
beforeğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.
Weproposea what-ifanalysistolocalizetheupperboundevent
ofğ‘’ğ‘–. Specifically, after the test is launched, we hook the async
threadthatposts ğ‘’ğ‘–atruntimeandsuspendit,keepingotherthreads
free. Meanwhile,we monitorthe testing threadto checkat which
statement the testing thread stops and waits for the suspended
threadtobecompleted.Supposethatthetestingthreadstopsata
statement ğ‘ on executing the event ğ‘’ğ‘—. We then deem operations
inğ‘ to depend on ğ‘’ğ‘–, and these operations will not be executed
untilğ‘’ğ‘–is processed. Thus, the anchor event ğ‘’ğ‘—triggered by ğ‘ is
upper bound event of ğ‘’ğ‘–. The idea behind the analysis is what-ifit
takesforevertocompletethelong-runningtaskthatcorresponds
ğ‘’ğ‘–;operationsinatestthatdependon ğ‘’ğ‘–willnotbeexecuteddue
to thread synchronization, and those that do not depend on ğ‘’ğ‘–will
be executed.
4.3 Scheduling Events
Next,weexploretheschedulingstrategiesforanasyncevent ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.
Given the lower and upper bound events of ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘, the anchor
events that liebetween this interval areidentified. Then, ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘is
scheduled before each of these anchor events. To manifest flaky
testfailuresassoonaspossible,weprioritizepositionsclosertothe
upper bound event (maximizing the runtime of the async event),
i.e.,wefirstschedule ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘justbeforeitsupperboundandthen
move towards its lower bound.
Fortworeasons,weonlyexplorepositionsbeforeanchorevents
in the scheduling space of an async event instead of all possible
positions. First, for non-anchor events in the scheduling space,
their execution orders may change from run to run. Using them as
hooksforeventschedulingmayintroduceinfeasibleorders.Second,
anchor events are boundary events of test statements. Scheduling
an async event before them is likely to trigger a flaky test failure.
5 METHODOLOGY
Figure2shows the workflow of our approach. For a given test, our
frameworkperformsaconcreteexecutiontotraceallthegenerated
events. Next, a map between the statements in the test and theirTracing  & 
mapping eventsIdentifying 
schedule spaceScheduling
eventsAn event map Event orders
Figure 2: Workflow of exposing flaky tests through system-
aticschedule exploration.
correspondingeventsiscreated.Thetestisexecutedmultipletimes
to compute possible schedules for async events. Consequently, a
set of event orders that might occur in execution environments arecreated.Finally,weexplorethesepossibleeventexecutionorders;ifatestfailureisdetectedduringtheexploration,thetestisidentified
as a flaky test (since we have already seen passing runs of the test).
5.1 Event Tracing and Mapping
EventtracingisoftenusedinthedynamicanalysisofAndroidapps.
Itcanbeachievedbysimplyloggingeventsthataregeneratedat
runtime.However,suchtechniquescannotfulfillourtask.Event
information( e.g.,eventid)producedinlogsisdynamicallygener-
atedbyAndroidruntimeandchangesineachrun.Ourapproach
requires an event identifier to identify an event uniquely across
different test runs. Async events that are identified during event
tracingneedtobehookedandscheduledinrunsthatareperformed
for event order exploration.
EventIdentification. Weidentify anevent basedon interactions
between the event and app under test at runtime. Two events trig-
geredindifferenttest runsareconsideredidenticalif:(1)they are
triggered by the same test statement; (2) they are processed by the
samesequenceofmethodsatruntime.Forinstance,a pressDown
event is associated with an identifier constructed using the line
numberofthestatementthattriggerstheeventandsignaturesofa
sequence of methods that process it. This practice of event identifi-
cationcomesfrom our investigation of the Android framework.
Tracing and Mapping. Algorithm 1outlines the procedure of
event tracing and mapping. First, it launches the app under testand takes control of the Android runtime with a module calledARTHandler . Given a test
ğ‘‡consisting of a series of statements,
ARTHandlerrunsthetest ğ‘‡inthetestingthreadandexecutesstate-
mentsonebyone(viatheAndroiddebugger).Whenonestatementisexecuted,ARTHandlersmonitorstheeventqueueoftheUIthreadandhooksinjectedevents.Foreachevent,ARTHandlerrecordsthe
tuple/angbracketleftğ‘–ğ‘ ğ´ğ‘ ğ‘¦ğ‘›ğ‘,ğ‘ ğ‘ ğ‘š/angbracketrightwhereğ‘–ğ‘ ğ´ğ‘ ğ‘¦ğ‘›ğ‘denotes whether it is an async
eventand ğ‘ ğ‘ğ‘šdenotesthesignaturesofasequenceofmethodsthat
have processed the event. This tuple and the line number of the
statementbeingexecutedformtheeventâ€™sidentifierandgetstored
in a list (Line 11-14). As stated before, a statement in the test might
performlong-running tasks,whichare executedinasync threads.
Whenthesetasksarecompletedisnon-deterministic,andtheasync
eventmight bepostedafter alongtime. Tonot missasyncevents
thataretriggeredbyasinglestatement,wekeephookingevents
untiltwocriteriaaresatisfied:(a)therearenoneweventsand(b)
the event queue of the UI thread is empty, which often indicates
the system is not running tasks. This practice is also used in the
371ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury
Algorithm1: Event tracing and mapping.
1Procedure runTest( App A, Test T, Android ART )
2 launchApp (ğ´,ğ´ğ‘…ğ‘‡);
3ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ â†attachHandler (ğ´,ğ´ğ‘…ğ‘‡);
4ğ¿ğ‘–ğ‘ ğ‘¡â†âˆ…;// storing pairs of a statement and events
5 launchTest (ğ´,ğ‘‡,ğ´ğ‘…ğ‘‡);
// UI threadâ€™s event queue
6ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘„â†getEventQ (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ );
7forsinğ‘‡do
8 runStatement (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ,ğ‘  );
9 ğ‘›â†getLineNum (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ,ğ‘  );
10 whileTruedo
11 ğ¸/angbracketleftğ‘–ğ‘ ğ´ğ‘ ğ‘¦ğ‘›ğ‘,ğ‘ ğ‘ _ğ‘š/angbracketrightâ† getEvent (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ );
12 ifE != Null then
13 ğ¿ğ‘–ğ‘ ğ‘¡â†ğ¿ğ‘–ğ‘ ğ‘¡âˆª{/angbracketleftğ‘–ğ‘ ğ´ğ‘ ğ‘¦ğ‘›ğ‘,ğ‘ ğ‘ _ğ‘š,ğ‘›/angbracketright}
14 else
15 ifisEmpty(ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘„)then
16 break;
17 end
18 end
19 end
20end
21returnğ¿ğ‘–ğ‘ ğ‘¡
Espresso testing framework. Finally, a map between a statement
and its events is returned via the List, and the first event of each
statementis identified as an anchor event.
5.2 IdentifyingEvent Schedule Space
To compute possible event execution orders, we perform a what-if
dynamicanalysistoresolveeventdependenciescausedbythread
synchronizationbetweenappsandtestingframeworks.Algorithm 2
shows the procedure of resolving event dependencies. It takes the
eventtrace ğ¿ğ‘–ğ‘ ğ‘¡generatedinthepreviousstepasinput.Foreach
async event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘inğ¿ğ‘–ğ‘ ğ‘¡, the algorithm launches the test and
starts to hook event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘(Line 4-8). Once hooked, the algorithm
suspends the thread that posts ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘such that ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘cannot be
posted (Line 9). Meanwhile, it keeps checking the status of the
testingthread(Line10-14).Ifthetestingthreadâ€™sstatusis WAITING,
itconsidersthetestingthreadisperformingthreadsynchronization
withthreadsintheappandwaitingfor ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘tobeexecuted.Thus,
we consider the statement ğ‘ that is being executed in the testing
thread attempts to trigger an event (e.g., ğ‘’ğ‘—) which depends on
ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.Therefore,theschedulespaceof ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘isboundedby ğ‘’ğ‘—,i.e.,
the first eventthat istriggered by ğ‘ .So statement ğ‘ isidentified as
theupperboundofschedulespaceofasyncevent ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.Statement
ğ‘ isrecordedandsetastheupperboundofevent ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘andğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘
isrestoredto ğ¿ğ‘–ğ‘ ğ‘¡.Finally,schedulespacesforallasynceventsin
ğ¿ğ‘–ğ‘ ğ‘¡are recorded.
5.3 Scheduling Events
Schedulespaceof eachasynceventinthe eventtrace ğ¿ğ‘–ğ‘ ğ‘¡isiden-
tifiedintheprevioussteps.Next,weexploreeventordersduring
test execution. An async event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘can be simply represented
by a triplet /angbracketleftğ‘–ğ‘‘,ğ‘›,ğ‘š/angbracketright, whereğ‘›andğ‘šare bounds of the scheduling
spaceofğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.Specifically, ğ‘›istheindexofthestatementintheAlgorithm2: Event scheduling exploration.
1Procedure explore( App A, Test T, Android ART, EventMap List )
2foreinğ¿ğ‘–ğ‘ ğ‘¡do
3 ifisAsync (e)then
4 launchApp (ğ´,ğ´ğ‘…ğ‘‡);
5 ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ â†attachHandler (ğ´,ğ´ğ‘…ğ‘‡);
6 launchTest (ğ´,ğ‘‡,ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ );
7 ğ‘‡ğ»ğ‘¡ğ‘’ğ‘ ğ‘¡â†getTestingThread (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ );
8 ğ‘‡ğ»ğ‘ğ‘ ğ‘¦ğ‘›ğ‘â†hookAsyncThread (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ );
9 suspend (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ,ğ‘‡ğ» ğ‘ğ‘ ğ‘¦ğ‘›ğ‘);
10 whileTruedo
11 ifisWaiting (ğ‘‡ğ»ğ‘¡ğ‘’ğ‘ ğ‘¡)then
12 break;
13 end
14 end
15 ğ‘šâ†getLineNum (ğ´ğ‘…ğ‘‡ğ»ğ‘ğ‘›ğ‘‘ğ‘™ğ‘’ğ‘Ÿ,ğ‘‡ğ» ğ‘¡ğ‘’ğ‘ ğ‘¡);
16 end
//ğ‘šis set as upper bound for ğ‘’
17 setUpperBound (ğ‘’,ğ‘š);
// update event ğ‘’(with upper bound ğ‘š) in List
18 updateEvent (ğ¿ğ‘–ğ‘ ğ‘¡,ğ‘’);
19 break;
20end
21returnğ¿ğ‘–ğ‘ ğ‘¡
test that triggers ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘, andğ‘šis the index of the statement that
triggersthe upper bound event of ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.
Similartoschedulingspaceidentification,wecanschedule ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘
by operating threads. We first hook event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘after the test is
launchedandsuspendthethreadthatposts ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.Then,wefree
thetestingthreadandmonitorwhetherthestatementbeingexe-
cuted is statement ğ‘š. Once statement ğ‘šis reached, we suspend
the testing thread and free the suspended thread to post ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘.
Aftertheasyncthreadisterminatedoridle,i.e.,event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘has
beenposted,wefreethetestingthread.Insuchaway,event ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘
can be executed beforestatement ğ‘š. In next testrun, we schedule
ğ‘’ğ‘ğ‘ ğ‘¦ğ‘›ğ‘to be executed prior to statement ğ‘šâˆ’1, until all statements
betweenğ‘›andğ‘šare explored. This procedure is repeated for each
asyncevent in ğ¿ğ‘–ğ‘ ğ‘¡.
6 IMPLEMENTATION
OursystemisimplementedinScalaandrunsonacomputerthat
connects a physical Android device or an emulator. Unlike existing
techniques, it requires no instrumentation on apps or the Android
framework and can be adapted to different versions of Android.
Taking Control of Android Runtime. The Android framework
supports running an app in the debug mode, under which the
Androidruntimecanbefullycontrolled.Specifically,weconnect
the Android runtime via Android Debug Bridge (ADB) and use
the Android debugger to execute the app under test. With the
debuggerâ€™shelp,wecanperformexecutionstepbystepandmonitor
the appstate duringthreadmanipulation.
Hooking Events. Android adopts the event-driven model and
manages events using an event queue. Each event implements
aninterfacemethodcalled enqueueMessage() andAndroidputs
372Flaky Test Detection in Android via Event Order Exploration ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
Table 1: Subject Apps
App Name Version #LOC #Stars Category
Amaze File Manager 3.4.3 92.2k 2.8k Tools
Youtube Extractor 2.0.0 2.7k 519 Video Players
AntennaPod 1.8.1 102.6k 2.7k Music & Audio
Backpack Design 2.0.7 84.2k 363 Productivity
Barista 3.5.0 67.9k 1.2k Productivity
CameraView 2.6.1 40.5k 2.9k Photography
Catroid 0.9.69 457.5k 0 Education
City Localizer 1.1 4k 0 Travel & Local
ConnectBot 1.9.6 119.7k 1.4k Communication
DuckDuckGo 5.43.0 211.3k 1.2k Tools
Espresso 1.0 17.3k 1.1k Maps & Navigation
Firefox Focus 8.5.1 44.5k 1.6k Communication
Firefox Lite 2.18 1598.4k 212 Communication
FlexBox 2.0.1 29.2k 15.2k Libraries & Demo
GnuCash 2.4.0 90.1k 1k Finance
IBS FoodAnalyzer 1.2 26.1k 0 Health & Fitness
Google I/O 7.0.14 73.5k 19.6k Books & Reference
Just Weather 1.0.0 5.9k 65 Weather
Kaspresso 1.1.0 66.3k 774 Productivity
KeePassDroid 2.5.3 159.7k 1.2k Tools
KickMaterial 1.0 79.1k 1.6k Crowdfunding
KISS Launcher 3.11.9 27.2k 1.4k Personalisation
MedLog 1.0 65k 0 Medical
Minimal To Do 1.2 27.5k 1.8k Productivity
MoneyManagerEx 02.14.994 170k 265 Finance
My Expenses 3.0.7.1 1835.6k 248 Finance
NYBus 1.0 6.9k 272 Transport
Omni Notes 6.0.5 105.9k 2k Productivity
OpenTasks 1.2.4 448k 724 Productivity
ownCloud 2.14.2 333.7k 2.9k Productivity
Sunflower 0.1.6 5.3k 10.1k Gardening
Surveyor 13.3.0 290.4k 13 Communication
WordPress 14.2-rc-2 461.7k 2.3k Productivity
events to the queue by calling this method. We set a breakpoint at
method enqueueMessage() inthedebugmode.Wheneveranevent
is generated and injected into the queue, we perform predefined
operations such as suspending the event-posting thread.
Operating Threads. We leverage the Android debugger to re-
trieve threads running in the app under test, including the testing
threadandUIthread.TheAndroiddebuggerprovidescommandsto
remotelyoperatethreads,e.g.,inspectingthreadstatusandsuspend-
ingaselectedthread.Italsoallowsustoobtainthecurrentstack
frames of a running thread that are used for event identification.
WiththeAndroiddebugger,wecanexecuteatestingthreadstep
by step and observe execution at the statement level by inserting a
breakpoint at each statement in the test.
7 EVALUATION
Weempiricallyevaluate FlakeScanner â€™seffectivenessindetecting
flakytests in the test suites of large-scale Android projects.
7.1 Subject Apps
OurtechniqueisdesignedforatestthatrunsontheAndroidframe-
work platform and checks whether it is flaky. To evaluate our tech-
nique, we need Android projects that contain such tests. Basically,
there are two types of tests in Android projects: instrumented testsTable 2: Root causes of reproduced flaky test failures and
theircategories.
Categories Description of root causes
Async Do not wait or wait not enough when accessing background
resources or services in an async manner for tasks such as
imagerendering,downloadingfrominternet,andinvoking
third party libraries.
Event orders Expecting an implicit event execution order that may not
always occur in the test execution. The unexpected event
execution order leads to app misbehavior such as the soft
keyboard disappearing late.
Data race Checkingunsynchronizeddatabecausecheckingoccursbe-
forethedatabeingupdatedduetothelackofthreadsynchro-
nization.
Lifecycle Performing app-state-sensitive operations on the incorrect
state,e.g.,the testingthreadattempts tooperate GUIwidgets
when the app is resumed state, which is prohibited.
that run on a physical device or Android emulator and local unit
teststhat run on local Java virtual machines. Thus, we need An-
droid projects that contain instrumented tests. Unfortunately, most
AndroidprojectsinexistingbenchmarkssuchasAndroTest[ 11],
industrialappbenchmark[ 44],anddatasetthatisusedforflaky
testempiricalstudy [ 42] have no instrumented tests.
Therefore,webuiltthefirstsubject-suite FlakyAppRepo inwhich
eachAndroidprojectcontainsinstrumentedteststhatarewrittenbydevelopers.AsshowninTable 1,FlakyAppRepo contains33Android
projects(majorityofthemarewell-known,suchasWordPress)andover5000instrumentedtestsfromdevelopers.WecollectedAndroid
projects containing instrumented tests as follows: (1) searching
popularopen-sourceAndroidprojectssuchasFirefoxandmanually
checking their repositories to select projects with instrumented
tests (intuitively, popular projects are well maintained and more
likely have developer tests); (2) searching keywords such as flak,
flakiness,orintermitontheGithubandmanuallycheckingsearched
repositories to select Android projects containing instrumented
tests. Furthermore, we explored these keywords in the commit
history as well to include already fixed flaky tests. A few searched
projects have 0 star on the Github. To achieve diversity, we did not
exclude these projects and kept them in the subject-suite.
7.2 Research Questions
Our evaluation aims to address the following research questions.
RQ1CanFlakeScanner detect known flaky tests that are reported
by developers?
RQ2How does FlakeScanner compare with existing techniques in
terms of number of detected flaky tests?
RQ3CanFlakeScanner be used to discover flaky tests in Android
projects that were previously unknown?
7.3 ExperimentSetup
To answer the research questions, we conduct three empirical stud-
iesontestcasesthatarewrittenbydevelopersinrealworldAndroid
appprojects.
7.3.1 Study 1. We first evaluate FlakeScanner â€™s effectiveness in
flaky test detection by running it on known flaky tests in Android
projects and checking how many of them are marked as flaky tests
byFlakeScanner .
373ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury
Table 3: Dynamic analysis based tools for detecting flaky
tests or concurrency issues in Android apps.
Approach Categories Instrumentation YearReason for not selected
RERUN Flaky tests No -
Shaker [40] Flaky tests No 2020
APEChecker [ 16]Async bugs No 2018publicly unavailable
ERVA [20] Event race Yes 2016Publicly unavailable
EventRacer [ 8]Event race Yes 2015
AsyncDroid [ 24]Async bugs Yes 2015Incompatible instrument API
DataSet. Weselected52knownflakytestsfrom FlakyAppRepo
thatarecausedbyconcurrencyorsynchronizationissuesforStudy
1.InAndroidprojects,flakytestsreportedbydevelopersaremarked
withadedicatedannotation @FlakyTest suchthatflakytestscanbe
automaticallyfilteredoutduringtestexecution.Weidentified269
known flakytests in FlakyAppRepo using @FlakyTest annotation.
We selected known tests from them in the following manner.
Concurrency We select concurrency related flaky tests by manu-
allyanalyzing(1)detailsaboutreportedflakytestssuchas
thereasonofwhyatestisflaky,whichcanbecollectedfrom
detailelement2of@FlakyTest annotation; (2) commit
messageswhenflakytestswereintroducedorfixedinasub-
sequent version;(3) whether there existstatements in tests
that are commonly involved in concurrency or synchroniza-
tion execution, e.g., operations of Runnable orAsyncTask
objects.
Passing Weraneachselectedtestinourexecutionenvironment
andensureditpassesintheinitialrun,avoidingfailuresdue
to environment setup.
Reproducing Wereproducetheflakytestfailureforeachselected
test by analyzing its commits messages and detailed descrip-
tionintherepositoryincludingrootcausesandfailingsce-
narios. For cases lacking the description, we contacted de-
veloperstoseekmoreinsightsonthefailurereproduction.
A test is selected if its failures is reproduced.
Intheend,52knownflakytestswereselectedfortheevaluation.
Meanwhile,wealsoanalyzedrootcausesoftheseflakytestfailures
andclassifiedthemintofourcategorieswhichareshowninTable 2.
7.3.2 Study 2. ToanswerRQ2,weevaluate FlakeScanner andexist-
ing tools on the data set used in Study 1. We reviewed most recent
workson flakytestsor eventracedetection forAndroidapps and
summarize them in Table 3. We selected the following approaches
for comparison.
â€¢RERUN.In practice, developers often run a test many times
to check whether the test is flaky. Similarly, we run a test
100 times and check whether the flaky test failure manifests
during execution. We call the approach 100RUNand takeit
as our baseline.
â€¢Shaker[40] is the most recently reported state-of-the-art
technique for detecting concurrency-related flaky tests in
Androidapps. Shakerattemptstomanifestaflakytestfailure
byaddingnoisesintheexecutionenvironmenttoaffectevent
execution order, e.g., changing CPU workload.
â€¢EventRacer [8] is a the-state-of-art dynamic technique of
detecting event races in Android apps, which infers races
2https://developer.android.com/reference/androidx/test/filters/FlakyTestbyanalyzingruntimetracesproducedbyaninstrumented
Android framework.
7.3.3 Study 3. To answer RQ3, we run FlakeScanner on tests in
FlakyAppRepo without @FlakyTest annotation,i.e.,teststhatare
notreportedasflakytests.Wefirstrunthesetestsinourexecution
environment and exclude tests that fail, then feed passing tests
toFlakeScanner and report tests that are labeled as flaky tests by
FlakeScanner .
EffectofDebugMode. FlakeScanner runsappsinthedebugmode.
The debug mode environment may affect flaky test detection by
impacting the Android event dispatching mechanism or increasing
execution workload. According to the official Android document3,
the difference between normal execution mode and debug mode is
thattheAndroidvirtualmachine(VM)loadsanadditionalAndroid
Runtime Tooling Interface (ARTTI) plugin that exposes runtime
internals (e.g., variable values). The ARTTI plugin runs as a VM-level component and it is transparent to event dispatching that
occursintheappâ€™smainthread,i.e.,runningappsindebugmode
does not impact the mechanism of event dispatching.
However,runningtheARTTIpluginandaccessingruntimein-
ternalsintheexecutioncouldincreasetheworkloadoftheVM.Duetotheworkloadchange,atestcanmanifesttheflakytestfailure.To
rule out such cases, for each test in our study, we pre-execute it to
ensure the test passes in the debug mode environment. To perform
end-to-endcomparisoninStudy2,werun 100RUNandShakerin
the debugmode environment as well.
ExecutionEnvironment. Weconductedexperimentsonaphysical
machine with 64 GB RAM and a 56 cores Intel(R) Xeon(R) E5-2660
v4 CPU, running a 64-bit Ubuntu 16.04 operating system. Each
execution instance runs in a Docker container to minimize the
potentialinferencebetweenrunninginstances.Appundertestruns
on an Android 9 (x86) emulator. One execution instance is for one
test case for which the Android emulator is initialized to a fresh
state at the beginning to provide a clean testing environment.
7.4 RQ1: Efficacy
Table4shows results on each known flaky test for study 1. The
first column indicates known test Ids, the second column showsapp names and testing frameworks used in apps, and the third
columnindicatestestmethodnames.Columnâ€œ#Eventâ€indicatesthenumberofeventsobservedby FlakeScanner duringdetection.â€œ#Runâ€
indicates the number of test runs in the event order exploration.
Columnâ€œTimeâ€reportsthetimethatisusedtodetectaflakytest.
Columnâ€œSuccâ€indicateswhetherthetestisidentifiedasaflakytest
byFlakeScanner .â€œ-â€indicatesthatatestisunabletobeexecuted
due to library compatibility issues. Firefox-Lite marked with â€œ*â€
(rows 39-40)is a previousversion (commit:465739510e). Notethat
some testnames in the table are shortened for readability.
Results. FlakeScanner successfullydetected45outof52known
flaky teststhat are from 10 Android projects (including a different
versionofFirefoxLite).Onaverage, FlakeScanner detectedaflaky
testin101seconds.Themaximumdetectiontimeisfortest20from
app AntennaPod (691 seconds). The minimum detection time is for
3https://source.android.com/devices/tech/dalvik/art-ti
374Flaky Test Detection in Android via Event Order Exploration ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
Table 4: Results on known flaky tests by FlakeScanner ,Shakerand 100RUN .
IdApp:
FrameworkMethod nameFlakeScanner Shaker 100Run
#Events #RunTime(s) Succ Time(s) SuccTime(s) Succ
1
Surveyor:
Espressocapture 58 248 4301.76 80
2 twoQuestions 104 248 2639.1 1236
3 multimedia 233 8542 1607.28 486
4 contactDetails 104 248 2819.93 1300
5Youtube
Extractor:
JUnit4testEncipheredVideo 4 224 112.136 9
6 testUnembeddable 7 322 151.783 103
7 testAgeRestrictVideo 5 312 103.136 78
8 testUsualVideo 5 314 107.094 77
9
MyExpenses:
EspressotestScenarioForBug5b.. 861 2100 583.922 704
10 editCommandKeeps.. 101 254 929.601 857
11 cloneCommandIncreases.. â€“ â€“â€“â€“949.274 879
12 changeOfFractionDigits.. 991 2170 672.984 693
13 changeOfFractionDigitsWith.. 991 2174 589.684 637
14
Firefox Lite:
EspressosaveImageThenDelete.. 627 5352 1954.07 1088
15 dismissMenu 165 276 1381 742
16 turnOnTurboMode.. 89 240 926.728 5
17 changeDisplayLang 189 372 3056.51 680
18
AntennaPod:
RobotiumtestGoToPreferences 107 356 418.265 4
19 testClickNavDrawer 132 260 2279.01 5
20 PlaybackSonicTest#..On.. 177 10691 3243.652 1128
21 PlaybackSonicTest#..Off.. 176 6495 1233.66 1231
22 PlaybackTest#..Off..Episodes 175 2142 1251.95 1237
23 PlaybackTest#..On..Episodes 162 2140 1071.3 1128
24
FlexBox:
EspressotestScrollToPosition..row 219 6306 416.673 837
25 testAddViewHolders.. 46 217 142.923 196
26 testChangeAttributes.. 23 220 102.37 3
27 testMinHeight..minHeight 58 7112 105.134 176
28 testJustifyContent..views 58 240 110.923 168
29 testJustifyContent_center 58 9158 211.181 173
30 testFlexWrap..column 55 232 107.355 180
31 testFirstViewGone..column 58 232 106.491 168
32 testChangeOrder..Params 26 436 102.21 178
33 testAlignItems..column 58 232 107.798 180
34 testAlignContent..column 58 232 107.021 189
35 testAlignContent..column 58 232 105.902 198
36 testAlignContent..Padding 56 242 107.256 193
37 testFlexLines..row 74 332 246.254 344
38 testFlexLines..column 69 236 221.768 366
39Firefox Lite( â˜…):
EspressobrowsingWebsite.. 642 294 2192.65 159
40 saveImageThenDelete.. 661 396 1415.6 1117
41
BackPack:
JUnit4test_with_description 41 211 146.354 289
42 test_with_title 40 211 150.643 303
43 test_bottom_sheet_style 32 430 149.486 207
44 test_alert_style 32 431 150.811 199
45 screenshotTestDialog.. 86 223 285.117 267
46 screenshotTestDialog 86 223 290.37 263
47 test_with_buttons 42 311 174.781 348
48Barista:
JUnit4overflowMenuClick_byTitle 130 382 240.39 309
49 openOverflowMenu_..Option 58 221 310.67 323
50 overflowMenuClick_byId 127 482 280.342 350
51Kaspresso:
JUnit4CommonFlakyTest#test 93 226 2232.77 1896
52 UiCommonFlakyTest#test 94 5295 2048.74 1941
Avg/SUM 169 3101 45861 15497 8
375ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury
test41,42,and47fromappBackPack(11seconds). FlakeScanner
detectedaflakytestwithin3testrunsonaverage.Themaximum
number of test runs is for test 20 from AntennaPod (10 runs). To
understand why FlakeScanner successfully detected failures within
a few test runs, we inspected source code of projects. The major
reason is that tests heavily use synchronization operations. Forinstance, test 2 from Surveyor contains 21 test statements, 15 of
which use the synchronization operation provided by Espresso [ 2]
onView() . When onView() is called in the test statement, the test-
ingthreadwaitsuntilbackgroundthreadscompletetasks.Asde-
signed,FlakeScanner does not perform event scheduling for those
cases since async events are bounded in the execution of a test
statement by the synchronization operation. Then FlakeScanner
canfocusonschedulingeventsforstatementsthatlacksynchro-
nizationoperations.Moreover, FlakeScanner prioritizesexploring
positions that are closer to the upper bound event, which likely
triggersflakytestfailures(Section 4.3).Thus,FlakeScanner could
detect failures within a few test runs. The results also show FlakeS-
canneris a practical tool. It successfully detected flaky test failures
for test 12 and 13 from MyExpenses, for which 991 events were
generatedintheexecution.Meanwhile, FlakeScanner workedon
Androidprojectsthatadoptdifferenttestingframeworkssuchas
Espresso [ 2] and Robotium [ 3].
FlakeScanner successfully detected 45 out of 52 known flaky
tests in 10 Android projects. On average, it detected a flaky test
within3 test runs.
7.5 RQ2:ComparisonwithExistingTechniques
AsshowninTable 4,FlakeScanner outperforms Shakerand100RUN
intermsofboththenumberofdetectedflakytestsandtheaverage
executiontime.Outofthe52knownflakytests, FlakeScanner de-
tectedthemostflakytests(45)andisfollowedby Shaker(15)and
100RUN(8). For execution time, FlakeScanner detected a flaky test
in101secondsonaverage,whichislessthan861secondsof Shaker
and497secondsof 100RUN.Formostoftests, 100RUNcouldnot
detectaflakytestfailureinthefirstfewrunsandkeptexecuting
themuntil reaching 100 times, which took a longer time.
Regarding the overlap between flaky tests detected by each tool,
FlakeScanner detected all the flaky tests detected by Shakerand
100RUN. ButShakerand100RUNfailed to detect the other flaky
teststhatweredetectedby FlakeScanner .Thebetterresultsfrom
FlakeScanner canbeexplainedasfollows: FlakeScanner caniden-
tify synchronizationoperations inthe testexecution andfocus on
schedulingeventsforstatementsthatlacksynchronizationoper-
ations. Unexpected event execution orders that cause flaky test
failures are more likely to explored by FlakeScanner .
Wealsoevaluated EventRacer sinceitusesdynamicanalysisto
infer event race for Android apps. EventRacer reported many possi-
ble races for each test run. On average, it reported 1237 races for a
test.Thisisbecause EventRacer focusesondetectingracesinAn-
droidappsanddoesnotanalyzeracesinwhichtestingframeworks
are involved. Furthermore, EventRacer infers races by analyzing
recordedtracesandcannotvalidatewhetherthereportedracescan
cause flakytest failures.FlakeScanner outperforms Shakerand100RUNin terms of both
the number of detected flaky tests and average execution time.
7.6 RQ3: Real-World Flaky Test Detection
We ran 1444 passing tests from the 33 Android projects in FlakyAp-
pRepowhich are not annotated as flaky tests (these tests may or
maynotbeflaky).Outofthese33projects, FlakeScanner detected
atleastoneflakytestfor19projects,andreported245flakytestsin
total. To validate previously unknown flaky tests that FlakeScanner
detected, we randomly selected 20 out of the detected 245 flaky
tests and reported them to developers. For each selected test, we
manually reproducedthe failure that FlakeScanner witnessed dur-
ingdetectionandgeneratedadetailedroot-cause-analysisreport,
and submitted the report on the Github. At the time of writing the
paper, we got responses on 15 test cases. Out of the 15 tests, 13
were confirmed as flaky tests and addressed by developers. For the
othertwotests,developersrepliedthatthereportedfailureswere
not encountered yet or not reproduced at their end, without giving
us further explanation. Our experience with flaky test reporting
shows so far that developers are more interested in identifying
whichtestsareflaky.Onceaflakytestisdetected,theyappearto
be more prone to removing them, rather than investigating why it
is flaky.
FlakeScanner detected 245 previously unknown flaky tests in 19
widely-used Android projects. Out of the reported 20 unknown
flakytests, 13 were confirmed and addressed by developers.
7.7 Threatsto Validity
ExternalValidity: Threatstoexternalvalidityrelatetothegener-
alizability of the experimental results. FlakeScanner is evaluated so
far on 33 Android projects. Our results may not generalise beyond
the33Androidprojectstowhichwehaveapplied FlakeScanner .T o
mitigate thisthreat, we not only choose Android projects that are
popular and well maintained but also include less popular Android
projects (i.e., less stars on the Github) which were searched on the
Githubvia keywords.
Internal Validity: Threats to internal validity concern factors
in our experimental methodology that may affect our results. InStudy 1, we note that 52 concurrency or synchronization related
knownflakytestsarechosenbymanuallyanalyzingtheirrelated
descriptions and commit messages, which might result in selection
bias. Similarly, we manually analyze failures detected by each tool
underevaluationandvalidatetheresults,whichmightintroduce
bias as well. To mitigate these risks, two authors of this paper
independentlyperformedthemanualtasks,andcross-checkedeach
otherâ€™sresults.
7.8 Data Availability
To facilitate future research on flaky tests, we make our prototype
FlakeScanner andsubject-suite FlakyAppRepo availableatlink https:
//github.com/AndroidFlakyTest
376Flaky Test Detection in Android via Event Order Exploration ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece
8 RELATED WORK
FlakyTestDetection. Belletal.[ 6]usecodecoveragebaseddif-
ferential analysis to identify flaky tests. A test is deemed flaky if it
fails during regression testing and its execution does not reach any
code that was recently changed by developers. Lam et al. propose
iDFlakies [ 27], a framework for detecting order dependent flaky
tests,teststhatfailwhenrunindifferentorders.Duttaetal.[ 13]
developanapproachtodetectrandomnumberrelatedflakytests,
tests that fail due to difference in the sequence of random numbers
generatedindifferentruns.Shietal.[ 39]proposeanapproachto
fixorder-dependentflakytestsbyleveragingpassingtests.Shiet
al.[37]propose toreruna testmultipletimes oneachmutant and
obtainreliablecoverageresultssuchthattheeffectsofflakytests
on mutation testing can be mitigated. In contrast, FlakeScanner de-
tectsconcurrencyorsynchronizationrelatedflakytestsinAndroid
projects by exploring feasible event execution orders.
Event Race Detection. Another branch of works that are close
to ours is event race detection [ 8,18â€“21,31â€“33,35,36,45]. In-
stead of detecting flaky tests, these works leverage dynamic and
static analysis to detect event races. For instance, ER Catcher [ 35],
DROIDRACER[ 32],EventRacer [8],CAFA[ 19],andnAdroid[ 18]
capturehappens-before-relation among events and infer possible
eventraces.Inaddition,Ozkanetal.[ 24]proposetodetectasyn-
chronous bugs by exploring different execution orders of eventhandlers in Android apps. SARD [
46] leverages happens-before
analysistodetectuse-after-freeissuesinAndroidapps.Thesetech-
niqueshavethepotentialtobeapplicableforflakytestdetection,
butfacechallengestocapturecompleteandprecisehappens-beforerelationswhenatestisexecutedbyatestingframework.Manyfalse
positivescanbereportedbyevent-racedetectorsduetoincomplete
happen-beforerelationsbeingcompute.Incontrast, FlakeScanner
performs a system-level dynamic analysis to capture precise event
dependenciesto avoid such false positives.
Empirical Studies on Flaky Tests. Multiple studies [ 14,26,30,42]
confirmconcurrencyasthemajorcauseofflakytests.Luoetal.[ 30]
performed an empirical analysis of flaky tests in 51 open-source
projects.Theyidentified Concurrency andAsyncwait asthemost
common cause of flaky tests. They pointed out that the majority of
thesecasesarosebecausetheydonotwaitforexternalresources.
Finally,theydescribedthecommonfixingstrategiesthedevelopers
use to fix flaky tests. In a separate study, Eck et al.[ 14] surveyed
21 professional developers to classify 200 flaky tests they fixed.
Theyidentifiedfourunreportedcausesofflakytests,whicharealso
considereddifficulttofix.Thorveetal.[ 42]conductedanempirical
studyofflakytestsinAndroidapps.Theysearched1000projects
for the commits related to flakiness and found only 77 relevantcommits from 29 projects. They found 36% of commits occurred
duetoconcurrencyrelatedissues.Fanetal.[ 16]proposedahybrid
approachtowardsmanifestingasynchronousbugsinAndroidapps
withfault patterns.
Concurrency Bug Detection. There have been several testing
based approaches [ 10,12,22,23,28,47] to identify concurrency
relatedbugs.Maple[ 47]proposedacoverage-drivenapproachtoex-
poseuntestedthreadinterleavings.Letko[ 28]proposedacombina-
tionoftestinganddynamicanalysiswith metaheuuristic techniques.Choudhary et al. [ 10] presented a coverage-guided approach for
generatingconcurrencyteststo detectbugs in thread-safeclasses.
Multiple related works [ 5,7,9,15,29,43] manipulated event or-
derstocontrolnon-determinisminmulti-threadedprograms.Liu
etal.[29]proposedadeterministicmultithreadingsystemthatre-
placespthreadslibrary in C/C++ apps. Emmi et al. [ 15] proposed a
searchprioritizationstrategytodiscoverconcurrencybugs.They
add non-determinism to deterministic schedulers by delaying their
next-scheduled task. Adamsen et al. [ 5] presented an automated
programrepairtechniqueforeventraceerrorsinJavaScript.Given
a repair policy, they controlled the event handler scheduling in the
browser to avoid bad orderings.
9 DISCUSSION
Flaky tests pose a significant problem in validating mobile apps. In
thispaper,wepresentedanapproachfordetectingflakytestsvia
systematic event order exploration. We introduced FlakeScanner ,a
tooltodetectflakytestsforAndroidapps. FlakeScanner explores
the space of possible execution environments which may cause
relevantthreadstointerleavedifferently.Duetothelackofatest-
ing benchmark for flaky tests, we created the first subject-suite
FlakyAppRepo that is used to study test flakiness. FlakyAppRepo
contains33widely-usedAndroidappswitharound2.5kstarson
average in GitHub. We applied FlakeScanner to tests from FlakyAp-
pRepo. Results show that FlakeScanner not only detected known
flaky tests but also reported 245 new flaky tests. We believe that
ourtoolandresultsholdoutpromisefortacklingflakytests,which
is a significant pain point in the practice of testing.
ACKNOWLEDGEMENTS
This work was partially supported by the National Research Foun-
dationSingapore(NationalSatelliteofExcellenceinTrustworthy
Software Systems).
REFERENCES
[1]2016. Flaky Tests at Google and How We Mitigate Them. https://testing.
googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html
[2] 2020. Espresso. https://developer.android.com/training/testing/espresso
[3] 2020. Robotium. https://github.com/RobotiumTech/robotium
[4]Christoffer Quist Adamsen, Gianluca Mezzetti, and Anders MÃ¸ller. 2015. Sys-tematic Execution of Android Test Suites in Adverse Conditions. In Proceed-
ings of the 2015 International Symposium on Software Testing and Analysis .
https://doi.org/10.1145/2771783.2771786
[5]C. Q. Adamsen, A. MÃ¸ller, R. Karim, M. Sridharan, F. Tip, and K. Sen. 2017.
Repairing Event Race Errors by Controlling Nondeterminism. In 2017 IEEE/ACM
39th International Conference on Software Engineering (ICSE) .https://doi.org/10.
1109/ICSE.2017.34
[6]J.Bell,O.Legunsen,M.Hilton,L.Eloussi,T.Yung,andD.Marinov.2018. DeFlaker:
AutomaticallyDetecting FlakyTests.In 2018IEEE/ACM 40thInternationalCon-
ference on Software Engineering (ICSE) .https://doi.org/10.1145/3180155.3180164
[7]Tom Bergan, Luis Ceze, and Dan Grossman. 2013. Input-covering schedules for
multithreaded programs. (2013). https://doi.org/10.1145/2509136.2509508
[8]Pavol Bielik, Veselin Raychev, and Martin Vechev. 2015. Scalable Race Detection
for Android Applications. (2015). https://doi.org/10.1145/2814270.2814303
[9]AhmedBouajjani,MichaelEmmi,ConstantinEnea,BurcuKulahciogluOzkan,
and Serdar Tasiran. 2017. Verifying Robustness of Event-Driven Asynchronous
Programs Against Concurrency .https://doi.org/10.1007/978-3-662-54434-1_7
[10]Ankit Choudhary, Shan Lu, and Michael Pradel. 2017. Efficient Detection of
ThreadSafetyViolationsviaCoverage-GuidedGenerationofConcurrentTests.
InIEEE/ACMInternationalConferenceonSoftwareEngineering .https://doi.org/
10.1109/ICSE.2017.32
[11]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Auto-
matedTestInputGenerationforAndroid:AreWeThereYet?.In Proceedingsofthe
377ESEC/FSE â€™21, August 23â€“28, 2021, Athens, Greece Zhen Dong, Abhishek Tiwari, Xiao Liang Yu, and Abhik Roychoudhury
2015 30th IEEE/ACM International Conference on Automated Software Engineering
(ASE).https://doi.org/10.1109/ASE.2015.89
[12]James Davis, Arun Thekumparampil, and Dongyoon Lee. 2017. Node.fz: Fuzzing
the Server-Side Event-Driven Architecture. In European Conference on Computer
Systems.https://doi.org/10.1145/3064176.3064188
[13]Saikat Dutta, August Shi, Rutvik Choudhary, Zhekun Zhang, Aryaman Jain, and
SasaMisailovic.2020.DetectingFlakyTestsinProbabilisticandMachineLearning
Applications.In Proceedingsofthe29thACMSIGSOFTInternationalSymposium
on Software Testing and Analysis .https://doi.org/10.1145/3395363.3397366
[14]Moritz Eck, Fabio Palomba, Marco Castelluccio, and Alberto Bacchelli. 2019. Un-
derstanding flaky tests: the developerâ€™s perspective. In 27th ACM Joint European
SoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering (ESEC/FSE) .https://doi.org/10.1145/3338906.3338945
[15]Michael Emmi, Shaz Qadeer, and Zvonimir Rakamaric. 2011. Delay-Bounded
Scheduling.In ProceedingsofSymposiumonPrinciplesofProgrammingLanguages .
https://doi.org/10.1145/1926385.1926432
[16]LinglingFan,TingSu,SenChen,GuozhuMeng,YangLiu,LihuaXu,andGeguangPu. 2018. Efficiently Manifesting Asynchronous Programming Errors in Android
Apps.InProceedingsofthe33rdACM/IEEEInternationalConferenceonAutomated
Software Engineering .https://doi.org/10.1145/3238147.3238170
[17]MartinFlower.2020. Eradicatingnon-determinismintests. https://martinfowler.
com/articles/nonDeterminism.html
[18]Xinwei Fu, Dongyoon Lee, and Changhee Jung. 2018. nAdroid: staticallydetecting ordering violations in Android applications. In Proceedings of the
2018 International Symposium on Code Generation and Optimization .https:
//doi.org/10.1145/3168829
[19]Chun-Hung Hsiao, Jie Yu, Satish Narayanasamy, Ziyun Kong, Cristiano Pereira,
GillesPokam,PeterChen,andJasonFlinn.2014. RaceDetectionforEvent-Driven
Mobile Applications. (2014). https://doi.org/10.1145/2594291.2594330
[20]Yongjian Hu, Iulian Neamtiu, and Arash Alavi. 2016. Automatically verifying
and reproducing event-based races in Android apps. In International Symposium
on Software Testing and Analysis .https://doi.org/10.1145/2931037.2931069
[21]Jeff Huang and Arun K. Rajagopalan. 2016. Precise and Maximal Race Detection
from Incomplete Traces. In Proceedings of the 2016 ACM SIGPLAN International
ConferenceonObject-OrientedProgramming,Systems,Languages,andApplications .
https://doi.org/10.1145/2983990.2984024
[22]CaseyKlein,MatthewFlatt,andRobertFindler.2010. RandomTestingforHigher-
Order, Stateful Programs. In Proceedings of the Conference on Object-Oriented
Programming Systems, Languages, and Applications .https://doi.org/10.1145/
1869459.1869505
[23]Bohuslav Krena, Zdenek Letko, Tomas Vojnar, and Shmuel Ur. 2010. A platform
for search-based testing of concurrent software. In International Workshop on
ParallelandDistributedSystems:Testing,Analysis,andDebugging .https://doi.
org/10.1145/1866210.1866215
[24]Burcu Kulahcioglu Ozkan, Michael Emmi, and Serdar Tasiran. 2015. Systematic
AsynchronyBugExplorationforAndroidApps.In InternationalConferenceon
Computer Aided Verification .https://doi.org/10.1007/978-3-319-21690-4_28
[25]Wing Lam, Patrice Godefroid, Suman Nath, Anirudh Santhiar, and Suresh Thum-
malapenta.2019. Rootcausingflakytestsinalarge-scaleindustrialsetting.In
Proceedingsofthe28thACMSIGSOFTInternationalSymposiumonSoftwareTesting
and Analysis .https://doi.org/10.1145/3293882.3330570
[26]Wing Lam, Kivanc Muslu, Hitesh Sajnani, and Suresh Thummalapenta. 2020. A
StudyontheLifecycleofFlakyTests.In 42ndInternationalConferenceonSoftware
Engineering .https://doi.org/10.1145/3377811.3381749
[27]W. Lam, R. Oei, A. Shi, D. Marinov, and T. Xie. 2019. iDFlakies: A Framework
forDetectingandPartiallyClassifyingFlakyTests.In 12thIEEEConferenceon
Software Testing, Validation and Verification .https://doi.org/10.1145/3238147.
3240465
[28]ZdenÄ›k Letko. 2013. Analysis and Testing of Concurrent Programs. Information
Sciences and Technologies Bulletin of the ACM Slovakia (2013).
[29]TongpingLiu,CharlieCurtsinger,andEmeryBerger.2011. Dthreads:Efficient
deterministic multithreading. In Proceedings of the 23rd ACM Symposium on
Operating Systems Principles .https://doi.org/10.1145/2043556.2043587
[30]Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An
empiricalanalysisofflakytests.In InternationalSymposiumonFoundationsof
Software Engineering (FSE) .https://doi.org/10.1145/2635868.2635920
[31]PallaviMaiyaandAdityaKanade.2017. EfficientComputationofHappens-before
Relation for Event-Driven Programs. In Proceedings of the 26th ACM SIGSOFT
International Symposium on Software Testing and Analysis .https://doi.org/10.1145/3092703.3092733
[32]PallaviMaiya,AdityaKanade,andRupakMajumdar.2014. RaceDetectionfor
Android Applications. In Proceedings of the 35th ACM SIGPLAN Conference on
Programming Language Design and Implementation .https://doi.org/10.1145/
2594291.2594311
[33]ArunK.RajagopalanandJeffHuang.2015.RDIT:RaceDetectionfromIncomplete
Traces.In Proceedingsofthe201510thJointMeetingonFoundationsofSoftware
Engineering .https://doi.org/10.1145/2786805.2803209
[34]Alan Romano, Zihe Song, Sampath Grandhi, Wei Yang, and Weihang Wang.
2021. An Empirical Analysis of UI-based Flaky Tests. In IEEE/ACM International
Conference on Software Engineering .https://doi.org/10.1109/ICSE43902.2021.
00141
[35]Navid Salehnamadi, Abdulaziz Alshayban, Iftekhar Ahmed, and Sam Malek.
2020. ER Catcher: A Static Analysis Framework for Accurate and Scalable Event-
Race Detection in Android. In Proceedings of the 35th IEEE/ACM International
Conference on Automated Software Engineering .https://doi.org/10.1145/3324884.
3416639
[36]Anirudh Santhiar, Shalini Kaleeswaran, and Aditya Kanade. 2016. EfficientRace Detection in the Presence of Programmatic Event Loops. In Proceedings
of the 25th International Symposium on Software Testing and Analysis .https:
//doi.org/10.1145/2931037.2931068
[37]AugustShi,JonathanBell,andDarkoMarinov.2019.Mitigatingtheeffectsofflaky
tests on mutation testing. In Proceedings of the 28th ACM SIGSOFT International
SymposiumonSoftwareTestingandAnalysis .https://doi.org/10.1145/3293882.
3330568
[38]A. Shi, A. Gyori, O. Legunsen, and D. Marinov. 2016. Detecting Assumptions
on Deterministic Implementations of Non-deterministic Specifications. In 2016
IEEE International Conference on Software Testing, Verification and Validation .
https://doi.org/10.1109/ICST.2016.40
[39]August Shi, Wing Lam, Reed Oei, Tao Xie, and Darko Marinov. 2019. iFixFlakies:
a framework for automatically fixing order-dependent flaky tests. In Proceedings
of the 27th ACM Joint Meeting on European Software Engineering Conference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC-FSE) .https:
//doi.org/10.1145/3338906.3338925
[40]DeniniSilva,LeopoldoTeixeira,andMarcelodâ€™Amorim.2020. ShakeIt!DetectingFlakyTestsCausedbyConcurrencywithShaker.In IEEEInternationalConference
on Software Maintenance and Evolution .https://doi.org/10.1109/ICSME46990.
2020.00037
[41]ValerioTerragni,PasqualeSalza,andFilomenaFerrucci.2020. AContainer-Based
InfrastructureforFuzzy-DrivenRootCausingofFlakyTests.In Proceedingsof
the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas
and Emerging Results .https://doi.org/10.1145/3377816.3381742
[42]Swapna Thorve, Chandani Sreshtha, and Na Meng. 2018. An Empirical Study of
FlakyTestsinAndroidApps.In InternationalConferenceonSoftwareMaintenance
and Evolution .https://doi.org/10.1109/ICSME.2018.00062
[43]Ermenegildo Tomasco, Omar Inverso, Bernd Fischer, Salvatore La Torre, and
Gennaro Parlato. 2015. Verifying Concurrent Programs by Memory Unwinding.
In21st International Conference on Tools and Algorithms for the Construction and
Analysis of Systems .https://doi.org/10.1007/978-3-662-46681-0_52
[44]Wenyu Wang, Dengfeng Li, Wei Yang, Yurui Cao, Zhenwen Zhang, Yuetang
Deng, and Tao Xie. 2018. An Empirical Study of Android Test Generation Tools
in Industrial Cases. In Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering .
[45]D. Wu, J. Liu, Y. Sui, S. Chen, and J. Xue. 2019. Precise Static Happens-Before
Analysis for Detecting UAF Order Violations in Android. In 2019 12th IEEE
Conference on Software Testing, Validation and Verification (ICST) .https://doi.
org/10.1109/ICST.2019.00035
[46]Diyu Wu, Jie Liu, Yulei Sui, Shiping Chen, and Jingling Xue. 2019. Precise Static
Happens-Before Analysis for Detecting UAF Order Violations in Android. In
2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST) .
https://doi.org/10.1109/ICST.2019.00035
[47]JieYu,SatishNarayanasamy,CristianoPereira,andGillesPokam.2012. Maple:A
Coverage-Driven Testing Tool for Multithreaded Programs. In Proceedings of the
ConferenceonObject-OrientedProgrammingSystems,Languages,andApplications .
https://doi.org/10.1145/2384616.2384651
[48]Sai Zhang, Darioush Jalali, Jochen Wuttke, KÄ±vanÃ§ MuÅŸlu, Wing Lam, Michael D.
Ernst, and David Notkin. 2014. Empirically revisiting the test independence
assumption. In Proceedings of the 2014 International Symposium on Software
Testing and Analysis .https://doi.org/10.1145/2610384.2610404
378