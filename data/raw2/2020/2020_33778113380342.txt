Big Code != Big Vocabulary:
Open-Vocabulary Models for Source Code
Rafael-Michael Karampatsis
University of Edinburgh
Edinburgh, United KingdomHlib Babii
Free University of Bozen-Bolzano
Bozen-Bolzano, ItalyRomain Robbes
Free University of Bozen-Bolzano
Bozen-Bolzano, Italy
Charles Sutton
Google Research and University of
Edinburgh
Mountain View, CA, United StatesAndrea Janes
Free University of Bozen-Bolzano
Bozen-Bolzano, Italy
ABSTRACT
Statistical language modeling techniques have successfully been
applied to large source code corpora, yielding a variety of new
software development tools, such as tools for code suggestion, im-
provingreadability,andAPImigration.Amajorissuewiththese
techniquesisthatcodeintroducesnewvocabularyatafarhigher
rate than natural language, as new identifier names proliferate.
Bothlargevocabulariesandout-of-vocabularyissuesseverelyaf-
fect Neural Language Models (NLMs) of source code, degrading
their performance and rendering them unable to scale.
In this paper, we address this issue by: 1) studying how various
modelling choices impact the resulting vocabulary on a large-scale
corpus of 13,362 projects; 2) presenting an open vocabulary source
code NLM that can scale to such a corpus, 100 times larger than in
previouswork;and3)showingthatsuchmodelsoutperformthe
state of the art on three distinct code corpora (Java, C, Python). To
our knowledge, these are the largest NLMs for code that have been
reported.
Alldatasets,code,andtrainedmodelsusedinthisworkarepublicly
available.
CCS CONCEPTS
•Softwareanditsengineering →Softwaremaintenancetools .
KEYWORDS
Naturalness of code, Neural Language Models, Byte-Pair Encoding
ACM Reference Format:
Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton,
and Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary
ModelsforSourceCode.In 42ndInternationalConferenceonSoftwareEn-
gineering(ICSE’20),May23–29,2020,Seoul,RepublicofKorea. ACM,New
York, NY, USA, 13 pages. https://doi.org/10.1145/3377811.3380342
ICSE ’20, May 23–29, 2020, Seoul, Republic of Korea
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7121-6/20/05.
https://doi.org/10.1145/3377811.33803421 INTRODUCTION
Manyworkshavetakenadvantageofthe“naturalness”ofsoftware
[44] to assist software engineering tasks, including code comple-
tion[76],improvingcodereadability[ 2],programrepair[ 20,78],
identifying buggy code [ 75] and API migration [ 38], among many
others[4].Theseapproachesanalyzelargeamountsofsourcecode,
ranging from hundreds to thousands of software projects, building
machine learning models of source code properties, inspired by
techniques from natural language processing (NLP).
When applying any NLP method to create any type of software
development tool, a crucial early decision is how to model soft-
ware’svocabulary.Thisisallthemoreimportantbecause,unlike
innaturallanguage, softwaredevelopersarefreetocreateany
identifierstheylike,andcanmakethemarbitrarilycomplex .
Because of this fundamental fact, any model that is trained on a
large-scalesoftwarecorpushastodealwithanextremelylargeand
sparsevocabulary(Section2).Rarewordscannotbemodelledeffec-
tively. Furthermore, if identifiers were not observed in the training
set,manyclassesofmodelscannotpredictthem,whichisknown
as theout-of-vocabulary (OOV) problem. Hellendoorn and Devanbu
observe this issue for the task of language modeling, showing that
aneurallanguagemodel(NLM)hasdifficultiesscalingbeyondas
fewasahundredprojects[ 41].Giventhatneuralapproachesare
thestate-of-the-artinNLP,findingwaystoscalethemtoalarger
software corpus is a very important goal.
Ourfirst contribution is a thorough study of the effects of the
vocabularydesignchoicesthatmustbemadewhencreatingany
NLP model of software (Section 4). The vocabulary design choices
westudyincludehowtohandlecomments,stringliterals,andwhite
space;whethertofilteroutinfrequenttokens;andwhetherandhowtosplitcompoundtokens,suchasnamesthatcontaincamelcaseand
underscores. We examine how these choices affect the vocabulary
size,whichaffectsthescalabilityofmodels,andhowtheyaffectthe
OOV rate, that is, how often the vocabulary fails to include names
thatappearinnewprojects.Wefindthatthechoiceshavealarge
impact,leadingtovariationsinvocabularysizeofupto threeorders
of magnitude. However, we find that the most common ways to
reduce vocabulary that were previously considered in the software
engineering literature, such as splitting identifiers according to
underscoresandcase,arenotenoughtoobtainavocabularyofa
manageablesize;advancedapproachessuchasadaptationsofthe
Byte-Pair Encoding (BPE) algorithm [ 34,79] are needed to reach
this goal and deal with the OOV problem.
10732020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
This work is licensed under a Creative Commons Attribution International 4.0 License
Thisempiricalstudymotivatesour secondcontribution.Drawing
on our results, we develop a large-scale open-vocabulary NLM for
source code (Section 5). To our knowledge, this is the first BPE
NLMforsourcecodereportedintheliterature.ThisNLMmodel
leverages BPE, beam search, and caching to both keep vocabulary
sizelowandsuccessfullypredictOOVtokens.Weshowthatthis
NLM is able to scale: we train it on up to 13,362 software projects,
yielding the largest NLM trained on source code we are aware of.
Finally, in our third contribution we extensively evaluate our
NLM (Sections 6–8). We show that the open-vocabulary NLM out-
performsboth n-gramLMsandclosedvocabularyNLMsforthetask
of code completion for several languages (Java, C, and Python). To
showthatimprovementinlanguagemodellingtransferstodown-
stream SE tasks, we conduct an experiment similar to Ray et al .
[75],whoshowedthatlanguagemodelscanbeusedtohighlight
buggycode.Indeed,wefindthatouropen-vocabularyNLMismore
effective than previous LMs at highlighting buggy code.
More broadly, these contributions may impact future develop-
ment software tools. First, source code LMs have been used in a
diverse variety of tools well beyondthe obvious application of au-
tocompletion, ranging from code readability [ 2] to program repair
[20].OurimprovedNLMcouldleadtoimprovementstoallofthese
tools. Second, recent results in NLP [ 28,46,69] show that NLMs
can be used as upstream tasks in transfer learning, leading to state-
of-the-art improvementin downstream tasks: for instance, a modelcanbepre-trainedasanNLM,andlateronfine-tunedasaclassifier.
Improved NLM architectures could lead to improved downstream
classifiers,especiallyif thelabelleddataisscarce.Whiletransferlearning from language models has been applied in software en-gineering [
77], it has not been applied to source code due to the
aforementioned vocabulary issues. Finally, the general insightsabout vocabulary design that we study are not specific to NLMs,
butarisewheneverwebuilddevelopmenttoolsbyapplyingNLP
methods to source code.
We conclude the paper in Section 9, and briefly describe the
artifacts used in this work and how to obtain them in Section 10.
2 BACKGROUND AND RELATED WORK
We first note that this work is a consolidation of two unpublished
worksoriginallyconductedindependently:oneworkfocusedonthe
impactofvariousvocabularychoicesontheresultingvocabulary
size and the training of NLMs [ 9], while the other work investi-
gated the specific vocabulary choice of Byte-Pair Encoding, andintroduced several improvements to the training procedure [
55].
Thispapercontainsjointworkthatimprovesonbothearlierworks
by investigating additional characteristics of the vocabulary, addi-
tional improvements to NLM training, an additional use case for
NLMs, and a more thorough empirical evaluation.
2.1 Language Modeling in NLP
A language model (LM) estimates the probabilities of sequences
of words based on a training corpus. In NLP, these models have
been applied to tasks such as speech recognition [ 24] and machine
translation [ 51]. Early language models were based on n-grams:
the probability of a token is computed based on the n−1 previous
tokens in the sequence. These had success in NLP applications, buthavetwoissues.First,theyoperateonsmallamountsofpreviouscontext,with
noftenrangingfrom3to6(e.g. n=6forJava[ 41]).
Increasing ndoes not scale well if the vocabulary is large: for a
vocabulary ofsize m, thereare mnpossible n-grams.Second, they
sufferfromdatasparsity:notallpossible n-gramsexistinthecorpus.
Smoothing [19] alleviates—but does not eliminate—the issue.
The current state-of-the-art in NLP is neural language models
(NLM)[12]. NLMs represent words in a continuous vector space,
such that words that are semantically similar are close in vector
space[64],allowingthemodeltoinferrelationshipsbetweenwords,
even if they do not appear in a specific context during training.
This allows these models to better deal with data sparsity, leading
to better performance. Current NLMs are based on architecturessuch as recurrent neural networks (RNN) [
63], long short-term
memory(LSTM)[ 45],orTransformer[ 85]thatmodel longrange
dependencies:astudyofLSTMNLMsshowedthattheyusecontext
as large as 250 words [56], much longer than n-grams.
2.2 Difficulties with Large Vocabularies
ML models in general, and NLMs in particular, do not handle large
vocabularies well. This is for several reasons:
Scalability. Duringpre-processing,eachwordisconvertedtoa
numerical representation, first via one-hot-encoding, producing
(sparse) vectors of length equal to the vocabulary. NLMs then con-
vertthesetowordembeddings,densewordvectorsofmuchsmaller
dimensions(usuallyinthehundreds),intheirfirstlayer.Foravo-cabularyofsize
mandembeddingsofsize n,theembeddinglayer
is a dense matrix of size m×n. A large m(e.g., 100,000 or more)
affects the memory required by the model as well as the amountof computation required for training. The output of an NLM is a
predictionoverthenexttoken,whichisaprobabilitydistribution
overtheentirevocabulary.Thismustbecomputedonceforeach
token in the training corpus many times during training. This can
be prohibitively slow for large vocabularies [16, 52].
Out-of-vocabulary (OOV). In traditional, closed-vocabulary mod-
els, the vocabulary must be known in advance and will be builtbasedonthetrainingcorpus.Anynewwordencounteredattest
time,calledout-of-vocabularywords,willnotbeabletobeone-hot
encoded as the resulting vector would exceed the expected dimen-
sions. A common workaround is to have a specific unknown token,
and replace any word not previously seen by this token. This loses
information, making the NLM unable to predict any new token,
which is particularly problematic for source code.
Rare Words. Deriving meaningful embeddings for rare words
is difficult since there is very little data to work with. Gong et al.
showthatthepropertythatsemanticallysimilarwordshavesimilar
embeddings does not hold for rare words: they hypothesize that
since the words are rarely seen, the embeddings are rarely updated
and thus stay close to their initialized values [ 35]. This issue is
likely to impact performance: a very large vocabulary has been
shown to negatively impact it, particularly with OOV words [51].
2.3 Handling Large Vocabularies in NLP
Anopenvocabularymodel isnotrestrictedtoafixed-sizedvocab-
ulary determined at training time. For instance, a character LM
1074predicts each word letter by letter: its vocabulary is the set of char-
acters; the OOV issue vanishes. However, it needs to model longer
dependenciesthanawordNLM,impactingperformance.Models
usingsubword units,o rsubwords, combine the strengths of char-
acterandtokenLMs.Asubwordunitisasequenceofcharacters
that occurs as a subsequence of some token in the training set; the
modeloutputsasequenceofsubwordunitsinsteadofasequence
of tokens. Many NLP models have used linguistically-motivated
subwords[ 11,24,59,65].Mikolov etal.foundthatsubwordmodels
improved on character models [ 65]. Sennrich et al.adapt the Byte-
PairEncoding(BPE)algorithmtodecomposewordsinsubwords,
improving rare word translation [ 79]. Kimet al.combine a charac-
terCNNwithaNLM[ 57].VaniaandLopezcompareLMs(words,
morphs, character n-grams, BPE) on several languages [83].
Another approach to the OOV problem are cache models and
copymechanisms [5,36,62],whichallowthemodeltore-usewords
thathaveappearedpreviously.ThishelpswiththeOOVproblem,
because such models can copy words that are not in their fixed
vocabulary,butitdoesnothelpthe firsttimeanOOVwordappears.
2.4 Language Modeling and Vocabulary in SE
LanguageModelsinSoftwareEngineering(SE). Seminalstudieshave
laidthegroundworkfortheuseoflanguagemodelsonsourcecode:
Gabel and Su show that software is very repetitive [ 33], which
motivates the use of statistical modelling for code. Hindle et al .
[44]build language models of source code, finding applications in
codecompletion.Nguyenetal.[ 68]augmented n-gramLMswith
semantic information such as the role of a token in the program,
e.g., variable, operator, etc. Tu et al.[81] find that software is even
more repetitive taking local context into account. Rahman et al.
find that while some aspects of software are not as repetitive as
previouslythought(non-syntaxelements),othersareevenmoreso
(APIsequences)[ 74].Othermodelsofsourcecodeincludeproba-
bilistic higher order grammars (PHOG) [14], which use ASTs, and
several types of RNNs, including LSTMs [26, 41, 88].
SE Applications of Language Models. Probabilistic code models
have enabled many applications in software engineering (see Alla-
manisetal .[4]forasurvey).Oneexampleisrecommendersystems
aiming to aid developers in writing or maintaining code: Hindle
etal.usedatoken-levelLMforcodecompletion[ 44],whilelater,
Frankset al.improved on performance with Tu’s cache [ 81] and
builtacodesuggestiontoolforEclipse[ 32].Anotherapplication
arerecommendationsystemsforvariable,method,andclassnames[
2,3,5]thatemployrelevantcodetokensastheLMcontext.Camp-
bellet al.[18] usedn-gram language models to detect syntax error
locationsinJavacode,andlaterusedanNLMforthesamepurpose
[78]. Rayet al.[75] showedthat buggycode has onaverage lower
probability than correct code, and that LMs can spot defects as
effectively as popular tools such as FindBugs.
Several approaches use neural machine translation, in which an
encoderLMispairedtoadecoderLM.ExamplesincluderecoveringnamesfromminifiedJavascriptcode[
10,84],orfromdecompiledC
code [50]. Other applications include program repair [ 20], learning
codechanges[ 82],orgeneratingsourcecodecomments[ 47].Guet
al.[37] generate API usage sequences for a given natural language
query.Theythenlearnjoint semanticrepresentationsofbilingualAPIcallsequencestosupportAPIcallmigration[ 38].Yinetal.[90]
mine pairs of natural language and code from Stack Overflow to
support tasks such as code synthesis from natural language.
Large vocabularies in SE. The majority of models of source code
usedclosedvocabularymodels.HellendoornandDevanburightly
notice that NLMs trained on a software corpus would struggle due
tovocabularysize[ 41],becauseidentifiers,whicharethebulkof
sourcecode,canbearbitrarilycomplex,andareoftencompound
words (e.g., thisIdentifierHas6WordsAnd2Numbers ), causing an
explosionofpossibleidentifiers.ToproduceanNLMthatcanbe
trained in a reasonable amount of time, Hellendoorn and Devanbu
impose drastic restrictions which would be expected to reduce
predictiveaccuracy,restrictingthetrainingsetto1%oftheoriginal
corpus[6]and thevocabulary toonlyinclude wordswhich occur
more than 5 times. Even so, the resulting vocabulary size is stillexceeds 76,000 words. Similarly, Pradel and Sen
[70]had a large
vocabularyof2.4millionuniquetokens:theylimitedittothe10,000
most common tokens to reduce inaccuracies due to rare words.
To limit this issue, previous work has segmented identifiers via
a heuristic called convention splitting, which splits identifiers on
camelcaseandunderscores[ 3].Eventhoughthissegmentationcan
handlesomeOOVtokens,itislimitedtocombinationsofsubtokens
appearing in the training set and thus unable to achieve a truly
openvocabulary.Additionally,manyofthesesubtokensarestillin-frequent,whichhindersthemodel’sabilitytoassignhighscorestotheircompositions.Forexample,despiteusingconventionsplitting,the implementation of code2seq from Alon et al.[
8] only keeps the
190,000 most common vocabulary words.
Several studies have empirically compared different techniques
forautomaticallysplittingidentifiers[ 30,43].Theseworksconsider
thesomewhat differentproblemofsplitting identifiersintowords
in a way that matches human judgment. Subword units may not
necessarily produce words that humans recognize, but they can be
triviallyreassembledintocompletetokensbeforetheyareshown
to a developer. Stemming [ 89] has also been used to reduce the
number of vocabulary words by only keeping their roots; this ishowever destructive. Malik et al.combined convention splitting
and stemming for type prediction [60].
Few SE approaches use caches. Tu et al.[81] and Hellendoorn
andDevanbu[ 41]usen-gramcaches.Li etal.augmentanRNNwith
acopymechanismbasedonpointernetworks[ 86]toimproveOOV
codecompletion[ 58].WhileitcanreuseanOOVwordafterseeing
it, it cannot predict the word’s first use, learn its representation, or
learn its dependencies, unlike our model. Copy mechanisms were
also used for program repair [20], and method naming [5].
3 DATASETS
We use code corpora from threepopular programming languages:
Java, C, and Python. We choose these languages because they have
differencesthatcouldaffecttheperformanceofLMs.Javahasex-
tensively been used in related work [ 6,26,41,44,68,81]. Unlike
Java, C is procedural, and makes it possible to write very terse
code.1Python is a multi-paradigm dynamic language with little
use of static typing. For Java we used the Java Github corpus of
Allamanis et al.[6], also used in [ 41]. The C and Python corpora
1For examples, see https://www.ioccc.org/.
1075Table 1: Corpus statistics for each code corpus.
Java C Python
Tokens Projects Tokens Projects Tokens Projects
Full1.44B 13362 1.68B 4601 1.05B 27535
Small 15.74M 107 37.64M 177 20.55M 307
BPE 64.84M 1000 241.38M 741 124.32M 2867
Valid. 3.83M 36 21.97M 141 14.65M 520
Test 5.33M 38 20.88M 73 14.42M 190
were mined following the procedure described in [ 6]; the C corpus
was mined in [ 29] and the Python corpus was mined in [ 31]. For
lexical analysis we used the Java lexer implemented in [ 41]2; for C
andPythonweusedthePygments3library.Descriptivestatistics
are in Table 1.
For Python and C we sampled 1% of the corpus for validation
and 1% for testing. Another 10% of the corpus was sampled as
a separate data set to learn subword encodings with BPE. The
restofthedatawasusedfortraining.Wealsoreportresultsona
smaller subset of 2% of our full training set. For Java, we used a
slightly different procedure to make our experiment comparable toa previous study [
41]. We divide the data into five subsets as in the
othertwolanguages.Thevalidationandtestsetsarethesameas
in [41], and our “small train” set is the same as their training set.
ToobtainthefullJavatrainset,wecollectallofthefilesintheJava
Github corpus that do not occur in the validation or test set. Of
these, we sampled 1000 random projects for the subword encoding
data set, and the remaining projects were used as the full train set.
Inthevocabularystudy,bothtrainingsetsandtestsetsareused.
TotrainLMs,wepreprocessthecorporatomatch[ 41],replacing
non-ASCIIcharacter sequencessuchas Chineseideogramsinside
strings with a special token ( <non-en> ), removing comments, and
keeping strings. Note that the lexer in [ 41] replaces all strings with
lengthof15charactersormorewiththeemptystring.InPython,
we do not add any special tokens to represent whitespace.
4 MODELING VOCABULARY
We study a series of modeling choices for source code vocabulary.
Thesechoicesmaybeimplicitlymadebyresearchers,withorwith-
outevaluatingalternatives;theymaynotalwaysbedocumented
in their studies. By making the choices explicit, we can study their
impactonthevocabulary.WereportresultsonJava;similarresults
can be observed for C and Python. Our evaluation criteria are:
Scalability Trainingofmodelsshouldscaletothousandsofprojects.
Scalability is influenced by the vocabulary size (number ofunique
words)andthecorpussize(numberoftokens).Wereportbothmet-
rics onour fulljava trainingset, andcompare themto abaseline
with percentages. For instance: 11.6M, 100% and 2.43B, 100%.
Information loss Modelsshouldbeabletorepresenttheoriginal
input as much as possible; out-of-vocabulary (OOV) tokens are
particularly undesirable. We build a vocabulary on the trainingset, and compare it with the test set vocabulary; we report the
percentage of new vocabulary words seen in the test set. As large
2https://github.com/SLP-team/SLP-Core
3http://pygments.org/docs/lexers/vocabulariesdonotscale,wereportOOVfortheunfilteredvocab-
ulary,anda smaller vocabulary (the75,000mostfrequentwords,
as in [41]). To show trends, we also plot OOV for: full vocabulary,
200K, 100K, 75K, 50K, and 25K. Such as: 42%, 79% ,.
Word frequency As rare words have worse representations than
frequent ones [ 35], increasing word frequency is desirable. Differ-
entmodellingchoicescanincreaseordecreasethenumberofrare
words. We report the percentage of the vocabulary that has a fre-
quency of 10 or less, and plot a bar chart showing the percentage
of vocabulary with frequencies of 1000+, 1000–101, 100–11, 10–2,
and 1. For instance: 83%, .
Baseline: 11.6M, 100%r2.43B, 100%r42%, 79%,r83%,
Ourbaselineisavocabularyofunsplittokens,exceptstringsand
commentsthataresplitbywhitespace(notdoingsoroughlydou-
blesthevocabulary).Thisvocabularyisextremelylarge:morethan
11millionuniquewordsonJava-large.TheOOVrateonthetest
set exceeds 40% with the full vocabulary, showing that developers
docreatemanynewidentifiers.Themostcommonwaytoshrink
vocabulary is to replace infrequent tokens with <unk>. Doing so
further worsens OOV issues: after reducing the vocabulary to a
moremanageable75K,closeto80%ofthetestvocabularyisunseen
in the training set. Many words are infrequent: 83% of vocabulary
wordshaveafrequencyof10orless,with25%occurringonlyonce.
4.1 Filtering the vocabulary
Simplestistofiltervocabularyitemsthataredeemedlessimportant.
Filtering is destructive: it thus needs to be thoroughly justified.
English. 11.4M, 98%r2.43B, 100%r35%, 76%,r83%,
Source code can contain many non-English words in identifiers,strings, and comments, either because developers use other lan-guages,orfortestingorinternationalizationpurposes.Handling
multilingual corpora is an NLP research topic in itself; we evaluate
the simplifying assumption to limit a corpus to English. This is not
trivial: dictionary-based heuristics have too many false positives
(e.g. acronyms). We use a simple heuristic: a word is non-English if
it contains non-ASCII characters. This is imperfect; “café”, “naïve”,
or “Heuristiken” are misclassified. Non-English words are replaced
witha <non-en> placeholder.Eventhen,thevocabularyshrinksby
only 2%, while OOV drops by only 3% at 75K.
Whitespace. 11.4M,98%r1.89B,78%r35%,76%,r83%,
Some applications (e.g., pretty-printers [3]) may care about the
layoutofsourcecode.Othersmaynot,givingimportanceonlyto
syntactic orsemantic aspects (unless code layout issyntactically
important,suchasinPython).Filteringoutwhitespacereducesthe
vocabulary only by a handful of tokens, but reduces corpus size by
22% (1.89B tokens).
Comments 10.8M, 93% r1.26B, 52% r38%,78%, r83%,
Comments often contain natural language, which is much lessrepetitive than code. While tasks such as detecting self-admittedtechnical debt [
25] rely on comments, others do not. Replacing
comments by placeholder tokens (e.g., <comment> ) significantly
reducescorpussize (afurther26%),but itseffectonvocabularyis
limited (6%, given that comments are already split on whitespace).
Strings.9.5M, 82%r1.15B, 47%r39%, 78%,r83%,
Similarly, string literals canbe filtered, replacing themby a place-
holder token like <string> . This does not reduce corpus size as
1076much (a further 5%), but shrinks vocabulary a further 11%, close
to 9.5 million words. This is still extremely large. We also evaluate
the configuration used in [ 41]: strings are kept, unsplit, but strings
longer than 15 characters are replaced by the empty string. For
consistencywithpreviouswork,weuseitas newbaseline .Itin-
creases vocabulary, OOV and infrequent tokens rate: 10.9M, 94% r
1.15B, 47%r39%, 80%,r84%,
Fulltokenvocabulariesrangeinthemillions,andhencedonot
scale. OOV and frequency issues are extremely important.
4.2 Word Splitting
Identifiers are the bulk of source code and its vocabulary. While
new identifiers can be created at will, developers tend to follow
conventions.Whenanidentifierismadeofseveralwords,inmost
conventions,thewordsarevisuallyseparatedtoeasereading,either
incamelCase or in snake_case [15]. Thus, an effective way to
reducevocabularyisto splitcompoundwordsaccordingtothese
word delimiters, as was done by Allamanis et al.[3].
The decision whether to split compound words or not has im-
portant ramifications. First, it introduces additional complexity:the LM can no longer rely on the assumption that source code isa sequence of tokens. Instead, compound words are predicted asa sequence of subtokens, albeit in a smaller vocabulary. Second,
subtokensincreasethelengthofthesequences,makingitharder
torelatethecurrentsubtokenstothepastcontext,asitincreases
insize.This makestheapproachunviablefor n-gramsas nwould
need to increase significantly to compensate.
Splitting tokens has advantages: most obviously, the vocabulary
canbemuchsmaller.Consequently,theOOVrateisreduced.Third,
a model may infer relationships between subtokens, even if thecomposedwordisrare,asthesubtokensaremorecommonthanthe composed word. Finally, using subtokens allows a model to
suggestneologisms, tokens unseen in the training data [3].
Splitting. 1.27M, 12%r1.81B, 157%r8%, 20%,r81%,
Wordsplittingviaconventionsdrasticallyreducesthevocabulary,
byaclosetoanorderofmagnitude(slightlymorethanamillion
words), at the cost of increasing corpus size by 57%. The impact
on the OOV rate is also very large, as it decreases by a factor of5 (in the unfiltered case; for a vocabulary of 75K it is a factor of
4). However, the effect on word frequency is limited, with only 3%
more words that are more frequent than 10 occurrences.
Case.1.09M, 10%r2.16B, 187% r9%, 21%, r83%,
Most commonly, words in different case (e.g. value,Value,VALUE)
will be distinct words for the LM. This could increase the vocab-
ulary, but removing case loses information. A possible solutionis toencode caseinformation in separatortokens (e.g.,
Valuebe-
comes <Upper> value ;VALUEbecomes <UPPER> value ).atthecost
of increasing sequence length. Case-insensitivity does decrease
thevocabulary,butnotbymuch(afurther2%),whilecorpussize
increases significantly (a further 30%). Thus, our following configu-
rations do not adopt it: our new baseline keeps case.
Word splitting is effective, but the vocabulary is still large (a
million words). OOV and frequency issues are still important.4.3 Subword splitting
As splitting on conventions is not enough, we explore further.
Numbers. 795K, 63%r1.85B, 102%r6%, 18%,r72%,
Numeric literals are responsible for a large proportion of the vo-
cabulary, yet theirvocabulary is very limited. Thus, an alternative
tofilteringthemoutistomodelthemasasequenceofdigitsand
characters. This yields a considerable decrease in vocabulary with
ourpreviousbaseline(37%),foronly2%increaseincorpussize.For
OOV,thereisaslightimprovementfora75Kvocabulary(2%),as
well as for frequency (28% of words occur 10 times or more).
Spiral.476K, 37%r1.89B, 104% r3%, 9%, r70%,
Several approaches exist that split a token into subtokens, butgo beyond conventions by using Mining Software Repositories
techniques, such as Samurai [ 30], LINSEN [ 23], Spiral [48], or even
neural approaches [ 61]. We applied the Spiral token splitter, which
isthestateoftheart.Weobservedafurther26%reductionofthe
vocabulary,fora2%increaseincorpussizecomparedtonumber
splitting.SpiralwasalsoveryeffectiveintermsofOOV,with9%of
unseen word when the vocabulary is limited to 75K, and 3% when
unfiltered(476Kwords).Theimpactonfrequencywaslimited.Even
if this is encouraging, the OOV rate is still high.
Other approaches. Stemming [ 89] can reduce vocabulary size,
but loses information: it is not always obvious how to recover the
original word from its stem. We found that applying stemmingcan further reduce vocabulary by 5%, which does not appear tobe a worthwhile tradeoff given the loss of information. Anotheroption is character models that achieve an open vocabulary by
predicting the source file one character a time. OOV issues vanish,
but unfortunately, this drastically inflates sequence lengths, so a
character model is not desirable.
While these strategies are effective, they do not go far enough;
vocabulary stays in the hundreds of thousands range. There are
still OOV issues for unseen data; most words are uncommon.
4.4 Subword splitting with BPE
ThefinalalternativeweevaluateissubwordsegmentationviaByte-
PairEncoding(BPE).BPEisanalgorithmoriginallydesignedfor
data compression, in which bytes that are not used in the data
replace the most frequently occurring byte pairs or sequences [ 34].
In subword segmentation, this corpus is represented as a sequence
of subwords. Special end-of-token </t>symbols are added to al-
low us to convert from a sequence of subword units back into a
sequenceoftokenswithease.Theapproachwasadaptedtobuild
NMTvocabularies[ 79]:themostfrequentlyoccurringsequences
of characters are merged to form new vocabulary words.
BPE builds up the vocabulary of subwords iteratively, at each
iterationatrainingcorpusissegmentedaccordingtothecurrent
vocabulary. The initial vocabulary contains all characters in the
data set and </t>, and the corpus is split into characters and </t>.
Then,allsymbolpairsappearinginthevocabularyarecounted.All
theappearancesofthemostfrequentpair (S1,S2)arereplacedwith
auniquenewsinglesymbol S1S2,whichisaddedtothevocabulary,
without removing S1orS2(which may still appear alone). This
procedure is called a merge operation. The algorithm stops after
agivenmaximumnumber nofmergeoperations;thisistheonly
1077Java Code:
public AttributeContext(M ethodsetter , Object value) {
this.value = value;
this.setter = setter;
}
Subword Units:
public</t> Attribute Con text</t> (</t> Method</t> set ter</t> ,</t> Object</t>
value</t> )</t> {</t> this</t> .</t> value</t> =</t> value</t> ;</t> this</t>
.</t> set ter</t> =</t> set ter</t> ;</t> }</t>
Figure 1: Example of Java code as a list of subword units.
parameter.Thefinaloutputofthealgorithmis(1)thenewvocab-
ulary, which contains all the initial characters plus the symbols
createdfromthemergeoperations,and(2)theorderedlistofmerge
operations performed in each iteration. New data is segmented by
splitting it into characters and merging in the same order.
BPEhasseveraladvantages.First,nowordisOOV;thevocab-
ularyalwayscontainsallsinglecharacters,sounknownwordsat
test time can be represented using those subwords, if no longer
subwords apply. Second, the vocabulary dynamically adapts tothe frequency of the sequences: common sequences will be rep-resented by a single word (eg,
exception ), while rare ones will
be segmented into more common subword units (such as roots,
prefixesandsuffixes);thishelps withsparsityissues.Finally,BPE
allows for fine-grained control of vocabulary size, by tuning the
numberofmergeoperations.Alargervocabularywillhavemore
complete wordsand less sequences, smallerones will havelonger
sequences. An example of a Java code snippet segmented into sub-
words is shown in Figure 1. We computed BPE for 1K, 2K, 5K, 10K
and 20K merges, on a held-out set of 1K project.
BPE Subwords. 10K, 1%r1.57B, 137% r0%, 0%,r1%,
WeapplyBPE(10Kmerges)toourJavacorpuswithpreprocessed
as in [41], which we use as a baseline for comparison. As expected,
theOOVissuesvanish,evenforan extremelysmallvocabulary.The
corpussizegrows,butnotmorethanpreviouschoicesweexplored.
Since BPE merges based on frequency, the resulting subtokens, no
matter their size, are frequent: more than 97% of the remaining
words occur more than 1,000 times in the corpus, with very few
wordsthatareinthehundreds,and1%lessthanten.Loweramounts
of merges result in a smaller vocabulary, at the cost of a larger
corpus size. Our largest BPE vocabulary, 20K, is 575 times smaller
than our initial baseline; our smallest is 11,500 times smaller.4
Qualitativeexamination. WhilethegoalofBPEisnottoproduce
human-readable tokens, we examine how closely the splits BPE
produces match human ones. We inspected 110 random identifiers,
and provide anecdotal evidence of the types of splits produced by
BPE. Our goal is not to provide strong evidence, but rather to give
a sense to the reader of what BPE splits look like in practice.
WhilesomesubwordsarereadableatBPE1K( FilerOutputrService</t> ), some subwords are not ( DefaultrMrutr
ablerTrererNode</t> ), but look good at 5K ( Defaultr
MutablerTreeNode</t> ). BPE handles rare words gracefully,
producing longer sequences of shorter units as expected. Someexamples include rare words due to typos (
inrculrdedr
4Note that including non-ASCII characters grows the vocabulary by ≈5,000 words in
each case; a solution is to apply BPE at the bytelevel, as done in [71]template</t> ) orforeignwords( vrormrerrkrmedirenraurfrlisrter</t>). Some rare words are split in root and
suffix ( Gridrify</t>), but some acronyms may be unexpectedly
split( IBrAN</t>).Further,BPEcansplitwordscorrectlywithout
case information (http rclient rlib</t>, at 5K).
BPEshrinkssourcecodevocabulary veryeffectively.Moreover,
most of the vocabulary is frequent, improving embeddings.
5 NEURAL LANGUAGE MODEL FOR CODE
We present our NLM for code based on subword units, which is
based on a Recurrent Neural Network (RNN). RNN LMs scan aninput sequence forward one token at a time, predicting a distri-
butionovereachtokengivenall of thepreviousones.RNNswith
gated units can learn when to forget information from the hidden
state and take newer, more important information into account[
45]. Among various gated units, GRUs [ 21] have been shown to
perform comparably to LSTMs [45] in different applications [22].
We intentionally selected a small model as our base model: a
single layer GRU NLM built upon subword units learned from BPE
(Section 4.4). For each vocabulary entry we learn a continuous
representationof512features,whiletheGRUstateisofthesame
size.Inallourexperimentsweusedalearningrateof0.1,dropoutof0.5[
80]andamaximumof50epochsofstochasticgradientdescent
with a minibatch size of 32 (for the small training sets) or 64 (for
thefull trainingsets).These hyper-parametersweretuned onthe
small train and validation sets. After each iteration we measurecross entropy on a validation set (Section 6). If the cross entropy
islargerthanthepreviousepochthenwehalvethelearningrate
and this can happen for a maximum of 4 times, otherwise training
stops.DuringtrainingoftheglobalmodelweunrolltheGRUfor
200timesteps,following[ 56].Ourimplementationisopensource
(GitHubURLomittedforreview).Wealsoexperimentwithlarger
capacity models (2048 hidden features and GRU state).
5.1 Selecting Subword Units with BPE
In our code LM, we address vocabulary issues by having the model
predictsubwords rather than full tokens at each time step. Sub-
words are inferred by BPE (Section 4.4) on a held out dataset of
projects that are separate from the training, validation, and test
sets.Weexperimentedwiththreeencodingsizes,i.e.,themaximum
number of merge operations: 2000, 5000, and 10000. To train the
LM,wefirstsegmentthetrain,validation,andtestsetsusingthe
learned encoding. We transform each token into a character se-
quence, adding </t>after every token. Then we apply in order the
merge operations from BPE to merge the characters into subword
units in the vocabulary.5As in [79] we do not merge pairs that
crosstokenboundaries.Finally,wetrainandtesttheNLMasusual
on the data segmented in subword units.
5.2 Predicting Tokens from Subword Units
Autocompletion algorithms present a ranked list of kpredicted
tokens rather than a single best prediction. With a model based on
subwordunits,itisnotobvioushowtogeneratethetop kpredic-
tions,becauseasingletokencouldbemadefrommanysubword
5We use the BPE implementation from https://github.com/rsennrich/subword-nmt
1078units. We approximate these using a custom variation of the beam
search algorithm. If the beam is large enough the algorithm can
give a good approximation of the top- kcomplete tokens.
TheNLMdefinesaprobability p(s1...sN)foranysubwordunit
sequence. The goal of the beam search is: given a history s1...sN
of subword units that already appear in a source file, predict the
nextmostlikely completetoken.A completetoken isasequenceof
subwordunits w1...wMthatcompriseexactlyonetoken:thatis,
wMendswith </t>andnoneoftheearliersubwordunitsdo.Beam
searchfindsthe khighestprobabilitycompletetokens,wherewe
denoteasingletokenasthesequenceofunits w1...wM,thatmax-
imizethemodel’sprobability p(w1...wM|s1...sN).Importantly,
thelength Mofthenewcompletetokenis notfixedinadvance,but
the goal is to search over complete tokens of different length.
Given a value of kand a beam size b, the algorithm starts by
queryingthemodeltoobtainitspredictionsofpossiblesubword
units, ranked by their probability. The algorithm uses two prior-
ity queues: one called candidates which ranks the sequences of
subword units that still need to be explored during the search, and
one called bestTokens which contains the khighest probability
complete tokens that have been expanded so far. Each candidate is
astructurewithtwofields, textwhichistheconcatenationofall
the subword units in the candidate, and probwhich is the product
of the probabilities of each subword unit in the candidate. Both of
the priority queues are sorted by the probability of the candidate.
In each iteration, the algorithm pops the bbest candidates from
thecandidates queue,expandsthemwithoneadditionalsubword
unit,andscorestheirexpansions.Ifanexpansioncreatesatoken
(thenewsubwordunitendswith </t>)thenitispushedontothe
token queue and the worst token is popped. This maintains the
invariantthat bestTokens hassizek.Ifthenewexpansionisnot
a complete token, then it is pushed onto the candidates queue,
where it can potentially be expanded in the next iteration.
5.3 Caching
We also implement a simple caching mechanism for our NLM to
exploit the locality of source code, particularly previously defined
identifiers. At test time, each time an identifier is encountered,
the5-tokenhistorythatprecededitisaddedtoacachealongside
it. Differently to n-grams, we do not store probabilities, as the
NLMwillcomputethem.Ifthecurrent5-tokenhistoryexistsinthecache,theidentifiersthatfolloweditareretrieved(thisisinpractice
very small, usually 1 or 2 identifiers). These identifiers are then
scoredbytheNLM,andtheirprobabilitiesarenormalizedto1.The
beamsearchdescribedearlieris thenrun,andthetwoprobability
distributionsaremerged,accordingtoacacheweightparameter:
cache_pred×cache_weiдht+beam_pred×(1−cache_weiдht ).The
top 10 of the merged predictions are then returned.
We set the cache weight to 0.3. Note that, like beam search, this
is a test-time only addition that does not affect training.
5.4 Dynamic adaptation to new projects
Aglobal LM , trained in a cross-project setting, will perform better
if it is adapted to a new project [ 44,81]. LMs with n-grams also
employ caches for this. Simply training an NLM from scratch on a
newprojectwillnothaveenoughdatatobeeffective,whiletraininga new model on both the original training set and the new project
would be impractical and computationally expensive.
Instead, we use a simple method of dynamically adapting our
globalNLMstoanewproject.Givenanewproject,westartwith
theglobalNLMandupdatethemodelparametersbytakingasingle
gradient step on each encountered sequence in the project after
testingonit.Thisseriesofupdatesisequivalenttoasingletraining
epoch on the new project. (In our evaluations in Section 6, we will
splituptheprojectfilesinsuchawaythatwearenevertraining
on our test set.) We unroll the GRU for 20 time steps instead of 200
as in our global models, in order to update the parameters morefrequently. We apply only one update for two reasons. First, it isfaster, allowing the model to quickly adapt to new identifiers in
theproject.Second,takingtoomanygradientstepsoverthenew
projectcouldcausetheNLMtogivetoomuchweighttothenew
project, losing information about the large training set.
6 EVALUATION
Intrinsic Evaluation: Language Modeling. A good language
modelassignshighprobabilitiestorealsentencesandlowproba-
bilitiestowrongones.Forcode,fragmentsthataremorelikelyto
occurinhuman-writtencodeshouldbeassignedhigherprobabil-
ity. Precise scoring of code fragments is essential for tasks such as
translating a program from one programming language to another
[54,66], code completion [ 32,76], and code synthesis from natural
language and vice versa [7, 27, 67, 73].
Asinpreviouswork,ourintrinsicmetricisthestandardcross
entropy.Crossentropydefinesascoreoverasequenceoftokens t1,
t2,...,t|C|.Foreachtoken ti,theprobability p(ti|t1, ...,ti−1)ofeach
token is estimated using the model under evaluation. Then the av-
eragepertokenentropyis Hp(C)=−1
|C|/summationtext|C|
i=1logp(ti|t1, ...,ti−1).
Cross entropy is the average number of bits required in every pre-
diction;lowervaluesarebetter.Itnotonlytakesintoaccountthe
correctness of the predictions, but also rewards high confidence.
OurNLMsdefineadistributionoversubwords,nottokens.To
compute cross entropy for subword NLMs, we segment each token
tiintosubwords ti=wi1...wiM.Thenwecomputetheproduct
p(ti|t1, ...,ti−1)=/producttextM
m=1p(wim|t1, ...,ti−1,wi1...wi,m−1),where
the right hand side can be computed by the subword NLM. This
probability allows us to compute the cross entropy Hp(C).
Extrinsic evaluation: Code Completion. Wereporttheper-
formance of our LMs on code completion, which is the task of
predictingeachtokeninatestcorpusgivenallofthepreviousto-
kensinthefile.Wemeasureperformancewithmeanreciprocalrank
(MRR),asiscommonincodecompletionevaluation[ 17,41,76,81].
EachtimetheLMmakesaprediction,wegetarankedlistof k=10
predictions. For each one, the reciprocal rank is the multiplicativeinverse ofthe rankof thefirst correctanswer.MRR isthe average
of reciprocal ranks for a sample of queries Q:
MRR=1
|Q||Q|/summationdisplay
i=11
rank i. (1)
A simplified description of MRR is that it averages top- kpredictive
performance across various k. Note that a correct suggestion at
rank 1 yields an MRR of 1; at rank 2, 0.5; at rank 10, 0.1. Thus, a
1079smalldifferenceinMRRcouldindicatealargechangeintheranked
list, especially for higher MRR values.
CodeCompletionScenarios. Weusethreescenariosfrompre-
vious work [ 41]: Eachstatic,dynamic, and maintenance settings
simulates a different way of incorporating NLMs in an IDE. The
task is always to predict test set tokens, but the training sets differ:
Static tests. The model is trained on a fixed training corpus, and
later evaluated on a separate test dataset. This is a cross-project
setting:train,validation,andtestssetsallcontainseparateprojects.
This simulates a single global LM that is trained on a large corpus
of projects and then deployed to clients without adaption.
Dynamic tests. In addition to the training set, the model can
updateitsparameters afterithasmadepredictionsonfilesinthe
testset(itnevertrainsontestdata).OurNLMsareadaptedusingthe
proceduredescribedinSection5.4.Aftereachproject,werestore
the model to the global LM learned from the train set only. This
simulates a setting in which some files from the project of interest
are available for dynamic adaptation.
Softwaremaintenancetests. Thisscenarioisevenclosertoreal
worldusage,simulatingeverydaydevelopmentwhereprogrammers
make small changes to existing code. The LMs are tested on one
fileatatimeinthetestset.Foreachtestfile F,thetrainsetplusall
otherfilesinthetestprojectexcept Fisusedastrainingdata.As
thisrequiresretrainingtheNLMonceperfileinthetestset,this
scenario was previously deemed infeasible for NLMs in [41].
Identifiersonly. RecentworkobservedthatLMsforcompletion
perform worse on identifiers than other tokens [ 42]. Therefore, we
also report model performance, i.e. entropy and MRR, on identifier
tokensonly(excludingprimitivetypes).Toclarifydifferencesbe-
tweenmethods,wealsoreport recallatrank1(R@1),thepercentage
of all identifier usages which are correctly predicted at rank 1, and
similarlyrecallatrank10(R@10),thepercentagewhenthecorrect
identifier appears anywhere in the model’s top 10 predictions.
7 RESEARCH QUESTIONS
RQ1. How does the performance of subword unit NLMs compare to
state-of-the-artLMsforcode? WecomparesubwordunitNLMsto
standardn-gram LMs [ 44], cache LMs [ 81], state-of-the-art n-gram
LMswithnestedcaching[ 41],token-levelNLMs[ 88],andheuris-
tic splitting NLMs [ 3]. We do not compare with PHOG [ 14] and
pointernetworkRNNs[ 58]:bothdonothaveafullimplementation
available. We do not evaluate character-level NLMs as they have
not shown benefits for NLP.
RQ2.CansubwordunitNLMsscaletolargecodecorpora?Doesthe
additional training data improve performance? Training on a larger
corpus may improve a model’s performance, but adding more data
tends to have diminishing returns. After some point, a model’s
performancesaturates.WeevaluateifNLMscanmakebetteruse
of large corpora than n-gram models. Moreover, training on larger
data uses introduces scaling issues. Thus, performance in terms of
runtime cost, memory usage, and storage becomes important.
RQ3.HowdoestheperformanceofsubwordunitNLMsvaryacross
programming languages? In principle the learning methods for
NLMs are language agnostic; however, the majority of studies eval-
uate only on Java. We check if code LMs are equally effective onotherprogramminglanguages:C’sterseness,orPython’slackof
type information could negatively impact an LM’s performance.
RQ4.IsthedynamicupdatingeffectivetoadaptsubwordunitNLMs
to new projects? New projects introduce many new identifiers that
do not appear even in a large cross-project corpus. An n-gram
LM can exploit the strong locality that characterises code through
caching [44,81]. Thus we ask whether NLMs can also benefit from
dynamic adaptation via the procedure presented in Section 5.4.6
We compare our dynamic adaption technique against two dynamic
n-gram models: cache LMs [81] and nested cache LMs [41].
RQ5. Are NLMs useful beyond code completion? NLMs in NLP
have shown to be useful in a variety of tasks, including translation
orsummarization;theyhavebeenrecentlyshowntobestateofthe
artintransferlearning.Whiletestingallofthesescenariosvastly
exceeds the scope of this paper, we test whether NLMs improve
upon n-gram LMs in the task of detecting buggy code [75].
8 RESULTS
Table 2 presents the evaluation metrics of all scenarios; we refer to
it continuously. We used the n-gram implementation7u s edi n[41]
withthesameparameters(n=6);allNLMsareours.Wecompute
MRR on the first million tokens of the test set, as in [41].
8.1 RQ1. Performance of Models
Because the full data set is so large, we compare the different vari-
antsofn-grammodelsagainsteachotheronthesmallJavatraining
set, and then we compare the best n-gram LM against our BPE
NLMonthelargeJavadataset.InTable2,weseethatthenested
cache model has the best performance of the n-gram models, with
alargeimprovementoverthesimplermodels(forexample,improv-
ing MRR from 58% to 77% on Java against the basic n-gram model).
This is consistent with the results of [ 41]. However, our BPE NLM
outperformsit.(Notethatcachemodelscannotbeevaluatedinthe
static scenario since the cache would adapt to the test set). Moving
tothelargedataset,wefindthattheBPENLMstilloutperforms
thenestedcachemodel,eventhoughthenestedcachemodelwas
specifically designed for code. While previous work [ 42] found
thatclosedNLMsunderperformedonidentifiers,wefindthatour
BPENLMsdonot.Inthedynamicscenario,74%ofidentifiersare
predictedwithinthetop10predictions,withuptonearly56%in
first position.
Openvsclosedvocabulary. Tospecificallyevaluatetheeffectof
relaxing the closed vocabulary assumption, we compare our open
vocabulary NLM to two closed vocabulary NLMs: one that uses
full tokens (Closed NLM), and another that splits tokens accord-
ing to conventions (Heuristic NLM). Those models have otherwise
the same architecture as the open vocabulary. In both cases, we
find that the open-vocabulary NLM significantly outperforms both
closed vocabulary NLMs, and can be trained even in the main-
tenance setting, unlike the closed versions. Of note, our closed
vocabulary NLM performs better than the one in [ 42], as it utilizes
afullyconnectedhiddenlayeranddropout.Finally,inTable3we
reporttheperformanceoftheopenvocabularyNLMswithdifferent
6A naive approach to the software maintenance scenario retrains the model from
scratch for every test file, which was rightly deemed infeasible for NLMs by [41]
7https://github.com/SLP-team/SLP-Core, version 0.1
1080Table 2: Performance of the various models (bold: best, underlined: second best).
MODELJava Java Identifiers C Python
Static Dynamic Maintenance Bugs Dynamic Static Dynamic Static Dynamic
EntMRR Ent MRR Ent MRR % Ent ↓R@1 R@10 MRR Ent MRR Ent MRR Ent MRR Ent MRR
SmallTrain
n-gram 6.25 53.16 5.54 56.21 5.30 58.32 1.81 17.24 34.66 22.26 6.51 55.20 4.14 57.34 5.30 43.63 4.81 47.39
Nested - - 3.65 66.66 2.94 71.43 - 37.46 56.85 43.87 - - 3.61 62.25 - - 4.05 54.02
Cache - - 3.43 69.09 3.32 70.23 - 40.13 59.52 46.57 - - 2.19 75.09 - - 3.22 62.27
Nested Cache - - 2.57 74.55 2.23 77.04 -49.93 70.09 56.81 -- 2.01 76.77 - - 2.89 65.97
Closed NLM 4.3062.28 3.07 71.01 - - 1.81 30.96 49.93 37.20 4.51 60.45 3.20 72.66 3.96 81.733.34 84.02
Heuristic NLM 4.4653.953.34 64.05 - - 1.04 39.54 58.37 45.28 4.82 52.30 3.67 61.43 4.29 65.42 3.56 71.35
BPE NLM (512) 4.77 63.75 2.5477.021.6078.69 3.2645.4967.37 52.66 4.32 62.781.7176.92 3.9181.662.7286.28
BPENLM (512) + cache - - - 77.42 - - - 50.49 68.1656.30 - - - - - - - -
BPE NLM (2048) 4.7764.27 2.08 77.30 - - 3.6048.22 69.79 55.37 4.22 64.50 1.59 78.27 3.66 81.712.6986.67
BPE NLM (2048) + cache -- - 78.29 --- 52.44 70.12 58.30 -- ---- --
LargeTrain
Nested Cache - - 2.49 75.02 2.17 77.38 -52.20 72.37 59.09 - - 1.67 84.33 -- 1.4571.22
BPE NLM (512) 3.1570.841.7279.941.0481.16 4.9251.4174.1359.03 3.1170.941.5677.59 3.0484.312.1487.06
BPENLM (512) + cache - - - 80.29 - - - 55.68 74.3061.94 - - - - - - - -
BPE NLM (2048) 2.40 75.81 1.23 82.41 - -5.9857.5472.1862.912.3880.17 1.36 83.242.0986.171.9087.59
BPENLM (2048) + cache -- - 83.27 --- 60.7473.7665.49 -- ---- --
Table3:EffectofvocabularysizeonJavaperformanceofour
open-vocabulary models (Python and C are similar).
Vocab SizeStatic Dynamic Maint. Bugs
Ent MRR Ent MRR Ent MRR % Ent ↓
Small Train
2000 4.90 62.87 2.33 75.66 1.46 77.48 3.07
5000 4.78 63.80 2.27 77.14 1.51 78.49 3.38
10000 4.77 63.75 2.54 77.02 1.60 78.69 3.26
Large Train
2000 3.59 68.87 1.84 77.69 1.03 78.85 4.095000 3.35 69.87 1.72 79.18 1.06 80.31 4.71
10000 3.15 70.84 1.72 79.94 1.04 81.16 4.92
vocabularysizes,obtainedafter2000,5000,and10000BPEmerge
operations. We see that performance on the small training set is
similar across vocabulary sizes: a large vocabulary is not required
for good performance.
Caches,andlargercapacity. Bothourcacheandincreasingmodel
capacity (from 512 to 2048 features) are beneficial, particularly for
the identifiers. The cache improves MRR by 3 to 4%, with moreimprovements for low ranks, which is especially important for
completion.Onthesmallcorpus,thelargemodelimprovesMRR
bynearly3%,asmallerimprovementthanaddingthecache.Both
improvements are complementary, increasing identifier MRR by
close to 6%.
OpenvocabularyNLMsareeffectivemodelsofsourcecode,even
on a small corpus, yielding state of the art performance.8.2 RQ2. Large Corpora
We contrast performance between small and large training sets.
Leveraging data. When trained on larger corpora, the perfor-
mance of n-gram models (including nested cache variants) gets
saturatedandtheyareunabletoeffectivelyleveragetheextrainfor-mation[
41].Incontrast,ourmodelcanbetterleveragetheincrease
intrainingdatawhentrainedonthefullcorpus.Inthestaticsce-
nario, our NLMs decrease entropy by about 1.5 bits, while MRR
increases by about 6%. More data helps our NLMs learn to synthe-
size identifiers from subwords better and with higher confidence.
TheimprovementsaresmallerbutstillexistwhentheNLMsuse
dynamic adaptation: for all encoding sizes the entropy improvesby 0.5 bits and MRR by 2 to 3%. In contrast, the nested cache
n-
gram model entropy improves by less than 0.1 bits and MRR by
lessthan0.4%.FromthatweconcludethatsubwordunitNLMscan
utilizealargecodecorpusbetterthan n-grammodels.Asshown
in Table 3, larger training corpora tend to favor NLMs with larger
vocabularies, particularly in terms of MRR; larger models leverage
theadditionaldataevenbetter.Forallmodels,theimprovements
are more visible for identifiers: the large train alone contributes
closeto7%ofMRRforidentifiers,versus3%overallfortheNLM.
Finally,largerNLMs(2048features)areevenbetteratleveragingthe
additionaltrainingdata, due totheirincreasedcapacity. Similarly,
thecachestillimprovesperformancefurther,evenwiththelarge
training set; both improvements complement each other.
Resource usage. While the nested cache n-gram model is compet-
itive with Java identifiers, this comes at a significant cost: resource
usage. Disk usage for n-gram models range from 150 to 500 Mb in
the small training set to 6to 8.5GB in the large training set. RAM
usageisevenmoreproblematic,asitrangesfromaround5GBin
the small training set, up to 50 to 60GB in the large training set.
1081This makes the large n-gram models unusable in practice as they
exceed the memory requirements of most machines.
Incontrast,theNLMsdonotvarysignificantlywithtrainingset
size;theirsizeisfixed.Theyrangefrom15MB(BPE2K)to45MB
(BPE 10K) on disk (up to 240MB for the large capacity models).
RAMusageforNLMsvarybetween2to4GBwhentraining(and
canbereducedattheexpense ofspeedbyre ducingbatchsize),and
is considerably lower at inference time (for actual code comple-
tion),rangingfrom250to400MB. Thus,ifwecomparepractically
applicablemodels,thesmallNLMoutperformsthesmallnestedcache
n-grammodelbyupto5.13%inidentifierMRR,andupto5.75%recall
at 1; the large NLM does so by 8.68% (MRR), and 10.81% (recall at 1).
The open vocabulary makes training NLMs on large corpora
scalable as vocabulary ceases to grow with corpus size; training
time scales linearly with added data. Our largest NLM (BPE 10k,2048 features), can process around 350 to 550 hundred thousandtokens per minute (roughly 100 to 300 projects per hour depend-ing on project size) on a consumer-grade GPU. This makes ourdynamic adaptation procedure, which trains one project for oneepoch, clearly feasible. Training the initial model is still a large
upfront cost, but it takes from a day (small NLM) up to two weeks
(large NLM) on our largest dataset, and needs to be performed
once. At inference time, predicting 10 tokens with beam search
takesafractionofasecond,fastenoughforactualuseinanIDE,
evenwithoutadditionaloptimization.Thisisnottruefortheclosed
models.
Open-vocabularyNLMscanscale;furthermore,theyleveragethe
increasedtrainingdataeffectively.Large n-grammodelsdo not
scale in terms of resources.
8.3 RQ3. Multiple Languages
We contrast Java performance with Python and C. We see interest-
ing differences between Java, Python, and C. First, n-gram models
performconsiderablyworseinPython,whileNLMsdoverywell.
WehypothesizethatthisisduetothesmallersizeofPythonprojects
in our corpus, which reduces opportunity for caching (the average
Pythonprojectis2to3timessmallerthantheaverageJavaproject).
Cprojects,ontheotherhand,arecompetitivewithJavaprojects,
particularly with caching; they are on average 2 times larger. In-terestingly, the nested and nested cache n-gram models performcomparativelyworseinCthaninJava:Cprojectstendtohaveaflatter structure, rendering the nesting assumption less effectivein this case. Finally, the (not applicable in practice) large n-grammodel outperforms our NLMs for C. We observed anectodal evi-
dence that there is considerable duplication in the C corpus, which
may affect this result [ 1]. For NLMs, the performance is more even
across the board, with overall slightly worse performance for C,
and somewhat better performance for Python.
Our NLM performance results hold for Java, C, and Python.
8.4 RQ4. Dynamic Adaptation
We evaluate the effectiveness of our proposed method for adaption
ofNLMsinthedynamicandmaintenancescenarios.ThisiscrucialforpracticalusageofNLMs,becausethedynamicandmaintenancescenariossimulate thesettingwhere thedeveloper ismodifyinga
large,existingproject.Usingwithin-projectdataprovidesalarge
performanceboost:Eventhoughwithineachscenario,ourNLMs
outperform n-grams, most n-gram models in the dynamic scenario
outperformNLMsinthestaticscenario.Theimprovementdueto
dynamic adaptation is greater than the improvement due to an
NLM.Ofnote,thesituationinthelargetrainingsetisdifferent:the
staticlargeNLMtrainedonthelargetrainingset outperformsthe
cachen-gramLMsinthedynamicscenario,andiscompetitivewith
it in the maintenance scenario, in other words, our large data set is
so large that it almostmakes up for not having within-project data,
but within-project information is clearly still crucial.
Once we apply the dynamic adaptation method to the NLMs,
the picture changes. With dynamic adaptation, our model achieves
better cross-entropy than the current state-of-the-art [ 41], making
it an effective technique to fine-tune an NLM on a specific project.
Using this method, it is even possible to evaluate NLMs on the
maintenance scenario, which was previously deemed infeasible by
[41]sincemultiplemodelshadtobecreated,eachtrainedonthe
entiretyofthetestsetminusonefile.Thisispossibleforusbecause
the combination of a small vocabulary size and our finetuning
methodrunningforonlyoneepochmakethisscenariomuchfaster.
OpenvsclosedNLMs. Interestingly,thedifferenceinperformance
between the open and closed vocabulary NLMs is larger in thedynamic setting. We hypothesize that dynamic adaptation helps
the open-vocabulary model to learn project-specific patterns about
OOV words; this is not possible for a closed vocabulary NLM.
Dynamic adaptation for NLMsyields the state of the art; static
NLMsarecompetitivewithsomedynamic n-grammodels,which
bodes well for transfer learning.
8.5 RQ5. Bug Detection
Previous work hasobserved that n-gram language modelscan de-
tect defects as they are less “natural” than correct code [ 75]. In
short,defectivelinesofcodehaveahighercross-entropythantheircorrectcounterparts.ToassesswhetherourcodeNLMisapplicable
beyond code completion, we compare the ability of different lan-
guage models to differentiate between the two on the well-known
Defects4j dataset [53]. Defects4J contains 357 real-world defects
from5systems.Bothabuggyandacorrectedversionofthesystem
areprovidedandthechangedlinescanbeextracted.Wecompute
the difference in entropy between the buggy and the fixed version
foreachofthediffpatchesprovided.Theextractedcodesnippets
usuallycontainsafewunchangedsurroundinglinesthatprovide
useful context for the LMs. We expect a better LM to have a larger
entropydifferencebetweenthedefectiveandthecorrectedversion.
We compute these metrics only for LMs in a static setting for
three reasons: 1)we simulated the setting inwhich a bug detector
istrainedononesetofprojectsandusedonunseenones,2)itisnot clear how caches would be used in this scenario (should the
LM“know”whichfileabugisin?),and3)doingsocouldinvolve
training two LMs for each defect, which is very expensive.
The results are shown in the Java "bugs" column in Tables 2
and 3. As we hypothesized, open vocabulary NLMs feature a larger
entropy drop for clean files than n-gram LMs or closed NLMs. The
1082dropinentropyis70%to100%forthesmalltrainingset,depending
onvocabularysizeandmodelcapacity(largerisbetter).Further-
more,thesemodelsbenefitfromalargetrainingset,withalarger
drop of 127 to 173%. We hypothesize that beyond data sparsity for
identifiers,theNLM’slongrangedependenciesareespeciallyuseful
in this task.
Open-vocabularyNLMarebetterbugdetectorsthan n-gramLMs,
particularly when trained on large corpora.
9 CONCLUSIONS
Sourcecodehasacriticaldifferencewithnaturallanguage:develop-
ers can arbitrarily create new words, greatly increasing vocabulary.
Thisisagreatobstaclefor closed-vocabulary NLMs,whichdonot
scale to large source code corpora. We first extensively studied vo-
cabularymodellingchoices,andshowedthattheonlyviableoption
isanopen-vocabulary NLM;allothervocabularychoicesresultin
large vocabularies, high OOV rates, and rare words.
Wethenpresentedanewopen-vocabularyNLMforsourcecode.
Bydefiningthemodelonsubwordunits,whicharecharactersubse-quences of tokens, the model is able to handle identifiers unseen intrainingwhileshrinkingvocabularyby threeordersofmagnitude.As
aconsequence,ourNLMcanscaletoverylargecorpora:wetrained
itondatasetsovera hundredtimeslarger thanhadbeenusedfor
previous code NLMs. Our NLM also uses beam search, dynamic
adaptation, and cachingto efficiently generate tokens and adapt to
newprojects.Finally,weshowedthatourNLMoutperformsrecent
state-of-the-artmodelsbasedonaddingnestedcachesto n-gram
language models for code completion and bug detection tasks, in a
variety of scenarios, and in three programming languages.
Of course, this study has limitations: While we tried to be ex-
haustive andevaluated a largenumber of scenarios,we could not
evaluate all the possible combinations (hundreds) due to the re-
sources needed, such as some large models or some large training
scenarios. For this reason, we also refrained to evaluate other NLM
architectures such as LSTMs [ 45], QRNNs [ 16], Transformers [ 85],
oradditionalneuralcachevariants[ 62,86].Forthesamereason,as
in [41]we alsolimited MRRto 1million tokens,which maycause
discrepancies with entropy metrics as they are not evaluated on
thesametestset.Wealsolimitedourselvestothreelanguages,and
did not fully evaluate the impact of code duplication [1].
Wealsohopethatthesimplicityandscalabilitywillenablelarge
capacitymodelsforcode,andthetransferlearningopportunities
they bring [ 28,72]; this has been explored in software engineer-
ing, albeit not for source code [ 77]. Improved language models for
code havethe potential toenable new toolsfor aiding coderead-
ability[2],programrepair[ 13,18,40,75],programsynthesis[ 39]
and translation between programming languages [ 54,66]. Finally,
the technique of using subword units is not limited to language
modeling, but can easily be incorporated into any neural model of
code,suchasmodelstosuggestreadablenames[ 3],summarizing
sourcecode[ 5,49],predictingbugs[ 70],detectingcodeclones[ 87],
comment generation [47], and variable de-obfuscation [10].Table 4: DOIs of artifacts used or produced by this work
Artifact DOI
Java corpus https://doi.org/10.7488/ds/1690
C corpus https://doi.org/10.5281/zenodo.3628775
Python corpus https://doi.org/10.5281/zenodo.3628784
Java, pre-processed https://doi.org/10.5281/zenodo.3628665
C, pre-processed https://doi.org/10.5281/zenodo.3628638Python, pre-processed https://doi.org/10.5281/zenodo.3628636
codeprep https://doi.org/10.5281/zenodo.3627130
OpenVocabCodeNLM https://doi.org/10.5281/zenodo.3629271
Trained models https://doi.org/10.5281/zenodo.3628628
10 ARTIFACTS
Several artifactswere used to conduct this study: data, source code,
andmodels.Toimprovereplicationofthiswork,thespecificversionofeachartifactusedinthisstudycanbereferencedviaaDOI.Table
4 lists the DOI of each artifact. This paper can be referenced when
any of these artifacts is used.
Datasets. The datasets described in 3 were published in previous
work:TheJavacorpuswasproducedbyAllamanis etal.[6],andalso
usedin[41].TheCcorpuswasminedin[ 29]andthePythoncorpus
wasminedin[ 31].Weusetherawdatasetsforthevocabularystudy,
butpreprocessthemforNLMtraining.Further,wedefinedtraining
andtestsetsfortheCandPythoncorpora,anddefinedthelarge
training set for the Java corpus.
Source code. We implemented the codepreplibrary that supports
a variety of pre-processing options for source code. We used code-
preptogather thevocabularystatistics presentedinsection 4.Re-
searchers that wish to use the library to pre-process source codefor their own study can find the library at: https://github.com/
giganticode/codeprep.
The open vocabulary language model described in 5, as well
as the scripts implementing the training procedure and the eval-
uationscenariosareavailableinthe OpenVocabCodeNLM library.
Researchers wishing to extend our model can find it on GitHub at:
https://github.com/mast-group/OpenVocabCodeNLM.
Models.Themodelsthatweretrainedandevaluatedinsection
8arealsomadeavailableforfurtheruse.Eachmodelwastrained
onGPUsforperiodsrangingfromafewhours,uptotwoweeks.
These models can be used as-is for inference in a code completion
scenario.Alternatively,theymaybefine-tunedforothertasks,such
as classification [46, 77].
11 ACKNOWLEDGEMENTS
This work was supported in part by the EPSRC Centre for Doc-
toral Training in Data Science, funded by the UK Engineering and
PhysicalSciencesResearchCouncil(grantEP/L016427/1)andthe
University of Edinburgh. This work was partially funded by theIDEALSandADVERBprojects,fundedbytheFreeUniversityof
Bozen-Bolzano. Parts of the results of this work were computed on
the Vienna Scientific Cluster (VSC).
1083REFERENCES
[1]Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine
learning models of code. In Proceedings of Onward! 2019. 143–153. https://doi.
org/10.1145/3359591.3359735
[2]Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles A. Sutton. 2014.
Learningnaturalcodingconventions.In ProceedingsofSIGSOFT/FSE2014.281–
293. https://doi.org/10.1145/2635868.2635883
[3]Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles A. Sutton. 2015.
Suggestingaccuratemethodandclassnames.In ProceedingsofESEC/FSE2015.
38–49. https://doi.org/10.1145/2786805.2786849
[4]Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles A. Sutton.
2018. ASurveyofMachineLearningforBigCodeandNaturalness. ACMComput.
Surv.51, 4 (2018), 81:1–81:37. https://doi.org/10.1145/3212695
[5]Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. 2016. A Convolutional
AttentionNetworkforExtremeSummarizationofSourceCode.In Proceedings
of ICML 2016, Vol. 48. 2091–2100. http://proceedings.mlr.press/v48/allamanis16.
html
[6]MiltiadisAllamanisandCharlesA.Sutton.2013. Miningsourcecoderepositories
atmassivescaleusinglanguagemodeling.In ProceedingsofMSR2013.207–216.
https://doi.org/10.1109/MSR.2013.6624029
[7]MiltiadisAllamanis,DanielTarlow,AndrewD.Gordon,andYiWei.2015.Bimodal
ModellingofSourceCodeandNaturalLanguage.In ProceedingsofICML2015,
Vol. 37. 2123–2132. http://proceedings.mlr.press/v37/allamanis15.html
[8]UriAlon,ShakedBrody,OmerLevy,andEranYahav.2019. code2seq:Generating
Sequences from Structured Representations of Code. In Proceedings of ICLR 2019.
https://openreview.net/forum?id=H1gKYo09tX
[9]Hlib Babii,Andrea Janes,and RomainRobbes.2019. ModelingVocabularyfor
BigCodeMachineLearning. CoRRabs/1904.01873(2019). http://arxiv.org/abs/
1904.01873
[10]Rohan Bavishi, Michael Pradel, and Koushik Sen. 2018. Context2Name: A Deep
Learning-Based Approach to Infer Natural Variable Names from Usage Contexts.
CoRRabs/1809.05193 (2018). arXiv:1809.05193
[11]Issam Bazzi. 2002. Modelling Out-of-vocabulary Words for Robust Speec h Recogni-
tion. Ph.D. Dissertation. Cambridge, MA, USA. AAI0804528.
[12]Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A Neural Probabilistic Language Model. J. Mach. Learn. Res. 3 (March 2003),
1137–1155. http://dl.acm.org/citation.cfm?id=944919.944966
[13]Sahil Bhatia and Rishabh Singh. 2016. Automated Correction for Syntax Er-rors in Programming Assignments using Recurrent Neural Networks. CoRR
abs/1603.06129 (2016). http://arxiv.org/abs/1603.06129
[14]Pavol Bielik, Veselin Raychev, and Martin T. Vechev. 2016. PHOG: Probabilis-tic Model for Code. In Proceedings of ICML 2016, Vol. 48. 2933–2942. http:
//proceedings.mlr.press/v48/bielik16.html
[15]David W. Binkley, Marcia Davis, Dawn J. Lawrie, and Christopher Morrell. 2009.
To CamelCase or Under_score. In Proceedings of ICPC 2009. 158–167. https:
//doi.org/10.1109/ICPC.2009.5090039
[16]JamesBradbury,StephenMerity,CaimingXiong,andRichardSocher.2017.Quasi-RecurrentNeuralNetworks.In ProceedingsofICLR2017. https://openreview.net/
forum?id=H1zJ-v5xl
[17]MarcelBruch,MartinMonperrus,andMiraMezini.2009.Learningfromexamples
toimprovecodecompletionsystems.In ProceedingsofESEC/FSE2009.213–222.
https://doi.org/10.1145/1595696.1595728
[18]Joshua Charles Campbell, Abram Hindle, and José Nelson Amaral. 2014. Syntax
ErrorsJustAren’tNatural:ImprovingErrorReportingwithLanguageModels.In
Proceedings of MSR 2014. 252–261. https://doi.org/10.1145/2597073.2597102
[19]Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothingtechniques for language modeling. Computer Speech & Language 13, 4 (1999),
359–394.
[20]Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, DenysPoshyvanyk, and Martin Monperrus. 2018. Sequencer: Sequence-to-sequence
learning for end-to-end program repair. arXiv preprint arXiv:1901.01808 (2018).
[21]Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
RepresentationsusingRNNEncoder-DecoderforStatisticalMachineTranslation.
InProceedings of EMNLP 2014. 1724–1734. https://doi.org/10.3115/v1/d14-1179
[22]JunyoungChung,ÇaglarGülçehre,KyungHyunCho,andYoshuaBengio.2014.
EmpiricalEvaluationofGatedRecurrentNeuralNetworksonSequenceModeling.
CoRRabs/1412.3555 (2014). http://arxiv.org/abs/1412.3555
[23]AnnaCorazza,SergioDiMartino,andValerioMaggio.2012. LINSEN:Anefficient
approach to split identifiers and expand abbreviations. In Proceedings of ICSM
2012. 233–242. https://doi.org/10.1109/ICSM.2012.6405277
[24]Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkö-
nen,VesaSiivola,MattiVarjokallio,EbruArisoy,MuratSaraçlar,andAndreasStol-
cke.2007. Morph-basedspeechrecognition andmodelingofout-of-vocabulary
wordsacrosslanguages. ACMTransactionson Speechand LanguageProcessing
(TSLP)5, 1 (2007), 3.[25]Everton da Silva Maldonado, Emad Shihab, and Nikolaos Tsantalis. 2017. Using
Natural Language Processingto Automatically Detect Self-Admitted Technical
Debt.IEEETransactionsonSoftwareEngineering 43,11(Nov2017),1044–1062.
https://doi.org/10.1109/TSE.2017.2654244
[26]Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016. A deep language model
for software code. arXiv preprint arXiv:1608.02715 (2016).
[27]AdityaDesai,SumitGulwani,VineetHingorani,NidhiJain,AmeyKarkare,Mark
Marron, Sailesh R, and Subhajit Roy. 2016. Program synthesis using natural
language. In Proceedings of ICSE 2016. 345–356. https://doi.org/10.1145/2884781.
2884786
[28]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019. BERT:
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.
InProceedings of NAACL-HLT 2019. 4171–4186. https://doi.org/10.18653/v1/
n19-1423
[29]Sergey Dudoladov. 2013. Statistical NLP for computer program source code: An
informationtheoreticperspectiveonprogramminglanguageverbosity. Master’s
thesis. School of Informatics, University of Edinburgh, United Kingdom.
[30]Eric Enslen, Emily Hill, Lori L. Pollock, and K. Vijay-Shanker. 2009. Mining
sourcecodetoautomaticallysplitidentifiersforsoftwareanalysis.In Proceedings
of MSR 2009. 71–80. https://doi.org/10.1109/MSR.2009.5069482
[31]Stefan Fiott. 2015. An Investigation of Statistical Language Modelling of Different
Programming Language Types Using Large Corpora. Master’s thesis. School of
Informatics, University of Edinburgh, United Kingdom.
[32]Christine Franks, Zhaopeng Tu, Premkumar T. Devanbu, and Vincent Hellen-
doorn. 2015. CACHECA: A Cache Language Model Based Code Suggestion Tool.
InProceedings of ICSE 2015 (Volume 2). 705–708. https://ieeexplore.ieee.org/
document/7203048
[33]MarkGabelandZhendongSu.2010. Astudyoftheuniquenessofsourcecode.
InProceedingsofSIGSOFT/FSE2010.147–156. https://doi.org/10.1145/1882291.
1882315
[34]Philip Gage. 1994. A New Algorithm for Data Compression. C Users J. 12, 2 (Feb.
1994), 23–38. http://dl.acm.org/citation.cfm?id=177910.177914
[35]ChengYue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-YanLiu. 2018. FRAGE: Frequency-Agnostic Word Representation. In Pro-
ceedings of NeurIPS 2018. 1341–1352. http://papers.nips.cc/paper/
7408-frage-frequency-agnostic-word-representation
[36]Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving Neural
LanguageModels witha ContinuousCache.In Proceedingsof ICLR2017. https:
//openreview.net/forum?id=B184E5qee
[37]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
APIlearning.In ProceedingsofSIGSOFT/FSE2016.631–642. https://doi.org/10.
1145/2950290.2950334
[38]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2017.DeepAM:
Migrate APIs with Multi-modal Sequence to Sequence Learning. In Proceedings
of IJCAI 2017. 3675–3681. https://doi.org/10.24963/ijcai.2017/514
[39]SumitGulwani,OleksandrPolozov,RishabhSingh,etal .2017. Programsynthesis.
Foundations and Trends® in Programming Languages 4, 1-2 (2017), 1–119.
[40]Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade. 2017. DeepFix:
FixingCommonCLanguageErrorsbyDeepLearning.In ProceedingsofAAAI
2017.1345–1351. http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14603
[41]VincentJ.HellendoornandPremkumarDevanbu.2017. AreDeepNeuralNet-
workstheBestChoiceforModelingSourceCode?.In ProceedingsofESEC/FSE
2017. 763–773. https://doi.org/10.1145/3106237.3106290
[42]Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli.
2019. When code completion fails: a case studyon real-worldcompletions.In
Proceedings of ICSE 2019. 960–970. https://doi.org/10.1109/ICSE.2019.00101
[43]Emily Hill, David Binkley, Dawn Lawrie, Lori Pollock, and K Vijay-Shanker.
2014. Anempiricalstudyofidentifiersplittingtechniques. EmpiricalSoftware
Engineering 19, 6 (2014), 1754–1780.
[44]AbramHindle,EarlT.Barr,ZhendongSu,MarkGabel,andPremkumarDevanbu.
2012. On the Naturalness of Software. In Proceedings of ICSE 2012. 837–847.
http://dl.acm.org/citation.cfm?id=2337223.2337322
[45]Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
NeuralComputation 9,8(Nov.1997),1735–1780. https://doi.org/10.1162/neco.
1997.9.8.1735
[46]JeremyHowardandSebastianRuder.2018. Universallanguagemodelfine-tuning
for text classification. In Proceedings of ACL 2019. 328–339. https://www.aclweb.
org/anthology/P18-1031/
[47]Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep Code Comment Gen-
eration.In ProceedingsofICPC2018.200–210. https://doi.org/10.1145/3196321.
3196334
[48]Michael Hucka. 2018. Spiral: splitters for identifiers in source code files. J. Open
Source Software 3, 24 (2018), 653.
[49]Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.Summarizing Source Code using a Neural Attention Model. In Proceedings of
ACL 2016. 2073–2083. http://www.aclweb.org/anthology/P16-1195
[50]Alan Jaffe, Jeremy Lacomis, Edward J Schwartz, Claire Le Goues, and Bogdan
Vasilescu.2018. MeaningfulVariableNamesforDecompiledCode:AMachine
1084Translation Approach. In Proceedings of ICPC 2018. 20–30. https://doi.org/10.
1145/3196321.3196330
[51]SébastienJean,KyungHyunCho,RolandMemisevic,andYoshuaBengio.2015.
On Using Very Large Target Vocabulary for Neural Machine Translation. In
Proceedings of ACL 2015. 1–10. https://doi.org/10.3115/v1/p15-1001
[52]Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410
(2016).
[53]RenéJust,DarioushJalali,andMichaelDErnst.2014. Defects4J:Adatabaseofex-
istingfaultstoenablecontrolledtestingstudiesforJavaprograms.In Proceedings
of ISSTA 2014. 437–440. https://doi.org/10.1145/2610384.2628055
[54]Svetoslav Karaivanov, Veselin Raychev, and Martin Vechev. 2014. Phrase-Based
Statistical Translation of Programming Languages. In Proceedings of Onward!
2014. 173–184. https://doi.org/10.1145/2661136.2661148
[55]Rafael-MichaelKarampatsisandCharlesA.Sutton.2019. MaybeDeepNeural
Networksare theBestChoiceforModeling SourceCode. CoRRabs/1903.05734
(2019). http://arxiv.org/abs/1903.05734
[56]Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp Nearby,
FuzzyFarAway:HowNeuralLanguageModelsUseContext.In Proceedingsof
ACL 2018. 284–294. https://doi.org/10.18653/v1/P18-1027
[57]YoonKim,YacineJernite,DavidSontag,andAlexanderMRush.2016. Character-
Aware Neural Language Models.. In Proceedings of AAAI 2016. 2741–2749. http:
//www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12489
[58]Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018. Code Completion with
Neural Attention and Pointer Networks. In Proceedings of IJCAI 2018. 4159–4165.
https://doi.org/10.24963/ijcai.2018/578
[59]Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better Word
RepresentationswithRecursiveNeuralNetworksforMorphology.In Proceedings
of CoNLL 2013. 104–113. https://www.aclweb.org/anthology/W13-3512/
[60]Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019. NL2Type: inferring
JavaScriptfunctiontypesfromnaturallanguageinformation.In Proceedingsof
ICSE 2019. 304–315. https://doi.org/10.1109/ICSE.2019.00045
[61]Vadim Markovtsev, Waren Long, Egor Bulychev, Romain Keramitas, Konstantin
Slavnov, and Gabor Markowski. 2018. Splitting source code identifiers using
Bidirectional LSTM Recurrent Neural Network. arXiv preprint arXiv:1805.11651
(2018).
[62]Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017.
PointerSentinelMixtureModels.In ProceedingsofICLR2017. https://openreview.
net/forum?id=Byj72udxe
[63]Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký, and Sanjeev Khu-
danpur. 2010. Recurrent neural network based language model. In Proceed-
ings of INTERSPEECH 2010. 1045–1048. http://www.isca-speech.org/archive/
interspeech_2010/i10_1045.html
[64]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
DistributedRepresentationsofWordsandPhrasesandTheirCompositionality.
InProceedings of NIPS 2013. USA, 3111–3119. http://dl.acm.org/citation.cfm?id=
2999792.2999959
[65]TomasMikolov,IlyaSutskever,AnoopDeoras,LeHaiSon,StefanKombrink,and
Jan Cernock. 2012. Subword Language Modeling With Neural Networks. (08
2012).
[66]Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen. 2013. Lexical
StatisticalMachineTranslationforLanguageMigration.In ProceedingsESEC/FSE
2013. 651–654. https://doi.org/10.1145/2491411.2494584
[67]ThanhNguyen,PeterC.Rigby,AnhTuanNguyen,MarkKaranfil,andTienN.
Nguyen. 2016. T2API: Synthesizing API Code Usage Templates from English
TextswithStatisticalTranslation.In ProceedingsofSIGSOFT/FSE2016.1013–1017.
https://doi.org/10.1145/2950290.2983931
[68]TungThanhNguyen,AnhTuanNguyen,HoanAnhNguyen,andTienN.Nguyen.
2013. A Statistical Semantic Language Model for Source Code. In Proceedings of
ESEC/FSE2013.NewYork,NY,USA,532–542. https://doi.org/10.1145/2491411.
2491458
[69]Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, ChristopherClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word
Representations. In Proceedings of NAACL-HLT 2018. 2227–2237. https://doi.org/
10.18653/v1/n18-1202
[70]Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to
Name-basedBugDetection. Proc.ACMProgram.Lang. 2,OOPSLA,Article147
(Oct. 2018), 25 pages. https://doi.org/10.1145/3276517
[71]Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.Improving language understanding by generative pre-training. Available:
https://blog.openai.com/language-unsupervised/ (2018).
[72]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Leo Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. Avail-
able: https://blog.openai.com/better-language-models/ (2019).
[73]Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. SWIM: Synthesizing
What I Mean: Code Search and Idiomatic Snippet Synthesis. In Proceedings of
ICSE 2016. 357–367. https://doi.org/10.1145/2884781.2884808[74]MusfiqurRahman,DharaniPalani,andPeterC.Rigby.2019. Naturalsoftware
revisited.In ProceedingsofICSE2019.37–48. https://doi.org/10.1109/ICSE.2019.
00022
[75]Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli, and Premkumar Devanbu. 2016. On the "Naturalness" of Buggy Code.
InProceedings of ICSE 2016. 428–439. https://doi.org/10.1145/2884781.2884848
[76]VeselinRaychev,MartinVechev,andEranYahav.2014. CodeCompletionwith
StatisticalLanguageModels.In ProceedingsofPLDI2014.419–428. https://doi.
org/10.1145/2594291.2594321
[77]Romain Robbes and Andrea Janes. 2019. Leveraging small software engineering
datasetswithpre-trainedneuralnetworks.In ProceedingsofICSE(NIER)2019.
29–32. https://doi.org/10.1109/ICSE-NIER.2019.00016
[78]EddieAntonioSantos,JoshuaCharlesCampbell,DhvaniPatel,AbramHindle,
and José Nelson Amaral. 2018. Syntax and Sensibility: Using language models to
detect and correct syntax errors. In Proceedings of SANER 2018. 311–322. https:
//doi.org/10.1109/SANER.2018.8330219
[79]Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proceedings of ACL 2016.
https://doi.org/10.18653/v1/p16-1162
[80]Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from
Overfitting. Journal of Machine Learning Research 15 (2014), 1929–1958. http:
//jmlr.org/papers/v15/srivastava14a.html
[81]ZhaopengTu,ZhendongSu,andPremkumarT.Devanbu.2014. Onthelocalness
of software. In Proceedings of SIGSOFT/FSE 2014. 269–280. https://doi.org/10.
1145/2635868.2635875
[82]MicheleTufano,JevgenijaPantiuchina,CodyWatson,GabrieleBavota,andDenys
Poshyvanyk. 2019. On Learning Meaningful Code Changes via Neural Machine
Translation. In Proceedings of ICSE 2019. 25–36. https://doi.org/10.1109/ICSE.
2019.00021
[83]ClaraVania andAdamLopez.2017. FromCharacters toWords toinBetween:Do We Capture Morphology?. In Proceedings of ACL 2017. 2016–2027. https:
//doi.org/10.18653/v1/P17-1184
[84]BogdanVasilescu,CaseyCasalnuovo,andPremkumarDevanbu.2017.Recovering
clear,naturalidentifiersfromobfuscatedJSnames.In ProceedingsofESEC/FSE
2017. 683–693. https://doi.org/10.1145/3106237.3106289
[85]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
y ouneed.InProceedingsofNIPS2017.5998–6008. http://papers.nips.cc/paper/
7181-attention-is-all-you-need
[86]Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer Net-
works. In Proceedings of NIPS 2015. 2692–2700. http://papers.nips.cc/paper/
5866-pointer-networks
[87]MartinWhite,MicheleTufano,ChristopherVendome,andDenysPoshyvanyk.
2016. Deep Learning Code Fragments for Code Clone Detection. In Proceedings
of ASE 2016. 87–98. https://doi.org/10.1145/2970276.2970326
[88]Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshy-
vanyk.2015. TowardDeepLearningSoftwareRepositories.In ProceedingsMSR
2015. 334–345. http://dl.acm.org/citation.cfm?id=2820518.2820559
[89]Peter Willett. 2006. The Porter stemming algorithm: then and now. Program40,
3 (2006), 219–223. https://doi.org/10.1108/00330330610681295
[90]PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGrahamNeubig.
2018. Learning to mine aligned code and natural language pairs from stack
overflow. In Proceedings of MSR 2018. 476–486. https://doi.org/10.1145/3196398.
3196408
1085