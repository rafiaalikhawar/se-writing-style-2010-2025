Causal Testing: Understanding Defects‚Äô Root Causes
Brittany Johnson
University of Massachusetts Amherst
Amherst, MA, USA
bjohnson@cs.umass.eduYuriy Brun
University of Massachusetts Amherst
Amherst, MA, USA
brun@cs.umass.eduAlexandra Meliou
University of Massachusetts Amherst
Amherst, MA, USA
ameli@cs.umass.edu
ABSTRACT
Understanding the root cause of a defect is critical to isolating and
repairingbuggybehavior. WepresentCausalTesting,anewmethod
of root-cause analysis that relies on the theory of counterfactual
causality to identify a set of executions that likely hold key causal
information necessary to understand and repair buggy behavior.
Using the Defects4J benchmark, we find that Causal Testing could
be applied to 71% of real-world defects, and for 77% of those, it can
help developers identify the root cause of the defect. A controlled
experimentwith37developersshowsthatCausalTestingimproves
participants‚Äô ability to identify the cause of the defect from 80% of
the time with standard testing tools to 86% of the time with Causal
Testing. TheparticipantsreportthatCausalTestingprovidesuseful
information they cannot get using tools such as JUnit. Holmes, our
prototype,open-sourceEclipsepluginimplementationofCausal
Testing, is available at http://holmes.cs.umass.edu/.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and
debugging.
KEYWORDS
CausalTesting,causality,theoryofcounterfactualcausality,soft-
ware debugging, test fuzzing, automated test generation, Holmes
ACM Reference Format:
Brittany Johnson, Yuriy Brun, and Alexandra Meliou. 2020. Causal Testing:
UnderstandingDefects‚ÄôRootCauses. In 42ndInternationalConferenceon
Software Engineering (ICSE ‚Äô20), May 23‚Äì29, 2020, Seoul, Republic of Ko-
rea.ACM,NewYork,NY,USA, 13pages.https://doi.org/10.1145/3377811.
3380377
1 INTRODUCTION
Debuggingandunderstandingsoftwarebehaviorisanimportant
part of building software systems. To help developers debug, many
existing approaches, such as spectrum-based fault localization [ 21,
41], aim to automatically localize bugs to a specific location in
the code [ 6,18]. However, finding the relevant line is often not
enough to help fix the bug [ 56]. Instead, developers need help
identifying and understanding the root cause of buggy behavior.
Whiletechniquessuchasdeltadebuggingcanminimizeafailing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage. Copyrightsforcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored. Abstractingwithcreditispermitted. Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô20, May 23‚Äì29, 2020, Seoul, Republic of Korea
¬© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380377test input [ 74] and a set of test-breaking changes [ 73], they do not
help explain whythe code is faulty [40].
To address this shortcoming of modern debugging tools, this
paperpresents CausalTesting,anoveltechniqueforidentifyingroot
causes of failing executions based on the theory of counterfactual
causality. CausalTestingtakesamanipulationistapproachtocausal
inference [ 71], modifying and executing tests to observe causal
relationshipsandderivecausalclaimsaboutthedefects‚Äôrootcauses.
Givenoneormorefailingexecutions,CausalTestingconducts
causal experiments by modifying the existing tests to produce a
small set of executions that differ minimally from the failing ones
butdonotexhibitthefaultybehavior. Byobservingabehaviorand
then purposefully changing the input to observe the behavioral
changes,CausalTestinginferscausalrelationships[ 71]: Thechange
in the input causesthe behavioral change. Causal Testing looks for
two kinds of minimally-different executions, ones whose inputs
are similar and ones whose execution paths are similar. Whenthedifferencesbetweenexecutions,eitherintheinputsorinthe
execution paths, are small, but exhibit different test behavior, these
small, causal differences can help developers understand what is
causing the faulty behavior.
Consider a developer working on a web-based geo-mapping ser-
vice (such as Google Maps or MapQuest) receiving a bug report
that the directions between ‚ÄúNew York, NY, USA‚Äù and ‚Äú900 Ren√©
L√©vesqueBlvd.WMontreal,QC,Canada‚Äùarewrong. Thedeveloper
replicates the faulty behavior and hypothesizes potential causes.
Maybe the special characters in ‚ÄúRen√© L√©vesque‚Äù caused a problem.Maybethefirstaddressbeingacityandthesecondaspecificbuild-ingcausedamismatchininternaldatatypes. Maybetherouteistoo
long and the service‚Äôs precomputing of some routes is causing theproblem. Maybe construction on the Tappan Zee Bridge along the
route has created flawed route information in the database. There
are many possible causes to consider. The developer decides to
step through the faulty execution, but the shortest path algorithm
coupled with precomputed-route caching and many optimizations
is complex, and it is not clear how the wrong route is produced.
The developer gets lost inside the many libraries and cache calls,
and the stack trace quickly becomes unmanageable.
Suppose,instead,atoolhadanalyzedthebugreport‚Äôstestand
presentedthedeveloperwiththeinformationinFigure 1. Thedevel-
operwouldquicklyseethatthespecialcharacters, thefirstaddressbeingacity,thelengthoftheroute,andtheconstructionarenottherootcauseoftheproblem. Instead,allthefailingtestcaseshaveoneaddressintheUnitedStatesandtheotherinCanada,whereasallthepassingtestcaseshaveboththestartingandendingaddressesinthe
same country. Further, the tool found a passing and a failing input
withminimalexecutiontracedifferences: thefailingexecutioncon-
tains acall to the metricConvert(pathSoFar) method butthe passing one
872020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
1 Failing: New York, NY, USA to
900 Ren√© L√©vesque Blvd. W Montreal , QC, Canada
2 Failing: Boston , MA, USA to
900 Ren√© L√©vesque Blvd. W Montreal , QC, Canada
3 Failing: New York, NY, USA to
1 Harbour Square, Toronto , ON, Ca nada
4 Passing: New York, NY, USA to
39 Dalton St, Boston , MA, USA
5 Passing: Toronto , ON, Canada to
900 Ren√© L√©vesque Blvd. W Montreal , QC, Canada
6 Passing: Vancouver , BC, Canada to
900 Ren√© L√©vesque Blvd. W Montreal , QC, Canada
Minimally-different execution traces:
7 Failing: Passing:
8 [...] [...]
9 findSubEndPoints(sor6, tar7); findSubEndPoints(sor6, tar7);
10 findSubEndPoints(sor7, tar8); findSubEndPoints(sor7, tar8);
11 metricConvert(pathSoFar) ;
12 findSubEndPoints(sor8, tar9); findSubEndPoints(sor8, tar9);
13 [...] [...]
Figure 1: Passing and failing tests for a geo-mapping service appli-
cation, and test execution traces.
doesnot.1Armedwiththisinformation,thedeveloperisnowbetter
equipped to find and edit code to address the root cause of the bug.
We implementCausal Testingin anopen-source, proof-of-con-
cept Eclipse plug-in, Holmes, that works on Java programs and
interfaces with JUnit. Holmes is publicly available at http://holmes.
cs.umass.edu/. WeevaluateCausalTestingintwoways. First,we
use Holmes in a controlled experiment. We asked 37 developers
to identify theroot causes of real-worlddefects, with and without
accesstoHolmes. Wefoundthatdeveloperscouldidentifytheroot
cause86%ofthetimewhenusingHolmes,butonly80%ofthetime
without it. Second, we evaluate Causal Testing‚Äôs applicability to
real-worlddefectsbyconsideringdefectsfromreal-worldprograms
in the Defects4J benchmark [ 45]. We found that Causal Testing
couldbeappliedto71%ofreal-worlddefects,andthatfor77%of
those, it could help developers identify the root cause.
A rich body of prior research aims to help developers debug
faulty behavior. Earlier-mentioned fault localization techniques [ 3,
6,18,21,32,33,41,47,48,70,75] rank code locations according
tothelikelihoodthattheycontainafault,forexampleusingtest
cases [41] or static code features [47, 48]. The test-based rankings
canbeimproved,forexample,bygeneratingextratests[ 6,75]or
by applying statistical causal inference to observational data [ 7,8].
Automatedtestgenerationcancreatenewtests,whichcanhelpdis-
cover buggy behavior and debug it [ 29,30,35,42], and techniques
canminimizetestsuites[ 38,54,68]andindividualtests[ 34,73,74]
to help deliver the most relevant debugging information to thedeveloper. These techniques can help developers identify where
thebugis. Bycontrast,CausalTestingfocusesonexplaining why
buggy behavior is taking place. Unlike these prior techniques,
CausalTestinggenerates pairsofverysimilartests thatnonetheless
exhibit different behavior. Relatedly, considering tests that exhibit
minimally different behavior, BugEx focuses on tests that differ
slightlyinbranchingbehavior[ 60]andDarwingeneratesteststhat
1Note that prior work, such as spectrum-based fault localization [ 21,41], can identify
the differences in the traces of existing tests; the key contribution of the tool we
describe here is generating the relevant executions with the goal of minimizing input
and execution trace differences.pass a version of the program without the defect but fail a version
with the defect [ 58]. Unlike these techniques, Causal Testing re-
quires only a single, faulty version of the code, and only a singlefailing test, and then conducts causal experiments and uses the
theory ofcounterfactual causality toproduce minimally-different
testsandexecutionsthathelpdevelopers understand thecauseof
the underlying defect.
Therestofthispaperisstructuredasfollows. Section 2illustrates
how Causal Testing can help developers on a real-world defect.
Sections3and4describe Causal Testing and Holmes, respectively.
Section5evaluateshowusefulHolmesisinidentifyingrootcauses
and Section 6evaluates how applicable Causal Testing is to real-
worlddefects. Section 7discussestheimplicationsofourfindings
and limitations and threats to the validity of our work. FinallySection8places our work in the context of related research, and
Section9summarizes our contributions.
2 MOTIVATING EXAMPLE
Consider Amaya, a developer who regularly contributes to open
source projects. Amaya codes primarily in Java and regularly uses
theEclipseIDE andJUnit. Amayais workingonaddressingabug
report in the Apache Commons Lang project. The report comes
with a failing test (see 1 in Figure 2).
Figure2showsAmaya‚ÄôsIDEassheworksonthisbug. Amaya
runs the test to reproduce the error and JUnit reports that an ex-
ceptionoccurredwhiletryingtocreatethenumber 0Xfade(see2in
Figure2). Amaya looks through the JUnit failure trace, looking for
theplacethecodethrewtheexception(see 3Figure2). Amayaob-
serves that the exception comes from within a switchstatment, and
thatthereisnocaseforthe eattheendof 0Xfade. Toaddsuchacase,
Amayaexaminestheother switchcasesandrealizesthateachcaseis
makingadifferentkindofnumber,e.g., thecasefor lcreateseither
alongorBigInteger. Since 0Xfadeis 64222, Amaya conjectures that this
numberfitsinan int,andcreatesanewmethodcallto createInteger()
inside of the case for e. Unfortunately, the test still fails.
Using the debugger to step through the test‚Äôs execution, Amaya
seesthe NumberFormatException thrownonline545(see 3inFigure 2).
Sheseesthattherearetwootherlocationstheinputtouches(see
4and5in Figure 2) during executionthat could beaffectingthe
outcome. Shenow realizesthat thecodeon lines497‚Äì545, despite
being where the exception was thrown, may not be the location of
the defect‚Äôs cause. She is feeling stuck.
Butthen,AmayaremembersafriendtellingheraboutHolmes,a
CausalTestingEclipseplug-inthathelpsdevelopersdebug. Holmes
tellsher thatthecode failsonthe input 0Xfade,but passesoninput
0xfade. The key difference is the lower case x. Also, according to
the execution trace provided by Holmes, these inputs differ inthe execution of line 458 (see
4in Figure 2). The ifstatement
fails to check for the 0Xprefix. Now, armed with the cause of the
defect, Amaya turns to the Internet to find out the hexadecimal
specificationandlearnsthatthetestisright, 0Xand 0xarebothvalid
prefixes for hexadecimal numbers. She augments the ifstatement
and the bug is resolved!
HolmesimplementsCausalTesting,anewtechniqueforhelping
understand root causes of behavior. Holmes takes a failing testcase (or test cases) and perturbs its inputs to generate a pool of
88




Figure 2: Amaya‚Äôs Eclipse IDE, while she is debugging a defect evidenced by a failing test.
possible inputs. For example, Holmes may perturb 0Xfadeto0XFADE,
0xfade,edafX,0Xfad,Xfade,fade,andmanymore. Holmesthenexecutes
alltheseinputstofindthosethatpasstheoriginaltest‚Äôsoracle,and,
next, selects from the passing test cases a small number such that
either their inputs or their execution traces are the most similar to
the original, failing test case. Those most-similar passing test cases
help the developer understand the key input difference that makes
the test pass. Sometimes, Holmes may find other failing test cases
whose inputs are even more similar to the passing ones than the
originalinput,anditwouldreportthosetoo. Theideaistoshow
the smallest difference that causes the behavior to change.
Holmes presents both the static (test input) and dynamic (execu-
tion trace) information to the developer to compare the minimally-
different passing and failing executions to better understand the
root cause of the bug. For example, for this bug, Holmes showsthe inputs,
0Xfadeand 0xfade, and the traces of the two executions,
showingthatthepassingtestentersamethodfrom createInteger that
the failing test cases do not, dictating to Amaya the expected code
behavior, leading her to fix the bug.
3 CAUSAL TESTING
Amaya‚Äôs debugging experience is based on what actual developers
did while debugging real defects in a real-world version of Apache
Commons Lang (taken from the Defects4J benchmark [ 45]). As
the example illustrates, software is complex and identifying root
causesofprogramfailuresischallenging. Thissectiondescribesour
CausalTestingapproachtocomputingandpresentingdevelopers
with information that can help identify root causes of failures.Figure3describestheCausalTestingapproach. Givenafailing
test,CausalTestingconductsaseriesofcausalexperimentsstarting
withtheoriginaltestsuite. CausalTestingprovidesexperimental
resultstodevelopersintheformofminimally-differentpassingand
failing tests, and traces of their executions.
3.1 Causal Experiments with Test Cases
CausalTestingmodifiestestcasestoconductcausalexperiments;
it observes system behavior and then reports the changes to test
inputsthatcausesystembehaviortochange. Tocreatethesetest

assertTrue("createNumber(String) 9b failed", 0xFADE == NumberUtils.createNumber("0Xfade").intValue());
	
0XFADE
0xfadeedafX0
0XfadXfade
fade
‚Ä¶





		


0Xfade  0xfade
0Xfade  0XFADE
0Xfade  edafX
0Xfade  0Xfad
0Xfade  Xfade
0Xfade  fade
	




	
0Xfade 0xfade
createNumber() createNumber()
str.isBlank() str.startsWith()
‚Ä¶‚Ä¶	 


	

Figure 3: Causal Testing computes minimally-different test inputs
that, nevertheless, produce different behavior.
89case modifications and to then identify the modifications that lead
to behavioral change, Causal Testing needs a systematic way of
perturbing inputs and of measuring test case similarity, which
we describe in this section. Once the experiments are complete,
CausalTestingreportstothedeveloperalistofminimallydifferent
passingandfailingtestcaseinputsandtheirexecutiontraces,to
help explain root causes of the failing behavior.
3.1.1 Perturbing Test Inputs. Toconductcausalexperiments,Causal
Testingstartswithafailingtest,whichweshallcallfromnowon
theoriginalfailingtest,andidentifiestheclassthistestistesting.
Causal Testing considers all the tests of that class, and generates
moretestsusingautomatedtestinputgeneration(andtheoracle
from the one failing test), to create a set of failing and passing
tests. Then,CausalTestingfuzzestheseexistingandgeneratedtestinputstofindadditionalteststhatexhibitexpectedandunexpected
behavior.
Theoretically,itisalsopossibleforCausalTestingtoperturbthe
testoracle. Forexample,itmightchangethe assertTrueinFigure 3to
assertFalse. However,perturbingtestoraclesisunlikelytoproduce
meaningfulinformationtoguidethedevelopertotherootcause,or,
atleast,islikelytoproducemisleadinginformation. Forexample,
making a test pass simply by changing the oracle does not provide
information about key differences in test inputsthat alter software
behavior. Assuch,CausalTestingfocusesonperturbingtestinputs
only.
TherearedifferentwaysCausalTestingcouldassemblesetsof
passing and failing tests. First, Causal Testing could simply rely on
the tests already in the test suite. Second, Causal Testing could use
automated test generation [ 1,26,55] to generate a large number of
test inputs. Third, Causal Testing could use test fuzzing to change
the existing tests‚Äô inputs to generate new, similar inputs. Fuzz test-
ing is an active research area [ 29,30,35,42,67] (although the term
fuzztestingisalsousedtomeansimplygeneratingtests[ 1])and
hasbeenappliedinthesecuritydomaintostress-testanapplication
and automatically discover vulnerabilities, e.g., [30, 35,67].
Whileinreal-worldsystems,existingtestsuitesoftencontain
both passing and failing tests, these suites are unlikely to havesimilar enough pairs of one passing, one failing tests to provideuseful information about the root cause. Still, it is worthwhileto consider these tests first, before trying to generate more. As
such,oursolutiontothechallengeofgeneratingsimilarinputsis
to (1) start with all existing tests, (2) use multiple fuzzers to fuzzthese tests, (3) generate many tests, and (4) filter those tests to
selecttheonessimilartotheoriginalfailingtest. Asweobserved
withHolmes, our proof-of-conceptCausal Testingtool (described
in Section 4), using multiple input fuzzers provided a diverse set
of perturbations, increasing the chances that Causal Testing finds
a set of minimally-different inputs and that at least one of them
would lead to a passing execution.
3.1.2 Input Similarity. Giventwoteststhatdifferintheirinputs
butsharethesameoracle,CausalTestingneedstomeasurethesim-ilaritybetweenthetwotests,asitsgoalistofindpairsofminimally-differentteststhatexhibitoppositebehavior. Conceptually,toapply
thetheoryofcausalinference,thetwotestsshoulddifferinonly
one factor. For example, imagine a software system that processesapartmentrentalapplications. Iftwoapplicationinputsareiden-tical in every way except one entry, and the software crashes onone but not on the other, this pair of inputs provides one pieceof evidence that the differing entry causesthe software to crash.
(Otherpairsthatalsoonlydifferinthatoneentrywouldprovide
more such evidence.) If the inputs differed in multiple entries, it
would be harder to know which entry is responsible. Thus, to help
developers understand root causes, Causal Testing needs to pre-
ciselymeasureinputsimilarity. Weproposetwowaystomeasure
input similarity: syntactic differences andexecution path differences.
Static Input Differences. The static input similarity can be
viewed at different scopes. First, inputs can agree in some and
differ in others of their arguments (e.g., parameters of a method
call). Agreementacrossmoreargumentsmakesinputsmoresimilar.
Second, each argument whose values for the two tests differ candiffer to varying degrees. A measure of that difference dependson the type of the argument. For arguments of type
String, the
Levenshtein distance (the minimum number of single-characteredits required to change one
Stringinto the other) is a reasonable
measure,thoughthereareothersaswell,suchasHammingdistance
(difference between two values at the bit level). For numerical
arguments, their numerical difference or ratio is often a reasonable
measure.
We found that relatively simple measures of similarity suffice
for general debugging, and likely workwell in many domains. Us-
ing Levenshtein or Hamming distance for Strings, the arithmetic
differencefornumericalvalues, andsumsofelementsdistancesfor
Arrays, worked reasonably well, in practice, on the 330 defects from
four different real-world systems we examined from the Defects4J
benchmark [ 45]. However, more generally, the semantics of sim-
ilarity measures are dependent on the domain. Some arguments
may play a bigger role than others, and the meaning of some types
may only make sense in the particular domain. For example, in
apartment rental applications, a difference in the address may play
a much smaller role than a difference in salary or credit history.
As such, how the similarity of each argument is measured, and
how the similarities of the different arguments are weighed are
specifictothedomainandmayrequirefinetuningbythedeveloper,
especiallyforcustomdatatypes(e.g.,project-specific Objecttypes).
Still,intheend,wefoundthatsimple,domain-agnosticmeasures
worked well in the domains we examined.
Execution Path Differences. Along with static differences,
twoinputs candiffer basedon theirdynamic behaviorat runtime.
One challenge when considering only static input differences isthat a statically similar input may not always yield an outcome
thatisrelevanttotheoriginalexecution. Forexample,itispossible
that two inputs that differ in only one character lead to completely
incomparable, unrelated executions. Therefore, Causal Testing
also collects and compares dynamic information in the form of the
execution path the input causes.
Beyond simplistic ways to compare executions, such as by their
lengths, comparing the statements and method calls in each execu-
tion provides information we found helpful to understanding root
causes. Thisalsostrengthensthecausalconnectionbetweenthein-putchangeandthebehaviorchange;iftwoinputs‚Äôexecutions,one
passing and one failing, only differ by one executed statement, it is
likely that one statement plays an important role in the behavioral
90change. Augmentingmethodcallswiththeirreturnvaluesprovides
additionalinsightsinsituationswherethebugisevidentnotbythe
sequence of statements executed but in the use of a method that
returns an unexpected value.
Both static and execution path measures of similarity can be
useful in identifying relevant tests that convey useful information
todevelopers. Inputsthataresimilarbothstaticallyandintermsofexecutionpathsholdpotentialtoconveyevenmoreusefulinforma-
tion,astheyhaveevenfewerdifferenceswiththeoriginalfailing
test. Therefore, Causal Testing prioritizes tests whose inputs are
statically and dynamically similar to the original failing test.
3.2 Communicating Root Causes to Developers
After generating and executing test inputs, Causal Testing ranks
them by similarity and selects a user-specified target number of
themostsimilarpassingtestcases. Inourexperience,threetests
was a good target, though, at times, a time-out was necessary
because finding three similar passing tests was computationallyinfeasible. Causal Testing reports tests as it finds them, produce
resultsforthedeveloperasquicklyaspossible,whileitperforms
more computation, looking for potentially more results.
Causal Testing collects the input and the execution traces for
each test it executes. These are, of course, used for determining
testcasesimilarity,butalsoholdthekeyinformationintermsof
whatdifferencesintestinputsleadtowhatbehavioralchanges. For
thepairsoffailingandpassingtests,CausalTestingpresentsthe
static differences in inputs, and the execution traces (along with
eachmethodcall‚Äôsargumentsandreturnvalues)withdifferences
highlighted. Becauseexecutiontracescanget large,parsingthem
can be difficult for developers; showing differences in the tracessimplifies this task. Causal Testing displays a minimized trace,
focused on the differences.
4 HOLMES: A CAUSAL TESTING PROTOTYPE
We have implemented Holmes, an open source Eclipse plug-in
CausalTestingprototype. Holmesisavailableat http://holmes.cs.
umass.edu/ and consists of four components: input and test case
generators, editdistancecalculators&comparers,a testexecutor&
comparator, and an output view.
4.1 Input & Test Case Generation
Holmesfirsttaskistocreateasetofcandidatetestcases. Holmes
first searches all tests in the current test suite for tests that are
similartotheoriginalfailingtestusingstringmatchingtodetermineiftwotestsaresimilar. Morespecifically,Holmesconvertstheentiretestfiletoastringandparsesitlinebyline. Thisisanapproximation
of test similarity. Future work can improve Holmes by considering
similarityindynamicexecutioninformationbetweenthetwotests,
orbycreatingnewtestsbyusingtestinputsfromothertestsbut
the oracle from the original failing test.
Next, Holmes proceeds to generate new tests. Holmes gets new
inputs for generating new tests in two ways:
‚Ä¢Test case generation. Holmes uses an existing test case
generationtool,EvoSuite[ 26]. WechoseEvoSuitebecause
itisastate-of-the-art,open-sourcetoolthatworkswithJavaand JUnit. Holmes determines the target class to generatetests from based on the class the original failing test tests.
Forexample,iftheoriginalfailingtestiscalled NumberUtilsTest ,
Holmes tells EvoSuite to generate tests for NumberUtils.T o
determineifatestisrelatedtotheoriginalfailure,Holmes
searches the generated tests for test cases that call the same
methodastheoriginaltest. Fromthisprocess,Holmeswill
get at least one valid input to use during fuzzing.
‚Ä¢Input fuzzing. To generate additional inputs for new tests,
Holmes fuzzes existing and generated testinputs. Holmesuses two off-the-shelf, open-source fuzzers, Peach
2and
Fuzzer3. To increase the chances that fuzzed inputs will
producepassingtests,Holmesprioritizes(whenavailable)inputs from passing tests. Holmes fuzzes the original in-
putandallvalidinputsfromgeneratedtestcases,againto
increase the chance of finding passing tests.
OnceHolmesrunstestgenerationandfuzzesthevalidinputs,
the next step is to determine which of the generated inputs are
most similar to the original.
4.2 Test Execution & Edit Distance Calculation
The current Holmes implementation uses static input similarity to
identifyminimally-differenttests. Usingonlystaticinputsimilarity
first provided us with a better understanding of how execution
informationcouldbecollectedandusedmosteffectively. Intheuser
study described in Section 5, we semi-automated using dynamic
execution trace information for evaluating Holmes. Future workcan improve Holmes by automatically using dynamic execution
trace information, as described in Section 3.1.2.
To evaluate static input differences, Holmes first determines
the data type of each argument in the method-under-test; this
determineshowHolmeswillcalculateeditdistance. Forarguments
with numerical values, Holmes calculates the absolute value of the
arithmeticdifferencebetweentheoriginalandgeneratedtestinput
argument. For example, inputs 1.0 and 4.0 have an edit distance
of 3.0. For Stringand charinputs, Holmes uses two different metrics.
First,HolmesdeterminestheHammingdistancebetweenthetwo
arguments. We elected to use Hamming distance first because
we found it increases the accuracy of the similarity measure for
randomly generated inputs. Once Holmes identifies inputs thatare similar using the Hamming distance, it uses the Levenshtein
distancetofurtherrefineitsfindings;inputsthatrequirethefewestcharacterchangestochangefromonetotheotheraremostsimilar.
Holmes uses an edit distance threshold of 3; tests whose inputs are
morethanaLevenshteindistanceof3awayfromtheoriginalfailing
tests are considered too different to be reported to the developer.
Holmesusestheexecutedtestbehaviortodeterminewhichin-
putssatisfytheoriginalfailingtest‚Äôsoracle. Then,Holmesattempts
tofurtherminimize thetestdifferencesby, foreachoriginalargu-
ment, iteratively replacing the original value with new input value
and executing the modified test to observe if the oracle is satisfied.
Holmesiteratestotrytofindthreesimilarpassingteststocompare
to the failing one.
2https://github.com/MozillaSecurity/peach
3https://github.com/mapbox/fuzzer
914.3 Communicating Root Causes to Developers
An important consideration when building a tool is how it will
communicatewiththedeveloper[ 39]. OnceHolmeshascomputeda
setofpassing(andasetoffailing)tests,itorganizestheinformation
for presentation. Holmes organizes tests by whether it passes or
fails, showing the original failing test at the top of the output
window, making it easy to compare the differences. Under each
test,Holmespresentsaminimizedtestexecutiontrace. Soastonotoverwhelmthedeveloperwithinformation, Holmes‚Äôuserinterfaceincludestheoptiontotoggleshowingandhidingtraceinformation.
4.4 Holmes‚Äô Limitations
We implemented Holmes as a prototype Causal Testing tool, to be
used in a controlled experiment with real users (see Section 5). We
havethusprioritizedensuringHolmesimplementstheaspectsof
Causal Testing we needed to evaluate, over fully automating it.
ThecurrentversionofHolmesautomatestestgeneration,exe-
cution,andstaticeditdistancecalculation. WeusedInTrace[ 36]to
collectruntimeexecutiontracesandthen manually incorporated
theexecutioninformationwiththetests. FutureversionsofHolmes
will automate the dynamic trace collection and comparison.
The current version of Holmes relies on the Defects4J bench-
mark[45]usedinourevaluations,andextendingittootherdefects
may require extending Holmes or setting those defects‚Äô projectsup in a particular way. For simplicity, Holmes works on single-argument tests with
Stringor primitive arguments. While this is
sufficient for the defects in Defects4J benchmark, this limitation
willneedtobeliftedfortestswithmultiplearguments. OurHolmes
prototypeimplementation isopen-source, toallowothersto build
on it and improve it.
5 CAUSAL TESTING EFFECTIVENESS
Wedesignedacontrolleduserstudyexperimentwith37developers
to answer the following three research questions:
RQ1:Does Causal Testing improve the developers‚Äô ability to iden-
tify the root causes of defects?
RQ2:DoesCausalTestingimprovethedevelopers‚Äôabilitytorepair
defects?
RQ3:Do developers find Causal Testing useful, and, if so, what
aspect of Causal Testing is most useful?
5.1 User Study Design
Causal Testing‚Äôs goal is to help developers determine thecause of
a test failure, thereby helping developers better understand andeliminate defects from their code. We designed our user studyand prototype version of Holmes to provide evidence of Causal
Testing‚Äôs usefulness, while also providing a foundation of what
information is useful for Causal Testing.
We randomly selected seven defects from Defects4J, from the
Apache Commons Lang project. We chose Apache Commons Lang
becauseit(1)isthemostwidelyknownprojectinDefects4J,(2)had
defectsthat requiredonlylimiteddomain knowledge,and (3)can
be developed in Eclipse.
Ouruserstudyconsistedofatrainingtaskandsixexperimen-
tal tasks. Each task mapped to one of the seven defects. Each
participantstartedwiththetrainingtask,andthenperformedsixexperimentaltasks. ThetrainingtaskandthreeoftheexperimentaltasksusedHolmesandtheotherthreeexperimentaltasksbelonged
to the control group and did not include the use of Holmes. The
orderof thetasks, andwhich taskswere part ofthe controlgroupand which part of the experimental group were all randomized.
For the training task, we provided an Eclipse project with a
defective code version and single failing test. We explained how
to execute the test suite via JUnit, and how to invoke Holmes. We
allowed participants to explore the code and ask questions, telling
themthat thegoalistochange thecodesothat allthattestspass.
Each task that followed was similar to the training task; controlgroup tasks did not have access to Holmes, experimental group
tasks did.
Werecordedaudioandthescreenforlateranalysis. Weasked
participantstocompleteacausalityquestionnaireaftereachtask
consisting of two questions: ‚ÄúWhat caused Test X to fail?‚Äù and
‚ÄúWhat changes did you make to fix it?‚Äù
At then end, the participants completed an exit survey with
open-ended questions, such as ‚ÄúWhat information did you findmost helpful when determining what caused tests to fail?‚Äù and
4-point Likert scale questions, such as ‚ÄúHow useful did you find
X?‚ÄùFortheLikert-scalequestions,wegaveparticipantstheoptions
‚Äúveryuseful‚Äù,‚Äúsomewhatuseful‚Äù,‚Äúnotuseful‚Äù,and‚Äúmisleadingor
harmful‚Äù. We also gave participants an opportunity to provide
additional feedback they saw fit.
Prior to our experiment, we conducted a pilot of our initial user
studydesignwith23studentsfromagraduatesoftwareengineering
course. Our pilot study consisted of 5 tasks and a mock-up version
of Holmes. We used lessons learned and challenges encounteredto finalize the design of our study. The 23 pilot participants didnot participate in the final study presented here. All final study
materials areavailable online at http://holmes.cs.umass.edu in the
user_study_materials directory.
5.2 Participants
We recruited a total of 39 participants from industry and academia:
15 undergraduate students, 12 PhD students, 9 Masters students,
2industrydevelopers,and1researchscientist. Participants‚Äôpro-gramming experience ranged from 1 to 30 years and experience
withJavarangedfromafewmonthsto15years. Allparticipants
reportedhavingpriorexperiencewithEclipseandJUnit. Weana-
lyzeddatafrom37participants;2undergraduateparticipants(P2
and P3) did not follow the instructions, so we removed them from
our dataset.
5.3 User Study Findings
We now summarize the results from our study.
RQ1: DoesCausalTestingimprovethedevelopers‚Äôabilitytoidentify the root causes of defects?
TheprimarygoalofCausalTestingistohelpdevelopersidentify
the root cause of test failures. To answer RQ1, we analyzed the
responsesparticipantsgavetothequestion‚ÄúWhatcausedTestX
to fail?‚Äù We markedresponses aseither correct (capturedfull and
the true cause) or incorrect (missing part or all of the true cause).
Figure4shows the root cause identification correctness results.
When using Holmes, developers correctly identified the cause 86%
92Defect Group Correct Incorrect Total
1Control 17 (89%) 2 (11%) 19
Holmes 17 (94%) 1 (5%) 18
2Control 12 (60%) 8 (40%) 20
Holmes 9 (53%) 8 (47%) 17
3Control 19 (95%) 1 (5%) 20
Holmes 16 (94%) 1 (6%) 17
4Control 15 (83%) 3 (17%) 18
Holmes 18 (95%) 1 (5%) 19
5Control 13 (87%) 2 (13%) 15
Holmes 21 (95%) 1 (5%) 22
6Control 12 (67%) 6 (33%) 18
Holmes 15 (79%) 4 (21%) 19
TotalControl 88 (80%) 22 (20%) 110
Holmes 96 (86%) 16 (14%) 112
Figure 4: Distributions of correct and incorrect cause descriptions,
per defect.
Average Resolution Time (in minutes)
Defect: 1 2 3 4 5 6
Control 16.5 10.6 6.8 12.9 3.7 10.0Holmes 17.0 12.7 6.4 17.7 4.9 10.1
Figure5: Theaveragetimedeveloperstooktoresolvethedefects,inminutes.
of the time (96 out of 112 times). The control group only identi-
fied the cause 80% of the time (88 out of 110). Fisher‚Äôs exact test
findsthatthesesamplescomefromdifferentdistributionswith83%
probability ( p=0.17).
For four of the six defects, (Defects 1, 4, 5, and 6), developers
using Holmes were more accurate when identifying root causesthan the control group. For Defects 1, 4, and 5, participants only
incorrectlyidentifiedthecauseapproximately5%ofthetimewhenusingHolmes,comparedto11‚Äì17%ofthetimewithoutHolmes. For
Defect 6, participants with Holmes identified the correct cause 79%
(15 out of 19) of the time; without Holmes they could only identify
thecorrectcause67%(12outof18)ofthetime. Ourfindingssuggestthat
CausalTestingsupportsandimprovesdeveloperability
to understand root causes, for at least some defects.
RQ2: DoesCausalTestingimprovethedevelopers‚Äôabilityto
repair defects?
While Causal Testing‚Äôs main goal is to help developers under-
standtherootcause,thisunderstandingmaybehelpfulinremoving
the defect as well. To answer RQ2, we analyzed participants‚Äô re-sponses to the question ‚ÄúWhat changes did you make to fix thecode?‚Äù We used the same evaluation criteria and labeling as for
RQ1. To determine if causal execution information improves devel-
opers‚Äô ability to debug andrepairdefects, we observed the time it
tookparticipantstocompleteeachtaskandthecorrectnessoftheir
repairs.Defect Group Correct Incorrect Total
1Control 16 (89%) 2 (11%) 18
Holmes 12 (86%) 2 (14%) 14
2Control 12 (100%) 0 (0%) 12
Holmes 7 (100%) 0 (0%) 7
3Control 19 (100%) 0 (0%) 19
Holmes 16 (100%) 0 (0%) 16
4Control 15 (100%) 0 (0%) 15
Holmes 19 (100%) 0 (0%) 19
5Control 12 (86%) 2 (14%) 14
Holmes 21 (95%) 1 (5%) 22
6Control 6 (75%) 2 (25%) 8
Holmes 5 (100%) 0 (0%) 5
TotalControl 80 (93%) 6 (7%) 86
Holmes 80 (96%) 3 (4%) 83
Figure6: Distributionofcorrectandincorrectrepairsimplemented
by participants, per defect.
Figure5showstheaveragetimeittookdeveloperstorepaireach
defect. We omitted times for flawed repair attempts that do not
addressthedefect. Onaverage,participantstookmoretimewith
Holmesonallbutonedefect(Defect3). Oneexplanationforthis
observation is that while Holmes helps developers understand the
root cause, this understanding takes time, which can reduce the
overallspeed of repair.
Figure6shows repair correctness results. When using Holmes,
developers correctly repairedthe defect 96% of thetime (80 out of
83) while the control group repaired the defect 93% of the time (80
out of 86).
For two of the six defects (Defects 5 and 6), developers us-
ing Holmes repaired the defect correctly more often (Defect 5:
95% vs. 86%; Defect 6: 100% vs. 75%). For Defects 2, 3, and 4, devel-
opers repaired the defect correctly 100% of the time both with and
withoutHolmes. Foronedefect(Defect1),developerswithHolmes
wereonlyabletorepairthedefectcorrectly86%(12outof14)of
thetimewhiledeveloperswithoutHolmescorrectlyfixeddefects
100% of the time.
Holmes did not demonstrate an observable advantage when
repairingdefects. Ourfindingssuggestthat CausalTestingsome-
timeshelpsdevelopersrepairdefects,butneitherconsistent-
ly nor statistically significantly.
RQ3: Do developers find Causal Testing useful, and, if so,
what aspect of Causal Testing is most useful?
ToanswerRQ3,weanalyzedpost-evaluationsurveyresponses
tothequestionaskingwhichinformationwasmostusefulwhen
understanding and debugging the defects. We extracted and ag-
gregated quantitative and qualitative results regarding information
most helpful when determining the cause of and fixing the defects.
WealsoanalyzedtheLikert-scaleratingsregardingtheusefulnessof
JUnit and the various components of causal execution information.
Overall, participants found the information provided by Holmes
more useful than other information available when understanding
anddebuggingthedefects. Outof37participants,17(46%)found
93the addition of at least one aspect of Holmes more useful than
outputprovidedbyJUnitalone. Further,15(41%)participantsfound
theadditionofHolmesatleastasusefulasJUnit. Theremaining
5(13%)foundtheadditionofHolmesnotasusefulasJUnitalone.
ThoughmajorityofparticipantsfoundHolmes‚Äôoutputmoreuseful,
JUnitandinteractivedebuggersareanimportantpartofdebugging.
Therefore, our expectations would be that Causal Testing would
augment those tools, not replace them.
Participants found the minimally-different passing tests Holmes
provided the most useful: 20 out of 37 participants (54%) rated this
piece of information as ‚ÄúVery Useful.‚Äù The passing and failing test
inputs that Holmes provided received ‚ÄúVery Useful‚Äù or ‚ÄúUseful‚Äù
rankings more often than the test execution traces. Finally, 18participants marked either the passing or failing execution traceas ‚ÄúNot Useful.‚Äù One participant felt the passing test traces were
‚ÄúMisleading or Harmful;‚Äù during their session, they noted that they
feltinsomecasestheexecutionpathswerenotassimilarasothers,
which made interpreting the output more confusing.
Togainabetterunderstandingofwhatpartsofcausalexecution
informationaremostuseful,andwhy,wealsoanalyzedparticipants‚Äô
qualitative responses to the questions asked in our post-evaluation
questionnaire.
What information did you find most helpful when determining what
caused tests to fail? Overall, 21 participants explicitly mentioned
someaspectofHolmesasbeingmosthelpful. For6ofthesepartici-pants,alltheinformationprovidedbyHolmeswasmosthelpfulfor
cause identification. Another 8 participants noted that specifically
thesimilarpassingandfailingtestsweremosthelpful. Forexample,
P36statedthesesimilartests whenpresented‚Äúsidebyside‚Äùmade
it ‚Äúeasy to catch a bug.‚Äù
Theother6participantsstatedtheexecutiontracesweremost
helpful. Oneparticipant‚ÄôsresponsesaidthatthepartsofHolmes
output that were most helpful was the output ‚Äúshowing methodcalls, parameters, and return values.‚Äù This was particularly true
whenthereweremultiplemethodcallsinanexecutionaccording
to P26: ‚Äúit was useful to see what was being passed to them and
what they were returning.‚Äù
Whatinformationdidyoufindmosthelpfulwhendecidingchangestomaketothecode? Overall,14participantsmentionedsomeaspectof
Holmes as being most helpful. Of these, 5 explicitly stated that the
similarpassingtestsweremosthelpfuloftheinformationprovided
by Holmes. P7, who often manually modified failing tests to better
understand expected behavior noted ‚Äúit helped to see what tests
werepassing,‚Äùwhich helpedhim‚Äúsee whatwas actuallyexpected
and valid.‚Äù
Fortheother4participants,theexecutiontracesweremosthelp-
ful for resolving the defect. One participant specifically mentioned
thatthereturnvaluesintheexecutiontracesforpassingandfailing
inputsweremosthelpfulbecausethenhecouldtell‚Äúwhichparts
are wrong.‚Äù
Wouldyouliketoaddanyadditionalfeedbacktosupplementyour
responses? Many participants used this question as an opportunity
to share why they thought Holmes was useful. Many reported
commentssuchas‚ÄúHolmesisgreat!‚Äù and‚Äúreallyhelpful.‚Äù Formany,
Holmes was most useful because it provided concrete, workingexamples of expected and non-expected behavior that help with
‚Äúpinpointing the cause of the bug.‚Äù
A participant noted that without Holmes, they felt like it was
‚Äúa bit slower to find the reason why the test failed.‚Äù Another par-
ticipantnotedthatthetraceprovidedbyHolmeswas‚Äúsomewhat
more useful‚Äù than the trace provided by JUnit.
In free-form, unprompted comments throughout the study, par-
ticipants often mentioned that the passing and failing tests andtraces were useful for their tasks; several participants explicitly
mentioned during their session that having the additional passing
andfailingtestswere‚Äúsuperuseful‚Äùandsavedthemtimeandeffort
in understanding and debugging the defect.
While the qualitative feedback is largely positive, it is impor-
tant to point out that we do not view Causal Testing tools as a
replacement forJUnit. The intent isfor themto complementeach
otherandhelpdevelopersunderstandanddebugsoftwarebehav-
ior. Three participants explicitly mentioned that Holmes is mostuseful in conjunction with JUnit and other tools available in the
IDE. Several participants highlighted the complementary nature of
thesetools. Forexample,P26explainedthatthoughHolmeswas
‚Äúvery useful when debugging the code,‚Äù it is most useful with other
debugging tools as ‚Äúit does not provide all information.‚Äù
Finally,participantsalsosuggestswaystoimproveHolmes. One
participantmentionedthatHolmesshouldaddtheabilitytoclick
on the output and jump to the related code in the IDE. Another
suggested making thedifferences between the passing andfailing
testsvisiblymoreexplicit. Threeparticipantsexplicitlysuggested,
rather than bolding the entire fuzzed input, only bolding the parts
thataredifferentfromtheoriginalfailingtest. Ourfindingssuggestthat
CausalTestingisusefulforbothcauseidentificationand
defectresolution,andiscomplementarytootherdebuggingtools.
6 CAUSAL TESTING APPLICABILITY TO
REAL-WORLD DEFECTS
ToevaluatetheusefulnessandapplicabilityofCausalTestingtoreal-
world defects, we conducted an evaluation on the Defects4J bench-
mark[45]. Defects4Jisacollectionofreproducibledefectsfoundin
real-world, open-source Java software projects: Apache Commons
Lang,ApacheCommonsMath,Closurecompiler,JFreeChart,and
Joda-Time. Foreachdefect,Defects4Jprovidesabuggyversionand
fixed version of the source code, along with the developer-written
testsuites, which includeoneor moretests thatfail onthe buggy
version but pass on the fixed version.
Wemanuallyexamined330defectsinfourofthefiveprojectsin
theDefects4Jbenchmarkandcategorizedthembasedonwhether
Causal Testing would work and whether it would be useful inidentifying the root cause of the defect. We excluded Joda-Time
from our analysis because of difficulty reproducing the defects.4
6.1 Evaluation Process
TodetermineapplicabilityofCausalTestingtodefectsintheDe-
fects4Jbenchmark,wefirstimportedthebuggyversionandfixed
versionintoEclipse. Wethenexecutedthedeveloper-writtentest
4Some such difficulties have been documented in the Joda-Time issue tracker:
https://github.com/dlew/joda-time-android/issues/37.
94suitesonthebuggyversiontoidentifythetargetfailingtestsand
the methods they tested.
Once we identified the target failing tests and methods under
test,weranHolmesusingthetargetfailingtests. IfHolmesranand
produced causal test pairs, we ran InTrace to produce execution
traces. Sometimes, Holmes was unable to produce an output. In
thesecases,weattemptedtoevaluateifamorematureversionof
Holmescouldhaveproducedanoutput. Todothis,wemanually
madesmallperturbationstothetestinputsinanattempttoproduce
reasonably similar passing tests. We made perturbations based on
thetypeofinputandhowamorematureCausalTestingtoolwould
work. For example, if the test input was a number, we made small
changes such as adding and subtracting increments of one from
the original value or making thenumber positive or negative. We
then executed the tests and attempted to produce causal test pairs.
In caseswhere Holmesor our manualanalysis wasable to pro-
ducesimilarpassingtests,wenextdeterminedifthisinformation
could be useful for understanding the root cause of that defect.
To do this, we first used the fixed version to determine what we
believed to be the root cause. If we were able to determine the
rootcause,wethenmadeadeterminationonwhetherthesimilar
passing tests and execution information would help developers
understand the root cause and repair the defect.
Weusedthisprocessandtheproducedinformationtocategorize
the defects, as we describe next.
6.2 Defect Applicability Categories
We categorized Causal Testing‚Äôs applicability to each defect into
the following five categories:
I.Works, useful,and fast. For these defects, Causal Testing
can produce at least one minimally-different passing test
thatcapturesitsrootcause. WereasonCausalTestingwould
be helpful to developers. In our estimate, the differencebetween the failing and minimally-different passing testsis reasonably small that it can be found on a reasonable
personalcomputer,reasonablyfast. Formostofthesedefects,
our existing Holmes implementation was able to produce
the useful output.
II.Works,useful,butslow. Forthesedefects,CausalTesting
can produce at least one minimally-different passing testthat captures its root cause, and this would be helpful todevelopers. However, the difference between the tests islarge, and, in our estimation, Causal Testing would need
additionalcomputationresources,suchasrunningovernight
or access to cloud computing. For most of these defects, our
current Holmes implementation was unable to produce the
necessary output, but a more mature version would.
III.Works, but is not useful. For these defects, Causal Test-
ing can produce at least one minimally different passingtest,butinourestimation,thistestwouldnotbeusefulto
understanding the root cause of the defect.
IV.Willnotwork. Forthesedefects,CausalTestingwouldnot
beabletoperturbthetests,andwouldtellthedeveloperit
cannot help right away.
V.Wecouldnotmakeadetermination. Becausethedefects
in our study are from real-world projects, some requiredApplicability Category
Project I II III IV V Total
Math 14 15 11 20 46 106
Lang 11 6 3 14 31 65Chart 2 4 1 1 18 26Closure 2 22 8 5 96 133
Total 29 47 23 40 191 330
Figure 7: Distribution of defects across five applicability categoriesdescribed in Section 6.2.
project-specific domain knowledge to understand. As we
are not the original projects‚Äô developers, for these defects,the lack of domain-specific knowledge prevented us from
understanding what information would help developers un-
derstand the root cause and debug, and we elected not to
speculate. Assuch,weoptednottomakeanestimationof
whether Causal Testing would be helpful for these defects.
6.3 Results
Figure7shows our defect classificationresults. Of the330 defects,
wecouldmakeadeterminationfor139. Ofthese,CausalTestingwould try to produce causal test pairs for 99 (71%). For the re-
maining40(29%),CausalTestingwouldsimplysayitcannothelp
and would not waste the developer‚Äôs time. Of these 99 defects, for
76(77%),CausalTestingcanproduceinformationhelpfuliniden-
tifyingtherootcause. For29(29%),asimplelocalIDE-basedtool
would work,and for47 (47%), atool wouldneed moresubstantial
resources, such as running overnight or on the cloud. The remain-
ing23(23%)wouldnotbenefitfromCausalTesting. Ourfindings
suggest that Causal Testing produces results for 71% of real-
world defects, and for 77% of those, it can help developers
identify and understand the root cause of the defect.
7 DISCUSSION
OurfindingssuggestthatCausalTestingcanbeusefulforunder-
standing root causes and debugging defects. This section discusses
implicationsofourfindings,aswellasthreatstothevalidityofour
studies and limitations of our approach.
Encapsulating causality in generated tests. Ouruserstudy
foundthathavingpassingandfailingteststhataresimilartothe
originalfailintestthatexposedadefectareusefulforunderstandinganddebuggingsoftwaredefects,thoughnotalldefects. Participantsfoundthepassingteststhatprovidedexamplesofexpectedbehaviorusefulforunderstandingwhyatestfailed. ThissuggeststhatCausal
Testing can be used to generate tests that encapsulate causality in
understanding defective behavior, and that an important aspectof debugging is being able to identify expected behavior when
software is behaving unexpectedly.
Executioninformationfordefectunderstanding&repair.
Execution traces can be useful for finding the location of a de-
fect [20], and understanding software behavior [ 10‚Äì14,28,46,53].
Our study has shown that such traces can also be useful for under-
standingrootcausesofdefects,and,insomecases,canhighlight
theserootcausesexplicitly. Participantsinourstudyfoundcom-
paring execution traces useful for understanding why the test was
95failing and how the code should behave differently for a fix. For
someparticipants, theexecutiontraceinformationwasthemostuseful of all information provided. These results support further
use of execution traces when conducting causal experiments.
CausalTestingasacomplementarytestingtechnique. Our
findingssupportCausalTestingasacomplementtoexistingdebug-
ging tools, such as JUnit. Understandably, participants sometimes
foundthemselvesneedinginformationthatHolmesdoesnotpro-
vide, especially once they understood the root cause and needed
to repair the defect. Our findings suggest that Causal Testing ismost useful for root cause identification. Still, a majority of the
participants in our study found Holmes useful for both cause iden-
tification and defect repair, despite, on average, taking longer to
resolve defects with Holmes. We speculate that increased familiar-
itywithCausalTestingwouldimprovedevelopers‚Äôabilitytouse
the right tool at the right time, improving debugging efficiency, as
supported by prior studies [39].
Supporting developers with useful tools. The goal of soft-
ware development tools is often to decrease developer effort, such
that developers will want to use that tool in practice. However,
researchsuggeststhatthefirstthingpractitionersconsiderwhende-cidingwhethertouseagiventoolisthattool‚Äôsusefulness[
59]. Our
study shows that participants often took more time to debug when
using Holmes; however, despite this and other challenges develop-
ersencountered,participantsstillgenerallyfoundHolmesuseful
for both understanding and debugging defects. This suggests that
animportantpartofevaluatingatoolintendedfordeveloperuseis
whetherthetoolprovidesusefulinformationincomparisonto,orin
our case, along with, existing tools available for the same problem.
7.1 Threats to Validity
External Validity. Our studies used Defects4J defects, a collec-
tion of curated, real-world defects. Our use of this well-known
andwidely-usedbenchmarkofreal-worlddefectsaimstoensure
ourresultsgeneralize. Weselecteddefectsfortheuserstudyran-
domly from those that worked with our current implementation of
Holmesandthatrequiredlittleornopriorprojectordomainknowl-
edge, with varying levels of difficulty. The applicability evaluation
considered all defects across four projects.
The user study used 37 participants, which is within range of
higher data confidence and is above average for similar user stud-
ies[9,25,50,62]. Ourstudyalsoreliedonparticipantswithdifferent
backgrounds and experience.
InternalValidity. Ouruserstudyparticipantswerevolunteers.
Thisleadstothepotentialforself-selectionbias. Wewereableto
recruitadiversesetofparticipants,somewhatmitigatingthisthreat.
Construct Validity. Part of our analysis of whether Causal
Testingwouldapplyandbeusefulfordebuggingspecificdefects
was manual. This leads to the potential for researcher bias. We
minimized this threat by developing and following concrete, repro-
ducible methodology and criteria for usefulness.
Theuserstudyaskedparticipantstounderstandanddebugcode
theyhadnotwritten,whichmaynotberepresentativeofasitationinwhichdevelopersaredebuggingcodetheyarefamiliarwith(but
is representative of a common scenario of developers debugging
others‚Äôcode). Weaimedtoselectdefectsforthestudythatrequiredlittle project and domain knowledge. Additionally, we did not
disclosethetruepurposeoftheuserstudytothesubjectsuntilafter
the end of each participant‚Äôs full session.
7.2 Limitations and Future Work
Causal Testing mutates tests‚Äô inputs while keeping the oraclesconstant (recall Section 3.1.1). This process makes an implicit as-
sumptionthatsmallperturbationsoftheinputsshouldnotaffect
the expected behavior, and, thus, if small perturbations do affect
thebehavior,knowingthisinformationisusefultothedeveloper
for understanding the root cause of why the faulty behavior is
takingplace. Thisassumptioniscommoninmanydomains,such
as testing autonomous cars [ 66] and other machine-learning-based
systems [ 57]. However,it also leads CausalTesting limitations. In
particular, some changes to the inputs doaffect expected behavior,
and using the unmodified oracle will not be valid in these cases.
Thiscan leadCausal Testingto generatepairs ofteststhat donot
capturecausalinformationabouttheexpectedbehaviorproperly.
Forexample, itcould produceatestthat passesbutthatuses the
wrongoracleandshould,infact, fail. Itremainsanopenquestion
whether such tests would be helpful for understanding root causes.
The causal test pair still indicates what minimal input change can
satisfy the oracle, which might still be useful for developers to un-
derstand the root causes, even if the passing test does not properly
capture the expected behavior.
FutureworkcouldextendCausalTestingtoincludeoraclemu-
tation. A fruitful line of research, when specifications, formal orinformal, are available, is to extract oracles from those specifica-tions. For example, Swami [
49] can extract test oracles (and gen-
erate tests) from structured, natural language specifications, and
Toradacu[ 31],Jdoctor[ 15],and@tComment[ 65]candosofrom
Javadoc specifications. Behavioral domain constraints [ 2,4,27],
data constraints [ 23,51,52], or temporal constraints [ 11,12,14,22,
53] can also act as oracles for the generated tests.
By fuzzing existing tests and focusing on test inputs that are
similarto theoriginal failingtest, CausalTestingattempts tomiti-
gate the risk that the tests‚Äô oracle will not apply. In a sense, a test‚Äôs
inputsmustsatisfyasetofcriteriafortheoracletoremainvalid,
andbymodifyingtheinputsonlyslightly(asdefinedbystaticor
dynamicbehavior),ourhopeisthatinsufficientlymanycases,thesecriteriawillnotbeviolated. Futureworkcouldconsiderimplement-
ing oracle-aware fuzzing that modifies inputs while specifically
attempting to keep the oracle valid.
Insomecases,itmaynotbepossibletogeneratepassingtests
bygeneratingnewtests. Forexample,codethatneverthrowsan
exception cannot have a test pass if that test‚Äôs oracle expects theexception to be thrown. In such cases, Causal Testing will not
producefalsepositiveresultsforthedeveloper,andwillsimplysay
no causal information could be produced.
Our studies have identified that Causal Testing is often, but not
always,helpful. Futureworkcanexaminepropertiesofdefectsor
testsforwhichCausalTestingismoreeffectiveatproducingcausal
information, and for which that causal information is more helpful
to developers. This information can, in turn, be used to improve
Causal Testing.
968 RELATED WORK
The closest work to Causal Testing is BugEx [ 60], which is also in-
spired by counterfactual causality. Given a failing test, BugEx uses
runtimeinformation,suchaswhetherabranchistaken,tofindpass-
ingandfailingteststhatdifferwithrespecttothatpieceofinforma-
tion. Darwin [ 58] targets regression failures and uses concrete and
symbolic execution to synthesize new tests such that each test dif-
fersincontrolflowwhenexecutedonthebuggyandthenon-buggy
versionofthecode. Bycontrast,CausalTestingrequiresonlyasin-
gleversionofthecode,andonlyasinglefailingtest,andgenerates
pairsofteststhatdiffer minimallyeitherstaticallyordynamically
(orboth)tohelpdevelopersunderstandtherootcauseofthedefect.
Deltadebugging[ 73,74]aimstohelpdevelopersunderstandthe
cause of a set of failing tests. Given a failing test, the underlying
ddminalgorithmminimizesthattest‚Äôsinputsuchthatremovingany
otherpieceofthetestmakesthetestpass[ 34]. Deltadebuggingcan
also be applied to a set of test-breaking code changes to minimize
that set, although in that scenario, multiple subsets that cannot be
reduced further are possible because of interactions between code
changes [ 64,74]. By contrast, Causal Testing does not minimize
an input or a set of changes, but rather produces otherinputs
(notnecessarilysmaller)thatdifferminimallybutcauserelevant
behavioral changes. The two techniques are likely complementary
in helping developers debug.
Whenappliedtocodechanges,deltadebuggingrequiresacorrect
code version and a set of changes that introduce a bug. Iterative
deltadebuggingdoesnotneedthecorrectversion,usingtheversion
historytoproduceacorrectversion[ 5]. Again,CausalTestingis
complementary,thoughfutureworkcouldextendCausalTesting
to consider the development history to guide fuzzing.
Faultlocalization(alsoknownasautomateddebugging)iscon-
cerned with locating the line or lines of code responsible for a
failingtest[ 3,41,70]. Spectralfaultlocalizationusesthefrequency
with which each code line executes on failing and passing tests
casestoidentifythesuspiciouslines[ 21,41]. Whentests(orfail-
ingtests)arenotavailable,staticcodeelementsordataaboutthe
process that created the software can be used to locate suspicious
lines [47,48]. Accounting for redundancy in test suites can im-
prove spectral fault localization precision [ 32,33]. MIMIC can
also improve fault localization precision by synthesizing additional
passingandfailingexecutions[ 75],andApollocandosobygen-
erating tests to maximize path constraint similarity [ 6]. Statistical
causalinferenceusesobservationaldatatoimprovefaultlocaliza-
tion precision [ 7,8]. Importantly, while statistical causal inference
aims to infer causality, it does not apply the manipulationist ap-
proach[71]thatCausalTestinguses;asaresult,CausalTestingcan
makemorepowerfulstatementsaboutthecausalrelationshipsit
discovers. Unfortunately, research has shown that giving develop-
ers the ground truth fault location (even from state-of-the-art fault
localizationtechniques)doesnotimprovethedevelopers‚Äôability
torepairdefects[ 56],likelybecauseunderstandingdefectcauses
requiresunderstandingmorecodethan justthelinesthatneedto
beedited. Bycontrast,CausalTestingdiscoversthechangestosoft-
ware inputs that causethe behavioral differences, and a controlled
experimenthasshownpromisethatCausalTestingpositivelyaf-
fects the developers‚Äô ability to understand defect causes.MutationtestingtargetsadifferentproblemthanCausalTesting,
andtheapproachesdiffersignificantly. Mutationtestingmutates
thesourcecodetoevaluatethequalityofatestsuite[ 43,44]. Causal
Testing doesnot mutatesource code(it perturbstest inputs)and
helpsdevelopersidentifyrootcausesofdefects,ratherthanimprove
testsuites(althoughitdoesgeneratenewtests.) Inaspecialcase
of Causal Testing, when the defect being analyzed is in software
whose input is a program (e.g., compiler), Causal Testing may rely
on code mutation operators to perturb the inputs.
Reproducingfieldfailures[ 37]isanimportantpartofdebugging
complementary to most of the above-described techniques, includ-
ingCausalTesting,whichrequireafailingtestcase. Fieldfailures
often tell more about software behavior than in-house testing [ 69].
Fuzz testing is the process of changing existing tests to generate
moretests[ 29,30](though,inindustry,fuzztestingisoftensynony-
mous with automated test input generation). Fuzz testing has been
usedmostoftentoidentifysecurityvulnerabilities[ 30,67]. Fuzzing
can be white-box, relying on the source code [ 30] or black-box,
relyingonlyonthespecificationorinputschema[ 42,67]. Causal
Testingusesfuzztestingandimprovementstofuzztestingresearch
can directly benefit Causal Testing by helping it to find similartest inputs that lead to different behavior. Fuzzing can be usedon complex inputs, such as programs [
35], which is necessary to
apply Causal Testing to software withsuch inputs (as is the casefor Closure, one of the subject programs we have studied). Fuzz
testingbyitselfdoesnotprovidethedeveloperwithinformationto
helpunderstanddefects‚Äôrootcauses,thoughthefailingtestcases
it generates can certainly serve as a starting point.
Thecentralgoalofautomatedtestgeneration(e.g.,EvoSuite[ 26],
andRandoop[ 55])andtestfuzzingisfindingnewfailingtestcases.
For example, combining fuzz testing, delta debugging, and tradi-tional testing can identify new defects, e.g., in SMT solvers [
17].
Automatedtestgenerationandfuzzingtypicallygeneratetestin-
puts, which can serve as regression tests [ 26] or require humans
to write test oracles. Without such oracles, one cannot know if
the tests pass or fail. Recent work on automatically extracting test
oracles from code comments can help [ 15,31,65]. Differential test-
ing can also produce oracles by comparing the executions of thesame inputs on multiple implementations of the same specifica-
tion [16,19,24,61,63,72]. Identifying defects by producing failing
tests is the precursor to Causal Testing, which uses a failing test to
help developers understand the defects‚Äô root cause.
9 CONTRIBUTIONS
We have presented Causal Testing, a novel method for identifying
rootcausesofsoftwaredefectsthatsupplementsexistingtesting
and debugging tools. Causal Testing is applicable to 71% of real-
world defects in theDefects4J benchmark, and for 77% ofthose, it
canhelpdevelopersidentifytherootcauseofthedefect. DevelopersusingHolmes,aproof-of-conceptimplementationofCausalTesting,
were more likely to correctly identify root causes than without
Holmes (86% vs. 80% of the time). Majority of developers who used
Holmesfounditmostusefulwhenattemptingtounderstandwhy
a test failed and in some cases how to repair the defect. Overall,
CausalTestingshowspromiseforimprovingthedebuggingprocess,
especially when used together with other debugging tools.
97ACKNOWLEDGMENTS
Thiswork issupported bythe NationalScience Foundationunder
grants no. CCF-1453474, IIS-1453543, and CCF-1744471, and by
Google and Oracle Labs.
REFERENCES
[1] AFL 2018. American fuzzy lop. http://lcamtuf.coredump.cx/afl/.
[2]Aniya Aggarwal, Pranay Lohia, Seema Nagar, Kuntal Dey, and Diptikalyan Saha.
2018. AutomatedtestgenerationtodetectindividualdiscriminationinAImodels.
CoRRabs/1809.03260 (2018), 1‚Äì8. https://arxiv.org/abs/1809.03260
[3]Hiralal Agrawal, Joseph R. Horgan, Saul London, and W. Eric Wong. 1995. Fault
localizationusingexecutionslicesanddataflowtests.In InternationalSymposium
on Software Reliability Engineering (ISSRE). Toulouse, France, 143‚Äì151. https:
//doi.org/10.1109/ISSRE.1995.497652
[4]Rico Angell, Brittany Johnson, Yuriy Brun, and Alexandra Meliou. 2018. Themis:
Automatically testing software for discrimination. In European Software Engi-
neeringConferenceandACMSIGSOFTInternationalSymposiumonFoundationsof
Software Engineering (ESEC/FSE) Demonstration track (6‚Äì9). Lake Buena Vista,
FL, USA, 871‚Äì875. https://doi.org/10.1145/3236024.3264590
[5]Cyrille Artho. 2011. Iterative delta debugging. International Journal on Software
Tools for Technology Transfer 13, 3 (2011), 223‚Äì246. https://doi.org/10.1007/978-
3-642-01702-5_13
[6]Shay Artzi, Julian Dolby, Frank Tip, and Marco Pistoia. 2010. Directed test
generationforeffectivefaultlocalization.In InternationalSymposiumonSoftware
TestingandAnalysis(ISSTA).Trento,Italy,49‚Äì60. https://doi.org/10.1145/1831708.
1831715
[7]GeorgeK.Baah,AndyPodgurski,andMaryJeanHarrold.2010. Causalinference
for statistical fault localization. In International Symposium on Software Test-
ingandAnalysis(ISSTA).Trento,Italy,73‚Äì84. https://doi.org/10.1145/1831708.
1831717
[8]George K. Baah, Andy Podgurski, and Mary Jean Harrold. 2011. Mitigating
the confounding effects of program dependences for effective fault localization.
InEuropean Software Engineering Conference and ACM SIGSOFT International
Symposium on Foundations of Software Engineering (ESEC/FSE). Szeged, Hungary,
146‚Äì156. https://doi.org/10.1145/2025113.2025136
[9]Titus Barik, Yoonki Song, Brittany Johnson, and Emerson Murphy-Hill. 2016.
From quick fixes to slow fixes: Reimagining static analysis resolutions to enable
designspaceexploration.In ProceedingsoftheInternationalConferenceonSoftware
Maintenance and Evolution (ICSME). Raleigh, NC, USA, 211‚Äì221. https://doi.org/
10.1109/ICSME.2016.63
[10]Ivan Beschastnikh, Jenny Abrahamson, Yuriy Brun, and Michael D. Ernst. 2011.
Synoptic: Studying logged behavior with inferred models. In European Software
Engineering Conference and ACM SIGSOFT International Symposium on Foun-
dations of Software Engineering (ESEC/FSE) Demonstration track (5‚Äì9). Szeged,
Hungary, 448‚Äì451. https://doi.org/10.1145/2025113.2025188
[11]IvanBeschastnikh,YuriyBrun,JennyAbrahamson,MichaelD.Ernst,andArvind
Krishnamurthy.2013. UnifyingFSM-inferencealgorithmsthroughdeclarative
specification. In ACM/IEEE International Conference on Software Engineering
(ICSE)(22‚Äì24).SanFrancisco,CA,USA,252‚Äì261. https://doi.org/10.1109/ICSE.
2013.6606571
[12]Ivan Beschastnikh, Yuriy Brun, Jenny Abrahamson, Michael D. Ernst, and
Arvind Krishnamurthy. 2015. Using declarative specification to improve the
understanding, extensibility, and comparison of model-inference algorithms.IEEE Transactions on Software Engineering (TSE) 41, 4 (April 2015), 408‚Äì428.
https://doi.org/10.1109/TSE.2014.2369047
[13]Ivan Beschastnikh, Yuriy Brun, Michael D. Ernst, Arvind Krishnamurthy, and
ThomasE.Anderson. 2011. Miningtemporal invariantsfrompartiallyordered
logs.ACM SIGOPS Operating Systems Review 45, 3 (Dec. 2011), 39‚Äì46. https:
//doi.org/10.1145/2094091.2094101
[14]Ivan Beschastnikh, Yuriy Brun, Sigurd Schneider, Michael Sloan, and Michael D.
Ernst.2011. Leveragingexistinginstrumentationtoautomaticallyinferinvariant-
constrained models. In European Software Engineering Conference and ACM SIG-
SOFTInternationalSymposiumonFoundationsofSoftwareEngineering(ESEC/FSE)
(5‚Äì9). Szeged, Hungary, 267‚Äì277. https://doi.org/10.1145/2025113.2025151
[15]AriannaBlasi,AlbertoGoffi,KonstantinKuznetsov,AlessandraGorla,MichaelD.
Ernst, Mauro Pezz√®, and Sergio Delgado Castellanos. 2018. Translating code
commentstoprocedurespecifications.In InternationalSymposiumonSoftware
Testing and Analysis (ISSTA). Amsterdam, Netherlands, 242‚Äì253. https://doi.org/
10.1145/3213846.3213872
[16]Chad Brubaker, Suman Jana, Baishakhi Ray, Sarfraz Khurshid, and Vitaly
Shmatikov. 2014. Using frankencerts for automated adversarial testing of certifi-
cate validation in SSL/TLS implementations. In IEEE Symposium on Security and
Privacy (S&P). San Jose, CA, USA, 114‚Äì129. https://doi.org/10.1109/SP.2014.15
[17]RobertBrummayerandArminBiere.2009. Fuzzinganddelta-debuggingSMT
solvers.In InternationalWorkshoponSatisfiabilityModuloTheories(SMT).Mon-
treal, QC, Canada, 1‚Äì5. https://doi.org/10.1145/1670412.1670413[18]Jos√© Campos, Rui Abreu, Gordon Fraser, and Marcelo d‚ÄôAmorim. 2013. Entropy-
based test generation for improved fault localization. In IEEE/ACM International
Conference on Automated Software Engineering (ASE). Silicon Valley, CA, USA,
257‚Äì267. https://doi.org/10.1109/ASE.2013.6693085
[19]YutingChenandZhendongSu.2015. Guideddifferentialtestingofcertificatevali-dationinSSL/TLSImplementations.In EuropeanSoftwareEngineeringConference
and ACM SIGSOFT International Symposium on Foundations of Software Engineer-
ing(ESEC/FSE).Bergamo,Italy,793‚Äì804. https://doi.org/10.1145/2786805.2786835
[20]ValentinDallmeier,ChristianLindig,andAndreasZeller.2005.Lightweightdefect
localization for Java. In European Conference on Object Oriented Programming
(ECOOP). Glasgow, UK, 528‚Äì550. https://doi.org/10.1007/11531142_23
[21]HigorAmariodeSouza,MarcosLordelloChaim,andFabioKon.2016. Spectrum-
based software fault localization: A survey of techniques, advances, and chal-
lenges.CoRRabs/1607.04347 (2016), 1‚Äì46. http://arxiv.org/abs/1607.04347
[22]Matthew B.Dwyer, GeorgeS. Avrunin,and JamesC. Corbett.1999. Patternsin
property specifications for finite-state verification. In ACM/IEEE International
ConferenceonSoftwareEngineering(ICSE).LosAngeles,CA,USA,411‚Äì420. https:
//doi.org/10.1145/302405.302672
[23]MichaelD.Ernst,JakeCockrell,WilliamG.Griswold,andDavidNotkin.2001.
Dynamicallydiscoveringlikelyprograminvariantstosupportprogramevolution.
IEEE Transactions on Software Engineering (TSE) 27, 2 (2001), 99‚Äì123. https:
//doi.org/10.1145/302405.302467
[24]RobertB.EvansandAlbertoSavoia.2007. Differentialtesting: Anewapproachtochangedetection.In EuropeanSoftwareEngineeringConferenceandACMSIGSOFT
InternationalSymposiumonFoundationsofSoftwareEngineering(ESEC/FSE)Poster
track. Dubrovnik, Croatia, 549‚Äì552. https://doi.org/10.1145/1295014.1295038
[25]Laura Faulkner. 2003. Beyond the five-user assumption: Benefits of increasedsample sizes in usability testing. Behavior Research Methods, Instruments, &
Computers 35, 3 (2003), 379‚Äì383. https://doi.org/10.3758/BF03195514
[26]Gordon Fraser and Andrea Arcuri. 2013. Whole test suite generation. IEEE
TransactionsonSoftwareEngineering(TSE) 39,2(February2013),276‚Äì291. https:
//doi.org/10.1109/TSE.2012.14
[27]Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness test-ing: Testing software for discrimination. In European Software Engineering
ConferenceandACMSIGSOFTInternationalSymposiumonFoundationsofSoft-
ware Engineering (ESEC/FSE) (6‚Äì8). Paderborn, Germany, 498‚Äì510. https:
//doi.org/10.1145/3106237.3106277
[28]Carlo Ghezzi, Mauro Pezz√®, Michele Sama, and Giordano Tamburrelli. 2014.Mining behavior models from user-intensive web applications. In ACM/IEEE
InternationalConferenceonSoftwareEngineering(ICSE).Hyderabad,India,277‚Äì
287.https://doi.org/10.1145/2568225.2568234
[29]Patrice Godefroid. 2007. Random testing for security: Blackbox vs. whiteboxfuzzing. In International Workshop on Random Testing (RT). Minneapolis, MN,
USA, 1.https://doi.org/10.1145/1292414.1292416
[30]Patrice Godefroid, Michael Y. Levin, and David A. Molnar. 2008. Automated
whitebox fuzz testing. In Network and Distributed System Security Symposium
(NDSS). San Diego, CA, USA, 151‚Äì166.
[31]Alberto Goffi, Alessandra Gorla, Michael D. Ernst, and Mauro Pezz√®. 2016. Auto-
matic generation of oracles for exceptional behaviors. In International Sympo-
siumon SoftwareTestingandAnalysis(ISSTA).Saarbr√ºcken, Genmany,213‚Äì224.
https://doi.org/10.1145/2931037.2931061
[32]Dan Hao, Ying Pan, Lu Zhang, Wei Zhao, Hong Mei, and Jiasu Sun. 2005. A
similarity-aware approach to testing based fault localization. In IEEE/ACM Inter-
nationalConferenceonAutomatedSoftwareEngineering(ASE).LongBeach,CA,
USA, 291‚Äì294. https://doi.org/10.1145/1101908.1101953
[33]Dan Hao, Lu Zhang, Hao Zhong, Hong Mei, and Jiasu Sun. 2005. Eliminating
harmfulredundancyfortesting-basedfaultlocalizationusingtestsuitereduction:
Anexperimentalstudy.In IEEEInternationalConferenceonSoftwareMaintenance
(ICSM). Budapest, Hungary, 683‚Äì686. https://doi.org/10.1109/ICSM.2005.43
[34]RalfHildebrandtandAndreasZeller.2000. Simplifyingfailure-inducinginput.In
International Symposium on Software Testing and Analysis (ISSTA). Portland, OR,
USA, 135‚Äì145. https://doi.org/10.1145/347324.348938
[35]Christian Holler, Kim Herzig, and Andreas Zeller. 2012. Fuzzing with code
fragments. In USENIX Security Symposium. Bellevue, WA, USA, 445‚Äì458.
[36] InTrace 2018. InTrace. https://mchr3k.github.io/org.intrace/.
[37]Wei Jin and Alessandro Orso. 2012. BugRedux: Reproducing field failures for in-
house debugging. In ACM/IEEE International Conference on Software Engineering
(ICSE). Zurich, Switzerland, 474‚Äì484. https://doi.org/10.1109/ICSE.2012.6227168
[38]WeiJin,AlessandroOrso,andTaoXie.2010. Automatedbehavioralregression
testing.In InternationalConferenceonSoftwareTesting,Verification,andValidation
(ICST). Paris, France, 137‚Äì146. https://doi.org/10.1109/ICST.2010.64
[39]BrittanyJohnson,RahulPandita,JustinSmith,DenaeFord,SarahElder,Emer-
son Murphy-Hill, Sarah Heckman, and Caitlin Sadowski. 2016. A cross-tool
communicationstudy onprogram analysistool notifications.In ACM SIGSOFT
InternationalSymposiumonFoundationsofSoftwareEngineering(FSE).Seattle,
WA, USA, 73‚Äì84. https://doi.org/10.1145/2950290.2950304
[40]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. Whydon‚Äôtsoftwaredevelopersusestaticanalysistoolstofindbugs? In
98Proceedings of the 2013 International Conference on Software Engineering. San
Fransisco, CA, USA, 672‚Äì681. https://doi.org/10.1109/ICSE.2013.6606613
[41] James A. Jones, Mary Jean Harrold, and John Stasko. 2002. Visualization of test
informationtoassistfaultlocalization.In InternationalConferenceonSoftware
Engineering(ICSE).Orlando,FL,USA,467‚Äì477. https://doi.org/10.1145/581339.
581397
[42]Jaeyeon Jung, AnmolSheth, Ben Greenstein, David Wetherall, GabrielMaganis,
and Tadayoshi Kohno. 2008. Privacy oracle: A system for finding application
leaks with black box differential testing. In ACM Conference on Computer and
Communications Security (CCS). Alexandria, VA, USA, 279‚Äì288. https://doi.org/
10.1145/1455770.1455806
[43]Ren√© Just. 2014. The Major mutation framework: Efficient and scalable mutation
analysisforJava.In InternationalSymposiumonSoftwareTestingandAnalysis
(ISSTA). San Jose, CA, USA, 433‚Äì436. https://doi.org/10.1145/2610384.2628053
[44]Ren√© Just, Michael D. Ernst, and Gordon Fraser. 2014. Efficient mutation anal-
ysisbypropagatingandpartitioninginfectedexecutionstates.In International
SymposiumonSoftwareTestingandAnalysis(ISSTA).SanJose,CA,USA,315‚Äì326.
https://doi.org/10.1145/2610384.2610388
[45]Ren√©Just,Darioush Jalali,andMichaelD.Ernst. 2014. Defects4J: Adatabaseof
existing faults to enable controlled testing studies for Java programs. In Proceed-
ings of the International Symposium on Software Testing and Analysis (ISSTA). San
Jose, CA, USA, 437‚Äì440. https://doi.org/10.1145/2610384.2628055
[46]Ivo Krka, Yuriy Brun, and Nenad Medvidovic. 2014. Automatic mining of specifi-
cations from invocation traces and method invariants. In ACM SIGSOFT Interna-
tional Symposium on Foundations of Software Engineering (FSE) (16‚Äì22). Hong
Kong, China, 178‚Äì189. https://doi.org/10.1145/2635868.2635890
[47]Tim Menzies, JeremyGreenwald, and Art Frank.2007. Data miningstatic code
attributestolearndefectpredictors. IEEETransactionsonSoftwareEngineering
33, 1 (January 2007), 2‚Äì13. https://doi.org/10.1109/TSE.2007.10
[48]TimMenzies,ZachMilton,BurakTurhan,BojanCukic,YueJiang,andAy≈üBener.
2010. Defect prediction from static code features: Current results, limitations,new approaches. Automated Software Engineering 17, 4 (May 2010), 375‚Äì407.
https://doi.org/10.1007/s10515-010-0069-5
[49]ManishMotwaniandYuriyBrun.2019. Automaticallygeneratingpreciseoracles
from structured natural language specifications. In ACM/IEEE International Con-
ference on Software Engineering (ICSE) (29‚Äì31). Montreal, QC, Canada, 188‚Äì199.
https://doi.org/10.1109/ICSE.2019.00035
[50]Kƒ±van√ß Mu≈ülu, Yuriy Brun, Michael D. Ernst, and David Notkin. 2015. Reducing
feedback delay of software development tools via continuous analyses. IEEE
Transactions on Software Engineering (TSE) 41, 8 (August 2015), 745‚Äì763. https:
//doi.org/10.1109/TSE.2015.2417161
[51]Kƒ±van√ß Mu≈ülu, Yuriy Brun, and Alexandra Meliou. 2013. Data debugging
with continuous testing. In European Software Engineering Conference and ACM
SIGSOFT International Symposium on Foundations of Software Engineering (ES-EC/FSE) New Ideas track (18‚Äì26). Saint Petersburg, Russia, 631‚Äì634. https:
//doi.org/10.1145/2491411.2494580
[52]Kƒ±van√ßMu≈ülu,YuriyBrun,andAlexandraMeliou.2015. Preventingdataerrors
with continuous testing. In ACM SIGSOFT International Symposium on Software
Testing and Analysis (ISSTA) (12‚Äì17). Baltimore, MD, USA, 373‚Äì384. https:
//doi.org/10.1145/2771783.2771792
[53]Tony Ohmann, Michael Herzberg, Sebastian Fiss, Armand Halbert, Marc Palyart,
IvanBeschastnikh,andYuriyBrun.2014. Behavioralresource-awaremodelinfer-
ence.InIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering
(ASE)(15‚Äì19).V√§ster√•s,Sweden,19‚Äì30. https://doi.org/10.1145/2642937.2642988
[54]Alessandro Orso, Nanjuan Shi, and Mary Jean Harrold. 2004. Scaling regression
testing to large software systems. In ACM SIGSOFT International Symposium on
FoundationsofSoftwareEngineering(FSE).NewportBeach, CA,USA,241‚Äì252.
https://doi.org/10.1145/1029894.1029928
[55]CarlosPachecoandMichaelD.Ernst.2007. Randoop: Feedback-directedrandom
testingforJava.In ConferenceonObject-orientedProgrammingSystemsandAp-
plications(OOPSLA).Montreal,QC,Canada,815‚Äì816. https://doi.org/10.1145/
1297846.1297902
[56]Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques
actually helping programmers? In International Symposium onSoftware Testing
andAnalysis(ISSTA).Toronto,ON,Canada,199‚Äì209. https://doi.org/10.1145/
2001420.2001445
[57]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Au-tomated whitebox testing of deep learning systems. In ACM Symposium on
OperatingSystemsPrinciples(SOSP).Shanghai,China,1‚Äì18. https://doi.org/10.
1145/3132747.3132785[58]DaweiQi,AbhikRoychoudhury,ZhenkaiLiang,andKapilVaswani.2012.Darwin:
An approach to debugging evolving programs. ACM Transactions on Software
Engineering and Methodology (TOSEM) 21, 3 (2012), 19:1‚Äì19:29. https://doi.org/
10.1145/2211616.2211622
[59]Cynthia K. Riemenschneider, Bill C. Hardgrave, and Fred D. Davis. 2002. Ex-
plaining software developer acceptance of methodologies: A comparison of five
theoreticalmodels. IEEETransactionsonSoftwareEngineering(TSE) 28,12(2002),
1135‚Äì1145. https://doi.org/10.1109/TSE.2002.1158287
[60]Jeremias R√∂√üler, Gordon Fraser, Andreas Zeller, and Alessandro Orso. 2012.
Isolating failure causes through test case generation. In International Symposium
onSoftwareTestingandAnalysis(ISSTA).Minneapolis,MN,USA,309‚Äì319. https:
//doi.org/10.1145/2338965.2336790
[61]VipinSamarandSangeetaPatni.2017.Differentialtestingforvariationalanalyses:
ExperiencefromdevelopingKConfigReader. CoRRabs/1706.09357(2017),1‚Äì18.
http://arxiv.org/abs/1706.09357
[62]Justin Smith, Brittany Johnson, Emerson Murphy-Hill, Bill Chu, andHeather Richter Lipford. 2015. Questions developers ask while diagnosingpotential security vulnerabilities with static analysis. In European Software
Engineering Conference and ACM SIGSOFT International Symposium on Foun-dations of Software Engineering (ESEC/FSE). Bergamo, Italy, 248‚Äì259. https:
//doi.org/10.1145/2786805.2786812
[63]Varun Srivastava, Michael D. Bond, Kathryn S. McKinley, and Vitaly Shmatikov.
2011. A security policy oracle: Detecting security holes using multiple API
implementations.In ACMSIGPLANConferenceonProgrammingLanguageDesign
andImplementation(PLDI).SanJose,CA,USA,343‚Äì354. https://doi.org/10.1145/
1993498.1993539
[64]Roykrong Sukkerd, Ivan Beschastnikh, Jochen Wuttke, Sai Zhang, and YuriyBrun. 2013. Understanding regression failures through test-passing and test-failing code changes. In International Conference on Software Engineering New
IdeasandEmergingResultsTrack(ICSENIER) (22‚Äì24).SanFrancisco,CA,USA,
1177‚Äì1180. https://doi.org/10.1109/ICSE.2013.6606672
[65]Shin Hwei Tan, Darko Marinov, Lin Tan, and Gary T. Leavens. 2012. @tCom-ment: Testing Javadoc comments to detect comment-code inconsistencies. In
InternationalConferenceonSoftwareTesting,Verification,andValidation(ICST).
Montreal, QC, Canada, 260‚Äì269. https://doi.org/10.1109/ICST.2012.106
[66]Yuchi Tianand, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest:
Automatedtestingofdeep-neural-network-drivenautonomouscars.In ACM/IEEE
International Conference on Software Engineering (ICSE). Gothenburg, Sweden,
303‚Äì314. https://doi.org/10.1145/3180155.3180220
[67]RobertJ.Walls,YuriyBrun,MarcLiberatore,andBrianNeilLevine.2015. Dis-
coveringspecificationviolationsinnetworkedsoftwaresystems.In International
Symposium on Software Reliability Engineering (ISSRE) (2‚Äì5). Gaithersburg, MD,
USA, 496‚Äì506. https://doi.org/10.1109/ISSRE.2015.7381842
[68]Kaiyuan Wang, Chenguang Zhu, Ahmet Celik, Jongwook Kim, Don Batory, and
Milos Gligoric. 2018. Towards refactoring-aware regression test selection. In
ACM/IEEE International Conference on Software Engineering (ICSE). Gothenburg,
Sweden, 233‚Äì244. https://doi.org/10.1145/3180155.3180254
[69]QianqianWang,YuriyBrun,andAlessandroOrso.2017. Behavioralexecution
comparison: Aretestsrepresentativeoffieldbehavior? In InternationalConference
onSoftwareTesting,Verification,andValidation(ICST) (13‚Äì18).Tokyo,Japan,321‚Äì
332.https://doi.org/10.1109/ICST.2017.36
[70]W. Eric Wong, Vidroha Debroy, and Byoungju Choi. 2010. A family of code
coverage-based heuristics for effective fault localization. Journal of Systems and
Software (JSS) 83, 2 (2010), 188‚Äì208. https://doi.org/10.1016/j.jss.2009.09.037
[71]JamesWoodward.2005. Makingthingshappen: Atheoryofcausalexplanation.
Oxford University Press.
[72]Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and un-derstanding bugs in C compilers. In ACM SIGPLAN Conference on Program-
mingLanguageDesignandImplementation(PLDI).SanJose,CA,USA,283‚Äì294.
https://doi.org/10.1145/1993498.1993532
[73]Andreas Zeller. 1999. Yesterday, my program worked. Today, it does not. Why?
InEuropeanSoftwareEngineeringConferenceandACMSIGSOFTSymposiumon
theFoundationsofSoftwareEngineering(ESEC/FSE).Toulouse,France,253‚Äì267.
https://doi.org/10.1145/318773.318946
[74]Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-
inducing input. IEEE Transactions on Software Engineering 28, 2 (February 2002),
183‚Äì200. https://doi.org/10.1109/32.988498
[75]Daniele Zuddas, Wei Jin, Fabrizio Pastore, Leonardo Mariani, and Alessandro
Orso.2014. MIMIC:Locatingandunderstandingbugsbyanalyzingmimicked
executions. In ACM/IEEE International Conference on Software Engineering (ICSE).
Hyderabad, India, 815‚Äì826. https://doi.org/10.1145/2642937.2643014
99