Plug the Database & Play With Automatic Testing:
Improving System Testing by Exploiting Persistent Data
Diego Clerissi
diego.clerissi@unimib.it
University of Milano - Bicocca
Milano, ItalyGiovanni Denaro
giovanni.denaro@unimib.it
University of Milano - Bicocca
Milano, Italy
Marco Mobilio
marco.mobilio@unimib.it
University of Milano - Bicocca
Milano, ItalyLeonardo Mariani
leonardo.mariani@unimib.it
University of Milano - Bicocca
Milano, Italy
ABSTRACT
A key challenge in automatic Web testing is the generation of
syntactically and semantically valid input values that can exercise
the many functionalities that impose constraints on the validity of
the inputs. Existing test case generation techniques either rely on
manuallycuratedcatalogsofvalues,orextractvaluesfromexternal
data sources, such as the Web or publicly available knowledge
bases. Unfortunately, relying on manual effort is generally tooexpensive for most practical applications, while domain-specific
andapplication-specificdatacanbehardlyfoundeitherontheWeb
or in general purpose knowledge bases.
This paper proposes DBInputs, a novel approach that reuses
thedatafromthedatabaseofthetargetWebapplication,toauto-
matically identify domain-specific and application-specific inputs,
andeffectivelyfulfillthevalidityconstraintspresentinthetested
Webpages.DBInputscanproperlycopewithsystemtestingand
maintenancetestingefforts,sincedatabasesarenaturallyandin-
expensively available in those phases. To extract valid inputs from
the application databases, DBInputs exploits the syntactic and se-
mantic similarity between the identifiers of the input fields and
the ones inthe tables of thedatabase, automatically resolving the
mismatchbetweentheuserinterfaceandtheschemaofthedata-
base. Our experiments provide initial evidence that DBInputs can
outperform both random input selection and Link, a competing
approach for searching inputs from knowledge bases.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging.
KEYWORDS
System testing, Web testing, Test inputs, Test generation.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ‚Äô20, September 21‚Äì25, 2020, Virtual Event, Australia
¬© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6768-4/20/09...$15.00
https://doi.org/10.1145/3324884.3416561ACM Reference Format:
Diego Clerissi, Giovanni Denaro, Marco Mobilio, and Leonardo Mariani.
2020. Plug the Database & Play With Automatic Testing: Improving Sys-
temTesting byExploitingPersistentData. In 35thIEEE/ACMInternational
Conference on Automated Software Engineering (ASE ‚Äô20), September 21‚Äì25, 2020, Virtual Event, Australia. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3324884.3416561
1 INTRODUCTION
Automatic Web testingaims atgenerating testcases thatexercise a
Web application under test using its user interface [1, 7, 12, 15, 19,
20]. In this context, a crucial challenge is how to effectively assign
theinputfieldsthatacceptfreetext,whicharecommonlypartof
the design of many Web pages with input values. The difficulty
of generating suitable input values for these free-text fields is a
well-known barrier to many automatic Web testing techniques,
which may consequently fail to thoroughly test some functionality
ormayevenoverlookentireareasoftheuserinterfacethatdepend
on missed input values.
Consider, for instance, the simple case of Web applications that
require the users to sign up before using any other functionality.Failing to sign up due to the inability to enter valid inputs in the
sign-upforms(e.g.,avalidusernameoremailaddress)willseverely
penalize the effectiveness of the test generator. Similarly, a Web
applicationforhandlinguserinsurancerecordsmayremainlargely
untestedifthetestgeneratorisunabletofulfilltherequirements
on the inputs to file any valid insurance record.
To achieve adequate testing, the test generator must be able
to exercise the target operations, such as the sign-up operationexemplified above, with both erroneous and valid inputs. While
producing erroneous inputs can be relatively easy, such as, generat-
ing a string that does not correspond to any valid email address or
socialsecuritynumber,generating validinputs,likeavalidemail
address or social security number, can be particularly challenging.
SomeautomaticWebtestingtechniquescircumventthisproblem
by allowing the testers to define a catalog of input values that test
generators can exploit for input selection [ 7,15,18‚Äì20]. Although
thisisapossibleoption,itisarguablyunattractive,sinceitshiftsthe
burdenofsolvingtheproblemtothemanualeffortofthetesters,
and thus lowers the degree of automation of the techniques. It
canalsobeextremelyexpensiveinthemanycaseswhereitdoes
not suffice to define just a bunch of values to be used everywhere,
662020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)
but rather testers should define ad-hoc catalogs of values for the
individual fields of many Web pages.
Thevalidityoftestinputsmaydependonsyntactic,semanticand
coherence constraints [ 4,16,17,25].Syntactic constraints concern
withtheformatofthestringsfilledintheinputfields.Forinstance,
an input field of a Web application may require dates expressed in
the format ‚Äúmm-dd-yyyy", and reject inputs in different formats,
suchas‚Äú11/03/2020"or‚Äú11-03-20". Semanticconstraints predicate
onthemeaningoftheinputvalues.Forinstance,aninputfieldmay
require a street address that truly exists because the application
aims to show the address on a map, rejecting any fake address
even if syntactically valid. Coherence constraints predicate on a set
of inputs that must be coherent with each other, beyond being
individuallyvalid.Forinstance,aWebpagemayletusersconfigureseveralpropertiesofaproducttopurchase,butletthemfinalizethe
purchasesonlyifthecombinationofthepropertiescomplywith
given business rules.
Allthesolutionsproposedsofarinvestigatedtheautomaticgen-
eration of valid inputs by integrating the test generator with other
servicesthatmaysupporttheretrievalofvalidinputvaluesfrom
external,publiclyavailablesources.BozkurtandHarmanproposea
technique that extracts values from Web Services that can act as
dataprovidersbasedontheanalysisoftheirAPIs[ 4],McMinnetal.
investigatetheeffectivenessofretrievingvaluesbyusingsearchen-
gines [17]. Mariani et al. exploits the Web of Data to automatically
obtain coherent sets of values [16].
A common drawback of these approaches is that they often fail
to retrieve valid values for inputs that depend on domain-specific
concepts or application-specific data. Domain-specific concepts, in-
cluding non-standard or even jargonic terminology, synonyms and
acronyms,caneasilybepartoftheidentifiersoftheinputfieldsina
Webapplication.Theygenerallywidenuptolargeextentthesyntac-
tic and semantic mismatch between the keywords searched by the
test generators and the keywords used in the external data sources,
severelyinhibitingtheidentificationofvalidinputs.Forinstance,
inourexperimentsweconsideredabudgetmanagementWebappli-
cation, and tried to generate test inputs using Link [ 16,28], which
exploits the Web of Data to obtain input values. Since the Webof Data does not cover well the budget management domain, es-pecially the budget-related terms used in the application undertest, Link consistently failed in generating valid inputs for that
application.
Application-specific data, that is, values that directly depend on
thestateoftheapplicationundertest,areanothercommoncauseof
missed inputs.For instance, anoperation may requirethe number
ofapreviouslyfiled insurancecontract,whichcan hardly befound
in any external data source.
In this paper, we propose DBInputs, a novel technique for auto-
maticWebtestingthat identifiesthetestinputsbydirectlyexploiting
the database of the Web application under test. Our approach lever-
agesontwokeyobservations:(i)Applicationdatabasesarenatively
craftedwithapplication-specificdataandnativelydesignedwith
domain-specific concepts, and thus they can cope well with theabove discussed open issues of the current techniques; (ii) Au-tomatic Web testing addresses system testing and maintenance
testingphases,andthuswecangenerallycountontheexistenceAPP DATABASE
(working copy)
APP DATABASETest Generator
Web-app under test
1. Compute 
similarities
2. Identify
top matches
per input field
3. Select best
matches and 
return dataDBInputsexecute
actions
query
for inputs
Figure 1: Automatic Web testing with DBInputs.
of test databases, or even production databases, without requiring
additional effort from the testers.
DBInputsiscapableofautomaticallymatchingtheidentifiersof
theinputfieldsofthetargetWebapplicationwiththeidentifiersof
the columns in the tables of the available database, exploiting both
the syntactic and the semantic similarity between the identifiers.
Thedatabaselikelycontainsvaliddataformostoperationsofthe
application,thusmakingDBInputsfeasible,effectiveanduseful.
Weliketorefertothispracticalapproachasa plug&play solution,
sincetestersshallonlyspecifytheconnectiontotheavailabledata-
base, and then DBInputs can pick useful data straightaway, taking
itself care of the possible naming mismatches.
WeevaluatedDBInputsbyequippingtheABTsystemtestgener-
ator[15]withthecapabilitytoidentifytestinputsby(i)generating
random values, (ii) using the competing technique Link [ 16,28]
thatexploitstheWebofDatatoidentifyvalidinputs,and(iii)using
DBInputs. The results reported in this paper provide empirical
evidence thatthe testcases generatedwith DBInputs explore the
functionalityoftheapplicationundertestmoreextensivelythan
the competingapproaches: DBInputs almost doubledthe number
of forms exercised with validinputs, still comparably exercising
the forms with invalidinputs too.
DBInputsisopen-sourceandavailable,alongwithalltheexper-
imental material and results reported in this paper, athttps://gitlab.com/DBInputs/dbinputs.
The paper is organized as follows. Section 2 provides a quick
overview of DBInputs. Section 3 rigorously describes the DBIn-
puts algorithm. Section 4 reports experimental results about DBIn-
puts and compares DBInputs to competing approaches. Section 5
discusses related work. Finally, Section 6 provides final remarks.
2 OVERVIEW OF DBINPUTS
DBInputs generates test inputs for a Web application under test
inafullyautomaticfashion byexploitingthedataavailableinthe
database of the application under test. In this section we introduce
thekeyintuitionsbehindDBInputsbyreferringtoasetofworkingexamples,whilewerigorouslypresentthealgorithmsthatdescribe
DBInputs in the next section.
Figure1illustrateshowDBInputsintegrateswithautomaticWeb
testing. The test generator queries DBInputs whenever it needs to
67
	
	

	

	

			
	
$'$

	!   
    $+&
$+&
! $))
$(* 
 
 $%(
$'+
$))!
$)'
Figure 2: Results of DBInputs for a sample Web form.
enter values in free-text input fields. To answer the queries, DBIn-
puts refers to a working copy of the application database that is
created the first time the testers configure the connection of DBIn-
putswithagivendatabase. Theworkingcopyclonestheschema
oftheapplicationdatabaseandimportsasubsetoftherecordsin
eachtable.Theexactlyimportedrecordsmaydependondifferent
strategies,asdescribedandexperimentedinSection4. DBInputs
just needs to connect to the working copy and use its data, regard-
less of the changes that the actual database may undergo while
thetestgeneratorexecutestheapplicationundertest.Noticethat
DBInputs isnot constrainedto anydatabaseformat andsupports
any SQL-based database.
To answer each query, DBInputs goes through three steps:
(i)First,itcomputesboth syntacticandsemanticsimilarities between
the identifiers of the input fields and the names of the database
columns;(ii)Then,itselectsthe setofcolumns thatprovidesbest
matchesforeachinputfield,byexploitingthesimilarityscoreswith
aclusteringalgorithm;(iii)Finally,itchoosesa singlebestmatch-
ingcolumn foreachinputfieldbypursuingthetradeoffbetween
(a) achieving high mean similarity of the matches and (b) limiting
theamountofdatabasetablestowhichthematchingcolumnsrefer
to.
Figure 2 exemplifies the three steps of DBInputs with refer-
encetotheregistrationformof MantisBugTracker (https://www.
mantisbt.org/), a Web application that we considered in our experi-
ments, and the corresponding database available online.
In the first step, DBInputs quantifies the similarity between the
identifiers of the input fields in the Web form and the names of the
database columns. DBInputs considers three identifiers for each
input field: (i) its label, which is the text that is visually associated
withtheinputfieldintheWebpage(e.g.,thetextonitsleftside),
identified with an adapted version of the algorithm by Becce et
al.[2],(ii)thevalueofthe id-propertyobtainedfromtheDocument
Object Model (DOM) of the page, and (iii) the value of the name-
property, still obtained from the DOM of the page.
Forinstance,thethreeidentifiersofthefirstinputfieldinFig-
ur e2ar e Username (label), user-name (id),and username (name).
The final similarity between an input field and a database column
is given by the average of the similarities computed from its identi-
fiers.Aswedescribeinfurtherdetailinthenextsection,DBInputs
computes similarity scores between identifiers and column names
combiningasyntacticdistance,namelythe editdistance [22],witha
semanticdistance,namelythe word2vecmodel [23].Theunderlying
intuitionisthatthematchingbetweentheidentifiersshouldbothbe
tolerant toirrelevant syntactic differences, and take thesemantics of
theidentifiersintoaccountregardlessoftheirsyntacticrepresentation.
In Figure2, thevalues associatedwith blue,red andgreen arrows
indicatethesimilarityscoresthat,inourexperiments,DBInputs
computedforthethreeinputfieldswithrespecttosomecolumns
inthreedatabasetables,namely, LOGIN_DATA ,USERSandEMAILS.
Forinstance,DBInputsmatchedalmostperfectly(similarity0.94)
theinputfield Username withthehomonymousdatabase column
username , but it matched very well (similarity 0.74) also the in-
put fieldE-mailwith the database column email, despite the slight
syntactic difference between the corresponding identifiers. Yet, for
another Web form of the application, it matched well (similarity
0.6) the input field Current issue with the syntactically different,
butsemanticallysimilardatabasecolumn Bug id.Thisflexibility
allowsDBInputstoretrievemeaningfulinputvaluesdespitethe
possible differences between the identifiers used in the application
andthedatabase.Differencesintheidentifiersareoftenintroduced
by design decisions taken at different times (e.g., by different devel-opers)ortakenwithdifferentgoalsinmind(e.g.,GUIlabelschosen
to favor user-friendliness, and column names chosen to comply
with naming conventions of data models).
In the second step, DBInputs identifies a set of ‚Äúgood-enough‚Äù
matchingcolumnsforeachinputfield.Todothis,itclustersthesimi-larityscorescomputedforeachinputfieldaccordingtotheirmutual
strengths (based on the ST-DBSCAN clustering method [ 24]), and
then selects the columns that correspond to the top cluster, that is,
theclusterthatincludesthebestsimilarityscores.Thisclustering
step allows DBInputs to focus on the most promising matching
columnsonly,whilestillkeepingthechoiceopenfortheinputfields
that can match multiple columns with different but comparable
similarities. Figure 2 highlights the similarity values that belong to
the same top cluster with a same colored background. There is a
single matching column in the top cluster for the field Real Name ,
while there are two and three candidates for the fields Username
andE-mail, respectively.
68Inthethirdstep,DBInputschoosesafinalsolution,consisting
ofasinglematchingcolumnforeveryinputfield,bypursuingboth
the goal of choosing matches with high similarity, and the goal
of limiting as much as possible the number of involved database
tables. The intuition that underlies this latter goal is that by select-
ingmultiple inputvalues fromthesame databasetable, weincrease
the likelihood of finding values that are coherent with each other.
Since the input fields that belong to the same Web pages are often
correlated (e.g., they represent information about the same domain
entities),DBInputsembracestheideathattheinputdatashouldbe
selected from a limited set of tables (since the information about a
domain entity is usually stored in a single, or few database tables).
To this end, DBInputs first identifies the solutions (a set of
matchesbetweentheinputfieldsandthedatabasecolumnswith
each input field occurring exactly once) with the highest mean
similarityscoresofthematches,usingagainclustering.Amongthe
solutions in the top cluster, DBInputs selects the one that involves
theminimumnumberoftables.ForexampleinFigure2,DBInputs
finally chooses to match the three input fields with three columns
of table USERS(solid edges indicate the finally selected matches).
By querying the best three columns of table USERSfor ran-
domlyselectedrecords,DBInputscouldfinallyreturntheinputs
that correspond to the username, real name, and email data ofa true user tracked in the database, e.g., john.doe, John Doe and
john.doe@provider.com.
3 AUTOMATICALLY EXTRACTING TEST
INPUTS FROM A DATABASE
This section presents DBInputs in detail. The pseudocode in Al-
gorithm 1 defines the inputs and the output of DBInputs, and
describes the organization of the algorithm in top-down fashion.
Algorithm 1 DBInputs: Map each input field to a database
column
Input:
1:F‚â°{f1...fn}: the input fields
2:T‚â°{t1...tm}: the tables in the database
Output:
3:M:F‚Üícols(T): a map that associates each input field with a
database column, according to our best-match heuristics
4:function DBInputs(F ,T)
5:if‚àÉM|/angbracketleftF,M/angbracketright‚ààCachethen return M
6:MM,S‚ÜêColumnsWithTopSimilarity(F ,T)‚äø¬ßAlg. 2
7:M‚ÜêIdentifyBestMatches(F ,T,MM,S)‚äø¬ßAlg. 3
8:Cache‚ÜêCache‚à™{/angbracketleftF,M/angbracketright}
9:returnM
DBInputs receives as input both the list of input fields in the
current Web page (input F) and the tables in the database (input
T), andreturns amap (output M) thatassociates eachfield witha
database column, according to the DBInputs best-match heuris-
tics.DBInputsusesacachetooptimizeperformancebyavoiding
multiplecomputationsofthemappingforthesamesetsofinput
fields(line5),andsplitsthecomputationintotwomacrosteps.Thefirststepidentifiesthedatabasecolumnsthatmatcheachfieldwith
comparably high similarity (function ColumnsWithTopSimilarityat line 6), while the second step computes the field-column map-
ping with the best tradeoff between high similarity scores and low
amountofinvolveddatabasetables(function IdentifyBestMatches at
line7).Theresultsof ColumnsWithTopSimilarity,i.e.,themulti-map
MMof the best columns for each field and the corresponding simi-
larity values S, are passed as inputs to IdentifyBestMatches. Each
new result is fed to the cache (line 8) to enable reuse at subsequent
queries,andthenreturnedastheresultofthecurrentquery(line9).
The next sections define the algorithms of ColumnsWithTopSim-
ilarityandIdentifyBestMatches, the core steps of DBInputs.
Algorithm2 Identifydatabasecolumnswithcomparablyhigh
similarity for each input field
Input:
1:F‚â°{f1...fn}: the input fields
2:T‚â°{t1...tm}: the tables in the database
Output:
3:MM:F‚Üí2cols(T): a multi-map that associates each input
field with the set of most similar columns
4:S:F√ócols(T)‚Üí(0,1):similarity measurement forall /angbracketleftfield,
column/angbracketrightpairs inMM
5:function ColumnsWithTopSimilarity(F, T )
6:MM‚Üê{ }
7:S‚Üê{ }
8:for each f‚ààFdo
9: SimC‚Üê{/angbracketleftc,Similarity( f,c)/angbracketright:c‚ààcols(T)}
10: Best C‚ÜêTopClusterDbscan( cols(T)clustered-by SimC)
11: MM‚ÜêMM‚à™{/angbracketleftf,Best C/angbracketright}
12: S‚ÜêS‚à™{/angbracketleft/angbracketleftf,c/angbracketright,simc/angbracketright:c‚ààBest C,/angbracketleftc,simc/angbracketright‚ààSimC}
13:returnMM,S
14:function Similarity(f, c )
15:tokens f‚ÜêLemmatizedTokens( id(f),name(f),label(f))
16:tokens c‚ÜêLemmatizedTokens(c.name)
17:for each ti‚ààtokens fdo
18: for each tj‚ààtokens cdo
19: synij‚ÜêNormalizedSyntacticSimilarity( ti,tj)
20: semij‚ÜêNormalizedSemanticSimilarity( ti,tj)
21: sij‚Üêmax(synij,semij)
22:return/summationtext.1
i,jsij
|tokens f|√ó|tokens c|
3.1 Finding Matches
Algorithm2 definesfunction ColumnsWithTopSimilarity thatiden-
tifiesthedatabasecolumnsthatmatcheachinputfieldwithcompa-
rably high similarity. It takes in input the set of fields Fand the set
of database tables T, and returns both a multi-map ( MM) that asso-
ciateseachinputfieldwiththesetofmostsimilardatabasecolumns,
and the corresponding similarity measurements (S ). The weighted
edgesshowninFigure 2exemplifytheresultof ColumnsWithTop-
Similarity. The algorithm first computes the similarity score foreach field-column pair (line 9), and then clusters the similarity
scores to identify the top scores for each field (line 10).
69ComputingSimilarityScores. Thisstepisfurtherdetailedinfunc-
tionSimilarity (Algorithm2,lines14‚Äì22).Itconsistsofdecomposing
theidentifiersofinputfieldsandcolumnsintotokensthatrepre-
senttheindependentwords(function LemmatizedTokens atlines15
and16),findingthebestsimilarityscoreofeachpairoffield-column
tokens(lines17‚Äì21)bychoosingthebestoftheireithersyntactic
or semantic similarity measurements, and averaging the similarity
scores across all pairs of tokens (line 22).
Forcomputingthesimilarity,DBInputscharacterizeseachinput
fieldf‚ààFasatriple /angbracketleftid,name,label/angbracketright,whereidandnamearethe
values of the id-property and name-property of the field, obtained
fromtheDOMoftheWebpage,and labelisthetextualcontentthat
theWebpagevisuallyassociateswiththeinputfield.Weidentify
thelabelsassociated with the input fields by analyzing the Web
pages using an adapted version of the algorithm of Becce et al. [ 2],
whichexploitsgoodpracticesinuser-interfacedesign(e.g.,itlooks
for labels located on the left or straight above the fields). More
specifically,DBInputsusesthevalueofthe placeholder attribute
of the input field as label. If such attribute has no value, DBInputs
uses the value of the <label> tag element that explicitly points at
the target input field (that is, the <label>element must include
aforattribute whose value is the identifier of the input field) as
label. If such anelement is not present, DBInputs uses the text in
theclosest <label> elementthatprecedesorfollowstheinputfield
asvalueofthelabel.Ifsuchanelementdoesnotexist,DBInputs
usesthetextintheheaderorfooteroftheHTMLcellthatcontains
theinputfield,ifany.Ifnoneoftheseoptionssucceeds,theinput
field is associated with no label.
Figure 2 shows the id,name, andlabelproperties for each input
field of the Web form in the figure. For instance, the input field
Username (in blue at the top) is characterized by user-username as
id,username as name, and Username as label (the text on its left
side). Any of these properties may capture a relevant descriptor of
an input field useful to find the best matching with the database
columns.DBInputssimplycharacterizeseachcolumnwithitsname
in the database schema.
LemmatizedTokens (lines 15 and 16) extracts the tokens con-
tained within the identifier in lemmatized form, that is, it splits the
identifier in substrings delimited by separators (e.g., blank spaces,
underscores and dash symbols) or with the camel-case notation,
homogenizes inflection and variant forms (e.g., filling becomes fill,
and better becomes good) and gets rid of any non-alphanumeric
character. For instance, the user_username identifier gets split into
the tokens userandusername.
DBInputs computes the syntactic similarities (line 19) as the
EditDistance [22]ofthetokenstrings,andthesemanticsimilarities
(line 20) based on a semantic Word2vec model [23] that exploits
wordembedding.Thesyntacticsimilaritymatchestheidentifiers
thatdifferforminordetails.Forexample,theeditdistanceyields
0.73 for the identifiers E-mailandemail. The semantic similarity
matchessyntacticallydifferentidentifiersthathavesimilarmean-
ing. For example, Word2vec yields 0.6 for the identifiers issueand
bug. DBInputs normalizes both the edit distance and Word2vec
measurements in the range (0..1)‚Äì1b eing the highest possible
similarity‚Äìtofosterthecomparabilityofthescores.Further,DBIn-
puts keeps the highestof the two scores (line21) to embrace best
similarity perspective in each case: an identifier and a name of aUSERS.username
LOGIN_DATA.username
USERS.realname
USERS.emailEMAILS.email(0.75)(0.94)
EMAILS.email_idUsername E-mailRealName
Similarity Score0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1Input Fields
(0.77) (0.68)
Figure 3: Similarities top clusters in Mantis Bug Tracker.
databasecolumnthatare eithersyntacticallyorsemanticallysimilar
are likely to represent a same concept.
Finally, DBInputs identifies the similarity of a field-column pair
byaveragingthesimilarityscoresacrossallcorrespondingtoken
pairs (line 22), to properly weight the collective contribution of
all tokens. For instance, with reference to the example of Figure 2,
the token useris part of the id-property of both the Username
andEmailinputfields, andmatches withhigh similaritywith the
column user_id.However,when consideringthecontributionofall
tokens,theoverallsimilarityscorewithrespecttocolumn user_id
is much higher for the former than for the latter input field.
ClusteringTopSimilarityScores. DBInputsclustersthesimilarity
scoresassociatedwitheachinputfield f(i.e.,thesetofsimilarity
scores associated with each pair /angbracketleftf,ci/angbracketrightwherefis a same input
field in the form and ciis one of the columns in the database) with
the clustering algorithm ST-DBSCAN [3,24], which is a version of
theclassicDBSCANalgorithmthatself-tunestheparametersofthe
algorithm based on the characteristics of the population of values
that are analyzed. Our algorithm selects the matches that occur in
thetopclusteridentifiedby ST-DBSCAN (Algorithm2,line10),that
is the cluster of the values with the highest similarity scores.
Figure 3 illustrates the output of the clustering for the three
input fields in the example of Figure 2. The x-axis represents the
possible similarity scores. The blue, red and green dots indicate
the similarity scores that DBInputs computed comparing everydatabase column to the input fields
Username ,Real Name , and
E-mail,respectively.Thedelimitedareashighlightthetop-clusters
of each field. For instance, the top-cluster for the field Username
(blue dots) contains two similarity scores both of 0.94 that asso-
ciate the field with the database columns USERS.username and
LOGIN_DATA.username ; The top-cluster for the field Real Name
(reddots)includesasinglesimilarityvalueof0.75thatcorresponds
tothedatabasecolumn USERS.realname .Thetop-clusterforthe
field E-mail(green dots) includes three similarity scores of 0.68,
0.77 and 0.77 that correspond to the columns EMAILS.email_id ,
USERS.email andEMAILS.email, respectively.
DBInputs incrementally collects the top-clusters and the top-
similaritiesfortheinputfields(Algorithm2,lines11and12),and
returns these results upon completing all iterations (line 13).
3.2 Selecting Best Matches
Algorithm3 definesfunction IdentifyBestMatches thatidentifies
the field-columnmapping that may asa whole both yield highsim-
ilarityscoreandrefertofewdatabasetables. IdentifyBestMatches
70Algorithm3 Identifythebest-matchingcolumnforeachinput
field
Input:
1:F‚â°{f1...fn}: the input fields
2:T‚â°{t1...tm}: the tables in the database
3:MM:F‚Üí2cols(T): a multi-map that associates each input
field with the set of most similar columns
4:S:F√ócols(T)‚Üí(0,1): similarity measurementsfor all /angbracketleftfield,
column/angbracketrightpairs inMM
Output:
5:M:F‚Üícols(T): a map that associates each input field with a
database column, according to our best-match heuristics
6:function IdentifyBestMatches(F, T, MM, S )
7:All_M‚Üê{/angbracketleft /angbracketleftf1,cf1/angbracketright.../angbracketleftfn,cfn/angbracketright/angbracketright:cfi‚ààMM(fi)}
8:Sim_M‚Üê{/angbracketleftM,/summationtext.1n
1S(/angbracketleftfi,cfi/angbracketrightM)
n/angbracketright:M‚ààAll_M}
9:Top_M‚ÜêTopClusterDbscan( All_Mclustered-by Sim_M)
10:TabCount _M‚Üê{/angbracketleftM,CountTables( M)/angbracketright:M‚ààTop_M}
11:MinTab_M‚ÜêMin(Top_Mmeasured by TabCount _M)
12:Best_M‚ÜêMax(MinTab_Mmeasured by Sim_M)
13:returnRandomChoice( Best_M)
takes in input the set of fields F, the set of database tables T, and
the results of function ColumnsWithTopSimilarity (Algorithm 2),
thatis,themulti-map MMthatassociatestheinputfieldstotheir
most similar database columns and the corresponding similarity
scoresS. Thus,IdentifyBestMatches limits its evaluation to the top-
scored field-column mappings identified by Algorithm 2. Identi-
fyBestMatches returns the map Mthat associates each field to a
single, best matching database column.
The algorithm of IdentifyBestMatches proceeds as follows. First,
IdentifyBestMatches generates the space of the candidate solutions
(line 7). A candidate solutionconsists of a complete match that
associates every input field with a single database column. Iden-
tifyBestMatches obtains the candidate solutions by unfolding the
high-similaritycandidatematchingcolumnsthat DBInputscom-
putedsofarandthat IdentifyBestMatches receivesininput.Then,
IdentifyBestMatches scores each candidate solution with a qual-
ity value computed as the mean of the similarity scores of the
matchesinthesolution(line8),andexploitsagainthealgorithm
ST-DBSCAN toidentifythetopclusterofcandidatesolutionsbased
onthisqualityvalue(line9).Next, IdentifyBestMatches scoresagain
thesolutionsinthetopclusteraccordingtothenumberoftables
towhichthedatabasecolumnsineachsolutionreferto(line10),
and further selects the candidate solutions for which this scorehastheminimumvalue(line11).Finally,ifmultiplesolutionsin-
volve the same (minimal) number of tables, IdentifyBestMatches
selects the one with maximum mean similarity (line 12), and yet a
randomsolutionoutoftheremainingones(line13).Thisprocess
privileges the usage of high-quality matches, while discriminating
solutionsthatinvolvefewdatabasetablesoutoftheonesthathave
comparable quality levels.
4 EMPIRICAL EVALUATION
Toevaluatetheeffectivenessof DBInputs,weintegratedourap-
proach in the ABT test generator (hereon ABT-DBInputs), and webenchmarked the effectiveness of ABT-DBInputs across a set of
experiments.
Across the experiments with ABT-DBInputs, we quantified the
impact of DBInputs on the effectiveness of the test generationprocess both in absolute terms and incomparison to alternative
strategies.Inparticular,wecomparedDBInputstoboththebase-
line strategy of selecting inputs as randomstrings and the strategy
implementedbytheLinkapproach,whichexploitssemantic-web
technologiestogeneratesemanticallymeaningfulinputs[ 16,28].
Semantic-web has indeed demonstrated to be a useful source of
semanticallymeaningfulandvalidvaluesfortesting[ 16,28].Toex-
periencethesealternativeapproachesweintegratedbotharandom
input generator and Link into ABT, namely obtaining the ABT-
Random and ABT-Link, respectively. Below,we shortly introduce
ABT and Link tools, then we discuss the research questions, the
evaluation metrics, the setup and the results of our experiments.
4.1 ABT
AutoBlackTest(ABT)isaanautomatictestcasegenerationsolution
for GUI and Web applications [ 14,15]. ABT exercises the function-
alities of the application under test by exploiting a Q-Learning
Agent[26]thatisresponsibleforselectingandexecutingactions,
whileincorporating theobservedresultsinto astate-basedmodel.
The model represents the knowledge about the behavior of the
application under test and is exploited to incrementally improve
theeffectivenessofthechoicestakenbythetestgenerationtool.Inparticular,ABTheuristicallyassociateshightesteffectivenesstothe
observationofsignificantGUIchanges,assumingthatsignificant
GUI changes imply the execution of significant computations. This
heuristic allows ABT to learn the execution paths that are morelikelyto significantlyexercise thesoftware undertest, andfocus
thetestingactivityaroundthesepaths.Theimplementationthat
weusedinourexperimentsisbasedon.NETandusestheSelenium
WebDrivertestingframework1tointeractwiththeWebapplication
under test.
4.2 Link
Link is a technique that can retrieve coherent sets of syntactically
and semantically correct input values by exploiting the Web of
Data[16].Inparticular,giventhedescriptorsassociatedwiththein-
putfieldsandasourceknowledgebasethatcanbequeriedthroughaSPARQLendpoint[
27],Linkssearchesforresourcesstoredinthe
knowledge base that match the provided descriptors. For instance,
Link can detect that a source knowledge base hosts person data
described by properties such as name, surname and age, that well
fit input descriptors such as first name, second name, and age. The
inputdescriptorsandthedescriptorsusedintheknowledgebase
are paired taking their semantics into consideration. If suitable
resourcesareidentified intheknowledgebase, Linkretrievesthe
associatedinstancesandusestheextractedvaluestoexercisethe
application under test.
4.3 Research Questions and Evaluation Metrics
We experimented the test generator ABT-DBInputs with 4 Web
applications with the aim of characterizing the effectiveness of the
1https://www.selenium.dev/projects/
71DBInputsinputgenerationalgorithm.Wefurtherexperimented
ABT-Random and ABT-Link with the same Web applications to
assess the significance of DBInputs compared to alternative ap-
proachesforinputgeneration.Moreindetail,ourexperimentswere
driven by the following research questions:
‚Ä¢RQ1. Effectiveness: To what extent can the inputs generated
by DBInputs foster effective testing of Web applications?
‚Ä¢RQ2: Comparison: Does DBInputs significantly improve test-
ing effectiveness with respect to inputs selected randomly or
based on semantic-web technologies?
To answer RQ1, we studied the effectiveness of ABT-DBInputs
whenconsideringdifferentwaysofseedingtheworkingcopydata-
basethatDBInputsexploitsforselectingtheinputdata.Specifically,
we studied three scenarios, in which the database of DBInputs is,
respectively, equalto(ABT-DBInputs =),partiallyoverlapping with
(ABT-DBInputs ‚àº),orentirelydisjoint from(ABT-DBInputs /nequal)the
actual database that the application under test uses at runtime. We
aimed to both evaluate the sensitivity of DBInputs with respect
to this configuration choice and find the configuration that makes
DBInputs yield the best effectiveness of ABT-DBInputs. In fact,
the initial configuration of the database may affect the capabil-ity of DBInputs to produce new or existing values, and thus the
likelihood of exercising specific application constraints.
Since a better capability to generate test inputs impacts on func-
tionalities with input fields, we focus our evaluation on forms withat leastan inputfield. Inparticular, we quantify theeffectiveness of
ABT-DBInputs in its three configurations based on the frequency
with which it generates test inputs that produce either validor
invalid outcomes for the exercised forms. We say that a test case
exercises an input form if it executes a sequence of actions that
(i) visualizes the input form on the user screen, (ii) enters appropri-
ateinputsintotheform,and(iii)makestheapplicationprocessthe
inputs of the form. We say that the outcome of exercising an input
form by a test case is validorinvalid if the application does not
or doesissue validations errors whileprocessing the inputsof theform, respectively.
We executed each DBInputs configuration 10 times per Web
application, and measured the F+oneandF‚àíoneat-least-once metrics,
which quantify the total number of forms that a technique exer-
cised with at least a valid and an invalid value, respectively. More
formally, let Fappbe the set of input forms in a Web application
under test, and f+the number of times ABT-DBInputs exercises a
formf‚ààFappwith a validoutcome within an execution, F+oneis
thenumberofdistinctforms fexercisedwithavalidinput( f+>0)
inanyofthe10executions.Symmetrically, F‚àíonecountsthenum-
ber of distinct forms exercised with invalid inputs ( f‚àí>0) in
any of the 10 executions. To capture the power of a technique toconsistently exercise the forms with valid and invalid values, we
also computed the at-least-half metricsF+
half,F‚àí
half,F‚àó
half, which
count the number of forms that a technique tested with valid-only
(thatis,f+>0),invalid-only(thatis, f‚àí>0),andbothvalidand
invalidvalues(thatis, f+>0‚àßf‚àí>0)withinasameexecution
for at least half of the executions.
ToanswerRQ2,wecomparedtheeffectivenessof ABT-DBInputs
in its three configurations with the effectiveness of ABT-RandomTable 1: Statistics on Web applications
Web App # Forms# Forms with # Inputs per form
constraints min mean max
Tickets 48 31 1 2.23 9
Erp 57 31 1 4.11 19
Insurance 8 2 1 5.5 8
Budgets 14 7 1 6.5 18
We excluded login forms and forms that may cause interactions with the outside
world (e.g., forms that may send emails) and thus also from the statistics.
andABT-Link.Weusedthesamesettingsandmetricsdefinedin
RQ1 to measure the effectiveness of the approaches.
4.4 Web Applications
We executed our experiments on 4 Web apps that cover 4 different
domains and include both open-source and industrial applications:
Tickets:WeselectedtheMantisopen-sourceissuetrackingsys-
tem(v.2.21.0,availableathttps://sourceforge.net/projects/mantisbt/),
which allows users and developers to create tickets and manage
issuesofapplications,andtoorganizeactivitiesintoprojectsand
categories. Thisapplication has been usedalso in other paperson
Web testing [5, 11, 21].
ERP: We selected the Dolibarr open-source enterprise-resource-
planning and customer-relationship-management system (v. 10.0.0,
available at https://sourceforge.net/projects/dolibarr/), which pro-
vides a large set of functionalities including resource planning,
invoicing,productandstoremanagement.Alsothisapplicationhas
been used in other papers on Web testing [8, 9].
Insurance : This is the Tricentis Vehicle Insurance demo appli-
cation(availableathttp://sampleapp.tricentis.com/),whichimple-
ments services for handling insurance data of vehicles. It includes
severalinputformswithchallengingconstraintsontheinputs,and
is thus relevant to our experiments.
Budgets: This is an application for planning and managing bud-
gets that was developed jointly with an industrial partner. We can-
notdisclosespecificdetailsduetonondisclosureagreementwith
theindustrialpartner,butweregrantedtherighttoreportsummary
statistics on the experiments that we did with this application.
All these applications maintain persistent data in a database,
intensively depend on form-based interactions, and include Web
formswithconstraintsonthevalidityoftheirinputs.Thus,these
applicationsarerepresentativeoftheclassofWebapplicationsthat
techniques for the generation of input values aim to address.
Table1summarizesthemainstatisticsabouttheWebapplica-
tions considered in our experiments: the number of input formsin each application (column # Forms), the number of forms that
includeconstraintsontheinputs(column #Formswithconstraints ),
and the minimum, mean and maximum number of input fields in a
single form (columns # Inputs per form ). The data in the table indi-
cate that a relevant portion of the input forms includes constraints
ontheinputs,withpercentagerangingfrom25%inthe Insurance
applicationto65%inthe Ticketsapplication.Thisindicatesthat
generatingvalidinputsis importanttothoroughlytesttheseWeb
applications. The numberof input fields present in a single form
with constraints ranges from a minimum of 1 to a maximum of
19, with the mean ranging from more than 2 to more than 6 input
fields per form across the considered applications, testifying the
variability of the scenarios to be addressed.
724.5 Experiment Setup
DataCollection.Tocollectthedataaboutthenumberofformsexer-
cisedwithvalid( f+)andinvalid( f‚àí)inputs,wemanuallyinspected
every form in the considered Web applications. For the forms with
constraintsontheirinputswecollected:(i)aformidentifier(usually
the title page), (ii) the actions that make the application process
theform(e.g.,clickingthesubmitbutton),and(iii)thepagesthat
can be reached after processing the form, identifying the graphical
elements that allow us to distinguish if invalid or valid inputs have
been submitted. We used this information to augment ABT with
an application-specific logger that tracks each time ABT exercises
a form with either valid or invalid inputs.
Database Preparation. For the research question RQ1, to study
therelativeeffectivenessof ABT-DBInputs =,ABT-DBInputs ‚àº,and
ABT-DBInputs /nequal,wegenerateddifferenttestdatabasesasfollows.
WesplittheoriginaldatabaseofeachconsideredWebapplicationin
two parts of equal sizes. In the experiments with ABT-DBInputs /nequal,
weusedthefirstpartofthesplitofeachdatabaseastheworking
databaseforDBInputsandthesecondpartastheruntimedatabase
of the application. In the experiments with ABT-DBInputs ‚àº,w e
usedthefirstpartastheworkingdatabaseforDBInputsandthe
union of a half of the first part and a half of the second part asthe runtime database of the application. In the experiments with
ABT-DBInputs =,weusedthefirstpartbothastheworkingdata-
base for DBInputs and the runtime database of the application.
This procedure allowed us to use testing and runtime databases of
the same size in all experiments, while guaranteeing of using only
datatakenfromtheoriginaldatabasesoftheapplications.Across
multiplerepetitionsoftheexperiments,weswappedtherolesof
theformerandthesecondpartofoursplitoftheoriginaldatabases,
to mitigate possible biases induced by the specific splits.
For all applications but Insurance , for which we had no access
to its database, we referred to the database available in produc-tion (
Budgets) or included in the downloaded material ( Erpand
Tickets)in March2020.For the Insurance applicationwe relied
on a (plausible) database independently designed and populated bytwocomputersciencemasterstudentsofouruniversity.Thedesign
of the database required to map each functionality into a data-
base table and each field into a table column, whereas the database
population was performed manually. Since we cannot control the
databaseoftheapplication,wecouldonlyexecute ABT-DBInputs /nequal
on the Insurance application.
Table2providesstatisticsaboutthedatabasesthatweused.The
ErpDolibarr Web application has the database with the largest
number of tables and columns per table. Budgets, the industrial
applicationforbudgetmanagement,hasamedium-sizedatabase
butthehighestnumberofrecords. Tickets,theMantisapplication,
hasalsoamedium-sizedatabasebutwithlimitednumberofrecords.
Finally, the Insurance application has the smallest database but a
good number of record.
Authentication. Exploring functionalities that depend on authen-
tication is a well-known challenge for automatic test generators,because it is generally impossible for the test generator to guessthe password of a user, or even decoding the password from theTable 2: Statistics on databases used by DBInputs
Database # tables# cols per tablemean # recordsmin mean max
Tickets 33 2 7.85 29 7.21
Erp 311 2 12.57 89 15.52
Insurance 7 3 6.29 10 19.86
Budgets 14 2 8.5 38 524.21
corresponding encrypted format stored in the database. In our ex-
periments, we worked around this problem by making ABT aware
of the password of a user for each of the considered applications.
Setup of Competing Approaches. We refer to the random genera-
tionofinputvaluesasthebaselineapproachtocompareDBInputs
to. It simply consists of producing a random string of randomlength every time an input is required. Link exploits the Web of
Data to generate test inputs. As in its original paper [ 16], we used
DBPedia, which includes semantically annotated data extractedfrom Wikipedia, as source knowledge base. For both approaches
we added a cache management system working the same as in
DBInputs for the fairness of the comparison.
Execution of the experiments. We executed each technique 10
timesoneachapplication,foratotalof180executions.Inparticular,
we executedall the techniques (the3 configurations of DBInputs
andthe2competingapproaches)onthe Tickets,ErpandBudgets
applications, while we only executed ABT-DBInputs /nequaland the 2
competing approaches on Insurance , since only the disjoint data-
base was available. Overall, the empirical evidence collected for
each technique amounts to the generation of 500 test cases per ap-
plication and thousands of automatic interactions with the various
inputforms,requiringintotalmorethanonemonthofcomputa-
tion.Theexperimental material,includingtheimplementationof
the three compared approaches and the experimental results are
available at https://gitlab.com/DBInputs/dbinputs.
4.6 Results on RQ1: Effectiveness of DBInputs
Figure 4 summarizes, for each technique, the measurements about
the forms executed with valid and invalid inputs across the experi-
ments with ABT-DBInputs /nequal,ABT-DBInputs =and
ABT-DBInputs ‚àº.The barsin theplots showthe numberof forms
thateachtechniqueexecutedwithvalid(/summationtext.1
appsF+one)andinvalid
(/summationtext.1
appsF‚àíone)inputsacrossthethreeapplicationsthatcanbetested
with all the configurations of DBInputs. The dotted lines on the
barsindicatethenumberofformsexercisedwithvalid(/summationtext.1
appsF+
half)
andinvalid(/summationtext.1
appsF‚àí
half)inputswithhalf-timeconsistency.The
solid linesindicate the numberof formsexercised with bothvalid
andinvalidvalues(/summationtext.1
appsF‚àó
half)withhalf-timeconsistency.The
toppartofeachbardistinguishesthenumberofformsexercisedby two configurations-only and uniquely exercised by a single
configuration of DBInputs, to quantify the unique capabilityof a
configuration to test certain forms compared to the others.
Overall, these plots show that ABT-DBInputs /nequaloutperformed
ABT-DBInputs ‚àºand ABT-DBInputs =on every metric:
‚Ä¢it covers the highest number of forms with valid values: 20%more forms than
ABT-DBInputs =and 11% more forms than
ABT-DBInputs ‚àº,
73# forms with valid inputs (F+
one) # forms with invalid inputs (F√Ø
one)
ABT
DBInputs íABT
DBInputs=ABT
DBInputs~ABT
DBInputs íABT
DBInputs=ABT
DBInputs~051015202530354045
Executed by every configuration Executed by two configurations
Executed by a single configuration F+
halfF-
halfF*
half
Figure 4: Forms exercised by DBInputs.
‚Ä¢itcoversthehighestnumberofformswithinvalidvalues:13%
more forms than ABT-DBInputs =and 19% more forms than
ABT-DBInputs ‚àº,
‚Ä¢itistheconfigurationthatcoversthehighestnumberofforms
not exercised by the other configurations: 29% and 17% of the
formscoveredwithvalidvalues,and23%and16%oftheforms
covered with invalid values are covered by at most one andnone
of the competing configurations, respectively.
‚Ä¢it is superior as for the ability of consistently exercising forms
withvalidandinvalidvalues,anditissignificantlybetteroncon-
sistentlyusing bothvalidandinvalidinputs(16formsconsistently
exercised by ABT-DBInputs /nequalversus 12 and 9 consistently exer-
cised by ABT-DBInputs ‚àºand ABT-DBInputs =, respectively).
The data indeed indicate that working with a disjoint dataset
facilitatesDBInputsinthegenerationofvalidoutcomesinmore
casesthanwiththeothertwoconfigurations.Thisisclear,forin-
stance, in forms that require entering new values, such as a form
forcreatinganew account,whichmayrequireanon-existinguser-
name to be successfully submitted. Conversely, when DBInputs
works with a database that is nearly the same as the application
database, it can easily generate valid values for forms that require
enteringexistingvalues.Despitethisintrinsicduality,usingadis-
joint database bringssome advantages. In fact, adatabase with all
existingvalueshasnochancetoproduceavalidvaluefortheforms
that require new values. On the contrary, with a disjoint database,
DBInputscanfirstcreateadomainentity(e.g.,anewissueinabug
tracking system) and then select the same values used for creating
that entity as inputs in other forms (e.g., in a search form), thus
satisfyingconstraintsonusingexistingvalues.Thisisareasonwhy
ABT-DBInputs /nequalperformed better than the other approaches.
Although ABT-DBInputs ‚àºmayinprincipleappeartobeagood
compromise in-between of the other two configurations, since the
workingcopydatabase includesbothnewand existingvalues, its
effectiveness inevitably dependson the luck of selecting theright
type of value at the right time.
Even ifABT-DBInputs /nequalis the configuration that performed
better,thetwoalternativeconfigurationscoveredformsnotcovered
byABT-DBInputs /nequal. Thisrevealsthe (expected) complementarity
of various configurations, and calls for future work on studying
DBInputs strategies that exploit this complementarity.# forms with valid inputs (F+
one) # forms with invalid inputs (F√Ø
one)
051015202530354045
ABT
DBInputs íABT
LINKABT
RandomABT
DBInputs íABT
LINKABT
Random
Executed by every technique Executed by two techniques
Executed by a single technique F+
halfF-
halfF*
half
Figure 5: Forms exercised by DBInputs, Random and Link.
4.7 Results on RQ2: Comparison
Figure5comparestheresultsof ABT-DBInputs /nequal,thebestDBIn-
puts configuration according to the results reported above, to
the results of the competing approaches ABT-Random and ABT-Link. The plots in this figure refer to all the considered appli-cations, including
Insurance , and this is why the new plot of
ABT-DBInputs /nequalshows larger measurements than in Figure 4.
















0.000.250.500.751.00
Total Add Edit Search Other
Operation categoryPortion of valid outcomes
Technique $%7√Ø'%,QSXW í $%7√Ø/,1. $%7√Ø5DQGRP
Figure6:PortionofvalidoutcomesgeneratedbyDBInputs,
Random and Link per operation category
The results in the plots confirm the main hypothesis of our
research,thatis,DBInputsismoreeffectivethanthecompetingap-
proaches in the generation of valid test inputs. The plot on the left
sideofthefigureshowsthat ABT-DBInputs /nequallargelyoutperformed
both ABT-Link and ABT-Random in generating valid inputs, exer-
cising up to 44 distinct forms with valid outcome, while ABT-Link
and ABT-Random exercised only24 and 25distinct forms, respec-
tively. The higher effectiveness of DBInputs with respect to the
competingtechniquesisconfirmedalsofocusingontheformsexer-cisedwithhalf-timeconsistency,since
ABT-DBInputs /nequal,ABT-Link
and ABT-Random consistently exercised 25, 11 and 10 forms with
valid inputs, respectively.
74Theplotontherightsideshowsthat ABT-DBInputs /nequalperformed
comparably wellto ABT-Link andABT-Random in generatingin-
valid inputs. ABT-DBInputs /nequalexercised a higher number of total
forms, but ABT-Random achieved slightly better consistency, exer-
cising 29forms withhalf-time consistency, 2more formsthan the
other techniques.
Finally, with reference to the extent to which the approaches
consistentlyexercisetheformswithbothvalidandinvalidinputs
(F‚àó
half),ABT-DBInputs /nequaloutperformed the competing techniques.
ABT-DBInputs /nequal,ABT-LinkandABT-Randomconsistentlyexer-
cised18,6and7formswithbothvalidandinvalidinputs,respec-
tively.
Weanalyzedthesedatainfurtherdetailbyinspectingtherelative
portion of valid inputs generated in each execution of a technique
with a considered application, that is, the ratio R+=/summationtext.1
Ff+
/summationtext.1
Ff++f‚àí
whereFare the forms of an application. We computed this indica-
torbothwithrespecttoallformsoftheapplications,andatthelevel
of the subsets of forms that represent different types of operations.
Tothis end,weclassifiedtheconsideredinput formsaccordingto
fourpossiblecategoriesofoperations: createoperations,whose
goal is creating new domain entities, updateoperations, whose
goalisupdatingexistingdomainentities, searchoperations,whose
goal is searching for domain entities, and otheroperations, i.e., all
remainingoperations.Figure6visualizestheboxplot(minimum,
maximum,medianandquartiles)ofthedistributionof R+perop-
erationcategoryandintotalfor ABT-DBInputs /nequal,ABT-Linkand
ABT-Random.
Resultsshowthat ABT-DBInputs /nequalgeneratesahighernumber
ofvalidvaluesthanABT-LinkandABT-Randomconsistentlyon
every class of functionality. We tested the statistical significance
of the results comparing ABT-DBInputs /nequalto both ABT-Link and
ABT-RandomusingtheWilcoxontest.Wecomputedtheeffect-size
to assess the magnitude of the difference, and used a significant
levelof0.05withBonferronicorrection,consideringthatweper-
formed multiple tests. The results indicate that ABT-DBInputs /nequal
performed significantly betterthanABT-LinkandABT-Randomin
all cases but the comparison between ABT-DBInputs /nequaland ABT-
Link for Searchoperations where the difference is not signifi-
cant. The effect-size is always large(effect-size greater than 0.5)
withtheexceptionofthecomparisonsbetween ABT-DBInputs /nequal
and ABT-Link for Addoperations and the comparison between
ABT-DBInputs /nequaland ABT-Random for Searchoperations, where
the effect-size is medium(effect-size between 0.3 and 0.5).
ThedualinterpretationoftheplotsinFigure6isthatthecompet-
ingapproachesgeneratemoreinvalidvaluesthan ABT-DBInputs /nequal.
However,aswealreadydiscussedwithreferencetoFigure5,the
ability of ABT-DBInputs /nequalof generating more valid inputs than
the other approaches leads to exercising a larger number of forms
of the applications, which in turn results in an increased testing
thoroughness,withoutcompromisingthecapabilityofexercising
forms with invalid inputs.
Letusremarkthattheresults obtainedwith ABT-Linkdepend
on the domain-specific characteristics of the applications undertest that are not well aligned with the DBPedia knowledge base
thatLinkrefersto,thusmakingABT-LinkdifferonlyslightlyfromABT-Random. This confirms the main weakness of using semantic
Web technologies to identify input values.
We inspected the forms that only a technique exercised with
eithervalidorinvalidinputstobetterunderstandandexemplifythe
success cases. DBInputs successfully entered domain-specific and
application-specificvaluesthattheotherapproachescannotobtain.
For instance, DBInputs successfully entered: ODA codes, whichare unique codes that identify orders, as well as IBAN, BIC and
SWIFTcodesinthe Budgetsapplication;customeridentifiersinthe
Erpapplication;alargesetofcoherentandheterogeneousvalues
that span multiple forms as needed for filing an insurance in the
Insurance application;issueidentifiersinthe Tickets application.
Notethatoftenthesevalueshavetosatisfybothdomain-specific
constraints, and application-specific checks implemented in the
Web forms that require these values (e.g., entering values that are
new or already exist in the database of the application), which are
almost impossible for the competing approaches to fulfill.
Nonetheless,therearesomecomplementaritiesamongtheap-
proaches.Forinstance,ABT-Linkmanagedtoenterinvaliddiscount
valuesinthe Erpapplication,whichDBInputsfailedtocover.ABT-
Random easily entered invalid values in input fields that check for
a specific syntax (e.g., date values in the Erpapplication) that were
sometime hard to cover with DBInputs.
4.8 Discussion
The empirical results show that DBInputs can improve the ability
of a test generator to exercise forms with valid values. This is
importanttoachieveagoodexploratoryabilityandbothreachand
exercise the functionalities of the Web application under test. The
configuration with a database that is disjunct from the one used by
the application consistently produced the best results, and thus, it
is also the recommended configuration to use.
WhileDBInputsprovedtobeeffectiveinconsistentlyproviding
the test cases with valid input data, its main weakness depends on
the database itself. Excluding the cases where the working copy
database is populated with few rows or several empty/null values,
thedatastoredinthedatabasearerarelyinvalidbyconstruction,
sincetheyreflectapossiblevalidstateoftheapplicationundertest,
making domain-specific constraints hard to invalidate. Thismay
still occur if the database schema or the Web page itself are poorly
designed(e.g.,acolumnpresentsaverygeneric orunintelligible
name or the input field has no relevant identifiers), identifying bad
matchesbetweentheinputsfieldsandthedatabasecolumnsand,
consequently, likely returning invalid data.
Whilegeneratingadominantnumberofinvalidvaluespenalizes
the effectiveness of the exploration, the results show that Link
andRandommanagedtogenerateuniqueinvalidvaluesforsome
forms. This suggests that DBInputs, Link and Random can becomplementary, especially in their capability to produce invalid
values,andanoptimaltestgenerationstrategycouldbe,potentially,
derivedbycombiningthem.Thisishoweveroutsidethescopeof
this study.
4.9 Threats to Validity
Themaininternalthreatstothevalidityofourexperimentarere-
latedto(i)themanualidentificationoftheinputformsthatevaluate
75constraints on their inputs, and (ii) the design of the logger that
collectsthedataonwhethertheseformsgetexercisedwithvalid
and invalid values at runtime. To mitigate these risks, we carefully
inspectedtheapplicationsmultipletimes,andwethoroughlytested
the logger with many executions, and we are in the end confident
of the accuracy of the current data.
The external validity threats concern the generality of our find-
ings.Ourresultsrefertoasignificantsampleofinputforms(127
in total) that consider different application domains and databases
with different amounts of tables and columns. We repeated each
experiment 10 times, to control for the randomness in the input
generation and test generation algorithms, thus obtaining stableexperimental evidence. However, we are aware that we cannot
claimthevalidityofourfindingsforanypossibleapplication,ap-
plication domain and type of database, and we are working to new
experiments to extend the current empirical data further on.
A specific threat concerns whether the effectiveness of DBIn-
putscangeneralize totest generators otherthan ABT. Inprinciple,
DBInputs works in the same way independently on the test gener-
ator that uses it, and there is no obvious technical reason why the
choice of the test generator should affect the results. We see this as
an interesting direction for extending our results in the future.
5 RELATED WORK
Systemtestingtechniques,andWebtestingtechniquesinparticular,focusonthegenerationofsequencesofeventsthatcancovertheex-ecutionspaceoftheapplicationundertest.Theexistingtechniques
embrace different strategies, including the randomized [ 1,6,7],
model-based[ 13,15,19]andinvariant-driven[ 10,20]exploration
oftheexecutionspace.Complementaltoeventgeneration,these
techniques use different approaches to fill the input fields with test
inputs while interacting with the application under test.
Generating random inputs is inexpensive [ 1,6,10], but may eas-
ilyfailwiththeWebformsthatincludeconstraintsonthevalidityof
the inputs. Indeed, our experiments indicate that ABT-Random in-
teractswellwiththeformswithoutconstraints,butABT-DBInputs
outperforms ABT-Random with the many forms that do.
An approach is to prepare a manually curated set of values and
pass them to the test generator [ 18,20]. A plain list of values is
relativelyeffective,sinceitdoesnotguaranteethattherightvalueisselected for the right input fields. For instance, the list may includedatevalues,butthetestgeneratormayeasilyendupwithselecting
other values in the list for the date-constrained input fields. A
more sophisticated approach consists of associating a type to each
value in the curated list [ 7,13,15,19]. While this may increase the
effectivenessofthetestingprocess,itrequiresdeveloperstoprepare
an extensive set of values and associated types for the many forms
of the application under test, which limits the practical benefits of
the approach.Indeed, DBInputs offers abetter compromise since
it does not require manual effort.
Some solutions address the problem of automatically generating
syntacticallyandsemanticallyrelevantinputsbyrelyingon external
sources. McMinn et al. exploit the Web as source of data, retrievingthe inputs with Web searches [
17,25]. Bozkurt and Harman define
a strategy that searches for Web Services that can be stimulated to
producevaluesofthetypesneededduringthetestingprocess.BothLink[16]andtheapproachbyWanwarangetal.[ 28]exploitedthe
WebofDataandsemantic-webtechnologiestoretrievevaluesof
the proper type that can be used to test applications.
As we already commented, the techniques that rely on external
sources can hardly produce application-specific data (e.g., the iden-
tifierofanissueenteredintoanissuetrackingsystem).Theycan
producesomedomain-specificdata,butonlyiftheexternaldomain
andthedomainoftheapplicationarealigned,whichisnotoften
the case. For instance, in our evaluation Link struggled copying
with the four Web applications and their domains.
DBInputs successfully enriches automatic Web testing with the
abilityofautomaticallyidentifyingbothdomain-andapplication-
specific data, by reusing the application database for testing pur-
poses.OurresultsshowthatDBInputscansignificantlyimprove
thecapabilityofthetestgeneratortoexercisethefunctionalitiesof
the application under test.
6 CONCLUSIONS
Thegenerationofvalidinputvaluescanbeabarriertoautomatic
Webtesting.Indeed,itisoftennecessarytoenterdomain-specific
and application-specific values into Web forms to successfully exe-
cutethecorrespondingfunctionalities.Failingtoobtainthesevaluesmaysignificantlylimittheeffectivenessofthetestcasesthatwould
not be able to exercise some of the functionalities, consequently
missing to explore part of the execution space of the application.
Toalleviatethisproblem,thispaperinvestigatestheideaofusing
theapplicationdatabaseasdatabaseofvaluesthatcanbeexploitedfortestcasesgeneration.ItpresentstheDBInputstechnique,which
originally embodies this intuition. DBInputs automatically han-
dlesthemismatchbetweenthecontentoftheWebpagesandthe
structure of the database, extracting inputs useful for testing. In
particular,DBInputsexploitssyntacticandsemanticdistanceswith
matching heuristics that successfully map the input fields to thecolumns of the tables in the database. This mapping enables the
extractionofvaluesfromtherecordsinthedatabasewithwhich
DBInputs fills the input fields of the application under test. Theempirical evaluation that we reported in the paper shows that
DBInputs can outperform the existing approaches based on either
randominputsoronretrievingdatafromexternalsourcesavailable
on the Web.
WearecurrentlyinvestigatinghowtoextendDBInputstoreuse
databases across applications, thus further improving its ability of
obtaining heterogeneous inputs to reveal problems in the appli-cations under test. We are also considering to extend DBInputsto mobile applications, to investigate the effectiveness of the so-lution in the mobile domain. Finally, we intend to evaluate the
scalabilityandfeasibilityofthetechniquewhenlargerdatabases
and applications are involved.
ACKNOWLEDGMENTS
We thank Fabio Meraviglia and Luca Guglielmo for their help with
the experiments.
This workhas beenpartially supported bythe H2020ERC PoC
projectAST(n.824939),andbytheSISMAnationalresearchproject
(MIUR, PRIN 2017, Contract 201752ENYB).
76REFERENCES
[1]ShayArtzi,JulianDolby,SimonHolmJensen,AndersMoller,andFrankTip.2011.
Aframeworkforautomatedtestingofjavascriptwebapplications.In Proceedings
of the 33rd International Conference on Software Engineering (ICSE).
[2]Giovanni Becce, Leonardo Mariani, Oliviero Riganelli, and Mauro Santoro. 2012.
Extracting Widget Descriptions from GUIs. In proceedings of the International
Conference on Fundamental Approaches to Software Engineering.
[3]Derya Birant and Alp Kut. 2007. ST-DBSCAN: An algorithm for clustering
spatial‚Äìtemporal data. Data & Knowledge Engineering 60, 1 (2007), 208‚Äì221.
[4]Mustafa Bozkurt and Mark Harman. 2011. Automatically generating realistic
testinputfromwebservices.In proceedingsoftheInternationalSymposiumon
Service Oriented System Engineering.
[5]MiroslavBures,KarelFrajtak,andBestounSAhmed.2018. Tapir:Automation
support of exploratorytesting using model reconstruction ofthe system under
test.IEEE Transactions on Reliability 67, 2 (2018), 557‚Äì580.
[6]Android Developers. [n.d.]. UI/Application Exerciser Monkey. https://developer.
android.com/studio/test/monkey
[7]AminMilaniFardandAliMesbah.2013. Feedback-directedexplorationofweb
applications to derive test models. In Proceedings of the International Symposium
on Software Reliability Engineering (ISSRE).
[8]MounaHammoudi,GreggRothermel,andAndreaStocco.2016. Waterfall:An
incrementalapproachforrepairingrecord-replaytestsofwebapplications.In Pro-
ceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering. 751‚Äì762.
[9]Mouna Hammoudi, Gregg Rothermel, and Paolo Tonella. 2016. Why do
record/replay tests of web applications break?. In 2016 IEEE International Confer-
ence on Software Testing, Verification and Validation (ICST). IEEE, 180‚Äì190.
[10]Phillip Heidegger and Peter Thiemann. 2010. Contract-Driven Testing of
JavaScriptCode.In Objects,Models,Components,Patterns,JanVitek(Ed.).Springer
Berlin Heidelberg, Berlin, Heidelberg, 154‚Äì172.
[11]Maurizio Leotta, Diego Clerissi, Filippo Ricca, and Paolo Tonella. 2013. Capture-
replay vs. programmable web testing: An empirical assessment during test case
evolution. In 2013 20th Working Conference on Reverse Engineering (WCRE). IEEE,
272‚Äì281.
[12]MaurizioLeotta,AndreaStocco,FilippoRicca,andPaoloTonella.2018. PESTO:
Automated migration of DOM-based Web tests towards the visual approach.
Software Testing, Verification & Reliability (STVR) 28, 4 (2018).
[13]Alessandro Marchetto, Paolo Tonella, and Filippo Ricca. 2008. State-Based Test-
ingofAjaxWebApplications.In ProceedingsoftheInternationalConferenceon
Software Testing, Verification, and Validation (ICST).
[14]Leonardo Mariani, Mauro Pezz√®, Oliviero Riganelli, and Mauro Santoro. 2012.
AutoBlackTest: Automatic Black-Box Testing of Interactive Applications. In Pro-
ceedings of the 5th IEEE International Conference on Software Testing, Verification
and Validation (ICST).
[15]Leonardo Mariani, Mauro Pezz√®, Oliviero Riganelli, and Mauro Santoro. 2014.
Automatic testing of GUI-based applications. Software Testing, Verification &
Reliability (STVR) 24, 5 (2014), 341‚Äì366.
[16]Leonardo Mariani, Mauro Pezz√®, Oliviero Riganelli, and Mauro Santoro. 2014.
Link:exploitingthewebofdatatogeneratetestinputs.In Proceedingsofthe2014
International Symposium on Software Testing and Analysis. 373‚Äì384.
[17]PhilMcMinn,MuzammilShahbaz,andMarkStevenson.2012. Search-BasedTest
Input Generation for String Data Types Using the Results of Web Queries. In
proceedingsoftheInternationalConferenceonSoftwareTesting,Verificationand
Validation.
[18]AtifM.MemonandQingXie.2005. StudyingtheFault-DetectionEffectiveness
of GUI Test Cases for Rapidly Evolving Software. IEEE Transactions on Software
Engineering (TSE) 31, 10 (2005), 884‚Äì896.
[19]AliMesbah,ArievanDeursen, andStefanLenselink.2012. CrawlingAjax-Based
WebApplicationsthroughDynamicAnalysisofUserInterfaceStateChanges.
ACM Transactions on the Web (TWEB) 6, 1 (2012), 3:1‚Äì3:30.
[20]AliMesbah,ArievanDeursen,andDannyRoest.2012.Invariant-BasedAutomatic
Testing of Modern Web Applications. IEEE Transactions on Software Engineering
(TSE)38, 1 (2012), 35‚Äì53.
[21]Harald Raffelt, Tiziana Margaria, Bernhard Steffen, and Maik Merten. 2008. Hy-
brid test of web applications with webtest. In Proceedings of the workshop on
Testing, analysis, and verification of web services and applications. 1‚Äì7.
[22]Eric Sven Ristad and Peter N Yianilos. 1998. Learning string-edit distance. IEEE
Transactions on Pattern Analysis and Machine Intelligence 20, 5 (1998), 522‚Äì532.
[23]Xin Rong. 2014. word2vec Parameter Learning Explained. arXiv preprint
arXiv:1411.2738 (2014).
[24]ErichSchubert,J√∂rgSander,MartinEster,HansPeterKriegel,andXiaoweiXu.
2017. DBSCANrevisited,revisited:whyandhowyoushould(still)useDBSCAN.
ACM Transactions on Database Systems (TODS) 42, 3 (2017), 1‚Äì21.
[25]MuzammilShahbaz,PhilMcMinn,andMarkStevenson.2015. Automaticgen-
eration of valid and invalid test data for string validation routines using web
searchesandregularexpressions. ScienceofComputerProgramming 97(2015),
405‚Äì425.[26]RichardSSuttonandAndrewGBarto.2018. Reinforcementlearning:Anintro-
duction. MIT press.
[27] W3C SPARQL Working Group. 2013. Sparql 1.1 overview. Technical Report.
[28] TanapuchWanwarang,NatanielBorges,LeonBettscheider,andAndreasZeller.
2020. Testing Apps With Real World Inputs. In Proceedings of the IEEE/ACM
International Conference on Automation of Software Test (AST).
77