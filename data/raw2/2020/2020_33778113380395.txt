Taxonomy ofReal FaultsinDeep LearningSystems
Nargiz Humbato
va
UniversitàdellaSvizzera italiana
Lugano,Switzerland
nargiz.humbatova@usi.chGunelJahangirova
UniversitàdellaSvizzera italiana
Lugano, Switzerland
gunel.jahangirova@usi.chGabriele Bavota
UniversitàdellaSvizzeraitaliana
Lugano,Switzerland
gabriele.bavota@usi.ch
Vincenzo Riccio
UniversitàdellaSvizzera italiana
Lugano,Switzerland
vincenzo.riccio@usi.chAndreaStocco
UniversitàdellaSvizzera italiana
Lugano, Switzerland
andrea.stocco@usi.chPaolo Tonella
UniversitàdellaSvizzeraitaliana
Lugano,Switzerland
paolo.tonella@usi.ch
ABSTRACT
The growing application of deep neural networks in safety-critical
domains makes the analysis of faults that occur in such systems of
enormousimportance.Inthispaperweintroducealargetaxonomy
offaultsindeeplearning(DL)systems.Wehavemanuallyanalysed
1059artefactsgatheredfromGitHubcommitsandissuesofprojects
thatusethe most popularDL frameworks (TensorFlow, Keras and
PyTorch)andfromrelatedStackOverﬂowposts.Structuredinter-
viewswith20researchersandpractitionersdescribingtheproblems
theyhaveencounteredintheirexperiencehaveenrichedourtax-
onomy with a variety of additional faults that did not emerge from
the other two sources. Our ﬁnal taxonomy was validated with a
surveyinvolvinganadditionalsetof21developers,conﬁrmingthat
almost all fault categories (13/15) were experienced by at least 50%
of the survey participants.
CCS CONCEPTS
•Software and its engineering →Software veriﬁcation and
validation.
KEYWORDS
deep learning, real faults, software testing,taxonomy
ACMReference Format:
NargizHumbatova,GunelJahangirova,GabrieleBavota,VincenzoRiccio,
AndreaStocco, andPaoloTonella. 2020.Taxonomyof RealFaultsinDeep
Learning Systems. In 42nd International Conference on Software Engineering
(ICSE’20),May23–29,2020,Seoul,RepublicofKorea. ACM,NewYork,NY,
USA,12pages.https://doi.org/10.1145/3377811.3380395
1 INTRODUCTION
Deep Learning (DL) is ﬁnding its way into a growing number of
areasinscienceandindustry.Itsapplicationrangesfromsupporting
daily activities such as converting voice to text, translating texts
from one language to another, to much more critical tasks such as
Permission to make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forproﬁtorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspeciﬁcpermissionand/ora
fee. Request permissionsfrom permissions@acm.org.
ICSE’20, May 23–29,2020, Seoul, South Korea
©2020 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-7121-6/20/05...$15.00
https://doi.org/10.1145/3377811.3380395fraud detection in creditcard companies, diagnosis and treatment
of diseases in medical ﬁeld, autonomous driving of vehicles. The
increasingdependenceofsafety-criticalsystemsonDLnetworks
makesthetypesoffaultsthatcanoccurinsuch systemsacrucial
topic. However, the notion of fault in DL systems is more complex
than in traditional software. In fact, the code that builds the DL
networkmightbebugfree,butthenetworkmightstilldeviatefrom
the expected behaviour due to faults introduced in the training
phase,suchasthemisconﬁgurationofsomelearningparameters
orthe use ofan unbalanced/non-representative training set.
The goal of this paper is to build a taxonomy of real faults in
DL systems. An example of a subject DL system could be an object
detectionsubsysteminanautonomouscar.Suchataxonomycan
be usefultoaid developersavoiding common pitfallsor canserve
asachecklistfortesters,motivatingthemtodeﬁnetestscenarios
that address speciﬁc fault types. We consider that a DL fault has
takenplacewhenthebehaviouroftheDLcomponentisinadequate
forthetaskathand(i.e.,itis“functionallyinsuﬃcient”,inlinewith
ISO/PAS21448:2019[ 6])andtherootcauseforsuchinadequacyisa
human mistake that occurred during DL development and training.
The taxonomy could also be used for fault seeding, as resemblance
withrealfaultsisanimportantfeatureforartiﬁciallyinjectedfaults.
A taxonomy is mainly a classiﬁcation mechanism[ 28]. Accord-
ingtoRowleyandFarrow[ 22],therearetwomainapproachesto
classiﬁcation:enumerativeandfaceted.In enumerativeclassiﬁca-
tion,theclassesarepredeﬁned.However,itisdiﬃculttoenumerate
allclassesinimmatureorevolvingdomains,whichisthecaseofDL
systems.Therefore,usingavettedtaxonomyoffaults[ 13]would
notbeappropriateforourpurpose.Incontrast,in facetedclassiﬁca-
tiontheemergingtraitsofclassescanbeextendedandcombined.
Forthisreasonweusedfacetedclassiﬁcation, i.e.,wecreatedthe
categories/subcategories ofour taxonomyinabottomup way, by
analysingvarioussourcesof information aboutDL faults.
Our methodology is based on the manual analysis of unstruc-
turedsourcesandinterviews.Westartedbymanuallyanalysing477
StackOverﬂow(SO)discussions,271issuesandpullrequests(PRs),
and 311 commits from GitHub repositories, in which developers
discuss/ﬁxissues encounteredwhile usingthree popular DLframe-
works. The goal of the manual analysis was to identify the root
causebehindtheproblem.Theoutputofthisstepwastheﬁrsthier-
archical taxonomy of faults related to the usage of DL frameworks.
Then, two of the authors interviewed 20 researchers and practi-
tioners to collect their experience on the usage of DL frameworks.
All the interviews were taped and transcribed, allowing an open
11102020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)
ICSE ’20, May 23–29, 2020,Seoul,South Korea Humbato va andJahangirova,et al.
coding procedure among all authors, by which we identiﬁed the
categoriesoffaultsmentionedbytheinterviewees.Thisallowed
to complement our preliminary taxonomy andto produce its ﬁnal
version.
Tovalidateourﬁnaltaxonomywehaveconductedasurveyto
which an additional set of 21 researchers/practitioners have re-
sponded.Inthesurvey,weincludedthecategoriesofthetaxonomy
alongwithadescriptionofthetypesoffaultstheyrepresent,and
askedtheparticipantstoindicatewhetherthesefaultshavebeen
encounteredintheirpriorexperiencewhendevelopingDLsystems.
Most faults (13/15) were experienced by 50% or more of the par-
ticipantsandnofaultcategoryremainednon-validated(theleast
frequent category wasconﬁrmed by24%participants).
Themaincontributionofthispaperisthevalidatedtaxonomy
ofrealfaultsinDLsystems.Toourknowledge,thisistheﬁrstwork
that includes interviews with developers on the faults related to
thedevelopmentofDLsystems.Withoutsuchinterviews,2inner
nodes and 27 leaf nodes of the taxonomy would be completely
missed.Severalothernodesarehighlyrepresentedininterviews,
but they appear quiterarely inthe otheranalysedartefacts.
Structureofthepaper . Section2gives an overview of related
work. Section 3describes the methodology used to collect faults,
to
build the taxonomy and to validate it. The ﬁnal version of the
taxonomy, along with the description of its categories and the
resultsofthevalidationsurveyarepresentedinSection 4.Section 5
containsadiscussionofourﬁndings,whileSection 6reviewsour
threatsto validity.Finally,Section 7concludes the paper.
2
RELATED WORK
2.1 RepositoryMining
OneoftheﬁrstpapersconsideringfaultsspeciﬁcallyinMachine
Learning(ML)systemsisthe empiricalstudybyThung etal.[27].
Theauthorsmanuallylabeled500bugreportsandﬁxesfromthebug
repositoriesofthreeopensourceprojects(ApacheMahout,Lucene,
and OpenNLP) to enable their classiﬁcation into the categories
proposed by Seaman et al.[24]. Descriptive statistics were used to
addressresearchquestionssuchashowoftenthebugsappear,how
severethebugsareandhowmucheﬀortisputintotheirresolution.
A similar study was published in 2017 by Sun et al.[26]. The
authors examined bug patterns and their evolution over time. As a
subject of the study the authors considered three ML projects from
GitHub repositories(Scikit-learn,CaﬀeandPaddle).The collected
issues have been organised into 7 categories. Manual analysis of
329successfullyclosedbugreportsallowedtheauthorstoassess
the fault category, ﬁx pattern, and eﬀort invested while dealing
withabug.
Themaindiﬀerencebetweenthesetwoworksandourstudyis
that they analysed bugs inthe frameworks themselves while we
focus on the faults experienced when building DL systems that use
aspeciﬁc framework.
Zhangetal.[30]studiedanumberof DLapplicationsdeveloped
using TensorFlow. They collected information about 175 Tensor-
Flow related bugs from SO and GitHub. Manual examination of
thesebugsallowedtheauthorstodeterminethechallengesdevelop-
ersfaceandthestrategiestheyusetodetectandlocalisefaults.The
authors also provide some insight into the root causes of bugs andinto the consequences that bugs have on the application behaviour.
Theauthorswereabletoclassifytheirdatasetintosevengeneral
kindsof root causes andfourtypes of symptoms.
In our study, we analyse DL applications that use the most pop-
ularDLframeworks[ 2],TensorFlow,PyTorchandKeras,notjust
theformer.Thepopularityofthesethreeframeworks(inparticular,
Keras)andtheirstrongprevalenceoverothersimilarproductsal-
lows us to consider them as representative of the current situation
intheﬁeld.Anothermethodologicaldiﬀerenceliesinthemining
of the SO database. Zhang et al.considered SO questions under the
constraint that they had at least one answer, while we analysed
only questions with an acceptedanswer, to be sure that the fault
was investigated indepthandsolved.
AsforGitHub,Zhang etal.usedonly11projectstocollectthe
faults.Afteracomplexﬁlteringandcleaningprocess,wewereable
touse564projects.Forfurthercomparison,Zhang etal.foundin
total175bugs,whichincludethosethatwediscardedasgeneric(i.e.,
nonDLspeciﬁc), whileourtaxonomybears 375DLspeciﬁcfaults
in total. It is diﬃcult to compare the overall number of analysed
artefacts as such statistics are not reported inZhang et al.’s paper.
Lastbutnotleast,wedecidednottolimitouranalysistojustSOand
GitHub:we includedinterviewswithresearchers and practitioners,
whichrevealedtobeakeycontributiontothetaxonomy.Adetailed
comparisonbetweenourandZhang etal.’staxonomyisreported
in Section 5.
Another work worth mentioning is a DL bug characterisation
by Islam et al.[17]. The aim of the authors is to ﬁnd what types
of bugs are observed more often and what are their causes and
impacts.They alsoinvestigatedwhether thecollectedissuesfollow
a common pattern and how this pattern evolved over time. The
authors studied a number of SO and GitHub bugs related to ﬁve
DLframeworks:Theano,Caﬀe,Keras,TensorFlowandPyTorch.To
perform their analysis, the authors labeled the dataset according to
aclassiﬁcation systemadapted from the 1984work byBeizer [ 13].
Forthecategorisationofthebugcauses,theauthorsadoptedthelist
ofrootcausesfromZhang etal.[30].Diﬀerentlyfromus,Islam etal.
didnothavetheaimofbuildingacomprehensivefaulttaxonomy.
Instead,theyperformedananalysisofvariousfaultpatternsand
studiedthecorrelation/distributionofbugsindiﬀerentframeworks,
reusingexisting taxonomies available in the literature.
2.2 Interviewswith Practitioners
Oneofthestudiesthatprovidessomeinsightintochallengesindus-
trialpractitionersfacewhiledevelopinganddeployingML-based
applicationsisby Lwakatare et al.[19]. Similarlyto our approach,
theauthorsconductedsemi-structuredinterviewstocollectdataof
interest.Basedon12interviews andaworkshopheldwithpracti-
tioners from six projects, a taxonomy that represents the evolution
stagesoftheuseofMLcomponentsinindustrialpracticewasob-
tained. The resulting taxonomy consists of ﬁve stages that include:
Experimentation and Prototyping ,Non-critical Deployment, Critical
Deployment, CascadingDeployment,and AutonomousMLCompo-
nents. The challenges associated with these stages fall into four
broadcategories: Assembledataset, Createmodel, Trainandevaluate
model, and Deploy model. For each of the categories, the authors
provide from 1 to 3 descriptionsof possible challenges developers
1111Taxonomyof Real Faults in Deep Learning Systems ICSE ’20, May 23–29, 2020,Seoul,South Korea
may experience on a particular evolution stage. Although some
of the challenges cover the leaf nodes in our taxonomy (namely,
imbalancedtrainingset andnotenoughdata ),therestofthedata
have completely diﬀerentnature andstructure.
In a work conducted in a similar way, Arpteg et al.[12] study
SoftwareEngineeringchallengesassociatedwithbuildingDLap-
plications. They took advantage of direct communications with
employeesfromsevenreal-worldprojects.Theauthorspresenta
list consisting of three main categories: development, production,
andorganisational challenges. Each of the categories consists of
severalsubcategories(12intotal)thatoutlinetheproblematicareas
andaremappedtothecasestudies,basedonwhethertheassociated
challenges were experiencedornot.
Fromtheprovidedinformation, CulturalDiﬀerences andEﬀort
estimation appeartobethemostprevalentchallengesamongthe
organisation related problems, while TestingandDependency man-
agement are the most frequent for development and production
stages,respectively.Thepresentedclassiﬁcationprovidesahigh-
level overview of theproblematic aspects of the development and
production process,while our study focuses onDL faults.
3 METHODOLOGY
3.1 Manual Analysis ofSoftwareArtefacts
Toderiveourinitialtaxonomyweconsideredthethreemostpopular
DL frameworks [ 2], TensorFlow, Keras and PyTorch. We manually
analysedfoursourcesofinformation:commits,issues,pullrequests
(PRs)fromGitHubrepositoriesusingTensorFlow,Keras,orPyTorch,
andSO discussionsrelatedto the three frameworks.
3.1.1 Mining GitHub. We used the GitHub search API [ 5] to iden-
tifyrepositoriesusingthethreeDLframeworkssubjectofourstudy.
TheAPI takesas aninput asearchstringandfetches sourcecode
ﬁles from GitHub repositories that match the search query. For
example,inPython,TensorFlowcanbeimportedbyusingthestate-
mentimport tensorflow as tf .Thus,weusedthesearchstring
“tensorﬂow” toidentifyallPythonﬁlesusing TensorFlow.Clearly,
this may result in a number of false positives, since the string “ten-
sorﬂow” may be presentinside a source ﬁleforotherreasons(e.g.,
as part of a String literal). However, the goal of this search is only
toidentify candidate projectsusingthethreeframeworks,andfalse
positivesareexcludedinsubsequentsteps.Thesearchstringswe
deﬁnedare“tensorﬂow”,“keras”,and“torch”.Welimitedthesearch
to Pythonsourceﬁles using the language:python argument.
While using the GitHubsearchAPI, a single requestcan return
1,000 results at most. To overcome this limitation, we generated
several requests, each having a speciﬁc size range. We used the
size:min..max argument to retrieve only ﬁles within a speciﬁc
sizerange.Inthisway,weincreasedthenumberofreturnedresults
toup1,000 ×n,wherenisthenumberofconsideredsizeranges.For
eachsearchstring,wesearchedforﬁleshavingasizerangingfrom
0to500,000bytes,withastepof250bytes.Overall,we generated
6,000 search requests, 2,000 for eachframework.
ForeachretrievedPythonﬁleweidentiﬁedthecorresponding
GitHub repository, and we extracted relevant attributes such as:
number of commits, number of contributors, number of issues/PRs,numberofstars[ 3],andnumberofforks[ 4].Then,weexcluded:
(i) personal repositories, classiﬁed as those having less than ﬁve
contributors;(ii)inactiverepositories, i.e.,havingnoopenissues;
(iii) repositories with trivial history, i.e.,having less than 100 com-
mits; and (iv) unpopular repositories, that we identiﬁed as those
withless than10 stars and10 forks.
Suchaprocessresultedintheselectionof151TensorFlowprojects,
237Kerasprojects,and326PyTorchprojects.Then,oneoftheau-
thors checked the selected repositories with the goal of excluding
tutorials,books,orcollectionsofcodeexamples,notrepresenting
real software systems used by developers in practice, and false
positives (i.e., projects containing the search strings in one of their
Pythonﬁles but not actually using the relatedframework).
This process left us with 121 TensorFlow, 175 Keras, and 268
PyTorchprojects.Foreachoftheretained564projects,wecollected
issues,PRs,andcommitslikelyrelatedtoﬁxingproblems/discussing
issues.ForissuesandPRs,weusedtheGitHubAPItoretrieveall
those labelled as either bug,defect, orerror. For commits, we
minedthechangelogoftherepositoriestoidentifyallthosehaving
a message that contained the patterns [ 15]: (“ﬁx” or “solve”) and
(“bug” or“issue”or“problem”or“defect” or“error”).
Then, for each framework, we selected a sample of 100 projects
for manual analysis. Instead of applying a random selection, we
selected the ones having the highest number of issues and PRs.
Forframeworksforwhichlessthan100projectshadatleastone
relevant issue/PR, we selected the remaining projects sorting them
by the number of relevant commits (i.e., commits matching the
patterndescribedabove).The100selectedprojectsaccountfora
total of 8,577 issues andPRs and28,423commits.
Before including these artefacts in our study, we manually in-
spected a random sample of 100 elements and found many (97)
false positives, i.e.,issues/PRs/commits that, while dealing with
fault-ﬁxingactivities,wereunrelatedtoissuesrelevanttotheusage
of the underlying DL framework (i.e., were about generic program-
ming bugs). Thus, we decided to perform a further cleaning step to
increasethe chanceofincluding relevant documents intheman-
ual analysis.We deﬁneda vocabulary ofrelevantwordsrelated to
DL (e.g.,“epoch”, “layer”), and excluded all artefacts that did not
contain any of these words. Speciﬁcally, we extracted the complete
listof 11,986stemmed words(i.e., “train”,“trained”,and“training”
were counted only once as “train”) composing the vocabulary of
the mined issues, PRsand commits. For commits, we searched for
the relevant words in their commit note, whereas for issues and
PRs we considered title, description, and all comments posted in
the discussion. We sorted the resulting words by frequency (i.e.,
number of artefacts in which they appear), and we removed the
longtailofwordsappearinginlessthan10artefacts.Thiswasdone
to reduce the manual eﬀort needed to select the words relevant for
DL from the resulting list of words. Indeed, even assuming that
oneoftheautomaticallydiscardedrarewordswererelevantforDL,
this would have resulted in missing at most nine documents in our
dataset. The remaining 3,076 words have been manually analysed:
we split this list into ﬁve batches of equal size, and each batch was
assigned to one author for inspection, with the goal of ﬂagging
the DL relevant words. All ﬂagged words were then discussed in a
meeting among all authors in which the ﬁnal list of 105 relevant
words was deﬁned. The list is available in our replication package
1112ICSE ’20, May 23–29, 2020,Seoul,South Korea Humbato va andJahangirova,et al.
[8], and includes words such as layer,train,tensor. After excluding
allartefactsnotcontainingatleastoneofthe105relevantwords,we
obtainedthe ﬁnal listof commits (1,981),and of issues/PRs (1,392).
3.1.2 Mining Stack Overflow. We used StackExchange Data Ex-
plorer [9] to get the list of SO posts relatedto TensorFlow,Keras
and PyTorch.StackExchange DataExplorer is a web interface that
allows the execution of SQL queries on data from Q&A sites, in-
cluding SO. For each framework, we created a query to get the
list of relevant posts. We ﬁrst checked if the name of a framework
is indicated in the post’s tags. Then, we ﬁltered out posts which
contained the word "how”, "install” or"build” in their title, to avoid
general how-to questions and requests for installation instructions.
We also excluded posts that did not have an accepted answer, to
ensure that we consider only questions with a conﬁrmed solution.
As a result, we obtained 9,935 posts for Tensorﬂow, 3,116 for Keras,
and653forPyTorch.Weorderedtheresultsofeachquerybythe
numberoftimestheposthasbeenviewed.Then,toselecttheposts
that may be addressing the most relevant faults, we selected the
top 1,000 mostviewedposts (overall,2,653 posts).
3.1.3 ManualLabelling. ThedatacollectedfromGitHubandSO
was manually analysed by all authors following an open coding
procedure [ 23]. The labelling process was supported by a web
application that we developed to classify the documents (i.e., to
describethereasonbehindtheissue)andtosolveconﬂictsbetween
the authors. Each author independently labelled the documents
assignedto her by deﬁning a descriptive label of the fault. During
the tagging, the web application shows the list of labels already
created,whichcanbeusedbyanevaluatorshouldanexistinglabel
apply to the fault under analysis. Although, in principle, this is
against the notion of open coding, little is still known on DL faults,
andthenumberofpossiblelabelsmaygrowexcessively.Thus,such
achoice wasmeant tohelp codersuseconsistent namingwithout
introducing substantial bias.
The authors followed a rigorous procedure for handling special
and corner cases. Speciﬁcally, (i) We marked as a false positive any
analysed artefact that either was not related to any issue-ﬁxing
activityorhappenedtobeanissueintheframeworkitselfrather
than in a DL system. (ii) If the analysed artefact concerned a ﬁx,
but the fault itself was not speciﬁc to DL systems, being rather a
commonprogrammingerror(e.g., wrongstoppingconditionina
forloop),wemarkeditas generic.(iii)Iftheartefactwasrelated
to issue-ﬁxing activitiesand itwasspeciﬁc ofDL systems, but the
evaluatorwas not able to traceback the root cause of the issue, the
unclearlabel wasassigned.
Wheninspectingthedocuments,wedidnotlimitouranalysis
by reading only speciﬁc parts of the document. Instead, we looked
attheentireSOdiscussions,aswellastheentirediscussionsand
related code changes in issues and PRs. For commits, we looked at
the commit noteas well as at the code diﬀ.
In cases where there was no agreement between the two evalua-
tors,thedocumentwasautomaticallyassignedbythewebplatform
toanadditionalevaluator.Incaseoffurtherdisagreementbetween
the three evaluators, conﬂicts were discussed and solved within
dedicatedmeetingsamong allauthors.
The labelling process involved six rounds, each followed by a
meetingamongallauthorstodiscusstheprocessandsolveconﬂicts.Table 1: Manual Labelling Process
RoundAnalyse
d
ConﬂictsRelevant New Inner New Leaf
Artefacts toDL Categories Categories
(1st lvl./2ndlvl./3dlvl.)
1 29 3 5 -/-/- 5
2
110 11 16 -/-/- 11
3 134 8 14 -/-/- 5
4 126 11 20 5/7/7 10
5 345 31 46 0/2/1 21
6 315 47 48 0/0/0 13
SubTotal 1059 111 149 5/9/8 65
Interviews 297 6 226 0/2/0 27
Total 1356 117 375 5/11/8 92
Table1reports statisticsfor eachof thesix rounds of labelling, in-
cluding:(i)thenumberofartefactsanalysedbyatleasttwoauthors;
(ii)thenumberofartefactsforwhichconﬂictsweresolvedinthe
followingmeeting;(iii)thenumberofartefactsthatreceivedalabel
identifyingfaultsrelevantforDLsystems;and(iv)thenumberof
new top/inner/leafcategoriesinthe taxonomy of faults.
In the ﬁrst three rounds we deﬁned a total of 21 (5+11+5) leaf
categoriesgroupingthe35DL-relevantartefacts.Withthegrowing
number of categories in round four, we started creating a hierar-
chical taxonomy (see Figure 1), with inner nodes grouping similar
“leafcategories”.
Table1shows the number of inner categories created in the
fourth,ﬁfth,andsixthroundorganisedbylevel(1stlevelcategories
arethemostgeneral).Wekepttrackofthenumberofinnercate-
goriesproducedduringourlabellingprocessanddecidedtostop
theprocess whenwe reached saturation for such inner categories,
i.e.,when a new labelling round did not result in the creation of
any newinnercategoriesin the taxonomy.
In the last two rounds, we increased the number of labels as-
signed to each author. We opted for a longer labelling period be-
causetheprocesswaswell-tunedandtherewasnoneedforregular
meetings.Overall,welabelled1,059documents,and111(10.48%)of
them required conﬂict resolution inthe open discussion meetings.
3.2 DeveloperInterviews
Unliketraditionalsystems,DLsystemshaveuniquecharacteristics,
astheirdecisionlogicisnotsolelyimplementedinthesourcecode,
but also determined by the training phase and the structure of the
DLmodel(e.g., numberoflayers).WhileSOpostsand GitHubarte-
facts are valuablesourcesof information for our study,the nature
oftheseplatformslimitstheissuesreportedtomostlycode-level
problems, hence possibly excluding issues encountered e.g.,during
model deﬁnition or training. To get a more complete picture, we
haveinterviewed20researchers/practitionerswithvariousback-
grounds and levels of expertise, focusing on the types of faults
encountered duringthe developmentof DL-basedsystems.
3.2.1 ParticipantRecruitment. Toacquireabalancedandwideview
on the problems occurring in the developmentof realDLsystems,
we involved two groups of developers: researchers and practition-
ers.Intheformergroup,weconsideredPhDstudents,Post-Docs
andProfessorsengagedinfrequentusageofDLasapartoftheir
research. The second group of interviewees included developers
working inindustryor freelancers,for whomthedevelopmentof
DL applicationswas the main domainof expertise.
Weexploitedthreediﬀerentsourcestoattractparticipants.First,
weselectedcandidatesfrompersonalcontacts.Thisresultedinalist
1113Taxonomyof Real Faults in Deep Learning Systems ICSE ’20, May 23–29, 2020,Seoul,South Korea
of39developers,20ofwhomwerecontactedviae-mail.Wereceived
12 positive responses from 9 researchers and 3 practitioners. To
balancetheratiobetweenresearchersandpractitioners,wereferred
toothertwosourcesofcandidates.OneofthemwasSO,whosetop
answerersareexperiencedDLdeveloperswithprovencapability
to help other developers solve recurring DL problems. To access
thetopanswererswereferredtostatisticsassociatedwiththetag
that represents each of the three frameworks we study. We used
the‘Last30Days ’and‘AllTime ’categoriesoftopanswerersand
extractedthetop10answerersfrombothcategoriesforeachtag
(DLframework), resultingin 60 candidates in total.
As there is no built-in contact form on SO, it was not possible
to get in touch with all of the 60 shortlisted users. We managed to
locateemailaddressesfor17ofthemfrom links to personalpages
that users left on their SO proﬁles. From 17of the contacted users,
wereceived6responses,ofwhich4werepositive(dividedinto3
practitioners and1 researcher).
The other source was Upwork [ 10], a large freelancing platform.
We created a job posting with the description of the interview pro-
cess on the Upwork website. The post was restricted to invited
public.Theinvitedcandidateswereselectedaccordingtothefol-
lowingcriteria:(i)acandidateproﬁleshouldrepresentanindividual
andnotacompany,(ii)thecandidate’sjobtitleshouldbeDL-related,
(iii)thecandidate’srecentlycompletedprojectsshouldmostlybe
DL-related, (iv) Upwork success rate of the candidate should be
higher than 90% and (v) the candidate should have earned more
than10,000USDontheUpworkplatform.From23invitationssent,
5candidatesacceptedtheoﬀer,butoneofthemwaslaterexcluded,
being a manager of a team of developers, and not a developer her-
self.
Overall,theparticipantrecruitmentprocedureleftuswith20suc-
cessfully conducted interviews, equally divided among researchers
andpractitioners(10pergroup).Detailedinformationonthepar-
ticipants’ DL experience is available in our replication package [ 8].
Forwhatconcernsthe‘overallcodingexperience’,amongtheinter-
viewedcandidatesthelowestvalueis2.5yearsandthehighestis
20 years (median=5.4). As for the DL-speciﬁc ‘relevant experience’,
the range is from 3months to 9 years (median=3).
TheintervieweesreportedtousePythonasamainprogramming
languagetodevelopDLapplications,withafewmentionstoMatlab,
R,Java,Scala,C++andC#.ConcerningtheusageofDLframeworks,
TensorFlowwasmentioned12times,Keras11,andPyTorch8times.
Thedomainsofexpertiseoftheintervieweescoverawidespectrum,
from Finance andRobotics to Forensics andMedical Imaging.
3.2.2 Interview Process. Since we are creating a taxonomy from
scratch, rather than classifying issues and problems into some
known structure, the interview questions had to be as generic and
open-endedaspossible. We optedfora semi-structuredinterview
[23],whichcombinesopen-endedquestions(toelicitunexpected
typesofinformation)withspeciﬁcquestions(tokeeptheinterview
within its scope andto aid interviewees withspeciﬁc questions).
Insemi-structuredinterviewstheinterviewerneedstoimprovise
newquestionsbasedontheinterviewee’sanswer,whichmightbea
challengingtask.Therefore,itmightbeusefultohaveanadditional
interviewerwhocanaskfollow-upquestions andsupport thepri-
mary interviewer in case of need. For this reason our interviewswere conducted by two authors simultaneously, but with diﬀerent
roles: one led the interview, while the other asked additional ques-
tionsonlywhenappropriate. Eachrolewasperformedbythesame
authorinalltheinterviews. TheworkbyHove etal.[16]shows
that half of the participants in their study talked much more when
the interviews were conducted by two interviewers instead of one.
This was the case also in our experience, as in all the interviews
the secondinterviewer askedat leasttwoadditional questions.
After collecting information about the interviewees’ general
and DL-speciﬁcprogramming experience,we proceeded with the
questionsfrom our interviewguide [8].
Ourﬁrstquestionwasverygeneralandwasphrasedas“What
typesofproblemsandbugshaveyou facedwhendevelopingML/DL
systems?”.Ouraimwiththisquestionwastoinitiatethetopicas
open-ended as possible, allowing the interviewees to talk about
their experience without directing them to any speciﬁc kind of
faults. Then, we proceeded with more speciﬁc questions, spanning
amongverybroadDLtopics,suchastrainingdata,modelstructure,
hyperparameters,lossfunctionandhardwareused.Weaskedthe
interviewees if theyeverexperienced issues and problems related
to these topics and then, if the answer was positive, we proceeded
withmore detailed questionsto understandthe relatedfault.
All of our interviews were conducted remotely (using Skype
videocalls),exceptonewhichwasconductedinperson.Thelength
oftheinterviewsvariedbetween26and52minutes,withanaverage
of 37 minutes. For each interview one of the two interviewers was
also the transcriber. For the transcription, we used Descript [ 1],
an automated speech recognition tool that converts audio/video
ﬁles into text. After the automated transcription was produced, the
transcriber checked itanddidmanual corrections incaseof need.
3.2.3 Open Coding. To proceed with open coding of the tran-
scribed interviews, one moderator and two evaluators were as-
signed to each interview. The moderator was always one of the
interviewers.Theﬁrstevaluatorwastheotherinterviewer,while
thesecondevaluatorwasoneoftheauthorswhodidnotparticipate
in the interview. The role of each evaluator was to perform the
open coding task. In contrast, the moderator’s role was to identify
andresolveinconsistently-labelledfragmentsoftextbetweenthe
evaluators (e.g., diﬀerent tags attached to the same fragment of
text). We decided to involve the interviewers in this task in two
roles (evaluator and moderator) because they were more informed
ofthecontentandcontextoftheinterview,havingbeenexposed
to the informaland meta aspects ofthe communication with the
interviewees.Thesecondevaluatorwhowasnotinvolvedinthe
interview ensuredthe presenceof adiﬀerentpointof view.
Twentyinterviewswereequallydividedamongtheauthorswho
did not participate in the interviewprocess. Each interviewer was
theevaluatorof10interviewsandthemoderatorfortheremain-
ing 10. The open coding was performed manually in the Google
Docs online tool. Overall, 297 pieces of text were tagged by the
evaluators.Amongthem,therewereonly6casesofconﬂict,where
the evaluators attached diﬀerent tags to the same fragment of text.
Moreover, there were 196 cases when one evaluator puta tag on a
fragmentoftext,whiletheotherdidnot.Amongthesecases,146
werekeptbythemoderators,whiletherestwerediscarded.Asa
result of this process, 245 ﬁnal tags were extracted. The number of
1114ICSE ’20, May 23–29, 2020,Seoul,South Korea Humbato va andJahangirova,et al.
tags per interview ranged between 5 and 22, with an average of 12
tags.
Once the open coding of all interviews was completed, a ﬁnal
meetingwithalltheauthorstookplace.Atthismeeting,authors
wentthroughtheﬁnallistoftags,focusinginparticularonthetags
thatweredeemednotrelatedtoissuesandproblemsinDLsystems,
but rather had a more general nature. After this discussion, 19 tags
wereremoved,leavingtheﬁnal226tagsavailableforthetaxonomy.
3.3 Taxonomy Construction andValidation
To build the taxonomy we used a bottom-up approach [ 29], where
weﬁrstgroupedtagsthatcorrespondtosimilarnotionsintocate-
gories.Then,wecreatedparentcategories,ensuringthatcategories
andtheir subcategoriesfollow an “is a” relationship.Eachversion
ofthetaxonomywasdiscussedandupdatedbyallauthorsinthe
physicalmeetingsassociatedwiththetaggingrounds.Attheend
oftheconstructionprocess,inaphysicalmeetingtheauthorswent
together through all the categories, subcategories and leaves of the
ﬁnal taxonomy for the ﬁnal (minor) adjustments.
Toensurethattheﬁnaltaxonomyiscomprehensiveandrepresen-
tative of real DL faults, we validated it through a survey involving
a new set of practitioners/researchers, diﬀerent from those who
participatedin the interviews.
To recruit candidates for the survey, we adopted the same strat-
egy and selection criteria as the one we used for the interview
process (Section 3.2.1). The ﬁrst group of candidates we contacted
was derived from authors’ personal contacts. We contacted 23 indi-
vidualsremainingfromourinitiallistand13ofthemactuallyﬁlled
thesurvey.Thesecondandthethirdgroupofcandidatescamefrom
SOandUpwork,respectively.FromtheSOplatformweselectedthe
top20answerers fromthe‘Last30Days’ and‘Alltime’categories
for each of the three considered frameworks. By the time we were
performing the survey, these two categories had partly changed
in terms of the featured users, sothere were new users alsoin the
top 10 lists. From the set of 120 users we discarded those who had
beenalreadycontactedfortheinterviews.Amongtheremaining
candidates, we were able to accesscontactdetails ofonly 20users.
We contacted all of them and 4 have completed the survey. For the
Upwork group, we created a new job posting with a ﬁxed payment
of 10 USD per job completion and sent an oﬀer to 26 users. Four
of them completed the survey. Overall, 21 participants took part in
oursurvey(10researchersand11practitioners),withaminimum
overall coding experience of 1 year and a maximum of 20 years
(median=4). Concerning the relevant DL experience, the minimum
was1 year andthe maximum 7years (median=3).
To create our survey form we used Qualtrics [ 7], a web-based
tool to conduct survey research, evaluations and other data collec-
tion activities. We started the survey with the same background
questions as inour interviews. Then, we proceededwith the ques-
tionsrelatedtoourﬁnaltaxonomy.Puttingthewholetaxonomy
structureinasingleﬁgureofthesurveywouldmakeitoverlycom-
plicated to read and understand. Therefore, we partitioned it by
innercategories,choosingeitherthetopmostinnercategory,when
itwas not toolarge,orits descendants, when itwasalarge one.
Foreachinnercategorythatpartitionedthetaxonomy,wecre-
ated a textual description including examples of its leaf tags. Inthe survey form, we presented the name of the category, its textual
description,andthenthreequestionsassociatedwithit.Theﬁrst
questionwasa“yes”or“no”questiononwhethertheparticipant
hadeverencountered thisproblem.Incaseof positiveanswer,we
had twomoreLikert-scalequestions ontheseverityoftheissue
and the amount of eﬀort required to identify and ﬁx it. In this way
we evaluated not only the mere occurrence of a taxonomy fault,
but alsoits severityas perceivedby developers.
In the ﬁnal partof our survey, in the form of a free-text answer,
weaskedtheparticipantstolistproblemsrelatedtoDLthatthey
haveencounteredbutwhichhadnotbeenmentionedinthesurvey.
Bydoingthiswecouldcheckwhetherourtaxonomycoveredall
the faults in the developer’s experience, and if it did not, we could
ﬁnd outwhat ismissing.
4 RESULTS
Thematerialusedtoconductour studyandthe(anonymised)col-
lecteddata are publiclyavailable for replication purposes [8].
4.1
The Final Taxonomy
Thetaxonomyisorganisedinto5toplevelcategories,3ofwhich
arefurtherdividedintoinnersubcategories.Thefulltaxonomyis
shown in Figure 1. The two numbers separated by a plus sign after
eachcategorynamerepresentthenumberofpostsassignedtosuch
a category during manual labelling and the number of occurrences
of such atag in the interviews after open coding, respectively.
Model.Thiscategoryofthetaxonomycoversfaultsrelatedto
the structure andpropertiesof aDL model.
Model Type & Properties. This category considers faults aﬀecting
themodelasawhole,ratherthanitsindividualaspects/components.
One such fault is a wrong selection of the model type, for example,
when a recurrent network was used instead of a convolutional
network for a task that required the latter. In addition, there are
several cases ofincorrect initialisation of a model,which resultin
theinstabilityofthegradients.Anothercommonpitfallfromthis
categoryisusingtoofewortoomanylayers,causingsuboptimal
network structure, which in turn leads to poor performance of the
model.Anexamplewasprovidedbyoneofourinterviewees:"when
we started, we were thinking that we needed at least four layers in
the encoder and the decoder and then we ended up having half of
them, like actually very shallow model and it was even better than
the bigger deeper model ".
Layers.Faultsinthiscategoryaﬀectaparticularlayerofaneural
network.Thisisalargetaxonomycategorythatwasfurtherdivided
intothe three innersubcategoriesdescribedbelow:
•Missing/Redundant/Wrong Layer. These faults represent cases
where adding, removing or changing the type of a speciﬁc layer
was needed to remedy the low accuracy of a network. This is
diﬀerentfrom the suboptimalnetwork structure ofModelType &
Properties category,asherethesolutionislocaltoaspeciﬁclayer,
rather than aﬀecting the whole model. An interviewee described
suchafault,whichwasrelated"nottothewrongarchitectureas
whole,butmoreusuallytothewrongtypeoflayer,becauseusually
inourﬁeldpeoplehaveappliedtypeoflayerswhichwerenotsuited
for the type ofinputwhichtheyare processing ".
1115Taxonomyof Real Faults in Deep Learning Systems ICSE ’20, May 23–29, 2020,Seoul,South Korea 
✁
✂
✄
☎
✄
✆
✝
✄
✞
✟
✠
✁
✡
☛
✁
☞
✡
✌
✍
✎
☎✏
✠
✠✑
✒
✠
✁
✓
☎
✎
☎
✔
✕
✝
✍✌
✠
✆
✍✖
✗
✘
✙
✚✛
✜
✢✣
✤
✥✦
✧
★
✩
✩
✪
✫
✧
✙✛
✬
✭
✣
✬
✦
✫
★
✮
✛
✜
✭
✣
✭
✦✯
✰
✱
✲
✳
✴
✲
✵✶
✱
✷
✸
✯
✲
✹
✺
✻
✼✽✾
✿
❀
❁
✫✮
❂
✮
❂
✧✛
❃
❄
✣
✬❅✭
✦❆
❇
❈❉
❊
❋
●
❍
❉
■❏
❑
❇
❍
❉
❑
▲
▼
❉
◆
❖
P
◗
❘
❙
❚❯
❱
❲❳❨
❩✹
✽
✻
✼✽
✺
✿❬
❭
❪
❫
❴
❱
❪
❫
❵
❛❜❝
❛
❭
❪
❫
❵
❛
✹
✻
✼
✽
✿
❞
❡
❢
❣
❤
✐❢
❥
❦
❧♠
❣
♠
♥
♠
♦
❧
♠♣
♦
♥
♠
❢
❣q
r
s
t
✉
❞
❡
❢
❣
❤❞
❦
♠
❤
✈
♥
♣♠
❣
♠
♥
♠
♦
❧
♠
♣
♦
♥
♠
❢
❣q
r
s
✇
✉
❞
❡
❢
❣❤♣
❦
❧
❦
①
♥
♠
❢
❣
❢
②✐
❢
❥
❦
❧q
t
sr
✉
✐
③
❧
♥
♠
④
❧
❦♠
❣
♠
♥
♠
♦
❧
♠
♣
♦
♥
♠
❢
❣
♣❢
②
⑤
⑥
⑥q
r
s
✇
✉♣
③
⑦❢
④
♥
♠
✐
♦
❧❣
❦
♥
❞
❢
❡
⑧♣
♥
❡
③
①
♥
③
❡
❦q
r
s
r⑨
✉❞
❡
❢
❣
❤❣
❦
♥
❞
❢
❡
⑧♦
❡
①
✈
♠
♥
❦
①
♥
③
❡
❦q
✇
s
t
✉❞
❡
❢
❣❤
♥
⑩
④
❦❢
②
♦
①
♥
♠
❶
♦
♥
♠
❢
❣②
③
❣
①
♥
♠
❢
❣q
r
st
✉
❷
❸
❹
❹
❸
❺
❻❹
❼
❽
❾
❷
❿
➀❿➁❾
❸
➂
❿
❾
❸
❼
❺❽
➃
❺➁
❾
❸
❼
❺q
r
s✇
✉
✐
♠
♣
♣
♠
❣
❤
❡
❦
❧
③♦
①
♥
♠
❶
♦
♥
♠
❢
❣②
③
❣
①
♥
♠
❢
❣q
r
s✇
✉➄
➅
➆
➇
➈
➉
➇
➊
➋
➌➍
➎
➏
➊
➐
➑
➍
➉
➒
➑➓
➆
➅
➐
➉
➇
➑
➎
➅
➐
➎
➔
➑
➅q
⑨
s
✇
✉
❞
❡
❢
❣
❤❥
❦
→
❣
❦
❥
♠
❣
④
③
♥♣
✈
♦
④
❦q
t
s
✇
✉
➣
↔
↕
➙
➛
➜
➝
↕
➞➙
➟
➠➟
➡
➢
➤
↕
➥
➢
↕
↕➦
➧
➙
➛➧
➙
➨
↕
➙
➩
↕
➦
➞
➟
➧
↕
➙
➜
➦➦
➜
➡
➤
↔➫
➭
➯
➲
➳❞
❡
❢
❣
❤❥
❦
→
❣
❦
❥❢
③
♥
④
③
♥
♣
✈
♦
④
❦q
➵
sr✉
➄
➅
➆
➇
➈
➸
➑
➺
➇
➑
➸➉
➇
➊
➋
➌
➻
➆
➋
➌
➊
➋
➌➍
➼
➎
➊
➑q
r
s✇
✉
➽
➾
❼
❺
❻
➚
➪
❾
➶
➾
❹
❸
➹➶❽
❼
➾
❿➁
❼❺➂❼
➪
➃
❾
❸
❼
❺❿
➪➪
❿
➘
➶➾q
r
s
r
✉
❧
♦
⑩
❦
❡
♣
➴❥
♠
✐
❦
❣
♣♠
❢
❣
♣✐
♠
♣
✐
♦
♥
①
✈q
✇
s➷
✉❹
➃
➬
❼
➮
❾
❸
❷
❿
➪❺
➃
❷
➬➶
➾
❼
❽❺
➶
➃
➾
❼
❺
❹
❸
❺
❾
➱
➶➪
❿
➘
➶
➾q
✇
s
✃
✉⑦
♠
♦
♣
❣
❦
❦
❥
❦
❥♠
❣
♦
❧
♦⑩
❦❡q
r
s✇
✉
✐
♠
♣
♣♠
❣
❤❥
❦
♣
♥
♠
❣
♦
♥
♠
❢
❣❐
❒
❮
❥
❦
❶
♠
①❦q
r
s✇
✉
♠
❣
①
❢
❡
❡❦
①
♥♣
♥
♦
♥
❦♣
✈
♦
❡
♠
❣
❤q
r
s
✇
✉
❞
❡
❢
❣
❤❡
❦
②
❦
❡
❦
❣
①
❦
♥
❢❐
❒
❮
❥
❦
❶
♠
①
❦q
t
s
✇
✉
✐
♠
♣
♣
♠
❣
❤♥
❡
♦
❣
♣
②
❦
❡
❢
②❥
♦
♥
♦
♥
❢
❐
❒
❮q
r
s
✇
✉❞
❡
❢
❣
❤
❥
♦
♥
♦④
♦
❡
♦
❧
❧
❦
❧
♠
♣
✐
❢
❣❐
❒
❮
♣q
r
s
✇
✉
➁
❿
➪
➪
❸
❺
❻➃
❺❹
➃
➮
➮
❼
➾
❾
➶
❰❼
➮
➶
➾
❿
❾
❸
❼
❺
❹
❼
❺Ï
Ð
ÑÒ
❾
➶
❺
❹
❼
➾
❹q
r
s
✇
✉
➨
↕➙
➩
➤
↔
Ó
➧
↕
➙
➟
↕Ô
ÕÖ
×
➟
➤
➙
Ó↕
↔➧
➙
Ó
➧
Ø
➤
➟
Ù
➤➟
↔
➜
➧
➙
➧
➙
➛
Ú
➟
➤
Ó
➟
➦
↕
↕
➢q
r
s
✇
✉
Û
Ü
Ý
Þ
ß
à
áâã
ä
à
å
ã
å
Þ
æ
å
ç
ç
è
æ
èæ
Üè
Þ
é
ê
å
Ü
ê
ë
Þ
ì
æ
â
Ý
Þí
î
ï
ð
ñ
ò
ï
ð
óq
✇
s
r✉
ô
õ
ö
÷
øù
öú
û
ü
û
ö
÷
ö
ýþÿ
ü
ÿ
ú
 
✁
✂
✄ö
ù
✄
õ
ÿ
ü
û
ö
÷q
r
s✇
✉
❥
❦
④❡
❦
①
♦
♥
❦
❥☎
❒
✆q
r
s
✇
✉
❞
❡
❢
❣
❤
③
♣
♦
❤
❦❢②
♠
✐
♦
❤❦❥
❦
①
❢
❥
♠
❣
❤
☎
❒
✆q
r
s✇
✉
ô
õ
ö
÷
ø
✁
ú
ÿ
ø
✄ö
ýù
✝
ÿ
✞✄
 
ö
✝
þ
✄
õõ
✄
ú
ü
ö
õ
ÿ
ü
û
ö
÷
✟
✠✡q
r
s
✇
✉
✐
♠
♣
♣♠
❣
❤♦
❡
❤
③
✐
❦
❣
♥♣
①❢
④
♠
❣
❤q
r
s✇
✉➄
➅
➆
➇
➈
➌
➑
➇
➍
➆
➅➌
➅
➎
➇
➍
➓
➑
➅
➌
➆☛
☞
✌q
r
s✇
✉
➏
➉
➍
➍
➉
➇
➈
➈
➐
➆
✍
➎
➐✎➎
➅
➉
➎
✍
➐
➑
➍➉
➇
➉
➌
➉
➎
➐
➉
➍
➎
➌
➉
➆
➇q
➵
s
✇
✉
➄
➅
➆
➇
➈
✏
☞
✑➋
➍
➎
➈
➑q
r
✇s
✇
✉
✐
♠
♣
♣
♠
❣
❤
☎
❒
✆①
♦
❧
❧q
r
s✇
✉
ô
õ
ö
÷
øõ
✄
ý
✄
õ
✄
÷
✞
✄
ü
öö
ù
✄
õ
ÿ
ü
û
ö
÷
ÿ
✝ø
õ
ÿ
ù
 q
r
s
✇
✉✒
❨
❵
❛
✓
✯
❳
❛
❩
❵
❨✲
✔
❱
✕
❳
✹
✽
✖
✼
✺✿
✒
❨
❵
❛
✓
✶
❛
✕
❝
❪✹
✻
✽
✼
✖
✺✿ô
õ
ö
÷
ø
ü
✄
÷
ú
ö
õú
 ÿ
ù
✄
✗
✘
û
ú
úû
÷
øú
✙
✁
✄✄
✚
✄
✛q
⑨
s
✇
✉
❞
❡
❢
❣❤
♥
❦
❣
♣
❢
❡♣
✈
♦
④❦
✜
❞
❡
❢
❣
❤♠
❣❥
❦
✢
♠
❣
❤
✣q
t
s
✇
✉
ô
õ
ö
÷
ø
ü
✄
÷
ú
ö
õú
 
ÿ
ù
✄
✗
ô
õ
ö
÷
øö
✁
ü
ù
✁
üù
ÿ
þ
þ
û
÷
ø
✛q
r
s
✇
✉
❞
❡
❢
❣
❤
♥
❦
❣
♣
❢
❡♣
✈
♦
④❦
✜
❢
♥
✈
❦
❡
✣q
r
➵
s
➵
✉
♥
❦
❣
♣
❢
❡
♣
✈
♦
④
❦✐
♠
♣
✐
♦
♥
①
✈q
✇
s
t
✉
✤
✥
✦
✧★
✩
✪
✫
✬
✭
✦
✮✯
✧
✬
✰✱
✲
✫
✱
✫
✳
✴
✴
✵
✶
✷✸
❑
❇
✹✺
❋
●
❍
❉
❇
✻✼
✹
❍
✽
▲✾
✿
▲
✿
❖
❀
◗
❁
❚✒
❨
❵
❛
✓
✶
❛
✕
❝
❪❜
❵
❨
❂
❱
❪
✹
✺
✼✺✿
❞
❡
❢
❣❤
♣
✈
♦
④
❦❢②
♠
❣④
③
♥
❥
♦♥
♦②
❢
❡
♦
✐
❦
♥
✈
❢
❥q
✃
s
✇
✉
❞
❡
❢
❣
❤
♣
✈
♦
④
❦❢
②
♠
❣④③
♥
❥
♦
♥
♦②❢
❡
♦
❧
♦
⑩
❦
❡q
r
✃s
t
✉
❞
❡
❢
❣
❤
♣
✈
♦
④
❦❢②
♠
❣④
③
♥
❥
♦
♥
♦q
✇
s
⑨
✉❞
❡
❢
❣
❤
♥
⑩
④
❦❢②
♠
❣④
③
♥
❥
♦♥
♦②❢
❡
♦
✐
❦
♥
✈
❢
❥q
❃
s
✇
✉
❞
❡
❢
❣
❤
♥
⑩
④
❦❢
②
♠
❣
④③
♥
❥
♦
♥
♦②❢
❡
♦
❧
♦
⑩
❦
❡q
r
s
✇
✉
❞
❡
❢
❣
❤
♥
⑩
④
❦❢②
♠
❣④
③
♥
❥
♦♥
♦q
✇
s➵
✉❞
❡
❢
❣
❤
♠
❣
④③
♥②
❢
❡
✐
♦
♥q
r
s
⑨
✉
❞
❡
❢
❣❤
♠
❣
④
③
♥②
❢
❡
✐
♦
♥
②
❢
❡❄
⑥
⑥q
t
s
✇
✉
❞
❡
❢
❣
❤
②
❢
❡
✐
♦
♥❢②
④
♦
♣
♣
❦
❥❞
❦
♠
❤
✈
♥
♣q
r
s✇
✉
♠
❣
①
❢
✐
④
♦
♥
♠
⑦
❧
❦♥
❦
❣
♣
❢
❡
♥
⑩
④
❦q
r
s✇
✉
❅
❲
✕
❳
❨
✕
❱
❨
❱
❂
❳
❪
❳
❨
❩✹
✖
✾
✼
✽
❆
✿♣
③
⑦
❢
④
♥
♠
✐
♦
❧❣
③✐
⑦
❦
❡
❢
②❦
④❢
①
✈
♣q
t
s
❃
✉❥
♦
♥
♦
⑦
♦
♥
①
✈
♠
❣
❤❡
❦
❇
③
♠
❡
❦
❥q
t
s
❈
✉
♣
③
⑦❢④
♥
♠
✐
♦
❧⑦
♦
♥
①
✈
♣
♠
❉
❦q
t
s
r✉
❊
❋
●
❍■❏
❑▲▼
◆
❏
❖
▼
❖
❍
P
❖
◗◗
❘
P
❘
❙
❘
P
❚
❯
▲
❍■q
r
s
✇
✉
❱
❲
❳
❳❲
❨
❩❬
❭
❩
❪❫
❴
❬
❲
❳
❴
❵
❲
❛
❨❜
❫
❛
❳
❳
❴
❨
❝❞
❭
❲
❩
❡
❵
❢q
✇
sr
✉♣
③
⑦
❢
④
♥
♠
✐
♦
❧❧
❦
♦
❡
❣
♠
❣
❤
❡
♦
♥
❦q
r
s⑨
✉Ó
➞
❣
↕
➢
➟
➧
➝
➜
➦Ù
➡
➢
➤
↔
❤➢
➜↔
➜
➝
➤
➟
➤
↔
Ó➟
➞
➙
➧
➙
➛q
t
s✐
✉❯
❵❩
❩
❜
❝
❛❭
❪
❫
❵
❛✹
❥
✼
✖
❆
✿❷
❸
❹
❹❸
❺
❻❷
❿
❹
❦
❸
❺
❻
❼
❽❸
❺
➂
❿
➪
❸
❰
➂
❿
➪
➃
➶
❹❾
❼
➹
➶
➾
❼q
r
s✇
✉❞
❡
❢
❣
❤
❧
❢
♣
♣②
③
❣
①
♥
♠
❢
❣①
♦
❧
①
③
❧
♦♥
♠
❢
❣q
⑨
s❃
✉
✐
♠
♣
♣♠
❣
❤
❧
❢
♣
♣②
③
❣
①
♥
♠
❢
❣q
✇
sr
✉➄
➅
➆
➇
➈➍
➑
➐
➑
❧
➌
➉
➆
➇
➆
➓➐
➆
➍
➍
➓
➋
➇
❧
➌
➉
➆
➇q
r
srr
✉♠
✿
❊
▼
❈
✿
▲
▼
❇
✹
♥
❋
❉
◆
▲
▼
✹
✺❖
❘
◗
♦
❚✐
♠
♣
♣
♠
❣
❤❶
♦
❧
♠
❥
♦♥
♠
❢
❣
♣
❦
♥q
r
s
✇
✉
♠
❣
①
❢
❡
❡❦
①
♥♥
❡
♦
♠
❣♣
♥
❦
♣
♥❥
♦
♥
♦
♣
④
❧
♠
♥q✇
s
➵
✉❞
❡
❢
❣
❤④
❦
❡
②
❢
❡
✐
♦
❣
①
❦✐
❦
♥
❡
♠
①q
r
sr✉
q
r
s
t
r
✉
✈
s
✇
✇
①
②
③
✉
④⑤
r
⑥
①
②
①
②
③⑦
⑥
⑧
⑥
⑨
⑩
❶
❷
❶
❸❹❺
❻
❼
❼
❻
✧
★❽
✥
✭
✬
✥
✦
❾
✭
❼
❼
❻
✧
★
✳
❿
❿
✵
✴
✴
✷✐
♠
♣
♣
♠
❣
❤
④
❡
❦
④
❡
❢
①
❦
♣
♣
♠
❣
❤♣
♥
❦
④
✜
♣
③
⑦
♣
♦
✐
④
❧
♠
❣
❤
➀❣
❢
❡
✐
♦
❧
♠
♣
♦
♥
♠
❢
❣
➀♠
❣
④
③
♥
♣
①
♦
❧
♠
❣
❤
➀❡
❦
♣
♠
❉
❦
❢
②
♥
✈
❦
♠
✐
♦
❤
❦
♣
➀❢
❶
❦
❡
♣
♦
✐
④
❧
♠
❣❤
➀❦
❣
①
❢
❥
♠
❣
❤
❢
②①
♦
♥
❦
❤
❢
❡
♠
①
♦
❧❥
♦
♥
♦
➀
④
♦
❥❥
♠
❣
❤
➀➁
➁
➁
♣
⑧
♠
④
➁
➁
➁
➀❥
♦
♥
♦
♣
✈
③
➂
♠
❣❤
➀♠
❣
♥
❦
❡
④
❢
❧
♦
♥
♠
❢
❣
✣
➃
➄
➅
➆
➇
➈
➄
➉
➊
➄
➅
➋
➉
➌
➌
➍
➆
➇➎
➏
➐
➑
➒
➓❞
❡
❢
❣
❤
④
❡
❦
④
❡
❢
①
❦
♣
♣
♠
❣
❤♣
♥
❦
④
✜
④
♠
✢
❦
❧
❦
❣
①
❢
❥
♠
❣
❤
➀④
♦
❥❥
♠
❣
❤
➀♥
❦
✢
♥
♣
❦
❤
✐
❦
❣
♥
♦
♥
♠
❢
❣➀❣❢❡
✐
♦
❧
♠
♣
♦
♥
♠
❢
❣
➀➁
➁
➁
♣
⑧
♠
④
➁
➁
➁
➀④
❢
♣
♠
♥
♠
❢
❣
♦
❧
❦
❣
①
❢
❥
♠
❣
❤➀①
✈
♦
❡
♦
①
♥
❦
❡
❦
❣
①
❢
❥
♠
❣
❤
✣✳
✕
❪
❫
❂
❫
❩
❳
❨
✹
✻
✼✻✿❞
❡
❢
❣❤❢
④
♥
♠
✐
♠
♣
♦
♥
♠
❢
❣②
③
❣
①
♥
♠
❢
❣q
r
s
➵
✉
✄
ù
ú
û
✝
ö
÷
ý
ö
õ✟
þ
ÿ
✘ö
ù
ü
û
✘
û
ú
✄
õ
ü
ö
ö✝
ö
ôq
t
s
✇
✉
✯
❨
❱
❫
❛
❫
❛
✓
➔
❱
❪
❱→
❝
❱
➣
❫
❪
❲
✹✽
✼
❆
✾
✿❧
❢
❞
❇
③
♦
❧
♠
♥
⑩♥
❡
♦
♠
❣
♠
❣❤
❥
♦
♥
♦q
✇
s
r
r
✉❣
❢
♥
❦
❣
❢③❤
✈♥
❡
♦
♠
❣
♠
❣❤
❥
♦
♥
♦q
✇
sr
❃
✉
ö
↔
✄
õ
✝
ÿ
ù
ù
û
÷
øö
✁
ü
ù
✁
ü✞
✝
ÿ
ú
ú
✄
ú
û
÷ü
õ
ÿ
û
÷
û
÷
ø
þ
ÿ
ü
ÿq
✇
s
r✉
♥
❢
❢
✐
♦
❣
⑩❢
③
♥
④
③
♥①
♦♥
❦
❤
❢❡
♠
❦
♣q
✇
s
r✉
♣
✐
♦
❧
❧
❡
♦
❣
❤
❦
❢
②❶
♦
❧
③
❦
♣
②
❢
❡
♦②
❦
♦
♥
③❡
❦q
✇
s
t
✉③
❣
⑦
♦
❧
♦
❣①
❦
❥♥
❡
♦
♠
❣
♠
❣❤
❥
♦
♥
♦q
✇
s
rr
✉❞
❡
❢
❣❤♣
❦
❧
❦
①
♥
♠
❢
❣
❢
②②
❦
♦
♥
③❡
❦
♣q
r
s✃
✉❞
❡
❢
❣❤
❧
♦
⑦
❦
❧
♣②
❢
❡
♥
❡
♦
♠
❣
♠
❣
❤❥
♦♥
♦q
r
sr
t
✉
❥
♠
♣
①
♦
❡
❥
♠
❣
❤♠
✐
④
❢
❡
♥
♦
❣
♥②
❦
♦
♥
③❡
❦
♣q
✇
s
t
✉✯
❨
❱
❫
❛
❫
❛
✓
✷
❨
❵
❭
❳
❩
❩✹
✾
✼
✖↕
✿➝
↕
Ø
➤
➦
➟
↕
↕
❣
➧
➛
➟
↕➙
➟
➧
➙
➟
↕
➜
➩
➜
➧
➦
➜
❣
➦
➤➝
➤
➝
↕
↔
➡q
✇
s
⑨
✉➅
➑
➓
➑
➅
➑
➇
❧
➑
➓
➆
➅➇
➆
➇
➛
➑
➜
➉
➍
➌
➉
➇
➈❧
➼
➑
❧
➝
➊
➆
➉
➇
➌q
✇
sr
✉
✐
♠
♣
♣
♠
❣
❤
❥
♦
♥
♦♦
③
❤
✐
❦
❣
♥
♦
♥
♠
❢
❣q
✇
s➵
✉
❡
❦
❥
③
❣❥
♦
❣
♥❥
♦♥
♦♦
③
❤
✐
❦
❣
♥
♦
♥
♠
❢
❣q
✇
s
r
✉➽
➾
❼
❺
❻❷
❿
❺❿
❻
➶
❷
➶
❺❾
❼
❽❷
➶
❷
❼
➾
➘➾
➶
❹
❼
➃
➾
➁
➶
❹q
✇
s❃
✉✐
♠
♣
♣♠
❣
❤❥
❡
❢
④
❢
③
♥
❧
♦
⑩
❦❡q
r
s
r
✉
➏
➉
➍
➍
➉
➇
➈➇➆
➅
➏
➎
➐
➉
➍
➎
➌
➉
➆
➇➐
➎
➔
➑
➅q
r
s✇
✉
✐
♠
♣
♣
♠
❣
❤♦
❶
❦
❡
♦
❤
❦④
❢
❢
❧
♠
❣
❤
❧
♦
⑩
❦
❡q
✇
s
r✉✐
♠
♣
♣
♠
❣
❤♣
❢②
♥
✐
♦✢❧
♦⑩
❦
❡q
r
s
✇
✉
❡
❦
❥
③
❣❥
♦
❣
♥♣
❢
②
♥
✐
♦
✢❧
♦
⑩
❦
❡q
r
s
✇
✉
❞
❡
❢
❣
❤
❧
♦
⑩
❦
❡♥
⑩
④
❦q
r
s
t
✉
❞
❡
❢
❣❤
♥
⑩
④
❦❢
②
④
❢
❢❧
♠
❣
❤❧
♦
⑩
❦
❡q
✇
sr
✉✐
♠
♣
♣
♠
❣
❤❥
❦
❣
♣
❦
❧
♦
⑩
❦
❡q
r
s
✇
✉✐
♠
♣
♣
♠
❣
❤➞
♦♥
♥
❦
❣
❧
♦
⑩
❦
❡q
r
s
✇
✉➟
➠
➡
➡
➠
➢
➤➥
➦
➧
➨
➩➢
➨
➫
➢
➭
➥➯
➲
➳
➢➤
➵
➫
➸
➧
➲
➺
➻
➼
➽
➾❯
❱
❲
❳
❨
✷
❨
❵
✕❳
❨
❪
❫
❳
❩✹
✖
✻✼
✖➚
✿
➪
➶
Ð
❾
➶
❺
❹
❼
➾
❸
❹➃
❹
➶
❰
❸
❺
❹
❾
➶
❿
❰
❼
❽Ï
➶
Ð
❾
➶
❺
❹
❼
➾q
r
s✇
✉
Figure 1:Final Taxonomy
1116ICSE ’20, May 23–29, 2020,Seoul,South Korea Humbato va andJahangirova,et al.
•Layer Properties. This category represents faults due to some
layer’s incorrect inner properties, such as its input/output shape,
input sample size, number of neurons in it. As per interviewee’s
description,"wesettoolargenumberofneuronsandwehadlike
veryslow training and validation ".
•Activation Function. Another important aspect of a neural net-
workistheactivationfunctionofneurons.Ifnotselectedprop-
erly,itcandramaticallyruinthemodel’sperformance.Oneinter-
viewee noted that "when I changed sigmoid activations into linear
activations in the speech recognition,it gaveme a gain ".
Tensors & Inputs. This category deals with problems related
to the wrong shape, type or format of the data. We encountered
twodiﬀerentclassesof faults inthis category:
Wrong Tensor Shape. A faultybehaviour manifests during some
operationontensorswithincompatibleshapesoronasingletensor
with incorrectly deﬁned shape. As shown in Figure 1, there is a
numberofpossiblecausesforawrongtensorshape, e.g.,missing
outputpadding,missingindexing, or,asitwasprovidedinoneof
the interviews, a case when a developer "was using a transposed
versionofthe tensor insteadofthe normal one ".
WrongInput. Afaultybehaviourisduetodatawithincompatible
format,typeorshapebeingusedasaninputtoalayeroramethod.
A wrong input to a method is a problem frequently observed in
traditional software, as well as in DL programming. However, in
DL, these faults happen to be of a speciﬁc nature, ranging from the
inputhavingunexpecteddatatype(e.g.,string insteadof ﬂoat)or
shape (a tensor of size 5x5instead of 5x10)to cases when the input
hasacompletely wrong format(e.g., awrongdata structure).One
interestingexample ofa wronginput formatwasprovided byour
interviewee: "my data was beingloaded in withchannel access ﬁrst
insteadoflast.Sothatactuallywasasilentbuganditwasrunning
and I actually don’tunderstandhow it evenran butit did ".
Training. This is the largest category in the taxonomy and it
includesawiderangeofissuesrelatedtoallfacetsofthetraining
process, such as the quality and preprocessing of training data,
tuning of hyperparameters, the choice of appropriate loss/opti-
misation function. It also accounts for the faultsoccurring when
testing/validating apreviously trainedmodel.
Hyperparameters. Developersfacealargenumberofproblems
whentuningthehyperparametersofaDLmodel.Themostreported
incorrect hyperparameters are learning rate, databatch size, and
numberofepochs.Whilesuboptimalvaluesfortheseparameters
do not necessarily lead to a crash or an error, they can aﬀect the
trainingtime andthe overallperformance achievedbythe model.
An example from the interviews is "when changing learning rate
from 1 or 2 orders of magnitude, we have found that it impacts the
performance ofabout up to 10%to 15%in termsofaccuracy ".
Loss Function. This category contains faults associated with the
lossfunction,speciﬁcallyitsselectionandcalculation.Wrongse-
lectionofthelossfunctionorusageofapredeﬁnedlossfunction
maynotadequatelyrepresentthe optimisationgoalsthatamodel
is expected to achieve. In its turn, a wrong calculation of a loss
functionoccurswhenacustomlossfunctionisimplementedand
some error in the implementation leads to the suboptimal or faulty
behaviour. As one interviewee noted, they needed "to get a morebalanced loss function than just something that can predict one class
verywell and then screws up the other ones ".
Validation/Testing. It includes problems related to testing and
validating a trained model, such as the bad choice of performance
metrics orfaultysplit ofdata intotraining andtestingdatasets.
PreprocessingofTrainingData. Preprocessingofatrainingdataset
is a labour-intensive process that signiﬁcantly aﬀects the perfor-
mance of a DL system. This is reﬂected in the large number of
elements in this category and in the high variety and number of
its leaves. At the high-level we have separated the faults in this
categoryintotwogroups:missingpreprocessingandwrongprepro-
cessing. The former refers to cases when a preprocessing step that
wouldleadtoabetterperformancehasnotbeenappliedatall.In
thelattercase,thepreprocessingstephasactuallybeenapplied,but
eitheritwasofanunsuitabletypeorwasappliedinanincorrect
way.Examplesofthemostfrequentissuesaremissingnormalisa-
tionstep,missinginputscalingorsubsampling,andwrongpixel
encoding. It is important to remark that preprocessing steps for
training data are heavily dependent on an area of application. This
explains the large variety of leaf tags in this category. We had to
omit some of them from the taxonomy ﬁgure, due to the lack of
space.
Optimiser. Thiscategory isrelatedtotheselection ofan unsuit-
ableoptimisationfunctionformodeltraining.Wrongselectionof
theoptimiser(e.g., Adamoptimiserinsteadofstochasticgradient
descent) or suboptimal tuning of its parameters (too low epsilon for
Adam optimiser ) can ruin the performance of amodel.
Training Data Quality. In this group fallall the aspects relevant
tothequalityoftrainingdata.Ingeneral,issuesoccurduetothe
complexity of the data and the need for manual eﬀort to ensure
ahighqualityoftrainingdata(e.g., tolabelandcleanthedata,to
remove the outliers). More speciﬁc cases of data collection chal-
lenges include privacy issues in the medical ﬁeld and constantly
changinguserinterfacesofwebpages,fromwhichthedataisgath-
eredautomatically.Allofthisleadstothemostfrequentissuein
thiscategory,whichis notenoughtrainingdata.Avariantofthis
problem is unbalanced training data, where one or more classes
inadatasetareunderrepresented.Moreover,togetagoodclassi-
ﬁcation model, it is important to ensure the provision of correct
labelsfortrainingdata.However,intheinterviewees’experience
gettingwronglabelsfortrainingdata isacommonandanannoying
issue. The set of other issues related to the quality of training data,
such as the lack of a standard format, missing pieces of data or the
presence of unrelated data (e.g., images from other domains) are
gatheredtogetherunderarathergeneraltag lowqualityoftraining
data,because speciﬁc issuesdepend onthe area of application.
Training Process. This category representsthe faults developers
faceduringtheprocessofmodeltraining,suchas wrongmanage-
ment of memory resources ormissing data augmentation. It also
containsleavesrepresentingtheexploitationofmodelsthataretoo
big to be ﬁtted into available memory or reference to non-existing
checkpoints during model restoration. Regarding the data augmen-
tation, one of the interviewees noted that it helped "to make the
datamorerealistictoworkbetterinlowlightenvironments",while
the other said that sometimes " you add more pictures to data set "
and as a result you can face "the overﬁtting of the network problem,
so sometimes data augmentation can help, sometimes it can damage ".
1117Taxonomyof Real Faults in Deep Learning Systems ICSE ’20, May 23–29, 2020,Seoul,South Korea
GPUUsage. Thistop-levelcategorygathersallkindsoffaults
related to the usage of GPU devices while working with DL. There
is no further division in this case as all the examples we found
represent very speciﬁc issues.
Some highlights from this category are: wrong reference to GPU
device,failedparallelism, incorrectstatesharingbetweensubprocesses,
faultytransfer ofdatato aGPUdevice.
API.Thispartofthetaxonomyrepresentsabroadcategoryof
problems arising from framework’s API usage. The most frequent
iswrongAPIusage,whichmeansthatadeveloperisusinganAPI
inawaythatdoesnotconformtothelogicsetoutbydevelopers
of the framework. Another illustrating example could be a missing
orwrongly positionedAPI call.
For each of the inner nodes of the resulting taxonomy we have
calculatedthepercentageofcontributingSO/GITfaultsthatarede-
tectableatruntime(i.e.,ledtoacrashorerror).Wedidnotconsider
interviewsinthiscalculationasinmanycasestherewasnoknowl-
edge on whether the fault led to a crash/error or not. As expected,
theTensors & Inputs branch of the taxonomy contained nodes with
thehighestpercentageofsuchfaults,speciﬁcally, WrongInput with
100%, 86%, and 80% for its inner nodes and Wrong Tensor Shape
with 76%. Another category with a high number of crashes/errors
causedbytheassociatedfaultsisthe LayerProperties nodeofthe
Modelbranch(85%) as well as the APIbranch(80%).
4.2 Contributions to theTaxonomy
Theﬁnaltaxonomywasbuiltusingtagsextractedfromtwodiﬀerent
sourcesofinformation:SO&GitHubartefactsandresearcher/prac-
titionerinterviews.Thetop5tagsobtainedfromSO&GitHubwith
theirrespectivenumberofoccurrences(shownas NN+MM,where
NNrefers to SO & GitHub; MMto interviews) are wrong tensor
shape(21+5),wrongshape ofinput dataforalayer (16+2),missing
preprocessing (11+22),wrongAPIusage (10+0)and wrongshapeof
inputdatafor amethod (6+0).
Fortheinterviews,thetop5tagsare missingpreprocessing (11+22),
suboptimalnetworkstructure (1+15),wrongpreprocessing (2+15),not
enoughtrainingdata (0+14)and wronglabelsfortrainingdata (1+12).
These lists have an intersection of only one tag (missing prepro-
cessing). The top 5 SO & GitHub list contains two tags that did not
occur in the other source (wrong API usage, wrong shape of input
dataforamethod ).Thetop5interviewlistcontainsonesuchtag
(not enough training data ). Moreover, the number of occurrences is
unbalanced between the two sources: for the top 5 SO & GitHub
tags, there are 64+29 occurrences, while for the top 5 interview
tagsthe number becomes 15+78. This shows thatthe two selected
sourcesare quitecomplementary.
Indeed, the complementarity between these sources of informa-
tionisreﬂectedintheoveralltaxonomystructure.Ifweconsider
the ﬁve top level categories in the taxonomy (i.e., the ﬁve direct
childrenoftherootnodeinthetaxonomy),wecanﬁndonecate-
gorytowhichinterviewtagshavenotcontributedatall,namely,
theAPIcategory. This might be due to the fact that API-related
problemsarespeciﬁcandtherefore,theydidnotcomeupduring
interviews, where interviewees tended to talk about more general
problems.Similarly,inthe GPUUsage categorythereisonlyonein-
terview tag. Tensors & Inputs is another category dominated by SO&GitHubtags, thenumberofwhichis twicethenumberofinter-
view tags. In contrast, the main contributors to the Modelcategory
are interviews.
The largest diﬀerence is for the Training category, where in-
terviews contributed 4 times more tags, which led to addition of
two more subcategories. The presence of training related faults
onlyintheinterviewsisexpected,asthesetypesofproblemscan
not usually be solved by asking a question on SO or opening an
issue on GitHub. Out of 18 pre-leaf categories, one consists of tags
providedonlybySO&GitHub(API )andanotheroneonlybyinter-
views (TrainingProcess ). Another pre-leaf category (TrainingData
Quality) was abstracted from the few existing leaves only after
collectingmoredatafromtheinterviews.Theremaining16consist
ofdiﬀerentproportionsofthetwosources,with6havinghigher
number ofSO & GitHub tags, 8 higher number of interview tags
and2 the same amount of tags from the twosources.
Overall, the distribution of the tags shows that SO & GitHub
artefactsandresearcher/practitionerinterviewsaretwoverydif-
ferent and complementary sources of information. Ignoring one of
them would provide an incomplete taxonomy, which would not be
representative ofthe full spectrumof real DL faults.
4.3 ValidationResults
The results of the validation survey are summarised in Table 2. For
each category, we report the percentage of “yes” and “no” answers
to thequestion asking whetherparticipants ever encounteredthe
related issues. We also show the perceived severity of each fault
category and the perceived eﬀort required to identify and ﬁx faults
in such a category. There is nocategory of faults that the survey
participantshaveneverencounteredintheirexperience,whichcon-
ﬁrmsthatallthecategoriesinthetaxonomyarerelevant.Themost
approved category is Training Data, with 95% of “yes” answers. Ac-
cordingtotherespondents,thiscategoryhas“Critical”severityand
requires “High” eﬀort for 61% and 78% of participants, respectively.
The least approved category is Missing/Redundant/Wrong Layer,
whichhasbeenexperiencedby24%ofthesurveyparticipants(a
nonnegligiblefractionofalltheparticipants).Acrossallthecate-
gories,theaveragerateof“yes”answersis66%,showingthatthe
ﬁnal taxonomy contains categories that match the experience of a
large majority of the participants (only two categories are below
50%). Participants conﬁrmed, on average, 9.7 categories (out of 15)
acrossallthe surveys.
Table 2:ValidationSurvey Results
CategoryResponse Severity EﬀortRequired
Yes No Minor Major Critical Low Medium High
Hyperparameters 86% 14% 44% 44% 11% 22% 33% 44%
Loss
Function 65% 35% 15% 54% 31% 23% 46% 31%
V
alidation & Testing 60% 40% 33% 42% 25% 50% 17% 33%
Pr
eprocessing of Training Data 86% 14% 56% 17% 28% 28% 39% 33%
Optimiser 57% 43% 75% 25% 0% 58% 33% 8%
T
raining Data 95% 5% 6% 33% 61% 6% 17% 78%
T
raining Process 68% 32% 31% 23% 46% 31% 15% 54%
Mo
del Type & Properties 81% 19% 44% 44% 13% 38% 44% 19%
Missing/Re
dundant/Wrong Layer 24% 76% 60% 40% 0% 60% 40% 0%
Lay
er Properties 76% 24% 44% 44% 13% 63% 31% 6%
A
ctivation Function 43% 57% 33% 67% 0% 67% 22% 11%
W
rong Input 62% 38% 62% 31% 8% 69% 31% 0%
W
rong Tensor Shape 67% 32% 57% 21% 21% 71% 29% 0%
GP
U Usage 52% 48% 55% 18% 27% 55% 18% 27%
API 67% 33% 43% 29% 29% 36% 43% 21%
1118ICSE ’20, May 23–29, 2020,Seoul,South Korea Humbato va andJahangirova,et al.
Someparticipantsprovidedexamplesoffaultstheythoughtwere
not part of the presented taxonomy. Three of them were generic
codingproblems, whileoneparticipantdescribed theeﬀectof the
fault, ratherthanits cause.
Theremainingthreecouldactuallybeplacedinourtaxonomy
under"missing API call", "wrong management of memory resources"
and"wrongselectionoffeatures".Wethinktheparticipantswerenot
able to locate the appropriate category in the taxonomy because
the descriptions in the survey did not include enough exemplar
cases,matching their speciﬁc experience.
5 DISCUSSION
FinalTaxonomyvs.RelatedWork. Toelaborateonthecompar-
ison with existing literature, we analysed the diﬀerences between
ourtaxonomyandthetaxonomyfromtheonlyworkwhereauthors
compiledtheirownclassiﬁcation offaults,ratherthanreusingan
existing one, which is by Zhang et al.[30]. To ease the comprehen-
sion, welist the categorieswiththe exactnamingand excerpts of
descriptionsfrom the corresponding publication [30]:
1. Incorrect Model Parameter or Structure (IPS) - “bugs related
to modelling mistakes arose from either an inappropriate model
parameter like learning rate or an incorrect model structure like
missingnodesorlayers”.Inourtaxonomywedistinguishthese-
lection of an appropriate model structure from the tuning of the
hyperparameters. So, in our taxonomy, class IPS corresponds to
two leaves: suboptimalnetwork structure andsuboptimalhyperpa-
rameterstuning,eachbelongingtoadiﬀerenttop-levelcategory –
ModelandTraining,respectively.
2. Unaligned Tensor (UT) - “a bug spotted in computation graph
construction phase when the shape of the input tensor does not
match what it isexpected”. Class UT can be mapped to the Wrong
Tensor Shape and partly to the Wrong Shape of Input Data (as far as
itconcernstensors) categoriesof our taxonomy.
3.Confusion with TensorFlow ComputationModel(CCM) - “bugs
arisewhenTFusersarenotfamiliarwiththeunderlayingcomputa-
tionmodelassumedbyTensorFlow”.CCMdealswiththedataﬂow
semanticsoftensors,whichmightbeunintuitivetonovices.Thisis
adiﬃcultythatdevelopersfacewhenstartingtoworkwithtensors
in general. We did not gather evidence for this fault because we
excludedexamples, toy programs, andtutorials from our analysis.
Zhangetal.didnotobservethisfaultinGitHub(onlyinSO).As
theyremark:“theycanbecommonmistakesmadebyTFusersand
discussedat StackOverﬂowseeking advice”.
4.TensorFlowAPIChange(APIC) -“anomaliescanbeexhibitedby
aTFprogramuponanewreleaseofTensorFlowlibraries”.Asthese
bugsarerelatedtotheevolutionoftheframework,theyaresimilar
to those that aﬀect any code using third party libraries. Hence, we
regardedthemasgenericprogrammingbugs,notDL-speciﬁcfaults.
5. TensorFlow API Misuse (APIM) - “bugs were introduced by TF
userswhodidnotfullyunderstandtheassumptionsmadebythe
APIs”. This class can be directly linked to the wrong API usage leaf
intheAPIcategory,withtheonlydiﬀerencethatinourcasethe
leafincludes APIs from three,not just one,DL frameworks.
6. Structure Ineﬃciency (SI) - “a major diﬀerence between SI
andIPSisthattheSIleadstoperformanceineﬃciencywhilethe
IPSleadstofunctionalincorrectness”.Thisclassofbugsissimilarto classIPS, diﬀering only in the observable eﬀects (functional
vs eﬃciency problems), which are not taken into account in our
taxonomy (we looked at the root cause of a fault, not at its eﬀects).
So,the mapping is the same as for IPS.
7. Others (O) - “other bugs that cannot be classiﬁed are included
inthistype.Thesebugsareusuallyprogrammingmistakesunre-
latedtoTensorFlow,suchasPythonprogrammingerrorsordata
preprocessing”. Generic programming errors are excluded from
our taxonomy, which is focused on DL-speciﬁc faults. Data prepro-
cessingerrorsinsteadcorrespondtothecategory Preprocessingof
Training Data.
In summary, Zhang et al.’s classes IPS, SI and APIM map to
3 leaf nodes of our taxonomy; classes UT and O map to 3 inner
nodes (although for 2 out of 3 the mapping ispartial). In total (see
Table1), our taxonomy has 24 inner nodes and 92 leaf nodes. So,
our
taxonomy contains 21 inner nodes (out of 24) that represent
newfaultcategorieswithrespecttoZhang etal.’s(19outof24if
wedonotcountdescendantsofmappednodes).Forwhatconcerns
the leaves, the computation is more diﬃcult when the mapping
is partial, because it is not always easy to decide which subset
of leaves is covered by classes UT and O. If we conservatively
overestimatethatallleavesthatdescendfromapartiallymapped
node are transitively covered, Zhang et al.’s classes would cover
13leafnodes,outof92,inourtaxonomy.Thismeansthat79leaf
categories have been discovered uniquely and only in our study.
Ontheotherhand,thetwounmappedclassesbyZhang etal.(CCM
and APIC) correspond to generic programming bugs or bugs faced
bynoviceswhoseekadviceabouttensorcomputationinSO.We
deliberately excludedsuch classesfrom our analysis.
Overall,alargeproportionofinnerfaultcategoriesandofleaf
fault categories in our taxonomy are new and unique to our study,
whichhencerepresentsasubstantial advancement oftheknowl-
edge of real DL faults over the previous work by Zhang et al.
FinalTaxonomyvs.MutationOperators. Mutantsareartiﬁ-
cial faults that are seeded into a program to test under the assump-
tion that fault revelation will translate from mutants to real faults.
TheworksbyMa etal.[20]andShen etal.[25]havemadeinitialat-
temptstodeﬁnemutationoperatorsforDLsystems.Thecombined
listofmutationoperatorsfromtheseworkscanbeclassiﬁedinto
twocategories:(1) Pre-TrainingMutations,appliedtothetraining
dataortothemodelstructurebeforetrainingisperformed;(2) Post-
TrainingMutations thatchangetheweights,biasesorstructureofa
model that has already been trained. For each pre-training mutant,
aftermutationthemodelmustberetrained,whileforpost-training
mutants noretrainingis needed.
Whethermutantsareavalidsubstituteforrealfaultshasbeen
anongoingdebatefortraditionalsoftware[ 11,14,18].Toobtainan
insight on the correspondence between the proposedDL mutants
and real faults inDL systems, we matched the mutation operators
from the literature to the faults in our taxonomy. Table 3lists each
pre-training mutation operator and provides the corresponding
taxonomycategorywhensuchamatchexists.Thisisthecasefor
allpre-training mutation operators except“DataShuﬄe”.
For whatconcerns the post-trainingmutants, there isnosingle
faultinthetaxonomyrelatedtothechangeofmodelparameters
afterthemodelhasalreadybeentrained.Indeed,thesemutation
1119Taxonomyof Real Faults in Deep Learning Systems ICSE ’20, May 23–29, 2020,Seoul,South Korea
Table 3: Taxonomy Tags forMutation Operators from[ 20]
Mutation Operator Taxonomy Category
Data Repetition Unbalancedtraining data
Lab
el Error Wrong labelsfortraining data
Data
Missing Not enoughtraining data
Data
Shuﬄe -
Noise
Perturbation Lowquality oftraining data
Lay
erRemoval Missing/redundant/wrong layer
Lay
erAddition Missing/redundant/wrong layer
A
ctivationFunctionRemoval Missingactivationfunction
operators are very artiﬁcial and we assume they have been pro-
posedastheydonotrequireretraining, i.e.,arecheapertogenerate.
However,their eﬀectiveness is stillto be demonstrated.
Overall,wecannoticethattheexistingmutationoperatorsdo
notcapturethewholevarietyofrealfaultspresentinourtaxonomy,
asoutof92uniquerealfaults(leafnodes)fromthetaxonomy,only
6haveacorrespondingmutationoperator.Whilesometaxonomy
categoriesmaynotbesuitabletobeturnedintomutationoperators,
we think that there is ample room for the design of novel mutation
operators for DL systemsbasedonthe outcome of our study.
StackOverﬂow vs.Real World. ThestudybyMeldrum etal.
[21], which analyses 226 papers that use SO, demonstrates the
growing impact of SO on software engineering research. However,
the authors note that this raises quality-related concerns, as the
utility and reliability of SO is not validated in any way. Indeed,
wecollectedfeedbackonthis issuewheninterviewingthetopSO
answerers. All SO interviewees agreed that the questions asked
onSOarenotentirelyrepresentativeoftheproblemsdevelopers
encounterwhenworkingonaDLproject.Oneintervieweenoted
thatthesequestionsare“somehowdiﬀerentfromthequestionsmy
colleagues will ask me ”, while the other called them “ two worlds,
completely diﬀerent worlds ". Interviewees further elaborated on
whytheythinkthisdiﬀerenceexists.Onereasoningwasthat“most
engineersintheindustryhavemoreexperienceinimplementing,in
tracing the code ”, so their problems are not like “how should I stack
these layers to make a valid model, but most questions on SO are like
model building or why does it diverge kind of questions ”. Another
argumentwasthatthequestionsonSOaremostly“sortofbeginner
questionsofpeoplethatdon’treallyunderstandthedocumentation ”
and that they are asked by people who “are extremely new to linear
algebra and to neuralnetworks ”.
Weaddressedthisissuebyexcludingexamples,toyprogramsand
tutorials from the set of analysed artefacts and by complementing
our taxonomy withdeveloper interviews.
CommonProblems. Ourinterviewswithdeveloperswerecon-
ducted to get information on DL faults. However,due to thesemi-
structured nature of these interviews, we ended up collecting in-
formation on more topics than that. The version incompatibility
between diﬀerent libraries and frameworks was one of intervie-
wees’mainconcerns.Theyalsoexpressedtheirdissatisfactionwith
thequalityof documentation available,with onedevelopernoting
that this problem is even bigger for non computer vision problems.
Anotherfamilyofproblems mentioned very oftenwasthe limited
support of DL frameworks for a number of tasks, such as implemen-
tationofcustomlossfunctionsandofcustomlayers,serialisationof
models, and optimisation of model structure for complex networks.Thelack of tools to support activities such as performance evalu-
ation, combining outputs of multiple models, converting models
fromoneframeworktoanotherwasyetanotherchallengingfactor
according to our interviewees.
6 THREATS TO VALIDITY
Internal. A threat to the internal validity of the study could be
the biasedtagging oftheartefactsfrom SO &GitHub,and ofthe
interviews.Tomitigatethis threat,eachartefactandinterviewwas
labelledbyatleasttwoevaluators.Also,itispossiblethatquestions
asked during the developer interviews might have been aﬀected
by the initial taxonomy based on SO & GitHub tags or that they
have directed theinterviewees towards speciﬁc typesof faults.To
prevent this from happening, we kept the questions as generic and
disjointfromtheinitialtaxonomyaspossible.Anotherthreatmight
berelatedtotheprocedureofbuildingthetaxonomystructurefrom
a set of tags. As there is no unique and correct way to perform this
task,theﬁnalstructuremighthavebeenaﬀectedbytheauthors’
pointof view.For this reason itwas validatedviasurvey.
External . The main threat to the external validity is general-
isation beyond the three considered frameworks, the dataset of
artefactsusedandtheinterviewsconducted.Weselectedtheframe-
works based on their popularity. Our selection was further con-
ﬁrmed by the list of frameworks that developers from both the
interviewsandsurveyhadusedintheirexperience.Tomakethe
ﬁnal taxonomy as comprehensive as possible, we labeled a large
number of artefactsfromSO & GitHubuntil we reached saturation
oftheinnercategories.Togetdiverseperspectivesfromtheinter-
views, we recruited developers with diﬀerent levels of expertise
andbackground, acrossawide range of domains.
7 CONCLUSION
WehaveconstructedataxonomyofrealDLfaults,basedonman-
ual analysis of 1,059 GitHub & SO artefacts and interviews with
20 developers. The taxonomy is composed of 5 main categories
containing375instancesof92uniquetypesoffaults.Tovalidate
the taxonomy, we conducted a survey with a diﬀerent set of 21
developerswhoconﬁrmedtherelevanceandcompletenessofthe
identiﬁed categories. In our future work we plan to use the pre-
sented taxonomy as a guidance to improve DL systems testing and
as asourcefor the deﬁnitionof novel mutation operators.
ACKNOWLEDGEMENTS
ThisworkwaspartiallysupportedbytheH2020projectPRECRIME,
funded under the ERC Advanced Grant 2017 Program (ERC Grant
Agreementn.787703).BavotathankstheSwissNationalScience
foundation for the ﬁnancial support through SNF Project JITRA,
No.172479.
REFERENCES
[1] 2019. Descript. https://www.descript.com
[2]2019. FrameworkData. https://towardsdatascience.com/deep-learning-
framework-power-scores-2018-23607ddf297a
[3] 2019. GitHub- About Stars. https://help.github.com/articles/about-stars/
[4] 2019. GitHub- Forking arepo. https://help.github.com/articles/fork-a-repo/
[5] 2019. GitHubSearchAPI. https://developer.github.com/v3/search/
[6]2019. ISO/PAS21448:2019Roadvehicles—Safetyoftheintendedfunctionality.
https://www.iso.org/standard/70939.html
1120ICSE ’20, May 23–29, 2020,Seoul,South Korea Humbato va andJahangirova,et al.
[7] 2019. Qualtrics. https://www.qualtrics.com
[8] 2019. Replication Package. https://github.com/dlfaults/dl_faults
[9]2019. StackExchange Data Explorer. https://data.stackexchange.com/
stackoverﬂow/query/new
[10] 2019. Upwork. https://www.upwork.com
[11]J. H. Andrews, L. C. Briand, and Y. Labiche. 2005. Is Mutation an Appropriate
ToolforTestingExperiments?.In Proceedingsofthe27thInternationalConference
onSoftwareEngineering (ICSE’05).ACM,NewYork,NY,USA,402–411. https:
//doi.org/10.1145/1062455.1062530
[12]AndersArpteg,BjörnBrinne,LukaCrnkovic-Friis,andJanBosch.2018. Software
engineeringchallengesofdeeplearning.In 201844thEuromicroConferenceon
SoftwareEngineering and AdvancedApplications(SEAA). IEEE,50–59.
[13]Boris Beizer. 1984. Software System Testing and Quality Assurance. Van Nostrand
ReinholdCo., NewYork, NY, USA.
[14]Muriel Daran. 1996. Software Error Analysis: A Real Case Study Involving Real
FaultsandMutations.In InProceedingsofthe1996ACMSIGSOFTInternational
SymposiumonSoftwareTestingand Analysis.ACM Press,158–171.
[15]Michael Fischer, Martin Pinzger, and Harald C. Gall. 2003. Populating a Release
History Database from Version Control and Bug Tracking Systems. In 19th
InternationalConferenceonSoftwareMaintenance (ICSM2003).
[16]Siw Elisabeth Hove andBente Anda.2005. Experiences from ConductingSemi-
structuredInterviewsinEmpiricalSoftwareEngineeringResearch.In Proceedings
ofthe11thIEEEInternationalSoftwareMetricsSymposium(METRICS’05).IEEE
ComputerSociety,Washington,DC,USA,23–. https://doi.org/10.1109/METRICS.
2005.24
[17]Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. 2019. A
ComprehensiveStudyonDeepLearningBugCharacteristics.In Proceedingsof
the201927thACMJointMeetingonEuropeanSoftwareEngineeringConference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE2019).ACM,
NewYork, NY, USA,510–520. https://doi.org/10.1145/3338906.3338955
[18]RenéJust,DarioushJalali,LauraInozemtseva,MichaelD.Ernst,ReidHolmes,and
Gordon Fraser. 2014. Are Mutants a Valid Substitute for Real Faults in Software
Testing?. In Proceedings of the 22Nd ACM SIGSOFT International Symposium
on Foundations of Software Engineering (FSE 2014). ACM, New York, NY, USA,
654–665. https://doi.org/10.1145/2635868.2635929
[19]Lucy Ellen Lwakatare, Aiswarya Raj, Jan Bosch, Helena Holmström Olsson, and
IvicaCrnkovic.2019.Ataxonomyofsoftwareengineeringchallengesformachine
learningsystems:Anempiricalinvestigation.In InternationalConferenceonAgile
SoftwareDevelopment. Springer, 227–243.
[20]LeiMa,FuyuanZhang,JiyuanSun,MinhuiXue,BoLi,FelixJuefei-Xu,ChaoXie,
Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang. 2018. DeepMutation: Mutation
Testing of Deep Learning Systems. In 29th IEEE International Symposium on
Software Reliability Engineering, ISSRE 2018, Memphis, TN, USA, October 15-18,
2018. 100–111. https://doi.org/10.1109/ISSRE.2018.00021
[21]SarahMeldrum,SherlockA.Licorish,andBastinTonyRoySavarimuthu.2017.
Crowdsourced Knowledgeon StackOverﬂow: A Systematic Mapping Study. In
Proceedings of the 21st International Conference on Evaluation and Assessment
inSoftwareEngineering (EASE’17).ACM,NewYork,NY,USA,180–185. https:
//doi.org/10.1145/3084226.3084267
[22]JenniferRowleyandRichardHartley.2017. Organizingknowledge:anintroduction
tomanaging access toinformation. Routledge.
[23]Carolyn B. Seaman. 1999. Qualitative Methods in Empirical Studies of Software
Engineering. IEEETrans. Softw. Eng. 25,4 (July 1999), 557–572. https://doi.org/
10.1109/32.799955
[24]Carolyn B. Seaman, Forrest Shull, Myrna Regardie, Denis Elbert, Raimund L.
Feldmann, Yuepu Guo, and Sally Godfrey. 2008. Defect Categorization: Mak-
ing Use of a Decade of Widely Varying Historical Data. In Proceedings of
the Second ACM-IEEE International Symposium on Empirical Software Engi-
neering and Measurement (ESEM ’08). ACM, New York, NY, USA, 149–157.
https://doi.org/10.1145/1414004.1414030
[25]W.Shen,J.Wan,andZ.Chen.2018. MuNN:MutationAnalysisofNeuralNet-
works. In 2018 IEEE International Conference on Software Quality, Reliability and
SecurityCompanion(QRS-C).108–115. https://doi.org/10.1109/QRS-C.2018.00032
[26]X. Sun, T. Zhou, G. Li, J. Hu, H. Yang, and B. Li. 2017. An Empirical Study on
Real Bugs for Machine Learning Programs. In 2017 24th Asia-Paciﬁc Software
EngineeringConference(APSEC).348–357. https://doi.org/10.1109/APSEC.2017.41
[27]FerdianThung,ShaoweiWang,DavidLo,andLingxiaoJiang.2012. AnEmpirical
Study of Bugs in Machine Learning Systems. In Proceedings of the 2012 IEEE 23rd
International Symposium on Software Reliability Engineering (ISSRE ’12). IEEE
Computer Society, Washington, DC, USA, 271–280. https://doi.org/10.1109/
ISSRE.2012.22
[28]Muhammad Usman, Ricardo Britto, Jürgen Börstler, and Emilia Mendes. 2017.
Taxonomies in software engineering: A systematic mapping study and a revised
taxonomy development method. Information and Software Technology 85 (2017),
43–59.
[29]G. Vijayaraghavan and C. Kramer. [n.d.]. Bug taxonomies: Use them to generate
bettertest. SoftwareTestingAnalysisand Review(STAR EAST) ([n.d.]).[30]YuhaoZhang,YifanChen,Shing-ChiCheung,YingfeiXiong,andLuZhang.2018.
AnEmpiricalStudyonTensorFlowProgramBugs.In Proceedingsofthe27thACM
SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018).
ACM,NewYork, NY, USA,129–140. https://doi.org/10.1145/3213846.3213866
1121