DEEPMETIS: Augmenting a Deep Learning Test
Set to Increase its Mutation Score
Vincenzo Riccio, Nargiz Humbatova, Gunel Jahangirova and Paolo Tonella
Universit `a della Svizzera Italiana
Lugano, Switzerland
Email: name.surname@usi.ch
Abstract —Deep Learning (DL) components are routinely inte-
grated into software systems that need to perform complex tasks
such as image or natural language processing. The adequacyof the test data used to test such systems can be assessed bytheir ability to expose artiﬁcially injected faults (mutations) thatsimulate real DL faults.
In this paper, we describe an approach to automatically
generate new test inputs that can be used to augment theexisting test set so that its capability to detect DL mutationsincreases. Our tool D
EEP METIS implements a search based input
generation strategy. To account for the non-determinism of thetraining and the mutation processes, our ﬁtness function involvesmultiple instances of the DL model under test. Experimentalresults show that D
EEP METIS is effective at augmenting the given
test set, increasing its capability to detect mutants by 63% onaverage. A leave-one-out experiment shows that the augmentedtest set is capable of exposing unseen mutants, which simulatethe occurrence of yet undetected faults.
Index T erms—deep learning, mutation testing, search based
software engineering
I. I NTRODUCTION
Deep Learning (DL) based software is widespread and has
been successfully applied to complex tasks such as image
processing and speech recognition. Systems including DLcomponents are also employed in safety and business-criticaldomains, e.g. autonomous driving and ﬁnancial trading. DLsystems possess the human-like ability to learn how to performa task from experience, i.e., the inputs seen during training [1],but such ability comes with the possibility to make errorswhen presented with new inputs. Therefore, it is crucial for DLsoftware developers and manufacturers to assess to what extentthese systems can be trusted in response to real-world inputs,as they could face scenarios that might be not sufﬁcientlyrepresented in the data from which they have learned.
Traditional test adequacy criteria, like code coverage, fail to
determine whether DL systems are adequately exercised by atest set since most of the DL systems’ behaviour depends ontheir training data, not the code. Recent research deﬁned ad-hoc white-box adequacy metrics, based on DL software’s inter-nal architecture, e.g., neuron [2]–[5] or surprise coverage [6].A limitation of these approaches is that their output cannot bedirectly associated with a root cause of a DL system’s failure,i.e., a DL fault [7].
On the other hand, mutation testing approaches evaluate
a test set against faults that are artiﬁcially injected into thesystem under test. So, the inability of a test set to exposeinjected faults (kill mutants in the mutation testing jargon)
can be interpreted as its inability to properly exercise themutated code [8]. The tool DeepCrime generates mutants of
DL systems by injecting artiﬁcial faults that resemble thosedescribed in the taxonomy of real DL faults by Humbatova etal. [9]. In this way, it addresses the challenge of simulatingreal-world DL faults [10]. Hence, a DL test set that cannot killa mutant generated by DeepCrime is also unlikely to expose
any real fault similar to the one injected by DeepCrime,i n
case such a fault affected the DL system under test. In sucha situation, the test set should be augmented with additionaltests that target the undetected fault.
In this paper, we introduce a novel and automated way
to augment existing test sets with inputs that kill mutantsgenerated by DeepCrime. Our goal is to increase the mutation
score of a test set by generating new inputs that kill themutants not killed by the original test set. To this aim, wepropose D
EEPMETIS, a search-based test generator for DL
systems that uses mutation adequacy as guidance. Intuitively,a mutant is killed if the correct behaviour is observed for aDL model under test, while a misbehaviour is observed on itsmutated version. However, mutation testing approaches shouldtake into account the stochastic nature of DL (in particular,of its training process) and of mutation generation (someDL mutations are non-deterministic) to properly measure thetest set’s ability to discriminate the original system from theartiﬁcially generated faulty versions [11]. In fact, observing adrop in accuracy between the original and the mutated modelis not enough to conclude that the mutant is killed sincesuch a drop might be due to random ﬂuctuations of accuracyassociated with the non-determinism of the training and themutation process. The mutation killing criterion proposed byJahangirova and Tonella [12] addresses this DL-speciﬁc chal-lenge by evaluating a test set on multiple re-trained instancesof the same model and applying statistical tests. D
EEPMETIS
adopts the same non-deterministic view on DL systems, andcorrespondingly, its generation process is guided by multipleinstances of the model being mutated.
Recently, DL-speciﬁc mutation operators have been used
for different tasks, such as program repair [13], adversarialinputs detection [14], generation of adversarial code snip-pets [15], and calculation of optimal oracles for autonomousvehicles [16], but no approach leveraged them to generate newinputs which augment an inadequate test set.
3552021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
DOI 10.1109/ASE51524.2021.000402021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) | 978-1-6654-0337-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/ASE51524.2021.9678764
978-1-6654-0337-5/21/$31.00  ©2021  IEEE
We evaluated D EEPMETIS on both a classiﬁcation problem
and a regression problem, using mutation operators provided
byDeepCrime. Results show that D EEPMETIS is effective
at generating inputs that improve a test set in terms of itsmutation killing ability. We also conducted a leave-one-outexperiment to simulate a practical usage scenario where anundetected fault affecting the DL system is unknown. In thisexperiment setting, one mutant produced by DeepCrime is
taken apart, while test augmentation is performed by D
EEP-
METIS based only on the remaining mutants. In this way, the
left-out mutants simulate a yet unknown fault. Results showthat left out mutants can be killed by the augmented test seton average 82% of the time.
II. B
ACKGROUND
A. Mutation Testing of DL Systems
Mutation testing is a technique that injects artiﬁcial faults
into a system under test guided by the assumption that theability to expose such artiﬁcial faults translates into the abilityto expose also real faults. In traditional software systems, themain decision logic of a program is implemented in its sourcecode, and synthetic faults are introduced by applying smallsyntactic changes to the source code. In contrast, the behaviourof a DL system is determined not only by the source code butalso by its training data, the structure of its neural networksor the tuning of various hyperparameters. As syntactic codechanges are not sufﬁcient to achieve realistic fault injection,DL mutation operators have a different nature [12].
DeepMutation [17] and MuNN [18] were the ﬁrst works to
recognise the need for mutation operators tailored speciﬁcallyto DL systems. In DeepMutation (later extended into a toolcalled DeepMutation++ [19]), the authors propose a set of
operators of two distinct categories: source level and model
level operators. Source level operators apply changes to train-
ing data or model structure before training is performed, whilemodel level operators alter weights, biases or the structureof an already trained model. Model level mutation operatorstend to be less costly as, unlike source-level operators, they donot require re-training. Mutation operators proposed in MuNN[18] solely belong to the latter category.
Jahangirova & Tonella [12] performed an extensive empiri-
cal evaluation of the mutation operators proposed in DeepMu-tation++ and MuNN and investigated the conﬁguration spaceof their parameters. For example, for a mutation operator thatimitates training with corrupted data by changing the labelsof training inputs to incorrect ones, the parameter would bethe percentage of mutated labels. According to their results,the choice of the parameter values affects the impact of themutation to a major extent.
Moreover, the authors propose a novel mutation killing
criterion, which takes into account the stochastic nature of DLsystems. Their deﬁnition requires multiple re-trainings of boththe original program and the mutant to obtain ndistinct model
instances of each (n =2 0 in their experiments). Then, they
measure whether the difference between 20 accuracies (or anyother quality metrics) obtained on original vs mutated modelTABLE I
MUTATION OPERATORS PROVIDED BY DeepCrime [20] AND NOT KILLED
BY THE INITIAL TEST SETS OF OUR CASE STUDIES
Group Mutation Operator Mutation Parameters
Training DataChange labels of training data (TCL)label to perform the mutation on
percentage of data to mutate
Remove portion of training data (TRD) percentage of data to delete
Unbalance training data (TUD) percentage of data to remove
Add noise to training data (TAN) percentage of data to mutate
Make output classes overlap (TCO) percentage of data to mutate
HyperparamsDecrease learning rate (HLR) new learning rate value
Change number of epochs (HNE) new number of training epochs
ActivationChange activation function (ACH)layer w/ non-linear activ. function
new activation function
Remove activation function (ARM) layer w/ non-linear activ. function
Add activation function to layer (AAL)layer w/ linear activation function
new activation function
Regularisation Add weights regularisation (RAW)layer w/o weights regularisation
new weights regulariser
Weights Change weights initialisation (WCI)layer to perform the mutation on
new weights initialiser
Optimisation Change optimisation function (OCH) new optimisation function
instances is statistically signiﬁcant (p value<0.05) and
whether the effect size is not “negligible”. If these conditionshold, the mutation is considered killed.
B. DeepCrime
DeepCrime [20] is a mutation testing tool designed for auto-
mated seeding of artiﬁcial faults (mutations) into DL systems.Its main difference from DeepMutation++ is that DeepCrime
is based on a set of mutation operators derived from real
faults.I nDeepCrime the authors propose 35 and implement
24source level mutation operators that target different aspects
of the development and training of DL systems. This set ofoperators was extracted from an existing taxonomy of realfaults in deep learning systems [9] and was complementedwith the issues found in the replication packages for the studiesby Islam et al. [21] and Zhang et al. [22]. To establish whethera mutation is killed or not, DeepCrime incorporates the notion
of statistical killing proposed by Jahangirova & Tonella [12],using by default 20 re-trainings for the original model and foreach of the applied mutations.
The mutation operators in DeepCrime have two types of
parameters: continuous and non-continuous. For example, theoperator that removes part of the training data has the con-tinuous parameter percentage, which decides what portion of
inputs should be deleted. Its value varies in the range 0%to 99% (as we cannot delete all training data). In contrast,mutation operators that operate on a per-layer basis have anon-continuous parameter layer, which determines the speciﬁc
layer of a neural network to mutate.
In case the parameter values are not speciﬁed by a user,
DeepCrime automatically computes the best conﬁguration for
the mutation operator. For non-continuous parameters, Deep-
Crime performs an exhaustive search by iterating through all of
the possible values for a parameter. In the case of continuousparameters, the computation is based on identifying the lowestand the highest possible values and performing a binary searchin this range. The aim of the search is to discover the mostchallenging and yet killable conﬁguration of the mutationoperator for a given test suite. For example, for the operator
356remove portion of training data (TRD in Table I) the binary
search ﬁrst checks if the most aggressive conﬁguration (99%)
is killed by the test data. If so, DeepCrime ﬁnds the middle
point in the range of possible values (49.5%) and checks it forkillability. If the middle point gets killed, the search continueson the lower part of the range (0% - 49.5%); otherwise, onthe upper half of the range (49.5% - 99%). This process isapplied in a recursive manner till the point when the size ofa new range becomes smaller than or equal to the desiredprecision /epsilon1. The observed value of the percentage parameter
that is not killed, which is /epsilon1-close to the least aggressive
killable conﬁguration, is the output of the binary search: thisnon killed mutant is the target of test generation.
The authors of DeepCrime also propose a deﬁnition of
mutation score per operator. The deﬁnition is based on theassumption that training data is a set of inputs to which atrained model is the most sensitive. Given a test set TS, its
mutation score (MS ) is the proportion of conﬁgurations killed
by both test and train set over those killed by the train set. Itis calculated as:
MS(MO,TS)=|K(MO,TS)∩K(MO,TRS ))|
|K(MO,TRS )|(1)
For example, if for the mutation operator TRD the least
aggressive killed conﬁguration found by binary search is 10%for the training data and 25% for the test data, the mutationscore will be computed as: MS=|[0.25 : 0.99]|/|[0.10 :
0.99]|= 0.74 / 0.89 = 0.83. The overall mutation score of
the test suite is computed as the average of mutation scoresacross all operators. The fact that DeepCrime offers a wide
selection of mutation operators that are based on real DLfaults and that it produces a statistically reliable outcome wasthe key motivation for us to choose this tool. The list ofDeepCrime’s mutation operators (with their parameters) thatproduced mutants that were not killed by the test sets used inour case studies can be found in Table I (killed mutants arenot the target of D
EEPMETIS’s input generation).
III. T HEDEEPMETIS TECHNIQUE
DEEPMETIS aims to augment an existing test set by ex-
tending it with mutant killing inputs that increase its mutationscore. The Algorithm 1 describes the main steps implementedin D
EEPMETIS to generate new inputs that kill mutants.
Starting from the original code of a DL model and an
existing test set, D EEPMETIS leverages DeepCrime to obtain
the conﬁgurations for which the considered mutation operatoris not killed by the original test set (for continuous operators,this is the most aggressive non killed conﬁguration found bybinary search). DeepCrime injects the corresponding mutation
into the model’s code and produces multiple original modeland mutant instances by executing ntimes the training process
on the original and the mutated model’s code, respectively(line 4). D
EEPMETIS uses evolutionary search to generate new
test inputs that can discriminate the original model instancesAlgorithm 1: Overall algorithm of D EEPMETIS
Input : tso: original test set
C: original DL program code
gmax: max number of generations
popsize: population size
mutop: mutation operatorn: number of re-training runs
o: number of original model instancesm: number of mutant instances
Output: ts
a: augmented test set
1generate original and mutant instances using DeepCrime
2original model instances M o←∅ ;
3mutant model instances M m←∅ ;
4Mo,M m←DeepCrime (C,mutop, n,m,o);
5start evolutionary search
6generation g ←0;
7archive A ←∅ ;
8initial population P0←INITPOPULATION (Mo,popsize);
9population P←P0;
10EVA L UAT E (P,Mo,Mm);
11A←UPDATE ARCHIVE (P);
12assign crowding distance to individuals
13P←SELECT (P,popsize);
14while g<g max do
15 g←g+1;
16 selection based on dominance/crowding distance
17 offspring Q ←SELTOURDCD( P,popsize);
18 substitute most dominated/misbehaving on M o
19 P←REPOPULATION (P,P0,A);
20 foreach q∈Qdo
21 q←MUTATE (q);
22 end
23 EVA L UAT E (P∪Q,Mo,Mm);
24 A←UPDATE ARCHIVE (P∪Q);
25 P←SELECT (P∪Q,popsize);
26end
27augment the test set with the archived inputs
28ts←tso∪A;
29return (ts)
from the mutated ones. The algorithm is based on NSGA-
II [23], a multi-objective evolutionary search algorithm largelyused in search-based software testing research [24]–[29].
After initialising variables g,A,P
0,P(lines 6-13), the evo-
lutionary steps are repeated for a given number of iterations,g
max. In each iteration, a population of individuals, i.e. test
inputs, is evolved, and their behaviour is evaluated againstthe original and mutant models. The result of such evalu-ation (lines 10 and 23) is the assignment of ﬁtness valuesto individuals. Based on ﬁtness values, the best individualsare identiﬁed and sorted by means of crowding distance
sorting [23], a technique that accounts for both dominance
between individuals according to the ﬁtness values as wellas the distance between individuals that belong to the samedominance front (lines 13 and 25). Then, we use tournamentselection to select the surviving individuals Q(line 17),
which are mutated by genetic operators (line 21). The worstindividuals are replaced by means of the repopulation operator,which re-introduces some of the initial seeds (P
0) into the
current population P(line 19). When mutation killing inputs
357Fig. 1. Digit input representation and mutation. (a) original input; (b) original
SVG model after vectorization; (c) SVG model mutated by moving a controlpoint; (d) mutated input
are generated, they are stored in an archive (lines 11 and 24).
Finally, the archived solutions are used to augment the initialtest suite (line 28). The test set improvement can be assessedby re-running DeepCrime to check if the previously non-killed
conﬁguration is now killed.
D
EEPMETIS’s evolutionary algorithm rewards individuals
that behave correctly on original models and misbehave onmutants. D
EEPMETIS is hybridised with novelty search, as
it also rewards individuals that exhibit the diversity of be-haviours [30], [31]. It uses an archive to store the best solutionsfound during the search in order to avoid cycling. It also usesrepopulation to escape the stagnation in local optima with thehigh basin of attraction. Preliminary experiments supported theadoption of our newly proposed non-standard twists in NSGA-II (such as the repopulation operator or the hybridisation withnovelty search), as they provide more diverse solutions thanthe standard algorithm.
A. Model-Based Input Representation
D
EEPMETIS belongs to the family of model-based test input
generators [32], i.e., tools that manipulate a model of the input
instead of directly modifying the input data (e.g., pixels). Inthe following, we will refer to the model used for manipulatinginputs as input generation model in order to distinguish it from
the models used in the DL training and prediction process.
Input data derived from an input generation model are more
likely to be realistic and belong to the input validity domainthan data subjected to low-level manipulation [29], [33]. Thisimplies that D
EEPMETIS is applicable to problems for which
an input generation model is available. The development ofinput generation models is the standard practice in severaldomains, such as cyber-physical systems, including safety-critical ones, e.g., automotive [34]. Below, we present inputgeneration models for domains we considered in our experi-mental evaluation: a vector image format for handwritten digitclassiﬁers and a 3D human eye region model for eye gazepredictors.
Digit Classiﬁcation. We consider handwritten digit samples
in the format adopted by the MNIST database [35]. Its inputsare originally encoded as 28×28images, with greyscale
levels that range from 0 to 255. As shown in Figure 1, wemodel them by adopting Scalable Vector Graphics (SVG)
1
as their representation. SVG is an XML-based vector imageformat for two-dimensional graphics, which deﬁnes shapesas combinations of cubic and quadratic B ´ezier curves. The
1https://www.w3.org/Graphics/SVG/control parameters determining the shape of a modelled digitare the start point, the end point and the control points ofeach B ´ezier curve. This representation helps in preserving
the smoothness and curvature of handwritten shapes afterminor manipulations of the curve parameters [29], [33]. Weuse the Potrace algorithm [36] to transform an MNIST inputinto its SVG model representation. This algorithm performsa sequence of operations to obtain a smooth vector imagestarting from a bitmap. To transform an SVG model back intoa28×28grayscale image, we perform rasterisation by using
two popular open-source libraries (LibRsvg
2and Cairo3).
Gaze Prediction. We focus on the input format for the
gaze estimator model proposed by Zhang et al. [37], whichtakes as an input an eye image and a 2Dhead rotation
angle (pitch and yaw) and predicts the eye gaze angle. The
eye images are generated by exploiting UnityEyes, a freely
available rendering framework [38]. Our eye model consists ofall the independent parameters used by UnityEyes to generatean eye image. They can be divided into two groups: thosethat cover various aspects related to an eye appearance (headangle, eye angle, pupil size, iris size, iris texture, skin texture)and others that describe the lighting (texture, rotation, ambientintensity, exposure for image-based lighting, and rotationand intensity for directional lighting). Only some of theseparameters are directly controllable when asking UnityEyesto generate new images, namely head rotation angles and eyerotation angles (the latter providing us with the ground-truthfor the gaze prediction), while the others are decided internallyby UnityEyes. All parameters are recorded by UnityEyes ina JSON ﬁle that accompanies a generated image. For eachpair of head and eye angles (controllable parameters), it ispossible to request UnityEyes to generate an arbitrary numberof eye images, differing among each other by the remain-ing, not directly controllable, parameters. When manipulatingUnityEyes’ parameters for the purpose of test generation, weneed to know the range in which each parameter falls inorder to ensure the validity of the manipulated values. Thus,for head and eye angles, we use the ranges suggested in theUnityEyes interface. To learn the valid ranges for the remaining
parameters, we generated a dataset of more than 1 millionimages and analysed the generated JSON ﬁles. The identiﬁed
ranges and the script used for such analysis are available inour replication package [39].
B. Fitness Functions
D
EEPMETIS optimises two ﬁtness functions, which measure
the ability of an individual to kill mutants and its diversity
from the solutions already encountered during the search.
Mutation Killing. The ﬁtness function f1measures how
close an individual is to misbehave on mutants. In particular,for a given mutant instance mut, its value is negative in
the presence of a misbehaviour, while its value is positiveand indicates the distance from a misbehaviour when the
2https://wiki.gnome.org/Projects/LibRsvg
3https://www.cairographics.org
358system behaves correctly. We estimate the distance from a
misbehaviour as the model’s conﬁdence in the predicted classfor classiﬁers or the difference between the tolerable error andthe actual prediction error for regressors. Hence, the lower thevalue assumed by such distance to misbehaviour, the higherthe mutant’s likelihood of misbehaving. To take into accountthe non-determinism of mutation and training, we generate andtrainmmutant instances. Correspondingly, the ﬁtness value
of an individual is computed as the sum of our misbehaviourcloseness metric over mmutant instances. The ﬁtness function
f
1has to be minimised:
minf1(x)=m i n/summationdisplay
mut∈ Mmeval mut(x) (2)
To compute f1for an individual x,D EEPMETIS executes the
minstances of the considered mutant with xas an input. The
deﬁnition of function eval is clearly problem speciﬁc.
Digit Classiﬁcation. The eval function exploits the classi-
ﬁer’s output softmax layer, which can be interpreted as theconﬁdence level assigned to each possible class. The predictedclass corresponds to the highest conﬁdence level, and there isa misbehaviour when the expected class has a conﬁdence levellower than another class. In particular, eval is calculated as the
difference between the conﬁdence associated with the expectedclass and the maximum conﬁdence associated with any otherclass when the prediction is correct; it is -1 otherwise.
Gaze Prediction. A misbehaviour is detected when the
prediction error exceeds the maximum tolerated error. Theprediction error is the difference between the model predictionand the expected prediction (provided as ground-truth byUnityEyes). Since predictions consist of a pair of eye rotationangles in radians (pitch and yaw), the error is calculated asthe angle between the expected vector and the predicted one.The maximum tolerated error can be set according to problem-speciﬁc requirements. In our study, we set it to 5degrees, as
this is an acceptable error in other gaze prediction applications[20], [37]. The value of f
1is the difference between such an
acceptable threshold and the actual gaze prediction error.
Diversity. The ﬁtness function f2represents an individual’s
sparseness with respect to individuals in the archive, and wewant to maximise it:
maxf
2(x)=m a x spars(x,A) (3)
whereAis the archive of solutions and xis the indi-
vidual being evaluated. Function spars measures the mini-
mum distance of an individual xfrom the solutions in the
archiveA:min y∈A,y/negationslash=xdist(x,y). The distance function (dist)
is computed on pairs of inputs and is domain-speciﬁc. ForDigit Classiﬁcation, it is computed as the Euclidean distancebetween pixel vectors. In the Gaze Prediction problem, we
use the genotypic distance, i.e., the distance between thechromosomes of two individuals, whose genes are the eyeparameters used by UnityEyes. Because in the chromosomethere are ﬂoat, vectorial and categorical gene values, to obtainan overall distance between chromosomes, we compute thedistances between genes of the same type, normalise themseparately and return the weighted sum of gene distances. Inparticular: for ﬂoat genes, we compute the difference dand
normalise it as d/(d+1); for the pitch and yaw angles’ pairs,
we calculate the angle between two vectors in radians (giventhe natural limits for eye rotation, the difference never exceeds1 radian); for categorical genes, we assign 0 to the distance ifthe genes contain the same category, 1 otherwise.
C. Initial Population
To obtain the initial population, we ﬁrst gather a set of seeds,
i.e., inputs on which the original models behave correctly.
Then, we select the most diverse seeds by computing pairwisedistances and greedily constructing the set of most diverseseeds, starting from a randomly selected ﬁrst seed up to thedesired population size. Then, initial individuals are obtainedby applying a mutation genetic operator to each selected seed.We considered as seeds the samples in the training set onwhich the models behave correctly.
D. Archive of Solutions
The best individuals encountered during the search are kept
in the archive of solutions [40]. This prevents the search for
novelty from cycling, a phenomenon where the population
moves from one area of the solution space to another andback again, without memory of the areas it has alreadyexplored [41]. At the end of the last iteration, the archive willcontain the ﬁnal solutions.
An individual of the population is a solution candidate to be
included in the archive if it behaves correctly on at least one oftheooriginal model instances and it triggers a misbehaviour
on at least one mutant model instance. When a new candidatesolution is found, it competes locally with similar solutionsalready in the archive so that only the best ones are kept, i.e.,those with the lowest value of ﬁtness function f
1.
In the archive used for Digit Classiﬁcation, a solution
competes with the archived inputs that are generated from thesame MNIST seed. In the archive used for Gaze Prediction,w e
do not rely on the starting seeds, as UnityEyes generates valideye images from any random vector of controllable parameterswithin the validity range without requiring to evolve them froman initially valid seed solution. Hence, we had to deﬁne asimilarity criterion for the archive used for Gaze Prediction:i f
the distance from the nearest neighbour in the archive is higherthan a threshold t
a, the new individual is kept in the archive.
Otherwise, the new candidate competes locally with its nearestneighbour in the archive. The threshold t
ais a parameter that
can be adjusted by a tester to obtain a proper trade-off betweenthe number of solutions that enter the archive and the diversityof the archive. To empirically choose the value of t
a,w e
recommend to (1) compute the minimum distance among arandomly selected set of diverse inputs; (2) choose a valuegreater than this number; (3) iteratively adjust this value basedon the corresponding archive size and similarity.
E. Genetic Operators
In multi-objective evolutionary algorithms, there are multi-
ple dimensions (in our case, f
1andf2) on which to compare
359Fig. 2. Eye input mutation
the individuals. We use the S ELECTION operator from NSGA-
II [23], which applies Pareto front analysis and promotes
individuals that are not dominated by any other individual.
This operator favours individuals with smaller non-dominationrank and, when the rank is equal (i.e., they belong to the samePareto front), it encourages diversity by favouring the one ina less dense region. The offspring of the current populationis obtained through tournament selection with the tournamentsize equal to 2, by choosing the best between each pair of
individuals being compared.
Each offspring individual is mutated by the M
UTATION
genetic operator, which is domain speciﬁc. For Digit Clas-
siﬁcation, the mutation genetic operator randomly chooses anSVG model’s point and applies a displacement to it in oneof the four directions in the 2D space. Then, the rasterisationoperation is applied to obtain the new digit image. For Gaze
Prediction, the mutation genetic operator randomly chooses agene from the individual’s chromosome and applies a displace-ment to its value. Then, an input image that corresponds to thenew values of the eye model’s parameters is supposed to begenerated. However, since only a small subset of parameterscan be controlled in UnityEyes, D
EEPMETIS generates a high
number of images and JSON ﬁle pairs (∼ 200) under the
desired controllable parameters. From these pairs, it selects theone that is closest to the desired mutant chromosome, checkingthat it has never been used before during the search. Figure 2shows an original eye image (left) and the correspondingmutated image (right) obtained by maintaining the controllableparameters unchanged.
During the search, exploration could get stuck in the local
optima, despite the use of ﬁtness function f
2to promote diver-
sity. To mitigate this situation and further vary the population,
DEEPMETIS uses the R EPOPULATION genetic operator, which
replaces at each iteration the individuals in the populationthat are behaving incorrectly on all the considered originalDL model’s instances. The repopulation operator also replacesa fraction of the most dominated individuals in the currentpopulation, i.e., the individuals at the bottom of the Paretofront ranking. The aggressiveness of this operator can be tunedby setting the range from which such fraction is uniformlysampled, i.e., the repopulation upper bound. As an example,if the repopulation upper bound is set to 10, at each iteration,
a number ris uniformly sampled between 1 and 10, and
then the rmost dominated individuals are replaced. The new
individuals are generated starting from a randomly chosenseed. Repopulation is applied when the archive is not empty.IV . E
XPERIMENTAL EV ALUATION
A. Subject Systems
We ran our experiments on two subject systems for which
a model of the input is available and can be manipulated viaour genetic operators: MNIST and UnityEyes.
MNIST is a publicly available dataset consisting of 70,000
images of hand-written digits. Typically, 60,000 images areused for training and the remaining 10,000 for testing. TheDL system consists of a DNN model that predicts whichdigit is represented by an input image. We considered thedeep convolutional neural network (CNN) provided by Keras,
4
because of its popularity, simplicity and effectiveness (99. 15%
test accuracy).
For the gaze prediction case study based on UnityEyes,
we use a multimodal CNN [42], which provides an im-plementation based on the LeNet network architecture [35]following the approach described in the work by Zhang etal. [37]. The CNN learns the mapping from an eye imageand a 2Dhead angle (pitch and yaw)t oa 2Deye gaze
angle. The dataset that we used for training and testing issupplied along with the model and consists of 129,285 eyeregion images (with 103,428 images used for training and25,857 for testing) synthesised with UnityEyes [38]. Each
image generated by UnityEyes is accompanied by a JSON ﬁle
describing 2Dhead angle, eye gaze vector, as well as other
parameters used to generate the image, such as skin textureand various lighting features. When presented to a model fortraining and prediction, the images are converted to grayscaleand cropped to 60×36pixels. The head angle and eye angle,
which represent the second input to the model and the groundtruth, respectively, are converted into radians.
B. Research Questions
We have performed a set of experiments to answer the
following research questions:
RQ1 (Effectiveness): Can D
EEPMETIS generate inputs
that improve a given test set in terms of mutation killing
capability?
To answer this research question, for each of our subject sys-
tems, we need an initial test set that we will then improve withthe help of D
EEPMETIS. The original test sets available for
these subjects are very large in size and successful in terms ofmutation score (100% for MNIST and 92.5% for UnityEyes).We, therefore, had to artiﬁcially construct a weaker test set forour case studies. For MNIST, we did so by removing the testinputs that are predicted with low conﬁdence (i.e., conﬁdenceless than 1) from the original test set. The elimination of suchinputs leads to a test set with smaller discriminative power, aslow conﬁdence inputs typically represent difﬁcult, corner casesthat are effective at discriminating a mutant from the originalmodel. For UnityEyes, which solves a regression, not a classi-ﬁcation problem, we instead removed inputs with the smalleststandard deviation of loss measured across 20 instances of theoriginal model. Such inputs are very discriminative, as mutants
4https://keras.io/examples/vision/mnist convnet/
360typically amplify the standard deviation of the error observed
for the original model, so the effect is more visible when westart from a small standard deviation. A similar approach toconstruct weak test sets for both classiﬁcation and regressionsystems was adopted in Humbatova et al. [20]. The approachwe used for classiﬁcation systems has also been previouslyused in the work by Jahangirova and Tonella [12] for weaktest set construction and by Byun et al. [43] for test inputprioritisation. The size of the weak test set for MNIST is 4,813elements, and for UnityEyes, it is 4,000 elements.
We then performed mutation testing of our subject sys-
tems considering the constructed weak test sets and usingDeepCrime. Out of the 24 mutation operators implementedinDeepCrime, 18 were applicable to MNIST and 17 to
UnityEyes. For operators with non-continuous parameters, weapplied every value from the list exhaustively. For operatorswith continuous parameters, we performed the binary searchon the full range of the parameter value space. We adopted thestatistical notion of mutation killing [12], using the Wilcoxontest to calculate the p-value and Cohen’s dto measure the
effect size. According to our procedure, statistical signiﬁcanceis reached when p
value<0.05and the effect size is
greater than “small”, i.e., Cohen’s d≥0.5. Overall, we
got 71 not killed mutants (i.e., mutated versions producedbyDeepCrime’s mutation operators) for MNIST and 38 for
UnityEyes.
As mutation testing suffers from the problem of equivalent
mutants, it is possible that some of the mutants not killedby our weak test set are not killable by any set of inputs,and therefore our attempts for generating inputs that kill thesemutants are vain. To avoid this situation, we use the deﬁnitionof “likely equivalent” mutants proposed by Humbatova etal. [20]. According to this deﬁnition, if a mutant is notkilled by the training data (i.e. the data the mutant shouldbe most sensitive to, as the mutant was trained on suchdata), then this mutant is deemed likely equivalent. After
ﬁltering out the likely equivalent mutants, we were left with19 mutants for MNIST and 10 for UnityEyes. The 19 MNISTmutants belong to 12 different mutation operators, while forUnityEyes 10 mutants are produced by 9 mutation operators.To make our experiments feasible, we further reduced theset of MNIST mutants by picking only one mutant for eachmutation operator.
We applied D
EEPMETIS to each of the 22 mutants. We
ﬁrst ran the initial population generation process 10 timesto obtain 10 different populations for each subject study.We then invoked the input generation process for each pairof the mutant and initial population, getting as a result 10runs of D
EEPMETIS on each mutant to account for the
non-deterministic search-based nature of our tool. In theseexperiments, D
EEPMETIS is run in the 1vs5 (1 original vs
5 mutant instances) conﬁguration.
This means that the number of mutant instances used by the
ﬁtness function f1(see Equation 2) is 5.
The next research question investigates other alternative
conﬁgurations of our tool.TABLE II
DEEPMETIS CONFIGURATIONS
Parameter MNIST UnityEyes
population size 100 12
generations 1000 100
archive threshold ta - 0.55
repopulation upper bound 10 2
RQ2 (Fitness Guidance): How does the ﬁtness function
based on a single mutant instance compare to the ﬁtnessfunction based on multiple mutant instances in guiding D
EEP-
METIS towards the generation of mutation killing inputs?
The aim of this research question is to identify whether
providing more instances of the same mutant to D EEPMETIS
increases its success in generating mutation-killing inputs. Forthis purpose, we ran D
EEPMETIS in 4 different modes by
providing it with either 1, 5, 10 or 20 instances of the samemutation (i.e., we conﬁgure it as 1vs1, 1vs5, 1vs10 and 1vs20).
Similarly to RQ1, we perform 10 runs using 10 different
initial populations. We do not evaluate extensively the effectof increasing the number of instances of the original model(e.g., 5vs5 or 10vs10), as preliminary experiments showedthat the effect of such alternative choices is negligible on theeffectiveness of the ﬁtness function, while at the same time issubstantially increasing the overall computation time.
RQ3 (Comparison with other Tools): Can we use existing
DL input generators to achieve comparable improvement inthe mutation killing capability of a test set?
To answer this research question, we compare D
EEPMETIS
to two state of the art test input generators for DL systems:DeepJanus [29] and DLFuzz [3]. DeepJanus is a model-
based tool that uses a multi-objective evolutionary algorithmto generate frontier inputs for DL systems. The frontier inputs
are deﬁned as pairs of inputs that are similar to each otherbut trigger different behaviours of a DL system. The idea isthat for a low-quality DL system, such a frontier will includepairs that intersect the validity domain, while for a high-qualityone, it will have a small or no intersection at all. In ourexperiments, we passed DeepJanus one instance of the original
model, and from the generated set of pairs of inputs, we useonly those inputs that do not trigger any misbehaviour in theoriginal model, as our goal is to obtain inputs that behavecorrectly on the original models but misbehave on the mutatedones. Another option could be passing DeepJanus the mutated
model and then using the misbehaving set of inputs. However,some preliminary runs showed that the misbehaving inputsfor the mutant almost never behave correctly on the originalmodel. Therefore, we excluded this setup from our comparisonstudy. DeepJanus can be applied to both UnityEyes and
MNIST. Moreover, it shares with D
EEPMETIS the same input
representation and mutation genetic operator, which guaranteesa fair comparison of the approaches.
DLFuzz is representative of search-based fuzzing testing
tools that generate test inputs by applying perturbations to
361the raw input (i.e., pixels) [44]. It aims to generate adversarial
inputs that maximise neuron coverage for a DL system undertest. For this purpose, DLFuzz iteratively selects neurons, the
activation of which would lead to increased neuron coverage,and applies perturbations to test inputs in order to activatethose neurons, so guiding DL systems towards exposing mis-behaviours. The publicly available version of DLFuzz
5does
not support regression systems. Therefore we could not applyit to UnityEyes. Moreover, this implementation does not workwith Python versions higher than 2.7.1, so we had to updatethe code to make it compatible with Python 3.8.
Similarly to D
EEPMETIS, both DeepJanus and DLFuzz are
affected by randomness, so we performed 10 runs of eachtool, each run using a different initial population. However,we ﬁxed the same population across runs of different tools toensure that the differences in their performance are not dueto the different starting points of the algorithms. As explainedbefore, DeepJanus uses the original model in its generation
process, not requiring a re-run for each mutant. In contrast,asDLFuzz generates only inputs that get misclassiﬁed by the
given DL model, we used the mutants. As a result, DLFuzz
had to be re-run for each considered mutant.
Overall, we performed 20 runs of DeepJanus (10 popula-
tions for the original model of both MNIST and UnityEyes),120 runs of DLFuzz (10 populations for 12 MNIST mutants)
and 220 runs of D
EEPMETIS (10 populations for 22 MNIST
and UnityEyes mutants).
For both tools, we used the conﬁguration reported as the
one achieving the best performance by their authors.
RQ4 (Fault Detection): Can the test set augmented by
DeepMetis expose more faults than the original test set?
This research question analyses whether D EEPMETIS de-
livers its promise of improving the test set so that it detectsmore faults. Since, to the best of our knowledge, there is nopublicly available dataset of reproducible real faults for DLsystems, we use DeepCrime mutants as a replacement for real
faults in a cross-validation setup.
Speciﬁcally, we perform cross-validation by leaving one
of the mutants out and augmenting the test set with all theinputs generated by D
EEPMETIS for the remaining mutants.
We ensure that none of the remaining mutants is generatedby the same mutation operator as the cross-validation mutant,assuming that mutants produced by the same operator mayhave similar properties. We then check if the augmented testset is able to kill the cross-validation mutant. This processis repeated separately for the inputs generated in each of the10 runs of D
EEPMETIS. We added the previously excluded
7 MNIST mutants to this analysis, as, although there are noinputs generated speciﬁcally for them, they can still serve ascross-validation mutants. Before proceeding with the exper-iment, we performed a redundancy analysis [20] among themutants of each subject to ensure that inputs generated forone mutant do not kill another mutant just because the latteris redundant with respect to the former. Redundancy analysis
5https://github.com/turned2670/DLFuzzshowed that all 10 UnityEyes mutants are non-redundant,while for MNIST, 6 out of 19 mutants are redundant. Weexcluded redundant mutants from further analysis, i.e., we didnot use them as cross-validation mutants.
C. Results
Columns Subject and MO in Table III indicate the DL
system and the mutation operator that provided the mutants
used by D
EEPMETIS for test input generation. For each
operator, we report in brackets the parameter values whichwere found by the binary/exhaustive search and were usedto generate the non killed mutant. For mutation operatorsthat manipulate the training data, this value indicates theratio of the affected data. For example, MNIST/TRD removes
89.72% of the training data. For the other operators, parametervalues with the preﬁx ‘l’ followed by a number indicate thelayer to which a mutation operator was applied. All the otherparameters specify the exact value used to inject the fault. Forexample, MNIST/ACH (l6; ‘sigmoid’) means that the activation
function of layer number 6 was changed from the original tothe ‘sigmoid’ one.
In Table III, the sub-columns Kindicate the killing prob-
ability, computed as the mutation score (see Equation (1))for continuous operators or as the binary killed/non-killedoutcome for discrete operator (since we did not apply D
EEP-
METIS to all the possible mutants produced by discrete oper-
ators, Equation (1) cannot be used for them). Column Weak
TSshows the killing probability Kof the initial, weak test
set. In the following columns, the sub-column Inputs shows
the average number of inputs generated across 10 runs byeach tool/tool conﬁguration, while the sub-column Kshows
the average killing probability of the test set augmented withthe generated inputs, computed across 10 runs.
1) RQ1: Effectiveness: The results for D
EEPMETIS in its
best conﬁguration (1vs5) show that for both subjects, theaugmentation of the initial test set with the D
EEPMETIS-
generated inputs leads to a substantial increase of the mutationscore. For MNIST, the improvement across the operators variesbetween 59% and 100%, with the average Kjumping from
2% to 89%. For UnityEyes, the improvement ranges from 2%to 100% on a per operator basis and the average Krises
from 29% to 68%. The number of generated inputs, whichwould require manual labelling, is 19 on average for MNISTand 517 for UnityEyes. As these numbers constitute only0.0003% of the training data set size for MNIST and 0.005%for UnityEyes, we consider the labelling effort associated with
D
EEPMETIS to be low.
RQ1:D EEPMETIS is able to achieve a substantial
improvement in killing probability on each of theprovided mutants. The magnitude of this improvementis 87% for MNIST and 39% for UnityEyes. Themanual labelling effort for the newly generated inputscan be deemed acceptable.
362TABLE III
RESULTS :COLUMN K(KILLING PROBABILITY )REPORTS MUTATION SCORE ,FOR CONTINUOUS OPERATORS ,AND BINARY KILLED /NON -KILLED
OUTCOME ,FOR DISCRETE OPERATORS ,BOTH A VERAGED ACROSS 10RUNS
SubjectWeak TS DEEP METIS DEEP METIS DEEP METIS DEEP METIS DeepJanus DLFuzz
MO (1vs1) (1vs5) (1vs10) (1vs20)
K Inputs K Inputs K Inputs K Inputs K Inputs K Inputs K
MNISTTCL (84.38%) 13% 21 92% 16 87% 18 90% 20 86% 8 62% 61 91%
TRD (89.72%) 6% 48 82% 40 89% 45 88% 18 78% 8 60% 119 85%
TUD (90.62%) 6% 23 78% 17 77% 20 73% 22 73% 8 18% 61 68%
TAN (100%) 0% 19 63% 19 81% 21 74% 22 79% 8 37% 67 43%
TCO (96.88%) 0% 14 49% 14 59% 17 69% 50 60% 8 29% 39 48%
HLR (0.064) 0% 42 85% 26 86% 27 86% 30 86% 8 70% 110 86%
HNE (1) 0% 47 87% 30 89% 35 90% 40 90% 8 64% 110 96%
ACH (l6; ’sigmoid’) 0% 20 100% 18 100% 23 100% 27 100% 8 100% 136 100%
ARM (l5) 0% 12 90% 11 100% 12 100% 14 90% 8 10% 91 100%
RAW (l0; ’l1 l2’) 0% 15 100% 11 100% 15 100% 16 100% 8 100% 52 100%
WCI (l0; ’ones’) 0% 21 100% 14 90% 24 100% 28 100% 8 80% 170 100%
OCH (’rmsprop’) 0% 15 100% 13 100% 18 100% 23 100% 8 100% 80 100%
Average 2% 25 86% 19 89% 23 89% 26 87% 8 61% 91 85%
UnityEyesTCL (21.88%) 86% 477 86% 536 88% 562 86% 604 86% 76 86% --
TRD (41.66%) 67% 335 79% 515 87% 70 67% 74 67% 76 67% --
TUD (100%) 0% 496 100% 587 100% 595 100% 662 100% 76 40% --
TAN (84.38%) 25% 379 15% 546 40% 669 38% 611 63% 76 25% --
HLR (0.0037) 39% 480 54% 557 61% 563 65% 597 72% 76 41% --
HNE (32) 70% 318 70% 454 72% 514 69% 528 70% 76 70% --
AAL (l9; ’signsoft’) 0% 40 0% 392 90% 402 60% 533 60% 76 0% --
RAW (l1; ’l2’) 0% 48 0% 480 60% 517 70% 557 60% 76 0% --
RAW (l3; ’l2’) 0% 40 0% 494 40% 549 60% 568 50% 76 0% --
WCI (l1; ’ones’) 0% 444 0% 611 40% 613 60% 123 0% 76 0% --
Average 29% 306 40% 517 68% 505 68% 486 63% 76 33% --
2) RQ2: Fitness Guidance: Columns D EEPMETIS (1vs1),
DEEPMETIS (1vs5), D EEPMETIS (1vs10), D EEPMETIS
(1vs20) report the results obtained when the ﬁtness function
uses 1, 5, 10 and all 20 instances of a mutant during the inputgeneration process, respectively. In the case of MNIST, for 3mutants out of 12, 1vs1 and 1vs5 provide the same results.For 6 operators, 1vs5 performs better; however, for 2 out ofthose, the improvement is marginal (1-3%). For the remaining3 operators, 1vs1 outperforms 1vs5, with the difference forone of the operators being only 2%. When we further compare1vs5 to 1vs10, the latter exhibits an improvement for 4, equalperformance for 5 and deterioration for 3 operators while beingsubstantially more expensive computationally. Overall, as alsoreﬂected in the average Kacross operators, for MNIST, the
optimal performance is obtained with 1vs5 and 1vs10 settings,which provide slightly better results than 1vs1 and 1vs20.
The results for UnityEyes show that 1vs5 and 1vs10 produce
the same average K(68%), which is slightly better than
1vs20 (63%), but is deﬁnitely superior when compared to 1vs1(40%). On a closer inspection, 1vs5 outperforms 1vs10 and1vs20 on 5 mutants out of 10, with the majority of them beingcontinuous operators, while 1vs10 is the best in 3 cases and1vs20 in 2. As was noted, 1vs20 on average performs similarlyto 1vs5 and 1vs10; however, in one case (WCI (l1; ’ones’)),it fails to produce any improvement at all.
The reason behind the comparative weakness of 1vs1 w.r.t.
the other settings is that its ﬁtness function has a verylimited range because it aggregates the eval value of a single
mutant instance, which provides restricted guidance to the testgeneration process.
The input generation for our experiments was performed
on various machines. It complicates the comparison of theexecution time between different conﬁgurations of D
EEP-
METIS. However, for each subject, we ensured to run all 4
conﬁgurations on the TUD operator (selected randomly) using
the same machine. For MNIST, we used a MacBook Prolaptop (2.2 GHz Intel Core i7, 6 cores, 16GB RAM), whilefor UnityEyes, we used Alienware Aurora R8 (3.60 GHz IntelCore i9-9900K, 8 cores, 32GB RAM, NVIDIA GeForce RTX2080 Ti 11 GB). For MNIST, this operator took 6, 22, 47and 57 minutes on average across 10 runs for 1vs1, 1vs5,1vs10 and 1vs20, respectively. For UnityEyes, the generationof inputs for one run on average lasted 53 (1vs1), 66 (1vs5),65 (1vs10), and 69 (1vs20) minutes. These results show that1vs5 is the optimal setting for balancing the improvement inmutation score and the time required to generate the inputs.
RQ2: The 1vs5 conﬁguration of D EEPMETIS proved
to be the optimal one. It outperforms 1vs1 by asubstantial margin, as a single mutant instance (1vs1)cannot provide enough guidance to generate effectiveinputs. The settings with a higher number of mu-tant instances are sometimes comparable in terms ofmutation score improvement, but they might requiresigniﬁcantly more computation time.
3) RQ3: Comparison with other Tools: Columns DLFuzz
and DeepJanus in Table III report the results for each of the
tools being compared to D
EEPMETIS. In the case of MNIST,
for 9 out of 12 mutants, D EEPMETIS (1vs5) performs better
than DeepJanus, while for the remaining mutants, they have
similar performance. The average Kacross all mutants for
DEEPMETIS (1vs5) is higher by 28% than for DeepJanus.
When it comes to the comparison between D EEPMETIS and
363DLFuzz,D EEPMETIS provides better results for 4 mutants,
DLFuzz for 3 mutants, and the outcome is equal for the
remaining 5. The average Kacross all mutants is 89% for
DEEPMETIS (1vs5) and 85% for DLFuzz. However, DLFuzz
generates 4.8 more inputs than D EEPMETIS (1vs5) and there-
fore requires much more manual labelling effort.
AsDLFuzz is not applicable to regression problems, the
comparison for the UnityEyes subject was only possible
between D EEPMETIS and DeepJanus. Results show that
DeepJanus is not able to produce any improvement in the
majority of the cases. The only exceptions are TUD and HLR
operators, where for the former, the average improvement is40% compared to 100% of D
EEPMETIS (1vs5), and for the
latter, the improvement of DeepJanus is limited to 2% vs 22%
of D EEPMETIS.
We performed statistical analysis on the comparison of the
results by each tool. For mutants with continuous parameters,we used the Wilcoxon statistical test to obtain the p-value
and the Vargha-Delaney ˆA
12to quantify the effect size.
For mutants with non-continuous parameters, we calculateconﬁdence intervals using Wilson’s method. When comparing
D
EEPMETIS and DeepJanus for MNIST, the difference is
statistically signiﬁcant (p-value <0.05 or conﬁdence intervals
do not intersect) for 7 mutants out of 12. For 5 out of 7mutants with continuous parameters, the effect size is large;for 1 mutant, it is medium, and for the remaining one, itis small. In case of D
EEPMETIS and DLFuzz, there is a
statistically signiﬁcant difference for 2 mutants. The effect sizeis negligible for 1, small for 3, medium for 2 and large for 1mutant. The results of the comparison of D
EEPMETIS (1vs5)
and DeepJanus on the UnityEyes subject are statistically
signiﬁcant for 5 out of 10 applied mutants. For the 6 mutantswith continuous parameters, the effects size ranges betweenlarge (3), small (2), and negligible (1).
When it comes to execution time comparison (conducted in
the same conditions as described for RQ2), for MNIST D
EEP-
METIS took on average 22 minutes, DeepJanus 9 minutes and
DLFuzz 24 minutes. For UnityEyes, D EEPMETIS took about
66 minutes on average and DeepJanus about 86 minutes.
RQ3:D EEPMETIS outperforms DLFuzz and Deep-
Janus in the task of augmenting a test set to improve
its mutation score.
4) RQ4: Fault Detection: Results are presented in Table
IV, where column MO speciﬁes the cross-validation mutant,
used to check the hypothesis that D EEPMETIS generated
inputs are also able to kill other, previously unseen mutants.Column Inputs indicates the average number of inputs that
were generated by D
EEPMETIS and added to the originally
weak test set across 10 runs. Finally, column Killed reports
the proportion of runs (out of 10) in which the augmented testset was able to kill the validation mutant.
For MNIST, almost all validation mutants were killed in all
10 runs, with the exception of ARM (l5) and RAW (l0, ’l2’)that were killed in 8 runs and WCI (l0; ’random
uniform’) thatTABLE IV
FAULT DETECTION
Subject MO Inputs Killed
MNISTTCL (84.38%) 107 10/10
TUD (90.62%) 106 10/10
TCO (96.88%) 109 10/10
HLR (0.064) 98 10/10
ACH (l6; ’hard sigmoid’) 123 10/10
ACH (l6; ’softplus’) 123 10/10
ACH (l6; ’softmax’) 123 10/10
ARM (l5) 112 8/10
RAW (l0; ’l1 l2’) 112 10/10
RAW (l0; ’l2’) 112 8/10
WCI (l0; ’ones’) 109 10/10
WCI (l0; ’random uniform’) 109 1/10
OCH (’rmsprop’) 111 10/10
UnityEyesTCL (21.88%) 4635 10/10
TRD (46.41%) 4655 1/10
TUD (100%) 4584 10/10
TAN (84.38%) 4625 10/10
HLR (0.0037) 4613 10/10
HNE (32) 4717 3/10
AAL (l9; ’signsoft’) 4779 6/10
RAW (l1; ’l2’) 4197 10/10
RAW (l3; ’l2’) 4197 5/10
WCI (l1; ’ones’) 4559 6/10
was killed in 1 run. The latter is an almost equivalent mutant,with a very low triviality score [20], which is very difﬁcult tokill for D
EEPMETIS. The results for UnityEyes also indicate
that D EEPMETIS is always able to kill the unseen mutant at
least once. For 5 mutants out of 10, the test set augmented with
DEEPMETIS inputs killed the mutant in 100% of the runs. In
all other cases except for TRD (46.41%) and HNE (32), the
augmented test set succeeds in 5 to 6 out of 10 runs.
RQ4: The mutation killing capability of the D EEP-
METIS-generated inputs holds also for previously un-
seen mutants, with 82% average success rate acrossour two subjects.
D. Threats to V alidity
Construct Validity: The choice of the distance metrics
may threaten our ﬁndings. We chose sound metrics for theconsidered domains. We used Euclidean distance when com-paring matrices of grayscale values (also used in previousstudies [29]). When comparing UnityEyes inputs, we used acombination of appropriate distances for each gene type in thechromosome.
Internal Validity: The main threat affecting the internal
validity of our results is the choice of mutation operatorsand mutation tool. We use DeepCrime, a DL mutation tool
that accounts for the stochastic nature of DL systems and DLspeciﬁc mutation operators by adopting the statistical notion ofmutation killing. Moreover, its operators are derived from realDL faults that ensure a higher degree of realism as comparedto alternatives.
364External Validity: The choice of the subject DL systems is
a possible threat to the external validity. To mitigate it, we
chose two diverse DL systems. One solves a classiﬁcation
problem, while another solves a regression problem. Theexecution of multiple original and mutant models may hinderthe generalisation to more complex DL problems, e.g. self-driving cars that require simulations to be evaluated. However,our results show that D
EEPMETIS generates effective inputs
with a limited number of models, i.e., 1 original and 5 mutated.A wider set of systems (including industrial ones) should beconsidered in future studies to further generalise our ﬁndings.
To ensure Reproducibility of our results, we share online
the source code of D
EEPMETIS, the considered subjects, and
the experimental data [39].
V. R ELATED WORK
A. Test Generation for DL Systems
Several works in the literature [2]–[4], [45], [46] propose
techniques that generate test inputs for DL systems by manip-ulating raw input data, i.e. they apply small perturbations toavailable real inputs. A limitation of these approaches is thelack of realism of the generated inputs. While these corruptedimages are useful for security testing as adversarial attacks,they are not necessarily representative of data captured bysensors of a real DL system.
Another family of testing techniques [29], [33], [47]–[51]
adopts a model-based approach that exploits model manipula-tion and model-based generation. Differently from raw inputmanipulation approaches, these techniques tend to generatemore realistic inputs if a faithful model of the input domainis adopted since the generated images are compliant with theconstraints of such a model. In this work, we adopt a model-based approach to improve the realism of the generated inputs.
Pei et al. propose a raw input manipulation technique
aimed at generating inputs that trigger inconsistencies betweenmultiple DL systems [2]. Other techniques manipulate rawimages and consider as failures the inconsistent behaviourstriggered by the original and transformed test inputs [3], [4],[45], [46].
Model-based approaches, proposed by Abdessalem et
al. [47]–[49] and Gambi et al. [50], aim to test advanceddriver-assistance systems by generating extreme and challeng-ing scenarios that maximise the number of detected systemfailures. Riccio and Tonella proposed a model-based approachthat produces test suites made of pairs of inputs that identifythe frontier of behaviours of a DL system, i.e. the inputs atwhich the DL system starts to misbehave [29]. Udeshi et al.generate inputs that highlight fairness violations by perturbingdiscriminatory parameters, e.g. gender [51]. Vahdat Pour etal. [15] use DL mutation to guide the generation of adversarialcode snippets for DL models tailored to the computation ofcode embeddings.
D
EEPMETIS differs from the existing approaches because
its goal is to increase the mutation killing ability of a testset. With the advent of DL mutation frameworks such asDeepMutation [17], MuNN [18] and DeepCrime [20], theproblem of achieving a high mutation score is increasinglyimportant, especially when mutants mimic real faults, as isthe case of DeepCrime [20].
D
EEPMETIS is the ﬁrst approach that can assist developers
in the challenging task of making a DL test set better atmutation killing.
B. Test Adequacy for DL Systems
Several test adequacy criteria have been proposed for DL
systems. Pei et al. [2] use the number of neuron activations of
the model to measure test adequacy. In particular, a neuronis considered activated if its output value is higher than apredeﬁned threshold. Ma et al. [52] propose a set of addi-tional adequacy criteria based on neuron activations. They useactivation values obtained from the training data and divide therange of values for each neuron into kbuckets. Kim et al. [6]
designed a test adequacy criterion, named surprise adequacy,
based on the degree of “surprise” of an input for the neuralnetwork. Similarly to Ma et al.’s criteria [52], bucketing isused to make the surprise measure an adequacy criterion: allkbuckets of surprise ranges must be covered by the test set.
X. Zhang et al. [53] observe how inputs are distributed acrossdifferent uncertainty patterns, i.e. combinations of alternativeuncertainty metrics (e.g., high prediction conﬁdence and lowvariation ratio). Although they do not deﬁne a proper adequacycriterion, they recommend generating additional test inputs tocover the least covered uncertainty patterns, and they showthat such inputs evade defences against adversarial attacks.
We adopt a test set’s mutation score as an adequacy crite-
rion. Like other criteria [2], [6], [52], our criterion uses thetraining set as a reference since it contains the inputs to whichthe model is mostly sensitive: the mutation score of a test setshould be as close as possible to the training set’s one.
Jahangirova & Tonella [12] compared mutation score to
other adequacy metrics such as neuron coverage [2] andsurprise coverage [6], showing that mutation score is moreeffective in differentiating between weak and strong test setsthan the existing alternatives.
D
EEPMETIS is the ﬁrst tool that uses mutation adequacy as
guidance for the generation of inputs that increase the mutationscore of an existing weak test set.
VI. C
ONCLUSIONS AND FUTURE WORK
We proposed D EEPMETIS, the ﬁrst automated test generator
for DL systems that can increase the mutation score of aweak test set, guided by mutation adequacy. Our empiricalevaluation shows that our tool outperforms state-of-the-art DLtest generators in this task. The test sets generated by D
EEP-
METIS can expose unknown faults, simulated in our leave-
one-out experiment by means of previously unseen mutants.In our future work, we plan to generalise our results to a widersample of DL systems, including industrial ones.
A
CKNOWLEDGMENT
This work was partially supported by the H2020 project
PRECRIME, funded under the ERC Advanced Grant 2017Program (ERC Grant Agreement n. 787703).
365REFERENCES
[1] C. D. Manning, P. Raghavan, and H. Sch ¨utze, Introduction to Infor-
mation Retrieval. New York, NY , USA: Cambridge University Press,
2008.
[2] K. Pei, Y . Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox
testing of deep learning systems,” in Proceedings of the 26th Symposium
on Operating Systems Principles. ACM, 2017, pp. 1–18.
[3] J. Guo, Y . Jiang, Y . Zhao, Q. Chen, and J. Sun, “Dlfuzz: differen-
tial fuzzing testing of deep learning systems,” in Proceedings of the
2018 ACM Joint Meeting on European Software Engineering Con-ference and Symposium on the Foundations of Software Engineering,ESEC/SIGSOFT FSE, 2018, pp. 739–743.
[4] Y . Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing
of deep-neural-network-driven autonomous cars,” in Proceedings of the
40th International Conference on Software Engineering , ser. ICSE ’18.
New York, NY , USA: ACM, 2018, pp. 303–314. [Online]. Available:http://doi.acm.org/10.1145/3180155.3180220
[5] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y . Liu, J. Zhao,
B. Li, J. Yin, and S. See, “Deephunter: A coverage-guided fuzztesting framework for deep neural networks,” in Proceedings of the
28th ACM SIGSOFT International Symposium on Software Testingand Analysis, ser. ISSTA 2019. New York, NY , USA: Associationfor Computing Machinery, 2019, p. 146–157. [Online]. Available:https://doi.org/10.1145/3293882.3330579
[6] J. Kim, R. Feldt, and S. Yoo, “Guiding deep learning system testing
using surprise adequacy,” in Proceedings of the 41st International
Conference on Software Engineering, ICSE, 2019, pp. 1039–1049.
[7] F. Harel-Canada, L. Wang, M. A. Gulzar, Q. Gu, and M. Kim, “Is neuron
coverage a meaningful measure for testing deep neural networks?”inESEC/FSE ’20: 28th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,Virtual Event, USA, November 8-13, 2020, 2020, pp. 851–862.
[8] Y . Jia and M. Harman, “An analysis and survey of the development of
mutation testing,” IEEE Transactions on Software Engineering, vol. 37,
no. 5, pp. 649–678, 2011.
[9] N. Humbatova, G. Jahangirova, G. Bavota, V . Riccio, A. Stocco, and
P. Tonella, “Taxonomy of real faults in deep learning systems,” inProceedings of 42nd International Conference on Software Engineering,ser. ICSE ’20. ACM, 2020, p. 12 pages.
[10] J. M. Zhang, M. Harman, L. Ma, and Y . Liu, “Machine learning test-
ing: Survey, landscapes and horizons,” IEEE Transactions on Software
Engineering, vol. Early Access, no. –, pp. 1–1, 2020.
[11] V . Riccio, G. Jahangirova, A. Stocco, N. Humbatova, M. Weiss, and
P. Tonella, “Testing machine learning based systems: a systematicmapping,” Empir . Softw. Eng., vol. 25, no. 6, pp. 5193–5254, 2020.
[Online]. Available: https://doi.org/10.1007/s10664-020-09881-0
[12] G. Jahangirova and P. Tonella, “An empirical evaluation of mutation
operators for deep learning systems,” in IEEE International Conference
on Software Testing, V eriﬁcation and V alidation, ser. ICST’20. IEEE,2020, p. 12 pages.
[13] J. Sohn, S. Kang, and S. Yoo, “Search based repair of deep neural
networks,” arXiv preprint arXiv:1912.12463, 2019.
[14] J. Wang, G. Dong, J. Sun, X. Wang, and P. Zhang, “Adversarial sample
detection for deep neural network through model mutation testing,” in2019 IEEE/ACM 41st International Conference on Software Engineering(ICSE). IEEE, 2019, pp. 1245–1256.
[15] M. V . Pour, Z. Li, L. Ma, and H. Hemmati, “A search-based testing
framework for deep neural networks of source code embedding,” inIEEE International Conference on Software Testing, V eriﬁcation andV alidation, ser. ICST’21. IEEE, 2021, p. 11 pages.
[16] J. Gunel, S. Andrea, and T. Paolo, “Quality metrics and oracles for au-
tonomous vehicles testing,” in 2021 IEEE 14th International Conference
on Software Testing, V alidation and V eriﬁcation (ICST). IEEE, 2021.
[17] L. Ma, F. Zhang, J. Sun, M. Xue, B. Li, F. Juefei-Xu, C. Xie,
L. Li, Y . Liu, J. Zhao, and Y . Wang, “Deepmutation: Mutation testingof deep learning systems,” in 29th IEEE International Symposium
on Software Reliability Engineering, ISSRE 2018, Memphis, TN,USA, October 15-18, 2018, 2018, pp. 100–111. [Online]. Available:https://doi.org/10.1109/ISSRE.2018.00021
[18] W. Shen, J. Wan, and Z. Chen, “Munn: Mutation analysis of neural
networks,” in 2018 IEEE International Conference on Software Quality,
Reliability and Security Companion (QRS-C), July 2018, pp. 108–115.[19] Q. Hu, L. Ma, X. Xie, B. Yu, Y . Liu, and J. Zhao, “Deepmutation++:
A mutation testing framework for deep learning systems,” in 2019 34th
IEEE/ACM International Conference on Automated Software Engineer-ing (ASE). IEEE, 2019, pp. 1158–1161.
[20] N. Humbatova, G. Jahangirova, and P. Tonella, “Deepcrime: Mutation
testing of deep learning systems based on real faults,” in Proceedings of
the 30th ACM SIGSOFT International Symposium on Software Testingand Analysis, 2021.
[21] M. J. Islam, G. Nguyen, R. Pan, and H. Rajan, “A comprehensive study
on deep learning bug characteristics,” in Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conferenceand Symposium on the Foundations of Software Engineering, ser.ESEC/FSE 2019. New York, NY , USA: ACM, 2019, pp. 510–520.[Online]. Available: http://doi.acm.org/10.1145/3338906.3338955
[22] Y . Zhang, Y . Chen, S.-C. Cheung, Y . Xiong, and L. Zhang, “An empirical
study on tensorﬂow program bugs,” in Proceedings of the 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis ,
ser. ISSTA 2018. New York, NY , USA: ACM, 2018, pp. 129–140.[Online]. Available: http://doi.acm.org/10.1145/3213846.3213866
[23] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist
multiobjective genetic algorithm: Nsga-ii,” IEEE Transactions on Evo-
lutionary Computation, vol. 6, no. 2, pp. 182–197, April 2002.
[24] A. Panichella, F. M. Kifetew, and P. Tonella, “Automated test case
generation as a many-objective optimisation problem with dynamicselection of the targets,” IEEE Transactions on Software Engineering,
vol. 44, no. 2, pp. 122–158, 2018.
[25] S. Yoo and M. Harman, “Pareto efﬁcient multi-objective test case
selection,” in Proceedings of the 2007 International Symposium
on Software Testing and Analysis, ser. ISSTA ’07. New York,NY , USA: ACM, 2007, pp. 140–150. [Online]. Available: http://doi.acm.org/10.1145/1273463.1273483
[26] ——, “Using hybrid algorithm for pareto efﬁcient multi-objective test
suite minimisation,” Journal of Systems and Software, vol. 83, no. 4,
pp. 689 – 701, 2010. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0164121209003069
[27] K. Mao, M. Harman, and Y . Jia, “Sapienz: Multi-objective automated
testing for android applications,” in Proceedings of the 25th
International Symposium on Software Testing and Analysis, ser. ISSTA2016. New York, NY , USA: ACM, 2016, pp. 94–105. [Online].Available: http://doi.acm.org/10.1145/2931037.2931054
[28] K. Lakhotia, M. Harman, and P. McMinn, “A multi-objective approach
to search-based test data generation,” in Proceedings of the 9th Annual
Conference on Genetic and Evolutionary Computation, ser. GECCO’07. New York, NY , USA: ACM, 2007, pp. 1098–1105. [Online].Available: http://doi.acm.org/10.1145/1276958.1277175
[29] V . Riccio and P. Tonella, “Model-based exploration of the frontier of
behaviours for deep learning system testing,” in Proceedings of the 28th
ACM Joint Meeting on European Software Engineering Conferenceand Symposium on the Foundations of Software Engineering,ser. ESEC/FSE 2020. New York, NY , USA: Association forComputing Machinery, 2020, p. 876–888. [Online]. Available: https://doi.org/10.1145/3368089.3409730
[30] J. Lehman and K. O. Stanley, “Abandoning objectives: Evolution
through the search for novelty alone,” Evolutionary Computation,
vol. 19, no. 2, pp. 189–223, 2011. [Online]. Available: https://doi.org/10.1162/EVCO
a00025
[31] B. Marculescu, R. Feldt, and R. Torkar, “Using exploration focused
techniques to augment search-based software testing: An experimentalevaluation,” in 2016 IEEE International Conference on Software Testing,
V eriﬁcation and V alidation (ICST), April 2016, pp. 69–79.
[32] M. Utting, A. Pretschner, and B. Legeard, “A taxonomy of model-based
testing approaches,” Software testing, veriﬁcation and reliability , vol. 22,
no. 5, pp. 297–312, 2012.
[33] T. Zohdinasab, V . Riccio, A. Gambi, and P. Tonella, “Deephyperion:
exploring the feature space of deep learning-based systems throughillumination search,” in Proceedings of the 30th ACM SIGSOFT Inter-
national Symposium on Software Testing and Analysis, 2021, pp. 79–90.
[34] C. Larman, Applying UML and Patterns: An Introduction to Object-
Oriented Analysis and Design. Prentice Hall, 1997.
[35] Y . LeCun, L. Bottou, Y . Bengio, P. Haffner et al., “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.
[36] P. Selinger, “Potrace: a polygon-based tracing algorithm,” http://potrace.
sourceforge.net/potrace.pdf, 2003.
366[37] X. Zhang, Y . Sugano, M. Fritz, and A. Bulling, “Appearance-based
gaze estimation in the wild,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015, pp. 4511–4520.
[38] E. Wood, T. Baltru ˇsaitis, L.-P. Morency, P. Robinson, and A. Bulling,
“Learning an appearance-based gaze estimator from one million synthe-
sised images,” in Proceedings of the Ninth Biennial ACM Symposium
on Eye Tracking Research & Applications, 2016, pp. 131–138.
[39] V . Riccio, N. Humbatova, G. Jahangirova, and P. Tonella, “Replica-
tion package for deepmetis,” https://github.com/testingautomated-usi/deepmetis, 2021.
[40] E. D. de Jong, “The incremental pareto-coevolution archive,” in Genetic
and Evolutionary Computation – GECCO 2004, K. Deb, Ed. Berlin,Heidelberg: Springer Berlin Heidelberg, 2004, pp. 525–536.
[41] J.-B. Mouret and J. Clune, “Illuminating search spaces by mapping
elites,” 2015.
[42] “An implementation of a multimodal cnn for appearance-based gaze
estimation.” https://github.com/dlsuroviki/UnityEyesModel, 2020.
[43] T. Byun, V . Sharma, A. Vijayakumar, S. Rayadurgam, and D. Cofer,
“Input prioritization for testing neural networks,” in 2019 IEEE
International Conference On Artiﬁcial Intelligence Testing (AITest).IEEE, 2019, pp. 63–70. [Online]. Available: https://doi.org/10.1109/AITest.2019.000-6
[44] S. Dola, M. B. Dwyer, and M. L. Soffa, “Distribution-aware test-
ing of neural networks using generative models,” arXiv preprint
arXiv:2102.13602, 2021.
[45] M. Zhang, Y . Zhang, L. Zhang, C. Liu, and S. Khurshid, “Deeproad:
Gan-based metamorphic testing and input validation framework forautonomous driving systems,” in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ASE,2018, pp. 132–142.
[46] S. Lee, S. Cha, D. Lee, and H. Oh, “Effective white-box testing
of deep neural networks with adaptive neuron-selection strategy,” inProceedings of the 29th ACM SIGSOFT International Symposium onSoftware Testing and Analysis, ser. ISSTA 2020. New York, NY , USA:Association for Computing Machinery, 2020, p. 165–176. [Online].Available: https://doi.org/10.1145/3395363.3397346[47] R. B. Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, “Testing
advanced driver assistance systems using multi-objective search andneural networks,” in Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering, ASE , 2016, pp. 63–74.
[48] R. B. Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and
T. Stifter, “Testing autonomous cars for feature interaction failuresusing many-objective search,” in Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, ser. ASE2018. New York, NY , USA: ACM, 2018, pp. 143–154. [Online].Available: http://doi.acm.org/10.1145/3238147.3238192
[49] R. B. Abdessalem, S. Nejati, L. C. Briand, and T. Stifter, “Testing
vision-based control systems using learnable evolutionary algorithms,”inProceedings of the 40th International Conference on Software
Engineering, ser. ICSE ’18. New York, NY , USA: ACM, 2018, pp.1016–1026. [Online]. Available: http://doi.acm.org/10.1145/3180155.3180160
[50] A. Gambi, M. M ¨uller, and G. Fraser, “Automatically testing self-driving
cars with search-based procedural content generation,” in Proceedings of
the 28th ACM SIGSOFT International Symposium on Software Testingand Analysis, ISSTA, 2019, pp. 318–328.
[51] S. Udeshi, P. Arora, and S. Chattopadhyay, “Automated directed
fairness testing,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering, ser. ASE 2018.New York, NY , USA: ACM, 2018, pp. 98–108. [Online]. Available:http://doi.acm.org/10.1145/3238147.3238165
[52] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen,
T. Su, L. Li, Y . Liu, J. Zhao, and Y . Wang, “Deepgauge:Multi-granularity testing criteria for deep learning systems,” inProceedings of the 33rd ACM/IEEE International Conference onAutomated Software Engineering, ser. ASE 2018. New York,NY , USA: ACM, 2018, pp. 120–131. [Online]. Available: http://doi.acm.org/10.1145/3238147.3238202
[53] X. Zhang, X. Xie, L. Ma, X. Du, Q. Hu, Y . Liu, J. Zhao, and S. Meng,
“Towards characterizing adversarial defects of deep learning softwarefrom the lens of uncertainty,” in Proceedings of 42nd International
Conference on Software Engineering, ser. ICSE ’20. ACM, 2020, p.12 pages.
367